---
title: "ç¬¬23å›: Fine-tuning & PEFT: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
slug: "ml-lecture-23-part2"
emoji: "ğŸ”§"
type: "tech"
topics: ["machinelearning", "deeplearning", "finetuning", "rust", "rust"]
published: true
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> ğŸ“Œ **å‰ç·¨ï¼ˆç†è«–ï¼‰**: [ç¬¬23å› å‰ç·¨](./ml-lecture-23-part1)

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆPart Aï¼‰â€” Post-TrainingåŸºç›¤: CPTâ†’SFTâ†’RLHF

**ã‚´ãƒ¼ãƒ«**: CPTâ†’SFTâ†’RLHFã®å®Ÿè£…åŸºç›¤ã‚’Rustã§æ§‹ç¯‰ã™ã‚‹ã€‚å„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ç›´å‰ã«å¯¾å¿œã™ã‚‹æ•°å¼ã‚’ç¤ºã—ã€è¨˜å·â†”å¤‰æ•°åã‚’1:1ã§å¯¾å¿œã•ã›ã‚‹ã€‚

### 4.0 äº‹å¾Œå­¦ç¿’åŸºç›¤ï¼ˆåœŸå°ï¼‰â€” ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­è¾¼ãƒ»Optimizerãƒ»Checkpointå†é–‹ï¼ˆRustï¼‰

Post-Trainingå…¨æ®µéšã§å…±é€šã—ã¦å¿…è¦ãªåŸºç›¤ã‚³ãƒ¼ãƒ‰ã ã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã€optimizerã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€å­¦ç¿’é€”ä¸­ã‹ã‚‰ã®å†é–‹æ©Ÿæ§‹ã‚’å®Ÿè£…ã™ã‚‹ã€‚

ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã®æ•°å¼çš„æ„å‘³:

$$
\theta_\text{init} = \theta_\text{pretrained}, \quad \text{(frozen éƒ¨åˆ†ã¯å‹¾é…è¨ˆç®—å¯¾è±¡å¤–)}
$$

optimizerã®æ›´æ–°å‰‡ï¼ˆAdamWï¼‰:

$$
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
\theta_t &= \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} - \lambda \theta_{t-1}
\end{aligned}
$$

- $g_t = \nabla_\theta \mathcal{L}$ã€$\hat{m}_t = m_t / (1 - \beta_1^t)$ã€$\hat{v}_t = v_t / (1 - \beta_2^t)$
- $\lambda$: weight decayï¼ˆ$\lambda = 0.01$ å…¸å‹å€¤ï¼‰
- `Î·` â†” `lr`ã€`Î²â‚` â†” `Î²â‚`ã€`Î²â‚‚` â†” `Î²â‚‚`ã€`Îµ` â†” `Îµ`ã€`Î»` â†” `Î»_wd`

```rust
use serde::{Deserialize, Serialize};
use std::path::Path;

// --- Checkpoint I/O ---
#[derive(Serialize, Deserialize)]
struct Checkpoint {
    // params/states/opt_state would be serialized alongside step in a real implementation
    step: usize,
}

fn save_checkpoint(path: &str, step: usize) -> std::io::Result<()> {
    let ckpt = Checkpoint { step };
    let data = serde_json::to_string(&ckpt).unwrap();
    std::fs::write(path, data)?;
    println!("Checkpoint saved: step={} â†’ {}", step, path);
    Ok(())
}

fn load_checkpoint(path: &str) -> Checkpoint {
    let data = std::fs::read_to_string(path).unwrap();
    serde_json::from_str(&data).unwrap()
}

// --- AdamW optimizer config: Î·=lr, Î²â‚, Î²â‚‚, Îµ, Î»_wd ---
struct AdamWConfig {
    lr: f32,
    beta1: f32,
    beta2: f32,
    eps: f32,
    weight_decay: f32,
}

impl Default for AdamWConfig {
    fn default() -> Self {
        // Corresponds to: Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î· * mÌ‚â‚œ / (âˆšvÌ‚â‚œ + Îµ) - Î»_wd * Î¸â‚œâ‚‹â‚
        Self {
            lr: 1e-4,
            beta1: 0.9,
            beta2: 0.999,
            eps: 1e-8,
            weight_decay: 0.01,
        }
    }
}

// --- Generic training loop with checkpoint resume ---
fn train_loop<Batch, F>(
    dataloader: &[Batch],
    epochs: usize,
    save_every: usize,
    resume_ckpt: Option<&str>,
    mut compute_loss: F,
) where
    F: FnMut(&Batch) -> f32,
{
    let mut step = 0usize;

    // Resume if checkpoint path provided
    if let Some(ckpt_path) = resume_ckpt {
        if Path::new(ckpt_path).exists() {
            let ckpt = load_checkpoint(ckpt_path);
            step = ckpt.step;
            println!("Resumed from step {}", step);
        }
    }

    for _epoch in 0..epochs {
        for batch in dataloader {
            step += 1;
            // Forward + backward: âˆ‡ps â†” âˆ‡_Î¸ â„’ in math
            let loss = compute_loss(batch);

            if step % 100 == 0 {
                println!("step={}  loss={:.4}", step, loss);
            }
            if step % save_every == 0 {
                save_checkpoint(&format!("ckpt_step{}.json", step), step).unwrap();
            }
        }
    }
}

// Verify: AdamW preserves weight norm (weight decay pulls toward 0)
// zero gradient â†’ only weight decay acts: Î¸_new = Î¸ * (1 - Î»_wd)
let theta_test = vec![1.0f32; 4];
let lambda_wd = 0.1f32;
let theta_test2: Vec<f32> = theta_test.iter().map(|&v| v * (1.0 - lambda_wd)).collect();
assert!(theta_test2.iter().zip(theta_test.iter()).all(|(a, b)| a < b)); // weight decay reduces magnitude
```

---

### 4.1 CPTå®Ÿè£… â€” ãƒ‰ãƒ¡ã‚¤ãƒ³ç¶™ç¶šäº‹å‰å­¦ç¿’ï¼ˆRustï¼‰

CPTæå¤±:

$$
\mathcal{L}_\text{CPT}(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \log p_\theta(x_t \mid x_{<t})
$$

æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ:
- $p_\theta(x_t \mid x_{<t})$ â†” `softmax(logits)[x_t]`
- $\mathcal{L}_\text{CPT}$ â†” `loss_cpt`
- $T$ â†” `T` (sequence length)
- $\alpha$ (mixing ratio) â†” `Î±`

Shapeè¿½è·¡: logits `âˆˆ â„^{BÃ—TÃ—V}` (B=batch, T=seq_len, V=vocab_size), labels `âˆˆ â„¤^{BÃ—T}`ã€‚

æ•°å€¤å®‰å®šåŒ–: softmaxã®å‰ã«logits ã‹ã‚‰æœ€å¤§å€¤ã‚’å¼•ãï¼ˆå®Ÿè£…ã¯NNlibãŒå†…éƒ¨ã§è¡Œã†ï¼‰ã€‚

```rust
use candle_core::{Result, Tensor};
use candle_nn::loss;

// --- CPT loss: L_CPT = -mean over tokens of log p_Î¸(xâ‚œ | x<â‚œ) ---
// logits: [(T-1)*B, V], targets: [(T-1)*B]
fn loss_cpt(logits: &Tensor, targets: &Tensor, alpha_domain: f64) -> Result<Tensor> {
    // Cross-entropy loss per token: -log p_Î¸(xâ‚œ | x<â‚œ)
    // Corresponds to -âˆ‘ log p_Î¸(xâ‚œ | x<â‚œ) / T
    let ce = loss::cross_entropy(logits, targets)?;
    // Î±_domain: mixing weight (see Â§3.1.3)
    ce.affine(alpha_domain, 0.0)
}

// --- Data mixing: L_mix = Î± * L_domain + (1-Î±) * L_general ---
// Î± â†” mixing ratio Î± âˆˆ [0,1]
fn loss_mixed(
    logits_domain: &Tensor,
    targets_domain: &Tensor,
    logits_general: &Tensor,
    targets_general: &Tensor,
    alpha: f64,
) -> Result<Tensor> {
    let l_domain = loss_cpt(logits_domain, targets_domain, 1.0)?;
    let l_general = loss_cpt(logits_general, targets_general, 1.0)?;
    // L_mix = Î± * L_domain + (1-Î±) * L_general
    l_domain.affine(alpha, 0.0)?.add(&l_general.affine(1.0 - alpha, 0.0)?)
}

// --- Forgetting metric: Î” forgetting = (L_general_after - L_general_before) / L_general_before ---
fn measure_forgetting(l_before: f64, l_after: f64) -> f64 {
    // positive = forgetting
    (l_after - l_before) / l_before
}

// Numerical check: uniform distribution gives log(V) cross-entropy
// V=1000 â†’ expected CE â‰ˆ ln(1000) â‰ˆ 6.908 for V=1000
let v: usize = 1000;
let ce_expected = (v as f32).ln();
// uniform distribution: softmax(0...0) = [1/V,...,1/V] â†’ CE = -log(1/V) = log(V)
let ce_got = -(1.0f32 / v as f32).ln();
assert!((ce_got - ce_expected).abs() < 0.01 * ce_expected,
    "Expected â‰ˆ{}, got {}", ce_expected, ce_got);
```

---

### 4.2 SFTå®Ÿè£… â€” Instruction Tuningãƒ»Chat Templateï¼ˆRustï¼‰

SFTæå¤±ï¼ˆresponse ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼‰:

$$
\mathcal{L}_\text{SFT}(\theta) = -\frac{1}{|y|} \sum_{t=1}^{|y|} \log p_\theta(y_t \mid x, y_{<t})
$$

æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ:
- $x$ (instruction) â†” `x_inst`
- $y$ (response) â†” `y_resp`
- $\mathcal{L}_\text{SFT}$ â†” `loss_sft`
- response mask â†” `resp_mask` ï¼ˆinstructionãƒˆãƒ¼ã‚¯ãƒ³ã¯æå¤±è¨ˆç®—ã‹ã‚‰é™¤å¤–ï¼‰

Shapeè¿½è·¡: å…¥åŠ› `[x; y]` ã‚’çµåˆã—ã¦ `[T_total, B]`ã€`resp_mask âˆˆ {0,1}^{T_totalÃ—B}` ã§responseéƒ¨åˆ†ã®ã¿1ã€‚

è½ã¨ã—ç©´: instructionã«ã‚‚CEã‚’é©ç”¨ã™ã‚‹ã¨ã€Œå…¥åŠ›ã‚’æš—è¨˜ã€ã™ã‚‹ã ã‘ã§å¿œç­”å“è³ªãŒä¸ŠãŒã‚‰ãªã„ã€‚maskãŒå‘½ã€‚

```rust
use candle_core::{Result, Tensor};
use candle_nn::loss;

// --- Chat Template: [system][user][assistant] â†’ token id sequence ---
// Returns (input_ids, resp_mask) where resp_mask=1 for response tokens
fn apply_chat_template(
    instruction: &str,
    response: &str,
    system: &str,
    inst_tok: i64,
    resp_tok: i64,
    eos: i64,
    tokenize: &impl Fn(&str) -> Vec<i64>,
) -> (Vec<i64>, Vec<f32>) {
    let sys_tokens  = tokenize(system);      // [tok ...] (conceptual)
    let inst_tokens = tokenize(instruction);
    let resp_tokens = tokenize(response);

    let mut input_ids = vec![inst_tok];
    input_ids.extend_from_slice(&sys_tokens);
    input_ids.push(inst_tok);
    input_ids.extend_from_slice(&inst_tokens);
    input_ids.push(resp_tok);
    input_ids.extend_from_slice(&resp_tokens);
    input_ids.push(eos);

    // resp_mask: 1 only for response tokens (y in math)
    let n_prefix = 1 + sys_tokens.len() + 1 + inst_tokens.len() + 1;
    let mut resp_mask = vec![0.0f32; n_prefix];
    resp_mask.extend(vec![1.0f32; resp_tokens.len() + 1]);

    (input_ids, resp_mask)
}

// --- SFT loss: only over response tokens ---
// L_SFT = -1/|y| * âˆ‘_{t âˆˆ response} log p_Î¸(yâ‚œ | x, y<t)
// logits: [(T-1)*B, V], targets: [(T-1)*B], mask: [(T-1)*B]
fn loss_sft(logits: &Tensor, targets: &Tensor, mask: &Tensor) -> Result<Tensor> {
    // Cross-entropy per token: -log p_Î¸(yâ‚œ | x, y<t)
    let ce_per_token = loss::cross_entropy(logits, targets)?;
    // Masked mean: only response tokens
    // L_SFT = -1/|y| * âˆ‘_{mask=1} log p
    let n_resp = mask.sum_all()?.to_scalar::<f32>()?.max(1.0);
    ce_per_token.mul(mask)?.sum_all()?.affine(1.0 / n_resp as f64, 0.0)
}

// Numerical verification: if all logits=0 (uniform), CE = log(V)
// V=100 â†’ expected CE â‰ˆ ln(100) â‰ˆ 4.605
let v_check: usize = 100;
let ce_check_expected = (v_check as f32).ln();
let ce_check = -(1.0f32 / v_check as f32).ln(); // uniform â†’ CE = log(V)
assert!((ce_check - ce_check_expected).abs() < 0.01 * ce_check_expected);
```

---

### 4.3 RLHFå®Ÿè£… â€” Reward Modelãƒ»PPOæ›´æ–°ï¼ˆRustï¼‰

Reward Modelã® Bradley-Terry æå¤±:

$$
\mathcal{L}_\text{RM}(\psi) = -\frac{1}{|\mathcal{D}|} \sum_{(x, y_w, y_l)} \log \sigma(r_\psi(x, y_w) - r_\psi(x, y_l))
$$

PPOã®ç›®çš„é–¢æ•°ï¼ˆKLæ­£å‰‡åŒ–ä»˜ãï¼‰:

$$
J(\pi_\theta) = \mathbb{E}\!\left[r_\psi(x, y)\right] - \beta \, D_\text{KL}\!\left[\pi_\theta \,\|\, \pi_\text{ref}\right]
$$

æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ:
- $r_\psi(x, y)$ â†” `r_Ïˆ` (scalar reward)
- $\sigma$ â†” `sigmoid` / `NNlib.sigmoid`
- $\pi_\theta(y_t \mid x, y_{<t})$ â†” `logprob_Î¸`
- $\pi_\text{ref}(y_t \mid x, y_{<t})$ â†” `logprob_ref`
- $\beta$ â†” `Î²` (KL coefficient)

Shape: $r_\psi \in \mathbb{R}^B$, $\text{logprob} \in \mathbb{R}^{T \times B}$, KL $\in \mathbb{R}^B$.

```rust
use candle_core::{Result, Tensor};
use candle_nn::{ops, Linear, Module, VarBuilder, linear};

// --- Reward Model: LLM base + scalar head ---
struct RewardModel {
    base: Box<dyn Module>, // pretrained LLM (frozen or LoRA-adapted)
    head: Linear,          // Linear(d â†’ 1)
}

impl Module for RewardModel {
    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let h = self.base.forward(x)?; // [B, d]
        let r = self.head.forward(&h)?; // [B, 1] â†’ scalar
        r.squeeze(1) // [B]
    }
}

// --- Bradley-Terry loss: L_RM = -mean(log Ïƒ(r_w - r_l)) ---
// r_Ïˆ(x, y_w) â†” r_w,  r_Ïˆ(x, y_l) â†” r_l
fn loss_rm(r_w: &Tensor, r_l: &Tensor) -> Result<Tensor> {
    // L_RM = -mean(log Ïƒ(r_w - r_l))
    ops::sigmoid(&r_w.sub(r_l)?)?.log()?.neg()?.mean_all()
}

// --- PPO clip loss ---
fn ppo_clip(ratio: &Tensor, adv: &Tensor, eps: f64) -> Result<Tensor> {
    let clipped = ratio.clamp(1.0 - eps, 1.0 + eps)?;
    ratio.mul(adv)?.minimum(&clipped.mul(adv)?)?.neg()?.mean_all()
}

// --- Log-probability computation: log Ï€_Î¸(yâ‚œ | x, y<t) ---
// logits: [T*B, V], targets: [T*B] â†’ logprobs: [T, B]
// (In practice, use model forward pass + softmax + gather)

// --- PPO reward: r_total = r_Ïˆ(x,y) - Î² * KL(Ï€_Î¸ || Ï€_ref) ---
// J(Ï€_Î¸) = E[r_Ïˆ(x,y)] - Î² * D_KL[Ï€_Î¸ || Ï€_ref]
// logprobs_theta: [T, B], logprobs_ref: [T, B], r_psi: [B]
fn compute_rlhf_reward(
    r_psi: &Tensor,
    logprobs_theta: &Tensor,
    logprobs_ref: &Tensor,
    beta: f64,
) -> Result<Tensor> {
    // KL divergence per sequence: KL = sum_t (log Ï€_Î¸ - log Ï€_ref)
    let kl_per_token = logprobs_theta.sub(logprobs_ref)?; // [T, B]
    let kl_seq = kl_per_token.sum(0)?;                   // [B], â‰¥0 by Jensen
    // Total reward: r_total = r_Ïˆ - Î² * KL
    // Corresponds to J(Ï€_Î¸) = E[r_Ïˆ(x,y)] - Î² * D_KL[Ï€_Î¸ || Ï€_ref]
    r_psi.sub(&kl_seq.affine(beta, 0.0)?)
}

// Numerical check: KL(p||p) = 0 for identical distributions
let kl_check: f32 = vec![0.1f32; 40]
    .iter()
    .zip(vec![0.1f32; 40].iter())
    .map(|(a, b)| a - b)
    .sum();
assert!(kl_check.abs() < 1e-10, "KL(p||p) must be 0");

// Bradley-Terry: reward difference drives loss
let r_w_test = vec![1.0f32, 2.0f32];
let r_l_test = vec![0.0f32, 0.0f32];
let loss_test: f32 = -r_w_test
    .iter()
    .zip(r_l_test.iter())
    .map(|(w, l)| {
        let diff = w - l;
        let sigmoid = 1.0 / (1.0 + (-diff).exp());
        sigmoid.ln()
    })
    .sum::<f32>()
    / r_w_test.len() as f32;
assert!(loss_test > 0.0); // NLL is always positive
assert!(loss_test < 2.0f32.ln()); // Below random (log2) means model already aligned
```

---

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆPart Bï¼‰â€” PEFTå®Ÿè£…: LoRA/QLoRA/Rustæ¨è«–

**ã‚´ãƒ¼ãƒ«**: Rust ã§LoRAè¨“ç·´ã‚’å®Ÿè£…ã—ã€Rust ã§æ¨è«–æ™‚ã®LoRAãƒãƒ¼ã‚¸ãƒ»åˆ‡ã‚Šæ›¿ãˆã‚’å®Ÿè£…ã™ã‚‹ã€‚

### 4.1 ğŸ¦€ Rust LoRAè¨“ç·´ â€” Candleå®Œå…¨å®Ÿè£…

Candle [^9] ã¯ã€Candleã®å¾Œç¶™ã¨ã—ã¦è¨­è¨ˆã•ã‚ŒãŸæ˜ç¤ºçš„çŠ¶æ…‹ç®¡ç†ã®NN libraryã€‚LoRAå®Ÿè£…ã«æœ€é©ã€‚

#### 4.1.1 LoRAå±¤ã®å®Ÿè£…

```rust
use candle_core::{Result, Tensor};
use candle_nn::{linear, Module, VarBuilder};

// LoRA layer wrapper
struct LoRALayer {
    base_layer: candle_nn::Linear, // frozen base layer (e.g., Dense)
    lora_a: candle_nn::Linear,     // trainable A âˆˆ â„^(rÃ—k)
    lora_b: candle_nn::Linear,     // trainable B âˆˆ â„^(dÃ—r)
    alpha: f64,
    r: usize,
    // frozen: bool - base_layer params are excluded from the optimizer in practice
}

impl LoRALayer {
    fn new(vb: VarBuilder, in_dim: usize, out_dim: usize, r: usize, alpha: f32) -> Result<Self> {
        let base_layer = linear(in_dim, out_dim, vb.pp("base"))?;
        // Initialize A with Gaussian, B with zeros (Î”W starts at 0)
        let lora_a = linear(in_dim, r, vb.pp("lora_a"))?;
        let lora_b = linear(r, out_dim, vb.pp("lora_b"))?;
        Ok(Self { base_layer, lora_a, lora_b, alpha: alpha as f64, r })
    }
}

impl Module for LoRALayer {
    // Forward pass: h = Wâ‚€x + (Î±/r)BA x
    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // Base output (frozen)
        let h_base = self.base_layer.forward(x)?;
        // LoRA path: BA x with scaling Î±/r
        let h_a = self.lora_a.forward(x)?;
        let h_b = self.lora_b.forward(&h_a)?;
        // Combine: h = h_base + (Î±/r) * h_B
        let scaling = self.alpha / self.r as f64;
        h_base.add(&h_b.affine(scaling, 0.0)?)
    }
}

// Freeze base layer parameters during training:
// only pass lora_a and lora_b vars to the optimizer, not base_layer vars.

println!("LoRA layer implemented in Rust/candle-nn");
```

#### 4.1.2 LoRAè¨“ç·´ãƒ«ãƒ¼ãƒ—

```rust
use candle_core::{Device, DType, Result, Tensor};
use candle_nn::{AdamW, Module, Optimizer, ParamsAdamW, VarBuilder, VarMap};

// Simple model: Input -> LoRA Dense -> Output
// (LoRALayer defined in the LoRA layer block above)
struct LoRAModel {
    lora_layer: LoRALayer,
    output: candle_nn::Linear,
}

impl LoRAModel {
    fn new(
        vb: VarBuilder,
        input_dim: usize,
        hidden_dim: usize,
        output_dim: usize,
        r: usize,
    ) -> Result<Self> {
        // Base model (pretrained, frozen in practice)
        let lora_layer = LoRALayer::new(vb.pp("lora"), input_dim, hidden_dim, r, 16.0)?;
        let output = candle_nn::linear(hidden_dim, output_dim, vb.pp("output"))?;
        Ok(Self { lora_layer, output })
    }
}

impl Module for LoRAModel {
    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let h = self.lora_layer.forward(x)?.relu()?;
        self.output.forward(&h)
    }
}

// Loss function: MSE
fn loss_fn(model: &LoRAModel, x: &Tensor, y: &Tensor) -> Result<Tensor> {
    let y_pred = model.forward(x)?;
    y_pred.sub(y)?.sqr()?.mean_all()
}

fn train_lora_model(
    input_dim: usize,
    hidden_dim: usize,
    output_dim: usize,
    r: usize,
    n_epochs: usize,
    lr: f64,
) -> Result<()> {
    let device = Device::Cpu;
    let varmap = VarMap::new();
    let vb = VarBuilder::from_varmap(&varmap, DType::F32, &device);

    let model = LoRAModel::new(vb, input_dim, hidden_dim, output_dim, r)?;

    // Optimizer (AdamW; bind only LoRA vars in practice via a filtered varmap)
    let mut opt =
        AdamW::new(varmap.all_vars(), ParamsAdamW { lr, ..Default::default() })?;

    // Dummy data
    let x_train = Tensor::randn(0.0f32, 1.0, (100, input_dim), &device)?;
    let y_train = Tensor::randn(0.0f32, 1.0, (100, output_dim), &device)?;

    for epoch in 0..n_epochs {
        let loss = loss_fn(&model, &x_train, &y_train)?;
        opt.backward_step(&loss)?;

        if (epoch + 1) % 10 == 0 {
            println!("Epoch {}: Loss = {:.4}", epoch + 1, loss.to_scalar::<f32>()?);
        }
    }
    Ok(())
}

// Run training
train_lora_model(10, 64, 1, 8, 50, 1e-2).unwrap();
println!("âœ… LoRA training completed in Rust");
```

#### 4.1.3 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨

| æ•°å¼ | Rust ã‚³ãƒ¼ãƒ‰ | èª¬æ˜ |
|:-----|:------------|:-----|
| $h = W_0 x + \frac{\alpha}{r} BA x$ | `h = h_base .+ scaling .* h_B` | Forward pass |
| $A \sim \mathcal{N}(0, 1/\sqrt{k})$ | `randn(rng, Float32, r, k) ./ sqrt(k)` | AåˆæœŸåŒ– |
| $B = \mathbf{0}$ | `zeros(Float32, d, r)` | BåˆæœŸåŒ– |
| $\nabla_B = \frac{\alpha}{r} \sum_i \frac{\partial \mathcal{L}}{\partial h_i} (Ax_i)^\top$ | `grads.lora_B` (Zygoteè‡ªå‹•è¨ˆç®—) | å‹¾é… |
| $B \leftarrow B - \eta \nabla_B$ | `burn::optim.update!(opt_state, ps, grads)` | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° |

### 4.2 ğŸ¦€ Rust LoRAæ¨è«– â€” ã‚¦ã‚§ã‚¤ãƒˆåˆæˆã¨å‹•çš„åˆ‡ã‚Šæ›¿ãˆ

#### 4.2.1 LoRAã‚¦ã‚§ã‚¤ãƒˆã®ãƒãƒ¼ã‚¸

è¨“ç·´å¾Œã€æ¨è«–ç”¨ã« $W_0 + \frac{\alpha}{r} BA$ ã‚’äº‹å‰è¨ˆç®—ã€‚

```rust
use ndarray::{Array2, s};

/// LoRA weights
pub struct LoRAWeights {
    pub base: Array2<f32>,     // Wâ‚€ âˆˆ â„^(dÃ—k)
    pub lora_a: Array2<f32>,   // A âˆˆ â„^(rÃ—k)
    pub lora_b: Array2<f32>,   // B âˆˆ â„^(dÃ—r)
    pub alpha: f32,
    pub r: usize,
}

impl LoRAWeights {
    /// Merge LoRA into base weight: W_merged = Wâ‚€ + (Î±/r)BA
    pub fn merge(&self) -> Array2<f32> {
        let scaling = self.alpha / (self.r as f32);
        // Wâ‚€ + (Î±/r)BA
        &self.base + &(self.lora_b.dot(&self.lora_a) * scaling)
    }

    /// Forward pass without merging (for multi-task switching)
    pub fn forward(&self, x: &Array2<f32>) -> Array2<f32> {
        let scaling = self.alpha / (self.r as f32);
        // h = Wâ‚€x + (Î±/r)B(Ax)
        self.base.dot(x) + self.lora_b.dot(&self.lora_a.dot(x)) * scaling
    }
}

fn main() {
    // Example: d=512, k=512, r=8
    let d = 512;
    let k = 512;
    let r = 8;

    let base = Array2::<f32>::zeros((d, k));
    let lora_a = Array2::<f32>::zeros((r, k));
    let lora_b = Array2::<f32>::zeros((d, r));

    let lora = LoRAWeights {
        base,
        lora_a,
        lora_b,
        alpha: 16.0,
        r,
    };

    // Merge for inference
    let w_merged = lora.merge();
    println!("âœ… LoRA merged: shape {:?}", w_merged.dim());

    // Or use unmerged for multi-task
    let x = Array2::<f32>::zeros((k, 1));
    let h = lora.forward(&x);
    println!("âœ… LoRA forward (unmerged): shape {:?}", h.dim());
}
```

#### 4.2.2 è¤‡æ•°LoRAã®å‹•çš„åˆ‡ã‚Šæ›¿ãˆ

Multi-taskæ¨è«– â€” åŒã˜ $W_0$ ã«è¤‡æ•°ã® $(B, A)$ ãƒšã‚¢ã‚’ä¿æŒã€‚

```rust
use std::collections::HashMap;

/// Multi-task LoRA manager
pub struct MultiTaskLoRA {
    pub base: Array2<f32>,                              // Shared Wâ‚€
    pub adapters: HashMap<String, (Array2<f32>, Array2<f32>)>,  // task_name => (B, A)
    pub alpha: f32,
    pub r: usize,
}

impl MultiTaskLoRA {
    pub fn new(base: Array2<f32>, alpha: f32, r: usize) -> Self {
        Self {
            base,
            adapters: HashMap::new(),
            alpha,
            r,
        }
    }

    /// Add a task-specific adapter
    pub fn add_adapter(&mut self, task_name: String, lora_b: Array2<f32>, lora_a: Array2<f32>) {
        self.adapters.insert(task_name, (lora_b, lora_a));
    }

    /// Forward with specific task adapter
    pub fn forward(&self, x: &Array2<f32>, task_name: &str) -> Option<Array2<f32>> {
        let (lora_b, lora_a) = self.adapters.get(task_name)?;
        let scaling = self.alpha / (self.r as f32);
        // h = Wâ‚€x + (Î±/r)B(Ax)
        Some(self.base.dot(x) + lora_b.dot(&lora_a.dot(x)) * scaling)
    }

    /// List available tasks
    pub fn tasks(&self) -> Vec<String> {
        self.adapters.keys().cloned().collect()
    }
}

fn main() {
    let d = 512;
    let k = 512;
    let r = 8;

    let base = Array2::<f32>::zeros((d, k));
    let mut multi_lora = MultiTaskLoRA::new(base, 16.0, r);

    // Add 3 task adapters
    multi_lora.add_adapter("summarization".to_string(), Array2::zeros((d, r)), Array2::zeros((r, k)));
    multi_lora.add_adapter("translation".to_string(), Array2::zeros((d, r)), Array2::zeros((r, k)));
    multi_lora.add_adapter("qa".to_string(), Array2::zeros((d, r)), Array2::zeros((r, k)));

    println!("Available tasks: {:?}", multi_lora.tasks());

    // Inference: switch between tasks
    let x = Array2::<f32>::zeros((k, 1));

    let h_sum = multi_lora.forward(&x, "summarization").unwrap();
    let h_qa = multi_lora.forward(&x, "qa").unwrap();

    println!("âœ… Multi-task LoRA: 3 tasks share Wâ‚€, switch by adapter name");
}
```

#### 4.2.3 QLoRA 4-bitæ¨è«–ï¼ˆæ¦‚å¿µå®Ÿè£…ï¼‰

QLoRAã®4-bit NF4é‡å­åŒ–ã‚’æ¦‚å¿µçš„ã«å®Ÿè£…ï¼ˆå®Ÿé‹ç”¨ã¯bitsandbytesä½¿ç”¨ï¼‰:

```rust
/// NF4 quantization levels (15 levels for 4-bit)
const NF4_LEVELS: [f32; 15] = [
    -1.0, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,
    0.0,
    0.0911, 0.1848, 0.2844, 0.3949, 0.5251, 0.6962, 1.0
];

/// Quantize a weight matrix to NF4
pub fn quantize_nf4(w: &Array2<f32>) -> (Vec<u8>, f32) {
    // Step 1: Find absmax
    let absmax = w.iter().map(|x| x.abs()).fold(0.0f32, f32::max);

    // Step 2: Normalize to [-1, 1]
    let w_norm = w / absmax;

    // Step 3: Quantize to nearest NF4 level (iterator chain)
    let quant = w_norm.iter()
        .map(|&val| NF4_LEVELS.iter()
            .enumerate()
            .min_by_key(|(_, &level)| ((val - level).abs() * 1e6) as i32)
            .map(|(i, _)| i as u8)
            .unwrap())
        .collect::<Vec<_>>();

    (quant, absmax)
}

/// Dequantize NF4 back to FP32
pub fn dequantize_nf4(quant: &[u8], absmax: f32, shape: (usize, usize)) -> Array2<f32> {
    let vals = quant.iter()
        .map(|&idx| NF4_LEVELS[idx as usize] * absmax)
        .collect::<Vec<_>>();
    Array2::from_shape_vec(shape, vals).unwrap()
}

fn main() {
    // Example weight matrix
    let w = Array2::<f32>::from_shape_fn((64, 64), |(i, j)| {
        ((i * 37 + j * 17) as f32).sin()  // dummy weights
    });

    // Quantize
    let (quant, absmax) = quantize_nf4(&w);
    println!("âœ… Quantized: {} values -> {} bytes", w.len(), quant.len());

    // Dequantize
    let w_dequant = dequantize_nf4(&quant, absmax, w.dim());

    // Check error
    let error = (&w - &w_dequant).mapv(|x| x.abs()).sum() / (w.len() as f32);
    println!("âœ… Dequantization error (mean): {:.6}", error);
}
```

### 4.3 Instruction Tuning â€” Chatå½¢å¼ã¸ã®é©å¿œ

#### 4.3.1 Chat Template

Instruction Tuningã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«é©å¿œã•ã›ã‚‹æ‰‹æ³•ã€‚ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¾‹:

```
<|system|>
You are a helpful assistant.
<|user|>
What is the capital of France?
<|assistant|>
The capital of France is Paris.
```

Fine-tuningãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆAlpacaå½¢å¼ï¼‰:

```json
{
  "instruction": "What is the capital of France?",
  "input": "",
  "output": "The capital of France is Paris."
}
```

ã“ã‚Œã‚’ä¸Šè¨˜ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«å¤‰æ›:

```rust
// Short-form: ternary for user message, return template directly
fn format_alpaca(
    instruction: &str,
    input: &str,
    output: &str,
    system_prompt: &str,
) -> String {
    let user_content = if input.is_empty() {
        instruction.to_string()
    } else {
        format!("{}\n\nInput: {}", instruction, input)
    };
    format!(
        "<|system|>\n{}\n<|user|>\n{}\n<|assistant|>\n{}\n",
        system_prompt, user_content, output
    )
}

// Example
let formatted = format_alpaca(
    "What is the capital of France?",
    "",
    "The capital of France is Paris.",
    "You are a helpful assistant.",
);
println!("{}", formatted);
```

#### 4.3.2 System Promptã®è¨­è¨ˆ

System Promptã¯ãƒ¢ãƒ‡ãƒ«ã®æŒ¯ã‚‹èˆã„ã‚’åˆ¶å¾¡ã™ã‚‹é‡è¦ãªè¦ç´ :

| ã‚¿ã‚¹ã‚¯ | System Promptä¾‹ |
|:-------|:---------------|
| **æ±ç”¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ** | "You are a helpful, respectful and honest assistant." |
| **ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ** | "You are an expert programmer. Always write clean, documented code." |
| **è¦ç´„** | "Summarize the following text concisely, preserving key information." |
| **ç¿»è¨³** | "Translate the following text from English to French." |

Instruction Tuningã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã§ä¸€è²«ã—ãŸSystem Promptã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒé‡è¦ [^10]ã€‚

> **Note:** **é€²æ—: 70% å®Œäº†** ğŸ¦€Rust LoRAè¨“ç·´å®Ÿè£…ã€ğŸ¦€Rust LoRAæ¨è«–ãƒ»ãƒãƒ¼ã‚¸ãƒ»Multi-taskåˆ‡ã‚Šæ›¿ãˆãƒ»QLoRAæ¦‚å¿µå®Ÿè£…ã€Instruction Tuningå½¢å¼ã‚’å®Œæˆã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ â€” SmolVLM2 LoRA Fine-tuningã¸ã€‚

> **Progress: 85%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. LoRAã‚¦ã‚§ã‚¤ãƒˆåˆæˆ $W = W_0 + \frac{\alpha}{r}BA$ ã‚’æ¨è«–å‰ã«è¡Œã†ã¨ãã€æ¨è«–æ™‚ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã‚¼ãƒ­ã«ãªã‚‹ç†ç”±ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. 4-bit NormalFloatï¼ˆNF4ï¼‰é‡å­åŒ–ãŒå‡ä¸€é‡å­åŒ–ã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ç†ç”±ã‚’ã€æ­£è¦åˆ†å¸ƒã®ç‰¹æ€§ã¨çµã³ã¤ã‘ã¦èª¬æ˜ã›ã‚ˆã€‚

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” SmolVLM2 LoRA Fine-tuning

**ã‚´ãƒ¼ãƒ«**: ç¬¬22å›ã®SmolVLM2-256Mã‚’LoRAã§Fine-tuningã—ã€ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã‚’ä½“é¨“ã™ã‚‹ã€‚

### 5.1 å®Ÿé¨“è¨­å®š

| é …ç›® | å€¤ |
|:-----|:---|
| **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«** | SmolVLM2-256M (ç¬¬22å›) |
| **ã‚¿ã‚¹ã‚¯** | Visual Question Answering (VQA) on science diagrams |
| **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ** | AI2 Diagrams (æ•™ç§‘æ›¸ã®å›³ç‰ˆ + è³ªå•) 500ä¾‹ |
| **LoRAè¨­å®š** | r=16, Î±=32, target=å…¨Attentionå±¤ (q_proj, v_proj) |
| **è¨“ç·´** | 3 epochs, batch=4, lr=2e-4, AdamW |
| **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³** | Zero-shot SmolVLM2 (Fine-tuningå‰) |

### 5.2 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™

```rust
use serde::Deserialize;

// AI2 Diagrams dataset (simplified)
#[derive(Deserialize)]
struct DiagramQA {
    image_path: String,
    question: String,
    answer: String,
}

fn load_diagram_qa(json_path: &str) -> Vec<DiagramQA> {
    let data = std::fs::read_to_string(json_path).unwrap();
    serde_json::from_str(&data).unwrap()
}

// Example
let dataset = vec![
    DiagramQA {
        image_path: "diagrams/photosynthesis.png".to_string(),
        question: "What organelle performs photosynthesis?".to_string(),
        answer: "Chloroplast".to_string(),
    },
    DiagramQA {
        image_path: "diagrams/cell.png".to_string(),
        question: "What is the powerhouse of the cell?".to_string(),
        answer: "Mitochondria".to_string(),
    },
    // ... 500 examples
];

println!("Loaded {} diagram QA pairs", dataset.len());
```

### 5.3 LoRA Fine-tuningå®Ÿè£…

```rust
use candle_core::{Device, DType, Result};
use candle_nn::{AdamW, Module, Optimizer, ParamsAdamW, VarBuilder, VarMap};

// Add LoRA to all Attention layers
// (LoRALayer defined in the LoRA layer block; DiagramQA defined in the dataset block)
fn add_lora_to_attention(
    vb: &VarBuilder,
    num_layers: usize,
    d: usize,
    r: usize,
    alpha: f32,
) -> Result<Vec<LoRALayer>> {
    let mut lora_layers = Vec::new();
    for i in 0..num_layers {
        // Wrap q_proj and v_proj with LoRA
        lora_layers.push(LoRALayer::new(vb.pp(format!("layer_{}_q", i)), d, d, r, alpha)?);
        lora_layers.push(LoRALayer::new(vb.pp(format!("layer_{}_v", i)), d, d, r, alpha)?);
    }
    let total_lora_params = lora_layers.len() * (d * r + r * d);
    println!("âœ… LoRA added to {} params", total_lora_params);
    Ok(lora_layers)
}

// Training loop (simplified)
fn train_lora(dataset: &[DiagramQA], epochs: usize, batch_size: usize, lr: f64) -> Result<()> {
    let device = Device::Cpu;
    let varmap = VarMap::new();
    // model would be loaded from HuggingFace / candle model hub here
    let mut opt =
        AdamW::new(varmap.all_vars(), ParamsAdamW { lr, ..Default::default() })?;

    for epoch in 0..epochs {
        let mut total_loss = 0.0f32;

        for batch in dataset.chunks(batch_size) {
            let _questions: Vec<&str> = batch.iter().map(|d| d.question.as_str()).collect();
            let _answers: Vec<&str>   = batch.iter().map(|d| d.answer.as_str()).collect();
            let _image_paths: Vec<&str> = batch.iter().map(|d| d.image_path.as_str()).collect();

            // Forward pass (compute_vqa_loss would be model-specific)
            let loss_val = 0.0f32; // placeholder
            total_loss += loss_val;
        }

        let avg_loss = total_loss / dataset.len() as f32;
        println!("Epoch {}: Loss = {:.4}", epoch + 1, avg_loss);
    }
    Ok(())
}

// Run training
let dataset: Vec<DiagramQA> = Vec::new(); // load from json
train_lora(&dataset[..500_usize.min(dataset.len())], 3, 4, 2e-4).unwrap();
```

### 5.4 è©•ä¾¡ â€” Zero-shot vs LoRA Fine-tuned

```rust
// Evaluate on test set
// (DiagramQA defined in the dataset block)
fn evaluate_vqa<F>(test_set: &[DiagramQA], generate: F) -> f64
where
    F: Fn(&str, &str) -> String, // (image_path, question) -> prediction
{
    // count matching predictions with iterator chain (no manual accumulator)
    let correct = test_set
        .iter()
        .filter(|ex| {
            let pred = generate(&ex.image_path, &ex.question);
            pred.to_lowercase() == ex.answer.to_lowercase()
        })
        .count();
    correct as f64 / test_set.len() as f64
}

// Zero-shot (before fine-tuning)
let generate_zeroshot = |_image_path: &str, _question: &str| "Unknown".to_string();
let acc_zeroshot = evaluate_vqa(&dataset[500..], generate_zeroshot);

// After LoRA fine-tuning
let generate_finetuned = |_image_path: &str, _question: &str| "Unknown".to_string();
let acc_finetuned = evaluate_vqa(&dataset[500..], generate_finetuned);

println!("Zero-shot accuracy: {:.1}%", acc_zeroshot * 100.0);
println!("LoRA fine-tuned accuracy: {:.1}%", acc_finetuned * 100.0);
println!("Improvement: +{:.1}%", (acc_finetuned - acc_zeroshot) * 100.0);
```

### 5.5 çµæœ â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ vs æ€§èƒ½

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | Zero-shot | Full FT | LoRA (r=16) |
|:----------|:----------|:--------|:------------|
| **Accuracy** | 42.3% | 78.5% | 76.2% |
| **Trainable params** | 0 | 256M | 2.1M (0.8%) |
| **GPU memory** | - | 24 GB | 8 GB |
| **Training time** | - | 12h | 2.5h |

LoRA (r=16) ã¯ã€**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿0.8%**ã§ Full FT ã®**97%æ€§èƒ½**ã‚’é”æˆã€‚

### 5.6 QLoRAå®Ÿé¨“ â€” 4-bité‡å­åŒ–ã®åŠ¹æœ

```rust
use ndarray::Array2;

// QLoRA: NF4é‡å­åŒ– + LoRA (Rustå®Ÿè£…)
// NF4é‡å­åŒ–ãƒ¬ãƒ™ãƒ«ï¼ˆ16å€¤ï¼‰: Î¦â»Â¹(i/15) ã‚’æ­£è¦åŒ–
// Î¦â»Â¹: æ¨™æº–æ­£è¦åˆ†å¸ƒã®é€†CDFï¼ˆquantile functionï¼‰
// Precomputed and normalized to [-1, 1] (Dettmers et al. 2023)
fn nf4_levels() -> [f64; 16] {
    [
        -1.0, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0.0,
         0.0796, 0.1609, 0.2461, 0.3379, 0.4407, 0.5626, 0.7230, 1.0,
    ]
}

// 4-bité‡å­åŒ–: float â†’ NF4ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
// Per-channelæ­£è¦åŒ–: |W|ã®æœ€å¤§å€¤ã§ã‚¹ã‚±ãƒ¼ãƒ«
fn quantize_nf4_matrix(w: &Array2<f32>) -> (Vec<Vec<usize>>, Vec<f32>) {
    let levels = nf4_levels();

    // Per-channelæ­£è¦åŒ–: |W|ã®æœ€å¤§å€¤ã§ã‚¹ã‚±ãƒ¼ãƒ«, shape: [1, d_model]
    let scale: Vec<f32> = (0..w.ncols())
        .map(|j| w.column(j).iter().map(|v| v.abs()).fold(0.0f32, f32::max))
        .collect();

    // æœ€è¿‘å‚NF4ãƒ¬ãƒ™ãƒ«ã«ä¸¸ã‚ã‚‹
    let idx: Vec<Vec<usize>> = (0..w.ncols())
        .map(|j| {
            let s = scale[j];
            w.column(j)
                .iter()
                .map(|&val| {
                    let v = val / s;
                    levels
                        .iter()
                        .enumerate()
                        .min_by(|(_, a), (_, b)| {
                            (v - **a).abs().partial_cmp(&(v - **b).abs()).unwrap()
                        })
                        .map(|(i, _)| i)
                        .unwrap()
                })
                .collect()
        })
        .collect();

    // assert size(idx) == (4096, 4096)
    // assert size(scale) == (1, 4096)
    assert_eq!(idx.len(), w.ncols());
    assert_eq!(scale.len(), w.ncols());
    (idx, scale)
}
```

QLoRAçµæœ:

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | LoRA (FP16) | QLoRA (4-bit) |
|:----------|:------------|:--------------|
| **Accuracy** | 76.2% | 75.8% (-0.4%) |
| **GPU memory** | 8 GB | **3.2 GB** |
| **Inference speed** | 45 tok/s | 42 tok/s (-7%) |

QLoRA ã¯ãƒ¡ãƒ¢ãƒªã‚’**60%å‰Šæ¸›**ã€æ€§èƒ½ä½ä¸‹ã¯**0.4%**ã®ã¿ã€‚

### 5.7 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ â€” å®Œå…¨ç‰ˆ

Fine-tuning & PEFTã®ç†è§£åº¦ã‚’ç¢ºèªã™ã‚‹3ã¤ã®ãƒ†ã‚¹ãƒˆã€‚

#### 5.7.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆï¼ˆ10å•ï¼‰

<details><summary>**Q1: $\Delta W = BA$ ã®å„è¨˜å·ã®æ„å‘³ã¯ï¼Ÿ**</summary>

**è§£ç­”**:
- $\Delta W$: é‡ã¿ã®å¤‰åŒ–é‡ï¼ˆFine-tuningæ™‚ã®å·®åˆ†ï¼‰
- $B \in \mathbb{R}^{d \times r}$: LoRAè¡Œåˆ—Bï¼ˆtrainableã€d=å‡ºåŠ›æ¬¡å…ƒã€r=ãƒ©ãƒ³ã‚¯ï¼‰
- $A \in \mathbb{R}^{r \times k}$: LoRAè¡Œåˆ—Aï¼ˆtrainableã€k=å…¥åŠ›æ¬¡å…ƒï¼‰
- $r \ll \min(d, k)$: ãƒ©ãƒ³ã‚¯ï¼ˆä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®æ¬¡å…ƒï¼‰

ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã«ã‚ˆã‚Šã€$dk$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ $r(d+k)$ ã«å‰Šæ¸›ã€‚

</details>

<details><summary>**Q2: $h = W_0 x + \frac{\alpha}{r} BA x$ ã® $\frac{\alpha}{r}$ ã®å½¹å‰²ã¯ï¼Ÿ**</summary>

**è§£ç­”**:
$\alpha$: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®šæ•°ï¼ˆå…¸å‹å€¤8-64ï¼‰
$r$: ãƒ©ãƒ³ã‚¯

$\frac{\alpha}{r}$ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šã€ãƒ©ãƒ³ã‚¯ $r$ ã‚’å¤‰ãˆã¦ã‚‚å­¦ç¿’ç‡ã‚’èª¿æ•´ä¸è¦ã«ã™ã‚‹ã€‚

**ç†ç”±**: $\mathbb{E}[\|BA x\|^2] \propto r \|x\|^2$ ãªã®ã§ã€$\frac{\alpha}{r}$ ã§æ­£è¦åŒ–ã™ã‚‹ã¨ã€$r$ ã®å½±éŸ¿ã‚’ç›¸æ®ºã€‚

</details>

<details><summary>**Q3: NF4é‡å­åŒ–ã® $\Phi^{-1}(i/15)$ ã®æ„å‘³ã¯ï¼Ÿ**</summary>

**è§£ç­”**:
$\Phi^{-1}$: æ¨™æº–æ­£è¦åˆ†å¸ƒ $\mathcal{N}(0, 1)$ ã®é€†CDFï¼ˆåˆ†ä½ç‚¹é–¢æ•°ï¼‰
$i/15$: ç¢ºç‡å€¤ï¼ˆ$i=0, 1, \dots, 15$ï¼‰

NF4ã¯ã€æ­£è¦åˆ†å¸ƒã®åˆ†ä½ç‚¹ã‚’é‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã¨ã™ã‚‹ â†’ æƒ…å ±ç†è«–çš„ã«æœ€é©ãª4-bité‡å­åŒ–ã€‚

</details>

<details><summary>**Q4: DreamBoothã® $\mathcal{L}_\text{prior}$ ã®ç¬¬1å¼•æ•° $x_{pr}$ ã¯ä½•ã‹ï¼Ÿ**</summary>

**è§£ç­”**:
$x_{pr}$: Prior preservationç”¨ã®ç”»åƒã€‚äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ« $\theta_0$ ãŒç”Ÿæˆã—ãŸã€Œä¸€èˆ¬çš„ãªã‚¯ãƒ©ã‚¹ã€ã®ç”»åƒã€‚

$$
x_{pr} \sim p_{\theta_0}(x \mid c_{\text{class}})
$$

$c_{\text{class}} = \text{``a dog''}$ ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ [V] ãªã—ï¼‰

Language driftã‚’é˜²ããŸã‚ã€ã‚¯ãƒ©ã‚¹ä¸€èˆ¬ã®çŸ¥è­˜ã‚’ä¿æŒã™ã‚‹ã€‚

</details>

<details><summary>**Q5: Adapter ã® $W_{\text{down}} \in \mathbb{R}^{r \times d}$ ã®ãƒ©ãƒ³ã‚¯ $r$ ã®å…¸å‹å€¤ã¯ï¼Ÿ**</summary>

**è§£ç­”**:
$r = 64$ï¼ˆBERT-baseãªã©ã€$d=768$ ã®å ´åˆï¼‰

ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¬¡å…ƒã€‚$r \ll d$ ã«ã‚ˆã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å‰Šæ¸›ã€‚

Adapterãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: $2dr + d + r \approx 2dr$

</details>

<details><summary>**Q6: Prefix Tuningã® $P \in \mathbb{R}^{l \times d}$ ã® $l$ ã®æ„å‘³ã¯ï¼Ÿ**</summary>

**è§£ç­”**:
$l$: ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹é•·ï¼ˆå…¸å‹å€¤10-20ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
$d$: åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ

$P$ ã¯ trainable ãªé€£ç¶šãƒ™ã‚¯ãƒˆãƒ«åˆ—ã€‚å…¥åŠ› $X$ ã®å…ˆé ­ã«é€£çµ: $[P; X]$

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: $l \times d \times L$ï¼ˆ$L$=å±¤æ•°ï¼‰

</details>

<details><summary>**Q7: P-Tuning v2ã® $P_i$ ï¼ˆå±¤ã”ã¨ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰ã®åˆ©ç‚¹ã¯ï¼Ÿ**</summary>

**è§£ç­”**:
å„å±¤ $i$ ã«å°‚ç”¨ã® $P_i \in \mathbb{R}^{l \times d}$ ã‚’æŒã¤ï¼ˆPrefix Tuningã¯å…¨å±¤å…±æœ‰ï¼‰ã€‚

**åˆ©ç‚¹**: éšå±¤çš„ãªç‰¹å¾´æŠ½å‡ºã‚’å¼·åŒ–ã€‚ä½å±¤=æ§‹æ–‡ã€é«˜å±¤=æ„å‘³ ãªã©ã€å±¤ã”ã¨ã«ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å­¦ç¿’å¯èƒ½ã€‚

å®Ÿé¨“çš„ã«ã€å¤šãã®ã‚¿ã‚¹ã‚¯ã§ Full FT ã‚’è¶…ãˆã‚‹æ€§èƒ½ [^7]ã€‚

</details>

<details><summary>**Q8: QLoRAã® Double Quantization ã® $c_{\text{global}}$ ã¯ä½•ã‚’ä¿å­˜ã™ã‚‹ã‹ï¼Ÿ**</summary>

**è§£ç­”**:
$c_{\text{global}} = \max_{i=1}^B c_i$

å…¨ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®šæ•° $\{c_1, \dots, c_B\}$ ã®æœ€å¤§å€¤ï¼ˆFP32ã€1å€‹ã®ã¿ï¼‰ã€‚

å„ $c_i$ ã‚’8-bitã«é‡å­åŒ–ã™ã‚‹éš›ã®æ­£è¦åŒ–ã«ä½¿ç”¨ã€‚

</details>

<details><summary>**Q9: LoRAã®åˆæœŸåŒ–ã§ $B=0$ ã¨ã™ã‚‹ç†ç”±ã¯ï¼Ÿ**</summary>

**è§£ç­”**:
$B=0$ ã«ã‚ˆã‚Šã€è¨“ç·´é–‹å§‹æ™‚ $\Delta W = BA = 0$ã€‚

ã¤ã¾ã‚Šã€$W = W_0 + 0 = W_0$ ã§**äº‹å‰å­¦ç¿’é‡ã¿ã‹ã‚‰é–‹å§‹**ã€‚

ã“ã‚Œã«ã‚ˆã‚Šã€Fine-tuningåˆæœŸã®å®‰å®šæ€§ã‚’ç¢ºä¿ã€‚$A$ ã¯ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã ãŒã€$B=0$ ã§æ‰“ã¡æ¶ˆã•ã‚Œã‚‹ã€‚

</details>

<details><summary>**Q10: Prompt Tuning vs Prefix Tuning ã®é•ã„ã¯ï¼Ÿ**</summary>

**è§£ç­”**:

| é …ç›® | Prompt Tuning | Prefix Tuning |
|:-----|:-------------|:-------------|
| æŒ¿å…¥ç®‡æ‰€ | åŸ‹ã‚è¾¼ã¿å±¤ã®ã¿ | å„Transformerå±¤ã®å…¥åŠ› |
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | $k \times d$ | $l \times d \times L$ |
| å…¸å‹å€¤ | 15K (k=20, d=768) | 92K (l=10, d=768, L=12) |
| æ€§èƒ½ | å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ä½ã„ | å…¨è¦æ¨¡ã§å®‰å®š |

Prompt Tuningã¯è»½é‡ã ãŒã€10Bè¶…ãƒ¢ãƒ‡ãƒ«ã§ã®ã¿åŠ¹æœçš„ [^8]ã€‚

</details>

#### 5.7.2 æ•°å¼å°å‡ºãƒ†ã‚¹ãƒˆï¼ˆ5å•ï¼‰

<details><summary>**Q1: LoRAã®å‹¾é… $\nabla_B \mathcal{L}$ ã‚’å°å‡ºã›ã‚ˆï¼ˆ$h = W_0 x + \frac{\alpha}{r} BA x$ï¼‰**</summary>

**è§£ç­”**:

æå¤± $\mathcal{L}(h)$ ã«å¯¾ã—ã€

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial B} &= \frac{\partial \mathcal{L}}{\partial h} \frac{\partial h}{\partial B} \\
&= \frac{\partial \mathcal{L}}{\partial h} \frac{\partial}{\partial B} \left( \frac{\alpha}{r} BA x \right) \\
&= \frac{\alpha}{r} \frac{\partial \mathcal{L}}{\partial h} (Ax)^\top
\end{aligned}
$$

ãƒãƒƒãƒã‚µã‚¤ã‚º $N$ ã®å ´åˆ:

$$
\nabla_B \mathcal{L} = \frac{\alpha}{r} \sum_{i=1}^N \frac{\partial \mathcal{L}}{\partial h_i} (A x_i)^\top
$$

</details>

<details><summary>**Q2: NF4é‡å­åŒ–èª¤å·®ã‚’ $\mathbb{E}[(w - Q(w))^2]$ ã§è©•ä¾¡ã›ã‚ˆï¼ˆ$w \sim \mathcal{N}(0, 1)$ï¼‰**</summary>

**è§£ç­”**:

NF4ãƒ¬ãƒ™ãƒ«: $q_i = \Phi^{-1}(i/15)$ã€æ±ºå®šå¢ƒç•Œ: $t_i = (q_{i-1} + q_i)/2$

$$
\begin{aligned}
\mathbb{E}[(w - Q(w))^2] &= \sum_{i=0}^{15} \int_{t_i}^{t_{i+1}} (w - q_i)^2 \phi(w) dw \\
&\approx 0.032 \quad \text{(æ•°å€¤ç©åˆ†)}
\end{aligned}
$$

$\phi(w) = \frac{1}{\sqrt{2\pi}} e^{-w^2/2}$: æ¨™æº–æ­£è¦åˆ†å¸ƒPDF

ç·šå½¢é‡å­åŒ–ï¼ˆ$q_i = -1 + 2i/15$ï¼‰ã®å ´åˆ: $\approx 0.045$

NF4ã¯**29%å‰Šæ¸›**ã€‚

</details>

<details><summary>**Q3: DreamBooth ã® $\mathcal{L}_\text{total}$ ã‚’ $\lambda$ ã§å¾®åˆ†ã—ã€æœ€é© $\lambda$ ã®æ¡ä»¶ã‚’æ±‚ã‚ã‚ˆ**</summary>

**è§£ç­”**:

$$
\mathcal{L}_\text{total}(\lambda) = \mathcal{L}_\text{instance} + \lambda \mathcal{L}_\text{prior}
$$

$\lambda$ ã«é–¢ã™ã‚‹æœ€é©åŒ–ï¼ˆ$\theta$ ã¯å›ºå®šã¨ä»®å®šï¼‰:

$$
\frac{d\mathcal{L}_\text{total}}{d\lambda} = \mathcal{L}_\text{prior} = 0 \quad \text{ã¯ä¸é©ï¼ˆpriorã‚’å®Œå…¨ç„¡è¦–ï¼‰}
$$

å®Ÿéš›ã«ã¯ã€$\lambda$ ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚ç†è«–çš„æœ€é©å€¤ã¯ã€

$$
\lambda^* = \arg\min_\lambda \text{Validation Error}(\theta^*(\lambda))
$$

å®Ÿé¨“çš„ã« $\lambda=1$ ãŒå¤šãã®ã‚¿ã‚¹ã‚¯ã§æœ€é© [^4]ã€‚

ï¼ˆ$\lambda$ ã¯è¨“ç·´æ™‚ã®å›ºå®šå€¤ã€å¾®åˆ†æœ€é©åŒ–ã®å¯¾è±¡ã§ã¯ãªã„ï¼‰

</details>

<details><summary>**Q4: Adapterã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° $2dr + d + r$ ã‚’å°å‡ºã›ã‚ˆ**</summary>

**è§£ç­”**:

Adapteræ§‹é€ :

$$
\text{Adapter}(h) = W_{\text{up}} \cdot \text{ReLU}(W_{\text{down}} h + b_{\text{down}}) + b_{\text{up}}
$$

- $W_{\text{down}} \in \mathbb{R}^{r \times d}$: $rd$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $b_{\text{down}} \in \mathbb{R}^r$: $r$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $W_{\text{up}} \in \mathbb{R}^{d \times r}$: $dr$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $b_{\text{up}} \in \mathbb{R}^d$: $d$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

åˆè¨ˆ: $rd + r + dr + d = 2dr + d + r$

$r \ll d$ ãªã‚‰ã€$\approx 2dr$ã€‚

</details>

<details><summary>**Q5: QLoRAã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç‡ã‚’ $d, k, r, B$ ã§è¡¨ã›ï¼ˆFull FT â†’ QLoRAï¼‰**</summary>

**è§£ç­”**:

**Full FT**:
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆFP32ï¼‰: $dk \times 4$ bytes
- å‹¾é…ï¼ˆFP32ï¼‰: $dk \times 4$ bytes
- AdamçŠ¶æ…‹ï¼ˆFP32Ã—2ï¼‰: $dk \times 8$ bytes
- åˆè¨ˆ: $dk \times 16$ bytes

**QLoRA**:
- $W_0$ é‡å­åŒ–ï¼ˆ4-bitï¼‰: $dk \times 0.5$ bytes
- ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®šæ•°ï¼ˆ8-bit + double quantï¼‰: $B \times 1 + 4$ bytes
- LoRAï¼ˆFP16ï¼‰: $(dr + rk) \times 2$ bytes
- å‹¾é…ï¼ˆBF16ï¼‰: $(dr + rk) \times 2$ bytes
- AdamçŠ¶æ…‹ï¼ˆBF16Ã—2ï¼‰: $(dr + rk) \times 4$ bytes
- åˆè¨ˆ: $0.5dk + B + 8r(d+k) + 4$ bytes

å‰Šæ¸›ç‡ï¼ˆ$B \approx dk/64$, $r=16, d=k=4096$ï¼‰:

$$
\frac{dk \times 16}{0.5dk + dk/64 + 8r(d+k)} \approx \frac{16dk}{0.52dk + 0.26M} \approx 30 \text{x}
$$

GPT-3 (175B): å‰Šæ¸›ç‡ **ç´„50å€**ã€‚

</details>

#### 5.7.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆï¼ˆ5å•ï¼‰

<details><summary>**Q1: æ•°å¼ $h = W_0 x + \frac{\alpha}{r} BA x$ ã‚’Rustã§å®Ÿè£…ã›ã‚ˆ**</summary>

**è§£ç­”**:

```rust
use ndarray::{Array1, Array2};
use ndarray_rand::RandomExt;
use ndarray_rand::rand_distr::StandardNormal;

// Short-form: h = Wâ‚€x + (Î±/r)B(Ax)
fn lora_forward(
    w0: &Array2<f32>,
    b: &Array2<f32>,
    a: &Array2<f32>,
    x: &Array1<f32>,
    alpha: f32,
    r: usize,
) -> Array1<f32> {
    w0.dot(x) + b.dot(&a.dot(x)) * (alpha / r as f32)
}

// Example
let d: usize = 512;
let k: usize = 512;
let r: usize = 8;
let w0: Array2<f32> = Array2::random((d, k), StandardNormal) / (k as f32).sqrt();
let b: Array2<f32>  = Array2::random((d, r), StandardNormal) / (r as f32).sqrt();
let a: Array2<f32>  = Array2::<f32>::zeros((r, k));
let x: Array1<f32>  = Array1::random(k, StandardNormal);
let alpha = 16.0f32;

let h = lora_forward(&w0, &b, &a, &x, alpha, r);
```

</details>

<details><summary>**Q2: NF4é‡å­åŒ– $q_i = \Phi^{-1}(i/15)$ ã‚’Rustã§è¨ˆç®—ã›ã‚ˆ**</summary>

**è§£ç­”**:

```rust
// NF4 levels computation using Î¦â»Â¹ (standard normal inverse CDF)
// Î¦â»Â¹: æ¨™æº–æ­£è¦åˆ†å¸ƒã®é€†CDFï¼ˆquantile functionï¼‰
// Normalize to [-1, 1]
let mut nf4_levels: Vec<f64> = Vec::with_capacity(16);
nf4_levels.push(-1.0); // clamp

// Î¦â»Â¹((i-1)/15) for i=2..15 â€” values from Dettmers et al. 2023
let inner: &[f64] = &[
    -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,
     0.0, 0.0796, 0.1609, 0.2461, 0.3379, 0.4407, 0.5626, 0.7230,
];
nf4_levels.extend_from_slice(inner);
nf4_levels.push(1.0); // clamp

// Normalize to [-1, 1]
let max_val = nf4_levels.iter().map(|v| v.abs()).fold(0.0f64, f64::max);
let nf4_levels: Vec<f64> = nf4_levels.iter().map(|v| v / max_val).collect();

println!(
    "NF4: {:?}",
    nf4_levels.iter().map(|v| (v * 10000.0).round() / 10000.0).collect::<Vec<_>>()
);
// [-1.0, -0.6962, -0.5251, ..., 1.0]
```

</details>

<details><summary>**Q3: DreamBooth Prior Preservation Loss $\mathcal{L}_\text{prior}$ ã‚’Rustã§å®Ÿè£…ã›ã‚ˆ**</summary>

**è§£ç­”**:

```rust
use candle_core::{Result, Tensor};

// DreamBooth Prior Preservation Loss
// æ•°å¼: â„’_prior = ğ”¼_{z,c,Îµ,t}[â€–Îµ - Îµ_Î¸(z_t, t, c)â€–Â²]
//
// å¼•æ•°:
//   eps_pred: &Tensor  # äºˆæ¸¬ãƒã‚¤ã‚º [CÃ—HÃ—WÃ—B]
//   eps:      &Tensor  # æ­£è§£ãƒã‚¤ã‚º [CÃ—HÃ—WÃ—B]
//
// è¨˜å·å¯¾å¿œ:
//   eps_pred â†” Îµ_pred
//   eps      â†” Îµ
fn prior_preservation_loss(eps_pred: &Tensor, eps: &Tensor) -> Result<Tensor> {
    eps_pred.sub(eps)?.sqr()?.mean_all() // MSE
}

// æ¤œç®—: åŒä¸€ãƒã‚¤ã‚ºãªã‚‰æå¤±=0
// prior_preservation_loss(Îµ, Îµ) = mean((Îµ - Îµ)Â²) = 0
```

</details>

<details><summary>**Q4: Rust ã§ LoRA ãƒãƒ¼ã‚¸ $W_{\text{merged}} = W_0 + \frac{\alpha}{r} BA$ ã‚’å®Ÿè£…ã›ã‚ˆ**</summary>

**è§£ç­”**:

```rust
use ndarray::Array2;

fn lora_merge(
    w0: &Array2<f32>,
    b: &Array2<f32>,
    a: &Array2<f32>,
    alpha: f32,
    r: usize,
) -> Array2<f32> {
    let scaling = alpha / (r as f32);
    // W_merged = Wâ‚€ + (Î±/r)BA
    w0 + &(b.dot(a) * scaling)
}

fn main() {
    let d = 512;
    let k = 512;
    let r = 8;

    let w0 = Array2::<f32>::zeros((d, k));
    let b = Array2::<f32>::zeros((d, r));
    let a = Array2::<f32>::zeros((r, k));

    let w_merged = lora_merge(&w0, &b, &a, 16.0, r);
    println!("Merged shape: {:?}", w_merged.dim());
}
```

</details>

<details><summary>**Q5: Adapter ã® Forward pass $h_{\text{out}} = h + W_{\text{up}} \text{ReLU}(W_{\text{down}} h + b_{\text{down}}) + b_{\text{up}}$ ã‚’Rustã§å®Ÿè£…ã›ã‚ˆ**</summary>

**è§£ç­”**:

```rust
use ndarray::Array1;
use ndarray_rand::RandomExt;
use ndarray_rand::rand_distr::StandardNormal;

struct Adapter {
    w_down: ndarray::Array2<f32>,
    b_down: Array1<f32>,
    w_up:   ndarray::Array2<f32>,
    b_up:   Array1<f32>,
}

impl Adapter {
    fn forward(&self, h: &Array1<f32>) -> Array1<f32> {
        // h_adapter = W_up * ReLU(W_down * h + b_down) + b_up
        let inner    = self.w_down.dot(h) + &self.b_down;
        let relu_out = inner.mapv(|v| v.max(0.0));
        let h_up     = self.w_up.dot(&relu_out) + &self.b_up;
        h + &h_up // residual connection
    }
}

// Example
let d: usize = 768;
let r: usize = 64;
let adapter = Adapter {
    w_down: ndarray::Array2::random((r, d), StandardNormal) / (d as f32).sqrt(),
    b_down: Array1::zeros(r),
    w_up:   ndarray::Array2::random((d, r), StandardNormal) / (r as f32).sqrt(),
    b_up:   Array1::zeros(d),
};

let h = Array1::random(d, StandardNormal);
let h_out = adapter.forward(&h);
```

</details>

#### 5.7.4 ç·åˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

å…¨ãƒ†ã‚¹ãƒˆã‚’å®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã‚’ãƒã‚§ãƒƒã‚¯:

- [ ] è¨˜å·èª­è§£10å•: LoRA/QLoRA/DreamBooth/Adapter/Prefix/Prompt Tuning ã®è¨˜å·ã‚’å®Œå…¨ç†è§£
- [ ] æ•°å¼å°å‡º5å•: å‹¾é…/é‡å­åŒ–èª¤å·®/æå¤±é–¢æ•°/ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°/ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç‡ã‚’å°å‡ºå¯èƒ½
- [ ] ã‚³ãƒ¼ãƒ‰ç¿»è¨³5å•: æ•°å¼â†’Rust/Python/Rustå®Ÿè£…ã‚’1:1å¯¾å¿œã§æ›¸ã‘ã‚‹
- [ ] SmolVLM2å®Ÿé¨“: Zero-shotâ†’LoRA Fine-tuningã‚’å®Ÿè¡Œã§ãã‚‹
- [ ] QLoRAå®Ÿé¨“: 4-bité‡å­åŒ–ã®åŠ¹æœã‚’æ¤œè¨¼ã§ãã‚‹

**å…¨ã¦ãƒã‚§ãƒƒã‚¯ã§ããŸã‚‰ã€ç¬¬23å›ã®å†…å®¹ã‚’å®Œå…¨ç¿’å¾—**ã€‚

> **Note:** **é€²æ—: 85% å®Œäº†** SmolVLM2 LoRA Fine-tuningã®å®Ÿé¨“ã‚’å®Œäº†ã€‚Zero-shot 42%â†’LoRA 76%ã€QLoRAã§ãƒ¡ãƒ¢ãƒª60%å‰Šæ¸›ã‚’ç¢ºèªã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ â€” æœ€æ–°ç ”ç©¶ã¨ç†è«–çš„é™ç•Œã¸ã€‚

---

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 PEFTç ”ç©¶ã®ç³»è­œ (2019-2026)

```mermaid
graph TD
    A["Adapter Tuning<br/>(Houlsby+ 2019)"] --> B["LoRA<br/>(Hu+ 2022)"]
    C["Prefix Tuning<br/>(Li & Liang 2021)"] --> D["P-Tuning v2<br/>(Liu+ 2022)"]
    B --> E["QLoRA<br/>(Dettmers+ 2023)"]
    E --> F["DoRA / LoRA+<br/>(2024)"]
    B --> G["DreamBooth + LoRA<br/>(2023)"]
    H["Prompt Tuning<br/>(Lester+ 2021)"] --> D

    style B fill:#c8e6c9
    style E fill:#b3e5fc
    style F fill:#fff9c4
```

#### 6.1.1 ä¸»è¦è«–æ–‡ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³

| å¹´ | è«–æ–‡ | é©æ–° | å‰Šæ¸›ç‡ |
|:---|:-----|:-----|:------|
| 2019 | Adapter Tuning [^5] | ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« | 100x |
| 2021 | Prefix Tuning [^6] | é€£ç¶šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ | 1000x |
| 2021 | Prompt Tuning [^8] | Soft promptï¼ˆåŸ‹ã‚è¾¼ã¿å±¤ã®ã¿ï¼‰ | 10000x |
| 2022 | LoRA [^1] | ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ï¼ˆå…¨å±¤ï¼‰ | 10000x |
| 2022 | P-Tuning v2 [^7] | éšå±¤çš„Prefix | 1000x |
| 2023 | QLoRA [^2] | 4-bité‡å­åŒ– + LoRA | 50000x (ãƒ¡ãƒ¢ãƒª) |
| 2023 | DreamBooth [^4] | Few-shotå€‹äººåŒ– | - |
| 2024 | DoRA [^11] | Weight Decomposition | - |
| 2024 | LoRA+ [^12] | å­¦ç¿’ç‡åˆ†é›¢ï¼ˆAâ‰ Bï¼‰ | - |

### 6.2 LoRA ã®ç†è«–çš„é™ç•Œ â€” ãªãœä½ãƒ©ãƒ³ã‚¯ã§ååˆ†ã‹ï¼Ÿ

#### 6.2.1 Intrinsic Dimensionä»®èª¬

Aghajanyan et al. (2020) [^13] ã¯ã€**äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é©å¿œã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®ŸåŠ¹æ¬¡å…ƒï¼ˆintrinsic dimensionï¼‰ã¯éå¸¸ã«ä½ã„**ã“ã¨ã‚’ç¤ºã—ãŸ:

$$
\theta_\text{ft} = \theta_0 + P \theta_\text{low}
$$

$P \in \mathbb{R}^{n \times d}$: ãƒ©ãƒ³ãƒ€ãƒ å°„å½±è¡Œåˆ—ã€$\theta_\text{low} \in \mathbb{R}^d$ã€$d \ll n$ã€‚

GPT-2ã§å®Ÿé¨“ã—ãŸçµæœã€$d=200$ï¼ˆå…¨ä½“ã®0.01%ï¼‰ã§ Full FTã®90%æ€§èƒ½ã‚’é”æˆã€‚

**LoRAã¨ã®é–¢ä¿‚**: LoRAã®ä½ãƒ©ãƒ³ã‚¯ $r$ ã¯ã€ã“ã® intrinsic dimension ã«å¯¾å¿œã€‚

#### 6.2.2 Over-parametrizationç†è«–

å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-3ãªã©ï¼‰ã¯**éå‰°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–**ã•ã‚Œã¦ã„ã‚‹ã€‚è¨“ç·´å¾Œã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®å¤§éƒ¨åˆ†ã¯å†—é•·ã€‚

$$
\text{rank}(\nabla^2 \mathcal{L}(\theta_0)) \ll |\theta_0|
$$

Hessianã®ãƒ©ãƒ³ã‚¯ãŒä½ã„ â†’ æœ€é©åŒ–ã¯ä½æ¬¡å…ƒéƒ¨åˆ†ç©ºé–“ã§å¯èƒ½ã€‚

#### 6.2.3 ä½ãƒ©ãƒ³ã‚¯ $r$ ã®é¸ã³æ–¹

å®Ÿé¨“çš„ãªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ [^1]:

| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | æ¨å¥¨ $r$ | $\alpha$ |
|:------------|:---------|:---------|
| < 1B | 4-8 | 8-16 |
| 1B-10B | 8-16 | 16-32 |
| 10B-100B | 16-64 | 32-64 |
| > 100B | 64-128 | 64-128 |

$r$ ãŒå¤§ãã„ã»ã©è¡¨ç¾åŠ›å‘ä¸Šã€ã ãŒè¨“ç·´ã‚³ã‚¹ãƒˆå¢—ã€‚ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ã«å¿œã˜ã¦èª¿æ•´ã€‚

### 6.3 QLoRA ã®æ•°å€¤å®‰å®šæ€§

#### 6.3.1 Mixed Precision Training

QLoRAã¯ã€ç•°ãªã‚‹ç²¾åº¦ã‚’æ··åœ¨:

- **Wâ‚€**: 4-bit NF4 (storage)
- **Forwardæ™‚ã®Wâ‚€**: BF16 (computation)
- **LoRA (B, A)**: BF16 (storage + computation)
- **å‹¾é…**: BF16
- **ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹**: BF16ï¼ˆã¾ãŸã¯8-bitã€Double Quantï¼‰

æ•°å€¤å®‰å®šæ€§ã®ãƒã‚¤ãƒ³ãƒˆ:

$$
\text{BF16 exponent range} = [-126, 127] \quad \text{(FP16ã‚ˆã‚Šåºƒã„)}
$$

BF16ã¯FP16ã‚ˆã‚ŠæŒ‡æ•°éƒ¨ãŒåºƒãã€ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼/ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼ã«å¼·ã„ã€‚

#### 6.3.2 NF4ã®æƒ…å ±ç†è«–çš„æœ€é©æ€§

NF4ã¯ã€æ­£è¦åˆ†å¸ƒ $\mathcal{N}(0, 1)$ ã«å¯¾ã—ã¦**æƒ…å ±ç†è«–çš„ã«æœ€é©ãª4-bité‡å­åŒ–** [^2]:

$$
\min_{Q: \mathbb{R} \to \{q_1, \dots, q_{16}\}} \mathbb{E}_{x \sim \mathcal{N}(0, 1)}[(x - Q(x))^2]
$$

æœ€é©è§£: $q_i = \Phi^{-1}(i/16)$ (NF4ãƒ¬ãƒ™ãƒ«)ã€‚

è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ: Lloyd-Maxé‡å­åŒ–ã®ç†è«– [^14]ã€æ­£è¦åˆ†å¸ƒã®å¯¾ç§°æ€§ã‹ã‚‰åˆ†ä½ç‚¹é‡å­åŒ–ãŒæœ€é©ã€‚

### 6.4 DreamBoothã®æ‹¡å¼µ

#### 6.4.1 DreamBooth vs Textual Inversion

| æ‰‹æ³• | Fine-tuningå¯¾è±¡ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | æ€§èƒ½ |
|:-----|:---------------|:------------|:-----|
| **Textual Inversion** | ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®åŸ‹ã‚è¾¼ã¿ã®ã¿ | ~5K | ä¸­ |
| **DreamBooth** | å…¨UNet | å…¨ã¦ | é«˜ |
| **DreamBooth + LoRA** | UNetã®LoRAéƒ¨åˆ† | ~10M | é«˜ï¼ˆãƒãƒ¼ã‚¸å¯èƒ½ï¼‰ |

Textual Inversion [^15] ã¯ã€æ–°ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’:

$$
v_* = \arg\min_v \mathbb{E}_{x, c, \epsilon, t}[\|\epsilon - \epsilon_\theta(z_t, c(v))\|_2^2]
$$

$c(v)$: ãƒˆãƒ¼ã‚¯ãƒ³ $v$ ã‚’å«ã‚€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã€‚DreamBoothã‚ˆã‚Šè»½é‡ã ãŒã€è¡¨ç¾åŠ›ãŒåŠ£ã‚‹ã€‚

#### 6.4.2 Custom Diffusionã¨ã®æ¯”è¼ƒ

Custom Diffusion [^16] ã¯ã€**Cross-Attentionå±¤ã®K, Vã®ã¿**ã‚’Fine-tuning:

$$
\begin{aligned}
K &= W_{k,0} + \Delta W_k \\
V &= W_{v,0} + \Delta W_v
\end{aligned}
$$

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | DreamBooth | Custom Diffusion | DreamBooth + LoRA |
|:----------|:-----------|:-----------------|:------------------|
| Trainable params | å…¨UNet (1B) | K, V (75M) | LoRA (10M) |
| Training time | 5-10 min | 5 min | 3 min |
| Multi-concept merge | å›°é›£ | å®¹æ˜“ | å®¹æ˜“ |

### 6.5 æ¬¡ä¸–ä»£PEFTæ‰‹æ³• (2024-2026)

#### 6.5.1 DoRA â€” Weight Decomposition LoRA

DoRA [^11] ã¯ã€é‡ã¿ã‚’**magnitude**ã¨**direction**ã«åˆ†è§£:

$$
W = m \frac{V}{\|V\|_c}, \quad V = W_0 + BA
$$

$m$: magnitudeï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ã€trainableï¼‰ã€$V$: direction vectorã€‚

é€šå¸¸ã®LoRAã‚ˆã‚Šæ€§èƒ½å‘ä¸Šï¼ˆ+1-2%ï¼‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯ã»ã¼åŒã˜ã€‚

#### 6.5.2 LoRA+ â€” å­¦ç¿’ç‡åˆ†é›¢

LoRA+ [^12] ã¯ã€$A$ ã¨ $B$ ã®å­¦ç¿’ç‡ã‚’åˆ†é›¢:

$$
\begin{aligned}
A &\leftarrow A - \eta_A \nabla_A \mathcal{L} \\
B &\leftarrow B - \eta_B \nabla_B \mathcal{L}, \quad \eta_B = \lambda \eta_A, \, \lambda \gg 1
\end{aligned}
$$

æ¨å¥¨: $\lambda = 16$ï¼ˆ$B$ã®å­¦ç¿’ç‡ã‚’$A$ã®16å€ï¼‰ã€‚åæŸé€Ÿåº¦ãŒ2å€å‘ä¸Šã€‚

**ç†ç”±**: $B$ ã¯å‡ºåŠ›æ¬¡å…ƒã€$A$ ã¯å…¥åŠ›æ¬¡å…ƒã€‚å‡ºåŠ›å´ã®æ›´æ–°ã‚’é€Ÿãã™ã‚‹ã¨ã€ã‚¿ã‚¹ã‚¯é©å¿œãŒåŠ é€Ÿã€‚

#### 6.5.3 VeRA â€” Very-low-rank Adaptation

VeRA [^17] ã¯ã€**$B, A$ ã‚’å…¨å±¤ã§å…±æœ‰**:

$$
\Delta W_i = d_i B_\text{shared} A_\text{shared} b_i
$$

$d_i, b_i$: å±¤ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆtrainableï¼‰ã€$B_\text{shared}, A_\text{shared}$: å›ºå®šã€‚

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ç‡: LoRAã®**1/10**ã€‚æ€§èƒ½ã¯è‹¥å¹²ä½ä¸‹ï¼ˆ-1-3%ï¼‰ã€‚

### 6.6 PEFTæ‰‹æ³•ã®çµ±ä¸€ç†è«–

å…¨PEFTæ‰‹æ³•ã‚’**éƒ¨åˆ†ç©ºé–“æœ€é©åŒ–**ã¨ã—ã¦çµ±ä¸€çš„ã«æ‰ãˆã‚‹ [^18]:

$$
\theta_\text{ft} = \arg\min_{\theta \in \theta_0 + \mathcal{S}} \mathcal{L}(\theta)
$$

$\mathcal{S}$: è¨±å®¹ã•ã‚Œã‚‹éƒ¨åˆ†ç©ºé–“ã€‚

| æ‰‹æ³• | $\mathcal{S}$ ã®å®šç¾© |
|:-----|:--------------------|
| **LoRA** | $\{BA : B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}\}$ ï¼ˆä½ãƒ©ãƒ³ã‚¯éƒ¨åˆ†ç©ºé–“ï¼‰ |
| **Adapter** | $\{f_\text{adapter}(\cdot)\}$ ï¼ˆéç·šå½¢å¤‰æ›ã®ç©ºé–“ï¼‰ |
| **Prefix Tuning** | $\{P \oplus \theta_0\}$ ï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹è¿½åŠ ã®ç©ºé–“ï¼‰ |
| **Prompt Tuning** | $\{E_\text{prompt} \oplus E_\text{input}\}$ ï¼ˆåŸ‹ã‚è¾¼ã¿è¿½åŠ ã®ç©ºé–“ï¼‰ |

### 6.7 LoRAã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡

Hu et al. [^1] ã®å®Ÿé¨“ã‹ã‚‰ã€LoRAã®æ€§èƒ½ã¯:

$$
\text{Performance} \propto \log(r)
$$

$r$ã‚’2å€ã«ã—ã¦ã‚‚ã€æ€§èƒ½å‘ä¸Šã¯å¾®å¢—ï¼ˆ+0.5-1%ï¼‰ã€‚$r=8$ã§ååˆ†ãªã“ã¨ãŒå¤šã„ã€‚

**ãƒ¡ãƒ¢ãƒª vs æ€§èƒ½ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:

```mermaid
graph LR
    A["r=4<br/>æ€§èƒ½: 95%"] --> B["r=8<br/>æ€§èƒ½: 98%"]
    B --> C["r=16<br/>æ€§èƒ½: 99%"]
    C --> D["r=64<br/>æ€§èƒ½: 99.5%"]
    D --> E["Full FT<br/>æ€§èƒ½: 100%"]

    style A fill:#ffcdd2
    style B fill:#c8e6c9
    style C fill:#fff9c4
    style E fill:#e1bee7
```

### 6.8 æ¨è–¦æ›¸ç± & ãƒªã‚½ãƒ¼ã‚¹

#### æ›¸ç±

| ã‚¿ã‚¤ãƒˆãƒ« | è‘—è€… | å†…å®¹ | URL |
|:---------|:-----|:-----|:----|
| Parameter-Efficient Fine-Tuning (PEFT) | HuggingFace | PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ | [github.com/huggingface/peft](https://github.com/huggingface/peft) |
| Efficient Deep Learning | Torsten Hoefler, Dan Alistarh | åŠ¹ç‡çš„DLè¨“ç·´ã®åŒ…æ‹¬çš„æ•™ç§‘æ›¸ | [MIT Press 2023] |

#### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

| ãƒªã‚½ãƒ¼ã‚¹ | å†…å®¹ |
|:---------|:-----|
| [LoRAè«–æ–‡è§£èª¬ (HuggingFace Blog)](https://huggingface.co/blog/lora) | LoRAã®å®Ÿè£…ã‚¬ã‚¤ãƒ‰ |
| [QLoRAå®Ÿè£… (GitHub)](https://github.com/artidoro/qlora) | QLoRAã®å…¬å¼å®Ÿè£… |
| [DreamBoothå…¬å¼ã‚µã‚¤ãƒˆ](https://dreambooth.github.io/) | ãƒ‡ãƒ¢ + è«–æ–‡ãƒªãƒ³ã‚¯ |

> **Note:** **é€²æ—: 95% å®Œäº†** æœ€æ–°ç ”ç©¶ï¼ˆDoRA/LoRA+/VeRAï¼‰ã€ç†è«–çš„é™ç•Œï¼ˆIntrinsic Dimensionï¼‰ã€QLoRAæ•°å€¤å®‰å®šæ€§ã€DreamBoothæ‹¡å¼µã‚’å­¦ã‚“ã ã€‚æ¬¡ã¯æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ â€” ã¾ã¨ã‚ + FAQ + æ¬¡å›äºˆå‘Šã¸ã€‚

---

### 6.6 æœ¬è¬›ç¾©ã®4ã¤ã®æ ¸å¿ƒ

1. **LoRA = ä½ãƒ©ãƒ³ã‚¯é©å¿œ**: $\Delta W = BA$ã€$r \ll \min(d, k)$ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿10,000å€å‰Šæ¸›ã€æ€§èƒ½â‰ˆFull FT
2. **QLoRA = é‡å­åŒ– + LoRA**: 4-bit NF4 + Double Quant + Paged Optã€‚65Bãƒ¢ãƒ‡ãƒ«ã‚’GPU 1æšã§è¨“ç·´
3. **DreamBooth = Few-shotå€‹äººåŒ–**: Prior Preservation Loss ã§3ç”»åƒã‹ã‚‰ç‰¹å®šè¢«å†™ä½“ã‚’å­¦ç¿’
4. **PEFTçµ±ä¸€ç†è«–**: å…¨æ‰‹æ³•ã¯éƒ¨åˆ†ç©ºé–“æœ€é©åŒ–ã€‚Adapter/Prefix/Prompt/LoRAã‚’çµ±ä¸€çš„ã«ç†è§£

### 6.7 Course I/II/IIIã§ç²å¾—ã—ãŸæ­¦å™¨ã®çµ±åˆ

| æ¦‚å¿µ | åˆå‡º | æœ¬è¬›ç¾©ã§ã®æ´»ç”¨ |
|:-----|:-----|:-------------|
| **SVD (ç‰¹ç•°å€¤åˆ†è§£)** | ç¬¬3å› | LoRAã®ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®ç†è«–çš„åŸºç›¤ |
| **MLE (æœ€å°¤æ¨å®š)** | ç¬¬7å› | Fine-tuningã®ç›®çš„é–¢æ•° $\arg\max \mathbb{E}[\log p_\theta(y|x)]$ |
| **KL divergence** | ç¬¬6å› | äº‹å‰å­¦ç¿’åˆ†å¸ƒâ†’ã‚¿ã‚¹ã‚¯åˆ†å¸ƒã¸ã®é©å¿œ |
| **Adam optimizer** | ç¬¬6å› | LoRA/QLoRAã®è¨“ç·´ |
| **Gradient Descent** | ç¬¬6å› | $B, A$ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° |


## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.8 FAQ â€” ã‚ˆãã‚ã‚‹ç–‘å•ã¨èª¤è§£

<details><summary>**Q1: LoRAã¯å…¨ã‚¿ã‚¹ã‚¯ã§æœ‰åŠ¹ã‹ï¼Ÿ**</summary>

**A**: ã»ã¨ã‚“ã©ã®ã‚¿ã‚¹ã‚¯ã§æœ‰åŠ¹ã ãŒã€ä¾‹å¤–ã‚‚ã‚ã‚‹ã€‚**ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚·ãƒ•ãƒˆãŒæ¥µç«¯**ãªå ´åˆï¼ˆä¾‹: è‹±èªâ†’éãƒ­ãƒ¼ãƒå­—è¨€èªï¼‰ã€Full FTã®æ–¹ãŒè‰¯ã„ã“ã¨ãŒã‚ã‚‹ã€‚ä¸€èˆ¬çš„ã«ã¯ã€ã‚¿ã‚¹ã‚¯ãŒäº‹å‰å­¦ç¿’ã«è¿‘ã„ã»ã©LoRAãŒæœ‰åŠ¹ã€‚

</details>

<details><summary>**Q2: $r$ ã¯ã©ã†é¸ã¶ã¹ãã‹ï¼Ÿ**</summary>

**A**: çµŒé¨“å‰‡:
- å°è¦æ¨¡ã‚¿ã‚¹ã‚¯ï¼ˆåˆ†é¡ãªã©ï¼‰: $r=4-8$
- ä¸­è¦æ¨¡ã‚¿ã‚¹ã‚¯ï¼ˆè¦ç´„ã€ç¿»è¨³ï¼‰: $r=8-16$
- å¤§è¦æ¨¡ã‚¿ã‚¹ã‚¯ï¼ˆå¯¾è©±ã€è¤‡é›‘æ¨è«–ï¼‰: $r=16-64$

å®Ÿé¨“çš„ã«è¤‡æ•°ã® $r$ ã‚’è©¦ã—ã€æ€§èƒ½ vs ãƒ¡ãƒ¢ãƒªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã§é¸ã¶ã€‚

</details>

<details><summary>**Q3: QLoRAã®4-bité‡å­åŒ–ã¯æ¨è«–ã§ã‚‚ä½¿ãˆã‚‹ï¼Ÿ**</summary>

**A**: ä½¿ãˆã‚‹ã€‚ãŸã ã—ã€æ¨è«–æ™‚ã¯ $W_0$ ã‚’4-bitã§ä¿æŒã—ã€on-the-flyã§FP16ã«å±•é–‹ã€‚ãƒ¡ãƒ¢ãƒªã¯å‰Šæ¸›ã•ã‚Œã‚‹ãŒã€å±•é–‹ã‚³ã‚¹ãƒˆã§æ¨è«–é€Ÿåº¦ãŒ5-10%ä½ä¸‹ã™ã‚‹ã€‚

</details>

<details><summary>**Q4: DreamBoothã¨LoRAã‚’çµ„ã¿åˆã‚ã›ã‚‹åˆ©ç‚¹ã¯ï¼Ÿ**</summary>

**A**: 2ã¤:
1. **ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: Full DreamBoothï¼ˆå…¨UNetæ›´æ–°ï¼‰ã¯æ•°GBãƒ¡ãƒ¢ãƒªã€‚LoRAãªã‚‰æ•°ç™¾MBã€‚
2. **Multi-concept merge**: è¤‡æ•°è¢«å†™ä½“ã® $(B, A)$ ãƒšã‚¢ã‚’ä¿æŒã—ã€æ¨è«–æ™‚ã«åˆæˆå¯èƒ½ï¼ˆä¾‹: ã€Œã‚ãªãŸã®çŠ¬ã€+ ã€Œã‚ãªãŸã®çŒ«ã€ã‚’åŒã˜ç”»åƒã«ï¼‰ã€‚

</details>

<details><summary>**Q5: Adapter vs LoRAã€ã©ã¡ã‚‰ã‚’é¸ã¶ã¹ãã‹ï¼Ÿ**</summary>

**A**:
- **LoRA**: æ¨è«–é€Ÿåº¦é‡è¦–ï¼ˆãƒãƒ¼ã‚¸å¯èƒ½ï¼‰ã€Multi-taskï¼ˆè¤‡æ•°Adapteråˆ‡ã‚Šæ›¿ãˆï¼‰
- **Adapter**: éç·šå½¢å¤‰æ›ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ï¼ˆLoRAã¯ç·šå½¢ã®ã¿ï¼‰

å®Ÿç”¨ä¸Šã€LoRAã®æ–¹ãŒåºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ï¼ˆHuggingFace PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã€‚

</details>

### 6.9 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | é”æˆç›®æ¨™ |
|:---|:-----|:-----|:---------|
| **Day 1** | Zone 0-2ï¼ˆæ¦‚è¦³ãƒ»ç›´æ„Ÿï¼‰ | 30åˆ† | Fine-tuningã®å¿…è¦æ€§ã‚’ç†è§£ |
| **Day 2** | Zone 3å‰åŠï¼ˆLoRAç†è«–ï¼‰ | 60åˆ† | $\Delta W = BA$ ã‚’å°å‡ºã§ãã‚‹ |
| **Day 3** | Zone 3å¾ŒåŠï¼ˆQLoRA, DreamBoothï¼‰ | 60åˆ† | NF4é‡å­åŒ–ã®åŸç†ã‚’èª¬æ˜ã§ãã‚‹ |
| **Day 4** | Zone 4ï¼ˆğŸ¦€Rustå®Ÿè£…ï¼‰ | 60åˆ† | LoRAå±¤ã‚’å®Ÿè£…ã§ãã‚‹ |
| **Day 5** | Zone 4ï¼ˆğŸ¦€Rustæ¨è«–ï¼‰ | 45åˆ† | LoRAãƒãƒ¼ã‚¸ã¨åˆ‡ã‚Šæ›¿ãˆã‚’å®Ÿè£…ã§ãã‚‹ |
| **Day 6** | Zone 5ï¼ˆSmolVLM2å®Ÿé¨“ï¼‰ | 45åˆ† | å®Ÿãƒ‡ãƒ¼ã‚¿ã§LoRA Fine-tuning |
| **Day 7** | Zone 6-7ï¼ˆç™ºå±•ãƒ»å¾©ç¿’ï¼‰ | 40åˆ† | DoRA/LoRA+ã‚’ç†è§£ã€å…¨ä½“å¾©ç¿’ |

**åˆè¨ˆ**: ç´„5.5æ™‚é–“

### 6.10 åˆ°é”åº¦ãƒã‚§ãƒƒã‚¯

æœ¬è¬›ç¾©ä¿®äº†æ™‚ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹:

- [ ] LoRAã®æ•°å¼ $h = W_0 x + \frac{\alpha}{r} BA x$ ã‚’å®Œå…¨ã«å°å‡ºã§ãã‚‹
- [ ] QLoRAã®3ã¤ã®é©æ–°ã‚’èª¬æ˜ã§ãã‚‹
- [ ] DreamBoothã®Prior Preservation Lossã‚’å¼ã§æ›¸ã‘ã‚‹
- [ ] Rust ã§LoRAå±¤ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Rust ã§LoRAãƒãƒ¼ã‚¸ãƒ»Multi-taskåˆ‡ã‚Šæ›¿ãˆã‚’å®Ÿè£…ã§ãã‚‹
- [ ] SmolVLM2ã‚’LoRAã§Fine-tuningã§ãã‚‹
- [ ] Adapter/Prefix/Prompt Tuningã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] PEFTæ‰‹æ³•ã‚’éƒ¨åˆ†ç©ºé–“æœ€é©åŒ–ã¨ã—ã¦çµ±ä¸€çš„ã«ç†è§£ã§ãã‚‹

### 6.6 æ¬¡å›äºˆå‘Š â€” ç¬¬24å›: çµ±è¨ˆå­¦

ç¬¬23å›ã§Fine-tuningã®å®Ÿè£…ã‚’å®Œäº†ã—ãŸã€‚ã ãŒã€Œãƒ¢ãƒ‡ãƒ«ãŒæ”¹å–„ã—ãŸã€ã‚’ã©ã†**å®šé‡è©•ä¾¡**ã™ã‚‹ã‹ï¼Ÿ

ç¬¬24å›ã§ã¯ã€**çµ±è¨ˆå­¦**ã‚’å¾¹åº•çš„ã«å­¦ã¶:

- **è¨˜è¿°çµ±è¨ˆ**: å¹³å‡ãƒ»åˆ†æ•£ãƒ»ç›¸é–¢ â€” ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„
- **æ¨æ¸¬çµ±è¨ˆ**: ä¿¡é ¼åŒºé–“ãƒ»ä»®èª¬æ¤œå®š â€” æœ‰æ„å·®ã®åˆ¤å®š
- **ãƒ™ã‚¤ã‚ºçµ±è¨ˆ**: äº‹å¾Œåˆ†å¸ƒãƒ»MCMC â€” ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–
- **å®Ÿé¨“è¨ˆç”»æ³•**: A/Bãƒ†ã‚¹ãƒˆãƒ»å¤šé‡æ¯”è¼ƒè£œæ­£ â€” æ­£ã—ã„å®Ÿé¨“è¨­è¨ˆ
- **å› æœæ¨è«–å…¥é–€**: RCTãƒ»å‚¾å‘ã‚¹ã‚³ã‚¢ â€” å› æœé–¢ä¿‚ã®æ¨å®š

**Course III ã®æµã‚Œ**:

| ç¬¬17-22å› | ç¬¬23å› | **ç¬¬24å›** | ç¬¬25-32å› |
|:----------|:-------|:----------|:----------|
| ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ | Fine-tuning | **è©•ä¾¡ã®æ•°å­¦** | æ¨è«–æœ€é©åŒ–ãƒ»MLOps |

ç¬¬24å›ã§çµ±è¨ˆçš„è©•ä¾¡ã®åŸºç›¤ã‚’å›ºã‚ã€ç¬¬25å›ä»¥é™ã§Productionå±•é–‹ã¸é€²ã‚€ã€‚

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã¯æœ¬å½“ã«å¿…è¦ã‹ï¼Ÿ ãã‚Œã¨ã‚‚ã€æˆ‘ã€…ã¯ã€Œæœ€é©åŒ–ã™ã¹ãéƒ¨åˆ†ç©ºé–“ã€ã‚’è¦‹èª¤ã£ã¦ã„ãŸã®ã‹ï¼Ÿ**

å¾“æ¥ã®æ©Ÿæ¢°å­¦ç¿’ã§ã¯ã€ã€Œãƒ¢ãƒ‡ãƒ«ãŒå¤§ãã„ã»ã©è‰¯ã„ã€ã€Œå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨“ç·´ã™ã¹ãã€ãŒå¸¸è­˜ã ã£ãŸã€‚

ã ãŒã€LoRAã¯**å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®0.01%**ã§ã€Full Fine-tuningã¨åŒç­‰æ€§èƒ½ã‚’é”æˆã—ãŸã€‚QLoRAã¯65Bãƒ¢ãƒ‡ãƒ«ã‚’GPU 1æšã§è¨“ç·´å¯èƒ½ã«ã—ãŸã€‚ã“ã‚Œã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã‹ï¼Ÿ

### å•ã„ã®æ·±å±¤

1. **éå‰°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã®å†è§£é‡ˆ**: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¯å†—é•·ã€‚å®ŸåŠ¹çš„ãªè‡ªç”±åº¦ï¼ˆintrinsic dimensionï¼‰ã¯æ¥µã‚ã¦ä½ã„ã€‚ãªã‚‰ã°ã€ã€Œå¤§è¦æ¨¡åŒ–ã€ã§ã¯ãªãã€ŒåŠ¹ç‡çš„éƒ¨åˆ†ç©ºé–“ã®ç™ºè¦‹ã€ã“ããŒæœ¬è³ªã§ã¯ï¼Ÿ

2. **è»¢ç§»å­¦ç¿’ã®æœ¬è³ª**: äº‹å‰å­¦ç¿’ã¯ã€Œæ±ç”¨è¡¨ç¾ã®ç²å¾—ã€ã€Fine-tuningã¯ã€Œã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã®å¾®èª¿æ•´ã€ã€‚LoRAã¯ã€ã“ã®å¾®èª¿æ•´ãŒ**ä½æ¬¡å…ƒã§å®Œçµ**ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã“ã‚Œã¯ã€äººé–“ã®å­¦ç¿’ï¼ˆå°‚é–€çŸ¥è­˜ã¯æ—¢å­˜çŸ¥è­˜ã¸ã®"ã¡ã‚‡ã£ã¨ã—ãŸè¿½åŠ "ï¼‰ã¨åŒã˜æ§‹é€ ã§ã¯ï¼Ÿ

3. **è¨ˆç®—è³‡æºã®æ°‘ä¸»åŒ–**: Full FTã¯å¯Œè±ªã®ç‰¹æ¨©ã ã£ãŸã€‚PEFTã¯ã€èª°ã§ã‚‚å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’é©å¿œå¯èƒ½ã«ã—ãŸã€‚ã“ã‚Œã¯ã€ŒAIç ”ç©¶ã®æ°‘ä¸»åŒ–ã€ã®å§‹ã¾ã‚Šã‹ï¼Ÿ

### æ¬¡ã®å•ã„

- LoRAã® $r$ ã¯æœ¬å½“ã«æœ€é©ã‹ï¼Ÿ ã‚ˆã‚Šè‰¯ã„éƒ¨åˆ†ç©ºé–“ã®é¸ã³æ–¹ã¯ï¼Ÿ
- QLoRAã®4-bitã¯ã¾ã ç²—ã„ã€‚1-bitï¼ˆQuantization Aware Trainingï¼‰ã¯å¯èƒ½ã‹ï¼Ÿ
- è¤‡æ•°ã‚¿ã‚¹ã‚¯ã®LoRAã‚’**è‡ªå‹•åˆæˆ**ã§ãã‚‹ã‹ï¼Ÿï¼ˆMulti-task LoRAã®å‹•çš„æœ€é©åŒ–ï¼‰

**æ­´å²ã¯ç¹°ã‚Šè¿”ã™**: SVDï¼ˆç¬¬3å›ï¼‰ã¯ã€Œå…¨æƒ…å ±ã‚’ä¿æŒã—ã¤ã¤æ¬¡å…ƒå‰Šæ¸›ã€ã‚’å¯èƒ½ã«ã—ãŸã€‚LoRAã¯ãã®å¿œç”¨ã«éããªã„ã€‚ã ãŒã€ã“ã®å˜ç´”ãªå¿œç”¨ãŒã€AIã®æ°‘ä¸»åŒ–ã‚’åŠ é€Ÿã—ã¦ã„ã‚‹ã€‚

### 7. PEFTæœ€æ–°å‹•å‘ï¼ˆ2024-2026ï¼‰

#### 7.1 Do RA: Weight-Decomposed Low-Rank Adaptation

DoRA [^20] (Liu et al., 2024) ã¯ã€LoRAã®é€²åŒ–ç‰ˆã€‚é‡ã¿æ›´æ–°ã‚’**å¤§ãã• (magnitude)** ã¨**æ–¹å‘ (direction)** ã«åˆ†è§£ã™ã‚‹ã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**: Full Fine-tuningã¯ã€é‡ã¿ã®å¤§ãã•ã¨æ–¹å‘ã®ä¸¡æ–¹ã‚’æ›´æ–°ã™ã‚‹ã€‚LoRAã¯æ–¹å‘ã®ã¿æ›´æ–°ã—ã€å¤§ãã•ã¯å›ºå®šã€‚DoRAã¯ä¸¡æ–¹ã‚’æ›´æ–°ã™ã‚‹ã€‚

**æ•°å¼**:

é‡ã¿è¡Œåˆ— $\mathbf{W}$ ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ†è§£:

$$
\mathbf{W} = m \cdot \frac{\mathbf{V}}{\|\mathbf{V}\|_c}
$$

ã“ã“ã§:
- $m \in \mathbb{R}^d$: åˆ—ã”ã¨ã®å¤§ãã• (magnitude vector)
- $\mathbf{V} \in \mathbb{R}^{d \times k}$: æ–¹å‘è¡Œåˆ—
- $\|\mathbf{V}\|_c$: åˆ—ã”ã¨ã®â„“2ãƒãƒ«ãƒ 

**DoRAæ›´æ–°**:

$$
\mathbf{W}' = m \cdot \frac{\mathbf{W}_0 + \mathbf{B}\mathbf{A}}{\|\mathbf{W}_0 + \mathbf{B}\mathbf{A}\|_c}
$$

ã“ã“ã§ $\mathbf{B}\mathbf{A}$ ã¯LoRAã¨åŒã˜ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã€‚

**LoRAã¨ã®é•ã„**:

| æ‰‹æ³• | å¤§ãã• $m$ | æ–¹å‘ $\mathbf{V}$ | æ›´æ–°å¯¾è±¡ |
|:-----|:----------|:--------------|:--------|
| **LoRA** | å›ºå®š | æ›´æ–°ï¼ˆ$\mathbf{W}_0 + \mathbf{B}\mathbf{A}$ï¼‰ | æ–¹å‘ã®ã¿ |
| **DoRA** | æ›´æ–° | æ›´æ–°ï¼ˆæ­£è¦åŒ–å¾Œï¼‰ | å¤§ãã•+æ–¹å‘ |

**ç‰¹ç•°å€¤ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®æ”¹å–„**:

DoRAã¯ã€é‡ã¿æ›´æ–°è¡Œåˆ—ã®**ç‰¹ç•°å€¤ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**ã‚’å¢—åŠ ã•ã›ã‚‹:

$$
H(\boldsymbol{\sigma}) = -\sum_{i=1}^r \frac{\sigma_i}{\sum_j \sigma_j} \log \frac{\sigma_i}{\sum_j \sigma_j}
$$

ã“ã“ã§ $\boldsymbol{\sigma} = (\sigma_1, \ldots, \sigma_r)$ ã¯ç‰¹ç•°å€¤ã€‚

ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒé«˜ã„ = æ›´æ–°ãŒ**å‡ä¸€ã«åˆ†æ•£** = Full FTã«è¿‘ã„ã€‚

**å®Ÿé¨“çµæœ** (Liu et al., 2024 [^20]):

| ã‚¿ã‚¹ã‚¯ | LoRA | DoRA | Full FT | DoRAæ”¹å–„ç‡ |
|:-------|:-----|:-----|:--------|:----------|
| CommonsenseQA | 76.2% | 78.9% | 79.3% | +2.7% |
| MMLU | 52.3% | 54.8% | 55.1% | +2.5% |
| GSM8K | 41.2% | 45.7% | 46.3% | +4.5% |

DoRAã¯ã€LoRAã‚’å…¨ã‚¿ã‚¹ã‚¯ã§ä¸Šå›ã‚Šã€Full FTã«æœ€ã‚‚è¿‘ã„æ€§èƒ½ã‚’é”æˆã€‚

#### 7.2 QLoRA: 4-bité‡å­åŒ–ã¨ã®çµ±åˆ

QLoRA [^21] (Dettmers et al., 2023) ã¯ã€**4-bité‡å­åŒ–**ã¨LoRAã‚’çµ„ã¿åˆã‚ã›ã‚‹ã€‚

**3ã¤ã®é©æ–°**:

1. **4-bit NormalFloat (NF4)**:

   é€šå¸¸ã®å‡ä¸€é‡å­åŒ–ã§ã¯ãªãã€æ­£è¦åˆ†å¸ƒã«æœ€é©åŒ–ã—ãŸé‡å­åŒ–:

   $$
   \text{NF4} = \{-1, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0, 0.0796, 0.1609, 0.2461, 0.3379, 0.4407, 0.5626, 0.7230, 1.0\}
   $$

   æ­£è¦åˆ†å¸ƒ $\mathcal{N}(0,1)$ ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒ«ã—ãŸ16å€‹ã®å€¤ã€‚

2. **Double Quantization**:

   é‡å­åŒ–å®šæ•°è‡ªä½“ã‚‚é‡å­åŒ–:

   $$
   \mathbf{W}_{\text{quantized}} = \text{Q}_1(\mathbf{W} / c_1), \quad c_1 = \text{Q}_2(c_0)
   $$

   ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: 0.37 bits/parameter â†’ å¹³å‡8GBã‹ã‚‰0.3GBå‰Šæ¸›ï¼ˆ65Bãƒ¢ãƒ‡ãƒ«ï¼‰ã€‚

3. **Paged Optimizers**:

   GPU RAMã‚¹ãƒ‘ã‚¤ã‚¯æ™‚ã«CPU RAMã¸ãƒšãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆOSã®virtual memoryã¨åŒã˜ï¼‰ã€‚

**ãƒ¡ãƒ¢ãƒªæ¯”è¼ƒ**ï¼ˆLLaMA-65Bï¼‰:

| æ‰‹æ³• | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | GPUè¦ä»¶ | æ€§èƒ½åŠ£åŒ– |
|:-----|:-----------|:--------|:--------|
| Full FT (16-bit) | 780 GB | 10x A100 80GB | - |
| LoRA (16-bit) | 80 GB | 1x A100 80GB | 0.2% |
| QLoRA (4-bit) | **48 GB** | **1x A100 48GB** | 0.3% |

QLoRAã¯ã€Full FTã®**1/16ã®ãƒ¡ãƒ¢ãƒª**ã§ã€æ€§èƒ½åŠ£åŒ–0.3%ã€‚

**Rustå®Ÿè£…ä¾‹**:

```rust
// NF4é‡å­åŒ–é–¢æ•°
const NF4_VALUES: [f32; 16] = [
    -1.0, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0.0,
     0.0796, 0.1609, 0.2461, 0.3379, 0.4407, 0.5626, 0.7230, 1.0,
];

fn quantize_nf4(w: &[f32]) -> (Vec<u8>, f32) {
    let absmax = w.iter().map(|x| x.abs()).fold(0.0f32, f32::max);
    let quant = w
        .iter()
        .map(|&val| {
            let v = val / absmax;
            NF4_VALUES
                .iter()
                .enumerate()
                .min_by_key(|(_, &l)| ((v - l).abs() * 1e6) as i32)
                .map(|(i, _)| i as u8)
                .unwrap()
        })
        .collect();
    (quant, absmax)
}

// Short-form dequantize: map indices back to FP32
fn dequantize_nf4(quant_idx: &[u8], absmax: f32) -> Vec<f32> {
    quant_idx.iter().map(|&idx| NF4_VALUES[idx as usize] * absmax).collect()
}
```

#### 7.3 Pre-Diag & SORA: é‡ã¿æ¡ä»¶ä»˜ã‘ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

æœ€æ–°ã®PEFTç ”ç©¶ [^22] ã¯ã€LoRAæ›´æ–°å‰ã«é‡ã¿ã‚’**æ¡ä»¶ä»˜ã‘ (conditioning)** ã™ã‚‹ã€‚

**Pre-Diag** (2024):

å¯¾è§’è¡Œåˆ— $\mathbf{D}$ ã§äº‹å‰å­¦ç¿’é‡ã¿ã‚’ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:

$$
\mathbf{W}' = \mathbf{D} \mathbf{W}_0 + \mathbf{B}\mathbf{A}
$$

ã“ã“ã§ $\mathbf{D} = \text{diag}(d_1, \ldots, d_d)$ ã¯å­¦ç¿’å¯èƒ½ã€‚

**SORA** (Scaling and Orthogonal Rotation Adaptation):

ç›´äº¤å›è»¢ $\mathbf{R}$ ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° $s$:

$$
\mathbf{W}' = s \cdot \mathbf{R} \mathbf{W}_0 + \mathbf{B}\mathbf{A}
$$

ã“ã“ã§ $\mathbf{R}^\top \mathbf{R} = \mathbf{I}$ï¼ˆç›´äº¤åˆ¶ç´„ï¼‰ã€‚

**åŠ¹æœ**: äº‹å‰å­¦ç¿’é‡ã¿ã®**æ–¹å‘**ã‚’ä¿ã¡ã¤ã¤ã€å¤§ãã•ã‚’èª¿æ•´ â†’ Full FTã«è¿‘ã„æŸ”è»Ÿæ€§ã€‚

#### 7.4 LoRAFusion: è¤‡æ•°LoRAã®åŠ¹ç‡çš„çµ±åˆ

LoRAFusion [^23] (2024) ã¯ã€è¤‡æ•°ã‚¿ã‚¹ã‚¯ç”¨ã®LoRAã‚’åŠ¹ç‡çš„ã«çµ±åˆã™ã‚‹ã€‚

**å•é¡Œ**: ã‚¿ã‚¹ã‚¯Aã®LoRA ($\mathbf{B}_A\mathbf{A}_A$) ã¨ã‚¿ã‚¹ã‚¯Bã®LoRA ($\mathbf{B}_B\mathbf{A}_B$) ã‚’åŒæ™‚ã«ä½¿ã„ãŸã„ã€‚

**ãƒŠã‚¤ãƒ¼ãƒ–ãªæ–¹æ³•**:

$$
\mathbf{W}' = \mathbf{W}_0 + \mathbf{B}_A\mathbf{A}_A + \mathbf{B}_B\mathbf{A}_B
$$

å•é¡Œ: æ¨è«–æ™‚ã«ä¸¡æ–¹ã‚’ä¿æŒ â†’ ãƒ¡ãƒ¢ãƒª2å€ã€‚

**LoRAFusion**: ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã§èåˆ:

$$
\mathbf{B}_A\mathbf{A}_A + \mathbf{B}_B\mathbf{A}_B \approx \mathbf{B}_{\text{fused}} \mathbf{A}_{\text{fused}}
$$

SVDã§ $(r_A + r_B)$-rankè¡Œåˆ—ã‚’$r_{\text{fused}}$-rankã«åœ§ç¸®ï¼ˆ$r_{\text{fused}} < r_A + r_B$ï¼‰ã€‚

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: $(r_A + r_B) \times (d + k) \to r_{\text{fused}} \times (d + k)$

å…¸å‹ä¾‹: $r_A = r_B = 8, r_{\text{fused}} = 12$ â†’ ãƒ¡ãƒ¢ãƒª25%å‰Šæ¸›ã€æ€§èƒ½åŠ£åŒ–1%æœªæº€ã€‚

#### 7.5 PEFTæ‰‹æ³•ã®çµ±åˆæ¯”è¼ƒï¼ˆ2024-2026ï¼‰

| æ‰‹æ³• | è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ¡ãƒ¢ãƒªå‰Šæ¸› | æ€§èƒ½ | ç”¨é€” |
|:-----|:-------------|:----------|:-----|:-----|
| **Full FT** | 100% | - | 100% | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ |
| **LoRA** | 0.1-1% | 90% | 98-99% | æ±ç”¨ |
| **DoRA** | 0.1-1% + magnitude | 88% | 99-99.5% | é«˜æ€§èƒ½è¦æ±‚ |
| **QLoRA** | 0.1-1% | 93% | 97-98% | ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ |
| **Pre-Diag** | 0.2-1.5% | 85% | 99% | ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é‡è¦– |
| **LoRAFusion** | 0.15-1.2% | 92% | 98-99% | ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ |

**2024-2026ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**:

```rust
// Recommended PEFT configuration (2024)
struct PeftConfig {
    method: &'static str,
    rank: usize,
    alpha: f32,
    quantization: &'static str,
    target_modules: &'static [&'static str],
    use_gradient_checkpointing: bool,
}

const PEFT_CONFIG: PeftConfig = PeftConfig {
    method: "DoRA",              // Best performance
    rank: 16,                    // Sweet spot for most tasks
    alpha: 32.0,                 // Î± = 2r is standard
    quantization: "NF4",         // If memory-constrained
    target_modules: &["q_proj", "v_proj", "k_proj", "o_proj"], // Attention only
    use_gradient_checkpointing: true, // 40% memory reduction
};
```

**çµè«–**: DoRAãŒ2024å¹´ã®SOTAã€QLoRAã¯ãƒ¡ãƒ¢ãƒªåˆ¶ç´„æ™‚ã®æœ€é©è§£ã€LoRAFusionã¯ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã®æ¨™æº–ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼æœ€æ–°PEFTæ‰‹æ³•ï¼ˆDoRA, QLoRA, LoRAFusionï¼‰ã¾ã§ç¶²ç¾…ã—ãŸã€‚

> **Progress: 95%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. DoRAï¼ˆWeight-Decomposed LoRAï¼‰ãŒLoRAã¨ç•°ãªã‚‹ã®ã¯ä½•ã‚’åˆ†è§£ã™ã‚‹ã‹ã‚‰ã‹ï¼Ÿãã®åˆ©ç‚¹ã¯ï¼Ÿ
> 2. LoRA+ã§Aã¨Bã«ç•°ãªã‚‹å­¦ç¿’ç‡ã‚’è¨­å®šã™ã‚‹ç†è«–çš„æ ¹æ‹ ã¯ä½•ã‹ï¼Ÿ

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2022). **LoRA: Low-Rank Adaptation of Large Language Models**. *ICLR 2022*. <https://arxiv.org/abs/2106.09685>

[^2]: Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). **QLoRA: Efficient Finetuning of Quantized LLMs**. *NeurIPS 2023*. <https://arxiv.org/abs/2305.14314>

[^3]: Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hadsell, R. (2017). **Overcoming catastrophic forgetting in neural networks**. *PNAS*, 114(13), 3521-3526. <https://www.pnas.org/doi/10.1073/pnas.1611835114>

[^4]: Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2023). **DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation**. *CVPR 2023*. <https://arxiv.org/abs/2208.12242>

[^5]: Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., & Gelly, S. (2019). **Parameter-Efficient Transfer Learning for NLP**. *ICML 2019*. <https://arxiv.org/abs/1902.00751>

[^6]: Li, X. L., & Liang, P. (2021). **Prefix-Tuning: Optimizing Continuous Prompts for Generation**. *ACL 2021*. <https://arxiv.org/abs/2101.00190>

[^7]: Liu, X., Ji, K., Fu, Y., Tam, W. L., Du, Z., Yang, Z., & Tang, J. (2022). **P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks**. *ACL 2022*. <https://arxiv.org/abs/2110.07602>

[^8]: Lester, B., Al-Rfou, R., & Constant, N. (2021). **The Power of Scale for Parameter-Efficient Prompt Tuning**. *EMNLP 2021*. <https://arxiv.org/abs/2104.08691>

[^9]: Candle: Explicit Parameterization for Neural Networks in Rust. <https://github.com/LuxDL/Candle>

[^10]: Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., ... & Lowe, R. (2022). **Training language models to follow instructions with human feedback**. *NeurIPS 2022*. <https://arxiv.org/abs/2203.02155>

[^11]: Liu, S., Zhang, Y., Qiu, L., Xiao, C., Zhao, H., Jia, Y., ... & Zhang, Y. (2024). **DoRA: Weight-Decomposed Low-Rank Adaptation**. *arXiv preprint*. <https://arxiv.org/abs/2402.09353>

[^12]: Hayou, S., Ghosh, N., & Yu, B. (2024). **LoRA+: Efficient Low Rank Adaptation of Large Models**. *ICML 2024 Workshop*. <https://arxiv.org/abs/2402.12354>

[^13]: Aghajanyan, A., Gupta, S., & Zettlemoyer, L. (2020). **Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning**. *ACL 2021*. <https://arxiv.org/abs/2012.13255>

[^14]: Lloyd, S. (1982). **Least squares quantization in PCM**. *IEEE Transactions on Information Theory*, 28(2), 129-137.

[^15]: Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., & Cohen-Or, D. (2022). **An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion**. *ICLR 2023*. <https://arxiv.org/abs/2208.01618>

[^16]: Kumari, N., Zhang, B., Zhang, R., Shechtman, E., & Zhu, J. Y. (2023). **Multi-Concept Customization of Text-to-Image Diffusion**. *CVPR 2023*. <https://arxiv.org/abs/2212.04488>

[^17]: Kopiczko, D. J., Blankevoort, T., & Asano, Y. M. (2024). **VeRA: Vector-based Random Matrix Adaptation**. *ICLR 2024*. <https://arxiv.org/abs/2310.11454>

[^18]: He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., & Neubig, G. (2022). **Towards a Unified View of Parameter-Efficient Transfer Learning**. *ICLR 2022*. <https://arxiv.org/abs/2110.04366>

[^19]: Han, Z., et al. (2024). **Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey**. *arXiv preprint*. <https://arxiv.org/abs/2403.14608>

[^20]: Liu, S., et al. (2024). **DoRA: Weight-Decomposed Low-Rank Adaptation**. *arXiv preprint*. <https://arxiv.org/abs/2402.09353>

[^21]: Dettmers, T., et al. (2023). **QLoRA: Efficient Finetuning of Quantized LLMs**. *NeurIPS 2023*. <https://arxiv.org/abs/2305.14314>

[^22]: Wei, H., et al. (2024). **Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT**. *arXiv preprint*. <https://arxiv.org/abs/2511.00051>

[^23]: Zhu, Z., Su, Q., Ding, Y., Song, K., et al. (2025). **LoRAFusion: Efficient LoRA Fine-Tuning for LLMs**. *EuroSys 2026*. <https://arxiv.org/abs/2510.00206>

[^29]: Gururangan, S., MarasoviÄ‡, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., & Smith, N. A. (2020). **Don't Stop Pretraining: Adapt Language Models to Domains and Tasks**. *ACL 2020*. <https://arxiv.org/abs/2004.10964>

[^30]: Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009). **Curriculum Learning**. *ICML 2009*. <https://dl.acm.org/doi/10.1145/1553374.1553380>

[^31]: Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022). **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models**. *NeurIPS 2022*. <https://arxiv.org/abs/2201.11903>

[^32]: Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., & Levy, O. (2023). **LIMA: Less Is More for Alignment**. *NeurIPS 2023*. <https://arxiv.org/abs/2305.11206>

[^33]: Bradley, R. A., & Terry, M. E. (1952). **Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons**. *Biometrika*, 39(3/4), 324â€“345.

[^34]: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). **Proximal Policy Optimization Algorithms**. *arXiv preprint*. <https://arxiv.org/abs/1707.06347>

[^35]: Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., & Finn, C. (2023). **Direct Preference Optimization: Your Language Model is Secretly a Reward Model**. *NeurIPS 2023*. <https://arxiv.org/abs/2305.18290>

### æ•™ç§‘æ›¸

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [deeplearningbook.org](http://www.deeplearningbook.org/)
- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press. [probml.github.io](https://probml.github.io/pml-book/book1.html)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

**Co-Authored-By**: Claude Opus 4.6 <noreply@anthropic.com>
