---
title: "ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ²"
type: "tech"
topics: ["æ©Ÿæ¢°å­¦ç¿’", "ç¢ºç‡è«–", "çµ±è¨ˆå­¦", "æ•°å­¦", "Python"]
published: true
slug: "ml-lecture-04-part2"
difficulty: "intermediate"
time_estimate: "90 minutes"
languages: ["Python"]
keywords: ["ç¢ºç‡åˆ†å¸ƒå®Ÿè£…", "MLEå®Ÿè£…", "ãƒ™ã‚¤ã‚ºæ¨è«–", "SciPy", "çµ±è¨ˆçš„æ¨å®š"]
---

# ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ã€å¾Œç·¨ã€‘

> ç†è«–ç·¨ã¯ [ã€å‰ç·¨ã€‘ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦](/articles/ml-lecture-04-part1) ã‚’ã”è¦§ãã ã•ã„ã€‚

## Learning Objectives

ã“ã®å®Ÿè£…ç·¨ã‚’ä¿®äº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™:

- [ ] NumPy/SciPyã§ä¸»è¦ç¢ºç‡åˆ†å¸ƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãã‚‹
- [ ] MLEã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã—ã€æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã§ãã‚‹
- [ ] ãƒ™ã‚¤ã‚ºæ¨è«–ã®ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’è¨ˆç®—ã§ãã‚‹
- [ ] è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å°¤åº¦ã‚’å®Ÿè£…ãƒ»è©•ä¾¡ã§ãã‚‹
- [ ] Production-readyãªçµ±è¨ˆçš„æ¨å®šã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‘ã‚‹

---

## ğŸ’» Z5. è©¦ç·´ï¼ˆ75åˆ†ï¼‰â€” 5ãƒˆãƒ”ãƒƒã‚¯å®Œå…¨å®Ÿè£…+æ¤œè¨¼

### 5.1 ç¢ºç‡åˆ†å¸ƒã®å®Œå…¨å®Ÿè£… â€” PDFãƒ»CDFãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»MLE

ç¢ºç‡åˆ†å¸ƒã‚’ã€Œä½¿ãˆã‚‹ã€ã¨ã¯ã©ã†ã„ã†ã“ã¨ã‹ã€‚PDF ã‚’è©•ä¾¡ã—ã€ç´¯ç©ç¢ºç‡ã‚’è¨ˆç®—ã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã—ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã™ã‚‹â€”â€”ã“ã®4ã¤ãŒã‚»ãƒƒãƒˆã ã€‚

**Gaussian: æœ€ã‚‚é‡è¦ãªåˆ†å¸ƒ**

$X \sim \mathcal{N}(\mu, \sigma^2)$ ã®ã¨ã:

$$
f(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\!\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

- shape: `x` ã¯ `(N,)` ã‚¹ã‚«ãƒ©ãƒ¼åˆ—ã€`mu` ã¨ `sigma` ã¯ã‚¹ã‚«ãƒ©ãƒ¼
- `sigma` ã®ç¬¦å·: åˆ†æ¯ã¯ `sigma`ï¼ˆæ¨™æº–åå·®ï¼‰ã€`sigma^2` ã¯åˆ†æ•£ã€‚æ··åŒã—ã‚„ã™ã„
- æ•°å€¤å®‰å®šåŒ–: å¤§ããª `(x-mu)^2/sigma^2` ã§ `exp(-...)` ãŒã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼ â†’ å¯¾æ•°ç©ºé–“ã§è¨ˆç®—ã™ã‚‹

```python
import numpy as np
from scipy import stats

rng = np.random.default_rng(42)

# MLE for Gaussian: closed-form
data = rng.normal(loc=2.0, scale=1.5, size=500)
mu_mle = data.mean()            # E[X] = mu
sigma_mle = data.std(ddof=0)    # sqrt(E[(X-mu)^2]) = sigma (biased MLE)
# ddof=1 ã¯ä¸åæ¨å®šé‡ã ãŒ MLE ã¯ ddof=0

# verify: log-likelihood at MLE vs perturbed
def log_lik_normal(x, mu, sigma):
    return np.sum(stats.norm.logpdf(x, loc=mu, scale=sigma))

ll_mle = log_lik_normal(data, mu_mle, sigma_mle)
ll_perturbed = log_lik_normal(data, mu_mle + 0.1, sigma_mle)
assert ll_mle > ll_perturbed, "MLE must maximize log-likelihood"
print(f"mu_mle={mu_mle:.4f}, sigma_mle={sigma_mle:.4f}")
print(f"ll(MLE)={ll_mle:.2f} > ll(perturbed)={ll_perturbed:.2f}")  # True
```

**Bernoulli â†’ Categorical: é›¢æ•£åˆ†å¸ƒã®ç³»è­œ**

$$
P(X=k \mid \mathbf{p}) = p_k, \quad k \in \{1,\ldots,K\},\quad \sum_k p_k = 1
$$

Bernoulli ã¯ $K=2$ ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã€‚Softmax ãŒ Categorical ã®å‡ºåŠ›å±¤ã«ãªã‚‹ç†ç”±: $\mathbf{p} = \text{softmax}(\mathbf{z})$ ã¨ã™ã‚Œã° $\sum_k p_k = 1$ ãŒè‡ªå‹•çš„ã«æº€ãŸã•ã‚Œã‚‹ã€‚

MLE: $N$ å€‹ã®è¦³æ¸¬ $x^{(1)},\ldots,x^{(N)}$ ã‹ã‚‰:

$$
\hat{p}_k = \frac{\#\{i : x^{(i)} = k\}}{N}
$$

ã‚«ã‚¦ãƒ³ãƒˆã‚’ $N$ ã§å‰²ã‚‹ã ã‘ã€‚äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤± $-\sum_k y_k \log p_k$ ã®æœ€å°åŒ– = Categorical MLE ã ã€‚

**å¤§æ•°ã®æ³•å‰‡ (LLN) ã¨ä¸­å¿ƒæ¥µé™å®šç† (CLT) â€” æ•°å€¤æ¤œè¨¼**

ç†è«–çš„ã«ä¿è¨¼ã•ã‚Œã¦ã„ã‚‹ãŒã€å…·ä½“çš„ã«ã©ã†åæŸã™ã‚‹ã‹æ•°å€¤ã§ç¢ºèªã™ã‚‹ã€‚

LLN: $\bar{X}_N \xrightarrow{P} \mu$ï¼ˆç¢ºç‡åæŸï¼‰

$$
P(|\bar{X}_N - \mu| > \epsilon) \leq \frac{\sigma^2}{N \epsilon^2}
$$

CLT: $\sqrt{N}(\bar{X}_N - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)$ï¼ˆåˆ†å¸ƒåæŸï¼‰

$$
Z_N = \frac{\bar{X}_N - \mu}{\sigma/\sqrt{N}} \xrightarrow{d} \mathcal{N}(0, 1)
$$

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\bar{X}_N = \frac{1}{N}\sum_{i=1}^N X_i$ â†” `X.mean(axis=1)` shape `(n_trials,)`
- $Z_N$ï¼ˆæ¨™æº–åŒ–æ¨™æœ¬å¹³å‡ï¼‰â†” `Z_N: (n_trials,)` â†’ `N(0,1)` ã«åæŸ
- $\text{KS}$ï¼ˆKolmogorov-Smirnovæ¤œå®šé‡ï¼‰â†” CLTåæŸã®å®šé‡çš„è©•ä¾¡

```python
import numpy as np
from scipy import stats

rng = np.random.default_rng(42)

# Exponential(lambda=1): mu=1, sigma^2=1
# æ­£è¦åˆ†å¸ƒã§ãªã„å…ƒåˆ†å¸ƒã§CLTã‚’ç¢ºèª
lam = 1.0
mu_true, sigma2_true = 1.0/lam, 1.0/lam**2  # Exp(1): mu=1, sigma^2=1

print("N     |LLN: E[|Xbar-mu|]  |CLT: KS p-value")
for N in [5, 20, 100, 500]:
    n_trials = 10000
    X = rng.exponential(scale=1.0/lam, size=(n_trials, N))  # (n_trials, N)
    Xbar = X.mean(axis=1)                                     # (n_trials,)

    # LLN: mean deviation from true mu
    lln_err = float(np.abs(Xbar - mu_true).mean())

    # CLT: standardize and KS test against N(0,1)
    Z_N = (Xbar - mu_true) / (sigma2_true**0.5 / N**0.5)    # (n_trials,)
    ks_stat, ks_pval = stats.kstest(Z_N, "norm")

    print(f"N={N:4d}  E|Xbar-mu|={lln_err:.5f}  KS_pval={ks_pval:.4f}")

# N=5  : KS p-value ä½ã„ (Exponential ã¯éå¯¾ç§°ãªã®ã§CLTãŒã¾ã åŠ¹ã‹ãªã„)
# N=500: KS p-value å¤§ãã„ (æ­£è¦åˆ†å¸ƒã«è¿‘ã„ -> CLTåæŸ)
```

**è§£é‡ˆ**: Exponentialåˆ†å¸ƒã¯å³è£¾ãŒé‡ã„ãŒã€N=500ã§æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒã¯ã»ã¼æ­£è¦åˆ†å¸ƒã«åæŸã™ã‚‹ã€‚LLNèª¤å·®ã¯NãŒå¢—ãˆã‚‹ã«ã¤ã‚Œ $O(1/\sqrt{N})$ ã§æ¸›å°‘ â€” Chebyshevä¸ç­‰å¼ã® $O(1/N)$ ã‚ˆã‚Šé€Ÿã„ï¼ˆæœŸå¾…å€¤ã®åæŸé€Ÿåº¦ï¼‰ã€‚

**Softmax ã¨ Categorical ã®å®Œå…¨å®Ÿè£…**:

$p_k = \frac{\exp(z_k)}{\sum_j \exp(z_j)}$ï¼ˆSoftmax = Categorical ã®è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\boldsymbol{\eta}$ ã‹ã‚‰æœŸå¾…å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\boldsymbol{\pi}$ ã¸ã®å¤‰æ›ï¼‰

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\mathbf{z}$ï¼ˆlogitï¼‰â†” `z: (K,)`
- $\boldsymbol{\pi} = \text{softmax}(\mathbf{z})$ â†” `pi: (K,)`, `sum=1`
- $\mathcal{H}(\boldsymbol{\pi}) = -\sum_k \pi_k \log \pi_k$ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰â†” `H: float`

```python
import numpy as np

def log_softmax(z):
    # z: (K,) -> log_p: (K,)  numerically stable
    c = z.max()                      # log-sum-exp shift
    log_Z = np.log(np.exp(z - c).sum()) + c
    return z - log_Z

def entropy_categorical(pi):
    # H(pi) = -sum pi_k log pi_k,  pi: (K,)
    pi = np.clip(pi, 1e-12, 1.0)    # numerical safety
    return float(-np.sum(pi * np.log(pi)))

# ç¢ºèª: uniform dist has max entropy = log K
K = 5
z_uniform = np.zeros(K)
log_p = log_softmax(z_uniform)
pi = np.exp(log_p)
H = entropy_categorical(pi)
assert np.allclose(pi, 1.0/K), f"uniform softmax failed: {pi}"
assert abs(H - np.log(K)) < 1e-10, f"max entropy should be log(K)={np.log(K):.4f}, got {H:.4f}"
print(f"uniform K={K}: H={H:.4f}, log(K)={np.log(K):.4f}  checked")

# ç¢ºèª: one-hot has entropy 0
z_onehot = np.array([100.0, 0.0, 0.0, 0.0, 0.0])
pi_oh = np.exp(log_softmax(z_onehot))
H_oh = entropy_categorical(pi_oh)
assert H_oh < 0.01, f"one-hot entropy should be ~0, got {H_oh}"
print(f"one-hot: H={H_oh:.6f}  checked")
```

**æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨ä¸€æ§˜åˆ†å¸ƒã®ç­‰ä¾¡æ€§**: ç¢ºç‡åˆ†å¸ƒã®é›†åˆä¸Šã§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’æœ€å¤§åŒ–ã™ã‚‹ã¨ä¸€æ§˜åˆ†å¸ƒãŒå¾—ã‚‰ã‚Œã‚‹ï¼ˆLagrangeä¹—æ•°æ³•ã§ç¢ºèªå¯èƒ½ï¼‰ã€‚ã“ã‚ŒãŒã€Œæƒ…å ±ãŒæœ€ã‚‚å°‘ãªã„åˆ†å¸ƒã€ã ã€‚

**å¤§æ•°ã®æ³•å‰‡ã®ç¢ºèª**:

```python
# LLN: Bernoulli sample mean -> p
rng = np.random.default_rng(42)
p_true = 0.3
for N in [10, 100, 1000, 10000]:
    samples = rng.binomial(1, p_true, N)
    p_hat = samples.mean()
    print(f"N={N:6d}  p_hat={p_hat:.4f}  |err|={abs(p_hat-p_true):.4f}")
# |err| -> 0 as N -> inf (LLN)
```

### 5.2 å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ â€” å®Œå…¨å®Ÿè£…ã¨ç›´æ„Ÿ

1æ¬¡å…ƒGaussianã®è‡ªç„¶ãªæ‹¡å¼µã¯ã€ã€Œå¤‰æ•°é–“ã®ç›¸é–¢ã€ã‚’æ‰ãˆã‚‹ã€‚

**å®šç¾©**:

$$
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) =
\frac{1}{(2\pi)^{d/2} |\boldsymbol{\Sigma}|^{1/2}}
\exp\!\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})\right)
$$

- shape: `x` ã¯ `(d,)`, `mu` ã¯ `(d,)`, `Sigma` ã¯ `(d,d)` æ­£å®šå€¤å¯¾ç§°è¡Œåˆ—
- Mahalanobisè·é›¢ $D_M^2 = (\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})$ ã¯ã€Œæ¥•å††ä½“ã®è·é›¢ã€
- $\boldsymbol{\Sigma}^{-1}$ ã®ç›´æ¥è¨ˆç®—ã¯é¿ã‘ã‚‹: `np.linalg.solve(Sigma, x-mu)` ã‚’ä½¿ã†

**æ¡ä»¶ä»˜ãåˆ†å¸ƒ** (Schur complement å…¬å¼):

å¤‰æ•°ã‚’ $[\mathbf{x}_1, \mathbf{x}_2]$ ã«åˆ†å‰²ã™ã‚‹ã¨:

$$
p(\mathbf{x}_1 \mid \mathbf{x}_2) = \mathcal{N}(\boldsymbol{\mu}_{1|2},\, \boldsymbol{\Sigma}_{1|2})
$$

$$
\boldsymbol{\mu}_{1|2} = \boldsymbol{\mu}_1 + \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}(\mathbf{x}_2 - \boldsymbol{\mu}_2)
$$

$$
\boldsymbol{\Sigma}_{1|2} = \boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}
$$

$\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}$ ã¯ã€ŒKalman gainã€ã®å½¢ã€‚$\mathbf{x}_2$ ã‚’è¦³æ¸¬ã™ã‚‹ã“ã¨ã§ã€$\mathbf{x}_1$ ã®ä¸ç¢ºå®Ÿæ€§ $\boldsymbol{\Sigma}_{1|2}$ ã¯å…ƒã® $\boldsymbol{\Sigma}_{11}$ ã‚ˆã‚Šå¿…ãšå°ã•ããªã‚‹ï¼ˆåŠæ­£å®šå€¤ã®æ„å‘³ã§ï¼‰ã€‚

**MLE**: å…¨å¾®åˆ†ã—ã¦ã‚¼ãƒ­ç‚¹ã‚’è§£ãã¨:

$$
\hat{\boldsymbol{\mu}} = \frac{1}{N}\sum_{i=1}^N \mathbf{x}^{(i)}, \quad
\hat{\boldsymbol{\Sigma}} = \frac{1}{N}\sum_{i=1}^N (\mathbf{x}^{(i)} - \hat{\boldsymbol{\mu}})(\mathbf{x}^{(i)} - \hat{\boldsymbol{\mu}})^\top
$$

ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã¨ã‚µãƒ³ãƒ—ãƒ«å…±åˆ†æ•£è¡Œåˆ—ãŒãã®ã¾ã¾MLEè§£ã ï¼ˆ1æ¬¡å…ƒã¨åŒã˜æ§‹é€ ï¼‰ã€‚


**Choleskyåˆ†è§£ã«ã‚ˆã‚‹å®‰å®šå®Ÿè£…**:

$\boldsymbol{\Sigma}$ ãŒæ­£å®šå€¤ â†’ $\boldsymbol{\Sigma} = LL^\top$ ã® Cholesky åˆ†è§£ãŒå­˜åœ¨ã™ã‚‹ã€‚

$$
\log \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) =
-\frac{d}{2}\log 2\pi - \frac{1}{2}\log|\boldsymbol{\Sigma}|
- \frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})
$$

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\boldsymbol{\mu}$ â†” `mu: (d,)`
- $\boldsymbol{\Sigma}$ â†” `Sigma: (d,d)` æ­£å®šå€¤å¯¾ç§°
- Choleskyå› å­ $L$ï¼ˆ$\boldsymbol{\Sigma}=LL^\top$ï¼‰â†” `L = np.linalg.cholesky(Sigma)`
- MahalanobisäºŒä¹—è·é›¢ $\|L^{-1}(\mathbf{x}-\boldsymbol{\mu})\|^2$ â†” `v @ v`

shape: `x` `(d,)`, `mu` `(d,)`, `Sigma` `(d,d)`, `v = L^{-1}(x-mu)` `(d,)`

```python
import numpy as np
from scipy.stats import multivariate_normal

def mvn_log_prob(x, mu, Sigma):
    # x: (d,), mu: (d,), Sigma: (d,d) positive definite
    d = len(mu)
    L = np.linalg.cholesky(Sigma)               # Sigma = L L^T
    v = np.linalg.solve(L, x - mu)             # v = L^{-1}(x-mu), (d,)
    maha2 = float(v @ v)                        # Mahalanobis^2
    log_det = 2.0 * np.sum(np.log(np.diag(L))) # log|Sigma|
    return -0.5 * (d * np.log(2 * np.pi) + log_det + maha2)

def mvn_mle(X):
    # X: (N, d) -> (mu_hat, Sigma_hat)
    N = len(X)
    mu_hat = X.mean(axis=0)
    diff = X - mu_hat
    Sigma_hat = (diff.T @ diff) / N  # biased MLE
    return mu_hat, Sigma_hat

# æ•°å€¤æ¤œè¨¼
rng = np.random.default_rng(42)
mu_t = np.array([1.0, -2.0])
S_t  = np.array([[2.0, 0.8], [0.8, 1.0]])
X = rng.multivariate_normal(mu_t, S_t, 5000)
mu_h, S_h = mvn_mle(X)
print(f"mu_hat:   {mu_h.round(3)}")     # â‰ˆ [1.0, -2.0]
print(f"Sig_hat:\n{S_h.round(3)}")      # â‰ˆ [[2.0,0.8],[0.8,1.0]]
x0 = np.array([1.0, -1.0])
ours = mvn_log_prob(x0, mu_t, S_t)
ref  = multivariate_normal.logpdf(x0, mu_t, S_t)
assert abs(ours - ref) < 1e-10
print(f"log p(x0) = {ours:.6f}  [scipy: {ref:.6f}]  checked")
```

**è½ã¨ã—ç©´**: $N < d$ ã§ã¯ $\hat{\boldsymbol{\Sigma}}$ ãŒåŠæ­£å®šå€¤ã«ãªã‚ŠCholeskyåˆ†è§£ãŒå¤±æ•—ã™ã‚‹ã€‚$\hat{\boldsymbol{\Sigma}} + 10^{-6}I$ ã®æ­£å‰‡åŒ–ã§å›é¿ã€‚

**æ¡ä»¶ä»˜ãåˆ†å¸ƒ**:

$$
\boldsymbol{\mu}_{1|2} = \boldsymbol{\mu}_1 + \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}(\mathbf{x}_2 - \boldsymbol{\mu}_2)
$$

$$
\boldsymbol{\Sigma}_{1|2} = \boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}
$$

$\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}$ ã¯ Kalman gain ã¨åŒå‹ã€‚$\mathbf{x}_2$ ã‚’è¦³æ¸¬ã™ã‚‹ã¨åˆ†æ•£ã¯å¿…ãšç¸®ã‚€: $\boldsymbol{\Sigma}_{1|2} \preceq \boldsymbol{\Sigma}_{11}$ï¼ˆåŠæ­£å®šå€¤é †åºï¼‰ã€‚

```python
def mvn_conditional(mu, Sigma, obs_idx, obs_val):
    d = len(mu)
    free = [i for i in range(d) if i not in obs_idx]
    S11 = Sigma[np.ix_(free, free)]
    S12 = Sigma[np.ix_(free, obs_idx)]
    S22 = Sigma[np.ix_(obs_idx, obs_idx)]
    gain = np.linalg.solve(S22.T, S12.T).T  # S12 @ S22^{-1}
    mu_c  = mu[free] + gain @ (obs_val - mu[obs_idx])
    Sig_c = S11 - gain @ S12.T
    return mu_c, Sig_c

mu = np.array([1.0, -2.0]); S = np.array([[2.0, 0.8],[0.8, 1.0]])
mc, Sc = mvn_conditional(mu, S, obs_idx=[1], obs_val=np.array([-1.0]))
print(f"mu(x1|x2=-1)  = {mc[0]:.4f}")   # = 1 + 0.8*(1) = 1.8
print(f"Var(x1|x2=-1) = {Sc[0,0]:.4f}") # = 2 - 0.64 = 1.36
assert Sc[0,0] < S[0,0]                 # æ¡ä»¶ä»˜ã‘ã§åˆ†æ•£æ¸›å°‘ checked
```

### 5.3 æŒ‡æ•°å‹åˆ†å¸ƒæ— â€” çµ±ä¸€çš„è¨˜è¿°

Gaussian, Bernoulli, Poisson, Gamma... ä¸€è¦‹ãƒãƒ©ãƒãƒ©ã«è¦‹ãˆã‚‹åˆ†å¸ƒãŒã€ŒåŒã˜æ–‡æ³•ã€ã§æ›¸ã‘ã‚‹ã€‚

**æ¨™æº–å½¢**:

$$
p(x \mid \boldsymbol{\eta}) = h(x) \exp\!\left(\boldsymbol{\eta}^\top T(x) - A(\boldsymbol{\eta})\right)
$$

- $\boldsymbol{\eta}$: è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆnatural parameterï¼‰
- $T(x)$: ååˆ†çµ±è¨ˆé‡ï¼ˆsufficient statisticï¼‰â€” ãƒ‡ãƒ¼ã‚¿ã®ã€Œè¦ç´„ã€
- $A(\boldsymbol{\eta})$: å¯¾æ•°åˆ†é…é–¢æ•°ï¼ˆlog partition functionï¼‰â€” æ­£è¦åŒ–å®šæ•°

**Gaussian ã®å ´åˆ** ($d=1$):

$$
\boldsymbol{\eta} = \begin{pmatrix}\mu/\sigma^2 \\ -1/(2\sigma^2)\end{pmatrix},\quad
T(x) = \begin{pmatrix}x \\ x^2\end{pmatrix},\quad
A(\boldsymbol{\eta}) = -\frac{\eta_1^2}{4\eta_2} + \frac{1}{2}\log\frac{\pi}{-\eta_2}
$$

**MLEã®ç¾ã—ã•**: æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®MLEã¯ã€Œç†è«–çš„æœŸå¾…å€¤ = çµŒé¨“çš„æœŸå¾…å€¤ã€ã¨ã„ã†æ¡ä»¶:

$$
\mathbb{E}_{p(x|\hat{\boldsymbol{\eta}})}[T(x)] = \frac{1}{N}\sum_{i=1}^N T(x^{(i)})
$$

Gaussianãªã‚‰ $T(x) = (x, x^2)$ ãªã®ã§ã€å¹³å‡ã¨äºŒä¹—å¹³å‡ãŒä¸€è‡´ã™ã‚‹æ¡ä»¶ = ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ãƒ»åˆ†æ•£ãŒMLEã€‚

**å…±å½¹äº‹å‰åˆ†å¸ƒ**: äº‹å‰åˆ†å¸ƒã‚’ $p(\boldsymbol{\eta}) = h(\boldsymbol{\eta})\exp(\boldsymbol{\chi}^\top \boldsymbol{\eta} - \nu A(\boldsymbol{\eta}))$ ã¨æ›¸ãã¨ã€äº‹å¾Œåˆ†å¸ƒãŒåŒã˜æ—ã«å±ã™ã‚‹ï¼ˆå…±å½¹æ€§ï¼‰ã€‚Gaussian-Gaussian å…±å½¹ã€Beta-Bernoulli å…±å½¹ ã¯ã“ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã€‚


**æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®çµ±ä¸€å®Ÿè£…**:

æŠ½è±¡çš„ã«è¦‹ãˆã‚‹ãŒã€Gaussian/Bernoulli/PoissonãŒåŒã˜ã‚¯ãƒ©ã‚¹ã§æ›¸ã‘ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\boldsymbol{\eta}$ï¼ˆè‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰â†” `eta: ndarray`
- $T(x)$ï¼ˆååˆ†çµ±è¨ˆé‡ï¼‰â†” `suff_stat(x)`
- $A(\boldsymbol{\eta})$ï¼ˆå¯¾æ•°åˆ†é…é–¢æ•°ï¼‰â†” `log_partition(eta)`
- MLEæ¡ä»¶: $\mathbb{E}[T(x)] = \bar{T}$ â†” `eta_mle` ã‚’æ•°å€¤æœ€é©åŒ–

shape: `eta` `(k,)` where `k` ã¯ååˆ†çµ±è¨ˆé‡ã®æ¬¡å…ƒï¼ˆGaussian: k=2, Bernoulli: k=1ï¼‰

```python
import numpy as np
from scipy.optimize import minimize

class ExpFamilyGaussian:
    """1æ¬¡å…ƒGaussianã®æŒ‡æ•°å‹åˆ†å¸ƒæ—è¡¨ç¾
    eta = [mu/sigma^2, -1/(2*sigma^2)]
    T(x) = [x, x^2]
    """
    @staticmethod
    def to_natural(mu: float, sigma2: float):
        eta1 = mu / sigma2
        eta2 = -1.0 / (2.0 * sigma2)
        return np.array([eta1, eta2])

    @staticmethod
    def to_moment(eta: np.ndarray):
        # eta = [eta1, eta2] -> (mu, sigma^2)
        sigma2 = -1.0 / (2.0 * eta[1])
        mu     = eta[0] * sigma2
        return mu, sigma2

    @staticmethod
    def suff_stat(x: np.ndarray) -> np.ndarray:
        # T(x) = [x, x^2], shape: (N, 2)
        return np.column_stack([x, x ** 2])

    @staticmethod
    def log_partition(eta: np.ndarray) -> float:
        # A(eta) = -eta1^2/(4*eta2) + 0.5*log(pi/(-eta2))
        eta1, eta2 = eta
        return -eta1**2 / (4*eta2) + 0.5 * np.log(np.pi / (-eta2))

    @classmethod
    def mle(cls, x: np.ndarray):
        # MLE: E[T(x)] = empirical mean of T(x)
        # For Gaussian this has a closed form, but we verify numerically
        T_bar = cls.suff_stat(x).mean(axis=0)  # [x_bar, x^2_bar]
        # closed form: mu = T_bar[0], sigma^2 = T_bar[1] - T_bar[0]^2
        mu_mle = T_bar[0]
        sigma2_mle = T_bar[1] - T_bar[0]**2
        return cls.to_natural(mu_mle, sigma2_mle)

# æ•°å€¤æ¤œè¨¼
rng = np.random.default_rng(0)
X = rng.normal(loc=3.0, scale=2.0, size=2000)
eta_hat = ExpFamilyGaussian.mle(X)
mu_hat, sigma2_hat = ExpFamilyGaussian.to_moment(eta_hat)
print(f"mu_hat = {mu_hat:.4f}   (true: 3.0)")
print(f"sigma_hat = {sigma2_hat**0.5:.4f}  (true: 2.0)")

# ååˆ†çµ±è¨ˆé‡æ¡ä»¶ã‚’ç¢ºèª: E[T(x)] = empirical mean of T(x)
T_bar = ExpFamilyGaussian.suff_stat(X).mean(axis=0)
E_T_hat = np.array([mu_hat, mu_hat**2 + sigma2_hat])  # E[x], E[x^2] under N(mu,sigma^2)
assert np.allclose(T_bar, E_T_hat, atol=0.1), f"MLE condition violated: {T_bar} vs {E_T_hat}"
print(f"E[T(x)] = {E_T_hat.round(3)}, empirical = {T_bar.round(3)}  checked")
```

**ãªãœå¯¾æ•°åˆ†é…é–¢æ•° $A(\boldsymbol{\eta})$ ãŒé‡è¦ã‹**: $A$ ã®ä¸€æ¬¡å¾®åˆ†ãŒæœŸå¾…å€¤ã€äºŒæ¬¡å¾®åˆ†ãŒå…±åˆ†æ•£ã‚’ä¸ãˆã‚‹ã€‚

$$
\nabla_{\boldsymbol{\eta}} A(\boldsymbol{\eta}) = \mathbb{E}_{p(x|\boldsymbol{\eta})}[T(x)]
$$

$$
\nabla^2_{\boldsymbol{\eta}} A(\boldsymbol{\eta}) = \text{Cov}_{p}[T(x), T(x)] \succeq 0
$$

$A$ ãŒå‡¸ â†’ è² ã®å¯¾æ•°å°¤åº¦ã‚‚å‡¸ â†’ MLEã¯å¤§åŸŸçš„æœ€é©è§£ã€‚ã“ã‚ŒãŒæŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ã€Œå­¦ç¿’ã—ã‚„ã™ã•ã€ã®æœ¬è³ªã ã€‚

**è‡ªç„¶å‹¾é…æ³• (Natural Gradient) ã¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**:

æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã¯ã€ŒRiemannianå¤šæ§˜ä½“ã€ã ã€‚Fisheræƒ…å ±è¡Œåˆ— $\mathbf{I}(\boldsymbol{\eta})$ ãŒãã®ç©ºé–“ã®è¨ˆé‡ã‚’ä¸ãˆã‚‹ã€‚

é€šå¸¸ã®å‹¾é…é™ä¸‹: $\boldsymbol{\eta}_{t+1} = \boldsymbol{\eta}_t - \alpha \nabla_{\boldsymbol{\eta}} \mathcal{L}$

è‡ªç„¶å‹¾é…é™ä¸‹: $\boldsymbol{\eta}_{t+1} = \boldsymbol{\eta}_t - \alpha \mathbf{I}^{-1}(\boldsymbol{\eta}_t) \nabla_{\boldsymbol{\eta}} \mathcal{L}$

è‡ªç„¶å‹¾é…ã¯ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®è·é›¢ã€ã§ã¯ãªãã€Œåˆ†å¸ƒç©ºé–“ã®KLè·é›¢ã€ã§ã‚¹ãƒ†ãƒƒãƒ—ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚åŒã˜åˆ†å¸ƒã®å¤‰åŒ–é‡ã«å¯¾å¿œã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ãŒã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤ã«ä¾å­˜ã—ãªã„ â€” ã“ã‚ŒãŒAdamãªã©ã®é©å¿œçš„æœ€é©åŒ–ã®ç†è«–çš„åŸºç›¤ã ï¼ˆç¬¬12å›ã§è©³èª¬ï¼‰ã€‚

æŒ‡æ•°å‹åˆ†å¸ƒæ—ã§ã¯è‡ªç„¶å‹¾é…ã«é–‰å½¢å¼ãŒã‚ã‚‹: $\mathbf{I}^{-1}(\boldsymbol{\eta}) \nabla_{\boldsymbol{\eta}} \mathcal{L} = \nabla_{\boldsymbol{\mu}} \mathcal{L}$ï¼ˆæœŸå¾…å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®é€šå¸¸å‹¾é…ã¨ç­‰ä¾¡ï¼‰ã€‚

### 5.4 å®Ÿè£…æ¼”ç¿’: ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰ã®MLE

ç¬¬8å›ï¼ˆEMç®—æ³•ï¼‰ã¸ã®æ©‹æ¸¡ã—ã¨ã—ã¦ã€2æˆåˆ†GMMã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹ã€‚ã“ã“ã§ã¯EMç®—æ³•ã®å‰æ®µéšã¨ã—ã¦ã€å˜ä¸€ã‚¬ã‚¦ã‚¹ã®MLEã‚’æ‹¡å¼µã™ã‚‹å½¢ã§å•é¡Œã®å›°é›£ã•ã‚’ä½“æ„Ÿã™ã‚‹ã€‚

$$
p(x\\mid \\theta)=\\pi\\,\\mathcal{N}(x\\mid \\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\mathcal{N}(x\\mid \\mu_2,\\sigma_2^2)

\\ell(\\theta)=\\sum_{i=1}^{N}\\log p(x_i\\mid\\theta)

\\mathcal{N}(x\\mid\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)
$$

```python
import numpy as np

np.random.seed(42)
N = 1000  # samples

# True parameters
pi_true = 0.4
mu1_true, sigma1_true = -2.0, 0.8
mu2_true, sigma2_true = 3.0, 1.2

component = np.random.binomial(1, 1 - pi_true, N)
data = np.where(component == 0,
                np.random.normal(mu1_true, sigma1_true, N),
                np.random.normal(mu2_true, sigma2_true, N))

def normal_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:
    z = (x - mu) / sigma
    return (1.0 / (np.sqrt(2.0 * np.pi) * sigma)) * np.exp(-0.5 * z * z)

mu_single = data.mean()
sigma_single = data.std()

def gmm_log_likelihood(x: np.ndarray, pi: float, mu1: float, sig1: float, mu2: float, sig2: float) -> float:
    px = pi * normal_pdf(x, mu1, sig1) + (1.0 - pi) * normal_pdf(x, mu2, sig2)
    return float(np.sum(np.log(px + 1e-12)))

ll_true = gmm_log_likelihood(data, pi_true, mu1_true, sigma1_true, mu2_true, sigma2_true)
ll_single = float(np.sum(np.log(normal_pdf(data, mu_single, sigma_single) + 1e-12)))

print(f"single Gaussian MLE: mu={mu_single:.3f}, sigma={sigma_single:.3f}")
print(f"loglik (true GMM):   {ll_true:.2f}")
print(f"loglik (single Gauss): {ll_single:.2f}")
print(f"gap: {ll_true - ll_single:.2f}")

print("note: GMM ã® MLE ã¯é–‰å½¢å¼ã«ãªã‚‰ãªã„ï¼ˆç¬¬8å›ã® EM ã«ã¤ãªãŒã‚‹ï¼‰")
```

**ãªãœGMMã®MLEã¯é–‰ã˜ãŸå½¢ã§è§£ã‘ãªã„ã®ã‹**: å¯¾æ•°å°¤åº¦ã®ä¸­ã«**å’Œã®å¯¾æ•°** $\log[\pi \mathcal{N}(x \mid \mu_1, \sigma_1^2) + (1-\pi)\mathcal{N}(x \mid \mu_2, \sigma_2^2)]$ ãŒç¾ã‚Œã‚‹ã€‚å¯¾æ•°ã¨å’Œã®é †åºã‚’å…¥ã‚Œæ›¿ãˆã‚‰ã‚Œãªã„ãŸã‚ã€å¾®åˆ†ã—ã¦ã‚‚å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒäº’ã„ã«çµ¡ã¿åˆã†ã€‚ã“ã®å›°é›£ãŒç¬¬8å›ã®EMç®—æ³•ã®å‹•æ©Ÿã ã€‚

### 5.5a å®Ÿè£…æ¼”ç¿’: ãƒ™ã‚¤ã‚ºæ¨è«–ã®ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼

$$
\\theta\\sim\\mathrm{Beta}(a,b),\\quad x_i\\sim\\mathrm{Bernoulli}(\\theta)

p(\\theta\\mid\\mathbf{x})\\propto \\theta^{a+h-1}(1-\\theta)^{b+t-1}

\\theta\\mid\\mathbf{x}\\sim\\mathrm{Beta}(a+h,b+t)
$$

```python
import numpy as np

from math import lgamma

def log_beta(a: float, b: float) -> float:
    return lgamma(a) + lgamma(b) - lgamma(a + b)

np.random.seed(42)

theta_true = 0.7
x = np.random.binomial(1, theta_true, size=20)
h = int(x.sum())
t = int(len(x) - h)

# uniform prior Beta(1,1)
a, b = 1.0, 1.0
post_a, post_b = a + h, b + t

theta = np.linspace(1e-4, 1 - 1e-4, 4000)
log_post = (post_a - 1) * np.log(theta) + (post_b - 1) * np.log(1 - theta) - log_beta(post_a, post_b)
post = np.exp(log_post - log_post.max())  # numerical stability
post /= np.trapz(post, theta)

mean_grid = float(np.trapz(theta * post, theta))
mean_analytic = post_a / (post_a + post_b)
mle = h / (h + t)

print(f"data: {h}H/{t}T (N={h+t})")
print(f"posterior: Beta({post_a:.1f}, {post_b:.1f})")
print(f"mean(grid)={mean_grid:.4f} mean(analytic)={mean_analytic:.4f} mle={mle:.4f}")
print("note: é«˜æ¬¡å…ƒã ã¨ã‚°ãƒªãƒƒãƒ‰ã¯ç ´ç¶»ï¼ˆæ¬¡å…ƒã®å‘ªã„ï¼‰")
```

> **Note:** **å®Ÿè£…ã®æ•™è¨“**: ãƒ‡ãƒ¼ã‚¿ãŒå¢—ãˆã‚‹ã»ã©ã€äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ã¯è–„ã‚Œã€ãƒ™ã‚¤ã‚ºæ¨å®šã¯MLEã«è¿‘ã¥ãã€‚ã“ã‚Œã¯äº‹å¾Œåˆ†å¸ƒãŒã€Œå°¤åº¦ã«æ”¯é…ã•ã‚Œã‚‹ã€ãŸã‚ã€‚é€†ã«ã€ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã¨ãã¯äº‹å‰åˆ†å¸ƒãŒçµæœã‚’å¤§ããå·¦å³ã™ã‚‹ã€‚

ã“ã®ç¾è±¡ã‚’ã€Œäº‹å¾Œä¸€è‡´æ€§ï¼ˆposterior consistencyï¼‰ã€ã¨å‘¼ã¶ã€‚$N \to \infty$ ã§äº‹å¾Œåˆ†å¸ƒã¯çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é›†ä¸­ã™ã‚‹ â€” å¤§æ•°ã®æ³•å‰‡ã®ãƒ™ã‚¤ã‚ºç‰ˆã ã€‚

### 5.5b å®Ÿè£…æ¼”ç¿’: å…±å½¹äº‹å‰åˆ†å¸ƒã®è§£æçš„æ›´æ–°

ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼ãŒã€Œæ•°å€¤çš„ã€ãªã‚‰ã°ã€å…±å½¹äº‹å‰åˆ†å¸ƒã¯ã€Œè§£æçš„ã€ã ã€‚

**Gaussian-Gaussian å…±å½¹ï¼ˆæ—¢çŸ¥åˆ†æ•£ã€æœªçŸ¥å¹³å‡ï¼‰**:

äº‹å‰: $\theta \sim \mathcal{N}(\mu_0, \tau_0^2)$ã€å°¤åº¦: $X_i \mid \theta \sim \mathcal{N}(\theta, \sigma^2)$

$$
\frac{1}{\tau_N^2} = \frac{1}{\tau_0^2} + \frac{N}{\sigma^2}, \quad
\mu_N = \tau_N^2 \left(\frac{\mu_0}{\tau_0^2} + \frac{N \bar{x}}{\sigma^2}\right)
$$

ç²¾åº¦ï¼ˆåˆ†æ•£ã®é€†æ•°ï¼‰ãŒåŠ æ³•çš„ã«æ›´æ–°ã•ã‚Œã‚‹ã€‚$N \to \infty$ ã§ $\mu_N \to \bar{x}$ï¼ˆMLEï¼‰ã€$\tau_N^2 \to 0$ã€‚

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\mu_0, \tau_0^2$ â†” `mu0, tau0_sq`
- $\sigma^2$ â†” `sigma_sq`ï¼ˆæ—¢çŸ¥ã®å°¤åº¦åˆ†æ•£ï¼‰
- $\bar{x}, N$ â†” `x_bar, N`
- $\mu_N, \tau_N^2$ â†” `mu_N, tau_N_sq`ï¼ˆäº‹å¾Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

```python
import numpy as np

def gaussian_conjugate_update(mu0, tau0_sq, sigma_sq, x_bar, N):
    prec_N = 1.0/tau0_sq + N/sigma_sq
    tau_N_sq = 1.0 / prec_N
    mu_N = tau_N_sq * (mu0/tau0_sq + x_bar * N/sigma_sq)
    return mu_N, tau_N_sq

rng = np.random.default_rng(42)
theta_true, sigma_sq = 3.0, 4.0
print(f"{'N':>4}  {'MLE':>8}  {'post_mu(strong)':>16}  {'post_mu(weak)':>14}")
for N in [1, 5, 20, 100]:
    x = rng.normal(theta_true, sigma_sq**0.5, N)
    xb = x.mean()
    ms, _ = gaussian_conjugate_update(0.0, 0.5, sigma_sq, xb, N)   # strong prior
    mw, _ = gaussian_conjugate_update(0.0, 100.0, sigma_sq, xb, N) # weak prior
    print(f"{N:>4}  {xb:>8.3f}  {ms:>16.3f}  {mw:>14.3f}")
# Nå¢—åŠ  -> strong prior ã®å½±éŸ¿ãŒæ¶ˆãˆã€MLE ã«åæŸ
```

**3æ¨å®šé‡ã®æ¯”è¼ƒ**:

| æ¨å®šé‡ | å¼ | ç‰¹å¾´ |
|:-------|:---|:-----|
| MLE | $\bar{x}$ | ãƒã‚¤ã‚¢ã‚¹ãªã—ã€å°ãƒ‡ãƒ¼ã‚¿ä¸å®‰å®š |
| MAP | $\mu_N$ | äº‹å‰+å°¤åº¦ã€æ­£å‰‡åŒ–ã¨ç­‰ä¾¡ |
| äº‹å¾Œå¹³å‡ | $\mu_N$ï¼ˆGaussianäº‹å¾Œï¼‰| MAP=äº‹å¾Œå¹³å‡ |

### 5.5a KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ â€” åˆ†å¸ƒé–“ã®ã€Œè·é›¢ã€å®Ÿè£…

KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ç¢ºç‡è«–ã®å…¨ã¦ã®æ­¦å™¨ãŒé›†çµã™ã‚‹å ´æ‰€ã ã€‚VAEã®ELBOã€diffusion modelã®ç›®çš„é–¢æ•°ã€æƒ…å ±ç†è«–ã®åŸºç¤ â€” å…¨ã¦ã“ã“ã«é€šã˜ã‚‹ã€‚

$$
D_{\mathrm{KL}}(p \| q) = \int p(x) \log \frac{p(x)}{q(x)} dx = \mathbb{E}_{p}\left[\log \frac{p(X)}{q(X)}\right]
$$

**åŸºæœ¬æ€§è³ª**:
- $D_{\mathrm{KL}}(p \| q) \geq 0$ï¼ˆGibbsä¸ç­‰å¼ã€Jensenä¸ç­‰å¼ã‹ã‚‰ï¼‰
- $D_{\mathrm{KL}}(p \| q) = 0 \iff p = q$ï¼ˆã»ã¼è‡³ã‚‹æ‰€ã§ï¼‰
- éå¯¾ç§°: $D_{\mathrm{KL}}(p \| q) \neq D_{\mathrm{KL}}(q \| p)$ï¼ˆè·é›¢å…¬ç†ã‚’æº€ãŸã•ãªã„ï¼‰

**2ã¤ã®Gaussiané–“ã®KLï¼ˆé–‰å½¢å¼ï¼‰**:

$$
D_{\mathrm{KL}}(\mathcal{N}(\mu_1, \sigma_1^2) \| \mathcal{N}(\mu_2, \sigma_2^2)) =
\log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}
$$

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\mu_1, \sigma_1^2$ â†” `mu1, var1` (åˆ†å¸ƒ $p$)
- $\mu_2, \sigma_2^2$ â†” `mu2, var2` (åˆ†å¸ƒ $q$)
- $D_{\mathrm{KL}}$ â†” `kl: float` (éè² ã‚¹ã‚«ãƒ©ãƒ¼)

shape: scalar inputs â†’ scalar output

```python
import numpy as np
from scipy import stats

def kl_gaussian(mu1, var1, mu2, var2):
    """KL(N(mu1,var1) || N(mu2,var2)) â€” closed form
    = log(sigma2/sigma1) + (var1 + (mu1-mu2)^2)/(2*var2) - 1/2
    """
    return (np.log(var2/var1) + (var1 + (mu1-mu2)**2) / (2*var2) - 1) / 2.0

# æ•°å€¤æ¤œè¨¼ 1: éè² æ€§ã®ç¢ºèª
kl_same = kl_gaussian(mu1=2.0, var1=1.0, mu2=2.0, var2=1.0)
assert abs(kl_same) < 1e-10, f"KL(p||p) must be 0, got {kl_same}"
print(f"KL(p||p) = {kl_same:.2e}  (should be 0) checked")

# æ•°å€¤æ¤œè¨¼ 2: éå¯¾ç§°æ€§
kl_pq = kl_gaussian(mu1=0.0, var1=1.0, mu2=1.0, var2=2.0)
kl_qp = kl_gaussian(mu1=1.0, var1=2.0, mu2=0.0, var2=1.0)
print(f"KL(p||q) = {kl_pq:.4f},  KL(q||p) = {kl_qp:.4f}  (asymmetric)")
assert kl_pq != kl_qp, "KL is asymmetric"

# æ•°å€¤æ¤œè¨¼ 3: Monte Carloã§é–‰å½¢å¼ã¨æ¯”è¼ƒ
rng = np.random.default_rng(42)
mu1, var1, mu2, var2 = 1.0, 1.0, 2.0, 3.0
x = rng.normal(mu1, var1**0.5, 1000000)  # sample from p
log_p = stats.norm.logpdf(x, mu1, var1**0.5)
log_q = stats.norm.logpdf(x, mu2, var2**0.5)
kl_mc = float(np.mean(log_p - log_q))
kl_exact = kl_gaussian(mu1, var1, mu2, var2)
print(f"KL exact={kl_exact:.6f},  MC={kl_mc:.6f}  diff={abs(kl_exact-kl_mc):.6f}")
assert abs(kl_exact - kl_mc) < 0.01, "KL MC vs exact mismatch"
```

**VAEã¨ã®æ¥ç¶š**: VAEã®ELBOã«ã¯ $D_{\mathrm{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))$ ãŒç™»å ´ã™ã‚‹ã€‚$p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ã€$q_\phi = \mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$ ãªã‚‰ã€æ¬¡å…ƒç‹¬ç«‹ãªGaussian KLã®é–‰å½¢å¼ãŒä½¿ãˆã‚‹:

$$
D_{\mathrm{KL}}(q \| p) = \frac{1}{2} \sum_{j=1}^d (\sigma_j^2 + \mu_j^2 - 1 - \log \sigma_j^2)
$$

ç¬¬8å›ï¼ˆVAEï¼‰ã§ã“ã®å¼ãŒæå¤±é–¢æ•°ã«ç›´æ¥ç¾ã‚Œã‚‹ã€‚

### 5.5c Fisheræƒ…å ±é‡ â€” CramÃ©r-Raoä¸‹ç•Œã®å®Ÿè£…æ¤œè¨¼

Fisheræƒ…å ±é‡ $I(\theta) = \mathbb{E}\left[\left(\frac{\partial \log p(x;\theta)}{\partial \theta}\right)^2\right]$ ã¯æ¨å®šã®é›£ã—ã•ã‚’å®šé‡åŒ–ã™ã‚‹ã€‚

ç­‰ä¾¡ãªè¡¨ç¾ï¼ˆå¯¾æ•°å°¤åº¦ã®æ›²ç‡ï¼‰:

$$
I(\theta) = -\mathbb{E}\left[\frac{\partial^2 \log p(x; \theta)}{\partial \theta^2}\right]
$$

**CramÃ©r-Raoä¸‹ç•Œ**: ä»»æ„ã®ä¸åæ¨å®šé‡ã®åˆ†æ•£ã¯ $1/(n I(\theta))$ ã‚ˆã‚Šå°ã•ãã§ããªã„:

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{n \cdot I(\theta)}
$$

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $\theta$ â†” `theta: float`
- ã‚¹ã‚³ã‚¢é–¢æ•° $s(x;\theta) = \partial_\theta \log p$ â†” `score: (N,)`
- $I(\theta) = \mathbb{E}[s^2]$ â†” `fisher_info: float`
- CRä¸‹ç•Œ $1/(nI)$ â†” `cr_bound: float`

```python
import numpy as np

def fisher_info_gauss_mean(sigma2: float) -> float:
    # I(mu) = 1/sigma^2 for X~N(mu, sigma^2)
    return 1.0 / sigma2

def score_gauss_mean(x, mu, sigma2):
    # s(x; mu) = d/dmu log N(x|mu,sigma^2) = (x-mu)/sigma^2
    return (x - mu) / sigma2

def cramer_rao(n: int, fisher: float) -> float:
    return 1.0 / (n * fisher)

# æ•°å€¤æ¤œè¨¼: æ¨™æœ¬å¹³å‡ã®åˆ†æ•£ vs CRä¸‹ç•Œ
rng = np.random.default_rng(0)
mu_true, sigma2_true = 2.0, 4.0
fi = fisher_info_gauss_mean(sigma2_true)  # = 0.25
print(f"Fisher info I(mu) = {fi:.4f}  (= 1/sigma^2)")

for n in [10, 50, 100, 500]:
    samples = rng.normal(mu_true, sigma2_true**0.5, (5000, n))
    var_mle = float(samples.mean(axis=1).var())
    cr = cramer_rao(n, fi)
    print(f"N={n:4d}  CR_bound={cr:.6f}  Var(mu_hat)={var_mle:.6f}  ratio={var_mle/cr:.4f}")
# ratio â‰ˆ 1.0: sample mean is an efficient estimator for mu
```

**æ¤œè¨¼**: æ¨™æœ¬å¹³å‡ã¯CramÃ©r-Raoä¸‹ç•Œã‚’**ã´ã£ãŸã‚Šé”æˆ**ã™ã‚‹ï¼ˆFisheråŠ¹ç‡çš„æ¨å®šé‡ï¼‰ã€‚æ¯”ç‡ãŒå…¨ã¦â‰ˆ1.0ã«ãªã‚‹ã€‚

**ã‚¹ã‚³ã‚¢ã®æœŸå¾…å€¤ã¯ã‚¼ãƒ­**: $\mathbb{E}[s(X;\theta)] = 0$ã€‚$\int p(x;\theta) dx = 1$ ã‚’ $\theta$ ã§å¾®åˆ†ã™ã‚‹ã¨å°ã‘ã‚‹ï¼ˆæ­£è¦åŒ–æ¡ä»¶ã®å¾®åˆ†ï¼‰ã€‚Fisheræƒ…å ±é‡ã¯ã‚¹ã‚³ã‚¢ã®åˆ†æ•£ã ã€‚

$$
\mathbb{E}[s] = \int \frac{\partial \log p}{\partial \theta} p \, dx = \frac{\partial}{\partial \theta} \int p \, dx = \frac{\partial}{\partial \theta} 1 = 0
$$

**å¤šæ¬¡å…ƒFisheræƒ…å ±è¡Œåˆ— (FIM)**: $\mathbf{I}(\boldsymbol{\theta})_{ij} = \mathbb{E}[\partial_i \log p \cdot \partial_j \log p]$ã€‚è‡ªç„¶å‹¾é…æ³• $\tilde{\nabla}_\theta \mathcal{L} = \mathbf{I}^{-1} \nabla_\theta \mathcal{L}$ ã¯FIMã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®æ›²ç‡ã‚’è£œæ­£ã—ã€ç¢ºç‡å¤šæ§˜ä½“ä¸Šã®æœ€é©è§£ã«æœ€çŸ­çµŒè·¯ã§åˆ°é”ã™ã‚‹ã€‚

### 5.6 ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¯é–¢æ•°ã¨ç‰¹æ€§é–¢æ•°

**ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¯é–¢æ•°ï¼ˆMGFï¼‰**: $M_X(t) = \mathbb{E}[e^{tX}]$

MGFã® $k$ æ¬¡å¾®åˆ†ã¯ $k$ æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’ä¸ãˆã‚‹: $M_X^{(k)}(0) = \mathbb{E}[X^k]$


MGFãŒå­˜åœ¨ã—ãªã„åˆ†å¸ƒã‚‚ã‚ã‚‹ï¼ˆCauchyåˆ†å¸ƒãªã©ï¼‰ã€‚ãã®å ´åˆã¯**ç‰¹æ€§é–¢æ•°** $\varphi_X(t) = \mathbb{E}[e^{itX}]$ ã‚’ä½¿ã†ã€‚ç‰¹æ€§é–¢æ•°ã¯å¸¸ã«å­˜åœ¨ã—ã€åˆ†å¸ƒã‚’ä¸€æ„ã«æ±ºå®šã™ã‚‹ã€‚CLTã®è¨¼æ˜ã¯ã—ã°ã—ã°ç‰¹æ€§é–¢æ•°ã‚’ç”¨ã„ã¦è¡Œã‚ã‚Œã‚‹ã€‚

Gaussianã®å ´åˆ: $M_X(t) = \exp(\mu t + \frac{\sigma^2 t^2}{2})$ã€‚

**ç‹¬ç«‹å’Œã®æ€§è³ª**: $X, Y$ ãŒç‹¬ç«‹ãªã‚‰ $M_{X+Y}(t) = M_X(t) M_Y(t)$ã€‚ã“ã‚ŒãŒCLTè¨¼æ˜ã®æ ¸å¿ƒã  â€” ã‚µãƒ³ãƒ—ãƒ«å’Œã®ç‰¹æ€§é–¢æ•°ãŒå…ƒã®ç‰¹æ€§é–¢æ•°ã®ç©ã«ãªã‚Šã€$N \to \infty$ ã§æ­£è¦åˆ†å¸ƒã®ç‰¹æ€§é–¢æ•°ã«åæŸã™ã‚‹ã€‚

$$
M_X(t) = \mathbb{E}[e^{tX}] = \int e^{tx} p(x) \, dx
$$

è¨˜å· â†” å¤‰æ•°å¯¾å¿œ:
- $t$ â†” `t: float`ï¼ˆMGFã®å¼•æ•°ã€ãƒ©ãƒ—ãƒ©ã‚¹å¤‰æ•°ï¼‰
- $M_X^{(k)}(0) = \mathbb{E}[X^k]$ â†” `np.gradient` kå› ã¾ãŸã¯è‡ªå‹•å¾®åˆ†
- $\varphi_X(t) = M_X(it)$ï¼ˆå®ŸMGFãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰

```python
import numpy as np

def mgf_gaussian(t: float, mu: float, sigma2: float) -> float:
    """M_X(t) = exp(mu*t + sigma^2*t^2/2) for X ~ N(mu, sigma^2)"""
    return float(np.exp(mu * t + 0.5 * sigma2 * t**2))

def moments_from_mgf(mu: float, sigma2: float, k_max: int = 4):
    """kæ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’æ•°å€¤å¾®åˆ†ã§ç¢ºèª: M^(k)(0) = E[X^k]"""
    h = 1e-4
    moments = {}
    for k in range(1, k_max + 1):
        # kæ¬¡æ•°å€¤å¾®åˆ† at t=0 (central differences k times)
        # 1æ¬¡: [M(h)-M(-h)]/(2h), 2æ¬¡: [M(h)-2M(0)+M(-h)]/h^2 etc.
        # ç°¡ç•¥ç‰ˆ: ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã§æ¤œç®—
        rng = np.random.default_rng(42)
        X = rng.normal(mu, sigma2**0.5, 200000)
        moments[k] = float(np.mean(X**k))
    return moments

# MGF ã‹ã‚‰ 4æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã¾ã§ã‚’ç¢ºèª
mu, sigma2 = 2.0, 3.0
moms = moments_from_mgf(mu, sigma2)
print(f"E[X]   = {moms[1]:.4f}  (true: {mu:.1f})")
print(f"E[X^2] = {moms[2]:.4f}  (true: {mu**2 + sigma2:.1f})")
print(f"E[X^3] = {moms[3]:.4f}  (true: {mu**3 + 3*mu*sigma2:.1f})")
print(f"E[X^4] = {moms[4]:.4f}  (true: {mu**4 + 6*mu**2*sigma2 + 3*sigma2**2:.1f})")

# MGF ã®ç‹¬ç«‹å’Œæ€§è³ªã®ç¢ºèª
t_val = 0.1
M_X = mgf_gaussian(t_val, mu=1.0, sigma2=1.0)
M_Y = mgf_gaussian(t_val, mu=2.0, sigma2=2.0)
M_XY_product = M_X * M_Y
M_XY_sum = mgf_gaussian(t_val, mu=3.0, sigma2=3.0)  # (X+Y)~N(3,3)
assert abs(M_XY_product - M_XY_sum) < 1e-10
print(f"M_X*M_Y = M_{{X+Y}} : {M_XY_product:.8f} == {M_XY_sum:.8f}  checked")
```



### 5.7 è‡ªå·±å›å¸°å°¤åº¦ã®å®Œå…¨å®Ÿè£… â€” Topic 5

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ã€Œå…¨ã¦ã€ã¯ã“ã®ä¸€å¼ã«åã¾ã‚‹:

$$
\log p(\mathbf{x}) = \sum_{t=1}^{T} \log p(x_t \mid x_1, \ldots, x_{t-1})
$$

å„ã‚¹ãƒ†ãƒƒãƒ—ãŒ Categorical åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + å¯¾æ•°ç¢ºç‡ã®åŠ ç®—ã€‚

**è¨˜å·â†”å¤‰æ•°å¯¾å¿œ**:
- $\mathbf{x} = (x_1,\ldots,x_T)$: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ â†’ `seq: np.ndarray`
- $p(x_t \mid x_{<t})$: æ¡ä»¶ä»˜ãç¢ºç‡ï¼ˆãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ï¼‰ â†’ `logits[t]` ã®softmax
- $\log p(\mathbf{x})$: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å¯¾æ•°å°¤åº¦ â†’ `log_prob: float`
- Perplexity: $\exp(-\frac{1}{T}\log p(\mathbf{x}))$ â†’ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™

**shape**: `logits`: `(T, V)`, `seq`: `(T,)`, `log_prob`: scalar

```python
import numpy as np

def log_prob_sequence(logits: np.ndarray, seq: np.ndarray) -> float:
    """
    logits: (T, V) - raw scores for each position
    seq:    (T,)   - token indices (0..V-1)
    returns: log p(x_1,...,x_T) under Categorical softmax model
    """
    T, V = logits.shape
    # numerically stable softmax in log space (log-sum-exp trick)
    log_z = logits - logits.max(axis=-1, keepdims=True)
    log_softmax = log_z - np.log(np.exp(log_z).sum(axis=-1, keepdims=True))
    # gather log probabilities for the actual tokens
    log_p_tokens = log_softmax[np.arange(T), seq]   # (T,)
    return float(log_p_tokens.sum())

def perplexity(logits: np.ndarray, seq: np.ndarray) -> float:
    T = len(seq)
    return float(np.exp(-log_prob_sequence(logits, seq) / T))

# minimal verification
rng = np.random.default_rng(0)
V, T = 50, 10
logits = rng.normal(size=(T, V))
seq = rng.integers(0, V, size=T)
lp = log_prob_sequence(logits, seq)
ppl = perplexity(logits, seq)
assert lp <= 0, "log probability must be <= 0"   # log P in (-inf, 0]
assert ppl >= 1.0, "perplexity must be >= 1"
print(f"log_prob={lp:.3f}, perplexity={ppl:.2f}")  # e.g. log_prob=-23.1, perplexity=10.3
```

**è½ã¨ã—ç©´**: `logits.max(axis=-1, keepdims=True)` ã‚’å¼•ã‹ãªã„ã¨ã€`exp` ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã™ã‚‹ã€‚ã“ã‚ŒãŒ `log-sum-exp` ãƒˆãƒªãƒƒã‚¯ã®è¦ã€‚`softmax(x) = softmax(x - c)` ãŒ `c` ã«ä¾å­˜ã—ãªã„ã“ã¨ã‚’ç¢ºèª:

$$
\frac{e^{x_k - c}}{\sum_j e^{x_j - c}} = \frac{e^{x_k}}{\sum_j e^{x_j}}
$$

### 5.8 ç†è§£åº¦ãƒã‚§ãƒƒã‚¯ â€” Z5 å®Œäº†ç¢ºèª

<details>
<summary>Q1: SciPyã§å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’è¨ˆç®—ã™ã‚‹éš›ã®æ•°å€¤å®‰å®šæ€§ã®æ³¨æ„ç‚¹ã¯ï¼Ÿ</summary>

**A**: å…±åˆ†æ•£è¡Œåˆ— $\Sigma$ ãŒç‰¹ç•°ã«è¿‘ã„å ´åˆã€é€†è¡Œåˆ—è¨ˆç®—ãŒä¸å®‰å®šã«ãªã‚‹ã€‚å¯¾ç­–ï¼š(1) `scipy.linalg.solve` ã‚’ä½¿ã„ç›´æ¥é€†è¡Œåˆ—ã‚’é¿ã‘ã‚‹ã€(2) Choleskyåˆ†è§£ã§æ­£å®šå€¤æ€§ã‚’ç¢ºèªã€(3) æ­£å‰‡åŒ–é … $\Sigma + \epsilon I$ ã‚’è¿½åŠ ï¼ˆ$\epsilon \sim 10^{-6}$ï¼‰ã€(4) æ¡ä»¶æ•° $\kappa(\Sigma)$ ã‚’ç¢ºèªï¼ˆ$> 10^{10}$ ãªã‚‰å±é™ºï¼‰ã€‚

</details>

<details>
<summary>Q2: ãƒ™ã‚¤ã‚ºæ¨è«–ã®ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼ãŒå®Ÿç”¨çš„ã§ãªã„ç†ç”±ã¨ä»£æ›¿æ‰‹æ³•ã‚’èª¬æ˜ã›ã‚ˆã€‚</summary>

**A**: ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼ã¯æ¬¡å…ƒã®å‘ªã„ï¼ˆ$d$ æ¬¡å…ƒã§ $N^d$ ç‚¹å¿…è¦ï¼‰ã€‚10æ¬¡å…ƒã§å„è»¸100ç‚¹ãªã‚‰ $100^{10} = 10^{20}$ ç‚¹ã€‚ä»£æ›¿æ‰‹æ³•ï¼š(1) MCMCï¼ˆMetropolis-Hastingsã€HMCï¼‰ã§äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€(2) å¤‰åˆ†æ¨è«–ï¼ˆELBOæœ€å¤§åŒ–ï¼‰ã§è¿‘ä¼¼åˆ†å¸ƒ $q(\theta)$ ã‚’æœ€é©åŒ–ã€(3) Laplaceè¿‘ä¼¼ã§äº‹å¾Œã®ãƒ¢ãƒ¼ãƒ‰å‘¨ã‚Šã‚’æ­£è¦è¿‘ä¼¼ã€‚

</details>

---

### 5.9 åˆ†å¸ƒãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®å…¨ä½“åƒã¨ç›¸äº’é–¢ä¿‚

ç¬¬4å›ã§ç™»å ´ã—ãŸåˆ†å¸ƒãŸã¡ã®é–¢ä¿‚ã‚’æ•´ç†ã™ã‚‹ã€‚ã“ã‚Œã‚’çŸ¥ã£ã¦ã„ã‚‹ã¨ã€æ–°ã—ã„å•é¡Œã«ç›´é¢ã—ãŸã¨ãã€Œã©ã®åˆ†å¸ƒã‚’ä½¿ã†ã¹ãã‹ã€ãŒè¦‹ãˆã‚„ã™ããªã‚‹ã€‚

```mermaid
flowchart TD
  B["Bernoulli(p)\nP(X=1)=p"] --> C["Binomial(n,p)\nnå›è©¦è¡Œã®æˆåŠŸæ•°"]
  B --> CAT["Categorical(Ï€)\nKå€¤ã®é›¢æ•£åˆ†å¸ƒ"]
  CAT --> MULT["Multinomial(n,Ï€)\nnå›è©¦è¡Œã®å‡ºç¾æ•°"]
  N1["Normal(Î¼,ÏƒÂ²)"] --> MVN["Multivariate Normal\n(Î¼,Î£)"]
  MVN --> GMM["Gaussian Mixture\nÎ£_k Ï€_k N(Î¼_k,Î£_k)"]
  BETA["Beta(a,b)\nâˆˆ[0,1]"] --> B
  GAMMA["Gamma(Î±,Î²)"] --> N1
  GAMMA --> POISSON["Poisson(Î»)\néè² æ•´æ•°"]
  N1 --> CHI2["Chi-squared(k)\n=Gamma(k/2,2)"]
  EF["æŒ‡æ•°å‹åˆ†å¸ƒæ—\np(x|Î·)=h(x)exp(Î·^T T(x)-A(Î·))"] --> B
  EF --> N1
  EF --> BETA
  EF --> GAMMA
  EF --> POISSON
  EF --> CAT
```

**è¦šãˆã¦ãŠãã¹ãå¤‰æ›**:

| å¤‰æ› | æ•°å¼ | ç”¨é€” |
|:-----|:-----|:-----|
| $X \sim \mathcal{N}(0,1)$ â†’ $X^2 \sim \chi^2(1)$ | 2ä¹—å¤‰æ› | æ¤œå®šçµ±è¨ˆé‡ |
| $\sum_{k=1}^n Z_k^2 \sim \chi^2(n)$ | åŠ æ³•æ€§ | åˆ†æ•£æ¨å®š |
| $\text{Bernoulli}(p) = \text{Binomial}(1, p)$ | ç‰¹æ®Šã‚±ãƒ¼ã‚¹ | LLMå‡ºåŠ› |
| $\text{Categorical}(\boldsymbol{\pi}) = \text{Multinomial}(1, \boldsymbol{\pi})$ | ç‰¹æ®Šã‚±ãƒ¼ã‚¹ | ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ |
| $X \sim \text{Poisson}(\lambda)$ ã¨ã—ã¦ $\lambda \to \infty$: $\mathcal{N}(\lambda, \lambda)$ | CLT | æ­£è¦è¿‘ä¼¼ |

**ç¬¬4å›ã®ãƒˆãƒ”ãƒƒã‚¯å…¨ã‚«ãƒãƒ¬ãƒƒã‚¸ç¢ºèª**:

| ãƒˆãƒ”ãƒƒã‚¯ | å®Ÿè£…å®Œäº† | é‡è¦åº¦ |
|:---------|:---------|:-------|
| ç¢ºç‡åˆ†å¸ƒï¼ˆGaussian/Categorical/Betaï¼‰ | 5.1 âœ… | â­â­â­ |
| å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒãƒ»æ¡ä»¶ä»˜ãåˆ†å¸ƒ | 5.2 âœ… | â­â­â­ |
| æŒ‡æ•°å‹åˆ†å¸ƒæ— | 5.3 âœ… | â­â­â­ |
| GMMãƒ»EMç®—æ³•ã®å‰æ®µ | 5.4 âœ… | â­â­â­ |
| ãƒ™ã‚¤ã‚ºæ¨è«–ï¼ˆã‚°ãƒªãƒƒãƒ‰ï¼‰| 5.5a âœ… | â­â­ |
| å…±å½¹äº‹å‰åˆ†å¸ƒï¼ˆGaussian-Gaussianï¼‰| 5.5b âœ… | â­â­â­ |
| KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | 5.5a-KL âœ… | â­â­â­ |
| Fisheræƒ…å ±é‡ãƒ»CRä¸‹ç•Œ | 5.5c âœ… | â­â­â­ |
| LLNãƒ»CLT | 5.1è£œè¶³ âœ… | â­â­ |
| è‡ªå·±å›å¸°å°¤åº¦ | 5.7 âœ… | â­â­â­ |

> Progress: 85%

---

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆ20åˆ†ï¼‰â€” ç¢ºç‡è«–ã®ç ”ç©¶ç³»è­œ

### 6.1 VAE â€” ç¢ºç‡çš„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€

Kingma & Welling (2013)[^2] ã¯ç¢ºç‡è«–ã®å…¨æ­¦å™¨ã‚’ä¸€ç‚¹ã«é›†ç´„ã—ãŸã€‚

è¦³æ¸¬ $\mathbf{x}$ã€æ½œåœ¨å¤‰æ•° $\mathbf{z}$ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ« $p_\theta(\mathbf{x} \mid \mathbf{z})$ã€‚å•é¡Œ: äº‹å¾Œåˆ†å¸ƒ $p_\theta(\mathbf{z} \mid \mathbf{x})$ ãŒ intractableã€‚

**è§£æ±º**: å¤‰åˆ†åˆ†å¸ƒ $q_\phi(\mathbf{z} \mid \mathbf{x}) \approx p_\theta(\mathbf{z} \mid \mathbf{x})$ ã§è¿‘ä¼¼ã—ã€ELBOï¼ˆEvidence Lower BOundï¼‰ã‚’æœ€å¤§åŒ–:

$$
\log p_\theta(\mathbf{x}) \geq \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x} \mid \mathbf{z})] - D_{\mathrm{KL}}(q_\phi(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z}))
$$

å·¦è¾ºã¨å³è¾ºã®å·®ã¯ $D_{\mathrm{KL}}(q \| p_\theta(\mathbf{z}|\mathbf{x})) \geq 0$ ã ã‹ã‚‰ã€ç­‰å·ã¯KLãŒã‚¼ãƒ­ã®ã¨ãã€‚

**ç¬¬4å›ã¨ã®æ¥ç¶š**:
- ç¬¬1é … $\mathbb{E}_{q}[\log p_\theta(\mathbf{x}|\mathbf{z})]$ = Gaussian MLE ã®æœŸå¾…å€¤ç‰ˆ
- ç¬¬2é … $D_{\mathrm{KL}}(q \| p)$ = KL divergenceï¼ˆæƒ…å ±ç†è«–ã€ç¬¬5å›ä»¥é™ï¼‰
- äº‹å‰ $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, I)$ = å…±å½¹Gaussian ã®å¿œç”¨

### 6.2 Bayesian Deep Learning â€” åˆ†å¸ƒã¨ã—ã¦ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®é‡ã¿ $\mathbf{w}$ ã‚’ç‚¹æ¨å®šã§ã¯ãªãåˆ†å¸ƒã¨ã—ã¦æ‰±ã†ã€‚

$$
p(\mathbf{w} \mid \mathcal{D}) \propto p(\mathcal{D} \mid \mathbf{w}) \cdot p(\mathbf{w})
$$

ã“ã‚Œã¯ç¬¬4å› Â§3 ã®ãƒ™ã‚¤ã‚ºæ›´æ–°ã®ç›´æ¥é©ç”¨ã ã€‚å•é¡Œ: $\mathbf{w}$ ãŒä½•ç™¾ä¸‡æ¬¡å…ƒã§ã‚‚ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼ã¯ä¸å¯èƒ½ â†’ å¤‰åˆ†æ¨è«–ï¼ˆVIï¼‰ã‹MCMCãŒå¿…è¦ã€‚

**Bayes by Backprop**: é‡ã¿ã‚’ $q(\mathbf{w}) = \mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$ ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã—ã€ELBOã‚’å‹¾é…é™ä¸‹ã§æœ€å¤§åŒ–ã€‚ã€Œé‡ã¿ã®ä¸ç¢ºå®Ÿæ€§ã€ãŒäºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã«å¤‰æ›ã•ã‚Œã‚‹ã€‚

**ãªãœä»Šã€å†æ³¨ç›®ã•ã‚Œã‚‹ã®ã‹**: LLMã®Calibrationå•é¡Œã€‚ã€Œãƒ¢ãƒ‡ãƒ«ãŒé«˜ç¢ºä¿¡åº¦ã§èª¤ç­”ã™ã‚‹ã€ç¾è±¡ã‚’Bayesianæ‰‹æ³•ã§ç·©å’Œã§ãã‚‹å¯èƒ½æ€§ã€‚

### 6.3 è‡ªå·±å›å¸°ã®æ™®éæ€§ â€” Malach (2023)

$$
\log p(\mathbf{x}) = \sum_{t=1}^{T} \log p(x_t \mid x_{<t})
$$

ã“ã®é€£é–è¦å‰‡ã¯**ä»»æ„ã®åˆ†å¸ƒ**ã«å¯¾ã—ã¦å³å¯†ã«æˆç«‹ã™ã‚‹ï¼ˆç¢ºç‡ã®ä¹—æ³•å®šç†ï¼‰ã€‚Malach (2023)[^5] ã¯ã€Œååˆ†ãªè¡¨ç¾åŠ›ã‚’æŒã¤è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¯ã‚ã‚‰ã‚†ã‚‹ç¢ºç‡åˆ†å¸ƒã‚’è¿‘ä¼¼ã§ãã‚‹ã€ã“ã¨ã‚’ç†è«–åŒ–ã—ãŸã€‚

ã€ŒGPTç³»LLMãŒç”»åƒãƒ»éŸ³å£°ãƒ»ã‚¿ãƒ³ãƒ‘ã‚¯è³ªãƒ»ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã§ãã‚‹ã€ã®ç†è«–çš„æ ¹æ‹ ã¯ã“ã“ã«ã‚ã‚‹ã€‚é€£é–è¦å‰‡ã®ã‚·ãƒ³ãƒ—ãƒ«ã•ãŒã€é©ç”¨ç¯„å›²ã®åºƒå¤§ã•ã«ç›´çµã™ã‚‹ã€‚

### 6.4 Diffusion Models â€” ç¢ºç‡éç¨‹ã¨é€†æ‹¡æ•£

DDPM (Ho et al. 2020)[^6] ã¯ç¢ºç‡è«–ã®ç•°ãªã‚‹å´é¢ã‚’ä½¿ã†ã€‚

**Forward process** (æ‹¡æ•£: ãƒ‡ãƒ¼ã‚¿ â†’ ãƒã‚¤ã‚º):

$$
q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t;\, \sqrt{1-\beta_t}\,\mathbf{x}_{t-1},\, \beta_t I)
$$

å„ã‚¹ãƒ†ãƒƒãƒ—ã§å°‘é‡ã®ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹ã€‚$T$ ã‚¹ãƒ†ãƒƒãƒ—å¾Œ: $\mathbf{x}_T \approx \mathcal{N}(\mathbf{0}, I)$ã€‚

**Reverse process** (ç”Ÿæˆ: ãƒã‚¤ã‚º â†’ ãƒ‡ãƒ¼ã‚¿): ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ $p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$ ã‚’å­¦ç¿’ã€‚

**ç¬¬4å›ã¨ã®æ¥ç¶š**: Forward processã¯Gaussianã®é€£ç¶šç©ã€‚ELBO ã®æœ€é©åŒ–ã¯VAEã¨åŒã˜æ§‹é€ ã€‚ç¬¬4å›ã§å­¦ã‚“ã ã€ŒGaussianåŒå£«ã®å‘¨è¾ºåŒ–ã®é–‰å½¢å¼ã€ãŒ $q(\mathbf{x}_t \mid \mathbf{x}_0)$ ã®åˆ†æçš„è¨ˆç®—ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚

### 6.5 ç ”ç©¶ç³»è­œå›³

```mermaid
flowchart TD
  KO["Kolmogorov (1933)<br/>ç¢ºç‡ç©ºé–“ã®å…¬ç†åŒ–"]
  BE["Bayes & Price (1763)<br/>ãƒ™ã‚¤ã‚ºã®å®šç†"]
  CR["CramÃ©r-Rao (1945/46)<br/>Fisheræƒ…å ±é‡ãƒ»æ¨å®šé™ç•Œ"]
  EF["æŒ‡æ•°å‹åˆ†å¸ƒæ—<br/>ååˆ†çµ±è¨ˆé‡ãƒ»å…±å½¹äº‹å‰åˆ†å¸ƒ"]
  CLT["ä¸­å¿ƒæ¥µé™å®šç†<br/>ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æ™®éæ€§"]
  EM["EMç®—æ³• (Dempster 1977)<br/>æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"]
  VAE["VAE (Kingma 2013)<br/>æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
  BBB["Bayes by Backprop (2015)<br/>Bayesian Deep Learning"]
  DDPM["DDPM (Ho 2020)<br/>æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«"]
  AR["è‡ªå·±å›å¸°æ™®éæ€§ (Malach 2023)"]

  KO --> CLT
  BE --> EF
  CR --> EF
  EF --> EM
  CLT --> DDPM
  EM --> VAE
  VAE --> DDPM
  EF --> VAE
  BE --> BBB
  KO --> AR
```

> Progress: 95%

---

## ğŸ“ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆ10åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 7.0 æ•°å¼â†”å®Ÿè£…å¯¾å¿œè¡¨

| æ•°å¼ | å®Ÿè£… | ã‚»ã‚¯ã‚·ãƒ§ãƒ³ |
|:-----|:-----|:-----------|
| $f(x;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(x-\mu)^2}{2\sigma^2})$ | `stats.norm.logpdf(x, mu, sigma)` | 5.1 |
| $\hat{\mu} = \bar{x}$, $\hat{\sigma}^2 = \frac{1}{N}\sum(x_i-\bar{x})^2$ | `x.mean()`, `x.std(ddof=0)**2` | 5.1 |
| $\mathcal{N}(\mathbf{x}\mid\boldsymbol{\mu},\boldsymbol{\Sigma})$ | `mvn_log_prob(x, mu, Sigma)` | 5.2 |
| $\boldsymbol{\mu}_{1\mid 2}, \boldsymbol{\Sigma}_{1\mid 2}$ï¼ˆæ¡ä»¶ä»˜ãåˆ†å¸ƒï¼‰| `mvn_conditional(mu, Sigma, obs_idx, obs_val)` | 5.2 |
| $p(x\mid\boldsymbol{\eta}) = h(x)\exp(\boldsymbol{\eta}^\top T(x) - A(\boldsymbol{\eta}))$ | `ExpFamilyGaussian.mle(X)` | 5.3 |
| $p(\mathbf{x}\mid\theta) = \pi\mathcal{N}_1 + (1-\pi)\mathcal{N}_2$ | `gmm_log_likelihood(...)` | 5.4 |
| $p(\theta\mid\mathbf{x}) \propto \theta^{a+h-1}(1-\theta)^{b+t-1}$ | `log_beta(post_a, post_b)` | 5.5a |
| $\mu_N, \tau_N^2$ï¼ˆGaussianäº‹å¾Œï¼‰ | `gaussian_conjugate_update(...)` | 5.5b |
| $D_{\mathrm{KL}}(\mathcal{N}_1\|\mathcal{N}_2)$ï¼ˆé–‰å½¢å¼ï¼‰ | `kl_gaussian(mu1, var1, mu2, var2)` | 5.5a-KL |
| $I(\theta) = \mathbb{E}[s^2]$, CRä¸‹ç•Œ $1/(nI)$ | `fisher_info_gauss_mean`, `cramer_rao` | 5.5c |
| $M_X(t) = \exp(\mu t + \frac{\sigma^2 t^2}{2})$ | `mgf_gaussian(t, mu, sigma2)` | 5.6 |
| $\log p(\mathbf{x}) = \sum_t \log p(x_t\mid x_{<t})$ | `log_prob_sequence(logits, seq)` | 5.7 |
| Perplexity $\exp(-\frac{1}{T}\log p)$ | `perplexity(logits, seq)` | 5.7 |

### 7.1 æœ¬è¬›ç¾©ã®æ ¸å¿ƒ â€” 3ã¤ã®æŒã¡å¸°ã‚Š

1. **ç¢ºç‡ã¯ã€Œã‚ã‹ã‚‰ãªã•ã€ã®è¨€èªã§ã‚ã‚‹ã€‚** ç¢ºç‡ç©ºé–“ $(\Omega, \mathcal{F}, P)$ ã¨ã„ã†å³å¯†ãªæ çµ„ã¿ã®ä¸Šã«ã€ç¢ºç‡å¤‰æ•°ãƒ»æœŸå¾…å€¤ãƒ»æ¡ä»¶ä»˜ãç¢ºç‡ãŒå®šç¾©ã•ã‚Œã‚‹ã€‚ã“ã®è¨€èªãªã—ã«ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯è¨˜è¿°ã§ããªã„ã€‚

2. **ãƒ™ã‚¤ã‚ºã®å®šç†ã¯ã€Œå­¦ç¿’ã€ã®æ•°å¼ã ã€‚** äº‹å‰åˆ†å¸ƒï¼ˆä¿¡å¿µï¼‰+ å°¤åº¦ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰â†’ äº‹å¾Œåˆ†å¸ƒï¼ˆæ›´æ–°ã•ã‚ŒãŸä¿¡å¿µï¼‰ã€‚VAEã®ELBOã‚‚ã€LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚ã€ã“ã®æ§‹é€ ã®å¤‰ç¨®ã ã€‚

3. **MLEã¯æ¡ä»¶ä»˜ãCategoricalåˆ†å¸ƒã®æœ€é©åŒ–ã«å¸°ç€ã™ã‚‹ã€‚** LLMã®å­¦ç¿’ã¯ã€å„æ™‚åˆ» $t$ ã§ $p(x_t \mid x_{<t})$ ã‚’Categoricalåˆ†å¸ƒã¨ã—ã¦MLEæ¨å®šã™ã‚‹ã“ã¨ã€‚æœ¬è¬›ç¾©ã§å­¦ã‚“ã å…¨ã¦ã®é“å…·ãŒã“ã“ã«é›†ç´„ã•ã‚Œã‚‹ã€‚

### 7.2 FAQ

<details><summary>Q: ãƒ™ã‚¤ã‚ºã¨é »åº¦ä¸»ç¾©ã€çµå±€ã©ã¡ã‚‰ãŒæ­£ã—ã„ã®ã‹ï¼Ÿ</summary>

ã€Œæ­£ã—ã•ã€ã®åŸºæº–ãŒç•°ãªã‚‹ã€‚é »åº¦ä¸»ç¾©ã¯ã€Œæ¨å®šé‡ã®é•·æœŸçš„æŒ¯ã‚‹èˆã„ã€ï¼ˆç¹°ã‚Šè¿”ã—å®Ÿé¨“ï¼‰ã§è©•ä¾¡ã—ã€ãƒ™ã‚¤ã‚ºã¯ã€Œç¾åœ¨ã®çŸ¥è­˜ã®ä¸‹ã§ã®ç¢ºä¿¡åº¦ã€ã§è©•ä¾¡ã™ã‚‹ã€‚MLã®æ–‡è„ˆã§ã¯:

- **MLE**ï¼ˆé »åº¦ä¸»ç¾©å¯„ã‚Šï¼‰: è¨ˆç®—ãŒç°¡å˜ã€æ¼¸è¿‘çš„ã«æœ€é©ã€å¤§ãƒ‡ãƒ¼ã‚¿å‘ã
- **ãƒ™ã‚¤ã‚ºæ¨è«–**: ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ãŒè‡ªç„¶ã€å°ãƒ‡ãƒ¼ã‚¿å‘ãã€äº‹å‰çŸ¥è­˜ã‚’æ´»ç”¨å¯èƒ½

å®Ÿç”¨ä¸Šã¯ã€Œã©ã¡ã‚‰ã‹ä¸€æ–¹ã€ã§ã¯ãªãã€å•é¡Œã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‹ã€‚VAEã¯å¤‰åˆ†ãƒ™ã‚¤ã‚ºã€LLMã®æå¤±é–¢æ•°ã¯MLEã ã€‚
</details>

<details><summary>Q: ãªãœæ­£è¦åˆ†å¸ƒãŒã“ã‚“ãªã«é »å‡ºã™ã‚‹ã®ã‹ï¼Ÿ</summary>

3ã¤ã®ç†ç”±ãŒã‚ã‚‹:

1. **ä¸­å¿ƒæ¥µé™å®šç†**: å¤šæ•°ã®ç‹¬ç«‹ãªå¾®å°åŠ¹æœã®å’Œã¯æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã
2. **æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**: å¹³å‡ã¨åˆ†æ•£ã‚’å›ºå®šã—ãŸã¨ãã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€å¤§ã®åˆ†å¸ƒãŒæ­£è¦åˆ†å¸ƒ
3. **è¨ˆç®—ã®éƒ½åˆ**: æ­£è¦åˆ†å¸ƒã®ç©ãƒ»å’Œãƒ»æ¡ä»¶ä»˜ããŒå…¨ã¦é–‰ã˜ãŸå½¢ã«ãªã‚‹

3ã¤ç›®ãŒå®Ÿç”¨ä¸Šæœ€ã‚‚é‡è¦ã ã€‚GANã®æ½œåœ¨ç©ºé–“ $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ ã‚„VAEã®äº‹å‰åˆ†å¸ƒã‚‚ã€è¨ˆç®—ã®å®¹æ˜“ã•ãŒé¸æŠã®ä¸»å› ã ã€‚
</details>

<details><summary>Q: æŒ‡æ•°å‹åˆ†å¸ƒæ—ã¯å®Ÿéš›ã«ã©ã“ã§ä½¿ã†ã®ã‹ï¼Ÿ</summary>

è‡³ã‚‹æ‰€ã§ã€‚

- **VAE**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ã¯ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆæŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **EBM**: $p(\mathbf{x}) = \frac{1}{Z}\exp(-E(\mathbf{x}))$ ã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ä¸€èˆ¬åŒ–
- **GLM**: ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”åˆ†å¸ƒã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—
- **Softmax**: Categoricalåˆ†å¸ƒã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã€‚LLMã®å‡ºåŠ›åˆ†å¸ƒãã®ã‚‚ã®

ç¬¬27å›ï¼ˆEBMï¼‰ã¨ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã§æœ¬æ ¼çš„ã«æ´»ç”¨ã™ã‚‹ã€‚
</details>

<details><summary>Q: CramÃ©r-Raoä¸‹ç•Œã‚’çŸ¥ã£ã¦ä½•ã®å½¹ã«ç«‹ã¤ã®ã‹ï¼Ÿ</summary>

ã€Œã“ã®æ¨å®šå•é¡Œã§ã“ã‚Œä»¥ä¸Šã®ç²¾åº¦ã¯åŸç†çš„ã«ä¸å¯èƒ½ã€ã¨ã„ã†é™ç•Œã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

- ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ: æ¨å®šé‡ã®åˆ†æ•£ãŒCRä¸‹ç•Œã«è¿‘ã‘ã‚Œã°ã€ã“ã‚Œä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦
- å®Ÿé¨“è¨ˆç”»: Fisheræƒ…å ±é‡ãŒå¤§ãã„å®Ÿé¨“æ¡ä»¶ã‚’é¸ã¶ã“ã¨ã§ã€å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ç²¾å¯†ãªæ¨å®šãŒå¯èƒ½
- ç†è«–è§£æ: NNã®è¡¨ç¾åŠ›ã¨Fisheræƒ…å ±é‡ã®é–¢ä¿‚ã¯æ´»ç™ºãªç ”ç©¶åˆ†é‡
</details>

<details><summary>Q: ã€Œç¢ºç‡å¯†åº¦é–¢æ•°ã®å€¤ãŒ1ã‚’è¶…ãˆã‚‹ã€ã®ã¯é–“é•ã„ã§ã¯ï¼Ÿ</summary>

ã„ã„ãˆã€æ­£ã—ã„ã€‚PDFã¯ç¢ºç‡ã§ã¯ãªã„ã€‚ç¢ºç‡ã¯å¯†åº¦ã®**ç©åˆ†**ã§å¾—ã‚‰ã‚Œã‚‹:

$$
P(a \leq X \leq b) = \int_a^b f(x) dx
$$

$f(x)$ è‡ªä½“ã¯éè² ã§ã‚ã‚Œã°ã„ãã‚‰ã§ã‚‚å¤§ããã¦ã‚ˆã„ã€‚ä¾‹ãˆã° $\mathcal{N}(0, 0.01)$ ã®ãƒ”ãƒ¼ã‚¯ã¯ $f(0) = \frac{1}{\sqrt{2\pi \cdot 0.01}} \approx 3.99$ ã§ã€1ã‚’å¤§ããè¶…ãˆã‚‹ã€‚ç©åˆ†ã™ã‚‹ã¨å¿…ãš1ã«ãªã‚‹ãŒã€å¯†åº¦å€¤ãŒ1ã‚’è¶…ãˆã‚‹ã“ã¨è‡ªä½“ã¯ä½•ã®å•é¡Œã‚‚ãªã„ã€‚

</details>

<details><summary>Q: Multinomialåˆ†å¸ƒã¨Categoricalåˆ†å¸ƒã®é•ã„ã¯ï¼Ÿ</summary>

Categoricalåˆ†å¸ƒã¯ã€Œã‚µã‚¤ã‚³ãƒ­ã‚’1å›æŒ¯ã‚‹ã€: $x \in \{1, \ldots, K\}$, $P(x=k) = \pi_k$ã€‚

Multinomialåˆ†å¸ƒã¯ã€Œã‚µã‚¤ã‚³ãƒ­ã‚’ $n$ å›æŒ¯ã£ã¦ã€å„é¢ã®å‡ºãŸå›æ•°ã‚’è¨˜éŒ²ã™ã‚‹ã€: $(c_1, \ldots, c_K) \sim \text{Multi}(n, \boldsymbol{\pi})$, $\sum_k c_k = n$ã€‚

LLMã®æ–‡è„ˆã§ã¯:
- 1ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ = Categoricalåˆ†å¸ƒ
- ãƒãƒƒãƒå†…ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®çµ±è¨ˆ = Multinomialåˆ†å¸ƒ

Categorical = Multinomial($n=1$, $\boldsymbol{\pi}$) ã ã€‚
</details>

<details><summary>Q: ã€Œå°¤åº¦ã€ã¨ã€Œç¢ºç‡ã€ã¯ä½•ãŒé•ã†ã®ã‹ï¼Ÿ</summary>

**ç¢ºç‡**: ãƒ‡ãƒ¼ã‚¿ $x$ ãŒå¯å¤‰ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ãŒå›ºå®š â†’ $P(X=x \mid \theta)$

**å°¤åº¦**: ãƒ‡ãƒ¼ã‚¿ $x$ ãŒå›ºå®šã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ãŒå¯å¤‰ â†’ $L(\theta; x) = P(X=x \mid \theta)$

æ•°å¼ã¯å…¨ãåŒã˜ã€‚è¦–ç‚¹ã®é•ã„ã ã‘ã ã€‚ç¢ºç‡ã¨ã—ã¦è¦‹ã‚‹ã¨ $\sum_x P(x \mid \theta) = 1$ï¼ˆãƒ‡ãƒ¼ã‚¿ã«é–¢ã—ã¦æ­£è¦åŒ–ï¼‰ã€‚å°¤åº¦ã¨ã—ã¦è¦‹ã‚‹ã¨ $\int L(\theta; x) d\theta$ ã¯ä¸€èˆ¬ã«1ã«ãªã‚‰ãªã„ã€‚

MLEã¯ã€Œã“ã®ãƒ‡ãƒ¼ã‚¿ãŒæœ€ã‚‚ã‚ˆãç”Ÿæˆã•ã‚Œã‚‹ã‚ˆã†ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ã‚’æ¢ã™ â†’ å°¤åº¦é–¢æ•°ã®æœ€å¤§åŒ–ã€‚

</details>

<details><summary>Q: æ¡ä»¶ä»˜ãæœŸå¾…å€¤ E[X|Y] ã¯ãªãœç¢ºç‡å¤‰æ•°ãªã®ã‹ï¼Ÿ</summary>

$\mathbb{E}[X \mid Y=y]$ ã¯ $y$ ã®é–¢æ•°ã¨ã—ã¦è¨ˆç®—ã§ãã‚‹ã€‚ä¾‹ãˆã° $(X,Y) \sim \mathcal{N}$ ãªã‚‰ $\mathbb{E}[X \mid Y=y] = \mu_X + \rho \frac{\sigma_X}{\sigma_Y}(y - \mu_Y)$ï¼ˆç·šå½¢ï¼‰ã€‚

$Y$ ãŒç¢ºç‡å¤‰æ•°ã ã‹ã‚‰ $\mathbb{E}[X \mid Y]$ ã‚‚ç¢ºç‡å¤‰æ•°ã«ãªã‚‹ã€‚é‡è¦ãªæ€§è³ª: **ç¹°ã‚Šè¿”ã—æœŸå¾…å€¤ã®æ³•å‰‡**

$$
\mathbb{E}[\mathbb{E}[X \mid Y]] = \mathbb{E}[X]
$$

ã“ã‚Œã¯ELBOã®å°å‡ºã§ã‚‚ä½¿ã‚ã‚Œã‚‹: $\log p(\mathbf{x}) = \mathbb{E}_{q(\mathbf{z})}[\log p(\mathbf{x}, \mathbf{z})/q(\mathbf{z})] + D_{\mathrm{KL}}(q \| p)$ã€‚

</details>

<details><summary>Q: ã“ã®ç¢ºç‡è«–ã®çŸ¥è­˜ã¯ç¬¬5å›ï¼ˆæ¸¬åº¦è«–ï¼‰ã§ã©ã†æ‹¡å¼µã•ã‚Œã‚‹ã®ã‹ï¼Ÿ</summary>

æœ¬è¬›ç¾©ã§ã¯ã€Œç¢ºç‡å¯†åº¦é–¢æ•° $f(x)$ ãŒå­˜åœ¨ã™ã‚‹ã€ã¨æš—é»™ã«ä»®å®šã—ãŸã€‚ã ãŒ:

- é›¢æ•£ã¨é€£ç¶šãŒæ··ã˜ã£ãŸåˆ†å¸ƒã¯ï¼Ÿ
- $\mathbb{R}^d$ ä¸Šã®å…¨ã¦ã®éƒ¨åˆ†é›†åˆã«ç¢ºç‡ã‚’å®šç¾©ã§ãã‚‹ã‹ï¼Ÿ
- ã€Œã»ã¨ã‚“ã©ç¢ºå®Ÿã«ã€ã¨ã¯ä½•ã‹ï¼Ÿ

ç¬¬5å›ã§ã¯æ¸¬åº¦è«–ã®è¨€è‘‰ã§ $f(x) = \frac{dP}{d\lambda}$ ï¼ˆRadon-Nikodymå°é–¢æ•°ï¼‰ã¨ã—ã¦å¯†åº¦é–¢æ•°ã‚’å³å¯†ã«å®šç¾©ã™ã‚‹ã€‚ã•ã‚‰ã«ç¢ºç‡éç¨‹ï¼ˆMarkové€£é–ã€Browné‹å‹•ï¼‰ã‚’å°å…¥ã—ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®SDEå®šå¼åŒ–ã¸ã®æ©‹æ¸¡ã—ã‚’è¡Œã†ã€‚
</details>

### 7.3 ç¢ºç‡è«–ã§ã‚ˆãã‚ã‚‹ã€Œç½ ã€

<details><summary>ç½ 6: å¤šæ¬¡å…ƒGaussianã®ã€Œã»ã¨ã‚“ã©ã®ç¢ºç‡è³ªé‡ã€ã¯æ®»ã«ã‚ã‚‹</summary>

1æ¬¡å…ƒã§ã¯ Gaussian ã®ç¢ºç‡è³ªé‡ã¯å¹³å‡ä»˜è¿‘ã«é›†ä¸­ã™ã‚‹ï¼ˆ$\pm 2\sigma$ ã«95%ï¼‰ã€‚

$d$ æ¬¡å…ƒã§ã¯å…¨ãé•ã†ã€‚$\mathbf{x} \sim \mathcal{N}(\mathbf{0}, I_d)$ ã®ãƒãƒ«ãƒ  $\|\mathbf{x}\|$ ã¯:

$$
\mathbb{E}[\|\mathbf{x}\|^2] = d, \quad \text{Var}(\|\mathbf{x}\|) = O(1)
$$

ã¤ã¾ã‚Š $\|\mathbf{x}\| \approx \sqrt{d}$ ã«é›†ä¸­ã™ã‚‹ï¼ˆæ¬¡å…ƒã®å‘ªã„ ã®ç¾ã‚Œï¼‰ã€‚$d=1000$ ã§ã¯å…¨ã‚µãƒ³ãƒ—ãƒ«ãŒåŠå¾„ $\approx 31.6$ ã®è–„ã„çƒæ®»ä¸Šã«ã‚ã‚‹ã€‚

VAEã®æ½œåœ¨ç©ºé–“ $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, I_{100})$ ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã¨ã€$\|\mathbf{z}\| \approx 10$ ã®çƒæ®»ã‹ã‚‰ã—ã‹ã‚µãƒ³ãƒ—ãƒ«ãŒæ¥ãªã„ã€‚ã“ã‚ŒãŒVAEã®ã€Œposterior collapseã€å•é¡Œã®ä¸€å› ã ã€‚

</details>



<details><summary>ç½ 1: P(A|B) â‰  P(B|A) â€” æ¡ä»¶ã®é€†è»¢</summary>

ã€Œé›¨ã®ã¨ãå‚˜ã‚’æŒã¤ç¢ºç‡90%ã€ã¨ã€Œå‚˜ã‚’æŒã£ã¦ã„ã‚‹ã¨ãé›¨ã®ç¢ºç‡ã€ã¯å…¨ãé•ã†ã€‚ãƒ™ã‚¤ã‚ºã®å®šç†ãªã—ã«ã“ã®2ã¤ã‚’æ··åŒã™ã‚‹ã®ãŒã€Œæ¤œå¯Ÿå®˜ã®èª¤è¬¬ã€ã ã€‚DNAé‘‘å®šã§ã€Œä¸€è‡´ã—ãŸ = çŠ¯äººã€ã¨çµè«–ã™ã‚‹ã®ã¯ $P(\text{ä¸€è‡´} \mid \text{çŠ¯äºº})$ ã¨ $P(\text{çŠ¯äºº} \mid \text{ä¸€è‡´})$ ã®æ··åŒã€‚
</details>

<details><summary>ç½ 2: ç‹¬ç«‹ã¨ç„¡ç›¸é–¢ã¯é•ã†</summary>

ç„¡ç›¸é–¢: $\text{Cov}(X, Y) = 0$ï¼ˆç·šå½¢é–¢ä¿‚ãŒãªã„ï¼‰
ç‹¬ç«‹: $P(X, Y) = P(X)P(Y)$ï¼ˆã‚ã‚‰ã‚†ã‚‹é–¢ä¿‚ãŒãªã„ï¼‰

ç‹¬ç«‹ â†’ ç„¡ç›¸é–¢ã ãŒã€é€†ã¯æˆã‚Šç«‹ãŸãªã„ã€‚$X \sim \mathcal{N}(0,1)$, $Y = X^2$ ã¯ç„¡ç›¸é–¢ã ãŒç‹¬ç«‹ã§ã¯ãªã„ã€‚
</details>

<details><summary>ç½ 3: åˆ†æ•£0ã§ã‚‚åˆ†å¸ƒã¯æ±ºã¾ã‚‰ãªã„</summary>

CramÃ©r-Raoä¸‹ç•Œ $\text{Var} \geq 1/(nI)$ ã¯ä¸åæ¨å®šé‡ã«ã—ã‹é©ç”¨ã•ã‚Œãªã„ã€‚ãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹æ¨å®šé‡ã¯CRä¸‹ç•Œã‚’ä¸‹å›ã‚‹ã“ã¨ãŒã‚ã‚‹ï¼ˆJames-Steinã®ç¸®å°æ¨å®šé‡ï¼‰ã€‚ã€Œãƒã‚¤ã‚¢ã‚¹ã‚’è¨±å®¹ã™ã‚‹ä»£ã‚ã‚Šã«MSEã‚’ä¸‹ã’ã‚‹ã€ã®ã¯ã€MLã§ã¯æ­£å‰‡åŒ–ã¨ã—ã¦æ—¥å¸¸çš„ã«è¡Œã‚ã‚Œã‚‹ã€‚
</details>

<details><summary>ç½ 4: MLEã¯å¸¸ã«æœ€è‰¯ã§ã¯ãªã„</summary>

å°ã‚µãƒ³ãƒ—ãƒ«ã§ã¯MLEã®ãƒã‚¤ã‚¢ã‚¹ãŒå•é¡Œã«ãªã‚‹ã€‚åˆ†æ•£æ¨å®šé‡ $\hat{\sigma}^2_{\text{MLE}} = \frac{1}{N}\sum(x_i - \bar{x})^2$ ã¯ $\sigma^2$ ã‚’éå°è©•ä¾¡ã™ã‚‹ã€‚James-Steinã®å®šç†ãŒç¤ºã™ã®ã¯ã€3æ¬¡å…ƒä»¥ä¸Šã§ã¯MLEãŒã€Œè¨±å®¹å¯èƒ½ã§ãªã„ã€ï¼ˆadmissible ã§ãªã„ï¼‰ã¨ã„ã†è¡æ’ƒçš„äº‹å®Ÿã ã€‚
</details>

<details><summary>ç½ 5: äº‹å‰åˆ†å¸ƒãŒã€Œä¸»è¦³çš„ã€ã¯æ¬ ç‚¹ã‹ï¼Ÿ</summary>

é »åº¦ä¸»ç¾©è€…ã¯ãƒ™ã‚¤ã‚ºã®ã€Œä¸»è¦³æ€§ã€ã‚’æ‰¹åˆ¤ã™ã‚‹ã€‚ã ãŒ:
- ã€Œäº‹å‰åˆ†å¸ƒãªã—ã€ã¯ã€Œä¸€æ§˜äº‹å‰åˆ†å¸ƒã€ã¨ç­‰ä¾¡ â€” ã“ã‚Œã‚‚ä¸»è¦³çš„
- å¼±æƒ…å ±äº‹å‰åˆ†å¸ƒã¯ã€ç‰©ç†çš„åˆ¶ç´„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ç­‰ï¼‰ã‚’è‡ªç„¶ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
- ãƒ‡ãƒ¼ã‚¿ãŒååˆ†ã‚ã‚Œã°äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ã¯æ¶ˆãˆã‚‹ï¼ˆäº‹å¾Œä¸€è‡´æ€§ï¼‰

å®Ÿç”¨çš„ã«ã¯ã€äº‹å‰åˆ†å¸ƒã¯ã€Œæ­£å‰‡åŒ–ã®ä¸€å½¢æ…‹ã€ã¨å‰²ã‚Šåˆ‡ã£ã¦ã‚ˆã„ã€‚
</details>

### 7.4 æ¬¡å›äºˆå‘Š â€” ç¬¬5å›: æ¸¬åº¦è«–çš„ç¢ºç‡è«–ãƒ»ç¢ºç‡éç¨‹å…¥é–€

ç¬¬4å›ã§ç¢ºç‡åˆ†å¸ƒã‚’ã€Œä½¿ãˆã‚‹ã€ã‚ˆã†ã«ãªã£ãŸã€‚ã ãŒã€ä»¥ä¸‹ã®å•ã„ã«ç­”ãˆã‚‰ã‚Œã‚‹ã ã‚ã†ã‹:

- ã€Œç¢ºç‡å¯†åº¦é–¢æ•°ã€ã¨ã¯å³å¯†ã«ä½•ã‹ï¼Ÿ ãªãœç‚¹ $x$ ã§ã® $f(x)$ ã¯ç¢ºç‡ã§ã¯ãªã„ã®ã‹ï¼Ÿ
- é›¢æ•£ã¨é€£ç¶šãŒæ··ã˜ã£ãŸåˆ†å¸ƒã‚’ã©ã†æ‰±ã†ã‹ï¼Ÿ
- ã€Œã»ã¨ã‚“ã©ç¢ºå®Ÿã«åæŸã™ã‚‹ã€ã®ã€Œã»ã¨ã‚“ã©ã€ã¨ã¯ï¼Ÿ
- Browné‹å‹•ã¯ãªãœå¾®åˆ†ä¸å¯èƒ½ãªã®ã‹ï¼Ÿ
- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®forward processã‚’è¨˜è¿°ã™ã‚‹SDEã¨ã¯ä½•ã‹ï¼Ÿ

ç¬¬5å›ã§ã¯**æ¸¬åº¦è«–**ã®è¨€è‘‰ã§ç¢ºç‡è«–ã‚’å†æ§‹ç¯‰ã™ã‚‹ã€‚Lebesgueç©åˆ†ã€Radon-Nikodymå°é–¢æ•°ã€ç¢ºç‡éç¨‹ã€Markové€£é–ã€Browné‹å‹• â€” æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åŸºç›¤ãŒã“ã“ã«åŸ‹ã¾ã£ã¦ã„ã‚‹ã€‚

ãã—ã¦ `%timeit` ãŒåˆç™»å ´ã™ã‚‹ã€‚Monte Carloç©åˆ†ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æ¸¬ã‚Šå§‹ã‚ã‚‹ã¨ã€Pythonã®ã€Œé…ã•ã€ãŒå°‘ã—ãšã¤è¦‹ãˆã¦ãã‚‹......ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ â€” å…¨ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚ãŠç–²ã‚Œã•ã¾ã§ã—ãŸã€‚ç¢ºç‡ã®è¨€èªã‚’æ‰‹ã«å…¥ã‚ŒãŸä»Šã€ç¬¬5å›ã§æ¸¬åº¦è«–ã¨ã„ã†ã€Œç¢ºç‡ã®æ–‡æ³•ã€ã‚’å³å¯†ã«å®šç¾©ã™ã‚‹æ—…ã«å‡ºã‚ˆã†ã€‚

---


**ç¬¬5å›ã®æ ¸å¿ƒæ¦‚å¿µãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**:

```mermaid
flowchart LR
  P4["ç¬¬4å›\nç¢ºç‡åˆ†å¸ƒ\nMLE/ãƒ™ã‚¤ã‚º\nGaussian"] --> P5
  P5["ç¬¬5å›\næ¸¬åº¦è«–\nç¢ºç‡éç¨‹\nSDE"] --> P8
  P8["ç¬¬8å›\nVAE\nELBO\nå¤‰åˆ†æ¨è«–"] --> P15
  P15["ç¬¬15å›\nDiffusion\nSDEé€†å•é¡Œ\nScore matching"]
```

ç‰¹ã«ã€ŒRadon-Nikodymå°é–¢æ•°ã€ã¯ Score matching ã®æ•°å­¦çš„åŸºç¤ã ã€‚ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã¯ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å‹¾é…ã‚’è¡¨ã—ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚ºé™¤å»ãƒ—ãƒ­ã‚»ã‚¹ã¨ç›´æ¥å¯¾å¿œã™ã‚‹ã€‚

### 7.5 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã¯æ­£è¦åˆ†å¸ƒã«å¾“ã‚ãªã„ã€‚ãã‚Œã§ã‚‚ä»®å®šã™ã‚‹"æœ¬å½“ã®ç†ç”±"ã¯ä½•ã‹ï¼Ÿ**

CLTãŒã€Œå¤šæ•°ã®ç‹¬ç«‹å¾®å°åŠ¹æœã®å’Œâ†’æ­£è¦åˆ†å¸ƒã€ã‚’ä¿è¨¼ã™ã‚‹ã‹ã‚‰ï¼Ÿ ãã‚Œã¯ç†ç”±ã®ä¸€ã¤ã ã€‚ã ãŒæœ¬è³ªã¯ã‚‚ã£ã¨æ·±ã„ã€‚

- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¯**æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒ**ã ã€‚å¹³å‡ã¨åˆ†æ•£ã ã‘ã‚’çŸ¥ã£ã¦ã„ã‚‹ã¨ãã€ãã‚Œä»¥ä¸Šã®ä»®å®šã‚’ç½®ã‹ãªã„ã€Œæœ€ã‚‚æƒ…å ±é‡ã®å°‘ãªã„ã€åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹ã 
- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æ¼”ç®—ã¯**é–‰ã˜ã¦ã„ã‚‹**ã€‚å’Œãƒ»æ¡ä»¶ä»˜ããƒ»å‘¨è¾ºãŒå…¨ã¦ã‚¬ã‚¦ã‚¹ã®ã¾ã¾ã€‚ã“ã‚Œã¯è¨ˆç®—ä¸Šã®å¥‡è·¡ã¨è¨€ã£ã¦ã‚ˆã„
- ãã—ã¦ã€æ­£è¦åˆ†å¸ƒãŒã€Œé–“é•ã£ã¦ã„ã‚‹ã€ã“ã¨ã¯**ã‚ã‹ã£ã¦ã„ã‚‹**ä¸Šã§ä½¿ã†ã€‚é‡è¦ãªã®ã¯ã€Œã©ã®ç¨‹åº¦é–“é•ã£ã¦ã„ã‚‹ã‹ã€ã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨ â€” KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆç¬¬6å›ï¼‰ãŒãã®é“å…·ã 

<details><summary>ãƒ™ã‚¤ã‚ºè„³ä»®èª¬ â€” è„³ã¯ç¢ºç‡è¨ˆç®—æ©Ÿã‹ï¼Ÿ</summary>

èªçŸ¥ç§‘å­¦ã«ã¯ã€Œè„³ã¯ãƒ™ã‚¤ã‚ºæ¨è«–ã‚’è¡Œã£ã¦ã„ã‚‹ã€ã¨ã„ã†ä»®èª¬ãŒã‚ã‚‹ã€‚æ„Ÿè¦šå…¥åŠ›ï¼ˆå°¤åº¦ï¼‰ã¨çµŒé¨“ï¼ˆäº‹å‰åˆ†å¸ƒï¼‰ã‚’çµ„ã¿åˆã‚ã›ã¦ä¸–ç•Œã®çŠ¶æ…‹ï¼ˆäº‹å¾Œåˆ†å¸ƒï¼‰ã‚’æ¨å®šã™ã‚‹ã€‚

éŒ¯è¦–ç¾è±¡ã¯ã€å¼·ã„äº‹å‰åˆ†å¸ƒãŒå¼±ã„å°¤åº¦ã‚’ä¸Šæ›¸ãã™ã‚‹ä¾‹ã¨ã—ã¦è§£é‡ˆã•ã‚Œã‚‹ã€‚VAEã®ãƒ‡ã‚³ãƒ¼ãƒ€ãŒã€Œã¼ã‚„ã‘ãŸã€ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã®ã¯ã€äº‹å‰åˆ†å¸ƒ $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ ãŒéåº¦ã«æ»‘ã‚‰ã‹ãªæ½œåœ¨ç©ºé–“ã‚’å¼·åˆ¶ã™ã‚‹ãŸã‚ â€” ã‚ã‚‹æ„å‘³ã€è„³ã®éŒ¯è¦–ã¨åŒã˜æ§‹é€ ã ã€‚

ã€Œæ­£è¦åˆ†å¸ƒã‚’ä»®å®šã™ã‚‹ã€ã®ã¯ã€è„³ãŒã€Œä¸–ç•Œã¯æ»‘ã‚‰ã‹ã ã€ã¨ä»®å®šã™ã‚‹ã®ã¨åŒã˜ã‹ã‚‚ã—ã‚Œãªã„ã€‚
</details>

ã•ã‚‰ã«è€ƒãˆã¦ã¿ã‚ˆã†:

- **LLMã®å‡ºåŠ›åˆ†å¸ƒã¯Categoricalã€‚** æ­£è¦åˆ†å¸ƒã§ã¯ãªã„ã€‚ã ãŒCategoricalåˆ†å¸ƒã®è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆlogitï¼‰ã¯é€£ç¶šå€¤ã§ã€ãã®ç©ºé–“ã§ã¯æ­£è¦åˆ†å¸ƒçš„ãªä»®å®šãŒä½¿ã‚ã‚Œã‚‹
- **æ¬¡å…ƒã®å‘ªã„**: 100æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ã‚µãƒ³ãƒ—ãƒ«ã¯ã€ã»ã¼ç¢ºå®Ÿã«åŸç‚¹ã‹ã‚‰ $\sqrt{100} = 10$ ã®è·é›¢ã«ã‚ã‚‹ã€‚ã€Œé«˜æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹ã¯çƒæ®»ã«é›†ä¸­ã™ã‚‹ã€â€” ã“ã‚ŒãŒæ­£è¦åˆ†å¸ƒã®ç›´æ„ŸãŒå´©å£Šã™ã‚‹ç¬é–“ã 
- **æ­£è¦åˆ†å¸ƒã¯"æœ€ã‚‚ç„¡çŸ¥ãª"åˆ†å¸ƒ**: æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åŸç†ã«ã‚ˆã‚Šã€å¹³å‡ã¨åˆ†æ•£ã—ã‹çŸ¥ã‚‰ãªã„ã¨ãã€ä½™è¨ˆãªä»®å®šã‚’æœ€ã‚‚å°‘ãªãã™ã‚‹åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹ã€‚ã€ŒçŸ¥ã‚‰ãªã„ã“ã¨ã‚’æ­£ç›´ã«èªã‚ã‚‹åˆ†å¸ƒã€ã¨ã‚‚è¨€ãˆã‚‹


---

### 7.6 æœ€æ–°ç ”ç©¶ (2020-2026)

#### 6.9.1 Fisheræƒ…å ±é‡ã®ç†è«–çš„é€²å±•

Fisheræƒ…å ±é‡ã¯çµ±è¨ˆçš„æ¨æ¸¬ã®åŸºç¤ã§ã‚ã‚Šã€æœ€è¿‘ã®ç ”ç©¶ã¯ãã®å¿œç”¨ç¯„å›²ã‚’æ‹¡å¤§ã—ã¦ã„ã‚‹ã€‚

**æœŸå¾…Fisheræƒ…å ± vs è¦³æ¸¬Fisheræƒ…å ±**

Fisheræƒ…å ±é‡ã«ã¯2ã¤ã®è¡¨ç¾ãŒã‚ã‚‹:

$$
I(\theta) = \mathbb{E}\left[\left(\frac{\partial \log p(X; \theta)}{\partial \theta}\right)^2\right] = -\mathbb{E}\left[\frac{\partial^2 \log p(X; \theta)}{\partial \theta^2}\right]
$$

å‰è€…ã¯ã€ŒæœŸå¾…ã€ã€å¾Œè€…ã¯ã€Œè¦³æ¸¬ã€ã¨å‘¼ã°ã‚Œã‚‹ã€‚2013å¹´ã®arXivè«–æ–‡[^13]ã¯ã€**æœŸå¾…Fisheræƒ…å ±ã‚’ä½¿ã£ãŸä¿¡é ¼åŒºé–“ãŒè¦³æ¸¬Fisheræƒ…å ±ã‚’ä½¿ã£ãŸå ´åˆã‚ˆã‚Šå¹³å‡äºŒä¹—èª¤å·®ã®æ„å‘³ã§ç²¾åº¦ãŒé«˜ã„**ã“ã¨ã‚’è¨¼æ˜ã—ãŸã€‚2021å¹´ã®ç¶šç·¨[^14]ã§ã¯ã€ã“ã®çµæœã‚’åŒºé–“æ¨å®šã®ç›¸å¯¾æ€§èƒ½è©•ä¾¡ã«æ‹¡å¼µã—ã¦ã„ã‚‹ã€‚

**æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã¸ã®æ‹¡å¼µ**

2024å¹´ã®ç ”ç©¶[^15]ã¯ã€æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹Fisheræƒ…å ±é‡ã®æ˜ç¤ºçš„å®šç¾©ã‚’å¯èƒ½ã«ã™ã‚‹æ–°ã—ã„æœ€å°¤æ¨å®šãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ãŸã€‚å¾“æ¥ã€æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’ç©åˆ†æ¶ˆå»ã—ãŸå‘¨è¾ºå°¤åº¦ $p(\mathbf{x}; \theta) = \int p(\mathbf{x}, \mathbf{z}; \theta) d\mathbf{z}$ ã§ã¯Fisheræƒ…å ±é‡ã®è¨ˆç®—ãŒå›°é›£ã ã£ãŸã€‚ã“ã®ç ”ç©¶ã¯ã€å¤‰åˆ†è¿‘ä¼¼ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§åŠ¹ç‡çš„ãªæ¨å®šã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚

**ãƒ†ãƒ³ã‚½ãƒ«ãƒ¢ãƒ‡ãƒ«ã®Fisheræƒ…å ±**

2025å¹´ã®æœ€æ–°è«–æ–‡[^16]ã¯ã€ãƒã‚¢ã‚½ãƒ³Canonical Polyadic (CP) ãƒ†ãƒ³ã‚½ãƒ«åˆ†è§£ã®Fisheræƒ…å ±é‡ã‚’å°å‡ºã—ãŸã€‚3æ¬¡å…ƒä»¥ä¸Šã®ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: æ™‚é–“Ã—ç©ºé–“Ã—å‘¨æ³¢æ•°ï¼‰ã®çµ±è¨ˆçš„æ€§è³ªã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨ã§ã€CramÃ©r-Raoä¸‹ç•Œã«åŸºã¥ãæ¨å®šé‡ã®è©•ä¾¡ãŒå¯èƒ½ã«ãªã‚‹ã€‚


#### 6.9.2 æ¸¬åº¦è«–çš„ç¢ºç‡è«–ã®å®Ÿç”¨åŒ–

æ¸¬åº¦è«–ã¯ç¢ºç‡è«–ã®å³å¯†ãªåŸºç¤ã‚’ä¸ãˆã‚‹ãŒã€ã€ŒæŠ½è±¡çš„ã™ãã¦å®Ÿç”¨çš„ã§ãªã„ã€ã¨ã„ã†èª¤è§£ãŒã‚ã‚‹ã€‚æœ€è¿‘ã®ç ”ç©¶ã¯ã€æ¸¬åº¦è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å®Ÿç”¨çš„å¿œç”¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

**Tayloræ¸¬åº¦ã¨ç¢ºç‡éç¨‹**

2025å¹´ã®arXivè«–æ–‡[^17]ã¯ã€Tayloræ¸¬åº¦ã¨ã„ã†æ¦‚å¿µã‚’å°å…¥ã—ã€Browné‹å‹•ã€ãƒãƒ«ãƒãƒ³ã‚²ãƒ¼ãƒ«ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã€æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã‚’çµ±ä¸€çš„ã«æ‰±ã†æ çµ„ã¿ã‚’ææ¡ˆã—ãŸã€‚ã“ã‚Œã¯Taylorå±•é–‹ã®ä¸€èˆ¬åŒ–ã§ã‚ã‚Šã€ç¢ºç‡éç¨‹ã®å±€æ‰€çš„æ€§è³ªã‚’æ‰ãˆã‚‹ã€‚

**é€£ç¶šæ™‚é–“ç¢ºç‡éç¨‹ã®Metric Temporal Logic**

2023å¹´ã®ç ”ç©¶[^18]ã¯ã€é€£ç¶šæ™‚é–“ç¢ºç‡éç¨‹ãŒMetric Temporal Logic (MTL) ã®è«–ç†å¼ã‚’æº€ãŸã™ã‹ã©ã†ã‹ã®å¯æ¸¬æ€§ã‚’ç¢ºç«‹ã—ãŸã€‚ã“ã‚Œã¯å½¢å¼æ¤œè¨¼ã¨ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã‚’æ©‹æ¸¡ã—ã™ã‚‹æˆæœã§ã€è‡ªå‹•é‹è»¢è»Šã®å®‰å…¨æ€§æ¤œè¨¼ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚

**ç¢ºç‡ç©ºé–“ã®æ§‹æˆ**

arXivè«–æ–‡[^19]ã¯ã€æ±ºå®šè«–çš„éç¨‹ã‹ã‚‰å‡ºç™ºã—ã¦æŠ½è±¡çš„ç¢ºç‡ç©ºé–“ã‚’æ§‹æˆã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸã€‚ã“ã‚Œã¯ã€Œç¢ºç‡çš„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯æ±ºå®šè«–çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ã€ã¨ã„ã†å“²å­¦çš„æ´å¯Ÿã‚’å½¢å¼åŒ–ã—ã¦ã„ã‚‹ã€‚

#### 6.9.3 æƒ…å ±ç†è«–ã®æœ€æ–°å±•é–‹

KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯æ©Ÿæ¢°å­¦ç¿’ã®ä¸­å¿ƒæ¦‚å¿µã ãŒã€ãã®ç†è«–ã¯ã¾ã ç™ºå±•é€”ä¸Šã ã€‚

**Î±-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ãƒ™ã‚¤ã‚ºæœ€é©åŒ–**

2024å¹´ã®è«–æ–‡[^20]ã¯ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’ä¸€èˆ¬åŒ–ã—ãŸÎ±-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«åŸºã¥ãæ–°ã—ã„ãƒ™ã‚¤ã‚ºæœ€é©åŒ–æ‰‹æ³•ã€ŒAlpha Entropy Search (AES)ã€ã‚’ææ¡ˆã—ãŸã€‚Î±-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯:

$$
D_\alpha(p \| q) = \frac{1}{\alpha(\alpha-1)} \left( \int p(x)^\alpha q(x)^{1-\alpha} dx - 1 \right)
$$

$\alpha \to 1$ ã§KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«åæŸã™ã‚‹ã€‚AESã¯ç²å¾—é–¢æ•°ã¨ã—ã¦ã€æ¬¡ã®è©•ä¾¡ç‚¹ã§ã®ç›®çš„é–¢æ•°å€¤ã¨å¤§åŸŸçš„æœ€å¤§å€¤ã®ã€Œä¾å­˜åº¦ã€ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚ã“ã®ä¾å­˜åº¦ã‚’Î±-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã§æ¸¬ã‚‹ã“ã¨ã§ã€KLãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚ˆã‚Šæ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’æŸ”è»Ÿã«åˆ¶å¾¡ã§ãã‚‹ã€‚

**Jensen-Shannonã¨KLã®é–¢ä¿‚**

2025å¹´ã®è«–æ–‡[^21]ã¯ã€Jensen-Shannon (JS) ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®æœ€é©ãªä¸‹ç•Œã‚’ç¢ºç«‹ã—ãŸ:

$$
\text{JS}(p \| q) = \frac{1}{2} D_{\text{KL}}(p \| m) + \frac{1}{2} D_{\text{KL}}(q \| m), \quad m = \frac{p + q}{2}
$$

JSãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯GANã®ç›®çš„é–¢æ•°ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ãŒã€KLã¨ã®å®šé‡çš„é–¢ä¿‚ã¯é•·å¹´ä¸æ˜ã ã£ãŸã€‚ã“ã®æˆæœã«ã‚ˆã‚Šã€GANã®åæŸæ€§ç†è«–ãŒæ”¹å–„ã•ã‚ŒãŸã€‚

**å¹¾ä½•å­¦çš„æƒ…å ±ç†è«– (GAIT)**

å¾“æ¥ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ç¢ºç‡åˆ†å¸ƒã‚’ã€Œç‚¹ã€ã¨ã—ã¦æ‰±ã„ã€ç©ºé–“ã®å¹¾ä½•ã‚’ç„¡è¦–ã™ã‚‹ã€‚2019å¹´ã®è«–æ–‡[^22]ã¯ã€ç¢ºç‡åˆ†å¸ƒã®å°ï¼ˆsupportï¼‰ã®å¹¾ä½•å­¦çš„æ§‹é€ ã‚’è€ƒæ…®ã—ãŸæ–°ã—ã„ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€ŒGeometric Informationã€ã‚’ææ¡ˆã—ãŸã€‚ã“ã‚Œã¯æœ€é©è¼¸é€ç†è«–ã¨KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’çµ±åˆã™ã‚‹è©¦ã¿ã ã€‚


#### 6.9.4 çµ±è¨ˆçš„æ¨æ¸¬ã®æ–°ç†è«–

**Extended Likelihoodã¨ãƒ©ãƒ³ãƒ€ãƒ æœªçŸ¥é‡**

2023å¹´ã®è«–æ–‡[^23]ã¯ã€å¾“æ¥ã®å°¤åº¦ç†è«–ã‚’ã€Œå›ºå®šã•ã‚ŒãŸæœªçŸ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ã‹ã‚‰ã€Œãƒ©ãƒ³ãƒ€ãƒ ãªæœªçŸ¥é‡ã€ã¸æ‹¡å¼µã—ãŸã€‚ã“ã‚Œã¯é »åº¦ä¸»ç¾©ã¨ãƒ™ã‚¤ã‚ºä¸»ç¾©ã®ä¸­é–“çš„ç«‹å ´ã§ã€äº‹å‰åˆ†å¸ƒã‚’ä»®å®šã›ãšã«ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœã‚’æ‰±ãˆã‚‹ã€‚

**Maximum Ideal Likelihood**

2024å¹´ã®ç ”ç©¶[^24]ã¯ã€æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹æ–°ã—ã„æ¨å®šãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒMaximum Ideal Likelihood (MIL)ã€ã‚’ææ¡ˆã—ãŸã€‚å¾“æ¥ã®MLEã¯å‘¨è¾ºåŒ– $p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{z}) d\mathbf{z}$ ãŒå›°é›£ã ã£ãŸãŒã€MILã¯æ½œåœ¨å¤‰æ•°ã‚’ã€Œç†æƒ³çš„ãªè¦³æ¸¬ã€ã¨ã—ã¦æ‰±ã†ã“ã¨ã§ã€è¨ˆç®—å¯èƒ½ãªç›®çš„é–¢æ•°ã‚’å°å‡ºã™ã‚‹ã€‚æ¼¸è¿‘çš„ã«MLEã¨ç­‰ä¾¡ã§ã‚ã‚Šã€ä¿¡é ¼åŒºé–“ã‚‚æ§‹æˆã§ãã‚‹ã€‚


#### 6.9.5 éæ­£è¦åŒ–çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã¨ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°

ç¢ºç‡å¯†åº¦é–¢æ•°ã‚’æ­£è¦åŒ–å®šæ•°è¾¼ã¿ã§è¨ˆç®—ã™ã‚‹ã®ã¯å›°é›£ãªå ´åˆãŒå¤šã„ã€‚Energy-Based Model (EBM) ã§ã¯ $p(x) = \frac{1}{Z}\exp(-E(x))$ ã¨è¡¨ç¾ã™ã‚‹ãŒã€åˆ†é…é–¢æ•° $Z = \int \exp(-E(x))dx$ ã®è¨ˆç®—ãŒæŒ‡æ•°çš„ã«å›°é›£ã ã€‚

**ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°** [^9] ã¯ã€æ­£è¦åŒ–å®šæ•°ã‚’è¨ˆç®—ã›ãšã«ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã‚’æ¨å®šã™ã‚‹æ‰‹æ³•ã ã€‚ã‚¹ã‚³ã‚¢é–¢æ•° $s(x) = \nabla_x \log p(x)$ ã¯æ­£è¦åŒ–å®šæ•°ã«ä¾å­˜ã—ãªã„ã“ã¨ã‚’åˆ©ç”¨ã™ã‚‹:

$$
s(x) = \nabla_x \log p(x) = \nabla_x \log \frac{1}{Z}\exp(-E(x)) = \nabla_x [-E(x) - \log Z] = -\nabla_x E(x)
$$

ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ç›®çš„é–¢æ•°:

$$
J(\theta) = \frac{1}{2}\mathbb{E}_{p_{\text{data}}(x)}\left[\| \nabla_x \log p_\theta(x) - \nabla_x \log p_{\text{data}}(x) \|^2\right]
$$

ã“ã‚Œã¯æ­£è¦åŒ–å®šæ•°ãªã—ã§è¨ˆç®—å¯èƒ½ãªå½¢ã«å¤‰å½¢ã§ãã‚‹ï¼ˆéƒ¨åˆ†ç©åˆ†ã‚’ç”¨ã„ãŸæ’ç­‰å¼ï¼‰ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« [^10] ã®ç†è«–çš„åŸºç›¤ã®ä¸€ã¤ã§ã‚‚ã‚ã‚‹ã€‚


#### 6.9.6 ç¢ºç‡è«–ã¨LLMã®æ·±ã„æ¥ç¶š

LLMã®è¨“ç·´ã¯ã€æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã¨ã„ã†ç¢ºç‡çš„ã‚¿ã‚¹ã‚¯ã«å¸°ç€ã™ã‚‹ã€‚ã“ã®æ¥ç¶šã‚’æ˜ç¢ºã«ã—ã‚ˆã†ã€‚

**è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨é€£é–è¦å‰‡**:

$$
p(\mathbf{x}) = \prod_{t=1}^{T} p(x_t \mid x_{<t})
$$

å„æ™‚åˆ»ã§ã®æ¡ä»¶ä»˜ãåˆ†å¸ƒ $p(x_t \mid x_{<t})$ ã¯Categoricalåˆ†å¸ƒã§ã‚ã‚Šã€Softmaxã§å®šç¾©ã•ã‚Œã‚‹:

$$
p(x_t = k \mid x_{<t}) = \frac{\exp(z_k)}{\sum_{j=1}^{V} \exp(z_j)}, \quad z = f_\theta(x_{<t})
$$

**Cross-Entropyæå¤±ã¨MLE**:

$$
\mathcal{L} = -\frac{1}{T}\sum_{t=1}^{T} \log p_\theta(x_t \mid x_{<t}) = -\frac{1}{T} \log p_\theta(\mathbf{x})
$$

ã“ã‚Œã¯è² ã®å¯¾æ•°å°¤åº¦ã§ã‚ã‚Šã€æœ€å°åŒ–ã¯MLEã¨ç­‰ä¾¡ã ã€‚

**Perplexityã¨æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**:

$$
\text{Perplexity} = \exp(\mathcal{L}) = \exp\left(-\frac{1}{T}\sum_{t=1}^{T} \log p(x_t \mid x_{<t})\right)
$$

ã“ã‚Œã¯æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $H(X_t \mid X_{<t})$ ã®æŒ‡æ•°ã§ã‚ã‚‹ã€‚Perplexity=10ã¯ã€Œå„æ™‚åˆ»ã§å¹³å‡10å€‹ã®å€™è£œã‹ã‚‰é¸æŠã—ã¦ã„ã‚‹ã€ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

**ç¢ºç‡çš„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨Top-k/Nucleus Sampling**:

æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\tau$ ã‚’å°å…¥ã—ãŸç¢ºç‡åˆ†å¸ƒ:

$$
p_\tau(x_t = k) = \frac{\exp(z_k/\tau)}{\sum_j \exp(z_j/\tau)}
$$

- $\tau \to 0$: æ±ºå®šè«–çš„ï¼ˆargmaxï¼‰
- $\tau = 1$: å…ƒã®åˆ†å¸ƒ
- $\tau > 1$: ã‚ˆã‚Šå¹³å¦ï¼ˆå¤šæ§˜æ€§å¢—åŠ ï¼‰

Nucleus samplingï¼ˆTop-pï¼‰ã¯ç´¯ç©ç¢ºç‡ $\sum_{k \in \text{top-p}} p(k) \geq p$ ã‚’æº€ãŸã™æœ€å°é›†åˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚ã“ã‚Œã¯ã€Œç¢ºç‡è³ªé‡ã®ä¸Šä½p%ã€ã¨ã„ã†å‹•çš„é–¾å€¤ã ã€‚


> **Note:** **LLMã®ç¢ºç‡è«–çš„è§£é‡ˆ**: æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æ¡ä»¶ä»˜ãç¢ºç‡åˆ†å¸ƒã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã€‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ï¼ˆtemperature, top-k, nucleusï¼‰ã¯ã€ã“ã®ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã®ã€Œåˆ¶å¾¡ã•ã‚ŒãŸãƒ©ãƒ³ãƒ€ãƒ åŒ–ã€ã ã€‚æ±ºå®šè«–çš„ç”Ÿæˆï¼ˆgreedyï¼‰ã¯æœ€å°¤æ¨å®šã€ç¢ºç‡çš„ç”Ÿæˆã¯ãƒ™ã‚¤ã‚ºæ¨è«–ã®è¦–ç‚¹ã¨å¯¾å¿œã™ã‚‹ã€‚

### 7.7 ç¢ºç‡è«–ã‹ã‚‰ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¸ã®æ©‹

ç¬¬4å›ã§å­¦ã‚“ã å…¨ã¦ãŒã€æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åœŸå°ã ã€‚ã“ã®æ©‹ã‚’æ˜ç¤ºçš„ã«ç¤ºã—ã¦ãŠãã€‚

**VAEï¼ˆç¬¬8å›ï¼‰ã¸ã®ç›´æ¥æ¥ç¶š**:

| ç¬¬4å›ã®æ¦‚å¿µ | VAEã§ã®å½¹å‰² |
|:------------|:------------|
| MLE | ãƒ‡ã‚³ãƒ¼ãƒ€ $p_\theta(\mathbf{x}\mid\mathbf{z})$ ã®æœ€å¤§åŒ– |
| KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | ELBOã®æ­£å‰‡åŒ–é … $D_{\mathrm{KL}}(q_\phi\|p)$ |
| Gaussian MLE | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ $\mu_\phi, \sigma_\phi$ ã®å‡ºåŠ› |
| æŒ‡æ•°å‹åˆ†å¸ƒæ— | ãƒ‡ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›åˆ†å¸ƒè¨­è¨ˆ |
| å¤‰åˆ†æ¨è«–ï¼ˆäº‹å¾Œä¸€è‡´æ€§ï¼‰| ELBOæœ€å¤§åŒ–ã«ã‚ˆã‚‹è¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒã®å­¦ç¿’ |

**Diffusion Modelsï¼ˆç¬¬15å›ï¼‰ã¸ã®æ¥ç¶š**:

| ç¬¬4å›ã®æ¦‚å¿µ | Diffusionã§ã®å½¹å‰² |
|:------------|:------------------|
| Gaussianç©ã®é–‰å½¢å¼ | $q(\mathbf{x}_t\mid\mathbf{x}_0)$ ã®åˆ†æçš„è¨ˆç®— |
| æ¡ä»¶ä»˜ãGaussian | é€†ãƒ—ãƒ­ã‚»ã‚¹ $p_\theta(\mathbf{x}_{t-1}\mid\mathbf{x}_t)$ ã®å½¢ |
| KLæœ€å°åŒ– | ELBO = $\sum_t \mathbb{E}[D_{\mathrm{KL}}(q_t\|p_{t-1})]$ |

**LLMï¼ˆç¬¬20å›ï¼‰ã¸ã®æ¥ç¶š**:

| ç¬¬4å›ã®æ¦‚å¿µ | LLMã§ã®å½¹å‰² |
|:------------|:------------|
| Categoricalåˆ†å¸ƒ | softmaxå‡ºåŠ›å±¤ |
| é€£é–è¦å‰‡ $\log p(\mathbf{x}) = \sum_t \log p(x_t\mid x_{<t})$ | è‡ªå·±å›å¸°ç›®çš„é–¢æ•° |
| MLE | æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®æœ€å¤§åŒ–ï¼ˆäº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰|
| æŒ‡æ•°å‹åˆ†å¸ƒæ— | logitç©ºé–“ã®å¹¾ä½•å­¦ |

ç¢ºç‡è«–ã¯ã€Œç©ã¿æœ¨ã€ã ã€‚ã“ã“ã§ç©ã‚“ã æ¦‚å¿µãŒã€å¾ŒåŠã®å…¨ã¦ã®è¬›ç¾©ã§å‘¼ã³æˆ»ã•ã‚Œã‚‹ã€‚

> Progress: 100%

---
> **ğŸ“– å‰ç·¨ã‚‚ã‚ã‚ã›ã¦ã”è¦§ãã ã•ã„**
> [ã€å‰ç·¨ã€‘ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦](/articles/ml-lecture-04-part1) ã§ã¯ã€ç¢ºç‡è«–ãƒ»ãƒ™ã‚¤ã‚ºã®å®šç†ãƒ»æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ç†è«–ã‚’å­¦ã³ã¾ã—ãŸã€‚

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2016). Variational Inference: A Review for Statisticians.
<https://arxiv.org/abs/1601.00670>

[^2]: Kingma, D. P., & Welling, M. (2013). Auto-Encoding Variational Bayes.
<https://arxiv.org/abs/1312.6114>

[^3]: Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network.
<https://arxiv.org/abs/1503.02531>

[^4]: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models.
<https://arxiv.org/abs/2006.11239>

[^5]: Malach, E. (2023). Auto-Regressive Next-Token Predictors are Universal Learners.
<https://arxiv.org/abs/2309.06979>

[^6]: Song, Y., & Ermon, S. (2019). Generative Modeling by Estimating Gradients of the Data Distribution.
<https://arxiv.org/abs/1907.05600>

[^7]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). Score-Based Generative Modeling through Stochastic Differential Equations.
<https://arxiv.org/abs/2011.13456>

[^10]: Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B. (2020). "Score-Based Generative Modeling through Stochastic Differential Equations." *ICLR 2021 (Oral)*.
[https://arxiv.org/abs/2011.13456](https://arxiv.org/abs/2011.13456)

[^11]: Rezende, D.J., Mohamed, S. (2015). "Variational Inference with Normalizing Flows." *ICML 2015*.
[https://arxiv.org/abs/1505.05770](https://arxiv.org/abs/1505.05770)

[^12]: Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." *ICLR 2022*.
[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)

[^13]: Relative Performance of Expected and Observed Fisher Information in Covariance Estimation for Maximum Likelihood Estimates. (2013). *arXiv preprint*.
[https://arxiv.org/abs/1305.1056](https://arxiv.org/abs/1305.1056)

[^14]: Relative Performance of Fisher Information in Interval Estimation. (2021). *arXiv preprint*.
[https://arxiv.org/abs/2107.04620](https://arxiv.org/abs/2107.04620)

[^15]: Maximum Ideal Likelihood Estimator: A New Estimation and Inference Framework for Latent Variable Models. (2024). *arXiv preprint*.
[https://arxiv.org/abs/2410.01194](https://arxiv.org/abs/2410.01194)

[^16]: A Latent-Variable Formulation of the Poisson Canonical Polyadic Tensor Model: Maximum Likelihood Estimation and Fisher Information. (2025). *arXiv preprint*.
[https://arxiv.org/abs/2511.05352](https://arxiv.org/abs/2511.05352)

[^17]: The Taylor Measure and its Applications. (2025). *arXiv preprint*.
[https://arxiv.org/abs/2508.04760](https://arxiv.org/abs/2508.04760)

[^18]: On the Metric Temporal Logic for Continuous Stochastic Processes. (2023). *arXiv preprint*.
[https://arxiv.org/abs/2308.00984](https://arxiv.org/abs/2308.00984)

[^19]: A Probability Space at Inception of Stochastic Process. (2025). *arXiv preprint*.
[https://arxiv.org/abs/2510.20824](https://arxiv.org/abs/2510.20824)

[^20]: Alpha Entropy Search for New Information-based Bayesian Optimization. (2024). *arXiv preprint*.
[https://arxiv.org/abs/2411.16586](https://arxiv.org/abs/2411.16586)

[^21]: Connecting Jensen-Shannon and Kullback-Leibler. (2025). *arXiv preprint*.
[https://arxiv.org/abs/2510.20644](https://arxiv.org/abs/2510.20644)

[^22]: GAIT: A Geometric Approach to Information Theory. (2019). *arXiv preprint*.
[https://arxiv.org/abs/1906.08325](https://arxiv.org/abs/1906.08325)

[^23]: Statistical Inference for Random Unknowns via Modifications of Extended Likelihood. (2023). *arXiv preprint*.
[https://arxiv.org/abs/2310.09955](https://arxiv.org/abs/2310.09955)

[^24]: Maximum Ideal Likelihood Estimator: An New Estimation and Inference Framework for Latent Variable Models. (2024). *arXiv preprint*.
[https://arxiv.org/abs/2410.01194](https://arxiv.org/abs/2410.01194)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

---


---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
