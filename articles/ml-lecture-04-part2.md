---
title: "ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ²"
type: "tech"
topics: ["machinelearning", "deeplearning", "probability", "python"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” ç¢ºç‡è«–ã‚’ã‚³ãƒ¼ãƒ‰ã«ç„¼ãã¤ã‘ã‚‹

### 4.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Python 3.10+ recommended
pip install numpy scipy matplotlib
```

æœ¬è¬›ç¾©ã¯Python 100%ã€‚NumPyã¨SciPyã®ã¿ä½¿ç”¨ã™ã‚‹ã€‚PyTorchã¯ä¸è¦ã ã€‚

### 4.2 ç¢ºç‡åˆ†å¸ƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

NumPyã¨SciPyã®ç¢ºç‡åˆ†å¸ƒé–¢æ•°ã‚’ä½“ç³»çš„ã«æ•´ç†ã™ã‚‹ã€‚

```python
import numpy as np
from scipy import stats

# Sampling, PDF/PMF, CDF, Quantile (PPF) for major distributions
distributions = {
    "Bernoulli(0.7)":     (stats.bernoulli(0.7), "discrete"),
    "Binomial(20,0.3)":   (stats.binom(20, 0.3), "discrete"),
    "Poisson(5)":         (stats.poisson(5), "discrete"),
    "Normal(0,1)":        (stats.norm(0, 1), "continuous"),
    "Gamma(3,2)":         (stats.gamma(3, scale=0.5), "continuous"),
    "Beta(2,5)":          (stats.beta(2, 5), "continuous"),
    "Exponential(2)":     (stats.expon(scale=0.5), "continuous"),
}

print(f"{'Distribution':<22} {'Mean':>8} {'Var':>8} {'Median':>8} {'Entropy':>8}")
print("-" * 58)
for name, (dist, dtype) in distributions.items():
    mean = dist.mean()
    var = dist.var()
    median = dist.median()
    entropy = dist.entropy()
    print(f"{name:<22} {mean:>8.3f} {var:>8.3f} {median:>8.3f} {entropy:>8.3f}")

# Important: scipy vs numpy interface
print("\n=== Sampling Interface Comparison ===")
print("NumPy:  np.random.normal(mu, sigma, N)  â†’ array of samples")
print("SciPy:  stats.norm(mu, sigma).rvs(N)     â†’ array of samples")
print("SciPy:  stats.norm(mu, sigma).pdf(x)     â†’ density at x")
print("SciPy:  stats.norm(mu, sigma).cdf(x)     â†’ P(X â‰¤ x)")
print("SciPy:  stats.norm(mu, sigma).ppf(q)     â†’ quantile (inverse CDF)")
print("SciPy:  stats.norm(mu, sigma).logpdf(x)  â†’ log density (MLEç”¨)")
```

### 4.3 LaTeXç¢ºç‡è¨˜æ³•ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

æ•°å¼ã§æ›¸ãâ†”è«–æ–‡ã§èª­ã‚€ã€ã‚’é«˜é€Ÿã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãŸã‚ã®å¯¾å¿œè¡¨ã€‚

| æ•°å¼ | LaTeX | èª­ã¿ | Python |
|:-----|:------|:-----|:-------|
| $P(A)$ | `P(A)` | ãƒ”ãƒ¼ ã‚¨ãƒ¼ | `p_a` |
| $P(A \mid B)$ | `P(A \mid B)` | ãƒ”ãƒ¼ ã‚¨ãƒ¼ ã‚®ãƒ–ãƒ³ ãƒ“ãƒ¼ | `p_a_given_b` |
| $\mathbb{E}[X]$ | `\mathbb{E}[X]` | ã‚¤ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ | `x.mean()` |
| $\text{Var}(X)$ | `\text{Var}(X)` | ãƒãƒªã‚¢ãƒ³ã‚¹ ã‚¨ãƒƒã‚¯ã‚¹ | `x.var()` |
| $\mathcal{N}(\mu, \sigma^2)$ | `\mathcal{N}(\mu, \sigma^2)` | ãƒãƒ¼ãƒãƒ« ãƒŸãƒ¥ãƒ¼ ã‚·ã‚°ãƒäºŒä¹— | `np.random.normal(mu, sigma)` |
| $\sim$ | `\sim` | ã—ãŸãŒã† / åˆ†å¸ƒã™ã‚‹ | sampling |
| $\overset{\text{i.i.d.}}{\sim}$ | `\overset{\text{i.i.d.}}{\sim}` | ç‹¬ç«‹åŒåˆ†å¸ƒã«ã—ãŸãŒã† | `for` loop sampling |
| $\propto$ | `\propto` | æ¯”ä¾‹ã™ã‚‹ | unnormalized |
| $\prod_{i=1}^{N}$ | `\prod_{i=1}^{N}` | ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ | `np.prod()` |
| $\arg\max_\theta$ | `\arg\max_\theta` | ã‚¢ãƒ¼ã‚°ãƒãƒƒã‚¯ã‚¹ ã‚·ãƒ¼ã‚¿ | `theta[np.argmax(...)]` |

### 4.4 è«–æ–‡èª­è§£ã®å®Ÿè·µ â€” 3ãƒ‘ã‚¹ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

ç¢ºç‡è«–ã®è«–æ–‡ã‚’èª­ã‚€ãŸã‚ã®ä½“ç³»çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚

```mermaid
graph TD
    P1["Pass 1: æ¦‚è¦æŠŠæ¡ (5åˆ†)<br/>Abstractâ†’Conclusionâ†’å›³è¡¨"]
    P2["Pass 2: æ§‹é€ ç†è§£ (30åˆ†)<br/>å®šç†ã®ä¸»å¼µâ†’ä»®å®šâ†’å¸°çµ"]
    P3["Pass 3: å†ç¾ (2-3æ™‚é–“)<br/>å°å‡ºã‚’ç´™ã§è¿½ã†â†’ã‚³ãƒ¼ãƒ‰å®Ÿè£…"]

    P1 -->|"èª­ã‚€ä¾¡å€¤ã‚ã‚Š?"| P2
    P2 -->|"æ·±ãç†è§£ã—ãŸã„?"| P3
```

**Pass 1 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ** â€” ç¢ºç‡è«–ã®è«–æ–‡ã«ç‰¹åŒ–:

```python
paper_pass1 = {
    "title": "",
    "authors": "",
    "year": "",
    "venue": "",
    # Probability-specific fields
    "distributions_used": [],       # e.g., ["Gaussian", "Categorical", "Dirichlet"]
    "key_assumptions": [],          # e.g., ["i.i.d.", "compact support", "finite variance"]
    "estimation_method": "",        # e.g., "MLE", "Bayesian", "Variational"
    "main_theorem": "",             # one-sentence statement
    "convergence_rate": "",         # e.g., "O(1/âˆšN)", "exponential"
    "experiments": "",
    "relevance_to_generative": "",  # connection to VAE/GAN/Diffusion
    "read_further": True,           # proceed to Pass 2?
}
```

### 4.5 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³

ç¢ºç‡è«–ã«ç‰¹åŒ–ã—ãŸ7ã¤ã®ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚

**ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç¢ºç‡å¯†åº¦é–¢æ•°ï¼ˆPDFï¼‰**

$$
f(x; \theta) = \text{formula}
$$

```python
def pdf(x: np.ndarray, theta: float) -> np.ndarray:
    """Direct translation of mathematical formula."""
    return formula(x, theta)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³2: æœŸå¾…å€¤ã®Monte Carloè¿‘ä¼¼**

$$
\mathbb{E}_{p(x)}[g(x)] = \int g(x) p(x) dx \approx \frac{1}{N}\sum_{i=1}^{N} g(x_i), \quad x_i \sim p(x)
$$

```python
samples = np.random.distribution(params, size=N)  # x_i ~ p(x)
expectation = np.mean(g(samples))                  # (1/N) Î£ g(x_i)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³3: å°¤åº¦ã¨å¯¾æ•°å°¤åº¦**

$$
\ell(\theta) = \sum_{i=1}^{N} \log p(x_i; \theta)
$$

```python
def log_likelihood(data: np.ndarray, theta: float) -> float:
    return np.sum(np.log(pdf(data, theta) + 1e-10))  # +Îµ for numerical stability
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒ™ã‚¤ã‚ºæ›´æ–°**

$$
p(\theta \mid \mathcal{D}) \propto p(\mathcal{D} \mid \theta) \cdot p(\theta)
$$

```python
# Grid approximation
theta_grid = np.linspace(0, 1, 1000)
prior = prior_pdf(theta_grid)
likelihood = np.prod([pdf(x, theta_grid) for x in data], axis=0)
posterior = likelihood * prior
posterior /= np.trapz(posterior, theta_grid)  # normalize
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³5: MLE via æ•°å€¤æœ€é©åŒ–**

$$
\hat{\theta} = \arg\max_\theta \ell(\theta)
$$

```python
from scipy.optimize import minimize_scalar
result = minimize_scalar(lambda t: -log_likelihood(data, t), bounds=(0, 10), method='bounded')
theta_mle = result.x
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³6: æ¡ä»¶ä»˜ãåˆ†å¸ƒã®è¨ˆç®—**

$$
p(y \mid x) = \frac{p(x, y)}{p(x)} = \frac{p(x, y)}{\sum_y p(x, y)}
$$

```python
joint = compute_joint(x, y)           # P(X, Y)
marginal_x = joint.sum(axis=1)        # P(X) = Î£_y P(X,y)
conditional = joint / marginal_x[:, None]  # P(Y|X)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³7: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨çµŒé¨“åˆ†å¸ƒ**

$$
\hat{p}(x) = \frac{1}{N}\sum_{i=1}^{N} \delta(x - x_i)
$$

```python
samples = np.random.distribution(params, size=N)
# Empirical distribution via histogram
counts, bin_edges = np.histogram(samples, bins=50, density=True)
```

:::details å…¨ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œè¡¨
| æ•°å­¦çš„æ¦‚å¿µ | æ•°å¼ | NumPyã‚³ãƒ¼ãƒ‰ |
|:----------|:-----|:-----------|
| æœŸå¾…å€¤ | $\mathbb{E}[X]$ | `samples.mean()` |
| åˆ†æ•£ | $\text{Var}(X)$ | `samples.var()` |
| å…±åˆ†æ•£è¡Œåˆ— | $\boldsymbol{\Sigma}$ | `np.cov(data.T)` |
| ç²¾åº¦è¡Œåˆ— | $\boldsymbol{\Lambda} = \boldsymbol{\Sigma}^{-1}$ | `np.linalg.inv(cov)` |
| Mahalanobisè·é›¢ | $(\mathbf{x}-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})$ | `(x-mu) @ inv_cov @ (x-mu)` |
| å¯¾æ•°å°¤åº¦ | $\sum_i \log p(x_i;\theta)$ | `np.sum(np.log(pdf(data, theta)))` |
| Softmax | $\frac{e^{x_i}}{\sum_j e^{x_j}}$ | `np.exp(x-x.max()) / np.exp(x-x.max()).sum()` |
| KL divergence | $\sum_i p_i \log(p_i/q_i)$ | `np.sum(p * np.log(p/q))` |
| ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | $x \sim \mathcal{N}(\mu,\sigma^2)$ | `np.random.normal(mu, sigma, N)` |
| æ¡ä»¶ä»˜ãç¢ºç‡ | $P(A \mid B)$ | `p_ab / p_b` |
:::

### 4.6 å®Ÿè£…æ¼”ç¿’: ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰ã®MLE

ç¬¬8å›ï¼ˆEMç®—æ³•ï¼‰ã¸ã®æ©‹æ¸¡ã—ã¨ã—ã¦ã€2æˆåˆ†GMMã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹ã€‚ã“ã“ã§ã¯EMç®—æ³•ã®å‰æ®µéšã¨ã—ã¦ã€å˜ä¸€ã‚¬ã‚¦ã‚¹ã®MLEã‚’æ‹¡å¼µã™ã‚‹å½¢ã§å•é¡Œã®å›°é›£ã•ã‚’ä½“æ„Ÿã™ã‚‹ã€‚

```python
import numpy as np

# Generate data from a 2-component Gaussian mixture
np.random.seed(42)
N = 1000
# True parameters
pi_true = 0.4  # mixing weight
mu1_true, sigma1_true = -2.0, 0.8
mu2_true, sigma2_true = 3.0, 1.2

# Sample
component = np.random.binomial(1, 1 - pi_true, N)
data = np.where(component == 0,
                np.random.normal(mu1_true, sigma1_true, N),
                np.random.normal(mu2_true, sigma2_true, N))

print(f"Generated {N} samples from GMM")
print(f"True: Ï€={pi_true}, Î¼â‚={mu1_true}, Ïƒâ‚={sigma1_true}, Î¼â‚‚={mu2_true}, Ïƒâ‚‚={sigma2_true}")

# Single Gaussian MLE (wrong model)
mu_single = data.mean()
sigma_single = data.std()
print(f"\nSingle Gaussian MLE: Î¼={mu_single:.3f}, Ïƒ={sigma_single:.3f}")
print("â†’ Clearly wrong! The data has two modes.")

# GMM log-likelihood (for given parameters)
def gmm_log_likelihood(data, pi, mu1, sig1, mu2, sig2):
    """Log-likelihood of 2-component GMM.

    corresponds to: L = Î£áµ¢ log[Ï€ N(xáµ¢|Î¼â‚,Ïƒâ‚Â²) + (1-Ï€) N(xáµ¢|Î¼â‚‚,Ïƒâ‚‚Â²)]
    """
    from scipy.stats import norm
    ll = np.sum(np.log(
        pi * norm.pdf(data, mu1, sig1) +
        (1 - pi) * norm.pdf(data, mu2, sig2) + 1e-10
    ))
    return ll

# Evaluate at true parameters vs single Gaussian
from scipy.stats import norm
ll_true = gmm_log_likelihood(data, pi_true, mu1_true, sigma1_true, mu2_true, sigma2_true)
ll_single = np.sum(np.log(norm.pdf(data, mu_single, sigma_single) + 1e-10))
print(f"\nLog-likelihood (true GMM params):  {ll_true:.2f}")
print(f"Log-likelihood (single Gaussian):  {ll_single:.2f}")
print(f"Difference: {ll_true - ll_single:.2f} (GMM is much better)")

# The challenge: MLE for GMM has no closed-form solution
# âˆ‚L/âˆ‚Î¼â‚ involves the "responsibility" Î³ which depends on all parameters
# â†’ EM algorithm (Lecture 8) solves this iteratively
print("\nâ†’ GMM MLE has no closed-form solution.")
print("â†’ The EM algorithm (ç¬¬8å›) iteratively maximizes the likelihood.")
print("â†’ Each E-step computes 'responsibilities', each M-step updates parameters.")
```

**ãªãœGMMã®MLEã¯é–‰ã˜ãŸå½¢ã§è§£ã‘ãªã„ã®ã‹**: å¯¾æ•°å°¤åº¦ã®ä¸­ã«**å’Œã®å¯¾æ•°** $\log[\pi \mathcal{N}(x \mid \mu_1, \sigma_1^2) + (1-\pi)\mathcal{N}(x \mid \mu_2, \sigma_2^2)]$ ãŒç¾ã‚Œã‚‹ã€‚å¯¾æ•°ã¨å’Œã®é †åºã‚’å…¥ã‚Œæ›¿ãˆã‚‰ã‚Œãªã„ãŸã‚ã€å¾®åˆ†ã—ã¦ã‚‚å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒäº’ã„ã«çµ¡ã¿åˆã†ã€‚ã“ã®å›°é›£ãŒç¬¬8å›ã®EMç®—æ³•ã®å‹•æ©Ÿã ã€‚

### 4.7 å®Ÿè£…æ¼”ç¿’: ãƒ™ã‚¤ã‚ºæ¨è«–ã®ã‚°ãƒªãƒƒãƒ‰è¿‘ä¼¼

```python
import numpy as np

def bayesian_grid_inference(data: np.ndarray, prior_a: float = 1.0, prior_b: float = 1.0,
                             n_grid: int = 10000):
    """Bayesian inference for Bernoulli parameter using grid approximation.

    Prior: Beta(a, b)
    Likelihood: Bernoulli(Î¸)
    Posterior âˆ Î¸^(a+h-1) (1-Î¸)^(b+t-1)

    Also computes posterior analytically for comparison.
    """
    theta_grid = np.linspace(0.001, 0.999, n_grid)
    heads = data.sum()
    tails = len(data) - heads

    # Prior: Beta(a, b)
    from math import gamma as gamma_fn
    B = gamma_fn(prior_a) * gamma_fn(prior_b) / gamma_fn(prior_a + prior_b)
    prior = theta_grid**(prior_a - 1) * (1 - theta_grid)**(prior_b - 1) / B

    # Likelihood: Bernoulli
    log_lik = heads * np.log(theta_grid) + tails * np.log(1 - theta_grid)
    likelihood = np.exp(log_lik - log_lik.max())  # numerical stability

    # Posterior âˆ Likelihood Ã— Prior
    posterior_unnorm = likelihood * prior
    posterior = posterior_unnorm / np.trapz(posterior_unnorm, theta_grid)

    # Analytical posterior: Beta(a+h, b+t)
    post_a = prior_a + heads
    post_b = prior_b + tails
    B_post = gamma_fn(post_a) * gamma_fn(post_b) / gamma_fn(post_a + post_b)
    posterior_analytic = theta_grid**(post_a - 1) * (1 - theta_grid)**(post_b - 1) / B_post

    # Summary statistics
    dx = theta_grid[1] - theta_grid[0]
    mean_grid = np.sum(theta_grid * posterior) * dx
    mean_analytic = post_a / (post_a + post_b)
    mle = heads / len(data) if len(data) > 0 else 0.5

    print(f"Data: {int(heads)}H / {int(tails)}T (N={len(data)})")
    print(f"Prior: Beta({prior_a}, {prior_b})")
    print(f"Posterior: Beta({post_a}, {post_b})")
    print(f"  Grid mean:      {mean_grid:.4f}")
    print(f"  Analytic mean:  {mean_analytic:.4f}")
    print(f"  MLE:            {mle:.4f}")
    print(f"  MAP:            {(post_a-1)/(post_a+post_b-2):.4f}" if post_a > 1 and post_b > 1 else "")

# Experiment with different data sizes and priors
np.random.seed(42)
true_theta = 0.7

print("=== Effect of Data Size ===\n")
for N in [5, 20, 100]:
    data = np.random.binomial(1, true_theta, N)
    bayesian_grid_inference(data)
    print()

print("=== Effect of Prior ===\n")
data_small = np.random.binomial(1, true_theta, 10)
for a, b, name in [(1, 1, "Uniform"), (0.5, 0.5, "Jeffreys"), (10, 3, "Strong prior Î¸â‰ˆ0.77"), (1, 10, "Wrong prior Î¸â‰ˆ0.09")]:
    print(f"--- {name} prior ---")
    bayesian_grid_inference(data_small, a, b)
    print()
```

:::message
**å®Ÿè£…ã®æ•™è¨“**: ãƒ‡ãƒ¼ã‚¿ãŒå¢—ãˆã‚‹ã»ã©ã€äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ã¯è–„ã‚Œã€ãƒ™ã‚¤ã‚ºæ¨å®šã¯MLEã«è¿‘ã¥ãã€‚ã“ã‚Œã¯äº‹å¾Œåˆ†å¸ƒãŒã€Œå°¤åº¦ã«æ”¯é…ã•ã‚Œã‚‹ã€ãŸã‚ã€‚é€†ã«ã€ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã¨ãã¯äº‹å‰åˆ†å¸ƒãŒçµæœã‚’å¤§ããå·¦å³ã™ã‚‹ã€‚

ã“ã®ç¾è±¡ã‚’ã€Œäº‹å¾Œä¸€è‡´æ€§ï¼ˆposterior consistencyï¼‰ã€ã¨å‘¼ã¶ã€‚$N \to \infty$ ã§äº‹å¾Œåˆ†å¸ƒã¯çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é›†ä¸­ã™ã‚‹ â€” å¤§æ•°ã®æ³•å‰‡ã®ãƒ™ã‚¤ã‚ºç‰ˆã ã€‚
:::

### 4.8 ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¯é–¢æ•°ã¨ç‰¹æ€§é–¢æ•°

**ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¯é–¢æ•°ï¼ˆMGFï¼‰**: $M_X(t) = \mathbb{E}[e^{tX}]$

MGFã® $k$ æ¬¡å¾®åˆ†ã¯ $k$ æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’ä¸ãˆã‚‹: $M_X^{(k)}(0) = \mathbb{E}[X^k]$

```python
import numpy as np

# MGF of Gaussian N(Î¼, ÏƒÂ²): M(t) = exp(Î¼t + ÏƒÂ²tÂ²/2)
mu, sigma = 2.0, 1.5

def gaussian_mgf(t: float, mu: float, sigma: float) -> float:
    """M(t) = exp(Î¼t + ÏƒÂ²tÂ²/2)"""
    return np.exp(mu * t + sigma**2 * t**2 / 2)

# Verify moments via numerical differentiation
dt = 1e-5
M0 = gaussian_mgf(0, mu, sigma)
M1 = (gaussian_mgf(dt, mu, sigma) - gaussian_mgf(-dt, mu, sigma)) / (2 * dt)
M2 = (gaussian_mgf(dt, mu, sigma) - 2*M0 + gaussian_mgf(-dt, mu, sigma)) / dt**2

print(f"Gaussian N({mu}, {sigma**2})")
print(f"E[X]  = M'(0) = {M1:.4f}  (theory: {mu})")
print(f"E[XÂ²] = M''(0) = {M2:.4f} (theory: {mu**2 + sigma**2:.4f})")
print(f"Var(X) = E[XÂ²] - E[X]Â² = {M2 - M1**2:.4f} (theory: {sigma**2:.4f})")

# Monte Carlo verification
samples = np.random.normal(mu, sigma, 100000)
print(f"\nMonte Carlo: E[X]={samples.mean():.4f}, E[XÂ²]={np.mean(samples**2):.4f}, Var={samples.var():.4f}")
```

MGFãŒå­˜åœ¨ã—ãªã„åˆ†å¸ƒã‚‚ã‚ã‚‹ï¼ˆCauchyåˆ†å¸ƒãªã©ï¼‰ã€‚ãã®å ´åˆã¯**ç‰¹æ€§é–¢æ•°** $\varphi_X(t) = \mathbb{E}[e^{itX}]$ ã‚’ä½¿ã†ã€‚ç‰¹æ€§é–¢æ•°ã¯å¸¸ã«å­˜åœ¨ã—ã€åˆ†å¸ƒã‚’ä¸€æ„ã«æ±ºå®šã™ã‚‹ã€‚CLTã®è¨¼æ˜ã¯ã—ã°ã—ã°ç‰¹æ€§é–¢æ•°ã‚’ç”¨ã„ã¦è¡Œã‚ã‚Œã‚‹ã€‚

:::message
**é€²æ—: 70% å®Œäº†** ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€LaTeXè¨˜æ³•ã€è«–æ–‡èª­è§£ã®3ãƒ‘ã‚¹ã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³7ãƒ‘ã‚¿ãƒ¼ãƒ³ã€GMMãƒ»ãƒ™ã‚¤ã‚ºæ¨è«–ã®å®Ÿè£…ã€MGFã¾ã§å®Œäº†ã€‚Zone 4 ã‚¯ãƒªã‚¢ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’æ—¥æœ¬èªã§èª­ã¿ä¸Šã’ã€å„è¨˜å·ã®æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

:::details Q1: $X \sim \mathcal{N}(\mu, \sigma^2)$
**èª­ã¿**: ã€Œç¢ºç‡å¤‰æ•°ã‚¨ãƒƒã‚¯ã‚¹ã¯æ­£è¦åˆ†å¸ƒãƒŸãƒ¥ãƒ¼ ã‚·ã‚°ãƒäºŒä¹—ã«ã—ãŸãŒã†ã€
- $X$: ç¢ºç‡å¤‰æ•°
- $\sim$: ã€Œã«ã—ãŸãŒã†ã€â€” ç¢ºç‡åˆ†å¸ƒã«å¾“ã†ã“ã¨ã‚’ç¤ºã™è¨˜å·
- $\mathcal{N}$: æ­£è¦åˆ†å¸ƒï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰
- $\mu$: å¹³å‡ï¼ˆãƒŸãƒ¥ãƒ¼ï¼‰
- $\sigma^2$: åˆ†æ•£ï¼ˆã‚·ã‚°ãƒã®äºŒä¹—ï¼‰
:::

:::details Q2: $P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}$
**èª­ã¿**: ã€Œãƒ”ãƒ¼ ã‚¨ãƒ¼ ã‚®ãƒ–ãƒ³ ãƒ“ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ”ãƒ¼ ãƒ“ãƒ¼ ã‚®ãƒ–ãƒ³ ã‚¨ãƒ¼ ã‹ã‘ã‚‹ ãƒ”ãƒ¼ ã‚¨ãƒ¼ ã‚ã‚‹ ãƒ”ãƒ¼ ãƒ“ãƒ¼ã€
- ã“ã‚Œã¯ãƒ™ã‚¤ã‚ºã®å®šç† [^1]
- $P(A \mid B)$: äº‹å¾Œç¢ºç‡ â€” $B$ ãŒèµ·ããŸä¸‹ã§ã® $A$ ã®ç¢ºç‡
- $P(B \mid A)$: å°¤åº¦ â€” $A$ ãŒçœŸã®ã¨ãã« $B$ ãŒè¦³æ¸¬ã•ã‚Œã‚‹ç¢ºç‡
- $P(A)$: äº‹å‰ç¢ºç‡ â€” $B$ ã‚’è¦‹ã‚‹å‰ã® $A$ ã®ç¢ºç‡
- $P(B)$: ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ â€” $B$ ã®å‘¨è¾ºç¢ºç‡
:::

:::details Q3: $\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{i=1}^{N} \log p(x_i; \theta)$
**èª­ã¿**: ã€Œã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆMLE ã‚¤ã‚³ãƒ¼ãƒ« ã‚¢ãƒ¼ã‚°ãƒãƒƒã‚¯ã‚¹ ã‚·ãƒ¼ã‚¿ ã‚·ã‚°ãƒ ã‚¢ã‚¤ ã‚¤ã‚³ãƒ¼ãƒ« 1 ã‹ã‚‰ ã‚¨ãƒŒ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ã‚¢ã‚¤ ã‚»ãƒŸã‚³ãƒ­ãƒ³ ã‚·ãƒ¼ã‚¿ã€
- $\hat{\theta}_{\text{MLE}}$: æœ€å°¤æ¨å®šé‡ï¼ˆãƒãƒƒãƒˆã¯ã€Œæ¨å®šé‡ã€ã®å°ï¼‰
- $\arg\max_\theta$: $\theta$ ã‚’å‹•ã‹ã—ã¦æœ€å¤§ã«ã™ã‚‹å€¤
- $\sum_{i=1}^{N}$: $N$ å€‹ã®ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã®å’Œ
- $\log p(x_i; \theta)$: $i$ ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿ã®å¯¾æ•°å°¤åº¦
- ã‚»ãƒŸã‚³ãƒ­ãƒ³ ;: $x_i$ ã¯ãƒ‡ãƒ¼ã‚¿ï¼ˆå›ºå®šï¼‰ã€$\theta$ ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¤‰æ•°ï¼‰ã‚’åŒºåˆ¥
:::

:::details Q4: $I(\theta) = -\mathbb{E}\left[\nabla_\theta^2 \log p(\mathbf{x}; \theta)\right]$
**èª­ã¿**: ã€Œã‚¢ã‚¤ ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ ã‚¤ãƒ¼ ãƒŠãƒ–ãƒ©ã‚·ãƒ¼ã‚¿äºŒä¹— ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚»ãƒŸã‚³ãƒ­ãƒ³ ã‚·ãƒ¼ã‚¿ã€
- $I(\theta)$: Fisheræƒ…å ±é‡ï¼ˆè¡Œåˆ—ï¼‰[^8]
- $-\mathbb{E}[\cdot]$: æœŸå¾…å€¤ã®ãƒã‚¤ãƒŠã‚¹
- $\nabla_\theta^2$: $\theta$ ã«é–¢ã™ã‚‹ãƒ˜ã‚·ã‚¢ãƒ³ï¼ˆ2æ¬¡å¾®åˆ†ï¼‰
- ç›´æ„Ÿ: å¯¾æ•°å°¤åº¦ã®æ›²ç‡ã®æœŸå¾…å€¤ã€‚æ›²ç‡ãŒå¤§ãã„ = ãƒ‡ãƒ¼ã‚¿ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦æƒ…å ±ã‚’å¤šãæŒã¤
:::

:::details Q5: $\sqrt{N}(\bar{X}_N - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)$
**èª­ã¿**: ã€Œãƒ«ãƒ¼ãƒˆã‚¨ãƒŒ ã‹ã‘ã‚‹ ã‚¨ãƒƒã‚¯ã‚¹ãƒãƒ¼ã‚¨ãƒŒ ãƒã‚¤ãƒŠã‚¹ ãƒŸãƒ¥ãƒ¼ ã¯ åˆ†å¸ƒåæŸã§ ãƒãƒ¼ãƒãƒ« ã‚¼ãƒ­ ã‚·ã‚°ãƒäºŒä¹— ã«åæŸã™ã‚‹ã€
- $\bar{X}_N = \frac{1}{N}\sum_{i=1}^{N}X_i$: æ¨™æœ¬å¹³å‡
- $\xrightarrow{d}$: åˆ†å¸ƒåæŸï¼ˆdistribution convergenceï¼‰
- ã“ã‚Œã¯ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCLTï¼‰ã®è¡¨ç¾
- $\sqrt{N}$ ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€åˆ†æ•£ãŒä¸€å®šã®ã¾ã¾åˆ†å¸ƒå½¢çŠ¶ãŒã‚¬ã‚¦ã‚¹ã«è¿‘ã¥ã
:::

:::details Q6: $p(\mathbf{x}) = \prod_{t=1}^{T} p(x_t \mid x_{<t})$
**èª­ã¿**: ã€Œãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« 1 ã‹ã‚‰ ãƒ†ã‚£ãƒ¼ ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ãƒ†ã‚£ãƒ¼ ã‚®ãƒ–ãƒ³ ã‚¨ãƒƒã‚¯ã‚¹ ãƒ¬ã‚¹ã‚¶ãƒ³ ãƒ†ã‚£ãƒ¼ã€
- $p(\mathbf{x})$: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®åŒæ™‚ç¢ºç‡
- $\prod_{t=1}^{T}$: æ™‚åˆ»1ã‹ã‚‰Tã¾ã§ã®ç©
- $p(x_t \mid x_{<t})$: éå»ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’æ¡ä»¶ã¨ã—ãŸ $x_t$ ã®æ¡ä»¶ä»˜ãç¢ºç‡
- ã“ã‚Œã¯ç¢ºç‡ã®é€£é–è¦å‰‡ã€‚LLMã®è‡ªå·±å›å¸°ç”Ÿæˆã®æ•°å­¦çš„åŸºç›¤ [^5]
:::

:::details Q7: $\text{Var}(\hat{\theta}) \geq \frac{1}{N \cdot I(\theta)}$
**èª­ã¿**: ã€Œãƒãƒªã‚¢ãƒ³ã‚¹ ã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆ ã¯ 1 ã‚ã‚‹ ã‚¨ãƒŒ ã‚¢ã‚¤ ã‚·ãƒ¼ã‚¿ ä»¥ä¸Šã€
- ã“ã‚Œã¯CramÃ©r-Raoä¸‹ç•Œ [^8]
- ã©ã‚“ãªä¸åæ¨å®šé‡ã§ã‚‚ã€åˆ†æ•£ã¯Fisheræƒ…å ±é‡ã®é€†æ•°ä»¥ä¸‹ã«ã¯ãªã‚‰ãªã„
- $N$ ãŒå¢—ãˆã‚‹ã¨ä¸‹ç•Œã¯å°ã•ããªã‚‹ = ã‚ˆã‚Šç²¾å¯†ãªæ¨å®šãŒå¯èƒ½
:::

:::details Q8: $p(\mathbf{x} \mid \boldsymbol{\eta}) = h(\mathbf{x}) \exp(\boldsymbol{\eta}^\top \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta}))$
**èª­ã¿**: ã€Œãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚®ãƒ–ãƒ³ ã‚¤ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ã‚¤ãƒ ã‚¨ãƒƒã‚¯ã‚¹ ã‹ã‘ã‚‹ ã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ« ã‚¤ãƒ¼ã‚¿ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚º ãƒ†ã‚£ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ãƒã‚¤ãƒŠã‚¹ ã‚¨ãƒ¼ ã‚¤ãƒ¼ã‚¿ã€
- ã“ã‚Œã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®æ¨™æº–å½¢
- $\boldsymbol{\eta}$: è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¤ãƒ¼ã‚¿ï¼‰
- $\mathbf{T}(\mathbf{x})$: ååˆ†çµ±è¨ˆé‡
- $A(\boldsymbol{\eta})$: å¯¾æ•°æ­£è¦åŒ–å®šæ•°ï¼ˆå¯¾æ•°åˆ†é…é–¢æ•°ï¼‰
- $h(\mathbf{x})$: åŸºåº•æ¸¬åº¦
:::

:::details Q9: $q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}_\phi(\mathbf{x}), \text{diag}(\boldsymbol{\sigma}^2_\phi(\mathbf{x})))$
**èª­ã¿**: ã€Œã‚­ãƒ¥ãƒ¼ ãƒ•ã‚¡ã‚¤ ã‚¼ãƒƒãƒˆ ã‚®ãƒ–ãƒ³ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¤ã‚³ãƒ¼ãƒ« ãƒãƒ¼ãƒãƒ« ãƒŸãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¤ ã‚¨ãƒƒã‚¯ã‚¹ ãƒ€ã‚¤ã‚¢ã‚° ã‚·ã‚°ãƒãƒ•ã‚¡ã‚¤äºŒä¹— ã‚¨ãƒƒã‚¯ã‚¹ã€
- $q_\phi$: å¤‰åˆ†è¿‘ä¼¼åˆ†å¸ƒï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\phi$ ã®NNï¼‰[^2]
- $\mathbf{z}$: æ½œåœ¨å¤‰æ•°
- $\boldsymbol{\mu}_\phi(\mathbf{x})$: NNãŒå‡ºåŠ›ã™ã‚‹å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«
- $\text{diag}(\boldsymbol{\sigma}^2_\phi(\mathbf{x}))$: NNãŒå‡ºåŠ›ã™ã‚‹å¯¾è§’å…±åˆ†æ•£è¡Œåˆ—
- ã“ã‚Œã¯VAEã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã€‚ç¬¬10å›ã§å®Œå…¨å°å‡ºã€‚
:::

:::details Q10: $\mathcal{L}(\theta) = -\frac{1}{T}\sum_{t=1}^{T} \log p_\theta(x_t \mid x_{<t})$
**èª­ã¿**: ã€Œã‚¨ãƒ« ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ ãƒ†ã‚£ãƒ¼ã¶ã‚“ã®ã‚¤ãƒ ã‚·ã‚°ãƒ ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« 1 ã‹ã‚‰ ãƒ†ã‚£ãƒ¼ ãƒ­ã‚° ãƒ”ãƒ¼ã‚·ãƒ¼ã‚¿ ã‚¨ãƒƒã‚¯ã‚¹ãƒ†ã‚£ãƒ¼ ã‚®ãƒ–ãƒ³ ã‚¨ãƒƒã‚¯ã‚¹ãƒ¬ã‚¹ã‚¶ãƒ³ãƒ†ã‚£ãƒ¼ã€
- ã“ã‚Œã¯LLMã®Cross-Entropy Lossï¼ˆç¬¬1å›ã§å°å…¥ï¼‰
- $-\log p_\theta(x_t \mid x_{<t})$: æ­£è§£ãƒˆãƒ¼ã‚¯ãƒ³ã®è² ã®å¯¾æ•°ç¢ºç‡
- $\frac{1}{T}\sum_{t=1}^{T}$: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®å¹³å‡
- Perplexity $= \exp(\mathcal{L})$ ã¯ã€Œå®ŸåŠ¹çš„ãªé¸æŠè‚¢æ•°ã€
:::

### 5.2 LaTeXè¨˜è¿°ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’LaTeXã§è¨˜è¿°ã›ã‚ˆã€‚

:::details Q1: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®PDF
```latex
f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
```
:::

:::details Q2: ãƒ™ã‚¤ã‚ºã®å®šç†ï¼ˆé€£ç¶šç‰ˆï¼‰
```latex
p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta) p(\theta)}{p(\mathcal{D})}
= \frac{p(\mathcal{D} \mid \theta) p(\theta)}{\int p(\mathcal{D} \mid \theta') p(\theta') d\theta'}
```
:::

:::details Q3: ä¸­å¿ƒæ¥µé™å®šç†
```latex
\frac{\bar{X}_N - \mu}{\sigma / \sqrt{N}} \xrightarrow{d} \mathcal{N}(0, 1)
\quad \text{as } N \to \infty
```
:::

:::details Q4: æŒ‡æ•°å‹åˆ†å¸ƒæ—
```latex
p(\mathbf{x} \mid \boldsymbol{\eta}) = h(\mathbf{x}) \exp\left(\boldsymbol{\eta}^\top \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta})\right)
```
:::

:::details Q5: Fisheræƒ…å ±é‡ã¨CramÃ©r-Raoä¸‹ç•Œ
```latex
I(\theta) = \mathbb{E}\left[\left(\frac{\partial}{\partial \theta} \log p(X; \theta)\right)^2\right],
\quad \text{Var}(\hat{\theta}) \geq \frac{1}{n I(\theta)}
```
:::

### 5.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

:::details Q1: æ¡ä»¶ä»˜ãç¢ºç‡ã®è¨ˆç®—
æ•°å¼: $P(Y=1 \mid X=0) = \frac{P(X=0, Y=1)}{P(X=0)} = \frac{P(X=0, Y=1)}{\sum_y P(X=0, Y=y)}$

```python
joint = np.array([[0.3, 0.1], [0.2, 0.4]])  # P(X,Y)
p_y1_given_x0 = joint[0, 1] / joint[0, :].sum()
print(f"P(Y=1|X=0) = {p_y1_given_x0:.4f}")
# Expected: 0.1 / (0.3 + 0.1) = 0.25
```
:::

:::details Q2: MLE for Poisson distribution
æ•°å¼: $\hat{\lambda}_{\text{MLE}} = \bar{x} = \frac{1}{N}\sum_{i=1}^{N} x_i$

```python
data = np.random.poisson(lam=4.5, size=1000)
lambda_mle = data.mean()
print(f"Î»_MLE = {lambda_mle:.4f} (true: 4.5)")
```
:::

:::details Q3: 2D Gaussian sampling and Mahalanobis distance
æ•°å¼: $d_M(\mathbf{x}) = \sqrt{(\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu})}$

```python
mu = np.array([1.0, 2.0])
Sigma = np.array([[2.0, 0.8], [0.8, 1.0]])
Sigma_inv = np.linalg.inv(Sigma)

x = np.array([3.0, 4.0])
diff = x - mu
d_mahal = np.sqrt(diff @ Sigma_inv @ diff)
d_euclid = np.linalg.norm(diff)
print(f"Mahalanobis: {d_mahal:.4f}, Euclidean: {d_euclid:.4f}")
```
:::

:::details Q4: CLT verification â€” exponential distribution
æ•°å¼: $X_i \sim \text{Exp}(\lambda)$, $\mathbb{E}[X_i] = 1/\lambda$, $\text{Var}(X_i) = 1/\lambda^2$

```python
lam = 2.0
N = 100
n_experiments = 50000

means = np.array([np.random.exponential(1/lam, N).mean() for _ in range(n_experiments)])
standardized = (means - 1/lam) / (1/(lam * np.sqrt(N)))
print(f"Standardized mean: {standardized.mean():.4f} (should â‰ˆ 0)")
print(f"Standardized std:  {standardized.std():.4f} (should â‰ˆ 1)")
```
:::

:::details Q5: Beta-Bernoulli conjugate update
æ•°å¼: Prior $\text{Beta}(\alpha, \beta)$ + Data $(h, t)$ â†’ Posterior $\text{Beta}(\alpha+h, \beta+t)$

```python
alpha, beta = 2.0, 5.0  # prior: we think Î¸ is low
data = np.array([1, 1, 1, 0, 1, 1, 0, 1, 1, 1])  # 8 heads, 2 tails
h, t = data.sum(), len(data) - data.sum()
post_a, post_b = alpha + h, beta + t
print(f"Prior mean: {alpha/(alpha+beta):.3f}")
print(f"Posterior mean: {post_a/(post_a+post_b):.3f}")
print(f"MLE: {h/len(data):.3f}")
# Posterior is pulled between prior and MLE
```
:::

### 5.4 è«–æ–‡èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®VAEè«–æ–‡ [^2] ã®Abstractã‹ã‚‰ç¢ºç‡è«–ã®è¦ç´ ã‚’æŠ½å‡ºã›ã‚ˆã€‚

:::details VAEåŸè«–æ–‡ Pass 1
**Kingma & Welling (2013). "Auto-Encoding Variational Bayes"**

ç¢ºç‡è«–çš„è¦ç´ ã®æŠ½å‡º:
1. **æ½œåœ¨å¤‰æ•°**: $\mathbf{z}$ â€” è¦³æ¸¬ã•ã‚Œãªã„ç¢ºç‡å¤‰æ•°
2. **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«**: $p_\theta(\mathbf{x} \mid \mathbf{z})$ â€” æ¡ä»¶ä»˜ãåˆ†å¸ƒï¼ˆãƒ‡ã‚³ãƒ¼ãƒ€ï¼‰
3. **äº‹å‰åˆ†å¸ƒ**: $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ â€” æ¨™æº–æ­£è¦åˆ†å¸ƒ
4. **äº‹å¾Œåˆ†å¸ƒ**: $p_\theta(\mathbf{z} \mid \mathbf{x})$ â€” ãƒ™ã‚¤ã‚ºã®å®šç†ã§å¾—ã‚‰ã‚Œã‚‹ãŒè¨ˆç®—å›°é›£
5. **å¤‰åˆ†è¿‘ä¼¼**: $q_\phi(\mathbf{z} \mid \mathbf{x})$ â€” äº‹å¾Œåˆ†å¸ƒã®è¿‘ä¼¼ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼‰
6. **ELBO**: $\mathcal{L}(\theta, \phi; \mathbf{x}) = \mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x} \mid \mathbf{z})] - D_{KL}[q_\phi(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z})]$
7. **MLE**: ELBOã®æœ€å¤§åŒ–ã¯å‘¨è¾ºå°¤åº¦ $\log p_\theta(\mathbf{x})$ ã®ä¸‹ç•Œã‚’æœ€å¤§åŒ–
8. **Reparameterization trick**: $\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}$, $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$

æœ¬è¬›ç¾©ã§å­¦ã‚“ã å…¨ã¦ã®é“å…·ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚
:::

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**Challenge 1**: ãƒ™ã‚¤ã‚ºæ¨è«–ã®å¯è¦–åŒ–

```python
import numpy as np

def bayesian_sequential_visualization(true_p: float, n_obs: int, prior_a: float = 1, prior_b: float = 1):
    """Visualize sequential Bayesian updating via summary statistics."""
    np.random.seed(42)
    a, b = prior_a, prior_b

    print(f"True Î¸ = {true_p}")
    print(f"{'Obs':>4} {'Data':>5} {'Post Mean':>10} {'Post Std':>10} {'95% CI':>20} {'MLE':>8}")
    print("-" * 65)

    heads_total, tails_total = 0, 0
    for i in range(1, n_obs + 1):
        x = np.random.binomial(1, true_p)
        heads_total += x
        tails_total += (1 - x)
        a_new = prior_a + heads_total
        b_new = prior_b + tails_total
        mean = a_new / (a_new + b_new)
        std = np.sqrt(a_new * b_new / ((a_new + b_new)**2 * (a_new + b_new + 1)))
        # 95% credible interval (approximate via normal)
        ci_low = max(0, mean - 1.96 * std)
        ci_high = min(1, mean + 1.96 * std)
        mle = heads_total / i
        if i <= 10 or i % 10 == 0 or i == n_obs:
            print(f"{i:>4} {'H' if x else 'T':>5} {mean:>10.4f} {std:>10.4f} "
                  f"[{ci_low:.3f}, {ci_high:.3f}]{' ':>4} {mle:>8.4f}")

bayesian_sequential_visualization(0.65, 100)
```

**Challenge 2**: Fisheræƒ…å ±é‡ã®æ•°å€¤è¨ˆç®—

```python
import numpy as np

def numerical_fisher_information(log_pdf_fn, theta: float, n_samples: int = 100000, dt: float = 1e-5):
    """Numerically compute Fisher information.

    I(Î¸) = E[(d/dÎ¸ log p(x;Î¸))Â²]

    Uses score function variance and Hessian methods, compares both.
    """
    # Method 1: Score function variance
    # Sample from p(x; Î¸) â€” need a sampler
    # For Bernoulli: x ~ Bernoulli(Î¸)
    samples = np.random.binomial(1, theta, n_samples).astype(float)

    # Score: d/dÎ¸ log p(x; Î¸) = x/Î¸ - (1-x)/(1-Î¸)
    scores = samples / theta - (1 - samples) / (1 - theta)
    I_score = np.mean(scores**2)

    # Method 2: Negative expected Hessian
    # dÂ²/dÎ¸Â² log p(x; Î¸) = -x/Î¸Â² - (1-x)/(1-Î¸)Â²
    hessians = -samples / theta**2 - (1 - samples) / (1 - theta)**2
    I_hessian = -np.mean(hessians)

    # Theory: I(Î¸) = 1/(Î¸(1-Î¸))
    I_theory = 1 / (theta * (1 - theta))

    print(f"Î¸ = {theta}")
    print(f"  Score variance:  I = {I_score:.4f}")
    print(f"  Negative Hessian: I = {I_hessian:.4f}")
    print(f"  Theory:          I = {I_theory:.4f}")
    return I_score, I_hessian, I_theory

print("=== Numerical Fisher Information for Bernoulli ===\n")
for theta in [0.1, 0.3, 0.5, 0.7, 0.9]:
    numerical_fisher_information(None, theta)
    print()
```

### 5.6 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸3: MLEæ¯”è¼ƒ â€” åˆ†å¸ƒã®å½“ã¦ã¯ã‚

```python
import numpy as np
from scipy.stats import norm, expon, gamma as gamma_dist

def fit_and_compare(data: np.ndarray):
    """Fit multiple distributions to data and compare log-likelihoods."""
    results = []

    # Gaussian MLE
    mu, sigma = data.mean(), data.std()
    ll_gauss = np.sum(norm.logpdf(data, mu, sigma))
    results.append(("Gaussian", ll_gauss, f"Î¼={mu:.3f}, Ïƒ={sigma:.3f}"))

    # Exponential MLE (for positive data only)
    if data.min() > 0:
        lam = 1 / data.mean()
        ll_exp = np.sum(expon.logpdf(data, scale=1/lam))
        results.append(("Exponential", ll_exp, f"Î»={lam:.3f}"))

    # Gamma MLE (method of moments)
    if data.min() > 0:
        mean_d = data.mean()
        var_d = data.var()
        alpha_hat = mean_d**2 / var_d
        beta_hat = mean_d / var_d
        ll_gamma = np.sum(gamma_dist.logpdf(data, alpha_hat, scale=1/beta_hat))
        results.append(("Gamma", ll_gamma, f"Î±={alpha_hat:.3f}, Î²={beta_hat:.3f}"))

    print(f"{'Distribution':<15} {'Log-Lik':>12} {'Parameters':<30}")
    print("-" * 60)
    for name, ll, params in sorted(results, key=lambda x: -x[1]):
        print(f"{name:<15} {ll:>12.2f} {params:<30}")
    print(f"\nBest fit: {sorted(results, key=lambda x: -x[1])[0][0]}")

# Test 1: Data from Gamma(3, 2)
np.random.seed(42)
data_gamma = np.random.gamma(3, 0.5, 500)  # shape=3, scale=0.5
print("=== Fitting data from Gamma(3, 0.5) ===\n")
fit_and_compare(data_gamma)

# Test 2: Data from mixture of Gaussians
data_mix = np.concatenate([
    np.random.normal(-2, 0.5, 300),
    np.random.normal(2, 0.8, 200)
])
print("\n=== Fitting data from Gaussian Mixture ===\n")
fit_and_compare(data_mix)
print("â†’ Single Gaussian is a bad fit for bimodal data!")
```

### 5.7 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸4: ç¢ºç‡å¤‰æ•°ã®å¤‰æ›ã‚’å¯è¦–åŒ–

```python
import numpy as np

def visualize_transform_stats(n_samples: int = 100000):
    """Demonstrate change of variables with various transformations."""
    np.random.seed(42)

    # Standard normal samples
    X = np.random.normal(0, 1, n_samples)

    transforms = [
        ("XÂ²", lambda x: x**2, "Chi-squared(1)"),
        ("exp(X)", lambda x: np.exp(x), "Log-Normal(0,1)"),
        ("|X|", lambda x: np.abs(x), "Half-Normal"),
        ("Î¦(X)", lambda x: norm.cdf(x), "Uniform(0,1)"),
        ("XÂ³", lambda x: x**3, "Heavy-tailed"),
    ]

    from scipy.stats import norm as norm_dist

    print(f"{'Transform':<12} {'Mean':>8} {'Std':>8} {'Skew':>8} {'Kurt':>8} {'Min':>8} {'Max':>8}")
    print("-" * 60)

    for name, fn, desc in transforms:
        Y = fn(X)
        # Remove inf/nan for safety
        Y = Y[np.isfinite(Y)]
        mean = Y.mean()
        std = Y.std()
        skew = np.mean(((Y - mean) / std)**3)
        kurt = np.mean(((Y - mean) / std)**4) - 3  # excess kurtosis
        print(f"{name:<12} {mean:>8.4f} {std:>8.4f} {skew:>8.4f} {kurt:>8.4f} {Y.min():>8.4f} {Y.max():>8.2f}")

    print(f"\nKey insight: Î¦(X) ~ Uniform(0,1) â€” the probability integral transform")
    print("This is the foundation of inverse transform sampling.")

visualize_transform_stats()
```

:::message
**ç¢ºç‡ç©åˆ†å¤‰æ›**: $X \sim F$ ã®ã¨ã $F(X) \sim \text{Uniform}(0,1)$ã€‚é€†ã« $U \sim \text{Uniform}(0,1)$ ã‹ã‚‰ $F^{-1}(U)$ ã§ä»»æ„ã®åˆ†å¸ƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãã‚‹ã€‚ã“ã‚ŒãŒé€†å¤‰æ›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åŸç†ã§ã‚ã‚Šã€Normalizing Flowï¼ˆç¬¬25å›ï¼‰ã®ç†è«–çš„å‡ºç™ºç‚¹ã§ã‚‚ã‚ã‚‹ã€‚
:::

### 5.8 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸5: å…±åˆ†æ•£è¡Œåˆ—ã®å›ºæœ‰å€¤åˆ†è§£ã¨ç¢ºç‡æ¥•å††

```python
import numpy as np

def probability_ellipse(mu: np.ndarray, Sigma: np.ndarray, n_samples: int = 5000):
    """Compute probability ellipse properties from covariance matrix."""
    # Eigendecomposition of Î£
    eigenvalues, eigenvectors = np.linalg.eigh(Sigma)

    print(f"Î¼ = {mu}")
    print(f"Î£ = \n{Sigma}")
    print(f"\nEigenvalues: {eigenvalues}")
    print(f"Eigenvectors:\n{eigenvectors}")

    # Ellipse axes: sqrt(eigenvalue) * chi2_quantile
    # For 95% confidence: chi2(2, 0.95) â‰ˆ 5.991
    chi2_95 = 5.991
    axis_lengths = np.sqrt(eigenvalues * chi2_95)
    angle = np.arctan2(eigenvectors[1, 1], eigenvectors[0, 1]) * 180 / np.pi

    print(f"\n95% probability ellipse:")
    print(f"  Semi-axis 1: {axis_lengths[0]:.4f}")
    print(f"  Semi-axis 2: {axis_lengths[1]:.4f}")
    print(f"  Rotation angle: {angle:.1f}Â°")
    print(f"  Area: {np.pi * axis_lengths[0] * axis_lengths[1]:.4f}")

    # Verify with samples
    samples = np.random.multivariate_normal(mu, Sigma, n_samples)
    # Mahalanobis distance for each sample
    diff = samples - mu
    Sigma_inv = np.linalg.inv(Sigma)
    mahal_sq = np.sum(diff @ Sigma_inv * diff, axis=1)
    # Points inside 95% ellipse have mahal_sq < chi2_95
    inside_95 = (mahal_sq < chi2_95).mean()
    print(f"\n  Empirical coverage (95% ellipse): {inside_95:.1%}")

    # Correlation coefficient
    rho = Sigma[0, 1] / np.sqrt(Sigma[0, 0] * Sigma[1, 1])
    print(f"  Correlation: Ï = {rho:.4f}")

# Example: highly correlated 2D Gaussian
mu = np.array([1.0, 2.0])
Sigma = np.array([[2.0, 1.5],
                   [1.5, 3.0]])
probability_ellipse(mu, Sigma)
```

### 5.9 ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] ç¢ºç‡ç©ºé–“ $(\Omega, \mathcal{F}, P)$ ã®3è¦ç´ ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Ïƒ-åŠ æ³•æ—ãŒãªãœå¿…è¦ã‹ã€ç›´æ„Ÿçš„ã«èª¬æ˜ã§ãã‚‹
- [ ] æœŸå¾…å€¤ã®ç·šå½¢æ€§ã‚’è¨¼æ˜ãªã—ã§ä½¿ãˆã‚‹
- [ ] ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’å°å‡ºã—ã€äº‹å¾Œâˆå°¤åº¦Ã—äº‹å‰ã¨è¨€ãˆã‚‹
- [ ] å…±å½¹äº‹å‰åˆ†å¸ƒã®æ„å‘³ã¨ä¸»è¦ãªçµ„ã¿åˆã‚ã›ã‚’3ã¤ä»¥ä¸ŠæŒ™ã’ã‚‰ã‚Œã‚‹
- [ ] ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãƒ»ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒãƒ»ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã®PDFã‚’æ›¸ã‘ã‚‹
- [ ] å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’å°å‡ºã§ãã‚‹
- [ ] æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®æ¨™æº–å½¢ã‚’æ›¸ãã€ã‚¬ã‚¦ã‚¹ã‚’å½“ã¦ã¯ã‚ã‚‰ã‚Œã‚‹
- [ ] MLEã®å°å‡ºæ‰‹é †ï¼ˆå¯¾æ•°å°¤åº¦â†’å¾®åˆ†â†’0ã¨ãŠãï¼‰ã‚’å®Ÿè¡Œã§ãã‚‹
- [ ] Fisheræƒ…å ±é‡ã®å®šç¾©ã¨2ã¤ã®è¡¨ç¾ã‚’æ›¸ã‘ã‚‹
- [ ] CramÃ©r-Raoä¸‹ç•Œã‚’è¿°ã¹ã€MLEã®æ¼¸è¿‘æœ‰åŠ¹æ€§ã¨æ¥ç¶šã§ãã‚‹
- [ ] CLTã‚’è¿°ã¹ã€ãªãœã‚¬ã‚¦ã‚¹ãŒé »å‡ºã™ã‚‹ã‹èª¬æ˜ã§ãã‚‹
- [ ] LLMã®æå¤±é–¢æ•°ã‚’æ¡ä»¶ä»˜ãç¢ºç‡ã®é€£é–è¦å‰‡ã§åˆ†è§£ã§ãã‚‹

:::message
**é€²æ—: 85% å®Œäº†** è¨˜å·èª­è§£10å•ã€LaTeX5å•ã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³5å•ã€è«–æ–‡èª­è§£1å•ã€å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸2å•ã‚’ã‚¯ãƒªã‚¢ã€‚Zone 5 å®Œäº†ã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

| ãƒªã‚½ãƒ¼ã‚¹ | URL | ç‰¹å¾´ |
|:---------|:----|:-----|
| 3Blue1Brown: Bayes | YouTube | è¦–è¦šçš„ç›´æ„Ÿ |
| StatQuest | YouTube | çµ±è¨ˆã®åŸºç¤ã‚’ä¸å¯§ã« |
| MIT 18.650 | OCW | æ•°ç†çµ±è¨ˆã®è¬›ç¾© |
| Stanford CS229 Notes | Web | MLè¦–ç‚¹ã®ç¢ºç‡è«– |

:::details ç”¨èªé›† â€” æœ¬è¬›ç¾©ã®å…¨ç”¨èª

| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| ç¢ºç‡ç©ºé–“ | Probability space | $(\Omega, \mathcal{F}, P)$ ã®ä¸‰ã¤çµ„ |
| Ïƒ-åŠ æ³•æ— | Ïƒ-algebra | è£œé›†åˆãƒ»å¯ç®—åˆä½µã§é–‰ã˜ãŸäº‹è±¡ã®æ— |
| ç¢ºç‡æ¸¬åº¦ | Probability measure | $P: \mathcal{F} \to [0,1]$, $P(\Omega)=1$ |
| ç¢ºç‡å¤‰æ•° | Random variable | å¯æ¸¬é–¢æ•° $X: \Omega \to \mathbb{R}$ |
| æœŸå¾…å€¤ | Expectation | $\mathbb{E}[X] = \int x \, dP$ |
| åˆ†æ•£ | Variance | $\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]$ |
| å…±åˆ†æ•£ | Covariance | $\text{Cov}(X,Y) = \mathbb{E}[(X-\mu_X)(Y-\mu_Y)]$ |
| æ¡ä»¶ä»˜ãç¢ºç‡ | Conditional probability | $P(A \mid B) = P(A \cap B)/P(B)$ |
| ãƒ™ã‚¤ã‚ºã®å®šç† | Bayes' theorem | äº‹å¾Œâˆå°¤åº¦Ã—äº‹å‰ |
| å…±å½¹äº‹å‰åˆ†å¸ƒ | Conjugate prior | äº‹å¾Œã¨åŒã˜åˆ†å¸ƒæ—ã®äº‹å‰åˆ†å¸ƒ |
| æŒ‡æ•°å‹åˆ†å¸ƒæ— | Exponential family | $p(x \mid \eta) = h(x)\exp(\eta^\top T(x) - A(\eta))$ |
| ååˆ†çµ±è¨ˆé‡ | Sufficient statistic | ãƒ‡ãƒ¼ã‚¿ã®å…¨æƒ…å ±ã‚’ä¿æŒã™ã‚‹çµ±è¨ˆé‡ |
| æœ€å°¤æ¨å®š | MLE | $\hat{\theta} = \arg\max \sum \log p(x_i; \theta)$ |
| MAPæ¨å®š | MAP | MLE + äº‹å‰åˆ†å¸ƒ |
| Fisheræƒ…å ±é‡ | Fisher information | $I(\theta) = \mathbb{E}[s(x;\theta)s(x;\theta)^\top]$ |
| CramÃ©r-Raoä¸‹ç•Œ | CramÃ©r-Rao bound | $\text{Var}(\hat{\theta}) \geq 1/(nI(\theta))$ |
| å¤§æ•°ã®æ³•å‰‡ | Law of large numbers | $\bar{X}_N \to \mu$ |
| ä¸­å¿ƒæ¥µé™å®šç† | Central limit theorem | $\sqrt{N}(\bar{X}_N - \mu) \to \mathcal{N}(0,\sigma^2)$ |
| ç‹¬ç«‹åŒåˆ†å¸ƒ | i.i.d. | å„ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒåŒã˜åˆ†å¸ƒã‹ã‚‰ç‹¬ç«‹ã«ã‚µãƒ³ãƒ—ãƒ« |
:::

```mermaid
mindmap
  root((ç¬¬4å›: ç¢ºç‡è«–))
    ç¢ºç‡ç©ºé–“
      Kolmogorovå…¬ç†
      Ïƒ-åŠ æ³•æ—
      ç¢ºç‡æ¸¬åº¦
    ç¢ºç‡å¤‰æ•°
      æœŸå¾…å€¤ãƒ»åˆ†æ•£
      ç‹¬ç«‹æ€§
      i.i.d.
    ãƒ™ã‚¤ã‚ºæ¨è«–
      ãƒ™ã‚¤ã‚ºã®å®šç†
      å…±å½¹äº‹å‰åˆ†å¸ƒ
      MAPæ¨å®š
    ç¢ºç‡åˆ†å¸ƒ
      é›¢æ•£: Bernoulli, Categorical
      é€£ç¶š: Gaussian, Gamma, Beta
      å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ
      æŒ‡æ•°å‹åˆ†å¸ƒæ—
    æ¨å®šç†è«–
      MLE
      Fisheræƒ…å ±é‡
      CramÃ©r-Raoä¸‹ç•Œ
    æ¥µé™å®šç†
      å¤§æ•°ã®æ³•å‰‡
      ä¸­å¿ƒæ¥µé™å®šç†
      Berry-Esseené™ç•Œ
    LLMæ¥ç¶š
      æ¡ä»¶ä»˜ãç¢ºç‡
      è‡ªå·±å›å¸°ç”Ÿæˆ
      Categoricalåˆ†å¸ƒ
```

:::message
**é€²æ—: 90% å®Œäº†** ç¢ºç‡è«–ã®ç ”ç©¶ç³»è­œã€æ¨è–¦æ›¸ç±ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹ã€ç”¨èªé›†ã€çŸ¥è­˜ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ã‚’ç¶²ç¾…ã€‚Zone 6 ã‚¯ãƒªã‚¢ã€‚
:::

---

### 6.2 æœ¬è¬›ç¾©ã®æ ¸å¿ƒ â€” 3ã¤ã®æŒã¡å¸°ã‚Š

1. **ç¢ºç‡ã¯ã€Œã‚ã‹ã‚‰ãªã•ã€ã®è¨€èªã§ã‚ã‚‹ã€‚** ç¢ºç‡ç©ºé–“ $(\Omega, \mathcal{F}, P)$ ã¨ã„ã†å³å¯†ãªæ çµ„ã¿ã®ä¸Šã«ã€ç¢ºç‡å¤‰æ•°ãƒ»æœŸå¾…å€¤ãƒ»æ¡ä»¶ä»˜ãç¢ºç‡ãŒå®šç¾©ã•ã‚Œã‚‹ã€‚ã“ã®è¨€èªãªã—ã«ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯è¨˜è¿°ã§ããªã„ã€‚

2. **ãƒ™ã‚¤ã‚ºã®å®šç†ã¯ã€Œå­¦ç¿’ã€ã®æ•°å¼ã ã€‚** äº‹å‰åˆ†å¸ƒï¼ˆä¿¡å¿µï¼‰+ å°¤åº¦ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰â†’ äº‹å¾Œåˆ†å¸ƒï¼ˆæ›´æ–°ã•ã‚ŒãŸä¿¡å¿µï¼‰ã€‚VAEã®ELBOã‚‚ã€LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚ã€ã“ã®æ§‹é€ ã®å¤‰ç¨®ã ã€‚

3. **MLEã¯æ¡ä»¶ä»˜ãCategoricalåˆ†å¸ƒã®æœ€é©åŒ–ã«å¸°ç€ã™ã‚‹ã€‚** LLMã®å­¦ç¿’ã¯ã€å„æ™‚åˆ» $t$ ã§ $p(x_t \mid x_{<t})$ ã‚’Categoricalåˆ†å¸ƒã¨ã—ã¦MLEæ¨å®šã™ã‚‹ã“ã¨ã€‚æœ¬è¬›ç¾©ã§å­¦ã‚“ã å…¨ã¦ã®é“å…·ãŒã“ã“ã«é›†ç´„ã•ã‚Œã‚‹ã€‚

### 6.3 FAQ

:::details Q: ãƒ™ã‚¤ã‚ºã¨é »åº¦ä¸»ç¾©ã€çµå±€ã©ã¡ã‚‰ãŒæ­£ã—ã„ã®ã‹ï¼Ÿ
ã€Œæ­£ã—ã•ã€ã®åŸºæº–ãŒç•°ãªã‚‹ã€‚é »åº¦ä¸»ç¾©ã¯ã€Œæ¨å®šé‡ã®é•·æœŸçš„æŒ¯ã‚‹èˆã„ã€ï¼ˆç¹°ã‚Šè¿”ã—å®Ÿé¨“ï¼‰ã§è©•ä¾¡ã—ã€ãƒ™ã‚¤ã‚ºã¯ã€Œç¾åœ¨ã®çŸ¥è­˜ã®ä¸‹ã§ã®ç¢ºä¿¡åº¦ã€ã§è©•ä¾¡ã™ã‚‹ã€‚MLã®æ–‡è„ˆã§ã¯:

- **MLE**ï¼ˆé »åº¦ä¸»ç¾©å¯„ã‚Šï¼‰: è¨ˆç®—ãŒç°¡å˜ã€æ¼¸è¿‘çš„ã«æœ€é©ã€å¤§ãƒ‡ãƒ¼ã‚¿å‘ã
- **ãƒ™ã‚¤ã‚ºæ¨è«–**: ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ãŒè‡ªç„¶ã€å°ãƒ‡ãƒ¼ã‚¿å‘ãã€äº‹å‰çŸ¥è­˜ã‚’æ´»ç”¨å¯èƒ½

å®Ÿç”¨ä¸Šã¯ã€Œã©ã¡ã‚‰ã‹ä¸€æ–¹ã€ã§ã¯ãªãã€å•é¡Œã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‹ã€‚VAEã¯å¤‰åˆ†ãƒ™ã‚¤ã‚ºã€LLMã®æå¤±é–¢æ•°ã¯MLEã ã€‚
:::

:::details Q: ãªãœæ­£è¦åˆ†å¸ƒãŒã“ã‚“ãªã«é »å‡ºã™ã‚‹ã®ã‹ï¼Ÿ
3ã¤ã®ç†ç”±ãŒã‚ã‚‹:

1. **ä¸­å¿ƒæ¥µé™å®šç†**: å¤šæ•°ã®ç‹¬ç«‹ãªå¾®å°åŠ¹æœã®å’Œã¯æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã
2. **æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**: å¹³å‡ã¨åˆ†æ•£ã‚’å›ºå®šã—ãŸã¨ãã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€å¤§ã®åˆ†å¸ƒãŒæ­£è¦åˆ†å¸ƒ
3. **è¨ˆç®—ã®éƒ½åˆ**: æ­£è¦åˆ†å¸ƒã®ç©ãƒ»å’Œãƒ»æ¡ä»¶ä»˜ããŒå…¨ã¦é–‰ã˜ãŸå½¢ã«ãªã‚‹

3ã¤ç›®ãŒå®Ÿç”¨ä¸Šæœ€ã‚‚é‡è¦ã ã€‚GANã®æ½œåœ¨ç©ºé–“ $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ ã‚„VAEã®äº‹å‰åˆ†å¸ƒã‚‚ã€è¨ˆç®—ã®å®¹æ˜“ã•ãŒé¸æŠã®ä¸»å› ã ã€‚
:::

:::details Q: æŒ‡æ•°å‹åˆ†å¸ƒæ—ã¯å®Ÿéš›ã«ã©ã“ã§ä½¿ã†ã®ã‹ï¼Ÿ
è‡³ã‚‹æ‰€ã§ã€‚

- **VAE**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ã¯ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆæŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **EBM**: $p(\mathbf{x}) = \frac{1}{Z}\exp(-E(\mathbf{x}))$ ã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ä¸€èˆ¬åŒ–
- **GLM**: ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”åˆ†å¸ƒã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—
- **Softmax**: Categoricalåˆ†å¸ƒã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã€‚LLMã®å‡ºåŠ›åˆ†å¸ƒãã®ã‚‚ã®

ç¬¬27å›ï¼ˆEBMï¼‰ã¨ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã§æœ¬æ ¼çš„ã«æ´»ç”¨ã™ã‚‹ã€‚
:::

:::details Q: CramÃ©r-Raoä¸‹ç•Œã‚’çŸ¥ã£ã¦ä½•ã®å½¹ã«ç«‹ã¤ã®ã‹ï¼Ÿ
ã€Œã“ã®æ¨å®šå•é¡Œã§ã“ã‚Œä»¥ä¸Šã®ç²¾åº¦ã¯åŸç†çš„ã«ä¸å¯èƒ½ã€ã¨ã„ã†é™ç•Œã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚

- ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ: æ¨å®šé‡ã®åˆ†æ•£ãŒCRä¸‹ç•Œã«è¿‘ã‘ã‚Œã°ã€ã“ã‚Œä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦
- å®Ÿé¨“è¨ˆç”»: Fisheræƒ…å ±é‡ãŒå¤§ãã„å®Ÿé¨“æ¡ä»¶ã‚’é¸ã¶ã“ã¨ã§ã€å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ç²¾å¯†ãªæ¨å®šãŒå¯èƒ½
- ç†è«–è§£æ: NNã®è¡¨ç¾åŠ›ã¨Fisheræƒ…å ±é‡ã®é–¢ä¿‚ã¯æ´»ç™ºãªç ”ç©¶åˆ†é‡
:::

:::details Q: ã€Œç¢ºç‡å¯†åº¦é–¢æ•°ã®å€¤ãŒ1ã‚’è¶…ãˆã‚‹ã€ã®ã¯é–“é•ã„ã§ã¯ï¼Ÿ
ã„ã„ãˆã€æ­£ã—ã„ã€‚PDFã¯ç¢ºç‡ã§ã¯ãªã„ã€‚ç¢ºç‡ã¯å¯†åº¦ã®**ç©åˆ†**ã§å¾—ã‚‰ã‚Œã‚‹:

$$
P(a \leq X \leq b) = \int_a^b f(x) dx
$$

$f(x)$ è‡ªä½“ã¯éè² ã§ã‚ã‚Œã°ã„ãã‚‰ã§ã‚‚å¤§ããã¦ã‚ˆã„ã€‚ä¾‹ãˆã° $\mathcal{N}(0, 0.01)$ ã®ãƒ”ãƒ¼ã‚¯ã¯ $f(0) = \frac{1}{\sqrt{2\pi \cdot 0.01}} \approx 3.99$ ã§ã€1ã‚’å¤§ããè¶…ãˆã‚‹ã€‚ç©åˆ†ã™ã‚‹ã¨å¿…ãš1ã«ãªã‚‹ãŒã€å¯†åº¦å€¤ãŒ1ã‚’è¶…ãˆã‚‹ã“ã¨è‡ªä½“ã¯ä½•ã®å•é¡Œã‚‚ãªã„ã€‚

```python
import numpy as np
sigma = 0.1
peak = 1 / np.sqrt(2 * np.pi * sigma**2)
print(f"N(0, {sigma**2}) peak density: {peak:.4f} >> 1.0")
print("But âˆ«f(x)dx = 1.0 always!")
```
:::

:::details Q: Multinomialåˆ†å¸ƒã¨Categoricalåˆ†å¸ƒã®é•ã„ã¯ï¼Ÿ
Categoricalåˆ†å¸ƒã¯ã€Œã‚µã‚¤ã‚³ãƒ­ã‚’1å›æŒ¯ã‚‹ã€: $x \in \{1, \ldots, K\}$, $P(x=k) = \pi_k$ã€‚

Multinomialåˆ†å¸ƒã¯ã€Œã‚µã‚¤ã‚³ãƒ­ã‚’ $n$ å›æŒ¯ã£ã¦ã€å„é¢ã®å‡ºãŸå›æ•°ã‚’è¨˜éŒ²ã™ã‚‹ã€: $(c_1, \ldots, c_K) \sim \text{Multi}(n, \boldsymbol{\pi})$, $\sum_k c_k = n$ã€‚

LLMã®æ–‡è„ˆã§ã¯:
- 1ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ = Categoricalåˆ†å¸ƒ
- ãƒãƒƒãƒå†…ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®çµ±è¨ˆ = Multinomialåˆ†å¸ƒ

Categorical = Multinomial($n=1$, $\boldsymbol{\pi}$) ã ã€‚
:::

:::details Q: ã“ã®ç¢ºç‡è«–ã®çŸ¥è­˜ã¯ç¬¬5å›ï¼ˆæ¸¬åº¦è«–ï¼‰ã§ã©ã†æ‹¡å¼µã•ã‚Œã‚‹ã®ã‹ï¼Ÿ
æœ¬è¬›ç¾©ã§ã¯ã€Œç¢ºç‡å¯†åº¦é–¢æ•° $f(x)$ ãŒå­˜åœ¨ã™ã‚‹ã€ã¨æš—é»™ã«ä»®å®šã—ãŸã€‚ã ãŒ:

- é›¢æ•£ã¨é€£ç¶šãŒæ··ã˜ã£ãŸåˆ†å¸ƒã¯ï¼Ÿ
- $\mathbb{R}^d$ ä¸Šã®å…¨ã¦ã®éƒ¨åˆ†é›†åˆã«ç¢ºç‡ã‚’å®šç¾©ã§ãã‚‹ã‹ï¼Ÿ
- ã€Œã»ã¨ã‚“ã©ç¢ºå®Ÿã«ã€ã¨ã¯ä½•ã‹ï¼Ÿ

ç¬¬5å›ã§ã¯æ¸¬åº¦è«–ã®è¨€è‘‰ã§ $f(x) = \frac{dP}{d\lambda}$ ï¼ˆRadon-Nikodymå°é–¢æ•°ï¼‰ã¨ã—ã¦å¯†åº¦é–¢æ•°ã‚’å³å¯†ã«å®šç¾©ã™ã‚‹ã€‚ã•ã‚‰ã«ç¢ºç‡éç¨‹ï¼ˆMarkové€£é–ã€Browné‹å‹•ï¼‰ã‚’å°å…¥ã—ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®SDEå®šå¼åŒ–ã¸ã®æ©‹æ¸¡ã—ã‚’è¡Œã†ã€‚
:::

### 6.4 ç¢ºç‡è«–ã§ã‚ˆãã‚ã‚‹ã€Œç½ ã€

:::details ç½ 1: P(A|B) â‰  P(B|A) â€” æ¡ä»¶ã®é€†è»¢
ã€Œé›¨ã®ã¨ãå‚˜ã‚’æŒã¤ç¢ºç‡90%ã€ã¨ã€Œå‚˜ã‚’æŒã£ã¦ã„ã‚‹ã¨ãé›¨ã®ç¢ºç‡ã€ã¯å…¨ãé•ã†ã€‚ãƒ™ã‚¤ã‚ºã®å®šç†ãªã—ã«ã“ã®2ã¤ã‚’æ··åŒã™ã‚‹ã®ãŒã€Œæ¤œå¯Ÿå®˜ã®èª¤è¬¬ã€ã ã€‚DNAé‘‘å®šã§ã€Œä¸€è‡´ã—ãŸ = çŠ¯äººã€ã¨çµè«–ã™ã‚‹ã®ã¯ $P(\text{ä¸€è‡´} \mid \text{çŠ¯äºº})$ ã¨ $P(\text{çŠ¯äºº} \mid \text{ä¸€è‡´})$ ã®æ··åŒã€‚
:::

:::details ç½ 2: ç‹¬ç«‹ã¨ç„¡ç›¸é–¢ã¯é•ã†
ç„¡ç›¸é–¢: $\text{Cov}(X, Y) = 0$ï¼ˆç·šå½¢é–¢ä¿‚ãŒãªã„ï¼‰
ç‹¬ç«‹: $P(X, Y) = P(X)P(Y)$ï¼ˆã‚ã‚‰ã‚†ã‚‹é–¢ä¿‚ãŒãªã„ï¼‰

ç‹¬ç«‹ â†’ ç„¡ç›¸é–¢ã ãŒã€é€†ã¯æˆã‚Šç«‹ãŸãªã„ã€‚$X \sim \mathcal{N}(0,1)$, $Y = X^2$ ã¯ç„¡ç›¸é–¢ã ãŒç‹¬ç«‹ã§ã¯ãªã„ã€‚
```python
import numpy as np
np.random.seed(42)
X = np.random.normal(0, 1, 100000)
Y = X**2
print(f"Cov(X, XÂ²) = {np.cov(X, Y)[0,1]:.4f} â‰ˆ 0 (uncorrelated)")
print(f"But E[Y|X=2] = 4, E[Y|X=-2] = 4 â†’ clearly not independent!")
```
:::

:::details ç½ 3: åˆ†æ•£0ã§ã‚‚åˆ†å¸ƒã¯æ±ºã¾ã‚‰ãªã„
CramÃ©r-Raoä¸‹ç•Œ $\text{Var} \geq 1/(nI)$ ã¯ä¸åæ¨å®šé‡ã«ã—ã‹é©ç”¨ã•ã‚Œãªã„ã€‚ãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹æ¨å®šé‡ã¯CRä¸‹ç•Œã‚’ä¸‹å›ã‚‹ã“ã¨ãŒã‚ã‚‹ï¼ˆJames-Steinã®ç¸®å°æ¨å®šé‡ï¼‰ã€‚ã€Œãƒã‚¤ã‚¢ã‚¹ã‚’è¨±å®¹ã™ã‚‹ä»£ã‚ã‚Šã«MSEã‚’ä¸‹ã’ã‚‹ã€ã®ã¯ã€MLã§ã¯æ­£å‰‡åŒ–ã¨ã—ã¦æ—¥å¸¸çš„ã«è¡Œã‚ã‚Œã‚‹ã€‚
:::

:::details ç½ 4: MLEã¯å¸¸ã«æœ€è‰¯ã§ã¯ãªã„
å°ã‚µãƒ³ãƒ—ãƒ«ã§ã¯MLEã®ãƒã‚¤ã‚¢ã‚¹ãŒå•é¡Œã«ãªã‚‹ã€‚åˆ†æ•£æ¨å®šé‡ $\hat{\sigma}^2_{\text{MLE}} = \frac{1}{N}\sum(x_i - \bar{x})^2$ ã¯ $\sigma^2$ ã‚’éå°è©•ä¾¡ã™ã‚‹ã€‚James-Steinã®å®šç†ãŒç¤ºã™ã®ã¯ã€3æ¬¡å…ƒä»¥ä¸Šã§ã¯MLEãŒã€Œè¨±å®¹å¯èƒ½ã§ãªã„ã€ï¼ˆadmissible ã§ãªã„ï¼‰ã¨ã„ã†è¡æ’ƒçš„äº‹å®Ÿã ã€‚
:::

:::details ç½ 5: äº‹å‰åˆ†å¸ƒãŒã€Œä¸»è¦³çš„ã€ã¯æ¬ ç‚¹ã‹ï¼Ÿ
é »åº¦ä¸»ç¾©è€…ã¯ãƒ™ã‚¤ã‚ºã®ã€Œä¸»è¦³æ€§ã€ã‚’æ‰¹åˆ¤ã™ã‚‹ã€‚ã ãŒ:
- ã€Œäº‹å‰åˆ†å¸ƒãªã—ã€ã¯ã€Œä¸€æ§˜äº‹å‰åˆ†å¸ƒã€ã¨ç­‰ä¾¡ â€” ã“ã‚Œã‚‚ä¸»è¦³çš„
- å¼±æƒ…å ±äº‹å‰åˆ†å¸ƒã¯ã€ç‰©ç†çš„åˆ¶ç´„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ç­‰ï¼‰ã‚’è‡ªç„¶ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
- ãƒ‡ãƒ¼ã‚¿ãŒååˆ†ã‚ã‚Œã°äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ã¯æ¶ˆãˆã‚‹ï¼ˆäº‹å¾Œä¸€è‡´æ€§ï¼‰

å®Ÿç”¨çš„ã«ã¯ã€äº‹å‰åˆ†å¸ƒã¯ã€Œæ­£å‰‡åŒ–ã®ä¸€å½¢æ…‹ã€ã¨å‰²ã‚Šåˆ‡ã£ã¦ã‚ˆã„ã€‚
:::

### 6.5 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| æ—¥ | å†…å®¹ | æ‰€è¦æ™‚é–“ |
|:---|:-----|:---------|
| Day 1 | Zone 0-2ï¼ˆä½“é¨“ãƒ»ç›´æ„Ÿï¼‰+ Zone 3 å‰åŠï¼ˆ3.1-3.4ï¼‰ | 2æ™‚é–“ |
| Day 2 | Zone 3 å¾ŒåŠï¼ˆ3.5-3.10 Boss Battleï¼‰ | 2æ™‚é–“ |
| Day 3 | Zone 4ï¼ˆå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ + GMM + ãƒ™ã‚¤ã‚ºæ¨è«–ï¼‰ | 2æ™‚é–“ |
| Day 4 | Zone 5ï¼ˆãƒ†ã‚¹ãƒˆ + ãƒãƒ£ãƒ¬ãƒ³ã‚¸å®Ÿè£…ï¼‰ | 1.5æ™‚é–“ |
| Day 5 | Zone 6-7ï¼ˆç™ºå±• + æŒ¯ã‚Šè¿”ã‚Šï¼‰ | 1æ™‚é–“ |
| Day 6 | å¾©ç¿’: ä¸»è¦å®šç†ã‚’ç´™ã«å†å°å‡º | 1æ™‚é–“ |
| Day 7 | ç¬¬5å›ã®äºˆç¿’: æ¸¬åº¦è«–ã®ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ | 1æ™‚é–“ |

### 6.6 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```python
import numpy as np

lecture4_progress = {
    "Zone 0: Quick Start": True,
    "Zone 1: Experience": True,
    "Zone 2: Intuition": True,
    "Zone 3.1: Probability Space": False,
    "Zone 3.2: Random Variables": False,
    "Zone 3.3: Bayes' Theorem": False,
    "Zone 3.4: Distributions": False,
    "Zone 3.5: Multivariate Normal": False,
    "Zone 3.6: Exponential Family": False,
    "Zone 3.7: MLE": False,
    "Zone 3.8: Fisher Information": False,
    "Zone 3.9: LLN & CLT": False,
    "Zone 3.10: Boss Battle": False,
    "Zone 4: Implementation": False,
    "Zone 5: Experiments": False,
    "Zone 6: Advanced": False,
    "Zone 7: Review": False,
}

completed = sum(v for v in lecture4_progress.values())
total = len(lecture4_progress)
print(f"=== ç¬¬4å› é€²æ—: {completed}/{total} ({100*completed/total:.0f}%) ===\n")
for zone, done in lecture4_progress.items():
    status = "[x]" if done else "[ ]"
    print(f"  {status} {zone}")
```

### 6.7 æ¬¡å›äºˆå‘Š â€” ç¬¬5å›: æ¸¬åº¦è«–çš„ç¢ºç‡è«–ãƒ»ç¢ºç‡éç¨‹å…¥é–€

ç¬¬4å›ã§ç¢ºç‡åˆ†å¸ƒã‚’ã€Œä½¿ãˆã‚‹ã€ã‚ˆã†ã«ãªã£ãŸã€‚ã ãŒã€ä»¥ä¸‹ã®å•ã„ã«ç­”ãˆã‚‰ã‚Œã‚‹ã ã‚ã†ã‹:

- ã€Œç¢ºç‡å¯†åº¦é–¢æ•°ã€ã¨ã¯å³å¯†ã«ä½•ã‹ï¼Ÿ ãªãœç‚¹ $x$ ã§ã® $f(x)$ ã¯ç¢ºç‡ã§ã¯ãªã„ã®ã‹ï¼Ÿ
- é›¢æ•£ã¨é€£ç¶šãŒæ··ã˜ã£ãŸåˆ†å¸ƒã‚’ã©ã†æ‰±ã†ã‹ï¼Ÿ
- ã€Œã»ã¨ã‚“ã©ç¢ºå®Ÿã«åæŸã™ã‚‹ã€ã®ã€Œã»ã¨ã‚“ã©ã€ã¨ã¯ï¼Ÿ
- Browné‹å‹•ã¯ãªãœå¾®åˆ†ä¸å¯èƒ½ãªã®ã‹ï¼Ÿ
- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®forward processã‚’è¨˜è¿°ã™ã‚‹SDEã¨ã¯ä½•ã‹ï¼Ÿ

ç¬¬5å›ã§ã¯**æ¸¬åº¦è«–**ã®è¨€è‘‰ã§ç¢ºç‡è«–ã‚’å†æ§‹ç¯‰ã™ã‚‹ã€‚Lebesgueç©åˆ†ã€Radon-Nikodymå°é–¢æ•°ã€ç¢ºç‡éç¨‹ã€Markové€£é–ã€Browné‹å‹• â€” æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åŸºç›¤ãŒã“ã“ã«åŸ‹ã¾ã£ã¦ã„ã‚‹ã€‚

ãã—ã¦ `%timeit` ãŒåˆç™»å ´ã™ã‚‹ã€‚Monte Carloç©åˆ†ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æ¸¬ã‚Šå§‹ã‚ã‚‹ã¨ã€Pythonã®ã€Œé…ã•ã€ãŒå°‘ã—ãšã¤è¦‹ãˆã¦ãã‚‹......ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ â€” å…¨ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚ãŠç–²ã‚Œã•ã¾ã§ã—ãŸã€‚ç¢ºç‡ã®è¨€èªã‚’æ‰‹ã«å…¥ã‚ŒãŸä»Šã€ç¬¬5å›ã§æ¸¬åº¦è«–ã¨ã„ã†ã€Œç¢ºç‡ã®æ–‡æ³•ã€ã‚’å³å¯†ã«å®šç¾©ã™ã‚‹æ—…ã«å‡ºã‚ˆã†ã€‚
:::

---


### 6.8 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã¯æ­£è¦åˆ†å¸ƒã«å¾“ã‚ãªã„ã€‚ãã‚Œã§ã‚‚ä»®å®šã™ã‚‹"æœ¬å½“ã®ç†ç”±"ã¯ä½•ã‹ï¼Ÿ**

CLTãŒã€Œå¤šæ•°ã®ç‹¬ç«‹å¾®å°åŠ¹æœã®å’Œâ†’æ­£è¦åˆ†å¸ƒã€ã‚’ä¿è¨¼ã™ã‚‹ã‹ã‚‰ï¼Ÿ ãã‚Œã¯ç†ç”±ã®ä¸€ã¤ã ã€‚ã ãŒæœ¬è³ªã¯ã‚‚ã£ã¨æ·±ã„ã€‚

- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¯**æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒ**ã ã€‚å¹³å‡ã¨åˆ†æ•£ã ã‘ã‚’çŸ¥ã£ã¦ã„ã‚‹ã¨ãã€ãã‚Œä»¥ä¸Šã®ä»®å®šã‚’ç½®ã‹ãªã„ã€Œæœ€ã‚‚æƒ…å ±é‡ã®å°‘ãªã„ã€åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹ã 
- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æ¼”ç®—ã¯**é–‰ã˜ã¦ã„ã‚‹**ã€‚å’Œãƒ»æ¡ä»¶ä»˜ããƒ»å‘¨è¾ºãŒå…¨ã¦ã‚¬ã‚¦ã‚¹ã®ã¾ã¾ã€‚ã“ã‚Œã¯è¨ˆç®—ä¸Šã®å¥‡è·¡ã¨è¨€ã£ã¦ã‚ˆã„
- ãã—ã¦ã€æ­£è¦åˆ†å¸ƒãŒã€Œé–“é•ã£ã¦ã„ã‚‹ã€ã“ã¨ã¯**ã‚ã‹ã£ã¦ã„ã‚‹**ä¸Šã§ä½¿ã†ã€‚é‡è¦ãªã®ã¯ã€Œã©ã®ç¨‹åº¦é–“é•ã£ã¦ã„ã‚‹ã‹ã€ã‚’å®šé‡åŒ–ã™ã‚‹ã“ã¨ â€” KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆç¬¬6å›ï¼‰ãŒãã®é“å…·ã 

:::details ãƒ™ã‚¤ã‚ºè„³ä»®èª¬ â€” è„³ã¯ç¢ºç‡è¨ˆç®—æ©Ÿã‹ï¼Ÿ
èªçŸ¥ç§‘å­¦ã«ã¯ã€Œè„³ã¯ãƒ™ã‚¤ã‚ºæ¨è«–ã‚’è¡Œã£ã¦ã„ã‚‹ã€ã¨ã„ã†ä»®èª¬ãŒã‚ã‚‹ã€‚æ„Ÿè¦šå…¥åŠ›ï¼ˆå°¤åº¦ï¼‰ã¨çµŒé¨“ï¼ˆäº‹å‰åˆ†å¸ƒï¼‰ã‚’çµ„ã¿åˆã‚ã›ã¦ä¸–ç•Œã®çŠ¶æ…‹ï¼ˆäº‹å¾Œåˆ†å¸ƒï¼‰ã‚’æ¨å®šã™ã‚‹ã€‚

éŒ¯è¦–ç¾è±¡ã¯ã€å¼·ã„äº‹å‰åˆ†å¸ƒãŒå¼±ã„å°¤åº¦ã‚’ä¸Šæ›¸ãã™ã‚‹ä¾‹ã¨ã—ã¦è§£é‡ˆã•ã‚Œã‚‹ã€‚VAEã®ãƒ‡ã‚³ãƒ¼ãƒ€ãŒã€Œã¼ã‚„ã‘ãŸã€ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã®ã¯ã€äº‹å‰åˆ†å¸ƒ $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ ãŒéåº¦ã«æ»‘ã‚‰ã‹ãªæ½œåœ¨ç©ºé–“ã‚’å¼·åˆ¶ã™ã‚‹ãŸã‚ â€” ã‚ã‚‹æ„å‘³ã€è„³ã®éŒ¯è¦–ã¨åŒã˜æ§‹é€ ã ã€‚

ã€Œæ­£è¦åˆ†å¸ƒã‚’ä»®å®šã™ã‚‹ã€ã®ã¯ã€è„³ãŒã€Œä¸–ç•Œã¯æ»‘ã‚‰ã‹ã ã€ã¨ä»®å®šã™ã‚‹ã®ã¨åŒã˜ã‹ã‚‚ã—ã‚Œãªã„ã€‚
:::

ã•ã‚‰ã«è€ƒãˆã¦ã¿ã‚ˆã†:

- **LLMã®å‡ºåŠ›åˆ†å¸ƒã¯Categoricalã€‚** æ­£è¦åˆ†å¸ƒã§ã¯ãªã„ã€‚ã ãŒCategoricalåˆ†å¸ƒã®è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆlogitï¼‰ã¯é€£ç¶šå€¤ã§ã€ãã®ç©ºé–“ã§ã¯æ­£è¦åˆ†å¸ƒçš„ãªä»®å®šãŒä½¿ã‚ã‚Œã‚‹
- **æ¬¡å…ƒã®å‘ªã„**: 100æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ã‚µãƒ³ãƒ—ãƒ«ã¯ã€ã»ã¼ç¢ºå®Ÿã«åŸç‚¹ã‹ã‚‰ $\sqrt{100} = 10$ ã®è·é›¢ã«ã‚ã‚‹ã€‚ã€Œé«˜æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹ã¯çƒæ®»ã«é›†ä¸­ã™ã‚‹ã€â€” ã“ã‚ŒãŒæ­£è¦åˆ†å¸ƒã®ç›´æ„ŸãŒå´©å£Šã™ã‚‹ç¬é–“ã 
- **æ­£è¦åˆ†å¸ƒã¯"æœ€ã‚‚ç„¡çŸ¥ãª"åˆ†å¸ƒ**: æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åŸç†ã«ã‚ˆã‚Šã€å¹³å‡ã¨åˆ†æ•£ã—ã‹çŸ¥ã‚‰ãªã„ã¨ãã€ä½™è¨ˆãªä»®å®šã‚’æœ€ã‚‚å°‘ãªãã™ã‚‹åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹ã€‚ã€ŒçŸ¥ã‚‰ãªã„ã“ã¨ã‚’æ­£ç›´ã«èªã‚ã‚‹åˆ†å¸ƒã€ã¨ã‚‚è¨€ãˆã‚‹

```python
import numpy as np

# High-dimensional Gaussian: samples concentrate on a thin shell
dims = [2, 10, 100, 1000]
n_samples = 10000

print("=== High-dimensional Gaussian Concentration ===")
print(f"{'d':>6} {'E[||x||]':>10} {'âˆšd':>8} {'Std':>8} {'Std/Mean':>10}")
print("-" * 45)
for d in dims:
    samples = np.random.normal(0, 1, (n_samples, d))
    norms = np.linalg.norm(samples, axis=1)
    print(f"{d:>6} {norms.mean():>10.4f} {np.sqrt(d):>8.4f} {norms.std():>8.4f} {norms.std()/norms.mean():>10.4f}")

print("\nâ†’ In high dimensions, ALL samples are near distance âˆšd from origin")
print("â†’ The 'center' of a Gaussian is EMPTY in high dimensions!")
print("â†’ This is why VAE latent spaces need careful design (ç¬¬10å›)")
```

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Bayes, T., Price, R. (1763). "An Essay towards solving a Problem in the Doctrine of Chances." *Philosophical Transactions of the Royal Society of London*, 53, 370-418.
@[card](https://doi.org/10.1098/rstl.1763.0053)

[^2]: Kingma, D.P., Welling, M. (2013). "Auto-Encoding Variational Bayes." *arXiv preprint*.
@[card](https://arxiv.org/abs/1312.6114)

[^3]: Hinton, G., Vinyals, O., Dean, J. (2015). "Distilling the Knowledge in a Neural Network." *arXiv preprint*.
@[card](https://arxiv.org/abs/1503.02531)

[^4]: Ho, J., Jain, A., Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS*.
@[card](https://arxiv.org/abs/2006.11239)

[^5]: Malach, E. (2023). "Auto-Regressive Next-Token Predictors are Universal Learners." *arXiv preprint*.
@[card](https://arxiv.org/abs/2309.06979)

[^6]: Kolmogorov, A.N. (1933). *Grundbegriffe der Wahrscheinlichkeitsrechnung*. Springer. English translation: *Foundations of the Theory of Probability* (1956).
@[card](https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations.pdf)
â€»å¤–éƒ¨å¤§å­¦PDFã®ãŸã‚ãƒªãƒ³ã‚¯åˆ‡ã‚Œã®å¯èƒ½æ€§ã‚ã‚Šï¼ˆãƒŸãƒ©ãƒ¼: [Internet Archive](https://archive.org/details/kolmogorov_202112) ã‚‚å‚ç…§ï¼‰

[^7]: LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., Huang, F.J. (2006). "A Tutorial on Energy-Based Learning." *Predicting Structured Data*, MIT Press.

[^8]: CramÃ©r, H. (1946). *Mathematical Methods of Statistics*. Princeton University Press. Rao, C.R. (1945). "Information and the Accuracy Attainable in the Estimation of Statistical Parameters." *Bulletin of the Calcutta Mathematical Society*, 37, 81-91.

[^9]: HyvÃ¤rinen, A. (2005). "Estimation of Non-Normalized Statistical Models by Score Matching." *Journal of Machine Learning Research*, 6, 695-709.

[^10]: Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B. (2020). "Score-Based Generative Modeling through Stochastic Differential Equations." *ICLR 2021 (Oral)*.
@[card](https://arxiv.org/abs/2011.13456)

[^11]: Rezende, D.J., Mohamed, S. (2015). "Variational Inference with Normalizing Flows." *ICML 2015*.
@[card](https://arxiv.org/abs/1505.05770)

[^12]: Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." *ICLR 2022*.
@[card](https://arxiv.org/abs/2106.09685)

### æ•™ç§‘æ›¸

- Bishop, C.M. (2006). *Pattern Recognition and Machine Learning*. Springer. [PDF available from Microsoft Research]
- Murphy, K.P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press. [Free online]
- Wasserman, L. (2004). *All of Statistics*. Springer.
- Casella, G., Berger, R.L. (2002). *Statistical Inference*. 2nd ed. Duxbury/Thomson.

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|
| $\Omega$ | æ¨™æœ¬ç©ºé–“ | 3.1 |
| $\mathcal{F}$ | Ïƒ-åŠ æ³•æ— | 3.1 |
| $P$ | ç¢ºç‡æ¸¬åº¦ | 3.1 |
| $X, Y, Z$ | ç¢ºç‡å¤‰æ•° | 3.2 |
| $\mathbb{E}[\cdot]$ | æœŸå¾…å€¤ | 3.2 |
| $\text{Var}(\cdot)$ | åˆ†æ•£ | 3.2 |
| $\text{Cov}(\cdot, \cdot)$ | å…±åˆ†æ•£ | 3.2 |
| $\boldsymbol{\mu}$ | å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« | 3.5 |
| $\boldsymbol{\Sigma}$ | å…±åˆ†æ•£è¡Œåˆ— | 3.5 |
| $\boldsymbol{\Lambda}$ | ç²¾åº¦è¡Œåˆ— $= \boldsymbol{\Sigma}^{-1}$ | 3.5 |
| $\boldsymbol{\eta}$ | è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 3.6 |
| $\mathbf{T}(\mathbf{x})$ | ååˆ†çµ±è¨ˆé‡ | 3.6 |
| $A(\boldsymbol{\eta})$ | å¯¾æ•°æ­£è¦åŒ–å®šæ•° | 3.6 |
| $\theta$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 3.7 |
| $\hat{\theta}$ | æ¨å®šé‡ | 3.7 |
| $I(\theta)$ | Fisheræƒ…å ±é‡ | 3.8 |
| $\mathcal{D}$ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | 3.3 |
| $\sim$ | ã€Œã«ã—ãŸãŒã†ã€ | å…¨èˆ¬ |
| $\propto$ | æ¯”ä¾‹ã™ã‚‹ | 3.3 |
| $\xrightarrow{P}$ | ç¢ºç‡åæŸ | 3.11 |
| $\xrightarrow{d}$ | åˆ†å¸ƒåæŸ | 3.11 |
| $\xrightarrow{\text{a.s.}}$ | æ¦‚åæŸ | 3.11 |
| $\overset{\text{i.i.d.}}{\sim}$ | ç‹¬ç«‹åŒåˆ†å¸ƒ | 3.2 |
| $\bar{X}_N$ | æ¨™æœ¬å¹³å‡ $\frac{1}{N}\sum X_i$ | 3.11 |
| $\ell(\theta)$ | å¯¾æ•°å°¤åº¦ | 3.7 |
| $s(\mathbf{x}; \theta)$ | ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_\theta \log p$ | 3.8 |
| $h(\mathbf{x})$ | åŸºåº•æ¸¬åº¦ | 3.6 |
| $\boldsymbol{\pi}$ | Categorical/Dirichletã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 1.1 |
| $\Gamma(\cdot)$ | ã‚¬ãƒ³ãƒé–¢æ•° | 3.4 |
| $\binom{n}{k}$ | äºŒé …ä¿‚æ•° | 3.4 |
| $|\boldsymbol{\Sigma}|$ | è¡Œåˆ—å¼ | 3.5 |
| $\delta(\cdot)$ | ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ã®ãƒ‡ãƒ«ã‚¿é–¢æ•° | 4.5 |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
