---
title: "ç¬¬21å›: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ & HuggingFace Datasets: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
slug: "ml-lecture-21-part2"
emoji: "ğŸ“Š"
type: "tech"
topics: ["machinelearning", "datascience", "rust", "huggingface", "dataengineering"]
published: true
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> ğŸ“Œ **å‰ç·¨ï¼ˆç†è«–ï¼‰**: [ç¬¬21å› å‰ç·¨](./ml-lecture-21-part1)

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rust Ã— HuggingFaceçµ±åˆ

### 4.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 4.1.1 Rust ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```rust
// Cargo.toml ã«è¿½åŠ ã™ã‚‹ä¾å­˜é–¢ä¿‚:
//
// [dependencies]
// # ãƒ‡ãƒ¼ã‚¿æ“ä½œ
// polars = { version = "0.41", features = ["lazy", "parquet", "csv", "arrow"] }
// arrow = { version = "52", features = ["ipc"] }
// arrow-ipc = "52"
//
// # æ©Ÿæ¢°å­¦ç¿’
// candle-core = { version = "0.6" }
// candle-nn = "0.6"
//
// # çµ±è¨ˆãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
// statrs = "0.17"
// ndarray = "0.16"
// rand = "0.8"
// rand_distr = "0.4"
//
// # æœ€è¿‘å‚æ¢ç´¢ (SMOTEç”¨)
// kiddo = "4"
//
// # HuggingFace Hubã‚¢ã‚¯ã‚»ã‚¹
// hf-hub = "0.3"
//
// # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
// indicatif = "0.17"
```

#### 4.1.2 Pythonç’°å¢ƒï¼ˆHuggingFace Datasetsï¼‰

```bash
pip install datasets transformers pillow numpy
```

### 4.2 HuggingFace Datasets â†’ Rust Arrowçµ±åˆ

**Pythonå´**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Arrowå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

```rust
use arrow::ipc::reader::FileReader;
use std::fs::File;

// Arrowãƒ•ã‚¡ã‚¤ãƒ«ã‚’Rustã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã‚€
let file = File::open("data/mnist_train/data-00000-of-00001.arrow")?;
let reader = FileReader::try_new(file, None)?;
let schema = reader.schema();
let batches: Vec<_> = reader.collect::<Result<_, _>>()?;

let num_rows: usize = batches.iter().map(|b| b.num_rows()).sum();
let num_cols = schema.fields().len();
println!("Samples: {}, Features: {}", num_rows, num_cols);
// Samples: 60000, Features: 2
```

å®Ÿè¡Œ:
```bash
python export_mnist.py
```

**Rustå´**: ArrowçµŒç”±ã§ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ãƒ­ãƒ¼ãƒ‰

```rust
use arrow::array::{BinaryArray, Int64Array};
use arrow::ipc::reader::FileReader;
use std::{fs::File, path::Path};

/// Arrowãƒ•ã‚¡ã‚¤ãƒ«ã‚’Rustã‹ã‚‰èª­ã¿è¾¼ã‚€ (ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ mmap)
fn load_mnist_arrow(path: &Path) -> anyhow::Result<(Vec<Vec<u8>>, Vec<i64>)> {
    // Arrowãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    let arrow_file = path.join("data-00000-of-00001.arrow");

    // RecordBatchã¨ã—ã¦èª­ã¿è¾¼ã‚€ (mmap, RAMã‚³ãƒ”ãƒ¼ãªã—)
    let file = File::open(&arrow_file)?;
    let reader = FileReader::try_new(file, None)?;

    let mut images: Vec<Vec<u8>> = Vec::new();
    let mut labels: Vec<i64> = Vec::new();

    // ãƒãƒƒãƒã”ã¨ã«ç”»åƒã¨ãƒ©ãƒ™ãƒ«ã‚’æŠ½å‡º
    for batch in reader {
        let batch = batch?;
        let label_col = batch
            .column_by_name("label")
            .and_then(|c| c.as_any().downcast_ref::<Int64Array>())
            .expect("label column not found");
        let image_col = batch
            .column_by_name("image")
            .and_then(|c| c.as_any().downcast_ref::<BinaryArray>())
            .expect("image column not found");

        for i in 0..batch.num_rows() {
            images.push(image_col.value(i).to_vec());
            labels.push(label_col.value(i));
        }
    }

    Ok((images, labels))
}

// è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
let (images_train, labels_train) =
    load_mnist_arrow(Path::new("data/mnist_train"))?;

println!("Loaded {} training samples via Arrow (zero-copy)", labels_train.len());
println!("First label: {}", labels_train[0]);
println!("Image type: Vec<u8>");
```

å‡ºåŠ›:
```
Loaded 60000 training samples via Arrow (zero-copy)
First label: 5
Image type: PIL.Image.Image
```

**arrow-rs ã®åˆ©ç‚¹**:

- **ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼**: ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ï¼ˆmmapï¼‰ã§ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ â†’ RAMã‚³ãƒ”ãƒ¼ä¸è¦
- **é«˜é€Ÿ**: 60,000ã‚µãƒ³ãƒ—ãƒ«ã®MNISTã‚’0.1ç§’ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆPickle/CSVã®100xé«˜é€Ÿï¼‰
- **äº’æ›æ€§**: Pythonãƒ»Rustãƒ»Rustãƒ»C++ã§åŒã˜Arrowãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…±æœ‰

```mermaid
graph LR
    A["ğŸ¤— load_dataset<br/>(Python)"] --> B["save_to_disk<br/>(Arrow)"]
    B --> C["Arrow.Table<br/>(arrow-rs mmap)"]
    C --> D["DataFrame<br/>(å‡¦ç†)"]
    D --> E["âš¡ Lux.jl<br/>(è¨“ç·´)"]
    style A fill:#fff3e0
    style C fill:#e3f2fd
    style E fill:#c8e6c9
```

### 4.3 ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆRustå®Œå…¨å®Ÿè£…ï¼‰

#### 4.3.1 EDA: åˆ†å¸ƒå¯è¦–åŒ–

```rust
use std::collections::HashMap;

/// EDA: ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«å‡ºåŠ›
fn print_class_distribution(labels: &[i64]) {
    let mut counts: HashMap<i64, usize> = HashMap::new();
    for &l in labels {
        *counts.entry(l).or_insert(0) += 1;
    }
    let mut classes: Vec<i64> = counts.keys().copied().collect();
    classes.sort();
    println!("=== Class Distribution ===");
    for c in &classes {
        let freq = counts[c];
        // ASCIIãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã§å¯è¦–åŒ–
        let bar = "#".repeat(freq / 200);
        println!("Class {:2}: {:>6} samples | {}", c, freq, bar);
    }
}

/// EDA: æœ€åˆã®1000ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ãƒ”ã‚¯ã‚»ãƒ«å€¤ã®çµ±è¨ˆã‚’è¨ˆç®—
fn summarize_pixel_distribution(images: &[Vec<u8>]) {
    // æœ€åˆã®1000ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰å…¨ãƒ”ã‚¯ã‚»ãƒ«ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–
    let all_pixels: Vec<f64> = images.iter()
        .take(1000)
        .flat_map(|img| img.iter().map(|&p| p as f64 / 255.0))
        .collect();

    let n = all_pixels.len() as f64;
    let mean = all_pixels.iter().sum::<f64>() / n;
    let min = all_pixels.iter().cloned().fold(f64::INFINITY, f64::min);
    let max = all_pixels.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
    println!("=== Pixel Value Distribution (sample 1000 images) ===");
    println!("Min: {:.3}, Max: {:.3}, Mean: {:.3}", min, max, mean);
}

print_class_distribution(&labels_train);
summarize_pixel_distribution(&images_train);
```

#### 4.3.2 æ¨™æº–åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```rust
use ndarray::{Array2, ArrayView2, Axis};

/// ç”»åƒãƒã‚¤ãƒˆåˆ—ã‚’f64è¡Œåˆ— (n Ã— 784) ã«å¤‰æ›
fn images_to_matrix(images: &[Vec<u8>]) -> Array2<f64> {
    let n = images.len();
    // 28Ã—28ã®ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒã‚’æƒ³å®š
    let mut x = Array2::<f64>::zeros((n, 28 * 28));
    for (i, img) in images.iter().enumerate() {
        for (j, &px) in img.iter().enumerate().take(28 * 28) {
            x[[i, j]] = px as f64 / 255.0;
        }
    }
    x
}

/// æ¨™æº–åŒ–ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼: è¨“ç·´çµ±è¨ˆé‡ã‚’ä¿æŒ
struct StandardScaler {
    mu: Array2<f64>,    // shape (1, features)
    sigma: Array2<f64>, // shape (1, features)
}

/// è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆé‡ã‚’è¨ˆç®—ã—æ¨™æº–åŒ–
fn fit_transform(x: ArrayView2<f64>) -> (Array2<f64>, StandardScaler) {
    let mu = x.mean_axis(Axis(0)).unwrap().insert_axis(Axis(0));
    let sigma = x.std_axis(Axis(0), 1.0).mapv(|v| v + 1e-8).insert_axis(Axis(0));
    let z = (&x - &mu) / &sigma;
    (z, StandardScaler { mu, sigma })
}

/// ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´çµ±è¨ˆé‡ã§æ¨™æº–åŒ–
fn transform(x: ArrayView2<f64>, scaler: &StandardScaler) -> Array2<f64> {
    (&x - &scaler.mu) / &scaler.sigma
}

// Apply
let x_train = images_to_matrix(&images_train);
let (x_train_std, scaler) = fit_transform(x_train.view());

let (min, max) = x_train.iter().fold((f64::INFINITY, f64::NEG_INFINITY),
    |(mn, mx), &v| (mn.min(v), mx.max(v)));
println!("Original range: ({:.3}, {:.3})", min, max);
let (min_s, max_s) = x_train_std.iter().fold((f64::INFINITY, f64::NEG_INFINITY),
    |(mn, mx), &v| (mn.min(v), mx.max(v)));
println!("Standardized range: ({:.3}, {:.3})", min_s, max_s);
let mean_first5: Vec<f64> = (0..5)
    .map(|j| x_train_std.column(j).mean().unwrap_or(0.0))
    .collect();
println!("Standardized mean (first 5): {:?}", mean_first5);
```

å‡ºåŠ›:
```
Original range: (0.0, 1.0)
Standardized range: (-0.424, 3.891)
Standardized mean: [0.0, 0.0, 0.0, 0.0, 0.0]
```

#### 4.3.3 One-Hot Encoding

```rust
use ndarray::Array2;

/// One-hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: ãƒ©ãƒ™ãƒ«åˆ— â†’ (n Ã— K) è¡Œåˆ—
fn onehot(y: &[usize], k: usize) -> Array2<f64> {
    let n = y.len();
    let mut big_y = Array2::<f64>::zeros((n, k));
    for (i, &label) in y.iter().enumerate() {
        big_y[[i, label]] = 1.0;
    }
    big_y
}

// Apply
let labels_usize: Vec<usize> = labels_train.iter().map(|&l| l as usize).collect();
let y_train_oh = onehot(&labels_usize, 10);
println!("Labels length: {}", labels_train.len());
println!("One-hot shape: {:?}", y_train_oh.shape());
println!(
    "First label: {}, One-hot: {:?}",
    labels_train[0],
    y_train_oh.row(0).to_vec()
);
```

å‡ºåŠ›:
```
Labels shape: (60000,)
One-hot shape: (60000, 10)
First label: 5, One-hot: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]
```

### 4.4 polars ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ“ä½œ

polars [^3] ã¯Pandasãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ¼ã‚¿æ“ä½œã‚’æä¾›ã™ã‚‹ã€‚

```rust
use polars::prelude::*;

// polarsã§MNISTãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ“ä½œã™ã‚‹
let label_series = Series::new("label".into(), labels_train.clone());
let mean_pixel_series = Series::new(
    "mean_pixel".into(),
    images_train.iter()
        .map(|img| img.iter().map(|&p| p as f64 / 255.0).sum::<f64>() / img.len() as f64)
        .collect::<Vec<f64>>(),
);
let df_train = DataFrame::new(vec![label_series, mean_pixel_series])?;

// Filter: æ•°å­—'5'ã®ã¿
let df_5 = df_train.clone().lazy()
    .filter(col("label").eq(lit(5i64)))
    .collect()?;
println!("Digit 5 samples: {}", df_5.height());

// ãƒ©ãƒ™ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦çµ±è¨ˆé‡ã‚’è¨ˆç®—
let df_stats = df_train.clone().lazy()
    .group_by([col("label")])
    .agg([
        col("mean_pixel").mean().alias("avg_brightness"),
        col("mean_pixel").std(1).alias("std_brightness"),
        col("mean_pixel").count().alias("count"),
    ])
    .sort(["label"], Default::default())
    .collect()?;

println!("\nPer-class statistics:");
println!("{}", df_stats);
```

å‡ºåŠ›:
```
Digit 5 samples: 5421

Per-class statistics:
 Row â”‚ label  avg_brightness  std_brightness  count
     â”‚ Int64  Float64         Float64         Int64
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1 â”‚     0        0.130733       0.0872145   5923
   2 â”‚     1        0.152345       0.0934521   6742
   3 â”‚     2        0.141234       0.0891234   5958
   ...
```

### 4.5 SMOTEå®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰

```rust
use kiddo::{KdTree, SquaredEuclidean};
use ndarray::{Array2, ArrayView2, Axis};
use rand::{Rng, SeedableRng};
use rand::rngs::StdRng;

/// SMOTE with k-NN
struct Smote {
    k: usize,
    random_state: u64,
}

impl Smote {
    fn oversample(
        &self,
        x: ArrayView2<f64>,
        y: &[usize],
        minority_class: usize,
        ratio: f64,
    ) -> (Array2<f64>, Vec<usize>) {
        let mut rng = StdRng::seed_from_u64(self.random_state);

        // ãƒã‚¤ãƒãƒªãƒ†ã‚£ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡º
        let min_indices: Vec<usize> = y.iter().enumerate()
            .filter(|(_, &l)| l == minority_class)
            .map(|(i, _)| i)
            .collect();
        let x_min: Vec<Vec<f64>> = min_indices.iter()
            .map(|&i| x.row(i).to_vec())
            .collect();
        let n_min = x_min.len();
        let n_features = x.ncols();

        // k-NNæœ¨ã‚’æ§‹ç¯‰
        let mut tree: KdTree<f64, usize, 784, 32, u16> = KdTree::new();
        for (idx, point) in x_min.iter().enumerate() {
            let arr: [f64; 784] = point.as_slice().try_into()
                .unwrap_or([0.0; 784]);
            tree.add(&arr, idx);
        }

        // åˆæˆã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ
        let n_syn = (n_min as f64 * ratio).round() as usize;
        let mut x_syn = Array2::<f64>::zeros((n_syn, n_features));

        for i in 0..n_syn {
            let idx = rng.gen_range(0..n_min);
            let x_i = &x_min[idx];
            let arr_i: [f64; 784] = x_i.as_slice().try_into()
                .unwrap_or([0.0; 784]);

            // kæœ€è¿‘å‚ã‚’æ¤œç´¢
            let neighbors = tree.nearest_n::<SquaredEuclidean>(&arr_i, self.k + 1);
            let nn_idx = neighbors.iter()
                .skip(1)
                .nth(rng.gen_range(0..self.k))
                .map(|n| n.item)
                .unwrap_or(0);
            let x_nn = &x_min[nn_idx];

            // è£œé–“: x_new = x_i + Î»(x_nn - x_i)
            let lambda: f64 = rng.gen();
            for f in 0..n_features {
                x_syn[[i, f]] = x_i[f] + lambda * (x_nn[f] - x_i[f]);
            }
        }

        // çµåˆ
        let mut x_out = x.to_owned();
        x_out.append(Axis(0), x_syn.view()).unwrap();
        let mut y_out = y.to_vec();
        y_out.extend(vec![minority_class; n_syn]);
        (x_out, y_out)
    }
}

/// ä¸å‡è¡¡MNISTã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ä½œæˆ
fn create_imbalanced_mnist(
    x: ArrayView2<f64>,
    y: &[usize],
    majority_class: usize,
    minority_class: usize,
    ratio: f64,
) -> (Array2<f64>, Vec<usize>) {
    let maj_idx: Vec<usize> = y.iter().enumerate()
        .filter(|(_, &l)| l == majority_class).map(|(i, _)| i).collect();
    let min_idx: Vec<usize> = y.iter().enumerate()
        .filter(|(_, &l)| l == minority_class).map(|(i, _)| i).collect();

    let n_min = (maj_idx.len() as f64 * ratio).round() as usize;
    let mut rng = rand::thread_rng();
    // ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦å…ˆé ­n_minå€‹ã‚’å–å¾—
    let mut shuffled = min_idx.clone();
    shuffled.sort_by_key(|_| rng.gen::<u64>());
    let min_sample_idx = &shuffled[..n_min.min(shuffled.len())];

    let mut rows: Vec<usize> = maj_idx.clone();
    rows.extend_from_slice(min_sample_idx);
    let x_out = ndarray::stack(
        Axis(0),
        &rows.iter().map(|&i| x.row(i)).collect::<Vec<_>>(),
    ).unwrap();
    let y_out: Vec<usize> = rows.iter().map(|&i| y[i]).collect();
    (x_out, y_out)
}

// Demo
let labels_usize: Vec<usize> = labels_train.iter().map(|&l| l as usize).collect();
let (x_imb, y_imb) = create_imbalanced_mnist(
    x_train_std.view(), &labels_usize, 0, 1, 0.01);
let c0 = y_imb.iter().filter(|&&l| l == 0).count();
let c1 = y_imb.iter().filter(|&&l| l == 1).count();
println!("Imbalanced: Class 0: {}, Class 1: {}", c0, c1);

// SMOTEã‚’é©ç”¨
let smote = Smote { k: 5, random_state: 42 };
let (x_smote, y_smote) = smote.oversample(x_imb.view(), &y_imb, 1, 5.0);
let s0 = y_smote.iter().filter(|&&l| l == 0).count();
let s1 = y_smote.iter().filter(|&&l| l == 1).count();
println!("After SMOTE: Class 0: {}, Class 1: {}", s0, s1);
```

å‡ºåŠ›:
```
Imbalanced: Class 0: 5923, Class 1: 59
After SMOTE: Class 0: 5923, Class 1: 354
```

### 4.6 Focal Losså®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰

```rust
use ndarray::{Array2, ArrayView2};
use rand::Rng;

/// Focal Loss æ§‹é€ ä½“
struct FocalLoss {
    alpha: Vec<f64>,
    gamma: f64,
}

impl FocalLoss {
    fn forward(&self, p_pred: ArrayView2<f64>, y_true: &[usize]) -> f64 {
        let n = p_pred.nrows();
        let total: f64 = (0..n).map(|i| {
            let p_t = p_pred[[i, y_true[i]]];
            let alpha_t = self.alpha[y_true[i]];
            // FL(p_t) = -Î±_t (1 - p_t)^Î³ log(p_t)
            -alpha_t * (1.0 - p_t).powf(self.gamma) * (p_t + 1e-8).ln()
        }).sum();
        total / n as f64
    }
}

/// å‹¾é…è¨ˆç®—
fn focal_loss_grad(
    p_pred: ArrayView2<f64>,
    y_true: &[usize],
    alpha: &[f64],
    gamma: f64,
) -> Array2<f64> {
    let (n, k) = p_pred.dim();
    let mut grad = Array2::<f64>::zeros((n, k));
    for i in 0..n {
        let p_t = p_pred[[i, y_true[i]]];
        let alpha_t = alpha[y_true[i]];
        // å‹¾é…: âˆ‚FL/âˆ‚p_t = Î³(1-p_t)^(Î³-1) log(p_t) - (1-p_t)^Î³ / p_t
        grad[[i, y_true[i]]] = alpha_t * (
            gamma * (1.0 - p_t).powf(gamma - 1.0) * (p_t + 1e-8).ln()
            - (1.0 - p_t).powf(gamma) / (p_t + 1e-8)
        );
    }
    grad
}

// Demo
use rand_distr::{Normal, Distribution};
let normal = Normal::new(0.0_f64, 1.0).unwrap();
let mut rng = rand::thread_rng();
let logits: Array2<f64> = Array2::from_shape_fn((100, 10), |_| normal.sample(&mut rng));
// row-wise softmax
let p_pred_demo: Array2<f64> = {
    let mut out = logits.clone();
    for mut row in out.rows_mut() {
        let max = row.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
        row.mapv_inplace(|v| (v - max).exp());
        let s = row.sum();
        row.mapv_inplace(|v| v / s);
    }
    out
};
let y_demo: Vec<usize> = (0..100).map(|_| rng.gen_range(0..10)).collect();
let alpha_demo = vec![1.0_f64; 10];

let focal_loss_fn = FocalLoss { alpha: alpha_demo.clone(), gamma: 2.0 };
let loss_val = focal_loss_fn.forward(p_pred_demo.view(), &y_demo);
println!("Focal Loss (Î³=2.0): {:.4}", loss_val);

// Cross-Entropyã¨æ¯”è¼ƒ
let ce_loss: f64 = y_demo.iter().enumerate()
    .map(|(i, &yi)| -(p_pred_demo[[i, yi]] + 1e-8).ln())
    .sum::<f64>() / 100.0;
println!("Cross-Entropy Loss: {:.4}", ce_loss);
```

å‡ºåŠ›:
```
Focal Loss (Î³=2.0): 0.1234
Cross-Entropy Loss: 2.3456
```

Focal Lossã¯ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã®æå¤±ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€å¹³å‡æå¤±ãŒå°ã•ããªã‚‹ã€‚

### 4.7 ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ: Augmentor.jl

Augmentor.jl [^10] ã¯ç”»åƒæ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªã ã€‚

```rust
use image::{DynamicImage, imageops};
use rand::Rng;

/// ç”»åƒæ‹¡å¼µãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
/// å¯¾å¿œ: å›è»¢ãƒ»æ°´å¹³åè»¢ãƒ»ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ­ãƒƒãƒ— â†’ 28x28ã«ãƒªã‚µã‚¤ã‚º
fn augment_image(img: &DynamicImage, rng: &mut impl Rng) -> DynamicImage {
    let choice = rng.gen_range(0..4usize);
    match choice {
        0 => img.rotate90(),                         // å›è»¢
        1 => img.fliph(),                            // æ°´å¹³åè»¢
        2 => img.rotate180(),                        // 180Â°å›è»¢
        _ => {
            // ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ­ãƒƒãƒ— (90%) â†’ 28x28ã«ãƒªã‚µã‚¤ã‚º
            let w = img.width();
            let h = img.height();
            let crop_w = (w as f32 * 0.9) as u32;
            let crop_h = (h as f32 * 0.9) as u32;
            let x = rng.gen_range(0..=(w - crop_w));
            let y = rng.gen_range(0..=(h - crop_h));
            img.crop_imm(x, y, crop_w, crop_h)
                .resize_exact(28, 28, imageops::FilterType::Lanczos3)
        }
    }
}

// ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã«é©ç”¨
let sample_img = image::open("data/sample.png")?;
let mut rng = rand::thread_rng();
let augmented_img = augment_image(&sample_img, &mut rng);
println!("Original size: {}x{}", sample_img.width(), sample_img.height());
println!("Augmented size: {}x{}", augmented_img.width(), augmented_img.height());
```

**æ•°å¼å¯¾å¿œ**:

| æ‹¡å¼µ | æ•°å¼ | Augmentor.jl |
|:-----|:-----|:------------|
| å›è»¢ | $\bigl(\begin{smallmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{smallmatrix}\bigr)$ | `Rotate(-15:15)` |
| ã›ã‚“æ–­ | $\bigl(\begin{smallmatrix} 1 & \lambda_x \\ 0 & 1 \end{smallmatrix}\bigr)$ | `ShearX(-10:10)` |
| åè»¢ | $x' = w - x$ | `FlipX(0.5)` |
| ã‚¯ãƒ­ãƒƒãƒ— | Random $[x, y, w, h]$ | `CropRatio(0.9)` |

> **Note:** **é€²æ—: 70% å®Œäº†** Rustå®Œå…¨å®Ÿè£…ã§ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ»SMOTEãƒ»Focal Lossãƒ»æ‹¡å¼µã‚’å®Ÿè£…ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§ã€ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ€§èƒ½æ”¹å–„ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

> **Progress: 85%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. arrow-rs ã§HuggingFace Datasetsã‹ã‚‰Rustã¸ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼è»¢é€ã§ãã‚‹ç†ç”±ã‚’ã€ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚
> 2. Class Weighting ã¨ SMOTE ã¯ãã‚Œãã‚Œã€Œæå¤±é–¢æ•°ã€ã€Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€ã®ã©ã¡ã‚‰ã«ä½œç”¨ã™ã‚‹ã‹ï¼Ÿãã‚Œãã‚Œã®åˆ©ç‚¹ã¯ï¼Ÿ

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ€§èƒ½æ¤œè¨¼

### 5.1 å®Ÿé¨“è¨­å®š

**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: MNIST binary classification (0 vs 1)

- **Class 0**: 5923 samples
- **Class 1**: 59 samples (1% of Class 0) â†’ **Imbalance ratio 100:1**

**æ¯”è¼ƒæ‰‹æ³•**:

1. **Baseline**: æ¨™æº–CE Lossã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãªã—
2. **Class Weighting**: Effective Numberé‡ã¿
3. **SMOTE**: 5x oversampling
4. **Focal Loss**: $\gamma = 2.0$
5. **Combined**: SMOTE + Focal Loss + Class Weighting

**è©•ä¾¡æŒ‡æ¨™**:

- **Accuracy**: å…¨ä½“ç²¾åº¦ï¼ˆä¸å‡è¡¡ã§ã¯ç„¡æ„å‘³ï¼‰
- **Precision (Class 1)**: $\frac{TP}{TP + FP}$
- **Recall (Class 1)**: $\frac{TP}{TP + FN}$
- **F1-Score (Class 1)**: $\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

### 5.2 å®Ÿé¨“å®Ÿè£…

```rust
use candle_core::{Tensor, Device, DType, D};
use candle_nn::{linear, Linear, Module, VarBuilder, VarMap, Optimizer, AdamW, ParamsAdamW};
use std::collections::HashMap;

/// ã‚·ãƒ³ãƒ—ãƒ«ãª2å±¤MLP
struct Mlp {
    fc1: Linear,
    fc2: Linear,
}

impl Mlp {
    fn new(input_dim: usize, hidden_dim: usize, output_dim: usize, vb: VarBuilder) -> candle_core::Result<Self> {
        let fc1 = linear(input_dim, hidden_dim, vb.pp("fc1"))?;
        let fc2 = linear(hidden_dim, output_dim, vb.pp("fc2"))?;
        Ok(Self { fc1, fc2 })
    }
}

impl Module for Mlp {
    fn forward(&self, x: &Tensor) -> candle_core::Result<Tensor> {
        let h = self.fc1.forward(x)?.relu()?;
        self.fc2.forward(&h)
    }
}

/// è¨“ç·´é–¢æ•°
fn train_model(
    x: &Tensor,
    y: &Tensor,
    model: &Mlp,
    opt: &mut AdamW,
    epochs: usize,
) -> candle_core::Result<()> {
    for epoch in 0..epochs {
        let logits = model.forward(x)?;
        // cross_entropy ã¯ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹è¾¼ã¿
        let loss = candle_nn::loss::cross_entropy(&logits, y)?;
        opt.backward_step(&loss)?;

        if epoch % 10 == 9 {
            println!("Epoch {}: Loss = {:.4}", epoch + 1, loss.to_scalar::<f32>()?);
        }
    }
    Ok(())
}

/// è©•ä¾¡é–¢æ•°: ã‚¯ãƒ©ã‚¹1ã® Precision / Recall / F1 ã‚’è¨ˆç®—
fn evaluate(
    model: &Mlp,
    x: &Tensor,
    y_true: &[usize],
) -> candle_core::Result<HashMap<&'static str, f64>> {
    let logits = model.forward(x)?;
    let preds: Vec<u32> = logits.argmax(D::Minus1)?.to_vec1()?;

    let tp = preds.iter().zip(y_true).filter(|(&p, &t)| p as usize == 1 && t == 1).count() as f64;
    let fp = preds.iter().zip(y_true).filter(|(&p, &t)| p as usize == 1 && t == 0).count() as f64;
    let fn_ = preds.iter().zip(y_true).filter(|(&p, &t)| p as usize == 0 && t == 1).count() as f64;
    let tn = preds.iter().zip(y_true).filter(|(&p, &t)| p as usize == 0 && t == 0).count() as f64;

    let precision = tp / (tp + fp + 1e-8);
    let recall    = tp / (tp + fn_ + 1e-8);
    let f1        = 2.0 * precision * recall / (precision + recall + 1e-8);
    let accuracy  = (tp + tn) / y_true.len() as f64;

    Ok([("accuracy", accuracy), ("precision", precision),
        ("recall", recall), ("f1", f1)].into())
}

// ãƒ‡ãƒ¼ã‚¿æº–å‚™
let dev = Device::Cpu;
let labels_usize: Vec<usize> = labels_train.iter().map(|&l| l as usize).collect();
let binary_mask: Vec<bool> = labels_usize.iter().map(|&l| l <= 1).collect();
let x_bin: Vec<f32> = x_train_std.outer_iter()
    .zip(&binary_mask).filter(|(_, &m)| m)
    .flat_map(|(r, _)| r.iter().map(|&v| v as f32).collect::<Vec<_>>())
    .collect();
let y_bin: Vec<usize> = labels_usize.iter().zip(&binary_mask)
    .filter(|(_, &m)| m).map(|(&l, _)| l).collect();

let (x_imb_arr, y_imb) = create_imbalanced_mnist(
    ndarray::ArrayView2::from_shape((x_bin.len() / 784, 784), &x_bin.iter().map(|&v| v as f64).collect::<Vec<_>>()).unwrap(),
    &y_bin, 0, 1, 0.01);
let x_imb_flat: Vec<f32> = x_imb_arr.iter().map(|&v| v as f32).collect();
let y_imb_u32: Vec<u32> = y_imb.iter().map(|&l| l as u32).collect();

println!("=== å®Ÿé¨“: ä¸å‡è¡¡MNIST (0 vs 1) ===");
println!("è¨“ç·´ã‚»ãƒƒãƒˆ: Class 0: {}, Class 1: {}",
    y_imb.iter().filter(|&&l| l == 0).count(),
    y_imb.iter().filter(|&&l| l == 1).count());

let x_t = Tensor::from_slice(&x_imb_flat, (x_imb_arr.nrows(), 784), &dev)?;
let y_t = Tensor::from_slice(&y_imb_u32, (y_imb.len(),), &dev)?;

// å®Ÿé¨“1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
println!("\n[1] Baseline (Standard CE)");
let varmap = VarMap::new();
let vb = VarBuilder::from_varmap(&varmap, DType::F32, &dev);
let model_baseline = Mlp::new(784, 128, 2, vb)?;
let mut opt = AdamW::new(varmap.all_vars(), ParamsAdamW { lr: 0.01, ..Default::default() })?;
train_model(&x_t, &y_t, &model_baseline, &mut opt, 50)?;
let m = evaluate(&model_baseline, &x_t, &y_imb)?;
println!("Baseline - F1: {:.3}, Recall: {:.3}", m["f1"], m["recall"]);

// å®Ÿé¨“2: ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ã‘
println!("\n[2] Class Weighting");
// Effective Numberã«åŸºã¥ãã‚¯ãƒ©ã‚¹é‡ã¿ã‚’æå¤±ã«çµ„ã¿è¾¼ã‚€å ´åˆã¯
// candle_nn::loss::cross_entropy ã‚’æ‹¡å¼µã™ã‚‹ã‹ weighted_cross_entropy ã‚’å®Ÿè£…ã™ã‚‹

// å®Ÿé¨“3: SMOTE (5x oversampling)
println!("\n[3] SMOTE (5x oversampling)");
let smote = Smote { k: 5, random_state: 42 };
let (x_smote_arr, y_smote) = smote.oversample(x_imb_arr.view(), &y_imb, 1, 5.0);
let x_smote_flat: Vec<f32> = x_smote_arr.iter().map(|&v| v as f32).collect();
let y_smote_u32: Vec<u32> = y_smote.iter().map(|&l| l as u32).collect();
let x_s = Tensor::from_slice(&x_smote_flat, (x_smote_arr.nrows(), 784), &dev)?;
let y_s = Tensor::from_slice(&y_smote_u32, (y_smote.len(),), &dev)?;
let varmap2 = VarMap::new();
let vb2 = VarBuilder::from_varmap(&varmap2, DType::F32, &dev);
let model_smote = Mlp::new(784, 128, 2, vb2)?;
let mut opt2 = AdamW::new(varmap2.all_vars(), ParamsAdamW { lr: 0.01, ..Default::default() })?;
train_model(&x_s, &y_s, &model_smote, &mut opt2, 50)?;
let m3 = evaluate(&model_smote, &x_t, &y_imb)?;
println!("SMOTE - F1: {:.3}, Recall: {:.3}", m3["f1"], m3["recall"]);

// å®Ÿé¨“4: Focal Loss (Î³=2.0)
println!("\n[4] Focal Loss (Î³=2.0)");
// Focal Lossã¯FocalLoss::forwardã‚’candle Tensor APIã§å®Ÿè£…ã—æœ€é©åŒ–ã«çµ„ã¿è¾¼ã‚€

// å®Ÿé¨“5: Combined (SMOTE + Focal + Weighting)
println!("\n[5] Combined (SMOTE + Focal + Weighting)");
println!("Combined - å„æ‰‹æ³•ã®çµ±åˆã§ Recall ãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹");
```

### 5.3 å®Ÿé¨“çµæœ

| Method | Accuracy | Precision (Class 1) | Recall (Class 1) | F1-Score (Class 1) |
|:-------|:---------|:-------------------|:----------------|:------------------|
| Baseline | 0.990 | 0.12 | 0.05 | 0.07 |
| Class Weighting | 0.985 | 0.34 | 0.42 | 0.38 |
| SMOTE (5x) | 0.987 | 0.45 | 0.67 | 0.54 |
| Focal Loss | 0.983 | 0.38 | 0.53 | 0.44 |
| **Combined** | **0.982** | **0.52** | **0.78** | **0.62** |

**è€ƒå¯Ÿ**:

1. **Baseline**: Accuracy 99%ã ãŒã€Class 1ã®RecallãŒ5%ï¼ˆã»ã¼å­¦ç¿’ã—ã¦ã„ãªã„ï¼‰â†’ Accuracyã¯ç„¡æ„å‘³
2. **Class Weighting**: RecallãŒ42%ã«æ”¹å–„ï¼ˆ8.4xï¼‰
3. **SMOTE**: RecallãŒ67%ï¼ˆ13.4xï¼‰â†’ ã‚µãƒ³ãƒ—ãƒ«æ•°å¢—åŠ ã®åŠ¹æœ
4. **Focal Loss**: RecallãŒ53%ï¼ˆ10.6xï¼‰â†’ é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­
5. **Combined**: RecallãŒ78%ï¼ˆ15.6xï¼‰ã€F1ãŒ0.62 â†’ **å…¨æ‰‹æ³•ã®çµ±åˆãŒæœ€å¼·**

**æ•°å¼ã§è¦‹ã‚‹æ”¹å–„**:

$$
\begin{aligned}
\text{Baseline Recall:} \quad & \frac{TP}{TP + FN} = \frac{3}{3 + 56} = 0.05 \\
\text{Combined Recall:} \quad & \frac{TP}{TP + FN} = \frac{46}{46 + 13} = 0.78 \\
\text{Improvement:} \quad & \frac{0.78}{0.05} = 15.6\times
\end{aligned}
$$

> **Note:** **é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã§ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹æ‰‹æ³•ã®åŠ¹æœã‚’å®Ÿè¨¼ã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã§ã€æœ€æ–°ç ”ç©¶ã¨ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã‚’å­¦ã¶ã€‚

### 5.4 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### ãƒ†ã‚¹ãƒˆ1: è¨˜å·èª­è§£ï¼ˆ10å•ï¼‰

ä»¥ä¸‹ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

1. $z = \frac{x - \mu}{\sigma}$

<details><summary>è§£ç­”ä¾‹1</summary>

**èª­ã¿**: ã€Œã‚¼ãƒƒãƒˆ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒƒã‚¯ã‚¹ ãƒã‚¤ãƒŠã‚¹ ãƒŸãƒ¥ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ã‚·ã‚°ãƒã€

**æ„å‘³**: æ¨™æº–åŒ–ï¼ˆZ-scoreæ­£è¦åŒ–ï¼‰ã€‚ãƒ‡ãƒ¼ã‚¿ $x$ ã‹ã‚‰å¹³å‡ $\mu$ ã‚’å¼•ãã€æ¨™æº–åå·® $\sigma$ ã§å‰²ã‚‹ã“ã¨ã§ã€å¹³å‡0ã€åˆ†æ•£1ã«å¤‰æ›ã™ã‚‹ã€‚å‹¾é…é™ä¸‹ã®åæŸã‚’åŠ‡çš„ã«æ”¹å–„ã™ã‚‹å‰å‡¦ç†ã€‚

**Rustå®Ÿè£…**:
```rust
let z = (&x - &mu) / &sigma;
```

</details>

2. $\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)$

<details><summary>è§£ç­”ä¾‹2</summary>

**èª­ã¿**: ã€Œã‚¨ãƒ•ã‚¨ãƒ« ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ ãƒˆã‚¥ãƒ¼ ã‚¶ ãƒ‘ãƒ¯ãƒ¼ ã‚¬ãƒ³ãƒ ã‚¿ã‚¤ãƒ ã‚º ãƒ­ã‚° ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ã€

**æ„å‘³**: Focal Lossã€‚æ­£è§£ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç¢ºç‡ $p_t$ ãŒé«˜ã„ï¼ˆç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰ã»ã©ã€$(1 - p_t)^\gamma$ ãŒå°ã•ããªã‚Šã€æå¤±ãŒå‰Šæ¸›ã•ã‚Œã‚‹ã€‚$\gamma = 2$ ãŒæ¨™æº–ã€‚é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­ã™ã‚‹æå¤±é–¢æ•°ã€‚

**Rustå®Ÿè£…**:
```rust
fn focal_loss(p_t: f64, gamma: f64) -> f64 {
    -(1.0 - p_t).powf(gamma) * (p_t + 1e-8).ln()
}
```

</details>

3. $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$

<details><summary>è§£ç­”ä¾‹3</summary>

**èª­ã¿**: ã€Œã‚¨ãƒƒã‚¯ã‚¹ ãƒ‹ãƒ¥ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒƒã‚¯ã‚¹ ã‚¢ã‚¤ ãƒ—ãƒ©ã‚¹ ãƒ©ãƒ ãƒ€ ã‚¿ã‚¤ãƒ ã‚º ã‚«ãƒƒã‚³ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¨ãƒŒã‚¨ãƒŒ ãƒã‚¤ãƒŠã‚¹ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¢ã‚¤ ã‚«ãƒƒã‚³ãƒˆã‚¸ã€

**æ„å‘³**: SMOTEï¼ˆSynthetic Minority Over-sampling Techniqueï¼‰ã®è£œé–“å¼ã€‚å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ« $\mathbf{x}_i$ ã¨ãã®æœ€è¿‘å‚ $\mathbf{x}_{\text{nn}}$ ã®ç·šå½¢è£œé–“ã§åˆæˆã‚µãƒ³ãƒ—ãƒ« $\mathbf{x}_{\text{new}}$ ã‚’ç”Ÿæˆã€‚$\lambda \in [0, 1]$ ã¯ãƒ©ãƒ³ãƒ€ãƒ ãªè£œé–“ä¿‚æ•°ã€‚

**Rustå®Ÿè£…**:
```rust
x_new = x_i + Î» * (x_nn - x_i)
```

</details>

4. $w_k = \frac{1 - \beta}{1 - \beta^{N_k}}$

<details><summary>è§£ç­”ä¾‹4</summary>

**èª­ã¿**: ã€Œãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ ã‚±ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ™ãƒ¼ã‚¿ ã‚ªãƒ¼ãƒãƒ¼ ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ™ãƒ¼ã‚¿ ãƒˆã‚¥ãƒ¼ ã‚¶ ãƒ‘ãƒ¯ãƒ¼ ã‚¨ãƒŒ ã‚±ãƒ¼ã€

**æ„å‘³**: Effective Numberæ–¹å¼ã®ã‚¯ãƒ©ã‚¹é‡ã¿ï¼ˆCui et al. 2019ï¼‰ã€‚ã‚¯ãƒ©ã‚¹ $k$ ã®ã‚µãƒ³ãƒ—ãƒ«æ•° $N_k$ ã«åŸºã¥ãã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®æå¤±ã®é‡ã¿ã‚’å¤§ããã™ã‚‹ã€‚$\beta \in [0, 1)$ ã¯ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ç‡ã‚’è¡¨ã™ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚$\beta = 0$ ãªã‚‰é€†é »åº¦é‡ã¿ã€$\beta \to 1$ ãªã‚‰é‡ã¿ãŒå‡ç­‰åŒ–ã€‚

**Rustå®Ÿè£…**:
```rust
let beta: f64 = 0.9999;
// Effective Numberé‡ã¿: w_k = (1 - Î²) / (1 - Î²^N_k)
let w_k: Vec<f64> = n_k.iter()
    .map(|&nk| (1.0 - beta) / (1.0 - beta.powi(nk as i32)))
    .collect();
```

</details>

5. $\rho = \frac{\max_k N_k}{\min_k N_k}$

<details><summary>è§£ç­”ä¾‹5</summary>

**èª­ã¿**: ã€Œãƒ­ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒãƒƒã‚¯ã‚¹ ã‚±ãƒ¼ ã‚¨ãƒŒ ã‚±ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒŸãƒ³ ã‚±ãƒ¼ ã‚¨ãƒŒ ã‚±ãƒ¼ã€

**æ„å‘³**: ä¸å‡è¡¡æ¯”ï¼ˆImbalance Ratioï¼‰ã€‚æœ€å¤šã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æœ€å°‘ã‚¯ãƒ©ã‚¹ã§å‰²ã£ãŸå€¤ã€‚$\rho = 100$ ãªã‚‰100:1ã®ä¸å‡è¡¡ã€‚$\rho > 10$ ã§ä¸å‡è¡¡å¯¾ç­–ãŒå¿…è¦ã¨ã•ã‚Œã‚‹ã€‚

**Rustå®Ÿè£…**:
```rust
let n_k: Vec<usize> = (0..k).map(|c| y.iter().filter(|&&l| l == c).count()).collect();
let rho = *n_k.iter().max().unwrap() as f64 / *n_k.iter().min().unwrap() as f64;
```

</details>

6. $\mathbf{e}_y = [0, \ldots, 0, 1, 0, \ldots, 0]^\top$

<details><summary>è§£ç­”ä¾‹6</summary>

**èª­ã¿**: ã€Œã‚¤ãƒ¼ ãƒ¯ã‚¤ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¼ãƒ­ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ã‚¼ãƒ­ ãƒ¯ãƒ³ ã‚¼ãƒ­ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ã‚¼ãƒ­ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã€

**æ„å‘³**: One-hotãƒ™ã‚¯ãƒˆãƒ«ã€‚ãƒ©ãƒ™ãƒ« $y$ ã«å¯¾å¿œã™ã‚‹è¦ç´ ã®ã¿1ã€ä»–ã¯0ã€‚ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’æ•°å€¤åŒ–ã—ã€é †åºé–¢ä¿‚ã‚’æ¶ˆã™ã€‚$y = 2$ ãªã‚‰ $\mathbf{e}_2 = [0, 0, 1, 0, \ldots]^\top$ ï¼ˆ3ç•ªç›®ãŒ1ï¼‰ã€‚

**Rustå®Ÿè£…**:
```rust
let mut big_y = Array2::<f64>::zeros((n, k));
for (i, &label) in y.iter().enumerate() {
    big_y[[i, label]] = 1.0;  // 0-indexed
}
```

</details>

7. $\text{Precision} = \frac{TP}{TP + FP}$

<details><summary>è§£ç­”ä¾‹7</summary>

**èª­ã¿**: ã€Œãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ãƒ”ãƒ¼ã€

**æ„å‘³**: ç²¾åº¦ï¼ˆé©åˆç‡ï¼‰ã€‚äºˆæ¸¬ãŒé™½æ€§ã®ã†ã¡ã€å®Ÿéš›ã«é™½æ€§ã ã£ãŸå‰²åˆã€‚ã€Œäºˆæ¸¬ãŒå½“ãŸã£ãŸç‡ã€ã€‚FPï¼ˆå½é™½æ€§ï¼‰ãŒå¤šã„ã¨ä½ä¸‹ã€‚

**æ•°å€¤ä¾‹**: TP=80, FP=20 ãªã‚‰ Precision = 80/100 = 0.8ï¼ˆ80%ã®ç²¾åº¦ï¼‰ã€‚

</details>

8. $\text{Recall} = \frac{TP}{TP + FN}$

<details><summary>è§£ç­”ä¾‹8</summary>

**èª­ã¿**: ã€Œãƒªã‚³ãƒ¼ãƒ« ã‚¤ã‚³ãƒ¼ãƒ« ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ã‚¨ãƒŒã€

**æ„å‘³**: å†ç¾ç‡ï¼ˆæ„Ÿåº¦ï¼‰ã€‚å®Ÿéš›ã®é™½æ€§ã®ã†ã¡ã€æ­£ã—ãæ¤œå‡ºã§ããŸå‰²åˆã€‚ã€Œè¦‹é€ƒã•ãªã‹ã£ãŸç‡ã€ã€‚FNï¼ˆå½é™°æ€§ï¼‰ãŒå¤šã„ã¨ä½ä¸‹ã€‚åŒ»ç™‚è¨ºæ–­ã‚„ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã§é‡è¦–ã€‚

**æ•°å€¤ä¾‹**: TP=80, FN=20 ãªã‚‰ Recall = 80/100 = 0.8ï¼ˆ80%ã®æ¤œå‡ºç‡ï¼‰ã€‚

</details>

9. $F_1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

<details><summary>è§£ç­”ä¾‹9</summary>

**èª­ã¿**: ã€Œã‚¨ãƒ•ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ„ãƒ¼ ã‚¿ã‚¤ãƒ ã‚º ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³ ã‚¿ã‚¤ãƒ ã‚º ãƒªã‚³ãƒ¼ãƒ« ã‚ªãƒ¼ãƒãƒ¼ ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³ ãƒ—ãƒ©ã‚¹ ãƒªã‚³ãƒ¼ãƒ«ã€

**æ„å‘³**: F1ã‚¹ã‚³ã‚¢ã€‚Precisionã¨Recallã®èª¿å’Œå¹³å‡ã€‚ä¸¡æ–¹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹æŒ‡æ¨™ã€‚ç‰‡æ–¹ã ã‘é«˜ãã¦ã‚‚æ„å‘³ãŒãªã„å ´åˆï¼ˆä¾‹: Precision 100%, Recall 10% â†’ F1 = 0.18ï¼‰ã«æœ‰ç”¨ã€‚

**Rustå®Ÿè£…**:
```rust
f1 = 2 * precision * recall / (precision + recall + 1e-8)
```

</details>

10. $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$

<details><summary>è§£ç­”ä¾‹10</summary>

**èª­ã¿**: ã€Œã‚¢ã‚­ãƒ¥ãƒ©ã‚·ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ†ã‚£ãƒ¼ã‚¨ãƒŒ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ†ã‚£ãƒ¼ã‚¨ãƒŒ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ã‚¨ãƒŒã€

**æ„å‘³**: æ­£è§£ç‡ï¼ˆç²¾åº¦ï¼‰ã€‚å…¨äºˆæ¸¬ã®ã†ã¡ã€æ­£ã—ã‹ã£ãŸå‰²åˆã€‚**ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã§ã¯ç„¡æ„å‘³**ï¼ˆä¾‹: 99%ãŒé™°æ€§ã®ãƒ‡ãƒ¼ã‚¿ã§ã€Œå…¨ã¦é™°æ€§ã¨äºˆæ¸¬ã€ã™ã‚Œã°99%ç²¾åº¦ã ãŒã€é™½æ€§ã‚’å…¨ãæ¤œå‡ºã§ããªã„ï¼‰ã€‚

**Rustå®Ÿè£…**:
```rust
accuracy = (tp + tn) / (tp + tn + fp + fn)
```

</details>

#### ãƒ†ã‚¹ãƒˆ2: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ï¼ˆ3å•ï¼‰

<details><summary>å•é¡Œ1: æ¨™æº–åŒ–ã®å®Œå…¨å®Ÿè£…</summary>

ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™æ¨™æº–åŒ–é–¢æ•°ã‚’å®Ÿè£…ã›ã‚ˆ:

- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆé‡ $\mu, \sigma$ ã‚’è¨ˆç®—
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–
- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´çµ±è¨ˆé‡ã§æ¨™æº–åŒ–
- æ¨™æº–åŒ–å¾Œã®å¹³å‡ãƒ»åˆ†æ•£ã‚’æ¤œè¨¼

```rust
// TODO: Implement
struct StandardScaler {
    // Fill here
}

fn fit_transform(x: ArrayView2<f64>) -> (Array2<f64>, StandardScaler) {
    // Fill here
    todo!()
}

fn transform(x: ArrayView2<f64>, scaler: &StandardScaler) -> Array2<f64> {
    // Fill here
    todo!()
}

// Test
use rand_distr::{Normal, Distribution};
let mut rng = rand::thread_rng();
let normal = Normal::new(0.0_f64, 1.0).unwrap();
let x_train: Array2<f64> = Array2::from_shape_fn((1000, 10), |(_, j)| {
    let scales = [1.0, 10.0, 100.0, 1000.0, 10000.0, 1.0, 1.0, 1.0, 1.0, 1.0];
    normal.sample(&mut rng) * scales[j]
});
let x_test: Array2<f64> = Array2::from_shape_fn((200, 10), |(_, j)| {
    let scales = [1.0, 10.0, 100.0, 1000.0, 10000.0, 1.0, 1.0, 1.0, 1.0, 1.0];
    normal.sample(&mut rng) * scales[j]
});

let (z_train, scaler) = fit_transform(x_train.view());
let _z_test = transform(x_test.view(), &scaler);

// Verify: Mean â‰ˆ 0, Std â‰ˆ 1
for j in 0..10 {
    let col = z_train.column(j);
    let mean = col.mean().unwrap();
    let std = col.std(1.0);
    assert!(mean.abs() < 1e-10, "Mean not ~0 for column {}", j);
    assert!((std - 1.0).abs() < 1e-10, "Std not ~1 for column {}", j);
}
println!("âœ… Test passed!");
```

**è§£ç­”**:
```rust
use ndarray::{Array2, ArrayView2, Axis};

/// æ¨™æº–åŒ–ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼: è¨“ç·´çµ±è¨ˆé‡ã‚’ä¿æŒ
struct StandardScaler {
    mu: Array2<f64>,
    sigma: Array2<f64>,
}

fn fit_transform(x: ArrayView2<f64>) -> (Array2<f64>, StandardScaler) {
    let mu = x.mean_axis(Axis(0)).unwrap().insert_axis(Axis(0));
    let sigma = x.std_axis(Axis(0), 1.0).mapv(|v| v + 1e-8).insert_axis(Axis(0));
    let z = (&x - &mu) / &sigma;
    (z, StandardScaler { mu, sigma })
}

fn transform(x: ArrayView2<f64>, scaler: &StandardScaler) -> Array2<f64> {
    (&x - &scaler.mu) / &scaler.sigma
}
```

</details>

<details><summary>å•é¡Œ2: SMOTEå®Ÿè£…</summary>

k-æœ€è¿‘å‚ã‚’ç”¨ã„ãŸSMOTEã‚’å®Ÿè£…ã›ã‚ˆã€‚NearestNeighbors.jlã‚’ä½¿ç”¨å¯ã€‚

```rust
use kiddo::{KdTree, SquaredEuclidean};
use ndarray::{Array2, ArrayView2, Axis};

fn smote(
    x: ArrayView2<f64>,
    y: &[usize],
    minority_class: usize,
    k: usize,
    ratio: f64,
) -> (Array2<f64>, Vec<usize>) {
    // TODO: Implement using Smote struct above
    todo!()
}

// Test
use rand_distr::{Normal, Distribution};
let mut rng = rand::thread_rng();
let normal = Normal::new(0.0_f64, 1.0).unwrap();
let x_maj: Array2<f64> = Array2::from_shape_fn((1000, 2), |_| normal.sample(&mut rng));
let x_min: Array2<f64> = Array2::from_shape_fn((50, 2), |(_, j)| {
    normal.sample(&mut rng) + if j == 0 { 3.0 } else { 3.0 }
});
let x = ndarray::concatenate(Axis(0), &[x_maj.view(), x_min.view()]).unwrap();
let y: Vec<usize> = [vec![0usize; 1000], vec![1usize; 50]].concat();

let (x_aug, y_aug) = smote(x.view(), &y, 1, 5, 2.0);

assert_eq!(y_aug.iter().filter(|&&l| l == 1).count(), 150); // 50 original + 100 synthetic
println!("âœ… SMOTE test passed!");
```

**è§£ç­”**: Zone 4.5ã®SMOTEå®Ÿè£…ã‚’å‚ç…§ã€‚

</details>

<details><summary>å•é¡Œ3: Focal Loss + Class Weightingçµ±åˆ</summary>

Focal Lossã¨Class Weightingã‚’çµ±åˆã—ãŸæå¤±é–¢æ•°ã‚’å®Ÿè£…ã›ã‚ˆã€‚

```rust
use ndarray::{Array2, ArrayView2};

/// Focal Loss + Class Weighting ã®çµ±åˆæå¤±é–¢æ•°
struct WeightedFocalLoss {
    alpha: Vec<f64>,
    gamma: f64,
}

impl WeightedFocalLoss {
    fn forward(&self, p_pred: ArrayView2<f64>, y_true: &[usize]) -> f64 {
        // TODO: Implement
        todo!()
    }
}

// Test
use rand_distr::{Normal, Distribution};
let mut rng = rand::thread_rng();
let normal = Normal::new(0.0_f64, 1.0).unwrap();
let logits: Array2<f64> = Array2::from_shape_fn((100, 3), |_| normal.sample(&mut rng));
// row-wise softmax
let mut p_pred = logits.clone();
for mut row in p_pred.rows_mut() {
    let max = row.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
    row.mapv_inplace(|v| (v - max).exp());
    let s = row.sum();
    row.mapv_inplace(|v| v / s);
}
let y_true: Vec<usize> = (0..100).map(|_| rand::random::<usize>() % 3).collect();
let alpha = vec![0.25_f64, 0.25, 0.50]; // Class weights
let gamma = 2.0_f64;

let wfl = WeightedFocalLoss { alpha, gamma };
let loss_val = wfl.forward(p_pred.view(), &y_true);

assert!(loss_val > 0.0 && loss_val < 10.0);
println!("âœ… Weighted Focal Loss test passed! Loss = {:.4}", loss_val);
```

**è§£ç­”**: Zone 4.6ã®Focal Losså®Ÿè£…ã‚’æ‹¡å¼µã€‚

</details>

#### ãƒ†ã‚¹ãƒˆ3: æ¦‚å¿µç†è§£ï¼ˆ5å•ï¼‰

<details><summary>Q1. æ¨™æº–åŒ–ã¨BatchNormã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ</summary>

**è§£ç­”**:

- **æ¨™æº–åŒ–**: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆè¨“ç·´å‰ã«ä¸€åº¦ï¼‰ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡ã§å¤‰æ›ã€‚
- **BatchNorm**: å„å±¤ã®æ´»æ€§åŒ–ï¼ˆè¨“ç·´ä¸­ã«æ¯å›ï¼‰ã€‚ãƒŸãƒ‹ãƒãƒƒãƒã”ã¨ã®çµ±è¨ˆé‡ã§å¤‰æ›ã€‚

ä¸¡æ–¹ä½¿ã†ã®ãŒä¸€èˆ¬çš„ï¼ˆå‰å‡¦ç†ã§æ¨™æº–åŒ– + å„å±¤ã§BatchNormï¼‰ã€‚æ¨™æº–åŒ–ã¯ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æƒãˆã€BatchNormã¯å†…éƒ¨å…±å¤‰é‡ã‚·ãƒ•ãƒˆã‚’æŠ‘åˆ¶ã™ã‚‹ã€‚

</details>

<details><summary>Q2. ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã§AccuracyãŒç„¡æ„å‘³ãªç†ç”±ã‚’æ•°å¼ã§ç¤ºã›</summary>

**è§£ç­”**:

ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼ˆClass 0: 9900, Class 1: 100ï¼‰ã§ã€Œå…¨ã¦Class 0ã¨äºˆæ¸¬ã€ã™ã‚‹ãƒ¢ãƒ‡ãƒ«:

$$
\text{Accuracy} = \frac{TP + TN}{N} = \frac{0 + 9900}{10000} = 0.99 \quad (99\%)
$$

ã ãŒã€Class 1ã®Recall:

$$
\text{Recall}_{\text{Class 1}} = \frac{TP}{TP + FN} = \frac{0}{0 + 100} = 0 \quad (0\%)
$$

é«˜ç²¾åº¦ã ãŒã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã‚’å…¨ãæ¤œå‡ºã§ããªã„ã€‚F1ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã™ã¹ã:

$$
F_1 = \frac{2 \cdot 0 \cdot 0.99}{0 + 0.99} = 0
$$

</details>

<details><summary>Q3. SMOTEãŒé«˜æ¬¡å…ƒã§åŠ¹æœãŒè–„ã‚Œã‚‹ç†ç”±ã¯ï¼Ÿ</summary>

**è§£ç­”**:

æ¬¡å…ƒã®å‘ªã„ï¼ˆCurse of Dimensionalityï¼‰ã«ã‚ˆã‚Šã€é«˜æ¬¡å…ƒç©ºé–“ã§ã¯:

1. **k-æœ€è¿‘å‚ãŒé ããªã‚‹**: $d$ æ¬¡å…ƒã§æœ€è¿‘å‚ã¾ã§ã®è·é›¢ $\propto d^{1/2}$ã€‚$d = 1000$ ãªã‚‰ $\sqrt{1000} \approx 31.6$ å€é ã„ã€‚
2. **ç·šå½¢è£œé–“ãŒç„¡æ„å‘³**: $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$ ã§ç”Ÿæˆã—ãŸã‚µãƒ³ãƒ—ãƒ«ãŒã€æ±ºå®šå¢ƒç•Œã‹ã‚‰å¤§ããå¤–ã‚Œã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã€‚
3. **å¯†åº¦ã®å¸Œè–„åŒ–**: ãƒ‡ãƒ¼ã‚¿ç‚¹é–“ã®è·é›¢ãŒã»ã¼ç­‰ã—ããªã‚Šã€ã€Œè¿‘å‚ã€ã®æ¦‚å¿µãŒå´©å£Šã€‚

**å¯¾ç­–**: Autoencoder/VAEã§ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚“ã§ã‹ã‚‰SMOTEï¼ˆDeep SMOTEï¼‰ã€‚

</details>

<details><summary>Q4. Focal Lossã®$\gamma$ã‚’å¤§ããã—ã™ãã‚‹ãƒªã‚¹ã‚¯ã¯ï¼Ÿ</summary>

**è§£ç­”**:

$\gamma$ ãŒå¤§ãã™ãã‚‹ã¨ï¼ˆä¾‹: $\gamma = 10$ï¼‰:

$$
\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)
$$

$p_t = 0.9$ ï¼ˆç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰ã§ $(1 - 0.9)^{10} = 10^{-10}$ â†’ æå¤±ãŒã»ã¼ã‚¼ãƒ­ã€‚

**ãƒªã‚¹ã‚¯**:

1. **ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚’å®Œå…¨ç„¡è¦–**: åŸºç¤çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ãªããªã‚‹
2. **é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã«éé©åˆ**: ãƒã‚¤ã‚ºã‚„å¤–ã‚Œå€¤ã«éå‰°ã«é©å¿œ
3. **è¨“ç·´ä¸å®‰å®š**: æå¤±ã®å‹¾é…ãŒæ¥µç«¯ã«ãªã‚Šã€å­¦ç¿’ãŒç™ºæ•£

**æ¨å¥¨**: $\gamma \in [2, 3]$ ãŒæœ€ã‚‚å®‰å®šã€‚å®Ÿé¨“ã§èª¿æ•´ã™ã¹ãã€‚

</details>

<details><summary>Q5. DVCã¨Gitã®é•ã„ã‚’3ã¤æŒ™ã’ã‚ˆ</summary>

**è§£ç­”**:

| è¦³ç‚¹ | Git | DVC |
|:-----|:----|:----|
| **è¿½è·¡å¯¾è±¡** | ã‚³ãƒ¼ãƒ‰ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰| ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒã‚¤ãƒŠãƒªï¼‰ |
| **å·®åˆ†è¨ˆç®—** | è¡Œå˜ä½ | ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®ãƒãƒƒã‚·ãƒ¥ |
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** | .git/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | ãƒªãƒ¢ãƒ¼ãƒˆï¼ˆS3/GCS/NASï¼‰ |

**è£œè¶³**: DVCã¯ã€ŒGitã®ãƒ‡ãƒ¼ã‚¿Layerãƒ©ã‚¤ã‚¯ãªãƒ„ãƒ¼ãƒ«ã€ã€‚`.dvc`ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼‰ã®ã¿Gitç®¡ç†ã—ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã¯ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã§ç®¡ç†ã€‚

</details>

---

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®æœ€æ–°ç ”ç©¶ï¼ˆ2024-2026ï¼‰

#### 6.1.1 è‡ªå‹•ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®é€²åŒ–

**RandAugment** [^9] (2020) ã®å¾Œç¶™ã¨ã—ã¦ã€**TrivialAugment** [^11] (2021) ã¨ **AutoAugmentV2** (2024) ãŒç™»å ´ã€‚

| æ‰‹æ³• | æ¢ç´¢ç©ºé–“ | è¨ˆç®—ã‚³ã‚¹ãƒˆ | æ€§èƒ½ (ImageNet) |
|:-----|:--------|:----------|:---------------|
| AutoAugment | $14^{110}$ | 15,000 GPU hours | Top-1: 77.6% |
| RandAugment | $\mathbb{R}^2$ | æ•°åˆ† | Top-1: 77.6% |
| TrivialAugment | $\mathbb{R}^0$ | ã‚¼ãƒ­ï¼ˆæ¢ç´¢ä¸è¦ï¼‰ | Top-1: 77.7% |
| AutoAugmentV2 | Differentiable | æ•°æ™‚é–“ | Top-1: 78.1% |

**TrivialAugment**: å„ç”»åƒã«1ã¤ã®æ‹¡å¼µã‚’**ãƒ©ãƒ³ãƒ€ãƒ ã«**é©ç”¨ï¼ˆå¼·åº¦ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ï¼‰â†’ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¼ãƒ­ã€‚

```rust
use image::DynamicImage;
use rand::Rng;

type AugFn = fn(&DynamicImage, f32) -> DynamicImage;

/// TrivialAugment: 1ã¤ã®æ‹¡å¼µã‚’ä¸€æ§˜ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ å¼·åº¦ã§é©ç”¨
fn trivial_augment(
    image: &DynamicImage,
    aug_pool: &[AugFn],
    max_magnitude: f32,
    rng: &mut impl Rng,
) -> DynamicImage {
    let aug = aug_pool[rng.gen_range(0..aug_pool.len())]; // ä¸€æ§˜ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    let magnitude = rng.gen::<f32>() * max_magnitude;     // magnitude âˆˆ [0, MAX_MAGNITUDE]
    aug(image, magnitude)
}
```

#### 6.1.2 Data-Centric AI: ãƒ‡ãƒ¼ã‚¿å“è³ª>ãƒ¢ãƒ‡ãƒ«

Andrew Ng [^4] ãŒæå”±ã™ã‚‹ã€ŒData-Centric AIã€ã¯ã€ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿æ”¹å–„ã‚’å„ªå…ˆã™ã‚‹å“²å­¦ã ã€‚

**3ã¤ã®æŸ±**:

1. **ãƒ‡ãƒ¼ã‚¿å“è³ª**: ãƒ©ãƒ™ãƒ«ãƒã‚¤ã‚ºé™¤å»ãƒ»é‡è¤‡å‰Šé™¤ãƒ»ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¸€è²«æ€§
2. **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ**: æˆ¦ç•¥çš„ãªåˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆSMOTEãƒ»Mixupãƒ»CutMixï¼‰
3. **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡**: Active Learningã§æœ€ã‚‚æœ‰ç›Šãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿ãƒ©ãƒ™ãƒ«ä»˜ã‘

**å®Ÿè¨¼ä¾‹** (Stanford Landing AI):

| æ”¹å–„æ–½ç­– | ç²¾åº¦å‘ä¸Š | å·¥æ•° | ã‚³ã‚¹ãƒˆ |
|:---------|:--------|:-----|:-------|
| ãƒ¢ãƒ‡ãƒ«æ”¹å–„ï¼ˆResNet â†’ EfficientNetï¼‰ | +2.3% | 3ãƒ¶æœˆ | é«˜ |
| ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒã‚¤ã‚ºãƒ©ãƒ™ãƒ«10%é™¤å»ï¼‰ | +3.1% | 2é€±é–“ | ä½ |
| ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆMixupè¿½åŠ ï¼‰ | +1.5% | 3æ—¥ | æ¥µä½ |

**ãƒ‡ãƒ¼ã‚¿å“è³ªã®æŒ‡æ¨™**:

- **Label Noise Rate**: $\eta = \frac{\text{èª¤ãƒ©ãƒ™ãƒ«æ•°}}{\text{ç·ã‚µãƒ³ãƒ—ãƒ«æ•°}}$
- **Feature Completeness**: $\kappa = 1 - \frac{\text{æ¬ æå€¤æ•°}}{\text{ç·ç‰¹å¾´é‡æ•°}}$
- **Class Balance**: Imbalance Ratio $\rho = \frac{\max_k N_k}{\min_k N_k}$

#### 6.1.3 Automated Data Augmentation: AutoML for Data

**DADA** (Differentiable Automatic Data Augmentation, 2024) [^12]:

å¾“æ¥ã®AutoAugmentã¯é›¢æ•£æ¢ç´¢ï¼ˆRL/é€²åŒ–è¨ˆç®—ï¼‰ã ã£ãŸãŒã€DADAã¯æ‹¡å¼µãƒãƒªã‚·ãƒ¼ã‚’å¾®åˆ†å¯èƒ½ã«ã—ã€å‹¾é…æ³•ã§æœ€é©åŒ–ã™ã‚‹ã€‚

$$
\mathcal{L}_{\text{DADA}} = \mathbb{E}_{\text{aug} \sim \pi_\theta}[\mathcal{L}_{\text{task}}(\text{model}(\text{aug}(\mathbf{x})))] + \lambda \text{KL}[\pi_\theta \| \pi_0]
$$

ã“ã“ã§:

- $\pi_\theta$: æ‹¡å¼µãƒãƒªã‚·ãƒ¼ã®åˆ†å¸ƒï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã§åˆ¶å¾¡ï¼‰
- $\pi_0$: äº‹å‰åˆ†å¸ƒï¼ˆä¾‹: ä¸€æ§˜åˆ†å¸ƒï¼‰
- $\mathcal{L}_{\text{task}}$: ã‚¿ã‚¹ã‚¯ã®æå¤±ï¼ˆä¾‹: Cross-Entropyï¼‰
- $\lambda$: æ­£å‰‡åŒ–é …ã®é‡ã¿

**åˆ©ç‚¹**: æ¢ç´¢ãŒå‹¾é…æ³•ã§é«˜é€Ÿï¼ˆAutoAugmentã®100xé«˜é€Ÿï¼‰ã€‚

### 6.2 ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°: DVCå…¥é–€

**å•é¡Œ**: ãƒ¢ãƒ‡ãƒ«ã¯Gitã§ç®¡ç†ã§ãã‚‹ãŒã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ•°GBã€œTBï¼‰ã¯ï¼Ÿ

**è§£æ±º**: DVC (Data Version Control) [^13] â€” Gitãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€‚

#### 6.2.1 DVCã®ä»•çµ„ã¿

```bash
# Initialize DVC
dvc init

# Add dataset to DVC tracking
dvc add data/mnist_train.arrow

# Git commit the .dvc file (metadata only)
git add data/mnist_train.arrow.dvc .gitignore
git commit -m "Add MNIST training data"

# Push data to remote storage (S3/GCS/Azure/NAS)
dvc remote add -d myremote s3://mybucket/dvc-storage
dvc push
```

**ä»•çµ„ã¿**:

1. `dvc add`ã§ãƒ‡ãƒ¼ã‚¿ã®MD5ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®— â†’ `.dvc`ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
2. å®Ÿãƒ‡ãƒ¼ã‚¿ã¯`.dvc/cache/`ã«ç§»å‹•ï¼ˆGitã¯è¿½è·¡ã—ãªã„ï¼‰
3. `.dvc`ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿Gitã«ã‚³ãƒŸãƒƒãƒˆï¼ˆæ•°KBï¼‰
4. `dvc push`ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
5. ä»–ã®äººã¯`dvc pull`ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

**æ•°å¼è¡¨ç¾**:

$$
\text{DVC}(\mathcal{D}) = (\text{hash}(\mathcal{D}), \text{metadata})
$$

$$
\text{Git}(\text{DVC}(\mathcal{D})) \quad \text{(only metadata, not data)}
$$

#### 6.2.2 å®Ÿé¨“å†ç¾ã®ãŸã‚ã®DVCãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```yaml
# dvc.yaml
stages:
  preprocess:
    cmd: python preprocess.py
    deps:
      - data/raw/mnist.csv
    params:
      - preprocess.normalize
      - preprocess.augment
    outs:
      - data/processed/mnist_train.arrow

  train:
    cmd: python train.py
    deps:
      - data/processed/mnist_train.arrow
      - src/model.py
    params:
      - train.epochs
      - train.learning_rate
    metrics:
      - metrics/train.json:
          cache: false
    outs:
      - models/vae_mnist.pth
```

`dvc repro`ã§å…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å†å®Ÿè¡Œ â†’ ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰æ›´ã‚’è‡ªå‹•è¿½è·¡ã€‚

**å†ç¾æ€§ã®ä¿è¨¼**:

$$
\text{Reproducibility} = f(\text{Code}, \text{Data}, \text{Params})
$$

DVCã¯å…¨ã¦ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† â†’ éå»ã®ä»»æ„ã®æ™‚ç‚¹ã‚’å®Œå…¨å†ç¾ã€‚

### 6.3 ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A["ğŸ“Š Classical Statistics<br/>Fisher 1922: MLE"] --> B["ğŸ”¬ Robust Statistics<br/>Huber 1964: M-estimator"]
    A --> C["ğŸ“ˆ EDA<br/>Tukey 1977: Boxplot"]
    B --> D["âš–ï¸ Imbalanced Learning<br/>Chawla 2002: SMOTE"]
    D --> E["ğŸ¯ Focal Loss<br/>Lin 2017: Hard Example Mining"]
    C --> F["ğŸ¤– AutoML<br/>Feurer 2015: Auto-sklearn"]
    F --> G["ğŸ”„ AutoAugment<br/>Cubuk 2019: RL-based"]
    G --> H["âš¡ RandAugment<br/>Cubuk 2020: Simplified"]
    H --> I["ğŸ² TrivialAugment<br/>MÃ¼ller 2021: Parameter-free"]
    E --> J["ğŸ“Š Data-Centric AI<br/>Ng 2021: Quality>Model"]
    J --> K["ğŸ”® 2024-2026<br/>Differentiable Aug/Active Learning"]
    style A fill:#e8f5e9
    style K fill:#fff9c4
```

**ä¸»è¦ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**:

| å¹´ | è²¢çŒ® | è«–æ–‡/äººç‰© | å½±éŸ¿ |
|:---|:-----|:---------|:-----|
| 1922 | æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ | Fisher | çµ±è¨ˆçš„æ¨è«–ã®åŸºç›¤ |
| 1977 | æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰ | Tukey | ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã®ä½“ç³»åŒ– |
| 2002 | SMOTE | Chawla et al. | ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ã®å®šç•ª |
| 2017 | Focal Loss | Lin et al. (ICCV) | One-stageæ¤œå‡ºå™¨ã‚’å®Ÿç”¨åŒ– |
| 2019 | AutoAugment | Cubuk et al. (CVPR) | è‡ªå‹•ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®é–‹å¹• |
| 2020 | RandAugment | Cubuk et al. (NeurIPS) | æ¢ç´¢ã‚³ã‚¹ãƒˆã‚’1/1000ã«å‰Šæ¸› |
| 2021 | Data-Centric AI | Andrew Ng | ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã‚’å®£è¨€ |

### 6.4 æ¬ æå€¤å‡¦ç†ã®ç†è«–ã¨å®Ÿè£…

ãƒ‡ãƒ¼ã‚¿ã®ä¸å®Œå…¨æ€§ã¯é¿ã‘ã‚‰ã‚Œãªã„ã€‚å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç´„15-40%ã«ã¯æ¬ æå€¤ãŒå­˜åœ¨ã™ã‚‹ã€‚é©åˆ‡ãªæ¬ æå€¤å‡¦ç†ãŒæ€§èƒ½ã‚’å·¦å³ã™ã‚‹ã€‚

#### 6.4.1 æ¬ æãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åˆ†é¡ï¼ˆRubin 1976ï¼‰

æ¬ æå€¤ã¯3ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«åˆ†é¡ã•ã‚Œã‚‹ [^14]:

**1. MCAR (Missing Completely At Random)**

$$
P(R = 0 \mid X_{\text{obs}}, X_{\text{miss}}) = P(R = 0)
$$

ã“ã“ã§ $R$ ã¯æ¬ æã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ï¼ˆ0=æ¬ æã€1=è¦³æ¸¬ï¼‰ã€‚æ¬ æã¯å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ  â†’ æ¬ æãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ã‚‚åã‚Šãªã—ã€‚

**ä¾‹**: ã‚»ãƒ³ã‚µãƒ¼æ•…éšœã§ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ãŒè¨˜éŒ²ã•ã‚Œãªã„ã€‚

**2. MAR (Missing At Random)**

$$
P(R = 0 \mid X_{\text{obs}}, X_{\text{miss}}) = P(R = 0 \mid X_{\text{obs}})
$$

æ¬ æãŒè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã«ä¾å­˜ï¼ˆæ¬ æå€¤è‡ªä½“ã«ã¯ä¾å­˜ã—ãªã„ï¼‰ã€‚

**ä¾‹**: é«˜é½¢è€…ã»ã©å¥åº·è¨ºæ–­ã®ã€Œä½“é‡ã€é …ç›®ã‚’è¨˜å…¥ã—ãªã„ â†’ å¹´é½¢ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼‰ã‹ã‚‰æ¬ æã‚’äºˆæ¸¬å¯èƒ½ã€‚

**3. MNAR (Missing Not At Random)**

$$
P(R = 0 \mid X_{\text{obs}}, X_{\text{miss}}) \neq P(R = 0 \mid X_{\text{obs}})
$$

æ¬ æãŒæ¬ æå€¤è‡ªä½“ã«ä¾å­˜ â†’ è£œå®ŒãŒå›°é›£ã€‚

**ä¾‹**: é«˜æ‰€å¾—è€…ã»ã©ã€Œå¹´åã€ã‚’è¨˜å…¥ã—ãªã„ â†’ æ¬ æå€¤ï¼ˆé«˜æ‰€å¾—ï¼‰ãã®ã‚‚ã®ãŒæ¬ æã‚’å¼•ãèµ·ã“ã™ã€‚

#### 6.4.2 æ¬ æå€¤è£œå®Œæ‰‹æ³•

| æ‰‹æ³• | æˆ¦ç•¥ | ä»®å®š | é©ç”¨å ´é¢ |
|:-----|:-----|:-----|:--------|
| **Listwise Deletion** | æ¬ æã‚’å«ã‚€è¡Œã‚’å‰Šé™¤ | MCAR | ãƒ‡ãƒ¼ã‚¿é‡ãŒååˆ†ï¼ˆ>10% redundancyï¼‰ |
| **Mean Imputation** | å¹³å‡å€¤ã§è£œå®Œ | MCAR | æ¬ æç‡<5%ã€åˆ†æ•£ãŒé‡è¦ã§ãªã„ |
| **KNN Imputation** | k-æœ€è¿‘å‚ã®å¹³å‡ | MAR | ç‰¹å¾´é‡é–“ã«ç›¸é–¢ |
| **MICE** | å¤šé‡ä»£å…¥ | MAR | çµ±è¨ˆçš„æ¨è«–ï¼ˆä¿¡é ¼åŒºé–“æ¨å®šï¼‰ |
| **MissForest** | Random Forestè£œå®Œ | MAR | éç·šå½¢é–¢ä¿‚ã€é«˜æ¬¡å…ƒ |
| **Deep Learning** | Autoencoderè£œå®Œ | MAR/MNAR | å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€è¤‡é›‘ãªä¾å­˜é–¢ä¿‚ |

#### 6.4.3 K-NN Imputationå®Ÿè£…

```rust
use kiddo::{KdTree, SquaredEuclidean};
use ndarray::{Array2, Axis};

/// K-NNè£œå®Œ: NaNå€¤ã‚’ k æœ€è¿‘å‚ã®å¹³å‡ã§è£œå®Œ
fn knn_impute(x: &mut Array2<f64>, k: usize) {
    let (n, d) = x.dim();

    for j in 0..d {
        // æ¬ æã‚¨ãƒ³ãƒˆãƒªã‚’æ¤œç´¢
        let missing_idx: Vec<usize> = (0..n).filter(|&i| x[[i, j]].is_nan()).collect();
        if missing_idx.is_empty() {
            continue; // ã“ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ã«ã¯æ¬ æãªã—
        }

        // ãƒ•ã‚£ãƒ¼ãƒãƒ£jã®è¦³æ¸¬å€¤ãŒã‚ã‚‹è¡Œ
        let observed_idx: Vec<usize> = (0..n).filter(|&i| !x[[i, j]].is_nan()).collect();

        // ãƒ•ã‚£ãƒ¼ãƒãƒ£jã‚’é™¤ãç‰¹å¾´é‡ã§k-NNæœ¨ã‚’æ§‹ç¯‰
        let other_features: Vec<usize> = (0..d).filter(|&f| f != j).collect();
        let obs_valid: Vec<usize> = observed_idx.iter().copied()
            .filter(|&i| other_features.iter().all(|&f| !x[[i, f]].is_nan()))
            .collect();

        if obs_valid.is_empty() {
            // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¹³å‡è£œå®Œ
            let mean_val = observed_idx.iter()
                .map(|&i| x[[i, j]])
                .sum::<f64>() / observed_idx.len() as f64;
            for &i in &missing_idx {
                x[[i, j]] = mean_val;
            }
            continue;
        }

        // k-NNæœ¨ã‚’æ§‹ç¯‰ (ç‰¹å¾´æ¬¡å…ƒæ•°ã«åˆã‚ã›ã¦å®šæ•°ã‚’èª¿æ•´)
        let mut tree: KdTree<f64, usize, 4, 32, u16> = KdTree::new();
        for &row in &obs_valid {
            let point: Vec<f64> = other_features.iter().map(|&f| x[[row, f]]).collect();
            let arr: [f64; 4] = point.as_slice().try_into().unwrap_or([0.0; 4]);
            tree.add(&arr, row);
        }

        // æ¬ æå€¤ã‚’è£œå®Œ
        for &i in &missing_idx {
            let query: Vec<f64> = other_features.iter().map(|&f| x[[i, f]]).collect();
            if query.iter().any(|v| v.is_nan()) {
                // ã‚¯ã‚¨ãƒªã« NaN ãŒã‚ã‚‹å ´åˆã¯å¹³å‡è£œå®Œ
                let mean_val = observed_idx.iter()
                    .map(|&r| x[[r, j]])
                    .sum::<f64>() / observed_idx.len() as f64;
                x[[i, j]] = mean_val;
                continue;
            }
            let arr: [f64; 4] = query.as_slice().try_into().unwrap_or([0.0; 4]);
            // k æœ€è¿‘å‚ã‚’æ¤œç´¢
            let neighbors = tree.nearest_n::<SquaredEuclidean>(&arr, k.min(obs_valid.len()));
            // è¿‘å‚ã®å¹³å‡ã§è£œå®Œ
            let imputed = neighbors.iter().map(|nb| x[[nb.item, j]]).sum::<f64>()
                / neighbors.len() as f64;
            x[[i, j]] = imputed;
        }
    }
}

// Example
use rand_distr::{Normal, Distribution};
let normal = Normal::new(0.0_f64, 1.0).unwrap();
let mut rng = rand::thread_rng();
let mut x_data: Array2<f64> = Array2::from_shape_fn((100, 5), |_| normal.sample(&mut rng));

// 10%ã®æ¬ æå€¤ã‚’å°å…¥ (MCAR)
let total = x_data.len();
let n_missing = (total as f64 * 0.1).round() as usize;
let mut indices: Vec<(usize, usize)> = (0..100).flat_map(|i| (0..5).map(move |j| (i, j))).collect();
indices.sort_by_key(|_| rand::random::<u64>());
for &(i, j) in indices.iter().take(n_missing) {
    x_data[[i, j]] = f64::NAN;
}

let n_before = x_data.iter().filter(|v| v.is_nan()).count();
println!("Missing values: {} / {}", n_before, total);
knn_impute(&mut x_data, 5);
let n_after = x_data.iter().filter(|v| v.is_nan()).count();
println!("After imputation: {} / {}", n_after, total);
```

å‡ºåŠ›:
```
Missing values: 50 / 500
After imputation: 0 / 500
```

#### 6.4.4 MICEï¼ˆMultiple Imputation by Chained Equationsï¼‰

MICEã¯å„ç‰¹å¾´é‡ã‚’ä»–ã®ç‰¹å¾´é‡ã§äºˆæ¸¬ã—ã€åå¾©çš„ã«è£œå®Œã™ã‚‹ [^15]ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. åˆæœŸåŒ–: å¹³å‡å€¤è£œå®Œ
2. å„ç‰¹å¾´é‡ $j$ ã«ã¤ã„ã¦:
   a. $j$ ä»¥å¤–ã®ç‰¹å¾´é‡ã‚’èª¬æ˜å¤‰æ•°ã€$j$ ã‚’ç›®çš„å¤‰æ•°ã¨ã—ã¦å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
   b. æ¬ æå€¤ã‚’äºˆæ¸¬å€¤ã§è£œå®Œ
3. åæŸã¾ã§ç¹°ã‚Šè¿”ã™ï¼ˆé€šå¸¸5-10å›ï¼‰
4. **è¤‡æ•°å›å®Ÿè¡Œ** â†’ è¤‡æ•°ã®è£œå®Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ â†’ çµ±è¨ˆé‡ã®ä¸ç¢ºå®Ÿæ€§ã‚’æ¨å®š

**æ•°å¼**:

$$
X_j^{(t+1)} = f_j(X_{-j}^{(t)}, \theta_j) + \epsilon
$$

ã“ã“ã§ $X_{-j}$ ã¯ $j$ ä»¥å¤–ã®å…¨ç‰¹å¾´é‡ã€$f_j$ ã¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã€$\epsilon$ ã¯ãƒã‚¤ã‚ºã€‚

**Rustå®Ÿè£…ï¼ˆç°¡æ˜“ç‰ˆï¼‰**:

```rust
use ndarray::{Array2, Axis};

/// MICEï¼ˆMultiple Imputation by Chained Equationsï¼‰ç°¡æ˜“å®Ÿè£…
/// ç·šå½¢å›å¸°ã«ã‚ˆã‚‹åå¾©çš„æ¬ æå€¤è£œå®Œ
fn mice_impute(x: &Array2<f64>, n_iter: usize, m: usize) -> Array2<f64> {
    let (n, d) = x.dim();
    let mut imputed_datasets: Vec<Array2<f64>> = Vec::with_capacity(m);

    for _ in 0..m {
        // m å€‹ã®è£œå®Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ
        let mut x_imp = x.clone();

        // åˆæœŸåŒ–: å¹³å‡è£œå®Œ
        for j in 0..d {
            let col: Vec<f64> = x_imp.column(j).iter()
                .filter(|v| !v.is_nan()).copied().collect();
            if col.is_empty() { continue; }
            let mean_val = col.iter().sum::<f64>() / col.len() as f64;
            for i in 0..n {
                if x_imp[[i, j]].is_nan() {
                    x_imp[[i, j]] = mean_val;
                }
            }
        }

        // åå¾©è£œå®Œ
        for _iter in 0..n_iter {
            for j in 0..d {
                let missing_mask_j: Vec<bool> = (0..n).map(|i| x[[i, j]].is_nan()).collect();
                if !missing_mask_j.iter().any(|&m| m) {
                    continue;
                }

                let obs_idx: Vec<usize> = (0..n).filter(|&i| !missing_mask_j[i]).collect();
                let miss_idx: Vec<usize> = (0..n).filter(|&i| missing_mask_j[i]).collect();

                // ç·šå½¢å›å¸°: X_j ~ X_{-j} (æœ€å°äºŒä¹—æ³•)
                let other: Vec<usize> = (0..d).filter(|&f| f != j).collect();
                // è¦³æ¸¬è¡Œã§è¨ˆç”»è¡Œåˆ—ã‚’çµ„ã¿ç«‹ã¦ã‚‹
                let x_obs_j: Vec<f64> = obs_idx.iter().map(|&i| x_imp[[i, j]]).collect();
                let x_pred: Vec<Vec<f64>> = obs_idx.iter()
                    .map(|&i| other.iter().map(|&f| x_imp[[i, f]]).collect())
                    .collect();

                // æœ€å°äºŒä¹—æ¨å®š (ç°¡æ˜“: æ“¬ä¼¼é€†è¡Œåˆ—)
                // äºˆæ¸¬å€¤ = è¦³æ¸¬å€¤ã®åˆ—ã”ã¨ã®å¹³å‡ (æœ€ç°¡æ˜“ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
                let pred_mean = x_obs_j.iter().sum::<f64>() / x_obs_j.len() as f64;
                for &i in &miss_idx {
                    x_imp[[i, j]] = pred_mean; // ç°¡æ˜“: ç·šå½¢å›å¸°ä¿‚æ•°ã§ç²¾åº¦å‘ä¸Šå¯èƒ½
                }
            }
        }

        imputed_datasets.push(x_imp);
    }

    // m å€‹ã®è£œå®Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¹³å‡ã‚’è¿”ã™
    let mut result = Array2::<f64>::zeros((n, d));
    for dataset in &imputed_datasets {
        result = result + dataset;
    }
    result / m as f64
}

// Example
let x_mice = mice_impute(&x_data, 10, 5);
println!("MICE imputation completed");
```

#### 6.4.5 æ¬ æå€¤ã®å¯è¦–åŒ–

```rust
use ndarray::Array2;

/// æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«ASCIIã§å¯è¦–åŒ–
fn print_missing_pattern(x: &Array2<f64>) {
    let (n, d) = x.dim();
    println!("Missing Data Pattern (row=sample, col=feature, 'â–ˆ'=missing):");
    println!("Features: {}", (0..d).map(|j| format!("{:3}", j)).collect::<Vec<_>>().join(""));
    // ã‚µãƒ³ãƒ—ãƒ«ãŒå¤šã„å ´åˆã¯æœ€åˆã®20è¡Œã®ã¿è¡¨ç¤º
    for i in 0..n.min(20) {
        let row: String = (0..d).map(|j| {
            if x[[i, j]].is_nan() { " â–ˆ " } else { " Â· " }
        }).collect();
        println!("{:3}:{}", i, row);
    }
    let total_missing = x.iter().filter(|v| v.is_nan()).count();
    let total = x.len();
    println!("Total missing: {} / {} ({:.1}%)", total_missing, total,
        100.0 * total_missing as f64 / total as f64);
}

// Visualize
print_missing_pattern(&x_data);
```

æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–ã§ã€MCAR/MAR/MNARã‚’è¨ºæ–­ã§ãã‚‹:

- **ãƒ©ãƒ³ãƒ€ãƒ ãªç‚¹åœ¨**: MCAR
- **ç‰¹å®šã®è¡Œ/åˆ—ã«é›†ä¸­**: MARï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã«ä¾å­˜ï¼‰
- **æ§‹é€ çš„ãƒ‘ã‚¿ãƒ¼ãƒ³**: MNARï¼ˆæ¬ æå€¤è‡ªä½“ã«ä¾å­˜ï¼‰

### 6.5 æœ€æ–°ç ”ç©¶å‹•å‘ï¼ˆ2024-2026ï¼‰

#### 6.5.1 LLMã«ã‚ˆã‚‹è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

GPT-4/Claudeã‚’ä½¿ã£ãŸè‡ªå‹•ãƒ©ãƒ™ãƒªãƒ³ã‚°ãŒå®Ÿç”¨åŒ–ã€‚

```rust
use reqwest::blocking::Client;
use serde_json::{json, Value};

/// LLM APIã‚’å‘¼ã³å‡ºã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
fn annotate_with_llm(
    text: &str,
    classes: &[&str],
    api_key: &str,
) -> anyhow::Result<String> {
    let prompt = format!(
        "Classify the following text into one of {}:\n\nText: {}\n\nAnswer with only the class name.",
        classes.join(", "),
        text
    );

    let client = Client::new();
    let body = json!({
        "model": "gpt-4",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0
    });

    let resp: Value = client
        .post("https://api.openai.com/v1/chat/completions")
        .bearer_auth(api_key)
        .json(&body)
        .send()?
        .json()?;

    let label = resp["choices"][0]["message"]["content"]
        .as_str()
        .unwrap_or("")
        .trim()
        .to_string();
    Ok(label)
}

// ä½¿ç”¨ä¾‹
let api_key = std::env::var("OPENAI_API_KEY").unwrap_or_default();
let label = annotate_with_llm(
    "I love this product!",
    &["positive", "negative", "neutral"],
    &api_key,
)?;
// => "positive"
```

**ç²¾åº¦**: Human baseline 95% â†’ GPT-4 93% (Stanfordç ”ç©¶)ã€‚ã‚³ã‚¹ãƒˆã¯äººé–“ã®1/100ã€‚

#### 6.4.2 Synthetic Data Generation: ãƒ‡ãƒ¼ã‚¿ãŒç„¡é™ã«ç”Ÿæˆã§ãã‚‹æ™‚ä»£

**Flamingo/Stable Diffusion**ã«ã‚ˆã‚‹ç”»åƒç”Ÿæˆ + LLMã«ã‚ˆã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ â†’ **åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**ã€‚

ä¾‹: **Synthetic ImageNet** â€” Stable Diffusionã§1Mç”»åƒç”Ÿæˆ â†’ ResNet-50è¨“ç·´ â†’ å®Ÿãƒ‡ãƒ¼ã‚¿ã®95%ç²¾åº¦é”æˆã€‚

**æ•°å¼**:

$$
\mathcal{D}_{\text{synthetic}} = \{(\text{GenerateImage}(\text{prompt}_i), \text{label}_i)\}_{i=1}^N
$$

**åˆ©ç‚¹**:

- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å•é¡Œå›é¿ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ä¸è¦ï¼‰
- ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«åˆ†å¸ƒã®è£œå®Œï¼ˆç¨€ãªã‚¯ãƒ©ã‚¹ã‚’å¤§é‡ç”Ÿæˆï¼‰
- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®æ¥µé™å½¢æ…‹

### 6.6 ä»Šå›ã®å­¦ã³ï¼ˆ3ã¤ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼‰

1. **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãŒè¨“ç·´ã‚’æ¡é•ã„ã«åŠ é€Ÿã™ã‚‹**

æ¨™æº–åŒ– $z = \frac{x - \mu}{\sigma}$ ã ã‘ã§ã€å­¦ç¿’ç‡ã‚’10000å€ã«ã§ãã€åæŸãŒåŠ‡çš„ã«é€Ÿããªã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„æ–¹ã‚’çŸ¥ã‚‰ãªã‘ã‚Œã°ã€ã©ã‚Œã ã‘å„ªã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚‚ç„¡æ„å‘³ã ã€‚

2. **ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã¯çµ±åˆæˆ¦ç•¥ã§å¯¾å‡¦ã™ã‚‹**

SMOTEï¼ˆã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼‰+ Focal Lossï¼ˆé›£ã—ã„ä¾‹ã«é›†ä¸­ï¼‰+ Class Weightingï¼ˆæå¤±ã®é‡ã¿ä»˜ã‘ï¼‰ã®çµ„ã¿åˆã‚ã›ã§ã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®Recallã‚’15.6å€æ”¹å–„ã§ããŸã€‚å˜ä¸€æ‰‹æ³•ã§ã¯ä¸ååˆ†ã€çµ±åˆãŒéµã ã€‚

3. **HuggingFace Datasets + Rust Arrow = ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼å‡¦ç†**

Pythonï¼ˆHF Datasetsï¼‰ã¨Rustï¼ˆarrow-rsï¼‰ã®é€£æºã§ã€æ•°GBç´šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’RAMã‚³ãƒ”ãƒ¼ãªã—ã§å‡¦ç†ã§ãã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŠ¹ç‡åŒ–ã¯ã€ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨åŒã˜ãã‚‰ã„é‡è¦ã ã€‚


## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.7 FAQ

<details><summary>Q1. æ¨™æº–åŒ–ã¨BatchNormã¯ä½•ãŒé•ã†ï¼Ÿ</summary>

**æ¨™æº–åŒ–**: ãƒ‡ãƒ¼ã‚¿å…¨ä½“ï¼ˆè¨“ç·´ã‚»ãƒƒãƒˆï¼‰ã®çµ±è¨ˆé‡ $\mu, \sigma$ ã§ä¸€åº¦å¤‰æ› â†’ è¨“ç·´å‰ã®å‰å‡¦ç†ã€‚

**BatchNorm**: ãƒŸãƒ‹ãƒãƒƒãƒã”ã¨ã«çµ±è¨ˆé‡ã‚’è¨ˆç®— â†’ å„å±¤ã§å‹•çš„ã«æ­£è¦åŒ– â†’ è¨“ç·´ä¸­ã®å†…éƒ¨å‡¦ç†ã€‚

$$
\begin{aligned}
\text{Standardization:} \quad & z = \frac{x - \mu_{\text{å…¨ãƒ‡ãƒ¼ã‚¿}}}{\sigma_{\text{å…¨ãƒ‡ãƒ¼ã‚¿}}} \\
\text{BatchNorm:} \quad & z = \frac{x - \mu_{\text{ãƒãƒƒãƒ}}}{\sigma_{\text{ãƒãƒƒãƒ}}}
\end{aligned}
$$

ä¸¡æ–¹ä½¿ã†ã®ãŒä¸€èˆ¬çš„ï¼ˆå‰å‡¦ç†ã§æ¨™æº–åŒ– + å„å±¤ã§BatchNormï¼‰ã€‚

</details>

<details><summary>Q2. SMOTEã¯é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§ã‚‚æœ‰åŠ¹ï¼Ÿ</summary>

**æ³¨æ„**: é«˜æ¬¡å…ƒï¼ˆ>100æ¬¡å…ƒï¼‰ã§ã¯SMOTEã®åŠ¹æœãŒè–„ã‚Œã‚‹ã€‚ç†ç”±:

- ç·šå½¢è£œé–“ $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$ ã¯ã€é«˜æ¬¡å…ƒç©ºé–“ã§ã¯ã€Œæ„å‘³ã®ã‚ã‚‹ã€ä¸­é–“ç‚¹ã‚’ç”Ÿæˆã—ã«ãã„ï¼ˆæ¬¡å…ƒã®å‘ªã„ï¼‰
- k-æœ€è¿‘å‚ãŒé ããªã‚Šã€è£œé–“ãŒç„¡æ„å‘³ã«ãªã‚‹

**å¯¾ç­–**:

- **Borderline-SMOTE**: æ±ºå®šå¢ƒç•Œä»˜è¿‘ã®ã¿ç”Ÿæˆ
- **ADASYN**: å¯†åº¦ã«å¿œã˜ã¦ç”Ÿæˆæ•°èª¿æ•´
- **Deep SMOTE**: Autoencoderã‚„VAEã®æ½œåœ¨ç©ºé–“ã§è£œé–“ï¼ˆä½æ¬¡å…ƒåŒ–å¾Œã« SMOTEï¼‰

</details>

<details><summary>Q3. Focal Lossã®$\gamma$ã¯ã©ã†é¸ã¶ï¼Ÿ</summary>

**æ¨å¥¨å€¤**: $\gamma = 2.0$ï¼ˆLin et al. 2017åŸè«–æ–‡ [^6]ï¼‰

**èª¿æ•´æ–¹é‡**:

- $\gamma = 0$: æ¨™æº–ã®Cross-Entropyï¼ˆç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚‚ç­‰ã—ãæ‰±ã†ï¼‰
- $\gamma = 1$: è»½åº¦ã®ç°¡å˜ã‚µãƒ³ãƒ—ãƒ«å‰Šæ¸›ï¼ˆä¸å‡è¡¡æ¯” < 10:1ï¼‰
- $\gamma = 2$: æ¨™æº–ï¼ˆä¸å‡è¡¡æ¯” 10-100:1ï¼‰
- $\gamma = 5$: æ¥µç«¯ãªä¸å‡è¡¡ï¼ˆ100:1ä»¥ä¸Šï¼‰

**å®Ÿé¨“**:

```rust
for &gamma in &[0.0_f64, 1.0, 2.0, 5.0] {
    let focal = FocalLoss { alpha: alpha_demo.clone(), gamma };
    // Train and evaluate
    let loss = focal.forward(p_pred_demo.view(), &y_demo);
    println!("Î³={}: approx_loss={:.4}", gamma, loss);
}
```

ä¸€èˆ¬ã« $\gamma \in [2, 3]$ ãŒæœ€ã‚‚å®‰å®šã™ã‚‹ã€‚

</details>

<details><summary>Q4. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã‚‚é©ç”¨ã™ã‚‹ï¼Ÿ</summary>

**çµ¶å¯¾ã«NO**ã€‚ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¯**è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿**ã«é©ç”¨ã™ã‚‹ã€‚

ç†ç”±:

- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯ã€ŒæœªçŸ¥ãƒ‡ãƒ¼ã‚¿ã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ â†’ æ‹¡å¼µã™ã‚‹ã¨éåº¦ã«æœ‰åˆ©ãªè©•ä¾¡ã«ãªã‚‹
- æ±åŒ–æ€§èƒ½ã‚’æ­£ã—ãæ¸¬å®šã§ããªããªã‚‹

**Test Time Augmentation (TTA)**ã®ä¾‹å¤–:

æ¨è«–æ™‚ã«è¤‡æ•°ã®æ‹¡å¼µç‰ˆã§äºˆæ¸¬ã—ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã™ã‚‹æ‰‹æ³•ã¯å­˜åœ¨ã™ã‚‹ãŒã€ã“ã‚Œã¯è©•ä¾¡ã§ã¯ãªãæ¨è«–ã®æŠ€è¡“ã€‚

$$
\hat{y} = \frac{1}{K}\sum_{k=1}^K f(\text{aug}_k(\mathbf{x}))
$$

è©•ä¾¡æ™‚ã¯TTA**ãªã—**ã§è¨ˆæ¸¬ã™ã‚‹ã€‚

</details>

<details><summary>Q5. DVCã¨Git LFSã®é•ã„ã¯ï¼Ÿ</summary>

| è¦³ç‚¹ | DVC | Git LFS |
|:-----|:----|:--------|
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** | ä»»æ„ï¼ˆS3/GCS/Azure/NASï¼‰ | GitHub LFSå°‚ç”¨ã‚µãƒ¼ãƒãƒ¼ |
| **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** | âœ… dvc.yaml ã§å®šç¾©å¯èƒ½ | âŒ ãªã— |
| **å†ç¾æ€§** | âœ… ã‚³ãƒ¼ãƒ‰+ãƒ‡ãƒ¼ã‚¿+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ±åˆ | âš ï¸ ãƒ‡ãƒ¼ã‚¿ã®ã¿ |
| **ã‚³ã‚¹ãƒˆ** | è‡ªå‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆå®‰ä¾¡ï¼‰ | GitHubèª²é‡‘ï¼ˆé«˜é¡ï¼‰ |
| **å­¦ç¿’æ›²ç·š** | ã‚„ã‚„æ€¥ | ç·©ã‚„ã‹ï¼ˆGitæ‹¡å¼µï¼‰ |

**æ¨å¥¨**: æœ¬æ ¼çš„ãªMLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ â†’ DVCã€‚å°è¦æ¨¡/å€‹äºº â†’ Git LFSã€‚

</details>

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | é‡è¦åº¦ |
|:---|:-----|:-----|:-------|
| 1æ—¥ç›® | Zone 0-2ï¼ˆã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã€œç›´æ„Ÿï¼‰ | 30åˆ† | â˜…â˜…â˜… |
| 2æ—¥ç›® | Zone 3ï¼ˆæ•°å¼ä¿®è¡Œ å‰åŠ: æ¨™æº–åŒ–ãƒ»One-Hotãƒ»Class Weightingï¼‰ | 60åˆ† | â˜…â˜…â˜… |
| 3æ—¥ç›® | Zone 3ï¼ˆæ•°å¼ä¿®è¡Œ å¾ŒåŠ: Focal Lossãƒ»SMOTEãƒ»Boss Battleï¼‰ | 60åˆ† | â˜…â˜…â˜… |
| 4æ—¥ç›® | Zone 4ï¼ˆå®Ÿè£…: HF Datasetsãƒ»Rustçµ±åˆãƒ»å‰å‡¦ç†å®Ÿè£…ï¼‰ | 90åˆ† | â˜…â˜…â˜… |
| 5æ—¥ç›® | Zone 5ï¼ˆå®Ÿé¨“: ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ€§èƒ½æ¤œè¨¼ï¼‰ | 60åˆ† | â˜…â˜…â˜… |
| 6æ—¥ç›® | Zone 6ï¼ˆç™ºå±•: æœ€æ–°ç ”ç©¶ãƒ»DVCï¼‰+ å¾©ç¿’ | 60åˆ† | â˜…â˜… |
| 7æ—¥ç›® | ç·å¾©ç¿’ + è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ + å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ | 90åˆ† | â˜…â˜…â˜… |

**é‡ç‚¹å¾©ç¿’ãƒã‚¤ãƒ³ãƒˆ**:

- [ ] æ¨™æº–åŒ–ã®æ•°å¼ $z = \frac{x - \mu}{\sigma}$ ã‚’æš—è¨˜ã—ã€Rustå®Ÿè£…ã‚’å†ç¾ã§ãã‚‹
- [ ] Focal Loss $\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)$ ã®ç›´æ„Ÿã‚’èª¬æ˜ã§ãã‚‹
- [ ] SMOTEè£œé–“ $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$ ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] HuggingFace Datasets â†’ Rust Arrowã® ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼çµ±åˆã‚’å®Ÿè£…ã§ãã‚‹
- [ ] ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€Baseline vs Combined ã®æ€§èƒ½å·®ã‚’å®Ÿé¨“ã§ç¤ºã›ã‚‹

### 6.9 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ:

- [ ] æ¨™æº–åŒ– $z = \frac{x-\mu}{\sigma}$ ã‚’Rustã§å®Ÿè£…ã—ã€å¹³å‡0ãƒ»åˆ†æ•£1ã‚’ç¢ºèªã§ãã‚‹
- [ ] Focal Loss $\text{FL}(p_t)=-(1-p_t)^\gamma\log(p_t)$ ã®å‹¾é…ã‚’å°å‡ºã§ãã‚‹
- [ ] SMOTEã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã—ã€åˆæˆã‚µãƒ³ãƒ—ãƒ«ãŒç·šåˆ†ä¸Šã«ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã§ãã‚‹
- [ ] HF Datasets â†’ arrow-rs ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼è»¢é€ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã§Baseline vs Combinedã®æ€§èƒ½å·®ã‚’å®Ÿé¨“ã§ç¤ºã›ã‚‹

### 6.10 æ¬¡å›äºˆå‘Š: ç¬¬22å›ã€Œãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆã€

ç¬¬20å›ã§VAE/GAN/Transformerã‚’å®Ÿè£…ã—ã€ç¬¬21å›ã§ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å­¦ã‚“ã ã€‚æ¬¡ã¯**ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆ**ã™ã‚‹ã€‚

**æ¬¡å›ã®å†…å®¹**:

- Vision-Languageãƒ¢ãƒ‡ãƒ«ã®ç†è«–ï¼ˆCLIP/BLIP-2/LLaVA/Qwen-VLï¼‰
- Cross-Modal Attentionã®æ•°å­¦ï¼ˆ$\text{Attention}(Q_{\text{text}}, K_{\text{image}}, V_{\text{image}})$ï¼‰
- Contrastive Learningå®Œå…¨ç‰ˆï¼ˆInfoNCE losså°å‡ºï¼‰
- ğŸ¦€ CLIP Rustè¨“ç·´å®Ÿè£…
- ğŸ¦€ SmolVLM2 Rustæ¨è«–å®Ÿè£…
- VQAãƒ»Image Captioningè©•ä¾¡

**æ¥ç¶š**:

- ç¬¬21å›: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®åŸºç›¤ã‚’å›ºã‚ãŸ
- **ç¬¬22å›**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’æ‰±ã†
- ç¬¬23å›: Fine-tuningï¼ˆäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é©å¿œï¼‰
- ç¬¬24å›: çµ±è¨ˆå­¦ãƒ»å› æœæ¨è«–ï¼ˆå®Ÿé¨“è¨­è¨ˆã®ç§‘å­¦ï¼‰

```mermaid
graph LR
    A["ğŸ“Š ç¬¬21å›<br/>Data Science"] --> B["ğŸ–¼ï¸ ç¬¬22å›<br/>Multimodal"]
    B --> C["ğŸ¯ ç¬¬23å›<br/>Fine-tuning"]
    C --> D["ğŸ“ˆ ç¬¬24å›<br/>Statistics"]
    style A fill:#e8f5e9
    style B fill:#fff9c4
```

**æº–å‚™ã™ã¹ãã“ã¨**:

- HuggingFace Datasetsã§ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆCOCO Captions / VQAv2ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰
- ç”»åƒåŸ‹ã‚è¾¼ã¿ï¼ˆViTï¼‰ã¨ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆBERTï¼‰ã®æ¬¡å…ƒã‚’æƒãˆã‚‹æ–¹æ³•ã‚’è€ƒãˆã‚‹
- Contrastive Lossï¼ˆå¯¾ç…§å­¦ç¿’ï¼‰ã®ç›´æ„Ÿã‚’æ´ã‚€ï¼ˆä¼¼ãŸç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã®è·é›¢ã‚’è¿‘ã¥ã‘ã‚‹ï¼‰

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œãƒ‡ãƒ¼ã‚¿ãªãã—ã¦å­¦ç¿’ãªã—ã€ã¯å½“ãŸã‚Šå‰ã ã€‚ã ãŒæœ¬å½“ã®å•ã„ã¯ â€” ãƒ‡ãƒ¼ã‚¿ã®**è³ª**ã‚’ã©ã†å®šç¾©ã™ã‚‹ã‹ï¼Ÿ**

å¾“æ¥ã®æ©Ÿæ¢°å­¦ç¿’ã¯ã€Œãƒ‡ãƒ¼ã‚¿ã¯ä¸ãˆã‚‰ã‚Œã‚‹ã‚‚ã®ã€ã¨ã—ã¦æ‰±ã£ã¦ããŸã€‚ã ãŒ2025å¹´ã€åˆæˆãƒ‡ãƒ¼ã‚¿ãƒ»LLMã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»Active Learningã®æ™‚ä»£ã«ã€ãƒ‡ãƒ¼ã‚¿ã¯ã€Œä½œã‚‹ã‚‚ã®ã€ã«ãªã£ãŸã€‚

**å•ã„**:

1. **åˆæˆãƒ‡ãƒ¼ã‚¿ã¯ã€Œæœ¬ç‰©ã€ã‹ï¼Ÿ** Stable Diffusionã§ç”Ÿæˆã—ãŸç”»åƒã§ImageNetç²¾åº¦95%é”æˆã§ãã‚‹ãªã‚‰ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦ãªã®ã‹ï¼Ÿ
2. **LLMã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯äººé–“ã‚’è¶…ãˆã‚‹ã‹ï¼Ÿ** GPT-4ã®ç²¾åº¦ãŒäººé–“ã®93% vs 95%ãªã‚‰ã€ã‚³ã‚¹ãƒˆ1/100ã§ååˆ†ã§ã¯ï¼Ÿ
3. **ãƒ‡ãƒ¼ã‚¿å“è³ªã®é™ç•Œåç›Šã¯ï¼Ÿ** ãƒ‡ãƒ¼ã‚¿ã‚’ã©ã“ã¾ã§ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚Œã°ã€Œååˆ†ã€ã‹ï¼Ÿéå‰°ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¯éå­¦ç¿’ã‚’æ‹›ãã®ã§ã¯ï¼Ÿ

**è­°è«–ã®è¦–ç‚¹**:

- **åˆæˆãƒ‡ãƒ¼ã‚¿ã®å±é™ºæ€§**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚¢ã‚¹ãŒãƒ‡ãƒ¼ã‚¿ã«æ··å…¥ â†’ ãƒ¢ãƒ‡ãƒ«ãŒç¾å®Ÿã‚’åæ˜ ã—ãªããªã‚‹
- **Human-in-the-loop**: å®Œå…¨è‡ªå‹•åŒ–ã¯ä¸å¯èƒ½ã€äººé–“ã®åˆ¤æ–­ãŒæœ€çµ‚é˜²è¡›ç·š
- **åˆ†å¸ƒã‚·ãƒ•ãƒˆ**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆåˆ†å¸ƒãŒä¹–é›¢ã™ã‚‹å•é¡Œã¯ã€ã©ã‚“ãªã«å“è³ªã‚’ä¸Šã’ã¦ã‚‚è§£æ±ºã—ãªã„

**æ­´å²çš„æ–‡è„ˆ**:

- **1950å¹´ä»£**: ãƒ‡ãƒ¼ã‚¿ã¯æ‰‹ä½œæ¥­ã§åé›†ï¼ˆæ•°ç™¾ã‚µãƒ³ãƒ—ãƒ«ï¼‰
- **1990å¹´ä»£**: ImageNetç™»å ´ï¼ˆ1400ä¸‡æšã®æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
- **2020å¹´ä»£**: LAION-5Bï¼ˆ50å„„æšã®è‡ªå‹•åé›†ï¼‰
- **2025å¹´**: åˆæˆãƒ‡ãƒ¼ã‚¿ãŒä¸»æµã«ãªã‚‹ï¼Ÿ

<details><summary>æ­´å²çš„è¦³ç‚¹: ãƒ‡ãƒ¼ã‚¿åé›†ã®é€²åŒ–</summary>

| æ™‚ä»£ | ãƒ‡ãƒ¼ã‚¿è¦æ¨¡ | åé›†æ–¹æ³• | ã‚³ã‚¹ãƒˆ | å“è³ª |
|:-----|:----------|:--------|:-------|:-----|
| 1950-1980 | æ•°ç™¾ã€œæ•°åƒ | æ‰‹å‹•å…¥åŠ› | æ¥µé«˜ | æ¥µé«˜ |
| 1980-2000 | æ•°ä¸‡ã€œæ•°åä¸‡ | æ‰‹å‹•+ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° | é«˜ | é«˜ |
| 2000-2020 | æ•°ç™¾ä¸‡ã€œæ•°åå„„ | ã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚·ãƒ³ã‚° | ä¸­ | ä¸­ |
| 2020-2025 | æ•°åå„„ã€œæ•°å…† | è‡ªå‹•åé›†+LLMãƒ•ã‚£ãƒ«ã‚¿ | ä½ | ä¸­ã€œä½ |
| **2025-** | **ç„¡é™ï¼ˆåˆæˆï¼‰** | **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«** | **æ¥µä½** | **ï¼Ÿ** |

åˆæˆãƒ‡ãƒ¼ã‚¿ã®ã€Œå“è³ªã€ã‚’ã©ã†å®šç¾©ã™ã‚‹ã‹ãŒã€æ¬¡ä¸–ä»£AIã®éµã ã€‚

</details>

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ ç¬¬21å›ã€Œãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ & HuggingFace Datasetsã€å®Œèµ°ï¼ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å…¨ã‚µã‚¤ã‚¯ãƒ«ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚æ¬¡å›ã¯ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆã¸ã€‚

> **Progress: 95%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. RandAugment ãŒå¾“æ¥ã®æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ç‚¹ã‚’ã€æ¢ç´¢ç©ºé–“ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚
> 2. DVCã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã™ã‚‹ã¨ãã€Gitæœ¬ä½“ã«ã¯ä½•ãŒä¿å­˜ã•ã‚Œã€å®Ÿãƒ‡ãƒ¼ã‚¿ã¯ã©ã“ã«ç½®ã‹ã‚Œã‚‹ã‹ï¼Ÿ

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Lhoest, Q., et al. (2021). "Datasets: A Community Library for Natural Language Processing". *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, 175-184.
<https://github.com/huggingface/datasets>

[^2]: Apache Arrow Development Team. (2024). "Apache Arrow: A Cross-Language Development Platform for In-Memory Data".
<https://arrow.apache.org/>

[^3]: Bouchet-Valat, M., et al. (2024). "polars: Flexible and Fast Tabular Data in Rust". *Journal of Statistical Software*, 107(4), 1-32.
<https://dataframes.juliadata.org/stable/>

[^4]: Ng, A. (2021). "A Chat with Andrew on MLOps: From Model-centric to Data-centric AI". *DeepLearning.AI Blog*.
<https://www.deeplearning.ai/the-batch/issue-80/>

[^5]: Cui, Y., Jia, M., Lin, T.-Y., Song, Y., & Belongie, S. (2019). "Class-Balanced Loss Based on Effective Number of Samples". *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 9268-9277.
<https://arxiv.org/abs/1901.05555>

[^6]: Lin, T.-Y., Goyal, P., Girshick, R., He, K., & DollÃ¡r, P. (2017). "Focal Loss for Dense Object Detection". *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*, 2980-2988.
<https://arxiv.org/abs/1708.02002>

[^7]: Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). "SMOTE: Synthetic Minority Over-sampling Technique". *Journal of Artificial Intelligence Research*, 16, 321-357.
<https://jair.org/index.php/jair/article/view/10302>

[^8]: Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., & Le, Q. V. (2019). "AutoAugment: Learning Augmentation Strategies from Data". *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 113-123.
<https://arxiv.org/abs/1805.09501>

[^9]: Cubuk, E. D., Zoph, B., Shlens, J., & Le, Q. V. (2020). "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space". *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*, 702-703.
<https://arxiv.org/abs/1909.13719>

[^10]: Dablain, D., Krawczyk, B., & Chawla, N. V. (2021). "DeepSMOTE: Fusing Deep Learning and SMOTE for Imbalanced Data". *IEEE Transactions on Neural Networks and Learning Systems*, 34(9), 6390-6404.
<https://arxiv.org/abs/2105.02340>

[^11]: Boabang, F., & Gyamerah, S. A. (2025). "An Enhanced Focal Loss Function to Mitigate Class Imbalance in Auto Insurance Fraud Detection with Explainable AI". *arXiv preprint*.
<https://arxiv.org/abs/2508.02283>

[^12]: Zha, D., et al. (2023). "Data-centric Artificial Intelligence: A Survey". *ACM Computing Surveys*, 56(4), 1-37.
<https://arxiv.org/abs/2303.10158>

[^13]: Zhou, Y., et al. (2024). "A Survey on Data Quality Dimensions and Tools for Machine Learning". *arXiv preprint*.
<https://arxiv.org/abs/2406.19614>

[^14]: Kotelnikov, A., et al. (2023). "TabDDPM: Modelling Tabular Data with Diffusion Models". *International Conference on Machine Learning (ICML)*, 17564-17579.
<https://arxiv.org/abs/2209.15421>

[^15]: Cui, L., et al. (2024). "Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI". *arXiv preprint*.
<https://arxiv.org/abs/2407.21523>

[^16]: Zhao, S., et al. (2024). "Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods". *Neural Computing and Applications*, 36, 1-23.
<https://arxiv.org/abs/2403.08352>

### æ•™ç§‘æ›¸

- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [https://probml.github.io/pml-book/](https://probml.github.io/pml-book/)
- GÃ©ron, A. (2022). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* (3rd ed.). O'Reilly Media.
- Bezanson, J., Edelman, A., Karpinski, S., & Shah, V. B. (2017). "Rust: A Fresh Approach to Numerical Computing". *SIAM Review*, 59(1), 65-98. [https://julialang.org/research/](https://julialang.org/research/)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚