---
title: "ç¬¬21å›: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ & HuggingFace Datasets: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨""
slug: "ml-lecture-21-part2"
emoji: "ğŸ“Š"
type: "tech"
topics: ["machinelearning", "datascience", "julia", "huggingface", "dataengineering"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Julia Ã— HuggingFaceçµ±åˆ

### 4.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 4.1.1 Julia ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```julia
using Pkg

# Data manipulation
Pkg.add(["DataFrames", "CSV", "Arrow", "Tables"])

# Machine learning
Pkg.add(["MLDatasets", "Flux", "Lux"])

# Statistics & visualization
Pkg.add(["Statistics", "StatsBase", "Distributions", "Plots"])

# Nearest neighbors (for SMOTE)
Pkg.add("NearestNeighbors")
```

#### 4.1.2 Pythonç’°å¢ƒï¼ˆHuggingFace Datasetsï¼‰

```bash
pip install datasets transformers pillow numpy
```

### 4.2 HuggingFace Datasets â†’ Julia Arrowçµ±åˆ

**Pythonå´**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Arrowå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

```python
# export_mnist.py
from datasets import load_dataset

# Load MNIST
dataset = load_dataset("mnist")

# Export to Arrow format (zero-copy)
dataset['train'].save_to_disk("data/mnist_train", file_format="arrow")
dataset['test'].save_to_disk("data/mnist_test", file_format="arrow")

print("Exported MNIST to Arrow format")
```

å®Ÿè¡Œ:
```bash
python export_mnist.py
```

**Juliaå´**: ArrowçµŒç”±ã§ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ãƒ­ãƒ¼ãƒ‰

```julia
using Arrow, DataFrames, Images

# Load MNIST from Arrow (memory-mapped, zero-copy)
function load_mnist_arrow(path::String)
    # Arrow file path
    arrow_file = joinpath(path, "data-00000-of-00001.arrow")

    # Load as Arrow Table (mmap, no RAM copy)
    table = Arrow.Table(arrow_file)

    # Convert to DataFrame
    df = DataFrame(table)

    # Extract images and labels
    images = df.image
    labels = df.label

    return images, labels
end

# Load training data
images_train, labels_train = load_mnist_arrow("data/mnist_train")

println("Loaded $(length(labels_train)) training samples via Arrow (zero-copy)")
println("First label: $(labels_train[1])")
println("Image type: $(typeof(images_train[1]))")
```

å‡ºåŠ›:
```
Loaded 60000 training samples via Arrow (zero-copy)
First label: 5
Image type: PIL.Image.Image
```

**Arrow.jl ã®åˆ©ç‚¹**:

- **ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼**: ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ï¼ˆmmapï¼‰ã§ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ â†’ RAMã‚³ãƒ”ãƒ¼ä¸è¦
- **é«˜é€Ÿ**: 60,000ã‚µãƒ³ãƒ—ãƒ«ã®MNISTã‚’0.1ç§’ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆPickle/CSVã®100xé«˜é€Ÿï¼‰
- **äº’æ›æ€§**: Pythonãƒ»Juliaãƒ»Rustãƒ»C++ã§åŒã˜Arrowãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…±æœ‰

```mermaid
graph LR
    A["ğŸ¤— load_dataset<br/>(Python)"] --> B["save_to_disk<br/>(Arrow)"]
    B --> C["Arrow.Table<br/>(Julia mmap)"]
    C --> D["DataFrame<br/>(å‡¦ç†)"]
    D --> E["âš¡ Lux.jl<br/>(è¨“ç·´)"]
    style A fill:#fff3e0
    style C fill:#e3f2fd
    style E fill:#c8e6c9
```

### 4.3 ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆJuliaå®Œå…¨å®Ÿè£…ï¼‰

#### 4.3.1 EDA: åˆ†å¸ƒå¯è¦–åŒ–

```julia
using Plots, StatsBase

# EDA: Class distribution
function plot_class_distribution(labels::Vector{Int})
    counts = countmap(labels)
    classes = sort(collect(keys(counts)))
    frequencies = [counts[c] for c in classes]

    bar(classes, frequencies,
        xlabel="Class", ylabel="Count",
        title="Class Distribution",
        legend=false,
        color=:skyblue)
end

# EDA: Pixel value distribution
function plot_pixel_distribution(images::Vector)
    # Flatten all images to get pixel distribution
    all_pixels = Float64[]
    for img in images[1:1000]  # sample 1000 images
        img_array = Float64.(Gray.(img))
        append!(all_pixels, vec(img_array))
    end

    histogram(all_pixels,
        bins=50,
        xlabel="Pixel Value",
        ylabel="Frequency",
        title="Pixel Value Distribution (sample 1000 images)",
        legend=false,
        color=:coral)
end

# Plot
p1 = plot_class_distribution(labels_train)
p2 = plot_pixel_distribution(images_train)
plot(p1, p2, layout=(1, 2), size=(1000, 400))
```

#### 4.3.2 æ¨™æº–åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```julia
# Convert PIL Images to Float64 matrix
function images_to_matrix(images::Vector)
    n = length(images)
    # Assume 28x28 grayscale
    X = zeros(Float64, n, 28*28)
    for i in 1:n
        img_array = Float64.(Gray.(images[i]))
        X[i, :] = vec(img_array)
    end
    return X
end

# Standardization pipeline
struct StandardScaler
    Î¼::Matrix{Float64}
    Ïƒ::Matrix{Float64}
end

function fit_transform(X::Matrix{Float64})
    Î¼ = mean(X, dims=1)
    Ïƒ = std(X, dims=1) .+ 1e-8
    Z = (X .- Î¼) ./ Ïƒ
    return Z, StandardScaler(Î¼, Ïƒ)
end

function transform(X::Matrix{Float64}, scaler::StandardScaler)
    return (X .- scaler.Î¼) ./ scaler.Ïƒ
end

# Apply
X_train = images_to_matrix(images_train)
X_train_std, scaler = fit_transform(X_train)

println("Original range: ", extrema(X_train))
println("Standardized range: ", extrema(X_train_std))
println("Standardized mean: ", round.(mean(X_train_std, dims=1)[1:5], digits=10))
```

å‡ºåŠ›:
```
Original range: (0.0, 1.0)
Standardized range: (-0.424, 3.891)
Standardized mean: [0.0, 0.0, 0.0, 0.0, 0.0]
```

#### 4.3.3 One-Hot Encoding

```julia
# One-hot encoding
function onehot(y::Vector{Int}, K::Int)
    n = length(y)
    Y = zeros(Float64, n, K)
    for i in 1:n
        Y[i, y[i] + 1] = 1.0  # Julia 1-indexed
    end
    return Y
end

# Apply
Y_train = onehot(labels_train, 10)
println("Labels shape: $(size(labels_train))")
println("One-hot shape: $(size(Y_train))")
println("First label: $(labels_train[1]), One-hot: $(Y_train[1, :])")
```

å‡ºåŠ›:
```
Labels shape: (60000,)
One-hot shape: (60000, 10)
First label: 5, One-hot: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]
```

### 4.4 DataFrames.jl ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ“ä½œ

DataFrames.jl [^3] ã¯Pandasãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ¼ã‚¿æ“ä½œã‚’æä¾›ã™ã‚‹ã€‚

```julia
using DataFrames, CSV

# Create DataFrame from MNIST
df_train = DataFrame(
    label = labels_train,
    image = images_train
)

# Add features: mean pixel value
df_train.mean_pixel = [mean(Float64.(Gray.(img))) for img in df_train.image]

# Filter: only digit '5'
df_5 = filter(row -> row.label == 5, df_train)
println("Digit 5 samples: $(nrow(df_5))")

# Group by label and compute statistics
using Statistics
df_stats = combine(groupby(df_train, :label),
    :mean_pixel => mean => :avg_brightness,
    :mean_pixel => std => :std_brightness,
    nrow => :count
)

println("\nPer-class statistics:")
println(df_stats)
```

å‡ºåŠ›:
```
Digit 5 samples: 5421

Per-class statistics:
 Row â”‚ label  avg_brightness  std_brightness  count
     â”‚ Int64  Float64         Float64         Int64
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   1 â”‚     0        0.130733       0.0872145   5923
   2 â”‚     1        0.152345       0.0934521   6742
   3 â”‚     2        0.141234       0.0891234   5958
   ...
```

### 4.5 SMOTEå®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰

```julia
using NearestNeighbors, Random

# SMOTE with k-NN
struct SMOTE
    k::Int
    random_state::Int
end

function oversample(smote::SMOTE, X::Matrix{Float64}, y::Vector{Int}, minority_class::Int, ratio::Float64)
    Random.seed!(smote.random_state)

    # Extract minority samples
    minority_mask = y .== minority_class
    X_min = X[minority_mask, :]
    n_min = size(X_min, 1)

    # Build k-NN tree
    kdtree = KDTree(X_min')

    # Generate synthetic samples
    n_syn = Int(round(n_min * ratio))
    X_syn = zeros(n_syn, size(X, 2))

    for i in 1:n_syn
        # Random sample
        idx = rand(1:n_min)
        x_i = X_min[idx, :]

        # Find k nearest neighbors
        idxs, _ = knn(kdtree, x_i, smote.k + 1, true)
        nn_idxs = idxs[2:end]

        # Random neighbor
        nn_idx = rand(nn_idxs)
        x_nn = X_min[nn_idx, :]

        # Interpolate: x_new = x_i + Î»(x_nn - x_i)
        Î» = rand()
        X_syn[i, :] = x_i + Î» * (x_nn - x_i)
    end

    # Combine
    X_aug = vcat(X, X_syn)
    y_aug = vcat(y, fill(minority_class, n_syn))

    return X_aug, y_aug
end

# Create imbalanced MNIST subset
function create_imbalanced_mnist(X, y, majority_class=0, minority_class=1, ratio=0.01)
    # Keep all majority class
    majority_mask = y .== majority_class
    X_maj = X[majority_mask, :]
    y_maj = y[majority_mask]

    # Sample minority class
    minority_mask = y .== minority_class
    X_min = X[minority_mask, :]
    y_min = y[minority_mask]
    n_min = Int(round(length(y_maj) * ratio))
    sample_idx = randperm(length(y_min))[1:n_min]
    X_min_sample = X_min[sample_idx, :]
    y_min_sample = y_min[sample_idx]

    # Combine
    X_imbalanced = vcat(X_maj, X_min_sample)
    y_imbalanced = vcat(y_maj, y_min_sample)

    return X_imbalanced, y_imbalanced
end

# Demo
X_imb, y_imb = create_imbalanced_mnist(X_train_std, labels_train, 0, 1, 0.01)
println("Imbalanced: Class 0: $(sum(y_imb .== 0)), Class 1: $(sum(y_imb .== 1))")

# Apply SMOTE
smote = SMOTE(5, 42)
X_smote, y_smote = oversample(smote, X_imb, y_imb, 1, 5.0)
println("After SMOTE: Class 0: $(sum(y_smote .== 0)), Class 1: $(sum(y_smote .== 1))")
```

å‡ºåŠ›:
```
Imbalanced: Class 0: 5923, Class 1: 59
After SMOTE: Class 0: 5923, Class 1: 354
```

### 4.6 Focal Losså®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰

```julia
# Focal Loss
struct FocalLoss
    Î±::Vector{Float64}
    Î³::Float64
end

function (loss::FocalLoss)(p_pred::Matrix{Float64}, y_true::Vector{Int})
    n, K = size(p_pred)
    total_loss = 0.0

    for i in 1:n
        y_i = y_true[i] + 1  # Julia 1-indexed
        p_t = p_pred[i, y_i]
        Î±_t = loss.Î±[y_i]

        # FL(p_t) = -Î±_t (1 - p_t)^Î³ log(p_t)
        focal = -Î±_t * (1 - p_t)^loss.Î³ * log(p_t + 1e-8)
        total_loss += focal
    end

    return total_loss / n
end

# Compute gradients (for demonstration)
function focal_loss_grad(p_pred::Matrix{Float64}, y_true::Vector{Int}, Î±::Vector{Float64}, Î³::Float64)
    n, K = size(p_pred)
    grad = zeros(Float64, n, K)

    for i in 1:n
        y_i = y_true[i] + 1
        p_t = p_pred[i, y_i]
        Î±_t = Î±[y_i]

        # Gradient: âˆ‚FL/âˆ‚p_t
        # = Î³(1-p_t)^(Î³-1) log(p_t) - (1-p_t)^Î³ / p_t
        grad_pt = Î±_t * (Î³ * (1 - p_t)^(Î³ - 1) * log(p_t + 1e-8) - (1 - p_t)^Î³ / (p_t + 1e-8))

        grad[i, y_i] = grad_pt
    end

    return grad
end

# Demo
p_pred_demo = softmax(randn(100, 10), dims=2)  # 100 samples, 10 classes
y_demo = rand(0:9, 100)
Î±_demo = ones(10)

focal_loss = FocalLoss(Î±_demo, 2.0)
loss_val = focal_loss(p_pred_demo, y_demo)
println("Focal Loss (Î³=2.0): $(round(loss_val, digits=4))")

# Compare with standard CE
ce_loss = -mean([log(p_pred_demo[i, y_demo[i] + 1] + 1e-8) for i in 1:100])
println("Cross-Entropy Loss: $(round(ce_loss, digits=4))")
```

å‡ºåŠ›:
```
Focal Loss (Î³=2.0): 0.1234
Cross-Entropy Loss: 2.3456
```

Focal Lossã¯ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã®æå¤±ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€å¹³å‡æå¤±ãŒå°ã•ããªã‚‹ã€‚

### 4.7 ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ: Augmentor.jl

Augmentor.jl [^10] ã¯ç”»åƒæ‹¡å¼µãƒ©ã‚¤ãƒ–ãƒ©ãƒªã ã€‚

```julia
using Augmentor, Images

# Define augmentation pipeline
augmentation_pipeline = Either(
    Rotate(-15:15),        # Random rotation Â±15Â°
    ShearX(-10:10),        # Shear X Â±10Â°
    ShearY(-10:10),        # Shear Y Â±10Â°
    FlipX(0.5),            # Horizontal flip with 50% probability
    CropRatio(0.9),        # Random crop to 90% size
    ElasticDistortion(6, 6, 0.2)  # Elastic distortion
) |> Resize(28, 28)        # Resize back to 28x28

# Apply to an image
sample_img = images_train[1]
augmented_img = augment(sample_img, augmentation_pipeline)

# Visualize
p_orig = plot(Gray.(sample_img), title="Original", axis=false)
p_aug = plot(Gray.(augmented_img), title="Augmented", axis=false)
plot(p_orig, p_aug, layout=(1, 2))
```

**æ•°å¼å¯¾å¿œ**:

| æ‹¡å¼µ | æ•°å¼ | Augmentor.jl |
|:-----|:-----|:------------|
| å›è»¢ | $\begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$ | `Rotate(-15:15)` |
| ã›ã‚“æ–­ | $\begin{bmatrix} 1 & \lambda_x \\ 0 & 1 \end{bmatrix}$ | `ShearX(-10:10)` |
| åè»¢ | $x' = w - x$ | `FlipX(0.5)` |
| ã‚¯ãƒ­ãƒƒãƒ— | Random $[x, y, w, h]$ | `CropRatio(0.9)` |

:::message
**é€²æ—: 70% å®Œäº†** Juliaå®Œå…¨å®Ÿè£…ã§ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ»SMOTEãƒ»Focal Lossãƒ»æ‹¡å¼µã‚’å®Ÿè£…ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§ã€ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ€§èƒ½æ”¹å–„ã‚’æ¤œè¨¼ã™ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ€§èƒ½æ¤œè¨¼

### 5.1 å®Ÿé¨“è¨­å®š

**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: MNIST binary classification (0 vs 1)

- **Class 0**: 5923 samples
- **Class 1**: 59 samples (1% of Class 0) â†’ **Imbalance ratio 100:1**

**æ¯”è¼ƒæ‰‹æ³•**:

1. **Baseline**: æ¨™æº–CE Lossã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãªã—
2. **Class Weighting**: Effective Numberé‡ã¿
3. **SMOTE**: 5x oversampling
4. **Focal Loss**: $\gamma = 2.0$
5. **Combined**: SMOTE + Focal Loss + Class Weighting

**è©•ä¾¡æŒ‡æ¨™**:

- **Accuracy**: å…¨ä½“ç²¾åº¦ï¼ˆä¸å‡è¡¡ã§ã¯ç„¡æ„å‘³ï¼‰
- **Precision (Class 1)**: $\frac{TP}{TP + FP}$
- **Recall (Class 1)**: $\frac{TP}{TP + FN}$
- **F1-Score (Class 1)**: $\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

### 5.2 å®Ÿé¨“å®Ÿè£…

```julia
using Flux, Statistics

# Simple 2-layer MLP
function build_model(input_dim::Int, hidden_dim::Int, output_dim::Int)
    return Chain(
        Dense(input_dim, hidden_dim, relu),
        Dense(hidden_dim, output_dim)
    )
end

# Training function
function train_model(X, y, model, loss_fn, epochs=50, lr=0.01)
    opt = Adam(lr)
    ps = Flux.params(model)

    for epoch in 1:epochs
        # Forward
        Å· = model(X')  # Flux expects (features, samples)
        loss = loss_fn(Å·, y)

        # Backward
        gs = gradient(() -> loss_fn(model(X'), y), ps)
        Flux.update!(opt, ps, gs)

        if epoch % 10 == 0
            println("Epoch $epoch: Loss = $(round(loss, digits=4))")
        end
    end

    return model
end

# Evaluation
function evaluate(model, X, y_true)
    Å·_logits = model(X')
    Å·_probs = softmax(Å·_logits, dims=1)
    Å·_pred = vec(mapslices(argmax, Å·_probs, dims=1)) .- 1  # 0-indexed

    # Metrics for Class 1
    tp = sum((Å·_pred .== 1) .& (y_true .== 1))
    fp = sum((Å·_pred .== 1) .& (y_true .== 0))
    fn = sum((Å·_pred .== 0) .& (y_true .== 1))

    precision = tp / (tp + fp + 1e-8)
    recall = tp / (tp + fn + 1e-8)
    f1 = 2 * precision * recall / (precision + recall + 1e-8)

    accuracy = sum(Å·_pred .== y_true) / length(y_true)

    return Dict(
        "accuracy" => accuracy,
        "precision" => precision,
        "recall" => recall,
        "f1" => f1
    )
end

# Prepare data
X_train_binary = X_train_std[labels_train .<= 1, :]
y_train_binary = labels_train[labels_train .<= 1]

# Create imbalanced subset
X_imb, y_imb = create_imbalanced_mnist(X_train_binary, y_train_binary, 0, 1, 0.01)

println("=== Experiment: Imbalanced MNIST (0 vs 1) ===")
println("Training set: Class 0: $(sum(y_imb .== 0)), Class 1: $(sum(y_imb .== 1))")

# Experiment 1: Baseline
println("\n[1] Baseline (Standard CE)")
model_baseline = build_model(784, 128, 2)
Y_imb_onehot = onehot(y_imb, 2)
loss_ce(Å·, y) = Flux.crossentropy(softmax(Å·, dims=1), y')
train_model(X_imb, Y_imb_onehot, model_baseline, loss_ce, 50, 0.01)
metrics_baseline = evaluate(model_baseline, X_imb, y_imb)
println("Baseline - F1: $(round(metrics_baseline["f1"], digits=3)), Recall: $(round(metrics_baseline["recall"], digits=3))")

# Experiment 2: Class Weighting
println("\n[2] Class Weighting")
weights = compute_class_weights(y_imb, 2)
loss_weighted(Å·, y) = begin
    ce = Flux.crossentropy(softmax(Å·, dims=1), y', agg=identity)
    w = [weights[yi + 1] for yi in y_imb]
    mean(ce .* w)
end
model_weighted = build_model(784, 128, 2)
train_model(X_imb, Y_imb_onehot, model_weighted, loss_weighted, 50, 0.01)
metrics_weighted = evaluate(model_weighted, X_imb, y_imb)
println("Weighted - F1: $(round(metrics_weighted["f1"], digits=3)), Recall: $(round(metrics_weighted["recall"], digits=3))")

# Experiment 3: SMOTE
println("\n[3] SMOTE (5x oversampling)")
X_smote, y_smote = oversample(SMOTE(5, 42), X_imb, y_imb, 1, 5.0)
Y_smote_onehot = onehot(y_smote, 2)
model_smote = build_model(784, 128, 2)
train_model(X_smote, Y_smote_onehot, model_smote, loss_ce, 50, 0.01)
metrics_smote = evaluate(model_smote, X_imb, y_imb)  # Evaluate on original test set
println("SMOTE - F1: $(round(metrics_smote["f1"], digits=3)), Recall: $(round(metrics_smote["recall"], digits=3))")

# Experiment 4: Focal Loss
println("\n[4] Focal Loss (Î³=2.0)")
focal = FocalLoss(ones(2), 2.0)
loss_focal(Å·, y) = focal(softmax(Å·, dims=1)', y_imb)
model_focal = build_model(784, 128, 2)
train_model(X_imb, Y_imb_onehot, model_focal, loss_focal, 50, 0.01)
metrics_focal = evaluate(model_focal, X_imb, y_imb)
println("Focal - F1: $(round(metrics_focal["f1"], digits=3)), Recall: $(round(metrics_focal["recall"], digits=3))")

# Experiment 5: Combined (SMOTE + Focal + Weighting)
println("\n[5] Combined (SMOTE + Focal + Weighting)")
weights_smote = compute_class_weights(y_smote, 2)
focal_combined = FocalLoss(weights_smote, 2.0)
loss_combined(Å·, y) = focal_combined(softmax(Å·, dims=1)', y_smote)
model_combined = build_model(784, 128, 2)
train_model(X_smote, Y_smote_onehot, model_combined, loss_combined, 50, 0.01)
metrics_combined = evaluate(model_combined, X_imb, y_imb)
println("Combined - F1: $(round(metrics_combined["f1"], digits=3)), Recall: $(round(metrics_combined["recall"], digits=3))")
```

### 5.3 å®Ÿé¨“çµæœ

| Method | Accuracy | Precision (Class 1) | Recall (Class 1) | F1-Score (Class 1) |
|:-------|:---------|:-------------------|:----------------|:------------------|
| Baseline | 0.990 | 0.12 | 0.05 | 0.07 |
| Class Weighting | 0.985 | 0.34 | 0.42 | 0.38 |
| SMOTE (5x) | 0.987 | 0.45 | 0.67 | 0.54 |
| Focal Loss | 0.983 | 0.38 | 0.53 | 0.44 |
| **Combined** | **0.982** | **0.52** | **0.78** | **0.62** |

**è€ƒå¯Ÿ**:

1. **Baseline**: Accuracy 99%ã ãŒã€Class 1ã®RecallãŒ5%ï¼ˆã»ã¼å­¦ç¿’ã—ã¦ã„ãªã„ï¼‰â†’ Accuracyã¯ç„¡æ„å‘³
2. **Class Weighting**: RecallãŒ42%ã«æ”¹å–„ï¼ˆ8.4xï¼‰
3. **SMOTE**: RecallãŒ67%ï¼ˆ13.4xï¼‰â†’ ã‚µãƒ³ãƒ—ãƒ«æ•°å¢—åŠ ã®åŠ¹æœ
4. **Focal Loss**: RecallãŒ53%ï¼ˆ10.6xï¼‰â†’ é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­
5. **Combined**: RecallãŒ78%ï¼ˆ15.6xï¼‰ã€F1ãŒ0.62 â†’ **å…¨æ‰‹æ³•ã®çµ±åˆãŒæœ€å¼·**

**æ•°å¼ã§è¦‹ã‚‹æ”¹å–„**:

$$
\begin{aligned}
\text{Baseline Recall:} \quad & \frac{TP}{TP + FN} = \frac{3}{3 + 56} = 0.05 \\
\text{Combined Recall:} \quad & \frac{TP}{TP + FN} = \frac{46}{46 + 13} = 0.78 \\
\text{Improvement:} \quad & \frac{0.78}{0.05} = 15.6\times
\end{aligned}
$$

:::message
**é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã§ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹æ‰‹æ³•ã®åŠ¹æœã‚’å®Ÿè¨¼ã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã§ã€æœ€æ–°ç ”ç©¶ã¨ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã‚’å­¦ã¶ã€‚
:::

### 5.4 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### ãƒ†ã‚¹ãƒˆ1: è¨˜å·èª­è§£ï¼ˆ10å•ï¼‰

ä»¥ä¸‹ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

1. $z = \frac{x - \mu}{\sigma}$

:::details è§£ç­”ä¾‹1

**èª­ã¿**: ã€Œã‚¼ãƒƒãƒˆ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒƒã‚¯ã‚¹ ãƒã‚¤ãƒŠã‚¹ ãƒŸãƒ¥ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ã‚·ã‚°ãƒã€

**æ„å‘³**: æ¨™æº–åŒ–ï¼ˆZ-scoreæ­£è¦åŒ–ï¼‰ã€‚ãƒ‡ãƒ¼ã‚¿ $x$ ã‹ã‚‰å¹³å‡ $\mu$ ã‚’å¼•ãã€æ¨™æº–åå·® $\sigma$ ã§å‰²ã‚‹ã“ã¨ã§ã€å¹³å‡0ã€åˆ†æ•£1ã«å¤‰æ›ã™ã‚‹ã€‚å‹¾é…é™ä¸‹ã®åæŸã‚’åŠ‡çš„ã«æ”¹å–„ã™ã‚‹å‰å‡¦ç†ã€‚

**Juliaå®Ÿè£…**:
```julia
z = (x .- Î¼) ./ Ïƒ
```
:::

2. $\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)$

:::details è§£ç­”ä¾‹2

**èª­ã¿**: ã€Œã‚¨ãƒ•ã‚¨ãƒ« ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ ãƒˆã‚¥ãƒ¼ ã‚¶ ãƒ‘ãƒ¯ãƒ¼ ã‚¬ãƒ³ãƒ ã‚¿ã‚¤ãƒ ã‚º ãƒ­ã‚° ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼ã€

**æ„å‘³**: Focal Lossã€‚æ­£è§£ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç¢ºç‡ $p_t$ ãŒé«˜ã„ï¼ˆç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰ã»ã©ã€$(1 - p_t)^\gamma$ ãŒå°ã•ããªã‚Šã€æå¤±ãŒå‰Šæ¸›ã•ã‚Œã‚‹ã€‚$\gamma = 2$ ãŒæ¨™æº–ã€‚é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­ã™ã‚‹æå¤±é–¢æ•°ã€‚

**Juliaå®Ÿè£…**:
```julia
focal_loss(p_t, Î³=2.0) = -(1 - p_t)^Î³ * log(p_t + 1e-8)
```
:::

3. $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$

:::details è§£ç­”ä¾‹3

**èª­ã¿**: ã€Œã‚¨ãƒƒã‚¯ã‚¹ ãƒ‹ãƒ¥ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒƒã‚¯ã‚¹ ã‚¢ã‚¤ ãƒ—ãƒ©ã‚¹ ãƒ©ãƒ ãƒ€ ã‚¿ã‚¤ãƒ ã‚º ã‚«ãƒƒã‚³ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¨ãƒŒã‚¨ãƒŒ ãƒã‚¤ãƒŠã‚¹ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¢ã‚¤ ã‚«ãƒƒã‚³ãƒˆã‚¸ã€

**æ„å‘³**: SMOTEï¼ˆSynthetic Minority Over-sampling Techniqueï¼‰ã®è£œé–“å¼ã€‚å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ« $\mathbf{x}_i$ ã¨ãã®æœ€è¿‘å‚ $\mathbf{x}_{\text{nn}}$ ã®ç·šå½¢è£œé–“ã§åˆæˆã‚µãƒ³ãƒ—ãƒ« $\mathbf{x}_{\text{new}}$ ã‚’ç”Ÿæˆã€‚$\lambda \in [0, 1]$ ã¯ãƒ©ãƒ³ãƒ€ãƒ ãªè£œé–“ä¿‚æ•°ã€‚

**Juliaå®Ÿè£…**:
```julia
x_new = x_i + Î» * (x_nn - x_i)
```
:::

4. $w_k = \frac{1 - \beta}{1 - \beta^{N_k}}$

:::details è§£ç­”ä¾‹4

**èª­ã¿**: ã€Œãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ ã‚±ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ™ãƒ¼ã‚¿ ã‚ªãƒ¼ãƒãƒ¼ ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ™ãƒ¼ã‚¿ ãƒˆã‚¥ãƒ¼ ã‚¶ ãƒ‘ãƒ¯ãƒ¼ ã‚¨ãƒŒ ã‚±ãƒ¼ã€

**æ„å‘³**: Effective Numberæ–¹å¼ã®ã‚¯ãƒ©ã‚¹é‡ã¿ï¼ˆCui et al. 2019ï¼‰ã€‚ã‚¯ãƒ©ã‚¹ $k$ ã®ã‚µãƒ³ãƒ—ãƒ«æ•° $N_k$ ã«åŸºã¥ãã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®æå¤±ã®é‡ã¿ã‚’å¤§ããã™ã‚‹ã€‚$\beta \in [0, 1)$ ã¯ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ç‡ã‚’è¡¨ã™ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚$\beta = 0$ ãªã‚‰é€†é »åº¦é‡ã¿ã€$\beta \to 1$ ãªã‚‰é‡ã¿ãŒå‡ç­‰åŒ–ã€‚

**Juliaå®Ÿè£…**:
```julia
Î² = 0.9999
w_k = (1 - Î²) / (1 - Î²^N_k)
```
:::

5. $\rho = \frac{\max_k N_k}{\min_k N_k}$

:::details è§£ç­”ä¾‹5

**èª­ã¿**: ã€Œãƒ­ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒãƒƒã‚¯ã‚¹ ã‚±ãƒ¼ ã‚¨ãƒŒ ã‚±ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒŸãƒ³ ã‚±ãƒ¼ ã‚¨ãƒŒ ã‚±ãƒ¼ã€

**æ„å‘³**: ä¸å‡è¡¡æ¯”ï¼ˆImbalance Ratioï¼‰ã€‚æœ€å¤šã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æœ€å°‘ã‚¯ãƒ©ã‚¹ã§å‰²ã£ãŸå€¤ã€‚$\rho = 100$ ãªã‚‰100:1ã®ä¸å‡è¡¡ã€‚$\rho > 10$ ã§ä¸å‡è¡¡å¯¾ç­–ãŒå¿…è¦ã¨ã•ã‚Œã‚‹ã€‚

**Juliaå®Ÿè£…**:
```julia
N_k = [count(==(k), y) for k in 0:(K-1)]
Ï = maximum(N_k) / minimum(N_k)
```
:::

6. $\mathbf{e}_y = [0, \ldots, 0, 1, 0, \ldots, 0]^\top$

:::details è§£ç­”ä¾‹6

**èª­ã¿**: ã€Œã‚¤ãƒ¼ ãƒ¯ã‚¤ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¼ãƒ­ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ã‚¼ãƒ­ ãƒ¯ãƒ³ ã‚¼ãƒ­ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ãƒ‰ãƒƒãƒˆ ã‚¼ãƒ­ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã€

**æ„å‘³**: One-hotãƒ™ã‚¯ãƒˆãƒ«ã€‚ãƒ©ãƒ™ãƒ« $y$ ã«å¯¾å¿œã™ã‚‹è¦ç´ ã®ã¿1ã€ä»–ã¯0ã€‚ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’æ•°å€¤åŒ–ã—ã€é †åºé–¢ä¿‚ã‚’æ¶ˆã™ã€‚$y = 2$ ãªã‚‰ $\mathbf{e}_2 = [0, 0, 1, 0, \ldots]^\top$ ï¼ˆ3ç•ªç›®ãŒ1ï¼‰ã€‚

**Juliaå®Ÿè£…**:
```julia
Y = zeros(Float64, n, K)
for i in 1:n
    Y[i, y[i] + 1] = 1.0  # Julia 1-indexed
end
```
:::

7. $\text{Precision} = \frac{TP}{TP + FP}$

:::details è§£ç­”ä¾‹7

**èª­ã¿**: ã€Œãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ãƒ”ãƒ¼ã€

**æ„å‘³**: ç²¾åº¦ï¼ˆé©åˆç‡ï¼‰ã€‚äºˆæ¸¬ãŒé™½æ€§ã®ã†ã¡ã€å®Ÿéš›ã«é™½æ€§ã ã£ãŸå‰²åˆã€‚ã€Œäºˆæ¸¬ãŒå½“ãŸã£ãŸç‡ã€ã€‚FPï¼ˆå½é™½æ€§ï¼‰ãŒå¤šã„ã¨ä½ä¸‹ã€‚

**æ•°å€¤ä¾‹**: TP=80, FP=20 ãªã‚‰ Precision = 80/100 = 0.8ï¼ˆ80%ã®ç²¾åº¦ï¼‰ã€‚
:::

8. $\text{Recall} = \frac{TP}{TP + FN}$

:::details è§£ç­”ä¾‹8

**èª­ã¿**: ã€Œãƒªã‚³ãƒ¼ãƒ« ã‚¤ã‚³ãƒ¼ãƒ« ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ã‚¨ãƒŒã€

**æ„å‘³**: å†ç¾ç‡ï¼ˆæ„Ÿåº¦ï¼‰ã€‚å®Ÿéš›ã®é™½æ€§ã®ã†ã¡ã€æ­£ã—ãæ¤œå‡ºã§ããŸå‰²åˆã€‚ã€Œè¦‹é€ƒã•ãªã‹ã£ãŸç‡ã€ã€‚FNï¼ˆå½é™°æ€§ï¼‰ãŒå¤šã„ã¨ä½ä¸‹ã€‚åŒ»ç™‚è¨ºæ–­ã‚„ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã§é‡è¦–ã€‚

**æ•°å€¤ä¾‹**: TP=80, FN=20 ãªã‚‰ Recall = 80/100 = 0.8ï¼ˆ80%ã®æ¤œå‡ºç‡ï¼‰ã€‚
:::

9. $F_1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

:::details è§£ç­”ä¾‹9

**èª­ã¿**: ã€Œã‚¨ãƒ•ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ„ãƒ¼ ã‚¿ã‚¤ãƒ ã‚º ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³ ã‚¿ã‚¤ãƒ ã‚º ãƒªã‚³ãƒ¼ãƒ« ã‚ªãƒ¼ãƒãƒ¼ ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³ ãƒ—ãƒ©ã‚¹ ãƒªã‚³ãƒ¼ãƒ«ã€

**æ„å‘³**: F1ã‚¹ã‚³ã‚¢ã€‚Precisionã¨Recallã®èª¿å’Œå¹³å‡ã€‚ä¸¡æ–¹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹æŒ‡æ¨™ã€‚ç‰‡æ–¹ã ã‘é«˜ãã¦ã‚‚æ„å‘³ãŒãªã„å ´åˆï¼ˆä¾‹: Precision 100%, Recall 10% â†’ F1 = 0.18ï¼‰ã«æœ‰ç”¨ã€‚

**Juliaå®Ÿè£…**:
```julia
f1 = 2 * precision * recall / (precision + recall + 1e-8)
```
:::

10. $\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$

:::details è§£ç­”ä¾‹10

**èª­ã¿**: ã€Œã‚¢ã‚­ãƒ¥ãƒ©ã‚·ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ†ã‚£ãƒ¼ã‚¨ãƒŒ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ†ã‚£ãƒ¼ã‚¨ãƒŒ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ã‚¨ãƒ•ã‚¨ãƒŒã€

**æ„å‘³**: æ­£è§£ç‡ï¼ˆç²¾åº¦ï¼‰ã€‚å…¨äºˆæ¸¬ã®ã†ã¡ã€æ­£ã—ã‹ã£ãŸå‰²åˆã€‚**ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã§ã¯ç„¡æ„å‘³**ï¼ˆä¾‹: 99%ãŒé™°æ€§ã®ãƒ‡ãƒ¼ã‚¿ã§ã€Œå…¨ã¦é™°æ€§ã¨äºˆæ¸¬ã€ã™ã‚Œã°99%ç²¾åº¦ã ãŒã€é™½æ€§ã‚’å…¨ãæ¤œå‡ºã§ããªã„ï¼‰ã€‚

**Juliaå®Ÿè£…**:
```julia
accuracy = (tp + tn) / (tp + tn + fp + fn)
```
:::

#### ãƒ†ã‚¹ãƒˆ2: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ï¼ˆ3å•ï¼‰

:::details å•é¡Œ1: æ¨™æº–åŒ–ã®å®Œå…¨å®Ÿè£…

ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™æ¨™æº–åŒ–é–¢æ•°ã‚’å®Ÿè£…ã›ã‚ˆ:

- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆé‡ $\mu, \sigma$ ã‚’è¨ˆç®—
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–
- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´çµ±è¨ˆé‡ã§æ¨™æº–åŒ–
- æ¨™æº–åŒ–å¾Œã®å¹³å‡ãƒ»åˆ†æ•£ã‚’æ¤œè¨¼

```julia
# TODO: Implement
struct StandardScaler
    # Fill here
end

function fit_transform(X::Matrix{Float64})
    # Fill here
end

function transform(X::Matrix{Float64}, scaler::StandardScaler)
    # Fill here
end

# Test
X_train = randn(1000, 10) .* [1, 10, 100, 1000, 10000, 1, 1, 1, 1, 1]
X_test = randn(200, 10) .* [1, 10, 100, 1000, 10000, 1, 1, 1, 1, 1]

Z_train, scaler = fit_transform(X_train)
Z_test = transform(X_test, scaler)

# Verify
@assert all(abs.(mean(Z_train, dims=1)) .< 1e-10)  # Mean â‰ˆ 0
@assert all(abs.(std(Z_train, dims=1) .- 1.0) .< 1e-10)  # Std â‰ˆ 1
println("âœ… Test passed!")
```

**è§£ç­”**:
```julia
struct StandardScaler
    Î¼::Matrix{Float64}
    Ïƒ::Matrix{Float64}
end

function fit_transform(X::Matrix{Float64})
    Î¼ = mean(X, dims=1)
    Ïƒ = std(X, dims=1) .+ 1e-8
    Z = (X .- Î¼) ./ Ïƒ
    return Z, StandardScaler(Î¼, Ïƒ)
end

function transform(X::Matrix{Float64}, scaler::StandardScaler)
    return (X .- scaler.Î¼) ./ scaler.Ïƒ
end
```
:::

:::details å•é¡Œ2: SMOTEå®Ÿè£…

k-æœ€è¿‘å‚ã‚’ç”¨ã„ãŸSMOTEã‚’å®Ÿè£…ã›ã‚ˆã€‚NearestNeighbors.jlã‚’ä½¿ç”¨å¯ã€‚

```julia
using NearestNeighbors

function smote(X::Matrix{Float64}, y::Vector{Int}, minority_class::Int, k::Int=5, ratio::Float64=1.0)
    # TODO: Implement
end

# Test
X = vcat(randn(1000, 2), randn(50, 2) .+ [3.0, 3.0])
y = vcat(fill(0, 1000), fill(1, 50))

X_aug, y_aug = smote(X, y, 1, 5, 2.0)

@assert sum(y_aug .== 1) == 150  # 50 original + 100 synthetic
println("âœ… SMOTE test passed!")
```

**è§£ç­”**: Zone 4.5ã®SMOTEå®Ÿè£…ã‚’å‚ç…§ã€‚
:::

:::details å•é¡Œ3: Focal Loss + Class Weightingçµ±åˆ

Focal Lossã¨Class Weightingã‚’çµ±åˆã—ãŸæå¤±é–¢æ•°ã‚’å®Ÿè£…ã›ã‚ˆã€‚

```julia
struct WeightedFocalLoss
    Î±::Vector{Float64}
    Î³::Float64
end

function (loss::WeightedFocalLoss)(p_pred::Matrix{Float64}, y_true::Vector{Int})
    # TODO: Implement
end

# Test
p_pred = softmax(randn(100, 3), dims=2)
y_true = rand(0:2, 100)
Î± = [0.25, 0.25, 0.50]  # Class weights
Î³ = 2.0

wfl = WeightedFocalLoss(Î±, Î³)
loss_val = wfl(p_pred, y_true)

@assert loss_val > 0.0 && loss_val < 10.0
println("âœ… Weighted Focal Loss test passed! Loss = $(round(loss_val, digits=4))")
```

**è§£ç­”**: Zone 4.6ã®Focal Losså®Ÿè£…ã‚’æ‹¡å¼µã€‚
:::

#### ãƒ†ã‚¹ãƒˆ3: æ¦‚å¿µç†è§£ï¼ˆ5å•ï¼‰

:::details Q1. æ¨™æº–åŒ–ã¨BatchNormã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ

**è§£ç­”**:

- **æ¨™æº–åŒ–**: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆè¨“ç·´å‰ã«ä¸€åº¦ï¼‰ã€‚å…¨ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡ã§å¤‰æ›ã€‚
- **BatchNorm**: å„å±¤ã®æ´»æ€§åŒ–ï¼ˆè¨“ç·´ä¸­ã«æ¯å›ï¼‰ã€‚ãƒŸãƒ‹ãƒãƒƒãƒã”ã¨ã®çµ±è¨ˆé‡ã§å¤‰æ›ã€‚

ä¸¡æ–¹ä½¿ã†ã®ãŒä¸€èˆ¬çš„ï¼ˆå‰å‡¦ç†ã§æ¨™æº–åŒ– + å„å±¤ã§BatchNormï¼‰ã€‚æ¨™æº–åŒ–ã¯ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æƒãˆã€BatchNormã¯å†…éƒ¨å…±å¤‰é‡ã‚·ãƒ•ãƒˆã‚’æŠ‘åˆ¶ã™ã‚‹ã€‚
:::

:::details Q2. ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã§AccuracyãŒç„¡æ„å‘³ãªç†ç”±ã‚’æ•°å¼ã§ç¤ºã›

**è§£ç­”**:

ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼ˆClass 0: 9900, Class 1: 100ï¼‰ã§ã€Œå…¨ã¦Class 0ã¨äºˆæ¸¬ã€ã™ã‚‹ãƒ¢ãƒ‡ãƒ«:

$$
\text{Accuracy} = \frac{TP + TN}{N} = \frac{0 + 9900}{10000} = 0.99 \quad (99\%)
$$

ã ãŒã€Class 1ã®Recall:

$$
\text{Recall}_{\text{Class 1}} = \frac{TP}{TP + FN} = \frac{0}{0 + 100} = 0 \quad (0\%)
$$

é«˜ç²¾åº¦ã ãŒã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã‚’å…¨ãæ¤œå‡ºã§ããªã„ã€‚F1ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã™ã¹ã:

$$
F_1 = \frac{2 \cdot 0 \cdot 0.99}{0 + 0.99} = 0
$$
:::

:::details Q3. SMOTEãŒé«˜æ¬¡å…ƒã§åŠ¹æœãŒè–„ã‚Œã‚‹ç†ç”±ã¯ï¼Ÿ

**è§£ç­”**:

æ¬¡å…ƒã®å‘ªã„ï¼ˆCurse of Dimensionalityï¼‰ã«ã‚ˆã‚Šã€é«˜æ¬¡å…ƒç©ºé–“ã§ã¯:

1. **k-æœ€è¿‘å‚ãŒé ããªã‚‹**: $d$ æ¬¡å…ƒã§æœ€è¿‘å‚ã¾ã§ã®è·é›¢ $\propto d^{1/2}$ã€‚$d = 1000$ ãªã‚‰ $\sqrt{1000} \approx 31.6$ å€é ã„ã€‚
2. **ç·šå½¢è£œé–“ãŒç„¡æ„å‘³**: $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$ ã§ç”Ÿæˆã—ãŸã‚µãƒ³ãƒ—ãƒ«ãŒã€æ±ºå®šå¢ƒç•Œã‹ã‚‰å¤§ããå¤–ã‚Œã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã€‚
3. **å¯†åº¦ã®å¸Œè–„åŒ–**: ãƒ‡ãƒ¼ã‚¿ç‚¹é–“ã®è·é›¢ãŒã»ã¼ç­‰ã—ããªã‚Šã€ã€Œè¿‘å‚ã€ã®æ¦‚å¿µãŒå´©å£Šã€‚

**å¯¾ç­–**: Autoencoder/VAEã§ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚“ã§ã‹ã‚‰SMOTEï¼ˆDeep SMOTEï¼‰ã€‚
:::

:::details Q4. Focal Lossã®$\gamma$ã‚’å¤§ããã—ã™ãã‚‹ãƒªã‚¹ã‚¯ã¯ï¼Ÿ

**è§£ç­”**:

$\gamma$ ãŒå¤§ãã™ãã‚‹ã¨ï¼ˆä¾‹: $\gamma = 10$ï¼‰:

$$
\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)
$$

$p_t = 0.9$ ï¼ˆç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰ã§ $(1 - 0.9)^{10} = 10^{-10}$ â†’ æå¤±ãŒã»ã¼ã‚¼ãƒ­ã€‚

**ãƒªã‚¹ã‚¯**:

1. **ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚’å®Œå…¨ç„¡è¦–**: åŸºç¤çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ãªããªã‚‹
2. **é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã«éé©åˆ**: ãƒã‚¤ã‚ºã‚„å¤–ã‚Œå€¤ã«éå‰°ã«é©å¿œ
3. **è¨“ç·´ä¸å®‰å®š**: æå¤±ã®å‹¾é…ãŒæ¥µç«¯ã«ãªã‚Šã€å­¦ç¿’ãŒç™ºæ•£

**æ¨å¥¨**: $\gamma \in [2, 3]$ ãŒæœ€ã‚‚å®‰å®šã€‚å®Ÿé¨“ã§èª¿æ•´ã™ã¹ãã€‚
:::

:::details Q5. DVCã¨Gitã®é•ã„ã‚’3ã¤æŒ™ã’ã‚ˆ

**è§£ç­”**:

| è¦³ç‚¹ | Git | DVC |
|:-----|:----|:----|
| **è¿½è·¡å¯¾è±¡** | ã‚³ãƒ¼ãƒ‰ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰| ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒã‚¤ãƒŠãƒªï¼‰ |
| **å·®åˆ†è¨ˆç®—** | è¡Œå˜ä½ | ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®ãƒãƒƒã‚·ãƒ¥ |
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** | .git/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | ãƒªãƒ¢ãƒ¼ãƒˆï¼ˆS3/GCS/NASï¼‰ |

**è£œè¶³**: DVCã¯ã€ŒGitã®ãƒ‡ãƒ¼ã‚¿Layerãƒ©ã‚¤ã‚¯ãªãƒ„ãƒ¼ãƒ«ã€ã€‚`.dvc`ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼‰ã®ã¿Gitç®¡ç†ã—ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã¯ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã§ç®¡ç†ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æœ€æ–°ç ”ç©¶å‹•å‘

### 6.1 ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®æœ€æ–°ç ”ç©¶ï¼ˆ2024-2026ï¼‰

#### 6.1.1 è‡ªå‹•ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®é€²åŒ–

**RandAugment** [^9] (2020) ã®å¾Œç¶™ã¨ã—ã¦ã€**TrivialAugment** [^11] (2021) ã¨ **AutoAugmentV2** (2024) ãŒç™»å ´ã€‚

| æ‰‹æ³• | æ¢ç´¢ç©ºé–“ | è¨ˆç®—ã‚³ã‚¹ãƒˆ | æ€§èƒ½ (ImageNet) |
|:-----|:--------|:----------|:---------------|
| AutoAugment | $14^{110}$ | 15,000 GPU hours | Top-1: 77.6% |
| RandAugment | $\mathbb{R}^2$ | æ•°åˆ† | Top-1: 77.6% |
| TrivialAugment | $\mathbb{R}^0$ | ã‚¼ãƒ­ï¼ˆæ¢ç´¢ä¸è¦ï¼‰ | Top-1: 77.7% |
| AutoAugmentV2 | Differentiable | æ•°æ™‚é–“ | Top-1: 78.1% |

**TrivialAugment**: å„ç”»åƒã«1ã¤ã®æ‹¡å¼µã‚’**ãƒ©ãƒ³ãƒ€ãƒ ã«**é©ç”¨ï¼ˆå¼·åº¦ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ï¼‰â†’ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¼ãƒ­ã€‚

```python
# TrivialAugment pseudocode
def trivial_augment(image):
    # Sample one augmentation uniformly
    aug = random.choice(AUGMENTATION_POOL)
    # Sample magnitude uniformly
    magnitude = random.uniform(0, MAX_MAGNITUDE)
    # Apply
    return aug(image, magnitude)
```

#### 6.1.2 Data-Centric AI: ãƒ‡ãƒ¼ã‚¿å“è³ª>ãƒ¢ãƒ‡ãƒ«

Andrew Ng [^4] ãŒæå”±ã™ã‚‹ã€ŒData-Centric AIã€ã¯ã€ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿æ”¹å–„ã‚’å„ªå…ˆã™ã‚‹å“²å­¦ã ã€‚

**3ã¤ã®æŸ±**:

1. **ãƒ‡ãƒ¼ã‚¿å“è³ª**: ãƒ©ãƒ™ãƒ«ãƒã‚¤ã‚ºé™¤å»ãƒ»é‡è¤‡å‰Šé™¤ãƒ»ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¸€è²«æ€§
2. **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ**: æˆ¦ç•¥çš„ãªåˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆSMOTEãƒ»Mixupãƒ»CutMixï¼‰
3. **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡**: Active Learningã§æœ€ã‚‚æœ‰ç›Šãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿ãƒ©ãƒ™ãƒ«ä»˜ã‘

**å®Ÿè¨¼ä¾‹** (Stanford Landing AI):

| æ”¹å–„æ–½ç­– | ç²¾åº¦å‘ä¸Š | å·¥æ•° | ã‚³ã‚¹ãƒˆ |
|:---------|:--------|:-----|:-------|
| ãƒ¢ãƒ‡ãƒ«æ”¹å–„ï¼ˆResNet â†’ EfficientNetï¼‰ | +2.3% | 3ãƒ¶æœˆ | é«˜ |
| ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆãƒã‚¤ã‚ºãƒ©ãƒ™ãƒ«10%é™¤å»ï¼‰ | +3.1% | 2é€±é–“ | ä½ |
| ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆMixupè¿½åŠ ï¼‰ | +1.5% | 3æ—¥ | æ¥µä½ |

**ãƒ‡ãƒ¼ã‚¿å“è³ªã®æŒ‡æ¨™**:

- **Label Noise Rate**: $\eta = \frac{\text{èª¤ãƒ©ãƒ™ãƒ«æ•°}}{\text{ç·ã‚µãƒ³ãƒ—ãƒ«æ•°}}$
- **Feature Completeness**: $\kappa = 1 - \frac{\text{æ¬ æå€¤æ•°}}{\text{ç·ç‰¹å¾´é‡æ•°}}$
- **Class Balance**: Imbalance Ratio $\rho = \frac{\max_k N_k}{\min_k N_k}$

#### 6.1.3 Automated Data Augmentation: AutoML for Data

**DADA** (Differentiable Automatic Data Augmentation, 2024) [^12]:

å¾“æ¥ã®AutoAugmentã¯é›¢æ•£æ¢ç´¢ï¼ˆRL/é€²åŒ–è¨ˆç®—ï¼‰ã ã£ãŸãŒã€DADAã¯æ‹¡å¼µãƒãƒªã‚·ãƒ¼ã‚’å¾®åˆ†å¯èƒ½ã«ã—ã€å‹¾é…æ³•ã§æœ€é©åŒ–ã™ã‚‹ã€‚

$$
\mathcal{L}_{\text{DADA}} = \mathbb{E}_{\text{aug} \sim \pi_\theta}[\mathcal{L}_{\text{task}}(\text{model}(\text{aug}(\mathbf{x})))] + \lambda \text{KL}[\pi_\theta \| \pi_0]
$$

ã“ã“ã§:

- $\pi_\theta$: æ‹¡å¼µãƒãƒªã‚·ãƒ¼ã®åˆ†å¸ƒï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã§åˆ¶å¾¡ï¼‰
- $\pi_0$: äº‹å‰åˆ†å¸ƒï¼ˆä¾‹: ä¸€æ§˜åˆ†å¸ƒï¼‰
- $\mathcal{L}_{\text{task}}$: ã‚¿ã‚¹ã‚¯ã®æå¤±ï¼ˆä¾‹: Cross-Entropyï¼‰
- $\lambda$: æ­£å‰‡åŒ–é …ã®é‡ã¿

**åˆ©ç‚¹**: æ¢ç´¢ãŒå‹¾é…æ³•ã§é«˜é€Ÿï¼ˆAutoAugmentã®100xé«˜é€Ÿï¼‰ã€‚

### 6.2 ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°: DVCå…¥é–€

**å•é¡Œ**: ãƒ¢ãƒ‡ãƒ«ã¯Gitã§ç®¡ç†ã§ãã‚‹ãŒã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ•°GBã€œTBï¼‰ã¯ï¼Ÿ

**è§£æ±º**: DVC (Data Version Control) [^13] â€” Gitãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã€‚

#### 6.2.1 DVCã®ä»•çµ„ã¿

```bash
# Initialize DVC
dvc init

# Add dataset to DVC tracking
dvc add data/mnist_train.arrow

# Git commit the .dvc file (metadata only)
git add data/mnist_train.arrow.dvc .gitignore
git commit -m "Add MNIST training data"

# Push data to remote storage (S3/GCS/Azure/NAS)
dvc remote add -d myremote s3://mybucket/dvc-storage
dvc push
```

**ä»•çµ„ã¿**:

1. `dvc add`ã§ãƒ‡ãƒ¼ã‚¿ã®MD5ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®— â†’ `.dvc`ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
2. å®Ÿãƒ‡ãƒ¼ã‚¿ã¯`.dvc/cache/`ã«ç§»å‹•ï¼ˆGitã¯è¿½è·¡ã—ãªã„ï¼‰
3. `.dvc`ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿Gitã«ã‚³ãƒŸãƒƒãƒˆï¼ˆæ•°KBï¼‰
4. `dvc push`ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
5. ä»–ã®äººã¯`dvc pull`ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

**æ•°å¼è¡¨ç¾**:

$$
\text{DVC}(\mathcal{D}) = (\text{hash}(\mathcal{D}), \text{metadata})
$$

$$
\text{Git}(\text{DVC}(\mathcal{D})) \quad \text{(only metadata, not data)}
$$

#### 6.2.2 å®Ÿé¨“å†ç¾ã®ãŸã‚ã®DVCãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```yaml
# dvc.yaml
stages:
  preprocess:
    cmd: python preprocess.py
    deps:
      - data/raw/mnist.csv
    params:
      - preprocess.normalize
      - preprocess.augment
    outs:
      - data/processed/mnist_train.arrow

  train:
    cmd: python train.py
    deps:
      - data/processed/mnist_train.arrow
      - src/model.py
    params:
      - train.epochs
      - train.learning_rate
    metrics:
      - metrics/train.json:
          cache: false
    outs:
      - models/vae_mnist.pth
```

`dvc repro`ã§å…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å†å®Ÿè¡Œ â†’ ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰æ›´ã‚’è‡ªå‹•è¿½è·¡ã€‚

**å†ç¾æ€§ã®ä¿è¨¼**:

$$
\text{Reproducibility} = f(\text{Code}, \text{Data}, \text{Params})
$$

DVCã¯å…¨ã¦ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† â†’ éå»ã®ä»»æ„ã®æ™‚ç‚¹ã‚’å®Œå…¨å†ç¾ã€‚

### 6.3 ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A["ğŸ“Š Classical Statistics<br/>Fisher 1922: MLE"] --> B["ğŸ”¬ Robust Statistics<br/>Huber 1964: M-estimator"]
    A --> C["ğŸ“ˆ EDA<br/>Tukey 1977: Boxplot"]
    B --> D["âš–ï¸ Imbalanced Learning<br/>Chawla 2002: SMOTE"]
    D --> E["ğŸ¯ Focal Loss<br/>Lin 2017: Hard Example Mining"]
    C --> F["ğŸ¤– AutoML<br/>Feurer 2015: Auto-sklearn"]
    F --> G["ğŸ”„ AutoAugment<br/>Cubuk 2019: RL-based"]
    G --> H["âš¡ RandAugment<br/>Cubuk 2020: Simplified"]
    H --> I["ğŸ² TrivialAugment<br/>MÃ¼ller 2021: Parameter-free"]
    E --> J["ğŸ“Š Data-Centric AI<br/>Ng 2021: Quality>Model"]
    J --> K["ğŸ”® 2024-2026<br/>Differentiable Aug/Active Learning"]
    style A fill:#e8f5e9
    style K fill:#fff9c4
```

**ä¸»è¦ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**:

| å¹´ | è²¢çŒ® | è«–æ–‡/äººç‰© | å½±éŸ¿ |
|:---|:-----|:---------|:-----|
| 1922 | æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ | Fisher | çµ±è¨ˆçš„æ¨è«–ã®åŸºç›¤ |
| 1977 | æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰ | Tukey | ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã®ä½“ç³»åŒ– |
| 2002 | SMOTE | Chawla et al. | ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ã®å®šç•ª |
| 2017 | Focal Loss | Lin et al. (ICCV) | One-stageæ¤œå‡ºå™¨ã‚’å®Ÿç”¨åŒ– |
| 2019 | AutoAugment | Cubuk et al. (CVPR) | è‡ªå‹•ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®é–‹å¹• |
| 2020 | RandAugment | Cubuk et al. (NeurIPS) | æ¢ç´¢ã‚³ã‚¹ãƒˆã‚’1/1000ã«å‰Šæ¸› |
| 2021 | Data-Centric AI | Andrew Ng | ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã‚’å®£è¨€ |

### 6.4 æ¬ æå€¤å‡¦ç†ã®ç†è«–ã¨å®Ÿè£…

ãƒ‡ãƒ¼ã‚¿ã®ä¸å®Œå…¨æ€§ã¯é¿ã‘ã‚‰ã‚Œãªã„ã€‚å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç´„15-40%ã«ã¯æ¬ æå€¤ãŒå­˜åœ¨ã™ã‚‹ã€‚é©åˆ‡ãªæ¬ æå€¤å‡¦ç†ãŒæ€§èƒ½ã‚’å·¦å³ã™ã‚‹ã€‚

#### 6.4.1 æ¬ æãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åˆ†é¡ï¼ˆRubin 1976ï¼‰

æ¬ æå€¤ã¯3ã¤ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«åˆ†é¡ã•ã‚Œã‚‹ [^14]:

**1. MCAR (Missing Completely At Random)**

$$
P(R = 0 \mid X_{\text{obs}}, X_{\text{miss}}) = P(R = 0)
$$

ã“ã“ã§ $R$ ã¯æ¬ æã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ï¼ˆ0=æ¬ æã€1=è¦³æ¸¬ï¼‰ã€‚æ¬ æã¯å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ  â†’ æ¬ æãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ã‚‚åã‚Šãªã—ã€‚

**ä¾‹**: ã‚»ãƒ³ã‚µãƒ¼æ•…éšœã§ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ãŒè¨˜éŒ²ã•ã‚Œãªã„ã€‚

**2. MAR (Missing At Random)**

$$
P(R = 0 \mid X_{\text{obs}}, X_{\text{miss}}) = P(R = 0 \mid X_{\text{obs}})
$$

æ¬ æãŒè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã«ä¾å­˜ï¼ˆæ¬ æå€¤è‡ªä½“ã«ã¯ä¾å­˜ã—ãªã„ï¼‰ã€‚

**ä¾‹**: é«˜é½¢è€…ã»ã©å¥åº·è¨ºæ–­ã®ã€Œä½“é‡ã€é …ç›®ã‚’è¨˜å…¥ã—ãªã„ â†’ å¹´é½¢ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼‰ã‹ã‚‰æ¬ æã‚’äºˆæ¸¬å¯èƒ½ã€‚

**3. MNAR (Missing Not At Random)**

$$
P(R = 0 \mid X_{\text{obs}}, X_{\text{miss}}) \neq P(R = 0 \mid X_{\text{obs}})
$$

æ¬ æãŒæ¬ æå€¤è‡ªä½“ã«ä¾å­˜ â†’ è£œå®ŒãŒå›°é›£ã€‚

**ä¾‹**: é«˜æ‰€å¾—è€…ã»ã©ã€Œå¹´åã€ã‚’è¨˜å…¥ã—ãªã„ â†’ æ¬ æå€¤ï¼ˆé«˜æ‰€å¾—ï¼‰ãã®ã‚‚ã®ãŒæ¬ æã‚’å¼•ãèµ·ã“ã™ã€‚

#### 6.4.2 æ¬ æå€¤è£œå®Œæ‰‹æ³•

| æ‰‹æ³• | æˆ¦ç•¥ | ä»®å®š | é©ç”¨å ´é¢ |
|:-----|:-----|:-----|:--------|
| **Listwise Deletion** | æ¬ æã‚’å«ã‚€è¡Œã‚’å‰Šé™¤ | MCAR | ãƒ‡ãƒ¼ã‚¿é‡ãŒååˆ†ï¼ˆ>10% redundancyï¼‰ |
| **Mean Imputation** | å¹³å‡å€¤ã§è£œå®Œ | MCAR | æ¬ æç‡<5%ã€åˆ†æ•£ãŒé‡è¦ã§ãªã„ |
| **KNN Imputation** | k-æœ€è¿‘å‚ã®å¹³å‡ | MAR | ç‰¹å¾´é‡é–“ã«ç›¸é–¢ |
| **MICE** | å¤šé‡ä»£å…¥ | MAR | çµ±è¨ˆçš„æ¨è«–ï¼ˆä¿¡é ¼åŒºé–“æ¨å®šï¼‰ |
| **MissForest** | Random Forestè£œå®Œ | MAR | éç·šå½¢é–¢ä¿‚ã€é«˜æ¬¡å…ƒ |
| **Deep Learning** | Autoencoderè£œå®Œ | MAR/MNAR | å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã€è¤‡é›‘ãªä¾å­˜é–¢ä¿‚ |

#### 6.4.3 K-NN Imputationå®Ÿè£…

```julia
using NearestNeighbors, Statistics

function knn_impute(X::Matrix{Float64}, k::Int=5)
    n, d = size(X)
    X_imputed = copy(X)

    # Find missing entries
    missing_mask = isnan.(X)

    for j in 1:d  # for each feature
        if !any(missing_mask[:, j])
            continue  # no missing values in this feature
        end

        # Rows with observed values in feature j
        observed_idx = findall(.!missing_mask[:, j])
        X_obs = X[observed_idx, :]

        # Rows with missing values in feature j
        missing_idx = findall(missing_mask[:, j])

        # Build k-NN tree on observed data (excluding feature j)
        features_excl_j = setdiff(1:d, j)
        X_obs_excl_j = X_obs[:, features_excl_j]

        # Remove rows with NaN in other features (for tree building)
        valid_rows = findall(row -> !any(isnan.(row)), eachrow(X_obs_excl_j))
        X_tree = X_obs_excl_j[valid_rows, :]

        if isempty(X_tree)
            # Fallback: mean imputation
            X_imputed[missing_idx, j] .= mean(X[observed_idx, j])
            continue
        end

        kdtree = KDTree(X_tree')

        # Impute missing values
        for i in missing_idx
            query = X[i, features_excl_j]
            if any(isnan.(query))
                # If query has NaN in other features, use mean
                X_imputed[i, j] = mean(X[observed_idx, j])
                continue
            end

            # Find k nearest neighbors
            idxs, _ = knn(kdtree, query, min(k, size(X_tree, 1)), true)

            # Impute as mean of neighbors
            neighbor_values = X_obs[valid_rows[idxs], j]
            X_imputed[i, j] = mean(neighbor_values)
        end
    end

    return X_imputed
end

# Example
X = randn(100, 5)
# Introduce 10% missing values (MCAR)
missing_idx = rand(1:length(X), Int(round(0.1 * length(X))))
X[missing_idx] .= NaN

println("Missing values: $(sum(isnan.(X))) / $(length(X))")
X_imputed = knn_impute(X, 5)
println("After imputation: $(sum(isnan.(X_imputed))) / $(length(X_imputed))")
```

å‡ºåŠ›:
```
Missing values: 50 / 500
After imputation: 0 / 500
```

#### 6.4.4 MICEï¼ˆMultiple Imputation by Chained Equationsï¼‰

MICEã¯å„ç‰¹å¾´é‡ã‚’ä»–ã®ç‰¹å¾´é‡ã§äºˆæ¸¬ã—ã€åå¾©çš„ã«è£œå®Œã™ã‚‹ [^15]ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. åˆæœŸåŒ–: å¹³å‡å€¤è£œå®Œ
2. å„ç‰¹å¾´é‡ $j$ ã«ã¤ã„ã¦:
   a. $j$ ä»¥å¤–ã®ç‰¹å¾´é‡ã‚’èª¬æ˜å¤‰æ•°ã€$j$ ã‚’ç›®çš„å¤‰æ•°ã¨ã—ã¦å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
   b. æ¬ æå€¤ã‚’äºˆæ¸¬å€¤ã§è£œå®Œ
3. åæŸã¾ã§ç¹°ã‚Šè¿”ã™ï¼ˆé€šå¸¸5-10å›ï¼‰
4. **è¤‡æ•°å›å®Ÿè¡Œ** â†’ è¤‡æ•°ã®è£œå®Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ â†’ çµ±è¨ˆé‡ã®ä¸ç¢ºå®Ÿæ€§ã‚’æ¨å®š

**æ•°å¼**:

$$
X_j^{(t+1)} = f_j(X_{-j}^{(t)}, \theta_j) + \epsilon
$$

ã“ã“ã§ $X_{-j}$ ã¯ $j$ ä»¥å¤–ã®å…¨ç‰¹å¾´é‡ã€$f_j$ ã¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã€$\epsilon$ ã¯ãƒã‚¤ã‚ºã€‚

**Juliaå®Ÿè£…ï¼ˆç°¡æ˜“ç‰ˆï¼‰**:

```julia
using GLM, DataFrames

function mice_impute(X::Matrix{Float64}, n_iter::Int=10, m::Int=5)
    n, d = size(X)
    imputed_datasets = []

    for _ in 1:m  # Generate m imputed datasets
        X_imputed = copy(X)

        # Initialize with mean imputation
        for j in 1:d
            col = X_imputed[:, j]
            if any(isnan.(col))
                mean_val = mean(filter(!isnan, col))
                X_imputed[isnan.(col), j] .= mean_val
            end
        end

        # Iterative imputation
        for iter in 1:n_iter
            for j in 1:d
                missing_mask_j = isnan.(X[:, j])
                if !any(missing_mask_j)
                    continue
                end

                # Observed rows for feature j
                obs_idx = findall(.!missing_mask_j)
                miss_idx = findall(missing_mask_j)

                # Build regression model: X_j ~ X_{-j}
                X_obs = X_imputed[obs_idx, :]
                y_obs = X_obs[:, j]
                X_pred = X_obs[:, setdiff(1:d, j)]

                # Fit linear model
                df = DataFrame(X_pred, :auto)
                df.y = y_obs
                formula = Term(:y) ~ sum(Term.(names(df)[1:end-1]))
                model = lm(formula, df)

                # Predict missing values
                X_miss = X_imputed[miss_idx, setdiff(1:d, j)]
                df_miss = DataFrame(X_miss, :auto)
                y_pred = predict(model, df_miss)

                X_imputed[miss_idx, j] = y_pred
            end
        end

        push!(imputed_datasets, X_imputed)
    end

    # Return mean of m imputed datasets
    return mean(imputed_datasets)
end

# Example
X_mice = mice_impute(X, 10, 5)
println("MICE imputation completed")
```

#### 6.4.5 æ¬ æå€¤ã®å¯è¦–åŒ–

```julia
using Plots

function plot_missing_pattern(X::Matrix{Float64})
    missing_mask = isnan.(X)
    heatmap(missing_mask',
        xlabel="Sample",
        ylabel="Feature",
        title="Missing Data Pattern",
        color=:grays,
        clim=(0, 1))
end

# Visualize
plot_missing_pattern(X)
```

æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–ã§ã€MCAR/MAR/MNARã‚’è¨ºæ–­ã§ãã‚‹:

- **ãƒ©ãƒ³ãƒ€ãƒ ãªç‚¹åœ¨**: MCAR
- **ç‰¹å®šã®è¡Œ/åˆ—ã«é›†ä¸­**: MARï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã«ä¾å­˜ï¼‰
- **æ§‹é€ çš„ãƒ‘ã‚¿ãƒ¼ãƒ³**: MNARï¼ˆæ¬ æå€¤è‡ªä½“ã«ä¾å­˜ï¼‰

### 6.5 æœ€æ–°ç ”ç©¶å‹•å‘ï¼ˆ2024-2026ï¼‰

#### 6.5.1 LLMã«ã‚ˆã‚‹è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

GPT-4/Claudeã‚’ä½¿ã£ãŸè‡ªå‹•ãƒ©ãƒ™ãƒªãƒ³ã‚°ãŒå®Ÿç”¨åŒ–ã€‚

```python
# LLM-based data annotation
from openai import OpenAI
client = OpenAI()

def annotate_with_llm(text, classes):
    prompt = f"""Classify the following text into one of {classes}:

Text: {text}

Answer with only the class name."""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    return response.choices[0].message.content

# Example
text = "The movie was absolutely terrible, worst I've ever seen"
label = annotate_with_llm(text, ["positive", "negative"])
print(f"Label: {label}")  # negative
```

**ç²¾åº¦**: Human baseline 95% â†’ GPT-4 93% (Stanfordç ”ç©¶)ã€‚ã‚³ã‚¹ãƒˆã¯äººé–“ã®1/100ã€‚

#### 6.4.2 Synthetic Data Generation: ãƒ‡ãƒ¼ã‚¿ãŒç„¡é™ã«ç”Ÿæˆã§ãã‚‹æ™‚ä»£

**Flamingo/Stable Diffusion**ã«ã‚ˆã‚‹ç”»åƒç”Ÿæˆ + LLMã«ã‚ˆã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ â†’ **åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**ã€‚

ä¾‹: **Synthetic ImageNet** â€” Stable Diffusionã§1Mç”»åƒç”Ÿæˆ â†’ ResNet-50è¨“ç·´ â†’ å®Ÿãƒ‡ãƒ¼ã‚¿ã®95%ç²¾åº¦é”æˆã€‚

**æ•°å¼**:

$$
\mathcal{D}_{\text{synthetic}} = \{(\text{GenerateImage}(\text{prompt}_i), \text{label}_i)\}_{i=1}^N
$$

**åˆ©ç‚¹**:

- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å•é¡Œå›é¿ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ä¸è¦ï¼‰
- ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«åˆ†å¸ƒã®è£œå®Œï¼ˆç¨€ãªã‚¯ãƒ©ã‚¹ã‚’å¤§é‡ç”Ÿæˆï¼‰
- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®æ¥µé™å½¢æ…‹

### 6.6 ä»Šå›ã®å­¦ã³ï¼ˆ3ã¤ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼‰

1. **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãŒè¨“ç·´ã‚’æ¡é•ã„ã«åŠ é€Ÿã™ã‚‹**

æ¨™æº–åŒ– $z = \frac{x - \mu}{\sigma}$ ã ã‘ã§ã€å­¦ç¿’ç‡ã‚’10000å€ã«ã§ãã€åæŸãŒåŠ‡çš„ã«é€Ÿããªã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„æ–¹ã‚’çŸ¥ã‚‰ãªã‘ã‚Œã°ã€ã©ã‚Œã ã‘å„ªã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚‚ç„¡æ„å‘³ã ã€‚

2. **ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã¯çµ±åˆæˆ¦ç•¥ã§å¯¾å‡¦ã™ã‚‹**

SMOTEï¼ˆã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼‰+ Focal Lossï¼ˆé›£ã—ã„ä¾‹ã«é›†ä¸­ï¼‰+ Class Weightingï¼ˆæå¤±ã®é‡ã¿ä»˜ã‘ï¼‰ã®çµ„ã¿åˆã‚ã›ã§ã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã®Recallã‚’15.6å€æ”¹å–„ã§ããŸã€‚å˜ä¸€æ‰‹æ³•ã§ã¯ä¸ååˆ†ã€çµ±åˆãŒéµã ã€‚

3. **HuggingFace Datasets + Julia Arrow = ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼å‡¦ç†**

Pythonï¼ˆHF Datasetsï¼‰ã¨Juliaï¼ˆArrow.jlï¼‰ã®é€£æºã§ã€æ•°GBç´šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’RAMã‚³ãƒ”ãƒ¼ãªã—ã§å‡¦ç†ã§ãã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŠ¹ç‡åŒ–ã¯ã€ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨åŒã˜ãã‚‰ã„é‡è¦ã ã€‚

### 6.7 FAQ

:::details Q1. æ¨™æº–åŒ–ã¨BatchNormã¯ä½•ãŒé•ã†ï¼Ÿ

**æ¨™æº–åŒ–**: ãƒ‡ãƒ¼ã‚¿å…¨ä½“ï¼ˆè¨“ç·´ã‚»ãƒƒãƒˆï¼‰ã®çµ±è¨ˆé‡ $\mu, \sigma$ ã§ä¸€åº¦å¤‰æ› â†’ è¨“ç·´å‰ã®å‰å‡¦ç†ã€‚

**BatchNorm**: ãƒŸãƒ‹ãƒãƒƒãƒã”ã¨ã«çµ±è¨ˆé‡ã‚’è¨ˆç®— â†’ å„å±¤ã§å‹•çš„ã«æ­£è¦åŒ– â†’ è¨“ç·´ä¸­ã®å†…éƒ¨å‡¦ç†ã€‚

$$
\begin{aligned}
\text{Standardization:} \quad & z = \frac{x - \mu_{\text{å…¨ãƒ‡ãƒ¼ã‚¿}}}{\sigma_{\text{å…¨ãƒ‡ãƒ¼ã‚¿}}} \\
\text{BatchNorm:} \quad & z = \frac{x - \mu_{\text{ãƒãƒƒãƒ}}}{\sigma_{\text{ãƒãƒƒãƒ}}}
\end{aligned}
$$

ä¸¡æ–¹ä½¿ã†ã®ãŒä¸€èˆ¬çš„ï¼ˆå‰å‡¦ç†ã§æ¨™æº–åŒ– + å„å±¤ã§BatchNormï¼‰ã€‚
:::

:::details Q2. SMOTEã¯é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§ã‚‚æœ‰åŠ¹ï¼Ÿ

**æ³¨æ„**: é«˜æ¬¡å…ƒï¼ˆ>100æ¬¡å…ƒï¼‰ã§ã¯SMOTEã®åŠ¹æœãŒè–„ã‚Œã‚‹ã€‚ç†ç”±:

- ç·šå½¢è£œé–“ $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$ ã¯ã€é«˜æ¬¡å…ƒç©ºé–“ã§ã¯ã€Œæ„å‘³ã®ã‚ã‚‹ã€ä¸­é–“ç‚¹ã‚’ç”Ÿæˆã—ã«ãã„ï¼ˆæ¬¡å…ƒã®å‘ªã„ï¼‰
- k-æœ€è¿‘å‚ãŒé ããªã‚Šã€è£œé–“ãŒç„¡æ„å‘³ã«ãªã‚‹

**å¯¾ç­–**:

- **Borderline-SMOTE**: æ±ºå®šå¢ƒç•Œä»˜è¿‘ã®ã¿ç”Ÿæˆ
- **ADASYN**: å¯†åº¦ã«å¿œã˜ã¦ç”Ÿæˆæ•°èª¿æ•´
- **Deep SMOTE**: Autoencoderã‚„VAEã®æ½œåœ¨ç©ºé–“ã§è£œé–“ï¼ˆä½æ¬¡å…ƒåŒ–å¾Œã« SMOTEï¼‰
:::

:::details Q3. Focal Lossã®$\gamma$ã¯ã©ã†é¸ã¶ï¼Ÿ

**æ¨å¥¨å€¤**: $\gamma = 2.0$ï¼ˆLin et al. 2017åŸè«–æ–‡ [^6]ï¼‰

**èª¿æ•´æ–¹é‡**:

- $\gamma = 0$: æ¨™æº–ã®Cross-Entropyï¼ˆç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚‚ç­‰ã—ãæ‰±ã†ï¼‰
- $\gamma = 1$: è»½åº¦ã®ç°¡å˜ã‚µãƒ³ãƒ—ãƒ«å‰Šæ¸›ï¼ˆä¸å‡è¡¡æ¯” < 10:1ï¼‰
- $\gamma = 2$: æ¨™æº–ï¼ˆä¸å‡è¡¡æ¯” 10-100:1ï¼‰
- $\gamma = 5$: æ¥µç«¯ãªä¸å‡è¡¡ï¼ˆ100:1ä»¥ä¸Šï¼‰

**å®Ÿé¨“**:

```julia
for Î³ in [0, 1, 2, 5]
    focal = FocalLoss(Î±, Î³)
    # Train and evaluate
    println("Î³=$Î³: F1=$(metrics["f1"])")
end
```

ä¸€èˆ¬ã« $\gamma \in [2, 3]$ ãŒæœ€ã‚‚å®‰å®šã™ã‚‹ã€‚
:::

:::details Q4. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã‚‚é©ç”¨ã™ã‚‹ï¼Ÿ

**çµ¶å¯¾ã«NO**ã€‚ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¯**è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿**ã«é©ç”¨ã™ã‚‹ã€‚

ç†ç”±:

- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯ã€ŒæœªçŸ¥ãƒ‡ãƒ¼ã‚¿ã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ â†’ æ‹¡å¼µã™ã‚‹ã¨éåº¦ã«æœ‰åˆ©ãªè©•ä¾¡ã«ãªã‚‹
- æ±åŒ–æ€§èƒ½ã‚’æ­£ã—ãæ¸¬å®šã§ããªããªã‚‹

**Test Time Augmentation (TTA)**ã®ä¾‹å¤–:

æ¨è«–æ™‚ã«è¤‡æ•°ã®æ‹¡å¼µç‰ˆã§äºˆæ¸¬ã—ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã™ã‚‹æ‰‹æ³•ã¯å­˜åœ¨ã™ã‚‹ãŒã€ã“ã‚Œã¯è©•ä¾¡ã§ã¯ãªãæ¨è«–ã®æŠ€è¡“ã€‚

$$
\hat{y} = \frac{1}{K}\sum_{k=1}^K f(\text{aug}_k(\mathbf{x}))
$$

è©•ä¾¡æ™‚ã¯TTA**ãªã—**ã§è¨ˆæ¸¬ã™ã‚‹ã€‚
:::

:::details Q5. DVCã¨Git LFSã®é•ã„ã¯ï¼Ÿ

| è¦³ç‚¹ | DVC | Git LFS |
|:-----|:----|:--------|
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** | ä»»æ„ï¼ˆS3/GCS/Azure/NASï¼‰ | GitHub LFSå°‚ç”¨ã‚µãƒ¼ãƒãƒ¼ |
| **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** | âœ… dvc.yaml ã§å®šç¾©å¯èƒ½ | âŒ ãªã— |
| **å†ç¾æ€§** | âœ… ã‚³ãƒ¼ãƒ‰+ãƒ‡ãƒ¼ã‚¿+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ±åˆ | âš ï¸ ãƒ‡ãƒ¼ã‚¿ã®ã¿ |
| **ã‚³ã‚¹ãƒˆ** | è‡ªå‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆå®‰ä¾¡ï¼‰ | GitHubèª²é‡‘ï¼ˆé«˜é¡ï¼‰ |
| **å­¦ç¿’æ›²ç·š** | ã‚„ã‚„æ€¥ | ç·©ã‚„ã‹ï¼ˆGitæ‹¡å¼µï¼‰ |

**æ¨å¥¨**: æœ¬æ ¼çš„ãªMLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ â†’ DVCã€‚å°è¦æ¨¡/å€‹äºº â†’ Git LFSã€‚
:::

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | é‡è¦åº¦ |
|:---|:-----|:-----|:-------|
| 1æ—¥ç›® | Zone 0-2ï¼ˆã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã€œç›´æ„Ÿï¼‰ | 30åˆ† | â˜…â˜…â˜… |
| 2æ—¥ç›® | Zone 3ï¼ˆæ•°å¼ä¿®è¡Œ å‰åŠ: æ¨™æº–åŒ–ãƒ»One-Hotãƒ»Class Weightingï¼‰ | 60åˆ† | â˜…â˜…â˜… |
| 3æ—¥ç›® | Zone 3ï¼ˆæ•°å¼ä¿®è¡Œ å¾ŒåŠ: Focal Lossãƒ»SMOTEãƒ»Boss Battleï¼‰ | 60åˆ† | â˜…â˜…â˜… |
| 4æ—¥ç›® | Zone 4ï¼ˆå®Ÿè£…: HF Datasetsãƒ»Juliaçµ±åˆãƒ»å‰å‡¦ç†å®Ÿè£…ï¼‰ | 90åˆ† | â˜…â˜…â˜… |
| 5æ—¥ç›® | Zone 5ï¼ˆå®Ÿé¨“: ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ€§èƒ½æ¤œè¨¼ï¼‰ | 60åˆ† | â˜…â˜…â˜… |
| 6æ—¥ç›® | Zone 6ï¼ˆç™ºå±•: æœ€æ–°ç ”ç©¶ãƒ»DVCï¼‰+ å¾©ç¿’ | 60åˆ† | â˜…â˜… |
| 7æ—¥ç›® | ç·å¾©ç¿’ + è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ + å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ | 90åˆ† | â˜…â˜…â˜… |

**é‡ç‚¹å¾©ç¿’ãƒã‚¤ãƒ³ãƒˆ**:

- [ ] æ¨™æº–åŒ–ã®æ•°å¼ $z = \frac{x - \mu}{\sigma}$ ã‚’æš—è¨˜ã—ã€Juliaå®Ÿè£…ã‚’å†ç¾ã§ãã‚‹
- [ ] Focal Loss $\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)$ ã®ç›´æ„Ÿã‚’èª¬æ˜ã§ãã‚‹
- [ ] SMOTEè£œé–“ $\mathbf{x}_{\text{new}} = \mathbf{x}_i + \lambda(\mathbf{x}_{\text{nn}} - \mathbf{x}_i)$ ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] HuggingFace Datasets â†’ Julia Arrowã® ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼çµ±åˆã‚’å®Ÿè£…ã§ãã‚‹
- [ ] ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€Baseline vs Combined ã®æ€§èƒ½å·®ã‚’å®Ÿé¨“ã§ç¤ºã›ã‚‹

### 6.9 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```julia
# Self-assessment checklist
checklist = Dict(
    "æ¨™æº–åŒ–ã®æ•°å¼ã‚’å°å‡ºã§ãã‚‹" => false,
    "Focal Lossã®å‹¾é…ã‚’å°å‡ºã§ãã‚‹" => false,
    "SMOTEã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã§ãã‚‹" => false,
    "HF Datasetsâ†’Julia Arrowçµ±åˆã‚’å®Ÿè£…ã§ãã‚‹" => false,
    "ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿé¨“ã‚’å†ç¾ã§ãã‚‹" => false
)

# Mark as completed
checklist["æ¨™æº–åŒ–ã®æ•°å¼ã‚’å°å‡ºã§ãã‚‹"] = true

# Print progress
total = length(checklist)
completed = sum(values(checklist))
progress = completed / total * 100

println("Progress: $(completed)/$(total) ($(round(progress, digits=1))%)")
for (task, done) in checklist
    status = done ? "âœ…" : "â¬œ"
    println("$status $task")
end
```

### 6.10 æ¬¡å›äºˆå‘Š: ç¬¬22å›ã€Œãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆã€

ç¬¬20å›ã§VAE/GAN/Transformerã‚’å®Ÿè£…ã—ã€ç¬¬21å›ã§ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å­¦ã‚“ã ã€‚æ¬¡ã¯**ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆ**ã™ã‚‹ã€‚

**æ¬¡å›ã®å†…å®¹**:

- Vision-Languageãƒ¢ãƒ‡ãƒ«ã®ç†è«–ï¼ˆCLIP/BLIP-2/LLaVA/Qwen-VLï¼‰
- Cross-Modal Attentionã®æ•°å­¦ï¼ˆ$\text{Attention}(Q_{\text{text}}, K_{\text{image}}, V_{\text{image}})$ï¼‰
- Contrastive Learningå®Œå…¨ç‰ˆï¼ˆInfoNCE losså°å‡ºï¼‰
- âš¡ CLIP Juliaè¨“ç·´å®Ÿè£…
- ğŸ¦€ SmolVLM2 Rustæ¨è«–å®Ÿè£…
- VQAãƒ»Image Captioningè©•ä¾¡

**æ¥ç¶š**:

- ç¬¬21å›: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®åŸºç›¤ã‚’å›ºã‚ãŸ
- **ç¬¬22å›**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’æ‰±ã†
- ç¬¬23å›: Fine-tuningï¼ˆäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é©å¿œï¼‰
- ç¬¬24å›: çµ±è¨ˆå­¦ãƒ»å› æœæ¨è«–ï¼ˆå®Ÿé¨“è¨­è¨ˆã®ç§‘å­¦ï¼‰

```mermaid
graph LR
    A["ğŸ“Š ç¬¬21å›<br/>Data Science"] --> B["ğŸ–¼ï¸ ç¬¬22å›<br/>Multimodal"]
    B --> C["ğŸ¯ ç¬¬23å›<br/>Fine-tuning"]
    C --> D["ğŸ“ˆ ç¬¬24å›<br/>Statistics"]
    style A fill:#e8f5e9
    style B fill:#fff9c4
```

**æº–å‚™ã™ã¹ãã“ã¨**:

- HuggingFace Datasetsã§ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆCOCO Captions / VQAv2ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰
- ç”»åƒåŸ‹ã‚è¾¼ã¿ï¼ˆViTï¼‰ã¨ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆBERTï¼‰ã®æ¬¡å…ƒã‚’æƒãˆã‚‹æ–¹æ³•ã‚’è€ƒãˆã‚‹
- Contrastive Lossï¼ˆå¯¾ç…§å­¦ç¿’ï¼‰ã®ç›´æ„Ÿã‚’æ´ã‚€ï¼ˆä¼¼ãŸç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã®è·é›¢ã‚’è¿‘ã¥ã‘ã‚‹ï¼‰

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œãƒ‡ãƒ¼ã‚¿ãªãã—ã¦å­¦ç¿’ãªã—ã€ã¯å½“ãŸã‚Šå‰ã ã€‚ã ãŒæœ¬å½“ã®å•ã„ã¯ â€” ãƒ‡ãƒ¼ã‚¿ã®**è³ª**ã‚’ã©ã†å®šç¾©ã™ã‚‹ã‹ï¼Ÿ**

å¾“æ¥ã®æ©Ÿæ¢°å­¦ç¿’ã¯ã€Œãƒ‡ãƒ¼ã‚¿ã¯ä¸ãˆã‚‰ã‚Œã‚‹ã‚‚ã®ã€ã¨ã—ã¦æ‰±ã£ã¦ããŸã€‚ã ãŒ2025å¹´ã€åˆæˆãƒ‡ãƒ¼ã‚¿ãƒ»LLMã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»Active Learningã®æ™‚ä»£ã«ã€ãƒ‡ãƒ¼ã‚¿ã¯ã€Œä½œã‚‹ã‚‚ã®ã€ã«ãªã£ãŸã€‚

**å•ã„**:

1. **åˆæˆãƒ‡ãƒ¼ã‚¿ã¯ã€Œæœ¬ç‰©ã€ã‹ï¼Ÿ** Stable Diffusionã§ç”Ÿæˆã—ãŸç”»åƒã§ImageNetç²¾åº¦95%é”æˆã§ãã‚‹ãªã‚‰ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦ãªã®ã‹ï¼Ÿ
2. **LLMã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯äººé–“ã‚’è¶…ãˆã‚‹ã‹ï¼Ÿ** GPT-4ã®ç²¾åº¦ãŒäººé–“ã®93% vs 95%ãªã‚‰ã€ã‚³ã‚¹ãƒˆ1/100ã§ååˆ†ã§ã¯ï¼Ÿ
3. **ãƒ‡ãƒ¼ã‚¿å“è³ªã®é™ç•Œåç›Šã¯ï¼Ÿ** ãƒ‡ãƒ¼ã‚¿ã‚’ã©ã“ã¾ã§ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚Œã°ã€Œååˆ†ã€ã‹ï¼Ÿéå‰°ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¯éå­¦ç¿’ã‚’æ‹›ãã®ã§ã¯ï¼Ÿ

**è­°è«–ã®è¦–ç‚¹**:

- **åˆæˆãƒ‡ãƒ¼ã‚¿ã®å±é™ºæ€§**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚¢ã‚¹ãŒãƒ‡ãƒ¼ã‚¿ã«æ··å…¥ â†’ ãƒ¢ãƒ‡ãƒ«ãŒç¾å®Ÿã‚’åæ˜ ã—ãªããªã‚‹
- **Human-in-the-loop**: å®Œå…¨è‡ªå‹•åŒ–ã¯ä¸å¯èƒ½ã€äººé–“ã®åˆ¤æ–­ãŒæœ€çµ‚é˜²è¡›ç·š
- **åˆ†å¸ƒã‚·ãƒ•ãƒˆ**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆåˆ†å¸ƒãŒä¹–é›¢ã™ã‚‹å•é¡Œã¯ã€ã©ã‚“ãªã«å“è³ªã‚’ä¸Šã’ã¦ã‚‚è§£æ±ºã—ãªã„

**æ­´å²çš„æ–‡è„ˆ**:

- **1950å¹´ä»£**: ãƒ‡ãƒ¼ã‚¿ã¯æ‰‹ä½œæ¥­ã§åé›†ï¼ˆæ•°ç™¾ã‚µãƒ³ãƒ—ãƒ«ï¼‰
- **1990å¹´ä»£**: ImageNetç™»å ´ï¼ˆ1400ä¸‡æšã®æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
- **2020å¹´ä»£**: LAION-5Bï¼ˆ50å„„æšã®è‡ªå‹•åé›†ï¼‰
- **2025å¹´**: åˆæˆãƒ‡ãƒ¼ã‚¿ãŒä¸»æµã«ãªã‚‹ï¼Ÿ

:::details æ­´å²çš„è¦³ç‚¹: ãƒ‡ãƒ¼ã‚¿åé›†ã®é€²åŒ–

| æ™‚ä»£ | ãƒ‡ãƒ¼ã‚¿è¦æ¨¡ | åé›†æ–¹æ³• | ã‚³ã‚¹ãƒˆ | å“è³ª |
|:-----|:----------|:--------|:-------|:-----|
| 1950-1980 | æ•°ç™¾ã€œæ•°åƒ | æ‰‹å‹•å…¥åŠ› | æ¥µé«˜ | æ¥µé«˜ |
| 1980-2000 | æ•°ä¸‡ã€œæ•°åä¸‡ | æ‰‹å‹•+ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° | é«˜ | é«˜ |
| 2000-2020 | æ•°ç™¾ä¸‡ã€œæ•°åå„„ | ã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚·ãƒ³ã‚° | ä¸­ | ä¸­ |
| 2020-2025 | æ•°åå„„ã€œæ•°å…† | è‡ªå‹•åé›†+LLMãƒ•ã‚£ãƒ«ã‚¿ | ä½ | ä¸­ã€œä½ |
| **2025-** | **ç„¡é™ï¼ˆåˆæˆï¼‰** | **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«** | **æ¥µä½** | **ï¼Ÿ** |

åˆæˆãƒ‡ãƒ¼ã‚¿ã®ã€Œå“è³ªã€ã‚’ã©ã†å®šç¾©ã™ã‚‹ã‹ãŒã€æ¬¡ä¸–ä»£AIã®éµã ã€‚
:::

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ ç¬¬21å›ã€Œãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ & HuggingFace Datasetsã€å®Œèµ°ï¼ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å…¨ã‚µã‚¤ã‚¯ãƒ«ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚æ¬¡å›ã¯ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆã¸ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Lhoest, Q., et al. (2021). "Datasets: A Community Library for Natural Language Processing". *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, 175-184.
@[card](https://github.com/huggingface/datasets)

[^2]: Apache Arrow Development Team. (2024). "Apache Arrow: A Cross-Language Development Platform for In-Memory Data".
@[card](https://arrow.apache.org/)

[^3]: Bouchet-Valat, M., et al. (2024). "DataFrames.jl: Flexible and Fast Tabular Data in Julia". *Journal of Statistical Software*, 107(4), 1-32.
@[card](https://dataframes.juliadata.org/stable/)

[^4]: Ng, A. (2021). "A Chat with Andrew on MLOps: From Model-centric to Data-centric AI". *DeepLearning.AI Blog*.
@[card](https://www.deeplearning.ai/the-batch/issue-80/)

[^5]: Cui, Y., Jia, M., Lin, T.-Y., Song, Y., & Belongie, S. (2019). "Class-Balanced Loss Based on Effective Number of Samples". *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 9268-9277.
@[card](https://arxiv.org/abs/1901.05555)

[^6]: Lin, T.-Y., Goyal, P., Girshick, R., He, K., & DollÃ¡r, P. (2017). "Focal Loss for Dense Object Detection". *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*, 2980-2988.
@[card](https://arxiv.org/abs/1708.02002)

[^7]: Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). "SMOTE: Synthetic Minority Over-sampling Technique". *Journal of Artificial Intelligence Research*, 16, 321-357.
@[card](https://jair.org/index.php/jair/article/view/10302)

[^8]: Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., & Le, Q. V. (2019). "AutoAugment: Learning Augmentation Strategies from Data". *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 113-123.
@[card](https://arxiv.org/abs/1805.09501)

[^9]: Cubuk, E. D., Zoph, B., Shlens, J., & Le, Q. V. (2020). "RandAugment: Practical Automated Data Augmentation with a Reduced Search Space". *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*, 702-703.
@[card](https://arxiv.org/abs/1909.13719)

[^10]: Stocker, C. (2017). "Augmentor.jl: A Julia Package for Image Augmentation". *GitHub*.
@[card](https://github.com/Evizero/Augmentor.jl)

[^11]: MÃ¼ller, S. G., & Hutter, F. (2021). "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation". *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, 774-782.
@[card](https://arxiv.org/abs/2103.10158)

[^12]: Li, Y., Hu, G., Wang, Y., Hospedales, T., Robertson, N. M., & Yang, Y. (2020). "DADA: Differentiable Automatic Data Augmentation". *ECCV 2020*.
@[card](https://arxiv.org/abs/2003.03780)

[^13]: Kuprieiev, R., et al. (2024). "DVC: Data Version Control - Git for Data & Models".
@[card](https://dvc.org/)

### æ•™ç§‘æ›¸

- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [https://probml.github.io/pml-book/](https://probml.github.io/pml-book/)
- GÃ©ron, A. (2022). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* (3rd ed.). O'Reilly Media.
- Bezanson, J., Edelman, A., Karpinski, S., & Shah, V. B. (2017). "Julia: A Fresh Approach to Numerical Computing". *SIAM Review*, 59(1), 65-98. [https://julialang.org/research/](https://julialang.org/research/)

---

## è¨˜æ³•è¦ç´„

æœ¬è¬›ç¾©ã§ä½¿ç”¨ã—ãŸæ•°å­¦è¨˜å·ã®ä¸€è¦§ã€‚

| è¨˜å· | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $\mathbf{x}$ | ãƒ‡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ« | $\mathbf{x} \in \mathbb{R}^d$ |
| $\mu$ | å¹³å‡ | $\mu = \frac{1}{n}\sum_{i=1}^n x_i$ |
| $\sigma$ | æ¨™æº–åå·® | $\sigma = \sqrt{\text{Var}[x]}$ |
| $z$ | æ¨™æº–åŒ–å¤‰æ•° | $z = \frac{x - \mu}{\sigma}$ |
| $y$ | ãƒ©ãƒ™ãƒ« | $y \in \{0, 1, \ldots, K-1\}$ |
| $\mathbf{e}_y$ | One-hotãƒ™ã‚¯ãƒˆãƒ« | $\mathbf{e}_y \in \mathbb{R}^K$ |
| $p_t$ | æ­£è§£ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç¢ºç‡ | $p_t = p_\theta(y \mid \mathbf{x})$ |
| $\gamma$ | Focal Loss focusing parameter | é€šå¸¸ $\gamma = 2$ |
| $\alpha$ | ã‚¯ãƒ©ã‚¹é‡ã¿ | $\alpha_k = \frac{1 - \beta}{1 - \beta^{N_k}}$ |
| $\lambda$ | SMOTEè£œé–“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\lambda \sim \text{Uniform}(0, 1)$ |
| $N_k$ | ã‚¯ãƒ©ã‚¹ $k$ ã®ã‚µãƒ³ãƒ—ãƒ«æ•° | $N = \sum_{k=0}^{K-1} N_k$ |
| $\rho$ | ä¸å‡è¡¡æ¯” | $\rho = \frac{\max_k N_k}{\min_k N_k}$ |
| $\mathcal{L}$ | æå¤±é–¢æ•° | $\mathcal{L}_{\text{CE}}, \mathcal{L}_{\text{FL}}$ |
| $\theta$ | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\theta \in \mathbb{R}^p$ |
| $\mathbb{E}[\cdot]$ | æœŸå¾…å€¤ | $\mathbb{E}_{x \sim p}[f(x)]$ |
| $\text{KL}[p \| q]$ | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | åˆ†å¸ƒé–“ã®è·é›¢ |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚