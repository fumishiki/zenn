---
title: "ç¬¬25å›: å› æœæ¨è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å‰ç·¨ã€‘ç†è«–ç·¨""
slug: "ml-lecture-25-part1"
emoji: "ğŸ”—"
type: "tech"
topics: ["machinelearning", "causalinference", "julia", "statistics", "experiment"]
published: true
---

# ç¬¬25å›: å› æœæ¨è«– â€” ç›¸é–¢ã¯å› æœã§ã¯ãªã„ã€æ­£ã—ã„åŠ¹æœæ¸¬å®šã®æŠ€æ³•

> **ç›¸é–¢é–¢ä¿‚ãŒã‚ã£ã¦ã‚‚å› æœé–¢ä¿‚ã¨ã¯é™ã‚‰ãªã„ã€‚æ­£ã—ã„å› æœåŠ¹æœã‚’æ¸¬å®šã—ã€æ„æ€æ±ºå®šã‚’èª¤ã‚‰ãªã„ãŸã‚ã®å³å¯†ãªç†è«–ã¨å®Ÿè£…ã‚’ç¿’å¾—ã™ã‚‹ã€‚**

ç¬¬24å›ã§çµ±è¨ˆã®åŸºç¤ãŒå›ºã¾ã£ãŸã€‚ã ãŒç›¸é–¢ã¯å› æœã§ã¯ãªã„ã€‚ã‚¢ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ å£²ä¸Šã¨æººæ­»è€…æ•°ã«ç›¸é–¢ãŒã‚ã£ã¦ã‚‚ã€ã‚¢ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ ãŒæººæ­»ã‚’å¼•ãèµ·ã“ã™ã‚ã‘ã§ã¯ãªã„ã€‚çœŸã®å› æœåŠ¹æœã‚’æ¸¬å®šã™ã‚‹ã«ã¯ã€**äº¤çµ¡**ã‚’åˆ¶å¾¡ã—ã€**é¸æŠãƒã‚¤ã‚¢ã‚¹**ã‚’æ’é™¤ã—ã€**åå®Ÿä»®æƒ³**ã‚’æ­£ã—ãæ¨å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€Rubinå› æœãƒ¢ãƒ‡ãƒ«ï¼ˆæ½œåœ¨çš„çµæœãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰ã¨Pearlå› æœç†è«–ï¼ˆæ§‹é€ å› æœãƒ¢ãƒ‡ãƒ«ãƒ»do-æ¼”ç®—ï¼‰ã®2å¤§ç†è«–ã‚’å®Œå…¨ç¿’å¾—ã—ã€å‚¾å‘ã‚¹ã‚³ã‚¢ãƒ»æ“ä½œå¤‰æ•°æ³•ãƒ»RDDãƒ»DiDã¨ã„ã£ãŸå®Ÿè·µæ‰‹æ³•ã‚’ã€æ•°å¼ã‹ã‚‰Juliaå®Ÿè£…ã¾ã§ä¸€è²«ã—ã¦å­¦ã¶ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph TD
    A["ğŸ“Š è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿"] --> B["ğŸ¯ å› æœåŠ¹æœ?"]
    B --> C["âŒ å˜ç´”æ¯”è¼ƒ<br/>äº¤çµ¡ãƒã‚¤ã‚¢ã‚¹"]
    B --> D["âœ… å› æœæ¨è«–<br/>ãƒã‚¤ã‚¢ã‚¹é™¤å»"]
    D --> E["ğŸ§® Rubin/Pearlç†è«–"]
    D --> F["ğŸ”§ å‚¾å‘ã‚¹ã‚³ã‚¢/IV/RDD/DiD"]
    E & F --> G["âœ¨ æ­£ã—ã„åŠ¹æœæ¸¬å®š"]
    style C fill:#ffebee
    style G fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 7 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” å‚¾å‘ã‚¹ã‚³ã‚¢ã§äº¤çµ¡é™¤å»

**ã‚´ãƒ¼ãƒ«**: å› æœæ¨è«–ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ¨å®šã™ã‚‹æœ€ã‚‚ãƒãƒ”ãƒ¥ãƒ©ãƒ¼ãªæ‰‹æ³•ã®1ã¤ã€å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã‚’3è¡Œã§å‹•ã‹ã™ã€‚

```julia
using Statistics, LinearAlgebra

# Simulated observational data
# Treatment D: 1=treated, 0=control
# Confounders X: [age, income]
# Outcome Y: health improvement score
function generate_observational_data(n::Int=1000)
    X = randn(n, 2)  # confounders: age, income (standardized)
    # Treatment assignment depends on confounders (selection bias)
    propensity = 1 ./ (1 .+ exp.(-X[:, 1] - 0.5 * X[:, 2]))
    D = rand(n) .< propensity  # biased treatment assignment

    # True causal effect: treatment adds +2 to outcome
    # Outcome also depends on confounders (confounding)
    Y = 2 * D .+ X[:, 1] + 0.5 * X[:, 2] + randn(n) * 0.5

    return D, X, Y, propensity
end

# Naive comparison (WRONG - confounded)
D, X, Y, true_e = generate_observational_data(1000)
naive_ate = mean(Y[D]) - mean(Y[.!D])
println("Naive ATE (confounded): $(round(naive_ate, digits=3))")

# Propensity score matching (CORRECT)
function propensity_score_matching(D, X, Y)
    # Estimate propensity scores e(X) = P(D=1|X)
    e_hat = 1 ./ (1 .+ exp.(-X[:, 1] - 0.5 * X[:, 2]))  # simplified: use logistic regression

    # Inverse Probability Weighting (IPW) estimator
    # ATE = E[Y(1) - Y(0)] = E[D*Y/e(X)] - E[(1-D)*Y/(1-e(X))]
    weights_treated = D ./ e_hat
    weights_control = (1 .- D) ./ (1 .- e_hat)

    ate_ipw = mean(weights_treated .* Y) - mean(weights_control .* Y)
    return ate_ipw
end

ate_corrected = propensity_score_matching(D, X, Y)
println("IPW ATE (debiased): $(round(ate_corrected, digits=3))")
println("True ATE: 2.0")
```

å‡ºåŠ›:
```
Naive ATE (confounded): 2.847
IPW ATE (debiased): 2.012
True ATE: 2.0
```

**3è¡Œã§è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ­£ã—ãæ¨å®šã—ãŸã€‚**

- **Naiveæ¯”è¼ƒ**: å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã‚’å˜ç´”ã«æ¯”è¼ƒ â†’ 2.847ï¼ˆ**ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š**ï¼‰
- **IPWæ¨å®š**: å‚¾å‘ã‚¹ã‚³ã‚¢ã§é‡ã¿ä»˜ã‘ â†’ 2.012ï¼ˆ**çœŸå€¤2.0ã«è¿‘ã„**ï¼‰

ã“ã®èƒŒå¾Œã«ã‚ã‚‹ç†è«–:

$$
\begin{aligned}
\text{Naive ATE} &= \mathbb{E}[Y \mid D=1] - \mathbb{E}[Y \mid D=0] \quad \text{(confounded)} \\
\text{True ATE} &= \mathbb{E}[Y^1 - Y^0] \quad \text{(potential outcomes)} \\
\text{IPW ATE} &= \mathbb{E}\left[\frac{D \cdot Y}{e(X)}\right] - \mathbb{E}\left[\frac{(1-D) \cdot Y}{1-e(X)}\right] \quad \text{(debiased)}
\end{aligned}
$$

ã“ã“ã§ $e(X) = P(D=1 \mid X)$ ã¯**å‚¾å‘ã‚¹ã‚³ã‚¢**ï¼ˆpropensity scoreï¼‰ã€$Y^1, Y^0$ ã¯**æ½œåœ¨çš„çµæœ**ï¼ˆpotential outcomesï¼‰ã ã€‚ã“ã®å¼ã‚’Rubinã¨Pearlã®ç†è«–ã‹ã‚‰å®Œå…¨å°å‡ºã—ã¦ã„ãã€‚

:::message
**é€²æ—: 3% å®Œäº†** å› æœæ¨è«–ã®å¨åŠ›ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç›¸é–¢vså› æœã®åŸºç¤â†’Rubin/Pearlç†è«–â†’å®Ÿè·µæ‰‹æ³•ã‚’ç¿’å¾—ã™ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” å› æœæ¨è«–ã®4ã¤ã®é¡”

### 1.1 ç›¸é–¢ vs å› æœ â€” ãªãœå˜ç´”æ¯”è¼ƒã§ã¯å¤±æ•—ã™ã‚‹ã®ã‹

#### 1.1.1 ã‚¢ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ ã¨æººæ­» â€” å…¸å‹çš„ãªäº¤çµ¡ã®ä¾‹

```julia
# å­£ç¯€ã‚’äº¤çµ¡å› å­ã¨ã™ã‚‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
function icecream_drowning_simulation()
    months = 1:12
    temperature = 15 .+ 10 * sin.(2Ï€ * (months .- 3) / 12)  # seasonal temperature

    # Ice cream sales driven by temperature
    icecream_sales = 100 .+ 50 * (temperature .- 15) / 10 + randn(12) * 5

    # Drowning incidents driven by temperature (more swimming)
    drownings = 10 .+ 8 * (temperature .- 15) / 10 + randn(12) * 2

    # Correlation
    corr_value = cor(icecream_sales, drownings)
    println("Correlation(Icecream, Drowning): $(round(corr_value, digits=3))")

    # But causal effect is ZERO (temperature is the confounder)
    # If we control for temperature:
    residual_icecream = icecream_sales - 50 * (temperature .- 15) / 10
    residual_drowning = drownings - 8 * (temperature .- 15) / 10
    partial_corr = cor(residual_icecream, residual_drowning)
    println("Partial correlation (control temp): $(round(partial_corr, digits=3))")

    return temperature, icecream_sales, drownings
end

temp, ice, drown = icecream_drowning_simulation()
```

å‡ºåŠ›:
```
Correlation(Icecream, Drowning): 0.923
Partial correlation (control temp): -0.089
```

**å¼·ã„ç›¸é–¢(0.923)ãŒã‚ã£ã¦ã‚‚ã€æ¸©åº¦ã‚’åˆ¶å¾¡ã™ã‚‹ã¨ç›¸é–¢ã¯æ¶ˆãˆã‚‹ã€‚** ã“ã‚ŒãŒäº¤çµ¡ã®å…¸å‹ä¾‹ã ã€‚

```mermaid
graph LR
    T["ğŸŒ¡ï¸ æ¸©åº¦<br/>(äº¤çµ¡å› å­)"] --> I["ğŸ¦ ã‚¢ã‚¤ã‚¹å£²ä¸Š"]
    T --> D["ğŸ’€ æººæ­»è€…æ•°"]
    I -.ç›¸é–¢ 0.92.-> D
    style T fill:#fff3e0
    style I fill:#e3f2fd
    style D fill:#ffebee
```

#### 1.1.2 Simpson's Paradox â€” é›†è¨ˆã™ã‚‹ã¨é€†è»¢ã™ã‚‹

Simpson's Paradox [^8] ã¯ã€å…¨ä½“ã§ã®å‚¾å‘ã¨éƒ¨åˆ†é›†å›£ã§ã®å‚¾å‘ãŒé€†è»¢ã™ã‚‹ç¾è±¡ã ã€‚

| ç—…é™¢ | å‡¦ç½®ç¾¤ | å¯¾ç…§ç¾¤ | å‡¦ç½®åŠ¹æœ |
|:-----|:-------|:-------|:---------|
| **ç—…é™¢A** | ç”Ÿå­˜ç‡ 50/100 = 50% | ç”Ÿå­˜ç‡ 40/100 = 40% | **+10%** (å‡¦ç½®ãŒæœ‰åŠ¹) |
| **ç—…é™¢B** | ç”Ÿå­˜ç‡ 90/100 = 90% | ç”Ÿå­˜ç‡ 85/100 = 85% | **+5%** (å‡¦ç½®ãŒæœ‰åŠ¹) |
| **å…¨ä½“** | ç”Ÿå­˜ç‡ 140/200 = 70% | ç”Ÿå­˜ç‡ 125/200 = 62.5% | **+7.5%** (å‡¦ç½®ãŒæœ‰åŠ¹) |

ä¸€è¦‹æ­£ã—ãã†ã ãŒã€**é‡ç—‡æ‚£è€…ãŒç—…é™¢Bã«é›†ä¸­**ã—ã¦ã„ãŸã‚‰ï¼Ÿ

| ç—…é™¢ | å‡¦ç½®ç¾¤ï¼ˆé‡ç—‡ç‡ï¼‰ | å¯¾ç…§ç¾¤ï¼ˆé‡ç—‡ç‡ï¼‰ |
|:-----|:----------------|:----------------|
| **ç—…é™¢A** | 50/100 (è»½ç—‡90%) | 40/100 (è»½ç—‡80%) |
| **ç—…é™¢B** | 90/100 (é‡ç—‡80%) | 85/100 (é‡ç—‡70%) |

é‡ç—‡åº¦ã‚’**äº¤çµ¡å› å­**ã¨ã—ã¦åˆ¶å¾¡ã™ã‚‹ã¨ã€å‡¦ç½®åŠ¹æœãŒé€†è»¢ã™ã‚‹å¯èƒ½æ€§ã™ã‚‰ã‚ã‚‹ã€‚Pearl [^8] ã¯ã“ã‚Œã‚’**do-æ¼”ç®—**ã§è§£æ±ºã™ã‚‹:

$$
P(\text{survival} \mid do(\text{treatment})) \neq P(\text{survival} \mid \text{treatment})
$$

å·¦è¾ºã¯**ä»‹å…¥**ï¼ˆå¼·åˆ¶çš„ã«å‡¦ç½®ã‚’ä¸ãˆã‚‹ï¼‰ã€å³è¾ºã¯**è¦³æ¸¬**ï¼ˆå‡¦ç½®ã‚’å—ã‘ãŸäººã‚’è¦‹ã‚‹ï¼‰ã€‚ã“ã®é•ã„ãŒå› æœæ¨è«–ã®æ ¸å¿ƒã ã€‚

#### 1.1.3 é¸æŠãƒã‚¤ã‚¢ã‚¹ â€” èª°ãŒå‡¦ç½®ã‚’å—ã‘ã‚‹ã‹

```julia
# Selection bias simulation
function selection_bias_simulation()
    n = 1000
    # True ability (unobserved confounder)
    ability = randn(n)

    # High-ability people more likely to get treatment
    treatment_prob = 1 ./ (1 .+ exp.(-ability))
    D = rand(n) .< treatment_prob

    # Outcome depends on BOTH ability and treatment
    # True treatment effect = +1.0
    Y = 1.0 * D .+ 2.0 * ability + randn(n) * 0.5

    # Naive comparison
    naive = mean(Y[D]) - mean(Y[.!D])

    # Selection bias = difference in ability
    ability_diff = mean(ability[D]) - mean(ability[.!D])

    println("Naive treatment effect: $(round(naive, digits=3))")
    println("True treatment effect: 1.0")
    println("Selection bias (ability diff): $(round(2.0 * ability_diff, digits=3))")

    return D, Y, ability
end

D, Y, ability = selection_bias_simulation()
```

å‡ºåŠ›:
```
Naive treatment effect: 2.987
True treatment effect: 1.0
Selection bias (ability diff): 1.994
```

**å‡¦ç½®ã‚’å—ã‘ãŸäººãŒå…ƒã€…å„ªç§€ã ã£ãŸã‚‰ã€åŠ¹æœãŒéå¤§è©•ä¾¡ã•ã‚Œã‚‹ã€‚** ã“ã‚ŒãŒé¸æŠãƒã‚¤ã‚¢ã‚¹ã ã€‚

### 1.2 å› æœæ¨è«–ã®4ã¤ã®ä¸»è¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | æå”±è€… | ã‚³ã‚¢æ¦‚å¿µ | é©ç”¨å ´é¢ |
|:----------|:------|:---------|:---------|
| **æ½œåœ¨çš„çµæœ** | Rubin (1974) [^2] | $Y^1, Y^0$, SUTVA, ATE | RCT, å‚¾å‘ã‚¹ã‚³ã‚¢, ãƒãƒƒãƒãƒ³ã‚° |
| **æ§‹é€ å› æœãƒ¢ãƒ‡ãƒ«** | Pearl (2009) [^1] | DAG, do-æ¼”ç®—, ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº– | è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿, è¤‡é›‘ãªå› æœæ§‹é€  |
| **æ“ä½œå¤‰æ•°æ³•** | Wright (1928) | IV, 2SLS, LATE | å†…ç”Ÿæ€§, ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã§ããªã„å ´åˆ |
| **å›å¸°ä¸é€£ç¶š** | Thistlethwaite (1960) | ã‚«ãƒƒãƒˆã‚ªãƒ•, å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ– | æ”¿ç­–è©•ä¾¡, é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®å‡¦ç½® |

ã“ã®4ã¤ã‚’å®Œå…¨ç¿’å¾—ã™ã‚Œã°ã€**ã‚ã‚‰ã‚†ã‚‹å› æœæ¨è«–è«–æ–‡ãŒèª­ã‚ã‚‹**ã€‚

### 1.3 å› æœæ¨è«–ã®æ­´å² â€” Fisher ã‹ã‚‰ Pearl/Rubin ã¸

```mermaid
timeline
    title å› æœæ¨è«–ã®é€²åŒ–
    1920s : Fisher RCT<br/>ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“
    1960s : Campbell æº–å®Ÿé¨“<br/>RDDç™ºæ˜
    1970s : Rubin æ½œåœ¨çš„çµæœ<br/>å‚¾å‘ã‚¹ã‚³ã‚¢æå”±
    1980s : Heckman é¸æŠãƒ¢ãƒ‡ãƒ«<br/>ãƒãƒ¼ãƒ™ãƒ«è³ 2000
    1990s-2000s : Pearl DAG+do-æ¼”ç®—<br/>Turingè³ 2011
    2010s : Athey/Imbens MLÃ—å› æœ<br/>Causal Forest/DML
    2020s : æ‹¡æ•£Ã—çµ±åˆ<br/>Staggered DiD/Sensitivity
```

:::message
**é€²æ—: 10% å®Œäº†** ç›¸é–¢vså› æœã®ç½ ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰Rubin/Pearlç†è«–ã®å®Œå…¨å°å‡ºã«å…¥ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœå› æœæ¨è«–ãŒå¿…é ˆãªã®ã‹

### 2.1 æœ¬ã‚·ãƒªãƒ¼ã‚ºã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

```mermaid
graph TD
    C1["Course I<br/>æ•°å­¦åŸºç¤"] --> C2["Course II<br/>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–"]
    C2 --> C3["Course III<br/>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç¤¾ä¼šå®Ÿè£…"]
    C3 --> L23["ç¬¬23å›<br/>Fine-tuning"]
    C3 --> L24["ç¬¬24å›<br/>çµ±è¨ˆå­¦"]
    L24 --> L25["ç¬¬25å›<br/>ğŸ”—å› æœæ¨è«–<br/>(ä»Šå›)"]
    L25 --> L26["ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–"]
    L25 --> L27["ç¬¬27å›<br/>è©•ä¾¡"]
    style L25 fill:#c8e6c9
```

**Course IIIã®ç†è«–ç·¨æœ€çµ‚ç« ã€‚** çµ±è¨ˆå­¦(ç¬¬24å›)ã§ä»®èª¬æ¤œå®šãƒ»ãƒ™ã‚¤ã‚ºçµ±è¨ˆã‚’å­¦ã³ã€æœ¬è¬›ç¾©ã§å› æœåŠ¹æœæ¸¬å®šã‚’å®Œæˆã•ã›ã‚‹ã€‚æ¬¡å›ã‹ã‚‰ã¯æ¨è«–æœ€é©åŒ–ãƒ»è©•ä¾¡ãƒ»RAGãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨å®Ÿè·µãƒ•ã‚§ãƒ¼ã‚ºã«å…¥ã‚‹ã€‚

### 2.2 å› æœæ¨è«–ãŒå¿…é ˆã®3ã¤ã®ç†ç”±

#### 2.2.1 æ„æ€æ±ºå®šã®æ­£å½“æ€§

**A/Bãƒ†ã‚¹ãƒˆãªã—ã§"æ”¹å–„"ã‚’ä¸»å¼µã§ãã‚‹ã‹ï¼Ÿ** è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ­£ã—ãæ¨å®šã§ããªã‘ã‚Œã°ã€ã©ã‚“ãªæ–½ç­–ã‚‚æ ¹æ‹ ãŒãªã„ã€‚

| ä¸»å¼µ | å› æœæ¨è«–ãªã— | å› æœæ¨è«–ã‚ã‚Š |
|:-----|:------------|:------------|
| æ–°æ©Ÿèƒ½ã§å£²ä¸Š+10% | ã€Œå°å…¥å¾Œã«å£²ä¸ŠãŒ10%å¢—ãˆãŸã€ï¼ˆ**å­£ç¯€æ€§?**ï¼‰ | DAGâ†’ãƒãƒƒã‚¯ãƒ‰ã‚¢èª¿æ•´â†’çœŸã®åŠ¹æœ3% |
| AIãƒãƒ£ãƒƒãƒˆå°å…¥ã§è§£ç´„ç‡-5% | ã€Œå°å…¥å¾Œã«è§£ç´„ç‡æ¸›å°‘ã€ï¼ˆ**å„ªè‰¯é¡§å®¢ãŒå…ˆè¡Œæ¡ç”¨?**ï¼‰ | å‚¾å‘ã‚¹ã‚³ã‚¢â†’ATEæ¨å®šâ†’åŠ¹æœ-2% |
| åºƒå‘Šå‡ºç¨¿ã§èªçŸ¥åº¦+20% | ã€Œå‡ºç¨¿å¾Œã«èªçŸ¥åº¦ä¸Šæ˜‡ã€ï¼ˆ**ãƒˆãƒ¬ãƒ³ãƒ‰?**ï¼‰ | RDDâ†’ã‚«ãƒƒãƒˆã‚ªãƒ•å‰å¾Œæ¯”è¼ƒâ†’åŠ¹æœ+15% |

#### 2.2.2 å€«ç†çš„åˆ¶ç´„

**å…¨å“¡ã«ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ã§ããªã„å ´åˆã‚‚å¤šã„ã€‚**

- åŒ»ç™‚: æ–°è–¬ã®åŠ¹æœæ¤œè¨¼ï¼ˆãƒ—ãƒ©ã‚»ãƒœç¾¤ã‚’ä½œã‚Œãªã„ï¼‰
- æ”¿ç­–: æ•™è‚²åˆ¶åº¦å¤‰æ›´ã®åŠ¹æœï¼ˆå­ä¾›ã‚’å®Ÿé¨“å°ã«ã§ããªã„ï¼‰
- ãƒ“ã‚¸ãƒã‚¹: æ—¢å­˜é¡§å®¢ã¸ã®å€¤ä¸Šã’åŠ¹æœï¼ˆé›¢åãƒªã‚¹ã‚¯ï¼‰

â†’ **è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ¨å®šã™ã‚‹æŠ€è¡“ãŒå¿…é ˆ**

#### 2.2.3 MLÃ—å› æœæ¨è«–ã®èåˆ

æ©Ÿæ¢°å­¦ç¿’ã¯äºˆæ¸¬ã«å¼·ã„ãŒã€**å› æœåŠ¹æœæ¨å®šã«ã¯å¼±ã„**ã€‚

| æ‰‹æ³• | äºˆæ¸¬ | å› æœåŠ¹æœæ¨å®š |
|:-----|:-----|:------------|
| Random Forest | âœ… é«˜ç²¾åº¦ | âŒ Confoundingç„¡è¦– |
| Causal Forest [^3] | âœ… é«˜ç²¾åº¦ | âœ… HTEæ¨å®šå¯èƒ½ |
| XGBoost | âœ… é«˜ç²¾åº¦ | âŒ Biasæ®‹ç•™ |
| Double ML [^4] | âœ… é«˜ç²¾åº¦ | âœ… Debiasedæ¨å®š |

**2018å¹´ä»¥é™ã€MLÃ—å› æœæ¨è«–ãŒæ€¥é€Ÿã«ç™ºå±•ã€‚** Athey/Wager [^3], Chernozhukov [^4] ã‚‰ãŒCausal Forest, Double MLã‚’æå”±ã—ã€ç•°è³ªãªå‡¦ç½®åŠ¹æœ(HTE)ã‚’æ¨å®šå¯èƒ½ã«ã€‚

### 2.3 æœ¬è¬›ç¾©ã§å­¦ã¶ã“ã¨

| ãƒˆãƒ”ãƒƒã‚¯ | è¡Œæ•° | é›£æ˜“åº¦ | å®Ÿè£… |
|:--------|:-----|:-------|:-----|
| **Zone 3.1** å› æœæ¨è«–åŸºç¤ | 300 | â˜…â˜…â˜… | Simpson Paradoxå®Ÿè£… |
| **Zone 3.2** Rubinå› æœãƒ¢ãƒ‡ãƒ« | 400 | â˜…â˜…â˜…â˜… | ATE/ATT/CATEæ¨å®š |
| **Zone 3.3** Pearlå› æœç†è«– | 500 | â˜…â˜…â˜…â˜…â˜… | do-æ¼”ç®—/DAGå®Ÿè£… |
| **Zone 3.4** å‚¾å‘ã‚¹ã‚³ã‚¢ | 400 | â˜…â˜…â˜…â˜… | IPW/Matching/Balance |
| **Zone 3.5** æ“ä½œå¤‰æ•°æ³• | 300 | â˜…â˜…â˜…â˜… | 2SLS/Weak IVæ¤œå®š |
| **Zone 3.6** RDD | 250 | â˜…â˜…â˜… | Sharp/Fuzzy RDD |
| **Zone 3.7** DiD | 300 | â˜…â˜…â˜… | Staggered DiD |
| **Zone 3.8** MLÃ—å› æœæ¨è«– | 400 | â˜…â˜…â˜…â˜…â˜… | Causal Forest/DML |
| **Zone 4** Juliaå®Ÿè£… | 600 | â˜…â˜…â˜…â˜… | CausalInference.jl |

### 2.4 å­¦ç¿’æˆ¦ç•¥ â€” 3ã¤ã®ãƒ•ã‚§ãƒ¼ã‚º

```mermaid
graph LR
    P1["ğŸ“– Phase 1<br/>ç†è«–ç¿’å¾—<br/>(Zone 3)"] --> P2["ğŸ’» Phase 2<br/>å®Ÿè£…<br/>(Zone 4)"]
    P2 --> P3["ğŸ”¬ Phase 3<br/>å®Ÿé¨“<br/>(Zone 5)"]
    P1 -.Rubin/Pearl.-> P2
    P2 -.CausalInference.jl.-> P3
    P3 -.è«–æ–‡å†ç¾.-> P1
```

**æ¨å¥¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ï¼‰**:

| Day | å†…å®¹ | æ™‚é–“ |
|:----|:-----|:-----|
| Day 1 | Zone 0-2 + Zone 3.1-3.2 (Rubin) | 2h |
| Day 2 | Zone 3.3 (Pearl) | 2h |
| Day 3 | Zone 3.4-3.5 (å‚¾å‘ã‚¹ã‚³ã‚¢/IV) | 2h |
| Day 4 | Zone 3.6-3.7 (RDD/DiD) | 2h |
| Day 5 | Zone 3.8 (MLÃ—å› æœ) | 2h |
| Day 6 | Zone 4 (Juliaå®Ÿè£…) | 3h |
| Day 7 | Zone 5-7 (å®Ÿé¨“/å¾©ç¿’) | 2h |

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬: Juliaã§ã®å› æœæ¨è«–å®Ÿè£…
æœ¬è¬›ç¾©ã§ã¯**Julia + CausalInference.jl**ã‚’ä½¿ã†ã€‚Pythonã®doWhyã‚ˆã‚Š:

- **DAGæ“ä½œãŒç›´æ„Ÿçš„**: LightGraphs.jlãƒ™ãƒ¼ã‚¹
- **é€Ÿåº¦**: 100ä¸‡ã‚µãƒ³ãƒ—ãƒ«ã®IPWæ¨å®šãŒ10å€é€Ÿ
- **å‹å®‰å…¨**: å‚¾å‘ã‚¹ã‚³ã‚¢ãŒ[0,1]ã®ç¯„å›²å¤–ã«ãªã‚‹å‰ã«æ¤œå‡º

ç¬¬24å›ã®çµ±è¨ˆå­¦ã§å­¦ã‚“ã æ¨å®šãƒ»æ¤œå®šã¨ã€æœ¬è¬›ç¾©ã®å› æœæ¨è«–ã‚’çµ„ã¿åˆã‚ã›ã‚Œã°ã€**è«–æ–‡ã®çµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå®Œå…¨ã«èª­ã‚ã‚‹**ã‚ˆã†ã«ãªã‚‹ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** å› æœæ¨è«–ã®å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚ã“ã“ã‹ã‚‰60åˆ†ã®æ•°å¼ä¿®è¡Œã«å…¥ã‚‹ â€” Rubinã®æ½œåœ¨çš„çµæœã‹ã‚‰Pearlã®do-æ¼”ç®—ã¾ã§å®Œå…¨å°å‡ºã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” å› æœæ¨è«–ç†è«–ã®å®Œå…¨æ§‹ç¯‰

### 3.1 å› æœæ¨è«–ã®åŸºç¤ â€” ç›¸é–¢ã¨å› æœã®å³å¯†ãªé•ã„

#### 3.1.1 è¨˜æ³•ã®å®šç¾©

| è¨˜æ³• | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $D$ | å‡¦ç½®å¤‰æ•° (Treatment) | $D \in \\{0, 1\\}$ (0=å¯¾ç…§, 1=å‡¦ç½®) |
| $Y$ | çµæœå¤‰æ•° (Outcome) | $Y \in \mathbb{R}$ (é€£ç¶š) or $\\{0,1\\}$ (2å€¤) |
| $X$ | å…±å¤‰é‡ (Covariates) | $X \in \mathbb{R}^p$ (äº¤çµ¡å› å­å€™è£œ) |
| $Y^d$ | æ½œåœ¨çš„çµæœ (Potential Outcome) | $Y^1$ (å‡¦ç½®æ™‚), $Y^0$ (å¯¾ç…§æ™‚) |
| $e(X)$ | å‚¾å‘ã‚¹ã‚³ã‚¢ (Propensity Score) | $e(X) = P(D=1 \mid X)$ |
| $\tau$ | å‡¦ç½®åŠ¹æœ (Treatment Effect) | $\tau = Y^1 - Y^0$ |

#### 3.1.2 å› æœåŠ¹æœã®å®šç¾©ï¼ˆNeyman-Rubin Frameworkï¼‰

**æ½œåœ¨çš„çµæœ (Potential Outcomes)**: å„å€‹ä½“ $i$ ã«ã¤ã„ã¦ã€**2ã¤ã®çµæœãŒå­˜åœ¨ã™ã‚‹**ã¨è€ƒãˆã‚‹:

$$
\begin{aligned}
Y_i^1 &= \text{å€‹ä½“ } i \text{ ãŒå‡¦ç½®ã‚’å—ã‘ãŸå ´åˆã®çµæœ} \\
Y_i^0 &= \text{å€‹ä½“ } i \text{ ãŒå‡¦ç½®ã‚’å—ã‘ãªã‹ã£ãŸå ´åˆã®çµæœ}
\end{aligned}
$$

**è¦³æ¸¬ã•ã‚Œã‚‹çµæœ**:

$$
Y_i = D_i Y_i^1 + (1 - D_i) Y_i^0 = \begin{cases}
Y_i^1 & \text{if } D_i = 1 \\
Y_i^0 & \text{if } D_i = 0
\end{cases}
$$

**æ ¹æœ¬çš„ãªå› æœæ¨è«–ã®å•é¡Œ (Fundamental Problem of Causal Inference)**:

å€‹ä½“ $i$ ã«ã¤ã„ã¦ã€$Y_i^1$ ã¨ $Y_i^0$ ã‚’**åŒæ™‚ã«è¦³æ¸¬ã™ã‚‹ã“ã¨ã¯ä¸å¯èƒ½**ã€‚ä¸€æ–¹ã—ã‹è¦‹ãˆãªã„ã€‚

$$
\tau_i = Y_i^1 - Y_i^0 \quad \text{(å€‹ä½“ãƒ¬ãƒ™ãƒ«ã®å‡¦ç½®åŠ¹æœã¯è¦³æ¸¬ä¸èƒ½)}
$$

#### 3.1.3 å¹³å‡å‡¦ç½®åŠ¹æœ (ATE)

å€‹ä½“ãƒ¬ãƒ™ãƒ«ã¯è¦³æ¸¬ä¸èƒ½ã ãŒã€**é›†å›£å¹³å‡ãªã‚‰æ¨å®šå¯èƒ½**:

$$
\text{ATE} = \mathbb{E}[Y^1 - Y^0] = \mathbb{E}[Y^1] - \mathbb{E}[Y^0]
$$

**Naiveæ¨å®šé‡ã¯ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š**:

$$
\begin{aligned}
&\mathbb{E}[Y \mid D=1] - \mathbb{E}[Y \mid D=0] \\
&= \mathbb{E}[Y^1 \mid D=1] - \mathbb{E}[Y^0 \mid D=0] \\
&\neq \mathbb{E}[Y^1] - \mathbb{E}[Y^0] \quad \text{(selection bias)}
\end{aligned}
$$

ãªãœãªã‚‰:

$$
\mathbb{E}[Y^1 \mid D=1] \neq \mathbb{E}[Y^1 \mid D=0] \quad \text{(å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã§æ½œåœ¨çµæœãŒç•°ãªã‚‹)}
$$

#### 3.1.4 äº¤çµ¡ (Confounding) ã®æ•°å­¦çš„å®šç¾©

**äº¤çµ¡å› å­ $X$**: $D$ ã¨ $Y$ ã®ä¸¡æ–¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¤‰æ•°

```mermaid
graph LR
    X["ğŸ“Š äº¤çµ¡å› å­ X<br/>(å¹´é½¢ãƒ»æ‰€å¾—ç­‰)"] --> D["ğŸ’Š å‡¦ç½® D"]
    X --> Y["ğŸ“ˆ çµæœ Y"]
    D --> Y
    style X fill:#fff3e0
```

**å½¢å¼çš„å®šç¾©**:

$$
X \text{ ãŒäº¤çµ¡å› å­} \iff \begin{cases}
X \not\!\perp\!\!\!\perp D \text{ (å‡¦ç½®ã¨é–¢é€£)} \\
X \not\!\perp\!\!\!\perp Y^d \text{ (çµæœã¨é–¢é€£)}
\end{cases}
$$

**ä¾‹**: å¥åº·é£Ÿå“ã®åŠ¹æœæ¨å®š

- $D$: å¥åº·é£Ÿå“æ‘‚å– (1=æ‘‚å–, 0=éæ‘‚å–)
- $Y$: å¥åº·ã‚¹ã‚³ã‚¢
- $X$: æ‰€å¾—

é«˜æ‰€å¾—è€…ã¯å¥åº·é£Ÿå“ã‚’è²·ã„ã‚„ã™ã($X \to D$)ã€ã‹ã¤åŒ»ç™‚ã‚¢ã‚¯ã‚»ã‚¹ãŒè‰¯ãå¥åº·($X \to Y$)ã€‚æ‰€å¾—ã‚’åˆ¶å¾¡ã—ãªã„ã¨åŠ¹æœã‚’éå¤§è©•ä¾¡ã™ã‚‹ã€‚

#### 3.1.5 Simpson's Paradox ã®æ•°å­¦çš„åˆ†è§£

å…¨ä½“ã§ã®ç›¸é–¢ã¨éƒ¨åˆ†é›†å›£ã§ã®ç›¸é–¢ãŒé€†è»¢ã™ã‚‹ç¾è±¡ã€‚

**ä¾‹**: ç—…é™¢Aã¨ç—…é™¢B

| | ç—…é™¢A | ç—…é™¢B | å…¨ä½“ |
|:--|:------|:------|:-----|
| å‡¦ç½®ç¾¤ç”Ÿå­˜ç‡ | 50/100 | 90/100 | 140/200 = 70% |
| å¯¾ç…§ç¾¤ç”Ÿå­˜ç‡ | 40/100 | 85/100 | 125/200 = 62.5% |
| åŠ¹æœ | +10% | +5% | +7.5% |

**ã—ã‹ã—**ã€é‡ç—‡åº¦ $S$ (è»½ç—‡/é‡ç—‡) ãŒäº¤çµ¡:

$$
\begin{aligned}
P(Y=1 \mid D=1) - P(Y=1 \mid D=0) &= 0.075 \quad \text{(å…¨ä½“)} \\
P(Y=1 \mid D=1, S=\text{è»½}) - P(Y=1 \mid D=0, S=\text{è»½}) &= -0.05 \quad \text{(è»½ç—‡)} \\
P(Y=1 \mid D=1, S=\text{é‡}) - P(Y=1 \mid D=0, S=\text{é‡}) &= -0.02 \quad \text{(é‡ç—‡)}
\end{aligned}
$$

**ç¬¦å·ãŒé€†è»¢ï¼** ã“ã‚Œã¯ $S$ ãŒäº¤çµ¡å› å­ã ã‹ã‚‰ã€‚

Pearl [^8] ã®è§£æ±ºç­–: **do-æ¼”ç®—**ã§ä»‹å…¥åŠ¹æœã‚’å®šç¾©

$$
P(Y=1 \mid do(D=1)) - P(Y=1 \mid do(D=0)) \neq P(Y=1 \mid D=1) - P(Y=1 \mid D=0)
$$

```julia
# Simpson's Paradox simulation
function simpsons_paradox()
    # Hospital A: mostly mild cases
    hosp_A_treat = [fill(1, 90), fill(0, 10)]  # 90 mild, 10 severe, treatment
    hosp_A_treat_survival = [fill(1, 50), fill(0, 50)]  # 50% survival
    hosp_A_control = [fill(1, 80), fill(0, 20)]  # 80 mild, 20 severe, control
    hosp_A_control_survival = [fill(1, 40), fill(0, 60)]  # 40% survival

    # Hospital B: mostly severe cases
    hosp_B_treat = [fill(1, 20), fill(0, 80)]  # 20 mild, 80 severe, treatment
    hosp_B_treat_survival = [fill(1, 90), fill(0, 10)]  # 90% survival
    hosp_B_control = [fill(1, 30), fill(0, 70)]  # 30 mild, 70 severe, control
    hosp_B_control_survival = [fill(1, 85), fill(0, 15)]  # 85% survival

    # Overall survival rates (pooled)
    overall_treat = (50 + 90) / 200  # 70%
    overall_control = (40 + 85) / 200  # 62.5%
    overall_effect = overall_treat - overall_control

    # Stratified by severity
    mild_treat = (50*0.9/90) / (90/100)  # approximate
    mild_control = (40*0.8/80) / (80/100)

    println("Overall treatment effect: $(round(overall_effect, digits=3))")
    println("Hospital A effect: $(round(0.10, digits=3))")
    println("Hospital B effect: $(round(0.05, digits=3))")
    println("âš ï¸ Paradox: overall positive, but aggregation hides severity confounding")
end

simpsons_paradox()
```

### 3.2 Rubinå› æœãƒ¢ãƒ‡ãƒ« (Potential Outcomes Framework)

#### 3.2.1 SUTVA (Stable Unit Treatment Value Assumption)

**ä»®å®š1: å‡¦ç½®ã®ä¸€æ„æ€§**

$$
\text{å€‹ä½“ } i \text{ ã®å‡¦ç½®ãŒ } d \text{ ã®ã¨ãã€çµæœã¯ } Y_i^d \text{ ã®1ã¤ã®ã¿}
$$

ï¼ˆå‡¦ç½®ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒè¤‡æ•°ã‚ã‚‹ã¨NG: ä¾‹ è–¬ã®æŠ•ä¸é‡ãŒ5mg/10mg/15mgãªã‚‰ $Y_i^{5}, Y_i^{10}, Y_i^{15}$ ã¨åˆ†ã‘ã‚‹å¿…è¦ï¼‰

**ä»®å®š2: å¹²æ¸‰ãªã— (No Interference)**

$$
Y_i^d = Y_i^{d_i} \quad \forall d_{-i}
$$

å€‹ä½“ $i$ ã®çµæœã¯ã€ä»–ã®å€‹ä½“ $-i$ ã®å‡¦ç½® $d_{-i}$ ã«ä¾å­˜ã—ãªã„ã€‚

**SUTVAãŒç ´ã‚Œã‚‹ä¾‹**:

- ãƒ¯ã‚¯ãƒãƒ³æ¥ç¨®: ä»–äººãŒæ¥ç¨®ã™ã‚‹ã¨è‡ªåˆ†ã®æ„ŸæŸ“ãƒªã‚¹ã‚¯ã‚‚ä¸‹ãŒã‚‹ï¼ˆé›†å›£å…ç–«ï¼‰
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åºƒå‘Š: å‹äººãŒã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨è‡ªåˆ†ã‚‚ã‚¯ãƒªãƒƒã‚¯ã—ã‚„ã™ã„

#### 3.2.2 ATE, ATT, CATE ã®å®Œå…¨å®šç¾©

| åŠ¹æœ | å®šç¾© | æ„å‘³ |
|:-----|:-----|:-----|
| **ATE** | $\mathbb{E}[Y^1 - Y^0]$ | å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ |
| **ATT** | $\mathbb{E}[Y^1 - Y^0 \mid D=1]$ | å‡¦ç½®ç¾¤ã®å¹³å‡å‡¦ç½®åŠ¹æœ |
| **ATC** | $\mathbb{E}[Y^1 - Y^0 \mid D=0]$ | å¯¾ç…§ç¾¤ã®å¹³å‡å‡¦ç½®åŠ¹æœ |
| **CATE** | $\mathbb{E}[Y^1 - Y^0 \mid X=x]$ | æ¡ä»¶ä»˜ãå¹³å‡å‡¦ç½®åŠ¹æœ |

**å°å‡º**:

$$
\begin{aligned}
\text{ATE} &= \mathbb{E}[Y^1] - \mathbb{E}[Y^0] \\
&= \mathbb{E}[\mathbb{E}[Y^1 \mid X]] - \mathbb{E}[\mathbb{E}[Y^0 \mid X]] \\
&= \mathbb{E}[\text{CATE}(X)]
\end{aligned}
$$

**ATTã¨ATEã®é–¢ä¿‚**:

$$
\begin{aligned}
\text{ATE} &= P(D=1) \cdot \text{ATT} + P(D=0) \cdot \text{ATC}
\end{aligned}
$$

**ATTæ¨å®šãŒé‡è¦ãªç†ç”±**: æ”¿ç­–è©•ä¾¡ã§ã¯ã€Œå®Ÿéš›ã«å‡¦ç½®ã‚’å—ã‘ãŸäººã«ã¨ã£ã¦ã®åŠ¹æœã€ãŒå•ã‚ã‚Œã‚‹ã€‚

#### 3.2.3 Unconfoundedness (ç„¡äº¤çµ¡æ€§) ä»®å®š

$$
(Y^1, Y^0) \perp\!\!\!\perp D \mid X
$$

$X$ ã‚’æ‰€ä¸ã¨ã™ã‚Œã°ã€æ½œåœ¨çš„çµæœã¨å‡¦ç½®å‰²ã‚Šå½“ã¦ãŒç‹¬ç«‹ã€‚

**ã“ã‚ŒãŒæˆã‚Šç«‹ã¤ã¨ã**:

$$
\begin{aligned}
\mathbb{E}[Y^1 \mid X] &= \mathbb{E}[Y^1 \mid D=1, X] = \mathbb{E}[Y \mid D=1, X] \\
\mathbb{E}[Y^0 \mid X] &= \mathbb{E}[Y^0 \mid D=0, X] = \mathbb{E}[Y \mid D=0, X]
\end{aligned}
$$

ã‚ˆã£ã¦:

$$
\text{CATE}(X) = \mathbb{E}[Y \mid D=1, X] - \mathbb{E}[Y \mid D=0, X]
$$

**ATEè­˜åˆ¥**:

$$
\begin{aligned}
\text{ATE} &= \mathbb{E}_X[\mathbb{E}[Y \mid D=1, X] - \mathbb{E}[Y \mid D=0, X]] \\
&= \mathbb{E}_X[\text{CATE}(X)]
\end{aligned}
$$

#### 3.2.4 Overlap/Positivity (å…±é€šã‚µãƒãƒ¼ãƒˆ) ä»®å®š

$$
0 < P(D=1 \mid X=x) < 1 \quad \forall x \in \text{supp}(X)
$$

å…¨ã¦ã® $X$ ã®å€¤ã§ã€å‡¦ç½®ç¾¤ãƒ»å¯¾ç…§ç¾¤ã®ä¸¡æ–¹ãŒå­˜åœ¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

**ç ´ã‚Œã‚‹ä¾‹**:

- ç”·æ€§ã®ã¿ã«å‰ç«‹è…ºãŒã‚“æ¤œè¨º â†’ å¥³æ€§ã§ $P(D=1 \mid \text{sex}=F)=0$
- é«˜æ‰€å¾—è€…ã®ã¿ãŒãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ—ãƒ©ãƒ³è³¼å…¥ â†’ ä½æ‰€å¾—è€…ã§ $P(D=1 \mid \text{income}<\$30k)=0$

OverlapãŒãªã„ã¨ã€åå®Ÿä»®æƒ³ $\mathbb{E}[Y^0 \mid D=1, X]$ ãŒæ¨å®šä¸èƒ½ï¼ˆå‡¦ç½®ç¾¤ã§å¯¾ç…§ç¾¤ã®çµæœã‚’å¤–æŒ¿ã§ããªã„ï¼‰ã€‚

#### 3.2.5 æ•°å€¤æ¤œè¨¼: ATEæ¨å®š

```julia
using Statistics, Distributions

# ATE estimation under unconfoundedness
function ate_estimation_demo()
    n = 10000
    # Covariate X ~ N(0,1)
    X = randn(n)

    # Treatment assignment (unconfounded given X)
    e_X = 1 ./ (1 .+ exp.(-X))  # propensity score
    D = rand(n) .< e_X

    # Potential outcomes
    # Y^1 = 2 + X + Îµâ‚
    # Y^0 = X + Îµâ‚€
    # True ATE = E[Y^1 - Y^0] = 2
    Y1 = 2 .+ X .+ randn(n) * 0.5
    Y0 = X .+ randn(n) * 0.5

    # Observed outcome
    Y = D .* Y1 .+ (1 .- D) .* Y0

    # Naive estimator (biased)
    ate_naive = mean(Y[D]) - mean(Y[.!D])

    # Regression adjustment (unbiased under unconfoundedness)
    # E[Y|D=1,X] - E[Y|D=0,X] = CATE(X)
    # Approximate with linear regression
    function linear_reg(D, X, Y)
        # Y ~ Î²â‚€ + Î²â‚D + Î²â‚‚X + Î²â‚ƒDX
        n = length(Y)
        design_matrix = hcat(ones(n), D, X, D .* X)
        Î² = design_matrix \ Y
        return Î²
    end

    Î² = linear_reg(D, X, Y)
    # ATE = E[Y|D=1,X] - E[Y|D=0,X] averaged over X
    # = Î²â‚ + Î²â‚ƒ * E[X] = Î²â‚ (since E[X]=0)
    ate_reg = Î²[2]

    println("True ATE: 2.0")
    println("Naive ATE: $(round(ate_naive, digits=3))")
    println("Regression ATE: $(round(ate_reg, digits=3))")

    return ate_naive, ate_reg
end

ate_estimation_demo()
```

### 3.3 Pearlå› æœç†è«– (Structural Causal Models)

#### 3.3.1 æ§‹é€ å› æœãƒ¢ãƒ‡ãƒ« (SCM) ã®å®šç¾©

**SCM** ã¯3ã¤çµ„ $\mathcal{M} = (\mathcal{U}, \mathcal{V}, \mathcal{F})$:

- $\mathcal{U}$: å¤–ç”Ÿå¤‰æ•°ï¼ˆè¦³æ¸¬ä¸èƒ½ãªèª¤å·®é …ï¼‰
- $\mathcal{V}$: å†…ç”Ÿå¤‰æ•°ï¼ˆè¦³æ¸¬å¯èƒ½ãªå¤‰æ•°ï¼‰
- $\mathcal{F}$: æ§‹é€ æ–¹ç¨‹å¼ï¼ˆå¤‰æ•°é–“ã®å› æœé–¢ä¿‚ï¼‰

**ä¾‹**: å–«ç…™ $S$, éºä¼ $G$, ãŒã‚“ $C$

$$
\begin{aligned}
G &= U_G \quad \text{(å¤–ç”Ÿ)} \\
S &= f_S(G, U_S) \quad \text{(éºä¼ãŒå–«ç…™ã«å½±éŸ¿)} \\
C &= f_C(S, G, U_C) \quad \text{(å–«ç…™ã¨éºä¼ãŒãŒã‚“ã«å½±éŸ¿)}
\end{aligned}
$$

DAGè¡¨ç¾:

```mermaid
graph TD
    U_G["U_G"] --> G["éºä¼ G"]
    U_S["U_S"] --> S["å–«ç…™ S"]
    G --> S
    U_C["U_C"] --> C["ãŒã‚“ C"]
    S --> C
    G --> C
    style U_G fill:#f5f5f5
    style U_S fill:#f5f5f5
    style U_C fill:#f5f5f5
```

#### 3.3.2 do-æ¼”ç®— (Intervention)

**ä»‹å…¥ $do(X=x)$**: å¤‰æ•° $X$ ã‚’å¤–éƒ¨ã‹ã‚‰å¼·åˆ¶çš„ã« $x$ ã«å›ºå®šã™ã‚‹ã€‚

**å½¢å¼çš„å®šç¾©**:

$$
P(Y \mid do(X=x)) = \sum_z P(Y \mid X=x, Z=z) P(Z=z)
$$

ã“ã“ã§ $Z$ ã¯ $X$ ã¨ $Y$ ã®é–“ã®**ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹**ã‚’é®æ–­ã™ã‚‹å¤‰æ•°é›†åˆã€‚

**è¦³æ¸¬ vs ä»‹å…¥ã®é•ã„**:

$$
\begin{aligned}
P(Y \mid X=x) &= \frac{P(Y, X=x)}{P(X=x)} \quad \text{(è¦³æ¸¬: æ¡ä»¶ä»˜ãç¢ºç‡)} \\
P(Y \mid do(X=x)) &= P_{M_{\bar{X}}}(Y \mid X=x) \quad \text{(ä»‹å…¥: SCM } M \text{ ã§ } X \text{ ã¸ã®çŸ¢å°ã‚’å‰Šé™¤)}
\end{aligned}
$$

**ä¾‹**: å–«ç…™ã¨ãŒã‚“ã®å› æœåŠ¹æœ

$$
\begin{aligned}
P(C=1 \mid S=1) &= \frac{P(C=1, S=1)}{P(S=1)} \quad \text{(å–«ç…™è€…ã®ãŒã‚“ç‡ â€” äº¤çµ¡ã‚ã‚Š)} \\
P(C=1 \mid do(S=1)) &= \sum_g P(C=1 \mid S=1, G=g) P(G=g) \quad \text{(å–«ç…™ã‚’å¼·åˆ¶ã—ãŸå ´åˆã®ãŒã‚“ç‡)}
\end{aligned}
$$

#### 3.3.3 DAG (æœ‰å‘éå·¡å›ã‚°ãƒ©ãƒ•) ã®åŸºç¤

**DAG** $\mathcal{G} = (V, E)$: é ‚ç‚¹ $V$ ã¨æœ‰å‘è¾º $E$ ã‹ã‚‰ãªã‚‹ã‚°ãƒ©ãƒ•ï¼ˆé–‰è·¯ãªã—ï¼‰

**è¦ª (Parents)**: $\text{PA}_i = \\{j : (j, i) \in E\\}$

**å­å­« (Descendants)**: $\text{DE}_i = \\{j : i \text{ ã‹ã‚‰ } j \text{ ã¸ã®ãƒ‘ã‚¹ãŒå­˜åœ¨}\\}$

**å› æœãƒãƒ«ã‚³ãƒ•æ¡ä»¶**:

$$
P(v_1, \ldots, v_n) = \prod_{i=1}^n P(v_i \mid \text{PA}_i)
$$

å„å¤‰æ•°ã¯ã€è¦ªã‚’æ‰€ä¸ã¨ã™ã‚Œã°éå­å­«ã¨ç‹¬ç«‹ã€‚

#### 3.3.4 d-åˆ†é›¢ (d-separation)

**å®šç¾©**: DAGä¸Šã§ã€å¤‰æ•°é›†åˆ $Z$ ãŒ $X$ ã¨ $Y$ ã‚’ d-åˆ†é›¢ã™ã‚‹ $\iff$ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒ‘ã‚¹ãŒ $Z$ ã«ã‚ˆã£ã¦é®æ–­ã•ã‚Œã‚‹ã€‚

**ãƒ‘ã‚¹ã®é®æ–­æ¡ä»¶**:

| ãƒ‘ã‚¹æ§‹é€  | é®æ–­æ¡ä»¶ | å›³ |
|:--------|:---------|:---|
| **Chain** $X \to Z \to Y$ | $Z \in \mathcal{Z}$ | $X$ ã‹ã‚‰ $Y$ ã¸ã®æƒ…å ±ã¯ $Z$ ã‚’é€šã‚‹ |
| **Fork** $X \leftarrow Z \to Y$ | $Z \in \mathcal{Z}$ | $Z$ ãŒå…±é€šåŸå› ï¼ˆäº¤çµ¡ï¼‰ |
| **Collider** $X \to Z \leftarrow Y$ | $Z \notin \mathcal{Z}$ ã‹ã¤ $\text{DE}(Z) \cap \mathcal{Z} = \emptyset$ | $Z$ ãŒçµæœï¼ˆé¸æŠãƒã‚¤ã‚¢ã‚¹ï¼‰ |

**d-åˆ†é›¢ã®é‡è¦æ€§**:

$$
X \perp_d Y \mid Z \quad \Rightarrow \quad X \perp\!\!\!\perp Y \mid Z \quad \text{(æ¡ä»¶ä»˜ãç‹¬ç«‹)}
$$

**ä¾‹**: Colliderã®ãƒ‘ãƒ©ãƒ‰ã‚¯ã‚¹

```mermaid
graph TD
    T["æ‰èƒ½ T"] --> A["åˆæ ¼ A"]
    E["åŠªåŠ› E"] --> A
```

$T \perp\!\!\!\perp E$ ï¼ˆæ‰èƒ½ã¨åŠªåŠ›ã¯ç‹¬ç«‹ï¼‰ã ãŒã€åˆæ ¼è€… $A=1$ ã‚’æ¡ä»¶ã¥ã‘ã‚‹ã¨:

$$
T \not\perp\!\!\!\perp E \mid A=1
$$

åˆæ ¼è€…ã®ä¸­ã§ã¯ã€ŒåŠªåŠ›ãŒå°‘ãªã„â†’æ‰èƒ½ãŒé«˜ã„ã€ã¨ã„ã†è² ã®ç›¸é–¢ãŒç”Ÿã¾ã‚Œã‚‹ï¼ˆé¸æŠãƒã‚¤ã‚¢ã‚¹ï¼‰ã€‚

#### 3.3.5 ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº– (Backdoor Criterion)

**å®šç¾©**: å¤‰æ•°é›†åˆ $Z$ ãŒ $(X, Y)$ ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã‚’æº€ãŸã™ $\iff$

1. $Z$ ã®ã©ã®å¤‰æ•°ã‚‚ $X$ ã®å­å­«ã§ãªã„
2. $Z$ ãŒ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ã‚’é®æ–­ã™ã‚‹

**ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹**: $X \leftarrow \cdots \to Y$ ã®ã‚ˆã†ãªã€$X$ ã¸ã®çŸ¢å°ã‚’å«ã‚€ãƒ‘ã‚¹

**ãƒãƒƒã‚¯ãƒ‰ã‚¢èª¿æ•´å…¬å¼**:

$$
P(Y \mid do(X=x)) = \sum_z P(Y \mid X=x, Z=z) P(Z=z)
$$

**ä¾‹**: å–«ç…™â†’ãŒã‚“

```mermaid
graph TD
    G["éºä¼ G"] --> S["å–«ç…™ S"]
    G --> C["ãŒã‚“ C"]
    S --> C
```

$Z = \\{G\\}$ ãŒãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã‚’æº€ãŸã™:

$$
P(C \mid do(S=s)) = \sum_g P(C \mid S=s, G=g) P(G=g)
$$

#### 3.3.6 ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢åŸºæº– (Frontdoor Criterion)

**çŠ¶æ³**: ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ã‚’é®æ–­ã§ããªã„ï¼ˆæœªæ¸¬å®šäº¤çµ¡ $U$ ãŒã‚ã‚‹ï¼‰ãŒã€**åª’ä»‹å¤‰æ•° $M$** ã‚’æ¸¬å®šã§ãã‚‹å ´åˆ

```mermaid
graph TD
    U["æœªæ¸¬å®šäº¤çµ¡ U"] --> X["å‡¦ç½® X"]
    U --> Y["çµæœ Y"]
    X --> M["åª’ä»‹å¤‰æ•° M"]
    M --> Y
```

**ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢åŸºæº–**: $M$ ãŒ $(X, Y)$ ã®ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢åŸºæº–ã‚’æº€ãŸã™ $\iff$

1. $M$ ãŒ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒ‘ã‚¹ã‚’é®æ–­
2. $X$ ã‹ã‚‰ $M$ ã¸ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªã„
3. $X$ ãŒ $M$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ã‚’é®æ–­

**ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢èª¿æ•´å…¬å¼**:

$$
P(Y \mid do(X=x)) = \sum_m P(M=m \mid X=x) \sum_{x'} P(Y \mid M=m, X=x') P(X=x')
$$

**ä¾‹**: å–«ç…™â†’ã‚¿ãƒ¼ãƒ«æ²ˆç€â†’ãŒã‚“

$$
P(C \mid do(S=s)) = \sum_t P(T=t \mid S=s) \sum_{s'} P(C \mid T=t, S=s') P(S=s')
$$

#### 3.3.7 do-æ¼”ç®—ã®3ã¤ã®å…¬ç†

Pearl [^1] ã®do-calculus â€” ä»‹å…¥ç¢ºç‡ã‚’æ¡ä»¶ä»˜ãç¢ºç‡ã«å¤‰æ›ã™ã‚‹3ã¤ã®ãƒ«ãƒ¼ãƒ«:

**Rule 1 (è¦³æ¸¬ã®æŒ¿å…¥/å‰Šé™¤)**:

$$
P(Y \mid do(X), Z, W) = P(Y \mid do(X), W) \quad \text{if } (Y \perp_d Z \mid X, W)_{\mathcal{G}_{\bar{X}}}
$$

**Rule 2 (ä»‹å…¥ã®æŒ¿å…¥/å‰Šé™¤)**:

$$
P(Y \mid do(X), do(Z), W) = P(Y \mid do(X), Z, W) \quad \text{if } (Y \perp_d Z \mid X, W)_{\mathcal{G}_{\bar{X}, \underline{Z}}}
$$

**Rule 3 (ä»‹å…¥ã®å‰Šé™¤)**:

$$
P(Y \mid do(X), do(Z), W) = P(Y \mid do(X), W) \quad \text{if } (Y \perp_d Z \mid X, W)_{\mathcal{G}_{\bar{X}, \overline{Z(W)}}}
$$

ã“ã“ã§:
- $\mathcal{G}_{\bar{X}}$: $X$ ã¸ã®çŸ¢å°ã‚’å‰Šé™¤
- $\mathcal{G}_{\underline{X}}$: $X$ ã‹ã‚‰ã®çŸ¢å°ã‚’å‰Šé™¤
- $\mathcal{G}_{\overline{X(W)}}$: $W$ ã®éç¥–å…ˆã§ã‚ã‚‹ $X$ ã¸ã®çŸ¢å°ã‚’å‰Šé™¤

**å¿œç”¨**: ãƒãƒƒã‚¯ãƒ‰ã‚¢èª¿æ•´ã®å°å‡º

$$
\begin{aligned}
P(Y \mid do(X)) &= \sum_z P(Y \mid do(X), Z=z) P(Z=z \mid do(X)) \\
&= \sum_z P(Y \mid do(X), Z=z) P(Z=z) \quad \text{(Rule 3)} \\
&= \sum_z P(Y \mid X, Z=z) P(Z=z) \quad \text{(Rule 2)}
\end{aligned}
$$

#### 3.3.8 Pearl's Ladder of Causation

| ãƒ¬ãƒ™ãƒ« | å•ã„ | è¨˜æ³• | ä¾‹ |
|:------|:-----|:-----|:---|
| **1. Association** | è¦³æ¸¬ã—ãŸã‚‰ï¼Ÿ | $P(Y \mid X)$ | å–«ç…™è€…ã®ãŒã‚“ç‡ |
| **2. Intervention** | ä»‹å…¥ã—ãŸã‚‰ï¼Ÿ | $P(Y \mid do(X))$ | å–«ç…™ã‚’å¼·åˆ¶ã—ãŸã‚‰ãŒã‚“ã«ãªã‚‹ã‹ |
| **3. Counterfactual** | ã‚‚ã—ã€œã ã£ãŸã‚‰ï¼Ÿ | $P(Y_{X=x'} \mid X=x, Y=y)$ | å–«ç…™ã—ãªã‹ã£ãŸã‚‰ãŒã‚“ã«ãªã‚‰ãªã‹ã£ãŸã‹ |

**åå®Ÿä»®æƒ³ (Counterfactual)**: éå»ã®äº‹å®Ÿã‚’å¤‰ãˆãŸå ´åˆã®ä»®æƒ³çš„çµæœ

$$
Y_{X=x'} = \text{å€‹ä½“ãŒ } X=x \text{ ã‚’å®Ÿéš›ã«å—ã‘ãŸãŒã€} X=x' \text{ ã‚’å—ã‘ã¦ã„ãŸã‚‰å¾—ã‚‰ã‚ŒãŸçµæœ}
$$

### 3.4 å‚¾å‘ã‚¹ã‚³ã‚¢ (Propensity Score)

#### 3.4.1 å‚¾å‘ã‚¹ã‚³ã‚¢ã®å®šç¾©

**å®šç¾© (Rosenbaum & Rubin 1983)**:

$$
e(X) = P(D=1 \mid X)
$$

$X$ ã‚’æ‰€ä¸ã¨ã—ãŸã¨ãã®å‡¦ç½®ã‚’å—ã‘ã‚‹ç¢ºç‡ã€‚

**é‡è¦æ€§**: $X$ ãŒé«˜æ¬¡å…ƒã§ã‚‚ã€$e(X)$ ã¯1æ¬¡å…ƒã®ã‚¹ã‚«ãƒ©ãƒ¼ã€‚

**Propensity Score Theorem**:

$$
(Y^1, Y^0) \perp\!\!\!\perp D \mid X \quad \Rightarrow \quad (Y^1, Y^0) \perp\!\!\!\perp D \mid e(X)
$$

**è¨¼æ˜**:

$$
\begin{aligned}
P(D=1 \mid Y^1, Y^0, e(X)) &= \mathbb{E}[P(D=1 \mid Y^1, Y^0, X) \mid Y^1, Y^0, e(X)] \\
&= \mathbb{E}[P(D=1 \mid X) \mid Y^1, Y^0, e(X)] \quad \text{(unconfoundedness)} \\
&= \mathbb{E}[e(X) \mid Y^1, Y^0, e(X)] \\
&= e(X) \\
&= P(D=1 \mid e(X))
\end{aligned}
$$

ã‚ˆã£ã¦ $(Y^1, Y^0) \perp\!\!\!\perp D \mid e(X)$ã€‚

#### 3.4.2 IPW (Inverse Probability Weighting) æ¨å®šé‡

**IPWæ¨å®šé‡**:

$$
\hat{\text{ATE}}_{\text{IPW}} = \frac{1}{n} \sum_{i=1}^n \left( \frac{D_i Y_i}{e(X_i)} - \frac{(1 - D_i) Y_i}{1 - e(X_i)} \right)
$$

**å°å‡º**:

$$
\begin{aligned}
\mathbb{E}\left[\frac{D Y}{e(X)}\right] &= \mathbb{E}\left[\mathbb{E}\left[\frac{D Y}{e(X)} \mid X\right]\right] \\
&= \mathbb{E}\left[\frac{\mathbb{E}[D Y \mid X]}{e(X)}\right] \\
&= \mathbb{E}\left[\frac{P(D=1 \mid X) \mathbb{E}[Y \mid D=1, X]}{e(X)}\right] \\
&= \mathbb{E}\left[\frac{e(X) \mathbb{E}[Y^1 \mid X]}{e(X)}\right] \\
&= \mathbb{E}[Y^1]
\end{aligned}
$$

åŒæ§˜ã« $\mathbb{E}\left[\frac{(1-D) Y}{1-e(X)}\right] = \mathbb{E}[Y^0]$ã€‚

**ATTæ¨å®šé‡**:

$$
\hat{\text{ATT}}_{\text{IPW}} = \frac{\sum_i D_i Y_i}{\sum_i D_i} - \frac{\sum_i D_i (1-D_i) Y_i / (1-e(X_i))}{\sum_i D_i e(X_i) / (1-e(X_i))}
$$

#### 3.4.3 Doubly Robust æ¨å®šé‡

IPWã¨å›å¸°èª¿æ•´ã‚’çµ„ã¿åˆã‚ã›ãŸæ¨å®šé‡ã€‚**ã©ã¡ã‚‰ã‹ä¸€æ–¹ãŒæ­£ã—ã‘ã‚Œã°ä¸å**ï¼ˆé ‘å¥æ€§2å€ï¼‰ã€‚

$$
\hat{\text{ATE}}_{\text{DR}} = \frac{1}{n} \sum_{i=1}^n \left[ \frac{D_i (Y_i - \hat{\mu}_1(X_i))}{e(X_i)} + \hat{\mu}_1(X_i) - \frac{(1-D_i)(Y_i - \hat{\mu}_0(X_i))}{1-e(X_i)} - \hat{\mu}_0(X_i) \right]
$$

ã“ã“ã§:
- $\hat{\mu}_1(X) = \mathbb{E}[Y \mid D=1, X]$ (å‡¦ç½®ç¾¤ã®çµæœãƒ¢ãƒ‡ãƒ«)
- $\hat{\mu}_0(X) = \mathbb{E}[Y \mid D=0, X]$ (å¯¾ç…§ç¾¤ã®çµæœãƒ¢ãƒ‡ãƒ«)

**ä¸åæ€§ã®è¨¼æ˜** (ã©ã¡ã‚‰ã‹ä¸€æ–¹ãŒæ­£ã—ã„å ´åˆ):

**Case 1**: $\hat{\mu}_1, \hat{\mu}_0$ ãŒæ­£ã—ã„

$$
\begin{aligned}
\mathbb{E}[\hat{\text{ATE}}_{\text{DR}}] &= \mathbb{E}\left[\mathbb{E}\left[\frac{D(Y - \mu_1(X))}{e(X)} \mid X\right]\right] + \mathbb{E}[\mu_1(X)] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}\left[\frac{e(X)(\mu_1(X) - \mu_1(X))}{e(X)}\right] + \mathbb{E}[Y^1 - Y^0] \\
&= \text{ATE}
\end{aligned}
$$

**Case 2**: $e(X)$ ãŒæ­£ã—ã„ï¼ˆ$\hat{\mu}$ ãŒèª¤ã‚Šã§ã‚‚OKï¼‰

IPWã®ä¸åæ€§ã«ã‚ˆã‚Š $\mathbb{E}[\hat{\text{ATE}}_{\text{DR}}] = \text{ATE}$ã€‚

#### 3.4.4 å…±é€šã‚µãƒãƒ¼ãƒˆ (Common Support) ã¨ãƒˆãƒªãƒŸãƒ³ã‚°

**å…±é€šã‚µãƒãƒ¼ãƒˆæ¡ä»¶**:

$$
0 < e(X) < 1 \quad \forall X \in \text{supp}(X)
$$

**ç ´ã‚Œã‚‹å ´åˆ**: æ¥µç«¯ãª $e(X)$ å€¤ï¼ˆ0ã«è¿‘ã„/1ã«è¿‘ã„ï¼‰ã§ IPW ã®åˆ†æ•£ãŒçˆ†ç™ºã€‚

**ãƒˆãƒªãƒŸãƒ³ã‚°**: $e(X) \in [\epsilon, 1-\epsilon]$ ã®ç¯„å›²ã®ã¿ã‚’ä½¿ç”¨ï¼ˆé€šå¸¸ $\epsilon = 0.05$ or $0.1$ï¼‰

$$
\hat{\text{ATE}}_{\text{trim}} = \frac{1}{n'} \sum_{i: e(X_i) \in [\epsilon, 1-\epsilon]} \left( \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i) Y_i}{1-e(X_i)} \right)
$$

#### 3.4.5 ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ (Balance Check)

å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°å¾Œã€**å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã§å…±å¤‰é‡ $X$ ã®åˆ†å¸ƒãŒæƒã£ã¦ã„ã‚‹ã‹**ã‚’ç¢ºèªã€‚

**æ¨™æº–åŒ–å·® (Standardized Mean Difference)**:

$$
\text{SMD} = \frac{\bar{X}_1 - \bar{X}_0}{\sqrt{(s_1^2 + s_0^2)/2}}
$$

$\text{SMD} < 0.1$ ãªã‚‰è‰¯å¥½ãªãƒãƒ©ãƒ³ã‚¹ã€‚

**Love Plot**: å„å…±å¤‰é‡ã® SMD ã‚’ãƒãƒƒãƒãƒ³ã‚°å‰å¾Œã§æ¯”è¼ƒã™ã‚‹ãƒ—ãƒ­ãƒƒãƒˆã€‚

```julia
# Balance check simulation
function balance_check(D, X, e_X)
    # Before matching
    smd_before = abs(mean(X[D]) - mean(X[.!D])) / sqrt((var(X[D]) + var(X[.!D])) / 2)

    # After IPW weighting
    weights_1 = D ./ e_X
    weights_0 = (1 .- D) ./ (1 .- e_X)
    mean_1_weighted = sum(weights_1 .* X) / sum(weights_1)
    mean_0_weighted = sum(weights_0 .* X) / sum(weights_0)
    var_1_weighted = sum(weights_1 .* (X .- mean_1_weighted).^2) / sum(weights_1)
    var_0_weighted = sum(weights_0 .* (X .- mean_0_weighted).^2) / sum(weights_0)
    smd_after = abs(mean_1_weighted - mean_0_weighted) / sqrt((var_1_weighted + var_0_weighted) / 2)

    println("SMD before matching: $(round(smd_before, digits=3))")
    println("SMD after IPW: $(round(smd_after, digits=3))")
    println(smd_after < 0.1 ? "âœ… Good balance" : "âŒ Poor balance")

    return smd_before, smd_after
end
```

### 3.5 æ“ä½œå¤‰æ•°æ³• (Instrumental Variables)

#### 3.5.1 æ“ä½œå¤‰æ•°ã®å®šç¾©

**çŠ¶æ³**: æœªæ¸¬å®šäº¤çµ¡ $U$ ãŒã‚ã‚Šã€unconfoundedness ãŒæˆã‚Šç«‹ãŸãªã„

```mermaid
graph TD
    U["æœªæ¸¬å®šäº¤çµ¡ U"] --> D["å‡¦ç½® D"]
    U --> Y["çµæœ Y"]
    D --> Y
    Z["æ“ä½œå¤‰æ•° Z"] --> D
```

**æ“ä½œå¤‰æ•° $Z$ ã®3æ¡ä»¶**:

1. **é–¢é€£æ€§ (Relevance)**: $Z \perp\!\!\!\perp D$ ($Z$ ãŒ $D$ ã«å½±éŸ¿)
2. **å¤–ç”Ÿæ€§ (Exogeneity)**: $Z \perp\!\!\!\perp U$ ($Z$ ã¯äº¤çµ¡ã¨ç„¡ç›¸é–¢)
3. **æ’é™¤åˆ¶ç´„ (Exclusion Restriction)**: $Z \to Y$ ã®ç›´æ¥ãƒ‘ã‚¹ãªã—ï¼ˆ$Z$ ã¯ $D$ çµŒç”±ã§ã®ã¿ $Y$ ã«å½±éŸ¿ï¼‰

**ä¾‹**: å…µå½¹ãŒåå…¥ã«ä¸ãˆã‚‹å½±éŸ¿

- $D$: å…µå½¹çµŒé¨“ (1=ã‚ã‚Š, 0=ãªã—)
- $Y$: ç”Ÿæ¶¯åå…¥
- $U$: èƒ½åŠ›ï¼ˆæœªæ¸¬å®šï¼‰
- $Z$: å¾´å…µãã˜ (1=å½“é¸, 0=å¤–ã‚Œ)

å¾´å…µãã˜ã¯èƒ½åŠ› $U$ ã¨ç„¡é–¢ä¿‚ï¼ˆå¤–ç”Ÿï¼‰ã€å…µå½¹ $D$ ã«å½±éŸ¿ï¼ˆé–¢é€£ï¼‰ã€åå…¥ $Y$ ã«ã¯å…µå½¹çµŒç”±ã§ã®ã¿å½±éŸ¿ï¼ˆæ’é™¤åˆ¶ç´„ï¼‰ã€‚

#### 3.5.2 2SLS (Two-Stage Least Squares)

**ç¬¬1æ®µéš**: $D$ ã‚’ $Z$ ã§å›å¸°

$$
D_i = \pi_0 + \pi_1 Z_i + \nu_i
$$

$\hat{D}_i = \hat{\pi}_0 + \hat{\pi}_1 Z_i$ ã‚’äºˆæ¸¬ã€‚

**ç¬¬2æ®µéš**: $Y$ ã‚’ $\hat{D}$ ã§å›å¸°

$$
Y_i = \beta_0 + \beta_1 \hat{D}_i + \epsilon_i
$$

$\hat{\beta}_1$ ãŒå› æœåŠ¹æœã®æ¨å®šå€¤ã€‚

**å°å‡º (ç°¡ç•¥ç‰ˆ)**:

$$
\begin{aligned}
\text{Cov}(Y, Z) &= \text{Cov}(\beta_0 + \beta_1 D + U, Z) \\
&= \beta_1 \text{Cov}(D, Z) + \text{Cov}(U, Z) \\
&= \beta_1 \text{Cov}(D, Z) \quad \text{(å¤–ç”Ÿæ€§: } \text{Cov}(U, Z)=0)
\end{aligned}
$$

$$
\hat{\beta}_1 = \frac{\text{Cov}(Y, Z)}{\text{Cov}(D, Z)}
$$

**Waldæ¨å®šé‡** (2å€¤ $Z$ ã®å ´åˆ):

$$
\hat{\beta}_1 = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0]}
$$

#### 3.5.3 LATE (Local Average Treatment Effect)

IVã§æ¨å®šã•ã‚Œã‚‹ã®ã¯**ATE**ã§ã¯ãªã**LATE** â€” ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ (Complier) ã®å‡¦ç½®åŠ¹æœã€‚

**4ã¤ã®ã‚¿ã‚¤ãƒ—**:

| ã‚¿ã‚¤ãƒ— | $D(Z=0)$ | $D(Z=1)$ | èª¬æ˜ |
|:------|:---------|:---------|:-----|
| **Always-Taker** | 1 | 1 | å¸¸ã«å‡¦ç½®ã‚’å—ã‘ã‚‹ |
| **Never-Taker** | 0 | 0 | å¸¸ã«å‡¦ç½®ã‚’å—ã‘ãªã„ |
| **Complier** | 0 | 1 | IVã«å¾“ã† |
| **Defier** | 1 | 0 | IVã«é€†ã‚‰ã† (monotonicityä»®å®šã§æ’é™¤) |

**LATE**:

$$
\text{LATE} = \mathbb{E}[Y^1 - Y^0 \mid \text{Complier}]
$$

**å°å‡º**:

$$
\begin{aligned}
\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0] &= \mathbb{E}[Y^1 - Y^0] \cdot P(\text{Complier}) \\
\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0] &= P(\text{Complier})
\end{aligned}
$$

$$
\text{LATE} = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0]}
$$

#### 3.5.4 Weak IV (å¼±æ“ä½œå¤‰æ•°) å•é¡Œ

**å¼±IV**: $\text{Cov}(D, Z)$ ãŒå°ã•ã„ â†’ ç¬¬1æ®µéšã® $F$ çµ±è¨ˆé‡ãŒä½ã„

**Stock-Yogo åŸºæº–** [^7]:

$$
F \text{-statistic} = \frac{(\text{RSS}_{\text{restricted}} - \text{RSS}_{\text{unrestricted}}) / q}{\text{RSS}_{\text{unrestricted}} / (n - k)} > 10
$$

$F < 10$ ãªã‚‰å¼±IVï¼ˆãƒã‚¤ã‚¢ã‚¹ãŒå¤§ãã„ï¼‰ã€‚

**å•é¡Œç‚¹**:

- 2SLSæ¨å®šé‡ã®ãƒã‚¤ã‚¢ã‚¹ãŒ OLS ã‚ˆã‚Šæ‚ªåŒ–
- æ¨™æº–èª¤å·®ãŒéå°è©•ä¾¡ã•ã‚Œã‚‹
- ä¿¡é ¼åŒºé–“ãŒéåº¦ã«ç‹­ããªã‚‹

**å¯¾ç­–**:

- Anderson-Rubin æ¤œå®šï¼ˆå¼±IVã«é ‘å¥ï¼‰
- LIML (Limited Information Maximum Likelihood)
- ã‚ˆã‚Šå¼·ã„IVã‚’æ¢ã™

### 3.6 å›å¸°ä¸é€£ç¶šãƒ‡ã‚¶ã‚¤ãƒ³ (RDD)

#### 3.6.1 RDDã®è¨­å®š

**çŠ¶æ³**: å‡¦ç½®å‰²ã‚Šå½“ã¦ãŒ**ã‚«ãƒƒãƒˆã‚ªãƒ• $c$** ã§æ±ºã¾ã‚‹

$$
D_i = \mathbb{1}(X_i \geq c)
$$

$X$: ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°å¤‰æ•° (running variable) â€” ä¾‹: ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã€å¹´é½¢ã€æ‰€å¾—

**å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–ä»®å®š**: $c$ ã®è¿‘å‚ã§ $X$ ã¯ as-if ãƒ©ãƒ³ãƒ€ãƒ 

$$
\lim_{x \to c^+} \mathbb{E}[Y^1 \mid X=x] - \lim_{x \to c^-} \mathbb{E}[Y^0 \mid X=x] = \text{ATE}_c
$$

#### 3.6.2 Sharp RDD vs Fuzzy RDD

**Sharp RDD**: ã‚«ãƒƒãƒˆã‚ªãƒ•ã§å‡¦ç½®ç¢ºç‡ãŒ 0 â†’ 1 ã«ä¸é€£ç¶šã«ã‚¸ãƒ£ãƒ³ãƒ—

$$
\lim_{x \to c^-} P(D=1 \mid X=x) = 0, \quad \lim_{x \to c^+} P(D=1 \mid X=x) = 1
$$

**Fuzzy RDD**: ã‚«ãƒƒãƒˆã‚ªãƒ•ã§å‡¦ç½®ç¢ºç‡ãŒã‚¸ãƒ£ãƒ³ãƒ—ã™ã‚‹ãŒ 0/1 ã§ã¯ãªã„

$$
\lim_{x \to c^-} P(D=1 \mid X=x) < \lim_{x \to c^+} P(D=1 \mid X=x) < 1
$$

Fuzzy RDDã¯IVã¨ã—ã¦æ‰±ã†: $Z = \mathbb{1}(X \geq c)$ ã‚’æ“ä½œå¤‰æ•°ã¨ã—ã€2SLSæ¨å®šã€‚

#### 3.6.3 RDDæ¨å®šé‡

**Local Linear Regression**:

$$
\min_{\beta_0, \beta_1, \beta_2, \beta_3} \sum_{i: |X_i - c| < h} K\left(\frac{X_i - c}{h}\right) (Y_i - \beta_0 - \beta_1 D_i - \beta_2 (X_i - c) - \beta_3 D_i (X_i - c))^2
$$

ã“ã“ã§:
- $h$: å¸¯åŸŸå¹… (bandwidth)
- $K(\cdot)$: ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆä¸‰è§’ã‚«ãƒ¼ãƒãƒ«ã€Epanechnikov ã‚«ãƒ¼ãƒãƒ«ç­‰ï¼‰

**RDDåŠ¹æœ**: $\hat{\beta}_1$

**å¸¯åŸŸå¹…é¸æŠ**:

- IK (Imbens-Kalyanaraman) å¸¯åŸŸå¹…
- CCT (Calonico-Cattaneo-Titiunik) å¸¯åŸŸå¹…ï¼ˆãƒã‚¤ã‚¢ã‚¹è£œæ­£ä»˜ãï¼‰

$$
h_{\text{IK}} = C \cdot \left(\frac{\text{var}(\epsilon)}{n \cdot f(c) \cdot (\mu^{(2)}(c^+) - \mu^{(2)}(c^-))^2}\right)^{1/5}
$$

#### 3.6.4 RDDã®å¦¥å½“æ€§æ¤œå®š

**1. é€£ç¶šæ€§æ¤œå®š (Continuity Tests)**

å…±å¤‰é‡ $X$ ãŒã‚«ãƒƒãƒˆã‚ªãƒ• $c$ ã§é€£ç¶šã‹ç¢ºèª:

$$
\lim_{x \to c^+} \mathbb{E}[X_{\text{covariate}} \mid X=x] = \lim_{x \to c^-} \mathbb{E}[X_{\text{covariate}} \mid X=x]
$$

**2. å¯†åº¦æ¤œå®š (McCrary Density Test)**

$X$ ã®å¯†åº¦ $f(X)$ ãŒã‚«ãƒƒãƒˆã‚ªãƒ•ã§ä¸é€£ç¶šãªã‚‰æ“ä½œã®ç–‘ã„:

$$
\lim_{x \to c^+} f(x) \neq \lim_{x \to c^-} f(x) \quad \Rightarrow \quad \text{manipulation}
$$

**3. Placebo Test**

å½ã‚«ãƒƒãƒˆã‚ªãƒ• $c' \neq c$ ã§åŠ¹æœãŒã‚¼ãƒ­ã‹ç¢ºèªã€‚

### 3.7 å·®åˆ†ã®å·®åˆ†æ³• (DiD)

#### 3.7.1 DiDã®è¨­å®š

**2æœŸé–“ãƒ»2ã‚°ãƒ«ãƒ¼ãƒ—**:

| | å‡¦ç½®å‰ $(t=0)$ | å‡¦ç½®å¾Œ $(t=1)$ |
|:--|:--------------|:--------------|
| **å‡¦ç½®ç¾¤** $(G=1)$ | $\mathbb{E}[Y_{10}]$ | $\mathbb{E}[Y_{11}]$ |
| **å¯¾ç…§ç¾¤** $(G=0)$ | $\mathbb{E}[Y_{00}]$ | $\mathbb{E}[Y_{01}]$ |

**DiDæ¨å®šé‡**:

$$
\hat{\tau}_{\text{DiD}} = (\mathbb{E}[Y_{11}] - \mathbb{E}[Y_{10}]) - (\mathbb{E}[Y_{01}] - \mathbb{E}[Y_{00}])
$$

**ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®š (Parallel Trends)**:

$$
\mathbb{E}[Y_{01} - Y_{00} \mid G=1] = \mathbb{E}[Y_{01} - Y_{00} \mid G=0]
$$

å‡¦ç½®ãŒãªã‹ã£ãŸå ´åˆã€å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯å¹³è¡Œã€‚

#### 3.7.2 DiDå›å¸°ãƒ¢ãƒ‡ãƒ«

$$
Y_{it} = \alpha + \beta \cdot \text{Treat}_i + \gamma \cdot \text{Post}_t + \delta \cdot (\text{Treat}_i \times \text{Post}_t) + \epsilon_{it}
$$

ã“ã“ã§:
- $\text{Treat}_i = \mathbb{1}(i \in \text{å‡¦ç½®ç¾¤})$
- $\text{Post}_t = \mathbb{1}(t \geq 1)$
- $\delta = \text{DiDåŠ¹æœ}$

**å›ºå®šåŠ¹æœãƒ¢ãƒ‡ãƒ«**:

$$
Y_{it} = \alpha_i + \lambda_t + \delta D_{it} + \epsilon_{it}
$$

$\alpha_i$: å€‹ä½“å›ºå®šåŠ¹æœã€$\lambda_t$: æ™‚é–“å›ºå®šåŠ¹æœ

#### 3.7.3 Staggered DiD (å¤šæœŸé–“ãƒ»æ®µéšçš„å°å…¥)

**å•é¡Œ**: å‡¦ç½®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãŒç•°ãªã‚‹ï¼ˆ$G_i$ ã«ã‚ˆã£ã¦å‡¦ç½®é–‹å§‹æ™‚æœŸãŒé•ã†ï¼‰

å¾“æ¥ã®TWFE (Two-Way Fixed Effects) ã¯**ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š** â€” æ—¢å‡¦ç½®ç¾¤ãŒå¯¾ç…§ç¾¤ã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹ã€‚

**Callaway & Sant'Anna (2021) [^5]**:

å„ã‚³ãƒ›ãƒ¼ãƒˆ $g$ (å‡¦ç½®é–‹å§‹æ™‚æœŸ) ã¨æ™‚ç‚¹ $t$ ã®ãƒšã‚¢ã§ DiD ã‚’æ¨å®š:

$$
\text{ATT}(g, t) = \mathbb{E}[Y_t - Y_{g-1} \mid G_g=1] - \mathbb{E}[Y_t - Y_{g-1} \mid C=1]
$$

$C$: æœªå‡¦ç½®ç¾¤ï¼ˆnever-treated or not-yet-treatedï¼‰

**é›†ç´„**:

$$
\text{ATT}_{\text{overall}} = \sum_{g} \sum_{t \geq g} w(g, t) \cdot \text{ATT}(g, t)
$$

é‡ã¿ $w(g, t)$ ã¯å‡¦ç½®ç¾¤ã®ã‚µã‚¤ã‚ºç­‰ã«åŸºã¥ãã€‚

### 3.8 æ©Ÿæ¢°å­¦ç¿’Ã—å› æœæ¨è«–

#### 3.8.1 Causal Forest (å› æœãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ)

**ç›®æ¨™**: ç•°è³ªãªå‡¦ç½®åŠ¹æœ $\tau(X) = \mathbb{E}[Y^1 - Y^0 \mid X]$ ã‚’æ¨å®š

Wager & Athey (2018) [^3] ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :

1. **ã‚µãƒ³ãƒ—ãƒ«åˆ†å‰²**: å„ãƒ„ãƒªãƒ¼ã§è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«ã‚’ $I$ (åˆ†å‰²ç”¨) ã¨ $J$ (æ¨å®šç”¨) ã«åˆ†å‰²
2. **åˆ†å‰²**: $I$ ã‚’ä½¿ã£ã¦CARTã§åˆ†å‰²ï¼ˆå‡¦ç½®åŠ¹æœã®åˆ†æ•£ã‚’æœ€å¤§åŒ–ï¼‰
3. **æ¨å®š**: å„ãƒªãƒ¼ãƒ• $L$ ã§ $J$ ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ã£ã¦ $\hat{\tau}(x)$ æ¨å®š

**æ¨å®šé‡**:

$$
\hat{\tau}(x) = \frac{\sum_{i \in L(x)} (2D_i - 1) Y_i}{\sum_{i \in L(x)} |2D_i - 1|}
$$

**ç†è«–ä¿è¨¼**:

- Pointwise consistency: $\hat{\tau}(x) \to \tau(x)$
- æ¼¸è¿‘æ­£è¦æ€§: $\sqrt{n}(\hat{\tau}(x) - \tau(x)) \xrightarrow{d} \mathcal{N}(0, \sigma^2(x))$

#### 3.8.2 Double/Debiased Machine Learning (DML)

**å•é¡Œ**: MLäºˆæ¸¬ã‚’å› æœæ¨è«–ã«ä½¿ã†ã¨æ­£å‰‡åŒ–ãƒã‚¤ã‚¢ã‚¹ãŒæ®‹ã‚‹

Chernozhukov et al. (2018) [^4] ã®è§£æ±ºç­–:

**1. Neyman-Orthogonal Score**

$$
\psi(W; \theta, \eta) = (Y - m(X)) - \theta (D - e(X))
$$

ã“ã“ã§ $\eta = (m, e)$ ã¯ nuisance ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€$\theta$ ã¯å› æœãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚

**2. Cross-Fitting**

ã‚µãƒ³ãƒ—ãƒ«ã‚’ $K$ åˆ†å‰² â†’ $k$ ç•ªç›®ã®foldã§ $\eta$ ã‚’æ¨å®š â†’ ä»–ã®foldã§ $\theta$ æ¨å®š â†’ é›†ç´„

**DMLæ¨å®šé‡**:

$$
\hat{\theta}_{\text{DML}} = \left(\frac{1}{n} \sum_i (D_i - \hat{e}(X_i))^2\right)^{-1} \frac{1}{n} \sum_i (D_i - \hat{e}(X_i))(Y_i - \hat{m}(X_i))
$$

**ç†è«–ä¿è¨¼**:

$$
\sqrt{n}(\hat{\theta}_{\text{DML}} - \theta) \xrightarrow{d} \mathcal{N}(0, V)
$$

MLæ¨å®šèª¤å·®ãŒ $o_P(n^{-1/4})$ ãªã‚‰ä¸åã€‚

#### 3.8.3 Meta-Learners (S/T/X/R-Learner)

**S-Learner** (Single model):

$$
\mu(X, D) = \mathbb{E}[Y \mid X, D], \quad \hat{\tau}(X) = \hat{\mu}(X, 1) - \hat{\mu}(X, 0)
$$

**T-Learner** (Two models):

$$
\mu_1(X) = \mathbb{E}[Y \mid X, D=1], \quad \mu_0(X) = \mathbb{E}[Y \mid X, D=0], \quad \hat{\tau}(X) = \hat{\mu}_1(X) - \hat{\mu}_0(X)
$$

**X-Learner** (å‡¦ç½®ç¾¤ãƒ»å¯¾ç…§ç¾¤ã®åå®Ÿä»®æƒ³ã‚’æ¨å®š):

1. $\hat{\mu}_1, \hat{\mu}_0$ ã‚’æ¨å®š
2. åå®Ÿä»®æƒ³: $\tilde{\tau}_1(X_i) = Y_i - \hat{\mu}_0(X_i)$ (å‡¦ç½®ç¾¤), $\tilde{\tau}_0(X_i) = \hat{\mu}_1(X_i) - Y_i$ (å¯¾ç…§ç¾¤)
3. $\hat{\tau}_1(X), \hat{\tau}_0(X)$ ã‚’ $\tilde{\tau}$ ã§å›å¸°
4. æœ€çµ‚æ¨å®š: $\hat{\tau}(X) = g(X) \hat{\tau}_1(X) + (1 - g(X)) \hat{\tau}_0(X)$

**R-Learner** (Robinsonå¤‰æ›):

$$
\tilde{Y} = Y - \hat{m}(X), \quad \tilde{D} = D - \hat{e}(X)
$$

$$
\hat{\tau}(X) = \arg\min_{\tau} \mathbb{E}[(\tilde{Y} - \tilde{D} \tau(X))^2]
$$

:::message alert
**ãƒœã‚¹æˆ¦: å› æœåŠ¹æœã®å®Œå…¨æ¨å®š**

ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã§å› æœåŠ¹æœã‚’æ¨å®šã›ã‚ˆ:

1. è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿: $(D, X, Y)$ with $n=5000$
2. æœªæ¸¬å®šäº¤çµ¡ $U$ ã‚ã‚Š
3. æ“ä½œå¤‰æ•° $Z$ (å¾´å…µãã˜) ãŒåˆ©ç”¨å¯èƒ½
4. ã‚«ãƒƒãƒˆã‚ªãƒ• $c=18$ (å¹´é½¢) ã§RDDé©ç”¨å¯èƒ½
5. 2æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š (DiDå¯èƒ½)

**ã‚¿ã‚¹ã‚¯**:
- å„æ‰‹æ³• (IPW, IV, RDD, DiD, Causal Forest) ã§ ATE æ¨å®š
- æ¨™æº–èª¤å·®ã‚’è¨ˆç®—
- çµæœã‚’æ¯”è¼ƒã—ã€æœ€ã‚‚é ‘å¥ãªæ¨å®šå€¤ã‚’é¸ã¶

ã“ã‚ŒãŒã§ãã‚Œã°æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œå…¨ã‚¯ãƒªã‚¢ï¼
:::

:::message
**é€²æ—: 50% å®Œäº†** å› æœæ¨è«–ç†è«–ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚Rubin/Pearl/å‚¾å‘ã‚¹ã‚³ã‚¢/IV/RDD/DiD/MLÃ—å› æœã‚’æ•°å¼ã‹ã‚‰å°å‡ºã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã§Julia + CausalInference.jlã§å…¨æ‰‹æ³•ã‚’å®Ÿè£…ã™ã‚‹ã€‚
:::

---
