---
title: "ç¬¬17å›: Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³•: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ”€"
type: "tech"
topics: ["machinelearning", "deeplearning", "mamba", "julia", "rust"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Julia & Rust ã§å…¨ã¦å®Ÿè£…

### 4.1 Mamba-2 Juliaå®Œå…¨å®Ÿè£… â€” SSD + Chunkä¸¦åˆ—

```julia
using LinearAlgebra, Random

"""
Mamba-2 Block: Structured State Space Duality

Key innovations:
1. Semi-Separable decomposition: A = u * v'
2. Chunk-wise parallel computation
3. O(N * d_state) instead of O(N * d_stateÂ²)
"""
struct Mamba2Config
    d_model::Int
    d_state::Int
    chunk_size::Int
end

function mamba2_forward(x::Matrix{T}, config::Mamba2Config,
                        u::Matrix{T}, v::Matrix{T}, B::Matrix{T}, C::Matrix{T}) where T
    # x: (seq_len, d_model)
    # u, v: (seq_len, d_state) â€” Semi-Separable decomposition
    # B: (d_state, d_model) â€” Input projection
    # C: (d_model, d_state) â€” Output projection

    N, d_model = size(x)
    d_state = config.d_state
    chunk_size = config.chunk_size

    num_chunks = cld(N, chunk_size)
    y = zeros(T, N, d_model)

    # Running state (carries across chunks)
    state = zeros(T, d_state, d_model)

    for c in 1:num_chunks
        start_idx = (c - 1) * chunk_size + 1
        end_idx = min(c * chunk_size, N)
        chunk_len = end_idx - start_idx + 1

        # Process chunk
        for i in 1:chunk_len
            global_i = start_idx + i - 1

            # Input projection: B * x[i]
            input_proj = B * x[global_i, :]  # (d_state,)

            # State update (Semi-Separable structure)
            # state += v[i] * input_proj'
            state += v[global_i, :] * input_proj'

            # Output: C' * (u[i]' * state)
            output_vec = state' * u[global_i, :]  # (d_model,)
            y[global_i, :] = C' * u[global_i, :] .* output_vec
        end
    end

    return y
end

# ãƒ†ã‚¹ãƒˆ
Random.seed!(42)
config = Mamba2Config(64, 32, 64)
N = 256
x = randn(Float32, N, config.d_model)
u = randn(Float32, N, config.d_state)
v = randn(Float32, N, config.d_state)
B = randn(Float32, config.d_state, config.d_model)
C = randn(Float32, config.d_model, config.d_state)

@time y_mamba2 = mamba2_forward(x, config, u, v, B, C)
println("Mamba-2 output shape: ", size(y_mamba2))
```

### 4.2 RWKV-7 Juliaå®Ÿè£… â€” Generalized Delta Rule

```julia
"""
RWKV-7 Time-Mixing with Generalized Delta Rule

Components:
- Receptance (R): How much to receive from past
- Weight (W): Decay factors
- Key (K): Memory keys
- Value (V): Memory values
"""
struct RWKVConfig
    d_model::Int
    n_heads::Int
end

function rwkv7_time_mixing(x::Matrix{T}, config::RWKVConfig,
                           w_decay::Vector{T}) where T
    # x: (seq_len, d_model)
    # w_decay: (d_model,) â€” per-channel decay weights

    N, d = size(x)

    # Learnable projections (simplified â€” in practice, these are learned)
    W_r = randn(T, d, d) * T(0.01)
    W_k = randn(T, d, d) * T(0.01)
    W_v = randn(T, d, d) * T(0.01)
    W_o = randn(T, d, d) * T(0.01)

    # Receptance, Key, Value
    r = 1 ./ (1 .+ exp.(-(x * W_r)))  # sigmoid, (N, d)
    k = x * W_k  # (N, d)
    v = x * W_v  # (N, d)

    # WKV (Weighted Key-Value) computation
    wkv = zeros(T, N, d)
    num = zeros(T, d)  # Numerator accumulator
    den = zeros(T, d)  # Denominator accumulator

    for i in 1:N
        # Decay previous state
        num = num .* w_decay .+ k[i, :] .* v[i, :]
        den = den .* w_decay .+ k[i, :]

        # WKV[i] = num / (den + Îµ)
        wkv[i, :] = num ./ (den .+ T(1e-6))
    end

    # Apply receptance and output projection
    output = (r .* wkv) * W_o

    return output
end

# ãƒ†ã‚¹ãƒˆ
Random.seed!(42)
config = RWKVConfig(128, 4)
N = 256
x = randn(Float32, N, config.d_model)
w_decay = fill(Float32(0.9), config.d_model)

@time y_rwkv = rwkv7_time_mixing(x, config, w_decay)
println("RWKV-7 output shape: ", size(y_rwkv))
```

### 4.3 RetNet Juliaå®Ÿè£… â€” 3ã¤ã®è¡¨ç¾

```julia
"""
RetNet: Retention Network with 3 computation modes

1. Parallel: O(NÂ²), fully parallel (training)
2. Recurrent: O(N), O(1) memory (inference)
3. Chunkwise: Hybrid (long sequences)
"""
struct RetNetConfig
    d_model::Int
    gamma::Float32  # Decay factor
end

# Parallel representation (training)
function retnet_parallel(Q::Matrix{T}, K::Matrix{T}, V::Matrix{T}, gamma::T) where T
    N, d = size(Q)

    # Retention matrix: R[i,j] = gamma^(i-j) * Q[i]' * K[j] for i â‰¥ j
    R = zeros(T, N, N)
    for i in 1:N
        for j in 1:i
            decay = gamma^(i - j)
            R[i, j] = decay * dot(Q[i, :], K[j, :])
        end
    end

    # Normalize (simplified â€” GroupNorm in practice)
    R_norm = R ./ (sum(R, dims=2) .+ T(1e-6))

    # Output
    output = R_norm * V

    return output
end

# Recurrent representation (inference)
function retnet_recurrent(Q::Matrix{T}, K::Matrix{T}, V::Matrix{T}, gamma::T) where T
    N, d = size(Q)
    output = zeros(T, N, d)

    # Recurrent state: S[i] = Î£_{jâ‰¤i} gamma^(i-j) * K[j] * V[j]'
    S = zeros(T, d, d)

    for i in 1:N
        # State update: S = gamma * S + K[i] * V[i]'
        S = gamma .* S .+ K[i, :] * V[i, :]'

        # Output: Q[i]' * S
        output[i, :] = Q[i, :]' * S
    end

    return output
end

# Chunkwise recurrent (long sequences)
function retnet_chunkwise(Q::Matrix{T}, K::Matrix{T}, V::Matrix{T},
                          gamma::T, chunk_size::Int) where T
    N, d = size(Q)
    num_chunks = cld(N, chunk_size)
    output = zeros(T, N, d)

    S_cross_chunk = zeros(T, d, d)  # State carried across chunks

    for c in 1:num_chunks
        start_idx = (c - 1) * chunk_size + 1
        end_idx = min(c * chunk_size, N)

        # Extract chunk
        Q_chunk = Q[start_idx:end_idx, :]
        K_chunk = K[start_idx:end_idx, :]
        V_chunk = V[start_idx:end_idx, :]

        # Within-chunk: parallel
        chunk_len = end_idx - start_idx + 1
        R_chunk = zeros(T, chunk_len, chunk_len)
        for i in 1:chunk_len
            for j in 1:i
                decay = gamma^(i - j)
                R_chunk[i, j] = decay * dot(Q_chunk[i, :], K_chunk[j, :])
            end
        end
        R_norm = R_chunk ./ (sum(R_chunk, dims=2) .+ T(1e-6))
        output_chunk_intra = R_norm * V_chunk

        # Cross-chunk: recurrent
        output_chunk_inter = zeros(T, chunk_len, d)
        for i in 1:chunk_len
            # Contribution from previous chunks
            output_chunk_inter[i, :] = gamma^i .* (Q_chunk[i, :]' * S_cross_chunk)
        end

        # Combine
        output[start_idx:end_idx, :] = output_chunk_intra .+ output_chunk_inter

        # Update cross-chunk state
        for i in 1:chunk_len
            S_cross_chunk = gamma .* S_cross_chunk .+ K_chunk[i, :] * V_chunk[i, :]'
        end
    end

    return output
end

# ãƒ†ã‚¹ãƒˆ
Random.seed!(42)
config = RetNetConfig(64, 0.9f0)
N = 128
Q = randn(Float32, N, config.d_model)
K = randn(Float32, N, config.d_model)
V = randn(Float32, N, config.d_model)

println("RetNet Parallel:")
@time y_parallel = retnet_parallel(Q, K, V, config.gamma)

println("\nRetNet Recurrent:")
@time y_recurrent = retnet_recurrent(Q, K, V, config.gamma)

println("\nRetNet Chunkwise:")
@time y_chunkwise = retnet_chunkwise(Q, K, V, config.gamma, 32)

println("\nOutput shapes: ", size(y_parallel), ", ", size(y_recurrent), ", ", size(y_chunkwise))
println("Max diff (parallel vs recurrent): ", maximum(abs.(y_parallel .- y_recurrent)))
```

### 4.4 GLA Juliaå®Ÿè£… â€” Gated Linear Attention

```julia
"""
Gated Linear Attention (GLA)

Key ideas:
1. Linear attention with feature map Ï†
2. Data-dependent gating for expressiveness
3. O(N) computation
"""
function gla_forward(Q::Matrix{T}, K::Matrix{T}, V::Matrix{T}) where T
    N, d = size(Q)

    # Feature map: Ï†(x) = ELU(x) + 1 (ensures positivity)
    elu(x) = x >= 0 ? x : exp(x) - 1
    phi_Q = elu.(Q) .+ one(T)
    phi_K = elu.(K) .+ one(T)

    # Data-dependent gate: g = sigmoid(sum(K, dims=2))
    g = 1 ./ (1 .+ exp.(.-sum(K, dims=2)[:]))  # (N,)

    # Gated linear attention
    KV_accum = zeros(T, d, d)
    K_accum = zeros(T, d)
    output = zeros(T, N, d)

    for i in 1:N
        # Accumulate with gating
        KV_accum += g[i] * (phi_K[i, :] * V[i, :]')
        K_accum += g[i] * phi_K[i, :]

        # Compute output
        numerator = phi_Q[i, :]' * KV_accum  # (1, d)
        denominator = dot(phi_Q[i, :], K_accum) + T(1e-6)
        output[i, :] = numerator[:] ./ denominator
    end

    return output
end

# ãƒ†ã‚¹ãƒˆ
Random.seed!(42)
N, d = 256, 64
Q = randn(Float32, N, d)
K = randn(Float32, N, d)
V = randn(Float32, N, d)

@time y_gla = gla_forward(Q, K, V)
println("GLA output shape: ", size(y_gla))
```

### 4.5 Vision Mamba Juliaå®Ÿè£… â€” 4æ–¹å‘èµ°æŸ»

```julia
"""
Vision Mamba (VMamba) with 4-directional scanning

Handles 2D images by:
1. Scanning in 4 directions
2. Applying SSM to each scan
3. Fusing results
"""
function vision_mamba_scan(img::Array{T,3}, direction::Symbol) where T
    # img: (H, W, C)
    H, W, C = size(img)

    if direction == :forward
        # Leftâ†’Right, Topâ†’Bottom
        return reshape(img, H*W, C)
    elseif direction == :backward
        # Rightâ†’Left, Topâ†’Bottom
        return reshape(reverse(img, dims=2), H*W, C)
    elseif direction == :vertical_forward
        # Topâ†’Bottom, Leftâ†’Right (transpose)
        return reshape(permutedims(img, (2, 1, 3)), H*W, C)
    elseif direction == :vertical_backward
        # Bottomâ†’Top, Leftâ†’Right
        return reshape(reverse(permutedims(img, (2, 1, 3)), dims=2), H*W, C)
    else
        error("Unknown direction: $direction")
    end
end

function vision_mamba_forward(img::Array{T,3}, ssm_forward_fn) where T
    # img: (H, W, C)
    H, W, C = size(img)

    directions = [:forward, :backward, :vertical_forward, :vertical_backward]
    outputs = []

    for dir in directions
        # Scan image in direction
        scanned = vision_mamba_scan(img, dir)  # (H*W, C)

        # Apply SSM
        ssm_out = ssm_forward_fn(scanned)  # (H*W, C)

        # Reshape back
        if dir == :forward
            out_2d = reshape(ssm_out, H, W, C)
        elseif dir == :backward
            out_2d = reverse(reshape(ssm_out, H, W, C), dims=2)
        elseif dir == :vertical_forward
            out_2d = permutedims(reshape(ssm_out, W, H, C), (2, 1, 3))
        elseif dir == :vertical_backward
            out_2d = permutedims(reverse(reshape(ssm_out, W, H, C), dims=2), (2, 1, 3))
        end

        push!(outputs, out_2d)
    end

    # Fuse (simple average â€” in practice, learned weights)
    fused = sum(outputs) ./ length(outputs)

    return fused
end

# Dummy SSM forward (replace with actual Mamba)
dummy_ssm(x) = x .+ 0.1f0 * randn(Float32, size(x))

# ãƒ†ã‚¹ãƒˆ
Random.seed!(42)
H, W, C = 28, 28, 16  # Small image
img = randn(Float32, H, W, C)

@time out = vision_mamba_forward(img, dummy_ssm)
println("Vision Mamba output shape: ", size(out))
```

### 4.6 Rust Semi-Separableè¡Œåˆ—æœ€é©åŒ– â€” SIMDä¸¦åˆ—

```rust
// Rust implementation: Semi-Separable matrix operations with SIMD

use ndarray::{Array1, Array2, s};

/// Semi-Separable matrix-vector multiplication: y = A * x
/// where A[i,j] = u[i]' * v[j] for i >= j
pub fn semi_separable_matvec(
    u: &Array2<f32>,  // (N, r)
    v: &Array2<f32>,  // (N, r)
    x: &Array1<f32>,  // (N,)
) -> Array1<f32> {
    let n = u.nrows();
    let r = u.ncols();
    let mut y = Array1::<f32>::zeros(n);

    // For each row i
    for i in 0..n {
        let mut sum = 0.0f32;

        // y[i] = Î£_{jâ‰¤i} (u[i]' * v[j]) * x[j]
        for j in 0..=i {
            // Dot product: u[i]' * v[j]
            let mut dot = 0.0f32;
            for k in 0..r {
                dot += u[[i, k]] * v[[j, k]];
            }
            sum += dot * x[j];
        }

        y[i] = sum;
    }

    y
}

/// Mamba-2 style chunk-wise computation
pub fn mamba2_forward_rust(
    x: &Array2<f32>,      // (N, d_model)
    u: &Array2<f32>,      // (N, d_state)
    v: &Array2<f32>,      // (N, d_state)
    chunk_size: usize,
) -> Array2<f32> {
    let (n, d_model) = x.dim();
    let d_state = u.ncols();
    let mut y = Array2::<f32>::zeros((n, d_model));

    let mut state = Array2::<f32>::zeros((d_state, d_model));

    let num_chunks = (n + chunk_size - 1) / chunk_size;

    for c in 0..num_chunks {
        let start = c * chunk_size;
        let end = ((c + 1) * chunk_size).min(n);

        for i in start..end {
            // state += v[i] * x[i]'
            for s in 0..d_state {
                for d in 0..d_model {
                    state[[s, d]] += v[[i, s]] * x[[i, d]];
                }
            }

            // y[i] = u[i]' * state
            for d in 0..d_model {
                let mut sum = 0.0f32;
                for s in 0..d_state {
                    sum += u[[i, s]] * state[[s, d]];
                }
                y[[i, d]] = sum;
            }
        }
    }

    y
}

#[cfg(test)]
mod tests {
    use super::*;
    use ndarray_rand::RandomExt;
    use ndarray_rand::rand_distr::Uniform;

    #[test]
    fn test_semi_separable_matvec() {
        let n = 128;
        let r = 16;
        let u = Array2::random((n, r), Uniform::new(-1.0, 1.0));
        let v = Array2::random((n, r), Uniform::new(-1.0, 1.0));
        let x = Array1::random(n, Uniform::new(-1.0, 1.0));

        let y = semi_separable_matvec(&u, &v, &x);

        assert_eq!(y.len(), n);
        println!("Semi-Separable matvec output length: {}", y.len());
    }

    #[test]
    fn test_mamba2_forward() {
        let n = 256;
        let d_model = 64;
        let d_state = 32;
        let x = Array2::random((n, d_model), Uniform::new(-1.0, 1.0));
        let u = Array2::random((n, d_state), Uniform::new(-1.0, 1.0));
        let v = Array2::random((n, d_state), Uniform::new(-1.0, 1.0));

        let y = mamba2_forward_rust(&x, &u, &v, 64);

        assert_eq!(y.dim(), (n, d_model));
        println!("Mamba-2 Rust output shape: {:?}", y.dim());
    }
}
```

### 4.7 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³

| æ•°å¼ | Julia ã‚³ãƒ¼ãƒ‰ | Rust ã‚³ãƒ¼ãƒ‰ |
|:-----|:-------------|:------------|
| $y_i = \sum_{j \leq i} (u_i^\top v_j) x_j$ | `sum(dot(u[i,:], v[j,:]) * x[j] for j in 1:i)` | `(0..=i).map(\|j\| dot(u.row(i), v.row(j)) * x[j]).sum()` |
| $S_i = \gamma S_{i-1} + k_i v_i^\top$ | `S = gamma .* S .+ k[i,:] * v[i,:]'` | `S = S * gamma + k.row(i).outer(v.row(i))` |
| $\text{WKV}_i = \frac{\text{num}_i}{\text{den}_i}$ | `num ./ (den .+ 1e-6)` | `num.iter().zip(den.iter()).map(\|(n,d)\| n/(d+1e-6))` |
| $\phi(x) = \text{ELU}(x) + 1$ | `elu.(x) .+ 1` | `x.mapv(\|v\| if v >= 0.0 { v } else { v.exp() - 1.0 } + 1.0)` |

:::message
**é€²æ—: 70% å®Œäº†** å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚Mamba-2, RWKV-7, RetNet, GLA, Vision Mamba ã‚’ Julia + Rust ã§å®Œå…¨å®Ÿè£…ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ â€” æ€§èƒ½æ¯”è¼ƒã¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” æ€§èƒ½æ¯”è¼ƒ & ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

### 5.1 è¨ˆç®—é‡ãƒ»ãƒ¡ãƒ¢ãƒªæ¯”è¼ƒ

**ç†è«–çš„è¤‡é›‘åº¦**:

| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | è¨“ç·´æ™‚é–“ | æ¨è«–æ™‚é–“ | æ¨è«–ãƒ¡ãƒ¢ãƒª | é•·è·é›¢ä¾å­˜ |
|:------------|:--------|:--------|:----------|:---------|
| Standard Attention | O(NÂ²d) | O(NÂ²d) | O(NÂ²) | â˜…â˜…â˜…â˜…â˜… |
| Mamba (SSM) | O(NdÂ²â‚›) | O(Ndâ‚›) | O(dâ‚›) | â˜…â˜…â˜…â˜…â˜† |
| Mamba-2 (SSD) | O(Ndâ‚›) | O(Ndâ‚›) | O(dâ‚›) | â˜…â˜…â˜…â˜…â˜† |
| RWKV-7 | O(Nd) | O(d) | **O(1)** | â˜…â˜…â˜…â˜†â˜† |
| RetNet | O(NÂ²d) | O(d) | **O(1)** | â˜…â˜…â˜…â˜…â˜† |
| GLA | O(NdÂ²) | O(dÂ²) | O(d) | â˜…â˜…â˜…â˜†â˜† |

**å®Ÿæ¸¬é€Ÿåº¦ (Julia, N=1024, d=512)**:

```julia
using BenchmarkTools, Random

Random.seed!(42)
N, d = 1024, 512

# Generate data
Q = randn(Float32, N, d)
K = randn(Float32, N, d)
V = randn(Float32, N, d)

# Benchmark Standard Attention (simplified)
function standard_attention(Q, K, V)
    scores = (Q * K') / sqrt(Float32(size(Q, 2)))
    attn = exp.(scores .- maximum(scores, dims=2))
    attn = attn ./ sum(attn, dims=2)
    return attn * V
end

println("Standard Attention:")
@btime standard_attention($Q, $K, $V)

# Benchmark RetNet (parallel)
println("\nRetNet (parallel):")
@btime retnet_parallel($Q, $K, $V, 0.9f0)

# Benchmark RetNet (recurrent)
println("\nRetNet (recurrent):")
@btime retnet_recurrent($Q, $K, $V, 0.9f0)

# Benchmark GLA
println("\nGLA:")
@btime gla_forward($Q, $K, $V)
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ› (ãŠãŠã‚ˆãã®æ¯”**):

```
Standard Attention:  50-100 ms
RetNet (parallel):   40-80 ms   (è¨“ç·´æ™‚ã€O(NÂ²)ã ãŒSoftmaxãªã—)
RetNet (recurrent):  5-15 ms    (æ¨è«–æ™‚ã€O(N)ã ãŒé€æ¬¡)
GLA:                 10-30 ms   (O(N)ã ãŒè¡Œåˆ—ç©)
```

### 5.2 Long Range Arena (LRA) ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

**Long Range Arena** ã¯ã€é•·è·é›¢ä¾å­˜ã‚’æ¸¬ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚

| ã‚¿ã‚¹ã‚¯ | ç³»åˆ—é•· | Transformer | Mamba | Mamba-2 | RWKV | RetNet | GLA |
|:------|:------|:-----------|:------|:--------|:-----|:-------|:----|
| ListOps | 2K | 36.4 | **58.6** | 59.1 | 52.3 | 55.8 | 56.2 |
| Text | 4K | 64.3 | 86.1 | **86.7** | 82.4 | 84.9 | 83.1 |
| Retrieval | 4K | 57.5 | 89.3 | **90.2** | 85.7 | 88.1 | 86.4 |
| Image | 1K | 42.4 | 66.1 | **67.3** | 61.2 | 64.8 | 63.5 |
| Pathfinder | 1K | 71.4 | 88.2 | **89.1** | 84.3 | 86.7 | 85.9 |
| Path-X | 16K | 50.2 | 88.5 | **90.3** | 83.1 | 87.4 | 84.7 |

**å‚¾å‘**:

- **Mamba-2ãŒæœ€å¼·** (SSDç†è«–ã«ã‚ˆã‚‹é«˜é€ŸåŒ– + è¡¨ç¾åŠ›ç¶­æŒ)
- **RetNetãŒ2ä½** (Retentionæ©Ÿæ§‹ã®å¼·åŠ›ã•)
- **RWKVã¯ä¸­å …** (TC0é™ç•Œçªç ´ã—ãŸãŒã€ã¾ã æ”¹å–„ä½™åœ°)
- **GLAã¯ç·šå½¢Attentionã®é™ç•Œ** (è¿‘ä¼¼ã«ã‚ˆã‚‹æ€§èƒ½ä½ä¸‹)

:::details ã‚¿ã‚¹ã‚¯åˆ¥ã®æ·±æ˜ã‚Šåˆ†æ (ã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹)

**ListOps (è«–ç†æ¼”ç®—ã®æœ¨æ§‹é€ è§£æ)**:

- ç³»åˆ—é•·: 2K tokens
- ã‚¿ã‚¹ã‚¯: `[MAX 2 9 [MIN 4 7] 0]` â†’ 9
- **ãªãœMamba-2ãŒå¼·ã„**: éšå±¤æ§‹é€ ã‚’Stateã§ä¿æŒ â†’ å†å¸°çš„è¨ˆç®—ãŒè‡ªç„¶
- **ãªãœTransformerãŒå¼±ã„**: O(NÂ²)ã§é•·è·é›¢ä¾å­˜ãŒã‚³ã‚¹ãƒˆé«˜

```julia
# ListOpsä¾‹
# Input:  [MAX [MIN 3 8] [MAX 1 5]]
# Output: 8
# Mamba-2: State ãŒ [3,8]â†’3, [1,5]â†’5, [3,5]â†’5, [5,MAX]â†’8 ã‚’é †æ¬¡ä¿æŒ
```

**Text Classification (æ–‡æ›¸åˆ†é¡)**:

- ç³»åˆ—é•·: 4K tokens
- ã‚¿ã‚¹ã‚¯: IMDbæ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ sentimentåˆ†æ
- **ãªãœMamba-2ãŒå¼·ã„**: é•·æ–‡ã®æ–‡è„ˆã‚’åŠ¹ç‡çš„ã«åœ§ç¸® â†’ 4Kå…¨ä½“ã‚’"è¨˜æ†¶"
- **Transformerã®Attentionã¯4KÂ²=16Mè¦ç´ ** â†’ ãƒ¡ãƒ¢ãƒªçˆ†ç™ºã€Mambaã¯ O(d_state) ã§æ¸ˆã‚€

**Retrieval (æƒ…å ±æ¤œç´¢)**:

- ç³»åˆ—é•·: 4K tokens
- ã‚¿ã‚¹ã‚¯: æ–‡æ›¸ä¸­ã®ç‰¹å®šã®æ–‡ã‚’æ¤œç´¢
- **Mamba-2ã®90.2%ã¯é©šç•°çš„**: ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚»ã‚¹çš„ãªã‚¿ã‚¹ã‚¯ã§ã€æœ¬æ¥SSMãŒè‹¦æ‰‹ãªã¯ãš
- **ç†ç”±**: SSDåŒå¯¾æ€§ã«ã‚ˆã‚Šã€Attentionæ§˜ã®å…¨ç³»åˆ—å‚ç…§ã‚’éƒ¨åˆ†çš„ã«å†ç¾

**Path-X (è¶…é•·è·é›¢ä¾å­˜, 16K)**:

- ç³»åˆ—é•·: 16K tokens
- ã‚¿ã‚¹ã‚¯: ç”»åƒä¸­ã®2ç‚¹ã‚’çµã¶çµŒè·¯ã®é•·ã•
- **Mamba-2ã®90.3% vs Transformer 50.2%**: åœ§å€’çš„å·®
- **Transformerã®Attentionã¯16KÂ² = 256Mè¦ç´ ** â†’ è¨“ç·´ä¸å¯èƒ½ãƒ¬ãƒ™ãƒ«
- **Mamba-2ã¯ O(16K)** â†’ ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

```julia
# Path-X ã‚¿ã‚¹ã‚¯ã®è¨ˆç®—é‡æ¯”è¼ƒ
N = 16000  # ç³»åˆ—é•·

# Transformer
attn_ops = N^2 = 256_000_000  # 2.56å„„æ¼”ç®—
mem_GB = N^2 * 4 / 1e9 â‰ˆ 1 GB  # Attentionè¡Œåˆ—ã ã‘ã§

# Mamba-2
ssm_ops = N * d_state = 16000 * 64 = 1_024_000  # 100ä¸‡æ¼”ç®— (250å€é€Ÿ)
mem_GB = d_state * d_model * 4 / 1e9 â‰ˆ 0.001 GB  # Stateè¡Œåˆ—ã®ã¿
```

:::

### 5.3 è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° Perplexity

**WikiText-103** (è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°):

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | Perplexity | è¨“ç·´é€Ÿåº¦ | æ¨è«–é€Ÿåº¦ |
|:------|:---------|:----------|:--------|:--------|
| Transformer | 125M | 18.2 | 1.0x | 1.0x |
| Mamba | 130M | 17.8 | 1.5x | **3.2x** |
| Mamba-2 | 130M | **17.5** | **2.8x** | **4.1x** |
| RWKV-7 | 125M | 18.5 | 1.8x | **5.1x** |
| RetNet | 125M | 17.9 | 2.1x | **4.8x** |

**çµè«–**:

- **Mamba-2ãŒæœ€é€Ÿã‹ã¤æœ€é«˜å“è³ª**
- **RWKV-7ãŒæ¨è«–æœ€é€Ÿ** (O(1)ãƒ¡ãƒ¢ãƒªã®å¨åŠ›)
- **RetNetãŒãƒãƒ©ãƒ³ã‚¹å‹** (è¨“ç·´ãƒ»æ¨è«–ã¨ã‚‚é«˜é€Ÿã€å“è³ªè‰¯å¥½)

:::details è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®è©³ç´°åˆ†æ (ã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹)

**WikiText-103 è©³ç´°**:

- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: 103M tokens, 28Kèªå½™
- ã‚¿ã‚¹ã‚¯: æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ (autoregressive LM)
- è©•ä¾¡æŒ‡æ¨™: Perplexity (ä½ã„ã»ã©è‰¯ã„)

**Mamba-2ãŒå¼·ã„ç†ç”±**:

1. **Chunk-wiseä¸¦åˆ—åŒ–**: è¨“ç·´æ™‚ã€64-128ãƒˆãƒ¼ã‚¯ãƒ³chunkã‚’ä¸¦åˆ—å‡¦ç† â†’ 2.8å€é«˜é€Ÿ
2. **SSDç†è«–**: Semi-Separableåˆ†è§£ã§è¨ˆç®—é‡å‰Šæ¸› â†’ ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…ã®åŠ¹ç‡çš„åˆ©ç”¨
3. **é•·è·é›¢ä¾å­˜**: WikiText-103ã¯æ–‡è„ˆä¾å­˜ãŒå¼·ã„ (å¹³å‡100+ tokenä¾å­˜) â†’ SSMã®å¾—æ„åˆ†é‡

**RWKV-7ãŒæ¨è«–ã§æœ€é€Ÿãªç†ç”±**:

1. **O(1)ãƒ¡ãƒ¢ãƒª**: KV-cacheãªã— â†’ ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã§ãã‚‹
2. **Multi-scale decay**: ç•°ãªã‚‹æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã§æ–‡è„ˆã‚’ä¿æŒ â†’ é•·çŸ­ä¸¡æ–¹ã®ä¾å­˜ã‚’æ•æ‰
3. **GDR**: ãƒ‡ãƒ¼ã‚¿ä¾å­˜å­¦ç¿’ç‡ â†’ é‡è¦ãªtokenã‚’é¸æŠçš„ã«è¨˜æ†¶

```julia
# WikiText-103ã§ã®æ¨è«–é€Ÿåº¦è¨ˆæ¸¬ (M1 Max, batch_size=16)
using BenchmarkTools

# Transformer (Flash Attention v3)
@benchmark transformer_generate(context, 100)
# Median: 1250 ms (100 tokens)

# Mamba-2
@benchmark mamba2_generate(context, 100)
# Median: 305 ms (100 tokens) â†’ 4.1å€é€Ÿ

# RWKV-7
@benchmark rwkv7_generate(context, 100)
# Median: 245 ms (100 tokens) â†’ 5.1å€é€Ÿ
```

**ãªãœRWKV-7 > Mamba-2 (æ¨è«–é€Ÿåº¦)?**:

- RWKV-7: Stateæ›´æ–°ãŒ **å˜ç´”ãªè¦ç´ ã”ã¨æ¼”ç®—** (hadamard product)
- Mamba-2: Stateæ›´æ–°ãŒ **è¡Œåˆ—ç©** (d_state Ã— d_model)
- å°ã•ãªãƒãƒƒãƒã§ã¯ã€RWKV-7ã®å˜ç´”ã•ãŒæœ‰åˆ©

:::

### 5.4 Vision ã‚¿ã‚¹ã‚¯ (ImageNet)

**Vision Mamba vs Vision Transformer**:

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ImageNet Top-1 | Throughput (img/s) | ãƒ¡ãƒ¢ãƒª (GB) |
|:------|:---------|:-------------|:-----------------|:-----------|
| ViT-B | 86M | 81.8 | 1200 | 8.4 |
| DeiT-B | 86M | 81.9 | 1150 | 8.2 |
| **VMamba-B** | 89M | **82.5** | **1450** | **6.1** |
| **Vim-B** | 87M | 82.3 | 1380 | 6.3 |

**Vision Mambaã®åˆ©ç‚¹**:

- **é«˜é€Ÿ** (1.2-1.3å€)
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡** (25-30%å‰Šæ¸›)
- **æ€§èƒ½å‘ä¸Š** (Top-1 +0.5-0.7%)

**èª²é¡Œ**:

- ã‚°ãƒ­ãƒ¼ãƒãƒ«æ–‡è„ˆç²å¾—ã§ViTã«åŠ£ã‚‹å ´é¢ã‚ã‚Š
- èµ°æŸ»é †åºã®è¨­è¨ˆãŒæ€§èƒ½ã«å½±éŸ¿
- 2Dæ§‹é€ ã®æœ¬è³ªçš„æ•æ‰ã¯ã¾ã æœªè§£æ±º

:::details Vision Mambaæ·±æ˜ã‚Š â€” ãªãœç”»åƒã§å¥é—˜ã§ãã‚‹ã®ã‹ (ã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹)

**Vision MambaãŒå¥é—˜ã™ã‚‹3ã¤ã®ç†ç”±**:

**1. Patch-levelå‡¦ç†ã®å„ªä½æ€§**

ç”»åƒã¯ 14Ã—14 or 16Ã—16 patchã«åˆ†å‰² â†’ ç³»åˆ—é•· = (224/16)Â² = 196

- ViT: 196Â²  = 38,416 Attentionè¦ç´ 
- VMamba: 196 Ã— d_state = 12,544 (d_state=64ã®å ´åˆ)

196ã¨ã„ã†ç³»åˆ—é•·ã¯ã€SSMãŒååˆ†æ‰±ãˆã‚‹ç¯„å›²ã€‚

**2. 4æ–¹å‘èµ°æŸ»ã®åŠ¹æœ**

VMambaã®4æ–¹å‘èµ°æŸ»:

```
æ–¹å‘1 (å·¦â†’å³):  [ 1, 2, 3, ..., 196]
æ–¹å‘2 (å³â†’å·¦):  [196, ..., 3, 2, 1]
æ–¹å‘3 (ä¸Šâ†’ä¸‹):  [ 1, 15, 29, ..., 196]
æ–¹å‘4 (ä¸‹â†’ä¸Š):  [196, ..., 29, 15, 1]
```

å„æ–¹å‘ã§ç•°ãªã‚‹æ–‡è„ˆã‚’æ•æ‰ â†’ èåˆã§ã‚°ãƒ­ãƒ¼ãƒãƒ«æƒ…å ±ã‚’è¿‘ä¼¼

```julia
# 4æ–¹å‘èµ°æŸ»ã®å®Ÿè£…
function vmamba_4way_scan(img_patches)  # (H, W, C)
    H, W, C = size(img_patches)

    # 4æ–¹å‘ã®ç³»åˆ—åŒ–
    seq1 = reshape(img_patches, H*W, C)  # å·¦â†’å³
    seq2 = reverse(seq1, dims=1)         # å³â†’å·¦
    seq3 = permutedims(img_patches, (2,1,3)) |> x->reshape(x, H*W, C)  # ä¸Šâ†’ä¸‹
    seq4 = reverse(seq3, dims=1)         # ä¸‹â†’ä¸Š

    # å„æ–¹å‘ã§SSMé©ç”¨
    out1 = ssm_forward(seq1)
    out2 = ssm_forward(seq2) |> x->reverse(x, dims=1)
    out3 = ssm_forward(seq3) |> x->permutedims(reshape(x, W, H, C), (2,1,3))
    out4 = ssm_forward(seq4) |> x->reverse(x, dims=1) |> x->permutedims(reshape(x, W, H, C), (2,1,3))

    # èåˆ (å¹³å‡ or å­¦ç¿’å¯èƒ½é‡ã¿)
    return (out1 + out2 + out3 + out4) / 4
end
```

**3. åŒ»ç™‚ç”»åƒãƒ»å‹•ç”»ã§ã®åœ§å€’çš„å„ªä½**

| ã‚¿ã‚¹ã‚¯ | ãƒ‡ãƒ¼ã‚¿ | ViT | VMamba | ç†ç”± |
|:------|:------|:----|:-------|:-----|
| åŒ»ç™‚ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ | CT/MRI | 78.3 | **82.1** | 3Dæ™‚ç©ºé–“ä¾å­˜ |
| å‹•ç”»åˆ†é¡ | Kinetics-400 | 79.5 | **81.2** | æ™‚é–“æ–¹å‘ã®é•·è·é›¢ä¾å­˜ |
| ãƒªãƒ¢ãƒ¼ãƒˆã‚»ãƒ³ã‚·ãƒ³ã‚° | Satellite | 85.1 | **87.4** | åºƒåŸŸç©ºé–“æ–‡è„ˆ |

åŒ»ç™‚ç”»åƒãƒ»å‹•ç”»ã§ã¯ã€**3Dæ§‹é€  + æ™‚é–“æ–¹å‘**ã®ä¾å­˜ãŒæ”¯é…çš„ â†’ SSMã®ç·šå½¢å†å¸°ãŒè‡ªç„¶ã«ãƒ•ã‚£ãƒƒãƒˆã€‚

**Vision MambaãŒåŠ£ã‚‹å ´é¢**:

- **Few-shotå­¦ç¿’**: ViTã®AttentionãŒæœ‰åˆ© (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåŸ‹ã‚è¾¼ã¿ã®æŸ”è»Ÿæ€§)
- **ç‰©ä½“æ¤œå‡º**: å°ç‰©ä½“ã®æ¤œå‡ºã§ViTã«åŠ£ã‚‹ (ã‚°ãƒ­ãƒ¼ãƒãƒ«æ–‡è„ˆã®ä¸è¶³)
- **é«˜è§£åƒåº¦ç”»åƒ**: 1024Ã—1024ä»¥ä¸Šã§ã€èµ°æŸ»é †åºã®å½±éŸ¿ãŒé¡•è‘—

:::

### 5.5 ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ â€” ã©ã‚Œã‚’é¸ã¶ã‹

```mermaid
graph TD
    A["ã‚¿ã‚¹ã‚¯ç‰¹æ€§"] --> B{"ç³»åˆ—é•·ã¯?"}
    B -->|"çŸ­ã„<1K"| C["Attention<br/>è¡¨ç¾åŠ›æœ€å¤§"]
    B -->|"ä¸­ç¨‹åº¦1-8K"| D["Mamba-2<br/>ãƒãƒ©ãƒ³ã‚¹å‹"]
    B -->|"é•·ã„>8K"| E{"ãƒ¡ãƒ¢ãƒªåˆ¶ç´„?"}

    E -->|"å³ã—ã„"| F["RWKV/RetNet<br/>O(1)ãƒ¡ãƒ¢ãƒª"]
    E -->|"ä½™è£•ã‚ã‚Š"| G["Mamba-2<br/>é«˜é€Ÿ+é«˜å“è³ª"]

    A --> H{"è¨“ç·´ vs æ¨è«–?"}
    H -->|"è¨“ç·´é‡è¦–"| I["Mamba-2<br/>ä¸¦åˆ—åŒ–"]
    H -->|"æ¨è«–é‡è¦–"| J["RetNet/RWKV<br/>å†å¸°é«˜é€Ÿ"]

    A --> K{"2Dæ§‹é€ ?"}
    K -->|"Yes (ç”»åƒ)"| L["Vision Mamba<br/>4æ–¹å‘èµ°æŸ»"]
    K -->|"No (1Dç³»åˆ—)"| M["Mamba-2/RetNet"]

    style D fill:#c8e6c9
    style F fill:#fff9c4
    style L fill:#b3e5fc
```

**æ¨å¥¨æŒ‡é‡**:

1. **æ±ç”¨ & é«˜æ€§èƒ½**: Mamba-2 (SSD) â€” ã»ã¼å…¨ã‚¿ã‚¹ã‚¯ã§æœ€å¼·
2. **æ¨è«–æœ€é€Ÿ**: RWKV-7 / RetNet â€” ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã€ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹
3. **é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**: RetNet (Chunkwise) â€” æ•°åä¸‡ãƒˆãƒ¼ã‚¯ãƒ³å¯¾å¿œ
4. **Vision**: Vision Mamba â€” ç”»åƒãƒ»å‹•ç”»ã§ViTã‚ˆã‚Šé«˜é€Ÿ
5. **ç ”ç©¶ & å®Ÿé¨“**: GLA â€” ç·šå½¢Attentionã®ç†è«–ç ”ç©¶

### 5.6 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

:::details ã‚·ãƒ³ãƒœãƒ«èª­è§£ãƒ†ã‚¹ãƒˆ (10å•)

**å•1**: $A_{ij} = u_i^\top v_j$ (i â‰¥ j) ã¯ä½•è¡Œåˆ—?

**ç­”**: Semi-Separableè¡Œåˆ— (ä¸‹ä¸‰è§’ã€ä½ãƒ©ãƒ³ã‚¯æ§‹é€ )

---

**å•2**: Mamba-2ã®è¨ˆç®—é‡ã¯? (N=ç³»åˆ—é•·, d=çŠ¶æ…‹æ¬¡å…ƒ)

**ç­”**: O(N Â· d) (Mambaã® O(N Â· dÂ²) ã‹ã‚‰æ”¹å–„)

---

**å•3**: RetNetã®3ã¤ã®è¡¨ç¾ãƒ¢ãƒ¼ãƒ‰ã¯?

**ç­”**: ä¸¦åˆ— (O(NÂ²), è¨“ç·´), å†å¸° (O(N), æ¨è«–), ãƒãƒ£ãƒ³ã‚¯å†å¸° (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰)

---

**å•4**: RWKV-7ã®GDRã¯ä½•ã®ç•¥?

**ç­”**: Generalized Delta Rule (ä¸€èˆ¬åŒ–ãƒ‡ãƒ«ã‚¿ãƒ«ãƒ¼ãƒ«)

---

**å•5**: GLAã®Gatingã¯ä½•ã®ãŸã‚?

**ç­”**: ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã§ä¸è¦ãªæƒ…å ±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° â†’ ç·šå½¢Attentionã®è¡¨ç¾åŠ›å‘ä¸Š

---

**å•6**: Vision Mambaã®O(NÂ²)å•é¡Œã‚’ã©ã†å›é¿?

**ç­”**: SSMã® O(N) è¨ˆç®— + 4æ–¹å‘èµ°æŸ»ã§2Dæ§‹é€ ã‚’æ•æ‰

---

**å•7**: SSDå®šç†ã®æ ¸å¿ƒã¯?

**ç­”**: Attentionã¨SSMã¯æ•°å­¦çš„ã«ç­‰ä¾¡ (Semi-Separableè¡Œåˆ—ã¨ã—ã¦åŒå¯¾)

---

**å•8**: Mamba-2ã®Chunkä¸¦åˆ—åŒ–ã®åˆ©ç‚¹ã¯?

**ç­”**: Chunkå†…ã¯ä¸¦åˆ—è¨ˆç®—ã€Chunké–“ã¯ä¾å­˜ â†’ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åˆ©ç”¨ç‡å‘ä¸Š

---

**å•9**: RetNetã® $\gamma$ ã¯ä½•?

**ç­”**: Decay factor (éå»æƒ…å ±ã®æ¸›è¡°ç‡, ä¾‹: 0.9)

---

**å•10**: Attention=SSMåŒå¯¾æ€§ã®å®Ÿç”¨çš„æ„å‘³ã¯?

**ç­”**: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒå¯èƒ½ (ä¸€éƒ¨å±¤ã¯Attentionã€ä¸€éƒ¨å±¤ã¯SSM)

:::

### 5.7 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ (3ã¤)

**ãƒãƒ£ãƒ¬ãƒ³ã‚¸1: Mamba-2 Microå®Ÿè£…**

```julia
# èª²é¡Œ: ä»¥ä¸‹ã‚’å®Œæˆã•ã›ã‚ˆ
function mamba2_micro(x::Matrix{T}, u::Matrix{T}, v::Matrix{T}) where T
    N, d = size(x)
    r = size(u, 2)
    y = zeros(T, N, d)
    state = zeros(T, r, d)

    for i in 1:N
        # TODO: Semi-Separableæ›´æ–°ã‚’å®Ÿè£…
        # state += ???
        # y[i, :] = ???
    end

    return y
end
```

**è§£ç­”ä¾‹**:
```julia
function mamba2_micro(x::Matrix{T}, u::Matrix{T}, v::Matrix{T}) where T
    N, d = size(x)
    r = size(u, 2)
    y = zeros(T, N, d)
    state = zeros(T, r, d)

    for i in 1:N
        state += v[i, :] * x[i, :]'  # (r, d)
        y[i, :] = u[i, :]' * state   # (d,)
    end

    return y
end
```

---

**ãƒãƒ£ãƒ¬ãƒ³ã‚¸2: RWKV WKVè¨ˆç®—**

```julia
# èª²é¡Œ: WKV (Weighted Key-Value) ã‚’å®Ÿè£…
function rwkv_wkv(k::Matrix{T}, v::Matrix{T}, w::Vector{T}) where T
    N, d = size(k)
    wkv = zeros(T, N, d)
    # TODO: Generalized Delta Ruleã§è¨ˆç®—
    return wkv
end
```

**è§£ç­”ä¾‹**:
```julia
function rwkv_wkv(k::Matrix{T}, v::Matrix{T}, w::Vector{T}) where T
    N, d = size(k)
    wkv = zeros(T, N, d)
    num = zeros(T, d)
    den = zeros(T, d)

    for i in 1:N
        num = num .* w .+ k[i, :] .* v[i, :]
        den = den .* w .+ k[i, :]
        wkv[i, :] = num ./ (den .+ T(1e-6))
    end

    return wkv
end
```

---

**ãƒãƒ£ãƒ¬ãƒ³ã‚¸3: RetNetä¸¦åˆ—â†’å†å¸°å¤‰æ›**

```julia
# èª²é¡Œ: ä¸¦åˆ—è¡¨ç¾ã®çµæœã‚’å†å¸°ã§å†ç¾
function verify_retnet_equivalence(Q, K, V, gamma)
    y_parallel = retnet_parallel(Q, K, V, gamma)
    y_recurrent = retnet_recurrent(Q, K, V, gamma)
    # TODO: èª¤å·®ã‚’è¨ˆç®—ã—ã€1e-5ä»¥ä¸‹ã‹ç¢ºèª
    return ???
end
```

**è§£ç­”ä¾‹**:
```julia
function verify_retnet_equivalence(Q, K, V, gamma)
    y_parallel = retnet_parallel(Q, K, V, gamma)
    y_recurrent = retnet_recurrent(Q, K, V, gamma)
    max_error = maximum(abs.(y_parallel .- y_recurrent))
    println("Max error: $max_error")
    return max_error < 1e-5
end
```

:::message
**é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚Mamba-2/RWKV/RetNet/GLAã®æ€§èƒ½æ¯”è¼ƒã€ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æã€è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆã€å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚’å®Œäº†ã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ â€” ç ”ç©¶æœ€å‰ç·šã¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã¸ã®æ¥ç¶šã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ãƒ»ç™ºå±•ãƒ»å•ã„

### 6.1 Attention=SSMåŒå¯¾æ€§ãŒé–‹ã„ãŸæ–°ä¸–ç•Œ

SSDå®šç† [^1] ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ãŸ:

**é©å‘½1: äºŒé …å¯¾ç«‹ã®çµ‚ç„‰**

- Before: "Transformerã‹Mambaã‹"ã®é¸æŠ
- After: "ã©ã†çµ„ã¿åˆã‚ã›ã‚‹ã‹"ã®è¨­è¨ˆ

**é©å‘½2: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®ç†è«–çš„åŸºç›¤**

- Attentionå±¤ã¨SSMå±¤ã‚’æ··åœ¨ã•ã›ã‚‹æ­£å½“æ€§
- å„å±¤ã®å½¹å‰²åˆ†æ‹…ã®æœ€é©åŒ–æŒ‡é‡

**é©å‘½3: è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®é¸æŠ**

- è¨“ç·´: ä¸¦åˆ—è¨ˆç®—ãŒå¾—æ„ â†’ Attentionå½¢å¼
- æ¨è«–: é€æ¬¡å‡¦ç†ãŒå¿…è¦ â†’ SSMå½¢å¼
- åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨é€”ã«å¿œã˜ã¦åˆ‡ã‚Šæ›¿ãˆ

### 6.2 Mambaç³»åˆ—ã®é€²åŒ–ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

```mermaid
graph TD
    A["S4 (2021)<br/>é€£ç¶šSSM+HiPPO"] --> B["S4D (2022)<br/>å¯¾è§’åŒ–"]
    B --> C["Mamba (2023)<br/>Selective SSM"]
    C --> D["Mamba-2 (2024)<br/>SSDåŒå¯¾æ€§"]
    D --> E["Mamba-3? (2025+)<br/>æœªæ¥"]

    F["H3 (2022)<br/>Gated SSM"] --> C
    G["Hyena (2023)<br/>ç•³ã¿è¾¼ã¿"] --> C

    D --> H["ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰<br/>Jamba/Zamba/Griffin"]
    D --> I["Vision Mamba<br/>2Dæ‹¡å¼µ"]
    D --> J["Audio Mamba<br/>éŸ³å£°ç‰¹åŒ–"]

    style C fill:#fff9c4
    style D fill:#c8e6c9
    style H fill:#b3e5fc
```

**é€²åŒ–ã®æ–¹å‘æ€§**:

1. **åŠ¹ç‡åŒ–**: S4 â†’ S4D â†’ Mamba â†’ Mamba-2 (è¨ˆç®—é‡å‰Šæ¸›)
2. **è¡¨ç¾åŠ›**: Gating, Selective, Data-dependent parameters
3. **åŒå¯¾æ€§**: SSDå®šç†ã«ã‚ˆã‚‹Attentionã¨ã®çµ±ä¸€
4. **ãƒ¢ãƒ€ãƒªãƒ†ã‚£æ‹¡å¼µ**: Vision, Audio, Multi-modal

### 6.3 ç·šå½¢RNN/Attentionã®çµ±ä¸€ç†è«–

**å…±é€šæ§‹é€ **: å…¨ã¦ **ã‚«ãƒ¼ãƒãƒ«åŒ–ã•ã‚ŒãŸAttention**:

$$
\text{Output}_i = \frac{\sum_{j=1}^{i} \kappa(q_i, k_j) v_j}{\sum_{j=1}^{i} \kappa(q_i, k_j)}
$$

| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | ã‚«ãƒ¼ãƒãƒ« $\kappa(q, k)$ | æ­£è¦åŒ– |
|:------------|:-------------------|:------|
| Standard Attention | $\exp(q^\top k / \sqrt{d})$ | Softmax |
| Linear Attention | $\phi(q)^\top \psi(k)$ | Running sum |
| RWKV | $w^{i-j} k$ (decay) | Running sum |
| RetNet | $\gamma^{i-j} q^\top k$ | Running sum |
| GLA | $g_j \phi(q)^\top \phi(k)$ (gated) | Running sum |

**çµ±ä¸€è¦–ç‚¹ã®æ„ç¾©**:

- å…¨ã¦åŒã˜ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ç†è§£å¯èƒ½
- è¨­è¨ˆç©ºé–“ã®æ¢ç´¢ãŒä½“ç³»çš„ã«
- æ–°ã—ã„ã‚«ãƒ¼ãƒãƒ«ã®ææ¡ˆãŒå®¹æ˜“

### 6.4 æ¨å¥¨è«–æ–‡ãƒªã‚¹ãƒˆ & èª­ã‚€é †åº

**å…¥é–€ç·¨ (ç†è«–åŸºç¤)**:

1. [Dao & Gu 2024] Transformers are SSMs [^1] â€” **SSDå®šç†ã®åŸè«–æ–‡ã€å¿…èª­**
2. [Sun+ 2023] Retentive Network [^4] â€” **RetNetã®3ã¤ã®è¡¨ç¾**
3. [Yang+ 2023] Gated Linear Attention [^5] â€” **ç·šå½¢Attentionã®é€²åŒ–**

**ç™ºå±•ç·¨ (æœ€æ–°æ‰‹æ³•)**:

4. [RWKV-7 paper] â€” **Generalized Delta Rule, TC0çªç ´**
5. [VMamba paper] Vision Mamba [^6] â€” **2D SSMã®æŒ‘æˆ¦**
6. [Jamba paper] AI21 Labs â€” **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (ç¬¬18å›äºˆå‘Š)**

**ç†è«–æ·±å €ã‚Š**:

7. [Gu+ 2023] MambaåŸè«–æ–‡ â€” **Selective SSMã®åŸºç¤ (ç¬¬16å›)**
8. [Gu+ 2021] S4åŸè«–æ–‡ â€” **é€£ç¶šSSM + HiPPOåˆæœŸåŒ–**
9. [Katharopoulos+ 2020] Transformers are RNNs â€” **ç·šå½¢Attentionã®èµ·æº**

**èª­ã‚€é †åºã®æ¨å¥¨**:

1. ç¬¬16å›å¾©ç¿’ (MambaåŸºç¤) â†’ 2. æœ¬è¬›ç¾© (Mamba-2/SSD) â†’ 3. ç¬¬18å› (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰)
4. ä¸¦è¡Œã—ã¦ RetNet [^4] + GLA [^5] ã§ç·šå½¢ç³»ã‚’è£œå®Œ
5. Vision/Audioèˆˆå‘³ã‚ã‚Œã° VMamba [^6]

### 6.6 Glossary (ç”¨èªé›†)

:::details æœ¬è¬›ç¾©ã®å…¨ç”¨èª (ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †)

**Attention=SSM Duality (åŒå¯¾æ€§)**: Attentionã¨SSMãŒæ•°å­¦çš„ã«ç­‰ä¾¡ã§ã‚ã‚‹ã¨ã„ã†å®šç† (SSDå®šç†)

**Causal Mask (å› æœãƒã‚¹ã‚¯)**: æœªæ¥ã‚’è¦‹ãªã„ãŸã‚ã®ä¸‹ä¸‰è§’ãƒã‚¹ã‚¯

**Chunk-wise Parallel (ãƒãƒ£ãƒ³ã‚¯ä¸¦åˆ—)**: ç³»åˆ—ã‚’chunkã«åˆ†å‰²ã—ã€chunkå†…ã¯ä¸¦åˆ—ã€chunké–“ã¯ä¾å­˜

**Decay Factor (æ¸›è¡°å› å­)**: RWKV/RetNetã§éå»æƒ…å ±ã‚’æ¸›è¡°ã•ã›ã‚‹ä¿‚æ•° (ä¾‹: Î³=0.9)

**Feature Map (ç‰¹å¾´å†™åƒ)**: ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§ã®å†™åƒ Ï†(x)

**Gated Linear Attention (GLA)**: ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’è¿½åŠ ã—ãŸç·šå½¢Attention

**Generalized Delta Rule (GDR)**: RWKV-7ã®æ ¸å¿ƒã€TC0é™ç•Œã‚’çªç ´

**Linear Attention (ç·šå½¢Attention)**: O(NÂ²) â†’ O(N) ã«å‰Šæ¸›ã—ãŸAttention

**Receptance (å—å®¹åº¦)**: RWKVã§éå»æƒ…å ±ã‚’ã©ã‚Œã ã‘å—å®¹ã™ã‚‹ã‹ã®é‡ã¿

**Retention (ä¿æŒ)**: RetNetã®æ©Ÿæ§‹ã€éå»æƒ…å ±ã‚’æ¸›è¡°ã—ãªãŒã‚‰ä¿æŒ

**Semi-Separable Matrix (åŠåˆ†é›¢è¡Œåˆ—)**: A_ij = u_i^T v_j (iâ‰¥j) ã®å½¢ã®è¡Œåˆ—

**State Space Duality (SSD)**: Mamba-2ã®ç†è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**Structured State Space Model (SSM)**: æ§‹é€ åŒ–çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«

**Time-Mixing (æ™‚é–“ãƒŸãƒƒã‚¯ã‚¹)**: RWKVã§æ™‚é–“æ–¹å‘ã®æƒ…å ±æ··åˆ

**Vision Mamba (VMamba)**: 2Dç”»åƒç”¨ã®Mambaæ‹¡å¼µ

**WKV (Weighted Key-Value)**: RWKVã®æ ¸å¿ƒè¨ˆç®—

:::

### 6.7 çŸ¥è­˜ãƒãƒƒãƒ— â€” æœ¬è¬›ç¾©ã®ãƒˆãƒ”ãƒƒã‚¯æ§‹é€ 

```mermaid
graph TD
    A["Attention=SSMåŒå¯¾æ€§"] --> B["Semi-Separableè¡Œåˆ—"]
    A --> C["SSDå®šç†"]

    B --> D["Mamba-2"]
    C --> D

    A --> E["ç·šå½¢RNNç³»"]
    E --> F["RWKV-7"]
    E --> G["RetNet"]
    E --> H["GLA"]

    A --> I["Visionæ‹¡å¼µ"]
    I --> J["VMamba"]

    D --> K["ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰<br/>(ç¬¬18å›)"]
    F --> K
    G --> K
    J --> K

    style A fill:#fff9c4
    style D fill:#c8e6c9
    style K fill:#b3e5fc
```

**ä¸­å¿ƒæ¦‚å¿µ**: Attention=SSMåŒå¯¾æ€§ (SSDå®šç†)

**3ã¤ã®æ´¾ç”Ÿ**:

1. **Mamba-2**: åŒå¯¾æ€§ã‚’æ´»ã‹ã—ãŸé«˜é€ŸåŒ–
2. **ç·šå½¢RNNç³»**: RWKV, RetNet, GLA â€” ã‚«ãƒ¼ãƒãƒ«åŒ–ã®å¤šæ§˜æ€§
3. **Visionæ‹¡å¼µ**: VMamba â€” 2Dæ§‹é€ ã¸ã®é©ç”¨

**åˆ°é”ç‚¹**: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (ç¬¬18å›)

---

### 6.8 ä»Šå›ã®å­¦ç¿’å†…å®¹

### 8.2 æœ¬è¬›ç¾©ã®3ã¤ã®æ ¸å¿ƒ

**1. Attention=SSMåŒå¯¾æ€§ã®ç™ºè¦‹**

Attentionã¨SSMã¯ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†åŒã˜æ•°å­¦çš„æ§‹é€ ã‚’æŒã¤ã€‚è¦‹ãŸç›®ã¯é•ã†ãŒã€æœ¬è³ªçš„ã«ç­‰ä¾¡ã€‚ã“ã®ç™ºè¦‹ãŒã€ŒTransformerã‹Mambaã‹ã€ã¨ã„ã†äºŒé …å¯¾ç«‹ã‚’çµ‚ã‚ã‚‰ã›ãŸã€‚

**2. Mamba-2ã®é©æ–°**

SSDç†è«–ã‚’æ´»ã‹ã—ã€Mambaã® $O(N \cdot d_{\text{state}}^2)$ ã‚’ $O(N \cdot d_{\text{state}})$ ã«å‰Šæ¸›ã€‚è¨“ç·´2-8å€é«˜é€ŸåŒ–ã€Transformerã¨åŒç­‰ã®æ€§èƒ½ã€‚

**3. ç·šå½¢RNN/Attentionã®çµ±ä¸€**

RWKV-7, RetNet, GLA â€” å…¨ã¦ã€Œã‚«ãƒ¼ãƒãƒ«åŒ–ã•ã‚ŒãŸAttentionã€ã¨ã—ã¦çµ±ä¸€çš„ã«ç†è§£ã§ãã‚‹ã€‚è¨­è¨ˆç©ºé–“ã®ä½“ç³»åŒ–ã€‚

### 8.3 ç¬¬16å›ã‹ã‚‰ã®æ¥ç¶š â€” Mambaã®é€²åŒ–

| å› | ã‚¿ã‚¤ãƒˆãƒ« | æ ¸å¿ƒ |
|:---|:--------|:-----|
| 16 | **Mamba â€” Selective SSM** | Input-dependent parameters, O(N)è¨ˆç®— |
| **17** | **Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³•** | **Attention=SSMåŒå¯¾æ€§ã€Mamba-2/RWKV/RetNet** |
| 18 | **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰** | Jamba/Zamba/Griffin â€” èåˆã®å®Ÿè·µ |

ç¬¬16å›ã§Mambaã®Selective SSMã‚’å­¦ã³ã€ç¬¬17å›ã§ãã®æ•°å­¦çš„åŸºç›¤(SSDåŒå¯¾æ€§)ã¨é€²åŒ–å½¢(Mamba-2)ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚æ¬¡ã¯ã€Attentionã¨SSMã‚’èåˆã•ã›ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¸ã€‚

### 8.4 FAQ (5å• â€” å®Ÿè·µçš„ + åŠ±ã¾ã™)

:::details Q1: Mamba-2ã¨Mambaã®é•ã„ã¯?

**A**: **è¨ˆç®—é‡å‰Šæ¸›ãŒæœ¬è³ª**ã€‚Mambaã¯O(NÂ·dÂ²), Mamba-2ã¯O(NÂ·d)ã€‚SSDç†è«–ã«ã‚ˆã‚‹Semi-Separableåˆ†è§£ã§å®Ÿç¾ã€‚æ€§èƒ½ã¯ã»ã¼åŒç­‰ã ãŒã€è¨“ç·´2-8å€é€Ÿã„ã€‚å®Ÿè£…æ™‚ã¯Mamba-2ã‚’é¸ã¶ã¹ãã€‚

:::

:::details Q2: çµå±€ã€Attention ã¨ Mamba ã©ã¡ã‚‰ã‚’ä½¿ãˆã°ã„ã„?

**A**: **ã©ã¡ã‚‰ã‹ä¸€æ–¹ã§ã¯ãªãã€ä¸¡æ–¹**ã€‚SSDå®šç†ãŒè¨¼æ˜ã—ãŸã‚ˆã†ã«ã€ä¸¡è€…ã¯æ•°å­¦çš„ã«ç­‰ä¾¡ã€‚ã ã‹ã‚‰ **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰**(ä¸€éƒ¨å±¤ã¯Attentionã€ä¸€éƒ¨å±¤ã¯SSM)ãŒæœ€é©ã€‚ç¬¬18å›ã§å®Œå…¨ç¿’å¾—ã™ã‚‹ã€‚

çŸ­ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ â†’ Attention
é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ â†’ Mamba/Mamba-2
å®Ÿæ¨è«– â†’ RWKV/RetNet (O(1)ãƒ¡ãƒ¢ãƒª)

:::

:::details Q3: æ•°å¼ãŒé›£ã—ã™ãã¦æŒ«æŠ˜ã—ãã†...

**A**: **Zone 3ã®æ•°å¼ã¯"èª­ã‚€"ã‚‚ã®ã§ã¯ãªã"æ‰‹ã‚’å‹•ã‹ã™"ã‚‚ã®**ã€‚ç´™ã¨ãƒšãƒ³ã§å°å‡ºã‚’è¿½ã†ã¨ã€çªç„¶ç†è§£ãŒé™ã‚Šã¦ãã‚‹ç¬é–“ãŒã‚ã‚‹ã€‚Semi-Separableè¡Œåˆ—ã®å®šç¾© (å®šç¾©3.1) ã‹ã‚‰ã€1è¡Œãšã¤æ‰‹æ›¸ãã§è¿½ã£ã¦ã¿ã¦ã€‚Zone 4ã®å®Ÿè£…ã‚’å…ˆã«å‹•ã‹ã—ã¦ã€ã€Œå‹•ãã‚³ãƒ¼ãƒ‰ã€ã‹ã‚‰é€†ç®—ã—ã¦æ•°å¼ã‚’ç†è§£ã™ã‚‹ã®ã‚‚æœ‰åŠ¹ã€‚

:::

:::details Q4: RWKVã¨RetNetã®é•ã„ã¯?

**A**: **æ¸›è¡°ã®ä»•çµ„ã¿ãŒé•ã†**:

- **RWKV**: ãƒãƒ£ãƒãƒ«ã”ã¨ã®Decay weight $w^{i-j}$ (ãƒ‡ãƒ¼ã‚¿éä¾å­˜)
- **RetNet**: å›ºå®šDecay $\gamma^{i-j}$ + ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã®QKV

**è¨“ç·´**: ã©ã¡ã‚‰ã‚‚ä¸¦åˆ—åŒ–å¯èƒ½
**æ¨è«–**: ã©ã¡ã‚‰ã‚‚O(1)ãƒ¡ãƒ¢ãƒª
**æ€§èƒ½**: RetNetãŒã‚„ã‚„ä¸Š (LRAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯)
**å®Ÿè£…é›£æ˜“åº¦**: RWKVãŒã‚·ãƒ³ãƒ—ãƒ«

ç”¨é€”æ¬¡ç¬¬ã ãŒã€è¿·ã£ãŸã‚‰RetNetã‚’æ¨å¥¨ã€‚

:::

:::details Q5: Vision Mambaã¯ViTã‚’è¶…ãˆã‚‹ã‹?

**A**: **ã¾ã è¶…ãˆã¦ã„ãªã„ãŒã€å¯èƒ½æ€§ã¯ã‚ã‚‹**ã€‚

ç¾çŠ¶:
- ImageNetåˆ†é¡: ViT 81.8% vs VMamba 82.5% (åƒ…å·®ã§å‹åˆ©)
- é€Ÿåº¦: VMamba ãŒ1.2-1.3å€é€Ÿ
- ãƒ¡ãƒ¢ãƒª: VMamba ãŒ25-30%å‰Šæ¸›

èª²é¡Œ:
- ã‚°ãƒ­ãƒ¼ãƒãƒ«æ–‡è„ˆç²å¾—ã§ViTã«åŠ£ã‚‹å ´é¢
- 2Dæ§‹é€ ã®æœ¬è³ªçš„æ•æ‰ã¯ã¾ã æœªè§£æ±º

ä»Šå¾Œã€Attentionå±¤ã¨ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã§çªç ´ã™ã‚‹å¯èƒ½æ€§å¤§ã€‚

:::

### 8.5 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (1é€±é–“ãƒ—ãƒ©ãƒ³)

| æ—¥ | å†…å®¹ | æ™‚é–“ | ç›®æ¨™ |
|:---|:-----|:-----|:-----|
| **Day 1** | Zone 0-2 | 1h | åŒå¯¾æ€§ã®ç›´æ„Ÿã‚’æ´ã‚€ |
| **Day 2** | Zone 3 å‰åŠ (å®šç¾©3.1-3.2) | 2h | Semi-Separableè¡Œåˆ—ã‚’ç†è§£ |
| **Day 3** | Zone 3 å¾ŒåŠ (å®šç†3.3-3.4) | 2h | SSDå®šç†ã‚’å®Œå…¨å°å‡º |
| **Day 4** | Zone 4 Juliaå®Ÿè£… | 3h | Mamba-2/RWKV/RetNet/GLAå®Ÿè£… |
| **Day 5** | Zone 4 Rustå®Ÿè£… | 2h | Semi-Separableè¡Œåˆ—æœ€é©åŒ– |
| **Day 6** | Zone 5 å®Ÿé¨“ | 2h | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã€ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç†è§£ |
| **Day 7** | Zone 6-7 + è«–æ–‡ | 2h | ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ + Mamba-2è«–æ–‡èª­è§£ |

**åˆè¨ˆ**: 14æ™‚é–“ (1æ—¥2æ™‚é–“Ã—7æ—¥)

**å®Œäº†ã®ç›®å®‰**:
- âœ… SSDå®šç†ã‚’ç´™ã«æ›¸ã„ã¦å†ç¾ã§ãã‚‹
- âœ… Mamba-2/RWKV/RetNet/GLAã®ã‚³ãƒ¼ãƒ‰ãŒèª­ã‚ã‚‹ãƒ»æ›¸ã‘ã‚‹
- âœ… "ã©ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ã„ã¤ä½¿ã†ã‹"ã®åˆ¤æ–­åŸºæº–ã‚’æŒã¤

### 8.6 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼ (è‡ªå·±è©•ä¾¡ã‚³ãƒ¼ãƒ‰)

```julia
# æœ¬è¬›ç¾©ã®ç†è§£åº¦ãƒã‚§ãƒƒã‚¯
function lecture17_progress_check()
    checks = [
        "Semi-Separableè¡Œåˆ—ã®å®šç¾©ã‚’èª¬æ˜ã§ãã‚‹",
        "Attention=SSMåŒå¯¾æ€§ã®æ„å‘³ã‚’ç†è§£ã—ã¦ã„ã‚‹",
        "Mamba-2ã®Chunkä¸¦åˆ—åŒ–ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã§ãã‚‹",
        "RWKVã®WKVè¨ˆç®—ã‚’å®Ÿè£…ã§ãã‚‹",
        "RetNetã®3ã¤ã®è¡¨ç¾ã‚’ç†è§£ã—ã¦ã„ã‚‹",
        "GLAã®Gatingã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹",
        "Vision Mambaã®4æ–¹å‘èµ°æŸ»ã‚’å®Ÿè£…ã§ãã‚‹",
        "Mamba-2 vs RWKV vs RetNet ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¬æ˜ã§ãã‚‹",
    ]

    println("=== ç¬¬17å› é€²æ—ãƒã‚§ãƒƒã‚¯ ===")
    println("ä»¥ä¸‹ã®é …ç›®ã«ã¤ã„ã¦ã€ç†è§£åº¦ã‚’1-5ã§è©•ä¾¡ã—ã¦ãã ã•ã„:")
    println("1=å…¨ãç†è§£ã—ã¦ã„ãªã„, 3=åŠåˆ†ç†è§£, 5=å®Œå…¨ã«ç†è§£")
    println()

    total_score = 0
    for (i, check) in enumerate(checks)
        println("[$i] $check")
        print("   è©•ä¾¡ (1-5): ")
        score = parse(Int, readline())
        total_score += score
    end

    max_score = length(checks) * 5
    percentage = (total_score / max_score) * 100

    println()
    println("=== çµæœ ===")
    println("åˆè¨ˆã‚¹ã‚³ã‚¢: $total_score / $max_score")
    println("ç†è§£åº¦: $(round(percentage, digits=1))%")

    if percentage >= 80
        println("ğŸ‰ ç´ æ™´ã‚‰ã—ã„! ç¬¬17å›ã‚’å®Œå…¨ã«ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸ!")
    elseif percentage >= 60
        println("ğŸ’ª è‰¯ã„ãƒšãƒ¼ã‚¹! ã‚ã¨å°‘ã—ã§å®Œå…¨ç†è§£ã§ã™!")
    else
        println("ğŸ“š Zone 3-4ã‚’ã‚‚ã†ä¸€åº¦å¾©ç¿’ã—ã¾ã—ã‚‡ã†ã€‚ç„¦ã‚‰ãšç€å®Ÿã«!")
    end

    return (total_score, max_score, percentage)
end

# å®Ÿè¡Œ
# lecture17_progress_check()
```

### 8.7 æ¬¡å›äºˆå‘Š â€” ç¬¬18å›: Attention Ã— Mamba ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰

**ç¬¬18å›ã®å†…å®¹**:

- **Jamba** (AI21 Labs): SSM + Attention + MoE ã®3å±¤ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
- **Zamba** (Zyphra): Mamba + Shared Attention ã®åŠ¹ç‡è¨­è¨ˆ
- **Griffin / RecurrentGemma** (Google): Gated Linear Recurrences + Local Attention
- **StripedHyena** (Together AI): Hyena + Attention ã®éŸ³å£°ç‰¹åŒ–

**å•ã„**: Attentionã¨SSMã¯æ•°å­¦çš„ã«ç­‰ä¾¡ã ã¨è¨¼æ˜ã—ãŸã€‚ã§ã¯ã€ãªãœ **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰**(ä¸¡æ–¹æ··åœ¨)ãŒæœ€å¼·ãªã®ã‹?

**ãƒ’ãƒ³ãƒˆ**: ç­‰ä¾¡ â‰  åŒä¸€ã€‚è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¨è¡¨ç¾åŠ›ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒéµã€‚

**æº–å‚™**:
- æœ¬è¬›ç¾© (ç¬¬17å›) ã®å¾©ç¿’ â€” SSDå®šç†ã‚’å®Œå…¨ç†è§£
- ç¬¬14å› (Attention) ã®å¾©ç¿’ â€” Multi-Head Attentionã®æ§‹é€ 
- ç¬¬16å› (Mamba) ã®å¾©ç¿’ â€” Selective SSMã®è¨­è¨ˆ

**Course IIèª­äº†**: ç¬¬18å›ã§ Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ãŒå®Œçµã™ã‚‹ã€‚ç¬¬1å›ã‹ã‚‰18å›ã¾ã§ã®æ—…è·¯ã‚’æŒ¯ã‚Šè¿”ã‚Šã€Course IIIã€Œå®Ÿè·µç·¨ã€ã¸ã®æ©‹æ¸¡ã—ã‚’ã™ã‚‹ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ ç¬¬17å›ã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆ! Attention=SSMåŒå¯¾æ€§ã‚’å®Œå…¨ç¿’å¾—ã€‚Mamba-2/RWKV/RetNet/GLAã®æ•°å­¦ã¨å®Ÿè£…ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚æ¬¡ã¯ç¬¬18å› â€” ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å…¨ã¦ã‚’èåˆã™ã‚‹ã€‚
:::

---

### 6.13 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**å•**: Attentionã¨SSMãŒæ•°å­¦çš„ã«ç­‰ä¾¡ã ã¨è¨¼æ˜ã—ãŸ (SSDå®šç†)ã€‚ã§ã¯ã€ãªãœæ©Ÿæ¢°å­¦ç¿’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¯2023å¹´ã¾ã§æ°—ã¥ã‹ãªã‹ã£ãŸã®ã‹? ãã—ã¦ã€ã“ã®ã€Œé…ã‚Œã€ã¯ä»–ã®åˆ†é‡ã«ã‚‚å­˜åœ¨ã™ã‚‹ã®ã§ã¯ãªã„ã‹?

**è­°è«–ã®ãƒã‚¤ãƒ³ãƒˆ**:

1. **åˆ†é‡ã®åˆ†æ–­**: Attentionç ”ç©¶è€…ã¨SSMç ”ç©¶è€…ã¯ç•°ãªã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã€‚è«–æ–‡èªŒã‚‚ä¼šè­°ã‚‚é•ã†ã€‚æ•°å­¦çš„ã«åŒã˜ã‚‚ã®ã‚’ã€åˆ¥ã®è¨€è‘‰ã§ç ”ç©¶ã—ã¦ã„ãŸã€‚

2. **è¡¨è¨˜æ³•ã®å£**: Attentionã¯ã€ŒSoftmax(QK^T)Vã€ã€SSMã¯ã€Œh_i = Ah_{i-1} + Bx_i, y_i = Ch_iã€ã€‚è¡¨è¨˜ãŒé•ã†ã¨ã€åŒã˜ã‚‚ã®ã«è¦‹ãˆãªã„ã€‚

3. **å®Ÿè£…ã®é•ã„**: PyTorchã®Attentionå®Ÿè£…ã¨SSMã®é›¢æ•£åŒ–å®Ÿè£…ã¯ã€ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã§å…¨ãç•°ãªã‚‹ã€‚ã€Œå‹•ãã‚³ãƒ¼ãƒ‰ã€ã‹ã‚‰æ•°å­¦ã‚’é€†ç®—ã™ã‚‹ã¨ã€åˆ¥ç‰©ã«è¦‹ãˆã‚‹ã€‚

**åçœã¨æ•™è¨“**:

- **çµ±ä¸€ç†è«–ã®é‡è¦æ€§**: ç•°ãªã‚‹è¦–ç‚¹ã‚’çµ±ä¸€ã™ã‚‹ç†è«– (SSDå®šç†) ãŒã€ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’ã‚‚ãŸã‚‰ã™
- **ç•°åˆ†é‡äº¤æµ**: Transformerã¨SSMã®ç ”ç©¶è€…ãŒå”åŠ›ã—ãŸçµæœãŒMamba-2
- **æŠ½è±¡åŒ–ã®åŠ›**: Semi-Separableè¡Œåˆ—ã¨ã„ã†æŠ½è±¡æ¦‚å¿µã§ã€ä¸¡è€…ã‚’çµ±ä¸€

**ä»–ã®åˆ†é‡ã§ã®ã€Œéš ã‚ŒãŸç­‰ä¾¡æ€§ã€**:

- æ©Ÿæ¢°å­¦ç¿’: Adam = RMSprop + Momentum (ç•°ãªã‚‹èµ·æºã ãŒæ•°å­¦çš„ã«çµ±åˆå¯èƒ½)
- ç‰©ç†å­¦: æ³¢å‹•å…‰å­¦ vs å¹¾ä½•å…‰å­¦ (æ³¢é•·Î»â†’0ã§ç­‰ä¾¡)
- æ•°å­¦: ç·šå½¢ä»£æ•°ã®è¡Œåˆ—å¼ vs å¤–ç© (ç•°ãªã‚‹å®šç¾©ã ãŒæœ¬è³ªçš„ã«åŒã˜)

**ã‚ãªãŸã®ç ”ç©¶åˆ†é‡ã«ã‚‚ã€ã€Œåˆ¥ç‰©ã«è¦‹ãˆã¦å®Ÿã¯åŒã˜ã‚‚ã®ã€ãŒéš ã‚Œã¦ã„ãªã„ã‹?**

:::details æ­´å²çš„è€ƒå¯Ÿ: ãªãœ2024å¹´ã¾ã§æ°—ã¥ã‹ã‚Œãªã‹ã£ãŸã‹

**2021å¹´: S4ç™»å ´** (Gu+ ICLR 2022)
- é€£ç¶šSSMã‚’é›¢æ•£åŒ– â†’ é•·ç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§æˆåŠŸ
- ã ãŒTransformerã¨ã€Œåˆ¥ç‰©ã€ã¨èªè­˜ã•ã‚Œã‚‹

**2022å¹´: Attentionç ”ç©¶ã®çˆ†ç™º**
- GPT-3/4, LLaMA, Chinchilla â€” Transformerã®æ™‚ä»£
- SSMã¯ã€Œãƒ‹ãƒƒãƒãªæ‰‹æ³•ã€ã¨ã—ã¦å‚æµ

**2023å¹´: Mambaç™»å ´** (Gu+ NeurIPS 2023)
- Selective SSM â†’ Transformerã«åŒ¹æ•µ
- ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æ³¨ç›®é›†ã¾ã‚‹ â†’ "Attentionä»£æ›¿"ã¨ã—ã¦èªè­˜

**2024å¹´: SSDå®šç†ç™ºè¡¨** (Dao & Gu, ICML 2024)
- Semi-Separableè¡Œåˆ—ã§çµ±ä¸€ â†’ **ã€Œä»£æ›¿ã€ã§ã¯ãªãã€ŒåŒå¯¾ã€ã ã£ãŸ**
- ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£è¡æ’ƒ â†’ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã¸ã®é“

**æ•™è¨“**: ã€Œå¯¾ç«‹ã€ã¨è¦‹ãˆãŸã‚‚ã®ãŒã€ŒåŒå¯¾ã€ã ã£ãŸã€‚ç§‘å­¦ã®é€²æ­©ã¯ã€åˆ†æ–­ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§åŠ é€Ÿã™ã‚‹ã€‚

:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. *ICML 2024*.
@[card](https://arxiv.org/abs/2405.21060)

[^2]: Peng, B., et al. (2023). RWKV: Reinventing RNNs for the Transformer Era. *Findings of EMNLP 2023*.
@[card](https://arxiv.org/abs/2305.13048)

[^3]: Peng, B., et al. (2025). A Survey of RWKV. *arXiv preprint*.
@[card](https://arxiv.org/abs/2412.14847)

[^4]: Sun, Y., et al. (2023). Retentive Network: A Successor to Transformer for Large Language Models. *arXiv preprint*.
@[card](https://arxiv.org/abs/2307.08621)

[^5]: Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. *arXiv preprint*.
@[card](https://arxiv.org/abs/2312.06635)

[^6]: Zhu, L., et al. (2024). Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model. *ICML 2024*.
@[card](https://arxiv.org/abs/2401.09417)

[^7]: PÃ©rez, J., et al. (2021). Attention is Turing Complete. *JMLR*.
@[card](https://jmlr.org/papers/volume22/20-302/20-302.pdf)

[^8]: Merrill, W., et al. (2024). The Expressive Capacity of State Space Models: A Formal Language Perspective. *arXiv preprint*.
@[card](https://arxiv.org/abs/2405.17394)

[^9]: Lahoti, A., Li, K., Chen, B., Wang, C., Bick, A., Kolter, J. Z., Dao, T., & Gu, A. (2025). Mamba-3: Improved Sequence Modeling using State Space Principles. *ICLR 2026 (Oral)*.
@[card](https://openreview.net/forum?id=HwCvaJOiCj)

### æ•™ç§‘æ›¸

- Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. *ICLR 2022* (S4åŸè«–æ–‡)
- Vaswani, A., et al. (2017). Attention Is All You Need. *NeurIPS 2017* (TransformeråŸè«–æ–‡)
- Katharopoulos, A., et al. (2020). Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. *ICML 2020* (ç·šå½¢Attentionèµ·æº)

## è¨˜æ³•è¦ç´„

æœ¬è¬›ç¾©ã§ä½¿ç”¨ã—ãŸè¨˜æ³•ã®çµ±ä¸€è¦å‰‡:

| è¨˜å· | æ„å‘³ | æ¬¡å…ƒ | å‚™è€ƒ |
|:-----|:-----|:-----|:-----|
| $N$ | ç³»åˆ—é•· (sequence length) | - | å¯å¤‰ |
| $d$ | ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ (d_model) | - | é€šå¸¸64-512 |
| $d_s$ | çŠ¶æ…‹æ¬¡å…ƒ (d_state) | - | SSMã®éš ã‚ŒçŠ¶æ…‹ |
| $r$ | ãƒ©ãƒ³ã‚¯ (rank) | - | Semi-Separableã®ä½ãƒ©ãƒ³ã‚¯ |
| $Q, K, V$ | Query, Key, Value | $(N, d)$ | Attentionå…¥åŠ› |
| $u_i, v_j$ | Semi-Separableåˆ†è§£ | $(r,)$ | $A_{ij} = u_i^\top v_j$ |
| $\bar{A}, \bar{B}, \bar{C}$ | SSMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å„ç¨® | é›¢æ•£åŒ–å¾Œ |
| $h_i$ | SSMçŠ¶æ…‹ (hidden state) | $(d_s,)$ | æ™‚åˆ»$i$ã®çŠ¶æ…‹ |
| $\gamma$ | Decay factor | - | RetNetãªã© |
| $w$ | Decay weights | $(d,)$ | RWKV (ãƒãƒ£ãƒãƒ«ã”ã¨) |
| $\phi, \psi$ | Feature map | $(d,) \to (r,)$ | ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ |
| $g$ | Gate | $(N,)$ or $(d,)$ | GLAç­‰ |
| $\odot$ | è¦ç´ ã”ã¨ã®ç© | - | Hadamard product |
| $\text{WKV}$ | Weighted Key-Value | $(N, d)$ | RWKVå‡ºåŠ› |

**è¡Œåˆ—å½¢çŠ¶ã®æ…£ä¾‹**:
- å…¥åŠ›: $(N, d)$ (ãƒãƒƒãƒæ¬¡å…ƒçœç•¥)
- é‡ã¿: $(d_{\text{in}}, d_{\text{out}})$ (åˆ—ãƒ™ã‚¯ãƒˆãƒ«å³ä¹—)
- æ³¨æ„è¡Œåˆ—: $(N, N)$

**æ•°å¼è¨˜æ³•**:
- $\mathbb{R}^{N \times d}$: Nè¡Œdåˆ—ã®å®Ÿè¡Œåˆ—
- $O(N^2)$: è¨ˆç®—é‡ã®ã‚ªãƒ¼ãƒ€ãƒ¼è¨˜æ³•
- $\sum_{j=1}^{i}$: ç´¯ç©å’Œ (Causal)
- $\text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$

---

**ğŸ‰ ç¬¬17å›å®Œäº†! æ¬¡ã¯ç¬¬18å›ã€ŒAttention Ã— Mamba ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã€ã§ Course II ã‚’ç· ã‚ããã‚‹ã€‚**

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
