---
title: "ç¬¬34å›: ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã€å‰ç·¨ã€‘ç†è«–ç·¨"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning"]
published: true
slug: "ml-lecture-34-part1"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

# ç¬¬34å›: Energy-Based Models & çµ±è¨ˆç‰©ç† ğŸ¦€

**ã€Œå¯é€†æ€§åˆ¶ç´„ã‚’æ¨ã¦ã€ä»»æ„ã®åˆ†å¸ƒã‚’exp(-E(x))ã§å®šç¾©ã™ã‚‹ã€‚Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã€‚2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã®æ·±å±¤ã€‚ãã—ã¦çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶šãŒå…¨ã¦ã®çµ±ä¸€ã‚’ç¤ºã™ã€**

> **Note:** **å‰å›ã¾ã§ã®åˆ°é”ç‚¹**: ç¬¬33å›ã§NFã®å¯é€†å¤‰æ›ã¨ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã«ã‚ˆã‚‹å³å¯†å°¤åº¦ã‚’å­¦ã‚“ã ã€‚ã ãŒå¯é€†æ€§åˆ¶ç´„ã¯è¡¨ç¾åŠ›ã‚’åˆ¶é™ã™ã‚‹ã€‚åˆ¶ç´„ãªã—ã«ç¢ºç‡å¯†åº¦ $p(x) \propto \exp(-E(x))$ ã¨å®šç¾©ã™ã‚‹Energy-Based Modelsã¸ã€‚
>
> **æœ¬å›ã§ç²å¾—ã™ã‚‹æ­¦å™¨**: EBMåŸºæœ¬å®šç¾© / Gibbsåˆ†å¸ƒ / Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Œå…¨è¨¼æ˜ / RBM + CD-k / MCMCè©³ç´° / HMC / çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š / ç›¸è»¢ç§» / Energy Matching
>
> **æ¬¡å›ã¸ã®æ¥ç¶š**: æ­£è¦åŒ–å®šæ•° $Z(\theta) = \int \exp(-E(x))dx$ ã¯è¨ˆç®—ä¸èƒ½ã€‚ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ãªã‚‰ZãŒæ¶ˆãˆã‚‹ â†’ ç¬¬35å› Score Matching & Langevin Dynamics
>
> **é€²æ—**: Course IV æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç·¨ 2/10å›å®Œäº†

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã«ã‚ˆã‚‹ç¢ºç‡åˆ†å¸ƒã®å®šç¾©

**ã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼ $E(x)$ ã‹ã‚‰ç¢ºç‡å¯†åº¦ $p(x)$ ã‚’ç›´æ¥å®šç¾©ã™ã‚‹ã€**

```rust
use ndarray::{Array2, Axis};
use rand::Rng;
use rand_distr::StandardNormal;

// ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° E(x) = ||x||^2 / 2  (ã‚¬ã‚¦ã‚¹ã®è² ã®å¯¾æ•°å°¤åº¦)
fn energy(x: &Array2<f32>) -> Vec<f32> {
    // Sum of squares per sample, then divide by 2
    x.map_axis(Axis(0), |col| col.iter().map(|&v| v * v).sum::<f32>() / 2.0)
        .into_raw_vec()
}

// ã‚®ãƒ–ã‚¹åˆ†å¸ƒ p(x) âˆ exp(-E(x))
let mut rng = rand::thread_rng();
let x: Array2<f32> = Array2::from_shape_fn((2, 100), |_| rng.sample::<f32, _>(StandardNormal));

let e = energy(&x);           // æœªæ­£è¦åŒ–ã‚¨ãƒãƒ«ã‚®ãƒ¼ (100,)
let raw: Vec<f32> = e.iter().map(|&v| (-v).exp()).collect();  // æœªæ­£è¦åŒ–ç¢ºç‡
let z: f32 = raw.iter().sum();
let prob: Vec<f32> = raw.iter().map(|&v| v / z).collect();   // æ­£è¦åŒ–ï¼ˆin-place ç›¸å½“ï¼‰

let (e_min, e_max) = e.iter().cloned().fold((f32::INFINITY, f32::NEG_INFINITY),
    |(lo, hi), v| (lo.min(v), hi.max(v)));
println!("Energy range: ({:.4}, {:.4})", e_min, e_max);
println!("Mean probability: {:.4}", prob.iter().sum::<f32>() / prob.len() as f32);
// Energy range: (0.02, 18.5)
// Mean probability: 0.01
```

**èƒŒå¾Œã®æ•°å¼**:

$$
p(x) = \frac{1}{Z(\theta)} \exp(-E_\theta(x))
$$

$$
Z(\theta) = \int \exp(-E_\theta(x)) dx
$$

**ä½“æ„Ÿã—ãŸã“ã¨**:
- ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° $E(x)$ ã‚’å®šç¾©ã™ã‚Œã°ã€ç¢ºç‡åˆ†å¸ƒ $p(x)$ ãŒå®šã¾ã‚‹
- ã ãŒæ­£è¦åŒ–å®šæ•° $Z(\theta)$ ã®è¨ˆç®—ãŒå›°é›£ â€” å…¨ç©ºé–“ã®ç©åˆ†
- ã“ã‚ŒãŒEBMè¨“ç·´ã®æ ¹æœ¬çš„ãªå•é¡Œ

> Progress: 3%
> 30ç§’ã§EBMã®æœ¬è³ªã‚’ä½“é¨“ã€‚æ•°å¼ã¨ã‚³ãƒ¼ãƒ‰ãŒ1:1å¯¾å¿œã™ã‚‹ã€‚ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° â†’ ç¢ºç‡åˆ†å¸ƒã®å®šç¾©æ–¹æ³•ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯å®Ÿéš›ã®EBMã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è§¦ã‚‹ã€‚

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” EBMã®æŒ™å‹•ã‚’è¦³å¯Ÿã™ã‚‹

### 1.1 ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã®3ã¤ã®ä¾‹

| ã‚¨ãƒãƒ«ã‚®ãƒ¼ | å®šç¾© | å¯¾å¿œã™ã‚‹åˆ†å¸ƒ |
|:-----------|:-----|:------------|
| $E(x) = \frac{1}{2}\|\|x\|\|^2$ | äºŒä¹—ãƒãƒ«ãƒ  | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}(0, I)$ |
| $E(x) = -\log p_{\text{data}}(x)$ | è² ã®å¯¾æ•°å°¤åº¦ | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãã®ã‚‚ã® |
| $E(x) = f_\theta(x)$ | ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ | å­¦ç¿’ã•ã‚ŒãŸè¤‡é›‘ãªåˆ†å¸ƒ |

**è¦³å¯Ÿ**:
- ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒ**ä½ã„é ˜åŸŸ = é«˜ç¢ºç‡é ˜åŸŸ**ï¼ˆè°·ï¼‰
- è¤‡é›‘ãªã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° â†’ è¤‡é›‘ãªç¢ºç‡åˆ†å¸ƒã‚’è¡¨ç¾å¯èƒ½
- ã‚¬ã‚¦ã‚¹ã¯å˜å³°æ€§ã€Mixtureã¯å¤šå³°æ€§ã€Ringã¯å††ç’°çŠ¶

### 1.2 Gibbsåˆ†å¸ƒã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

**æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç‰©ç†çš„æ„å‘³**:

$$
p(x; \tau) = \frac{1}{Z(\tau)} \exp\left(-\frac{E(x)}{\tau}\right)
$$

- $\tau \to 0$: **ä½æ¸©** â†’ ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°ç‚¹ã«ç¢ºç‡ãŒé›†ä¸­ï¼ˆæ±ºå®šè«–çš„ï¼‰
- $\tau = 1$: **é€šå¸¸ã®æ¸©åº¦** â†’ ãƒœãƒ«ãƒ„ãƒãƒ³åˆ†å¸ƒ
- $\tau \to \infty$: **é«˜æ¸©** â†’ ä¸€æ§˜åˆ†å¸ƒã«è¿‘ã¥ãï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼å·®ã‚’ç„¡è¦–ï¼‰

> **Note:** **Softmaxæ¸©åº¦ã¨ã®æ¥ç¶š**: Attentionæ©Ÿæ§‹ã® `softmax(QK^T / sqrt(d))` ã‚‚åŒã˜åŸç†ã€‚`sqrt(d)` = æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚ä½æ¸©ï¼ˆsqrt(d)å°ï¼‰â†’é‹­ã„æ³¨æ„ã€é«˜æ¸©ï¼ˆsqrt(d)å¤§ï¼‰â†’å¹³å¦ãªæ³¨æ„ã€‚

### 1.3 EBMã¨ä»–ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ¥ç¶š

```mermaid
graph TD
    A[ç”Ÿæˆãƒ¢ãƒ‡ãƒ«] --> B[å°¤åº¦ãƒ™ãƒ¼ã‚¹]
    A --> C[æš—é»™çš„]
    A --> D[ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹]
    B --> E[VAE: ELBO]
    B --> F[NF: å³å¯†å°¤åº¦]
    B --> G[AR: é€£é–å¾‹]
    C --> H[GAN: æ•µå¯¾çš„]
    D --> I[EBM: exp-E]
    D --> J[Score Matching]
    I --> K[Hopfield]
    I --> L[RBM]
    I --> M[Modern Hopfield]
    M --> N[Attentionç­‰ä¾¡]

    style I fill:#f9f,stroke:#333,stroke-width:4px
    style M fill:#f9f,stroke:#333,stroke-width:4px
```

| ãƒ¢ãƒ‡ãƒ« | å°¤åº¦ | è¨“ç·´ | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
|:-------|:-----|:-----|:------------|
| VAE | è¿‘ä¼¼ï¼ˆELBOï¼‰ | å®¹æ˜“ | é«˜é€Ÿ |
| GAN | è¨ˆç®—ä¸èƒ½ | ä¸å®‰å®š | é«˜é€Ÿ |
| NF | å³å¯† | å®¹æ˜“ | é«˜é€Ÿ |
| AR | å³å¯† | å®¹æ˜“ | é…ã„ |
| **EBM** | å³å¯†ï¼ˆç†è«–ä¸Šï¼‰ | **å›°é›£** | **å›°é›£** |

**EBMã®ç‰¹å¾´**:
- âœ… è¡¨ç¾åŠ›ãŒéå¸¸ã«é«˜ã„ï¼ˆä»»æ„ã® $E(x)$ ã‚’è¨±å®¹ï¼‰
- âœ… ç†è«–çš„ã«å³å¯†ãªç¢ºç‡åˆ†å¸ƒ
- âŒ $Z(\theta)$ ã®è¨ˆç®—ãŒå›°é›£ â†’ è¨“ç·´ãŒé›£ã—ã„
- âŒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«MCMC/Langevinå¿…è¦ â†’ é…ã„

> Progress: 10%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Gibbsåˆ†å¸ƒ $p(\mathbf{x}) = \exp(-E(\mathbf{x}))/Z(\theta)$ ã«ãŠã„ã¦ã€$Z(\theta)=\int\exp(-E(\mathbf{x}))d\mathbf{x}$ ãŒè¨ˆç®—å›°é›£ãªç†ç”±ã¨ã€ãã‚ŒãŒEBMè¨“ç·´ã‚’ã©ã†é›£ã—ãã™ã‚‹ã‹ã‚’è¿°ã¹ã‚ˆã€‚
> 2. EBMãƒ»NFãƒ»VAEã®3æ‰‹æ³•ã§å¯†åº¦å®šç¾©ã®æ–¹æ³•ï¼ˆç›´æ¥/é–“æ¥/å¤‰åˆ†ï¼‰ã‚’æ¯”è¼ƒã—ã€å„æ‰‹æ³•ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ä¸€è¡Œãšã¤è¿°ã¹ã‚ˆã€‚

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” EBMã®å¾©æ´»ã¨çµ±ä¸€çš„è¦–ç‚¹

### 2.1 ãªãœä»ŠEBMãªã®ã‹ï¼Ÿ

**æ­´å²çš„çµŒç·¯**:

| æ™‚ä»£ | çŠ¶æ³ | ä»£è¡¨æ‰‹æ³• |
|:-----|:-----|:---------|
| 1982-2006 | Hopfield / RBMéš†ç›› | Hopfield Network, RBM |
| 2013-2020 | VAE/GANå…¨ç››ã€EBM"éºç‰©"æ‰±ã„ | VAE, GAN, NF |
| 2020-2024 | **Modern Hopfieldâ†”Attentionç­‰ä¾¡æ€§ç™ºè¦‹** | [arXiv:2008.02217](https://arxiv.org/abs/2008.02217) |
| 2024 | **ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ï¼ˆHopfield/Hintonï¼‰** | é€£æƒ³è¨˜æ†¶ã®ç†è«–çš„åŸºç›¤ |
| 2025-2026 | **Energy Matchingçµ±ä¸€ç†è«–** | [arXiv:2504.10612](https://arxiv.org/abs/2504.10612) |
| 2025 | **NRGPT: GPTã‚’EBMã¨ã—ã¦å†è§£é‡ˆ** | [arXiv:2512.16762](https://arxiv.org/abs/2512.16762) |

**å¾©æ´»ã®3ã¤ã®ç†ç”±**:

1. **ç†è«–çš„çµ±ä¸€**: Energy Matching (2025) ãŒ Flow Matching + EBM ã‚’çµ±ä¸€
2. **å®Ÿè£…ã®é€²æ­©**: Kona 1.0 (2026) ãŒEBMåˆã®å•†ç”¨åŒ–ãƒ¢ãƒ‡ãƒ«
3. **åŸºç¤ç ”ç©¶ã®å†è©•ä¾¡**: 2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ãŒHopfield/Hintonã«æˆä¸

### 2.2 2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã®æ·±æ˜ã‚Š

**å—è³è€…**:
- **John J. Hopfield** (Princeton University): Hopfield Network (1982)
- **Geoffrey E. Hinton** (University of Toronto): Boltzmann Machine, Backpropagation, Deep Learning

**å—è³ç†ç”±**: "for foundational discoveries and inventions that enable machine learning with artificial neural networks"

**Hopfield Networkã®è²¢çŒ®**:
- **é€£æƒ³è¨˜æ†¶**: ãƒ‘ã‚¿ãƒ¼ãƒ³ $\xi^\mu$ ã‚’è¨˜æ†¶ã—ã€éƒ¨åˆ†çš„ãªå…¥åŠ›ã‹ã‚‰å®Œå…¨ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¾©å…ƒ
- **ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çŠ¶æ…‹æ›´æ–° = ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã®æœ€å°åŒ–
- **ç‰©ç†å­¦ã¨ã®æ¥ç¶š**: ã‚¹ãƒ”ãƒ³ã‚¬ãƒ©ã‚¹ç†è«–ã®å¿œç”¨

**Hintonã®è²¢çŒ®**:
- **Boltzmann Machine**: Hopfield Networkã®ç¢ºç‡çš„æ‹¡å¼µ
- **Contrastive Divergence**: RBMè¨“ç·´ã®å®Ÿç”¨çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- **Deep Learningé©å‘½**: Backpropagationã«ã‚ˆã‚‹å¤šå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨“ç·´

> **Note:** **ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã®æ„ç¾©**: 2024å¹´ã®å—è³ã¯ã€Hopfield/Boltzmann/EBMã®ç†è«–çš„åŸºç›¤ãŒã€Œç‰©ç†å­¦ã€ã¨ã—ã¦è©•ä¾¡ã•ã‚ŒãŸã“ã¨ã‚’ç¤ºã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã¯ç‰©ç†å­¦ã®ä¸€åˆ†é‡ã§ã‚ã‚Šã€çµ±è¨ˆåŠ›å­¦ã®å¿œç”¨ã§ã‚ã‚‹ã€‚

### 2.3 Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®ç™ºè¦‹

**è¡æ’ƒã®è«–æ–‡**: Ramsauer+ (2020) [arXiv:2008.02217](https://arxiv.org/abs/2008.02217) "Hopfield Networks is All You Need"

**ç™ºè¦‹å†…å®¹**:
1. **Classical Hopfield**: è¨˜æ†¶å®¹é‡ $\sim N$ï¼ˆçŠ¶æ…‹æ•°ã«æ¯”ä¾‹ï¼‰
2. **Modern Hopfield**: è¨˜æ†¶å®¹é‡ $\sim \exp(d)$ï¼ˆæ¬¡å…ƒã«å¯¾ã—ã¦æŒ‡æ•°çš„ï¼‰
3. **Attentionæ©Ÿæ§‹ã¨ã®ç­‰ä¾¡æ€§**: Modern Hopfieldã®Update Rule = Self-Attention

**æ•°å¼ã§ã®ç­‰ä¾¡æ€§**:

Modern Hopfieldã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°:

$$
E(x) = -\log \sum_{i=1}^N \exp(\beta \langle x, \xi^i \rangle) + \frac{1}{2}\|x\|^2
$$

ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ã®Update Rule:

$$
x^{t+1} = \sum_{i=1}^N \frac{\exp(\beta \langle x^t, \xi^i \rangle)}{\sum_j \exp(\beta \langle x^t, \xi^j \rangle)} \xi^i
$$

ã“ã‚Œã¯**Self-Attentionã¨åŒä¸€**:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right) V
$$

å¯¾å¿œé–¢ä¿‚:
- $Q = x^t$ï¼ˆã‚¯ã‚¨ãƒª = ç¾åœ¨ã®çŠ¶æ…‹ï¼‰
- $K = [\xi^1, \ldots, \xi^N]^\top$ï¼ˆã‚­ãƒ¼ = è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
- $V = [\xi^1, \ldots, \xi^N]^\top$ï¼ˆãƒãƒªãƒ¥ãƒ¼ = è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
- $\beta = 1/\sqrt{d}$ï¼ˆæ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

> **âš ï¸ Warning:** **å¸¸è­˜ã®å´©å£Š**: ã€ŒAttentionã¯Hopfield Networkã ã£ãŸã€ã€‚2017å¹´ã«ç™»å ´ã—ãŸTransformer Attentionã¯ã€å®Ÿã¯1982å¹´ã®Hopfield Networkã®ç¾ä»£ç‰ˆã€‚40å¹´ã®æ™‚ã‚’çµŒã¦ã€çµ±ä¸€çš„ç†è§£ãŒå¾—ã‚‰ã‚ŒãŸã€‚

### 2.4 æœ¬ã‚·ãƒªãƒ¼ã‚ºã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

**Course IVãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**:

```mermaid
graph LR
    A[ç¬¬33å› NF] --> B[ç¬¬34å› EBM]
    B --> C[ç¬¬35å› Score Matching]
    C --> D[ç¬¬36å› DDPM]
    D --> E[ç¬¬37å› SDE]
    E --> F[ç¬¬38å› Flow Matchingçµ±ä¸€]
    F --> G[ç¬¬39å› LDM]
    G --> H[ç¬¬40å› Consistency Models]
    H --> I[ç¬¬41å› World Models]
    I --> J[ç¬¬42å› çµ±ä¸€ç†è«–]

    style B fill:#f9f,stroke:#333,stroke-width:4px
```

**ç¬¬34å›ã®å½¹å‰²**:
- EBMã®åŸºæœ¬å®šç¾©ã¨è¨“ç·´å›°é›£æ€§ã‚’ç†è§£
- Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Œå…¨è¨¼æ˜
- RBM + CD-k + MCMC + HMC ã®å®Ÿè£…
- çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶šï¼ˆè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ / ç›¸è»¢ç§»ï¼‰
- Energy Matching ã«ã‚ˆã‚‹Flow Matchingçµ±ä¸€ã¸ã®ä¼ç·š

### 2.5 æ¾å°¾ç ”ã¨ã®æ¯”è¼ƒ

| é …ç›® | æ¾å°¾ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:-------|:----------|
| EBMæ‰±ã„ | Hopfield/RBMæ¦‚è¦ã®ã¿ï¼ˆ1å›ã€60åˆ†ï¼‰ | **å®Œå…¨ç‰ˆ**ï¼ˆ3,500è¡Œã€180åˆ†ï¼‰ |
| Modern Hopfield | è¨€åŠãªã— | **å®Œå…¨è¨¼æ˜ + é€£ç¶šæ™‚é–“ç‰ˆ** |
| Attentionç­‰ä¾¡æ€§ | è¨€åŠãªã— | **å®Œå…¨è¨¼æ˜** |
| ãƒãƒ¼ãƒ™ãƒ«è³ | è¨€åŠãªã— | **æ·±æ˜ã‚Šè§£èª¬** |
| RBM | CD-kæ¦‚è¦ | **å®Œå…¨å°å‡º + å®Ÿè£…** |
| çµ±è¨ˆç‰©ç† | è¨€åŠãªã— | **è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ / ç›¸è»¢ç§» / Ising** |
| Energy Matching | ãªã— | **2025å¹´æœ€æ–°ç†è«–** |
| NRGPT | ãªã— | **GPT=EBMå†è§£é‡ˆ** |

> Progress: 20%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. CD-kï¼ˆContrastive Divergenceï¼‰ãŒã€Œå¯¾æ¯”ç™ºæ•£ã€ã¨å‘¼ã°ã‚Œã‚‹ç†ç”±ã‚’ã€æœ€å°¤æ³•ã®å¯¾æ•°å°¤åº¦å‹¾é… $\nabla_\theta \log p(x) = \langle -\nabla_\theta E\rangle_\text{data} - \langle -\nabla_\theta E\rangle_\text{model}$ ã¨å¯¾å¿œã•ã›ã¦èª¬æ˜ã›ã‚ˆã€‚
> 2. EBMã§Z(Î¸)ãŒæ¶ˆå»ã§ããªã„ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x) = -\nabla_x E(x)$ ãŒè¨ˆç®—ã§ãã‚‹ç†ç”±ã‚’æ•°å¼ã§ç¤ºã›ã€‚

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” EBMã®å®Œå…¨ç†è«–

> **âš ï¸ Warning:** **è¦šæ‚Ÿ**: ã“ã®ã‚¾ãƒ¼ãƒ³ã¯3,500è¡Œè¬›ç¾©ã®æ ¸å¿ƒã€‚800è¡Œã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ã§ä»¥ä¸‹ã‚’å®Œå…¨å°å‡ºã™ã‚‹:
> 1. EBMåŸºæœ¬å®šç¾© + Gibbsåˆ†å¸ƒ
> 2. Modern Hopfieldå®Œå…¨ç‰ˆ
> 3. Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Œå…¨è¨¼æ˜
> 4. Classical Hopfieldæ­´å²
> 5. RBMå®Œå…¨ç‰ˆï¼ˆCD-k / PCDï¼‰
> 6. MCMCç†è«–ï¼ˆè©³ç´°é‡£ã‚Šåˆã„ / Ergodicityï¼‰
> 7. HMCï¼ˆHamiltonian MC / Leapfrogï¼‰
> 8. Langevin Dynamicsæ¦‚è¦
> 9. çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶šï¼ˆè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ / å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰
> 10. ç›¸è»¢ç§» / Ising / Grokking
> 11. Energy Matching
> 12. Energy-based World Models
>
> ãƒšãƒ³ã¨ç´™ã‚’ç”¨æ„ã€‚æ•°å¼ã‚’"èª­ã‚€"ã®ã§ã¯ãªã"å°å‡º"ã™ã‚‹ã€‚

### 3.1 EBMã®åŸºæœ¬å®šç¾©

#### 3.1.1 ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã¨ç¢ºç‡åˆ†å¸ƒ

**å®šç¾©** (Energy-Based Model):

ãƒ‡ãƒ¼ã‚¿ $x \in \mathcal{X}$ ã«å¯¾ã—ã¦ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° $E_\theta: \mathcal{X} \to \mathbb{R}$ ã‚’å®šç¾©ã™ã‚‹ã€‚ç¢ºç‡åˆ†å¸ƒã¯ä»¥ä¸‹ã®Gibbsåˆ†å¸ƒã§ä¸ãˆã‚‰ã‚Œã‚‹:

$$
p_\theta(x) = \frac{1}{Z(\theta)} \exp(-E_\theta(x))
$$

ã“ã“ã§ $Z(\theta)$ ã¯ **æ­£è¦åŒ–å®šæ•°**ï¼ˆPartition Functionï¼‰:

$$
Z(\theta) = \int_{\mathcal{X}} \exp(-E_\theta(x)) dx
$$

**è§£é‡ˆ**:
- $E_\theta(x)$ ãŒ**ä½ã„**ã»ã© $p_\theta(x)$ ãŒ**é«˜ã„**
- ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°ç‚¹ = æœ€ã‚‚ç¢ºç‡ãŒé«˜ã„çŠ¶æ…‹
- $Z(\theta)$ ã¯å…¨ç©ºé–“ã®ç©åˆ† â†’ **è¨ˆç®—å›°é›£**

#### 3.1.2 è² ã®å¯¾æ•°å°¤åº¦ã¨EBM

ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}(x)$ ã‚’EBMã§è¿‘ä¼¼ã—ãŸã„ã€‚è² ã®å¯¾æ•°å°¤åº¦:

$$
-\log p_\theta(x) = E_\theta(x) + \log Z(\theta)
$$

ã“ã®å¼ã®æ„å‘³:
- **ç¬¬1é …** $E_\theta(x)$: ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’**ä½ã**ã™ã‚‹ï¼ˆãƒ‡ãƒ¼ã‚¿é ˜åŸŸã§ç¢ºç‡ã‚’ä¸Šã’ã‚‹ï¼‰
- **ç¬¬2é …** $\log Z(\theta)$: å…¨ä½“ã®æ­£è¦åŒ–ï¼ˆä»–ã®é ˜åŸŸã§ç¢ºç‡ã‚’ä¸‹ã’ã‚‹ï¼‰

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\mathcal{D} = \{x^{(i)}\}_{i=1}^N$ ã«å¯¾ã™ã‚‹è² ã®å¯¾æ•°å°¤åº¦:

$$
\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^N \left[E_\theta(x^{(i)}) + \log Z(\theta)\right]
$$

#### 3.1.3 EBMè¨“ç·´ã®å›°é›£æ€§

**å•é¡Œ**: $Z(\theta)$ ã®å‹¾é…è¨ˆç®—:

$$
\frac{\partial \log Z(\theta)}{\partial \theta} = \frac{1}{Z(\theta)} \frac{\partial Z(\theta)}{\partial \theta}
$$

$$
= \frac{1}{Z(\theta)} \int_{\mathcal{X}} \frac{\partial \exp(-E_\theta(x))}{\partial \theta} dx
$$

$$
= \frac{1}{Z(\theta)} \int_{\mathcal{X}} \exp(-E_\theta(x)) \left(-\frac{\partial E_\theta(x)}{\partial \theta}\right) dx
$$

$$
= -\int_{\mathcal{X}} p_\theta(x) \frac{\partial E_\theta(x)}{\partial \theta} dx
$$

$$
= -\mathbb{E}_{x \sim p_\theta} \left[\frac{\partial E_\theta(x)}{\partial \theta}\right]
$$

ã—ãŸãŒã£ã¦ã€æå¤±é–¢æ•°ã®å‹¾é…:

$$
\frac{\partial \mathcal{L}(\theta)}{\partial \theta} = \frac{1}{N} \sum_{i=1}^N \frac{\partial E_\theta(x^{(i)})}{\partial \theta} - \mathbb{E}_{x \sim p_\theta} \left[\frac{\partial E_\theta(x)}{\partial \theta}\right]
$$

**è§£é‡ˆ**:
- **ç¬¬1é …**: ãƒ‡ãƒ¼ã‚¿ $x^{(i)}$ ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’**ä¸‹ã’ã‚‹**å‹¾é…ï¼ˆæ­£ä¾‹ï¼‰
- **ç¬¬2é …**: ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒ $p_\theta$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã—ãŸã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’**ä¸Šã’ã‚‹**å‹¾é…ï¼ˆè² ä¾‹ï¼‰

**å›°é›£æ€§**:
1. $\mathbb{E}_{x \sim p_\theta}[\cdot]$ ã®è¨ˆç®—ã« $p_\theta$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦
2. $p_\theta$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«MCMCãŒå¿…è¦ â†’ é…ã„
3. å„å‹¾é…ã‚¹ãƒ†ãƒƒãƒ—ã§MCMCã‚’åæŸã•ã›ã‚‹å¿…è¦ â†’ éå¸¸ã«é…ã„

> **Note:** **EBMè¨“ç·´ã®æœ¬è³ª**: ã€Œãƒ‡ãƒ¼ã‚¿é ˜åŸŸã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’ä¸‹ã’ã‚‹ã€ã¨ã€Œãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®é ˜åŸŸã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’ä¸Šã’ã‚‹ã€ã®ãƒãƒ©ãƒ³ã‚¹ã€‚è² ä¾‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå›°é›£ã€‚Contrastive Divergence (CD-k) ã¯ã“ã®è¿‘ä¼¼æ‰‹æ³•ã€‚

### 3.2 Modern Hopfield Networkå®Œå…¨ç‰ˆ

#### 3.2.1 Classical Hopfield Networkå¾©ç¿’ï¼ˆ1982ï¼‰

**Classical Hopfield Energy**:

$$
E(x) = -\frac{1}{2} \sum_{i,j} W_{ij} x_i x_j = -\frac{1}{2} x^\top W x
$$

ã“ã“ã§ $x \in \{-1, +1\}^N$ï¼ˆäºŒå€¤çŠ¶æ…‹ï¼‰ã€$W$ ã¯å¯¾ç§°è¡Œåˆ—ï¼ˆ$W = W^\top$ï¼‰ã€å¯¾è§’æˆåˆ† $W_{ii} = 0$ã€‚

**ãƒ‘ã‚¿ãƒ¼ãƒ³è¨˜æ†¶**:

$M$ å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ $\{\xi^\mu\}_{\mu=1}^M$ ã‚’è¨˜æ†¶ã™ã‚‹ãŸã‚ã€Hebbã®å­¦ç¿’å‰‡:

$$
W_{ij} = \frac{1}{N} \sum_{\mu=1}^M \xi^\mu_i \xi^\mu_j
$$

**Update Rule**ï¼ˆéåŒæœŸæ›´æ–°ï¼‰:

$$
x_i \leftarrow \text{sign}\left(\sum_j W_{ij} x_j\right)
$$

**è¨˜æ†¶å®¹é‡**: $M \lesssim 0.14 N$ï¼ˆçŠ¶æ…‹æ•°ã«æ¯”ä¾‹ï¼‰

#### 3.2.2 Modern Hopfield Network (2020)

**Modern Hopfield Energy** (Ramsauer+ 2020):

$$
E(x) = -\text{lse}\left(\beta X^\top x\right) + \frac{1}{2}\|x\|^2 + \frac{1}{\beta}\log M + \frac{M}{2\beta}
$$

ã“ã“ã§:
- $X = [\xi^1, \ldots, \xi^M] \in \mathbb{R}^{d \times M}$: è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³è¡Œåˆ—
- $\text{lse}(z) = \log \sum_i \exp(z_i)$: log-sum-exp
- $\beta > 0$: é€†æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $x \in \mathbb{R}^d$: **é€£ç¶šçŠ¶æ…‹**

å®šæ•°é …ã‚’ç„¡è¦–ã™ã‚‹ã¨:

$$
E(x) = -\text{lse}\left(\beta X^\top x\right) + \frac{1}{2}\|x\|^2
$$

**Update Rule**ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ï¼‰:

$$
\nabla_x E(x) = -\frac{\beta X \exp(\beta X^\top x)}{\sum_j \exp(\beta X^\top_j x)} + x = 0
$$

ã—ãŸãŒã£ã¦:

$$
x^{t+1} = X \text{softmax}(\beta X^\top x^t)
$$

$$
= \sum_{i=1}^M \frac{\exp(\beta \langle x^t, \xi^i \rangle)}{\sum_j \exp(\beta \langle x^t, \xi^j \rangle)} \xi^i
$$

**è¨˜æ†¶å®¹é‡**: $M \lesssim \exp(d)$ï¼ˆ**æŒ‡æ•°çš„**ï¼‰

**ç†è«–çš„ä¿è¨¼** (Ramsauer+ 2020):
- **å®šç†**: $\beta = d$ ã®ã¨ãã€$M = \exp(c d)$ï¼ˆ$c$ ã¯å®šæ•°ï¼‰å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨˜æ†¶å¯èƒ½
- **æ¤œç´¢èª¤å·®**: $\|x^* - \xi^{\mu^*}\| \lesssim \exp(-d)$ï¼ˆæŒ‡æ•°çš„ã«å°ã•ã„ï¼‰
- **åæŸ**: 1å›ã®æ›´æ–°ã§æœ€è¿‘æ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åæŸ

#### 3.2.3 Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Œå…¨è¨¼æ˜

**Claim**: Modern Hopfieldã®Update Rule = Self-Attention

**è¨¼æ˜**:

Modern Hopfieldã®æ›´æ–°å¼:

$$
x^{t+1} = \sum_{i=1}^M \frac{\exp(\beta \langle x^t, \xi^i \rangle)}{\sum_j \exp(\beta \langle x^t, \xi^j \rangle)} \xi^i
$$

è¡Œåˆ—å½¢å¼ã§æ›¸ãã¨:

$$
x^{t+1} = X \text{softmax}(\beta X^\top x^t)
$$

Self-Attention:

$$
\text{Attention}(Q, K, V) = V \cdot \text{softmax}\left(\frac{K^\top Q}{\sqrt{d}}\right)
$$

ã“ã“ã§:
- $Q = x^t$ï¼ˆã‚¯ã‚¨ãƒªï¼‰
- $K = X$ï¼ˆã‚­ãƒ¼ï¼‰
- $V = X$ï¼ˆãƒãƒªãƒ¥ãƒ¼ï¼‰
- $\beta = 1/\sqrt{d}$ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼‰

ä»£å…¥ã™ã‚‹ã¨:

$$
\text{Attention}(x^t, X, X) = X \cdot \text{softmax}\left(\frac{X^\top x^t}{\sqrt{d}}\right)
$$

$\beta = 1/\sqrt{d}$ ã¨ã™ã‚‹ã¨:

$$
= X \cdot \text{softmax}(\beta X^\top x^t)
$$

$$
= x^{t+1}
$$

**çµè«–**: Modern Hopfieldã®çŠ¶æ…‹æ›´æ–° = Self-Attention$\quad \blacksquare$

> **Note:** **ç­‰ä¾¡æ€§ã®æ„å‘³**:
> 1. **Transformerã¯é€£æƒ³è¨˜æ†¶ãƒã‚·ãƒ³**: Attentionã¯è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆKey-Valueï¼‰ã‹ã‚‰æœ€è¿‘æ¥ã‚’æ¤œç´¢
> 2. **è¨˜æ†¶å®¹é‡**: Modern Hopfieldã®æŒ‡æ•°çš„å®¹é‡ â†’ Transformerã®é•·è·é›¢ä¾å­˜æ€§èƒ½ã®ç†è«–çš„æ ¹æ‹ 
> 3. **ç‰©ç†å­¦çš„è§£é‡ˆ**: Attentionã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ã®Update Rule

#### 3.2.4 Modern Hopfieldé€£ç¶šæ™‚é–“ç‰ˆï¼ˆ2025ï¼‰

**è«–æ–‡**: Santos+ (2025) [arXiv:2502.10122](https://arxiv.org/abs/2502.10122) "Modern Hopfield Networks with Continuous-Time Memories"

**å•é¡Œæ„è­˜**: é›¢æ•£çš„ãªè¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ $\{\xi^i\}_{i=1}^M$ â†’ å¤§è¦æ¨¡è¨˜æ†¶ã§è¨ˆç®—ã‚³ã‚¹ãƒˆãƒ»ãƒ¡ãƒ¢ãƒªãŒçˆ†ç™º

**ææ¡ˆ**: é€£ç¶šçš„ãªè¨˜æ†¶ $\xi(t)$ï¼ˆ$t \in [0, T]$ï¼‰ã‚’å°å…¥

**é€£ç¶šæ™‚é–“ã‚¨ãƒãƒ«ã‚®ãƒ¼**:

$$
E(x) = -\int_0^T \log \exp(\beta \langle x, \xi(t) \rangle) dt + \frac{1}{2}\|x\|^2
$$

**Update Rule**:

$$
x^{t+1} = \int_0^T \frac{\exp(\beta \langle x^t, \xi(t) \rangle)}{\int_0^T \exp(\beta \langle x^t, \xi(s) \rangle) ds} \xi(t) dt
$$

**é›¢æ•£ â†’ é€£ç¶šã®å¯¾å¿œ**:

| é›¢æ•£ Modern Hopfield | é€£ç¶šæ™‚é–“ç‰ˆ |
|:--------------------|:---------|
| $\sum_{i=1}^M$ | $\int_0^T dt$ |
| $\xi^i$ | $\xi(t)$ |
| $M$ å€‹ã®è¨˜æ†¶ | é€£ç¶šçš„ãªè¨˜æ†¶ |

**å¿œç”¨**: å‹•ç”»ç”Ÿæˆãƒ»æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é€£æƒ³è¨˜æ†¶

### 3.3 Classical Hopfield Networkï¼ˆæ­´å²ï¼‰

#### 3.3.1 èµ·æºï¼ˆ1982ï¼‰

**John J. Hopfield** (1982) "Neural networks and physical systems with emergent collective computational abilities"

**å‹•æ©Ÿ**: è„³ã®é€£æƒ³è¨˜æ†¶ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–

**Classical Hopfieldã®å®šç¾©**:

çŠ¶æ…‹: $x \in \{-1, +1\}^N$

ã‚¨ãƒãƒ«ã‚®ãƒ¼:

$$
E(x) = -\frac{1}{2} x^\top W x - b^\top x
$$

ã“ã“ã§ $W$ ã¯å¯¾ç§°è¡Œåˆ—ï¼ˆ$W = W^\top$ï¼‰ã€å¯¾è§’æˆåˆ† $W_{ii} = 0$ã€‚

**å‹•åŠ›å­¦**ï¼ˆéåŒæœŸæ›´æ–°ï¼‰:

$$
x_i(t+1) = \text{sign}\left(\sum_j W_{ij} x_j(t) + b_i\right)
$$

**ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¸›å°‘å®šç†**:

å„æ›´æ–°ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¯å˜èª¿ã«æ¸›å°‘ï¼ˆã¾ãŸã¯ä¸å¤‰ï¼‰:

$$
E(x(t+1)) \leq E(x(t))
$$

**è¨¼æ˜**:

$x_i$ ã‚’æ›´æ–°å‰ $x_i^{\text{old}}$ã€æ›´æ–°å¾Œ $x_i^{\text{new}}$ ã¨ã™ã‚‹ã€‚

$$
\Delta E = E(x^{\text{new}}) - E(x^{\text{old}})
$$

$$
= -\frac{1}{2}(x^{\text{new}})^\top W x^{\text{new}} + \frac{1}{2}(x^{\text{old}})^\top W x^{\text{old}}
$$

$x_i$ ä»¥å¤–ã¯å¤‰åŒ–ã—ãªã„ã®ã§ã€$W_{ii} = 0$ ã‚ˆã‚Š:

$$
\Delta E = -x_i^{\text{new}} \left(\sum_j W_{ij} x_j\right) + x_i^{\text{old}} \left(\sum_j W_{ij} x_j\right)
$$

$$
= (x_i^{\text{old}} - x_i^{\text{new}}) \left(\sum_j W_{ij} x_j\right)
$$

æ›´æ–°å‰‡ $x_i^{\text{new}} = \text{sign}\left(\sum_j W_{ij} x_j\right)$ ã‚ˆã‚Šã€$x_i^{\text{new}}$ ã¨ $\sum_j W_{ij} x_j$ ã¯åŒç¬¦å·ï¼ˆã¾ãŸã¯0ï¼‰ã€‚

ã—ãŸãŒã£ã¦:
- $x_i^{\text{new}} \neq x_i^{\text{old}}$ ã®ã¨ãã€$(x_i^{\text{old}} - x_i^{\text{new}})$ ã¨ $\sum_j W_{ij} x_j$ ã¯é€†ç¬¦å· â†’ $\Delta E \leq 0$
- $x_i^{\text{new}} = x_i^{\text{old}}$ ã®ã¨ãã€$\Delta E = 0$

$\blacksquare$

**çµè«–**: éåŒæœŸæ›´æ–°ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒå˜èª¿æ¸›å°‘ â†’ å±€æ‰€æœ€å°å€¤ï¼ˆå›ºå®šç‚¹ï¼‰ã«åæŸ

#### 3.3.2 Hebbã®å­¦ç¿’å‰‡

**ç›®çš„**: ãƒ‘ã‚¿ãƒ¼ãƒ³ $\{\xi^\mu\}\_{\mu=1}^M$ ã‚’è¨˜æ†¶ã™ã‚‹é‡ã¿è¡Œåˆ— $W$ ã‚’å­¦ç¿’

**Hebbian Rule**:

$$
W_{ij} = \frac{1}{N} \sum_{\mu=1}^M \xi^\mu_i \xi^\mu_j
$$

**ç›´æ„Ÿ**: "Neurons that fire together, wire together"

**è¨˜æ†¶å®¹é‡**:

ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ $\xi^\mu \in \{-1, +1\}^N$ã€ãƒ©ãƒ³ãƒ€ãƒ ã§ç›´äº¤ã«è¿‘ã„ã¨ãã€è¨˜æ†¶å®¹é‡:

$$
M_{\max} \approx 0.14 N
$$

**è¨¼æ˜ã®è©³ç´°**:

ãƒ‘ã‚¿ãƒ¼ãƒ³ $\xi^\mu$ ãŒå›ºå®šç‚¹ã§ã‚ã‚‹ãŸã‚ã«ã¯ã€å…¨ã¦ã® $i$ ã«ã¤ã„ã¦:

$$
\xi^\mu_i = \text{sign}\left(\sum_j W_{ij} \xi^\mu_j\right)
$$

Hebbã®å­¦ç¿’å‰‡ $W_{ij} = \frac{1}{N} \sum_{\nu=1}^M \xi^\nu_i \xi^\nu_j$ ã‚’ä»£å…¥:

$$
\xi^\mu_i = \text{sign}\left(\sum_j \frac{1}{N} \sum_{\nu=1}^M \xi^\nu_i \xi^\nu_j \xi^\mu_j\right)
$$

$$
= \text{sign}\left(\frac{1}{N} \sum_{\nu=1}^M \xi^\nu_i \sum_j \xi^\nu_j \xi^\mu_j\right)
$$

$$
= \text{sign}\left(\frac{1}{N} \sum_{\nu=1}^M \xi^\nu_i \langle \xi^\nu, \xi^\mu \rangle\right)
$$

ã“ã“ã§ $\langle \xi^\nu, \xi^\mu \rangle = \sum_j \xi^\nu_j \xi^\mu_j$ ã¯å†…ç©ã€‚

**ä¿¡å·é …** ($\mu = \nu$):

$$
\frac{1}{N} \xi^\mu_i \langle \xi^\mu, \xi^\mu \rangle = \frac{1}{N} \xi^\mu_i \cdot N = \xi^\mu_i
$$

ã“ã‚Œã¯æ­£ã—ã„ç¬¦å·ã€‚

**ãƒã‚¤ã‚ºé …** ($\mu \neq \nu$):

$$
\frac{1}{N} \sum_{\nu \neq \mu} \xi^\nu_i \langle \xi^\nu, \xi^\mu \rangle
$$

ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒãƒ©ãƒ³ãƒ€ãƒ ãªã‚‰ã€$\langle \xi^\nu, \xi^\mu \rangle \approx 0$ï¼ˆç›´äº¤ã«è¿‘ã„ï¼‰ã€‚ã ãŒå®Œå…¨ã«0ã§ã¯ãªã„ã€‚

**çµ±è¨ˆçš„è§£æ**:

$\xi^\nu_i, \xi^\mu_j$ ãŒi.i.d. Bernoulli(1/2)ï¼ˆå€¤ $\pm 1$ï¼‰ã¨ã™ã‚‹ã¨:

$$
\mathbb{E}[\langle \xi^\nu, \xi^\mu \rangle] = \sum_j \mathbb{E}[\xi^\nu_j \xi^\mu_j] = 0
$$

$$
\text{Var}[\langle \xi^\nu, \xi^\mu \rangle] = \sum_j \text{Var}[\xi^\nu_j \xi^\mu_j] = N
$$

ã—ãŸãŒã£ã¦ã€$\langle \xi^\nu, \xi^\mu \rangle \sim \mathcal{N}(0, N)$ï¼ˆä¸­å¿ƒæ¥µé™å®šç†ï¼‰ã€‚

ãƒã‚¤ã‚ºé …ã®åˆ†æ•£:

$$
\text{Var}\left[\sum_{\nu \neq \mu} \xi^\nu_i \langle \xi^\nu, \xi^\mu \rangle\right] \approx (M-1) N
$$

æ¨™æº–åå·® $\sim \sqrt{MN}$ã€‚

**ä¿¡å·å¯¾é›‘éŸ³æ¯”** (SNR):

$$
\text{SNR} = \frac{\text{ä¿¡å·}}{\text{ãƒã‚¤ã‚º}} = \frac{N}{\sqrt{MN}} = \sqrt{\frac{N}{M}}
$$

$\text{sign}$ ãŒé«˜ç¢ºç‡ã§æ­£ã—ãå‹•ä½œã™ã‚‹ã«ã¯ã€$\text{SNR} \gg 1$:

$$
\sqrt{\frac{N}{M}} \gg 1 \quad \Rightarrow \quad M \ll N
$$

**å³å¯†ãªè§£æ** (Amit, Gutfreund, Sompolinsky 1985):

èª¤ã‚Šç¢ºç‡ $P_{\text{error}} \approx \frac{1}{2\sqrt{\pi}} \int_{\text{SNR}}^\infty e^{-z^2/2} dz$

$P_{\text{error}} < 0.01$ ã‚’è¦æ±‚ã™ã‚‹ã¨ã€$\text{SNR} > 2.33$ ãŒå¿…è¦ã€‚

$$
\sqrt{\frac{N}{M}} > 2.33 \quad \Rightarrow \quad M < \frac{N}{5.43} \approx 0.184 N
$$

çµŒé¨“çš„ã«ã¯ $M_{\max} \approx 0.14 N$ ãŒå®Ÿç”¨çš„ãªé™ç•Œ$\quad \blacksquare$

**æ•°å€¤ä¾‹**:
- $N = 100$: $M_{\max} \approx 14$ ãƒ‘ã‚¿ãƒ¼ãƒ³
- $N = 1000$: $M_{\max} \approx 140$ ãƒ‘ã‚¿ãƒ¼ãƒ³

#### 3.3.3 Classical Hopfieldã®é™ç•Œ

**å•é¡Œç‚¹**:
1. **å®¹é‡åˆ¶é™**: $M \sim 0.14 N$ â€” çŠ¶æ…‹æ•°ã«æ¯”ä¾‹ï¼ˆç·šå½¢ï¼‰
2. **ã‚¹ãƒ‘ãƒªã‚¢ã‚¹å›ºå®šç‚¹**: è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ä»¥å¤–ã®å›ºå®šç‚¹ãŒå­˜åœ¨
3. **ãƒ‘ã‚¿ãƒ¼ãƒ³å¹²æ¸‰**: é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ··åŒã•ã‚Œã‚‹

**ã‚¹ãƒ‘ãƒªã‚¢ã‚¹å›ºå®šç‚¹ã®ä¾‹**:

2ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ $\xi^1 = [+1, +1, +1, +1]$ã€$\xi^2 = [+1, -1, -1, +1]$ ã‚’è¨˜æ†¶ã€‚

é‡ã¿è¡Œåˆ—:

$$
W = \frac{1}{4}(\xi^1 (\xi^1)^\top + \xi^2 (\xi^2)^\top)
$$

ã‚¹ãƒ‘ãƒªã‚¢ã‚¹å›ºå®šç‚¹: $x = [+1, -1, +1, -1]$ ã‚‚å®‰å®šï¼ˆ$\xi^1$ ã¨ $\xi^2$ ã®æ··åˆï¼‰

**Modern Hopfieldã«ã‚ˆã‚‹è§£æ±º**:
- æŒ‡æ•°çš„å®¹é‡ $M \sim \exp(d)$
- 1å›æ›´æ–°ã§åæŸ
- ã‚¹ãƒ‘ãƒªã‚¢ã‚¹å›ºå®šç‚¹ã®ç†è«–çš„æŠ‘åˆ¶

### 3.4 Restricted Boltzmann Machineå®Œå…¨ç‰ˆ

#### 3.4.1 RBMã®å®šç¾©

**æ§‹é€ **:
- **å¯è¦–å±¤**ï¼ˆVisibleï¼‰: $v \in \{0, 1\}^{n_v}$
- **éš ã‚Œå±¤**ï¼ˆHiddenï¼‰: $h \in \{0, 1\}^{n_h}$
- **é‡ã¿**: $W \in \mathbb{R}^{n_v \times n_h}$
- **ãƒã‚¤ã‚¢ã‚¹**: $b \in \mathbb{R}^{n_v}$ã€$c \in \mathbb{R}^{n_h}$

**åˆ¶ç´„**: å¯è¦–å±¤å†…ã®æ¥ç¶šãªã—ã€éš ã‚Œå±¤å†…ã®æ¥ç¶šãªã—ï¼ˆäºŒéƒ¨ã‚°ãƒ©ãƒ•ï¼‰

**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**:

$$
E(v, h) = -v^\top W h - b^\top v - c^\top h
$$

**åŒæ™‚åˆ†å¸ƒ**:

$$
p(v, h) = \frac{1}{Z} \exp(-E(v, h))
$$

$$
Z = \sum_{v, h} \exp(-E(v, h))
$$

**å‘¨è¾ºåˆ†å¸ƒ**:

$$
p(v) = \sum_h p(v, h) = \frac{1}{Z} \sum_h \exp(-E(v, h))
$$

#### 3.4.2 æ¡ä»¶ä»˜ãåˆ†å¸ƒ

**äºŒéƒ¨æ§‹é€ ã®åˆ©ç‚¹**: æ¡ä»¶ä»˜ãåˆ†å¸ƒãŒå› æ•°åˆ†è§£

$$
p(h | v) = \prod_{j=1}^{n_h} p(h_j | v)
$$

$$
p(v | h) = \prod_{i=1}^{n_v} p(v_i | h)
$$

**å°å‡º**:

$$
p(h_j = 1 | v) = \frac{p(h_j = 1, v)}{\sum_{h_j'} p(h_j = h_j', v)}
$$

$$
= \frac{\exp(c_j + \sum_i W_{ij} v_i)}{\exp(c_j + \sum_i W_{ij} v_i) + 1}
$$

$$
= \sigma\left(c_j + \sum_i W_{ij} v_i\right)
$$

ã“ã“ã§ $\sigma(x) = 1/(1 + \exp(-x))$ ã¯ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã€‚

åŒæ§˜ã«:

$$
p(v_i = 1 | h) = \sigma\left(b_i + \sum_j W_{ij} h_j\right)
$$

#### 3.4.3 Partition Function $Z$ ã®è¨ˆç®—å›°é›£æ€§

$$
Z = \sum_{v \in \{0,1\}^{n_v}} \sum_{h \in \{0,1\}^{n_h}} \exp(-E(v, h))
$$

è¨ˆç®—é‡: $O(2^{n_v + n_h})$ â€” æŒ‡æ•°çš„

**å‘¨è¾ºåŒ–ãƒˆãƒªãƒƒã‚¯**:

$$
Z = \sum_v \exp(b^\top v) \sum_h \exp(h^\top(W^\top v + c))
$$

$$
= \sum_v \exp(b^\top v) \prod_j (1 + \exp(c_j + \sum_i W_{ij} v_i))
$$

ã“ã‚Œã§ã‚‚ $O(2^{n_v})$ â€” å®Ÿç”¨ä¸å¯

#### 3.4.4 Contrastive Divergence (CD-k)

**Hintonã®ã‚¢ã‚¤ãƒ‡ã‚¢** (2002): è² ã®å¯¾æ•°å°¤åº¦ã®å‹¾é…ã‚’è¿‘ä¼¼

**å®Œå…¨ãªå‹¾é…**:

$$
\frac{\partial \log p(v)}{\partial W_{ij}} = \mathbb{E}_{h \sim p(h|v)} [v_i h_j] - \mathbb{E}_{v', h' \sim p(v', h')} [v'_i h'_j]
$$

- **ç¬¬1é …**: ãƒ‡ãƒ¼ã‚¿ä¾å­˜é …ï¼ˆæ­£ä¾‹ï¼‰â€” è¨ˆç®—å®¹æ˜“
- **ç¬¬2é …**: ãƒ¢ãƒ‡ãƒ«ä¾å­˜é …ï¼ˆè² ä¾‹ï¼‰â€” $p(v', h')$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ï¼ˆå›°é›£ï¼‰

**CD-kè¿‘ä¼¼**:

1. **åˆæœŸåŒ–**: $v^{(0)} = v_{\text{data}}$ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰
2. **kå›ã®Gibbsã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**:
   - $h^{(t)} \sim p(h | v^{(t)})$
   - $v^{(t+1)} \sim p(v | h^{(t)})$
3. **è¿‘ä¼¼å‹¾é…**:

$$
\frac{\partial \log p(v)}{\partial W_{ij}} \approx \mathbb{E}_{h \sim p(h|v^{(0)})} [v^{(0)}_i h_j] - \mathbb{E}_{h \sim p(h|v^{(k)})} [v^{(k)}_i h_j]
$$

**k=1ã®ã¨ã** (CD-1):
- 1å›ã ã‘Gibbsã‚¹ãƒ†ãƒƒãƒ—
- è² ä¾‹ $v^{(1)}$ ã¯ $v^{(0)}$ ã‹ã‚‰è¿‘ã„ â†’ ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š
- ã ãŒå®Ÿç”¨çš„ã«æ©Ÿèƒ½ã™ã‚‹ï¼ˆçµŒé¨“çš„ï¼‰

**åæŸæ€§**: CD-kã¯ $\log p(v)$ ã‚’ç›´æ¥æœ€å¤§åŒ–ã—ãªã„ã€‚åˆ¥ã®ç›®çš„é–¢æ•°ï¼ˆContrastive Divergenceï¼‰ã‚’æœ€å°åŒ– â†’ ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š

#### 3.4.5 Persistent Contrastive Divergence (PCD)

**å•é¡Œ**: CD-kã¯æ¯å›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆæœŸåŒ– â†’ è² ä¾‹ãŒå¸¸ã«ãƒ‡ãƒ¼ã‚¿è¿‘å‚

**PCD** (Tieleman 2008):
- **Persistent Chain**: Markové€£é–ã‚’æ°¸ç¶šçš„ã«ç¶­æŒ
- å„ãƒŸãƒ‹ãƒãƒƒãƒã§ã€å‰å›ã® $v^{(k)}$ ã‹ã‚‰ç¶™ç¶š
- â†’ è² ä¾‹ãŒãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒã«ã‚ˆã‚Šè¿‘ã„

**åˆ©ç‚¹**: CD-kã‚ˆã‚Šãƒã‚¤ã‚¢ã‚¹ãŒå°‘ãªã„ã€é•·ã„ãƒã‚§ãƒ¼ãƒ³ã‚’ç¶­æŒ

### 3.5 MCMCç†è«–ï¼ˆç¬¬5å›åŸºç¤ã®æ·±åŒ–ï¼‰

> **Note:** **ç¬¬5å›ã§ã®åŸºç¤**: Markové€£é–ãƒ»Metropolis-Hastingsã®åŸºç¤ã‚’å°å…¥æ¸ˆã¿ã€‚æœ¬å›ã¯EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ–‡è„ˆã§ã®ç†è«–æ·±åŒ–ã€‚

#### 3.5.1 Markové€£é–å¾©ç¿’

**å®šç¾©**: çŠ¶æ…‹ç©ºé–“ $\mathcal{X}$ ä¸Šã®ç¢ºç‡éç¨‹ $\{X_t\}_{t=0}^\infty$

**Markovæ€§**: $P(X_{t+1} | X_0, \ldots, X_t) = P(X_{t+1} | X_t)$

**é·ç§»ã‚«ãƒ¼ãƒãƒ«**: $T(x' | x) = P(X_{t+1} = x' | X_t = x)$

#### 3.5.2 è©³ç´°é‡£ã‚Šåˆã„ï¼ˆDetailed Balanceï¼‰

**å®šç¾©**: ç¢ºç‡åˆ†å¸ƒ $\pi(x)$ ãŒé·ç§»ã‚«ãƒ¼ãƒãƒ« $T(x' | x)$ ã«é–¢ã—ã¦è©³ç´°é‡£ã‚Šåˆã„ã‚’æº€ãŸã™:

$$
\pi(x) T(x' | x) = \pi(x') T(x | x')
$$

**å®šç†**: è©³ç´°é‡£ã‚Šåˆã„ã‚’æº€ãŸã™ã¨ãã€$\pi$ ã¯ $T$ ã®å®šå¸¸åˆ†å¸ƒã€‚

**è¨¼æ˜**:

$$
\sum_x \pi(x) T(x' | x) = \sum_x \pi(x') T(x | x') = \pi(x') \sum_x T(x | x') = \pi(x')
$$

$\blacksquare$

#### 3.5.3 ã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰æ€§ï¼ˆErgodicityï¼‰

**å®šç¾©**: Markové€£é–ãŒã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰çš„ â‡” ä»»æ„ã®åˆæœŸåˆ†å¸ƒã‹ã‚‰å®šå¸¸åˆ†å¸ƒã«åæŸ

**æ¡ä»¶**:
1. **æ—¢ç´„æ€§**ï¼ˆIrreducibilityï¼‰: å…¨ã¦ã®çŠ¶æ…‹ãŒç›¸äº’ã«åˆ°é”å¯èƒ½
2. **éå‘¨æœŸæ€§**ï¼ˆAperiodicityï¼‰: å‘¨æœŸçš„ãªã‚µã‚¤ã‚¯ãƒ«ãŒãªã„

**å®šç†**: æ—¢ç´„ãƒ»éå‘¨æœŸçš„ã§è©³ç´°é‡£ã‚Šåˆã„ã‚’æº€ãŸã™Markové€£é–ã¯ã€å®šå¸¸åˆ†å¸ƒã«åæŸã€‚

#### 3.5.4 Metropolis-Hastingså®Œå…¨ç‰ˆ

**ç›®æ¨™**: ç›®æ¨™åˆ†å¸ƒ $\pi(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ$\pi$ ã‹ã‚‰ã®ç›´æ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯å›°é›£ï¼‰

**ææ¡ˆåˆ†å¸ƒ**: $q(x' | x)$ï¼ˆ$x$ ã‹ã‚‰ $x'$ ã‚’ææ¡ˆï¼‰

**å—ç†ç¢ºç‡**:

$$
\alpha(x' | x) = \min\left(1, \frac{\pi(x') q(x | x')}{\pi(x) q(x' | x)}\right)
$$

**è©³ç´°é‡£ã‚Šåˆã„ã®è¨¼æ˜**:

é·ç§»ã‚«ãƒ¼ãƒãƒ«:

$$
T(x' | x) = q(x' | x) \alpha(x' | x) + \delta(x - x') r(x)
$$

ã“ã“ã§ $r(x) = 1 - \int q(x' | x) \alpha(x' | x) dx'$ ã¯æ£„å´ç¢ºç‡ã€‚

è©³ç´°é‡£ã‚Šåˆã„ã‚’ç¤ºã™:

$$
\pi(x) q(x' | x) \alpha(x' | x) = \pi(x) q(x' | x) \min\left(1, \frac{\pi(x') q(x | x')}{\pi(x) q(x' | x)}\right)
$$

$$
= \min\left(\pi(x) q(x' | x), \pi(x') q(x | x')\right)
$$

å¯¾ç§°æ€§ã‚ˆã‚Š:

$$
= \pi(x') q(x | x') \min\left(1, \frac{\pi(x) q(x' | x)}{\pi(x') q(x | x')}\right)
$$

$$
= \pi(x') q(x | x') \alpha(x | x')
$$

$\blacksquare$

#### 3.5.5 Gibbs Sampling

**è¨­å®š**: å¤šå¤‰é‡åˆ†å¸ƒ $\pi(x_1, \ldots, x_d)$ ã‹ã‚‰ã€æ¡ä»¶ä»˜ãåˆ†å¸ƒ $\pi(x_i | x_{-i})$ ãŒåˆ©ç”¨å¯èƒ½

**è©³ç´°é‡£ã‚Šåˆã„**:

Gibbs Samplingã¯Metropolis-Hastingsã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ï¼ˆå—ç†ç¢ºç‡ = 1ï¼‰

$\blacksquare$

**RBMã¨ã®æ¥ç¶š**:

RBMã®Gibbs Sampling:
- $h \sim p(h | v)$ï¼ˆéš ã‚Œå±¤ã‚’æ›´æ–°ï¼‰
- $v \sim p(v | h)$ï¼ˆå¯è¦–å±¤ã‚’æ›´æ–°ï¼‰

ã“ã‚Œã‚’äº¤äº’ã«ç¹°ã‚Šè¿”ã™ â†’ $p(v, h)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«

### 3.6 Hamiltonian Monte Carlo (HMC)

#### 3.6.1 å‹•æ©Ÿ

**å•é¡Œ**: Metropolis-Hastings / Gibbs Samplingã¯é«˜æ¬¡å…ƒã§åŠ¹ç‡ãŒæ‚ªã„
- ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ â†’ é…ã„æ··åˆ
- é«˜æ¬¡å…ƒã§å—ç†ç‡ãŒä½ä¸‹

**HMC**: HamiltonåŠ›å­¦ã‚’åˆ©ç”¨ã—ã¦åŠ¹ç‡çš„ã«æ¢ç´¢

#### 3.6.2 HamiltonianåŠ›å­¦å¾©ç¿’

**ç³»**: ä½ç½® $q \in \mathbb{R}^d$ã€é‹å‹•é‡ $p \in \mathbb{R}^d$

**Hamiltonian**:

$$
H(q, p) = U(q) + K(p)
$$

- $U(q)$: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼
- $K(p) = \frac{1}{2}p^\top M^{-1} p$: é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆ$M$ ã¯è³ªé‡è¡Œåˆ—ï¼‰

**Hamiltonæ–¹ç¨‹å¼**:

$$
\frac{dq}{dt} = \frac{\partial H}{\partial p} = M^{-1} p
$$

$$
\frac{dp}{dt} = -\frac{\partial H}{\partial q} = -\nabla U(q)
$$

**ä¿å­˜å‰‡**: $H(q, p)$ ã¯æ™‚é–“ã§ä¸å¤‰

**ä½“ç©ä¿å­˜**: ä½ç›¸ç©ºé–“ã§ã®ä½“ç©ãŒä¿å­˜ã•ã‚Œã‚‹ï¼ˆLiouvilleå®šç†ï¼‰

#### 3.6.3 HMCã®ã‚¢ã‚¤ãƒ‡ã‚¢

**ç›®æ¨™**: åˆ†å¸ƒ $\pi(q)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**æ‹¡å¼µ**: è£œåŠ©å¤‰æ•° $p$ ã‚’å°å…¥ã—ã€åŒæ™‚åˆ†å¸ƒã‚’å®šç¾©:

$$
\pi(q, p) = \frac{1}{Z} \exp(-H(q, p))
$$

ã“ã“ã§ $H(q, p) = -\log \pi(q) + \frac{1}{2}p^\top M^{-1} p$

$U(q) = -\log \pi(q)$ï¼ˆè² ã®å¯¾æ•°å¯†åº¦ï¼‰ã¨ã™ã‚‹ã¨ã€$\pi(q, p)$ ã‹ã‚‰ã®å‘¨è¾ºåˆ†å¸ƒ $\int \pi(q, p) dp = \pi(q)$

**æˆ¦ç•¥**:
1. Hamiltonæ–¹ç¨‹å¼ã«å¾“ã£ã¦ $(q, p)$ ã‚’æ™‚é–“ç™ºå±•
2. Hamiltonian $H$ ãŒä¿å­˜ â†’ å—ç†ç¢ºç‡ = 1ï¼ˆç†è«–ä¸Šï¼‰
3. $p$ ã‚’å‘¨è¾ºåŒ– â†’ $q$ ã®ã‚µãƒ³ãƒ—ãƒ«

#### 3.6.4 Leapfrogç©åˆ†

**å•é¡Œ**: Hamiltonæ–¹ç¨‹å¼ã®é€£ç¶šæ™‚é–“ç©åˆ†ã¯è¨ˆç®—ä¸å¯ â†’ é›¢æ•£åŒ–ãŒå¿…è¦

**Leapfrogæ³•**ï¼ˆã‚·ãƒ³ãƒ—ãƒ¬ã‚¯ãƒ†ã‚£ãƒƒã‚¯ç©åˆ†ï¼‰:

$$
p_{t+\epsilon/2} = p_t - \frac{\epsilon}{2} \nabla U(q_t)
$$

$$
q_{t+\epsilon} = q_t + \epsilon M^{-1} p_{t+\epsilon/2}
$$

$$
p_{t+\epsilon} = p_{t+\epsilon/2} - \frac{\epsilon}{2} \nabla U(q_{t+\epsilon})
$$

**æ€§è³ª**:
- **å¯é€†**: $(q_t, p_t) \to (q_{t+\epsilon}, p_{t+\epsilon})$ ã¨é€†å‘ããŒåŒä¸€ã®å¤‰æ›
- **ä½“ç©ä¿å­˜**: ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ = 1
- **è¿‘ä¼¼çš„ã«Hamiltonianä¿å­˜**: é›¢æ•£åŒ–èª¤å·® $O(\epsilon^3)$ per step

#### 3.6.5 HMCã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- $\epsilon$: ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼ˆå°ã•ã„ã»ã©æ­£ç¢ºã€å¤§ãã„ã»ã©æ¢ç´¢ç¯„å›²åºƒã„ï¼‰
- $L$: Leapfrog stepsæ•°ï¼ˆå¤§ãã„ã»ã©é ãã¾ã§ç§»å‹•ï¼‰
- $M$: è³ªé‡è¡Œåˆ—ï¼ˆé€šå¸¸ $M = I$ï¼‰

#### 3.6.6 No-U-Turn Sampler (NUTS)

**å•é¡Œ**: HMCã¯ $L$ ã¨ $\epsilon$ ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¿…è¦

**NUTS** (Hoffman & Gelman 2014):
- $L$ ã‚’é©å¿œçš„ã«æ±ºå®š
- U-turnï¼ˆè»Œé“ãŒæŠ˜ã‚Šè¿”ã™ï¼‰ã‚’æ¤œå‡ºã—ã¦è‡ªå‹•åœæ­¢
- Stanç­‰ã®PPLã§æ¨™æº–å®Ÿè£…

### 3.7 Langevin Dynamicsæ¦‚è¦

> **Note:** **å®Œå…¨ç‰ˆã¯ç¬¬35å›**: Score Matching & Langevin Dynamics ã§è©³ç´°å°å‡ºã€‚æœ¬å›ã¯EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ã—ã¦ã®ä½ç½®ã¥ã‘ã®ã¿ã€‚

**Langevin Dynamics**:

$$
dx_t = -\nabla U(x_t) dt + \sqrt{2} dW_t
$$

ã“ã“ã§ $dW_t$ ã¯Browné‹å‹•ã€‚

**é›¢æ•£åŒ–**ï¼ˆEuler-Maruyamaï¼‰:

$$
x_{t+1} = x_t - \epsilon \nabla U(x_t) + \sqrt{2\epsilon} \, \xi_t
$$

ã“ã“ã§ $\xi_t \sim \mathcal{N}(0, I)$ã€‚

**EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¸ã®å¿œç”¨**:

$U(x) = E_\theta(x)$ ã¨ã™ã‚‹ã¨:

$$
x_{t+1} = x_t - \epsilon \nabla_x E_\theta(x_t) + \sqrt{2\epsilon} \, \xi_t
$$

ã“ã‚Œã¯ $p_\theta(x) \propto \exp(-E_\theta(x))$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

**åˆ©ç‚¹**: Metropoliså—ç†ãƒ»æ£„å´ä¸è¦ â†’ é«˜æ¬¡å…ƒã§åŠ¹ç‡çš„

**å•é¡Œ**: $\epsilon \to 0$ ã®æ¥µé™ã§æ­£ç¢ºï¼ˆé›¢æ•£åŒ–èª¤å·®ï¼‰

### 3.8 çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š

#### 3.8.1 Gibbsåˆ†å¸ƒã¨çµ±è¨ˆåŠ›å­¦

**çµ±è¨ˆåŠ›å­¦ã®ã‚«ãƒãƒ‹ã‚«ãƒ«åˆ†å¸ƒ**:

æ¸©åº¦ $T$ ã®ã¨ãã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ $E$ ã®çŠ¶æ…‹ã®ç¢ºç‡:

$$
p(x) = \frac{1}{Z} \exp\left(-\frac{E(x)}{k_B T}\right)
$$

ã“ã“ã§ $k_B$ ã¯Boltzmannå®šæ•°ã€$Z$ ã¯åˆ†é…é–¢æ•°ã€‚

**EBMã¨ã®å¯¾å¿œ**:
- $k_B T = 1$ ã¨ã™ã‚‹ã¨ $p(x) = \frac{1}{Z} \exp(-E(x))$
- EBMã®Gibbsåˆ†å¸ƒ = ã‚«ãƒãƒ‹ã‚«ãƒ«åˆ†å¸ƒ

#### 3.8.2 è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼

**Helmholtzè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼**:

$$
F = -k_B T \log Z
$$

**è§£é‡ˆ**:
- $F$ ãŒæœ€å° â‡” ç†±å¹³è¡¡çŠ¶æ…‹
- $Z$ ãŒå¤§ãã„ â‡” $F$ ãŒå°ã•ã„ â‡” å¤šãã®çŠ¶æ…‹ãŒè¨±å®¹ã•ã‚Œã‚‹

**EBMã¨ã®å¯¾å¿œ**:

$$
F(\theta) = -\log Z(\theta) = -\log \int \exp(-E_\theta(x)) dx
$$

EBMè¨“ç·´ = $F(\theta)$ ã®æœ€å°åŒ–

#### 3.8.3 Grokking as Phase Transitionï¼ˆè©³ç´°ç‰ˆï¼‰

**Grokkingç¾è±¡**: è¨“ç·´ãƒ­ã‚¹ãŒæ—©æœŸã«åæŸå¾Œã€å¤§å¹…ã«é…ã‚Œã¦æ±åŒ–æ€§èƒ½ãŒæ€¥ä¸Šæ˜‡ã™ã‚‹ç¾è±¡ï¼ˆPower+ 2022ï¼‰

**çµ±è¨ˆç‰©ç†çš„è§£é‡ˆ** (Liu+ 2023, Varma+ 2023):

Grokkingã¯ **ä¸€æ¬¡ç›¸è»¢ç§»** ã¨ã—ã¦ç†è§£å¯èƒ½ã€‚

**è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼**:

$$
F(\theta; T) = E_{\text{train}}(\theta) - T S(\theta)
$$

- $E_{\text{train}}(\theta)$: è¨“ç·´èª¤å·®ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼é …ï¼‰
- $S(\theta)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¹±é›‘ã•ï¼‰
- $T$: æœ‰åŠ¹æ¸©åº¦ï¼ˆå­¦ç¿’ç‡ / weight decay ã«å¯¾å¿œï¼‰

**2ã¤ã®çŠ¶æ…‹**:

1. **è¨˜æ†¶ç›¸** (Memorization Phase):
   - é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: $S(\theta) \gg 0$
   - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æš—è¨˜ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¤‡é›‘ï¼‰
   - æ±åŒ–ã—ãªã„

2. **æ±åŒ–ç›¸** (Generalization Phase):
   - ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: $S(\theta) \approx 0$
   - ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ«ãƒ¼ãƒ«ã‚’å­¦ç¿’ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ•´ç†ï¼‰
   - é«˜ã„æ±åŒ–æ€§èƒ½

**ç›¸è»¢ç§»ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹**:

è¨“ç·´åˆæœŸ: è¨˜æ†¶ç›¸ãŒå®‰å®šï¼ˆ$E$ ä¸‹ãŒã‚‹ã€$S$ é«˜ã„ï¼‰
â†“
é•·æ™‚é–“è¨“ç·´: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚³ã‚¹ãƒˆãŒåŠ¹ã„ã¦ãã‚‹
â†“
è‡¨ç•Œç‚¹é€šé: $F_{\text{memorization}} = F_{\text{generalization}}$
â†“
æ±åŒ–ç›¸ã¸é·ç§»: $S$ æ€¥æ¸›ã€æ±åŒ–æ€§èƒ½æ€¥ä¸Šæ˜‡

**é·ç§»ç¢ºç‡** (Metropolis-like):

$$
P(\text{memorization} \to \text{generalization}) \propto \exp\left(-\frac{\Delta F}{T_{\text{eff}}}\right)
$$

ã“ã“ã§:

$$
\Delta F = F_{\text{gen}} - F_{\text{mem}} = \Delta E - T \Delta S
$$

**æ¡ä»¶**:
- $\Delta E > 0$: æ±åŒ–è§£ã¯è¨“ç·´èª¤å·®ãŒã‚ãšã‹ã«é«˜ã„
- $\Delta S < 0$: æ±åŒ–è§£ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒä½ã„
- $T$ é«˜ã„ï¼ˆå­¦ç¿’ç‡é«˜ã„ï¼‰: é·ç§»ã—ã‚„ã™ã„
- $T$ ä½ã„ï¼ˆweight decayå¼·ã„ï¼‰: é·ç§»ã—ã«ãã„ â†’ GrokkingãŒè¦³æ¸¬ã•ã‚Œã‚‹

**å…¸å‹çš„ãªGrokkingæ›²ç·š**:

```mermaid
graph LR
    A[Epoch 0-100] -->|"Train loss â†“<br/>Test loss â†“<br/>Entropy é«˜"| B[è¨˜æ†¶ç›¸]
    B -->|"Epoch 100-1000<br/>Train/Test flat<br/>Entropy é«˜ç¶­æŒ"| C[æº–å®‰å®šçŠ¶æ…‹]
    C -->|"è‡¨ç•Œç‚¹<br/>Entropy æ€¥æ¸›"| D[ç›¸è»¢ç§»]
    D -->|"Epoch 1000-1500<br/>Test loss â†“â†“<br/>Entropy ä½"| E[æ±åŒ–ç›¸]

    style D fill:#ff9,stroke:#333,stroke-width:4px
```

**Ising model ã¨ã®é¡ä¼¼**:

Ising modelï¼ˆç£æ€§ä½“ãƒ¢ãƒ‡ãƒ«ï¼‰ã§ã‚‚ä¸€æ¬¡ç›¸è»¢ç§»ãŒèµ·ã“ã‚‹:
- é«˜æ¸©: ã‚¹ãƒ”ãƒ³ãŒãƒ©ãƒ³ãƒ€ãƒ ï¼ˆé«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰
- ä½æ¸©: ã‚¹ãƒ”ãƒ³ãŒæ•´åˆ—ï¼ˆä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰
- è‡¨ç•Œæ¸©åº¦ $T_c$ ã§ç›¸è»¢ç§»

NNã®Grokking:
- é«˜å­¦ç¿’ç‡: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒãƒ©ãƒ³ãƒ€ãƒ ï¼ˆè¨˜æ†¶ç›¸ï¼‰
- ä½å­¦ç¿’ç‡ + weight decay: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ•´åˆ—ï¼ˆæ±åŒ–ç›¸ï¼‰
- è‡¨ç•Œepochæ•°ã§ç›¸è»¢ç§»

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- Epoch 0-500: Train loss â†“, Test loss æ¨ªã°ã„, Entropy é«˜
- Epoch 500-2000: å…¨ã¦æ¨ªã°ã„ï¼ˆæº–å®‰å®šçŠ¶æ…‹ï¼‰
- Epoch 2000-2500: Test loss æ€¥æ¸›ï¼ˆGrokking!ï¼‰, Entropy æ€¥æ¸›
- Epoch 2500+: æ±åŒ–ç›¸ã«å®‰å®š

#### 3.8.4 å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼

**è¨­å®š**: çœŸã®åˆ†å¸ƒ $p^*(x)$ã€è¿‘ä¼¼åˆ†å¸ƒ $q(x)$

**å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼**:

$$
\mathcal{F}(q) = \mathbb{E}_{q(x)} [E(x)] + H(q)
$$

ã“ã“ã§ $H(q) = -\int q(x) \log q(x) dx$ ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€‚

**å®šç†**: $\mathcal{F}(q) \geq F$ ï¼ˆç­‰å·æˆç«‹ â‡” $q = p^*$ï¼‰

**è¨¼æ˜**:

$$
\mathcal{F}(q) - F = \mathbb{E}_{q} [E(x)] + H(q) + \log Z
$$

$$
= \mathbb{E}_{q} [E(x)] + \mathbb{E}_{q} [-\log q(x)] + \log Z
$$

$$
= \mathbb{E}_{q} [-\log q(x) + E(x) + \log Z]
$$

$$
= \mathbb{E}_{q} \left[\log \frac{\exp(-E(x))}{q(x) Z}\right]
$$

$$
= \mathbb{E}_{q} \left[\log \frac{p^*(x)}{q(x)}\right]
$$

$$
= D_{\text{KL}}(q \| p^*) \geq 0
$$

$\blacksquare$

**å¤‰åˆ†æ¨è«–ã¨ã®æ¥ç¶š**: VAEã®ELBO = å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°åŒ–

### 3.9 ç›¸è»¢ç§» / Isingæ¨¡å‹ / Grokking

#### 3.9.1 Isingæ¨¡å‹

**å®šç¾©**:

ã‚¹ãƒ”ãƒ³ $s_i \in \{-1, +1\}$ ãŒæ ¼å­ä¸Šã«é…ç½®ã€‚

ã‚¨ãƒãƒ«ã‚®ãƒ¼:

$$
E(s) = -J \sum_{\langle i, j \rangle} s_i s_j - h \sum_i s_i
$$

- $J$: ç›¸äº’ä½œç”¨ï¼ˆ$J > 0$ ã§å¼·ç£æ€§ï¼‰
- $h$: å¤–éƒ¨ç£å ´

**Gibbsåˆ†å¸ƒ**:

$$
p(s) = \frac{1}{Z} \exp(-\beta E(s))
$$

ã“ã“ã§ $\beta = 1/(k_B T)$ ã¯é€†æ¸©åº¦ã€‚

**ç›¸è»¢ç§»**: æ¸©åº¦ $T$ ãŒè‡¨ç•Œæ¸©åº¦ $T_c$ ã‚’ä¸‹å›ã‚‹ã¨ã€è‡ªç™ºç£åŒ–ãŒç™ºç”Ÿ

#### 3.9.2 Grokking = ä¸€æ¬¡ç›¸è»¢ç§»

**Grokking** (Power+ 2022): ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒéå­¦ç¿’å¾Œã«çªç„¶ä¸€èˆ¬åŒ–

**ç¾è±¡**:
1. è¨“ç·´èª¤å·®ã¯ã‚¨ãƒãƒƒã‚¯100ã§0ã«åæŸ
2. æ¤œè¨¼èª¤å·®ã¯ã‚¨ãƒãƒƒã‚¯100-10000ã§åœæ»
3. ã‚¨ãƒãƒƒã‚¯10000ä»¥é™ã€æ¤œè¨¼èª¤å·®ãŒçªç„¶ä½ä¸‹ â†’ ä¸€èˆ¬åŒ–

**çµ±è¨ˆç‰©ç†çš„èª¬æ˜** (ICLR 2024):

Grokkingã¯**ä¸€æ¬¡ç›¸è»¢ç§»**ã¨ã—ã¦ç†è§£å¯èƒ½ã€‚

**è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—**:

$$
F(\theta) = E(\theta) - TS(\theta)
$$

- $E(\theta)$: ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆè¨“ç·´èª¤å·®ï¼‰
- $S(\theta)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§ï¼‰
- $T$: æ¸©åº¦ï¼ˆå­¦ç¿’ç‡ã«å¯¾å¿œï¼‰

**2ã¤ã®ç›¸**:

1. **ãƒ¡ãƒ¢ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ç›¸**ï¼ˆå±€æ‰€æœ€é©ï¼‰:
   - $E$ ä½ã„ï¼ˆè¨“ç·´èª¤å·®å°ï¼‰
   - $S$ ä½ã„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éé©åˆï¼‰
   - **é«˜ã‚¨ãƒãƒ«ã‚®ãƒ¼çŠ¶æ…‹**ï¼ˆä¸€èˆ¬åŒ–æ€§èƒ½ä½ã„ï¼‰

2. **ä¸€èˆ¬åŒ–ç›¸**ï¼ˆå¤§åŸŸæœ€é©ï¼‰:
   - $E$ ä½ã„ï¼ˆè¨“ç·´èª¤å·®å°ï¼‰
   - $S$ é«˜ã„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ï¼‰
   - **ä½ã‚¨ãƒãƒ«ã‚®ãƒ¼çŠ¶æ…‹**ï¼ˆä¸€èˆ¬åŒ–æ€§èƒ½é«˜ã„ï¼‰

**ç›¸è»¢ç§»ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **:

è¨“ç·´åˆæœŸ:
- SGDãŒãƒ¡ãƒ¢ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ç›¸ã«æ•ç²
- ã‚¨ãƒãƒ«ã‚®ãƒ¼éšœå£ãŒé«˜ãã€ä¸€èˆ¬åŒ–ç›¸ã«ç§»è¡Œã§ããªã„

é•·æ™‚é–“è¨“ç·´:
- ç¢ºç‡çš„ãƒã‚¤ã‚ºï¼ˆSGDã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ï¼‰ãŒã‚¨ãƒãƒ«ã‚®ãƒ¼éšœå£ã‚’ä¹—ã‚Šè¶Šãˆã‚‹
- **ç›¸è»¢ç§»**: ãƒ¡ãƒ¢ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ç›¸ â†’ ä¸€èˆ¬åŒ–ç›¸
- æ¤œè¨¼èª¤å·®ãŒçªç„¶ä½ä¸‹

**æ•°å¼**:

ã‚¨ãƒãƒ«ã‚®ãƒ¼éšœå£ã®é«˜ã• $\Delta F$:

$$
\Delta F = F_{\text{barrier}} - F_{\text{mem}}
$$

ç›¸è»¢ç§»ç¢ºç‡:

$$
P_{\text{transition}} \propto \exp(-\Delta F / T)
$$

$T$ ãŒé«˜ã„ï¼ˆå­¦ç¿’ç‡å¤§ï¼‰ã¾ãŸã¯ $\Delta F$ ãŒä½ã„ã»ã©ã€ç›¸è»¢ç§»ãŒæ—©ãèµ·ã“ã‚‹ã€‚

**å®Ÿé¨“çš„æ¤œè¨¼**:
- å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹ â†’ GrokkingãŒæ—©ãèµ·ã“ã‚‹
- Weight Decayã‚’åŠ ãˆã‚‹ â†’ $\Delta F$ ã‚’ä¸‹ã’ã‚‹ â†’ GrokkingãŒæ—©ãèµ·ã“ã‚‹

#### 3.9.3 Isingæ¨¡å‹ã¨ã®æ¥ç¶š

**Isingæ¨¡å‹** â†” **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ** å¯¾å¿œ:

| Isingæ¨¡å‹ | ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ |
|:----------|:---------------|
| ã‚¹ãƒ”ãƒ³ $s_i \in \{-1, +1\}$ | ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ´»æ€§ $h_i$ |
| ç›¸äº’ä½œç”¨ $J_{ij}$ | é‡ã¿ $W_{ij}$ |
| å¤–éƒ¨ç£å ´ $h_i$ | ãƒã‚¤ã‚¢ã‚¹ $b_i$ |
| æ¸©åº¦ $T$ | ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« / å­¦ç¿’ç‡ |
| ç›¸è»¢ç§» | Grokking / å­¦ç¿’ã®ç›¸è»¢ç§» |

**Curieæ¸©åº¦** $T_c$:

$$
k_B T_c = J \cdot z
$$

ã“ã“ã§ $z$ ã¯é…ä½æ•°ï¼ˆæœ€è¿‘æ¥ã‚¹ãƒ”ãƒ³æ•°ï¼‰ã€‚

$T < T_c$: å¼·ç£æ€§ç›¸ï¼ˆå…¨ã‚¹ãƒ”ãƒ³ãŒæƒã†ï¼‰
$T > T_c$: å¸¸ç£æ€§ç›¸ï¼ˆã‚¹ãƒ”ãƒ³ãŒãƒ©ãƒ³ãƒ€ãƒ ï¼‰

**ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ç›¸è»¢ç§»**:

$T_c$ ã«ç›¸å½“ã™ã‚‹ã€Œè‡¨ç•Œå­¦ç¿’ç‡ã€ãŒå­˜åœ¨:
- å­¦ç¿’ç‡ < $T_c$: éå­¦ç¿’ç›¸ï¼ˆãƒ¡ãƒ¢ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
- å­¦ç¿’ç‡ > $T_c$: ä¸€èˆ¬åŒ–ç›¸

#### 3.9.4 è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®å¤‰åˆ†åŸç†

**Helmholtzè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–åŸç†**:

ç†±å¹³è¡¡çŠ¶æ…‹ã§ã¯ã€è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ $F$ ãŒæœ€å°:

$$
F = \langle E \rangle - TS
$$

**å°å‡º**:

ã‚«ãƒãƒ‹ã‚«ãƒ«åˆ†å¸ƒ:

$$
p(x) = \frac{1}{Z} \exp(-\beta E(x))
$$

$$
Z = \sum_x \exp(-\beta E(x))
$$

å¹³å‡ã‚¨ãƒãƒ«ã‚®ãƒ¼:

$$
\langle E \rangle = \sum_x p(x) E(x)
$$

ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼:

$$
S = -\sum_x p(x) \log p(x)
$$

è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼:

$$
F = -k_B T \log Z
$$

$$
= \langle E \rangle - TS
$$

**å¤‰åˆ†åŸç†**:

ä»»æ„ã®åˆ†å¸ƒ $q(x)$ ã«å¯¾ã—ã€å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼:

$$
\mathcal{F}(q) = \sum_x q(x) E(x) + k_B T \sum_x q(x) \log q(x)
$$

**å®šç†**: $\mathcal{F}(q) \geq F$ï¼ˆç­‰å·æˆç«‹ â‡” $q = p$ï¼‰

**è¨¼æ˜**ï¼ˆæ—¢å‡ºã®å†æ²ï¼‰:

$$
\mathcal{F}(q) - F = D_{\text{KL}}(q \| p) \geq 0
$$

$\blacksquare$

**EBMè¨“ç·´ã¨ã®æ¥ç¶š**:

EBMè¨“ç·´ = $F(\theta)$ ã®æœ€å°åŒ– = å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°åŒ– = VAEã®ELBOæœ€å¤§åŒ–ã¨åŒä¸€åŸç†

### 3.10 Energy Matchingï¼ˆ2025ï¼‰â€” Flow Matching + EBMçµ±ä¸€ç†è«–

**è«–æ–‡**: [arXiv:2504.10612](https://arxiv.org/abs/2504.10612) "Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling"

#### 3.10.1 å‹•æ©Ÿ

**Flow Matchingã®å•é¡Œ**:
- é«˜é€Ÿè¨“ç·´ãƒ»ç”Ÿæˆï¼ˆæ±ºå®šè«–çš„OTç›´ç·šè¼¸é€ï¼‰
- ã ãŒå¤šå³°çš„åˆ†å¸ƒã®è¡¨ç¾ãŒå›°é›£ï¼ˆç›´ç·šè¼¸é€ã¯å˜å³°çš„ã«åæŸã—ã‚„ã™ã„ï¼‰

**EBMã®å•é¡Œ**:
- è¡¨ç¾åŠ›ãŒæ¥µã‚ã¦é«˜ã„ï¼ˆä»»æ„ã®åˆ†å¸ƒï¼‰
- ã ãŒè¨“ç·´ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå›°é›£ï¼ˆMCMCå¿…è¦ï¼‰

**Energy Matchingã®ææ¡ˆ**:
- ä¸¡è€…ã®åˆ©ç‚¹ã‚’çµ±åˆ
- é æ–¹ã§ã¯Flow Matchingï¼ˆé«˜é€Ÿè¼¸é€ï¼‰
- ãƒ‡ãƒ¼ã‚¿å¤šæ§˜ä½“è¿‘å‚ã§ã¯EBMï¼ˆå¤šå³°çš„è¡¨ç¾ï¼‰

#### 3.10.2 å®šå¼åŒ–

**æ™‚é–“ä¾å­˜ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**:

$$
E(x, t) = E_{\text{transport}}(x, t) + \tau(t) \cdot E_{\text{entropic}}(x)
$$

ã“ã“ã§:
- $E_{\text{transport}}(x, t) = \frac{1}{2}\|x - \mu_t\|^2$: OTè¼¸é€é …ï¼ˆ$\mu_t = t x_{\text{data}} + (1-t)x_{\text{noise}}$ï¼‰
- $E_{\text{entropic}}(x) = -\log \rho(x)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒƒã‚¯é …ï¼ˆå­¦ç¿’ã•ã‚Œã‚‹ã‚¹ã‚«ãƒ©ãƒ¼å ´ï¼‰
- $\tau(t)$: æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ$\tau(0) = 0$ã€$\tau(1) = 1$ï¼‰

**æ™‚é–“ä¾å­˜Gibbsåˆ†å¸ƒ**:

$$
p_t(x) = \frac{1}{Z_t} \exp(-E(x, t))
$$

#### 3.10.3 æ™‚é–“ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å½¹å‰²

**$t = 0$**ï¼ˆãƒã‚¤ã‚ºåˆ†å¸ƒï¼‰:

$$
E(x, 0) = \frac{1}{2}\|x - x_{\text{noise}}\|^2 + 0 \cdot E_{\text{entropic}}(x)
$$

â†’ ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºåˆ†å¸ƒï¼ˆå˜å³°çš„ï¼‰

**$t \in (0, 1)$**ï¼ˆè¼¸é€ä¸­ï¼‰:

$$
E(x, t) = \frac{1}{2}\|x - \mu_t\|^2 + \tau(t) \cdot E_{\text{entropic}}(x)
$$

- $\tau(t)$ å°: OTç›´ç·šè¼¸é€ãŒæ”¯é…çš„ï¼ˆé«˜é€Ÿç§»å‹•ï¼‰
- $\tau(t)$ å¤§: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒƒã‚¯é …ãŒå¯„ä¸ï¼ˆå¤šå³°æ€§å‡ºç¾ï¼‰

**$t = 1$**ï¼ˆãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒï¼‰:

$$
E(x, 1) = 0 + 1 \cdot E_{\text{entropic}}(x)
$$

â†’ ç´”ç²‹ãªEBMï¼ˆå¤šå³°çš„åˆ†å¸ƒï¼‰

#### 3.10.4 è¨“ç·´ç›®çš„é–¢æ•°

**Flow Matchingã¨ã®æ¥ç¶š**:

ãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t(x)$:

$$
v_t(x) = -\nabla_x E(x, t)
$$

$$
= -(x - \mu_t) - \tau(t) \nabla_x E_{\text{entropic}}(x)
$$

**è¨“ç·´æå¤±** (Conditional Flow Matching):

$$
\mathcal{L}(\theta) = \mathbb{E}_{t, x_{\text{data}}, x_t} \left[\|v_\theta(x_t, t) - v_t^*(x_t)\|^2\right]
$$

ã“ã“ã§ $x_t = \mu_t + \epsilon$ã€$\epsilon \sim \mathcal{N}(0, \sigma_t^2 I)$ã€‚

**é‡è¦**: $E_{\text{entropic}}(x)$ ã¯**æ™‚é–“ç‹¬ç«‹**ã®ã‚¹ã‚«ãƒ©ãƒ¼å ´ã¨ã—ã¦å­¦ç¿’ã•ã‚Œã‚‹ã€‚

#### 3.10.5 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**Probability Flow ODE**:

$$
\frac{dx}{dt} = v_t(x)
$$

$$
= -(x - \mu_t) - \tau(t) \nabla_x E_{\text{entropic}}(x)
$$

**ç‰¹å¾´**:
- $t = 0 \to 0.5$: é«˜é€ŸOTè¼¸é€ï¼ˆå¤§åŸŸç§»å‹•ï¼‰
- $t = 0.5 \to 1$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒƒã‚¯é …ãŒåŠ¹ãï¼ˆå±€æ‰€èª¿æ•´ãƒ»å¤šå³°æ€§ï¼‰

#### 3.10.6 ç†è«–çš„ä¿è¨¼

**å®šç†** (Energy Matching, 2025):

æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\tau(t)$ ãŒé©åˆ‡ãªã‚‰ã€$p_1(x) \approx p_{\text{data}}(x)$ ã¨ãªã‚‹ã€‚

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

$t = 0$: $p_0(x) = \mathcal{N}(0, I)$ï¼ˆæ—¢çŸ¥ï¼‰

$t = 1$: $p_1(x) = \frac{1}{Z} \exp(-E_{\text{entropic}}(x))$ï¼ˆEBMï¼‰

Probability Flow ODEã¯ $p_t$ ã‚’ä¿å­˜ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ï¼ˆé€£ç¶šæ€§æ–¹ç¨‹å¼ï¼‰:

$$
\frac{\partial p_t}{\partial t} + \nabla_x \cdot (p_t v_t) = 0
$$

ã—ãŸãŒã£ã¦ã€$p_1$ ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«è¿‘ã¥ã$\quad \blacksquare$

#### 3.10.7 å®Ÿé¨“çµæœï¼ˆ2025è«–æ–‡ã‚ˆã‚Šï¼‰

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ãƒ¢ãƒ‡ãƒ« | FID | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚é–“ |
|:------------|:-------|:----|:----------------|
| CIFAR-10 | DDPM | 3.17 | 1000 steps (50s) |
| CIFAR-10 | Flow Matching | 3.92 | 100 steps (5s) |
| CIFAR-10 | **Energy Matching** | **2.84** | 100 steps (5s) |
| ImageNet 64x64 | EDM | 2.44 | 79 steps (4s) |
| ImageNet 64x64 | **Energy Matching** | **2.21** | 50 steps (2.5s) |

**çµè«–**: Energy Matching = Flow Matchingã®é€Ÿåº¦ + EBMã®è¡¨ç¾åŠ›

#### 3.10.8 å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ

**æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**:

$$
\tau(t) = \begin{cases}
0 & t < t_0 \\
\frac{t - t_0}{1 - t_0} & t \geq t_0
\end{cases}
$$

è«–æ–‡ã§ã¯ $t_0 = 0.5$ ãŒæ¨å¥¨ï¼ˆå‰åŠã¯OTã€å¾ŒåŠã¯EBMï¼‰ã€‚

### 3.11 Energy-based World Models â€” ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¨ã—ã¦ã®EBM

**è«–æ–‡**: ç¬¬41å›ã§å®Œå…¨ç‰ˆã€‚æœ¬å›ã¯æ¦‚å¿µã®ã¿ã€‚

#### 3.11.1 World Modelsã®å®šç¾©

**World Model**: ç’°å¢ƒã® dynamics $p(s_{t+1} | s_t, a_t)$ ã‚’å­¦ç¿’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«

- $s_t$: çŠ¶æ…‹ï¼ˆç”»åƒãƒ»ã‚»ãƒ³ã‚µãƒ¼å€¤ãªã©ï¼‰
- $a_t$: è¡Œå‹•
- $s_{t+1}$: æ¬¡çŠ¶æ…‹

#### 3.11.2 Energy-basedå®šå¼åŒ–

**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**:

$$
E(s_{t+1} | s_t, a_t) = -\log p(s_{t+1} | s_t, a_t)
$$

**æ¡ä»¶ä»˜ãGibbsåˆ†å¸ƒ**:

$$
p(s_{t+1} | s_t, a_t) = \frac{1}{Z(s_t, a_t)} \exp(-E(s_{t+1} | s_t, a_t))
$$

#### 3.11.3 ç‰©ç†æ³•å‰‡ã®çµ„ã¿è¾¼ã¿

**ç‰©ç†åˆ¶ç´„**:

ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã«ç‰©ç†æ³•å‰‡ã‚’ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰:

$$
E(s_{t+1} | s_t, a_t) = E_{\text{data}}(s_{t+1} | s_t, a_t) + \lambda E_{\text{physics}}(s_{t+1})
$$

ä¾‹: ä¿å­˜å‰‡

$$
E_{\text{physics}}(s_{t+1}) = \|E_{\text{kinetic}}(s_{t+1}) + E_{\text{potential}}(s_{t+1}) - E_{\text{total}}\|^2
$$

#### 3.11.4 å¤šå³°çš„æœªæ¥äºˆæ¸¬

**å¾“æ¥ã®æ±ºå®šè«–çš„World Model**:

$$
s_{t+1} = f_\theta(s_t, a_t)
$$

â†’ å˜ä¸€ã®æœªæ¥ã®ã¿äºˆæ¸¬

**Energy-based World Model**:

$$
p(s_{t+1} | s_t, a_t) = \frac{1}{Z} \exp(-E(s_{t+1} | s_t, a_t))
$$

â†’ è¤‡æ•°ã®æœªæ¥ã‚’ç¢ºç‡åˆ†å¸ƒã¨ã—ã¦è¡¨ç¾

**ä¾‹**: ãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ¼ãƒ ã§ç‰©ä½“ã‚’æ´ã‚€
- æˆåŠŸãƒ‘ã‚¹ï¼ˆç¢ºç‡80%ï¼‰
- å¤±æ•—ãƒ‘ã‚¹1ï¼ˆç¢ºç‡10%ï¼‰: ç‰©ä½“ãŒæ»‘ã‚‹
- å¤±æ•—ãƒ‘ã‚¹2ï¼ˆç¢ºç‡10%ï¼‰: ç‰©ä½“ãŒè»¢ãŒã‚‹

EBMã¯3ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã‚’å…¨ã¦è¡¨ç¾å¯èƒ½ã€‚

#### 3.11.5 JEPA / V-JPAã¨ã®æ¥ç¶š

**JEPA** (LeCun 2023): Joint-Embedding Predictive Architecture

**ã‚¨ãƒãƒ«ã‚®ãƒ¼è¦–ç‚¹ã§ã®å†è§£é‡ˆ**:

$$
E(z_{t+1}, z_t, a_t) = \|z_{t+1} - f_\theta(z_t, a_t)\|^2
$$

ã“ã“ã§ $z_t = \text{Encoder}(s_t)$ ã¯æ½œåœ¨è¡¨ç¾ã€‚

**V-JEPA**: Videoã§ã®JEPAå®Ÿè£… â†’ ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹é€£æƒ³è¨˜æ†¶ã¨ã—ã¦ç†è§£å¯èƒ½

#### 3.11.6 Transfusionã¨ã®æ¥ç¶š

**Transfusion** (2024): ARï¼ˆé›¢æ•£ï¼‰ + Diffusionï¼ˆé€£ç¶šï¼‰ã‚’çµ±ä¸€

**ã‚¨ãƒãƒ«ã‚®ãƒ¼è¦–ç‚¹**:

$$
E(x_{\text{image}}, x_{\text{text}}) = E_{\text{discrete}}(x_{\text{text}}) + E_{\text{continuous}}(x_{\text{image}})
$$

- $E_{\text{discrete}}$: è‡ªå·±å›å¸°ã®è² ã®å¯¾æ•°å°¤åº¦
- $E_{\text{continuous}}$: Diffusionã®ã‚¨ãƒãƒ«ã‚®ãƒ¼

â†’ ä¸¡è€…ã‚’å˜ä¸€ã®EBMã¨ã—ã¦çµ±ä¸€çš„ã«è¨“ç·´

---

> Progress: 50%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Modern Hopfield Networkã¨Transformerã®Self-AttentionãŒæ•°å­¦çš„ã«ç­‰ä¾¡ã«ãªã‚‹æ ¸å¿ƒã‚¹ãƒ†ãƒƒãƒ—ã‚’ç¤ºã—ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° $F = -\text{lse}(\beta, X^\top\xi)$ ã®å½¹å‰²ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. çµ±è¨ˆç‰©ç†ã®è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ $F = U - TS$ ã¨EBMã®å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å¯¾å¿œã•ã›ã€EBMã®è¨“ç·´ãŒã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ã¨ç­‰ä¾¡ãªç†ç”±ã‚’è¿°ã¹ã‚ˆã€‚

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Hopfield, J. J. (1982). "Neural networks and physical systems with emergent collective computational abilities." *Proceedings of the National Academy of Sciences*, 79(8), 2554-2558.
<https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554>

[^2]: Hinton, G. E. (2002). "Training products of experts by minimizing contrastive divergence." *Neural Computation*, 14(8), 1771-1800.
<https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf>

[^3]: Ramsauer, H., et al. (2020). "Hopfield Networks is All You Need." *ICLR 2021*.
<https://arxiv.org/abs/2008.02217>

[^4]: Santos, S., et al. (2025). "Modern Hopfield Networks with Continuous-Time Memories." *arXiv:2502.10122*.
<https://arxiv.org/abs/2502.10122>

[^5]: Dehmamy, N., et al. (2025). "NRGPT: An Energy-based Alternative for GPT." *arXiv:2512.16762*.
<https://arxiv.org/abs/2512.16762>

[^6]: Energy Matching (Balcerak, M., et al., 2025). "Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling." *arXiv:2504.10612*.
<https://arxiv.org/abs/2504.10612>

[^7]: Tieleman, T. (2008). "Training restricted Boltzmann machines using approximations to the likelihood gradient." *ICML 2008*.

[^8]: Hoffman, M. D., & Gelman, A. (2014). "The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo." *Journal of Machine Learning Research*, 15(1), 1593-1623.

[^9]: Smolensky, P. (1986). "Information processing in dynamical systems: Foundations of harmony theory." In *Parallel Distributed Processing*, Vol. 1.

[^10]: Nobel Prize (2024). "The Nobel Prize in Physics 2024." John J. Hopfield and Geoffrey E. Hinton.
<https://www.nobelprize.org/prizes/physics/2024/summary/>

[^11]: LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). "A tutorial on energy-based learning." In *Predicting Structured Data*, MIT Press.

### æ•™ç§‘æ›¸

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Chapter on EBMs]
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [Chapter 20: Deep Generative Models]
- MacKay, D. J. C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. [Chapter on Boltzmann Machines]
- Barber, D. (2012). *Bayesian Reasoning and Machine Learning*. Cambridge University Press. [Chapter on EBMs]

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
