---
title: "ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å‰ç·¨ã€‘ç†è«–ç·¨"
emoji: "ğŸ²"
type: "tech"
topics: ["machinelearning", "deeplearning", "probability", "python"]
published: true
---


# ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ â€” ä¸ç¢ºå®Ÿæ€§ã‚’æ•°å­¦ã§é£¼ã„ãªã‚‰ã™

> **ç¢ºç‡ã¨ã¯ã€Œã‚ã‹ã‚‰ãªã•ã€ã®è¨€èªã ã€‚ã“ã®è¨€èªã‚’æ“ã‚Œã‚‹è€…ã ã‘ãŒã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ¬è³ªã«è§¦ã‚Œã‚‰ã‚Œã‚‹ã€‚**

ç¬¬3å›ã§è¡Œåˆ—ã®åˆ†è§£ã¨å¾®åˆ†ã‚’æ‰‹ã«å…¥ã‚ŒãŸã€‚SVDã§ç©ºé–“ã‚’åˆ†è§£ã—ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã§å¤‰æ›ã®å±€æ‰€çš„æŒ¯ã‚‹èˆã„ã‚’æ‰ãˆã€è‡ªå‹•å¾®åˆ†ã§Backpropagationã®æ•°å­¦çš„åŸºç›¤ã‚’ç†è§£ã—ãŸã€‚ã ãŒã€ã“ã“ã§æ ¹æœ¬çš„ãªå•ã„ãŒç«‹ã¡ã¯ã ã‹ã‚‹ â€” **ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€Œãƒã‚¤ã‚ºã€ãŒã‚ã‚‹ã€‚ä¸ç¢ºå®Ÿæ€§ã‚’ã©ã†æ‰±ã†ã®ã‹ï¼Ÿ**

ç·šå½¢ä»£æ•°ã¯ã€Œç¢ºå®šã—ãŸé‡ã€ã®æ•°å­¦ã ã€‚è¡Œåˆ— $A$ ã‚’ã‹ã‘ã‚Œã°ã€ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{x}$ ã¯ç¢ºå®šçš„ã« $A\mathbf{x}$ ã«å¤‰æ›ã•ã‚Œã‚‹ã€‚ã ãŒç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã¯é•ã†ã€‚åŒã˜å…¥åŠ›ã«å¯¾ã—ã¦å‡ºåŠ›ãŒã°ã‚‰ã¤ãã€‚åŒã˜æ–‡è„ˆã«å¯¾ã—ã¦LLMãŒæ¯å›é•ã†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ã€‚ã“ã®ã€Œã°ã‚‰ã¤ãã€ã‚’è¨˜è¿°ã™ã‚‹æ•°å­¦ãŒç¢ºç‡è«–ã ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€ç¢ºç‡ç©ºé–“ã®å³å¯†ãªå®šç¾©ã‹ã‚‰å§‹ã‚ã¦ã€ç¢ºç‡åˆ†å¸ƒã®è¨˜è¿°ãƒ»æ“ä½œãƒ»æ¨å®šã‚’å®Œå…¨ã«ç¿’å¾—ã™ã‚‹ã€‚ãã—ã¦ã“ã‚ŒãŒå˜ãªã‚‹æ•°å­¦ã®æ¼”ç¿’ã§ã¯ãªã„ã“ã¨ã‚’ã€LLMã®è‡ªå·±å›å¸°ç”Ÿæˆ $p(x_t \mid x_{<t})$ â€” ã¾ã•ã«æ¡ä»¶ä»˜ãç¢ºç‡ãã®ã‚‚ã® â€” ã‚’é€šã˜ã¦ä½“æ„Ÿã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ² ç¢ºç‡ç©ºé–“<br/>(Î©,F,P)"] --> B["ğŸ“Š ç¢ºç‡å¤‰æ•°<br/>X: Î©â†’â„"]
    B --> C["ğŸ“ˆ ç¢ºç‡åˆ†å¸ƒ<br/>é›¢æ•£ãƒ»é€£ç¶š"]
    C --> D["ğŸ”„ ãƒ™ã‚¤ã‚ºæ¨è«–<br/>äº‹å¾Œâˆå°¤åº¦Ã—äº‹å‰"]
    D --> E["ğŸ“ MLE/MAP<br/>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š"]
    E --> F["ğŸ¯ Fisheræƒ…å ±é‡<br/>æ¨å®šã®é™ç•Œ"]
    style A fill:#e1f5fe
    style F fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’3è¡Œã§å‹•ã‹ã™

**ã‚´ãƒ¼ãƒ«**: ç¢ºç‡ã®æ ¸å¿ƒã‚’30ç§’ã§ä½“é¨“ã™ã‚‹ã€‚

```python
import numpy as np

# Bayes' theorem: P(A|B) = P(B|A) * P(A) / P(B)
prior = 0.01            # P(disease) = 1%
sensitivity = 0.95      # P(positive | disease) = 95%
false_positive = 0.05   # P(positive | healthy) = 5%
p_positive = sensitivity * prior + false_positive * (1 - prior)
posterior = sensitivity * prior / p_positive
print(f"Prior:     {prior:.2%}")
print(f"Posterior: {posterior:.2%}")   # 16.1% â€” not 95%!
```

å‡ºåŠ›:
```
Prior:     1.00%
Posterior: 16.10%
```

**ã“ã®3è¡Œã®è£ã«ã‚ã‚‹æ•°å¼**:

$$
P(\text{disease} \mid \text{positive}) = \frac{P(\text{positive} \mid \text{disease}) \cdot P(\text{disease})}{P(\text{positive})}
$$

æ¤œæŸ»ã®æ„Ÿåº¦ãŒ95%ã§ã‚‚ã€äº‹å‰ç¢ºç‡ãŒ1%ãªã‚‰ã€é™½æ€§ã¨å‡ºã¦ã‚‚å®Ÿéš›ã«ç—…æ°—ã§ã‚ã‚‹ç¢ºç‡ã¯**ãŸã£ãŸ16%**ã€‚ç›´æ„Ÿã«åã™ã‚‹ã€‚ã ãŒæ•°å¼ã¯å˜˜ã‚’ã¤ã‹ãªã„ã€‚ã“ã‚ŒãŒãƒ™ã‚¤ã‚ºã®å®šç† [^1] ã®åŠ›ã ã€‚

ã“ã®ã€Œäº‹å‰ã®ä¿¡å¿µã‚’æ–°ã—ã„è¨¼æ‹ ã§æ›´æ–°ã™ã‚‹ã€æ§‹é€ ã¯ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ ¹å¹¹ã«ç¾ã‚Œã‚‹ã€‚VAEã®äº‹å¾Œåˆ†å¸ƒ $q_\phi(\mathbf{z} \mid \mathbf{x})$ [^2] ã‚‚ã€ãƒ™ã‚¤ã‚ºã®å®šç†ã®å¤‰åˆ†è¿‘ä¼¼ã«ä»–ãªã‚‰ãªã„ã€‚

:::message
**é€²æ—: 3% å®Œäº†** ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’ã€Œå‹•ã‹ã—ã¦ã€ä½“é¨“ã—ãŸã€‚ç›´æ„Ÿã¨æ•°å­¦ã®ã‚®ãƒ£ãƒƒãƒ— â€” ã“ã‚ŒãŒç¢ºç‡è«–ã‚’å­¦ã¶ç†ç”±ã ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ç¢ºç‡åˆ†å¸ƒã‚’è§¦ã£ã¦éŠã¶

### 1.1 é›¢æ•£åˆ†å¸ƒ â€” Categoricalåˆ†å¸ƒã¨Softmax

LLMãŒæ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸ã¶ã¨ãã€èªå½™å…¨ä½“ã®ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—ã™ã‚‹ã€‚ã“ã‚Œã¯**Categoricalåˆ†å¸ƒ**ãã®ã‚‚ã®ã ã€‚

$$
p(x = k) = \pi_k, \quad \sum_{k=1}^{K} \pi_k = 1, \quad \pi_k \geq 0
$$

SoftmaxãŒè¿”ã™ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ã¯ã€ã¾ã•ã«Categoricalåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\boldsymbol{\pi} = (\pi_1, \ldots, \pi_K)$ ã ã€‚

```python
import numpy as np

def categorical_sample(logits: np.ndarray, temperature: float = 1.0, n_samples: int = 10000) -> np.ndarray:
    """Sample from categorical distribution via softmax.

    corresponds to: p(x=k) = exp(z_k/T) / Î£_j exp(z_j/T)
    """
    scaled = logits / temperature
    probs = np.exp(scaled - np.max(scaled))
    probs /= probs.sum()
    return np.random.choice(len(logits), size=n_samples, p=probs)

# Simulate LLM next-token prediction
vocab = ["the", "a", "cat", "dog", "sat", "on", "mat", "ran"]
logits = np.array([2.5, 1.0, 3.0, 0.5, 2.0, 1.5, 0.8, 0.3])

print("=== LLM Next-Token Sampling Simulation ===\n")
print(f"{'Token':<8} {'Logit':>6} | {'T=0.5':>8} {'T=1.0':>8} {'T=2.0':>8}")
print("-" * 50)

for T in [0.5, 1.0, 2.0]:
    samples = categorical_sample(logits, T)
    unique, counts = np.unique(samples, return_counts=True)
    freq = np.zeros(len(vocab))
    for u, c in zip(unique, counts):
        freq[u] = c / len(samples)
    if T == 0.5:
        all_freqs = [freq]
    else:
        all_freqs.append(freq)

for i, word in enumerate(vocab):
    row = f"{word:<8} {logits[i]:>6.1f} |"
    for freq in all_freqs:
        row += f" {freq[i]:>7.1%}"
    print(row)
```

| æ¸©åº¦ $T$ | åŠ¹æœ | LLMã§ã®ç”¨é€” |
|:---------|:-----|:-----------|
| $T \to 0$ | æœ€å¤§ç¢ºç‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿é¸æŠï¼ˆgreedyï¼‰ | ç¿»è¨³ãƒ»è¦ç´„ï¼ˆæ­£ç¢ºã•é‡è¦–ï¼‰ |
| $T = 1.0$ | ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’é€šã‚Šã®åˆ†å¸ƒ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ |
| $T > 1.0$ | åˆ†å¸ƒãŒå¹³å¦åŒ–ï¼ˆå¤šæ§˜æ€§UPï¼‰ | å‰µä½œãƒ»ãƒ–ãƒ¬ã‚¹ãƒˆ |

Hintonã‚‰ã®çŸ¥è­˜è’¸ç•™è«–æ–‡ [^3] ã§ä½“ç³»åŒ–ã•ã‚ŒãŸã“ã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ç¬¬1å›ã§ä½“é¨“ã—ãŸSoftmaxã®æ‹¡å¼µã ã€‚ã“ã“ã§é‡è¦ãªã®ã¯ã€**æ¸©åº¦ã‚’å¤‰ãˆã¦ã‚‚Categoricalåˆ†å¸ƒã§ã‚ã‚‹ã“ã¨è‡ªä½“ã¯å¤‰ã‚ã‚‰ãªã„**ã¨ã„ã†ç‚¹ã€‚åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\boldsymbol{\pi}$ ãŒå¤‰ã‚ã‚‹ã ã‘ã ã€‚

:::message
ã“ã“ã§å¤šãã®äººãŒæ··ä¹±ã™ã‚‹ã®ãŒã€Œlogitã¨ã¯ä½•ã‹ã€ã ã€‚logitã¯Softmaxé©ç”¨**å‰**ã®ç”Ÿã®ã‚¹ã‚³ã‚¢ã€‚ç¢ºç‡ã§ã¯ãªã„ã€‚Softmaxã‚’é€šã—ã¦åˆã‚ã¦Categoricalåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\pi_k$ ã«ãªã‚‹ã€‚
:::

### 1.2 é€£ç¶šåˆ†å¸ƒ â€” ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ä¸‡èƒ½æ€§

æ©Ÿæ¢°å­¦ç¿’ã§æœ€ã‚‚é »ç¹ã«ç¾ã‚Œã‚‹é€£ç¶šåˆ†å¸ƒ â€” ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆæ­£è¦åˆ†å¸ƒï¼‰ã€‚

$$
\mathcal{N}(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
$$

```python
import numpy as np

def gaussian_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:
    """Gaussian probability density function.

    corresponds to: N(x|Î¼,ÏƒÂ²) = (2Ï€ÏƒÂ²)^{-1/2} exp(-(x-Î¼)Â²/(2ÏƒÂ²))
    """
    return (1 / np.sqrt(2 * np.pi * sigma**2)) * np.exp(-((x - mu)**2) / (2 * sigma**2))

x = np.linspace(-6, 6, 1000)
configs = [(0, 1, "Î¼=0,Ïƒ=1 (æ¨™æº–æ­£è¦)"), (0, 0.5, "Î¼=0,Ïƒ=0.5 (é‹­ã„)"),
           (2, 1, "Î¼=2,Ïƒ=1 (å³ã‚·ãƒ•ãƒˆ)"), (0, 2, "Î¼=0,Ïƒ=2 (åºƒã„)")]

print(f"{'Config':<25} {'Peak':>8} {'P(|x|<1)':>10} {'P(|x|<2)':>10}")
print("-" * 55)
for mu, sigma, name in configs:
    pdf = gaussian_pdf(x, mu, sigma)
    peak = gaussian_pdf(np.array([mu]), mu, sigma)[0]
    dx = x[1] - x[0]
    p1 = np.sum(pdf[np.abs(x - mu) < 1]) * dx
    p2 = np.sum(pdf[np.abs(x - mu) < 2]) * dx
    print(f"{name:<25} {peak:>8.4f} {p1:>9.1%} {p2:>9.1%}")
```

å‡ºåŠ›:
```
Config                       Peak   P(|x|<1)   P(|x|<2)
-------------------------------------------------------
Î¼=0,Ïƒ=1 (æ¨™æº–æ­£è¦)         0.3989     68.3%     95.4%
Î¼=0,Ïƒ=0.5 (é‹­ã„)           0.7979     95.4%    100.0%
Î¼=2,Ïƒ=1 (å³ã‚·ãƒ•ãƒˆ)         0.3989     68.3%     95.4%
Î¼=0,Ïƒ=2 (åºƒã„)             0.1995     38.3%     68.3%
```

**68-95-99.7ãƒ«ãƒ¼ãƒ«**: æ¨™æº–æ­£è¦åˆ†å¸ƒã§ã¯ã€$\pm 1\sigma$ ã«68.3%ã€$\pm 2\sigma$ ã«95.4%ã€$\pm 3\sigma$ ã«99.7%ã®ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹ã€‚

ãªãœã‚¬ã‚¦ã‚¹åˆ†å¸ƒãŒã“ã‚Œã»ã©é‡è¦ã‹ï¼Ÿ ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCLTï¼‰ãŒãã®ç­”ãˆã  â€” **ç‹¬ç«‹ãªç¢ºç‡å¤‰æ•°ã®å’Œã¯ã€å…ƒã®åˆ†å¸ƒãŒä½•ã§ã‚ã‚Œã€æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã**ã€‚ã“ã‚Œã¯å¾Œã®Zone 3ã§å³å¯†ã«å°å‡ºã™ã‚‹ã€‚

:::details ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¨LLM â€” éš ã‚ŒãŸæ¥ç¶š
LLMã®å­¦ç¿’ã§ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–ã«ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‚’ä½¿ã†ã€‚Heã®åˆæœŸåŒ– $\mathcal{N}(0, 2/n)$ ã‚„Xavierã®åˆæœŸåŒ– $\mathcal{N}(0, 2/(n_{in}+n_{out}))$ ã¯ã€å‹¾é…ã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«åˆ†æ•£ã‚’ç²¾å¯†ã«è¨­å®šã—ã¦ã„ã‚‹ã€‚

VAE [^2] ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€æ½œåœ¨å¤‰æ•°ã®åˆ†å¸ƒã‚’ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$ ã§è¿‘ä¼¼ã™ã‚‹ã€‚ã€Œãªãœã‚¬ã‚¦ã‚¹ãªã®ã‹ã€ã¯ã€æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®æ€§è³ªã¨è¨ˆç®—ã®éƒ½åˆã«ã‚ˆã‚‹ â€” ã“ã‚Œã‚‚æœ¬è¬›ç¾©ã§æ‰±ã†ã€‚

æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« [^4] ã®å‰æ–¹éç¨‹ã¯ã€ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‚’æ®µéšçš„ã«åŠ ãˆã¦ã„ãæ“ä½œã ã€‚$q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I})$ â€” ã“ã“ã§ã‚‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãŒä¸»å½¹ã‚’æ¼”ã˜ã‚‹ã€‚
:::

### 1.3 ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒã‹ã‚‰Multinomialã¸ â€” é›¢æ•£åˆ†å¸ƒã®ç³»è­œ

ç¢ºç‡åˆ†å¸ƒã¯å­¤ç«‹ã—ã¦å­˜åœ¨ã™ã‚‹ã®ã§ã¯ãªãã€**æ—ï¼ˆãƒ•ã‚¡ãƒŸãƒªãƒ¼ï¼‰**ã‚’å½¢æˆã™ã‚‹ã€‚

```mermaid
graph TD
    B["Bernoulli<br/>p(x=1)=p<br/>ã‚³ã‚¤ãƒ³1å›"]
    Bin["Binomial<br/>B(n,p)<br/>ã‚³ã‚¤ãƒ³nå›"]
    Cat["Categorical<br/>Cat(Ï€â‚,...,Ï€K)<br/>ã‚µã‚¤ã‚³ãƒ­1å›"]
    Multi["Multinomial<br/>Multi(n,Ï€)<br/>ã‚µã‚¤ã‚³ãƒ­nå›"]
    Poi["Poisson<br/>Poi(Î»)<br/>ç¨€ãªäº‹è±¡ã®ã‚«ã‚¦ãƒ³ãƒˆ"]

    B -->|"nå›ç¹°ã‚Šè¿”ã—"| Bin
    B -->|"Ké¢ã«æ‹¡å¼µ"| Cat
    Cat -->|"nå›ç¹°ã‚Šè¿”ã—"| Multi
    Bin -->|"nâ†’âˆ,pâ†’0<br/>np=Î»"| Poi

    style B fill:#e3f2fd
    style Cat fill:#fff3e0
```

```python
import numpy as np

# Bernoulli: coin flip
p_heads = 0.7
flips = np.random.binomial(1, p_heads, size=10000)
print(f"Bernoulli(p={p_heads}): mean={flips.mean():.3f} (theory={p_heads})")

# Binomial: n coin flips
n, p = 20, 0.3
binom_samples = np.random.binomial(n, p, size=10000)
print(f"Binomial(n={n},p={p}): mean={binom_samples.mean():.2f} (theory={n*p}), var={binom_samples.var():.2f} (theory={n*p*(1-p):.2f})")

# Categorical: dice roll (LLM token selection)
probs = np.array([0.1, 0.3, 0.05, 0.4, 0.15])
labels = ["A", "B", "C", "D", "E"]
cat_samples = np.random.choice(len(probs), size=10000, p=probs)
print(f"\nCategorical samples (10000 draws):")
for i, label in enumerate(labels):
    freq = (cat_samples == i).mean()
    print(f"  {label}: freq={freq:.3f} (theory={probs[i]:.3f})")

# Poisson: rare event counting
lam = 3.0
pois_samples = np.random.poisson(lam, size=10000)
print(f"\nPoisson(Î»={lam}): mean={pois_samples.mean():.2f} (theory={lam}), var={pois_samples.var():.2f} (theory={lam})")
```

| åˆ†å¸ƒ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å¹³å‡ | åˆ†æ•£ | MLå¿œç”¨ |
|:-----|:----------|:-----|:-----|:-------|
| Bernoulli($p$) | $p \in [0,1]$ | $p$ | $p(1-p)$ | äºŒå€¤åˆ†é¡ |
| Binomial($n, p$) | $n \in \mathbb{N}, p \in [0,1]$ | $np$ | $np(1-p)$ | ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ |
| Categorical($\boldsymbol{\pi}$) | $\pi_k \geq 0, \sum \pi_k = 1$ | â€” | â€” | **LLMæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠ** |
| Poisson($\lambda$) | $\lambda > 0$ | $\lambda$ | $\lambda$ | ç¨€ãªäº‹è±¡ |
| Gaussian($\mu, \sigma^2$) | $\mu \in \mathbb{R}, \sigma^2 > 0$ | $\mu$ | $\sigma^2$ | **VAEæ½œåœ¨ç©ºé–“** |

### 1.4 Dirichletåˆ†å¸ƒ â€” Categoricalåˆ†å¸ƒã®ãƒ™ã‚¤ã‚ºäº‹å‰åˆ†å¸ƒ

LLMã®å‡ºåŠ›ãŒCategoricalåˆ†å¸ƒãªã‚‰ã€ãã®ãƒ™ã‚¤ã‚ºäº‹å‰åˆ†å¸ƒã¯ä½•ã‹ï¼Ÿ ç­”ãˆã¯**Dirichletåˆ†å¸ƒ**ã ã€‚

$$
\text{Dir}(\boldsymbol{\pi} \mid \boldsymbol{\alpha}) = \frac{\Gamma(\sum_k \alpha_k)}{\prod_k \Gamma(\alpha_k)} \prod_{k=1}^{K} \pi_k^{\alpha_k - 1}
$$

Dirichletåˆ†å¸ƒã¯ã€Œç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ« $\boldsymbol{\pi}$ ã®ä¸Šã®ç¢ºç‡åˆ†å¸ƒã€ã ã€‚$\boldsymbol{\alpha}$ ãŒé›†ä¸­åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€$\alpha_k$ ãŒå¤§ãã„ã»ã© $\pi_k$ ã«è³ªé‡ãŒé›†ä¸­ã™ã‚‹ã€‚

```python
import numpy as np

def dirichlet_samples(alpha: np.ndarray, n_samples: int = 5) -> np.ndarray:
    """Sample probability vectors from Dirichlet(Î±).

    corresponds to: Ï€ ~ Dir(Î±), Î£Ï€_k = 1, Ï€_k â‰¥ 0
    """
    samples = np.random.dirichlet(alpha, size=n_samples)
    return samples

# Explore Dirichlet with different concentration parameters
K = 5  # 5-class categorical (like a tiny vocabulary)
print("=== Dirichlet Distribution â€” Prior for Categorical ===\n")

configs = [
    (np.ones(K) * 0.1, "Î±=0.1 (sparse â€” few classes dominate)"),
    (np.ones(K) * 1.0, "Î±=1.0 (uniform â€” any Ï€ equally likely)"),
    (np.ones(K) * 10.0, "Î±=10 (concentrated â€” all Ï€_k â‰ˆ 1/K)"),
    (np.array([10, 1, 1, 1, 1]), "Î±=[10,1,1,1,1] (class 0 dominant)"),
]

for alpha, name in configs:
    samples = dirichlet_samples(alpha, n_samples=3)
    print(f"{name}")
    for s in samples:
        print(f"  Ï€ = [{', '.join(f'{p:.3f}' for p in s)}]")
    mean = alpha / alpha.sum()
    print(f"  E[Ï€] = [{', '.join(f'{m:.3f}' for m in mean)}]")
    entropy = -np.mean([np.sum(s * np.log(s + 1e-10)) for s in dirichlet_samples(alpha, 10000)])
    print(f"  E[H(Ï€)] = {entropy:.3f} (average entropy of sampled distributions)")
    print()
```

| $\alpha$ | ç›´æ„Ÿ | LLMæ–‡è„ˆ |
|:---------|:-----|:--------|
| $\alpha_k \ll 1$ | ã‚¹ãƒ‘ãƒ¼ã‚¹ â€” å°‘æ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«é›†ä¸­ | æ±ºå®šçš„ãªå‡ºåŠ› |
| $\alpha_k = 1$ | ä¸€æ§˜ â€” ã‚ã‚‰ã‚†ã‚‹åˆ†å¸ƒãŒç­‰ç¢ºç‡ | ç„¡æƒ…å ±äº‹å‰åˆ†å¸ƒ |
| $\alpha_k \gg 1$ | é›†ä¸­ â€” ä¸€æ§˜åˆ†å¸ƒã«è¿‘ã„ | ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„å‡ºåŠ› |

**ãƒ™ã‚¤ã‚ºæ›´æ–°**: Categoricalå°¤åº¦ + Dirichletäº‹å‰ â†’ Dirichletäº‹å¾Œï¼ˆå…±å½¹ãƒšã‚¢ï¼‰ã€‚

$$
\boldsymbol{\pi} \sim \text{Dir}(\boldsymbol{\alpha}) \quad \to \quad \boldsymbol{\pi} \mid \text{data} \sim \text{Dir}(\boldsymbol{\alpha} + \text{counts})
$$

ã“ã‚Œã¯LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹ã€Œå‡ºåŠ›åˆ†å¸ƒã®äº‹å‰çŸ¥è­˜ã€ã®æ•°å­¦çš„è¡¨ç¾ã ã€‚

### 1.5 åŒæ™‚åˆ†å¸ƒãƒ»å‘¨è¾ºåˆ†å¸ƒãƒ»æ¡ä»¶ä»˜ãåˆ†å¸ƒ

ç¢ºç‡ã®3ã¤ã®é¡”ã‚’ã€ã‚³ãƒ¼ãƒ‰ã§ä½“æ„Ÿã™ã‚‹ã€‚

```python
import numpy as np

# Joint distribution P(X, Y) as a 2D table
# X = weather (0=sunny, 1=rainy), Y = umbrella (0=no, 1=yes)
joint = np.array([
    [0.40, 0.10],  # sunny: no umbrella, umbrella
    [0.05, 0.45],  # rainy: no umbrella, umbrella
])

print("=== Joint Distribution P(X, Y) ===")
print(f"           No Umbrella  Umbrella")
print(f"Sunny:     {joint[0,0]:.2f}         {joint[0,1]:.2f}")
print(f"Rainy:     {joint[1,0]:.2f}         {joint[1,1]:.2f}")

# Marginal: P(X) = Î£_y P(X, y)
p_x = joint.sum(axis=1)
print(f"\n=== Marginal P(X) ===")
print(f"P(Sunny) = {p_x[0]:.2f}, P(Rainy) = {p_x[1]:.2f}")

# Marginal: P(Y) = Î£_x P(x, Y)
p_y = joint.sum(axis=0)
print(f"\n=== Marginal P(Y) ===")
print(f"P(No Umbrella) = {p_y[0]:.2f}, P(Umbrella) = {p_y[1]:.2f}")

# Conditional: P(Y|X) = P(X,Y) / P(X)
p_y_given_x = joint / p_x[:, np.newaxis]
print(f"\n=== Conditional P(Y|X) ===")
print(f"P(No Umbrella | Sunny) = {p_y_given_x[0,0]:.3f}")
print(f"P(Umbrella | Sunny)    = {p_y_given_x[0,1]:.3f}")
print(f"P(No Umbrella | Rainy) = {p_y_given_x[1,0]:.3f}")
print(f"P(Umbrella | Rainy)    = {p_y_given_x[1,1]:.3f}")

# Verify: each row sums to 1
print(f"\nRow sums (must be 1.0): {p_y_given_x.sum(axis=1)}")
```

| æ¦‚å¿µ | æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | ç›´æ„Ÿ |
|:-----|:-----|:-------|:-----|
| åŒæ™‚åˆ†å¸ƒ | $P(X, Y)$ | `joint` | 2ã¤ã®å¤‰æ•°ã®å…¨çµ„ã¿åˆã‚ã› |
| å‘¨è¾ºåˆ†å¸ƒ | $P(X) = \sum_y P(X, y)$ | `joint.sum(axis=1)` | ç‰‡æ–¹ã‚’è¶³ã—ä¸Šã’ã‚‹ |
| æ¡ä»¶ä»˜ãåˆ†å¸ƒ | $P(Y \mid X) = \frac{P(X,Y)}{P(X)}$ | `joint / p_x[:, None]` | çŸ¥ã£ã¦ã„ã‚‹æƒ…å ±ã§çµã‚‹ |

**LLMã¨ã®æ¥ç¶š**: LLMã®è‡ªå·±å›å¸°ç”Ÿæˆã¯æ¡ä»¶ä»˜ãåˆ†å¸ƒã®é€£é–ã ã€‚

$$
p(\mathbf{x}) = p(x_1) \cdot p(x_2 \mid x_1) \cdot p(x_3 \mid x_1, x_2) \cdots = \prod_{t=1}^{T} p(x_t \mid x_{<t})
$$

ã€ŒThe cat sat on theã€ã®æ¬¡ã«æ¥ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡ $p(x_7 \mid x_1, \ldots, x_6)$ â€” ã“ã‚Œã¯æ¡ä»¶ä»˜ãåˆ†å¸ƒä»¥å¤–ã®ä½•ç‰©ã§ã‚‚ãªã„ã€‚Malach (2023) [^5] ã¯ã€ã“ã®ã‚ˆã†ãªè‡ªå·±å›å¸°æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬å™¨ãŒä»»æ„ã®ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒã‚·ãƒ³è¨ˆç®—å¯èƒ½ãªé–¢æ•°ã‚’è¿‘ä¼¼ã§ãã‚‹ã€Œæ™®éå­¦ç¿’å™¨ã€ã§ã‚ã‚‹ã“ã¨ã‚’ç†è«–çš„ã«ç¤ºã—ãŸã€‚

> **Zone 1 ã¾ã¨ã‚**: Categoricalåˆ†å¸ƒï¼ˆLLMã®Softmaxå‡ºåŠ›ï¼‰â†’ Gaussianåˆ†å¸ƒï¼ˆVAEã®æ½œåœ¨ç©ºé–“ï¼‰â†’ æ¡ä»¶ä»˜ãåˆ†å¸ƒï¼ˆè‡ªå·±å›å¸°ç”Ÿæˆ $\prod_t p(x_t \mid x_{<t})$ï¼‰ã€‚ç¢ºç‡åˆ†å¸ƒã¯ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ã€Œè¨€èªã€ã ã€‚

:::message
**é€²æ—: 10% å®Œäº†** é›¢æ•£åˆ†å¸ƒãƒ»é€£ç¶šåˆ†å¸ƒãƒ»åŒæ™‚/å‘¨è¾º/æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’ã€Œè§¦ã£ã¦ã€ç†è§£ã—ãŸã€‚Zone 0-1 ã‚¯ãƒªã‚¢ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœç¢ºç‡è«–ãŒAIã®å¿ƒè‡“ãªã®ã‹

### 2.1 ç¢ºç‡ã‚’å­¦ã¶ã€Œæœ¬å½“ã®ç†ç”±ã€

å¤šãã®MLå…¥é–€ã¯ã€Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å©ã‘ã°å‹•ãã€ã‹ã‚‰å§‹ã¾ã‚‹ã€‚ã ãŒè«–æ–‡ã‚’èª­ã‚‚ã†ã¨ã—ãŸç¬é–“ã€ç¢ºç‡ã®å£ã«ã¶ã¤ã‹ã‚‹ã€‚

- VAEã®æå¤±é–¢æ•° $\mathcal{L} = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{KL}[q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})]$ â€” æœŸå¾…å€¤ã€æ¡ä»¶ä»˜ãåˆ†å¸ƒã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®å¡Š
- Diffusionã®å‰æ–¹éç¨‹ $q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I})$ â€” æ¡ä»¶ä»˜ãã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ãƒãƒ«ã‚³ãƒ•é€£é–
- GANã®ç›®çš„é–¢æ•° $\min_G \max_D \mathbb{E}_{\mathbf{x} \sim p_\text{data}}[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})}[\log(1 - D(G(\mathbf{z})))]$ â€” æœŸå¾…å€¤ã¨ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**ç¢ºç‡è«–ãªã—ã«ã€ã“ã‚Œã‚‰ã®å¼ã¯1æ–‡å­—ã‚‚èª­ã‚ãªã„ã€‚**

> ã“ã®ç« ã‚’èª­ã‚ã°: ç¢ºç‡è«–ãŒæ®‹ã‚Š36å›ã®å…¨è¬›ç¾©ã§ã©ã†ä½¿ã‚ã‚Œã‚‹ã‹ã€å…¨ä½“åƒãŒè¦‹ãˆã‚‹ã€‚

### 2.2 Course Iï¼ˆæ•°å­¦åŸºç¤ç·¨ï¼‰ã§ã®ä½ç½®ã¥ã‘

```mermaid
graph TD
    L1["ç¬¬1å›: æ¦‚è«–<br/>æ•°å¼ã®èª­ã¿æ–¹"]
    L2["ç¬¬2å›: ç·šå½¢ä»£æ•° I<br/>ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—"]
    L3["ç¬¬3å›: ç·šå½¢ä»£æ•° II<br/>SVDãƒ»è¡Œåˆ—å¾®åˆ†"]
    L4["ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦<br/>åˆ†å¸ƒãƒ»ãƒ™ã‚¤ã‚ºãƒ»MLE"]
    L5["ç¬¬5å›: æ¸¬åº¦è«–ãƒ»ç¢ºç‡éç¨‹<br/>å³å¯†ãªç¢ºç‡"]
    L6["ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–<br/>KLãƒ»SGD"]
    L7["ç¬¬7å›: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«æ¦‚è¦<br/>MLE"]
    L8["ç¬¬8å›: æ½œåœ¨å¤‰æ•° & EM<br/>éš ã‚Œå¤‰æ•°"]

    L1 --> L2 --> L3 --> L4
    L4 -->|"ç¢ºç‡åˆ†å¸ƒãŒã‚ã‹ã£ãŸ<br/>â†’å³å¯†ã«å®šç¾©ã—ãŸã„"| L5
    L5 --> L6 --> L7 --> L8

    style L4 fill:#ffeb3b
```

ç¬¬4å›ã¯**Course Iã®æŠ˜ã‚Šè¿”ã—åœ°ç‚¹**ã ã€‚ç¬¬1-3å›ã§é›ãˆãŸç·šå½¢ä»£æ•°ã®ä¸Šã«ã€ä¸ç¢ºå®Ÿæ€§ã®æ•°å­¦ã‚’ç©ã‚€ã€‚ãã—ã¦ç¬¬5å›ï¼ˆæ¸¬åº¦è«–ï¼‰ã§ç¢ºç‡è«–ã‚’å³å¯†ã«å†å®šç¾©ã—ã€ç¬¬6å›ï¼ˆæƒ…å ±ç†è«–ï¼‰ã§KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã„ã†ã€Œåˆ†å¸ƒé–“ã®è·é›¢ã€ã‚’æ‰‹ã«å…¥ã‚Œã‚‹ã€‚

| å› | ãƒ†ãƒ¼ãƒ | ç¬¬4å›ã¨ã®æ¥ç¶š |
|:---|:------|:-------------|
| ç¬¬2å› | ç·šå½¢ä»£æ•° I | å…±åˆ†æ•£è¡Œåˆ—ã¯**å¯¾ç§°æ­£å®šå€¤è¡Œåˆ—** â€” å›ºæœ‰å€¤åˆ†è§£ã®å¯¾è±¡ |
| ç¬¬3å› | ç·šå½¢ä»£æ•° II | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒ**ç¢ºç‡å¤‰æ•°ã®å¤‰æ›**ã«å¿…è¦ï¼ˆç¬¬25å› NFï¼‰ |
| **ç¬¬4å›** | **ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦** | **æœ¬è¬›ç¾©** |
| ç¬¬5å› | æ¸¬åº¦è«– | ç¢ºç‡å¯†åº¦é–¢æ•°ã®**å³å¯†ãªå®šç¾©**ï¼ˆRadon-Nikodymï¼‰ |
| ç¬¬6å› | æƒ…å ±ç†è«– | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ = **åˆ†å¸ƒé–“ã®éå¯¾ç§°è·é›¢** |
| ç¬¬8å› | EMç®—æ³• | **æ½œåœ¨å¤‰æ•°ã®å‘¨è¾ºåŒ–** = æœ¬è¬›ç¾©ã®ç›´æ¥çš„æ‹¡å¼µ |

### 2.3 æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ–

| è¦³ç‚¹ | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:------------|:----------|
| ç¢ºç‡è«–ã®æ‰±ã„ | ã€Œå‰æçŸ¥è­˜ã€ã¨ã—ã¦çœç•¥ | **8æ™‚é–“ã‹ã‘ã¦å®Œå…¨ç¿’å¾—** |
| ãƒ™ã‚¤ã‚ºæ¨è«– | æ•°å¼ã®çµæœã ã‘ | **äº‹å‰â†’å°¤åº¦â†’äº‹å¾Œã®å…¨ã‚¹ãƒ†ãƒƒãƒ—å°å‡º** |
| MLE | å®šç¾©ã®ã¿ | **æ­£å‰‡æ¡ä»¶ãƒ»æ¼¸è¿‘æ­£è¦æ€§ãƒ»Fisheræƒ…å ±é‡ã¾ã§** |
| æŒ‡æ•°å‹åˆ†å¸ƒæ— | è¨€åŠãªã— | **çµ±ä¸€çš„ç†è§£ â†’ VAE/EBMã®ç†è«–åŸºç›¤** |
| å®Ÿè£… | ãªã— | **NumPyã§å…¨åˆ†å¸ƒã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…** |

ã€Œå‰æçŸ¥è­˜ã§ç‰‡ä»˜ã‘ã‚‹ã€ã¨ã¯ã€ã¤ã¾ã‚Šã€Œã‚ã‹ã‚‰ãªãã¦ã‚‚å…ˆã«é€²ã‚ã€ã¨ã„ã†ã“ã¨ã ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ãã‚Œã‚’è¨±ã•ãªã„ã€‚ç¢ºç‡è«–ã®åœŸå°ãŒè„†ã„ã¨ã€ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ä»¥é™ã§å¿…ãšå´©å£Šã™ã‚‹ã€‚

### 2.4 3ã¤ã®å­¦ç¿’ãƒ¡ã‚¿ãƒ•ã‚¡ â€” ç¢ºç‡è«–ã‚’ã€Œä½“ã§è¦šãˆã‚‹ã€

ç¢ºç‡è«–ã¯æŠ½è±¡åº¦ãŒé«˜ã„ã€‚3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ã§ç›´æ„Ÿã‚’æ´ã‚‚ã†ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡1: ç¢ºç‡ç©ºé–“ = RPGã®ä¸–ç•Œè¨­å®š**

$(\Omega, \mathcal{F}, P)$ ã¯ã‚²ãƒ¼ãƒ ã®ä¸–ç•Œè¨­å®šã ã€‚$\Omega$ ã¯å…¨ã¦ã®å¯èƒ½ãªã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒãƒƒãƒ—ï¼‰ã€$\mathcal{F}$ ã¯ã€Œè¦³æ¸¬å¯èƒ½ãªã‚¤ãƒ™ãƒ³ãƒˆã®é›†åˆã€ï¼ˆã‚¯ã‚¨ã‚¹ãƒˆä¸€è¦§ï¼‰ã€$P$ ã¯å„ã‚¤ãƒ™ãƒ³ãƒˆã®ç™ºç”Ÿç¢ºç‡ï¼ˆãƒ¬ã‚¢ãƒ‰ãƒ­ãƒƒãƒ—ç‡ï¼‰ã€‚ç¢ºç‡å¤‰æ•° $X$ ã¯ã€Œãƒ€ãƒ³ã‚¸ãƒ§ãƒ³ã®å ±é…¬ã€â€” åŒã˜ãƒ€ãƒ³ã‚¸ãƒ§ãƒ³ã§ã‚‚æ¯å›é•ã†å ±é…¬ï¼ˆç¢ºç‡çš„ï¼‰ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡2: ãƒ™ã‚¤ã‚ºæ›´æ–° = æ¢åµã®æ¨ç†**

æ®ºäººäº‹ä»¶ã€‚å®¹ç–‘è€…ãŒ3äººã€‚æœ€åˆã¯å…¨å“¡ç­‰ã—ãç–‘ã‚ã—ã„ï¼ˆäº‹å‰åˆ†å¸ƒ: ä¸€æ§˜ï¼‰ã€‚å‡¶å™¨ã«Aã®æŒ‡ç´‹ãŒè¦‹ã¤ã‹ã£ãŸï¼ˆãƒ‡ãƒ¼ã‚¿: å°¤åº¦ãŒé«˜ã„ï¼‰ã€‚Aã®äº‹å¾Œç¢ºç‡ãŒä¸ŠãŒã‚‹ã€‚ã ãŒAã«ã¯é‰„å£ã®ã‚¢ãƒªãƒã‚¤ãŒå‡ºã¦ããŸï¼ˆæ–°ãŸãªãƒ‡ãƒ¼ã‚¿ï¼‰ã€‚äº‹å¾Œåˆ†å¸ƒãŒã¾ãŸæ›´æ–°ã•ã‚Œã‚‹ã€‚**ãƒ™ã‚¤ã‚ºæ¨è«– = è¨¼æ‹ ã‚’ç©ã¿é‡ã­ã¦ä¿¡å¿µã‚’æ›´æ–°ã™ã‚‹éç¨‹**ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡3: MLE = çŠ¯äººå½“ã¦ã‚²ãƒ¼ãƒ ã®æœ€é©æˆ¦ç•¥**

ç®±ã®ä¸­ã«ã‚³ã‚¤ãƒ³ãŒã‚ã‚‹ã€‚è¡¨ã®å‡ºã‚‹ç¢ºç‡ $p$ ã¯ä¸æ˜ã€‚10å›æŠ•ã’ã¦8å›è¡¨ãŒå‡ºãŸã€‚$p$ ã¯ã„ãã¤ã ã¨ã€Œæœ€ã‚‚ã‚ã‚Šãã†ã€ã‹ï¼Ÿ $p = 0.8$ ãŒå°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ â€” ã“ã‚ŒãŒMLEã€‚ã€Œãƒ‡ãƒ¼ã‚¿ãŒæœ€ã‚‚èµ·ã“ã‚Šã‚„ã™ã‹ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ã‚’é¸ã¶ã€‚

### 2.5 LLMã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚° â€” æ¡ä»¶ä»˜ãç¢ºç‡ã¨è‡ªå·±å›å¸°

ç¬¬4å›ã®LLMæ¥ç¶šã¯**æ¡ä»¶ä»˜ãç¢ºç‡**ã¨**è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«** $p(x_t \mid x_{<t})$ ã ã€‚

LLMã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¯ã€ä»¥ä¸‹ã®ç¢ºç‡ã®é€£é–è¦å‰‡ï¼ˆchain ruleï¼‰ã«æ”¯é…ã•ã‚Œã‚‹:

$$
p(x_1, x_2, \ldots, x_T) = \prod_{t=1}^{T} p(x_t \mid x_1, \ldots, x_{t-1})
$$

ã“ã‚Œã¯ç¢ºç‡ã®å…¬ç†ã‹ã‚‰å°ã‹ã‚Œã‚‹æ’ç­‰å¼ã ã€‚ä½•ã®ä»®å®šã‚‚è¿‘ä¼¼ã‚‚å…¥ã£ã¦ã„ãªã„ã€‚LLMã¯ã“ã®å„æ¡ä»¶ä»˜ãç¢ºç‡ $p(x_t \mid x_{<t})$ ã‚’Transformerã§è¿‘ä¼¼ã—ã€Softmaxå‡ºåŠ›ã§Categoricalåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ã„ã‚‹ã€‚

```mermaid
graph LR
    X1["xâ‚='The'"] --> P2["p(xâ‚‚|xâ‚)"]
    P2 --> X2["xâ‚‚='cat'"]
    X2 --> P3["p(xâ‚ƒ|xâ‚,xâ‚‚)"]
    P3 --> X3["xâ‚ƒ='sat'"]
    X3 --> P4["p(xâ‚„|xâ‚,...,xâ‚ƒ)"]
    P4 --> X4["xâ‚„='on'"]

    style P2 fill:#fff3e0
    style P3 fill:#fff3e0
    style P4 fill:#fff3e0
```

**è¦šãˆã¦ãŠã„ã¦ã»ã—ã„**: ã“ã®è‡ªå·±å›å¸°æ§‹é€ ã¯ç¬¬15å›ï¼ˆè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼‰ã§æœ¬æ ¼çš„ã«æ‰±ã†ã€‚ä»Šã¯ã€ŒLLMã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ = æ¡ä»¶ä»˜ãç¢ºç‡ã®é€£é–ã€ã¨ã„ã†æ¥ç¶šã‚’æŠŠæ¡ã—ã¦ãŠã‘ã°ååˆ†ã ã€‚

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬ â€” Pythonã®çµ‚ã‚ã‚Šã®å§‹ã¾ã‚Š
æœ¬è¬›ç¾©ã¯Python 100%ã ã€‚NumPyã§å…¨ã¦ã®åˆ†å¸ƒã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã™ã‚‹ã€‚ã€ŒPythonã¯ä¾¿åˆ©ã ã€ã¨å®‰å¿ƒã—ã¦ã»ã—ã„ã€‚

......ãŸã ã—ã€ç¬¬5å›ã‹ã‚‰ `%timeit` ãŒç™»å ´ã™ã‚‹ã€‚Monte Carloç©åˆ†ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æ¸¬ã‚Šå§‹ã‚ã‚‹ã¨ã€Pythonã®ã€Œé…ã•ã€ãŒå°‘ã—ãšã¤è¦‹ãˆã¦ãã‚‹ã€‚ç¬¬9å›ã§JuliaãŒåˆç™»å ´ã—ãŸã¨ãã€ELBOè¨ˆç®—ãŒ50å€é€Ÿããªã‚‹è¡æ’ƒãŒå¾…ã£ã¦ã„ã‚‹ã€‚

ä»Šã¯Pythonã‚’ä¿¡ã˜ã¦ã€ç¢ºç‡è«–ã«é›†ä¸­ã—ã‚ˆã†ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** ç¢ºç‡è«–ãŒã‚·ãƒªãƒ¼ã‚ºå…¨ä½“ã§ã©ã†ä½¿ã‚ã‚Œã‚‹ã‹ã€å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚Zone 2 ã‚¯ãƒªã‚¢ã€‚ã„ã‚ˆã„ã‚ˆæ•°å¼ä¿®è¡Œã®æœ¬ç•ªã«å…¥ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” ç¢ºç‡è«–ã®å…¨æ­¦è£…

ã“ã“ã‹ã‚‰ãŒæœ¬ç•ªã ã€‚ç¢ºç‡ç©ºé–“ã®å³å¯†ãªå®šç¾©ã‹ã‚‰ã€ãƒ™ã‚¤ã‚ºã®å®šç†ã€æŒ‡æ•°å‹åˆ†å¸ƒæ—ã€MLEã€Fisheræƒ…å ±é‡ã¾ã§ â€” ç¢ºç‡è«–ã®æ­¦å™¨åº«ã‚’ä¸€æ°—ã«åŸ‹ã‚ã‚‹ã€‚

```mermaid
graph TD
    A["3.1 ç¢ºç‡ç©ºé–“ (Î©,F,P)"] --> B["3.2 ç¢ºç‡å¤‰æ•°ã¨æœŸå¾…å€¤"]
    B --> C["3.3 ãƒ™ã‚¤ã‚ºã®å®šç†"]
    C --> D["3.4 ä¸»è¦ãªç¢ºç‡åˆ†å¸ƒ"]
    D --> E["3.5 å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ"]
    E --> F["3.6 æŒ‡æ•°å‹åˆ†å¸ƒæ—"]
    F --> G["3.7 MLEå®Œå…¨ç‰ˆ"]
    G --> H["3.8 Fisheræƒ…å ±é‡"]
    H --> I["3.9 å¤§æ•°ã®æ³•å‰‡ã¨CLT"]
    I --> J["âš”ï¸ Boss Battle"]

    style J fill:#ff5252,color:#fff
```

### 3.1 ç¢ºç‡ç©ºé–“ã®å®šç¾© â€” (Î©, F, P)

å…¨ã¦ã®ç¢ºç‡ã®è­°è«–ã¯ã€3ã¤çµ„ $(\Omega, \mathcal{F}, P)$ ã‹ã‚‰å§‹ã¾ã‚‹ã€‚Kolmogorov [^6] ãŒ1933å¹´ã«ç¢ºç«‹ã—ãŸã“ã®å…¬ç†ç³»ãŒã€ç¢ºç‡è«–ã®åœŸå°ã ã€‚

**å®šç¾©ï¼ˆç¢ºç‡ç©ºé–“ï¼‰**: ç¢ºç‡ç©ºé–“ã¨ã¯ä¸‰ã¤çµ„ $(\Omega, \mathcal{F}, P)$ ã§ã‚ã‚Š:

1. **æ¨™æœ¬ç©ºé–“** $\Omega$ â€” èµ·ã“ã‚Šã†ã‚‹å…¨ã¦ã®çµæœã®é›†åˆ
2. **äº‹è±¡ã®Ïƒ-åŠ æ³•æ—** $\mathcal{F}$ â€” $\Omega$ ã®éƒ¨åˆ†é›†åˆã®æ—ã§ã€ä»¥ä¸‹ã‚’æº€ãŸã™:
   - $\Omega \in \mathcal{F}$
   - $A \in \mathcal{F} \Rightarrow A^c \in \mathcal{F}$ï¼ˆè£œé›†åˆã§é–‰ã˜ã‚‹ï¼‰
   - $A_1, A_2, \ldots \in \mathcal{F} \Rightarrow \bigcup_{n=1}^{\infty} A_n \in \mathcal{F}$ï¼ˆå¯ç®—åˆä½µã§é–‰ã˜ã‚‹ï¼‰
3. **ç¢ºç‡æ¸¬åº¦** $P: \mathcal{F} \to [0,1]$ â€” ä»¥ä¸‹ã‚’æº€ãŸã™é–¢æ•°:
   - $P(\Omega) = 1$ï¼ˆæ­£è¦åŒ–ï¼‰
   - äº’ã„ã«ç´ ãª $A_1, A_2, \ldots \in \mathcal{F}$ ã«å¯¾ã—ã¦ $P\left(\bigcup_{n=1}^{\infty} A_n\right) = \sum_{n=1}^{\infty} P(A_n)$ï¼ˆÏƒ-åŠ æ³•æ€§ï¼‰

:::message
ã“ã“ã§å¤šãã®äººãŒæ··ä¹±ã™ã‚‹ã®ãŒã€ŒãªãœÏƒ-åŠ æ³•æ—ãŒå¿…è¦ãªã®ã‹ã€ã ã€‚Î© ã®ã‚ã‚‰ã‚†ã‚‹éƒ¨åˆ†é›†åˆã«ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚ˆã†ã¨ã™ã‚‹ã¨ã€æ•°å­¦çš„ã«çŸ›ç›¾ãŒç”Ÿã˜ã‚‹ï¼ˆVitaliã®éå¯æ¸¬é›†åˆï¼‰ã€‚Ïƒ-åŠ æ³•æ—ã¯ã€Œç¢ºç‡ã‚’å®šç¾©ã§ãã‚‹éƒ¨åˆ†é›†åˆã€ã‚’åˆ¶é™ã™ã‚‹ â€” ç¬¬5å›ï¼ˆæ¸¬åº¦è«–ï¼‰ã§è©³ã—ãæ‰±ã†ã€‚ä»Šã¯ã€Œç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‹å¯¾è±¡ã‚’åˆ¶é™ã™ã‚‹ä»•çµ„ã¿ã€ã¨ç†è§£ã™ã‚Œã°ååˆ†ã ã€‚
:::

```python
import numpy as np

# Example: probability space for a fair die
# Î© = {1, 2, 3, 4, 5, 6}
omega = {1, 2, 3, 4, 5, 6}

# F = power set of Î© (all subsets) â€” 2^6 = 64 events
# P: uniform probability
def P(event: set) -> float:
    """Probability measure for a fair die.

    corresponds to: P(A) = |A| / |Î©| for uniform distribution
    """
    return len(event) / len(omega)

# Verify axioms
print("=== Kolmogorov Axioms Verification ===")
print(f"1. P(Î©) = {P(omega)} (must be 1)")
print(f"2. P(âˆ…) = {P(set())} (must be 0)")

A = {1, 3, 5}  # odd numbers
B = {2, 4, 6}  # even numbers
print(f"3. P(odd) = {P(A):.4f}")
print(f"   P(even) = {P(B):.4f}")
print(f"   P(odd âˆª even) = {P(A | B):.4f}")
print(f"   P(odd) + P(even) = {P(A) + P(B):.4f} (Ïƒ-additivity)")

# Non-trivial example: conditional probability
C = {1, 2, 3}  # â‰¤ 3
D = {2, 4, 6}  # even
# P(D|C) = P(Dâˆ©C) / P(C)
p_d_given_c = P(D & C) / P(C)
print(f"\nP(even | â‰¤3) = P({{2}}) / P({{1,2,3}}) = {P(D & C):.4f} / {P(C):.4f} = {p_d_given_c:.4f}")
```

**Ïƒ-åŠ æ³•æ—ãŒã€Œãªãœã€å¿…è¦ã‹ã®ç›´æ„Ÿ**: ã‚µã‚¤ã‚³ãƒ­ã®6é¢ãªã‚‰ã€å…¨éƒ¨åˆ†é›†åˆï¼ˆå†ªé›†åˆï¼‰ã«ç¢ºç‡ã‚’å®šç¾©ã§ãã‚‹ã€‚ã ãŒ $\Omega = \mathbb{R}$ï¼ˆé€£ç¶šç©ºé–“ï¼‰ã§ã¯ã€å†ªé›†åˆã®ã™ã¹ã¦ã«ã€Œé•·ã•ã€ã‚’å®šç¾©ã™ã‚‹ã“ã¨ãŒä¸å¯èƒ½ã§ã‚ã‚‹ã“ã¨ãŒBanach-Tarskiã®ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹ã‹ã‚‰å¸°çµã•ã‚Œã‚‹ã€‚Ïƒ-åŠ æ³•æ—ã¯ã“ã®å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã®æ•°å­¦çš„è£…ç½®ã ã€‚ç¬¬5å›ã§Lebesgueæ¸¬åº¦ã¨Borelé›†åˆã¨ã—ã¦å³å¯†ã«æ‰±ã†ã€‚

### 3.2 ç¢ºç‡å¤‰æ•°ã¨æœŸå¾…å€¤ãƒ»åˆ†æ•£

**å®šç¾©ï¼ˆç¢ºç‡å¤‰æ•°ï¼‰**: ç¢ºç‡ç©ºé–“ $(\Omega, \mathcal{F}, P)$ ä¸Šã®ç¢ºç‡å¤‰æ•° $X$ ã¨ã¯ã€å¯æ¸¬é–¢æ•° $X: \Omega \to \mathbb{R}$ ã§ã‚ã‚‹ã€‚

ã€Œå¯æ¸¬ã€ã¨ã¯ã€ä»»æ„ã®Borelé›†åˆ $B \subseteq \mathbb{R}$ ã«å¯¾ã—ã¦ $X^{-1}(B) = \{\omega \in \Omega : X(\omega) \in B\} \in \mathcal{F}$ ãŒæˆã‚Šç«‹ã¤ã“ã¨ã€‚ç›´æ„Ÿçš„ã«ã¯ã€Œ$X$ ã®å€¤ã«é–¢ã™ã‚‹ä»»æ„ã®å•ã„ï¼ˆ$X \leq a$ ãªã©ï¼‰ã«ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã€ã¨ã„ã†ã“ã¨ã ã€‚

**æœŸå¾…å€¤**ï¼ˆé›¢æ•£ã®å ´åˆï¼‰:

$$
\mathbb{E}[X] = \sum_{x} x \cdot P(X = x)
$$

**æœŸå¾…å€¤**ï¼ˆé€£ç¶šã®å ´åˆï¼‰:

$$
\mathbb{E}[X] = \int_{-\infty}^{\infty} x \cdot f_X(x) \, dx
$$

ã“ã“ã§ $f_X(x)$ ã¯ç¢ºç‡å¯†åº¦é–¢æ•°ï¼ˆPDFï¼‰ã€‚

**åˆ†æ•£**:

$$
\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

**å…±åˆ†æ•£**:

$$
\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
$$

```python
import numpy as np

# Expectation, Variance, Covariance from first principles
np.random.seed(42)
n = 100000

# Discrete: roll two dice
X = np.random.randint(1, 7, size=n)  # die 1
Y = np.random.randint(1, 7, size=n)  # die 2
Z = X + Y  # sum

print("=== Expectation & Variance ===")
print(f"E[X] = {X.mean():.4f} (theory = 3.5)")
print(f"E[Y] = {Y.mean():.4f} (theory = 3.5)")
print(f"E[Z] = E[X+Y] = {Z.mean():.4f} (theory = 7.0)")
print(f"Var(X) = {X.var():.4f} (theory = {(6**2-1)/12:.4f})")
print(f"Var(Z) = {Z.var():.4f} (theory = {2*(6**2-1)/12:.4f})")

# Linearity of expectation: E[aX+b] = aE[X]+b
a, b = 3, -2
print(f"\nE[{a}X+({b})] = {(a*X+b).mean():.4f} (theory = {a*3.5+b:.4f})")
print(f"Var({a}X+({b})) = {(a*X+b).var():.4f} (theory = {a**2*(6**2-1)/12:.4f})")

# Covariance and independence
print(f"\nCov(X, Y) = {np.cov(X, Y, ddof=0)[0,1]:.4f} (theory â‰ˆ 0, independent)")
print(f"Cov(X, Z) = {np.cov(X, Z, ddof=0)[0,1]:.4f} (theory = Var(X) = {(6**2-1)/12:.4f})")
```

| æ€§è³ª | æ•°å¼ | åå‰ |
|:-----|:-----|:-----|
| ç·šå½¢æ€§ | $\mathbb{E}[aX + b] = a\mathbb{E}[X] + b$ | æœŸå¾…å€¤ã®ç·šå½¢æ€§ |
| ç‹¬ç«‹ãªã‚‰ | $\mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]$ | ç‹¬ç«‹ã®å¸°çµ |
| åˆ†æ•£ã®å¤‰æ› | $\text{Var}(aX+b) = a^2 \text{Var}(X)$ | åˆ†æ•£ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° |
| ç‹¬ç«‹ã®å’Œ | $\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y)$ | ç‹¬ç«‹ãªã‚‰åˆ†æ•£ã‚‚åŠ æ³•çš„ |

**ç‹¬ç«‹æ€§ã®å®šç¾©**: ç¢ºç‡å¤‰æ•° $X, Y$ ãŒç‹¬ç«‹ $\iff$ $P(X \in A, Y \in B) = P(X \in A) \cdot P(Y \in B)$ ãŒå…¨ã¦ã®äº‹è±¡ $A, B$ ã§æˆç«‹ã€‚

ç‹¬ç«‹æ€§ã¯æ©Ÿæ¢°å­¦ç¿’ã§é »ç¹ã«ä»®å®šã•ã‚Œã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒ**ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰**ã§ã‚ã‚‹ã¨ã„ã†ä»®å®š â€” $\mathbf{x}_1, \ldots, \mathbf{x}_N \overset{\text{i.i.d.}}{\sim} p(\mathbf{x}; \theta)$ â€” ã¯MLEï¼ˆ3.7ç¯€ï¼‰ã®å‡ºç™ºç‚¹ã ã€‚

### 3.3 ãƒ™ã‚¤ã‚ºã®å®šç† â€” äº‹å¾Œæ¨è«–ã®å‡ºç™ºç‚¹

**å®šç†ï¼ˆãƒ™ã‚¤ã‚ºã®å®šç†ï¼‰**: äº‹è±¡ $A, B$ ã«å¯¾ã—ã¦ $P(B) > 0$ ã®ã¨ã:

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

**è¨¼æ˜**: æ¡ä»¶ä»˜ãç¢ºç‡ã®å®šç¾© $P(A \mid B) = P(A \cap B) / P(B)$ ã¨ $P(B \mid A) = P(A \cap B) / P(A)$ ã‹ã‚‰ã€$P(A \cap B) = P(B \mid A) P(A)$ ã‚’ä»£å…¥ã™ã‚Œã°ç›´ã¡ã«å¾—ã‚‰ã‚Œã‚‹ã€‚$\square$

é€£ç¶šç¢ºç‡å¤‰æ•°ã®å ´åˆã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã®ãƒ™ã‚¤ã‚ºæ¨è«–ã¯:

$$
\underbrace{p(\theta \mid \mathcal{D})}_{\text{äº‹å¾Œåˆ†å¸ƒ}} = \frac{\overbrace{p(\mathcal{D} \mid \theta)}^{\text{å°¤åº¦}} \cdot \overbrace{p(\theta)}^{\text{äº‹å‰åˆ†å¸ƒ}}}{\underbrace{p(\mathcal{D})}_{\text{ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹}}}
$$

ã“ã“ã§ $\mathcal{D} = \{x_1, \ldots, x_N\}$ ã¯ãƒ‡ãƒ¼ã‚¿ã€$p(\mathcal{D}) = \int p(\mathcal{D} \mid \theta) p(\theta) d\theta$ ã¯ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ï¼ˆå‘¨è¾ºå°¤åº¦ï¼‰ã€‚

```python
import numpy as np

def bayesian_coin_update(prior_a: float, prior_b: float, heads: int, tails: int):
    """Bayesian updating with Beta-Bernoulli conjugate pair.

    Prior:     Beta(a, b)
    Likelihood: Bernoulli(Î¸)
    Posterior:  Beta(a + heads, b + tails)

    corresponds to: p(Î¸|D) âˆ Î¸^(a+h-1) (1-Î¸)^(b+t-1)
    """
    post_a = prior_a + heads
    post_b = prior_b + tails
    post_mean = post_a / (post_a + post_b)
    post_var = (post_a * post_b) / ((post_a + post_b)**2 * (post_a + post_b + 1))
    return post_a, post_b, post_mean, post_var

# Start with uniform prior Beta(1,1) = Uniform(0,1)
a, b = 1.0, 1.0
print("=== Sequential Bayesian Updating ===")
print(f"Prior: Beta({a:.0f},{b:.0f}), E[Î¸]={a/(a+b):.3f}\n")

# Observe coins sequentially
observations = [1, 1, 0, 1, 1, 1, 0, 1, 1, 1,  # 8H, 2T
                1, 1, 0, 1, 1, 1, 0, 1, 1, 1]  # 8H, 2T total: 16H, 4T
for i, obs in enumerate(observations, 1):
    a, b, mean, var = bayesian_coin_update(a, b, obs, 1 - obs)
    if i in [1, 5, 10, 20]:
        print(f"After {i:2d} obs: Beta({a:.0f},{b:.0f}), E[Î¸]={mean:.3f}, Std={np.sqrt(var):.3f}")

print(f"\nTrue Î¸: 0.800")
print(f"MLE:    {16/20:.3f}")
print(f"Bayes:  {a/(a+b):.3f} (Beta posterior mean)")
```

:::message
ã“ã“ãŒæœ€åˆã®ã¤ã¾ãšããƒã‚¤ãƒ³ãƒˆã  â€” **ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ $p(\mathcal{D})$ ã®è¨ˆç®—ãŒå›°é›£**ã€‚ã“ã‚Œã¯ $\theta$ ã«é–¢ã™ã‚‹ç©åˆ† $\int p(\mathcal{D} \mid \theta) p(\theta) d\theta$ ã§ã‚ã‚Šã€å¤šãã®å ´åˆè§£æçš„ã«è¨ˆç®—ã§ããªã„ã€‚ã“ã®å›°é›£ãŒç¬¬8å›ï¼ˆEMç®—æ³•ï¼‰ã¨ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã®å‹•æ©Ÿã«ãªã‚‹ã€‚VAEã¯ã“ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã®ä¸‹ç•Œï¼ˆELBOï¼‰ã‚’æœ€å¤§åŒ–ã™ã‚‹æ‰‹æ³•ã ã€‚
:::

**å…±å½¹äº‹å‰åˆ†å¸ƒ**: äº‹å‰åˆ†å¸ƒã¨äº‹å¾Œåˆ†å¸ƒãŒåŒã˜åˆ†å¸ƒæ—ã«å±ã™ã‚‹ã¨ãã€ãã®äº‹å‰åˆ†å¸ƒã‚’**å…±å½¹äº‹å‰åˆ†å¸ƒ**ã¨ã„ã†ã€‚

| å°¤åº¦ | å…±å½¹äº‹å‰åˆ†å¸ƒ | äº‹å¾Œåˆ†å¸ƒ |
|:-----|:-----------|:---------|
| Bernoulli($\theta$) | Beta($a, b$) | Beta($a + h, b + t$) |
| Gaussian($\mu$, æ—¢çŸ¥$\sigma^2$) | Gaussian($\mu_0, \sigma_0^2$) | Gaussian($\mu_N, \sigma_N^2$) |
| Poisson($\lambda$) | Gamma($\alpha, \beta$) | Gamma($\alpha + \sum x_i, \beta + N$) |
| Categorical($\boldsymbol{\pi}$) | Dirichlet($\boldsymbol{\alpha}$) | Dirichlet($\boldsymbol{\alpha} + \text{counts}$) |

ã“ã“ã§ã€Gaussianè¡Œã®äº‹å¾Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯:

$$
\sigma_N^2 = \left(\frac{1}{\sigma_0^2} + \frac{N}{\sigma^2}\right)^{-1}, \quad \mu_N = \sigma_N^2 \left(\frac{\mu_0}{\sigma_0^2} + \frac{N \bar{x}}{\sigma^2}\right)
$$

å…±å½¹äº‹å‰åˆ†å¸ƒãŒä¾¿åˆ©ãªç†ç”±ã¯ã€äº‹å¾Œåˆ†å¸ƒã®è¨ˆç®—ãŒ**é–‰ã˜ãŸå½¢**ã§å¾—ã‚‰ã‚Œã‚‹ã“ã¨ã ã€‚æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼ˆ3.6ç¯€ï¼‰ã¨ã®æ·±ã„é–¢ä¿‚ãŒã‚ã‚‹ã€‚

:::details Jeffreysäº‹å‰åˆ†å¸ƒ â€” å®¢è¦³ãƒ™ã‚¤ã‚ºã®è©¦ã¿
äº‹å‰åˆ†å¸ƒã‚’ã©ã†é¸ã¶ã‹ã¯ä¸»è¦³çš„ãªåˆ¤æ–­ã ã€‚Harold Jeffreysã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰æ›ã«å¯¾ã—ã¦ä¸å¤‰ãªã€Œå®¢è¦³çš„ã€äº‹å‰åˆ†å¸ƒã‚’ææ¡ˆã—ãŸ:

$$
p_J(\theta) \propto \sqrt{\det I(\theta)}
$$

ã“ã“ã§ $I(\theta)$ ã¯Fisheræƒ…å ±è¡Œåˆ—ï¼ˆ3.8ç¯€ï¼‰ã€‚Bernoulliå°¤åº¦ã«å¯¾ã™ã‚‹Jeffreysäº‹å‰åˆ†å¸ƒã¯ $\text{Beta}(1/2, 1/2)$ â€” Uå­—å‹ã®åˆ†å¸ƒã§ã€0ã¨1ã®è¿‘ãã«è³ªé‡ã‚’é›†ä¸­ã•ã›ã‚‹ã€‚

Jeffreysäº‹å‰åˆ†å¸ƒã¯ä½æ¬¡å…ƒã§ã¯æœ‰ç”¨ã ãŒã€é«˜æ¬¡å…ƒã§ã¯ã€Œæ‹¡æ•£ã—ã™ãã‚‹ã‹é›†ä¸­ã—ã™ãã‚‹ã€å•é¡ŒãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã€‚å®Ÿç”¨ä¸Šã¯ã€å¼±æƒ…å ±äº‹å‰åˆ†å¸ƒï¼ˆweakly informative priorï¼‰ã‚’æ¨å¥¨ã™ã‚‹å£°ãŒå¤šã„ã€‚
:::

### 3.4 ä¸»è¦ãªç¢ºç‡åˆ†å¸ƒ â€” é›¢æ•£ã¨é€£ç¶š

#### 3.4.1 é›¢æ•£åˆ†å¸ƒã®è©³ç´°

**Bernoulliåˆ†å¸ƒ**: $X \sim \text{Bernoulli}(p)$

$$
P(X = x) = p^x (1-p)^{1-x}, \quad x \in \{0, 1\}
$$

$$
\mathbb{E}[X] = p, \quad \text{Var}(X) = p(1-p)
$$

**Binomialåˆ†å¸ƒ**: $X \sim \text{Binomial}(n, p)$ â€” $n$ å›ã®ç‹¬ç«‹ãªBernoulliè©¦è¡Œã®æˆåŠŸå›æ•°

$$
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}, \quad k \in \{0, 1, \ldots, n\}
$$

$$
\mathbb{E}[X] = np, \quad \text{Var}(X) = np(1-p)
$$

**Poissonåˆ†å¸ƒ**: $X \sim \text{Poisson}(\lambda)$ â€” å˜ä½æ™‚é–“ã‚ãŸã‚Šã®äº‹è±¡ç™ºç”Ÿå›æ•°

$$
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k \in \{0, 1, 2, \ldots\}
$$

$$
\mathbb{E}[X] = \lambda, \quad \text{Var}(X) = \lambda
$$

Poissonåˆ†å¸ƒã¯Binomialåˆ†å¸ƒã®æ¥µé™ã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹: $n \to \infty$, $p \to 0$, $np = \lambda$ ã®ã¨ã $\text{Binomial}(n,p) \to \text{Poisson}(\lambda)$ã€‚

```python
import numpy as np
from math import comb, factorial

def binomial_pmf(k: int, n: int, p: float) -> float:
    """P(X=k) = C(n,k) p^k (1-p)^(n-k)"""
    return comb(n, k) * p**k * (1 - p)**(n - k)

def poisson_pmf(k: int, lam: float) -> float:
    """P(X=k) = Î»^k e^{-Î»} / k!"""
    return lam**k * np.exp(-lam) / factorial(k)

# Poisson as limit of Binomial
lam = 5.0
print(f"Poisson limit theorem: Binomial(n, Î»/n) â†’ Poisson(Î»={lam})")
print(f"{'k':<4} {'Poisson':>10} {'Bin(10)':>10} {'Bin(100)':>10} {'Bin(1000)':>10}")
print("-" * 48)
for k in range(11):
    pois = poisson_pmf(k, lam)
    b10 = binomial_pmf(k, 10, lam/10)
    b100 = binomial_pmf(k, 100, lam/100)
    b1000 = binomial_pmf(k, 1000, lam/1000)
    print(f"{k:<4} {pois:>10.6f} {b10:>10.6f} {b100:>10.6f} {b1000:>10.6f}")
```

#### 3.4.2 é€£ç¶šåˆ†å¸ƒã®è©³ç´°

**Gaussianåˆ†å¸ƒ** (1.2ç¯€ã§å°å…¥æ¸ˆã¿): $X \sim \mathcal{N}(\mu, \sigma^2)$

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

**Gammaåˆ†å¸ƒ**: $X \sim \text{Gamma}(\alpha, \beta)$ï¼ˆshape $\alpha$, rate $\beta$ï¼‰

$$
f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}, \quad x > 0
$$

$$
\mathbb{E}[X] = \frac{\alpha}{\beta}, \quad \text{Var}(X) = \frac{\alpha}{\beta^2}
$$

**Betaåˆ†å¸ƒ**: $X \sim \text{Beta}(\alpha, \beta)$

$$
f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1} (1-x)^{\beta-1}, \quad x \in (0, 1)
$$

$$
\mathbb{E}[X] = \frac{\alpha}{\alpha+\beta}, \quad \text{Var}(X) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
$$

Betaåˆ†å¸ƒã¯Bernoulli/Binomialã®å…±å½¹äº‹å‰åˆ†å¸ƒã€‚$\alpha = \beta = 1$ ã§ä¸€æ§˜åˆ†å¸ƒã«ä¸€è‡´ã™ã‚‹ã€‚

```python
import numpy as np

def gamma_pdf(x: np.ndarray, alpha: float, beta: float) -> np.ndarray:
    """Gamma PDF: f(x) = Î²^Î± / Î“(Î±) x^{Î±-1} exp(-Î²x)"""
    from math import gamma as gamma_fn
    return (beta**alpha / gamma_fn(alpha)) * x**(alpha - 1) * np.exp(-beta * x)

def beta_pdf(x: np.ndarray, a: float, b: float) -> np.ndarray:
    """Beta PDF: f(x) = Î“(a+b)/(Î“(a)Î“(b)) x^{a-1} (1-x)^{b-1}"""
    from math import gamma as gamma_fn
    B = gamma_fn(a) * gamma_fn(b) / gamma_fn(a + b)
    return x**(a - 1) * (1 - x)**(b - 1) / B

# Gamma distribution properties
x_gamma = np.linspace(0.01, 15, 1000)
for alpha, beta in [(1, 1), (2, 1), (3, 0.5), (5, 1)]:
    pdf = gamma_pdf(x_gamma, alpha, beta)
    mean_theory = alpha / beta
    var_theory = alpha / beta**2
    # Numerical verification via trapezoidal rule
    dx = x_gamma[1] - x_gamma[0]
    mean_num = np.sum(x_gamma * pdf) * dx
    var_num = np.sum((x_gamma - mean_num)**2 * pdf) * dx
    print(f"Gamma(Î±={alpha},Î²={beta}): E[X]={mean_theory:.2f} (num:{mean_num:.2f}), "
          f"Var={var_theory:.2f} (num:{var_num:.2f})")

print()
# Beta distribution â€” conjugate prior for Bernoulli
x_beta = np.linspace(0.001, 0.999, 1000)
for a, b in [(1, 1), (2, 5), (5, 2), (0.5, 0.5)]:
    pdf = beta_pdf(x_beta, a, b)
    mean_theory = a / (a + b)
    name = "Uniform" if (a == 1 and b == 1) else f"Beta({a},{b})"
    if a == 0.5 and b == 0.5:
        name = "Jeffreys"
    dx = x_beta[1] - x_beta[0]
    mode_x = x_beta[np.argmax(pdf)]
    print(f"{name:>12}: E[Î¸]={mean_theory:.3f}, modeâ‰ˆ{mode_x:.3f}")
```

### 3.5 å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒ â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åŸºç¤è¨€èª

$d$ æ¬¡å…ƒã®å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã¯:

$$
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{d/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)
$$

ã“ã“ã§ $\boldsymbol{\mu} \in \mathbb{R}^d$ ã¯å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã€$\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}$ ã¯å…±åˆ†æ•£è¡Œåˆ—ï¼ˆå¯¾ç§°æ­£å®šå€¤ï¼‰ã€‚

**ç²¾åº¦è¡Œåˆ—**: $\boldsymbol{\Lambda} = \boldsymbol{\Sigma}^{-1}$ ã‚’ç²¾åº¦è¡Œåˆ—ã¨å‘¼ã¶ã€‚å¯†åº¦é–¢æ•°ã¯ç²¾åº¦è¡Œåˆ—ã‚’ä½¿ã†ã¨:

$$
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1}) \propto \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Lambda} (\mathbf{x} - \boldsymbol{\mu})\right)
$$

ç²¾åº¦è¡Œåˆ—ã® $(i,j)$ æˆåˆ†ãŒ0ã§ã‚ã‚‹ã“ã¨ã¯ã€$X_i$ ã¨ $X_j$ ãŒä»–ã®å¤‰æ•°ã‚’æ¡ä»¶ä»˜ã‘ãŸã¨ãæ¡ä»¶ä»˜ãç‹¬ç«‹ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚ã“ã‚ŒãŒã‚¬ã‚¦ã‚¹ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆGaussian Graphical Modelï¼‰ã®åŸºç›¤ã ã€‚

**æ¡ä»¶ä»˜ãåˆ†å¸ƒã¨å‘¨è¾ºåˆ†å¸ƒ**:

$\mathbf{x} = (\mathbf{x}_1, \mathbf{x}_2)^\top$ ã¨åˆ†å‰²ã—ã€å¯¾å¿œã™ã‚‹å¹³å‡ã¨å…±åˆ†æ•£ã‚‚åˆ†å‰²ã™ã‚‹:

$$
\boldsymbol{\mu} = \begin{pmatrix} \boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{pmatrix}, \quad \boldsymbol{\Sigma} = \begin{pmatrix} \boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12} \\ \boldsymbol{\Sigma}_{21} & \boldsymbol{\Sigma}_{22} \end{pmatrix}
$$

ã“ã®ã¨ã:

**å‘¨è¾ºåˆ†å¸ƒ**: $\mathbf{x}_1 \sim \mathcal{N}(\boldsymbol{\mu}_1, \boldsymbol{\Sigma}_{11})$ â€” å˜ã«å¯¾å¿œã™ã‚‹éƒ¨åˆ†ã‚’æŠœãå‡ºã™ã ã‘

**æ¡ä»¶ä»˜ãåˆ†å¸ƒ**:

$$
\mathbf{x}_1 \mid \mathbf{x}_2 \sim \mathcal{N}\left(\boldsymbol{\mu}_1 + \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}(\mathbf{x}_2 - \boldsymbol{\mu}_2), \; \boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}\right)
$$

æ¡ä»¶ä»˜ãå…±åˆ†æ•£ $\boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}$ ã¯ç·šå½¢ä»£æ•°ã®**Schurè£œè¡Œåˆ—**ãã®ã‚‚ã®ã€‚ç¬¬2å›ã§å­¦ã‚“ã é“å…·ãŒã“ã“ã§å†ç™»å ´ã™ã‚‹ã€‚

```python
import numpy as np

# 2D Gaussian: conditional and marginal
mu = np.array([1.0, 2.0])
Sigma = np.array([[1.0, 0.8],
                   [0.8, 1.5]])

# Marginal of x1
print(f"Marginal xâ‚: N({mu[0]}, {Sigma[0,0]})")
print(f"Marginal xâ‚‚: N({mu[1]}, {Sigma[1,1]})")

# Conditional x1 | x2 = 3.0
x2_obs = 3.0
# Î¼_{1|2} = Î¼â‚ + Î£â‚â‚‚ Î£â‚‚â‚‚â»Â¹ (xâ‚‚ - Î¼â‚‚)
mu_cond = mu[0] + Sigma[0, 1] / Sigma[1, 1] * (x2_obs - mu[1])
# Î£_{1|2} = Î£â‚â‚ - Î£â‚â‚‚ Î£â‚‚â‚‚â»Â¹ Î£â‚‚â‚  (Schur complement)
sigma_cond = Sigma[0, 0] - Sigma[0, 1]**2 / Sigma[1, 1]
print(f"\nConditional xâ‚ | xâ‚‚={x2_obs}:")
print(f"  Î¼_{'{1|2}'} = {mu[0]} + {Sigma[0,1]}/{Sigma[1,1]} Ã— ({x2_obs} - {mu[1]}) = {mu_cond:.4f}")
print(f"  ÏƒÂ²_{'{1|2}'} = {Sigma[0,0]} - {Sigma[0,1]}Â²/{Sigma[1,1]} = {sigma_cond:.4f}")

# Verify by sampling
samples = np.random.multivariate_normal(mu, Sigma, size=100000)
mask = np.abs(samples[:, 1] - x2_obs) < 0.1
cond_samples = samples[mask, 0]
print(f"\nNumerical verification (samples near xâ‚‚={x2_obs}):")
print(f"  Mean: {cond_samples.mean():.4f} (theory: {mu_cond:.4f})")
print(f"  Var:  {cond_samples.var():.4f} (theory: {sigma_cond:.4f})")
```

**Mahalanobisè·é›¢**: å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æŒ‡æ•°éƒ¨ã«ç¾ã‚Œã‚‹ $(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})$ ã¯Mahalanobisè·é›¢ã®äºŒä¹—ã ã€‚ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã¨ç•°ãªã‚Šã€å…±åˆ†æ•£æ§‹é€ ã‚’è€ƒæ…®ã—ãŸã€Œå½¢ã«åˆã£ãŸè·é›¢ã€ã‚’æ¸¬ã‚‹ã€‚ç­‰Mahalanobisè·é›¢ã®æ›²é¢ã¯æ¥•å††ä½“ã‚’å½¢æˆã—ã€ãã®ä¸»è»¸ã¯å…±åˆ†æ•£è¡Œåˆ—ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã€è»¸ã®é•·ã•ã¯å›ºæœ‰å€¤ã®å¹³æ–¹æ ¹ã«æ¯”ä¾‹ã™ã‚‹ã€‚

:::details VAEã®æ½œåœ¨ç©ºé–“ â€” å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®ç›´æ¥å¿œç”¨
VAE [^2] ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯å…¥åŠ› $\mathbf{x}$ ã«å¯¾ã—ã¦:

$$
q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mathbf{z} \mid \boldsymbol{\mu}_\phi(\mathbf{x}), \text{diag}(\boldsymbol{\sigma}_\phi^2(\mathbf{x})))
$$

ã‚’å‡ºåŠ›ã™ã‚‹ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒ $\boldsymbol{\mu}$ ã¨ $\boldsymbol{\sigma}^2$ ã‚’äºˆæ¸¬ã—ã€æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚å¯¾è§’å…±åˆ†æ•£ã‚’ä»®å®šã™ã‚‹ã®ã¯è¨ˆç®—åŠ¹ç‡ã®ãŸã‚ â€” ç¬¬10å›ï¼ˆVAEåŸºç¤ï¼‰ã§ã€ã“ã®ä»®å®šã®å¸°çµã¨æ”¹å–„ç­–ã‚’è©³ã—ãè­°è«–ã™ã‚‹ã€‚
:::

### 3.6 æŒ‡æ•°å‹åˆ†å¸ƒæ— â€” çµ±ä¸€çš„ç†è§£

å¤šãã®é‡è¦ãªåˆ†å¸ƒã¯ã€**æŒ‡æ•°å‹åˆ†å¸ƒæ—**ï¼ˆExponential Familyï¼‰ã¨ã„ã†1ã¤ã®æ çµ„ã¿ã§çµ±ä¸€çš„ã«è¨˜è¿°ã§ãã‚‹ã€‚

**å®šç¾©**: ç¢ºç‡åˆ†å¸ƒãŒä»¥ä¸‹ã®å½¢ã§æ›¸ã‘ã‚‹ã¨ãã€æŒ‡æ•°å‹åˆ†å¸ƒæ—ã«å±ã™ã‚‹:

$$
p(\mathbf{x} \mid \boldsymbol{\eta}) = h(\mathbf{x}) \exp\left(\boldsymbol{\eta}^\top \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta})\right)
$$

| è¦ç´  | è¨˜å· | æ„å‘³ |
|:-----|:-----|:-----|
| è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\boldsymbol{\eta}$ | åˆ†å¸ƒã‚’æŒ‡å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| ååˆ†çµ±è¨ˆé‡ | $\mathbf{T}(\mathbf{x})$ | ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºã™ã‚‹çµ±è¨ˆé‡ |
| å¯¾æ•°æ­£è¦åŒ–å®šæ•° | $A(\boldsymbol{\eta})$ | $\int h(\mathbf{x}) \exp(\boldsymbol{\eta}^\top \mathbf{T}(\mathbf{x})) d\mathbf{x} = \exp(A(\boldsymbol{\eta}))$ |
| åŸºåº•æ¸¬åº¦ | $h(\mathbf{x})$ | $\boldsymbol{\eta}$ ã«ä¾å­˜ã—ãªã„å› å­ |

**æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®é©šãã¹ãæ€§è³ª**:

1. $\nabla_{\boldsymbol{\eta}} A(\boldsymbol{\eta}) = \mathbb{E}[\mathbf{T}(\mathbf{x})]$ â€” å¯¾æ•°æ­£è¦åŒ–å®šæ•°ã®å‹¾é…ãŒååˆ†çµ±è¨ˆé‡ã®æœŸå¾…å€¤
2. $\nabla_{\boldsymbol{\eta}}^2 A(\boldsymbol{\eta}) = \text{Cov}[\mathbf{T}(\mathbf{x})]$ â€” ãƒ˜ã‚·ã‚¢ãƒ³ãŒå…±åˆ†æ•£
3. $A(\boldsymbol{\eta})$ ã¯å‡¸é–¢æ•° â€” æœ€é©åŒ–ã«éƒ½åˆãŒã‚ˆã„
4. MLEã¯ $\mathbb{E}[\mathbf{T}(\mathbf{x})] = \frac{1}{N}\sum_{i=1}^{N} \mathbf{T}(\mathbf{x}_i)$ ã§é–‰ã˜ãŸå½¢

```python
import numpy as np

# Gaussian as exponential family
# N(x|Î¼,ÏƒÂ²) = (2Ï€ÏƒÂ²)^{-1/2} exp(-(x-Î¼)Â²/(2ÏƒÂ²))
# = (2Ï€)^{-1/2} exp(Î¼x/ÏƒÂ² - xÂ²/(2ÏƒÂ²) - Î¼Â²/(2ÏƒÂ²) - log Ïƒ)
# Î· = (Î¼/ÏƒÂ², -1/(2ÏƒÂ²))
# T(x) = (x, xÂ²)
# A(Î·) = -Î·â‚Â²/(4Î·â‚‚) - 1/2 log(-2Î·â‚‚) + 1/2 log(2Ï€)?
# Simpler: verify E[T(x)] = âˆ‡A(Î·)

mu, sigma = 2.0, 1.5

# Natural parameters
eta1 = mu / sigma**2
eta2 = -1 / (2 * sigma**2)
print(f"Gaussian N({mu}, {sigma**2})")
print(f"Natural parameters: Î·â‚ = Î¼/ÏƒÂ² = {eta1:.4f}, Î·â‚‚ = -1/(2ÏƒÂ²) = {eta2:.4f}")

# Sufficient statistics
samples = np.random.normal(mu, sigma, size=100000)
T1 = samples.mean()       # E[Tâ‚(x)] = E[x] = Î¼
T2 = (samples**2).mean()  # E[Tâ‚‚(x)] = E[xÂ²] = Î¼Â² + ÏƒÂ²
print(f"\nSufficient statistics (empirical):")
print(f"  E[Tâ‚(x)] = E[x] = {T1:.4f} (theory: {mu})")
print(f"  E[Tâ‚‚(x)] = E[xÂ²] = {T2:.4f} (theory: {mu**2 + sigma**2:.4f})")

# Show major distributions as exponential family
print("\n=== Distributions as Exponential Family ===")
print(f"{'Distribution':<20} {'Î· (natural param)':<25} {'T(x) (suff. stat.)':<20}")
print("-" * 65)
print(f"{'Bernoulli(p)':<20} {'log(p/(1-p))':<25} {'x':<20}")
print(f"{'Poisson(Î»)':<20} {'log(Î»)':<25} {'x':<20}")
print(f"{'Gaussian(Î¼,ÏƒÂ²)':<20} {'(Î¼/ÏƒÂ², -1/(2ÏƒÂ²))':<25} {'(x, xÂ²)':<20}")
print(f"{'Gamma(Î±,Î²)':<20} {'(Î±-1, -Î²)':<25} {'(log x, x)':<20}")
print(f"{'Beta(Î±,Î²)':<20} {'(Î±-1, Î²-1)':<25} {'(log x, log(1-x))':<20}")
```

**æ•°å€¤æ¤œè¨¼ â€” æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®æ€§è³ª $\nabla A = \mathbb{E}[T(x)]$**:

```python
import numpy as np

# Verify: âˆ‚A/âˆ‚Î· = E[T(x)] for Bernoulli
# Bernoulli as exp family: p(x|Î·) = exp(Î·x - log(1+exp(Î·)))
# Î· = log(p/(1-p)), A(Î·) = log(1+exp(Î·)), T(x) = x

def bernoulli_A(eta: float) -> float:
    """Log-normalizer A(Î·) = log(1 + exp(Î·))"""
    return np.log(1 + np.exp(eta))

def bernoulli_dA(eta: float, dt: float = 1e-6) -> float:
    """Numerical derivative of A"""
    return (bernoulli_A(eta + dt) - bernoulli_A(eta - dt)) / (2 * dt)

def bernoulli_d2A(eta: float, dt: float = 1e-5) -> float:
    """Numerical second derivative of A"""
    return (bernoulli_A(eta + dt) - 2 * bernoulli_A(eta) + bernoulli_A(eta - dt)) / dt**2

print("=== Exponential Family Property Verification ===\n")
print(f"{'p':>6} {'Î·':>8} {'âˆ‚A/âˆ‚Î·':>10} {'E[T(x)]':>10} {'âˆ‚Â²A/âˆ‚Î·Â²':>10} {'Var[T(x)]':>10}")
print("-" * 58)

for p in [0.1, 0.3, 0.5, 0.7, 0.9]:
    eta = np.log(p / (1 - p))  # natural parameter
    dA = bernoulli_dA(eta)      # should equal E[x] = p
    d2A = bernoulli_d2A(eta)    # should equal Var[x] = p(1-p)

    # Verify by sampling
    samples = np.random.binomial(1, p, 100000).astype(float)
    E_T = samples.mean()
    V_T = samples.var()

    print(f"{p:>6.1f} {eta:>8.4f} {dA:>10.6f} {E_T:>10.6f} {d2A:>10.6f} {V_T:>10.6f}")

print("\nâ†’ âˆ‚A/âˆ‚Î· = E[T(x)] and âˆ‚Â²A/âˆ‚Î·Â² = Var[T(x)] confirmed!")
print("â†’ This is WHY exponential families are mathematically elegant.")
```

**Softmaxé–¢æ•°ã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®æ­£è¦åŒ–**: Categoricalåˆ†å¸ƒã‚’æŒ‡æ•°å‹åˆ†å¸ƒæ—ã¨ã—ã¦æ›¸ãã¨ $p(x=k \mid \boldsymbol{\eta}) = \exp(\eta_k - A(\boldsymbol{\eta}))$ ã§ã€$A(\boldsymbol{\eta}) = \log \sum_k \exp(\eta_k)$ â€” ã“ã‚Œã¯ã¾ã•ã«log-sum-expã€ã¤ã¾ã‚ŠSoftmaxã®æ­£è¦åŒ–å®šæ•°ã ã€‚LLMã®å‡ºåŠ›å±¤ã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹ã€‚

:::message
æŒ‡æ•°å‹åˆ†å¸ƒæ—ãŒé‡è¦ãªç†ç”±ã¯3ã¤ã‚ã‚‹:

1. **å…±å½¹äº‹å‰åˆ†å¸ƒãŒè‡ªå‹•çš„ã«å­˜åœ¨ã™ã‚‹** â€” ãƒ™ã‚¤ã‚ºæ¨è«–ãŒç°¡æ½”
2. **MLEãŒååˆ†çµ±è¨ˆé‡ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãƒãƒƒãƒãƒ³ã‚°ã«å¸°ç€** â€” åŠ¹ç‡çš„ãªæ¨å®š
3. **Fisheræƒ…å ±é‡ãŒå¯¾æ•°æ­£è¦åŒ–å®šæ•°ã®ãƒ˜ã‚·ã‚¢ãƒ³ã«ä¸€è‡´** â€” æ¨å®šã®æœ€é©æ€§ç†è«–ã«ç›´çµ

EBMï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼‰[^7] ã® $p(\mathbf{x}) = \frac{1}{Z(\theta)} \exp(-E_\theta(\mathbf{x}))$ ã¯æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ä¸€èˆ¬åŒ–ã§ã‚ã‚Šã€ç¬¬27å›ã§è©³ã—ãæ‰±ã†ã€‚
:::

### 3.7 æœ€å°¤æ¨å®šï¼ˆMLEï¼‰å®Œå…¨ç‰ˆ

**å•é¡Œè¨­å®š**: ãƒ‡ãƒ¼ã‚¿ $\mathcal{D} = \{\mathbf{x}_1, \ldots, \mathbf{x}_N\} \overset{\text{i.i.d.}}{\sim} p(\mathbf{x}; \theta)$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’æ¨å®šã—ãŸã„ã€‚

**å®šç¾©ï¼ˆæœ€å°¤æ¨å®šé‡ï¼‰**:

$$
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \prod_{i=1}^{N} p(\mathbf{x}_i; \theta) = \arg\max_\theta \sum_{i=1}^{N} \log p(\mathbf{x}_i; \theta)
$$

ç©ã‚’å¯¾æ•°ã§å’Œã«å¤‰æ›ã™ã‚‹ã®ã¯ã€æ•°å€¤çš„å®‰å®šæ€§ã¨å¾®åˆ†ã®å®¹æ˜“ã•ã®ãŸã‚ã€‚

**å°¤åº¦æ–¹ç¨‹å¼**: $\hat{\theta}_{\text{MLE}}$ ã¯ä»¥ä¸‹ã‚’æº€ãŸã™:

$$
\frac{\partial}{\partial \theta} \sum_{i=1}^{N} \log p(\mathbf{x}_i; \theta) = \mathbf{0}
$$

**ä¾‹: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®MLE**

$x_1, \ldots, x_N \overset{\text{i.i.d.}}{\sim} \mathcal{N}(\mu, \sigma^2)$ ã®ã¨ã:

$$
\ell(\mu, \sigma^2) = \sum_{i=1}^{N} \log \mathcal{N}(x_i \mid \mu, \sigma^2) = -\frac{N}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(x_i - \mu)^2
$$

$\mu$ ã§åå¾®åˆ†ã—ã¦0ã¨ãŠã:

$$
\frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2}\sum_{i=1}^{N}(x_i - \mu) = 0 \implies \hat{\mu}_{\text{MLE}} = \frac{1}{N}\sum_{i=1}^{N}x_i = \bar{x}
$$

$\sigma^2$ ã§åå¾®åˆ†ã—ã¦0ã¨ãŠã:

$$
\frac{\partial \ell}{\partial \sigma^2} = -\frac{N}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^{N}(x_i - \bar{x})^2 = 0 \implies \hat{\sigma}^2_{\text{MLE}} = \frac{1}{N}\sum_{i=1}^{N}(x_i - \bar{x})^2
$$

åˆ†æ¯ãŒ $N$ ã§ã‚ã£ã¦ $N-1$ ã§ã¯ãªã„ã“ã¨ã«æ³¨æ„ã€‚MLEã®åˆ†æ•£æ¨å®šé‡ã¯**ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚‹**ï¼ˆéå°è©•ä¾¡ã™ã‚‹ï¼‰ã€‚ä¸åæ¨å®šé‡ã¯ $N-1$ ã§å‰²ã‚‹ã€‚

```python
import numpy as np

def gaussian_mle(data: np.ndarray):
    """MLE for Gaussian parameters.

    corresponds to: Î¼_MLE = (1/N)Î£xáµ¢, ÏƒÂ²_MLE = (1/N)Î£(xáµ¢-Î¼)Â²
    """
    mu_mle = data.mean()
    sigma2_mle = ((data - mu_mle)**2).mean()  # biased
    sigma2_unbiased = ((data - mu_mle)**2).sum() / (len(data) - 1)  # unbiased
    return mu_mle, sigma2_mle, sigma2_unbiased

# Ground truth
true_mu, true_sigma = 3.0, 2.0
np.random.seed(42)

print(f"True parameters: Î¼={true_mu}, ÏƒÂ²={true_sigma**2}")
print(f"\n{'N':>6} {'Î¼_MLE':>8} {'ÏƒÂ²_MLE':>10} {'ÏƒÂ²_unbiased':>12} {'|Î¼-Î¼Ì‚|':>8}")
print("-" * 50)
for N in [5, 10, 50, 100, 1000, 10000]:
    data = np.random.normal(true_mu, true_sigma, size=N)
    mu_hat, s2_mle, s2_unb = gaussian_mle(data)
    print(f"{N:>6} {mu_hat:>8.4f} {s2_mle:>10.4f} {s2_unb:>12.4f} {abs(mu_hat-true_mu):>8.4f}")
```

**MLEã®æ¼¸è¿‘çš„æ€§è³ª**ï¼ˆæ­£å‰‡æ¡ä»¶ã®ä¸‹ã§ï¼‰:

1. **ä¸€è‡´æ€§**: $\hat{\theta}_{\text{MLE}} \xrightarrow{P} \theta^*$ï¼ˆ$N \to \infty$ ã§çœŸã®å€¤ã«ç¢ºç‡åæŸï¼‰
2. **æ¼¸è¿‘æ­£è¦æ€§**: $\sqrt{N}(\hat{\theta}_{\text{MLE}} - \theta^*) \xrightarrow{d} \mathcal{N}(0, I(\theta^*)^{-1})$
3. **æ¼¸è¿‘æœ‰åŠ¹æ€§**: æ¼¸è¿‘åˆ†æ•£ãŒCramÃ©r-Raoä¸‹ç•Œã«åˆ°é”

ã“ã“ã§ $I(\theta^*)$ ã¯Fisheræƒ…å ±é‡ï¼ˆæ¬¡ç¯€ï¼‰ã€‚MLEãŒã€Œæœ€ã‚‚åŠ¹ç‡çš„ãªæ¨å®šé‡ã€ã§ã‚ã‚‹ç†ç”±ãŒã“ã“ã«ã‚ã‚‹ã€‚

:::message alert
MLEã®æ­£å‰‡æ¡ä»¶ï¼ˆæš—é»™ã®ä»®å®šï¼‰:
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ãŒã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§ã‚ã‚‹ã‹ã€å°¤åº¦ãŒæœ‰ç•Œ
- çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta^*$ ãŒå†…ç‚¹
- $p(\mathbf{x}; \theta)$ ãŒ $\theta$ ã«ã¤ã„ã¦3å›å¾®åˆ†å¯èƒ½
- Fisheræƒ…å ±é‡ $I(\theta)$ ãŒæ­£å®šå€¤

ã“ã‚Œã‚‰ãŒç ´ã‚Œã‚‹ã¨ã€MLEã¯ä¸€è‡´æ€§ã™ã‚‰ä¿è¨¼ã•ã‚Œãªã„ã€‚æ··åˆãƒ¢ãƒ‡ãƒ«ã®ç‰¹ç•°æ€§ï¼ˆç¬¬8å›EMï¼‰ã¯ãã®å…¸å‹ä¾‹ã ã€‚
:::

**ä¾‹: Bernoulliåˆ†å¸ƒã®MLE**

$x_1, \ldots, x_N \overset{\text{i.i.d.}}{\sim} \text{Bernoulli}(p)$ ã®ã¨ã:

$$
\ell(p) = \sum_{i=1}^{N} [x_i \log p + (1 - x_i) \log(1-p)] = h \log p + (N - h) \log(1-p)
$$

ã“ã“ã§ $h = \sum_{i=1}^{N} x_i$ ã¯æˆåŠŸå›æ•°ã€‚å¾®åˆ†ã—ã¦0ã¨ãŠã:

$$
\frac{\partial \ell}{\partial p} = \frac{h}{p} - \frac{N - h}{1 - p} = 0 \implies \hat{p}_{\text{MLE}} = \frac{h}{N}
$$

ç›´æ„Ÿé€šã‚Šã€ŒæˆåŠŸå›æ•° / è©¦è¡Œå›æ•°ã€ãŒMLEã€‚

**ä¾‹: Poissonåˆ†å¸ƒã®MLE**

$x_1, \ldots, x_N \overset{\text{i.i.d.}}{\sim} \text{Poisson}(\lambda)$ ã®ã¨ã:

$$
\ell(\lambda) = \sum_{i=1}^{N} [x_i \log \lambda - \lambda - \log(x_i!)]
$$

$$
\frac{\partial \ell}{\partial \lambda} = \frac{\sum x_i}{\lambda} - N = 0 \implies \hat{\lambda}_{\text{MLE}} = \bar{x}
$$

æ¨™æœ¬å¹³å‡ãŒMLEã«ãªã‚‹ã€‚æŒ‡æ•°å‹åˆ†å¸ƒæ—ã§ã¯ã€MLEã¯å¸¸ã«ååˆ†çµ±è¨ˆé‡ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãƒãƒƒãƒãƒ³ã‚°ã«å¸°ç€ã™ã‚‹ã€‚

```python
import numpy as np

# MLE comparison across distributions
np.random.seed(42)
N = 1000

# Bernoulli MLE
p_true = 0.65
bern_data = np.random.binomial(1, p_true, N)
p_mle = bern_data.mean()
print(f"Bernoulli(p={p_true}): pÌ‚_MLE = {p_mle:.4f}")

# Poisson MLE
lam_true = 4.2
pois_data = np.random.poisson(lam_true, N)
lam_mle = pois_data.mean()
print(f"Poisson(Î»={lam_true}): Î»Ì‚_MLE = {lam_mle:.4f}")

# Exponential MLE: Î»Ì‚ = 1/xÌ„
rate_true = 2.5
exp_data = np.random.exponential(1/rate_true, N)
rate_mle = 1 / exp_data.mean()
print(f"Exponential(Î»={rate_true}): Î»Ì‚_MLE = {rate_mle:.4f}")

# Categorical MLE: Ï€Ì‚_k = (count of k) / N
probs_true = np.array([0.1, 0.3, 0.15, 0.35, 0.1])
cat_data = np.random.choice(5, N, p=probs_true)
probs_mle = np.bincount(cat_data, minlength=5) / N
print(f"\nCategorical MLE:")
print(f"  True:  {probs_true}")
print(f"  MLE:   {probs_mle}")
print(f"  |diff|: {np.abs(probs_true - probs_mle).max():.4f}")
```

**MAPæ¨å®š**: MLE + äº‹å‰åˆ†å¸ƒ = MAPï¼ˆMaximum A Posterioriï¼‰:

$$
\hat{\theta}_{\text{MAP}} = \arg\max_\theta \left[\sum_{i=1}^{N} \log p(\mathbf{x}_i; \theta) + \log p(\theta)\right]
$$

ã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒ $p(\theta) = \mathcal{N}(0, \tau^2)$ ã‚’ã‹ã‘ã‚‹ã¨ã€$\log p(\theta) = -\theta^2/(2\tau^2) + \text{const}$ ãªã®ã§ã€MLEã«**L2æ­£å‰‡åŒ–**ãŒåŠ ã‚ã‚‹ã€‚ã¤ã¾ã‚ŠRidgeå›å¸°ã®ãƒ™ã‚¤ã‚ºçš„è§£é‡ˆã¯MAPæ¨å®šã ã€‚

**æ­£å‰‡åŒ–ã¨ãƒ™ã‚¤ã‚ºã®å¯¾å¿œ**:

| æ­£å‰‡åŒ– | äº‹å‰åˆ†å¸ƒ | MAPæ¨å®š |
|:-------|:--------|:-------|
| L2ï¼ˆRidgeï¼‰ | $\mathcal{N}(0, \tau^2)$ | $\hat{\theta}_{\text{MAP}} = \arg\max [\ell(\theta) - \frac{\lambda}{2}\|\theta\|_2^2]$ |
| L1ï¼ˆLassoï¼‰ | Laplace$(0, b)$ | $\hat{\theta}_{\text{MAP}} = \arg\max [\ell(\theta) - \lambda\|\theta\|_1]$ |
| Dropout | Spike-and-slab | å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç¢ºç‡çš„ã«0ã«ãªã‚‹äº‹å‰åˆ†å¸ƒ |

```python
import numpy as np

def map_vs_mle_bernoulli(data: np.ndarray, prior_a: float = 1, prior_b: float = 1):
    """Compare MLE and MAP for Bernoulli with Beta prior.

    MLE: pÌ‚ = h/N
    MAP: pÌ‚ = (h + a - 1) / (N + a + b - 2) for a,b > 1
    """
    N = len(data)
    h = data.sum()
    t = N - h

    mle = h / N if N > 0 else 0.5
    # MAP for Beta prior: mode of Beta(a+h, b+t)
    post_a = prior_a + h
    post_b = prior_b + t
    if post_a > 1 and post_b > 1:
        map_est = (post_a - 1) / (post_a + post_b - 2)
    else:
        map_est = float('nan')
    bayes_mean = post_a / (post_a + post_b)

    return mle, map_est, bayes_mean

# Edge case: N=0 (no data)
np.random.seed(42)
print("=== MLE vs MAP vs Bayes Mean ===\n")
print(f"{'N':>4} {'h':>3} {'MLE':>8} {'MAP':>8} {'Bayes':>8}")
print("-" * 35)

for N in [0, 1, 3, 10, 100]:
    if N == 0:
        data = np.array([])
    else:
        data = np.random.binomial(1, 0.7, N)
    h = data.sum() if N > 0 else 0
    mle, map_est, bayes = map_vs_mle_bernoulli(data, 2, 2)
    mle_str = f"{h/N:.4f}" if N > 0 else "undef"
    print(f"{N:>4} {int(h):>3} {mle_str:>8} {map_est:>8.4f} {bayes:>8.4f}")

print("\nKey: With Beta(2,2) prior, MAP/Bayes shrink toward 0.5 when data is scarce")
print("As Nâ†’âˆ, all three converge to the true parameter")
```

ã“ã®ã€ŒMLEã¨MAPã®é–¢ä¿‚ã€ã¯ã€LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã‚‚é‡è¦ã ã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã€Œäº‹å‰åˆ†å¸ƒã€ã¨è¦‹ãªã—ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å°¤åº¦ã§MAPæ›´æ–°ã™ã‚‹ â€” ã“ã‚ŒãŒLoRA [^12] ã‚„adapteræ‰‹æ³•ã®ç¢ºç‡è«–çš„è§£é‡ˆã ã€‚

### 3.8 Fisheræƒ…å ±é‡ã¨CramÃ©r-Raoä¸‹ç•Œ

**å®šç¾©ï¼ˆFisheræƒ…å ±é‡ï¼‰**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã«é–¢ã™ã‚‹ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’ $s(\mathbf{x}; \theta) = \nabla_\theta \log p(\mathbf{x}; \theta)$ ã¨ã™ã‚‹ã¨ã:

$$
I(\theta) = \mathbb{E}\left[s(\mathbf{x}; \theta) s(\mathbf{x}; \theta)^\top\right] = -\mathbb{E}\left[\nabla_\theta^2 \log p(\mathbf{x}; \theta)\right]
$$

2ã¤ç›®ã®ç­‰å·ã¯æ­£å‰‡æ¡ä»¶ã®ä¸‹ã§æˆã‚Šç«‹ã¤ã€‚ç›´æ„Ÿçš„ã«ã¯ã€Fisheræƒ…å ±é‡ã¯ã€Œå¯¾æ•°å°¤åº¦ã®æ›²ç‡ã€ã‚’æ¸¬ã£ã¦ã„ã‚‹ã€‚æ›²ç‡ãŒå¤§ãã„ã»ã©ã€ãƒ‡ãƒ¼ã‚¿ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦å¤šãã®æƒ…å ±ã‚’æŒã¤ã€‚

**å®šç†ï¼ˆCramÃ©r-Raoä¸‹ç•Œï¼‰**: ä¸åæ¨å®šé‡ $\hat{\theta}$ ã®åˆ†æ•£ã¯ä»¥ä¸‹ã§ä¸‹ã‹ã‚‰æŠ¼ã•ãˆã‚‰ã‚Œã‚‹ [^8]:

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{N \cdot I(\theta)}
$$

**ã“ã®ä¸‹ç•Œã«åˆ°é”ã™ã‚‹æ¨å®šé‡ã‚’æœ‰åŠ¹æ¨å®šé‡ã¨å‘¼ã¶ã€‚** MLEã¯æ¼¸è¿‘çš„ã«æœ‰åŠ¹ã§ã‚ã‚‹ã€‚

```python
import numpy as np

def fisher_information_bernoulli(p: float) -> float:
    """Fisher information for Bernoulli(p).

    I(p) = 1/(p(1-p))

    Derivation:
    log P(x|p) = x log p + (1-x) log(1-p)
    d/dp log P = x/p - (1-x)/(1-p)
    dÂ²/dpÂ² log P = -x/pÂ² - (1-x)/(1-p)Â²
    E[-dÂ²/dpÂ²] = 1/p + 0/(1-p)? No:
    E[-dÂ²/dpÂ²] = p/pÂ² + (1-p)/(1-p)Â² = 1/p + 1/(1-p) = 1/(p(1-p))
    """
    return 1 / (p * (1 - p))

def fisher_information_gaussian(sigma: float) -> np.ndarray:
    """Fisher information matrix for N(Î¼, ÏƒÂ²).

    I(Î¼,ÏƒÂ²) = diag(1/ÏƒÂ², 1/(2Ïƒâ´))
    """
    return np.diag([1/sigma**2, 1/(2*sigma**4)])

# CramÃ©r-Rao bound for Bernoulli
print("=== CramÃ©r-Rao Bound: Bernoulli(p) ===\n")
for p in [0.1, 0.3, 0.5, 0.7, 0.9]:
    I_p = fisher_information_bernoulli(p)
    for N in [10, 100, 1000]:
        cr_bound = 1 / (N * I_p)
        # MLE estimator: pÌ‚ = (# heads)/N, Var(pÌ‚) = p(1-p)/N
        mle_var = p * (1 - p) / N
        print(f"p={p}, N={N:4d}: CR bound={cr_bound:.6f}, Var(pÌ‚_MLE)={mle_var:.6f}, "
              f"efficient={'YES' if np.isclose(cr_bound, mle_var) else 'NO'}")
    print()

# Fisher information for Gaussian
print("=== Fisher Information: Gaussian ===")
sigma = 2.0
I_gauss = fisher_information_gaussian(sigma)
print(f"N(Î¼, ÏƒÂ²={sigma**2}):")
print(f"  I(Î¼,ÏƒÂ²) = diag({I_gauss[0,0]:.4f}, {I_gauss[1,1]:.6f})")
print(f"  CR bound on Var(Î¼Ì‚) = ÏƒÂ²/N = {sigma**2:.4f}/N")
print(f"  CR bound on Var(ÏƒÌ‚Â²) = 2Ïƒâ´/N = {2*sigma**4:.4f}/N")
```

**ã‚¹ã‚³ã‚¢é–¢æ•°ã¨Fisheræƒ…å ±é‡ã®æ·±ã„æ„å‘³**: ã‚¹ã‚³ã‚¢é–¢æ•° $s(\mathbf{x}; \theta) = \nabla_\theta \log p(\mathbf{x}; \theta)$ ã®ã€Œã‚¹ã‚³ã‚¢ã€ã¨ã„ã†åå‰ã¯ã€ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’ã©ã®æ–¹å‘ã«ã€ŒæŠ¼ã™ã€ã‹ã‚’ç¤ºã™ã“ã¨ã«ç”±æ¥ã™ã‚‹ã€‚Fisheræƒ…å ±é‡ã¯ã“ã®ã€ŒæŠ¼ã™åŠ›ã€ã®åˆ†æ•£ â€” ã¤ã¾ã‚Šãƒ‡ãƒ¼ã‚¿ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã©ã‚Œã ã‘ã€Œæƒ…å ±ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€ã‚’å®šé‡åŒ–ã™ã‚‹ã€‚

:::details Score Matchingã¨ã®æ¥ç¶š â€” ç¬¬28å›ã¸ã®ä¼ç·š
Score Matchingã¯ç¢ºç‡åˆ†å¸ƒ $p(\mathbf{x})$ ã®ã‚¹ã‚³ã‚¢ $\nabla_\mathbf{x} \log p(\mathbf{x})$ ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã€HyvÃ¤rinen (2005) [^9] ãŒææ¡ˆã—ãŸã€‚

æ³¨æ„: ã“ã“ã§ã€Œã‚¹ã‚³ã‚¢ã€ã¯**ãƒ‡ãƒ¼ã‚¿ç©ºé–“**ã§ã®å‹¾é…ã€‚Fisheræƒ…å ±é‡ã®ã‚¹ã‚³ã‚¢é–¢æ•°ã¯**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“**ã§ã®å‹¾é…ã€‚åå‰ã¯åŒã˜ã ãŒå¯¾è±¡ãŒé•ã†ã€‚

- Fisher score: $\nabla_\theta \log p(\mathbf{x}; \theta)$ â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å®šã«ä½¿ã†
- Stein score: $\nabla_\mathbf{x} \log p(\mathbf{x})$ â€” ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»ç”Ÿæˆã«ä½¿ã†

å¾Œè€…ãŒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ« [^4] ã®ç†è«–çš„åŸºç›¤ã§ã‚ã‚Šã€Song & Ermon (2020) [^10] ãŒSDEæ çµ„ã¿ã§çµ±ä¸€ã—ãŸã€‚ç¬¬28å›ã§æœ¬æ ¼çš„ã«æ‰±ã†ã€‚
:::

### 3.9 ç¢ºç‡å¤‰æ•°ã®å¤‰æ› â€” Change of Variables

ç¢ºç‡å¤‰æ•° $X$ ã«é–¢æ•° $g$ ã‚’é©ç”¨ã—ã¦ $Y = g(X)$ ã‚’å¾—ã‚‹ã¨ãã€$Y$ ã®åˆ†å¸ƒã¯ã©ã†ãªã‚‹ã‹ã€‚

**é›¢æ•£ã®å ´åˆ**: $P(Y = y) = \sum_{\{x : g(x) = y\}} P(X = x)$ï¼ˆé€†åƒã®ç¢ºç‡ã‚’è¶³ã™ï¼‰

**é€£ç¶šã®å ´åˆï¼ˆ1æ¬¡å…ƒã€$g$ ãŒå˜èª¿ï¼‰**:

$$
f_Y(y) = f_X(g^{-1}(y)) \cdot \left|\frac{dg^{-1}}{dy}\right|
$$

**å¤šæ¬¡å…ƒï¼ˆãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ï¼‰**: $\mathbf{Y} = g(\mathbf{X})$ ã®ã¨ã:

$$
f_\mathbf{Y}(\mathbf{y}) = f_\mathbf{X}(g^{-1}(\mathbf{y})) \cdot \left|\det \frac{\partial g^{-1}}{\partial \mathbf{y}}\right|
$$

ç¬¬3å›ã§å­¦ã‚“ã ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒã“ã“ã«ç™»å ´ã™ã‚‹ã€‚ã“ã®å¤‰æ›å…¬å¼ã¯ç¬¬25å›ï¼ˆNormalizing Flowsï¼‰ã®æ ¸å¿ƒã ã€‚Flowãƒ¢ãƒ‡ãƒ«ã¯å¯é€†å¤‰æ› $g$ ã‚’ç¹°ã‚Šè¿”ã—é©ç”¨ã—ã€å˜ç´”ãªåˆ†å¸ƒï¼ˆã‚¬ã‚¦ã‚¹ï¼‰ã‚’è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«å¤‰æ›ã™ã‚‹ã€‚

```python
import numpy as np

# Example: log-normal distribution via change of variables
# If X ~ N(Î¼, ÏƒÂ²), then Y = exp(X) ~ LogNormal(Î¼, ÏƒÂ²)
np.random.seed(42)
mu, sigma = 1.0, 0.5
N = 100000

X = np.random.normal(mu, sigma, N)
Y = np.exp(X)  # change of variables: g(x) = exp(x)

# Theory: E[Y] = exp(Î¼ + ÏƒÂ²/2), Var(Y) = (exp(ÏƒÂ²)-1)exp(2Î¼+ÏƒÂ²)
E_Y_theory = np.exp(mu + sigma**2 / 2)
V_Y_theory = (np.exp(sigma**2) - 1) * np.exp(2 * mu + sigma**2)

print("=== Change of Variables: X~N(Î¼,ÏƒÂ²) â†’ Y=exp(X)~LogNormal ===")
print(f"E[Y] empirical: {Y.mean():.4f}, theory: {E_Y_theory:.4f}")
print(f"Var(Y) empirical: {Y.var():.4f}, theory: {V_Y_theory:.4f}")

# Verify density via change of variables formula
# f_Y(y) = f_X(log(y)) * |d(log y)/dy| = f_X(log(y)) * 1/y
y_grid = np.linspace(0.01, 15, 1000)
f_Y_formula = (1 / (y_grid * sigma * np.sqrt(2 * np.pi))) * np.exp(-(np.log(y_grid) - mu)**2 / (2 * sigma**2))
print(f"\nDensity check: âˆ«f_Y(y)dy â‰ˆ {np.trapz(f_Y_formula, y_grid):.6f} (should be 1.0)")
```

:::message
å¤‰æ›å…¬å¼ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ $|\det J|$ ã¯ã€Œä½“ç©ã®ä¼¸ç¸®ç‡ã€ã ã€‚å¤‰æ› $g$ ãŒç©ºé–“ã‚’å¼•ãä¼¸ã°ã›ã°ç¢ºç‡å¯†åº¦ã¯è–„ããªã‚Šã€åœ§ç¸®ã™ã‚Œã°æ¿ƒããªã‚‹ã€‚Normalizing Flow [^11] ã¯ã“ã®æ€§è³ªã‚’åˆ©ç”¨ã—ã¦ã€è¤‡é›‘ãªåˆ†å¸ƒã®æ­£ç¢ºãªå°¤åº¦ã‚’è¨ˆç®—ã™ã‚‹ã€‚ç¬¬25å›ã§è©³ã—ãæ‰±ã†ã€‚
:::

### 3.10 ç¢ºç‡çš„ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«å…¥é–€

ç¢ºç‡å¤‰æ•°é–“ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚°ãƒ©ãƒ•ã§è¡¨ç¾ã™ã‚‹æ çµ„ã¿ã€‚

**ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**ï¼ˆæœ‰å‘ã‚°ãƒ©ãƒ•ï¼‰:

```mermaid
graph TD
    R["Rain<br/>P(R)"] --> W["Wet Grass<br/>P(W|R,S)"]
    S["Sprinkler<br/>P(S|R)"] --> W

    style R fill:#e3f2fd
    style S fill:#fff3e0
    style W fill:#c8e6c9
```

åŒæ™‚åˆ†å¸ƒãŒã‚°ãƒ©ãƒ•ã®æ§‹é€ ã«æ²¿ã£ã¦å› æ•°åˆ†è§£ã•ã‚Œã‚‹:

$$
P(R, S, W) = P(R) \cdot P(S \mid R) \cdot P(W \mid R, S)
$$

**d-separation**: ã‚°ãƒ©ãƒ•ã®æ§‹é€ ã‹ã‚‰æ¡ä»¶ä»˜ãç‹¬ç«‹æ€§ã‚’èª­ã¿å–ã‚Œã‚‹ã€‚ä¸Šã®ä¾‹ã§ã€$R$ ãŒè¦³æ¸¬ã•ã‚Œã‚‹ã¨ $S$ ã¨ $W$ ã¯æ¡ä»¶ä»˜ãç‹¬ç«‹ã§ã¯ãªã„ï¼ˆexplaining awayï¼‰ã€‚

**ãƒãƒ«ã‚³ãƒ•ç¢ºç‡å ´ï¼ˆMRFï¼‰**ï¼ˆç„¡å‘ã‚°ãƒ©ãƒ•ï¼‰:

$$
P(\mathbf{x}) = \frac{1}{Z} \prod_{c \in \mathcal{C}} \psi_c(\mathbf{x}_c)
$$

ã“ã“ã§ $\psi_c$ ã¯ã‚¯ãƒªãƒ¼ã‚¯ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã€$Z$ ã¯åˆ†é…é–¢æ•°ã€‚EBM [^7] ã¯MRFã®ä¸€èˆ¬åŒ–ã¨è¦‹ãªã›ã‚‹ã€‚

```python
import numpy as np

# Simple Bayesian Network: Rain â†’ Sprinkler, Rain â†’ WetGrass, Sprinkler â†’ WetGrass
# P(R), P(S|R), P(W|R,S)
P_R = np.array([0.8, 0.2])  # [no rain, rain]
P_S_given_R = np.array([[0.6, 0.4],   # S|no rain
                          [0.01, 0.99]]) # S|rain  (sprinkler OFF when raining)
P_W_given_RS = np.array([
    [[0.99, 0.01],  # W|no rain, no sprinkler
     [0.1, 0.9]],   # W|no rain, sprinkler ON
    [[0.2, 0.8],    # W|rain, no sprinkler
     [0.01, 0.99]]  # W|rain, sprinkler ON
])

# Compute joint P(R, S, W)
joint = np.zeros((2, 2, 2))
for r in range(2):
    for s in range(2):
        for w in range(2):
            joint[r, s, w] = P_R[r] * P_S_given_R[r, s] * P_W_given_RS[r, s, w]

print(f"Sum of joint (must be 1.0): {joint.sum():.6f}")

# Query: P(Rain | WetGrass=1) via Bayes
p_w1 = joint[:, :, 1].sum()  # P(W=1)
p_r_given_w1 = joint[:, :, 1].sum(axis=1) / p_w1  # P(R|W=1)
print(f"\nP(Rain | Wet Grass) = {p_r_given_w1[1]:.4f}")
print(f"P(No Rain | Wet Grass) = {p_r_given_w1[0]:.4f}")

# Explaining away: P(Rain | WetGrass=1, Sprinkler=1) vs P(Rain | WetGrass=1, Sprinkler=0)
p_r_given_w1_s1 = joint[:, 1, 1] / joint[:, 1, 1].sum()
p_r_given_w1_s0 = joint[:, 0, 1] / joint[:, 0, 1].sum()
print(f"\nExplaining away:")
print(f"P(Rain | Wet, Sprinkler ON)  = {p_r_given_w1_s1[1]:.4f}")
print(f"P(Rain | Wet, Sprinkler OFF) = {p_r_given_w1_s0[1]:.4f}")
print("â†’ If sprinkler explains wet grass, rain becomes less likely (explaining away)")
```

ç¢ºç‡çš„ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã¯ã€VAEã®ç”Ÿæˆéç¨‹ ($\mathbf{z} \to \mathbf{x}$) ã‚„HMMã€Diffusionã®ãƒãƒ«ã‚³ãƒ•é€£é–ã‚’çµ±ä¸€çš„ã«è¨˜è¿°ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã ã€‚ç¬¬8å›ï¼ˆEMç®—æ³•ï¼‰ã§GMMã®æ½œåœ¨å¤‰æ•°ã‚’æ‰±ã†ã¨ãã€ã“ã®æ çµ„ã¿ãŒæ´»ãã¦ãã‚‹ã€‚

### 3.11 å¤§æ•°ã®æ³•å‰‡ã¨ä¸­å¿ƒæ¥µé™å®šç†

#### å¤§æ•°ã®æ³•å‰‡ï¼ˆLLNï¼‰

**å®šç†ï¼ˆå¼±å¤§æ•°ã®æ³•å‰‡ï¼‰**: i.i.d.ç¢ºç‡å¤‰æ•° $X_1, X_2, \ldots$ ãŒ $\mathbb{E}[|X_1|] < \infty$ ã‚’æº€ãŸã™ã¨ã:

$$
\bar{X}_N = \frac{1}{N}\sum_{i=1}^{N} X_i \xrightarrow{P} \mathbb{E}[X_1] \quad (N \to \infty)
$$

**å®šç†ï¼ˆå¼·å¤§æ•°ã®æ³•å‰‡ï¼‰**: åŒã˜æ¡ä»¶ã®ä¸‹ã§:

$$
\bar{X}_N \xrightarrow{\text{a.s.}} \mathbb{E}[X_1] \quad (N \to \infty)
$$

ã€Œa.s.ã€ã¯ã€Œæ¦‚åæŸï¼ˆalmost surelyï¼‰ã€â€” ç¢ºç‡1ã§åæŸã™ã‚‹ã€‚ç¬¬5å›ã§æ¸¬åº¦è«–çš„ã«å³å¯†åŒ–ã™ã‚‹ã€‚

#### ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCLTï¼‰

**å®šç†ï¼ˆä¸­å¿ƒæ¥µé™å®šç†ï¼‰**: i.i.d.ç¢ºç‡å¤‰æ•° $X_1, X_2, \ldots$ ãŒ $\mathbb{E}[X_1] = \mu$, $\text{Var}(X_1) = \sigma^2 < \infty$ ã‚’æº€ãŸã™ã¨ã:

$$
\frac{\bar{X}_N - \mu}{\sigma / \sqrt{N}} \xrightarrow{d} \mathcal{N}(0, 1) \quad (N \to \infty)
$$

ç­‰ä¾¡çš„ã«æ›¸ãã¨: $\sqrt{N}(\bar{X}_N - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)$ã€‚åæŸé€Ÿåº¦ã¯ $O(1/\sqrt{N})$ â€” å®Ÿç”¨ä¸Šã€$N \geq 30$ ã§è¿‘ä¼¼ãŒååˆ†ã«åŠ¹ãï¼ˆBerry-Esseené™ç•Œã«ã‚ˆã‚‹å®šé‡çš„ä¿è¨¼ï¼‰ã€‚

:::details ğŸ“– Berry-Esseené™ç•Œã®è©³ç´°ï¼ˆç™ºå±•ï¼‰
CLTã®åæŸé€Ÿåº¦ã‚’å®šé‡åŒ–ã™ã‚‹Berry-Esseenä¸ç­‰å¼:

$$
\sup_x \left|P\left(\frac{\bar{X}_N - \mu}{\sigma/\sqrt{N}} \leq x\right) - \Phi(x)\right| \leq \frac{C \rho}{\sigma^3 \sqrt{N}}
$$

ã“ã“ã§ $\rho = \mathbb{E}[|X - \mu|^3]$ ã¯3æ¬¡çµ¶å¯¾ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã€$C$ ã¯çµ¶å¯¾å®šæ•°ï¼ˆ$C \leq 0.4748$ï¼‰ã€‚CLTãŒã€ŒåæŸã™ã‚‹ã€ã¨ã—ã‹è¨€ã‚ãªã„ã®ã«å¯¾ã—ã€Berry-Esseené™ç•Œã¯ã€Œã©ã‚Œã ã‘é€ŸãåæŸã™ã‚‹ã‹ã€ã‚’ $O(1/\sqrt{N})$ ã§å®šé‡åŒ–ã™ã‚‹ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯CLTã®çµè«–ã§ååˆ†ã ãŒã€åæŸã®é€Ÿã•ã‚’è­°è«–ã™ã‚‹å ´é¢ã§å‚ç…§ã•ã‚ŒãŸã„ã€‚
:::

```python
import numpy as np

def demonstrate_clt(dist_name: str, sampler, true_mean: float, true_var: float, N_values: list):
    """Demonstrate CLT for various distributions."""
    print(f"\n=== CLT for {dist_name} ===")
    print(f"True mean={true_mean:.2f}, True var={true_var:.2f}")
    print(f"{'N':>8} {'Sample mean':>12} {'Std of mean':>12} {'Ïƒ/âˆšN (theory)':>14} {'Normalized':>10}")
    print("-" * 60)

    n_experiments = 10000
    for N in N_values:
        means = np.array([sampler(N).mean() for _ in range(n_experiments)])
        sample_std = means.std()
        theory_std = np.sqrt(true_var / N)
        # Test normality of standardized means
        standardized = (means - true_mean) / theory_std
        print(f"{N:>8} {means.mean():>12.4f} {sample_std:>12.4f} {theory_std:>14.4f} "
              f"std={standardized.std():>5.3f}")

# Uniform[0,1]: Î¼=0.5, ÏƒÂ²=1/12
demonstrate_clt("Uniform[0,1]",
                lambda n: np.random.uniform(0, 1, n),
                0.5, 1/12, [1, 5, 30, 100, 1000])

# Exponential(Î»=2): Î¼=0.5, ÏƒÂ²=0.25
demonstrate_clt("Exponential(Î»=2)",
                lambda n: np.random.exponential(0.5, n),
                0.5, 0.25, [1, 5, 30, 100, 1000])

# Bernoulli(p=0.3): Î¼=0.3, ÏƒÂ²=0.21
demonstrate_clt("Bernoulli(p=0.3)",
                lambda n: np.random.binomial(1, 0.3, n).astype(float),
                0.3, 0.21, [1, 5, 30, 100, 1000])
```

**CLTãŒé‡è¦ãªç†ç”±**:

1. **MLEã®æ¼¸è¿‘æ­£è¦æ€§ã®æ ¹æ‹ **: MLEæ¨å®šé‡ã¯ååˆ†çµ±è¨ˆé‡ã®å¹³å‡ â†’ CLTã«ã‚ˆã‚Šæ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã
2. **ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãŒé »å‡ºã™ã‚‹ç†ç”±**: å¤šæ•°ã®å°ã•ãªåŠ¹æœã®å’Œ â†’ CLTã«ã‚ˆã‚Šè¿‘ä¼¼çš„ã«ã‚¬ã‚¦ã‚¹
3. **ä¿¡é ¼åŒºé–“ã®æ§‹æˆ**: $\bar{X}_N \pm z_{\alpha/2} \cdot \sigma/\sqrt{N}$ â€” CLTãŒæ­£å½“åŒ–

```mermaid
graph TD
    LLN["å¤§æ•°ã®æ³•å‰‡<br/>XÌ„_N â†’ Î¼<br/>(ä¸€è‡´æ€§)"]
    CLT["ä¸­å¿ƒæ¥µé™å®šç†<br/>âˆšN(XÌ„_N-Î¼) â†’ N(0,ÏƒÂ²)<br/>(åˆ†å¸ƒåæŸ)"]
    BE["Berry-Esseen<br/>åæŸé€Ÿåº¦ O(1/âˆšN)<br/>(å®šé‡çš„ä¿è¨¼)"]
    MLE["MLEæ¼¸è¿‘æ­£è¦æ€§<br/>âˆšN(Î¸Ì‚-Î¸*) â†’ N(0,Iâ»Â¹)<br/>(æ¨å®šã®ç²¾åº¦)"]
    CR["CramÃ©r-Raoä¸‹ç•Œ<br/>Var â‰¥ 1/(NI)<br/>(æ¨å®šã®é™ç•Œ)"]

    LLN --> CLT --> BE
    CLT --> MLE
    MLE --> CR
```

### 3.12 âš”ï¸ Boss Battle â€” è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å°¤åº¦ã‚’å®Œå…¨åˆ†è§£ã›ã‚ˆ

æº–å‚™ã¯ã„ã„ã‹ã€‚ã“ã“ã¾ã§ã®å…¨æ­¦å™¨ã‚’ä½¿ã£ã¦ã€LLMã®å­¦ç¿’ç›®æ¨™ã‚’ç¢ºç‡è«–ã®è¨€è‘‰ã§å®Œå…¨ã«è¨˜è¿°ã™ã‚‹ã€‚

**å•é¡Œ**: ãƒ†ã‚­ã‚¹ãƒˆ $\mathbf{x} = (x_1, x_2, \ldots, x_T)$ ã«å¯¾ã—ã¦ã€è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å¯¾æ•°å°¤åº¦ã‚’å°å‡ºã—ã€å„ã‚¹ãƒ†ãƒƒãƒ—ãŒCategoricalåˆ†å¸ƒã®MLEã«å¸°ç€ã™ã‚‹ã“ã¨ã‚’ç¤ºã›ã€‚

**è§£ç­”**:

**Step 1**: ç¢ºç‡ã®é€£é–è¦å‰‡ï¼ˆchain ruleï¼‰ã«ã‚ˆã‚Š:

$$
p_\theta(\mathbf{x}) = \prod_{t=1}^{T} p_\theta(x_t \mid x_{<t})
$$

ã“ã‚Œã¯ä½•ã®è¿‘ä¼¼ã‚‚ä»®å®šã‚‚ãªã„æ’ç­‰å¼ï¼ˆç¢ºç‡ã®å…¬ç†ã‹ã‚‰å°å‡ºå¯èƒ½ï¼‰ã€‚

**Step 2**: å¯¾æ•°ã‚’å–ã‚‹ã¨:

$$
\log p_\theta(\mathbf{x}) = \sum_{t=1}^{T} \log p_\theta(x_t \mid x_{<t})
$$

**Step 3**: å„ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ¢ãƒ‡ãƒ«ã¯Softmaxå‡ºåŠ› $\hat{\boldsymbol{\pi}}_t = \text{softmax}(f_\theta(x_{<t}))$ ã‚’è¨ˆç®—ã€‚$f_\theta$ ã¯Transformerãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€‚ã“ã‚Œã¯Categoricalåˆ†å¸ƒ:

$$
p_\theta(x_t = k \mid x_{<t}) = [\hat{\boldsymbol{\pi}}_t]_k = \frac{\exp([f_\theta(x_{<t})]_k)}{\sum_{j=1}^{V} \exp([f_\theta(x_{<t})]_j)}
$$

ã“ã“ã§ $V$ ã¯èªå½™ã‚µã‚¤ã‚ºã€‚

**Step 4**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\mathcal{D} = \{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(N)}\}$ ã«å¯¾ã™ã‚‹å¯¾æ•°å°¤åº¦:

$$
\mathcal{L}(\theta) = \frac{1}{N}\sum_{n=1}^{N} \sum_{t=1}^{T_n} \log p_\theta(x_t^{(n)} \mid x_{<t}^{(n)})
$$

**Step 5**: $-\mathcal{L}(\theta)$ ã‚’æœ€å°åŒ–ã™ã‚‹ã®ãŒCross-Entropy Lossï¼ˆç¬¬1å›ã§ä½“é¨“ã—ãŸå¼ï¼‰ã€‚

$$
\text{Cross-Entropy Loss} = -\frac{1}{N}\sum_{n=1}^{N} \frac{1}{T_n}\sum_{t=1}^{T_n} \log p_\theta(x_t^{(n)} \mid x_{<t}^{(n)})
$$

**ã“ã‚Œã¯ã¾ã•ã«ã€å„æ™‚åˆ» $t$ ã§ã®Categoricalåˆ†å¸ƒã«å¯¾ã™ã‚‹MLEã®è² ã®å¯¾æ•°å°¤åº¦ã«ä»–ãªã‚‰ãªã„ã€‚**

```python
import numpy as np

def autoregressive_nll(logits_sequence: list, targets: list) -> float:
    """Compute negative log-likelihood for autoregressive model.

    corresponds to: -L(Î¸) = -(1/T) Î£_t log p_Î¸(x_t | x_{<t})

    Each logits_sequence[t] is a vector of size V (vocabulary)
    Each targets[t] is the true token index
    """
    total_nll = 0.0
    T = len(targets)
    for t in range(T):
        logits = logits_sequence[t]
        # softmax â†’ Categorical distribution
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / exp_logits.sum()
        # negative log probability of correct token
        total_nll -= np.log(probs[targets[t]] + 1e-10)
    return total_nll / T

# Simulate a tiny autoregressive model
np.random.seed(42)
V = 10  # vocabulary size
T = 5   # sequence length

# Random logits (simulating Transformer output at each step)
logits_seq = [np.random.randn(V) for _ in range(T)]
targets = [3, 7, 1, 5, 2]

nll = autoregressive_nll(logits_seq, targets)
perplexity = np.exp(nll)
print(f"Autoregressive NLL: {nll:.4f}")
print(f"Perplexity:         {perplexity:.2f}")
print(f"Random baseline:    NLL={np.log(V):.4f}, PPL={V}")
print(f"\nInterpretation: The model is choosing from ~{perplexity:.0f} equally likely tokens")
print(f"(perfect model: PPL=1, random guess: PPL={V})")
```

> **Boss Battle æ”»ç•¥å®Œäº†**: è‡ªå·±å›å¸°LLMã®æå¤±é–¢æ•°ã¯ã€æ¡ä»¶ä»˜ãç¢ºç‡ã®é€£é–è¦å‰‡ + å„ã‚¹ãƒ†ãƒƒãƒ—ã®Categoricalåˆ†å¸ƒã®MLE + å¯¾æ•°å¤‰æ› â€” å…¨ã¦æœ¬è¬›ç¾©ã§å­¦ã‚“ã é“å…·ã ã‘ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã€‚

:::message
**é€²æ—: 50% å®Œäº†** ç¢ºç‡ç©ºé–“ã®å…¬ç†ã‹ã‚‰å§‹ã¾ã‚Šã€ãƒ™ã‚¤ã‚ºã®å®šç†ã€ä¸»è¦åˆ†å¸ƒã€å¤šå¤‰é‡æ­£è¦ã€æŒ‡æ•°å‹åˆ†å¸ƒæ—ã€MLEã€Fisheræƒ…å ±é‡ã€CLTã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ãŸã€‚Boss Battleã§ã¯è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å°¤åº¦ã‚’å®Œå…¨ã«åˆ†è§£ã—ãŸã€‚Zone 3 ã‚¯ãƒªã‚¢ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Bayes, T., Price, R. (1763). "An Essay towards solving a Problem in the Doctrine of Chances." *Philosophical Transactions of the Royal Society of London*, 53, 370-418.
@[card](https://doi.org/10.1098/rstl.1763.0053)

[^2]: Kingma, D.P., Welling, M. (2013). "Auto-Encoding Variational Bayes." *arXiv preprint*.
@[card](https://arxiv.org/abs/1312.6114)

[^3]: Hinton, G., Vinyals, O., Dean, J. (2015). "Distilling the Knowledge in a Neural Network." *arXiv preprint*.
@[card](https://arxiv.org/abs/1503.02531)

[^4]: Ho, J., Jain, A., Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS*.
@[card](https://arxiv.org/abs/2006.11239)

[^5]: Malach, E. (2023). "Auto-Regressive Next-Token Predictors are Universal Learners." *arXiv preprint*.
@[card](https://arxiv.org/abs/2309.06979)

[^6]: Kolmogorov, A.N. (1933). *Grundbegriffe der Wahrscheinlichkeitsrechnung*. Springer. English translation: *Foundations of the Theory of Probability* (1956).
@[card](https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations.pdf)
â€»å¤–éƒ¨å¤§å­¦PDFã®ãŸã‚ãƒªãƒ³ã‚¯åˆ‡ã‚Œã®å¯èƒ½æ€§ã‚ã‚Šï¼ˆãƒŸãƒ©ãƒ¼: [Internet Archive](https://archive.org/details/kolmogorov_202112) ã‚‚å‚ç…§ï¼‰

[^7]: LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., Huang, F.J. (2006). "A Tutorial on Energy-Based Learning." *Predicting Structured Data*, MIT Press.

[^8]: CramÃ©r, H. (1946). *Mathematical Methods of Statistics*. Princeton University Press. Rao, C.R. (1945). "Information and the Accuracy Attainable in the Estimation of Statistical Parameters." *Bulletin of the Calcutta Mathematical Society*, 37, 81-91.

[^9]: HyvÃ¤rinen, A. (2005). "Estimation of Non-Normalized Statistical Models by Score Matching." *Journal of Machine Learning Research*, 6, 695-709.

[^10]: Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B. (2020). "Score-Based Generative Modeling through Stochastic Differential Equations." *ICLR 2021 (Oral)*.
@[card](https://arxiv.org/abs/2011.13456)

[^11]: Rezende, D.J., Mohamed, S. (2015). "Variational Inference with Normalizing Flows." *ICML 2015*.
@[card](https://arxiv.org/abs/1505.05770)

[^12]: Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." *ICLR 2022*.
@[card](https://arxiv.org/abs/2106.09685)

[^27]: Wang, J., & Ramdas, A. (2024). False Discovery Control in Multiple Testing: A Brief Overview of Theories and Methodologies.
@[card](https://arxiv.org/abs/2411.10647)

[^28]: Xu, Z., & Ramdas, A. (2024). Online multiple testing with e-values. *JMLR*, 25(238), 1-40.

[^29]: Bates, S., Angelopoulos, A. N., Lei, L., Malik, J., & Jordan, M. I. (2023). Max-Rank: Efficient Multiple Testing for Conformal Prediction.
@[card](https://arxiv.org/abs/2311.10900)

### è£œéº â€” ç¾ä»£ã®å¤šé‡æ¤œå®šè£œæ­£æ‰‹æ³• (2023-2024)

:::message
**æ·±å±¤å­¦ç¿’æ™‚ä»£ã®çµ±è¨ˆçš„æ¤œå®š**: æ•°ä¸‡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŒæ™‚ãƒ†ã‚¹ãƒˆã™ã‚‹çŠ¶æ³ï¼ˆä¾‹: éºä¼å­ç™ºç¾è§£æã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®é‡ã¿æ¤œå®šï¼‰ã§ã¯ã€å¾“æ¥ã®Bonferroniè£œæ­£ã§ã¯ä¿å®ˆçš„ã™ãã‚‹ã€‚FDR (False Discovery Rate) åˆ¶å¾¡[^27]ã¨e-values[^28]ã«ã‚ˆã‚‹æœ€æ–°æ‰‹æ³•ã‚’ç´¹ä»‹ã€‚
:::

#### False Discovery Rate (FDR) åˆ¶å¾¡ã®åŸç†

**å®šç¾©**: æ£„å´ã—ãŸå¸°ç„¡ä»®èª¬ã®ã†ã¡ã€çœŸã«æ­£ã—ã„ï¼ˆType I errorï¼‰ã‚‚ã®ã®æœŸå¾…å‰²åˆ:

$$
\text{FDR} = \mathbb{E}\left[\frac{V}{R}\right], \quad V = \text{èª¤æ¤œå‡ºæ•°}, \quad R = \text{æ£„å´æ•°}
$$

**Benjamini-Hochberg (BH) æ‰‹é †** (1995):

1. $m$ å€‹ã®æ¤œå®šã® p å€¤ã‚’æ˜‡é †ã«ä¸¦ã¹æ›¿ãˆ: $p_{(1)} \leq p_{(2)} \leq \cdots \leq p_{(m)}$
2. æœ€å¤§ã® $k$ ã‚’è¦‹ã¤ã‘ã‚‹: $p_{(k)} \leq \frac{k}{m} \alpha$
3. å¸°ç„¡ä»®èª¬ $H_{(1)}, \ldots, H_{(k)}$ ã‚’æ£„å´

**ç†è«–ä¿è¨¼**: ç‹¬ç«‹ã¾ãŸã¯æ­£ã®ä¾å­˜æ€§ã®ä¸‹ã§ã€$\text{FDR} \leq \alpha$ã€‚

#### æœ€æ–°ã®ç™ºå±• (2024)

##### 1. e-values ã«ã‚ˆã‚‹ FDR åˆ¶å¾¡[^28]

**e-value** $E$: å¸°ç„¡ä»®èª¬ã®ä¸‹ã§ $\mathbb{E}[E] \leq 1$ ã‚’æº€ãŸã™éè² ç¢ºç‡å¤‰æ•°ã€‚p-value ã®ä»£æ›¿ã€‚

**e-BH æ‰‹é †**:

$$
k^* = \max \left\{ k : \frac{1}{k} \sum_{i=1}^k E_{(i)} \geq \frac{m}{k \alpha} \right\}
$$

ã“ã“ã§ $E_{(1)} \geq E_{(2)} \geq \cdots \geq E_{(m)}$ ã¯ e-values ã®é™é †ã€‚

**åˆ©ç‚¹**:
- ä»»æ„ã®ä¾å­˜æ€§ã«å¯¾ã—ã¦ FDR åˆ¶å¾¡
- ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¤œå®šï¼ˆé€æ¬¡çš„ä»®èª¬æ¤œå®šï¼‰ã«å¯¾å¿œ
- è¤‡æ•°ã®ç‹¬ç«‹ãªç ”ç©¶çµæœã®çµ±åˆãŒå®¹æ˜“ï¼ˆe-values ã¯ç©ã§çµåˆå¯èƒ½ï¼‰

##### 2. Conformal Prediction ã«ãŠã‘ã‚‹å¤šé‡æ¤œå®š[^29]

æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ã§ã€è¤‡æ•°ã®äºˆæ¸¬åŒºé–“ã‚’åŒæ™‚æ§‹ç¯‰ã™ã‚‹å ´åˆ:

$$
\mathcal{C}(\mathbf{x}) = \left\{ y : s(\mathbf{x}, y) \leq q_{1-\alpha} \right\}
$$

ã“ã“ã§ $s$ ã¯ã‚¹ã‚³ã‚¢é–¢æ•°ã€$q_{1-\alpha}$ ã¯ $(1-\alpha)$ åˆ†ä½ç‚¹ã€‚

**Max-Rank è£œæ­£**: $m$ å€‹ã®äºˆæ¸¬åŒºé–“ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã€åˆ†ä½ç‚¹ã‚’èª¿æ•´:

$$
q_{1-\alpha'} \quad \text{where} \quad \alpha' = \frac{\alpha}{m \cdot \text{rank}(s_{\text{new}})}
$$

**åŠ¹æœ**: é€šå¸¸ã®Bonferroniè£œæ­£ ($\alpha/m$) ã‚ˆã‚Š power ãŒé«˜ã„ï¼ˆç‰¹ã«ä¾å­˜æ€§ãŒå¼·ã„å ´åˆï¼‰ã€‚

#### å®Ÿè£…ä¾‹

```python
import numpy as np
from scipy.stats import norm

def benjamini_hochberg_fdr(p_values: np.ndarray, alpha: float = 0.05) -> np.ndarray:
    """BHæ‰‹é †ã«ã‚ˆã‚‹FDRåˆ¶å¾¡"""
    m = len(p_values)
    # p-valueã‚’æ˜‡é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_idx = np.argsort(p_values)
    sorted_p = p_values[sorted_idx]

    # BHé–¾å€¤: k/m * alpha
    bh_threshold = np.arange(1, m + 1) / m * alpha

    # æœ€å¤§ã®kã‚’è¦‹ã¤ã‘ã‚‹
    reject_idx = np.where(sorted_p <= bh_threshold)[0]

    if len(reject_idx) == 0:
        return np.zeros(m, dtype=bool)

    k_max = reject_idx[-1]

    # æ£„å´ã™ã‚‹ä»®èª¬ã®ãƒã‚¹ã‚¯
    reject_mask = np.zeros(m, dtype=bool)
    reject_mask[sorted_idx[:k_max + 1]] = True

    return reject_mask

# ä½¿ç”¨ä¾‹: 1000å€‹ã®æ¤œå®š
m = 1000
# çœŸã®å¸°ç„¡ä»®èª¬900å€‹ã€å¯¾ç«‹ä»®èª¬100å€‹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
p_null = np.random.uniform(0, 1, 900)  # H0ãŒçœŸ
p_alt = np.random.beta(0.5, 5, 100)    # H1ãŒçœŸï¼ˆå°ã•ã„på€¤ï¼‰
p_values = np.concatenate([p_null, p_alt])

# Bonferroniè£œæ­£
reject_bonf = p_values < 0.05 / m
print(f"Bonferroni: {reject_bonf.sum()}å€‹æ£„å´")  # ~5å€‹

# BH-FDR
reject_bh = benjamini_hochberg_fdr(p_values, alpha=0.05)
print(f"BH-FDR: {reject_bh.sum()}å€‹æ£„å´")  # ~80å€‹
```

#### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¤œå®š: LORD++ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

æ™‚ç³»åˆ—çš„ã«ä»®èª¬ãŒåˆ°ç€ã™ã‚‹å ´åˆï¼ˆä¾‹: A/Bãƒ†ã‚¹ãƒˆã®é€æ¬¡åˆ¤å®šï¼‰:

$$
\alpha_t = \gamma \alpha_{t-1} + (1 - \gamma) \frac{\alpha}{t}
$$

ã“ã“ã§ $\gamma \in (0, 1)$ ã¯æ¸›è¡°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚

**æ‰‹é †**:
1. $t$ ç•ªç›®ã®ä»®èª¬ã«å¯¾ã—ã€$p_t \leq \alpha_t$ ãªã‚‰æ£„å´
2. æ£„å´ã—ãŸå ´åˆã€æ¬¡ã®é–¾å€¤ã‚’æ›´æ–°

**ä¿è¨¼**: $\text{FDR} \leq \alpha$ ã‚’æ™‚é–“å…¨ä½“ã§ä¿æŒã€‚

### æ•™ç§‘æ›¸

- Bishop, C.M. (2006). *Pattern Recognition and Machine Learning*. Springer. [PDF available from Microsoft Research]
- Murphy, K.P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press. [Free online]
- Wasserman, L. (2004). *All of Statistics*. Springer.
- Casella, G., Berger, R.L. (2002). *Statistical Inference*. 2nd ed. Duxbury/Thomson.

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|
| $\Omega$ | æ¨™æœ¬ç©ºé–“ | 3.1 |
| $\mathcal{F}$ | Ïƒ-åŠ æ³•æ— | 3.1 |
| $P$ | ç¢ºç‡æ¸¬åº¦ | 3.1 |
| $X, Y, Z$ | ç¢ºç‡å¤‰æ•° | 3.2 |
| $\mathbb{E}[\cdot]$ | æœŸå¾…å€¤ | 3.2 |
| $\text{Var}(\cdot)$ | åˆ†æ•£ | 3.2 |
| $\text{Cov}(\cdot, \cdot)$ | å…±åˆ†æ•£ | 3.2 |
| $\boldsymbol{\mu}$ | å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« | 3.5 |
| $\boldsymbol{\Sigma}$ | å…±åˆ†æ•£è¡Œåˆ— | 3.5 |
| $\boldsymbol{\Lambda}$ | ç²¾åº¦è¡Œåˆ— $= \boldsymbol{\Sigma}^{-1}$ | 3.5 |
| $\boldsymbol{\eta}$ | è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 3.6 |
| $\mathbf{T}(\mathbf{x})$ | ååˆ†çµ±è¨ˆé‡ | 3.6 |
| $A(\boldsymbol{\eta})$ | å¯¾æ•°æ­£è¦åŒ–å®šæ•° | 3.6 |
| $\theta$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 3.7 |
| $\hat{\theta}$ | æ¨å®šé‡ | 3.7 |
| $I(\theta)$ | Fisheræƒ…å ±é‡ | 3.8 |
| $\mathcal{D}$ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | 3.3 |
| $\sim$ | ã€Œã«ã—ãŸãŒã†ã€ | å…¨èˆ¬ |
| $\propto$ | æ¯”ä¾‹ã™ã‚‹ | 3.3 |
| $\xrightarrow{P}$ | ç¢ºç‡åæŸ | 3.11 |
| $\xrightarrow{d}$ | åˆ†å¸ƒåæŸ | 3.11 |
| $\xrightarrow{\text{a.s.}}$ | æ¦‚åæŸ | 3.11 |
| $\overset{\text{i.i.d.}}{\sim}$ | ç‹¬ç«‹åŒåˆ†å¸ƒ | 3.2 |
| $\bar{X}_N$ | æ¨™æœ¬å¹³å‡ $\frac{1}{N}\sum X_i$ | 3.11 |
| $\ell(\theta)$ | å¯¾æ•°å°¤åº¦ | 3.7 |
| $s(\mathbf{x}; \theta)$ | ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_\theta \log p$ | 3.8 |
| $h(\mathbf{x})$ | åŸºåº•æ¸¬åº¦ | 3.6 |
| $\boldsymbol{\pi}$ | Categorical/Dirichletã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 1.1 |
| $\Gamma(\cdot)$ | ã‚¬ãƒ³ãƒé–¢æ•° | 3.4 |
| $\binom{n}{k}$ | äºŒé …ä¿‚æ•° | 3.4 |
| $|\boldsymbol{\Sigma}|$ | è¡Œåˆ—å¼ | 3.5 |
| $\delta(\cdot)$ | ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ã®ãƒ‡ãƒ«ã‚¿é–¢æ•° | 4.5 |

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
