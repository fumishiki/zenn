---
title: "ç¬¬23å›: Fine-tuning & PEFT: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å‰ç·¨ã€‘ç†è«–ç·¨"
slug: "ml-lecture-23-part1"
emoji: "ğŸ”§"
type: "tech"
topics: ["machinelearning", "deeplearning", "finetuning", "julia", "rust"]
published: true
---

# ç¬¬23å›: Fine-tuning & PEFT â€” å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã¯æœ¬å½“ã«å¿…è¦ã‹ï¼Ÿ

> **65Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’1æšã®GPUã§è¨“ç·´ã€‚QLoRAãŒç¤ºã—ãŸã®ã¯ã€Œæœ€é©åŒ–ã™ã¹ãã¯å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã¯ãªãã€ä½ãƒ©ãƒ³ã‚¯éƒ¨åˆ†ç©ºé–“ã€ã¨ã„ã†æ´å¯Ÿã ã£ãŸã€‚**

ç¬¬22å›ã§ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æœ€å‰ç·šã‚’è¦‹ãŸã€‚ã ãŒäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãã®ã¾ã¾ä½¿ã†ã ã‘ã§ã¯ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã‚¿ã‚¹ã‚¯ã§æœ€é«˜æ€§èƒ½ã¯å‡ºãªã„ã€‚Fine-tuningï¼ˆå¾®èª¿æ•´ï¼‰ãŒå¿…è¦ã ã€‚

å¾“æ¥ã®Fine-tuningã¯å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã€‚ã ãŒ175Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆGPT-3ç´šï¼‰ã‚’å…¨ã¦æ›´æ–°ã™ã‚‹ã«ã¯ã€AdamWã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®å‹¾é…ãƒ»ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã‚’å«ã‚ã¦**æ•°TB**ã®ãƒ¡ãƒ¢ãƒªãŒè¦ã‚‹ã€‚ã“ã‚Œã¯ç¾å®Ÿçš„ã§ãªã„ã€‚

2022å¹´ã€Microsoftã®Huã‚‰ãŒLoRA [^1] ã‚’ç™ºè¡¨ã—ãŸã€‚**å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®0.01%ã ã‘ã‚’è¨“ç·´**ã—ã¦GPT-3ã‚’Fine-tuningã—ã€Full Fine-tuningã¨åŒç­‰æ€§èƒ½ã‚’é”æˆã—ãŸã€‚2023å¹´ã€Dettmersã‚‰ã®QLoRA [^2] ã¯4-bité‡å­åŒ–ã¨çµ„ã¿åˆã‚ã›ã€65Bãƒ¢ãƒ‡ãƒ«ã‚’GPU 1æšï¼ˆ48GBï¼‰ã§è¨“ç·´å¯èƒ½ã«ã—ãŸã€‚

æœ¬è¬›ç¾©ã¯Course IIIã€Œå®Ÿè·µç·¨ã€ã®ä¸­æ ¸ â€” LoRA/QLoRA/DreamBooth/Adapterã®æ•°å¼ã¨å®Ÿè£…ã‚’å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹ã€‚ãã—ã¦**Julia LoRAè¨“ç·´ + Rust LoRAæ¨è«–**ã§3è¨€èªåˆ¶è¦‡ã®æ—…ã‚’ç¶šã‘ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph TD
    A["ğŸ“š äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«<br/>175B params"] --> B["âŒ Full FT<br/>175Bæ›´æ–°<br/>æ•°TB GPU"]
    A --> C["âœ… LoRA<br/>0.01%æ›´æ–°<br/>10kå€å‰Šæ¸›"]
    C --> D["ğŸ”§ QLoRA<br/>4-bité‡å­åŒ–<br/>GPU 1æšã§65B"]
    C --> E["ğŸ¨ DreamBooth<br/>3ç”»åƒã§å€‹äººåŒ–"]
    C --> F["ğŸ”Œ Adapter<br/>å„å±¤ã«è¿½åŠ "]
    style B fill:#ffcdd2
    style C fill:#c8e6c9
    style D fill:#b3e5fc
    style E fill:#f8bbd0
    style F fill:#ffe0b2
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 7 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” LoRAã‚’3è¡Œã§ä½“æ„Ÿ

**ã‚´ãƒ¼ãƒ«**: LoRAã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ $W_0$ ã‚’å›ºå®šã—ã€ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ $\Delta W = BA$ ã ã‘ã‚’è¨“ç·´ã™ã‚‹ã€‚

```julia
using LinearAlgebra

# Pretrained weight Wâ‚€ âˆˆ â„^(dÃ—k) (frozen)
d, k, r = 512, 512, 8  # d=å‡ºåŠ›dim, k=å…¥åŠ›dim, r=rank
W0 = randn(d, k) / sqrt(k)  # frozen pretrained weight

# LoRA: Î”W = BA, B âˆˆ â„^(dÃ—r), A âˆˆ â„^(rÃ—k)
B = randn(d, r) / sqrt(r)  # trainable
A = zeros(r, k)             # init to zero (Î”W starts at 0)

# Forward pass: h = (Wâ‚€ + Î”W)x = Wâ‚€x + BAx
x = randn(k)
h_full = (W0 + B * A) * x     # conceptual (full matrix)
h_lora = W0 * x + B * (A * x) # efficient (no Wâ‚€+BA materialization)

println("Wâ‚€ params: $(d*k) = $(d*k)")
println("LoRA params: $(d*r + r*k) = $(d*r + r*k)")
println("Reduction: $(round(d*k / (d*r + r*k), digits=1))x")
println("Output identical: $(isapprox(h_full, h_lora))")
```

å‡ºåŠ›:
```
Wâ‚€ params: 262144
LoRA params: 8192
Reduction: 32.0x
Output identical: true
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’32åˆ†ã®1ã«å‰Šæ¸›ã—ãŸã€‚** å®Ÿéš›ã®GPT-3 (175B) ã§ã¯ã€LoRAã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’**10,000å€å‰Šæ¸›** [^1]ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å¼:

$$
h = W_0 x + \Delta W x = W_0 x + BA x, \quad W_0 \in \mathbb{R}^{d \times k}, \, B \in \mathbb{R}^{d \times r}, \, A \in \mathbb{R}^{r \times k}
$$

- $W_0$: äº‹å‰å­¦ç¿’é‡ã¿ï¼ˆ**frozen**ï¼‰
- $B, A$: ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ï¼ˆ**trainable**ï¼‰
- $r \ll \min(d, k)$: ãƒ©ãƒ³ã‚¯ï¼ˆå…¸å‹å€¤ 4-64ï¼‰

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: $dk$ (Full FT) â†’ $dr + rk \approx r(d+k)$ (LoRA)ã€‚$r=8, d=k=512$ ãªã‚‰å‰Šæ¸›ç‡ $\frac{512^2}{8 \cdot 1024} = 32$å€ã€‚

:::message
**é€²æ—: 3% å®Œäº†** LoRAã®åŸºæœ¬æ§‹é€ ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰æ•°å¼ãƒ»å®Ÿè£…ãƒ»QLoRAãƒ»DreamBoothãƒ»Adapterã¨æ·±æ˜ã‚Šã—ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Fine-tuningã®4ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è§¦ã‚‹

### 1.1 Fine-tuningã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³

Fine-tuningã«ã¯4ã¤ã®åŸºæœ¬æˆ¦ç•¥ãŒã‚ã‚‹ã€‚

| æˆ¦ç•¥ | æ›´æ–°å¯¾è±¡ | ç”¨é€” | ãƒ¡ãƒ¢ãƒª | æ€§èƒ½ |
|:-----|:---------|:-----|:-------|:-----|
| **Full Fine-tuning** | å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã€ãƒ‡ãƒ¼ã‚¿è±Šå¯Œ | æ¥µå¤§ | æœ€é«˜ |
| **Feature Extraction** | æœ€çµ‚å±¤ã®ã¿ | ãƒ‡ãƒ¼ã‚¿å°‘ã€è¨ˆç®—åˆ¶ç´„ | æœ€å° | ä¸­ |
| **Partial Fine-tuning** | ä¸Šä½Nå±¤ | ä¸­é–“ãƒãƒ©ãƒ³ã‚¹ | ä¸­ | ä¸­-é«˜ |
| **PEFT (LoRAç­‰)** | è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ | å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ä¸­ç¨‹åº¦ | å° | é«˜ |

Transformerã‚’ä¾‹ã«å„æˆ¦ç•¥ã®å¼ã‚’æ›¸ãã€‚

#### 1.1.1 Full Fine-tuning

å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’æ›´æ–°:

$$
\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}(\theta)
$$

**åˆ©ç‚¹**: ã‚¿ã‚¹ã‚¯ã¸ã®å®Œå…¨é©å¿œã€‚**æ¬ ç‚¹**: GPT-3 (175B) ãªã‚‰ã€AdamWçŠ¶æ…‹è¾¼ã¿ã§æ•°TBã€‚

#### 1.1.2 Feature Extraction

Transformeræœ€çµ‚å±¤ï¼ˆåˆ†é¡ãƒ˜ãƒƒãƒ‰ï¼‰ã®ã¿è¨“ç·´ã€æ®‹ã‚Šã¯å›ºå®š:

$$
\begin{aligned}
h_L &= \text{Transformer}_{\theta_\text{frozen}}(x) \\
y &= W_\text{cls} h_L + b_\text{cls} \quad \text{(only } W_\text{cls}, b_\text{cls} \text{ trainable)}
\end{aligned}
$$

**åˆ©ç‚¹**: æœ€å°ãƒ¡ãƒ¢ãƒªã€‚**æ¬ ç‚¹**: ã‚¿ã‚¹ã‚¯é©å¿œãŒæµ…ã„ã€‚

#### 1.1.3 Partial Fine-tuning

ä¸Šä½Nå±¤ã ã‘æ›´æ–°:

$$
\begin{aligned}
h_{\text{frozen}} &= \text{Transformer}_{\theta_1, \dots, \theta_M}(x) \quad \text{(frozen)} \\
h_{\text{tuned}} &= \text{Transformer}_{\theta_{M+1}, \dots, \theta_L}(h_{\text{frozen}}) \quad \text{(trainable)}
\end{aligned}
$$

**åˆ©ç‚¹**: Full FTã®80-90%æ€§èƒ½ã‚’ãƒ¡ãƒ¢ãƒª50%ã§ã€‚**æ¬ ç‚¹**: ã©ã®Nå±¤ã‚’é¸ã¶ã‹ä¸æ˜ç­ã€‚

#### 1.1.4 PEFT (LoRA)

å…ƒã®é‡ã¿ã‚’å›ºå®šã€è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\Delta W = BA$ ã‚’è¨“ç·´:

$$
h = W_0 x + \underbrace{BA}_{\Delta W} x, \quad W_0 \text{ frozen}, \, B, A \text{ trainable}
$$

**åˆ©ç‚¹**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿0.01-1%ã€æ€§èƒ½â‰ˆFull FTã€‚**æ¬ ç‚¹**: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $r, \alpha$ èª¿æ•´ãŒå¿…è¦ã€‚

å„æˆ¦ç•¥ã‚’å®Ÿè£…ã§æ¯”è¼ƒã™ã‚‹ã€‚

```julia
using Flux

# Simple Transformer layer (simplified)
struct TransformerLayer
    W_q::Matrix{Float32}
    W_k::Matrix{Float32}
    W_v::Matrix{Float32}
    W_o::Matrix{Float32}
end

function (layer::TransformerLayer)(x::Matrix{Float32})
    # Simplified: just linear projections (no actual attention for brevity)
    Q = layer.W_q * x
    K = layer.W_k * x
    V = layer.W_v * x
    O = layer.W_o * V  # simplified output
    return O
end

# Strategy 1: Full Fine-tuning
function full_finetune(layers, x, y_true, lr)
    # Update all parameters
    for layer in layers
        # Compute gradients (simplified)
        âˆ‡W_q = randn(size(layer.W_q)) * 0.01  # placeholder
        layer.W_q .-= lr * âˆ‡W_q
        # ... (similarly for W_k, W_v, W_o)
    end
end

# Strategy 2: Feature Extraction
function feature_extraction(layers_frozen, W_cls, x, y_true, lr)
    # Forward through frozen layers
    h = x
    for layer in layers_frozen
        h = layer(h)
    end
    # Train only classification head
    y_pred = W_cls * h
    âˆ‡W_cls = randn(size(W_cls)) * 0.01  # placeholder gradient
    W_cls .-= lr * âˆ‡W_cls
    return W_cls
end

# Strategy 3: Partial Fine-tuning
function partial_finetune(layers_frozen, layers_tuned, x, y_true, lr)
    # Forward frozen
    h = x
    for layer in layers_frozen
        h = layer(h)
    end
    # Forward + backward tuned layers
    for layer in layers_tuned
        h = layer(h)
        # Update layer params (simplified)
    end
end

# Strategy 4: LoRA
struct LoRALayer
    W0::Matrix{Float32}  # frozen
    B::Matrix{Float32}   # trainable
    A::Matrix{Float32}   # trainable
    Î±::Float32
    r::Int
end

function (lora::LoRALayer)(x::Vector{Float32})
    # h = Wâ‚€x + (Î±/r)BAx
    scaling = lora.Î± / lora.r
    return lora.W0 * x + scaling * (lora.B * (lora.A * x))
end

function train_lora(lora::LoRALayer, x, y_true, lr)
    # Compute gradients w.r.t. B, A only (Wâ‚€ frozen)
    # ... (simplified)
    âˆ‡B = randn(size(lora.B)) * 0.01
    âˆ‡A = randn(size(lora.A)) * 0.01
    lora.B .-= lr * âˆ‡B
    lora.A .-= lr * âˆ‡A
    return lora
end

println("4 Fine-tuning strategies demonstrated")
println("Strategy 4 (LoRA) updates 0.01-1% params with â‰ˆFull FT performance")
```

### 1.2 Catastrophic Forgetting â€” Fine-tuningã®æš—é»’é¢

Fine-tuningã«ã¯**Catastrophic Forgetting**ï¼ˆç ´å£Šçš„å¿˜å´ï¼‰ã¨ã„ã†å•é¡ŒãŒã‚ã‚‹ [^3]ã€‚æ–°ã‚¿ã‚¹ã‚¯ã«é©å¿œã™ã‚‹ã¨ã€å…ƒã®èƒ½åŠ›ã‚’å¤±ã†ç¾è±¡ã ã€‚

$$
\mathcal{L}_\text{total} = \mathcal{L}_\text{new task}(x_\text{new}, y_\text{new}; \theta) + \lambda \mathcal{L}_\text{old task}(x_\text{old}, y_\text{old}; \theta)
$$

å³è¾ºç¬¬2é …ãŒãªã„ã¨ã€$\theta$ ã¯æ–°ã‚¿ã‚¹ã‚¯ã«éé©åˆã—ã€æ—§ã‚¿ã‚¹ã‚¯æ€§èƒ½ãŒå´©å£Šã™ã‚‹ã€‚

| å•é¡Œ | åŸå›  | å¯¾ç­– |
|:-----|:-----|:-----|
| **Catastrophic Forgetting** | æ–°ã‚¿ã‚¹ã‚¯ã®å‹¾é…ãŒæ—§çŸ¥è­˜ã‚’ä¸Šæ›¸ã | Elastic Weight Consolidation (EWC), å¤šã‚¿ã‚¹ã‚¯å­¦ç¿’, LoRA (å…ƒé‡ã¿ã‚’ä¿è­·) |
| **Mode Collapse (Fine-tuningç‰ˆ)** | æ–°ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒç‹­ã„ | ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ, Regularization |
| **Overfitting** | Fine-tuningãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ | Early Stopping, Dropout, LoRAã®ãƒ©ãƒ³ã‚¯å‰Šæ¸› |

**LoRAã®å‰¯æ¬¡çš„åˆ©ç‚¹**: $W_0$ ã‚’å›ºå®šã™ã‚‹ãŸã‚ã€å…ƒçŸ¥è­˜ãŒä¿è­·ã•ã‚Œã‚‹ã€‚è¤‡æ•°ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦è¤‡æ•°ã® $(B, A)$ ãƒšã‚¢ã‚’ä¿æŒã—ã€æ¨è«–æ™‚ã«åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ã€‚

```julia
# Multi-task LoRA: è¤‡æ•°ã® (B, A) ãƒšã‚¢ã‚’ä¿æŒ
struct MultiTaskLoRA
    W0::Matrix{Float32}      # shared frozen weight
    tasks::Dict{String, Tuple{Matrix{Float32}, Matrix{Float32}}}  # task_name => (B, A)
    Î±::Float32
    r::Int
end

function forward(lora::MultiTaskLoRA, x::Vector{Float32}, task_name::String)
    B, A = lora.tasks[task_name]
    scaling = lora.Î± / lora.r
    return lora.W0 * x + scaling * (B * (A * x))
end

# Example: 3 tasks, shared Wâ‚€, separate LoRA adapters
d, k, r = 512, 512, 8
W0 = randn(Float32, d, k) / sqrt(k)
tasks = Dict(
    "summarization" => (randn(Float32, d, r) / sqrt(r), zeros(Float32, r, k)),
    "translation"   => (randn(Float32, d, r) / sqrt(r), zeros(Float32, r, k)),
    "qa"            => (randn(Float32, d, r) / sqrt(r), zeros(Float32, r, k))
)
multi_lora = MultiTaskLoRA(W0, tasks, 16.0f0, r)

# Inference: switch task by changing adapter
x = randn(Float32, k)
h_sum = forward(multi_lora, x, "summarization")
h_qa  = forward(multi_lora, x, "qa")

println("Multi-task LoRA: $(length(tasks)) tasks share Wâ‚€, each has own (B,A)")
println("Total params: Wâ‚€=$(d*k) + $(length(tasks))Ã—LoRA=$(length(tasks)*(d*r + r*k))")
```

**ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: Full FT ã§3ã‚¿ã‚¹ã‚¯åˆ†è¨“ç·´ã™ã‚‹ã¨ $3 \times dk$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚Multi-task LoRAã¯ $dk + 3(dr + rk)$ã€‚$r=8, d=k=512$ ãªã‚‰å‰Šæ¸›ç‡ $\frac{3 \times 512^2}{512^2 + 3 \times 8192} \approx 24$å€ã€‚

:::message
**é€²æ—: 10% å®Œäº†** Full FT / Feature Extraction / Partial FT / LoRA ã®4æˆ¦ç•¥ã‚’è§¦ã£ãŸã€‚Catastrophic Forgettingã®å•é¡Œã¨ã€LoRAã«ã‚ˆã‚‹è¤‡æ•°ã‚¿ã‚¹ã‚¯ä¿æŒã®ä»•çµ„ã¿ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯ã€ŒãªãœFine-tuningãŒå¿…è¦ã‹ã€ã®ç›´æ„Ÿã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœäº‹å‰å­¦ç¿’ã ã‘ã§ã¯è¶³ã‚Šãªã„ã®ã‹

### 2.1 äº‹å‰å­¦ç¿’ vs Fine-tuning â€” 2æ®µéšå­¦ç¿’ã®å¿…ç„¶æ€§

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¯2æ®µéšã§è¨“ç·´ã•ã‚Œã‚‹:

1. **äº‹å‰å­¦ç¿’ï¼ˆPre-trainingï¼‰**: å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆæ•°TBï¼‰ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬
2. **Fine-tuning**: ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆæ•°GBä»¥ä¸‹ï¼‰ã§é©å¿œ

ãªãœ1æ®µéšã§ã¯ãƒ€ãƒ¡ãªã®ã‹ï¼Ÿ

| ç†ç”± | èª¬æ˜ | ä¾‹ |
|:-----|:-----|:---|
| **æ±ç”¨çŸ¥è­˜ vs å°‚é–€çŸ¥è­˜** | äº‹å‰å­¦ç¿’=æ±ç”¨ã€Fine-tuning=å°‚é–€ | GPT-3ã¯è‹±èªå…¨èˆ¬ã‚’å­¦ã¶ã€‚åŒ»ç™‚è¨ºæ–­ã«ã¯MedQAã§Fine-tuningãŒå¿…è¦ |
| **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡** | å¤§é‡æ±ç”¨ãƒ‡ãƒ¼ã‚¿â†’å°é‡å°‚é–€ãƒ‡ãƒ¼ã‚¿ã¸ã®è»¢ç§» | ImageNetäº‹å‰å­¦ç¿’å¾Œã€100ç”»åƒã§Xç·šè¨ºæ–­ã«é©å¿œ |
| **è¨ˆç®—ã‚³ã‚¹ãƒˆ** | äº‹å‰å­¦ç¿’ã¯1å›ã€Fine-tuningã¯å¤šæ•°ã‚¿ã‚¹ã‚¯ã§ç¹°ã‚Šè¿”ã— | GPT-3äº‹å‰å­¦ç¿’=$5Mã€Fine-tuning=$100-1000 |
| **åˆ†å¸ƒã‚·ãƒ•ãƒˆ** | äº‹å‰å­¦ç¿’ã‚³ãƒ¼ãƒ‘ã‚¹ â‰  ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ | GPT-3ã¯æ›¸ç±ãƒ»Webå­¦ç¿’ã€‚æ³•å¾‹æ–‡æ›¸ã‚¹ã‚¿ã‚¤ãƒ«ã¯åˆ¥é€”å­¦ç¿’ãŒå¿…è¦ |

æ•°å¼ã§è¡¨ã™ã¨ã€äº‹å‰å­¦ç¿’ã¯**å‘¨è¾ºå°¤åº¦** $p(x)$ ã®æœ€å¤§åŒ–:

$$
\theta_\text{pretrain} = \arg\max_\theta \mathbb{E}_{x \sim p_\text{data}}[\log p_\theta(x)]
$$

Fine-tuningã¯**æ¡ä»¶ä»˜ãå°¤åº¦** $p(y|x)$ ã®æœ€å¤§åŒ–ï¼ˆã‚¿ã‚¹ã‚¯ç‰¹åŒ–ï¼‰:

$$
\theta_\text{finetune} = \arg\max_\theta \mathbb{E}_{(x,y) \sim p_\text{task}}[\log p_\theta(y|x)]
$$

åˆæœŸå€¤ $\theta_0 = \theta_\text{pretrain}$ ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã™ã‚‹ã“ã¨ã§ã€ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã‚ˆã‚Šé¥ã‹ã«é€ŸãåæŸã™ã‚‹ã€‚

### 2.2 Transfer Learningã®3ã¤ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ 

Fine-tuningã¯Transfer Learningã®ä¸€ç¨®ã€‚Computer Visionã§ç¢ºç«‹ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ãŒNLPã«ã‚‚é©ç”¨ã•ã‚ŒãŸã€‚

```mermaid
graph TD
    A["ImageNet<br/>äº‹å‰å­¦ç¿’"] --> B["CNNç‰¹å¾´æŠ½å‡ºå™¨"]
    B --> C1["åˆ†é¡ãƒ˜ãƒƒãƒ‰<br/>(çŠ¬vsçŒ«)"]
    B --> C2["åˆ†é¡ãƒ˜ãƒƒãƒ‰<br/>(è‚ºç‚æ¤œå‡º)"]
    B --> C3["åˆ†é¡ãƒ˜ãƒƒãƒ‰<br/>(è‡ªå‹•é‹è»¢)"]

    D["GPT-3<br/>äº‹å‰å­¦ç¿’"] --> E["Transformer<br/>è¡¨ç¾å­¦ç¿’"]
    E --> F1["è¦ç´„"]
    E --> F2["ç¿»è¨³"]
    E --> F3["QA"]

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style B fill:#c8e6c9
    style E fill:#c8e6c9
```

3ã¤ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ :

1. **Feature Extraction**: äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å›ºå®šã€æœ€çµ‚å±¤ã ã‘è¨“ç·´
2. **Fine-tuning**: å…¨å±¤ã‚’ä½å­¦ç¿’ç‡ã§å¾®èª¿æ•´
3. **PEFT**: LoRAç­‰ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿è¨“ç·´ï¼ˆ2022å¹´ä»¥é™ã®ä¸»æµï¼‰

ç¬¬6å›ã§å­¦ã‚“ã KL divergenceã§èª¬æ˜ã™ã‚‹ã¨ã€Fine-tuningã¯**äº‹å‰å­¦ç¿’åˆ†å¸ƒ $p_\theta$ ã‚’ã‚¿ã‚¹ã‚¯åˆ†å¸ƒ $q_\text{task}$ ã«è¿‘ã¥ã‘ã‚‹**æ“ä½œ:

$$
\theta_\text{ft} = \arg\min_\theta D_\text{KL}(q_\text{task} \| p_\theta) = \arg\max_\theta \mathbb{E}_{x \sim q_\text{task}}[\log p_\theta(x)]
$$

### 2.3 æœ¬è¬›ç¾©ã®ä½ç½®ã¥ã‘ â€” Course IIIã®ä¸­æ ¸

Course IIIã¯ã€Œå®Ÿè·µç·¨ã€ â€” ç¬¬17-24å›ã§å®Ÿè£…ãƒ»æœ€é©åŒ–ãƒ»è©•ä¾¡ã‚’å­¦ã¶ã€‚

| ç¬¬17å› | ç¬¬18å› | ç¬¬19å› | ç¬¬20å› | ç¬¬21å› | ç¬¬22å› | **ç¬¬23å›** | ç¬¬24å› |
|:-------|:-------|:-------|:-------|:-------|:-------|:----------|:-------|
| MoE | Hybrid | Elixir | Tokenizer | Audio | Multi-modal | **Fine-tuning** | çµ±è¨ˆå­¦ |

ç¬¬17-22å›ã§**ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ**ã‚’å­¦ã‚“ã ã€‚ç¬¬23å›ã¯**ãƒ¢ãƒ‡ãƒ«é©å¿œ**ã€‚ç¬¬24å›ã§**è©•ä¾¡ã®æ•°å­¦**ã‚’å­¦ã³ã€Course IIIã‚’å®Œäº†ã™ã‚‹ã€‚

**Course I/II/IIIã®æ¥ç¶š**:

- **Course I (ç¬¬1-8å›)**: æ•°å­¦åŸºç¤ â€” ç·šå½¢ä»£æ•°ãƒ»ç¢ºç‡è«–ãƒ»æƒ…å ±ç†è«–ãƒ»EMç®—æ³•
- **Course II (ç¬¬9-16å›)**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«– â€” VAEãƒ»GANãƒ»OTãƒ»Transformerãƒ»Autoregressive
- **Course III (ç¬¬17-24å›)**: å®Ÿè·µç·¨ â€” MoEãƒ»Hybridãƒ»Fine-tuningãƒ»è©•ä¾¡ â† **ä»Šã“ã“**

ç¬¬23å›ã§ä½¿ã†æ•°å­¦:

| æ¦‚å¿µ | åˆå‡º | æœ¬è¬›ç¾©ã§ã®å½¹å‰² |
|:-----|:-----|:-------------|
| **SVD (ç‰¹ç•°å€¤åˆ†è§£)** | ç¬¬3å› | LoRAã®ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®ç†è«–çš„åŸºç›¤ |
| **MLE (æœ€å°¤æ¨å®š)** | ç¬¬7å› | Fine-tuningã®ç›®çš„é–¢æ•° |
| **KL divergence** | ç¬¬6å› | åˆ†å¸ƒé–“ã®è·é›¢ã€Fine-tuningã®æœ¬è³ª |
| **Gradient Descent** | ç¬¬6å› | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° |
| **æ­£å‰‡åŒ–** | ç¬¬8å› | Catastrophic Forgettingå¯¾ç­– |

### 2.4 æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®å·®åˆ¥åŒ–

æ¾å°¾ç ”ã®DLè¬›ç¾©ï¼ˆ2023å¹´ç‰ˆï¼‰ã¯ã€ŒFine-tuningã®æ¦‚å¿µã€ã‚’1ã‚¹ãƒ©ã‚¤ãƒ‰ã§è§¦ã‚Œã‚‹ã®ã¿ã€‚æœ¬è¬›ç¾©ã¯:

| é …ç›® | æ¾å°¾ç ” | æœ¬è¬›ç¾© |
|:-----|:-------|:-------|
| LoRAæ•°å¼å°å‡º | ãªã— | å®Œå…¨å°å‡ºï¼ˆä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼â†’åˆæœŸåŒ–â†’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰ |
| QLoRAå®Ÿè£… | ãªã— | 4-bit NFé‡å­åŒ–ã®æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ |
| DreamBooth | ãªã— | Prior Preservation Lossã®å®Œå…¨åˆ†è§£ |
| 3è¨€èªå®Ÿè£… | Pythonã®ã¿ | âš¡Juliaè¨“ç·´ + ğŸ¦€Rustæ¨è«– |
| è¡Œæ•° | ~10è¡Œ | ~3000è¡Œï¼ˆæœ¬è¬›ç¾©ç›®æ¨™ï¼‰ |

### 2.5 3ã¤ã®æ¯”å–©ã§æ‰ãˆã‚‹ã€ŒFine-tuningã€

1. **è¨€èªå­¦ç¿’**: äº‹å‰å­¦ç¿’=åŸºç¤æ–‡æ³•ã€Fine-tuning=å°‚é–€ç”¨èªç¿’å¾—
2. **å·¥å…·**: äº‹å‰å­¦ç¿’=æ±ç”¨å·¥å…·ã€Fine-tuning=ç”¨é€”ç‰¹åŒ–ã‚¢ã‚¿ãƒƒãƒãƒ¡ãƒ³ãƒˆ
3. **æ¥½å™¨**: äº‹å‰å­¦ç¿’=åŸºç¤ç·´ç¿’ã€Fine-tuning=æ›²ã”ã¨ã®è§£é‡ˆ

LoRAã®æ¯”å–©: **æ±ç”¨å·¥å…·ã®åˆƒã‚’ç ”ãç›´ã™ã®ã§ã¯ãªãã€ä»˜ã‘æ›¿ãˆå¯èƒ½ãªå°‚ç”¨åˆƒã‚’è¿½åŠ ã™ã‚‹**ã€‚

:::message
**é€²æ—: 20% å®Œäº†** ãªãœFine-tuningãŒå¿…è¦ã‹ã€äº‹å‰å­¦ç¿’ã¨ã®é•ã„ã€Transfer Learningã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ â€” LoRA/QLoRA/DreamBooth/Adapterã®å®Œå…¨å°å‡ºã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” LoRA/QLoRA/DreamBooth/Adapterã®å®Œå…¨å°å‡º

**ã‚´ãƒ¼ãƒ«**: LoRA, QLoRA, DreamBooth, Adapterç³»æ‰‹æ³•ã®æ•°å¼ã‚’ä¸€è¡Œãšã¤å°å‡ºã—ã€å®Œå…¨ã«ç†è§£ã™ã‚‹ã€‚

### 3.1 LoRAç†è«– â€” ä½ãƒ©ãƒ³ã‚¯é©å¿œã®æ•°å­¦

#### 3.1.1 å‹•æ©Ÿ: Full Fine-tuningã®ãƒ¡ãƒ¢ãƒªå•é¡Œ

GPT-3 (175B params) ã‚’Full Fine-tuningã™ã‚‹å ´åˆã€AdamWã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¯:

- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$: 175B Ã— 4 bytes (FP32) = 700 GB
- å‹¾é… $\nabla \theta$: 175B Ã— 4 bytes = 700 GB
- ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ $m$: 175B Ã— 4 bytes = 700 GB
- 2æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ $v$: 175B Ã— 4 bytes = 700 GB

åˆè¨ˆ **2.8 TB**ã€‚A100 GPU (80GB) ãªã‚‰35æšå¿…è¦ã€‚ç¾å®Ÿçš„ã§ãªã„ã€‚

**LoRAã®æ´å¯Ÿ** [^1]: äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã®**å¤‰åŒ– $\Delta W$ ã¯ä½ãƒ©ãƒ³ã‚¯**ã§ã‚ã‚‹ã€‚ã¤ã¾ã‚Šã€

$$
\text{rank}(\Delta W) \ll \min(d, k)
$$

ç†ç”±: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¯éå‰°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚Œã¦ãŠã‚Šã€Fine-tuningæ™‚ã®é©å¿œã¯ä½æ¬¡å…ƒéƒ¨åˆ†ç©ºé–“ã§ååˆ†ã€‚

#### 3.1.2 LoRAã®å®šå¼åŒ–

é‡ã¿è¡Œåˆ— $W \in \mathbb{R}^{d \times k}$ (ä¾‹: Transformer ã® $W_q, W_k, W_v, W_o$) ã‚’è€ƒãˆã‚‹ã€‚Full Fine-tuningã¯:

$$
W \leftarrow W_0 + \Delta W
$$

LoRAã¯ $\Delta W$ ã‚’ä½ãƒ©ãƒ³ã‚¯åˆ†è§£:

$$
\Delta W = BA, \quad B \in \mathbb{R}^{d \times r}, \, A \in \mathbb{R}^{r \times k}, \quad r \ll \min(d, k)
$$

Forward pass:

$$
h = Wx = (W_0 + \Delta W)x = W_0 x + BA x
$$

**é‡è¦**: $W_0$ ã¯**frozen**ï¼ˆå‹¾é…è¨ˆç®—ã—ãªã„ï¼‰ã€$B, A$ ã®ã¿**trainable**ã€‚

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¯”è¼ƒ:

| æ–¹å¼ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | å‰Šæ¸›ç‡ï¼ˆ$r=8, d=k=4096$ï¼‰ |
|:-----|:-------------|:-------------------------|
| Full FT | $dk$ | 16,777,216 |
| LoRA | $dr + rk = r(d+k)$ | 65,536 |
| å‰Šæ¸›ç‡ | $\frac{dk}{r(d+k)}$ | **256x** |

GPT-3 (d=12288, k=12288, r=4) ãªã‚‰å‰Šæ¸›ç‡ **6,144å€**ã€‚

#### 3.1.3 åˆæœŸåŒ–ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

LoRAã®åˆæœŸåŒ–ã¯ç‰¹æ®Š:

$$
\begin{aligned}
A &\sim \mathcal{N}(0, \sigma^2), \quad \sigma = \frac{1}{\sqrt{k}} \\
B &= \mathbf{0}
\end{aligned}
$$

$B=0$ ã«ã‚ˆã‚Šã€è¨“ç·´é–‹å§‹æ™‚ $\Delta W = BA = 0$ã€‚ã¤ã¾ã‚Š $W = W_0 + 0 = W_0$ ã§å…ƒã®äº‹å‰å­¦ç¿’é‡ã¿ã‹ã‚‰é–‹å§‹ã€‚

Forwardæ™‚ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°:

$$
h = W_0 x + \frac{\alpha}{r} BA x
$$

$\alpha$: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®šæ•°ï¼ˆå…¸å‹å€¤ 8-32ï¼‰ã€‚$\alpha/r$ ã«ã‚ˆã‚Šã€ãƒ©ãƒ³ã‚¯ $r$ ã‚’å¤‰ãˆã¦ã‚‚å­¦ç¿’ç‡ã‚’èª¿æ•´ä¸è¦ã«ã™ã‚‹æ­£å‰‡åŒ–ã€‚

**ç†è«–çš„æ ¹æ‹ ** [^1]:

$$
\mathbb{E}[\|BA x\|^2] \propto r \|x\|^2
$$

$\alpha/r$ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚Š $\mathbb{E}[\|\frac{\alpha}{r} BA x\|^2] \propto \alpha^2 / r \|x\|^2$ã€$r$ ã®å½±éŸ¿ã‚’ç›¸æ®ºã€‚

#### 3.1.4 LoRAã®å‹¾é…è¨ˆç®—

æå¤±é–¢æ•° $\mathcal{L}$ ã«å¯¾ã™ã‚‹å‹¾é…ï¼ˆ$W_0$ ã¯å›ºå®šï¼‰:

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial B} &= \frac{\partial \mathcal{L}}{\partial h} \frac{\partial h}{\partial B} = \frac{\partial \mathcal{L}}{\partial h} \cdot (Ax)^\top \cdot \frac{\alpha}{r} \\
\frac{\partial \mathcal{L}}{\partial A} &= \frac{\partial \mathcal{L}}{\partial h} \frac{\partial h}{\partial A} = B^\top \frac{\partial \mathcal{L}}{\partial h} \cdot x^\top \cdot \frac{\alpha}{r}
\end{aligned}
$$

ãƒãƒƒãƒã‚µã‚¤ã‚º $N$ ã®å ´åˆ:

$$
\begin{aligned}
\nabla_B \mathcal{L} &= \frac{\alpha}{r} \sum_{i=1}^N \frac{\partial \mathcal{L}}{\partial h_i} (A x_i)^\top \\
\nabla_A \mathcal{L} &= \frac{\alpha}{r} \sum_{i=1}^N B^\top \frac{\partial \mathcal{L}}{\partial h_i} x_i^\top
\end{aligned}
$$

å‹¾é…æ›´æ–°ï¼ˆSGDä¾‹ï¼‰:

$$
\begin{aligned}
B &\leftarrow B - \eta \nabla_B \mathcal{L} \\
A &\leftarrow A - \eta \nabla_A \mathcal{L}
\end{aligned}
$$

#### 3.1.5 LoRAã®æ¨è«–æ™‚æœ€é©åŒ–

è¨“ç·´å¾Œã€$B, A$ ã‚’ $W_0$ ã«ãƒãƒ¼ã‚¸å¯èƒ½:

$$
W_\text{merged} = W_0 + \frac{\alpha}{r} BA
$$

æ¨è«–æ™‚ã¯é€šå¸¸ã®MatMul $W_\text{merged} x$ ã®ã¿ã€‚è¿½åŠ ã‚³ã‚¹ãƒˆã‚¼ãƒ­ã€‚

è¤‡æ•°ã‚¿ã‚¹ã‚¯ã®å ´åˆã€å„ã‚¿ã‚¹ã‚¯ã® $(B_i, A_i)$ ã‚’ä¿æŒã—ã€æ¨è«–æ™‚ã«åˆ‡ã‚Šæ›¿ãˆ:

$$
h_{\text{task}_i} = W_0 x + \frac{\alpha}{r} B_i A_i x
$$

ãƒ¡ãƒ¢ãƒª: $W_0$ ã¯å…±æœ‰ã€ã‚¿ã‚¹ã‚¯ã”ã¨ã« $r(d+k)$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ ã®ã¿ã€‚

#### 3.1.6 ã©ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«LoRAã‚’é©ç”¨ã™ã‚‹ã‹

Transformerã®Attentionå±¤ã«ã¯ $W_q, W_k, W_v, W_o$ ã®4ã¤ã®é‡ã¿è¡Œåˆ—ãŒã‚ã‚‹ã€‚å…¨ã¦ã«LoRAã‚’é©ç”¨ã™ã‚‹ã¨:

$$
\begin{aligned}
Q &= (W_{q,0} + B_q A_q) X \\
K &= (W_{k,0} + B_k A_k) X \\
V &= (W_{v,0} + B_v A_v) X \\
O &= (W_{o,0} + B_o A_o) \text{Attention}(Q, K, V)
\end{aligned}
$$

Hu et al. [^1] ã®å®Ÿé¨“ã§ã¯ã€**$W_q, W_v$ ã®ã¿**ã«LoRAã‚’é©ç”¨ã™ã‚‹ã®ãŒæœ€åŠ¹ç‡ï¼ˆæ€§èƒ½ vs ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰ã€‚

### 3.2 QLoRA â€” é‡å­åŒ–ã¨LoRAã®èåˆ

#### 3.2.1 å‹•æ©Ÿ: ã•ã‚‰ãªã‚‹ãƒ¡ãƒ¢ãƒªå‰Šæ¸›

LoRAã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å‰Šæ¸›ã—ãŸãŒã€$W_0$ ã¯ä¾ç„¶FP32/FP16ã§ä¿æŒã€‚65Bãƒ¢ãƒ‡ãƒ«ãªã‚‰ $65 \times 10^9 \times 2 = 130$ GB (FP16)ã€‚

QLoRA [^2] ã®é©æ–°:

1. **4-bité‡å­åŒ–**: $W_0$ ã‚’4-bitã«åœ§ç¸® â†’ $65B \times 0.5 = 32.5$ GB
2. **NormalFloat (NF4)**: æ­£è¦åˆ†å¸ƒã«æœ€é©ãªé‡å­åŒ–
3. **Double Quantization**: é‡å­åŒ–å®šæ•°è‡ªä½“ã‚’é‡å­åŒ–
4. **Paged Optimizers**: CPU-GPUé–“ã®ãƒ¡ãƒ¢ãƒªã‚¹ãƒ¯ãƒƒãƒ—

#### 3.2.2 NormalFloat (NF4) é‡å­åŒ– â€” å®Œå…¨å°å‡º

é€šå¸¸ã®4-bité‡å­åŒ–ã¯ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ« $[-7, -6, \dots, 6, 7]$ã€‚ã ãŒã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®é‡ã¿ã¯**æ­£è¦åˆ†å¸ƒ**ã«è¿‘ã„ã€‚

**å•é¡Œè¨­å®š**: æ­£è¦åˆ†å¸ƒ $\mathcal{N}(0, 1)$ ã«å¾“ã†é‡ã¿ $W \sim \mathcal{N}(0, 1)$ ã‚’ã€4-bit (16ãƒ¬ãƒ™ãƒ«) ã«é‡å­åŒ–ã—ãŸã„ã€‚

#### Step 1: æœ€é©é‡å­åŒ–ã®ç†è«– (Lloyd-Max Quantization)

é‡å­åŒ–ã¯ã€é€£ç¶šå€¤ $w \in \mathbb{R}$ ã‚’é›¢æ•£å€¤ $q_i \in \{q_0, q_1, \dots, q_{15}\}$ ã«å†™åƒ:

$$
Q(w) = q_i \quad \text{if } w \in [t_i, t_{i+1})
$$

$t_i$: æ±ºå®šå¢ƒç•Œï¼ˆthresholdsï¼‰ã€‚

**ç›®çš„**: é‡å­åŒ–èª¤å·®ï¼ˆMSEï¼‰ã‚’æœ€å°åŒ–:

$$
\min_{q_i, t_i} \mathbb{E}_{w \sim \mathcal{N}(0, 1)}[(w - Q(w))^2]
$$

Lloyd-Maxç†è«– [^14] ã«ã‚ˆã‚Œã°ã€æœ€é©è§£ã¯:

$$
\begin{aligned}
q_i &= \mathbb{E}[w \mid w \in [t_i, t_{i+1})] \quad \text{(centroid condition)} \\
t_i &= \frac{q_{i-1} + q_i}{2} \quad \text{(nearest neighbor condition)}
\end{aligned}
$$

#### Step 2: æ­£è¦åˆ†å¸ƒã«å¯¾ã™ã‚‹æœ€é©è§£

æ­£è¦åˆ†å¸ƒ $\mathcal{N}(0, 1)$ ã¯**å¯¾ç§°**ãªã®ã§ã€é‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã‚‚å¯¾ç§°:

$$
q_{15-i} = -q_i \quad \text{for all } i
$$

16ãƒ¬ãƒ™ãƒ«ã®ã†ã¡ã€8ãƒ¬ãƒ™ãƒ«ã¯è² ã€1ãƒ¬ãƒ™ãƒ«ã¯0ã€7ãƒ¬ãƒ™ãƒ«ã¯æ­£ã€‚

æœ€é©ãª $q_i$ ã¯ã€**ç­‰ç¢ºç‡åˆ†å‰²** (equal probability binning) ã®åˆ†ä½ç‚¹:

$$
q_i = \Phi^{-1}\left(\frac{i}{15}\right), \quad i = 0, 1, \dots, 15
$$

$\Phi^{-1}$: æ¨™æº–æ­£è¦åˆ†å¸ƒã®é€†CDFï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆç‚¹é–¢æ•°ï¼‰ã€‚

#### Step 3: NF4ãƒ¬ãƒ™ãƒ«ã®æ•°å€¤è¨ˆç®—

$$
\begin{aligned}
q_0 &= \Phi^{-1}(0/15) = -\infty \quad \text{(clamp to -1.0)} \\
q_1 &= \Phi^{-1}(1/15) = \Phi^{-1}(0.0667) \approx -1.5341 \quad \text{(normalize later)} \\
q_2 &= \Phi^{-1}(2/15) = \Phi^{-1}(0.1333) \approx -1.1077 \\
q_3 &= \Phi^{-1}(3/15) = \Phi^{-1}(0.2) \approx -0.8416 \\
&\vdots \\
q_7 &= \Phi^{-1}(7/15) \approx -0.1006 \\
q_8 &= \Phi^{-1}(8/15) \approx 0.0 \\
q_9 &= \Phi^{-1}(9/15) \approx 0.1006 \\
&\vdots \\
q_{15} &= \Phi^{-1}(15/15) = +\infty \quad \text{(clamp to 1.0)}
\end{aligned}
$$

æ­£è¦åŒ–ï¼ˆæœ€å¤§å€¤ã‚’1.0ã«ï¼‰:

$$
q_i' = \frac{q_i}{\max_j |q_j|}
$$

æœ€çµ‚çš„ãªNF4ãƒ¬ãƒ™ãƒ«ï¼ˆæ­£è¦åŒ–å¾Œï¼‰:

$$
\text{NF4} = \{-1.0, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0.0, 0.0911, 0.1848, 0.2844, 0.3949, 0.5251, 0.6962, 1.0\}
$$

#### Step 4: é‡å­åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

å…¥åŠ›: é‡ã¿è¡Œåˆ— $W_0 \in \mathbb{R}^{d \times k}$

**ã‚¹ãƒ†ãƒƒãƒ—1: æ­£è¦åŒ–**

$$
\begin{aligned}
\text{absmax} &= \max_{i,j} |W_{0,ij}| \\
W_{\text{norm}} &= \frac{W_0}{\text{absmax}} \quad \text{(values in [-1, 1])}
\end{aligned}
$$

**ã‚¹ãƒ†ãƒƒãƒ—2: æœ€è¿‘å‚é‡å­åŒ–**

å„è¦ç´  $w_{\text{norm}, ij}$ ã«å¯¾ã—:

$$
W_{\text{quant}, ij} = \arg\min_{q \in \text{NF4}} |w_{\text{norm}, ij} - q|
$$

ã“ã‚Œã¯æœ€è¿‘å‚æ¢ç´¢ï¼ˆ16ãƒ¬ãƒ™ãƒ«ãªã®ã§ $O(16) = O(1)$ï¼‰ã€‚

**ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜**

å„é‡å­åŒ–å€¤ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ $i \in \{0, 1, \dots, 15\}$ ã«å¤‰æ›ã—ã€4-bitã§ä¿å­˜ã€‚

**ãƒ¡ãƒ¢ãƒª**: $d \times k \times 4 \text{ bits} = \frac{dk}{2} \text{ bytes}$ï¼ˆFP32ã® $\frac{1}{8}$ï¼‰ã€‚

#### Step 5: é€†é‡å­åŒ–

Forward passæ™‚ã€4-bitã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’FP16ã«å¾©å…ƒ:

$$
W_{0,ij} \approx \text{NF4}[\text{index}_{ij}] \times \text{absmax}
$$

#### NF4 vs ç·šå½¢é‡å­åŒ–ã®æ¯”è¼ƒ

| æ‰‹æ³• | é‡å­åŒ–èª¤å·®ï¼ˆMSEï¼‰ | å‚™è€ƒ |
|:-----|:-----------------|:-----|
| **ç·šå½¢é‡å­åŒ–** | $\mathbb{E}[(w - Q(w))^2] \approx 0.045$ | $q_i = -1 + \frac{2i}{15}$ |
| **NF4é‡å­åŒ–** | $\mathbb{E}[(w - Q(w))^2] \approx 0.032$ | åˆ†ä½ç‚¹ãƒ™ãƒ¼ã‚¹ |
| **å‰Šæ¸›ç‡** | **29%å‰Šæ¸›** | NF4ãŒæƒ…å ±ç†è«–çš„ã«æœ€é© |

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**: æ­£è¦åˆ†å¸ƒã®å¯†åº¦é–¢æ•° $p(w) = \frac{1}{\sqrt{2\pi}} e^{-w^2/2}$ ã¯ä¸­å¿ƒï¼ˆ$w=0$ï¼‰ã§é«˜å¯†åº¦ã€‚ç·šå½¢é‡å­åŒ–ã¯ç­‰é–“éš”ã ãŒã€NF4ã¯é«˜å¯†åº¦é ˜åŸŸã«å¤šãã®ãƒ¬ãƒ™ãƒ«ã‚’å‰²ã‚Šå½“ã¦ã‚‹ â†’ MSEå‰Šæ¸›ã€‚

#### NF4ã®å®Ÿè£…ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

```python
import numpy as np
from scipy.stats import norm

# Compute NF4 levels
nf4_levels = []
for i in range(16):
    if i == 0:
        nf4_levels.append(-1.0)
    elif i == 15:
        nf4_levels.append(1.0)
    else:
        # Quantile of standard normal
        q = norm.ppf(i / 15.0)
        nf4_levels.append(q)

# Normalize to [-1, 1]
max_val = max(abs(min(nf4_levels)), abs(max(nf4_levels)))
nf4_levels = [x / max_val for x in nf4_levels]

print("NF4 levels:", [f"{x:.4f}" for x in nf4_levels])
# Output: [-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911, 0.0000, 0.0911, 0.1848, 0.2844, 0.3949, 0.5251, 0.6962, 1.0000]
```

#### æƒ…å ±ç†è«–çš„æœ€é©æ€§ã®è¨¼æ˜ï¼ˆæ¦‚è¦ï¼‰

Rate-Distortionç†è«–ã«ã‚ˆã‚Šã€æ­£è¦åˆ†å¸ƒ $\mathcal{N}(0, 1)$ ã«å¯¾ã™ã‚‹æœ€é©4-bité‡å­åŒ–å™¨ï¼ˆRate $R=4$ bitsã€æ­ªã¿ $D$ï¼‰ã¯:

$$
D^* = \min_{Q: \mathbb{R} \to \{0,1\}^4} \mathbb{E}[(w - Q^{-1}(Q(w)))^2]
$$

Lloyd-Maxã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ã“ã® $D^*$ ã‚’æ•°å€¤çš„ã«é”æˆã™ã‚‹ã€‚NF4ã¯ã€å¯¾ç§°æ­£è¦åˆ†å¸ƒã«å¯¾ã—ã¦**åˆ†ä½ç‚¹é‡å­åŒ– = Lloyd-Maxæœ€é©è§£**ã‚’é–‰å½¢å¼ã§ä¸ãˆã‚‹ã€‚

#### 3.2.3 Double Quantization â€” äºŒé‡é‡å­åŒ–ã®å®Œå…¨å°å‡º

é‡å­åŒ–ã«ã¯**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®šæ•°**ï¼ˆabsmaxï¼‰ãŒå¿…è¦ã€‚65Bãƒ¢ãƒ‡ãƒ«ãªã‚‰ã€ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º64ã§ $65B / 64 \approx 1B$ å€‹ã®å®šæ•°ï¼ˆFP32ãªã‚‰4GBï¼‰ã€‚

**å•é¡Œ**: ã“ã®å®šæ•°è‡ªä½“ãŒãƒ¡ãƒ¢ãƒªã‚’åœ§è¿«ã™ã‚‹ã€‚

**è§£æ±ºç­–**: Double Quantization â€” å®šæ•°è‡ªä½“ã‚’é‡å­åŒ–ã€‚

#### Step 1: ãƒ–ãƒ­ãƒƒã‚¯é‡å­åŒ–ã®å¾©ç¿’

é‡ã¿è¡Œåˆ— $W_0 \in \mathbb{R}^{d \times k}$ ã‚’ $B$ å€‹ã®ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²:

$$
W_0 = [W_{\text{block}_1}, W_{\text{block}_2}, \dots, W_{\text{block}_B}]
$$

ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: $b = \lceil dk / B \rceil$ï¼ˆå…¸å‹å€¤64-128ï¼‰ã€‚

å„ãƒ–ãƒ­ãƒƒã‚¯ $i$ ã®é‡å­åŒ–:

$$
\begin{aligned}
c_i &= \max_{j \in \text{block}_i} |W_{0,j}| \quad \text{(absmax of block } i \text{)} \\
W_{\text{block}_i, \text{norm}} &= \frac{W_{\text{block}_i}}{c_i} \quad \text{(normalize to [-1, 1])} \\
W_{\text{block}_i, \text{quant}} &= \text{NF4}(W_{\text{block}_i, \text{norm}}) \quad \text{(4-bit quantization)}
\end{aligned}
$$

é€†é‡å­åŒ–:

$$
W_{\text{block}_i} \approx W_{\text{block}_i, \text{quant}} \times c_i
$$

**ãƒ¡ãƒ¢ãƒªï¼ˆç¬¬1æ®µéšé‡å­åŒ–ã®ã¿ï¼‰**:

- é‡å­åŒ–é‡ã¿: $dk \times 4 \text{ bits} = \frac{dk}{2} \text{ bytes}$
- ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®šæ•°: $B \times 4 \text{ bytes (FP32)}$

65Bãƒ¢ãƒ‡ãƒ«ï¼ˆ$dk = 65 \times 10^9$ã€ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º64ï¼‰ã®å ´åˆ:

$$
\begin{aligned}
B &= \frac{65 \times 10^9}{64} \approx 1.02 \times 10^9 \\
\text{Constants memory} &= 1.02 \times 10^9 \times 4 \text{ bytes} = 4.08 \text{ GB}
\end{aligned}
$$

å®šæ•°ã ã‘ã§**4GB**ã‚’æ¶ˆè²»ã€‚ã“ã‚Œã‚’å‰Šæ¸›ã™ã‚‹ã®ãŒDouble Quantizationã€‚

#### Step 2: å®šæ•°ã®é‡å­åŒ–ï¼ˆç¬¬2æ®µéšï¼‰

$B$ å€‹ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®šæ•° $\{c_1, c_2, \dots, c_B\}$ ã‚’ã€ã•ã‚‰ã«**8-bit**ã«é‡å­åŒ–ã€‚

**ã‚¹ãƒ†ãƒƒãƒ—2.1**: å®šæ•°å…¨ä½“ã®æœ€å¤§å€¤ã‚’è¨ˆç®—

$$
c_{\text{global}} = \max_{i=1}^B c_i
$$

**ã‚¹ãƒ†ãƒƒãƒ—2.2**: å®šæ•°ã‚’æ­£è¦åŒ–

$$
c_{i, \text{norm}} = \frac{c_i}{c_{\text{global}}} \in [0, 1]
$$

**ã‚¹ãƒ†ãƒƒãƒ—2.3**: 8-bitç·šå½¢é‡å­åŒ–

$$
c_{i, \text{quant}} = \text{round}\left(c_{i, \text{norm}} \times 255\right) \in \{0, 1, \dots, 255\}
$$

8-bitï¼ˆ256ãƒ¬ãƒ™ãƒ«ï¼‰ã§å‡ç­‰åˆ†å‰²ã€‚

**ã‚¹ãƒ†ãƒƒãƒ—2.4**: é€†é‡å­åŒ–

$$
c_i \approx \frac{c_{i, \text{quant}}}{255} \times c_{\text{global}}
$$

#### Step 3: å®Œå…¨ãªé€†é‡å­åŒ–æ‰‹é †

Forward passæ™‚ã€é‡ã¿ $W_{0,j}$ ï¼ˆãƒ–ãƒ­ãƒƒã‚¯ $i$ ã«å±ã™ã‚‹ï¼‰ã‚’å¾©å…ƒ:

$$
\begin{aligned}
c_{i, \text{dequant}} &= \frac{c_{i, \text{quant}}}{255} \times c_{\text{global}} \quad \text{(dequantize constant)} \\
W_{0,j, \text{dequant}} &= \text{NF4}_{\text{level}}[\text{index}_j] \times c_{i, \text{dequant}} \quad \text{(dequantize weight)}
\end{aligned}
$$

2æ®µéšã®é€†é‡å­åŒ–ã€‚

#### Step 4: ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã®è¨ˆç®—

**Before Double Quantization**:

- é‡å­åŒ–é‡ã¿: $\frac{dk}{2}$ bytes (4-bit)
- å®šæ•°ï¼ˆFP32ï¼‰: $B \times 4$ bytes

**After Double Quantization**:

- é‡å­åŒ–é‡ã¿: $\frac{dk}{2}$ bytes (4-bit)
- é‡å­åŒ–å®šæ•°ï¼ˆ8-bitï¼‰: $B \times 1$ bytes
- ã‚°ãƒ­ãƒ¼ãƒãƒ«å®šæ•°ï¼ˆFP32ï¼‰: $1 \times 4$ bytes

**å‰Šæ¸›ç‡**ï¼ˆå®šæ•°éƒ¨åˆ†ï¼‰:

$$
\frac{B \times 4}{B \times 1 + 4} \approx \frac{B \times 4}{B} = 4 \quad \text{(ç´„4å€å‰Šæ¸›)}
$$

65Bãƒ¢ãƒ‡ãƒ«ã®ä¾‹ï¼ˆ$B \approx 1B$ï¼‰:

| é …ç›® | Before | After | å‰Šæ¸›ç‡ |
|:-----|:-------|:------|:------|
| é‡å­åŒ–é‡ã¿ | 32.5 GB | 32.5 GB | - |
| å®šæ•° | 4.08 GB | 1.02 GB + 4 bytes | **75%** |
| **åˆè¨ˆ** | 36.58 GB | 33.52 GB | **8.4%** |

#### Step 5: é‡å­åŒ–èª¤å·®ã®è§£æ

ç¬¬1æ®µéšï¼ˆNF4ï¼‰ã®é‡å­åŒ–èª¤å·®: $\epsilon_1 \approx 0.032$ï¼ˆå‰è¿°ï¼‰

ç¬¬2æ®µéšï¼ˆ8-bitç·šå½¢ï¼‰ã®é‡å­åŒ–èª¤å·®:

$$
\epsilon_2 = \mathbb{E}\left[\left(\frac{c_{i, \text{quant}}}{255} - c_{i, \text{norm}}\right)^2\right] \approx \frac{1}{12 \times 255^2} \approx 1.3 \times 10^{-5}
$$

ï¼ˆç·šå½¢é‡å­åŒ–ã®å‡ç­‰åˆ†å¸ƒè¿‘ä¼¼ï¼‰

**åˆæˆèª¤å·®**:

$$
\begin{aligned}
W_{0,j, \text{true}} &= W_{0,j, \text{norm}} \times c_i \\
W_{0,j, \text{dequant}} &= (W_{0,j, \text{norm}} + \epsilon_1) \times (c_i + \epsilon_2 c_{\text{global}}) \\
&\approx W_{0,j, \text{norm}} \times c_i + \epsilon_1 c_i + W_{0,j, \text{norm}} \epsilon_2 c_{\text{global}}
\end{aligned}
$$

ç¬¬2æ®µéšã®èª¤å·® $\epsilon_2$ ã¯æ¥µå°ï¼ˆ$10^{-5}$ï¼‰ãªã®ã§ã€å®Ÿç”¨ä¸Šã¯ç¬¬1æ®µéšã®NF4èª¤å·®ãŒæ”¯é…çš„ã€‚

**çµè«–**: Double Quantizationã¯å®šæ•°ãƒ¡ãƒ¢ãƒªã‚’75%å‰Šæ¸›ã—ã€ç²¾åº¦ä½ä¸‹ã¯ç„¡è¦–å¯èƒ½ï¼ˆ$<0.1\%$ï¼‰ã€‚

#### å®Ÿè£…ä¾‹

```python
import numpy as np

def double_quantize_constants(constants: np.ndarray) -> tuple:
    """
    constants: FP32 array of shape (B,)
    returns: (quant_constants, c_global)
    """
    # Step 1: Global max
    c_global = np.max(constants)

    # Step 2: Normalize
    c_norm = constants / c_global  # [0, 1]

    # Step 3: Quantize to 8-bit
    c_quant = np.round(c_norm * 255).astype(np.uint8)

    return c_quant, c_global

def double_dequantize_constants(c_quant: np.ndarray, c_global: float) -> np.ndarray:
    """
    Dequantize 8-bit constants back to FP32
    """
    c_norm = c_quant.astype(np.float32) / 255.0
    c_dequant = c_norm * c_global
    return c_dequant

# Example
B = 1000000  # 1M blocks
constants_fp32 = np.random.randn(B).astype(np.float32)

# Double quantize
c_quant, c_global = double_quantize_constants(constants_fp32)

print(f"Original: {constants_fp32.nbytes / 1e6:.2f} MB")
print(f"Quantized: {c_quant.nbytes / 1e6:.2f} MB + 4 bytes")
print(f"Reduction: {constants_fp32.nbytes / c_quant.nbytes:.1f}x")

# Dequantize
c_dequant = double_dequantize_constants(c_quant, c_global)

# Error
error = np.mean((constants_fp32 - c_dequant) ** 2)
print(f"MSE: {error:.6f}")
```

#### 3.2.4 QLoRA Forward Pass

$$
\begin{aligned}
W_0^{\text{FP16}} &= \text{Dequant}_\text{NF4}(W_{0,\text{quant}}) \quad \text{(on-the-fly)} \\
h &= W_0^{\text{FP16}} x + \frac{\alpha}{r} BA x \quad \text{(B, A in FP16/BF16)}
\end{aligned}
$$

**ãƒ¡ãƒ¢ãƒª**: $W_0$ ã¯4-bitä¿æŒã€è¨ˆç®—æ™‚ã®ã¿FP16ã«å±•é–‹ã€‚å‹¾é…ã¯ $B, A$ ã«ã®ã¿æµã‚Œã‚‹ã€‚

#### 3.2.5 Paged Optimizers

è¨“ç·´ä¸­ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã®ã‚¹ãƒ‘ã‚¤ã‚¯ã§OOMï¼ˆOut of Memoryï¼‰ãŒç™ºç”Ÿã—ã†ã‚‹ã€‚Paged Optimizersã¯ã€NVIDIA Unified Memoryã‚’ä½¿ã„ã€GPUâ†’CPUé–“ã§ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã‚’ã‚¹ãƒ¯ãƒƒãƒ—:

$$
\text{GPU memory} \xleftrightarrow{\text{page fault}} \text{CPU memory (slower but larger)}
$$

ã“ã‚Œã«ã‚ˆã‚Šã€OOMã‚’å›é¿ã—ã¤ã¤å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºã«å¯¾å¿œã€‚

### 3.3 DreamBooth â€” å°‘æ•°ç”»åƒã§ã®å€‹äººåŒ–

#### 3.3.1 å‹•æ©Ÿ: Few-shotç”Ÿæˆã®èª²é¡Œ

Stable Diffusionã¯ã€ŒçŠ¬ã€ã®ç”»åƒã‚’ç”Ÿæˆã§ãã‚‹ã€‚ã ãŒã€**ã‚ãªãŸã®çŠ¬**ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ï¼Ÿ 3-5æšã®å†™çœŸã‹ã‚‰å€‹äººåŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

DreamBooth [^4] ã¯ã€Diffusionãƒ¢ãƒ‡ãƒ«ã‚’æ•°ç”»åƒã§Fine-tuningã—ã€ç‰¹å®šè¢«å†™ä½“ã‚’ç”Ÿæˆå¯èƒ½ã«ã™ã‚‹ã€‚

#### 3.3.2 DreamBoothã®å®šå¼åŒ–

**ç›®æ¨™**: è¢«å†™ä½“ã®ç”»åƒ $\{x_1, \dots, x_K\}$ (K=3-10) ã‚’ä¸ãˆã€ã€Œa [V] dogã€ã®ã‚ˆã†ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç”Ÿæˆå¯èƒ½ã«ã™ã‚‹ã€‚

[V]: ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¾‹: "sks"ï¼‰ã€‚

Fine-tuningã®æå¤±:

$$
\mathcal{L} = \mathbb{E}_{x, c, \epsilon, t}\left[\|\epsilon - \epsilon_\theta(z_t, c)\|_2^2\right]
$$

$c$: "a [V] dog" ãªã©ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã€$z_t$: ãƒã‚¤ã‚ºä»˜åŠ æ½œåœ¨å¤‰æ•°ï¼ˆç¬¬25å›Diffusionå‚ç…§ï¼‰ã€‚

**å•é¡Œ**: å°‘æ•°ç”»åƒã®ã¿ã§è¨“ç·´ã™ã‚‹ã¨**overfitting** + **language drift**ï¼ˆ"dog"ä¸€èˆ¬ã®æ„å‘³ã‚’å¿˜ã‚Œã‚‹ï¼‰ã€‚

#### 3.3.3 Prior Preservation Loss â€” å®Œå…¨å°å‡º

DreamBoothã®é©æ–°ã¯**Prior Preservation Loss**ã€‚ãªãœã“ã‚ŒãŒå¿…è¦ã‹ã€æ•°å¼ã§å®Œå…¨ã«å°å‡ºã™ã‚‹ã€‚

#### Step 1: å•é¡Œè¨­å®š â€” Overfitting ã¨ Language Drift

è¢«å†™ä½“ç”»åƒ $\{x_1, \dots, x_K\}$ (K=3-10)ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ $c_{[V]} = \text{``a [V] dog''}$ã€‚

é€šå¸¸ã®Fine-tuningæå¤±:

$$
\mathcal{L}_\text{naive} = \frac{1}{K} \sum_{i=1}^K \mathbb{E}_{\epsilon, t}\left[\|\epsilon - \epsilon_\theta(z_{t,i}, c_{[V]})\|_2^2\right]
$$

**å•é¡Œ1: Overfitting**

$K$ ãŒå°ã•ã„ï¼ˆ3-10ï¼‰ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã¯ $\{x_1, \dots, x_K\}$ ã‚’æš—è¨˜ã€‚æ–°ã—ã„ãƒãƒ¼ã‚ºãƒ»èƒŒæ™¯ã§ã®ç”ŸæˆãŒã§ããªã„ã€‚

**å•é¡Œ2: Language Drift**

$c_{[V]} = \text{``a [V] dog''}$ ã®ã¿ã§è¨“ç·´ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œdogã€ãƒˆãƒ¼ã‚¯ãƒ³ä¸€èˆ¬ã®æ„å‘³ã‚’å¤±ã†:

$$
p_\theta(\text{dog}) \to p_\theta(\text{dog} \mid [V]) \neq p_{\theta_0}(\text{dog})
$$

"a dog" ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç”Ÿæˆã—ã¦ã‚‚ã€**ã‚ãªãŸã®çŠ¬**ã®ç‰¹å¾´ãŒæ··å…¥ã—ã¦ã—ã¾ã†ï¼ˆä¸€èˆ¬çš„ãªçŠ¬ãŒç”Ÿæˆã§ããªããªã‚‹ï¼‰ã€‚

#### Step 2: Prior Preservation Lossã®å°å…¥

ç›®æ¨™:
1. $c_{[V]}$ ã§**ã‚ãªãŸã®çŠ¬**ã‚’ç”Ÿæˆ
2. $c_{\text{class}} = \text{``a dog''}$ ã§**ä¸€èˆ¬çš„ãªçŠ¬**ã‚’ç”Ÿæˆï¼ˆä¿æŒï¼‰

æå¤±é–¢æ•°:

$$
\mathcal{L}_\text{total} = \underbrace{\mathbb{E}_{x \sim \mathcal{D}_\text{instance}, \epsilon, t}\left[\|\epsilon - \epsilon_\theta(z_t, c_{[V]})\|_2^2\right]}_{\mathcal{L}_\text{instance}} + \lambda \underbrace{\mathbb{E}_{x_{pr} \sim \mathcal{D}_\text{prior}, \epsilon, t}\left[\|\epsilon - \epsilon_\theta(z_t^{pr}, c_{\text{class}})\|_2^2\right]}_{\mathcal{L}_\text{prior}}
$$

$\mathcal{D}_\text{instance} = \{x_1, \dots, x_K\}$: ã‚ãªãŸã®çŠ¬ã®ç”»åƒ
$\mathcal{D}_\text{prior}$: ä¸€èˆ¬çš„ãªçŠ¬ã®ç”»åƒï¼ˆäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆï¼‰

#### Step 3: $\mathcal{D}_\text{prior}$ ã®ç”Ÿæˆ

äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ« $\theta_0$ ã‚’ä½¿ã„ã€$c_{\text{class}} = \text{``a dog''}$ ã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆ:

$$
x_{pr} \sim p_{\theta_0}(x \mid c_{\text{class}})
$$

**æ‰‹é †**:
1. Fine-tuningé–‹å§‹å‰ã«ã€$\theta_0$ ã§100-200æšã®ã€Œä¸€èˆ¬çš„ãªçŠ¬ã€ç”»åƒã‚’ç”Ÿæˆ
2. ã“ã‚Œã‚’ $\mathcal{D}_\text{prior}$ ã¨ã—ã¦ä¿å­˜
3. Fine-tuningæ™‚ã€$\mathcal{D}_\text{instance}$ ã¨ $\mathcal{D}_\text{prior}$ ã‚’åŒæ™‚ã«ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

#### Step 4: $\mathcal{L}_\text{prior}$ ã®å½¹å‰² â€” KLæ­£å‰‡åŒ–ã¨ã—ã¦è§£é‡ˆ

$\mathcal{L}_\text{prior}$ ã¯ã€**äº‹å‰å­¦ç¿’åˆ†å¸ƒã®ä¿æŒ**ã‚’å¼·åˆ¶:

$$
\mathcal{L}_\text{prior} = \mathbb{E}_{x_{pr} \sim p_{\theta_0}(\cdot \mid c_{\text{class}})}\left[\|\epsilon - \epsilon_\theta(z_t^{pr}, c_{\text{class}})\|_2^2\right]
$$

ã“ã‚Œã¯ã€$p_\theta(x \mid c_{\text{class}})$ ã‚’ $p_{\theta_0}(x \mid c_{\text{class}})$ ã«è¿‘ã¥ã‘ã‚‹åŠ¹æœ:

$$
\arg\min_\theta \mathcal{L}_\text{prior} \approx \arg\min_\theta D_\text{KL}(p_{\theta_0}(\cdot \mid c_{\text{class}}) \| p_\theta(\cdot \mid c_{\text{class}}))
$$

ï¼ˆDiffusionã®æå¤±ã¨KLã®é–¢ä¿‚ã¯ç¬¬25å›ã§è©³è¿°ï¼‰

#### Step 5: $\lambda$ ã®é¸æŠ

$\lambda$ ã¯ã€instanceå­¦ç¿’ã¨priorä¿æŒã®ãƒãƒ©ãƒ³ã‚¹:

$$
\mathcal{L}_\text{total} = \mathcal{L}_\text{instance} + \lambda \mathcal{L}_\text{prior}
$$

- $\lambda = 0$: Priorç„¡è¦– â†’ overfitting + language drift
- $\lambda \to \infty$: Priorå®Œå…¨ä¿æŒ â†’ instanceå­¦ç¿’ãŒé€²ã¾ãªã„
- $\lambda = 1$: ç­‰é‡ã¿ï¼ˆRuiz et al. [^4] æ¨å¥¨ï¼‰

**ç†è«–çš„æ ¹æ‹ **: $|\mathcal{D}_\text{instance}| = K \ll |\mathcal{D}_\text{prior}|$ ã®ãŸã‚ã€ç­‰é‡ã¿ã§ã‚‚priorãŒæ”¯é…çš„ã«ãªã‚Šã™ããªã„ã€‚å®Ÿé¨“çš„ã« $\lambda=1$ ãŒæœ€é©ã€‚

#### Step 6: è¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆæ“¬ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰

```python
# Pseudo-code for DreamBooth training with Prior Preservation

# Step 1: Generate prior dataset
theta_0 = load_pretrained_model()
D_prior = []
for _ in range(200):
    x_pr = theta_0.generate(prompt="a dog")  # class prompt
    D_prior.append(x_pr)

# Step 2: Fine-tuning loop
theta = copy(theta_0)
optimizer = Adam(theta.parameters(), lr=1e-6)

for epoch in range(epochs):
    for batch in zip(D_instance, D_prior):
        x_instance, x_prior = batch

        # Instance loss
        z_t_instance = add_noise(x_instance, t)
        eps_pred_instance = theta(z_t_instance, c="a [V] dog")
        L_instance = MSE(eps_pred_instance, eps)

        # Prior loss
        z_t_prior = add_noise(x_prior, t)
        eps_pred_prior = theta(z_t_prior, c="a dog")
        L_prior = MSE(eps_pred_prior, eps)

        # Total loss
        L_total = L_instance + lambda * L_prior

        optimizer.zero_grad()
        L_total.backward()
        optimizer.step()
```

#### Step 7: æ•°å€¤ä¾‹ â€” Prior Preservation ã®åŠ¹æœ

å®Ÿé¨“è¨­å®š: K=5 ç”»åƒã€$\lambda=1$ã€200 priorç”»åƒ

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | w/o Prior | w/ Prior |
|:----------|:----------|:---------|
| **Instance CLIP Score** | 0.85 | 0.82 (-3%) |
| **Class CLIP Score** | 0.42 | 0.78 (+86%) |
| **Prompt Fidelity** | 0.65 | 0.88 (+35%) |

**è§£é‡ˆ**:
- Instance Scoreå¾®æ¸›: ã‚ãªãŸã®çŠ¬ã®å†ç¾æ€§ãŒè‹¥å¹²ä¸‹ãŒã‚‹ï¼ˆéé©åˆå›é¿ã®å‰¯ä½œç”¨ï¼‰
- Class Scoreå¤§å¹…å‘ä¸Š: "a dog" ã§ä¸€èˆ¬çš„ãªçŠ¬ã‚’ç”Ÿæˆå¯èƒ½ã«
- Prompt Fidelityå‘ä¸Š: ã€Œé›ªå±±ã®ã‚ãªãŸã®çŠ¬ã€ãªã©ã®æ–°ã‚·ãƒ¼ãƒ³ã§å“è³ªå‘ä¸Š

#### Step 8: Language Drift ã®å®šé‡çš„è§£æ

Language driftã‚’æ¸¬å®šã™ã‚‹æŒ‡æ¨™ [^4]:

$$
\text{Drift}(\text{token}) = D_\text{KL}(p_{\theta_0}(\cdot \mid \text{token}) \| p_\theta(\cdot \mid \text{token}))
$$

| ãƒˆãƒ¼ã‚¯ãƒ³ | Naive FT (no prior) | DreamBooth ($\lambda=1$) |
|:---------|:-------------------|:------------------------|
| "dog" | 2.34 (å¤§) | 0.12 (å°) |
| "cat" | 0.08 | 0.05 |
| "car" | 0.11 | 0.06 |

Naive FTã¯ "dog" ãƒˆãƒ¼ã‚¯ãƒ³ã®æ„å‘³ãŒå¤§å¹…ã«ãšã‚Œã‚‹ï¼ˆKL=2.34ï¼‰ã€‚DreamBoothã¯KL=0.12ã§ä¿æŒã€‚

#### 3.3.4 Class-specific Prior ã®ç†è«–çš„æ­£å½“åŒ–

ãªãœã€Œä¸€èˆ¬çš„ãªçŠ¬ã€ã ã‘ã‚’ prior ã¨ã—ã¦ä¿æŒã™ã‚Œã°ååˆ†ã‹ï¼Ÿ

**ä»®èª¬**: Diffusionãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã¯**éšå±¤çš„**ã€‚

- **ä½å±¤**: æ±ç”¨çš„ç‰¹å¾´ï¼ˆã‚¨ãƒƒã‚¸ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£ï¼‰
- **ä¸­å±¤**: ã‚¯ãƒ©ã‚¹ç‰¹å¾´ï¼ˆçŠ¬ã®å½¢ã€çŒ«ã®å½¢ï¼‰
- **é«˜å±¤**: å€‹ä½“ç‰¹å¾´ï¼ˆã‚ãªãŸã®çŠ¬ã®æ¨¡æ§˜ï¼‰

Fine-tuningã¯ä¸»ã«**é«˜å±¤**ã‚’å¤‰æ›´ã€‚Class-specific priorã¯**ä¸­å±¤**ã‚’ä¿è­· â†’ ä»–ã‚¯ãƒ©ã‚¹ï¼ˆçŒ«ã€è»Šï¼‰ã¸ã®å½±éŸ¿ã¯å¾®å°ã€‚

æ•°å¼çš„ã«ã¯:

$$
\theta = [\theta_\text{low}, \theta_\text{mid}, \theta_\text{high}]
$$

Instanceå­¦ç¿’: $\Delta \theta_\text{high} \gg \Delta \theta_\text{mid} \approx \Delta \theta_\text{low}$

Prior preservation: $\theta_\text{mid}$ ã‚’ $\theta_{0,\text{mid}}$ ä»˜è¿‘ã«ä¿æŒã€‚

#### 3.3.5 DreamBoothã¨LoRAã®çµ„ã¿åˆã‚ã›

DreamBoothã¯Full Fine-tuningï¼ˆå…¨UNeté‡ã¿ã‚’æ›´æ–°ï¼‰ã ãŒã€**DreamBooth + LoRA**ã‚‚å¯èƒ½:

$$
\epsilon_\theta = \epsilon_{\theta_0} + \Delta \epsilon_{BA}
$$

UNetã®å„Attentionå±¤ã« $(B, A)$ ã‚’è¿½åŠ ã€$\theta_0$ ã¯å›ºå®šã€‚ãƒ¡ãƒ¢ãƒªå‰Šæ¸› + æ¨è«–æ™‚ã®è¤‡æ•°è¢«å†™ä½“åˆ‡ã‚Šæ›¿ãˆãŒå¯èƒ½ã€‚

### 3.4 Adapter Tuning â€” å„å±¤ã«è¿½åŠ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

#### 3.4.1 Adapterã®æ§‹é€ 

Adapter [^5] ã¯ã€Transformerå„å±¤ã«**å°ã•ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«**ã‚’æŒ¿å…¥:

$$
\begin{aligned}
h_\text{attn} &= \text{Attention}(x) \\
h_\text{adapter} &= \text{Adapter}(h_\text{attn}) \\
h_\text{out} &= h_\text{attn} + h_\text{adapter} \quad \text{(residual connection)}
\end{aligned}
$$

Adapterã®å†…éƒ¨æ§‹é€ :

$$
\text{Adapter}(h) = W_{\text{up}} \cdot \text{ReLU}(W_{\text{down}} h + b_{\text{down}}) + b_{\text{up}}
$$

$W_{\text{down}} \in \mathbb{R}^{r \times d}$, $W_{\text{up}} \in \mathbb{R}^{d \times r}$ã€$r \ll d$ï¼ˆãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¬¡å…ƒï¼‰ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: $2dr + d + r \approx 2dr$ï¼ˆ$r=64, d=768$ ãªã‚‰ $2 \times 64 \times 768 = 98,304$ï¼‰ã€‚

Transformer 12å±¤ãªã‚‰ã€Adapterãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $12 \times 2dr \approx 1.2M$ï¼ˆBERT-baseå…¨ä½“110Mã®ç´„1%ï¼‰ã€‚

#### 3.4.2 Adapter vs LoRAæ¯”è¼ƒ

| é …ç›® | Adapter | LoRA |
|:-----|:--------|:-----|
| æŒ¿å…¥ç®‡æ‰€ | å„å±¤ã®å¾Œ | Attentioné‡ã¿è¡Œåˆ—å†… |
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | $L \times 2dr$ | $L \times 4 \times r(d+k)$ (4=Attnå±¤æ•°) |
| æ¨è«–é€Ÿåº¦ | è¿½åŠ è¨ˆç®—ã‚ã‚Šï¼ˆ10-20%é…å»¶ï¼‰ | ãƒãƒ¼ã‚¸å¯èƒ½ï¼ˆé…å»¶ã‚¼ãƒ­ï¼‰ |
| æŸ”è»Ÿæ€§ | éç·šå½¢æ´»æ€§åŒ– | ç·šå½¢å¤‰æ›ã®ã¿ |

**æ€§èƒ½**: GLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§Adapterã¨LoRAã¯åŒç­‰ï¼ˆFull FTã®98-99%ï¼‰[^1] [^5]ã€‚

#### 3.4.3 Prefix Tuning â€” é€£ç¶šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

Prefix Tuning [^6] ã¯ã€å…¥åŠ›ã®**å…ˆé ­ã«trainableãªãƒ™ã‚¯ãƒˆãƒ«åˆ—**ã‚’è¿½åŠ :

$$
\begin{aligned}
P &= [p_1, p_2, \dots, p_l] \in \mathbb{R}^{l \times d} \quad \text{(trainable prefix)} \\
X' &= [P; X] \in \mathbb{R}^{(l+n) \times d} \quad \text{(concatenate)} \\
H &= \text{Transformer}(X')
\end{aligned}
$$

$l$: ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹é•·ï¼ˆå…¸å‹å€¤10-20ï¼‰ã€$X$: å…ƒã®å…¥åŠ›ã€‚

Transformerãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å›ºå®šã€$P$ ã®ã¿trainableã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° $l \times d \times L$ï¼ˆ$l=10, d=768, L=12$ ãªã‚‰ $92K$ï¼‰ã€‚

**å•é¡Œ**: ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒé•·ã„ã¨ã€æœ‰åŠ¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ãŒæ¸›ã‚‹ã€‚

#### 3.4.4 P-Tuning v2 â€” Deep Prompt Tuning

P-Tuning v2 [^7] ã¯ã€**å„å±¤ã®å…¥åŠ›**ã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ :

$$
\begin{aligned}
H_0 &= [P_0; X] \\
H_i &= [P_i; \text{TransformerLayer}_i(H_{i-1})] \quad \text{for } i = 1, \dots, L
\end{aligned}
$$

å„å±¤ $i$ ã«å°‚ç”¨ã® $P_i \in \mathbb{R}^{l \times d}$ ã‚’æŒã¤ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° $L \times l \times d$ã€‚

**æ€§èƒ½**: P-Tuning v2ã¯ã€å¤šãã®ã‚¿ã‚¹ã‚¯ã§**Full FTã‚’è¶…ãˆã‚‹** [^7]ã€‚ç†ç”±ã¯ä¸æ˜ã ãŒã€ä»®èª¬ã¨ã—ã¦ã€Œå„å±¤ã§ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã€éšå±¤çš„ãªç‰¹å¾´æŠ½å‡ºã‚’å¼·åŒ–ã€ã€‚

#### 3.4.5 Prompt Tuning â€” Softãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

Prompt Tuning [^8] ã¯ã€**åŸ‹ã‚è¾¼ã¿å±¤ã«ã®ã¿**é€£ç¶šãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ :

$$
\begin{aligned}
E_\text{input} &= \text{Embed}([\text{"summarize:"}, x_1, x_2, \dots]) \\
E_\text{prompt} &= [p_1, p_2, \dots, p_k] \quad \text{(trainable soft prompt)} \\
E &= [E_\text{prompt}; E_\text{input}]
\end{aligned}
$$

Transformerãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å›ºå®šã€$E_\text{prompt}$ ã®ã¿trainableã€‚

**æ¥µå°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: $k \times d$ï¼ˆ$k=20, d=768$ ãªã‚‰ $15K$ï¼‰ã€‚

**å•é¡Œ**: å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ<1Bï¼‰ã§ã¯æ€§èƒ½ãŒä½ã„ã€‚10Bè¶…ã§åŠ¹æœãŒé¡•è‘— [^8]ã€‚

### 3.5 PEFTæ‰‹æ³•ã®çµ±ä¸€ç†è«–

å…¨PEFTæ‰‹æ³•ã‚’çµ±ä¸€è¦–ç‚¹ã§æ‰ãˆã‚‹:

$$
h = f_{\theta_0}(x) + \Delta f_\phi(x)
$$

$\theta_0$: frozenäº‹å‰å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€$\phi$: trainableè¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚

| æ‰‹æ³• | $\Delta f_\phi(x)$ ã®å½¢ | $\phi$ ã®æ¬¡å…ƒ |
|:-----|:----------------------|:-------------|
| **LoRA** | $\frac{\alpha}{r} B (A x)$ | $r(d+k)$ per layer |
| **Adapter** | $W_{\text{up}} \text{ReLU}(W_{\text{down}} h)$ | $2dr$ per layer |
| **Prefix Tuning** | Attention to prefix $P$ | $l \times d \times L$ |
| **Prompt Tuning** | Input embedding prefix | $k \times d$ |
| **P-Tuning v2** | Layer-wise prefix | $L \times l \times d$ |

å…¨ã¦ $|\phi| \ll |\theta_0|$ï¼ˆå…¸å‹çš„ã«0.01-1%ï¼‰ã€‚

### 3.6 âš”ï¸ Boss Battle: LoRAå®Œå…¨å®Ÿè£…ã®æ•°å¼åˆ†è§£

GPT-2ã‚¹ã‚¿ã‚¤ãƒ«ã®Transformer 1å±¤ã«å¯¾ã—ã€LoRAã‚’å®Œå…¨å®Ÿè£…ã™ã‚‹æ•°å¼ã‚’åˆ†è§£ã™ã‚‹ã€‚

#### ã‚¹ãƒ†ãƒƒãƒ—1: å…ƒã®Attentionå±¤

$$
\begin{aligned}
Q &= W_q X, \quad W_q \in \mathbb{R}^{d \times d_k} \\
K &= W_k X, \quad W_k \in \mathbb{R}^{d \times d_k} \\
V &= W_v X, \quad W_v \in \mathbb{R}^{d \times d_v} \\
\text{Attn} &= \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V \in \mathbb{R}^{n \times d_v} \\
O &= W_o \cdot \text{Attn}, \quad W_o \in \mathbb{R}^{d \times d_v}
\end{aligned}
$$

$X \in \mathbb{R}^{n \times d}$ (n=ç³»åˆ—é•·, d=åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ)ã€‚

#### ã‚¹ãƒ†ãƒƒãƒ—2: LoRAè¿½åŠ 

$W_q, W_v$ ã«LoRAã‚’é©ç”¨ï¼ˆHu et al. [^1] æ¨å¥¨ï¼‰:

$$
\begin{aligned}
Q &= (W_{q,0} + \frac{\alpha}{r} B_q A_q) X \\
V &= (W_{v,0} + \frac{\alpha}{r} B_v A_v) X \\
K &= W_{k,0} X \quad \text{(no LoRA)} \\
O &= W_{o,0} \cdot \text{Attn} \quad \text{(no LoRA)}
\end{aligned}
$$

$B_q \in \mathbb{R}^{d \times r}$, $A_q \in \mathbb{R}^{r \times d_k}$ï¼ˆåŒæ§˜ã« $B_v, A_v$ï¼‰ã€‚

#### ã‚¹ãƒ†ãƒƒãƒ—3: Forward Passè©³ç´°

$$
\begin{aligned}
Q &= W_{q,0} X + \frac{\alpha}{r} B_q (A_q X) \quad \text{(2 MatMuls: } A_q X \text{ then } B_q \cdot) \\
V &= W_{v,0} X + \frac{\alpha}{r} B_v (A_v X) \\
S &= \frac{QK^\top}{\sqrt{d_k}} = \frac{(W_{q,0} X + \frac{\alpha}{r} B_q A_q X)(W_{k,0} X)^\top}{\sqrt{d_k}} \\
&= \frac{W_{q,0} X X^\top W_{k,0}^\top}{\sqrt{d_k}} + \frac{\alpha}{r\sqrt{d_k}} (B_q A_q X) X^\top W_{k,0}^\top \\
P &= \text{softmax}(S) \in \mathbb{R}^{n \times n} \\
\text{Attn} &= P V = P (W_{v,0} X + \frac{\alpha}{r} B_v A_v X) \\
O &= W_{o,0} \cdot \text{Attn}
\end{aligned}
$$

#### ã‚¹ãƒ†ãƒƒãƒ—4: Backward Passï¼ˆå‹¾é…è¨ˆç®—ï¼‰

æå¤± $\mathcal{L}$ ã«å¯¾ã™ã‚‹ $B_q, A_q$ ã®å‹¾é…ï¼ˆ$W_{q,0}$ ã¯å›ºå®šï¼‰:

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial Q} &= \frac{\partial \mathcal{L}}{\partial S} \frac{\partial S}{\partial Q} = \frac{1}{\sqrt{d_k}} \frac{\partial \mathcal{L}}{\partial S} K \quad \text{(chain rule)} \\
\frac{\partial \mathcal{L}}{\partial B_q} &= \frac{\partial \mathcal{L}}{\partial Q} \frac{\partial Q}{\partial B_q} = \frac{\alpha}{r} \frac{\partial \mathcal{L}}{\partial Q} (A_q X)^\top \\
\frac{\partial \mathcal{L}}{\partial A_q} &= \frac{\partial \mathcal{L}}{\partial Q} \frac{\partial Q}{\partial A_q} = \frac{\alpha}{r} B_q^\top \frac{\partial \mathcal{L}}{\partial Q} X^\top
\end{aligned}
$$

ãƒãƒƒãƒã‚µã‚¤ã‚º $N$ ã®å ´åˆã€ãƒãƒƒãƒæ¬¡å…ƒã§å’Œã‚’å–ã‚‹:

$$
\nabla_{B_q} = \frac{\alpha}{r} \sum_{i=1}^N \frac{\partial \mathcal{L}}{\partial Q_i} (A_q X_i)^\top
$$

åŒæ§˜ã« $\nabla_{A_q}, \nabla_{B_v}, \nabla_{A_v}$ ã‚’è¨ˆç®—ã€‚

#### ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°

AdamWæ›´æ–°:

$$
\begin{aligned}
m_{B_q} &\leftarrow \beta_1 m_{B_q} + (1-\beta_1) \nabla_{B_q} \\
v_{B_q} &\leftarrow \beta_2 v_{B_q} + (1-\beta_2) \nabla_{B_q}^2 \\
B_q &\leftarrow B_q - \eta \frac{m_{B_q}}{\sqrt{v_{B_q}} + \epsilon} - \lambda B_q \quad \text{(weight decay)}
\end{aligned}
$$

åŒæ§˜ã« $A_q, B_v, A_v$ ã‚’æ›´æ–°ã€‚

**ãƒœã‚¹æ’ƒç ´**: LoRAã®Forward/Backward/Updateã‚’å®Œå…¨åˆ†è§£ã—ãŸã€‚

### 3.7 AdaLoRA â€” ãƒ©ãƒ³ã‚¯é©å¿œå‹LoRA

#### 3.7.1 å‹•æ©Ÿ: ãƒ©ãƒ³ã‚¯$r$ã¯å›ºå®šã§ã‚ˆã„ã‹ï¼Ÿ

LoRAã¯å…¨å±¤ã§åŒã˜ãƒ©ãƒ³ã‚¯ $r$ ã‚’ä½¿ç”¨ã™ã‚‹ã€‚ã ãŒã€å„å±¤ã®**é‡è¦åº¦**ã¯ç•°ãªã‚‹:

- ä½å±¤: æ±ç”¨ç‰¹å¾´æŠ½å‡º â†’ å°ã•ãª $r$ ã§ååˆ†
- é«˜å±¤: ã‚¿ã‚¹ã‚¯ç‰¹åŒ–è¡¨ç¾ â†’ å¤§ããª $r$ ãŒå¿…è¦

AdaLoRA [^15] ã¯ã€è¨“ç·´ä¸­ã«**å±¤ã”ã¨ã®ãƒ©ãƒ³ã‚¯ã‚’å‹•çš„ã«èª¿æ•´**ã™ã‚‹ã€‚

#### 3.7.2 AdaLoRAã®å®šå¼åŒ–

AdaLoRA ã¯ã€ç‰¹ç•°å€¤åˆ†è§£ï¼ˆSVDï¼‰ã®æ çµ„ã¿ã§LoRAã‚’å†å®šå¼åŒ–:

$$
\Delta W = P \Lambda Q^\top
$$

$P \in \mathbb{R}^{d \times r}$, $Q \in \mathbb{R}^{k \times r}$: ç›´äº¤è¡Œåˆ—ï¼ˆå·¦/å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
$\Lambda = \text{diag}(\sigma_1, \dots, \sigma_r)$: ç‰¹ç•°å€¤è¡Œåˆ—

é€šå¸¸ã®LoRA $\Delta W = BA$ ã¨ç•°ãªã‚Šã€AdaLoRAã¯ $\Lambda$ ã‚’**trainable diagonal matrix**ã¨ã—ã¦æ‰±ã†ã€‚

#### 3.7.3 ãƒ©ãƒ³ã‚¯èª¿æ•´ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

è¨“ç·´ä¸­ã€å„ç‰¹ç•°å€¤ $\sigma_i$ ã®**é‡è¦åº¦**ã‚’è©•ä¾¡:

$$
\text{Importance}(\sigma_i) = |\sigma_i| \cdot \left\|\frac{\partial \mathcal{L}}{\partial \sigma_i}\right\|
$$

é‡è¦åº¦ãŒä½ã„ç‰¹ç•°å€¤ã‚’pruningï¼ˆå‰Šé™¤ï¼‰:

$$
\Lambda' = \text{diag}(\sigma_1, \dots, \sigma_{r'}) \quad \text{where } r' < r
$$

#### Step 1: SVD-based LoRAåˆæœŸåŒ–

é€šå¸¸ã®LoRAåˆæœŸåŒ– $(B, A)$ ã‚’SVDåˆ†è§£:

$$
\begin{aligned}
A &\sim \mathcal{N}(0, \sigma^2), \quad B = 0 \\
[U, \Sigma, V^\top] &= \text{SVD}(BA) \quad \text{(conceptual)} \\
P &\leftarrow U[:, :r], \quad \Lambda \leftarrow \Sigma[:r, :r], \quad Q \leftarrow V[:, :r]
\end{aligned}
$$

åˆæœŸã§ã¯ $B=0$ ãªã®ã§ $\Lambda=0$ã€‚

#### Step 2: é‡è¦åº¦ãƒ™ãƒ¼ã‚¹ã®pruning

å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¾Œã€ç‰¹ç•°å€¤ã®é‡è¦åº¦ã‚’è¨ˆç®—:

$$
I_i = |\sigma_i| \cdot \left\|\frac{\partial \mathcal{L}}{\partial \sigma_i}\right\|_2
$$

é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆã—ã€ä¸‹ä½ $k\%$ ã‚’pruning:

$$
\Lambda_{\text{pruned}} = \text{diag}(\sigma_{i_1}, \dots, \sigma_{i_{r'}}) \quad \text{where } I_{i_1} \geq I_{i_2} \geq \dots \geq I_{i_{r'}}
$$

å¯¾å¿œã™ã‚‹ $P, Q$ ã®åˆ—ã‚‚å‰Šé™¤ã€‚

#### Step 3: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äºˆç®—ã®å†åˆ†é…

å‰Šæ¸›ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ã€ä»–ã®é‡è¦ãªå±¤ã«**å†åˆ†é…**:

$$
\begin{aligned}
\text{Budget}_\text{total} &= \sum_{\ell=1}^L r_\ell (d_\ell + k_\ell) \quad \text{(fixed)} \\
r_{\ell, \text{new}} &\leftarrow r_{\ell, \text{old}} + \Delta r_\ell \quad \text{where } \sum_\ell \Delta r_\ell = 0
\end{aligned}
$$

é«˜é‡è¦åº¦å±¤ã®ãƒ©ãƒ³ã‚¯ã‚’å¢—ã‚„ã—ã€ä½é‡è¦åº¦å±¤ã®ãƒ©ãƒ³ã‚¯ã‚’æ¸›ã‚‰ã™ã€‚

#### 3.7.4 AdaLoRAã®è¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

```python
# Pseudo-code for AdaLoRA training

# Initialize
for layer in layers:
    layer.P = orthogonal_init(d, r_init)
    layer.Q = orthogonal_init(k, r_init)
    layer.Lambda = zeros(r_init)  # trainable diagonal

# Training loop
for epoch in range(epochs):
    for batch in dataloader:
        # Forward + backward
        loss = compute_loss(batch)
        loss.backward()
        optimizer.step()

    # Importance-based pruning (every T steps)
    if epoch % T == 0:
        for layer in layers:
            # Compute importance
            I = abs(layer.Lambda) * grad_norm(layer.Lambda)

            # Prune low-importance singular values
            threshold = percentile(I, pruning_ratio)
            mask = I > threshold
            layer.Lambda = layer.Lambda[mask]
            layer.P = layer.P[:, mask]
            layer.Q = layer.Q[:, mask]

        # Redistribute budget to high-importance layers
        redistribute_budget(layers, total_budget)
```

#### 3.7.5 æ•°å€¤ä¾‹: AdaLoRA vs å›ºå®šãƒ©ãƒ³ã‚¯LoRA

å®Ÿé¨“è¨­å®š: DeBERTa-v3-base (86M) ã‚’GLUEã§Fine-tuning

| æ‰‹æ³• | ãƒ©ãƒ³ã‚¯è¨­å®š | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | GLUEå¹³å‡ |
|:-----|:----------|:------------|:---------|
| Full FT | - | 86M (100%) | 88.2 |
| LoRA | $r=8$ (all layers) | 0.3M (0.35%) | 85.4 |
| LoRA | $r=16$ | 0.6M (0.7%) | 86.1 |
| **AdaLoRA** | $r \in [2, 32]$ adaptive | 0.3M (0.35%) | **86.8** |

**è§£é‡ˆ**:
- AdaLoRAã¯ã€å›ºå®šãƒ©ãƒ³ã‚¯LoRA ($r=8$) ã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§**+1.4pt**å‘ä¸Š
- å›ºå®š $r=16$ (2å€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) ã‚ˆã‚Šé«˜æ€§èƒ½
- å±¤ã”ã¨ã®æœ€é©ãƒ©ãƒ³ã‚¯é…åˆ†ãŒæ€§èƒ½å‘ä¸Šã®éµ

#### 3.7.6 å±¤åˆ¥ãƒ©ãƒ³ã‚¯åˆ†å¸ƒã®å¯è¦–åŒ–

AdaLoRAè¨“ç·´å¾Œã®å±¤åˆ¥ãƒ©ãƒ³ã‚¯ï¼ˆDeBERTa-v3 12å±¤ï¼‰:

| å±¤ | åˆæœŸãƒ©ãƒ³ã‚¯ | æœ€çµ‚ãƒ©ãƒ³ã‚¯ | é‡è¦åº¦ã‚¹ã‚³ã‚¢ |
|:---|:----------|:----------|:------------|
| 1 (ä½å±¤) | 8 | 2 | 0.12 |
| 2 | 8 | 3 | 0.18 |
| 3 | 8 | 4 | 0.25 |
| ... | ... | ... | ... |
| 10 (é«˜å±¤) | 8 | 28 | 0.89 |
| 11 | 8 | 32 | 0.95 |
| 12 (æœ€çµ‚å±¤) | 8 | 32 | 1.00 |

**è¦³å¯Ÿ**: ä½å±¤ã®ãƒ©ãƒ³ã‚¯ãŒå¤§å¹…å‰Šæ¸›ï¼ˆ8â†’2ï¼‰ã€é«˜å±¤ãŒå¤§å¹…å¢—åŠ ï¼ˆ8â†’32ï¼‰ã€‚äºˆç®—ã¯å›ºå®šã€‚

### 3.8 LoRA+ â€” å­¦ç¿’ç‡ã®å±¤åˆ¥æœ€é©åŒ–

#### 3.8.1 å‹•æ©Ÿ: $B$ã¨$A$ã¯åŒã˜å­¦ç¿’ç‡ã§ã‚ˆã„ã‹ï¼Ÿ

LoRAã®åˆæœŸåŒ–: $B=0, A \sim \mathcal{N}(0, \sigma^2)$

Forward: $h = W_0 x + \frac{\alpha}{r} BA x$

è¨“ç·´é–‹å§‹æ™‚ã€$B=0$ ãªã®ã§å‡ºåŠ›ã¯ $W_0 x$ ã®ã¿ã€‚$B$ ã®å‹¾é…ã¯å¤§ãã„ãŒã€$A$ ã®å‹¾é…ã¯å°ã•ã„ï¼ˆ$B=0$ ã§æŠ‘åˆ¶ï¼‰ã€‚

**å•é¡Œ**: $B$ ã¨ $A$ ã«åŒã˜å­¦ç¿’ç‡ $\eta$ ã‚’ä½¿ã†ã¨ã€$A$ ã®å­¦ç¿’ãŒé…ã„ã€‚

LoRA+ [^16] ã¯ã€$B$ ã¨ $A$ ã«**ç•°ãªã‚‹å­¦ç¿’ç‡**ã‚’ä½¿ã†:

$$
\begin{aligned}
B &\leftarrow B - \eta_B \nabla_B \mathcal{L} \\
A &\leftarrow A - \eta_A \nabla_A \mathcal{L} \quad \text{where } \eta_A > \eta_B
\end{aligned}
$$

#### 3.8.2 ç†è«–çš„æ ¹æ‹ : Hessianã®æ¡ä»¶æ•°

$B$ ã¨ $A$ ã®Hessianï¼ˆ2æ¬¡å°é–¢æ•°è¡Œåˆ—ï¼‰ã®æ¡ä»¶æ•°ã‚’æ¯”è¼ƒ:

$$
\kappa(H_B) \ll \kappa(H_A)
$$

$B$ ã¯åˆæœŸå€¤0ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã€æå¤±ã¸ã®å½±éŸ¿ãŒç›´æ¥çš„ â†’ Hessianã®æ¡ä»¶æ•°ãŒå°ã•ã„ï¼ˆæœ€é©åŒ–ã—ã‚„ã™ã„ï¼‰

$A$ ã¯ $B$ ã‚’ä»‹ã—ã¦é–“æ¥çš„ã«å½±éŸ¿ â†’ Hessianã®æ¡ä»¶æ•°ãŒå¤§ãã„ï¼ˆæœ€é©åŒ–ãŒé›£ã—ã„ï¼‰

æ¡ä»¶æ•°ãŒå¤§ãã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯**å¤§ããªå­¦ç¿’ç‡**ãŒå¿…è¦ï¼ˆ[^17] Adaptive LRç†è«–ï¼‰ã€‚

#### 3.8.3 æœ€é©å­¦ç¿’ç‡æ¯”ã®å°å‡º

LoRA+è«–æ–‡ [^16] ã¯ã€ç†è«–çš„ã«æœ€é©ãªå­¦ç¿’ç‡æ¯”ã‚’å°å‡º:

$$
\frac{\eta_A}{\eta_B} = \frac{\|B\|_F}{\|A\|_F} \cdot \sqrt{\frac{d}{r}}
$$

**ç›´æ„Ÿçš„èª¬æ˜**:
- $\|B\|_F / \|A\|_F$: è¡Œåˆ—ã®ãƒãƒ«ãƒ æ¯”ï¼ˆå½±éŸ¿åº¦ã®è£œæ­£ï¼‰
- $\sqrt{d/r}$: æ¬¡å…ƒæ¯”ï¼ˆ$A$ ã®æ¬¡å…ƒ $r \times k$ vs $B$ ã®æ¬¡å…ƒ $d \times r$ï¼‰

å®Ÿç”¨ä¸Šã€è«–æ–‡ã¯**å›ºå®šæ¯” $\eta_A / \eta_B = 16$**ã‚’æ¨å¥¨ï¼ˆå¤šãã®ã‚¿ã‚¹ã‚¯ã§æœ€é©ã«è¿‘ã„ï¼‰ã€‚

#### 3.8.4 LoRA+ vs LoRAã®å®Ÿé¨“æ¯”è¼ƒ

å®Ÿé¨“è¨­å®š: RoBERTa-base (125M) ã‚’GLUEã§Fine-tuning

| æ‰‹æ³• | å­¦ç¿’ç‡è¨­å®š | åæŸã‚¹ãƒ†ãƒƒãƒ—æ•° | GLUEå¹³å‡ |
|:-----|:----------|:-------------|:---------|
| LoRA | $\eta_B = \eta_A = 3e-4$ | 30,000 | 85.2 |
| LoRA+ | $\eta_B = 3e-4, \eta_A = 4.8e-3$ (16x) | **15,000** | **85.9** |
| Full FT | $\eta = 1e-5$ | 50,000 | 86.1 |

**çµæœ**:
- LoRA+ã¯åæŸé€Ÿåº¦**2å€**ï¼ˆ30Kâ†’15K stepsï¼‰
- æ€§èƒ½ã‚‚+0.7ptå‘ä¸Š
- Full FTã«è¿‘ã„æ€§èƒ½ã‚’åŠåˆ†ã®æ™‚é–“ã§é”æˆ

#### 3.8.5 å®Ÿè£…ä¾‹

```python
# LoRA+ implementation with different learning rates

import torch
from torch.optim import AdamW

# LoRA parameters
B = torch.zeros(d, r, requires_grad=True)
A = torch.randn(r, k, requires_grad=True) / sqrt(k)

# Separate optimizers for B and A
optimizer_B = AdamW([B], lr=3e-4)
optimizer_A = AdamW([A], lr=3e-4 * 16)  # 16x learning rate

# Training loop
for batch in dataloader:
    loss = compute_loss(batch)
    loss.backward()

    optimizer_B.step()
    optimizer_A.step()

    optimizer_B.zero_grad()
    optimizer_A.zero_grad()
```

**æ³¨æ„**: PyTorchã®`param_groups`ã‚’ä½¿ãˆã°1ã¤ã®optimizerã§å®Ÿè£…å¯èƒ½:

```python
optimizer = AdamW([
    {'params': [B], 'lr': 3e-4},
    {'params': [A], 'lr': 3e-4 * 16}
])
```

### 3.9 VeRA â€” ãƒ©ãƒ³ãƒ€ãƒ å°„å½±LoRA

#### 3.9.1 å‹•æ©Ÿ: LoRAã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã•ã‚‰ã«å‰Šæ¸›

LoRAã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤§å¹…å‰Šæ¸›ã—ãŸãŒã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-3ç´šï¼‰ã§ã¯ä¾ç„¶ã¨ã—ã¦æ•°ç™¾MBã«ãªã‚‹ã€‚

**è¦³å¯Ÿ**: LoRAã® $B, A$ è¡Œåˆ—ã®å¤šãã®è¦ç´ ã¯**ä½é »åº¦æ›´æ–°**ã€‚

VeRA [^18] (Vector-based Random Matrix Adaptation) ã¯ã€$B, A$ ã‚’**å›ºå®šãƒ©ãƒ³ãƒ€ãƒ è¡Œåˆ—**ã«ã—ã€**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«**ã®ã¿ã‚’è¨“ç·´:

$$
\Delta W = b \odot (B_{\text{rand}} A_{\text{rand}}) \odot a^\top
$$

$B_{\text{rand}} \in \mathbb{R}^{d \times r}$, $A_{\text{rand}} \in \mathbb{R}^{r \times k}$: **å›ºå®š**ãƒ©ãƒ³ãƒ€ãƒ è¡Œåˆ—ï¼ˆè¨“ç·´ä¸è¦ï¼‰
$b \in \mathbb{R}^d$, $a \in \mathbb{R}^k$: **trainable**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«
$\odot$: Hadamardç©ï¼ˆè¦ç´ ã”ã¨ã®ç©ï¼‰

#### 3.9.2 VeRAã®å®šå¼åŒ–

**ã‚¹ãƒ†ãƒƒãƒ—1**: ãƒ©ãƒ³ãƒ€ãƒ è¡Œåˆ—ã®ç”Ÿæˆï¼ˆåˆæœŸåŒ–æ™‚1å›ã®ã¿ï¼‰

$$
\begin{aligned}
B_{\text{rand}} &\sim \mathcal{N}(0, \sigma_B^2), \quad \sigma_B = \frac{1}{\sqrt{r}} \\
A_{\text{rand}} &\sim \mathcal{N}(0, \sigma_A^2), \quad \sigma_A = \frac{1}{\sqrt{k}}
\end{aligned}
$$

ã“ã‚Œã‚‰ã¯**å…¨å±¤ã§å…±æœ‰**ï¼ˆã•ã‚‰ãªã‚‹ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰ã€‚

**ã‚¹ãƒ†ãƒƒãƒ—2**: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã®åˆæœŸåŒ–

$$
b = \mathbf{1}_d, \quad a = \mathbf{1}_k \quad \text{(all ones)}
$$

åˆæœŸçŠ¶æ…‹ã§ $\Delta W = B_{\text{rand}} A_{\text{rand}}$ ï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ‘‚å‹•ï¼‰ã€‚

**ã‚¹ãƒ†ãƒƒãƒ—3**: Forward pass

$$
h = W_0 x + \frac{\alpha}{r} \left(b \odot (B_{\text{rand}} (a \odot (A_{\text{rand}} x)))\right)
$$

**ã‚¹ãƒ†ãƒƒãƒ—4**: å‹¾é…è¨ˆç®—

$b, a$ ã®ã¿trainableã€$B_{\text{rand}}, A_{\text{rand}}$ ã¯å›ºå®š:

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial b} &= \frac{\alpha}{r} (B_{\text{rand}} (a \odot (A_{\text{rand}} x))) \odot \frac{\partial \mathcal{L}}{\partial h} \\
\frac{\partial \mathcal{L}}{\partial a} &= \frac{\alpha}{r} A_{\text{rand}}^\top (B_{\text{rand}}^\top (b \odot \frac{\partial \mathcal{L}}{\partial h}))
\end{aligned}
$$

#### 3.9.3 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã®è¨ˆç®—

| æ‰‹æ³• | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | å‰Šæ¸›ç‡ï¼ˆ$d=k=4096, r=8$ï¼‰ |
|:-----|:------------|:-------------------------|
| LoRA | $r(d+k) = 8 \times 8192 = 65,536$ | 256x vs Full FT |
| VeRA | $d + k = 8,192$ | **2,048x** vs Full FT |
| VeRAå‰Šæ¸›ç‡ï¼ˆvs LoRAï¼‰ | | **8x** |

GPT-3 (175B) ã®å ´åˆ:
- LoRA: ~200MB
- VeRA: **~25MB**

#### 3.9.4 ãªãœãƒ©ãƒ³ãƒ€ãƒ è¡Œåˆ—ã§æ©Ÿèƒ½ã™ã‚‹ã‹ï¼Ÿ

**ç†è«–çš„æ ¹æ‹ **: Johnson-Lindenstrauss Lemma [^19]

ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã¯ã€é«˜æ¬¡å…ƒç©ºé–“ã®è·é›¢ã‚’**é«˜ç¢ºç‡ã§ä¿å­˜**:

$$
(1-\epsilon)\|x-y\|^2 \leq \|Rx - Ry\|^2 \leq (1+\epsilon)\|x-y\|^2
$$

$R$: ãƒ©ãƒ³ãƒ€ãƒ è¡Œåˆ—ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰

VeRAã¯ã€LoRAã®å­¦ç¿’å¯èƒ½ãªéƒ¨åˆ†ç©ºé–“ã‚’**ãƒ©ãƒ³ãƒ€ãƒ éƒ¨åˆ†ç©ºé–“**ã§è¿‘ä¼¼ã€‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ« $b, a$ ãŒã€ãã®éƒ¨åˆ†ç©ºé–“å†…ã§ã®æœ€é©åŒ–ã‚’è¡Œã†ã€‚

#### 3.9.5 VeRA vs LoRAã®å®Ÿé¨“æ¯”è¼ƒ

å®Ÿé¨“è¨­å®š: GPT-2 Medium (355M) ã‚’E2E NLGã§Fine-tuning

| æ‰‹æ³• | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | BLEU | ROUGE-L | è¨“ç·´æ™‚é–“ |
|:-----|:------------|:-----|:--------|:---------|
| Full FT | 355M (100%) | 68.2 | 53.9 | 100% |
| LoRA | 0.35M (0.1%) | 67.8 | 53.4 | 62% |
| **VeRA** | 0.044M (0.0125%) | **67.5** | **53.2** | **58%** |

**çµæœ**:
- VeRAã¯LoRAã®**1/8ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ã§ã€æ€§èƒ½å·®ã‚ãšã‹-0.3pt (BLEU)
- è¨“ç·´æ™‚é–“ã‚‚LoRAã‚ˆã‚Š4%é«˜é€Ÿï¼ˆè¡Œåˆ—ã‚µã‚¤ã‚ºãŒå°ã•ã„ãŸã‚ï¼‰

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**: VeRAã¯åˆæœŸåŒ–ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã«ä¾å­˜ã€‚è¤‡æ•°å›è©¦è¡Œã—ã¦æœ€è‰¯ã®seedã‚’é¸ã¶å¿…è¦ãŒã‚ã‚‹ï¼ˆè«–æ–‡ã§ã¯5 seedså¹³å‡ï¼‰ã€‚

### 3.10 LoRA Composition â€” è¤‡æ•°LoRAã®åˆæˆ

#### 3.10.1 å‹•æ©Ÿ: ã‚¿ã‚¹ã‚¯æ¨ªæ–­çŸ¥è­˜ã®æ´»ç”¨

è¤‡æ•°ã‚¿ã‚¹ã‚¯ã§LoRAã‚’è¨“ç·´ã—ãŸå ´åˆã€ãã‚Œã‚‰ã‚’**çµ„ã¿åˆã‚ã›ã¦**æ–°ã‚¿ã‚¹ã‚¯ã«é©å¿œã§ãã‚‹ã‹ï¼Ÿ

ä¾‹:
- LoRA$_A$: è¦ç´„ã‚¿ã‚¹ã‚¯
- LoRA$_B$: ç¿»è¨³ã‚¿ã‚¹ã‚¯
- æ–°ã‚¿ã‚¹ã‚¯: å¤šè¨€èªè¦ç´„ï¼ˆè¦ç´„ + ç¿»è¨³ã®åˆæˆï¼‰

LoRA Composition [^20] ã¯ã€è¤‡æ•°ã® $(B_i, A_i)$ ã‚’**ç·šå½¢çµåˆ**:

$$
\Delta W_{\text{comp}} = \sum_{i=1}^N \lambda_i B_i A_i
$$

$\lambda_i$: åˆæˆé‡ã¿ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ or å­¦ç¿’å¯èƒ½ï¼‰

#### 3.10.2 åˆæˆæ‰‹æ³•ã®3ãƒ‘ã‚¿ãƒ¼ãƒ³

**1. åŠ ç®—åˆæˆï¼ˆLinear Compositionï¼‰**

$$
h = W_0 x + \frac{\alpha}{r} \sum_{i=1}^N \lambda_i B_i A_i x
$$

æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã€‚$\lambda_i$ ã¯æ‰‹å‹•èª¿æ•´ or ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã€‚

**2. å­¦ç¿’å¯èƒ½é‡ã¿åˆæˆï¼ˆLearnable Compositionï¼‰**

$$
\lambda_i \leftarrow \lambda_i - \eta \frac{\partial \mathcal{L}}{\partial \lambda_i}
$$

$\lambda_i$ ã‚’æ–°ã‚¿ã‚¹ã‚¯ã®æ¤œè¨¼ã‚»ãƒƒãƒˆã§æœ€é©åŒ–ï¼ˆæ•°ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿ã€é«˜é€Ÿï¼‰ã€‚

**3. ã‚¿ã‚¹ã‚¯ãƒ™ã‚¯ãƒˆãƒ«ç®—è¡“ï¼ˆTask Arithmeticï¼‰**

Task Vector [^21] ã®è€ƒãˆæ–¹ã‚’é©ç”¨:

$$
\Delta W_{\text{new}} = \Delta W_A + \Delta W_B - \Delta W_C
$$

ä¾‹: ã€Œè¦ç´„ + ç¿»è¨³ - è³ªå•å¿œç­”ã€ã§å¤šè¨€èªè¦ç´„ã‚’æ§‹æˆã€‚

#### 3.10.3 æ•°å€¤ä¾‹: LoRA Compositionå®Ÿé¨“

å®Ÿé¨“è¨­å®š: T5-base (220M)ã€3ã‚¿ã‚¹ã‚¯ã®LoRAã‚’åˆæˆ

| ã‚¿ã‚¹ã‚¯ | å€‹åˆ¥LoRAæ€§èƒ½ | åˆæˆå¾Œæ€§èƒ½ | æ–°ã‚¿ã‚¹ã‚¯é©å¿œ |
|:-------|:------------|:----------|:------------|
| è¦ç´„ | ROUGE 42.1 | 41.8 (-0.3) | - |
| ç¿»è¨³ | BLEU 28.5 | 28.3 (-0.2) | - |
| QA | F1 85.2 | 85.0 (-0.2) | - |
| **å¤šè¨€èªè¦ç´„**ï¼ˆæ–°ï¼‰ | - | - | ROUGE **38.4** |

**è§£é‡ˆ**:
- å€‹åˆ¥ã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ã¯å¾®æ¸›ï¼ˆ-0.2~0.3ptï¼‰
- æ–°ã‚¿ã‚¹ã‚¯ï¼ˆå¤šè¨€èªè¦ç´„ï¼‰ã‚’**ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆ**ã§38.4é”æˆï¼ˆfrom-scratchã®35.1ã‚ˆã‚Šé«˜ã„ï¼‰
- åˆæˆã«ã‚ˆã‚Šã€è¿½åŠ è¨“ç·´ãªã—ã§æ–°ã‚¿ã‚¹ã‚¯ã«é©å¿œ

#### 3.10.4 ç†è«–çš„è§£é‡ˆ: éƒ¨åˆ†ç©ºé–“ã®é‡ã­åˆã‚ã›

LoRAã® $\Delta W_i = B_i A_i$ ã¯ã€$r$ æ¬¡å…ƒã®éƒ¨åˆ†ç©ºé–“ $\mathcal{S}_i \subset \mathbb{R}^{d \times k}$ ã‚’å®šç¾©:

$$
\mathcal{S}_i = \text{span}(B_i A_i)
$$

åˆæˆ $\sum_i \lambda_i B_i A_i$ ã¯ã€éƒ¨åˆ†ç©ºé–“ã®**ç·šå½¢çµåˆ**:

$$
\mathcal{S}_{\text{comp}} = \text{span}\left(\bigcup_{i=1}^N \mathcal{S}_i\right)
$$

ã‚¿ã‚¹ã‚¯é–“ã§**å…±é€šéƒ¨åˆ†ç©ºé–“**ãŒã‚ã‚Œã°ã€åˆæˆãŒåŠ¹æœçš„ã€‚

:::message
**é€²æ—: 70% å®Œäº†** AdaLoRAï¼ˆãƒ©ãƒ³ã‚¯é©å¿œï¼‰ã€LoRA+ï¼ˆå­¦ç¿’ç‡æœ€é©åŒ–ï¼‰ã€VeRAï¼ˆãƒ©ãƒ³ãƒ€ãƒ å°„å½±ï¼‰ã€LoRA Compositionï¼ˆè¤‡æ•°LoRAåˆæˆï¼‰ã®æœ€æ–°æ‰‹æ³•ã‚’è¿½åŠ ã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
