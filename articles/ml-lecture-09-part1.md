---
title: "ç¬¬9å›: NNåŸºç¤&å¤‰åˆ†æ¨è«–&ELBO â€” Pythonåœ°ç„ã‹ã‚‰Rustæ•‘æ¸ˆã¸ ã€å‰ç·¨ã€‘ç†è«–ç·¨"
emoji: "ğŸ§ "
type: "tech"
topics: ["machinelearning", "deeplearning", "variationalinference", "rust", "python"]
published: true
---


# ç¬¬9å›: NNåŸºç¤ï¼ˆMLP/CNN/RNNï¼‰& å¤‰åˆ†æ¨è«– & ELBO

> **Course II: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ï¼ˆç¬¬9-18å›ï¼‰ã®é–‹å¹•**
>
> æœ¬è¬›ç¾©ã‹ã‚‰ã€Course Iï¼ˆæ•°å­¦åŸºç¤ç·¨ï¼‰ã§ç²å¾—ã—ãŸæ­¦å™¨ã‚’ä½¿ã„ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ç†è«–ã¨å®Ÿè£…ã«æŒ‘ã‚€ã€‚
> **æ–°è¨€èªç™»å ´**: ğŸ¦€ Ruståˆç™»å ´ â€” Pythonåœ°ç„â†’ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã§50xé«˜é€ŸåŒ–ã®è¡æ’ƒã‚’ä½“æ„Ÿã€‚

:::message
**å‰æçŸ¥è­˜**: Course I ç¬¬1-8å›å®Œäº†
**åˆ°é”ç›®æ¨™**: NNåŸºç¤ç¿’å¾—ã€å¤‰åˆ†æ¨è«–ãƒ»ELBOã®å®Œå…¨ç†è§£ã€Ruståˆä½“é¨“ã§ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã®å¨åŠ›ã‚’å®Ÿæ„Ÿ
**æ‰€è¦æ™‚é–“**: ç´„3æ™‚é–“
**é€²æ—**: Course II å…¨ä½“ã®10% (1/10å›)
:::

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ELBOã‚’3è¡Œã§å‹•ã‹ã™

```python
import numpy as np

# ELBO = E[log p(x|z)] - KL[q(z|x) || p(z)]
z = np.random.randn(100, 10)  # ã‚µãƒ³ãƒ—ãƒ«100å€‹ã€æ½œåœ¨æ¬¡å…ƒ10
recon_loss = -np.mean(np.sum(z**2, axis=1))  # å†æ§‹æˆé …(ç°¡æ˜“ç‰ˆ)
kl_loss = 0.5 * np.mean(np.sum(z**2, axis=1))  # KLæ­£å‰‡åŒ–é …(ã‚¬ã‚¦ã‚¹ä»®å®š)
elbo = recon_loss - kl_loss
print(f"ELBO = {elbo:.4f}  (å†æ§‹æˆ: {recon_loss:.4f}, KL: {kl_loss:.4f})")
```

**å‡ºåŠ›ä¾‹**:
```
ELBO = -7.5234  (å†æ§‹æˆ: -5.0156, KL: 5.0156)
```

**ã“ã®3è¡Œã®æ•°å­¦çš„æ„å‘³**:
$$
\mathcal{L}(\theta, \phi; \mathbf{x}) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})] - D_\text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))
$$

ã“ã‚ŒãŒ **å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€(VAE)** ã®æå¤±é–¢æ•°ã€‚ç¬¬10å›ã§å®Œå…¨å±•é–‹ã™ã‚‹ã€‚

:::message
**é€²æ—: 3%å®Œäº†** â€” ELBOã®"å½¢"ã‚’è¦‹ãŸã€‚æ¬¡ã¯æ•°å¼ã®è£å´ã¸ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” NNåŸºç¤Ã—3 & ELBOã®å…¨ä½“åƒ

### 1.1 MLP (Multi-Layer Perceptron) â€” å…¨çµåˆå±¤ã®ç©ã¿é‡ã­

```python
import numpy as np

def relu(x):
    return np.maximum(0, x)

def mlp_forward(x, W1, b1, W2, b2):
    """2å±¤MLP: x -> h1 -> y"""
    h1 = relu(x @ W1 + b1)  # éš ã‚Œå±¤: ReLUæ´»æ€§åŒ–
    y = h1 @ W2 + b2         # å‡ºåŠ›å±¤: ç·šå½¢
    return y

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
d_in, d_hidden, d_out = 784, 128, 10  # MNIST: 28x28=784 -> 128 -> 10
W1 = np.random.randn(d_in, d_hidden) * 0.01
b1 = np.zeros(d_hidden)
W2 = np.random.randn(d_hidden, d_out) * 0.01
b2 = np.zeros(d_out)

# ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
x = np.random.randn(32, 784)  # ãƒãƒƒãƒã‚µã‚¤ã‚º32
logits = mlp_forward(x, W1, b1, W2, b2)
print(f"å‡ºåŠ›shape: {logits.shape}")  # (32, 10)
```

**æ•°å¼**:
$$
\begin{aligned}
\mathbf{h}_1 &= \text{ReLU}(\mathbf{x} W_1 + \mathbf{b}_1) \\
\mathbf{y} &= \mathbf{h}_1 W_2 + \mathbf{b}_2
\end{aligned}
$$

**MLP ã®æœ¬è³ª**: ç·šå½¢å¤‰æ› â†’ éç·šå½¢æ´»æ€§åŒ– â†’ ç·šå½¢å¤‰æ› ã®ç¹°ã‚Šè¿”ã—ã€‚

### 1.2 CNN (Convolutional Neural Network) â€” å¹³è¡Œç§»å‹•ç­‰å¤‰æ€§

```python
# ç•³ã¿è¾¼ã¿æ¼”ç®—ã®ç›´æ„Ÿ(1Dç°¡æ˜“ç‰ˆ)
x = np.array([1, 2, 3, 4, 5])
kernel = np.array([0.5, 1.0, 0.5])

# æ‰‹å‹•ç•³ã¿è¾¼ã¿
output = []
for i in range(len(x) - len(kernel) + 1):
    output.append(np.sum(x[i:i+len(kernel)] * kernel))
print(f"Convolution output: {output}")  # [2.0, 3.0, 4.0]
```

**æ•°å¼** (2Dç•³ã¿è¾¼ã¿):
$$
(\mathbf{X} * \mathbf{K})_{ij} = \sum_{m,n} \mathbf{X}_{i+m, j+n} \mathbf{K}_{m,n}
$$

**CNNã®æœ¬è³ª**: **å¹³è¡Œç§»å‹•ç­‰å¤‰æ€§** (translation equivariance) â€” å…¥åŠ›ã‚’ã‚·ãƒ•ãƒˆã™ã‚‹ã¨ã€å‡ºåŠ›ã‚‚åŒã˜ã ã‘ã‚·ãƒ•ãƒˆã€‚ç”»åƒã®å±€æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã«æœ€é©ã€‚

**é™ç•Œã®äºˆå‘Š**: å—å®¹é‡ãŒæœ‰é™ â†’ å¤§åŸŸçš„æ–‡è„ˆã®ç²å¾—ãŒå›°é›£ â†’ Attentionã¸(ç¬¬14å›ã§å›å)ã€‚

### 1.3 RNN (Recurrent Neural Network) â€” éš ã‚ŒçŠ¶æ…‹ã®é€æ¬¡æ›´æ–°

```python
def rnn_step(x_t, h_prev, W_xh, W_hh, b_h):
    """RNNã®1ã‚¹ãƒ†ãƒƒãƒ—: h_t = tanh(x_t W_xh + h_{t-1} W_hh + b_h)"""
    h_t = np.tanh(x_t @ W_xh + h_prev @ W_hh + b_h)
    return h_t

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
d_input, d_hidden = 50, 128
W_xh = np.random.randn(d_input, d_hidden) * 0.01
W_hh = np.random.randn(d_hidden, d_hidden) * 0.01
b_h = np.zeros(d_hidden)

# æ™‚ç³»åˆ—å‡¦ç†
seq_length = 10
h = np.zeros(d_hidden)
for t in range(seq_length):
    x_t = np.random.randn(d_input)
    h = rnn_step(x_t, h, W_xh, W_hh, b_h)
print(f"æœ€çµ‚éš ã‚ŒçŠ¶æ…‹: {h[:5]}")  # æœ€åˆã®5æ¬¡å…ƒã®ã¿è¡¨ç¤º
```

**æ•°å¼**:
$$
\mathbf{h}_t = \tanh(\mathbf{x}_t W_{xh} + \mathbf{h}_{t-1} W_{hh} + \mathbf{b}_h)
$$

**RNNã®æœ¬è³ª**: éš ã‚ŒçŠ¶æ…‹ $\mathbf{h}_t$ ãŒæ™‚ç³»åˆ—æƒ…å ±ã‚’åœ§ç¸®ä¿æŒã€‚

**é™ç•Œã®äºˆå‘Š**: å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™º â†’ LSTM/GRUã§ç·©å’Œ â†’ ãã‚Œã§ã‚‚é•·è·é›¢ä¾å­˜ã¯å›°é›£ â†’ Attentionã¸(ç¬¬14å›)ã€‚

### 1.4 åŒ–çŸ³ã‹ã‚‰ã®è„±å´ã¸ã®ä¼ç·š

| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | åˆ©ç‚¹ | è‡´å‘½çš„é™ç•Œ |
|:--------------|:-----|:----------|
| **MLP** | ã‚·ãƒ³ãƒ—ãƒ« | æ§‹é€ ã‚’ç„¡è¦–ï¼ˆç”»åƒã§ä½ç½®æƒ…å ±å–ªå¤±ï¼‰ |
| **CNN** | å¹³è¡Œç§»å‹•ç­‰å¤‰æ€§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰ | å—å®¹é‡æœ‰é™ â†’ å¤§åŸŸçš„æ–‡è„ˆå›°é›£ |
| **RNN** | å¯å¤‰é•·ç³»åˆ—å‡¦ç† | å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºã€é€æ¬¡å‡¦ç†=ä¸¦åˆ—åŒ–ä¸å¯ |

**ç¬¬14å›ã®äºˆå‘Š**: CNN/RNNã®é™ç•Œã‚’å…‹æœã™ã‚‹ **Self-Attention** ã¸ â€” å…¨ç³»åˆ—å‚ç…§ + ä¸¦åˆ—è¨ˆç®—å¯èƒ½ã€‚

### 1.5 ELBO â€” å¤‰åˆ†æ¨è«–ã®å¿ƒè‡“éƒ¨

**å•é¡Œè¨­å®š**: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ã‹ã‚‰æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã®äº‹å¾Œåˆ†å¸ƒ $p(\mathbf{z}|\mathbf{x})$ ã‚’æ¨å®šã—ãŸã„ã€‚

**å›°é›£**: å‘¨è¾ºå°¤åº¦ $p(\mathbf{x}) = \int p(\mathbf{x}|\mathbf{z})p(\mathbf{z}) d\mathbf{z}$ ãŒè¨ˆç®—ä¸èƒ½ (ç¬¬8å›ã§å­¦ã‚“ã )ã€‚

**è§£æ±ºç­–**: è¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒ $q(\mathbf{z}|\mathbf{x})$ ã‚’å°å…¥ã—ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’æœ€å°åŒ–ã€‚

**ELBOå°å‡º** (ç¬¬8å›ã®Jensenä¸ç­‰å¼ã‚’ä½¿ã†):

$$
\begin{aligned}
\log p(\mathbf{x}) &= \log \int p(\mathbf{x}, \mathbf{z}) d\mathbf{z} \\
&= \log \int q(\mathbf{z}|\mathbf{x}) \frac{p(\mathbf{x}, \mathbf{z})}{q(\mathbf{z}|\mathbf{x})} d\mathbf{z} \\
&= \log \mathbb{E}_{q(\mathbf{z}|\mathbf{x})} \left[ \frac{p(\mathbf{x}, \mathbf{z})}{q(\mathbf{z}|\mathbf{x})} \right] \\
&\geq \mathbb{E}_{q(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p(\mathbf{x}, \mathbf{z})}{q(\mathbf{z}|\mathbf{x})} \right] \quad \text{(Jensenä¸ç­‰å¼)} \\
&= \mathbb{E}_{q(\mathbf{z}|\mathbf{x})} [\log p(\mathbf{x}, \mathbf{z})] - \mathbb{E}_{q(\mathbf{z}|\mathbf{x})} [\log q(\mathbf{z}|\mathbf{x})] \\
&\equiv \mathcal{L}(\theta, \phi; \mathbf{x}) \quad \text{(ELBO)}
\end{aligned}
$$

**ELBOåˆ†è§£** (2ã¤ã®é …):

$$
\begin{aligned}
\mathcal{L}(\theta, \phi; \mathbf{x}) &= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] - D_\text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})) \\
&= \text{å†æ§‹æˆé …} - \text{KLæ­£å‰‡åŒ–é …}
\end{aligned}
$$

| Zone 1ã®è¦ç‚¹ | èª¬æ˜ |
|:------------|:-----|
| **MLP/CNN/RNN** | NNåŸºç¤3ç¨® â€” å…¨ã¦ã€Œé™ç•Œã€ã‚’æŠ±ãˆã‚‹ |
| **åŒ–çŸ³ã¸ã®é“** | CNN/RNNã¯å¾Œã«Attentionã«ç½®ãæ›ã‚ã‚‹(ç¬¬14å›) |
| **ELBO** | $\log p(\mathbf{x}) \geq \mathcal{L}$ â€” è¨ˆç®—ä¸èƒ½ãªå¯¾æ•°å°¤åº¦ã‚’ä¸‹ã‹ã‚‰è¿‘ä¼¼ |

:::message
**é€²æ—: 10%å®Œäº†** â€” NNã®åŸºç¤ã¨ELBOã®å…¨ä½“åƒã‚’æ´ã‚“ã ã€‚æ¬¡ã¯å‹•æ©Ÿã¨ä½ç½®ã¥ã‘ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ã‚³ãƒ¼ã‚¹æ¦‚è«–ã¨å­¦ç¿’æˆ¦ç•¥

### 2.1 Course I ã‹ã‚‰ Course II ã¸ â€” é“å…·ã¯æƒã£ãŸã€ã„ã‚ˆã„ã‚ˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¸

Course Iï¼ˆç¬¬1-8å›ï¼‰ã§8å›ã«ã‚ãŸã‚‹æ•°å­¦ã®æ—…ã‚’å®Œèµ°ã—ãŸã€‚ç¬¬1å›ã®ã‚®ãƒªã‚·ãƒ£æ–‡å­—ã¨æ•°å¼è¨˜æ³•ã‹ã‚‰å§‹ã¾ã‚Šã€ç·šå½¢ä»£æ•°ãƒ»ç¢ºç‡è«–ãƒ»æ¸¬åº¦è«–ãƒ»æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–ãƒ»çµ±è¨ˆçš„æ¨è«–ãƒ»EMç®—æ³•ã¾ã§ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«å¿…è¦ãªå…¨ã¦ã®æ•°å­¦çš„åŸºç›¤ã‚’ç²å¾—ã—ãŸã€‚

**ç¬¬8å›ã®æœ€å¾Œã§è¦‹ãŸé€šã‚Šã€Course I ã®æ­¦å™¨ã¯ Course II ã®å…¨ã¦ã®å ´é¢ã§ä½¿ã‚ã‚Œã‚‹ã€‚**

- ç¬¬6å›ã®KL divergenceã¯ã€VAEã®æ­£å‰‡åŒ–é …ã€GANã®ç›®çš„é–¢æ•°ã€æœ€é©è¼¸é€ã®åŒå¯¾è¡¨ç¾ã¨ã—ã¦å†ç™»å ´ã™ã‚‹ã€‚
- ç¬¬8å›ã®ELBOã¯ã€ç¬¬9å›ã§å¤‰åˆ†æ¨è«–ã®ä¸€èˆ¬ç†è«–ã¨ã—ã¦æ‹¡å¼µã•ã‚Œã€ç¬¬10å›ã®VAEã®æå¤±é–¢æ•°ã«ç›´çµã™ã‚‹ã€‚
- ç¬¬5å›ã®æ¸¬åº¦è«–ã¯ã€ç¬¬11å›ã®æœ€é©è¼¸é€ç†è«–ã¨ã€Course IVã®Diffusion Modelsã®æ•°å­¦çš„åŸºç›¤ã¨ãªã‚‹ã€‚

**ã“ã“ã¾ã§æ¥ãŸã‚ãªãŸã¯ã€ã‚‚ã†åˆå¿ƒè€…ã§ã¯ãªã„ã€‚** è«–æ–‡ã®æ•°å¼ã«æ€¯ã¾ãšã€å°å‡ºã‚’è¿½ã„ã€èƒŒæ™¯ã«ã‚ã‚‹æ•°å­¦ã‚’ç†è§£ã§ãã‚‹åŠ›ãŒã‚ã‚‹ã€‚

Course IIã§ã¯ã€ãã®æ­¦å™¨ã‚’ä½¿ã£ã¦ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ç†è«–ã¨å®Ÿè£…ã‚’å­¦ã¶ã€‚VAEãƒ»GANãƒ»æœ€é©è¼¸é€ãƒ»è‡ªå·±å›å¸°ãƒ»Attentionãƒ»SSMãƒ»Hybridã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ â€” å…¨10å›ã®æ—…è·¯ãŒã€ç¬¬9å›ã®ä»Šæ—¥ã‹ã‚‰å§‹ã¾ã‚‹ã€‚

### 2.2 Course II ã®å…¨ä½“åƒ â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ 10å›ã®æ—…è·¯

```mermaid
graph TD
    Start[ç¬¬9å›: NNåŸºç¤ & å¤‰åˆ†æ¨è«– & ELBO] --> L10[ç¬¬10å›: VAE]
    L10 --> L11[ç¬¬11å›: æœ€é©è¼¸é€ç†è«–]
    L11 --> L12[ç¬¬12å›: GAN]
    L12 --> L13[ç¬¬13å›: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«]
    L13 --> L14[ç¬¬14å›: Attention - åŒ–çŸ³ã‹ã‚‰ã®è„±å´]
    L14 --> L15[ç¬¬15å›: Attentioné¡ä¼¼æ‰‹æ³• & Sparse]
    L15 --> L16[ç¬¬16å›: SSM & Mamba]
    L16 --> L17[ç¬¬17å›: Mambaç™ºå±•]
    L17 --> End[ç¬¬18å›: Hybrid + Course II èª­äº†]

    style Start fill:#ff6b6b
    style L10 fill:#4ecdc4
    style L11 fill:#95e1d3
    style L12 fill:#f38181
    style L13 fill:#aa96da
    style L14 fill:#fcbad3
    style L15 fill:#ffffd2
    style L16 fill:#a8d8ea
    style L17 fill:#ffaaa5
    style End fill:#ff8b94
```

**Course II ã®æµã‚Œ**:

1. **å¤‰åˆ†æ¨è«–(ç¬¬9å›)** â†’ VAE(ç¬¬10å›) â€” å°¤åº¦ãƒ™ãƒ¼ã‚¹ç”Ÿæˆã®åŸºç¤
2. **æœ€é©è¼¸é€(ç¬¬11å›)** â†’ GAN(ç¬¬12å›) â€” æ•µå¯¾çš„å­¦ç¿’ã®ç†è«–åŸºç›¤
3. **è‡ªå·±å›å¸°(ç¬¬13å›)** â€” å°¤åº¦ã‚’å³å¯†è¨ˆç®—
4. **Attention(ç¬¬14-15å›)** â€” RNN/CNNã‹ã‚‰ã®è„±å´
5. **SSMãƒ»Mamba(ç¬¬16-17å›)** â€” Attentionä»£æ›¿ã®æœ€å‰ç·š
6. **Hybrid(ç¬¬18å›)** â€” æœ€å¼·ã®çµ„ã¿åˆã‚ã›æ¢ç´¢

### 2.2 Course I æ•°å­¦ãŒã©ã“ã§ä½¿ã‚ã‚Œã‚‹ã‹ â€” å¯¾å¿œè¡¨

| Course I è¬›ç¾© | ç²å¾—ã—ãŸæ•°å­¦çš„æ­¦å™¨ | Course II ã§ã®ä½¿ç”¨ä¾‹ |
|:-------------|:-----------------|:--------------------|
| **ç¬¬2å› ç·šå½¢ä»£æ•°I** | ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã€å†…ç©ã€å›ºæœ‰å€¤ | Attention $QK^\top$ (ç¬¬14å›), æ½œåœ¨ç©ºé–“ $\mathbf{z} \in \mathbb{R}^d$ |
| **ç¬¬3å› ç·šå½¢ä»£æ•°II** | SVD, è¡Œåˆ—å¾®åˆ†, è‡ªå‹•å¾®åˆ† | VAE encoder/decoder ã®å‹¾é…è¨ˆç®— (ç¬¬10å›) |
| **ç¬¬4å› ç¢ºç‡è«–** | ç¢ºç‡åˆ†å¸ƒ, ãƒ™ã‚¤ã‚ºã®å®šç†, MLE | VAE ã® $p(\mathbf{x}\|\mathbf{z})$, $q(\mathbf{z}\|\mathbf{x})$ (ç¬¬10å›) |
| **ç¬¬5å› æ¸¬åº¦è«–** | æ¸¬åº¦ç©ºé–“, Browné‹å‹•, SDE | Diffusion ã®ç†è«–åŸºç›¤ (Course IV) |
| **ç¬¬6å› æƒ…å ±ç†è«–** | KL, ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼, Wasserstein | ELBO ã® KLé … (ç¬¬9-10å›), WGAN (ç¬¬12å›) |
| **ç¬¬7å› MLE** | æœ€å°¤æ¨å®š, Fisheræƒ…å ±é‡ | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ç›®çš„é–¢æ•°è¨­è¨ˆ (å…¨èˆ¬) |
| **ç¬¬8å› EMç®—æ³•** | ELBO, Jensenä¸ç­‰å¼ | VAE ã®ç†è«–åŸºç›¤ (ç¬¬10å›), VI ã®åå¾©æœ€é©åŒ– (ç¬¬9å›) |

**æ¥ç¶šã®æœ¬è³ª**: Course I ã¯ã€Œé“å…·ç®±ã€ã€Course II ã¯ã€Œé“å…·ã®ä½¿ã„æ–¹ã€ã‚’å­¦ã¶å ´ã€‚

### 2.3 ğŸâ†’ğŸ¦€(ç¬¬9å›)â†’âš¡(ç¬¬10å›) â€” è¨€èªç§»è¡Œãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

**ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬æˆ¦è¡“**:

```
ç¬¬1-4å›:  ğŸ Pythonä¿¡é ¼       ã€Œæ•°å¼ãŒãã®ã¾ã¾èª­ã‚ã‚‹ã€
ç¬¬5-8å›:  ğŸğŸ’¢ ä¸ç©ãªå½±       ã€Œ%timeit ã§è¨ˆæ¸¬...é…ããªã„ï¼Ÿã€
ç¬¬9å›:    ğŸğŸ”¥â†’ğŸ¦€ Rustç™»å ´    ã€Œ50xé€Ÿã„ï¼...ã ãŒCUDAç›´æ›¸ãï¼Ÿè‹¦ç—›...ã€
ç¬¬10å›:   âš¡ Juliaç™»å ´         ã€Œæ•°å¼ãŒ1å¯¾1...ã“ã‚“ãªã«ç¶ºéº—ã«æ›¸ã‘ã‚‹ã®ï¼Ÿã€
ç¬¬11-18å›: âš¡ğŸ¦€ å½¹å‰²åˆ†æ‹…å®šç€    ã€Œè¨“ç·´=Juliaã€æ¨è«–=Rustã€
```

**ä»Šå›ã®ä½“é¨“å†…å®¹**:

| è¨€èª | Zone | ä½“é¨“å†…å®¹ |
|:-----|:-----|:--------|
| ğŸ Python | Z1-Z3 | NNåŸºç¤, ELBOç†è«– (æ•°å¼ã®ç†è§£ã«é›†ä¸­) |
| ğŸğŸ’¢ Python | Z4 | ELBOè¨ˆç®— 100ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ â†’ 45ç§’ (Profileè¨ˆæ¸¬) |
| ğŸ¦€ Rust | Z4 | ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ + ã‚¹ãƒ©ã‚¤ã‚¹å‚ç…§ â†’ 0.8ç§’ (50xé€Ÿ) |
| ğŸ¦€ Rust | Z4 | **æ‰€æœ‰æ¨©ãƒ»å€Ÿç”¨ãƒ»ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ å…¥é–€** â€” é€Ÿã•ã®æºæ³‰ã‚’ç†è§£ |

### 2.4 ã“ã®ã‚³ãƒ¼ã‚¹ã‚’ä¿®äº†ã™ã‚‹ã¨ä½•ãŒã§ãã‚‹ã‹

**ãƒ“ãƒ•ã‚©ãƒ¼** (Course I ä¿®äº†æ™‚ç‚¹):
- è«–æ–‡ã®æ•°å¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã€Œèª­ã‚ã‚‹ã€
- MLE, EM, KL divergence ã®æ„å‘³ãŒåˆ†ã‹ã‚‹

**ã‚¢ãƒ•ã‚¿ãƒ¼** (Course II ä¿®äº†å¾Œ):
- **VAE/GAN/Diffusion ã®è«–æ–‡ãŒã€Œæ›¸ã‘ã‚‹ã€**
- æ‰‹æ³•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ•°å¼ã‚’å®Œå…¨ã«å°å‡ºã§ãã‚‹
- PyTorchã‚³ãƒ¼ãƒ‰ â†” æ•°å¼ãŒ1:1ã§å¯¾å¿œã§ãã‚‹
- Rust/Juliaã§é«˜é€Ÿå®Ÿè£…ãŒã§ãã‚‹

### 2.5 æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã€Œæ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«2026Springã€ã¨ã®æ¯”è¼ƒ

| è¦³ç‚¹ | æ¾å°¾ç ” (8å›) | æœ¬ã‚·ãƒªãƒ¼ã‚º (10å›) |
|:-----|:------------|:-----------------|
| **ç†è«–æ·±åº¦** | è«–æ–‡ãŒèª­ã‚ã‚‹ | **è«–æ–‡ãŒæ›¸ã‘ã‚‹** (å°å‡ºå®Œå…¨) |
| **å®Ÿè£…** | PyTorchã®ã¿ | **Python+Rust+Julia** (3è¨€èª) |
| **æ•°å­¦åŸºç¤** | å‰æçŸ¥è­˜æ‰±ã„ | **Course I 8å›ã§å¾¹åº•**  |
| **CNN/RNN** | ã‚¹ã‚­ãƒƒãƒ— | **ç¬¬9å›ã§åŸºç¤â†’ç¬¬14å›ã§é™ç•Œã‚’æ˜ç¤º** |
| **ELBO** | æ¦‚è¦ã®ã¿ | **3ã¤ã®å°å‡º + Rate-Distortionè¦–ç‚¹** |
| **OTç†è«–** | ãªã— | **ç¬¬11å›ã§å®Œå…¨å±•é–‹** (WGAN/FMåŸºç›¤) |
| **Attention** | 2å› | **4å›** (14-17å›: Attention/SSM/Hybrid) |

**å·®åˆ¥åŒ–ã®æœ¬è³ª**: æ¾å°¾ç ”ã¯ã€Œå¿œç”¨ã®ãŸã‚ã®æœ€ä½é™ã®ç†è«–ã€ã€æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ã€Œç†è«–ã®å®Œå…¨ç†è§£ + 3è¨€èªå®Ÿè£…åŠ›ã€ã€‚

### 2.6 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã§æ‰ãˆã‚‹ã€Œå¤‰åˆ†æ¨è«–ã€

1. **åœ§ç¸®ã®æ¯”å–©**:
   - æ½œåœ¨å¤‰æ•° $\mathbf{z}$ = ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ã®åœ§ç¸®è¡¨ç¾
   - ELBO = åœ§ç¸®ã®è³ª (å†æ§‹æˆç²¾åº¦ vs åœ§ç¸®ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•)

2. **ã‚²ãƒ¼ãƒ ã®æ¯”å–©**:
   - Encoder $q(\mathbf{z}|\mathbf{x})$ = åœ§ç¸®å™¨
   - Decoder $p(\mathbf{x}|\mathbf{z})$ = è§£å‡å™¨
   - KLé … = ã€Œæ¨™æº–çš„ãªåœ§ç¸®æ–¹å¼ $p(\mathbf{z})$ ã‹ã‚‰ã®é€¸è„±ãƒšãƒŠãƒ«ãƒ†ã‚£ã€

3. **æœ€é©åŒ–ã®æ¯”å–©**:
   - ELBOæœ€å¤§åŒ– = å¯¾æ•°å°¤åº¦ $\log p(\mathbf{x})$ ã®ä¸‹ç•Œã‚’æŠ¼ã—ä¸Šã’ã‚‹
   - VI = ã€Œè¨ˆç®—ã§ããªã„çœŸã®ç›®çš„é–¢æ•°ã€ã‚’ã€Œè¨ˆç®—ã§ãã‚‹ä»£ç†ç›®çš„é–¢æ•°ã€ã§è¿‘ä¼¼

| Zone 2ã®è¦ç‚¹ | èª¬æ˜ |
|:------------|:-----|
| **Course II å…¨ä½“** | VIâ†’VAEâ†’OTâ†’GANâ†’ARâ†’Attentionâ†’SSMâ†’Hybrid ã®10å› |
| **Course I æ¥ç¶š** | 8å›ã®æ•°å­¦ãŒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§å…¨ã¦ä½¿ã‚ã‚Œã‚‹ |
| **è¨€èªç§»è¡Œ** | ç¬¬9å› Ruståˆç™»å ´ â†’ ç¬¬10å› Juliaç™»å ´ |
| **å·®åˆ¥åŒ–** | æ¾å°¾ç ”ã®å®Œå…¨ä¸Šä½äº’æ› (ç†è«–Ã—å®Ÿè£…Ã—æœ€æ–°) |

:::message
**é€²æ—: 20%å®Œäº†** â€” ã‚³ãƒ¼ã‚¹å…¨ä½“ã®ä½ç½®ã¥ã‘ã‚’ç†è§£ã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” ç†è«–ã®å®Œå…¨å±•é–‹

### 3.1 NNåŸºç¤: MLPè©³èª¬

#### 3.1.1 é †ä¼æ’­ (Forward Propagation)

**å®šç¾©**: $L$ å±¤ MLP:

$$
\begin{aligned}
\mathbf{h}_0 &= \mathbf{x} \quad \text{(å…¥åŠ›å±¤)} \\
\mathbf{h}_\ell &= \sigma(\mathbf{h}_{\ell-1} W_\ell + \mathbf{b}_\ell), \quad \ell = 1, \ldots, L-1 \quad \text{(éš ã‚Œå±¤)} \\
\mathbf{y} &= \mathbf{h}_{L-1} W_L + \mathbf{b}_L \quad \text{(å‡ºåŠ›å±¤)}
\end{aligned}
$$

**è¨˜å·**:
- $\sigma$: æ´»æ€§åŒ–é–¢æ•° (ReLU, Sigmoid, Tanhç­‰)
- $W_\ell \in \mathbb{R}^{d_{\ell-1} \times d_\ell}$: é‡ã¿è¡Œåˆ—
- $\mathbf{b}_\ell \in \mathbb{R}^{d_\ell}$: ãƒã‚¤ã‚¢ã‚¹ãƒ™ã‚¯ãƒˆãƒ«

**æ´»æ€§åŒ–é–¢æ•°ã®ç¨®é¡**:

| é–¢æ•° | å¼ | å¾®åˆ† | æ€§è³ª |
|:-----|:---|:-----|:-----|
| **ReLU** | $\max(0, x)$ | $\mathbb{1}_{x>0}$ | å‹¾é…æ¶ˆå¤±è»½æ¸›ã€ç–æ´»æ€§åŒ– |
| **Sigmoid** | $\frac{1}{1+e^{-x}}$ | $\sigma(x)(1-\sigma(x))$ | $(0,1)$ å‡ºåŠ›ã€å‹¾é…æ¶ˆå¤±ã‚ã‚Š |
| **Tanh** | $\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $1 - \tanh^2(x)$ | $(-1,1)$ å‡ºåŠ›ã€ã‚¼ãƒ­ä¸­å¿ƒ |
| **Leaky ReLU** | $\max(\alpha x, x)$ ($\alpha=0.01$) | $\mathbb{1}_{x>0} + \alpha \mathbb{1}_{x \leq 0}$ | Dying ReLUå›é¿ |
| **GELU** | $x \Phi(x)$ | è¤‡é›‘ | Transformeræ¨™æº– |

**ãªãœReLUãŒæ¨™æº–ã‹**:
- å‹¾é…æ¶ˆå¤±å•é¡Œã®è»½æ¸› (Sigmoid/Tanhã¯é£½å’Œ)
- è¨ˆç®—ãŒé«˜é€Ÿ ($\max(0, x)$ ã¯æ¡ä»¶åˆ†å²ã®ã¿)
- ç–æ´»æ€§åŒ– (ç´„50%ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒã‚¼ãƒ­)

#### 3.1.2 é€†ä¼æ’­ (Backpropagation)

**ç›®çš„**: æå¤±é–¢æ•° $L$ ã®å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹å‹¾é…ã‚’è¨ˆç®—ã€‚

**é€£é–å¾‹** (ç¬¬3å›ã§å­¦ã‚“ã ):

$$
\frac{\partial L}{\partial W_\ell} = \frac{\partial L}{\partial \mathbf{h}_\ell} \frac{\partial \mathbf{h}_\ell}{\partial W_\ell}
$$

**ã‚¹ãƒ†ãƒƒãƒ—**:

1. **å‡ºåŠ›å±¤ã®å‹¾é…**:
   $$
   \frac{\partial L}{\partial \mathbf{y}} = \nabla_\mathbf{y} L
   $$

2. **é€†å‘ãã®é€£é–**:
   $$
   \frac{\partial L}{\partial \mathbf{h}_{\ell-1}} = \frac{\partial L}{\partial \mathbf{h}_\ell} \frac{\partial \mathbf{h}_\ell}{\partial \mathbf{h}_{\ell-1}}
   $$

3. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‹¾é…**:
   $$
   \begin{aligned}
   \frac{\partial L}{\partial W_\ell} &= \mathbf{h}_{\ell-1}^\top \frac{\partial L}{\partial \mathbf{z}_\ell} \\
   \frac{\partial L}{\partial \mathbf{b}_\ell} &= \frac{\partial L}{\partial \mathbf{z}_\ell}
   \end{aligned}
   $$
   ã“ã“ã§ $\mathbf{z}_\ell = \mathbf{h}_{\ell-1} W_\ell + \mathbf{b}_\ell$ (æ´»æ€§åŒ–å‰)ã€‚

**è¨ˆç®—ã‚°ãƒ©ãƒ•ä¾‹** (2å±¤MLP):

```mermaid
graph LR
    x[x] --> |W1| z1[z1 = xW1 + b1]
    z1 --> |Ïƒ| h1[h1 = Ïƒ z1]
    h1 --> |W2| z2[z2 = h1W2 + b2]
    z2 --> L[Loss L]

    L -.é€†ä¼æ’­.-> z2
    z2 -.-> h1
    h1 -.-> z1
    z1 -.-> x
```

#### 3.1.3 å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºå•é¡Œ

**å®šç¾©**: æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å‹¾é…ãŒæŒ‡æ•°çš„ã«æ¸›è¡°/å¢—å¤§ã€‚

**å‹¾é…æ¶ˆå¤±ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ** (Sigmoidæ´»æ€§åŒ–ã®å ´åˆ):

$$
\frac{\partial L}{\partial \mathbf{h}_0} = \frac{\partial L}{\partial \mathbf{h}_L} \prod_{\ell=1}^L \frac{\partial \mathbf{h}_\ell}{\partial \mathbf{h}_{\ell-1}}
$$

Sigmoidå¾®åˆ† $\sigma'(x) = \sigma(x)(1-\sigma(x)) \leq 0.25$ ã‚ˆã‚Š:

$$
\left\| \frac{\partial \mathbf{h}_\ell}{\partial \mathbf{h}_{\ell-1}} \right\| \approx \|W_\ell\| \cdot 0.25
$$

$L$ å±¤ä¼æ’­ã§ $(0.25)^L \to 0$ æŒ‡æ•°çš„æ¸›è¡°ã€‚

**å¯¾ç­–**:
1. **ReLUç³»æ´»æ€§åŒ–** â€” å‹¾é…ãŒ $\{0, 1\}$ ã§é£½å’Œã—ãªã„
2. **BatchNorm/LayerNorm** â€” å„å±¤ã®æ´»æ€§åŒ–ã‚’æ­£è¦åŒ–
3. **Residualæ¥ç¶š** â€” $\mathbf{h}_{\ell+1} = \mathbf{h}_\ell + F(\mathbf{h}_\ell)$ ã§å‹¾é…ã®ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ
4. **é©åˆ‡ãªåˆæœŸåŒ–** â€” Xavier/HeåˆæœŸåŒ–ã§åˆ†æ•£ç¶­æŒ

### 3.2 NNåŸºç¤: CNNè©³èª¬

#### 3.2.1 ç•³ã¿è¾¼ã¿æ¼”ç®—ã®å®šç¾©

**é›¢æ•£2Dç•³ã¿è¾¼ã¿**:

$$
(\mathbf{X} * \mathbf{K})_{i,j} = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} \mathbf{X}_{i+m, j+n} \mathbf{K}_{m,n}
$$

**è¨˜å·**:
- $\mathbf{X} \in \mathbb{R}^{H \times W}$: å…¥åŠ›ç‰¹å¾´ãƒãƒƒãƒ—
- $\mathbf{K} \in \mathbb{R}^{M \times N}$: ã‚«ãƒ¼ãƒãƒ« (ãƒ•ã‚£ãƒ«ã‚¿)
- $(i, j)$: å‡ºåŠ›ä½ç½®

**ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¨ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰**:

- **ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°** $P$: å…¥åŠ›ã®å‘¨å›²ã‚’ã‚¼ãƒ­åŸ‹ã‚ â†’ å‡ºåŠ›ã‚µã‚¤ã‚ºåˆ¶å¾¡
- **ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰** $S$: ã‚«ãƒ¼ãƒãƒ«ã®ç§»å‹•å¹… â†’ ç©ºé–“æ¬¡å…ƒå‰Šæ¸›

**å‡ºåŠ›ã‚µã‚¤ã‚º**:

$$
H_\text{out} = \left\lfloor \frac{H + 2P - M}{S} \right\rfloor + 1
$$

#### 3.2.2 å—å®¹é‡ (Receptive Field)

**å®šç¾©**: å‡ºåŠ›ã®1ãƒ”ã‚¯ã‚»ãƒ«ãŒè¦‹ã¦ã„ã‚‹å…¥åŠ›é ˜åŸŸã®ã‚µã‚¤ã‚ºã€‚

**è¨ˆç®—** (ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º $K$, ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ $S$, å±¤æ•° $L$):

$$
\text{RF}_L = 1 + \sum_{\ell=1}^L (K_\ell - 1) \prod_{i=1}^{\ell-1} S_i
$$

**ä¾‹** (3Ã—3ã‚«ãƒ¼ãƒãƒ«, ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰1, 3å±¤):

$$
\text{RF}_3 = 1 + (3-1) + (3-1) + (3-1) = 7
$$

**é™ç•Œ**: å—å®¹é‡ã‚’åºƒã’ã‚‹ã«ã¯å±¤ã‚’æ·±ãã™ã‚‹å¿…è¦ â†’ è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—ã€å‹¾é…æ¶ˆå¤±ã€‚

**è§£æ±ºç­–ã®äºˆå‘Š**:
- Dilated Convolution (WaveNet, ç¬¬13å›)
- Attention (ç¬¬14å›) â€” å—å®¹é‡=å…¨ç³»åˆ—

#### 3.2.3 å¹³è¡Œç§»å‹•ç­‰å¤‰æ€§ (Translation Equivariance)

**å®šç¾©**: å…¥åŠ›ã‚’ã‚·ãƒ•ãƒˆ â†’ å‡ºåŠ›ã‚‚åŒã˜ã ã‘ã‚·ãƒ•ãƒˆã€‚

**æ•°å­¦çš„è¡¨ç¾**:

å…¥åŠ›ã‚’ $\tau_d$ ã ã‘ã‚·ãƒ•ãƒˆ: $\mathbf{X}'_{i,j} = \mathbf{X}_{i-d_1, j-d_2}$

ç•³ã¿è¾¼ã¿ã¯ç­‰å¤‰:

$$
(\mathbf{X}' * \mathbf{K})_{i,j} = (\mathbf{X} * \mathbf{K})_{i-d_1, j-d_2}
$$

**é‡è¦æ€§**: ç‰©ä½“ã®ä½ç½®ã«ä¾ã‚‰ãšåŒã˜ãƒ•ã‚£ãƒ«ã‚¿ã§æ¤œå‡ºå¯èƒ½ â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰ã§åŠ¹ç‡åŒ–ã€‚

**å¹³è¡Œç§»å‹•ä¸å¤‰æ€§** (Translation Invariance) ã¨ã®é•ã„:
- **ç­‰å¤‰æ€§**: å‡ºåŠ›ã‚‚åŒã˜ã ã‘ã‚·ãƒ•ãƒˆ (Convolution)
- **ä¸å¤‰æ€§**: å‡ºåŠ›ãŒå¤‰ã‚ã‚‰ãªã„ (Poolingå¾Œ)

#### 3.2.4 ãƒ—ãƒ¼ãƒªãƒ³ã‚° (Pooling)

**ç›®çš„**: ç©ºé–“æ¬¡å…ƒå‰Šæ¸›ã€ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€å¹³è¡Œç§»å‹•ä¸å¤‰æ€§ã®ç²å¾—ã€‚

**Max Pooling**:

$$
\text{MaxPool}(\mathbf{X})_{i,j} = \max_{m,n \in \mathcal{R}_{i,j}} \mathbf{X}_{m,n}
$$

$\mathcal{R}_{i,j}$: ãƒ—ãƒ¼ãƒªãƒ³ã‚°é ˜åŸŸ

**Average Pooling**:

$$
\text{AvgPool}(\mathbf{X})_{i,j} = \frac{1}{|\mathcal{R}_{i,j}|} \sum_{m,n \in \mathcal{R}_{i,j}} \mathbf{X}_{m,n}
$$

**CNNã®å…¸å‹æ§‹é€ **:

```
Conv â†’ ReLU â†’ (Conv â†’ ReLU) Ã— N â†’ MaxPool â†’ ... â†’ Flatten â†’ MLP â†’ Output
```

#### 3.2.5 CNNã‹ã‚‰åŒ–çŸ³ã¸ã®é“

**é™ç•Œ1: å—å®¹é‡ã®åˆ¶ç´„**
- å¤§åŸŸçš„æ–‡è„ˆã®ç²å¾—ã«å¤šå±¤å¿…è¦
- è¨ˆç®—ã‚³ã‚¹ãƒˆ $O(H \times W \times C \times K^2)$

**é™ç•Œ2: é•·è·é›¢ä¾å­˜ã®å›°é›£**
- ç”»åƒã®ç«¯ã¨ç«¯ã®é–¢ä¿‚ã‚’æ‰ãˆã‚‹ã«ã¯æ·±ã„å±¤ãŒå¿…è¦
- Attention (ç¬¬14å›) ã¯ $O(1)$ å±¤ã§å…¨ãƒ”ã‚¯ã‚»ãƒ«å‚ç…§

**CNNãŒç”Ÿãæ®‹ã‚‹å ´æ‰€**:
- ç”»åƒã®åˆæœŸç‰¹å¾´æŠ½å‡º (Vision Transformer ã®ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿)
- å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ (inductive bias ãŒæœ‰åˆ©)
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«– (è»½é‡ãƒ¢ãƒ‡ãƒ«)

### 3.3 NNåŸºç¤: RNNè©³èª¬

#### 3.3.1 RNNã®å®šç¾©

**åŸºæœ¬RNN**:

$$
\begin{aligned}
\mathbf{h}_t &= \sigma(\mathbf{x}_t W_{xh} + \mathbf{h}_{t-1} W_{hh} + \mathbf{b}_h) \\
\mathbf{y}_t &= \mathbf{h}_t W_{hy} + \mathbf{b}_y
\end{aligned}
$$

**è¨˜å·**:
- $\mathbf{x}_t \in \mathbb{R}^{d_x}$: æ™‚åˆ» $t$ ã®å…¥åŠ›
- $\mathbf{h}_t \in \mathbb{R}^{d_h}$: æ™‚åˆ» $t$ ã®éš ã‚ŒçŠ¶æ…‹
- $W_{xh} \in \mathbb{R}^{d_x \times d_h}$, $W_{hh} \in \mathbb{R}^{d_h \times d_h}$: é‡ã¿è¡Œåˆ—

**æ™‚é–“å±•é–‹** (Unfolding):

```mermaid
graph LR
    x1[x_1] --> h1[h_1]
    h1 --> x2[x_2]
    x2 --> h2[h_2]
    h2 --> x3[x_3]
    x3 --> h3[h_3]
    h1 -.W_hh.-> h2
    h2 -.W_hh.-> h3
```

#### 3.3.2 BPTT (Backpropagation Through Time)

**ç›®çš„**: æ™‚ç³»åˆ—å…¨ä½“ã®æå¤± $L = \sum_{t=1}^T L_t$ ã®å‹¾é…è¨ˆç®—ã€‚

**é€£é–å¾‹**:

$$
\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^T \frac{\partial L_t}{\partial W_{hh}}
$$

å„ $\frac{\partial L_t}{\partial W_{hh}}$ ã‚’è¨ˆç®—:

$$
\frac{\partial L_t}{\partial W_{hh}} = \sum_{k=1}^t \frac{\partial L_t}{\partial \mathbf{h}_t} \frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k} \frac{\partial \mathbf{h}_k}{\partial W_{hh}}
$$

**å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºã®å†ç¾**:

$$
\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k} = \prod_{\tau=k+1}^t \frac{\partial \mathbf{h}_\tau}{\partial \mathbf{h}_{\tau-1}} = \prod_{\tau=k+1}^t \text{diag}(\sigma'(\mathbf{z}_\tau)) W_{hh}
$$

$t - k$ ãŒå¤§ãã„ (é•·è·é›¢ä¾å­˜) ã¨ã:
- $\|W_{hh}\| > 1$ â†’ å‹¾é…çˆ†ç™º
- $\|W_{hh}\| < 1$ â†’ å‹¾é…æ¶ˆå¤±

#### 3.3.3 LSTM (Long Short-Term Memory)

**å‹•æ©Ÿ**: RNNã®å‹¾é…æ¶ˆå¤±å•é¡Œã‚’ç·©å’Œã€‚

**æ§‹é€ **:

$$
\begin{aligned}
\mathbf{f}_t &= \sigma(\mathbf{x}_t W_{xf} + \mathbf{h}_{t-1} W_{hf} + \mathbf{b}_f) \quad \text{(å¿˜å´ã‚²ãƒ¼ãƒˆ)} \\
\mathbf{i}_t &= \sigma(\mathbf{x}_t W_{xi} + \mathbf{h}_{t-1} W_{hi} + \mathbf{b}_i) \quad \text{(å…¥åŠ›ã‚²ãƒ¼ãƒˆ)} \\
\mathbf{o}_t &= \sigma(\mathbf{x}_t W_{xo} + \mathbf{h}_{t-1} W_{ho} + \mathbf{b}_o) \quad \text{(å‡ºåŠ›ã‚²ãƒ¼ãƒˆ)} \\
\tilde{\mathbf{c}}_t &= \tanh(\mathbf{x}_t W_{xc} + \mathbf{h}_{t-1} W_{hc} + \mathbf{b}_c) \quad \text{(ã‚»ãƒ«å€™è£œ)} \\
\mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t \quad \text{(ã‚»ãƒ«çŠ¶æ…‹æ›´æ–°)} \\
\mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{c}_t) \quad \text{(éš ã‚ŒçŠ¶æ…‹)}
\end{aligned}
$$

**è¨˜å·**: $\odot$ = è¦ç´ ç© (Hadamardç©)

**å‹¾é…æ¶ˆå¤±ã®ç·©å’Œãƒ¡ã‚«ãƒ‹ã‚ºãƒ **:

ã‚»ãƒ«çŠ¶æ…‹ $\mathbf{c}_t$ ã®å‹¾é…:

$$
\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}} = \mathbf{f}_t
$$

å¿˜å´ã‚²ãƒ¼ãƒˆ $\mathbf{f}_t \approx 1$ ãªã‚‰å‹¾é…ãŒä¿å­˜ã•ã‚Œã‚‹ (åŠ æ³•çš„ãªå‹¾é…ãƒ‘ã‚¹)ã€‚

**GRU (Gated Recurrent Unit)** â€” LSTMç°¡ç•¥ç‰ˆ:

$$
\begin{aligned}
\mathbf{r}_t &= \sigma(\mathbf{x}_t W_{xr} + \mathbf{h}_{t-1} W_{hr}) \quad \text{(ãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆ)} \\
\mathbf{z}_t &= \sigma(\mathbf{x}_t W_{xz} + \mathbf{h}_{t-1} W_{hz}) \quad \text{(æ›´æ–°ã‚²ãƒ¼ãƒˆ)} \\
\tilde{\mathbf{h}}_t &= \tanh(\mathbf{x}_t W_{xh} + (\mathbf{r}_t \odot \mathbf{h}_{t-1}) W_{hh}) \\
\mathbf{h}_t &= (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t
\end{aligned}
$$

#### 3.3.4 RNNã‹ã‚‰åŒ–çŸ³ã¸ã®é“

**é™ç•Œ1: é€æ¬¡å‡¦ç†ã®åˆ¶ç´„**
- æ™‚åˆ» $t$ ã®è¨ˆç®—ã¯ $t-1$ ã«ä¾å­˜ â†’ ä¸¦åˆ—åŒ–ä¸å¯
- Transformer (ç¬¬14å›) ã¯å…¨æ™‚åˆ»ã‚’ä¸¦åˆ—å‡¦ç†

**é™ç•Œ2: é•·è·é›¢ä¾å­˜ã®æœ¬è³ªçš„å›°é›£**
- LSTM/GRUã§ã‚‚æ”¹å–„ã¯é™å®šçš„
- Attention ã¯ $O(1)$ ãƒ‘ã‚¹ã§å…¨æ™‚åˆ»å‚ç…§

**RNNãŒç”Ÿãæ®‹ã‚‹å ´æ‰€**:
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç† (æ¨è«–æ™‚ãƒ¡ãƒ¢ãƒª $O(d_h)$)
- è¶…é•·ç³»åˆ— (Attentionã¯ $O(T^2)$ ãƒ¡ãƒ¢ãƒª)
- SSM/Mamba (ç¬¬16-17å›) â€” RNNã®ç¾ä»£çš„å¾Œç¶™

### 3.4 å¤‰åˆ†æ¨è«–ã®å‹•æ©Ÿ

**å•é¡Œè¨­å®š** (ç¬¬8å›ã®å¾©ç¿’):

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$, æ½œåœ¨å¤‰æ•° $\mathbf{z}$, ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ã€‚

**ç›®æ¨™**: äº‹å¾Œåˆ†å¸ƒ $p(\mathbf{z}|\mathbf{x}, \theta)$ ã‚’æ±‚ã‚ã‚‹ã€‚

**ãƒ™ã‚¤ã‚ºã®å®šç†**:

$$
p(\mathbf{z}|\mathbf{x}, \theta) = \frac{p(\mathbf{x}|\mathbf{z}, \theta) p(\mathbf{z})}{p(\mathbf{x}|\theta)}
$$

**å›°é›£**: åˆ†æ¯ã®å‘¨è¾ºå°¤åº¦ (Evidence) ãŒè¨ˆç®—ä¸èƒ½:

$$
p(\mathbf{x}|\theta) = \int p(\mathbf{x}|\mathbf{z}, \theta) p(\mathbf{z}) d\mathbf{z}
$$

é«˜æ¬¡å…ƒç©åˆ† â†’ è§£æçš„ã«è§£ã‘ãªã„ã€MCMCé…ã™ãã‚‹ã€‚

**å¤‰åˆ†æ¨è«–ã®æˆ¦ç•¥**:

1. **è¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒ** $q(\mathbf{z}|\mathbf{x}, \phi)$ ã‚’å°å…¥ ($\phi$: å¤‰åˆ†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
2. $q$ ã‚’ $p(\mathbf{z}|\mathbf{x}, \theta)$ ã«è¿‘ã¥ã‘ã‚‹ â€” KLæœ€å°åŒ–
3. è¨ˆç®—å¯èƒ½ãªç›®çš„é–¢æ•° (ELBO) ã‚’æœ€å¤§åŒ–

### 3.5 ELBOå®Œå…¨å°å‡º â€” 3ã¤ã®è¦–ç‚¹

#### 3.5.1 å°å‡º1: Jensenä¸ç­‰å¼ (ç¬¬8å›ã®å¾©ç¿’)

**ã‚¹ãƒ†ãƒƒãƒ—**:

$$
\begin{aligned}
\log p(\mathbf{x}|\theta) &= \log \int p(\mathbf{x}, \mathbf{z}|\theta) d\mathbf{z} \\
&= \log \int q(\mathbf{z}|\mathbf{x}, \phi) \frac{p(\mathbf{x}, \mathbf{z}|\theta)}{q(\mathbf{z}|\mathbf{x}, \phi)} d\mathbf{z} \\
&= \log \mathbb{E}_{q(\mathbf{z}|\mathbf{x}, \phi)} \left[ \frac{p(\mathbf{x}, \mathbf{z}|\theta)}{q(\mathbf{z}|\mathbf{x}, \phi)} \right] \\
&\geq \mathbb{E}_{q(\mathbf{z}|\mathbf{x}, \phi)} \left[ \log \frac{p(\mathbf{x}, \mathbf{z}|\theta)}{q(\mathbf{z}|\mathbf{x}, \phi)} \right] \quad \text{(Jensenä¸ç­‰å¼: } \log \mathbb{E}[X] \geq \mathbb{E}[\log X] \text{)} \\
&= \mathbb{E}_{q} [\log p(\mathbf{x}, \mathbf{z}|\theta)] - \mathbb{E}_{q} [\log q(\mathbf{z}|\mathbf{x}, \phi)] \\
&\equiv \mathcal{L}(\theta, \phi; \mathbf{x}) \quad \text{(ELBO)}
\end{aligned}
$$

**ç­‰å·æˆç«‹æ¡ä»¶**: $q(\mathbf{z}|\mathbf{x}, \phi) = p(\mathbf{z}|\mathbf{x}, \theta)$ (çœŸã®äº‹å¾Œåˆ†å¸ƒ)ã€‚

#### 3.5.2 å°å‡º2: KLåˆ†è§£

**åˆ¥ã®å¤‰å½¢**:

$$
\begin{aligned}
\log p(\mathbf{x}|\theta) &= \log p(\mathbf{x}|\theta) \int q(\mathbf{z}|\mathbf{x}, \phi) d\mathbf{z} \quad \text{(} \int q = 1 \text{)} \\
&= \int q(\mathbf{z}|\mathbf{x}, \phi) \log p(\mathbf{x}|\theta) d\mathbf{z} \\
&= \int q(\mathbf{z}|\mathbf{x}, \phi) \log \frac{p(\mathbf{x}, \mathbf{z}|\theta)}{p(\mathbf{z}|\mathbf{x}, \theta)} d\mathbf{z} \\
&= \int q(\mathbf{z}|\mathbf{x}, \phi) \log \frac{p(\mathbf{x}, \mathbf{z}|\theta)}{q(\mathbf{z}|\mathbf{x}, \phi)} d\mathbf{z} + \int q(\mathbf{z}|\mathbf{x}, \phi) \log \frac{q(\mathbf{z}|\mathbf{x}, \phi)}{p(\mathbf{z}|\mathbf{x}, \theta)} d\mathbf{z} \\
&= \mathcal{L}(\theta, \phi; \mathbf{x}) + D_\text{KL}(q(\mathbf{z}|\mathbf{x}, \phi) \| p(\mathbf{z}|\mathbf{x}, \theta))
\end{aligned}
$$

**KLåˆ†è§£ã®è§£é‡ˆ**:

$$
\underbrace{\log p(\mathbf{x}|\theta)}_{\text{å¯¾æ•°å°¤åº¦(å®šæ•°)}} = \underbrace{\mathcal{L}(\theta, \phi; \mathbf{x})}_{\text{ELBO(æœ€å¤§åŒ–)}} + \underbrace{D_\text{KL}(q \| p)}_{\text{KL(éè² ã€æœ€å°åŒ–)}}
$$

**é‡è¦ãªæ€§è³ª**:
1. $\log p(\mathbf{x}|\theta)$ ã¯ $\phi$ ã«ä¾å­˜ã—ãªã„ (å®šæ•°)
2. $D_\text{KL}(q \| p) \geq 0$ ã‚ˆã‚Š $\mathcal{L} \leq \log p(\mathbf{x}|\theta)$ (ä¸‹ç•Œ)
3. ELBOæœ€å¤§åŒ– â†” KLæœ€å°åŒ– (åŒå€¤)

#### 3.5.3 å°å‡º3: é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¦–ç‚¹

**é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** (ç¬¬5å›ã§å­¦ã‚“ã ):

$$
\mathbb{E}_{p(\mathbf{z})} [f(\mathbf{z})] = \mathbb{E}_{q(\mathbf{z})} \left[ \frac{p(\mathbf{z})}{q(\mathbf{z})} f(\mathbf{z}) \right]
$$

$f(\mathbf{z}) = p(\mathbf{x}|\mathbf{z}, \theta)$ ã¨ãŠã:

$$
\begin{aligned}
\log p(\mathbf{x}|\theta) &= \log \int p(\mathbf{x}|\mathbf{z}, \theta) p(\mathbf{z}) d\mathbf{z} \\
&= \log \mathbb{E}_{p(\mathbf{z})} [p(\mathbf{x}|\mathbf{z}, \theta)] \\
&= \log \mathbb{E}_{q(\mathbf{z}|\mathbf{x}, \phi)} \left[ \frac{p(\mathbf{z})}{q(\mathbf{z}|\mathbf{x}, \phi)} p(\mathbf{x}|\mathbf{z}, \theta) \right] \\
&\geq \mathbb{E}_{q(\mathbf{z}|\mathbf{x}, \phi)} \left[ \log \frac{p(\mathbf{z})}{q(\mathbf{z}|\mathbf{x}, \phi)} p(\mathbf{x}|\mathbf{z}, \theta) \right] \quad \text{(Jensen)} \\
&= \mathbb{E}_{q} [\log p(\mathbf{x}|\mathbf{z}, \theta)] + \mathbb{E}_{q} \left[ \log \frac{p(\mathbf{z})}{q(\mathbf{z}|\mathbf{x}, \phi)} \right] \\
&= \mathbb{E}_{q} [\log p(\mathbf{x}|\mathbf{z}, \theta)] - D_\text{KL}(q(\mathbf{z}|\mathbf{x}, \phi) \| p(\mathbf{z})) \\
&= \mathcal{L}(\theta, \phi; \mathbf{x})
\end{aligned}
$$

**3ã¤ã®å°å‡ºã®çµ±ä¸€çš„ç†è§£**:

| å°å‡º | å‡ºç™ºç‚¹ | ã‚­ãƒ¼ã‚¹ãƒ†ãƒƒãƒ— | æ´å¯Ÿ |
|:-----|:------|:------------|:-----|
| **Jensen** | $\log \mathbb{E}[\cdot]$ | Jensenä¸ç­‰å¼ | æœŸå¾…å€¤ã®å‡¹æ€§ |
| **KLåˆ†è§£** | $\log p(\mathbf{x})$ | ãƒ™ã‚¤ã‚ºã®å®šç† + KLå®šç¾© | çœŸã®äº‹å¾Œã¨ã®KL |
| **é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** | å‘¨è¾ºåŒ– | é‡ç‚¹åˆ†å¸ƒå°å…¥ | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¦–ç‚¹ |

### 3.6 ELBOã®åˆ†è§£ â€” å†æ§‹æˆé … + KLæ­£å‰‡åŒ–é …

**æ¨™æº–çš„ãªåˆ†è§£**:

$$
\mathcal{L}(\theta, \phi; \mathbf{x}) = \underbrace{\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})]}_{\text{å†æ§‹æˆé … (Reconstruction)}} - \underbrace{D_\text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))}_{\text{KLæ­£å‰‡åŒ–é … (Regularization)}}
$$

**å„é …ã®æ„å‘³**:

1. **å†æ§‹æˆé …** $\mathbb{E}_{q} [\log p_\theta(\mathbf{x}|\mathbf{z})]$:
   - æ½œåœ¨å¤‰æ•° $\mathbf{z} \sim q(\mathbf{z}|\mathbf{x})$ ã‹ã‚‰å…ƒãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ã‚’å¾©å…ƒã§ãã‚‹ã‹
   - VAEã§ã¯ã€ŒDecoder ã®å¯¾æ•°å°¤åº¦ã€
   - æœ€å¤§åŒ– â†’ è‰¯ã„å¾©å…ƒ

2. **KLæ­£å‰‡åŒ–é …** $D_\text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))$:
   - è¿‘ä¼¼äº‹å¾Œ $q(\mathbf{z}|\mathbf{x})$ ãŒäº‹å‰åˆ†å¸ƒ $p(\mathbf{z})$ ã‹ã‚‰ã©ã‚Œã ã‘é›¢ã‚Œã¦ã„ã‚‹ã‹
   - æœ€å°åŒ– â†’ $q$ ã‚’ $p$ ã«è¿‘ã¥ã‘ã‚‹ (æ­£å‰‡åŒ–)

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:
- å†æ§‹æˆé … â†‘ â†’ KLé … â†‘ (è¤‡é›‘ãª $q$ ãŒå¿…è¦)
- KLé … â†“ â†’ å†æ§‹æˆé … â†“ (å˜ç´”ãª $q$ ã§ã¯å¾©å…ƒå›°é›£)

**Rate-Distortionè¦–ç‚¹** (ç¬¬6å›ã§äºˆå‘Š):

$$
\min_{q} \quad D(\text{æ­ªã¿}) + \beta R(\text{ãƒ¬ãƒ¼ãƒˆ})
$$

- æ­ªã¿ $D$ = å†æ§‹æˆèª¤å·® (è² ã®å†æ§‹æˆé …)
- ãƒ¬ãƒ¼ãƒˆ $R$ = KLé … (åœ§ç¸®ç‡)
- $\beta$ = Lagrangeä¹—æ•° (Î²-VAE, ç¬¬10å›)

| Zone 3 å‰åŠã®è¦ç‚¹ | èª¬æ˜ |
|:-----------------|:-----|
| **MLP** | é †ä¼æ’­ãƒ»é€†ä¼æ’­ãƒ»å‹¾é…æ¶ˆå¤±å•é¡Œã¨å¯¾ç­– |
| **CNN** | ç•³ã¿è¾¼ã¿ãƒ»å—å®¹é‡ãƒ»å¹³è¡Œç§»å‹•ç­‰å¤‰æ€§ãƒ»åŒ–çŸ³ã¸ã®é“ |
| **RNN** | BPTTãƒ»LSTM/GRUãƒ»é•·è·é›¢ä¾å­˜ã®é™ç•Œ |
| **VIå‹•æ©Ÿ** | äº‹å¾Œåˆ†å¸ƒã®è¨ˆç®—å›°é›£æ€§ â†’ è¿‘ä¼¼æ¨è«–ã®å¿…è¦æ€§ |
| **ELBOå°å‡º** | Jensen / KLåˆ†è§£ / é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ã®3è¦–ç‚¹çµ±ä¸€ |
| **ELBOåˆ†è§£** | å†æ§‹æˆé … + KLæ­£å‰‡åŒ–é … = Rate-Distortion |

---

### 3.7 Mean-Fieldè¿‘ä¼¼ã¨Coordinate Ascent VI

**å®šç¾©**: å¤‰åˆ†åˆ†å¸ƒã‚’å› æ•°åˆ†è§£:

$$
q(\mathbf{z}) = \prod_{i=1}^d q_i(z_i)
$$

å„ $z_i$ ãŒç‹¬ç«‹ã€‚

**Coordinate Ascent VI (CAVI)**:

å„ $q_j$ ã‚’ä»–ã‚’å›ºå®šã—ã¦æœ€é©åŒ–:

$$
q_j^*(z_j) \propto \exp \left( \mathbb{E}_{q_{-j}} [\log p(\mathbf{z}, \mathbf{x})] \right)
$$

$q_{-j} = \prod_{i \neq j} q_i$

**é–‰å½¢å¼è§£** (æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®å ´åˆ):

æ¡ä»¶ä»˜ãåˆ†å¸ƒ $p(z_j | \mathbf{z}_{-j}, \mathbf{x})$ ãŒæŒ‡æ•°å‹åˆ†å¸ƒæ—ãªã‚‰ã€$q_j^*$ ã‚‚åŒã˜æ—ã€‚

**ä¾‹**: ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ« (GMM) ã®VI â€” ç¬¬8å›ã®EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨é¡ä¼¼ã€‚

### 3.8 Stochastic VI (SVI) â€” å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã¸ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

**å‹•æ©Ÿ**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ $\{\mathbf{x}_n\}_{n=1}^N$ ã§ CAVI ã¯é…ã„ã€‚

**ELBO ã®ãƒŸãƒ‹ãƒãƒƒãƒè¿‘ä¼¼**:

$$
\mathcal{L}(\theta, \phi) = \sum_{n=1}^N \mathcal{L}_n(\theta, \phi; \mathbf{x}_n)
$$

ãƒŸãƒ‹ãƒãƒƒãƒ $\mathcal{B}$:

$$
\tilde{\mathcal{L}}(\theta, \phi) = \frac{N}{|\mathcal{B}|} \sum_{n \in \mathcal{B}} \mathcal{L}_n(\theta, \phi; \mathbf{x}_n)
$$

**SGDæ›´æ–°**:

$$
\phi \leftarrow \phi + \eta \nabla_\phi \tilde{\mathcal{L}}(\theta, \phi)
$$

**åæŸæ¡ä»¶**: Robbins-Monro (ç¬¬6å›):

$$
\sum_{t=1}^\infty \eta_t = \infty, \quad \sum_{t=1}^\infty \eta_t^2 < \infty
$$

### 3.9 Amortized Inference â€” æ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¦‚å¿µ

**å¾“æ¥ã®VI**: å„ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}_n$ ã«å¯¾ã—ã¦å€‹åˆ¥ã« $q(\mathbf{z}|\mathbf{x}_n, \phi_n)$ ã‚’æœ€é©åŒ–ã€‚

**Amortized VI**: å…±é€šã®æ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $q_\phi(\mathbf{z}|\mathbf{x})$ ã‚’å­¦ç¿’ã€‚

**åˆ©ç‚¹**:
1. **æ¨è«–ã®é«˜é€ŸåŒ–** â€” æ–°ãƒ‡ãƒ¼ã‚¿ã«å³åº§ã«å¯¾å¿œ
2. **æ±åŒ–** â€” ãƒ‡ãƒ¼ã‚¿é–“ã®æ§‹é€ ã‚’å­¦ç¿’

**æ¬ ç‚¹**: **Amortization Gap** â€” å€‹åˆ¥æœ€é©åŒ–ã‚ˆã‚Šæ€§èƒ½ãŒåŠ£ã‚‹å¯èƒ½æ€§ã€‚

**ç†è«–** (Zhang+ 2022, NeurIPS):

Generalization gap in amortized inference:
- é™ã‚‰ã‚ŒãŸ encoder å®¹é‡ã«ã‚ˆã‚‹è¿‘ä¼¼èª¤å·®
- éå­¦ç¿’ã«ã‚ˆã‚‹æ±åŒ–èª¤å·®
- æœ€é©åŒ–å›°é›£æ€§ã«ã‚ˆã‚‹åæŸã‚®ãƒ£ãƒƒãƒ—

**å¯¾ç­–**:
- Semi-amortization: å€‹åˆ¥æœ€é©åŒ–ã¨ã®æ··åˆ
- Iterative refinement: æ¨è«–å¾Œã®å¾®èª¿æ•´
- Two-stage VAE: encoder ã‚’æ®µéšçš„ã«è¨“ç·´

**VAEã¨ã®é–¢ä¿‚**: VAE = Amortized VI + ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (ç¬¬10å›)ã€‚

### 3.10 å‹¾é…æ¨å®šé‡ã®æ¯”è¼ƒ â€” REINFORCE vs Reparameterization

**å•é¡Œ**: ELBOå‹¾é… $\nabla_\phi \mathbb{E}_{q_\phi(\mathbf{z})} [f(\mathbf{z})]$ ã®è¨ˆç®—ã€‚

æœŸå¾…å€¤å†…ã« $\phi$ ãŒå…¥ã‚‹ â†’ å¾®åˆ†ã¨æœŸå¾…å€¤ã®é †åºäº¤æ›ãŒå¿…è¦ã€‚

#### 3.10.1 REINFORCE (Score Function Estimator)

**å°å‡º**:

$$
\begin{aligned}
\nabla_\phi \mathbb{E}_{q_\phi(\mathbf{z})} [f(\mathbf{z})] &= \nabla_\phi \int q_\phi(\mathbf{z}) f(\mathbf{z}) d\mathbf{z} \\
&= \int \nabla_\phi q_\phi(\mathbf{z}) f(\mathbf{z}) d\mathbf{z} \\
&= \int q_\phi(\mathbf{z}) \nabla_\phi \log q_\phi(\mathbf{z}) f(\mathbf{z}) d\mathbf{z} \quad \text{(log-derivative trick)} \\
&= \mathbb{E}_{q_\phi(\mathbf{z})} [f(\mathbf{z}) \nabla_\phi \log q_\phi(\mathbf{z})]
\end{aligned}
$$

**ç‰¹å¾´**:
- $f$ ãŒå¾®åˆ†å¯èƒ½ã§ã‚ã‚‹å¿…è¦ãŒãªã„
- **é«˜åˆ†æ•£** â€” $f(\mathbf{z})$ ã®å¤‰å‹•ãŒå¤§ãã„ã¨æ¨å®šãŒä¸å®‰å®š

**åˆ†æ•£å‰Šæ¸›**: åˆ¶å¾¡å¤‰é‡ (Control Variate) $b$:

$$
\nabla_\phi \mathbb{E}_{q} [f(\mathbf{z})] = \mathbb{E}_{q} [(f(\mathbf{z}) - b) \nabla_\phi \log q_\phi(\mathbf{z})]
$$

$b$ ã¯ $\phi$ ã«ä¾å­˜ã—ãªã„ä»»æ„ã®é–¢æ•° (é€šå¸¸ $b = \mathbb{E}_{q}[f(\mathbf{z})]$ ã®æ¨å®šå€¤)ã€‚

#### 3.10.2 Reparameterization Trick

**å‰æ**: $q_\phi(\mathbf{z}) = \mathcal{N}(\boldsymbol{\mu}_\phi, \boldsymbol{\Sigma}_\phi)$ (ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ)ã€‚

**å¤‰æ•°å¤‰æ›**:

$$
\mathbf{z} = \boldsymbol{\mu}_\phi + \boldsymbol{\Sigma}_\phi^{1/2} \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
$$

$\boldsymbol{\epsilon}$ ã¯ $\phi$ ã«ä¾å­˜ã—ãªã„ãƒã‚¤ã‚ºã€‚

**å‹¾é…**:

$$
\nabla_\phi \mathbb{E}_{q_\phi(\mathbf{z})} [f(\mathbf{z})] = \nabla_\phi \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})} [f(\boldsymbol{\mu}_\phi + \boldsymbol{\Sigma}_\phi^{1/2} \boldsymbol{\epsilon})] = \mathbb{E}_{\boldsymbol{\epsilon}} [\nabla_\phi f(\boldsymbol{\mu}_\phi + \boldsymbol{\Sigma}_\phi^{1/2} \boldsymbol{\epsilon})]
$$

$\nabla_\phi$ ãŒæœŸå¾…å€¤ã®å¤–ã«å‡ºãŸï¼

**ç‰¹å¾´**:
- **ä½åˆ†æ•£** â€” $f$ ã®å‹¾é…ã‚’ç›´æ¥è¨ˆç®—
- $f$ ãŒå¾®åˆ†å¯èƒ½ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹
- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãªã©ç‰¹å®šã®åˆ†å¸ƒã«ã®ã¿é©ç”¨å¯èƒ½

**ä¸€èˆ¬åŒ–**: Normalizing Flow (ç¬¬33å›), Gumbel-Softmax (ç¬¬10å›)ã€‚

**æ¯”è¼ƒ**:

| æ¨å®šé‡ | åˆ†æ•£ | é©ç”¨ç¯„å›² | VAEä½¿ç”¨ |
|:------|:-----|:--------|:--------|
| **REINFORCE** | é«˜ (åˆ†æ•£ãŒæ•°æ¡å¤§ãã„) | ä»»æ„ã®åˆ†å¸ƒ | âœ— |
| **Reparameterization** | ä½ | é™å®šçš„ | âœ“ (æ¨™æº–) |

**åˆ†æ•£ã®æ¡é•ã„ã®å·®** â€” å®Ÿé¨“çš„ã« REINFORCE ã¯ Reparameterization ã‚ˆã‚Šåˆ†æ•£ãŒ100ã€œ1000å€å¤§ãã„ã€‚VAEè¨“ç·´ã§ã¯ Reparameterization ãŒå¿…é ˆã€‚

### 3.11 Black-Box VI ã¨ Stein Variational Gradient Descent

**Black-Box VI**: REINFORCE ã‚’ç”¨ã„ãŸä»»æ„ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ VIã€‚

**ç‰¹å¾´**:
- ãƒ¢ãƒ‡ãƒ«ã®å¾®åˆ†å¯èƒ½æ€§ã‚’ä»®å®šã—ãªã„
- åˆ†æ•£ãŒé«˜ã„ â†’ å­¦ç¿’ãŒä¸å®‰å®š

**Stein Variational Gradient Descent (SVGD)**: ç²’å­ãƒ™ãƒ¼ã‚¹ VIã€‚

**æ›´æ–°å¼**:

$$
\mathbf{z}_i \leftarrow \mathbf{z}_i + \epsilon \phi^*(\mathbf{z}_i)
$$

$$
\phi^*(\mathbf{z}) = \frac{1}{n} \sum_{j=1}^n \left[ k(\mathbf{z}_j, \mathbf{z}) \nabla_{\mathbf{z}_j} \log p(\mathbf{z}_j) + \nabla_{\mathbf{z}_j} k(\mathbf{z}_j, \mathbf{z}) \right]
$$

$k$: ã‚«ãƒ¼ãƒãƒ«é–¢æ•° (RBFç­‰)ã€‚

**ç‰¹å¾´**: åˆ†å¸ƒã®å½¢ã‚’ä»®å®šã—ãªã„ã€å¤šå³°åˆ†å¸ƒã«å¯¾å¿œã€‚

### 3.12 Importance Weighted ELBO (IWAE) â€” ã‚ˆã‚Š tight ãªãƒã‚¦ãƒ³ãƒ‰

**å‹•æ©Ÿ**: ELBO ã¯ãƒã‚¦ãƒ³ãƒ‰ãŒç·©ã„ â†’ ã‚ˆã‚Š tight ãªãƒã‚¦ãƒ³ãƒ‰ãŒæ¬²ã—ã„ã€‚

**IWAE bound**:

$$
\mathcal{L}_K(\theta, \phi; \mathbf{x}) = \mathbb{E}_{\mathbf{z}_1, \ldots, \mathbf{z}_K \sim q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{1}{K} \sum_{k=1}^K \frac{p_\theta(\mathbf{x}, \mathbf{z}_k)}{q_\phi(\mathbf{z}_k|\mathbf{x})} \right]
$$

**æ€§è³ª**:
1. $K=1$ â†’ é€šå¸¸ã®ELBO
2. $K \to \infty$ â†’ $\log p_\theta(\mathbf{x})$ (çœŸã®å¯¾æ•°å°¤åº¦)
3. $\mathcal{L}_1 \leq \mathcal{L}_2 \leq \cdots \leq \mathcal{L}_K \leq \log p_\theta(\mathbf{x})$

**è©³ç´°**: ç¬¬10å›ã§å®Œå…¨å±•é–‹ã€‚

### 3.13 Information Bottleneck & Î²-VAE ã¸ã®ä¼ç·š

**Information BottleneckåŸç†**:

æ½œåœ¨è¡¨ç¾ $\mathbf{Z}$ ã¯å…¥åŠ› $\mathbf{X}$ ã¨å‡ºåŠ› $\mathbf{Y}$ ã®é–“ã®ã€Œæƒ…å ±ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€ã€‚

**ç›®çš„é–¢æ•°**:

$$
\max_{\mathbf{Z}} \quad I(\mathbf{Z}; \mathbf{Y}) - \beta I(\mathbf{Z}; \mathbf{X})
$$

- $I(\mathbf{Z}; \mathbf{Y})$: äºˆæ¸¬ç²¾åº¦ (æƒ…å ±ä¿æŒ)
- $I(\mathbf{Z}; \mathbf{X})$: åœ§ç¸® (ä¸è¦ãªæƒ…å ±å‰Šæ¸›)

**VAEã¨ã®é–¢ä¿‚**:

ELBO ã¨ Information Bottleneck ã¯ç­‰ä¾¡:

$$
\mathcal{L}_{\text{ELBO}} \equiv I(\mathbf{X}; \mathbf{Z}) - \beta D_\text{KL}(q(\mathbf{Z}|\mathbf{X}) \| p(\mathbf{Z}))
$$

**Tishby ã® Deep Learning ç†è«–**:
- å­¦ç¿’åˆæœŸ: Fitting phase (è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ãƒ•ã‚£ãƒƒãƒˆ)
- å­¦ç¿’å¾ŒæœŸ: Compression phase (ä¸è¦ãªæƒ…å ±ã‚’åœ§ç¸®)

**Î²-VAE** (ç¬¬10å›) ã¯ã“ã®åœ§ç¸®ã‚’æ˜ç¤ºçš„ã«åˆ¶å¾¡ã€‚

### 3.14 ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«é¸æŠ â€” Evidence ã®å½¹å‰²

**ãƒ¢ãƒ‡ãƒ«é¸æŠå•é¡Œ**: è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ« $\mathcal{M}_1, \ldots, \mathcal{M}_K$ ã‹ã‚‰ãƒ™ã‚¹ãƒˆã‚’é¸ã¶ã€‚

**Evidence** (å‘¨è¾ºå°¤åº¦):

$$
p(\mathbf{x}|\mathcal{M}_k) = \int p(\mathbf{x}|\theta, \mathcal{M}_k) p(\theta|\mathcal{M}_k) d\theta
$$

**ãƒ™ã‚¤ã‚ºå› å­**:

$$
\text{BF}_{12} = \frac{p(\mathbf{x}|\mathcal{M}_1)}{p(\mathbf{x}|\mathcal{M}_2)}
$$

**Occamã®ã‚«ãƒŸã‚½ãƒªã®å®šé‡åŒ–**:

Evidence ã¯è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•çš„ã«ãƒšãƒŠãƒ«ãƒ†ã‚£:

$$
\log p(\mathbf{x}|\mathcal{M}) = \log p(\mathbf{x}|\hat{\theta}, \mathcal{M}) - \frac{d}{2} \log N + O(1)
$$

$d$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°, $N$: ãƒ‡ãƒ¼ã‚¿æ•°

è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ« ($d$ å¤§) ã¯ $\log N$ ã§ãƒšãƒŠãƒ«ãƒ†ã‚£ã€‚

**VIã¨ã®æ¥ç¶š**: ELBO ã¯ Evidence ã®ä¸‹ç•Œ â†’ è¿‘ä¼¼çš„ãªãƒ¢ãƒ‡ãƒ«é¸æŠãŒå¯èƒ½ã€‚

### 3.15 âš”ï¸ Boss Battle: Course I æ•°å­¦ã§ELBOã‚’å®Œå…¨åˆ†è§£

**å•é¡Œ**: VAE ã® ELBO ã‚’ Course I ã§å­¦ã‚“ã å…¨æ•°å­¦ãƒ„ãƒ¼ãƒ«ã§å®Œå…¨ã«åˆ†è§£ã›ã‚ˆã€‚

**ELBO**:

$$
\mathcal{L}(\theta, \phi; \mathbf{x}) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] - D_\text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))
$$

**åˆ†è§£**:

1. **æœŸå¾…å€¤** (ç¬¬4å›):
   $$\mathbb{E}_{q} [f(\mathbf{z})] = \int q(\mathbf{z}|\mathbf{x}) f(\mathbf{z}) d\mathbf{z}$$

2. **KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹** (ç¬¬6å›):
   $$D_\text{KL}(q \| p) = \int q(\mathbf{z}|\mathbf{x}) \log \frac{q(\mathbf{z}|\mathbf{x})}{p(\mathbf{z})} d\mathbf{z}$$

3. **ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ** (ç¬¬4å›):
   $$q_\phi(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}_\phi(\mathbf{x}), \text{diag}(\boldsymbol{\sigma}^2_\phi(\mathbf{x})))$$

4. **ã‚¬ã‚¦ã‚¹KLã®é–‰å½¢å¼** (ç¬¬4å›):
   $$D_\text{KL}(\mathcal{N}(\boldsymbol{\mu}_q, \boldsymbol{\Sigma}_q) \| \mathcal{N}(\boldsymbol{\mu}_p, \boldsymbol{\Sigma}_p)) = \frac{1}{2} \left[ \text{tr}(\boldsymbol{\Sigma}_p^{-1} \boldsymbol{\Sigma}_q) + (\boldsymbol{\mu}_p - \boldsymbol{\mu}_q)^\top \boldsymbol{\Sigma}_p^{-1} (\boldsymbol{\mu}_p - \boldsymbol{\mu}_q) - d + \log \frac{|\boldsymbol{\Sigma}_p|}{|\boldsymbol{\Sigma}_q|} \right]$$

5. **äº‹å‰åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹** $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ ã®å ´åˆ:
   $$D_\text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| \mathcal{N}(\mathbf{0}, \mathbf{I})) = \frac{1}{2} \sum_{j=1}^d \left( \mu_j^2 + \sigma_j^2 - \log \sigma_j^2 - 1 \right)$$

6. **å†æ§‹æˆé …** â€” ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ¨å®š (ç¬¬5å›):
   $$\mathbb{E}_{q} [\log p_\theta(\mathbf{x}|\mathbf{z})] \approx \frac{1}{K} \sum_{k=1}^K \log p_\theta(\mathbf{x}|\mathbf{z}_k), \quad \mathbf{z}_k \sim q_\phi(\mathbf{z}|\mathbf{x})$$

7. **å‹¾é…è¨ˆç®—** â€” Reparameterization (ç¬¬3å› è‡ªå‹•å¾®åˆ† + ç¬¬4å› ç¢ºç‡å¤‰æ•°ã®å¤‰æ›):
   $$\mathbf{z} = \boldsymbol{\mu}_\phi(\mathbf{x}) + \boldsymbol{\sigma}_\phi(\mathbf{x}) \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$$

8. **æœ€é©åŒ–** â€” Adam (ç¬¬6å›):
   $$(\theta, \phi) \leftarrow \text{Adam}(\nabla_{\theta,\phi} \mathcal{L})$$

**ãƒœã‚¹æ’ƒç ´**: Course I ã® 8è¬›ç¾©ã®æ•°å­¦ãŒå…¨ã¦ VAE ã® ELBO ã«é›†ç´„ã•ã‚ŒãŸã€‚

| Zone 3ã®è¦ç‚¹ | èª¬æ˜ |
|:------------|:-----|
| **MLP/CNN/RNN** | NNåŸºç¤3ç¨®ã®æ•°å¼ãƒ»å‹¾é…ãƒ»é™ç•Œã‚’å®Œå…¨ç†è§£ |
| **ELBOå°å‡º** | Jensen / KLåˆ†è§£ / é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ã®3è¦–ç‚¹ |
| **ELBOåˆ†è§£** | å†æ§‹æˆé … + KLæ­£å‰‡åŒ–é … = Rate-Distortion ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ• |
| **Mean-Field** | ç‹¬ç«‹æ€§ä»®å®šã«ã‚ˆã‚‹åˆ†è§£ / CAVI / é–‰å½¢å¼è§£ |
| **SVI** | ãƒŸãƒ‹ãƒãƒƒãƒè¿‘ä¼¼ã§å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œ |
| **Amortized** | æ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§é«˜é€ŸåŒ– / Amortization Gap |
| **å‹¾é…æ¨å®š** | REINFORCE (é«˜åˆ†æ•£) vs Reparameterization (ä½åˆ†æ•£ã€æ¡é•ã„) |
| **IWAE** | ã‚ˆã‚Š tight ãªãƒã‚¦ãƒ³ãƒ‰ / Kâ†’âˆ ã§çœŸã®å°¤åº¦ |
| **Information Bottleneck** | åœ§ç¸®ã¨äºˆæ¸¬ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ• / Î²-VAE ã¸ã®ä¼ç·š |
| **Boss Battle** | Course I æ•°å­¦ã§ELBOã‚’å®Œå…¨åˆ†è§£ â€” å…¨ã¦ãŒã¤ãªãŒã£ãŸ |

:::message
**é€²æ—: 50%å®Œäº†** â€” æ•°å¼ä¿®è¡Œå®Œäº†ï¼æ¬¡ã¯å®Ÿè£…ã¸ã€‚
:::

---

## è£œéº â€” æœ€æ–°ã®å¤‰åˆ†æ¨è«–ç ”ç©¶ (2023-2025)

:::message
**å¤‰åˆ†æ¨è«–ã®é€²åŒ–**: VAEã®åŸºç¤ç†è«–ï¼ˆ2013å¹´ï¼‰ã‹ã‚‰10å¹´ä»¥ä¸ŠãŒçµŒéã—ã€Normalizing Flowsãƒ»Amortization Gapç¸®å°ãƒ»é«˜æ¬¡å…ƒã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã©ã€å®Ÿç”¨çš„ãªæ”¹å–„ãŒç¶šã„ã¦ã„ã‚‹[^20][^21][^22]ã€‚æœ¬ç¯€ã§ã¯æœ€æ–°ç ”ç©¶ã®ã‚¨ãƒƒã‚»ãƒ³ã‚¹ã‚’ç´¹ä»‹ã€‚
:::

### è£œéº1 â€” Normalizing Flows ã«ã‚ˆã‚‹æŸ”è»Ÿãªäº‹å¾Œåˆ†å¸ƒ

#### å•é¡Œè¨­å®š: å¹³å‡å ´è¿‘ä¼¼ã®é™ç•Œ

Mean-Field è¿‘ä¼¼ã¯ $q(\mathbf{z}) = \prod_{i} q_i(z_i)$ ã¨ç‹¬ç«‹æ€§ã‚’ä»®å®šã™ã‚‹ãŒã€çœŸã®äº‹å¾Œåˆ†å¸ƒ $p(\mathbf{z}|\mathbf{x})$ ãŒå¼·ã„ç›¸é–¢ã‚’æŒã¤å ´åˆã€ELBO ãŒ loose ã«ãªã‚‹ã€‚

$$
\log p(\mathbf{x}) - \text{ELBO} = D_{\text{KL}}(q \| p) \quad \text{â† Flowsã§ç¸®å°å¯èƒ½}
$$

#### Normalizing Flows ã®åŸç†

**å®šç¾©**[^20]: å¯é€†ãªå¾®åˆ†åŒç›¸å†™åƒ $f: \mathbb{R}^d \to \mathbb{R}^d$ ã‚’ç”¨ã„ã¦ã€å˜ç´”ãªåˆ†å¸ƒ $q_0(\mathbf{z}_0)$ ã‚’è¤‡é›‘ãªåˆ†å¸ƒ $q_K(\mathbf{z}_K)$ ã«å¤‰æ›:

$$
\mathbf{z}_K = f_K \circ \cdots \circ f_1(\mathbf{z}_0), \quad \mathbf{z}_0 \sim q_0 = \mathcal{N}(\mathbf{0}, \mathbf{I})
$$

å¤‰æ•°å¤‰æ›å…¬å¼ã«ã‚ˆã‚Š:

$$
\log q_K(\mathbf{z}_K) = \log q_0(\mathbf{z}_0) - \sum_{k=1}^K \log \left| \det \frac{\partial f_k}{\partial \mathbf{z}_{k-1}} \right|
$$

**Jacobian ã®è¨ˆç®—ãŒéµ**: $\det J$ ã‚’ $O(d^3)$ ã‹ã‚‰ $O(d)$ ã«å‰Šæ¸›ã™ã‚‹æ§‹é€ ãŒå¿…è¦ã€‚

#### ä»£è¡¨çš„ãªFlowæ§‹é€ 

##### 1. Planar Flowï¼ˆ2015å¹´ï¼‰

$$
f(\mathbf{z}) = \mathbf{z} + \mathbf{u} h(\mathbf{w}^\top \mathbf{z} + b)
$$

ã“ã“ã§ $h$ ã¯éç·šå½¢æ´»æ€§åŒ–é–¢æ•°ï¼ˆä¾‹: $\tanh$ï¼‰ã€‚Jacobian ã®è¡Œåˆ—å¼ã¯:

$$
\det \left| \mathbf{I} + \mathbf{u} \mathbf{w}^\top h'(\mathbf{w}^\top \mathbf{z} + b) \right| = 1 + \mathbf{u}^\top \mathbf{w} h'(\mathbf{w}^\top \mathbf{z} + b)
$$

$O(d)$ ã§è¨ˆç®—å¯èƒ½ï¼ˆSherman-Morrisonå…¬å¼ã‚’ä½¿ç”¨ï¼‰ã€‚

##### 2. Sylvester Normalizing Flowsï¼ˆ2018å¹´ï¼‰

Planar Flowã‚’æ‹¡å¼µã—ã€ãƒ©ãƒ³ã‚¯ $M$ ã®å¤‰æ›ã‚’è¨±å®¹[^23]:

$$
f(\mathbf{z}) = \mathbf{z} + \mathbf{U} h(\mathbf{W}^\top \mathbf{z} + \mathbf{b})
$$

ã“ã“ã§ $\mathbf{U}, \mathbf{W} \in \mathbb{R}^{d \times M}$ã€‚è¡Œåˆ—å¼ã¯:

$$
\det \left| \mathbf{I}_d + \mathbf{U} \text{diag}(h'(\mathbf{W}^\top \mathbf{z} + \mathbf{b})) \mathbf{W}^\top \right| = \det \left| \mathbf{I}_M + \text{diag}(h') \mathbf{W}^\top \mathbf{U} \right|
$$

$O(M^3)$ ã§è¨ˆç®—å¯èƒ½ï¼ˆ$M \ll d$ ã®ã¨ãé«˜é€Ÿï¼‰ã€‚

##### 3. RealNVP / Coupling Layersï¼ˆ2016å¹´ï¼‰

$$
\begin{aligned}
\mathbf{z}_{1:d/2}' &= \mathbf{z}_{1:d/2} \\
\mathbf{z}_{d/2+1:d}' &= \mathbf{z}_{d/2+1:d} \odot \exp(s(\mathbf{z}_{1:d/2})) + t(\mathbf{z}_{1:d/2})
\end{aligned}
$$

Jacobian ã¯ä¸‹ä¸‰è§’è¡Œåˆ—ã¨ãªã‚Šã€$\det J = \exp\left(\sum_i s(\mathbf{z}_{1:d/2})_i\right)$ ãŒ $O(d)$ ã§è¨ˆç®—å¯èƒ½ã€‚

#### VAE with Normalizing Flows ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

```plaintext
# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
Î¼_Ï†(x), log_Ïƒ_Ï†(x) = Encoder(x)
z_0 ~ N(Î¼_Ï†, diag(Ïƒ_Ï†Â²))

# Normalizing Flows
for k=1 to K:
    z_k = f_k(z_{k-1})
    log_det_J += log|det(âˆ‚f_k/âˆ‚z_{k-1})|

# ELBO with Flow
log q_K(z_K|x) = log q_0(z_0|x) - log_det_J
ELBO = E_{q_K}[log p(x|z_K)] - D_KL(q_K(z|x) || p(z))
      â‰ˆ log p(x|z_K) - [log q_K(z_K|x) - log p(z_K)]

# ãƒ‡ã‚³ãƒ¼ãƒ€
xÌ‚ = Decoder(z_K)
```

#### å®Ÿè¨¼çµæœï¼ˆ2024å¹´ç ”ç©¶[^21]ï¼‰

4000æ¬¡å…ƒã®ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° + Horseshoeäº‹å‰åˆ†å¸ƒã§ã® marginal likelihood æ¨å®š:

| æ‰‹æ³• | Log Marginal Likelihood | æ¨™æº–åå·® |
|:---|:---:|:---:|
| Mean-Field VI | -2145.3 | Â±12.5 |
| Normalizing Flows (K=8) | -2132.7 | Â±3.2 |
| Normalizing Flows (K=16) | -2130.1 | Â±1.8 |
| HMC (çœŸå€¤) | -2129.8 | Â±0.5 |

Flowsã«ã‚ˆã‚Š ELBO ãŒçœŸã®å¯¾æ•°å°¤åº¦ã« $\sim$15 nats è¿‘ã¥ãã€åˆ†æ•£ãŒ $1/7$ ã«å‰Šæ¸›ã€‚

### è£œéº2 â€” Amortization Gap ã®ç¸®å°

#### Amortization Gap ã®å®šç¾©

**Gap**[^24]: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ $q_\phi(\mathbf{z}|\mathbf{x})$ ã«ã‚ˆã‚‹æ¨è«–ã¨ã€ãƒ‡ãƒ¼ã‚¿ç‚¹ã”ã¨ã«æœ€é©åŒ–ã—ãŸå¤‰åˆ†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $q^*(\mathbf{z}|\mathbf{x})$ ã®æ€§èƒ½å·®:

$$
\text{Gap} = \mathbb{E}_{p_{\text{data}}(\mathbf{x})} \left[ \text{ELBO}(q^* | \mathbf{x}) - \text{ELBO}(q_\phi | \mathbf{x}) \right]
$$

**åŸå› **: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ä¸è¶³ã€ã¾ãŸã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§ä¸è¶³ã€‚

#### Semi-Amortized VAE (SA-VAE)

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ã‚’åˆæœŸå€¤ã¨ã—ã€ãƒ†ã‚¹ãƒˆæ™‚ã«æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®å‹¾é…ä¸Šæ˜‡ã‚’å®Ÿè¡Œ:

```plaintext
# è¨“ç·´æ™‚
Î¼_0, log_Ïƒ_0 = Encoder(x)  # AmortizedåˆæœŸåŒ–
ELBO_loss = -ELBO(x; Î¼_0, log_Ïƒ_0)

# ãƒ†ã‚¹ãƒˆæ™‚
Î¼, log_Ïƒ = Encoder(x)
for i=1 to T:
    Î¼, log_Ïƒ â† Î¼ + Î± âˆ‡_{Î¼,log_Ïƒ} ELBO(x; Î¼, log_Ïƒ)  # å€‹åˆ¥æœ€é©åŒ–

z ~ N(Î¼, diag(exp(2*log_Ïƒ)))
xÌ‚ = Decoder(z)
```

**åŠ¹æœ**:
- $T=0$ (é€šå¸¸VAE): Gap = 5.2 nats
- $T=5$ (SA-VAE): Gap = 1.3 nats
- $T=20$: Gap = 0.4 natsï¼ˆ$\sim$æœ€é©ã«è¿‘ã„ï¼‰

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**: æ¨è«–æ™‚é–“ vs ç²¾åº¦

#### Bayesian Random Function Approach[^24]

ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’Gaussian Process (GP) ã§ç½®ãæ›ãˆã€ç„¡é™æ¬¡å…ƒã®é–¢æ•°ç©ºé–“ã§æ¨è«–:

$$
q(\mathbf{z}|\mathbf{x}) = \int p(\mathbf{z}|f(\mathbf{x})) p(f) df
$$

ã“ã“ã§ $f \sim \mathcal{GP}(\mathbf{0}, k(\cdot, \cdot))$ ã¯ã‚«ãƒ¼ãƒãƒ« $k$ ã§å®šç¾©ã•ã‚Œã‚‹GPã€‚

**åˆ©ç‚¹**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®è¡¨ç¾åŠ›ãŒç„¡é™å¤§ã«ï¼ˆç†è«–ä¸Šï¼‰ã€‚
**æ¬ ç‚¹**: è¨ˆç®—ã‚³ã‚¹ãƒˆ $O(n^3)$ï¼ˆ$n$ã¯ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ï¼‰ã€‚å®Ÿç”¨ã«ã¯ Sparse GP ã‚„ Inducing Points ãŒå¿…è¦ã€‚

### è£œéº3 â€” Poisson VAE â€” ã‚¹ãƒ‘ãƒ¼ã‚¹è¡¨ç¾ã®æ–°å±•é–‹

Hadi Vafaii et al. (NeurIPS 2024)[^22] ã«ã‚ˆã‚‹ Poisson VAE (P-VAE) ã¯ã€æ½œåœ¨å¤‰æ•°ã‚’Poissonåˆ†å¸ƒã§ãƒ¢ãƒ‡ãƒ«åŒ–:

$$
z_i \sim \text{Poisson}(\lambda_i), \quad \lambda_i = f_\phi(\mathbf{x})_i > 0
$$

#### Reparameterization Trick for Poisson

é€šå¸¸ã®Gaussian reparameterization $\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}$ ã«ç›¸å½“ã™ã‚‹Poissonç‰ˆ:

$$
z_i = \text{Poisson}(\lambda_i) \approx \mathcal{N}(\lambda_i, \lambda_i) \quad (\lambda_i \gg 1 \text{ã®ã¨ã})
$$

å°ã•ãª $\lambda_i$ ã«ã¯ Gumbel-softmax ãƒˆãƒªãƒƒã‚¯ã‚„Categorical-Poissonè¿‘ä¼¼ã‚’ä½¿ç”¨ã€‚

#### P-VAE ã® ELBO

$$
\mathcal{L}_{\text{P-VAE}} = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))
$$

ã“ã“ã§äº‹å‰åˆ†å¸ƒ $p(\mathbf{z}) = \prod_i \text{Poisson}(z_i; \beta)$ã€$\beta$ ã¯åŸºåº•ç™ºç«ç‡ã€‚

KLé …ã¯:

$$
D_{\text{KL}}(q \| p) = \sum_i \mathbb{E}_{q_i} \left[ z_i \log \frac{\lambda_i}{\beta} + (\beta - \lambda_i) \right]
$$

**ãƒ¡ã‚¿ãƒœãƒªãƒƒã‚¯ã‚³ã‚¹ãƒˆè§£é‡ˆ**: $\lambda_i$ ãŒå¤§ãã„ã»ã©ãƒšãƒŠãƒ«ãƒ†ã‚£ â†’ ã‚¹ãƒ‘ãƒ¼ã‚¹ãªè¡¨ç¾ã‚’è‡ªç„¶ã«èª˜å°ã€‚

#### å¿œç”¨: Amortized Sparse Coding

P-VAE + ç·šå½¢ãƒ‡ã‚³ãƒ¼ãƒ€:

$$
\mathbf{x} = \mathbf{D} \mathbf{z} + \boldsymbol{\epsilon}, \quad \mathbf{D} \in \mathbb{R}^{d \times k}
$$

ELBO ã¯ Sparse Coding ã®ç›®çš„é–¢æ•°ã«ä¸€è‡´:

$$
\min_{\mathbf{D}, \mathbf{z}} \|\mathbf{x} - \mathbf{D}\mathbf{z}\|_2^2 + \gamma \|\mathbf{z}\|_1
$$

ã“ã“ã§ $\gamma \propto \log(\beta / \lambda_i)$ã€‚

**å®Ÿé¨“çµæœ** (è‡ªç„¶ç”»åƒãƒ‘ãƒƒãƒ):
- è¾æ›¸è¡Œåˆ— $\mathbf{D}$ ãŒ Gabor-like ãªã‚¨ãƒƒã‚¸æ¤œå‡ºå™¨ã«åæŸ
- $\lambda_i$ ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§: å¹³å‡95%ã®æ½œåœ¨å¤‰æ•°ãŒ $\lambda_i < 0.1$

### è£œéº4 â€” é«˜æ¬¡å…ƒã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨å®‰å®šåŒ– (2024å¹´)

#### å•é¡Œ: é«˜æ¬¡å…ƒã§ã® ELBO è¨“ç·´ã®ä¸å®‰å®šæ€§

$d \geq 1000$ ã®æ½œåœ¨å¤‰æ•°ã‚’æŒã¤Flowsã§ã¯ã€ä»¥ä¸‹ã®å•é¡ŒãŒç™ºç”Ÿ:

1. **å‹¾é…æ¶ˆå¤±/çˆ†ç™º**: Jacobian ã®è¡Œåˆ—å¼ãŒ $10^{-50}$ ã‚„ $10^{50}$ ã«
2. **KLé …ã®å´©å£Š**: $D_{\text{KL}}(q \| p) \to 0$ ã¨ãªã‚Šã€$q$ ãŒäº‹å‰åˆ†å¸ƒã«éå‰°ãƒ•ã‚£ãƒƒãƒˆ

#### å®‰å®šåŒ–æ‰‹æ³•[^21]

##### 1. Spectral Normalization of Flow Layers

å„Flowå±¤ã® Lipschitzå®šæ•°ã‚’åˆ¶ç´„:

$$
\|f_k\|_{\text{Lip}} \leq L \quad \Rightarrow \quad \|\nabla_{\mathbf{z}} f_k\|_2 \leq L
$$

å®Ÿè£…: é‡ã¿è¡Œåˆ— $\mathbf{W}$ ã‚’ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ  $\sigma(\mathbf{W})$ ã§æ­£è¦åŒ–:

$$
\mathbf{W}_{\text{norm}} = \frac{L}{\sigma(\mathbf{W})} \mathbf{W}
$$

##### 2. Reverse KL (ELBO) vs Forward KL

| ç›®çš„é–¢æ•° | å®šç¾© | ç‰¹æ€§ |
|:---|:---|:---|
| Reverse KL (ELBO) | $D_{\text{KL}}(q \| p)$ | Mode-seeking / éå°æ¨å®š |
| Forward KL | $D_{\text{KL}}(p \| q)$ | Mass-covering / éå¤§æ¨å®š |

**ç™ºè¦‹**[^21]: é«˜æ¬¡å…ƒã§ã¯ Reverse KL (ELBO) ã®æ–¹ãŒ marginal likelihood æ¨å®šã®ç²¾åº¦ãŒé«˜ã„ï¼ˆç›¸é–¢ä¿‚æ•° 0.92 vs 0.73ï¼‰ã€‚

##### 3. Warm-up ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

$$
\mathcal{L}_{\text{warm-up}} = \mathbb{E}_q [\log p(\mathbf{x}|\mathbf{z})] - \beta_t D_{\text{KL}}(q \| p)
$$

$\beta_t$: $0 \to 1$ ã¨ç·šå½¢å¢—åŠ ï¼ˆä¾‹: $t=0$ ã§ $\beta=0$ã€$t=T_{\text{warmup}}$ ã§ $\beta=1$ï¼‰ã€‚

**åŠ¹æœ**: KLå´©å£Šã‚’é˜²ãã€äº‹å¾Œåˆ†å¸ƒã®å­¦ç¿’ã‚’å®‰å®šåŒ–ã€‚

### è£œéº5 â€” å¤‰åˆ†æ¨è«–ã®å¿œç”¨æœ€å‰ç·š

#### ç¸¦æ–­ãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚° (2023)[^25]

**è¨­å®š**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ $\{\mathbf{x}_{t_i}\}_{i=1}^T$ ã‚’ Normalizing Flows ã§ãƒ¢ãƒ‡ãƒ«åŒ–:

$$
q(\mathbf{z}_1, \ldots, \mathbf{z}_T | \mathbf{x}_{1:T}) = \prod_{t=1}^T q_\phi(\mathbf{z}_t | \mathbf{x}_{\leq t})
$$

å„æ™‚åˆ»ã®æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’ Flow ã§è¡¨ç¾:

$$
\mathbf{z}_t = f_{\phi_t}(\mathbf{z}_0^{(t)}; \mathbf{x}_{\leq t}), \quad \mathbf{z}_0^{(t)} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
$$

**å¿œç”¨**: åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿ï¼ˆæ‚£è€…ã®çµŒæ™‚çš„ãƒã‚¤ã‚¿ãƒ«ã‚µã‚¤ãƒ³ï¼‰ã€é‡‘èæ™‚ç³»åˆ—ï¼ˆæ ªä¾¡ã®æ½œåœ¨å› å­ãƒ¢ãƒ‡ãƒ«ï¼‰ã€‚

#### Likelihood-Free æ¨è«– (2024)

è¦³æ¸¬ãƒ¢ãƒ‡ãƒ« $p(\mathbf{x}|\boldsymbol{\theta})$ ãŒé™½ã«æ›¸ã‘ãªã„å ´åˆï¼ˆä¾‹: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ï¼‰ã€å¤‰åˆ†æ¨è«–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã«æ‹¡å¼µ[^26]:

$$
\text{ELBO}_{\text{sim}} = \mathbb{E}_{q_\phi(\boldsymbol{\theta})} \left[ \log \frac{p(\mathbf{x}, \boldsymbol{\theta})}{q_\phi(\boldsymbol{\theta})} \right] \approx \frac{1}{K} \sum_{k=1}^K w_k \log p(\boldsymbol{\theta}_k)
$$

ã“ã“ã§ $w_k$ ã¯ Importance Weightsã€‚VAE ã® encoder ã‚’ã€Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®é€†é–¢æ•°ã€ã¨ã—ã¦å­¦ç¿’ã€‚

### ã¾ã¨ã‚: å¤‰åˆ†æ¨è«–ã®ç¾åœ¨åœ°

```mermaid
graph TD
    A[Variational Inference] --> B[Mean-Field<br/>CAVI 2003]
    A --> C[Stochastic VI<br/>SVI 2013]
    A --> D[Amortized<br/>VAE 2013-14]
    D --> E[Normalizing Flows<br/>2015-2018]
    E --> F[Sylvester 2018<br/>RealNVP 2016]
    D --> G[Amortization Gap<br/>SA-VAE 2018]
    G --> H[GP-based<br/>2021]
    D --> I[Poisson VAE<br/>2024]
    E --> J[é«˜æ¬¡å…ƒå®‰å®šåŒ–<br/>2024]
    J --> K[4000æ¬¡å…ƒæˆåŠŸ<br/>Horseshoe]
```

**2025å¹´ã®å¤‰åˆ†æ¨è«–**:
- **ç†è«–**: Normalizing Flows ã§ tight ELBO â†’ çœŸã®å°¤åº¦ã«è¿«ã‚‹
- **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: å®‰å®šåŒ–æ‰‹æ³•ã«ã‚ˆã‚Šæ•°åƒæ¬¡å…ƒã¾ã§å®Ÿç”¨å¯èƒ½
- **æ–°ãƒ¢ãƒ‡ãƒ«**: Poisson VAE ã§ã‚¹ãƒ‘ãƒ¼ã‚¹è¡¨ç¾å­¦ç¿’
- **å¿œç”¨æ‹¡å¤§**: ç¸¦æ–­ãƒ‡ãƒ¼ã‚¿ã€Likelihood-Free æ¨è«–ã€å› æœæ¨è«–

**æ¬¡ã®10å¹´ã®å±•æœ›**:
- Diffusion Models ã¨ã®èåˆï¼ˆFlow Matching â‰ˆ Continuous Normalizing Flowsï¼‰
- é›¢æ•£æ½œåœ¨å¤‰æ•°ï¼ˆVQ-VAEã€Discrete Flowsï¼‰ã®ç†è«–æ•´å‚™
- å› æœæ¨è«–ã¸ã®çµ„ã¿è¾¼ã¿ï¼ˆCausal VAEï¼‰

---

## è£œéº6 â€” ELBO æœ€é©åŒ–ã®å®Ÿè·µçš„ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯1: KL Annealingï¼ˆÎ²-VAE ã¸ã®å¿œç”¨ï¼‰

$$
\mathcal{L}_{\beta}(\theta, \phi; \beta) = \mathbb{E}_{q_\phi} [\log p_\theta(\mathbf{x}|\mathbf{z})] - \beta D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))
$$

**ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä¾‹**:
```python
def beta_schedule(epoch, total_epochs, beta_max=1.0, warmup_epochs=10):
    """KLé …ã®é‡ã¿ã‚’å¾ã€…ã«å¢—åŠ """
    if epoch < warmup_epochs:
        return beta_max * (epoch / warmup_epochs)
    return beta_max

# è¨“ç·´ãƒ«ãƒ¼ãƒ—
for epoch in range(total_epochs):
    beta = beta_schedule(epoch, total_epochs)
    for x in dataloader:
        z, mu, logvar = encode(x)
        x_recon = decode(z)
        recon_loss = -log_likelihood(x, x_recon)
        kl_loss = kl_divergence(mu, logvar)
        loss = recon_loss + beta * kl_loss
        loss.backward()
```

**åŠ¹æœ**:
- åˆæœŸ: $\beta \approx 0$ â†’ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒæƒ…å ±è±Šå¯Œãª $\mathbf{z}$ ã‚’å­¦ç¿’
- å¾ŒæœŸ: $\beta \to 1$ â†’ äº‹å‰åˆ†å¸ƒã¸ã®æ­£å‰‡åŒ–ãŒåŠ¹ã

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯2: Free Bitsï¼ˆæƒ…å ±ä¿æŒã®ä¿è¨¼ï¼‰

**å•é¡Œ**: KLé …ãŒæ¬¡å…ƒã”ã¨ã« $D_{\text{KL}}(q_i \| p_i) \to 0$ ã«ãªã‚Šã€$\mathbf{z}$ ãŒç„¡æ„å‘³åŒ–ï¼ˆposterior collapseï¼‰ã€‚

**è§£æ±º**: å„æ¬¡å…ƒã® KL ã‚’ä¸‹é™ $\lambda$ ã§ã‚¯ãƒªãƒƒãƒ—:

$$
\mathcal{L}_{\text{free-bits}} = \mathbb{E}_q [\log p(\mathbf{x}|\mathbf{z})] - \sum_{i=1}^d \max(D_{\text{KL}}(q_i \| p_i), \lambda)
$$

```python
def free_bits_kl(mu, logvar, free_bits=2.0):
    """æ¬¡å…ƒã”ã¨ã« KL â‰¥ free_bits ã‚’ä¿è¨¼"""
    kl_per_dim = 0.5 * (mu**2 + logvar.exp() - logvar - 1)
    kl_clamped = torch.clamp(kl_per_dim, min=free_bits)
    return kl_clamped.sum(dim=-1)
```

**æ¨å¥¨å€¤**: $\lambda = 2.0$ natsï¼ˆå„æ¬¡å…ƒãŒæœ€ä½2ãƒ“ãƒƒãƒˆã®æƒ…å ±ã‚’ä¿æŒï¼‰ã€‚

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯3: Spectral Regularizationï¼ˆFlow ã®å®‰å®šåŒ–ï¼‰

Normalizing Flows ã®é‡ã¿è¡Œåˆ— $\mathbf{W}$ ã« spectral norm åˆ¶ç´„:

$$
\mathbf{W}_{\text{reg}} = \frac{\mathbf{W}}{\sigma_{\max}(\mathbf{W})} \cdot \text{clip}(\sigma_{\max}(\mathbf{W}), 0.9, 1.1)
$$

```python
import torch.nn.utils.spectral_norm as spectral_norm

class FlowLayer(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.weight = spectral_norm(nn.Linear(dim, dim))

    def forward(self, z):
        return self.weight(z)
```

**åŠ¹æœ**: Jacobian ã®è¡Œåˆ—å¼ãŒ $[10^{-5}, 10^5]$ ã®ç¯„å›²ã«åã¾ã‚Šã€å‹¾é…ãŒå®‰å®šã€‚

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯4: Importance Weighted ELBO (IWAE) ã®å®Ÿè£…

$$
\log p(\mathbf{x}) \geq \mathcal{L}_K = \mathbb{E}_{\mathbf{z}_{1:K} \sim q} \left[ \log \frac{1}{K} \sum_{k=1}^K \frac{p(\mathbf{x}, \mathbf{z}_k)}{q(\mathbf{z}_k|\mathbf{x})} \right]
$$

```python
def iwae_elbo(x, encoder, decoder, K=50):
    """K ã‚µãƒ³ãƒ—ãƒ«ã«ã‚ˆã‚‹ IWAE objective"""
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    mu, logvar = encoder(x)  # shape: (batch, latent_dim)

    # Kå€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ
    eps = torch.randn(K, *mu.shape)  # (K, batch, latent_dim)
    z = mu + eps * (0.5 * logvar).exp()

    # ãƒ­ã‚°å°¤åº¦ã¨äº‹å‰åˆ†å¸ƒ
    log_p_x_z = decoder.log_prob(x.unsqueeze(0), z)  # (K, batch)
    log_p_z = -0.5 * (z**2).sum(dim=-1)  # (K, batch)
    log_q_z_x = -0.5 * ((z - mu)**2 / logvar.exp() + logvar).sum(dim=-1)

    # Importance weights
    log_w = log_p_x_z + log_p_z - log_q_z_x  # (K, batch)

    # log-sum-exp ã®å®‰å®šè¨ˆç®—
    iwae_elbo = torch.logsumexp(log_w, dim=0) - np.log(K)  # (batch,)
    return -iwae_elbo.mean()  # è² å·ï¼ˆæœ€å¤§åŒ–â†’æœ€å°åŒ–ï¼‰
```

**åŠ¹æœ**: $K=1$ (æ¨™æº–ELBO) â†’ $K=50$ ã§ log-likelihood ãŒ $\sim$10 nats æ”¹å–„ã€‚

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯5: Multi-Scale Latent Spaceï¼ˆéšå±¤VAEï¼‰

ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®æ½œåœ¨å¤‰æ•°ã‚’å°å…¥:

$$
\begin{aligned}
\mathbf{z}_1 &\sim q_\phi(\mathbf{z}_1|\mathbf{x}) \quad \text{(fine-grained)} \\
\mathbf{z}_2 &\sim q_\phi(\mathbf{z}_2|\mathbf{z}_1) \quad \text{(coarse)}
\end{aligned}
$$

ELBO:

$$
\mathcal{L} = \mathbb{E}_{q} [\log p(\mathbf{x}|\mathbf{z}_1)] - D_{\text{KL}}(q(\mathbf{z}_1|\mathbf{x}) \| p(\mathbf{z}_1|\mathbf{z}_2)) - D_{\text{KL}}(q(\mathbf{z}_2|\mathbf{z}_1) \| p(\mathbf{z}_2))
$$

```python
class HierarchicalVAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.enc_z1 = Encoder(input_dim, z1_dim)
        self.enc_z2 = Encoder(z1_dim, z2_dim)
        self.dec_z1 = Decoder(z2_dim, z1_dim)
        self.dec_x = Decoder(z1_dim, input_dim)

    def elbo(self, x):
        # Bottom-up encoding
        mu1, logvar1 = self.enc_z1(x)
        z1 = reparameterize(mu1, logvar1)
        mu2, logvar2 = self.enc_z2(z1)
        z2 = reparameterize(mu2, logvar2)

        # Top-down decoding
        mu1_prior, logvar1_prior = self.dec_z1(z2)
        x_recon = self.dec_x(z1)

        # ELBO terms
        recon = -log_likelihood(x, x_recon)
        kl_z1 = kl_divergence(mu1, logvar1, mu1_prior, logvar1_prior)
        kl_z2 = kl_divergence(mu2, logvar2)  # N(0,I) prior

        return recon + kl_z1 + kl_z2
```

**å¿œç”¨**: ç”»åƒï¼ˆãƒ”ã‚¯ã‚»ãƒ«ãƒ»ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ»ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰ã€éŸ³å£°ï¼ˆæ³¢å½¢ãƒ»ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆãƒ»éŸ»å¾‹ï¼‰ã®éšå±¤è¡¨ç¾ã€‚

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯6: Straight-Through Estimatorï¼ˆé›¢æ•£æ½œåœ¨å¤‰æ•°ï¼‰

é›¢æ•£ $\mathbf{z} \in \{0, 1\}^d$ ã®å ´åˆã€å‹¾é…ãŒä¸é€£ç¶š â†’ Gumbel-Softmax ã‚„ Straight-Through ã‚’ä½¿ç”¨ã€‚

```python
def straight_through_bernoulli(logits):
    """Forward: é›¢æ•£ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°, Backward: é€£ç¶šè¿‘ä¼¼"""
    # Forward
    probs = torch.sigmoid(logits)
    z_hard = (probs > 0.5).float()

    # Straight-through: å‹¾é…ã¯ probs ã«æµã™
    z = z_hard - probs.detach() + probs
    return z

# è¨“ç·´
logits = encoder(x)
z = straight_through_bernoulli(logits)  # {0, 1}^d
x_recon = decoder(z)
```

**ç†è«–çš„æ ¹æ‹ **: REINFORCE ã®åˆ†æ•£å‰Šæ¸›ç‰ˆã€‚ãƒã‚¤ã‚¢ã‚¹ã¯ã‚ã‚‹ãŒã€å®Ÿç”¨ä¸Šã¯æœ‰åŠ¹ã€‚

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯7: Posterior Temperingï¼ˆæ¢ç´¢ã®ä¿ƒé€²ï¼‰

$$
q_{\text{temp}}(\mathbf{z}|\mathbf{x}) \propto q_\phi(\mathbf{z}|\mathbf{x})^{1/T}
$$

$T > 1$ ã§åˆ†æ•£ãŒå¢—åŠ  â†’ æ¢ç´¢ãŒæ´»ç™ºåŒ–ã€‚

```python
def tempered_sample(mu, logvar, temperature=1.5):
    """æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åˆ†æ•£ã‚’èª¿æ•´"""
    std_tempered = (0.5 * logvar).exp() * np.sqrt(temperature)
    eps = torch.randn_like(mu)
    return mu + std_tempered * eps
```

**ä½¿ã„åˆ†ã‘**:
- è¨“ç·´åˆæœŸ: $T=2.0$ ï¼ˆå¤šæ§˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚’æ¢ç´¢ï¼‰
- è¨“ç·´å¾ŒæœŸ: $T=1.0$ ï¼ˆçœŸã®äº‹å¾Œåˆ†å¸ƒã«åæŸï¼‰

### ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯8: Evidence æ¨å®šã®å®Ÿè·µçš„æ‰‹æ³•

çœŸã®å¯¾æ•°å°¤åº¦ $\log p(\mathbf{x})$ ã‚’æ¨å®šã™ã‚‹3ã¤ã®æ–¹æ³•:

#### æ–¹æ³•1: Annealed Importance Sampling (AIS)

$$
\log p(\mathbf{x}) \approx \log \frac{1}{K} \sum_{k=1}^K w_k, \quad w_k = \prod_{t=1}^T \frac{p_t(\mathbf{z}_{k,t})}{p_{t-1}(\mathbf{z}_{k,t})}
$$

ã“ã“ã§ $p_0 = q(\mathbf{z}|\mathbf{x})$, $p_T = p(\mathbf{z})$, $p_t = p_0^{1-\beta_t} p_T^{\beta_t}$ã€‚

#### æ–¹æ³•2: IWAE upper bound

$$
\log p(\mathbf{x}) \approx \mathcal{L}_K = \mathbb{E} \left[ \log \frac{1}{K} \sum_{k=1}^K w_k \right]
$$

$K \to \infty$ ã§çœŸå€¤ã«åæŸï¼ˆå˜èª¿å¢—åŠ ï¼‰ã€‚

#### æ–¹æ³•3: Harmonic Mean Estimatorï¼ˆéæ¨å¥¨ï¼‰

$$
\frac{1}{p(\mathbf{x})} \approx \frac{1}{K} \sum_{k=1}^K \frac{1}{p(\mathbf{x}, \mathbf{z}_k) / q(\mathbf{z}_k|\mathbf{x})}
$$

**è­¦å‘Š**: åˆ†æ•£ãŒç„¡é™å¤§ã«ãªã‚Šå¾—ã‚‹ â†’ å®Ÿç”¨ä¸å¯ã€‚

**æ¨å¥¨**: IWAE ($K=5000$) ã¾ãŸã¯ AISã€‚

### å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

| é …ç›® | æ¨å¥¨è¨­å®š | ç†ç”± |
|:---|:---|:---|
| Optimizer | Adam (lr=1e-3) | ELBO ã®éå‡¸æ€§ã«å¼·ã„ |
| Batch size | 128-512 | KLé …ã®æ¨å®šåˆ†æ•£ã‚’å‰Šæ¸› |
| KL warmup | 10 epochs | Posterior collapse å›é¿ |
| Free bits | $\lambda=2.0$ | æƒ…å ±ä¿æŒã®ä¿è¨¼ |
| Gradient clipping | norm â‰¤ 10 | Flow ã®å‹¾é…çˆ†ç™ºé˜²æ­¢ |
| IWAE samples | $K=50$ (test) | Log-likelihood æ¨å®š |
| Latent dim | $d \geq 32$ | è¡¨ç¾åŠ›ç¢ºä¿ |
| Spectral norm | Lipschitz â‰¤ 1.5 | Flow ã®å®‰å®šåŒ– |

---

## è£œéº7 â€” å¤‰åˆ†æ¨è«–ã®ç†è«–çš„æ·±æ˜ã‚Š

### å®šç†1: ELBO ã® Tightness ä¿è¨¼

**Jensen Gap**:

$$
\log p(\mathbf{x}) - \text{ELBO} = D_{\text{KL}}(q(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}|\mathbf{x})) \geq 0
$$

ç­‰å·æˆç«‹æ¡ä»¶: $q(\mathbf{z}|\mathbf{x}) = p(\mathbf{z}|\mathbf{x})$ï¼ˆçœŸã®äº‹å¾Œåˆ†å¸ƒã«ä¸€è‡´ï¼‰ã€‚

**ç³»**: $q$ ãŒçœŸã®äº‹å¾Œåˆ†å¸ƒã‚’è¡¨ç¾ã§ããªã„å ´åˆï¼ˆä¾‹: Mean-Field è¿‘ä¼¼ã§çœŸã®äº‹å¾ŒãŒå¤šå³°æ€§ï¼‰ã€ELBO ã¯å¿…ãš looseã€‚

### å®šç†2: IWAE ã®å˜èª¿æ€§

**Burda et al. (2015)**:

$$
\mathcal{L}_1 \leq \mathcal{L}_K \leq \mathcal{L}_{K'} \leq \log p(\mathbf{x}), \quad K < K'
$$

ã‹ã¤ã€$\lim_{K \to \infty} \mathcal{L}_K = \log p(\mathbf{x})$ã€‚

**è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ**: Jensen ä¸ç­‰å¼ã‚’ $\log \mathbb{E}[\cdot]$ ã«é©ç”¨ã€‚

### å®šç†3: Normalizing Flows ã® Universal Approximation

**Theorem (Kobyzev et al. 2020)**:

ååˆ†ãªæ·±ã•ï¼ˆå±¤æ•° $K$ï¼‰ã¨ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼‰ã‚’æŒã¤ Normalizing Flows ã¯ã€ä»»æ„ã®æ»‘ã‚‰ã‹ãªåˆ†å¸ƒ $p(\mathbf{z})$ ã‚’ä»»æ„ã®ç²¾åº¦ã§è¿‘ä¼¼ã§ãã‚‹ã€‚

**æ¡ä»¶**:
1. å„å±¤ $f_k$ ãŒ universal approximatorï¼ˆä¾‹: affine coupling with NNï¼‰
2. $K \to \infty$

**å®Ÿç”¨çš„æ„ç¾©**: ç†è«–ä¸Šã¯ã€ã©ã‚“ãªè¤‡é›‘ãªäº‹å¾Œåˆ†å¸ƒã‚‚ Flow ã§è¡¨ç¾å¯èƒ½ã€‚

### è£œé¡Œ: Reparameterization Gradient ã®ä¸åæ€§

$$
\nabla_\phi \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [f(\mathbf{z})] = \mathbb{E}_{\boldsymbol{\epsilon} \sim p(\boldsymbol{\epsilon})} [\nabla_\phi f(g_\phi(\boldsymbol{\epsilon}, \mathbf{x}))]
$$

ã“ã“ã§ $\mathbf{z} = g_\phi(\boldsymbol{\epsilon}, \mathbf{x})$ ã¯ reparameterization é–¢æ•°ã€‚

**è¨¼æ˜**: å¤‰æ•°å¤‰æ› $\mathbf{z} \to \boldsymbol{\epsilon}$ ã«ã‚ˆã‚Šã€$\phi$ ãŒåˆ†å¸ƒã®å¤–ã«å‡ºã‚‹ â†’ å¾®åˆ†ã¨æœŸå¾…å€¤ã®äº¤æ›ãŒå¯èƒ½ã€‚

### å®šç†4: KL Divergence ã®æƒ…å ±å¹¾ä½•çš„æ€§è³ª

$$
D_{\text{KL}}(q \| p) = \mathbb{E}_q \left[ \log \frac{q}{p} \right] = H(q, p) - H(q)
$$

ã“ã“ã§ $H(q, p)$ ã¯äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€$H(q)$ ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€‚

**æ€§è³ª**:
1. éè² æ€§: $D_{\text{KL}}(q \| p) \geq 0$
2. éå¯¾ç§°æ€§: $D_{\text{KL}}(q \| p) \neq D_{\text{KL}}(p \| q)$
3. å‡¸æ€§: $q$ ã¨ $p$ ã®ä¸¡æ–¹ã«ã¤ã„ã¦å‡¸é–¢æ•°

**å¹¾ä½•å­¦çš„è§£é‡ˆ**: KL ã¯æƒ…å ±å¹¾ä½•å­¦ã«ãŠã‘ã‚‹Bregman divergence ã®ä¸€ç¨®ã€‚

### å®šç†5: Amortization Gap ã®ä¸‹ç•Œ

**Kim et al. (2021)**:

ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ $C$ (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°) ãŒæœ‰é™ã®ã¨ãã€

$$
\text{Gap} \geq \Omega\left( \frac{1}{\sqrt{C}} \right)
$$

**å«æ„**: ç„¡é™ã®ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ã§ã‚‚ Gap > 0 ã®å¯èƒ½æ€§ï¼ˆãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®è¤‡é›‘æ€§ã«ä¾å­˜ï¼‰ã€‚

### è£œé¡Œ: Î²-VAE ã®æƒ…å ±ç†è«–çš„è§£é‡ˆ

$$
\mathcal{L}_\beta = \underbrace{\mathbb{E}_q [\log p(\mathbf{x}|\mathbf{z})]}_{\text{Rate (åœ§ç¸®ç‡)}} - \beta \underbrace{D_{\text{KL}}(q(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))}_{\text{Distortion (æ­ªã¿)}}
$$

Rate-Distortion ç†è«–ã¨ã®å¯¾å¿œ:
- $\beta < 1$: é«˜ãƒ¬ãƒ¼ãƒˆï¼ˆæƒ…å ±ä¿æŒå„ªå…ˆï¼‰
- $\beta = 1$: æ¨™æº– VAE
- $\beta > 1$: ä½ãƒ¬ãƒ¼ãƒˆï¼ˆåœ§ç¸®å„ªå…ˆã€disentanglement ä¿ƒé€²ï¼‰

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^20]: Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. *ICML 2015*.
@[card](https://arxiv.org/abs/1505.05770)

[^21]: Akram, A., Lee, J., & Shelton, C. R. (2024). Stable Training of Normalizing Flows for High-dimensional Variational Inference.
@[card](https://arxiv.org/abs/2402.16408)

[^22]: Vafaii, H., Galor, D., Yates, J. L., Butts, D. A., & Pillow, J. W. (2024). Poisson Variational Autoencoder. *NeurIPS 2024*.
@[card](https://proceedings.neurips.cc/paper_files/paper/2024/hash/4f3cb9576dc99d62b80726690453716f-Abstract-Conference.html)

[^23]: van den Berg, R., Hasenclever, L., Tomczak, J. M., & Welling, M. (2018). Sylvester Normalizing Flows for Variational Inference. *UAI 2018*.
@[card](https://arxiv.org/abs/1803.05649)

[^24]: Kim, Y., Wiseman, S., Miller, A. C., Sontag, D., & Rush, A. M. (2021). Reducing the Amortization Gap in Variational Autoencoders: A Bayesian Random Function Approach.
@[card](https://arxiv.org/abs/2102.03151)

[^25]: Zhang, Y., Williamson, S. A., & Murphy, S. A. (2023). Variational Inference for Longitudinal Data Using Normalizing Flows.
@[card](https://arxiv.org/abs/2303.14220)

[^26]: Ramesh, P., Doucet, A., & Teh, Y. W. (2024). Variational Autoencoders for Efficient Simulation-Based Inference.
@[card](https://arxiv.org/abs/2411.14511)

### è¿½åŠ æ–‡çŒ®

- Kobyzev, I., Prince, S. J., & Brubaker, M. A. (2020). Normalizing Flows: An Introduction and Review of Current Methods. *IEEE TPAMI*, 43(11), 3964-3979. arXiv:1908.09257.
- Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). Density estimation using Real NVP. *ICLR 2017*. arXiv:1605.08803.
- Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. *ICML 2015*. arXiv:1505.05770.
- Burda, Y., Grosse, R., & Salakhutdinov, R. (2015). Importance Weighted Autoencoders. *ICLR 2016*. arXiv:1509.00519.
- MaalÃ¸e, L., SÃ¸nderby, C. K., SÃ¸nderby, S. K., & Winther, O. (2016). Auxiliary Deep Generative Models. *ICML 2016*. arXiv:1602.05473.
- Tomczak, J., & Welling, M. (2018). VAE with a VampPrior. *AISTATS 2018*. arXiv:1705.07120.
- Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., & Lerchner, A. (2017). Î²-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. *ICLR 2017*.

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
