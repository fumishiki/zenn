---
title: "ç¬¬43å› (Part 2): Diffusion Transformers & é«˜é€Ÿç”Ÿæˆ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼""
emoji: "ğŸ¨"
type: "tech"
topics: ["machinelearning", "deeplearning", "diffusiontransformers", "julia", "dit"]
published: true
---
## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” 3è¨€èªã§DiTã‚’å®Ÿè£…ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: âš¡Julia ã§ DiT è¨“ç·´ã€ğŸ¦€Rust ã§æ¨è«–ã€ğŸ”®Elixir ã§åˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã€‚

### 4.1 âš¡ Julia: Mini-DiT è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**è¨“ç·´ã®å…¨ä½“åƒ**:
1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (MNIST)
2. DiT ãƒ¢ãƒ‡ãƒ«å®šç¾© (Lux.jl)
3. æ‹¡æ•£ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (DDPM noise schedule)
4. æå¤±é–¢æ•° (MSE between predicted & true noise)
5. è¨“ç·´ãƒ«ãƒ¼ãƒ— (Adam optimizer)

**å®Œå…¨å®Ÿè£…**:
```julia
using Lux, Optimisers, Zygote, MLUtils, Statistics

# 1. DiT Model Definition
function create_dit(; patch_size=4, hidden_dim=256, num_layers=6, num_heads=4)
    # Patchify layer
    patchify = Dense(patch_size^2, hidden_dim)

    # Positional Encoding (learned)
    pe = NamedTuple{(:pe,)}((randn(Float32, hidden_dim, (28Ã·patch_size)^2),))

    # DiT blocks
    blocks = Chain([
        Chain(
            LayerNorm(hidden_dim),
            MultiHeadAttention(hidden_dim, num_heads),
            LayerNorm(hidden_dim),
            Dense(hidden_dim, 4*hidden_dim, gelu),
            Dense(4*hidden_dim, hidden_dim)
        )
        for _ in 1:num_layers
    ]...)

    # Unpatchify layer
    unpatchify = Dense(hidden_dim, patch_size^2)

    return Chain(patchify, blocks, unpatchify), pe
end

# 2. Diffusion Schedule (DDPM)
function get_noise_schedule(T=1000)
    Î²_start, Î²_end = 1e-4, 0.02
    Î² = range(Î²_start, Î²_end, length=T)
    Î± = 1 .- Î²
    Î±_bar = cumprod(Î±)
    return (; Î², Î±, Î±_bar)
end

# 3. Training Step
function train_step(model, ps, st, x, schedule, t, opt_state)
    # Sample noise
    Îµ = randn(Float32, size(x))

    # Forward diffusion: x_t = âˆšá¾±_tÂ·x + âˆš(1-á¾±_t)Â·Îµ
    Î±_bar_t = schedule.Î±_bar[t]
    x_t = sqrt(Î±_bar_t) .* x .+ sqrt(1 - Î±_bar_t) .* Îµ

    # Predict noise
    loss, grads = withgradient(ps) do p
        Îµ_pred, _ = model(x_t, p, st)
        mean((Îµ_pred .- Îµ).^2)  # MSE loss
    end

    # Update parameters
    opt_state, ps = Optimisers.update(opt_state, ps, grads[1])

    return loss, ps, opt_state
end

# 4. Training Loop
function train_dit(; epochs=10, batch_size=64)
    # Load MNIST (dummy data for demonstration)
    x_train = randn(Float32, 28, 28, 1, 1000)  # 1000 samples

    # Initialize model
    model, pe = create_dit()
    ps, st = Lux.setup(Random.default_rng(), model)
    opt_state = Optimisers.setup(Adam(1e-4), ps)

    # Noise schedule
    schedule = get_noise_schedule()

    # Training
    for epoch in 1:epochs
        total_loss = 0.0
        for batch in eachbatch(x_train, size=batch_size)
            t = rand(1:1000)  # random timestep
            loss, ps, opt_state = train_step(model, ps, st, batch, schedule, t, opt_state)
            total_loss += loss
        end
        println("Epoch $epoch: Loss = $(total_loss / (size(x_train, 4) Ã· batch_size))")
    end

    return model, ps, st
end

# Run training
model, ps, st = train_dit(epochs=5)
println("âœ… Mini-DiT trained on MNIST!")
```

**Julia ã®å¼·ã¿**:
- **Lux.jl** â€” Pure functional NN library (JAX-like)
- **Zygote.jl** â€” Reverse mode AD (è‡ªå‹•å¾®åˆ†)
- **MLUtils.jl** â€” Data loading & batching
- **Reactant.jl** (æœªä½¿ç”¨ã ãŒé‡è¦) â€” GPU AOT compilation

### 4.2 ğŸ¦€ Rust: DiT æ¨è«–ã‚µãƒ¼ãƒãƒ¼

**æ¨è«–ã®å…¨ä½“åƒ**:
1. Candle ã§ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
2. DDPM sampling loop
3. ãƒãƒƒãƒå‡¦ç†
4. HTTP API (Axum)

**å®Œå…¨å®Ÿè£…**:
```rust
use candle_core::{Tensor, Device, DType};
use candle_nn::{Linear, VarBuilder, Module};
use anyhow::Result;

// DiT Block (simplified)
struct DiTBlock {
    attn: Linear,
    mlp: Linear,
}

impl DiTBlock {
    fn new(vb: VarBuilder, hidden_dim: usize) -> Result<Self> {
        let attn = Linear::new(vb.pp("attn").get((hidden_dim, hidden_dim))?, None);
        let mlp = Linear::new(vb.pp("mlp").get((4*hidden_dim, hidden_dim))?, None);
        Ok(Self { attn, mlp })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let a = self.attn.forward(x)?;
        let x = (x + a)?;  // residual
        let m = self.mlp.forward(&x)?;
        x + m  // residual
    }
}

// DiT Model
struct DiT {
    blocks: Vec<DiTBlock>,
}

impl DiT {
    fn new(vb: VarBuilder, num_layers: usize, hidden_dim: usize) -> Result<Self> {
        let mut blocks = Vec::new();
        for i in 0..num_layers {
            blocks.push(DiTBlock::new(vb.pp(&format!("block_{}", i)), hidden_dim)?);
        }
        Ok(Self { blocks })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let mut x = x.clone();
        for block in &self.blocks {
            x = block.forward(&x)?;
        }
        Ok(x)
    }
}

// DDPM Sampling
fn ddpm_sample(model: &DiT, schedule: &NoiseSchedule, shape: &[usize]) -> Result<Tensor> {
    let device = Device::Cpu;
    let mut x_t = Tensor::randn(0f32, 1.0, shape, &device)?;

    for t in (0..schedule.T).rev() {
        // Predict noise
        let epsilon_pred = model.forward(&x_t)?;

        // DDPM update: x_{t-1} = (x_t - Î²_t/âˆš(1-á¾±_t)Â·Îµ_Î¸) / âˆšÎ±_t + Ïƒ_tÂ·z
        let alpha_t = schedule.alpha[t];
        let alpha_bar_t = schedule.alpha_bar[t];
        let beta_t = schedule.beta[t];

        let coeff1 = (1.0 / alpha_t.sqrt())?;
        let coeff2 = (beta_t / (1.0 - alpha_bar_t).sqrt())?;
        let mean = ((x_t - (epsilon_pred * coeff2)?)? * coeff1)?;

        let z = if t > 0 {
            Tensor::randn(0f32, 1.0, shape, &device)?
        } else {
            Tensor::zeros(shape, DType::F32, &device)?
        };

        let sigma_t = beta_t.sqrt()?;
        x_t = (mean + (z * sigma_t)?)?;
    }

    Ok(x_t)
}

// HTTP Server (Axum)
#[tokio::main]
async fn main() -> Result<()> {
    use axum::{routing::post, Router, Json};
    use serde::{Deserialize, Serialize};

    #[derive(Deserialize)]
    struct GenerateRequest {
        prompt: String,
        num_samples: usize,
    }

    #[derive(Serialize)]
    struct GenerateResponse {
        images: Vec<Vec<f32>>,
    }

    async fn generate(Json(req): Json<GenerateRequest>) -> Json<GenerateResponse> {
        // Load model (dummy)
        let vb = VarBuilder::zeros(DType::F32, &Device::Cpu);
        let model = DiT::new(vb, 12, 768).unwrap();
        let schedule = NoiseSchedule::new(1000);

        // Generate
        let mut images = Vec::new();
        for _ in 0..req.num_samples {
            let img = ddpm_sample(&model, &schedule, &[1, 28, 28]).unwrap();
            images.push(img.to_vec1::<f32>().unwrap());
        }

        Json(GenerateResponse { images })
    }

    let app = Router::new().route("/generate", post(generate));
    axum::Server::bind(&"0.0.0.0:3000".parse()?)
        .serve(app.into_make_service())
        .await?;

    Ok(())
}

struct NoiseSchedule {
    T: usize,
    beta: Vec<f32>,
    alpha: Vec<f32>,
    alpha_bar: Vec<f32>,
}

impl NoiseSchedule {
    fn new(T: usize) -> Self {
        let beta: Vec<f32> = (0..T).map(|i| {
            1e-4 + (0.02 - 1e-4) * (i as f32 / T as f32)
        }).collect();
        let alpha: Vec<f32> = beta.iter().map(|b| 1.0 - b).collect();
        let mut alpha_bar = vec![alpha[0]];
        for i in 1..T {
            alpha_bar.push(alpha_bar[i-1] * alpha[i]);
        }
        Self { T, beta, alpha, alpha_bar }
    }
}
```

**Rust ã®å¼·ã¿**:
- **Candle** â€” HuggingFace ã® Rust ML framework
- **Axum** â€” é«˜é€Ÿ HTTP server (Tokio)
- **Zero-copy** â€” ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
- **å‹å®‰å…¨æ€§** â€” ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã‚¨ãƒ©ãƒ¼æ¤œå‡º

### 4.3 ğŸ”® Elixir: åˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°

**åˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã®å…¨ä½“åƒ**:
1. OTP Supervisor â€” è€éšœå®³æ€§
2. GenServer â€” ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚­ãƒ¥ãƒ¼
3. Load Balancing â€” GPUä¸¦åˆ—

**å®Œå…¨å®Ÿè£…**:
```elixir
defmodule DiT.Supervisor do
  use Supervisor

  def start_link(init_arg) do
    Supervisor.start_link(__MODULE__, init_arg, name: __MODULE__)
  end

  @impl true
  def init(_init_arg) do
    children = [
      {DiT.Worker, name: :worker_1, gpu_id: 0},
      {DiT.Worker, name: :worker_2, gpu_id: 1},
      {DiT.LoadBalancer, workers: [:worker_1, :worker_2]}
    ]

    Supervisor.init(children, strategy: :one_for_one)
  end
end

defmodule DiT.Worker do
  use GenServer

  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: opts[:name])
  end

  @impl true
  def init(opts) do
    gpu_id = opts[:gpu_id]
    # Initialize Rust NIF (Native Implemented Function)
    {:ok, model} = DiTNif.load_model(gpu_id)
    {:ok, %{model: model, gpu_id: gpu_id, queue: :queue.new()}}
  end

  @impl true
  def handle_call({:generate, prompt}, from, state) do
    # Add to queue
    queue = :queue.in({from, prompt}, state.queue)
    # Process immediately if queue was empty
    if :queue.len(state.queue) == 0 do
      process_next(state)
    else
      {:noreply, %{state | queue: queue}}
    end
  end

  defp process_next(state) do
    case :queue.out(state.queue) do
      {{:value, {from, prompt}}, queue} ->
        # Call Rust inference
        {:ok, image} = DiTNif.generate(state.model, prompt)
        GenServer.reply(from, {:ok, image})
        {:noreply, %{state | queue: queue}}
      {:empty, _} ->
        {:noreply, state}
    end
  end
end

defmodule DiT.LoadBalancer do
  use GenServer

  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  @impl true
  def init(opts) do
    workers = opts[:workers]
    {:ok, %{workers: workers, idx: 0}}
  end

  def generate(prompt) do
    GenServer.call(__MODULE__, {:generate, prompt})
  end

  @impl true
  def handle_call({:generate, prompt}, _from, state) do
    # Round-robin load balancing
    worker = Enum.at(state.workers, state.idx)
    idx = rem(state.idx + 1, length(state.workers))

    # Delegate to worker
    result = GenServer.call(worker, {:generate, prompt}, :infinity)
    {:reply, result, %{state | idx: idx}}
  end
end

# Rust NIF (Native Implemented Function) interface
defmodule DiTNif do
  use Rustler, otp_app: :dit, crate: "dit_nif"

  def load_model(_gpu_id), do: :erlang.nif_error(:nif_not_loaded)
  def generate(_model, _prompt), do: :erlang.nif_error(:nif_not_loaded)
end
```

**Elixir ã®å¼·ã¿**:
- **OTP Supervision** â€” ãƒ—ãƒ­ã‚»ã‚¹ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ™‚ã®è‡ªå‹•å†èµ·å‹•
- **GenServer** â€” ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°
- **Rustler** â€” Rust FFI (ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¨è«–)
- **åˆ†æ•£** â€” BEAM VM ã®è€éšœå®³æ€§

### 4.4 é«˜é€ŸSampling â€” DPM-Solver++ & EDM

**DPM-Solver++** [Lu+ 2022] [^9] (ç¬¬36å›ã®æ‹¡å¼µ):
- **åŠç·šå½¢ODEã‚½ãƒ«ãƒãƒ¼** â€” 1000 ã‚¹ãƒ†ãƒƒãƒ— â†’ 20 ã‚¹ãƒ†ãƒƒãƒ—
- **é«˜æ¬¡ç²¾åº¦** â€” Runge-Kutta æ³•ã®æ”¹è‰¯

**æ•°å¼** (2æ¬¡ DPM-Solver++):
$$
\mathbf{x}_{t_{i-1}} = \frac{\alpha_{t_{i-1}}}{\alpha_{t_i}} \mathbf{x}_{t_i} - \sigma_{t_{i-1}} \left( e^{-h_i} - 1 \right) \left( \epsilon_\theta^{(1)} + \frac{1}{2r_i} (\epsilon_\theta^{(1)} - \epsilon_\theta^{(2)}) \right)
$$
- $h_i = \lambda_{t_{i-1}} - \lambda_{t_i}$ â€” log-SNR step
- $\epsilon_\theta^{(1)}, \epsilon_\theta^{(2)}$ â€” 2æ®µéšã®ãƒã‚¤ã‚ºäºˆæ¸¬

**å®Ÿè£…**:
```julia
# DPM-Solver++ (2nd order)
function dpm_solver_pp(model, x_T, schedule, num_steps=20)
    T = schedule.T
    timesteps = Int.(round.(range(T, 1, length=num_steps)))

    x_t = x_T
    for i in 1:length(timesteps)-1
        t_i = timesteps[i]
        t_im1 = timesteps[i+1]

        # 1st-order prediction
        Îµ_1 = model(x_t, t_i)
        Î±_t = sqrt(schedule.Î±_bar[t_i])
        Î±_tm1 = sqrt(schedule.Î±_bar[t_im1])
        Ïƒ_t = sqrt(1 - schedule.Î±_bar[t_i])
        Ïƒ_tm1 = sqrt(1 - schedule.Î±_bar[t_im1])

        Î»_t = log(Î±_t / Ïƒ_t)
        Î»_tm1 = log(Î±_tm1 / Ïƒ_tm1)
        h = Î»_tm1 - Î»_t

        x_tm1_1st = (Î±_tm1 / Î±_t) .* x_t .- Ïƒ_tm1 .* (exp(-h) - 1) .* Îµ_1

        # 2nd-order correction
        Îµ_2 = model(x_tm1_1st, t_im1)
        r = (t_im1 - t_i) / (t_i - (i > 1 ? timesteps[i-1] : T))
        x_t = (Î±_tm1 / Î±_t) .* x_t .- Ïƒ_tm1 .* (exp(-h) - 1) .* (Îµ_1 .+ 0.5 / r .* (Îµ_1 .- Îµ_2))
    end

    return x_t
end
```

**EDM** [Karras+ 2022] [^10] (ç¬¬37å›ã®æ‹¡å¼µ):
- **æœ€é©ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«** â€” Ïƒ(t) ã®è¨­è¨ˆ
- **Deterministic/Stochastic çµ±åˆ** â€” Heun's method

**æ•°å¼**:
$$
\frac{d\mathbf{x}}{dt} = \frac{\mathbf{x} - D_\theta(\mathbf{x}, \sigma(t))}{\sigma(t)}
$$
- $D_\theta$ â€” Denoiser (EDM ã®è¡¨è¨˜)
- $\sigma(t) = t$ â€” æ™‚é–“ = ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«

**å®Ÿè£…**:
```julia
# EDM Sampling (Heun's method)
function edm_sample(model, schedule, num_steps=18)
    Ïƒ_min, Ïƒ_max = 0.002, 80.0
    Ï = 7.0

    # Noise schedule
    Ïƒ_steps = (Ïƒ_max^(1/Ï) .+ range(0, 1, length=num_steps) .* (Ïƒ_min^(1/Ï) - Ïƒ_max^(1/Ï))).^Ï

    # Initialize
    x_t = randn(size...) .* Ïƒ_max

    for i in 1:length(Ïƒ_steps)-1
        Ïƒ_i = Ïƒ_steps[i]
        Ïƒ_im1 = Ïƒ_steps[i+1]

        # Denoiser prediction
        D_i = model(x_t, Ïƒ_i)

        # Euler step
        d_i = (x_t - D_i) / Ïƒ_i
        x_euler = x_t + (Ïƒ_im1 - Ïƒ_i) * d_i

        # Heun's 2nd-order correction
        if Ïƒ_im1 > 0
            D_im1 = model(x_euler, Ïƒ_im1)
            d_im1 = (x_euler - D_im1) / Ïƒ_im1
            x_t = x_t + (Ïƒ_im1 - Ïƒ_i) * (d_i + d_im1) / 2
        else
            x_t = x_euler
        end
    end

    return x_t
end
```

**DPM-Solver++ vs EDM**:
- **DPM-Solver++**: DDPM ã®ç›´æ¥é«˜é€ŸåŒ– (log-SNR ç©ºé–“ã§ã® solver)
- **EDM**: SDE ã®æœ€é©åŒ– (Heun's method + Ïƒ(t) è¨­è¨ˆ)
- **é€Ÿåº¦**: ä¸¡æ–¹ã¨ã‚‚ 20 ã‚¹ãƒ†ãƒƒãƒ—ã§ DDPM 1000 ã‚¹ãƒ†ãƒƒãƒ—ç›¸å½“

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®70%å®Œäº†ï¼** å®Ÿè£…ã‚¾ãƒ¼ãƒ³å®Œèµ°ã€‚âš¡Julia è¨“ç·´ + ğŸ¦€Rust æ¨è«– + ğŸ”®Elixir åˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚° + é«˜é€ŸSampling ã‚’å…¨ã¦å®Ÿè£…ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ â€” aMUSEd-256 ãƒ‡ãƒ¢ã¨ Tiny DiT æ¼”ç¿’ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” aMUSEd-256 & Tiny DiT

**ã‚´ãƒ¼ãƒ«**: aMUSEd-256 ã§ 12 ã‚¹ãƒ†ãƒƒãƒ—é«˜é€Ÿç”»åƒç”Ÿæˆã‚’ä½“é¨“ã—ã€Tiny DiT on MNIST ã§ç†è«–ã‚’å®Ÿè£…ã«è½ã¨ã™ã€‚

### 5.1 aMUSEd-256 æ¨è«–ãƒ‡ãƒ¢ â€” 12ã‚¹ãƒ†ãƒƒãƒ—é«˜é€Ÿç”»åƒç”Ÿæˆ

**aMUSEd** [Patel+ 2024] [^11] ã¯ HuggingFace ãŒé–‹ç™ºã—ãŸ **Masked Image Model (MIM)** â€” Diffusion ã§ã¯ãªãã€é›¢æ•£çš„ãªãƒã‚¹ã‚¯äºˆæ¸¬ã§ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã€‚

**aMUSEd ã®ç‰¹å¾´**:
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: U-ViT (U-Net + Vision Transformer)
- **è¨“ç·´æ–¹å¼**: Masked token prediction (BERT-like)
- **Sampling**: 12 ã‚¹ãƒ†ãƒƒãƒ— (DDPM ã® 1000 ã‚¹ãƒ†ãƒƒãƒ— vs 83å€é«˜é€Ÿ)
- **ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º**: aMUSEd-256 (âˆ¼250M params) â€” ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œå¯èƒ½

**Diffusion vs MIM**:
| é …ç›® | Diffusion (DDPM) | MIM (aMUSEd) |
|:-----|:-----------------|:-------------|
| æ½œåœ¨ç©ºé–“ | é€£ç¶š (ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚º) | é›¢æ•£ (VQ-VAE ãƒˆãƒ¼ã‚¯ãƒ³) |
| è¨“ç·´ç›®æ¨™ | MSE(Îµ_pred, Îµ_true) | CrossEntropy(token_pred, token_true) |
| Sampling | 1000 ã‚¹ãƒ†ãƒƒãƒ— (iterative denoising) | 12 ã‚¹ãƒ†ãƒƒãƒ— (iterative unmasking) |
| é€Ÿåº¦ | é…ã„ | é€Ÿã„ (é›¢æ•£çš„ãªã®ã§é«˜é€Ÿ) |
| å“è³ª | é«˜ã„ (SD1.5 ãƒ¬ãƒ™ãƒ«) | ä¸­ç¨‹åº¦ (ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—å“è³ª) |

**aMUSEd ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°éç¨‹**:
1. å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯ `[MASK]` ã§åˆæœŸåŒ–
2. å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã€Œæœ€ã‚‚ç¢ºä¿¡åº¦ã®ä½ã„ãƒˆãƒ¼ã‚¯ãƒ³ã€ã‚’äºˆæ¸¬
3. äºˆæ¸¬ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã§ãƒã‚¹ã‚¯ã‚’ç½®æ›
4. 12 ã‚¹ãƒ†ãƒƒãƒ—å¾Œã€å…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒäºˆæ¸¬æ¸ˆã¿ â†’ ç”»åƒç”Ÿæˆå®Œäº†

**HuggingFace Diffusers ã§ã®å®Ÿè¡Œ**:
```python
from diffusers import AmusedPipeline
import torch

# Load aMUSEd-256 model
pipe = AmusedPipeline.from_pretrained("amused/amused-256", torch_dtype=torch.float16)
pipe = pipe.to("cuda")

# Generate image (12 steps)
prompt = "a photo of a cat wearing sunglasses"
image = pipe(
    prompt=prompt,
    num_inference_steps=12,  # 12 steps (vs DDPM 1000 steps)
    generator=torch.manual_seed(42)
).images[0]

image.save("amused_cat.png")
print(f"âœ… Generated image in 12 steps!")
```

**Julia ç‰ˆ (HuggingFace.jl çµŒç”±)**:
```julia
using PythonCall

# Import Diffusers
diffusers = pyimport("diffusers")
torch = pyimport("torch")

# Load pipeline
pipe = diffusers.AmusedPipeline.from_pretrained(
    "amused/amused-256",
    torch_dtype=torch.float16
)
pipe = pipe.to("cuda")

# Generate
prompt = "a photo of a cat wearing sunglasses"
result = pipe(
    prompt=prompt,
    num_inference_steps=12,
    generator=torch.manual_seed(42)
)
image = result.images[0]

# Save
image.save("amused_cat_julia.png")
println("âœ… aMUSEd-256 inference complete (Julia + PyCall)")
```

**aMUSEd vs DiT ã®æ¯”è¼ƒ**:
- **aMUSEd**: é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ç©ºé–“ (VQ-VAE) â€” BERT ã® Masked Language Modeling ã‚’ç”»åƒã«é©ç”¨
- **DiT**: é€£ç¶šãƒã‚¤ã‚ºç©ºé–“ (DDPM) â€” Transformer ã§ denoising

**ã©ã¡ã‚‰ãŒå„ªã‚Œã¦ã„ã‚‹ï¼Ÿ**
- **é€Ÿåº¦**: aMUSEd (12 steps) > DiT (50-100 steps with DPM-Solver++)
- **å“è³ª**: DiT (SD3/FLUX) > aMUSEd (ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ãƒ¬ãƒ™ãƒ«)
- **ç”¨é€”**: aMUSEd = ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆ / DiT = é«˜å“è³ªç”Ÿæˆ

### 5.2 Tiny DiT on MNIST â€” CPU 5åˆ†ã§å®Œèµ°

**Goal**: MNIST ã§ DiT ã‚’è¨“ç·´ã—ã€æ‰‹æ›¸ãæ•°å­—ã‚’ç”Ÿæˆã™ã‚‹ã€‚

**ä»•æ§˜**:
- ãƒ¢ãƒ‡ãƒ«: DiT-Tiny (4 layers, 128 hidden dim, 4 heads)
- ãƒ‡ãƒ¼ã‚¿: MNIST 28Ã—28 grayscale
- Patch size: 4Ã—4 (49 patches)
- è¨“ç·´æ™‚é–“: CPU ã§ 5 åˆ† (1 epoch)
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: âˆ¼800K

**å®Œå…¨å®Ÿè£…**:
```julia
using Flux, MLDatasets, Statistics, ProgressMeter

# 1. Data Loading
function load_mnist()
    train_x, train_y = MNIST.traindata(Float32)
    # Normalize to [-1, 1]
    train_x = (train_x .- 0.5) ./ 0.5
    # Add channel dimension
    train_x = reshape(train_x, 28, 28, 1, :)
    return train_x, train_y
end

# 2. DiT-Tiny Model
struct DiTBlock
    attn::MultiHeadAttention
    mlp::Chain
    ln1::LayerNorm
    ln2::LayerNorm
end

Flux.@functor DiTBlock

function DiTBlock(dim::Int, heads::Int)
    DiTBlock(
        MultiHeadAttention(dim, heads=heads),
        Chain(Dense(dim, 4*dim, gelu), Dense(4*dim, dim)),
        LayerNorm(dim),
        LayerNorm(dim)
    )
end

function (block::DiTBlock)(x)
    # Pre-norm + Attention + Residual
    x = x + block.attn(block.ln1(x))
    # Pre-norm + MLP + Residual
    x = x + block.mlp(block.ln2(x))
    return x
end

struct DiTTiny
    patchify::Dense
    blocks::Vector{DiTBlock}
    unpatchify::Dense
    pos_emb::Array{Float32, 2}  # [dim, num_patches]
end

Flux.@functor DiTTiny (patchify, blocks, unpatchify)

function DiTTiny(; patch_size=4, dim=128, depth=4, heads=4)
    H, W = 28, 28
    num_patches = (H Ã· patch_size) * (W Ã· patch_size)
    patch_dim = patch_size * patch_size

    DiTTiny(
        Dense(patch_dim, dim),
        [DiTBlock(dim, heads) for _ in 1:depth],
        Dense(dim, patch_dim),
        randn(Float32, dim, num_patches) .* 0.02  # learnable positional encoding
    )
end

function (model::DiTTiny)(x, t)
    # Patchify
    patches = patchify(x, 4)  # [num_patches, batch, patch_dim]
    z = model.patchify(patches)  # [num_patches, batch, dim]

    # Add positional encoding
    z = z .+ model.pos_emb

    # DiT blocks
    for block in model.blocks
        z = block(z)
    end

    # Unpatchify
    patches_out = model.unpatchify(z)
    x_out = unpatchify(patches_out, 4, size(x))

    return x_out
end

# 3. Patchify / Unpatchify
function patchify(x, P)
    B, H, W, C = size(x, 4), size(x, 1), size(x, 2), size(x, 3)
    N_h, N_w = H Ã· P, W Ã· P
    patches = zeros(Float32, P*P*C, N_h * N_w, B)

    for b in 1:B
        idx = 1
        for i in 0:N_h-1
            for j in 0:N_w-1
                patch = x[i*P+1:(i+1)*P, j*P+1:(j+1)*P, :, b]
                patches[:, idx, b] = vec(patch)
                idx += 1
            end
        end
    end
    return patches  # [patch_dim, num_patches, batch]
end

function unpatchify(patches, P, img_shape)
    H, W, C, B = img_shape
    N_h, N_w = H Ã· P, W Ã· P
    x = zeros(Float32, H, W, C, B)

    for b in 1:B
        idx = 1
        for i in 0:N_h-1
            for j in 0:N_w-1
                patch = reshape(patches[:, idx, b], P, P, C)
                x[i*P+1:(i+1)*P, j*P+1:(j+1)*P, :, b] = patch
                idx += 1
            end
        end
    end
    return x
end

# 4. Training
function train_dit_mnist(; epochs=1, batch_size=128, lr=1e-4)
    # Load data
    train_x, _ = load_mnist()
    train_x = train_x[:, :, :, 1:10000]  # Use 10k samples for speed

    # Initialize model
    model = DiTTiny()
    opt = Adam(lr)

    # Noise schedule (DDPM)
    T = 1000
    Î² = range(1e-4, 0.02, length=T)
    Î± = 1 .- Î²
    á¾± = cumprod(Î±)

    # Training loop
    @showprogress for epoch in 1:epochs
        total_loss = 0.0
        num_batches = 0

        for i in 1:batch_size:size(train_x, 4)-batch_size
            batch = train_x[:, :, :, i:i+batch_size-1]

            # Sample timestep
            t = rand(1:T)

            # Forward diffusion
            Îµ = randn(Float32, size(batch))
            x_t = sqrt(á¾±[t]) .* batch .+ sqrt(1 - á¾±[t]) .* Îµ

            # Compute loss and gradients
            loss, grads = Flux.withgradient(model) do m
                Îµ_pred = m(x_t, t)
                mean((Îµ_pred .- Îµ).^2)
            end

            # Update
            Flux.update!(opt, model, grads[1])

            total_loss += loss
            num_batches += 1
        end

        avg_loss = total_loss / num_batches
        println("Epoch $epoch: Loss = $avg_loss")
    end

    return model
end

# 5. Sampling
function sample_dit(model, schedule, num_samples=16)
    T = 1000
    x_t = randn(Float32, 28, 28, 1, num_samples)

    @showprogress for t in T:-1:1
        Îµ_pred = model(x_t, t)

        Î±_t = schedule.Î±[t]
        á¾±_t = schedule.á¾±[t]
        Î²_t = schedule.Î²[t]

        # DDPM update
        if t > 1
            z = randn(Float32, size(x_t))
        else
            z = zeros(Float32, size(x_t))
        end

        x_t = (x_t .- Î²_t / sqrt(1 - á¾±_t) .* Îµ_pred) ./ sqrt(Î±_t) .+ sqrt(Î²_t) .* z
    end

    return x_t
end

# Run training
println("Training Tiny DiT on MNIST...")
model = train_dit_mnist(epochs=1)

# Sample
schedule = (Î²=Î², Î±=Î±, á¾±=á¾±)
samples = sample_dit(model, schedule, 16)

# Save samples
using Images
grid = mosaicview([samples[:,:,1,i] for i in 1:16], nrow=4, npad=2)
save("tiny_dit_samples.png", colorview(Gray, grid))
println("âœ… Tiny DiT trained and sampled!")
```

**è¨“ç·´çµæœ** (äºˆæƒ³):
- Epoch 1: Loss = 0.15-0.25
- Epoch 5: Loss = 0.05-0.10
- ç”Ÿæˆå“è³ª: MNIST æ•°å­—ã® rough shape ãŒç”Ÿæˆã•ã‚Œã‚‹ (5 epoch ã§ recognizable)

**æ¼”ç¿’èª²é¡Œ**:
1. **Patch size ã‚’å¤‰ãˆã‚‹**: 4Ã—4 â†’ 7Ã—7 (patchæ•° 16 â†’ 4) â€” ã©ã†å¤‰ã‚ã‚‹ï¼Ÿ
2. **Depth ã‚’å¢—ã‚„ã™**: 4 layers â†’ 8 layers â€” æ€§èƒ½å‘ä¸Šï¼Ÿ
3. **AdaLN-Zero ã‚’è¿½åŠ **: Class-conditional DiT (æ•°å­—ãƒ©ãƒ™ãƒ«ã§æ¡ä»¶ä»˜ã‘)

### 5.3 aMUSEd vs DiT ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒ

**æ¯”è¼ƒå®Ÿé¨“**: MNIST ã§ aMUSEd-style MIM ã¨ DiT-style Diffusion ã‚’æ¯”è¼ƒ

**aMUSEd-style MIM å®Ÿè£…**:
```julia
# Masked Image Modeling (simplified)
function train_mim_mnist(; epochs=1)
    train_x, _ = load_mnist()

    # Quantize images to 16 levels (discrete tokens)
    train_x_quantized = round.(Int, (train_x .+ 1) .* 7.5)  # [0, 15]

    model = DiTTiny()  # same architecture
    opt = Adam(1e-4)

    for epoch in 1:epochs
        total_loss = 0.0
        num_batches = 0

        for i in 1:128:size(train_x, 4)-128
            batch = train_x_quantized[:, :, :, i:i+127]

            # Randomly mask 50% of patches
            mask = rand(Float32, size(batch)) .< 0.5
            batch_masked = batch .* mask

            # Predict masked tokens
            loss, grads = Flux.withgradient(model) do m
                pred = m(batch_masked, 0)  # no timestep
                # CrossEntropy loss
                mean((pred .- batch).^2)  # simplified as MSE
            end

            Flux.update!(opt, model, grads[1])
            total_loss += loss
            num_batches += 1
        end

        println("Epoch $epoch: MIM Loss = $(total_loss / num_batches)")
    end

    return model
end
```

**æ¯”è¼ƒçµæœ** (äºˆæƒ³):
| ãƒ¢ãƒ‡ãƒ« | è¨“ç·´æ™‚é–“ (1 epoch) | Sampling æ™‚é–“ (16 samples) | å“è³ª (ä¸»è¦³) |
|:-------|:-------------------|:---------------------------|:-----------|
| DiT (DDPM) | 5 min | 2 min (1000 steps) | High |
| MIM (aMUSEd-style) | 5 min | 10 sec (12 steps) | Medium |

**çµè«–**: MIM ã¯ Sampling ãŒåœ§å€’çš„ã«é€Ÿã„ãŒã€å“è³ªã¯ Diffusion ã«åŠ£ã‚‹ã€‚ç”¨é€”ã«å¿œã˜ã¦é¸æŠã€‚

### 5.4 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

**å•1**: DiT ã® Patchify ã§ã€256Ã—256 ç”»åƒã‚’ 16Ã—16 ãƒ‘ãƒƒãƒã«åˆ†å‰²ã™ã‚‹ã¨ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¯ã„ãã¤ï¼Ÿ
<details>
<summary>è§£ç­”</summary>

$$
N = \frac{H}{P} \times \frac{W}{P} = \frac{256}{16} \times \frac{256}{16} = 16 \times 16 = 256
$$
</details>

**å•2**: AdaLN-Zero ã®ã€ŒZero åˆæœŸåŒ–ã€ã¯ãªãœé‡è¦ï¼Ÿ
<details>
<summary>è§£ç­”</summary>

è¨“ç·´åˆæœŸã« $\gamma = 0, \beta = 0$ â†’ AdaLN ã®å‡ºåŠ› = 0 â†’ Residual æ¥ç¶šãŒæ’ç­‰å†™åƒã«ãªã‚Šã€å‹¾é…ãŒå®‰å®šã™ã‚‹ã€‚æ¡ä»¶ $c$ ã®å½±éŸ¿ã‚’å¾ã€…ã«å­¦ç¿’ã§ãã‚‹ã€‚
</details>

**å•3**: MM-DiT (SD3) ã® Joint Attention ã§ã¯ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãŒåŒã˜ Transformer ã§å‡¦ç†ã•ã‚Œã‚‹ã€‚ã“ã‚Œã®åˆ©ç‚¹ã¯ï¼Ÿ
<details>
<summary>è§£ç­”</summary>

ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãŒ **åŒã˜æ½œåœ¨ç©ºé–“** ã§ç›¸äº’ä½œç”¨ â†’ ãƒ†ã‚­ã‚¹ãƒˆãŒç”»åƒç”Ÿæˆã‚’ã‚ˆã‚Šå¼·ãæ¡ä»¶ä»˜ã‘ã§ãã‚‹ã€‚Classifier-Free Guidance ã§ã¯åˆ¥ã€…ã«å‡¦ç†ã—ã¦ã„ãŸãŒã€MM-DiT ã§ã¯çµ±åˆã•ã‚Œã¦åŠ¹ç‡çš„ã€‚
</details>

**å•4**: DPM-Solver++ ã¯ DDPM ã® 1000 ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½•ã‚¹ãƒ†ãƒƒãƒ—ã«å‰Šæ¸›ã§ãã‚‹ï¼Ÿ
<details>
<summary>è§£ç­”</summary>

20 ã‚¹ãƒ†ãƒƒãƒ— (50å€é«˜é€ŸåŒ–)ã€‚åŠç·šå½¢ODE solver ã§é«˜æ¬¡ç²¾åº¦ã‚’å®Ÿç¾ã€‚
</details>

**å•5**: aMUSEd ãŒ 12 ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆã§ãã‚‹ç†ç”±ã¯ï¼Ÿ
<details>
<summary>è§£ç­”</summary>

**é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ç©ºé–“** (VQ-VAE) ã§ Masked token prediction ã‚’è¡Œã†ãŸã‚ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã€Œæœ€ã‚‚ç¢ºä¿¡åº¦ã®ä½ã„ãƒˆãƒ¼ã‚¯ãƒ³ã€ã‚’äºˆæ¸¬ã—ã€ãƒã‚¹ã‚¯ã‚’ç½®æ›ã€‚é€£ç¶šãƒã‚¤ã‚ºé™¤å» (Diffusion) ã‚ˆã‚Šã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒå°‘ãªãæ¸ˆã‚€ã€‚
</details>

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**Challenge 1**: DiT ã« Class-conditional generation ã‚’è¿½åŠ 
- ãƒ’ãƒ³ãƒˆ: AdaLN-Zero ã® $\mathbf{c}$ ã« class embedding ã‚’è¿½åŠ 
- å®Ÿè£…: `c = vcat(t_emb, class_emb)` ã¨ã—ã¦ AdaLN ã«æ¸¡ã™

**Challenge 2**: DPM-Solver++ ã§ DiT ã® Sampling ã‚’é«˜é€ŸåŒ–
- ãƒ’ãƒ³ãƒˆ: Zone 4.4 ã®å®Ÿè£…ã‚’ DiT ã«çµ±åˆ
- ç›®æ¨™: 1000 ã‚¹ãƒ†ãƒƒãƒ— â†’ 20 ã‚¹ãƒ†ãƒƒãƒ—

**Challenge 3**: aMUSEd-256 ã§ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è©¦ã™
- ä¾‹: "a dog in a spacesuit", "abstract art with geometric shapes"
- è¦³å¯Ÿ: ã©ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å“è³ªãŒé«˜ã„ï¼Ÿ

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®85%å®Œäº†ï¼** å®Ÿé¨“ã‚¾ãƒ¼ãƒ³å®Œèµ°ã€‚aMUSEd-256 ãƒ‡ãƒ¢ã¨ Tiny DiT on MNIST ã§ã€ç†è«–ã‚’å®Ÿè£…ã«è½ã¨ã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ â€” æœ€æ–°ç ”ç©¶ã¨ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã€‚
:::

---

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” æœ€æ–°ç ”ç©¶ã¨ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ + ã¾ã¨ã‚

**ã‚´ãƒ¼ãƒ«**: 2024-2026 æœ€æ–°ç ”ç©¶ã‚’æ•´ç†ã—ã€DiT ã®æœªæ¥ã¨æœªè§£æ±ºå•é¡Œã‚’ç†è§£ã™ã‚‹ã€‚

### 6.1 DiT ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œå›³

```mermaid
graph TD
    A["Vision Transformer<br/>Dosovitskiy+ 2020"] --> B["DiT<br/>Peebles & Xie 2023"]
    B --> C["SD3 (MM-DiT)<br/>Esser+ 2024"]
    B --> D["FLUX<br/>Black Forest Labs 2024"]
    B --> E["SiT<br/>Ma+ 2024"]
    C --> F["Inference-Time Scaling<br/>Reflect-DiT 2025"]
    D --> G["Commercial Applications<br/>Apache 2.0"]
    E --> H["Stochastic Interpolants<br/>Theory"]

    style B fill:#ffd700
    style C fill:#98fb98
    style D fill:#98fb98
```

**ä¸–ä»£åˆ¥ã®é€²åŒ–**:
1. **ç¬¬1ä¸–ä»£ (2020-2022)**: ViT â€” Transformer ã‚’ Vision ã«é©ç”¨
2. **ç¬¬2ä¸–ä»£ (2023)**: DiT â€” Transformer ã‚’ Diffusion ã«é©ç”¨
3. **ç¬¬3ä¸–ä»£ (2024)**: MM-DiT â€” Multimodal çµ±åˆ (ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆ)
4. **ç¬¬4ä¸–ä»£ (2025-)**: Inference-Time Scaling â€” Test-time ã§ã®æ€§èƒ½å‘ä¸Š

### 6.2 2024-2026 æœ€æ–°ç ”ç©¶

#### SD3 (Stable Diffusion 3) â€” MM-DiT ã®å•†ç”¨åŒ–

**è«–æ–‡**: Esser+ (2024) "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis" [^3]

**é©æ–°ç‚¹**:
1. **MM-DiT** â€” Image ã¨ Text ã‚’åŒã˜ Transformer ã§å‡¦ç†
2. **Rectified Flow** â€” Flow Matching ã®ä¸€ç¨® (ç¬¬38å›ã§å­¦ã‚“ã )
3. **3ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€** â€” CLIP-L + CLIP-G + T5-XXL

**æ€§èƒ½**:
- Human preference: SD3 > SDXL > DALL-E 3
- Text-to-Image Benchmark: SD3 ãŒ Midjourney v6 ã«åŒ¹æ•µ

**åˆ¶ç´„**:
- ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: å•†ç”¨åˆ©ç”¨ã«åˆ¶é™ã‚ã‚Š (æœ‰æ–™ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å¿…è¦)
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: 2B (Medium) / 8B (Large) â€” GPU ãƒ¡ãƒ¢ãƒªè¦æ±‚ãŒé«˜ã„

#### FLUX â€” DiT ã®å•†ç”¨ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¢ãƒ‡ãƒ«

**é–‹ç™º**: Black Forest Labs (Stable Diffusion å‰µè¨­è€…ãŒè¨­ç«‹) [^4]

**é©æ–°ç‚¹**:
1. **Apache 2.0 ãƒ©ã‚¤ã‚»ãƒ³ã‚¹** â€” å®Œå…¨å•†ç”¨åˆ©ç”¨å¯èƒ½
2. **æ”¹è‰¯ã•ã‚ŒãŸ DiT** â€” ã‚ˆã‚ŠåŠ¹ç‡çš„ãª Attention
3. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç†è§£ã®å‘ä¸Š** â€” T5 + CLIP ã®çµ±åˆæœ€é©åŒ–

**ãƒ¢ãƒ‡ãƒ« variant**:
- FLUX.1-pro: æœ€é«˜å“è³ª (API ã®ã¿)
- FLUX.1-dev: é–‹ç™ºç”¨ (éå•†ç”¨)
- FLUX.1-schnell: é«˜é€Ÿç‰ˆ (4 ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆ)

**æ€§èƒ½**:
- Quality: FLUX > SD3 (ç‰¹ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¿ å®Ÿåº¦)
- Speed: FLUX-schnell = 4 ã‚¹ãƒ†ãƒƒãƒ—ã§ high quality

#### SiT (Scalable Interpolant Transformers) â€” ç†è«–çš„çµ±åˆ

**è«–æ–‡**: Ma+ (2024) "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers" [^8]

**é©æ–°ç‚¹**:
1. **Stochastic Interpolants** (ç¬¬38å›ã§å­¦ã‚“ã ) ã‚’ DiT ã«çµ±åˆ
2. **Flow ã¨ Diffusion ã®çµ±ä¸€** â€” ç¢ºç‡çš„è£œé–“ã§ä¸¡æ–¹ã‚’ã‚«ãƒãƒ¼
3. **Scaling Laws** â€” DiT ã¨åŒæ§˜ã« Transformer ã§ Scaling å¯èƒ½

**æ•°å¼** (å¾©ç¿’):
$$
\mathbf{x}_t = \alpha(t) \mathbf{x}_0 + \beta(t) \mathbf{x}_1 + \gamma(t) \mathbf{z}
$$
- $\gamma(t) = 0$ â†’ Flow Matching
- $\gamma(t) > 0$ â†’ Stochastic Interpolant

**æ€§èƒ½**:
- ImageNet 256Ã—256: FID = 2.06 (DiT-XL/2: FID = 2.27)
- Scaling: åŒæ§˜ã« Transformer ã® Scaling Laws ã«å¾“ã†

#### D2iT / DyDiT++ â€” Dynamic DiT

**è«–æ–‡**:
- D2iT (Dynamic DiT): CVPR 2025 [^12]
- DyDiT++ (2025): arXiv:2504.06803 [^13]

**é©æ–°ç‚¹**:
1. **å‹•çš„è¨ˆç®—é‡å‰²ã‚Šå½“ã¦** â€” é‡è¦ãªé ˜åŸŸã«è¨ˆç®—ã‚’é›†ä¸­
2. **Token pruning** â€” ä¸è¦ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‹•çš„ã«å‰Šé™¤
3. **Adaptive depth** â€” é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«ã¯æ·±ã„å±¤ã‚’ä½¿ç”¨

**åŠ¹æœ**:
- è¨ˆç®—é‡å‰Šæ¸›: âˆ¼30% (åŒå“è³ªã§)
- é€Ÿåº¦å‘ä¸Š: 1.5å€é«˜é€ŸåŒ–

**æ•°å¼**:
$$
\text{Keep}_i = \mathbb{1}[\text{Importance}(\mathbf{z}_i) > \tau]
$$
- $\text{Importance}(\mathbf{z}_i)$ â€” ãƒˆãƒ¼ã‚¯ãƒ³ $i$ ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢
- $\tau$ â€” é–¾å€¤ (å‹•çš„ã«èª¿æ•´)

#### Z-Image â€” æ¬¡ä¸–ä»£ç”»åƒç”Ÿæˆ

**è«–æ–‡**: arXiv:2511.22699 (2025 H2) [^14]

**è©³ç´°ã¯æœªå…¬é–‹** â€” ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ¨æ¸¬:
- **Z-space optimization** â€” æ½œåœ¨ç©ºé–“ã®æœ€é©åŒ–æ‰‹æ³•ï¼Ÿ
- **Zero-shot adaptation** â€” äº‹å‰è¨“ç·´ãƒ¢ãƒ‡ãƒ«ã® zero-shot é©ç”¨ï¼Ÿ

**æ¢ç´¢ãƒ’ãƒ³ãƒˆ**: `"Z-Image generation 2025 arXiv"` ã§æ¤œç´¢

### 6.3 Inference-Time Scaling â€” 2025-2026 ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆ

**å¾“æ¥ã® Scaling**: Training-time Scaling Laws
$$
L(N) = A \cdot N^{-\alpha} + L_\infty
$$
- $N$ = ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° (è¨“ç·´æ™‚ã«å›ºå®š)

**æ–°ã—ã„ Scaling**: Inference-Time Scaling (ç¬¬49å›ã§è©³è¿°)
$$
L(C) = B \cdot C^{-\beta} + L_\infty
$$
- $C$ = æ¨è«–æ™‚ã®è¨ˆç®—é‡ (å¯å¤‰)

**Reflect-DiT** [arXiv:2503.12271] [^15] (ICCV 2025):
- **Self-Reflection** â€” ç”Ÿæˆçµæœã‚’è‡ªå·±è©•ä¾¡ã—ã€å†ç”Ÿæˆ
- **Iterative refinement** â€” è¤‡æ•°å›ã® denoising ã§å“è³ªå‘ä¸Š
- **Test-time Training** â€” æ¨è«–æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´

**æ•°å¼**:
$$
\mathbf{x}_{t-1}^{(k+1)} = \mathbf{x}_{t-1}^{(k)} + \eta \nabla_{\mathbf{x}} \text{Quality}(\mathbf{x}_{t-1}^{(k)})
$$
- $k$ â€” Reflection iteration
- $\text{Quality}(\cdot)$ â€” å“è³ªè©•ä¾¡é–¢æ•° (CLIP score / FID)

**åŠ¹æœ**:
- FID æ”¹å–„: 5.2 â†’ 3.8 (åŒã˜ãƒ¢ãƒ‡ãƒ«ã§)
- è¨ˆç®—ã‚³ã‚¹ãƒˆ: 2-3å€ (Reflection ã®ãŸã‚)

**Inference-Time Scaling ã®æœªæ¥** (ç¬¬49å›ã§æ‰±ã†):
- Training Scaling Laws ã®é™ç•Œ â†’ Inference-Time Scaling ã¸ã‚·ãƒ•ãƒˆ
- ã€Œå¤§ããªãƒ¢ãƒ‡ãƒ«ã€â†’ã€Œè³¢ã„æ¨è«–ã€

### 6.4 æœªè§£æ±ºå•é¡Œ

**å•é¡Œ1: Scaling ã®é™ç•Œ**
- DiT ã¯ 8B params ã¾ã§è¨“ç·´ã•ã‚Œã¦ã„ã‚‹ãŒã€ã•ã‚‰ã«å¤§ããã™ã‚‹ã¨ï¼Ÿ
- **ä»®èª¬**: 100B params DiT ã¯æ„å‘³ãŒã‚ã‚‹ã‹ï¼Ÿ
- **èª²é¡Œ**: GPU ãƒ¡ãƒ¢ãƒªãƒ»è¨“ç·´æ™‚é–“ãƒ»ãƒ‡ãƒ¼ã‚¿é‡

**å•é¡Œ2: Long-range Dependencies**
- Self-Attention ã¯ $O(N^2)$ â€” é«˜è§£åƒåº¦ç”»åƒ (4K) ã§ã¯è¨ˆç®—ä¸å¯èƒ½
- **ç¾çŠ¶**: Latent space ã§åœ§ç¸® (SD3 ã¯ 64Ã—64 latent)
- **æœªæ¥**: Sparse Attention / Linear Attention / State Space Models (Mamba ç­‰)

**å•é¡Œ3: Controllability**
- DiT ã¯ Text-conditional ã ãŒã€ç´°ã‹ã„åˆ¶å¾¡ (ãƒãƒ¼ã‚ºãƒ»æ§‹å›³) ã¯å›°é›£
- **ç¾çŠ¶**: ControlNet (ç¬¬44å›ã§æ‰±ã†) ã§è§£æ±º
- **æœªæ¥**: Unified Multimodal Models (ç¬¬49å›ã§æ‰±ã†)

**å•é¡Œ4: Temporal Consistency (å‹•ç”»ç”Ÿæˆ)**
- DiT ã¯é™æ­¢ç”»ã®ã¿ â€” å‹•ç”»ç”Ÿæˆã«ã¯æ™‚é–“è»¸ãŒå¿…è¦
- **ç¾çŠ¶**: CogVideoX / Sora 2 (ç¬¬45å›ã§æ‰±ã†)
- **æœªæ¥**: 4D DiT (ç©ºé–“3æ¬¡å…ƒ + æ™‚é–“1æ¬¡å…ƒ)

**å•é¡Œ5: 3D Generation**
- DiT ã¯ 2D ã®ã¿ â€” 3D ç”Ÿæˆã«ã¯ NeRF / 3DGS ã¨ã®çµ±åˆãŒå¿…è¦
- **ç¾çŠ¶**: DreamFusion (ç¬¬46å›ã§æ‰±ã†)
- **æœªæ¥**: Native 3D DiT

### 6.5 ç ”ç©¶ãƒ†ãƒ¼ãƒã®è¦‹ã¤ã‘æ–¹

**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: Gap Analysis**
- æ—¢å­˜æ‰‹æ³•ã® **é™ç•Œ** ã‚’ç‰¹å®š
- ä¾‹: DiT ã¯ $O(N^2)$ â€” Linear Attention DiT ã§è§£æ±ºï¼Ÿ

**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ç†è«–æ‹¡å¼µ**
- æ—¢å­˜ç†è«–ã‚’ **ä¸€èˆ¬åŒ–**
- ä¾‹: SiT ã® Stochastic Interpolants ã‚’ Flow Matching ã®ä¸€èˆ¬åŒ–ã¨ã—ã¦æ‰ãˆã‚‹

**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ3: å†ç¾å®Ÿé¨“**
- è«–æ–‡ã‚’ **å®Œå…¨å†ç¾** â†’ æ”¹å–„ç‚¹ã‚’ç™ºè¦‹
- ä¾‹: DiT ã‚’ MNIST ã§å†ç¾ â†’ AdaLN-Zero ã®åˆæœŸåŒ–æ–¹æ³•ã‚’å¤‰ãˆãŸã‚‰ï¼Ÿ

**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ4: ç•°åˆ†é‡çµ±åˆ**
- ä»–åˆ†é‡ã®æ‰‹æ³•ã‚’ **è»¢ç”¨**
- ä¾‹: ODE solver (æ•°å€¤è§£æ) ã‚’ Diffusion ã«é©ç”¨ â†’ DPM-Solver++

**2026 ä»¥é™ã®äºˆæ¸¬**:
1. **Inference-Time Scaling ãŒä¸»æµã«** â€” Training Laws ã®é™ç•Œ
2. **Multimodal çµ±åˆ** â€” ç”»åƒãƒ»éŸ³å£°ãƒ»å‹•ç”»ãƒ»3D ã‚’1ãƒ¢ãƒ‡ãƒ«ã§
3. **World Models** â€” ç‰©ç†æ³•å‰‡ã‚’ç†è§£ã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ« (ç¬¬41å›ãƒ»ç¬¬49å›)
4. **Sparse/Linear Attention** â€” $O(N^2)$ ã®å…‹æœ

### 6.6 æ¨è–¦æ–‡çŒ®

**ä¸»è¦è«–æ–‡**:
1. Vision Transformer (ViT): Dosovitskiy+ 2020 [^1]
2. Diffusion Transformers (DiT): Peebles & Xie 2023 [^2]
3. Stable Diffusion 3 (MM-DiT): Esser+ 2024 [^3]
4. FLUX: Black Forest Labs 2024 [^4]
5. SiT: Ma+ 2024 [^8]
6. DPM-Solver++: Lu+ 2022 [^9]
7. EDM: Karras+ 2022 [^10]
8. aMUSEd: Patel+ 2024 [^11]
9. Reflect-DiT: arXiv:2503.12271 [^15]

**æ•™ç§‘æ›¸**:
- "Deep Learning" (Goodfellow, Bengio, Courville) â€” ç¬¬20ç«  Generative Models
- "Probabilistic Machine Learning" (Kevin Murphy) â€” ç¬¬27ç«  Diffusion Models

**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹**:
- HuggingFace Diffusers: https://huggingface.co/docs/diffusers/
- Papers With Code â€” Diffusion Models: https://paperswithcode.com/task/image-generation

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®95%å®Œäº†ï¼** ç™ºå±•ã‚¾ãƒ¼ãƒ³å®Œèµ°ã€‚æœ€æ–°ç ”ç©¶ã¨æœªè§£æ±ºå•é¡Œã‚’æ•´ç†ã—ãŸã€‚æ¬¡ã¯æœ€çµ‚ã‚¾ãƒ¼ãƒ³ â€” æŒ¯ã‚Šè¿”ã‚Šã¨æ¬¡å›äºˆå‘Šã€‚
:::

---


**ã‚´ãƒ¼ãƒ«**: ç¬¬43å›ã®è¦ç‚¹ã‚’æ•´ç†ã—ã€Course V ã®æ—…è·¯ã‚’è¦‹æ¸¡ã™ã€‚

### 6.7 ç¬¬43å›ã®è¦ç‚¹

**1. U-Net â†’ DiT ã®é©å‘½**:
- **å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã®æ”¾æ£„** â€” CNN ã®å±€æ‰€æ€§ã‚’æ¨ã¦ã€Self-Attention ã§å¤§åŸŸçš„é–¢ä¿‚ã‚’å­¦ç¿’
- **Scaling Laws ã®é©ç”¨** â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° $N$ âˆ æ€§èƒ½å‘ä¸Š (8B params ã¾ã§)
- **å®Ÿä¸–ç•Œã§ã®å„ªä½** â€” SD3 / FLUX ãŒ DALL-E 3 / Midjourney ã«åŒ¹æ•µ

**2. DiT ã®å¿ƒè‡“éƒ¨ â€” AdaLN-Zero**:
- æ‹¡æ•£ã‚¹ãƒ†ãƒƒãƒ— $t$ ã¨æ¡ä»¶ $c$ ã‚’ **æ­£è¦åŒ–å±¤ã«æ³¨å…¥**
- **Zero åˆæœŸåŒ–** â€” Residual æ¥ç¶šãŒè¨“ç·´åˆæœŸã®å‹¾é…ã‚’å®‰å®šåŒ–
- æ•°å¼: $\text{AdaLN-Zero}(\mathbf{x}, \mathbf{c}) = \gamma(\mathbf{c}) \odot \text{LN}(\mathbf{x}) + \beta(\mathbf{c})$

**3. MM-DiT (SD3) â€” Multimodal çµ±åˆ**:
- ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ **åŒã˜ Transformer** ã§å‡¦ç†
- **Joint Attention** â€” ç”»åƒ â†” ãƒ†ã‚­ã‚¹ãƒˆã®ç›¸äº’ä½œç”¨
- **3ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€** â€” CLIP-L + CLIP-G + T5-XXL

**4. é«˜é€ŸSampling**:
- **DPM-Solver++** â€” 1000 ã‚¹ãƒ†ãƒƒãƒ— â†’ 20 ã‚¹ãƒ†ãƒƒãƒ— (50å€é«˜é€ŸåŒ–)
- **EDM** â€” æœ€é©ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\sigma(t)$ + Heun's method

**5. aMUSEd vs DiT**:
- **aMUSEd** â€” é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ç©ºé–“ (VQ-VAE) ã§ 12 ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆ
- **DiT** â€” é€£ç¶šãƒã‚¤ã‚ºç©ºé–“ (DDPM) ã§é«˜å“è³ªç”Ÿæˆ
- **ç”¨é€”**: aMUSEd = ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  / DiT = é«˜å“è³ª

### 6.8 FAQ

**Q1: DiT ã¯ U-Net ã‚’å®Œå…¨ã«ç½®ãæ›ãˆã‚‹ï¼Ÿ**
A: **ç”¨é€”æ¬¡ç¬¬**ã€‚DiT ã¯ Scaling Laws ã«å¾“ã†ãŸã‚ã€å¤§è¦æ¨¡è¨“ç·´ã§ U-Net ã‚’è¶…ãˆã‚‹ã€‚ãŸã ã—ã€å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯ U-Net ã®å¸°ç´ãƒã‚¤ã‚¢ã‚¹ãŒæœ‰åˆ©ãªå ´åˆã‚‚ã‚ã‚‹ã€‚å•†ç”¨ãƒ¢ãƒ‡ãƒ« (SD3/FLUX) ã¯ DiT ã«ç§»è¡Œæ¸ˆã¿ã€‚

**Q2: AdaLN-Zero ã®ã€ŒZero åˆæœŸåŒ–ã€ã‚’å¿˜ã‚ŒãŸã‚‰ï¼Ÿ**
A: è¨“ç·´åˆæœŸã«æ¡ä»¶ $c$ ã®å½±éŸ¿ãŒå¼·ã™ãã¦ã€å‹¾é…ãŒä¸å®‰å®šã«ãªã‚‹ã€‚æœ€æ‚ªã®å ´åˆã€è¨“ç·´ãŒç™ºæ•£ã™ã‚‹ã€‚Zero åˆæœŸåŒ–ã«ã‚ˆã‚Šã€Residual æ¥ç¶šãŒè¨“ç·´åˆæœŸã¯æ’ç­‰å†™åƒã«ãªã‚Šã€å®‰å®šã™ã‚‹ã€‚

**Q3: MM-DiT ã¯ Classifier-Free Guidance ã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ï¼Ÿ**
A: **ç†è«–çš„ã«ã¯å„ªã‚Œã¦ã„ã‚‹**ã€‚CFG ã§ã¯æ¡ä»¶ä»˜ã/ç„¡æ¡ä»¶ã‚’åˆ¥ã€…ã«å‡¦ç†ã™ã‚‹ãŒã€MM-DiT ã§ã¯ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãŒåŒã˜æ½œåœ¨ç©ºé–“ã§ç›¸äº’ä½œç”¨ã™ã‚‹ã€‚ãŸã ã—ã€å®Ÿè£…ã®è¤‡é›‘ã•ã¨è¨“ç·´ã‚³ã‚¹ãƒˆã¯ MM-DiT ã®æ–¹ãŒé«˜ã„ã€‚

**Q4: aMUSEd ã® 12 ã‚¹ãƒ†ãƒƒãƒ—ã¯ Diffusion ã§ã‚‚å¯èƒ½ï¼Ÿ**
A: **DPM-Solver++ / EDM ã§ 20 ã‚¹ãƒ†ãƒƒãƒ—ã¾ã§å‰Šæ¸›å¯èƒ½**ã€‚ãŸã ã—ã€aMUSEd ã® 12 ã‚¹ãƒ†ãƒƒãƒ—ã«ã¯åŠã°ãªã„ã€‚é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ç©ºé–“ (MIM) ã®æ–¹ãŒã€é€£ç¶šãƒã‚¤ã‚ºç©ºé–“ (Diffusion) ã‚ˆã‚Šå°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—ã§æ¸ˆã‚€å‚¾å‘ãŒã‚ã‚‹ã€‚

**Q5: DiT ã®æœªæ¥ã¯ï¼Ÿ**
A: **3ã¤ã®æ–¹å‘**: (1) Inference-Time Scaling (Reflect-DiT) â€” æ¨è«–æ™‚ã«æ€§èƒ½å‘ä¸Šã€(2) Multimodal çµ±åˆ (ç¬¬49å›) â€” å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’1ãƒ¢ãƒ‡ãƒ«ã§ã€(3) World Models (ç¬¬41å›ãƒ»ç¬¬49å›) â€” ç‰©ç†æ³•å‰‡ã‚’ç†è§£ã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€‚

### 6.9 ã‚ˆãã‚ã‚‹é–“é•ã„

**é–“é•ã„1: Patchify ã§ flatten ã®é †åºã‚’é–“é•ãˆã‚‹**
```julia
# âŒ Wrong
patch = vec(x[i*P+1:(i+1)*P, j*P+1:(j+1)*P, :])  # channel ãŒå…ˆ

# âœ… Correct
patch = reshape(x[i*P+1:(i+1)*P, j*P+1:(j+1)*P, :], P*P*C)  # spatial ãŒå…ˆ
```

**é–“é•ã„2: AdaLN-Zero ã§ $\gamma, \beta$ ã‚’ shared ã«ã™ã‚‹**
```julia
# âŒ Wrong: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã§åŒã˜ Î³, Î²
Î³ = Î³_mlp(c)  # [D] â€” scalar per dimension
x_out = Î³' .* x_norm .+ Î²'  # broadcasting wrong

# âœ… Correct: ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«ç•°ãªã‚‹ Î³, Î² (å¿…è¦ã«å¿œã˜ã¦)
# ã¾ãŸã¯ã€å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã§ shared ãªã‚‰ broadcasting æ­£ã—ãä½¿ã†
```

**é–“é•ã„3: MM-DiT ã§ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ concat ã›ãšã«åˆ¥ã€…ã«å‡¦ç†**
```julia
# âŒ Wrong: åˆ¥ã€…ã® Attention
attn_img = attn(z_img)
attn_txt = attn(z_txt)

# âœ… Correct: Joint Attention
z = vcat(z_img, z_txt)
attn = attn_joint(z)
```

### 6.10 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (1é€±é–“ãƒ—ãƒ©ãƒ³)

| æ—¥ | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ |
|:---|:-------|:-----|
| 1 | Zone 0-2 èª­äº† + AdaLN-Zero å®Ÿè£… | 2h |
| 2 | Zone 3 èª­äº† (æ•°å¼ä¿®è¡Œ) + Boss Battle | 3h |
| 3 | Zone 4 å‰åŠ (Julia è¨“ç·´) | 2h |
| 4 | Zone 4 å¾ŒåŠ (Rust æ¨è«– + Elixir) | 2h |
| 5 | Zone 5 (aMUSEd ãƒ‡ãƒ¢ + Tiny DiT) | 2h |
| 6 | Zone 6 (æœ€æ–°ç ”ç©¶) + è«–æ–‡3æœ¬èª­ã‚€ | 3h |
| 7 | æ¼”ç¿’èª²é¡Œ + ç·å¾©ç¿’ | 2h |

**åˆè¨ˆ**: 16æ™‚é–“ (1æ—¥ 2-3æ™‚é–“ Ã— 1é€±é–“)

### 6.11 æ¬¡å›äºˆå‘Š: ç¬¬44å› éŸ³å£°ç”Ÿæˆ

**ãƒ†ãƒ¼ãƒ**: éŸ³å£°ç”Ÿæˆ (TTS / Music) â€” Neural Audio Codecs â†’ Zero-shot TTS â†’ Flow Matching for Audio

**ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯**:
1. **Neural Audio Codecs** â€” SoundStream â†’ EnCodec â†’ WavTokenizer â†’ Mimi
2. **Zero-shot TTS** â€” VALL-E 2 / F5-TTS / XTTS
3. **Music Generation** â€” MusicGen / Stable Audio / Suno v4.5
4. **Flow Matching for Audio** â€” MelodyFlow / Audio Diffusion â†’ Flow Matching ç§»è¡Œ
5. **Audio è©•ä¾¡æŒ‡æ¨™** â€” FAD â†’ KAD / CLAP Score

**æ¥ç¶š**:
- **ç¬¬43å› DiT**: ç”»åƒç”Ÿæˆã®æ¬¡ä¸–ä»£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **ç¬¬44å› éŸ³å£°**: éŸ³å£°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¸ã®æ‹¡å¼µ
- **ç¬¬45å› å‹•ç”»**: æ™‚ç©ºé–“æ‹¡å¼µ (ç”»åƒ+éŸ³å£° â†’ å‹•ç”»)

**Course V ã®æµã‚Œ**:
```mermaid
graph LR
    A["ç¬¬43å›<br/>DiT (ç”»åƒ)"] --> B["ç¬¬44å›<br/>éŸ³å£°"]
    B --> C["ç¬¬45å›<br/>å‹•ç”»"]
    C --> D["ç¬¬46å›<br/>3D"]
    D --> E["ç¬¬47å›<br/>Motion/4D"]
    E --> F["ç¬¬48å›<br/>ç§‘å­¦å¿œç”¨"]
    F --> G["ç¬¬49å›<br/>Unified/Inference"]
    G --> H["ç¬¬50å›<br/>å’æ¥­åˆ¶ä½œ"]
```

**åˆ°é”ç›®æ¨™ (Course V ä¿®äº†æ™‚)**:
- å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ (ç”»åƒãƒ»éŸ³å£°ãƒ»å‹•ç”»ãƒ»3Dãƒ»ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ç§‘å­¦) ã§ã®ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…
- 3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯èƒ½åŠ› (âš¡Julia è¨“ç·´ + ğŸ¦€Rust æ¨è«– + ğŸ”®Elixir é…ä¿¡)
- 2025-2026 ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ç†è§£ (Flow Matching / Inference-Time Scaling / Modal Unification)
- è«–æ–‡ãŒæ›¸ã‘ã‚‹ (Course IV) + ã‚·ã‚¹ãƒ†ãƒ ãŒä½œã‚Œã‚‹ (Course V)

**æº–å‚™ã™ã‚‹ã“ã¨**:
- PyTorch Audio / torchaudio ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- HuggingFace Transformers (éŸ³å£°ãƒ¢ãƒ‡ãƒ«ç”¨)
- Diffusers (Stable Audio ç”¨)

:::message
**ç¬¬43å›å®Œäº†ï¼ Course V ã‚¹ã‚¿ãƒ¼ãƒˆãƒ€ãƒƒã‚·ãƒ¥æˆåŠŸã€‚** DiTãƒ»MM-DiTãƒ»SiTãƒ»é«˜é€ŸSampling ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚æ¬¡ã¯éŸ³å£°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¸ â€” é™æ­¢ç”»ã‹ã‚‰æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¸ã®æ‹¡å¼µã€‚ç¬¬44å›ã§ä¼šãŠã†ï¼
:::

---

## ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€ŒU-Netã¯"éºç‰©"ã€‚Stable Diffusion ã¯æ—¢ã«éå»ã§ã¯ï¼Ÿã€**

2023å¹´ã€DiT è«–æ–‡ãŒç™ºè¡¨ã•ã‚ŒãŸæ™‚ã€å¤šãã®ç ”ç©¶è€…ã¯æ‡ç–‘çš„ã ã£ãŸ:
- ã€ŒU-Net ã¯ CNN ã®å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã‚’æ´»ç”¨ã—ã¦ã„ã‚‹ â€” ãªãœæ¨ã¦ã‚‹ï¼Ÿã€
- ã€ŒTransformer ã¯ $O(N^2)$ â€” ç”»åƒç”Ÿæˆã«ã¯éåŠ¹ç‡ã§ã¯ï¼Ÿã€
- ã€ŒDDPM / LDM ã¯æ—¢ã«ååˆ†é«˜å“è³ª â€” ãªãœå¤‰ãˆã‚‹ï¼Ÿã€

2024å¹´ã€SD3 ã¨ FLUX ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸã€‚ä¸¡æ–¹ã¨ã‚‚ DiT ãƒ™ãƒ¼ã‚¹ã€‚

2025å¹´ç¾åœ¨ã€DiT ã¯ **äº‹å®Ÿä¸Šã®æ¨™æº–** ã«ãªã£ãŸ:
- DALL-E 4 (æœªå…¬é–‹ã ãŒ DiT ã¨æ¨æ¸¬)
- Midjourney v7 (DiT ãƒ™ãƒ¼ã‚¹ã¨å™‚)
- ä¸­å›½ã®ä¸»è¦ãƒ¢ãƒ‡ãƒ« (Wan-2.1 / HunyuanVideo) ã‚‚ DiT

**å•ã„**:
1. **U-Net ã®å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã¯æœ¬å½“ã«å¿…è¦ã ã£ãŸã®ã‹ï¼Ÿ** â€” ãã‚Œã¨ã‚‚ã€ãƒ‡ãƒ¼ã‚¿é‡ãŒå¢—ãˆã‚Œã°ä¸è¦ã«ãªã‚‹ï¼Ÿ
2. **Transformer ã® $O(N^2)$ ã¯æœ¬å½“ã«å•é¡Œã‹ï¼Ÿ** â€” Latent space åœ§ç¸®ã§å›é¿ã§ãã‚‹ãªã‚‰ï¼Ÿ
3. **æ¬¡ã®"éºç‰©"ã¯ä½•ã‹ï¼Ÿ** â€” DiT ã‚‚10å¹´å¾Œã«ã¯éå»ã®æŠ€è¡“ã«ãªã‚‹ï¼Ÿ

**è­°è«–ãƒã‚¤ãƒ³ãƒˆ**:
- **å¸°ç´ãƒã‚¤ã‚¢ã‚¹ vs ãƒ‡ãƒ¼ã‚¿é§†å‹•**: å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ U-Net ãŒå‹ã¤ãŒã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ DiT ãŒå‹ã¤ã€‚ã§ã¯ã€ã€Œå°‘ãªã„ã€ã¨ã€Œå¤§è¦æ¨¡ã€ã®å¢ƒç•Œã¯ã©ã“ï¼Ÿ
- **Scaling Laws ã®æ™®éæ€§**: DiT ãŒ Scaling Laws ã«å¾“ã†ãªã‚‰ã€100B params DiT ã¯æ„å‘³ãŒã‚ã‚‹ï¼Ÿãã‚Œã¨ã‚‚é™ç•ŒãŒã‚ã‚‹ï¼Ÿ
- **æ¬¡ä¸–ä»£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: Transformer ã®æ¬¡ã¯ä½•ï¼Ÿ State Space Models (Mamba)ï¼Ÿ ãã‚Œã¨ã‚‚æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ï¼Ÿ

**æ­´å²çš„æ–‡è„ˆ**:
- 2015: CNN ãŒç”»åƒèªè­˜ã‚’æ”¯é… (ResNet)
- 2020: Vision Transformer (ViT) ãŒ CNN ã‚’è¶…ãˆã‚‹
- 2023: DiT ãŒ U-Net ã‚’è¶…ãˆã‚‹
- 2025: DiT ãŒæ¨™æº–ã«
- 20XX: ???

**ã‚ãªãŸã®è€ƒãˆã¯ï¼Ÿ** â€” æ¬¡ã®é©å‘½ã¯ä½•ã‹ï¼Ÿ

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale". *ICLR 2021*.
@[card](https://arxiv.org/abs/2010.11929)

[^2]: Peebles, W., & Xie, S. (2023). "Scalable Diffusion Models with Transformers". *ICCV 2023*.
@[card](https://arxiv.org/abs/2212.09748)

[^3]: Esser, P., Kulal, S., Blattmann, A., Entezari, R., MÃ¼ller, J., Saini, H., ... & Rombach, R. (2024). "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis". *arXiv:2403.03206*.
@[card](https://arxiv.org/abs/2403.03206)

[^4]: Black Forest Labs. (2024). "FLUX: A New Era of Generative AI". *Official Blog*.
@[card](https://blackforestlabs.ai/announcing-black-forest-labs/)

[^5]: Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). "Layer Normalization". *arXiv:1607.06450*.
@[card](https://arxiv.org/abs/1607.06450)

[^7]: Hendrycks, D., & Gimpel, K. (2016). "Gaussian Error Linear Units (GELUs)". *arXiv:1606.08415*.
@[card](https://arxiv.org/abs/1606.08415)

[^8]: Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-Eijnden, E., & Xie, S. (2024). "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers". *ICML 2024*.
@[card](https://arxiv.org/abs/2401.08740)

[^9]: Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., & Zhu, J. (2022). "DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models". *NeurIPS 2022*.
@[card](https://arxiv.org/abs/2211.01095)

[^10]: Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). "Elucidating the Design Space of Diffusion-Based Generative Models". *NeurIPS 2022*.
@[card](https://arxiv.org/abs/2206.00364)

[^11]: Patel, S., Katsch, M., Thulke, D., Daras, G., Shi, H., Karrer, B., ... & Susskind, J. (2024). "aMUSEd: An Open MUSE Reproduction". *arXiv:2410.14086*.
@[card](https://arxiv.org/abs/2410.14086)

[^12]: Jia, W., Huang, M., Chen, N., Zhang, L., & Mao, Z. (2025). "D2iT: Dynamic Diffusion Transformer for Accurate Image Generation". *CVPR 2025*. arXiv:2504.09454.
@[card](https://arxiv.org/abs/2504.09454)

[^13]: DyDiT++ (2025). "Improved Dynamic Diffusion Transformers". *arXiv:2504.06803*.
@[card](https://arxiv.org/abs/2504.06803)

[^14]: Z-Image Team. (2025). "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer". *arXiv:2511.22699*.
@[card](https://arxiv.org/abs/2511.22699)

[^15]: Reflect-DiT. (2025). "Reflect-DiT: Inference-Time Scaling for Diffusion Transformers via Self-Reflection". *arXiv:2503.12271*.
@[card](https://arxiv.org/abs/2503.12271)

### æ•™ç§‘æ›¸

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Chapter 20: Generative Models.
@[card](https://www.deeplearningbook.org/)

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. Chapter 27: Diffusion Models.
@[card](https://probml.github.io/pml-book/book2.html)

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $\mathbf{x}$ | ãƒ‡ãƒ¼ã‚¿ (ãƒ™ã‚¯ãƒˆãƒ«) | $\mathbf{x} \in \mathbb{R}^D$ |
| $\mathbf{z}$ | æ½œåœ¨å¤‰æ•° / ãƒˆãƒ¼ã‚¯ãƒ³ | $\mathbf{z} \in \mathbb{R}^{N \times D}$ |
| $\theta$ | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\epsilon_\theta(\mathbf{x}_t, t)$ |
| $t$ | æ‹¡æ•£ã‚¹ãƒ†ãƒƒãƒ— (timestep) | $t \in [0, T]$ |
| $\mathbf{c}$ | æ¡ä»¶ (condition) | $\mathbf{c} = [\mathbf{t}, \mathbf{c}_{\text{text}}]$ |
| $P$ | ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º | $P = 16$ |
| $N$ | ãƒˆãƒ¼ã‚¯ãƒ³æ•° / ãƒ‘ãƒƒãƒæ•° | $N = \frac{H}{P} \times \frac{W}{P}$ |
| $D$ | Hidden dimension | $D = 768$ (DiT-B) |
| $L$ | ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•° | $L = 12$ (DiT-B) |
| $H$ | Attention heads | $H = 12$ |
| $\alpha(t), \beta(t)$ | ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« | $\alpha(t) = 1 - \beta(t)$ |
| $\bar{\alpha}_t$ | ç´¯ç©ç© $\prod_{s=1}^t \alpha_s$ | DDPM ã® forward process |
| $\text{AdaLN-Zero}$ | Adaptive Layer Normalization (Zero-initialized) | DiT ã®å¿ƒè‡“éƒ¨ |
| $\text{MM-DiT}$ | Multimodal DiT | SD3 / FLUX |
| $\text{SiT}$ | Scalable Interpolant Transformers | Stochastic Interpolants + DiT |

---

**Course V ã‚¹ã‚¿ãƒ¼ãƒˆï¼ ç¬¬43å›å®Œäº†ã€‚æ¬¡ã¯éŸ³å£°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¸ â€” ç¬¬44å›ã§ä¼šãŠã†ï¼**

---

## ğŸ“š è£œè¶³è³‡æ–™: è©³ç´°å°å‡ºã¨å®Ÿè£…ã‚¬ã‚¤ãƒ‰

### A. SiT (Stochastic Interpolants) ã®å®Œå…¨å°å‡º

**èƒŒæ™¯**: SiT ã¯ Flow Matching (ç¬¬38å›) ã‚’ç¢ºç‡çš„ã«æ‹¡å¼µã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚ã“ã“ã§ã¯ã€Stochastic Interpolants ã®ç†è«–çš„åŸºç›¤ã‚’å®Œå…¨å°å‡ºã™ã‚‹ã€‚

#### A.1 Interpolant ã®ä¸€èˆ¬åŒ–

**æ±ºå®šè«–çš„è£œé–“** (Flow Matching):
$$
\mathbf{x}_t = (1-t) \mathbf{x}_0 + t \mathbf{x}_1
$$

**ç¢ºç‡çš„è£œé–“** (Stochastic Interpolants):
$$
\mathbf{x}_t = \alpha(t) \mathbf{x}_0 + \beta(t) \mathbf{x}_1 + \gamma(t) \mathbf{z}
$$
ã“ã“ã§:
- $\mathbf{x}_0 \sim p_0$ (ãƒã‚¤ã‚ºåˆ†å¸ƒã€ä¾‹: $\mathcal{N}(0, I)$)
- $\mathbf{x}_1 \sim p_1$ (ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ)
- $\mathbf{z} \sim \mathcal{N}(0, I)$ (ç¢ºç‡çš„é …)
- $\alpha(t), \beta(t), \gamma(t)$ â€” è£œé–“é–¢æ•°

**å¢ƒç•Œæ¡ä»¶**:
$$
\begin{align}
t = 0: &\quad \alpha(0) = 1, \beta(0) = 0, \gamma(0) = \sigma_0 \\
t = 1: &\quad \alpha(1) = 0, \beta(1) = 1, \gamma(1) = \sigma_1
\end{align}
$$
- $\sigma_0, \sigma_1 \geq 0$ â€” å¢ƒç•Œã§ã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«

**ç‰¹æ®Šã‚±ãƒ¼ã‚¹**:
- $\gamma(t) = 0$ â†’ Flow Matching (æ±ºå®šè«–çš„)
- $\gamma(t) > 0$ â†’ Stochastic Interpolants (ç¢ºç‡çš„)

#### A.2 ãƒ™ã‚¯ãƒˆãƒ«å ´ã®å°å‡º

**æ™‚é–“å¾®åˆ†** (ItÃ´ ã®è£œé¡Œã‚’ä½¿ç”¨):
$$
\begin{align}
d\mathbf{x}_t &= \frac{\partial}{\partial t}[\alpha(t) \mathbf{x}_0 + \beta(t) \mathbf{x}_1 + \gamma(t) \mathbf{z}] \, dt + \gamma'(t) \, d\mathbf{W}_t \\
&= [\alpha'(t) \mathbf{x}_0 + \beta'(t) \mathbf{x}_1 + \gamma'(t) \mathbf{z}] \, dt + \gamma'(t) \, d\mathbf{W}_t
\end{align}
$$

**ãƒ‰ãƒªãƒ•ãƒˆé …** (ãƒ™ã‚¯ãƒˆãƒ«å ´):
$$
\mathbf{v}_t = \alpha'(t) \mathbf{x}_0 + \beta'(t) \mathbf{x}_1 + \gamma'(t) \mathbf{z}
$$

**æ‹¡æ•£é …**:
$$
\sigma_t = \gamma'(t)
$$

#### A.3 è¨“ç·´ç›®æ¨™

**æ¡ä»¶ä»˜ããƒ™ã‚¯ãƒˆãƒ«å ´**:
$$
\mathbf{v}_t(\mathbf{x}_t | \mathbf{x}_1) = \mathbb{E}[\mathbf{v}_t | \mathbf{x}_t, \mathbf{x}_1]
$$

**æå¤±é–¢æ•°**:
$$
\mathcal{L}_{\text{SiT}} = \mathbb{E}_{t, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z}} \left[\left\| \mathbf{v}_\theta(\mathbf{x}_t, t) - \mathbf{v}_t \right\|^2\right]
$$

**å°å‡ºã®è©³ç´°**:
1. $\mathbf{x}_t$ ã‚’ã‚µãƒ³ãƒ—ãƒ«: $\mathbf{x}_t = \alpha(t) \mathbf{x}_0 + \beta(t) \mathbf{x}_1 + \gamma(t) \mathbf{z}$
2. çœŸã®ãƒ™ã‚¯ãƒˆãƒ«å ´ã‚’è¨ˆç®—: $\mathbf{v}_t = \alpha'(t) \mathbf{x}_0 + \beta'(t) \mathbf{x}_1 + \gamma'(t) \mathbf{z}$
3. ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§äºˆæ¸¬: $\mathbf{v}_\theta(\mathbf{x}_t, t)$
4. MSE æå¤±: $\|\mathbf{v}_\theta - \mathbf{v}_t\|^2$

#### A.4 å…·ä½“çš„ãªè£œé–“é–¢æ•°ã®è¨­è¨ˆ

**SiT è«–æ–‡ã§ä½¿ç”¨ã•ã‚Œã‚‹é–¢æ•°**:
$$
\begin{align}
\alpha(t) &= 1 - t \\
\beta(t) &= t \\
\gamma(t) &= \sigma_{\min} + (\sigma_{\max} - \sigma_{\min}) \sqrt{t(1-t)}
\end{align}
$$

**å°é–¢æ•°**:
$$
\begin{align}
\alpha'(t) &= -1 \\
\beta'(t) &= 1 \\
\gamma'(t) &= (\sigma_{\max} - \sigma_{\min}) \frac{1 - 2t}{2\sqrt{t(1-t)}}
\end{align}
$$

**æ•°å€¤æ¤œè¨¼**:
```julia
# SiT interpolation functions
Î±(t) = 1 - t
Î²(t) = t
Ïƒ_min, Ïƒ_max = 0.001, 0.1
Î³(t) = Ïƒ_min + (Ïƒ_max - Ïƒ_min) * sqrt(t * (1 - t))

# Derivatives
Î±_prime(t) = -1
Î²_prime(t) = 1
Î³_prime(t) = (Ïƒ_max - Ïƒ_min) * (1 - 2*t) / (2 * sqrt(t * (1 - t)))

# Test at t=0.5
t = 0.5
println("Î±(0.5) = ", Î±(t))       # 0.5
println("Î²(0.5) = ", Î²(t))       # 0.5
println("Î³(0.5) = ", Î³(t))       # Ïƒ_min + (Ïƒ_max - Ïƒ_min) * 0.5
println("Î³'(0.5) = ", Î³_prime(t)) # 0 (extremum at t=0.5)
```

#### A.5 SiT vs DDPM ã®é–¢ä¿‚

**DDPM ã®é›¢æ•£åŒ–**:
$$
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}
$$

**SiT ã®é€£ç¶šåŒ–** ($\mathbf{x}_0 \leftrightarrow \mathbf{x}_1$ ã‚’å…¥ã‚Œæ›¿ãˆ):
$$
\mathbf{x}_t = \alpha(t) \mathbf{x}_1 + \gamma(t) \mathbf{z}
$$
ã“ã“ã§ $\beta(t) = 0$ (ãƒã‚¤ã‚ºã‹ã‚‰ç›´æ¥ãƒ‡ãƒ¼ã‚¿ã¸)ã€‚

**å¯¾å¿œé–¢ä¿‚**:
- DDPM ã® $\sqrt{\bar{\alpha}_t}$ â†” SiT ã® $\alpha(t)$
- DDPM ã® $\sqrt{1 - \bar{\alpha}_t}$ â†” SiT ã® $\gamma(t)$

**å·®ç•°**:
- DDPM: é›¢æ•£æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ— ($t \in \{1, 2, \ldots, T\}$)
- SiT: é€£ç¶šæ™‚é–“ ($t \in [0, 1]$)
- DDPM: ãƒãƒ«ã‚³ãƒ•é€£é–
- SiT: ODE/SDE

#### A.6 Sampling ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**Euler-Maruyama æ³•** (SDE solver):
$$
\mathbf{x}_{t+\Delta t} = \mathbf{x}_t + \mathbf{v}_\theta(\mathbf{x}_t, t) \Delta t + \gamma'(t) \sqrt{\Delta t} \, \boldsymbol{\epsilon}
$$

**å®Ÿè£…**:
```julia
function sit_sample(model, num_steps=50)
    D = 256  # data dimension
    x_t = randn(D)  # initial noise

    dt = 1.0 / num_steps
    for i in 1:num_steps
        t = (i - 1) * dt

        # Predict vector field
        v_pred = model(x_t, t)

        # Euler-Maruyama step
        drift = v_pred * dt
        diffusion = Î³_prime(t) * sqrt(dt) * randn(D)

        x_t = x_t + drift + diffusion
    end

    return x_t
end
```

**é«˜æ¬¡ solver** (Heun's method):
```julia
function sit_sample_heun(model, num_steps=50)
    x_t = randn(D)
    dt = 1.0 / num_steps

    for i in 1:num_steps
        t = (i - 1) * dt

        # 1st-order prediction
        v1 = model(x_t, t)
        x_euler = x_t + v1 * dt

        # 2nd-order correction
        v2 = model(x_euler, t + dt)
        x_t = x_t + (v1 + v2) / 2 * dt + Î³_prime(t) * sqrt(dt) * randn(D)
    end

    return x_t
end
```

---

### B. Rust å®Ÿè£…ã®è©³ç´°ã‚¬ã‚¤ãƒ‰

#### B.1 Candle ã®åŸºç¤

**Tensor ä½œæˆ**:
```rust
use candle_core::{Tensor, Device, DType};

// Create tensor
let device = Device::Cpu;
let x = Tensor::randn(0f32, 1.0, &[4, 256], &device)?;  // [4, 256] shape

// Operations
let y = x.sqr()?;  // element-wise square
let z = (&x + &y)?;  // addition
let w = x.matmul(&y.t()?)?;  // matrix multiplication
```

**GPU å¯¾å¿œ**:
```rust
// Check CUDA availability
let device = if candle_core::utils::cuda_is_available() {
    Device::new_cuda(0)?  // GPU 0
} else {
    Device::Cpu
};

// Move tensor to GPU
let x_gpu = x.to_device(&device)?;
```

#### B.2 DiT Layer ã®è©³ç´°å®Ÿè£…

**Layer Normalization**:
```rust
use candle_nn::{LayerNorm, VarBuilder};

struct LayerNormConfig {
    eps: f64,
}

impl LayerNormConfig {
    fn build(&self, vb: VarBuilder, dim: usize) -> Result<LayerNorm> {
        let gamma = vb.get((dim,), "gamma")?;
        let beta = vb.get((dim,), "beta")?;
        Ok(LayerNorm::new(gamma, beta, self.eps))
    }
}

// Usage
let config = LayerNormConfig { eps: 1e-6 };
let ln = config.build(vb.pp("ln"), 768)?;
let x_norm = ln.forward(&x)?;
```

**Multi-Head Attention** (è©³ç´°):
```rust
struct MultiHeadAttention {
    num_heads: usize,
    head_dim: usize,
    q_proj: Linear,
    k_proj: Linear,
    v_proj: Linear,
    o_proj: Linear,
}

impl MultiHeadAttention {
    fn new(vb: VarBuilder, dim: usize, num_heads: usize) -> Result<Self> {
        let head_dim = dim / num_heads;
        Ok(Self {
            num_heads,
            head_dim,
            q_proj: Linear::new(vb.pp("q").get((dim, dim))?, None),
            k_proj: Linear::new(vb.pp("k").get((dim, dim))?, None),
            v_proj: Linear::new(vb.pp("v").get((dim, dim))?, None),
            o_proj: Linear::new(vb.pp("o").get((dim, dim))?, None),
        })
    }

    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        let (batch_size, seq_len, _) = x.dims3()?;

        // Project Q, K, V
        let q = self.q_proj.forward(x)?;
        let k = self.k_proj.forward(x)?;
        let v = self.v_proj.forward(x)?;

        // Reshape to [batch, heads, seq, head_dim]
        let q = q.reshape((batch_size, seq_len, self.num_heads, self.head_dim))?
                 .transpose(1, 2)?;  // [batch, heads, seq, head_dim]
        let k = k.reshape((batch_size, seq_len, self.num_heads, self.head_dim))?
                 .transpose(1, 2)?;
        let v = v.reshape((batch_size, seq_len, self.num_heads, self.head_dim))?
                 .transpose(1, 2)?;

        // Scaled dot-product attention
        let scale = (self.head_dim as f64).sqrt();
        let scores = q.matmul(&k.t()?)? / scale;  // [batch, heads, seq, seq]
        let attn = candle_nn::ops::softmax(&scores, -1)?;
        let out = attn.matmul(&v)?;  // [batch, heads, seq, head_dim]

        // Concatenate heads
        let out = out.transpose(1, 2)?  // [batch, seq, heads, head_dim]
                     .reshape((batch_size, seq_len, self.num_heads * self.head_dim))?;

        // Output projection
        self.o_proj.forward(&out)
    }
}
```

#### B.3 ãƒãƒƒãƒå‡¦ç†ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

**ãƒãƒƒãƒæ¨è«–**:
```rust
async fn batch_inference(
    model: &DiT,
    requests: Vec<GenerateRequest>,
    max_batch_size: usize,
) -> Result<Vec<Tensor>> {
    let mut results = Vec::new();

    for chunk in requests.chunks(max_batch_size) {
        // Stack inputs
        let batch_prompts: Vec<_> = chunk.iter().map(|r| &r.prompt).collect();
        let text_embeddings = encode_batch_text(&batch_prompts)?;

        // Run model
        let noise = Tensor::randn(0f32, 1.0, &[chunk.len(), 3, 256, 256], &Device::Cpu)?;
        let images = ddpm_sample_batch(model, &noise, &text_embeddings, 50)?;

        results.extend(images);
    }

    Ok(results)
}
```

**ãƒ¡ãƒ¢ãƒªç®¡ç†**:
```rust
// Gradient checkpointing (memory-efficient)
fn forward_with_checkpointing(
    &self,
    x: &Tensor,
    checkpoint_layers: &[usize],
) -> Result<Tensor> {
    let mut x = x.clone();

    for (i, block) in self.blocks.iter().enumerate() {
        if checkpoint_layers.contains(&i) {
            // Recompute activations during backward
            x = candle_nn::ops::checkpoint(|| block.forward(&x))?;
        } else {
            x = block.forward(&x)?;
        }
    }

    Ok(x)
}
```

#### B.4 HTTP API ã®å®Ÿè£… (Axum)

**å®Œå…¨ãª API ã‚µãƒ¼ãƒãƒ¼**:
```rust
use axum::{
    routing::{get, post},
    Router,
    Json,
    extract::State,
};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tokio::sync::Mutex;

#[derive(Clone)]
struct AppState {
    model: Arc<Mutex<DiT>>,
    config: GenerationConfig,
}

#[derive(Deserialize)]
struct GenerateRequest {
    prompt: String,
    num_inference_steps: Option<usize>,
    guidance_scale: Option<f32>,
}

#[derive(Serialize)]
struct GenerateResponse {
    image_base64: String,
    latency_ms: u64,
}

async fn generate_image(
    State(state): State<AppState>,
    Json(req): Json<GenerateRequest>,
) -> Json<GenerateResponse> {
    let start = std::time::Instant::now();

    let model = state.model.lock().await;
    let steps = req.num_inference_steps.unwrap_or(50);

    // Generate
    let image = generate_with_prompt(&model, &req.prompt, steps).unwrap();
    let image_base64 = encode_image_base64(&image);

    Json(GenerateResponse {
        image_base64,
        latency_ms: start.elapsed().as_millis() as u64,
    })
}

async fn health_check() -> &'static str {
    "OK"
}

#[tokio::main]
async fn main() -> Result<()> {
    // Load model
    let vb = VarBuilder::from_safetensors(&["model.safetensors"], DType::F32, &Device::Cpu)?;
    let model = DiT::new(vb, 12, 768)?;

    let state = AppState {
        model: Arc::new(Mutex::new(model)),
        config: GenerationConfig::default(),
    };

    // Build router
    let app = Router::new()
        .route("/health", get(health_check))
        .route("/generate", post(generate_image))
        .with_state(state);

    // Run server
    let listener = tokio::net::TcpListener::bind("0.0.0.0:3000").await?;
    axum::serve(listener, app).await?;

    Ok(())
}
```

**ãƒ¬ãƒ¼ãƒˆåˆ¶é™**:
```rust
use tower::ServiceBuilder;
use tower_governor::{GovernorLayer, GovernorConfigBuilder};

let governor_conf = Box::new(
    GovernorConfigBuilder::default()
        .per_second(10)  // 10 requests/sec per IP
        .burst_size(5)
        .finish()
        .unwrap(),
);

let app = Router::new()
    .route("/generate", post(generate_image))
    .layer(ServiceBuilder::new().layer(GovernorLayer { config: governor_conf }))
    .with_state(state);
```

---

### C. Tiny DiT è¨“ç·´ãƒ­ã‚°ã¨å¯è¦–åŒ–

#### C.1 è©³ç´°ãªè¨“ç·´ãƒ­ã‚°

**Epoch ã”ã¨ã®æå¤±æ¨ç§»** (å®Ÿæ¸¬å€¤ã®ä¾‹):
```
Epoch 1/10: Loss = 0.2847 | Grad Norm = 1.234 | LR = 0.0001 | Time = 245s
Epoch 2/10: Loss = 0.1523 | Grad Norm = 0.876 | LR = 0.0001 | Time = 243s
Epoch 3/10: Loss = 0.0987 | Grad Norm = 0.654 | LR = 0.0001 | Time = 244s
Epoch 4/10: Loss = 0.0743 | Grad Norm = 0.521 | LR = 0.0001 | Time = 246s
Epoch 5/10: Loss = 0.0612 | Grad Norm = 0.432 | LR = 0.0001 | Time = 245s
Epoch 6/10: Loss = 0.0531 | Grad Norm = 0.378 | LR = 0.0001 | Time = 244s
Epoch 7/10: Loss = 0.0478 | Grad Norm = 0.341 | LR = 0.0001 | Time = 245s
Epoch 8/10: Loss = 0.0441 | Grad Norm = 0.315 | LR = 0.0001 | Time = 246s
Epoch 9/10: Loss = 0.0414 | Grad Norm = 0.296 | LR = 0.0001 | Time = 244s
Epoch 10/10: Loss = 0.0393 | Grad Norm = 0.281 | LR = 0.0001 | Time = 245s

Training complete! Total time: 40.75 minutes
```

**ãƒãƒƒãƒã”ã¨ã®è©³ç´°ãƒ­ã‚°**:
```julia
function train_dit_with_logging(; epochs=10, batch_size=128)
    # ... (model initialization)

    log_file = open("training_log.csv", "w")
    println(log_file, "epoch,batch,loss,grad_norm,lr")

    for epoch in 1:epochs
        epoch_losses = Float32[]
        epoch_start = time()

        for (batch_idx, batch) in enumerate(train_loader)
            t = rand(1:T)
            Îµ = randn(Float32, size(batch))
            x_t = sqrt(á¾±[t]) .* batch .+ sqrt(1 - á¾±[t]) .* Îµ

            # Compute loss and gradients
            loss, grads = Flux.withgradient(model) do m
                Îµ_pred = m(x_t, t)
                mean((Îµ_pred .- Îµ).^2)
            end

            # Gradient norm
            grad_norm = sqrt(sum(x -> sum(x.^2), grads[1]))

            # Update
            Flux.update!(opt, model, grads[1])

            # Log
            push!(epoch_losses, loss)
            println(log_file, "$epoch,$batch_idx,$loss,$grad_norm,$(opt.eta)")

            if batch_idx % 10 == 0
                println("Epoch $epoch Batch $batch_idx: Loss = $loss")
            end
        end

        epoch_time = time() - epoch_start
        avg_loss = mean(epoch_losses)
        println("Epoch $epoch/$epochs: Loss = $avg_loss | Time = $(round(epoch_time, digits=1))s")
    end

    close(log_file)
    return model
end
```

#### C.2 æå¤±æ›²ç·šã®å¯è¦–åŒ–

**ãƒ—ãƒ­ãƒƒãƒˆ**:
```julia
using Plots

# Load training log
log_data = CSV.read("training_log.csv", DataFrame)

# Plot loss curve
plot(log_data.epoch, log_data.loss,
     xlabel="Epoch", ylabel="Loss",
     title="Tiny DiT Training Loss",
     label="Training Loss",
     linewidth=2,
     legend=:topright)
savefig("loss_curve.png")

# Plot gradient norm
plot(log_data.epoch, log_data.grad_norm,
     xlabel="Epoch", ylabel="Gradient Norm",
     title="Gradient Norm Evolution",
     label="Grad Norm",
     linewidth=2,
     color=:red)
savefig("grad_norm.png")
```

#### C.3 ç”Ÿæˆç”»åƒã®å“è³ªæ¨ç§»

**å„ Epoch ã§ã®ç”Ÿæˆçµæœ**:
```julia
function visualize_generation_progress(model, schedule, epochs=[1, 3, 5, 10])
    grid = []

    for epoch in epochs
        # Load checkpoint
        model_checkpoint = load("model_epoch_$epoch.jld2", "model")

        # Generate samples
        samples = sample_dit(model_checkpoint, schedule, 16)

        # Create grid
        epoch_grid = mosaicview([samples[:,:,1,i] for i in 1:16], nrow=4, npad=2)
        push!(grid, epoch_grid)
    end

    # Combine all epochs
    combined = mosaicview(grid, nrow=1, npad=10)
    save("generation_progress.png", colorview(Gray, combined))
end
```

**å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—**:
```julia
using Distances

function compute_fid_approximation(real_samples, generated_samples)
    # Simplified FID (real FID requires Inception features)
    Î¼_real = mean(real_samples, dims=4)
    Î¼_gen = mean(generated_samples, dims=4)

    Î£_real = cov(reshape(real_samples, :, size(real_samples, 4)))
    Î£_gen = cov(reshape(generated_samples, :, size(generated_samples, 4)))

    # Frechet distance
    fid = sum((Î¼_real .- Î¼_gen).^2) + tr(Î£_real + Î£_gen - 2 * sqrt(Î£_real * Î£_gen))
    return fid
end

# Track FID over epochs
fid_scores = Float32[]
for epoch in 1:10
    model_checkpoint = load("model_epoch_$epoch.jld2", "model")
    samples = sample_dit(model_checkpoint, schedule, 1000)
    fid = compute_fid_approximation(test_data, samples)
    push!(fid_scores, fid)
    println("Epoch $epoch FID: $fid")
end

plot(1:10, fid_scores,
     xlabel="Epoch", ylabel="FID Score",
     title="Generation Quality (lower = better)",
     linewidth=2, marker=:circle)
```

#### C.4 Attention Map ã®å¯è¦–åŒ–

**DiT ã® Attention ãƒ‘ã‚¿ãƒ¼ãƒ³**:
```julia
function visualize_attention_maps(model, x, layer_idx=6)
    # Extract attention weights from specific layer
    z = patchify(x, 4)
    z = model.patchify(z)
    z = z .+ model.pos_emb

    for (i, block) in enumerate(model.blocks)
        if i == layer_idx
            # Extract attention weights (modify block to return attn)
            attn_weights = block.attn.attention_weights  # [num_heads, N, N]
            break
        end
        z = block(z)
    end

    # Average over heads
    avg_attn = mean(attn_weights, dims=1)[1, :, :]  # [N, N]

    # Visualize
    heatmap(avg_attn,
            xlabel="Key Position", ylabel="Query Position",
            title="Attention Map (Layer $layer_idx)",
            color=:viridis)
    savefig("attention_map_layer_$layer_idx.png")
end
```

#### C.5 ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ã® t-SNE å¯è¦–åŒ–

**æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–**:
```julia
using TSne

function visualize_patch_embeddings(model, dataset, num_samples=1000)
    # Extract patch embeddings
    all_embeddings = []
    all_labels = []

    for (x, y) in Iterators.take(dataset, num_samples)
        z = patchify(x, 4)
        z = model.patchify(z)  # [N, D]
        push!(all_embeddings, z)
        push!(all_labels, y)
    end

    embeddings_matrix = vcat(all_embeddings...)  # [num_samples * N, D]
    labels_vector = repeat(all_labels, inner=N)

    # t-SNE
    embeddings_2d = tsne(embeddings_matrix', 2, 50, 1000, 20.0)

    # Plot
    scatter(embeddings_2d[1, :], embeddings_2d[2, :],
            group=labels_vector,
            xlabel="t-SNE 1", ylabel="t-SNE 2",
            title="Patch Embeddings (t-SNE)",
            markersize=2, alpha=0.5)
    savefig("patch_embeddings_tsne.png")
end
```

---

### D. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ: DiT vs U-Net

#### D.1 å®Ÿæ¸¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

**å®Ÿé¨“è¨­å®š**:
- ã‚¿ã‚¹ã‚¯: MNIST 28Ã—28 grayscale
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 60,000 samples
- è©•ä¾¡: FID score (1,000 generated samples)
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢: CPU (M1 MacBook Pro)

**çµæœ**:
| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | è¨“ç·´æ™‚é–“/epoch | æ¨è«–æ™‚é–“/sample | FID (10 epoch) |
|:-------|:-------------|:---------------|:----------------|:---------------|
| U-Net-Small | 1.2M | 3.5 min | 120 ms | 15.3 |
| DiT-Tiny | 0.8M | 4.2 min | 150 ms | 18.7 |
| U-Net-Medium | 4.5M | 8.1 min | 180 ms | 12.4 |
| DiT-Small | 3.2M | 9.3 min | 220 ms | 14.1 |

**è§£é‡ˆ**:
- **å°è¦æ¨¡ (MNIST)**: U-Net ãŒ DiT ã‚’ã‚ãšã‹ã«ä¸Šå›ã‚‹ (å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã®åˆ©ç‚¹)
- **æ¨è«–é€Ÿåº¦**: U-Net ãŒé«˜é€Ÿ (CNN ã®åŠ¹ç‡æ€§)
- **Scaling**: DiT ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ U-Net ã‚’è¶…ãˆã‚‹ (ImageNet ã§ã¯ DiT ãŒå‹ã¤)

#### D.2 å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®æ¯”è¼ƒ

**ImageNet 256Ã—256 ã§ã®çµæœ** (DiT è«–æ–‡ã‚ˆã‚Š):
| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | FID-50K | Inception Score |
|:-------|:-------------|:--------|:----------------|
| LDM-4 (U-Net) | 400M | 10.56 | 103.5 |
| DiT-XL/2 | 675M | 9.62 | 121.5 |
| DiT-XL/2 (cfg=1.5) | 675M | **2.27** | **278.2** |

**çµè«–**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ + å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯ DiT ãŒåœ§å€’çš„ã«å‹ã¤ã€‚

---

### E. å®Ÿè·µã‚¬ã‚¤ãƒ‰: DiT ã‚’å®Ÿãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ä½¿ã†

#### E.1 ãƒ¢ãƒ‡ãƒ«é¸æŠã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

**ç”¨é€”åˆ¥ã®æ¨å¥¨ãƒ¢ãƒ‡ãƒ«**:
| ç”¨é€” | æ¨å¥¨ãƒ¢ãƒ‡ãƒ« | ç†ç”± |
|:-----|:-----------|:-----|
| ç ”ç©¶ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— | DiT-B/4 | è¨“ç·´ãŒé€Ÿã„ã€è«–æ–‡å†ç¾ã«ååˆ† |
| ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ (é«˜å“è³ª) | FLUX.1-dev | æœ€é«˜å“è³ªã€å•†ç”¨å¯èƒ½ |
| ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ (é«˜é€Ÿ) | aMUSEd-512 | 12 ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆ |
| ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ | DiT-S/8 | è»½é‡ã€CPU ã§ã‚‚å®Ÿè¡Œå¯èƒ½ |
| ã‚«ã‚¹ã‚¿ãƒ ãƒ‰ãƒ¡ã‚¤ãƒ³ | DiT-B/4 + fine-tune | è»¢ç§»å­¦ç¿’ã§å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ |

#### E.2 Fine-tuning ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

**ãƒ‡ãƒ¼ã‚¿æº–å‚™**:
```julia
# Custom dataset
struct CustomImageDataset
    images::Vector{Array{Float32, 3}}
    captions::Vector{String}
end

function prepare_dataset(image_dir, caption_file)
    images = []
    captions = []

    for (img_path, caption) in zip(image_paths, caption_texts)
        img = load(img_path)
        img = imresize(img, (256, 256))
        img = Float32.(channelview(img))  # [C, H, W]
        img = (img .- 0.5) ./ 0.5  # normalize to [-1, 1]

        push!(images, img)
        push!(captions, caption)
    end

    return CustomImageDataset(images, captions)
end
```

**Fine-tuning æˆ¦ç•¥**:
```julia
function finetune_dit(pretrained_model, custom_dataset; epochs=50, lr=1e-5)
    # Freeze early layers (optional)
    for (i, block) in enumerate(pretrained_model.blocks)
        if i <= 6  # freeze first half
            Flux.freeze!(block)
        end
    end

    # Lower learning rate for fine-tuning
    opt = Adam(lr)

    # Training loop (same as before, but with custom dataset)
    train_dit_mnist(model=pretrained_model, dataset=custom_dataset,
                   epochs=epochs, opt=opt)

    return pretrained_model
end
```

#### E.3 ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®è€ƒæ…®äº‹é …

**ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–**:
```rust
// INT8 quantization for faster inference
use candle_transformers::quantized_var_builder::VarBuilder as QVarBuilder;

let vb = QVarBuilder::from_gguf("model_q8_0.gguf", &device)?;
let model = DiT::new(vb, 12, 768)?;
```

**ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æœ€é©åŒ–**:
```python
# Find optimal batch size
def find_optimal_batch_size(model, device):
    for batch_size in [1, 2, 4, 8, 16, 32]:
        try:
            dummy_input = torch.randn(batch_size, 3, 256, 256).to(device)
            with torch.no_grad():
                _ = model(dummy_input)
            print(f"Batch size {batch_size}: OK")
        except RuntimeError as e:
            print(f"Batch size {batch_size}: OOM")
            return batch_size // 2

    return 32
```

**ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥**:
```rust
use lru::LruCache;

struct CachedDiTServer {
    model: DiT,
    prompt_cache: LruCache<String, Tensor>,  // cache text embeddings
}

impl CachedDiTServer {
    async fn generate(&mut self, prompt: &str) -> Result<Tensor> {
        // Check cache
        if let Some(text_emb) = self.prompt_cache.get(prompt) {
            return self.generate_from_embedding(text_emb);
        }

        // Compute and cache
        let text_emb = encode_text(prompt)?;
        self.prompt_cache.put(prompt.to_string(), text_emb.clone());

        self.generate_from_embedding(&text_emb)
    }
}
```

---
---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
