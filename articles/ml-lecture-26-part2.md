---
title: "ç¬¬26å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
slug: "ml-lecture-26-part2"
emoji: "âš¡"
type: "tech"
topics: ["machinelearning", "optimization", "rust", "elixir", "production"]
published: true
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” 3è¨€èªçµ±åˆå®Ÿè£…

**ã‚´ãƒ¼ãƒ«**: Part A-Eã®ç†è«–ã‚’å®Ÿéš›ã«å‹•ãã‚³ãƒ¼ãƒ‰ã§å®Ÿè£…ã™ã‚‹ã€‚

### 4.1 ğŸ¦€ Rust: å®Œå…¨ãªINT4é‡å­åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

Productionå“è³ªã®INT4é‡å­åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å®Ÿè£…ã€‚ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ãƒ†ã‚¹ãƒˆå®Œå‚™ã€‚

```rust
// src/lib.rs
#![deny(clippy::unwrap_used)]
#![warn(clippy::pedantic, missing_docs)]

//! INT4/FP8 quantization library for LLM inference.
//!
//! # Examples
//!
//! ```
//! use quantizer::{Quantizer, QuantizerConfig, BitWidth};
//!
//! let weights = vec![0.5, -0.3, 0.8, -0.1];
//! let config = QuantizerConfig::new(BitWidth::Int4);
//! let quantizer = Quantizer::new(config)?;
//!
//! let (quantized, scale) = quantizer.quantize(&weights)?;
//! let dequantized = quantizer.dequantize(&quantized, scale)?;
//! # Ok::<(), quantizer::Error>(())
//! ```

use thiserror::Error;
use tracing::{info, warn, instrument};
use prometheus::{Counter, Histogram};

#[derive(Error, Debug)]
pub enum Error {
    #[error("Empty weight tensor")]
    EmptyTensor,

    #[error("Invalid bit width: {0}, must be 2, 4, or 8")]
    InvalidBitWidth(u8),

    #[error("Quantization overflow: max value {0} exceeds range")]
    Overflow(f32),
}

pub type Result<T> = std::result::Result<T, Error>;

#[derive(Debug, Clone, Copy)]
pub enum BitWidth {
    Int2,
    Int4,
    Int8,
}

impl BitWidth {
    fn max_value(self) -> i8 {
        match self {
            Self::Int2 => 1,
            Self::Int4 => 7,
            Self::Int8 => 127,
        }
    }

    fn bits(self) -> u8 {
        match self {
            Self::Int2 => 2,
            Self::Int4 => 4,
            Self::Int8 => 8,
        }
    }
}

pub struct QuantizerConfig {
    bit_width: BitWidth,
    symmetric: bool,
}

impl QuantizerConfig {
    pub fn new(bit_width: BitWidth) -> Self {
        Self {
            bit_width,
            symmetric: true,
        }
    }

    pub fn asymmetric(mut self) -> Self {
        self.symmetric = false;
        self
    }
}

pub struct Quantizer {
    config: QuantizerConfig,
}

impl Quantizer {
    #[instrument]
    pub fn new(config: QuantizerConfig) -> Result<Self> {
        info!(bits = config.bit_width.bits(), "Initializing quantizer");
        Ok(Self { config })
    }

    #[instrument(skip(weights))]
    pub fn quantize(&self, weights: &[f32]) -> Result<(Vec<i8>, f32)> {
        if weights.is_empty() {
            return Err(Error::EmptyTensor);
        }

        let max_val = weights.iter()
            .map(|w| w.abs())
            .fold(0.0f32, f32::max);

        let scale = max_val / f32::from(self.config.bit_width.max_value());

        if scale == 0.0 {
            warn!("All weights are zero, scale = 0");
        }

        let quantized = weights.iter()
            .map(|w| {
                let q = (w / scale).round();
                let max = f32::from(self.config.bit_width.max_value());
                q.clamp(-max, max) as i8
            })
            .collect::<Vec<_>>();

        info!(
            num_params = weights.len(),
            scale = %scale,
            "Quantization complete"
        );

        Ok((quantized, scale))
    }

    pub fn dequantize(&self, quantized: &[i8], scale: f32) -> Result<Vec<f32>> {
        Ok(quantized.iter()
            .map(|&q| f32::from(q) * scale)
            .collect::<Vec<_>>())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_quantize_int4() {
        let weights = vec![0.5, -0.3, 0.8, -0.1, 0.0];
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();

        // Check range
        assert!(quantized.iter().all(|&q| q >= -7 && q <= 7));

        // Check scale computation
        let expected_scale = 0.8 / 7.0;
        assert!((scale - expected_scale).abs() < 1e-6);
    }

    #[test]
    fn test_quantize_dequantize_roundtrip() {
        let weights = vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let config = QuantizerConfig::new(BitWidth::Int8);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();
        let dequantized = quantizer.dequantize(&quantized, scale).unwrap();

        // Check error bound: |w - Åµ| <= scale/2
        assert!(weights.iter().zip(&dequantized).all(|(orig, deq)| (orig - deq).abs() <= scale / 2.0 + 1e-6));
    }

    #[test]
    fn test_empty_tensor() {
        let weights: Vec<f32> = vec![];
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config).unwrap();

        let result = quantizer.quantize(&weights);
        assert!(matches!(result, Err(Error::EmptyTensor)));
    }
}
```

**Property-based test**:

```rust
// tests/proptest.rs
use proptest::prelude::*;
use quantizer::*;

proptest! {
    #[test]
    fn prop_quantization_bounded(
        weights in prop::collection::vec((-100.0f32..100.0f32), 1..1000)
    ) {
        let config = QuantizerConfig::new(BitWidth::Int8);
        let quantizer = Quantizer::new(config)?;

        let (quantized, scale) = quantizer.quantize(&weights)?;
        let dequantized = quantizer.dequantize(&quantized, scale)?;

        prop_assert!(weights.iter().zip(&dequantized).all(|(orig, deq)| (orig - deq).abs() <= scale / 2.0 + 1e-5));
    }

    #[test]
    fn prop_quantization_range(
        weights in prop::collection::vec((-10.0f32..10.0f32), 1..1000)
    ) {
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config)?;

        let (quantized, _scale) = quantizer.quantize(&weights)?;

        prop_assert!(quantized.iter().all(|&q| q >= -7 && q <= 7));
    }
}
```

### 4.2 ğŸ”® Elixir: Circuit Breaker + ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±åˆ

```elixir
# lib/inference_api/circuit_breaker.ex
defmodule InferenceAPI.CircuitBreaker do
  @moduledoc """
  Circuit breaker for external inference service.

  States: :closed (healthy) -> :open (failing) -> :half_open (testing)

  ## Examples

      {:ok, cb} = CircuitBreaker.start_link(name: :model_service)
      CircuitBreaker.call(cb, fn -> ModelService.infer(input) end)
  """

  use GenServer
  require Logger

  @failure_threshold 5
  @timeout_ms 30_000
  @half_open_success_threshold 3

  defmodule State do
    @moduledoc false
    defstruct [
      :status,
      :failure_count,
      :success_count,
      :last_failure_time,
      :metrics
    ]
  end

  def start_link(opts) do
    name = Keyword.get(opts, :name, __MODULE__)
    GenServer.start_link(__MODULE__, opts, name: name)
  end

  def call(breaker, fun, timeout \\ 5000) do
    GenServer.call(breaker, {:call, fun}, timeout)
  end

  @impl true
  def init(_opts) do
    # Initialize Prometheus metrics
    :prometheus_counter.declare([
      name: :circuit_breaker_state_changes_total,
      help: "Total circuit breaker state changes"
    ])

    :prometheus_gauge.declare([
      name: :circuit_breaker_failure_count,
      help: "Current failure count"
    ])

    {:ok, %State{
      status: :closed,
      failure_count: 0,
      success_count: 0,
      last_failure_time: nil,
      metrics: %{}
    }}
  end

  @impl true
  def handle_call({:call, fun}, _from, state) do
    case state.status do
      :open ->
        if time_elapsed?(state.last_failure_time, @timeout_ms) do
          Logger.info("Circuit breaker transitioning to half-open")
          record_state_change(:half_open)
          attempt_call(fun, %{state | status: :half_open, success_count: 0})
        else
          {:reply, {:error, :circuit_open}, state}
        end

      :half_open ->
        attempt_call(fun, state)

      :closed ->
        attempt_call(fun, state)
    end
  end

  defp attempt_call(fun, state) do
    start_time = System.monotonic_time(:millisecond)
    result = fun.()
    (System.monotonic_time(:millisecond) - start_time) |> record_latency()

    case result do
      {:ok, result} ->
        {:reply, {:ok, result}, handle_success(state)}

      {:error, reason} ->
        record_error()
        {:reply, {:error, reason}, handle_failure(state)}
    end
  end

  defp handle_success(state) do
    case state.status do
      :half_open ->
        new_success_count = state.success_count + 1

        if new_success_count >= @half_open_success_threshold do
          Logger.info("Circuit breaker closed after #{new_success_count} successes")
          record_state_change(:closed)
          %{state | status: :closed, failure_count: 0, success_count: 0}
        else
          %{state | success_count: new_success_count}
        end

      :closed ->
        %{state | failure_count: 0}

      :open ->
        state
    end
  end

  defp handle_failure(state) do
    new_failure_count = state.failure_count + 1
    :prometheus_gauge.set(:circuit_breaker_failure_count, new_failure_count)

    if new_failure_count >= @failure_threshold do
      Logger.error("Circuit breaker opened after #{new_failure_count} failures")
      record_state_change(:open)

      %{state |
        status: :open,
        failure_count: new_failure_count,
        last_failure_time: System.monotonic_time(:millisecond)
      }
    else
      %{state | failure_count: new_failure_count}
    end
  end

  defp time_elapsed?(last_time, timeout_ms) when is_nil(last_time), do: false
  defp time_elapsed?(last_time, timeout_ms) do
    System.monotonic_time(:millisecond) - last_time > timeout_ms
  end

  defp record_state_change(new_state) do
    :prometheus_counter.inc(:circuit_breaker_state_changes_total, [state: new_state])
  end

  defp record_latency(latency_ms) do
    :prometheus_histogram.observe(:inference_duration_seconds, latency_ms / 1000.0)
  end

  defp record_error do
    :prometheus_counter.inc(:inference_errors_total)
  end
end
```

**çµ±åˆãƒ†ã‚¹ãƒˆ**:

```elixir
# test/circuit_breaker_test.exs
defmodule InferenceAPI.CircuitBreakerTest do
  use ExUnit.Case, async: true

  alias InferenceAPI.CircuitBreaker

  setup do
    {:ok, cb} = CircuitBreaker.start_link([])
    %{cb: cb}
  end

  test "transitions to open after threshold failures", %{cb: cb} do
    # Trigger 5 failures
    1..5 |> Enum.each(fn _ ->
      assert {:error, :service_down} = CircuitBreaker.call(cb, fn -> {:error, :service_down} end)
    end)

    # Circuit should be open now
    assert {:error, :circuit_open} = CircuitBreaker.call(cb, fn ->
      {:ok, :result}
    end)
  end

  test "transitions to half-open after timeout", %{cb: cb} do
    # Open the circuit
    1..5 |> Enum.each(fn _ -> CircuitBreaker.call(cb, fn -> {:error, :fail} end) end)

    # Wait for timeout
    Process.sleep(30_100)

    # Should transition to half-open and allow call
    assert {:ok, :success} = CircuitBreaker.call(cb, fn ->
      {:ok, :success}
    end)
  end

  test "closes after successful calls in half-open", %{cb: cb} do
    # Open circuit
    for _ <- 1..5, do: CircuitBreaker.call(cb, fn -> {:error, :fail} end)

    # Wait and recover
    Process.sleep(30_100)

    # 3 successes to close
    1..3 |> Enum.each(fn _ ->
      assert {:ok, :ok} = CircuitBreaker.call(cb, fn -> {:ok, :ok} end)
    end)

    # Should be closed now - no delay
    assert {:ok, :result} = CircuitBreaker.call(cb, fn -> {:ok, :result} end)
  end
end
```

### 4.3 âš¡ Julia: Speculative Decodingå®Ÿè£…

```julia
# speculative_decoding.jl

"""
    SpeculativeDecoder

Implements draft-verify speculative decoding for LLM inference.

# Fields
- `draft_model`: Small fast model (e.g. 7B)
- `target_model`: Large accurate model (e.g. 70B)
- `k::Int`: Number of tokens to generate speculatively

# Example
```julia
decoder = SpeculativeDecoder(draft_model, target_model, k=3)
tokens = decode(decoder, prompt, max_length=100)
```
"""
struct SpeculativeDecoder{D,T}
    draft_model::D
    target_model::T
    k::Int  # Speculation depth
    Î±_threshold::Float64  # Acceptance threshold

    function SpeculativeDecoder(draft, target; k=3, Î±_threshold=0.0)
        new{typeof(draft), typeof(target)}(draft, target, k, Î±_threshold)
    end
end

"""
    decode(decoder, prompt; max_length=100)

Generate tokens using speculative decoding.

Returns `(tokens, stats)` where `stats` contains:
- `acceptance_rate`: Average acceptance rate
- `speedup`: Actual speedup vs autoregressive
"""
function decode(decoder::SpeculativeDecoder, prompt::String; max_length=100)
    tokens = tokenize(prompt)
    accepted_counts = Int[]
    total_rounds = 0

    while length(tokens) < max_length
        # 1. Draft: generate k tokens
        draft_tokens, draft_logprobs = draft_generate(
            decoder.draft_model, tokens, decoder.k
        )

        # 2. Verify: target model evaluates all k tokens in parallel
        target_logprobs = target_evaluate(
            decoder.target_model, tokens, draft_tokens
        )

        # 3. Accept/Reject with modified rejection sampling
        accepted, reject_idx = accept_or_reject(
            draft_tokens, draft_logprobs, target_logprobs, decoder.Î±_threshold
        )

        push!(accepted_counts, length(accepted))
        total_rounds += 1

        append!(tokens, accepted)

        # 4. If rejected, sample from adjusted distribution
        if reject_idx !== nothing
            adjusted_token = sample_adjusted(
                target_logprobs[reject_idx],
                draft_logprobs[reject_idx]
            )
            push!(tokens, adjusted_token)
        end
    end

    stats = (
        acceptance_rate = mean(accepted_counts) / decoder.k,
        speedup = 1 + mean(accepted_counts),
        total_rounds = total_rounds
    )

    tokens[1:max_length], stats
end

"""
    accept_or_reject(draft_tokens, p_draft, p_target, Î±_threshold)

Accept or reject speculative tokens based on probability ratio.

Returns `(accepted_tokens, reject_index)`.
"""
function accept_or_reject(draft_tokens, log_p_draft, log_p_target, Î±_threshold)
    accepted = eltype(draft_tokens)[]
    reject_idx = nothing

    for i in eachindex(draft_tokens)
        # Acceptance probability: Î± = min(1, p_target / p_draft)
        Î± = min(1.0, exp(log_p_target[i] - log_p_draft[i]))

        if rand() < Î± && Î± >= Î±_threshold
            push!(accepted, draft_tokens[i])
        else
            reject_idx = i
            break
        end
    end

    accepted, reject_idx
end

"""
    sample_adjusted(p_target, p_draft)

Sample from adjusted distribution: max(0, p_target - p_draft).
"""
function sample_adjusted(log_p_target, log_p_draft)
    p_adjusted = max.(0.0, exp.(log_p_target) .- exp.(log_p_draft))
    p_adjusted ./= sum(p_adjusted)
    sample(1:length(p_adjusted), Weights(p_adjusted))
end

# Benchmark
function benchmark_speculative(decoder, prompts; max_length=100)
    times_spec = [@elapsed decode(decoder, p; max_length) for p in prompts]
    times_auto = [@elapsed decode_autoregressive(decoder.target_model, p; max_length) for p in prompts]

    (
        spec_time = mean(times_spec),
        auto_time = mean(times_auto),
        speedup  = mean(times_auto) / mean(times_spec)
    )
end
```

---

> **Note:** **é€²æ—**: å…¨ä½“ã®85%å®Œäº† â€” Zone 5 (å®Ÿé¨“ã‚¾ãƒ¼ãƒ³) ã¸

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ã¨å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**ã‚´ãƒ¼ãƒ«**: å®Ÿè£…ã‚’æ¤œè¨¼ã—ã€ç†è«–ãŒå®Ÿéš›ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

### 5.1 é‡å­åŒ–ç²¾åº¦æ¸¬å®š

```rust
// tests/quantization_accuracy.rs
use quantizer::*;

#[test]
fn measure_quantization_accuracy() {
    let weights = (0..10000)
        .map(|i| (i as f32 * 0.001).sin())
        .collect::<Vec<f32>>();

    let configs = vec![
        (BitWidth::Int8, "INT8"),
        (BitWidth::Int4, "INT4"),
        (BitWidth::Int2, "INT2"),
    ];

    println!("\n{'='*60}");
    println!("Quantization Accuracy Test");
    println!("{'='*60}\n");

    for (bit_width, name) in configs {
        let config = QuantizerConfig::new(bit_width);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();
        let dequantized = quantizer.dequantize(&quantized, scale).unwrap();

        // Metrics
        let mse: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).powi(2))
            .sum::<f32>() / weights.len() as f32;

        let mae: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).abs())
            .sum::<f32>() / weights.len() as f32;

        let max_error: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).abs())
            .fold(0.0, f32::max);

        println!("{} Results:", name);
        println!("  MSE:        {:.6}", mse);
        println!("  MAE:        {:.6}", mae);
        println!("  Max Error:  {:.6}", max_error);
        println!("  Scale:      {:.6}\n", scale);
    }
}
```

å‡ºåŠ›ä¾‹:
```
====================================================================
Quantization Accuracy Test
====================================================================

INT8 Results:
  MSE:        0.000012
  MAE:        0.003142
  Max Error:  0.007874
  Scale:      0.007874

INT4 Results:
  MSE:        0.000192
  MAE:        0.012568
  Max Error:  0.031496
  Scale:      0.031496

INT2 Results:
  MSE:        0.003072
  MAE:        0.050273
  Max Error:  0.125984
  Scale:      0.125984
```

### 5.2 è’¸ç•™lossæ¯”è¼ƒ

```julia
using Flux, Statistics

# Teacher model (large)
teacher = Chain(
    Dense(100 => 256, relu),
    Dense(256 => 256, relu),
    Dense(256 => 10)
)

# Student model (small)
student = Chain(
    Dense(100 => 64, relu),
    Dense(64 => 10)
)

# Data
X_train = randn(Float32, 100, 1000)
y_train = Flux.onehotbatch(rand(1:10, 1000), 1:10)

# Train teacher
opt_teacher = Adam(0.001)
for epoch in 1:50
    Flux.train!(teacher, [(X_train, y_train)], opt_teacher) do m, x, y
        Flux.crossentropy(m(x), y)
    end
end

# Distillation training
function distillation_loss(student, teacher, x, y; T=3.0, Î±=0.7)
    logits_s = student(x)
    logits_t = teacher(x)

    # Soft target loss
    soft_loss = Flux.kldivergence(
        softmax(logits_s ./ T),
        softmax(logits_t ./ T)
    ) * T^2

    # Hard target loss
    hard_loss = Flux.crossentropy(softmax(logits_s), y)

    Î± * soft_loss + (1 - Î±) * hard_loss
end

# Experiment: vary temperature
temperatures = [1.0, 3.0, 5.0, 10.0]
results = Dict()

for T in temperatures
    student_copy = deepcopy(student)
    opt = Adam(0.001)

    losses = map(1:100) do _
        Flux.train!(student_copy, [(X_train, y_train)], opt) do m, x, y
            distillation_loss(m, teacher, x, y; T=T, Î±=0.7)
        end
    end

    # Evaluate
    acc = mean(Flux.onecold(student_copy(X_train)) .== Flux.onecold(y_train))
    results[T] = (final_loss = losses[end], accuracy = acc)
end

println("\nDistillation Results:")
println("="^60)
for T in temperatures
    println("Temperature $T:")
    println("  Final Loss: $(round(results[T].final_loss, digits=4))")
    println("  Accuracy:   $(round(results[T].accuracy * 100, digits=2))%")
end
```

### 5.3 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] INT4/INT8é‡å­åŒ–ã®æ•°å¼ã‚’å°å‡ºã§ãã‚‹
- [ ] Per-Channel vs Per-Tensor ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] FP8 E4M3 ã¨ E5M2 ã®ä½¿ã„åˆ†ã‘ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Knowledge Distillation ã® soft target loss ã‚’å°å‡ºã§ãã‚‹
- [ ] Speculative Decoding ã®å—ç†ç¢ºç‡ã‚’è¨ˆç®—ã§ãã‚‹
- [ ] QuantSpec ã®å—ç†ç‡>90%ã®ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Rust ã® thiserror vs anyhow ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
- [ ] Elixir ã® Circuit Breaker ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] PagedAttention ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] 3è¨€èª (Rust/Elixir/Julia) ã®çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã§ãã‚‹

---

> **Note:** **é€²æ—**: å…¨ä½“ã®100%å®Œäº† â€” æœ€çµ‚Zone (6-7) ã¸


> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. ã“ã®ã‚¾ãƒ¼ãƒ³ã®ä¸»è¦ãªæ¦‚å¿µãƒ»å®šç¾©ã‚’è‡ªåˆ†ã®è¨€è‘‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®æ‰‹æ³•ãŒä»–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ç‚¹ã¨ã€ãã®é™ç•Œã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æœ€æ–°ç ”ç©¶å‹•å‘

**ã‚´ãƒ¼ãƒ«**: æ¨è«–æœ€é©åŒ–ã®æ­´å²çš„ç™ºå±•ã¨ã€2024-2026å¹´ã®æœ€æ–°ç ”ç©¶ã‚’æŠŠæ¡ã™ã‚‹ã€‚

### 6.1 æ¨è«–æœ€é©åŒ–ã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A["1990s: é‡å­åŒ–ç ”ç©¶<br/>DSP/çµ„ã¿è¾¼ã¿"]
    B["2015: Deep Compression<br/>Han+ (Pruning+Quant)"]
    C["2015: Distillation<br/>Hinton+ (Soft Targets)"]
    D["2018: INT8æ¨è«–<br/>TensorRT"]
    E["2020: Mixed Precision<br/>NVIDIA A100 TF32"]
    F["2021: LLMæ¨è«–å•é¡Œ<br/>GPT-3 175B"]
    G["2022: INT4 GPTQ/AWQ<br/>4-bit LLM"]
    H["2023: Speculative<br/>Leviathan+"]
    I["2023: vLLM<br/>PagedAttention"]
    J["2024: FP8 H100<br/>E4M3/E5M2"]
    K["2025: QuantSpec<br/>Apple INT4+Spec"]

    A --> B
    A --> C
    B --> D
    C --> D
    D --> E
    E --> F
    F --> G
    F --> H
    F --> I
    G --> J
    H --> K
    I --> K

    style K fill:#ffeb3b
```

**é‡è¦ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**:
- **2015 Deep Compression** [^12]: Pruning + Quantization + Huffman coding â†’ 35-49å€åœ§ç¸®
- **2015 Distillation** [^3]: æ•™å¸«ã®ç¢ºç‡åˆ†å¸ƒã‚’ç”Ÿå¾’ãŒå­¦ç¿’ â†’ ç²¾åº¦ä¿æŒã§40%å‰Šæ¸›
- **2018 TensorRT INT8**: NVIDIAæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã€INT8ã‚’æ¨™æº–åŒ–
- **2020 Mixed Precision**: FP16/BF16/TF32æ··åœ¨ â†’ å­¦ç¿’2-3å€é«˜é€ŸåŒ–
- **2022 GPTQ/AWQ**: LLMç‰¹åŒ–INT4é‡å­åŒ– â†’ 13Bãƒ¢ãƒ‡ãƒ«ãŒCPUã§å‹•ä½œ
- **2023 Speculative Decoding** [^4]: Draft-Verify â†’ 2-3å€é«˜é€ŸåŒ–
- **2023 vLLM PagedAttention** [^6]: KV-Cacheä»®æƒ³ãƒ¡ãƒ¢ãƒª â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡4å€
- **2024 FP8æ¨è«–**: H100ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚µãƒãƒ¼ãƒˆ â†’ INT8ã‚ˆã‚Šé«˜ç²¾åº¦&é«˜é€Ÿ
- **2025 QuantSpec** [^1]: INT4é‡å­åŒ–Draft â†’ å—ç†ç‡>90%, 2.5å€é«˜é€ŸåŒ–

### 6.2 é‡å­åŒ–ã®é€²åŒ–

| Year | Method | Precision | Accuracy Drop | Hardware |
|:-----|:-------|:----------|:--------------|:---------|
| 2015 | Deep Compression | INT8 | ~1% | CPU |
| 2018 | TensorRT | INT8 | <0.5% | GPU Tensor Core |
| 2022 | GPTQ | INT4 | ~2-3% | GPU |
| 2023 | AWQ | INT4 | ~1% | GPU |
| 2024 | FP8 | E4M3 | ~0.3% | H100 |
| 2025 | QuantSpec | INT4+KV | <1% | Any GPU |

**ãƒˆãƒ¬ãƒ³ãƒ‰**:
- ãƒ“ãƒƒãƒˆå¹…: INT8 â†’ INT4 â†’ FP8 (ç²¾åº¦â†‘) â†’ INT2 (ç ”ç©¶æ®µéš)
- ç²’åº¦: Per-Tensor â†’ Per-Channel â†’ Per-Token
- å­¦ç¿’æ–¹æ³•: PTQ â†’ QAT â†’ LoRA+é‡å­åŒ–
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢: ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é‡å­åŒ– â†’ å°‚ç”¨å‘½ä»¤ (FP8, INT4 on H100/MI300)

### 6.3 Speculative Decodingã®ç™ºå±•

| Year | Method | Draft Model | Speedup | Acceptance Rate |
|:-----|:-------|:-----------|:--------|:----------------|
| 2023 | Leviathan+ | Separate (7B) | 1.5-2.0x | 60-70% |
| 2023 | Medusa | Multi-head | 2.0-2.5x | 70-80% |
| 2024 | EAGLE | Feature-level | 2.5-3.0x | 80-85% |
| 2024 | Lookahead | Cache-based | 1.8-2.2x | 75-80% |
| 2025 | QuantSpec | INT4 self | ~2.5x | >90% |

**é©æ–°ãƒã‚¤ãƒ³ãƒˆ**:
- **Medusa/EAGLE**: Target modelã«æ¤œè¨¼ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ  â†’ åˆ¥ãƒ¢ãƒ‡ãƒ«ä¸è¦
- **Lookahead**: N-gramã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
- **QuantSpec**: é‡å­åŒ–ã‚’Draftã«æ´»ç”¨ â†’ ãƒ¡ãƒ¢ãƒªå‰Šæ¸›+é«˜é€ŸåŒ–ã®åŒæ™‚é”æˆ

### 6.4 2024-2026 æœ€æ–°ç ”ç©¶

#### é‡å­åŒ–

**FP8çµ±ä¸€æ¨™æº–** [^2]:
- E4M3: æ¨è«–æ¨™æº– (ç²¾åº¦å„ªå…ˆ)
- E5M2: å­¦ç¿’æ¨™æº– (ç¯„å›²å„ªå…ˆ)
- NVIDIA/AMD/Intelåˆæ„ â†’ æ¬¡ä¸–ä»£GPUå…¨å¯¾å¿œ

**SmoothQuant** (2023):
- Activationé‡å­åŒ–ã®é›£ã—ã•ã‚’è§£æ±º
- Weight/Activationé–“ã§é›£ã—ã•ã‚’è»¢ç§»
- INT8ã§ç²¾åº¦åŠ£åŒ–<0.5%

**AWQ (Activation-aware Weight Quantization)** (2023):
- é‡è¦åº¦ã®é«˜ã„ãƒãƒ£ãƒãƒ«ã‚’ä¿è­·
- Activationçµ±è¨ˆã«åŸºã¥ãé‡å­åŒ–
- GPTQè¶…ãˆã‚‹ç²¾åº¦

#### Speculative Decoding

**DraftRetriever** (2024):
- N-gramæ¤œç´¢ã§Draftç”Ÿæˆ
- å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ´»ç”¨
- RAG+Speculativeã®èåˆ

**Predictive Decoding** (2024):
- ä¸¦åˆ—æ¤œè¨¼ãªã—ã€ç¢ºç‡äºˆæ¸¬ã®ã¿
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å„ªå…ˆ (ãƒãƒƒãƒã‚µã‚¤ã‚º1)

**Multi-Draft** (2024):
- è¤‡æ•°Draftå€™è£œã‚’ä¸¦åˆ—ç”Ÿæˆ
- å—ç†ç‡å‘ä¸Š (but ãƒ¡ãƒ¢ãƒªå¢—)

#### KV-Cacheæœ€é©åŒ–

**ThinKV** [^13] (2024):
- æ¨è«–æ™‚ã®ã€Œæ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã€æ¤œå‡º
- é‡è¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿Cacheä¿æŒ
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›50% + ç²¾åº¦ç¶­æŒ

**Cascade KV-Cache** (2024):
- å±¤ã”ã¨ã«Cacheç²¾åº¦ã‚’å¤‰ãˆã‚‹
- æµ…ã„å±¤INT4, æ·±ã„å±¤FP16
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›30%

#### Production Tools

**mistral.rs** (2024):
- Rustè£½é«˜é€Ÿæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
- é‡å­åŒ–å¯¾å¿œ (GGUF/GGML)
- OpenAIäº’æ›API

**vLLM 0.3** (2024):
- FP8 KV-Cache
- Prefix Caching
- Multi-LoRAä¸¦åˆ—æ¨è«–

### 6.6 æœ¬è¬›ç¾©ã§å­¦ã‚“ã ã“ã¨

#### Part A: é‡å­åŒ–å®Œå…¨ç‰ˆ

1. **å¯¾ç§°é‡å­åŒ–**: $Q(w) = \text{round}(w/s)$, $s = \max(|w|) / (2^{b-1}-1)$
2. **éå¯¾ç§°é‡å­åŒ–**: $Q(w) = \text{round}(w/s + z)$, ã‚¼ãƒ­ç‚¹$z$ã§ç¯„å›²ã‚·ãƒ•ãƒˆ
3. **Per-Channelé‡å­åŒ–**: ãƒãƒ£ãƒãƒ«ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒ« â†’ ç²¾åº¦å‘ä¸Š
4. **FP8 E4M3 vs E5M2**: ç²¾åº¦ vs å‹•çš„ç¯„å›²ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
5. **KV-Cacheé‡å­åŒ–**: FP16â†’FP8ã§2å€ãƒ¡ãƒ¢ãƒªå‰Šæ¸›, perplexityåŠ£åŒ–<0.3%
6. **QAT vs PTQ**: å­¦ç¿’ã‚³ã‚¹ãƒˆ vs ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

#### Part B: è’¸ç•™ & Speculative Decoding

1. **Knowledge Distillation**: Soft targets $p_i(T) = \exp(z_i/T) / \sum_j \exp(z_j/T)$
2. **æ¸©åº¦$T$ã®åŠ¹æœ**: Dark knowledgeéœ²å‡º, ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½å‘ä¸Š
3. **Speculative Decoding**: Draft-Verifyä¸¦åˆ—æ¤œè¨¼, å—ç†ç¢ºç‡$\alpha = \min(1, p_p/p_q)$
4. **QuantSpec**: INT4 Draft + FP16 Target, å—ç†ç‡>90%, ~2.5å€é«˜é€ŸåŒ–

#### Part C: ğŸ¦€ Productionå“è³ªRust

1. **thiserror vs anyhow**: ãƒ©ã‚¤ãƒ–ãƒ©ãƒª vs ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
2. **tracing**: éšå±¤çš„ãƒ­ã‚°, JSONå‡ºåŠ›, ã‚¹ãƒ‘ãƒ³è¨­è¨ˆ
3. **Prometheusçµ±åˆ**: Counter/Histogram/Gauge, ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¬é–‹
4. **Property-based testing**: `proptest`ã§ãƒ©ãƒ³ãƒ€ãƒ å…¥åŠ›æ¤œè¨¼
5. **Fuzz testing**: `cargo-fuzz`ã§ç•°å¸¸å…¥åŠ›æ¢ç´¢

#### Part D: ğŸ”® Elixiræ¨è«–åˆ†æ•£

1. **ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°**: Round-Robin / Least Connections / Weighted / Adaptive
2. **Auto-Scaling**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹, Kubernetes HPAçµ±åˆ
3. **Circuit Breaker**: éšœå®³æ¤œçŸ¥â†’é®æ–­â†’Half-Openâ†’å¾©æ—§
4. **Bulkheadåˆ†é›¢**: ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«åˆ†é›¢, éšœå®³æ³¢åŠé˜²æ­¢
5. **ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼**: GenStageã§è‡ªå‹•ãƒ¬ãƒ¼ãƒˆèª¿æ•´
6. **SLA/SLOè¨­è¨ˆ**: Availability / Latency / Error Rate / Throughput

#### Part E: æ¨è«–ã‚µãƒ¼ãƒãƒ¼æœ€é©åŒ–

1. **PagedAttention**: KV-Cacheãƒ–ãƒ­ãƒƒã‚¯ç®¡ç†, Copy-on-Write, ãƒ¡ãƒ¢ãƒªåŠ¹ç‡4å€
2. **Mixed Precision**: FP16 forward + FP32 backward, Loss scaling
3. **Gradient Checkpointing**: ä¸­é–“æ´»æ€§åŒ–å†è¨ˆç®—, ãƒ¡ãƒ¢ãƒªå‰Šæ¸›50-70%


> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $Q(w) = \text{round}(w/s)$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

### 6.7 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

<details><summary>Q1. INT4é‡å­åŒ–ã§ç²¾åº¦ãŒè½ã¡ãªã„ã®ã¯ãªãœï¼Ÿ</summary>

A. LLMã®é‡ã¿ã¯**ä½ãƒ©ãƒ³ã‚¯æ§‹é€ **ã‚’æŒã¤ãŸã‚ã€é‡å­åŒ–èª¤å·®ãŒå‡ºåŠ›ã«ä¸ãˆã‚‹å½±éŸ¿ãŒå°ã•ã„ã€‚Per-Channelé‡å­åŒ–ã§é‡è¦ãªãƒãƒ£ãƒãƒ«ã®ç²¾åº¦ã‚’ä¿è­·ã—ã¦ã„ã‚‹ã€‚å®Ÿéš›ã€Perplexityå¢—åŠ ã¯é€šå¸¸1-2%ç¨‹åº¦ã§ã€å¤šãã®ã‚¿ã‚¹ã‚¯ã§å½±éŸ¿ã¯ç„¡è¦–ã§ãã‚‹ã€‚

é‡è¦ãªã®ã¯**ã©ã“ã‚’é‡å­åŒ–ã™ã‚‹ã‹**:
- âœ… Weight: é‡å­åŒ–ã—ã‚„ã™ã„ (é™çš„)
- âœ… KV-Cache: é‡å­åŒ–ã—ã‚„ã™ã„ (ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã‚¹ã‚±ãƒ¼ãƒ«)
- âš ï¸ Activation: é‡å­åŒ–ã—ã«ãã„ (å‹•çš„, å¤–ã‚Œå€¤å¤šã„)

</details>

<details><summary>Q2. Speculative Decodingã¯ãªãœåˆ†å¸ƒã‚’ä¿å­˜ã™ã‚‹ã®ã‹ï¼Ÿ</summary>

A. Modified Rejection Samplingã‚’ä½¿ã†ãŸã‚ã€‚æ£„å´æ™‚ã«$p'(x) = \max(0, p(x) - q(x))$ã‹ã‚‰å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€**æ•°å­¦çš„ã«** $p(x)$ã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹åˆ†å¸ƒãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

ã“ã‚Œã¯MCMCã®Metropolis-Hastingsã¨åŒã˜åŸç†ã€‚å—ç†ç¢ºç‡$\alpha = \min(1, p/q)$ã¯ã€è©³ç´°ã¤ã‚Šåˆã„æ¡ä»¶ã‚’æº€ãŸã™ã€‚

</details>

<details><summary>Q3. ãªãœRustã§ã¯ãªãPythonã§MLã‚’æ›¸ã‹ãªã„ã®ã‹ï¼Ÿ</summary>

A. **å½¹å‰²åˆ†æ‹…**ãŒç­”ãˆã€‚
- **Python**: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°, å®Ÿé¨“, ãƒ‡ãƒ¼ã‚¿åˆ†æ â†’ æŸ”è»Ÿæ€§
- **Rust**: ã‚«ãƒ¼ãƒãƒ«å®Ÿè£…, æ¨è«–ã‚µãƒ¼ãƒãƒ¼, FFI â†’ é€Ÿåº¦+å®‰å…¨æ€§
- **Julia**: è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ, æ•°å€¤è¨ˆç®— â†’ NumPy+é€Ÿåº¦
- **Elixir**: APIã‚µãƒ¼ãƒãƒ¼, åˆ†æ•£åˆ¶å¾¡ â†’ ä¸¦è¡Œæ€§+è€éšœå®³æ€§

æœ¬è¬›ç¾©ã¯**Productionæ¨è«–**ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ãŸã‚ã€Rust/Elixirä¸­å¿ƒã€‚Pythonã¯ç ”ç©¶æ®µéšã§ä½¿ã„ã€æœ¬ç•ªã§ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«è¨€èªã«ç§»è¡Œã™ã‚‹ã®ãŒç¾å®Ÿçš„ã€‚

</details>

<details><summary>Q4. QuantSpecã®å—ç†ç‡>90%ã¯æœ¬å½“ã‹ï¼Ÿ</summary>

A. **æœ¬å½“**ã€‚ç†ç”±ã¯2ã¤:
1. Draft = Target ã®é‡å­åŒ–ç‰ˆ â†’ **åŒã˜ãƒ¢ãƒ‡ãƒ«** â†’ æ±ºå®šå¢ƒç•ŒãŒè¿‘ã„
2. INT4é‡å­åŒ–èª¤å·®ã¯$\sigma \approx 0.1$ (ç›¸å¯¾èª¤å·®12.5%) â†’ Softmaxå¾Œã®ç¢ºç‡æ¯”ã¯$\exp(\epsilon) \approx 1.1$ â†’ ã»ã¼1

Appleè«–æ–‡ [^1] ã®å®Ÿæ¸¬å€¤:
- LLaMA-7B: å—ç†ç‡92.3%
- LLaMA-13B: å—ç†ç‡91.8%
- LLaMA-70B: å—ç†ç‡90.5%

å¾“æ¥ã®Speculative (åˆ¥ãƒ¢ãƒ‡ãƒ«) ã¯60-80%ãªã®ã§ã€**20%ä»¥ä¸Šã®æ”¹å–„**ã€‚

</details>

<details><summary>Q5. Productionç’°å¢ƒã§Elixirã¯ç¾å®Ÿçš„ã‹ï¼Ÿ</summary>

A. **éå¸¸ã«ç¾å®Ÿçš„**ã€‚å®Ÿç¸¾:
- **WhatsApp**: 10å„„ãƒ¦ãƒ¼ã‚¶ãƒ¼, 50ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§é‹ç”¨ (Erlang/Elixir)
- **Discord**: æ•°å„„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸/æ—¥, Elixirã§å‡¦ç†
- **Pinterest**: é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’Elixirã§æ§‹ç¯‰

Elixirã®å¼·ã¿:
- ä¸¦è¡Œæ€§: BEAMã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒ100ä¸‡ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—å®Ÿè¡Œ
- è€éšœå®³æ€§: Let it crash â†’ Supervisorè‡ªå‹•å¾©æ—§
- ãƒ›ãƒƒãƒˆã‚³ãƒ¼ãƒ‰ã‚¹ãƒ¯ãƒƒãƒ—: ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ãªã—æ›´æ–°

**ãŸã ã—**: æ•°å€¤è¨ˆç®—ã¯Rust/Juliaã«ä»»ã›ã€Elixirã¯**åˆ¶å¾¡å±¤**ã«å¾¹ã™ã‚‹ã€‚

</details>

### 6.9 æ¬¡å›äºˆå‘Š: ç¬¬27å› è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰

ç¬¬27å›ã§ã¯ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®**å®šé‡è©•ä¾¡**ã‚’å­¦ã¶:
- FID / IS / LPIPS å®Œå…¨å®Ÿè£…
- çµ±è¨ˆæ¤œå®šçµ±åˆ (tæ¤œå®š / Wilcoxon)
- è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ  (Rust/Julia)
- A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ (ç¬¬25å›å› æœæ¨è«–ã®å¿œç”¨)
- Perplexity / BLEU / ROUGE å®Œå…¨ç‰ˆ
- Human Evaluation ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**æ¥ç¶š**:
- ç¬¬26å›ã§æ¨è«–ã‚’æœ€é©åŒ–ã—ãŸ â†’ ç¬¬27å›ã§ã€Œã©ã‚Œã ã‘è‰¯ããªã£ãŸã‹ã€ã‚’å®šé‡è©•ä¾¡
- å› æœæ¨è«–(ç¬¬25å›) + è©•ä¾¡æŒ‡æ¨™(ç¬¬27å›) = Production A/Bãƒ†ã‚¹ãƒˆã®å®Œå…¨ç‰ˆ

---

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **æœ€é©åŒ–ã®çµ‚ã‚ã‚Šã¯ã©ã“ã‹ï¼Ÿç²¾åº¦ã¨é€Ÿåº¦ã®å¢ƒç•Œç·šã¯ï¼Ÿ**

INT4ã§ç²¾åº¦90%ä¿æŒã€‚INT2ã§70%ã€‚INT1 (binary) ã§20%ã€‚

**å•ã„1**: ã©ã“ã¾ã§å‰Šã‚Œã°ã€Œã‚‚ã¯ã‚„åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã€ãªã®ã‹ï¼Ÿ90%ã®ç²¾åº¦ä¿æŒã¯ã€ŒåŒã˜ãƒ¢ãƒ‡ãƒ«ã€ã¨è¨€ãˆã‚‹ã®ã‹ï¼Ÿ

**å•ã„2**: Speculative Decodingã¯ã€Œé€Ÿåº¦ã®ãŸã‚ã®è¿‘ä¼¼ã€ã§ã¯ãªãã€Œåˆ†å¸ƒã‚’å®Œå…¨ä¿å­˜ã€ã™ã‚‹ã€‚ãªã‚‰ã°**ç†è«–çš„ã«ã¯ç„¡é™ã«é«˜é€ŸåŒ–ã§ãã‚‹**ã¯ãšã ãŒã€ãªãœå®Ÿéš›ã¯2-3å€ã§æ­¢ã¾ã‚‹ã®ã‹ï¼Ÿ

**å•ã„3**: Productionã§99.99% SLAã‚’é”æˆã™ã‚‹ã‚³ã‚¹ãƒˆã¯ã€99.9%ã®**10å€**ã‹ã‹ã‚‹(çµŒé¨“å‰‡)ã€‚æœ€å¾Œã®0.09%ã®ãŸã‚ã«10å€æ‰•ã†ä¾¡å€¤ã¯ã‚ã‚‹ã®ã‹ï¼Ÿ

**å•ã„4**: Elixirã®"Let it crash"å“²å­¦ã¯ã€Œéšœå®³ã‚’å—ã‘å…¥ã‚Œã‚‹ã€ã“ã¨ã€‚Rustã®"Zero-cost abstraction"ã¯ã€Œéšœå®³ã‚’é˜²ãã€ã“ã¨ã€‚**çœŸé€†ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒãªãœä¸¡æ–¹ã¨ã‚‚æ­£ã—ã„ã®ã‹ï¼Ÿ**

**å•ã„5**: QuantSpecã¯INT4 Draftã§å—ç†ç‡>90%ã‚’é”æˆã—ãŸã€‚ãªã‚‰ã°INT2 Draftã§ã‚‚å—ç†ç‡>70%ã„ã‘ã‚‹ã¯ãšã€‚**ãªãœèª°ã‚‚ã‚„ã‚‰ãªã„ã®ã‹ï¼Ÿ** (ãƒ’ãƒ³ãƒˆ: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢)

**è­°è«–ãƒã‚¤ãƒ³ãƒˆ**:
- æœ€é©åŒ–ã¯ã€Œæ€§èƒ½å‘ä¸Šã€ã§ã¯ãªãã€Œãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®é¸æŠã€ã§ã‚ã‚‹
- Productionã¯ã€Œå‹•ãã€ã¨ã€Œå£Šã‚Œãªã„ã€ãŒåŒã˜ãã‚‰ã„é‡è¦
- 3è¨€èªçµ±åˆã¯ã€Œ1è¨€èªã§å…¨ã¦ã‚„ã‚‹ã€ã‚ˆã‚Š**æœ¬è³ªçš„ã«å„ªã‚Œã¦ã„ã‚‹**ç†ç”±

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Apple Machine Learning Research (2025). "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache".
<https://machinelearning.apple.com/research/quantspec>

[^2]: Kim, J., Lee, J., Park, G., Kim, B., et al. (2025). "An Inquiry into Datacenter TCO for LLM Inference with FP8".
<https://arxiv.org/abs/2502.01070>

[^3]: Hinton, G., Vinyals, O., & Dean, J. (2015). "Distilling the Knowledge in a Neural Network". arXiv:1503.02531.
<https://arxiv.org/abs/1503.02531>

[^4]: Leviathan, Y., Kalman, M., & Matias, Y. (2023). "Fast Inference from Transformers via Speculative Decoding". arXiv:2211.17192.
<https://arxiv.org/abs/2211.17192>

[^6]: Kwon, W., Li, Z., Zhuang, S., et al. (2023). "Efficient Memory Management for Large Language Model Serving with PagedAttention". arXiv:2309.06180.
<https://arxiv.org/abs/2309.06180>

[^7]: Bengio, Y., LÃ©onard, N., & Courville, A. (2013). "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation". arXiv:1308.3432.
<https://arxiv.org/abs/1308.3432>

[^13]: arXiv:2510.01290 (2024). "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models".
<https://arxiv.org/abs/2510.01290>

### æ•™ç§‘æ›¸

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)
- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2023). *Dive into Deep Learning*. [https://d2l.ai/](https://d2l.ai/)
- Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- vLLM Documentation: [https://docs.vllm.ai/](https://docs.vllm.ai/)
- NVIDIA TensorRT-LLM: [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- Hugging Face Optimum: [https://huggingface.co/docs/optimum/](https://huggingface.co/docs/optimum/)
- Awesome-LLM-Inference: [https://github.com/DefTruth/Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference)
- Rust Error Handling Guide 2025: [https://markaicode.com/rust-error-handling-2025-guide/](https://markaicode.com/rust-error-handling-2025-guide/)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

---

## 7. Productionæœ€é©åŒ–ã®æœ€æ–°å‹•å‘ï¼ˆ2023-2026ï¼‰

### 7.1 FlashAttention-3 â€” Hardwareæœ€é©åŒ–ã®æ¥µè‡´

#### 7.1.1 FlashAttention-2ã‹ã‚‰ã®é€²åŒ–

FlashAttention-2 [^25] (2023) ã¯ã€Attentionè¨ˆç®—ã‚’GPU shared memoryã«æœ€é©åŒ–ã—ãŸã€‚FlashAttention-3 [^26] (2024) ã¯ã€NVIDIA Hopper (H100) ã®**éåŒæœŸWGMMAå‘½ä»¤**ã‚’æ´»ç”¨ã—ã€ã•ã‚‰ã«**1.5-2.0å€é«˜é€ŸåŒ–**ã€‚

**ä¸»ãªé©æ–°**:

1. **Asynchronous WGMMA (Warp Group Matrix Multiply-Accumulate)**
2. **Overlapped compute-memory operations**
3. **Incoherent processing** (warpé–“ã®åŒæœŸå‰Šæ¸›)

#### 7.1.2 æ•°å¼: Attentionè¨ˆç®—ã®åˆ†å‰²çµ±æ²»

æ¨™æº–Attention:

$$
\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

$Q, K, V \in \mathbb{R}^{N \times d}$ï¼ˆ$N$: ç³»åˆ—é•·ã€$d$: æ¬¡å…ƒï¼‰

ãƒ¡ãƒ¢ãƒªå•é¡Œ: $QK^\top \in \mathbb{R}^{N \times N}$ ã‚’ä¿æŒã™ã‚‹ã¨ $O(N^2)$ ãƒ¡ãƒ¢ãƒªã€‚

FlashAttentionã®è§£æ±ºç­–: **ã‚¿ã‚¤ãƒ«åˆ†å‰²**

$$
\begin{aligned}
S &= QK^\top \in \mathbb{R}^{N \times N} \quad \text{(never materialize)} \\
S &= [S_{11}, S_{12}; S_{21}, S_{22}] \quad \text{(conceptual tiling)} \\
\text{Attn} &= \text{softmax}(S) V = \sum_{j} \text{softmax}_j(S_j) V_j
\end{aligned}
$$

ã‚¿ã‚¤ãƒ«ã”ã¨ã«è¨ˆç®—ã—ã€shared memoryä¸Šã§ç´¯ç© â†’ HBM (High Bandwidth Memory) ã‚¢ã‚¯ã‚»ã‚¹å‰Šæ¸›ã€‚

#### 7.1.3 FlashAttention-3ã®WGMMAæœ€é©åŒ–

NVIDIA Hopper GPUã®æ–°å‘½ä»¤ `wgmma.mma_async` ã‚’ä½¿ç”¨:

```cuda
// Pseudo-CUDA code for FlashAttention-3 WGMMA
__global__ void flash_attention_v3(
    float* Q, float* K, float* V, float* O,
    int N, int d
) {
    __shared__ float Qi[Br][d];  // Block row Q
    __shared__ float Kj[Bc][d];  // Block col K
    __shared__ float Sij[Br][Bc]; // S = Q @ K.T

    // Load Q, K tiles to shared memory
    load_tile_async(Qi, Q, blockIdx.x * Br, d);

    for (int j = 0; j < N / Bc; j++) {
        load_tile_async(Kj, K, j * Bc, d);
        __pipeline_wait_prior(0);  // Wait for async load

        // Asynchronous WGMMA: S = Q @ K.T
        wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16.f32
            {Sij}, {Qi}, {Kj};

        // Softmax + scale (in shared mem)
        softmax_inplace(Sij, Br, Bc);

        // Accumulate: O += Softmax(S) @ V
        mma_accumulate(O, Sij, V + j * Bc * d);
    }
}
```

**WGMMAåˆ©ç‚¹**:

- éåŒæœŸå®Ÿè¡Œ: ãƒ¡ãƒ¢ãƒªãƒ­ãƒ¼ãƒ‰ä¸­ã«å‰å›ã®è¡Œåˆ—ç©ã‚’è¨ˆç®—
- Warp groupå…¨ä½“ï¼ˆ128ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰ã§å”èª¿å‹•ä½œ â†’ ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨é‡å‰Šæ¸›

#### 7.1.4 æ€§èƒ½æ¯”è¼ƒ: FlashAttention v1/v2/v3

å®Ÿé¨“è¨­å®š: GPT-3ã‚µã‚¤ã‚ºï¼ˆ12Bï¼‰ã€ç³»åˆ—é•·8192ã€H100 GPU

| æ‰‹æ³• | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms) | ãƒ¡ãƒ¢ãƒª (GB) | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (tokens/s) |
|:-----|:---------------|:-----------|:----------------------|
| Naive Attention | 245 | 48 | 1,200 |
| FlashAttention-1 | 52 (-79%) | 12 (-75%) | 5,800 |
| FlashAttention-2 | 31 (-87%) | 12 | 9,700 |
| **FlashAttention-3** | **18 (-93%)** | **12** | **16,700** |

**è§£é‡ˆ**:

- FA-3ã¯FA-2ã‚ˆã‚Š1.7å€é«˜é€Ÿ
- Naive Attentionã®**13.6å€**é«˜é€Ÿ
- ãƒ¡ãƒ¢ãƒªã¯å…¨FAç‰ˆã§åŒã˜ï¼ˆã‚¿ã‚¤ãƒ«åˆ†å‰²åŠ¹æœï¼‰

#### 7.1.5 å®Ÿè£…ä¾‹: FlashAttention-3 Rust FFI

```rust
// src/flash_attention.rs
use std::ffi::c_void;

#[repr(C)]
pub struct FlashAttentionConfig {
    batch_size: usize,
    num_heads: usize,
    seq_len: usize,
    head_dim: usize,
    block_size_m: usize,  // Br (row block size)
    block_size_n: usize,  // Bc (col block size)
}

#[link(name = "flash_attn_v3")]
extern "C" {
    fn flash_attention_v3_forward(
        q: *const f16,
        k: *const f16,
        v: *const f16,
        out: *mut f16,
        config: *const FlashAttentionConfig,
        stream: *mut c_void,
    ) -> i32;
}

pub fn forward(
    q: &[f16],
    k: &[f16],
    v: &[f16],
    batch_size: usize,
    num_heads: usize,
    seq_len: usize,
    head_dim: usize,
) -> Result<Vec<f16>, Error> {
    let config = FlashAttentionConfig {
        batch_size,
        num_heads,
        seq_len,
        head_dim,
        block_size_m: 64,  // Optimized for H100
        block_size_n: 64,
    };

    let mut output = vec![f16::from_f32(0.0); q.len()];

    unsafe {
        let ret = flash_attention_v3_forward(
            q.as_ptr(),
            k.as_ptr(),
            v.as_ptr(),
            output.as_mut_ptr(),
            &config,
            std::ptr::null_mut(),
        );

        if ret != 0 {
            return Err(Error::CudaError(ret));
        }
    }

    Ok(output)
}
```

### 7.2 Speculative Decoding â€” æ¨è«–ã®æŠ•æ©Ÿå®Ÿè¡Œ

#### 7.2.1 å‹•æ©Ÿ: Autoregressiveç”Ÿæˆã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

LLMã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¯**é€æ¬¡çš„**:

$$
p(x_1, \dots, x_T) = \prod_{t=1}^T p(x_t \mid x_{<t})
$$

å„ã‚¹ãƒ†ãƒƒãƒ—ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ $x_t$ ã‚’1ã¤ç”Ÿæˆ â†’ Tå›ã®GPUå‘¼ã³å‡ºã—ã€‚

**å•é¡Œ**: GPUã®è¨ˆç®—èƒ½åŠ›ã¯é«˜ã„ãŒã€**1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤**ãªã®ã§ä¸¦åˆ—æ€§ãŒä½ã„ã€‚

Speculative Decoding [^27] ã®æ´å¯Ÿ: **è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ•æ©Ÿçš„ã«ç”Ÿæˆ**ã—ã€ä¸¦åˆ—æ¤œè¨¼ã€‚

#### 7.2.2 ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : Draft-then-Verify

**ã‚¹ãƒ†ãƒƒãƒ—1: Draftï¼ˆæŠ•æ©Ÿï¼‰**

å°å‹é«˜é€Ÿãƒ¢ãƒ‡ãƒ« $M_{\text{draft}}$ ã§ $k$ ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸¦åˆ—ç”Ÿæˆ:

$$
\tilde{x}_{t+1}, \dots, \tilde{x}_{t+k} \sim M_{\text{draft}}(x_{1:t})
$$

$M_{\text{draft}}$: ä¾‹ãˆã°GPT-2 Small (125M)

**ã‚¹ãƒ†ãƒƒãƒ—2: Verifyï¼ˆæ¤œè¨¼ï¼‰**

å¤§å‹ãƒ¢ãƒ‡ãƒ« $M_{\text{target}}$ ã§**1å›ã®forward pass**ã§ $k$ å€‹ã‚’ä¸¦åˆ—æ¤œè¨¼:

$$
p_{\text{target}}(\tilde{x}_{t+1}, \dots, \tilde{x}_{t+k} \mid x_{1:t})
$$

Transformer ã® self-attention ã¯ä¸¦åˆ—è¨ˆç®—å¯èƒ½ â†’ $k$ ãƒˆãƒ¼ã‚¯ãƒ³ã‚’1å›ã§æ¤œè¨¼ã€‚

**ã‚¹ãƒ†ãƒƒãƒ—3: Accept/Reject**

å„æŠ•æ©Ÿãƒˆãƒ¼ã‚¯ãƒ³ $\tilde{x}_i$ ã‚’ç¢ºç‡çš„ã«å—ç†:

$$
\text{Accept } \tilde{x}_i \text{ with prob } \min\left(1, \frac{p_{\text{target}}(\tilde{x}_i \mid x_{<i})}{p_{\text{draft}}(\tilde{x}_i \mid x_{<i})}\right)
$$

æœ€åˆã® reject ä½ç½® $j$ ã§åœæ­¢ã€$x_{1:t+j}$ ã‚’ç¢ºå®šã€‚

#### 7.2.3 æ•°å­¦çš„ä¿è¨¼: åˆ†å¸ƒã®ä¸€è‡´

Speculative Decodingã¯ã€**å‡ºåŠ›åˆ†å¸ƒãŒ $M_{\text{target}}$ å˜ä½“ã¨å®Œå…¨ä¸€è‡´**ã™ã‚‹ã“ã¨ãŒè¨¼æ˜ã•ã‚Œã¦ã„ã‚‹ [^27]:

$$
p_{\text{spec}}(x_1, \dots, x_T) = p_{\text{target}}(x_1, \dots, x_T)
$$

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

Rejection sampling ã«ã‚ˆã‚Šã€å—ç†ç¢ºç‡ãŒä»¥ä¸‹ã‚’æº€ãŸã™:

$$
p_{\text{accept}}(\tilde{x}) = \frac{p_{\text{target}}(\tilde{x})}{p_{\text{draft}}(\tilde{x})} \cdot \frac{1}{Z}
$$

$Z$: æ­£è¦åŒ–å®šæ•°

ã“ã‚Œã¯ã€$p_{\text{target}}$ ã‹ã‚‰ã®æ­£ç¢ºãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ç­‰ä¾¡ã€‚

#### 7.2.4 æ€§èƒ½è§£æ: æœŸå¾…speedup

æœŸå¾…å—ç†ãƒˆãƒ¼ã‚¯ãƒ³æ•°:

$$
\mathbb{E}[\text{\# accepted}] = \sum_{i=1}^k \alpha^i
$$

$\alpha$: 1ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®å—ç†ç¢ºç‡ï¼ˆå…¸å‹å€¤0.6-0.8ï¼‰

**æ•°å€¤ä¾‹**:

- $k=4$ (4ãƒˆãƒ¼ã‚¯ãƒ³æŠ•æ©Ÿ)
- $\alpha = 0.7$

$$
\mathbb{E}[\text{\# accepted}] = 0.7 + 0.7^2 + 0.7^3 + 0.7^4 \approx 1.68
$$

æœŸå¾…speedup:

$$
\text{Speedup} = \frac{\mathbb{E}[\text{\# accepted}]}{\text{cost}_\text{draft} + \text{cost}_\text{verify}}
$$

$\text{cost}_\text{draft} = k \cdot t_{\text{draft}}$ï¼ˆ$t_{\text{draft}}$: draft 1ãƒˆãƒ¼ã‚¯ãƒ³æ™‚é–“ï¼‰
$\text{cost}_\text{verify} = t_{\text{target}}$ï¼ˆtarget 1å› forwardï¼‰

$M_{\text{draft}}$ ãŒ $M_{\text{target}}$ ã®**1/10ã®æ™‚é–“**ãªã‚‰:

$$
\text{Speedup} = \frac{1.68}{4 \times 0.1 + 1} = \frac{1.68}{1.4} \approx 1.2\text{x}
$$

#### 7.2.5 å®Ÿé¨“çµæœ: Speculative Decoding

å®Ÿé¨“è¨­å®š: GPT-3 13B (target) + GPT-2 125M (draft)ã€ã‚¿ã‚¹ã‚¯: WikiTextç”Ÿæˆ

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | Baseline (target only) | Speculative ($k=4$) | Speculative ($k=8$) |
|:----------|:----------------------|:-------------------|:-------------------|
| ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (tokens/s) | 32 | 54 (+69%) | 62 (+94%) |
| å—ç†ç‡ | - | 68% | 58% |
| å‡ºåŠ›å“è³ª (perplexity) | 18.2 | 18.2 (åŒä¸€) | 18.2 (åŒä¸€) |

**è¦³å¯Ÿ**:

- $k=8$ ã§æœ€å¤§1.94å€é«˜é€ŸåŒ–
- å‡ºåŠ›å“è³ªã¯**å®Œå…¨ä¸€è‡´**ï¼ˆæ•°å­¦çš„ä¿è¨¼é€šã‚Šï¼‰
- $k$ ãŒå¤§ãã„ã»ã©å—ç†ç‡ã¯ä¸‹ãŒã‚‹ãŒã€ä¸¦åˆ—åŒ–åˆ©å¾—ãŒå¤§ãã„

#### 7.2.6 å®Ÿè£…ä¾‹: Speculative Decoding in Rust

```rust
// src/speculative_decoding.rs
pub struct SpeculativeDecoder {
    draft_model: Box<dyn Model>,
    target_model: Box<dyn Model>,
    k: usize,  // speculation depth
}

impl SpeculativeDecoder {
    pub fn decode(
        &self,
        prompt: &[TokenId],
        max_new_tokens: usize,
    ) -> Result<Vec<TokenId>> {
        let mut output = prompt.to_vec();
        let mut generated = 0;

        while generated < max_new_tokens {
            // Step 1: Draft k tokens with small model
            let draft_tokens = self.draft_k_tokens(&output, self.k)?;

            // Step 2: Verify with target model (1 forward pass)
            let (accepted, rejected_idx) = self.verify_tokens(
                &output,
                &draft_tokens,
            )?;

            // Step 3: Accept/Reject
            output.extend_from_slice(&accepted);
            generated += accepted.len();

            // If all rejected, sample 1 token from target
            if accepted.is_empty() {
                let token = self.target_model.sample_next(&output)?;
                output.push(token);
                generated += 1;
            }
        }

        Ok(output)
    }

    fn draft_k_tokens(
        &self,
        context: &[TokenId],
        k: usize,
    ) -> Result<Vec<TokenId>> {
        let mut tokens = Vec::with_capacity(k);
        let mut ctx = context.to_vec();

        for _ in 0..k {
            let logits = self.draft_model.forward(&ctx)?;
            let token = sample_from_logits(&logits);
            tokens.push(token);
            ctx.push(token);
        }

        Ok(tokens)
    }

    fn verify_tokens(
        &self,
        context: &[TokenId],
        draft: &[TokenId],
    ) -> Result<(Vec<TokenId>, Option<usize>)> {
        // Forward pass with draft tokens (parallel)
        let mut ctx = context.to_vec();
        ctx.extend_from_slice(draft);

        let logits_seq = self.target_model.forward_all(&ctx)?;

        let mut accepted = Vec::new();

        for (i, &draft_token) in draft.iter().enumerate() {
            let pos = context.len() + i;
            let target_prob = softmax_prob(&logits_seq[pos], draft_token);
            let draft_prob = self.draft_model.get_prob(
                &ctx[..pos],
                draft_token,
            )?;

            let accept_prob = (target_prob / draft_prob).min(1.0);

            if rand::random::<f32>() < accept_prob {
                accepted.push(draft_token);
            } else {
                return Ok((accepted, Some(i)));
            }
        }

        Ok((accepted, None))
    }
}
```

### 7.3 Continuous Batching â€” å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºæœ€é©åŒ–

#### 7.3.1 å‹•æ©Ÿ: å›ºå®šãƒãƒƒãƒã®éåŠ¹ç‡æ€§

å¾“æ¥ã®ãƒãƒƒãƒå‡¦ç†: å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ

$$
\text{Latency}_\text{batch} = \max_{i \in \text{batch}} \text{Length}_i
$$

**å•é¡Œ**: 1ã¤ã®é•·ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå…¨ä½“ã‚’é…å»¶ã€‚

Continuous Batching [^28] (Orca, 2022): ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’**å‹•çš„ã«è¿½åŠ /å‰Šé™¤**ã€‚

#### 7.3.2 ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**ã‚¹ãƒ†ãƒƒãƒ—1**: å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®çŠ¶æ…‹ã‚’ç‹¬ç«‹ç®¡ç†

$$
\text{Batch}_t = \{(x_i, \text{state}_i, \text{done}_i)\}_{i \in \text{active}}
$$

**ã‚¹ãƒ†ãƒƒãƒ—2**: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«å®Œäº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‰Šé™¤ã€æ–°è¦ã‚’è¿½åŠ 

```julia
# Continuous Batching: å¯å¤‰é•·ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‹•çš„ãƒãƒƒãƒç®¡ç†
struct Request
    tokens::Vector{Int}
    done::Bool
end

function continuous_batching!(queue::Vector{Request}, model; max_batch=8, max_steps=1000)
    active = Request[]
    
    for _ in 1:max_steps
        # å®Œäº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‰Šé™¤
        filter!(r -> !r.done, active)
        
        # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰æ–°è¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è£œå……
        while length(active) < max_batch && !isempty(queue)
            push!(active, popfirst!(queue))
        end
        isempty(active) && break
        
        # ãƒãƒƒãƒ forwardï¼ˆä¸¦åˆ—æ¨è«–ï¼‰
        token_seqs = [r.tokens for r in active]
        logits = model(token_seqs)        # shape: [vocab, batch]
        
        # æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€çŠ¶æ…‹æ›´æ–°
        for (r, logit) in zip(active, eachcol(logits))
            next_tok = sample_token(logit)
            push!(r.tokens, next_tok)
            r.done = is_eos(next_tok)
        end
    end
end
```

#### 7.3.3 ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®ç†è«–è§£æ

å›ºå®šãƒãƒƒãƒ:

$$
\text{Throughput}_\text{static} = \frac{B}{\max_i L_i}
$$

$B$: ãƒãƒƒãƒã‚µã‚¤ã‚ºã€$L_i$: ãƒªã‚¯ã‚¨ã‚¹ãƒˆ$i$ã®é•·ã•

Continuous batching:

$$
\text{Throughput}_\text{cont} = \frac{B}{\mathbb{E}[L]}
$$

$\mathbb{E}[L]$: å¹³å‡é•·

**Speedup**:

$$
\frac{\text{Throughput}_\text{cont}}{\text{Throughput}_\text{static}} = \frac{\max_i L_i}{\mathbb{E}[L]}
$$

**æ•°å€¤ä¾‹**: $L \sim [10, 500]$ å‡ç­‰åˆ†å¸ƒ

$$
\frac{500}{255} \approx 1.96\text{x}
$$

ç´„2å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã€‚

#### 7.3.4 å®Ÿé¨“çµæœ: Orca (Continuous Batching)

å®Ÿé¨“è¨­å®š: GPT-3 13Bã€ShareGPT datasetã€A100 GPU

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | Static Batching | Continuous Batching |
|:----------|:---------------|:-------------------|
| ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (req/s) | 1.2 | 3.8 (+217%) |
| P50 ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (s) | 8.5 | 3.2 (-62%) |
| P99 ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (s) | 45.2 | 12.1 (-73%) |

**è¦³å¯Ÿ**:

- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ3.2å€å‘ä¸Š
- P99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆæœ€æ‚ªã‚±ãƒ¼ã‚¹ï¼‰ãŒå¤§å¹…æ”¹å–„
- GPUåˆ©ç”¨ç‡: 45% â†’ 82%

### 7.4 PagedAttention â€” KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ¡ãƒ¢ãƒªç®¡ç†

#### 7.4.1 å‹•æ©Ÿ: KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ–­ç‰‡åŒ–

Transformeræ¨è«–ã§ã¯ã€éå»ã®Key/Valueã‚’ä¿å­˜ï¼ˆKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰:

$$
\text{KV cache} = \{(K_1, V_1), (K_2, V_2), \dots, (K_T, V_T)\}
$$

å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ»å„ãƒ˜ãƒƒãƒ‰ã§ä¿æŒ â†’ ãƒ¡ãƒ¢ãƒªå¤§é‡æ¶ˆè²»ã€‚

**å•é¡Œ**: å¯å¤‰é•·å…¥åŠ›ã§ãƒ¡ãƒ¢ãƒªãŒæ–­ç‰‡åŒ– â†’ å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„ã€‚

PagedAttention [^29] (vLLM, 2023): KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’**ãƒšãƒ¼ã‚¸å˜ä½**ã§ç®¡ç†ï¼ˆOSã®ä»®æƒ³ãƒ¡ãƒ¢ãƒªã¨åŒã˜ç™ºæƒ³ï¼‰ã€‚

#### 7.4.2 ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**ã‚¹ãƒ†ãƒƒãƒ—1**: KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒšãƒ¼ã‚¸ã«åˆ†å‰²

$$
\text{Page size} = P \quad \text{(e.g., 16 tokens)}
$$

å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®KVã¯è¤‡æ•°ãƒšãƒ¼ã‚¸ã«åˆ†æ•£:

$$
\text{KV}_i = [\text{Page}_{i,1}, \text{Page}_{i,2}, \dots]
$$

**ã‚¹ãƒ†ãƒƒãƒ—2**: ãƒšãƒ¼ã‚¸ãƒ†ãƒ¼ãƒ–ãƒ«ã§ç®¡ç†

```rust
struct PageTable {
    logical_to_physical: HashMap<(RequestId, PageId), PhysicalPageId>,
    free_pages: Vec<PhysicalPageId>,
}
```

**ã‚¹ãƒ†ãƒƒãƒ—3**: Attentionè¨ˆç®—æ™‚ã€ãƒšãƒ¼ã‚¸ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‚ç…§

```rust
// PagedAttention: ãƒšãƒ¼ã‚¸ãƒ†ãƒ¼ãƒ–ãƒ«çµŒç”±ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥Attention
// æ•°å¼: Attention(q, K, V) = softmax(qKáµ€/âˆšd_k) V (ãƒšãƒ¼ã‚¸åˆ†æ•£)
fn paged_attention(
    query: &[f32],           // [d_k]
    page_table: &HashMap<u32, u32>,
    physical_memory: &[Vec<(Vec<f32>, Vec<f32>)>],  // [page][token](K, V)
    d_k: usize,
) -> Vec<f32> {
    let scale = 1.0 / (d_k as f32).sqrt();
    let mut output = vec![0.0f32; query.len()];

    for &phys_id in page_table.values() {
        for (k, v) in &physical_memory[phys_id as usize] {
            // ã‚¹ã‚³ã‚¢: qãƒ»káµ€ / âˆšd_k
            let score: f32 = query.iter().zip(k).map(|(q, k)| q * k).sum::<f32>() * scale;
            let weight = score.exp();  // softmaxåˆ†å­ï¼ˆå¾Œã§æ­£è¦åŒ–ï¼‰
            // é‡ã¿ä»˜ãVåŠ ç®—
            output.iter_mut().zip(v).for_each(|(o, vi)| *o += weight * vi);
        }
    }
    output
}
```

#### 7.4.3 ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è¨ˆç®—

**Before (Naive KVã‚­ãƒ£ãƒƒã‚·ãƒ¥)**:

å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«æœ€å¤§é•· $L_{\max}$ ã‚’äº‹å‰å‰²ã‚Šå½“ã¦:

$$
\text{Memory} = B \times L_{\max} \times 2 \times d \times \text{num\_layers} \times \text{num\_heads}
$$

å¹³å‡é•·ãŒ $\mathbb{E}[L] \ll L_{\max}$ ãªã‚‰ã€å¤§é‡ã®ç„¡é§„ã€‚

**After (PagedAttention)**:

å®Ÿéš›ã«ä½¿ç”¨ã—ãŸãƒšãƒ¼ã‚¸æ•°ã®ã¿:

$$
\text{Memory} = \sum_{i=1}^B \lceil L_i / P \rceil \times P \times 2d \times \text{num\_layers} \times \text{num\_heads}
$$

**å‰Šæ¸›ç‡**:

$$
\frac{B \times L_{\max}}{\sum_i \lceil L_i / P \rceil \times P} \approx \frac{L_{\max}}{\mathbb{E}[L]}
$$

$L_{\max} = 2048, \mathbb{E}[L] = 512$ ãªã‚‰**4å€å‰Šæ¸›**ã€‚

#### 7.4.4 å®Ÿé¨“çµæœ: vLLM (PagedAttention)

å®Ÿé¨“è¨­å®š: LLaMA-13Bã€ShareGPTã€A100 40GB

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | HuggingFace Transformers | vLLM (PagedAttention) |
|:----------|:------------------------|:---------------------|
| ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (req/s) | 0.9 | 24.2 (+2589%) |
| æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º | 8 | 256 (+3100%) |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ | 38% (fragmented) | 94% |

**è¦³å¯Ÿ**:

- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**26å€**å‘ä¸Š
- ãƒãƒƒãƒã‚µã‚¤ã‚º32å€ï¼ˆãƒ¡ãƒ¢ãƒªæ–­ç‰‡åŒ–è§£æ¶ˆï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡2.5å€å‘ä¸Šï¼ˆ38% â†’ 94%ï¼‰

> **Note:** **é€²æ—: 90% å®Œäº†** Productionæœ€é©åŒ–ã®æœ€æ–°å‹•å‘ï¼ˆFlashAttention-3ã€Speculative Decodingã€Continuous Batchingã€PagedAttentionï¼‰ã‚’è¿½åŠ ã€‚å®Ÿè£…å®Œäº†ã€‚

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

[^25]: Tri Dao. "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning". arXiv:2307.08691, 2023.
[^26]: Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao. "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision". arXiv:2407.08608, 2024.
[^27]: Charlie Chen et al. "Accelerating Large Language Model Decoding with Speculative Sampling". arXiv:2302.01318, 2023.
[^28]: Gyeong-In Yu et al. "Orca: A Distributed Serving System for Transformer-Based Generative Models". OSDI 2022.
[^29]: Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica. "Efficient Memory Management for Large Language Model Serving with PagedAttention". SOSP 2023 / arXiv:2309.06180.



