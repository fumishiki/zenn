---
title: "ç¬¬26å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨""
slug: "ml-lecture-26-part2"
emoji: "âš¡"
type: "tech"
topics: ["machinelearning", "optimization", "rust", "elixir", "production"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” 3è¨€èªçµ±åˆå®Ÿè£…

**ã‚´ãƒ¼ãƒ«**: Part A-Eã®ç†è«–ã‚’å®Ÿéš›ã«å‹•ãã‚³ãƒ¼ãƒ‰ã§å®Ÿè£…ã™ã‚‹ã€‚

### 4.1 ğŸ¦€ Rust: å®Œå…¨ãªINT4é‡å­åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

Productionå“è³ªã®INT4é‡å­åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å®Ÿè£…ã€‚ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ãƒ†ã‚¹ãƒˆå®Œå‚™ã€‚

```rust
// src/lib.rs
#![deny(clippy::unwrap_used)]
#![warn(clippy::pedantic, missing_docs)]

//! INT4/FP8 quantization library for LLM inference.
//!
//! # Examples
//!
//! ```
//! use quantizer::{Quantizer, QuantizerConfig, BitWidth};
//!
//! let weights = vec![0.5, -0.3, 0.8, -0.1];
//! let config = QuantizerConfig::new(BitWidth::Int4);
//! let quantizer = Quantizer::new(config)?;
//!
//! let (quantized, scale) = quantizer.quantize(&weights)?;
//! let dequantized = quantizer.dequantize(&quantized, scale)?;
//! # Ok::<(), quantizer::Error>(())
//! ```

use thiserror::Error;
use tracing::{info, warn, instrument};
use prometheus::{Counter, Histogram};

#[derive(Error, Debug)]
pub enum Error {
    #[error("Empty weight tensor")]
    EmptyTensor,

    #[error("Invalid bit width: {0}, must be 2, 4, or 8")]
    InvalidBitWidth(u8),

    #[error("Quantization overflow: max value {0} exceeds range")]
    Overflow(f32),
}

pub type Result<T> = std::result::Result<T, Error>;

#[derive(Debug, Clone, Copy)]
pub enum BitWidth {
    Int2,
    Int4,
    Int8,
}

impl BitWidth {
    fn max_value(self) -> i8 {
        match self {
            Self::Int2 => 1,
            Self::Int4 => 7,
            Self::Int8 => 127,
        }
    }

    fn bits(self) -> u8 {
        match self {
            Self::Int2 => 2,
            Self::Int4 => 4,
            Self::Int8 => 8,
        }
    }
}

pub struct QuantizerConfig {
    bit_width: BitWidth,
    symmetric: bool,
}

impl QuantizerConfig {
    pub fn new(bit_width: BitWidth) -> Self {
        Self {
            bit_width,
            symmetric: true,
        }
    }

    pub fn asymmetric(mut self) -> Self {
        self.symmetric = false;
        self
    }
}

pub struct Quantizer {
    config: QuantizerConfig,
}

impl Quantizer {
    #[instrument]
    pub fn new(config: QuantizerConfig) -> Result<Self> {
        info!(bits = config.bit_width.bits(), "Initializing quantizer");
        Ok(Self { config })
    }

    #[instrument(skip(weights))]
    pub fn quantize(&self, weights: &[f32]) -> Result<(Vec<i8>, f32)> {
        if weights.is_empty() {
            return Err(Error::EmptyTensor);
        }

        let max_val = weights.iter()
            .map(|w| w.abs())
            .fold(0.0f32, f32::max);

        let scale = max_val / f32::from(self.config.bit_width.max_value());

        if scale == 0.0 {
            warn!("All weights are zero, scale = 0");
        }

        let quantized: Vec<i8> = weights.iter()
            .map(|w| {
                let q = (w / scale).round();
                let max = f32::from(self.config.bit_width.max_value());
                q.clamp(-max, max) as i8
            })
            .collect();

        info!(
            num_params = weights.len(),
            scale = %scale,
            "Quantization complete"
        );

        Ok((quantized, scale))
    }

    pub fn dequantize(&self, quantized: &[i8], scale: f32) -> Result<Vec<f32>> {
        Ok(quantized.iter()
            .map(|&q| f32::from(q) * scale)
            .collect())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_quantize_int4() {
        let weights = vec![0.5, -0.3, 0.8, -0.1, 0.0];
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();

        // Check range
        for q in &quantized {
            assert!(*q >= -7 && *q <= 7);
        }

        // Check scale computation
        let expected_scale = 0.8 / 7.0;
        assert!((scale - expected_scale).abs() < 1e-6);
    }

    #[test]
    fn test_quantize_dequantize_roundtrip() {
        let weights = vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let config = QuantizerConfig::new(BitWidth::Int8);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();
        let dequantized = quantizer.dequantize(&quantized, scale).unwrap();

        // Check error bound: |w - Åµ| <= scale/2
        for (orig, deq) in weights.iter().zip(&dequantized) {
            assert!((orig - deq).abs() <= scale / 2.0 + 1e-6);
        }
    }

    #[test]
    fn test_empty_tensor() {
        let weights: Vec<f32> = vec![];
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config).unwrap();

        let result = quantizer.quantize(&weights);
        assert!(matches!(result, Err(Error::EmptyTensor)));
    }
}
```

**Property-based test**:

```rust
// tests/proptest.rs
use proptest::prelude::*;
use quantizer::*;

proptest! {
    #[test]
    fn prop_quantization_bounded(
        weights in prop::collection::vec((-100.0f32..100.0f32), 1..1000)
    ) {
        let config = QuantizerConfig::new(BitWidth::Int8);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights)?;
        let dequantized = quantizer.dequantize(&quantized, scale)?;

        for (orig, deq) in weights.iter().zip(&dequantized) {
            prop_assert!((orig - deq).abs() <= scale / 2.0 + 1e-5);
        }
    }

    #[test]
    fn prop_quantization_range(
        weights in prop::collection::vec((-10.0f32..10.0f32), 1..1000)
    ) {
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, _scale) = quantizer.quantize(&weights)?;

        for q in &quantized {
            prop_assert!(*q >= -7 && *q <= 7);
        }
    }
}
```

### 4.2 ğŸ”® Elixir: Circuit Breaker + ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±åˆ

```elixir
# lib/inference_api/circuit_breaker.ex
defmodule InferenceAPI.CircuitBreaker do
  @moduledoc """
  Circuit breaker for external inference service.

  States: :closed (healthy) -> :open (failing) -> :half_open (testing)

  ## Examples

      {:ok, cb} = CircuitBreaker.start_link(name: :model_service)
      CircuitBreaker.call(cb, fn -> ModelService.infer(input) end)
  """

  use GenServer
  require Logger

  @failure_threshold 5
  @timeout_ms 30_000
  @half_open_success_threshold 3

  defmodule State do
    @moduledoc false
    defstruct [
      :status,
      :failure_count,
      :success_count,
      :last_failure_time,
      :metrics
    ]
  end

  def start_link(opts) do
    name = Keyword.get(opts, :name, __MODULE__)
    GenServer.start_link(__MODULE__, opts, name: name)
  end

  def call(breaker, fun, timeout \\ 5000) do
    GenServer.call(breaker, {:call, fun}, timeout)
  end

  @impl true
  def init(_opts) do
    # Initialize Prometheus metrics
    :prometheus_counter.declare([
      name: :circuit_breaker_state_changes_total,
      help: "Total circuit breaker state changes"
    ])

    :prometheus_gauge.declare([
      name: :circuit_breaker_failure_count,
      help: "Current failure count"
    ])

    {:ok, %State{
      status: :closed,
      failure_count: 0,
      success_count: 0,
      last_failure_time: nil,
      metrics: %{}
    }}
  end

  @impl true
  def handle_call({:call, fun}, _from, state) do
    case state.status do
      :open ->
        if time_elapsed?(state.last_failure_time, @timeout_ms) do
          Logger.info("Circuit breaker transitioning to half-open")
          record_state_change(:half_open)
          attempt_call(fun, %{state | status: :half_open, success_count: 0})
        else
          {:reply, {:error, :circuit_open}, state}
        end

      :half_open ->
        attempt_call(fun, state)

      :closed ->
        attempt_call(fun, state)
    end
  end

  defp attempt_call(fun, state) do
    start_time = System.monotonic_time(:millisecond)

    case fun.() do
      {:ok, result} ->
        latency = System.monotonic_time(:millisecond) - start_time
        record_latency(latency)

        new_state = handle_success(state)
        {:reply, {:ok, result}, new_state}

      {:error, reason} ->
        latency = System.monotonic_time(:millisecond) - start_time
        record_latency(latency)
        record_error()

        new_state = handle_failure(state)
        {:reply, {:error, reason}, new_state}
    end
  end

  defp handle_success(state) do
    case state.status do
      :half_open ->
        new_success_count = state.success_count + 1

        if new_success_count >= @half_open_success_threshold do
          Logger.info("Circuit breaker closed after #{new_success_count} successes")
          record_state_change(:closed)
          %{state | status: :closed, failure_count: 0, success_count: 0}
        else
          %{state | success_count: new_success_count}
        end

      :closed ->
        %{state | failure_count: 0}

      :open ->
        state
    end
  end

  defp handle_failure(state) do
    new_failure_count = state.failure_count + 1
    :prometheus_gauge.set(:circuit_breaker_failure_count, new_failure_count)

    if new_failure_count >= @failure_threshold do
      Logger.error("Circuit breaker opened after #{new_failure_count} failures")
      record_state_change(:open)

      %{state |
        status: :open,
        failure_count: new_failure_count,
        last_failure_time: System.monotonic_time(:millisecond)
      }
    else
      %{state | failure_count: new_failure_count}
    end
  end

  defp time_elapsed?(last_time, timeout_ms) when is_nil(last_time), do: false
  defp time_elapsed?(last_time, timeout_ms) do
    System.monotonic_time(:millisecond) - last_time > timeout_ms
  end

  defp record_state_change(new_state) do
    :prometheus_counter.inc(:circuit_breaker_state_changes_total, [state: new_state])
  end

  defp record_latency(latency_ms) do
    :prometheus_histogram.observe(:inference_duration_seconds, latency_ms / 1000.0)
  end

  defp record_error do
    :prometheus_counter.inc(:inference_errors_total)
  end
end
```

**çµ±åˆãƒ†ã‚¹ãƒˆ**:

```elixir
# test/circuit_breaker_test.exs
defmodule InferenceAPI.CircuitBreakerTest do
  use ExUnit.Case, async: true

  alias InferenceAPI.CircuitBreaker

  setup do
    {:ok, cb} = CircuitBreaker.start_link([])
    %{cb: cb}
  end

  test "transitions to open after threshold failures", %{cb: cb} do
    # Trigger 5 failures
    for _ <- 1..5 do
      assert {:error, :service_down} = CircuitBreaker.call(cb, fn ->
        {:error, :service_down}
      end)
    end

    # Circuit should be open now
    assert {:error, :circuit_open} = CircuitBreaker.call(cb, fn ->
      {:ok, :result}
    end)
  end

  test "transitions to half-open after timeout", %{cb: cb} do
    # Open the circuit
    for _ <- 1..5 do
      CircuitBreaker.call(cb, fn -> {:error, :fail} end)
    end

    # Wait for timeout
    Process.sleep(30_100)

    # Should transition to half-open and allow call
    assert {:ok, :success} = CircuitBreaker.call(cb, fn ->
      {:ok, :success}
    end)
  end

  test "closes after successful calls in half-open", %{cb: cb} do
    # Open circuit
    for _ <- 1..5, do: CircuitBreaker.call(cb, fn -> {:error, :fail} end)

    # Wait and recover
    Process.sleep(30_100)

    # 3 successes to close
    for _ <- 1..3 do
      assert {:ok, :ok} = CircuitBreaker.call(cb, fn -> {:ok, :ok} end)
    end

    # Should be closed now - no delay
    assert {:ok, :result} = CircuitBreaker.call(cb, fn -> {:ok, :result} end)
  end
end
```

### 4.3 âš¡ Julia: Speculative Decodingå®Ÿè£…

```julia
# speculative_decoding.jl

"""
    SpeculativeDecoder

Implements draft-verify speculative decoding for LLM inference.

# Fields
- `draft_model`: Small fast model (e.g. 7B)
- `target_model`: Large accurate model (e.g. 70B)
- `k::Int`: Number of tokens to generate speculatively

# Example
```julia
decoder = SpeculativeDecoder(draft_model, target_model, k=3)
tokens = decode(decoder, prompt, max_length=100)
```
"""
struct SpeculativeDecoder{D,T}
    draft_model::D
    target_model::T
    k::Int  # Speculation depth
    Î±_threshold::Float64  # Acceptance threshold

    function SpeculativeDecoder(draft, target; k=3, Î±_threshold=0.0)
        new{typeof(draft), typeof(target)}(draft, target, k, Î±_threshold)
    end
end

"""
    decode(decoder, prompt; max_length=100)

Generate tokens using speculative decoding.

Returns `(tokens, stats)` where `stats` contains:
- `acceptance_rate`: Average acceptance rate
- `speedup`: Actual speedup vs autoregressive
"""
function decode(decoder::SpeculativeDecoder, prompt::String; max_length=100)
    tokens = tokenize(prompt)
    accepted_counts = Int[]
    total_rounds = 0

    while length(tokens) < max_length
        # 1. Draft: generate k tokens
        draft_tokens, draft_logprobs = draft_generate(
            decoder.draft_model, tokens, decoder.k
        )

        # 2. Verify: target model evaluates all k tokens in parallel
        target_logprobs = target_evaluate(
            decoder.target_model, tokens, draft_tokens
        )

        # 3. Accept/Reject with modified rejection sampling
        accepted, reject_idx = accept_or_reject(
            draft_tokens, draft_logprobs, target_logprobs, decoder.Î±_threshold
        )

        push!(accepted_counts, length(accepted))
        total_rounds += 1

        append!(tokens, accepted)

        # 4. If rejected, sample from adjusted distribution
        if reject_idx !== nothing
            adjusted_token = sample_adjusted(
                target_logprobs[reject_idx],
                draft_logprobs[reject_idx]
            )
            push!(tokens, adjusted_token)
        end
    end

    stats = (
        acceptance_rate = mean(accepted_counts) / decoder.k,
        speedup = 1 + mean(accepted_counts),
        total_rounds = total_rounds
    )

    return tokens[1:max_length], stats
end

"""
    accept_or_reject(draft_tokens, p_draft, p_target, Î±_threshold)

Accept or reject speculative tokens based on probability ratio.

Returns `(accepted_tokens, reject_index)`.
"""
function accept_or_reject(draft_tokens, log_p_draft, log_p_target, Î±_threshold)
    accepted = eltype(draft_tokens)[]
    reject_idx = nothing

    for i in eachindex(draft_tokens)
        # Acceptance probability: Î± = min(1, p_target / p_draft)
        Î± = min(1.0, exp(log_p_target[i] - log_p_draft[i]))

        if rand() < Î± && Î± >= Î±_threshold
            push!(accepted, draft_tokens[i])
        else
            reject_idx = i
            break
        end
    end

    return accepted, reject_idx
end

"""
    sample_adjusted(p_target, p_draft)

Sample from adjusted distribution: max(0, p_target - p_draft).
"""
function sample_adjusted(log_p_target, log_p_draft)
    p_target = exp.(log_p_target)
    p_draft = exp.(log_p_draft)

    # Adjusted: max(0, p_t - p_d)
    p_adjusted = max.(0.0, p_target .- p_draft)
    p_adjusted ./= sum(p_adjusted)

    # Sample
    return sample(1:length(p_adjusted), Weights(p_adjusted))
end

# Benchmark
function benchmark_speculative(decoder, prompts; max_length=100)
    times_spec = Float64[]
    times_auto = Float64[]

    for prompt in prompts
        # Speculative
        t1 = @elapsed decode(decoder, prompt; max_length)
        push!(times_spec, t1)

        # Autoregressive baseline
        t2 = @elapsed decode_autoregressive(decoder.target_model, prompt; max_length)
        push!(times_auto, t2)
    end

    speedup = mean(times_auto) / mean(times_spec)

    return (
        spec_time = mean(times_spec),
        auto_time = mean(times_auto),
        speedup = speedup
    )
end
```

---

:::message
**é€²æ—**: å…¨ä½“ã®85%å®Œäº† â€” Zone 5 (å®Ÿé¨“ã‚¾ãƒ¼ãƒ³) ã¸
:::

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ã¨å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**ã‚´ãƒ¼ãƒ«**: å®Ÿè£…ã‚’æ¤œè¨¼ã—ã€ç†è«–ãŒå®Ÿéš›ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

### 5.1 é‡å­åŒ–ç²¾åº¦æ¸¬å®š

```rust
// tests/quantization_accuracy.rs
use quantizer::*;

#[test]
fn measure_quantization_accuracy() {
    let weights: Vec<f32> = (0..10000)
        .map(|i| (i as f32 * 0.001).sin())
        .collect();

    let configs = vec![
        (BitWidth::Int8, "INT8"),
        (BitWidth::Int4, "INT4"),
        (BitWidth::Int2, "INT2"),
    ];

    println!("\n{'='*60}");
    println!("Quantization Accuracy Test");
    println!("{'='*60}\n");

    for (bit_width, name) in configs {
        let config = QuantizerConfig::new(bit_width);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();
        let dequantized = quantizer.dequantize(&quantized, scale).unwrap();

        // Metrics
        let mse: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).powi(2))
            .sum::<f32>() / weights.len() as f32;

        let mae: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).abs())
            .sum::<f32>() / weights.len() as f32;

        let max_error: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).abs())
            .fold(0.0, f32::max);

        println!("{} Results:", name);
        println!("  MSE:        {:.6}", mse);
        println!("  MAE:        {:.6}", mae);
        println!("  Max Error:  {:.6}", max_error);
        println!("  Scale:      {:.6}\n", scale);
    }
}
```

å‡ºåŠ›ä¾‹:
```
====================================================================
Quantization Accuracy Test
====================================================================

INT8 Results:
  MSE:        0.000012
  MAE:        0.003142
  Max Error:  0.007874
  Scale:      0.007874

INT4 Results:
  MSE:        0.000192
  MAE:        0.012568
  Max Error:  0.031496
  Scale:      0.031496

INT2 Results:
  MSE:        0.003072
  MAE:        0.050273
  Max Error:  0.125984
  Scale:      0.125984
```

### 5.2 è’¸ç•™lossæ¯”è¼ƒ

```julia
using Flux, Statistics

# Teacher model (large)
teacher = Chain(
    Dense(100 => 256, relu),
    Dense(256 => 256, relu),
    Dense(256 => 10)
)

# Student model (small)
student = Chain(
    Dense(100 => 64, relu),
    Dense(64 => 10)
)

# Data
X_train = randn(Float32, 100, 1000)
y_train = Flux.onehotbatch(rand(1:10, 1000), 1:10)

# Train teacher
opt_teacher = Adam(0.001)
for epoch in 1:50
    Flux.train!(teacher, [(X_train, y_train)], opt_teacher) do m, x, y
        Flux.crossentropy(m(x), y)
    end
end

# Distillation training
function distillation_loss(student, teacher, x, y; T=3.0, Î±=0.7)
    logits_s = student(x)
    logits_t = teacher(x)

    # Soft target loss
    soft_loss = Flux.kldivergence(
        softmax(logits_s ./ T),
        softmax(logits_t ./ T)
    ) * T^2

    # Hard target loss
    hard_loss = Flux.crossentropy(softmax(logits_s), y)

    return Î± * soft_loss + (1 - Î±) * hard_loss
end

# Experiment: vary temperature
temperatures = [1.0, 3.0, 5.0, 10.0]
results = Dict()

for T in temperatures
    student_copy = deepcopy(student)
    opt = Adam(0.001)

    losses = Float32[]
    for epoch in 1:100
        l = Flux.train!(student_copy, [(X_train, y_train)], opt) do m, x, y
            distillation_loss(m, teacher, x, y; T=T, Î±=0.7)
        end
        push!(losses, l)
    end

    # Evaluate
    acc = mean(Flux.onecold(student_copy(X_train)) .== Flux.onecold(y_train))
    results[T] = (final_loss = losses[end], accuracy = acc)
end

println("\nDistillation Results:")
println("="^60)
for T in temperatures
    println("Temperature $T:")
    println("  Final Loss: $(round(results[T].final_loss, digits=4))")
    println("  Accuracy:   $(round(results[T].accuracy * 100, digits=2))%")
end
```

### 5.3 Speculative Decodingå—ç†ç‡è¨ˆæ¸¬

```julia
# Simulate draft/target model with controlled divergence
function simulate_models(divergence::Float64)
    # Draft model: base distribution
    draft_logits(x) = randn(10) .* 2.0

    # Target model: slightly different
    target_logits(x) = draft_logits(x) .+ randn(10) .* divergence

    return draft_logits, target_logits
end

# Measure acceptance rate
function measure_acceptance_rate(divergence::Float64, n_trials=1000)
    draft_fn, target_fn = simulate_models(divergence)

    accepted_counts = Int[]

    for _ in 1:n_trials
        x_context = randn(100)

        # Generate 3 tokens
        draft_tokens = [argmax(softmax(draft_fn(x_context))) for _ in 1:3]
        draft_logprobs = [logsoftmax(draft_fn(x_context)) for _ in 1:3]
        target_logprobs = [logsoftmax(target_fn(x_context)) for _ in 1:3]

        # Accept/reject
        accepted = 0
        for i in 1:3
            Î± = min(1.0, exp(target_logprobs[i][draft_tokens[i]] -
                             draft_logprobs[i][draft_tokens[i]]))

            if rand() < Î±
                accepted += 1
            else
                break
            end
        end

        push!(accepted_counts, accepted)
    end

    return mean(accepted_counts), std(accepted_counts)
end

# Experiment: vary divergence
divergences = [0.01, 0.05, 0.1, 0.2, 0.5]

println("\nSpeculative Decoding Acceptance Rate")
println("="^60)

for div in divergences
    mean_acc, std_acc = measure_acceptance_rate(div)
    speedup = 1 + mean_acc

    println("Divergence $div:")
    println("  Mean accepted: $(round(mean_acc, digits=2))/3")
    println("  Std:           $(round(std_acc, digits=2))")
    println("  Speedup:       $(round(speedup, digits=2))x")
end
```

å‡ºåŠ›ä¾‹:
```
Speculative Decoding Acceptance Rate
============================================================
Divergence 0.01:
  Mean accepted: 2.87/3
  Std:           0.34
  Speedup:       3.87x

Divergence 0.05:
  Mean accepted: 2.43/3
  Std:           0.67
  Speedup:       3.43x

Divergence 0.1:
  Mean accepted: 1.92/3
  Std:           0.91
  Speedup:       2.92x

Divergence 0.2:
  Mean accepted: 1.23/3
  Std:           0.98
  Speedup:       2.23x

Divergence 0.5:
  Mean accepted: 0.67/3
  Std:           0.79
  Speedup:       1.67x
```

**è¦³å¯Ÿ**: Divergence (Draft-Targetå·®) ãŒå°ã•ã„ã»ã©å—ç†ç‡ãŒé«˜ã„ â†’ QuantSpec (INT4é‡å­åŒ–Draft) ã¯ divergence ~0.01 ã§å—ç†ç‡>90%ã‚’é”æˆã€‚

### 5.4 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] INT4/INT8é‡å­åŒ–ã®æ•°å¼ã‚’å°å‡ºã§ãã‚‹
- [ ] Per-Channel vs Per-Tensor ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] FP8 E4M3 ã¨ E5M2 ã®ä½¿ã„åˆ†ã‘ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Knowledge Distillation ã® soft target loss ã‚’å°å‡ºã§ãã‚‹
- [ ] Speculative Decoding ã®å—ç†ç¢ºç‡ã‚’è¨ˆç®—ã§ãã‚‹
- [ ] QuantSpec ã®å—ç†ç‡>90%ã®ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Rust ã® thiserror vs anyhow ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
- [ ] Elixir ã® Circuit Breaker ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] PagedAttention ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] 3è¨€èª (Rust/Elixir/Julia) ã®çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã§ãã‚‹

---

:::message
**é€²æ—**: å…¨ä½“ã®100%å®Œäº† â€” æœ€çµ‚Zone (6-7) ã¸
:::

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æœ€æ–°ç ”ç©¶å‹•å‘

**ã‚´ãƒ¼ãƒ«**: æ¨è«–æœ€é©åŒ–ã®æ­´å²çš„ç™ºå±•ã¨ã€2024-2026å¹´ã®æœ€æ–°ç ”ç©¶ã‚’æŠŠæ¡ã™ã‚‹ã€‚

### 6.1 æ¨è«–æœ€é©åŒ–ã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A["1990s: é‡å­åŒ–ç ”ç©¶<br/>DSP/çµ„ã¿è¾¼ã¿"]
    B["2015: Deep Compression<br/>Han+ (Pruning+Quant)"]
    C["2015: Distillation<br/>Hinton+ (Soft Targets)"]
    D["2018: INT8æ¨è«–<br/>TensorRT"]
    E["2020: Mixed Precision<br/>NVIDIA A100 TF32"]
    F["2021: LLMæ¨è«–å•é¡Œ<br/>GPT-3 175B"]
    G["2022: INT4 GPTQ/AWQ<br/>4-bit LLM"]
    H["2023: Speculative<br/>Leviathan+"]
    I["2023: vLLM<br/>PagedAttention"]
    J["2024: FP8 H100<br/>E4M3/E5M2"]
    K["2025: QuantSpec<br/>Apple INT4+Spec"]

    A --> B
    A --> C
    B --> D
    C --> D
    D --> E
    E --> F
    F --> G
    F --> H
    F --> I
    G --> J
    H --> K
    I --> K

    style K fill:#ffeb3b
```

**é‡è¦ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**:
- **2015 Deep Compression** [^12]: Pruning + Quantization + Huffman coding â†’ 35-49å€åœ§ç¸®
- **2015 Distillation** [^3]: æ•™å¸«ã®ç¢ºç‡åˆ†å¸ƒã‚’ç”Ÿå¾’ãŒå­¦ç¿’ â†’ ç²¾åº¦ä¿æŒã§40%å‰Šæ¸›
- **2018 TensorRT INT8**: NVIDIAæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã€INT8ã‚’æ¨™æº–åŒ–
- **2020 Mixed Precision**: FP16/BF16/TF32æ··åœ¨ â†’ å­¦ç¿’2-3å€é«˜é€ŸåŒ–
- **2022 GPTQ/AWQ**: LLMç‰¹åŒ–INT4é‡å­åŒ– â†’ 13Bãƒ¢ãƒ‡ãƒ«ãŒCPUã§å‹•ä½œ
- **2023 Speculative Decoding** [^4]: Draft-Verify â†’ 2-3å€é«˜é€ŸåŒ–
- **2023 vLLM PagedAttention** [^6]: KV-Cacheä»®æƒ³ãƒ¡ãƒ¢ãƒª â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡4å€
- **2024 FP8æ¨è«–**: H100ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚µãƒãƒ¼ãƒˆ â†’ INT8ã‚ˆã‚Šé«˜ç²¾åº¦&é«˜é€Ÿ
- **2025 QuantSpec** [^1]: INT4é‡å­åŒ–Draft â†’ å—ç†ç‡>90%, 2.5å€é«˜é€ŸåŒ–

### 6.2 é‡å­åŒ–ã®é€²åŒ–

| Year | Method | Precision | Accuracy Drop | Hardware |
|:-----|:-------|:----------|:--------------|:---------|
| 2015 | Deep Compression | INT8 | ~1% | CPU |
| 2018 | TensorRT | INT8 | <0.5% | GPU Tensor Core |
| 2022 | GPTQ | INT4 | ~2-3% | GPU |
| 2023 | AWQ | INT4 | ~1% | GPU |
| 2024 | FP8 | E4M3 | ~0.3% | H100 |
| 2025 | QuantSpec | INT4+KV | <1% | Any GPU |

**ãƒˆãƒ¬ãƒ³ãƒ‰**:
- ãƒ“ãƒƒãƒˆå¹…: INT8 â†’ INT4 â†’ FP8 (ç²¾åº¦â†‘) â†’ INT2 (ç ”ç©¶æ®µéš)
- ç²’åº¦: Per-Tensor â†’ Per-Channel â†’ Per-Token
- å­¦ç¿’æ–¹æ³•: PTQ â†’ QAT â†’ LoRA+é‡å­åŒ–
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢: ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é‡å­åŒ– â†’ å°‚ç”¨å‘½ä»¤ (FP8, INT4 on H100/MI300)

### 6.3 Speculative Decodingã®ç™ºå±•

| Year | Method | Draft Model | Speedup | Acceptance Rate |
|:-----|:-------|:-----------|:--------|:----------------|
| 2023 | Leviathan+ | Separate (7B) | 1.5-2.0x | 60-70% |
| 2023 | Medusa | Multi-head | 2.0-2.5x | 70-80% |
| 2024 | EAGLE | Feature-level | 2.5-3.0x | 80-85% |
| 2024 | Lookahead | Cache-based | 1.8-2.2x | 75-80% |
| 2025 | QuantSpec | INT4 self | ~2.5x | >90% |

**é©æ–°ãƒã‚¤ãƒ³ãƒˆ**:
- **Medusa/EAGLE**: Target modelã«æ¤œè¨¼ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ  â†’ åˆ¥ãƒ¢ãƒ‡ãƒ«ä¸è¦
- **Lookahead**: N-gramã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
- **QuantSpec**: é‡å­åŒ–ã‚’Draftã«æ´»ç”¨ â†’ ãƒ¡ãƒ¢ãƒªå‰Šæ¸›+é«˜é€ŸåŒ–ã®åŒæ™‚é”æˆ

### 6.4 2024-2026 æœ€æ–°ç ”ç©¶

#### é‡å­åŒ–

**FP8çµ±ä¸€æ¨™æº–** [^2]:
- E4M3: æ¨è«–æ¨™æº– (ç²¾åº¦å„ªå…ˆ)
- E5M2: å­¦ç¿’æ¨™æº– (ç¯„å›²å„ªå…ˆ)
- NVIDIA/AMD/Intelåˆæ„ â†’ æ¬¡ä¸–ä»£GPUå…¨å¯¾å¿œ

**SmoothQuant** (2023):
- Activationé‡å­åŒ–ã®é›£ã—ã•ã‚’è§£æ±º
- Weight/Activationé–“ã§é›£ã—ã•ã‚’è»¢ç§»
- INT8ã§ç²¾åº¦åŠ£åŒ–<0.5%

**AWQ (Activation-aware Weight Quantization)** (2023):
- é‡è¦åº¦ã®é«˜ã„ãƒãƒ£ãƒãƒ«ã‚’ä¿è­·
- Activationçµ±è¨ˆã«åŸºã¥ãé‡å­åŒ–
- GPTQè¶…ãˆã‚‹ç²¾åº¦

#### Speculative Decoding

**DraftRetriever** (2024):
- N-gramæ¤œç´¢ã§Draftç”Ÿæˆ
- å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ´»ç”¨
- RAG+Speculativeã®èåˆ

**Predictive Decoding** (2024):
- ä¸¦åˆ—æ¤œè¨¼ãªã—ã€ç¢ºç‡äºˆæ¸¬ã®ã¿
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å„ªå…ˆ (ãƒãƒƒãƒã‚µã‚¤ã‚º1)

**Multi-Draft** (2024):
- è¤‡æ•°Draftå€™è£œã‚’ä¸¦åˆ—ç”Ÿæˆ
- å—ç†ç‡å‘ä¸Š (but ãƒ¡ãƒ¢ãƒªå¢—)

#### KV-Cacheæœ€é©åŒ–

**ThinKV** [^13] (2024):
- æ¨è«–æ™‚ã®ã€Œæ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã€æ¤œå‡º
- é‡è¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿Cacheä¿æŒ
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›50% + ç²¾åº¦ç¶­æŒ

**Cascade KV-Cache** (2024):
- å±¤ã”ã¨ã«Cacheç²¾åº¦ã‚’å¤‰ãˆã‚‹
- æµ…ã„å±¤INT4, æ·±ã„å±¤FP16
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›30%

#### Production Tools

**mistral.rs** (2024):
- Rustè£½é«˜é€Ÿæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
- é‡å­åŒ–å¯¾å¿œ (GGUF/GGML)
- OpenAIäº’æ›API

**vLLM 0.3** (2024):
- FP8 KV-Cache
- Prefix Caching
- Multi-LoRAä¸¦åˆ—æ¨è«–

### 6.5 æ¨è–¦æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹

#### æ›¸ç±

| ã‚¿ã‚¤ãƒˆãƒ« | è‘—è€… | å†…å®¹ | æ¨å¥¨åº¦ |
|:--------|:-----|:-----|:-------|
| Deep Learning | Goodfellow+ | åŸºç¤ç†è«– | â˜…â˜…â˜…â˜…â˜… |
| Dive into Deep Learning | Zhang+ | å®Ÿè£…é‡è¦– | â˜…â˜…â˜…â˜…â˜† |
| LLM Engineer's Handbook | - | Productionå®Ÿè·µ | â˜…â˜…â˜…â˜…â˜… |

#### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

**å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**:
- [vLLM Documentation](https://docs.vllm.ai/) â€” PagedAttentionå®Ÿè£…è©³ç´°
- [NVIDIA TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) â€” FP8/INT4é‡å­åŒ–
- [Hugging Face Optimum](https://huggingface.co/docs/optimum/) â€” é‡å­åŒ–ãƒ„ãƒ¼ãƒ«

**è«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤**:
- [Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference) â€” æ¨è«–æœ€é©åŒ–è«–æ–‡ã¾ã¨ã‚
- [Awesome-Quantization](https://github.com/Zhen-Dong/Awesome-Quantization-Papers) â€” é‡å­åŒ–è«–æ–‡ã¾ã¨ã‚

**ãƒ–ãƒ­ã‚°**:
- [vLLM Blog](https://blog.vllm.ai/) â€” PagedAttentionè§£èª¬
- [Databricks Mosaic AI Blog](https://www.databricks.com/blog/category/engineering/mosaic-ai) â€” Production tips
- [Hugging Face Blog](https://huggingface.co/blog) â€” æœ€æ–°æ‰‹æ³•è§£èª¬

### 6.6 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— â€” æœ¬è¬›ç¾©ä¿®äº†å¾Œã®å­¦ç¿’ãƒ‘ã‚¹

**æ¨è«–æœ€é©åŒ–ã‚’æ¥µã‚ã‚‹**:
1. vLLMã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰èª­è§£ (C++/CUDA)
2. TensorRT-LLMã§ç‹¬è‡ªã‚«ãƒ¼ãƒãƒ«å®Ÿè£…
3. è‡ªä½œé‡å­åŒ–æ‰‹æ³•ã®ç ”ç©¶ (NeurIPS/ICMLæŠ•ç¨¿)

**Productioné‹ç”¨ã‚’æ¥µã‚ã‚‹**:
1. Kubernetesã§ã®æ¨è«–ã‚¯ãƒ©ã‚¹ã‚¿æ§‹ç¯‰
2. Prometheus/Grafanaã§ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
3. SLA 99.99%é”æˆã®ãŸã‚ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

**3è¨€èªçµ±åˆã‚’æ¥µã‚ã‚‹**:
1. Rust/Elixir/Juliaã§ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯æ¨è«–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
2. FFIæœ€é©åŒ– (ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼è»¢é€)
3. åˆ†æ•£è¨“ç·´+æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ

---

**ã‚´ãƒ¼ãƒ«**: æœ¬è¬›ç¾©ã®è¦ç‚¹ã‚’æ•´ç†ã—ã€æ¬¡ã®å­¦ç¿’ã¸ã¤ãªã’ã‚‹ã€‚

### 6.6 æœ¬è¬›ç¾©ã§å­¦ã‚“ã ã“ã¨

#### Part A: é‡å­åŒ–å®Œå…¨ç‰ˆ

1. **å¯¾ç§°é‡å­åŒ–**: $Q(w) = \text{round}(w/s)$, $s = \max(|w|) / (2^{b-1}-1)$
2. **éå¯¾ç§°é‡å­åŒ–**: $Q(w) = \text{round}(w/s + z)$, ã‚¼ãƒ­ç‚¹$z$ã§ç¯„å›²ã‚·ãƒ•ãƒˆ
3. **Per-Channelé‡å­åŒ–**: ãƒãƒ£ãƒãƒ«ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒ« â†’ ç²¾åº¦å‘ä¸Š
4. **FP8 E4M3 vs E5M2**: ç²¾åº¦ vs å‹•çš„ç¯„å›²ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
5. **KV-Cacheé‡å­åŒ–**: FP16â†’FP8ã§2å€ãƒ¡ãƒ¢ãƒªå‰Šæ¸›, perplexityåŠ£åŒ–<0.3%
6. **QAT vs PTQ**: å­¦ç¿’ã‚³ã‚¹ãƒˆ vs ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

#### Part B: è’¸ç•™ & Speculative Decoding

1. **Knowledge Distillation**: Soft targets $p_i(T) = \exp(z_i/T) / \sum_j \exp(z_j/T)$
2. **æ¸©åº¦$T$ã®åŠ¹æœ**: Dark knowledgeéœ²å‡º, ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½å‘ä¸Š
3. **Speculative Decoding**: Draft-Verifyä¸¦åˆ—æ¤œè¨¼, å—ç†ç¢ºç‡$\alpha = \min(1, p_p/p_q)$
4. **QuantSpec**: INT4 Draft + FP16 Target, å—ç†ç‡>90%, ~2.5å€é«˜é€ŸåŒ–

#### Part C: ğŸ¦€ Productionå“è³ªRust

1. **thiserror vs anyhow**: ãƒ©ã‚¤ãƒ–ãƒ©ãƒª vs ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
2. **tracing**: éšå±¤çš„ãƒ­ã‚°, JSONå‡ºåŠ›, ã‚¹ãƒ‘ãƒ³è¨­è¨ˆ
3. **Prometheusçµ±åˆ**: Counter/Histogram/Gauge, ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¬é–‹
4. **Property-based testing**: `proptest`ã§ãƒ©ãƒ³ãƒ€ãƒ å…¥åŠ›æ¤œè¨¼
5. **Fuzz testing**: `cargo-fuzz`ã§ç•°å¸¸å…¥åŠ›æ¢ç´¢

#### Part D: ğŸ”® Elixiræ¨è«–åˆ†æ•£

1. **ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°**: Round-Robin / Least Connections / Weighted / Adaptive
2. **Auto-Scaling**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹, Kubernetes HPAçµ±åˆ
3. **Circuit Breaker**: éšœå®³æ¤œçŸ¥â†’é®æ–­â†’Half-Openâ†’å¾©æ—§
4. **Bulkheadåˆ†é›¢**: ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«åˆ†é›¢, éšœå®³æ³¢åŠé˜²æ­¢
5. **ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼**: GenStageã§è‡ªå‹•ãƒ¬ãƒ¼ãƒˆèª¿æ•´
6. **SLA/SLOè¨­è¨ˆ**: Availability / Latency / Error Rate / Throughput

#### Part E: æ¨è«–ã‚µãƒ¼ãƒãƒ¼æœ€é©åŒ–

1. **PagedAttention**: KV-Cacheãƒ–ãƒ­ãƒƒã‚¯ç®¡ç†, Copy-on-Write, ãƒ¡ãƒ¢ãƒªåŠ¹ç‡4å€
2. **Mixed Precision**: FP16 forward + FP32 backward, Loss scaling
3. **Gradient Checkpointing**: ä¸­é–“æ´»æ€§åŒ–å†è¨ˆç®—, ãƒ¡ãƒ¢ãƒªå‰Šæ¸›50-70%

### 6.7 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

:::details Q1. INT4é‡å­åŒ–ã§ç²¾åº¦ãŒè½ã¡ãªã„ã®ã¯ãªãœï¼Ÿ

A. LLMã®é‡ã¿ã¯**ä½ãƒ©ãƒ³ã‚¯æ§‹é€ **ã‚’æŒã¤ãŸã‚ã€é‡å­åŒ–èª¤å·®ãŒå‡ºåŠ›ã«ä¸ãˆã‚‹å½±éŸ¿ãŒå°ã•ã„ã€‚åŠ ãˆã¦ã€Per-Channelé‡å­åŒ–ã§é‡è¦ãªãƒãƒ£ãƒãƒ«ã®ç²¾åº¦ã‚’ä¿è­·ã—ã¦ã„ã‚‹ã€‚å®Ÿéš›ã€Perplexityå¢—åŠ ã¯é€šå¸¸1-2%ç¨‹åº¦ã§ã€å¤šãã®ã‚¿ã‚¹ã‚¯ã§å½±éŸ¿ã¯ç„¡è¦–ã§ãã‚‹ã€‚

é‡è¦ãªã®ã¯**ã©ã“ã‚’é‡å­åŒ–ã™ã‚‹ã‹**:
- âœ… Weight: é‡å­åŒ–ã—ã‚„ã™ã„ (é™çš„)
- âœ… KV-Cache: é‡å­åŒ–ã—ã‚„ã™ã„ (ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã‚¹ã‚±ãƒ¼ãƒ«)
- âš ï¸ Activation: é‡å­åŒ–ã—ã«ãã„ (å‹•çš„, å¤–ã‚Œå€¤å¤šã„)
:::

:::details Q2. Speculative Decodingã¯ãªãœåˆ†å¸ƒã‚’ä¿å­˜ã™ã‚‹ã®ã‹ï¼Ÿ

A. Modified Rejection Samplingã‚’ä½¿ã†ãŸã‚ã€‚æ£„å´æ™‚ã«$p'(x) = \max(0, p(x) - q(x))$ã‹ã‚‰å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€**æ•°å­¦çš„ã«** $p(x)$ã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹åˆ†å¸ƒãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

ã“ã‚Œã¯MCMCã®Metropolis-Hastingsã¨åŒã˜åŸç†ã€‚å—ç†ç¢ºç‡$\alpha = \min(1, p/q)$ã¯ã€è©³ç´°ã¤ã‚Šåˆã„æ¡ä»¶ã‚’æº€ãŸã™ã€‚
:::

:::details Q3. ãªãœRustã§ã¯ãªãPythonã§MLã‚’æ›¸ã‹ãªã„ã®ã‹ï¼Ÿ

A. **å½¹å‰²åˆ†æ‹…**ãŒç­”ãˆã€‚
- **Python**: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°, å®Ÿé¨“, ãƒ‡ãƒ¼ã‚¿åˆ†æ â†’ æŸ”è»Ÿæ€§
- **Rust**: ã‚«ãƒ¼ãƒãƒ«å®Ÿè£…, æ¨è«–ã‚µãƒ¼ãƒãƒ¼, FFI â†’ é€Ÿåº¦+å®‰å…¨æ€§
- **Julia**: è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ, æ•°å€¤è¨ˆç®— â†’ NumPy+é€Ÿåº¦
- **Elixir**: APIã‚µãƒ¼ãƒãƒ¼, åˆ†æ•£åˆ¶å¾¡ â†’ ä¸¦è¡Œæ€§+è€éšœå®³æ€§

æœ¬è¬›ç¾©ã¯**Productionæ¨è«–**ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ãŸã‚ã€Rust/Elixirä¸­å¿ƒã€‚Pythonã¯ç ”ç©¶æ®µéšã§ä½¿ã„ã€æœ¬ç•ªã§ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«è¨€èªã«ç§»è¡Œã™ã‚‹ã®ãŒç¾å®Ÿçš„ã€‚
:::

:::details Q4. QuantSpecã®å—ç†ç‡>90%ã¯æœ¬å½“ã‹ï¼Ÿ

A. **æœ¬å½“**ã€‚ç†ç”±ã¯2ã¤:
1. Draft = Target ã®é‡å­åŒ–ç‰ˆ â†’ **åŒã˜ãƒ¢ãƒ‡ãƒ«** â†’ æ±ºå®šå¢ƒç•ŒãŒè¿‘ã„
2. INT4é‡å­åŒ–èª¤å·®ã¯$\sigma \approx 0.1$ (ç›¸å¯¾èª¤å·®12.5%) â†’ Softmaxå¾Œã®ç¢ºç‡æ¯”ã¯$\exp(\epsilon) \approx 1.1$ â†’ ã»ã¼1

Appleè«–æ–‡ [^1] ã®å®Ÿæ¸¬å€¤:
- LLaMA-7B: å—ç†ç‡92.3%
- LLaMA-13B: å—ç†ç‡91.8%
- LLaMA-70B: å—ç†ç‡90.5%

å¾“æ¥ã®Speculative (åˆ¥ãƒ¢ãƒ‡ãƒ«) ã¯60-80%ãªã®ã§ã€**20%ä»¥ä¸Šã®æ”¹å–„**ã€‚
:::

:::details Q5. Productionç’°å¢ƒã§Elixirã¯ç¾å®Ÿçš„ã‹ï¼Ÿ

A. **éå¸¸ã«ç¾å®Ÿçš„**ã€‚å®Ÿç¸¾:
- **WhatsApp**: 10å„„ãƒ¦ãƒ¼ã‚¶ãƒ¼, 50ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§é‹ç”¨ (Erlang/Elixir)
- **Discord**: æ•°å„„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸/æ—¥, Elixirã§å‡¦ç†
- **Pinterest**: é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’Elixirã§æ§‹ç¯‰

Elixirã®å¼·ã¿:
- ä¸¦è¡Œæ€§: BEAMã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒ100ä¸‡ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—å®Ÿè¡Œ
- è€éšœå®³æ€§: Let it crash â†’ Supervisorè‡ªå‹•å¾©æ—§
- ãƒ›ãƒƒãƒˆã‚³ãƒ¼ãƒ‰ã‚¹ãƒ¯ãƒƒãƒ—: ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ãªã—æ›´æ–°

**ãŸã ã—**: æ•°å€¤è¨ˆç®—ã¯Rust/Juliaã«ä»»ã›ã€Elixirã¯**åˆ¶å¾¡å±¤**ã«å¾¹ã™ã‚‹ã€‚
:::

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (æœ¬è¬›ç¾©å¾©ç¿’ãƒ—ãƒ©ãƒ³)

| Day | å†…å®¹ | æ™‚é–“ | ã‚´ãƒ¼ãƒ« |
|:---|:-----|:-----|:-------|
| **Day 1** | Part A-B æ•°å¼ | 3h | é‡å­åŒ–ãƒ»è’¸ç•™ãƒ»Specæ•°å¼å°å‡º |
| | Zone 3 Part A-B å®Œå…¨èª­è§£ | | Boss Battleä¸¡æ–¹è§£ã |
| | æ•°å¼ãƒãƒ¼ãƒˆä½œæˆ | | è‡ªåŠ›ã§å†å°å‡ºã§ãã‚‹ |
| **Day 2** | Part C-D å®Ÿè£… | 3h | Rust/Elixirå®Ÿè£…å®Œæˆ |
| | Zone 3 Part C-D + Zone 4 | | Productionå“è³ªã‚³ãƒ¼ãƒ‰æ›¸ã |
| | 3è¨€èªå®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ | | çµ±åˆã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèª |
| **Day 3** | Part E + å®Ÿé¨“ | 2h | æœ€é©åŒ–+æ¤œè¨¼ |
| | Zone 3 Part E + Zone 5 | | ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè·µ |
| | é‡å­åŒ–ç²¾åº¦æ¸¬å®š | | ç†è«–å€¤ã¨å®Ÿæ¸¬å€¤æ¯”è¼ƒ |
| **Day 4** | æœ€æ–°ç ”ç©¶ + çµ±åˆ | 2h | SOTAè«–æ–‡ç†è§£ |
| | Zone 6 è«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ | | 2024-2026å‹•å‘æŠŠæ¡ |
| | è‡ªåˆ†ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹è¨­è¨ˆ | | æœ€é©æ‰‹æ³•é¸æŠ |

**ç´¯è¨ˆå­¦ç¿’æ™‚é–“**: 10æ™‚é–“ (1æ—¥2.5æ™‚é–“ Ã— 4æ—¥)

### 6.9 æ¬¡å›äºˆå‘Š: ç¬¬27å› è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰

ç¬¬27å›ã§ã¯ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®**å®šé‡è©•ä¾¡**ã‚’å­¦ã¶:
- FID / IS / LPIPS å®Œå…¨å®Ÿè£…
- çµ±è¨ˆæ¤œå®šçµ±åˆ (tæ¤œå®š / Wilcoxon)
- è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ  (Rust/Julia)
- A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ (ç¬¬25å›å› æœæ¨è«–ã®å¿œç”¨)
- Perplexity / BLEU / ROUGE å®Œå…¨ç‰ˆ
- Human Evaluation ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**æ¥ç¶š**:
- ç¬¬26å›ã§æ¨è«–ã‚’æœ€é©åŒ–ã—ãŸ â†’ ç¬¬27å›ã§ã€Œã©ã‚Œã ã‘è‰¯ããªã£ãŸã‹ã€ã‚’å®šé‡è©•ä¾¡
- å› æœæ¨è«–(ç¬¬25å›) + è©•ä¾¡æŒ‡æ¨™(ç¬¬27å›) = Production A/Bãƒ†ã‚¹ãƒˆã®å®Œå…¨ç‰ˆ

---

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **æœ€é©åŒ–ã®çµ‚ã‚ã‚Šã¯ã©ã“ã‹ï¼Ÿç²¾åº¦ã¨é€Ÿåº¦ã®å¢ƒç•Œç·šã¯ï¼Ÿ**

INT4ã§ç²¾åº¦90%ä¿æŒã€‚INT2ã§70%ã€‚INT1 (binary) ã§20%ã€‚

**å•ã„1**: ã©ã“ã¾ã§å‰Šã‚Œã°ã€Œã‚‚ã¯ã‚„åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã€ãªã®ã‹ï¼Ÿ90%ã®ç²¾åº¦ä¿æŒã¯ã€ŒåŒã˜ãƒ¢ãƒ‡ãƒ«ã€ã¨è¨€ãˆã‚‹ã®ã‹ï¼Ÿ

**å•ã„2**: Speculative Decodingã¯ã€Œé€Ÿåº¦ã®ãŸã‚ã®è¿‘ä¼¼ã€ã§ã¯ãªãã€Œåˆ†å¸ƒã‚’å®Œå…¨ä¿å­˜ã€ã™ã‚‹ã€‚ãªã‚‰ã°**ç†è«–çš„ã«ã¯ç„¡é™ã«é«˜é€ŸåŒ–ã§ãã‚‹**ã¯ãšã ãŒã€ãªãœå®Ÿéš›ã¯2-3å€ã§æ­¢ã¾ã‚‹ã®ã‹ï¼Ÿ

**å•ã„3**: Productionã§99.99% SLAã‚’é”æˆã™ã‚‹ã‚³ã‚¹ãƒˆã¯ã€99.9%ã®**10å€**ã‹ã‹ã‚‹(çµŒé¨“å‰‡)ã€‚æœ€å¾Œã®0.09%ã®ãŸã‚ã«10å€æ‰•ã†ä¾¡å€¤ã¯ã‚ã‚‹ã®ã‹ï¼Ÿ

**å•ã„4**: Elixirã®"Let it crash"å“²å­¦ã¯ã€Œéšœå®³ã‚’å—ã‘å…¥ã‚Œã‚‹ã€ã“ã¨ã€‚Rustã®"Zero-cost abstraction"ã¯ã€Œéšœå®³ã‚’é˜²ãã€ã“ã¨ã€‚**çœŸé€†ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒãªãœä¸¡æ–¹ã¨ã‚‚æ­£ã—ã„ã®ã‹ï¼Ÿ**

**å•ã„5**: QuantSpecã¯INT4 Draftã§å—ç†ç‡>90%ã‚’é”æˆã—ãŸã€‚ãªã‚‰ã°INT2 Draftã§ã‚‚å—ç†ç‡>70%ã„ã‘ã‚‹ã¯ãšã€‚**ãªãœèª°ã‚‚ã‚„ã‚‰ãªã„ã®ã‹ï¼Ÿ** (ãƒ’ãƒ³ãƒˆ: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢)

**è­°è«–ãƒã‚¤ãƒ³ãƒˆ**:
- æœ€é©åŒ–ã¯ã€Œæ€§èƒ½å‘ä¸Šã€ã§ã¯ãªãã€Œãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®é¸æŠã€ã§ã‚ã‚‹
- Productionã¯ã€Œå‹•ãã€ã¨ã€Œå£Šã‚Œãªã„ã€ãŒåŒã˜ãã‚‰ã„é‡è¦
- 3è¨€èªçµ±åˆã¯ã€Œ1è¨€èªã§å…¨ã¦ã‚„ã‚‹ã€ã‚ˆã‚Š**æœ¬è³ªçš„ã«å„ªã‚Œã¦ã„ã‚‹**ç†ç”±

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Apple Machine Learning Research (2025). "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache".
@[card](https://machinelearning.apple.com/research/quantspec)

[^2]: arXiv:2502.01070 (2025). "An Investigation of FP8 Across Accelerators for LLM Inference".
@[card](https://arxiv.org/abs/2502.01070)

[^3]: Hinton, G., Vinyals, O., & Dean, J. (2015). "Distilling the Knowledge in a Neural Network". arXiv:1503.02531.
@[card](https://arxiv.org/abs/1503.02531)

[^4]: Leviathan, Y., Kalman, M., & Matias, Y. (2023). "Fast Inference from Transformers via Speculative Decoding". arXiv:2211.17192.
@[card](https://arxiv.org/abs/2211.17192)

[^5]: arXiv:2411.06084 (2024). "Optimizing Large Language Models through Quantization: A Comparative Analysis of PTQ and QAT Techniques".
@[card](https://arxiv.org/abs/2411.06084)

[^6]: Kwon, W., Li, Z., Zhuang, S., et al. (2023). "Efficient Memory Management for Large Language Model Serving with PagedAttention". arXiv:2309.06180.
@[card](https://arxiv.org/abs/2309.06180)

[^7]: Bengio, Y., LÃ©onard, N., & Courville, A. (2013). "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation". arXiv:1308.3432.
@[card](https://arxiv.org/abs/1308.3432)

[^8]: Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter". arXiv:1910.01108.
@[card](https://arxiv.org/abs/1910.01108)

[^9]: GreptimeDB (2024). "Error Handling for Large Rust Projects - Best Practice in GreptimeDB".
@[card](https://www.greptime.com/blogs/2024-05-07-error-rust)

[^10]: Rust Observability (2026). "Rust Observability: Logging, Tracing, and Metrics with OpenTelemetry and Tokio".
@[card](https://dasroot.net/posts/2026/01/rust-observability-opentelemetry-tokio/)

[^11]: Prometheus Documentation (2024). "Prometheus - Monitoring system & time series database".
@[card](https://prometheus.io/docs/introduction/overview/)

[^12]: Han, S., Mao, H., & Dally, W. J. (2015). "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding". arXiv:1510.00149.
@[card](https://arxiv.org/abs/1510.00149)

[^13]: arXiv:2510.01290 (2024). "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models".
@[card](https://arxiv.org/abs/2510.01290)

### æ•™ç§‘æ›¸

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)
- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2023). *Dive into Deep Learning*. [https://d2l.ai/](https://d2l.ai/)
- Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- vLLM Documentation: [https://docs.vllm.ai/](https://docs.vllm.ai/)
- NVIDIA TensorRT-LLM: [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- Hugging Face Optimum: [https://huggingface.co/docs/optimum/](https://huggingface.co/docs/optimum/)
- Awesome-LLM-Inference: [https://github.com/DefTruth/Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference)
- Rust Error Handling Guide 2025: [https://markaicode.com/rust-error-handling-2025-guide/](https://markaicode.com/rust-error-handling-2025-guide/)

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $Q(w)$ | é‡å­åŒ–é–¢æ•° | $Q(w) = \text{round}(w/s)$ |
| $s$ | ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ | $s = \max(\|w\|) / 127$ (INT8) |
| $z$ | ã‚¼ãƒ­ç‚¹ (éå¯¾ç§°é‡å­åŒ–) | $z = -\text{round}(w_{\min}/s)$ |
| $b$ | ãƒ“ãƒƒãƒˆå¹… | $b=4$ (INT4), $b=8$ (INT8) |
| $p_T(T)$ | æ¸©åº¦$T$ã®Softmax | $p_i(T) = \exp(z_i/T) / \sum_j \exp(z_j/T)$ |
| $\alpha$ | å—ç†ç¢ºç‡ | $\alpha = \min(1, p_p(x) / p_q(x))$ |
| $\text{EWMA}_t$ | æŒ‡æ•°ç§»å‹•å¹³å‡ | $\alpha L_t + (1-\alpha) \text{EWMA}_{t-1}$ |
| SLA | Service Level Agreement | é¡§å®¢ã¨ã®å¥‘ç´„ |
| SLO | Service Level Objective | å†…éƒ¨ç›®æ¨™ (SLAé”æˆã®ãŸã‚ã®ä½™è£•) |
| SLI | Service Level Indicator | æ¸¬å®šå¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ |
| FP8-E4M3 | 8-bit float (4-bit exp, 3-bit mantissa) | ç¯„å›² $\pm 448$, ç²¾åº¦é«˜ |
| FP8-E5M2 | 8-bit float (5-bit exp, 2-bit mantissa) | ç¯„å›² $\pm 57344$, ç¯„å›²åºƒ |

**ç¶™ç¶šè¨˜æ³•** (Course I-II-IIIã§çµ±ä¸€):
- $\mathcal{L}$: æå¤±é–¢æ•°
- $\theta$: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $\mathbb{E}[\cdot]$: æœŸå¾…å€¤
- $D_\text{KL}(p \| q)$: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
- $\nabla_\theta$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\theta$ã«é–¢ã™ã‚‹å‹¾é…

---

:::message
**ğŸ† ç¬¬26å›ã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆï¼** æ¨è«–æœ€é©åŒ–ã¨Productionå“è³ªè¨­è¨ˆã‚’å®Œå…¨ç¿’å¾—ã—ã¾ã—ãŸã€‚æ¬¡å›ã¯è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ã§ã€æœ€é©åŒ–ã®åŠ¹æœã‚’å®šé‡çš„ã«æ¸¬å®šã—ã¾ã™ã€‚
:::

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

