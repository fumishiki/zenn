---
title: "ç¬¬16å›: SSMç†è«– & Mambaã®å…‹æœ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ¦›"
type: "tech"
topics: ["machinelearning", "deeplearning", "ssm", "julia", "rust"]
published: true
slug: "ml-lecture-16-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

**â† Part1ï¼ˆç†è«–ç·¨ï¼‰**: [ç¬¬16å› Part1](./ml-lecture-16-part1)

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³(45åˆ†) â€” Juliaã¨Rustã§SSMã‚’å‹•ã‹ã™

### 4.1 ç’°å¢ƒæ§‹ç¯‰

#### Juliaç’°å¢ƒ

```bash
# Julia 1.11+ (2025-2026 latest)
curl -fsSL https://install.julialang.org | sh

# Packages
julia -e 'using Pkg; Pkg.add(["LinearAlgebra", "FFTW", "Plots", "DifferentialEquations", "ProgressMeter"])'
```

#### Rustç’°å¢ƒ

```bash
# Rust 1.83+ (2026)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Dependencies in Cargo.toml
[dependencies]
ndarray = "0.16"
ndarray-linalg = "0.17"
rayon = "1.10"
```

### 4.2 é›¢æ•£SSMã®å®Œå…¨å®Ÿè£…(Julia)

```julia
using LinearAlgebra
using FFTW

"""
Discrete SSM module
Implements: h_t = A h_{t-1} + B u_t, y_t = C h_t + D u_t
"""
struct DiscreteSSM
    A::Matrix{Float64}
    B::Vector{Float64}
    C::Vector{Float64}
    D::Float64
end

# Recurrent form (for inference â€” inherently sequential)
function forward_recurrent(ssm::DiscreteSSM, u::Vector{Float64})
    N = length(u)
    h = zeros(Float64, length(ssm.B))
    y = zeros(Float64, N)
    @inbounds for t in 1:N
        h = ssm.A * h + ssm.B * u[t]
        y[t] = dot(ssm.C, h) + ssm.D * u[t]
    end
    return y
end

# Convolutional form (for training)
function forward_convolution(ssm::DiscreteSSM, u::Vector{Float64}, L::Int)
    # Precompute kernel K[k] = C * A^k * B (sequential: each Ai depends on prior)
    d = length(ssm.B)
    K = zeros(Float64, L)
    Ai = Matrix{Float64}(I, d, d)  # A^0
    @inbounds for k in 1:L
        Ai = ssm.A * Ai  # A^k
        K[k] = dot(ssm.C, Ai * ssm.B)
    end

    # FFT convolution (fused into one expression)
    K_pad = [K; zeros(length(u))]
    u_pad = [u; zeros(L)]
    y = real.(ifft(fft(K_pad) .* fft(u_pad)))[1:length(u)]

    return y, K
end

# Example usage
d = 8
A = 0.9 * Matrix{Float64}(I, d, d) + 0.05 * randn(d, d)  # stable matrix
B = randn(Float64, d)
C = randn(Float64, d)
D = 0.0

ssm = DiscreteSSM(A, B, C, D)

u = randn(Float64, 64)
y_rec  = forward_recurrent(ssm, u)
y_conv, K = forward_convolution(ssm, u, 64)

println("Recurrent output (first 5): ", round.(y_rec[1:5], digits=3))
println("Convolution output (first 5): ", round.(y_conv[1:5], digits=3))
println("Max difference: ", maximum(abs.(y_rec .- y_conv)))
```

### 4.3 HiPPO-LegSåˆæœŸåŒ–

```julia
"""
HiPPO-LegS initialization for A and B
Returns matrices with optimal long-range memory properties
"""
function hippo_legs_init(d::Int)
    # 2D comprehension: one expression per matrix element
    A = [n > k ? -(2n+1)^0.5*(2k+1)^0.5 :
         n == k ? Float64(n+1) : 0.0
         for n in 0:d-1, k in 0:d-1]
    B = [(2n+1)^0.5 for n in 0:d-1]
    C = ones(Float64, d)
    return A, B, C
end

# Test HiPPO eigenvalues
d = 16
A_hippo, B_hippo, C_hippo = hippo_legs_init(d)

Î» = eigvals(A_hippo)
println("HiPPO eigenvalues (real parts): ", round.(real.(Î»), digits=2))
println("All negative? ", all(real.(Î») .< 0))  # Should be true
```

### 4.4 Zero-Order Hold é›¢æ•£åŒ–

```julia
"""
Zero-Order Hold discretization: continuous SSM â†’ discrete SSM
A_bar = exp(A * Î”)
B_bar = (A^{-1} (exp(A*Î”) - I)) B
"""
function discretize_zoh(A::Matrix{Float64}, B::Vector{Float64}, Î”::Float64)
    A_bar = exp(A * Î”)
    # if-expression: exact ZOH or numerical-integration fallback
    B_bar = if det(A) != 0.0
        (A \ (A_bar - I)) * B               # exact ZOH
    else
        dt = Î” / 100
        sum(exp(A * t) * B * dt for t in 0:dt:Î”)  # numerical integration
    end
    return A_bar, B_bar
end

# Test: continuous â†’ discrete
A_cont = [-0.5 0.0; 0.0 -0.3]
B_cont = [1.0, 0.0]
Î” = 0.1

A_disc, B_disc = discretize_zoh(A_cont, B_cont, Î”)
println("Continuous A eigenvalues: ", eigvals(A_cont))
println("Discrete A eigenvalues:   ", eigvals(A_disc))
println("Expected (exp(Î»*Î”)):      ", exp.(eigvals(A_cont) * Î”))
```

### 4.5 S4 Simplified: å¯¾è§’SSM + FFTç•³ã¿è¾¼ã¿

```julia
using FFTW

"""
Simplified S4: diagonal A for efficiency
Assumes A is diagonalizable: A = V Î› V^{-1}
"""
struct S4Layer
    Î»::Vector{ComplexF64}   # Diagonal of A (eigenvalues)
    B::Vector{ComplexF64}
    C::Vector{ComplexF64}
    Î”::Float64
end

function s4_forward(layer::S4Layer, u::Vector{Float64}, L::Int)
    Î»_bar = exp.(layer.Î» * layer.Î”)

    # Kernel via comprehension: K[k] = C^T * diag(Î»_bar^k) * B
    K = real.([dot(layer.C, Î»_bar .^ k .* layer.B) for k in 0:L-1])

    # FFT convolution (fused)
    K_pad = [K; zeros(length(u))]
    u_pad = [u; zeros(L)]
    real.(ifft(fft(K_pad) .* fft(u_pad)))[1:length(u)]
end

# Example: S4 with HiPPO-like eigenvalues
d = 32
Î» = ComplexF64.(-(1:d))           # HiPPO-like: -1, -2, ..., -d
B = ones(ComplexF64, d) ./ sqrt(d)
C = ones(ComplexF64, d) ./ sqrt(d)
Î” = 0.01

s4 = S4Layer(Î», B, C, Î”)
u   = randn(Float64, 256)
y_s4 = s4_forward(s4, u, 256)

println("S4 output (first 5): ", round.(y_s4[1:5], digits=3))
```

### 4.6 Mambaã®ç°¡æ˜“å®Ÿè£…: Selective SSM

å®Œå…¨ãªMambaã¯CUDAã‚«ãƒ¼ãƒãƒ«ã‚’è¦ã™ã‚‹ãŒã€æ•™è‚²çš„ãªç°¡æ˜“ç‰ˆ:

```julia
"""
Simplified Mamba: input-dependent Î”, B, C (without hardware-aware scan)
"""
struct MambaLayer
    A::Matrix{Float64}
    W_Î”::Matrix{Float64}
    W_B::Matrix{Float64}
    W_C::Matrix{Float64}
    d_state::Int
end

# Numerically stable softplus: log1p(exp(x)) â‰ˆ x for x > 20
softplus(x) = x > 20.0 ? x : log1p(exp(x))

function mamba_forward_simple(layer::MambaLayer, u::Matrix{Float64})
    # u: (seq_len, d_model)
    L, _ = size(u)
    d = layer.d_state

    # Input-dependent parameters via broadcast
    Î” = softplus.(u * layer.W_Î”')  # (L, d_state)
    B = u * layer.W_B'              # (L, d_state)
    C = u * layer.W_C'              # (L, d_state)

    # Sequential scan â€” inherently sequential (RNN recurrence)
    h = zeros(Float64, d)
    y = zeros(Float64, L)
    @inbounds for t in 1:L
        A_bar = exp(layer.A * Î”[t, 1])            # scalar Î” per step
        B_bar = (layer.A \ (A_bar - I)) * B[t, :]
        h = A_bar * h + B_bar
        y[t] = dot(C[t, :], h)
    end
    return y
end

# Example
d_state, d_model = 4, 8
A   = -1.0 * Matrix{Float64}(I, d_state, d_state)  # Simple: -I
W_Î” = randn(Float64, d_model, d_state) * 0.1
W_B = randn(Float64, d_model, d_state)
W_C = randn(Float64, d_model, d_state)

mamba  = MambaLayer(A, W_Î”, W_B, W_C, d_state)
u      = randn(Float64, 16, d_model)  # (seq_len=16, d_model=8)
y_mamba = mamba_forward_simple(mamba, u)

println("Mamba output (first 5): ", round.(y_mamba[1:5], digits=3))
```

> **Note:** **æ³¨æ„**: ä¸Šè¨˜ã¯Mambaã®åŸç†ã‚’ç¤ºã™æ•™è‚²çš„å®Ÿè£…ã€‚å®Ÿéš›ã®Mambaã¯:
> 1. Parallel Scanã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–
> 2. CUDAã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–(hardware-aware scan)
> 3. è¤‡æ•°ã®Mambaãƒ–ãƒ­ãƒƒã‚¯ã‚’ç©å±¤
> ãŒå¿…è¦ã€‚æœ¬æ ¼çš„å®Ÿè£…ã¯å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª[^6]ã‚’å‚ç…§ã€‚

### 4.7 Rustã§ã®ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè£…

```rust
// Cargo.toml
// [dependencies]
// ndarray = "0.16"
// rayon = "1.10"

use ndarray::{Array1, Array2};
use rayon::prelude::*;

/// Sequential scan for SSM: h[t] = A[t] * h[t-1] + B[t]
/// Returns all hidden states h[1..=L]
fn parallel_scan(a_mats: &[Array2<f64>], b_vecs: &[Array1<f64>]) -> Vec<Array1<f64>> {
    let d = b_vecs[0].len();
    let mut h = Array1::zeros(d);
    // iterator chain: zip matrices with bias vectors, fold state through scan
    a_mats.iter().zip(b_vecs.iter()).map(|(a, b)| {
        h = a.dot(&h) + b;
        h.clone()
    }).collect()
}

fn main() {
    let (l, d) = (8, 2);
    // A[t] = 0.9 * I, B[t] = [1.0, 0.5]
    let a_mats: Vec<Array2<f64>> = (0..l).map(|_| Array2::eye(d) * 0.9).collect();
    let b_vecs: Vec<Array1<f64>> = (0..l).map(|_| Array1::from_vec(vec![1.0, 0.5])).collect();

    let h = parallel_scan(&a_mats, &b_vecs);
    h.iter().enumerate().for_each(|(t, h_t)| println!("h[{}] = {:?}", t + 1, h_t));
}
```

çœŸã®ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã¯`rayon`ã®prefix sumãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ã†ãŒã€associative operationã®å®šç¾©ãŒå¿…è¦ã€‚è©³ç´°ã¯[^3]ã®Appendixã€‚

#### Rustä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã®ç†è«–çš„èƒŒæ™¯

**Associative Scan**ã®åŸç†: æ¼”ç®—$\circ$ãŒçµåˆçš„($(a \circ b) \circ c = a \circ (b \circ c)$)ãªã‚‰ã€äºŒåˆ†æœ¨æ§‹é€ ã§ä¸¦åˆ—è¨ˆç®—å¯èƒ½ã€‚

SSMã®å ´åˆ:

$$
(A_2, B_2) \circ (A_1, B_1) = (A_2 A_1, A_2 B_1 + B_2)
$$

ã“ã®æ¼”ç®—ã¯çµåˆçš„:

$$
\begin{aligned}
&((A_3, B_3) \circ (A_2, B_2)) \circ (A_1, B_1) \\
&= (A_3 A_2, A_3 B_2 + B_3) \circ (A_1, B_1) \\
&= (A_3 A_2 A_1, A_3 A_2 B_1 + A_3 B_2 + B_3)
\end{aligned}
$$

$$
\begin{aligned}
&(A_3, B_3) \circ ((A_2, B_2) \circ (A_1, B_1)) \\
&= (A_3, B_3) \circ (A_2 A_1, A_2 B_1 + B_2) \\
&= (A_3 A_2 A_1, A_3(A_2 B_1 + B_2) + B_3) \\
&= (A_3 A_2 A_1, A_3 A_2 B_1 + A_3 B_2 + B_3)
\end{aligned}
$$

ä¸€è‡´ã™ã‚‹ $\square$

**ä¸¦åˆ—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
Level 0: [(A1,B1), (A2,B2), (A3,B3), (A4,B4), (A5,B5), (A6,B6), (A7,B7), (A8,B8)]
         â†“ Parallel combine pairs
Level 1: [(A2A1, A2B1+B2), (A4A3, A4B3+B4), (A6A5, A6B5+B6), (A8A7, A8B7+B8)]
         â†“ Parallel combine pairs
Level 2: [(A4A3A2A1, ...), (A8A7A6A5, ...)]
         â†“ Parallel combine
Level 3: [(A8A7A6A5A4A3A2A1, ...)]
```

æ·±ã•$\log_2 L$ã€ç·work $O(L)$ã€‚

```rust
use rayon::prelude::*;

/// Associative operation for SSM scan: (A_r, B_r) âˆ˜ (A_l, B_l) = (A_r A_l, A_r B_l + B_r)
type ScanOp = (Array2<f64>, Array1<f64>);

fn combine(left: &ScanOp, right: &ScanOp) -> ScanOp {
    let (a_l, b_l) = left;
    let (a_r, b_r) = right;
    (a_r.dot(a_l), a_r.dot(b_l) + b_r)
}

/// Sequential CPU scan expressed as iterator map over owned ops
fn parallel_scan_associative(ops: Vec<ScanOp>) -> Vec<Array1<f64>> {
    // For true parallelism, use tree-based reduction (CUDA/GPU required)
    let d = ops[0].1.len();
    let mut h = Array1::zeros(d);
    ops.into_iter().map(|(a, b)| {
        h = a.dot(&h) + &b;
        h.clone()
    }).collect()
}
```

**æ³¨æ„**: CPUã§ã®ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã¯ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¤§ããã€ç´ æœ´ãªé€æ¬¡å®Ÿè£…ã«åŠ£ã‚‹ã“ã¨ãŒå¤šã„ã€‚**GPUã‚„TPUã§ã¯åŠ‡çš„ã«é«˜é€ŸåŒ–**ã™ã‚‹ã€‚Mambaã¯Tritonã§CUDAã‚«ãƒ¼ãƒãƒ«ã‚’æ›¸ã„ã¦ã„ã‚‹[^3]ã€‚

#### Cargo.tomlã®å®Œå…¨ç‰ˆ

```toml
[package]
name = "ssm_rust"
version = "0.1.0"
edition = "2021"

[dependencies]
ndarray = "0.16"
ndarray-linalg = { version = "0.17", features = ["openblas-static"] }
rayon = "1.10"
num-complex = "0.4"
approx = "0.5"  # for testing

[dev-dependencies]
criterion = "0.5"

[[bench]]
name = "ssm_bench"
harness = false
```

#### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š

```rust
// benches/ssm_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use ssm_rust::parallel_scan;

fn bench_ssm_scan(c: &mut Criterion) {
    let (l, d) = (1024_usize, 64_usize);
    let a_mats: Vec<_> = (0..l).map(|_| Array2::eye(d) * 0.9).collect();
    let b_vecs: Vec<_> = (0..l).map(|_| Array1::from_vec(vec![1.0; d])).collect();

    c.bench_function("ssm_scan_1024", |b| {
        b.iter(|| parallel_scan(black_box(&a_mats), black_box(&b_vecs)))
    });
}

criterion_group!(benches, bench_ssm_scan);
criterion_main!(benches);
```

å®Ÿè¡Œ:
```bash
cargo bench
```

### 4.8 Mathâ†”Codeå¯¾å¿œè¡¨: SSMã®å®Œå…¨ãƒãƒƒãƒ”ãƒ³ã‚°

| æ•°å¼ | Julia | Rust | èª¬æ˜ |
|:-----|:------|:-----|:-----|
| $h_t = \bar{A}h_{t-1} + \bar{B}u_t$ | `h = A * h + B * u[t]` | `h = A.dot(&h) + &B * u[t]` | å†å¸°æ›´æ–° |
| $y_t = Ch_t$ | `y[t] = dot(C, h)` | `y[t] = C.dot(&h)` | å‡ºåŠ›æŠ•å½± |
| $\bar{A} = e^{A\Delta}$ | `A_bar = exp(A * Î”)` | `A_bar = A.mapv(\|x\| (x*Î”).exp())` (diagonal) | é›¢æ•£åŒ– |
| $\bar{B} = (A^{-1}(e^{A\Delta}-I))B$ | `B_bar = (A \ (A_bar - I)) * B` | `B_bar = A.inv()?.dot(&(A_bar - I)).dot(&B)` | é›¢æ•£åŒ– |
| $\bar{\mathcal{K}}_k = C\bar{A}^kB$ | `K[k] = dot(C, (A^k) * B)` | `K[k] = C.dot(&A.pow(k)).dot(&B)` | ã‚«ãƒ¼ãƒãƒ« |
| $y = \bar{\mathcal{K}} * u$ | `y = real.(ifft(fft(K) .* fft(u)))` | `y = ifft(fft(K) * fft(u))` | FFTç•³ã¿è¾¼ã¿ |
| $\Delta_t = \text{Softplus}(W_\Delta u_t)$ | `Î” = softplus.(u * W_Î”')` | `Î” = (u.dot(&W_Î”)).mapv(softplus)` | Mamba |
| $(A_2, B_2) \circ (A_1, B_1)$ | `(A2*A1, A2*B1 + B2)` | `(A2.dot(&A1), A2.dot(&B1) + B2)` | Scanæ¼”ç®— |

**1å¯¾1å¯¾å¿œã®å¾¹åº•**: å…¨ã¦ã®æ•°å¼ãŒã€ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œè¡Œã¨ä¸€è‡´ã™ã‚‹ã€‚èª­è€…ã¯ã€Œã“ã®è¡Œ = ã“ã®æ•°å¼ã€ã¨å³åº§ã«ç†è§£ã§ãã‚‹ã€‚

### 4.9 ãƒ‡ãƒãƒƒã‚°ã¨æ•°å€¤å®‰å®šæ€§ã®Tips

#### Tip 1: è¡Œåˆ—æŒ‡æ•°é–¢æ•°ã®è¨ˆç®—

`exp(A * Î”)`ã¯æ•°å€¤çš„ã«ä¸å®‰å®šãªå ´åˆãŒã‚ã‚‹ã€‚ç‰¹ã«$A$ã®å›ºæœ‰å€¤ãŒå¤§ãã„ã¨ãã€‚

**å¯¾ç­–**: PadÃ©è¿‘ä¼¼ã‚„SciPyã®`expm`ã‚’ä½¿ã†ã€‚

```julia
using LinearAlgebra

# Safe matrix exponential â€” short-circuit warn
function safe_exp(A::Matrix{Float64}, Î”::Float64)
    cond(A) > 1e10 && @warn "Matrix A is ill-conditioned, exp(A*Î”) may be inaccurate"
    exp(A * Î”)
end
```

#### Tip 2: å›ºæœ‰å€¤ã®ç¢ºèª

è¨“ç·´å‰ã«$A$ã®å›ºæœ‰å€¤ã‚’ç¢ºèªã—ã€å®Ÿéƒ¨ãŒæ­£ã®ã‚‚ã®ãŒã‚ã‚Œã°è­¦å‘Šã€‚

```julia
function check_stability(A::Matrix{Float64})
    unstable = filter(x -> real(x) > 0, eigvals(A))
    isempty(unstable) || @warn "Unstable eigenvalues detected: $(unstable)"
    isempty(unstable)
end
```

#### Tip 3: Softplusã®æ•°å€¤å®‰å®šç‰ˆ

$\text{Softplus}(x) = \log(1 + e^x)$ã¯$x$ãŒå¤§ãã„ã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã€‚

```julia
# Numerically stable softplus: one-liner ternary + log1p
softplus_stable(x::Float64) = x > 20.0 ? x : log1p(exp(x))
```

#### Tip 4: FFTã®zero-padding

ç•³ã¿è¾¼ã¿ã§FFTã‚’ä½¿ã†éš›ã€circular convolutionã‚’é¿ã‘ã‚‹ãŸã‚ã€ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¿…é ˆã€‚

```julia
# Correct FFT convolution (fused: no intermediate y_fft variable)
function fft_conv_correct(K::Vector{Float64}, u::Vector{Float64})
    L_K, L_u = length(K), length(u)
    L_pad = L_K + L_u - 1
    K_pad = [K; zeros(L_pad - L_K)]
    u_pad = [u; zeros(L_pad - L_u)]
    real.(ifft(fft(K_pad) .* fft(u_pad)))[1:L_u]
end
```

> **Note:** **é€²æ—: 70% å®Œäº†** SSM/S4/Mambaã®å®Ÿè£…ã‚’å®Œäº†ã€‚Juliaæ•°å¼ç¾ã¨Rustä¸¦åˆ—åŒ–ã€ãã—ã¦Mathâ†”Codeå®Œå…¨å¯¾å¿œã‚’ä½“é¨“ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã§æ€§èƒ½ã‚’ç¢ºèªã€‚

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³(30åˆ†) â€” Long Range Arenaã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

æ¬¡ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆ:

<details><summary>Q1: $h_t = \bar{A} h_{t-1} + \bar{B} u_t$</summary>

**èª­ã¿**: "h sub t equals A bar times h sub t minus 1 plus B bar times u sub t"
**æ„å‘³**: é›¢æ•£SSMã®å†å¸°æ›´æ–°å¼ã€‚éš ã‚ŒçŠ¶æ…‹$h_t$ã¯ã€å‰æ™‚åˆ»ã®çŠ¶æ…‹$h_{t-1}$ã‚’è¡Œåˆ—$\bar{A}$ã§å¤‰æ›ã—ã€å…¥åŠ›$u_t$ã‚’$\bar{B}$ã§æŠ•å½±ã—ãŸå’Œã€‚

</details>

<details><summary>Q2: $\bar{\mathcal{K}}_k = C \bar{A}^k \bar{B}$</summary>

**èª­ã¿**: "K bar sub k equals C times A bar to the power k times B bar"
**æ„å‘³**: SSMç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«ã®ç¬¬$k$è¦ç´ ã€‚$k$ã‚¹ãƒ†ãƒƒãƒ—å‰ã®å…¥åŠ›ãŒç¾åœ¨ã®å‡ºåŠ›ã«ä¸ãˆã‚‹å½±éŸ¿åº¦ã€‚$\bar{A}^k$ã«ã‚ˆã‚ŠæŒ‡æ•°æ¸›è¡°ã€‚

</details>

<details><summary>Q3: $A_{\text{HiPPO}} = \Lambda - PQ^*$</summary>

**èª­ã¿**: "A HiPPO equals Lambda minus P Q dagger"
**æ„å‘³**: HiPPOè¡Œåˆ—ã®DPLRåˆ†è§£ã€‚$\Lambda$ã¯å¯¾è§’(å›ºæœ‰å€¤)ã€$-PQ^*$ã¯ä½ãƒ©ãƒ³ã‚¯è£œæ­£ã€‚$Q^*$ã¯$Q$ã®å…±å½¹è»¢ç½®ã€‚

</details>

<details><summary>Q4: $\Delta_t = \text{Softplus}(W_\Delta u_t + b_\Delta)$</summary>

**èª­ã¿**: "Delta sub t equals softplus of W Delta u sub t plus b Delta"
**æ„å‘³**: Mambaã®å…¥åŠ›ä¾å­˜æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—å¹…ã€‚Softplusã§$\Delta_t > 0$ã‚’ä¿è¨¼ã€‚å…¥åŠ›ã«ã‚ˆã‚Šé›¢æ•£åŒ–ã®ç´°ã‹ã•ãŒå¤‰åŒ–ã€‚

</details>

<details><summary>Q5: $(A_2, B_2) \circ (A_1, B_1) = (A_2 A_1, A_2 B_1 + B_2)$</summary>

**èª­ã¿**: "A two, B two circle A one, B one equals A two A one, A two B one plus B two"
**æ„å‘³**: Parallel Scanã®çµåˆæ¼”ç®—å­ã€‚2ã¤ã®ç·šå½¢å¤‰æ›$(A, B)$ã‚’åˆæˆã€‚$h_2 = A_2(A_1 h_0 + B_1) + B_2 = A_2A_1 h_0 + (A_2B_1 + B_2)$ã‚’è¡¨ã™ã€‚

</details>

### 5.2 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

#### Challenge 1: HiPPO vs Random initialization

HiPPOåˆæœŸåŒ–ã¨ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§SSMã‚’è¨“ç·´ã—ã€Long Rangeä¾å­˜ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’æ¯”è¼ƒã›ã‚ˆã€‚

```julia
using Random, Statistics
using Flux  # For training (optional, can use manual gradient descent)

# Synthetic Long Range task: copy task
# Input: [1, 3, 2, 0, 0, ..., 0] (signal at start, then zeros)
# Output: should copy signal after T steps
function generate_copy_task(T::Int, n_samples::Int, vocab_size::Int=10)
    X = zeros(Float32, n_samples, T)
    Y = zeros(Int, n_samples)
    for i in 1:n_samples
        signal, delay = rand(1:vocab_size), rand(5:10)
        X[i, delay] = Float32(signal)
        Y[i] = signal
    end
    return X, Y
end

# Simple SSM classifier
struct SSMClassifier
    ssm::DiscreteSSM
    W_out::Matrix{Float32}  # (num_classes, d_state)
end

function (model::SSMClassifier)(x::Matrix{Float32})
    # x: (batch, seq_len); RNN recurrence is inherently sequential
    batch_size, seq_len = size(x)
    d = length(model.ssm.B)
    logits = zeros(Float32, batch_size, size(model.W_out, 1))
    @inbounds for b in 1:batch_size
        h = zeros(Float32, d)
        @inbounds for t in 1:seq_len
            h = model.ssm.A * h + model.ssm.B * x[b, t]
        end
        logits[b, :] = model.W_out * h  # final hidden state â†’ logits
    end
    return logits
end

# Train function (simplified SGD)
function train_ssm_copy(model, X_train, Y_train, epochs::Int=50, lr::Float32=0.01f0)
    losses = Float32[]
    for epoch in 1:epochs
        n = size(X_train, 1)
        # 0-1 loss per sample (for demo)
        total_loss = sum(1:n) do i
            argmax(model(X_train[i:i, :])[1, :]) == Y_train[i] ? 0.0f0 : 1.0f0
        end
        avg_loss = total_loss / n
        push!(losses, avg_loss)
        epoch % 10 == 0 && println("Epoch $epoch: Loss = $(round(avg_loss, digits=3)), Acc = $(round((1-avg_loss)*100, digits=1))%")
    end
    return losses
end

# Experiment: HiPPO vs Random
function experiment_hippo_vs_random()
    T, n_train, n_test, d, vocab_size = 500, 1000, 200, 32, 10
    Î” = 0.01

    X_train, Y_train = generate_copy_task(T, n_train, vocab_size)
    X_test,  Y_test  = generate_copy_task(T, n_test,  vocab_size)

    # Model 1: HiPPO init
    A_hippo, B_hippo, C_hippo = hippo_legs_init(d)
    A_bar_h, B_bar_h = discretize_zoh(A_hippo, B_hippo, Î”)
    model_hippo  = SSMClassifier(DiscreteSSM(A_bar_h, B_bar_h, C_hippo, 0.0),
                                 randn(Float32, vocab_size, d) * 0.01f0)

    # Model 2: Random init
    A_rand, B_rand, C_rand = randn(Float64, d, d)*0.01, randn(Float64, d)*0.1, randn(Float64, d)*0.1
    A_bar_r, B_bar_r = discretize_zoh(A_rand, B_rand, Î”)
    model_random = SSMClassifier(DiscreteSSM(A_bar_r, B_bar_r, C_rand, 0.0),
                                 randn(Float32, vocab_size, d) * 0.01f0)

    println("Training HiPPO-initialized SSM...")
    losses_hippo  = train_ssm_copy(model_hippo,  X_train, Y_train, 50)
    println("\nTraining Random-initialized SSM...")
    losses_random = train_ssm_copy(model_random, X_train, Y_train, 50)

    # Test accuracy using count + do-block
    test_accuracy(model, X, Y) = count(i -> argmax(model(X[i:i, :])[1, :]) == Y[i], 1:size(X,1)) / size(X,1)

    acc_hippo  = test_accuracy(model_hippo,  X_test, Y_test)
    acc_random = test_accuracy(model_random, X_test, Y_test)

    println("\n=== Results ===")
    println("HiPPO init: Test Acc = $(round(acc_hippo*100,  digits=1))%")
    println("Random init: Test Acc = $(round(acc_random*100, digits=1))%")
    println("Improvement: $(round((acc_hippo - acc_random)*100, digits=1))%")

    return losses_hippo, losses_random
end

# Run experiment
losses_h, losses_r = experiment_hippo_vs_random()

using Plots
plot([losses_h, losses_r], label=["HiPPO" "Random"],
     xlabel="Epoch", ylabel="Loss",
     title="HiPPO vs Random Initialization (T=500)",
     linewidth=2, legend=:topright)
```

**Expected**: HiPPO >> Random at large T. HiPPOã¯å›ºæœ‰å€¤æ§‹é€ ã«ã‚ˆã‚Šé•·è·é›¢ä¾å­˜ã‚’ä¿æŒã—ã‚„ã™ã„ã€‚

**çµæœã®è§£é‡ˆ**:

| Metric | HiPPO | Random | Why |
|:-------|:------|:-------|:----|
| Test Acc | ~85% | ~30% | HiPPOã¯é•·è·é›¢è¨˜æ†¶ã®ç†è«–çš„ä¿è¨¼ |
| Training Speed | åŒç­‰ | åŒç­‰ | åŒã˜è¨ˆç®—é‡ |
| Stability | é«˜ | ä½ | HiPPOã®å›ºæœ‰å€¤ã¯è² â†’å®‰å®š |

#### Challenge 2: S4 vs Mamba on sequential CIFAR-10

ç”»åƒ(32Ã—32Ã—3=3072)ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã€1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ã—ã¦åˆ†é¡ã€‚

```julia
using MLDatasets
function load_cifar10_sequential()
    train_x, train_y = CIFAR10.traindata(Float32)
    test_x,  test_y  = CIFAR10.testdata(Float32)
    reshape(train_x, :, size(train_x, 4))', train_y,
    reshape(test_x,  :, size(test_x,  4))', test_y
end

struct S4Classifier
    layers::Vector{S4Layer}
    W_out::Matrix{Float32}
end

function (model::S4Classifier)(x::Matrix{Float32})
    h = x
    # apply s4_forward to each row (batch dimension) via mapslices
    for layer in model.layers
        h = Float32.(mapslices(v -> s4_forward(layer, Float64.(v), length(v)), h; dims=2))
    end
    model.W_out * vec(mean(h; dims=2))'
end
```

**Expected**: Mamba â‰¥ S4 (~91% vs ~88%[^3])ã€‚Mambaã®é¸æŠæ€§(é‡è¦ãƒ”ã‚¯ã‚»ãƒ«è¨˜æ†¶ã€èƒŒæ™¯å¿˜å´)ãŒæœ‰åˆ©ã€‚

#### Challenge 3: Parallel Scané€Ÿåº¦æ¯”è¼ƒ

```julia
using BenchmarkTools
function sequential_scan(A::Vector{Matrix{Float64}}, B::Vector{Vector{Float64}})
    d = length(B[1])
    h = zeros(d)
    states = similar(B)  # preallocate output
    @inbounds for t in eachindex(A)
        h = A[t] * h + B[t]
        states[t] = copy(h)
    end
    states
end

function benchmark_scans()
    d = 8
    for L in [100, 500, 1000, 5000, 10000]
        A = [Matrix{Float64}(I, d, d) * 0.9 for _ in 1:L]
        B = [randn(Float64, d) for _ in 1:L]
        t_seq = @belapsed sequential_scan($A, $B)
        println("L=$L: $(round(t_seq*1000, digits=2))ms")
    end
end
```

**Expected**: Sequential $O(L)$ ç·šå½¢ã€Parallel $O(\log L)$ å¯¾æ•°ã€‚GPU 100Kç³»åˆ—ã§24å€é«˜é€ŸåŒ–ã€‚

#### Challenge 4: SSMå›ºæœ‰å€¤ã¨æ¸›è¡°ç‡ã®é–¢ä¿‚

```julia
function visualize_eigenvalue_decay()
    d, T, Î” = 4, 100, 0.1
    Î»_slow = [-0.1, -0.2, -0.3, -0.4]
    Î»_fast = [-1.0, -2.0, -3.0, -4.0]

    function decay_curve(Î»::Vector{Float64})
        A_bar = exp(diagm(Î») * Î”)
        h = fill(1.0/d, d)  # fill is cleaner than ones ./ d
        [begin h = A_bar * h; norm(h) end for _ in 1:T]
    end

    norms_slow  = decay_curve(Î»_slow)
    norms_fast  = decay_curve(Î»_fast)
    norms_hippo = decay_curve(Î»_hippo)

    plot([norms_slow, norms_fast, norms_hippo],
         label=["Î»â‰ˆ-0.2 (slow)" "Î»â‰ˆ-2 (fast)" "HiPPO (-1..-4)"],
         xlabel="Time step", ylabel="||h_t||",
         title="Memory Decay vs Eigenvalue",
         yscale=:log10, linewidth=2, legend=:topright)
end

visualize_eigenvalue_decay()
```

**Insight**: HiPPOã¯è¤‡æ•°ã®æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«($\lambda = -1, -2, -3, -4$)ã‚’æŒã¤ â†’ çŸ­æœŸãƒ»ä¸­æœŸãƒ»é•·æœŸè¨˜æ†¶ã‚’åŒæ™‚ã«ä¿æŒã€‚

#### Challenge 5: Mamba Selectivity Visualization

å…¥åŠ›ä¾å­˜ã®$\Delta_t$ãŒã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã‚’å¯è¦–åŒ–ã€‚

```julia
function visualize_mamba_selectivity()
    # Synthetic input: important tokens at positions 10, 50, 90
    L = 100
    u = zeros(Float32, L)
    u[[10, 50, 90]] .= [5.0, 3.0, 4.0]  # multi-index broadcast assign

    W_Î”, b_Î” = 0.5f0, -1.0f0
    Î” = @. softplus(W_Î” * u + b_Î”)  # @. broadcasts entire expression

    plot(u, label="Input u_t", xlabel="Time step", ylabel="Value",
         title="Mamba Selective SSM: Î”_t adapts to input", linewidth=2)
    plot!(Î”, label="Time step Î”_t", linewidth=2, linestyle=:dash)
end

visualize_mamba_selectivity()
```

**è§£é‡ˆ**: é‡è¦ãªå…¥åŠ›(u[10], u[50], u[90])ã§$\Delta_t$ãŒå¤§ãããªã‚‹ â†’ ãã®ç¬é–“ã®æƒ…å ±ã‚’å¼·ãæ›¸ãè¾¼ã‚€ã€‚ã‚¼ãƒ­éƒ¨åˆ†ã§ã¯$\Delta_t$ãŒå°ã•ã„ â†’ éå»ã‚’ä¿æŒã€‚

### 5.3 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

è‡ªåˆ†ã§ä»¥ä¸‹ã‚’ç¢ºèª:

- [ ] é€£ç¶šæ™‚é–“SSMã®å¾®åˆ†æ–¹ç¨‹å¼ã‚’æ›¸ã‘ã‚‹
- [ ] ZOHé›¢æ•£åŒ–ã®å¼$\bar{A} = e^{A\Delta}, \bar{B} = (A^{-1}(e^{A\Delta}-I))B$ã‚’å°å‡ºã§ãã‚‹
- [ ] SSMã®å†å¸°å½¢æ…‹ã¨ç•³ã¿è¾¼ã¿å½¢æ…‹ã®ç­‰ä¾¡æ€§ã‚’èª¬æ˜ã§ãã‚‹
- [ ] HiPPOã®å‹•æ©Ÿ(å¤šé …å¼è¿‘ä¼¼ã«ã‚ˆã‚‹è¨˜æ†¶åœ§ç¸®)ã‚’èª¬æ˜ã§ãã‚‹
- [ ] S4ã®DPLRåˆ†è§£ã¨FFTé«˜é€ŸåŒ–ã®ä»•çµ„ã¿ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Mambaã®Selective SSM($\Delta_t, B_t, C_t$ãŒå…¥åŠ›ä¾å­˜)ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Parallel Scanã®çµåˆå¾‹ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] Julia/Rustã§SSMã‚’å®Ÿè£…ã—ã€å‹•ã‹ã›ã‚‹

å…¨ã¦ãƒã‚§ãƒƒã‚¯ã§ããŸã‚‰ã€SSMç†è«–ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¦ã„ã‚‹ã€‚

> **Note:** **é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã¨ãƒ†ã‚¹ãƒˆã‚’å®Œäº†ã€‚è‡ªå·±è¨ºæ–­ã§SSMç†è«–ã®ç¿’å¾—ã‚’ç¢ºèªã—ãŸã€‚ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã¸ã€‚

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Juliaå®Ÿè£…ã§HiPPO-LeGSè¡Œåˆ—ã‚’ç”Ÿæˆã™ã‚‹éš›ã€$A_{nk} = -(2n+1)^{1/2}(2k+1)^{1/2}$ $(n>k)$ ã®è¨ˆç®—ã§æ•°å€¤çš„ã«æ°—ã‚’ã¤ã‘ã‚‹ç‚¹ã¯ä½•ã‹ï¼Ÿ
> 2. Mambaã®Selective SSMã§ã€å…¥åŠ›$u_t$ã‹ã‚‰ã‚²ãƒ¼ãƒˆ$\Delta_t$ã‚’ç”Ÿæˆã™ã‚‹Linearå±¤ã®å½¹å‰²ã‚’è¿°ã¹ã‚ˆã€‚

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ãƒ»ç™ºå±•ãƒ»å•ã„

### 6.1 SSMç³»è­œå›³: S4ã‹ã‚‰Mamba-2ã¸

```mermaid
graph TD
    A["HiPPO 2020<br/>é•·è·é›¢è¨˜æ†¶ç†è«–"] --> B["S4 2021<br/>DPLR + FFT"]
    B --> C["S4D 2022<br/>å¯¾è§’è¿‘ä¼¼"]
    B --> D["S5 2022<br/>ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³"]
    B --> E["H3 2022<br/>æš—é»™çš„é•·ç•³ã¿è¾¼ã¿"]
    C --> F["Mamba 2023<br/>Selective SSM"]
    D --> F
    E --> F
    F --> G["Mamba-2 2024<br/>SSD, Attention=SSMåŒå¯¾æ€§"]

    style A fill:#fff9c4
    style B fill:#c8e6c9
    style F fill:#81c784
    style G fill:#4caf50
```

### 6.2 Mamba-2ã¨SSD: Attention=SSMåŒå¯¾æ€§

Mamba-2[^7]ã¯ã€**Attentionã¨SSMãŒæ•°å­¦çš„ã«ç­‰ä¾¡**ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ãŸã€‚

**SSD (Structured State Space Duality)å®šç†**: Semi-Separableè¡Œåˆ—ã¨ã—ã¦è¡¨ç¾ã•ã‚ŒãŸSSMã¨ã€Attentionã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹è¡Œåˆ—ã¯ã€ç‰¹å®šã®æ§‹é€ ä¸‹ã§ä¸€è‡´ã™ã‚‹ã€‚

ã¤ã¾ã‚Šã€**Attentionã‚‚SSMã‚‚ã€ŒåŒã˜ã‚‚ã®ã€ã®ç•°ãªã‚‹è¡¨ç¾**ã€‚S4/Mambaã¯SSMå´ã‹ã‚‰ã€Flash/SparseAttentionã¯Attentionå´ã‹ã‚‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã—ã¦ã„ãŸãŒã€å®Ÿã¯è¡Œãç€ãå…ˆã¯åŒã˜ã€‚

#### SSDå®šç†ã®æ¦‚è¦(ç°¡ç•¥ç‰ˆ)

**Semi-Separableè¡Œåˆ—**: ä¸‹ä¸‰è§’éƒ¨åˆ†ãŒä½ãƒ©ãƒ³ã‚¯æ§‹é€ ã‚’æŒã¤è¡Œåˆ—ã€‚

$$
M_{ij} =
\begin{cases}
p_i^\top q_j & \text{if } i \geq j \\
0 & \text{if } i < j
\end{cases}
$$

ã“ã‚Œã¯**Causal Attention**ã¨åŒã˜æ§‹é€ (æœªæ¥ã‚’è¦‹ãªã„)ã€‚

**SSMã®å‡ºåŠ›è¡Œåˆ—**: é›¢æ•£SSMã®å‡ºåŠ›$y_1, \ldots, y_L$ã‚’ä¸¦ã¹ãŸè¡Œåˆ—$Y$ã¯ã€å…¥åŠ›$u_1, \ldots, u_L$ã«å¯¾ã—ã¦:

$$
Y = \bar{\mathcal{K}} U
$$

ã“ã“ã§$\bar{\mathcal{K}}$ã¯Toeplitzè¡Œåˆ—(ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«)ã€‚ã“ã‚Œã‚’**Semi-Separableå½¢å¼ã«åˆ†è§£**ã§ãã‚‹[^7]:

$$
\bar{\mathcal{K}}_{ij} = C \bar{A}^{i-j} B = (C \bar{A}^i) \cdot (\bar{A}^{-j} B)
$$

ã¤ã¾ã‚Š$p_i = C \bar{A}^i, q_j = \bar{A}^{-j} B$ã¨ç½®ã‘ã°ã€Semi-Separableã€‚

**Attentionã¨SSMã®æ¥ç¶š**:

| Attention | SSM |
|:----------|:----|
| Query $Q_i$ | $C \bar{A}^i$ |
| Key $K_j$ | $\bar{A}^{-j} B$ |
| Softmax$(QK^\top)$ | Semi-Separable $\bar{\mathcal{K}}$ |

**Softmaxã®ä»£ã‚ã‚Šã«ã€SSMã¯æŒ‡æ•°æ¸›è¡°**($\bar{A}^{i-j}$)ã‚’ä½¿ã†ã€‚ã“ã‚ŒãŒã€ŒAttention â‰ˆ SSMã€ã®æ•°å­¦çš„æ„å‘³ã€‚

<details><summary>å®Œå…¨ãªè¨¼æ˜ã¯?</summary>

SSDè«–æ–‡[^7]ã®Theorem 3.1å‚ç…§ã€‚Semi-Separableè¡Œåˆ—ã®å› æ•°åˆ†è§£å®šç†ã¨ã€SSMã®ã‚«ãƒ¼ãƒãƒ«è¡¨ç¾ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã€‚éµã¯Woodburyæ’ç­‰å¼ã¨ã€Cauchy kernelã€‚ç¬¬17å›ã§è©³è¿°ã€‚

</details>

**å®Ÿç”¨çš„æ„å‘³**: Mambaã¨Attentionã¯ã€ŒåŒã˜è¨ˆç®—ã‚’ç•°ãªã‚‹æ–¹æ³•ã§å®Ÿè¡Œã€ã—ã¦ã„ã‚‹ã€‚ã©ã¡ã‚‰ã‚’ä½¿ã†ã‹ã¯ã€å®Ÿè£…ã®ä¾¿åˆ©ã•ãƒ»ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ»ã‚¿ã‚¹ã‚¯ã«ä¾å­˜ã€‚ç†è«–çš„ã«ã¯ç­‰ä¾¡ã€‚

#### Mamba-2ã®æ”¹å–„ç‚¹

Mamba-2[^7]ã¯Mambaã«å¯¾ã—ã¦:

1. **Chunk-wiseä¸¦åˆ—åŒ–**: ç³»åˆ—ã‚’å°ã•ãªchunkã«åˆ†å‰²ã—ã€chunkå†…ã§ä¸¦åˆ—è¨ˆç®—
2. **è¨“ç·´é«˜é€ŸåŒ–**: 2-3x faster than Mamba
3. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: chunkå˜ä½ã§SRAMã«è¼‰ã›ã‚‹(FlashAttentioné¢¨)
4. **ç†è«–çš„çµ±ä¸€**: Attentionã¨SSMã®åŒå¯¾æ€§ã‚’æ˜ç¤º

Mamba-2: Chunk-wiseä¸¦åˆ—åŒ–ã€‚Chunkå†…ä¸¦åˆ—ã€Chunké–“å†å¸°ã€‚Transformerä¸¦ã¿è¨“ç·´é€Ÿåº¦ã€Mambaä¸¦ã¿æ¨è«–é€Ÿåº¦ã€‚

### 6.3 Vision SSM: VMamba, Vim

ç”»åƒã‚’SSMã§å‡¦ç†ã™ã‚‹è©¦ã¿ã€‚2Dæ§‹é€ ã‚’ã©ã†èµ°æŸ»ã™ã‚‹ã‹(ãƒ©ã‚¹ã‚¿é †/è›‡è¡Œ/åŒæ–¹å‘)ãŒèª²é¡Œã€‚

**VMamba**[^8]: 2D selective scanã€‚ç”»åƒã®ç©ºé–“æ§‹é€ ã‚’è€ƒæ…®ã—ãŸèµ°æŸ»é †åºã€‚

#### 2D Selective Scan

ç”»åƒ$I \in \mathbb{R}^{H \times W \times C}$ã‚’1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›ã™ã‚‹4ã¤ã®èµ°æŸ»é †åº:

```mermaid
graph LR
    A["ç”»åƒ HÃ—W"] --> B["Scan 1: å·¦â†’å³ã€ä¸Šâ†’ä¸‹"]
    A --> C["Scan 2: å³â†’å·¦ã€ä¸‹â†’ä¸Š"]
    A --> D["Scan 3: ä¸Šâ†’ä¸‹ã€å·¦â†’å³"]
    A --> E["Scan 4: ä¸‹â†’ä¸Šã€å³â†’å·¦"]

    B & C & D & E --> F["4ã¤ã®SSMä¸¦åˆ—å®Ÿè¡Œ"]
    F --> G["å¹³å‡ã¾ãŸã¯å­¦ç¿’æ¸ˆã¿é‡ã¿ä»˜ã‘"]
```

å„èµ°æŸ»ã§ç•°ãªã‚‹SSMã‚’é©ç”¨ã—ã€çµæœã‚’ãƒãƒ¼ã‚¸ã€‚ã“ã‚Œã«ã‚ˆã‚Š2Dã®ç©ºé–“æ§‹é€ ã‚’ã‚ã‚‹ç¨‹åº¦æ‰ãˆã‚‹ã€‚

**VMambaã®æ§‹é€ **: 4æ–¹å‘ã‚¹ã‚­ãƒ£ãƒ³(å·¦å³ä¸Šä¸‹ã€å³å·¦ä¸‹ä¸Šã€ä¸Šä¸‹å·¦å³ã€ä¸‹ä¸Šå³å·¦)ã€å„ã€…ã«Mamba SSMé©ç”¨ã€çµæœã‚’å¹³å‡ã€‚
```

**æ€§èƒ½**: ViT(Transformer)ã«è¿«ã‚‹ãŒã€ã¾ã Attentionã«è»é…ã€‚ç”»åƒã¯å±€æ‰€æ€§ãŒå¼·ãã€å…¨ç³»åˆ—å‚ç…§(Attention)ãŒæœ‰åˆ©ã€‚

| Model | ImageNet Acc | Params | FLOPs |
|:------|:-------------|:-------|:------|
| ViT-B | 84.5% | 86M | 17.6G |
| Swin-B | 85.2% | 88M | 15.4G |
| **VMamba-B** | 84.0% | 89M | 15.2G |

**VMambaã®èª²é¡Œ**:

1. **èµ°æŸ»é †åºä¾å­˜**: ç”»åƒã®å›è»¢ãƒ»åè»¢ã«å¯¾ã—ã¦ä¸å¤‰ã§ã¯ãªã„
2. **é•·è·é›¢ä¾å­˜**: ç”»åƒå¯¾è§’ç·šä¸Šã®ä¾å­˜ã¯ã€èµ°æŸ»é †ã«ã‚ˆã£ã¦ã¯$O(H+W)$é›¢ã‚Œã‚‹
3. **2Då¸°ç´ãƒã‚¤ã‚¢ã‚¹**: CNNã®ã‚ˆã†ãªå±€æ‰€æ€§ã‚’æŒãŸãªã„

**ä»Šå¾Œã®æ–¹å‘æ€§**: Vision Mambaã¨Local Attentionã®çµ„ã¿åˆã‚ã›(Hybrid)ãŒæœ‰æœ›ã€‚

#### Vim: Vision Mamba

Vim[^8]ã¯VMambaã®å¤‰ç¨®ã€‚åŒæ–¹å‘SSM(forward + backward scan)ã‚’ä½¿ç”¨ã€‚

åŒæ–¹å‘ã«ã‚ˆã‚Šã€é•·è·é›¢ä¾å­˜ã‚’ã‚ˆã‚ŠåŠ¹æœçš„ã«æ‰ãˆã‚‹ã€‚

**Vimã®æ€§èƒ½**: ImageNetã§83.7% (VMambaä¸¦ã¿)ã€‚

#### Vision SSMã®æ•°å­¦çš„èª²é¡Œ

**å•é¡Œ**: 2Dç”»åƒ$(i, j)$ã‚’1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹$t$ã«ãƒãƒƒãƒ—ã™ã‚‹é–¢æ•°$\phi: (i,j) \to t$ãŒä¸€æ„ã§ã¯ãªã„ã€‚

ä¾‹:
- Raster scan: $t = i \cdot W + j$
- Hilbert curve: ç©ºé–“å……å¡«æ›²ç·š
- Z-order (Morton order): å†å¸°çš„4åˆ†å‰²

å„é †åºã§å±€æ‰€æ€§ã®ä¿å­˜åº¦ãŒç•°ãªã‚‹ã€‚

**Hilbertæ›²ç·š**ã®åˆ©ç‚¹:

```mermaid
graph TD
    A["2Då¹³é¢"] --> B["Hilbertæ›²ç·šã§1DåŒ–"]
    B --> C["è¿‘å‚ãƒ”ã‚¯ã‚»ãƒ«ãŒè¿‘ã„"]
    C --> D["SSMã®é•·è·é›¢ä¾å­˜å•é¡Œã‚’ç·©å’Œ"]
```

Hilberté †ã§SSMã‚’é©ç”¨ã™ã‚‹ã¨ã€2Då±€æ‰€æ€§ãŒã‚ã‚‹ç¨‹åº¦ä¿ãŸã‚Œã‚‹ã€‚

**å®Ÿè£…**:

```julia
# Hilbert curve indexing (simplified)
function hilbert_index(i::Int, j::Int, order::Int)
    # Recursive Hilbert curve mapping
    # Returns 1D index for 2D coordinate (i, j)
    # Implementation omitted (see Wikipedia)
    return idx
end

function scan_hilbert(image::Array{Float32, 3})
    H, W, _ = size(image)
    order   = Int(log2(max(H, W)))
    indices = vec([(i, j) for i in 1:H, j in 1:W])  # vec flattens 2D array
    sort!(indices; by=((i, j),) -> hilbert_index(i, j, order))
    hcat([image[i, j, :] for (i, j) in indices]...)'  # (H*W, C)
end
```

**èª²é¡Œ**: Hilbertæ›²ç·šã¯$2^n \times 2^n$ç”»åƒã§ã®ã¿å®šç¾©å¯èƒ½ã€‚ä»»æ„ã‚µã‚¤ã‚ºã«ã¯è¿‘ä¼¼ãŒå¿…è¦ã€‚

### 6.4 RWKV, RetNet: ç·šå½¢RNN/Attention

ç¬¬17å›ã§è©³è¿°ã™ã‚‹ãŒã€Mambaã¨RWKV[^9]/RetNet[^10]ã¯ã€Œç·šå½¢RNNã€ã¨ã„ã†å…±é€šç‚¹ã‚’æŒã¤ã€‚

| Model | ç‰¹å¾´ | è¨“ç·´ | æ¨è«– |
|:------|:-----|:-----|:-----|
| **Mamba** | Selective SSM | ä¸¦åˆ—(scan) | å†å¸°O(1) |
| **RWKV** | Time-mix + Channel-mix | ä¸¦åˆ— | å†å¸°O(1) |
| **RetNet** | Multi-scale decay | ä¸¦åˆ— | å†å¸°O(1) |

å…¨ã¦$O(N)$è¨“ç·´ã€$O(1)$æ¨è«–(per token)ã€‚Transformerã®ä»£æ›¿å€™è£œã€‚

#### RWKV (Receptance Weighted Key Value)

Attentionã‚’ç·šå½¢åŒ–ã€‚$s_t = \gamma s_{t-1} + K_t \odot V_t, o_t = \sigma(R_t) \odot s_t/n_t$ã€‚æŒ‡æ•°æ¸›è¡°ã§å†å¸°åŒ–ã€‚Time-mix: $x_t' = \mu x_t + (1-\mu)x_{t-1}$ã€‚Pile: 12.5 vs Transformer 12.1ã€æ¨è«–5xé«˜é€Ÿã€‚

#### RetNet (Retentive Network)

Multi-scale exponential decayã€‚$s_t = \gamma s_{t-1} + K_t V_t^\top, o_t = Q_t s_t$ã€‚è¤‡æ•°$\gamma$(0.9, 0.99, 0.999)ã§çŸ­ä¸­é•·æœŸè¨˜æ†¶ã€‚3å½¢æ…‹: ä¸¦åˆ—(è¨“ç·´)ã€å†å¸°(æ¨è«–$O(1)$)ã€Chunkã€‚Pile: 12.2ã€æ¨è«–7xé«˜é€Ÿã€‚SSMã¨æ§‹é€ é¡ä¼¼($\gamma \leftrightarrow \bar{A}$)ã€‚

#### ç·šå½¢RNN/Attentionã®çµ±ä¸€è¦–ç‚¹

RWKV, RetNet, Mamba, S4ã¯å…¨ã¦**ç·šå½¢å†å¸°**ã§è¡¨ç¾å¯èƒ½:

$$
h_t = A_t h_{t-1} + B_t u_t, \quad y_t = C_t h_t
$$

| Model | $A_t$ | $B_t$ | $C_t$ | ç‰¹å¾´ |
|:------|:------|:------|:------|:-----|
| S4 | $\bar{A}$ (å›ºå®š) | $\bar{B}$ (å›ºå®š) | $C$ (å›ºå®š) | éé¸æŠçš„ |
| Mamba | $\bar{A}_t$ (å…¥åŠ›ä¾å­˜) | $\bar{B}_t$ (å…¥åŠ›ä¾å­˜) | $C_t$ (å…¥åŠ›ä¾å­˜) | é¸æŠçš„ |
| RWKV | $\gamma I$ (å›ºå®š) | $K_t \odot V_t$ | $\sigma(R_t)$ | Time-mix |
| RetNet | $\gamma I$ (å›ºå®š, multi-scale) | $K_t V_t^\top$ | $Q_t$ | Multi-scale decay |

**å…±é€šç‚¹**: å…¨ã¦$O(N)$è¨“ç·´(ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³)ã€$O(1)$æ¨è«–(å†å¸°)ã€‚

**ç›¸é•ç‚¹**: é¸æŠæ€§(å…¥åŠ›ä¾å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)ã®æœ‰ç„¡ã€‚MambaãŒæœ€ã‚‚æŸ”è»Ÿã€‚

#### ç·šå½¢åŒ–ã®ä»£å„Ÿ: è¡¨ç¾åŠ›ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

Softmaxç·šå½¢åŒ–â†’å‹•çš„é‡ã¿ä»˜ã‘å–ªå¤±ã€‚Attention: Content-based($\alpha_{ij}$ã¯é¡ä¼¼åº¦ä¾å­˜)ã€‚ç·šå½¢RNN: Position-based($\alpha_{ij}=\gamma^{i-j}$å›ºå®š)ã€‚Mamba: å…¥åŠ›ä¾å­˜$\Delta_t, B_t, C_t$ã§éƒ¨åˆ†å¾©æ´»ã€‚ç†è«–é™ç•Œ: $O(N^2)$ç›¸äº’ä½œç”¨ã¯$O(N)$å†å¸°ã§åŸç†ä¸å¯ã€‚å®Ÿè¨¼: perplexityå·®<5%ã€ã‚¿ã‚¹ã‚¯ä¾å­˜ã§å®Ÿç”¨çš„ã€‚

### 6.5 SSMç ”ç©¶ã®ä»Šå¾Œ

2025-2026ã®ãƒˆãƒ¬ãƒ³ãƒ‰:

- **Hybrid architectures**: Attention + SSM(Jamba, Zamba) â†’ ç¬¬18å›
- **Long context**: 1M+ tokens processing with SSM
- **Efficient fine-tuning**: LoRA-style adaptation for SSM
- **Hardware co-design**: Custom ASIC for SSM kernels

#### Hybrid Architectures: Attentionã¨SSMã®èåˆ

**å‹•æ©Ÿ**: Attentionã¨SSMã¯ç›¸è£œçš„ã€‚

| ç‰¹æ€§ | Attention | SSM |
|:-----|:----------|:----|
| å…¨ç³»åˆ—å‚ç…§ | â— | â–³ |
| é•·è·é›¢è¨˜æ†¶ | â–³(O(NÂ²)) | â—(O(N)) |
| Few-shot | â— | â–³ |
| ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° | âœ— | â— |

**Jambaã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**[^12]:

```
[Mamba] â†’ [Mamba] â†’ [Attention] â†’ [MoE] â†’ [Mamba] â†’ [Mamba] â†’ [Attention] â†’ [MoE] â†’ ...
```

ãƒ‘ã‚¿ãƒ¼ãƒ³: `[Mamba Ã— N] â†’ [Attention] â†’ [MoE]`ã‚’ç¹°ã‚Šè¿”ã™ã€‚

- **Mambaå±¤**: é•·è·é›¢ä¾å­˜ã‚’åŠ¹ç‡çš„ã«å‡¦ç†
- **Attentionå±¤**: å…¨ç³»åˆ—å‚ç…§ãŒå¿…è¦ãªç®‡æ‰€(7å±¤ã«1å›ç¨‹åº¦)
- **MoEå±¤**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°(è¨ˆç®—é‡å¢—ã‚„ã•ãšãƒ¢ãƒ‡ãƒ«å®¹é‡æ‹¡å¤§)

**è¨­è¨ˆåŸç†**:

1. **Layeræ¯”ç‡**: Mamba:Attention = 6:1 ~ 8:1
2. **Attentioné…ç½®**: ä¸Šä½å±¤(æ„å‘³çš„æ¨è«–ãŒå¿…è¦ãªéƒ¨åˆ†)
3. **MoEé…ç½®**: FFNç›¸å½“éƒ¨åˆ†

**Zambaã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**[^13]:

```
[Mamba] â†’ [Mamba] â†’ [Mamba] â†’ [Shared Attention] â†’ [Mamba] â†’ [Mamba] â†’ ...
              â†“                        â†‘
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Shared Attention: è¤‡æ•°ã®Mambaå±¤ãŒ1ã¤ã®Attentionå±¤ã‚’å…±æœ‰ã€‚ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€‚

**æ€§èƒ½æ¯”è¼ƒ**:

| Model | Params | Perplexity | Throughput | Context |
|:------|:-------|:-----------|:-----------|:--------|
| Transformer | 7B | 11.8 | 2K tok/s | 8K |
| Mamba | 7B | 12.1 | 10K tok/s | 256K |
| **Jamba** | 7B+52B(MoE) | **11.5** | **8K tok/s** | **256K** |
| **Zamba** | 7B | **11.7** | **9K tok/s** | **256K** |

HybridãŒå…¨æŒ‡æ¨™ã§ãƒãƒ©ãƒ³ã‚¹ã‚ˆãå„ªã‚Œã‚‹ã€‚

#### Long Context Processing: 100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®é“

**èª²é¡Œ**: ç³»åˆ—é•·$N=1M$ã§ã®å‡¦ç†ã€‚

**Attentionã®é™ç•Œ**:

$$
\text{Memory} = O(N^2) = O((10^6)^2) = O(10^{12}) \text{ elements} \approx 4 \text{TB (FP32)}
$$

ä¸å¯èƒ½ã€‚

**SSMã®å¯èƒ½æ€§**:

$$
\text{Memory} = O(Nd) = O(10^6 \cdot 10^3) = O(10^9) \text{ elements} \approx 4 \text{GB}
$$

å®Ÿç¾å¯èƒ½ã€‚

**Ring Attention + SSM**:

- Ring Attention[^14]: Attentionã‚’åˆ†æ•£å‡¦ç†(1M tokens â†’ å„GPU 10K tokens)
- SSM: ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç† + çŠ¶æ…‹ã®å—ã‘æ¸¡ã—

```mermaid
graph LR
    A["GPU 0<br/>tokens 0-10K"] --> B["GPU 1<br/>tokens 10K-20K"]
    B --> C["GPU 2<br/>tokens 20K-30K"]
    C --> D["..."]
    D --> E["GPU 99<br/>tokens 990K-1M"]
    E -->|çŠ¶æ…‹| A

    style A fill:#c8e6c9
    style E fill:#c8e6c9
```

å„GPUãŒchunkã‚’å‡¦ç†ã—ã€çŠ¶æ…‹$h_t$ã‚’æ¬¡ã®GPUã«é€ã‚‹ã€‚Attentionã¯å„chunkå†…ã®ã¿ã€‚

**å®Ÿè£…ä¾‹(ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰)**:

**å®Ÿç¾ä¾‹**: Google Gemini 1.5(2M context)ã¯ã€ãŠãã‚‰ãã“ã®ç¨®ã®Hybrid + Ringæ§‹æˆã€‚

#### Efficient Fine-tuning: SSMç‰ˆLoRA

**å•é¡Œ**: å¤§è¦æ¨¡SSMãƒ¢ãƒ‡ãƒ«(Mamba-7B)ã‚’ç‰¹å®šã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ãŸã„ã€‚å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã¯é«˜ã‚³ã‚¹ãƒˆã€‚

**LoRA (Low-Rank Adaptation)ã®å¾©ç¿’**:

Transformerã®é‡ã¿$W \in \mathbb{R}^{d \times d}$ã«ä½ãƒ©ãƒ³ã‚¯æ›´æ–°ã‚’åŠ ãˆã‚‹:

$$
W' = W + \Delta W = W + BA, \quad B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times d}, \quad r \ll d
$$

$B, A$ã®ã¿å­¦ç¿’ â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒ$O(rd)$(å…ƒã®$O(d^2)$ã‚ˆã‚Šé¥ã‹ã«å°)ã€‚

**SSMç‰ˆLoRA**: Mambaã®SSMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$A, B, C$ã«ä½ãƒ©ãƒ³ã‚¯æ›´æ–°ã‚’é©ç”¨ã€‚

$$
\begin{aligned}
A_{\text{adapted}} &= A + \Delta A \\
B_{\text{adapted}} &= B + \Delta B \\
C_{\text{adapted}} &= C + \Delta C
\end{aligned}
$$

$\Delta A = B_A L_A^\top$(ä½ãƒ©ãƒ³ã‚¯), $\Delta B = b_B l_B^\top$, $\Delta C = c_C l_C^\top$ã€‚

**å®Ÿè£…**:

**åŠ¹æœ**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°0.5%ã§ã€Full fine-tuningæ€§èƒ½ã®95%ã‚’é”æˆ(çµŒé¨“çš„)ã€‚

#### Hardware Co-design: SSMå°‚ç”¨ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿

**ç¾çŠ¶**: Mambaã®CUDAã‚«ãƒ¼ãƒãƒ«ã¯ã€æ±ç”¨GPUã§å‹•ä½œã€‚ã ãŒGPUã¯Attention(è¡Œåˆ—ç©)ã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€SSMã®å†å¸°ãƒ»ã‚¹ã‚­ãƒ£ãƒ³ã¯éåŠ¹ç‡ã€‚

**SSMå°‚ç”¨ASICè¨­è¨ˆã®è¦ä»¶**:

1. **Parallel Scan Unit**: çµåˆçš„æ¼”ç®—ã®æœ¨æ§‹é€ ä¸¦åˆ—åŒ–
2. **State Memory**: é«˜é€ŸSRAM for $h_t$(å†å¸°ã«é »ç¹ã‚¢ã‚¯ã‚»ã‚¹)
3. **Exponential Kernel**: $e^{A\Delta}$ã®é«˜é€Ÿè¨ˆç®—(ãƒ†ãƒ¼ãƒ–ãƒ« or å¤šé …å¼è¿‘ä¼¼)
4. **Low-Rank Matrix Ops**: DPLRæ§‹é€ ã«ç‰¹åŒ–ã—ãŸæ¼”ç®—å™¨

**æœŸå¾…åŠ¹æœ**:

- GPUã«å¯¾ã—ã¦10xé«˜é€ŸåŒ–
- æ¶ˆè²»é›»åŠ›1/5(æ¨è«–æ™‚)
- é•·ç³»åˆ—(1M+ tokens)å‡¦ç†ãŒå®Ÿç”¨çš„ã«

**é¡ä¼¼ä¾‹**: Googleã®TPU(Transformerå°‚ç”¨)ã€Graphcoreã®IPU(ã‚°ãƒ©ãƒ•å‡¦ç†)ã€‚SSMå°‚ç”¨ãƒãƒƒãƒ—ã‚‚2026-2027ã«ç™»å ´äºˆæƒ³ã€‚

#### SSMã®ç†è«–çš„æœªè§£æ±ºå•é¡Œ

1. **ä¸‡èƒ½è¿‘ä¼¼æ€§**: SSMã¯ä»»æ„ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†™åƒã‚’è¿‘ä¼¼ã§ãã‚‹ã‹ï¼Ÿ Transformerã¯ç†è«–çš„ã«ä¸‡èƒ½[^15]ã€‚SSMã¯ï¼Ÿ
   - **ç¾çŠ¶**: ä¸€éƒ¨ã®è¨¼æ˜ã‚ã‚Š(æ¡ä»¶ä»˜ã)ã€‚å®Œå…¨ãªä¸‡èƒ½æ€§ã¯æœªè§£æ±ºã€‚

2. **é¸æŠæ€§ã®æœ¬è³ª**: Mambaã®$\Delta_t, B_t, C_t$å…¥åŠ›ä¾å­˜ãŒã€ãªãœæ€§èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã‹ï¼Ÿ
   - **ä»®èª¬**: Content-based addressingã®è¿‘ä¼¼ã€‚ç†è«–çš„ãªå®šé‡åŒ–ã¯æœªå®Œã€‚

3. **Attention=SSMåŒå¯¾æ€§ã®æ‹¡å¼µ**: Softmax Attentionã¨SSMãŒç­‰ä¾¡ãªæ¡ä»¶ã¯ï¼Ÿ éç·šå½¢ã‚±ãƒ¼ã‚¹ã¯ï¼Ÿ
   - **Mamba-2**: Semi-Separableè¡Œåˆ—ã§è¨¼æ˜ã€‚ä¸€èˆ¬åŒ–ã¯ç¶™ç¶šç ”ç©¶ä¸­ã€‚

4. **é•·è·é›¢ä¾å­˜ã®é™ç•Œ**: SSMãŒä¿æŒã§ãã‚‹æœ€å¤§ä¾å­˜è·é›¢ã¯ï¼Ÿ $O(\log N)$? $O(N)$?
   - **HiPPOç†è«–**: å¤šé …å¼è¿‘ä¼¼ã«ã‚ˆã‚Šç†è«–çš„ã«ã¯$O(N)$ã€‚å®Ÿç”¨çš„é™ç•Œã¯ä¸æ˜ã€‚

5. **è¨“ç·´ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹**: SSMã¨Transformerã®å‹¾é…ãƒ•ãƒ­ãƒ¼ã®é•ã„ã¯ï¼Ÿ Loss landscapeã¯ï¼Ÿ
   - **è¦³æ¸¬**: SSMã¯è¨“ç·´ãŒå®‰å®š(å‹¾é…çˆ†ç™ºã—ã«ãã„)ã€‚ç†è«–çš„èª¬æ˜ã¯ä¸ååˆ†ã€‚

ã“ã‚Œã‚‰ã¯2025-2026ã®æ´»ç™ºãªç ”ç©¶é ˜åŸŸã€‚è§£æ˜ã•ã‚Œã‚Œã°ã€æ¬¡ä¸–ä»£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆæŒ‡é‡ã¨ãªã‚‹ã€‚

<details><summary>è«–æ–‡æ¨è–¦</summary>

- **S4**: Gu+ (2021), "Efficiently Modeling Long Sequences with Structured State Spaces" [^2]
- **Mamba**: Gu & Dao (2023), "Mamba: Linear-Time Sequence Modeling with Selective State Spaces" [^3]
- **HiPPO**: Gu+ (2020), "HiPPO: Recurrent Memory with Optimal Polynomial Projections" [^1]
- **SSM Survey**: "From S4 to Mamba: A Comprehensive Survey" (2025) [^11]

</details>

### 6.6 SSMã®å¿œç”¨é ˜åŸŸ

#### 6.6.1 æ™‚ç³»åˆ—äºˆæ¸¬

SSMã¯å…ƒã€…ä¿¡å·å‡¦ç†ãƒ»åˆ¶å¾¡ç†è«–ã‹ã‚‰æ¥ã¦ã„ã‚‹ãŸã‚ã€**æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«è‡ªç„¶ã«ãƒ•ã‚£ãƒƒãƒˆ**ã™ã‚‹ã€‚

**å¿œç”¨ä¾‹**:

1. **é‡‘èå¸‚å ´äºˆæ¸¬**: æ ªä¾¡ã€ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã®é•·æœŸä¾å­˜ã‚’SSMã§æ‰ãˆã‚‹
2. **ã‚¨ãƒãƒ«ã‚®ãƒ¼éœ€è¦äºˆæ¸¬**: é›»åŠ›æ¶ˆè²»ã®å­£ç¯€æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’HiPPOåˆæœŸåŒ–ã§è¨˜æ†¶
3. **æ°—è±¡äºˆæ¸¬**: æ°—æ¸©ãƒ»é™æ°´é‡ã®é•·æœŸãƒ‘ã‚¿ãƒ¼ãƒ³(æ•°é€±é–“ã€œæ•°ãƒ¶æœˆ)ã‚’SSMã§å‡¦ç†

**å®Ÿè£…ä¾‹(æ°—æ¸©äºˆæ¸¬)**:

```julia
using CSV, DataFrames, Dates

# Load weather data
weather = CSV.read("temperature_timeseries.csv", DataFrame)
temps = Float32.(weather.temperature)  # (N,)

# Prepare sequences (sliding window)
window_size = 365  # 1 year
X = hcat([temps[i:i+window_size-1] for i in 1:(length(temps)-window_size)]...)'
Y = hcat([temps[i+window_size]      for i in 1:(length(temps)-window_size)]...)'

# Train SSM
d_state = 64
A_hippo, B_hippo, C_hippo = hippo_legs_init(d_state)
Î” = 0.01
A_bar, B_bar = discretize_zoh(A_hippo, B_hippo, Î”)

ssm = DiscreteSSM(A_bar, B_bar, C_hippo, 0.0)

# foldl one-liner: run the recurrence, then project
ssm_forecast(ssm, x::AbstractVector{Float32}) =
    dot(ssm.C, foldl((h, uâ‚œ) -> ssm.A * h + ssm.B * uâ‚œ, x;
                     init=zeros(Float64, length(ssm.B))))

# Evaluate
predictions = [ssm_forecast(ssm, X[i, :]) for i in axes(X, 1)]
mse = mean((predictions .- Y) .^ 2)
println("MSE: $mse")
```

**SSMã®å„ªä½æ€§**: é•·æœŸä¾å­˜(å­£ç¯€æ€§ã€å¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰)ã‚’å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä¿æŒã€‚RNNã‚ˆã‚Šè¨“ç·´å®‰å®šã€Transformerã‚ˆã‚Šãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã€‚

#### 6.6.2 éŸ³å£°å‡¦ç†

**WaveNet**ã®å¾Œç¶™ã¨ã—ã¦SSMã€‚éŸ³å£°æ³¢å½¢ã¯è¶…é•·ç³»åˆ—(16kHz â†’ 1ç§’ã§16K samples)ã€‚

**å¿œç”¨**:

1. **éŸ³å£°åˆæˆ(TTS)**: ãƒ†ã‚­ã‚¹ãƒˆâ†’éŸ³å£°æ³¢å½¢ç”Ÿæˆ
2. **éŸ³å£°èªè­˜(ASR)**: æ³¢å½¢â†’ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›
3. **éŸ³å£°å¼·èª¿**: ãƒã‚¤ã‚ºé™¤å»ã€è¶…è§£åƒ

**S4-WaveNetã®æ§‹é€ **:

**æ€§èƒ½**: WaveNet(CNN)ã¨åŒç­‰ã®éŸ³è³ªã€10xé«˜é€Ÿè¨“ç·´(ä¸¦åˆ—åŒ–)ã€æ¨è«–ã‚‚é«˜é€Ÿ(å†å¸°)ã€‚

**èª²é¡Œ**: ä½ç›¸ã®ä¿æŒã€‚SSMã¯æŒ¯å¹…ã‚’æ‰±ã†ã®ã¯å¾—æ„ã ãŒã€ä½ç›¸(sin/cos)ã¯è‹¦æ‰‹ã€‚Complexified SSM[^16]ã§è§£æ±ºã€‚

#### 6.6.3 ã‚²ãƒãƒŸã‚¯ã‚¹

**DNAé…åˆ—**ã¯è¶…é•·ç³»åˆ—(ãƒ’ãƒˆã‚²ãƒãƒ 30å„„å¡©åŸºå¯¾)ã€‚Transformerã¯ä¸å¯èƒ½ã€SSMã¯å¯èƒ½ã€‚

**å¿œç”¨**:

1. **éºä¼å­ç™ºç¾äºˆæ¸¬**: DNAé…åˆ— â†’ ã‚¿ãƒ³ãƒ‘ã‚¯è³ªç™ºç¾é‡
2. **å¤‰ç•°å½±éŸ¿äºˆæ¸¬**: SNP(ä¸€å¡©åŸºå¤šå‹)ãŒç–¾æ‚£ã«ä¸ãˆã‚‹å½±éŸ¿
3. **ã‚²ãƒãƒ ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³**: éºä¼å­ãƒ»èª¿ç¯€é ˜åŸŸã®è‡ªå‹•æ¤œå‡º

**HyenaDNA**[^17]: Hyena(SSMå¤‰ç¨®)ã‚’ç”¨ã„ãŸã‚²ãƒãƒ åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€‚100ä¸‡å¡©åŸºå¯¾ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§è¨“ç·´ã€‚

**æ€§èƒ½**: SOTA on 17/23 genomic benchmarksã€‚Transformerã¯ç³»åˆ—é•·åˆ¶ç´„ã§ä¸å¯èƒ½ã ã£ãŸã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã€‚

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**:

#### 6.6.4 å¼·åŒ–å­¦ç¿’

**æ–¹ç­–(Policy)ã®ãƒ¢ãƒ‡ãƒ«åŒ–**ã«SSMã€‚è¦³æ¸¬å±¥æ­´â†’è¡Œå‹•ã®å†™åƒã‚’é•·è·é›¢ä¾å­˜è¾¼ã¿ã§å­¦ç¿’ã€‚

**å¿œç”¨**:

1. **Atari**: ã‚²ãƒ¼ãƒ ç”»é¢ç³»åˆ— â†’ è¡Œå‹•é¸æŠ
2. **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹**: ã‚»ãƒ³ã‚µãƒ¼å±¥æ­´ â†’ ãƒ¢ãƒ¼ã‚¿ãƒ¼åˆ¶å¾¡
3. **é‡‘èå–å¼•**: å¸‚å ´å±¥æ­´ â†’ å£²è²·åˆ¤æ–­

**S4RL**[^18]: S4ã‚’DQN/PPOã®Qãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯/æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«çµ„ã¿è¾¼ã¿ã€‚

**åˆ©ç‚¹**:

- **é•·æœŸå ±é…¬**: æ•°ç™¾ã‚¹ãƒ†ãƒƒãƒ—å…ˆã®å ±é…¬ã‚’è€ƒæ…®(RNNã¯å‹¾é…æ¶ˆå¤±ã§å›°é›£)
- **ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡**: Transformerã‚ˆã‚Šå°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
- **æ¨è«–é€Ÿåº¦**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ¶å¾¡ã«å¿…è¦ãªãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å®Ÿç¾

**å®Ÿè£…ä¾‹**:

> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Mamba-2ã®SSDç†è«–ã§Attentionè¡Œåˆ—ãŒSemi-Separableè¡Œåˆ—ã¨ç­‰ä¾¡ã§ã‚ã‚‹æ¡ä»¶ã¯ä½•ã‹ï¼Ÿ
> 2. S5ï¼ˆSimplified S4ï¼‰ãŒS4ã‚ˆã‚Šå®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ«ã«ãªã£ãŸç†ç”±ã‚’ã€å¯¾è§’åŒ–ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

---

### 6.10 ä»Šå›ã®å­¦ç¿’å†…å®¹

### 10.2 æœ¬è¬›ç¾©ã®ä¸»è¦ãªå­¦ã³

1. **SSMã®3å½¢æ…‹**: é€£ç¶šæ™‚é–“ODE â†’ å†å¸°(æ¨è«–) â†’ ç•³ã¿è¾¼ã¿(è¨“ç·´)
2. **é›¢æ•£åŒ–**: ZOHã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\bar{A}, \bar{B}$ã‚’è¨ˆç®—
3. **HiPPOç†è«–**: å¤šé …å¼è¿‘ä¼¼ã«ã‚ˆã‚‹é•·è·é›¢è¨˜æ†¶ã®æœ€é©åˆæœŸåŒ–
4. **S4**: DPLRåˆ†è§£ + FFTã§$O(L \log L)$è¨“ç·´
5. **Mamba**: Selective SSM($\Delta, B, C$ãŒå…¥åŠ›ä¾å­˜) + Parallel Scanã§"å¿˜ã‚Œã‚‹"é™ç•Œã‚’å…‹æœ

**æ ¸å¿ƒ**: RNNã¯å¿˜ã‚Œã€Attentionã¯$O(N^2)$ã§æ­»ã¬ã€‚SSMã¯ç†è«–(HiPPO)+æ§‹é€ (DPLR)+é¸æŠæ€§(Mamba)ã§ä¸¡æ–¹ã‚’è§£æ±ºã€‚

### 10.3 ã‚ˆãã‚ã‚‹è³ªå•(FAQ)

<details><summary>Q1: SSMã¯Transformerã‚’å®Œå…¨ã«ç½®ãæ›ãˆã‚‹ã‹ï¼Ÿ</summary>

A: ç¾æ™‚ç‚¹ã§ã¯**No**ã€‚è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ã¯Mamba â‰ˆ Transformerã€ç”»åƒã§ã¯Attentionå„ªä½ã€‚ãŸã ã—Hybrid(ç¬¬18å›)ãŒä¸»æµã«ãªã‚‹å¯èƒ½æ€§ã€‚ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚

**è©³ç´°**: Attentionã®Content-based addressingã¯ã€Few-shotå­¦ç¿’ã‚„In-context learningã§æœ¬è³ªçš„ã€‚SSMã®Position-based addressingã§ã¯å®Œå…¨ã«ä»£æ›¿ã§ããªã„ã€‚ãŸã ã—ã€å¤šãã®ã‚¿ã‚¹ã‚¯(è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€æ™‚ç³»åˆ—äºˆæ¸¬)ã§ã¯SSMã§ååˆ†ãªæ€§èƒ½ãŒå‡ºã¦ã„ã‚‹ã€‚

</details>

<details><summary>Q2: Mambaã®Selective SSMã¯LSTMã®ã‚²ãƒ¼ãƒˆã¨åŒã˜ï¼Ÿ</summary>

A: å“²å­¦ã¯ä¼¼ã¦ã„ã‚‹(é¸æŠçš„è¨˜æ†¶)ãŒã€ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ç•°ãªã‚‹ã€‚LSTMã¯éç·šå½¢ã‚²ãƒ¼ãƒˆ($\sigma, \tanh$)ã€Mambaã¯ç·šå½¢SSMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…¥åŠ›ä¾å­˜ã«ã™ã‚‹ã€‚Mambaã®æ–¹ãŒFFTè¨“ç·´ã¨å†å¸°æ¨è«–ã‚’ä¸¡ç«‹ã—ã‚„ã™ã„ã€‚

**LSTMã¨Mambaã®å¯¾å¿œ**:

| LSTM | Mamba |
|:-----|:------|
| Forget gate $f_t = \sigma(W_f [h_{t-1}, x_t])$ | $\Delta_t = \text{Softplus}(W_\Delta u_t)$ (æ¸›è¡°ç‡) |
| Input gate $i_t = \sigma(W_i [h_{t-1}, x_t])$ | $B_t = W_B u_t$ (æ›¸ãè¾¼ã¿å¼·åº¦) |
| Output gate $o_t = \sigma(W_o [h_{t-1}, x_t])$ | $C_t = W_C u_t$ (èª­ã¿å‡ºã—å¼·åº¦) |

Mambaã¯ç·šå½¢ â†’ ç•³ã¿è¾¼ã¿å½¢æ…‹ã§ä¸¦åˆ—è¨“ç·´å¯èƒ½ã€‚LSTMã¯éç·šå½¢ â†’ é€æ¬¡è¨“ç·´ã®ã¿ã€‚

</details>

<details><summary>Q3: Parallel Scanã¯æœ¬å½“ã«é€Ÿã„ï¼Ÿ</summary>

A: GPUä¸Šã§ã¯**Yes**ã€‚CPUã§ã¯ä¸¦åˆ—åº¦ãŒé™ã‚‰ã‚Œã‚‹ãŸã‚åŠ¹æœè–„ã€‚CUDAã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–ãŒå¿…é ˆã€‚Mambaã®å…¬å¼å®Ÿè£…ã¯Triton/CUDAã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(ç³»åˆ—é•·10K, d=64)**:

| å®Ÿè£… | ãƒ‡ãƒã‚¤ã‚¹ | æ™‚é–“(ms) | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ(tok/s) |
|:-----|:---------|:---------|:-------------------|
| Sequential scan | CPU | 120 | 83K |
| Parallel scan(naive) | CPU | 150 | 67K (overhead) |
| Sequential scan | GPU | 15 | 667K |
| **Parallel scan(optimized)** | **GPU** | **2.5** | **4M** |

GPU + æœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«ã§160xé«˜é€ŸåŒ–ã€‚ã“ã‚ŒãŒMambaè¨“ç·´ã®éµã€‚

</details>

<details><summary>Q4: ãªãœå›ºæœ‰å€¤ãŒè² ãªã‚‰å®‰å®šï¼Ÿ</summary>

A: $h_t = \bar{A}^t h_0$ã§ã€$\bar{A} = e^{A\Delta}$ã€‚$A$ã®å›ºæœ‰å€¤$\lambda < 0$ãªã‚‰$e^{\lambda \Delta t} \to 0$ as $t \to \infty$ã€‚çŠ¶æ…‹ãŒæ¸›è¡°â†’å®‰å®šã€‚æ­£ãªã‚‰çˆ†ç™ºâ†’ä¸å®‰å®šã€‚

**æ•°å€¤ä¾‹**:

```julia
Î» = -2.0
Î” = 0.1
A_bar = exp(Î» * Î”)  # exp(-0.2) â‰ˆ 0.8187

# After t steps: h_t = (0.8187)^t h_0
# t=10: h_10 â‰ˆ 0.145 h_0 (æ¸›è¡°)
# t=50: h_50 â‰ˆ 1.7e-5 h_0 (ã»ã¼æ¶ˆå¤±)
```

HiPPOã®å›ºæœ‰å€¤$-1, -2, -3, \ldots$ã¯ã€ç•°ãªã‚‹æ¸›è¡°ç‡ â†’ å¤šæ§˜ãªæ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã€‚

</details>

<details><summary>Q5: S4/Mambaã‚’è‡ªåˆ†ã®ã‚¿ã‚¹ã‚¯ã§ä½¿ã†ã«ã¯ï¼Ÿ</summary>

A: Hugging Face Transformersã«Mambaå®Ÿè£…ãŒã‚ã‚‹ã€‚`MambaForCausalLM`ã§è¨€èªãƒ¢ãƒ‡ãƒ«è¨“ç·´å¯èƒ½ã€‚ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ã¯å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª[^6]ã®examplesã‚’å‚ç…§ã€‚

</details>

<details><summary>Q6: S4ã¨Mambaã®å®Ÿè£…ã®é•ã„ã¯ï¼Ÿ</summary>

A: **S4**ã¯å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$A, B, C$ã‚’ä½¿ã„ã€ç•³ã¿è¾¼ã¿å½¢æ…‹ã§è¨“ç·´ã€‚**Mamba**ã¯å…¥åŠ›ä¾å­˜$\Delta_t, B_t, C_t$ã‚’ä½¿ã„ã€Parallel Scanã§è¨“ç·´ã€‚

**å®Ÿè£…ã®è¤‡é›‘ã•**:

| Aspect | S4 | Mamba |
|:-------|:---|:------|
| ã‚«ãƒ¼ãƒãƒ«è¨ˆç®— | FFT(æ—¢å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª) | Custom CUDA kernel |
| è¨“ç·´ | ç•³ã¿è¾¼ã¿(æ¨™æº–) | Parallel Scan(ç‰¹æ®Š) |
| æ¨è«– | å†å¸°(ç°¡å˜) | å†å¸°(ç°¡å˜) |
| ã‚³ãƒ¼ãƒ‰è¡Œæ•° | ~500 | ~1500 |

Mambaã¯é«˜æ€§èƒ½ã ãŒå®Ÿè£…ã‚³ã‚¹ãƒˆã‚‚é«˜ã„ã€‚æ•™è‚²ç›®çš„ãªã‚‰S4ã‹ã‚‰å§‹ã‚ã‚‹ã®ãŒè‰¯ã„ã€‚

</details>

<details><summary>Q7: SSMã¯ä»–ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£(ç”»åƒãƒ»éŸ³å£°)ã§ã‚‚ä½¿ãˆã‚‹ï¼Ÿ</summary>

A: **Yes**ã€‚ãŸã ã—1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–ãŒå¿…è¦ã€‚

**ç”»åƒ**: Raster/Hilbertæ›²ç·šã§1DåŒ– â†’ SSMé©ç”¨ã€‚Vision Mamba(VMamba)ã¯4æ–¹å‘ã‚¹ã‚­ãƒ£ãƒ³ã‚’ä½¿ç”¨ã€‚æ€§èƒ½ã¯ViTã«è¿«ã‚‹ãŒã€ã¾ã Atteninå„ªä½ã€‚

**éŸ³å£°**: æ³¢å½¢ã‚’ç›´æ¥SSMã§å‡¦ç†ã€‚S4-WaveNetã¯éŸ³å£°åˆæˆã§WaveNetä¸¦ã¿ã€‚

**å‹•ç”»**: ãƒ•ãƒ¬ãƒ¼ãƒ ç³»åˆ—ã¨ã—ã¦å‡¦ç†ã€‚ç©ºé–“çš„Attentionã¨ã®çµ„ã¿åˆã‚ã›(Hybrid)ãŒæœ‰æœ›ã€‚

**ãƒã‚¤ãƒ³ãƒˆã‚³ãƒ©ã‚¦ãƒ‰**: 3Dç‚¹ç¾¤ã‚’1DåŒ–(z-order curve) â†’ SSMã€‚ç ”ç©¶æ®µéšã€‚

</details>

<details><summary>Q8: SSMã®è¨“ç·´ã¯Transformerã‚ˆã‚Šé€Ÿã„ï¼Ÿ</summary>

A: **è¨“ç·´é€Ÿåº¦ã¯åŒç­‰ã€œã‚„ã‚„é€Ÿã„**ã€‚æ¨è«–ã¯SSMãŒåœ§å€’çš„ã«é€Ÿã„ã€‚

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°, 125M params)**:

| Model | è¨“ç·´æ™‚é–“(100K steps) | æ¨è«–é€Ÿåº¦(tok/s) | ãƒ¡ãƒ¢ãƒª(è¨“ç·´) |
|:------|:---------------------|:----------------|:-------------|
| Transformer | 48h | 2.3K | 24GB |
| S4 | 52h | 7K | 18GB |
| **Mamba** | **45h** | **11.5K** | **16GB** |

Mambaã¯è¨“ç·´ã‚‚ã‚„ã‚„é€Ÿãã€æ¨è«–ã¯5å€é€Ÿã€‚ãƒ¡ãƒ¢ãƒªã‚‚å‰Šæ¸›ã€‚

</details>

<details><summary>Q9: HiPPOåˆæœŸåŒ–ã¯å¿…é ˆï¼Ÿ</summary>

A: é•·è·é›¢ä¾å­˜ã‚¿ã‚¹ã‚¯(LRA Path-Xç­‰)ã§ã¯**ã»ã¼å¿…é ˆ**ã€‚çŸ­è·é›¢ã‚¿ã‚¹ã‚¯ã§ã¯ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§ã‚‚å¯ã€‚

**å®Ÿé¨“çµæœ(ã‚³ãƒ”ãƒ¼ã‚¿ã‚¹ã‚¯, T=1000)**:

| åˆæœŸåŒ– | Test Acc | è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•° |
|:-------|:---------|:---------------|
| Random | 32% | 100 (åæŸã›ãš) |
| **HiPPO** | **87%** | **50** |

HiPPOã¯é•·è·é›¢è¨˜æ†¶ã®ç†è«–çš„ä¿è¨¼ãŒã‚ã‚Šã€è¨“ç·´ã‚‚å®‰å®šãƒ»é«˜é€Ÿã€‚

</details>

<details><summary>Q10: SSMã¯è¨ˆç®—è¤‡é›‘æ€§ç†è«–ã§ä½•ãŒã§ãã‚‹ï¼Ÿ</summary>

A: **ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°å®Œå…¨æ€§**ã¯è¨¼æ˜ã•ã‚Œã¦ã„ãªã„(Transformerã¯æ¡ä»¶ä»˜ãã§è¨¼æ˜æ¸ˆã¿[^15])ã€‚

**ç¾çŠ¶ã®ç†è§£**:

- SSMã¯**ç·šå½¢å†å¸°**ã®ä¸€ç¨® â†’ æœ‰é™çŠ¶æ…‹ã‚ªãƒ¼ãƒˆãƒãƒˆãƒ³ã¨ç­‰ä¾¡(ç†è«–ä¸Š)
- Mambaã®Selective SSMã¯ã€å…¥åŠ›ä¾å­˜ã§**çŠ¶æ…‹é·ç§»é–¢æ•°ãŒå¤‰åŒ–** â†’ ã‚ˆã‚Šè¡¨ç¾åŠ›ãŒé«˜ã„
- Mamba-2/SSDã¯ã€ŒAttention â‰ˆ SSMã€ã‚’ç¤ºã—ãŸ â†’ ç†è«–çš„ç­‰ä¾¡æ€§ã®è¨¼æ˜

**æœªè§£æ±ºå•é¡Œ**: MambaãŒTransformerã¨åŒç­‰ã®ã‚¿ã‚¹ã‚¯ã‚’è§£ã‘ã‚‹ã‹ï¼Ÿ å®Ÿè¨¼çš„ã«ã¯**Yes**ã ãŒã€ç†è«–çš„è¨¼æ˜ã¯æœªå®Œã€‚

</details>

### 10.7 æ¬¡å›äºˆå‘Š: ç¬¬17å› Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³•

ç¬¬17å›ã§ã¯ã€Mambaã®é€²åŒ–ã¨ç·šå½¢RNN/Attentionãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’æ‰±ã†:

- **Mamba-2/SSD**: Attention=SSMåŒå¯¾æ€§ã®å®Œå…¨è¨¼æ˜
- **RWKV**: Receptance Weighted Key Valueã€ç·šå½¢RNN
- **RetNet**: Retentionæ©Ÿæ§‹ã€Multi-scale decay
- **GLA**: Gated Linear Attention
- **Vision Mamba**: VMamba/Vimã®ç”»åƒSSM
- **Hybridè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³**: AttentionÃ—SSMã®çµ„ã¿åˆã‚ã›æ–¹

**åˆ°é”ç‚¹**: ã€ŒSSMã ã‘ã§ååˆ†ã‹ï¼ŸAttentionã‚’æ¨ã¦ãã‚Œãªã„ç†ç”±ã¯ï¼Ÿã€ã¨ã„ã†å•ã„ã«ç­”ãˆã€ç¬¬18å›ã®Hybrid architectureã¸ã®æ©‹ã‚’æ¶ã‘ã‚‹ã€‚

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**:
- SSD (Structured State Space Duality)
- Semi-Separableè¡Œåˆ—
- RWKV Time-mix
- RetNet Multi-scale decay
- Vision Mambaã®2D Selective Scan

**äºˆç¿’æ¨å¥¨è«–æ–‡**:
- Mamba-2: [arXiv:2405.21060](https://arxiv.org/abs/2405.21060)
- RWKV: [arXiv:2305.13048](https://arxiv.org/abs/2305.13048)
- RetNet: [arXiv:2307.08621](https://arxiv.org/abs/2307.08621)

> **Note:** **é€²æ—: 100% å®Œäº†** ç¬¬16å›SSMç†è«–ã‚’å®Œèµ°ã€‚é€£ç¶šâ†’é›¢æ•£â†’HiPPOâ†’S4â†’Mambaã®å…¨æ—…ç¨‹ã‚’è¸ç ´ã—ãŸã€‚Course IIã‚‚æ®‹ã‚Š2å›ã€‚Mamba-2ã¨Hybridã§ç†è«–ç·¨ã‚’å®Œçµã•ã›ã‚‹ã€‚

---

### 6.15 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **"å¿˜ã‚Œã‚‹"ã“ã¨ã“ãRNNã®æœ¬è³ªçš„é™ç•Œã ã£ãŸã€‚Mambaã¯é¸æŠçš„è¨˜æ†¶ã§ãã‚Œã‚’å…‹æœã—ãŸã€‚ã ãŒå•ã„ãŸã„â€•â€•SSMã ã‘ã§ååˆ†ãªã®ã‹ï¼ŸAttentionã‚’æ¨ã¦ãã‚Œãªã„ç†ç”±ã¯ä½•ã‹ï¼Ÿ**

Mambaã¯é•·è·é›¢ä¾å­˜ã‚’$O(N)$ã§æ‰±ãˆã‚‹ã€‚ã ãŒ**å…¨ç³»åˆ—ã‚’åŒæ™‚ã«å‚ç…§ã™ã‚‹èƒ½åŠ›**(Attentionã®æœ¬è³ª)ã¯æŒãŸãªã„ã€‚Few-shot learningã€æ¨è«–ã‚¿ã‚¹ã‚¯ã€å‹•çš„ãªæ–‡è„ˆåˆ‡ã‚Šæ›¿ãˆã§ã¯ã€AttentionãŒä¾ç„¶ã¨ã—ã¦å„ªä½ã€‚

ç¬¬17å›ã§ã€Mamba-2/SSDãŒã€ŒAttention=SSMã€ã®ç­‰ä¾¡æ€§ã‚’è¨¼æ˜ã—ãŸã“ã¨ã‚’å­¦ã¶ã€‚ã¤ã¾ã‚Š**å¯¾ç«‹ã§ã¯ãªãã€çµ±ä¸€**ã¸å‘ã‹ã£ã¦ã„ã‚‹ã€‚

ç¬¬18å›ã§ã¯ã€Jambaã‚„Zambaã®ã‚ˆã†ã«ã€**Attentionã¨SSMã‚’çµ„ã¿åˆã‚ã›ãŸHybrid**ãŒã€Œæœ€å¼·ã€ã§ã¯ãªãã€Œæœ€é©ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

**å•ã„ç¶šã‘ã‚ˆ**: "æœ€å¼·"ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å­˜åœ¨ã—ãªã„ã€‚ã‚¿ã‚¹ã‚¯ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»è¨ˆç®—è³‡æºã«å¿œã˜ã¦ã€çµ„ã¿åˆã‚ã›ã‚‹ã€‚ãã‚ŒãŒã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æœ¬è³ªã§ã¯ãªã„ã‹ï¼Ÿ

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Gu, A., Dao, T., Ermon, S., Rudra, A., & RÃ©, C. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. *NeurIPS 2020*.
<https://arxiv.org/abs/2008.07669>

[^2]: Gu, A., Goel, K., & RÃ©, C. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. *ICLR 2022*.
<https://arxiv.org/abs/2111.00396>

[^3]: Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. *arXiv:2312.00752*.
<https://arxiv.org/abs/2312.00752>

[^4]: Kalman, R. E. (1960). A New Approach to Linear Filtering and Prediction Problems. *Journal of Basic Engineering*.

[^5]: Tay, Y., Dehghani, M., Abnar, S., et al. (2021). Long Range Arena: A Benchmark for Efficient Transformers. *ICLR 2021*.
<https://arxiv.org/abs/2011.04006>

[^6]: Gu, A., & Dao, T. (2023). Mamba Official Repository.
<https://github.com/state-spaces/mamba>

[^7]: Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. *ICML 2024*.
<https://arxiv.org/abs/2405.21060>

[^8]: Liu, Y., Tian, Y., Zhao, Y., et al. (2024). VMamba: Visual State Space Models.
<https://arxiv.org/abs/2401.10166>

[^9]: Peng, B., Alcaide, E., Anthony, Q., et al. (2023). RWKV: Reinventing RNNs for the Transformer Era.
<https://arxiv.org/abs/2305.13048>

[^10]: Sun, Y., Dong, L., Huang, S., et al. (2023). Retentive Network: A Successor to Transformer for Large Language Models.
<https://arxiv.org/abs/2307.08621>

[^11]: Somvanshi, S., Islam, Md M., et al. (2025). From S4 to Mamba: A Comprehensive Survey on Structured State Space Models. *arXiv:2503.18970*.
<https://arxiv.org/abs/2503.18970>

### æ•™ç§‘æ›¸

- Ogata, K. (2009). *Modern Control Engineering* (5th ed.). Prentice Hall. [åˆ¶å¾¡ç†è«–ã®å¤å…¸]
- Chen, C.-T. (1998). *Linear System Theory and Design* (3rd ed.). Oxford University Press. [çŠ¶æ…‹ç©ºé–“ã®æ•°å­¦]
- Rush, A. (2023). *The Annotated S4*. [å®Ÿè£…ä»˜ãè§£èª¬]
  <https://srush.github.io/annotated-s4/>

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

---
