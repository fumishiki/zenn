---
title: "ç¬¬16å›: SSMç†è«– & Mambaã®å…‹æœ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ¦›"
type: "tech"
topics: ["machinelearning", "deeplearning", "ssm", "rust", "rust"]
published: true
slug: "ml-lecture-16-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

**â† Part1ï¼ˆç†è«–ç·¨ï¼‰**: [ç¬¬16å› Part1](./ml-lecture-16-part1)

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰(45åˆ†) â€” Rustã¨Rustã§SSMã‚’å‹•ã‹ã™

### 4.1 ç’°å¢ƒæ§‹ç¯‰

#### Rustç’°å¢ƒ

```bash
# Rust (cargo 1.75+, ndarray 0.16)
curl -fsSL https://install.julialang.org | sh

# Packages
julia -e 'using Pkg; Pkg.add(["LinearAlgebra", "FFTW", "Plots", "DifferentialEquations", "ProgressMeter"])'
```

#### Rustç’°å¢ƒ

```bash
# Rust 1.83+ (2026)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Dependencies in Cargo.toml
[dependencies]
ndarray = "0.16"
ndarray-linalg = "0.17"
rayon = "1.10"
```

### 4.2 é›¢æ•£SSMã®å®Œå…¨å®Ÿè£…(Rust)

```rust
use ndarray::{Array1, Array2};
use num_complex::Complex;
use rustfft::FftPlanner;
use rand::prelude::*;
use rand_distr::StandardNormal;

/// Trait for discrete state-space model forward passes.
pub trait StateSpaceModel {
    /// Recurrent inference: y_t = C h_t,  h_t = A h_{t-1} + B u_t
    fn forward_recurrent(&self, u: &[f64]) -> Vec<f64>;
    /// Convolutional training: y = KÌ„ * u,  KÌ„_k = C Ä€áµ BÌ„
    fn forward_conv(&self, u: &[f64], l: usize) -> Vec<f64>;
}

/// Discrete SSM: h_t = Ä€ h_{t-1} + BÌ„ u_t,  y_t = C h_t + D u_t
struct DiscreteSSM {
    a: Array2<f64>,
    b: Array1<f64>,
    c: Array1<f64>,
    d: f64,
}

impl DiscreteSSM {
    // Recurrent form (for inference â€” inherently sequential)
    fn forward_recurrent(&self, u: &[f64]) -> Vec<f64> {
        let mut h = Array1::<f64>::zeros(self.b.len());
        u.iter().map(|&u_t| {
            // h_t = Ä€ h_{t-1} + BÌ„ u_t
            h = self.a.dot(&h) + &self.b * u_t;
            // y_t = C h_t + D u_t
            self.c.dot(&h) + self.d * u_t
        }).collect()
    }

    // Convolutional form (for training)
    fn forward_convolution(&self, u: &[f64], l: usize) -> (Vec<f64>, Vec<f64>) {
        // SSM convolution kernel: KÌ„_k = C Ä€áµ BÌ„,  k = 0..L
        let state_dim = self.b.len();
        let mut ai = Array2::<f64>::eye(state_dim); // Ä€â° = I
        let kernel: Vec<f64> = (0..l).map(|_| {
            ai = self.a.dot(&ai); // Ä€áµâºÂ¹
            self.c.dot(&ai.dot(&self.b))  // KÌ„_k = C Ä€áµ BÌ„
        }).collect();

        // FFT convolution (zero-padded to avoid circular aliasing)
        let pad_len = l + u.len();
        let mut k_pad: Vec<Complex<f64>> = kernel.iter().map(|&x| Complex::new(x, 0.0))
            .chain(std::iter::repeat(Complex::new(0.0, 0.0)).take(u.len()))
            .collect();
        let mut u_pad: Vec<Complex<f64>> = u.iter().map(|&x| Complex::new(x, 0.0))
            .chain(std::iter::repeat(Complex::new(0.0, 0.0)).take(l))
            .collect();
        let mut planner = FftPlanner::new();
        let fft  = planner.plan_fft_forward(pad_len);
        let ifft = planner.plan_fft_inverse(pad_len);
        fft.process(&mut k_pad);
        fft.process(&mut u_pad);
        let mut conv: Vec<Complex<f64>> = k_pad.iter().zip(&u_pad).map(|(a, b)| a * b).collect();
        ifft.process(&mut conv);
        let scale = 1.0 / pad_len as f64;
        let y: Vec<f64> = conv[..u.len()].iter().map(|c| c.re * scale).collect();
        (y, kernel)
    }
}

fn main() {
    let mut rng = rand::thread_rng();
    let d = 8_usize;
    // stable matrix: 0.9 * I + 0.05 * randn
    let noise: Array2<f64> = Array2::from_shape_fn((d, d), |_| rng.sample::<f64, _>(StandardNormal) * 0.05);
    let a = Array2::<f64>::eye(d) * 0.9 + noise;
    let b: Array1<f64> = (0..d).map(|_| rng.sample(StandardNormal)).collect();
    let c: Array1<f64> = (0..d).map(|_| rng.sample(StandardNormal)).collect();
    let ssm = DiscreteSSM { a, b, c, d: 0.0 };

    let u: Vec<f64> = (0..64).map(|_| rng.sample(StandardNormal)).collect();
    let y_rec = ssm.forward_recurrent(&u);
    let (y_conv, _k) = ssm.forward_convolution(&u, 64);

    println!("Recurrent output (first 5): {:?}", &y_rec[..5]);
    println!("Convolution output (first 5): {:?}", &y_conv[..5]);
    let max_diff = y_rec.iter().zip(&y_conv)
        .map(|(a, b)| (a - b).abs())
        .fold(f64::NEG_INFINITY, f64::max);
    println!("Max difference: {max_diff:.6}");
}
```

### 4.3 HiPPO-LegSåˆæœŸåŒ–

```rust
use ndarray::{Array1, Array2};
use ndarray_linalg::Eig;

/// HiPPO-LegS initialization for A and B.
/// Returns matrices with optimal long-range memory properties.
// HiPPO-LegS: A_{nk} = -âˆš(2n+1)Â·âˆš(2k+1)  (n>k),  A_{nn} = n+1,  B_n = âˆš(2n+1)
fn hippo_legs_init(d: usize) -> (Array2<f64>, Array1<f64>, Array1<f64>) {
    // A_{nk} = -âˆš(2n+1)âˆš(2k+1) for n>k,  A_{nn} = n+1,  0 otherwise
    let a = Array2::from_shape_fn((d, d), |(n, k)| {
        if n > k {
            -((2 * n + 1) as f64).sqrt() * ((2 * k + 1) as f64).sqrt()
        } else if n == k {
            (n + 1) as f64
        } else {
            0.0
        }
    });
    // B_n = âˆš(2n+1)
    let b: Array1<f64> = (0..d).map(|n| ((2 * n + 1) as f64).sqrt()).collect();
    let c = Array1::<f64>::ones(d);
    (a, b, c)
}

fn main() {
    let d = 16;
    let (a_hippo, _b_hippo, _c_hippo) = hippo_legs_init(d);
    let (eigs, _) = a_hippo.eig().expect("eigendecomposition failed");
    let real_parts: Vec<f64> = eigs.iter().map(|c| c.re).collect();
    println!("HiPPO eigenvalues (real parts): {real_parts:.2?}");
    println!("All negative? {}", real_parts.iter().all(|&r| r < 0.0));
}
```

### 4.4 Zero-Order Hold é›¢æ•£åŒ–

```rust
use ndarray::{array, Array1, Array2};
use ndarray_linalg::{Eig, Expm, Solve};

/// Zero-Order Hold (ZOH) discretization: continuous SSM â†’ discrete SSM.
///   Ä€ = exp(AÂ·Î”)                        [matrix exponential]
///   BÌ„ = Aâ»Â¹Â·(Ä€ - I)Â·B                  [ZOH exact formula]
// Discretization: Ä€ = exp(Î”Â·A),  BÌ„ = (Ä€ - I)Aâ»Â¹B  (ZOH)
fn discretize_zoh(
    a: &Array2<f64>,
    b: &Array1<f64>,
    delta: f64,
) -> (Array2<f64>, Array1<f64>) {
    // Ä€ = exp(AÂ·Î”)
    let a_bar = (a * delta).expm().expect("matrix exponential failed");
    let i = Array2::<f64>::eye(a.nrows());
    // BÌ„ = Aâ»Â¹Â·(Ä€ - I)Â·B  (exact ZOH; fallback to numerical integration if A singular)
    let b_bar = a.solve(&(&a_bar - &i))
        .map(|m| m.dot(b))
        .unwrap_or_else(|_| {
            let dt = delta / 100.0;
            (0..100).map(|k| {
                let t = k as f64 * dt;
                (a * t).expm().expect("expm failed").dot(b) * dt
            }).fold(Array1::<f64>::zeros(b.len()), |acc, x| acc + x)
        });
    (a_bar, b_bar)
}

fn main() {
    let a_cont = array![[-0.5, 0.0], [0.0, -0.3]];
    let b_cont = array![1.0, 0.0];
    let delta = 0.1_f64;

    let (a_disc, _b_disc) = discretize_zoh(&a_cont, &b_cont, delta);
    let (eig_cont, _) = a_cont.eig().unwrap();
    let (eig_disc, _) = a_disc.eig().unwrap();
    println!("Continuous A eigenvalues: {:?}", eig_cont);
    println!("Discrete A eigenvalues:   {:?}", eig_disc);
    let expected: Vec<_> = eig_cont.iter().map(|c| (c * delta).exp()).collect();
    println!("Expected (exp(Î»*Î”)):      {:?}", expected);
}
```

### 4.5 S4 Simplified: å¯¾è§’SSM + FFTç•³ã¿è¾¼ã¿

```rust
use ndarray::Array1;
use num_complex::Complex;
use rustfft::FftPlanner;
use rand::prelude::*;
use rand_distr::StandardNormal;

/// Simplified S4: diagonal A for efficiency.
/// Assumes A is diagonalizable: A = V Î› V^{-1}
struct S4Layer {
    lambda: Array1<Complex<f64>>, // diagonal of A (eigenvalues)
    b: Array1<Complex<f64>>,
    c: Array1<Complex<f64>>,
    delta: f64,
}

impl S4Layer {
    fn forward(&self, u: &[f64], l: usize) -> Vec<f64> {
        // Î»Ì„ = exp(Î»Â·Î”)  â€” discretized diagonal eigenvalues
        let lambda_bar: Array1<Complex<f64>> = self.lambda.mapv(|lam| (lam * self.delta).exp());
        // KÌ„_k = Re(Cáµ€Â·diag(Î»Ì„áµ)Â·B)  â€” SSM convolution kernel via diagonal trick
        let kernel: Vec<f64> = (0..l).map(|k| {
            let lam_k: Array1<Complex<f64>> = lambda_bar.mapv(|lb| lb.powi(k as i32));  // Î»Ì„áµ
            self.c.iter().zip(&self.b).zip(&lam_k)
                .map(|((ci, bi), lki)| ci * lki * bi)  // cáµ¢Â·Î»Ì„áµ¢áµÂ·báµ¢
                .sum::<Complex<f64>>()
                .re  // take real part
        }).collect();

        // FFT convolution (zero-padded)
        let pad_len = kernel.len() + u.len();
        let mut k_pad: Vec<Complex<f64>> = kernel.iter().map(|&x| Complex::new(x, 0.0))
            .chain(std::iter::repeat(Complex::new(0.0, 0.0)).take(u.len()))
            .collect();
        let mut u_pad: Vec<Complex<f64>> = u.iter().map(|&x| Complex::new(x, 0.0))
            .chain(std::iter::repeat(Complex::new(0.0, 0.0)).take(l))
            .collect();
        let mut planner = FftPlanner::new();
        let fft  = planner.plan_fft_forward(pad_len);
        let ifft = planner.plan_fft_inverse(pad_len);
        fft.process(&mut k_pad);
        fft.process(&mut u_pad);
        let mut conv: Vec<Complex<f64>> = k_pad.iter().zip(&u_pad).map(|(a, b)| a * b).collect();
        ifft.process(&mut conv);
        let scale = 1.0 / pad_len as f64;
        conv[..u.len()].iter().map(|c| c.re * scale).collect()
    }
}

fn main() {
    let mut rng = rand::thread_rng();
    let d = 32_usize;
    // HiPPO-like eigenvalues: -1, -2, ..., -d
    let lambda: Array1<Complex<f64>> = (1..=d).map(|i| Complex::new(-(i as f64), 0.0)).collect();
    let inv_sqrt_d = 1.0 / (d as f64).sqrt();
    let b = Array1::from_elem(d, Complex::new(inv_sqrt_d, 0.0));
    let c = Array1::from_elem(d, Complex::new(inv_sqrt_d, 0.0));
    let s4 = S4Layer { lambda, b, c, delta: 0.01 };

    let u: Vec<f64> = (0..256).map(|_| rng.sample(StandardNormal)).collect();
    let y_s4 = s4.forward(&u, 256);
    println!("S4 output (first 5): {:?}", &y_s4[..5]);
}
```

### 4.6 Mambaã®ç°¡æ˜“å®Ÿè£…: Selective SSM

å®Œå…¨ãªMambaã¯CUDAã‚«ãƒ¼ãƒãƒ«ã‚’è¦ã™ã‚‹ãŒã€æ•™è‚²çš„ãªç°¡æ˜“ç‰ˆ:

```rust
use ndarray::{Array1, Array2};
use ndarray_linalg::{Expm, Solve};
use rand::prelude::*;
use rand_distr::StandardNormal;

/// Simplified Mamba: input-dependent Î”, B, C (without hardware-aware scan)
struct MambaLayer {
    a: Array2<f64>,
    w_delta: Array2<f64>,
    w_b: Array2<f64>,
    w_c: Array2<f64>,
    d_state: usize,
}

/// Numerically stable softplus: log1p(exp(x)) â‰ˆ x for x > 20
fn softplus(x: f64) -> f64 {
    if x > 20.0 { x } else { x.exp().ln_1p() }
}

impl MambaLayer {
    // Mamba forward: u: (L, d_model) â†’ y: (L,)
    // Selective SSM: Î”_t, B_t, C_t are input-dependent (unlike S4)
    fn forward(&self, u: &Array2<f64>) -> Vec<f64> {
        let (l, _) = u.dim();
        let d = self.d_state;
        // Î”_t = Softplus(W_Î” u_t)  â€” input-dependent step size (L, d_state)
        let delta = u.dot(&self.w_delta.t()).mapv(softplus);
        // B_t = W_B u_t  â€” input-dependent input projection (L, d_state)
        let b_mat = u.dot(&self.w_b.t());
        // C_t = W_C u_t  â€” input-dependent output projection (L, d_state)
        let c_mat = u.dot(&self.w_c.t());

        // Sequential scan: h_t = Ä€_t h_{t-1} + BÌ„_t,  y_t = C_tÂ·h_t
        let mut h = Array1::<f64>::zeros(d);
        let i_mat = Array2::<f64>::eye(d);
        (0..l).map(|t| {
            let dt = delta[[t, 0]]; // scalar Î”_t per step
            // Ä€_t = exp(Î”_tÂ·A)  â€” ZOH discretization (input-dependent)
            let a_bar = (&self.a * dt).expm().expect("expm failed");
            let a_bar_minus_i = &a_bar - &i_mat;
            let b_t = b_mat.row(t).to_owned();
            // BÌ„_t = Aâ»Â¹Â·(Ä€_t - I)Â·B_t  â€” ZOH for input matrix
            let b_bar = self.a.solve(&a_bar_minus_i)
                .map(|m| m.dot(&b_t))
                .unwrap_or_else(|_| a_bar_minus_i.dot(&b_t));
            // h_t = Ä€_t h_{t-1} + BÌ„_t
            h = a_bar.dot(&h) + b_bar;
            // y_t = C_t Â· h_t
            c_mat.row(t).dot(&h)
        }).collect()
    }
}

fn main() {
    let mut rng = rand::thread_rng();
    let (d_state, d_model) = (4_usize, 8_usize);
    let a = -Array2::<f64>::eye(d_state); // Simple: -I
    let w_delta = Array2::from_shape_fn((d_model, d_state), |_| rng.sample::<f64, _>(StandardNormal) * 0.1);
    let w_b     = Array2::from_shape_fn((d_model, d_state), |_| rng.sample(StandardNormal));
    let w_c     = Array2::from_shape_fn((d_model, d_state), |_| rng.sample(StandardNormal));
    let mamba = MambaLayer { a, w_delta, w_b, w_c, d_state };

    let u: Array2<f64> = Array2::from_shape_fn((16, d_model), |_| rng.sample(StandardNormal));
    let y_mamba = mamba.forward(&u);
    println!("Mamba output (first 5): {:?}", &y_mamba[..5]);
}
```

> **Note:** **æ³¨æ„**: ä¸Šè¨˜ã¯Mambaã®åŸç†ã‚’ç¤ºã™æ•™è‚²çš„å®Ÿè£…ã€‚å®Ÿéš›ã®Mambaã¯:
> 1. Parallel Scanã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–
> 2. CUDAã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–(hardware-aware scan)
> 3. è¤‡æ•°ã®Mambaãƒ–ãƒ­ãƒƒã‚¯ã‚’ç©å±¤
> ãŒå¿…è¦ã€‚æœ¬æ ¼çš„å®Ÿè£…ã¯å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª[^6]ã‚’å‚ç…§ã€‚

### 4.7 Rustã§ã®ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè£…

```rust
// Cargo.toml
// [dependencies]
// ndarray = "0.16"
// rayon = "1.10"

use ndarray::{Array1, Array2};
use rayon::prelude::*;

/// Sequential scan for SSM: h_t = A_tÂ·h_{t-1} + B_t
/// Returns all hidden states h[1..=L]
// SSM scan: h_t = A_t h_{t-1} + B_t  (associative â†’ parallelizable with Blelloch scan)
fn parallel_scan(a_mats: &[Array2<f64>], b_vecs: &[Array1<f64>]) -> Vec<Array1<f64>> {
    let d = b_vecs[0].len();
    let mut h = Array1::zeros(d);
    // zip (A_t, B_t) pairs, map h_t = A_t h_{t-1} + B_t via iterator
    a_mats.iter().zip(b_vecs.iter()).map(|(a, b)| {
        h = a.dot(&h) + b;
        h.clone()
    }).collect()
}

fn main() {
    let (l, d) = (8, 2);
    // A[t] = 0.9 * I, B[t] = [1.0, 0.5]
    let a_mats: Vec<Array2<f64>> = (0..l).map(|_| Array2::eye(d) * 0.9).collect();
    let b_vecs: Vec<Array1<f64>> = (0..l).map(|_| Array1::from_vec(vec![1.0, 0.5])).collect();

    let h = parallel_scan(&a_mats, &b_vecs);
    h.iter().enumerate().for_each(|(t, h_t)| println!("h[{}] = {:?}", t + 1, h_t));
}
```

çœŸã®ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã¯`rayon`ã®prefix sumãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ã†ãŒã€associative operationã®å®šç¾©ãŒå¿…è¦ã€‚è©³ç´°ã¯[^3]ã®Appendixã€‚

#### Rustä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã®ç†è«–çš„èƒŒæ™¯

**Associative Scan**ã®åŸç†: æ¼”ç®—$\circ$ãŒçµåˆçš„($(a \circ b) \circ c = a \circ (b \circ c)$)ãªã‚‰ã€äºŒåˆ†æœ¨æ§‹é€ ã§ä¸¦åˆ—è¨ˆç®—å¯èƒ½ã€‚

SSMã®å ´åˆ:

$$
(A_2, B_2) \circ (A_1, B_1) = (A_2 A_1, A_2 B_1 + B_2)
$$

ã“ã®æ¼”ç®—ã¯çµåˆçš„:

$$
\begin{aligned}
&((A_3, B_3) \circ (A_2, B_2)) \circ (A_1, B_1) \\
&= (A_3 A_2, A_3 B_2 + B_3) \circ (A_1, B_1) \\
&= (A_3 A_2 A_1, A_3 A_2 B_1 + A_3 B_2 + B_3)
\end{aligned}
$$

$$
\begin{aligned}
&(A_3, B_3) \circ ((A_2, B_2) \circ (A_1, B_1)) \\
&= (A_3, B_3) \circ (A_2 A_1, A_2 B_1 + B_2) \\
&= (A_3 A_2 A_1, A_3(A_2 B_1 + B_2) + B_3) \\
&= (A_3 A_2 A_1, A_3 A_2 B_1 + A_3 B_2 + B_3)
\end{aligned}
$$

ä¸€è‡´ã™ã‚‹ $\square$

**ä¸¦åˆ—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
Level 0: [(A1,B1), (A2,B2), (A3,B3), (A4,B4), (A5,B5), (A6,B6), (A7,B7), (A8,B8)]
         â†“ Parallel combine pairs
Level 1: [(A2A1, A2B1+B2), (A4A3, A4B3+B4), (A6A5, A6B5+B6), (A8A7, A8B7+B8)]
         â†“ Parallel combine pairs
Level 2: [(A4A3A2A1, ...), (A8A7A6A5, ...)]
         â†“ Parallel combine
Level 3: [(A8A7A6A5A4A3A2A1, ...)]
```

æ·±ã•$\log_2 L$ã€ç·work $O(L)$ã€‚

```rust
use rayon::prelude::*;

/// Associative operation for SSM scan: (A_r, B_r) âˆ˜ (A_l, B_l) = (A_r A_l, A_r B_l + B_r)
type ScanOp = (Array2<f64>, Array1<f64>);

// (A_r, B_r) âˆ˜ (A_l, B_l) = (A_r A_l,  A_r B_l + B_r)   [associative SSM composition]
fn combine(left: &ScanOp, right: &ScanOp) -> ScanOp {
    let (a_l, b_l) = left;
    let (a_r, b_r) = right;
    (a_r.dot(a_l), a_r.dot(b_l) + b_r)
}

/// Sequential CPU scan expressed as iterator map over owned ops
/// (True Blelloch parallel scan requires GPU/CUDA; this is the sequential reference)
fn parallel_scan_associative(ops: Vec<ScanOp>) -> Vec<Array1<f64>> {
    let d = ops[0].1.len();
    let mut h = Array1::zeros(d);
    // h_t = A_t h_{t-1} + B_t  via iterator (equivalent to fold)
    ops.into_iter().map(|(a, b)| {
        h = a.dot(&h) + &b;  // h_t = A_t h_{t-1} + B_t
        h.clone()
    }).collect()
}
```

**æ³¨æ„**: CPUã§ã®ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã¯ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¤§ããã€ç´ æœ´ãªé€æ¬¡å®Ÿè£…ã«åŠ£ã‚‹ã“ã¨ãŒå¤šã„ã€‚**GPUã‚„TPUã§ã¯åŠ‡çš„ã«é«˜é€ŸåŒ–**ã™ã‚‹ã€‚Mambaã¯Tritonã§CUDAã‚«ãƒ¼ãƒãƒ«ã‚’æ›¸ã„ã¦ã„ã‚‹[^3]ã€‚

#### Cargo.tomlã®å®Œå…¨ç‰ˆ

```toml
[package]
name = "ssm_rust"
version = "0.1.0"
edition = "2021"

[dependencies]
ndarray = "0.16"
ndarray-linalg = { version = "0.17", features = ["openblas-static"] }
rayon = "1.10"
num-complex = "0.4"
approx = "0.5"  # for testing

[dev-dependencies]
criterion = "0.5"

[[bench]]
name = "ssm_bench"
harness = false
```

#### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š

```rust
// benches/ssm_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use ssm_rust::parallel_scan;

fn bench_ssm_scan(c: &mut Criterion) {
    let (l, d) = (1024_usize, 64_usize);
    let a_mats: Vec<_> = (0..l).map(|_| Array2::eye(d) * 0.9).collect();
    let b_vecs: Vec<_> = (0..l).map(|_| Array1::from_vec(vec![1.0; d])).collect();

    c.bench_function("ssm_scan_1024", |b| {
        b.iter(|| parallel_scan(black_box(&a_mats), black_box(&b_vecs)))
    });
}

criterion_group!(benches, bench_ssm_scan);
criterion_main!(benches);
```

å®Ÿè¡Œ:
```bash
cargo bench
```

### 4.8 Mathâ†”Codeå¯¾å¿œè¡¨: SSMã®å®Œå…¨ãƒãƒƒãƒ”ãƒ³ã‚°

| æ•°å¼ | Rust | Rust | èª¬æ˜ |
|:-----|:------|:-----|:-----|
| $h_t = \bar{A}h_{t-1} + \bar{B}u_t$ | `h = A * h + B * u[t]` | `h = A.dot(&h) + &B * u[t]` | å†å¸°æ›´æ–° |
| $y_t = Ch_t$ | `y[t] = dot(C, h)` | `y[t] = C.dot(&h)` | å‡ºåŠ›æŠ•å½± |
| $\bar{A} = e^{A\Delta}$ | `A_bar = exp(A * Î”)` | `A_bar = A.mapv(\|x\| (x*Î”).exp())` (diagonal) | é›¢æ•£åŒ– |
| $\bar{B} = (A^{-1}(e^{A\Delta}-I))B$ | `B_bar = (A \ (A_bar - I)) * B` | `B_bar = A.inv()?.dot(&(A_bar - I)).dot(&B)` | é›¢æ•£åŒ– |
| $\bar{\mathcal{K}}_k = C\bar{A}^kB$ | `K[k] = dot(C, (A^k) * B)` | `K[k] = C.dot(&A.pow(k)).dot(&B)` | ã‚«ãƒ¼ãƒãƒ« |
| $y = \bar{\mathcal{K}} * u$ | `y = real.(ifft(fft(K) .* fft(u)))` | `y = ifft(fft(K) * fft(u))` | FFTç•³ã¿è¾¼ã¿ |
| $\Delta_t = \text{Softplus}(W_\Delta u_t)$ | `Î” = softplus.(u * W_Î”')` | `Î” = (u.dot(&W_Î”)).mapv(softplus)` | Mamba |
| $(A_2, B_2) \circ (A_1, B_1)$ | `(A2*A1, A2*B1 + B2)` | `(A2.dot(&A1), A2.dot(&B1) + B2)` | Scanæ¼”ç®— |

**1å¯¾1å¯¾å¿œã®å¾¹åº•**: å…¨ã¦ã®æ•°å¼ãŒã€ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œè¡Œã¨ä¸€è‡´ã™ã‚‹ã€‚èª­è€…ã¯ã€Œã“ã®è¡Œ = ã“ã®æ•°å¼ã€ã¨å³åº§ã«ç†è§£ã§ãã‚‹ã€‚

### 4.9 ãƒ‡ãƒãƒƒã‚°ã¨æ•°å€¤å®‰å®šæ€§ã®Tips

#### Tip 1: è¡Œåˆ—æŒ‡æ•°é–¢æ•°ã®è¨ˆç®—

`exp(A * Î”)`ã¯æ•°å€¤çš„ã«ä¸å®‰å®šãªå ´åˆãŒã‚ã‚‹ã€‚ç‰¹ã«$A$ã®å›ºæœ‰å€¤ãŒå¤§ãã„ã¨ãã€‚

**å¯¾ç­–**: PadÃ©è¿‘ä¼¼ã‚„SciPyã®`expm`ã‚’ä½¿ã†ã€‚

```rust
use ndarray::Array2;
use ndarray_linalg::Expm;

/// Safe matrix exponential â€” warns if A is ill-conditioned.
fn safe_exp(a: &Array2<f64>, delta: f64) -> Array2<f64> {
    // rough ill-conditioning check via norm ratio
    let norm = a.iter().map(|x| x * x).sum::<f64>().sqrt();
    if norm > 1e10 {
        eprintln!("Warning: Matrix A is ill-conditioned, exp(A*Î”) may be inaccurate");
    }
    (a * delta).expm().expect("matrix exponential failed")
}
```

#### Tip 2: å›ºæœ‰å€¤ã®ç¢ºèª

è¨“ç·´å‰ã«$A$ã®å›ºæœ‰å€¤ã‚’ç¢ºèªã—ã€å®Ÿéƒ¨ãŒæ­£ã®ã‚‚ã®ãŒã‚ã‚Œã°è­¦å‘Šã€‚

```rust
use ndarray::Array2;
use ndarray_linalg::Eig;
use num_complex::Complex;

/// Returns true if all eigenvalues of A have strictly negative real parts.
fn check_stability(a: &Array2<f64>) -> bool {
    let (eigs, _) = a.eig().expect("eigendecomposition failed");
    let unstable: Vec<Complex<f64>> = eigs.into_iter().filter(|c| c.re > 0.0).collect();
    if !unstable.is_empty() {
        eprintln!("Warning: Unstable eigenvalues detected: {:?}", unstable);
    }
    unstable.is_empty()
}
```

#### Tip 3: Softplusã®æ•°å€¤å®‰å®šç‰ˆ

$\text{Softplus}(x) = \log(1 + e^x)$ã¯$x$ãŒå¤§ãã„ã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã€‚

```rust
/// Numerically stable softplus: avoids overflow for large x.
#[inline]
fn softplus_stable(x: f64) -> f64 {
    if x > 20.0 { x } else { x.exp().ln_1p() }
}
```

#### Tip 4: FFTã®zero-padding

ç•³ã¿è¾¼ã¿ã§FFTã‚’ä½¿ã†éš›ã€circular convolutionã‚’é¿ã‘ã‚‹ãŸã‚ã€ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¿…é ˆã€‚

```rust
use num_complex::Complex;
use rustfft::FftPlanner;

/// Correct FFT convolution with zero-padding to avoid circular aliasing.
fn fft_conv_correct(kernel: &[f64], u: &[f64]) -> Vec<f64> {
    let l_pad = kernel.len() + u.len() - 1;
    let mut k_pad: Vec<Complex<f64>> = kernel.iter().map(|&x| Complex::new(x, 0.0))
        .chain(std::iter::repeat(Complex::new(0.0, 0.0)).take(l_pad - kernel.len()))
        .collect();
    let mut u_pad: Vec<Complex<f64>> = u.iter().map(|&x| Complex::new(x, 0.0))
        .chain(std::iter::repeat(Complex::new(0.0, 0.0)).take(l_pad - u.len()))
        .collect();
    let mut planner = FftPlanner::new();
    let fft  = planner.plan_fft_forward(l_pad);
    let ifft = planner.plan_fft_inverse(l_pad);
    fft.process(&mut k_pad);
    fft.process(&mut u_pad);
    let mut conv: Vec<Complex<f64>> = k_pad.iter().zip(&u_pad).map(|(a, b)| a * b).collect();
    ifft.process(&mut conv);
    let scale = 1.0 / l_pad as f64;
    conv[..u.len()].iter().map(|c| c.re * scale).collect()
}
```

> **Note:** **é€²æ—: 70% å®Œäº†** SSM/S4/Mambaã®å®Ÿè£…ã‚’å®Œäº†ã€‚Rustæ•°å¼ç¾ã¨Rustä¸¦åˆ—åŒ–ã€ãã—ã¦Mathâ†”Codeå®Œå…¨å¯¾å¿œã‚’ä½“é¨“ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã§æ€§èƒ½ã‚’ç¢ºèªã€‚

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼(30åˆ†) â€” Long Range Arenaã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

æ¬¡ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆ:

<details><summary>Q1: $h_t = \bar{A} h_{t-1} + \bar{B} u_t$</summary>

**èª­ã¿**: "h sub t equals A bar times h sub t minus 1 plus B bar times u sub t"
**æ„å‘³**: é›¢æ•£SSMã®å†å¸°æ›´æ–°å¼ã€‚éš ã‚ŒçŠ¶æ…‹$h_t$ã¯ã€å‰æ™‚åˆ»ã®çŠ¶æ…‹$h_{t-1}$ã‚’è¡Œåˆ—$\bar{A}$ã§å¤‰æ›ã—ã€å…¥åŠ›$u_t$ã‚’$\bar{B}$ã§æŠ•å½±ã—ãŸå’Œã€‚

</details>

<details><summary>Q2: $\bar{\mathcal{K}}_k = C \bar{A}^k \bar{B}$</summary>

**èª­ã¿**: "K bar sub k equals C times A bar to the power k times B bar"
**æ„å‘³**: SSMç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«ã®ç¬¬$k$è¦ç´ ã€‚$k$ã‚¹ãƒ†ãƒƒãƒ—å‰ã®å…¥åŠ›ãŒç¾åœ¨ã®å‡ºåŠ›ã«ä¸ãˆã‚‹å½±éŸ¿åº¦ã€‚$\bar{A}^k$ã«ã‚ˆã‚ŠæŒ‡æ•°æ¸›è¡°ã€‚

</details>

<details><summary>Q3: $A_{\text{HiPPO}} = \Lambda - PQ^*$</summary>

**èª­ã¿**: "A HiPPO equals Lambda minus P Q dagger"
**æ„å‘³**: HiPPOè¡Œåˆ—ã®DPLRåˆ†è§£ã€‚$\Lambda$ã¯å¯¾è§’(å›ºæœ‰å€¤)ã€$-PQ^*$ã¯ä½ãƒ©ãƒ³ã‚¯è£œæ­£ã€‚$Q^*$ã¯$Q$ã®å…±å½¹è»¢ç½®ã€‚

</details>

<details><summary>Q4: $\Delta_t = \text{Softplus}(W_\Delta u_t + b_\Delta)$</summary>

**èª­ã¿**: "Delta sub t equals softplus of W Delta u sub t plus b Delta"
**æ„å‘³**: Mambaã®å…¥åŠ›ä¾å­˜æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—å¹…ã€‚Softplusã§$\Delta_t > 0$ã‚’ä¿è¨¼ã€‚å…¥åŠ›ã«ã‚ˆã‚Šé›¢æ•£åŒ–ã®ç´°ã‹ã•ãŒå¤‰åŒ–ã€‚

</details>

<details><summary>Q5: $(A_2, B_2) \circ (A_1, B_1) = (A_2 A_1, A_2 B_1 + B_2)$</summary>

**èª­ã¿**: "A two, B two circle A one, B one equals A two A one, A two B one plus B two"
**æ„å‘³**: Parallel Scanã®çµåˆæ¼”ç®—å­ã€‚2ã¤ã®ç·šå½¢å¤‰æ›$(A, B)$ã‚’åˆæˆã€‚$h_2 = A_2(A_1 h_0 + B_1) + B_2 = A_2A_1 h_0 + (A_2B_1 + B_2)$ã‚’è¡¨ã™ã€‚

</details>

### 5.2 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

#### Challenge 1: HiPPO vs Random initialization

HiPPOåˆæœŸåŒ–ã¨ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§SSMã‚’è¨“ç·´ã—ã€Long Rangeä¾å­˜ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’æ¯”è¼ƒã›ã‚ˆã€‚

```rust
use ndarray::{Array1, Array2};
use rand::prelude::*;
use rand_distr::{StandardNormal, Uniform};

/// Generate synthetic copy task: signal placed at a random delay, then zeros.
/// Returns (X, Y) where X is (n_samples Ã— T) and Y holds the target label.
fn generate_copy_task(
    t: usize,
    n_samples: usize,
    vocab_size: usize,
) -> (Array2<f32>, Vec<usize>) {
    let mut rng = rand::thread_rng();
    let mut x = Array2::<f32>::zeros((n_samples, t));
    let mut y = vec![0usize; n_samples];
    for i in 0..n_samples {
        let signal: usize = rng.sample(Uniform::new(1, vocab_size + 1));
        let delay: usize  = rng.sample(Uniform::new(5, 11));
        x[[i, delay]] = signal as f32;
        y[i] = signal;
    }
    (x, y)
}

/// Minimal SSM classifier: fixed A/B/C weights, linear output head.
struct SsmClassifier {
    a: Array2<f64>,
    b: Array1<f64>,
    c: Array1<f64>,
    w_out: Array2<f32>, // (num_classes, d_state)
}

impl SsmClassifier {
    /// Run recurrent SSM on one sample row, return logits over vocab.
    fn predict_one(&self, x_row: &[f32]) -> Vec<f32> {
        let mut h = Array1::<f64>::zeros(self.b.len());
        for &u_t in x_row {
            h = self.a.dot(&h) + &self.b * u_t as f64;
        }
        let h_f32: Array1<f32> = h.mapv(|v| v as f32);
        self.w_out.dot(&h_f32).to_vec()
    }
}

fn argmax(v: &[f32]) -> usize {
    v.iter().enumerate()
        .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
        .map(|(i, _)| i)
        .unwrap_or(0)
}

/// Simplified 0-1 loss evaluation (no gradient updates â€” demo only).
fn train_ssm_copy(
    model: &SsmClassifier,
    x_train: &Array2<f32>,
    y_train: &[usize],
    epochs: usize,
) -> Vec<f32> {
    let n = x_train.nrows();
    (0..epochs).map(|epoch| {
        let total_loss: f32 = (0..n).map(|i| {
            let logits = model.predict_one(x_train.row(i).as_slice().unwrap());
            if argmax(&logits) == y_train[i] { 0.0 } else { 1.0 }
        }).sum::<f32>();
        let avg_loss = total_loss / n as f32;
        if (epoch + 1) % 10 == 0 {
            println!("Epoch {}: Loss = {:.3}, Acc = {:.1}%",
                epoch + 1, avg_loss, (1.0 - avg_loss) * 100.0);
        }
        avg_loss
    }).collect()
}

fn test_accuracy(model: &SsmClassifier, x: &Array2<f32>, y: &[usize]) -> f64 {
    let n = x.nrows();
    let correct = (0..n).filter(|&i| {
        let logits = model.predict_one(x.row(i).as_slice().unwrap());
        argmax(&logits) == y[i]
    }).count();
    correct as f64 / n as f64
}

fn experiment_hippo_vs_random() -> (Vec<f32>, Vec<f32>) {
    let (t, n_train, n_test, d, vocab_size) = (500, 1000, 200, 32, 10);
    let delta = 0.01_f64;
    let mut rng = rand::thread_rng();

    let (x_train, y_train) = generate_copy_task(t, n_train, vocab_size);
    let (x_test,  y_test)  = generate_copy_task(t, n_test,  vocab_size);

    // Model 1: HiPPO init
    let (a_hippo, b_hippo, c_hippo) = hippo_legs_init(d);
    let (a_bar_h, b_bar_h) = discretize_zoh(&a_hippo, &b_hippo, delta);
    let w_out_h: Array2<f32> = Array2::from_shape_fn((vocab_size, d), |_|
        rng.sample::<f32, _>(StandardNormal) * 0.01);
    let model_hippo = SsmClassifier { a: a_bar_h, b: b_bar_h, c: c_hippo, w_out: w_out_h };

    // Model 2: Random init
    let a_rand: Array2<f64> = Array2::from_shape_fn((d, d), |_| rng.sample::<f64, _>(StandardNormal) * 0.01);
    let b_rand: Array1<f64> = (0..d).map(|_| rng.sample::<f64, _>(StandardNormal) * 0.1).collect();
    let c_rand: Array1<f64> = (0..d).map(|_| rng.sample::<f64, _>(StandardNormal) * 0.1).collect();
    let (a_bar_r, b_bar_r) = discretize_zoh(&a_rand, &b_rand, delta);
    let w_out_r: Array2<f32> = Array2::from_shape_fn((vocab_size, d), |_|
        rng.sample::<f32, _>(StandardNormal) * 0.01);
    let model_random = SsmClassifier { a: a_bar_r, b: b_bar_r, c: c_rand, w_out: w_out_r };

    println!("Training HiPPO-initialized SSM...");
    let losses_hippo  = train_ssm_copy(&model_hippo,  &x_train, &y_train, 50);
    println!("\nTraining Random-initialized SSM...");
    let losses_random = train_ssm_copy(&model_random, &x_train, &y_train, 50);

    let acc_hippo  = test_accuracy(&model_hippo,  &x_test, &y_test);
    let acc_random = test_accuracy(&model_random, &x_test, &y_test);

    println!("\n=== Results ===");
    println!("HiPPO init: Test Acc = {:.1}%",  acc_hippo  * 100.0);
    println!("Random init: Test Acc = {:.1}%", acc_random * 100.0);
    println!("Improvement: {:.1}%", (acc_hippo - acc_random) * 100.0);
    (losses_hippo, losses_random)
}

fn main() {
    let (_losses_h, _losses_r) = experiment_hippo_vs_random();
}
```

**Expected**: HiPPO >> Random at large T. HiPPOã¯å›ºæœ‰å€¤æ§‹é€ ã«ã‚ˆã‚Šé•·è·é›¢ä¾å­˜ã‚’ä¿æŒã—ã‚„ã™ã„ã€‚

**çµæœã®è§£é‡ˆ**:

| Metric | HiPPO | Random | Why |
|:-------|:------|:-------|:----|
| Test Acc | ~85% | ~30% | HiPPOã¯é•·è·é›¢è¨˜æ†¶ã®ç†è«–çš„ä¿è¨¼ |
| Training Speed | åŒç­‰ | åŒç­‰ | åŒã˜è¨ˆç®—é‡ |
| Stability | é«˜ | ä½ | HiPPOã®å›ºæœ‰å€¤ã¯è² â†’å®‰å®š |

#### Challenge 2: S4 vs Mamba on sequential CIFAR-10

ç”»åƒ(32Ã—32Ã—3=3072)ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã€1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ã—ã¦åˆ†é¡ã€‚

```rust
use ndarray::{Array1, Array2, Axis};

/// Load CIFAR-10 and flatten each image to a 1-D sequence (3072 pixels).
/// Returns (X_train, y_train, X_test, y_test) where X is (n_samples Ã— 3072).
fn load_cifar10_sequential() -> (Array2<f32>, Vec<usize>, Array2<f32>, Vec<usize>) {
    // In a real project use the `cifar` crate or load from raw binary files.
    // Here we return random tensors to keep the example self-contained.
    use rand::prelude::*; use rand_distr::StandardNormal; let mut rng = rand::thread_rng();
    let (n_train, n_test, seq_len) = (50_000, 10_000, 3072);
    let x_train = Array2::from_shape_fn((n_train, seq_len), |_| rng.sample::<f32, _>(StandardNormal));
    let y_train = (0..n_train).map(|_| rng.gen_range(0..10)).collect();
    let x_test  = Array2::from_shape_fn((n_test, seq_len), |_| rng.sample::<f32, _>(StandardNormal));
    let y_test  = (0..n_test).map(|_| rng.gen_range(0..10)).collect();
    (x_train, y_train, x_test, y_test)
}

struct S4Classifier {
    layers: Vec<S4Layer>,
    w_out: Array2<f32>,
}

impl S4Classifier {
    // x: (batch, seq_len)
    fn forward(&self, x: &Array2<f32>) -> Array2<f32> {
        // apply each S4 layer row-wise (one row = one sample)
        let mut h: Array2<f64> = x.mapv(|v| v as f64);
        for layer in &self.layers {
            let rows: Vec<Vec<f64>> = (0..h.nrows())
                .map(|i| { let row: Vec<f64> = h.row(i).to_vec(); let l = row.len(); layer.forward(&row, l) })
                .collect();
            h = Array2::from_shape_fn((x.nrows(), rows[0].len()), |(i, j)| rows[i][j]);
        }
        // mean over seq dimension â†’ (batch,) â†’ project to (num_classes, batch)
        let mean_h: Array1<f32> = h.mean_axis(Axis(1)).unwrap().mapv(|v| v as f32);
        self.w_out.dot(&mean_h).insert_axis(Axis(0))
    }
}
```

**Expected**: Mamba â‰¥ S4 (~91% vs ~88%[^3])ã€‚Mambaã®é¸æŠæ€§(é‡è¦ãƒ”ã‚¯ã‚»ãƒ«è¨˜æ†¶ã€èƒŒæ™¯å¿˜å´)ãŒæœ‰åˆ©ã€‚

#### Challenge 3: Parallel Scané€Ÿåº¦æ¯”è¼ƒ

```rust
use ndarray::{Array1, Array2};
use rand::prelude::*;
use rand_distr::StandardNormal;
use std::time::Instant;

fn sequential_scan(a_mats: &[Array2<f64>], b_vecs: &[Array1<f64>]) -> Vec<Array1<f64>> {
    let d = b_vecs[0].len();
    let mut h = Array1::<f64>::zeros(d);
    // iterator chain: zip matrices with bias vectors, fold state through scan
    a_mats.iter().zip(b_vecs.iter()).map(|(a, b)| {
        h = a.dot(&h) + b;
        h.clone()
    }).collect()
}

fn benchmark_scans() {
    let mut rng = rand::thread_rng();
    let d = 8_usize;
    for &l in &[100_usize, 500, 1000, 5000, 10000] {
        let a_mats: Vec<Array2<f64>> = (0..l).map(|_| Array2::eye(d) * 0.9).collect();
        let b_vecs: Vec<Array1<f64>> = (0..l)
            .map(|_| (0..d).map(|_| rng.sample(StandardNormal)).collect())
            .collect();
        let start = Instant::now();
        let _ = sequential_scan(&a_mats, &b_vecs);
        let elapsed_ms = start.elapsed().as_secs_f64() * 1000.0;
        println!("L={l}: {elapsed_ms:.2}ms");
    }
}
```

**Expected**: Sequential $O(L)$ ç·šå½¢ã€Parallel $O(\log L)$ å¯¾æ•°ã€‚GPU 100Kç³»åˆ—ã§24å€é«˜é€ŸåŒ–ã€‚

#### Challenge 4: SSMå›ºæœ‰å€¤ã¨æ¸›è¡°ç‡ã®é–¢ä¿‚

```rust
use ndarray::Array1;

/// Compute ||h_t|| over time for a diagonal SSM with given eigenvalues.
fn decay_curve(eigenvalues: &[f64], delta: f64, t_steps: usize) -> Vec<f64> {
    let d = eigenvalues.len();
    // diagonal A_bar = diag(exp(Î» * Î”))
    let a_diag: Vec<f64> = eigenvalues.iter().map(|&lam| (lam * delta).exp()).collect();
    let mut h: Array1<f64> = Array1::from_elem(d, 1.0 / d as f64);
    (0..t_steps).map(|_| {
        h = Array1::from_shape_fn(d, |i| a_diag[i] * h[i]);
        h.iter().map(|&v| v * v).sum::<f64>().sqrt()
    }).collect()
}

fn visualize_eigenvalue_decay() {
    let (d, t, delta) = (4, 100, 0.1_f64);
    let _ = d; // d is encoded in the eigenvalue slices below
    let lambda_slow  = [-0.1, -0.2, -0.3, -0.4];
    let lambda_fast  = [-1.0, -2.0, -3.0, -4.0];
    let lambda_hippo = [-1.0, -2.0, -3.0, -4.0]; // placeholder; substitute real HiPPO eigs

    let norms_slow  = decay_curve(&lambda_slow,  delta, t);
    let norms_fast  = decay_curve(&lambda_fast,  delta, t);
    let norms_hippo = decay_curve(&lambda_hippo, delta, t);

    // Print first 10 steps as tabular proxy for visualization
    println!("{:>6} {:>12} {:>12} {:>12}", "step", "slow", "fast", "hippo");
    for i in 0..10 {
        println!("{:>6} {:>12.6} {:>12.6} {:>12.6}",
            i + 1, norms_slow[i], norms_fast[i], norms_hippo[i]);
    }
}

fn main() {
    visualize_eigenvalue_decay();
}
```

**Insight**: HiPPOã¯è¤‡æ•°ã®æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«($\lambda = -1, -2, -3, -4$)ã‚’æŒã¤ â†’ çŸ­æœŸãƒ»ä¸­æœŸãƒ»é•·æœŸè¨˜æ†¶ã‚’åŒæ™‚ã«ä¿æŒã€‚

#### Challenge 5: Mamba Selectivity Visualization

å…¥åŠ›ä¾å­˜ã®$\Delta_t$ãŒã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã‚’å¯è¦–åŒ–ã€‚

```rust
/// Visualize how input-dependent Î”_t adapts to signal strength.
fn visualize_mamba_selectivity() {
    let l = 100_usize;
    let mut u = vec![0.0f32; l];
    // important tokens at positions 10, 50, 90
    u[10] = 5.0; u[50] = 3.0; u[90] = 4.0;

    let (w_delta, b_delta) = (0.5f32, -1.0f32);
    // softplus broadcasts over u element-wise
    let delta: Vec<f32> = u.iter()
        .map(|&x| softplus_stable((w_delta * x + b_delta) as f64) as f32)
        .collect();

    // Print every 10th step as proxy for visualization
    println!("{:>6} {:>12} {:>12}", "step", "u_t", "Î”_t");
    for i in (0..l).step_by(10) {
        println!("{:>6} {:>12.4} {:>12.4}", i, u[i], delta[i]);
    }
}

fn main() {
    visualize_mamba_selectivity();
}
```

**è§£é‡ˆ**: é‡è¦ãªå…¥åŠ›(u[10], u[50], u[90])ã§$\Delta_t$ãŒå¤§ãããªã‚‹ â†’ ãã®ç¬é–“ã®æƒ…å ±ã‚’å¼·ãæ›¸ãè¾¼ã‚€ã€‚ã‚¼ãƒ­éƒ¨åˆ†ã§ã¯$\Delta_t$ãŒå°ã•ã„ â†’ éå»ã‚’ä¿æŒã€‚

### 5.3 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

è‡ªåˆ†ã§ä»¥ä¸‹ã‚’ç¢ºèª:

- [ ] é€£ç¶šæ™‚é–“SSMã®å¾®åˆ†æ–¹ç¨‹å¼ã‚’æ›¸ã‘ã‚‹
- [ ] ZOHé›¢æ•£åŒ–ã®å¼$\bar{A} = e^{A\Delta}, \bar{B} = (A^{-1}(e^{A\Delta}-I))B$ã‚’å°å‡ºã§ãã‚‹
- [ ] SSMã®å†å¸°å½¢æ…‹ã¨ç•³ã¿è¾¼ã¿å½¢æ…‹ã®ç­‰ä¾¡æ€§ã‚’èª¬æ˜ã§ãã‚‹
- [ ] HiPPOã®å‹•æ©Ÿ(å¤šé …å¼è¿‘ä¼¼ã«ã‚ˆã‚‹è¨˜æ†¶åœ§ç¸®)ã‚’èª¬æ˜ã§ãã‚‹
- [ ] S4ã®DPLRåˆ†è§£ã¨FFTé«˜é€ŸåŒ–ã®ä»•çµ„ã¿ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Mambaã®Selective SSM($\Delta_t, B_t, C_t$ãŒå…¥åŠ›ä¾å­˜)ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Parallel Scanã®çµåˆå¾‹ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] Rust/Rustã§SSMã‚’å®Ÿè£…ã—ã€å‹•ã‹ã›ã‚‹

å…¨ã¦ãƒã‚§ãƒƒã‚¯ã§ããŸã‚‰ã€SSMç†è«–ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¦ã„ã‚‹ã€‚

> **Note:** **é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã¨ãƒ†ã‚¹ãƒˆã‚’å®Œäº†ã€‚è‡ªå·±è¨ºæ–­ã§SSMç†è«–ã®ç¿’å¾—ã‚’ç¢ºèªã—ãŸã€‚ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã¸ã€‚

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Rustå®Ÿè£…ã§HiPPO-LeGSè¡Œåˆ—ã‚’ç”Ÿæˆã™ã‚‹éš›ã€$A_{nk} = -(2n+1)^{1/2}(2k+1)^{1/2}$ $(n>k)$ ã®è¨ˆç®—ã§æ•°å€¤çš„ã«æ°—ã‚’ã¤ã‘ã‚‹ç‚¹ã¯ä½•ã‹ï¼Ÿ
> 2. Mambaã®Selective SSMã§ã€å…¥åŠ›$u_t$ã‹ã‚‰ã‚²ãƒ¼ãƒˆ$\Delta_t$ã‚’ç”Ÿæˆã™ã‚‹Linearå±¤ã®å½¹å‰²ã‚’è¿°ã¹ã‚ˆã€‚

---

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 SSMç³»è­œå›³: S4ã‹ã‚‰Mamba-2ã¸

```mermaid
graph TD
    A["HiPPO 2020<br/>é•·è·é›¢è¨˜æ†¶ç†è«–"] --> B["S4 2021<br/>DPLR + FFT"]
    B --> C["S4D 2022<br/>å¯¾è§’è¿‘ä¼¼"]
    B --> D["S5 2022<br/>ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³"]
    B --> E["H3 2022<br/>æš—é»™çš„é•·ç•³ã¿è¾¼ã¿"]
    C --> F["Mamba 2023<br/>Selective SSM"]
    D --> F
    E --> F
    F --> G["Mamba-2 2024<br/>SSD, Attention=SSMåŒå¯¾æ€§"]

    style A fill:#fff9c4
    style B fill:#c8e6c9
    style F fill:#81c784
    style G fill:#4caf50
```

### 6.2 Mamba-2ã¨SSD: Attention=SSMåŒå¯¾æ€§

Mamba-2[^7]ã¯ã€**Attentionã¨SSMãŒæ•°å­¦çš„ã«ç­‰ä¾¡**ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ãŸã€‚

**SSD (Structured State Space Duality)å®šç†**: Semi-Separableè¡Œåˆ—ã¨ã—ã¦è¡¨ç¾ã•ã‚ŒãŸSSMã¨ã€Attentionã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹è¡Œåˆ—ã¯ã€ç‰¹å®šã®æ§‹é€ ä¸‹ã§ä¸€è‡´ã™ã‚‹ã€‚

ã¤ã¾ã‚Šã€**Attentionã‚‚SSMã‚‚ã€ŒåŒã˜ã‚‚ã®ã€ã®ç•°ãªã‚‹è¡¨ç¾**ã€‚S4/Mambaã¯SSMå´ã‹ã‚‰ã€Flash/SparseAttentionã¯Attentionå´ã‹ã‚‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã—ã¦ã„ãŸãŒã€å®Ÿã¯è¡Œãç€ãå…ˆã¯åŒã˜ã€‚

#### SSDå®šç†ã®æ¦‚è¦(ç°¡ç•¥ç‰ˆ)

**Semi-Separableè¡Œåˆ—**: ä¸‹ä¸‰è§’éƒ¨åˆ†ãŒä½ãƒ©ãƒ³ã‚¯æ§‹é€ ã‚’æŒã¤è¡Œåˆ—ã€‚

$$
M_{ij} =
\begin{cases}
p_i^\top q_j & \text{if } i \geq j \\
0 & \text{if } i < j
\end{cases}
$$

ã“ã‚Œã¯**Causal Attention**ã¨åŒã˜æ§‹é€ (æœªæ¥ã‚’è¦‹ãªã„)ã€‚

**SSMã®å‡ºåŠ›è¡Œåˆ—**: é›¢æ•£SSMã®å‡ºåŠ›$y_1, \ldots, y_L$ã‚’ä¸¦ã¹ãŸè¡Œåˆ—$Y$ã¯ã€å…¥åŠ›$u_1, \ldots, u_L$ã«å¯¾ã—ã¦:

$$
Y = \bar{\mathcal{K}} U
$$

ã“ã“ã§$\bar{\mathcal{K}}$ã¯Toeplitzè¡Œåˆ—(ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«)ã€‚ã“ã‚Œã‚’**Semi-Separableå½¢å¼ã«åˆ†è§£**ã§ãã‚‹[^7]:

$$
\bar{\mathcal{K}}_{ij} = C \bar{A}^{i-j} B = (C \bar{A}^i) \cdot (\bar{A}^{-j} B)
$$

ã¤ã¾ã‚Š$p_i = C \bar{A}^i, q_j = \bar{A}^{-j} B$ã¨ç½®ã‘ã°ã€Semi-Separableã€‚

**Attentionã¨SSMã®æ¥ç¶š**:

| Attention | SSM |
|:----------|:----|
| Query $Q_i$ | $C \bar{A}^i$ |
| Key $K_j$ | $\bar{A}^{-j} B$ |
| Softmax$(QK^\top)$ | Semi-Separable $\bar{\mathcal{K}}$ |

**Softmaxã®ä»£ã‚ã‚Šã«ã€SSMã¯æŒ‡æ•°æ¸›è¡°**($\bar{A}^{i-j}$)ã‚’ä½¿ã†ã€‚ã“ã‚ŒãŒã€ŒAttention â‰ˆ SSMã€ã®æ•°å­¦çš„æ„å‘³ã€‚

<details><summary>å®Œå…¨ãªè¨¼æ˜ã¯?</summary>

SSDè«–æ–‡[^7]ã®Theorem 3.1å‚ç…§ã€‚Semi-Separableè¡Œåˆ—ã®å› æ•°åˆ†è§£å®šç†ã¨ã€SSMã®ã‚«ãƒ¼ãƒãƒ«è¡¨ç¾ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã€‚éµã¯Woodburyæ’ç­‰å¼ã¨ã€Cauchy kernelã€‚ç¬¬17å›ã§è©³è¿°ã€‚

</details>

**å®Ÿç”¨çš„æ„å‘³**: Mambaã¨Attentionã¯ã€ŒåŒã˜è¨ˆç®—ã‚’ç•°ãªã‚‹æ–¹æ³•ã§å®Ÿè¡Œã€ã—ã¦ã„ã‚‹ã€‚ã©ã¡ã‚‰ã‚’ä½¿ã†ã‹ã¯ã€å®Ÿè£…ã®ä¾¿åˆ©ã•ãƒ»ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ»ã‚¿ã‚¹ã‚¯ã«ä¾å­˜ã€‚ç†è«–çš„ã«ã¯ç­‰ä¾¡ã€‚

#### Mamba-2ã®æ”¹å–„ç‚¹

Mamba-2[^7]ã¯Mambaã«å¯¾ã—ã¦:

1. **Chunk-wiseä¸¦åˆ—åŒ–**: ç³»åˆ—ã‚’å°ã•ãªchunkã«åˆ†å‰²ã—ã€chunkå†…ã§ä¸¦åˆ—è¨ˆç®—
2. **è¨“ç·´é«˜é€ŸåŒ–**: 2-3x faster than Mamba
3. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: chunkå˜ä½ã§SRAMã«è¼‰ã›ã‚‹(FlashAttentioné¢¨)
4. **ç†è«–çš„çµ±ä¸€**: Attentionã¨SSMã®åŒå¯¾æ€§ã‚’æ˜ç¤º

Mamba-2: Chunk-wiseä¸¦åˆ—åŒ–ã€‚Chunkå†…ä¸¦åˆ—ã€Chunké–“å†å¸°ã€‚Transformerä¸¦ã¿è¨“ç·´é€Ÿåº¦ã€Mambaä¸¦ã¿æ¨è«–é€Ÿåº¦ã€‚

### 6.3 Vision SSM: VMamba, Vim

ç”»åƒã‚’SSMã§å‡¦ç†ã™ã‚‹è©¦ã¿ã€‚2Dæ§‹é€ ã‚’ã©ã†èµ°æŸ»ã™ã‚‹ã‹(ãƒ©ã‚¹ã‚¿é †/è›‡è¡Œ/åŒæ–¹å‘)ãŒèª²é¡Œã€‚

**VMamba**[^8]: 2D selective scanã€‚ç”»åƒã®ç©ºé–“æ§‹é€ ã‚’è€ƒæ…®ã—ãŸèµ°æŸ»é †åºã€‚

#### 2D Selective Scan

ç”»åƒ$I \in \mathbb{R}^{H \times W \times C}$ã‚’1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›ã™ã‚‹4ã¤ã®èµ°æŸ»é †åº:

```mermaid
graph LR
    A["ç”»åƒ HÃ—W"] --> B["Scan 1: å·¦â†’å³ã€ä¸Šâ†’ä¸‹"]
    A --> C["Scan 2: å³â†’å·¦ã€ä¸‹â†’ä¸Š"]
    A --> D["Scan 3: ä¸Šâ†’ä¸‹ã€å·¦â†’å³"]
    A --> E["Scan 4: ä¸‹â†’ä¸Šã€å³â†’å·¦"]

    B & C & D & E --> F["4ã¤ã®SSMä¸¦åˆ—å®Ÿè¡Œ"]
    F --> G["å¹³å‡ã¾ãŸã¯å­¦ç¿’æ¸ˆã¿é‡ã¿ä»˜ã‘"]
```

å„èµ°æŸ»ã§ç•°ãªã‚‹SSMã‚’é©ç”¨ã—ã€çµæœã‚’ãƒãƒ¼ã‚¸ã€‚ã“ã‚Œã«ã‚ˆã‚Š2Dã®ç©ºé–“æ§‹é€ ã‚’ã‚ã‚‹ç¨‹åº¦æ‰ãˆã‚‹ã€‚

**VMambaã®æ§‹é€ **: 4æ–¹å‘ã‚¹ã‚­ãƒ£ãƒ³(å·¦å³ä¸Šä¸‹ã€å³å·¦ä¸‹ä¸Šã€ä¸Šä¸‹å·¦å³ã€ä¸‹ä¸Šå³å·¦)ã€å„ã€…ã«Mamba SSMé©ç”¨ã€çµæœã‚’å¹³å‡ã€‚
```

**æ€§èƒ½**: ViT(Transformer)ã«è¿«ã‚‹ãŒã€ã¾ã Attentionã«è»é…ã€‚ç”»åƒã¯å±€æ‰€æ€§ãŒå¼·ãã€å…¨ç³»åˆ—å‚ç…§(Attention)ãŒæœ‰åˆ©ã€‚

| Model | ImageNet Acc | Params | FLOPs |
|:------|:-------------|:-------|:------|
| ViT-B | 84.5% | 86M | 17.6G |
| Swin-B | 85.2% | 88M | 15.4G |
| **VMamba-B** | 84.0% | 89M | 15.2G |

**VMambaã®èª²é¡Œ**:

1. **èµ°æŸ»é †åºä¾å­˜**: ç”»åƒã®å›è»¢ãƒ»åè»¢ã«å¯¾ã—ã¦ä¸å¤‰ã§ã¯ãªã„
2. **é•·è·é›¢ä¾å­˜**: ç”»åƒå¯¾è§’ç·šä¸Šã®ä¾å­˜ã¯ã€èµ°æŸ»é †ã«ã‚ˆã£ã¦ã¯$O(H+W)$é›¢ã‚Œã‚‹
3. **2Då¸°ç´ãƒã‚¤ã‚¢ã‚¹**: CNNã®ã‚ˆã†ãªå±€æ‰€æ€§ã‚’æŒãŸãªã„

**ä»Šå¾Œã®æ–¹å‘æ€§**: Vision Mambaã¨Local Attentionã®çµ„ã¿åˆã‚ã›(Hybrid)ãŒæœ‰æœ›ã€‚

#### Vim: Vision Mamba

Vim[^8]ã¯VMambaã®å¤‰ç¨®ã€‚åŒæ–¹å‘SSM(forward + backward scan)ã‚’ä½¿ç”¨ã€‚

åŒæ–¹å‘ã«ã‚ˆã‚Šã€é•·è·é›¢ä¾å­˜ã‚’ã‚ˆã‚ŠåŠ¹æœçš„ã«æ‰ãˆã‚‹ã€‚

**Vimã®æ€§èƒ½**: ImageNetã§83.7% (VMambaä¸¦ã¿)ã€‚

#### Vision SSMã®æ•°å­¦çš„èª²é¡Œ

**å•é¡Œ**: 2Dç”»åƒ$(i, j)$ã‚’1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹$t$ã«ãƒãƒƒãƒ—ã™ã‚‹é–¢æ•°$\phi: (i,j) \to t$ãŒä¸€æ„ã§ã¯ãªã„ã€‚

ä¾‹:
- Raster scan: $t = i \cdot W + j$
- Hilbert curve: ç©ºé–“å……å¡«æ›²ç·š
- Z-order (Morton order): å†å¸°çš„4åˆ†å‰²

å„é †åºã§å±€æ‰€æ€§ã®ä¿å­˜åº¦ãŒç•°ãªã‚‹ã€‚

**Hilbertæ›²ç·š**ã®åˆ©ç‚¹:

```mermaid
graph TD
    A["2Då¹³é¢"] --> B["Hilbertæ›²ç·šã§1DåŒ–"]
    B --> C["è¿‘å‚ãƒ”ã‚¯ã‚»ãƒ«ãŒè¿‘ã„"]
    C --> D["SSMã®é•·è·é›¢ä¾å­˜å•é¡Œã‚’ç·©å’Œ"]
```

Hilberté †ã§SSMã‚’é©ç”¨ã™ã‚‹ã¨ã€2Då±€æ‰€æ€§ãŒã‚ã‚‹ç¨‹åº¦ä¿ãŸã‚Œã‚‹ã€‚

**å®Ÿè£…**:

```rust
use ndarray::{prelude::*, stack, Axis};

// Hilbert curve indexing (simplified)
fn hilbert_index(i: usize, j: usize, order: usize) -> u64 {
    // Recursive Hilbert curve mapping
    // Returns 1D index for 2D coordinate (i, j)
    // Implementation omitted (see Wikipedia)
    idx
}

fn scan_hilbert(image: &Array3<f32>) -> Array2<f32> {
    let (h, w, _c) = image.dim();
    let order = (h.max(w) as f64).log2() as usize;
    let mut indices: Vec<(usize, usize)> =
        (0..h).flat_map(|i| (0..w).map(move |j| (i, j))).collect();
    indices.sort_by_key(|&(i, j)| hilbert_index(i, j, order));
    // Stack pixel channels into (H*W, C)
    let rows: Vec<Array1<f32>> = indices.iter()
        .map(|&(i, j)| image.slice(s![i, j, ..]).to_owned())
        .collect();
    stack(Axis(0), &rows.iter().map(|r| r.view()).collect::<Vec<_>>()).unwrap()
}
```

**èª²é¡Œ**: Hilbertæ›²ç·šã¯$2^n \times 2^n$ç”»åƒã§ã®ã¿å®šç¾©å¯èƒ½ã€‚ä»»æ„ã‚µã‚¤ã‚ºã«ã¯è¿‘ä¼¼ãŒå¿…è¦ã€‚

### 6.4 RWKV, RetNet: ç·šå½¢RNN/Attention

ç¬¬17å›ã§è©³è¿°ã™ã‚‹ãŒã€Mambaã¨RWKV[^9]/RetNet[^10]ã¯ã€Œç·šå½¢RNNã€ã¨ã„ã†å…±é€šç‚¹ã‚’æŒã¤ã€‚

| Model | ç‰¹å¾´ | è¨“ç·´ | æ¨è«– |
|:------|:-----|:-----|:-----|
| **Mamba** | Selective SSM | ä¸¦åˆ—(scan) | å†å¸°O(1) |
| **RWKV** | Time-mix + Channel-mix | ä¸¦åˆ— | å†å¸°O(1) |
| **RetNet** | Multi-scale decay | ä¸¦åˆ— | å†å¸°O(1) |

å…¨ã¦$O(N)$è¨“ç·´ã€$O(1)$æ¨è«–(per token)ã€‚Transformerã®ä»£æ›¿å€™è£œã€‚

#### RWKV (Receptance Weighted Key Value)

Attentionã‚’ç·šå½¢åŒ–ã€‚$s_t = \gamma s_{t-1} + K_t \odot V_t, o_t = \sigma(R_t) \odot s_t/n_t$ã€‚æŒ‡æ•°æ¸›è¡°ã§å†å¸°åŒ–ã€‚Time-mix: $x_t' = \mu x_t + (1-\mu)x_{t-1}$ã€‚Pile: 12.5 vs Transformer 12.1ã€æ¨è«–5xé«˜é€Ÿã€‚

#### RetNet (Retentive Network)

Multi-scale exponential decayã€‚$s_t = \gamma s_{t-1} + K_t V_t^\top, o_t = Q_t s_t$ã€‚è¤‡æ•°$\gamma$(0.9, 0.99, 0.999)ã§çŸ­ä¸­é•·æœŸè¨˜æ†¶ã€‚3å½¢æ…‹: ä¸¦åˆ—(è¨“ç·´)ã€å†å¸°(æ¨è«–$O(1)$)ã€Chunkã€‚Pile: 12.2ã€æ¨è«–7xé«˜é€Ÿã€‚SSMã¨æ§‹é€ é¡ä¼¼($\gamma \leftrightarrow \bar{A}$)ã€‚

#### ç·šå½¢RNN/Attentionã®çµ±ä¸€è¦–ç‚¹

RWKV, RetNet, Mamba, S4ã¯å…¨ã¦**ç·šå½¢å†å¸°**ã§è¡¨ç¾å¯èƒ½:

$$
h_t = A_t h_{t-1} + B_t u_t, \quad y_t = C_t h_t
$$

| Model | $A_t$ | $B_t$ | $C_t$ | ç‰¹å¾´ |
|:------|:------|:------|:------|:-----|
| S4 | $\bar{A}$ (å›ºå®š) | $\bar{B}$ (å›ºå®š) | $C$ (å›ºå®š) | éé¸æŠçš„ |
| Mamba | $\bar{A}_t$ (å…¥åŠ›ä¾å­˜) | $\bar{B}_t$ (å…¥åŠ›ä¾å­˜) | $C_t$ (å…¥åŠ›ä¾å­˜) | é¸æŠçš„ |
| RWKV | $\gamma I$ (å›ºå®š) | $K_t \odot V_t$ | $\sigma(R_t)$ | Time-mix |
| RetNet | $\gamma I$ (å›ºå®š, multi-scale) | $K_t V_t^\top$ | $Q_t$ | Multi-scale decay |

**å…±é€šç‚¹**: å…¨ã¦$O(N)$è¨“ç·´(ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³)ã€$O(1)$æ¨è«–(å†å¸°)ã€‚

**ç›¸é•ç‚¹**: é¸æŠæ€§(å…¥åŠ›ä¾å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)ã®æœ‰ç„¡ã€‚MambaãŒæœ€ã‚‚æŸ”è»Ÿã€‚

#### ç·šå½¢åŒ–ã®ä»£å„Ÿ: è¡¨ç¾åŠ›ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

Softmaxç·šå½¢åŒ–â†’å‹•çš„é‡ã¿ä»˜ã‘å–ªå¤±ã€‚Attention: Content-based($\alpha_{ij}$ã¯é¡ä¼¼åº¦ä¾å­˜)ã€‚ç·šå½¢RNN: Position-based($\alpha_{ij}=\gamma^{i-j}$å›ºå®š)ã€‚Mamba: å…¥åŠ›ä¾å­˜$\Delta_t, B_t, C_t$ã§éƒ¨åˆ†å¾©æ´»ã€‚ç†è«–é™ç•Œ: $O(N^2)$ç›¸äº’ä½œç”¨ã¯$O(N)$å†å¸°ã§åŸç†ä¸å¯ã€‚å®Ÿè¨¼: perplexityå·®<5%ã€ã‚¿ã‚¹ã‚¯ä¾å­˜ã§å®Ÿç”¨çš„ã€‚

### 6.5 SSMç ”ç©¶ã®ä»Šå¾Œ

2025-2026ã®ãƒˆãƒ¬ãƒ³ãƒ‰:

- **Hybrid architectures**: Attention + SSM(Jamba, Zamba) â†’ ç¬¬18å›
- **Long context**: 1M+ tokens processing with SSM
- **Efficient fine-tuning**: LoRA-style adaptation for SSM
- **Hardware co-design**: Custom ASIC for SSM kernels

#### Hybrid Architectures: Attentionã¨SSMã®èåˆ

**å‹•æ©Ÿ**: Attentionã¨SSMã¯ç›¸è£œçš„ã€‚

| ç‰¹æ€§ | Attention | SSM |
|:-----|:----------|:----|
| å…¨ç³»åˆ—å‚ç…§ | â— | â–³ |
| é•·è·é›¢è¨˜æ†¶ | â–³(O(NÂ²)) | â—(O(N)) |
| Few-shot | â— | â–³ |
| ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° | âœ— | â— |

**Jambaã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**[^12]:

```
[Mamba] â†’ [Mamba] â†’ [Attention] â†’ [MoE] â†’ [Mamba] â†’ [Mamba] â†’ [Attention] â†’ [MoE] â†’ ...
```

ãƒ‘ã‚¿ãƒ¼ãƒ³: `[Mamba Ã— N] â†’ [Attention] â†’ [MoE]`ã‚’ç¹°ã‚Šè¿”ã™ã€‚

- **Mambaå±¤**: é•·è·é›¢ä¾å­˜ã‚’åŠ¹ç‡çš„ã«å‡¦ç†
- **Attentionå±¤**: å…¨ç³»åˆ—å‚ç…§ãŒå¿…è¦ãªç®‡æ‰€(7å±¤ã«1å›ç¨‹åº¦)
- **MoEå±¤**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°(è¨ˆç®—é‡å¢—ã‚„ã•ãšãƒ¢ãƒ‡ãƒ«å®¹é‡æ‹¡å¤§)

**è¨­è¨ˆåŸç†**:

1. **Layeræ¯”ç‡**: Mamba:Attention = 6:1 ~ 8:1
2. **Attentioné…ç½®**: ä¸Šä½å±¤(æ„å‘³çš„æ¨è«–ãŒå¿…è¦ãªéƒ¨åˆ†)
3. **MoEé…ç½®**: FFNç›¸å½“éƒ¨åˆ†

**Zambaã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**[^13]:

```
[Mamba] â†’ [Mamba] â†’ [Mamba] â†’ [Shared Attention] â†’ [Mamba] â†’ [Mamba] â†’ ...
              â†“                        â†‘
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Shared Attention: è¤‡æ•°ã®Mambaå±¤ãŒ1ã¤ã®Attentionå±¤ã‚’å…±æœ‰ã€‚ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€‚

**æ€§èƒ½æ¯”è¼ƒ**:

| Model | Params | Perplexity | Throughput | Context |
|:------|:-------|:-----------|:-----------|:--------|
| Transformer | 7B | 11.8 | 2K tok/s | 8K |
| Mamba | 7B | 12.1 | 10K tok/s | 256K |
| **Jamba** | 7B+52B(MoE) | **11.5** | **8K tok/s** | **256K** |
| **Zamba** | 7B | **11.7** | **9K tok/s** | **256K** |

HybridãŒå…¨æŒ‡æ¨™ã§ãƒãƒ©ãƒ³ã‚¹ã‚ˆãå„ªã‚Œã‚‹ã€‚

#### Long Context Processing: 100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®é“

**èª²é¡Œ**: ç³»åˆ—é•·$N=1M$ã§ã®å‡¦ç†ã€‚

**Attentionã®é™ç•Œ**:

$$
\text{Memory} = O(N^2) = O((10^6)^2) = O(10^{12}) \text{ elements} \approx 4 \text{TB (FP32)}
$$

ä¸å¯èƒ½ã€‚

**SSMã®å¯èƒ½æ€§**:

$$
\text{Memory} = O(Nd) = O(10^6 \cdot 10^3) = O(10^9) \text{ elements} \approx 4 \text{GB}
$$

å®Ÿç¾å¯èƒ½ã€‚

**Ring Attention + SSM**:

- Ring Attention[^14]: Attentionã‚’åˆ†æ•£å‡¦ç†(1M tokens â†’ å„GPU 10K tokens)
- SSM: ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç† + çŠ¶æ…‹ã®å—ã‘æ¸¡ã—

```mermaid
graph LR
    A["GPU 0<br/>tokens 0-10K"] --> B["GPU 1<br/>tokens 10K-20K"]
    B --> C["GPU 2<br/>tokens 20K-30K"]
    C --> D["..."]
    D --> E["GPU 99<br/>tokens 990K-1M"]
    E -->|çŠ¶æ…‹| A

    style A fill:#c8e6c9
    style E fill:#c8e6c9
```

å„GPUãŒchunkã‚’å‡¦ç†ã—ã€çŠ¶æ…‹$h_t$ã‚’æ¬¡ã®GPUã«é€ã‚‹ã€‚Attentionã¯å„chunkå†…ã®ã¿ã€‚

**å®Ÿè£…ä¾‹(ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰)**:

**å®Ÿç¾ä¾‹**: Google Gemini 1.5(2M context)ã¯ã€ãŠãã‚‰ãã“ã®ç¨®ã®Hybrid + Ringæ§‹æˆã€‚

#### Efficient Fine-tuning: SSMç‰ˆLoRA

**å•é¡Œ**: å¤§è¦æ¨¡SSMãƒ¢ãƒ‡ãƒ«(Mamba-7B)ã‚’ç‰¹å®šã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ãŸã„ã€‚å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã¯é«˜ã‚³ã‚¹ãƒˆã€‚

**LoRA (Low-Rank Adaptation)ã®å¾©ç¿’**:

Transformerã®é‡ã¿$W \in \mathbb{R}^{d \times d}$ã«ä½ãƒ©ãƒ³ã‚¯æ›´æ–°ã‚’åŠ ãˆã‚‹:

$$
W' = W + \Delta W = W + BA, \quad B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times d}, \quad r \ll d
$$

$B, A$ã®ã¿å­¦ç¿’ â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒ$O(rd)$(å…ƒã®$O(d^2)$ã‚ˆã‚Šé¥ã‹ã«å°)ã€‚

**SSMç‰ˆLoRA**: Mambaã®SSMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$A, B, C$ã«ä½ãƒ©ãƒ³ã‚¯æ›´æ–°ã‚’é©ç”¨ã€‚

$$
\begin{aligned}
A_{\text{adapted}} &= A + \Delta A \\
B_{\text{adapted}} &= B + \Delta B \\
C_{\text{adapted}} &= C + \Delta C
\end{aligned}
$$

$\Delta A = B_A L_A^\top$(ä½ãƒ©ãƒ³ã‚¯), $\Delta B = b_B l_B^\top$, $\Delta C = c_C l_C^\top$ã€‚

**å®Ÿè£…**:

**åŠ¹æœ**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°0.5%ã§ã€Full fine-tuningæ€§èƒ½ã®95%ã‚’é”æˆ(çµŒé¨“çš„)ã€‚

#### Hardware Co-design: SSMå°‚ç”¨ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿

**ç¾çŠ¶**: Mambaã®CUDAã‚«ãƒ¼ãƒãƒ«ã¯ã€æ±ç”¨GPUã§å‹•ä½œã€‚ã ãŒGPUã¯Attention(è¡Œåˆ—ç©)ã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€SSMã®å†å¸°ãƒ»ã‚¹ã‚­ãƒ£ãƒ³ã¯éåŠ¹ç‡ã€‚

**SSMå°‚ç”¨ASICè¨­è¨ˆã®è¦ä»¶**:

1. **Parallel Scan Unit**: çµåˆçš„æ¼”ç®—ã®æœ¨æ§‹é€ ä¸¦åˆ—åŒ–
2. **State Memory**: é«˜é€ŸSRAM for $h_t$(å†å¸°ã«é »ç¹ã‚¢ã‚¯ã‚»ã‚¹)
3. **Exponential Kernel**: $e^{A\Delta}$ã®é«˜é€Ÿè¨ˆç®—(ãƒ†ãƒ¼ãƒ–ãƒ« or å¤šé …å¼è¿‘ä¼¼)
4. **Low-Rank Matrix Ops**: DPLRæ§‹é€ ã«ç‰¹åŒ–ã—ãŸæ¼”ç®—å™¨

**æœŸå¾…åŠ¹æœ**:

- GPUã«å¯¾ã—ã¦10xé«˜é€ŸåŒ–
- æ¶ˆè²»é›»åŠ›1/5(æ¨è«–æ™‚)
- é•·ç³»åˆ—(1M+ tokens)å‡¦ç†ãŒå®Ÿç”¨çš„ã«

**é¡ä¼¼ä¾‹**: Googleã®TPU(Transformerå°‚ç”¨)ã€Graphcoreã®IPU(ã‚°ãƒ©ãƒ•å‡¦ç†)ã€‚SSMå°‚ç”¨ãƒãƒƒãƒ—ã‚‚2026-2027ã«ç™»å ´äºˆæƒ³ã€‚

#### SSMã®ç†è«–çš„æœªè§£æ±ºå•é¡Œ

1. **ä¸‡èƒ½è¿‘ä¼¼æ€§**: SSMã¯ä»»æ„ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†™åƒã‚’è¿‘ä¼¼ã§ãã‚‹ã‹ï¼Ÿ Transformerã¯ç†è«–çš„ã«ä¸‡èƒ½[^15]ã€‚SSMã¯ï¼Ÿ
   - **ç¾çŠ¶**: ä¸€éƒ¨ã®è¨¼æ˜ã‚ã‚Š(æ¡ä»¶ä»˜ã)ã€‚å®Œå…¨ãªä¸‡èƒ½æ€§ã¯æœªè§£æ±ºã€‚

2. **é¸æŠæ€§ã®æœ¬è³ª**: Mambaã®$\Delta_t, B_t, C_t$å…¥åŠ›ä¾å­˜ãŒã€ãªãœæ€§èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã‹ï¼Ÿ
   - **ä»®èª¬**: Content-based addressingã®è¿‘ä¼¼ã€‚ç†è«–çš„ãªå®šé‡åŒ–ã¯æœªå®Œã€‚

3. **Attention=SSMåŒå¯¾æ€§ã®æ‹¡å¼µ**: Softmax Attentionã¨SSMãŒç­‰ä¾¡ãªæ¡ä»¶ã¯ï¼Ÿ éç·šå½¢ã‚±ãƒ¼ã‚¹ã¯ï¼Ÿ
   - **Mamba-2**: Semi-Separableè¡Œåˆ—ã§è¨¼æ˜ã€‚ä¸€èˆ¬åŒ–ã¯ç¶™ç¶šç ”ç©¶ä¸­ã€‚

4. **é•·è·é›¢ä¾å­˜ã®é™ç•Œ**: SSMãŒä¿æŒã§ãã‚‹æœ€å¤§ä¾å­˜è·é›¢ã¯ï¼Ÿ $O(\log N)$? $O(N)$?
   - **HiPPOç†è«–**: å¤šé …å¼è¿‘ä¼¼ã«ã‚ˆã‚Šç†è«–çš„ã«ã¯$O(N)$ã€‚å®Ÿç”¨çš„é™ç•Œã¯ä¸æ˜ã€‚

5. **è¨“ç·´ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹**: SSMã¨Transformerã®å‹¾é…ãƒ•ãƒ­ãƒ¼ã®é•ã„ã¯ï¼Ÿ Loss landscapeã¯ï¼Ÿ
   - **è¦³æ¸¬**: SSMã¯è¨“ç·´ãŒå®‰å®š(å‹¾é…çˆ†ç™ºã—ã«ãã„)ã€‚ç†è«–çš„èª¬æ˜ã¯ä¸ååˆ†ã€‚

ã“ã‚Œã‚‰ã¯2025-2026ã®æ´»ç™ºãªç ”ç©¶é ˜åŸŸã€‚è§£æ˜ã•ã‚Œã‚Œã°ã€æ¬¡ä¸–ä»£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆæŒ‡é‡ã¨ãªã‚‹ã€‚

<details><summary>è«–æ–‡æ¨è–¦</summary>

- **S4**: Gu+ (2021), "Efficiently Modeling Long Sequences with Structured State Spaces" [^2]
- **Mamba**: Gu & Dao (2023), "Mamba: Linear-Time Sequence Modeling with Selective State Spaces" [^3]
- **HiPPO**: Gu+ (2020), "HiPPO: Recurrent Memory with Optimal Polynomial Projections" [^1]
- **SSM Survey**: "From S4 to Mamba: A Comprehensive Survey" (2025) [^11]

</details>

### 6.6 SSMã®å¿œç”¨é ˜åŸŸ

#### 6.6.1 æ™‚ç³»åˆ—äºˆæ¸¬

SSMã¯å…ƒã€…ä¿¡å·å‡¦ç†ãƒ»åˆ¶å¾¡ç†è«–ã‹ã‚‰æ¥ã¦ã„ã‚‹ãŸã‚ã€**æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«è‡ªç„¶ã«ãƒ•ã‚£ãƒƒãƒˆ**ã™ã‚‹ã€‚

**å¿œç”¨ä¾‹**:

1. **é‡‘èå¸‚å ´äºˆæ¸¬**: æ ªä¾¡ã€ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã®é•·æœŸä¾å­˜ã‚’SSMã§æ‰ãˆã‚‹
2. **ã‚¨ãƒãƒ«ã‚®ãƒ¼éœ€è¦äºˆæ¸¬**: é›»åŠ›æ¶ˆè²»ã®å­£ç¯€æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’HiPPOåˆæœŸåŒ–ã§è¨˜æ†¶
3. **æ°—è±¡äºˆæ¸¬**: æ°—æ¸©ãƒ»é™æ°´é‡ã®é•·æœŸãƒ‘ã‚¿ãƒ¼ãƒ³(æ•°é€±é–“ã€œæ•°ãƒ¶æœˆ)ã‚’SSMã§å‡¦ç†

**å®Ÿè£…ä¾‹(æ°—æ¸©äºˆæ¸¬)**:

```rust
use csv::Reader;
use ndarray::prelude::*;

// Load weather data
let temps: Vec<f32> = Reader::from_path("temperature_timeseries.csv")?
    .records()
    .map(|r| r.unwrap()[0].parse::<f32>().unwrap())
    .collect();

// Prepare sequences (sliding window)
let window_size = 365usize; // 1 year
let n = temps.len() - window_size;
let x_seqs: Vec<&[f32]> = (0..n).map(|i| &temps[i..i + window_size]).collect();
let y_vals: Vec<f32>    = (0..n).map(|i| temps[i + window_size]).collect();

// Train SSM
let d_state = 64usize;
let (a_hippo, b_hippo, c_hippo) = hippo_legs_init(d_state);
let delta = 0.01f64;
let (a_bar, b_bar) = discretize_zoh(&a_hippo.view(), &b_hippo.view(), delta);

let ssm = DiscreteSSM { a: a_bar, b: b_bar, c: c_hippo };

// foldl one-liner: run the recurrence, then project
let ssm_forecast = |x: &[f32]| -> f64 {
    let h = x.iter().fold(vec![0f64; ssm.b.len()], |h, &ut| {
        ssm.a.dot(&Array1::from(h)).iter()
            .zip(&ssm.b)
            .map(|(&ah, &bi)| ah + bi * ut as f64)
            .collect()
    });
    ssm.c.iter().zip(&h).map(|(&ci, &hi)| ci * hi).sum()
};

// Evaluate
let predictions: Vec<f64> = x_seqs.iter().map(|x| ssm_forecast(x)).collect();
let mse: f64 = predictions.iter().zip(&y_vals)
    .map(|(&p, &y)| (p - y as f64).powi(2))
    .sum::<f64>() / n as f64;
println!("MSE: {mse}");
```

**SSMã®å„ªä½æ€§**: é•·æœŸä¾å­˜(å­£ç¯€æ€§ã€å¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰)ã‚’å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä¿æŒã€‚RNNã‚ˆã‚Šè¨“ç·´å®‰å®šã€Transformerã‚ˆã‚Šãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã€‚

#### 6.6.2 éŸ³å£°å‡¦ç†

**WaveNet**ã®å¾Œç¶™ã¨ã—ã¦SSMã€‚éŸ³å£°æ³¢å½¢ã¯è¶…é•·ç³»åˆ—(16kHz â†’ 1ç§’ã§16K samples)ã€‚

**å¿œç”¨**:

1. **éŸ³å£°åˆæˆ(TTS)**: ãƒ†ã‚­ã‚¹ãƒˆâ†’éŸ³å£°æ³¢å½¢ç”Ÿæˆ
2. **éŸ³å£°èªè­˜(ASR)**: æ³¢å½¢â†’ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›
3. **éŸ³å£°å¼·èª¿**: ãƒã‚¤ã‚ºé™¤å»ã€è¶…è§£åƒ

**S4-WaveNetã®æ§‹é€ **:

**æ€§èƒ½**: WaveNet(CNN)ã¨åŒç­‰ã®éŸ³è³ªã€10xé«˜é€Ÿè¨“ç·´(ä¸¦åˆ—åŒ–)ã€æ¨è«–ã‚‚é«˜é€Ÿ(å†å¸°)ã€‚

**èª²é¡Œ**: ä½ç›¸ã®ä¿æŒã€‚SSMã¯æŒ¯å¹…ã‚’æ‰±ã†ã®ã¯å¾—æ„ã ãŒã€ä½ç›¸(sin/cos)ã¯è‹¦æ‰‹ã€‚Complexified SSM[^16]ã§è§£æ±ºã€‚

#### 6.6.3 ã‚²ãƒãƒŸã‚¯ã‚¹

**DNAé…åˆ—**ã¯è¶…é•·ç³»åˆ—(ãƒ’ãƒˆã‚²ãƒãƒ 30å„„å¡©åŸºå¯¾)ã€‚Transformerã¯ä¸å¯èƒ½ã€SSMã¯å¯èƒ½ã€‚

**å¿œç”¨**:

1. **éºä¼å­ç™ºç¾äºˆæ¸¬**: DNAé…åˆ— â†’ ã‚¿ãƒ³ãƒ‘ã‚¯è³ªç™ºç¾é‡
2. **å¤‰ç•°å½±éŸ¿äºˆæ¸¬**: SNP(ä¸€å¡©åŸºå¤šå‹)ãŒç–¾æ‚£ã«ä¸ãˆã‚‹å½±éŸ¿
3. **ã‚²ãƒãƒ ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³**: éºä¼å­ãƒ»èª¿ç¯€é ˜åŸŸã®è‡ªå‹•æ¤œå‡º

**HyenaDNA**[^17]: Hyena(SSMå¤‰ç¨®)ã‚’ç”¨ã„ãŸã‚²ãƒãƒ åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€‚100ä¸‡å¡©åŸºå¯¾ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§è¨“ç·´ã€‚

**æ€§èƒ½**: SOTA on 17/23 genomic benchmarksã€‚Transformerã¯ç³»åˆ—é•·åˆ¶ç´„ã§ä¸å¯èƒ½ã ã£ãŸã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã€‚

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**:

#### 6.6.4 å¼·åŒ–å­¦ç¿’

**æ–¹ç­–(Policy)ã®ãƒ¢ãƒ‡ãƒ«åŒ–**ã«SSMã€‚è¦³æ¸¬å±¥æ­´â†’è¡Œå‹•ã®å†™åƒã‚’é•·è·é›¢ä¾å­˜è¾¼ã¿ã§å­¦ç¿’ã€‚

**å¿œç”¨**:

1. **Atari**: ã‚²ãƒ¼ãƒ ç”»é¢ç³»åˆ— â†’ è¡Œå‹•é¸æŠ
2. **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹**: ã‚»ãƒ³ã‚µãƒ¼å±¥æ­´ â†’ ãƒ¢ãƒ¼ã‚¿ãƒ¼åˆ¶å¾¡
3. **é‡‘èå–å¼•**: å¸‚å ´å±¥æ­´ â†’ å£²è²·åˆ¤æ–­

**S4RL**[^18]: S4ã‚’DQN/PPOã®Qãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯/æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«çµ„ã¿è¾¼ã¿ã€‚

**åˆ©ç‚¹**:

- **é•·æœŸå ±é…¬**: æ•°ç™¾ã‚¹ãƒ†ãƒƒãƒ—å…ˆã®å ±é…¬ã‚’è€ƒæ…®(RNNã¯å‹¾é…æ¶ˆå¤±ã§å›°é›£)
- **ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡**: Transformerã‚ˆã‚Šå°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
- **æ¨è«–é€Ÿåº¦**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ¶å¾¡ã«å¿…è¦ãªãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å®Ÿç¾

**å®Ÿè£…ä¾‹**:

> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Mamba-2ã®SSDç†è«–ã§Attentionè¡Œåˆ—ãŒSemi-Separableè¡Œåˆ—ã¨ç­‰ä¾¡ã§ã‚ã‚‹æ¡ä»¶ã¯ä½•ã‹ï¼Ÿ
> 2. S5ï¼ˆSimplified S4ï¼‰ãŒS4ã‚ˆã‚Šå®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ«ã«ãªã£ãŸç†ç”±ã‚’ã€å¯¾è§’åŒ–ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

---


## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.10 ä»Šå›ã®å­¦ç¿’å†…å®¹

### 10.2 æœ¬è¬›ç¾©ã®ä¸»è¦ãªå­¦ã³

1. **SSMã®3å½¢æ…‹**: é€£ç¶šæ™‚é–“ODE â†’ å†å¸°(æ¨è«–) â†’ ç•³ã¿è¾¼ã¿(è¨“ç·´)
2. **é›¢æ•£åŒ–**: ZOHã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\bar{A}, \bar{B}$ã‚’è¨ˆç®—
3. **HiPPOç†è«–**: å¤šé …å¼è¿‘ä¼¼ã«ã‚ˆã‚‹é•·è·é›¢è¨˜æ†¶ã®æœ€é©åˆæœŸåŒ–
4. **S4**: DPLRåˆ†è§£ + FFTã§$O(L \log L)$è¨“ç·´
5. **Mamba**: Selective SSM($\Delta, B, C$ãŒå…¥åŠ›ä¾å­˜) + Parallel Scanã§"å¿˜ã‚Œã‚‹"é™ç•Œã‚’å…‹æœ

**æ ¸å¿ƒ**: RNNã¯å¿˜ã‚Œã€Attentionã¯$O(N^2)$ã§æ­»ã¬ã€‚SSMã¯ç†è«–(HiPPO)+æ§‹é€ (DPLR)+é¸æŠæ€§(Mamba)ã§ä¸¡æ–¹ã‚’è§£æ±ºã€‚

### 10.3 ã‚ˆãã‚ã‚‹è³ªå•(FAQ)

<details><summary>Q1: SSMã¯Transformerã‚’å®Œå…¨ã«ç½®ãæ›ãˆã‚‹ã‹ï¼Ÿ</summary>

A: ç¾æ™‚ç‚¹ã§ã¯**No**ã€‚è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ã¯Mamba â‰ˆ Transformerã€ç”»åƒã§ã¯Attentionå„ªä½ã€‚ãŸã ã—Hybrid(ç¬¬18å›)ãŒä¸»æµã«ãªã‚‹å¯èƒ½æ€§ã€‚ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚

**è©³ç´°**: Attentionã®Content-based addressingã¯ã€Few-shotå­¦ç¿’ã‚„In-context learningã§æœ¬è³ªçš„ã€‚SSMã®Position-based addressingã§ã¯å®Œå…¨ã«ä»£æ›¿ã§ããªã„ã€‚ãŸã ã—ã€å¤šãã®ã‚¿ã‚¹ã‚¯(è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€æ™‚ç³»åˆ—äºˆæ¸¬)ã§ã¯SSMã§ååˆ†ãªæ€§èƒ½ãŒå‡ºã¦ã„ã‚‹ã€‚

</details>

<details><summary>Q2: Mambaã®Selective SSMã¯LSTMã®ã‚²ãƒ¼ãƒˆã¨åŒã˜ï¼Ÿ</summary>

A: å“²å­¦ã¯ä¼¼ã¦ã„ã‚‹(é¸æŠçš„è¨˜æ†¶)ãŒã€ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ç•°ãªã‚‹ã€‚LSTMã¯éç·šå½¢ã‚²ãƒ¼ãƒˆ($\sigma, \tanh$)ã€Mambaã¯ç·šå½¢SSMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…¥åŠ›ä¾å­˜ã«ã™ã‚‹ã€‚Mambaã®æ–¹ãŒFFTè¨“ç·´ã¨å†å¸°æ¨è«–ã‚’ä¸¡ç«‹ã—ã‚„ã™ã„ã€‚

**LSTMã¨Mambaã®å¯¾å¿œ**:

| LSTM | Mamba |
|:-----|:------|
| Forget gate $f_t = \sigma(W_f [h_{t-1}, x_t])$ | $\Delta_t = \text{Softplus}(W_\Delta u_t)$ (æ¸›è¡°ç‡) |
| Input gate $i_t = \sigma(W_i [h_{t-1}, x_t])$ | $B_t = W_B u_t$ (æ›¸ãè¾¼ã¿å¼·åº¦) |
| Output gate $o_t = \sigma(W_o [h_{t-1}, x_t])$ | $C_t = W_C u_t$ (èª­ã¿å‡ºã—å¼·åº¦) |

Mambaã¯ç·šå½¢ â†’ ç•³ã¿è¾¼ã¿å½¢æ…‹ã§ä¸¦åˆ—è¨“ç·´å¯èƒ½ã€‚LSTMã¯éç·šå½¢ â†’ é€æ¬¡è¨“ç·´ã®ã¿ã€‚

</details>

<details><summary>Q3: Parallel Scanã¯æœ¬å½“ã«é€Ÿã„ï¼Ÿ</summary>

A: GPUä¸Šã§ã¯**Yes**ã€‚CPUã§ã¯ä¸¦åˆ—åº¦ãŒé™ã‚‰ã‚Œã‚‹ãŸã‚åŠ¹æœè–„ã€‚CUDAã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–ãŒå¿…é ˆã€‚Mambaã®å…¬å¼å®Ÿè£…ã¯Triton/CUDAã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(ç³»åˆ—é•·10K, d=64)**:

| å®Ÿè£… | ãƒ‡ãƒã‚¤ã‚¹ | æ™‚é–“(ms) | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ(tok/s) |
|:-----|:---------|:---------|:-------------------|
| Sequential scan | CPU | 120 | 83K |
| Parallel scan(naive) | CPU | 150 | 67K (overhead) |
| Sequential scan | GPU | 15 | 667K |
| **Parallel scan(optimized)** | **GPU** | **2.5** | **4M** |

GPU + æœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«ã§160xé«˜é€ŸåŒ–ã€‚ã“ã‚ŒãŒMambaè¨“ç·´ã®éµã€‚

</details>

<details><summary>Q4: ãªãœå›ºæœ‰å€¤ãŒè² ãªã‚‰å®‰å®šï¼Ÿ</summary>

A: $h_t = \bar{A}^t h_0$ã§ã€$\bar{A} = e^{A\Delta}$ã€‚$A$ã®å›ºæœ‰å€¤$\lambda < 0$ãªã‚‰$e^{\lambda \Delta t} \to 0$ as $t \to \infty$ã€‚çŠ¶æ…‹ãŒæ¸›è¡°â†’å®‰å®šã€‚æ­£ãªã‚‰çˆ†ç™ºâ†’ä¸å®‰å®šã€‚

**æ•°å€¤ä¾‹**:

```rust
let lambda = -2.0f64;
let delta  =  0.1f64;
let a_bar  = (lambda * delta).exp(); // exp(-0.2) â‰ˆ 0.8187

// After t steps: h_t = (0.8187)^t h_0
// t=10: h_10 â‰ˆ 0.145 h_0 (æ¸›è¡°)
// t=50: h_50 â‰ˆ 1.7e-5 h_0 (ã»ã¼æ¶ˆå¤±)
```

HiPPOã®å›ºæœ‰å€¤$-1, -2, -3, \ldots$ã¯ã€ç•°ãªã‚‹æ¸›è¡°ç‡ â†’ å¤šæ§˜ãªæ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã€‚

</details>

<details><summary>Q5: S4/Mambaã‚’è‡ªåˆ†ã®ã‚¿ã‚¹ã‚¯ã§ä½¿ã†ã«ã¯ï¼Ÿ</summary>

A: Hugging Face Transformersã«Mambaå®Ÿè£…ãŒã‚ã‚‹ã€‚`MambaForCausalLM`ã§è¨€èªãƒ¢ãƒ‡ãƒ«è¨“ç·´å¯èƒ½ã€‚ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ã¯å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª[^6]ã®examplesã‚’å‚ç…§ã€‚

</details>

<details><summary>Q6: S4ã¨Mambaã®å®Ÿè£…ã®é•ã„ã¯ï¼Ÿ</summary>

A: **S4**ã¯å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$A, B, C$ã‚’ä½¿ã„ã€ç•³ã¿è¾¼ã¿å½¢æ…‹ã§è¨“ç·´ã€‚**Mamba**ã¯å…¥åŠ›ä¾å­˜$\Delta_t, B_t, C_t$ã‚’ä½¿ã„ã€Parallel Scanã§è¨“ç·´ã€‚

**å®Ÿè£…ã®è¤‡é›‘ã•**:

| Aspect | S4 | Mamba |
|:-------|:---|:------|
| ã‚«ãƒ¼ãƒãƒ«è¨ˆç®— | FFT(æ—¢å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª) | Custom CUDA kernel |
| è¨“ç·´ | ç•³ã¿è¾¼ã¿(æ¨™æº–) | Parallel Scan(ç‰¹æ®Š) |
| æ¨è«– | å†å¸°(ç°¡å˜) | å†å¸°(ç°¡å˜) |
| ã‚³ãƒ¼ãƒ‰è¡Œæ•° | ~500 | ~1500 |

Mambaã¯é«˜æ€§èƒ½ã ãŒå®Ÿè£…ã‚³ã‚¹ãƒˆã‚‚é«˜ã„ã€‚æ•™è‚²ç›®çš„ãªã‚‰S4ã‹ã‚‰å§‹ã‚ã‚‹ã®ãŒè‰¯ã„ã€‚

</details>

<details><summary>Q7: SSMã¯ä»–ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£(ç”»åƒãƒ»éŸ³å£°)ã§ã‚‚ä½¿ãˆã‚‹ï¼Ÿ</summary>

A: **Yes**ã€‚ãŸã ã—1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–ãŒå¿…è¦ã€‚

**ç”»åƒ**: Raster/Hilbertæ›²ç·šã§1DåŒ– â†’ SSMé©ç”¨ã€‚Vision Mamba(VMamba)ã¯4æ–¹å‘ã‚¹ã‚­ãƒ£ãƒ³ã‚’ä½¿ç”¨ã€‚æ€§èƒ½ã¯ViTã«è¿«ã‚‹ãŒã€ã¾ã Atteninå„ªä½ã€‚

**éŸ³å£°**: æ³¢å½¢ã‚’ç›´æ¥SSMã§å‡¦ç†ã€‚S4-WaveNetã¯éŸ³å£°åˆæˆã§WaveNetä¸¦ã¿ã€‚

**å‹•ç”»**: ãƒ•ãƒ¬ãƒ¼ãƒ ç³»åˆ—ã¨ã—ã¦å‡¦ç†ã€‚ç©ºé–“çš„Attentionã¨ã®çµ„ã¿åˆã‚ã›(Hybrid)ãŒæœ‰æœ›ã€‚

**ãƒã‚¤ãƒ³ãƒˆã‚³ãƒ©ã‚¦ãƒ‰**: 3Dç‚¹ç¾¤ã‚’1DåŒ–(z-order curve) â†’ SSMã€‚ç ”ç©¶æ®µéšã€‚

</details>

<details><summary>Q8: SSMã®è¨“ç·´ã¯Transformerã‚ˆã‚Šé€Ÿã„ï¼Ÿ</summary>

A: **è¨“ç·´é€Ÿåº¦ã¯åŒç­‰ã€œã‚„ã‚„é€Ÿã„**ã€‚æ¨è«–ã¯SSMãŒåœ§å€’çš„ã«é€Ÿã„ã€‚

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°, 125M params)**:

| Model | è¨“ç·´æ™‚é–“(100K steps) | æ¨è«–é€Ÿåº¦(tok/s) | ãƒ¡ãƒ¢ãƒª(è¨“ç·´) |
|:------|:---------------------|:----------------|:-------------|
| Transformer | 48h | 2.3K | 24GB |
| S4 | 52h | 7K | 18GB |
| **Mamba** | **45h** | **11.5K** | **16GB** |

Mambaã¯è¨“ç·´ã‚‚ã‚„ã‚„é€Ÿãã€æ¨è«–ã¯5å€é€Ÿã€‚ãƒ¡ãƒ¢ãƒªã‚‚å‰Šæ¸›ã€‚

</details>

<details><summary>Q9: HiPPOåˆæœŸåŒ–ã¯å¿…é ˆï¼Ÿ</summary>

A: é•·è·é›¢ä¾å­˜ã‚¿ã‚¹ã‚¯(LRA Path-Xç­‰)ã§ã¯**ã»ã¼å¿…é ˆ**ã€‚çŸ­è·é›¢ã‚¿ã‚¹ã‚¯ã§ã¯ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§ã‚‚å¯ã€‚

**å®Ÿé¨“çµæœ(ã‚³ãƒ”ãƒ¼ã‚¿ã‚¹ã‚¯, T=1000)**:

| åˆæœŸåŒ– | Test Acc | è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•° |
|:-------|:---------|:---------------|
| Random | 32% | 100 (åæŸã›ãš) |
| **HiPPO** | **87%** | **50** |

HiPPOã¯é•·è·é›¢è¨˜æ†¶ã®ç†è«–çš„ä¿è¨¼ãŒã‚ã‚Šã€è¨“ç·´ã‚‚å®‰å®šãƒ»é«˜é€Ÿã€‚

</details>

<details><summary>Q10: SSMã¯è¨ˆç®—è¤‡é›‘æ€§ç†è«–ã§ä½•ãŒã§ãã‚‹ï¼Ÿ</summary>

A: **ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°å®Œå…¨æ€§**ã¯è¨¼æ˜ã•ã‚Œã¦ã„ãªã„(Transformerã¯æ¡ä»¶ä»˜ãã§è¨¼æ˜æ¸ˆã¿[^15])ã€‚

**ç¾çŠ¶ã®ç†è§£**:

- SSMã¯**ç·šå½¢å†å¸°**ã®ä¸€ç¨® â†’ æœ‰é™çŠ¶æ…‹ã‚ªãƒ¼ãƒˆãƒãƒˆãƒ³ã¨ç­‰ä¾¡(ç†è«–ä¸Š)
- Mambaã®Selective SSMã¯ã€å…¥åŠ›ä¾å­˜ã§**çŠ¶æ…‹é·ç§»é–¢æ•°ãŒå¤‰åŒ–** â†’ ã‚ˆã‚Šè¡¨ç¾åŠ›ãŒé«˜ã„
- Mamba-2/SSDã¯ã€ŒAttention â‰ˆ SSMã€ã‚’ç¤ºã—ãŸ â†’ ç†è«–çš„ç­‰ä¾¡æ€§ã®è¨¼æ˜

**æœªè§£æ±ºå•é¡Œ**: MambaãŒTransformerã¨åŒç­‰ã®ã‚¿ã‚¹ã‚¯ã‚’è§£ã‘ã‚‹ã‹ï¼Ÿ å®Ÿè¨¼çš„ã«ã¯**Yes**ã ãŒã€ç†è«–çš„è¨¼æ˜ã¯æœªå®Œã€‚

</details>

### 10.7 æ¬¡å›äºˆå‘Š: ç¬¬17å› Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³•

ç¬¬17å›ã§ã¯ã€Mambaã®é€²åŒ–ã¨ç·šå½¢RNN/Attentionãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’æ‰±ã†:

- **Mamba-2/SSD**: Attention=SSMåŒå¯¾æ€§ã®å®Œå…¨è¨¼æ˜
- **RWKV**: Receptance Weighted Key Valueã€ç·šå½¢RNN
- **RetNet**: Retentionæ©Ÿæ§‹ã€Multi-scale decay
- **GLA**: Gated Linear Attention
- **Vision Mamba**: VMamba/Vimã®ç”»åƒSSM
- **Hybridè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³**: AttentionÃ—SSMã®çµ„ã¿åˆã‚ã›æ–¹

**åˆ°é”ç‚¹**: ã€ŒSSMã ã‘ã§ååˆ†ã‹ï¼ŸAttentionã‚’æ¨ã¦ãã‚Œãªã„ç†ç”±ã¯ï¼Ÿã€ã¨ã„ã†å•ã„ã«ç­”ãˆã€ç¬¬18å›ã®Hybrid architectureã¸ã®æ©‹ã‚’æ¶ã‘ã‚‹ã€‚

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**:
- SSD (Structured State Space Duality)
- Semi-Separableè¡Œåˆ—
- RWKV Time-mix
- RetNet Multi-scale decay
- Vision Mambaã®2D Selective Scan

**äºˆç¿’æ¨å¥¨è«–æ–‡**:
- Mamba-2: [arXiv:2405.21060](https://arxiv.org/abs/2405.21060)
- RWKV: [arXiv:2305.13048](https://arxiv.org/abs/2305.13048)
- RetNet: [arXiv:2307.08621](https://arxiv.org/abs/2307.08621)

> **Note:** **é€²æ—: 100% å®Œäº†** ç¬¬16å›SSMç†è«–ã‚’å®Œèµ°ã€‚é€£ç¶šâ†’é›¢æ•£â†’HiPPOâ†’S4â†’Mambaã®å…¨æ—…ç¨‹ã‚’è¸ç ´ã—ãŸã€‚Course IIã‚‚æ®‹ã‚Š2å›ã€‚Mamba-2ã¨Hybridã§ç†è«–ç·¨ã‚’å®Œçµã•ã›ã‚‹ã€‚

---

### 6.15 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **"å¿˜ã‚Œã‚‹"ã“ã¨ã“ãRNNã®æœ¬è³ªçš„é™ç•Œã ã£ãŸã€‚Mambaã¯é¸æŠçš„è¨˜æ†¶ã§ãã‚Œã‚’å…‹æœã—ãŸã€‚ã ãŒå•ã„ãŸã„â€•â€•SSMã ã‘ã§ååˆ†ãªã®ã‹ï¼ŸAttentionã‚’æ¨ã¦ãã‚Œãªã„ç†ç”±ã¯ä½•ã‹ï¼Ÿ**

Mambaã¯é•·è·é›¢ä¾å­˜ã‚’$O(N)$ã§æ‰±ãˆã‚‹ã€‚ã ãŒ**å…¨ç³»åˆ—ã‚’åŒæ™‚ã«å‚ç…§ã™ã‚‹èƒ½åŠ›**(Attentionã®æœ¬è³ª)ã¯æŒãŸãªã„ã€‚Few-shot learningã€æ¨è«–ã‚¿ã‚¹ã‚¯ã€å‹•çš„ãªæ–‡è„ˆåˆ‡ã‚Šæ›¿ãˆã§ã¯ã€AttentionãŒä¾ç„¶ã¨ã—ã¦å„ªä½ã€‚

ç¬¬17å›ã§ã€Mamba-2/SSDãŒã€ŒAttention=SSMã€ã®ç­‰ä¾¡æ€§ã‚’è¨¼æ˜ã—ãŸã“ã¨ã‚’å­¦ã¶ã€‚ã¤ã¾ã‚Š**å¯¾ç«‹ã§ã¯ãªãã€çµ±ä¸€**ã¸å‘ã‹ã£ã¦ã„ã‚‹ã€‚

ç¬¬18å›ã§ã¯ã€Jambaã‚„Zambaã®ã‚ˆã†ã«ã€**Attentionã¨SSMã‚’çµ„ã¿åˆã‚ã›ãŸHybrid**ãŒã€Œæœ€å¼·ã€ã§ã¯ãªãã€Œæœ€é©ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

**å•ã„ç¶šã‘ã‚ˆ**: "æœ€å¼·"ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å­˜åœ¨ã—ãªã„ã€‚ã‚¿ã‚¹ã‚¯ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»è¨ˆç®—è³‡æºã«å¿œã˜ã¦ã€çµ„ã¿åˆã‚ã›ã‚‹ã€‚ãã‚ŒãŒã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æœ¬è³ªã§ã¯ãªã„ã‹ï¼Ÿ

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Gu, A., Dao, T., Ermon, S., Rudra, A., & RÃ©, C. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. *NeurIPS 2020*.
<https://arxiv.org/abs/2008.07669>

[^2]: Gu, A., Goel, K., & RÃ©, C. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. *ICLR 2022*.
<https://arxiv.org/abs/2111.00396>

[^3]: Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. *arXiv:2312.00752*.
<https://arxiv.org/abs/2312.00752>

[^4]: Kalman, R. E. (1960). A New Approach to Linear Filtering and Prediction Problems. *Journal of Basic Engineering*.

[^5]: Tay, Y., Dehghani, M., Abnar, S., et al. (2021). Long Range Arena: A Benchmark for Efficient Transformers. *ICLR 2021*.
<https://arxiv.org/abs/2011.04006>

[^6]: Gu, A., & Dao, T. (2023). Mamba Official Repository.
<https://github.com/state-spaces/mamba>

[^7]: Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. *ICML 2024*.
<https://arxiv.org/abs/2405.21060>

[^8]: Liu, Y., Tian, Y., Zhao, Y., et al. (2024). VMamba: Visual State Space Models.
<https://arxiv.org/abs/2401.10166>

[^9]: Peng, B., Alcaide, E., Anthony, Q., et al. (2023). RWKV: Reinventing RNNs for the Transformer Era.
<https://arxiv.org/abs/2305.13048>

[^10]: Sun, Y., Dong, L., Huang, S., et al. (2023). Retentive Network: A Successor to Transformer for Large Language Models.
<https://arxiv.org/abs/2307.08621>

[^11]: Somvanshi, S., Islam, Md M., et al. (2025). From S4 to Mamba: A Comprehensive Survey on Structured State Space Models. *arXiv:2503.18970*.
<https://arxiv.org/abs/2503.18970>

### æ•™ç§‘æ›¸

- Ogata, K. (2009). *Modern Control Engineering* (5th ed.). Prentice Hall. [åˆ¶å¾¡ç†è«–ã®å¤å…¸]
- Chen, C.-T. (1998). *Linear System Theory and Design* (3rd ed.). Oxford University Press. [çŠ¶æ…‹ç©ºé–“ã®æ•°å­¦]
- Rush, A. (2023). *The Annotated S4*. [å®Ÿè£…ä»˜ãè§£èª¬]
  <https://srush.github.io/annotated-s4/>

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

---
