---
title: "ç¬¬16å›: SSMç†è«– & Mambaã®å…‹æœ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ¦›"
type: "tech"
topics: ["machinelearning", "deeplearning", "ssm", "julia", "rust"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³(45åˆ†) â€” Juliaã¨Rustã§SSMã‚’å‹•ã‹ã™

### 4.1 ç’°å¢ƒæ§‹ç¯‰

#### Juliaç’°å¢ƒ

```bash
# Julia 1.11+ (2025-2026 latest)
curl -fsSL https://install.julialang.org | sh

# Packages
julia -e 'using Pkg; Pkg.add(["LinearAlgebra", "FFTW", "Plots", "DifferentialEquations", "ProgressMeter"])'
```

#### Rustç’°å¢ƒ

```bash
# Rust 1.83+ (2026)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Dependencies in Cargo.toml
[dependencies]
ndarray = "0.16"
ndarray-linalg = "0.17"
rayon = "1.10"
```

### 4.2 é›¢æ•£SSMã®å®Œå…¨å®Ÿè£…(Julia)

```julia
using LinearAlgebra
using FFTW

"""
Discrete SSM module
Implements: h_t = A h_{t-1} + B u_t, y_t = C h_t + D u_t
"""
struct DiscreteSSM
    A::Matrix{Float64}
    B::Vector{Float64}
    C::Vector{Float64}
    D::Float64
end

# Recurrent form (for inference)
function forward_recurrent(ssm::DiscreteSSM, u::Vector{Float64})
    N = length(u)
    d = length(ssm.B)

    h = zeros(Float64, d)
    y = zeros(Float64, N)

    for t in 1:N
        h = ssm.A * h + ssm.B * u[t]
        y[t] = dot(ssm.C, h) + ssm.D * u[t]
    end

    return y
end

# Convolutional form (for training)
function forward_convolution(ssm::DiscreteSSM, u::Vector{Float64}, L::Int)
    # Precompute kernel K[k] = C * A^k * B
    d = length(ssm.B)
    K = zeros(Float64, L)
    Ai = Matrix{Float64}(I, d, d)  # A^0

    for k in 1:L
        Ai = ssm.A * Ai  # A^k
        K[k] = dot(ssm.C, Ai * ssm.B)
    end

    # FFT convolution
    K_pad = [K; zeros(length(u))]
    u_pad = [u; zeros(length(K))]

    y_fft = fft(K_pad) .* fft(u_pad)
    y = real.(ifft(y_fft))[1:length(u)]

    return y, K
end

# Example usage
d = 8
A = 0.9 * Matrix{Float64}(I, d, d) + 0.05 * randn(d, d)  # stable matrix
B = randn(Float64, d)
C = randn(Float64, d)
D = 0.0

ssm = DiscreteSSM(A, B, C, D)

u = randn(Float64, 64)
y_rec = forward_recurrent(ssm, u)
y_conv, K = forward_convolution(ssm, u, 64)

println("Recurrent output (first 5): ", round.(y_rec[1:5], digits=3))
println("Convolution output (first 5): ", round.(y_conv[1:5], digits=3))
println("Max difference: ", maximum(abs.(y_rec - y_conv)))
```

### 4.3 HiPPO-LegSåˆæœŸåŒ–

```julia
"""
HiPPO-LegS initialization for A and B
Returns matrices with optimal long-range memory properties
"""
function hippo_legs_init(d::Int)
    A = zeros(Float64, d, d)
    B = zeros(Float64, d)

    for n in 0:d-1
        for k in 0:d-1
            if n > k
                A[n+1, k+1] = -(2*n + 1)^0.5 * (2*k + 1)^0.5
            elseif n == k
                A[n+1, k+1] = Float64(n + 1)
            end
        end
        B[n+1] = (2*n + 1)^0.5
    end

    # Initialize C randomly (or all ones)
    C = ones(Float64, d)

    return A, B, C
end

# Test HiPPO eigenvalues
d = 16
A_hippo, B_hippo, C_hippo = hippo_legs_init(d)

Î» = eigvals(A_hippo)
println("HiPPO eigenvalues (real parts): ", round.(real.(Î»), digits=2))
println("All negative? ", all(real.(Î») .< 0))  # Should be true
```

### 4.4 Zero-Order Hold é›¢æ•£åŒ–

```julia
"""
Zero-Order Hold discretization: continuous SSM â†’ discrete SSM
A_bar = exp(A * Î”)
B_bar = (A^{-1} (exp(A*Î”) - I)) B
"""
function discretize_zoh(A::Matrix{Float64}, B::Vector{Float64}, Î”::Float64)
    d = size(A, 1)

    # A_bar = exp(A * Î”)
    A_bar = exp(A * Î”)

    # B_bar = (A^{-1} (A_bar - I)) B
    # Use matrix exponential properties for numerical stability
    if det(A) != 0.0
        B_bar = (A \ (A_bar - I)) * B
    else
        # Numerical integration fallback
        dt = Î” / 100
        B_bar = sum([exp(A * t) * B * dt for t in 0:dt:Î”])
    end

    return A_bar, B_bar
end

# Test: continuous â†’ discrete
A_cont = [-0.5 0.0; 0.0 -0.3]
B_cont = [1.0, 0.0]
Î” = 0.1

A_disc, B_disc = discretize_zoh(A_cont, B_cont, Î”)
println("Continuous A eigenvalues: ", eigvals(A_cont))
println("Discrete A eigenvalues: ", eigvals(A_disc))
println("Expected (exp(Î»*Î”)): ", exp.(eigvals(A_cont) * Î”))
```

### 4.5 S4 Simplified: å¯¾è§’SSM + FFTç•³ã¿è¾¼ã¿

```julia
using FFTW

"""
Simplified S4: diagonal A for efficiency
Assumes A is diagonalizable: A = V Î› V^{-1}
"""
struct S4Layer
    Î»::Vector{ComplexF64}   # Diagonal of A (eigenvalues)
    B::Vector{ComplexF64}
    C::Vector{ComplexF64}
    Î”::Float64
end

function s4_forward(layer::S4Layer, u::Vector{Float64}, L::Int)
    d = length(layer.Î»)

    # Discretize
    Î»_bar = exp.(layer.Î» * layer.Î”)

    # Compute kernel via closed form: K[k] = C^T * diag(Î»_bar^k) * B
    K = zeros(ComplexF64, L)
    for k in 0:L-1
        K[k+1] = dot(layer.C, (Î»_bar .^ k) .* layer.B)
    end

    # FFT convolution
    K_real = real.(K)  # If C, B chosen to make K real
    K_pad = [K_real; zeros(length(u))]
    u_pad = [u; zeros(length(K_real))]

    y_fft = fft(K_pad) .* fft(u_pad)
    y = real.(ifft(y_fft))[1:length(u)]

    return y
end

# Example: S4 with HiPPO-like eigenvalues
d = 32
Î» = ComplexF64.(-(1:d))  # HiPPO-like: -1, -2, ..., -d
B = ones(ComplexF64, d) ./ sqrt(d)
C = ones(ComplexF64, d) ./ sqrt(d)
Î” = 0.01

s4 = S4Layer(Î», B, C, Î”)

u = randn(Float64, 256)
y_s4 = s4_forward(s4, u, 256)

println("S4 output (first 5): ", round.(y_s4[1:5], digits=3))
```

### 4.6 Mambaã®ç°¡æ˜“å®Ÿè£…: Selective SSM

å®Œå…¨ãªMambaã¯CUDAã‚«ãƒ¼ãƒãƒ«ã‚’è¦ã™ã‚‹ãŒã€æ•™è‚²çš„ãªç°¡æ˜“ç‰ˆ:

```julia
"""
Simplified Mamba: input-dependent Î”, B, C (without hardware-aware scan)
"""
struct MambaLayer
    A::Matrix{Float64}
    W_Î”::Matrix{Float64}
    W_B::Matrix{Float64}
    W_C::Matrix{Float64}
    d_state::Int
end

function mamba_forward_simple(layer::MambaLayer, u::Matrix{Float64})
    # u: (seq_len, d_model)
    L, D = size(u)
    d = layer.d_state

    # Compute input-dependent parameters
    Î” = softplus.(u * layer.W_Î”')  # (L, d_state)
    B = u * layer.W_B'               # (L, d_state)
    C = u * layer.W_C'               # (L, d_state)

    # Sequential scan (simplified, not parallelized)
    h = zeros(Float64, d)
    y = zeros(Float64, L)

    for t in 1:L
        # Discretize with Î”[t]
        A_bar = exp(layer.A * Î”[t, 1])  # Simplified: scalar Î”
        B_bar = (layer.A \ (A_bar - I)) * B[t, :]

        # Update
        h = A_bar * h + B_bar
        y[t] = dot(C[t, :], h)
    end

    return y
end

softplus(x) = log(1 + exp(x))

# Example
d_state, d_model = 4, 8
A = -1.0 * Matrix{Float64}(I, d_state, d_state)  # Simple: -I
W_Î” = randn(Float64, d_model, d_state) * 0.1
W_B = randn(Float64, d_model, d_state)
W_C = randn(Float64, d_model, d_state)

mamba = MambaLayer(A, W_Î”, W_B, W_C, d_state)

u = randn(Float64, 16, d_model)  # (seq_len=16, d_model=8)
y_mamba = mamba_forward_simple(mamba, u)

println("Mamba output (first 5): ", round.(y_mamba[1:5], digits=3))
```

:::message
**æ³¨æ„**: ä¸Šè¨˜ã¯Mambaã®åŸç†ã‚’ç¤ºã™æ•™è‚²çš„å®Ÿè£…ã€‚å®Ÿéš›ã®Mambaã¯:
1. Parallel Scanã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–
2. CUDAã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–(hardware-aware scan)
3. è¤‡æ•°ã®Mambaãƒ–ãƒ­ãƒƒã‚¯ã‚’ç©å±¤
ãŒå¿…è¦ã€‚æœ¬æ ¼çš„å®Ÿè£…ã¯å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª[^6]ã‚’å‚ç…§ã€‚
:::

### 4.7 Rustã§ã®ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè£…

```rust
// Cargo.toml
// [dependencies]
// ndarray = "0.16"
// rayon = "1.10"

use ndarray::{Array1, Array2};
use rayon::prelude::*;

/// Parallel scan for SSM: h[t] = A[t] * h[t-1] + B[t]
/// Returns all hidden states h[0..L]
fn parallel_scan(
    A: &Vec<Array2<f64>>,  // Vec of (d, d) matrices
    B: &Vec<Array1<f64>>,  // Vec of (d,) vectors
) -> Vec<Array1<f64>> {
    let L = A.len();
    let d = B[0].len();

    // Base case: sequential scan (for simplicity)
    let mut h = vec![Array1::zeros(d)];
    for t in 0..L {
        let h_next = A[t].dot(&h[t]) + &B[t];
        h.push(h_next);
    }

    h[1..].to_vec()  // Return h[1..L]
}

fn main() {
    let L = 8;
    let d = 2;

    // Example: A[t] = 0.9 * I
    let A: Vec<Array2<f64>> = (0..L)
        .map(|_| Array2::eye(d) * 0.9)
        .collect();

    // B[t] = random
    let B: Vec<Array1<f64>> = (0..L)
        .map(|_| Array1::from_vec(vec![1.0, 0.5]))
        .collect();

    let h = parallel_scan(&A, &B);

    println!("Hidden states:");
    for (t, h_t) in h.iter().enumerate() {
        println!("h[{}] = {:?}", t+1, h_t);
    }
}
```

çœŸã®ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã¯`rayon`ã®prefix sumãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ã†ãŒã€associative operationã®å®šç¾©ãŒå¿…è¦ã€‚è©³ç´°ã¯[^3]ã®Appendixã€‚

#### Rustä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã®ç†è«–çš„èƒŒæ™¯

**Associative Scan**ã®åŸç†: æ¼”ç®—$\circ$ãŒçµåˆçš„($(a \circ b) \circ c = a \circ (b \circ c)$)ãªã‚‰ã€äºŒåˆ†æœ¨æ§‹é€ ã§ä¸¦åˆ—è¨ˆç®—å¯èƒ½ã€‚

SSMã®å ´åˆ:

$$
(A_2, B_2) \circ (A_1, B_1) = (A_2 A_1, A_2 B_1 + B_2)
$$

ã“ã®æ¼”ç®—ã¯çµåˆçš„:

$$
\begin{aligned}
&((A_3, B_3) \circ (A_2, B_2)) \circ (A_1, B_1) \\
&= (A_3 A_2, A_3 B_2 + B_3) \circ (A_1, B_1) \\
&= (A_3 A_2 A_1, A_3 A_2 B_1 + A_3 B_2 + B_3)
\end{aligned}
$$

$$
\begin{aligned}
&(A_3, B_3) \circ ((A_2, B_2) \circ (A_1, B_1)) \\
&= (A_3, B_3) \circ (A_2 A_1, A_2 B_1 + B_2) \\
&= (A_3 A_2 A_1, A_3(A_2 B_1 + B_2) + B_3) \\
&= (A_3 A_2 A_1, A_3 A_2 B_1 + A_3 B_2 + B_3)
\end{aligned}
$$

ä¸€è‡´ã™ã‚‹ $\square$

**ä¸¦åˆ—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
Level 0: [(A1,B1), (A2,B2), (A3,B3), (A4,B4), (A5,B5), (A6,B6), (A7,B7), (A8,B8)]
         â†“ Parallel combine pairs
Level 1: [(A2A1, A2B1+B2), (A4A3, A4B3+B4), (A6A5, A6B5+B6), (A8A7, A8B7+B8)]
         â†“ Parallel combine pairs
Level 2: [(A4A3A2A1, ...), (A8A7A6A5, ...)]
         â†“ Parallel combine
Level 3: [(A8A7A6A5A4A3A2A1, ...)]
```

æ·±ã•$\log_2 L$ã€ç·work $O(L)$ã€‚

```rust
use rayon::prelude::*;

/// Associative operation for SSM scan
type ScanOp = (Array2<f64>, Array1<f64>);

fn combine(left: &ScanOp, right: &ScanOp) -> ScanOp {
    let (A_left, B_left) = left;
    let (A_right, B_right) = right;

    let A_new = A_right.dot(A_left);
    let B_new = A_right.dot(B_left) + B_right;

    (A_new, B_new)
}

/// True parallel scan using Rayon (conceptual)
fn parallel_scan_associative(ops: Vec<ScanOp>) -> Vec<Array1<f64>> {
    // Rayon's scan requires sequential accumulation
    // For true parallelism, use tree-based reduction
    // This is conceptual; production requires CUDA/GPU

    let L = ops.len();
    let d = ops[0].1.len();

    // Sequential fallback (Rust CPU)
    let mut h = Array1::zeros(d);
    let mut results = Vec::with_capacity(L);

    for (A, B) in ops {
        h = A.dot(&h) + &B;
        results.push(h.clone());
    }

    results
}
```

**æ³¨æ„**: CPUã§ã®ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã¯ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¤§ããã€ç´ æœ´ãªé€æ¬¡å®Ÿè£…ã«åŠ£ã‚‹ã“ã¨ãŒå¤šã„ã€‚**GPUã‚„TPUã§ã¯åŠ‡çš„ã«é«˜é€ŸåŒ–**ã™ã‚‹ã€‚Mambaã¯Tritonã§CUDAã‚«ãƒ¼ãƒãƒ«ã‚’æ›¸ã„ã¦ã„ã‚‹[^3]ã€‚

#### Cargo.tomlã®å®Œå…¨ç‰ˆ

```toml
[package]
name = "ssm_rust"
version = "0.1.0"
edition = "2021"

[dependencies]
ndarray = "0.16"
ndarray-linalg = { version = "0.17", features = ["openblas-static"] }
rayon = "1.10"
num-complex = "0.4"
approx = "0.5"  # for testing

[dev-dependencies]
criterion = "0.5"

[[bench]]
name = "ssm_bench"
harness = false
```

#### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š

```rust
// benches/ssm_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use ssm_rust::{parallel_scan, DiscreteSSM};

fn bench_ssm_scan(c: &mut Criterion) {
    let L = 1024;
    let d = 64;

    let A: Vec<_> = (0..L).map(|_| Array2::eye(d) * 0.9).collect();
    let B: Vec<_> = (0..L).map(|_| Array1::from_vec(vec![1.0; d])).collect();

    c.bench_function("ssm_scan_1024", |b| {
        b.iter(|| {
            parallel_scan(black_box(&A), black_box(&B))
        });
    });
}

criterion_group!(benches, bench_ssm_scan);
criterion_main!(benches);
```

å®Ÿè¡Œ:
```bash
cargo bench
```

### 4.8 Mathâ†”Codeå¯¾å¿œè¡¨: SSMã®å®Œå…¨ãƒãƒƒãƒ”ãƒ³ã‚°

| æ•°å¼ | Julia | Rust | èª¬æ˜ |
|:-----|:------|:-----|:-----|
| $h_t = \bar{A}h_{t-1} + \bar{B}u_t$ | `h = A * h + B * u[t]` | `h = A.dot(&h) + &B * u[t]` | å†å¸°æ›´æ–° |
| $y_t = Ch_t$ | `y[t] = dot(C, h)` | `y[t] = C.dot(&h)` | å‡ºåŠ›æŠ•å½± |
| $\bar{A} = e^{A\Delta}$ | `A_bar = exp(A * Î”)` | `A_bar = A.mapv(\|x\| (x*Î”).exp())` (diagonal) | é›¢æ•£åŒ– |
| $\bar{B} = (A^{-1}(e^{A\Delta}-I))B$ | `B_bar = (A \ (A_bar - I)) * B` | `B_bar = A.inv()?.dot(&(A_bar - I)).dot(&B)` | é›¢æ•£åŒ– |
| $\bar{\mathcal{K}}_k = C\bar{A}^kB$ | `K[k] = dot(C, (A^k) * B)` | `K[k] = C.dot(&A.pow(k)).dot(&B)` | ã‚«ãƒ¼ãƒãƒ« |
| $y = \bar{\mathcal{K}} * u$ | `y = real.(ifft(fft(K) .* fft(u)))` | `y = ifft(fft(K) * fft(u))` | FFTç•³ã¿è¾¼ã¿ |
| $\Delta_t = \text{Softplus}(W_\Delta u_t)$ | `Î” = softplus.(u * W_Î”')` | `Î” = (u.dot(&W_Î”)).mapv(softplus)` | Mamba |
| $(A_2, B_2) \circ (A_1, B_1)$ | `(A2*A1, A2*B1 + B2)` | `(A2.dot(&A1), A2.dot(&B1) + B2)` | Scanæ¼”ç®— |

**1å¯¾1å¯¾å¿œã®å¾¹åº•**: å…¨ã¦ã®æ•°å¼ãŒã€ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œè¡Œã¨ä¸€è‡´ã™ã‚‹ã€‚èª­è€…ã¯ã€Œã“ã®è¡Œ = ã“ã®æ•°å¼ã€ã¨å³åº§ã«ç†è§£ã§ãã‚‹ã€‚

### 4.9 ãƒ‡ãƒãƒƒã‚°ã¨æ•°å€¤å®‰å®šæ€§ã®Tips

#### Tip 1: è¡Œåˆ—æŒ‡æ•°é–¢æ•°ã®è¨ˆç®—

`exp(A * Î”)`ã¯æ•°å€¤çš„ã«ä¸å®‰å®šãªå ´åˆãŒã‚ã‚‹ã€‚ç‰¹ã«$A$ã®å›ºæœ‰å€¤ãŒå¤§ãã„ã¨ãã€‚

**å¯¾ç­–**: PadÃ©è¿‘ä¼¼ã‚„SciPyã®`expm`ã‚’ä½¿ã†ã€‚

```julia
using LinearAlgebra

# Safe matrix exponential
function safe_exp(A::Matrix{Float64}, Î”::Float64)
    # Check condition number
    if cond(A) > 1e10
        @warn "Matrix A is ill-conditioned, exp(A*Î”) may be inaccurate"
    end

    return exp(A * Î”)
end
```

#### Tip 2: å›ºæœ‰å€¤ã®ç¢ºèª

è¨“ç·´å‰ã«$A$ã®å›ºæœ‰å€¤ã‚’ç¢ºèªã—ã€å®Ÿéƒ¨ãŒæ­£ã®ã‚‚ã®ãŒã‚ã‚Œã°è­¦å‘Šã€‚

```julia
function check_stability(A::Matrix{Float64})
    Î» = eigvals(A)
    unstable = filter(x -> real(x) > 0, Î»)

    if !isempty(unstable)
        @warn "Unstable eigenvalues detected: $(unstable)"
        return false
    end
    return true
end
```

#### Tip 3: Softplusã®æ•°å€¤å®‰å®šç‰ˆ

$\text{Softplus}(x) = \log(1 + e^x)$ã¯$x$ãŒå¤§ãã„ã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã€‚

```julia
# Numerically stable softplus
function softplus_stable(x::Float64)
    if x > 20.0
        return x  # log(1 + e^x) â‰ˆ x for large x
    else
        return log(1 + exp(x))
    end
end
```

#### Tip 4: FFTã®zero-padding

ç•³ã¿è¾¼ã¿ã§FFTã‚’ä½¿ã†éš›ã€circular convolutionã‚’é¿ã‘ã‚‹ãŸã‚ã€ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¿…é ˆã€‚

```julia
# Correct FFT convolution
function fft_conv_correct(K::Vector{Float64}, u::Vector{Float64})
    L_K, L_u = length(K), length(u)
    L_pad = L_K + L_u - 1

    K_pad = [K; zeros(L_pad - L_K)]
    u_pad = [u; zeros(L_pad - L_u)]

    y_fft = fft(K_pad) .* fft(u_pad)
    y = real.(ifft(y_fft))[1:L_u]

    return y
end
```

:::message
**é€²æ—: 70% å®Œäº†** SSM/S4/Mambaã®å®Ÿè£…ã‚’å®Œäº†ã€‚Juliaæ•°å¼ç¾ã¨Rustä¸¦åˆ—åŒ–ã€ãã—ã¦Mathâ†”Codeå®Œå…¨å¯¾å¿œã‚’ä½“é¨“ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã§æ€§èƒ½ã‚’ç¢ºèªã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³(30åˆ†) â€” Long Range Arenaã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

æ¬¡ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆ:

:::details Q1: $h_t = \bar{A} h_{t-1} + \bar{B} u_t$
**èª­ã¿**: "h sub t equals A bar times h sub t minus 1 plus B bar times u sub t"
**æ„å‘³**: é›¢æ•£SSMã®å†å¸°æ›´æ–°å¼ã€‚éš ã‚ŒçŠ¶æ…‹$h_t$ã¯ã€å‰æ™‚åˆ»ã®çŠ¶æ…‹$h_{t-1}$ã‚’è¡Œåˆ—$\bar{A}$ã§å¤‰æ›ã—ã€å…¥åŠ›$u_t$ã‚’$\bar{B}$ã§æŠ•å½±ã—ãŸå’Œã€‚
:::

:::details Q2: $\bar{\mathcal{K}}_k = C \bar{A}^k \bar{B}$
**èª­ã¿**: "K bar sub k equals C times A bar to the power k times B bar"
**æ„å‘³**: SSMç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«ã®ç¬¬$k$è¦ç´ ã€‚$k$ã‚¹ãƒ†ãƒƒãƒ—å‰ã®å…¥åŠ›ãŒç¾åœ¨ã®å‡ºåŠ›ã«ä¸ãˆã‚‹å½±éŸ¿åº¦ã€‚$\bar{A}^k$ã«ã‚ˆã‚ŠæŒ‡æ•°æ¸›è¡°ã€‚
:::

:::details Q3: $A_{\text{HiPPO}} = \Lambda - PQ^*$
**èª­ã¿**: "A HiPPO equals Lambda minus P Q dagger"
**æ„å‘³**: HiPPOè¡Œåˆ—ã®DPLRåˆ†è§£ã€‚$\Lambda$ã¯å¯¾è§’(å›ºæœ‰å€¤)ã€$-PQ^*$ã¯ä½ãƒ©ãƒ³ã‚¯è£œæ­£ã€‚$Q^*$ã¯$Q$ã®å…±å½¹è»¢ç½®ã€‚
:::

:::details Q4: $\Delta_t = \text{Softplus}(W_\Delta u_t + b_\Delta)$
**èª­ã¿**: "Delta sub t equals softplus of W Delta u sub t plus b Delta"
**æ„å‘³**: Mambaã®å…¥åŠ›ä¾å­˜æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—å¹…ã€‚Softplusã§$\Delta_t > 0$ã‚’ä¿è¨¼ã€‚å…¥åŠ›ã«ã‚ˆã‚Šé›¢æ•£åŒ–ã®ç´°ã‹ã•ãŒå¤‰åŒ–ã€‚
:::

:::details Q5: $(A_2, B_2) \circ (A_1, B_1) = (A_2 A_1, A_2 B_1 + B_2)$
**èª­ã¿**: "A two, B two circle A one, B one equals A two A one, A two B one plus B two"
**æ„å‘³**: Parallel Scanã®çµåˆæ¼”ç®—å­ã€‚2ã¤ã®ç·šå½¢å¤‰æ›$(A, B)$ã‚’åˆæˆã€‚$h_2 = A_2(A_1 h_0 + B_1) + B_2 = A_2A_1 h_0 + (A_2B_1 + B_2)$ã‚’è¡¨ã™ã€‚
:::

### 5.2 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

#### Challenge 1: HiPPO vs Random initialization

HiPPOåˆæœŸåŒ–ã¨ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§SSMã‚’è¨“ç·´ã—ã€Long Rangeä¾å­˜ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’æ¯”è¼ƒã›ã‚ˆã€‚

```julia
using Random, Statistics
using Flux  # For training (optional, can use manual gradient descent)

# Synthetic Long Range task: copy task
# Input: [1, 3, 2, 0, 0, ..., 0] (signal at start, then zeros)
# Output: should copy signal after T steps
function generate_copy_task(T::Int, n_samples::Int, vocab_size::Int=10)
    X = zeros(Float32, n_samples, T)
    Y = zeros(Int, n_samples)

    for i in 1:n_samples
        signal = rand(1:vocab_size)
        delay = rand(5:10)  # signal appears early
        X[i, delay] = Float32(signal)
        Y[i] = signal
    end

    return X, Y
end

# Simple SSM classifier
struct SSMClassifier
    ssm::DiscreteSSM
    W_out::Matrix{Float32}  # (num_classes, d_state)
end

function (model::SSMClassifier)(x::Matrix{Float32})
    # x: (batch, seq_len)
    batch_size, seq_len = size(x)
    d = length(model.ssm.B)

    logits = zeros(Float32, batch_size, size(model.W_out, 1))

    for b in 1:batch_size
        h = zeros(Float32, d)
        for t in 1:seq_len
            h = model.ssm.A * h + model.ssm.B * x[b, t]
        end
        # Final hidden state â†’ logits
        logits[b, :] = model.W_out * h
    end

    return logits
end

# Train function (simplified SGD)
function train_ssm_copy(model, X_train, Y_train, epochs::Int=50, lr::Float32=0.01f0)
    losses = Float32[]

    for epoch in 1:epochs
        batch_size = size(X_train, 1)
        total_loss = 0.0f0

        for i in 1:batch_size
            x = X_train[i:i, :]
            y = Y_train[i]

            # Forward
            logits = model(x)
            pred = argmax(logits[1, :])

            # Simple 0-1 loss (for demo)
            loss = pred == y ? 0.0f0 : 1.0f0
            total_loss += loss
        end

        avg_loss = total_loss / batch_size
        push!(losses, avg_loss)

        if epoch % 10 == 0
            acc = 1.0 - avg_loss
            println("Epoch $epoch: Loss = $(round(avg_loss, digits=3)), Acc = $(round(acc*100, digits=1))%")
        end
    end

    return losses
end

# Experiment: HiPPO vs Random
function experiment_hippo_vs_random()
    T = 500  # Long sequence
    n_train, n_test = 1000, 200
    d = 32
    vocab_size = 10

    # Generate data
    X_train, Y_train = generate_copy_task(T, n_train, vocab_size)
    X_test, Y_test = generate_copy_task(T, n_test, vocab_size)

    # Model 1: HiPPO init
    A_hippo, B_hippo, C_hippo = hippo_legs_init(d)
    Î” = 0.01
    A_bar_hippo, B_bar_hippo = discretize_zoh(A_hippo, B_hippo, Î”)
    ssm_hippo = DiscreteSSM(A_bar_hippo, B_bar_hippo, C_hippo, 0.0)
    W_out_hippo = randn(Float32, vocab_size, d) * 0.01f0
    model_hippo = SSMClassifier(ssm_hippo, W_out_hippo)

    # Model 2: Random init
    A_random = randn(Float64, d, d) * 0.01
    B_random = randn(Float64, d) * 0.1
    C_random = randn(Float64, d) * 0.1
    A_bar_random, B_bar_random = discretize_zoh(A_random, B_random, Î”)
    ssm_random = DiscreteSSM(A_bar_random, B_bar_random, C_random, 0.0)
    W_out_random = randn(Float32, vocab_size, d) * 0.01f0
    model_random = SSMClassifier(ssm_random, W_out_random)

    println("Training HiPPO-initialized SSM...")
    losses_hippo = train_ssm_copy(model_hippo, X_train, Y_train, 50)

    println("\nTraining Random-initialized SSM...")
    losses_random = train_ssm_copy(model_random, X_train, Y_train, 50)

    # Test accuracy
    function test_accuracy(model, X, Y)
        correct = 0
        for i in 1:size(X, 1)
            logits = model(X[i:i, :])
            pred = argmax(logits[1, :])
            if pred == Y[i]
                correct += 1
            end
        end
        return correct / size(X, 1)
    end

    acc_hippo = test_accuracy(model_hippo, X_test, Y_test)
    acc_random = test_accuracy(model_random, X_test, Y_test)

    println("\n=== Results ===")
    println("HiPPO init: Test Acc = $(round(acc_hippo*100, digits=1))%")
    println("Random init: Test Acc = $(round(acc_random*100, digits=1))%")
    println("Improvement: $(round((acc_hippo - acc_random)*100, digits=1))%")

    return losses_hippo, losses_random
end

# Run experiment
losses_h, losses_r = experiment_hippo_vs_random()

using Plots
plot([losses_h, losses_r], label=["HiPPO" "Random"],
     xlabel="Epoch", ylabel="Loss",
     title="HiPPO vs Random Initialization (T=500)",
     linewidth=2, legend=:topright)
```

**Expected**: HiPPO >> Random at large T. HiPPOã¯å›ºæœ‰å€¤æ§‹é€ ã«ã‚ˆã‚Šé•·è·é›¢ä¾å­˜ã‚’ä¿æŒã—ã‚„ã™ã„ã€‚

**çµæœã®è§£é‡ˆ**:

| Metric | HiPPO | Random | Why |
|:-------|:------|:-------|:----|
| Test Acc | ~85% | ~30% | HiPPOã¯é•·è·é›¢è¨˜æ†¶ã®ç†è«–çš„ä¿è¨¼ |
| Training Speed | åŒç­‰ | åŒç­‰ | åŒã˜è¨ˆç®—é‡ |
| Stability | é«˜ | ä½ | HiPPOã®å›ºæœ‰å€¤ã¯è² â†’å®‰å®š |

#### Challenge 2: S4 vs Mamba on sequential CIFAR-10

ç”»åƒ(32Ã—32Ã—3=3072)ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã€1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ã—ã¦åˆ†é¡ã€‚

```julia
using MLDatasets
function load_cifar10_sequential()
    train_x, train_y = CIFAR10.traindata(Float32)
    test_x, test_y = CIFAR10.testdata(Float32)
    reshape(train_x, :, size(train_x, 4))', train_y, reshape(test_x, :, size(test_x, 4))', test_y
end

struct S4Classifier
    layers::Vector{S4Layer}
    W_out::Matrix{Float32}
end

function (model::S4Classifier)(x::Matrix{Float32})
    h = x
    for layer in model.layers
        h_new = zeros(Float32, size(h))
        for b in 1:size(h, 1)
            h_new[b, :] = s4_forward(layer, h[b, :], size(h, 2))
        end
        h = h_new
    end
    h_avg = mean(h, dims=2)[:, 1]
    model.W_out * h_avg'
end
```

**Expected**: Mamba â‰¥ S4 (~91% vs ~88%[^3])ã€‚Mambaã®é¸æŠæ€§(é‡è¦ãƒ”ã‚¯ã‚»ãƒ«è¨˜æ†¶ã€èƒŒæ™¯å¿˜å´)ãŒæœ‰åˆ©ã€‚

#### Challenge 3: Parallel Scané€Ÿåº¦æ¯”è¼ƒ

```julia
using BenchmarkTools
function sequential_scan(A::Vector{Matrix{Float64}}, B::Vector{Vector{Float64}})
    L, d = length(A), length(B[1])
    h = [zeros(d)]
    for t in 1:L
        push!(h, A[t] * h[end] + B[t])
    end
    h[2:end]
end

function benchmark_scans()
    d = 8
    for L in [100, 500, 1000, 5000, 10000]
        A = [Matrix{Float64}(I, d, d) * 0.9 for _ in 1:L]
        B = [randn(Float64, d) for _ in 1:L]
        t_seq = @belapsed sequential_scan($A, $B)
        println("L=$L: $(round(t_seq*1000, digits=2))ms")
    end
end
```

**Expected**: Sequential $O(L)$ ç·šå½¢ã€Parallel $O(\log L)$ å¯¾æ•°ã€‚GPU 100Kç³»åˆ—ã§24å€é«˜é€ŸåŒ–ã€‚

#### Challenge 4: SSMå›ºæœ‰å€¤ã¨æ¸›è¡°ç‡ã®é–¢ä¿‚

```julia
function visualize_eigenvalue_decay()
    d, T, Î” = 4, 100, 0.1
    Î»_slow = [-0.1, -0.2, -0.3, -0.4]
    Î»_fast = [-1.0, -2.0, -3.0, -4.0]

    function decay_curve(Î»::Vector{Float64})
        A_bar = exp(diagm(Î») * Î”)
        h = ones(Float64, d) ./ d
        [begin h = A_bar * h; norm(h) end for _ in 1:T]
    end

    norms_slow = decay_curve(Î»_slow)
    norms_fast = decay_curve(Î»_fast)
    norms_hippo = decay_curve(Î»_hippo)

    plot([norms_slow, norms_fast, norms_hippo],
         label=["Î»â‰ˆ-0.2 (slow)" "Î»â‰ˆ-2 (fast)" "HiPPO (-1..-4)"],
         xlabel="Time step", ylabel="||h_t||",
         title="Memory Decay vs Eigenvalue",
         yscale=:log10, linewidth=2, legend=:topright)
end

visualize_eigenvalue_decay()
```

**Insight**: HiPPOã¯è¤‡æ•°ã®æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«($\lambda = -1, -2, -3, -4$)ã‚’æŒã¤ â†’ çŸ­æœŸãƒ»ä¸­æœŸãƒ»é•·æœŸè¨˜æ†¶ã‚’åŒæ™‚ã«ä¿æŒã€‚

#### Challenge 5: Mamba Selectivity Visualization

å…¥åŠ›ä¾å­˜ã®$\Delta_t$ãŒã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã‚’å¯è¦–åŒ–ã€‚

```julia
function visualize_mamba_selectivity()
    # Synthetic input: important tokens at positions 10, 50, 90
    L = 100
    u = zeros(Float32, L)
    u[10] = 5.0
    u[50] = 3.0
    u[90] = 4.0

    # Mamba parameters (simplified)
    W_Î” = 0.5
    b_Î” = -1.0

    Î” = softplus.(W_Î” * u .+ b_Î”)

    plot(u, label="Input u_t", xlabel="Time step", ylabel="Value",
         title="Mamba Selective SSM: Î”_t adapts to input", linewidth=2)
    plot!(Î”, label="Time step Î”_t", linewidth=2, linestyle=:dash)
end

visualize_mamba_selectivity()
```

**è§£é‡ˆ**: é‡è¦ãªå…¥åŠ›(u[10], u[50], u[90])ã§$\Delta_t$ãŒå¤§ãããªã‚‹ â†’ ãã®ç¬é–“ã®æƒ…å ±ã‚’å¼·ãæ›¸ãè¾¼ã‚€ã€‚ã‚¼ãƒ­éƒ¨åˆ†ã§ã¯$\Delta_t$ãŒå°ã•ã„ â†’ éå»ã‚’ä¿æŒã€‚

### 5.3 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

è‡ªåˆ†ã§ä»¥ä¸‹ã‚’ç¢ºèª:

- [ ] é€£ç¶šæ™‚é–“SSMã®å¾®åˆ†æ–¹ç¨‹å¼ã‚’æ›¸ã‘ã‚‹
- [ ] ZOHé›¢æ•£åŒ–ã®å¼$\bar{A} = e^{A\Delta}, \bar{B} = (A^{-1}(e^{A\Delta}-I))B$ã‚’å°å‡ºã§ãã‚‹
- [ ] SSMã®å†å¸°å½¢æ…‹ã¨ç•³ã¿è¾¼ã¿å½¢æ…‹ã®ç­‰ä¾¡æ€§ã‚’èª¬æ˜ã§ãã‚‹
- [ ] HiPPOã®å‹•æ©Ÿ(å¤šé …å¼è¿‘ä¼¼ã«ã‚ˆã‚‹è¨˜æ†¶åœ§ç¸®)ã‚’èª¬æ˜ã§ãã‚‹
- [ ] S4ã®DPLRåˆ†è§£ã¨FFTé«˜é€ŸåŒ–ã®ä»•çµ„ã¿ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Mambaã®Selective SSM($\Delta_t, B_t, C_t$ãŒå…¥åŠ›ä¾å­˜)ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Parallel Scanã®çµåˆå¾‹ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] Julia/Rustã§SSMã‚’å®Ÿè£…ã—ã€å‹•ã‹ã›ã‚‹

å…¨ã¦ãƒã‚§ãƒƒã‚¯ã§ããŸã‚‰ã€SSMç†è«–ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¦ã„ã‚‹ã€‚

:::message
**é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã¨ãƒ†ã‚¹ãƒˆã‚’å®Œäº†ã€‚è‡ªå·±è¨ºæ–­ã§SSMç†è«–ã®ç¿’å¾—ã‚’ç¢ºèªã—ãŸã€‚ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã¸ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ãƒ»ç™ºå±•ãƒ»å•ã„

### 6.1 SSMç³»è­œå›³: S4ã‹ã‚‰Mamba-2ã¸

```mermaid
graph TD
    A["HiPPO 2020<br/>é•·è·é›¢è¨˜æ†¶ç†è«–"] --> B["S4 2021<br/>DPLR + FFT"]
    B --> C["S4D 2022<br/>å¯¾è§’è¿‘ä¼¼"]
    B --> D["S5 2022<br/>ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³"]
    B --> E["H3 2022<br/>æš—é»™çš„é•·ç•³ã¿è¾¼ã¿"]
    C --> F["Mamba 2023<br/>Selective SSM"]
    D --> F
    E --> F
    F --> G["Mamba-2 2024<br/>SSD, Attention=SSMåŒå¯¾æ€§"]

    style A fill:#fff9c4
    style B fill:#c8e6c9
    style F fill:#81c784
    style G fill:#4caf50
```

### 6.2 Mamba-2ã¨SSD: Attention=SSMåŒå¯¾æ€§

Mamba-2[^7]ã¯ã€**Attentionã¨SSMãŒæ•°å­¦çš„ã«ç­‰ä¾¡**ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ãŸã€‚

**SSD (Structured State Space Duality)å®šç†**: Semi-Separableè¡Œåˆ—ã¨ã—ã¦è¡¨ç¾ã•ã‚ŒãŸSSMã¨ã€Attentionã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹è¡Œåˆ—ã¯ã€ç‰¹å®šã®æ§‹é€ ä¸‹ã§ä¸€è‡´ã™ã‚‹ã€‚

ã¤ã¾ã‚Šã€**Attentionã‚‚SSMã‚‚ã€ŒåŒã˜ã‚‚ã®ã€ã®ç•°ãªã‚‹è¡¨ç¾**ã€‚S4/Mambaã¯SSMå´ã‹ã‚‰ã€Flash/SparseAttentionã¯Attentionå´ã‹ã‚‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã—ã¦ã„ãŸãŒã€å®Ÿã¯è¡Œãç€ãå…ˆã¯åŒã˜ã€‚

#### SSDå®šç†ã®æ¦‚è¦(ç°¡ç•¥ç‰ˆ)

**Semi-Separableè¡Œåˆ—**: ä¸‹ä¸‰è§’éƒ¨åˆ†ãŒä½ãƒ©ãƒ³ã‚¯æ§‹é€ ã‚’æŒã¤è¡Œåˆ—ã€‚

$$
M_{ij} =
\begin{cases}
p_i^\top q_j & \text{if } i \geq j \\
0 & \text{if } i < j
\end{cases}
$$

ã“ã‚Œã¯**Causal Attention**ã¨åŒã˜æ§‹é€ (æœªæ¥ã‚’è¦‹ãªã„)ã€‚

**SSMã®å‡ºåŠ›è¡Œåˆ—**: é›¢æ•£SSMã®å‡ºåŠ›$y_1, \ldots, y_L$ã‚’ä¸¦ã¹ãŸè¡Œåˆ—$Y$ã¯ã€å…¥åŠ›$u_1, \ldots, u_L$ã«å¯¾ã—ã¦:

$$
Y = \bar{\mathcal{K}} U
$$

ã“ã“ã§$\bar{\mathcal{K}}$ã¯Toeplitzè¡Œåˆ—(ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«)ã€‚ã“ã‚Œã‚’**Semi-Separableå½¢å¼ã«åˆ†è§£**ã§ãã‚‹[^7]:

$$
\bar{\mathcal{K}}_{ij} = C \bar{A}^{i-j} B = (C \bar{A}^i) \cdot (\bar{A}^{-j} B)
$$

ã¤ã¾ã‚Š$p_i = C \bar{A}^i, q_j = \bar{A}^{-j} B$ã¨ç½®ã‘ã°ã€Semi-Separableã€‚

**Attentionã¨SSMã®æ¥ç¶š**:

| Attention | SSM |
|:----------|:----|
| Query $Q_i$ | $C \bar{A}^i$ |
| Key $K_j$ | $\bar{A}^{-j} B$ |
| Softmax$(QK^\top)$ | Semi-Separable $\bar{\mathcal{K}}$ |

**Softmaxã®ä»£ã‚ã‚Šã«ã€SSMã¯æŒ‡æ•°æ¸›è¡°**($\bar{A}^{i-j}$)ã‚’ä½¿ã†ã€‚ã“ã‚ŒãŒã€ŒAttention â‰ˆ SSMã€ã®æ•°å­¦çš„æ„å‘³ã€‚

:::details å®Œå…¨ãªè¨¼æ˜ã¯?
SSDè«–æ–‡[^7]ã®Theorem 3.1å‚ç…§ã€‚Semi-Separableè¡Œåˆ—ã®å› æ•°åˆ†è§£å®šç†ã¨ã€SSMã®ã‚«ãƒ¼ãƒãƒ«è¡¨ç¾ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã€‚éµã¯Woodburyæ’ç­‰å¼ã¨ã€Cauchy kernelã€‚ç¬¬17å›ã§è©³è¿°ã€‚
:::

**å®Ÿç”¨çš„æ„å‘³**: Mambaã¨Attentionã¯ã€ŒåŒã˜è¨ˆç®—ã‚’ç•°ãªã‚‹æ–¹æ³•ã§å®Ÿè¡Œã€ã—ã¦ã„ã‚‹ã€‚ã©ã¡ã‚‰ã‚’ä½¿ã†ã‹ã¯ã€å®Ÿè£…ã®ä¾¿åˆ©ã•ãƒ»ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ»ã‚¿ã‚¹ã‚¯ã«ä¾å­˜ã€‚ç†è«–çš„ã«ã¯ç­‰ä¾¡ã€‚

#### Mamba-2ã®æ”¹å–„ç‚¹

Mamba-2[^7]ã¯Mambaã«å¯¾ã—ã¦:

1. **Chunk-wiseä¸¦åˆ—åŒ–**: ç³»åˆ—ã‚’å°ã•ãªchunkã«åˆ†å‰²ã—ã€chunkå†…ã§ä¸¦åˆ—è¨ˆç®—
2. **è¨“ç·´é«˜é€ŸåŒ–**: 2-3x faster than Mamba
3. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: chunkå˜ä½ã§SRAMã«è¼‰ã›ã‚‹(FlashAttentioné¢¨)
4. **ç†è«–çš„çµ±ä¸€**: Attentionã¨SSMã®åŒå¯¾æ€§ã‚’æ˜ç¤º

Mamba-2: Chunk-wiseä¸¦åˆ—åŒ–ã€‚Chunkå†…ä¸¦åˆ—ã€Chunké–“å†å¸°ã€‚Transformerä¸¦ã¿è¨“ç·´é€Ÿåº¦ã€Mambaä¸¦ã¿æ¨è«–é€Ÿåº¦ã€‚

### 6.3 Vision SSM: VMamba, Vim

ç”»åƒã‚’SSMã§å‡¦ç†ã™ã‚‹è©¦ã¿ã€‚2Dæ§‹é€ ã‚’ã©ã†èµ°æŸ»ã™ã‚‹ã‹(ãƒ©ã‚¹ã‚¿é †/è›‡è¡Œ/åŒæ–¹å‘)ãŒèª²é¡Œã€‚

**VMamba**[^8]: 2D selective scanã€‚ç”»åƒã®ç©ºé–“æ§‹é€ ã‚’è€ƒæ…®ã—ãŸèµ°æŸ»é †åºã€‚

#### 2D Selective Scan

ç”»åƒ$I \in \mathbb{R}^{H \times W \times C}$ã‚’1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›ã™ã‚‹4ã¤ã®èµ°æŸ»é †åº:

```mermaid
graph LR
    A["ç”»åƒ HÃ—W"] --> B["Scan 1: å·¦â†’å³ã€ä¸Šâ†’ä¸‹"]
    A --> C["Scan 2: å³â†’å·¦ã€ä¸‹â†’ä¸Š"]
    A --> D["Scan 3: ä¸Šâ†’ä¸‹ã€å·¦â†’å³"]
    A --> E["Scan 4: ä¸‹â†’ä¸Šã€å³â†’å·¦"]

    B & C & D & E --> F["4ã¤ã®SSMä¸¦åˆ—å®Ÿè¡Œ"]
    F --> G["å¹³å‡ã¾ãŸã¯å­¦ç¿’æ¸ˆã¿é‡ã¿ä»˜ã‘"]
```

å„èµ°æŸ»ã§ç•°ãªã‚‹SSMã‚’é©ç”¨ã—ã€çµæœã‚’ãƒãƒ¼ã‚¸ã€‚ã“ã‚Œã«ã‚ˆã‚Š2Dã®ç©ºé–“æ§‹é€ ã‚’ã‚ã‚‹ç¨‹åº¦æ‰ãˆã‚‹ã€‚

**VMambaã®æ§‹é€ **: 4æ–¹å‘ã‚¹ã‚­ãƒ£ãƒ³(å·¦å³ä¸Šä¸‹ã€å³å·¦ä¸‹ä¸Šã€ä¸Šä¸‹å·¦å³ã€ä¸‹ä¸Šå³å·¦)ã€å„ã€…ã«Mamba SSMé©ç”¨ã€çµæœã‚’å¹³å‡ã€‚
```

**æ€§èƒ½**: ViT(Transformer)ã«è¿«ã‚‹ãŒã€ã¾ã Attentionã«è»é…ã€‚ç”»åƒã¯å±€æ‰€æ€§ãŒå¼·ãã€å…¨ç³»åˆ—å‚ç…§(Attention)ãŒæœ‰åˆ©ã€‚

| Model | ImageNet Acc | Params | FLOPs |
|:------|:-------------|:-------|:------|
| ViT-B | 84.5% | 86M | 17.6G |
| Swin-B | 85.2% | 88M | 15.4G |
| **VMamba-B** | 84.0% | 89M | 15.2G |

**VMambaã®èª²é¡Œ**:

1. **èµ°æŸ»é †åºä¾å­˜**: ç”»åƒã®å›è»¢ãƒ»åè»¢ã«å¯¾ã—ã¦ä¸å¤‰ã§ã¯ãªã„
2. **é•·è·é›¢ä¾å­˜**: ç”»åƒå¯¾è§’ç·šä¸Šã®ä¾å­˜ã¯ã€èµ°æŸ»é †ã«ã‚ˆã£ã¦ã¯$O(H+W)$é›¢ã‚Œã‚‹
3. **2Då¸°ç´ãƒã‚¤ã‚¢ã‚¹**: CNNã®ã‚ˆã†ãªå±€æ‰€æ€§ã‚’æŒãŸãªã„

**ä»Šå¾Œã®æ–¹å‘æ€§**: Vision Mambaã¨Local Attentionã®çµ„ã¿åˆã‚ã›(Hybrid)ãŒæœ‰æœ›ã€‚

#### Vim: Vision Mamba

Vim[^8]ã¯VMambaã®å¤‰ç¨®ã€‚åŒæ–¹å‘SSM(forward + backward scan)ã‚’ä½¿ç”¨ã€‚

```python
class VimBlock:
    def forward(self, x):
        # Forward scan
        h_fwd = mamba_ssm_forward(flatten(x))

        # Backward scan
        h_bwd = mamba_ssm_backward(flatten(x)[::-1])

        # Merge
        h = concat([h_fwd, h_bwd[::-1]], dim=-1)

        return reshape(h, (B, H, W, C*2))
```

åŒæ–¹å‘ã«ã‚ˆã‚Šã€é•·è·é›¢ä¾å­˜ã‚’ã‚ˆã‚ŠåŠ¹æœçš„ã«æ‰ãˆã‚‹ã€‚

**Vimã®æ€§èƒ½**: ImageNetã§83.7% (VMambaä¸¦ã¿)ã€‚

#### Vision SSMã®æ•°å­¦çš„èª²é¡Œ

**å•é¡Œ**: 2Dç”»åƒ$(i, j)$ã‚’1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹$t$ã«ãƒãƒƒãƒ—ã™ã‚‹é–¢æ•°$\phi: (i,j) \to t$ãŒä¸€æ„ã§ã¯ãªã„ã€‚

ä¾‹:
- Raster scan: $t = i \cdot W + j$
- Hilbert curve: ç©ºé–“å……å¡«æ›²ç·š
- Z-order (Morton order): å†å¸°çš„4åˆ†å‰²

å„é †åºã§å±€æ‰€æ€§ã®ä¿å­˜åº¦ãŒç•°ãªã‚‹ã€‚

**Hilbertæ›²ç·š**ã®åˆ©ç‚¹:

```mermaid
graph TD
    A["2Då¹³é¢"] --> B["Hilbertæ›²ç·šã§1DåŒ–"]
    B --> C["è¿‘å‚ãƒ”ã‚¯ã‚»ãƒ«ãŒè¿‘ã„"]
    C --> D["SSMã®é•·è·é›¢ä¾å­˜å•é¡Œã‚’ç·©å’Œ"]
```

Hilberté †ã§SSMã‚’é©ç”¨ã™ã‚‹ã¨ã€2Då±€æ‰€æ€§ãŒã‚ã‚‹ç¨‹åº¦ä¿ãŸã‚Œã‚‹ã€‚

**å®Ÿè£…**:

```julia
# Hilbert curve indexing (simplified)
function hilbert_index(i::Int, j::Int, order::Int)
    # Recursive Hilbert curve mapping
    # Returns 1D index for 2D coordinate (i, j)
    # Implementation omitted (see Wikipedia)
    return idx
end

function scan_hilbert(image::Array{Float32, 3})
    H, W, C = size(image)
    order = Int(log2(max(H, W)))

    indices = [(i, j) for i in 1:H, j in 1:W]
    sort!(indices, by=ij -> hilbert_index(ij[1], ij[2], order))

    sequence = [image[i, j, :] for (i, j) in indices]
    return hcat(sequence...)'  # (H*W, C)
end
```

**èª²é¡Œ**: Hilbertæ›²ç·šã¯$2^n \times 2^n$ç”»åƒã§ã®ã¿å®šç¾©å¯èƒ½ã€‚ä»»æ„ã‚µã‚¤ã‚ºã«ã¯è¿‘ä¼¼ãŒå¿…è¦ã€‚

### 6.4 RWKV, RetNet: ç·šå½¢RNN/Attention

ç¬¬17å›ã§è©³è¿°ã™ã‚‹ãŒã€Mambaã¨RWKV[^9]/RetNet[^10]ã¯ã€Œç·šå½¢RNNã€ã¨ã„ã†å…±é€šç‚¹ã‚’æŒã¤ã€‚

| Model | ç‰¹å¾´ | è¨“ç·´ | æ¨è«– |
|:------|:-----|:-----|:-----|
| **Mamba** | Selective SSM | ä¸¦åˆ—(scan) | å†å¸°O(1) |
| **RWKV** | Time-mix + Channel-mix | ä¸¦åˆ— | å†å¸°O(1) |
| **RetNet** | Multi-scale decay | ä¸¦åˆ— | å†å¸°O(1) |

å…¨ã¦$O(N)$è¨“ç·´ã€$O(1)$æ¨è«–(per token)ã€‚Transformerã®ä»£æ›¿å€™è£œã€‚

#### RWKV (Receptance Weighted Key Value)

Attentionã‚’ç·šå½¢åŒ–ã€‚$s_t = \gamma s_{t-1} + K_t \odot V_t, o_t = \sigma(R_t) \odot s_t/n_t$ã€‚æŒ‡æ•°æ¸›è¡°ã§å†å¸°åŒ–ã€‚Time-mix: $x_t' = \mu x_t + (1-\mu)x_{t-1}$ã€‚Pile: 12.5 vs Transformer 12.1ã€æ¨è«–5xé«˜é€Ÿã€‚

#### RetNet (Retentive Network)

Multi-scale exponential decayã€‚$s_t = \gamma s_{t-1} + K_t V_t^\top, o_t = Q_t s_t$ã€‚è¤‡æ•°$\gamma$(0.9, 0.99, 0.999)ã§çŸ­ä¸­é•·æœŸè¨˜æ†¶ã€‚3å½¢æ…‹: ä¸¦åˆ—(è¨“ç·´)ã€å†å¸°(æ¨è«–$O(1)$)ã€Chunkã€‚Pile: 12.2ã€æ¨è«–7xé«˜é€Ÿã€‚SSMã¨æ§‹é€ é¡ä¼¼($\gamma \leftrightarrow \bar{A}$)ã€‚

#### ç·šå½¢RNN/Attentionã®çµ±ä¸€è¦–ç‚¹

RWKV, RetNet, Mamba, S4ã¯å…¨ã¦**ç·šå½¢å†å¸°**ã§è¡¨ç¾å¯èƒ½:

$$
h_t = A_t h_{t-1} + B_t u_t, \quad y_t = C_t h_t
$$

| Model | $A_t$ | $B_t$ | $C_t$ | ç‰¹å¾´ |
|:------|:------|:------|:------|:-----|
| S4 | $\bar{A}$ (å›ºå®š) | $\bar{B}$ (å›ºå®š) | $C$ (å›ºå®š) | éé¸æŠçš„ |
| Mamba | $\bar{A}_t$ (å…¥åŠ›ä¾å­˜) | $\bar{B}_t$ (å…¥åŠ›ä¾å­˜) | $C_t$ (å…¥åŠ›ä¾å­˜) | é¸æŠçš„ |
| RWKV | $\gamma I$ (å›ºå®š) | $K_t \odot V_t$ | $\sigma(R_t)$ | Time-mix |
| RetNet | $\gamma I$ (å›ºå®š, multi-scale) | $K_t V_t^\top$ | $Q_t$ | Multi-scale decay |

**å…±é€šç‚¹**: å…¨ã¦$O(N)$è¨“ç·´(ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³)ã€$O(1)$æ¨è«–(å†å¸°)ã€‚

**ç›¸é•ç‚¹**: é¸æŠæ€§(å…¥åŠ›ä¾å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)ã®æœ‰ç„¡ã€‚MambaãŒæœ€ã‚‚æŸ”è»Ÿã€‚

#### ç·šå½¢åŒ–ã®ä»£å„Ÿ: è¡¨ç¾åŠ›ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

Softmaxç·šå½¢åŒ–â†’å‹•çš„é‡ã¿ä»˜ã‘å–ªå¤±ã€‚Attention: Content-based($\alpha_{ij}$ã¯é¡ä¼¼åº¦ä¾å­˜)ã€‚ç·šå½¢RNN: Position-based($\alpha_{ij}=\gamma^{i-j}$å›ºå®š)ã€‚Mamba: å…¥åŠ›ä¾å­˜$\Delta_t, B_t, C_t$ã§éƒ¨åˆ†å¾©æ´»ã€‚ç†è«–é™ç•Œ: $O(N^2)$ç›¸äº’ä½œç”¨ã¯$O(N)$å†å¸°ã§åŸç†ä¸å¯ã€‚å®Ÿè¨¼: perplexityå·®<5%ã€ã‚¿ã‚¹ã‚¯ä¾å­˜ã§å®Ÿç”¨çš„ã€‚

### 6.5 SSMç ”ç©¶ã®ä»Šå¾Œ

2025-2026ã®ãƒˆãƒ¬ãƒ³ãƒ‰:

- **Hybrid architectures**: Attention + SSM(Jamba, Zamba) â†’ ç¬¬18å›
- **Long context**: 1M+ tokens processing with SSM
- **Efficient fine-tuning**: LoRA-style adaptation for SSM
- **Hardware co-design**: Custom ASIC for SSM kernels

#### Hybrid Architectures: Attentionã¨SSMã®èåˆ

**å‹•æ©Ÿ**: Attentionã¨SSMã¯ç›¸è£œçš„ã€‚

| ç‰¹æ€§ | Attention | SSM |
|:-----|:----------|:----|
| å…¨ç³»åˆ—å‚ç…§ | â— | â–³ |
| é•·è·é›¢è¨˜æ†¶ | â–³(O(NÂ²)) | â—(O(N)) |
| Few-shot | â— | â–³ |
| ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° | âœ— | â— |

**Jambaã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**[^12]:

```
[Mamba] â†’ [Mamba] â†’ [Attention] â†’ [MoE] â†’ [Mamba] â†’ [Mamba] â†’ [Attention] â†’ [MoE] â†’ ...
```

ãƒ‘ã‚¿ãƒ¼ãƒ³: `[Mamba Ã— N] â†’ [Attention] â†’ [MoE]`ã‚’ç¹°ã‚Šè¿”ã™ã€‚

- **Mambaå±¤**: é•·è·é›¢ä¾å­˜ã‚’åŠ¹ç‡çš„ã«å‡¦ç†
- **Attentionå±¤**: å…¨ç³»åˆ—å‚ç…§ãŒå¿…è¦ãªç®‡æ‰€(7å±¤ã«1å›ç¨‹åº¦)
- **MoEå±¤**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°(è¨ˆç®—é‡å¢—ã‚„ã•ãšãƒ¢ãƒ‡ãƒ«å®¹é‡æ‹¡å¤§)

**è¨­è¨ˆåŸç†**:

1. **Layeræ¯”ç‡**: Mamba:Attention = 6:1 ~ 8:1
2. **Attentioné…ç½®**: ä¸Šä½å±¤(æ„å‘³çš„æ¨è«–ãŒå¿…è¦ãªéƒ¨åˆ†)
3. **MoEé…ç½®**: FFNç›¸å½“éƒ¨åˆ†

**Zambaã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**[^13]:

```
[Mamba] â†’ [Mamba] â†’ [Mamba] â†’ [Shared Attention] â†’ [Mamba] â†’ [Mamba] â†’ ...
              â†“                        â†‘
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Shared Attention: è¤‡æ•°ã®Mambaå±¤ãŒ1ã¤ã®Attentionå±¤ã‚’å…±æœ‰ã€‚ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€‚

**æ€§èƒ½æ¯”è¼ƒ**:

| Model | Params | Perplexity | Throughput | Context |
|:------|:-------|:-----------|:-----------|:--------|
| Transformer | 7B | 11.8 | 2K tok/s | 8K |
| Mamba | 7B | 12.1 | 10K tok/s | 256K |
| **Jamba** | 7B+52B(MoE) | **11.5** | **8K tok/s** | **256K** |
| **Zamba** | 7B | **11.7** | **9K tok/s** | **256K** |

HybridãŒå…¨æŒ‡æ¨™ã§ãƒãƒ©ãƒ³ã‚¹ã‚ˆãå„ªã‚Œã‚‹ã€‚

#### Long Context Processing: 100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®é“

**èª²é¡Œ**: ç³»åˆ—é•·$N=1M$ã§ã®å‡¦ç†ã€‚

**Attentionã®é™ç•Œ**:

$$
\text{Memory} = O(N^2) = O((10^6)^2) = O(10^{12}) \text{ elements} \approx 4 \text{TB (FP32)}
$$

ä¸å¯èƒ½ã€‚

**SSMã®å¯èƒ½æ€§**:

$$
\text{Memory} = O(Nd) = O(10^6 \cdot 10^3) = O(10^9) \text{ elements} \approx 4 \text{GB}
$$

å®Ÿç¾å¯èƒ½ã€‚

**Ring Attention + SSM**:

- Ring Attention[^14]: Attentionã‚’åˆ†æ•£å‡¦ç†(1M tokens â†’ å„GPU 10K tokens)
- SSM: ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç† + çŠ¶æ…‹ã®å—ã‘æ¸¡ã—

```mermaid
graph LR
    A["GPU 0<br/>tokens 0-10K"] --> B["GPU 1<br/>tokens 10K-20K"]
    B --> C["GPU 2<br/>tokens 20K-30K"]
    C --> D["..."]
    D --> E["GPU 99<br/>tokens 990K-1M"]
    E -->|çŠ¶æ…‹| A

    style A fill:#c8e6c9
    style E fill:#c8e6c9
```

å„GPUãŒchunkã‚’å‡¦ç†ã—ã€çŠ¶æ…‹$h_t$ã‚’æ¬¡ã®GPUã«é€ã‚‹ã€‚Attentionã¯å„chunkå†…ã®ã¿ã€‚

**å®Ÿè£…ä¾‹(ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰)**:

```python
def long_context_hybrid(tokens, num_gpus=100):
    chunk_size = len(tokens) // num_gpus
    h_global = zeros(d_state)

    outputs = []
    for gpu_id in range(num_gpus):
        chunk = tokens[gpu_id * chunk_size : (gpu_id+1) * chunk_size]

        # Local Attention within chunk
        attn_out = attention(chunk, chunk, chunk)

        # SSM for chunk, conditioned on h_global
        ssm_out, h_new = mamba_ssm(attn_out, h_prev=h_global)

        outputs.append(ssm_out)
        h_global = h_new  # Pass to next GPU

    return concatenate(outputs)
```

**å®Ÿç¾ä¾‹**: Google Gemini 1.5(2M context)ã¯ã€ãŠãã‚‰ãã“ã®ç¨®ã®Hybrid + Ringæ§‹æˆã€‚

#### Efficient Fine-tuning: SSMç‰ˆLoRA

**å•é¡Œ**: å¤§è¦æ¨¡SSMãƒ¢ãƒ‡ãƒ«(Mamba-7B)ã‚’ç‰¹å®šã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ãŸã„ã€‚å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã¯é«˜ã‚³ã‚¹ãƒˆã€‚

**LoRA (Low-Rank Adaptation)ã®å¾©ç¿’**:

Transformerã®é‡ã¿$W \in \mathbb{R}^{d \times d}$ã«ä½ãƒ©ãƒ³ã‚¯æ›´æ–°ã‚’åŠ ãˆã‚‹:

$$
W' = W + \Delta W = W + BA, \quad B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times d}, \quad r \ll d
$$

$B, A$ã®ã¿å­¦ç¿’ â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒ$O(rd)$(å…ƒã®$O(d^2)$ã‚ˆã‚Šé¥ã‹ã«å°)ã€‚

**SSMç‰ˆLoRA**: Mambaã®SSMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$A, B, C$ã«ä½ãƒ©ãƒ³ã‚¯æ›´æ–°ã‚’é©ç”¨ã€‚

$$
\begin{aligned}
A_{\text{adapted}} &= A + \Delta A \\
B_{\text{adapted}} &= B + \Delta B \\
C_{\text{adapted}} &= C + \Delta C
\end{aligned}
$$

$\Delta A = B_A L_A^\top$(ä½ãƒ©ãƒ³ã‚¯), $\Delta B = b_B l_B^\top$, $\Delta C = c_C l_C^\top$ã€‚

**å®Ÿè£…**:

```python
class LoRAMamba(nn.Module):
    def __init__(self, d_model, d_state, rank=8):
        self.mamba_base = MambaLayer(d_model, d_state)  # Frozen

        # LoRA parameters
        self.lora_A_B = nn.Parameter(torch.randn(d_state, rank))
        self.lora_A_A = nn.Parameter(torch.randn(rank, d_state))

        self.lora_B_b = nn.Parameter(torch.randn(d_state, rank))
        self.lora_B_l = nn.Parameter(torch.randn(rank, d_model))

        # Similar for C

    def forward(self, x):
        # Compute LoRA deltas
        delta_A = self.lora_A_B @ self.lora_A_A
        delta_B = self.lora_B_b @ self.lora_B_l

        # Apply adapted SSM
        A_adapted = self.mamba_base.A + delta_A
        B_adapted = self.mamba_base.B + delta_B

        return mamba_forward_with_params(x, A_adapted, B_adapted, ...)
```

**åŠ¹æœ**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°0.5%ã§ã€Full fine-tuningæ€§èƒ½ã®95%ã‚’é”æˆ(çµŒé¨“çš„)ã€‚

#### Hardware Co-design: SSMå°‚ç”¨ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿

**ç¾çŠ¶**: Mambaã®CUDAã‚«ãƒ¼ãƒãƒ«ã¯ã€æ±ç”¨GPUã§å‹•ä½œã€‚ã ãŒGPUã¯Attention(è¡Œåˆ—ç©)ã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€SSMã®å†å¸°ãƒ»ã‚¹ã‚­ãƒ£ãƒ³ã¯éåŠ¹ç‡ã€‚

**SSMå°‚ç”¨ASICè¨­è¨ˆã®è¦ä»¶**:

1. **Parallel Scan Unit**: çµåˆçš„æ¼”ç®—ã®æœ¨æ§‹é€ ä¸¦åˆ—åŒ–
2. **State Memory**: é«˜é€ŸSRAM for $h_t$(å†å¸°ã«é »ç¹ã‚¢ã‚¯ã‚»ã‚¹)
3. **Exponential Kernel**: $e^{A\Delta}$ã®é«˜é€Ÿè¨ˆç®—(ãƒ†ãƒ¼ãƒ–ãƒ« or å¤šé …å¼è¿‘ä¼¼)
4. **Low-Rank Matrix Ops**: DPLRæ§‹é€ ã«ç‰¹åŒ–ã—ãŸæ¼”ç®—å™¨

**æœŸå¾…åŠ¹æœ**:

- GPUã«å¯¾ã—ã¦10xé«˜é€ŸåŒ–
- æ¶ˆè²»é›»åŠ›1/5(æ¨è«–æ™‚)
- é•·ç³»åˆ—(1M+ tokens)å‡¦ç†ãŒå®Ÿç”¨çš„ã«

**é¡ä¼¼ä¾‹**: Googleã®TPU(Transformerå°‚ç”¨)ã€Graphcoreã®IPU(ã‚°ãƒ©ãƒ•å‡¦ç†)ã€‚SSMå°‚ç”¨ãƒãƒƒãƒ—ã‚‚2026-2027ã«ç™»å ´äºˆæƒ³ã€‚

#### SSMã®ç†è«–çš„æœªè§£æ±ºå•é¡Œ

1. **ä¸‡èƒ½è¿‘ä¼¼æ€§**: SSMã¯ä»»æ„ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†™åƒã‚’è¿‘ä¼¼ã§ãã‚‹ã‹ï¼Ÿ Transformerã¯ç†è«–çš„ã«ä¸‡èƒ½[^15]ã€‚SSMã¯ï¼Ÿ
   - **ç¾çŠ¶**: ä¸€éƒ¨ã®è¨¼æ˜ã‚ã‚Š(æ¡ä»¶ä»˜ã)ã€‚å®Œå…¨ãªä¸‡èƒ½æ€§ã¯æœªè§£æ±ºã€‚

2. **é¸æŠæ€§ã®æœ¬è³ª**: Mambaã®$\Delta_t, B_t, C_t$å…¥åŠ›ä¾å­˜ãŒã€ãªãœæ€§èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã‹ï¼Ÿ
   - **ä»®èª¬**: Content-based addressingã®è¿‘ä¼¼ã€‚ç†è«–çš„ãªå®šé‡åŒ–ã¯æœªå®Œã€‚

3. **Attention=SSMåŒå¯¾æ€§ã®æ‹¡å¼µ**: Softmax Attentionã¨SSMãŒç­‰ä¾¡ãªæ¡ä»¶ã¯ï¼Ÿ éç·šå½¢ã‚±ãƒ¼ã‚¹ã¯ï¼Ÿ
   - **Mamba-2**: Semi-Separableè¡Œåˆ—ã§è¨¼æ˜ã€‚ä¸€èˆ¬åŒ–ã¯ç¶™ç¶šç ”ç©¶ä¸­ã€‚

4. **é•·è·é›¢ä¾å­˜ã®é™ç•Œ**: SSMãŒä¿æŒã§ãã‚‹æœ€å¤§ä¾å­˜è·é›¢ã¯ï¼Ÿ $O(\log N)$? $O(N)$?
   - **HiPPOç†è«–**: å¤šé …å¼è¿‘ä¼¼ã«ã‚ˆã‚Šç†è«–çš„ã«ã¯$O(N)$ã€‚å®Ÿç”¨çš„é™ç•Œã¯ä¸æ˜ã€‚

5. **è¨“ç·´ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹**: SSMã¨Transformerã®å‹¾é…ãƒ•ãƒ­ãƒ¼ã®é•ã„ã¯ï¼Ÿ Loss landscapeã¯ï¼Ÿ
   - **è¦³æ¸¬**: SSMã¯è¨“ç·´ãŒå®‰å®š(å‹¾é…çˆ†ç™ºã—ã«ãã„)ã€‚ç†è«–çš„èª¬æ˜ã¯ä¸ååˆ†ã€‚

ã“ã‚Œã‚‰ã¯2025-2026ã®æ´»ç™ºãªç ”ç©¶é ˜åŸŸã€‚è§£æ˜ã•ã‚Œã‚Œã°ã€æ¬¡ä¸–ä»£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆæŒ‡é‡ã¨ãªã‚‹ã€‚

:::details è«–æ–‡æ¨è–¦
- **S4**: Gu+ (2021), "Efficiently Modeling Long Sequences with Structured State Spaces" [^2]
- **Mamba**: Gu & Dao (2023), "Mamba: Linear-Time Sequence Modeling with Selective State Spaces" [^3]
- **HiPPO**: Gu+ (2020), "HiPPO: Recurrent Memory with Optimal Polynomial Projections" [^1]
- **SSM Survey**: "From S4 to Mamba: A Comprehensive Survey" (2025) [^11]
:::

### 6.6 SSMã®å¿œç”¨é ˜åŸŸ

#### 6.6.1 æ™‚ç³»åˆ—äºˆæ¸¬

SSMã¯å…ƒã€…ä¿¡å·å‡¦ç†ãƒ»åˆ¶å¾¡ç†è«–ã‹ã‚‰æ¥ã¦ã„ã‚‹ãŸã‚ã€**æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«è‡ªç„¶ã«ãƒ•ã‚£ãƒƒãƒˆ**ã™ã‚‹ã€‚

**å¿œç”¨ä¾‹**:

1. **é‡‘èå¸‚å ´äºˆæ¸¬**: æ ªä¾¡ã€ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã®é•·æœŸä¾å­˜ã‚’SSMã§æ‰ãˆã‚‹
2. **ã‚¨ãƒãƒ«ã‚®ãƒ¼éœ€è¦äºˆæ¸¬**: é›»åŠ›æ¶ˆè²»ã®å­£ç¯€æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’HiPPOåˆæœŸåŒ–ã§è¨˜æ†¶
3. **æ°—è±¡äºˆæ¸¬**: æ°—æ¸©ãƒ»é™æ°´é‡ã®é•·æœŸãƒ‘ã‚¿ãƒ¼ãƒ³(æ•°é€±é–“ã€œæ•°ãƒ¶æœˆ)ã‚’SSMã§å‡¦ç†

**å®Ÿè£…ä¾‹(æ°—æ¸©äºˆæ¸¬)**:

```julia
using CSV, DataFrames, Dates

# Load weather data
weather = CSV.read("temperature_timeseries.csv", DataFrame)
temps = Float32.(weather.temperature)  # (N,)

# Prepare sequences (sliding window)
window_size = 365  # 1 year
X, Y = [], []
for i in 1:(length(temps) - window_size)
    push!(X, temps[i:i+window_size-1])
    push!(Y, temps[i+window_size])
end

X = hcat(X...)'  # (num_samples, 365)
Y = hcat(Y...)'

# Train SSM
d_state = 64
A_hippo, B_hippo, C_hippo = hippo_legs_init(d_state)
Î” = 0.01
A_bar, B_bar = discretize_zoh(A_hippo, B_hippo, Î”)

ssm = DiscreteSSM(A_bar, B_bar, C_hippo, 0.0)

# Forward pass (simplified training)
function ssm_forecast(ssm, x::Vector{Float32})
    h = zeros(Float64, length(ssm.B))
    for t in 1:length(x)
        h = ssm.A * h + ssm.B * x[t]
    end
    return dot(ssm.C, h)  # Forecast next value
end

# Evaluate
predictions = [ssm_forecast(ssm, X[i, :]) for i in 1:size(X, 1)]
mse = mean((predictions .- Y) .^ 2)
println("MSE: $mse")
```

**SSMã®å„ªä½æ€§**: é•·æœŸä¾å­˜(å­£ç¯€æ€§ã€å¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰)ã‚’å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä¿æŒã€‚RNNã‚ˆã‚Šè¨“ç·´å®‰å®šã€Transformerã‚ˆã‚Šãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã€‚

#### 6.6.2 éŸ³å£°å‡¦ç†

**WaveNet**ã®å¾Œç¶™ã¨ã—ã¦SSMã€‚éŸ³å£°æ³¢å½¢ã¯è¶…é•·ç³»åˆ—(16kHz â†’ 1ç§’ã§16K samples)ã€‚

**å¿œç”¨**:

1. **éŸ³å£°åˆæˆ(TTS)**: ãƒ†ã‚­ã‚¹ãƒˆâ†’éŸ³å£°æ³¢å½¢ç”Ÿæˆ
2. **éŸ³å£°èªè­˜(ASR)**: æ³¢å½¢â†’ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›
3. **éŸ³å£°å¼·èª¿**: ãƒã‚¤ã‚ºé™¤å»ã€è¶…è§£åƒ

**S4-WaveNetã®æ§‹é€ **:

```python
class S4WaveNet(nn.Module):
    def __init__(self, num_layers=30, d_model=256, d_state=64):
        self.layers = [S4Layer(d_model, d_state) for _ in range(num_layers)]
        self.output = nn.Linear(d_model, 1)  # Predict next sample

    def forward(self, x):
        # x: (batch, seq_len) waveform
        for layer in self.layers:
            x = layer(x) + x  # Residual
        return self.output(x)
```

**æ€§èƒ½**: WaveNet(CNN)ã¨åŒç­‰ã®éŸ³è³ªã€10xé«˜é€Ÿè¨“ç·´(ä¸¦åˆ—åŒ–)ã€æ¨è«–ã‚‚é«˜é€Ÿ(å†å¸°)ã€‚

**èª²é¡Œ**: ä½ç›¸ã®ä¿æŒã€‚SSMã¯æŒ¯å¹…ã‚’æ‰±ã†ã®ã¯å¾—æ„ã ãŒã€ä½ç›¸(sin/cos)ã¯è‹¦æ‰‹ã€‚Complexified SSM[^16]ã§è§£æ±ºã€‚

#### 6.6.3 ã‚²ãƒãƒŸã‚¯ã‚¹

**DNAé…åˆ—**ã¯è¶…é•·ç³»åˆ—(ãƒ’ãƒˆã‚²ãƒãƒ 30å„„å¡©åŸºå¯¾)ã€‚Transformerã¯ä¸å¯èƒ½ã€SSMã¯å¯èƒ½ã€‚

**å¿œç”¨**:

1. **éºä¼å­ç™ºç¾äºˆæ¸¬**: DNAé…åˆ— â†’ ã‚¿ãƒ³ãƒ‘ã‚¯è³ªç™ºç¾é‡
2. **å¤‰ç•°å½±éŸ¿äºˆæ¸¬**: SNP(ä¸€å¡©åŸºå¤šå‹)ãŒç–¾æ‚£ã«ä¸ãˆã‚‹å½±éŸ¿
3. **ã‚²ãƒãƒ ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³**: éºä¼å­ãƒ»èª¿ç¯€é ˜åŸŸã®è‡ªå‹•æ¤œå‡º

**HyenaDNA**[^17]: Hyena(SSMå¤‰ç¨®)ã‚’ç”¨ã„ãŸã‚²ãƒãƒ åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€‚100ä¸‡å¡©åŸºå¯¾ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§è¨“ç·´ã€‚

**æ€§èƒ½**: SOTA on 17/23 genomic benchmarksã€‚Transformerã¯ç³»åˆ—é•·åˆ¶ç´„ã§ä¸å¯èƒ½ã ã£ãŸã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã€‚

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**:

```python
# DNA tokenization
DNA_VOCAB = {"A": 0, "C": 1, "G": 2, "T": 3, "N": 4}

def tokenize_dna(sequence: str):
    return [DNA_VOCAB.get(base, 4) for base in sequence.upper()]

# Long-range regulatory element detection
class GenomicSSM:
    def forward(self, tokens):
        # tokens: (batch, 1_000_000) â€” 1M bp
        embeddings = self.embed(tokens)  # (batch, 1M, d_model)

        # SSM layers
        for ssm_layer in self.ssm_layers:
            embeddings = ssm_layer(embeddings)

        # Classify regulatory regions
        logits = self.classifier(embeddings)  # (batch, 1M, num_classes)
        return logits
```

#### 6.6.4 å¼·åŒ–å­¦ç¿’

**æ–¹ç­–(Policy)ã®ãƒ¢ãƒ‡ãƒ«åŒ–**ã«SSMã€‚è¦³æ¸¬å±¥æ­´â†’è¡Œå‹•ã®å†™åƒã‚’é•·è·é›¢ä¾å­˜è¾¼ã¿ã§å­¦ç¿’ã€‚

**å¿œç”¨**:

1. **Atari**: ã‚²ãƒ¼ãƒ ç”»é¢ç³»åˆ— â†’ è¡Œå‹•é¸æŠ
2. **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹**: ã‚»ãƒ³ã‚µãƒ¼å±¥æ­´ â†’ ãƒ¢ãƒ¼ã‚¿ãƒ¼åˆ¶å¾¡
3. **é‡‘èå–å¼•**: å¸‚å ´å±¥æ­´ â†’ å£²è²·åˆ¤æ–­

**S4RL**[^18]: S4ã‚’DQN/PPOã®Qãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯/æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«çµ„ã¿è¾¼ã¿ã€‚

**åˆ©ç‚¹**:

- **é•·æœŸå ±é…¬**: æ•°ç™¾ã‚¹ãƒ†ãƒƒãƒ—å…ˆã®å ±é…¬ã‚’è€ƒæ…®(RNNã¯å‹¾é…æ¶ˆå¤±ã§å›°é›£)
- **ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡**: Transformerã‚ˆã‚Šå°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
- **æ¨è«–é€Ÿåº¦**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ¶å¾¡ã«å¿…è¦ãªãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å®Ÿç¾

**å®Ÿè£…ä¾‹**:

```python
class S4Policy(nn.Module):
    def __init__(self, obs_dim, action_dim, d_state=64):
        self.s4_layers = [S4Layer(obs_dim, d_state) for _ in range(4)]
        self.policy_head = nn.Linear(obs_dim, action_dim)

    def forward(self, obs_sequence):
        # obs_sequence: (batch, seq_len, obs_dim)
        h = obs_sequence
        for layer in self.s4_layers:
            h = layer(h)

        # Last hidden state â†’ action distribution
        return self.policy_head(h[:, -1, :])
```

### 6.7 SSMãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©³ç´°: Long Range Arena

**Long Range Arena (LRA)**[^5]ã¯ã€é•·è·é›¢ä¾å­˜ã‚’æ¸¬å®šã™ã‚‹æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚6ã‚¿ã‚¹ã‚¯ã€‚

#### Task 1: ListOps (ç³»åˆ—é•· 2K)

**ã‚¿ã‚¹ã‚¯**: å…¥ã‚Œå­ãƒªã‚¹ãƒˆæ¼”ç®—ã®çµæœã‚’äºˆæ¸¬ã€‚

ä¾‹: `[MAX 4 [MIN 2 3] [MAX 1 5]]` â†’ `5`

**é›£ã—ã•**: ãƒã‚¹ãƒˆãŒæ·±ã„(æœ€å¤§10å±¤)ã€‚å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¾å­˜é–¢ä¿‚ã‚’è¿½è·¡å¿…è¦ã€‚

**çµæœ**:

| Model | Accuracy |
|:------|:---------|
| Transformer | 36.4% |
| S4 | 58.3% |
| **Mamba** | **59.7%** |

Transformerã¯ListOpsã§å£Šæ»…çš„ã€‚é•·è·é›¢ã®å…¥ã‚Œå­ã‚’è¿½ãˆãªã„ã€‚SSMã¯å‹åˆ©ã€‚

#### Task 2: Text Classification (ç³»åˆ—é•· 4K)

**ã‚¿ã‚¹ã‚¯**: IMDbæ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼(4K tokens)ã®æ„Ÿæƒ…åˆ†é¡ã€‚

**é›£ã—ã•**: ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æœ€åˆã¨æœ€å¾Œã§æ„è¦‹ãŒé€†è»¢ã™ã‚‹å ´åˆã‚ã‚Šã€‚å…¨ä½“ã‚’è¦‹æ¸¡ã™å¿…è¦ã€‚

**çµæœ**:

| Model | Accuracy |
|:------|:---------|
| Transformer | 64.3% |
| S4 | 86.8% |
| **Mamba** | **87.1%** |

Transformerã¯4Kã§æ€§èƒ½åŠ£åŒ–ã€‚SSMã¯é•·æ–‡ã‚’å®‰å®šå‡¦ç†ã€‚

#### Task 3: Retrieval (ç³»åˆ—é•· 4K)

**ã‚¿ã‚¹ã‚¯**: 2ã¤ã®æ–‡æ›¸(å„2K tokens)ãŒåŒã˜ãƒˆãƒ”ãƒƒã‚¯ã‹åˆ¤å®šã€‚

**é›£ã—ã•**: æ–‡æ›¸é–“ã®å¯¾å¿œã‚’ã€4Ké›¢ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³é–“ã§è¦‹ã¤ã‘ã‚‹ã€‚

**çµæœ**:

| Model | Accuracy |
|:------|:---------|
| Transformer | 57.5% |
| S4 | 90.5% |
| **Mamba** | **90.9%** |

SSMã®åœ§å‹ã€‚é•·è·é›¢ãƒãƒƒãƒãƒ³ã‚°ã«å¼·ã„ã€‚

#### Task 4: Image Classification (ç³»åˆ—é•· 1K)

**ã‚¿ã‚¹ã‚¯**: CIFAR-10ç”»åƒ(32Ã—32Ã—3 = 1024 pixels)ã‚’1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ã—ã¦åˆ†é¡ã€‚

**é›£ã—ã•**: 2Dæ§‹é€ ã‚’1Dèµ°æŸ»ã§ä¿æŒã€‚

**çµæœ**:

| Model | Accuracy |
|:------|:---------|
| Transformer | 89.3% |
| S4 | 88.7% |
| **Mamba** | 89.1% |

TransformerãŒåƒ…å·®ã§å‹åˆ©ã€‚ç”»åƒã¯å…¨ãƒ”ã‚¯ã‚»ãƒ«å‚ç…§(Attention)ãŒè‹¥å¹²æœ‰åˆ©ã€‚SSMã‚‚å¥é—˜ã€‚

#### Task 5: Pathfinder (ç³»åˆ—é•· 1K)

**ã‚¿ã‚¹ã‚¯**: ç”»åƒä¸­ã®2ç‚¹ãŒç·šã§ç¹‹ãŒã£ã¦ã„ã‚‹ã‹åˆ¤å®šã€‚

**é›£ã—ã•**: é•·ã„æ›²ç·šã‚’ãŸã©ã‚‹å¿…è¦ã€‚

**çµæœ**:

| Model | Accuracy |
|:------|:---------|
| Transformer | 71.5% |
| S4 | 86.1% |
| **Mamba** | 86.4% |

SSMãŒå‹åˆ©ã€‚çµŒè·¯è¿½è·¡ã¯é•·è·é›¢ä¾å­˜ã®å…¸å‹ã€‚

#### Task 6: Path-X (ç³»åˆ—é•· 16K)

**ã‚¿ã‚¹ã‚¯**: Pathfinderã®16å€é•·ãƒãƒ¼ã‚¸ãƒ§ãƒ³(128Ã—128ç”»åƒ)ã€‚

**é›£ã—ã•**: 16K pixelsã®ç³»åˆ—ã€‚Transformerã¯ãƒ¡ãƒ¢ãƒªä¸è¶³ã§å®Ÿè¡Œä¸å¯èƒ½ã€‚

**çµæœ**:

| Model | Accuracy |
|:------|:---------|
| Transformer | **Fail** (OOM) |
| **S4** | **88.1%** |
| Mamba | 88.5% |

Transformerã¯Path-Xã‚’è§£ã‘ãªã„(16KÂ²ã®Attentionè¡Œåˆ— = 1GB)ã€‚SSMã®ã¿å®Ÿè¡Œå¯èƒ½ã€‚

**ç·åˆè©•ä¾¡**:

- **Transformer**: çŸ­ç³»åˆ—(1K)ã§ã¯æœ€å¼·ã€é•·ç³»åˆ—(4K+)ã§å´©å£Š
- **S4**: å…¨ã‚¿ã‚¹ã‚¯ã§å®‰å®šã€ç‰¹ã«è¶…é•·ç³»åˆ—(16K)ã§å”¯ä¸€ã®é¸æŠè‚¢
- **Mamba**: S4ã‚’ã»ã¼å…¨ã¦ã®ã‚¿ã‚¹ã‚¯ã§ä¸Šå›ã‚‹ã€‚é¸æŠæ€§ã®åŠ¹æœ

### 6.8 æ•™ç§‘æ›¸ãƒ»ãƒªã‚½ãƒ¼ã‚¹

#### ä¸»è¦æ•™ç§‘æ›¸

| æ›¸ç± | è‘—è€… | å†…å®¹ | ãƒ¬ãƒ™ãƒ« |
|:-----|:-----|:-----|:-------|
| **Modern Control Engineering** | Ogata (2009) | åˆ¶å¾¡ç†è«–ã®å¤å…¸ã€‚çŠ¶æ…‹ç©ºé–“ã®æ•°å­¦çš„åŸºç¤ | å­¦éƒ¨ã€œé™¢ |
| **Linear System Theory and Design** | Chen (1998) | SSMã®æ•°å­¦ã€‚å¯åˆ¶å¾¡æ€§ã€å¯è¦³æ¸¬æ€§ | é™¢ |
| **Deep Learning** | Goodfellow+ (2016) | RNN/LSTMç« ã€‚SSMã¨ã®å¯¾æ¯”ã«æœ‰ç”¨ | å­¦éƒ¨ã€œé™¢ |
| **Dive into Deep Learning** | Zhang+ (2023) | æœ€æ–°ç‰ˆã«SSMç« ã‚ã‚Š(2025 edition) | å­¦éƒ¨ |

#### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

| è³‡æº | èª¬æ˜ | URL |
|:-----|:-----|:----|
| **å…¬å¼å®Ÿè£…** | state-spaces/mamba | [GitHub](https://github.com/state-spaces/mamba) |
| **Annotated S4** | Rushè§£èª¬ã€‚å®Ÿè£…ä»˜ã | [Annotated S4](https://srush.github.io/annotated-s4/) |
| **Long Range Arena** | Benchmark suite | [GitHub](https://github.com/google-research/long-range-arena) |
| **Hazy Research Blog** | Guç ”ç©¶å®¤ã®ãƒ–ãƒ­ã‚°ã€‚HiPPO/S4ã®ç›´æ„Ÿçš„è§£èª¬ | [Blog](https://hazyresearch.stanford.edu/blog) |
| **Together AI Tech Report** | Mamba/SSMã®ç”£æ¥­å¿œç”¨ | [Together](https://together.ai/blog) |
| **SSM Survey (2025)** | S4â†’Mambaã®åŒ…æ‹¬çš„ã‚µãƒ¼ãƒ™ã‚¤ | [arXiv:2503.18970](https://arxiv.org/abs/2503.18970) |

#### å®Ÿè£…ãƒªãƒã‚¸ãƒˆãƒª

| Repo | è¨€èª | ç‰¹å¾´ |
|:-----|:-----|:-----|
| [state-spaces/mamba](https://github.com/state-spaces/mamba) | Python/CUDA | å…¬å¼ã€‚Tritonã‚«ãƒ¼ãƒãƒ« |
| [state-spaces/s4](https://github.com/state-spaces/s4) | Python/JAX | S4åŸè«–æ–‡ã®å®Ÿè£… |
| [mamba-minimal](https://github.com/johnma2006/mamba-minimal) | Python | æ•™è‚²çš„æœ€å°å®Ÿè£…(300è¡Œ) |
| [mamba.rs](https://github.com/huggingface/mamba.rs) | Rust | Hugging Face Rustãƒãƒ¼ãƒˆ |
| [Mamba.jl](https://github.com/CarpeAI/Mamba.jl) | Julia | Juliaå®Ÿè£…(ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£) |

#### è«–æ–‡èª­è§£ã‚¬ã‚¤ãƒ‰

**S4ã‚’èª­ã‚€é †åº**:

1. **HiPPOè«–æ–‡**[^1] (2020): é•·è·é›¢è¨˜æ†¶ã®ç†è«–çš„åŸºç›¤ã‚’ç†è§£
2. **S4è«–æ–‡**[^2] (2021): DPLRåˆ†è§£ã¨FFTé«˜é€ŸåŒ–
3. **Annotated S4**: å®Ÿè£…ã¨æ•°å¼ã®å¯¾å¿œã‚’è¿½ã†
4. **S4Dè«–æ–‡** (2022): å¯¾è§’è¿‘ä¼¼ã®ç°¡ç•¥åŒ–
5. **Mambaè«–æ–‡**[^3] (2023): Selective SSMã¸ã®é€²åŒ–

**Mambaã‚’èª­ã‚€é †åº**:

1. **S4ã‚’å…ˆã«ç†è§£** (ä¸Šè¨˜)
2. **Mambaè«–æ–‡**[^3]: Selective SSMã®å‹•æ©Ÿã¨è¨­è¨ˆ
3. **Hardware-aware scanã®Appendix**: CUDAã‚«ãƒ¼ãƒãƒ«ã®è©³ç´°
4. **Mamba-2/SSDè«–æ–‡**[^7]: Attentionã¨SSMã®ç­‰ä¾¡æ€§
5. **å…¬å¼å®Ÿè£…**: Tritonã‚³ãƒ¼ãƒ‰ã‚’èª­ã‚€

**ã¤ã¾ãšããƒã‚¤ãƒ³ãƒˆã¨å¯¾ç­–**:

| ã¤ã¾ãšã | å¯¾ç­– |
|:---------|:-----|
| è¡Œåˆ—æŒ‡æ•°é–¢æ•°$e^{At}$ | ç¬¬2å›ç·šå½¢ä»£æ•°IIã‚’å¾©ç¿’ã€‚ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ãƒ»å¯¾è§’åŒ– |
| ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«$\bar{\mathcal{K}}$ | é›¢æ•£ç•³ã¿è¾¼ã¿ã®å®šç¾©ã‚’ç¢ºèªã€‚FFTã®åŸç†(ç¬¬4å›) |
| HiPPOå¤šé …å¼è¿‘ä¼¼ | ç›´äº¤å¤šé …å¼(Legendre)ã®æ€§è³ªã‚’èª¿ã¹ã‚‹ |
| DPLRåˆ†è§£ | Woodburyæ’ç­‰å¼ã€ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®æ€§è³ª(ç¬¬3å›SVD) |
| Parallel Scan | çµåˆå¾‹ã®ç¢ºèªã€‚prefix sumã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¡æ¨ |

### 6.9 ç”¨èªé›†(å®Œå…¨ç‰ˆ)

| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| **SSM** | State Space Model | çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ã€‚éš ã‚ŒçŠ¶æ…‹$h_t$ã‚’ä»‹ã—ã¦å…¥å‡ºåŠ›ã‚’å¤‰æ› |
| **HiPPO** | High-order Polynomial Projection Operators | å¤šé …å¼å°„å½±æ¼”ç®—å­ã€‚é•·è·é›¢è¨˜æ†¶ã®æœ€é©åˆæœŸåŒ–ç†è«– |
| **DPLR** | Diagonal Plus Low-Rank | å¯¾è§’+ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã€‚$A = \Lambda - PQ^*$ |
| **ZOH** | Zero-Order Hold | é›¢æ•£åŒ–æ‰‹æ³•ã€‚åŒºé–“å†…ã§å…¥åŠ›ã‚’å®šæ•°ã¨ä»®å®š |
| **Selective SSM** | Selective State Space Model | å…¥åŠ›ä¾å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\Delta_t, B_t, C_t$ã‚’æŒã¤SSM |
| **Parallel Scan** | Parallel Prefix Scan | çµåˆçš„æ¼”ç®—ã®ä¸¦åˆ—ç´¯ç©è¨ˆç®—ã€‚$O(\log N)$æ·±åº¦ |
| **LTI** | Linear Time-Invariant | ç·šå½¢æ™‚ä¸å¤‰ã‚·ã‚¹ãƒ†ãƒ ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ™‚é–“ã§å¤‰åŒ–ã—ãªã„ |
| **Causal Masking** | Causal Masking | æœªæ¥ã‚’è¦‹ãªã„åˆ¶ç´„ã€‚$i < j$ã§$M_{ij} = 0$ |
| **Semi-Separable Matrix** | Semi-Separable Matrix | ä¸‹ä¸‰è§’ãŒä½ãƒ©ãƒ³ã‚¯æ§‹é€ ã®è¡Œåˆ— |
| **Toeplitz Matrix** | Toeplitz Matrix | å¯¾è§’ç·šä¸Šã®å€¤ãŒä¸€å®šã®è¡Œåˆ—ã€‚ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ« |
| **Cauchy Kernel** | Cauchy Kernel | $K(\omega) = \sum_i \frac{c_i}{\omega - \lambda_i}$ã€‚S4ã®FFTé«˜é€ŸåŒ– |
| **Recurrent Form** | Recurrent Form | SSMã®å†å¸°å½¢æ…‹ã€‚$h_t = \bar{A}h_{t-1} + \bar{B}u_t$ |
| **Convolutional Form** | Convolutional Form | SSMã®ç•³ã¿è¾¼ã¿å½¢æ…‹ã€‚$y = \bar{\mathcal{K}} * u$ |
| **Chunk-wise Processing** | Chunk-wise Processing | ç³»åˆ—ã‚’å°ã•ãªchunkã«åˆ†å‰²ã—ã¦å‡¦ç†ã€‚Mamba-2 |
| **Ring Attention** | Ring Attention | åˆ†æ•£Attentionã€‚å„GPUãŒchunkã‚’æŒã¡ã€ringçŠ¶ã«é€šä¿¡ |
| **LoRA** | Low-Rank Adaptation | ä½ãƒ©ãƒ³ã‚¯é©å¿œã€‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®åŠ¹ç‡åŒ– |
| **Content-based Addressing** | Content-based Addressing | å†…å®¹ã«åŸºã¥ãã‚¢ãƒ‰ãƒ¬ãƒƒã‚·ãƒ³ã‚°ã€‚Attentionç‰¹æœ‰ |
| **Position-based Addressing** | Position-based Addressing | ä½ç½®ã«åŸºã¥ãã‚¢ãƒ‰ãƒ¬ãƒƒã‚·ãƒ³ã‚°ã€‚ç·šå½¢RNNç‰¹æœ‰ |

:::message
**é€²æ—: 95% å®Œäº†** SSMã®æœ€å‰ç·šã¨ç ”ç©¶å‹•å‘ã‚’æŠŠæ¡ã€‚æ¬¡ã¯æŒ¯ã‚Šè¿”ã‚Šã¨æ¬¡å›äºˆå‘Šã€‚
:::

---

### 6.10 ä»Šå›ã®å­¦ç¿’å†…å®¹

### 10.2 æœ¬è¬›ç¾©ã®ä¸»è¦ãªå­¦ã³

1. **SSMã®3å½¢æ…‹**: é€£ç¶šæ™‚é–“ODE â†’ å†å¸°(æ¨è«–) â†’ ç•³ã¿è¾¼ã¿(è¨“ç·´)
2. **é›¢æ•£åŒ–**: ZOHã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\bar{A}, \bar{B}$ã‚’è¨ˆç®—
3. **HiPPOç†è«–**: å¤šé …å¼è¿‘ä¼¼ã«ã‚ˆã‚‹é•·è·é›¢è¨˜æ†¶ã®æœ€é©åˆæœŸåŒ–
4. **S4**: DPLRåˆ†è§£ + FFTã§$O(L \log L)$è¨“ç·´
5. **Mamba**: Selective SSM($\Delta, B, C$ãŒå…¥åŠ›ä¾å­˜) + Parallel Scanã§"å¿˜ã‚Œã‚‹"é™ç•Œã‚’å…‹æœ

**æ ¸å¿ƒ**: RNNã¯å¿˜ã‚Œã€Attentionã¯$O(N^2)$ã§æ­»ã¬ã€‚SSMã¯ç†è«–(HiPPO)+æ§‹é€ (DPLR)+é¸æŠæ€§(Mamba)ã§ä¸¡æ–¹ã‚’è§£æ±ºã€‚

### 10.3 ã‚ˆãã‚ã‚‹è³ªå•(FAQ)

:::details Q1: SSMã¯Transformerã‚’å®Œå…¨ã«ç½®ãæ›ãˆã‚‹ã‹ï¼Ÿ
A: ç¾æ™‚ç‚¹ã§ã¯**No**ã€‚è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ã¯Mamba â‰ˆ Transformerã€ç”»åƒã§ã¯Attentionå„ªä½ã€‚ãŸã ã—Hybrid(ç¬¬18å›)ãŒä¸»æµã«ãªã‚‹å¯èƒ½æ€§ã€‚ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚

**è©³ç´°**: Attentionã®Content-based addressingã¯ã€Few-shotå­¦ç¿’ã‚„In-context learningã§æœ¬è³ªçš„ã€‚SSMã®Position-based addressingã§ã¯å®Œå…¨ã«ä»£æ›¿ã§ããªã„ã€‚ãŸã ã—ã€å¤šãã®ã‚¿ã‚¹ã‚¯(è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€æ™‚ç³»åˆ—äºˆæ¸¬)ã§ã¯SSMã§ååˆ†ãªæ€§èƒ½ãŒå‡ºã¦ã„ã‚‹ã€‚
:::

:::details Q2: Mambaã®Selective SSMã¯LSTMã®ã‚²ãƒ¼ãƒˆã¨åŒã˜ï¼Ÿ
A: å“²å­¦ã¯ä¼¼ã¦ã„ã‚‹(é¸æŠçš„è¨˜æ†¶)ãŒã€ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ç•°ãªã‚‹ã€‚LSTMã¯éç·šå½¢ã‚²ãƒ¼ãƒˆ($\sigma, \tanh$)ã€Mambaã¯ç·šå½¢SSMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…¥åŠ›ä¾å­˜ã«ã™ã‚‹ã€‚Mambaã®æ–¹ãŒFFTè¨“ç·´ã¨å†å¸°æ¨è«–ã‚’ä¸¡ç«‹ã—ã‚„ã™ã„ã€‚

**LSTMã¨Mambaã®å¯¾å¿œ**:

| LSTM | Mamba |
|:-----|:------|
| Forget gate $f_t = \sigma(W_f [h_{t-1}, x_t])$ | $\Delta_t = \text{Softplus}(W_\Delta u_t)$ (æ¸›è¡°ç‡) |
| Input gate $i_t = \sigma(W_i [h_{t-1}, x_t])$ | $B_t = W_B u_t$ (æ›¸ãè¾¼ã¿å¼·åº¦) |
| Output gate $o_t = \sigma(W_o [h_{t-1}, x_t])$ | $C_t = W_C u_t$ (èª­ã¿å‡ºã—å¼·åº¦) |

Mambaã¯ç·šå½¢ â†’ ç•³ã¿è¾¼ã¿å½¢æ…‹ã§ä¸¦åˆ—è¨“ç·´å¯èƒ½ã€‚LSTMã¯éç·šå½¢ â†’ é€æ¬¡è¨“ç·´ã®ã¿ã€‚
:::

:::details Q3: Parallel Scanã¯æœ¬å½“ã«é€Ÿã„ï¼Ÿ
A: GPUä¸Šã§ã¯**Yes**ã€‚CPUã§ã¯ä¸¦åˆ—åº¦ãŒé™ã‚‰ã‚Œã‚‹ãŸã‚åŠ¹æœè–„ã€‚CUDAã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–ãŒå¿…é ˆã€‚Mambaã®å…¬å¼å®Ÿè£…ã¯Triton/CUDAã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(ç³»åˆ—é•·10K, d=64)**:

| å®Ÿè£… | ãƒ‡ãƒã‚¤ã‚¹ | æ™‚é–“(ms) | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ(tok/s) |
|:-----|:---------|:---------|:-------------------|
| Sequential scan | CPU | 120 | 83K |
| Parallel scan(naive) | CPU | 150 | 67K (overhead) |
| Sequential scan | GPU | 15 | 667K |
| **Parallel scan(optimized)** | **GPU** | **2.5** | **4M** |

GPU + æœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«ã§160xé«˜é€ŸåŒ–ã€‚ã“ã‚ŒãŒMambaè¨“ç·´ã®éµã€‚
:::

:::details Q4: ãªãœå›ºæœ‰å€¤ãŒè² ãªã‚‰å®‰å®šï¼Ÿ
A: $h_t = \bar{A}^t h_0$ã§ã€$\bar{A} = e^{A\Delta}$ã€‚$A$ã®å›ºæœ‰å€¤$\lambda < 0$ãªã‚‰$e^{\lambda \Delta t} \to 0$ as $t \to \infty$ã€‚çŠ¶æ…‹ãŒæ¸›è¡°â†’å®‰å®šã€‚æ­£ãªã‚‰çˆ†ç™ºâ†’ä¸å®‰å®šã€‚

**æ•°å€¤ä¾‹**:

```julia
Î» = -2.0
Î” = 0.1
A_bar = exp(Î» * Î”)  # exp(-0.2) â‰ˆ 0.8187

# After t steps: h_t = (0.8187)^t h_0
# t=10: h_10 â‰ˆ 0.145 h_0 (æ¸›è¡°)
# t=50: h_50 â‰ˆ 1.7e-5 h_0 (ã»ã¼æ¶ˆå¤±)
```

HiPPOã®å›ºæœ‰å€¤$-1, -2, -3, \ldots$ã¯ã€ç•°ãªã‚‹æ¸›è¡°ç‡ â†’ å¤šæ§˜ãªæ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã€‚
:::

:::details Q5: S4/Mambaã‚’è‡ªåˆ†ã®ã‚¿ã‚¹ã‚¯ã§ä½¿ã†ã«ã¯ï¼Ÿ
A: Hugging Face Transformersã«mambaå®Ÿè£…ã‚ã‚Šã€‚`MambaForCausalLM`ã§è¨€èªãƒ¢ãƒ‡ãƒ«è¨“ç·´å¯èƒ½ã€‚ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ã¯å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª[^6]ã®examplesã‚’å‚ç…§ã€‚

**Hugging Faceä½¿ç”¨ä¾‹**:

```python
from transformers import MambaForCausalLM, AutoTokenizer

model = MambaForCausalLM.from_pretrained("state-spaces/mamba-130m")
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")

# Inference
inputs = tokenizer("The future of AI is", return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0]))
```

**ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯(æ™‚ç³»åˆ—äºˆæ¸¬)**:

```python
from mamba_ssm import Mamba

class TimeSeriesSSM(nn.Module):
    def __init__(self, input_dim, d_model=256, d_state=16, num_layers=4):
        self.embed = nn.Linear(input_dim, d_model)
        self.mamba_layers = [Mamba(d_model, d_state) for _ in range(num_layers)]
        self.output = nn.Linear(d_model, 1)

    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        x = self.embed(x)
        for layer in self.mamba_layers:
            x = layer(x) + x  # Residual
        return self.output(x[:, -1, :])  # Predict next value
```
:::

:::details Q6: S4ã¨Mambaã®å®Ÿè£…ã®é•ã„ã¯ï¼Ÿ
A: **S4**ã¯å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$A, B, C$ã‚’ä½¿ã„ã€ç•³ã¿è¾¼ã¿å½¢æ…‹ã§è¨“ç·´ã€‚**Mamba**ã¯å…¥åŠ›ä¾å­˜$\Delta_t, B_t, C_t$ã‚’ä½¿ã„ã€Parallel Scanã§è¨“ç·´ã€‚

**å®Ÿè£…ã®è¤‡é›‘ã•**:

| Aspect | S4 | Mamba |
|:-------|:---|:------|
| ã‚«ãƒ¼ãƒãƒ«è¨ˆç®— | FFT(æ—¢å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª) | Custom CUDA kernel |
| è¨“ç·´ | ç•³ã¿è¾¼ã¿(æ¨™æº–) | Parallel Scan(ç‰¹æ®Š) |
| æ¨è«– | å†å¸°(ç°¡å˜) | å†å¸°(ç°¡å˜) |
| ã‚³ãƒ¼ãƒ‰è¡Œæ•° | ~500 | ~1500 |

Mambaã¯é«˜æ€§èƒ½ã ãŒå®Ÿè£…ã‚³ã‚¹ãƒˆã‚‚é«˜ã„ã€‚æ•™è‚²ç›®çš„ãªã‚‰S4ã‹ã‚‰å§‹ã‚ã‚‹ã®ãŒè‰¯ã„ã€‚
:::

:::details Q7: SSMã¯ä»–ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£(ç”»åƒãƒ»éŸ³å£°)ã§ã‚‚ä½¿ãˆã‚‹ï¼Ÿ
A: **Yes**ã€‚ãŸã ã—1Dã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–ãŒå¿…è¦ã€‚

**ç”»åƒ**: Raster/Hilbertæ›²ç·šã§1DåŒ– â†’ SSMé©ç”¨ã€‚Vision Mamba(VMamba)ã¯4æ–¹å‘ã‚¹ã‚­ãƒ£ãƒ³ã‚’ä½¿ç”¨ã€‚æ€§èƒ½ã¯ViTã«è¿«ã‚‹ãŒã€ã¾ã Atteninå„ªä½ã€‚

**éŸ³å£°**: æ³¢å½¢ã‚’ç›´æ¥SSMã§å‡¦ç†ã€‚S4-WaveNetã¯éŸ³å£°åˆæˆã§WaveNetä¸¦ã¿ã€‚

**å‹•ç”»**: ãƒ•ãƒ¬ãƒ¼ãƒ ç³»åˆ—ã¨ã—ã¦å‡¦ç†ã€‚ç©ºé–“çš„Attentionã¨ã®çµ„ã¿åˆã‚ã›(Hybrid)ãŒæœ‰æœ›ã€‚

**ãƒã‚¤ãƒ³ãƒˆã‚³ãƒ©ã‚¦ãƒ‰**: 3Dç‚¹ç¾¤ã‚’1DåŒ–(z-order curve) â†’ SSMã€‚ç ”ç©¶æ®µéšã€‚
:::

:::details Q8: SSMã®è¨“ç·´ã¯Transformerã‚ˆã‚Šé€Ÿã„ï¼Ÿ
A: **è¨“ç·´é€Ÿåº¦ã¯åŒç­‰ã€œã‚„ã‚„é€Ÿã„**ã€‚æ¨è«–ã¯SSMãŒåœ§å€’çš„ã«é€Ÿã„ã€‚

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°, 125M params)**:

| Model | è¨“ç·´æ™‚é–“(100K steps) | æ¨è«–é€Ÿåº¦(tok/s) | ãƒ¡ãƒ¢ãƒª(è¨“ç·´) |
|:------|:---------------------|:----------------|:-------------|
| Transformer | 48h | 2.3K | 24GB |
| S4 | 52h | 7K | 18GB |
| **Mamba** | **45h** | **11.5K** | **16GB** |

Mambaã¯è¨“ç·´ã‚‚ã‚„ã‚„é€Ÿãã€æ¨è«–ã¯5å€é€Ÿã€‚ãƒ¡ãƒ¢ãƒªã‚‚å‰Šæ¸›ã€‚
:::

:::details Q9: HiPPOåˆæœŸåŒ–ã¯å¿…é ˆï¼Ÿ
A: é•·è·é›¢ä¾å­˜ã‚¿ã‚¹ã‚¯(LRA Path-Xç­‰)ã§ã¯**ã»ã¼å¿…é ˆ**ã€‚çŸ­è·é›¢ã‚¿ã‚¹ã‚¯ã§ã¯ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§ã‚‚å¯ã€‚

**å®Ÿé¨“çµæœ(ã‚³ãƒ”ãƒ¼ã‚¿ã‚¹ã‚¯, T=1000)**:

| åˆæœŸåŒ– | Test Acc | è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•° |
|:-------|:---------|:---------------|
| Random | 32% | 100 (åæŸã›ãš) |
| **HiPPO** | **87%** | **50** |

HiPPOã¯é•·è·é›¢è¨˜æ†¶ã®ç†è«–çš„ä¿è¨¼ãŒã‚ã‚Šã€è¨“ç·´ã‚‚å®‰å®šãƒ»é«˜é€Ÿã€‚
:::

:::details Q10: SSMã¯è¨ˆç®—è¤‡é›‘æ€§ç†è«–ã§ä½•ãŒã§ãã‚‹ï¼Ÿ
A: **ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°å®Œå…¨æ€§**ã¯è¨¼æ˜ã•ã‚Œã¦ã„ãªã„(Transformerã¯æ¡ä»¶ä»˜ãã§è¨¼æ˜æ¸ˆã¿[^15])ã€‚

**ç¾çŠ¶ã®ç†è§£**:

- SSMã¯**ç·šå½¢å†å¸°**ã®ä¸€ç¨® â†’ æœ‰é™çŠ¶æ…‹ã‚ªãƒ¼ãƒˆãƒãƒˆãƒ³ã¨ç­‰ä¾¡(ç†è«–ä¸Š)
- Mambaã®Selective SSMã¯ã€å…¥åŠ›ä¾å­˜ã§**çŠ¶æ…‹é·ç§»é–¢æ•°ãŒå¤‰åŒ–** â†’ ã‚ˆã‚Šè¡¨ç¾åŠ›ãŒé«˜ã„
- Mamba-2/SSDã¯ã€ŒAttention â‰ˆ SSMã€ã‚’ç¤ºã—ãŸ â†’ ç†è«–çš„ç­‰ä¾¡æ€§ã®è¨¼æ˜

**æœªè§£æ±ºå•é¡Œ**: MambaãŒTransformerã¨åŒç­‰ã®ã‚¿ã‚¹ã‚¯ã‚’è§£ã‘ã‚‹ã‹ï¼Ÿ å®Ÿè¨¼çš„ã«ã¯**Yes**ã ãŒã€ç†è«–çš„è¨¼æ˜ã¯æœªå®Œã€‚
:::

### 10.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«(1é€±é–“ãƒ—ãƒ©ãƒ³)

| Day | å†…å®¹ | æ™‚é–“ | ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ |
|:----|:-----|:-----|:---------------|
| Day 1 | Zone 0-2(å°å…¥ãƒ»ç›´æ„Ÿ) | 1h | â˜ SSMå†å¸°å½¢æ…‹ã‚’å®Ÿè£… â˜ å›ºæœ‰å€¤ã¨æ¸›è¡°ã®é–¢ä¿‚ã‚’ç†è§£ |
| Day 2 | Zone 3.1-3.3(é€£ç¶šSSMãƒ»é›¢æ•£åŒ–ãƒ»ç•³ã¿è¾¼ã¿) | 2h | â˜ ZOHé›¢æ•£åŒ–ã‚’å°å‡º â˜ FFTç•³ã¿è¾¼ã¿ã‚’å®Ÿè£… |
| Day 3 | Zone 3.4-3.5(HiPPOãƒ»S4) | 2h | â˜ HiPPOè¡Œåˆ—ã‚’æ§‹ç¯‰ â˜ DPLRåˆ†è§£ã‚’ç†è§£ |
| Day 4 | Zone 3.6-3.8(Mambaç†è«–) | 2h | â˜ Selective SSMã‚’å°å‡º â˜ Parallel Scanã‚’å®Ÿè£… |
| Day 5 | Zone 4(Julia/Rustå®Ÿè£…) | 3h | â˜ Julia S4å®Ÿè£… â˜ Rust scanã‚«ãƒ¼ãƒãƒ« |
| Day 6 | Zone 5(å®Ÿé¨“ãƒ»ãƒ†ã‚¹ãƒˆ) | 2h | â˜ LRAå®Ÿé¨“ â˜ HiPPO vs Randomæ¯”è¼ƒ |
| Day 7 | Zone 6-7(ç™ºå±•ãƒ»å¾©ç¿’) | 2h | â˜ Mamba-2ã‚’ç†è§£ â˜ å…¨ä½“ã‚’æŒ¯ã‚Šè¿”ã‚Š |

**Total**: 14æ™‚é–“ã€‚1æ—¥2æ™‚é–“ãƒšãƒ¼ã‚¹ã§1é€±é–“ã€‚

#### è©³ç´°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«(æ™‚é–“åˆ¥)

**Week 1: ç†è«–ç·¨(å‰åŠ)**

| æ™‚é–“å¸¯ | æœˆ | ç« | æ°´ | æœ¨ | é‡‘ | åœŸ | æ—¥ |
|:-------|:---|:---|:---|:---|:---|:---|:---|
| æœ(1h) | Z0-1 | Z3.1 | Z3.4 | Z3.6 | å¾©ç¿’ | Z4.1-3 | Z6.1-2 |
| å¤œ(1h) | Z2 | Z3.2-3 | Z3.5 | Z3.7-8 | Z5.1-2 | Z4.4-7 | Z7+ç·å¾©ç¿’ |

**é€²æ—ç¢ºèª**:

- **Day 3çµ‚äº†æ™‚**: SSMç†è«–ã®70%å®Œäº†ã€‚HiPPO/S4ã‚’ç†è§£ã§ãã¦ã„ã‚Œã°OKã€‚
- **Day 5çµ‚äº†æ™‚**: å®Ÿè£…åŠ›ã®80%å®Œäº†ã€‚Juliaã§å‹•ãã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Œã°OKã€‚
- **Day 7çµ‚äº†æ™‚**: å…¨ä½“ã®100%å®Œäº†ã€‚è«–æ–‡ã‚’èª­ã‚€æº–å‚™å®Œäº†ã€‚

#### æŒ«æŠ˜ã—ãªã„ãŸã‚ã®Tips

1. **æ•°å¼ã¯æ‰‹ã§æ›¸ã**: èª­ã‚€ã ã‘ã§ã¯èº«ã«ã¤ã‹ãªã„ã€‚ç´™ã¨ãƒšãƒ³ã§å°å‡ºã‚’è¿½ã†ã€‚
2. **ã‚³ãƒ¼ãƒ‰ã‚’å‹•ã‹ã™**: å…¨ã¦ã®å¼ã‚’Juliaã§å®Ÿè£…ã€‚å‹•ã‹ãªã„ã¨ã‚ã‹ã‚‰ãªã„ã€‚
3. **å°ã•ãå§‹ã‚ã‚‹**: $d=4, L=16$ã®å°ã•ãªSSMã‹ã‚‰ã€‚ã„ããªã‚Š$d=256$ã¯ç„¡ç†ã€‚
4. **è¦–è¦šåŒ–**: Plotsã§çŠ¶æ…‹$h_t$ã€ã‚«ãƒ¼ãƒãƒ«$\bar{\mathcal{K}}$ã€æ¸›è¡°ã‚’å¯è¦–åŒ–ã€‚
5. **ä»²é–“ã‚’ä½œã‚‹**: Discordã‚„Slackã§å­¦ç¿’ä»²é–“ã‚’è¦‹ã¤ã‘ã‚‹ã€‚è©°ã¾ã£ãŸã‚‰ç›¸è«‡ã€‚

#### ã¤ã¾ãšãã‚„ã™ã„ãƒã‚¤ãƒ³ãƒˆã¨å¯¾ç­–(è©³ç´°ç‰ˆ)

| ã¤ã¾ãšã | ç—‡çŠ¶ | å¯¾ç­– | å‚ç…§ |
|:---------|:-----|:-----|:-----|
| è¡Œåˆ—æŒ‡æ•°é–¢æ•° | $e^{At}$ã®è¨ˆç®—ãŒã‚ã‹ã‚‰ãªã„ | ç¬¬2å›ç·šå½¢ä»£æ•°IIã‚’å¾©ç¿’ã€‚ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ãƒ»å¯¾è§’åŒ– | Zone 3.2 |
| é›¢æ•£åŒ–ã®å¼ | $\bar{B} = (A^{-1}(e^{A\Delta}-I))B$ã®æ„å‘³ | ç©åˆ†$\int_0^\Delta e^{A\tau} d\tau$ã‚’å°å‡º | Zone 3.2 |
| HiPPOå¤šé …å¼ | Legendreå¤šé …å¼ã®ç›´äº¤æ€§ | ç›´äº¤å¤šé …å¼ã®æ€§è³ªã‚’èª¿ã¹ã‚‹(Wikipedia) | Zone 3.4 |
| DPLRåˆ†è§£ | ãªãœä½ãƒ©ãƒ³ã‚¯? | Woodburyæ’ç­‰å¼ã€SVD(ç¬¬3å›)ã‚’å¾©ç¿’ | Zone 3.5 |
| Cauchyæ ¸ | FFTã¨ã®é–¢ä¿‚ | è¤‡ç´ è§£æã®ç•™æ•°å®šç†(ç™ºå±•) | Zone 3.5 |
| Parallel Scan | çµåˆå¾‹ãŒã‚ã‹ã‚‰ãªã„ | Prefix sumã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’èª¿ã¹ã‚‹ | Zone 3.7 |
| Juliaæ§‹æ–‡ | å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ | ç¬¬10å›Juliaå…¥é–€ã‚’å¾©ç¿’ | Zone 4 |
| Rustæ‰€æœ‰æ¨© | å€Ÿç”¨ã‚¨ãƒ©ãƒ¼ | ç¬¬9å›Rustå…¥é–€ã‚’å¾©ç¿’ | Zone 4.7 |

#### è¿½åŠ æ¼”ç¿’å•é¡Œ(ä¸Šç´šè€…å‘ã‘)

:::details æ¼”ç¿’1: S4ã®å›ºæœ‰å€¤å®‰å®šæ€§ã®è¨¼æ˜

**å•é¡Œ**: HiPPOè¡Œåˆ—$A$ã®å›ºæœ‰å€¤ã®å®Ÿéƒ¨ãŒå…¨ã¦è² ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã›ã€‚

**ãƒ’ãƒ³ãƒˆ**: $A$ã¯Normalè¡Œåˆ—($AA^* = A^*A$)ã€‚GerÅ¡gorinå††æ¿å®šç†ã‚’ç”¨ã„ã‚‹ã€‚

**è§£ç­”ã®æ–¹é‡**:
1. HiPPO-LegSè¡Œåˆ—ã®æ§‹é€ ã‚’ç¢ºèª
2. å„è¡Œã®å¯¾è§’æˆåˆ†ã¨éå¯¾è§’æˆåˆ†ã®å’Œã‚’è¨ˆç®—
3. GerÅ¡gorinå††æ¿ãŒå…¨ã¦å·¦åŠå¹³é¢ã«ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™
:::

:::details æ¼”ç¿’2: Mambaã®é¸æŠæ€§ã®åŠ¹æœã‚’å®šé‡åŒ–

**å•é¡Œ**: å…¥åŠ›ä¾å­˜$\Delta_t$ãŒå›ºå®š$\Delta$ã«æ¯”ã¹ã¦ã€ã©ã‚Œã ã‘æ€§èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã‹å®šé‡åŒ–ã›ã‚ˆã€‚

**å®Ÿé¨“è¨­è¨ˆ**:
1. Mamba-130Mãƒ¢ãƒ‡ãƒ«ã§$\Delta_t = \text{const}$(S4åŒ–)ã¨$\Delta_t = \text{Softplus}(W u_t)$(Mamba)ã‚’æ¯”è¼ƒ
2. LRAã®6ã‚¿ã‚¹ã‚¯ã§ç²¾åº¦ã‚’è¨ˆæ¸¬
3. å„ã‚¿ã‚¹ã‚¯ã§ã®æ”¹å–„ç‡ã‚’ç®—å‡º

**äºˆæƒ³**: ListOps(å…¥ã‚Œå­ä¾å­˜)ã§æœ€å¤§æ”¹å–„ã€Image(å±€æ‰€ä¾å­˜)ã§æœ€å°æ”¹å–„ã€‚
:::

:::details æ¼”ç¿’3: Linear Attentionã¨S4ã®é–¢ä¿‚

**å•é¡Œ**: Linear Attention(Performer)ã¨S4ã®æ•°å­¦çš„é–¢ä¿‚ã‚’å°å‡ºã›ã‚ˆã€‚

**ãƒ’ãƒ³ãƒˆ**: Performerã¯$\text{Attention}(Q,K,V) = \phi(Q) (\phi(K)^\top V)$ã¨åˆ†è§£ã€‚S4ã¯$y = (CA^k B) * u$ã€‚$\phi$ã‚’ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã¨è¦‹ãªã›ã°...?

**ç™ºå±•**: Mamba-2ã®SSDå®šç†ã¨æ¥ç¶šã§ãã‚‹ã‹ï¼Ÿ
:::

:::details æ¼”ç¿’4: SSMã®ä¸‡èƒ½è¿‘ä¼¼å®šç†

**å•é¡Œ**: SSMãŒä»»æ„ã®é€£ç¶šé–¢æ•°$f: \mathbb{R}^L \to \mathbb{R}^L$ã‚’è¿‘ä¼¼ã§ãã‚‹ã“ã¨ã‚’ç¤ºã›(ã¾ãŸã¯ã§ããªã„åä¾‹ã‚’ç¤ºã›)ã€‚

**å‚è€ƒ**: Universal Approximation Theorem(NN) / Transformerã®è¡¨ç¾åŠ›[^15]

**ç¾çŠ¶**: æœªè§£æ±ºã€‚å…¥åŠ›ä¾å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(Mamba)ãŒã‚ã‚Œã°å¯èƒ½æ€§é«˜ã„ã€‚å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(S4)ã§ã¯é™ç•Œã‚ã‚Šã€‚
:::

#### è‡ªå·±è©•ä¾¡ãƒ†ã‚¹ãƒˆ(100ç‚¹æº€ç‚¹)

**ç†è«–(50ç‚¹)**:

- [ ] é€£ç¶šSSM $\frac{dh}{dt} = Ah + Bu$ã‚’èª¬æ˜ã§ãã‚‹ (5ç‚¹)
- [ ] ZOHé›¢æ•£åŒ–ã‚’å°å‡ºã§ãã‚‹ (10ç‚¹)
- [ ] SSMã®å†å¸°ãƒ»ç•³ã¿è¾¼ã¿å½¢æ…‹ã‚’å¤‰æ›ã§ãã‚‹ (10ç‚¹)
- [ ] HiPPOã®å‹•æ©Ÿã‚’èª¬æ˜ã§ãã‚‹ (5ç‚¹)
- [ ] S4ã®DPLRåˆ†è§£ã‚’ç†è§£ã—ã¦ã„ã‚‹ (10ç‚¹)
- [ ] Mambaã®Selective SSMã‚’å°å‡ºã§ãã‚‹ (10ç‚¹)

**å®Ÿè£…(30ç‚¹)**:

- [ ] Juliaã§SSMå†å¸°ã‚’å®Ÿè£…ã§ãã‚‹ (5ç‚¹)
- [ ] FFTç•³ã¿è¾¼ã¿ã‚’å®Ÿè£…ã§ãã‚‹ (5ç‚¹)
- [ ] HiPPOåˆæœŸåŒ–ã‚’å®Ÿè£…ã§ãã‚‹ (5ç‚¹)
- [ ] S4 Cauchyã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè£…ã§ãã‚‹ (10ç‚¹)
- [ ] Rustã§Parallel Scanã‚’å®Ÿè£…ã§ãã‚‹ (5ç‚¹)

**å¿œç”¨(20ç‚¹)**:

- [ ] LRAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ (10ç‚¹)
- [ ] SSMã‚’æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«é©ç”¨ã§ãã‚‹ (10ç‚¹)

**60ç‚¹ä»¥ä¸Š**: SSMç†è«–ã‚’ç¿’å¾—ã€‚è«–æ–‡ã‚’èª­ã‚ã‚‹ã€‚
**80ç‚¹ä»¥ä¸Š**: SSMã‚’å®Ÿè£…ã§ãã‚‹ã€‚ç ”ç©¶ã«å¿œç”¨å¯èƒ½ã€‚
**100ç‚¹**: SSMã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã€‚æ–°æ‰‹æ³•ã‚’ææ¡ˆã§ãã‚‹ã€‚

### 10.5 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: SSMã‚’è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§ä½¿ã†

#### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1: æ™‚ç³»åˆ—äºˆæ¸¬(åˆç´š)

**ç›®æ¨™**: æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã§SSMã‚’è¨“ç·´ã—ã€ç¿Œæ—¥ã®ä¾¡æ ¼ã‚’äºˆæ¸¬ã€‚RMSE < 5%, æ–¹å‘çš„ä¸­ç‡ > 55%ã€‚

```julia
using CSV, DataFrames, Flux, Statistics

df = CSV.read("AAPL_1year.csv", DataFrame)
prices = Float32.(df.Close)

# Sliding window
function create_sequences(data, window_size=256)
    X, Y = [], []
    for i in 1:(length(data) - window_size)
        push!(X, data[i:i+window_size-1])
        push!(Y, data[i+window_size])
    end
    hcat(X...)', hcat(Y...)'
end

X, Y = create_sequences(prices)
Î¼, Ïƒ = mean(X), std(X)
X, Y = (X .- Î¼) ./ Ïƒ, (Y .- Î¼) ./ Ïƒ

model = Chain(S4Layer(1, 64, 0.01), S4Layer(64, 64, 0.01),
              S4Layer(64, 64, 0.01), S4Layer(64, 1, 0.01))
loss(x, y) = Flux.mse(model(x), y)

Flux.train!(loss, Flux.params(model), [(X, Y)], Flux.ADAM(0.001))
```

#### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2: é•·æ–‡æ›¸åˆ†é¡(ä¸­ç´š)

**ç›®æ¨™**: 10Kãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã€‚Acc > 85%, Transformer-Baseã¨åŒç­‰ã€‚

```python
from transformers import MambaForSequenceClassification, AutoTokenizer

model = MambaForSequenceClassification.from_pretrained(
    "state-spaces/mamba-370m", num_labels=3)
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")

text = "Apple announces new iPhone with AI features..."
inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=10000)
pred = model(**inputs).logits.argmax(-1)
print(f"Category: {['Politics', 'Sports', 'Tech'][pred]}")
```

#### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ3: å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ äºˆæ¸¬(ä¸Šç´š)

**ç›®æ¨™**: Moving MNIST(64Ã—64Ã—10)ã‹ã‚‰æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’äºˆæ¸¬ã€‚PSNR > 25dB, SSIM > 0.85ã€‚
**å®Ÿè£…ã®éµ**: 4æ–¹å‘ã‚¹ã‚­ãƒ£ãƒ³ã€Spatialä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€Chunk-wiseå‡¦ç†ã€‚

### 10.6 ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ»ãƒªã‚½ãƒ¼ã‚¹

#### Discord/Slackã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£

| ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ | è¨€èª | ç‰¹å¾´ | URL |
|:-------------|:-----|:-----|:----|
| **Hazy Research** | EN | S4/Mambaé–‹ç™ºãƒãƒ¼ãƒ å…¬å¼ | [Link](https://discord.gg/hazyresearch) |
| **EleutherAI** | EN | ã‚ªãƒ¼ãƒ—ãƒ³LLMé–‹ç™ºã€‚SSMè­°è«–æ´»ç™º | [Link](https://discord.gg/eleutherai) |
| **AI Alignment** | EN | SSMå®‰å…¨æ€§ç ”ç©¶ | [Link](https://discord.gg/aialignment) |
| **æ—¥æœ¬èªAIã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£** | JP | SSMæ—¥æœ¬èªæƒ…å ±äº¤æ› | Twitter #SSM_jp |

#### GitHubãƒªãƒã‚¸ãƒˆãƒª(æ³¨ç›®)

| Repo | èª¬æ˜ | Stars |
|:-----|:-----|:------|
| [state-spaces/mamba](https://github.com/state-spaces/mamba) | å…¬å¼å®Ÿè£… | 12K+ |
| [johnma2006/mamba-minimal](https://github.com/johnma2006/mamba-minimal) | æ•™è‚²çš„æœ€å°å®Ÿè£…(300è¡Œ) | 3K+ |
| [huggingface/transformers](https://github.com/huggingface/transformers) | Mambaçµ±åˆæ¸ˆã¿ | 130K+ |
| [mamba-chat](https://github.com/haotian-liu/mamba-chat) | MambaÃ—LLaVA(ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«) | 1K+ |

#### arXiv Followæ¨å¥¨

æ¯é€±æ–°ã—ã„SSMè«–æ–‡ãŒå‡ºã‚‹ã€‚ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§arXiv alertã‚’è¨­å®š:

- "state space model"
- "Mamba"
- "selective SSM"
- "linear RNN"
- "structured state space"

### 10.7 æ¬¡å›äºˆå‘Š: ç¬¬17å› Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³•

ç¬¬17å›ã§ã¯ã€Mambaã®é€²åŒ–ã¨ç·šå½¢RNN/Attentionãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’æ‰±ã†:

- **Mamba-2/SSD**: Attention=SSMåŒå¯¾æ€§ã®å®Œå…¨è¨¼æ˜
- **RWKV**: Receptance Weighted Key Valueã€ç·šå½¢RNN
- **RetNet**: Retentionæ©Ÿæ§‹ã€Multi-scale decay
- **GLA**: Gated Linear Attention
- **Vision Mamba**: VMamba/Vimã®ç”»åƒSSM
- **Hybridè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³**: AttentionÃ—SSMã®çµ„ã¿åˆã‚ã›æ–¹

**åˆ°é”ç‚¹**: ã€ŒSSMã ã‘ã§ååˆ†ã‹ï¼ŸAttentionã‚’æ¨ã¦ãã‚Œãªã„ç†ç”±ã¯ï¼Ÿã€ã¨ã„ã†å•ã„ã«ç­”ãˆã€ç¬¬18å›ã®Hybrid architectureã¸ã®æ©‹ã‚’æ¶ã‘ã‚‹ã€‚

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**:
- SSD (Structured State Space Duality)
- Semi-Separableè¡Œåˆ—
- RWKV Time-mix
- RetNet Multi-scale decay
- Vision Mambaã®2D Selective Scan

**äºˆç¿’æ¨å¥¨è«–æ–‡**:
- Mamba-2: [arXiv:2405.21060](https://arxiv.org/abs/2405.21060)
- RWKV: [arXiv:2305.13048](https://arxiv.org/abs/2305.13048)
- RetNet: [arXiv:2307.08621](https://arxiv.org/abs/2307.08621)

:::message
**é€²æ—: 100% å®Œäº†** ç¬¬16å›SSMç†è«–ã‚’å®Œèµ°ã€‚é€£ç¶šâ†’é›¢æ•£â†’HiPPOâ†’S4â†’Mambaã®å…¨æ—…ç¨‹ã‚’è¸ç ´ã—ãŸã€‚Course IIã‚‚æ®‹ã‚Š2å›ã€‚Mamba-2ã¨Hybridã§ç†è«–ç·¨ã‚’å®Œçµã•ã›ã‚‹ã€‚
:::

---

### 6.15 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **"å¿˜ã‚Œã‚‹"ã“ã¨ã“ãRNNã®æœ¬è³ªçš„é™ç•Œã ã£ãŸã€‚Mambaã¯é¸æŠçš„è¨˜æ†¶ã§ãã‚Œã‚’å…‹æœã—ãŸã€‚ã ãŒå•ã„ãŸã„â€•â€•SSMã ã‘ã§ååˆ†ãªã®ã‹ï¼ŸAttentionã‚’æ¨ã¦ãã‚Œãªã„ç†ç”±ã¯ä½•ã‹ï¼Ÿ**

Mambaã¯é•·è·é›¢ä¾å­˜ã‚’$O(N)$ã§æ‰±ãˆã‚‹ã€‚ã ãŒ**å…¨ç³»åˆ—ã‚’åŒæ™‚ã«å‚ç…§ã™ã‚‹èƒ½åŠ›**(Attentionã®æœ¬è³ª)ã¯æŒãŸãªã„ã€‚Few-shot learningã€æ¨è«–ã‚¿ã‚¹ã‚¯ã€å‹•çš„ãªæ–‡è„ˆåˆ‡ã‚Šæ›¿ãˆã§ã¯ã€AttentionãŒä¾ç„¶ã¨ã—ã¦å„ªä½ã€‚

ç¬¬17å›ã§ã€Mamba-2/SSDãŒã€ŒAttention=SSMã€ã®ç­‰ä¾¡æ€§ã‚’è¨¼æ˜ã—ãŸã“ã¨ã‚’å­¦ã¶ã€‚ã¤ã¾ã‚Š**å¯¾ç«‹ã§ã¯ãªãã€çµ±ä¸€**ã¸å‘ã‹ã£ã¦ã„ã‚‹ã€‚

ç¬¬18å›ã§ã¯ã€Jambaã‚„Zambaã®ã‚ˆã†ã«ã€**Attentionã¨SSMã‚’çµ„ã¿åˆã‚ã›ãŸHybrid**ãŒã€Œæœ€å¼·ã€ã§ã¯ãªãã€Œæœ€é©ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

**å•ã„ç¶šã‘ã‚ˆ**: "æœ€å¼·"ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å­˜åœ¨ã—ãªã„ã€‚ã‚¿ã‚¹ã‚¯ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»è¨ˆç®—è³‡æºã«å¿œã˜ã¦ã€çµ„ã¿åˆã‚ã›ã‚‹ã€‚ãã‚ŒãŒã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æœ¬è³ªã§ã¯ãªã„ã‹ï¼Ÿ

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Gu, A., Dao, T., Ermon, S., Rudra, A., & RÃ©, C. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. *NeurIPS 2020*.
@[card](https://arxiv.org/abs/2008.07669)

[^2]: Gu, A., Goel, K., & RÃ©, C. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. *ICLR 2022*.
@[card](https://arxiv.org/abs/2111.00396)

[^3]: Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. *arXiv:2312.00752*.
@[card](https://arxiv.org/abs/2312.00752)

[^4]: Kalman, R. E. (1960). A New Approach to Linear Filtering and Prediction Problems. *Journal of Basic Engineering*.

[^5]: Tay, Y., Dehghani, M., Abnar, S., et al. (2021). Long Range Arena: A Benchmark for Efficient Transformers. *ICLR 2021*.
@[card](https://arxiv.org/abs/2011.04006)

[^6]: Gu, A., & Dao, T. (2023). Mamba Official Repository.
@[card](https://github.com/state-spaces/mamba)

[^7]: Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. *ICML 2024*.
@[card](https://arxiv.org/abs/2405.21060)

[^8]: Liu, Y., Tian, Y., Zhao, Y., et al. (2024). VMamba: Visual State Space Models.
@[card](https://arxiv.org/abs/2401.10166)

[^9]: Peng, B., Alcaide, E., Anthony, Q., et al. (2023). RWKV: Reinventing RNNs for the Transformer Era.
@[card](https://arxiv.org/abs/2305.13048)

[^10]: Sun, Y., Dong, L., Huang, S., et al. (2023). Retentive Network: A Successor to Transformer for Large Language Models.
@[card](https://arxiv.org/abs/2307.08621)

[^11]: Somvanshi, S., Islam, Md M., et al. (2025). From S4 to Mamba: A Comprehensive Survey on Structured State Space Models. *arXiv:2503.18970*.
@[card](https://arxiv.org/abs/2503.18970)

### æ•™ç§‘æ›¸

- Ogata, K. (2009). *Modern Control Engineering* (5th ed.). Prentice Hall. [åˆ¶å¾¡ç†è«–ã®å¤å…¸]
- Chen, C.-T. (1998). *Linear System Theory and Design* (3rd ed.). Oxford University Press. [çŠ¶æ…‹ç©ºé–“ã®æ•°å­¦]
- Rush, A. (2023). *The Annotated S4*. [å®Ÿè£…ä»˜ãè§£èª¬]
  @[card](https://srush.github.io/annotated-s4/)

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | æ–‡è„ˆ |
|:-----|:-----|:-----|
| $u_t, u(t)$ | å…¥åŠ›ä¿¡å· | é›¢æ•£/é€£ç¶šæ™‚é–“ |
| $h_t, h(t)$ | éš ã‚ŒçŠ¶æ…‹ | é›¢æ•£/é€£ç¶šæ™‚é–“ |
| $y_t, y(t)$ | å‡ºåŠ›ä¿¡å· | é›¢æ•£/é€£ç¶šæ™‚é–“ |
| $A, B, C, D$ | SSMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | é€£ç¶šæ™‚é–“ |
| $\bar{A}, \bar{B}$ | é›¢æ•£åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | é›¢æ•£æ™‚é–“ |
| $\bar{\mathcal{K}}$ | SSMç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ« | é›¢æ•£æ™‚é–“ |
| $\Delta$ | æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—å¹… | é›¢æ•£åŒ– |
| $\Lambda$ | å¯¾è§’è¡Œåˆ—(å›ºæœ‰å€¤) | S4 |
| $P, Q$ | ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ— | S4 DPLR |
| $\Delta_t, B_t, C_t$ | å…¥åŠ›ä¾å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | Mamba |
| $d$ | çŠ¶æ…‹æ¬¡å…ƒ | SSM |
| $L, N$ | ç³»åˆ—é•· | ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ |
| $D$ | ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒ | ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ |
