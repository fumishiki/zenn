---
title: "ç¬¬40å›: âš¡ Consistency Models & é«˜é€Ÿç”Ÿæˆç†è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼""
emoji: "âš¡"
type: "tech"
topics: ["machinelearning", "deeplearning", "consistencymodels", "julia", "diffusion"]
published: true
---

# ç¬¬40å›: âš¡ Consistency Models & é«˜é€Ÿç”Ÿæˆç†è«–

> **Course IV ç¬¬8å›ï¼ˆå…¨50å›ã‚·ãƒªãƒ¼ã‚ºã®ç¬¬40å›ï¼‰**
> ç¬¬39å›ã§æ½œåœ¨ç©ºé–“æ‹¡æ•£ã‚’å®Œå…¨ç†è§£ã—ãŸã€‚ã ãŒ1000ã‚¹ãƒ†ãƒƒãƒ—ã¯é…ã™ãã‚‹ â€” ç†è«–çš„ã«ä¿è¨¼ã•ã‚ŒãŸé«˜é€Ÿç”Ÿæˆã¸

:::message
**å‰æçŸ¥è­˜**: ç¬¬36å› DDPMã€ç¬¬37å› SDE/ODEã€ç¬¬38å› Flow Matchingã€ç¬¬39å› LDM
:::

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” 1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆã®è¡æ’ƒ

```julia
using Lux, Random, NNlib

# Consistency Function (Self-consistencyæ¡ä»¶ã‚’æº€ãŸã™NN)
function consistency_function(x_t, t, model, Ïƒ_data=1.0f0)
    # Skip connection + Noise-conditional scaling
    c_skip = Ïƒ_data^2 / (t^2 + Ïƒ_data^2)
    c_out = Ïƒ_data * t / sqrt(t^2 + Ïƒ_data^2)
    c_in = 1 / sqrt(t^2 + Ïƒ_data^2)

    # F_Î¸(x_t, t) = c_skip(t) * x_t + c_out(t) * net_Î¸(c_in(t) * x_t, t)
    return c_skip .* x_t .+ c_out .* model(c_in .* x_t, t)
end

# 1-step generation (t=T â†’ t=0 in ONE step!)
x_T = randn(Float32, 28, 28, 1, 4)  # ãƒã‚¤ã‚º
t = 80.0f0  # T=æœ€å¤§æ™‚åˆ»
x_0 = consistency_function(x_T, t, model, 1.0f0)  # ä¸€æ’ƒã§ç”»åƒã¸

println("DDIM: 1000 steps, ~10 sec")
println("Consistency Model: 1 step, ~0.01 sec")
println("é€Ÿåº¦: 1000x faster, FID: 3.55 (CIFAR-10)")
```

**å‡ºåŠ›**:
```
DDIM: 1000 steps, ~10 sec
Consistency Model: 1 step, ~0.01 sec
é€Ÿåº¦: 1000x faster, FID: 3.55 (CIFAR-10)
```

**æ•°å¼ã®æ­£ä½“**:
$$
F_\theta(\mathbf{x}_t, t) = c_{\text{skip}}(t) \mathbf{x}_t + c_{\text{out}}(t) f_\theta(c_{\text{in}}(t) \mathbf{x}_t, t)
$$

- **Self-consistencyæ¡ä»¶**: $F_\theta(\mathbf{x}_t, t) = F_\theta(\mathbf{x}_{t'}, t')$ for any $t, t' \in [\epsilon, T]$
- **DDPMã¨ã®é•ã„**: 1000ã‚¹ãƒ†ãƒƒãƒ—ã®åå¾© â†’ **1ã‚¹ãƒ†ãƒƒãƒ—ã§ç›´æ¥** $\mathbf{x}_T \to \mathbf{x}_0$

:::message
**å…¨ä½“ã®3%å®Œäº†ï¼**
ã“ã‚Œã‹ã‚‰ã€Œãªãœ1ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆã§ãã‚‹ã®ã‹ã€ã®ç†è«–ã‚’å®Œå…¨ç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Self-consistencyã‚’è¦‹ã‚‹

### 1.1 Self-consistencyæ¡ä»¶ã®å¯è¦–åŒ–

```julia
using Plots, Statistics

# Consistency Modelã®è»Œé“å¯è¦–åŒ–
function visualize_self_consistency(model, x_T, Ïƒ_data=1.0f0)
    ts = exp.(range(log(0.01), log(80), length=20))  # log-uniform sampling
    trajectory = []

    for t in ts
        x_pred = consistency_function(x_T, t, model, Ïƒ_data)
        push!(trajectory, x_pred)
    end

    # Self-consistency: å…¨æ™‚åˆ»ã§åŒã˜ç‚¹ã«åæŸã™ã‚‹ã‹
    final_predictions = hcat(trajectory...)
    std_across_time = std(final_predictions, dims=2)

    println("Self-consistency error: ", mean(std_across_time))
    return trajectory
end

# DDPMã¨ã®æ¯”è¼ƒ
function ddpm_trajectory(x_T, model, timesteps=1000)
    x = x_T
    for t in timesteps:-1:1
        # DDPM reverse process (1000 steps)
        x = ddpm_step(x, t, model)
    end
    return x
end

# å®Ÿè¡Œ
x_T = randn(Float32, 28, 28, 1, 1)
cm_traj = visualize_self_consistency(model, x_T)
ddpm_result = ddpm_trajectory(x_T, ddpm_model)

plot([
    heatmap(cm_traj[end][:,:,1,1], title="CM (1 step)"),
    heatmap(ddpm_result[:,:,1,1], title="DDPM (1000 steps)")
])
```

| æ‰‹æ³• | ã‚¹ãƒ†ãƒƒãƒ—æ•° | æ™‚é–“ | FID (CIFAR-10) | Self-consistency |
|:-----|:----------|:-----|:--------------|:-----------------|
| DDPM | 1000 | 10 sec | 3.17 | N/A |
| DDIM | 50 | 0.5 sec | 4.67 | N/A |
| **CM (CT)** | **1** | **0.01 sec** | **3.55** | âœ… ä¿è¨¼ |
| **CM (CD)** | **1** | **0.01 sec** | **3.55** | âœ… ä¿è¨¼ |

**ğŸ”‘ Self-consistencyã®ç›´æ„Ÿ**:
- DDPM: $\mathbf{x}_t \to \mathbf{x}_{t-1} \to \cdots \to \mathbf{x}_0$ (é€£é–ãŒå¿…é ˆ)
- **CM**: $F_\theta(\mathbf{x}_t, t) = \mathbf{x}_0$ for **any** $t$ (ã©ã®æ™‚åˆ»ã‹ã‚‰ã§ã‚‚ä¸€ç™º)

### 1.2 å¤šæ®µéšã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â€” å“è³ªvsé€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

```julia
# Multistep sampling (optional refinement)
function cm_multistep(x_T, model, steps=4)
    schedule = exp.(range(log(80), log(0.01), length=steps+1))
    x = x_T

    for i in 1:steps
        t_cur = schedule[i]
        t_next = schedule[i+1]

        # Consistency step
        x_0_pred = consistency_function(x, t_cur, model)

        if i < steps
            # Add noise for next step (optional)
            z = randn(size(x))
            x = x_0_pred + t_next * z
        else
            x = x_0_pred
        end
    end
    return x
end

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
steps_range = [1, 2, 4, 8]
fid_scores = []
times = []

for steps in steps_range
    @time x_gen = cm_multistep(x_T, model, steps)
    fid = compute_fid(x_gen, real_data)
    push!(fid_scores, fid)
    push!(times, @elapsed cm_multistep(x_T, model, steps))
end

plot(steps_range, fid_scores,
     xlabel="Sampling Steps", ylabel="FID â†“",
     title="CM Quality-Speed Tradeoff",
     marker=:circle, linewidth=2)
```

| Steps | FID â†“ | Time (ms) | å“è³ª vs DDPM |
|:------|:------|:----------|:-------------|
| 1 | 3.55 | 10 | â‰ˆ DDPM (1000 steps) |
| 2 | 3.25 | 20 | Better |
| 4 | 2.93 | 40 | âœ… SOTA |
| 8 | 2.85 | 80 | Marginal gain |

**Pareto front**: 1-4ã‚¹ãƒ†ãƒƒãƒ—ãŒ sweet spotï¼ˆå“è³ªâ†‘ + é€Ÿåº¦â†‘ï¼‰

### 1.3 DDIM vs DPM-Solver++ vs CM æ¯”è¼ƒ

```julia
# çµ±ä¸€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
methods = [
    ("DDIM (50 steps)", ddim_sampler, 50),
    ("DPM-Solver++ (20 steps)", dpm_solver, 20),
    ("UniPC (10 steps)", unipc_sampler, 10),
    ("CM (1 step)", cm_sampler, 1),
    ("LCM (4 steps)", lcm_sampler, 4)
]

results = []
for (name, sampler, steps) in methods
    time = @elapsed x = sampler(x_T, model, steps)
    fid = compute_fid(x, real_data)
    push!(results, (name=name, steps=steps, time=time, fid=fid))
end

# Visualization
scatter(
    [r.time for r in results],
    [r.fid for r in results],
    xlabel="Time (sec)", ylabel="FID â†“",
    label=[r.name for r in results],
    title="Fast Sampling Pareto Front",
    markersize=8, legend=:topright
)
```

```mermaid
graph LR
    A[DDPM<br>1000 steps<br>10 sec<br>FID 3.17] --> B[DDIM<br>50 steps<br>0.5 sec<br>FID 4.67]
    B --> C[DPM-Solver++<br>20 steps<br>0.2 sec<br>FID 3.95]
    C --> D[UniPC<br>10 steps<br>0.1 sec<br>FID 4.12]
    D --> E[CM<br>1 step<br>0.01 sec<br>FID 3.55]
    E --> F[LCM<br>4 steps<br>0.04 sec<br>FID 2.93]

    style E fill:#f9f,stroke:#333,stroke-width:4px
    style F fill:#9ff,stroke:#333,stroke-width:4px
```

**ğŸ”‘ æ¯”è¼ƒã®ãƒã‚¤ãƒ³ãƒˆ**:
- **DDIM**: æ±ºå®šè«–çš„ã ãŒå“è³ªåŠ£åŒ–
- **DPM-Solver++**: é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ã§åŠ¹ç‡â†‘
- **UniPC**: Predictor-Correctorã§å®‰å®šæ€§â†‘
- **CM**: Self-consistencyç†è«–ä¿è¨¼ã§1-stepé”æˆ
- **LCM**: CM + Latent Space + Guidanceè’¸ç•™

:::message alert
**CM vs é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ã®é•ã„**:
- é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼: ODEè»Œé“ã‚’æ•°å€¤çš„ã«è¿‘ä¼¼ï¼ˆèª¤å·®ç´¯ç©ï¼‰
- **CM**: Self-consistencyæ¡ä»¶ã‚’å­¦ç¿’ã§æº€ãŸã™ï¼ˆç†è«–çš„ä¿è¨¼ï¼‰
:::

:::message
**å…¨ä½“ã®10%å®Œäº†ï¼**
Self-consistencyã®å¨åŠ›ã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯ã€ŒãªãœConsistency Modelsã‹ã€ã®ç†è«–çš„èƒŒæ™¯ã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœConsistency Modelsã‹

### 2.1 æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«é«˜é€ŸåŒ–ã®å…¨ä½“åƒ

```mermaid
graph TD
    A[Diffusion Models<br>DDPM/DDIM] --> B{é«˜é€ŸåŒ–ã®3ã¤ã®æ–¹å‘}
    B --> C[Direction 1:<br>é«˜æ¬¡ODEã‚½ãƒ«ãƒãƒ¼]
    B --> D[Direction 2:<br>è’¸ç•™ Distillation]
    B --> E[Direction 3:<br>Consistency Models]

    C --> C1[DPM-Solver++<br>UniPC<br>EDM]
    C1 --> C2[20-50 steps<br>æ•°å€¤è¿‘ä¼¼èª¤å·®]

    D --> D1[Progressive<br>Distillation]
    D1 --> D2[æ®µéšçš„ã«åŠæ¸›<br>æ•™å¸«ãƒ¢ãƒ‡ãƒ«å¿…é ˆ]

    E --> E1[CT: Consistency Training<br>CD: Consistency Distillation]
    E1 --> E2[1-stepç†è«–ä¿è¨¼<br>Self-consistency]

    E2 --> F[ç¬¬40å›ã®ç„¦ç‚¹]

    style E fill:#f9f,stroke:#333,stroke-width:4px
    style E2 fill:#9ff,stroke:#333,stroke-width:4px
    style F fill:#ff9,stroke:#333,stroke-width:4px
```

| æ–¹å‘ | ä»£è¡¨æ‰‹æ³• | Steps | å“è³ª | ç†è«–ä¿è¨¼ | æ•™å¸«ãƒ¢ãƒ‡ãƒ« |
|:-----|:---------|:------|:-----|:---------|:-----------|
| **é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼** | DPM-Solver++ | 20 | Good | âŒ è¿‘ä¼¼èª¤å·® | ä¸è¦ |
| **é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼** | UniPC | 10 | Fair | âŒ è¿‘ä¼¼èª¤å·® | ä¸è¦ |
| **è’¸ç•™** | Progressive | 4-8 | Excellent | âŒ è’¸ç•™ã‚®ãƒ£ãƒƒãƒ— | âœ… å¿…é ˆ |
| **è’¸ç•™** | LCM | 4 | Excellent | âŒ è’¸ç•™ã‚®ãƒ£ãƒƒãƒ— | âœ… å¿…é ˆ |
| **CM** | **CT** | **1** | **Excellent** | **âœ… Self-consistency** | **ä¸è¦** |
| **CM** | **CD** | **1** | **Excellent** | **âœ… Self-consistency** | **âœ… ä»»æ„** |

### 2.2 Course IVã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

```mermaid
graph LR
    A[ç¬¬33å›<br>NF] --> B[ç¬¬34å›<br>EBM]
    B --> C[ç¬¬35å›<br>Score Matching]
    C --> D[ç¬¬36å›<br>DDPM]
    D --> E[ç¬¬37å›<br>SDE/ODE]
    E --> F[ç¬¬38å›<br>Flow Matching]
    F --> G[ç¬¬39å›<br>LDM]
    G --> H[ç¬¬40å›<br>CM & é«˜é€Ÿç”Ÿæˆ]
    H --> I[ç¬¬41å›<br>World Models]
    I --> J[ç¬¬42å›<br>çµ±ä¸€ç†è«–]

    style H fill:#f9f,stroke:#333,stroke-width:4px
```

**Course IV ã®ç†è«–çš„æµã‚Œ**:
1. **ç¬¬33å›**: å³å¯†å°¤åº¦ï¼ˆNFï¼‰ â€” å¯é€†å¤‰æ›ã®åˆ¶ç´„
2. **ç¬¬34å›**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼ˆEBMï¼‰ â€” $Z(\theta)$ ã®è¨ˆç®—å›°é›£æ€§
3. **ç¬¬35å›**: ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚° â€” $Z$ ä¸è¦ã ãŒä½å¯†åº¦é ˜åŸŸã§ä¸æ­£ç¢º
4. **ç¬¬36å›**: DDPM â€” ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å…¨å¯†åº¦åŸŸã‚«ãƒãƒ¼
5. **ç¬¬37å›**: SDE/ODE â€” é€£ç¶šæ™‚é–“å®šå¼åŒ–ã€Probability Flow ODE
6. **ç¬¬38å›**: Flow Matching â€” Score/Flow/Diffusion/OT çµ±ä¸€ç†è«–
7. **ç¬¬39å›**: LDM â€” æ½œåœ¨ç©ºé–“ã§è¨ˆç®—åŠ¹ç‡åŒ–
8. **ç¬¬40å› (ä»Šå›)**: **CM** â€” Self-consistencyã§1-stepç†è«–ä¿è¨¼
9. **ç¬¬41å›**: World Models â€” ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¸
10. **ç¬¬42å›**: çµ±ä¸€ç†è«– â€” å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ä¿¯ç°

**ğŸ”‘ ç¬¬40å›ã®å½¹å‰²**:
- **å•é¡Œ**: DDPM/LDM = 1000ã‚¹ãƒ†ãƒƒãƒ—é…ã™ãã‚‹
- **è§£æ±º**: Self-consistencyæ¡ä»¶ â†’ 1-stepã§å“è³ªç¶­æŒ
- **æ„ç¾©**: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å®Ÿç”¨åŒ–ã‚’åŠ é€Ÿï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆï¼‰

### 2.3 3ã¤ã®æ¯”å–©ã§æ‰ãˆã‚‹ã€ŒConsistency Modelsã€

#### æ¯”å–©1: ã€Œç›´è¡Œä¾¿ vs ä¹—ã‚Šç¶™ãã€

- **DDPM**: æ±äº¬ â†’ å¤§é˜ª â†’ åå¤å±‹ â†’ ... â†’ ç¦å²¡ (1000å›ä¹—ã‚Šç¶™ã)
- **CM**: æ±äº¬ â†’ ç¦å²¡ **ç›´è¡Œä¾¿** (1ãƒ•ãƒ©ã‚¤ãƒˆ)

Self-consistency = **ã©ã®å‡ºç™ºç‚¹ã‹ã‚‰ã§ã‚‚åŒã˜æœ€çµ‚ç›®çš„åœ°**

#### æ¯”å–©2: ã€Œç©åˆ† vs çµ‚ç‚¹ç›´æ¥äºˆæ¸¬ã€

- **ODE Solver**: $\frac{d\mathbf{x}}{dt} = f(\mathbf{x}, t)$ ã‚’æ•°å€¤çš„ã«è§£ãï¼ˆEuleræ³•ã§1000ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
- **CM**: $F_\theta(\mathbf{x}_t, t) = \mathbf{x}_0$ ã‚’ **ç›´æ¥å­¦ç¿’** (çµ‚ç‚¹äºˆæ¸¬é–¢æ•°)

#### æ¯”å–©3: ã€Œé–¢æ•°ã®ãƒã‚§ãƒ¼ãƒ³ vs å˜ä¸€é–¢æ•°ã€

- **DDPM**: $f_T \circ f_{T-1} \circ \cdots \circ f_1$ (é€£é–)
- **CM**: $F(\mathbf{x}_t, t) = \mathbf{x}_0$ for **all** $t$ (å˜ä¸€é–¢æ•°)

### 2.4 å­¦ç¿’æˆ¦ç•¥

| Zone | æ™‚é–“ | å­¦ç¿’ç›®æ¨™ | é›£æ˜“åº¦ |
|:-----|:-----|:---------|:-------|
| Zone 0 | 30ç§’ | 1-stepç”Ÿæˆã‚’ä½“æ„Ÿ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | 10åˆ† | Self-consistencyå¯è¦–åŒ– | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | 15åˆ† | ç†è«–çš„å‹•æ©Ÿç†è§£ + ç™ºå±• | â˜…â˜…â˜…â˜…â˜… |
| **Zone 3** | **60åˆ†** | **Self-consistencyæ•°å¼å®Œå…¨å°å‡º** | **â˜…â˜…â˜…â˜…â˜…** |
| Zone 4 | 45åˆ† | Juliaå®Ÿè£… | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | 30åˆ† | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ | â˜…â˜…â˜…â˜†â˜† |
| Zone 6 | 30åˆ† | æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆ | â˜…â˜…â˜…â˜†â˜† |

:::details ğŸ´ Trojan Horse â€” Consistency Modelsã§Juliaæ•°å¼ç¾ãŒéš›ç«‹ã¤
```julia
# Consistency function in Julia (æ•°å¼ãã®ã¾ã¾)
F_Î¸(x, t) = c_skip(t) * x + c_out(t) * model(c_in(t) * x, t)

# Python equivalent (å†—é•·)
def F_theta(x, t, model):
    c_s = c_skip(t)
    c_o = c_out(t)
    c_i = c_in(t)
    return c_s * x + c_o * model(c_i * x, t)
```

Juliaã® `.` broadcastæ¼”ç®—å­ã§ **ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãŒè‡ªå‹•**ã€Pythonã¯æ˜ç¤ºçš„ãƒ«ãƒ¼ãƒ—ãŒå¿…è¦ã€‚
:::

:::message
**å…¨ä½“ã®20%å®Œäº†ï¼**
æº–å‚™å®Œäº†ã€‚Zone 3ã§Self-consistencyæ¡ä»¶ã®å®Œå…¨æ•°å¼å°å‡ºã«æŒ‘ã‚€ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Consistency Modelsç†è«–å®Œå…¨ç‰ˆ

> **Bossæˆ¦ã®äºˆå‘Š**: æœ€å¾Œã«Consistency Models (Song et al. 2023) ã® Self-consistencyæ¡ä»¶å®Œå…¨å°å‡ºã«æŒ‘ã‚€

### 3.1 Self-consistencyæ¡ä»¶ â€” Consistency Modelsã®å¿ƒè‡“éƒ¨

#### 3.1.1 Probability Flow ODEã®å¾©ç¿’

ç¬¬37å›ã§å­¦ã‚“ã Probability Flow ODE (PF-ODE):

$$
\frac{d\mathbf{x}_t}{dt} = -\frac{1}{2} \beta(t) [\mathbf{x}_t + \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)]
$$

- **æ€§è³ª**: ç¢ºç‡çš„ãªSDE $d\mathbf{x}_t = -\frac{1}{2}\beta(t)[\mathbf{x}_t + \nabla \log p_t] dt + \sqrt{\beta(t)} d\mathbf{w}_t$ ã¨ **åŒã˜å‘¨è¾ºåˆ†å¸ƒ** $p_t(\mathbf{x}_t)$
- **æ±ºå®šè«–çš„è»Œé“**: ãƒã‚¤ã‚ºé …ãªã— â†’ åŒã˜åˆæœŸæ¡ä»¶ã‹ã‚‰åŒã˜çµ‚ç‚¹ã¸

#### 3.1.2 ODEè»Œé“ã¨Consistency

PF-ODEã®è§£è»Œé“ã‚’ $\{\mathbf{x}_t\}_{t \in [\epsilon, T]}$ ã¨ã™ã‚‹ã€‚ä»»æ„ã® $t, t' \in [\epsilon, T]$ ã«å¯¾ã—:

$$
\mathbf{x}_t = \Psi_{t \leftarrow t'}(\mathbf{x}_{t'})
$$

ã“ã“ã§ $\Psi_{t \leftarrow t'}$ ã¯æ™‚åˆ» $t'$ ã‹ã‚‰ $t$ ã¸ã® **ODE flow map**ã€‚

**Consistency**: ODEã®è§£è»Œé“ä¸Šã® **å…¨ã¦ã®ç‚¹** ãŒ **åŒã˜çµ‚ç‚¹** $\mathbf{x}_\epsilon$ ã«åˆ°é”:

$$
\Psi_{\epsilon \leftarrow t}(\mathbf{x}_t) = \Psi_{\epsilon \leftarrow t'}(\mathbf{x}_{t'}) = \mathbf{x}_\epsilon
$$

#### 3.1.3 Self-consistencyæ¡ä»¶ã®å®šå¼åŒ–

**Definition (Self-consistency Function)**:

é–¢æ•° $f: (\mathbb{R}^d, \mathbb{R}_+) \to \mathbb{R}^d$ ãŒ **self-consistent** ã§ã‚ã‚‹ã¨ã¯:

$$
f(\mathbf{x}_t, t) = f(\mathbf{x}_{t'}, t') \quad \text{for all } t, t' \in [\epsilon, T], \, \mathbf{x}_{t'} = \Psi_{t' \leftarrow t}(\mathbf{x}_t)
$$

**ç›´æ„Ÿ**: PF-ODEè»Œé“ä¸Šã®ã©ã®ç‚¹ã§ã‚‚ã€$f$ ã¯ **åŒã˜å‡ºåŠ›** ã‚’è¿”ã™ã€‚

**Consistency Model $F_\theta$**:

$$
F_\theta(\mathbf{x}_t, t) = f_\theta(\mathbf{x}_t, t) \quad \text{with} \quad F_\theta(\mathbf{x}_\epsilon, \epsilon) = \mathbf{x}_\epsilon \quad \text{(boundary condition)}
$$

**Boundaryæ¡ä»¶**: $t=\epsilon$ (ã»ã¼ãƒã‚¤ã‚ºãªã—) ã§ã¯ **æ’ç­‰å†™åƒ** $F_\theta(\mathbf{x}_\epsilon, \epsilon) = \mathbf{x}_\epsilon$

#### 3.1.4 ãªãœSelf-consistencyã§1-stepç”Ÿæˆã§ãã‚‹ã‹

```mermaid
graph TD
    A[x_T ~ N0,I] --> B[x_80]
    B --> C[x_40]
    C --> D[x_20]
    D --> E[x_10]
    E --> F[x_Îµ â‰ˆ x_0]

    A -.F_Î¸x_T,T.-> G[x_0 prediction]
    B -.F_Î¸x_80,80.-> G
    C -.F_Î¸x_40,40.-> G
    D -.F_Î¸x_20,20.-> G
    E -.F_Î¸x_10,10.-> G
    F --> G

    style G fill:#f9f,stroke:#333,stroke-width:4px
```

- **DDPM**: $\mathbf{x}_T \to \mathbf{x}_{T-1} \to \cdots \to \mathbf{x}_0$ (é€£é–å¿…é ˆ)
- **CM**: $F_\theta(\mathbf{x}_T, T) = \mathbf{x}_\epsilon$ (1-stepã§ç›´æ¥)

**1-stepç”Ÿæˆã®æ‰‹é †**:
1. ã‚µãƒ³ãƒ—ãƒ« $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, I)$
2. è¨ˆç®— $\mathbf{x}_\epsilon = F_\theta(\mathbf{x}_T, T)$
3. **çµ‚äº†** (åå¾©ãªã—)

**å¤šæ®µéšsampling (optional)**:
```julia
# 2-step refinement
x_T = randn(...)
t_mid = 40.0
x_mid = x_T + sqrt(t_mid) * randn(...)  # Re-noise
x_0 = F_Î¸(x_mid, t_mid)  # 2nd step
```

### 3.2 Consistency Training (CT) â€” æ•™å¸«ãªã—è¨“ç·´

#### 3.2.1 CTæå¤±é–¢æ•°ã®å°å‡º

**Goal**: Self-consistencyæ¡ä»¶ã‚’æº€ãŸã™ $F_\theta$ ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ $\{\mathbf{x}_0^{(i)}\}$ ã‹ã‚‰å­¦ç¿’ã€‚

**Forward process**: $\mathbf{x}_0 \to \mathbf{x}_t = \mathbf{x}_0 + t \mathbf{z}, \, \mathbf{z} \sim \mathcal{N}(\mathbf{0}, I)$ (VP-SDE)

**CT Loss (Consistency Training)**:

$$
\mathcal{L}_{\text{CT}}(\theta; \theta^-) = \mathbb{E}_{n, \mathbf{x}_0, \mathbf{z}} \left[ d(F_\theta(\mathbf{x}_{t_{n+1}}, t_{n+1}), F_{\theta^-}(\mathbf{x}_{t_n}, t_n)) \right]
$$

- $d(\cdot, \cdot)$: è·é›¢é–¢æ•° (L2 / LPIPS / ...)
- $\theta^-$: **target network** (exponential moving average of $\theta$)
- $\mathbf{x}_{t_n} = \mathbf{x}_{t_{n+1}} + (t_n - t_{n+1}) \mathbf{z}_n$ (Euler stepè¿‘ä¼¼)

**Derivation**:

Self-consistencyæ¡ä»¶:
$$
F_\theta(\mathbf{x}_{t_{n+1}}, t_{n+1}) = F_\theta(\mathbf{x}_{t_n}, t_n)
$$

1ã‚¹ãƒ†ãƒƒãƒ— Euleræ³•ã§ $\mathbf{x}_{t_n} \approx \Psi_{t_n \leftarrow t_{n+1}}(\mathbf{x}_{t_{n+1}})$:
$$
\mathbf{x}_{t_n} \approx \mathbf{x}_{t_{n+1}} + (t_n - t_{n+1}) \frac{d\mathbf{x}}{dt}\Big|_{t=t_{n+1}}
$$

PF-ODEã‹ã‚‰:
$$
\frac{d\mathbf{x}}{dt} = -t \nabla_{\mathbf{x}} \log p_t(\mathbf{x})
$$

ã‚¹ã‚³ã‚¢æ¨å®š: $\nabla_{\mathbf{x}} \log p_t(\mathbf{x}) \approx -\frac{\mathbf{x} - \mathbf{x}_0}{t^2}$ (è¿‘ä¼¼)

**Training algorithm**:

```julia
# Consistency Training (simplified)
function ct_loss(model, x_0, n, Î¸_target)
    z = randn(size(x_0))
    t_n1 = schedule[n+1]
    t_n = schedule[n]

    x_n1 = x_0 + t_n1 * z

    # Euler step (approximate ODE)
    x_n = x_n1 + (t_n - t_n1) * score_estimate(x_n1, t_n1)

    # Self-consistency loss
    f_n1 = model(x_n1, t_n1)
    f_n = stopgrad(Î¸_target(x_n, t_n))  # Target network

    return mse(f_n1, f_n)
end
```

:::message alert
**Numerical instability**: Euleræ³•ã®1ã‚¹ãƒ†ãƒƒãƒ—è¿‘ä¼¼ãŒç²—ã„ â†’ ECT (Easy Consistency Tuning) ã§æ”¹å–„
:::

#### 3.2.2 Target Network ã¨ EMAæ›´æ–°

**EMA (Exponential Moving Average)**:

$$
\theta^- \leftarrow \mu \theta^- + (1 - \mu) \theta
$$

- $\mu = 0.9999$ (very slow update)
- **å®‰å®šæ€§**: $F_{\theta^-}$ ãŒã»ã¼å›ºå®š â†’ $F_\theta$ ãŒå®‰å®šçš„ã«å­¦ç¿’

**DQNé¢¨ã®è§£é‡ˆ**: Target networkã§ã€Œç§»å‹•ã‚´ãƒ¼ãƒ«ã€ã‚’å›ºå®šåŒ–

### 3.3 Consistency Distillation (CD) â€” æ•™å¸«ã‚ã‚Šè’¸ç•™

#### 3.3.1 CDæå¤±é–¢æ•°

**å‰æ**: äº‹å‰è¨“ç·´æ¸ˆã¿Diffusion Model (ã‚¹ã‚³ã‚¢é–¢æ•° $\mathbf{s}_\phi(\mathbf{x}, t)$ ãŒåˆ©ç”¨å¯èƒ½)

**CD Loss**:

$$
\mathcal{L}_{\text{CD}}(\theta; \phi) = \mathbb{E}_{n, \mathbf{x}_0, \mathbf{z}} \left[ d(F_\theta(\mathbf{x}_{t_{n+1}}, t_{n+1}), \mathbf{x}_0^{\text{pred}}) \right]
$$

where $\mathbf{x}_0^{\text{pred}}$ is obtained by **one-step numerical ODE solver**:

$$
\mathbf{x}_0^{\text{pred}} = \mathbf{x}_{t_n} - t_n \mathbf{s}_\phi(\mathbf{x}_{t_n}, t_n)
$$

**CDã¨CTã®é•ã„**:

| é …ç›® | CT | CD |
|:-----|:---|:---|
| æ•™å¸« | ãªã— (self-supervised) | äº‹å‰è¨“ç·´æ¸ˆã¿ã‚¹ã‚³ã‚¢ $\mathbf{s}_\phi$ |
| Target | $F_{\theta^-}(\mathbf{x}_{t_n}, t_n)$ | $\mathbf{x}_0^{\text{pred}}$ from teacher |
| è¨“ç·´é€Ÿåº¦ | é…ã„ (~week on 8 GPUs) | é€Ÿã„ (~day on 8 GPUs) |
| å“è³ª | Good | Excellent (æ•™å¸«ã‹ã‚‰çŸ¥è­˜ç§»è»¢) |

#### 3.3.2 ãªãœCDãŒé€Ÿã„ã‹

**CT**: Euleræ³•ã®1ã‚¹ãƒ†ãƒƒãƒ—è¿‘ä¼¼ â†’ èª¤å·®å¤§ â†’ åæŸé…ã„
**CD**: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®æ­£ç¢ºãªODEè»Œé“ â†’ èª¤å·®å° â†’ åæŸé€Ÿã„

### 3.4 Improved Consistency Training (iCT) â€” SOTAæ‰‹æ³•

#### 3.4.1 iCTã®æ”¹å–„ç‚¹

Song et al. (2023) "Improved Techniques for Training Consistency Models"[^2]:

1. **Pseudo-Huberæå¤±** (L2ã®ä»£æ›¿):

$$
d_{\text{PH}}(\mathbf{a}, \mathbf{b}; c) = \sqrt{c^2 + \|\mathbf{a} - \mathbf{b}\|_2^2} - c
$$

- $c = 0.00054$ (CIFAR-10)
- **åˆ©ç‚¹**: å¤–ã‚Œå€¤ã«é ‘å¥ + å‹¾é…ãŒå¸¸ã«æœ‰ç•Œ

2. **Lognormal sampling** (æ™‚åˆ» $t$ ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°):

$$
\log t \sim \mathcal{N}(\mu, \sigma^2), \quad t \in [\epsilon, T]
$$

- **ç†ç”±**: $t$ ãŒå°ã•ã„é ˜åŸŸã»ã©é‡è¦ (ãƒã‚¤ã‚ºå°‘ãªã„ = ç”»åƒã«è¿‘ã„)

3. **Improved discretization**:

$$
t_k = \left( \epsilon^{1/\rho} + \frac{k}{N-1}(T^{1/\rho} - \epsilon^{1/\rho}) \right)^\rho, \quad k = 0, \ldots, N-1
$$

- $\rho = 7$ (polynomial schedule)

4. **Multi-scale training** (ç•°ãªã‚‹ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã§åŒæ™‚è¨“ç·´)

**Result**: CIFAR-10 FID **1.88** (1-step), **1.25** (2-step) â€” SOTA

#### 3.4.2 iCT vs CT vs CD

| æ‰‹æ³• | æ•™å¸« | FID (1-step) | è¨“ç·´æ™‚é–“ |
|:-----|:-----|:-------------|:---------|
| CT | ãªã— | 9.28 | ~week |
| iCT | ãªã— | **1.88** | ~week |
| CD (from DDPM) | DDPM | 3.55 | ~day |

### 3.5 Easy Consistency Tuning (ECT) â€” ICLR 2025

#### 3.5.1 ECTã®æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢

Geng et al. (2025) "Consistency Models Made Easy"[^3]:

**Problem**: CT/iCTã¯è¨“ç·´ãŒé‡ã„ (1 week on 8 GPUs)

**Solution**: **ODEè»Œé“ã‚’å¾®åˆ†æ–¹ç¨‹å¼ã¨ã—ã¦ç›´æ¥è¡¨ç¾** â†’ Euleræ³•ã®ä»£ã‚ã‚Šã« **analytical ODE solution**

**Key insight**: PF-ODEã®è§£ã‚’ **closed-form**ã§è¨ˆç®—:

$$
\mathbf{x}_{t'} = \alpha(t, t') \mathbf{x}_t + \beta(t, t') \mathbf{x}_0
$$

where:
$$
\alpha(t, t') = \frac{t'}{t}, \quad \beta(t, t') = t' - t
$$

**ECT Loss**:

$$
\mathcal{L}_{\text{ECT}}(\theta) = \mathbb{E}_{t, t', \mathbf{x}_0} \left[ d_{\text{PH}}(F_\theta(\mathbf{x}_t, t), F_\theta(\mathbf{x}_{t'}, t')) \right]
$$

- **No Euler step** â†’ æ•°å€¤èª¤å·®ã‚¼ãƒ­
- **No target network** â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡â†‘

#### 3.5.2 ECT vs iCT ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

CIFAR-10çµæœ:

| æ‰‹æ³• | è¨“ç·´æ™‚é–“ (1 A100) | FID (1-step) | FID (2-step) |
|:-----|:------------------|:-------------|:-------------|
| iCT | ~168 hours (7 days) | 1.88 | 1.25 |
| **ECT** | **1 hour** | **2.73** | **2.05** |

**Speed-up**: **168x faster** training for comparable quality

### 3.6 DPM-Solver++ â€” é«˜æ¬¡ODEã‚½ãƒ«ãƒãƒ¼

#### 3.6.1 DPM-Solverã®ç†è«–

Lu et al. (2022) "DPM-Solver++"[^4]:

**PF-ODE** (data prediction form):

$$
\frac{d\mathbf{x}_t}{dt} = \frac{\mathbf{x}_t - \mathbf{x}_0(\mathbf{x}_t, t)}{t}
$$

where $\mathbf{x}_0(\mathbf{x}_t, t)$ is **data prediction model** (ç¬¬36å›ã§å­¦ã‚“ã  $\hat{\mathbf{x}}_0$äºˆæ¸¬)

**Taylor expansion**:

$$
\mathbf{x}_{t_{n-1}} = \mathbf{x}_{t_n} + \int_{t_n}^{t_{n-1}} \frac{\mathbf{x}_s - \mathbf{x}_0(\mathbf{x}_s, s)}{s} ds
$$

**1st-order DPM-Solver** (Exponential integrator):

$$
\mathbf{x}_{t_{n-1}} = \frac{t_{n-1}}{t_n} \mathbf{x}_{t_n} + (t_{n-1} - t_n) \mathbf{x}_0(\mathbf{x}_{t_n}, t_n)
$$

**2nd-order DPM-Solver++**:

$$
\mathbf{x}_{t_{n-1}} = \frac{t_{n-1}}{t_n} \mathbf{x}_{t_n} + (t_{n-1} - t_n) \left[ \mathbf{x}_0(\mathbf{x}_{t_n}, t_n) + r_n (\mathbf{x}_0(\mathbf{x}_{t_n}, t_n) - \mathbf{x}_0(\mathbf{x}_{t_{n-0.5}}, t_{n-0.5})) \right]
$$

where $r_n = \frac{t_{n-1} - t_n}{t_n - t_{n-0.5}}$ (correction coefficient)

#### 3.6.2 DPM-Solver++ vs DDIM

```julia
# 1st-order DPM-Solver (â‰ˆ DDIM deterministic)
function dpm_solver_1st(x_t, t_cur, t_next, model)
    x_0_pred = model(x_t, t_cur)  # Data prediction
    x_next = (t_next / t_cur) * x_t + (t_next - t_cur) * x_0_pred
    return x_next
end

# 2nd-order DPM-Solver++
function dpm_solver_2nd(x_t, t_cur, t_next, model, x_0_prev)
    x_0_cur = model(x_t, t_cur)

    # Mid-point
    t_mid = (t_cur + t_next) / 2
    x_mid = (t_mid / t_cur) * x_t + (t_mid - t_cur) * x_0_cur
    x_0_mid = model(x_mid, t_mid)

    # Correction
    r = (t_next - t_cur) / (t_cur - t_mid)
    x_next = (t_next / t_cur) * x_t +
             (t_next - t_cur) * (x_0_cur + r * (x_0_cur - x_0_mid))
    return x_next
end
```

| ã‚½ãƒ«ãƒãƒ¼ | Order | NFE (20 steps) | FID (ImageNet 256) |
|:---------|:------|:---------------|:-------------------|
| DDIM | 1 | 20 | 12.24 |
| DPM-Solver | 1 | 20 | 9.36 |
| DPM-Solver++ | 2 | 20 | **7.51** |
| DPM-Solver++ | 2 | 10 | 9.64 |

**é«˜æ¬¡åŒ–ã®åŠ¹æœ**: åŒã˜NFEã§å“è³ªâ†‘ or å°‘ãªã„NFEã§åŒå“è³ª

### 3.7 UniPC â€” Unified Predictor-Corrector

#### 3.7.1 UniPCã®è¨­è¨ˆæ€æƒ³

Zhao et al. (2023) "UniPC"[^5]:

**Predictor-Corrector framework**:

1. **Predictor**: æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã‚’äºˆæ¸¬
2. **Corrector**: äºˆæ¸¬ã‚’è£œæ­£ (ç²¾åº¦å‘ä¸Š)

**UniC (Unified Corrector)**:

$$
\tilde{\mathbf{x}}_{t_{n-1}} = \text{Corrector}(\mathbf{x}_{t_{n-1}}^{\text{pred}}, \mathbf{x}_{t_n})
$$

**UniP (Unified Predictor)**: ä»»æ„ã®order $k$ ã«å¯¾å¿œ

$$
\mathbf{x}_{t_{n-1}} = \frac{t_{n-1}}{t_n} \mathbf{x}_{t_n} + \sum_{i=0}^{k-1} c_i \mathbf{x}_0(\mathbf{x}_{t_{n-i}}, t_{n-i})
$$

#### 3.7.2 UniPC vs DPM-Solver++

| æ‰‹æ³• | Order | NFE (10 steps) | FID (CIFAR-10) |
|:-----|:------|:---------------|:---------------|
| DPM-Solver++ | 2 | 10 | 4.12 |
| **UniPC** | **3** | **10** | **3.87** |

**Correctorã®åŠ¹æœ**: é«˜æ¬¡åŒ–ã ã‘ã§ãªãã€äºˆæ¸¬èª¤å·®ã®è£œæ­£ã§å“è³ªâ†‘

### 3.8 âš”ï¸ Boss Battle: Self-consistencyæ¡ä»¶ã®å®Œå…¨è¨¼æ˜

**Challenge**: Consistency Models (Song et al. 2023)[^1] ã® Theorem 1 ã‚’å®Œå…¨è¨¼æ˜ã›ã‚ˆã€‚

**Theorem 1 (Self-consistency)**:

$f: \mathbb{R}^d \times \mathbb{R}_+ \to \mathbb{R}^d$ ãŒä»¥ä¸‹ã‚’æº€ãŸã™ã¨ã™ã‚‹:

1. **Boundary condition**: $f(\mathbf{x}, \epsilon) = \mathbf{x}$ for all $\mathbf{x} \in \mathbb{R}^d$
2. **Lipschitz continuity**: $\|f(\mathbf{x}, t) - f(\mathbf{x}', t')\| \leq L(\|\mathbf{x} - \mathbf{x}'\| + |t - t'|)$

ã“ã®ã¨ãã€PF-ODEè§£è»Œé“ä¸Šã®ä»»æ„ã®2ç‚¹ $(\mathbf{x}_t, t), (\mathbf{x}_{t'}, t')$ ã«å¯¾ã—:

$$
\lim_{\Delta t \to 0} f(\mathbf{x}_t, t) = \lim_{\Delta t \to 0} f(\mathbf{x}_{t'}, t') = \mathbf{x}_\epsilon
$$

**Proof**:

Step 1: **ODEã®é€£ç¶šæ€§**

PF-ODE: $\frac{d\mathbf{x}}{dt} = -t \nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ ã¯ Lipschitzé€£ç¶š (ç¬¬37å›ã§è¨¼æ˜æ¸ˆã¿)

â†’ è§£è»Œé“ $\mathbf{x}_t$ ã¯ $t$ ã«é–¢ã—ã¦é€£ç¶šå¾®åˆ†å¯èƒ½

Step 2: **Boundaryæ¡ä»¶ã®é©ç”¨**

$t \to \epsilon$ ã§:
$$
f(\mathbf{x}_t, t) \to f(\mathbf{x}_\epsilon, \epsilon) = \mathbf{x}_\epsilon \quad \text{(boundary condition)}
$$

Step 3: **Lipschitzé€£ç¶šæ€§ã«ã‚ˆã‚‹ä¸€æ§˜åæŸ**

ä»»æ„ã® $t, t'$ ã«å¯¾ã—:
$$
\|f(\mathbf{x}_t, t) - f(\mathbf{x}_{t'}, t')\| \leq L(\|\mathbf{x}_t - \mathbf{x}_{t'}\| + |t - t'|)
$$

ODEè»Œé“ä¸Š: $\mathbf{x}_{t'} = \Psi_{t' \leftarrow t}(\mathbf{x}_t)$

$t, t' \to \epsilon$ ã§ $\|\mathbf{x}_t - \mathbf{x}_{t'}\| \to 0$ (é€£ç¶šæ€§)

â†’ $\|f(\mathbf{x}_t, t) - f(\mathbf{x}_{t'}, t')\| \to 0$

Step 4: **Self-consistency**

$$
f(\mathbf{x}_t, t) = f(\mathbf{x}_{t'}, t') = \mathbf{x}_\epsilon \quad \text{for all } t, t' \in [\epsilon, T]
$$

**QED** âˆ

:::message
**Bossæˆ¦ã‚¯ãƒªã‚¢ï¼**
Self-consistencyæ¡ä»¶ã®æ•°å­¦çš„åŸºç›¤ã‚’å®Œå…¨ç†è§£ã—ãŸã€‚ã“ã‚ŒãŒ1-stepç”Ÿæˆã®ç†è«–çš„ä¿è¨¼ã€‚
:::

:::message
**å…¨ä½“ã®50%å®Œäº†ï¼**
æ•°å¼ä¿®è¡ŒZoneå‰åŠå®Œäº†ã€‚æ¬¡ã¯è’¸ç•™æ‰‹æ³•ã¨Rectified Flowçµ±åˆã¸ã€‚
:::

### 3.9 Progressive Distillation â€” æ®µéšçš„ã‚¹ãƒ†ãƒƒãƒ—æ•°åŠæ¸›

#### 3.9.1 Progressive Distillationã®åŸç†

Salimans & Ho (2022) "Progressive Distillation for Fast Sampling"[^6]:

**Idea**: Nã‚¹ãƒ†ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã‚’æ•™å¸«ã¨ã—ã¦ã€N/2ã‚¹ãƒ†ãƒƒãƒ—ã®ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã‚’è’¸ç•™

**Procedure**:
1. æ•™å¸«: DDPM (1024 steps) ã‚’è¨“ç·´
2. ç”Ÿå¾’1: æ•™å¸«ã‹ã‚‰512 stepsãƒ¢ãƒ‡ãƒ«ã‚’è’¸ç•™
3. ç”Ÿå¾’2: ç”Ÿå¾’1ã‹ã‚‰256 stepsãƒ¢ãƒ‡ãƒ«ã‚’è’¸ç•™
4. ... (ç¹°ã‚Šè¿”ã—)
5. æœ€çµ‚: 4 steps ãƒ¢ãƒ‡ãƒ«

**Distillation loss**:

$$
\mathcal{L}_{\text{PD}}(\theta_{\text{student}}) = \mathbb{E}_{\mathbf{x}_0, t, \epsilon} \left[ \|\mathbf{x}_0^{\text{teacher}} - \mathbf{x}_0^{\text{student}}\|^2 \right]
$$

where:
- æ•™å¸«: 2ã‚¹ãƒ†ãƒƒãƒ—ã§ $\mathbf{x}_t \to \mathbf{x}_{t/2} \to \mathbf{x}_0^{\text{teacher}}$
- ç”Ÿå¾’: 1ã‚¹ãƒ†ãƒƒãƒ—ã§ $\mathbf{x}_t \to \mathbf{x}_0^{\text{student}}$

#### 3.9.2 Progressive Distillation vs CM

| æ‰‹æ³• | ã‚¹ãƒ†ãƒƒãƒ—å‰Šæ¸› | è¨“ç·´ã‚³ã‚¹ãƒˆ | å“è³ª |
|:-----|:-------------|:-----------|:-----|
| Progressive Distillation | 1024â†’4 (æ®µéšçš„) | ~DDPMè¨“ç·´æ™‚é–“ | Excellent |
| **Consistency Models** | **ä»»æ„â†’1** | **~DDPMè¨“ç·´æ™‚é–“** | **Excellent** |

**å·®åˆ†**:
- PD: æ®µéšçš„è’¸ç•™ (512â†’256â†’128â†’...â†’4)
- CM: **ç›´æ¥1-step**ã‚’å­¦ç¿’

### 3.10 Latent Consistency Models (LCM) â€” æ½œåœ¨ç©ºé–“ã§ã®é«˜é€Ÿç”Ÿæˆ

#### 3.10.1 LCMã®è¨­è¨ˆ

Luo et al. (2023) "Latent Consistency Models"[^7]:

**Motivation**: Consistency Modelsã‚’ **Latent Diffusion** (ç¬¬39å›) ã«é©ç”¨

**Key components**:
1. **Latent space**: VAE encoder/decoder (ç¬¬10å›)
2. **Consistency function**: æ½œåœ¨ç©ºé–“ $\mathbf{z}_t$ ä¸Šã§å®šç¾©
3. **Classifier-Free Guidanceè’¸ç•™** (ç¬¬39å›ã®CFG)

**LCM Consistency function**:

$$
F_\theta(\mathbf{z}_t, t, \mathbf{c}) = c_{\text{skip}}(t) \mathbf{z}_t + c_{\text{out}}(t) f_\theta(c_{\text{in}}(t) \mathbf{z}_t, t, \mathbf{c})
$$

where $\mathbf{c}$ is **text conditioning** (CLIP embedding)

#### 3.10.2 LCM Distillation

**Guidance Distillation**:

æ•™å¸«ãƒ¢ãƒ‡ãƒ« (Stable Diffusion) ã® **CFGå‡ºåŠ›**ã‚’è’¸ç•™:

$$
\mathbf{z}_0^{\text{teacher}} = \mathbf{z}_0^{\text{uncond}} + w (\mathbf{z}_0^{\text{cond}} - \mathbf{z}_0^{\text{uncond}})
$$

LCM loss:

$$
\mathcal{L}_{\text{LCM}}(\theta) = \mathbb{E} \left[ d(F_\theta(\mathbf{z}_{t_{n+1}}, t_{n+1}, \mathbf{c}), \mathbf{z}_0^{\text{teacher}}) \right]
$$

#### 3.10.3 LCM Performance

**SDXL-LCM** (768x768):

| Steps | Time (A100) | FID â†“ | Aesthetic Score â†‘ |
|:------|:-----------|:------|:------------------|
| SDXL (50 steps) | 5 sec | 23.4 | 5.8 |
| **LCM (4 steps)** | **0.4 sec** | **24.1** | **5.6** |

**Speed-up**: **12.5x faster**, å“è³ªã»ã¼åŒç­‰

**Training cost**: 32 A100-hours (vs SDXL: ~10,000 A100-hours)

### 3.11 Rectified Flow Distillation â€” ç›´ç·šåŒ–ã«ã‚ˆã‚‹1-stepç”Ÿæˆ

#### 3.11.1 InstaFlowã®åŸç†

Liu et al. (2023) "InstaFlow"[^8]:

**Rectified Flow** (ç¬¬38å›):
- **ReFlow**: æ›²ç·šè»Œé“ â†’ ç›´ç·šè»Œé“ã«"æ•´æµ"
- **1-stepè’¸ç•™**: ç›´ç·šè»Œé“ãªã‚‰1ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜ç²¾åº¦

**InstaFlow procedure**:
1. Stable Diffusion â†’ Rectified Flowå¤‰æ›
2. ReFlow 2å› (è»Œé“ã‚’ç›´ç·šåŒ–)
3. 1-stepè’¸ç•™

**1-step distillation loss**:

$$
\mathcal{L}_{\text{InstaFlow}}(\theta) = \mathbb{E}_{\mathbf{x}_0, \mathbf{x}_1, t} \left[ \|\mathbf{v}_\theta(\mathbf{x}_t, t) - (\mathbf{x}_1 - \mathbf{x}_0)\|^2 \right]
$$

where $\mathbf{v}_\theta$ is **velocity field** (ç¬¬38å›)

#### 3.11.2 InstaFlow vs LCM

| æ‰‹æ³• | ãƒ™ãƒ¼ã‚¹ | Steps | FID (MS-COCO) | è¨“ç·´æ™‚é–“ |
|:-----|:-------|:------|:--------------|:---------|
| SD 1.5 (50 steps) | Diffusion | 50 | 23.0 | - |
| LCM (4 steps) | Diffusion | 4 | 24.1 | 32 A100-h |
| **InstaFlow (1 step)** | **Rectified Flow** | **1** | **23.3** | **199 A100-h** |

**InstaFlowã®å„ªä½æ€§**: 1ã‚¹ãƒ†ãƒƒãƒ—ã§å“è³ªç¶­æŒï¼ˆç›´ç·šè»Œé“ã®åˆ©ç‚¹ï¼‰

### 3.12 Adversarial Post-Training (DMD2) â€” GANè’¸ç•™

#### 3.12.1 DMD2ã®è¨­è¨ˆæ€æƒ³

Lin et al. (2025) "Diffusion Adversarial Post-Training"[^9]:

**Motivation**: Diffusionäº‹å‰è¨“ç·´ â†’ GAN post-trainingã§1-stepç”Ÿæˆ

**Two-stage training**:
1. **Pre-training**: DDPM/LDMã§ç¢ºç‡åˆ†å¸ƒå­¦ç¿’
2. **Post-training**: Adversarial lossã§1-step Generatorã«è’¸ç•™

**DMD2 loss**:

$$
\mathcal{L}_{\text{DMD2}} = \mathcal{L}_{\text{adv}} + \lambda_{\text{score}} \mathcal{L}_{\text{score}}
$$

- $\mathcal{L}_{\text{adv}}$: GAN adversarial loss (ç¬¬12å›)
- $\mathcal{L}_{\text{score}}$: Score distillation (Diffusionæ•™å¸«ã‹ã‚‰)

**Score distillation**:

$$
\mathcal{L}_{\text{score}} = \mathbb{E}_{\mathbf{x}_0, t} \left[ \|\mathbf{s}_\theta(\mathbf{x}_t, t) - \mathbf{s}_{\text{teacher}}(\mathbf{x}_t, t)\|^2 \right]
$$

#### 3.12.2 DMD2 Performance

**Video generation** (2-second, 1280x720, 24fps):

| æ‰‹æ³• | Steps | Time | å“è³ª |
|:-----|:------|:-----|:-----|
| Diffusion baseline | 50 | 50 sec | High |
| **DMD2 (Seaweed-APT)** | **1** | **1 sec** | **Comparable** |

**1024px image generation**:

| æ‰‹æ³• | Steps | FID â†“ |
|:-----|:------|:------|
| Stable Diffusion 3 | 50 | 10.2 |
| **DMD2** | **1** | **12.8** |

**Trade-off**: å“è³ªã‚ãšã‹ã«ä½ä¸‹ï¼ˆFID 10.2â†’12.8ï¼‰ã€é€Ÿåº¦50xâ†‘

### 3.13 Consistency Trajectory Models (CTM) â€” è»Œé“å…¨ä½“ã®ä¸€è²«æ€§

#### 3.13.1 CTMã®å‹•æ©Ÿ

Kim et al. (2023) "Consistency Trajectory Models"[^11]:

**CMã®é™ç•Œ**:
- Self-consistency: $F_\theta(\mathbf{x}_t, t) = F_\theta(\mathbf{x}_{t'}, t')$
- å•é¡Œ: 2ç‚¹é–“ã®ä¸€è²«æ€§ã®ã¿ â†’ **è»Œé“å…¨ä½“**ã®æ•´åˆæ€§ã¯ä¿è¨¼ãªã—

**CTMã®ã‚¢ã‚¤ãƒ‡ã‚¢**: PF-ODEè»Œé“å…¨ä½“ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–

$$
\mathbf{g}_\theta(\mathbf{x}_t, t, t') = \mathbf{x}_{t'} \quad \text{for any } t, t' \in [\epsilon, T]
$$

- **Generalization**: CM ($t'=\epsilon$å›ºå®š) â†’ CTM ($t'$å¯å¤‰)
- **åˆ©ç‚¹**: ä»»æ„ã®æ™‚åˆ»é–“é·ç§»ã‚’å­¦ç¿’ â†’ ã‚ˆã‚ŠæŸ”è»Ÿãªsampling

#### 3.13.2 CTMè¨“ç·´

**CTM loss**:

$$
\mathcal{L}_{\text{CTM}}(\theta) = \mathbb{E}_{t, t', \mathbf{x}_0} \left[ d(\mathbf{g}_\theta(\mathbf{x}_t, t, t'), \mathbf{x}_{t'}^{\text{ODE}}) \right]
$$

where $\mathbf{x}_{t'}^{\text{ODE}}$ ã¯PF-ODEã®1ã‚¹ãƒ†ãƒƒãƒ—è§£:

$$
\mathbf{x}_{t'}^{\text{ODE}} = \mathbf{x}_t + \int_t^{t'} -s \nabla_{\mathbf{x}} \log p_s(\mathbf{x}_s) ds
$$

**å®Ÿè£…**:

```julia
# Consistency Trajectory Model
struct CTM{M}
    backbone::M
end

function (ctm::CTM)(x_t, t, t_prime, ps, st)
    # Map x_t at time t to x_t' at time t'
    net_out, st = ctm.backbone(x_t, t, t_prime, ps, st)
    return net_out, st
end

# CTM training loss
function ctm_loss(model, x_0, t, t_prime, score_model, ps, st)
    z = randn(size(x_0))
    x_t = x_0 .+ t .* z

    # ODE step (ground truth)
    score = score_model(x_t, t)
    x_t_prime_true = x_t .+ (t_prime - t) .* (-t .* score)

    # CTM prediction
    x_t_prime_pred, st = model(x_t, t, t_prime, ps, st)

    loss = mean((x_t_prime_pred .- x_t_prime_true).^2)
    return loss, st
end
```

#### 3.13.3 CTM vs CM

| é …ç›® | CM | CTM |
|:-----|:---|:----|
| å‡ºåŠ› | $F_\theta(\mathbf{x}_t, t) = \mathbf{x}_\epsilon$ (å›ºå®šçµ‚ç‚¹) | $\mathbf{g}_\theta(\mathbf{x}_t, t, t')$ (å¯å¤‰çµ‚ç‚¹) |
| Flexibility | ä½ (çµ‚ç‚¹å›ºå®š) | é«˜ (ä»»æ„æ™‚åˆ»é·ç§») |
| è¨“ç·´ | Self-consistencyæ¡ä»¶ | Trajectory consistency |
| Sampling | 1-step or multistep | **Long jumpå¯èƒ½** |

**CTMã®åˆ©ç‚¹**:
- **Long jumps**: $T \to T/2 \to T/4 \to \epsilon$ (å¤§ããªã‚¹ãƒ†ãƒƒãƒ—å¹…)
- **Adaptive steps**: å“è³ªãŒæ‚ªã„é ˜åŸŸã§ç´°ã‹ãã‚¹ãƒ†ãƒƒãƒ—

### 3.14 å“è³ª vs é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ• â€” Pareto Frontåˆ†æ

#### 3.13.1 Pareto Frontã®å¯è¦–åŒ–

```julia
using Plots

# å„æ‰‹æ³•ã® (é€Ÿåº¦, å“è³ª) ãƒ—ãƒ­ãƒƒãƒˆ
methods = [
    ("DDPM (1000 steps)", 10.0, 3.17),
    ("DDIM (50 steps)", 0.5, 4.67),
    ("DPM-Solver++ (20 steps)", 0.2, 3.95),
    ("UniPC (10 steps)", 0.1, 4.12),
    ("LCM (4 steps)", 0.04, 4.25),
    ("CM (1 step)", 0.01, 3.55),
    ("InstaFlow (1 step)", 0.01, 4.10),
    ("DMD2 (1 step)", 0.01, 5.20)
]

times = [m[2] for m in methods]
fids = [m[3] for m in methods]
labels = [m[1] for m in methods]

scatter(times, fids,
        xlabel="Sampling Time (sec)", ylabel="FID â†“",
        xscale=:log10, label=reshape(labels, 1, :),
        title="Quality-Speed Pareto Front",
        markersize=8, legend=:outertopright)

# Pareto front curve
pareto_idx = [1, 2, 3, 5, 6]  # Dominant points
plot!(times[pareto_idx], fids[pareto_idx],
      linestyle=:dash, linewidth=2, color=:red,
      label="Pareto Front")
```

**Pareto Frontè§£é‡ˆ**:
- **DDPM**: æœ€é«˜å“è³ªã€æœ€é…
- **CM**: 1-step, å“è³ªç¶­æŒ
- **LCM**: 4-step sweet spot (å“è³ªâ†‘)
- **DMD2**: 1-step, å“è³ªã‚„ã‚„åŠ£åŒ–

#### 3.13.2 é«˜é€ŸåŒ–ã®ç†è«–çš„é™ç•Œ â€” æƒ…å ±ç†è«–çš„ä¸‹ç•Œ

**Theorem (Sampling complexity lower bound)**:

ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}$ ã‹ã‚‰ $\epsilon$-è¿‘ä¼¼ã‚µãƒ³ãƒ—ãƒ« (TVè·é›¢ã§) ã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ã€å°‘ãªãã¨ã‚‚ $\Omega(\log(1/\epsilon))$ å›ã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãŒå¿…è¦ã€‚

**Proof (Sketch)**:

Step 1: **æƒ…å ±é‡ã®è¦³ç‚¹**

ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ = $\mathcal{N}(\mathbf{0}, I)$ (ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $H_0$) ã‹ã‚‰ $p_{\text{data}}$ (ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $H_{\text{data}}$) ã¸ã®å¤‰æ›

å¿…è¦ãªæƒ…å ±é‡: $\Delta H = H_{\text{data}} - H_0$

Step 2: **1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šã®æƒ…å ±ç²å¾—**

å„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§å¾—ã‚‰ã‚Œã‚‹æƒ…å ±é‡: $I_{\text{step}} \leq C \log d$ (æ¬¡å…ƒ $d$ ã«ä¾å­˜)

Step 3: **ä¸‹ç•Œ**

$$
N \geq \frac{\Delta H}{I_{\text{step}}} = \Omega\left(\frac{H_{\text{data}}}{C \log d}\right)
$$

è‡ªç„¶ç”»åƒ: $H_{\text{data}} \approx 8 \times H \times W$ bits (CIFAR-10: $8 \times 32 \times 32 = 8192$ bits)

â†’ $N \geq \Omega(\log d / \epsilon)$

Step 4: **å®Ÿè·µçš„å«æ„**

- é«˜æ¬¡å…ƒ ($d=3072$ for CIFAR-10): $\log d \approx 11$
- High quality ($\epsilon=0.01$): $N \geq 100$ steps (ç†è«–çš„ä¸‹ç•Œ)
- **CM 1-step**: ä¸‹ç•Œã‚’ç ´ã‚‹ï¼Ÿ â†’ **No**, äº‹å‰è¨“ç·´ã§æƒ…å ±ã‚’å­¦ç¿’æ¸ˆã¿

**QED** âˆ

:::message alert
**1-stepç”Ÿæˆã®ç§˜å¯†**:
- CM 1-step â‰  æƒ…å ±ç†è«–çš„ä¸‹ç•Œã®æ‰“ç ´
- **äº‹å‰è¨“ç·´ (CT/CD) ã§ $\Omega(\log d)$ ç›¸å½“ã®æƒ…å ±ã‚’å­¦ç¿’**
- æ¨è«–æ™‚ã¯å­¦ç¿’æ¸ˆã¿çŸ¥è­˜ã®**èª­ã¿å‡ºã—**ã®ã¿
:::

**Rate-Distortionç†è«–ã¨ã®æ¥ç¶š**:

Shannon ã® Rate-Distortion é–¢æ•°:

$$
R(D) = \min_{p(\hat{\mathbf{x}}|\mathbf{x}): \mathbb{E}[d(\mathbf{x}, \hat{\mathbf{x}})] \leq D} I(\mathbf{x}; \hat{\mathbf{x}})
$$

- $R(D)$: æ­ªã¿ $D$ ã‚’è¨±å®¹ã—ãŸã¨ãã®æœ€å°ãƒ¬ãƒ¼ãƒˆ
- Consistency Models: $D=\text{FID}$, $R=N_{\text{steps}}$

**Pareto front** = Rate-Distortionæ›²ç·šã®é›¢æ•£è¿‘ä¼¼

**Empirical Rate-Distortionæ›²ç·š**:

- $C$: ãƒ¢ãƒ‡ãƒ«ä¾å­˜å®šæ•°
- $Q_{\max}$: ç„¡é™ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å“è³ªä¸Šé™

**Empirical observation**:

| Steps | FID (CIFAR-10) | Quality gain |
|:------|:---------------|:-------------|
| 1 | 3.55 | - |
| 2 | 3.25 | +0.30 |
| 4 | 2.93 | +0.32 |
| 8 | 2.85 | +0.08 |
| 1000 | 3.17 | -0.68 (!) |

**Diminishing returns**: 8ã‚¹ãƒ†ãƒƒãƒ—ä»¥é™ã¯å“è³ªæ”¹å–„ã‚ãšã‹

:::message alert
**1000ã‚¹ãƒ†ãƒƒãƒ—ã®é€†èª¬**: DDPMã®1000ã‚¹ãƒ†ãƒƒãƒ—ã‚ˆã‚Šã€CM 4ã‚¹ãƒ†ãƒƒãƒ—ã®æ–¹ãŒé«˜å“è³ª (FID 2.93 vs 3.17)
â†’ ã‚¹ãƒ†ãƒƒãƒ—æ•°â‰ å“è³ªä¿è¨¼ã€**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ**ãŒæœ¬è³ª
:::

:::message
**å…¨ä½“ã®70%å®Œäº†ï¼**
è’¸ç•™æ‰‹æ³•å®Œå…¨ç¶²ç¾…ã€‚æ¬¡ã¯å®Ÿè£…Zoneã§ã“ã‚Œã‚‰ã‚’å‹•ã‹ã™ã€‚
:::

---
