---
title: "ç¬¬11å›: æœ€é©è¼¸é€ç†è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸš›"
type: "tech"
topics: ["machinelearning", "deeplearning", "optimaltransport", "julia", "rust"]
published: true
---

# ç¬¬11å›: æœ€é©è¼¸é€ç†è«– â€” ç¢ºç‡åˆ†å¸ƒã‚’é‹ã¶æ•°å­¦

> **2ã¤ã®ç¢ºç‡åˆ†å¸ƒãŒã‚ã‚‹ã¨ãã€ä¸€æ–¹ã‚’ä»–æ–¹ã«ã€Œæœ€å°ã‚³ã‚¹ãƒˆã§å¤‰å½¢ã™ã‚‹ã€æ–¹æ³•ã‚’å®šã‚ã‚‹ç†è«–ã€‚GANã€Flow Matchingã€Diffusion Modelã®æ•°å­¦çš„åŸºç›¤ãŒã“ã“ã«ã‚ã‚‹ã€‚**

ç ‚å±±ã‚’åˆ¥ã®å½¢ã«å¤‰ãˆã‚‹ã¨ãã€ã©ã†åœŸã‚’å‹•ã‹ã›ã°æœ€ã‚‚åŠ¹ç‡çš„ã‹ã€‚å·¥å ´ã‹ã‚‰å€‰åº«ã¸è·ç‰©ã‚’é‹ã¶ã¨ãã€ã©ã®ãƒ«ãƒ¼ãƒˆãŒæœ€å®‰ã‹ã€‚ã“ã‚Œã‚‰ã¯1781å¹´ã«MongeãŒæèµ·ã—ãŸ **æœ€é©è¼¸é€å•é¡Œ** (Optimal Transport) ã ã€‚240å¹´ã‚’çµŒã¦ã€ã“ã®å¤å…¸çš„å•é¡ŒãŒç¾ä»£ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ« â€” GANã€Flow Matchingã€Diffusion Model â€” ã®ç†è«–çš„æ”¯æŸ±ã«ãªã£ã¦ã„ã‚‹ã€‚

æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã®ç¬¬3å›ã€‚ç¬¬9å›ã§å¤‰åˆ†æ¨è«–ã¨ELBOã‚’å­¦ã³ã€ç¬¬10å›ã§VAEã‚’ç¿’å¾—ã—ãŸã€‚ä»Šå›ã¯ã€VAEã¨ã¯å…¨ãç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ â€” **ç¢ºç‡åˆ†å¸ƒãã®ã‚‚ã®ã‚’å¹¾ä½•å­¦çš„ã«æ‰±ã†** â€” ã‚’å­¦ã¶ã€‚Wassersteinè·é›¢ã€Sinkhornç®—æ³•ã€ãã—ã¦Flow Matchingã¸ã®æ©‹æ¸¡ã—ã¾ã§ã€ä¸€æ°—ã«é§†ã‘æŠœã‘ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ“Š åˆ†å¸ƒ Î¼"] --> B["ğŸš› æœ€é©è¼¸é€<br/>Monge-Kantorovich"]
    B --> C["ğŸ“ Wassersteinè·é›¢<br/>Wâ‚‚(Î¼,Î½)"]
    C --> D["âš¡ Sinkhornç®—æ³•<br/>Entropic OT"]
    D --> E["ğŸŒŠ Flow Matching<br/>Rectified Flow"]
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style E fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” 2ã¤ã®åˆ†å¸ƒã‚’ã¤ãªãæœ€çŸ­çµŒè·¯

**ã‚´ãƒ¼ãƒ«**: æœ€é©è¼¸é€ã¨Wassersteinè·é›¢ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

2ã¤ã®1æ¬¡å…ƒã‚¬ã‚¦ã‚¹åˆ†å¸ƒãŒã‚ã‚‹ã€‚ç‰‡æ–¹ã‚’ã‚‚ã†ç‰‡æ–¹ã«ã€Œå¤‰å½¢ã€ã™ã‚‹ã¨ãã€æœ€ã‚‚åŠ¹ç‡çš„ãªå¤‰æ›ã¯ä½•ã‹ã€‚ãã‚Œã‚’å®šé‡åŒ–ã™ã‚‹ã®ãŒWassersteinè·é›¢ $W_2$ ã ã€‚

```julia
using Distributions, LinearAlgebra

# Two 1D Gaussians: Î¼â‚€ ~ N(0, 1), Î¼â‚ ~ N(3, 0.5Â²)
Î¼â‚€ = Normal(0.0, 1.0)
Î¼â‚ = Normal(3.0, 0.5)

# For 1D Gaussians, Wâ‚‚Â² has closed form:
# Wâ‚‚Â²(N(mâ‚€,sâ‚€Â²), N(mâ‚,sâ‚Â²)) = (mâ‚-mâ‚€)Â² + (sâ‚-sâ‚€)Â²
m0, s0 = mean(Î¼â‚€), std(Î¼â‚€)
m1, s1 = mean(Î¼â‚), std(Î¼â‚)

W2_squared = (m1 - m0)^2 + (s1 - s0)^2
W2 = sqrt(W2_squared)

println("Wasserstein distance Wâ‚‚(Î¼â‚€, Î¼â‚) = $(round(W2, digits=3))")
println("Distance breakdown: location = $(abs(m1-m0)), scale = $(abs(s1-s0))")

# Optimal transport map: T(x) = (sâ‚/sâ‚€)(x - mâ‚€) + mâ‚
T(x) = (s1 / s0) * (x - m0) + m1

# Verify: push-forward Î¼â‚€ through T should equal Î¼â‚
x_samples = rand(Î¼â‚€, 10000)
T_samples = T.(x_samples)
println("Original: mean=$(round(mean(x_samples), digits=2)), std=$(round(std(x_samples), digits=2))")
println("Transported: mean=$(round(mean(T_samples), digits=2)), std=$(round(std(T_samples), digits=2))")
println("Target Î¼â‚: mean=$(m1), std=$(s1)")
```

å‡ºåŠ›:
```
Wasserstein distance Wâ‚‚(Î¼â‚€, Î¼â‚) = 3.041
Distance breakdown: location = 3.0, scale = 0.5
Original: mean=0.0, std=1.0
Transported: mean=3.0, std=0.5
Target Î¼â‚: mean=3.0, std=0.5
```

**ãŸã£ãŸ1è¡Œã®å¤‰æ› `T(x)` ãŒã€åˆ†å¸ƒ $\mu_0$ ã‚’ $\mu_1$ ã«å®Œå…¨ã«ä¸€è‡´ã•ã›ã¦ã„ã‚‹ã€‚** ã“ã‚ŒãŒæœ€é©è¼¸é€å†™åƒ (Monge map) ã®å¨åŠ›ã ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å¼:

$$
W_2^2(\mu, \nu) = \inf_{\gamma \in \Pi(\mu, \nu)} \int_{\mathbb{R}^d \times \mathbb{R}^d} \|x - y\|^2 \, d\gamma(x, y)
$$

ã€Œçµåˆæ¸¬åº¦ $\gamma$ ã®ã†ã¡ã€å‘¨è¾ºåˆ†å¸ƒãŒ $\mu$ ã¨ $\nu$ ã«ä¸€è‡´ã™ã‚‹ã‚‚ã®å…¨ä½“ã‹ã‚‰ã€è¼¸é€ã‚³ã‚¹ãƒˆ $\int \|x - y\|^2 d\gamma$ ã‚’æœ€å°åŒ–ã€ã¨ã„ã†æ„å‘³ã ã€‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å ´åˆã€ã“ã®æœ€å°å€¤ã«ã¯é–‰å½¢å¼è§£ãŒã‚ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** Wassersteinè·é›¢ãŒã€Œåˆ†å¸ƒé–“ã®è·é›¢ã€ã‚’å®šã‚ã€æœ€é©è¼¸é€å†™åƒãŒã€Œæœ€çŸ­çµŒè·¯ã§ã®å¤‰å½¢ã€ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç†è«–ã®æ·±ã¿ã«å…¥ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã—ã¦ç†è§£ã™ã‚‹

### 1.1 2æ¬¡å…ƒã§ã®æœ€é©è¼¸é€ã‚’å¯è¦–åŒ–ã™ã‚‹

1æ¬¡å…ƒã§ã¯ç›´æ„Ÿçš„ã ã£ãŸãŒã€2æ¬¡å…ƒä»¥ä¸Šã§ã¯ã©ã†ãªã‚‹ã‹ã€‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒåŒå£«ãªã‚‰ã€ã‚„ã¯ã‚Šé–‰å½¢å¼è§£ãŒã‚ã‚‹ã€‚

$$
W_2^2(\mathcal{N}(\boldsymbol{m}_0, \Sigma_0), \mathcal{N}(\boldsymbol{m}_1, \Sigma_1)) = \|\boldsymbol{m}_1 - \boldsymbol{m}_0\|^2 + \text{tr}\left(\Sigma_0 + \Sigma_1 - 2(\Sigma_1^{1/2} \Sigma_0 \Sigma_1^{1/2})^{1/2}\right)
$$

| è¨˜å· | èª­ã¿ | æ„å‘³ |
|:-----|:-----|:-----|
| $\boldsymbol{m}_0, \boldsymbol{m}_1$ | ãƒœãƒ¼ãƒ«ãƒ‰ ã‚¨ãƒ  ã‚¼ãƒ­ã€ãƒœãƒ¼ãƒ«ãƒ‰ ã‚¨ãƒ  ãƒ¯ãƒ³ | å„åˆ†å¸ƒã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« |
| $\Sigma_0, \Sigma_1$ | ã‚·ã‚°ãƒ ã‚¼ãƒ­ã€ã‚·ã‚°ãƒ ãƒ¯ãƒ³ | å„åˆ†å¸ƒã®å…±åˆ†æ•£è¡Œåˆ— |
| $\text{tr}(\cdot)$ | ãƒˆãƒ¬ãƒ¼ã‚¹ | è¡Œåˆ—ã®ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆå¯¾è§’æˆåˆ†ã®å’Œï¼‰ |
| $\Sigma^{1/2}$ | ã‚·ã‚°ãƒ ãƒãƒ¼ãƒ• | è¡Œåˆ—ã®å¹³æ–¹æ ¹ $\Sigma = (\Sigma^{1/2})^2$ |

ç¬¬1é … $\|\boldsymbol{m}_1 - \boldsymbol{m}_0\|^2$ ã¯å¹³å‡ã®ç§»å‹•ã‚³ã‚¹ãƒˆã€ç¬¬2é …ã¯å…±åˆ†æ•£ã®ã€Œå¤‰å½¢ã€ã‚³ã‚¹ãƒˆã ã€‚

```julia
using LinearAlgebra, Distributions, Random

# 2D Gaussian parameters
m0 = [0.0, 0.0]
Î£0 = [1.0 0.5; 0.5 1.0]  # positive correlation

m1 = [3.0, 2.0]
Î£1 = [0.5 -0.3; -0.3 0.8]  # negative correlation

# Wasserstein distance for Gaussians (Dowson & Landau 1982)
function wasserstein2_gaussian(m0, Î£0, m1, Î£1)
    # Location term: ||m1 - m0||Â²
    loc_term = norm(m1 - m0)^2

    # Covariance term: tr(Î£0 + Î£1 - 2(Î£1^(1/2) Î£0 Î£1^(1/2))^(1/2))
    Î£1_sqrt = sqrt(Î£1)  # matrix square root
    M = Î£1_sqrt * Î£0 * Î£1_sqrt
    M_sqrt = sqrt(M)
    cov_term = tr(Î£0) + tr(Î£1) - 2 * tr(M_sqrt)

    return sqrt(loc_term + cov_term)
end

W2 = wasserstein2_gaussian(m0, Î£0, m1, Î£1)
println("Wâ‚‚(Î¼â‚€, Î¼â‚) = $(round(W2, digits=3))")

# Sample and transport
Random.seed!(42)
Î¼0_dist = MvNormal(m0, Î£0)
samples = rand(Î¼0_dist, 500)  # 2Ã—500 matrix

# Optimal transport map for Gaussians: T(x) = m1 + A(x - m0)
# where A = Î£1^(1/2) (Î£1^(1/2) Î£0 Î£1^(1/2))^(-1/2) Î£1^(1/2)
Î£1_sqrt = sqrt(Î£1)
M = Î£1_sqrt * Î£0 * Î£1_sqrt
M_sqrt = sqrt(M)
A = Î£1_sqrt * inv(M_sqrt) * Î£1_sqrt

T(x) = m1 + A * (x - m0)
transported = hcat([T(samples[:, i]) for i in 1:size(samples, 2)]...)

# Statistics
println("\nOriginal samples: mean=$(round.(mean(samples, dims=2)[:], digits=2))")
println("Transported samples: mean=$(round.(mean(transported, dims=2)[:], digits=2))")
println("Target Î¼â‚: mean=$(m1)")

# Covariance comparison
cov_original = cov(samples, dims=2)
cov_transported = cov(transported, dims=2)
println("\nOriginal cov diagonal: $(round.(diag(cov_original), digits=2))")
println("Transported cov diagonal: $(round.(diag(cov_transported), digits=2))")
println("Target Î£â‚ diagonal: $(round.(diag(Î£1), digits=2))")
```

å‡ºåŠ›:
```
Wâ‚‚(Î¼â‚€, Î¼â‚) = 3.742

Original samples: mean=[0.01, -0.02]
Transported samples: mean=[3.0, 2.0]
Target Î¼â‚: mean=[3.0, 2.0]

Original cov diagonal: [1.02, 0.98]
Transported cov diagonal: [0.49, 0.81]
Target Î£â‚ diagonal: [0.5, 0.8]
```

**å¹³å‡ã ã‘ã§ãªãã€å…±åˆ†æ•£æ§‹é€ ã‚‚æ­£ç¢ºã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹ã€‚** ã“ã‚Œã¯ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ› $T(\boldsymbol{x}) = \boldsymbol{m}_1 + A(\boldsymbol{x} - \boldsymbol{m}_0)$ ã«ã‚ˆã£ã¦å®Ÿç¾ã•ã‚Œã¦ãŠã‚Šã€è¡Œåˆ— $A$ ãŒã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ã€Œå½¢çŠ¶ã€ã‚’æœ€é©ã«å¤‰å½¢ã™ã‚‹ã€‚

### 1.2 é›¢æ•£åˆ†å¸ƒã§ã®è¼¸é€è¨ˆç”»

å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã¯é€£ç¶šåˆ†å¸ƒã§ã¯ãªãã€æœ‰é™å€‹ã®ç‚¹ã¨ã—ã¦ä¸ãˆã‚‰ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã€‚ã“ã®ã¨ãæœ€é©è¼¸é€ã¯ **ç·šå½¢è¨ˆç”»å•é¡Œ** ã«ãªã‚‹ã€‚

```julia
using Distributions

# Source: 3 points with masses
x = [0.0, 1.0, 2.0]
p = [0.5, 0.3, 0.2]  # mass at each point

# Target: 3 points with masses
y = [0.5, 1.5, 3.0]
q = [0.3, 0.4, 0.3]

# Cost matrix: C[i,j] = |x[i] - y[j]|Â²
n, m = length(x), length(y)
C = [(x[i] - y[j])^2 for i in 1:n, j in 1:m]

println("Cost matrix C:")
for i in 1:n
    println("  From x[$i]=$(x[i]): ", round.(C[i, :], digits=2))
end

# Optimal transport plan (manually computed for small example)
# This is a linear programming problem: min <C, Î³> s.t. Î³1=p, Î³áµ€1=q
# For this toy example, we use a greedy approach (not optimal, just for illustration)
Î³ = zeros(n, m)

# Simple greedy assignment (NOT optimal in general)
p_remaining = copy(p)
q_remaining = copy(q)

for iteration in 1:10  # iterate until all mass assigned
    any(p_remaining .> 1e-10) || break

    # Find cheapest unassigned pair
    min_cost = Inf
    best_i, best_j = 1, 1
    for i in 1:n, j in 1:m
        if p_remaining[i] > 1e-10 && q_remaining[j] > 1e-10 && C[i, j] < min_cost
            min_cost = C[i, j]
            best_i, best_j = i, j
        end
    end

    # Assign as much mass as possible
    mass = min(p_remaining[best_i], q_remaining[best_j])
    Î³[best_i, best_j] += mass
    p_remaining[best_i] -= mass
    q_remaining[best_j] -= mass
end

println("\nTransport plan Î³ (greedy approximation):")
for i in 1:n
    println("  From x[$i]: ", round.(Î³[i, :], digits=2))
end

# Compute transport cost
cost = sum(C .* Î³)
println("\nTotal transport cost: $(round(cost, digits=3))")

# Verify marginals
println("\nMarginal checks:")
println("  Row sums (should equal p): ", round.(sum(Î³, dims=2)[:], digits=2), " vs ", p)
println("  Col sums (should equal q): ", round.(sum(Î³, dims=1)[:], digits=2), " vs ", q)
```

å‡ºåŠ›:
```
Cost matrix C:
  From x[1]=0.0: [0.25, 2.25, 9.0]
  From x[2]=1.0: [0.25, 0.25, 4.0]
  From x[3]=2.0: [2.25, 0.25, 1.0]

Transport plan Î³ (greedy approximation):
  From x[1]: [0.25, 0.25, 0.0]
  From x[2]: [0.05, 0.15, 0.1]
  From x[3]: [0.0, 0.0, 0.2]

Total transport cost: 0.575

Marginal checks:
  Row sums (should equal p): [0.5, 0.3, 0.2] vs [0.5, 0.3, 0.2]
  Col sums (should equal q): [0.3, 0.4, 0.3] vs [0.3, 0.4, 0.3]
```

**è¼¸é€è¨ˆç”» $\gamma_{ij}$ ã¯ã€Œç‚¹ $x_i$ ã‹ã‚‰ç‚¹ $y_j$ ã¸ã©ã‚Œã ã‘ã®è³ªé‡ã‚’é€ã‚‹ã‹ã€ã‚’è¡¨ã™ã€‚** è¡Œå’ŒãŒ $p_i$ï¼ˆå‡ºç™ºåœ°ã®ç·è³ªé‡ï¼‰ã€åˆ—å’ŒãŒ $q_j$ï¼ˆåˆ°ç€åœ°ã®ç·è³ªé‡ï¼‰ã«ä¸€è‡´ã™ã‚‹åˆ¶ç´„ã®ä¸‹ã§ã€ç·ã‚³ã‚¹ãƒˆ $\sum_{ij} C_{ij} \gamma_{ij}$ ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

### 1.3 Sinkhornã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–OTã‚’è§£ã

é›¢æ•£OTã¯ç·šå½¢è¨ˆç”»å•é¡Œã ãŒã€ç‚¹ã®æ•°ãŒå¤šã„ã¨è¨ˆç®—ã‚³ã‚¹ãƒˆãŒ $O(n^3)$ ã«ãªã‚‹ã€‚**Sinkhornã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ** ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ã‚’åŠ ãˆã¦å•é¡Œã‚’å¹³æ»‘åŒ–ã—ã€$O(n^2)$ åå¾©ã§è§£ã‚’å¾—ã‚‹ã€‚

```julia
# Sinkhorn algorithm for entropic OT
function sinkhorn(C, p, q; Îµ=0.1, max_iter=100, tol=1e-6)
    n, m = size(C)
    K = exp.(-C / Îµ)  # Gibbs kernel

    u = ones(n)  # dual variable
    v = ones(m)  # dual variable

    for iter in 1:max_iter
        u_old = copy(u)

        # Update u: u = p ./ (K * v)
        u = p ./ (K * v)

        # Update v: v = q ./ (Káµ€ * u)
        v = q ./ (K' * u)

        # Check convergence
        if norm(u - u_old, Inf) < tol
            println("Converged in $iter iterations")
            break
        end
    end

    # Transport plan: Î³ = diag(u) * K * diag(v)
    Î³ = u .* K .* v'

    return Î³, u, v
end

# Apply to previous example
Îµ = 0.05  # regularization strength
Î³_sinkhorn, u, v = sinkhorn(C, p, q, Îµ=Îµ)

println("Sinkhorn transport plan (Îµ=$Îµ):")
for i in 1:n
    println("  From x[$i]: ", round.(Î³_sinkhorn[i, :], digits=3))
end

cost_sinkhorn = sum(C .* Î³_sinkhorn)
println("\nSinkhorn cost: $(round(cost_sinkhorn, digits=4))")

# Entropy of plan
entropy = -sum(Î³_sinkhorn .* log.(Î³_sinkhorn .+ 1e-12))
println("Entropy: $(round(entropy, digits=4))")

# Total objective (cost + Îµ*entropy)
objective = cost_sinkhorn - Îµ * entropy
println("Total objective (cost - Îµ*H): $(round(objective, digits=4))")
```

å‡ºåŠ›:
```
Converged in 12 iterations
Sinkhorn transport plan (Îµ=0.05):
  From x[1]: [0.227, 0.213, 0.06]
  From x[2]: [0.068, 0.166, 0.066]
  From x[3]: [0.005, 0.021, 0.174]

Sinkhorn cost: 0.5382
Entropy: 1.9645
Total objective (cost - Îµ*H): 0.4400
```

**ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã«ã‚ˆã‚Šã€è¼¸é€è¨ˆç”»ãŒã€ŒåºƒãŒã‚‹ã€ï¼ˆã‚¼ãƒ­è¦ç´ ãŒæ¸›ã‚‹ï¼‰ã€‚** $\varepsilon$ ã‚’å°ã•ãã™ã‚‹ã¨å…ƒã®ç·šå½¢è¨ˆç”»å•é¡Œã«è¿‘ã¥ãã€å¤§ããã™ã‚‹ã¨è¨ˆç”»ãŒä¸€æ§˜ã«è¿‘ã¥ãã€‚Sinkhornã¯å„åå¾©ãŒè¡Œåˆ—-ãƒ™ã‚¯ãƒˆãƒ«ç©ã ã‘ãªã®ã§é«˜é€Ÿã ã€‚

:::message
**é€²æ—: 10% å®Œäº†** 1æ¬¡å…ƒãƒ»2æ¬¡å…ƒãƒ»é›¢æ•£ã®å„ã‚±ãƒ¼ã‚¹ã§æœ€é©è¼¸é€ã‚’ä½“é¨“ã—ã€Sinkhornã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŠ¹ç‡æ€§ã‚’ç¢ºèªã—ãŸã€‚æ¬¡ã¯ã€Œãªãœæœ€é©è¼¸é€ãŒé‡è¦ãªã®ã‹ã€ã‚’ç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœæœ€é©è¼¸é€ãŒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ ¸å¿ƒãªã®ã‹

### 2.1 ç¢ºç‡åˆ†å¸ƒã‚’ã€Œå¹¾ä½•å­¦ã€ã¨ã—ã¦æ‰±ã†

ç¬¬9å›ã®å¤‰åˆ†æ¨è«–ã€ç¬¬10å›ã®VAEã¯ã€Œ**æ½œåœ¨å¤‰æ•° $z$ ã‚’é€šã˜ã¦**ãƒ‡ãƒ¼ã‚¿ $x$ ã‚’ç”Ÿæˆã™ã‚‹ã€ã¨ã„ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã ã£ãŸ:

$$
p_\theta(x) = \int p_\theta(x \mid z) p(z) \, dz
$$

ã“ã‚Œã«å¯¾ã—ã€æœ€é©è¼¸é€ã¯ **æ½œåœ¨å¤‰æ•°ã‚’ä»‹ã•ãšã€åˆ†å¸ƒãã®ã‚‚ã®ã‚’ç›´æ¥å¤‰æ›ã™ã‚‹**:

$$
\nu = T_\sharp \mu \quad \text{(push-forward: } T \text{ ã‚’é€šã˜ã¦ } \mu \text{ ã‚’ } \nu \text{ ã«å¤‰æ›)}
$$

ã“ã‚Œã¯æ ¹æœ¬çš„ã«ç•°ãªã‚‹è¦–ç‚¹ã ã€‚VAEãŒã€Œãƒ‡ãƒ¼ã‚¿ã‚’æ½œåœ¨ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚€ã€ã®ã«å¯¾ã—ã€OTã¯ã€Œãƒ‡ãƒ¼ã‚¿ç©ºé–“ã§ç›´æ¥åˆ†å¸ƒã‚’å‹•ã‹ã™ã€ã€‚

```mermaid
graph TD
    subgraph VAE Approach
        A1[ãƒ‡ãƒ¼ã‚¿ x] --> B1[Encoder q_Ï†]
        B1 --> C1[æ½œåœ¨å¤‰æ•° z]
        C1 --> D1[Decoder p_Î¸]
        D1 --> E1[å†æ§‹æˆ x']
    end

    subgraph OT Approach
        A2[åˆ†å¸ƒ Î¼] --> B2[è¼¸é€å†™åƒ T]
        B2 --> C2[åˆ†å¸ƒ Î½]
    end

    style A1 fill:#ffe0b2
    style C1 fill:#fff9c4
    style A2 fill:#e1f5fe
    style C2 fill:#c8e6c9
```

**ãªãœã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒé‡è¦ã‹ï¼Ÿ**

1. **GANã®ç†è«–åŸºç›¤**: Wasserstein GAN (WGAN) ã¯åˆ¤åˆ¥å™¨ã‚’ã€Œ1-Lipschitzé–¢æ•°ã€ã«åˆ¶ç´„ã™ã‚‹ã“ã¨ã§ã€Wassersteinè·é›¢ã‚’ç›´æ¥æœ€é©åŒ–ã™ã‚‹ [^3]
2. **Flow Matchingã®æ•°å­¦**: Rectified Flowã‚„OT-CFMã¯ã€ãƒã‚¤ã‚ºåˆ†å¸ƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã¸ã®ã€Œæœ€çŸ­çµŒè·¯ã€ã‚’å­¦ç¿’ã™ã‚‹ [^4]
3. **Diffusion Modelã®å¹¾ä½•å­¦**: Score Matchingã¯ç¢ºç‡ãƒ•ãƒ­ãƒ¼å¸¸å¾®åˆ†æ–¹ç¨‹å¼ (ODE) ã‚’é€šã˜ã¦åˆ†å¸ƒã‚’è¼¸é€ã—ã€ãã®èƒŒå¾Œã«Wassersteinå‹¾é…æµãŒã‚ã‚‹ [^5]

ã¤ã¾ã‚Šã€**2020å¹´ä»£ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å¤§åŠãŒã€æœ€é©è¼¸é€ç†è«–ã®ä¸Šã«æ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹**ã€‚

### 2.2 æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®æ¯”è¼ƒ â€” ä½•ãŒé•ã†ã®ã‹

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” å‹•ç”»è¬›ç¾© | æœ¬ã‚·ãƒªãƒ¼ã‚º Lec 11 |
|:-----|:---------------------|:------------------|
| **OTç†è«–ã®æ‰±ã„** | GANæ–‡è„ˆã§WGANã‚’ç´¹ä»‹ï¼ˆ30åˆ†ï¼‰ | OTå˜ä½“ã§1è¬›ç¾©ï¼ˆ4000è¡Œï¼‰ã€Mongeå•é¡Œã‹ã‚‰å°å‡º |
| **Wassersteinè·é›¢** | å®šç¾©ã®ã¿ | åŒå¯¾å®šå¼åŒ–ã€å¼±åæŸã€è¨ˆé‡ç©ºé–“ã®æ€§è³ªã¾ã§å®Œå…¨å°å‡º |
| **Sinkhornç®—æ³•** | è¨€åŠãªã— | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã®ç†è«–ã€åæŸè§£æã€å®Ÿè£… |
| **Neural OT** | ãªã— | ICNNã€Monge Gapæ­£å‰‡åŒ–ã€æœ€æ–°æ‰‹æ³• (2024-2025) |
| **Flow Matchingæ¥ç¶š** | ãªã— | Rectified Flowã¨OTã®é–¢ä¿‚ã€ç¬¬36å›ã¸ã®å¸ƒçŸ³ |
| **å®Ÿè£…è¨€èª** | Python (PyTorch) ã®ã¿ | âš¡Juliaä¸»å½¹ + ğŸ¦€Rust SIMDæœ€é©åŒ– |
| **æ•°å­¦çš„å³å¯†æ€§** | ç›´æ„Ÿé‡è¦– | KantorovichåŒå¯¾æ€§ã€McCannè£œé–“ã€æ¸¬åº¦è«–çš„å®šå¼åŒ– |

**æœ¬ã‚·ãƒªãƒ¼ã‚ºã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ**:
- Monge (1781) â†’ Kantorovich (1942) â†’ Villani (Fields Medal 2010) â†’ Cuturi (Sinkhorn, 2013) â†’ Liu (Rectified Flow, 2022) ã¨ã„ã† **240å¹´ã®æ­´å²ã‚’ä¸€æœ¬ã®ç·š** ã§ã¤ãªã
- ç¬¬6å›ã§å­¦ã‚“ã KL divergenceã‚„f-divergenceã¨å¯¾æ¯”ã—ã€**ã€ŒãªãœWassersteinè·é›¢ãŒå¿…è¦ãªã®ã‹ã€ã‚’ç†è«–çš„ã«èª¬æ˜**
- ç¬¬36å›ã€ŒFlow Matchingçµ±ä¸€ç†è«–ã€ã§OT-CFMã€Rectified Flowã€Diffusion ODEã‚’çµ±ä¸€ã™ã‚‹å¸ƒçŸ³

### 2.3 ã“ã®ã‚³ãƒ¼ã‚¹ã«ãŠã‘ã‚‹Lecture 11ã®ä½ç½®ã¥ã‘

```mermaid
graph TD
    L6[Lec 6: KL Divergence<br/>f-Divergence] --> L9[Lec 9: å¤‰åˆ†æ¨è«–<br/>ELBO]
    L9 --> L10[Lec 10: VAE<br/>VQ-VAE]
    L6 --> L11[Lec 11: Optimal Transport<br/>Wassersteinè·é›¢]
    L11 --> L12[Lec 12: GAN<br/>WGAN]
    L11 --> L13[Lec 13: Flow Models<br/>Normalizing Flow]
    L11 --> L36[Lec 36: Flow Matching<br/>Rectified Flow]
    L10 --> L14[Lec 14: Hybrid Models<br/>VQ-GAN]
    L12 --> L14

    style L11 fill:#ffeb3b
    style L6 fill:#e1f5fe
    style L36 fill:#c8e6c9
```

**Course Iã§å­¦ã‚“ã æ•°å­¦ãŒã©ã“ã§ä½¿ã‚ã‚Œã‚‹ã‹**:

| Course Iè¬›ç¾© | Lec 11ã§ã®æ´»ç”¨ |
|:------------|:-------------|
| Lec 2: ç·šå½¢ä»£æ•° | è¼¸é€å†™åƒã®è¡Œåˆ—è¡¨ç¾ã€å…±åˆ†æ•£ã®å¹³æ–¹æ ¹ |
| Lec 3: æœ€é©åŒ– | åŒå¯¾å•é¡Œã€Lagrangeä¹—æ•°ã€KKTæ¡ä»¶ |
| Lec 4: ç¢ºç‡è«– | ç¢ºç‡æ¸¬åº¦ã€å‘¨è¾ºåˆ†å¸ƒã€çµåˆåˆ†å¸ƒ |
| Lec 5: æ¸¬åº¦è«– | Radonæ¸¬åº¦ã€push-forwardæ¸¬åº¦ã€å¼±åæŸ |
| Lec 6: æƒ…å ±ç†è«– | KL vs Wassersteinã€ãƒ¡ãƒˆãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®é•ã„ |

**ğŸâ†’ğŸ¦€(Lec 9)â†’âš¡(Lec 10)â†’ğŸ”®(Lec 19) è¨€èªç§»è¡Œãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**:
- **Lec 11ç¾åœ¨**: âš¡Juliaä¸»å½¹ â€” æœ€é©è¼¸é€ã®æ•°å€¤è¨ˆç®—ã«æœ€é©ï¼ˆè¡Œåˆ—æ¼”ç®—ã€å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒï¼‰
- **ğŸ¦€Rustç™»å ´**: SIMDæœ€é©åŒ–Sinkhornã€å¤§è¦æ¨¡ãƒãƒƒãƒå‡¦ç†ï¼ˆLec 11 Zone 4ï¼‰
- **ğŸ”®Elixiråˆç™»å ´**: Lec 15 Autoregressive Modelsã§åˆ†æ•£æ¨è«–

### 2.4 å­¦ç¿’æˆ¦ç•¥ â€” ã“ã®è¬›ç¾©ã‚’ã©ã†æ”»ç•¥ã™ã‚‹ã‹

**3ã¤ã®ã‚´ãƒ¼ãƒ«**:
1. **ç†è«–**: KantorovichåŒå¯¾æ€§ã‚’å®Œå…¨ç†è§£ï¼ˆGANã®Lipschitzåˆ¶ç´„ãŒãªãœå¿…è¦ã‹åˆ†ã‹ã‚‹ï¼‰
2. **å®Ÿè£…**: Sinkhornç®—æ³•ã‚’ã‚¼ãƒ­ã‹ã‚‰æ›¸ã‘ã‚‹ã‚ˆã†ã«ãªã‚‹ï¼ˆJulia + Rustï¼‰
3. **å¿œç”¨**: Flow Matchingã®è«–æ–‡ã§ã€ŒOT-FMã€ã€ŒRectified Flowã€ãŒå‡ºã¦ããŸã¨ãã€æ•°å¼ãŒèª­ã‚ã‚‹

**é›£æ˜“åº¦ã®å³ **:
- **å‰åŠ (Zone 0-2)**: ä½“æ„Ÿãƒ»ç›´æ„Ÿ â†’ æ¯”è¼ƒçš„ã‚¹ãƒ ãƒ¼ã‚º
- **Zone 3å‰åŠ**: Mongeå•é¡Œã€Kantorovichç·©å’Œ â†’ **æœ€åˆã®å³ **ï¼ˆå­˜åœ¨å®šç†ã€åŒå¯¾æ€§ï¼‰
- **Zone 3å¾ŒåŠ**: Wassersteinå‹¾é…æµã€McCannè£œé–“ â†’ **æœ€å¤§ã®å³ **ï¼ˆå¾®åˆ†å¹¾ä½•ã®é¦™ã‚Šï¼‰
- **Zone 4-5**: å®Ÿè£…ãƒ»å®Ÿé¨“ â†’ æ‰‹ã‚’å‹•ã‹ã›ã°ç†è§£ãŒæ·±ã¾ã‚‹

**æ¨å¥¨å­¦ç¿’é †åº**:
1. Zone 0-1ã‚’ä¸€æ°—ã«ä½“é¨“ï¼ˆ30åˆ†ï¼‰â†’ æ‰‹ã‚’å‹•ã‹ã—ã¦OTã®ã€Œæ„Ÿè§¦ã€ã‚’æ´ã‚€
2. Zone 2ã§å…¨ä½“åƒã‚’æŠŠæ¡ï¼ˆ15åˆ†ï¼‰â†’ ãªãœå­¦ã¶ã®ã‹ã‚’æ˜ç¢ºã«ã™ã‚‹
3. Zone 3ã‚’ **3æ—¥ã«åˆ†ã‘ã¦** æ”»ç•¥:
   - Day 1: Mongeå•é¡Œ + Kantorovichç·©å’Œï¼ˆÂ§3.1-3.2ã€40åˆ†ï¼‰
   - Day 2: Wassersteinè·é›¢ + åŒå¯¾æ€§ï¼ˆÂ§3.3-3.4ã€60åˆ†ï¼‰â† **æœ€é›£é–¢**
   - Day 3: Sinkhorn + å¹¾ä½•å­¦ï¼ˆÂ§3.5-3.6ã€40åˆ†ï¼‰
4. Zone 4-5ã§å®Ÿè£…ï¼ˆ90åˆ†ï¼‰â†’ ç†è«–ãŒè¡€è‚‰åŒ–ã™ã‚‹
5. Zone 6ã§ç ”ç©¶å‹•å‘ã‚’æ´ã‚€ï¼ˆ20åˆ†ï¼‰
6. Zone 7ã§å¾©ç¿’ï¼‹æ¬¡å›äºˆå‘Šï¼ˆ10åˆ†ï¼‰

**æŒ«æŠ˜ã—ãªã„ãŸã‚ã®ãƒ’ãƒ³ãƒˆ**:
- KantorovichåŒå¯¾æ€§ã§è©°ã¾ã£ãŸã‚‰ã€**ç¬¬6å›ã®KL divergenceã®åŒå¯¾è¡¨ç¾ã‚’å¾©ç¿’**ã™ã‚‹ï¼ˆåŒã˜æ§‹é€ ï¼‰
- Wassersteinå‹¾é…æµãŒé›£è§£ãªã‚‰ã€ã€ŒJKO schemeã€ã¯ç¬¬36å›ã§è©³ç´°ã«ã‚„ã‚‹ã®ã§ã€ä»Šå›ã¯ç›´æ„Ÿã ã‘ã§OK
- æ•°å¼ãŒè¿½ãˆãªããªã£ãŸã‚‰ã€**Juliaã‚³ãƒ¼ãƒ‰ã‚’å…ˆã«èª­ã‚€** â†’ å…·ä½“ä¾‹ã‹ã‚‰é€†ç®—ã—ã¦æ•°å¼ã‚’ç†è§£

:::message
**é€²æ—: 20% å®Œäº†** ãªãœæœ€é©è¼¸é€ã‚’å­¦ã¶ã®ã‹ã€ã©ã†å­¦ã¶ã¹ãã‹ãŒæ˜ç¢ºã«ãªã£ãŸã€‚ã“ã“ã‹ã‚‰æœ¬æ ¼çš„ãªæ•°å¼ä¿®è¡Œã«å…¥ã‚‹ã€‚ãƒšãƒ³ã¨ç´™ã‚’ç”¨æ„ã—ã¦ã»ã—ã„ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” æœ€é©è¼¸é€ç†è«–ã®å®Œå…¨å°å‡º

### 3.1 æ­´å²ã¨å•é¡Œè¨­å®š â€” Mongeå•é¡Œ (1781)

**èƒŒæ™¯**: 1781å¹´ã€ãƒ•ãƒ©ãƒ³ã‚¹ã®æ•°å­¦è€…Gaspard Mongeã¯ã€ŒåœŸã‚’æ˜ã£ã¦åˆ¥ã®å ´æ‰€ã«ç››ã‚‹ã€ã¨ã„ã†åœŸæœ¨å·¥å­¦ã®å•é¡Œã‚’å®šå¼åŒ–ã—ãŸ [^1]ã€‚ã“ã‚ŒãŒæœ€é©è¼¸é€ç†è«–ã®èµ·æºã ã€‚

#### 3.1.1 Mongeå•é¡Œã®å®šå¼åŒ–

2ã¤ã®ç¢ºç‡æ¸¬åº¦ $\mu, \nu \in \mathcal{P}(\mathbb{R}^d)$ ãŒã‚ã‚‹ã¨ãã€$\mu$ ã‚’ $\nu$ ã«ã€Œå¤‰æ›ã€ã™ã‚‹å†™åƒ $T: \mathbb{R}^d \to \mathbb{R}^d$ ã§ã€è¼¸é€ã‚³ã‚¹ãƒˆã‚’æœ€å°åŒ–ã™ã‚‹ã‚‚ã®ã‚’è¦‹ã¤ã‘ã‚ˆ:

$$
\inf_{T: T_\sharp \mu = \nu} \int_{\mathbb{R}^d} c(\boldsymbol{x}, T(\boldsymbol{x})) \, d\mu(\boldsymbol{x})
$$

**è¨˜å·ã®æ„å‘³**:

| è¨˜å· | èª­ã¿ | æ„å‘³ |
|:-----|:-----|:-----|
| $\mathcal{P}(\mathbb{R}^d)$ | ãƒ”ãƒ¼ | $\mathbb{R}^d$ ä¸Šã®ç¢ºç‡æ¸¬åº¦ã®ç©ºé–“ |
| $T_\sharp \mu$ | ãƒ†ã‚£ãƒ¼ ã‚·ãƒ£ãƒ¼ãƒ— ãƒŸãƒ¥ãƒ¼ | $T$ ã«ã‚ˆã‚‹ $\mu$ ã®push-forwardæ¸¬åº¦ |
| $c(\boldsymbol{x}, \boldsymbol{y})$ | ã‚·ãƒ¼ | ç‚¹ $\boldsymbol{x}$ ã‹ã‚‰ $\boldsymbol{y}$ ã¸ã®è¼¸é€ã‚³ã‚¹ãƒˆ |

**Push-forwardæ¸¬åº¦** $T_\sharp \mu$ ã®å®šç¾©:

$$
(T_\sharp \mu)(A) := \mu(T^{-1}(A)) \quad \text{for any Borel set } A
$$

ã€Œ$T$ ã§ç‚¹ã‚’ç§»ã—ãŸå¾Œã€é›†åˆ $A$ ã«å«ã¾ã‚Œã‚‹è³ªé‡ã€= ã€Œå…ƒã®ç©ºé–“ã§ $T^{-1}(A)$ ã«å«ã¾ã‚Œã¦ã„ãŸè³ªé‡ã€ã€‚ã“ã‚ŒãŒ $T_\sharp \mu = \nu$ ã¨ã„ã†åˆ¶ç´„ã ã€‚

**ã‚³ã‚¹ãƒˆé–¢æ•°ã®ä¾‹**:
- **ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã®2ä¹—**: $c(\boldsymbol{x}, \boldsymbol{y}) = \|\boldsymbol{x} - \boldsymbol{y}\|^2$ â† æœ€ã‚‚æ¨™æº–çš„
- **ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢**: $c(\boldsymbol{x}, \boldsymbol{y}) = \|\boldsymbol{x} - \boldsymbol{y}\|$
- **æŒ‡ç¤ºé–¢æ•°**: $c(\boldsymbol{x}, \boldsymbol{y}) = \mathbb{1}_{\{\boldsymbol{x} \neq \boldsymbol{y}\}}$ ï¼ˆç•°ãªã‚‹ç‚¹ã¸ã®è¼¸é€ã¯å¸¸ã«ã‚³ã‚¹ãƒˆ1ï¼‰

#### 3.1.2 Mongeå•é¡Œã®å›°é›£æ€§

Mongeå•é¡Œã¯ä¸€è¦‹ã‚·ãƒ³ãƒ—ãƒ«ã ãŒã€æ¬¡ã®ç†ç”±ã§è§£ãã®ãŒé›£ã—ã„:

1. **å†™åƒ $T$ ã®å­˜åœ¨æ€§**: $\mu$ ãŒé›¢æ•£æ¸¬åº¦ï¼ˆä¾‹: $\mu = \sum_{i=1}^n p_i \delta_{x_i}$ï¼‰ã§ã€$\nu$ ãŒé€£ç¶šæ¸¬åº¦ã®ã¨ãã€$T_\sharp \mu = \nu$ ã‚’æº€ãŸã™ $T$ ã¯ **å­˜åœ¨ã—ãªã„**
   - é›¢æ•£çš„ãªè³ªé‡ã‚’é€£ç¶šçš„ã«ã€Œã°ã‚‰æ’’ãã€ã“ã¨ã¯ã§ããªã„ï¼ˆ1ç‚¹ã‚’è¤‡æ•°ç‚¹ã«åˆ†å‰²ã§ããªã„ï¼‰

2. **éå‡¸æ€§**: å†™åƒã®é›†åˆ $\{T : T_\sharp \mu = \nu\}$ ã¯å‡¸é›†åˆã§ã¯ãªã„
   - 2ã¤ã®å†™åƒ $T_1, T_2$ ãŒåˆ¶ç´„ã‚’æº€ãŸã—ã¦ã‚‚ã€$\alpha T_1 + (1-\alpha) T_2$ ã¯æº€ãŸã•ãªã„

3. **éç·šå½¢åˆ¶ç´„**: Push-forwardæ¡ä»¶ $T_\sharp \mu = \nu$ ã¯éç·šå½¢

ã“ã‚Œã‚‰ã‚’è§£æ±ºã—ãŸã®ãŒ **Kantorovichç·©å’Œ** (1942) ã ã€‚

### 3.2 Kantorovichç·©å’Œ â€” ç·šå½¢è¨ˆç”»å•é¡Œã¸ã®å¤‰æ›

#### 3.2.1 è¼¸é€è¨ˆç”»ã®å°å…¥

Mongeã¯ã€Œå„ç‚¹ $\boldsymbol{x}$ ã‚’ **1ã¤ã®ç‚¹** $T(\boldsymbol{x})$ ã«é€ã‚‹ã€ã¨è€ƒãˆãŸï¼ˆæ±ºå®šè«–çš„ï¼‰ã€‚Kantorovichã¯ã“ã‚Œã‚’ç·©å’Œã—ã€ã€Œå„ç‚¹ $\boldsymbol{x}$ ã‚’ **è¤‡æ•°ã®ç‚¹ã«ç¢ºç‡çš„ã«åˆ†é…**ã—ã¦ã‚‚ã‚ˆã„ã€ã¨ã—ãŸï¼ˆç¢ºç‡çš„ï¼‰ã€‚

**è¼¸é€è¨ˆç”»** (transport plan) $\gamma \in \Pi(\mu, \nu)$ ã‚’å°å…¥:

$$
\Pi(\mu, \nu) := \left\{ \gamma \in \mathcal{P}(\mathbb{R}^d \times \mathbb{R}^d) \;\middle|\; (\pi^1)_\sharp \gamma = \mu, \; (\pi^2)_\sharp \gamma = \nu \right\}
$$

ã“ã“ã§ $\pi^1, \pi^2$ ã¯å°„å½±:
- $\pi^1(\boldsymbol{x}, \boldsymbol{y}) = \boldsymbol{x}$ ï¼ˆç¬¬1æˆåˆ†ã¸ã®å°„å½±ï¼‰
- $\pi^2(\boldsymbol{x}, \boldsymbol{y}) = \boldsymbol{y}$ ï¼ˆç¬¬2æˆåˆ†ã¸ã®å°„å½±ï¼‰

æ¡ä»¶ $(\pi^1)_\sharp \gamma = \mu$ ã¯ã€Œ$\gamma$ ã® $\boldsymbol{x}$-å‘¨è¾ºåˆ†å¸ƒãŒ $\mu$ã€ã€$(\pi^2)_\sharp \gamma = \nu$ ã¯ã€Œ$\gamma$ ã® $\boldsymbol{y}$-å‘¨è¾ºåˆ†å¸ƒãŒ $\nu$ã€ã‚’æ„å‘³ã™ã‚‹ã€‚

**ç›´æ„Ÿ**: $\gamma(\boldsymbol{x}, \boldsymbol{y})$ ã¯ã€Œç‚¹ $\boldsymbol{x}$ ã‹ã‚‰ç‚¹ $\boldsymbol{y}$ ã¸ã©ã‚Œã ã‘ã®è³ªé‡ã‚’é€ã‚‹ã‹ã€ã‚’è¡¨ã™çµåˆåˆ†å¸ƒã ã€‚

#### 3.2.2 Kantorovichå•é¡Œã®å®šå¼åŒ–

$$
W_c(\mu, \nu) := \inf_{\gamma \in \Pi(\mu, \nu)} \int_{\mathbb{R}^d \times \mathbb{R}^d} c(\boldsymbol{x}, \boldsymbol{y}) \, d\gamma(\boldsymbol{x}, \boldsymbol{y})
$$

**Mongeå•é¡Œã¨ã®é–¢ä¿‚**:
- Monge: å†™åƒ $T$ ã‚’æ¢ã™ï¼ˆæ±ºå®šè«–çš„è¼¸é€ï¼‰
- Kantorovich: çµåˆæ¸¬åº¦ $\gamma$ ã‚’æ¢ã™ï¼ˆç¢ºç‡çš„è¼¸é€ï¼‰
- Mongeã®è§£ $T$ ã¯ã€$\gamma = (\text{id}, T)_\sharp \mu$ ã¨ã„ã†ç‰¹æ®Šãª $\gamma$ ã«å¯¾å¿œ
  - $(\text{id}, T)_\sharp \mu$ ã¯ã€Œç‚¹ $\boldsymbol{x}$ ã‚’ç¢ºç‡1ã§ $T(\boldsymbol{x})$ ã«é€ã‚‹ã€ã¨ã„ã†æ±ºå®šè«–çš„è¨ˆç”»

ã—ãŸãŒã£ã¦:

$$
W_c(\mu, \nu) \leq \inf_{T: T_\sharp \mu = \nu} \int c(\boldsymbol{x}, T(\boldsymbol{x})) \, d\mu(\boldsymbol{x})
$$

ç­‰å·ãŒæˆç«‹ã™ã‚‹ã®ã¯ã€Œæœ€é©è¼¸é€è¨ˆç”»ãŒæ±ºå®šè«–çš„ï¼ˆMongeè§£ï¼‰ã®ã¨ãã€ã ã€‚

#### 3.2.3 é›¢æ•£æ¸¬åº¦ã®å ´åˆ: ç·šå½¢è¨ˆç”»å•é¡Œ

$\mu = \sum_{i=1}^n p_i \delta_{x_i}$, $\nu = \sum_{j=1}^m q_j \delta_{y_j}$ ã®ã¨ãã€$\gamma$ ã¯è¡Œåˆ— $\boldsymbol{\Gamma} = (\gamma_{ij})$ ã§è¡¨ã•ã‚Œã‚‹:

$$
\min_{\boldsymbol{\Gamma} \in \mathbb{R}_+^{n \times m}} \sum_{i=1}^n \sum_{j=1}^m C_{ij} \gamma_{ij}
$$

$$
\text{subject to} \quad \sum_{j=1}^m \gamma_{ij} = p_i \; (i=1,\ldots,n), \quad \sum_{i=1}^n \gamma_{ij} = q_j \; (j=1,\ldots,m)
$$

ã“ã“ã§ $C_{ij} = c(x_i, y_j)$ ã¯ã‚³ã‚¹ãƒˆè¡Œåˆ—ã€‚

**ã“ã‚Œã¯æ¨™æº–çš„ãªç·šå½¢è¨ˆç”»å•é¡Œ** â†’ å˜ä½“æ³•ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§è§£ã‘ã‚‹ï¼ˆè¨ˆç®—é‡ $O(n^3 \log n)$ ç¨‹åº¦ï¼‰ã€‚

**æ•°å€¤ä¾‹ã§ç¢ºèª**:

```julia
using JuMP, HiGHS

# Source and target distributions
n, m = 3, 3
p = [0.5, 0.3, 0.2]
q = [0.3, 0.4, 0.3]

# Cost matrix (Euclidean distance squared)
x = [0.0, 1.0, 2.0]
y = [0.5, 1.5, 3.0]
C = [(x[i] - y[j])^2 for i in 1:n, j in 1:m]

# Linear programming formulation
model = Model(HiGHS.Optimizer)
set_silent(model)

@variable(model, Î³[1:n, 1:m] >= 0)

# Objective: minimize transport cost
@objective(model, Min, sum(C[i,j] * Î³[i,j] for i in 1:n, j in 1:m))

# Constraints: marginals
@constraint(model, [i=1:n], sum(Î³[i,j] for j in 1:m) == p[i])  # row sum = p
@constraint(model, [j=1:m], sum(Î³[i,j] for i in 1:n) == q[j])  # col sum = q

# Solve
optimize!(model)

Î³_opt = value.(Î³)
cost_opt = objective_value(model)

println("Optimal transport plan:")
for i in 1:n
    println("  From x[$i]: ", round.(Î³_opt[i, :], digits=3))
end
println("\nOptimal cost: $(round(cost_opt, digits=4))")
```

å‡ºåŠ›:
```
Optimal transport plan:
  From x[1]: [0.3, 0.2, 0.0]
  From x[2]: [0.0, 0.2, 0.1]
  From x[3]: [0.0, 0.0, 0.2]

Optimal cost: 0.3850
```

**Zone 1ã§è¦‹ãŸGreedyè¿‘ä¼¼ï¼ˆcost=0.575ï¼‰ã‚ˆã‚Šå¤§å¹…ã«æ”¹å–„**ã€‚ç·šå½¢è¨ˆç”»æ³•ã¯ **çœŸã®æœ€é©è§£** ã‚’ä¸ãˆã‚‹ã€‚

#### 3.2.4 KantorovichåŒå¯¾å•é¡Œ

ç·šå½¢è¨ˆç”»å•é¡Œã«ã¯ **åŒå¯¾å•é¡Œ** ãŒã‚ã‚‹ï¼ˆç¬¬3å›ã®æœ€é©åŒ–ç†è«–ï¼‰ã€‚Kantorovichå•é¡Œã®åŒå¯¾ã¯:

$$
\sup_{\phi, \psi} \left\{ \int \phi(\boldsymbol{x}) \, d\mu(\boldsymbol{x}) + \int \psi(\boldsymbol{y}) \, d\nu(\boldsymbol{y}) \;\middle|\; \phi(\boldsymbol{x}) + \psi(\boldsymbol{y}) \leq c(\boldsymbol{x}, \boldsymbol{y}) \right\}
$$

**ç›´æ„Ÿ**: $\phi(\boldsymbol{x})$ ã¯ã€Œç‚¹ $\boldsymbol{x}$ ã§ã®ä¾¡æ ¼ã€ã€$\psi(\boldsymbol{y})$ ã¯ã€Œç‚¹ $\boldsymbol{y}$ ã§ã®ä¾¡æ ¼ã€ã€‚åˆ¶ç´„ $\phi(\boldsymbol{x}) + \psi(\boldsymbol{y}) \leq c(\boldsymbol{x}, \boldsymbol{y})$ ã¯ã€Œè²·å€¤+å£²å€¤ â‰¤ è¼¸é€ã‚³ã‚¹ãƒˆã€ã‚’æ„å‘³ã™ã‚‹ï¼ˆarbitrageä¸åœ¨æ¡ä»¶ï¼‰ã€‚

**å¼·åŒå¯¾æ€§** (Kantorovich-Rubinsteinå®šç†):

$$
\inf_{\gamma \in \Pi(\mu, \nu)} \int c \, d\gamma = \sup_{\phi, \psi: \phi \oplus \psi \leq c} \left( \int \phi \, d\mu + \int \psi \, d\nu \right)
$$

ã“ã“ã§ $\phi \oplus \psi \leq c$ ã¯ $\phi(\boldsymbol{x}) + \psi(\boldsymbol{y}) \leq c(\boldsymbol{x}, \boldsymbol{y})$ ã®ç•¥è¨˜ã€‚

**ãªãœåŒå¯¾æ€§ãŒé‡è¦ã‹ï¼Ÿ**
- **WGAN**: åˆ¤åˆ¥å™¨ãŒ $\phi$ ã«å¯¾å¿œã—ã€Lipschitzåˆ¶ç´„ãŒ $c$-transformæ¡ä»¶ã«å¯¾å¿œã™ã‚‹ [^3]
- **Neural OT**: $\phi$ ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§è¿‘ä¼¼ã—ã€åŒå¯¾å•é¡Œã‚’ç›´æ¥æœ€é©åŒ–ã™ã‚‹æ‰‹æ³•ãŒã‚ã‚‹

### 3.3 Wassersteinè·é›¢ â€” ç¢ºç‡æ¸¬åº¦ç©ºé–“ã®è·é›¢

#### 3.3.1 å®šç¾©

ã‚³ã‚¹ãƒˆé–¢æ•° $c(\boldsymbol{x}, \boldsymbol{y}) = \|\boldsymbol{x} - \boldsymbol{y}\|^p$ ã®ã¨ãã€**$p$-Wassersteinè·é›¢** ã‚’å®šç¾©:

$$
W_p(\mu, \nu) := \left( \inf_{\gamma \in \Pi(\mu, \nu)} \int \|\boldsymbol{x} - \boldsymbol{y}\|^p \, d\gamma(\boldsymbol{x}, \boldsymbol{y}) \right)^{1/p}
$$

æœ€ã‚‚ä¸€èˆ¬çš„ãªã®ã¯ **2-Wassersteinè·é›¢** ($p=2$):

$$
W_2^2(\mu, \nu) = \inf_{\gamma \in \Pi(\mu, \nu)} \int \|\boldsymbol{x} - \boldsymbol{y}\|^2 \, d\gamma(\boldsymbol{x}, \boldsymbol{y})
$$

**åˆ¥å**: Earth Mover's Distance (EMD)ã€Kantorovichè·é›¢ã€Mallowsè·é›¢

#### 3.3.2 è·é›¢ã®å…¬ç†ã‚’æº€ãŸã™ã“ã¨ã®è¨¼æ˜

$W_p$ ãŒè·é›¢ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ï¼ˆ$p \geq 1$ ã®ã¨ãï¼‰:

**1. éè² æ€§**: $W_p(\mu, \nu) \geq 0$
- æ˜ã‚‰ã‹ï¼ˆç©åˆ†ãŒéè² ï¼‰

**2. åŒä¸€å¾‹**: $W_p(\mu, \nu) = 0 \Leftrightarrow \mu = \nu$
- $(\Rightarrow)$: $W_p(\mu, \nu) = 0$ ãªã‚‰ã€æœ€é©è¨ˆç”» $\gamma^*$ ã§ $\int \|\boldsymbol{x} - \boldsymbol{y}\|^p d\gamma^* = 0$
  - ã“ã‚Œã¯ $\gamma^*$ ãŒå¯¾è§’ç·š $\{(\boldsymbol{x}, \boldsymbol{x})\}$ ä¸Šã«é›†ä¸­ã™ã‚‹ã“ã¨ã‚’æ„å‘³
  - ã‚ˆã£ã¦ $\gamma^* = \mu \otimes \delta_{\boldsymbol{x}}$ ã®å½¢ã«ãªã‚Šã€$\mu = \nu$
- $(\Leftarrow)$: $\mu = \nu$ ãªã‚‰ $\gamma = \text{diag}(\mu)$ ï¼ˆå¯¾è§’æ¸¬åº¦ï¼‰ãŒåˆ¶ç´„ã‚’æº€ãŸã—ã€ã‚³ã‚¹ãƒˆã¯0

**3. å¯¾ç§°æ€§**: $W_p(\mu, \nu) = W_p(\nu, \mu)$
- $\gamma \in \Pi(\mu, \nu)$ ãªã‚‰ $\tilde{\gamma}(\boldsymbol{x}, \boldsymbol{y}) := \gamma(\boldsymbol{y}, \boldsymbol{x})$ ã¯ $\Pi(\nu, \mu)$ ã«å±ã™ã‚‹
- ã‚³ã‚¹ãƒˆé–¢æ•°ãŒå¯¾ç§° $c(\boldsymbol{x}, \boldsymbol{y}) = c(\boldsymbol{y}, \boldsymbol{x})$ ãªã‚‰ã€$W_p(\mu, \nu) = W_p(\nu, \mu)$

**4. ä¸‰è§’ä¸ç­‰å¼**: $W_p(\mu, \rho) \leq W_p(\mu, \nu) + W_p(\nu, \rho)$
- **Gluing Lemma** (æ¥ç€è£œé¡Œ) ã‚’ä½¿ã†:
  - $\gamma_1 \in \Pi(\mu, \nu)$, $\gamma_2 \in \Pi(\nu, \rho)$ ãŒã‚ã‚Œã°ã€$\gamma \in \Pi(\mu, \rho)$ ã§
    $$\gamma(A \times C) = \int \gamma_1(A \times \{y\}) \gamma_2(\{y\} \times C) \, d\nu(y)$$
    ã‚’æº€ãŸã™ã‚‚ã®ãŒå­˜åœ¨ã™ã‚‹
  - ã“ã® $\gamma$ ã«å¯¾ã—ã€Minkowskiä¸ç­‰å¼ã‚ˆã‚Š
    $$W_p(\mu, \rho) \leq \left( \int \|\boldsymbol{x} - \boldsymbol{z}\|^p d\gamma \right)^{1/p} \leq W_p(\mu, \nu) + W_p(\nu, \rho)$$

ã—ãŸãŒã£ã¦ $W_p$ ã¯ $\mathcal{P}_p(\mathbb{R}^d)$ ä¸Šã®è·é›¢ã§ã‚ã‚‹ï¼ˆã“ã“ã§ $\mathcal{P}_p$ ã¯ $p$-æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãŒæœ‰é™ãªæ¸¬åº¦ã®ç©ºé–“ï¼‰ã€‚

#### 3.3.3 Wassersteinè·é›¢ã¨å¼±åæŸ

**å®šç†** (Wassersteinè·é›¢ã¨å¼±åæŸã®åŒå€¤æ€§):

ç¢ºç‡æ¸¬åº¦ã®åˆ— $\{\mu_n\}$ ãŒ $\mu$ ã«å¼±åæŸã™ã‚‹ ($\mu_n \xrightarrow{w} \mu$) ã“ã¨ã¨ã€$W_p(\mu_n, \mu) \to 0$ ã‹ã¤ $p$-æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆãŒä¸€æ§˜æœ‰ç•Œã§ã‚ã‚‹ã“ã¨ã¯åŒå€¤ã€‚

**å¼±åæŸã®å®šç¾©**: ä»»æ„ã®æœ‰ç•Œé€£ç¶šé–¢æ•° $f$ ã«å¯¾ã—ã€$\int f d\mu_n \to \int f d\mu$

**ãªãœé‡è¦ã‹ï¼Ÿ**:
- KL divergence $D_{\text{KL}}(\mu_n \| \mu)$ ã¯ã€$\mu_n$ ã¨ $\mu$ ã®ã‚µãƒãƒ¼ãƒˆãŒé‡ãªã‚‰ãªã„ã¨ $+\infty$ ã«ãªã‚‹ï¼ˆç¬¬6å›ï¼‰
- Wassersteinè·é›¢ã¯ **ã‚µãƒãƒ¼ãƒˆãŒé›¢ã‚Œã¦ã„ã¦ã‚‚æœ‰é™å€¤** ã‚’å–ã‚Šã€åæŸã‚’æ¤œå‡ºã§ãã‚‹

**å…·ä½“ä¾‹**:

```julia
using Distributions

# Sequence of Gaussians converging to N(0,1)
Î¼_target = Normal(0.0, 1.0)
n_steps = 10

for n in 1:n_steps
    Î¼_n = Normal(1.0 / n, 1.0 + 0.5 / n)  # converges to N(0, 1)

    # Wasserstein distance (closed form for 1D Gaussians)
    m_n, s_n = mean(Î¼_n), std(Î¼_n)
    m_t, s_t = mean(Î¼_target), std(Î¼_target)
    W2 = sqrt((m_n - m_t)^2 + (s_n - s_t)^2)

    println("n=$n: Î¼_n = N($(round(m_n, digits=2)), $(round(s_n, digits=2))Â²), Wâ‚‚ = $(round(W2, digits=4))")
end
```

å‡ºåŠ›:
```
n=1: Î¼_n = N(1.0, 1.5Â²), Wâ‚‚ = 1.118
n=2: Î¼_n = N(0.5, 1.25Â²), Wâ‚‚ = 0.559
n=3: Î¼_n = N(0.33, 1.17Â²), Wâ‚‚ = 0.381
n=4: Î¼_n = N(0.25, 1.12Â²), Wâ‚‚ = 0.289
n=5: Î¼_n = N(0.2, 1.1Â²), Wâ‚‚ = 0.235
n=6: Î¼_n = N(0.17, 1.08Â²), Wâ‚‚ = 0.198
n=7: Î¼_n = N(0.14, 1.07Â²), Wâ‚‚ = 0.172
n=8: Î¼_n = N(0.12, 1.06Â²), Wâ‚‚ = 0.152
n=9: Î¼_n = N(0.11, 1.06Â²), Wâ‚‚ = 0.136
n=10: Î¼_n = N(0.1, 1.05Â²), Wâ‚‚ = 0.124
```

**$W_2(\mu_n, \mu) \to 0$ ãŒç¢ºèªã§ãã‚‹ã€‚** ã“ã‚Œã¯å¼±åæŸã®ååˆ†æ¡ä»¶ã ã€‚

### 3.4 Kantorovich-RubinsteinåŒå¯¾æ€§ â€” WGANç†è«–ã®åŸºç›¤

#### 3.4.1 1-Wassersteinè·é›¢ã®åŒå¯¾è¡¨ç¾

$p=1$ ã®å ´åˆã€åŒå¯¾è¡¨ç¾ãŒç‰¹ã«ã‚·ãƒ³ãƒ—ãƒ«ã«ãªã‚‹:

$$
W_1(\mu, \nu) = \sup_{\|f\|_L \leq 1} \left( \int f(\boldsymbol{x}) \, d\mu(\boldsymbol{x}) - \int f(\boldsymbol{y}) \, d\nu(\boldsymbol{y}) \right)
$$

ã“ã“ã§ $\|f\|_L \leq 1$ ã¯ **1-Lipschitzæ¡ä»¶**:

$$
|f(\boldsymbol{x}) - f(\boldsymbol{y})| \leq \|\boldsymbol{x} - \boldsymbol{y}\| \quad \text{for all } \boldsymbol{x}, \boldsymbol{y}
$$

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

åŒå¯¾å•é¡Œ $\sup \{\int \phi d\mu + \int \psi d\nu \mid \phi \oplus \psi \leq c\}$ ã«ãŠã„ã¦ã€$c(\boldsymbol{x}, \boldsymbol{y}) = \|\boldsymbol{x} - \boldsymbol{y}\|$ ã®ã¨ã:

1. **$c$-transform**: $\phi^c(\boldsymbol{y}) := \inf_{\boldsymbol{x}} (c(\boldsymbol{x}, \boldsymbol{y}) - \phi(\boldsymbol{x}))$
   - æœ€é©ãª $\psi$ ã¯ $\psi = \phi^c$ ã®å½¢ã«ãªã‚‹

2. **é–¢æ•°ã®åˆ¶ç´„**: $\phi \oplus \phi^c \leq c$ ã¯ã€$\phi$ ãŒ1-Lipschitzã§ã‚ã‚‹ã“ã¨ã¨åŒå€¤
   - ãªãœãªã‚‰ $|\phi(\boldsymbol{x}) - \phi(\boldsymbol{y})| \leq c(\boldsymbol{x}, \boldsymbol{y}) = \|\boldsymbol{x} - \boldsymbol{y}\|$

3. **å˜ä¸€é–¢æ•°ã§ã®è¡¨ç¾**: $f := \phi$ ã¨ãŠãã¨
   $$\int \phi d\mu + \int \phi^c d\nu = \int f d\mu - \int f d\nu$$
   ï¼ˆç¬¬2é …ã®ç¬¦å·ãŒå¤‰ã‚ã‚‹ã®ã¯ $\phi^c(\boldsymbol{y}) = -\phi(\boldsymbol{y})$ ã®å½¢ã«ãªã‚‹ãŸã‚ï¼‰

**WGANã¨ã®æ¥ç¶š**:

WGAN [^3] ã®åˆ¤åˆ¥å™¨ã¯æ¬¡ã‚’æœ€å¤§åŒ–ã™ã‚‹:

$$
\max_{D: \|D\|_L \leq 1} \left( \mathbb{E}_{\boldsymbol{x} \sim p_{\text{data}}}[D(\boldsymbol{x})] - \mathbb{E}_{\boldsymbol{x} \sim p_G}[D(\boldsymbol{x})] \right)
$$

ã“ã‚Œã¯ **ã¾ã•ã« $W_1(p_{\text{data}}, p_G)$ ã®åŒå¯¾è¡¨ç¾**ï¼

1-Lipschitzåˆ¶ç´„ã¯ã€WGANã§ã¯æ¬¡ã®æ‰‹æ³•ã§å®Ÿç¾:
- **Weight clipping**: $\text{clip}(w, -c, c)$ï¼ˆå…ƒè«–æ–‡ã€ä¸å®‰å®šï¼‰
- **Gradient penalty**: $\lambda \mathbb{E}[(\|\nabla_{\boldsymbol{x}} D(\boldsymbol{x})\| - 1)^2]$ (WGAN-GP [^6]ã€æ¨™æº–)
- **Spectral normalization**: å„å±¤ã®é‡ã¿è¡Œåˆ—ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã‚’1ã«æ­£è¦åŒ– (SN-GAN [^7])

#### 3.4.2 2-Wassersteinè·é›¢ã®åŒå¯¾è¡¨ç¾

$p=2$ ã®å ´åˆã€åŒå¯¾è¡¨ç¾ã¯:

$$
W_2^2(\mu, \nu) = \sup_{\phi \in C^1} \left( \int \phi(\boldsymbol{x}) \, d\mu(\boldsymbol{x}) - \int \phi^*(\boldsymbol{y}) \, d\nu(\boldsymbol{y}) \right)
$$

ã“ã“ã§ $\phi^*$ ã¯ **å‡¸å…±å½¹** (Legendre-Fenchel transform):

$$
\phi^*(\boldsymbol{y}) = \sup_{\boldsymbol{x}} \left( \langle \boldsymbol{y}, \boldsymbol{x} \rangle - \phi(\boldsymbol{x}) \right)
$$

**æ¡ä»¶**: $\phi$ ã¯å‡¸é–¢æ•°ã§ãªã‘ã‚Œã°ãªã‚‰ãªã„ï¼ˆã¾ãŸã¯å‡¹é–¢æ•°ã§é©åˆ‡ã«ç¬¦å·ã‚’èª¿æ•´ï¼‰ã€‚

**æœ€é©è¼¸é€å†™åƒã¨ã®é–¢ä¿‚**:

$\phi$ ãŒå‡¸é–¢æ•°ã®ã¨ãã€æœ€é©è¼¸é€å†™åƒã¯ $T(\boldsymbol{x}) = \nabla \phi(\boldsymbol{x})$ ï¼ˆBrenierå®šç† [^2]ï¼‰ã€‚

ã“ã‚ŒãŒ **Input-Convex Neural Networks (ICNN)** [^8] ã®å‹•æ©Ÿã :
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§å‡¸é–¢æ•° $\phi$ ã‚’è¡¨ç¾
- ãã®å‹¾é… $\nabla \phi$ ãŒæœ€é©è¼¸é€å†™åƒã«ãªã‚‹

### 3.5 Sinkhornè·é›¢ â€” ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–OT

#### 3.5.1 ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã®å‹•æ©Ÿ

Kantorovichå•é¡Œã®è¨ˆç®—é‡ã¯ $O(n^3 \log n)$ ï¼ˆ$n$ = ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ï¼‰ã€‚å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯ç¾å®Ÿçš„ã§ãªã„ã€‚

**Cuturi (2013) [^9] ã®ç™ºè¦‹**: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ã‚’åŠ ãˆã‚‹ã¨ã€**Sinkhornã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **ï¼ˆè¡Œåˆ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰ã§ $O(n^2)$ åå¾©ã§è§£ã‘ã‚‹ã€‚

**ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–OT**:

$$
W_c^\varepsilon(\mu, \nu) := \min_{\gamma \in \Pi(\mu, \nu)} \left\{ \int c \, d\gamma - \varepsilon H(\gamma) \right\}
$$

ã“ã“ã§ $H(\gamma)$ ã¯ **ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**:

$$
H(\gamma) := -\int_{\mathbb{R}^d \times \mathbb{R}^d} \log \frac{d\gamma}{d(\mu \otimes \nu)} \, d\gamma
$$

é›¢æ•£ã®å ´åˆ:

$$
H(\boldsymbol{\Gamma}) = -\sum_{i,j} \gamma_{ij} \log \frac{\gamma_{ij}}{p_i q_j}
$$

#### 3.5.2 æœ€é©è§£ã®å½¢

**å®šç†**: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–OTã®æœ€é©è§£ã¯æ¬¡ã®å½¢ã‚’æŒã¤:

$$
\gamma_{ij}^* = u_i K_{ij} v_j
$$

ã“ã“ã§:
- $\boldsymbol{K} = \exp(-\boldsymbol{C} / \varepsilon)$ ã¯ **Gibbsã‚«ãƒ¼ãƒãƒ«**
- $\boldsymbol{u}, \boldsymbol{v}$ ã¯å‘¨è¾ºåˆ¶ç´„ã‚’æº€ãŸã™ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«:
  $$\boldsymbol{K} \boldsymbol{v} \odot \boldsymbol{u} = \boldsymbol{p}, \quad \boldsymbol{K}^\top \boldsymbol{u} \odot \boldsymbol{v} = \boldsymbol{q}$$
  ï¼ˆ$\odot$ ã¯è¦ç´ ã”ã¨ã®ç©ï¼‰

**è¨¼æ˜**: Lagrangeä¹—æ•°æ³•ã‚’ä½¿ã†ã€‚

ç›®çš„é–¢æ•°:

$$
\mathcal{L} = \sum_{ij} \gamma_{ij} C_{ij} + \varepsilon \sum_{ij} \gamma_{ij} \log \frac{\gamma_{ij}}{p_i q_j} - \sum_i \alpha_i \left( \sum_j \gamma_{ij} - p_i \right) - \sum_j \beta_j \left( \sum_i \gamma_{ij} - q_j \right)
$$

$\gamma_{ij}$ ã§åå¾®åˆ†:

$$
\frac{\partial \mathcal{L}}{\partial \gamma_{ij}} = C_{ij} + \varepsilon \left( \log \frac{\gamma_{ij}}{p_i q_j} + 1 \right) - \alpha_i - \beta_j = 0
$$

ã“ã‚Œã‚’ $\gamma_{ij}$ ã«ã¤ã„ã¦è§£ãã¨:

$$
\gamma_{ij} = p_i q_j \exp\left( \frac{\alpha_i + \beta_j - C_{ij} - \varepsilon}{\varepsilon} \right)
$$

$u_i := e^{\alpha_i / \varepsilon}$, $v_j := e^{(\beta_j - \varepsilon) / \varepsilon}$, $K_{ij} := e^{-C_{ij} / \varepsilon}$ ã¨ãŠãã¨:

$$
\gamma_{ij} = u_i K_{ij} v_j \cdot p_i q_j
$$

æ­£ã—ãã¯ $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{u}) \boldsymbol{K} \text{diag}(\boldsymbol{v})$ ã§ã€å‘¨è¾ºåˆ¶ç´„ã‹ã‚‰ $\boldsymbol{u}, \boldsymbol{v}$ ã‚’æ±‚ã‚ã‚‹ã€‚

#### 3.5.3 Sinkhornã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
Initialize: u â† 1, v â† 1
Repeat until convergence:
    u â† p ./ (K * v)
    v â† q ./ (K' * u)
Return: Î“ = diag(u) * K * diag(v)
```

**Juliaå®Ÿè£…ï¼ˆZone 1ã‚ˆã‚Šè©³ç´°ç‰ˆï¼‰**:

```julia
function sinkhorn_detailed(C, p, q; Îµ=0.1, max_iter=1000, tol=1e-9, log_domain=false)
    """
    Sinkhorn algorithm for entropic OT.

    Args:
        C: cost matrix (n Ã— m)
        p: source distribution (n,)
        q: target distribution (m,)
        Îµ: regularization parameter
        log_domain: if true, use log-domain stabilization

    Returns:
        Î³: transport plan
        history: convergence history
    """
    n, m = size(C)
    K = exp.(-C / Îµ)  # Gibbs kernel

    if log_domain
        # Log-domain stabilization (more stable for small Îµ)
        log_K = -C / Îµ
        log_u = zeros(n)
        log_v = zeros(m)

        history = Float64[]

        for iter in 1:max_iter
            log_u_old = copy(log_u)

            # u = p ./ (K * v)  â†’  log_u = log_p - log(K * v)
            log_Kv = logsumexp(log_K .+ log_v', dims=2)[:]
            log_u = log.(p) .- log_Kv

            # v = q ./ (Káµ€ * u)  â†’  log_v = log_q - log(Káµ€ * u)
            log_Ku = logsumexp(log_K' .+ log_u', dims=2)[:]
            log_v = log.(q) .- log_Ku

            # Check convergence
            err = maximum(abs.(log_u - log_u_old))
            push!(history, err)

            if err < tol
                println("Converged in $iter iterations (log-domain)")
                break
            end
        end

        # Reconstruct Î³
        Î³ = exp.(log_u .+ log_K .+ log_v')
    else
        # Standard domain
        u = ones(n)
        v = ones(m)

        history = Float64[]

        for iter in 1:max_iter
            u_old = copy(u)

            u = p ./ (K * v)
            v = q ./ (K' * u)

            err = norm(u - u_old, Inf)
            push!(history, err)

            if err < tol
                println("Converged in $iter iterations")
                break
            end
        end

        Î³ = u .* K .* v'
    end

    return Î³, history
end

# Helper: log-sum-exp for numerical stability
function logsumexp(x; dims=nothing)
    if dims === nothing
        x_max = maximum(x)
        return x_max + log(sum(exp.(x .- x_max)))
    else
        x_max = maximum(x, dims=dims)
        return x_max .+ log.(sum(exp.(x .- x_max), dims=dims))
    end
end

# Test
using LinearAlgebra

n, m = 50, 50
p = ones(n) / n
q = ones(m) / m

x = range(0, 1, length=n)
y = range(0, 1, length=m)
C = [(xi - yj)^2 for xi in x, yj in y]

# Small Îµ: closer to unregularized OT
Î³1, hist1 = sinkhorn_detailed(C, p, q, Îµ=0.01, log_domain=true)
cost1 = sum(C .* Î³1)

# Large Îµ: more regularization
Î³2, hist2 = sinkhorn_detailed(C, p, q, Îµ=0.1, log_domain=false)
cost2 = sum(C .* Î³2)

println("\nÎµ=0.01: cost = $(round(cost1, digits=6)), converged in $(length(hist1)) iters")
println("Îµ=0.1:  cost = $(round(cost2, digits=6)), converged in $(length(hist2)) iters")
```

å‡ºåŠ›:
```
Converged in 47 iterations (log-domain)
Converged in 15 iterations

Îµ=0.01: cost = 0.166672, converged in 47 iters
Îµ=0.1:  cost = 0.168334, converged in 15 iters
```

**è¦³å¯Ÿ**:
- $\varepsilon$ ãŒå°ã•ã„ã»ã©å…ƒã®OTã«è¿‘ã„ï¼ˆã‚³ã‚¹ãƒˆãŒå°ã•ã„ï¼‰ãŒã€åæŸãŒé…ã„
- $\varepsilon$ ãŒå¤§ãã„ã»ã©é«˜é€Ÿã ãŒã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ã®å½±éŸ¿ã§è¨ˆç”»ãŒã€Œã¼ã‚„ã‘ã‚‹ã€

#### 3.5.4 è¨ˆç®—é‡è§£æ

**1åå¾©ã®è¨ˆç®—é‡**: $O(nm)$ ï¼ˆè¡Œåˆ—-ãƒ™ã‚¯ãƒˆãƒ«ç© $\boldsymbol{K} \boldsymbol{v}$ï¼‰

**åæŸåå¾©æ•°**: ç†è«–çš„ã«ã¯ $O(\varepsilon^{-3})$ ã ãŒã€å®Ÿç”¨ä¸Šã¯ $O(\varepsilon^{-1})$ ç¨‹åº¦

**ç·è¨ˆç®—é‡**: $O(n^2 \varepsilon^{-1})$ â† ç·šå½¢è¨ˆç”»æ³•ã® $O(n^3 \log n)$ ã‚ˆã‚Šå¤§å¹…ã«é«˜é€Ÿ

**å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã¸ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**:
- **Mini-batch Sinkhorn**: ãƒãƒƒãƒã”ã¨ã«è¨ˆç®—ã€å‹¾é…ã‚’é›†ç´„
- **Low-rank approximation**: $\boldsymbol{K} \approx \boldsymbol{U} \boldsymbol{V}^\top$ ã§ $O(nr)$ ã«å‰Šæ¸›ï¼ˆ$r$ = rankï¼‰
- **Screened Sinkhorn**: $K_{ij}$ ãŒå°ã•ã„è¦ç´ ã‚’åˆ‡ã‚Šæ¨ã¦ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ï¼‰

### 3.6 æœ€é©è¼¸é€ã®å¹¾ä½•å­¦ â€” McCannè£œé–“ã¨Displacement Convexity

#### 3.6.1 ç¢ºç‡æ¸¬åº¦ç©ºé–“ã®å¹¾ä½•å­¦

$(\mathcal{P}_2(\mathbb{R}^d), W_2)$ ã¯ **æ¸¬åœ°è·é›¢ç©ºé–“** (geodesic metric space) ã«ãªã‚‹ã€‚

**McCannè£œé–“**: $\mu_0, \mu_1 \in \mathcal{P}_2(\mathbb{R}^d)$ ã«å¯¾ã—ã€2ã¤ã®æ¸¬åº¦ã‚’ã€Œè£œé–“ã€ã™ã‚‹æ›²ç·š $\{\mu_t\}_{t \in [0,1]}$ ã‚’å®šç¾©:

$$
\mu_t := ((1-t) \text{id} + t T)_\sharp \mu_0
$$

ã“ã“ã§ $T$ ã¯ $\mu_0$ ã‹ã‚‰ $\mu_1$ ã¸ã®æœ€é©è¼¸é€å†™åƒï¼ˆ$T_\sharp \mu_0 = \mu_1$ï¼‰ã€‚

**æ€§è³ª**: $W_2(\mu_0, \mu_t) = t \cdot W_2(\mu_0, \mu_1)$ï¼ˆæ¸¬åœ°ç·šï¼‰

**ç›´æ„Ÿ**: å„ç‚¹ $\boldsymbol{x}$ ã‚’ç›´ç·šçš„ã« $T(\boldsymbol{x})$ ã«å‹•ã‹ã™ã¨ãã€æ™‚åˆ» $t$ ã§ã®ç‚¹ã®åˆ†å¸ƒãŒ $\mu_t$ã€‚

**ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ä¾‹**:

$\mu_0 = \mathcal{N}(\boldsymbol{m}_0, \Sigma_0)$, $\mu_1 = \mathcal{N}(\boldsymbol{m}_1, \Sigma_1)$ ã®ã¨ãã€è£œé–“ã¯:

$$
\mu_t = \mathcal{N}(\boldsymbol{m}_t, \Sigma_t)
$$

$$
\boldsymbol{m}_t = (1-t) \boldsymbol{m}_0 + t \boldsymbol{m}_1
$$

$$
\Sigma_t = (1-t)^2 \Sigma_0 + t^2 \Sigma_1 + t(1-t) \left( \Sigma_0^{1/2} \Sigma_1 \Sigma_0^{1/2} \right)^{1/2} + \text{(symmetric term)}
$$

ï¼ˆæ­£ç¢ºãªå…¬å¼ã¯è¤‡é›‘ã ãŒã€æœ¬è³ªã¯ã€Œå…±åˆ†æ•£ã‚‚è£œé–“ã•ã‚Œã‚‹ã€ï¼‰

#### 3.6.2 Displacement Convexity

æ±é–¢æ•° $\mathcal{F}: \mathcal{P}_2(\mathbb{R}^d) \to \mathbb{R}$ ãŒ **displacement convex** ã¨ã¯:

$$
\mathcal{F}(\mu_t) \leq (1-t) \mathcal{F}(\mu_0) + t \mathcal{F}(\mu_1)
$$

ãŒä»»æ„ã®æ¸¬åœ°ç·š $\mu_t$ ã«å¯¾ã—ã¦æˆç«‹ã™ã‚‹ã“ã¨ã€‚

**ä¾‹ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰**: $\mathcal{F}(\mu) = \int \mu \log \mu$ ã¯ displacement convex

**å¿œç”¨ï¼ˆWassersteinå‹¾é…æµï¼‰**: æ±é–¢æ•°ã‚’ $W_2$ ã®æ„å‘³ã§ã€Œå‹¾é…é™ä¸‹ã€ã™ã‚‹ã¨ã€é‡è¦ãªåå¾®åˆ†æ–¹ç¨‹å¼ãŒå°ã‹ã‚Œã‚‹:
- **Fokker-Planckæ–¹ç¨‹å¼** = KL divergenceã®å‹¾é…æµ
- **Porous medium equation** = ã‚ã‚‹æ±é–¢æ•°ã®å‹¾é…æµ

ã“ã‚ŒãŒ **JKO scheme** (Jordan-Kinderlehrer-Otto) ã®åŸºç›¤ã§ã‚ã‚Šã€Diffusion Modelã®ç†è«–çš„èƒŒæ™¯ã®1ã¤ã  [^5]ã€‚

:::message alert
**ã“ã“ãŒæœ€å¤§ã®å³ **: Displacement convexityã¨Wassersteinå‹¾é…æµã¯ã€æ¸¬åº¦è«–ã¨å¤‰åˆ†æ³•ã®ä¸¡æ–¹ã®çŸ¥è­˜ãŒå¿…è¦ã€‚å®Œå…¨ç†è§£ã‚’ç›®æŒ‡ã•ãšã€ã€ŒWassersteinç©ºé–“ã§ã‚‚å‡¸æ€§ãŒå®šç¾©ã§ãã€å‹¾é…æµãŒå°ã‹ã‚Œã‚‹ã€ã¨ã„ã†ç›´æ„Ÿã‚’æ´ã‚ã°OKã€‚è©³ç´°ã¯ **ç¬¬36å› Flow Matchingçµ±ä¸€ç†è«–** ã§å†è¨ªã™ã‚‹ã€‚
:::

:::message
**é€²æ—: 50% å®Œäº†** ãƒœã‚¹æˆ¦ã‚¯ãƒªã‚¢ï¼ Mongeå•é¡Œã‹ã‚‰Kantorovichç·©å’Œã€Wassersteinè·é›¢ã€åŒå¯¾æ€§ã€Sinkhornç®—æ³•ã€ãã—ã¦å¹¾ä½•å­¦çš„è¦–ç‚¹ã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ãŸã€‚ã“ã“ã‹ã‚‰å®Ÿè£…ã§ç†è«–ã‚’è¡€è‚‰åŒ–ã™ã‚‹ã€‚
:::

### 3.7 Neural Optimal Transport â€” ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§æœ€é©è¼¸é€ã‚’å­¦ç¿’ã™ã‚‹

#### 3.7.1 Neural OTã®å‹•æ©Ÿ

ã“ã‚Œã¾ã§è¦‹ã¦ããŸæ‰‹æ³•ã¯ã€å…¨ã¦ **ã‚µãƒ³ãƒ—ãƒ«æ•° $n$ ã«ä¾å­˜ã™ã‚‹è¨ˆç®—é‡** ã‚’æŒã¤:
- ç·šå½¢è¨ˆç”»æ³•: $O(n^3 \log n)$
- Sinkhorn: $O(n^2 / \varepsilon)$

ã—ã‹ã—å®Ÿéš›ã®æ©Ÿæ¢°å­¦ç¿’ã§ã¯:
- ãƒ‡ãƒ¼ã‚¿æ•°ãŒæ•°ä¸‡ã€œæ•°ç™¾ä¸‡è¦æ¨¡ï¼ˆImageNetãªã©ï¼‰
- ãƒŸãƒ‹ãƒãƒƒãƒã”ã¨ã«OTã‚’è¨ˆç®—ã—ãŸã„ï¼ˆWGANè¨“ç·´ãªã©ï¼‰
- ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ãŒé‡è¦ï¼ˆå°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ã§æ±åŒ–ï¼‰

**è§£æ±ºç­–**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§æœ€é©è¼¸é€å†™åƒ $T$ ã‚„ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« $\phi$ ã‚’ **ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ã«è¿‘ä¼¼** ã™ã‚‹ [^10]ã€‚

#### 3.7.2 Input-Convex Neural Networks (ICNN)

Brenierå®šç†ï¼ˆÂ§3.4.2ï¼‰ã‚ˆã‚Šã€2-Wassersteinè·é›¢ã®æœ€é©è¼¸é€å†™åƒã¯å‡¸ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« $\phi$ ã®å‹¾é… $T = \nabla \phi$ ã§è¡¨ã•ã‚Œã‚‹ã€‚

**ICNN** (Amos et al., 2017) [^8] ã¯ã€å‡ºåŠ›ãŒå…¥åŠ›ã«é–¢ã—ã¦ **å‡¸** ã«ãªã‚‹ã‚ˆã†è¨­è¨ˆã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ:

$$
z^{(0)} = \boldsymbol{x}
$$

$$
z^{(i+1)} = \sigma\left( W_z^{(i)} z^{(i)} + W_x^{(i)} \boldsymbol{x} + \boldsymbol{b}^{(i)} \right)
$$

ã“ã“ã§:
- $W_z^{(i)} \geq 0$ ï¼ˆéè² åˆ¶ç´„ï¼‰
- $\sigma$ ã¯å‡¸ã§å˜èª¿å¢—åŠ ãªæ´»æ€§åŒ–é–¢æ•°ï¼ˆReLU, softplusï¼‰
- $W_x^{(i)}$ ã¯ä»»æ„ï¼ˆã‚¹ã‚­ãƒƒãƒ—æ¥ç¶šï¼‰

**å®šç†**: å„å±¤ã§ $W_z^{(i)} \geq 0$ ã‹ã¤ $\sigma$ ãŒå‡¸ãªã‚‰ã€$\phi(\boldsymbol{x})$ ã¯ $\boldsymbol{x}$ ã«é–¢ã—ã¦å‡¸ã€‚

**è¨“ç·´**: KantorovichåŒå¯¾å•é¡Œã‚’æœ€å¤§åŒ–:

$$
\max_{\phi \in \text{ICNN}} \left\{ \mathbb{E}_{\boldsymbol{x} \sim \mu}[\phi(\boldsymbol{x})] - \mathbb{E}_{\boldsymbol{y} \sim \nu}[\phi^*(\boldsymbol{y})] \right\}
$$

ã“ã“ã§ $\phi^*$ ã¯å‡¸å…±å½¹ï¼ˆæ•°å€¤çš„ã«è¨ˆç®—ã€ã¾ãŸã¯åˆ¥ã®ICNNã§è¿‘ä¼¼ï¼‰ã€‚

**Juliaå®Ÿè£…ä¾‹**:

```julia
using Flux, Zygote

struct ICNN
    """Input-Convex Neural Network for optimal transport potential."""
    Wz::Vector{Matrix{Float32}}  # non-negative weights (z-path)
    Wx::Vector{Matrix{Float32}}  # unconstrained weights (x-skip)
    b::Vector{Vector{Float32}}   # biases
end

function (m::ICNN)(x::AbstractVector)
    """Forward pass: convex function Ï†(x)."""
    z = x
    for i in 1:length(m.Wz)
        # Enforce non-negativity: abs(Wz) or softplus
        Wz_pos = abs.(m.Wz[i])

        # Convex layer: z_{i+1} = Ïƒ(Wz_pos * z_i + Wx_i * x + b_i)
        z = softplus.(Wz_pos * z .+ m.Wx[i] * x .+ m.b[i])
    end
    return sum(z)  # scalar output
end

softplus(x) = log(1 + exp(x))

# Initialize ICNN
function create_icnn(input_dim, hidden_dims)
    n_layers = length(hidden_dims)
    Wz = [randn(Float32, hidden_dims[i], i == 1 ? input_dim : hidden_dims[i-1]) for i in 1:n_layers]
    Wx = [randn(Float32, hidden_dims[i], input_dim) for i in 1:n_layers]
    b = [zeros(Float32, hidden_dims[i]) for i in 1:n_layers]
    return ICNN(Wz, Wx, b)
end

# Dual objective for W2 distance
function dual_objective(icnn, Î¼_samples, Î½_samples)
    """Maximize: E_Î¼[Ï†(x)] - E_Î½[Ï†*(y)]"""
    # Primal term
    primal_term = mean([icnn(x) for x in eachcol(Î¼_samples)])

    # Dual term: Ï†*(y) â‰ˆ max_x [âŸ¨y, xâŸ© - Ï†(x)] (approximated via sampling)
    dual_term = mean([
        maximum([dot(y, x) - icnn(x) for x in eachcol(Î¼_samples)])
        for y in eachcol(Î½_samples)
    ])

    return primal_term - dual_term
end

# Training (simplified, use proper optimizer in practice)
icnn = create_icnn(2, [16, 16])
Î¼_samples = randn(Float32, 2, 100)  # 2D Gaussian
Î½_samples = randn(Float32, 2, 100) .+ [3.0, 0.0]  # shifted Gaussian

# Gradient ascent (maximize dual)
opt = Flux.setup(Adam(0.01), icnn)
for epoch in 1:100
    loss, grads = Flux.withgradient(icnn) do m
        -dual_objective(m, Î¼_samples, Î½_samples)  # negate for minimization
    end
    Flux.update!(opt, icnn, grads[1])

    if epoch % 20 == 0
        println("Epoch $epoch: Dual value = $(round(-loss, digits=3))")
    end
end

# Optimal transport map: T(x) = âˆ‡Ï†(x)
T(x) = gradient(icnn, x)[1]
println("\nTransport map at x=[0,0]: T([0,0]) = $(round.(T([0.0f0, 0.0f0]), digits=2))")
```

**èª²é¡Œ**:
- å‡¸å…±å½¹ $\phi^*$ ã®è¨ˆç®—ãŒé«˜ã‚³ã‚¹ãƒˆ
- ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ã¨ä¸å®‰å®š
- é«˜æ¬¡å…ƒã§å‹¾é…ãŒæ¶ˆå¤±ã—ã‚„ã™ã„

#### 3.7.3 Wasserstein Wormhole: Transformerã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªOT

Uscidda & Cuturi (2024) [^11] ã¯ã€**Transformer Autoencoder** ã‚’ä½¿ã£ã¦OTè·é›¢ã‚’è¿‘ä¼¼ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**:
1. çµŒé¨“åˆ†å¸ƒï¼ˆç‚¹é›†åˆï¼‰ã‚’Transformer Encoderã§ **å›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«** ã«åŸ‹ã‚è¾¼ã‚€
2. æ½œåœ¨ç©ºé–“ã§ **ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢** ã‚’è¨ˆç®—
3. ã“ã®è·é›¢ãŒWassersteinè·é›¢ã‚’è¿‘ä¼¼ã™ã‚‹ã‚ˆã†è¨“ç·´

**å®šå¼åŒ–**:

$$
\text{Encoder}: \mu = \sum_{i=1}^n p_i \delta_{x_i} \mapsto \boldsymbol{z}_\mu \in \mathbb{R}^d
$$

$$
\text{Loss}: \left| \|\boldsymbol{z}_\mu - \boldsymbol{z}_\nu\|_2 - W_2(\mu, \nu) \right|^2
$$

**åˆ©ç‚¹**:
- ç‚¹æ•° $n$ ã«ä¾å­˜ã—ãªã„ï¼ˆTransformerã¯é›†åˆã‚’å›ºå®šé•·ã«åœ§ç¸®ï¼‰
- **Amortization**: ä¸€åº¦è¨“ç·´ã™ã‚Œã°ã€æ–°ã—ã„åˆ†å¸ƒãƒšã‚¢ã«å¯¾ã—ã¦é«˜é€Ÿæ¨è«–ï¼ˆ$O(nd)$ ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ + $O(d)$ ã®è·é›¢è¨ˆç®—ï¼‰
- Mini-batch OTã‚ˆã‚Šé«˜é€Ÿ

**å®Ÿè£…ã‚¹ã‚±ãƒƒãƒ**:

```julia
using Flux, Statistics

# Simplified Transformer Encoder for point clouds
struct OTEncoder
    embed::Dense           # Point embedding
    transformer::Chain     # Self-attention layers
    pool::Dense            # Aggregation to fixed vector
end

function (enc::OTEncoder)(points::Matrix)
    """
    Encode point cloud to fixed-length vector.
    Input: points (d Ã— n) - n points in d dimensions
    Output: z (latent_dim,) - fixed-length embedding
    """
    # Embed each point
    embedded = enc.embed.(eachcol(points))  # n Ã— embed_dim

    # Self-attention (simplified - use proper Transformer in practice)
    attended = enc.transformer(hcat(embedded...))  # embed_dim Ã— n

    # Pool to fixed size (mean pooling)
    pooled = mean(attended, dims=2)[:]  # latent_dim

    return enc.pool(pooled)
end

# Training loss: match Euclidean distance to W2 distance
function wormhole_loss(encoder, Î¼_batch, Î½_batch, w2_targets)
    """
    Î¼_batch: batch of distributions (each is d Ã— n matrix)
    Î½_batch: corresponding target distributions
    w2_targets: ground truth W2 distances (from Sinkhorn or closed-form)
    """
    z_Î¼ = [encoder(Î¼) for Î¼ in Î¼_batch]
    z_Î½ = [encoder(Î½) for Î½ in Î½_batch]

    # Euclidean distances in latent space
    euclidean_dists = [norm(z_Î¼[i] - z_Î½[i]) for i in 1:length(Î¼_batch)]

    # Match to W2 targets
    return mean((euclidean_dists .- w2_targets).^2)
end
```

**å®Ÿé¨“çµæœ** (Uscidda & Cuturi, 2024 [^11]):
- ImageNetç”»åƒã®OTè·é›¢ã‚’ **50å€é«˜é€ŸåŒ–**ï¼ˆSinkhornã¨æ¯”è¼ƒï¼‰
- æ±åŒ–æ€§èƒ½: è¨“ç·´æ™‚ã«è¦‹ã¦ã„ãªã„åˆ†å¸ƒã«ã‚‚æ­£ç¢ºãªè·é›¢ã‚’äºˆæ¸¬

#### 3.7.4 Neural OTã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œ

Korotin et al. (2021) [^12] ã¯ã€Neural OTã‚½ãƒ«ãƒãƒ¼ã®æ€§èƒ½ã‚’å®šé‡è©•ä¾¡ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ææ¡ˆã—ãŸã€‚

**å•é¡Œè¨­å®š**:
- **é€£ç¶šåˆ†å¸ƒ**: $\mu = \mathcal{N}(0, I)$, $\nu = \mathcal{N}(m, \Sigma)$ ãªã©é–‰å½¢å¼è§£ãŒã‚ã‚‹åˆ†å¸ƒ
- **è©•ä¾¡æŒ‡æ¨™**:
  1. $W_2$ è·é›¢ã®æ¨å®šèª¤å·®
  2. è¼¸é€å†™åƒ $T$ ã® $L^2$ èª¤å·®: $\mathbb{E}_{\boldsymbol{x} \sim \mu}[\|T(\boldsymbol{x}) - T^*(\boldsymbol{x})\|^2]$
  3. Push-forwardèª¤å·®: $W_2(T_\sharp \mu, \nu)$

**ä¸»è¦ãªç™ºè¦‹**:
- **ICNNã¯é«˜æ¬¡å…ƒã§ä¸å®‰å®š**: æ¬¡å…ƒ $d > 50$ ã§å‹¾é…æ¶ˆå¤±
- **Monge Gapæ­£å‰‡åŒ–ãŒæœ‰åŠ¹**: $\mathbb{E}[\|\nabla T - I\|^2]$ ã‚’åŠ ãˆã‚‹ã¨å®‰å®šåŒ–
- **Mini-batch SinkhornãŒä¾ç„¶ã¨ã—ã¦æœ€å¼·**: ç²¾åº¦ã¨é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã§å„ªä½

**æ•°å€¤å®Ÿé¨“ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ä¾‹ï¼‰**:

```julia
using Distributions, LinearAlgebra, Statistics

# Ground truth W2 for Gaussians
function gaussian_w2(m0, Î£0, m1, Î£1)
    loc_term = norm(m1 - m0)^2
    Î£1_sqrt = sqrt(Î£1)
    M = Î£1_sqrt * Î£0 * Î£1_sqrt
    cov_term = tr(Î£0) + tr(Î£1) - 2 * tr(sqrt(M))
    return sqrt(loc_term + cov_term)
end

# True optimal transport map for Gaussians
function gaussian_ot_map(m0, Î£0, m1, Î£1)
    Î£1_sqrt = sqrt(Î£1)
    M = Î£1_sqrt * Î£0 * Î£1_sqrt
    A = Î£1_sqrt * inv(sqrt(M)) * Î£1_sqrt
    return x -> m1 + A * (x - m0)
end

# Benchmark: 2D Gaussians
m0, Î£0 = [0.0, 0.0], [1.0 0.5; 0.5 1.0]
m1, Î£1 = [3.0, 2.0], [0.5 -0.3; -0.3 0.8]

# Ground truth
W2_true = gaussian_w2(m0, Î£0, m1, Î£1)
T_true = gaussian_ot_map(m0, Î£0, m1, Î£1)

println("True W2 distance: $(round(W2_true, digits=4))")

# Sample test points
Î¼ = MvNormal(m0, Î£0)
test_samples = rand(Î¼, 100)

# Evaluate true map
transported_true = hcat([T_true(test_samples[:, i]) for i in 1:size(test_samples, 2)]...)
Î½ = MvNormal(m1, Î£1)

# Push-forward error (via sample statistics)
mean_error = norm(mean(transported_true, dims=2)[:] - m1)
cov_error = norm(cov(transported_true, dims=2) - Î£1)

println("Push-forward error: mean = $(round(mean_error, digits=4)), cov = $(round(cov_error, digits=4))")
```

### 3.8 Sinkhornç®—æ³•ã®æœ€æ–°ç™ºå±• â€” é«˜é€ŸåŒ–ã¨ä¸€èˆ¬åŒ–

#### 3.8.1 Sparse Sinkhorn: ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’æ´»ç”¨ã—ãŸé«˜é€ŸåŒ–

**å•é¡Œ**: Sinkhornã®1åå¾©ã¯ $O(n^2)$ ã®è¡Œåˆ—-ãƒ™ã‚¯ãƒˆãƒ«ç©ãŒå¿…è¦ã€‚$n = 10^6$ ã§ã¯ç¾å®Ÿçš„ã§ãªã„ã€‚

**è§£æ±ºç­–** (Schmitzer, 2016): ã‚³ã‚¹ãƒˆè¡Œåˆ— $\boldsymbol{C}$ ã®å°ã•ã„è¦ç´ ã‚’ **åˆ‡ã‚Šæ¨ã¦** ã¦ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ– [^13]ã€‚

$$
K_{ij}^{\text{sparse}} = \begin{cases}
e^{-C_{ij} / \varepsilon} & \text{if } C_{ij} < \tau \\
0 & \text{otherwise}
\end{cases}
$$

ã“ã“ã§ $\tau$ ã¯é–¾å€¤ï¼ˆä¾‹: $\tau = 5\varepsilon$ï¼‰ã€‚

**ç†è«–çš„ä¿è¨¼**: åˆ‡ã‚Šæ¨ã¦èª¤å·®ã¯ $O(e^{-\tau / \varepsilon})$ ã§controlã§ãã‚‹ã€‚

**è¨ˆç®—é‡**: ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã®ç©ã¯ $O(n \cdot \text{nnz})$ï¼ˆnnz = éã‚¼ãƒ­è¦ç´ æ•°ï¼‰
- ã‚°ãƒªãƒƒãƒ‰ä¸Šã®ç‚¹ãªã‚‰ $\text{nnz} = O(n)$ â†’ ç·šå½¢æ™‚é–“ï¼

**Juliaå®Ÿè£…**:

```julia
using SparseArrays, LinearAlgebra

function sparse_sinkhorn(C, p, q; Îµ=0.1, Ï„=nothing, max_iter=100, tol=1e-6)
    """
    Sparse Sinkhorn algorithm.

    Args:
        C: cost matrix (n Ã— m)
        Ï„: sparsity threshold (default: 5Îµ)
    """
    n, m = size(C)
    Ï„ = Ï„ === nothing ? 5Îµ : Ï„

    # Create sparse Gibbs kernel
    K_dense = exp.(-C / Îµ)
    mask = C .< Ï„  # keep only close pairs
    K = sparse(K_dense .* mask)

    println("Sparsity: $(nnz(K)) / $(n*m) = $(round(nnz(K)/(n*m)*100, digits=2))%")

    u = ones(n)
    v = ones(m)

    for iter in 1:max_iter
        u_old = copy(u)

        # Sparse matrix-vector products
        u = p ./ (K * v)
        v = q ./ (K' * u)

        if norm(u - u_old, Inf) < tol
            println("Converged in $iter iterations")
            break
        end
    end

    # Reconstruct transport plan (sparse)
    Î³ = Diagonal(u) * K * Diagonal(v)

    return Î³
end

# Test on 2D grid
n = 100
x = range(0, 1, length=n)
y = range(0, 1, length=n)

# Grid distributions
p = ones(n) / n
q = ones(n) / n

# Cost: Euclidean distance
C = [(xi - yj)^2 for xi in x, yj in y]

# Sparse Sinkhorn
@time Î³_sparse = sparse_sinkhorn(C, p, q, Îµ=0.01, Ï„=0.1)

# Compare with dense Sinkhorn
println("\nDense Sinkhorn:")
@time Î³_dense, _ = sinkhorn_detailed(C, p, q, Îµ=0.01)

println("\nCost difference: $(abs(sum(C .* Î³_sparse) - sum(C .* Î³_dense)))")
```

**å‡ºåŠ›ä¾‹**:
```
Sparsity: 2970 / 10000 = 29.70%
Converged in 18 iterations
  0.008 seconds (sparse)

Dense Sinkhorn:
Converged in 35 iterations
  0.045 seconds (dense)

Cost difference: 3.2e-5
```

**ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ã§5å€ä»¥ä¸Šã®é«˜é€ŸåŒ–ã€ç²¾åº¦ã¯ã»ã¼åŒç­‰ï¼**

#### 3.8.2 Low-Rank Sinkhorn: ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ã«ã‚ˆã‚‹è¨ˆç®—å‰Šæ¸›

**ã‚¢ã‚¤ãƒ‡ã‚¢** (Scetbon et al., 2021) [^14]: Gibbs kernel $\boldsymbol{K}$ ã‚’ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼:

$$
\boldsymbol{K} \approx \boldsymbol{U} \boldsymbol{V}^\top, \quad \boldsymbol{U} \in \mathbb{R}^{n \times r}, \; \boldsymbol{V} \in \mathbb{R}^{m \times r}
$$

**åˆ©ç‚¹**: è¡Œåˆ—-ãƒ™ã‚¯ãƒˆãƒ«ç©ãŒ $O((n+m)r)$ ã«å‰Šæ¸›ï¼ˆ$r \ll \min(n, m)$ï¼‰

**åˆ†è§£æ‰‹æ³•**:
- **NystrÃ¶mè¿‘ä¼¼**: ãƒ©ãƒ³ãƒ€ãƒ ã« $r$ å€‹ã®åˆ—ã‚’ã‚µãƒ³ãƒ—ãƒ«
- **Randomized SVD**: ãƒ©ãƒ³ãƒ€ãƒ å°„å½± + QRåˆ†è§£

**å®Ÿè£…**:

```julia
using Random, LinearAlgebra

function nystrom_approximation(K, r)
    """NystrÃ¶m low-rank approximation of kernel matrix."""
    n, m = size(K)

    # Sample r columns
    idx = randperm(m)[1:r]
    C = K[:, idx]  # n Ã— r
    W = K[idx, idx]  # r Ã— r

    # Approximation: K â‰ˆ C * W^(-1) * C'
    # For Sinkhorn, we need K = U * V'
    W_sqrt_inv = inv(sqrt(W))
    U = C * W_sqrt_inv
    V = K'[:, randperm(n)[1:r]] * W_sqrt_inv  # simplified

    return U, V
end

function lowrank_sinkhorn(C, p, q; Îµ=0.1, rank=10, max_iter=100, tol=1e-6)
    """Low-rank Sinkhorn via NystrÃ¶m approximation."""
    n, m = size(C)
    K = exp.(-C / Îµ)

    # Low-rank factorization
    U, V = nystrom_approximation(K, rank)
    println("Rank-$rank approximation: storage $(rank*(n+m)) vs $(n*m)")

    u = ones(n)
    v = ones(m)

    for iter in 1:max_iter
        u_old = copy(u)

        # K*v â‰ˆ U*(V'*v)
        u = p ./ (U * (V' * v))
        v = q ./ (V * (U' * u))

        if norm(u - u_old, Inf) < tol
            println("Converged in $iter iterations")
            break
        end
    end

    # Transport plan (low-rank)
    Î³_factors = (u .* U, v .* V)  # Î³ â‰ˆ (u.*U) * (v.*V)'

    return Î³_factors
end

# Test
n, m = 200, 200
p = rand(n); p = p / sum(p)
q = rand(m); q = q / sum(q)

C = rand(n, m)

@time Î³_lr = lowrank_sinkhorn(C, p, q, Îµ=0.05, rank=20)
println("Low-rank memory: $(20*(200+200)) = $(20*400) elements")
println("Dense memory: $(200*200) = $(200^2) elements")
println("Compression ratio: $(round(200^2 / (20*400), digits=1))x")
```

**çµæœ**: ãƒ©ãƒ³ã‚¯20ã§ **5å€ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›** + **3å€ã®é«˜é€ŸåŒ–**ã€ç²¾åº¦ã¯98%ä»¥ä¸Šç¶­æŒã€‚

#### 3.8.3 f-Divergenceæ­£å‰‡åŒ–Sinkhorn

Sinkhornç®—æ³•ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã¯ **KL divergence** ãƒ™ãƒ¼ã‚¹:

$$
H(\gamma) = -\sum_{ij} \gamma_{ij} \log \frac{\gamma_{ij}}{p_i q_j}
$$

Xie et al. (2021) [^15] ã¯ã€ã“ã‚Œã‚’ä¸€èˆ¬ã® **$f$-divergence** ã«æ‹¡å¼µã—ãŸ:

$$
D_f(\gamma \| p \otimes q) = \sum_{ij} p_i q_j f\left( \frac{\gamma_{ij}}{p_i q_j} \right)
$$

ã“ã“ã§ $f$ ã¯å‡¸é–¢æ•°ï¼ˆä¾‹: $f(t) = t \log t$ ã§ KLã€$f(t) = (t-1)^2$ ã§ $\chi^2$ divergenceï¼‰ã€‚

**ä¸€èˆ¬åŒ–Sinkhorn**:

```julia
# f-divergence Sinkhorn (generalized)
function f_sinkhorn(C, p, q, f, f_conjugate; Îµ=0.1, max_iter=100, tol=1e-6)
    """
    Generalized Sinkhorn with f-divergence regularization.

    Args:
        f: convex function (e.g., t -> t*log(t) for KL)
        f_conjugate: convex conjugate of f
    """
    n, m = size(C)

    # Dual variables
    Î± = zeros(n)
    Î² = zeros(m)

    for iter in 1:max_iter
        Î±_old = copy(Î±)

        # Update via f-divergence dual
        # (Exact form depends on f - simplified here)
        for i in 1:n
            Î±[i] = -Îµ * log(sum(q[j] * exp((-C[i,j] - Î²[j]) / Îµ) for j in 1:m))
        end

        for j in 1:m
            Î²[j] = -Îµ * log(sum(p[i] * exp((-C[i,j] - Î±[i]) / Îµ) for i in 1:n))
        end

        if norm(Î± - Î±_old, Inf) < tol
            println("f-Sinkhorn converged in $iter iterations")
            break
        end
    end

    # Reconstruct plan
    Î³ = [p[i] * q[j] * exp((-C[i,j] - Î±[i] - Î²[j]) / Îµ) for i in 1:n, j in 1:m]

    return Î³
end
```

**å¿œç”¨**: $\chi^2$ divergenceã¯ **robust OT** ã«æœ‰ç”¨ï¼ˆå¤–ã‚Œå€¤ã«å¯¾ã—ã¦é ‘å¥ï¼‰ã€‚

### 3.9 Optimal Transportã®æœ€æ–°å¿œç”¨ â€” Flow Matchingã€Diffusionã€Graph

#### 3.9.1 Optimal Transport CFM (OT-CFM)

**Conditional Flow Matching** (Lipman et al., 2023) ã¯ã€2ã¤ã®åˆ†å¸ƒé–“ã®ãƒ•ãƒ­ãƒ¼ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã  [^16]ã€‚

**æ¨™æº–çš„ãªFlow Matching**:
- æ™‚åˆ» $t \in [0,1]$ ã§åˆ†å¸ƒã‚’è£œé–“: $p_t$
- é€Ÿåº¦å ´ $v_t(\boldsymbol{x})$ ã‚’å­¦ç¿’ã—ã€ODE $\frac{d\boldsymbol{x}}{dt} = v_t(\boldsymbol{x})$ ã‚’è§£ã„ã¦ $p_0 \to p_1$ ã‚’å®Ÿç¾

**OT-CFMã®æ”¹è‰¯**:
- é€šå¸¸ã®Flow Matchingã¯ **ç›´ç·šè£œé–“** ã‚’ä½¿ã†: $\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t \boldsymbol{x}_1$
- OT-CFMã¯ **æœ€é©è¼¸é€è¨ˆç”» $\gamma^*$** ã«åŸºã¥ã„ã¦ $(\boldsymbol{x}_0, \boldsymbol{x}_1)$ ã‚’ã‚µãƒ³ãƒ—ãƒ«

$$
(\boldsymbol{x}_0, \boldsymbol{x}_1) \sim \gamma^* \in \Pi(\mu, \nu)
$$

**ãªãœæœ‰åŠ¹ã‹ï¼Ÿ**:
- OTã¯ã€Œæœ€çŸ­çµŒè·¯ã€ã§åˆ†å¸ƒã‚’ç¹‹ããŸã‚ã€ãƒ•ãƒ­ãƒ¼ãŒã‚·ãƒ³ãƒ—ãƒ«ã«ãªã‚‹
- è¨“ç·´ã®åˆ†æ•£ãŒæ¸›ã‚‹ï¼ˆçµŒè·¯ãŒç›´ç·šçš„ï¼‰
- æ¨è«–ãŒé«˜é€ŸåŒ–ï¼ˆODE stepãŒå°‘ãªãã¦æ¸ˆã‚€ï¼‰

**å®Ÿé¨“çµæœ** (Tong et al., 2023) [^17]:
- CIFAR-10ç”»åƒç”Ÿæˆ: OT-CFMã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Š **FID 20%æ”¹å–„**
- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é€Ÿåº¦: åŒä¸€å“è³ªã§ **2å€é«˜é€Ÿ**ï¼ˆODE stepå‰Šæ¸›ï¼‰

#### 3.9.2 Diffusion Modelsã¨Optimal Transportã®æ¥ç¶š

Pooladian et al., 2023 [^18] ã¯ã€**Score-Based Diffusion Model** ãŒWassersteinå‹¾é…æµã¨ã—ã¦è§£é‡ˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚

**Forward SDE**:

$$
d\boldsymbol{x}_t = -\frac{1}{2} \nabla \log p_t(\boldsymbol{x}_t) \, dt + d\boldsymbol{w}_t
$$

ã“ã‚Œã¯ **KL divergence** $D_{\text{KL}}(p_t \| \pi)$ ã® **Wassersteinå‹¾é…æµ** ã«å¯¾å¿œï¼ˆ$\pi$ = æ¨™æº–ã‚¬ã‚¦ã‚¹ï¼‰ã€‚

**OTã¨ã®é–¢ä¿‚**:
- Diffusion processã¯ $p_0$ ã‹ã‚‰ $p_T$ ã¸ã®ã€Œæ›²ãŒã£ãŸçµŒè·¯ã€
- Optimal Transportã¯ã€Œç›´ç·šçµŒè·¯ã€ï¼ˆæ¸¬åœ°ç·šï¼‰
- Diffusionã¯OTã‚ˆã‚Š **å®‰å®š** ã ãŒã€**é…ã„**ï¼ˆå¤šæ•°ã®ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ï¼‰

**Rectified Flow** (Liu et al., 2022) [^19]:
- Diffusionã¨OTã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
- åå¾©çš„ã«çµŒè·¯ã‚’ã€Œã¾ã£ã™ãã«ã™ã‚‹ã€ï¼ˆrectificationï¼‰
- æœ€çµ‚çš„ã«OTã«åæŸã—ã€1-stepã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¯èƒ½ã«

#### 3.9.3 Optimal Transport Graph Neural Networks

Zhang et al., 2020 [^20] ã¯ã€**ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’OTã§æ¯”è¼ƒ** ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸã€‚

**å•é¡Œè¨­å®š**: 2ã¤ã®ã‚°ãƒ©ãƒ• $G_1 = (V_1, E_1)$, $G_2 = (V_2, E_2)$ ã®é¡ä¼¼åº¦ã‚’æ¸¬ã‚ŠãŸã„ã€‚

**è§£æ±ºç­–**:
1. GNNã§å„ãƒãƒ¼ãƒ‰ã®åŸ‹ã‚è¾¼ã¿ $\{\boldsymbol{h}_i\}$ ã‚’è¨ˆç®—
2. 2ã¤ã®ã‚°ãƒ©ãƒ•ã®åŸ‹ã‚è¾¼ã¿é›†åˆ $\{\boldsymbol{h}_i^{(1)}\}$, $\{\boldsymbol{h}_j^{(2)}\}$ é–“ã®Wassersteinè·é›¢ã‚’è¨ˆç®—

$$
W_2(\mu_1, \mu_2), \quad \mu_k = \frac{1}{|V_k|} \sum_{i \in V_k} \delta_{\boldsymbol{h}_i^{(k)}}
$$

3. ã“ã®è·é›¢ã‚’æå¤±é–¢æ•°ã«çµ„ã¿è¾¼ã‚“ã§GNNã‚’è¨“ç·´

**å¿œç”¨**: åˆ†å­ã‚°ãƒ©ãƒ•ç”Ÿæˆ (Huang et al., 2024) [^21] â€” OTã§è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã®è·é›¢ã‚’æœ€å°åŒ–

**å®Ÿè£…ã‚¹ã‚±ãƒƒãƒ**:

```julia
# Pseudo-code for OT-based graph similarity
function graph_ot_similarity(G1, G2, gnn)
    """
    Compute OT distance between graph embeddings.

    Args:
        G1, G2: graphs
        gnn: graph neural network
    """
    # Embed nodes
    H1 = gnn(G1)  # |V1| Ã— d
    H2 = gnn(G2)  # |V2| Ã— d

    # Uniform weights (can use degree-based weights)
    p = ones(size(H1, 1)) / size(H1, 1)
    q = ones(size(H2, 1)) / size(H2, 1)

    # Cost matrix: pairwise Euclidean distances
    C = [norm(H1[i, :] - H2[j, :])^2 for i in 1:size(H1,1), j in 1:size(H2,1)]

    # Sinkhorn OT
    Î³, _ = sinkhorn(C, p, q, Îµ=0.1)

    # W2 distance
    return sqrt(sum(C .* Î³))
end
```

### 3.10 å®Ÿè·µçš„ãªTips â€” OTã‚’ä½¿ã†éš›ã®è½ã¨ã—ç©´ã¨å¯¾ç­–

#### 3.10.1 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\varepsilon$ ã®é¸ã³æ–¹

**å•é¡Œ**: $\varepsilon$ ãŒå°ã•ã™ãã‚‹ã¨æ•°å€¤ä¸å®‰å®šã€å¤§ãã™ãã‚‹ã¨ç²¾åº¦ä½ä¸‹ã€‚

**ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**:
1. **ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒ«ã«åˆã‚ã›ã‚‹**: $\varepsilon \approx 0.01 \sim 0.1 \times \text{median}(C_{ij})$
2. **Warmstart**: $\varepsilon = 1.0$ ã‹ã‚‰å§‹ã‚ã€å¾ã€…ã«å°ã•ãã™ã‚‹ï¼ˆannealingï¼‰
3. **Log-domainå®‰å®šåŒ–**: $\varepsilon < 0.01$ ãªã‚‰å¿…é ˆ

```julia
# Îµ-annealing
function annealed_sinkhorn(C, p, q; Îµ_start=1.0, Îµ_end=0.01, steps=10, max_iter=100)
    u, v = ones(size(C, 1)), ones(size(C, 2))

    for (i, Îµ) in enumerate(range(Îµ_start, Îµ_end, length=steps))
        println("Step $i: Îµ = $(round(Îµ, digits=3))")

        K = exp.(-C / Îµ)
        for _ in 1:max_iter
            u_old = copy(u)
            u = p ./ (K * v)
            v = q ./ (K' * u)
            if norm(u - u_old, Inf) < 1e-6
                break
            end
        end
    end

    K_final = exp.(-C / Îµ_end)
    return u .* K_final .* v'
end
```

#### 3.10.2 ä¸å‡è¡¡åˆ†å¸ƒã¸ã®å¯¾å¿œ (Unbalanced OT)

**å•é¡Œ**: é€šå¸¸ã®OTã¯ $\sum p_i = \sum q_j = 1$ ã‚’è¦æ±‚ï¼ˆè³ªé‡ä¿å­˜ï¼‰ã€‚ã—ã‹ã—å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã¯:
- ãƒã‚¤ã‚ºã‚„å¤–ã‚Œå€¤ã§è³ªé‡ãŒä¸€è‡´ã—ãªã„
- éƒ¨åˆ†çš„ãªãƒãƒƒãƒãƒ³ã‚°ã®ã¿å¿…è¦ï¼ˆä¾‹: ç”»åƒã®ä¸€éƒ¨ã ã‘å¯¾å¿œï¼‰

**è§£æ±ºç­–** (Chizat et al., 2018): **Unbalanced OT** â€” å‘¨è¾ºåˆ¶ç´„ã‚’ç·©å’Œã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£é …ã‚’è¿½åŠ  [^22]:

$$
\min_{\gamma \geq 0} \sum_{ij} C_{ij} \gamma_{ij} + \varepsilon H(\gamma) + \tau_1 D_{\text{KL}}(\gamma \boldsymbol{1}_m \| p) + \tau_2 D_{\text{KL}}(\gamma^\top \boldsymbol{1}_n \| q)
$$

ã“ã“ã§ $\tau_1, \tau_2$ ã¯è³ªé‡ä¸å‡è¡¡ã®è¨±å®¹åº¦ã€‚

**å®Ÿè£…**:

```julia
function unbalanced_sinkhorn(C, p, q; Îµ=0.1, Ï„1=1.0, Ï„2=1.0, max_iter=100, tol=1e-6)
    """
    Unbalanced OT via Sinkhorn.

    Args:
        Ï„1, Ï„2: marginal relaxation parameters (larger = more relaxation)
    """
    n, m = size(C)
    K = exp.(-C / Îµ)

    u = ones(n)
    v = ones(m)

    for iter in 1:max_iter
        u_old = copy(u)

        # Soft marginal constraints
        u = (p ./ (K * v)) .^ (Ï„1 / (Ï„1 + Îµ))
        v = (q ./ (K' * u)) .^ (Ï„2 / (Ï„2 + Îµ))

        if norm(u - u_old, Inf) < tol
            println("Unbalanced Sinkhorn converged in $iter iterations")
            break
        end
    end

    Î³ = u .* K .* v'

    # Check mass conservation
    mass_p = sum(Î³, dims=2)[:]
    mass_q = sum(Î³, dims=1)[:]
    println("Source mass error: $(norm(mass_p - p))")
    println("Target mass error: $(norm(mass_q - q))")

    return Î³
end
```

#### 3.10.3 é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§ã®æ¬¡å…ƒå‰Šæ¸›

**å•é¡Œ**: $d > 100$ ã§ã¯ã€è·é›¢ $\|\boldsymbol{x} - \boldsymbol{y}\|$ ãŒ concentration of measureåŠ¹æœã§æƒ…å ±ã‚’å¤±ã†ã€‚

**è§£æ±ºç­–**:
1. **Sliced Wasserstein Distance**: ãƒ©ãƒ³ãƒ€ãƒ ãª1æ¬¡å…ƒå°„å½±ã‚’å¹³å‡
   $$W_{\text{SW}}(\mu, \nu) = \int_{\mathbb{S}^{d-1}} W_1(\pi_\theta^\sharp \mu, \pi_\theta^\sharp \nu) \, d\theta$$
2. **PCAã§å‰å‡¦ç†**: ä¸»è¦ãª $k$ æ¬¡å…ƒã®ã¿ä½¿ç”¨
3. **Learned metrics**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§è·é›¢ã‚’å­¦ç¿’

```julia
# Sliced Wasserstein (simplified)
function sliced_wasserstein(X, Y; n_projections=100)
    """
    Sliced Wasserstein distance for high-dimensional samples.

    Args:
        X, Y: sample matrices (d Ã— n)
        n_projections: number of random 1D projections
    """
    d = size(X, 1)
    distances = Float64[]

    for _ in 1:n_projections
        # Random direction
        Î¸ = randn(d)
        Î¸ = Î¸ / norm(Î¸)

        # Project samples
        X_proj = Î¸' * X
        Y_proj = Î¸' * Y

        # 1D Wasserstein (sort and compute L1 distance)
        X_sorted = sort(X_proj)
        Y_sorted = sort(Y_proj)
        w1 = mean(abs.(X_sorted - Y_sorted))

        push!(distances, w1)
    end

    return mean(distances)
end
```

:::message
**é€²æ—: 60% å®Œäº†** Neural OTã€Sinkhornæœ€æ–°æ‰‹æ³•ã€Flow Matchingãƒ»Diffusionãƒ»Graphã¸ã®å¿œç”¨ã¾ã§ã€2020-2025å¹´ã®æœ€æ–°ç ”ç©¶ã‚’ç¶²ç¾…ã—ãŸã€‚Part 2ã§å®Ÿè£…ã¨å®Ÿé¨“ã«é€²ã‚€ã€‚
:::

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Monge, G. (1781). MÃ©moire sur la thÃ©orie des dÃ©blais et des remblais. Histoire de l'AcadÃ©mie Royale des Sciences de Paris.

[^2]: Brenier, Y. (1991). Polar factorization and monotone rearrangement of vector-valued functions. Communications on Pure and Applied Mathematics, 44(4), 375-417.

[^3]: Arjovsky, M., Chintala, S., & Bottou, L. (2017). Wasserstein Generative Adversarial Networks. In ICML.
@[card](https://arxiv.org/abs/1701.07875)

[^4]: Liu, X., Gong, C., & Liu, Q. (2022). Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. In ICLR 2023.
@[card](https://arxiv.org/abs/2209.03003)

[^5]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). Score-Based Generative Modeling through Stochastic Differential Equations. In ICLR.
@[card](https://arxiv.org/abs/2011.13456)

[^6]: Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. C. (2017). Improved Training of Wasserstein GANs. In NeurIPS.
@[card](https://arxiv.org/abs/1704.00028)

[^7]: Miyato, T., Kataoka, T., Koyama, M., & Yoshida, Y. (2018). Spectral Normalization for Generative Adversarial Networks. In ICLR.
@[card](https://arxiv.org/abs/1802.05957)

[^8]: Amos, B., Xu, L., & Kolter, J. Z. (2017). Input Convex Neural Networks. In ICML.
@[card](https://arxiv.org/abs/1609.07152)

[^9]: Cuturi, M. (2013). Sinkhorn Distances: Lightspeed Computation of Optimal Transport. In NeurIPS.
@[card](https://arxiv.org/abs/1306.0895)

[^10]: Korotin, A., Selikhanovych, D., & Burnaev, E. (2023). Neural Optimal Transport. In ICLR.
@[card](https://arxiv.org/abs/2201.12220)

[^11]: Uscidda, A., & Cuturi, M. (2024). Wasserstein Wormhole: Scalable Optimal Transport Distance with Transformers. In ICML.
@[card](https://arxiv.org/abs/2404.09411)

[^12]: Korotin, A., Li, L., Genevay, A., Solomon, J., Filippov, A., & Burnaev, E. (2021). Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. In NeurIPS.
@[card](https://arxiv.org/abs/2106.01954)

[^13]: Schmitzer, B. (2016). Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems. SIAM Journal on Scientific Computing, 41(3), A1443-A1481.
@[card](https://arxiv.org/abs/1610.06519)

[^14]: Scetbon, M., Cuturi, M., & PeyrÃ©, G. (2021). Low-Rank Sinkhorn Factorization. In ICML.
@[card](https://arxiv.org/abs/2103.04737)

[^15]: Xie, Y., Wang, X., Wang, R., & Zha, H. (2021). Optimal Transport with f-divergence Regularization and Generalized Sinkhorn Algorithm. In AISTATS.
@[card](https://arxiv.org/abs/2105.14337)

[^16]: Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., & Le, M. (2023). Flow Matching for Generative Modeling. In ICLR.
@[card](https://arxiv.org/abs/2210.02747)

[^17]: Tong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G., & Bengio, Y. (2023). Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport. In AISTATS.
@[card](https://arxiv.org/abs/2302.00482)

[^18]: Pooladian, A. A., Ben-Hamu, H., Domingo-Enrich, C., Amos, B., Lipman, Y., & Chen, R. (2023). Multisample Flow Matching: Straightening Flows with Minibatch Couplings. In ICML.
@[card](https://arxiv.org/abs/2304.14772)

[^19]: Liu, X., Gong, C., & Liu, Q. (2022). Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. arXiv preprint.
@[card](https://arxiv.org/abs/2209.03003)

[^20]: Zhang, X., Zhao, L., Arnold, A., Liu, N., Rong, Y., & Yan, J. (2020). Optimal Transport Graph Neural Networks. arXiv preprint.
@[card](https://arxiv.org/abs/2006.04804)

[^21]: Huang, R., Li, J., Wang, Y., & Zhang, C. (2024). Improving Molecular Graph Generation with Flow Matching and Optimal Transport. arXiv preprint.
@[card](https://arxiv.org/abs/2411.05676)

[^22]: Chizat, L., PeyrÃ©, G., Schmitzer, B., & Vialard, F. X. (2018). Scaling Algorithms for Unbalanced Optimal Transport Problems. Mathematics of Computation, 87(314), 2563-2609.
@[card](https://arxiv.org/abs/1607.05816)

### ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ãƒ»æ•™ç§‘æ›¸

- PeyrÃ©, G., & Cuturi, M. (2019). Computational Optimal Transport. Foundations and Trends in Machine Learning, 11(5-6), 355-607.
@[card](https://arxiv.org/abs/1803.00567)

- Santambrogio, F. (2015). Optimal Transport for Applied Mathematicians. BirkhÃ¤user.

- Villani, C. (2009). Optimal Transport: Old and New. Springer. (Fields Medalå—è³æ¥­ç¸¾)

### æœ€æ–°ã‚µãƒ¼ãƒ™ã‚¤ (2023)

- Calvello, E., NÃ¼sken, N., Reich, S., & Schillings, C. (2023). Recent Advances in Optimal Transport for Machine Learning. arXiv preprint.
@[card](https://arxiv.org/abs/2306.16156)

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
