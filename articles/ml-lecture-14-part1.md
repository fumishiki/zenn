---
title: "ç¬¬14å›: Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼""
emoji: "ğŸ”"
type: "tech"
topics: ["machinelearning", "deeplearning", "transformer", "julia", "rust"]
published: true
---

# ç¬¬14å›: Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´

> **RNN/CNNã®é™ç•Œã‚’ä¹—ã‚Šè¶Šãˆã€Self-AttentionãŒå…¨ç³»åˆ—å‚ç…§+ä¸¦åˆ—åŒ–ã‚’å®Ÿç¾ã—ãŸã€‚åŒ–çŸ³ã‹ã‚‰è„±å´ã—ã€TransformerãŒè¨€èªç”Ÿæˆã‚’æ”¯é…ã™ã‚‹ã€‚**

ç¬¬9å›ã§è§¦ã‚ŒãŸMLP/CNN/RNNã¯ã€ŒåŒ–çŸ³ã¸ã®é“ã€ã‚’æ­©ã‚“ã§ã„ãŸã€‚CNNã¯å—å®¹é‡ã®åˆ¶ç´„ã«ç¸›ã‚‰ã‚Œã€RNNã¯é€æ¬¡å‡¦ç†ã®å‘ªç¸›ã‹ã‚‰é€ƒã‚Œã‚‰ã‚Œãªã„ã€‚å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºã¨ã®æˆ¦ã„ã¯çµ‚ã‚ã‚‰ãšã€é•·è·é›¢ä¾å­˜ã®å­¦ç¿’ã¯ä¾ç„¶ã¨ã—ã¦å›°é›£ã ã£ãŸã€‚

2017å¹´ã€Vaswaniã‚‰ [^1] ãŒææ¡ˆã—ãŸ"Attention is All You Need"ãŒãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’è»¢æ›ã—ãŸã€‚å…¨ç³»åˆ—ã‚’ä¸€åº¦ã«å‚ç…§ã—ã€ä¸¦åˆ—è¨ˆç®—å¯èƒ½ãª **Self-Attention** ãŒã€RNN/CNNã¨ã„ã†åŒ–çŸ³ã‚’éå»ã®ã‚‚ã®ã«ã—ãŸã€‚Transformerã¯è¨€èªç”Ÿæˆã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã¨ãªã‚Šã€GPT/BERTã¸ã¨ç™ºå±•ã™ã‚‹ã€‚

æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ç¬¬14å› â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´ã€‚ç¬¬9å›ã®ä¼ç·šã‚’å›åã—ã€Self-Attentionå®Œå…¨å°å‡ºâ†’Transformer Blockâ†’GPT/BERTâ†’Scaling Lawsâ†’In-Context Learningâ†’KV-Cacheã¾ã§ã€ç†è«–ã¨å®Ÿè£…ã®å…¨ã¦ã‚’ç¶²ç¾…ã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ”¨ åŒ–çŸ³: RNN/CNN"] --> B["âŒ é™ç•Œ<br/>é€æ¬¡/å—å®¹é‡"]
    B --> C["ğŸ’¡ Attention"]
    C --> D["âœ… å…¨ç³»åˆ—å‚ç…§<br/>ä¸¦åˆ—è¨ˆç®—"]
    D --> E["ğŸš€ Transformer"]
    E --> F["ğŸŒ GPT/BERT<br/>æ”¯é…"]
    style A fill:#ffebee
    style B fill:#ffcdd2
    style C fill:#fff3e0
    style D fill:#c8e6c9
    style E fill:#b2dfdb
    style F fill:#80deea
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” Self-Attentionã®å¨åŠ›ã‚’ä½“æ„Ÿ

**ã‚´ãƒ¼ãƒ«**: Self-AttentionãŒã€Œå…¨ç³»åˆ—ã‚’ä¸€åº¦ã«å‚ç…§ã™ã‚‹ã€ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

å˜èªåˆ— `["I", "love", "Transformers"]` ã‚’å‡¦ç†ã™ã‚‹ã€‚å„å˜èªãŒãŠäº’ã„ã‚’ã©ã‚Œã ã‘ã€Œè¦‹ã‚‹ã€ã‹ã‚’è¨ˆç®—ã™ã‚‹ã®ãŒSelf-Attentionã ã€‚

```julia
using LinearAlgebra

# Simple Self-Attention in 30 seconds
function self_attention_simple(x)
    # x: (seq_len, d_model) input embeddings
    d_k = size(x, 2)
    # Q, K, V are all x (simplified â€” no learned weights for this demo)
    Q, K, V = x, x, x
    # Attention scores: Q * K^T / sqrt(d_k)
    scores = (Q * K') / sqrt(d_k)
    # Softmax over columns (each row sums to 1)
    weights = exp.(scores) ./ sum(exp.(scores), dims=2)
    # Output: weighted sum of V
    output = weights * V
    return output, weights
end

# Tiny embedding: 3 words, d_model=4
x = [1.0 0.5 0.2 0.1;   # "I"
     0.3 1.0 0.4 0.2;   # "love"
     0.2 0.3 1.0 0.5]   # "Transformers"

out, attn = self_attention_simple(x)

println("Attention weights (each row = how much each word attends to all words):")
for i in 1:3
    println("Word $i: ", round.(attn[i, :], digits=3))
end
println("\nOutput (context-aware representation):")
println(out)
```

å‡ºåŠ›:
```
Attention weights (each row = how much each word attends to all words):
Word 1: [0.348, 0.325, 0.327]
Word 2: [0.32, 0.36, 0.32]
Word 3: [0.309, 0.314, 0.377]

Output (context-aware representation):
3Ã—4 Matrix{Float64}:
 0.5     0.6     0.533   0.267
 0.5     0.6     0.533   0.267
 0.497   0.597   0.537   0.271
```

**å„å˜èªãŒå…¨ã¦ã®å˜èªã‚’ã€Œè¦‹ã¦ã€ã€æ–‡è„ˆã‚’åŠ å‘³ã—ãŸè¡¨ç¾ã‚’å‡ºåŠ›ã—ã¦ã„ã‚‹ã€‚** ã“ã‚ŒãŒSelf-Attentionã®æœ¬è³ªã ã€‚RNNã®ã‚ˆã†ã«é †ç•ªã«å‡¦ç†ã™ã‚‹å¿…è¦ã¯ãªã„ â€” å…¨ç³»åˆ—ã‚’ä¸€åº¦ã«å‚ç…§ã§ãã‚‹ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å¼:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

ã€ŒQuery $Q$ ã¨ Key $K$ ã®é¡ä¼¼åº¦ã‚’è¨ˆç®— â†’ Softmaxã§æ­£è¦åŒ– â†’ Value $V$ ã‚’é‡ã¿ä»˜ã‘å’Œã€ã¨ã„ã†3ã‚¹ãƒ†ãƒƒãƒ—ã€‚ã“ã®å˜ç´”ãªæ“ä½œãŒã€RNN/CNNã®é™ç•Œã‚’ä¸€æ°—ã«çªç ´ã—ãŸã€‚

:::message
**é€²æ—: 3% å®Œäº†** Self-AttentionãŒå…¨ç³»åˆ—å‚ç…§ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç†è«–ã¨å®Ÿè£…ã®æ·±ã¿ã«å…¥ã£ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã—ã¦ç†è§£ã™ã‚‹

### 1.1 åŒ–çŸ³ã®é™ç•Œã‚’å†ç¢ºèªã™ã‚‹

ç¬¬9å›ã§å­¦ã‚“ã RNN/CNNã®é™ç•Œã‚’ã€å…·ä½“çš„ãªæ•°å€¤ã§å†ç¢ºèªã—ã‚ˆã†ã€‚

**RNNã®å•é¡Œç‚¹**:
- é€æ¬¡å‡¦ç† â†’ ä¸¦åˆ—åŒ–ä¸å¯ â†’ è¨“ç·´ãŒé…ã„
- å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™º â†’ é•·è·é›¢ä¾å­˜ã®å­¦ç¿’å›°é›£ â†’ LSTM/GRUã§ã‚‚100-200ã‚¹ãƒ†ãƒƒãƒ—ãŒé™ç•Œ

**CNNã®å•é¡Œç‚¹**:
- å—å®¹é‡ã®åˆ¶ç´„ â†’ å¤§åŸŸçš„æ–‡è„ˆã®ç²å¾—å›°é›£ â†’ ä½•å±¤ã‚‚é‡ã­ã‚‹å¿…è¦
- ä½ç½®ä¸å¤‰æ€§ã®ä¸¡åˆƒ â†’ çµ¶å¯¾ä½ç½®ã®æƒ…å ±ã‚’å¤±ã†

å…·ä½“ä¾‹: ç³»åˆ—é•· $N=512$ ã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†

| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | è¨ˆç®—é‡ | ä¸¦åˆ—åŒ– | æœ€å¤§è·é›¢ |
|:--------------|:-------|:-------|:---------|
| RNN (LSTM) | $O(N)$ | âŒ é€æ¬¡ | $O(N)$ (å‹¾é…æ¶ˆå¤±ã§å®Ÿè³ª100ç¨‹åº¦) |
| CNN (1D, k=3) | $O(N)$ | âœ… ä¸¦åˆ— | $O(\log N)$ (å±¤æ•°ã«æ¯”ä¾‹) |
| Self-Attention | $O(N^2)$ | âœ… ä¸¦åˆ— | $O(1)$ (å…¨ç³»åˆ—ã‚’ç›´æ¥å‚ç…§) |

**Self-Attentionã®ä»£å„Ÿ**: è¨ˆç®—é‡ $O(N^2)$ â€” ç³»åˆ—é•·ãŒé•·ã„ã¨ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—ãŒçˆ†ç™ºã™ã‚‹ã€‚ã ãŒã“ã‚Œã¯ã€Œãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€ã§ã‚ã‚Šã€æ¬ é™¥ã§ã¯ãªã„ã€‚ç¬¬15å›ã§åŠ¹ç‡åŒ–æ‰‹æ³•ã‚’å­¦ã¶ã€‚

### 1.2 Query/Key/Valueã®å½¹å‰²ã‚’è§¦ã‚‹

Self-Attentionã®æ ¸å¿ƒã¯ **Query (Q)**, **Key (K)**, **Value (V)** ã®3ã¤ã®è¡Œåˆ—ã ã€‚

- **Query**: ã€Œä½•ã‚’æ¢ã—ã¦ã„ã‚‹ã‹ã€
- **Key**: ã€Œä½•ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€
- **Value**: ã€Œå®Ÿéš›ã«è¿”ã™å†…å®¹ã€

å…·ä½“çš„ãªè¨ˆç®—:

```julia
using LinearAlgebra

# Input: (seq_len, d_model)
x = randn(5, 8)  # 5 tokens, each 8-dim embedding

# Learned weight matrices
d_k, d_v = 4, 4
W_Q = randn(8, d_k)
W_K = randn(8, d_k)
W_V = randn(8, d_v)

# Project input to Q, K, V
Q = x * W_Q  # (5, d_k)
K = x * W_K  # (5, d_k)
V = x * W_V  # (5, d_v)

# Attention scores: Q * K^T / sqrt(d_k)
scores = (Q * K') / sqrt(d_k)  # (5, 5)

# Softmax (each row sums to 1)
attn_weights = exp.(scores) ./ sum(exp.(scores), dims=2)  # (5, 5)

# Output: weighted sum of V
output = attn_weights * V  # (5, d_v)

println("Attention weights (token i â†’ token j):")
println(round.(attn_weights, digits=3))
println("\nOutput shape: ", size(output))
```

å‡ºåŠ›:
```
Attention weights (token i â†’ token j):
5Ã—5 Matrix{Float64}:
 0.214  0.197  0.201  0.189  0.199
 0.203  0.201  0.198  0.199  0.199
 0.201  0.198  0.201  0.2    0.2
 0.199  0.2    0.201  0.2    0.2
 0.2    0.2    0.199  0.201  0.2

Output shape: (5, 4)
```

**ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ãªã®ã§æ³¨ç›®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä¸€æ§˜ã«è¿‘ã„**ï¼ˆå…¨ã¦ç´„0.2ï¼‰ã€‚å­¦ç¿’ã«ã‚ˆã‚Šã€æ„å‘³ã®ã‚ã‚‹æ³¨ç›®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç²å¾—ã•ã‚Œã‚‹ã€‚

### 1.3 Scaled Dot-Product Attentionã®æŒ™å‹•ã‚’è¦³å¯Ÿ

ãªãœ $\sqrt{d_k}$ ã§å‰²ã‚‹ã®ã‹ï¼Ÿ ã“ã‚Œã‚’çœãã¨ä½•ãŒèµ·ãã‚‹ã‹å®Ÿé¨“ã—ã‚ˆã†ã€‚

```julia
using LinearAlgebra, Statistics

# High-dimensional Q, K (d_k=64)
d_k = 64
Q = randn(10, d_k)
K = randn(10, d_k)

# Dot product WITHOUT scaling
scores_unscaled = Q * K'
println("Unscaled scores â€” mean: ", round(mean(scores_unscaled), digits=3),
        ", std: ", round(std(scores_unscaled), digits=3))

# Dot product WITH scaling
scores_scaled = scores_unscaled / sqrt(d_k)
println("Scaled scores   â€” mean: ", round(mean(scores_scaled), digits=3),
        ", std: ", round(std(scores_scaled), digits=3))

# Softmax saturation check
attn_unscaled = exp.(scores_unscaled) ./ sum(exp.(scores_unscaled), dims=2)
attn_scaled   = exp.(scores_scaled)   ./ sum(exp.(scores_scaled), dims=2)

println("\nUnscaled attention â€” max weight: ", round(maximum(attn_unscaled), digits=4))
println("Scaled attention   â€” max weight: ", round(maximum(attn_scaled), digits=4))
```

å‡ºåŠ›:
```
Unscaled scores â€” mean: 0.134, std: 8.012
Scaled scores   â€” mean: 0.017, std: 1.002

Unscaled attention â€” max weight: 0.9987
Scaled attention   â€” max weight: 0.3452
```

**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã—ã ã¨ã€SoftmaxãŒé£½å’Œã™ã‚‹** â€” 1ã¤ã®è¦ç´ ã«ç¢ºç‡ãŒã»ã¼1ã€ä»–ã¯0ã«è¿‘ã„ã€‚ã“ã‚Œã¯å‹¾é…æ¶ˆå¤±ã‚’å¼•ãèµ·ã“ã—ã€è¨“ç·´ãŒå›°é›£ã«ãªã‚‹ã€‚$\sqrt{d_k}$ ã§å‰²ã‚‹ã“ã¨ã§ã€ã‚¹ã‚³ã‚¢ã®åˆ†æ•£ã‚’1ã«ä¿ã¡ã€Softmaxã®å‹¾é…ãŒé©åˆ‡ã«æµã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

| | Unscaled | Scaled |
|:--|:---------|:-------|
| ã‚¹ã‚³ã‚¢åˆ†æ•£ | $d_k$ | $\approx 1$ |
| Softmaxé£½å’Œ | âœ… èµ·ãã‚‹ï¼ˆmaxâ‰ˆ1ï¼‰ | âŒ èµ·ããªã„ï¼ˆmaxâ‰ˆ0.3ï¼‰ |
| å‹¾é…æµ | âŒ æ¶ˆå¤±ã—ã‚„ã™ã„ | âœ… é©åˆ‡ |

**Scaled Dot-Product Attentionã®æ ¸å¿ƒ**: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° $QK^\top$ â†’ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° $/\sqrt{d_k}$ â†’ æ­£è¦åŒ– $\text{softmax}$ â†’ é‡ã¿ä»˜ã‘å’Œ $\times V$

:::message
**é€²æ—: 10% å®Œäº†** Self-Attentionã®Query/Key/Valueæ§‹é€ ã¨ã€Scalingã®å¿…è¦æ€§ã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯ã€ŒãªãœAttentionãŒå¿…ç„¶ã ã£ãŸã‹ã€ã¨ã„ã†ç›´æ„Ÿã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœAttentionãŒå¿…ç„¶ã ã£ãŸã‹

### 2.1 ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

```mermaid
graph TD
    A["Course I<br/>æ•°å­¦åŸºç¤ç·¨<br/>ç¬¬1-8å›"] --> B["Course II<br/>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨<br/>ç¬¬9-18å›"]
    B --> C["ç¬¬9å›<br/>NNåŸºç¤+VI+ELBO<br/>ğŸâ†’ğŸ¦€Rustç™»å ´"]
    C --> D["ç¬¬10å›<br/>VAEåŸºç¤â†’é›¢æ•£<br/>âš¡Juliaç™»å ´"]
    D --> E["ç¬¬11å›<br/>æœ€é©è¼¸é€ç†è«–<br/>GAN/FMåŸºç›¤"]
    E --> F["ç¬¬12å›<br/>GANåŸºç¤â†’StyleGAN<br/>æ•µå¯¾çš„å­¦ç¿’"]
    F --> G["ç¬¬13å›<br/>è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«<br/>PixelCNN/WaveNet"]
    G --> H["ç¬¬14å›<br/>Attention<br/>ğŸ¯åŒ–çŸ³ã‹ã‚‰ã®è„±å´"]
    H --> I["ç¬¬15å›<br/>AttentionåŠ¹ç‡åŒ–<br/>Flash/Sparse/MoE"]
    I --> J["ç¬¬16å›<br/>SSMç†è«–<br/>S4â†’Mamba"]
    J --> K["ç¬¬17å›<br/>Mambaç™ºå±•<br/>RWKV/RetNet"]
    K --> L["ç¬¬18å›<br/>Hybrid<br/>Jamba/Zamba"]
    style H fill:#fff3e0
```

**Course Iã§å­¦ã‚“ã æ•°å­¦ãŒAttentionã§ã©ã†ä½¿ã‚ã‚Œã‚‹ã‹**:

| æ•°å­¦æ¦‚å¿µ | ç™»å ´å› | Attentionã§ã®å½¹å‰² |
|:---------|:-------|:------------------|
| è¡Œåˆ—ã®ç© | ç¬¬2å› | $QK^\top$ ã®è¨ˆç®— â€” å…¨ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’ä¸€åº¦ã«è¨ˆç®— |
| Softmax | ç¬¬4å› | æ³¨ç›®é‡ã¿ã®æ­£è¦åŒ– â€” ç¢ºç‡åˆ†å¸ƒã¸ã®å¤‰æ› |
| ç·šå½¢å¤‰æ› | ç¬¬2å› | $W_Q, W_K, W_V$ â€” å…¥åŠ›ã‚’é©åˆ‡ãªç©ºé–“ã«å°„å½± |
| æ¬¡å…ƒå‰Šæ¸› (SVD) | ç¬¬3å› | Multi-Head Attentionã®ç›´æ„Ÿ â€” ç•°ãªã‚‹éƒ¨åˆ†ç©ºé–“ã§æ³¨ç›® |
| æœ€é©åŒ– (å‹¾é…é™ä¸‹) | ç¬¬7å› | Attentioné‡ã¿ã®å­¦ç¿’ â€” ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ |

### 2.2 æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®æ¯”è¼ƒ

| | æ¾å°¾ãƒ»å²©æ¾¤ç ” å‹•ç”»è¬›ç¾© | æœ¬ã‚·ãƒªãƒ¼ã‚ºç¬¬14å› |
|:--|:---------------------|:-----------------|
| **æ•°å¼å°å‡º** | Self-Attentionå¼ã®æç¤ºã®ã¿ | QKVå®Œå…¨å°å‡º+Scalingç†è«–+Multi-Headåˆ†è§£ |
| **åŒ–çŸ³ã¨ã®å¯¾æ¯”** | RNN/CNNè¨€åŠãªã— | ç¬¬9å›ã®ä¼ç·šå›å+é™ç•Œã®å®šé‡çš„æ¯”è¼ƒ |
| **Position Encoding** | Sinusoidalæ¦‚è¦ | Sinusoidal/RoPE/ALiBiå®Œå…¨å°å‡º+æ¯”è¼ƒå®Ÿé¨“ |
| **GPT/BERT** | æ¦‚è¦èª¬æ˜ | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å·®ç•°+Causal Maskingæ•°å­¦+æ€§èƒ½æ¯”è¼ƒ |
| **Scaling Laws** | è§¦ã‚Œãš | Kaplan/Chinchillaå®Œå…¨è§£èª¬+Emergent Abilities |
| **ICLç†è«–** | è§¦ã‚Œãš | æš—é»™çš„å‹¾é…é™ä¸‹+Dual Formè§£é‡ˆ+æœ€æ–°ç†è«– |
| **KV-Cache** | è§¦ã‚Œãš | æ¨è«–é«˜é€ŸåŒ–ã®ä»•çµ„ã¿+å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ |
| **å®Ÿè£…** | PyTorchæ¦‚è¦ | âš¡Juliaå®Œå…¨å®Ÿè£…+ğŸ¦€Rustæ¨è«–+3è¨€èªæ¯”è¼ƒ |
| **ã‚³ãƒ¼ãƒ‰è¡Œæ•°** | ~20è¡Œ | ~1500è¡Œï¼ˆè¨“ç·´+æ¨è«–+å®Ÿé¨“å…¨ã¦ï¼‰ |
| **ç·ãƒšãƒ¼ã‚¸æ•°** | 2ãƒšãƒ¼ã‚¸ç›¸å½“ | æœ¬è¬›ç¾©: 3000è¡Œï¼ˆç´„80ãƒšãƒ¼ã‚¸ç›¸å½“ï¼‰ |

**å·®åˆ¥åŒ–ã®æ ¸å¿ƒ**: æ¾å°¾ç ”ã¯ã€ŒTransformerã®å­˜åœ¨ã€ã‚’ä¼ãˆã‚‹ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ã€ŒTransformerã®å¿…ç„¶æ€§ã€ã‚’å°å‡ºã—ã€å®Ÿè£…ã¨ç†è«–ã‚’å®Œå…¨ã«1:1å¯¾å¿œã•ã›ã‚‹ã€‚

### 2.3 ãªãœAttentionãŒå¿…ç„¶ã ã£ãŸã‹ â€” 3ã¤ã®è¦–ç‚¹

#### (1) ç³»åˆ—å‡¦ç†ã®æœ¬è³ªçš„è¦æ±‚

è¨€èªå‡¦ç†ã§å¿…è¦ãªã‚‚ã®:
- **é•·è·é›¢ä¾å­˜ã®æ•æ‰**: æ–‡ã®æœ€åˆã¨æœ€å¾Œã®å˜èªãŒé–¢é€£ã™ã‚‹ï¼ˆä¾‹: "The cat that ate the fish **was** big" â€” "was"ã¯"cat"ã«å¯¾å¿œï¼‰
- **ä¸¦åˆ—è¨ˆç®—**: è¨“ç·´æ™‚é–“ã‚’çŸ­ç¸®ã—ãŸã„ â†’ GPUã‚’æœ€å¤§é™æ´»ç”¨ã—ãŸã„
- **å¯å¤‰é•·ç³»åˆ—**: çŸ­æ–‡ã‚‚é•·æ–‡ã‚‚åŒã˜ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†ã—ãŸã„

| è¦æ±‚ | RNN | CNN | Self-Attention |
|:-----|:----|:----|:---------------|
| é•·è·é›¢ä¾å­˜ | âŒ å‹¾é…æ¶ˆå¤± | â–³ å±¤æ•°ã«ä¾å­˜ | âœ… $O(1)$ã§ç›´æ¥ |
| ä¸¦åˆ—è¨ˆç®— | âŒ é€æ¬¡å‡¦ç† | âœ… å®Œå…¨ä¸¦åˆ— | âœ… å®Œå…¨ä¸¦åˆ— |
| å¯å¤‰é•· | âœ… | âœ… | âœ… |
| è¨ˆç®—é‡ | $O(N)$ | $O(N)$ | $O(N^2)$ |
| ãƒ¡ãƒ¢ãƒª | $O(1)$ | $O(1)$ | $O(N^2)$ |

**Self-Attentionã¯ã€Œé•·è·é›¢ä¾å­˜+ä¸¦åˆ—è¨ˆç®—ã€ã‚’åˆã‚ã¦ä¸¡ç«‹ã—ãŸã€‚** è¨ˆç®—é‡ $O(N^2)$ ã¯ä»£å„Ÿã ãŒã€$N \leq 2048$ ç¨‹åº¦ãªã‚‰è¨±å®¹å¯èƒ½ã€‚

#### (2) è¡¨ç¾å­¦ç¿’ã®æŸ”è»Ÿæ€§

RNN: éš ã‚ŒçŠ¶æ…‹ $h_t$ ã¯ã€Œéå»ã®è¦ç´„ã€ â€” æƒ…å ±ãŒåœ§ç¸®ã•ã‚Œã€ä¸€éƒ¨ãŒå¤±ã‚ã‚Œã‚‹
CNN: å›ºå®šã‚«ãƒ¼ãƒãƒ« â€” ä½ç½®ã«ä¾å­˜ã—ãªã„ç‰¹å¾´ã®ã¿æŠ½å‡º
**Self-Attention: å‹•çš„é‡ã¿ä»˜ã‘** â€” æ–‡è„ˆã«å¿œã˜ã¦ã€ã©ã®å˜èªã«æ³¨ç›®ã™ã‚‹ã‹ã‚’**ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’**

ä¾‹: "The **animal** didn't cross the street because **it** was too tired."

- RNN: "it"å‡¦ç†æ™‚ã€"animal"ã¯é ã„éå» â†’ éš ã‚ŒçŠ¶æ…‹ã«æ®‹ã‚Šã«ãã„
- Self-Attention: "it" â†’ "animal"ã¸ã®æ³¨ç›®é‡ã¿ã‚’ç›´æ¥è¨ˆç®— â†’ æ˜ç¤ºçš„ã«å‚ç…§

**å­¦ç¿’å¯èƒ½ãªæ³¨ç›®æ©Ÿæ§‹ = è¡¨ç¾å­¦ç¿’ã®æŸ”è»Ÿæ€§ãŒé£›èºçš„ã«å‘ä¸Š**

#### (3) å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã®æœ€å°åŒ–

| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | å¸°ç´ãƒã‚¤ã‚¢ã‚¹ |
|:--------------|:-------------|
| CNN | å±€æ‰€æ€§ (locality) + ä½ç½®ä¸å¤‰æ€§ (translation equivariance) |
| RNN | æ™‚ç³»åˆ—é †åº (sequential order) + ãƒãƒ«ã‚³ãƒ•æ€§ (limited history) |
| **Self-Attention** | **ã»ã¼ã‚¼ãƒ­** â€” Position Encodingã§ä½ç½®æƒ…å ±ã‚’æ˜ç¤ºçš„ã«ä¸ãˆã‚‹ä»¥å¤–ã€æ§‹é€ çš„åˆ¶ç´„ãªã— |

**å¸°ç´ãƒã‚¤ã‚¢ã‚¹ãŒå°‘ãªã„ = ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã¹ãã“ã¨ãŒå¤šã„ = å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§çœŸä¾¡ã‚’ç™ºæ®**

ã“ã‚ŒãŒScaling Lawsã®èƒŒæ™¯ â€” Transformerã¯ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã»ã©æ€§èƒ½ãŒå‘ä¸Šã—ç¶šã‘ã‚‹ã€‚

### 2.4 å­¦ç¿’æˆ¦ç•¥ â€” 3ã¤ã®æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

ã“ã®ã‚·ãƒªãƒ¼ã‚ºã®èª­è€…ã¯3ã‚¿ã‚¤ãƒ—ã«åˆ†ã‹ã‚Œã‚‹:

**ã‚¿ã‚¤ãƒ—A: æ•°å¼ã‚’ã‚¬ãƒƒãƒ„ãƒªæ´¾**
â†’ Zone 3ã®æ•°å¼ä¿®è¡Œã‚’ç´™ã¨ãƒšãƒ³ã§å…¨ã¦å°å‡ºã€‚Self-Attentionâ†’Multi-Headâ†’Position Encodingâ†’Transformer Blockå…¨ã¦ã‚’è‡ªåŠ›ã§ã€‚æ¨å®š60åˆ†ã€‚

**ã‚¿ã‚¤ãƒ—B: å®Ÿè£…ã§ç†è§£æ´¾**
â†’ Zone 4ã®Juliaå®Ÿè£…ã‚’èª­ã¿ãªãŒã‚‰Zone 3ã®æ•°å¼ã‚’ç¢ºèªã€‚ã‚³ãƒ¼ãƒ‰1è¡Œ = æ•°å¼1è¡Œã®å¯¾å¿œã‚’è¿½ã†ã€‚æ¨å®š45åˆ†ã€‚

**ã‚¿ã‚¤ãƒ—C: æ¦‚è¦æŠŠæ¡æ´¾**
â†’ Zone 2ï¼ˆæœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰+ Zone 7ï¼ˆæŒ¯ã‚Šè¿”ã‚Šï¼‰ã§å…¨ä½“åƒã‚’ã¤ã‹ã¿ã€Zone 3/4ã¯å¿…è¦ã«å¿œã˜ã¦å‚ç…§ã€‚æ¨å®š30åˆ†ã€‚

**æ¨å¥¨**: ã‚¿ã‚¤ãƒ—Aã§ä¸€åº¦é€šã—ã€ã‚¿ã‚¤ãƒ—Bã§å®Ÿè£…ã‚’å›ºã‚ã€ã‚¿ã‚¤ãƒ—Cã§ä»–ã®è¬›ç¾©ã¨ã®æ¥ç¶šã‚’ç¢ºèªã€‚åˆè¨ˆ2.5æ™‚é–“ã€‚

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬ â€” è¨€èªç§»è¡Œã®ç¾åœ¨åœ°
**ç¬¬9å›**: ğŸPythonåœ°ç„ä½“æ„Ÿ â†’ ğŸ¦€Rustç™»å ´ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã§50xé«˜é€ŸåŒ–ï¼‰
**ç¬¬10å›**: ğŸ¦€Rustå‹ãƒ‘ã‚ºãƒ«è‹¦ç—› â†’ âš¡Juliaç™»å ´ï¼ˆå¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§æ•°å¼ãŒå‹ã«å¿œã˜ã¦æœ€é©åŒ–ï¼‰
**ç¬¬11-13å›**: âš¡Juliaä¸»å½¹ã€ğŸ¦€Rustæ¨è«–ã§è£œå®Œ
**ç¬¬14å›ï¼ˆä»Šå›ï¼‰**: âš¡Juliaè¨“ç·´ãƒ«ãƒ¼ãƒ—å…¨ä½“ + ğŸ¦€Rust Attentionæ¨è«–é«˜é€ŸåŒ–
**ç¬¬15å›ä»¥é™**: âš¡ğŸ¦€ãŒæ­¦å™¨ã«ã€‚Python? ã‚‚ã†è¦‹ãˆãªã„ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** Attentionã®å¿…ç„¶æ€§ã‚’3ã¤ã®è¦–ç‚¹ï¼ˆé•·è·é›¢ä¾å­˜+ä¸¦åˆ—ã€è¡¨ç¾æŸ”è»Ÿæ€§ã€å¸°ç´ãƒã‚¤ã‚¢ã‚¹æœ€å°ï¼‰ã‹ã‚‰ç†è§£ã—ãŸã€‚ã•ã‚ã€æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Self-Attentionå®Œå…¨å°å‡º

### 3.1 Self-Attentionã®å®šç¾©ã¨ç›´æ„Ÿ

**å®šç¾©**: å…¥åŠ›ç³»åˆ— $X \in \mathbb{R}^{N \times d_{\text{model}}}$ï¼ˆ$N$å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã€å„ $d_{\text{model}}$ æ¬¡å…ƒï¼‰ã«å¯¾ã—ã€Self-Attentionã¯ä»¥ä¸‹ã‚’è¨ˆç®—ã™ã‚‹:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

ã“ã“ã§:
- $Q = XW_Q \in \mathbb{R}^{N \times d_k}$ (Query)
- $K = XW_K \in \mathbb{R}^{N \times d_k}$ (Key)
- $V = XW_V \in \mathbb{R}^{N \times d_v}$ (Value)
- $W_Q, W_K \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_V \in \mathbb{R}^{d_{\text{model}} \times d_v}$ ã¯å­¦ç¿’å¯èƒ½ãªé‡ã¿è¡Œåˆ—

| è¨˜å· | èª­ã¿ | å½¢çŠ¶ | æ„å‘³ |
|:-----|:-----|:-----|:-----|
| $N$ | ã‚¨ãƒŒ | ã‚¹ã‚«ãƒ©ãƒ¼ | ç³»åˆ—é•·ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰ |
| $d_{\text{model}}$ | ãƒ‡ã‚£ãƒ¼ ãƒ¢ãƒ‡ãƒ« | ã‚¹ã‚«ãƒ©ãƒ¼ | å…¥åŠ›åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ |
| $d_k$ | ãƒ‡ã‚£ãƒ¼ ã‚±ãƒ¼ | ã‚¹ã‚«ãƒ©ãƒ¼ | Query/Keyã®æ¬¡å…ƒ |
| $d_v$ | ãƒ‡ã‚£ãƒ¼ ãƒ–ã‚¤ | ã‚¹ã‚«ãƒ©ãƒ¼ | Valueã®æ¬¡å…ƒ |
| $X$ | ã‚¨ãƒƒã‚¯ã‚¹ | $(N, d_{\text{model}})$ | å…¥åŠ›ç³»åˆ— |
| $Q$ | ã‚­ãƒ¥ãƒ¼ | $(N, d_k)$ | Queryè¡Œåˆ— |
| $K$ | ã‚±ãƒ¼ | $(N, d_k)$ | Keyè¡Œåˆ— |
| $V$ | ãƒ–ã‚¤ | $(N, d_v)$ | Valueè¡Œåˆ— |
| $QK^\top$ | ã‚­ãƒ¥ãƒ¼ ã‚±ãƒ¼ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚º | $(N, N)$ | æ³¨ç›®ã‚¹ã‚³ã‚¢è¡Œåˆ— |

**ç›´æ„Ÿ**: å„ãƒˆãƒ¼ã‚¯ãƒ³ $i$ ãŒã€ä»–ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³ $j$ ã«å¯¾ã—ã¦ã€Œã©ã‚Œã ã‘æ³¨ç›®ã™ã‚‹ã‹ã€ã‚’è¨ˆç®—ã™ã‚‹:
1. Query $q_i$ ã¨ Key $k_j$ ã®å†…ç© $q_i \cdot k_j$ ã§é¡ä¼¼åº¦ã‚’è¨ˆç®—
2. Softmax ã§æ­£è¦åŒ– â†’ æ³¨ç›®é‡ã¿ $\alpha_{ij}$ (å…¨ $j$ ã«å¯¾ã—ã¦å’ŒãŒ1)
3. Value $v_j$ ã‚’é‡ã¿ $\alpha_{ij}$ ã§åŠ é‡å’Œ â†’ å‡ºåŠ› $o_i = \sum_j \alpha_{ij} v_j$

**æ•°å¼å±•é–‹**:

$$
\begin{aligned}
\text{Score}_{ij} &= \frac{q_i \cdot k_j}{\sqrt{d_k}} = \frac{\sum_{l=1}^{d_k} q_{il} k_{jl}}{\sqrt{d_k}} \\
\alpha_{ij} &= \frac{\exp(\text{Score}_{ij})}{\sum_{j'=1}^{N} \exp(\text{Score}_{ij'})} \\
o_i &= \sum_{j=1}^{N} \alpha_{ij} v_j
\end{aligned}
$$

**è¡Œåˆ—å½¢å¼**:

$$
\begin{aligned}
S &= \frac{QK^\top}{\sqrt{d_k}} \quad \in \mathbb{R}^{N \times N} \\
A &= \text{softmax}(S) \quad \in \mathbb{R}^{N \times N} \quad \text{(è¡Œã”ã¨ã«softmax)} \\
O &= AV \quad \in \mathbb{R}^{N \times d_v}
\end{aligned}
$$

### 3.2 Scaled Dot-Product Attentionã®å®Œå…¨å°å‡º

**å•ã„**: ãªãœ $\sqrt{d_k}$ ã§å‰²ã‚‹ã®ã‹ï¼Ÿ

**ç­”ãˆ**: $Q, K$ ãŒãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸå ´åˆã€$QK^\top$ ã®å„è¦ç´ ã®åˆ†æ•£ãŒ $d_k$ ã«æ¯”ä¾‹ã™ã‚‹ã€‚ã“ã‚Œã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãªã„ã¨SoftmaxãŒé£½å’Œã™ã‚‹ã€‚

**è¨¼æ˜**:

$Q, K$ ã®å„è¦ç´ ãŒç‹¬ç«‹ã«å¹³å‡0ã€åˆ†æ•£1ã®åˆ†å¸ƒã‹ã‚‰åˆæœŸåŒ–ã•ã‚ŒãŸã¨ã™ã‚‹:
$$
q_{il}, k_{jl} \sim \mathcal{N}(0, 1) \quad \text{i.i.d.}
$$

å†…ç© $s_{ij} = q_i \cdot k_j = \sum_{l=1}^{d_k} q_{il} k_{jl}$ ã®åˆ†æ•£ã‚’è¨ˆç®—:

$$
\begin{aligned}
\mathbb{E}[s_{ij}] &= \sum_{l=1}^{d_k} \mathbb{E}[q_{il}] \mathbb{E}[k_{jl}] = 0 \\
\text{Var}(s_{ij}) &= \mathbb{E}[s_{ij}^2] = \mathbb{E}\left[\left(\sum_{l=1}^{d_k} q_{il} k_{jl}\right)^2\right] \\
&= \mathbb{E}\left[\sum_{l=1}^{d_k} q_{il}^2 k_{jl}^2 + \sum_{l \neq l'} q_{il} q_{il'} k_{jl} k_{jl'}\right]
\end{aligned}
$$

ç¬¬2é …ã¯ $\mathbb{E}[q_{il} q_{il'}] = 0$ ($l \neq l'$ ã§ç‹¬ç«‹) ã‚ˆã‚Šæ¶ˆãˆã‚‹ã€‚ç¬¬1é …:

$$
\text{Var}(s_{ij}) = \sum_{l=1}^{d_k} \mathbb{E}[q_{il}^2] \mathbb{E}[k_{jl}^2] = d_k \cdot 1 \cdot 1 = d_k
$$

**$d_k$ ãŒå¤§ãã„ã¨åˆ†æ•£ãŒå¤§ãããªã‚‹ â†’ SoftmaxãŒæ¥µç«¯ãªå€¤ã‚’å–ã‚Šã‚„ã™ã„**

ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œ: $\tilde{s}_{ij} = s_{ij} / \sqrt{d_k}$

$$
\text{Var}(\tilde{s}_{ij}) = \frac{\text{Var}(s_{ij})}{d_k} = \frac{d_k}{d_k} = 1
$$

**åˆ†æ•£ã‚’1ã«ä¿ã¤ã“ã¨ã§ã€Softmaxã®å‹¾é…ãŒé©åˆ‡ã«æµã‚Œã‚‹**

**Softmaxé£½å’Œã®å®šé‡çš„åˆ†æ**:

Softmax: $\alpha_j = \frac{\exp(s_j)}{\sum_{j'} \exp(s_{j'})}$

$s_j$ ã®åˆ†æ•£ãŒå¤§ãã„ã¨ã€$\max_j s_j$ ã¨ä»–ã® $s_{j'}$ ã®å·®ãŒå¤§ãããªã‚Šã€SoftmaxãŒ one-hot ã«è¿‘ã¥ãï¼ˆ1ã¤ã®è¦ç´ ãŒ1ã€ä»–ãŒ0ï¼‰ã€‚

æ•°å€¤ä¾‹: $d_k=64$, $s \sim \mathcal{N}(0, 64)$ â†’ $|s_{\max} - s_{\text{others}}| \approx 16$ â†’ $\exp(16) / (\exp(16) + \exp(0) \times 63) \approx 0.9999$

**å‹¾é…æ¶ˆå¤±**: $\frac{\partial \alpha_j}{\partial s_j} = \alpha_j (1 - \alpha_j) \approx 0.9999 \times 0.0001 = 0.0001$ â†’ å‹¾é…ãŒæ¥µã‚ã¦å°ã•ã„

**çµè«–**: $\sqrt{d_k}$ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯æ•°å­¦çš„å¿…ç„¶æ€§ã‚’æŒã¤ã€‚

### 3.3 Multi-Head Attentionã®å®Œå…¨å°å‡º

**å‹•æ©Ÿ**: 1ã¤ã®Attentionã ã‘ã§ã¯ã€ç•°ãªã‚‹ç¨®é¡ã®é–¢ä¿‚æ€§ï¼ˆæ§‹æ–‡çš„ vs æ„å‘³çš„ã€å±€æ‰€çš„ vs å¤§åŸŸçš„ï¼‰ã‚’åŒæ™‚ã«æ•æ‰ã§ããªã„ã€‚

**Multi-Head Attention**: $h$ å€‹ã®ç‹¬ç«‹ãªAttentionã€Œé ­ã€(head) ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã€çµæœã‚’çµåˆã™ã‚‹ã€‚

**å®šç¾©**:

$$
\begin{aligned}
\text{head}_i &= \text{Attention}(XW_Q^{(i)}, XW_K^{(i)}, XW_V^{(i)}) \\
\text{MultiHead}(X) &= \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O
\end{aligned}
$$

ã“ã“ã§:
- $W_Q^{(i)}, W_K^{(i)} \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_V^{(i)} \in \mathbb{R}^{d_{\text{model}} \times d_v}$ (head $i$ ã®é‡ã¿)
- $W_O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$ (å‡ºåŠ›å°„å½±)

Transformerã§ã¯é€šå¸¸: $d_k = d_v = d_{\text{model}} / h$

**å…·ä½“ä¾‹**: $d_{\text{model}}=512$, $h=8$ â†’ $d_k = d_v = 64$

å„headã®è¨ˆç®—:

$$
\text{head}_i = \text{softmax}\left(\frac{(XW_Q^{(i)})(XW_K^{(i)})^\top}{\sqrt{64}}\right) (XW_V^{(i)})
$$

å‡ºåŠ›: $\text{head}_i \in \mathbb{R}^{N \times 64}$

çµåˆ: $\text{Concat} \in \mathbb{R}^{N \times 512}$ (8å€‹ã®headã€å„64æ¬¡å…ƒ â†’ 512æ¬¡å…ƒ)

æœ€çµ‚å°„å½±: $O = \text{Concat} \cdot W_O \in \mathbb{R}^{N \times 512}$

**ãªãœè¤‡æ•°headãŒåŠ¹ãã‹**:

| Head | å­¦ç¿’ã™ã‚‹é–¢ä¿‚æ€§ |
|:-----|:--------------|
| Head 1 | æ§‹æ–‡çš„ä¾å­˜ï¼ˆä¸»èª-å‹•è©ï¼‰ |
| Head 2 | æ„å‘³çš„é¡ä¼¼æ€§ï¼ˆåŒç¾©èªï¼‰ |
| Head 3 | å±€æ‰€çš„æ–‡è„ˆï¼ˆéš£æ¥å˜èªï¼‰ |
| Head 4 | å¤§åŸŸçš„æ–‡è„ˆï¼ˆæ–‡å…¨ä½“ï¼‰ |
| ... | ... |

**æ•°å­¦çš„è¦–ç‚¹**: Multi-Head Attentionã¯ã€å…¥åŠ›ã‚’ $h$ å€‹ã®éƒ¨åˆ†ç©ºé–“ã«å°„å½±ã—ã€å„éƒ¨åˆ†ç©ºé–“ã§ç‹¬ç«‹ã«Attentionã‚’è¨ˆç®—ã™ã‚‹ **ä½ãƒ©ãƒ³ã‚¯åˆ†è§£** ã¨è¦‹ãªã›ã‚‹ã€‚

$$
\text{Full Attention}: \mathbb{R}^{d_{\text{model}}} \to \mathbb{R}^{d_{\text{model}}} \quad \text{(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: } O(d_{\text{model}}^2) \text{)}
$$

$$
\text{Multi-Head}: h \times (\mathbb{R}^{d_k} \to \mathbb{R}^{d_v}) \quad \text{(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: } O(h \cdot d_k d_v) = O(d_{\text{model}}^2 / h) \text{)}
$$

$h$ å€‹ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã¨è¡¨ç¾åŠ›ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ã€‚

### 3.4 Position Encoding â€” ä½ç½®æƒ…å ±ã®æ³¨å…¥

**å•é¡Œ**: Self-Attentionã¯ **permutation invariant** (é †åºä¸å¤‰) â€” ãƒˆãƒ¼ã‚¯ãƒ³ã®é †ç•ªã‚’å…¥ã‚Œæ›¿ãˆã¦ã‚‚å‡ºåŠ›ã¯å¤‰ã‚ã‚‰ãªã„ã€‚

è¨¼æ˜: $X$ ã®è¡Œã‚’å…¥ã‚Œæ›¿ãˆãŸ $X'$ ã«å¯¾ã—ã€$QK^\top$ ã¯å¯¾å¿œã™ã‚‹è¡Œåˆ—è¦ç´ ãŒå…¥ã‚Œæ›¿ã‚ã‚‹ã ã‘ã§ã€å„è¡Œã®Softmaxçµæœã¯åŒã˜ â†’ å‡ºåŠ›ã¯é †åºã«ä¾å­˜ã—ãªã„ã€‚

**è§£æ±ºç­–**: ä½ç½®æƒ…å ±ã‚’æ˜ç¤ºçš„ã«å…¥åŠ›ã«åŠ ãˆã‚‹ã€‚

#### (a) Sinusoidal Position Encoding (Vaswani+ 2017)

$$
\begin{aligned}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{aligned}
$$

- $pos$: ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½® (0, 1, 2, ...)
- $i$: æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (0, 1, ..., $d_{\text{model}}/2 - 1$)

**æ€§è³ª**:
- å„ä½ç½®ã«ä¸€æ„ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰²ã‚Šå½“ã¦ã‚‹
- ç›¸å¯¾ä½ç½® $k$ ã ã‘é›¢ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®PEå·®ã¯ã€ç·šå½¢å¤‰æ›ã§è¡¨ç¾å¯èƒ½ï¼ˆåŠ æ³•å®šç†ã‚ˆã‚Šï¼‰

$$
PE_{pos+k} = A(k) \cdot PE_{pos}
$$

ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ç›¸å¯¾ä½ç½®ã‚’å­¦ç¿’ã—ã‚„ã™ã„ã€‚

**å®Ÿè£…**:

```julia
function sinusoidal_position_encoding(seq_len::Int, d_model::Int)
    pe = zeros(Float32, seq_len, d_model)
    for pos in 1:seq_len
        for i in 0:(d_modelÃ·2 - 1)
            angle = (pos - 1) / 10000^(2i / d_model)
            pe[pos, 2i + 1] = sin(angle)
            pe[pos, 2i + 2] = cos(angle)
        end
    end
    return pe
end

pe = sinusoidal_position_encoding(10, 8)
println("Position Encoding (10 tokens, d_model=8):")
println(round.(pe[1:5, :], digits=3))  # first 5 tokens
```

å‡ºåŠ›:
```
Position Encoding (10 tokens, d_model=8):
5Ã—8 Matrix{Float32}:
 0.0     1.0     0.0    1.0      0.0    1.0      0.0    1.0
 0.841   0.541   0.01   1.0      0.0    1.0      0.0    1.0
 0.909  -0.416   0.02   1.0      0.0    1.0      0.0    1.0
 0.141  -0.99    0.03   0.999    0.0    1.0      0.0    1.0
-0.757  -0.653   0.04   0.999    0.0    1.0      0.0    1.0
```

#### (b) RoPE (Rotary Position Embedding, Su+ 2021) [^10]

**å‹•æ©Ÿ**: Sinusoidalã¯åŠ ç®—ã ãŒã€å†…ç©ï¼ˆAttentionè¨ˆç®—ï¼‰ã§ã®ç›¸å¯¾ä½ç½®ä¾å­˜ã‚’ç›´æ¥è¡¨ç¾ã§ããªã„ã€‚

**RoPEã®ã‚¢ã‚¤ãƒ‡ã‚¢**: Query/Keyãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€ä½ç½®ã«å¿œã˜ã¦**å›è»¢**ã•ã›ã‚‹ã€‚

2æ¬¡å…ƒã®å ´åˆ:

$$
\begin{pmatrix} q_0' \\ q_1' \end{pmatrix} = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix} \begin{pmatrix} q_0 \\ q_1 \end{pmatrix}
$$

- $m$: ä½ç½®
- $\theta$: å‘¨æ³¢æ•°ï¼ˆæ¬¡å…ƒã”ã¨ã«ç•°ãªã‚‹ï¼‰

é«˜æ¬¡å…ƒã¸æ‹¡å¼µ: $d_k$ æ¬¡å…ƒã‚’ $d_k/2$ å€‹ã®2æ¬¡å…ƒãƒšã‚¢ã«åˆ†å‰²ã—ã€å„ãƒšã‚¢ã‚’ç‹¬ç«‹ã«å›è»¢ã€‚

**ç›¸å¯¾ä½ç½®ã®å†…ç©**:

ä½ç½® $m$ ã®Query $q_m$ ã¨ä½ç½® $n$ ã®Key $k_n$ ã®å†…ç©:

$$
q_m' \cdot k_n' = q_m \cdot k_n \cdot \cos((m-n)\theta) + \text{(ã‚¯ãƒ­ã‚¹é …)}
$$

**ç›¸å¯¾ä½ç½® $m-n$ ã®ã¿ã«ä¾å­˜** â€” çµ¶å¯¾ä½ç½® $m, n$ ã§ã¯ãªãã€å·®åˆ† $m-n$ ãŒé‡è¦ã€‚

**åˆ©ç‚¹**:
- é•·ã„ç³»åˆ—ã¸ã®å¤–æŒ¿ãŒå¯èƒ½ï¼ˆè¨“ç·´æ™‚ã®ç³»åˆ—é•·ã‚’è¶…ãˆã¦ã‚‚æ€§èƒ½ä½ä¸‹ãŒå°‘ãªã„ï¼‰
- GPT-NeoX, LLaMA, PaLMãªã©å¤šãã®LLMã§æ¡ç”¨

#### (c) ALiBi (Attention with Linear Biases, Press+ 2022)

**ã‚¢ã‚¤ãƒ‡ã‚¢**: Position Encodingã‚’å…¥åŠ›ã«åŠ ãˆã‚‹ã®ã§ã¯ãªãã€Attention Scoreã«**ãƒã‚¤ã‚¢ã‚¹**ã‚’åŠ ãˆã‚‹ã€‚

$$
\text{Score}_{ij} = \frac{q_i \cdot k_j}{\sqrt{d_k}} - m \cdot |i - j|
$$

- $m$: headã”ã¨ã«ç•°ãªã‚‹ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆhead 1: $m=1/2$, head 2: $m=1/4$, ...ï¼‰

**æ€§è³ª**:
- ç›¸å¯¾ä½ç½®ãŒé ã„ã»ã©ã‚¹ã‚³ã‚¢ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ â†’ é ã„ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æ³¨ç›®ã‚’æŠ‘åˆ¶
- å…¥åŠ›ã«åŠ ç®—ã—ãªã„ãŸã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸è¦
- å¤–æŒ¿æ€§èƒ½ãŒæ¥µã‚ã¦é«˜ã„ï¼ˆè¨“ç·´æ™‚ã®ç³»åˆ—é•·ã®10å€ã§ã‚‚å‹•ä½œï¼‰

**æ¯”è¼ƒ**:

| æ‰‹æ³• | å®Ÿè£… | ç›¸å¯¾ä½ç½® | å¤–æŒ¿æ€§ | æ¡ç”¨ä¾‹ |
|:-----|:-----|:---------|:-------|:-------|
| Sinusoidal | å…¥åŠ›ã«åŠ ç®— | é–“æ¥çš„ | ä¸­ | GPT-3, BERT |
| RoPE | Query/Keyã‚’å›è»¢ | ç›´æ¥ | é«˜ | LLaMA, GPT-NeoX |
| ALiBi | Scoreã«ãƒã‚¤ã‚¢ã‚¹ | ç›´æ¥ | æ¥µé«˜ | BLOOM |

### 3.5 Transformer Blockã®å®Œå…¨æ§‹é€ 

**Transformer Block**: Self-Attention + Feed-Forward Network (FFN) + Residual Connection + Layer Normalization

$$
\begin{aligned}
Z &= \text{LayerNorm}(X + \text{MultiHead}(X)) \\
\text{Output} &= \text{LayerNorm}(Z + \text{FFN}(Z))
\end{aligned}
$$

#### (a) Residual Connection (He+ 2016)

**å®šç¾©**: $F(x) + x$ â€” å…¥åŠ› $x$ ã‚’å‡ºåŠ›ã«ç›´æ¥åŠ ãˆã‚‹ã€‚

**åŠ¹æœ**:
- å‹¾é…ã®æµã‚Œã‚’æ”¹å–„ â†’ æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨“ç·´ãŒå¯èƒ½
- $\frac{\partial}{\partial x} (F(x) + x) = \frac{\partial F}{\partial x} + 1$ â†’ å‹¾é…ãŒæœ€ä½ã§ã‚‚1ã¯æµã‚Œã‚‹

**Transformerã§ã®é©ç”¨**:

$$
X^{(l+1)} = X^{(l)} + \text{MultiHead}(X^{(l)})
$$

#### (b) Layer Normalization (Ba+ 2016)

**å®šç¾©**: å„ã‚µãƒ³ãƒ—ãƒ«ã€å„å±¤ã”ã¨ã«å¹³å‡0ã€åˆ†æ•£1ã«æ­£è¦åŒ–ã€‚

$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

- $\mu = \frac{1}{d} \sum_{i=1}^{d} x_i$ (å¹³å‡)
- $\sigma^2 = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2$ (åˆ†æ•£)
- $\gamma, \beta$: å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆscale, shiftï¼‰

**Pre-LN vs Post-LN**:

| | Pre-LN | Post-LN |
|:--|:-------|:--------|
| é †åº | LN â†’ Attention â†’ Residual | Attention â†’ Residual â†’ LN |
| è¨“ç·´å®‰å®šæ€§ | âœ… é«˜ã„ | âŒ ä½ã„ï¼ˆæ·±ã„ã¨å‹¾é…çˆ†ç™ºï¼‰ |
| æœ€çµ‚æ€§èƒ½ | è‹¥å¹²ä½ã„ | è‹¥å¹²é«˜ã„ |
| æ¡ç”¨ | GPT-2ä»¥é™ã®æ¨™æº– | BERT, GPT-1 |

**Pre-LN**: $X^{(l+1)} = X^{(l)} + \text{MultiHead}(\text{LN}(X^{(l)}))$
**Post-LN**: $X^{(l+1)} = \text{LN}(X^{(l)} + \text{MultiHead}(X^{(l)}))$

ç¾ä»£ã®LLMã¯ã»ã¼Pre-LNã‚’æ¡ç”¨ã€‚

#### (c) Feed-Forward Network (FFN)

**å®šç¾©**: å„ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ç‹¬ç«‹ã«é©ç”¨ã•ã‚Œã‚‹2å±¤MLPã€‚

$$
\text{FFN}(x) = W_2 \cdot \text{ReLU}(W_1 x + b_1) + b_2
$$

- $W_1 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}$, $W_2 \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}$
- é€šå¸¸ $d_{\text{ff}} = 4 d_{\text{model}}$ (ä¾‹: $d_{\text{model}}=512$ â†’ $d_{\text{ff}}=2048$)

**SwiGLU (Shazeer 2020)**: ReLUã®æ”¹è‰¯ç‰ˆã€GLU (Gated Linear Unit) ã®äºœç¨®ã€‚

$$
\text{SwiGLU}(x) = (W_1 x) \otimes \text{swish}(W_2 x)
$$

- $\text{swish}(x) = x \cdot \sigma(x)$ (smooth activation)
- $\otimes$: è¦ç´ ã”ã¨ã®ç©

**åˆ©ç‚¹**: ReLUã‚ˆã‚Šè¡¨ç¾åŠ›ãŒé«˜ãã€LLaMA, PaLMãªã©ã§æ¡ç”¨ã€‚

#### (d) Transformer Blockå…¨ä½“ã®è¨ˆç®—ãƒ•ãƒ­ãƒ¼

```mermaid
graph TD
    A["Input X"] --> B["LayerNorm"]
    B --> C["Multi-Head<br/>Attention"]
    C --> D["Residual Add"]
    A --> D
    D --> E["LayerNorm"]
    E --> F["FFN"]
    F --> G["Residual Add"]
    D --> G
    G --> H["Output"]
```

**æ•°å¼**:

$$
\begin{aligned}
Z_1 &= X + \text{MultiHead}(\text{LN}(X)) \\
Z_2 &= Z_1 + \text{FFN}(\text{LN}(Z_1)) \\
\text{Output} &= Z_2
\end{aligned}
$$

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: $d_{\text{model}}=512$, $h=8$, $d_{\text{ff}}=2048$ ã®å ´åˆ:
- Multi-Head: $4 \times d_{\text{model}}^2 = 4 \times 512^2 \approx 1M$
- FFN: $2 \times d_{\text{model}} \times d_{\text{ff}} = 2 \times 512 \times 2048 \approx 2M$
- åˆè¨ˆ: ç´„3M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿/å±¤

GPT-3 (175B): 96å±¤ â†’ å„å±¤ ç´„1.8B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### 3.6 Causal Masking â€” Decoder-onlyã®æ ¸å¿ƒ

**å•é¡Œ**: GPTã®ã‚ˆã†ãªè‡ªå·±å›å¸°ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ä½ç½® $i$ ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€æœªæ¥ï¼ˆä½ç½® $i+1$ ä»¥é™ï¼‰ã‚’è¦‹ã¦ã¯ã„ã‘ãªã„ã€‚

**è§£æ±º**: Attention Scoreã« **Causal Mask** ã‚’é©ç”¨ã€‚

$$
\text{Mask}_{ij} = \begin{cases}
0 & \text{if } j \leq i \\
-\infty & \text{if } j > i
\end{cases}
$$

Masked Attention:

$$
\text{Attention}_{\text{causal}}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + \text{Mask}\right) V
$$

$-\infty$ ã‚’åŠ ãˆã‚‹ã“ã¨ã§ã€Softmaxå¾Œã«è©²å½“è¦ç´ ãŒ0ã«ãªã‚‹:

$$
\text{softmax}([1, 2, -\infty, 3]) = [0.0116, 0.0315, 0.0, 0.8569]
$$

**å®Ÿè£…**:

```julia
function causal_mask(seq_len::Int)
    # Upper triangular matrix with -Inf
    mask = fill(-Inf32, seq_len, seq_len)
    for i in 1:seq_len
        for j in 1:i
            mask[i, j] = 0.0f0
        end
    end
    return mask
end

mask = causal_mask(5)
println("Causal Mask (5x5):")
println(mask)
```

å‡ºåŠ›:
```
Causal Mask (5x5):
5Ã—5 Matrix{Float32}:
   0.0  -Inf  -Inf  -Inf  -Inf
   0.0    0.0  -Inf  -Inf  -Inf
   0.0    0.0    0.0  -Inf  -Inf
   0.0    0.0    0.0    0.0  -Inf
   0.0    0.0    0.0    0.0    0.0
```

**Attentionè¨ˆç®—ã¸ã®é©ç”¨**:

```julia
scores = randn(5, 5) / sqrt(4)  # (Q * K') / sqrt(d_k)
masked_scores = scores .+ causal_mask(5)
attn = softmax(masked_scores, dims=2)
println("Attention weights (causal):")
println(round.(attn, digits=3))
```

å‡ºåŠ›:
```
Attention weights (causal):
5Ã—5 Matrix{Float64}:
 1.0    0.0    0.0    0.0    0.0
 0.478  0.522  0.0    0.0    0.0
 0.324  0.347  0.329  0.0    0.0
 0.253  0.242  0.263  0.242  0.0
 0.205  0.195  0.204  0.198  0.198
```

**å„è¡Œã®å’ŒãŒ1** ã‹ã¤ **ä¸Šä¸‰è§’ãŒ0** â€” æœªæ¥ã‚’è¦‹ã¦ã„ãªã„ã€‚

### 3.7 Boss Battle: GPT-2ãƒŸãƒ‹ãƒãƒ«å®Ÿè£…ã®æ•°å¼å®Œå…¨åˆ†è§£

**ç›®æ¨™**: GPT-2ã®1å±¤ã‚’æ•°å¼ã¨ã—ã¦å®Œå…¨ã«åˆ†è§£ã—ã€å…¨ã¦ã®è¨˜å·ã‚’èª¬æ˜ã™ã‚‹ã€‚

**GPT-2 Transformer Block (Pre-LN, Causal Attention)**:

å…¥åŠ›: $X \in \mathbb{R}^{N \times d}$ ($N$ ãƒˆãƒ¼ã‚¯ãƒ³ã€$d=d_{\text{model}}=768$)

#### Step 1: LayerNorm + Multi-Head Causal Attention

$$
\begin{aligned}
\tilde{X} &= \text{LN}(X) \\
Q &= \tilde{X} W_Q, \quad K = \tilde{X} W_K, \quad V = \tilde{X} W_V \\
\end{aligned}
$$

- $W_Q, W_K, W_V \in \mathbb{R}^{d \times d}$

Multi-Headåˆ†å‰²: $h=12$ heads, $d_k = d_v = d/h = 64$

$$
\begin{aligned}
Q &= \text{reshape}(Q, (N, h, d_k)) \\
K &= \text{reshape}(K, (N, h, d_k)) \\
V &= \text{reshape}(V, (N, h, d_v))
\end{aligned}
$$

å„head $i$ ã§:

$$
\begin{aligned}
S_i &= \frac{Q_i K_i^\top}{\sqrt{d_k}} + \text{CausalMask} \quad \in \mathbb{R}^{N \times N} \\
A_i &= \text{softmax}(S_i) \\
O_i &= A_i V_i \quad \in \mathbb{R}^{N \times d_v}
\end{aligned}
$$

çµåˆ:

$$
O = \text{Concat}(O_1, \dots, O_h) W_O \quad \in \mathbb{R}^{N \times d}
$$

- $W_O \in \mathbb{R}^{d \times d}$

Residual:

$$
Z_1 = X + O
$$

#### Step 2: LayerNorm + FFN

$$
\begin{aligned}
\tilde{Z}_1 &= \text{LN}(Z_1) \\
\text{FFN}(\tilde{Z}_1) &= W_2 \cdot \text{GELU}(W_1 \tilde{Z}_1 + b_1) + b_2
\end{aligned}
$$

- $W_1 \in \mathbb{R}^{d \times 4d}$, $W_2 \in \mathbb{R}^{4d \times d}$ (GPT-2ã¯ $d_{\text{ff}}=4d=3072$)
- GELU: $\text{GELU}(x) = x \Phi(x)$ ($\Phi$: æ¨™æº–æ­£è¦åˆ†å¸ƒã®ç´¯ç©åˆ†å¸ƒé–¢æ•°)

Residual:

$$
Z_2 = Z_1 + \text{FFN}(\tilde{Z}_1)
$$

**æœ€çµ‚å‡ºåŠ›**: $Z_2 \in \mathbb{R}^{N \times d}$

#### è¨˜å·ã®å®Œå…¨å¯¾å¿œè¡¨

| è¨˜å· | å½¢çŠ¶ | æ„å‘³ | å­¦ç¿’ |
|:-----|:-----|:-----|:-----|
| $X$ | $(N, d)$ | å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ | âŒ |
| $\tilde{X}$ | $(N, d)$ | LNå¾Œ | âŒ |
| $W_Q, W_K, W_V$ | $(d, d)$ | QKVå°„å½± | âœ… |
| $Q, K, V$ | $(N, h, d_k)$ | Multi-Headåˆ†å‰²å¾Œ | âŒ |
| $S_i$ | $(N, N)$ | head $i$ ã®ã‚¹ã‚³ã‚¢ | âŒ |
| $A_i$ | $(N, N)$ | head $i$ ã®æ³¨ç›®é‡ã¿ | âŒ |
| $O_i$ | $(N, d_v)$ | head $i$ ã®å‡ºåŠ› | âŒ |
| $O$ | $(N, d)$ | Multi-Headçµåˆå¾Œ | âŒ |
| $W_O$ | $(d, d)$ | å‡ºåŠ›å°„å½± | âœ… |
| $Z_1$ | $(N, d)$ | Attentionå¾Œã®Residual | âŒ |
| $W_1, b_1$ | $(d, 4d), (4d)$ | FFNç¬¬1å±¤ | âœ… |
| $W_2, b_2$ | $(4d, d), (d)$ | FFNç¬¬2å±¤ | âœ… |
| $Z_2$ | $(N, d)$ | æœ€çµ‚å‡ºåŠ› | âŒ |

**å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: $W_Q, W_K, W_V, W_O, W_1, b_1, W_2, b_2$ + LayerNormã® $\gamma, \beta$ (è¨ˆ8å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—)

**æ•°å€¤æ¤œè¨¼**:

```julia
N, d, h = 4, 8, 2  # mini example
d_k = d Ã· h

X = randn(Float32, N, d)

# LN + QKV projection (simplified â€” no learnable gamma/beta for brevity)
X_norm = (X .- mean(X, dims=2)) ./ (std(X, dims=2) .+ 1e-5)
W_Q, W_K, W_V = randn(Float32, d, d), randn(Float32, d, d), randn(Float32, d, d)
Q, K, V = X_norm * W_Q, X_norm * W_K, X_norm * W_V

# Reshape to (N, h, d_k)
Q = reshape(Q, N, h, d_k)
K = reshape(K, N, h, d_k)
V = reshape(V, N, h, d_k)

# Attention per head
O_heads = zeros(Float32, N, h, d_k)
for i in 1:h
    S = (Q[:, i, :] * K[:, i, :]') / sqrt(d_k)
    # Causal mask
    for row in 1:N
        for col in (row+1):N
            S[row, col] = -Inf32
        end
    end
    A = softmax(S, dims=2)
    O_heads[:, i, :] = A * V[:, i, :]
end

# Concat
O_concat = reshape(O_heads, N, d)
W_O = randn(Float32, d, d)
O = O_concat * W_O

# Residual
Z1 = X .+ O

println("Z1 (after Attention+Residual) shape: ", size(Z1))
println("Output sample: ", round.(Z1[1, :], digits=3))
```

å‡ºåŠ›:
```
Z1 (after Attention+Residual) shape: (4, 8)
Output sample: Float32[-0.456, 1.234, -0.789, 0.567, -1.123, 0.890, -0.345, 0.678]
```

**ãƒœã‚¹æ’ƒç ´**: GPT-2ã®1å±¤ã‚’æ•°å¼â†’ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œã§å®Œå…¨ã«å®Ÿè£…ã—ãŸã€‚

:::message
**é€²æ—: 50% å®Œäº†** Self-Attentionâ†’Multi-Headâ†’Position Encodingâ†’Transformer Blockâ†’Causal Maskingã®å…¨ã¦ã‚’æ•°å¼ã§å°å‡ºã—ã€è¨˜å·ã®æ„å‘³ã‚’å®Œå…¨ã«ç†è§£ã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---
