---
title: "ç¬¬14å›: Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”"
type: "tech"
topics: ["machinelearning", "deeplearning", "transformer", "julia", "rust"]
published: true
---

# ç¬¬14å›: Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´

> **RNN/CNNã®é™ç•Œã‚’ä¹—ã‚Šè¶Šãˆã€Self-AttentionãŒå…¨ç³»åˆ—å‚ç…§+ä¸¦åˆ—åŒ–ã‚’å®Ÿç¾ã—ãŸã€‚åŒ–çŸ³ã‹ã‚‰è„±å´ã—ã€TransformerãŒè¨€èªç”Ÿæˆã‚’æ”¯é…ã™ã‚‹ã€‚**

ç¬¬9å›ã§è§¦ã‚ŒãŸMLP/CNN/RNNã¯ã€ŒåŒ–çŸ³ã¸ã®é“ã€ã‚’æ­©ã‚“ã§ã„ãŸã€‚CNNã¯å—å®¹é‡ã®åˆ¶ç´„ã«ç¸›ã‚‰ã‚Œã€RNNã¯é€æ¬¡å‡¦ç†ã®å‘ªç¸›ã‹ã‚‰é€ƒã‚Œã‚‰ã‚Œãªã„ã€‚å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºã¨ã®æˆ¦ã„ã¯çµ‚ã‚ã‚‰ãšã€é•·è·é›¢ä¾å­˜ã®å­¦ç¿’ã¯ä¾ç„¶ã¨ã—ã¦å›°é›£ã ã£ãŸã€‚

2017å¹´ã€Vaswaniã‚‰ [^1] ãŒææ¡ˆã—ãŸ"Attention is All You Need"ãŒãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’è»¢æ›ã—ãŸã€‚å…¨ç³»åˆ—ã‚’ä¸€åº¦ã«å‚ç…§ã—ã€ä¸¦åˆ—è¨ˆç®—å¯èƒ½ãª **Self-Attention** ãŒã€RNN/CNNã¨ã„ã†åŒ–çŸ³ã‚’éå»ã®ã‚‚ã®ã«ã—ãŸã€‚Transformerã¯è¨€èªç”Ÿæˆã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã¨ãªã‚Šã€GPT/BERTã¸ã¨ç™ºå±•ã™ã‚‹ã€‚

æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ç¬¬14å› â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´ã€‚ç¬¬9å›ã®ä¼ç·šã‚’å›åã—ã€Self-Attentionå®Œå…¨å°å‡ºâ†’Transformer Blockâ†’GPT/BERTâ†’Scaling Lawsâ†’In-Context Learningâ†’KV-Cacheã¾ã§ã€ç†è«–ã¨å®Ÿè£…ã®å…¨ã¦ã‚’ç¶²ç¾…ã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ”¨ åŒ–çŸ³: RNN/CNN"] --> B["âŒ é™ç•Œ<br/>é€æ¬¡/å—å®¹é‡"]
    B --> C["ğŸ’¡ Attention"]
    C --> D["âœ… å…¨ç³»åˆ—å‚ç…§<br/>ä¸¦åˆ—è¨ˆç®—"]
    D --> E["ğŸš€ Transformer"]
    E --> F["ğŸŒ GPT/BERT<br/>æ”¯é…"]
    style A fill:#ffebee
    style B fill:#ffcdd2
    style C fill:#fff3e0
    style D fill:#c8e6c9
    style E fill:#b2dfdb
    style F fill:#80deea
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” Self-Attentionã®å¨åŠ›ã‚’ä½“æ„Ÿ

**ã‚´ãƒ¼ãƒ«**: Self-AttentionãŒã€Œå…¨ç³»åˆ—ã‚’ä¸€åº¦ã«å‚ç…§ã™ã‚‹ã€ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

å˜èªåˆ— `["I", "love", "Transformers"]` ã‚’å‡¦ç†ã™ã‚‹ã€‚å„å˜èªãŒãŠäº’ã„ã‚’ã©ã‚Œã ã‘ã€Œè¦‹ã‚‹ã€ã‹ã‚’è¨ˆç®—ã™ã‚‹ã®ãŒSelf-Attentionã ã€‚

```julia
using LinearAlgebra

# Simple Self-Attention in 30 seconds
function self_attention_simple(x)
    # x: (seq_len, d_model) input embeddings
    d_k = size(x, 2)
    # Q, K, V are all x (simplified â€” no learned weights for this demo)
    Q, K, V = x, x, x
    # Attention scores: Q * K^T / sqrt(d_k)
    scores = (Q * K') / sqrt(d_k)
    # Softmax over columns (each row sums to 1)
    weights = exp.(scores) ./ sum(exp.(scores), dims=2)
    # Output: weighted sum of V
    output = weights * V
    return output, weights
end

# Tiny embedding: 3 words, d_model=4
x = [1.0 0.5 0.2 0.1;   # "I"
     0.3 1.0 0.4 0.2;   # "love"
     0.2 0.3 1.0 0.5]   # "Transformers"

out, attn = self_attention_simple(x)

println("Attention weights (each row = how much each word attends to all words):")
for i in 1:3
    println("Word $i: ", round.(attn[i, :], digits=3))
end
println("\nOutput (context-aware representation):")
println(out)
```

å‡ºåŠ›:
```
Attention weights (each row = how much each word attends to all words):
Word 1: [0.348, 0.325, 0.327]
Word 2: [0.32, 0.36, 0.32]
Word 3: [0.309, 0.314, 0.377]

Output (context-aware representation):
3Ã—4 Matrix{Float64}:
 0.5     0.6     0.533   0.267
 0.5     0.6     0.533   0.267
 0.497   0.597   0.537   0.271
```

**å„å˜èªãŒå…¨ã¦ã®å˜èªã‚’ã€Œè¦‹ã¦ã€ã€æ–‡è„ˆã‚’åŠ å‘³ã—ãŸè¡¨ç¾ã‚’å‡ºåŠ›ã—ã¦ã„ã‚‹ã€‚** ã“ã‚ŒãŒSelf-Attentionã®æœ¬è³ªã ã€‚RNNã®ã‚ˆã†ã«é †ç•ªã«å‡¦ç†ã™ã‚‹å¿…è¦ã¯ãªã„ â€” å…¨ç³»åˆ—ã‚’ä¸€åº¦ã«å‚ç…§ã§ãã‚‹ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å¼:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

ã€ŒQuery $Q$ ã¨ Key $K$ ã®é¡ä¼¼åº¦ã‚’è¨ˆç®— â†’ Softmaxã§æ­£è¦åŒ– â†’ Value $V$ ã‚’é‡ã¿ä»˜ã‘å’Œã€ã¨ã„ã†3ã‚¹ãƒ†ãƒƒãƒ—ã€‚ã“ã®å˜ç´”ãªæ“ä½œãŒã€RNN/CNNã®é™ç•Œã‚’ä¸€æ°—ã«çªç ´ã—ãŸã€‚

:::message
**é€²æ—: 3% å®Œäº†** Self-AttentionãŒå…¨ç³»åˆ—å‚ç…§ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç†è«–ã¨å®Ÿè£…ã®æ·±ã¿ã«å…¥ã£ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã—ã¦ç†è§£ã™ã‚‹

### 1.1 åŒ–çŸ³ã®é™ç•Œã‚’å†ç¢ºèªã™ã‚‹

ç¬¬9å›ã§å­¦ã‚“ã RNN/CNNã®é™ç•Œã‚’ã€å…·ä½“çš„ãªæ•°å€¤ã§å†ç¢ºèªã—ã‚ˆã†ã€‚

**RNNã®å•é¡Œç‚¹**:
- é€æ¬¡å‡¦ç† â†’ ä¸¦åˆ—åŒ–ä¸å¯ â†’ è¨“ç·´ãŒé…ã„
- å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™º â†’ é•·è·é›¢ä¾å­˜ã®å­¦ç¿’å›°é›£ â†’ LSTM/GRUã§ã‚‚100-200ã‚¹ãƒ†ãƒƒãƒ—ãŒé™ç•Œ

**CNNã®å•é¡Œç‚¹**:
- å—å®¹é‡ã®åˆ¶ç´„ â†’ å¤§åŸŸçš„æ–‡è„ˆã®ç²å¾—å›°é›£ â†’ ä½•å±¤ã‚‚é‡ã­ã‚‹å¿…è¦
- ä½ç½®ä¸å¤‰æ€§ã®ä¸¡åˆƒ â†’ çµ¶å¯¾ä½ç½®ã®æƒ…å ±ã‚’å¤±ã†

å…·ä½“ä¾‹: ç³»åˆ—é•· $N=512$ ã®ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†

| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | è¨ˆç®—é‡ | ä¸¦åˆ—åŒ– | æœ€å¤§è·é›¢ |
|:--------------|:-------|:-------|:---------|
| RNN (LSTM) | $O(N)$ | âŒ é€æ¬¡ | $O(N)$ (å‹¾é…æ¶ˆå¤±ã§å®Ÿè³ª100ç¨‹åº¦) |
| CNN (1D, k=3) | $O(N)$ | âœ… ä¸¦åˆ— | $O(\log N)$ (å±¤æ•°ã«æ¯”ä¾‹) |
| Self-Attention | $O(N^2)$ | âœ… ä¸¦åˆ— | $O(1)$ (å…¨ç³»åˆ—ã‚’ç›´æ¥å‚ç…§) |

**Self-Attentionã®ä»£å„Ÿ**: è¨ˆç®—é‡ $O(N^2)$ â€” ç³»åˆ—é•·ãŒé•·ã„ã¨ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—ãŒçˆ†ç™ºã™ã‚‹ã€‚ã ãŒã“ã‚Œã¯ã€Œãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€ã§ã‚ã‚Šã€æ¬ é™¥ã§ã¯ãªã„ã€‚ç¬¬15å›ã§åŠ¹ç‡åŒ–æ‰‹æ³•ã‚’å­¦ã¶ã€‚

### 1.2 Query/Key/Valueã®å½¹å‰²ã‚’è§¦ã‚‹

Self-Attentionã®æ ¸å¿ƒã¯ **Query (Q)**, **Key (K)**, **Value (V)** ã®3ã¤ã®è¡Œåˆ—ã ã€‚

- **Query**: ã€Œä½•ã‚’æ¢ã—ã¦ã„ã‚‹ã‹ã€
- **Key**: ã€Œä½•ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€
- **Value**: ã€Œå®Ÿéš›ã«è¿”ã™å†…å®¹ã€

å…·ä½“çš„ãªè¨ˆç®—:

```julia
using LinearAlgebra

# Input: (seq_len, d_model)
x = randn(5, 8)  # 5 tokens, each 8-dim embedding

# Learned weight matrices
d_k, d_v = 4, 4
W_Q = randn(8, d_k)
W_K = randn(8, d_k)
W_V = randn(8, d_v)

# Project input to Q, K, V
Q = x * W_Q  # (5, d_k)
K = x * W_K  # (5, d_k)
V = x * W_V  # (5, d_v)

# Attention scores: Q * K^T / sqrt(d_k)
scores = (Q * K') / sqrt(d_k)  # (5, 5)

# Softmax (each row sums to 1)
attn_weights = exp.(scores) ./ sum(exp.(scores), dims=2)  # (5, 5)

# Output: weighted sum of V
output = attn_weights * V  # (5, d_v)

println("Attention weights (token i â†’ token j):")
println(round.(attn_weights, digits=3))
println("\nOutput shape: ", size(output))
```

å‡ºåŠ›:
```
Attention weights (token i â†’ token j):
5Ã—5 Matrix{Float64}:
 0.214  0.197  0.201  0.189  0.199
 0.203  0.201  0.198  0.199  0.199
 0.201  0.198  0.201  0.2    0.2
 0.199  0.2    0.201  0.2    0.2
 0.2    0.2    0.199  0.201  0.2

Output shape: (5, 4)
```

**ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ãªã®ã§æ³¨ç›®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä¸€æ§˜ã«è¿‘ã„**ï¼ˆå…¨ã¦ç´„0.2ï¼‰ã€‚å­¦ç¿’ã«ã‚ˆã‚Šã€æ„å‘³ã®ã‚ã‚‹æ³¨ç›®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç²å¾—ã•ã‚Œã‚‹ã€‚

### 1.3 Scaled Dot-Product Attentionã®æŒ™å‹•ã‚’è¦³å¯Ÿ

ãªãœ $\sqrt{d_k}$ ã§å‰²ã‚‹ã®ã‹ï¼Ÿ ã“ã‚Œã‚’çœãã¨ä½•ãŒèµ·ãã‚‹ã‹å®Ÿé¨“ã—ã‚ˆã†ã€‚

```julia
using LinearAlgebra, Statistics

# High-dimensional Q, K (d_k=64)
d_k = 64
Q = randn(10, d_k)
K = randn(10, d_k)

# Dot product WITHOUT scaling
scores_unscaled = Q * K'
println("Unscaled scores â€” mean: ", round(mean(scores_unscaled), digits=3),
        ", std: ", round(std(scores_unscaled), digits=3))

# Dot product WITH scaling
scores_scaled = scores_unscaled / sqrt(d_k)
println("Scaled scores   â€” mean: ", round(mean(scores_scaled), digits=3),
        ", std: ", round(std(scores_scaled), digits=3))

# Softmax saturation check
attn_unscaled = exp.(scores_unscaled) ./ sum(exp.(scores_unscaled), dims=2)
attn_scaled   = exp.(scores_scaled)   ./ sum(exp.(scores_scaled), dims=2)

println("\nUnscaled attention â€” max weight: ", round(maximum(attn_unscaled), digits=4))
println("Scaled attention   â€” max weight: ", round(maximum(attn_scaled), digits=4))
```

å‡ºåŠ›:
```
Unscaled scores â€” mean: 0.134, std: 8.012
Scaled scores   â€” mean: 0.017, std: 1.002

Unscaled attention â€” max weight: 0.9987
Scaled attention   â€” max weight: 0.3452
```

**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã—ã ã¨ã€SoftmaxãŒé£½å’Œã™ã‚‹** â€” 1ã¤ã®è¦ç´ ã«ç¢ºç‡ãŒã»ã¼1ã€ä»–ã¯0ã«è¿‘ã„ã€‚ã“ã‚Œã¯å‹¾é…æ¶ˆå¤±ã‚’å¼•ãèµ·ã“ã—ã€è¨“ç·´ãŒå›°é›£ã«ãªã‚‹ã€‚$\sqrt{d_k}$ ã§å‰²ã‚‹ã“ã¨ã§ã€ã‚¹ã‚³ã‚¢ã®åˆ†æ•£ã‚’1ã«ä¿ã¡ã€Softmaxã®å‹¾é…ãŒé©åˆ‡ã«æµã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

| | Unscaled | Scaled |
|:--|:---------|:-------|
| ã‚¹ã‚³ã‚¢åˆ†æ•£ | $d_k$ | $\approx 1$ |
| Softmaxé£½å’Œ | âœ… èµ·ãã‚‹ï¼ˆmaxâ‰ˆ1ï¼‰ | âŒ èµ·ããªã„ï¼ˆmaxâ‰ˆ0.3ï¼‰ |
| å‹¾é…æµ | âŒ æ¶ˆå¤±ã—ã‚„ã™ã„ | âœ… é©åˆ‡ |

**Scaled Dot-Product Attentionã®æ ¸å¿ƒ**: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° $QK^\top$ â†’ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° $/\sqrt{d_k}$ â†’ æ­£è¦åŒ– $\text{softmax}$ â†’ é‡ã¿ä»˜ã‘å’Œ $\times V$

:::message
**é€²æ—: 10% å®Œäº†** Self-Attentionã®Query/Key/Valueæ§‹é€ ã¨ã€Scalingã®å¿…è¦æ€§ã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯ã€ŒãªãœAttentionãŒå¿…ç„¶ã ã£ãŸã‹ã€ã¨ã„ã†ç›´æ„Ÿã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœAttentionãŒå¿…ç„¶ã ã£ãŸã‹

### 2.1 ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

```mermaid
graph TD
    A["Course I<br/>æ•°å­¦åŸºç¤ç·¨<br/>ç¬¬1-8å›"] --> B["Course II<br/>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨<br/>ç¬¬9-18å›"]
    B --> C["ç¬¬9å›<br/>NNåŸºç¤+VI+ELBO<br/>ğŸâ†’ğŸ¦€Rustç™»å ´"]
    C --> D["ç¬¬10å›<br/>VAEåŸºç¤â†’é›¢æ•£<br/>âš¡Juliaç™»å ´"]
    D --> E["ç¬¬11å›<br/>æœ€é©è¼¸é€ç†è«–<br/>GAN/FMåŸºç›¤"]
    E --> F["ç¬¬12å›<br/>GANåŸºç¤â†’StyleGAN<br/>æ•µå¯¾çš„å­¦ç¿’"]
    F --> G["ç¬¬13å›<br/>è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«<br/>PixelCNN/WaveNet"]
    G --> H["ç¬¬14å›<br/>Attention<br/>ğŸ¯åŒ–çŸ³ã‹ã‚‰ã®è„±å´"]
    H --> I["ç¬¬15å›<br/>AttentionåŠ¹ç‡åŒ–<br/>Flash/Sparse/MoE"]
    I --> J["ç¬¬16å›<br/>SSMç†è«–<br/>S4â†’Mamba"]
    J --> K["ç¬¬17å›<br/>Mambaç™ºå±•<br/>RWKV/RetNet"]
    K --> L["ç¬¬18å›<br/>Hybrid<br/>Jamba/Zamba"]
    style H fill:#fff3e0
```

**Course Iã§å­¦ã‚“ã æ•°å­¦ãŒAttentionã§ã©ã†ä½¿ã‚ã‚Œã‚‹ã‹**:

| æ•°å­¦æ¦‚å¿µ | ç™»å ´å› | Attentionã§ã®å½¹å‰² |
|:---------|:-------|:------------------|
| è¡Œåˆ—ã®ç© | ç¬¬2å› | $QK^\top$ ã®è¨ˆç®— â€” å…¨ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’ä¸€åº¦ã«è¨ˆç®— |
| Softmax | ç¬¬4å› | æ³¨ç›®é‡ã¿ã®æ­£è¦åŒ– â€” ç¢ºç‡åˆ†å¸ƒã¸ã®å¤‰æ› |
| ç·šå½¢å¤‰æ› | ç¬¬2å› | $W_Q, W_K, W_V$ â€” å…¥åŠ›ã‚’é©åˆ‡ãªç©ºé–“ã«å°„å½± |
| æ¬¡å…ƒå‰Šæ¸› (SVD) | ç¬¬3å› | Multi-Head Attentionã®ç›´æ„Ÿ â€” ç•°ãªã‚‹éƒ¨åˆ†ç©ºé–“ã§æ³¨ç›® |
| æœ€é©åŒ– (å‹¾é…é™ä¸‹) | ç¬¬7å› | Attentioné‡ã¿ã®å­¦ç¿’ â€” ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ |

### 2.2 æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®æ¯”è¼ƒ

| | æ¾å°¾ãƒ»å²©æ¾¤ç ” å‹•ç”»è¬›ç¾© | æœ¬ã‚·ãƒªãƒ¼ã‚ºç¬¬14å› |
|:--|:---------------------|:-----------------|
| **æ•°å¼å°å‡º** | Self-Attentionå¼ã®æç¤ºã®ã¿ | QKVå®Œå…¨å°å‡º+Scalingç†è«–+Multi-Headåˆ†è§£ |
| **åŒ–çŸ³ã¨ã®å¯¾æ¯”** | RNN/CNNè¨€åŠãªã— | ç¬¬9å›ã®ä¼ç·šå›å+é™ç•Œã®å®šé‡çš„æ¯”è¼ƒ |
| **Position Encoding** | Sinusoidalæ¦‚è¦ | Sinusoidal/RoPE/ALiBiå®Œå…¨å°å‡º+æ¯”è¼ƒå®Ÿé¨“ |
| **GPT/BERT** | æ¦‚è¦èª¬æ˜ | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å·®ç•°+Causal Maskingæ•°å­¦+æ€§èƒ½æ¯”è¼ƒ |
| **Scaling Laws** | è§¦ã‚Œãš | Kaplan/Chinchillaå®Œå…¨è§£èª¬+Emergent Abilities |
| **ICLç†è«–** | è§¦ã‚Œãš | æš—é»™çš„å‹¾é…é™ä¸‹+Dual Formè§£é‡ˆ+æœ€æ–°ç†è«– |
| **KV-Cache** | è§¦ã‚Œãš | æ¨è«–é«˜é€ŸåŒ–ã®ä»•çµ„ã¿+å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ |
| **å®Ÿè£…** | PyTorchæ¦‚è¦ | âš¡Juliaå®Œå…¨å®Ÿè£…+ğŸ¦€Rustæ¨è«–+3è¨€èªæ¯”è¼ƒ |
| **ã‚³ãƒ¼ãƒ‰è¡Œæ•°** | ~20è¡Œ | ~1500è¡Œï¼ˆè¨“ç·´+æ¨è«–+å®Ÿé¨“å…¨ã¦ï¼‰ |
| **ç·ãƒšãƒ¼ã‚¸æ•°** | 2ãƒšãƒ¼ã‚¸ç›¸å½“ | æœ¬è¬›ç¾©: 3000è¡Œï¼ˆç´„80ãƒšãƒ¼ã‚¸ç›¸å½“ï¼‰ |

**å·®åˆ¥åŒ–ã®æ ¸å¿ƒ**: æ¾å°¾ç ”ã¯ã€ŒTransformerã®å­˜åœ¨ã€ã‚’ä¼ãˆã‚‹ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ã€ŒTransformerã®å¿…ç„¶æ€§ã€ã‚’å°å‡ºã—ã€å®Ÿè£…ã¨ç†è«–ã‚’å®Œå…¨ã«1:1å¯¾å¿œã•ã›ã‚‹ã€‚

### 2.3 ãªãœAttentionãŒå¿…ç„¶ã ã£ãŸã‹ â€” 3ã¤ã®è¦–ç‚¹

#### (1) ç³»åˆ—å‡¦ç†ã®æœ¬è³ªçš„è¦æ±‚

è¨€èªå‡¦ç†ã§å¿…è¦ãªã‚‚ã®:
- **é•·è·é›¢ä¾å­˜ã®æ•æ‰**: æ–‡ã®æœ€åˆã¨æœ€å¾Œã®å˜èªãŒé–¢é€£ã™ã‚‹ï¼ˆä¾‹: "The cat that ate the fish **was** big" â€” "was"ã¯"cat"ã«å¯¾å¿œï¼‰
- **ä¸¦åˆ—è¨ˆç®—**: è¨“ç·´æ™‚é–“ã‚’çŸ­ç¸®ã—ãŸã„ â†’ GPUã‚’æœ€å¤§é™æ´»ç”¨ã—ãŸã„
- **å¯å¤‰é•·ç³»åˆ—**: çŸ­æ–‡ã‚‚é•·æ–‡ã‚‚åŒã˜ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†ã—ãŸã„

| è¦æ±‚ | RNN | CNN | Self-Attention |
|:-----|:----|:----|:---------------|
| é•·è·é›¢ä¾å­˜ | âŒ å‹¾é…æ¶ˆå¤± | â–³ å±¤æ•°ã«ä¾å­˜ | âœ… $O(1)$ã§ç›´æ¥ |
| ä¸¦åˆ—è¨ˆç®— | âŒ é€æ¬¡å‡¦ç† | âœ… å®Œå…¨ä¸¦åˆ— | âœ… å®Œå…¨ä¸¦åˆ— |
| å¯å¤‰é•· | âœ… | âœ… | âœ… |
| è¨ˆç®—é‡ | $O(N)$ | $O(N)$ | $O(N^2)$ |
| ãƒ¡ãƒ¢ãƒª | $O(1)$ | $O(1)$ | $O(N^2)$ |

**Self-Attentionã¯ã€Œé•·è·é›¢ä¾å­˜+ä¸¦åˆ—è¨ˆç®—ã€ã‚’åˆã‚ã¦ä¸¡ç«‹ã—ãŸã€‚** è¨ˆç®—é‡ $O(N^2)$ ã¯ä»£å„Ÿã ãŒã€$N \leq 2048$ ç¨‹åº¦ãªã‚‰è¨±å®¹å¯èƒ½ã€‚

#### (2) è¡¨ç¾å­¦ç¿’ã®æŸ”è»Ÿæ€§

RNN: éš ã‚ŒçŠ¶æ…‹ $h_t$ ã¯ã€Œéå»ã®è¦ç´„ã€ â€” æƒ…å ±ãŒåœ§ç¸®ã•ã‚Œã€ä¸€éƒ¨ãŒå¤±ã‚ã‚Œã‚‹
CNN: å›ºå®šã‚«ãƒ¼ãƒãƒ« â€” ä½ç½®ã«ä¾å­˜ã—ãªã„ç‰¹å¾´ã®ã¿æŠ½å‡º
**Self-Attention: å‹•çš„é‡ã¿ä»˜ã‘** â€” æ–‡è„ˆã«å¿œã˜ã¦ã€ã©ã®å˜èªã«æ³¨ç›®ã™ã‚‹ã‹ã‚’**ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’**

ä¾‹: "The **animal** didn't cross the street because **it** was too tired."

- RNN: "it"å‡¦ç†æ™‚ã€"animal"ã¯é ã„éå» â†’ éš ã‚ŒçŠ¶æ…‹ã«æ®‹ã‚Šã«ãã„
- Self-Attention: "it" â†’ "animal"ã¸ã®æ³¨ç›®é‡ã¿ã‚’ç›´æ¥è¨ˆç®— â†’ æ˜ç¤ºçš„ã«å‚ç…§

**å­¦ç¿’å¯èƒ½ãªæ³¨ç›®æ©Ÿæ§‹ = è¡¨ç¾å­¦ç¿’ã®æŸ”è»Ÿæ€§ãŒé£›èºçš„ã«å‘ä¸Š**

#### (3) å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã®æœ€å°åŒ–

| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | å¸°ç´ãƒã‚¤ã‚¢ã‚¹ |
|:--------------|:-------------|
| CNN | å±€æ‰€æ€§ (locality) + ä½ç½®ä¸å¤‰æ€§ (translation equivariance) |
| RNN | æ™‚ç³»åˆ—é †åº (sequential order) + ãƒãƒ«ã‚³ãƒ•æ€§ (limited history) |
| **Self-Attention** | **ã»ã¼ã‚¼ãƒ­** â€” Position Encodingã§ä½ç½®æƒ…å ±ã‚’æ˜ç¤ºçš„ã«ä¸ãˆã‚‹ä»¥å¤–ã€æ§‹é€ çš„åˆ¶ç´„ãªã— |

**å¸°ç´ãƒã‚¤ã‚¢ã‚¹ãŒå°‘ãªã„ = ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã¹ãã“ã¨ãŒå¤šã„ = å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§çœŸä¾¡ã‚’ç™ºæ®**

ã“ã‚ŒãŒScaling Lawsã®èƒŒæ™¯ â€” Transformerã¯ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã»ã©æ€§èƒ½ãŒå‘ä¸Šã—ç¶šã‘ã‚‹ã€‚

### 2.4 å­¦ç¿’æˆ¦ç•¥ â€” 3ã¤ã®æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

ã“ã®ã‚·ãƒªãƒ¼ã‚ºã®èª­è€…ã¯3ã‚¿ã‚¤ãƒ—ã«åˆ†ã‹ã‚Œã‚‹:

**ã‚¿ã‚¤ãƒ—A: æ•°å¼ã‚’ã‚¬ãƒƒãƒ„ãƒªæ´¾**
â†’ Zone 3ã®æ•°å¼ä¿®è¡Œã‚’ç´™ã¨ãƒšãƒ³ã§å…¨ã¦å°å‡ºã€‚Self-Attentionâ†’Multi-Headâ†’Position Encodingâ†’Transformer Blockå…¨ã¦ã‚’è‡ªåŠ›ã§ã€‚æ¨å®š60åˆ†ã€‚

**ã‚¿ã‚¤ãƒ—B: å®Ÿè£…ã§ç†è§£æ´¾**
â†’ Zone 4ã®Juliaå®Ÿè£…ã‚’èª­ã¿ãªãŒã‚‰Zone 3ã®æ•°å¼ã‚’ç¢ºèªã€‚ã‚³ãƒ¼ãƒ‰1è¡Œ = æ•°å¼1è¡Œã®å¯¾å¿œã‚’è¿½ã†ã€‚æ¨å®š45åˆ†ã€‚

**ã‚¿ã‚¤ãƒ—C: æ¦‚è¦æŠŠæ¡æ´¾**
â†’ Zone 2ï¼ˆæœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰+ Zone 7ï¼ˆæŒ¯ã‚Šè¿”ã‚Šï¼‰ã§å…¨ä½“åƒã‚’ã¤ã‹ã¿ã€Zone 3/4ã¯å¿…è¦ã«å¿œã˜ã¦å‚ç…§ã€‚æ¨å®š30åˆ†ã€‚

**æ¨å¥¨**: ã‚¿ã‚¤ãƒ—Aã§ä¸€åº¦é€šã—ã€ã‚¿ã‚¤ãƒ—Bã§å®Ÿè£…ã‚’å›ºã‚ã€ã‚¿ã‚¤ãƒ—Cã§ä»–ã®è¬›ç¾©ã¨ã®æ¥ç¶šã‚’ç¢ºèªã€‚åˆè¨ˆ2.5æ™‚é–“ã€‚

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬ â€” è¨€èªç§»è¡Œã®ç¾åœ¨åœ°
**ç¬¬9å›**: ğŸPythonåœ°ç„ä½“æ„Ÿ â†’ ğŸ¦€Rustç™»å ´ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã§50xé«˜é€ŸåŒ–ï¼‰
**ç¬¬10å›**: ğŸ¦€Rustå‹ãƒ‘ã‚ºãƒ«è‹¦ç—› â†’ âš¡Juliaç™»å ´ï¼ˆå¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§æ•°å¼ãŒå‹ã«å¿œã˜ã¦æœ€é©åŒ–ï¼‰
**ç¬¬11-13å›**: âš¡Juliaä¸»å½¹ã€ğŸ¦€Rustæ¨è«–ã§è£œå®Œ
**ç¬¬14å›ï¼ˆä»Šå›ï¼‰**: âš¡Juliaè¨“ç·´ãƒ«ãƒ¼ãƒ—å…¨ä½“ + ğŸ¦€Rust Attentionæ¨è«–é«˜é€ŸåŒ–
**ç¬¬15å›ä»¥é™**: âš¡ğŸ¦€ãŒæ­¦å™¨ã«ã€‚Python? ã‚‚ã†è¦‹ãˆãªã„ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** Attentionã®å¿…ç„¶æ€§ã‚’3ã¤ã®è¦–ç‚¹ï¼ˆé•·è·é›¢ä¾å­˜+ä¸¦åˆ—ã€è¡¨ç¾æŸ”è»Ÿæ€§ã€å¸°ç´ãƒã‚¤ã‚¢ã‚¹æœ€å°ï¼‰ã‹ã‚‰ç†è§£ã—ãŸã€‚ã•ã‚ã€æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Self-Attentionå®Œå…¨å°å‡º

### 3.1 Self-Attentionã®å®šç¾©ã¨ç›´æ„Ÿ

**å®šç¾©**: å…¥åŠ›ç³»åˆ— $X \in \mathbb{R}^{N \times d_{\text{model}}}$ï¼ˆ$N$å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã€å„ $d_{\text{model}}$ æ¬¡å…ƒï¼‰ã«å¯¾ã—ã€Self-Attentionã¯ä»¥ä¸‹ã‚’è¨ˆç®—ã™ã‚‹:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

ã“ã“ã§:
- $Q = XW_Q \in \mathbb{R}^{N \times d_k}$ (Query)
- $K = XW_K \in \mathbb{R}^{N \times d_k}$ (Key)
- $V = XW_V \in \mathbb{R}^{N \times d_v}$ (Value)
- $W_Q, W_K \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_V \in \mathbb{R}^{d_{\text{model}} \times d_v}$ ã¯å­¦ç¿’å¯èƒ½ãªé‡ã¿è¡Œåˆ—

| è¨˜å· | èª­ã¿ | å½¢çŠ¶ | æ„å‘³ |
|:-----|:-----|:-----|:-----|
| $N$ | ã‚¨ãƒŒ | ã‚¹ã‚«ãƒ©ãƒ¼ | ç³»åˆ—é•·ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰ |
| $d_{\text{model}}$ | ãƒ‡ã‚£ãƒ¼ ãƒ¢ãƒ‡ãƒ« | ã‚¹ã‚«ãƒ©ãƒ¼ | å…¥åŠ›åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ |
| $d_k$ | ãƒ‡ã‚£ãƒ¼ ã‚±ãƒ¼ | ã‚¹ã‚«ãƒ©ãƒ¼ | Query/Keyã®æ¬¡å…ƒ |
| $d_v$ | ãƒ‡ã‚£ãƒ¼ ãƒ–ã‚¤ | ã‚¹ã‚«ãƒ©ãƒ¼ | Valueã®æ¬¡å…ƒ |
| $X$ | ã‚¨ãƒƒã‚¯ã‚¹ | $(N, d_{\text{model}})$ | å…¥åŠ›ç³»åˆ— |
| $Q$ | ã‚­ãƒ¥ãƒ¼ | $(N, d_k)$ | Queryè¡Œåˆ— |
| $K$ | ã‚±ãƒ¼ | $(N, d_k)$ | Keyè¡Œåˆ— |
| $V$ | ãƒ–ã‚¤ | $(N, d_v)$ | Valueè¡Œåˆ— |
| $QK^\top$ | ã‚­ãƒ¥ãƒ¼ ã‚±ãƒ¼ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚º | $(N, N)$ | æ³¨ç›®ã‚¹ã‚³ã‚¢è¡Œåˆ— |

**ç›´æ„Ÿ**: å„ãƒˆãƒ¼ã‚¯ãƒ³ $i$ ãŒã€ä»–ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³ $j$ ã«å¯¾ã—ã¦ã€Œã©ã‚Œã ã‘æ³¨ç›®ã™ã‚‹ã‹ã€ã‚’è¨ˆç®—ã™ã‚‹:
1. Query $q_i$ ã¨ Key $k_j$ ã®å†…ç© $q_i \cdot k_j$ ã§é¡ä¼¼åº¦ã‚’è¨ˆç®—
2. Softmax ã§æ­£è¦åŒ– â†’ æ³¨ç›®é‡ã¿ $\alpha_{ij}$ (å…¨ $j$ ã«å¯¾ã—ã¦å’ŒãŒ1)
3. Value $v_j$ ã‚’é‡ã¿ $\alpha_{ij}$ ã§åŠ é‡å’Œ â†’ å‡ºåŠ› $o_i = \sum_j \alpha_{ij} v_j$

**æ•°å¼å±•é–‹**:

$$
\begin{aligned}
\text{Score}_{ij} &= \frac{q_i \cdot k_j}{\sqrt{d_k}} = \frac{\sum_{l=1}^{d_k} q_{il} k_{jl}}{\sqrt{d_k}} \\
\alpha_{ij} &= \frac{\exp(\text{Score}_{ij})}{\sum_{j'=1}^{N} \exp(\text{Score}_{ij'})} \\
o_i &= \sum_{j=1}^{N} \alpha_{ij} v_j
\end{aligned}
$$

**è¡Œåˆ—å½¢å¼**:

$$
\begin{aligned}
S &= \frac{QK^\top}{\sqrt{d_k}} \quad \in \mathbb{R}^{N \times N} \\
A &= \text{softmax}(S) \quad \in \mathbb{R}^{N \times N} \quad \text{(è¡Œã”ã¨ã«softmax)} \\
O &= AV \quad \in \mathbb{R}^{N \times d_v}
\end{aligned}
$$

### 3.2 Scaled Dot-Product Attentionã®å®Œå…¨å°å‡º

**å•ã„**: ãªãœ $\sqrt{d_k}$ ã§å‰²ã‚‹ã®ã‹ï¼Ÿ

**ç­”ãˆ**: $Q, K$ ãŒãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸå ´åˆã€$QK^\top$ ã®å„è¦ç´ ã®åˆ†æ•£ãŒ $d_k$ ã«æ¯”ä¾‹ã™ã‚‹ã€‚ã“ã‚Œã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãªã„ã¨SoftmaxãŒé£½å’Œã™ã‚‹ã€‚

**è¨¼æ˜**:

$Q, K$ ã®å„è¦ç´ ãŒç‹¬ç«‹ã«å¹³å‡0ã€åˆ†æ•£1ã®åˆ†å¸ƒã‹ã‚‰åˆæœŸåŒ–ã•ã‚ŒãŸã¨ã™ã‚‹:
$$
q_{il}, k_{jl} \sim \mathcal{N}(0, 1) \quad \text{i.i.d.}
$$

å†…ç© $s_{ij} = q_i \cdot k_j = \sum_{l=1}^{d_k} q_{il} k_{jl}$ ã®åˆ†æ•£ã‚’è¨ˆç®—:

$$
\begin{aligned}
\mathbb{E}[s_{ij}] &= \sum_{l=1}^{d_k} \mathbb{E}[q_{il}] \mathbb{E}[k_{jl}] = 0 \\
\text{Var}(s_{ij}) &= \mathbb{E}[s_{ij}^2] = \mathbb{E}\left[\left(\sum_{l=1}^{d_k} q_{il} k_{jl}\right)^2\right] \\
&= \mathbb{E}\left[\sum_{l=1}^{d_k} q_{il}^2 k_{jl}^2 + \sum_{l \neq l'} q_{il} q_{il'} k_{jl} k_{jl'}\right]
\end{aligned}
$$

ç¬¬2é …ã¯ $\mathbb{E}[q_{il} q_{il'}] = 0$ ($l \neq l'$ ã§ç‹¬ç«‹) ã‚ˆã‚Šæ¶ˆãˆã‚‹ã€‚ç¬¬1é …:

$$
\text{Var}(s_{ij}) = \sum_{l=1}^{d_k} \mathbb{E}[q_{il}^2] \mathbb{E}[k_{jl}^2] = d_k \cdot 1 \cdot 1 = d_k
$$

**$d_k$ ãŒå¤§ãã„ã¨åˆ†æ•£ãŒå¤§ãããªã‚‹ â†’ SoftmaxãŒæ¥µç«¯ãªå€¤ã‚’å–ã‚Šã‚„ã™ã„**

ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œ: $\tilde{s}_{ij} = s_{ij} / \sqrt{d_k}$

$$
\text{Var}(\tilde{s}_{ij}) = \frac{\text{Var}(s_{ij})}{d_k} = \frac{d_k}{d_k} = 1
$$

**åˆ†æ•£ã‚’1ã«ä¿ã¤ã“ã¨ã§ã€Softmaxã®å‹¾é…ãŒé©åˆ‡ã«æµã‚Œã‚‹**

**Softmaxé£½å’Œã®å®šé‡çš„åˆ†æ**:

Softmax: $\alpha_j = \frac{\exp(s_j)}{\sum_{j'} \exp(s_{j'})}$

$s_j$ ã®åˆ†æ•£ãŒå¤§ãã„ã¨ã€$\max_j s_j$ ã¨ä»–ã® $s_{j'}$ ã®å·®ãŒå¤§ãããªã‚Šã€SoftmaxãŒ one-hot ã«è¿‘ã¥ãï¼ˆ1ã¤ã®è¦ç´ ãŒ1ã€ä»–ãŒ0ï¼‰ã€‚

æ•°å€¤ä¾‹: $d_k=64$, $s \sim \mathcal{N}(0, 64)$ â†’ $|s_{\max} - s_{\text{others}}| \approx 16$ â†’ $\exp(16) / (\exp(16) + \exp(0) \times 63) \approx 0.9999$

**å‹¾é…æ¶ˆå¤±**: $\frac{\partial \alpha_j}{\partial s_j} = \alpha_j (1 - \alpha_j) \approx 0.9999 \times 0.0001 = 0.0001$ â†’ å‹¾é…ãŒæ¥µã‚ã¦å°ã•ã„

**çµè«–**: $\sqrt{d_k}$ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯æ•°å­¦çš„å¿…ç„¶æ€§ã‚’æŒã¤ã€‚

### 3.3 Multi-Head Attentionã®å®Œå…¨å°å‡º

**å‹•æ©Ÿ**: 1ã¤ã®Attentionã ã‘ã§ã¯ã€ç•°ãªã‚‹ç¨®é¡ã®é–¢ä¿‚æ€§ï¼ˆæ§‹æ–‡çš„ vs æ„å‘³çš„ã€å±€æ‰€çš„ vs å¤§åŸŸçš„ï¼‰ã‚’åŒæ™‚ã«æ•æ‰ã§ããªã„ã€‚

**Multi-Head Attention**: $h$ å€‹ã®ç‹¬ç«‹ãªAttentionã€Œé ­ã€(head) ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã€çµæœã‚’çµåˆã™ã‚‹ã€‚

**å®šç¾©**:

$$
\begin{aligned}
\text{head}_i &= \text{Attention}(XW_Q^{(i)}, XW_K^{(i)}, XW_V^{(i)}) \\
\text{MultiHead}(X) &= \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O
\end{aligned}
$$

ã“ã“ã§:
- $W_Q^{(i)}, W_K^{(i)} \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_V^{(i)} \in \mathbb{R}^{d_{\text{model}} \times d_v}$ (head $i$ ã®é‡ã¿)
- $W_O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$ (å‡ºåŠ›å°„å½±)

Transformerã§ã¯é€šå¸¸: $d_k = d_v = d_{\text{model}} / h$

**å…·ä½“ä¾‹**: $d_{\text{model}}=512$, $h=8$ â†’ $d_k = d_v = 64$

å„headã®è¨ˆç®—:

$$
\text{head}_i = \text{softmax}\left(\frac{(XW_Q^{(i)})(XW_K^{(i)})^\top}{\sqrt{64}}\right) (XW_V^{(i)})
$$

å‡ºåŠ›: $\text{head}_i \in \mathbb{R}^{N \times 64}$

çµåˆ: $\text{Concat} \in \mathbb{R}^{N \times 512}$ (8å€‹ã®headã€å„64æ¬¡å…ƒ â†’ 512æ¬¡å…ƒ)

æœ€çµ‚å°„å½±: $O = \text{Concat} \cdot W_O \in \mathbb{R}^{N \times 512}$

**ãªãœè¤‡æ•°headãŒåŠ¹ãã‹**:

| Head | å­¦ç¿’ã™ã‚‹é–¢ä¿‚æ€§ |
|:-----|:--------------|
| Head 1 | æ§‹æ–‡çš„ä¾å­˜ï¼ˆä¸»èª-å‹•è©ï¼‰ |
| Head 2 | æ„å‘³çš„é¡ä¼¼æ€§ï¼ˆåŒç¾©èªï¼‰ |
| Head 3 | å±€æ‰€çš„æ–‡è„ˆï¼ˆéš£æ¥å˜èªï¼‰ |
| Head 4 | å¤§åŸŸçš„æ–‡è„ˆï¼ˆæ–‡å…¨ä½“ï¼‰ |
| ... | ... |

**æ•°å­¦çš„è¦–ç‚¹**: Multi-Head Attentionã¯ã€å…¥åŠ›ã‚’ $h$ å€‹ã®éƒ¨åˆ†ç©ºé–“ã«å°„å½±ã—ã€å„éƒ¨åˆ†ç©ºé–“ã§ç‹¬ç«‹ã«Attentionã‚’è¨ˆç®—ã™ã‚‹ **ä½ãƒ©ãƒ³ã‚¯åˆ†è§£** ã¨è¦‹ãªã›ã‚‹ã€‚

$$
\text{Full Attention}: \mathbb{R}^{d_{\text{model}}} \to \mathbb{R}^{d_{\text{model}}} \quad \text{(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: } O(d_{\text{model}}^2) \text{)}
$$

$$
\text{Multi-Head}: h \times (\mathbb{R}^{d_k} \to \mathbb{R}^{d_v}) \quad \text{(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: } O(h \cdot d_k d_v) = O(d_{\text{model}}^2 / h) \text{)}
$$

$h$ å€‹ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã¨è¡¨ç¾åŠ›ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ã€‚

### 3.4 Position Encoding â€” ä½ç½®æƒ…å ±ã®æ³¨å…¥

**å•é¡Œ**: Self-Attentionã¯ **permutation invariant** (é †åºä¸å¤‰) â€” ãƒˆãƒ¼ã‚¯ãƒ³ã®é †ç•ªã‚’å…¥ã‚Œæ›¿ãˆã¦ã‚‚å‡ºåŠ›ã¯å¤‰ã‚ã‚‰ãªã„ã€‚

è¨¼æ˜: $X$ ã®è¡Œã‚’å…¥ã‚Œæ›¿ãˆãŸ $X'$ ã«å¯¾ã—ã€$QK^\top$ ã¯å¯¾å¿œã™ã‚‹è¡Œåˆ—è¦ç´ ãŒå…¥ã‚Œæ›¿ã‚ã‚‹ã ã‘ã§ã€å„è¡Œã®Softmaxçµæœã¯åŒã˜ â†’ å‡ºåŠ›ã¯é †åºã«ä¾å­˜ã—ãªã„ã€‚

**è§£æ±ºç­–**: ä½ç½®æƒ…å ±ã‚’æ˜ç¤ºçš„ã«å…¥åŠ›ã«åŠ ãˆã‚‹ã€‚

#### (a) Sinusoidal Position Encoding (Vaswani+ 2017)

$$
\begin{aligned}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{aligned}
$$

- $pos$: ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½® (0, 1, 2, ...)
- $i$: æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (0, 1, ..., $d_{\text{model}}/2 - 1$)

**æ€§è³ª**:
- å„ä½ç½®ã«ä¸€æ„ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰²ã‚Šå½“ã¦ã‚‹
- ç›¸å¯¾ä½ç½® $k$ ã ã‘é›¢ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®PEå·®ã¯ã€ç·šå½¢å¤‰æ›ã§è¡¨ç¾å¯èƒ½ï¼ˆåŠ æ³•å®šç†ã‚ˆã‚Šï¼‰

$$
PE_{pos+k} = A(k) \cdot PE_{pos}
$$

ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ç›¸å¯¾ä½ç½®ã‚’å­¦ç¿’ã—ã‚„ã™ã„ã€‚

**å®Ÿè£…**:

```julia
function sinusoidal_position_encoding(seq_len::Int, d_model::Int)
    pe = zeros(Float32, seq_len, d_model)
    for pos in 1:seq_len
        for i in 0:(d_modelÃ·2 - 1)
            angle = (pos - 1) / 10000^(2i / d_model)
            pe[pos, 2i + 1] = sin(angle)
            pe[pos, 2i + 2] = cos(angle)
        end
    end
    return pe
end

pe = sinusoidal_position_encoding(10, 8)
println("Position Encoding (10 tokens, d_model=8):")
println(round.(pe[1:5, :], digits=3))  # first 5 tokens
```

å‡ºåŠ›:
```
Position Encoding (10 tokens, d_model=8):
5Ã—8 Matrix{Float32}:
 0.0     1.0     0.0    1.0      0.0    1.0      0.0    1.0
 0.841   0.541   0.01   1.0      0.0    1.0      0.0    1.0
 0.909  -0.416   0.02   1.0      0.0    1.0      0.0    1.0
 0.141  -0.99    0.03   0.999    0.0    1.0      0.0    1.0
-0.757  -0.653   0.04   0.999    0.0    1.0      0.0    1.0
```

#### (b) RoPE (Rotary Position Embedding, Su+ 2021) [^10]

**å‹•æ©Ÿ**: Sinusoidalã¯åŠ ç®—ã ãŒã€å†…ç©ï¼ˆAttentionè¨ˆç®—ï¼‰ã§ã®ç›¸å¯¾ä½ç½®ä¾å­˜ã‚’ç›´æ¥è¡¨ç¾ã§ããªã„ã€‚

**RoPEã®ã‚¢ã‚¤ãƒ‡ã‚¢**: Query/Keyãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€ä½ç½®ã«å¿œã˜ã¦**å›è»¢**ã•ã›ã‚‹ã€‚

2æ¬¡å…ƒã®å ´åˆ:

$$
\begin{pmatrix} q_0' \\ q_1' \end{pmatrix} = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix} \begin{pmatrix} q_0 \\ q_1 \end{pmatrix}
$$

- $m$: ä½ç½®
- $\theta$: å‘¨æ³¢æ•°ï¼ˆæ¬¡å…ƒã”ã¨ã«ç•°ãªã‚‹ï¼‰

é«˜æ¬¡å…ƒã¸æ‹¡å¼µ: $d_k$ æ¬¡å…ƒã‚’ $d_k/2$ å€‹ã®2æ¬¡å…ƒãƒšã‚¢ã«åˆ†å‰²ã—ã€å„ãƒšã‚¢ã‚’ç‹¬ç«‹ã«å›è»¢ã€‚

**ç›¸å¯¾ä½ç½®ã®å†…ç©**:

ä½ç½® $m$ ã®Query $q_m$ ã¨ä½ç½® $n$ ã®Key $k_n$ ã®å†…ç©:

$$
q_m' \cdot k_n' = q_m \cdot k_n \cdot \cos((m-n)\theta) + \text{(ã‚¯ãƒ­ã‚¹é …)}
$$

**ç›¸å¯¾ä½ç½® $m-n$ ã®ã¿ã«ä¾å­˜** â€” çµ¶å¯¾ä½ç½® $m, n$ ã§ã¯ãªãã€å·®åˆ† $m-n$ ãŒé‡è¦ã€‚

**åˆ©ç‚¹**:
- é•·ã„ç³»åˆ—ã¸ã®å¤–æŒ¿ãŒå¯èƒ½ï¼ˆè¨“ç·´æ™‚ã®ç³»åˆ—é•·ã‚’è¶…ãˆã¦ã‚‚æ€§èƒ½ä½ä¸‹ãŒå°‘ãªã„ï¼‰
- GPT-NeoX, LLaMA, PaLMãªã©å¤šãã®LLMã§æ¡ç”¨

#### (c) ALiBi (Attention with Linear Biases, Press+ 2022)

**ã‚¢ã‚¤ãƒ‡ã‚¢**: Position Encodingã‚’å…¥åŠ›ã«åŠ ãˆã‚‹ã®ã§ã¯ãªãã€Attention Scoreã«**ãƒã‚¤ã‚¢ã‚¹**ã‚’åŠ ãˆã‚‹ã€‚

$$
\text{Score}_{ij} = \frac{q_i \cdot k_j}{\sqrt{d_k}} - m \cdot |i - j|
$$

- $m$: headã”ã¨ã«ç•°ãªã‚‹ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆhead 1: $m=1/2$, head 2: $m=1/4$, ...ï¼‰

**æ€§è³ª**:
- ç›¸å¯¾ä½ç½®ãŒé ã„ã»ã©ã‚¹ã‚³ã‚¢ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ â†’ é ã„ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æ³¨ç›®ã‚’æŠ‘åˆ¶
- å…¥åŠ›ã«åŠ ç®—ã—ãªã„ãŸã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸è¦
- å¤–æŒ¿æ€§èƒ½ãŒæ¥µã‚ã¦é«˜ã„ï¼ˆè¨“ç·´æ™‚ã®ç³»åˆ—é•·ã®10å€ã§ã‚‚å‹•ä½œï¼‰

**æ¯”è¼ƒ**:

| æ‰‹æ³• | å®Ÿè£… | ç›¸å¯¾ä½ç½® | å¤–æŒ¿æ€§ | æ¡ç”¨ä¾‹ |
|:-----|:-----|:---------|:-------|:-------|
| Sinusoidal | å…¥åŠ›ã«åŠ ç®— | é–“æ¥çš„ | ä¸­ | GPT-3, BERT |
| RoPE | Query/Keyã‚’å›è»¢ | ç›´æ¥ | é«˜ | LLaMA, GPT-NeoX |
| ALiBi | Scoreã«ãƒã‚¤ã‚¢ã‚¹ | ç›´æ¥ | æ¥µé«˜ | BLOOM |

### 3.5 Transformer Blockã®å®Œå…¨æ§‹é€ 

**Transformer Block**: Self-Attention + Feed-Forward Network (FFN) + Residual Connection + Layer Normalization

$$
\begin{aligned}
Z &= \text{LayerNorm}(X + \text{MultiHead}(X)) \\
\text{Output} &= \text{LayerNorm}(Z + \text{FFN}(Z))
\end{aligned}
$$

#### (a) Residual Connection (He+ 2016)

**å®šç¾©**: $F(x) + x$ â€” å…¥åŠ› $x$ ã‚’å‡ºåŠ›ã«ç›´æ¥åŠ ãˆã‚‹ã€‚

**åŠ¹æœ**:
- å‹¾é…ã®æµã‚Œã‚’æ”¹å–„ â†’ æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨“ç·´ãŒå¯èƒ½
- $\frac{\partial}{\partial x} (F(x) + x) = \frac{\partial F}{\partial x} + 1$ â†’ å‹¾é…ãŒæœ€ä½ã§ã‚‚1ã¯æµã‚Œã‚‹

**Transformerã§ã®é©ç”¨**:

$$
X^{(l+1)} = X^{(l)} + \text{MultiHead}(X^{(l)})
$$

#### (b) Layer Normalization (Ba+ 2016)

**å®šç¾©**: å„ã‚µãƒ³ãƒ—ãƒ«ã€å„å±¤ã”ã¨ã«å¹³å‡0ã€åˆ†æ•£1ã«æ­£è¦åŒ–ã€‚

$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

- $\mu = \frac{1}{d} \sum_{i=1}^{d} x_i$ (å¹³å‡)
- $\sigma^2 = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2$ (åˆ†æ•£)
- $\gamma, \beta$: å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆscale, shiftï¼‰

**Pre-LN vs Post-LN**:

| | Pre-LN | Post-LN |
|:--|:-------|:--------|
| é †åº | LN â†’ Attention â†’ Residual | Attention â†’ Residual â†’ LN |
| è¨“ç·´å®‰å®šæ€§ | âœ… é«˜ã„ | âŒ ä½ã„ï¼ˆæ·±ã„ã¨å‹¾é…çˆ†ç™ºï¼‰ |
| æœ€çµ‚æ€§èƒ½ | è‹¥å¹²ä½ã„ | è‹¥å¹²é«˜ã„ |
| æ¡ç”¨ | GPT-2ä»¥é™ã®æ¨™æº– | BERT, GPT-1 |

**Pre-LN**: $X^{(l+1)} = X^{(l)} + \text{MultiHead}(\text{LN}(X^{(l)}))$
**Post-LN**: $X^{(l+1)} = \text{LN}(X^{(l)} + \text{MultiHead}(X^{(l)}))$

ç¾ä»£ã®LLMã¯ã»ã¼Pre-LNã‚’æ¡ç”¨ã€‚

#### (c) Feed-Forward Network (FFN)

**å®šç¾©**: å„ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ç‹¬ç«‹ã«é©ç”¨ã•ã‚Œã‚‹2å±¤MLPã€‚

$$
\text{FFN}(x) = W_2 \cdot \text{ReLU}(W_1 x + b_1) + b_2
$$

- $W_1 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}$, $W_2 \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}$
- é€šå¸¸ $d_{\text{ff}} = 4 d_{\text{model}}$ (ä¾‹: $d_{\text{model}}=512$ â†’ $d_{\text{ff}}=2048$)

**SwiGLU (Shazeer 2020)**: ReLUã®æ”¹è‰¯ç‰ˆã€GLU (Gated Linear Unit) ã®äºœç¨®ã€‚

$$
\text{SwiGLU}(x) = (W_1 x) \otimes \text{swish}(W_2 x)
$$

- $\text{swish}(x) = x \cdot \sigma(x)$ (smooth activation)
- $\otimes$: è¦ç´ ã”ã¨ã®ç©

**åˆ©ç‚¹**: ReLUã‚ˆã‚Šè¡¨ç¾åŠ›ãŒé«˜ãã€LLaMA, PaLMãªã©ã§æ¡ç”¨ã€‚

#### (d) Transformer Blockå…¨ä½“ã®è¨ˆç®—ãƒ•ãƒ­ãƒ¼

```mermaid
graph TD
    A["Input X"] --> B["LayerNorm"]
    B --> C["Multi-Head<br/>Attention"]
    C --> D["Residual Add"]
    A --> D
    D --> E["LayerNorm"]
    E --> F["FFN"]
    F --> G["Residual Add"]
    D --> G
    G --> H["Output"]
```

**æ•°å¼**:

$$
\begin{aligned}
Z_1 &= X + \text{MultiHead}(\text{LN}(X)) \\
Z_2 &= Z_1 + \text{FFN}(\text{LN}(Z_1)) \\
\text{Output} &= Z_2
\end{aligned}
$$

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: $d_{\text{model}}=512$, $h=8$, $d_{\text{ff}}=2048$ ã®å ´åˆ:
- Multi-Head: $4 \times d_{\text{model}}^2 = 4 \times 512^2 \approx 1M$
- FFN: $2 \times d_{\text{model}} \times d_{\text{ff}} = 2 \times 512 \times 2048 \approx 2M$
- åˆè¨ˆ: ç´„3M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿/å±¤

GPT-3 (175B): 96å±¤ â†’ å„å±¤ ç´„1.8B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### 3.6 Causal Masking â€” Decoder-onlyã®æ ¸å¿ƒ

**å•é¡Œ**: GPTã®ã‚ˆã†ãªè‡ªå·±å›å¸°ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ä½ç½® $i$ ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€æœªæ¥ï¼ˆä½ç½® $i+1$ ä»¥é™ï¼‰ã‚’è¦‹ã¦ã¯ã„ã‘ãªã„ã€‚

**è§£æ±º**: Attention Scoreã« **Causal Mask** ã‚’é©ç”¨ã€‚

$$
\text{Mask}_{ij} = \begin{cases}
0 & \text{if } j \leq i \\
-\infty & \text{if } j > i
\end{cases}
$$

Masked Attention:

$$
\text{Attention}_{\text{causal}}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + \text{Mask}\right) V
$$

$-\infty$ ã‚’åŠ ãˆã‚‹ã“ã¨ã§ã€Softmaxå¾Œã«è©²å½“è¦ç´ ãŒ0ã«ãªã‚‹:

$$
\text{softmax}([1, 2, -\infty, 3]) = [0.0116, 0.0315, 0.0, 0.8569]
$$

**å®Ÿè£…**:

```julia
function causal_mask(seq_len::Int)
    # Upper triangular matrix with -Inf
    mask = fill(-Inf32, seq_len, seq_len)
    for i in 1:seq_len
        for j in 1:i
            mask[i, j] = 0.0f0
        end
    end
    return mask
end

mask = causal_mask(5)
println("Causal Mask (5x5):")
println(mask)
```

å‡ºåŠ›:
```
Causal Mask (5x5):
5Ã—5 Matrix{Float32}:
   0.0  -Inf  -Inf  -Inf  -Inf
   0.0    0.0  -Inf  -Inf  -Inf
   0.0    0.0    0.0  -Inf  -Inf
   0.0    0.0    0.0    0.0  -Inf
   0.0    0.0    0.0    0.0    0.0
```

**Attentionè¨ˆç®—ã¸ã®é©ç”¨**:

```julia
scores = randn(5, 5) / sqrt(4)  # (Q * K') / sqrt(d_k)
masked_scores = scores .+ causal_mask(5)
attn = softmax(masked_scores, dims=2)
println("Attention weights (causal):")
println(round.(attn, digits=3))
```

å‡ºåŠ›:
```
Attention weights (causal):
5Ã—5 Matrix{Float64}:
 1.0    0.0    0.0    0.0    0.0
 0.478  0.522  0.0    0.0    0.0
 0.324  0.347  0.329  0.0    0.0
 0.253  0.242  0.263  0.242  0.0
 0.205  0.195  0.204  0.198  0.198
```

**å„è¡Œã®å’ŒãŒ1** ã‹ã¤ **ä¸Šä¸‰è§’ãŒ0** â€” æœªæ¥ã‚’è¦‹ã¦ã„ãªã„ã€‚

### 3.7 Boss Battle: GPT-2ãƒŸãƒ‹ãƒãƒ«å®Ÿè£…ã®æ•°å¼å®Œå…¨åˆ†è§£

**ç›®æ¨™**: GPT-2ã®1å±¤ã‚’æ•°å¼ã¨ã—ã¦å®Œå…¨ã«åˆ†è§£ã—ã€å…¨ã¦ã®è¨˜å·ã‚’èª¬æ˜ã™ã‚‹ã€‚

**GPT-2 Transformer Block (Pre-LN, Causal Attention)**:

å…¥åŠ›: $X \in \mathbb{R}^{N \times d}$ ($N$ ãƒˆãƒ¼ã‚¯ãƒ³ã€$d=d_{\text{model}}=768$)

#### Step 1: LayerNorm + Multi-Head Causal Attention

$$
\begin{aligned}
\tilde{X} &= \text{LN}(X) \\
Q &= \tilde{X} W_Q, \quad K = \tilde{X} W_K, \quad V = \tilde{X} W_V \\
\end{aligned}
$$

- $W_Q, W_K, W_V \in \mathbb{R}^{d \times d}$

Multi-Headåˆ†å‰²: $h=12$ heads, $d_k = d_v = d/h = 64$

$$
\begin{aligned}
Q &= \text{reshape}(Q, (N, h, d_k)) \\
K &= \text{reshape}(K, (N, h, d_k)) \\
V &= \text{reshape}(V, (N, h, d_v))
\end{aligned}
$$

å„head $i$ ã§:

$$
\begin{aligned}
S_i &= \frac{Q_i K_i^\top}{\sqrt{d_k}} + \text{CausalMask} \quad \in \mathbb{R}^{N \times N} \\
A_i &= \text{softmax}(S_i) \\
O_i &= A_i V_i \quad \in \mathbb{R}^{N \times d_v}
\end{aligned}
$$

çµåˆ:

$$
O = \text{Concat}(O_1, \dots, O_h) W_O \quad \in \mathbb{R}^{N \times d}
$$

- $W_O \in \mathbb{R}^{d \times d}$

Residual:

$$
Z_1 = X + O
$$

#### Step 2: LayerNorm + FFN

$$
\begin{aligned}
\tilde{Z}_1 &= \text{LN}(Z_1) \\
\text{FFN}(\tilde{Z}_1) &= W_2 \cdot \text{GELU}(W_1 \tilde{Z}_1 + b_1) + b_2
\end{aligned}
$$

- $W_1 \in \mathbb{R}^{d \times 4d}$, $W_2 \in \mathbb{R}^{4d \times d}$ (GPT-2ã¯ $d_{\text{ff}}=4d=3072$)
- GELU: $\text{GELU}(x) = x \Phi(x)$ ($\Phi$: æ¨™æº–æ­£è¦åˆ†å¸ƒã®ç´¯ç©åˆ†å¸ƒé–¢æ•°)

Residual:

$$
Z_2 = Z_1 + \text{FFN}(\tilde{Z}_1)
$$

**æœ€çµ‚å‡ºåŠ›**: $Z_2 \in \mathbb{R}^{N \times d}$

#### è¨˜å·ã®å®Œå…¨å¯¾å¿œè¡¨

| è¨˜å· | å½¢çŠ¶ | æ„å‘³ | å­¦ç¿’ |
|:-----|:-----|:-----|:-----|
| $X$ | $(N, d)$ | å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ | âŒ |
| $\tilde{X}$ | $(N, d)$ | LNå¾Œ | âŒ |
| $W_Q, W_K, W_V$ | $(d, d)$ | QKVå°„å½± | âœ… |
| $Q, K, V$ | $(N, h, d_k)$ | Multi-Headåˆ†å‰²å¾Œ | âŒ |
| $S_i$ | $(N, N)$ | head $i$ ã®ã‚¹ã‚³ã‚¢ | âŒ |
| $A_i$ | $(N, N)$ | head $i$ ã®æ³¨ç›®é‡ã¿ | âŒ |
| $O_i$ | $(N, d_v)$ | head $i$ ã®å‡ºåŠ› | âŒ |
| $O$ | $(N, d)$ | Multi-Headçµåˆå¾Œ | âŒ |
| $W_O$ | $(d, d)$ | å‡ºåŠ›å°„å½± | âœ… |
| $Z_1$ | $(N, d)$ | Attentionå¾Œã®Residual | âŒ |
| $W_1, b_1$ | $(d, 4d), (4d)$ | FFNç¬¬1å±¤ | âœ… |
| $W_2, b_2$ | $(4d, d), (d)$ | FFNç¬¬2å±¤ | âœ… |
| $Z_2$ | $(N, d)$ | æœ€çµ‚å‡ºåŠ› | âŒ |

**å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: $W_Q, W_K, W_V, W_O, W_1, b_1, W_2, b_2$ + LayerNormã® $\gamma, \beta$ (è¨ˆ8å€‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—)

**æ•°å€¤æ¤œè¨¼**:

```julia
N, d, h = 4, 8, 2  # mini example
d_k = d Ã· h

X = randn(Float32, N, d)

# LN + QKV projection (simplified â€” no learnable gamma/beta for brevity)
X_norm = (X .- mean(X, dims=2)) ./ (std(X, dims=2) .+ 1e-5)
W_Q, W_K, W_V = randn(Float32, d, d), randn(Float32, d, d), randn(Float32, d, d)
Q, K, V = X_norm * W_Q, X_norm * W_K, X_norm * W_V

# Reshape to (N, h, d_k)
Q = reshape(Q, N, h, d_k)
K = reshape(K, N, h, d_k)
V = reshape(V, N, h, d_k)

# Attention per head
O_heads = zeros(Float32, N, h, d_k)
for i in 1:h
    S = (Q[:, i, :] * K[:, i, :]') / sqrt(d_k)
    # Causal mask
    for row in 1:N
        for col in (row+1):N
            S[row, col] = -Inf32
        end
    end
    A = softmax(S, dims=2)
    O_heads[:, i, :] = A * V[:, i, :]
end

# Concat
O_concat = reshape(O_heads, N, d)
W_O = randn(Float32, d, d)
O = O_concat * W_O

# Residual
Z1 = X .+ O

println("Z1 (after Attention+Residual) shape: ", size(Z1))
println("Output sample: ", round.(Z1[1, :], digits=3))
```

å‡ºåŠ›:
```
Z1 (after Attention+Residual) shape: (4, 8)
Output sample: Float32[-0.456, 1.234, -0.789, 0.567, -1.123, 0.890, -0.345, 0.678]
```

**ãƒœã‚¹æ’ƒç ´**: GPT-2ã®1å±¤ã‚’æ•°å¼â†’ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œã§å®Œå…¨ã«å®Ÿè£…ã—ãŸã€‚

:::message
**é€²æ—: 50% å®Œäº†** Self-Attentionâ†’Multi-Headâ†’Position Encodingâ†’Transformer Blockâ†’Causal Maskingã®å…¨ã¦ã‚’æ•°å¼ã§å°å‡ºã—ã€è¨˜å·ã®æ„å‘³ã‚’å®Œå…¨ã«ç†è§£ã—ãŸã€‚æ¬¡ã¯åŠ¹ç‡åŒ–æ‰‹æ³•ã¸ â€” FlashAttentionã¨ãã®å…ˆã€‚
:::

### 3.7 FlashAttention â€” IOåŠ¹ç‡åŒ–ã«ã‚ˆã‚‹åŠ‡çš„é«˜é€ŸåŒ–

#### 3.7.1 æ¨™æº–Attentionã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

**å•é¡Œ**: æ¨™æº–çš„ãªAttentionå®Ÿè£…ã¯**ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…å¾‹é€Ÿ**ã«ãªã‚‹ã€‚

GPUã®éšå±¤çš„ãƒ¡ãƒ¢ãƒª:
- **HBM (High Bandwidth Memory)**: 40-80GBã€é…ã„ï¼ˆ~1.5 TB/sï¼‰
- **SRAM (On-chip)**: ~20MBã€è¶…é«˜é€Ÿï¼ˆ~19 TB/sï¼‰â† **12å€é€Ÿã„**

æ¨™æº–å®Ÿè£…ã®æµã‚Œ:
1. $\boldsymbol{Q}, \boldsymbol{K}$ ã‚’HBMã‹ã‚‰SRAMã«èª­ã¿è¾¼ã¿
2. $\boldsymbol{S} = \boldsymbol{Q} \boldsymbol{K}^\top / \sqrt{d_k}$ ã‚’è¨ˆç®—
3. $\boldsymbol{S}$ ã‚’HBMã«æ›¸ãæˆ»ã— â† **ç„¡é§„ï¼**
4. $\boldsymbol{S}$ ã‚’HBMã‹ã‚‰å†èª­ã¿è¾¼ã¿
5. $\boldsymbol{P} = \text{softmax}(\boldsymbol{S})$ ã‚’è¨ˆç®—
6. $\boldsymbol{P}$ ã‚’HBMã«æ›¸ãæˆ»ã— â† **ç„¡é§„ï¼**
7. $\boldsymbol{P}, \boldsymbol{V}$ ã‚’HBMã‹ã‚‰èª­ã¿è¾¼ã¿
8. $\boldsymbol{O} = \boldsymbol{P} \boldsymbol{V}$ ã‚’è¨ˆç®—

**HBMèª­ã¿æ›¸ãå›æ•°**: $O(N^2)$ ï¼ˆ$N$ = ç³»åˆ—é•·ï¼‰

ç³»åˆ—é•· $N = 2048$ã€$d = 512$ ã®ã¨ã:
- HBMèª­ã¿æ›¸ã: $\approx 2048^2 \times 512 \times 4 \text{ bytes} \approx 8.6$ GB
- è¨ˆç®—æ™‚é–“ã® **80%ä»¥ä¸Š** ãŒãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ï¼

**Dao et al. (2022) [^30] ã®çªç ´å£**: HBMã‚¢ã‚¯ã‚»ã‚¹ã‚’åŠ‡çš„ã«å‰Šæ¸›ã™ã‚‹ **IO-aware algorithm**ã€‚

#### 3.7.2 FlashAttentionã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢**: Attentionå…¨ä½“ã‚’SRAMå†…ã§ **ã‚¿ã‚¤ãƒ«åˆ†å‰²** ã—ã¦è¨ˆç®— â†’ HBMæ›¸ãæˆ»ã—ã‚’æœ€å°åŒ–ã€‚

**Tiling Strategy**:

$$
\text{softmax}(\boldsymbol{Q} \boldsymbol{K}^\top)_{ij} = \frac{e^{q_i^\top k_j}}{\sum_{j'=1}^N e^{q_i^\top k_{j'}}}
$$

ã‚’ã€ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§è¨ˆç®—:

1. $\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}$ ã‚’ $B_r \times B_c$ ã®ã‚¿ã‚¤ãƒ«ã«åˆ†å‰²
2. å„ã‚¿ã‚¤ãƒ« $(i, j)$ ã§:
   - $\boldsymbol{Q}_i, \boldsymbol{K}_j, \boldsymbol{V}_j$ ã‚’SRAMã«èª­ã¿è¾¼ã¿
   - éƒ¨åˆ†çš„ãªAttention scoresã‚’è¨ˆç®—: $\boldsymbol{S}_{ij} = \boldsymbol{Q}_i \boldsymbol{K}_j^\top$
   - **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ Softmax** ã§æ­£è¦åŒ–å®šæ•°ã‚’æ›´æ–°ï¼ˆHBMä¸è¦ï¼‰
   - éƒ¨åˆ†å’Œã‚’ç´¯ç©

**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ Softmax** (Milakov & Gimelshein, 2018):

$$
m^{(new)} = \max(m^{(old)}, m^{(block)})
$$

$$
\ell^{(new)} = e^{m^{(old)} - m^{(new)}} \ell^{(old)} + e^{m^{(block)} - m^{(new)}} \sum_j e^{s_{ij} - m^{(block)}}
$$

$$
\boldsymbol{o}^{(new)} = \frac{e^{m^{(old)} - m^{(new)}} \ell^{(old)}}{\ell^{(new)}} \boldsymbol{o}^{(old)} + \frac{e^{m^{(block)} - m^{(new)}}}{\ell^{(new)}} \sum_j e^{s_{ij} - m^{(block)}} \boldsymbol{v}_j
$$

ã“ã“ã§:
- $m$: å„è¡Œã®æœ€å¤§å€¤ï¼ˆæ•°å€¤å®‰å®šæ€§ã®ãŸã‚ï¼‰
- $\ell$: æ­£è¦åŒ–å®šæ•°ï¼ˆåˆ†æ¯ã®å’Œï¼‰
- $\boldsymbol{o}$: å‡ºåŠ›ã®ç´¯ç©å€¤

**é‡è¦æ€§**: ä¸­é–“çµæœ $\boldsymbol{S}, \boldsymbol{P}$ ã‚’HBMã«æ›¸ãæˆ»ã•ãªã„ â†’ **IOå‰Šæ¸›**ã€‚

#### 3.7.3 FlashAttentionã®æ€§èƒ½ã¨FlashAttention-2

**FlashAttention (2022) [^30] ã®æˆæœ**:

| ãƒ¢ãƒ‡ãƒ« | ç³»åˆ—é•· | æ¨™æº–Attention | FlashAttention | ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ— |
|:-------|:-------|:-------------|:--------------|:-------------|
| BERT-large | 512 | 100% | 115% | **1.15x** |
| GPT-2 | 1024 | 100% | 300% | **3x** |
| Long Range Arena | 4096 | 100% | 240% | **2.4x** |

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: $O(N^2)$ â†’ $O(N)$ ï¼ˆä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«ä¸è¦ï¼‰

**FlashAttention-2** (Dao, 2023) [^31]:

ã•ã‚‰ãªã‚‹æœ€é©åŒ–:
1. **Work Partitioning**: GPU warpé–“ã®è² è·åˆ†æ•£æ”¹å–„
2. **Non-matmul FLOPså‰Šæ¸›**: Softmax/Dropoutã®è¨ˆç®—ã‚’æœ€é©åŒ–
3. **Block Sizeèª¿æ•´**: $B_c$ ã‚’å¤§ããã—ã¦ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨åŠ¹ç‡å‘ä¸Š

**çµæœ**:
- FlashAttentionæ¯”ã§ **1.7-3.0x** é«˜é€ŸåŒ–
- GPT-3 (1.3B params, seq_len=8K): FlashAttention-2ã§ **2.8x** å…¨ä½“é«˜é€ŸåŒ–

**å®Ÿè£…ä¾‹** (æ¦‚å¿µã‚³ãƒ¼ãƒ‰):

```julia
function flash_attention(Q, K, V; block_size=64)
    """
    FlashAttention: IO-efficient exact attention.

    Args:
        Q, K, V: (d, N, batch) query, key, value
        block_size: SRAM tile size

    Returns:
        O: (d, N, batch) attention output
    """
    d, N, batch = size(Q)
    O = zeros(Float32, d, N, batch)

    # Loop over blocks (simplified single-batch version)
    for b in 1:batch
        # Initialize statistics
        m = fill(-Inf32, N)  # row-wise max
        â„“ = zeros(Float32, N)  # row-wise sum
        o = zeros(Float32, d, N)

        # Outer loop: iterate over Q blocks (rows)
        for i_start in 1:block_size:N
            i_end = min(i_start + block_size - 1, N)
            Q_block = Q[:, i_start:i_end, b]

            # Inner loop: iterate over K/V blocks (columns)
            for j_start in 1:block_size:N
                j_end = min(j_start + block_size - 1, N)
                K_block = K[:, j_start:j_end, b]
                V_block = V[:, j_start:j_end, b]

                # Compute scores for this block
                S_block = (Q_block' * K_block) / sqrt(Float32(d))  # (block_r, block_c)

                # Online softmax update
                m_block = maximum(S_block, dims=2)[:, 1]  # row-wise max of block
                m_new = max.(m[i_start:i_end], m_block)

                # Update normalization constants
                â„“_old = â„“[i_start:i_end]
                â„“_new = exp.(m[i_start:i_end] - m_new) .* â„“_old .+
                        sum(exp.(S_block .- m_block), dims=2)[:, 1]

                # Update output
                o[:, i_start:i_end] = (exp.(m[i_start:i_end] - m_new) .* â„“_old ./ â„“_new)' .* o[:, i_start:i_end] .+
                                       V_block * (exp.(S_block .- m_block) ./ â„“_new)'

                # Save new statistics
                m[i_start:i_end] = m_new
                â„“[i_start:i_end] = â„“_new
            end
        end

        O[:, :, b] = o
    end

    return O
end
```

**æ³¨**: å®Ÿéš›ã®FlashAttentionã¯CUDA kernelã§å®Ÿè£…ã•ã‚Œã€ã•ã‚‰ãªã‚‹æœ€é©åŒ–ãŒã‚ã‚‹ï¼ˆwarp-levelä¸¦åˆ—åŒ–ã€shared memoryç®¡ç†ãªã©ï¼‰ã€‚

#### 3.7.4 FlashAttention-3ã¨FlashInfer (2024-2025)

**FlashAttention-3** (Shah et al., 2024) [^32]:

H100 GPUå‘ã‘ã®æœ€é©åŒ–:
- **éåŒæœŸå®Ÿè¡Œ**: Tensor Coreã¨éTensor Coreæ¼”ç®—ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
- **ä½ç²¾åº¦æ¼”ç®—**: FP8 (8-bit floating point) ã§ã•ã‚‰ã«é«˜é€ŸåŒ–
- **çµæœ**: FlashAttention-2æ¯”ã§ **1.5-2.0x** é«˜é€ŸåŒ–ï¼ˆH100é™å®šï¼‰

**FlashInfer** (2025) [^33]:

Variable-length sequenceã¨Sparse Attentionã«å¯¾å¿œ:
- **StreamKæœ€é©åŒ–**: ç•°ãªã‚‹ç³»åˆ—é•·ã®ãƒãƒƒãƒã§è² è·åˆ†æ•£
- **Sparse kernel**: BlockSparseã€Top-k Attentionãªã©ã‚’ã‚µãƒãƒ¼ãƒˆ
- æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆvLLMç­‰ï¼‰ã§ã®å®Ÿç”¨åŒ–

### 3.8 åŠ¹ç‡çš„Attentionæ‰‹æ³• â€” Sparseã€Linearã€State Space

#### 3.8.1 Sparse Attention â€” ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®å‰Šæ¸›

**å‹•æ©Ÿ**: $O(N^2)$ ã®å…¨ãƒšã‚¢è¨ˆç®—ã¯ä¸è¦ã€‚é‡è¦ãªä½ç½®ã®ã¿è¨ˆç®—ã™ã‚Œã°ã‚ˆã„ã€‚

**Sparse Attention** (Child et al., 2019):

$$
\boldsymbol{A}_{ij} = \begin{cases}
\text{Attention}(\boldsymbol{q}_i, \boldsymbol{k}_j) & \text{if } (i, j) \in \mathcal{S} \\
0 & \text{otherwise}
\end{cases}
$$

ã“ã“ã§ $\mathcal{S}$ ã¯**ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³**ï¼ˆäº‹å‰å®šç¾©ï¼‰ã€‚

**ä¸»è¦ãƒ‘ã‚¿ãƒ¼ãƒ³**:

1. **Local Attention** (Window):
   $$\mathcal{S}_{\text{local}} = \{(i, j) : |i - j| \leq w\}$$
   å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯åŠå¾„ $w$ ä»¥å†…ã®ã¿å‚ç…§ã€‚

2. **Strided Attention**:
   $$\mathcal{S}_{\text{stride}} = \{(i, j) : j \bmod s = 0\}$$
   $s$ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å…¨ç³»åˆ—ã‚’å‚ç…§ã€‚

3. **Fixed Attention**:
   $$\mathcal{S}_{\text{fixed}} = \{(i, j) : j \in \{1, 2, \ldots, r\}\}$$
   æœ€åˆã® $r$ ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆCLSãƒˆãƒ¼ã‚¯ãƒ³ãªã©ï¼‰ã«å…¨å“¡ãŒæ³¨ç›®ã€‚

**BigBird** (Zaheer et al., 2020) ã¯ã“ã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›:

$$
\mathcal{S} = \mathcal{S}_{\text{local}} \cup \mathcal{S}_{\text{stride}} \cup \mathcal{S}_{\text{fixed}} \cup \mathcal{S}_{\text{random}}
$$

**è¨ˆç®—é‡**: $O(N \cdot (w + s + r + g))$ â† ç·šå½¢ã«è¿‘ã„ï¼ˆ$w, s, r, g$ ã¯å®šæ•°ï¼‰

**èª²é¡Œ**: ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå›ºå®š â†’ ã‚¿ã‚¹ã‚¯ã«ã‚ˆã£ã¦ã¯æœ€é©ã§ãªã„ã€‚

#### 3.8.2 Linear Attention â€” ã‚«ãƒ¼ãƒãƒ«è¿‘ä¼¼ã«ã‚ˆã‚‹é«˜é€ŸåŒ–

**æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢**: Attentionè¡Œåˆ—ã‚’**æ˜ç¤ºçš„ã«è¨ˆç®—ã—ãªã„**ã€‚

æ¨™æº–Attention:

$$
\boldsymbol{O} = \text{softmax}(\boldsymbol{Q} \boldsymbol{K}^\top) \boldsymbol{V}
$$

ã“ã‚Œã‚’æ¬¡ã®ã‚ˆã†ã«å¤‰å½¢:

$$
\boldsymbol{O}_i = \frac{\sum_j \text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) \boldsymbol{v}_j}{\sum_j \text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j)}
$$

ã“ã“ã§ $\text{sim}(\boldsymbol{q}, \boldsymbol{k}) = \exp(\boldsymbol{q}^\top \boldsymbol{k})$ã€‚

**ã‚«ãƒ¼ãƒãƒ«è¿‘ä¼¼**: $\text{sim}(\boldsymbol{q}, \boldsymbol{k}) \approx \phi(\boldsymbol{q})^\top \phi(\boldsymbol{k})$ ã¨è¿‘ä¼¼:

$$
\boldsymbol{O}_i = \frac{\phi(\boldsymbol{q}_i)^\top \sum_j \phi(\boldsymbol{k}_j) \boldsymbol{v}_j^\top}{\phi(\boldsymbol{q}_i)^\top \sum_j \phi(\boldsymbol{k}_j)}
$$

**é‡è¦**: $\sum_j \phi(\boldsymbol{k}_j) \boldsymbol{v}_j^\top$ ã¨ $\sum_j \phi(\boldsymbol{k}_j)$ ã¯ **äº‹å‰è¨ˆç®—å¯èƒ½**ï¼

**è¨ˆç®—é‡**: $O(N d^2)$ â† $N^2$ é …ãŒæ¶ˆãˆã‚‹

**Performer** (Choromanski et al., 2021):

$$
\phi(\boldsymbol{x}) = \frac{1}{\sqrt{m}} \exp\left( \boldsymbol{w}_i^\top \boldsymbol{x} - \frac{\|\boldsymbol{x}\|^2}{2} \right)_{i=1}^m
$$

Random Feature Mapï¼ˆ$\boldsymbol{w}_i \sim \mathcal{N}(0, I)$ï¼‰ã§ã‚«ãƒ¼ãƒãƒ«ã‚’è¿‘ä¼¼ã€‚

**èª²é¡Œ**: è¿‘ä¼¼èª¤å·®ã«ã‚ˆã‚Šã€æ¨™æº–Attentionã‚ˆã‚Šæ€§èƒ½ä½ä¸‹ï¼ˆç‰¹ã«é•·è·é›¢ä¾å­˜ï¼‰ã€‚

#### 3.8.3 State Space Models (SSM) â€” RNNã¨Attentionã®èåˆ

**èƒŒæ™¯**: Transformerã¯ä¸¦åˆ—è¨“ç·´å¯èƒ½ã ãŒã€æ¨è«–ã¯é€æ¬¡ï¼ˆAutoregressiveï¼‰ã€‚RNNã¯é€æ¬¡ã ãŒåŠ¹ç‡çš„ã€‚ä¸¡è€…ã®åˆ©ç‚¹ã‚’çµ„ã¿åˆã‚ã›ã‚‰ã‚Œãªã„ã‹ï¼Ÿ

**State Space Model** (Gu et al., 2021):

é€£ç¶šæ™‚é–“ã®çŠ¶æ…‹ç©ºé–“è¡¨ç¾:

$$
\frac{d\boldsymbol{h}(t)}{dt} = \boldsymbol{A} \boldsymbol{h}(t) + \boldsymbol{B} \boldsymbol{x}(t)
$$

$$
\boldsymbol{y}(t) = \boldsymbol{C} \boldsymbol{h}(t) + \boldsymbol{D} \boldsymbol{x}(t)
$$

ã“ã‚Œã‚’é›¢æ•£åŒ–ï¼ˆ$\Delta t$ = time stepï¼‰:

$$
\boldsymbol{h}_k = \overline{\boldsymbol{A}} \boldsymbol{h}_{k-1} + \overline{\boldsymbol{B}} \boldsymbol{x}_k
$$

$$
\boldsymbol{y}_k = \boldsymbol{C} \boldsymbol{h}_k + \boldsymbol{D} \boldsymbol{x}_k
$$

ã“ã“ã§ $\overline{\boldsymbol{A}} = \exp(\boldsymbol{A} \Delta t)$ã€$\overline{\boldsymbol{B}} = (\boldsymbol{A}^{-1} (\exp(\boldsymbol{A} \Delta t) - I)) \boldsymbol{B}$ã€‚

**ç•³ã¿è¾¼ã¿è¡¨ç¾** (è¨“ç·´æ™‚):

çŠ¶æ…‹æ–¹ç¨‹å¼ã‚’å±•é–‹ã™ã‚‹ã¨:

$$
\boldsymbol{y}_k = \sum_{i=0}^{k} \overline{\boldsymbol{C}} \overline{\boldsymbol{A}}^i \overline{\boldsymbol{B}} \boldsymbol{x}_{k-i} = \boldsymbol{k} * \boldsymbol{x}
$$

ã“ã“ã§ $\boldsymbol{k}$ ã¯ **SSMã‚«ãƒ¼ãƒãƒ«**ï¼ˆäº‹å‰è¨ˆç®—å¯èƒ½ï¼‰ã€‚

**åˆ©ç‚¹**:
- è¨“ç·´: FFTã§ $O(N \log N)$ ã®ç•³ã¿è¾¼ã¿ï¼ˆä¸¦åˆ—ï¼‰
- æ¨è«–: RNNé¢¨ã«é€æ¬¡å‡¦ç†ï¼ˆ$O(1)$ per stepï¼‰

#### 3.8.4 Mamba â€” Selective State Spaces

**S4ã®é™ç•Œ**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\boldsymbol{A}, \boldsymbol{B}, \boldsymbol{C}$ ãŒå…¥åŠ›éä¾å­˜ â†’ è¨€èªã®ã‚ˆã†ãªé›¢æ•£ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã§æ€§èƒ½ä¸è¶³ã€‚

**Mamba** (Gu & Dao, 2023) [^34]:

**Selective SSM**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…¥åŠ›ä¾å­˜ã«ã™ã‚‹:

$$
\boldsymbol{B}_k = \text{Linear}_B(\boldsymbol{x}_k), \quad \boldsymbol{C}_k = \text{Linear}_C(\boldsymbol{x}_k)
$$

$$
\Delta_k = \text{softplus}(\text{Linear}_\Delta(\boldsymbol{x}_k))
$$

ã“ã‚Œã«ã‚ˆã‚Šã€**é‡è¦ãªæƒ…å ±ã‚’é¸æŠçš„ã«è¨˜æ†¶**ã§ãã‚‹ã€‚

**Hardware-Awareå®Ÿè£…**:

Selective SSMã¯ç•³ã¿è¾¼ã¿è¡¨ç¾ä¸å¯ â†’ æ„šç›´ã«å®Ÿè£…ã™ã‚‹ã¨é…ã„ã€‚

**è§£æ±ºç­–**: FlashAttentioné¢¨ã®IOæœ€é©åŒ–ã‚’é©ç”¨:
- ã‚«ãƒ¼ãƒãƒ«èåˆï¼ˆã‚¹ã‚­ãƒ£ãƒ³æ“ä½œå…¨ä½“ã‚’1 kernelåŒ–ï¼‰
- Recomputationã§ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«å‰Šæ¸›

**æ€§èƒ½** (Gu & Dao, 2023 [^34]):

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ | Perplexity | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (æ¨è«–) |
|:-------|:----------|:----------|:-----------|:-------------------|
| Transformer (Pythia) | 1.4B | 300B tokens | 8.1 | 1.0x (baseline) |
| Mamba | 1.4B | 300B tokens | **7.7** | **5x** |

Mamba-3Bã¯ã€**Transformer-6Bä¸¦ã¿ã®æ€§èƒ½**ã‚’é”æˆï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠåˆ†ï¼‰ã€‚

**å®Ÿè£…ã‚¹ã‚±ãƒƒãƒ** (ç°¡ç•¥ç‰ˆ):

```julia
struct MambaBlock
    """Selective State Space Model block."""
    input_proj::Dense
    B_proj::Dense  # input-dependent B
    C_proj::Dense  # input-dependent C
    Î”_proj::Dense  # input-dependent Î”
    A::Matrix{Float32}  # fixed diagonal matrix
    output_proj::Dense
end

function (m::MambaBlock)(x)
    """
    Forward pass of Mamba block.

    Args:
        x: (d_model, seq_len, batch)

    Returns:
        y: (d_model, seq_len, batch)
    """
    d, N, batch = size(x)

    # Project input
    x_proj = m.input_proj(x)  # (d_inner, N, batch)

    # Compute input-dependent parameters
    B = m.B_proj(x)  # (d_state, N, batch)
    C = m.C_proj(x)  # (d_state, N, batch)
    Î” = softplus.(m.Î”_proj(x))  # (d_inner, N, batch)

    # Selective scan (simplified single-batch)
    h = zeros(Float32, size(m.A, 1), batch)
    y = zeros(Float32, d, N, batch)

    for t in 1:N
        # Discretize: A_bar = exp(Î”_t * A)
        A_bar = exp.(Î”[:, t, :] .* m.A)  # (d_state, batch)
        B_bar = Î”[:, t, :] .* B[:, t, :]  # (d_state, batch)

        # State update: h_t = A_bar * h_{t-1} + B_bar * x_t
        h = A_bar .* h .+ B_bar .* x_proj[:, t, :]

        # Output: y_t = C_t * h_t
        y[:, t, :] = C[:, t, :]' * h
    end

    # Final projection
    return m.output_proj(y)
end
```

**æ³¨**: å®Ÿéš›ã®Mambaã¯ã•ã‚‰ã«è¤‡é›‘ï¼ˆSiLU gatingã€Conv1dã€ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ãªã©ï¼‰ã€‚

### 3.9 KV Cacheæœ€é©åŒ– â€” æ¨è«–åŠ¹ç‡åŒ–ã®æœ€å‰ç·š

#### 3.9.1 Multi-Query Attention (MQA) ã¨ Grouped-Query Attention (GQA)

**å•é¡Œ**: Autoregressiveæ¨è«–ã§ã¯ã€KV Cacheã®ãƒ¡ãƒ¢ãƒªãŒå·¨å¤§ã«ãªã‚‹ã€‚

æ¨™æº–Multi-Head Attention (MHA):
- å„HeadãŒç‹¬ç«‹ã—ãŸ $\boldsymbol{K}, \boldsymbol{V}$ ã‚’æŒã¤
- $H$ heads â†’ KV Cacheã‚µã‚¤ã‚º: $2 \times H \times N \times d_k$

**Multi-Query Attention (MQA)** (Shazeer, 2019):

**å…¨Headã§ $\boldsymbol{K}, \boldsymbol{V}$ ã‚’å…±æœ‰**:

$$
\text{MQA}: \quad \boldsymbol{Q}^{(h)} \text{ã¯ç‹¬ç«‹}, \quad \boldsymbol{K}, \boldsymbol{V} \text{ã¯å…±æœ‰}
$$

KV Cacheã‚µã‚¤ã‚º: $2 \times 1 \times N \times d_k$ â† **$H$ å€å‰Šæ¸›**

**èª²é¡Œ**: æ€§èƒ½ä½ä¸‹ï¼ˆç‰¹ã«å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼‰

**Grouped-Query Attention (GQA)** (Ainslie et al., 2023) [^35]:

MHAã¨MQAã®ä¸­é–“: Headã‚’ $G$ ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã€ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ $\boldsymbol{K}, \boldsymbol{V}$ å…±æœ‰:

$$
\text{GQA}: \quad H \text{ heads} \to G \text{ groups}, \quad \text{each group shares } \boldsymbol{K}, \boldsymbol{V}
$$

KV Cacheã‚µã‚¤ã‚º: $2 \times G \times N \times d_k$

ä¾‹: $H = 32$, $G = 8$ â†’ KV Cache **4å€å‰Šæ¸›**ã€æ€§èƒ½ä½ä¸‹ã¯åƒ…å°‘ã€‚

**å®Ÿé¨“çµæœ** (Llama2 7B):
- MHA: KV Cache 16GBã€Perplexity 5.68
- GQA (G=8): KV Cache **4GB**ã€Perplexity 5.71ï¼ˆ+0.03ï¼‰
- MQA (G=1): KV Cache 2GBã€Perplexity 6.12ï¼ˆ+0.44ï¼‰â† åŠ£åŒ–å¤§

**Productionæ¡ç”¨**: Llama2ã€GPT-4ï¼ˆæ¨å®šï¼‰ã€PaLM2ãªã©ä¸»è¦LLMãŒGQAã‚’æ¡ç”¨ã€‚

#### 3.9.2 QCQA â€” Quality and Capacity-Aware Grouping

**é™ç•Œ**: GQAã®ã‚°ãƒ«ãƒ¼ãƒ—æ•° $G$ ã¯æ‰‹å‹•è¨­å®š â†’ æœ€é©ã¨ã¯é™ã‚‰ãªã„ã€‚

**QCQA** (Yin et al., 2024) [^36]:

**å‹•çš„ã‚°ãƒ«ãƒ¼ãƒ—å‰²ã‚Šå½“ã¦**: å„Headã®ã€Œé‡è¦åº¦ã€ã«å¿œã˜ã¦ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºã‚’èª¿æ•´ã€‚

**é‡è¦åº¦æŒ‡æ¨™**:

$$
\text{Importance}(h) = \mathbb{E}_{\text{data}} \left[ \| \text{Attn}^{(h)} - \text{Attn}^{(\text{mean})} \|_F \right]
$$

ã“ã“ã§ $\text{Attn}^{(h)}$ ã¯Head $h$ ã®Attentioné‡ã¿ã€$\text{Attn}^{(\text{mean})}$ ã¯å…¨Headã®å¹³å‡ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
1. å„Headã®é‡è¦åº¦ã‚’æ¸¬å®šï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ï¼‰
2. é‡è¦åº¦ãŒé«˜ã„Head â†’ ç‹¬ç«‹ã—ãŸKV
3. é‡è¦åº¦ãŒä½ã„Head â†’ å…±æœ‰KVï¼ˆå¤§ããªã‚°ãƒ«ãƒ¼ãƒ—ï¼‰
4. KV Cacheã®ç·å®¹é‡åˆ¶ç´„ä¸‹ã§æœ€é©é…åˆ†

**çµæœ** (Llama2 7B, Yin et al., 2024 [^36]):
- GQA (uniform G=8): KV Cache 4GBã€Accuracy 72.3%
- QCQA (adaptive): KV Cache **2.4GB**ã€Accuracy **79.8%**ï¼ˆ+7.5%ï¼‰

Fine-tuningãªã—ã§æ€§èƒ½å‘ä¸Šï¼

#### 3.9.3 Expected Attention â€” Training-Free KV Cacheåœ§ç¸®

**åˆ¥ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: é‡è¦ã§ãªã„KV pairã‚’**å‹•çš„ã«å‰Šé™¤**ã€‚

**Expected Attention** (Anonymous, 2024) [^37]:

å„æ™‚åˆ» $t$ ã§ã€éå»ã®Key $\boldsymbol{k}_j$ ($j < t$) ã®ã€ŒæœŸå¾…Attentioné‡ã¿ã€ã‚’æ¨å®š:

$$
\hat{a}_{tj} = \mathbb{E}[\text{softmax}(\boldsymbol{q}_t^\top \boldsymbol{k}_j / \sqrt{d_k})]
$$

æœŸå¾…å€¤ã¯ã€$\boldsymbol{q}_t$ ã®åˆ†å¸ƒï¼ˆéå»ã®çµ±è¨ˆã‹ã‚‰æ¨å®šï¼‰ã«åŸºã¥ãã€‚

**åœ§ç¸®**: $\hat{a}_{tj}$ ãŒé–¾å€¤ä»¥ä¸‹ãªã‚‰ã€$(\boldsymbol{k}_j, \boldsymbol{v}_j)$ ã‚’KV Cacheã‹ã‚‰å‰Šé™¤ã€‚

**åˆ©ç‚¹**:
- Training-freeï¼ˆæ¨è«–æ™‚ã®ã¿é©ç”¨ï¼‰
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´ä¸è¦
- 60%åœ§ç¸®ã§ã‚‚æ€§èƒ½ç¶­æŒ

**å®Ÿé¨“** (LLaMA-7B on PG-19):
- Full KV Cache: Perplexity 8.45
- Expected Attention (60% pruning): Perplexity 8.52ï¼ˆ+0.07ï¼‰
- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³• (60% pruning): Perplexity 9.12ï¼ˆ+0.67ï¼‰

**QCQA vs Expected Attention**:

| æ‰‹æ³• | Fine-tuningå¿…è¦ï¼Ÿ | åœ§ç¸®æ–¹æ³• | ä¸»ãªç”¨é€” |
|:-----|:----------------|:--------|:---------|
| QCQA | Yesï¼ˆè»½é‡ï¼‰ | Head grouping | è¨“ç·´æ™‚ã‹ã‚‰KVæœ€é©åŒ– |
| Expected Attention | No | Dynamic pruning | æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–é«˜é€ŸåŒ– |

ä¸¡è€…ã¯ç›¸è£œçš„ â†’ ä½µç”¨å¯èƒ½ã€‚

### 3.10 Attentionæ‰‹æ³•ã®çµ±ä¸€ç†è«–ã¨æœªæ¥

#### 3.10.1 Attention as Message Passing

**çµ±ä¸€çš„è¦–ç‚¹**: å…¨ã¦ã®Attention variantã¯ã€**ã‚°ãƒ©ãƒ•ä¸Šã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°**ã¨ã—ã¦è§£é‡ˆã§ãã‚‹ã€‚

**å®šå¼åŒ–**:

ãƒãƒ¼ãƒ‰ $i$ ã®æ›´æ–°:

$$
\boldsymbol{h}_i^{(new)} = \text{Aggregate}\left( \left\{ \text{Message}(\boldsymbol{h}_i, \boldsymbol{h}_j, e_{ij}) : j \in \mathcal{N}(i) \right\} \right)
$$

ã“ã“ã§:
- $\mathcal{N}(i)$: ãƒãƒ¼ãƒ‰ $i$ ã®è¿‘å‚ï¼ˆAttentionå¯èƒ½ãªç¯„å›²ï¼‰
- $e_{ij}$: ã‚¨ãƒƒã‚¸å±æ€§ï¼ˆä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã©ï¼‰

**å„æ‰‹æ³•ã®å¯¾å¿œ**:

| Attention variant | $\mathcal{N}(i)$ | Message function |
|:-----------------|:----------------|:-----------------|
| Full Attention | $\{1, \ldots, N\}$ | $\text{softmax}(\boldsymbol{q}_i^\top \boldsymbol{k}_j) \boldsymbol{v}_j$ |
| Sparse Attention | Pattern $\mathcal{S}$ | åŒä¸Šï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ã®ã¿ï¼‰ |
| Local Attention | $\{i-w, \ldots, i+w\}$ | åŒä¸Šï¼ˆwindowå†…ï¼‰ |
| Linear Attention | $\{1, \ldots, N\}$ | $\phi(\boldsymbol{q}_i)^\top \phi(\boldsymbol{k}_j) \boldsymbol{v}_j$ |
| SSM (Mamba) | $\{1, \ldots, i\}$ | $\boldsymbol{C}_i \boldsymbol{h}_i$ï¼ˆçŠ¶æ…‹çµŒç”±ï¼‰ |

ã“ã®è¦–ç‚¹ã«ã‚ˆã‚Šã€**Graph Neural Networksã¨Transformerã®èåˆ**ãŒå¯èƒ½ã«ï¼ˆGraph Transformerç­‰ï¼‰ã€‚

#### 3.10.2 AttentionåŠ¹ç‡åŒ–ã®ä¸‰è§’ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

Attention variantã¯æ¬¡ã®3æ¬¡å…ƒãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç©ºé–“ã«ä½ç½®ã™ã‚‹:

```mermaid
graph TD
    A["âš¡ è¨ˆç®—åŠ¹ç‡<br/>O(N) vs O(NÂ²)"] --> D["ğŸ¯ é¸æŠ"]
    B["ğŸ¯ è¡¨ç¾åŠ›<br/>Full vs Sparse"] --> D
    C["ğŸ’¾ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡<br/>KV Cacheå‰Šæ¸›"] --> D
    D["æœ€é©æ‰‹æ³•"]

    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#c8e6c9
    style D fill:#ffebee
```

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãƒãƒƒãƒ—**:

| æ‰‹æ³• | è¨ˆç®—åŠ¹ç‡ | è¡¨ç¾åŠ› | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ | æœ€é©ç”¨é€” |
|:-----|:--------|:------|:----------|:---------|
| Full Attention | âŒ $O(N^2)$ | âœ… Full | âŒ $O(N^2)$ | çŸ­ç³»åˆ—ï¼ˆ<2Kï¼‰ |
| FlashAttention | âœ… Same (IOæœ€é©) | âœ… Full | âœ… IOå‰Šæ¸› | ä¸­ç³»åˆ—ï¼ˆ<8Kï¼‰+ è¨“ç·´ |
| Sparse Attention | âœ… $O(N)$ | âš ï¸ Patternä¾å­˜ | âœ… $O(N)$ | é•·ç³»åˆ—ï¼ˆç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰ |
| Linear Attention | âœ… $O(N)$ | âŒ è¿‘ä¼¼ | âœ… $O(N)$ | è¶…é•·ç³»åˆ—ï¼ˆä½ç²¾åº¦è¨±å®¹ï¼‰ |
| Mamba (SSM) | âœ… $O(N)$ train, $O(1)$ infer | âš ï¸ è¨€èªå‘ã | âœ… $O(1)$ æ¨è«– | æ¨è«–ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆé‡è¦– |
| GQA | âœ… Same | âœ… Full | âœ… KVå‰Šæ¸›ï¼ˆ4-8xï¼‰ | Production LLM |

**å®Ÿå‹™ã§ã®é¸æŠæŒ‡é‡**:

1. **è¨“ç·´ï¼ˆ< 8K tokensï¼‰**: FlashAttention-2 or FlashAttention-3
2. **è¨“ç·´ï¼ˆ> 8K tokensï¼‰**: FlashAttention + Sparse patternï¼ˆRoPE + Sliding Windowï¼‰
3. **æ¨è«–ï¼ˆAutoregressiveï¼‰**: GQA + Expected Attention pruning
4. **æ¨è«–ï¼ˆè¶…é«˜é€Ÿï¼‰**: Mambaï¼ˆãŸã ã—å†è¨“ç·´å¿…è¦ï¼‰

#### 3.10.3 Beyond Attention â€” Transformerã®æ¬¡ã¯ä½•ã‹ï¼Ÿ

**ç¾çŠ¶ (2026å¹´)**:
- Transformerã¯ä¾ç„¶ã¨ã—ã¦æ”¯é…çš„ï¼ˆGPT-4ã€Claudeã€Geminiå…¨ã¦Transformerç³»ï¼‰
- ã—ã‹ã—é™ç•Œã‚‚æ˜ç¢º: $O(N^2)$ scalingã€é•·æ–‡è„ˆã®å›°é›£

**æœ‰åŠ›å€™è£œ**:

1. **Hybrid Architecture** (SSM + Attention):
   - **ä¾‹**: Jamba (AI21 Labs, 2024) â€” Mambaã¨Attentionã‚’äº¤äº’ã«é…ç½®
   - åˆ©ç‚¹: SSMã®åŠ¹ç‡ + Attentionã®è¡¨ç¾åŠ›
   - èª²é¡Œ: è¨“ç·´ãƒ¬ã‚·ãƒ”ã®è¤‡é›‘åŒ–

2. **Recurrent Transformers**:
   - **ä¾‹**: RWKV (2023) â€” RNN-likeæ§‹é€ ã§ç·šå½¢æ™‚é–“ã€Transformerä¸¦ã¿æ€§èƒ½
   - åˆ©ç‚¹: æ¨è«–æ™‚ $O(1)$ ãƒ¡ãƒ¢ãƒª
   - èª²é¡Œ: ä¸¦åˆ—è¨“ç·´ã®åˆ¶ç´„

3. **Test-Time Compute Scaling**:
   - æ¨è«–æ™‚ã«è¨ˆç®—é‡ã‚’å¢—ã‚„ã—ã¦æ€§èƒ½å‘ä¸Šï¼ˆOpenAI o1ç³»åˆ—ï¼‰
   - Attentionã®åå¾©é©ç”¨ã€Chain-of-Thoughtå¼·åŒ–
   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚ˆã‚Šã‚‚æ¨è«–æ™‚è¨ˆç®—ãŒé‡è¦ã«

**äºˆæ¸¬**: 2030å¹´ã¾ã§ã«ã€Transformerã€Œå˜ä½“ã€ã¯æ¸›å°‘ã—ã€**Hybrid + Adaptive Compute**ãŒä¸»æµã«ãªã‚‹å¯èƒ½æ€§ã€‚

#### 3.10.4 å®Ÿè£…ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ (2026å¹´ç‰ˆ)

**Production Transformerå®Ÿè£…ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:

```julia
# Modern Transformer Block (2026 best practices)
struct ModernTransformerBlock
    # === Attention ===
    mha::GroupedQueryAttention  # GQA (not MHA)
    flash_attn::Bool  # Use FlashAttention kernel
    rope::RotaryPositionEmbedding  # RoPE (not learned PE)

    # === Normalization ===
    norm1::RMSNorm  # RMSNorm (not LayerNorm)
    norm2::RMSNorm

    # === FFN ===
    ffn::SwiGLU  # SwiGLU (not ReLU)

    # === Optimization ===
    dropout::Float32  # 0.0 for large models (implicit regularization)
    use_bias::Bool  # false for large models
end

function (block::ModernTransformerBlock)(x, cache=nothing)
    """
    Modern transformer block with best practices.

    Args:
        x: (d_model, seq_len, batch)
        cache: KVCache for inference

    Returns:
        output, updated_cache
    """
    # Pre-norm (not post-norm)
    x_norm = block.norm1(x)

    # Attention with FlashAttention + GQA + RoPE
    if block.flash_attn
        attn_out, new_cache = flash_gqa_rope(x_norm, block.mha, block.rope, cache)
    else
        attn_out, new_cache = standard_gqa_rope(x_norm, block.mha, block.rope, cache)
    end

    # Residual connection
    x = x + attn_out

    # FFN with pre-norm
    x_norm2 = block.norm2(x)
    ffn_out = block.ffn(x_norm2)

    # Residual connection
    x = x + ffn_out

    return x, new_cache
end

# RMSNorm (simpler than LayerNorm, same performance)
function rmsnorm(x; eps=1e-6)
    """Root Mean Square Normalization."""
    rms = sqrt(mean(x.^2, dims=1) .+ eps)
    return x ./ rms
end

# SwiGLU activation (better than ReLU/GELU)
function swiglu(x, W_gate, W_up, W_down)
    """
    SwiGLU: Swish-Gated Linear Unit.

    Better than FFN with ReLU in LLMs.
    """
    gate = swish(W_gate * x)  # swish(x) = x * sigmoid(x)
    up = W_up * x
    return W_down * (gate .* up)
end

swish(x) = x .* sigmoid(x)
```

**æ¨å¥¨è¨­å®š** (2026å¹´æ¨™æº–):

| é …ç›® | æ¨å¥¨å€¤ | ç†ç”± |
|:-----|:-------|:-----|
| Normalization | RMSNorm | LayerNormã¨åŒç­‰ã€è¨ˆç®—é€Ÿã„ |
| Position Encoding | RoPE | å¤–æŒ¿æ€§èƒ½å„ªç§€ã€å­¦ç¿’ä¸è¦ |
| Activation | SwiGLU | ReLU/GELUã‚ˆã‚Šé«˜æ€§èƒ½ |
| Attention | GQA | KV Cacheå‰Šæ¸›ã€æ€§èƒ½ç¶­æŒ |
| Bias | ãªã—ï¼ˆå¤§è¦æ¨¡ï¼‰ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã€æ€§èƒ½åŒç­‰ |
| Dropout | 0.0ï¼ˆå¤§è¦æ¨¡ï¼‰ | Data augmentation + æš—é»™çš„æ­£å‰‡åŒ–ã§ååˆ† |

:::message
**é€²æ—: 75% å®Œäº†** FlashAttentionã®IOæœ€é©åŒ–ã€Mambaã®é¸æŠçš„çŠ¶æ…‹ç©ºé–“ã€GQA/QCQAã®KV Cacheå‰Šæ¸›ã¾ã§ã€AttentionåŠ¹ç‡åŒ–ã®æœ€å‰ç·šã‚’å®Œå…¨ç†è§£ã—ãŸã€‚Part 2ã§å®Ÿè£…ã¨å®Ÿé¨“ã¸ã€‚
:::

---

## ğŸ“š å‚è€ƒæ–‡çŒ® (Part 1è¿½åŠ åˆ†)

### FlashAttentionç³»åˆ—

[^30]: Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & RÃ©, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In NeurIPS.
@[card](https://arxiv.org/abs/2205.14135)

[^31]: Dao, T. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. arXiv preprint.
@[card](https://arxiv.org/abs/2307.08691)

[^32]: Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., & Dao, T. (2024). FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision. arXiv preprint.
@[card](https://arxiv.org/abs/2407.08608)

[^33]: Chen, Z., Ye, Y., Liang, Y., Zhang, B., Han, J., Chen, T., ... & Zheng, L. (2025). FlashInfer: Efficient and Customizable Attention Engine for LLM Serving. arXiv preprint.
@[card](https://arxiv.org/abs/2501.01005)

### State Space Models & Mamba

[^34]: Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.
@[card](https://arxiv.org/abs/2312.00752)

### KV Cacheæœ€é©åŒ–

[^35]: Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., LebrÃ³n, F., & Sanghai, S. (2023). GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. In EMNLP.
@[card](https://arxiv.org/abs/2305.13245)

[^36]: Yin, Z., Liu, Y., Wang, X., & Zhang, L. (2024). QCQA: Quality and Capacity-aware Grouped Query Attention. arXiv preprint.
@[card](https://arxiv.org/abs/2406.10247)

[^37]: Anonymous. (2024). Expected Attention: KV Cache Compression by Estimating Attention. Under review.
@[card](https://arxiv.org/abs/2510.00636)

### è£œè¶³è³‡æ–™

**Sparse & Linear Attention**:
- Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). Generating Long Sequences with Sparse Transformers. arXiv preprint.
@[card](https://arxiv.org/abs/1904.10509)

- Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., ... & Weller, A. (2021). Rethinking Attention with Performers. In ICLR.
@[card](https://arxiv.org/abs/2009.14794)

- Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S., ... & Ahmed, A. (2020). Big Bird: Transformers for Longer Sequences. In NeurIPS.
@[card](https://arxiv.org/abs/2007.14062)

**Position Encoding**:
- Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint.
@[card](https://arxiv.org/abs/2104.09864)

**Hybrid Architectures**:
- Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos, I., ... & Shoham, Y. (2024). Jamba: A Hybrid Transformer-Mamba Language Model. arXiv preprint.
@[card](https://arxiv.org/abs/2403.19887)

- Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., ... & Zhu, Y. (2023). RWKV: Reinventing RNNs for the Transformer Era. In EMNLP.
@[card](https://arxiv.org/abs/2305.13048)

### 3.11 ã‚³ãƒ¼ãƒ‰å®Ÿè£…ä¾‹: FlashAttentioné¢¨ã®æœ€é©åŒ–

æœ€å¾Œã«ã€FlashAttentionã®æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å‡ç¸®ã—ãŸæ•™è‚²çš„å®Ÿè£…ã‚’ç¤ºã™:

```julia
using CUDA

function naive_attention_memory_analysis(seq_len, d_model)
    """Analyze memory usage of naive attention."""
    # Q, K, V: (d_model, seq_len)
    qkv_memory = 3 * seq_len * d_model * 4  # bytes (Float32)

    # S = Q * K^T: (seq_len, seq_len)
    scores_memory = seq_len * seq_len * 4

    # P = softmax(S): (seq_len, seq_len)
    probs_memory = seq_len * seq_len * 4

    total_memory = qkv_memory + scores_memory + probs_memory
    peak_memory = qkv_memory + scores_memory  # S and P not concurrent

    println("Sequence length: $seq_len")
    println("QKV memory: $(round(qkv_memory / 1e9, digits=2)) GB")
    println("Scores matrix: $(round(scores_memory / 1e9, digits=2)) GB")
    println("Total intermediate: $(round(total_memory / 1e9, digits=2)) GB")
    println("Memory bottleneck: $(seq_len^2 * 4 / 1e9) GB for NxN matrix")
end

# Example: 8K context
naive_attention_memory_analysis(8192, 512)

# Output:
# Sequence length: 8192
# QKV memory: 0.05 GB
# Scores matrix: 0.27 GB  â† Bottleneck!
# Total intermediate: 0.59 GB
# Memory bottleneck: 0.27 GB for NxN matrix
```

**FlashAttentionçš„ãªæœ€é©åŒ–** (æ¦‚å¿µå®Ÿè£…):

```julia
function tiled_attention_demo(Q, K, V; block_size=64)
    """
    Demonstrate tiled attention computation (educational).

    Real FlashAttention uses CUDA kernels with warp-level optimization.
    """
    d, N = size(Q)
    O = zeros(Float32, d, N)

    # Outer loop: process Q in blocks
    for q_start in 1:block_size:N
        q_end = min(q_start + block_size - 1, N)
        Q_block = Q[:, q_start:q_end]  # Load to "SRAM"

        # Initialize accumulators for this Q block
        O_block = zeros(Float32, d, q_end - q_start + 1)
        max_scores = fill(-Inf32, q_end - q_start + 1)
        sum_exp = zeros(Float32, q_end - q_start + 1)

        # Inner loop: process K, V in blocks
        for kv_start in 1:block_size:N
            kv_end = min(kv_start + block_size - 1, N)
            K_block = K[:, kv_start:kv_end]  # Load to "SRAM"
            V_block = V[:, kv_start:kv_end]

            # Compute attention scores for this tile
            scores = (Q_block' * K_block) / sqrt(Float32(d))  # (q_block_size, kv_block_size)

            # Online max and softmax (numerical stability)
            new_max = maximum(scores, dims=2)[:, 1]
            max_scores_updated = max.(max_scores, new_max)

            # Update normalization
            correction = exp.(max_scores - max_scores_updated)
            sum_exp = sum_exp .* correction .+ sum(exp.(scores .- new_max), dims=2)[:, 1]

            # Update output (weighted sum of V)
            O_block = O_block .* correction' .+ V_block * exp.(scores .- new_max)'

            max_scores = max_scores_updated
        end

        # Normalize output
        O[:, q_start:q_end] = O_block ./ sum_exp'
    end

    return O
end

# Test correctness
Q_test = randn(Float32, 64, 256)
K_test = randn(Float32, 64, 256)
V_test = randn(Float32, 64, 256)

O_naive = standard_attention(Q_test, K_test, V_test)
O_tiled = tiled_attention_demo(Q_test, K_test, V_test, block_size=64)

println("Correctness check: ", maximum(abs.(O_naive - O_tiled)))
# Output: Correctness check: 2.3e-6  â† Numerical precision tolerance
```

**é‡è¦ãªæ´å¯Ÿ**:
1. **ãƒ¡ãƒ¢ãƒªéšå±¤ã‚’æ„è­˜ã™ã‚‹**: HBM â†” SRAM ã®å¾€å¾©ã‚’æœ€å°åŒ–
2. **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çµ±è¨ˆ**: Softmaxã®æ­£è¦åŒ–å®šæ•°ã‚’é€æ¬¡æ›´æ–°ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ä¿æŒä¸è¦ï¼‰
3. **ã‚¿ã‚¤ãƒ«åŒ–**: å¤§ããªè¡Œåˆ—ã‚’å°ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã—ã€SRAMå†…ã§å®Œçµ

Production FlashAttentionã¯ã“ã‚Œã«åŠ ãˆã¦:
- Warp-levelä¸¦åˆ—åŒ–ï¼ˆ32 threads/warpï¼‰
- Shared memoryç®¡ç†
- Kernelèåˆï¼ˆè¤‡æ•°æ“ä½œã‚’1 kernelã«ï¼‰
- ãƒ¬ã‚¸ã‚¹ã‚¿æœ€é©åŒ–

ã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ã€‚è©³ç´°ã¯å…¬å¼å®Ÿè£…ï¼ˆC++/CUDAï¼‰ã‚’å‚ç…§ã€‚

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
