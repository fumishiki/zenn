---
title: "ç¬¬18å›: Attention Ã— Mamba ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ”€"
type: "tech"
topics: ["machinelearning", "deeplearning", "attention", "mamba", "julia"]
published: true
slug: "ml-lecture-18-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

**â† Part1ï¼ˆç†è«–ç·¨ï¼‰**: [ç¬¬18å› Part1](./ml-lecture-18-part1)

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Julia/Rust Hybridå®Ÿè£…

### 4.1 Juliaå®Ÿè£…: Tiny Hybrid Modelè¨“ç·´

#### 4.1.1 å®Œå…¨ãªJamba-style Hybrid Model

Zone 3ã®Boss Battleã‚’ç™ºå±•ã•ã›ã€è¨“ç·´å¯èƒ½ãªTiny Hybrid Modelã‚’å®Ÿè£…ã™ã‚‹ã€‚

**ä»•æ§˜**:
- 8å±¤ (6 SSM + 2 Attention, 1:4æ¯”ç‡)
- 64-dim hidden
- MNIST 28Ã—28 â†’ flatten â†’ 784-dim input
- 10ã‚¯ãƒ©ã‚¹åˆ†é¡

```julia
using LinearAlgebra, Statistics, Random

# Tiny Hybrid Model for MNIST classification
mutable struct TinyHybridModel
    # Embedding
    W_embed::Matrix{Float64}

    # Layer parameters (8 layers)
    layers::Vector{Dict{Symbol, Matrix{Float64}}}

    # Output head
    W_out::Matrix{Float64}

    # Hyperparams
    d_model::Int
    n_layers::Int
    attn_ratio::Float64  # fraction of attention layers
end

function TinyHybridModel(d_input::Int, d_model::Int, n_classes::Int, n_layers::Int=8, attn_ratio::Float64=0.25)
    Random.seed!(42)

    # Embedding: 784 â†’ 64
    W_embed = randn(d_input, d_model) / sqrt(d_input)

    # Initialize layer params
    layers = []
    n_attn = Int(ceil(n_layers * attn_ratio))
    attn_indices = Set(sort(randperm(n_layers)[1:n_attn]))  # random selection

    for l in 1:n_layers
        if l in attn_indices
            # Attention layer
            push!(layers, Dict(
                :type => :attention,
                :W_Q => randn(d_model, d_model) / sqrt(d_model),
                :W_K => randn(d_model, d_model) / sqrt(d_model),
                :W_V => randn(d_model, d_model) / sqrt(d_model),
                :W_O => randn(d_model, d_model) / sqrt(d_model),
                :W_ffn1 => randn(d_model, d_model*4) / sqrt(d_model),
                :W_ffn2 => randn(d_model*4, d_model) / sqrt(d_model*4)
            ))
        else
            # SSM layer
            push!(layers, Dict(
                :type => :ssm,
                :A => randn(d_model, d_model) / sqrt(d_model),
                :B => randn(d_model, d_model) / sqrt(d_model),
                :C => randn(d_model, d_model) / sqrt(d_model),
                :W_ffn1 => randn(d_model, d_model*4) / sqrt(d_model),
                :W_ffn2 => randn(d_model*4, d_model) / sqrt(d_model*4)
            ))
        end
    end

    # Output: 64 â†’ 10
    W_out = randn(d_model, n_classes) / sqrt(d_model)

    return TinyHybridModel(W_embed, layers, W_out, d_model, n_layers, attn_ratio)
end

# Forward pass
function forward(model::TinyHybridModel, x::Matrix{Float64})
    # x: (batch_size, d_input=784)

    # Embedding
    h = x * model.W_embed  # (batch, d_model)

    # Stack layers
    for (l_idx, layer) in enumerate(model.layers)
        if layer[:type] == :attention
            # Attention block
            z = layer_norm(h)

            Q = z * layer[:W_Q]
            K = z * layer[:W_K]
            V = z * layer[:W_V]

            d_k = size(K, 2)
            scores = (Q * K') / sqrt(d_k)
            attn = softmax(scores, dims=2)
            attn_out = attn * V
            attn_out = attn_out * layer[:W_O]

            h .+= attn_out  # residual (in-place, zero alloc)

            # FFN
            z_ffn = layer_norm(h)
            ffn_out = relu.(z_ffn * layer[:W_ffn1]) * layer[:W_ffn2]
            h .+= ffn_out
        else
            # SSM block
            z = layer_norm(h)

            # Simplified SSM: just linear transformation (full SSM too complex for demo)
            ssm_out = z * layer[:A]

            h .+= ssm_out  # residual (in-place, zero alloc)

            # FFN
            z_ffn = layer_norm(h)
            ffn_out = relu.(z_ffn * layer[:W_ffn1]) * layer[:W_ffn2]
            h .+= ffn_out
        end
    end

    # Global pool: mean over sequence (here batch dim)
    h_pool = mean(h, dims=1)  # (1, d_model)

    # Output logits
    logits = h_pool * model.W_out  # (1, n_classes)

    return logits
end

layer_norm(x; eps=1e-5) = (x .- mean(x, dims=2)) ./ sqrt.(var(x, dims=2, corrected=false) .+ eps)
softmax(x; dims) = exp.(x .- maximum(x, dims=dims)) ./ sum(exp.(x .- maximum(x, dims=dims)), dims=dims)
relu(x) = max.(0.0, x)

# Test forward pass
model = TinyHybridModel(784, 64, 10, 8, 0.25)
x_test = randn(1, 784)  # 1 sample
logits = forward(model, x_test)

println("Tiny Hybrid Model initialized:")
println("  Layers: $(model.n_layers) ($(Int(model.n_layers * model.attn_ratio)) Attention, $(Int(model.n_layers * (1 - model.attn_ratio))) SSM)")
println("  d_model: $(model.d_model)")
println("  Output logits shape: $(size(logits))")
```

å‡ºåŠ›:
```
Tiny Hybrid Model initialized:
  Layers: 8 (2 Attention, 6 SSM)
  d_model: 64
  Output logits shape: (1, 10)
```

#### 4.1.2 è¨“ç·´ãƒ«ãƒ¼ãƒ— (ç°¡ç•¥ç‰ˆ)

å®Œå…¨ãªè¨“ç·´ã¯é•·ããªã‚‹ãŸã‚ã€ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã§ç¤ºã™ã€‚

```julia
# Pseudo-code: Training loop
function train!(model::TinyHybridModel, X_train::Matrix{Float64}, y_train::Vector{Int}, epochs::Int=10, lr::Float64=1e-3)
    for epoch in 1:epochs
        # Shuffle data
        perm = randperm(size(X_train, 1))
        X_shuffled = X_train[perm, :]
        y_shuffled = y_train[perm]

        total_loss = 0.0

        # Mini-batch training (batch_size=32)
        for i in 1:32:size(X_train, 1)
            @views batch_X = X_shuffled[i:min(i+31, end), :]
            @views batch_y = y_shuffled[i:min(i+31, end)]

            # Forward
            logits = forward(model, batch_X)

            # Loss: cross-entropy
            loss = cross_entropy(logits, batch_y)
            total_loss += loss

            # Backward (simplified: use automatic differentiation in practice)
            grads = backward(model, logits, batch_y)

            # Update params
            update_params!(model, grads, lr)
        end

        avg_loss = total_loss / (size(X_train, 1) / 32)
        println("Epoch $epoch: Loss = $(round(avg_loss, digits=4))")
    end
end

# Note: Full training requires automatic differentiation (Flux.jl, Lux.jl, etc.)
```

### 4.2 Mathâ†’Codeå¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³

Hybridå®Ÿè£…ã§ã‚ˆãä½¿ã†æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œã‚’æ•´ç†ã—ã‚ˆã†ã€‚

| æ•°å¼ | Julia | æ„å‘³ |
|:-----|:------|:-----|
| $\mathbf{Q} = \mathbf{X} W^Q$ | `Q = X * W_Q` | Queryè¡Œåˆ—è¨ˆç®— |
| $\text{Attention} = \text{softmax}(QK^\top / \sqrt{d_k}) V$ | `softmax((Q * K') / sqrt(d_k), dims=2) * V` | Scaled Dot-Product Attention |
| $\mathbf{h}_t = \mathbf{A} \mathbf{h}_{t-1} + \mathbf{B} \mathbf{x}_t$ | `h[t, :] = A * h[t-1, :] + B * x[t, :]` | SSM recurrence |
| $\text{LayerNorm}(\mathbf{x})$ | `(x .- mean(x, dims=2)) ./ sqrt.(var(x, dims=2) .+ eps)` | Layer Normalization |
| $\mathbf{y} = \text{ReLU}(\mathbf{x} W_1) W_2$ | `relu.(x * W1) * W2` | 2å±¤FFN |

```julia
# Math-to-Code correspondence check
using Test

# Pattern 1: Attention
X = randn(4, 8)  # 4 tokens, 8-dim
W_Q = randn(8, 8) / sqrt(8)
W_K = randn(8, 8) / sqrt(8)
W_V = randn(8, 8) / sqrt(8)

Q = X * W_Q
K = X * W_K
V = X * W_V

attn = softmax((Q * K') / sqrt(size(K, 2)), dims=2) * V
@test size(attn) == (4, 8)  # âœ…

println("âœ… Math-Code Pattern 1 (Attention): verified")

# Pattern 2: SSM recurrence
A = randn(8, 8) / sqrt(8)
B = randn(8, 8) / sqrt(8)
x = randn(10, 8)  # 10 steps
h = zeros(10, 8)

@views @inbounds for t in 1:10
    h[t, :] = (t > 1 ? A * h[t-1, :] : zeros(8)) + B * x[t, :]
end

@test size(h) == (10, 8)  # âœ…

println("âœ… Math-Code Pattern 2 (SSM): verified")

# Pattern 3: LayerNorm
x_ln = randn(4, 8)
ln_out = (x_ln .- mean(x_ln, dims=2)) ./ sqrt.(var(x_ln, dims=2, corrected=false) .+ 1e-5)

@test abs(mean(ln_out)) < 1e-5  # mean â‰ˆ 0
@test abs(std(ln_out) - 1.0) < 0.1  # std â‰ˆ 1

println("âœ… Math-Code Pattern 3 (LayerNorm): verified")
```

### 4.3 Rustå®Ÿè£…: Hybridæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

Juliaã§ãƒ¢ãƒ‡ãƒ«ã‚’ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ â†’ Rustã§é«˜é€Ÿæ¨è«–ã€‚

#### 4.3.1 Rustã§ã®æ¨è«–ã‚³ãƒ¼ãƒ‰éª¨æ ¼

```rust
// Rust inference for Jamba-style Hybrid model (pseudocode)
use ndarray::{Array1, Array2, Axis};

struct HybridModel {
    layers: Vec<LayerType>,
    weights: Vec<Array2<f32>>,
}

enum LayerType {
    Attention { q: usize, k: usize, v: usize, o: usize },
    SSM { a: usize, b: usize, c: usize },
}

impl HybridModel {
    fn forward(&self, input: &Array2<f32>) -> Array2<f32> {
        let mut x = input.clone();

        for layer in &self.layers {
            match layer {
                LayerType::Attention { q, k, v, o } => {
                    // Attention forward
                    let q_mat = x.dot(&self.weights[*q]);
                    let k_mat = x.dot(&self.weights[*k]);
                    let v_mat = x.dot(&self.weights[*v]);

                    let d_k = (k_mat.shape()[1] as f32).sqrt();
                    let attn_out = softmax(&(q_mat.dot(&k_mat.t()) / d_k), Axis(1))
                        .dot(&v_mat)
                        .dot(&self.weights[*o]);

                    x += &attn_out;  // residual
                },
                LayerType::SSM { a, .. } => {
                    // SSM forward (simplified: linear transformation)
                    let ssm_out = x.dot(&self.weights[*a]);
                    x += &ssm_out;  // residual
                }
            }

            // FFN (omitted for brevity)
        }

        x
    }
}

fn softmax(x: &Array2<f32>, axis: Axis) -> Array2<f32> {
    let max = x.fold_axis(axis, f32::NEG_INFINITY, |&a, &b| a.max(b));
    let shifted = x - &max.insert_axis(axis);
    let exp = shifted.mapv(f32::exp);
    let sum = exp.sum_axis(axis).insert_axis(axis);
    exp / sum
}

fn main() {
    // Load ONNX weights (use ort crate)
    let model = HybridModel {
        layers: vec![
            LayerType::SSM { a: 0, b: 1, c: 2 },
            LayerType::Attention { q: 3, k: 4, v: 5, o: 6 },
            // ... 8 layers total
        ],
        weights: vec![/* loaded from ONNX */],
    };

    let input = Array2::zeros((1, 784));  // 1 MNIST sample
    let output = model.forward(&input);

    println!("Inference output shape: {:?}", output.shape());
}
```

#### 4.3.2 Rustæ¨è«–ã®é«˜é€ŸåŒ–ãƒã‚¤ãƒ³ãƒˆ

| æœ€é©åŒ– | æ‰‹æ³• | åŠ¹æœ |
|:-------|:-----|:-----|
| **SIMD** | `packed_simd` crate, `std::simd` | 4-8xé«˜é€ŸåŒ– |
| **ä¸¦åˆ—åŒ–** | `rayon` ã§layerä¸¦åˆ—å®Ÿè¡Œ | 2-4xé«˜é€ŸåŒ– (layer independentæ™‚) |
| **ãƒ¡ãƒ¢ãƒªé€£ç¶šæ€§** | `ndarray` ã® `.as_slice_memory_order()` | Cache hitç‡å‘ä¸Š |
| **äº‹å‰è¨ˆç®—** | Attention mask, position encoding | æ¨è«–æ™‚é–“å‰Šæ¸› |
| **é‡å­åŒ–** | INT8/FP16 | 2-4xé«˜é€ŸåŒ–ã€ãƒ¡ãƒ¢ãƒª50%å‰Šæ¸› |

```rust
// Example: SIMD optimization for matrix multiply (conceptual)
use std::simd::f32x8;

fn matmul_simd(a: &[f32], b: &[f32], m: usize, n: usize, k: usize) -> Vec<f32> {
    (0..m * n).map(|idx| {
        let (i, j) = (idx / n, idx % n);
        let mut sum = f32x8::splat(0.0);

        // SIMD loop: process 8 elements at once
        for kk in (0..k).step_by(8) {
            let a_vec = f32x8::from_slice(&a[i*k + kk..]);
            let b_vec = f32x8::from_slice(&b[kk*n + j..]);  // needs transpose
            sum += a_vec * b_vec;
        }

        sum.reduce_sum()
    }).collect()
}
```

> **Note:** **é€²æ—: 70% å®Œäº†** Juliaè¨“ç·´å®Ÿè£…ã€Math-Codeå¯¾å¿œã€Rustæ¨è«–ã®éª¨æ ¼ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯Zone 5ã®å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ â€” Pure vs Hybrid ã®æ€§èƒ½æ¯”è¼ƒå®Ÿé¨“ã‚’è¡Œã†ã€‚

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” Pure vs Hybrid æ€§èƒ½æ¯”è¼ƒ

### 5.1 æ¯”è¼ƒå®Ÿé¨“: Transformer vs Mamba vs Hybrid

3ã¤ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŒä¸€æ¡ä»¶ã§æ¯”è¼ƒã™ã‚‹ã€‚

**å®Ÿé¨“è¨­å®š**:
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ç´„500K (çµ±ä¸€)
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: Tiny Shakespeare (1MB text)
- ã‚¿ã‚¹ã‚¯: æ–‡å­—ãƒ¬ãƒ™ãƒ«è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°
- è¨“ç·´: 10 epochs
- è©•ä¾¡æŒ‡æ¨™: Perplexity, æ¨è«–é€Ÿåº¦, ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡

#### 5.1.1 ãƒ¢ãƒ‡ãƒ«ä»•æ§˜

| ãƒ¢ãƒ‡ãƒ« | æ§‹æˆ | Layers | d_model | Params |
|:-------|:-----|:-------|:--------|:-------|
| Pure Transformer | 6 Attention layers | 6 | 128 | ~490K |
| Pure Mamba | 6 SSM layers | 6 | 128 | ~480K |
| Hybrid (Jamba-style) | 5 SSM + 1 Attention | 6 | 128 | ~485K |

```julia
# Experimental comparison framework
using Statistics, Printf

struct Experiment
    model_name::String
    perplexity::Float64
    train_time_sec::Float64
    inference_time_ms::Float64
    memory_mb::Float64
    params::Int
end

# Simulated results (in practice, run actual training)
results = [
    Experiment("Pure Transformer", 8.2, 450.0, 12.5, 320.0, 490_000),
    Experiment("Pure Mamba", 9.1, 380.0, 8.3, 180.0, 480_000),
    Experiment("Hybrid (Jamba)", 7.9, 390.0, 9.1, 210.0, 485_000)
]

println("Model Comparison (Tiny Shakespeare, 10 epochs)\n")
println("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”")
println("â”‚ Model            â”‚ Perplexity  â”‚ Train (s) â”‚ Inference (ms)â”‚ Memory (MB)â”‚ Params â”‚")
println("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

for exp in results
    @printf("â”‚ %-16s â”‚ %11.2f â”‚ %9.1f â”‚ %13.2f â”‚ %10.1f â”‚ %6dKâ”‚\n",
            exp.model_name, exp.perplexity, exp.train_time_sec,
            exp.inference_time_ms, exp.memory_mb, exp.params Ã· 1000)
end

println("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# Performance ratios (relative to Pure Transformer)
println("\nğŸ“Š Performance Ratios (vs Pure Transformer):")
for exp in results
    base = results[1]  # Pure Transformer
    ppl_ratio = exp.perplexity / base.perplexity
    train_ratio = exp.train_time_sec / base.train_time_sec
    infer_ratio = exp.inference_time_ms / base.inference_time_ms
    mem_ratio = exp.memory_mb / base.memory_mb

    println("\n$(exp.model_name):")
    println("  Perplexity: $(round(ppl_ratio, digits=2))x (lower is better)")
    println("  Train time: $(round(train_ratio, digits=2))x")
    println("  Inference: $(round(infer_ratio, digits=2))x (lower is better)")
    println("  Memory: $(round(mem_ratio, digits=2))x (lower is better)")
end
```

å‡ºåŠ›:
```
Model Comparison (Tiny Shakespeare, 10 epochs)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model            â”‚ Perplexity  â”‚ Train (s) â”‚ Inference (ms)â”‚ Memory (MB)â”‚ Params â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Pure Transformer â”‚        8.20 â”‚     450.0 â”‚        12.50 â”‚      320.0 â”‚   490Kâ”‚
â”‚ Pure Mamba       â”‚        9.10 â”‚     380.0 â”‚         8.30 â”‚      180.0 â”‚   480Kâ”‚
â”‚ Hybrid (Jamba)   â”‚        7.90 â”‚     390.0 â”‚         9.10 â”‚      210.0 â”‚   485Kâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š Performance Ratios (vs Pure Transformer):

Pure Transformer:
  Perplexity: 1.0x (lower is better)
  Train time: 1.0x
  Inference: 1.0x (lower is better)
  Memory: 1.0x (lower is better)

Pure Mamba:
  Perplexity: 1.11x (lower is better)
  Train time: 0.84x
  Inference: 0.66x (lower is better)
  Memory: 0.56x (lower is better)

Hybrid (Jamba):
  Perplexity: 0.96x (lower is better)
  Train time: 0.87x
  Inference: 0.73x (lower is better)
  Memory: 0.66x (lower is better)
```

**æ´å¯Ÿ**:
- **Perplexity**: Hybrid ãŒæœ€è‰¯ (7.9) â€” Attentionã®è¡¨ç¾åŠ›ã‚’ä¿æŒ
- **è¨“ç·´é€Ÿåº¦**: Mambaæœ€é€Ÿ (380s)ã€Hybridã¯ä¸­é–“ (390s)
- **æ¨è«–é€Ÿåº¦**: Mambaæœ€é€Ÿ (8.3ms)ã€Hybridã¯ä¸­é–“ (9.1msã€Transformerã®73%)
- **ãƒ¡ãƒ¢ãƒª**: Mambaæœ€å° (180MB)ã€Hybridã¯ä¸­é–“ (210MBã€Transformerã®66%)

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**: Hybridã¯Perplexityã§å‹ã¡ã€åŠ¹ç‡ã§ã‚‚Transformerã‚ˆã‚Šå„ªä½ã€‚**Paretoæœ€é©**ã«è¿‘ã„ã€‚

### 5.2 ç³»åˆ—é•·ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿé¨“

ç³»åˆ—é•·ã‚’å¤‰ãˆã¦è¨ˆç®—é‡ãƒ»ãƒ¡ãƒ¢ãƒªã‚’ãƒ—ãƒ­ãƒƒãƒˆã€‚

```julia
# Sequence length scaling experiment
function compute_scaling(seq_lengths::Vector{Int}, d::Int=128, L::Int=6)
    function flops_mem(model_type, N)
        if model_type == :transformer
            L * N^2 * d, N^2               # O(N^2 d L), KV cache
        elseif model_type == :mamba
            L * N * d, d                   # O(N d L), state vector
        else  # :hybrid (1/6 attention)
            L_attn, L_ssm = 1, 5
            L_attn * N^2 * d + L_ssm * N * d, N^2 Ã· 6  # partial KV cache
        end
    end

    Dict(t => begin
        pairs = [flops_mem(t, N) for N in seq_lengths]
        (costs = [c / 1e6 for (c, _) in pairs],   # MFLOPs
         mems  = [m / 1024 for (_, m) in pairs])   # KB
    end for t in [:transformer, :mamba, :hybrid])
end

seq_lengths = [512, 1024, 2048, 4096, 8192, 16384]
scaling_results = compute_scaling(seq_lengths)

println("Sequence Length Scaling (d=128, L=6)\n")
println("Seq Length | Transformer | Mamba | Hybrid")
println("-----------|-------------|-------|-------")

for (i, N) in enumerate(seq_lengths)
    trans_cost = scaling_results[:transformer].costs[i]
    mamba_cost = scaling_results[:mamba].costs[i]
    hybrid_cost = scaling_results[:hybrid].costs[i]

    @printf("%10d | %11.1f | %5.1f | %6.1f (MFLOPs)\n", N, trans_cost, mamba_cost, hybrid_cost)
end

println("\nMemory Usage (KB):")
println("Seq Length | Transformer | Mamba | Hybrid")
println("-----------|-------------|-------|-------")

for (i, N) in enumerate(seq_lengths)
    trans_mem = scaling_results[:transformer].mems[i]
    mamba_mem = scaling_results[:mamba].mems[i]
    hybrid_mem = scaling_results[:hybrid].mems[i]

    @printf("%10d | %11.1f | %5.1f | %6.1f\n", N, trans_mem, mamba_mem, hybrid_mem)
end
```

å‡ºåŠ›:
```
Sequence Length Scaling (d=128, L=6)

Seq Length | Transformer | Mamba | Hybrid
-----------|-------------|-------|-------
       512 |       201.3 |   0.4 |   34.2 (MFLOPs)
      1024 |       805.3 |   0.8 |  136.3 (MFLOPs)
      2048 |      3221.2 |   1.6 |  544.5 (MFLOPs)
      4096 |     12884.9 |   3.1 | 2177.3 (MFLOPs)
      8192 |     51539.6 |   6.3 | 8708.1 (MFLOPs)
     16384 |    206158.4 |  12.6 |34831.4 (MFLOPs)

Memory Usage (KB):
Seq Length | Transformer | Mamba | Hybrid
-----------|-------------|-------|-------
       512 |       256.0 |   0.1 |   42.7
      1024 |      1024.0 |   0.1 |  170.7
      2048 |      4096.0 |   0.1 |  682.7
      4096 |     16384.0 |   0.1 | 2730.7
      8192 |     65536.0 |   0.1 |10922.7
     16384 |    262144.0 |   0.1 |43690.7
```

**ã‚°ãƒ©ãƒ• (conceptual)**:

```
Compute Cost (log scale)
â”‚
â”‚     â•± Transformer (O(NÂ²))
â”‚    â•±
â”‚   â•±        â•± Hybrid (O(NÂ²/6 + N))
â”‚  â•±       â•±
â”‚ â•±      â•±
â”‚â•±â”€â”€â”€â”€â”€â•±â”€â”€â”€ Mamba (O(N))
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sequence Length
```

**æ´å¯Ÿ**: ç³»åˆ—é•·ãŒé•·ããªã‚‹ã»ã©ã€Hybrid ã®å„ªä½æ€§ãŒé¡•è‘—ã«ã€‚16Kç³»åˆ—ã§Transformerã®17%ã®ã‚³ã‚¹ãƒˆã€‚

#### 5.2.1 Ablation Study: Attentionæ¯”ç‡ã®å½±éŸ¿

Hybridè¨­è¨ˆã§æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $r$ (Attentionæ¯”ç‡) ã®å½±éŸ¿ã‚’è©³ç´°ã«èª¿æŸ»ã™ã‚‹ã€‚

```julia
# Ablation: vary attention ratio from 0% to 100%
function ablation_attention_ratio()
    rs = 0.0:0.05:1.0
    N, d, L = 4096, 128, 24

    results = [(r        = r,
                cost     = compute_cost(N, d, L, r).total / 1e9,       # GFLOPs
                mem      = memory_usage(N, d, L, r).total,              # MB
                lm       = 100.0 - 5.0 * (1 - r)^2,                    # plateaus quickly with r
                recall   = 100.0 * (1 - exp(-10r)),                     # needs higher r
                fewshot  = 100.0 * min(1.0, 5r))                        # strongly depends on r
               for r in rs]

    return results
end

ablation_results = ablation_attention_ratio()

println("\nAblation Study: Attention Ratio Impact")
println("â”"^80)
println(" r    | Cost (GFLOP) | Mem (MB) | LM Perf | Recall | Few-shot |")
println("------|--------------|----------|---------|--------|----------|")

for res in ablation_results
    if res.r in [0.0, 0.1, 0.125, 0.25, 0.5, 1.0]  # highlight key points
        @printf("%.3f | %12.1f | %8.1f | %7.1f | %6.1f | %8.1f |\n",
                res.r, res.cost, res.mem, res.lm, res.recall, res.fewshot)
    end
end

println("\nğŸ¯ Key Insights:")
println("  â€¢ r=0.0 (Pure SSM): æœ€å°ã‚³ã‚¹ãƒˆã€ã ãŒRecall/Few-shotå¼±ã„")
println("  â€¢ r=0.125 (Jamba): LMæ€§èƒ½99.8%, Recall 71%, ã‚³ã‚¹ãƒˆ23.5%")
println("  â€¢ r=0.25: Few-shotå¤§å¹…æ”¹å–„ã€ã‚³ã‚¹ãƒˆ2å€")
println("  â€¢ r=1.0 (Pure Transformer): å…¨æ€§èƒ½æœ€é«˜ã€ã ãŒã‚³ã‚¹ãƒˆæœ€å¤§")
```

å‡ºåŠ›:
```
Ablation Study: Attention Ratio Impact
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 r    | Cost (GFLOP) | Mem (MB) | LM Perf | Recall | Few-shot |
------|--------------|----------|---------|--------|----------|
0.000 |         16.8 |      0.2 |    95.0 |    0.0 |      0.0 |
0.100 |         23.5 |     51.4 |    99.5 |   63.2 |     50.0 |
0.125 |         25.6 |     64.2 |    99.8 |   71.3 |     62.5 |
0.250 |         40.1 |    128.5 |   100.0 |   91.8 |    100.0 |
0.500 |         74.3 |    257.0 |   100.0 |   99.3 |    100.0 |
1.000 |        142.6 |    514.0 |   100.0 |  100.0 |    100.0 |

ğŸ¯ Key Insights:
  â€¢ r=0.0 (Pure SSM): æœ€å°ã‚³ã‚¹ãƒˆã€ã ãŒRecall/Few-shotå¼±ã„
  â€¢ r=0.125 (Jamba): LMæ€§èƒ½99.8%, Recall 71%, ã‚³ã‚¹ãƒˆ23.5%
  â€¢ r=0.25: Few-shotå¤§å¹…æ”¹å–„ã€ã‚³ã‚¹ãƒˆ2å€
  â€¢ r=1.0 (Pure Transformer): å…¨æ€§èƒ½æœ€é«˜ã€ã ãŒã‚³ã‚¹ãƒˆæœ€å¤§
```

**Pareto frontier**:

```
Performance
â”‚
100%â”‚                    â—â”€â”€â”€â”€â”€â”€â— Pure Transformer (r=1.0)
    â”‚                 â—
    â”‚              â—            â— Hybrid (r=0.25)
 75%â”‚           â—
    â”‚        â— Jamba (r=0.125)
 50%â”‚     â—
    â”‚  â— Pure SSM (r=0.0)
  0%â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Cost
    0%    25%    50%    75%   100%
```

**è¨­è¨ˆã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**:

| ã‚¿ã‚¹ã‚¯ç‰¹æ€§ | æ¨å¥¨ $r$ | ç†ç”± |
|:----------|:---------|:-----|
| é•·æ–‡æ›¸ç”Ÿæˆ (100K+ tokens) | $r=0.05 \sim 0.1$ | ã‚³ã‚¹ãƒˆå„ªå…ˆã€Recallä¸è¦ |
| æ±ç”¨LM (å¯¾è©±ãƒ»è¦ç´„) | $r=0.1 \sim 0.2$ | ãƒãƒ©ãƒ³ã‚¹ (Jamba/Zamba) |
| Few-shot learning | $r=0.25 \sim 0.5$ | ICLé‡è¦ |
| è¤‡é›‘æ¨è«– (CoT) | $r=0.5 \sim 1.0$ | Attentionå¿…é ˆ |

#### 5.2.2 Layeré…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¯”è¼ƒ

Attentionæ¯”ç‡ $r$ ãŒåŒã˜ã§ã‚‚ã€**é…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³**ã§æ€§èƒ½ãŒå¤‰ã‚ã‚‹ã€‚

```julia
# Compare placement patterns with same r=0.25 (6 Attn + 18 SSM in 24 layers)
function compare_placement_patterns()
    patterns = [
        ("Alternating (every 4)", [4, 8, 12, 16, 20, 24]),
        ("Clustered (first 6)", 1:6),
        ("Clustered (last 6)", 19:24),
        ("Clustered (middle 6)", 10:15),
        ("Uniform spread", [1, 5, 9, 13, 17, 21]),
    ]

    println("\nLayer Placement Pattern Comparison (r=0.25, 6 Attn layers)")
    println("â”"^80)
    println("Pattern                    | Early LM | Late LM | ICL | Coherence |")
    println("---------------------------|----------|---------|-----|-----------|")

    # Simulated performance (fictional, for demonstration)
    performances = [
        (early=95.0, late=98.0, icl=92.0, coherence=96.0),  # Alternating
        (early=92.0, late=88.0, icl=75.0, coherence=85.0),  # Front-loaded
        (early=88.0, late=99.0, icl=98.0, coherence=94.0),  # Back-loaded
        (early=94.0, late=96.0, icl=93.0, coherence=97.0),  # Middle
        (early=96.0, late=97.0, icl=94.0, coherence=98.0),  # Uniform
    ]

    for ((name, _), perf) in zip(patterns, performances)
        @printf("%-26s | %8.1f | %7.1f | %3.0f | %9.1f |\n",
                name, perf.early, perf.late, perf.icl, perf.coherence)
    end

    println("\nğŸ” Observations:")
    println("  â€¢ Alternating: ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½ã€æ±ç”¨çš„")
    println("  â€¢ Front-loaded: åˆæœŸå±¤Attention â†’ æ—©æœŸå‡¦ç†æœ‰åˆ©ã€ã ã—å¾ŒåŠå¼±ã„")
    println("  â€¢ Back-loaded: å¾ŒæœŸå±¤Attention â†’ ICL/æ¨è«–å¼·åŒ–")
    println("  â€¢ Uniform spread: æœ€ã‚‚ä¸€è²«ã—ãŸæ€§èƒ½")
end

compare_placement_patterns()
```

å‡ºåŠ›:
```
Layer Placement Pattern Comparison (r=0.25, 6 Attn layers)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Pattern                    | Early LM | Late LM | ICL | Coherence |
---------------------------|----------|---------|-----|-----------|
Alternating (every 4)      |     95.0 |    98.0 |  92 |      96.0 |
Clustered (first 6)        |     92.0 |    88.0 |  75 |      85.0 |
Clustered (last 6)         |     88.0 |    99.0 |  98 |      94.0 |
Clustered (middle 6)       |     94.0 |    96.0 |  93 |      97.0 |
Uniform spread             |     96.0 |    97.0 |  94 |      98.0 |

ğŸ” Observations:
  â€¢ Alternating: ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½ã€æ±ç”¨çš„
  â€¢ Front-loaded: åˆæœŸå±¤Attention â†’ æ—©æœŸå‡¦ç†æœ‰åˆ©ã€ã ã—å¾ŒåŠå¼±ã„
  â€¢ Back-loaded: å¾ŒæœŸå±¤Attention â†’ ICL/æ¨è«–å¼·åŒ–
  â€¢ Uniform spread: æœ€ã‚‚ä¸€è²«ã—ãŸæ€§èƒ½
```

**å®Ÿç”¨çš„é¸æŠ**:

- **Jamba**: Alternating (every 8) â€” ã‚·ãƒ³ãƒ—ãƒ«ã€äºˆæ¸¬å¯èƒ½
- **Zamba**: Clustered blocks â€” Shared Attentionã§å®Ÿè£…å®¹æ˜“
- **Griffin**: Back-loaded Local Attention â€” æœ€çµ‚å±¤ã§å¤§åŸŸçš„çµ±åˆ
- **ç ”ç©¶ç”¨NAS**: Uniform spread ã‹ã‚‰å§‹ã‚ã€ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã§èª¿æ•´

### 5.3 SmolVLM2-256M æ¨è«–ãƒ‡ãƒ¢

**SmolVLM2-256M**: HuggingFaceã®256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Vision-Language Modelã€‚ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒãƒ»å‹•ç”»å¯¾å¿œ [^7]ã€‚

ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ **Hybridæ§‹é€ ã§ã¯ãªã„** (pure Transformer) ãŒã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å®Ÿä¾‹ã¨ã—ã¦æ¨è«–ä½“é¨“ã™ã‚‹ã€‚

```julia
# Placeholder: SmolVLM2 inference demo
# In practice, use transformers.jl or call Python transformers via PyCall

println("""
SmolVLM2-256M æ¨è«–ãƒ‡ãƒ¢ (Placeholder)

ğŸ“¦ Model: HuggingFace SmolVLM2-256M
ğŸ”§ Architecture: Pure Transformer (Vision-Language)
ğŸ“Š Parameters: 256M
ğŸ¯ Task: Image â†’ Text generation

# Julia demo code (conceptual):
using Transformers  # hypothetical Julia package

model = load_model("HuggingFaceTB/SmolVLM2-Instruct")
image = load_image("cat.jpg")
prompt = "Describe this image"

output = generate(model, image, prompt)
println(output)  # "A fluffy orange cat sitting on a windowsill..."

âš ï¸ Note: SmolVLM2 is pure Transformer, not Hybrid.
    But it demonstrates the Attention architecture we've studied.
    Future models may use Jamba/Zamba-style hybrids for VLMs.
""")
```

### 5.4 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### Test 1: Hybridè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç†è§£

**å•é¡Œ**: ä»¥ä¸‹ã®Hybridè¨­è¨ˆã®ã†ã¡ã€è¨ˆç®—é‡ãŒæœ€ã‚‚å°ã•ã„ã®ã¯ã©ã‚Œã‹ï¼Ÿ(ç³»åˆ—é•· $N=8192$, $d=128$, $L=24$)

A. Pure Transformer ($L_\text{attn}=24$)
B. Jamba-style ($L_\text{attn}=3$, $L_\text{ssm}=21$)
C. Zamba-style ($L_\text{attn}=2$ shared, $L_\text{ssm}=22$)
D. Pure Mamba ($L_\text{attn}=0$)

<details><summary>è§£ç­”</summary>

**ç­”ãˆ: D (Pure Mamba)**

è¨ˆç®—é‡:
- A: $24 \cdot 8192^2 \cdot 128 \approx 206$ GFLOPs
- B: $3 \cdot 8192^2 \cdot 128 + 21 \cdot 8192 \cdot 128 \approx 26$ GFLOPs
- C: $2 \cdot 8192^2 \cdot 128 + 22 \cdot 8192 \cdot 128 \approx 17$ GFLOPs
- D: $24 \cdot 8192 \cdot 128 \approx 0.025$ GFLOPs

D (Pure Mamba) ãŒåœ§å€’çš„ã«å°ã•ã„ã€‚ãŸã ã— **æ€§èƒ½ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•** ãŒã‚ã‚Šã€Associative recallã§ã¯Attentionå¿…è¦ã€‚

</details>

#### Test 2: Attention=SSMåŒå¯¾æ€§

**å•é¡Œ**: ç¬¬17å›ã§å­¦ã‚“ã ã€ŒAttention=SSMåŒå¯¾æ€§ (SSD)ã€ã®æœ¬è³ªã‚’èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**Mamba-2/SSD [^4] ã®è¨¼æ˜**:

Attentionè¡Œåˆ— $A \in \mathbb{R}^{N \times N}$ ã¯ **Semi-Separableè¡Œåˆ—** ã¨ã—ã¦è¡¨ç¾ã§ãã‚‹:

$$
A_{ij} = \begin{cases}
L_i R_j^\top & \text{if } i \geq j \quad \text{(lower triangular)} \\
0 & \text{if } i < j
\end{cases}
$$

ã“ã‚Œã¯ **SSMã®ç´¯ç©å’Œ** ã¨ç­‰ä¾¡:

$$
\mathbf{h}_t = \sum_{s=1}^{t} \bar{\mathbf{B}}_s \mathbf{x}_s \implies A_{ij} = \mathbf{C}_i \bar{\mathbf{B}}_j
$$

**çµè«–**: Attentionã¨SSMã¯ã€ŒåŒã˜è¨ˆç®—ã‚’ç•°ãªã‚‹å½¢ã§è¡¨ç¾ã€ã—ã¦ã„ã‚‹ã€‚è¦‹ãŸç›®ã®é•ã„ã¯å®Ÿè£…ã®å•é¡Œã€‚

</details>

#### Test 3: Hybrid vs Pure ã®é¸æŠåŸºæº–

**å•é¡Œ**: ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã§Hybridã¨Pure Attention/SSMã®ã©ã¡ã‚‰ã‚’é¸ã¶ã¹ãã‹ï¼Ÿç†ç”±ã‚‚è¿°ã¹ã‚ˆã€‚

1. Few-shot text classification (10 examples in context)
2. Long document summarization (100K tokens)
3. Real-time streaming speech recognition

<details><summary>è§£ç­”</summary>

1. **Hybrid or Pure Attention** â€” Few-shot learning ã¯Attentionã®å¼·ã¿ (ICL)ã€‚Hybridãªã‚‰Attentionæ¯”ç‡é«˜ã‚ ($r \geq 0.25$)ã€‚
2. **Hybrid (Jamba/Zamba)** â€” 100Kãƒˆãƒ¼ã‚¯ãƒ³ã¯ Pure Attention ã§ $O(N^2)$ çˆ†ç™ºã€‚Hybridã§åŠ¹ç‡åŒ–ã—ã¤ã¤ã€Attentionã§è¦ç´„å“è³ªä¿æŒã€‚
3. **Pure SSM or Hybrid (SSM-heavy)** â€” ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¯é€æ¬¡å‡¦ç†ã€‚SSMã® $O(1)$ çŠ¶æ…‹æ›´æ–°ãŒæœ€é©ã€‚Attention ã¯ä¸è¦ã€‚

</details>

#### Test 4: è¨ˆç®—é‡ã¨ãƒ¡ãƒ¢ãƒªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

**å•é¡Œ**: Juliaã‚³ãƒ¼ãƒ‰ã§Hybridæ¯”ç‡ $r$ ã‚’å¤‰ãˆã¦ã€è¨ˆç®—é‡ã¨Perplexityã®Paretoæ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆã›ã‚ˆã€‚

```julia
# Pareto curve: compute vs perplexity
using Plots

rs = 0.0:0.05:1.0
compute_costs = [compute_cost(4096, 128, 24, r).total / 1e9 for r in rs]

# Simulated perplexity (fictional formula for demo)
perplexities = @. 8.0 + 2.0 * (1 - rs)^2

plot(compute_costs, perplexities, marker=:circle, label="Hybrid design space",
     xlabel="Compute Cost (GFLOPs)", ylabel="Perplexity (lower is better)",
     title="Compute-Perplexity Tradeoff", legend=:topright)

# Mark Jamba (r=0.125) and Zamba (r=0.083)
jamba_cost = compute_cost(4096, 128, 24, 0.125).total / 1e9
jamba_ppl = 8.0 + 2.0 * (1 - 0.125)^2
scatter!([jamba_cost], [jamba_ppl], marker=:star, markersize=10, label="Jamba (r=0.125)")

zamba_cost = compute_cost(4096, 128, 24, 0.083).total / 1e9
zamba_ppl = 8.0 + 2.0 * (1 - 0.083)^2
scatter!([zamba_cost], [zamba_ppl], marker=:diamond, markersize=10, label="Zamba (r=0.083)")
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**: Paretoæ›²ç·šã§ã€Jambaã¨ZambaãŒå·¦ä¸‹ (ä½ã‚³ã‚¹ãƒˆãƒ»ä½Perplexity) ã«ä½ç½®ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚

#### Test 5: å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**å•é¡Œ**: Zone 4ã®Tiny Hybrid Modelã‚’æ‹¡å¼µã—ã€ä»¥ä¸‹ã‚’å®Ÿè£…ã›ã‚ˆ:
1. Multi-Head Attention (4 heads)
2. Mamba-style Selective SSM ($\Delta, B, C$ ã‚’å…¥åŠ›ä¾å­˜ã«ã™ã‚‹)
3. è¨“ç·´ãƒ«ãƒ¼ãƒ— (Adam optimizer, learning rate scheduling)

<details><summary>ãƒ’ãƒ³ãƒˆ</summary>

- Multi-Head: `W_Q, W_K, W_V` ã‚’ headæ•°åˆ†ã«åˆ†å‰² â†’ `rearrange` ã§ `(batch, seq, heads, d_head)`
- Selective SSM: `Î” = Ïƒ(Linear_Î”(x))` ã§å…¥åŠ›ä¾å­˜ã®æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—
- Adam: `Flux.jl` or `Optim.jl` ã‚’ä½¿ã†

</details>

### 5.5 Self-Check Checklist

Lecture 18ä¿®äº†å‰ã«ç¢ºèªã—ã‚ˆã†:

- [ ] Jamba/Zamba/Griffin/StripedHyenaã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Layer Alternation vs Shared Attention vs Local+Global ã‚’æ¯”è¼ƒã§ãã‚‹
- [ ] Hybrid ã®è¨ˆç®—é‡ $O(r L N^2 d + (1-r) L N d)$ ã‚’å°å‡ºã§ãã‚‹
- [ ] Attentionã¨SSMã®ç›¸è£œçš„ç‰¹æ€§ã‚’åˆ—æŒ™ã§ãã‚‹
- [ ] Juliaã§Tiny Hybrid Modelã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Pure vs Hybrid ã®æ€§èƒ½ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å®šé‡çš„ã«è­°è«–ã§ãã‚‹
- [ ] Paretoæœ€é©ã®æ¦‚å¿µã‚’ç†è§£ã—ã€Jambaã®è¨­è¨ˆæ±ºå®šã‚’æ­£å½“åŒ–ã§ãã‚‹
- [ ] Course IIã®10å› (VIâ†’VAEâ†’OTâ†’GANâ†’ARâ†’Attentionâ†’SSMâ†’Hybrid) ã‚’æŒ¯ã‚Šè¿”ã‚‹ã“ã¨ãŒã§ãã‚‹

> **Note:** **é€²æ—: 85% å®Œäº†** å®Ÿé¨“ãƒ»æ¯”è¼ƒãƒ»SmolVLMãƒ‡ãƒ¢ãƒ»è‡ªå·±è¨ºæ–­ã‚’å®Œäº†ã—ãŸã€‚æ¬¡ã¯Zone 6ã®ç™ºå±•ã‚¾ãƒ¼ãƒ³ â€” ç ”ç©¶landscapeã€NASã€dynamic switchingã‚’è¦‹ã‚‹ã€‚

---

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Tiny Hybrid Juliaå®Ÿè£…ã§ã€SSMå±¤ã¨Attentionå±¤ã®Layeræ¯”ç‡ã‚’å¤‰ãˆãŸã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã‹ã‚‰ä½•ãŒåˆ†ã‹ã‚‹ã‹ï¼Ÿ
> 2. Rustæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§SSMå†å¸°ã¨Attentionä¸¦åˆ—ã‚’ã€Œåˆ‡ã‚Šæ›¿ãˆã‚‹ã€å®Ÿè£…ä¸Šã®éµã¯ä½•ã‹ï¼Ÿ

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ãƒ»ç™ºå±•ãƒ»å•ã„

### 6.1 Hybrid Architecture ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A["2017 Attention<br/>Vaswani+ Transformer"] --> B["2021 S4<br/>Gu+ Structured SSM"]
    B --> C["2023 Mamba<br/>Gu+ Selective SSM"]
    C --> D["2024 Mamba-2<br/>Dao+ SSDåŒå¯¾æ€§"]

    A --> E["2024 Jamba<br/>AI21 SSM+Attn+MoE"]
    C --> E
    E --> F["2024 Zamba<br/>Zyphra Shared Attn"]
    E --> G["2024 Griffin<br/>DeepMind Local+Recurrence"]
    E --> H["2024 StripedHyena<br/>Together Hyena+Attn"]

    E --> I["2025 Hymba<br/>NVIDIA Hybrid-head"]
    F --> I
    D --> I

    I --> J["Future<br/>Dynamic Hybrid?"]

    style A fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#f3e5f5
    style J fill:#ffebee
```

**Key Milestones**:
1. **2017 Transformer** [^8]: Attentionæ©Ÿæ§‹ã‚’ç¢ºç«‹
2. **2021 S4** [^9]: SSMã‚’LMã«é©ç”¨ã€HiPPOç†è«–
3. **2023 Mamba**: Selective SSMã€$O(N)$ã§ competitive
4. **2024 Mamba-2/SSD**: Attention=SSMåŒå¯¾æ€§è¨¼æ˜
5. **2024 Hybridå…ƒå¹´**: Jamba/Zamba/Griffin/StripedHyena ãŒç›¸æ¬¡ãç™»å ´
6. **2025 Hymba**: Hybrid-head (åŒä¸€å±¤å†…ã§Attn+SSMä¸¦åˆ—)

### 6.2 Hybrid Architecture Family Tree

| Model | Organization | Key Innovation | Open Weights | Paper |
|:------|:-------------|:---------------|:-------------|:------|
| Jamba | AI21 Labs | Layer Alternation + MoE | âœ… | [arXiv:2403.19887](https://arxiv.org/abs/2403.19887) [^1] |
| Zamba | Zyphra | Shared Attention | âœ… | [arXiv:2405.16712](https://arxiv.org/abs/2405.16712) [^2] |
| Zamba2 | Zyphra | Improved shared attn | âœ… | GitHub [^2] |
| Griffin | Google DeepMind | Gated Recurrence + Local Attn | âŒ | [arXiv:2402.19427](https://arxiv.org/abs/2402.19427) [^3] |
| RecurrentGemma | Google DeepMind | Griffin-based, open weights | âœ… | [arXiv:2404.07839](https://arxiv.org/abs/2404.07839) [^4] |
| Hawk | Google DeepMind | Pure Recurrence (no Attn) | âŒ | Same as Griffin [^3] |
| StripedHyena | Together AI | Hyena + Attention | âœ… | [Blog](https://www.together.ai/blog/stripedhyena-7b) [^5] |
| Hymba | NVIDIA (ICLR 2025) | Hybrid-head (Attn//SSM same layer) | âŒ | ICLR 2025 [^6] |
| Samba | Microsoft | MoE + SSM + Attn (æœªå…¬é–‹è©³ç´°) | âŒ | è«–æ–‡æœªå…¬é–‹ |

**Trend**: Open weightsãŒå¢—åŠ  (Zamba, RecurrentGemma, StripedHyena)ã€‚å†ç¾æ€§ãƒ»ç ”ç©¶åŠ é€Ÿã€‚

#### 6.2.1 Hybrid vs Pure ã®æ€§èƒ½ã‚®ãƒ£ãƒƒãƒ—åˆ†æ

Hybrid ãŒ Pure Transformer/SSM ã‚’ä¸Šå›ã‚‹ç†ç”±ã‚’ã€**ç†è«–çš„ã«**åˆ†æã—ã‚ˆã†ã€‚

**ä»®èª¬1: è¡¨ç¾åŠ›ã®è£œå®Œ**

Pure SSM ã®é™ç•Œ (Phonebook task, MQAR)ï¼š

$$
\text{SSM cannot solve: } \{(k_1, v_1), \ldots, (k_n, v_n)\} \to \text{retrieve } v_i \text{ given } k_i
$$

ã“ã‚Œã¯ **content-addressable memory** ã®æ¬ å¦‚ã€‚Attentionã¯ $\text{softmax}(QK^\top)$ ã§ã“ã‚Œã‚’å®Ÿç¾ã€‚

**ä»®èª¬2: è¨ˆç®—åŠ¹ç‡ã®æœ€é©åŒ–**

Pure Transformer ã®é™ç•Œ (é•·ç³»åˆ—):

$$
O(N^2) \text{ Attention} \to \text{ãƒ¡ãƒ¢ãƒªãƒ»è¨ˆç®—ãŒçˆ†ç™º}
$$

SSMã¯ $O(N)$ ã§å¤§åŸŸçš„æ–‡è„ˆã‚’åœ§ç¸® â†’ Attentionã®è² è·å‰Šæ¸›ã€‚

**ç†è«–çš„æ çµ„ã¿: Universal Approximation + Efficiency**

$$
\begin{aligned}
\text{Hybrid} &= \text{Attention}(\text{high expressivity}) + \text{SSM}(\text{efficiency}) \\
&\approx \text{Turing complete} \cap O(N) \text{ average}
\end{aligned}
$$

**æ•°å­¦çš„è¨¼æ˜ (æ¦‚ç•¥)**:

1. **SSM ã¯ Context-Free Language (CFL) ã‚’èªè­˜å¯èƒ½** (Merrill+ 2023)
2. **Attention ã¯ Context-Sensitive Language (CSL) ã‚’èªè­˜å¯èƒ½** (Merrill+ 2022)
3. **Hybrid ã¯ CSL âˆª CFL** â†’ ã‚ˆã‚Šåºƒã„ã‚¯ãƒ©ã‚¹ã‚’ã‚«ãƒãƒ¼

```julia
# Theoretical expressivity comparison
function expressivity_score(model_type::Symbol)
    # Fictional metric: expressivity on various task classes
    scores = Dict(
        :pure_transformer => Dict(:cfl => 100, :csl => 100, :recall => 100, :efficiency => 30),
        :pure_ssm => Dict(:cfl => 95, :csl => 60, :recall => 40, :efficiency => 100),
        :hybrid => Dict(:cfl => 98, :csl => 95, :recall => 85, :efficiency => 80)
    )
    return scores[model_type]
end

println("\nExpressivity-Efficiency Trade-off")
println("â”"^70)
println("Model             | CFL | CSL | Recall | Efficiency | Overall |")
println("------------------|-----|-----|--------|------------|---------|")

for model in [:pure_transformer, :pure_ssm, :hybrid]
    scores = expressivity_score(model)
    overall = mean(values(scores))  # mean over all dimensions
    @printf("%-17s | %3d | %3d | %6d | %10d | %7.1f |\n",
            String(model), scores[:cfl], scores[:csl], scores[:recall], scores[:efficiency], overall)
end

println("\nğŸ¯ Hybrid dominates in overall score by balancing all dimensions")
```

å‡ºåŠ›:
```
Expressivity-Efficiency Trade-off
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Model             | CFL | CSL | Recall | Efficiency | Overall |
------------------|-----|-----|--------|------------|---------|
pure_transformer  | 100 | 100 |    100 |         30 |    82.5 |
pure_ssm          |  95 |  60 |     40 |        100 |    73.8 |
hybrid            |  98 |  95 |     85 |         80 |    89.5 |

ğŸ¯ Hybrid dominates in overall score by balancing all dimensions
```

#### 6.2.2 Frontier Models (2025-2026)

**Hymba (NVIDIA, ICLR 2025)**:

é©æ–°: **Hybrid-head** â€” åŒä¸€å±¤å†…ã§Attentionã¨SSMã‚’ä¸¦åˆ—å®Ÿè¡Œã€‚

$$
\mathbf{y} = \alpha \cdot \text{Attention}(\mathbf{x}) + \beta \cdot \text{SSM}(\mathbf{x}) + \gamma \cdot \text{MLP}(\mathbf{x})
$$

where $\alpha, \beta, \gamma$ ã¯å­¦ç¿’å¯èƒ½ãªé‡ã¿ã€‚

**åˆ©ç‚¹**:
- Layerå˜ä½ã§ã¯ãªãã€**headå˜ä½**ã§æ··åˆ â†’ ãã‚ç´°ã‹ã„åˆ¶å¾¡
- Attention headæ•°ã‚’æ¸›ã‚‰ã—ã€SSM headã§è£œå®Œ â†’ è¨ˆç®—é‡å‰Šæ¸›

**Hymba vs Llama-3.2-3B**:

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | Llama-3.2-3B | Hymba (3B) | æ”¹å–„ |
|:----------|:-------------|:-----------|:-----|
| Accuracy (avg) | 65.0% | **66.3%** | +1.3% |
| KV-Cache size | 1.0x | **0.086x** | 11.67xå‰Šæ¸› |
| Throughput | 1.0x | **3.49x** | 3.49xé«˜é€Ÿ |

**Samba (Microsoft, æœªå…¬é–‹è©³ç´°)**:

MoE + SSM + Attention ã®3è¦ç´ çµ±åˆã€‚å ±å‘Šã«ã‚ˆã‚Œã°:
- çŸ­ç³»åˆ—: Transformerè¶…ãˆ
- é•·ç³»åˆ— (220K+): SSMã§åŠ¹ç‡çš„å‡¦ç†

**äºˆæ¸¬: 2026å¹´å¾ŒåŠã®ãƒˆãƒ¬ãƒ³ãƒ‰**:
1. **Adaptive Hybrid**: å…¥åŠ›ã«å¿œã˜ã¦å‹•çš„ã«Attn/SSMæ¯”ç‡å¤‰æ›´
2. **Hardware-aware Hybrid**: GPU/TPUç‰¹æ€§ã«æœ€é©åŒ–ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³
3. **Multi-modal Hybrid**: Vision/Audio ã§ç•°ãªã‚‹Hybridè¨­è¨ˆ

### 6.3 Neural Architecture Search (NAS) for Hybrid

Hybridè¨­è¨ˆç©ºé–“ã¯åºƒå¤§ â†’ æ‰‹å‹•æ¢ç´¢ã¯éåŠ¹ç‡ â†’ **NAS**ã§è‡ªå‹•æ¢ç´¢ã€‚

#### 6.3.1 NAS Formulation

**ç›®çš„**: æœ€é©ãªHybridè¨­è¨ˆ $\alpha^*$ ã‚’è¦‹ã¤ã‘ã‚‹ã€‚

$$
\begin{aligned}
\alpha^* &= \arg\min_{\alpha \in \mathcal{A}} \mathcal{L}_\text{val}(\alpha, w^*(\alpha)) \\
\text{where } w^*(\alpha) &= \arg\min_{w} \mathcal{L}_\text{train}(\alpha, w)
\end{aligned}
$$

**è¨­è¨ˆç©ºé–“** $\mathcal{A}$:
- Layer type per layer: $\{\text{Attention}, \text{SSM}\}^L$
- Attention headæ•°: $\{1, 2, 4, 8, 16, 32\}$
- SSM state dim: $\{8, 16, 32, 64\}$
- Shared weights: $\{\text{Yes}, \text{No}\}$

**æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
1. **DARTS** [^10]: å¾®åˆ†å¯èƒ½NAS â€” é‡ã¿ä»˜ãå’Œ $\alpha_i \cdot \text{op}_i(\mathbf{x})$ ã§ç·©å’Œ
2. **Evolutionary Search**: éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  â€” mutation/crossover
3. **Reinforcement Learning**: ENAS [^11] â€” RNNã§ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç”Ÿæˆ
4. **Random Search + Early Stopping**: æ„å¤–ã¨åŠ¹æœçš„

```julia
# Pseudo-code: NAS for Hybrid design
function nas_hybrid_search(n_trials::Int=100)
    best_arch = nothing
    best_val_loss = Inf

    for trial in 1:n_trials
        # Sample architecture
        arch = sample_architecture()

        # Train briefly (proxy task)
        model = build_model(arch)
        train!(model, train_data, epochs=5)

        # Validate
        val_loss = evaluate(model, val_data)

        if val_loss < best_val_loss
            best_val_loss = val_loss
            best_arch = arch
        end

        println("Trial $trial: val_loss=$val_loss, arch=$arch")
    end

    return best_arch
end

# Random sampling from design space (0-50% attention, 24 layers)
sample_architecture() = (L=24, r_attn=rand() * 0.5, pattern=rand([:alternation, :shared, :local_global]))
```

**èª²é¡Œ**: NAS ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆå¤§ (100+ trials Ã— è¨“ç·´)ã€‚**Proxy task** (å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«) ã§åˆæœŸæ¢ç´¢ â†’ æœ¬ç•ªã§ fine-tuneã€‚

#### 6.3.2 AutoML for Hybrid: æœ€æ–°å‹•å‘

| æ‰‹æ³• | ç‰¹å¾´ | é©ç”¨ä¾‹ |
|:-----|:-----|:------|
| **One-Shot NAS** | 1å›ã®è¨“ç·´ã§ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | SPOS [^12] |
| **Weight Sharing** | å…¨å€™è£œã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰ | ENAS [^11] |
| **Hyperband** | Early stopping Ã— Random search | AutoML-Zero [^13] |
| **Neural Predictor** | å°è¦æ¨¡ã§æ€§èƒ½äºˆæ¸¬ â†’ æœ¬ç•ªè¨“ç·´å‰Šæ¸› | BANANAS [^14] |

**æœªæ¥ã®æ–¹å‘æ€§**: Hybrid NASã§ã€ã‚¿ã‚¹ã‚¯ç‰¹åŒ–å‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è‡ªå‹•ç”Ÿæˆã€‚

### 6.4 Dynamic Hybrid: ã‚¿ã‚¹ã‚¯é©å¿œçš„åˆ‡æ›¿

**ç¾çŠ¶ã®Hybrid**: å›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³ (Jamba: å¸¸ã«8å±¤ã«1å±¤Attention)

**æ¬¡ä¸–ä»£**: **å‹•çš„åˆ‡æ›¿** â€” å…¥åŠ›ãƒ»ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦Attention/SSMã‚’é¸æŠã€‚

#### 6.4.1 Dynamic Routing

$$
\text{Layer}_l(\mathbf{x}) = \begin{cases}
\text{Attention}(\mathbf{x}) & \text{if } g(\mathbf{x}) > \tau \\
\text{SSM}(\mathbf{x}) & \text{otherwise}
\end{cases}
$$

where $g(\mathbf{x})$ ã¯ "Attentionå¿…è¦åº¦" ã‚¹ã‚³ã‚¢:

$$
g(\mathbf{x}) = \sigma(\mathbf{w}^\top \mathbf{h}_{\text{global}}(\mathbf{x}))
$$

**è¨“ç·´**: $g$ ã‚‚å­¦ç¿’å¯èƒ½ â†’ Gumbel-Softmax relaxation [^15]ã€‚

```julia
# Dynamic routing pseudo-code
function dynamic_hybrid_layer(x::Matrix{Float64}, w_gate::Vector{Float64}, threshold::Float64=0.5)
    h_global   = mean(x, dims=1)                       # (1, d) global feature
    gate_score = sigmoid(dot(w_gate, vec(h_global)))   # scalar routing score
    gate_score > threshold ? attention_layer(x) : ssm_layer(x)  # "need attention" : "SSM sufficient"
end

sigmoid(x) = 1.0 / (1.0 + exp(-x))
```

**åˆ©ç‚¹**:
- **Adaptive**: ç°¡å˜ãªå…¥åŠ› â†’ SSM (é«˜é€Ÿ)ã€è¤‡é›‘ãªå…¥åŠ› â†’ Attention (é«˜ç²¾åº¦)
- **Efficiency**: å¹³å‡è¨ˆç®—é‡å‰Šæ¸›

**èª²é¡Œ**:
- Gateå­¦ç¿’ã®é›£ã—ã• (å‹¾é…æ¶ˆå¤±)
- æ¨è«–æ™‚ã®åˆ†å²äºˆæ¸¬ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

#### 6.4.2 Mixture of Hybrid Experts (MoHE)

MoE [^16] + Hybrid ã®èåˆ:

$$
\mathbf{y} = \sum_{i=1}^{K} p_i(\mathbf{x}) \cdot \text{Expert}_i(\mathbf{x})
$$

where $\text{Expert}_i$ ã¯ç•°ãªã‚‹Hybridè¨­è¨ˆ (r_attn_i, pattern_i)ã€‚

**ä¾‹**:
- Expert 1: Attention-heavy ($r=0.5$) â€” Few-shot tasks
- Expert 2: SSM-heavy ($r=0.1$) â€” Long context
- Expert 3: Balanced ($r=0.25$) â€” General

Router $p_i(\mathbf{x})$ ãŒå…¥åŠ›ã«å¿œã˜ã¦å°‚é–€å®¶ã‚’é¸æŠã€‚

**å®Ÿè£…æ¦‚å¿µ**:

```julia
# Mixture of Hybrid Experts (MoHE)
struct MoHELayer
    experts::Vector{HybridExpert}  # K experts with different r_attn
    router::Matrix{Float64}        # Router weights
end

struct HybridExpert
    r_attn::Float64  # Attention ratio for this expert
    layers::Vector{Dict}
end

function mohe_forward(mohe::MoHELayer, x::Matrix{Float64})
    probs = softmax(mean(x, dims=1) * mohe.router, dims=2)  # (1, K) router scores
    # Weighted sum over experts (generator accumulation, zero temp alloc)
    sum(probs[i] .* hybrid_forward(expert, x) for (i, expert) in enumerate(mohe.experts))
end

# Initialize MoHE with 3 experts
experts = [
    HybridExpert(0.5, []),   # Attention-heavy
    HybridExpert(0.1, []),   # SSM-heavy
    HybridExpert(0.25, [])   # Balanced
]

mohe = MoHELayer(experts, randn(64, 3) / sqrt(64))

println("MoHE initialized with $(length(experts)) experts")
println("  Expert 1: r_attn=0.5 (Attention-heavy for Few-shot)")
println("  Expert 2: r_attn=0.1 (SSM-heavy for Long context)")
println("  Expert 3: r_attn=0.25 (Balanced for General)")
```

**MoHE ã®åˆ©ç‚¹**:

1. **Task-specific optimization**: å„ExpertãŒç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–
2. **Load balancing**: RouterãŒè‡ªå‹•çš„ã«è² è·åˆ†æ•£
3. **Graceful degradation**: 1ã¤ã®ExpertãŒå¼±ãã¦ã‚‚ã€ä»–ãŒã‚«ãƒãƒ¼

**èª²é¡Œ**:

1. **Expert collapse**: ä¸€éƒ¨ã®Expertã®ã¿ä½¿ç”¨ã•ã‚Œã‚‹ (Switch Transformer [^16] ã®å•é¡Œ)
2. **Routing overhead**: Routerè¨ˆç®—ã®è¿½åŠ ã‚³ã‚¹ãƒˆ
3. **è¨“ç·´ä¸å®‰å®šæ€§**: è¤‡æ•°Expertã®åŒæ™‚æœ€é©åŒ–

#### 6.4.3 Continuous Hybrid: å¾®åˆ†å¯èƒ½ãªArchitectureé¸æŠ

Dynamic Routingã®æ¥µé™: **é€£ç¶šçš„ãªArchitectureé¸æŠ**ã€‚

$$
\mathbf{y} = \int_{r \in [0,1]} p(r \mid \mathbf{x}) \cdot \text{Hybrid}_r(\mathbf{x}) \, dr
$$

å®Ÿè£…ã¯é›¢æ•£åŒ–:

$$
\mathbf{y} \approx \sum_{i=1}^{M} p(r_i \mid \mathbf{x}) \cdot \text{Hybrid}_{r_i}(\mathbf{x})
$$

where $r_i \in \{0, 0.1, 0.2, \ldots, 1.0\}$ã€‚

**DARTS [^10] é¢¨ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:

$$
\begin{aligned}
\alpha_i &= \frac{\exp(w_i)}{\sum_j \exp(w_j)} \quad \text{(Gumbel-Softmax)} \\
\mathbf{y} &= \sum_{i=1}^{M} \alpha_i \cdot \text{Hybrid}_{r_i}(\mathbf{x})
\end{aligned}
$$

è¨“ç·´ä¸­ã« $w_i$ ã‚’å­¦ç¿’ â†’ æœ€é©ãª $r$ ã‚’è‡ªå‹•ç™ºè¦‹ã€‚

**åˆ©ç‚¹**: äººæ‰‹ã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ä¸è¦

**èª²é¡Œ**: ãƒ¡ãƒ¢ãƒªæ¶ˆè²»å¤§ (å…¨å€™è£œã‚’åŒæ™‚ä¿æŒ)

### 6.5 Recommended Books & Resources

#### Books

| æ›¸ç± | è‘—è€… | å†…å®¹ | é–¢é€£ |
|:-----|:-----|:-----|:-----|
| **Attention Is All You Need** | Vaswani+ (2017) | TransformeråŸè«–æ–‡ | Lec 14åŸºç¤ |
| **Deep Learning** | Goodfellow+ (2016) | DLæ•™ç§‘æ›¸ã€RNN/CNNåŸºç¤ | Lec 9åŸºç¤ |
| **Probabilistic Machine Learning** | Murphy (2022-2023) | ãƒ™ã‚¤ã‚ºMLå®Œå…¨ç‰ˆ | Course Iç¢ºç‡è«– |
| **State Space Models (survey)** | Somvanshi+ (2025) | S4â†’Mambaã‚µãƒ¼ãƒ™ã‚¤ | [arXiv:2503.18970](https://arxiv.org/abs/2503.18970) [^6] |

#### Online Resources

| ãƒªã‚½ãƒ¼ã‚¹ | URL | å†…å®¹ |
|:---------|:----|:-----|
| **Jamba Blog** | [ai21.com/blog](https://www.ai21.com/blog/announcing-jamba/) | Jambaè¨­è¨ˆè§£èª¬ |
| **Zamba GitHub** | [github.com/Zyphra/Zamba2](https://github.com/Zyphra/Zamba2) | Zambaå®Ÿè£… |
| **Mambaå…¬å¼** | [github.com/state-spaces/mamba](https://github.com/state-spaces/mamba) | Mambaå®Ÿè£…ãƒ»è«–æ–‡ |
| **FlashAttention** | [github.com/Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention) | IOæœ€é©åŒ– |

#### Research Papers (2024-2026)

è¿½åŠ ã§èª­ã‚€ã¹ãè«–æ–‡:

1. **Hymba** (ICLR 2025) [^6]: Hybrid-head architecture (åŒä¸€å±¤å†…ã§Attn//SSM)
2. **Long-context SSM** [arXiv:2507.12442](https://arxiv.org/abs/2507.12442): SSM hybridé•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ€§èƒ½åˆ†æ
3. **Samba** (æœªå…¬é–‹): Microsoft MoE+SSM+Attn hybrid
4. **CPA O(n log n) Attention** (Nature 2025): æº–ç·šå½¢Attentionè¿‘ä¼¼

### 6.6 ç”¨èªé›† (Lecture 18)

| ç”¨èª | å®šç¾© |
|:-----|:-----|
| **Hybrid Architecture** | Attentionã¨SSMã‚’åŒä¸€ãƒ¢ãƒ‡ãƒ«å†…ã§çµ„ã¿åˆã‚ã›ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ |
| **Layer Alternation** | Attentionå±¤ã¨SSMå±¤ã‚’äº¤äº’é…ç½®ã™ã‚‹è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ (Jamba) |
| **Shared Attention** | è¤‡æ•°ã®SSMå±¤ã§1ã¤ã®Attentionå±¤ã‚’å…±æœ‰ã™ã‚‹è¨­è¨ˆ (Zamba) |
| **Local Attention** | è¿‘å‚ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å‚ç…§ã™ã‚‹Attention ($O(N \cdot w)$) (Griffin) |
| **Gated Linear Recurrence** | Gatingæ©Ÿæ§‹ä»˜ãã®ç·šå½¢RNN (Griffin/Hawk) |
| **Hyena** | Gated convolutionãƒ™ãƒ¼ã‚¹ã®SSMé¡ä¼¼æ‰‹æ³• (StripedHyena) |
| **Attention=SSM Duality** | Attentionè¡Œåˆ—ã¨SSMãŒæ•°å­¦çš„ã«ç­‰ä¾¡ (Mamba-2/SSDè¨¼æ˜) |
| **Pareto Optimal** | è¤‡æ•°ç›®çš„ (æ€§èƒ½ãƒ»åŠ¹ç‡) ã§æ”¹å–„ä½™åœ°ãªã— |
| **Associative Recall** | Key-Valueæ¤œç´¢ã‚¿ã‚¹ã‚¯ã€‚AttentionãŒå¾—æ„ã€SSMãŒè‹¦æ‰‹ |
| **Semi-Separable Matrix** | $A_{ij} = L_i R_j^\top$ (ä¸‹ä¸‰è§’) å½¢å¼ã®è¡Œåˆ— (SSD) |
| **Dynamic Routing** | å…¥åŠ›ã«å¿œã˜ã¦Attention/SSMã‚’å‹•çš„é¸æŠ |
| **MoE (Mixture of Experts)** | è¤‡æ•°ã®å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚° |
| **Neural Architecture Search (NAS)** | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è‡ªå‹•æ¢ç´¢ |
| **Hybrid-head** | åŒä¸€å±¤å†…ã§Attentionã¨SSMã‚’ä¸¦åˆ—å®Ÿè¡Œ (Hymba) |
| **MoHE (Mixture of Hybrid Experts)** | ç•°ãªã‚‹Hybridè¨­è¨ˆã‚’æŒã¤è¤‡æ•°Expertã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚° |
| **Continuous Hybrid** | å¾®åˆ†å¯èƒ½ãªé€£ç¶šçš„Architectureé¸æŠ |
| **DARTS (Differentiable Architecture Search)** | å¾®åˆ†å¯èƒ½NASæ‰‹æ³• |
| **Gumbel-Softmax** | é›¢æ•£é¸æŠã®å¾®åˆ†å¯èƒ½ç·©å’Œ |
| **Expert Collapse** | MoEã§ä¸€éƒ¨Expertã®ã¿ä½¿ç”¨ã•ã‚Œã‚‹å•é¡Œ |
| **Load Balancing** | Experté–“ã®è² è·åˆ†æ•£ |
| **Context-Free Language (CFL)** | æ–‡è„ˆè‡ªç”±è¨€èª (SSMãŒèªè­˜å¯èƒ½) |
| **Context-Sensitive Language (CSL)** | æ–‡è„ˆä¾å­˜è¨€èª (AttentionãŒèªè­˜å¯èƒ½) |
| **Turing Completeness** | ä»»æ„ã®è¨ˆç®—ã‚’å®Ÿè¡Œå¯èƒ½ãªèƒ½åŠ› |
| **Hardware-aware Design** | GPU/TPUç‰¹æ€§ã«æœ€é©åŒ–ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ |
| **Adaptive Hybrid** | å…¥åŠ›ãƒ»ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦å‹•çš„ã«Attn/SSMæ¯”ç‡å¤‰æ›´ |
| **Multi-modal Hybrid** | Vision/Audioãªã©ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã§ç•°ãªã‚‹Hybridè¨­è¨ˆ |

### 6.7 Knowledge Mindmap

```mermaid
mindmap
  root((Hybrid<br/>Architecture))
    Motivation
      Attentioné™ç•Œ<br/>O(NÂ²)
      SSMé™ç•Œ<br/>Recallå¼±ã„
      ç›¸è£œçš„
    Design Patterns
      Layer Alternation<br/>Jamba
      Shared Attention<br/>Zamba
      Local+Global<br/>Griffin
      Weighted Mix<br/>StripedHyena
    Theory
      Compute O(rLNÂ²d)
      Attention=SSM<br/>Duality
      Pareto Optimal
    Implementation
      Juliaè¨“ç·´
      Rustæ¨è«–
      Math-Code 1:1
    Future
      NAS
      Dynamic Routing
      MoE Hybrid
```

> **Note:** **é€²æ—: 95% å®Œäº†** ç ”ç©¶landscapeã€NASã€Dynamic Hybridã€å‚è€ƒæ–‡çŒ®ã‚’å®Œäº†ã—ãŸã€‚æœ€å¾Œã¯Zone 7 â€” Course IIæŒ¯ã‚Šè¿”ã‚Š + Course IIIäºˆå‘Šã€‚

---

### 6.8 ä»Šå›ã®å­¦ç¿’å†…å®¹

### 8.2 ğŸ† Course II: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ å®Œå…¨èª­äº†

**ãŠã‚ã§ã¨ã†ï¼** ç¬¬9å›ã‹ã‚‰å§‹ã¾ã£ãŸ10å›ã®æ—…è·¯ã‚’å®Œèµ°ã—ãŸã€‚

```mermaid
graph LR
    L9[ç¬¬9å›<br/>å¤‰åˆ†æ¨è«–] --> L10[ç¬¬10å›<br/>VAE]
    L10 --> L11[ç¬¬11å›<br/>æœ€é©è¼¸é€]
    L11 --> L12[ç¬¬12å›<br/>GAN]
    L12 --> L13[ç¬¬13å›<br/>è‡ªå·±å›å¸°]
    L13 --> L14[ç¬¬14å›<br/>Attention]
    L14 --> L15[ç¬¬15å›<br/>AttnåŠ¹ç‡åŒ–]
    L15 --> L16[ç¬¬16å›<br/>SSM&Mamba]
    L16 --> L17[ç¬¬17å›<br/>Mambaç™ºå±•]
    L17 --> L18[ç¬¬18å›<br/>Hybrid<br/>âœ…å®Œäº†]

    style L18 fill:#4caf50,color:#fff
```

### 8.3 åˆ°é”ç‚¹ã®ç¢ºèª â€” ãƒ“ãƒ•ã‚©ãƒ¼ãƒ»ã‚¢ãƒ•ã‚¿ãƒ¼

**Before Course II** (ç¬¬8å›çµ‚äº†æ™‚ç‚¹):
- âŒ ã€ŒVAEã®ELBOå°å‡ºãŒåˆ†ã‹ã‚‰ãªã„ã€
- âŒ ã€ŒGANã®è¨“ç·´ãŒä¸å®‰å®šãªç†ç”±ãŒè¬ã€
- âŒ ã€ŒAttentionã®è¨ˆç®—é‡ãŒ$O(N^2)$ãªã®ã¯çŸ¥ã£ã¦ã‚‹ã‘ã©ã€ãªãœï¼Ÿã€
- âŒ ã€ŒMambaã¨ã‹SSMã£ã¦ä½•ï¼Ÿèã„ãŸã“ã¨ãªã„ã€
- âŒ ã€Œè«–æ–‡ã®æ‰‹æ³•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå‘ªæ–‡ã«ã—ã‹è¦‹ãˆãªã„ã€

**After Course II** (ç¬¬18å›å®Œäº†æ™‚ç‚¹):
- âœ… **ELBOå°å‡ºã‚’3é€šã‚Šã®æ–¹æ³• (Jensen/KLåˆ†è§£/é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°) ã§èª¬æ˜ã§ãã‚‹**
- âœ… **GANè¨“ç·´ã®Nashå‡è¡¡ãƒ»Mode Collapseãƒ»WGAN-GPã®ç†è«–çš„æ ¹æ‹ ã‚’è¨¼æ˜ã§ãã‚‹**
- âœ… **Attentionã®$QK^\top/\sqrt{d_k}$ã‚’è¡Œåˆ—æ¼”ç®—ã¨ã—ã¦å®Œå…¨ç†è§£ã€FlashAttentionã®Tilingæˆ¦ç•¥ã‚‚èª¬æ˜ã§ãã‚‹**
- âœ… **Mambaã®Selective SSMã€HiPPOç†è«–ã€Attention=SSMåŒå¯¾æ€§ (SSD) ã‚’æ•°å¼ã§å°å‡ºã§ãã‚‹**
- âœ… **Jamba/Zamba/Griffinã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¯”è¼ƒã—ã€Paretoæœ€é©ã®æ¦‚å¿µã§è©•ä¾¡ã§ãã‚‹**
- âœ… **è«–æ–‡ã®æ‰‹æ³•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’èª­ã‚“ã§ã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œã§å®Ÿè£…ã§ãã‚‹**

**å¤‰åŒ–ã®æœ¬è³ª**: ã€Œæ‰‹æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹ã€â†’ã€Œ**ç†è«–ã‚’å°å‡ºã—ã€å®Ÿè£…ã—ã€è©•ä¾¡ã§ãã‚‹**ã€

### 8.4 ğŸâ†’ğŸ¦€â†’âš¡ è¨€èªç§»è¡Œã®æŒ¯ã‚Šè¿”ã‚Š

Course IIã¯**ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬æˆ¦è¡“**ã§Pythonâ†’Julia/Rustã¸ç§»è¡Œã—ãŸæ—…ã§ã‚‚ã‚ã£ãŸã€‚

| Lecture | è¨€èªæ§‹æˆ | ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ |
|:--------|:---------|:-------------|
| ç¬¬9å› | ğŸ50% ğŸ¦€åˆç™»å ´ | **Rustç™»å ´**: Pythonâ†’Rust 50xé«˜é€ŸåŒ–ã®è¡æ’ƒ |
| ç¬¬10å› | ğŸ30% âš¡Juliaåˆç™»å ´ ğŸ¦€ | **Juliaç™»å ´**: å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§æ•°å¼ãŒå‹ã«å¿œã˜ã¦æœ€é©åŒ– |
| ç¬¬11å› | âš¡Juliaä¸»å½¹ ğŸ¦€ | OT/Wassersteinå®Ÿè£…ã§Juliaæœ¬æ ¼æ´»ç”¨ |
| ç¬¬12-13å› | âš¡è¨“ç·´ ğŸ¦€æ¨è«– | GAN/ARè¨“ç·´=Juliaã€æ¨è«–=Ruståˆ†æ¥­ç¢ºç«‹ |
| ç¬¬14-15å› | âš¡ğŸ¦€ | Attentionå®Ÿè£…ã§Julia/Rustä¸¡è¼ª |
| ç¬¬16-17å› | âš¡ğŸ¦€ | SSM/Mambaå®Ÿè£…ã§Juliaæ•°å€¤è¨ˆç®—ã®å¨åŠ› |
| ç¬¬18å› | âš¡ğŸ¦€ (ğŸæ¶ˆæ»…) | **Pythonã¯éå»ã«**ã€‚Julia/RustãŒæ¨™æº– |

**å­¦ã³**:
- **Julia**: æ•°å¼â†’ã‚³ãƒ¼ãƒ‰1:1ã€REPLé§†å‹•é–‹ç™ºã€å‹å®‰å®šæ€§ãŒç”Ÿç”£æ€§ã‚’10å€ã«
- **Rust**: ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã€æ‰€æœ‰æ¨©ã€å‹å®‰å…¨ãŒæ¨è«–ã‚’100å€é«˜é€ŸåŒ–
- **Python**: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—å°‚ç”¨ã€‚æœ¬ç•ªã¯Julia/Rust

**æ„Ÿæƒ³** (fictional student voice):
> ã€Œæœ€åˆã¯ã€Pythonã§ååˆ†ã€ã¨æ€ã£ã¦ãŸã€‚ã§ã‚‚ç¬¬9å›ã§Rustã®50xé«˜é€ŸåŒ–ã‚’è¦‹ã¦ã€ç¬¬10å›ã§Juliaã®æ•°å¼ç¾ã«è§¦ã‚Œã¦ã€ã‚‚ã†æˆ»ã‚Œãªã„ã€‚Pythonã¯"ä¾¿åˆ©"ã ã‘ã©"é…ã„"ã—"å‹ãŒãªã„"ã€‚Julia/Rustã¯"é€Ÿã„"ã—"å®‰å…¨"ã€‚Course IIIã§ã“ã®2è¨€èªã‚’æ­¦å™¨ã«å®Ÿè·µã™ã‚‹ã€

### 8.5 ç†è«–ã®çµ±ä¸€çš„ç†è§£

Course IIã§å­¦ã‚“ã å…¨ã¦ãŒ **ã¤ãªãŒã£ã¦ã„ã‚‹**ã€‚

| å› | ã‚³ã‚¢æ¦‚å¿µ | çµ±ä¸€çš„è¦–ç‚¹ |
|:---|:---------|:----------|
| 9 | ELBO | **å¤‰åˆ†æ¨è«– = å°¤åº¦ä¸‹ç•Œæœ€å¤§åŒ–** |
| 10 | VAE | ELBO + NN â†’ **è‡ªå‹•å¤‰åˆ†æ¨è«–** |
| 11 | OT | **ç¢ºç‡æ¸¬åº¦é–“ã®è·é›¢ = æœ€å°è¼¸é€ã‚³ã‚¹ãƒˆ** |
| 12 | GAN | Nashå‡è¡¡ = **MinMax Game** |
| 13 | AR | **é€£é–å¾‹åˆ†è§£ = å°¤åº¦è¨ˆç®—å¯èƒ½** |
| 14 | Attention | **å…¨ç³»åˆ—å‚ç…§ = $O(N^2)$ ã®ä»£å„Ÿ** |
| 15 | AttentionåŠ¹ç‡åŒ– | Flash/Sparse/Linear = **$O(N^2)$å›é¿ã®è©¦ã¿** |
| 16 | SSM/Mamba | **çŠ¶æ…‹ç©ºé–“ = ç·šå½¢æ™‚é–“è¨˜æ†¶** |
| 17 | Mamba-2/SSD | **Attention=SSM = åŒã˜ã‚‚ã®ã®ç•°ãªã‚‹è¡¨ç¾** |
| 18 | Hybrid | **ç›¸è£œçš„çµ„ã¿åˆã‚ã› = Paretoæœ€é©** |

**å¤§çµ±ä¸€**: å…¨ã¦ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ $p_\theta(x)$ or $p_\theta(x,z)$ ã®å­¦ç¿’ã€‚å¤‰åˆ†æ¨è«–ãƒ»OTãƒ»Nashå‡è¡¡ã¯ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§åŒã˜ã‚´ãƒ¼ãƒ«ã‚’ç›®æŒ‡ã™ã€‚

### 8.6 Course Iã®æ•°å­¦ â€” æ´»ç”¨ã®å®Ÿä¾‹

Course I (ç¬¬1-8å›) ã®æ•°å­¦ãŒã€Course IIã§ã©ã†ä½¿ã‚ã‚ŒãŸã‹:

| Course I | Course II ã§ã®æ´»ç”¨ |
|:---------|:------------------|
| **ç·šå½¢ä»£æ•°** (ç¬¬2-3å›) | Attention $QK^\top$ã€SVD (æ½œåœ¨ç©ºé–“)ã€è¡Œåˆ—å¾®åˆ† (Backprop) |
| **ç¢ºç‡è«–** (ç¬¬4å›) | VAEäº‹å¾Œåˆ†å¸ƒã€GANåˆ†å¸ƒãƒãƒƒãƒãƒ³ã‚°ã€ARå°¤åº¦ |
| **æ¸¬åº¦è«–** (ç¬¬5å›) | OT (æ¸¬åº¦é–“è·é›¢)ã€Diffusion (ç¢ºç‡æ¸¬åº¦ã®æµã‚Œ) |
| **æƒ…å ±ç†è«–** (ç¬¬6å›) | ELBO (KLé …)ã€GAN (JSD)ã€Rate-Distortion |
| **æœ€é©åŒ–** (ç¬¬7å›) | GANè¨“ç·´ (Nashå‡è¡¡)ã€VAEè¨“ç·´ (å‹¾é…é™ä¸‹) |
| **æ½œåœ¨å¤‰æ•°** (ç¬¬8å›) | VAE (å¤‰åˆ†æ¨è«–)ã€GAN (æš—é»™çš„æ½œåœ¨å¤‰æ•°) |

**å…¨ã¦ãŒã¤ãªãŒã‚‹**: Course Iã¯"éƒ¨å“"ã€Course IIã¯"çµ„ã¿ç«‹ã¦"ã€‚

### 8.7 FAQ â€” ã‚ˆãã‚ã‚‹è³ªå•

#### Q1: Hybridã¯å¸¸ã«Pure Attentionã‚„SSMã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã®ã‹ï¼Ÿ

**A**: **No**ã€‚ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚

- **Pure Attentionå„ªä½**: Few-shot learningã€è¤‡é›‘ãªæ¨è«– (CoT)ã€çŸ­ç³»åˆ— ($N < 1024$)
- **Pure SSMå„ªä½**: è¶…é•·ç³»åˆ— ($N > 100K$)ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã€ãƒ¡ãƒ¢ãƒªåˆ¶ç´„å³ã—ã„ç’°å¢ƒ
- **Hybridå„ªä½**: ãƒãƒ©ãƒ³ã‚¹å‹ã‚¿ã‚¹ã‚¯ (é•·æ–‡è¦ç´„ã€å¯¾è©±ã€æ±ç”¨LM)

**No Free Lunchå®šç†**: ä¸‡èƒ½ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å­˜åœ¨ã—ãªã„ã€‚

#### Q2: Jambaã¨Zambaã®ã©ã¡ã‚‰ã‚’é¸ã¶ã¹ãã‹ï¼Ÿ

**A**: **ç”¨é€”æ¬¡ç¬¬**ã€‚

- **Jamba**: MoEã§å¤§è¦æ¨¡ (52B total)ã€256K contextã€æ±ç”¨LLM
- **Zamba**: Compact (7B)ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ã€ãƒ‡ãƒã‚¤ã‚¹åˆ¶ç´„ç’°å¢ƒ

#### Q3: Hybridå®Ÿè£…ã¯é›£ã—ã„ã®ã‹ï¼Ÿ

**A**: **ä¸­ç¨‹åº¦**ã€‚

- Attentionå®Ÿè£…çµŒé¨“ã‚ã‚Š â†’ Hybridè¿½åŠ ã¯å®¹æ˜“ (SSMå±¤ã‚’æŒ¿å…¥ã™ã‚‹ã ã‘)
- SSMå®Ÿè£… (Mamba) ã¯è¤‡é›‘ â†’ **æ—¢å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨æ¨å¥¨** (`mamba-ssm`, `transformers`)

#### Q4: Dynamic Hybridã¯å®Ÿç”¨åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ

**A**: **ã¾ã ç ”ç©¶æ®µéš** (2026å¹´2æœˆæ™‚ç‚¹)ã€‚

- Gateå­¦ç¿’ã®é›£ã—ã•ã€æ¨è«–æ™‚ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒèª²é¡Œ
- ä»Šå¾Œ2-3å¹´ã§å®Ÿç”¨åŒ–ã®å¯èƒ½æ€§

#### Q5: Course IIIã§ã¯ä½•ã‚’å­¦ã¶ã®ã‹ï¼Ÿ

**A**: **ç†è«–â†’å®Ÿè·µã®æ©‹æ¸¡ã—**ã€‚

- è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€/åˆ†æ•£è¨“ç·´)
- è©•ä¾¡æŒ‡æ¨™ (FID/LPIPS/Perplexity)
- ãƒ‡ãƒ—ãƒ­ã‚¤ (ONNX/é‡å­åŒ–/æœ€é©åŒ–)
- **Elixirç™»å ´** (ç¬¬19å›) â€” åˆ†æ•£æ¨è«–ãƒ»è€éšœå®³æ€§
- MLOps (Monitoring/Logging/A/Bãƒ†ã‚¹ãƒˆ)

### 8.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (å¾©ç¿’ & Course IIIæº–å‚™)

| é€± | å¾©ç¿’å†…å®¹ | æº–å‚™å†…å®¹ |
|:---|:---------|:---------|
| **Week 1** | ç¬¬9-12å›å¾©ç¿’ (VI/VAE/OT/GAN) | Julia/Rusté–‹ç™ºç’°å¢ƒæ•´å‚™ |
| **Week 2** | ç¬¬13-16å›å¾©ç¿’ (AR/Attention/SSM) | Elixirç’°å¢ƒæ§‹ç¯‰ (ç¬¬19å›æº–å‚™) |
| **Week 3** | ç¬¬17-18å›å¾©ç¿’ (Mambaç™ºå±•/Hybrid) | Course IIIç¬¬19å›äºˆç¿’ (åˆ†æ•£æ¨è«–) |
| **Week 4** | Course IIå…¨ä½“é€šã— | ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: Tiny Hybridå®Ÿè£… |

**ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¾‹**:
- Tiny Hybrid Model (Julia) ã‚’ MNIST è¨“ç·´
- Rustæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
- Pure Transformer/Mamba/Hybrid æ€§èƒ½æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ

### 8.9 æ¬¡å›äºˆå‘Š â€” Course III ç¬¬19å›: ç†è«–ã‹ã‚‰å®Ÿè£…ã¸

**ç¬¬19å›: ç’°å¢ƒæ§‹ç¯‰ & FFI & åˆ†æ•£åŸºç›¤ â€” 3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã®æ—…ãŒå§‹ã¾ã‚‹**

**Course IIå®Œçµã€Course IIIé–‹å¹•**: ç†è«–ã®ç¿’å¾—ã¯å®Œäº†ã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã ã€‚Course IIIï¼ˆç¬¬19-32å›ã€å…¨14å›ï¼‰ã§ã¯ã€âš¡Juliaè¨“ç·´ãƒ»ğŸ¦€Rustæ¨è«–ãƒ»ğŸ”®Elixiré…ä¿¡ã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

ğŸ”® **Elixiråˆç™»å ´**: ç¬¬19å›ã§BEAM VMä¸Šã®é–¢æ•°å‹è¨€èªElixirãŒç™»å ´ã™ã‚‹ã€‚åˆ†æ•£ãƒ»ä¸¦è¡Œãƒ»è€éšœå®³æ€§ãŒè¨€èªãƒ¬ãƒ™ãƒ«ã§çµ„ã¿è¾¼ã¾ã‚Œã€Productionå“è³ªã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚’å®Ÿç¾ã™ã‚‹ã€‚

**Course IIIå…¨ä½“åƒï¼ˆç¬¬19-32å›ï¼‰**:
- **åŸºç›¤ç·¨ï¼ˆL19-22ï¼‰**: ç’°å¢ƒæ§‹ç¯‰ãƒ»VAE/GAN/Transformerå®Ÿè£…ãƒ»ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«
- **æœ€é©åŒ–ç·¨ï¼ˆL23-26ï¼‰**: Fine-tuningãƒ»PEFTãƒ»çµ±è¨ˆå­¦ãƒ»å› æœæ¨è«–ãƒ»æ¨è«–æœ€é©åŒ–
- **å®Ÿè·µç·¨ï¼ˆL27-30ï¼‰**: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ»RAGãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
- **é‹ç”¨ç·¨ï¼ˆL31-32ï¼‰**: MLOpsãƒ»Productionçµ±åˆ

**æº–å‚™äº‹é …**:
1. Julia 1.11+ / Rust 1.83+ / Elixir 1.17+ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
2. FFIæ¦‚å¿µã®å¾©ç¿’ï¼ˆç¬¬9-18å›ã§æ—¢å‡ºï¼‰
3. å®Ÿè£…ç’°å¢ƒã®æ§‹ç¯‰æº–å‚™ï¼ˆç¬¬19å›ã§è©³ç´°è§£èª¬ï¼‰

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰
>
> **Course II: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ å®Œå…¨èª­äº†ï¼**
>
> 10å›ã®æ—…è·¯ã§ã€å¤‰åˆ†æ¨è«–ãƒ»VAEãƒ»æœ€é©è¼¸é€ãƒ»GANãƒ»è‡ªå·±å›å¸°ãƒ»Attentionãƒ»SSMãƒ»Mambaãƒ»Hybridã®ç†è«–ã¨å®Ÿè£…ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚
>
> **ã€Œè«–æ–‡ãŒèª­ã‚ã‚‹ã€â†’ã€Œè«–æ–‡ãŒæ›¸ã‘ã‚‹ã€ãƒ¬ãƒ™ãƒ«ã«åˆ°é”ã€‚**
>
> æ¬¡ã¯Course IIIã€Œå®Ÿè·µç·¨ã€ã§ã€ç†è«–ã‚’ã€Œå‹•ãã‚·ã‚¹ãƒ†ãƒ ã€ã«å¤‰ãˆã‚‹æŠ€è¡“ã‚’èº«ã«ã¤ã‘ã‚‹ã€‚
>
> ğŸš€ **Let's dive into Course III!**

---

### 6.13 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

### å•ã„: "æœ€å¼·"ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å­˜åœ¨ã—ãªã„ã®ã‹ï¼Ÿ

Jambaã€Zambaã€Griffinã€StripedHyena â€” ã©ã‚Œã‚‚ã€Œãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã€ã‚’æ¨™æ¦œã™ã‚‹ã€‚ã ãŒã€**ã©ã‚ŒãŒ"æœ€å¼·"ãªã®ã‹ï¼Ÿ**

**ç­”ãˆ: "æœ€å¼·"ã¯å­˜åœ¨ã—ãªã„ã€‚**

ãªãœãªã‚‰:

1. **No Free Lunchå®šç†**: å…¨ã¦ã®ã‚¿ã‚¹ã‚¯ã§æœ€è‰¯ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å­˜åœ¨ã—ãªã„ã€‚ã‚¿ã‚¹ã‚¯Aã§æœ€è‰¯ â†’ ã‚¿ã‚¹ã‚¯Bã§åŠ£ã‚‹ã€‚
2. **Paretoæœ€é©**: æ€§èƒ½ãƒ»åŠ¹ç‡ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»è¨“ç·´æ™‚é–“ â€” è¤‡æ•°ç›®çš„ã§å…¨ã¦æœ€è‰¯ã¯ä¸å¯èƒ½ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®é¸æŠã€‚
3. **æ–‡è„ˆä¾å­˜**: çŸ­ç³»åˆ—ãªã‚‰Attentionã€è¶…é•·ç³»åˆ—ãªã‚‰SSMã€ãƒãƒ©ãƒ³ã‚¹ãªã‚‰Hybridã€‚ç”¨é€”æ¬¡ç¬¬ã€‚

**æœ¬è³ªçš„ãªå•ã„**: ã§ã¯ã€æˆ‘ã€…ã¯ä½•ã‚’ç›®æŒ‡ã™ã¹ãã‹ï¼Ÿ

**ç­”ãˆ: "çµ„ã¿åˆã‚ã›ã®æœ€é©åŒ–"**

- Attentionã®å…¨ç³»åˆ—å‚ç…§
- SSMã®åŠ¹ç‡çš„è¨˜æ†¶
- MoEã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£
- å‹•çš„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é©å¿œæ€§

**ã“ã‚Œã‚‰å…¨ã¦ã‚’ã€ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‹è¨­è¨ˆåŠ›**ã“ããŒã€æ¬¡ä¸–ä»£LLMã®éµã ã€‚

**æŒ‘ç™ºçš„ãªå•ã„**:
- Hybridã®"æ¬¡"ã¯ä½•ã‹ï¼Ÿ â†’ **Meta-Hybrid** (Hybridã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è‡ªä½“ã‚’å‹•çš„ç”Ÿæˆ)
- Attention=SSMåŒå¯¾æ€§ã®"æ¬¡"ã¯ï¼Ÿ â†’ **çµ±ä¸€ç†è«–** (Attention, SSM, Diffusion, Flow ã‚’1ã¤ã®æ çµ„ã¿ã§)
- äººé–“ã®è„³ã¯Hybridã‹ï¼Ÿ â†’ **ç¥çµŒç§‘å­¦ã¨ã®æ¥ç¶š** (è„³ã®ç•°ãªã‚‹é ˜åŸŸ = ç•°ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£?)

**æœ€å¾Œã®å•ã„**:
> ã€Œ"æœ€å¼·"ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¢ã™ã®ã§ã¯ãªãã€**"çµ„ã¿åˆã‚ã›"ã®åŠ›ã‚’ä¿¡ã˜ã‚‹ã“ã¨**ã€‚ã“ã‚ŒãŒAIç ”ç©¶ã®æ¬¡ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã§ã¯ãªã„ã‹ï¼Ÿã€

---

> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Neural Architecture Search (NAS) ãŒãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰SSMè¨­è¨ˆã«å¿œç”¨ã•ã‚ŒãŸå ´åˆã€æ¢ç´¢ç©ºé–“ã¯ã©ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ã‹ï¼Ÿ
> 2. Dynamic Hybridï¼ˆã‚¿ã‚¹ã‚¯é©å¿œçš„åˆ‡æ›¿ï¼‰ã¯é™çš„ãªLayeräº¤äº’é…ç½®ã¨æ¯”ã¹ã€ã©ã®ã‚ˆã†ãªã‚·ãƒŠãƒªã‚ªã§å„ªä½ã‹ï¼Ÿ

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Lieber, O., Lenz, B., et al. (2024). "Jamba: A Hybrid Transformer-Mamba Language Model". *arXiv:2403.19887*.
<https://arxiv.org/abs/2403.19887>

[^2]: Glorioso, P., Anthony, Q., et al. (2024). "Zamba: A Compact 7B SSM Hybrid Model". *arXiv:2405.16712*.
<https://arxiv.org/abs/2405.16712>

[^3]: De, S., Smith, S. L., et al. (2024). "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models". *arXiv:2402.19427*.
<https://arxiv.org/abs/2402.19427>

[^4]: Google DeepMind (2024). "RecurrentGemma: Moving Past Transformers for Efficient Open Language Models". *arXiv:2404.07839*.
<https://arxiv.org/abs/2404.07839>

[^5]: Together AI (2024). "StripedHyena: Paving the way to efficient architectures".
<https://www.together.ai/blog/stripedhyena-7b>

[^6]: Somvanshi, S., et al. (2025). "From S4 to Mamba: A Comprehensive Survey on Structured State Space Models". *arXiv:2503.18970*.
<https://arxiv.org/abs/2503.18970>

[^7]: Mitra, S., et al. (2025). "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length". *arXiv:2507.12442*.
<https://arxiv.org/abs/2507.12442>

[^8]: Vaswani, A., Shazeer, N., et al. (2017). "Attention Is All You Need". *NeurIPS 2017*.
<https://arxiv.org/abs/1706.03762>

[^9]: Gu, A., Goel, K., RÃ©, C. (2021). "Efficiently Modeling Long Sequences with Structured State Spaces". *ICLR 2022*.
<https://arxiv.org/abs/2111.00396>

[^10]: Liu, H., Simonyan, K., Yang, Y. (2018). "DARTS: Differentiable Architecture Search". *ICLR 2019*.
<https://arxiv.org/abs/1806.09055>

[^11]: Pham, H., Guan, M. Y., et al. (2018). "Efficient Neural Architecture Search via Parameter Sharing". *ICML 2018*.
<https://arxiv.org/abs/1802.03268>

[^12]: Guo, Z., Zhang, X., et al. (2020). "Single Path One-Shot Neural Architecture Search with Uniform Sampling". *ECCV 2020*.
<https://arxiv.org/abs/1904.00420>

[^13]: Real, E., Liang, C., et al. (2020). "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch". *ICML 2020*.
<https://arxiv.org/abs/2003.03384>

[^14]: White, C., Neiswanger, W., et al. (2021). "BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search". *AAAI 2021*.
<https://arxiv.org/abs/1910.11858>

[^15]: Jang, E., Gu, S., Poole, B. (2017). "Categorical Reparameterization with Gumbel-Softmax". *ICLR 2017*.
<https://arxiv.org/abs/1611.01144>

[^16]: Shazeer, N., Mirhoseini, A., et al. (2017). "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer". *ICLR 2017*.
<https://arxiv.org/abs/1701.06538>

### æ•™ç§‘æ›¸

- Murphy, K. P. (2022-2023). *Probabilistic Machine Learning: An Introduction / Advanced Topics*. MIT Press. [probml.github.io](https://probml.github.io/)
- Goodfellow, I., Bengio, Y., Courville, A. (2016). *Deep Learning*. MIT Press. [deeplearningbook.org](https://www.deeplearningbook.org/)
- Gu, A., et al. (2025). *State Space Models: From Classical Control to Modern Sequence Modeling*. (Survey paper, draft).

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

