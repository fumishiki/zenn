---
title: "第1回: 概論: 数式と論文の読み方 — 30秒の驚き→数式修行→実装マスター 【前編】理論編"
emoji: "🧭"
type: "tech"
topics: ["machinelearning", "deeplearning", "math", "statistics", "python"]
published: true
slug: "ml-lecture-01-part1"
difficulty: "beginner"
time_estimate: "90 minutes"
languages: ["Python"]
keywords: ["数式記法", "LaTeX", "Softmax", "Attention", "Cross-Entropy"]
---


# 第1回: 概論 — 数式と論文の読み方

> **数式が"読めない"のは才能ではなく語彙の問題。50記号を覚えれば論文が"読める"。**

## 🗺️ この回の地図（5トピック）

- **数式記法マスター**: ギリシャ文字 / 添字 / 演算子 / 集合 / 論理 / 関数 / 微分 / 確率（記号を「音」にして、意味に落とす）
- **Boss Battle: Attention完全読解**: `$\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(QK^\top/\sqrt{d_k})V$` を一文字残らず分解する
- **論文読解法**: arXiv / 3パスリーディング / 構造スキャン（Abstract→図→数式→実験）
- **LaTeX & 数式↔コード**: 数式の最小語彙と、翻訳の型（式→変数→shape→実装）
- **環境構築**: Python/IDE/ノートの型（手を動かすのは後編）

> **📖 この記事は前編（理論編）です**
> 実装編は [【後編】第1回: 概論 — ツールとワークフロー](/articles/ml-lecture-01-part2) をご覧ください。

---

この1文に「いやいや、そんなわけないだろ」と思っただろうか。気持ちはわかる。論文を開いて `$\mathcal{L}(\theta, \phi; \mathbf{x})$` のような記号が並ぶと、反射的に閉じたくなる。あの感覚を覚えている人は多いはずだ。

だが考えてほしい。英語を読めなかった頃、英字新聞は暗号に見えた。アルファベットを覚え、単語を覚え、文法を理解した今、それは「普通の文章」になっている。数式も同じだ。記号のアルファベットを覚え、記法の文法を理解すれば、論文は「著者の思考を追体験できるドキュメント」に変わる。

本講義は全50回の第1回 — 冒険のはじまりだ。ここでは数式を読む基礎体力と、論文を構造的に読解する技術を身につける。



**所要時間の目安**:

| ゾーン | 内容 | 時間 | 難易度 |
|:-------|:-----|:-----|:-------|
| Zone 1 | クイックスタート | 30秒 | ★☆☆☆☆ |
| Zone 2 | 体験ゾーン | 10分 | ★★☆☆☆ |
| Zone 3 | 直感ゾーン | 20分 | ★★☆☆☆ |
| Zone 4 | 数式修行ゾーン | 60分 | ★★★★☆ |

---

## 🚀 Z1. クイックスタート（30秒）— 数式は動かせる

**ゴール**: 数式が「読める」感覚を30秒で体験する。

以下のコードを実行してほしい。たった3行で、機械学習の中核にある数式を「動かせる」。

```python
import numpy as np

# Softmax: p_i = exp(x_i) / Σ_j exp(x_j)
logits = np.array([2.0, 1.0, 0.1])
probs = np.exp(logits) / np.sum(np.exp(logits))
print(f"logits: {logits}")
print(f"probs:  {np.round(probs, 4)}")
print(f"sum:    {np.sum(probs):.6f}")  # must be 1.0
```

出力:
```
logits: [2.  1.  0.1]
probs:  [0.6590 0.2424 0.0986]
sum:    1.000000
```

**この3行の裏にある数式**:

```math
p_i = \frac{\exp(x_i)}{\sum_{j=1}^{K} \exp(x_j)}
```

見てほしい。`np.exp(logits)` が `$\exp(x_i)$`、`np.sum(np.exp(logits))` が `$\sum_j \exp(x_j)$`。数式とコードが1対1で対応している。

このSoftmax関数は現代のLLMの心臓部だ。GPT、Claude、Gemini — 全てがこの関数を使って次のトークンの確率分布を計算している。Transformerの原論文 [^1] でもAttention機構の中核としてSoftmaxが使われている。

「え、数式ってコードに直せるの？」 — そう、直せる。全50回を通じて、この感覚を徹底的に鍛える。

> **Checkpoint:** ここまでで「数式 = コードで動かせる」を体感した。残りのゾーンで、同じ感覚を数式全般へ拡張する。

> Progress: 5%

---

## 🎮 Z2. 体験ゾーン（10分）— 数式を声に出して読む

### 1.1 数式を「声に出して読む」

数式を読めない最大の原因は、**声に出したことがない**からだ。英単語を覚えるとき発音しないで暗記する人はいない。数式も同じだ。

以下の数式を声に出して読んでみてほしい。

```math
\nabla_\theta \mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta \ell(f_\theta(\mathbf{x}_i), y_i)
```

**読み方**: 「ナブラ シータ エル シータ イコール イチ エヌ ぶんの シグマ アイ イコール イチ から エヌ ナブラ シータ スモール エル エフ シータ エックス アイ カンマ ワイ アイ」

......長い。だが構造は単純だ:

| 記号 | 読み | 意味 |
|:-----|:-----|:-----|
| `$\nabla_\theta$` | ナブラ シータ | パラメータ `$\theta$` に関する勾配 |
| `$\mathcal{L}(\theta)$` | エル シータ | 損失関数（全体の平均） |
| `$\frac{1}{N}\sum_{i=1}^{N}$` | エヌぶんのイチ シグマ | N個のデータの平均 |
| `$\ell(f_\theta(\mathbf{x}_i), y_i)$` | スモール エル | 1個のデータの損失 |
| `$f_\theta(\mathbf{x}_i)$` | エフ シータ エックス アイ | モデルの予測 |

**これが勾配降下法の数式だ。** ニューラルネットの学習で毎ステップ計算される式 — 誤差逆伝播（backpropagation）はこの微分規則を計算グラフに沿って適用することで勾配を効率に計算する[^2]。「ナブラ」を見たら「勾配」と反射的に読めるようになれば、もう数式はただの文章だ。

> **Note:** ここで多くの人が混乱するのが `$\mathcal{L}$`（カリグラフィック体のエル）と `$\ell$`（スモールエル）の違いだ。慣例として `$\mathcal{L}$` は「全体の損失」、`$\ell$` は「1サンプルの損失」を指すことが多い。だが著者によって流儀が異なるので、必ず論文中の定義を確認すること。

### 1.2 数式パラメータを触って遊ぶ

数式の感覚を掴むには、パラメータを変えて挙動を見るのが一番早い。


出力:

**温度 `$T$` が低いと「確信的」、高いと「均等」になる。** ChatGPTの `temperature` パラメータの正体がこれだ。LLMが使うSoftmaxの数式を、数行のコードで完全に理解できる。

```math
p_i = \frac{\exp(x_i / T)}{\sum_{j=1}^{K} \exp(x_j / T)}
```

`$T \to 0$` で one-hot（argmax）に近づき、`$T \to \infty$` で一様分布に近づく。数式から読み取れる性質だ。

この温度付きSoftmaxは、Hintonらの知識蒸留（Knowledge Distillation）論文 [^3] で体系的に導入された。大きなモデル（教師）の「柔らかい」出力分布を小さなモデル（生徒）に学習させる手法であり、高温の `$T$` で教師の出力を「ソフト化」するのが鍵だ。

<details><summary>温度パラメータの数学的直感</summary>
`$T \to 0$` のとき、`$x_i / T$` は最大の `$x_i$` だけが `$+\infty$` に発散し、他は相対的に `$-\infty$` に近づく。`$T \to 0$` の極限で、最大の `$x_i$` に対応する Softmax 出力が 1 に収束し、他は 0 に収束するから one-hot。

`$T \to \infty$` のとき、`$x_i / T \to 0$` なので全ての `$\exp(x_i / T) \to \exp(0) = 1$`。均等に1なので一様分布。

この 2 つの極限を紙に書いて確かめてみてほしい。数式の性質を直接「導出」する感覚が身につく。

ちなみに、LLMの推論で `$T = 0$` を指定するとgreedy decodingになるのは、まさにこの数学的性質のためだ。この話は第15回（自己回帰モデル）で本格的に扱う。
</details>

### 1.3 Attention — 現代AIの心臓部を触る

もう1つ、現代の機械学習で最も重要な数式を体験しよう。

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
```

GPT、BERT、Vision Transformer、Stable Diffusion — 2024-2026年の主要モデル全てがこの数式の上に立っている。**たった1つの数式が、AIの全てを動かしている**というのは言い過ぎではない。この式はVaswaniらの "Attention Is All You Need" [^1] で提案され、それ以降の深層学習の方向性を決定的に変えた。


**数式のどこがコードのどこか、わかるだろうか？**

| 数式 | コード | 意味 |
|:-----|:-------|:-----|
| `$QK^\top$` | `Q @ K.T` | クエリとキーの類似度行列 |
| `$\sqrt{d_k}$` | `np.sqrt(d_k)` | スケーリング因子 |
| `$\text{softmax}(\cdot)$` | `np.exp(...) / sum` | 確率への正規化 |
| `$(\cdot) V$` | `weights @ V` | 値の加重和 |

この数式→コード対応が「見える」なら、すでに数式リテラシーの第一歩を踏み出している。

ここで「なぜ `$\sqrt{d_k}$` で割るのか？」という疑問が湧いた人は鋭い。内積 `$QK^\top$` の値は次元数 `$d_k$` が大きいほど絶対値が大きくなる傾向がある。大きすぎる値にSoftmaxをかけると one-hot に近づいてしまい、勾配が消失する。`$\sqrt{d_k}$` で割ることで値のスケールを安定させているのだ。Vaswaniら [^1] は原論文でこの理由を明確に述べている:「`$d_k$` が大きいとき、内積の大きさが増大してSoftmaxが極端に飽和した領域に押し込まれ、勾配が非常に小さくなる」。

> **一言で言えば**: Attention = 「類似度で重みづけした値の加重和」

この直感を第2回（線形代数 I）と第16回（Transformer完全版）で数学的に深める。

<details><summary>Attentionの計算量</summary>
`$Q, K, V \in \mathbb{R}^{n \times d}$` のとき:
- `$QK^\top$`: `$O(n^2 d)$` — 全トークン対の類似度を計算
- softmax: `$O(n^2)$`
- weights `$\times V$`: `$O(n^2 d)$`

合計 `$O(n^2 d)$`。シーケンス長 `$n$` に対して二乗で計算量が増える。これが「長い文脈は高コスト」の理由であり、Flash Attention（第16回）やSSM（第26回）で解決を試みる対象だ。
</details>

### 1.4 LLMの学習目標 — Cross-Entropy Loss

LLMが毎ステップ最小化しているのが以下の損失関数だ:

```math
\mathcal{L}(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \log p_\theta(x_t \mid x_{<t})
```

分解すると:
- `$\theta$` — モデルパラメータ
- `$T$` — シーケンス長（トークン数）
- `$x_t$` — `$t$` 番目のトークン（正解）
- `$x_{<t}$` — `$t$` より前のトークン列（文脈）
- `$p_\theta(x_t \mid x_{<t})$` — モデルが `$x_t$` に割り当てた確率
- `$\log$` — 対数（確率の積を和に変換する技）
- `$-$` — 最小化のための符号反転

**一言で言えば**: 「各トークンについて、モデルが正解に割り当てた確率の対数を取り、平均の符号を反転したもの」。


この Cross-Entropy Loss は第6回（情報理論）で理論的に深堀りし、Perplexity `$= \exp(\mathcal{L})$` の情報理論的意味を `$2^{H(p)}$` と接続する。今は「LLMの学習 = この数式の最小化」とだけ覚えておけば十分だ。

### 1.5 4つの数式はどう接続するか

ここまでで体験した4つの数式を整理しよう。実はこれらは独立ではなく、LLMの推論・学習パイプラインの中で繋がっている。


**推論時**（生成時）:
1. 入力トークン列が Attention 層を通過する
2. 出力ロジットが得られる
3. Softmax で確率分布に変換する（Temperature で制御）
4. 確率分布からサンプリングして次のトークンを選ぶ

**学習時**:
1. 上記1-3に加えて、正解トークンに対する Cross-Entropy Loss を計算
2. Loss から勾配を逆伝播（Backpropagation）[^2]
3. パラメータを更新


<details><summary>NumPy と PyTorch の対応</summary>
ここまで全て NumPy で書いてきた。PyTorch だと「同じ数学」を次の API に写像できる。

- Softmax: `np.exp(z)/np.sum(np.exp(z))` → `torch.softmax(z, dim=-1)`
- 温度付き Softmax: `softmax(z/T)` → `torch.softmax(z / T, dim=-1)`
- Cross-Entropy: `-∑ y_i log p_i` → `torch.nn.functional.cross_entropy(logits, target)`

**重要**: 便利APIを先に覚えると「式の意味」がブラックボックス化する。まず NumPy で式と1:1対応を掴み、その後に PyTorch の高速実装に移る。
</details>

> **Zone 1 まとめ**: Softmax（確率への変換）→ Temperature（分布の鋭さ制御）[^3] → Attention（類似度加重和）[^1] → Cross-Entropy（学習目標）。この4つが LLM の心臓部を形成している。数式で書けば4行。コードで書けば各20行。それが数十億パラメータのモデルを動かしている。

> **Checkpoint:** Softmax、Temperature、Attention、Cross-Entropy Loss の4つの数式を「触って」理解し、それらの接続を確認した。

> Progress: 25%

---

### 🎯 Quick Check — Zone 2

理解度を確認しましょう:

1. **`$\mathcal{L}_{\text{CE}} = -\sum_{i=1}^{C} y_i \log \hat{y}_i$` の各記号が何を指すか？**
   <details><summary>解答</summary>
   `\mathcal{L}_{\text{CE}}`: Cross-Entropy Loss、`C`: クラス数、`y_i`: 正解（one-hot の第i成分）、`\hat{y}_i`: 予測確率（Softmax出力）、`\sum`: 全クラスに足し上げる
   </details>

2. **Temperature を上げると分布はどう変わるか？（「鋭い/鈍い」を言語化せよ）**
   <details><summary>解答</summary>
   `T` を上げると `logits/T` が小さくなり、Softmax の指数差が縮む。結果として分布は平坦（鈍い）になり、サンプリングのランダム性が増える。
   </details>

---

> **Note:** **前編（理論編）の実装コードは Z1 クイックスタートのみ**。それ以外は数式と論文読解に全振りする（実装編は後編）。

## 🧩 Z3. 直感ゾーン（20分）— 数式リテラシーが最優先の理由

### 2.1 なぜ「数式の読み書き」が最初なのか

多くの機械学習入門は「Pythonでモデルを動かす」から始まる。それ自体は悪くない。だが、その先で確実に壁にぶつかる。

- 論文を読もうとして数式で止まる
- ハイパーパラメータの意味がわからず試行錯誤
- 新手法が出ても「何が新しいのか」判断できない
- バグの原因が数式の誤解にあることに気づかない

**核心はこうだ: 数式が読めないと、ライブラリのユーザーから先に進めない。**

このシリーズが「概論 — 数式と論文の読み方」から始まる理由がここにある。数式の読み書き能力は、残り39回全ての土台になる。

> **この章を読めば**: 全50回シリーズの地図が手に入る。どこに何があり、各講義がどう繋がっているかが見える。

### 2.2 本シリーズの全体構成

全50回は5つのコースに分かれている。

| コース | 回 | テーマ |
|:------|:---|:------|
| **Course I** | 第1-8回 | 数学基礎編（線形代数・確率・情報理論・最適化） |
| **Course II** | 第9-18回 | 生成モデル理論編（VAE / GAN / Flow / Diffusion） |
| **Course III** | 第19-32回 | 実装・本番編（最適化・分散・圧縮・評価・デプロイ） |
| **Course IV** | 第33-42回 | 拡散モデル理論編（DDPM→CFG→逆問題→Flow/ODE） |
| **Course V** | 第43-50回 | 応用・フロンティア（画像・動画・3D・音声・マルチモーダル） |

### 2.3 差別化の3軸

本シリーズが貫く柱は3つある:

1. **理論**: 結果の暗記ではなく、導出過程を追って「なぜその式か」を説明できる
2. **実装**: 数式をコードに落とし、動く形で理解を固定する（後編以降で加速する）
3. **最新**: 生成モデルの主要ファミリーを俯瞰し、拡散・Flow・Transformer 系まで繋げる

「論文が書ける」とはどういうことか。数式を「結果」として暗記するのではなく、**導出過程を自力で再現できる**ということだ。ELBOの分解 [^4]、KLダイバージェンスの解析解、Score Matchingの等価性証明 — これらを「見たことがある」ではなく「自分で導ける」レベルまで持っていく。


### 2.4 Course I（第1-8回）ロードマップ

本講義はCourse I「数学基礎編」の初回だ。Course Iの8回で何を学ぶのか、全体像を示す。


| 回 | テーマ | 核心 | LLM/Transformerとの接点 |
|:---|:------|:-----|:----------------------|
| **第1回** | 概論: 数式と論文の読み方 | 数式リテラシー | Softmax, Attention [^1], Cross-Entropy |
| **第2回** | 線形代数 I | ベクトル空間・行列 | `$QK^\top$` の内積、埋め込み空間 |
| **第3回** | 線形代数 II | SVD・行列微分 | ヤコビアン、Backpropagation [^2] |
| **第4回** | 確率論・統計学 | 分布・ベイズ | `$p(x_t \mid x_{<t})$` 自己回帰、Softmax分布 |
| **第5回** | 測度論・確率過程 | 厳密な確率 | トークン空間上の確率測度 |
| **第6回** | 情報理論・最適化 | KL・SGD | Perplexity `$= 2^H$`、Cross-Entropy Loss |
| **第7回** | 生成モデル概要 & MLE | 尤度最大化 | 次トークン予測 `$= \arg\max p(x_t \mid x_{<t}; \theta)$` |
| **第8回** | 潜在変数 & EM算法 | 隠れ変数 | Transformer隠れ層、VAE [^4] への橋渡し |

**各講義の「限界」が、次講義の「動機」になる。** これが40回を貫く設計原則だ。

たとえば第2回で線形代数を習得すると、「不確実性をどう数学的に扱うのか？」という問いが生まれる。それが第4回（確率論）の動機になる。第4回で確率分布を扱えると、「もっと厳密に確率を定義できないか？」 — それが第5回（測度論）の動機になる。

この連鎖が第40回まで途切れない。

### 2.5 3つの比喩で捉える「数式リテラシー」

数式を読む力を3つの比喩で考えてみよう。

**比喩1: 楽譜**

五線譜が読めない人にとって、楽譜はただの黒い点の集合だ。だが音楽理論を学べば、楽譜から音楽が「聞こえる」ようになる。数式も同じ — 記号の意味を知れば、数式から「動作」が見える。`$\sum_{i=1}^{N}$` を見て「ループだ」とわかる。`$\nabla_\theta$` を見て「勾配降下の方向だ」とわかる。

**比喩2: 設計図**

建築の設計図を見て、完成形の建物を想像できるのはプロだけだ。数式は機械学習アルゴリズムの「設計図」であり、読めれば実装前にアルゴリズムの動作を頭の中で走らせることができる。

**比喩3: プログラミング言語**

そう、数式は「最も古いプログラミング言語」だ。

| プログラミング | 数学記法 | 例 |
|:-------------|:---------|:---|
| 変数宣言 | 集合への所属 | `x: float` ↔ `$x \in \mathbb{R}$` |
| forループ | 総和 | `sum(...)` ↔ `$\sum$` |
| 積の累積 | 総乗 | `prod(...)` ↔ `$\prod$` |
| if文 | 指示関数 | `if cond:` ↔ `$\mathbb{1}[\cdot]$` |
| 関数定義 | 写像 | `def f(x):` ↔ `$f: X \to Y$` |
| 型注釈 | 空間の指定 | `x: np.ndarray` ↔ `$\mathbf{x} \in \mathbb{R}^d$` |

プログラミング言語にある概念は、数学記法にも全て存在する。

この対応は偶然ではない。プログラミング言語は数学の記法を形式化したものだからだ。だからこそ、プログラマは数式に対して本質的な優位性を持っている。`$\sum_{i=1}^{N} f(x_i)$` を見て `sum(f(x[i]) for i in range(N))` と読める能力は、プログラミング未経験者にはない。

**あなたが数式を「難しい」と感じるのは、文法が違うだけだ。** 同じ計算を、一方は `$\sum$` で書き、他方は `for` で書く。中身は同じ。本稿では常に両方を並べて示す。数式を見たら「これをコードにするとどうなるか？」と考える癖をつける。それが最も効率的な学習法だ。

**ここで一つ断言する。** 数式が「読めない」のではない。「読み方を教わっていない」だけだ。アルファベットを教わらずに英語を読めないのは当然で、数式記号の読み方を教わらずに論文を読めないのも当然だ。

これから、そのアルファベットを一つずつ教える。

### 2.6 ローカル完結ポリシー

本シリーズの全50回は**ローカルマシンだけで完結する**。Google Colabは不要。

| 項目 | 最低スペック | 推奨スペック |
|:-----|:-----------|:-----------|
| CPU | Intel i5 / Apple M1 | Apple M2+ / AMD Ryzen 7 |
| RAM | 8GB | 16GB |
| GPU | **不要**（CPU完結） | 内蔵GPU (Metal/Vulkan) |
| ストレージ | 10GB空き | 20GB空き |

Course I（第1-8回）は合成データ・2Dトイデータのみを使い、全て1分以内に実行できる。「GPUがないから手を動かせない」という言い訳は、このシリーズでは通用しない。

### 2.7 効果的な学習戦略

40回を最大限に活用するための戦略を提示する。

#### 3周読みのススメ

各講義は「1回読めばOK」ではない。3周するのが最も効果的だ。

| 周目 | 目的 | 所要時間 | フォーカス |
|:-----|:-----|:--------|:---------|
| **1周目** | 全体の地図を掴む | 表示時間の80% | Zone 1-4 を通読（後編 Zone 5-7 は見出しだけ眺める） |
| **2周目** | 手を動かす | 表示時間の100% | Zone 1 のコードを自力で書き直す（後編の実装課題も解く） |
| **3周目** | 接続を見る | 表示時間の50% | 次の講義を読んだ後に戻り、接続を確認する |

**1周目で100%理解しようとしないこと。** 第1回を読んでいる段階では、第2回の知識がないため理解できない部分がある。第2回を読んだ後に戻ると「ああ、これはそういう意味だったのか」と繋がる。それが設計意図だ。

#### ノートの取り方

紙のノートに以下の3つを書く。デジタルでもいいが、数式は手書きの方が定着する。

1. **記号辞書**: 新しい記号に出会ったら「`$\nabla$` = 勾配（ナブラ）」のように書き溜める
2. **数式→コード対応表**: `Σ → np.sum`, `∫ → np.mean(samples)` のように
3. **???リスト**: わからなかったことを書く。次の講義で解消されたら線を引く


#### 写経 vs 理解

**写経は理解の代替にならない。** しかし、理解の「きっかけ」にはなる。

推奨フロー:
1. コードを**見ずに**数式だけからコードを書く（10分試す）
2. 書けなかったら、答えのコードを**見て**写す
3. 写したら、もう一度**見ずに**書く
4. 3回やって書けなければ、数式の理解が足りない — 解説を再読する

このサイクルを「Softmax」「Attention」「Cross-Entropy Loss」の3つで実践してみてほしい。

#### 論文の読み方: 3パスリーディング（arXiv対応）

学習は「記事を読む」だけで完結しない。論文を読むための最短ループは次だ。

**Pass 1（全体像: 10分）**

1. タイトルとAbstractを読む（目的・設定・主張を1行で言い換える）
2. 図を眺める（1枚だけで何が新しいかを掴む）
3. 結論を読む（何が分かり、何が分からないか）

**Pass 2（技術核: 30-60分）**

1. `Notation` / `Preliminaries` を読む（ここで止まったら「記号辞書」に追記）
2. 主張（定理・命題・損失関数）の定義を読む（式の「役」を言語化する）
3. 実験の設定を読む（何を比較し、何を勝ちと言っているか）

**Pass 3（再現: 1-3時間）**

1. 主要式を自分で導く（途中式を省略しない）
2. 既存実装を読む（「式→実装」の対応を追う）
3. 1つだけ再現する（最小構成でいい。完全再現は後回し）

> **Note:** **コツ**: Pass 1 で「読む価値があるか」を判定し、Pass 2 で「理解の芯」を取り、Pass 3 で「手で固定」する。Pass 2 をやらずに Pass 3 へ行くと、写経地獄になる。

> **Checkpoint:** シリーズの全体構成と学習戦略を把握した。ここから数式の記号と記法を一気に押さえる。

> Progress: 45%

### 2.8 深層生成モデルの全体像

本シリーズで扱う生成モデルのファミリーを俯瞰する。第1回で概観を掴んでおくことで、以降の各講義の位置づけが明確になる。


#### 各ファミリーの特徴比較

| ファミリー | 核心アイデア | 代表論文 | 本シリーズ |
|:---|:---|:---|:---|
| VAE | 潜在変数 + 変分推論 | Kingma & Welling (2013)[^4] | 第9-10回 |
| GAN | 生成器 vs 判別器の敵対的訓練 | Goodfellow et al. (2014)[^8] | 第15-16回 |
| 自己回帰 | 条件付き確率の連鎖 | Vaswani et al. (2017)[^1] | 第13-16回 |
| 拡散 | ノイズ付加→除去の反復 | Ho et al. (2020)[^5] | 第11-14回 |
| Flow Matching | ODE による確率パス | Lipman et al. (2022)[^6] | 第17-20回 |
| Transformer + 拡散 | DiT アーキテクチャ | Peebles & Xie (2022)[^7] | 第21-24回 |

#### 深層生成モデルの歴史タイムライン

生成モデルの主役は何度も交代している。しかし、実際に起きているのは「数学の共通化」だ。

| 年 | マイルストーン | 何が起きたか | 数学の核 |
|:---:|:---|:---|:---|
| 2013 | VAE[^4] | 潜在変数モデルを変分推論で学習可能にした | `KL` と `ELBO` |
| 2014 | GAN[^8] | 尤度ではなく識別で学習する枠組みを示した | ミニマックス最適化 |
| 2017 | Transformer[^1] | Attention をスケール可能にし、系列の主役を置き換えた | 内積 + Softmax |
| 2020 | DDPM[^5] | 連続時間極限へ繋がる「ノイズ→復元」学習を定式化した | 条件付きガウス + 反復 |
| 2022 | Flow Matching[^6] | ODE で確率パスを直接学習する見方を強くした | `p_t` の時間発展 |
| 2022 | DiT[^7] | Transformer を拡散のバックボーンへ持ち込んだ | 行列演算の支配 |

この表を「暗記」する必要はない。重要なのは、どの手法も結局は次の5語で書ける点だ。

1. **確率**（分布を置く）
2. **期待値**（平均をとる）
3. **勾配**（最適化する）
4. **内積**（類似度を測る）
5. **変換**（写像で表現する）

Zone 4 の目的は、この5語を支える記号を「読める」状態にすることだ。

#### なぜ「深層生成モデル」を学ぶのか

2024-2025 の AI ブームの本質は**生成モデル**にある:

1. **LLM (GPT, Claude, Gemini)** — テキストの自己回帰生成モデル
2. **画像生成 (DALL-E 3, Stable Diffusion, FLUX)** — 拡散/Flow Matching による画像生成
3. **動画生成 (Sora, Runway)** — 時空間の拡散モデル
4. **音声生成 (Whisper, WaveNet)** — 自己回帰/拡散の音声モデル
5. **3D生成 (DreamFusion, Score Jacobian)** — SDS による3D最適化

これらすべてが**同じ数学的基盤**の上に成り立っている。本シリーズはその基盤を体系的に学ぶ。

| 応用 | 基盤モデル | 数学的核心 | 本シリーズの対応 |
|:---|:---|:---|:---|
| ChatGPT | Transformer (自己回帰) | `$p(x_t \mid x_{<t})$` | 第13-16回 |
| DALL-E 3 | U-Net + Diffusion | `$\epsilon_\theta(\mathbf{x}_t, t)$` | 第11-14回 |
| Stable Diffusion 3 | DiT + Flow Matching | `$v_\theta(\mathbf{x}_t, t)$` | 第17-24回 |
| GPT-4V | Vision Transformer + LLM | Multi-modal fusion | 第33-36回 |
| Sora | Spatial-temporal DiT | 3D attention + diffusion | 第37-40回 |

### 🎯 Quick Check — Zone 3

理解度を確認しましょう:

1. **本シリーズの3つの柱は？**
   <details><summary>解答</summary>
   理論（導出過程まで追う）・実装（式を動かして理解を固定）・最新（生成モデルの主要ファミリーを俯瞰し繋ぐ）
   </details>

2. **Course I（第1-8回）の目的は？**
   <details><summary>解答</summary>
   生成モデル理論を学ぶための数学的基盤（線形代数・確率論・情報理論・最適化・測度論）を完全装備すること
   </details>

---

## 📐 Z4. 数式修行ゾーン（60分）— 記号を「読める」ようにする

> **目標**: ギリシャ文字・添字・演算子・集合・論理・関数の記法を網羅し、Transformer の数式を**一文字残らず**読めるようにする。

本シリーズで最も重要なゾーンだ。ここをクリアすれば、以降の39回の講義で「記号がわからなくて止まる」ことは二度とない。逆にここを曖昧にすると、毎回つまずく。急がず一つずつ確認しよう。

### 3.0 記号読解プロトコル（最短で「読める」ようになる手順）

数式を読むときに「なんとなく眺める」から抜けるために、手順を固定する。

**プロトコル（8ステップ）**

1. **対象を宣言する**: これは「定義」か「目的関数」か「更新式」か
2. **左辺の型を言う**: スカラーか、ベクトルか、行列か
3. **右辺の核を特定する**: `=` の右で一番外側の演算は何か（足す/掛ける/期待値/Softmax）
4. **添字を住所として読む**: `i,j,t,l` が「何を区別しているか」を1行で言う
5. **集合と範囲を決める**: `i=1..N`、`x∈R^d`、`t=1..T` のように「どこまで回すか」を確定する
6. **正規化を疑う**: `softmax`、`/Σ`、`/N` を見たら「何を確率/平均にしているか」を言語化する
7. **形状（shape）で検算する**: 行列積の寸法が繋がっているかを確認する
8. **1行の日本語に翻訳する**: 「何を、どうして、何になるか」を文章化する

> **Note:** **ルール**: 途中で止まったら、その場で「記号辞書」を増やす。辞書が増えるほど、次の論文が読みやすくなる。

**例: 勾配降下法**

```math
\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta)
```

1行翻訳はこうだ。

「パラメータ `$\theta$` を、損失 `$\mathcal{L}$` を下げる向き（勾配）へ、学習率 `$\alpha$` の大きさだけ動かす」

この翻訳が即答できれば、以降に出てくる更新式は全て読める。更新式は表面が違っても、構造は似ているからだ。

### 3.1 ギリシャ文字 — 数式のアルファベット

機械学習の論文で頻出するギリシャ文字を、用途ごとに整理する。「この記号は何に使われることが多いか」を知っておくだけで、論文を開いたときの初見殺しが大幅に減る。

#### パラメータ系（モデルの中身を表す）

| 記号 | 読み | LaTeX | 典型的な用途 |
|:---:|:---:|:---:|:---|
| `$\theta$` | シータ | `\theta` | モデルのパラメータ全般。`$p_\theta(\mathbf{x})$` のように下付きで「このモデルのパラメータは `$\theta$`」と示す |
| `$\phi$` | ファイ | `\phi` | `$\theta$` と区別したい第2のパラメータ群。VAE[^4]ではエンコーダのパラメータを `$\phi$`、デコーダを `$\theta$` とする |
| `$\psi$` | プサイ | `\psi` | 第3のパラメータ群。Teacher-Student 構成[^3]で教師を `$\psi$`、生徒を `$\theta$` とすることがある |
| `$\omega, w$` | オメガ | `\omega` | 個々の重み（weight）。`$\theta = \{w_1, w_2, \ldots\}$` |

> **Note:** **覚え方**: `$\theta$`（主役）→ `$\phi$`（相方）→ `$\psi$`（第三者）の順で「パラメータ三兄弟」と覚える。Kingma & Welling の VAE 論文[^4]を読めば、`$\theta$` と `$\phi$` の役割分担が自然に身につく。

#### 統計量系（データの性質を表す）

| 記号 | 読み | LaTeX | 典型的な用途 |
|:---:|:---:|:---:|:---|
| `$\mu$` | ミュー | `\mu` | 平均（mean）。`$\mu = \mathbb{E}[X]$` |
| `$\sigma$` | シグマ | `\sigma` | 標準偏差（standard deviation）。`$\sigma^2$` は分散 |
| `$\Sigma$` | 大シグマ | `\Sigma` | 共分散行列。`$\Sigma \in \mathbb{R}^{d \times d}$`。小文字 `$\sigma$` と大文字 `$\Sigma$` は意味が違う |
| `$\rho$` | ロー | `\rho` | 相関係数。`$\rho \in [-1, 1]$` |
| `$\tau$` | タウ | `\tau` | 温度パラメータ（Temperature）。Zone 1 で見た Softmax の `$T$` はこの `$\tau$` で書かれることも多い |

<details><summary>ミニ演習: 温度パラメータの記号揺れ</summary>
同じ概念でも、論文によって記号が異なる。温度パラメータは以下のバリエーションがある:

- Hinton et al. (2015)[^3]: `$T$` を使用 — `$q_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}$`
- 他の論文: `$\tau$` を使用 — `$\text{softmax}(z_i / \tau)$`
- 一部の強化学習論文: `$\beta$` を使用（逆温度 `$\beta = 1/T$` として）

**重要**: 記号は著者が定義するものであり、「正解」は存在しない。論文を読むときは冒頭の記号定義を**必ず**確認する習慣をつけよう。
</details>

#### 演算系（操作を表す）

| 記号 | 読み | LaTeX | 典型的な用途 |
|:---:|:---:|:---:|:---|
| `$\nabla$` | ナブラ | `\nabla` | 勾配演算子。`$\nabla_\theta \mathcal{L}$` は「損失 `$\mathcal{L}$` の `$\theta$` についての勾配」 |
| `$\partial$` | パーシャル/デル | `\partial` | 偏微分。`$\frac{\partial f}{\partial x}$` |
| `$\alpha$` | アルファ | `\alpha` | 学習率。`$\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}$` |
| `$\epsilon$` | イプシロン | `\epsilon` | 微小量。数値安定化の `$\log(p + \epsilon)$` や、ノイズ `$\epsilon \sim \mathcal{N}(0, I)$` |
| `$\lambda$` | ラムダ | `\lambda` | 正則化係数。`$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{main}} + \lambda \mathcal{L}_{\text{reg}}$` |
| `$\eta$` | イータ | `\eta` | 学習率（`$\alpha$` の代替）。文献によって `$\alpha$` か `$\eta$` のどちらか |
| `$\gamma$` | ガンマ | `\gamma` | 割引率（強化学習）、モメンタム係数 |

#### 確率・分布系

| 記号 | 読み | LaTeX | 典型的な用途 |
|:---:|:---:|:---:|:---|
| `$\pi$` | パイ | `\pi` | 方策（policy）。確率分布を直接意味することは少ない |
| `$\xi$` | グザイ | `\xi` | 潜在変数の別名。`$\xi \sim p(\xi)$` |
| `$\zeta$` | ゼータ | `\zeta` | 補助変数。頻度は低いが理論系論文で登場 |
| `$\kappa$` | カッパ | `\kappa` | 集中度パラメータ（von Mises分布など） |

> **Note:** **全部覚える必要はない。** ここは辞書だ。論文を読んでいて「この記号なんだっけ」となったら戻ってくればいい。繰り返し参照するうちに自然に覚える。

#### ミニ演習: ギリシャ文字クイズ（即答できるまで）

以下を「声に出して」読んで、用途を1行で言えるか確認しよう。詰まった記号は、ノートの「記号辞書」に追記する。

1. `$\theta$` と `$\phi$` の役割分担は？
2. `$\mu$` と `$\Sigma$` は何を表すことが多い？
3. `$\epsilon \sim \mathcal{N}(0, I)$` は何の典型か？
4. `$\lambda$` が出たら、まず何を疑う？
5. `$\tau$` が出たら、Softmax と何が繋がる？
6. `$\nabla_\theta$` の下付きは何を意味する？
7. `$\partial$` が出たら、何が「偏」なのか？
8. `$\pi$` が出たら「円周率」ではなく何を想像すべき？

<details><summary>解答</summary>

1. 多くの論文で `$\theta$` は生成モデル（デコーダ）、`$\phi$` は近似事後（エンコーダ）を表す（VAE[^4]が典型）。
2. `$\mu$` は平均、`$\Sigma$` は共分散行列（ガウス分布など）。
3. ノイズ注入の典型。拡散モデル[^5]やVAE[^4]で頻出。
4. 正則化項の係数（L2、KL、制約ペナルティなど）を疑う。
5. 温度付きSoftmax。分布の鋭さ（エントロピー）制御に繋がる。
6. `$\theta$` に関する勾配、つまり「どのパラメータを動かすか」の指定。
7. 多変数のうち、特定の変数だけを動かす偏微分。
8. 強化学習なら方策（policy）。生成モデルなら混合分布の重みとして出ることもある。
</details>


### 3.2 添字（Subscript / Superscript）の文法

数式の「文法」の中で最も重要なのが**添字**だ。`$x$` と `$x_i$` と `$x_i^{(t)}$` と `$x_{i,j}^{(l)}$` は、同じ `$x$` でもまったく異なる情報を持つ。

#### 下付き添字（Subscript）: 「どの要素か」

```math
\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}
\quad \text{ベクトル } \mathbf{x} \text{ の } i \text{ 番目の成分を } x_i \text{ と書く}
```

| パターン | 例 | 意味 |
|:---|:---|:---|
| 要素番号 | `$x_i$` | ベクトルの `$i$` 番目 |
| 行列要素 | `$A_{ij}$` or `$a_{ij}$` | `$i$` 行 `$j$` 列 |
| パラメータ指定 | `$p_\theta$` | パラメータ `$\theta$` を持つ分布 `$p$` |
| 時刻 | `$x_t$` | 時刻 `$t$` でのデータ |
| 層番号 | `$h_l$` | `$l$` 番目の層の隠れ状態 |

> **Note:** **紛らわしいケース**: `$x_t$` が「時刻 `$t$`」か「`$t$` 番目の要素」かは文脈依存。論文の冒頭で定義されるので、必ず確認する。拡散モデル[^5]では `$x_t$` は「ノイズステップ `$t$` での画像」を意味する。

#### 上付き添字（Superscript）: 「何乗か」「何回目か」

| パターン | 例 | 意味 |
|:---|:---|:---|
| べき乗 | `$x^2$` | `$x$` の2乗 |
| サンプル番号 | `$x^{(i)}$` | `$i$` 番目のサンプル。丸括弧で区別 |
| 反復回数 | `$\theta^{(t)}$` | `$t$` 回目の更新後のパラメータ |
| 層番号 | `$W^{(l)}$` | `$l$` 番目の層の重み行列 |
| 転置 | `$A^\top$` or `$A^T$` | 行列の転置 |
| 逆行列 | `$A^{-1}$` | 逆行列 |

<details><summary>ミニ演習: 添字を「住所」として読む</summary>
次の3つを、1行の日本語に翻訳せよ。

1. `$x_i$`
2. `$x_t$`
3. `$W_{ij}^{(l)}$`

<details><summary>解答</summary>

1. ベクトル（または列）`$x$` の `$i$` 番目の成分。
2. 文脈次第だが、時刻 `$t$` の値（拡散ならノイズステップ `$t$` の状態）。
3. 第 `$l$` 層の重み行列 `$W$` の `$i$` 行 `$j$` 列成分。
</details>
</details>


#### 複合添字: `$W_{ij}^{(l)}$`

複数の添字が組み合わさる場合:

```math
W_{ij}^{(l)} = \text{第 } l \text{ 層の重み行列の } (i, j) \text{ 成分}
```

**読み方のルール**:
1. まず上付き添字を読む: 「`$l$` 層目の」
2. 次に下付き添字を読む: 「`$i$` 行 `$j$` 列の」
3. 本体を読む: 「重み `$W$`」

→ 全体: 「`$l$` 層目の重み行列 `$W$` の `$i$` 行 `$j$` 列成分」


### 3.3 演算子・特殊記法

#### 総和 `$\sum$` と総乗 `$\prod$`

機械学習で最も頻繁に登場する演算子:

```math
\sum_{i=1}^{n} x_i = x_1 + x_2 + \cdots + x_n
```

```math
\prod_{i=1}^{n} x_i = x_1 \cdot x_2 \cdots x_n
```

**Cross-Entropy Loss（再掲）**を `$\sum$` で書き直す:

```math
\mathcal{L}_{\text{CE}} = -\sum_{i=1}^{C} y_i \log \hat{y}_i
```

ここで `$C$` はクラス数、`$y_i$` は正解ラベル（one-hot）、`$\hat{y}_i$` はモデルの予測確率。

`$\prod$` は**尤度関数**で登場する。独立なデータ `$\{x^{(1)}, \ldots, x^{(N)}\}$` の同時確率:

```math
p(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(N)} \mid \theta) = \prod_{i=1}^{N} p(\mathbf{x}^{(i)} \mid \theta)
```

対数を取ると `$\prod$` が `$\sum$` になる — これが**対数尤度**を使う理由:

```math
\log p = \sum_{i=1}^{N} \log p(\mathbf{x}^{(i)} \mid \theta)
```

ここで重要なのは「`$\log$` を取ると `$\prod$` が `$\sum$` になる」だけではない。`$\log$` は次の3つを同時に満たす強い道具だ。

1. **単調増加**: `$\theta$` に関する最適化で、`$\prod$` を最大化する代わりに `$\sum$` を最大化できる
2. **数値安定**: 小さい確率を掛け続けると0に潰れる（アンダーフロー）。`$\log$` はそれを防ぐ
3. **微分が簡単**: 積の微分より和の微分の方が計算グラフが単純になる

#### 最尤推定（MLE）と負の対数尤度（NLL）

機械学習の多くは「尤度を最大化する」問題として書ける。

```math
\hat{\theta} = \arg\max_\theta \prod_{i=1}^{N} p(\mathbf{x}^{(i)} \mid \theta)
```

対数を取っても最適解は変わらない（単調性）。したがって、

```math
\hat{\theta} = \arg\max_\theta \sum_{i=1}^{N} \log p(\mathbf{x}^{(i)} \mid \theta)
```

さらに「最大化」は「最小化」に直せる。これが負の対数尤度（NLL）だ。

```math
\hat{\theta} = \arg\min_\theta \left( - \sum_{i=1}^{N} \log p(\mathbf{x}^{(i)} \mid \theta) \right)
```

Zone 1 で出てきた Cross-Entropy は、この NLL の特殊ケースとして現れる。分類の文脈では、正解クラスの確率を最大化したいので、結局「`$-\log p$`」を足し上げる形になる。

#### log-sum-exp と Softmax の数値安定性

Softmax の定義は次だ。

```math
\mathrm{softmax}(\mathbf{z})_i = \frac{\exp(z_i)}{\sum_{j=1}^{K}\exp(z_j)}
```

しかし、そのまま計算すると `$\exp(z)$` が容易に発散する。そこで使うのが次の恒等式だ。

```math
\log \sum_{j=1}^{K}\exp(z_j) = m + \log \sum_{j=1}^{K}\exp(z_j - m)
```

ただし `$m = \max_j z_j$`。`$z_j - m \le 0$` なので、指数が安全な範囲に収まる。

Softmax を安定化する典型変形は「最大値を引いてから計算する」だ。

```math
\mathrm{softmax}(\mathbf{z})_i
= \frac{\exp(z_i - m)}{\sum_{j=1}^{K}\exp(z_j - m)}
```

この変形は値を変えない（分子分母を同じ `$\exp(-m)$` で割っているだけ）一方で、計算の安定性を劇的に改善する。

<details><summary>ミニ演習: Softmax の不変性</summary>
次を証明せよ。

```math
\mathrm{softmax}(\mathbf{z}) = \mathrm{softmax}(\mathbf{z} + c\mathbf{1})
```

ここで `$c\in\mathbb{R}$`、`$\mathbf{1}$` は全要素が1のベクトル。

<details><summary>解答</summary>
分子は `$\exp(z_i + c) = \exp(c)\exp(z_i)$`、分母は `$\sum_j \exp(z_j + c) = \exp(c)\sum_j \exp(z_j)$`。よって `$\exp(c)$` が約分され、値は不変。
</details>
</details>


#### argmax / argmin

```math
\hat{y} = \arg\max_{i} p(y = i \mid \mathbf{x})
```

「確率を最大にする**インデックス**を返す」演算。`$\max$` が**値**を返すのに対し、`$\arg\max$` は**位置**を返す。


#### 期待値 `$\mathbb{E}$`

```math
\mathbb{E}_{x \sim p}[f(x)] = \int f(x) \, p(x) \, dx
```

> **Note:** 厳密にはルベーグ測度 `$d\mu(x)$` に対する積分ですが、第5回で測度論を扱うまではリーマン積分の記法 `$dx$` を使います。

「`$x$` を分布 `$p$` からサンプリングしたとき、`$f(x)$` の平均値」。離散の場合は積分が総和になる:

```math
\mathbb{E}_{x \sim p}[f(x)] = \sum_{x} f(x) \, p(x)
```

VAE[^4] の目的関数 ELBO は期待値で書かれる:

```math
\text{ELBO} = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x}|\mathbf{z}) \right] - D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))
```

「エンコーダ `$q_\phi$` からサンプルした `$\mathbf{z}$` で、デコーダ `$p_\theta$` がデータ `$\mathbf{x}$` を復元する対数確率の期待値」に、KLダイバージェンス正則化項を引いたもの。


#### ノルム `$\|\cdot\|$`

```math
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^{n} x_i^2} \quad \text{(L2ノルム / ユークリッドノルム)}
```

```math
\|\mathbf{x}\|_1 = \sum_{i=1}^{n} |x_i| \quad \text{(L1ノルム / マンハッタンノルム)}
```

Attention[^1] で `$\sqrt{d_k}$` でスケーリングするのは、`$\mathbf{q}^\top \mathbf{k}$` の分散が `$d_k$` に比例するため、ノルムのスケールを揃える目的がある。


#### KL ダイバージェンス `$D_{\text{KL}}$`

2つの確率分布の「距離」（厳密には非対称なので距離ではない）:

```math
D_{\text{KL}}(q \| p) = \mathbb{E}_{q}\left[\log \frac{q(x)}{p(x)}\right] = \sum_{x} q(x) \log \frac{q(x)}{p(x)}
```

**性質**:
- `$D_{\text{KL}}(q \| p) \geq 0$`（非負性、ギブスの不等式）
- `$D_{\text{KL}}(q \| p) = 0 \iff q = p$`
- `$D_{\text{KL}}(q \| p) \neq D_{\text{KL}}(p \| q)$`（非対称）

VAE[^4] では `$D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))$` が正則化として機能する。エンコーダが出力する分布を事前分布 `$p(\mathbf{z}) = \mathcal{N}(0, I)$` に近づける役割を持つ。


### 3.4 集合論の記号 — データの「住所」を表す

#### 数の集合

| 記号 | 名前 | 意味 | 例 |
|:---:|:---|:---|:---|
| `$\mathbb{N}$` | 自然数 | `$\{0, 1, 2, \ldots\}$` or `$\{1, 2, 3, \ldots\}$` | クラスラベル `$y \in \{0, 1, \ldots, C-1\}$` |
| `$\mathbb{Z}$` | 整数 | `$\{\ldots, -2, -1, 0, 1, 2, \ldots\}$` | インデックス |
| `$\mathbb{R}$` | 実数 | 連続値全体 | 重みパラメータ `$w \in \mathbb{R}$` |
| `$\mathbb{R}^n$` | `$n$`次元実数ベクトル空間 | | 入力 `$\mathbf{x} \in \mathbb{R}^n$` |
| `$\mathbb{R}^{m \times n}$` | `$m \times n$` 実数行列の空間 | | 重み行列 `$W \in \mathbb{R}^{m \times n}$` |
| `$\mathbb{R}^+$` | 正の実数 | `$(0, \infty)$` | 標準偏差 `$\sigma \in \mathbb{R}^+$` |

> **Note:** **「`$\mathbf{x} \in \mathbb{R}^{768}$`」の読み方**: 「`$\mathbf{x}$` は768次元の実数ベクトル空間の要素」。つまり768個の実数値を並べたベクトル。BERT の隠れ層の次元が768なので、BERT の出力は `$\mathbb{R}^{768}$` に住んでいる。

#### 集合の演算

| 記号 | 読み | 意味 | 例 |
|:---:|:---|:---|:---|
| `$\in$` | 属する | 要素が集合に含まれる | `$x \in \mathbb{R}$` |
| `$\notin$` | 属さない | | `$-1 \notin \mathbb{N}$` (0始まりの場合) |
| `$\subset$` | 部分集合 | | `$\mathbb{N} \subset \mathbb{Z} \subset \mathbb{R}$` |
| `$\cup$` | 和集合 | OR | 訓練データ `$\cup$` 検証データ |
| `$\cap$` | 共通集合 | AND | `$A \cap B = \emptyset$`（互いに素） |
| `$\setminus$` | 差集合 | 引く | `$\mathbb{R} \setminus \{0\}$`（0を除く実数） |
| `$\emptyset$` | 空集合 | 要素なし | |
| `$|A|$` or `$\#A$` | 濃度 | 集合の要素数 | `$|\mathcal{D}| = N$`（データセットのサイズ） |

#### 直積（Cartesian Product）と冪集合（Power Set）

論文を読むと、空間を組み合わせて「住所」を作ることがよくある。

**直積**

```math
X \times Y = \{(x,y) : x\in X,\ y\in Y\}
```

例: 画像 `$\mathbf{x}\in\mathbb{R}^{H\times W\times C}$` とラベル `$y\in\{1,\ldots,C\}$` のペアは、

```math
(\mathbf{x},y)\in \mathbb{R}^{H\times W\times C}\times \{1,\ldots,C\}
```

という「直積空間」に住む。

**冪集合**

```math
\mathcal{P}(X) = \{A : A \subseteq X\}
```

冪集合は「`$X$` の部分集合全体の集合」。例えば特徴選択やサブセット最適化で登場する。

<details><summary>ミニ演習: 直積を読む</summary>
次を日本語1行に翻訳せよ。

```math
f: \mathbb{R}^d \times \mathbb{N} \to \mathbb{R}
```

<details><summary>解答</summary>
「`$d$` 次元実数ベクトルと自然数（インデックスなど）のペアを入力に取り、実数を返す関数」。
</details>
</details>

#### 区間記法

| 記法 | 意味 | 範囲 |
|:---|:---|:---|
| `$[a, b]$` | 閉区間 | `$a \leq x \leq b$` |
| `$(a, b)$` | 開区間 | `$a < x < b$` |
| `$[a, b)$` | 半開区間 | `$a \leq x < b$` |
| `$[0, 1]$` | — | 確率値の範囲。Sigmoid の値域 |
| `$(0, \infty)$` | — | 正の実数。ReLU の正の部分 |
| `$(-\infty, \infty)$` | — | `$\mathbb{R}$` 全体 |

#### データセットの集合表現

機械学習では、データセットを集合として記述する:

```math
\mathcal{D} = \{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^{N}
```

「`$N$` 個の入力-ラベルのペアからなるデータセット `$\mathcal{D}$`」


### 3.5 論理記号 — 数式の「接続詞」

#### 基本の論理記号

| 記号 | 読み | 意味 | Python |
|:---:|:---|:---|:---|
| `$\forall$` | for all | すべての〜について | `all(...)` |
| `$\exists$` | there exists | 〜が存在する | `any(...)` |
| `$\implies$` | implies | ならば | `if ... then ...` |
| `$\iff$` | if and only if | 同値 | `==` (論理的等価) |
| `$\land$` | and | かつ | `and` |
| `$\lor$` | or | または | `or` |
| `$\neg$` | not | 否定 | `not` |

#### 論文でよく見る論理表現

**1. 全称量化子 `$\forall$`**

```math
\forall x \in \mathbb{R}: \quad e^x > 0
```

「すべての実数 `$x$` について、`$e^x$` は正」


**2. 存在量化子 `$\exists$`**

```math
\exists \theta^* : \quad \mathcal{L}(\theta^*) \leq \mathcal{L}(\theta) \quad \forall \theta
```

「損失を最小にするパラメータ `$\theta^*$` が存在する」

**3. 含意 `$\implies$`**

```math
\text{Softmax}(\mathbf{z})_i > 0 \quad \forall i \implies \sum_i \text{Softmax}(\mathbf{z})_i = 1
```

Softmax[^1] の性質: 「すべての出力が正ならば、合計は1」。実際にはこれは Softmax の定義から自動的に成り立つ。

**4. 同値 `$\iff$`**

```math
\hat{y} = \arg\max_i p_i \iff p_{\hat{y}} \geq p_j \quad \forall j
```

「`$\hat{y}$` が argmax であることと、`$p_{\hat{y}}$` がすべての `$p_j$` 以上であることは同値」

<details><summary>ミニ演習: 含意の読み替え（対偶）</summary>
次の主張を「対偶」の形に書き換えよ。

```math
P \implies Q
```

また、次の2つが一般には同値でない理由を一言で述べよ。

```math
P \implies Q,\quad Q \implies P
```

<details><summary>解答</summary>
対偶は `$\neg Q \implies \neg P$`。含意は向きを持ち、一般には片方向の条件しか言っていないため、逆向き `$Q\implies P$` は別物。
</details>
</details>

> **Note:** 論理記号は「文章の圧縮」だ。記号を見て意味が出ないなら、逆に文章へ展開してから読み直すと一気に通る。
> 特に `$\forall$` と `$\exists$` は「範囲」が全てなので、`$x\in X$` を必ずセットで読む。
> 「どの集合の上で主張しているか」を落とすと、数式は途端に意味が崩れる。
> だから最初に「範囲」を読む。

<details><summary>論文英語と論理記号の対応</summary>
論文本文では記号の代わりに英語で書かれることが多い:

| 記号 | 英語表現 |
|:---:|:---|
| `$\forall$` | "for all", "for any", "for every" |
| `$\exists$` | "there exists", "there is" |
| `$\implies$` | "implies", "then", "it follows that" |
| `$\iff$` | "if and only if", "iff", "is equivalent to" |
| s.t. | "such that", "subject to" — `$\exists x \text{ s.t. } f(x) = 0$` |

特に "s.t." は最適化問題で頻出:
```math
\min_\theta \mathcal{L}(\theta) \quad \text{s.t.} \quad \|\theta\|_2 \leq \lambda
```
</details>

### 3.6 関数の記法 — 写像の読み方

#### 関数の定義域・値域

```math
f: \mathbb{R}^n \to \mathbb{R}^m
```

「`$f$` は `$n$` 次元実数ベクトルを受け取り、`$m$` 次元実数ベクトルを返す関数（写像）」

| 要素 | 記号 | 意味 |
|:---|:---|:---|
| 関数名 | `$f$` | 写像そのもの |
| 定義域 (domain) | `$\mathbb{R}^n$` | 入力の住所 |
| 値域 (codomain) | `$\mathbb{R}^m$` | 出力の住所 |
| 矢印 | `$\to$` | 「〜から〜への対応」 |

#### ニューラルネットワークを写像として読む

1層のニューラルネットワーク:

```math
f_\theta: \mathbb{R}^n \to \mathbb{R}^m, \quad f_\theta(\mathbf{x}) = \sigma(W\mathbf{x} + \mathbf{b})
```

- `$\theta = \{W, \mathbf{b}\}$`: パラメータ集合
- `$W \in \mathbb{R}^{m \times n}$`: 重み行列
- `$\mathbf{b} \in \mathbb{R}^m$`: バイアスベクトル
- `$\sigma$`: 活性化関数

**多層の合成**:

```math
f = f^{(L)} \circ f^{(L-1)} \circ \cdots \circ f^{(1)}
```

`$\circ$` は**関数合成**（composition）。`$(g \circ f)(x) = g(f(x))$`。


#### 特殊な関数記法

| 記法 | 意味 | 例 |
|:---|:---|:---|
| `$f: X \to Y$` | `$X$` から `$Y$` への写像 | `$\text{Softmax}: \mathbb{R}^C \to \Delta^{C-1}$` |
| `$f \circ g$` | 関数合成 | 多層ネットワーク |
| `$f^{-1}$` | 逆関数 | Normalizing Flow |
| `$f'(x)$` or `$\frac{df}{dx}$` | 導関数 | 勾配計算 |
| `$\nabla f$` | 勾配（多変数） | `$\nabla_\theta \mathcal{L}$` |
| `$\mathcal{O}(n)$` | 計算量 | Attention は `$\mathcal{O}(n^2 d)$` |
| `$\mathbb{1}[\cdot]$` | 指示関数 | `$\mathbb{1}[y = k]$` — one-hot |

> **Note:** **`$\Delta^{C-1}$`** は**確率単体** (probability simplex)。`$C$` 次元ベクトルで「全要素が非負かつ総和が1」を満たすものの集合:
> ```math
> \Delta^{C-1} = \left\{ \mathbf{p} \in \mathbb{R}^C : p_i \geq 0, \sum_{i=1}^{C} p_i = 1 \right\}
> ```
> Softmax[^1] の値域はまさにこの確率単体。


### 3.7 微分の記法 — 勾配の読み方

機械学習の最適化は**勾配降下法**に基づく。勾配を理解するには微分の記法を知る必要がある。ここでは微分の「計算方法」ではなく「記法の読み方」に集中する。計算の詳細は第2回以降で扱う。

#### 導関数（1変数）

```math
f'(x) = \frac{df}{dx} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
```

`$f'(x)$` と `$\frac{df}{dx}$` は同じもの。前者はラグランジュ記法、後者はライプニッツ記法と呼ばれる。

| 関数 | 導関数 | 機械学習での用途 |
|:---|:---|:---|
| `$f(x) = x^n$` | `$f'(x) = nx^{n-1}$` | べき乗の微分 |
| `$f(x) = e^x$` | `$f'(x) = e^x$` | Softmax の微分 |
| `$f(x) = \log x$` | `$f'(x) = \frac{1}{x}$` | Cross-Entropy の微分 |
| `$f(x) = \max(0, x)$` | `$f'(x) = \mathbb{1}[x > 0]$` | ReLU の微分 |
| `$\sigma(x) = \frac{1}{1+e^{-x}}$` | `$\sigma'(x) = \sigma(x)(1-\sigma(x))$` | Sigmoid の微分 |

#### 偏微分（多変数）

```math
\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x_1, \ldots, x_n)}{h}
```

「他の変数を固定して、`$x_i$` だけ動かしたときの変化率」。`$\partial$` (パーシャル/デル) が `$d$` (ディー) と異なるのは、多変数であることを明示するため。

#### 勾配ベクトル `$\nabla f$`

```math
\nabla f(\mathbf{x}) = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}
```

すべての偏微分を縦に並べたベクトル。「`$f$` が最も急速に増加する方向」を指す。勾配降下法では、この逆方向（`$-\nabla f$`）にパラメータを更新する。

#### 形状（shape）で読む: 勾配/Jacobian/Hessian の違い

ここで一度、記号を「形状」で固定する。微分で混乱する原因の半分は「何がスカラーで、何がベクトルか」を曖昧に読むことにある。

| 対象 | 入力 | 出力 | 微分の形 | 名前 |
|:---|:---|:---|:---|:---|
| `$f:\mathbb{R}^n\to\mathbb{R}$` | ベクトル | スカラー | `$\nabla f(\mathbf{x})\in\mathbb{R}^n$` | 勾配 |
| `$\mathbf{f}:\mathbb{R}^n\to\mathbb{R}^m$` | ベクトル | ベクトル | `$J(\mathbf{x})\in\mathbb{R}^{m\times n}$` | ヤコビ行列 |
| `$f:\mathbb{R}^n\to\mathbb{R}$` | ベクトル | スカラー | `$H(\mathbf{x})\in\mathbb{R}^{n\times n}$` | ヘッセ行列 |

「勾配はベクトル」「ヤコビは行列」「ヘッセも行列」。この3行だけは、反射で出てくるまで染み込ませたい。

<details><summary>ミニ演習: 形状の即答</summary>
次の関数の微分の形状（ベクトル/行列）を答えよ。

1. `$f(\mathbf{x}) = \|\mathbf{x}\|_2^2$`（`$\mathbf{x}\in\mathbb{R}^d$`）
2. `$\mathbf{f}(\mathbf{x}) = W\mathbf{x} + \mathbf{b}$`（`$W\in\mathbb{R}^{m\times n}$`）
3. `$g(W) = \|W\|_F^2$`（`$W\in\mathbb{R}^{m\times n}$`）

<details><summary>解答</summary>

1. 出力がスカラーなので勾配は `$\mathbb{R}^d$` のベクトル。
2. 出力が `$\mathbb{R}^m$` なのでヤコビは `$m\times n$` 行列（実際は定数で `$J=W$`）。
3. 出力がスカラーなので「`$W$` に関する勾配」は `$m\times n$` と同じ形（行列）になる。
</details>
</details>

#### 例: 偏微分を「文章」に落とす

```math
f(x,y) = x^2y
```

このとき、

```math
\frac{\partial f}{\partial x} = 2xy,\quad \frac{\partial f}{\partial y} = x^2
```

翻訳はこうだ。

1. `$\partial/\partial x$`: 「`$y$` を固定して `$x$` を動かす」ので `$x^2$` だけが微分され、`$2x$` になる。最後に `$y$` が掛かる。
2. `$\partial/\partial y$`: 「`$x$` を固定して `$y$` を動かす」ので、`$y$` は1、`$x^2$` は定数として残る。

ここで重要なのは、計算よりも「固定する変数を言えるか」だ。

#### 連鎖律（Chain Rule）— 誤差逆伝播法の心臓

合成関数の微分規則。誤差逆伝播（backpropagation）は、この規則を計算グラフに沿って系統的に適用するだけのアルゴリズムだ[^2]。

```math
\frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}
```

多変数版:

```math
\frac{\partial \mathcal{L}}{\partial W^{(l)}} = \frac{\partial \mathcal{L}}{\partial h^{(L)}} \cdot \frac{\partial h^{(L)}}{\partial h^{(L-1)}} \cdots \frac{\partial h^{(l+1)}}{\partial h^{(l)}} \cdot \frac{\partial h^{(l)}}{\partial W^{(l)}}
```

「損失 `$\mathcal{L}$` の第 `$l$` 層の重み `$W^{(l)}$` についての勾配は、出力層から第 `$l$` 層まで偏微分を**掛け算で伝播**させたもの」。これが **backpropagation** の名前の由来。

<details><summary>ミニ演習: 連鎖律を1行で読む</summary>
次を「記号読解プロトコル（3.0）」に従って1行に翻訳せよ。

```math
\frac{d}{dx}\log(\sigma(x)) = \frac{1}{\sigma(x)}\cdot \sigma(x)(1-\sigma(x))
```

<details><summary>解答</summary>
「`$\log$` の微分で `$1/\sigma(x)$` が出て、Sigmoid の微分で `$\sigma(x)(1-\sigma(x))$` が掛かる」なので、結果は `$1-\sigma(x)$`。
</details>
</details>



#### ヤコビ行列とヘッセ行列（プレビュー）

ここでは名前だけ紹介。詳細は第2回（線形代数）と第3回（最適化）で扱う。

| 行列 | 定義 | サイズ | 用途 |
|:---|:---|:---|:---|
| **ヤコビ行列** `$J$` | `$J_{ij} = \frac{\partial f_i}{\partial x_j}$` | `$m \times n$` | ベクトル値関数の微分。連鎖律の行列版 |
| **ヘッセ行列** `$H$` | `$H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$` | `$n \times n$` | 2次微分。曲率の情報。Newton 法で使用 |

### 3.8 確率論の記法 — 生成モデルの言語

深層生成モデルは本質的に確率モデルだ。確率論の記法を読めなければ、VAE[^4] も拡散モデル[^5] も理解できない。

#### 確率分布の記法

| 記法 | 読み方 | 意味 |
|:---|:---|:---|
| `$p(x)$` | 「`$p$` の `$x$`」 | `$x$` の確率（離散）/ 確率密度（連続） |
| `$p(x \mid y)$` | 「`$y$` が与えられたときの `$x$` の確率」 | 条件付き確率 |
| `$p(x, y)$` | 「`$x$` と `$y$` の同時確率」 | 同時分布 |
| `$x \sim p$` | 「`$x$` は `$p$` に従う」 | サンプリング |
| `$\mathcal{N}(\mu, \sigma^2)$` | 「平均 `$\mu$`、分散 `$\sigma^2$` の正規分布」 | ガウス分布 |
| `$\text{Cat}(\boldsymbol{\pi})$` | 「パラメータ `$\boldsymbol{\pi}$` のカテゴリカル分布」 | 離散分布 |

> **⚠️ Warning:** **注意**: 連続変数では `$p(x)$` は「確率」ではなく「密度」だ。`$p(x)=0.1$` と書いてあっても「`$x$` になる確率が0.1」とは言えない。確率は区間に対して定義される:
> ```math
> P(a \le X \le b) = \int_a^b p(x)\,dx
> ```

#### 確率の連鎖律（Chain Rule of Probability）

確率にも「連鎖律」がある。`$T$` 個の確率変数 `$x_{1:T}$` に対して、

```math
p(x_{1:T}) = \prod_{t=1}^{T} p(x_t \mid x_{1:t-1})
```

これが自己回帰モデル（LLM）の出発点だ。Transformer[^1] の「次トークン予測」は、この式の右辺の各因子をモデル化している。

<details><summary>ミニ演習: 連鎖律を展開せよ</summary>
`$p(x_1,x_2,x_3)$` を条件付き確率の積に分解せよ。

<details><summary>解答</summary>
`$p(x_1,x_2,x_3)=p(x_3\mid x_1,x_2)\,p(x_2\mid x_1)\,p(x_1)$`。
</details>
</details>

#### 独立・条件付き独立

独立は「情報が不要になる」ことを意味する。

```math
X \perp Y \iff p(x,y)=p(x)p(y)
```

条件付き独立は「`$Z$` を知ったら `$X$` と `$Y$` が独立になる」だ。

```math
X \perp Y \mid Z \iff p(x,y\mid z)=p(x\mid z)p(y\mid z)
```

生成モデルでは「潜在変数 `$Z$` を入れると条件付き独立が生まれる」ことが多い。VAE[^4] の標準的な仮定がまさにそれだ。

#### ベイズの定理

```math
p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta) \, p(\theta)}{p(\mathcal{D})}
```

| 項 | 名前 | 意味 |
|:---|:---|:---|
| `$p(\theta \mid \mathcal{D})$` | 事後分布 (posterior) | データを観測した後のパラメータの信念 |
| `$p(\mathcal{D} \mid \theta)$` | 尤度 (likelihood) | パラメータが与えられたときのデータの生成確率 |
| `$p(\theta)$` | 事前分布 (prior) | データを見る前のパラメータの信念 |
| `$p(\mathcal{D})$` | エビデンス (evidence) | 周辺尤度。正規化定数 |

```math
\underbrace{p(\theta \mid \mathcal{D})}_{\text{posterior}} \propto \underbrace{p(\mathcal{D} \mid \theta)}_{\text{likelihood}} \cdot \underbrace{p(\theta)}_{\text{prior}}
```

**VAE[^4] との接続**: VAE は `$p(\mathcal{D})$` = `$\int p(\mathbf{x}, \mathbf{z}) d\mathbf{z}$` が計算困難なため、事後分布 `$p(\mathbf{z} \mid \mathbf{x})$` を近似分布 `$q_\phi(\mathbf{z} \mid \mathbf{x})$` で近似する。これが変分推論の核心アイデア。

#### エントロピー / クロスエントロピー / KL（生成モデルの損失言語）

この3つは、同じ情報量の言葉を別の角度から言っているだけだ。

**エントロピー（不確実性）**

```math
H(p) = -\sum_x p(x)\log p(x)
```

**クロスエントロピー（当てに行くコスト）**

```math
H(p,q) = -\sum_x p(x)\log q(x)
```

**KL（ズレのコスト）**

```math
D_{\mathrm{KL}}(p\|q)=\sum_x p(x)\log \frac{p(x)}{q(x)} = H(p,q)-H(p)
```

この関係式は超重要だ。なぜなら「`$H(p)$` はデータ生成過程が決める定数」なので、学習で最小化できるのは実質的に `$H(p,q)$`（クロスエントロピー）になるからだ。LLM の学習で最小化しているのが Cross-Entropy なのは、この理由による。

<details><summary>ミニ演習: KLの非対称性</summary>
次の主張が一般には成り立たない理由を説明せよ。

```math
D_{\mathrm{KL}}(p\|q)=D_{\mathrm{KL}}(q\|p)
```

<details><summary>解答</summary>
定義の分数 `$\log(p/q)$` の向きが逆になるため、重み付けされる領域が変わる。特に `$p$` が大きいのに `$q$` が小さい領域は強く罰されるが、その逆は同じ強さでは罰されない。
</details>
</details>

#### 主要な確率分布

| 分布 | 記法 | パラメータ | 用途 |
|:---|:---|:---|:---|
| 正規 (Gaussian) | `$\mathcal{N}(\mu, \sigma^2)$` | 平均 `$\mu$`, 分散 `$\sigma^2$` | VAE の潜在空間、ノイズ |
| 多変量正規 | `$\mathcal{N}(\boldsymbol{\mu}, \Sigma)$` | 平均ベクトル, 共分散行列 | 多次元潜在変数 |
| カテゴリカル | `$\text{Cat}(\boldsymbol{\pi})$` | 確率ベクトル `$\boldsymbol{\pi}$` | 離散ラベル予測 |
| ベルヌーイ | `$\text{Bern}(p)$` | 成功確率 `$p$` | 2値分類 |
| 一様 | `$\text{Uniform}(a, b)$` | 区間 `$[a, b]$` | 初期化、事前分布 |


> **Note:** **ここまでの道のり**: 3.1 ギリシャ文字 → 3.2 添字 → 3.3 演算子 → 3.4 集合 → 3.5 論理 → 3.6 関数 → 3.7 微分 → 3.8 確率。これで数式を読む「語彙」と「文法」が揃った。Boss Battle で総力戦に臨もう。

### 3.9 Boss Battle: Transformer の Scaled Dot-Product Attention を完全読解

ここまでの知識を総動員して、機械学習で最も重要な式の一つを**一文字残らず**読む。

Vaswani et al. (2017) "Attention Is All You Need"[^1] の式 (1):

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
```

#### Step 1: 記号の確認

| 記号 | 意味 | 住所 |
|:---:|:---|:---|
| `$Q$` | Query 行列 | `$Q \in \mathbb{R}^{n \times d_k}$` |
| `$K$` | Key 行列 | `$K \in \mathbb{R}^{m \times d_k}$` |
| `$V$` | Value 行列 | `$V \in \mathbb{R}^{m \times d_v}$` |
| `$K^\top$` | `$K$` の転置 | `$K^\top \in \mathbb{R}^{d_k \times m}$` |
| `$d_k$` | Key の次元数 | `$d_k \in \mathbb{N}$` |
| `$\sqrt{d_k}$` | スケーリング因子 | `$\sqrt{d_k} \in \mathbb{R}^+$` |

#### Step 2: 計算の分解


**1. `$QK^\top \in \mathbb{R}^{n \times m}$`** — 類似度行列

`$(n \times d_k) \cdot (d_k \times m) = (n \times m)$`。`$n$` 個のクエリと `$m$` 個のキーの全ペアの内積を計算。

**2. `$\frac{QK^\top}{\sqrt{d_k}}$`** — スケーリング

内積の値は `$d_k$` が大きいほど大きくなる（各次元の寄与が加算されるため）。具体的には、`$Q$` と `$K$` の各要素が平均0、分散1のとき、`$\mathbf{q}^\top \mathbf{k}$` の分散は `$d_k$` になる[^1]。`$\sqrt{d_k}$` で割ることで分散を1に正規化する。

**3. `$\text{softmax}(\cdot)$`** — 確率分布への変換

各行に対して Softmax を適用。`$n$` 個のクエリそれぞれについて、`$m$` 個のキーに対する「注意の重み」が確率分布になる。

**4. `$\text{softmax}(\cdot) V$`** — 加重平均

`$(n \times m) \cdot (m \times d_v) = (n \times d_v)$`。重み付き平均で Value を集約。

#### Step 3: Python で完全再現
ここは「数式を動かせる」感覚を、コードなしで固定する。最小例で手計算してみよう。

**設定（超小型）**

- クエリは1個（`$n=1$`）、キーは2個（`$m=2$`）
- 次元は `$d_k=2$`
- `$q = \begin{pmatrix}1 & 0\end{pmatrix}$`（`$1\times 2$`）
- `$K = \begin{pmatrix}1 & 0\\ 0 & 1\end{pmatrix}$`（`$2\times 2$`）
- `$V = \begin{pmatrix}10 & 0\\ 0 & 10\end{pmatrix}$`（`$2\times 2$`）

このとき `$Q$` は `$q$` 1行だけの行列として `$Q=q$`。

**Step A: 類似度（内積）**

```math
QK^\top = \begin{pmatrix}1 & 0\end{pmatrix}
\begin{pmatrix}1 & 0\\ 0 & 1\end{pmatrix}
=
\begin{pmatrix}1 & 0\end{pmatrix}
```

つまり「キー1とは似ている（1）」「キー2とは似ていない（0）」。

**Step B: スケーリング**

```math
\frac{QK^\top}{\sqrt{d_k}}
=
\frac{1}{\sqrt{2}}
\begin{pmatrix}1 & 0\end{pmatrix}
=
\begin{pmatrix}\tfrac{1}{\sqrt{2}} & 0\end{pmatrix}
```

**Step C: Softmax（重み）**

```math
\mathrm{softmax}\left(\begin{pmatrix}\tfrac{1}{\sqrt{2}} & 0\end{pmatrix}\right)
=
\begin{pmatrix}
\frac{e^{1/\sqrt{2}}}{e^{1/\sqrt{2}}+e^0} &
\frac{e^0}{e^{1/\sqrt{2}}+e^0}
\end{pmatrix}
```

`$1/\sqrt{2}\approx 0.707$` なので `$e^{0.707}\approx 2.03$`。よって重みはおおよそ、

```math
\begin{pmatrix}
\frac{2.03}{2.03+1} &
\frac{1}{2.03+1}
\end{pmatrix}
=
\begin{pmatrix}
0.67 & 0.33
\end{pmatrix}
```

**Step D: 加重平均**

```math
\begin{pmatrix}0.67 & 0.33\end{pmatrix}
\begin{pmatrix}10 & 0\\ 0 & 10\end{pmatrix}
=
\begin{pmatrix}6.7 & 3.3\end{pmatrix}
```

読み替えると「値ベクトルの(10,0)を67%採用し、(0,10)を33%採用した混合」だ。Attention は、この混合を文脈に応じて動的に作る。

#### Step 4: Multi-Head Attention

単一の Attention を複数の「ヘッド」で並列実行し、結果を連結する[^1]:

```math
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
```

```math
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
```

- `$h$`: ヘッド数。原論文では `$h = 8$`
- `$W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$`: 第 `$i$` ヘッドの Query 射影
- `$d_k = d_v = d_{\text{model}} / h$`: 各ヘッドの次元。`$d_{\text{model}} = 512$` なら `$d_k = 64$`
- `$W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$`: 出力射影


> **Note:** **Boss Battle クリア！** Transformer の Attention 式を一文字残らず読めるようになった。Zone 1 で「なんとなく」理解していた Softmax、添字、スケーリングの意味が、今は**完全に**説明できるはずだ。

> **Checkpoint:** 数式の記号体系を一通り押さえた。後編ではツールとワークフローを整備して、「読む→実装→検証」を回せる状態に持ち込む。

> Progress: 100%

### 3.10 Notation 総仕上げ: ELBO の完全読解

Boss Battle で Attention を読んだ。同じプロトコルを使って、深層生成モデル理論の核心式 — VAE の ELBO[^4] に挑もう。第9回で本格的に扱うが、今は「記号が全部読める」ことを確認するのが目的だ。

```math
\mathcal{L}_{\text{ELBO}}(\theta,\phi;\mathbf{x}) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\!\left[\log p_\theta(\mathbf{x}|\mathbf{z})\right] - D_{\mathrm{KL}}\!\left(q_\phi(\mathbf{z}|\mathbf{x}) \,\|\, p(\mathbf{z})\right)
```

**Step 1: 記号の確認**（3.0 プロトコル ステップ1-5）

| 記号 | 読み | 型 | 住所 | 意味 |
|:---:|:---|:---|:---|:---|
| `$\mathcal{L}_{\text{ELBO}}$` | エル・エルボ | スカラー | `$\mathbb{R}$` | Evidence Lower Bound（証拠下界） |
| `$\theta$` | シータ | パラメータ | — | デコーダのパラメータ |
| `$\phi$` | ファイ | パラメータ | — | エンコーダのパラメータ |
| `$\mathbf{x}$` | ボールド エックス | ベクトル | `$\mathbb{R}^d$` | 観測データ（画像・テキスト等） |
| `$\mathbf{z}$` | ボールド ゼット | ベクトル | `$\mathbb{R}^k$` | 潜在変数（隠れた表現） |
| `$q_\phi(\mathbf{z}\mid\mathbf{x})$` | キュー ファイ | 分布 | — | 近似事後分布（エンコーダ出力） |
| `$p_\theta(\mathbf{x}\mid\mathbf{z})$` | ピー シータ | 分布 | — | 尤度（デコーダ出力） |
| `$p(\mathbf{z})$` | ピー ゼット | 分布 | — | 事前分布 `$= \mathcal{N}(0,I)$` |
| `$\mathbb{E}_{q_\phi}[\cdot]$` | 期待値（キュー下） | スカラー | `$\mathbb{R}$` | `$q_\phi$` 分布下での平均 |
| `$D_{\mathrm{KL}}(q\|p)$` | KL ダイバージェンス | スカラー | `$\mathbb{R}^{\geq 0}$` | 分布間のズレ（非対称） |

**Step 2: 式の構造 — 1行翻訳**

```math
\underbrace{\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\!\left[\log p_\theta(\mathbf{x}|\mathbf{z})\right]}_{\text{再構成項（大きいほどよい）}}
-
\underbrace{D_{\mathrm{KL}}\!\left(q_\phi(\mathbf{z}|\mathbf{x}) \,\|\, p(\mathbf{z})\right)}_{\text{正則化項（小さいほどよい）}}
```

「エンコーダ `$q_\phi$` が推定した潜在変数 `$\mathbf{z}$` からデータを再現する対数確率の期待値（再構成項）から、エンコーダ分布と事前分布のズレ（正則化項）を引いたもの」。最大化することで、データをよく再現しつつ潜在空間が構造を持つモデルを学習する。

**Step 3: なぜこれが「下界」なのか**

```math
\log p_\theta(\mathbf{x}) \;\geq\; \mathcal{L}_{\text{ELBO}}(\theta,\phi;\mathbf{x})
```

左辺 `$\log p_\theta(\mathbf{x}) = \log \int p_\theta(\mathbf{x},\mathbf{z})\,d\mathbf{z}$` は積分が解析的に解けず計算不能だ。右辺の ELBO は計算可能。だから ELBO を最大化することが、計算不能な「真の目標」への代理になる — これが変分推論の核心だ。証明は第9回で行う。

> **Note:** ここで確認してほしい: この式の記号は全て3.1〜3.8で学んだものだ。`$\mathbb{E}$`（期待値, 3.3）、`$D_{\mathrm{KL}}$`（KL, 3.3）、`$q_\phi, p_\theta, p$`（確率記法, 3.8）、`$\mathbf{x},\mathbf{z}\in\mathbb{R}^d$`（集合, 3.4）、`$\theta,\phi$`（ギリシャ文字, 3.1）。第9回まで「この記号はあの節で習った」と指差せる状態が完成した。

### 🎯 Quick Check — Zone 4

数式記法の理解度を確認しましょう:

1. **`$\nabla_\theta \mathcal{L}(\theta)$` を声に出して読み、意味を説明せよ**
   <details><summary>解答</summary>
   「ナブラ シータ エル シータ」= パラメータθに関する損失関数Lの勾配
   </details>

2. **Attentionの式 `$\text{Attention}(Q,K,V) = \text{softmax}(QK^\top/\sqrt{d_k})V$` で、`$\sqrt{d_k}$` で割る理由は？**
   <details><summary>解答</summary>
   内積の値が次元数dₖに応じて増大すると、Softmaxが極端に飽和して勾配消失するため、スケーリングで安定化する
   </details>

---

> **📖 続きは後編へ**
> [【後編】第1回: 概論 — ツールとワークフロー](/articles/ml-lecture-01-part2) では、Hugging Face・Git/jj・Obsidian・OSSライセンスを完全実装します。

---


### チェックポイント（Z4）

- `$\forall, \exists, \Rightarrow, \Leftrightarrow$` を文章として読める
- 写像 `$f: X \to Y$` を『入力空間→出力空間』として解釈できる
- 期待値 `$\mathbb{E}$` と確率 `$P(\cdot)$` の役割を区別できる
- Attention式を『記号の辞書→積→softmax→加重和』に分解できる
- `$D_{\mathrm{KL}}(p\|q)=H(p,q)-H(p)$` を「何が定数か」まで含めて説明できる
- Softmaxの「定数シフト不変性」`$\text{softmax}(z)=\text{softmax}(z+c\mathbf{1})$` を証明できる
- ELBO 式で `$\theta$` と `$\phi$` がそれぞれ何のパラメータか説明できる
## 参考文献

### 主要論文

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. *Advances in Neural Information Processing Systems*, 30.
<https://arxiv.org/abs/1706.03762>
[^2]: Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2015). Automatic Differentiation in Machine Learning: a Survey.
<https://arxiv.org/abs/1502.05767>
[^3]: Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network.
<https://arxiv.org/abs/1503.02531>
[^4]: Kingma, D. P. & Welling, M. (2013). Auto-Encoding Variational Bayes.
<https://arxiv.org/abs/1312.6114>
[^5]: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models.
<https://arxiv.org/abs/2006.11239>
[^6]: Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., & Le, M. (2022). Flow Matching for Generative Modeling.
<https://arxiv.org/abs/2210.02747>
[^7]: Peebles, W. & Xie, S. (2022). Scalable Diffusion Models with Transformers.
<https://arxiv.org/abs/2212.09748>
[^8]: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative Adversarial Networks.
<https://arxiv.org/abs/1406.2661>
---

**第1回 前編完 — 後編「ツールとワークフロー」および第2回「線形代数 I」に続く**

---

## 著者リンク

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

---

## ライセンス

本記事は [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)（クリエイティブ・コモンズ 表示 - 非営利 - 継承 4.0 国際）の下でライセンスされています。

### ⚠️ 利用制限について

**本コンテンツは個人の学習目的に限り利用可能です。**

**以下のケースは事前の明示的な許可なく利用することを固く禁じます:**

1. **企業・組織内での利用（営利・非営利問わず）**
   - 社内研修、教育カリキュラム、社内Wikiへの転載
   - 大学・研究機関での講義利用
   - 非営利団体での研修利用
   - **理由**: 組織内利用では帰属表示が削除されやすく、無断改変のリスクが高いため

2. **有料スクール・情報商材・セミナーでの利用**
   - 受講料を徴収する場での配布、スクリーンショットの掲示、派生教材の作成

3. **LLM/AIモデルの学習データとしての利用**
   - 商用モデルのPre-training、Fine-tuning、RAGの知識ソースとして本コンテンツをスクレイピング・利用すること

4. **勝手に内容を有料化する行為全般**
   - 有料note、有料記事、Kindle出版、有料動画コンテンツ、Patreon限定コンテンツ等

**個人利用に含まれるもの:**
- 個人の学習・研究
- 個人的なノート作成（個人利用に限る）
- 友人への元記事リンク共有

**組織での導入をご希望の場合**は、必ず著者に連絡を取り、以下を遵守してください:
- 全ての帰属表示リンクを維持
- 利用方法を著者に報告

**無断利用が発覚した場合**、使用料の請求およびSNS等での公表を行う場合があります。
