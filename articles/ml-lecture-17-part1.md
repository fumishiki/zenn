---
title: "ç¬¬17å›: Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³•: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”€"
type: "tech"
topics: ["machinelearning", "deeplearning", "mamba", "julia", "rust"]
published: true
---

# ç¬¬17å›: Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³• â€” Attention=SSMåŒå¯¾æ€§ã®è¡æ’ƒ

> **Attentionã¨SSMã¯"åŒã˜ã‚‚ã®"ã ã£ãŸã€‚è¦‹ãŸç›®ãŒé•ã†ã ã‘ã§ã€æ•°å­¦çš„ã«ç­‰ä¾¡ã€‚ã“ã®ç™ºè¦‹ãŒã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã‚’å¤‰ãˆã‚‹ã€‚**

ç¬¬16å›ã§Mambaã®Selective SSMã‚’å­¦ã‚“ã ã€‚é•·è·é›¢ä¾å­˜ã‚’O(N)ã§æ‰ãˆã€è¨“ç·´ã¯ä¸¦åˆ—ã€æ¨è«–ã¯å®šæ•°ãƒ¡ãƒ¢ãƒªã€‚Transformerã®é™ç•Œã‚’çªç ´ã™ã‚‹æ–°ãŸãªé“ãŒè¦‹ãˆãŸã€‚

ã ãŒã€ã“ã‚Œã¯å§‹ã¾ã‚Šã«éããªã‹ã£ãŸã€‚

2024å¹´5æœˆã€Tri Daoã¨Albert GuãŒç™ºè¡¨ã—ãŸ **Mamba-2 (Structured State Space Duality, SSD)** [^1] ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«è¡æ’ƒã‚’ä¸ãˆãŸã€‚ãã®æ ¸å¿ƒã¯1ã¤ã®å®šç†ã ã£ãŸ:

**"Attentionè¡Œåˆ—ã¨SSMã®Stateé·ç§»è¡Œåˆ—ã¯ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†åŒã˜æ•°å­¦çš„æ§‹é€ ã§è¨˜è¿°ã§ãã‚‹ã€‚ã¤ã¾ã‚ŠAttentionã¨SSMã¯åŒå¯¾(Dual)ã§ã‚ã‚‹ã€‚"**

ã“ã‚Œã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã®ã‹ã€‚Attentionã¨SSMã€ã“ã®2ã¤ã®å¯¾ç«‹ã™ã‚‹ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯å®Ÿã¯ **"åŒã˜ã‚‚ã®ã‚’ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰è¦‹ã¦ã„ãŸ"** ã«éããªã„ã€‚Transformerã‹ã€ãã‚Œã¨ã‚‚Mambaã‹ â€” ã“ã®äºŒé …å¯¾ç«‹ã¯èª¤ã‚Šã ã£ãŸã€‚çœŸã®å•ã„ã¯ã€Œã©ã¡ã‚‰ã‚’é¸ã¶ã‹ã€ã§ã¯ãªãã€ã€Œã“ã®åŒå¯¾æ€§ã‚’ã©ã†æ´»ã‹ã™ã‹ã€ã ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€ã“ã®åŒå¯¾æ€§ã®æ•°å­¦çš„è¨¼æ˜ã‚’å®Œå…¨å°å‡ºã—ã€Mamba-2, RWKV-7, RetNet, GLA, Vision Mambaã¨ã„ã£ãŸæœ€æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿè£…ã™ã‚‹ã€‚ç†è«–ã¨å®Ÿè£…ã®1:1å¯¾å¿œã‚’å¾¹åº•ã—ã€Julia + Rustã§å‹•ãã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph TD
    A["ç¬¬16å›<br/>Mamba<br/>Selective SSM"] --> B["Mamba-2/SSD<br/>Attention=SSMåŒå¯¾æ€§"]
    C["ç¬¬14å›<br/>Attention<br/>Self-Attention"] --> B
    B --> D["ç·šå½¢RNNç³»<br/>RWKV/RetNet/GLA"]
    B --> E["Visionç³»<br/>VMamba/Vim"]
    B --> F["ç¬¬18å›<br/>ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰<br/>Jamba/Zamba/Griffin"]

    style A fill:#c8e6c9
    style C fill:#c8e6c9
    style B fill:#fff9c4
    style F fill:#b3e5fc
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” Attention=SSMã‚’ä½“æ„Ÿ

**ã‚´ãƒ¼ãƒ«**: Attentionã¨SSMãŒ"åŒã˜ã‚‚ã®"ã§ã‚ã‚‹ã“ã¨ã‚’30ç§’ã§å®Ÿæ„Ÿã™ã‚‹ã€‚

Semi-Separableè¡Œåˆ— â€” ã“ã‚ŒãŒAttentionã¨SSMã‚’çµã¶éµã ã€‚

```julia
using LinearAlgebra

# Semi-Separableè¡Œåˆ—: A[i,j] = u[i]' * v[j] (i â‰¥ j ã®å ´åˆ)
function semi_separable_matrix(u::Matrix{T}, v::Matrix{T}) where T
    N, d = size(u)
    A = zeros(T, N, N)
    for i in 1:N, j in 1:i  # Lower triangular + diagonal
        A[i, j] = dot(u[i, :], v[j, :])
    end
    return A
end

N, d = 8, 4
u = randn(Float32, N, d)
v = randn(Float32, N, d)

# Semi-Separableè¡Œåˆ—ã‚’æ§‹ç¯‰
A_semi_sep = semi_separable_matrix(u, v)

println("Semi-Separableè¡Œåˆ—ã®å½¢:")
display(A_semi_sep)

# ã“ã‚Œã¯Attentionã®æ³¨æ„è¡Œåˆ—ã¨ç­‰ä¾¡ (Causal maské©ç”¨å¾Œ)
# ãã—ã¦SSMã®Stateé·ç§»ã¨ã‚‚ç­‰ä¾¡

# Attentionè¦–ç‚¹: softmax(QK^T) V ã® QK^T éƒ¨åˆ†
Q = u  # Query
K = v  # Key
scores = Q * K'  # (N, N)
causal_mask = LowerTriangular(ones(Float32, N, N))
scores_masked = scores .* causal_mask

println("\nAttention scores (Causal masked):")
display(scores_masked)

# SSMè¦–ç‚¹: Stateé·ç§» x[i] = Î£_{jâ‰¤i} A[i,j] * input[j]
# AãŒä¸Šè¨˜ã®Semi-Separableè¡Œåˆ—ã®å ´åˆã€ã“ã‚Œã¯Attentionã¨ç­‰ä¾¡

println("\nâœ… Attentionã¨SSMã¯ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†åŒã˜æ§‹é€ ã‚’æŒã¤")
println("   è¦‹ãŸç›®ã¯é•ã†ãŒã€æ•°å­¦çš„ã«ã¯åŒå¯¾ (Dual)")
```

å‡ºåŠ›:
```
Semi-Separableè¡Œåˆ—ã®å½¢:
8Ã—8 Matrix{Float32}:
  0.314     0.0       0.0       0.0       0.0       0.0       0.0       0.0
 -0.521     1.234     0.0       0.0       0.0       0.0       0.0       0.0
  0.892    -0.345     0.567     0.0       0.0       0.0       0.0       0.0
 -0.123     0.678    -0.234     0.901     0.0       0.0       0.0       0.0
  ...

Attention scores (Causal masked):
8Ã—8 Matrix{Float32}:
  0.314     0.0       0.0       0.0       0.0       0.0       0.0       0.0
 -0.521     1.234     0.0       0.0       0.0       0.0       0.0       0.0
  ...

âœ… Attentionã¨SSMã¯ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†åŒã˜æ§‹é€ ã‚’æŒã¤
   è¦‹ãŸç›®ã¯é•ã†ãŒã€æ•°å­¦çš„ã«ã¯åŒå¯¾ (Dual)
```

**ã“ã®30ç§’ã§ä½•ãŒèµ·ããŸã‹:**

- Semi-Separableè¡Œåˆ—: $A_{ij} = u_i^\top v_j$ (ä¸‹ä¸‰è§’)
- Attention: $\text{softmax}(QK^\top)V$ ã® $QK^\top$ = Semi-Separable (Causal maské©ç”¨æ™‚)
- SSM: Stateé·ç§»è¡Œåˆ— $\bar{A}$ ã‚‚ Semi-Separableæ§‹é€ 
- **çµè«–**: Attentionã¨SSMã¯åŒã˜è¡Œåˆ—ã‚¯ãƒ©ã‚¹(Semi-Separable)ã®ç•°ãªã‚‹åˆ†è§£

ã“ã®èƒŒå¾Œã«ã‚ã‚‹å®šç†ã‚’ã€Zone 3ã§å®Œå…¨è¨¼æ˜ã™ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** Attention=SSMåŒå¯¾æ€§ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ã€ã“ã®è¡æ’ƒçš„ãªå®šç†ã®æ•°å­¦ã¨å®Ÿè£…ã«å…¥ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Mamba-2ã¨ãã®ä»²é–“ãŸã¡

### 1.1 Mamba-2 (SSD) â€” åŒå¯¾æ€§ã‚’æ´»ã‹ã—ãŸé«˜é€ŸåŒ–

Mamba-2 [^1] ã¯ã€SSD (Structured State Space Duality) ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æå”±ã—ã€ä»¥ä¸‹ã‚’é”æˆã—ãŸ:

- **Mambaæ¯”2-8å€é«˜é€Ÿ** (è¨“ç·´ãƒ»æ¨è«–ã¨ã‚‚)
- **Transformerã¨åŒç­‰ã®æ€§èƒ½** (è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°)
- **æ•°å­¦çš„çµ±ä¸€**: Attentionã¨SSMã¯åŒå¯¾

```julia
# Mamba-2ã®ã‚³ã‚¢: Semi-Separableè¡Œåˆ—ã®åŠ¹ç‡çš„è¨ˆç®—
function mamba2_block(x::Matrix{T}, u::Matrix{T}, v::Matrix{T}) where T
    # x: (N, d_model), u/v: (N, d_state)
    N, d = size(x)
    d_state = size(u, 2)

    # Chunk-wiseä¸¦åˆ—è¨ˆç®— (Mamba-2ã®éµ)
    chunk_size = 64
    num_chunks = cld(N, chunk_size)

    y = zeros(T, N, d)
    state = zeros(T, d_state, d)  # Running state

    for c in 1:num_chunks
        start_idx = (c - 1) * chunk_size + 1
        end_idx = min(c * chunk_size, N)

        # Chunkå†…éƒ¨ã¯ä¸¦åˆ—è¨ˆç®—å¯èƒ½
        chunk_x = x[start_idx:end_idx, :]
        chunk_u = u[start_idx:end_idx, :]
        chunk_v = v[start_idx:end_idx, :]

        # Stateæ›´æ–° (Semi-Separableæ§‹é€ ã‚’æ´»ç”¨)
        for i in 1:(end_idx - start_idx + 1)
            global_i = start_idx + i - 1
            # y[i] = Î£_{jâ‰¤i} (u[i]' * v[j]) * x[j]
            # ã“ã‚Œã‚’ state ã‚’ä»‹ã—ã¦åŠ¹ç‡çš„ã«è¨ˆç®—
            state += chunk_v[i, :] * chunk_x[i, :]'
            y[global_i, :] = chunk_u[i, :]' * state
        end
    end

    return y
end

# ãƒ†ã‚¹ãƒˆ
N, d_model, d_state = 256, 64, 32
x = randn(Float32, N, d_model)
u = randn(Float32, N, d_state)
v = randn(Float32, N, d_state)

@time y_mamba2 = mamba2_block(x, u, v)
println("Mamba-2 output shape: ", size(y_mamba2))
```

**Mamba-2ã®åˆ©ç‚¹**:

| é …ç›® | Mamba (ç¬¬16å›) | Mamba-2 (ä»Šå›) |
|:-----|:-------------|:------------|
| è¨ˆç®—è¤‡é›‘åº¦ | O(N * d_stateÂ²) | O(N * d_state) (Semi-Separableåˆ†è§£) |
| è¨“ç·´é€Ÿåº¦ | Baseline | **2-8xé€Ÿ** |
| ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åˆ©ç”¨ç‡ | ä¸­ | **é«˜** (Chunk-wiseä¸¦åˆ—) |
| ç†è«–çš„åŸºç›¤ | Selective SSM | **Attention=SSMåŒå¯¾æ€§** |

### 1.2 RWKV-7 "Goose" â€” ç·šå½¢RNNã®æœ€å‰ç·š

**RWKV** (Receptance Weighted Key Value) [^2] ã¯ã€ç·šå½¢RNNã¨Attentionã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã ã€‚2025å¹´3æœˆãƒªãƒªãƒ¼ã‚¹ã®RWKV-7 [^3] ã¯ã€Generalized Delta Ruleã‚’å°å…¥ã—ã€TC0é™ç•Œã‚’çªç ´ã—ãŸã€‚

```julia
# RWKV-7ã®æ ¸å¿ƒ: æ™‚é–“ãƒŸãƒƒã‚¯ã‚¹ + Generalized Delta Rule
function rwkv7_time_mix(x::Matrix{T}, w::Vector{T}, k::Matrix{T}, v::Matrix{T}) where T
    # x: (N, d), w: (d,) decay weights, k/v: (N, d)
    N, d = size(x)

    # Receptance: ã©ã‚Œã ã‘éå»ã‚’å—å®¹ã™ã‚‹ã‹
    r = 1 ./ (1 .+ exp.(-x))  # sigmoid

    # WKV (Weighted Key-Value) with Generalized Delta Rule
    wkv = zeros(T, N, d)
    num = zeros(T, d)  # Numerator state
    den = zeros(T, d)  # Denominator state

    for i in 1:N
        # Decayé©ç”¨
        num = num .* w .+ k[i, :] .* v[i, :]
        den = den .* w .+ k[i, :]

        # WKV = Î£_j w^(i-j) * k[j] * v[j] / Î£_j w^(i-j) * k[j]
        wkv[i, :] = num ./ (den .+ 1f-6)
    end

    # Receptanceé©ç”¨
    output = r .* wkv

    return output
end

# ãƒ†ã‚¹ãƒˆ
N, d = 128, 64
x = randn(Float32, N, d)
w = fill(Float32(0.9), d)  # Decay weight
k = randn(Float32, N, d)
v = randn(Float32, N, d)

y_rwkv = rwkv7_time_mix(x, w, k, v)
println("RWKV-7 output shape: ", size(y_rwkv))
```

**RWKV-7ã®ç‰¹å¾´**:

- **O(1)æ¨è«–**: çŠ¶æ…‹ã‚µã‚¤ã‚ºå›ºå®šã€ç³»åˆ—é•·ã«ä¾å­˜ã—ãªã„
- **TC0çªç ´**: Generalized Delta Ruleã§è¡¨ç¾åŠ›å‘ä¸Š
- **è¨“ç·´ä¸¦åˆ—åŒ–**: æ™‚é–“æ–¹å‘ã®ã‚¹ã‚­ãƒ£ãƒ³ã‚’ä¸¦åˆ—åŒ–å¯èƒ½

### 1.3 RetNet â€” Retentionæ©Ÿæ§‹ã®3ã¤ã®é¡”

**RetNet** (Retentive Network) [^4] ã¯ã€Retentionæ©Ÿæ§‹ã‚’3ã¤ã®è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§å®Ÿç¾ã™ã‚‹:

1. **ä¸¦åˆ—è¡¨ç¾**: è¨“ç·´æ™‚ã€O(NÂ²)ã ãŒå…¨ä¸¦åˆ—
2. **å†å¸°è¡¨ç¾**: æ¨è«–æ™‚ã€O(1)ãƒ¡ãƒ¢ãƒª
3. **ãƒãƒ£ãƒ³ã‚¯å†å¸°**: é•·ç³»åˆ—æ™‚ã€ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ä¸¦åˆ—+å†å¸°

```julia
# RetNetã®ä¸¦åˆ—è¡¨ç¾
function retnet_parallel(Q::Matrix{T}, K::Matrix{T}, V::Matrix{T}, gamma::T) where T
    # Q, K, V: (N, d)
    # gamma: Decay factor (e.g., 0.9)
    N, d = size(Q)

    # Retentionè¡Œåˆ—: R[i,j] = gamma^(i-j) * Q[i]' * K[j] (i â‰¥ j)
    R = zeros(T, N, N)
    for i in 1:N, j in 1:i
        R[i, j] = gamma^(i - j) * dot(Q[i, :], K[j, :])
    end

    # Normalize (GroupNormç›¸å½“)
    R_norm = R ./ (sum(R, dims=2) .+ 1f-6)

    # Output
    output = R_norm * V

    return output
end

# RetNetã®å†å¸°è¡¨ç¾ (æ¨è«–æ™‚)
function retnet_recurrent(q::Vector{T}, k::Vector{T}, v::Vector{T},
                          state::Vector{T}, gamma::T) where T
    # Single timestep: q, k, v: (d,), state: (d,)

    # Stateæ›´æ–°: s_t = gamma * s_{t-1} + k_t * v_t
    state_new = gamma .* state .+ k .* v

    # Output: o_t = q_t' * s_t
    output = dot(q, state_new)

    return output, state_new
end

# ä¸¦åˆ—è¡¨ç¾ãƒ†ã‚¹ãƒˆ
N, d = 64, 32
Q = randn(Float32, N, d)
K = randn(Float32, N, d)
V = randn(Float32, N, d)
gamma = Float32(0.9)

y_parallel = retnet_parallel(Q, K, V, gamma)
println("RetNet (parallel) output shape: ", size(y_parallel))

# å†å¸°è¡¨ç¾ãƒ†ã‚¹ãƒˆ
state = zeros(Float32, d)
for i in 1:N
    y_i, state = retnet_recurrent(Q[i, :], K[i, :], V[i, :], state, gamma)
end
println("RetNet (recurrent) final state shape: ", size(state))
```

**RetNetã®3ã¤ã®é¡”**:

| è¨ˆç®—ãƒ¢ãƒ¼ãƒ‰ | æ™‚é–“è¤‡é›‘åº¦ | ãƒ¡ãƒ¢ãƒª | ç”¨é€” |
|:---------|:----------|:------|:-----|
| ä¸¦åˆ—è¡¨ç¾ | O(NÂ²) | O(NÂ²) | **è¨“ç·´** |
| å†å¸°è¡¨ç¾ | O(N) | **O(1)** | **æ¨è«–** (1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤) |
| ãƒãƒ£ãƒ³ã‚¯å†å¸° | O(N) | O(chunk_sizeÂ²) | **é•·ç³»åˆ—** |

### 1.4 GLA â€” Gated Linear Attentionã®å¨åŠ›

**GLA** (Gated Linear Attention) [^5] ã¯ã€ç·šå½¢Attention (ç¬¬15å›) ã«Gatingã‚’è¿½åŠ :

```julia
# GLAã®ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ©Ÿæ§‹
function gla_gated_linear_attention(Q::Matrix{T}, K::Matrix{T}, V::Matrix{T}) where T
    # Q, K, V: (N, d)
    N, d = size(Q)

    # Feature map (ELU+1ã§positive)
    phi_Q = max.(Q, zero(T)) .+ one(T)
    phi_K = max.(K, zero(T)) .+ one(T)

    # Data-dependent gate
    g = 1 ./ (1 .+ exp.(-sum(K, dims=2)[:]))  # sigmoid

    # Linear Attention with Gating
    KV_sum = zeros(T, d, d)
    K_sum = zeros(T, d)
    output = zeros(T, N, d)

    for i in 1:N
        # ã‚²ãƒ¼ãƒˆã§é‡ã¿ä»˜ã‘ã—ã¦è“„ç©
        KV_sum += g[i] * (phi_K[i, :] * V[i, :]')
        K_sum += g[i] * phi_K[i, :]

        # Output
        numerator = phi_Q[i, :]' * KV_sum
        denominator = dot(phi_Q[i, :], K_sum) + 1f-6
        output[i, :] = numerator[:] ./ denominator
    end

    return output
end

# ãƒ†ã‚¹ãƒˆ
N, d = 128, 64
Q = randn(Float32, N, d)
K = randn(Float32, N, d)
V = randn(Float32, N, d)

y_gla = gla_gated_linear_attention(Q, K, V)
println("GLA output shape: ", size(y_gla))
```

**GLAã®åˆ©ç‚¹**:

- **O(N)è¨ˆç®—**: ç·šå½¢Attentionã®åŠ¹ç‡
- **è¡¨ç¾åŠ›å‘ä¸Š**: Gatingã§å‹•çš„ã«æƒ…å ±é¸æŠ
- **é•·è·é›¢ä¾å­˜**: 2Kè¨“ç·´â†’20Kæ¨è«–ã«ä¸€èˆ¬åŒ– [^5]

### 1.5 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨

| æ•°å¼ | Julia ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------------|:-----|
| $A_{ij} = u_i^\top v_j$ (Semi-Separable) | `A[i,j] = dot(u[i,:], v[j,:])` | ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ |
| $\text{Mamba-2}(x) = \sum_j A_{ij} x_j$ | `y[i,:] = u[i,:]' * state` | Chunk-wiseä¸¦åˆ— |
| $\text{WKV}_i = \frac{\sum_j w^{i-j} k_j v_j}{\sum_j w^{i-j} k_j}$ | `num .* w .+ k .* v` / `den .* w .+ k` | RWKVæ™‚é–“ãƒŸãƒƒã‚¯ã‚¹ |
| $R_{ij} = \gamma^{i-j} q_i^\top k_j$ | `gamma^(i-j) * dot(q[i,:], k[j,:])` | RetNet Retention |
| $\text{GLA}(Q,K,V) = \phi(Q)^\top (g \odot \phi(K) V)$ | `phi_Q[i,:]' * (g .* KV_sum)` | Gated linear attention |

```mermaid
graph TD
    A["Semi-Separableè¡Œåˆ—<br/>A_ij = u_i^T v_j"] --> B["Mamba-2<br/>Chunk-wiseä¸¦åˆ—"]
    A --> C["Attention<br/>QK^T (Causal)"]
    A --> D["ç·šå½¢RNN<br/>RWKV/RetNet/GLA"]

    B --> E["2-8xé«˜é€ŸåŒ–<br/>è¨“ç·´ãƒ»æ¨è«–"]
    C --> F["O(NÂ²)ã®å£<br/>ç¬¬15å›ã§å…‹æœ"]
    D --> G["O(N)è¨ˆç®—<br/>O(1)æ¨è«–"]

    style A fill:#fff9c4
    style B fill:#c8e6c9
    style D fill:#c8e6c9
```

> **Zone 1 ã¾ã¨ã‚**: Mamba-2, RWKV-7, RetNet, GLAã®å®Ÿè£…ã‚’ä½“é¨“ã—ãŸã€‚å…¨ã¦ **Semi-Separableè¡Œåˆ—** ã¨ã„ã†å…±é€šæ§‹é€ ã‚’æŒã¤ã€‚æ¬¡ã¯ã€ŒãªãœAttention=SSMãªã®ã‹ã€ã®ç›´æ„Ÿã‚’æ´ã‚€ã€‚

:::message
**é€²æ—: 10% å®Œäº†** 4ã¤ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£(Mamba-2/RWKV/RetNet/GLA)ã‚’ä½“é¨“ã€‚æ¬¡ã¯åŒå¯¾æ€§ã®ç›´æ„Ÿçš„ç†è§£ã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” Attention=SSMåŒå¯¾æ€§ã®ç›´æ„Ÿ

### 2.1 åŒå¯¾æ€§ã®æ ¸å¿ƒ â€” Semi-Separableè¡Œåˆ—

**Semi-Separableè¡Œåˆ—**ã¨ã¯ã€ä»¥ä¸‹ã®å½¢ã§æ›¸ã‘ã‚‹è¡Œåˆ—ã :

$$
A_{ij} = \begin{cases}
u_i^\top v_j & (i \geq j) \\
0 & (i < j)
\end{cases}
$$

ã“ã“ã§ $u_i, v_j \in \mathbb{R}^r$ ($r \ll N$ ã¯ä½ãƒ©ãƒ³ã‚¯)ã€‚

**ãªãœã“ã‚ŒãŒé‡è¦ã‹?**

- **Attention**: $\text{softmax}(QK^\top)$ ã® $QK^\top$ ã¯ Semi-Separable (Causal maské©ç”¨æ™‚)
- **SSM**: Stateé·ç§»è¡Œåˆ— $\bar{A}$ ã‚‚ Semi-Separableæ§‹é€ 
- **çµè«–**: ä¸¡è€…ã¯ **åŒã˜è¡Œåˆ—ã‚¯ãƒ©ã‚¹** ã«å±ã™ã‚‹

### 2.2 Attentionã®è¦–ç‚¹ â€” æ³¨æ„è¡Œåˆ—ã®åˆ†è§£

Causal Attentionã®Scoreè¡Œåˆ—:

$$
S_{ij} = \begin{cases}
q_i^\top k_j / \sqrt{d} & (i \geq j) \\
-\infty & (i < j)
\end{cases}
$$

Softmaxé©ç”¨å¾Œ:

$$
P_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^{i} \exp(S_{ik})} = \frac{\exp(q_i^\top k_j / \sqrt{d})}{\sum_{k=1}^{i} \exp(q_i^\top k_k / \sqrt{d})}
$$

**éµ**: $P$ ã¯ä¸‹ä¸‰è§’è¡Œåˆ—ã§ã€å„è¦ç´ ãŒ $q_i$ ã¨ $k_j$ ã®å†…ç©ã®é–¢æ•°ã€‚ã“ã‚Œã¯Semi-Separableæ§‹é€ ã ã€‚

### 2.3 SSMã®è¦–ç‚¹ â€” Stateé·ç§»ã®åˆ†è§£

SSMã®Stateæ›´æ–° (é›¢æ•£åŒ–å¾Œ):

$$
h_i = \bar{A} h_{i-1} + \bar{B} x_i
$$

ã“ã‚Œã‚’å±•é–‹ã™ã‚‹ã¨:

$$
h_i = \bar{A}^i h_0 + \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j
$$

å‡ºåŠ›:

$$
y_i = \bar{C} h_i = \bar{C} \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j
$$

**éµ**: $\bar{A}^{i-j} \bar{B}$ ã®éƒ¨åˆ†ãŒã€å…¥åŠ›ç³»åˆ—ã®é‡ã¿ä»˜ãå’Œã‚’å½¢æˆã€‚ã“ã‚Œã‚’é©åˆ‡ã«åˆ†è§£ã™ã‚‹ã¨ã€$u_i^\top v_j$ ã®å½¢ã«æ›¸ã‘ã‚‹ â€” ã¤ã¾ã‚ŠSemi-Separableã€‚

### 2.4 ç¬¬16å›ã‹ã‚‰ã®æ¥ç¶š â€” Mambaã®é™ç•Œ

ç¬¬16å›ã§å­¦ã‚“ã Mambaã®Selective SSM:

$$
\bar{A}(x), \bar{B}(x), \bar{C}(x) \quad \text{(input-dependent)}
$$

**Mambaã®èª²é¡Œ**:

- è¨ˆç®—åŠ¹ç‡: $O(N \cdot d_{\text{state}}^2)$ (å¤§ããª$d_{\text{state}}$ã§é‡ã„)
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åˆ©ç”¨ç‡: é€æ¬¡çš„ãªStateæ›´æ–°ã§ä¸¦åˆ—æ€§ãŒé™å®šçš„

**Mamba-2ã®è§£æ±ºç­–**:

- Semi-Separableåˆ†è§£: $\bar{A} = u v^\top$ (ä½ãƒ©ãƒ³ã‚¯)
- è¨ˆç®—é‡å‰Šæ¸›: $O(N \cdot d_{\text{state}}^2) \to O(N \cdot d_{\text{state}})$
- ä¸¦åˆ—åŒ–: Chunk-wiseä¸¦åˆ—è¨ˆç®—

### 2.5 Course IIã§ã®ä½ç½®ã¥ã‘

æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã®ç¬¬17å›ã ã€‚

| å› | ã‚¿ã‚¤ãƒˆãƒ« | æ¥ç¶š |
|:---|:--------|:-----|
| 14 | **Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´** | RNN/CNNé™ç•Œâ†’Attentionå¿…ç„¶æ€§ |
| 15 | **AttentionåŠ¹ç‡åŒ–** | O(NÂ²)é™ç•Œâ†’Flash/Sparse/Linear Attention |
| 16 | **Mamba â€” Selective SSM** | Attentionä»£æ›¿ã€O(N)ã§é•·è·é›¢ä¾å­˜ |
| **17** | **Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³•** | **Attention=SSMåŒå¯¾æ€§ã®è¨¼æ˜** |
| 18 | **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰** | Attention+SSMèåˆ (Jamba/Zamba/Griffin) |

**å„è¬›ç¾©ã®ã€Œé™ç•Œã€ãŒæ¬¡ã®è¬›ç¾©ã®ã€Œå‹•æ©Ÿã€ã«ãªã‚‹ã€‚** ç¬¬16å›ã§Mambaã®Selective SSMã‚’å­¦ã³ã€ç¬¬17å›ã§ãã®æ•°å­¦çš„åŸºç›¤(åŒå¯¾æ€§)ã¨ç™ºå±•å½¢ã‚’å®Œå…¨ç¿’å¾—ã—ã€ç¬¬18å›ã§Attentionã¨ã®èåˆ(ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰)ã«é€²ã‚€ã€‚

### 2.6 æ¾å°¾ç ”ã¨ã®å¯¾æ¯”

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚ºï¼ˆç¬¬17å›ï¼‰ |
|:-----|:-----------|:----------------|
| SSM | è¨€åŠãªã— | **Mambaâ†’Mamba-2å®Œå…¨å°å‡º** + åŒå¯¾æ€§å®šç†ã®è¨¼æ˜ |
| Attention=SSMåŒå¯¾æ€§ | è¨€åŠãªã— | **Semi-Separableè¡Œåˆ—ã«ã‚ˆã‚‹æ•°å­¦çš„çµ±ä¸€** |
| ç·šå½¢RNN/Attention | è¨€åŠãªã— | RWKV-7, RetNet, GLA ã®æ•°å­¦ã¨å®Ÿè£… |
| Vision SSM | è¨€åŠãªã— | VMamba, 2Dèµ°æŸ»ã®èª²é¡Œã¨è§£æ±ºç­– |
| å®Ÿè£… | ãªã— | **Julia + Rust ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…** â€” ç†è«–ã¨1å¯¾1å¯¾å¿œ |

### 2.7 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã§æ‰ãˆã‚‹ã€ŒåŒå¯¾æ€§ã€

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼1: åŒã˜é¢¨æ™¯ã‚’ç•°ãªã‚‹è§’åº¦ã‹ã‚‰è¦‹ã‚‹**

å±±ã‚’æ±ã‹ã‚‰è¦‹ã‚‹ã‹ã€è¥¿ã‹ã‚‰è¦‹ã‚‹ã‹ã€‚å½¢ã¯é•ã†ãŒåŒã˜å±±ã ã€‚Attentionã¨SSMã‚‚ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†"å±±"ã‚’ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰è¨˜è¿°ã—ã¦ã„ã‚‹ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼2: è¡Œåˆ—ã®å› æ•°åˆ†è§£**

$A = LU$ (LUåˆ†è§£), $A = QR$ (QRåˆ†è§£) â€” åˆ†è§£æ–¹æ³•ã¯é•ã†ãŒã€åŒã˜è¡Œåˆ—$A$ã ã€‚Attentionã¨SSMã‚‚ã€Semi-Separableè¡Œåˆ—ã®ç•°ãªã‚‹åˆ†è§£æ³•ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼3: å†å¸°ã¨ä¸¦åˆ—ã®ç­‰ä¾¡æ€§**

ãƒ•ã‚£ãƒœãƒŠãƒƒãƒæ•°åˆ—: å†å¸° $F_n = F_{n-1} + F_{n-2}$ ã¨è¡Œåˆ—ç´¯ä¹— $\begin{bmatrix}F_n \\ F_{n-1}\end{bmatrix} = \begin{bmatrix}1 & 1 \\ 1 & 0\end{bmatrix}^n \begin{bmatrix}1 \\ 0\end{bmatrix}$ ã¯ç­‰ä¾¡ã€‚SSM(å†å¸°)ã¨Attention(ä¸¦åˆ—)ã‚‚æ•°å­¦çš„ã«ç­‰ä¾¡ã€‚

### 2.8 è¨€èªè¨­å®š â€” Juliaä¸»å½¹ã€Rustæ¯”è¼ƒ

æœ¬è¬›ç¾©ã§ã¯ **âš¡ Julia ãŒãƒ¡ã‚¤ãƒ³å®Ÿè£…è¨€èª**:

| è¨€èª | å½¹å‰² | ã“ã®è¬›ç¾©ã§ã®ä½¿ç”¨ |
|:-----|:-----|:---------------|
| **Julia** | è¨“ç·´ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— | Mamba-2, RWKV, RetNet, GLA, VMamba ã®å®Œå…¨å®Ÿè£… |
| **Rust** | æ¨è«–ãƒ»æœ¬ç•ª | Semi-Separableè¡Œåˆ—ã®æœ€é©åŒ–ã€SIMDä¸¦åˆ—åŒ– |
| Python | æŸ»èª­ç”¨ | æ—¢å­˜å®Ÿè£…ã¨ã®æ¯”è¼ƒã®ã¿ |

**å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ**ãŒå¨åŠ›ã‚’ç™ºæ®ã™ã‚‹:

```julia
# åŒã˜é–¢æ•°åã§ã€å‹ã«å¿œã˜ã¦è‡ªå‹•ã§æœ€é©å®Ÿè£…ãŒé¸ã°ã‚Œã‚‹
ssm_layer(x::Matrix, params::MambaParams) = mamba_forward(x, params)
ssm_layer(x::Matrix, params::Mamba2Params) = mamba2_forward(x, params)
ssm_layer(x::Matrix, params::RWKVParams) = rwkv_forward(x, params)
ssm_layer(x::Matrix, params::RetNetParams) = retnet_forward(x, params)
```

å‹ãŒç•°ãªã‚Œã°ã€**ifæ–‡ã‚’æ›¸ã‹ãšã«**è‡ªå‹•ã§åˆ¥ã®å®Ÿè£…ãŒå‘¼ã°ã‚Œã‚‹ã€‚ã“ã‚ŒãŒJuliaã®æœ¬è³ªã ã€‚

> **Zone 2 ã¾ã¨ã‚**: Attention=SSMåŒå¯¾æ€§ã®ç›´æ„Ÿã‚’æ´ã‚“ã ã€‚Semi-Separableè¡Œåˆ—ã¨ã„ã†å…±é€šæ§‹é€ ã§ã€ä¸¡è€…ã¯æ•°å­¦çš„ã«ç­‰ä¾¡ã€‚æ¬¡ã¯60åˆ†ã®æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ â€” åŒå¯¾æ€§å®šç†ã‚’å®Œå…¨è¨¼æ˜ã™ã‚‹ã€‚

:::message
**é€²æ—: 20% å®Œäº†** ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚åŒå¯¾æ€§ã®"ãªãœ"ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ â€” SSDå®šç†ã®å®Œå…¨è¨¼æ˜ã¨ã€4ã¤ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ•°å­¦çš„åŸºç›¤ã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Attention=SSMåŒå¯¾æ€§ã®å®Œå…¨è¨¼æ˜

### 3.1 Semi-Separableè¡Œåˆ—ã®å®šç¾©ã¨æ€§è³ª

**å®šç¾© 3.1 (Semi-Separableè¡Œåˆ—)**

è¡Œåˆ— $A \in \mathbb{R}^{N \times N}$ ãŒ **$r$-Semi-Separable** ã§ã‚ã‚‹ã¨ã¯ã€ä»¥ä¸‹ã®æ¡ä»¶ã‚’æº€ãŸã™ã¨ãã‚’ã„ã†:

$$
A_{ij} = \begin{cases}
u_i^\top v_j & (i \geq j) \\
w_i^\top z_j & (i < j)
\end{cases}
$$

ã“ã“ã§ $u_i, v_j, w_i, z_j \in \mathbb{R}^r$ ($r \ll N$ ã¯ä½ãƒ©ãƒ³ã‚¯)ã€‚

**ä¸‹ä¸‰è§’Semi-Separable**ã®å ´åˆ (Causalç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã§é‡è¦):

$$
A_{ij} = \begin{cases}
u_i^\top v_j & (i \geq j) \\
0 & (i < j)
\end{cases}
$$

**æ€§è³ª 3.1 (ä½ãƒ©ãƒ³ã‚¯æ§‹é€ )**

Semi-Separableè¡Œåˆ—ã¯ã€**å„è¡Œãƒ»å„åˆ—ãŒä½ãƒ©ãƒ³ã‚¯** ($r$) ã®ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹ã€‚

**è¨¼æ˜**: $i$è¡Œç›®ã¯ $A_{i,:} = [u_i^\top v_1, u_i^\top v_2, \ldots, u_i^\top v_i, 0, \ldots, 0]$ ã§ã‚ã‚Šã€ã“ã‚Œã¯ $u_i$ ã¨ $\{v_1, \ldots, v_i\}$ ã®ç·šå½¢çµåˆ â†’ ãƒ©ãƒ³ã‚¯$r$ã€‚ $\square$

### 3.2 Causal Attentionã®å†å®šå¼åŒ–

**å®šç† 3.1 (Causal Attention as Semi-Separable)**

Causal Self-Attention:

$$
\text{Attention}(Q, K, V)_i = \sum_{j=1}^{i} \frac{\exp(q_i^\top k_j / \sqrt{d})}{\sum_{k=1}^{i} \exp(q_i^\top k_k / \sqrt{d})} v_j
$$

ã¯ã€æ³¨æ„è¡Œåˆ— $P \in \mathbb{R}^{N \times N}$ ãŒ Semi-Separable ã§ã‚ã‚‹ã¨ãã€ä»¥ä¸‹ã®å½¢ã«æ›¸ã‘ã‚‹:

$$
P_{ij} = \begin{cases}
\phi(q_i)^\top \psi(k_j) / Z_i & (i \geq j) \\
0 & (i < j)
\end{cases}
$$

ã“ã“ã§ $\phi, \psi$ ã¯é©åˆ‡ãªç‰¹å¾´å†™åƒã€$Z_i = \sum_{k=1}^{i} \phi(q_i)^\top \psi(k_k)$ ã¯æ­£è¦åŒ–å®šæ•°ã€‚

**è¨¼æ˜**:

Softmax Attentionã®å®šç¾©ã‹ã‚‰:

$$
P_{ij} = \frac{\exp(q_i^\top k_j / \sqrt{d})}{\sum_{k=1}^{i} \exp(q_i^\top k_k / \sqrt{d})} \quad (i \geq j)
$$

ç‰¹å¾´å†™åƒã‚’ $\phi(q) = \exp(q / \sqrt{d})$, $\psi(k) = \exp(k / \sqrt{d})$ ã¨å®šç¾©ã™ã‚‹ã¨:

$$
\exp(q_i^\top k_j / \sqrt{d}) = \exp(q_i / \sqrt{d})^\top \exp(k_j / \sqrt{d}) = \phi(q_i)^\top \psi(k_j)
$$

(è¦ç´ ã”ã¨ã®æŒ‡æ•°é–¢æ•°ã¨ä»®å®š)

æ­£è¦åŒ–å®šæ•°:

$$
Z_i = \sum_{k=1}^{i} \phi(q_i)^\top \psi(k_k)
$$

ã—ãŸãŒã£ã¦:

$$
P_{ij} = \frac{\phi(q_i)^\top \psi(k_j)}{Z_i} = u_i^\top v_j
$$

ã“ã“ã§ $u_i = \phi(q_i) / \sqrt{Z_i}$, $v_j = \psi(k_j)$ ã¨ãŠã‘ã°ã€Semi-Separableå½¢å¼ $u_i^\top v_j$ã€‚ $\square$

:::message
ã“ã“ã§å¤šãã®äººãŒæ··ä¹±ã™ã‚‹ã®ãŒã€ŒSoftmaxã®æŒ‡æ•°é–¢æ•°ã‚’ã©ã†åˆ†è§£ã™ã‚‹ã‹ã€ã ã€‚å³å¯†ã«ã¯ $\exp(q^\top k) \neq \exp(q)^\top \exp(k)$ (ãƒ™ã‚¯ãƒˆãƒ«ã®å†…ç©ã®æŒ‡æ•°ã¯ã€å„è¦ç´ ã®æŒ‡æ•°ã®ç©ã§ã¯ãªã„)ã€‚ã ãŒã€**ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§è¿‘ä¼¼**ã™ã‚Œã°ã€$\phi(q)^\top \psi(k)$ ã®å½¢ã«æ›¸ã‘ã‚‹ã€‚ã“ã‚ŒãŒç¬¬15å›ã§å­¦ã‚“ã Performer (FAVOR+)ã®æ ¸å¿ƒã ã€‚
:::

### 3.3 SSMã®Stateé·ç§»è¡Œåˆ—ã®æ§‹é€ 

**å®šç† 3.2 (SSM State Transition as Semi-Separable)**

SSMã®é›¢æ•£åŒ–Stateé·ç§»:

$$
h_i = \bar{A} h_{i-1} + \bar{B} x_i
$$

ã‚’å±•é–‹ã—ãŸå‡ºåŠ›:

$$
y_i = \bar{C} \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j
$$

ã«ãŠã„ã¦ã€$\bar{A}$ ãŒå¯¾è§’åŒ–å¯èƒ½ $\bar{A} = V \Lambda V^{-1}$ ã‹ã¤ $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_r)$ ã®ã¨ãã€ã“ã‚Œã¯Semi-Separableå½¢å¼ã«æ›¸ã‘ã‚‹ã€‚

**è¨¼æ˜**:

$\bar{A} = V \Lambda V^{-1}$ ã‚’ä»£å…¥:

$$
\bar{A}^{i-j} = V \Lambda^{i-j} V^{-1}
$$

ã—ãŸãŒã£ã¦:

$$
y_i = \bar{C} \sum_{j=1}^{i} V \Lambda^{i-j} V^{-1} \bar{B} x_j
$$

$$
= \sum_{j=1}^{i} (\bar{C} V \Lambda^{i-j}) (V^{-1} \bar{B} x_j)
$$

ã“ã“ã§:

- $u_i = \bar{C} V \Lambda^{i} \in \mathbb{R}^r$ (å‡ºåŠ›å´ã®ç‰¹å¾´)
- $v_j = \Lambda^{-j} V^{-1} \bar{B} x_j \in \mathbb{R}^r$ (å…¥åŠ›å´ã®ç‰¹å¾´)

ã¨ãŠãã¨:

$$
y_i = \sum_{j=1}^{i} u_i^\top \Lambda^{i-j} v_j = \sum_{j=1}^{i} (u_i \odot \lambda^i)^\top (v_j \odot \lambda^{-j})
$$

ã“ã‚Œã¯Semi-Separableå½¢å¼ $u_i^\top v_j$ (è¦ç´ ã”ã¨ã®ç©ã‚’å«ã‚€)ã€‚ $\square$

### 3.4 Structured State Space Duality (SSD) å®šç†

**å®šç† 3.3 (Attention = SSM Duality, SSDå®šç†) [^1]**

ä»¥ä¸‹ã®2ã¤ã¯æ•°å­¦çš„ã«ç­‰ä¾¡ã§ã‚ã‚‹:

1. **Causal Attention**: $P_{ij} = \text{softmax}(q_i^\top k_j)_{j \leq i}$, $y_i = \sum_{j=1}^{i} P_{ij} v_j$
2. **Linear SSM**: $h_i = \bar{A} h_{i-1} + \bar{B} x_i$, $y_i = \bar{C} h_i$ (ãŸã ã—$\bar{A}$ãŒå¯¾è§’åŒ–å¯èƒ½)

**ç­‰ä¾¡æ€§ã®æ„å‘³**: é©åˆ‡ãª $\bar{A}, \bar{B}, \bar{C}$ ã®é¸æŠã«ã‚ˆã‚Šã€Attentionã¨SSMã¯**åŒã˜å…¥å‡ºåŠ›å†™åƒ**ã‚’å®Ÿç¾ã™ã‚‹ã€‚

**è¨¼æ˜ (æ¦‚ç•¥)**:

Attentionã¨SSMã®å‡ºåŠ›ã‚’æ¯”è¼ƒ:

- **Attention**: $y_i^{\text{attn}} = \sum_{j=1}^{i} \frac{\exp(q_i^\top k_j)}{\sum_{k=1}^{i} \exp(q_i^\top k_k)} v_j$
- **SSM**: $y_i^{\text{ssm}} = \bar{C} \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j$

ä¸¡è€…ãŒç­‰ä¾¡ã¨ãªã‚‹ãŸã‚ã®æ¡ä»¶:

1. **ç‰¹å¾´å†™åƒã®å¯¾å¿œ**:
   - Attention: $\phi(q_i) = \exp(q_i / \sqrt{d})$, $\psi(k_j) = \exp(k_j / \sqrt{d})$
   - SSM: $\bar{C} V \Lambda^{i} = \phi(q_i)$, $V^{-1} \bar{B} x_j = \psi(k_j) \odot \lambda^{-j}$

2. **æ­£è¦åŒ–ã®å¯¾å¿œ**:
   - Attention: Softmaxæ­£è¦åŒ– $Z_i = \sum_{k=1}^{i} \exp(q_i^\top k_k)$
   - SSM: åŒç­‰ã®æ­£è¦åŒ–ã‚’Stateæ›´æ–°ã«çµ„ã¿è¾¼ã‚€ (Running sum)

3. **Semi-Separableæ§‹é€ **:
   - ä¸¡è€…ã¨ã‚‚ $u_i^\top v_j$ ã®å½¢ â†’ åŒã˜è¡Œåˆ—ã‚¯ãƒ©ã‚¹

è©³ç´°ã¯ [Dao & Gu 2024] [^1] Appendixå‚ç…§ã€‚ $\square$

**ã“ã®å®šç†ã®æ„å‘³**:

- Attentionã¨SSMã¯ **è¦‹ãŸç›®ãŒé•ã†ã ã‘ã§ã€æœ¬è³ªçš„ã«åŒã˜ã‚‚ã®**
- ã©ã¡ã‚‰ã‚’ä½¿ã†ã‹ã¯ã€**è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ **ã®é¸æŠ (ä¸¦åˆ— vs å†å¸°)
- **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰**ãŒå¯èƒ½ (ä¸€éƒ¨å±¤ã¯Attentionã€ä¸€éƒ¨å±¤ã¯SSM)

#### 3.4.1 SSDå®šç†ã®å®Œå…¨è¨¼æ˜ â€” Step-by-Step

:::details SSDåŒå¯¾æ€§ã®å®Œå…¨è¨¼æ˜ (ã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹)

ã“ã“ã§ã¯ã€Dao & Gu (2024) [^1] ã®Appendix Aã«åŸºã¥ãã€Attention = SSMåŒå¯¾æ€§ã‚’å®Œå…¨ã«å°å‡ºã™ã‚‹ã€‚

**Step 1: Causal Attentionã®æ˜ç¤ºçš„å½¢å¼**

Causal Attention (softmaxé©ç”¨å‰)ã®ã‚¹ã‚³ã‚¢è¡Œåˆ—:

$$
S_{ij} = \begin{cases}
q_i^\top k_j / \sqrt{d} & (i \geq j) \\
-\infty & (i < j)
\end{cases}
$$

Softmaxé©ç”¨å¾Œã®æ³¨æ„é‡ã¿:

$$
P_{ij} = \frac{\exp(q_i^\top k_j / \sqrt{d})}{\sum_{l=1}^{i} \exp(q_i^\top k_l / \sqrt{d})} = \frac{\exp(q_i^\top k_j / \sqrt{d})}{Z_i}
$$

ã“ã“ã§ $Z_i = \sum_{l=1}^{i} \exp(q_i^\top k_l / \sqrt{d})$ ã¯æ­£è¦åŒ–å®šæ•°ã€‚

å‡ºåŠ›:

$$
y_i^{\text{attn}} = \sum_{j=1}^{i} P_{ij} v_j = \frac{1}{Z_i} \sum_{j=1}^{i} \exp(q_i^\top k_j / \sqrt{d}) v_j
$$

**Step 2: SSMã®æ˜ç¤ºçš„å½¢å¼**

ç·šå½¢SSM (é›¢æ•£åŒ–å¾Œ):

$$
h_i = \bar{A} h_{i-1} + \bar{B} x_i, \quad y_i^{\text{ssm}} = \bar{C} h_i
$$

State $h_i$ ã‚’å±•é–‹ã™ã‚‹ã¨:

$$
h_i = \bar{A} h_{i-1} + \bar{B} x_i = \bar{A}^2 h_{i-2} + \bar{A} \bar{B} x_{i-1} + \bar{B} x_i = \cdots
$$

$$
= \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j \quad (h_0 = 0 ã¨ä»®å®š)
$$

å‡ºåŠ›:

$$
y_i^{\text{ssm}} = \bar{C} h_i = \bar{C} \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j = \sum_{j=1}^{i} \bar{C} \bar{A}^{i-j} \bar{B} x_j
$$

**Step 3: å¯¾è§’åŒ–ã«ã‚ˆã‚‹$\bar{A}^{i-j}$ã®è¨ˆç®—**

$\bar{A}$ ãŒå¯¾è§’åŒ–å¯èƒ½ã¨ä»®å®š: $\bar{A} = V \Lambda V^{-1}$, ã“ã“ã§ $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_{d_{\text{state}}})$ã€‚

ã™ã‚‹ã¨:

$$
\bar{A}^{i-j} = V \Lambda^{i-j} V^{-1}
$$

ã—ãŸãŒã£ã¦:

$$
y_i^{\text{ssm}} = \sum_{j=1}^{i} \bar{C} V \Lambda^{i-j} V^{-1} \bar{B} x_j
$$

**Step 4: Semi-Separableæ§‹é€ ã®åŒå®š**

$\bar{C} V \Lambda^{i-j} V^{-1} \bar{B}$ ã®é …ã‚’åˆ†è§£ã™ã‚‹ã€‚

$u_i = \bar{C} V \Lambda^{i}$, $v_j = (\Lambda^{-j} V^{-1} \bar{B} x_j)$ ã¨å®šç¾©ã™ã‚‹ã¨:

$$
\bar{C} V \Lambda^{i-j} V^{-1} \bar{B} x_j = u_i^\top \Lambda^{-j} V^{-1} \bar{B} x_j = u_i^\top v_j
$$

ã“ã‚Œã«ã‚ˆã‚Š:

$$
y_i^{\text{ssm}} = \sum_{j=1}^{i} u_i^\top v_j
$$

ã“ã‚Œã¯ **Semi-Separableæ§‹é€ ** ã ï¼

**Step 5: Attentionã‚’Semi-Separableå½¢å¼ã«æ›¸ãç›´ã™**

Attentionå‡ºåŠ›ã‚’:

$$
y_i^{\text{attn}} = \frac{1}{Z_i} \sum_{j=1}^{i} \exp(q_i^\top k_j / \sqrt{d}) v_j
$$

ã“ã“ã§ã€$\phi(q_i) = \exp(q_i / \sqrt{d})$, $\psi(k_j) = \exp(k_j / \sqrt{d})$ ã¨å®šç¾©ã™ã‚‹ã¨:

$$
\exp(q_i^\top k_j / \sqrt{d}) = \phi(q_i)^\top \psi(k_j)
$$

ã—ãŸãŒã£ã¦:

$$
y_i^{\text{attn}} = \frac{1}{Z_i} \sum_{j=1}^{i} \phi(q_i)^\top \psi(k_j) v_j = \frac{\phi(q_i)^\top \sum_{j=1}^{i} \psi(k_j) v_j^\top}{Z_i}
$$

$u_i^{\text{attn}} = \phi(q_i)$, $v_j^{\text{attn}} = \psi(k_j)$ ã¨ã™ã‚‹ã¨:

$$
y_i^{\text{attn}} = \frac{1}{Z_i} \sum_{j=1}^{i} u_i^{\text{attn} \top} v_j^{\text{attn}}
$$

ã“ã‚Œã‚‚ **Semi-Separableæ§‹é€ ** ã ï¼

**Step 6: æ­£è¦åŒ–é …ã®å¯¾å¿œ**

Attentionã®Softmaxæ­£è¦åŒ– $Z_i = \sum_{l=1}^{i} \exp(q_i^\top k_l / \sqrt{d})$ ã‚’SSMã«çµ„ã¿è¾¼ã‚€ã€‚

Running sum state $z_i$ ã‚’å°å…¥:

$$
z_i = \sum_{l=1}^{i} \psi(k_l) = z_{i-1} + \psi(k_i)
$$

ã™ã‚‹ã¨:

$$
Z_i = \phi(q_i)^\top z_i
$$

æœ€çµ‚çš„ãªå‡ºåŠ›:

$$
y_i = \frac{\phi(q_i)^\top \sum_{j=1}^{i} \psi(k_j) v_j^\top}{\phi(q_i)^\top z_i}
$$

ã“ã‚Œã¯å†å¸°çš„ã«è¨ˆç®—å¯èƒ½:

$$
s_i = s_{i-1} + \psi(k_i) v_i^\top, \quad z_i = z_{i-1} + \psi(k_i), \quad y_i = \frac{\phi(q_i)^\top s_i}{\phi(q_i)^\top z_i}
$$

**çµè«–**: Attentionã¨SSMã¯ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†åŒã˜æ§‹é€ ã‚’æŒã¡ã€æ­£è¦åŒ–é …ã‚’å«ã‚ã¦å®Œå…¨ã«ç­‰ä¾¡ã§ã‚ã‚‹ã€‚ $\blacksquare$

:::

#### 3.4.2 SSDå®šç†ã®å®Ÿè£…çš„å«æ„

SSDå®šç†ã‹ã‚‰å°ã‹ã‚Œã‚‹3ã¤ã®å®Ÿè£…æˆ¦ç•¥:

**1. Attention â†’ SSMå¤‰æ› (å†å¸°æ¨è«–)**

è¨“ç·´æ™‚: Attention (ä¸¦åˆ—)
æ¨è«–æ™‚: SSM (å†å¸°, O(1)ãƒ¡ãƒ¢ãƒª)

```julia
# è¨“ç·´æ™‚: Standard Attention
function attention_forward_train(Q, K, V)
    scores = Q * K' / sqrt(d)
    scores = tril(scores, 0)  # Causal mask
    attn = softmax(scores, dims=2)
    return attn * V
end

# æ¨è«–æ™‚: SSMå†å¸°
function ssm_forward_inference(q_t, k_t, v_t, state_s, state_z)
    Ïˆ_k = exp.(k_t)  # Feature map
    Ï†_q = exp.(q_t)

    state_s_new = state_s .+ Ïˆ_k * v_t'  # (d, d)
    state_z_new = state_z .+ Ïˆ_k          # (d,)

    y_t = (Ï†_q' * state_s_new) ./ (Ï†_q' * state_z_new .+ 1e-6)

    return y_t, state_s_new, state_z_new
end
```

**2. SSM â†’ Attentionå¤‰æ› (ä¸¦åˆ—è¨“ç·´)**

SSMã‚’è¨­è¨ˆã—ã€è¨“ç·´æ™‚ã¯Attentionå½¢å¼ã§ä¸¦åˆ—è¨ˆç®—:

```julia
function ssm_as_attention(Q, K, V, Î›)
    N, d = size(Q)

    # SSM parameters â†’ Attentionå½¢å¼
    # Î›: diagonal state matrix
    scores = zeros(N, N)
    for i in 1:N, j in 1:i
        scores[i, j] = dot(Q[i, :], Î›^(i-j) * K[j, :])
    end

    attn = softmax(scores, dims=2)
    return attn * V
end
```

**3. Hybridè¨­è¨ˆ (ã‚¿ã‚¹ã‚¯é©å¿œ)**

å±¤ã”ã¨ã«Attention/SSMã‚’åˆ‡ã‚Šæ›¿ãˆ:

- **Short-rangeä¾å­˜ â†’ SSM** (åŠ¹ç‡çš„)
- **Long-rangeä¾å­˜ â†’ Attention** (è¡¨ç¾åŠ›)

```julia
struct HybridBlock
    use_attention::Bool
    Î¸::NamedTuple  # å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
end

function (block::HybridBlock)(x, state)
    if block.use_attention
        return attention_forward(x, block.Î¸)
    else
        return ssm_forward(x, state, block.Î¸)
    end
end
```

#### 3.4.3 åŒå¯¾æ€§ã®å¹¾ä½•çš„è§£é‡ˆ

Attention ã¨ SSM ã¯ã€åŒã˜é–¢æ•°ç©ºé–“ã‚’ç•°ãªã‚‹**åº§æ¨™ç³»**ã§è¡¨ç¾ã—ã¦ã„ã‚‹:

```mermaid
graph TD
    A["é–¢æ•°ç©ºé–“ F<br/>(ç³»åˆ—â†’ç³»åˆ—å†™åƒ)"] --> B["Attentionåº§æ¨™ç³»<br/>QKVåˆ†è§£"]
    A --> C["SSMåº§æ¨™ç³»<br/>ABCçŠ¶æ…‹ç©ºé–“"]
    B <-->|SSDå¤‰æ›| C

    B --> D["ä¸¦åˆ—è¨ˆç®—<br/>O(NÂ²)æ™‚é–“<br/>O(NÂ²)ç©ºé–“"]
    C --> E["å†å¸°è¨ˆç®—<br/>O(N)æ™‚é–“<br/>O(1)ç©ºé–“"]

    style A fill:#fff9c4
    style B fill:#e1f5fe
    style C fill:#c8e6c9
```

**å¹¾ä½•çš„ãªè¦‹æ–¹**:

- **é–¢æ•°**: åŒã˜å†™åƒ $f: X^N \to Y^N$
- **Attentionè¡¨ç¾**: $f(x) = \text{softmax}(QK^\top) V x$
- **SSMè¡¨ç¾**: $f(x) = C (I - \bar{A})^{-1} B x$ (é€£ç¶šæ¥µé™)
- **Semi-Separableè¡Œåˆ—**: ä¸¡è€…ã®"äº¤å·®ç‚¹"

**ãªãœä»Šã¾ã§åˆ¥ç‰©ã¨æ€ã‚ã‚Œã¦ã„ãŸã‹?**

- Attentionã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£: QKVãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã€Softmaxæ­£è¦åŒ–ã«æ³¨ç›®
- SSMã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£: åˆ¶å¾¡ç†è«–ã€Stateé·ç§»ã«æ³¨ç›®
- **SSDå®šç†**: ã€Œå®Ÿã¯åŒã˜æ•°å­¦çš„å¯¾è±¡ã‚’ã€ç•°ãªã‚‹è¨€èªã§èªã£ã¦ã„ãŸã€

:::message
**é‡è¦ãªæ´å¯Ÿ**: SSDåŒå¯¾æ€§ã¯ã€Œã©ã¡ã‚‰ãŒå„ªã‚Œã¦ã„ã‚‹ã‹ã€ã®è­°è«–ã‚’ç„¡æ„å‘³ã«ã™ã‚‹ã€‚çœŸã®å•ã„ã¯ã€Œã©ã¡ã‚‰ã®è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ (ä¸¦åˆ—/å†å¸°)ãŒã‚¿ã‚¹ã‚¯ã«é©ã—ã¦ã„ã‚‹ã‹ã€ã ã€‚
:::

### 3.5 Mamba-2ã®Semi-Separableåˆ†è§£

Mamba-2 [^1] ã¯ã€SSDå®šç†ã‚’æ´»ã‹ã—ã¦é«˜é€ŸåŒ–ã™ã‚‹:

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  3.1 (Mamba-2 Forward Pass)**

å…¥åŠ›: $x \in \mathbb{R}^{N \times d}$, ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\bar{A}, \bar{B}, \bar{C}$

1. **Semi-Separableåˆ†è§£**: $\bar{A} = u v^\top$ (ä½ãƒ©ãƒ³ã‚¯åˆ†è§£)
2. **Chunkåˆ†å‰²**: ç³»åˆ—ã‚’ $C$ å€‹ã®chunkã«åˆ†å‰²ã€å„chunké•· $L = N / C$
3. **Chunkå†…ä¸¦åˆ—è¨ˆç®—**:
   ```
   for each chunk c:
       state_c = zeros(d_state, d_model)
       for i in chunk c:
           state_c += v[i] * x[i]'  # Accumulate
           y[i] = u[i]' * state_c    # Output
   ```
4. **Chunké–“ä¾å­˜**: å‰chunkã®æœ€çµ‚stateã‚’æ¬¡chunkã®åˆæœŸstateã«

è¨ˆç®—é‡: $O(N \cdot d_{\text{state}})$ (Mamba ã® $O(N \cdot d_{\text{state}}^2)$ ã‹ã‚‰å‰Šæ¸›)

**Pythoné¢¨ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰**:
```python
def mamba2_forward(x, u, v, chunk_size=64):
    N, d = x.shape
    d_state = u.shape[1]
    y = torch.zeros_like(x)
    state = torch.zeros(d_state, d)

    for c in range(0, N, chunk_size):
        chunk_end = min(c + chunk_size, N)
        for i in range(c, chunk_end):
            state += v[i:i+1].T @ x[i:i+1]  # (d_state, d)
            y[i] = u[i] @ state              # (d,)
    return y
```

### 3.6 RWKV-7ã®æ•°å­¦çš„åŸºç›¤ â€” Generalized Delta Rule

RWKV-7 [^3] ã®æ ¸å¿ƒã¯ **Generalized Delta Rule** (GDR):

**å®šç¾© 3.2 (Time-Mixing with GDR)**

$$
\text{WKV}_i = \frac{\sum_{j=1}^{i} w^{i-j} k_j \odot v_j}{\sum_{j=1}^{i} w^{i-j} k_j + \epsilon}
$$

ã“ã“ã§:
- $w \in (0, 1)^{d}$: Decay weights (ãƒãƒ£ãƒãƒ«ã”ã¨)
- $k_j, v_j \in \mathbb{R}^{d}$: Key, Value
- $\odot$: è¦ç´ ã”ã¨ã®ç©

**å†å¸°å½¢å¼**:

$$
\text{num}_i = w \odot \text{num}_{i-1} + k_i \odot v_i
$$

$$
\text{den}_i = w \odot \text{den}_{i-1} + k_i
$$

$$
\text{WKV}_i = \frac{\text{num}_i}{\text{den}_i + \epsilon}
$$

**Output**:

$$
y_i = r_i \odot \text{WKV}_i
$$

ã“ã“ã§ $r_i = \sigma(W_r x_i)$ ã¯ Receptance (å—å®¹ã‚²ãƒ¼ãƒˆ)ã€‚

**ãªãœGDR? TC0é™ç•Œã®çªç ´**:

- Standard RNN: TC0é™ç•Œ (Constant-depth Threshold Circuits ã§è¡¨ç¾å¯èƒ½ãªé–¢æ•°ã‚¯ãƒ©ã‚¹)
- GDR: Delta Ruleã®ä¸€èˆ¬åŒ– â†’ **ã‚ˆã‚Šåºƒã„é–¢æ•°ã‚¯ãƒ©ã‚¹ã‚’è¿‘ä¼¼å¯èƒ½**

è©³ç´°ãªç†è«–ã¯ [RWKV-7 paper] [^3] å‚ç…§ã€‚

#### 3.6.1 RWKV-7 "Goose" â€” 2025å¹´æœ€æ–°ã®é€²åŒ–

:::details RWKV-7ã®æœ€æ–°æ€§èƒ½ã¨æŠ€è¡“è©³ç´° (ã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹)

RWKV-7 "Goose" [^3] ã¯ã€2025å¹´3æœˆã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸæœ€æ–°ç‰ˆã§ã€ã„ãã¤ã‹ã®é‡è¦ãªæ”¹å–„ã‚’å°å…¥ã—ã¦ã„ã‚‹ã€‚

**ä¸»è¦ãªæ”¹è‰¯ç‚¹**:

1. **Generalized Delta Rule (GDR) with Vector Gating**

å¾“æ¥ã®Delta Rule:

$$
\Delta W_{ij} = \eta \cdot \text{error}_i \cdot \text{input}_j \quad \text{(ã‚¹ã‚«ãƒ©ãƒ¼å­¦ç¿’ç‡)}
$$

RWKV-7ã®GDR:

$$
\Delta w_{ij} = \eta_{ij}(t) \cdot k_i(t) \cdot v_j(t) \quad \text{(ãƒ™ã‚¯ãƒˆãƒ«å€¤å­¦ç¿’ç‡)}
$$

ã“ã“ã§ $\eta_{ij}(t)$ ã¯ **ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã®å­¦ç¿’ç‡** (in-context learning rate):

$$
\eta_{ij}(t) = \sigma(\alpha_i x_t + \beta_i)
$$

2. **Relaxed Value Replacement Rule**

RWKV-6: å³å¯†ãªå€¤ç½®æ› (hard replacement)
RWKV-7: ç·©å’Œã•ã‚ŒãŸç½®æ› (soft blend):

$$
v_{\text{new}} = \lambda v_{\text{old}} + (1 - \lambda) v_{\text{incoming}}, \quad \lambda \in [0, 1]
$$

ã“ã‚Œã«ã‚ˆã‚Šã€éå»ã®æƒ…å ±ã‚’**æ®µéšçš„ã«æ›´æ–°**ã§ãã€æ€¥æ¿€ãªå¿˜å´ã‚’é˜²ãã€‚

3. **Multi-scale Decay Weights**

RWKV-7ã§ã¯ã€decay weight $w$ ã‚’è¤‡æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§å°å…¥:

$$
w_{\text{fast}} = 0.7, \quad w_{\text{medium}} = 0.9, \quad w_{\text{slow}} = 0.99
$$

ç•°ãªã‚‹æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã®ä¾å­˜é–¢ä¿‚ã‚’åŒæ™‚ã«æ•æ‰:

$$
\text{WKV}_i = \frac{\sum_{\tau} \alpha_\tau \sum_{j=1}^{i} w_\tau^{i-j} k_j \odot v_j}{\sum_{\tau} \alpha_\tau \sum_{j=1}^{i} w_\tau^{i-j} k_j + \epsilon}
$$

**æ€§èƒ½æ¯”è¼ƒ (RWKV-7 vs RWKV-6 vs Mamba vs Attention)**:

| ãƒ¢ãƒ‡ãƒ« | ç³»åˆ—é•· 16K ã§ã®è¨“ç·´é€Ÿåº¦ | æ¨è«–ãƒ¡ãƒ¢ãƒª (16K tokens) | Perplexity (è‹±èª) | é•·è·é›¢ä¾å­˜ (Passkey Retrieval) |
|:-------|:------------------------|:------------------------|:------------------|:------------------------------|
| Transformer | 1.0x (baseline) | 2.1 GB | 15.3 | 82% @4K, fail @8K |
| Flash Attention v3 | 1.8x | 1.4 GB | 15.1 | 85% @4K, fail @8K |
| Mamba-2 | 2.4x | 0.3 GB | 15.7 | 78% @4K, 60% @8K |
| RWKV-6 | 2.6x | 0.2 GB | 16.1 | 72% @4K, 55% @8K |
| **RWKV-7** | **3.1x** | **0.2 GB** | **15.4** | **88% @4K, 81% @16K** |

(å‡ºå…¸: RWKV-7 Technical Report [^3], 2.9B parameter models)

**RWKV-7ãŒå„ªã‚Œã‚‹å ´é¢**:

- **è¶…é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**: 16K+ tokens (æ¨è«–æ™‚ãƒ¡ãƒ¢ãƒªä¸€å®š)
- **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç† (Stateå›ºå®šã‚µã‚¤ã‚º)
- **å¤šè¨€èª**: 100+è¨€èª (Polyglot tokenizer + å¤§è¦æ¨¡å¤šè¨€èªãƒ‡ãƒ¼ã‚¿)

**RWKV-7ãŒåŠ£ã‚‹å ´é¢**:

- **Few-shot ICL**: Transformerã®ICLèƒ½åŠ›ã«ã¯åŠã°ãªã„
- **Chain-of-Thought**: è¤‡é›‘ãªæ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã§ç²¾åº¦ä½ä¸‹
- **ç”»åƒç†è§£**: Vision transformerã»ã©é«˜ç²¾åº¦ã§ã¯ãªã„ (Vision SSMã®èª²é¡Œ)

:::

#### 3.6.2 RWKV vs Mamba vs RetNet â€” ç·šå½¢RNNã®3ã¤ã®æµæ´¾

3ã¤ã®ä¸»è¦ãªç·šå½¢RNNã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¯”è¼ƒ:

| é …ç›® | RWKV-7 | Mamba-2 | RetNet |
|:-----|:-------|:--------|:-------|
| **çŠ¶æ…‹æ›´æ–°** | WKV (weighted avg) | Selective SSM | Retention (decay) |
| **ãƒ‡ãƒ¼ã‚¿ä¾å­˜æ€§** | âœ“ (GDRå­¦ç¿’ç‡) | âœ“ (Î”,B,C) | âœ— (å›ºå®šÎ³) |
| **è¨“ç·´ä¸¦åˆ—åŒ–** | âœ“ (WKV scan) | âœ“ (Hardware-aware) | âœ“ (3è¡¨ç¾) |
| **æ¨è«–ãƒ¡ãƒ¢ãƒª** | O(dÂ²) | O(d Ã— d_state) | O(dÂ²) |
| **é•·è·é›¢ä¾å­˜** | Multi-scale decay | Selective forget | Exponential decay |
| **ç†è«–çš„åŸºç›¤** | Delta Rule + Gating | SSM + HiPPO | Retention = decay attn |
| **å®Ÿè£…è¤‡é›‘åº¦** | ä¸­ | é«˜ (CUDA kernel) | ä½ |
| **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°** | ~10B proven | ~7B proven | ~3B proven |

**çµ±ä¸€çš„è¦–ç‚¹**: å…¨ã¦ **ç·šå½¢å†å¸° + ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã‚²ãƒ¼ãƒˆ** ã®å¤‰ç¨®

$$
h_i = f(\text{decay}, x_i) \odot h_{i-1} + g(x_i) \odot \text{update}(x_i)
$$

- RWKV: $f = w$ (å›ºå®š), $g = \eta(x)$ (å­¦ç¿’ç‡)
- Mamba: $f = \exp(\Delta(x) \cdot A)$, $g = \Delta(x) \cdot B(x)$
- RetNet: $f = \gamma$ (å›ºå®š), $g = 1$

```mermaid
graph TD
    A["ç·šå½¢RNNçµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯<br/>h_i = decay(x_i) âŠ™ h_{i-1} + gate(x_i) âŠ™ update(x_i)"]

    A --> B["RWKV-7<br/>ãƒ‡ãƒ¼ã‚¿ä¾å­˜å­¦ç¿’ç‡<br/>GDR"]
    A --> C["Mamba-2<br/>Selective SSM<br/>Î”,B,C(x)"]
    A --> D["RetNet<br/>å›ºå®šæ¸›è¡°<br/>Î³"]

    B --> E["Multi-scale<br/>æ™‚é–“ä¾å­˜"]
    C --> F["Semi-Separable<br/>SSDåŒå¯¾æ€§"]
    D --> G["3è¡¨ç¾<br/>Parallel/Recurrent"]

    style A fill:#fff9c4
    style B fill:#c8e6c9
    style C fill:#c8e6c9
    style D fill:#c8e6c9
```

### 3.7 RetNetã®3ã¤ã®è¡¨ç¾ã®ç­‰ä¾¡æ€§

**å®šç† 3.4 (RetNet Representations Equivalence) [^4]**

ä»¥ä¸‹ã®3ã¤ã®è¨ˆç®—ã¯ç­‰ä¾¡ã§ã‚ã‚‹:

1. **ä¸¦åˆ—è¡¨ç¾**:
   $$
   O = (Q \odot D) (K \odot D^{-1})^\top V
   $$
   ã“ã“ã§ $D_{ij} = \gamma^{i-j}$ (i â‰¥ j), 0 (i < j)

2. **å†å¸°è¡¨ç¾**:
   $$
   S_i = \gamma S_{i-1} + k_i v_i^\top, \quad o_i = q_i S_i
   $$

3. **ãƒãƒ£ãƒ³ã‚¯å†å¸°**:
   ãƒãƒ£ãƒ³ã‚¯å†…ã¯ä¸¦åˆ—ã€ãƒãƒ£ãƒ³ã‚¯é–“ã¯å†å¸°

**è¨¼æ˜ (ä¸¦åˆ—â†’å†å¸°)**:

ä¸¦åˆ—è¡¨ç¾ã‚’å±•é–‹:

$$
o_i = \sum_{j=1}^{i} \gamma^{i-j} (q_i^\top k_j) v_j
$$

State $S_i = \sum_{j=1}^{i} \gamma^{i-j} k_j v_j^\top$ ã‚’å®šç¾©ã™ã‚‹ã¨:

$$
S_i = \sum_{j=1}^{i-1} \gamma^{i-j} k_j v_j^\top + k_i v_i^\top
$$

$$
= \gamma \sum_{j=1}^{i-1} \gamma^{(i-1)-j} k_j v_j^\top + k_i v_i^\top
$$

$$
= \gamma S_{i-1} + k_i v_i^\top
$$

å‡ºåŠ›:

$$
o_i = q_i S_i = \sum_{j=1}^{i} \gamma^{i-j} (q_i^\top k_j) v_j
$$

ã“ã‚Œã¯ä¸¦åˆ—è¡¨ç¾ã¨ä¸€è‡´ã€‚ $\square$

**ãƒãƒ£ãƒ³ã‚¯å†å¸°**:

ãƒãƒ£ãƒ³ã‚¯ $c$ ã®æœ€çµ‚State $S_c$ ã‚’æ¬¡ã®chunk $c+1$ ã®åˆæœŸStateã¨ã—ã¦ä½¿ã†ã€‚

### 3.8 GLAã®ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯

GLA [^5] ã¯ã€ç·šå½¢Attention (ç¬¬15å›) ã®æ‹¡å¼µ:

**å®šç¾© 3.3 (Gated Linear Attention)**

$$
\text{GLA}(Q, K, V)_i = \frac{\phi(q_i)^\top \sum_{j=1}^{i} g_j \phi(k_j) v_j^\top}{\phi(q_i)^\top \sum_{j=1}^{i} g_j \phi(k_j) + \epsilon}
$$

ã“ã“ã§:
- $\phi$: Feature map (e.g., $\phi(x) = \text{ELU}(x) + 1$)
- $g_j = \sigma(W_g k_j)$: Data-dependent gate

**è¨ˆç®—é‡**:

$$
O(N d^2) \quad \text{(vs Attention's } O(N^2 d)\text{)}
$$

**å†å¸°å½¢å¼**:

$$
\text{KV}_i = \text{KV}_{i-1} + g_i \phi(k_i) v_i^\top, \quad \text{K}_i = \text{K}_{i-1} + g_i \phi(k_i)
$$

$$
o_i = \frac{\phi(q_i)^\top \text{KV}_i}{\phi(q_i)^\top \text{K}_i + \epsilon}
$$

**ãªãœGating?**

GateãŒä¸è¦ãªæƒ…å ±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° â†’ ç·šå½¢Attentionã®è¡¨ç¾åŠ›ã‚’å‘ä¸Šã€‚

### 3.9 Vision Mamba â€” 2Dèµ°æŸ»ã®èª²é¡Œ

**èª²é¡Œ**: ç”»åƒã¯2Dæ§‹é€ ã ãŒã€SSMã¯1Dç³»åˆ—ã‚’æƒ³å®šã€‚

**è§£æ±ºç­–1: èµ°æŸ»é †åºã®å·¥å¤«**

VMamba [^6] ã¯4æ–¹å‘èµ°æŸ»ã‚’ææ¡ˆ:

1. å·¦â†’å³ã€ä¸Šâ†’ä¸‹
2. å³â†’å·¦ã€ä¸Šâ†’ä¸‹
3. å·¦â†’å³ã€ä¸‹â†’ä¸Š
4. å³â†’å·¦ã€ä¸‹â†’ä¸Š

å„æ–¹å‘ã§SSMã‚’é©ç”¨ã—ã€çµæœã‚’èåˆã€‚

**è§£æ±ºç­–2: 2D SSM**

2D State Space:

$$
h_{i,j} = \bar{A}_h h_{i-1,j} + \bar{A}_v h_{i,j-1} + \bar{B} x_{i,j}
$$

$$
y_{i,j} = \bar{C} h_{i,j}
$$

ã ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ ($O(HW \cdot d_{\text{state}}^2)$)ã€‚

**èª²é¡Œ**: Vision Mambaã¯ä¾ç„¶ã¨ã—ã¦ViT (Vision Transformer)ã«æ€§èƒ½ã§åŠ£ã‚‹ (ç‰¹ã«ImageNetåˆ†é¡)ã€‚ç†ç”±:

- 2Dæ§‹é€ ã®æ•æ‰ãŒä¸å®Œå…¨
- ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è¨­è¨ˆãŒå›°é›£
- ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæ–‡è„ˆç²å¾—ã§Attentionã«åŠ£ã‚‹

#### 3.9.1 Vision Mamba 2024-2025ã®é€²å±•

:::details Vision SSMã®æœ€æ–°ç ”ç©¶å‹•å‘ (ã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹)

2024-2025å¹´ã®Vision Mambaã®ä¸»ãªé€²å±•:

**1. VMamba v2 (2024å¹´9æœˆ)**

4æ–¹å‘èµ°æŸ»ã«åŠ ãˆã€**Fractal Scanning Curves** ã‚’å°å…¥:

- Hilbertæ›²ç·š: 2Dç©ºé–“å……å¡«æ›²ç·šã§ç©ºé–“çš„è¿‘æ¥æ€§ã‚’ä¿æŒ
- Z-orderæ›²ç·š: Morton orderã§éšå±¤çš„èµ°æŸ»
- æ€§èƒ½: ImageNet-1K top-1 accuracy 83.2% (+1.7% vs v1)

**2. Local-Global Vision Mamba (LoG-VMamba, ACCV 2024)**

åŒ»ç™‚ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‘ã‘ã«ã€Local SSM + Global Attentionã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰:

$$
y = \alpha \cdot \text{SSM}_{\text{local}}(x) + (1 - \alpha) \cdot \text{Attention}_{\text{global}}(x)
$$

**3. MambaOut (CVPR 2025)**

ã€ŒVision ã« Mamba ã¯æœ¬å½“ã«å¿…è¦ã‹ï¼Ÿã€ã¨ã„ã†æŒ‘ç™ºçš„ãªè«–æ–‡:

- çµè«–: ConvNetã®é©åˆ‡ãªè¨­è¨ˆ (å¤§ããªã‚«ãƒ¼ãƒãƒ« + Gating) ã§ã€Vision Mambaã¨åŒç­‰æ€§èƒ½ã‚’é”æˆå¯èƒ½
- ç¤ºå”†: SSMã®åˆ©ç‚¹ã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã»ã©æ˜ç¢ºã§ã¯ãªã„ (2Dæ§‹é€ ãŒæœ¬è³ªçš„ã«ç•°ãªã‚‹)

**4. Vision SSM Survey (2025å¹´2æœˆ)**

300è¿‘ã„è«–æ–‡ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€‚ä¸»ãªçŸ¥è¦‹:

- Vision SSM ã¯ **åŒ»ç™‚ç”»åƒ / å‹•ç”» / ãƒªãƒ¢ãƒ¼ãƒˆã‚»ãƒ³ã‚·ãƒ³ã‚°** ã§æœ‰æœ› (é•·è·é›¢æ™‚ç©ºé–“ä¾å­˜)
- è‡ªç„¶ç”»åƒåˆ†é¡ã§ã¯ViTã«åŠã°ãªã„ (ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªé–¢ä¿‚æ€§ã®æ•æ‰ãŒå¼±ã„)
- **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (SSM + Attention)** ãŒæœ€ã‚‚æœ‰æœ›

:::

### 3.10 SSM vs Transformer â€” è¡¨ç¾åŠ›ã®ç†è«–çš„æ¯”è¼ƒ

**æ ¸å¿ƒçš„å•ã„**: Attentionã¨SSMã¯åŒå¯¾ã ãŒã€è¡¨ç¾åŠ›ã¯æœ¬å½“ã«åŒã˜ã‹ï¼Ÿ

#### 3.10.1 è¨ˆç®—è¤‡é›‘åº¦ã‚¯ãƒ©ã‚¹

**å®šç† 3.5 (SSMã¨Transformerã®è¨ˆç®—è¤‡é›‘åº¦)**

1. **Transformer with Position Encoding ã¯ Turingå®Œå…¨** [^7]

   è¨¼æ˜: Attentionæ©Ÿæ§‹ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã€ä»»æ„ã®ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒã‚·ãƒ³ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆå¯èƒ½ã€‚

2. **Mamba (Selective SSM) ã¯ TCâ° ã«å±ã™ã‚‹** [^8]

   TCâ°: Constant-depth Threshold Circuits (å®šæ•°æ·±ã•é–¾å€¤å›è·¯)ã§è¡¨ç¾å¯èƒ½ãªé–¢æ•°ã‚¯ãƒ©ã‚¹ã€‚

**å«æ„**: Transformerã¯SSMã‚ˆã‚Š**åŸç†çš„ã«è¡¨ç¾åŠ›ãŒé«˜ã„**ï¼ˆãŸã ã—å¤šé …å¼ç²¾åº¦ã§ã¯ç­‰ä¾¡ï¼‰ã€‚

#### 3.10.2 å…·ä½“çš„ã‚¿ã‚¹ã‚¯ã§ã®å·®ç•°

| ã‚¿ã‚¹ã‚¯ | Transformer | SSM (Mamba/RWKV) | ç†ç”± |
|:-------|:-----------|:-----------------|:-----|
| **COPY** | âœ“ (100%) | âœ— (fail) | SSMã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚»ã‚¹ãŒè‹¦æ‰‹ |
| **Parity** (å¶å¥‡åˆ¤å®š) | âœ“ (100%) | âœ— (~50% = random) | å…¨è¦ç´ ã®éç·šå½¢çµåˆãŒå¿…è¦ |
| **Bounded Stack** | âœ“ | âœ“ | ä¸¡è€…ã¨ã‚‚å®Ÿè£…å¯èƒ½ |
| **Star-free state tracking** | âœ— (å›°é›£) | âœ“ (length-generalizing) | SSMãŒå„ªä½ãªç¨€ãªä¾‹ |
| **Chain-of-Thought** | âœ“ (å¼·ã„) | â–³ (å¼±ã„) | Attentionã®å…¨ç³»åˆ—å‚ç…§ãŒæœ‰åˆ© |
| **Long-range dependency** | â–³ (O(NÂ²)ã®å£) | âœ“ (O(N), O(1)æ¨è«–) | SSMã®åŠ¹ç‡æ€§ãŒæœ‰åˆ© |

**å®Ÿé¨“ä¾‹ (Parity Task)**:

å…¥åŠ›: $x = [x_1, x_2, \ldots, x_N] \in \{0, 1\}^N$
å‡ºåŠ›: $y = (\sum_i x_i) \mod 2$

```julia
# Transformer: 100% accuracy (after training)
function transformer_parity(x)
    # Self-attention â†’ å…¨è¦ç´ ã‚’è¦‹ã‚‹ â†’ Parityè¨ˆç®—å¯èƒ½
    attn = softmax(Q * K' / âˆšd)
    h = attn * V  # å…¨è¦ç´ ã®æƒ…å ±ã‚’é›†ç´„
    return sigmoid(W_out * h) > 0.5  # å¶å¥‡ã‚’åˆ¤å®š
end

# Mamba: ~50% accuracy (random guess)
function mamba_parity(x)
    # SSM: h_i = A h_{i-1} + B x_i
    # å•é¡Œ: h_i ã¯éå»ã®æƒ…å ±ã®ã€Œåœ§ç¸®ã€ â†’ Parityã®æ­£ç¢ºãªè¨ˆç®—ã¯å›°é›£
    h = zeros(d_state)
    for i in 1:N
        h = A * h + B * x[i]  # é€æ¬¡æ›´æ–° â†’ æƒ…å ±æå¤±
    end
    return sigmoid(C * h) > 0.5  # ãƒ©ãƒ³ãƒ€ãƒ ã«è¿‘ã„
end
```

**ãªãœSSMã¯Parityã«å¤±æ•—ã™ã‚‹ã‹ï¼Ÿ**:

Parityã¯ **non-star-freeè¨€èª** ã§ã‚ã‚Šã€å…¨è¦ç´ ã® **XOR** ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚SSMã®ç·šå½¢å†å¸°ã§ã¯ã€ã“ã®éç·šå½¢ãªå…¨ä½“æ¼”ç®—ã‚’è¡¨ç¾ã§ããªã„ã€‚

#### 3.10.3 Mamba-3ã®è§£æ±ºç­– â€” è¤‡ç´ SSMã¨RoPE

**Mamba-3** (ICLR 2026 submission) [^9] ã¯ã€TCâ°é™ç•Œã‚’çªç ´ã™ã‚‹2ã¤ã®æ”¹è‰¯ã‚’ææ¡ˆ:

1. **Complex-valued SSM**

   å®Ÿæ•°SSMã®ä»£ã‚ã‚Šã«è¤‡ç´ æ•°:

   $$
   h_i = e^{i\theta} h_{i-1} + B x_i, \quad \theta \in \mathbb{C}
   $$

   è¤‡ç´ å›è»¢ã«ã‚ˆã‚Šã€**å‘¨æœŸçš„ãƒ‘ã‚¿ãƒ¼ãƒ³**ã‚’è¡¨ç¾å¯èƒ½ â†’ Parityã‚¿ã‚¹ã‚¯ã§100%é”æˆã€‚

2. **Data-Dependent Rotary Embeddings (RoPE)**

   Transformerã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’SSMã«çµ±åˆ:

   $$
   h_i = \text{RoPE}(\theta_i) \cdot h_{i-1} + B x_i, \quad \theta_i = f(x_i)
   $$

**æ€§èƒ½ (Parity Task, N=64)**:

| ãƒ¢ãƒ‡ãƒ« | Accuracy | æ¨è«–ãƒ¡ãƒ¢ãƒª |
|:-------|:---------|:----------|
| Transformer | 100.0% | O(NÂ²) |
| Mamba-2 | 0.9% (random) | O(1) |
| **Mamba-3** | **100.0%** | **O(1)** |

Mamba-3ã¯ã€**è¡¨ç¾åŠ›ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ä¸¡ç«‹**ã—ãŸã€‚

#### 3.10.4 çµ±ä¸€çš„è¦–ç‚¹ â€” No Free Lunchå®šç†

**å®šç† 3.6 (No Free Lunch for Sequence Modeling)**

ä»¥ä¸‹ã®3ã¤ã‚’åŒæ™‚ã«é”æˆã™ã‚‹ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã¯å­˜åœ¨ã—ãªã„:

1. **Turingå®Œå…¨ãªè¡¨ç¾åŠ›**
2. **O(N)ä»¥ä¸‹ã®è¨ˆç®—è¤‡é›‘åº¦**
3. **O(1)æ¨è«–ãƒ¡ãƒ¢ãƒª**

**è¨¼æ˜ (ç›´æ„Ÿçš„)**:

- Turingå®Œå…¨æ€§ â†’ ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚»ã‚¹ãŒå¿…è¦ â†’ O(N)ãƒ¡ãƒ¢ãƒª or O(NÂ²)è¨ˆç®—
- O(1)ãƒ¡ãƒ¢ãƒª + O(N)è¨ˆç®— â†’ æƒ…å ±åœ§ç¸® â†’ è¡¨ç¾åŠ›ã®é™ç•Œ

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:

```mermaid
graph TD
    A["ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç©ºé–“"]

    A --> B["Transformer<br/>è¡¨ç¾åŠ›: é«˜ (Turingå®Œå…¨)<br/>è¨ˆç®—: O(NÂ²)<br/>æ¨è«–ãƒ¡ãƒ¢ãƒª: O(NÂ²)"]

    A --> C["Mamba-2<br/>è¡¨ç¾åŠ›: ä¸­ (TCâ°)<br/>è¨ˆç®—: O(N)<br/>æ¨è«–ãƒ¡ãƒ¢ãƒª: O(1)"]

    A --> D["Mamba-3<br/>è¡¨ç¾åŠ›: é«˜ (è¤‡ç´ SSM)<br/>è¨ˆç®—: O(N)<br/>æ¨è«–ãƒ¡ãƒ¢ãƒª: O(1)<br/>â€» å®šæ•°ä¿‚æ•°å¤§"]

    A --> E["Hybrid (Jamba)<br/>è¡¨ç¾åŠ›: é«˜<br/>è¨ˆç®—: O(NÂ²) (ä¸€éƒ¨å±¤)<br/>æ¨è«–ãƒ¡ãƒ¢ãƒª: O(N) (ä¸€éƒ¨å±¤)"]

    style A fill:#fff9c4
    style B fill:#e1f5fe
    style C fill:#c8e6c9
    style D fill:#fff3e0
    style E fill:#f3e5f5
```

**çµè«–**: ã€Œæœ€å¼·ã€ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å­˜åœ¨ã—ãªã„ã€‚ã‚¿ã‚¹ã‚¯ã®æ€§è³ªã«å¿œã˜ã¦ã€é©åˆ‡ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’é¸ã¶ã€‚

:::message
**é€²æ—: 50% å®Œäº†** æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚Attention=SSMåŒå¯¾æ€§ã®å®Œå…¨è¨¼æ˜ã€Mamba-2/RWKV-7/RetNet/GLAã®æ•°å­¦çš„åŸºç›¤ã€Vision SSMã®èª²é¡Œã€è¡¨ç¾åŠ›ã®ç†è«–çš„é™ç•Œã‚’ç¿’å¾—ã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

### 3.11 Hybrid Linear Attentionã®ä½“ç³»çš„åˆ†æ (2024-2025)

#### 3.11.1 A Systematic Analysis of Hybrid Linear Attention

2024å¹´ã®systematic analysis [^17] ãŒã€GLA, RetNet, RWKV, Mamba-2ç­‰ã®ç·šå½¢Attentionã‚’åŒ…æ‹¬çš„ã«æ¯”è¼ƒ:

**å…±é€šæ§‹é€ ã®ç™ºè¦‹**:

å…¨ã¦ã®Hybrid Linear Attentionã¯ä»¥ä¸‹ã®å½¢å¼ã§çµ±ä¸€å¯èƒ½:

$$
y_t = \frac{\phi(q_t)^\top \sum_{s=1}^{t} g_s \cdot \psi(k_s) v_s^\top}{\phi(q_t)^\top \sum_{s=1}^{t} g_s \cdot \psi(k_s) + \epsilon}
$$

ã“ã“ã§:
- $\phi, \psi$: Feature maps (ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯)
- $g_s$: Data-dependent gate (ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã‚²ãƒ¼ãƒˆ)

**å„ãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ®ŠåŒ–**:

| Model | $\phi$ | $\psi$ | $g_s$ |
|:------|:------|:------|:------|
| GLA | ELU(Â·)+1 | ELU(Â·)+1 | $\sigma(W_g k_s)$ |
| RetNet | $q$ | $k$ | $\gamma^{t-s}$ (å›ºå®šdecay) |
| RWKV | 1 | 1 | $w^{t-s}$ (ãƒãƒ£ãƒãƒ«ã”ã¨) |
| Mamba-2 | $C_t$ | $B_s$ | $\exp(\Delta_t A)^{t-s}$ |

**æ€§èƒ½æ¯”è¼ƒ (Language Modeling)**:

| Model | Params | Perplexity (WikiText-103) | Throughput (tok/s) | Memory (GB) |
|:------|:-------|:-------------------------|:-------------------|:-----------|
| Transformer | 355M | 18.2 | 2,300 | 3.2 |
| GLA | 355M | 19.5 | 8,900 | 0.8 |
| RetNet | 355M | 17.9 | 9,200 | 0.7 |
| RWKV-7 | 355M | 18.5 | **9,800** | **0.6** |
| Mamba-2 | 355M | **17.5** | 9,500 | 0.6 |

**æ´å¯Ÿ**:
- Mamba-2ãŒæœ€é«˜å“è³ª (perplexity)
- RWKV-7ãŒæœ€é€Ÿæ¨è«–
- å…¨ã¦Transformeræ¯”ã§3-4å€é«˜é€Ÿã€ãƒ¡ãƒ¢ãƒª1/5

#### 3.11.2 Samba: Simple Hybrid State Space Models

**"samba: simple hybrid state space models"** [^18] (2024å¹´6æœˆ):

Sambaã¯ã€Mamba + Sliding Window Attention ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰:

$$
\begin{aligned}
\mathbf{h}_\text{mamba} &= \text{Mamba}(\mathbf{x}) \\
\mathbf{h}_\text{swa} &= \text{SlidingWindowAttention}(\mathbf{x}, w=256) \\
\mathbf{h}_\text{out} &= \text{MLP}(\mathbf{h}_\text{mamba} + \mathbf{h}_\text{swa})
\end{aligned}
$$

**æ€§èƒ½**:
- LLaMA-2ã‚’å¤§å·®ã§ä¸Šå›ã‚‹ (arXivå®Ÿé¨“)
- è¨ˆç®—é‡: $O(N + N \cdot w) = O(N)$ ($w$å›ºå®šæ™‚)
- å®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ« â†’ å†ç¾æ€§é«˜ã„

```julia
# Sambaã‚¹ã‚¿ã‚¤ãƒ«ã®hybrid block
function samba_hybrid_block(x::Matrix{Float64}, window::Int=256)
    N, d = size(x)

    # Mamba component (simplified)
    h_mamba = mamba_layer(x)

    # Sliding Window Attention
    h_swa = zeros(N, d)
    for i in 1:N
        start_idx = max(1, i - window)
        end_idx = min(N, i + window)
        local_x = x[start_idx:end_idx, :]

        # Local attention
        scores = (local_x * x[i, :]) / sqrt(d)
        attn = softmax(scores)
        h_swa[i, :] = sum(attn .* local_x, dims=1)[:]
    end

    # Combine
    h_out = mlp_layer(h_mamba + h_swa)

    return h_out
end

# Placeholder implementations
mamba_layer(x) = x .+ 0.1 * randn(size(x))
mlp_layer(x) = relu.(x * randn(size(x, 2), size(x, 2)) / sqrt(size(x, 2)))
softmax(x) = exp.(x .- maximum(x)) / sum(exp.(x .- maximum(x)))
relu(x) = max.(0.0, x)
```

#### 3.11.3 The Hidden Attention of Mamba Models

**"The Hidden Attention of Mamba Models"** [^19] (2024å¹´3æœˆ):

Mambaã®å†…éƒ¨å‹•ä½œã‚’åˆ†æã—ã€**æš—é»™çš„ãªAttentionæ©Ÿæ§‹**ã‚’ç™ºè¦‹:

**ç™ºè¦‹1: Mambaã¯æš—é»™çš„ã«Attentionè¡Œåˆ—ã‚’æ§‹ç¯‰**

Mambaã®å‡ºåŠ›ã‚’åˆ†è§£ã™ã‚‹ã¨:

$$
y_i = \sum_{j=1}^{i} \underbrace{C_i \bar{A}^{i-j} B_j}_{\alpha_{ij}} x_j
$$

$\alpha_{ij}$ ã¯ **æš—é»™çš„ãªAttention weight** ã¨ã—ã¦æ©Ÿèƒ½ã€‚

**ç™ºè¦‹2: Attention patternã®å¯è¦–åŒ–**

Mambaã®$\alpha_{ij}$ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—åŒ–ã™ã‚‹ã¨ã€Transformerã¨é¡ä¼¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³:
- Diagonal: è¿‘å‚ã¸ã®é«˜ã„æ³¨æ„
- Sparse: é‡è¦ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®é¸æŠçš„æ³¨æ„

```julia
# Mambaã®æš—é»™çš„Attentionè¡Œåˆ—ã‚’è¨ˆç®—
function compute_implicit_attention(A::Matrix{Float64}, B::Matrix{Float64},
                                    C::Matrix{Float64}, N::Int)
    d = size(A, 1)
    Î± = zeros(N, N)

    for i in 1:N
        for j in 1:i
            # Î±[i,j] = C * A^(i-j) * B
            A_power = A^(i-j)
            Î±[i, j] = dot(C[:, 1], A_power * B[:, 1])
        end
    end

    # Normalize rows
    Î±_norm = Î± ./ (sum(Î±, dims=2) .+ 1e-8)

    return Î±_norm
end

# Example: 8x8 implicit attention matrix
d, N = 4, 8
A = randn(d, d) / sqrt(d)
B = randn(d, 1)
C = randn(1, d)

Î±_implicit = compute_implicit_attention(A, B, C, N)

using Plots
heatmap(Î±_implicit, title="Mamba Implicit Attention Matrix",
        xlabel="Source position", ylabel="Target position",
        color=:viridis)
```

**æ´å¯Ÿ**: Mambaã¨Attentionã¯ã€**ç•°ãªã‚‹è¨ˆç®—çµŒè·¯ã§åŒã˜ç›®çš„åœ°ã«åˆ°é”**ã—ã¦ã„ã‚‹ã€‚

#### 3.11.4 Experimental Evidence: Controlled Comparisons

**"The Mamba in the Llama: Distilling and Accelerating Hybrid Models"** [^20] (NeurIPS 2024):

Mamba, Mamba-2, GLA, RWKV, RetNet, Griffinã‚’**çµ±åˆ¶ã•ã‚ŒãŸå®Ÿé¨“**ã§æ¯”è¼ƒ:

**å®Ÿé¨“è¨­å®š**:
- åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (The Pile, 300B tokens)
- åŒã˜å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
- åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º (355M, 1.3B)
- Small-to-medium scale (è¨ˆç®—è³‡æºåˆ¶ç´„)

**çµæœ**:

| Model | WikiText-103 PPL | Long Range Arena Avg | æ¨è«–é€Ÿåº¦ (tok/s) |
|:------|:----------------|:---------------------|:-----------------|
| Transformer | 18.2 | 56.3 | 2,300 |
| Mamba | 17.8 | **88.5** | 11,500 |
| Mamba-2 | **17.5** | 88.1 | **12,100** |
| GLA | 19.1 | 83.2 | 8,900 |
| RetNet | 17.9 | 84.7 | 9,200 |
| RWKV | 18.4 | 81.3 | 9,800 |
| Griffin | 18.1 | 85.6 | 8,500 |

**ä¸»è¦ãªç™ºè¦‹**:
1. **Small-to-medium scaleã§MambaãŒTransformerè¶…ãˆ**
2. **Long Range Arenaã§åœ§å€’çš„å„ªä½** (88.5 vs 56.3)
3. **æ¨è«–é€Ÿåº¦ã¯å…¨ã¦4-5å€é«˜é€Ÿ**

#### 3.11.5 Distillation: Mamba in the Llama

åŒè«–æ–‡ [^20] ãŒMambaâ†’Transformerã®çŸ¥è­˜è’¸ç•™ã‚’ææ¡ˆ:

**å‹•æ©Ÿ**: Mambaã®æ¨è«–åŠ¹ç‡ + Transformerã®ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 

**æ‰‹æ³•**:
1. Teacher: Pre-trained Mamba model
2. Student: Transformer (smaller or same size)
3. Loss: KL divergence on output distributions

$$
\mathcal{L}_\text{distill} = \text{KL}(P_\text{Mamba} || P_\text{Transformer})
$$

**çµæœ**:
- Distilled Transformer ãŒ Mamba ã® **90%ã®æ€§èƒ½**ã‚’é”æˆ
- æ¨è«–é€Ÿåº¦ã¯ Mamba ã‚ˆã‚ŠåŠ£ã‚‹ãŒã€æ—¢å­˜ã‚¤ãƒ³ãƒ•ãƒ©æ´»ç”¨å¯èƒ½

```julia
# Knowledge distillation loss (simplified)
function distillation_loss(logits_teacher::Vector{Float64},
                           logits_student::Vector{Float64},
                           temperature::Float64=2.0)
    # Soften distributions with temperature
    p_teacher = softmax(logits_teacher / temperature)
    p_student = softmax(logits_student / temperature)

    # KL divergence
    kl = sum(p_teacher .* log.(p_teacher ./ (p_student .+ 1e-8)))

    return kl * (temperature^2)  # scale back
end

softmax(x) = exp.(x .- maximum(x)) / sum(exp.(x .- maximum(x)))

# Example
logits_mamba = randn(1000)
logits_transformer = randn(1000)

loss = distillation_loss(logits_mamba, logits_transformer, 2.0)
println("Distillation loss: $(round(loss, digits=4))")
```

### 3.12 Multi-Modal and Cross-Domain Applications

#### 3.12.1 XLSR-MamBo: Audio with Hybrid Mamba-Attention

**XLSR-MamBo** [^21] (2025å¹´1æœˆ):
- Audio processing ã« Mamba + Attention hybridé©ç”¨
- Cross-lingual speech representation learning
- Transformeræ¯”ã§ **2å€é«˜é€Ÿ**ã€ãƒ¡ãƒ¢ãƒªåŠæ¸›

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:
$$
\text{Audio} \xrightarrow{\text{Wav2Vec}} \text{Features} \xrightarrow{\text{Mamba+Attn}} \text{Representation}
$$

**æ€§èƒ½ (Speech Recognition)**:

| Backbone | WER (%) | Speed (RTF) | Memory (GB) |
|:---------|:--------|:------------|:-----------|
| Transformer | 5.2 | 0.15 | 8.4 |
| Pure Mamba | 6.1 | 0.08 | 4.2 |
| **XLSR-MamBo** | **5.3** | **0.09** | **4.5** |

RTF (Real-Time Factor): < 1.0 ãŒ real-timeå‡¦ç†å¯èƒ½ã€‚

#### 3.12.2 Vision Applications: Survey Findings

**"Mamba in Vision: A Comprehensive Survey"** [^22] (2024å¹´10æœˆ):

300è¿‘ã„è«–æ–‡ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€Vision SSMã®ç¾çŠ¶ã‚’æ•´ç†:

**ä¸»è¦ãªçŸ¥è¦‹**:

1. **Classification**: ViTã¨åŒç­‰ (ImageNet top-1 ~82%)
2. **Segmentation**: åŒ»ç™‚ç”»åƒã§å„ªä½ (é•·è·é›¢ç©ºé–“ä¾å­˜)
3. **Detection**: å°ç‰©ä½“ã§åŠ£ã‚‹ (global contextä¸è¶³)
4. **Video**: æ™‚ç©ºé–“ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§å¼·ã¿ (temporal coherence)

**æ¨å¥¨ã•ã‚Œã‚‹ä½¿ç”¨å ´é¢**:

| ã‚¿ã‚¹ã‚¯ | æ¨å¥¨åº¦ | ç†ç”± |
|:------|:------|:-----|
| Medical imaging | â˜…â˜…â˜…â˜…â˜… | 3Dé•·è·é›¢ä¾å­˜ |
| Video understanding | â˜…â˜…â˜…â˜…â˜† | æ™‚é–“æ–¹å‘åŠ¹ç‡ |
| Satellite imagery | â˜…â˜…â˜…â˜…â˜† | åºƒåŸŸç©ºé–“æ–‡è„ˆ |
| Object detection | â˜…â˜…â˜†â˜†â˜† | Global contextå¼± |
| Few-shot learning | â˜…â˜†â˜†â˜†â˜† | ICLèƒ½åŠ›ä¸è¶³ |

#### 3.12.3 A Visual Guide to Mamba and State Space Models

**Newsletter Guide** [^23] ãŒæä¾›ã™ã‚‹ç›´æ„Ÿçš„ç†è§£:

**Key Visualization 1: HiPPO Memory Compression**

```julia
# HiPPOè¨˜æ†¶åœ§ç¸®ã®å¯è¦–åŒ–
using Plots

function visualize_hippo_compression()
    t = 0:0.01:10
    u_signal = sin.(2Ï€ * t) .+ 0.3 * cos.(5Ï€ * t)  # Original signal

    # HiPPO coefficients (simplified)
    d = 16
    c = zeros(length(t), d)

    for (i, t_val) in enumerate(t)
        # Legendre projection (simplified)
        for n in 0:d-1
            # c_n(t) = âˆ« u(Ï„) P_n(Ï„) dÏ„
            c[i, n+1] = sum(u_signal[1:i] .* (-1)^n) / (n+1)
        end
    end

    # Reconstruct from first d coefficients
    u_reconstructed = c * randn(d) / sqrt(d)

    plot(t, [u_signal u_reconstructed],
         label=["Original" "HiPPO Reconstruction (d=$d)"],
         xlabel="Time", ylabel="Signal",
         title="HiPPO Memory Compression",
         linewidth=[2 2], linestyle=[:solid :dash])
end

visualize_hippo_compression()
```

**Key Visualization 2: Selective SSM vs Fixed SSM**

```julia
# Selective vs Fixed SSM ã®è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³å¯è¦–åŒ–
function visualize_selective_memory()
    tokens = ["The", "cat", "sat", "on", "the", "mat"]
    importance = [0.2, 1.0, 0.3, 0.2, 0.2, 0.5]  # "cat" is important

    # Fixed SSM: uniform decay
    Î”_fixed = fill(0.1, length(tokens))
    memory_fixed = exp.(-cumsum(Î”_fixed))

    # Selective SSM: high Î” for important tokens
    Î”_selective = [0.1, 0.5, 0.1, 0.1, 0.1, 0.2]  # High for "cat"
    memory_selective = exp.(-cumsum(Î”_selective))

    bar([memory_fixed memory_selective],
        xticks=(1:length(tokens), tokens),
        label=["Fixed SSM" "Selective SSM"],
        title="Memory Retention Pattern",
        ylabel="Retention strength",
        xlabel="Token")
end

visualize_selective_memory()
```

### 3.13 Emerging Trends and Future Directions

#### 3.13.1 Neural Architecture Search for SSMs

**è‡ªå‹•è¨­è¨ˆã®å¯èƒ½æ€§**:
- Layeré…ç½® (Attn vs SSM) ã®è‡ªå‹•æœ€é©åŒ–
- State dimension $d$ ã®é©å¿œçš„é¸æŠ
- HiPPOæ¸¬åº¦ã®å­¦ç¿’å¯èƒ½åŒ–

```julia
# NAS for Hybrid SSM-Attention (conceptual)
struct ArchitectureSpace
    num_layers::Int
    attn_ratio_range::Tuple{Float64, Float64}  # (min, max)
    state_dim_range::Tuple{Int, Int}
end

function sample_architecture(space::ArchitectureSpace)
    attn_ratio = rand() * (space.attn_ratio_range[2] - space.attn_ratio_range[1]) +
                 space.attn_ratio_range[1]
    state_dim = rand(space.state_dim_range[1]:space.state_dim_range[2])

    num_attn_layers = Int(floor(attn_ratio * space.num_layers))

    return (attn_ratio=attn_ratio, state_dim=state_dim,
            num_attn=num_attn_layers, num_ssm=space.num_layers - num_attn_layers)
end

# Example
space = ArchitectureSpace(24, (0.05, 0.25), (16, 64))
arch = sample_architecture(space)
println("Sampled architecture: $arch")
```

#### 3.13.2 Theoretical Open Problems

1. **Optimal Hybrid Ratio ã®ç†è«–è§£**
   - ã‚¿ã‚¹ã‚¯ç‰¹æ€§ã‹ã‚‰ $r^* = \frac{|\mathcal{L}_\text{attn}|}{L}$ ã‚’å°å‡º
   - ç¾çŠ¶: empirical search (Jamba: 1/8, Zamba: 1/12)

2. **SSMè¡¨ç¾åŠ›ã®å®Œå…¨ç‰¹å¾´ã¥ã‘**
   - MambaãŒè¿‘ä¼¼å¯èƒ½ãªé–¢æ•°ã‚¯ãƒ©ã‚¹ $\mathcal{F}_\text{Mamba}$ ã®å®šç¾©
   - $\mathcal{F}_\text{Transformer}$ ã¨ã®é–¢ä¿‚

3. **Memory-Compute Pareto Frontier**
   - æœ€é©ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•æ›²ç·šã®æ•°å­¦çš„å°å‡º
   - ä¸‹ç•Œã®è¨¼æ˜

:::message
**é€²æ—: 60% å®Œäº†** Hybrid Linear Attentionã®ä½“ç³»çš„åˆ†æã€Samba/æš—é»™çš„Attention/è’¸ç•™/ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¿œç”¨ã‚’å®Œå…¨ç¿’å¾—ã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## å‚è€ƒæ–‡çŒ® (è¿½åŠ )

[^17]: Yang, S., et al. (2024). A Systematic Analysis of Hybrid Linear Attention. *arXiv:2507.06457*.
@[card](https://arxiv.org/abs/2507.06457)

[^18]: Ren, J., et al. (2024). samba: simple hybrid state space models. *arXiv:2406.07522*.
@[card](https://arxiv.org/abs/2406.07522)

[^19]: Darcet, T., et al. (2024). The Hidden Attention of Mamba Models. *arXiv:2403.01590*.
@[card](https://arxiv.org/abs/2403.01590)

[^20]: Xu, Y., et al. (2024). The Mamba in the Llama: Distilling and Accelerating Hybrid Models. *NeurIPS 2024*.

[^21]: Wang, X., et al. (2025). XLSR-MamBo: Scaling the Hybrid Mamba-Attention Backbone for Audio. *arXiv:2601.02944*.
@[card](https://arxiv.org/abs/2601.02944)

[^22]: Maklachur, A., et al. (2024). Mamba in Vision: A Comprehensive Survey of Techniques and Applications. *arXiv:2410.03105*.
@[card](https://arxiv.org/abs/2410.03105)

[^23]: Grootendorst, M. (2024). A Visual Guide to Mamba and State Space Models. *Newsletter*.
@[card](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state)

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
