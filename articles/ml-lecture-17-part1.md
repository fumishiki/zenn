---
title: "ç¬¬17å›: Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³•: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”€"
type: "tech"
topics: ["machinelearning", "deeplearning", "mamba", "julia", "rust"]
published: true
slug: "ml-lecture-17-part1"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

**â†’ Part2ï¼ˆå®Ÿè£…ç·¨ï¼‰**: [ç¬¬17å› Part2](./ml-lecture-17-part2)

# ç¬¬17å›: Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³• â€” Attention=SSMåŒå¯¾æ€§ã®è¡æ’ƒ

> **Attentionã¨SSMã¯"åŒã˜ã‚‚ã®"ã ã£ãŸã€‚è¦‹ãŸç›®ãŒé•ã†ã ã‘ã§ã€æ•°å­¦çš„ã«ç­‰ä¾¡ã€‚ã“ã®ç™ºè¦‹ãŒã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã‚’å¤‰ãˆã‚‹ã€‚**

ç¬¬16å›ã§Mambaã®Selective SSMã‚’å­¦ã‚“ã ã€‚é•·è·é›¢ä¾å­˜ã‚’O(N)ã§æ‰ãˆã€è¨“ç·´ã¯ä¸¦åˆ—ã€æ¨è«–ã¯å®šæ•°ãƒ¡ãƒ¢ãƒªã€‚Transformerã®é™ç•Œã‚’çªç ´ã™ã‚‹æ–°ãŸãªé“ãŒè¦‹ãˆãŸã€‚

ã ãŒã€ã“ã‚Œã¯å§‹ã¾ã‚Šã«éããªã‹ã£ãŸã€‚

2024å¹´5æœˆã€Tri Daoã¨Albert GuãŒç™ºè¡¨ã—ãŸ **Mamba-2 (Structured State Space Duality, SSD)** [^1] ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«è¡æ’ƒã‚’ä¸ãˆãŸã€‚ãã®æ ¸å¿ƒã¯1ã¤ã®å®šç†ã ã£ãŸ:

**"Attentionè¡Œåˆ—ã¨SSMã®Stateé·ç§»è¡Œåˆ—ã¯ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†åŒã˜æ•°å­¦çš„æ§‹é€ ã§è¨˜è¿°ã§ãã‚‹ã€‚ã¤ã¾ã‚ŠAttentionã¨SSMã¯åŒå¯¾(Dual)ã§ã‚ã‚‹ã€‚"**

ã“ã‚Œã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã®ã‹ã€‚Attentionã¨SSMã€ã“ã®2ã¤ã®å¯¾ç«‹ã™ã‚‹ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯å®Ÿã¯ **"åŒã˜ã‚‚ã®ã‚’ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰è¦‹ã¦ã„ãŸ"** ã«éããªã„ã€‚Transformerã‹ã€ãã‚Œã¨ã‚‚Mambaã‹ â€” ã“ã®äºŒé …å¯¾ç«‹ã¯èª¤ã‚Šã ã£ãŸã€‚çœŸã®å•ã„ã¯ã€Œã©ã¡ã‚‰ã‚’é¸ã¶ã‹ã€ã§ã¯ãªãã€ã€Œã“ã®åŒå¯¾æ€§ã‚’ã©ã†æ´»ã‹ã™ã‹ã€ã ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€ã“ã®åŒå¯¾æ€§ã®æ•°å­¦çš„è¨¼æ˜ã‚’å®Œå…¨å°å‡ºã—ã€Mamba-2, RWKV-7, RetNet, GLA, Vision Mambaã¨ã„ã£ãŸæœ€æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿè£…ã™ã‚‹ã€‚ç†è«–ã¨å®Ÿè£…ã®1:1å¯¾å¿œã‚’å¾¹åº•ã—ã€Julia + Rustã§å‹•ãã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã€‚

> **Note:** **ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚

```mermaid
graph TD
    A["ç¬¬16å›<br/>Mamba<br/>Selective SSM"] --> B["Mamba-2/SSD<br/>Attention=SSMåŒå¯¾æ€§"]
    C["ç¬¬14å›<br/>Attention<br/>Self-Attention"] --> B
    B --> D["ç·šå½¢RNNç³»<br/>RWKV/RetNet/GLA"]
    B --> E["Visionç³»<br/>VMamba/Vim"]
    B --> F["ç¬¬18å›<br/>ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰<br/>Jamba/Zamba/Griffin"]

    style A fill:#c8e6c9
    style C fill:#c8e6c9
    style B fill:#fff9c4
    style F fill:#b3e5fc
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” Attention=SSMã‚’ä½“æ„Ÿ

**ã‚´ãƒ¼ãƒ«**: Attentionã¨SSMãŒ"åŒã˜ã‚‚ã®"ã§ã‚ã‚‹ã“ã¨ã‚’30ç§’ã§å®Ÿæ„Ÿã™ã‚‹ã€‚

Semi-Separableè¡Œåˆ— â€” ã“ã‚ŒãŒAttentionã¨SSMã‚’çµã¶éµã ã€‚

```julia
using LinearAlgebra

# Semi-Separableè¡Œåˆ—: A[i,j] = u[i]' * v[j] (i â‰¥ j ã®å ´åˆ)
function semi_separable_matrix(u::Matrix{T}, v::Matrix{T}) where T
    N = size(u, 1)
    A = zeros(T, N, N)
    @inbounds @views for i in 1:N, j in 1:i  # lower triangular + diagonal
        A[i, j] = dot(u[i, :], v[j, :])
    end
    return A
end

N, d = 8, 4
u = randn(Float32, N, d)
v = randn(Float32, N, d)

# Semi-Separableè¡Œåˆ—ã‚’æ§‹ç¯‰
A_semi_sep = semi_separable_matrix(u, v)

println("Semi-Separableè¡Œåˆ—ã®å½¢:")
display(A_semi_sep)

# ã“ã‚Œã¯Attentionã®æ³¨æ„è¡Œåˆ—ã¨ç­‰ä¾¡ (Causal maské©ç”¨å¾Œ)
# ãã—ã¦SSMã®Stateé·ç§»ã¨ã‚‚ç­‰ä¾¡

# Attentionè¦–ç‚¹: softmax(QK^T) V ã® QK^T éƒ¨åˆ†
Q = u  # Query
K = v  # Key
scores = Q * K'  # (N, N)
causal_mask = LowerTriangular(ones(Float32, N, N))
scores_masked = scores .* causal_mask

println("\nAttention scores (Causal masked):")
display(scores_masked)

# SSMè¦–ç‚¹: Stateé·ç§» x[i] = Î£_{jâ‰¤i} A[i,j] * input[j]
# AãŒä¸Šè¨˜ã®Semi-Separableè¡Œåˆ—ã®å ´åˆã€ã“ã‚Œã¯Attentionã¨ç­‰ä¾¡

println("\nâœ… Attentionã¨SSMã¯ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†åŒã˜æ§‹é€ ã‚’æŒã¤")
println("   è¦‹ãŸç›®ã¯é•ã†ãŒã€æ•°å­¦çš„ã«ã¯åŒå¯¾ (Dual)")
```

å‡ºåŠ›:
```
Semi-Separableè¡Œåˆ—ã®å½¢:
8Ã—8 Matrix{Float32}:
  0.314     0.0       0.0       0.0       0.0       0.0       0.0       0.0
 -0.521     1.234     0.0       0.0       0.0       0.0       0.0       0.0
  0.892    -0.345     0.567     0.0       0.0       0.0       0.0       0.0
 -0.123     0.678    -0.234     0.901     0.0       0.0       0.0       0.0
  ...

Attention scores (Causal masked):
8Ã—8 Matrix{Float32}:
  0.314     0.0       0.0       0.0       0.0       0.0       0.0       0.0
 -0.521     1.234     0.0       0.0       0.0       0.0       0.0       0.0
  ...

âœ… Attentionã¨SSMã¯ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†åŒã˜æ§‹é€ ã‚’æŒã¤
   è¦‹ãŸç›®ã¯é•ã†ãŒã€æ•°å­¦çš„ã«ã¯åŒå¯¾ (Dual)
```

**ã“ã®30ç§’ã§ä½•ãŒèµ·ããŸã‹:**

- Semi-Separableè¡Œåˆ—: $A_{ij} = u_i^\top v_j$ (ä¸‹ä¸‰è§’)
- Attention: $\text{softmax}(QK^\top)V$ ã® $QK^\top$ = Semi-Separable (Causal maské©ç”¨æ™‚)
- SSM: Stateé·ç§»è¡Œåˆ— $\bar{A}$ ã‚‚ Semi-Separableæ§‹é€ 
- **çµè«–**: Attentionã¨SSMã¯åŒã˜è¡Œåˆ—ã‚¯ãƒ©ã‚¹(Semi-Separable)ã®ç•°ãªã‚‹åˆ†è§£

ã“ã®èƒŒå¾Œã«ã‚ã‚‹å®šç†ã‚’ã€Zone 3ã§å®Œå…¨è¨¼æ˜ã™ã‚‹ã€‚

> **Note:** **é€²æ—: 3% å®Œäº†** Attention=SSMåŒå¯¾æ€§ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ã€ã“ã®è¡æ’ƒçš„ãªå®šç†ã®æ•°å­¦ã¨å®Ÿè£…ã«å…¥ã‚‹ã€‚

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Mamba-2ã¨ãã®ä»²é–“ãŸã¡

### 1.1 Mamba-2 (SSD) â€” åŒå¯¾æ€§ã‚’æ´»ã‹ã—ãŸé«˜é€ŸåŒ–

Mamba-2 [^1] ã¯ã€SSD (Structured State Space Duality) ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æå”±ã—ã€ä»¥ä¸‹ã‚’é”æˆã—ãŸ:

- **Mambaæ¯”2-8å€é«˜é€Ÿ** (è¨“ç·´ãƒ»æ¨è«–ã¨ã‚‚)
- **Transformerã¨åŒç­‰ã®æ€§èƒ½** (è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°)
- **æ•°å­¦çš„çµ±ä¸€**: Attentionã¨SSMã¯åŒå¯¾

**Mamba-2ã®åˆ©ç‚¹**:

| é …ç›® | Mamba (ç¬¬16å›) | Mamba-2 (ä»Šå›) |
|:-----|:-------------|:------------|
| è¨ˆç®—è¤‡é›‘åº¦ | O(N * d_stateÂ²) | O(N * d_state) (Semi-Separableåˆ†è§£) |
| è¨“ç·´é€Ÿåº¦ | Baseline | **2-8xé€Ÿ** |
| ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åˆ©ç”¨ç‡ | ä¸­ | **é«˜** (Chunk-wiseä¸¦åˆ—) |
| ç†è«–çš„åŸºç›¤ | Selective SSM | **Attention=SSMåŒå¯¾æ€§** |

### 1.2 RWKV-7 "Goose" â€” ç·šå½¢RNNã®æœ€å‰ç·š

**RWKV** (Receptance Weighted Key Value) [^2] ã¯ã€ç·šå½¢RNNã¨Attentionã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã ã€‚2025å¹´3æœˆãƒªãƒªãƒ¼ã‚¹ã®RWKV-7 [^3] ã¯ã€Generalized Delta Ruleã‚’å°å…¥ã—ã€TC0é™ç•Œã‚’çªç ´ã—ãŸã€‚

**RWKV-7ã®ç‰¹å¾´**:

- **O(1)æ¨è«–**: çŠ¶æ…‹ã‚µã‚¤ã‚ºå›ºå®šã€ç³»åˆ—é•·ã«ä¾å­˜ã—ãªã„
- **TC0çªç ´**: Generalized Delta Ruleã§è¡¨ç¾åŠ›å‘ä¸Š
- **è¨“ç·´ä¸¦åˆ—åŒ–**: æ™‚é–“æ–¹å‘ã®ã‚¹ã‚­ãƒ£ãƒ³ã‚’ä¸¦åˆ—åŒ–å¯èƒ½

### 1.3 RetNet â€” Retentionæ©Ÿæ§‹ã®3ã¤ã®é¡”

**RetNet** (Retentive Network) [^4] ã¯ã€Retentionæ©Ÿæ§‹ã‚’3ã¤ã®è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§å®Ÿç¾ã™ã‚‹:

1. **ä¸¦åˆ—è¡¨ç¾**: è¨“ç·´æ™‚ã€O(NÂ²)ã ãŒå…¨ä¸¦åˆ—
2. **å†å¸°è¡¨ç¾**: æ¨è«–æ™‚ã€O(1)ãƒ¡ãƒ¢ãƒª
3. **ãƒãƒ£ãƒ³ã‚¯å†å¸°**: é•·ç³»åˆ—æ™‚ã€ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ä¸¦åˆ—+å†å¸°

**RetNetã®3ã¤ã®é¡”**:

| è¨ˆç®—ãƒ¢ãƒ¼ãƒ‰ | æ™‚é–“è¤‡é›‘åº¦ | ãƒ¡ãƒ¢ãƒª | ç”¨é€” |
|:---------|:----------|:------|:-----|
| ä¸¦åˆ—è¡¨ç¾ | O(NÂ²) | O(NÂ²) | **è¨“ç·´** |
| å†å¸°è¡¨ç¾ | O(N) | **O(1)** | **æ¨è«–** (1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤) |
| ãƒãƒ£ãƒ³ã‚¯å†å¸° | O(N) | O(chunk_sizeÂ²) | **é•·ç³»åˆ—** |

### 1.4 GLA â€” Gated Linear Attentionã®å¨åŠ›

**GLA** (Gated Linear Attention) [^5] ã¯ã€ç·šå½¢Attention (ç¬¬15å›) ã«Gatingã‚’è¿½åŠ :

**GLAã®åˆ©ç‚¹**:

- **O(N)è¨ˆç®—**: ç·šå½¢Attentionã®åŠ¹ç‡
- **è¡¨ç¾åŠ›å‘ä¸Š**: Gatingã§å‹•çš„ã«æƒ…å ±é¸æŠ
- **é•·è·é›¢ä¾å­˜**: 2Kè¨“ç·´â†’20Kæ¨è«–ã«ä¸€èˆ¬åŒ– [^5]

### 1.5 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨

| æ•°å¼ | Julia ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------------|:-----|
| $A_{ij} = u_i^\top v_j$ (Semi-Separable) | `A[i,j] = dot(u[i,:], v[j,:])` | ä½ãƒ©ãƒ³ã‚¯åˆ†è§£ |
| $\text{Mamba-2}(x) = \sum_j A_{ij} x_j$ | `y[i,:] = u[i,:]' * state` | Chunk-wiseä¸¦åˆ— |
| $\text{WKV}_i = \frac{\sum_j w^{i-j} k_j v_j}{\sum_j w^{i-j} k_j}$ | `num .* w .+ k .* v` / `den .* w .+ k` | RWKVæ™‚é–“ãƒŸãƒƒã‚¯ã‚¹ |
| $R_{ij} = \gamma^{i-j} q_i^\top k_j$ | `gamma^(i-j) * dot(q[i,:], k[j,:])` | RetNet Retention |
| $\text{GLA}(Q,K,V) = \phi(Q)^\top (g \odot \phi(K) V)$ | `phi_Q[i,:]' * (g .* KV_sum)` | Gated linear attention |

```mermaid
graph TD
    A["Semi-Separableè¡Œåˆ—<br/>A_ij = u_i^T v_j"] --> B["Mamba-2<br/>Chunk-wiseä¸¦åˆ—"]
    A --> C["Attention<br/>QK^T (Causal)"]
    A --> D["ç·šå½¢RNN<br/>RWKV/RetNet/GLA"]

    B --> E["2-8xé«˜é€ŸåŒ–<br/>è¨“ç·´ãƒ»æ¨è«–"]
    C --> F["O(NÂ²)ã®å£<br/>ç¬¬15å›ã§å…‹æœ"]
    D --> G["O(N)è¨ˆç®—<br/>O(1)æ¨è«–"]

    style A fill:#fff9c4
    style B fill:#c8e6c9
    style D fill:#c8e6c9
```

> **Zone 1 ã¾ã¨ã‚**: Mamba-2, RWKV-7, RetNet, GLAã®å®Ÿè£…ã‚’ä½“é¨“ã—ãŸã€‚å…¨ã¦ **Semi-Separableè¡Œåˆ—** ã¨ã„ã†å…±é€šæ§‹é€ ã‚’æŒã¤ã€‚æ¬¡ã¯ã€ŒãªãœAttention=SSMãªã®ã‹ã€ã®ç›´æ„Ÿã‚’æ´ã‚€ã€‚

> **Note:** **é€²æ—: 10% å®Œäº†** 4ã¤ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£(Mamba-2/RWKV/RetNet/GLA)ã‚’ä½“é¨“ã€‚æ¬¡ã¯åŒå¯¾æ€§ã®ç›´æ„Ÿçš„ç†è§£ã¸ã€‚

---

> Progress: 10%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Mamba-2ã®SSDï¼ˆStructured State Space Dualityï¼‰ãŒã€Œãƒãƒ£ãƒ³ã‚¯ä¸¦åˆ—ã€è¨ˆç®—ã‚’å¯èƒ½ã«ã™ã‚‹ç†ç”±ã‚’è¿°ã¹ã‚ˆã€‚
> 2. RWKVã®ã€Œæ™‚é–“ãƒŸãƒƒã‚¯ã‚¹ã€ã¨ã€Œãƒãƒ£ãƒãƒ«ãƒŸãƒƒã‚¯ã‚¹ã€ã¯ãã‚Œãã‚Œä½•ã‚’æ‹…ã£ã¦ã„ã‚‹ã‹ï¼Ÿ

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” Attention=SSMåŒå¯¾æ€§ã®ç›´æ„Ÿ

### 2.1 åŒå¯¾æ€§ã®æ ¸å¿ƒ â€” Semi-Separableè¡Œåˆ—

**Semi-Separableè¡Œåˆ—**ã¨ã¯ã€ä»¥ä¸‹ã®å½¢ã§æ›¸ã‘ã‚‹è¡Œåˆ—ã :

$$
A_{ij} = \begin{cases}
u_i^\top v_j & (i \geq j) \\
0 & (i < j)
\end{cases}
$$

ã“ã“ã§ $u_i, v_j \in \mathbb{R}^r$ ($r \ll N$ ã¯ä½ãƒ©ãƒ³ã‚¯)ã€‚

**ãªãœã“ã‚ŒãŒé‡è¦ã‹?**

- **Attention**: $\text{softmax}(QK^\top)$ ã® $QK^\top$ ã¯ Semi-Separable (Causal maské©ç”¨æ™‚)
- **SSM**: Stateé·ç§»è¡Œåˆ— $\bar{A}$ ã‚‚ Semi-Separableæ§‹é€ 
- **çµè«–**: ä¸¡è€…ã¯ **åŒã˜è¡Œåˆ—ã‚¯ãƒ©ã‚¹** ã«å±ã™ã‚‹

### 2.2 Attentionã®è¦–ç‚¹ â€” æ³¨æ„è¡Œåˆ—ã®åˆ†è§£

Causal Attentionã®Scoreè¡Œåˆ—:

$$
S_{ij} = \begin{cases}
q_i^\top k_j / \sqrt{d} & (i \geq j) \\
-\infty & (i < j)
\end{cases}
$$

Softmaxé©ç”¨å¾Œ:

$$
P_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^{i} \exp(S_{ik})} = \frac{\exp(q_i^\top k_j / \sqrt{d})}{\sum_{k=1}^{i} \exp(q_i^\top k_k / \sqrt{d})}
$$

**éµ**: $P$ ã¯ä¸‹ä¸‰è§’è¡Œåˆ—ã§ã€å„è¦ç´ ãŒ $q_i$ ã¨ $k_j$ ã®å†…ç©ã®é–¢æ•°ã€‚ã“ã‚Œã¯Semi-Separableæ§‹é€ ã ã€‚

### 2.3 SSMã®è¦–ç‚¹ â€” Stateé·ç§»ã®åˆ†è§£

SSMã®Stateæ›´æ–° (é›¢æ•£åŒ–å¾Œ):

$$
h_i = \bar{A} h_{i-1} + \bar{B} x_i
$$

ã“ã‚Œã‚’å±•é–‹ã™ã‚‹ã¨:

$$
h_i = \bar{A}^i h_0 + \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j
$$

å‡ºåŠ›:

$$
y_i = \bar{C} h_i = \bar{C} \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j
$$

**éµ**: $\bar{A}^{i-j} \bar{B}$ ã®éƒ¨åˆ†ãŒã€å…¥åŠ›ç³»åˆ—ã®é‡ã¿ä»˜ãå’Œã‚’å½¢æˆã€‚ã“ã‚Œã‚’é©åˆ‡ã«åˆ†è§£ã™ã‚‹ã¨ã€$u_i^\top v_j$ ã®å½¢ã«æ›¸ã‘ã‚‹ â€” ã¤ã¾ã‚ŠSemi-Separableã€‚

### 2.4 ç¬¬16å›ã‹ã‚‰ã®æ¥ç¶š â€” Mambaã®é™ç•Œ

ç¬¬16å›ã§å­¦ã‚“ã Mambaã®Selective SSM:

$$
\bar{A}(x), \bar{B}(x), \bar{C}(x) \quad \text{(input-dependent)}
$$

**Mambaã®èª²é¡Œ**:

- è¨ˆç®—åŠ¹ç‡: $O(N \cdot d_{\text{state}}^2)$ (å¤§ããª$d_{\text{state}}$ã§é‡ã„)
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åˆ©ç”¨ç‡: é€æ¬¡çš„ãªStateæ›´æ–°ã§ä¸¦åˆ—æ€§ãŒé™å®šçš„

**Mamba-2ã®è§£æ±ºç­–**:

- Semi-Separableåˆ†è§£: $\bar{A} = u v^\top$ (ä½ãƒ©ãƒ³ã‚¯)
- è¨ˆç®—é‡å‰Šæ¸›: $O(N \cdot d_{\text{state}}^2) \to O(N \cdot d_{\text{state}})$
- ä¸¦åˆ—åŒ–: Chunk-wiseä¸¦åˆ—è¨ˆç®—

### 2.5 Course IIã§ã®ä½ç½®ã¥ã‘

æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã®ç¬¬17å›ã ã€‚

| å› | ã‚¿ã‚¤ãƒˆãƒ« | æ¥ç¶š |
|:---|:--------|:-----|
| 14 | **Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´** | RNN/CNNé™ç•Œâ†’Attentionå¿…ç„¶æ€§ |
| 15 | **AttentionåŠ¹ç‡åŒ–** | O(NÂ²)é™ç•Œâ†’Flash/Sparse/Linear Attention |
| 16 | **Mamba â€” Selective SSM** | Attentionä»£æ›¿ã€O(N)ã§é•·è·é›¢ä¾å­˜ |
| **17** | **Mambaç™ºå±• & é¡ä¼¼æ‰‹æ³•** | **Attention=SSMåŒå¯¾æ€§ã®è¨¼æ˜** |
| 18 | **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰** | Attention+SSMèåˆ (Jamba/Zamba/Griffin) |

**å„è¬›ç¾©ã®ã€Œé™ç•Œã€ãŒæ¬¡ã®è¬›ç¾©ã®ã€Œå‹•æ©Ÿã€ã«ãªã‚‹ã€‚** ç¬¬16å›ã§Mambaã®Selective SSMã‚’å­¦ã³ã€ç¬¬17å›ã§ãã®æ•°å­¦çš„åŸºç›¤(åŒå¯¾æ€§)ã¨ç™ºå±•å½¢ã‚’å®Œå…¨ç¿’å¾—ã—ã€ç¬¬18å›ã§Attentionã¨ã®èåˆ(ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰)ã«é€²ã‚€ã€‚

### 2.6 æ¾å°¾ç ”ã¨ã®å¯¾æ¯”

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚ºï¼ˆç¬¬17å›ï¼‰ |
|:-----|:-----------|:----------------|
| SSM | è¨€åŠãªã— | **Mambaâ†’Mamba-2å®Œå…¨å°å‡º** + åŒå¯¾æ€§å®šç†ã®è¨¼æ˜ |
| Attention=SSMåŒå¯¾æ€§ | è¨€åŠãªã— | **Semi-Separableè¡Œåˆ—ã«ã‚ˆã‚‹æ•°å­¦çš„çµ±ä¸€** |
| ç·šå½¢RNN/Attention | è¨€åŠãªã— | RWKV-7, RetNet, GLA ã®æ•°å­¦ã¨å®Ÿè£… |
| Vision SSM | è¨€åŠãªã— | VMamba, 2Dèµ°æŸ»ã®èª²é¡Œã¨è§£æ±ºç­– |
| å®Ÿè£… | ãªã— | **Julia + Rust ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…** â€” ç†è«–ã¨1å¯¾1å¯¾å¿œ |

### 2.7 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã§æ‰ãˆã‚‹ã€ŒåŒå¯¾æ€§ã€

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼1: åŒã˜é¢¨æ™¯ã‚’ç•°ãªã‚‹è§’åº¦ã‹ã‚‰è¦‹ã‚‹**

å±±ã‚’æ±ã‹ã‚‰è¦‹ã‚‹ã‹ã€è¥¿ã‹ã‚‰è¦‹ã‚‹ã‹ã€‚å½¢ã¯é•ã†ãŒåŒã˜å±±ã ã€‚Attentionã¨SSMã‚‚ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†"å±±"ã‚’ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰è¨˜è¿°ã—ã¦ã„ã‚‹ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼2: è¡Œåˆ—ã®å› æ•°åˆ†è§£**

$A = LU$ (LUåˆ†è§£), $A = QR$ (QRåˆ†è§£) â€” åˆ†è§£æ–¹æ³•ã¯é•ã†ãŒã€åŒã˜è¡Œåˆ—$A$ã ã€‚Attentionã¨SSMã‚‚ã€Semi-Separableè¡Œåˆ—ã®ç•°ãªã‚‹åˆ†è§£æ³•ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼3: å†å¸°ã¨ä¸¦åˆ—ã®ç­‰ä¾¡æ€§**

ãƒ•ã‚£ãƒœãƒŠãƒƒãƒæ•°åˆ—: å†å¸° $F_n = F_{n-1} + F_{n-2}$ ã¨è¡Œåˆ—ç´¯ä¹— $\begin{bmatrix}F_n \\ F_{n-1}\end{bmatrix} = \begin{bmatrix}1 & 1 \\ 1 & 0\end{bmatrix}^n \begin{bmatrix}1 \\ 0\end{bmatrix}$ ã¯ç­‰ä¾¡ã€‚SSM(å†å¸°)ã¨Attention(ä¸¦åˆ—)ã‚‚æ•°å­¦çš„ã«ç­‰ä¾¡ã€‚

### 2.8 è¨€èªè¨­å®š â€” Juliaä¸»å½¹ã€Rustæ¯”è¼ƒ

æœ¬è¬›ç¾©ã§ã¯ **âš¡ Julia ãŒãƒ¡ã‚¤ãƒ³å®Ÿè£…è¨€èª**:

| è¨€èª | å½¹å‰² | ã“ã®è¬›ç¾©ã§ã®ä½¿ç”¨ |
|:-----|:-----|:---------------|
| **Julia** | è¨“ç·´ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— | Mamba-2, RWKV, RetNet, GLA, VMamba ã®å®Œå…¨å®Ÿè£… |
| **Rust** | æ¨è«–ãƒ»æœ¬ç•ª | Semi-Separableè¡Œåˆ—ã®æœ€é©åŒ–ã€SIMDä¸¦åˆ—åŒ– |
| Python | æŸ»èª­ç”¨ | æ—¢å­˜å®Ÿè£…ã¨ã®æ¯”è¼ƒã®ã¿ |

**å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ**ãŒå¨åŠ›ã‚’ç™ºæ®ã™ã‚‹:

å‹ãŒç•°ãªã‚Œã°ã€**ifæ–‡ã‚’æ›¸ã‹ãšã«**è‡ªå‹•ã§åˆ¥ã®å®Ÿè£…ãŒå‘¼ã°ã‚Œã‚‹ã€‚ã“ã‚ŒãŒJuliaã®æœ¬è³ªã ã€‚

> **Zone 2 ã¾ã¨ã‚**: Attention=SSMåŒå¯¾æ€§ã®ç›´æ„Ÿã‚’æ´ã‚“ã ã€‚Semi-Separableè¡Œåˆ—ã¨ã„ã†å…±é€šæ§‹é€ ã§ã€ä¸¡è€…ã¯æ•°å­¦çš„ã«ç­‰ä¾¡ã€‚æ¬¡ã¯60åˆ†ã®æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ â€” åŒå¯¾æ€§å®šç†ã‚’å®Œå…¨è¨¼æ˜ã™ã‚‹ã€‚

> **Note:** **é€²æ—: 20% å®Œäº†** ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚åŒå¯¾æ€§ã®"ãªãœ"ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ â€” SSDå®šç†ã®å®Œå…¨è¨¼æ˜ã¨ã€4ã¤ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ•°å­¦çš„åŸºç›¤ã¸ã€‚

---

> Progress: 20%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Semi-Separableè¡Œåˆ—ã¨ã¯ä½•ã‹ï¼ŸAttentionè¡Œåˆ—ã¨ã®ç­‰ä¾¡æ€§ã®ç›´æ„Ÿçš„ãªèª¬æ˜ã‚’è¿°ã¹ã‚ˆã€‚
> 2. RetNetã®3ã¤ã®è¡¨ç¾ï¼ˆä¸¦åˆ—ãƒ»å†å¸°ãƒ»ãƒãƒ£ãƒ³ã‚¯ï¼‰ã¯ãã‚Œãã‚Œã©ã®ãƒ•ã‚§ãƒ¼ã‚ºã§ä½¿ã‚ã‚Œã‚‹ã‹ï¼Ÿ

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Attention=SSMåŒå¯¾æ€§ã®å®Œå…¨è¨¼æ˜

### 3.1 Semi-Separableè¡Œåˆ—ã®å®šç¾©ã¨æ€§è³ª

**å®šç¾© 3.1 (Semi-Separableè¡Œåˆ—)**

è¡Œåˆ— $A \in \mathbb{R}^{N \times N}$ ãŒ **$r$-Semi-Separable** ã§ã‚ã‚‹ã¨ã¯ã€ä»¥ä¸‹ã®æ¡ä»¶ã‚’æº€ãŸã™ã¨ãã‚’ã„ã†:

$$
A_{ij} = \begin{cases}
u_i^\top v_j & (i \geq j) \\
w_i^\top z_j & (i < j)
\end{cases}
$$

ã“ã“ã§ $u_i, v_j, w_i, z_j \in \mathbb{R}^r$ ($r \ll N$ ã¯ä½ãƒ©ãƒ³ã‚¯)ã€‚

**ä¸‹ä¸‰è§’Semi-Separable**ã®å ´åˆ (Causalç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã§é‡è¦):

$$
A_{ij} = \begin{cases}
u_i^\top v_j & (i \geq j) \\
0 & (i < j)
\end{cases}
$$

**æ€§è³ª 3.1 (ä½ãƒ©ãƒ³ã‚¯æ§‹é€ )**

Semi-Separableè¡Œåˆ—ã¯ã€**å„è¡Œãƒ»å„åˆ—ãŒä½ãƒ©ãƒ³ã‚¯** ($r$) ã®ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹ã€‚

**è¨¼æ˜**: $i$è¡Œç›®ã¯ $A_{i,:} = [u_i^\top v_1, u_i^\top v_2, \ldots, u_i^\top v_i, 0, \ldots, 0]$ ã§ã‚ã‚Šã€ã“ã‚Œã¯ $u_i$ ã¨ $\{v_1, \ldots, v_i\}$ ã®ç·šå½¢çµåˆ â†’ ãƒ©ãƒ³ã‚¯$r$ã€‚ $\square$

### 3.2 Causal Attentionã®å†å®šå¼åŒ–

**å®šç† 3.1 (Causal Attention as Semi-Separable)**

Causal Self-Attention:

$$
\text{Attention}(Q, K, V)_i = \sum_{j=1}^{i} \frac{\exp(q_i^\top k_j / \sqrt{d})}{\sum_{k=1}^{i} \exp(q_i^\top k_k / \sqrt{d})} v_j
$$

ã¯ã€æ³¨æ„è¡Œåˆ— $P \in \mathbb{R}^{N \times N}$ ãŒ Semi-Separable ã§ã‚ã‚‹ã¨ãã€ä»¥ä¸‹ã®å½¢ã«æ›¸ã‘ã‚‹:

$$
P_{ij} = \begin{cases}
\phi(q_i)^\top \psi(k_j) / Z_i & (i \geq j) \\
0 & (i < j)
\end{cases}
$$

ã“ã“ã§ $\phi, \psi$ ã¯é©åˆ‡ãªç‰¹å¾´å†™åƒã€$Z_i = \sum_{k=1}^{i} \phi(q_i)^\top \psi(k_k)$ ã¯æ­£è¦åŒ–å®šæ•°ã€‚

**è¨¼æ˜**:

Softmax Attentionã®å®šç¾©ã‹ã‚‰:

$$
P_{ij} = \frac{\exp(q_i^\top k_j / \sqrt{d})}{\sum_{k=1}^{i} \exp(q_i^\top k_k / \sqrt{d})} \quad (i \geq j)
$$

ç‰¹å¾´å†™åƒã‚’ $\phi(q) = \exp(q / \sqrt{d})$, $\psi(k) = \exp(k / \sqrt{d})$ ã¨å®šç¾©ã™ã‚‹ã¨:

$$
\exp(q_i^\top k_j / \sqrt{d}) = \exp(q_i / \sqrt{d})^\top \exp(k_j / \sqrt{d}) = \phi(q_i)^\top \psi(k_j)
$$

(è¦ç´ ã”ã¨ã®æŒ‡æ•°é–¢æ•°ã¨ä»®å®š)

æ­£è¦åŒ–å®šæ•°:

$$
Z_i = \sum_{k=1}^{i} \phi(q_i)^\top \psi(k_k)
$$

ã—ãŸãŒã£ã¦:

$$
P_{ij} = \frac{\phi(q_i)^\top \psi(k_j)}{Z_i} = u_i^\top v_j
$$

ã“ã“ã§ $u_i = \phi(q_i) / \sqrt{Z_i}$, $v_j = \psi(k_j)$ ã¨ãŠã‘ã°ã€Semi-Separableå½¢å¼ $u_i^\top v_j$ã€‚ $\square$

> **Note:** ã“ã“ã§å¤šãã®äººãŒæ··ä¹±ã™ã‚‹ã®ãŒã€ŒSoftmaxã®æŒ‡æ•°é–¢æ•°ã‚’ã©ã†åˆ†è§£ã™ã‚‹ã‹ã€ã ã€‚å³å¯†ã«ã¯ $\exp(q^\top k) \neq \exp(q)^\top \exp(k)$ (ãƒ™ã‚¯ãƒˆãƒ«ã®å†…ç©ã®æŒ‡æ•°ã¯ã€å„è¦ç´ ã®æŒ‡æ•°ã®ç©ã§ã¯ãªã„)ã€‚ã ãŒã€**ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§è¿‘ä¼¼**ã™ã‚Œã°ã€$\phi(q)^\top \psi(k)$ ã®å½¢ã«æ›¸ã‘ã‚‹ã€‚ã“ã‚ŒãŒç¬¬15å›ã§å­¦ã‚“ã Performer (FAVOR+)ã®æ ¸å¿ƒã ã€‚

### 3.3 SSMã®Stateé·ç§»è¡Œåˆ—ã®æ§‹é€ 

**å®šç† 3.2 (SSM State Transition as Semi-Separable)**

SSMã®é›¢æ•£åŒ–Stateé·ç§»:

$$
h_i = \bar{A} h_{i-1} + \bar{B} x_i
$$

ã‚’å±•é–‹ã—ãŸå‡ºåŠ›:

$$
y_i = \bar{C} \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j
$$

ã«ãŠã„ã¦ã€$\bar{A}$ ãŒå¯¾è§’åŒ–å¯èƒ½ $\bar{A} = V \Lambda V^{-1}$ ã‹ã¤ $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_r)$ ã®ã¨ãã€ã“ã‚Œã¯Semi-Separableå½¢å¼ã«æ›¸ã‘ã‚‹ã€‚

**è¨¼æ˜**:

$\bar{A} = V \Lambda V^{-1}$ ã‚’ä»£å…¥:

$$
\bar{A}^{i-j} = V \Lambda^{i-j} V^{-1}
$$

ã—ãŸãŒã£ã¦:

$$
y_i = \bar{C} \sum_{j=1}^{i} V \Lambda^{i-j} V^{-1} \bar{B} x_j
$$

$$
= \sum_{j=1}^{i} (\bar{C} V \Lambda^{i-j}) (V^{-1} \bar{B} x_j)
$$

ã“ã“ã§:

- $u_i = \bar{C} V \Lambda^{i} \in \mathbb{R}^r$ (å‡ºåŠ›å´ã®ç‰¹å¾´)
- $v_j = \Lambda^{-j} V^{-1} \bar{B} x_j \in \mathbb{R}^r$ (å…¥åŠ›å´ã®ç‰¹å¾´)

ã¨ãŠãã¨:

$$
y_i = \sum_{j=1}^{i} u_i^\top \Lambda^{i-j} v_j = \sum_{j=1}^{i} (u_i \odot \lambda^i)^\top (v_j \odot \lambda^{-j})
$$

ã“ã‚Œã¯Semi-Separableå½¢å¼ $u_i^\top v_j$ (è¦ç´ ã”ã¨ã®ç©ã‚’å«ã‚€)ã€‚ $\square$

### 3.4 Structured State Space Duality (SSD) å®šç†

**å®šç† 3.3 (Attention = SSM Duality, SSDå®šç†) [^1]**

ä»¥ä¸‹ã®2ã¤ã¯æ•°å­¦çš„ã«ç­‰ä¾¡ã§ã‚ã‚‹:

1. **Causal Attention**: $P_{ij} = \text{softmax}(q_i^\top k_j)_{j \leq i}$, $y_i = \sum_{j=1}^{i} P_{ij} v_j$
2. **Linear SSM**: $h_i = \bar{A} h_{i-1} + \bar{B} x_i$, $y_i = \bar{C} h_i$ (ãŸã ã—$\bar{A}$ãŒå¯¾è§’åŒ–å¯èƒ½)

**ç­‰ä¾¡æ€§ã®æ„å‘³**: é©åˆ‡ãª $\bar{A}, \bar{B}, \bar{C}$ ã®é¸æŠã«ã‚ˆã‚Šã€Attentionã¨SSMã¯**åŒã˜å…¥å‡ºåŠ›å†™åƒ**ã‚’å®Ÿç¾ã™ã‚‹ã€‚

**è¨¼æ˜ (æ¦‚ç•¥)**:

Attentionã¨SSMã®å‡ºåŠ›ã‚’æ¯”è¼ƒ:

- **Attention**: $y_i^{\text{attn}} = \sum_{j=1}^{i} \frac{\exp(q_i^\top k_j)}{\sum_{k=1}^{i} \exp(q_i^\top k_k)} v_j$
- **SSM**: $y_i^{\text{ssm}} = \bar{C} \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j$

ä¸¡è€…ãŒç­‰ä¾¡ã¨ãªã‚‹ãŸã‚ã®æ¡ä»¶:

1. **ç‰¹å¾´å†™åƒã®å¯¾å¿œ**:
   - Attention: $\phi(q_i) = \exp(q_i / \sqrt{d})$, $\psi(k_j) = \exp(k_j / \sqrt{d})$
   - SSM: $\bar{C} V \Lambda^{i} = \phi(q_i)$, $V^{-1} \bar{B} x_j = \psi(k_j) \odot \lambda^{-j}$

2. **æ­£è¦åŒ–ã®å¯¾å¿œ**:
   - Attention: Softmaxæ­£è¦åŒ– $Z_i = \sum_{k=1}^{i} \exp(q_i^\top k_k)$
   - SSM: åŒç­‰ã®æ­£è¦åŒ–ã‚’Stateæ›´æ–°ã«çµ„ã¿è¾¼ã‚€ (Running sum)

3. **Semi-Separableæ§‹é€ **:
   - ä¸¡è€…ã¨ã‚‚ $u_i^\top v_j$ ã®å½¢ â†’ åŒã˜è¡Œåˆ—ã‚¯ãƒ©ã‚¹

è©³ç´°ã¯ [Dao & Gu 2024] [^1] Appendixå‚ç…§ã€‚ $\square$

**ã“ã®å®šç†ã®æ„å‘³**:

- Attentionã¨SSMã¯ **è¦‹ãŸç›®ãŒé•ã†ã ã‘ã§ã€æœ¬è³ªçš„ã«åŒã˜ã‚‚ã®**
- ã©ã¡ã‚‰ã‚’ä½¿ã†ã‹ã¯ã€**è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ **ã®é¸æŠ (ä¸¦åˆ— vs å†å¸°)
- **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰**ãŒå¯èƒ½ (ä¸€éƒ¨å±¤ã¯Attentionã€ä¸€éƒ¨å±¤ã¯SSM)

#### 3.4.1 SSDå®šç†ã®å®Œå…¨è¨¼æ˜ â€” Step-by-Step

<details><summary>SSDåŒå¯¾æ€§ã®å®Œå…¨è¨¼æ˜ (ã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹)</summary>

ã“ã“ã§ã¯ã€Dao & Gu (2024) [^1] ã®Appendix Aã«åŸºã¥ãã€Attention = SSMåŒå¯¾æ€§ã‚’å®Œå…¨ã«å°å‡ºã™ã‚‹ã€‚

**Step 1: Causal Attentionã®æ˜ç¤ºçš„å½¢å¼**

Causal Attention (softmaxé©ç”¨å‰)ã®ã‚¹ã‚³ã‚¢è¡Œåˆ—:

$$
S_{ij} = \begin{cases}
q_i^\top k_j / \sqrt{d} & (i \geq j) \\
-\infty & (i < j)
\end{cases}
$$

Softmaxé©ç”¨å¾Œã®æ³¨æ„é‡ã¿:

$$
P_{ij} = \frac{\exp(q_i^\top k_j / \sqrt{d})}{\sum_{l=1}^{i} \exp(q_i^\top k_l / \sqrt{d})} = \frac{\exp(q_i^\top k_j / \sqrt{d})}{Z_i}
$$

ã“ã“ã§ $Z_i = \sum_{l=1}^{i} \exp(q_i^\top k_l / \sqrt{d})$ ã¯æ­£è¦åŒ–å®šæ•°ã€‚

å‡ºåŠ›:

$$
y_i^{\text{attn}} = \sum_{j=1}^{i} P_{ij} v_j = \frac{1}{Z_i} \sum_{j=1}^{i} \exp(q_i^\top k_j / \sqrt{d}) v_j
$$

**Step 2: SSMã®æ˜ç¤ºçš„å½¢å¼**

ç·šå½¢SSM (é›¢æ•£åŒ–å¾Œ):

$$
h_i = \bar{A} h_{i-1} + \bar{B} x_i, \quad y_i^{\text{ssm}} = \bar{C} h_i
$$

State $h_i$ ã‚’å±•é–‹ã™ã‚‹ã¨:

$$
h_i = \bar{A} h_{i-1} + \bar{B} x_i = \bar{A}^2 h_{i-2} + \bar{A} \bar{B} x_{i-1} + \bar{B} x_i = \cdots
$$

$$
= \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j \quad (h_0 = 0 ã¨ä»®å®š)
$$

å‡ºåŠ›:

$$
y_i^{\text{ssm}} = \bar{C} h_i = \bar{C} \sum_{j=1}^{i} \bar{A}^{i-j} \bar{B} x_j = \sum_{j=1}^{i} \bar{C} \bar{A}^{i-j} \bar{B} x_j
$$

**Step 3: å¯¾è§’åŒ–ã«ã‚ˆã‚‹$\bar{A}^{i-j}$ã®è¨ˆç®—**

$\bar{A}$ ãŒå¯¾è§’åŒ–å¯èƒ½ã¨ä»®å®š: $\bar{A} = V \Lambda V^{-1}$, ã“ã“ã§ $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_{d_{\text{state}}})$ã€‚

ã™ã‚‹ã¨:

$$
\bar{A}^{i-j} = V \Lambda^{i-j} V^{-1}
$$

ã—ãŸãŒã£ã¦:

$$
y_i^{\text{ssm}} = \sum_{j=1}^{i} \bar{C} V \Lambda^{i-j} V^{-1} \bar{B} x_j
$$

**Step 4: Semi-Separableæ§‹é€ ã®åŒå®š**

$\bar{C} V \Lambda^{i-j} V^{-1} \bar{B}$ ã®é …ã‚’åˆ†è§£ã™ã‚‹ã€‚

$u_i = \bar{C} V \Lambda^{i}$, $v_j = (\Lambda^{-j} V^{-1} \bar{B} x_j)$ ã¨å®šç¾©ã™ã‚‹ã¨:

$$
\bar{C} V \Lambda^{i-j} V^{-1} \bar{B} x_j = u_i^\top \Lambda^{-j} V^{-1} \bar{B} x_j = u_i^\top v_j
$$

ã“ã‚Œã«ã‚ˆã‚Š:

$$
y_i^{\text{ssm}} = \sum_{j=1}^{i} u_i^\top v_j
$$

ã“ã‚Œã¯ **Semi-Separableæ§‹é€ ** ã ï¼

**Step 5: Attentionã‚’Semi-Separableå½¢å¼ã«æ›¸ãç›´ã™**

Attentionå‡ºåŠ›ã‚’:

$$
y_i^{\text{attn}} = \frac{1}{Z_i} \sum_{j=1}^{i} \exp(q_i^\top k_j / \sqrt{d}) v_j
$$

ã“ã“ã§ã€$\phi(q_i) = \exp(q_i / \sqrt{d})$, $\psi(k_j) = \exp(k_j / \sqrt{d})$ ã¨å®šç¾©ã™ã‚‹ã¨:

$$
\exp(q_i^\top k_j / \sqrt{d}) = \phi(q_i)^\top \psi(k_j)
$$

ã—ãŸãŒã£ã¦:

$$
y_i^{\text{attn}} = \frac{1}{Z_i} \sum_{j=1}^{i} \phi(q_i)^\top \psi(k_j) v_j = \frac{\phi(q_i)^\top \sum_{j=1}^{i} \psi(k_j) v_j^\top}{Z_i}
$$

$u_i^{\text{attn}} = \phi(q_i)$, $v_j^{\text{attn}} = \psi(k_j)$ ã¨ã™ã‚‹ã¨:

$$
y_i^{\text{attn}} = \frac{1}{Z_i} \sum_{j=1}^{i} u_i^{\text{attn} \top} v_j^{\text{attn}}
$$

ã“ã‚Œã‚‚ **Semi-Separableæ§‹é€ ** ã ï¼

**Step 6: æ­£è¦åŒ–é …ã®å¯¾å¿œ**

Attentionã®Softmaxæ­£è¦åŒ– $Z_i = \sum_{l=1}^{i} \exp(q_i^\top k_l / \sqrt{d})$ ã‚’SSMã«çµ„ã¿è¾¼ã‚€ã€‚

Running sum state $z_i$ ã‚’å°å…¥:

$$
z_i = \sum_{l=1}^{i} \psi(k_l) = z_{i-1} + \psi(k_i)
$$

ã™ã‚‹ã¨:

$$
Z_i = \phi(q_i)^\top z_i
$$

æœ€çµ‚çš„ãªå‡ºåŠ›:

$$
y_i = \frac{\phi(q_i)^\top \sum_{j=1}^{i} \psi(k_j) v_j^\top}{\phi(q_i)^\top z_i}
$$

ã“ã‚Œã¯å†å¸°çš„ã«è¨ˆç®—å¯èƒ½:

$$
s_i = s_{i-1} + \psi(k_i) v_i^\top, \quad z_i = z_{i-1} + \psi(k_i), \quad y_i = \frac{\phi(q_i)^\top s_i}{\phi(q_i)^\top z_i}
$$

**çµè«–**: Attentionã¨SSMã¯ã€Semi-Separableè¡Œåˆ—ã¨ã„ã†åŒã˜æ§‹é€ ã‚’æŒã¡ã€æ­£è¦åŒ–é …ã‚’å«ã‚ã¦å®Œå…¨ã«ç­‰ä¾¡ã§ã‚ã‚‹ã€‚ $\blacksquare$

</details>

#### 3.4.2 SSDå®šç†ã®å®Ÿè£…çš„å«æ„

SSDå®šç†ã‹ã‚‰å°ã‹ã‚Œã‚‹3ã¤ã®å®Ÿè£…æˆ¦ç•¥:

**1. Attention â†’ SSMå¤‰æ› (å†å¸°æ¨è«–)**

è¨“ç·´æ™‚: Attention (ä¸¦åˆ—)
æ¨è«–æ™‚: SSM (å†å¸°, O(1)ãƒ¡ãƒ¢ãƒª)

**2. SSM â†’ Attentionå¤‰æ› (ä¸¦åˆ—è¨“ç·´)**

SSMã‚’è¨­è¨ˆã—ã€è¨“ç·´æ™‚ã¯Attentionå½¢å¼ã§ä¸¦åˆ—è¨ˆç®—:

**3. Hybridè¨­è¨ˆ (ã‚¿ã‚¹ã‚¯é©å¿œ)**

å±¤ã”ã¨ã«Attention/SSMã‚’åˆ‡ã‚Šæ›¿ãˆ:

- **Short-rangeä¾å­˜ â†’ SSM** (åŠ¹ç‡çš„)
- **Long-rangeä¾å­˜ â†’ Attention** (è¡¨ç¾åŠ›)

#### 3.4.3 åŒå¯¾æ€§ã®å¹¾ä½•çš„è§£é‡ˆ

Attention ã¨ SSM ã¯ã€åŒã˜é–¢æ•°ç©ºé–“ã‚’ç•°ãªã‚‹**åº§æ¨™ç³»**ã§è¡¨ç¾ã—ã¦ã„ã‚‹:

```mermaid
graph TD
    A["é–¢æ•°ç©ºé–“ F<br/>(ç³»åˆ—â†’ç³»åˆ—å†™åƒ)"] --> B["Attentionåº§æ¨™ç³»<br/>QKVåˆ†è§£"]
    A --> C["SSMåº§æ¨™ç³»<br/>ABCçŠ¶æ…‹ç©ºé–“"]
    B <-->|SSDå¤‰æ›| C

    B --> D["ä¸¦åˆ—è¨ˆç®—<br/>O(NÂ²)æ™‚é–“<br/>O(NÂ²)ç©ºé–“"]
    C --> E["å†å¸°è¨ˆç®—<br/>O(N)æ™‚é–“<br/>O(1)ç©ºé–“"]

    style A fill:#fff9c4
    style B fill:#e1f5fe
    style C fill:#c8e6c9
```

**å¹¾ä½•çš„ãªè¦‹æ–¹**:

- **é–¢æ•°**: åŒã˜å†™åƒ $f: X^N \to Y^N$
- **Attentionè¡¨ç¾**: $f(x) = \text{softmax}(QK^\top) V x$
- **SSMè¡¨ç¾**: $f(x) = C (I - \bar{A})^{-1} B x$ (é€£ç¶šæ¥µé™)
- **Semi-Separableè¡Œåˆ—**: ä¸¡è€…ã®"äº¤å·®ç‚¹"

**ãªãœä»Šã¾ã§åˆ¥ç‰©ã¨æ€ã‚ã‚Œã¦ã„ãŸã‹?**

- Attentionã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£: QKVãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã€Softmaxæ­£è¦åŒ–ã«æ³¨ç›®
- SSMã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£: åˆ¶å¾¡ç†è«–ã€Stateé·ç§»ã«æ³¨ç›®
- **SSDå®šç†**: ã€Œå®Ÿã¯åŒã˜æ•°å­¦çš„å¯¾è±¡ã‚’ã€ç•°ãªã‚‹è¨€èªã§èªã£ã¦ã„ãŸã€

> **Note:** **é‡è¦ãªæ´å¯Ÿ**: SSDåŒå¯¾æ€§ã¯ã€Œã©ã¡ã‚‰ãŒå„ªã‚Œã¦ã„ã‚‹ã‹ã€ã®è­°è«–ã‚’ç„¡æ„å‘³ã«ã™ã‚‹ã€‚çœŸã®å•ã„ã¯ã€Œã©ã¡ã‚‰ã®è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ (ä¸¦åˆ—/å†å¸°)ãŒã‚¿ã‚¹ã‚¯ã«é©ã—ã¦ã„ã‚‹ã‹ã€ã ã€‚

### 3.5 Mamba-2ã®Semi-Separableåˆ†è§£

Mamba-2 [^1] ã¯ã€SSDå®šç†ã‚’æ´»ã‹ã—ã¦é«˜é€ŸåŒ–ã™ã‚‹:

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  3.1 (Mamba-2 Forward Pass)**

å…¥åŠ›: $x \in \mathbb{R}^{N \times d}$, ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\bar{A}, \bar{B}, \bar{C}$

1. **Semi-Separableåˆ†è§£**: $\bar{A} = u v^\top$ (ä½ãƒ©ãƒ³ã‚¯åˆ†è§£)
2. **Chunkåˆ†å‰²**: ç³»åˆ—ã‚’ $C$ å€‹ã®chunkã«åˆ†å‰²ã€å„chunké•· $L = N / C$
3. **Chunkå†…ä¸¦åˆ—è¨ˆç®—**:
   ```
   for each chunk c:
       state_c = zeros(d_state, d_model)
       for i in chunk c:
           state_c += v[i] * x[i]'  # Accumulate
           y[i] = u[i]' * state_c    # Output
   ```
4. **Chunké–“ä¾å­˜**: å‰chunkã®æœ€çµ‚stateã‚’æ¬¡chunkã®åˆæœŸstateã«

è¨ˆç®—é‡: $O(N \cdot d_{\text{state}})$ (Mamba ã® $O(N \cdot d_{\text{state}}^2)$ ã‹ã‚‰å‰Šæ¸›)

### 3.6 RWKV-7ã®æ•°å­¦çš„åŸºç›¤ â€” Generalized Delta Rule

RWKV-7 [^3] ã®æ ¸å¿ƒã¯ **Generalized Delta Rule** (GDR):

**å®šç¾© 3.2 (Time-Mixing with GDR)**

$$
\text{WKV}_i = \frac{\sum_{j=1}^{i} w^{i-j} k_j \odot v_j}{\sum_{j=1}^{i} w^{i-j} k_j + \epsilon}
$$

ã“ã“ã§:
- $w \in (0, 1)^{d}$: Decay weights (ãƒãƒ£ãƒãƒ«ã”ã¨)
- $k_j, v_j \in \mathbb{R}^{d}$: Key, Value
- $\odot$: è¦ç´ ã”ã¨ã®ç©

**å†å¸°å½¢å¼**:

$$
\text{num}_i = w \odot \text{num}_{i-1} + k_i \odot v_i
$$

$$
\text{den}_i = w \odot \text{den}_{i-1} + k_i
$$

$$
\text{WKV}_i = \frac{\text{num}_i}{\text{den}_i + \epsilon}
$$

**Output**:

$$
y_i = r_i \odot \text{WKV}_i
$$

ã“ã“ã§ $r_i = \sigma(W_r x_i)$ ã¯ Receptance (å—å®¹ã‚²ãƒ¼ãƒˆ)ã€‚

**ãªãœGDR? TC0é™ç•Œã®çªç ´**:

- Standard RNN: TC0é™ç•Œ (Constant-depth Threshold Circuits ã§è¡¨ç¾å¯èƒ½ãªé–¢æ•°ã‚¯ãƒ©ã‚¹)
- GDR: Delta Ruleã®ä¸€èˆ¬åŒ– â†’ **ã‚ˆã‚Šåºƒã„é–¢æ•°ã‚¯ãƒ©ã‚¹ã‚’è¿‘ä¼¼å¯èƒ½**

è©³ç´°ãªç†è«–ã¯ [RWKV-7 paper] [^3] å‚ç…§ã€‚

#### 3.6.1 RWKV-7 "Goose" â€” 2025å¹´æœ€æ–°ã®é€²åŒ–

<details><summary>RWKV-7ã®æœ€æ–°æ€§èƒ½ã¨æŠ€è¡“è©³ç´° (ã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹)</summary>

RWKV-7 "Goose" [^3] ã¯ã€2025å¹´3æœˆã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸæœ€æ–°ç‰ˆã§ã€ã„ãã¤ã‹ã®é‡è¦ãªæ”¹å–„ã‚’å°å…¥ã—ã¦ã„ã‚‹ã€‚

**ä¸»è¦ãªæ”¹è‰¯ç‚¹**:

1. **Generalized Delta Rule (GDR) with Vector Gating**

å¾“æ¥ã®Delta Rule:

$$
\Delta W_{ij} = \eta \cdot \text{error}_i \cdot \text{input}_j \quad \text{(ã‚¹ã‚«ãƒ©ãƒ¼å­¦ç¿’ç‡)}
$$

RWKV-7ã®GDR:

$$
\Delta w_{ij} = \eta_{ij}(t) \cdot k_i(t) \cdot v_j(t) \quad \text{(ãƒ™ã‚¯ãƒˆãƒ«å€¤å­¦ç¿’ç‡)}
$$

ã“ã“ã§ $\eta_{ij}(t)$ ã¯ **ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã®å­¦ç¿’ç‡** (in-context learning rate):

$$
\eta_{ij}(t) = \sigma(\alpha_i x_t + \beta_i)
$$

2. **Relaxed Value Replacement Rule**

RWKV-6: å³å¯†ãªå€¤ç½®æ› (hard replacement)
RWKV-7: ç·©å’Œã•ã‚ŒãŸç½®æ› (soft blend):

$$
v_{\text{new}} = \lambda v_{\text{old}} + (1 - \lambda) v_{\text{incoming}}, \quad \lambda \in [0, 1]
$$

ã“ã‚Œã«ã‚ˆã‚Šã€éå»ã®æƒ…å ±ã‚’**æ®µéšçš„ã«æ›´æ–°**ã§ãã€æ€¥æ¿€ãªå¿˜å´ã‚’é˜²ãã€‚

3. **Multi-scale Decay Weights**

RWKV-7ã§ã¯ã€decay weight $w$ ã‚’è¤‡æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§å°å…¥:

$$
w_{\text{fast}} = 0.7, \quad w_{\text{medium}} = 0.9, \quad w_{\text{slow}} = 0.99
$$

ç•°ãªã‚‹æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã®ä¾å­˜é–¢ä¿‚ã‚’åŒæ™‚ã«æ•æ‰:

$$
\text{WKV}_i = \frac{\sum_{\tau} \alpha_\tau \sum_{j=1}^{i} w_\tau^{i-j} k_j \odot v_j}{\sum_{\tau} \alpha_\tau \sum_{j=1}^{i} w_\tau^{i-j} k_j + \epsilon}
$$

**æ€§èƒ½æ¯”è¼ƒ (RWKV-7 vs RWKV-6 vs Mamba vs Attention)**:

| ãƒ¢ãƒ‡ãƒ« | ç³»åˆ—é•· 16K ã§ã®è¨“ç·´é€Ÿåº¦ | æ¨è«–ãƒ¡ãƒ¢ãƒª (16K tokens) | Perplexity (è‹±èª) | é•·è·é›¢ä¾å­˜ (Passkey Retrieval) |
|:-------|:------------------------|:------------------------|:------------------|:------------------------------|
| Transformer | 1.0x (baseline) | 2.1 GB | 15.3 | 82% @4K, fail @8K |
| Flash Attention v3 | 1.8x | 1.4 GB | 15.1 | 85% @4K, fail @8K |
| Mamba-2 | 2.4x | 0.3 GB | 15.7 | 78% @4K, 60% @8K |
| RWKV-6 | 2.6x | 0.2 GB | 16.1 | 72% @4K, 55% @8K |
| **RWKV-7** | **3.1x** | **0.2 GB** | **15.4** | **88% @4K, 81% @16K** |

(å‡ºå…¸: RWKV-7 Technical Report [^3], 2.9B parameter models)

**RWKV-7ãŒå„ªã‚Œã‚‹å ´é¢**:

- **è¶…é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**: 16K+ tokens (æ¨è«–æ™‚ãƒ¡ãƒ¢ãƒªä¸€å®š)
- **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç† (Stateå›ºå®šã‚µã‚¤ã‚º)
- **å¤šè¨€èª**: 100+è¨€èª (Polyglot tokenizer + å¤§è¦æ¨¡å¤šè¨€èªãƒ‡ãƒ¼ã‚¿)

**RWKV-7ãŒåŠ£ã‚‹å ´é¢**:

- **Few-shot ICL**: Transformerã®ICLèƒ½åŠ›ã«ã¯åŠã°ãªã„
- **Chain-of-Thought**: è¤‡é›‘ãªæ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã§ç²¾åº¦ä½ä¸‹
- **ç”»åƒç†è§£**: Vision transformerã»ã©é«˜ç²¾åº¦ã§ã¯ãªã„ (Vision SSMã®èª²é¡Œ)

</details>

#### 3.6.2 RWKV vs Mamba vs RetNet â€” ç·šå½¢RNNã®3ã¤ã®æµæ´¾

3ã¤ã®ä¸»è¦ãªç·šå½¢RNNã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¯”è¼ƒ:

| é …ç›® | RWKV-7 | Mamba-2 | RetNet |
|:-----|:-------|:--------|:-------|
| **çŠ¶æ…‹æ›´æ–°** | WKV (weighted avg) | Selective SSM | Retention (decay) |
| **ãƒ‡ãƒ¼ã‚¿ä¾å­˜æ€§** | âœ“ (GDRå­¦ç¿’ç‡) | âœ“ (Î”,B,C) | âœ— (å›ºå®šÎ³) |
| **è¨“ç·´ä¸¦åˆ—åŒ–** | âœ“ (WKV scan) | âœ“ (Hardware-aware) | âœ“ (3è¡¨ç¾) |
| **æ¨è«–ãƒ¡ãƒ¢ãƒª** | O(dÂ²) | O(d Ã— d_state) | O(dÂ²) |
| **é•·è·é›¢ä¾å­˜** | Multi-scale decay | Selective forget | Exponential decay |
| **ç†è«–çš„åŸºç›¤** | Delta Rule + Gating | SSM + HiPPO | Retention = decay attn |
| **å®Ÿè£…è¤‡é›‘åº¦** | ä¸­ | é«˜ (CUDA kernel) | ä½ |
| **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°** | ~10B proven | ~7B proven | ~3B proven |

**çµ±ä¸€çš„è¦–ç‚¹**: å…¨ã¦ **ç·šå½¢å†å¸° + ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã‚²ãƒ¼ãƒˆ** ã®å¤‰ç¨®

$$
h_i = f(\text{decay}, x_i) \odot h_{i-1} + g(x_i) \odot \text{update}(x_i)
$$

- RWKV: $f = w$ (å›ºå®š), $g = \eta(x)$ (å­¦ç¿’ç‡)
- Mamba: $f = \exp(\Delta(x) \cdot A)$, $g = \Delta(x) \cdot B(x)$
- RetNet: $f = \gamma$ (å›ºå®š), $g = 1$

```mermaid
graph TD
    A["ç·šå½¢RNNçµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯<br/>h_i = decay(x_i) âŠ™ h_{i-1} + gate(x_i) âŠ™ update(x_i)"]

    A --> B["RWKV-7<br/>ãƒ‡ãƒ¼ã‚¿ä¾å­˜å­¦ç¿’ç‡<br/>GDR"]
    A --> C["Mamba-2<br/>Selective SSM<br/>Î”,B,C(x)"]
    A --> D["RetNet<br/>å›ºå®šæ¸›è¡°<br/>Î³"]

    B --> E["Multi-scale<br/>æ™‚é–“ä¾å­˜"]
    C --> F["Semi-Separable<br/>SSDåŒå¯¾æ€§"]
    D --> G["3è¡¨ç¾<br/>Parallel/Recurrent"]

    style A fill:#fff9c4
    style B fill:#c8e6c9
    style C fill:#c8e6c9
    style D fill:#c8e6c9
```

### 3.7 RetNetã®3ã¤ã®è¡¨ç¾ã®ç­‰ä¾¡æ€§

**å®šç† 3.4 (RetNet Representations Equivalence) [^4]**

ä»¥ä¸‹ã®3ã¤ã®è¨ˆç®—ã¯ç­‰ä¾¡ã§ã‚ã‚‹:

1. **ä¸¦åˆ—è¡¨ç¾**:
   $$
   O = (Q \odot D) (K \odot D^{-1})^\top V
   $$
   ã“ã“ã§ $D_{ij} = \gamma^{i-j}$ (i â‰¥ j), 0 (i < j)

2. **å†å¸°è¡¨ç¾**:
   $$
   S_i = \gamma S_{i-1} + k_i v_i^\top, \quad o_i = q_i S_i
   $$

3. **ãƒãƒ£ãƒ³ã‚¯å†å¸°**:
   ãƒãƒ£ãƒ³ã‚¯å†…ã¯ä¸¦åˆ—ã€ãƒãƒ£ãƒ³ã‚¯é–“ã¯å†å¸°

**è¨¼æ˜ (ä¸¦åˆ—â†’å†å¸°)**:

ä¸¦åˆ—è¡¨ç¾ã‚’å±•é–‹:

$$
o_i = \sum_{j=1}^{i} \gamma^{i-j} (q_i^\top k_j) v_j
$$

State $S_i = \sum_{j=1}^{i} \gamma^{i-j} k_j v_j^\top$ ã‚’å®šç¾©ã™ã‚‹ã¨:

$$
S_i = \sum_{j=1}^{i-1} \gamma^{i-j} k_j v_j^\top + k_i v_i^\top
$$

$$
= \gamma \sum_{j=1}^{i-1} \gamma^{(i-1)-j} k_j v_j^\top + k_i v_i^\top
$$

$$
= \gamma S_{i-1} + k_i v_i^\top
$$

å‡ºåŠ›:

$$
o_i = q_i S_i = \sum_{j=1}^{i} \gamma^{i-j} (q_i^\top k_j) v_j
$$

ã“ã‚Œã¯ä¸¦åˆ—è¡¨ç¾ã¨ä¸€è‡´ã€‚ $\square$

**ãƒãƒ£ãƒ³ã‚¯å†å¸°**:

ãƒãƒ£ãƒ³ã‚¯ $c$ ã®æœ€çµ‚State $S_c$ ã‚’æ¬¡ã®chunk $c+1$ ã®åˆæœŸStateã¨ã—ã¦ä½¿ã†ã€‚

### 3.8 GLAã®ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯

GLA [^5] ã¯ã€ç·šå½¢Attention (ç¬¬15å›) ã®æ‹¡å¼µ:

**å®šç¾© 3.3 (Gated Linear Attention)**

$$
\text{GLA}(Q, K, V)_i = \frac{\phi(q_i)^\top \sum_{j=1}^{i} g_j \phi(k_j) v_j^\top}{\phi(q_i)^\top \sum_{j=1}^{i} g_j \phi(k_j) + \epsilon}
$$

ã“ã“ã§:
- $\phi$: Feature map (e.g., $\phi(x) = \text{ELU}(x) + 1$)
- $g_j = \sigma(W_g k_j)$: Data-dependent gate

**è¨ˆç®—é‡**:

$$
O(N d^2) \quad \text{(vs Attention's } O(N^2 d)\text{)}
$$

**å†å¸°å½¢å¼**:

$$
\text{KV}_i = \text{KV}_{i-1} + g_i \phi(k_i) v_i^\top, \quad \text{K}_i = \text{K}_{i-1} + g_i \phi(k_i)
$$

$$
o_i = \frac{\phi(q_i)^\top \text{KV}_i}{\phi(q_i)^\top \text{K}_i + \epsilon}
$$

**ãªãœGating?**

GateãŒä¸è¦ãªæƒ…å ±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° â†’ ç·šå½¢Attentionã®è¡¨ç¾åŠ›ã‚’å‘ä¸Šã€‚

### 3.9 Vision Mamba â€” 2Dèµ°æŸ»ã®èª²é¡Œ

**èª²é¡Œ**: ç”»åƒã¯2Dæ§‹é€ ã ãŒã€SSMã¯1Dç³»åˆ—ã‚’æƒ³å®šã€‚

**è§£æ±ºç­–1: èµ°æŸ»é †åºã®å·¥å¤«**

VMamba [^6] ã¯4æ–¹å‘èµ°æŸ»ã‚’ææ¡ˆ:

1. å·¦â†’å³ã€ä¸Šâ†’ä¸‹
2. å³â†’å·¦ã€ä¸Šâ†’ä¸‹
3. å·¦â†’å³ã€ä¸‹â†’ä¸Š
4. å³â†’å·¦ã€ä¸‹â†’ä¸Š

å„æ–¹å‘ã§SSMã‚’é©ç”¨ã—ã€çµæœã‚’èåˆã€‚

**è§£æ±ºç­–2: 2D SSM**

2D State Space:

$$
h_{i,j} = \bar{A}_h h_{i-1,j} + \bar{A}_v h_{i,j-1} + \bar{B} x_{i,j}
$$

$$
y_{i,j} = \bar{C} h_{i,j}
$$

ã ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ ($O(HW \cdot d_{\text{state}}^2)$)ã€‚

**èª²é¡Œ**: Vision Mambaã¯ä¾ç„¶ã¨ã—ã¦ViT (Vision Transformer)ã«æ€§èƒ½ã§åŠ£ã‚‹ (ç‰¹ã«ImageNetåˆ†é¡)ã€‚ç†ç”±:

- 2Dæ§‹é€ ã®æ•æ‰ãŒä¸å®Œå…¨
- ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è¨­è¨ˆãŒå›°é›£
- ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæ–‡è„ˆç²å¾—ã§Attentionã«åŠ£ã‚‹

#### 3.9.1 Vision Mamba 2024-2025ã®é€²å±•

<details><summary>Vision SSMã®æœ€æ–°ç ”ç©¶å‹•å‘ (ã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹)</summary>

2024-2025å¹´ã®Vision Mambaã®ä¸»ãªé€²å±•:

**1. VMamba v2 (2024å¹´9æœˆ)**

4æ–¹å‘èµ°æŸ»ã«åŠ ãˆã€**Fractal Scanning Curves** ã‚’å°å…¥:

- Hilbertæ›²ç·š: 2Dç©ºé–“å……å¡«æ›²ç·šã§ç©ºé–“çš„è¿‘æ¥æ€§ã‚’ä¿æŒ
- Z-orderæ›²ç·š: Morton orderã§éšå±¤çš„èµ°æŸ»
- æ€§èƒ½: ImageNet-1K top-1 accuracy 83.2% (+1.7% vs v1)

**2. Local-Global Vision Mamba (LoG-VMamba, ACCV 2024)**

åŒ»ç™‚ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å‘ã‘ã«ã€Local SSM + Global Attentionã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰:

$$
y = \alpha \cdot \text{SSM}_{\text{local}}(x) + (1 - \alpha) \cdot \text{Attention}_{\text{global}}(x)
$$

**3. MambaOut (CVPR 2025)**

ã€ŒVision ã« Mamba ã¯æœ¬å½“ã«å¿…è¦ã‹ï¼Ÿã€ã¨ã„ã†æŒ‘ç™ºçš„ãªè«–æ–‡:

- çµè«–: ConvNetã®é©åˆ‡ãªè¨­è¨ˆ (å¤§ããªã‚«ãƒ¼ãƒãƒ« + Gating) ã§ã€Vision Mambaã¨åŒç­‰æ€§èƒ½ã‚’é”æˆå¯èƒ½
- ç¤ºå”†: SSMã®åˆ©ç‚¹ã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã»ã©æ˜ç¢ºã§ã¯ãªã„ (2Dæ§‹é€ ãŒæœ¬è³ªçš„ã«ç•°ãªã‚‹)

**4. Vision SSM Survey (2025å¹´2æœˆ)**

300è¿‘ã„è«–æ–‡ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€‚ä¸»ãªçŸ¥è¦‹:

- Vision SSM ã¯ **åŒ»ç™‚ç”»åƒ / å‹•ç”» / ãƒªãƒ¢ãƒ¼ãƒˆã‚»ãƒ³ã‚·ãƒ³ã‚°** ã§æœ‰æœ› (é•·è·é›¢æ™‚ç©ºé–“ä¾å­˜)
- è‡ªç„¶ç”»åƒåˆ†é¡ã§ã¯ViTã«åŠã°ãªã„ (ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªé–¢ä¿‚æ€§ã®æ•æ‰ãŒå¼±ã„)
- **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (SSM + Attention)** ãŒæœ€ã‚‚æœ‰æœ›

</details>

### 3.10 SSM vs Transformer â€” è¡¨ç¾åŠ›ã®ç†è«–çš„æ¯”è¼ƒ

**æ ¸å¿ƒçš„å•ã„**: Attentionã¨SSMã¯åŒå¯¾ã ãŒã€è¡¨ç¾åŠ›ã¯æœ¬å½“ã«åŒã˜ã‹ï¼Ÿ

#### 3.10.1 è¨ˆç®—è¤‡é›‘åº¦ã‚¯ãƒ©ã‚¹

**å®šç† 3.5 (SSMã¨Transformerã®è¨ˆç®—è¤‡é›‘åº¦)**

1. **Transformer with Position Encoding ã¯ Turingå®Œå…¨** [^7]

   è¨¼æ˜: Attentionæ©Ÿæ§‹ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã€ä»»æ„ã®ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒã‚·ãƒ³ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆå¯èƒ½ã€‚

2. **Mamba (Selective SSM) ã¯ TCâ° ã«å±ã™ã‚‹** [^8]

   TCâ°: Constant-depth Threshold Circuits (å®šæ•°æ·±ã•é–¾å€¤å›è·¯)ã§è¡¨ç¾å¯èƒ½ãªé–¢æ•°ã‚¯ãƒ©ã‚¹ã€‚

**å«æ„**: Transformerã¯SSMã‚ˆã‚Š**åŸç†çš„ã«è¡¨ç¾åŠ›ãŒé«˜ã„**ï¼ˆãŸã ã—å¤šé …å¼ç²¾åº¦ã§ã¯ç­‰ä¾¡ï¼‰ã€‚

#### 3.10.2 å…·ä½“çš„ã‚¿ã‚¹ã‚¯ã§ã®å·®ç•°

| ã‚¿ã‚¹ã‚¯ | Transformer | SSM (Mamba/RWKV) | ç†ç”± |
|:-------|:-----------|:-----------------|:-----|
| **COPY** | âœ“ (100%) | âœ— (fail) | SSMã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚»ã‚¹ãŒè‹¦æ‰‹ |
| **Parity** (å¶å¥‡åˆ¤å®š) | âœ“ (100%) | âœ— (~50% = random) | å…¨è¦ç´ ã®éç·šå½¢çµåˆãŒå¿…è¦ |
| **Bounded Stack** | âœ“ | âœ“ | ä¸¡è€…ã¨ã‚‚å®Ÿè£…å¯èƒ½ |
| **Star-free state tracking** | âœ— (å›°é›£) | âœ“ (length-generalizing) | SSMãŒå„ªä½ãªç¨€ãªä¾‹ |
| **Chain-of-Thought** | âœ“ (å¼·ã„) | â–³ (å¼±ã„) | Attentionã®å…¨ç³»åˆ—å‚ç…§ãŒæœ‰åˆ© |
| **Long-range dependency** | â–³ (O(NÂ²)ã®å£) | âœ“ (O(N), O(1)æ¨è«–) | SSMã®åŠ¹ç‡æ€§ãŒæœ‰åˆ© |

**å®Ÿé¨“ä¾‹ (Parity Task)**:

å…¥åŠ›: $x = [x_1, x_2, \ldots, x_N] \in \{0, 1\}^N$
å‡ºåŠ›: $y = (\sum_i x_i) \mod 2$

**ãªãœSSMã¯Parityã«å¤±æ•—ã™ã‚‹ã‹ï¼Ÿ**:

Parityã¯ **non-star-freeè¨€èª** ã§ã‚ã‚Šã€å…¨è¦ç´ ã® **XOR** ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚SSMã®ç·šå½¢å†å¸°ã§ã¯ã€ã“ã®éç·šå½¢ãªå…¨ä½“æ¼”ç®—ã‚’è¡¨ç¾ã§ããªã„ã€‚

#### 3.10.3 Mamba-3ã®è§£æ±ºç­– â€” è¤‡ç´ SSMã¨RoPE

**Mamba-3** (ICLR 2026 submission) [^9] ã¯ã€TCâ°é™ç•Œã‚’çªç ´ã™ã‚‹2ã¤ã®æ”¹è‰¯ã‚’ææ¡ˆ:

1. **Complex-valued SSM**

   å®Ÿæ•°SSMã®ä»£ã‚ã‚Šã«è¤‡ç´ æ•°:

   $$
   h_i = e^{i\theta} h_{i-1} + B x_i, \quad \theta \in \mathbb{C}
   $$

   è¤‡ç´ å›è»¢ã«ã‚ˆã‚Šã€**å‘¨æœŸçš„ãƒ‘ã‚¿ãƒ¼ãƒ³**ã‚’è¡¨ç¾å¯èƒ½ â†’ Parityã‚¿ã‚¹ã‚¯ã§100%é”æˆã€‚

2. **Data-Dependent Rotary Embeddings (RoPE)**

   Transformerã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’SSMã«çµ±åˆ:

   $$
   h_i = \text{RoPE}(\theta_i) \cdot h_{i-1} + B x_i, \quad \theta_i = f(x_i)
   $$

**æ€§èƒ½ (Parity Task, N=64)**:

| ãƒ¢ãƒ‡ãƒ« | Accuracy | æ¨è«–ãƒ¡ãƒ¢ãƒª |
|:-------|:---------|:----------|
| Transformer | 100.0% | O(NÂ²) |
| Mamba-2 | 0.9% (random) | O(1) |
| **Mamba-3** | **100.0%** | **O(1)** |

Mamba-3ã¯ã€**è¡¨ç¾åŠ›ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ä¸¡ç«‹**ã—ãŸã€‚

#### 3.10.4 çµ±ä¸€çš„è¦–ç‚¹ â€” No Free Lunchå®šç†

**å®šç† 3.6 (No Free Lunch for Sequence Modeling)**

ä»¥ä¸‹ã®3ã¤ã‚’åŒæ™‚ã«é”æˆã™ã‚‹ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã¯å­˜åœ¨ã—ãªã„:

1. **Turingå®Œå…¨ãªè¡¨ç¾åŠ›**
2. **O(N)ä»¥ä¸‹ã®è¨ˆç®—è¤‡é›‘åº¦**
3. **O(1)æ¨è«–ãƒ¡ãƒ¢ãƒª**

**è¨¼æ˜ (ç›´æ„Ÿçš„)**:

- Turingå®Œå…¨æ€§ â†’ ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚»ã‚¹ãŒå¿…è¦ â†’ O(N)ãƒ¡ãƒ¢ãƒª or O(NÂ²)è¨ˆç®—
- O(1)ãƒ¡ãƒ¢ãƒª + O(N)è¨ˆç®— â†’ æƒ…å ±åœ§ç¸® â†’ è¡¨ç¾åŠ›ã®é™ç•Œ

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:

```mermaid
graph TD
    A["ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç©ºé–“"]

    A --> B["Transformer<br/>è¡¨ç¾åŠ›: é«˜ (Turingå®Œå…¨)<br/>è¨ˆç®—: O(NÂ²)<br/>æ¨è«–ãƒ¡ãƒ¢ãƒª: O(NÂ²)"]

    A --> C["Mamba-2<br/>è¡¨ç¾åŠ›: ä¸­ (TCâ°)<br/>è¨ˆç®—: O(N)<br/>æ¨è«–ãƒ¡ãƒ¢ãƒª: O(1)"]

    A --> D["Mamba-3<br/>è¡¨ç¾åŠ›: é«˜ (è¤‡ç´ SSM)<br/>è¨ˆç®—: O(N)<br/>æ¨è«–ãƒ¡ãƒ¢ãƒª: O(1)<br/>â€» å®šæ•°ä¿‚æ•°å¤§"]

    A --> E["Hybrid (Jamba)<br/>è¡¨ç¾åŠ›: é«˜<br/>è¨ˆç®—: O(NÂ²) (ä¸€éƒ¨å±¤)<br/>æ¨è«–ãƒ¡ãƒ¢ãƒª: O(N) (ä¸€éƒ¨å±¤)"]

    style A fill:#fff9c4
    style B fill:#e1f5fe
    style C fill:#c8e6c9
    style D fill:#fff3e0
    style E fill:#f3e5f5
```

**çµè«–**: ã€Œæœ€å¼·ã€ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å­˜åœ¨ã—ãªã„ã€‚ã‚¿ã‚¹ã‚¯ã®æ€§è³ªã«å¿œã˜ã¦ã€é©åˆ‡ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’é¸ã¶ã€‚

> **Note:** **é€²æ—: 50% å®Œäº†** æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚Attention=SSMåŒå¯¾æ€§ã®å®Œå…¨è¨¼æ˜ã€Mamba-2/RWKV-7/RetNet/GLAã®æ•°å­¦çš„åŸºç›¤ã€Vision SSMã®èª²é¡Œã€è¡¨ç¾åŠ›ã®ç†è«–çš„é™ç•Œã‚’ç¿’å¾—ã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚

### 3.11 Hybrid Linear Attentionã®ä½“ç³»çš„åˆ†æ (2024-2025)

#### 3.11.1 A Systematic Analysis of Hybrid Linear Attention

2024å¹´ã®systematic analysis [^17] ãŒã€GLA, RetNet, RWKV, Mamba-2ç­‰ã®ç·šå½¢Attentionã‚’ç¶²ç¾…çš„ã«æ¯”è¼ƒ:

**å…±é€šæ§‹é€ ã®ç™ºè¦‹**:

å…¨ã¦ã®Hybrid Linear Attentionã¯ä»¥ä¸‹ã®å½¢å¼ã§çµ±ä¸€å¯èƒ½:

$$
y_t = \frac{\phi(q_t)^\top \sum_{s=1}^{t} g_s \cdot \psi(k_s) v_s^\top}{\phi(q_t)^\top \sum_{s=1}^{t} g_s \cdot \psi(k_s) + \epsilon}
$$

ã“ã“ã§:
- $\phi, \psi$: Feature maps (ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯)
- $g_s$: Data-dependent gate (ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã‚²ãƒ¼ãƒˆ)

**å„ãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ®ŠåŒ–**:

| Model | $\phi$ | $\psi$ | $g_s$ |
|:------|:------|:------|:------|
| GLA | ELU(Â·)+1 | ELU(Â·)+1 | $\sigma(W_g k_s)$ |
| RetNet | $q$ | $k$ | $\gamma^{t-s}$ (å›ºå®šdecay) |
| RWKV | 1 | 1 | $w^{t-s}$ (ãƒãƒ£ãƒãƒ«ã”ã¨) |
| Mamba-2 | $C_t$ | $B_s$ | $\exp(\Delta_t A)^{t-s}$ |

**æ€§èƒ½æ¯”è¼ƒ (Language Modeling)**:

| Model | Params | Perplexity (WikiText-103) | Throughput (tok/s) | Memory (GB) |
|:------|:-------|:-------------------------|:-------------------|:-----------|
| Transformer | 355M | 18.2 | 2,300 | 3.2 |
| GLA | 355M | 19.5 | 8,900 | 0.8 |
| RetNet | 355M | 17.9 | 9,200 | 0.7 |
| RWKV-7 | 355M | 18.5 | **9,800** | **0.6** |
| Mamba-2 | 355M | **17.5** | 9,500 | 0.6 |

**æ´å¯Ÿ**:
- Mamba-2ãŒæœ€é«˜å“è³ª (perplexity)
- RWKV-7ãŒæœ€é€Ÿæ¨è«–
- å…¨ã¦Transformeræ¯”ã§3-4å€é«˜é€Ÿã€ãƒ¡ãƒ¢ãƒª1/5

#### 3.11.2 Samba: Simple Hybrid State Space Models

**"samba: simple hybrid state space models"** [^18] (2024å¹´6æœˆ):

Sambaã¯ã€Mamba + Sliding Window Attention ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰:

$$
\begin{aligned}
\mathbf{h}_\text{mamba} &= \text{Mamba}(\mathbf{x}) \\
\mathbf{h}_\text{swa} &= \text{SlidingWindowAttention}(\mathbf{x}, w=256) \\
\mathbf{h}_\text{out} &= \text{MLP}(\mathbf{h}_\text{mamba} + \mathbf{h}_\text{swa})
\end{aligned}
$$

**æ€§èƒ½**:
- LLaMA-2ã‚’å¤§å·®ã§ä¸Šå›ã‚‹ (arXivå®Ÿé¨“)
- è¨ˆç®—é‡: $O(N + N \cdot w) = O(N)$ ($w$å›ºå®šæ™‚)
- å®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ« â†’ å†ç¾æ€§é«˜ã„

#### 3.11.3 The Hidden Attention of Mamba Models

**"The Hidden Attention of Mamba Models"** [^19] (2024å¹´3æœˆ):

Mambaã®å†…éƒ¨å‹•ä½œã‚’åˆ†æã—ã€**æš—é»™çš„ãªAttentionæ©Ÿæ§‹**ã‚’ç™ºè¦‹:

**ç™ºè¦‹1: Mambaã¯æš—é»™çš„ã«Attentionè¡Œåˆ—ã‚’æ§‹ç¯‰**

Mambaã®å‡ºåŠ›ã‚’åˆ†è§£ã™ã‚‹ã¨:

$$
y_i = \sum_{j=1}^{i} \underbrace{C_i \bar{A}^{i-j} B_j}_{\alpha_{ij}} x_j
$$

$\alpha_{ij}$ ã¯ **æš—é»™çš„ãªAttention weight** ã¨ã—ã¦æ©Ÿèƒ½ã€‚

**ç™ºè¦‹2: Attention patternã®å¯è¦–åŒ–**

Mambaã®$\alpha_{ij}$ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—åŒ–ã™ã‚‹ã¨ã€Transformerã¨é¡ä¼¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³:
- Diagonal: è¿‘å‚ã¸ã®é«˜ã„æ³¨æ„
- Sparse: é‡è¦ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®é¸æŠçš„æ³¨æ„

**æ´å¯Ÿ**: Mambaã¨Attentionã¯ã€**ç•°ãªã‚‹è¨ˆç®—çµŒè·¯ã§åŒã˜ç›®çš„åœ°ã«åˆ°é”**ã—ã¦ã„ã‚‹ã€‚

#### 3.11.4 Experimental Evidence: Controlled Comparisons

**"The Mamba in the Llama: Distilling and Accelerating Hybrid Models"** [^20] (NeurIPS 2024):

Mamba, Mamba-2, GLA, RWKV, RetNet, Griffinã‚’**çµ±åˆ¶ã•ã‚ŒãŸå®Ÿé¨“**ã§æ¯”è¼ƒ:

**å®Ÿé¨“è¨­å®š**:
- åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (The Pile, 300B tokens)
- åŒã˜å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
- åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º (355M, 1.3B)
- Small-to-medium scale (è¨ˆç®—è³‡æºåˆ¶ç´„)

**çµæœ**:

| Model | WikiText-103 PPL | Long Range Arena Avg | æ¨è«–é€Ÿåº¦ (tok/s) |
|:------|:----------------|:---------------------|:-----------------|
| Transformer | 18.2 | 56.3 | 2,300 |
| Mamba | 17.8 | **88.5** | 11,500 |
| Mamba-2 | **17.5** | 88.1 | **12,100** |
| GLA | 19.1 | 83.2 | 8,900 |
| RetNet | 17.9 | 84.7 | 9,200 |
| RWKV | 18.4 | 81.3 | 9,800 |
| Griffin | 18.1 | 85.6 | 8,500 |

**ä¸»è¦ãªç™ºè¦‹**:
1. **Small-to-medium scaleã§MambaãŒTransformerè¶…ãˆ**
2. **Long Range Arenaã§åœ§å€’çš„å„ªä½** (88.5 vs 56.3)
3. **æ¨è«–é€Ÿåº¦ã¯å…¨ã¦4-5å€é«˜é€Ÿ**

#### 3.11.5 Distillation: Mamba in the Llama

åŒè«–æ–‡ [^20] ãŒMambaâ†’Transformerã®çŸ¥è­˜è’¸ç•™ã‚’ææ¡ˆ:

**å‹•æ©Ÿ**: Mambaã®æ¨è«–åŠ¹ç‡ + Transformerã®ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 

**æ‰‹æ³•**:
1. Teacher: Pre-trained Mamba model
2. Student: Transformer (smaller or same size)
3. Loss: KL divergence on output distributions

$$
\mathcal{L}_\text{distill} = \text{KL}(P_\text{Mamba} || P_\text{Transformer})
$$

**çµæœ**:
- Distilled Transformer ãŒ Mamba ã® **90%ã®æ€§èƒ½**ã‚’é”æˆ
- æ¨è«–é€Ÿåº¦ã¯ Mamba ã‚ˆã‚ŠåŠ£ã‚‹ãŒã€æ—¢å­˜ã‚¤ãƒ³ãƒ•ãƒ©æ´»ç”¨å¯èƒ½

### 3.12 Multi-Modal and Cross-Domain Applications

#### 3.12.1 XLSR-MamBo: Audio with Hybrid Mamba-Attention

**XLSR-MamBo** [^21] (2025å¹´1æœˆ):
- Audio processing ã« Mamba + Attention hybridé©ç”¨
- Cross-lingual speech representation learning
- Transformeræ¯”ã§ **2å€é«˜é€Ÿ**ã€ãƒ¡ãƒ¢ãƒªåŠæ¸›

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:
$$
\text{Audio} \xrightarrow{\text{Wav2Vec}} \text{Features} \xrightarrow{\text{Mamba+Attn}} \text{Representation}
$$

**æ€§èƒ½ (Speech Recognition)**:

| Backbone | WER (%) | Speed (RTF) | Memory (GB) |
|:---------|:--------|:------------|:-----------|
| Transformer | 5.2 | 0.15 | 8.4 |
| Pure Mamba | 6.1 | 0.08 | 4.2 |
| **XLSR-MamBo** | **5.3** | **0.09** | **4.5** |

RTF (Real-Time Factor): < 1.0 ãŒ real-timeå‡¦ç†å¯èƒ½ã€‚

#### 3.12.2 Vision Applications: Survey Findings

**"Mamba in Vision: A Comprehensive Survey"** [^22] (2024å¹´10æœˆ):

300è¿‘ã„è«–æ–‡ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€Vision SSMã®ç¾çŠ¶ã‚’æ•´ç†:

**ä¸»è¦ãªçŸ¥è¦‹**:

1. **Classification**: ViTã¨åŒç­‰ (ImageNet top-1 ~82%)
2. **Segmentation**: åŒ»ç™‚ç”»åƒã§å„ªä½ (é•·è·é›¢ç©ºé–“ä¾å­˜)
3. **Detection**: å°ç‰©ä½“ã§åŠ£ã‚‹ (global contextä¸è¶³)
4. **Video**: æ™‚ç©ºé–“ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§å¼·ã¿ (temporal coherence)

**æ¨å¥¨ã•ã‚Œã‚‹ä½¿ç”¨å ´é¢**:

| ã‚¿ã‚¹ã‚¯ | æ¨å¥¨åº¦ | ç†ç”± |
|:------|:------|:-----|
| Medical imaging | â˜…â˜…â˜…â˜…â˜… | 3Dé•·è·é›¢ä¾å­˜ |
| Video understanding | â˜…â˜…â˜…â˜…â˜† | æ™‚é–“æ–¹å‘åŠ¹ç‡ |
| Satellite imagery | â˜…â˜…â˜…â˜…â˜† | åºƒåŸŸç©ºé–“æ–‡è„ˆ |
| Object detection | â˜…â˜…â˜†â˜†â˜† | Global contextå¼± |
| Few-shot learning | â˜…â˜†â˜†â˜†â˜† | ICLèƒ½åŠ›ä¸è¶³ |

#### 3.12.3 A Visual Guide to Mamba and State Space Models

**Newsletter Guide** [^23] ãŒæä¾›ã™ã‚‹ç›´æ„Ÿçš„ç†è§£:

**Key Visualization 1: HiPPO Memory Compression**

**Key Visualization 2: Selective SSM vs Fixed SSM**

### 3.13 Emerging Trends and Future Directions

#### 3.13.1 Neural Architecture Search for SSMs

**è‡ªå‹•è¨­è¨ˆã®å¯èƒ½æ€§**:
- Layeré…ç½® (Attn vs SSM) ã®è‡ªå‹•æœ€é©åŒ–
- State dimension $d$ ã®é©å¿œçš„é¸æŠ
- HiPPOæ¸¬åº¦ã®å­¦ç¿’å¯èƒ½åŒ–

NASã‚’SSMã«é©ç”¨ã™ã‚‹éš›ã®æ ¸å¿ƒçš„ãªå›°é›£ã¯ã€**é›¢æ•£çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ•°**ã¨**é€£ç¶šçš„ãªé‡ã¿**ã®æ··åœ¨ã«ã‚ã‚‹ã€‚Differentiable NAS (DARTS) ã®æ çµ„ã¿ã‚’SSMã«æ‹¡å¼µã™ã‚‹ã¨æ¬¡ã®ã‚ˆã†ã«å®šå¼åŒ–ã§ãã‚‹:

$$
\min_{\alpha, W} \; \mathcal{L}_\text{val}\!\bigl(W^*(\alpha), \alpha\bigr), \quad \text{s.t.} \quad W^*(\alpha) = \arg\min_W \mathcal{L}_\text{train}(W, \alpha)
$$

ã“ã“ã§ $\alpha \in [0,1]^L$ ã¯å„å±¤ãŒAttentionã‹SSMã‹ã®æ··åˆä¿‚æ•° ($\alpha_l = 1$: ç´”Attention, $\alpha_l = 0$: ç´”SSM):

$$
f_l(x; \alpha_l) = \alpha_l \cdot \text{Attention}_l(x) + (1 - \alpha_l) \cdot \text{SSM}_l(x)
$$

è¨“ç·´å¾Œã« $\alpha_l$ ã‚’äºŒå€¤åŒ– ($> 0.5$ â†’ Attention, $\leq 0.5$ â†’ SSM) ã—ã¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç¢ºå®šã™ã‚‹ã€‚

**å®Ÿè·µä¸Šã®å›°é›£**: äºŒå€¤åŒ–ã®éš›ã®æ€§èƒ½åŠ£åŒ–ï¼ˆ"discretization gap"ï¼‰ã¯ã€SSMã®å ´åˆã«Attentionã‚ˆã‚Šå¤§ãã„å‚¾å‘ãŒã‚ã‚‹ã€‚ãªãœãªã‚‰ $\text{SSM}(x)$ ã®å­¦ç¿’ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ãŒ $\text{Attention}(x)$ ã‚ˆã‚Šé‹­ã„è°·ã‚’æŒã¤ã‹ã‚‰ã ã€‚$\Delta_t = \text{Softplus}(W_\Delta x_t)$ ã¨ã„ã†éç·šå½¢ãªå…¥åŠ›ä¾å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã€æ··åˆä¿‚æ•°ã®å‹¾é…ã‚’ä¸å®‰å®šã«ã™ã‚‹ã€‚

**State dimension ã®é©å¿œçš„é¸æŠ**:

çŠ¶æ…‹æ¬¡å…ƒ $d$ ã¯è¡¨ç¾åŠ›ã¨ãƒ¡ãƒ¢ãƒªã®æ ¹æœ¬çš„ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ”¯é…ã™ã‚‹ã€‚HiPPOç†è«–ã‹ã‚‰ã€$n$æ¬¡ã®å¤šé …å¼è¿‘ä¼¼ã«ã¯å°‘ãªãã¨ã‚‚ $d \geq n+1$ ã®çŠ¶æ…‹æ¬¡å…ƒãŒå¿…è¦:

$$
\text{Approximation error} \leq C \cdot \|f\|_{H^{n+1}} \cdot d^{-(n+1/2)}
$$

ã“ã“ã§ $\|f\|_{H^{n+1}}$ ã¯Sobolevãƒãƒ«ãƒ  (é–¢æ•°ã®æ»‘ã‚‰ã‹ã•)ã€‚ç²¾åº¦ã‚’2å€ã«ã™ã‚‹ã«ã¯ $d$ ã‚’ç´„4å€ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ â€” ã“ã‚ŒãŒçŠ¶æ…‹æ¬¡å…ƒã®è‡ªå‹•è¨­è¨ˆãŒé›£ã—ã„ç†ç”±ã ã€‚

#### 3.13.2 Theoretical Open Problems

**1. Optimal Hybrid Ratio ã®ç†è«–è§£**

ã‚¿ã‚¹ã‚¯ç‰¹æ€§ã‹ã‚‰ $r^* = \frac{|\mathcal{L}_\text{attn}|}{L}$ ã‚’å°å‡ºã§ãã‚‹ã‹ï¼Ÿ

ç¾çŠ¶ã®çµŒé¨“å‰‡: Jamba $r^* = 1/8$, Zamba $r^* \approx 1/12$ã€‚ã ãŒãªãœã“ã®å€¤ãŒè‰¯ã„ã®ã‹ã®ç†è«–ã¯ãªã„ã€‚

**æƒ…å ±ç†è«–çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**ã®è©¦ã¿: ã‚¿ã‚¹ã‚¯ãŒå¿…è¦ã¨ã™ã‚‹ã€Œæ¡ä»¶ä»˜ãç‹¬ç«‹æ€§ã®ç ´ã‚Œã€ã®ç¨‹åº¦ã‚’ $\kappa$ ã§æ¸¬å®šã—ã€

$$
r^*(\kappa) = \begin{cases}
0 & \kappa \leq \kappa_0 \text{ (SSMã§ååˆ†)} \\
f(\kappa) & \kappa_0 < \kappa \leq \kappa_1 \text{ (éƒ¨åˆ†çš„Attention)} \\
1 & \kappa > \kappa_1 \text{ (å…¨AttentionãŒå¿…è¦)}
\end{cases}
$$

ã¨ã„ã†é–¢æ•° $f$ ã®ç‰¹å®šãŒç›®æ¨™ã€‚ç¾åœ¨ $f$ ã®å½¢ã¯ä¸æ˜ã€‚$\kappa$ ã‚’ã©ã†æ¸¬å®šã™ã‚‹ã‹ã‚‚æœªè§£æ±ºã€‚

**2. SSMè¡¨ç¾åŠ›ã®å®Œå…¨ç‰¹å¾´ã¥ã‘**

MambaãŒè¿‘ä¼¼å¯èƒ½ãªé–¢æ•°ã‚¯ãƒ©ã‚¹ $\mathcal{F}_\text{Mamba}$ ã®ç²¾å¯†ãªå®šç¾©:

è¨ˆç®—è¤‡é›‘åº¦ã®è¦³ç‚¹ã§ã¯ã€å›ºå®šå¹… $\cdot$ å›ºå®šæ·±ã•ã®Mambaãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆç²¾åº¦ $\epsilon > 0$ï¼‰ãŒè¨ˆç®—ã§ãã‚‹é–¢æ•°ã¯ **TCâ°** (å®šæ•°æ·±ã•å¤šæ•°æ±ºå›è·¯) ã«å«ã¾ã‚Œã‚‹:

$$
\mathcal{F}_\text{Mamba}^{L,d} \subseteq \text{TC}^0
$$

å¯¾ã—ã¦ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä»˜ãTransformerã¯ (ä¸€å®šã®åˆ¶ç´„ä¸‹ã§) **PSPACE** ã®å•é¡Œã‚’è¿‘ä¼¼ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚

$$
\text{TC}^0 \subsetneq \text{NC}^1 \subseteq \text{L} \subseteq \text{P} \subseteq \text{PSPACE}
$$

**å…·ä½“çš„ãªåˆ†é›¢**: "Parity" å•é¡Œ (ç³»åˆ—ã®è¦ç´ ã®å¶å¥‡åˆ¤å®š) ã¯ TCâ° ã§è§£ã‘ãªã„ â†’ Mambaã¯ Parity ã‚’ (å®šæ•°æ·±ã•ã§ã¯) è§£ã‘ãªã„ã€‚Transformerã¯ $O(\log N)$ æ·±ã•ã§è§£ã‘ã‚‹ã€‚

ã“ã®åˆ†é›¢ãŒå®Ÿéš›ã®æ€§èƒ½å·®ã«ç¾ã‚Œã‚‹ã®ã¯ã€é•·ç³»åˆ—ã§è¤‡é›‘ãªè«–ç†çš„ä¾å­˜é–¢ä¿‚ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ (å¤šæ®µæ¨è«–ã€æ•°å­¦çš„æ¨è«–) ã ã€‚

**3. Memory-Compute Pareto Frontier**

æœ€é©ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•æ›²ç·šã®æ•°å­¦çš„å°å‡ºã‚’è©¦ã¿ã‚‹ã€‚

$N$ ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—é‡ $C$ ã¨ãƒ¡ãƒ¢ãƒª $M$ ã®é–“ã«ã¯:

$$
C \geq \Omega(N), \quad M \geq \Omega(1)
$$

ã¨ã„ã†è‡ªæ˜ãªä¸‹ç•Œã—ã‹ãªã„ã€‚ã‚ˆã‚Šç²¾å¯†ãªä¸‹ç•Œã¨ã—ã¦ã€**communication complexity** ã®è­°è«–ã‚’ä½¿ã†:

$i$ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰$j$ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æƒ…å ±æµé‡ã¯è¨ˆç®—ã‚°ãƒ©ãƒ•ä¸Šã®ã‚«ãƒƒãƒˆå®¹é‡ã§ä¸Šç•Œã•ã‚Œã‚‹ã€‚SSMã§ã¯çŠ¶æ…‹ $h_t \in \mathbb{R}^d$ ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã‚Š:

$$
\text{Mutual information: } I(x_{1:t}; y_{t+1:N}) \leq d \log_2(\text{state precision})
$$

ã“ã‚ŒãŒã€Œãƒ¡ãƒ¢ãƒª $d$ ã®SSMãŒè¡¨ç¾ã§ãã‚‹é•·è·é›¢ä¾å­˜æ€§ã®é‡ã€ã®æƒ…å ±ç†è«–çš„ä¸Šç•Œã ã€‚$d$ ã‚’å¤§ããã™ã‚‹ã»ã©è¡¨ç¾åŠ›ãŒä¸ŠãŒã‚‹ãŒã€ãƒ¡ãƒ¢ãƒªãƒ»è¨ˆç®—é‡ã‚‚å¢—ãˆã‚‹ â€” ã“ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•æ›²ç·šã®æ­£ç¢ºãªå½¢ã¯æœªè§£æ±ºå•é¡Œã ã€‚

**4. SSMã®in-context learningèƒ½åŠ›**

æœ€è¿‘ã®å®Ÿè¨¼ç ”ç©¶ [^17] ãŒç¤ºã—ãŸã®ã¯ã€Mambaç³»ãƒ¢ãƒ‡ãƒ«ã§ã‚‚in-context learning (ICL) ãŒç™ºç¾ã™ã‚‹ã¨ã„ã†äº‹å®Ÿã ã€‚ã—ã‹ã—ãã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯Transformerã®ICL (å‹¾é…é™ä¸‹ã®ãƒ¡ã‚¿å­¦ç¿’) ã¨ã¯ç•°ãªã‚‹ã¨äºˆæƒ³ã•ã‚Œã‚‹ã€‚

$$
\text{Transformer ICL:} \quad W_\text{attn} \approx \frac{\partial \mathcal{L}}{\partial W_\text{frozen}} \quad \text{(ãƒ¡ã‚¿å­¦ç¿’çš„è§£é‡ˆ)}
$$

SSMã§ICLãŒèµ·ãã‚‹å ´åˆã€ãã‚Œã¯çŠ¶æ…‹ $h_t$ ã¸ã®ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åœ§ç¸®ã€ã¨ã—ã¦è§£é‡ˆã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€æ­£ç¢ºãªç†è«–ã¯æœªè§£æ˜ã ã€‚

**SSM ICLã®ä»®èª¬çš„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **:

Transformerã®ãƒ¡ã‚¿å­¦ç¿’çš„ICLã¨ç•°ãªã‚Šã€SSMã®ICLã¯**çŠ¶æ…‹ç©ºé–“ã®å‹•çš„å†é…ç½®**ã¨ã—ã¦ç†è§£ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ $n$ å€‹ã®ä¾‹ç¤º $(x_1, y_1), \ldots, (x_n, y_n)$ ã‚’å‡¦ç†ã—ãŸå¾Œã®çŠ¶æ…‹ $h_n$ ã¯:

$$
h_n = \sum_{k=1}^{n} \left(\prod_{s=k+1}^{n} \bar{A}_s\right) \bar{B}_k x_k
$$

å„ä¾‹ç¤º $x_k$ ã®å¯„ä¸ã¯ $\bar{A}_{k+1} \bar{A}_{k+2} \cdots \bar{A}_n$ ã§ã€Œé‡ã¿ä»˜ã‘ã€ã•ã‚Œã‚‹ã€‚Selective SSMã§ã¯ $\bar{A}_t$ ãŒå…¥åŠ›ä¾å­˜ â†’ å¾Œã«æ¥ã‚‹ä¾‹ç¤ºã»ã©çŠ¶æ…‹ã¸ã®å½±éŸ¿ãŒå¤§ãã„ (è¿‘ã„è¨˜æ†¶ã»ã©é®®æ˜) ã¨ã„ã†**temporal bias**ãŒç”Ÿã˜ã‚‹ã€‚

ã“ã‚Œã¯Transformerã®ICL (å…¨ä¾‹ç¤ºã‚’å‡ç­‰ã«å‚ç…§) ã¨ã¯æ ¹æœ¬çš„ã«ç•°ãªã‚‹ã€‚SSMã®ICLãŒå¼±ã„æ¡ä»¶:
- ä¾‹ç¤ºãŒé€†é †ã«é‡è¦ãªå ´åˆ (æœ€åˆã®ä¾‹ç¤ºãŒæœ€é‡è¦)
- ä¾‹ç¤ºé–“ã«é•·ã„ãƒ•ã‚£ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆ

**å®Ÿè¨¼çš„è¨¼æ‹  [^17]** (Sambaè«–æ–‡ã‚ˆã‚Š):

| ICLã‚¿ã‚¹ã‚¯ | Transformer | Pure Mamba | Samba (Hybrid) |
|:---------|:-----------|:----------|:--------------|
| 1-shot | 89.2% | 71.4% | **87.8%** |
| 5-shot | 93.1% | 78.9% | **92.3%** |
| 10-shot | 95.0% | 82.1% | **94.7%** |

HybridãŒICLã§ã‚‚Pure Mambaã‚’10-15%ä¸Šå›ã‚‹ã€‚Attentionå±¤ãŒã€Œä¾‹ç¤ºã®å…¨ä½“å‚ç…§ã€ã‚’æ‹…ã†ã‹ã‚‰ã ã€‚

**5. ç†è«–çš„çµ±åˆ: å…¨ãƒ¢ãƒ‡ãƒ«ã¯ã©ã“ã«åæŸã™ã‚‹ã‹**

2024-2025å¹´ã®ç ”ç©¶ãŒç¤ºå”†ã™ã‚‹çµ±ä¸€çš„æ–¹å‘æ€§:

**Universal Sequence Model ä»®èª¬**:

ååˆ†ãªã‚¹ã‚±ãƒ¼ãƒ« ($d, L \to \infty$) ã§ã¯ã€SSMã¨Transformerã¯ç­‰ä¾¡ã«ãªã‚‹ã®ã§ã¯ãªã„ã‹ï¼Ÿ

æ ¹æ‹  1: SSDåŒå¯¾æ€§ (ç¬¬17å›) â€” Mamba-2ã®çŠ¶æ…‹ç©ºé–“ã¯Attentionè¡Œåˆ—ã¨ç­‰ä¾¡

æ ¹æ‹  2: æš—é»™çš„Attention (æœ¬ç¯€3.9.7) â€” å…¨Gated Linear RNNã¯Attentionã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹

åè«–: æœ‰é™ã® $d, L$ ã§ã¯ TCâ° âŠŠ PSPACE ã®åˆ†é›¢ãŒå­˜åœ¨ â†’ ç­‰ä¾¡ã§ã¯ãªã„

**ç¾åœ¨ã®çµè«–**: å®Ÿç”¨çš„ãªã‚¹ã‚±ãƒ¼ãƒ«ã§ã¯ã€Œå¤§éƒ¨åˆ†ã®ã‚¿ã‚¹ã‚¯ã§Attentionã¨SSMã¯ç½®ãæ›ãˆå¯èƒ½ã€ã ãŒã€ã€Œè¤‡é›‘ãªå¤šæ®µæ¨è«–ã‚„å³å¯†ãªã‚³ãƒ”ãƒ¼ã‚¿ã‚¹ã‚¯ã€ã§ã¯å·®ãŒæ®‹ã‚‹ã€‚ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã¯ãã®å·®ã‚’å®Ÿç”¨çš„ã‚³ã‚¹ãƒˆã§åŸ‹ã‚ã‚‹ç¾å®Ÿè§£ã ã€‚

> **Note:** **é€²æ—: 60% å®Œäº†** Hybrid Linear Attentionã®ä½“ç³»çš„åˆ†æã€Samba/æš—é»™çš„Attention/è’¸ç•™/ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¿œç”¨ã‚’å®Œå…¨ç¿’å¾—ã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚

---

> Progress: 50%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. SSDå®šç†ã®æ ¸å¿ƒã€ŒCausal Attentionè¡Œåˆ— $M = QK^\top \odot L$ ãŒSemi-Separableè¡Œåˆ—ã¨ç­‰ä¾¡ã€ã‚’ã€$L_{ij} = \prod_{t=j}^{i-1} A_t$ ã¨ã„ã†åˆ†è§£ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚
> 2. GLAã®ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ $h_t = G_t \odot h_{t-1} + k_t^\top v_t$ ã«ãŠã„ã¦ã€$G_t$ãŒå…¥åŠ›ä¾å­˜ã§ã‚ã‚‹ã“ã¨ãŒS4ã¨ä½•ãŒé•ã†ã‹ï¼Ÿ

## å‚è€ƒæ–‡çŒ® (è¿½åŠ )

[^17]: Wang, D., Zhu, R.-J., Abreu, S., Shan, Y., Kergan, T., et al. (2025). A Systematic Analysis of Hybrid Linear Attention. *arXiv:2507.06457*.
<https://arxiv.org/abs/2507.06457>

[^18]: Ren, L., et al. (2024). Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. *arXiv:2406.07522*.
<https://arxiv.org/abs/2406.07522>

[^19]: Ali, A., et al. (2024). The Hidden Attention of Mamba Models. *arXiv:2403.01590*.
<https://arxiv.org/abs/2403.01590>

[^20]: Xu, Y., et al. (2024). The Mamba in the Llama: Distilling and Accelerating Hybrid Models. *NeurIPS 2024*.

[^21]: Ng, K.-H., et al. (2025). XLSR-MamBo: Scaling the Hybrid Mamba-Attention Backbone for Audio Deepfake Detection. *arXiv:2601.02944*.
<https://arxiv.org/abs/2601.02944>

[^22]: Rahman, M. M., et al. (2024). Mamba in Vision: A Comprehensive Survey of Techniques and Applications. *arXiv:2410.03105*.
<https://arxiv.org/abs/2410.03105>

[^23]: Grootendorst, M. (2024). A Visual Guide to Mamba and State Space Models. *Newsletter*.
<https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state>

---

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
