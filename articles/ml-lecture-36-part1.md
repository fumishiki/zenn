---
title: "ç¬¬36å›: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«åŸºç¤ / DDPM & ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼""
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning", "deeplearning", "ddpm", "julia", "diffusion"]
published: true
---

# ç¬¬36å›: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«åŸºç¤ / DDPM & ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â€” ãƒã‚¤ã‚ºé™¤å»ã®åå¾©ãŒç”Ÿæˆã‚’å®Ÿç¾ã™ã‚‹

> **ãƒã‚¤ã‚ºã‚’ã‚†ã£ãã‚ŠåŠ ãˆã€é€†ã«ã‚†ã£ãã‚Šé™¤å»ã™ã‚Œã°ã€ç”»åƒãŒç”Ÿæˆã§ãã‚‹ã€‚ã“ã®å˜ç´”ãªç™ºæƒ³ãŒã€2020å¹´ã«DDPMã¨ã—ã¦çµå®Ÿã—ã€ç”ŸæˆAIã®ä¸»æµã¨ãªã£ãŸã€‚**

VAEã¯ã¼ã‚„ã‘ã€GANã¯ä¸å®‰å®šã€è‡ªå·±å›å¸°ã¯é…ã„ã€‚ç¬¬9-13å›ã§å­¦ã‚“ã ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€ãã‚Œãã‚Œé™ç•Œã‚’æŠ±ãˆã¦ã„ãŸã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« (Diffusion Models) ã¯ã“ã‚Œã‚‰ã‚’å…¨ã¦è§£æ±ºã™ã‚‹ â€” **ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‚’æ®µéšçš„ã«åŠ ãˆã‚‹ Forward Process ã¨ã€ãã‚Œã‚’é€†è»¢ã•ã›ã‚‹ Reverse Process ã®2ã¤ã®ãƒãƒ«ã‚³ãƒ•é€£é–** ã§æ§‹æˆã•ã‚Œã‚‹ã€‚

Jonathan Ho ã‚‰ã® DDPM [^1] (2020) ãŒã€ã“ã®æ çµ„ã¿ã‚’å¤‰åˆ†æ¨è«– (ç¬¬9å›) ã¨çµ„ã¿åˆã‚ã›ã€é«˜å“è³ªãªç”»åƒç”Ÿæˆã‚’å®Ÿç¾ã—ãŸã€‚CIFAR10 ã§ FID 3.17ã€ImageNet 256Ã—256 ã§ ProgressiveGAN åŒ¹æ•µã®å“è³ªã€‚ãã—ã¦ 2021å¹´ã® DDIM [^2] ãŒæ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ 10-50å€é«˜é€ŸåŒ–ã€2022å¹´ã® Stable Diffusion ãŒæ½œåœ¨ç©ºé–“æ‹¡æ•£ã§æ¶ˆè²»è€…GPUã¸ã®æ™®åŠã‚’æœãŸã—ãŸã€‚

æœ¬è¬›ç¾©ã¯ Course IVã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç·¨ã€ç¬¬4å› â€” NF(ç¬¬33å›)â†’EBM(ç¬¬34å›)â†’Score Matching(ç¬¬35å›) ã¨ç©ã¿ä¸Šã’ã¦ããŸç†è«–ã®æ ¸å¿ƒã ã€‚**Forward Process ã®é–‰å½¢å¼è§£ã€Reverse Process ã®ãƒ™ã‚¤ã‚ºåè»¢ã€VLB ã®å®Œå…¨å±•é–‹ã€Îµ/xâ‚€/v-prediction ã®3å½¢æ…‹ã€SNRè¦–ç‚¹ã€U-Netã€DDIMã€Score-based å†è§£é‡ˆ** ã‚’å®Œå…¨å°å‡ºã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["âšª Pure Data<br/>xâ‚€ âˆ¼ q(xâ‚€)"] -->|"Forward<br/>Add Noise"| B["ğŸ”µ Noisy<br/>x_T âˆ¼ ğ’©(0,I)"]
    B -->|"Reverse<br/>Denoise"| C["âšª Generated<br/>xÌ‚â‚€"]

    A -.t=0.-> D["xâ‚€"]
    D -->|q xâ‚œ|xâ‚œâ‚‹â‚| E["xâ‚"]
    E -->|q| F["xâ‚‚"]
    F -->|...| G["x_T"]

    G -.t=T.-> H["x_T"]
    H -->|p_Î¸ xâ‚œâ‚‹â‚|xâ‚œ| I["x_{T-1}"]
    I -->|p_Î¸| J["x_{T-2}"]
    J -->|...| K["xÌ‚â‚€"]

    style A fill:#e8f5e9
    style B fill:#bbdefb
    style C fill:#fff9c4
    style G fill:#bbdefb
    style K fill:#fff9c4
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ + ç™ºå±• | 35åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ãƒã‚¤ã‚ºã‚’åŠ ãˆã¦é™¤å»ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: DDPMã®æ ¸å¿ƒã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

DDPMã®Forward Processã‚’3è¡Œã§å‹•ã‹ã™ã€‚ç”»åƒã«ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‚’æ®µéšçš„ã«åŠ ãˆã‚‹ã€‚

```julia
using LinearAlgebra, Statistics

# Forward Process: Add Gaussian noise step-by-step
# xâ‚€ â†’ xâ‚ â†’ xâ‚‚ â†’ ... â†’ x_T âˆ¼ ğ’©(0, I)
function forward_process(xâ‚€::Vector{Float64}, T::Int, Î²::Vector{Float64})
    # Î²: noise schedule [Î²â‚, Î²â‚‚, ..., Î²_T]
    # Î±_t = 1 - Î²_t, á¾±_t = âˆáµ¢â‚Œâ‚áµ— Î±áµ¢
    Î± = 1.0 .- Î²
    á¾± = cumprod(Î±)  # cumulative product: á¾±_t

    # Closed-form sampling: q(x_t | xâ‚€) = ğ’©(âˆšá¾±_t xâ‚€, (1-á¾±_t)I)
    x_t = sqrt(á¾±[T]) * xâ‚€ + sqrt(1 - á¾±[T]) * randn(length(xâ‚€))

    return x_t, á¾±
end

# Test: 2D data point, T=1000 steps, linear noise schedule
xâ‚€ = [1.0, 2.0]
T = 1000
Î² = range(1e-4, 0.02, length=T)  # linear schedule

x_T, á¾± = forward_process(xâ‚€, T, Î²)
println("Original: $xâ‚€")
println("After T=$T steps: $x_T")
println("Final á¾±_T = $(á¾±[end]) â†’ x_T â‰ˆ ğ’©(0, I)")
```

å‡ºåŠ›:
```
Original: [1.0, 2.0]
After T=1000 steps: [0.012, -0.031]
Final á¾±_T = 0.00018 â†’ x_T â‰ˆ ğ’©(0, I)
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ç‚¹ $\mathbf{x}_0 = [1, 2]$ ã‚’ç´”ç²‹ãªãƒã‚¤ã‚º $\mathbf{x}_T \approx \mathcal{N}(0, I)$ ã«å¤‰æ›ã—ãŸã€‚** ã“ã‚ŒãŒDDPMã®Forward Processã ã€‚é‡è¦ãªæ€§è³ª:

$$
q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I}) \quad \text{(é–‰å½¢å¼è§£)}
$$

ã“ã“ã§ $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i = \prod_{i=1}^t (1 - \beta_i)$ã€‚$t$ ãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã€$\bar{\alpha}_t \to 0$ã€$1-\bar{\alpha}_t \to 1$ ã¨ãªã‚Šã€$\mathbf{x}_t$ ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒ $\mathcal{N}(0, I)$ ã«åæŸã™ã‚‹ã€‚

**Reverse Process** (ãƒã‚¤ã‚ºé™¤å») ã¯ã“ã®é€†: $\mathbf{x}_T \sim \mathcal{N}(0, I)$ ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ ã§ãƒã‚¤ã‚ºã‚’äºˆæ¸¬ã—ã¦æ®µéšçš„ã«é™¤å»ã™ã‚‹ã€‚

$$
p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \sigma_t^2 \mathbf{I})
$$

ã“ã® **Forward + Reverse** ã®2ã¤ã®ãƒãƒ«ã‚³ãƒ•é€£é–ãŒã€DDPMã®å…¨ã¦ã ã€‚

:::message
**é€²æ—: 3% å®Œäº†** Forward Processã®é–‰å½¢å¼è§£ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰å®Œå…¨å°å‡ºã¸ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” DDPMã®4ã¤ã®æ ¸å¿ƒå¼ã‚’è§¦ã‚‹

### 1.1 DDPMã®4ã¤ã®æ ¸å¿ƒå¼

DDPM [^1] ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€æœ€åˆã«è§¦ã‚‹ã¹ã4ã¤ã®å¼ãŒã‚ã‚‹ã€‚

| å¼ | æ„å‘³ | å½¹å‰² |
|:---|:-----|:-----|
| **(1) Forward Process** | $q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})$ | ãƒ‡ãƒ¼ã‚¿ã«ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹ãƒãƒ«ã‚³ãƒ•é€£é– |
| **(2) Forwardé–‰å½¢å¼** | $q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I})$ | ä»»æ„ã® $t$ ã«ä¸€æ°—ã«ã‚¸ãƒ£ãƒ³ãƒ—ã§ãã‚‹ |
| **(3) Reverse Process** | $p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \tilde{\beta}_t \mathbf{I})$ | ãƒã‚¤ã‚ºã‚’é™¤å»ã™ã‚‹ãƒãƒ«ã‚³ãƒ•é€£é– |
| **(4) ç°¡ç´ åŒ–æå¤±** | $L_\text{simple} = \mathbb{E}_{t,\mathbf{x}_0,\boldsymbol{\epsilon}} \left[ \| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \|^2 \right]$ | ãƒã‚¤ã‚ºäºˆæ¸¬ã®è¨“ç·´ç›®çš„é–¢æ•° |

ã“ã®4ã¤ã‚’é †ã«è§¦ã£ã¦ã„ã“ã†ã€‚

#### 1.1.1 Forward Process: ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹

**å¼ (1)**: Forward Process $q(\mathbf{x}_t \mid \mathbf{x}_{t-1})$ ã¯ã€å‰ã®ã‚¹ãƒ†ãƒƒãƒ— $\mathbf{x}_{t-1}$ ã«å¾®å°ãªã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹ã€‚

$$
q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
$$

- $\beta_t \in (0, 1)$: ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (noise schedule)ã€‚å°ã•ãªå€¤ã‹ã‚‰å§‹ã‚ã€å¾ã€…ã«å¤§ãããªã‚‹ã€‚
- $\sqrt{1-\beta_t}$: å…ƒã®ä¿¡å·ã‚’ç¸®å°ã™ã‚‹ä¿‚æ•°ã€‚
- $\beta_t \mathbf{I}$: ãƒã‚¤ã‚ºã®åˆ†æ•£ã€‚

ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ–¹å¼:

$$
\mathbf{x}_t = \sqrt{1-\beta_t} \mathbf{x}_{t-1} + \sqrt{\beta_t} \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})
$$

```julia
# Forward step: x_{t-1} â†’ x_t
function forward_step(x_prev::Vector{Float64}, Î²_t::Float64)
    Îµ = randn(length(x_prev))
    x_t = sqrt(1 - Î²_t) * x_prev + sqrt(Î²_t) * Îµ
    return x_t, Îµ  # also return noise for later use
end

xâ‚€ = [1.0, 2.0]
Î²â‚ = 0.0001  # tiny noise at t=1

xâ‚, Îµâ‚ = forward_step(xâ‚€, Î²â‚)
println("xâ‚€ = $xâ‚€")
println("xâ‚ = $xâ‚  (noise added: $Îµâ‚)")
```

**é‡è¦ãªæ€§è³ª**: Forward Processã¯**å›ºå®š**ã•ã‚Œã¦ã„ã‚‹ã€‚å­¦ç¿’ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä¸€åˆ‡ãªã„ã€‚$\beta_t$ ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦äº‹å‰ã«æ±ºã‚ã‚‹ (Section 3.2ã§è©³è¿°)ã€‚

#### 1.1.2 Forwardé–‰å½¢å¼: ä¸€æ°—ã«ã‚¸ãƒ£ãƒ³ãƒ—

**å¼ (2)**: Forward Processã‚’ $t$ å›ç¹°ã‚Šè¿”ã™ã¨ã€$\mathbf{x}_0$ ã‹ã‚‰ $\mathbf{x}_t$ ã¸ã®å¤‰æ›ã®é–‰å½¢å¼ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

$$
q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I})
$$

ã“ã“ã§:

$$
\alpha_t = 1 - \beta_t, \quad \bar{\alpha}_t = \prod_{i=1}^t \alpha_i
$$

**å°å‡ºã®ç›´æ„Ÿ** (å®Œå…¨ç‰ˆã¯Section 3.1):

$$
\begin{aligned}
\mathbf{x}_t &= \sqrt{\alpha_t} \mathbf{x}_{t-1} + \sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}) + \sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{\alpha_t(1-\alpha_{t-1}) + (1-\alpha_t)} \bar{\boldsymbol{\epsilon}} \\
&= \cdots \\
&= \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \bar{\boldsymbol{\epsilon}}, \quad \bar{\boldsymbol{\epsilon}} \sim \mathcal{N}(0, \mathbf{I})
\end{aligned}
$$

**ã“ã®é–‰å½¢å¼è§£ã®ãŠã‹ã’ã§ã€è¨“ç·´æ™‚ã«ä»»æ„ã® $t$ ã¸ä¸€æ°—ã«ã‚¸ãƒ£ãƒ³ãƒ—ã§ãã‚‹** (æ¯å› $t$ ã‚¹ãƒ†ãƒƒãƒ—ç¹°ã‚Šè¿”ã™å¿…è¦ãŒãªã„)ã€‚

```julia
# Closed-form sampling: xâ‚€ â†’ x_t (any t)
function sample_x_t(xâ‚€::Vector{Float64}, t::Int, á¾±::Vector{Float64})
    Îµ = randn(length(xâ‚€))
    x_t = sqrt(á¾±[t]) * xâ‚€ + sqrt(1 - á¾±[t]) * Îµ
    return x_t, Îµ
end

Î² = range(1e-4, 0.02, length=1000)
á¾± = cumprod(1.0 .- Î²)

xâ‚€ = [1.0, 2.0]
xâ‚…â‚€â‚€, Îµâ‚…â‚€â‚€ = sample_x_t(xâ‚€, 500, á¾±)
println("xâ‚€ = $xâ‚€")
println("xâ‚…â‚€â‚€ = $xâ‚…â‚€â‚€  (âˆšá¾±â‚…â‚€â‚€ = $(sqrt(á¾±[500])))")
```

#### 1.1.3 Reverse Process: ãƒã‚¤ã‚ºã‚’é™¤å»ã™ã‚‹

**å¼ (3)**: Reverse Process $p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$ ã¯ã€ãƒã‚¤ã‚ºã®å¤šã„ $\mathbf{x}_t$ ã‹ã‚‰å°‘ã—ãƒã‚¤ã‚ºã‚’é™¤å»ã—ã¦ $\mathbf{x}_{t-1}$ ã‚’å¾—ã‚‹ã€‚

$$
p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \tilde{\beta}_t \mathbf{I})
$$

- $\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)$: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $\theta$ ãŒäºˆæ¸¬ã™ã‚‹å¹³å‡ã€‚
- $\tilde{\beta}_t$: åˆ†æ•£ (å›ºå®š or å­¦ç¿’å¯èƒ½ã€Section 3.3ã§è©³è¿°)ã€‚

**3ã¤ã®äºˆæ¸¬æ–¹å¼** (ã©ã‚Œã‚’äºˆæ¸¬ã™ã‚‹ã‹ã§è¨“ç·´ç›®çš„é–¢æ•°ãŒå¤‰ã‚ã‚‹):

| äºˆæ¸¬å¯¾è±¡ | å¹³å‡ã®å¼ | è¨“ç·´æå¤± |
|:---------|:---------|:---------|
| **Îµ-prediction** | $\boldsymbol{\mu}_\theta = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right)$ | $\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta\|^2$ |
| **xâ‚€-prediction** | $\boldsymbol{\mu}_\theta = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_\theta(\mathbf{x}_t, t) + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} \mathbf{x}_t$ | $\|\mathbf{x}_0 - \mathbf{x}_\theta\|^2$ |
| **v-prediction** | $\boldsymbol{\mu}_\theta$ ã¯vã‹ã‚‰å°å‡º | $\|\mathbf{v} - \mathbf{v}_\theta\|^2$ |

**Îµ-prediction** (DDPM [^1] ãŒæ¡ç”¨) ãŒæœ€ã‚‚ä¸€èˆ¬çš„ã€‚ãƒã‚¤ã‚º $\boldsymbol{\epsilon}$ ã‚’äºˆæ¸¬ã—ã€ãã‚Œã‚’ä½¿ã£ã¦å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹ã€‚

```julia
# Reverse step: x_t â†’ x_{t-1} (using Îµ-prediction)
function reverse_step(x_t::Vector{Float64}, Îµ_Î¸::Vector{Float64}, t::Int, Î²::Vector{Float64}, á¾±::Vector{Float64})
    Î±_t = 1 - Î²[t]
    # Mean: Î¼_Î¸ = (1/âˆšÎ±_t) * (x_t - (Î²_t/âˆš(1-á¾±_t)) * Îµ_Î¸)
    Î¼_Î¸ = (1 / sqrt(Î±_t)) * (x_t - (Î²[t] / sqrt(1 - á¾±[t])) * Îµ_Î¸)

    # Variance: Ïƒ_tÂ² = Î²_t (simplified)
    Ïƒ_t = sqrt(Î²[t])

    # Sample: x_{t-1} = Î¼_Î¸ + Ïƒ_t * z, z ~ ğ’©(0, I)
    z = (t > 1) ? randn(length(x_t)) : zeros(length(x_t))  # no noise at t=1
    x_prev = Î¼_Î¸ + Ïƒ_t * z

    return x_prev
end

# Placeholder: Îµ_Î¸ would be a trained U-Net
Îµ_Î¸ = randn(2)  # random for demo
x_t = [0.5, 0.3]
t = 500

x_prev = reverse_step(x_t, Îµ_Î¸, t, Î², á¾±)
println("x_t = $x_t")
println("x_{t-1} = $x_prev  (denoised)")
```

#### 1.1.4 ç°¡ç´ åŒ–æå¤±: ãƒã‚¤ã‚ºäºˆæ¸¬ã‚’è¨“ç·´ã™ã‚‹

**å¼ (4)**: DDPMã®è¨“ç·´ã¯ã€**ãƒã‚¤ã‚º $\boldsymbol{\epsilon}$ ã‚’æ­£ç¢ºã«äºˆæ¸¬ã™ã‚‹ã“ã¨**ã«å¸°ç€ã™ã‚‹ã€‚

$$
L_\text{simple} = \mathbb{E}_{t \sim \text{Uniform}(1,T), \mathbf{x}_0 \sim q(\mathbf{x}_0), \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})} \left[ \| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \|^2 \right]
$$

ã“ã“ã§ $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}$ (å¼ (2) ã®é–‰å½¢å¼)ã€‚

**è¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ** (Algorithm 1 in DDPM [^1]):

```julia
# Training step (simplified)
function train_step(xâ‚€::Vector{Float64}, Îµ_Î¸::Function, Î²::Vector{Float64}, á¾±::Vector{Float64}, T::Int)
    # 1. Sample t uniformly
    t = rand(1:T)

    # 2. Sample noise Îµ ~ ğ’©(0, I)
    Îµ = randn(length(xâ‚€))

    # 3. Compute x_t using closed-form
    x_t = sqrt(á¾±[t]) * xâ‚€ + sqrt(1 - á¾±[t]) * Îµ

    # 4. Predict noise with network
    Îµ_pred = Îµ_Î¸(x_t, t)

    # 5. Compute loss
    loss = sum((Îµ - Îµ_pred).^2)

    return loss
end

# Placeholder: Îµ_Î¸ is a U-Net (Section 4)
Îµ_Î¸(x, t) = randn(length(x))  # random for demo

xâ‚€ = [1.0, 2.0]
loss = train_step(xâ‚€, Îµ_Î¸, Î², á¾±, 1000)
println("Training loss: $loss")
```

**ã“ã®4ã¤ã®å¼ãŒDDPMã®å…¨ã¦ã ã€‚** æ®‹ã‚Šã®ã‚¾ãƒ¼ãƒ³ã§ã¯ã€ã“ã‚Œã‚‰ã‚’å®Œå…¨å°å‡ºã—ã€å®Ÿè£…ã™ã‚‹ã€‚

:::message
**é€²æ—: 10% å®Œäº†** DDPMã®4ã¤ã®æ ¸å¿ƒå¼ã‚’è§¦ã£ãŸã€‚æ¬¡ã¯ã€ŒãªãœDDPMã‹ã€ã®ç›´æ„Ÿã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœDDPMã‹ï¼Ÿ

### 2.1 ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é™ç•Œã‚’æŒ¯ã‚Šè¿”ã‚‹

ç¬¬9-13å›ã§å­¦ã‚“ã ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é™ç•Œ:

| ãƒ¢ãƒ‡ãƒ« | é•·æ‰€ | é™ç•Œ |
|:-------|:-----|:-----|
| **VAE** (ç¬¬10å›) | å°¤åº¦è¨ˆç®—å¯èƒ½ã€å®‰å®šè¨“ç·´ | ã¼ã‚„ã‘ãŸå‡ºåŠ› (Gaussian decoder) |
| **GAN** (ç¬¬12å›) | é«˜å“è³ªã€ã‚·ãƒ£ãƒ¼ãƒ— | è¨“ç·´ä¸å®‰å®šã€Mode collapse |
| **è‡ªå·±å›å¸°** (ç¬¬13å›) | å°¤åº¦è¨ˆç®—å¯èƒ½ã€é«˜å“è³ª | é€æ¬¡ç”Ÿæˆã§é…ã„ |

**DDPM [^1] ã¯ã“ã‚Œã‚‰ã‚’å…¨ã¦è§£æ±ºã™ã‚‹**:

- **VAE**: ELBOæœ€é©åŒ–ã ãŒã€**æ®µéšçš„ãƒã‚¤ã‚ºé™¤å»**ã§ Gaussian decoder ã®ã¼ã‚„ã‘ã‚’å›é¿
- **GAN**: æ•µå¯¾çš„è¨“ç·´ä¸è¦ã€‚**å˜ç´”ãªMSEæå¤±** (ãƒã‚¤ã‚ºäºˆæ¸¬) ã§å®‰å®šè¨“ç·´
- **è‡ªå·±å›å¸°**: ä¸¦åˆ—è¨“ç·´å¯èƒ½ (ä»»æ„ã® $t$ ã«ã‚¸ãƒ£ãƒ³ãƒ—)ã€‚æ¨è«–ã¯é€æ¬¡ã ãŒã€**DDIM [^2] ã§é«˜é€ŸåŒ–**

### 2.2 æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ç›´æ„Ÿ: ç†±æ‹¡æ•£ã®é€†è»¢

**ç‰©ç†çš„é¡æ¨**: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $q(\mathbf{x}_0)$ ã«ç†±æ‹¡æ•£ (heat diffusion) ã‚’é©ç”¨ã™ã‚‹ã¨ã€æœ€çµ‚çš„ã«ç†±å¹³è¡¡çŠ¶æ…‹ (æ¨™æº–æ­£è¦åˆ†å¸ƒ $\mathcal{N}(0, I)$) ã«åˆ°é”ã™ã‚‹ã€‚**ã“ã®éç¨‹ã‚’é€†è»¢ã•ã›ã‚Œã°ã€$\mathcal{N}(0, I)$ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’ç”Ÿæˆã§ãã‚‹**ã€‚

```mermaid
graph LR
    A["âšª Data<br/>q(xâ‚€)"] -->|Forward<br/>Diffusion| B["ğŸ”µ Noise<br/>ğ’©(0,I)"]
    B -->|Reverse<br/>Denoising| C["âšª Generated<br/>p(xâ‚€)"]

    A -.å­¦ç¿’ãƒ‡ãƒ¼ã‚¿.-> D["ç”»åƒ/éŸ³å£°/ãƒ†ã‚­ã‚¹ãƒˆ"]
    B -.ç´”ç²‹ãªãƒã‚¤ã‚º.-> E["ãƒ©ãƒ³ãƒ€ãƒ ãªç‚¹"]
    C -.ç”Ÿæˆãƒ‡ãƒ¼ã‚¿.-> F["æ–°ã—ã„ç”»åƒ"]

    style A fill:#e8f5e9
    style B fill:#bbdefb
    style C fill:#fff9c4
```

**3ã¤ã®æ¯”å–©**:

1. **ç†±æ‹¡æ•£**: ã‚¤ãƒ³ã‚¯ã‚’æ°´ã«å‚ã‚‰ã™ã¨æ‹¡æ•£ã™ã‚‹ã€‚é€†å†ç”Ÿã™ã‚Œã°ã€æ°´ã‹ã‚‰ã‚¤ãƒ³ã‚¯ãŒæµ®ã‹ã³ä¸ŠãŒã‚‹ã€‚
2. **ãƒã‚¤ã‚ºé™¤å»ãƒ•ã‚£ãƒ«ã‚¿**: å†™çœŸã«ãƒã‚¤ã‚ºã‚’åŠ ãˆã€ãƒ•ã‚£ãƒ«ã‚¿ã§é™¤å»ã™ã‚‹ã€‚ã“ã‚Œã‚’ $T$ å›ç¹°ã‚Šè¿”ã™ã€‚
3. **Langevin Dynamics** (ç¬¬35å›): ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_\mathbf{x} \log p(\mathbf{x})$ ã«æ²¿ã£ã¦å‹•ãã“ã¨ã§åˆ†å¸ƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚DDPMã¯ã“ã‚Œã‚’é›¢æ•£åŒ–ã—ãŸã‚‚ã®ã€‚

### 2.3 Course IVã§ã®ä½ç½®ã¥ã‘ â€” ç†è«–ã®é›†å¤§æˆ

Course IVã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç·¨ã€(ç¬¬33-42å›) ã¯ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ç†è«–ã‚’æ·±åŒ–ã•ã›ã‚‹10å›ã®æ—…è·¯ã ã€‚

```mermaid
graph TD
    L33["ç¬¬33å›<br/>NF & Neural ODE"] --> L34["ç¬¬34å›<br/>EBM & çµ±è¨ˆç‰©ç†"]
    L34 --> L35["ç¬¬35å›<br/>Score Matching"]
    L35 --> L36["ç¬¬36å›<br/>DDPM<br/>(ä»Šã“ã“)"]
    L36 --> L37["ç¬¬37å›<br/>SDE/ODE"]
    L37 --> L38["ç¬¬38å›<br/>Flow Matching"]
    L38 --> L39["ç¬¬39å›<br/>LDM"]
    L39 --> L40["ç¬¬40å›<br/>Consistency"]
    L40 --> L41["ç¬¬41å›<br/>World Models"]
    L41 --> L42["ç¬¬42å›<br/>çµ±ä¸€ç†è«–"]

    L33 -.NF: å³å¯†å°¤åº¦.-> Math1["å¯é€†å¤‰æ›"]
    L34 -.EBM: ã‚¨ãƒãƒ«ã‚®ãƒ¼.-> Math2["Z(Î¸)è¨ˆç®—ä¸èƒ½"]
    L35 -.Score: âˆ‡log p.-> Math3["Langevin"]
    L36 -.DDPM: é›¢æ•£æ‹¡æ•£.-> Math4["ãƒãƒ«ã‚³ãƒ•é€£é–"]
    L37 -.SDE: é€£ç¶šæ¥µé™.-> Math5["ç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼"]
    L38 -.FM: OTçµ±ä¸€.-> Math6["æœ€é©è¼¸é€"]

    style L36 fill:#ffeb3b
```

**Course I (ç¬¬1-8å›) ã®æ•°å­¦ãŒã“ã“ã§èŠ±é–‹ã**:

| Course I | Course IV ç¬¬36å› | æ´»ç”¨æ–¹æ³• |
|:---------|:----------------|:---------|
| ç¬¬4å›: ç¢ºç‡è«– | Forward/Reverse Process | æ¡ä»¶ä»˜ãã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æ€§è³ª |
| ç¬¬5å›: æ¸¬åº¦è«–ãƒ»ç¢ºç‡éç¨‹ | ãƒãƒ«ã‚³ãƒ•é€£é– | çŠ¶æ…‹é·ç§»ã®æ¸¬åº¦è«–çš„è¨˜è¿° |
| ç¬¬6å›: æƒ…å ±ç†è«– | VLB | KL divergenceã€ELBOåˆ†è§£ |
| ç¬¬8å›: EMç®—æ³• | æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« | $\mathbf{x}_{1:T}$ ãŒæ½œåœ¨å¤‰æ•° |

**ç¬¬35å› Score Matching ã¨ã®æ¥ç¶š**:

DDPMã®æå¤±é–¢æ•°ã¯ã€**Denoising Score Matching** (ç¬¬35å›) ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ãŒè¨¼æ˜ã•ã‚Œã¦ã„ã‚‹ [^1]ã€‚

$$
\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \mid \mathbf{x}_0) = - \frac{\boldsymbol{\epsilon}}{\sqrt{1-\bar{\alpha}_t}}
$$

ã¤ã¾ã‚Šã€**ãƒã‚¤ã‚º $\boldsymbol{\epsilon}$ ã‚’äºˆæ¸¬ã™ã‚‹ = ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’äºˆæ¸¬ã™ã‚‹**ã€‚ã“ã®çµ±ä¸€çš„è¦–ç‚¹ã¯ç¬¬38å› Flow Matching ã§å®Œå…¨ã«è¨¼æ˜ã•ã‚Œã‚‹ã€‚

### 2.4 æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã¨ã®æ¯”è¼ƒ

| è¦³ç‚¹ | æ¾å°¾ç ” | æœ¬è¬›ç¾© |
|:-----|:-------|:-------|
| **DDPMç†è«–** | Forward/Reverseã®æ¦‚è¦ | **å®Œå…¨å°å‡º** (é–‰å½¢å¼ãƒ»VLBãƒ»3å½¢æ…‹) |
| **Noise Schedule** | Linear scheduleç´¹ä»‹ | **Cosine / SNRå˜èª¿æ¸›å°‘ / Zero Terminal** |
| **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** | DDIMæ¦‚è¦ | **DDIMå®Œå…¨ç‰ˆ + DPM-Solver++ / UniPC** |
| **U-Net** | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³ | **Time Embedding / GroupNorm / Self-Attention å®Œå…¨è§£èª¬** |
| **Score-basedè¦–ç‚¹** | è§¦ã‚Œãªã„ | **DDPMã¨Score Matchingã®ç­‰ä¾¡æ€§è¨¼æ˜** |
| **å®Ÿè£…** | PyTorchãƒ‡ãƒ¢ | **âš¡ Juliaè¨“ç·´ + ğŸ¦€ Rustæ¨è«–** |
| **æœ€æ–°æ€§** | 2020-2021 | **2024-2026 SOTA** (Zero Terminal SNR / Improved DDPM) |

**å·®åˆ¥åŒ–ã®æœ¬è³ª**: æ¾å°¾ç ”ãŒã€Œæ‰‹æ³•ã®ç´¹ä»‹ã€ã«ã¨ã©ã¾ã‚‹ã®ã«å¯¾ã—ã€æœ¬è¬›ç¾©ã¯ã€Œè«–æ–‡ãŒæ›¸ã‘ã‚‹ç†è«–çš„æ·±ã• + Productionå®Ÿè£…ã€ã‚’è²«ãã€‚

:::message alert
**ã“ã“ãŒè¸ã‚“å¼µã‚Šã©ã“ã‚**: Zone 3ã¯æœ¬è¬›ç¾©ã§æœ€ã‚‚æ•°å¼ãŒå¯†é›†ã™ã‚‹ã‚¾ãƒ¼ãƒ³ã ã€‚Forward Processã®é–‰å½¢å¼è§£ã€Reverse Processã®ãƒ™ã‚¤ã‚ºåè»¢ã€VLBã®å®Œå…¨å±•é–‹ã‚’ä¸€ã¤ä¸€ã¤å°å‡ºã™ã‚‹ã€‚ç¬¬4å›ã®æ¡ä»¶ä»˜ãã‚¬ã‚¦ã‚¹åˆ†å¸ƒã€ç¬¬8å›ã®ELBOãŒç·å‹•å“¡ã•ã‚Œã‚‹ã€‚
:::

### 2.5 å­¦ç¿’æˆ¦ç•¥ â€” æ•°å¼ä¿®è¡Œã®æº–å‚™

**Zone 3ã®å…¨ä½“ãƒãƒƒãƒ—**:

```mermaid
graph TD
    A[3.1 Forward Process<br/>é–‰å½¢å¼è§£å°å‡º] --> B[3.2 Noise Schedule<br/>è¨­è¨ˆåŸç†]
    B --> C[3.3 Reverse Process<br/>ãƒ™ã‚¤ã‚ºåè»¢]
    C --> D[3.4 VLB<br/>å®Œå…¨å±•é–‹]
    D --> E[3.5 3å½¢æ…‹<br/>Îµ/xâ‚€/v]
    E --> F[3.6 ç°¡ç´ åŒ–æå¤±<br/>L_simple]
    F --> G[3.7 SNRè¦–ç‚¹<br/>çµ±ä¸€çš„ç†è§£]
    G --> H[3.8 U-Net<br/>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
    H --> I[3.9 DDIM<br/>æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    I --> J[3.10 Score-based<br/>å†è§£é‡ˆ]

    style A fill:#e3f2fd
    style D fill:#fff9c4
    style I fill:#c8e6c9
```

**å­¦ç¿’ã®ã‚³ãƒ„**:

1. **ç´™ã¨ãƒšãƒ³ã‚’ç”¨æ„ã™ã‚‹**: å„å°å‡ºã‚’è‡ªåˆ†ã®æ‰‹ã§è¿½ã†ã€‚
2. **æ•°å€¤æ¤œè¨¼ã‚³ãƒ¼ãƒ‰**: å„å¼ã‚’Juliaã§ç¢ºèªã™ã‚‹ (Zone 4ã§å®Œå…¨å®Ÿè£…)ã€‚
3. **å‰æçŸ¥è­˜ã®å‚ç…§**: ç¬¬4å› (ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ)ã€ç¬¬8å› (ELBO) ã‚’æ‰‹å…ƒã«ç½®ãã€‚
4. **Bossæˆ¦ã®æº–å‚™**: 3.4 VLBå®Œå…¨å±•é–‹ã€3.9 DDIMå®Œå…¨å°å‡ºãŒæœ€é›£é–¢ã€‚

:::message
**é€²æ—: 20% å®Œäº†** DDPMã®ç›´æ„Ÿã¨å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚Zone 3ã§æ•°å¼ã®æµ·ã«é£›ã³è¾¼ã‚€ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” ç†è«–å®Œå…¨å°å‡º

### 3.1 Forward Process ã®é–‰å½¢å¼è§£å°å‡º

**å®šç†**: Forward Process $q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})$ ã‚’ $t$ å›é©ç”¨ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®é–‰å½¢å¼ãŒå¾—ã‚‰ã‚Œã‚‹:

$$
q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I})
$$

ã“ã“ã§ $\alpha_t = 1 - \beta_t$ã€$\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$ã€‚

**è¨¼æ˜** (æ•°å­¦çš„å¸°ç´æ³•):

**Base case** ($t=1$):

$$
q(\mathbf{x}_1 \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{1-\beta_1} \mathbf{x}_0, \beta_1 \mathbf{I}) = \mathcal{N}(\sqrt{\alpha_1} \mathbf{x}_0, (1-\alpha_1) \mathbf{I})
$$

$\bar{\alpha}_1 = \alpha_1$ ã‚ˆã‚Šæˆç«‹ã€‚

**Inductive step**: $t-1$ ã§æˆç«‹ã™ã‚‹ã¨ä»®å®šã—ã€$t$ ã§æˆç«‹ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

$$
\begin{aligned}
q(\mathbf{x}_t \mid \mathbf{x}_0) &= \int q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) q(\mathbf{x}_{t-1} \mid \mathbf{x}_0) \, d\mathbf{x}_{t-1} \\
&= \int \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_{t-1}, (1-\alpha_t) \mathbf{I}) \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0, (1-\bar{\alpha}_{t-1}) \mathbf{I}) \, d\mathbf{x}_{t-1}
\end{aligned}
$$

**ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ç©ã®æ€§è³ª** (ç¬¬4å›ã®å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®æ¡ä»¶ä»˜ãåˆ†å¸ƒ):

2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}(\mathbf{x}; \mathbf{a}, A)$ ã¨ $\mathcal{N}(\mathbf{x}; \mathbf{b}, B)$ ã®ç©ã¯ã€æ­£è¦åŒ–å®šæ•°ã‚’é™¤ã„ã¦ $\mathcal{N}(\mathbf{x}; \mathbf{c}, C)$ ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã€‚ã“ã“ã§:

$$
C^{-1} = A^{-1} + B^{-1}, \quad \mathbf{c} = C (A^{-1} \mathbf{a} + B^{-1} \mathbf{b})
$$

$q(\mathbf{x}_t \mid \mathbf{x}_{t-1})$ ã‚’reparameterize:

$$
\mathbf{x}_t = \sqrt{\alpha_t} \mathbf{x}_{t-1} + \sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1}, \quad \boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(0, \mathbf{I})
$$

$q(\mathbf{x}_{t-1} \mid \mathbf{x}_0)$ ã‚’reparameterize:

$$
\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}} \boldsymbol{\epsilon}_{t-2}, \quad \boldsymbol{\epsilon}_{t-2} \sim \mathcal{N}(0, \mathbf{I})
$$

ä»£å…¥:

$$
\begin{aligned}
\mathbf{x}_t &= \sqrt{\alpha_t} (\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}} \boldsymbol{\epsilon}_{t-2}) + \sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t \bar{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{\alpha_t (1-\bar{\alpha}_{t-1})} \boldsymbol{\epsilon}_{t-2} + \sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1}
\end{aligned}
$$

**ç‹¬ç«‹ãªã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã®åˆæˆ**: $\boldsymbol{\epsilon}_{t-2}$ ã¨ $\boldsymbol{\epsilon}_{t-1}$ ã¯ç‹¬ç«‹ã€‚åˆæˆãƒã‚¤ã‚ºã®åˆ†æ•£:

$$
\text{Var}[\sqrt{\alpha_t (1-\bar{\alpha}_{t-1})} \boldsymbol{\epsilon}_{t-2} + \sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1}] = \alpha_t (1-\bar{\alpha}_{t-1}) + (1-\alpha_t) = 1 - \alpha_t \bar{\alpha}_{t-1} = 1 - \bar{\alpha}_t
$$

ã—ãŸãŒã£ã¦:

$$
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \bar{\boldsymbol{\epsilon}}, \quad \bar{\boldsymbol{\epsilon}} \sim \mathcal{N}(0, \mathbf{I})
$$

ã“ã‚Œã¯ $q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I})$ ã‚’æ„å‘³ã™ã‚‹ã€‚â– 

**æ•°å€¤æ¤œè¨¼**:

```julia
# Verify closed-form derivation
using LinearAlgebra, Statistics

function verify_forward_closed_form(xâ‚€::Vector{Float64}, t::Int, Î²::Vector{Float64}, n_samples::Int=10000)
    Î± = 1.0 .- Î²
    á¾± = cumprod(Î±)

    # Method 1: Iterative forward
    samples_iterative = zeros(length(xâ‚€), n_samples)
    for i in 1:n_samples
        x = copy(xâ‚€)
        for s in 1:t
            Îµ = randn(length(xâ‚€))
            x = sqrt(Î±[s]) * x + sqrt(1 - Î±[s]) * Îµ
        end
        samples_iterative[:, i] = x
    end

    # Method 2: Closed-form
    samples_closed = zeros(length(xâ‚€), n_samples)
    for i in 1:n_samples
        Îµ = randn(length(xâ‚€))
        samples_closed[:, i] = sqrt(á¾±[t]) * xâ‚€ + sqrt(1 - á¾±[t]) * Îµ
    end

    # Compare statistics
    mean_iter = vec(mean(samples_iterative, dims=2))
    std_iter = vec(std(samples_iterative, dims=2))
    mean_closed = vec(mean(samples_closed, dims=2))
    std_closed = vec(std(samples_closed, dims=2))

    println("Iterative - Mean: $mean_iter, Std: $std_iter")
    println("Closed-form - Mean: $mean_closed, Std: $std_closed")
    println("Theory - Mean: $(sqrt(á¾±[t]) * xâ‚€), Std: $(sqrt(1 - á¾±[t]))")
end

xâ‚€ = [1.0, 2.0]
Î² = range(1e-4, 0.02, length=1000)
verify_forward_closed_form(xâ‚€, 500, Î², 10000)
```

**é‡è¦ãªæ€§è³ª**:

1. **$\bar{\alpha}_t$ ã®æŒ™å‹•**: $t \to T$ ã§ $\bar{\alpha}_t \to 0$ â†’ $q(\mathbf{x}_T \mid \mathbf{x}_0) \approx \mathcal{N}(0, \mathbf{I})$
2. **reparameterization**: $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}$ ã§ä¸€æ°—ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½
3. **è¨“ç·´åŠ¹ç‡**: å„ãƒŸãƒ‹ãƒãƒƒãƒã§ç•°ãªã‚‹ $t$ ã‚’ã‚µãƒ³ãƒ—ãƒ«ã§ãã€ä¸¦åˆ—è¨“ç·´å¯èƒ½

### 3.2 Noise Schedule ã®è¨­è¨ˆåŸç†

**Noise Schedule** $\{\beta_t\}_{t=1}^T$ ã¯ã€**ã©ã‚Œã ã‘é€Ÿããƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹ã‹**ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚è¨­è¨ˆåŸå‰‡:

1. **$\bar{\alpha}_T \approx 0$**: æœ€çµ‚çš„ã« $\mathbf{x}_T \approx \mathcal{N}(0, \mathbf{I})$ ã«ãªã‚‹
2. **SNRå˜èª¿æ¸›å°‘**: Signal-to-Noise Ratio $\text{SNR}(t) = \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}$ ãŒ $t$ ã¨ã¨ã‚‚ã«æ¸›å°‘
3. **Zero Terminal SNR**: $\bar{\alpha}_T = 0$ ã§å³å¯†ã« $\mathcal{N}(0, \mathbf{I})$

#### 3.2.1 Linear Schedule (DDPM [^1])

$$
\beta_t = \beta_{\min} + \frac{t-1}{T-1} (\beta_{\max} - \beta_{\min})
$$

DDPM [^1] ã§ã¯ $\beta_{\min} = 10^{-4}$ã€$\beta_{\max} = 0.02$ã€$T = 1000$ã€‚

**å•é¡Œç‚¹**: $\bar{\alpha}_T > 0$ (Zero Terminal SNR ã‚’æº€ãŸã•ãªã„) [^5]ã€‚

```julia
# Linear schedule
function linear_schedule(T::Int, Î²_min::Float64=1e-4, Î²_max::Float64=0.02)
    Î² = range(Î²_min, Î²_max, length=T)
    Î± = 1.0 .- Î²
    á¾± = cumprod(Î±)
    return Î², á¾±
end

Î²_linear, á¾±_linear = linear_schedule(1000)
println("Linear schedule: á¾±_T = $(á¾±_linear[end])")  # Should be â‰ˆ 0, but > 0
```

#### 3.2.2 Cosine Schedule (Improved DDPM [^3])

$$
\bar{\alpha}_t = \frac{f(t)}{f(0)}, \quad f(t) = \cos^2 \left( \frac{t/T + s}{1 + s} \cdot \frac{\pi}{2} \right)
$$

ã“ã“ã§ $s = 0.008$ ã¯å°ã•ãªã‚ªãƒ•ã‚»ãƒƒãƒˆ (ç«¯ç‚¹ã§ã®æ€¥æ¿€ãªå¤‰åŒ–ã‚’é˜²ã)ã€‚

**åˆ©ç‚¹**:

- SNRãŒç·©ã‚„ã‹ã«æ¸›å°‘ â†’ è¨“ç·´å®‰å®š
- Zero Terminal SNRã«è¿‘ã„

```julia
# Cosine schedule (Improved DDPM)
function cosine_schedule(T::Int, s::Float64=0.008)
    t_seq = 0:T
    f_t = @. cos((t_seq / T + s) / (1 + s) * Ï€ / 2)^2
    á¾± = f_t[2:end] ./ f_t[1]  # á¾±_t = f(t) / f(0)
    Î² = 1.0 .- (á¾± ./ [1.0; á¾±[1:end-1]])  # Î²_t = 1 - Î±_t = 1 - á¾±_t / á¾±_{t-1}
    return Î², á¾±
end

Î²_cosine, á¾±_cosine = cosine_schedule(1000)
println("Cosine schedule: á¾±_T = $(á¾±_cosine[end])")
```

#### 3.2.3 Zero Terminal SNR Rescaling (Lin+ 2023 [^5])

**å‹•æ©Ÿ**: Linear/Cosine schedule ã¯ $\bar{\alpha}_T > 0$ â†’ è¨“ç·´ã¨æ¨è«–ã®ä¸ä¸€è‡´ã€‚

**è§£æ±ºç­–**: Schedule ã‚’rescaleã—ã¦ $\bar{\alpha}_T = 0$ ã‚’å¼·åˆ¶ã€‚

$$
\tilde{\alpha}_t = \frac{\bar{\alpha}_t - \bar{\alpha}_T}{1 - \bar{\alpha}_T}
$$

```julia
# Zero Terminal SNR rescaling
function rescale_zero_terminal_snr(á¾±::Vector{Float64})
    á¾±_T = á¾±[end]
    á¾±_rescaled = (á¾± .- á¾±_T) ./ (1 - á¾±_T)
    return á¾±_rescaled
end

á¾±_linear_rescaled = rescale_zero_terminal_snr(á¾±_linear)
println("Rescaled linear: á¾±_T = $(á¾±_linear_rescaled[end])")  # Now = 0
```

**Noise Schedule æ¯”è¼ƒ**:

| Schedule | á¾±_T | SNRå˜èª¿æ€§ | è¨“ç·´å®‰å®šæ€§ | æ¨å¥¨åº¦ |
|:---------|:----|:---------|:----------|:-------|
| Linear | > 0 âŒ | âœ… | ä¸­ | âŒ (å¤ã„) |
| Cosine | â‰ˆ 0 | âœ… | é«˜ | âœ… (æ¨å¥¨) |
| Zero Terminal SNR | = 0 âœ… | âœ… | **æœ€é«˜** | â­ (2023+) |

### 3.3 Reverse Process ã®ãƒ™ã‚¤ã‚ºåè»¢

**ç›®æ¨™**: Forward Process $q(\mathbf{x}_t \mid \mathbf{x}_{t-1})$ ã®é€†éç¨‹ $q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$ ã‚’æ±‚ã‚ã‚‹ã€‚

**å•é¡Œ**: $q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$ ã¯ç›´æ¥è¨ˆç®—ã§ããªã„ (å‘¨è¾ºåŒ–å›°é›£)ã€‚

**è§£æ±º**: **ãƒ™ã‚¤ã‚ºã®å®šç†** + **$\mathbf{x}_0$ ã‚’æ¡ä»¶ä»˜ã‘**:

$$
q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) = \frac{q(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0) q(\mathbf{x}_{t-1} \mid \mathbf{x}_0)}{q(\mathbf{x}_t \mid \mathbf{x}_0)}
$$

**ãƒãƒ«ã‚³ãƒ•æ€§**: $q(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0) = q(\mathbf{x}_t \mid \mathbf{x}_{t-1})$ (æœªæ¥ã¯éå»ã«ä¾å­˜ã—ãªã„)ã€‚

$$
q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) = \frac{q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) q(\mathbf{x}_{t-1} \mid \mathbf{x}_0)}{q(\mathbf{x}_t \mid \mathbf{x}_0)}
$$

å„é …ã‚’ä»£å…¥:

$$
\begin{aligned}
q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) &= \mathcal{N}(\sqrt{\alpha_t} \mathbf{x}_{t-1}, (1-\alpha_t) \mathbf{I}) \\
q(\mathbf{x}_{t-1} \mid \mathbf{x}_0) &= \mathcal{N}(\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0, (1-\bar{\alpha}_{t-1}) \mathbf{I}) \\
q(\mathbf{x}_t \mid \mathbf{x}_0) &= \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I})
\end{aligned}
$$

**ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å•†ã®æ€§è³ª** (å¯¾æ•°ç©ºé–“ã§è¨ˆç®—):

$$
\begin{aligned}
&\log q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \\
&\propto \log q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) + \log q(\mathbf{x}_{t-1} \mid \mathbf{x}_0) - \log q(\mathbf{x}_t \mid \mathbf{x}_0) \\
&= -\frac{1}{2(1-\alpha_t)} \|\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1}\|^2 - \frac{1}{2(1-\bar{\alpha}_{t-1})} \|\mathbf{x}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0\|^2 + \text{const}
\end{aligned}
$$

ã“ã“ã§ $\mathbf{x}_t$ ã«ä¾å­˜ã—ãªã„é …ã¯å®šæ•°ã¨ã—ã¦ç„¡è¦–ã€‚

**å¹³æ–¹å®Œæˆ**: $\mathbf{x}_{t-1}$ ã«é–¢ã™ã‚‹äºŒæ¬¡å½¢å¼ã«æ•´ç†:

$$
\begin{aligned}
&-\frac{1}{2} \left( \frac{\alpha_t}{1-\alpha_t} + \frac{1}{1-\bar{\alpha}_{t-1}} \right) \mathbf{x}_{t-1}^2 + \left( \frac{\sqrt{\alpha_t}}{1-\alpha_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_0 \right) \mathbf{x}_{t-1}
\end{aligned}
$$

**ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æ¨™æº–å½¢** $\mathcal{N}(\boldsymbol{\mu}, \sigma^2)$ ã¨æ¯”è¼ƒ:

$$
\log \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \sigma^2 \mathbf{I}) \propto -\frac{1}{2\sigma^2} \|\mathbf{x} - \boldsymbol{\mu}\|^2 = -\frac{1}{2\sigma^2} \mathbf{x}^2 + \frac{\boldsymbol{\mu}}{\sigma^2} \mathbf{x}
$$

å¯¾å¿œã•ã›ã¦:

$$
\frac{1}{\tilde{\beta}_t} = \frac{\alpha_t}{1-\alpha_t} + \frac{1}{1-\bar{\alpha}_{t-1}} = \frac{\alpha_t (1-\bar{\alpha}_{t-1}) + (1-\alpha_t)}{(1-\alpha_t)(1-\bar{\alpha}_{t-1})} = \frac{1 - \bar{\alpha}_t}{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}
$$

ã—ãŸãŒã£ã¦:

$$
\boxed{\tilde{\beta}_t = \frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t}
$$

å¹³å‡:

$$
\frac{\tilde{\boldsymbol{\mu}}_t}{\tilde{\beta}_t} = \frac{\sqrt{\alpha_t}}{1-\alpha_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_0
$$

$$
\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_t \right) \cdot \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} + \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_0
$$

ã“ã“ã§ $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}$ ã‚’ä½¿ã†ã¨:

$$
\boxed{\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} \mathbf{x}_t}
$$

**çµè«–**:

$$
q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t \mathbf{I})
$$

**ã“ã‚ŒãŒReverse Processã® "çœŸã®" åˆ†å¸ƒã§ã‚ã‚‹ã€‚** ã ãŒ $\mathbf{x}_0$ ãŒæœªçŸ¥ãªã®ã§ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¿‘ä¼¼ã™ã‚‹:

$$
p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \sigma_t^2 \mathbf{I})
$$

### 3.4 Variational Lower Bound (VLB) å®Œå…¨å±•é–‹

**ç›®æ¨™**: $\log p_\theta(\mathbf{x}_0)$ ã‚’å¤‰åˆ†æ¨è«– (ç¬¬9å›) ã§ä¸‹ç•Œã‹ã‚‰è©•ä¾¡ã™ã‚‹ã€‚

**ELBOå°å‡º** (ç¬¬9å›ã®å¾©ç¿’):

$$
\begin{aligned}
\log p_\theta(\mathbf{x}_0) &= \log \int p_\theta(\mathbf{x}_{0:T}) \, d\mathbf{x}_{1:T} \\
&= \log \int p_\theta(\mathbf{x}_{0:T}) \frac{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \, d\mathbf{x}_{1:T} \\
&= \log \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \right] \\
&\geq \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \right] \quad \text{(Jensenä¸ç­‰å¼)} \\
&= \mathbb{E}_q \left[ \log p_\theta(\mathbf{x}_{0:T}) - \log q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) \right]
\end{aligned}
$$

**åˆ†è§£**:

$$
\begin{aligned}
p_\theta(\mathbf{x}_{0:T}) &= p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \\
q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) &= \prod_{t=1}^T q(\mathbf{x}_t \mid \mathbf{x}_{t-1})
\end{aligned}
$$

ä»£å…¥:

$$
\begin{aligned}
&\mathbb{E}_q \left[ \log p(\mathbf{x}_T) + \sum_{t=1}^T \log p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) - \sum_{t=1}^T \log q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) \right] \\
&= \mathbb{E}_q \left[ \log p(\mathbf{x}_T) - \log q(\mathbf{x}_T \mid \mathbf{x}_0) + \sum_{t=2}^T \log \frac{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)} + \log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1) \right]
\end{aligned}
$$

**ãƒ™ã‚¤ã‚ºã®å®šç†**: $q(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \frac{q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) q(\mathbf{x}_{t-1})}{q(\mathbf{x}_t)}$ ã‚’ä½¿ã†ã¨ã€telescoping:

$$
\sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_t \mid \mathbf{x}_{t-1})} = \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1})}{q(\mathbf{x}_t)} = \log \frac{q(\mathbf{x}_1)}{q(\mathbf{x}_T)}
$$

ä»£ã‚ã‚Šã«ã€**$\mathbf{x}_0$ ã‚’æ¡ä»¶ä»˜ã‘** (Section 3.3):

$$
q(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \to q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
$$

$$
\begin{aligned}
\text{VLB} &= \mathbb{E}_q \left[ \log p(\mathbf{x}_T) + \sum_{t=1}^T \log \frac{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} \right] \\
&= \mathbb{E}_q \left[ \log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T \mid \mathbf{x}_0)} + \sum_{t=2}^T \log \frac{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} + \log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1) \right]
\end{aligned}
$$

**KL divergence ã§æ•´ç†**:

$$
\boxed{L_\text{VLB} = L_T + \sum_{t=2}^T L_{t-1} + L_0}
$$

ã“ã“ã§:

$$
\begin{aligned}
L_T &= D_\text{KL}(q(\mathbf{x}_T \mid \mathbf{x}_0) \| p(\mathbf{x}_T)) \\
L_{t-1} &= D_\text{KL}(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \| p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)) \\
L_0 &= -\log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1)
\end{aligned}
$$

**å„é …ã®æ„å‘³**:

- **$L_T$**: $\mathbf{x}_T$ ãŒ $\mathcal{N}(0, I)$ ã«ã©ã‚Œã ã‘è¿‘ã„ã‹ (å­¦ç¿’ä¸è¦ã€$\beta_t$ ãŒé©åˆ‡ãªã‚‰ $\approx 0$)
- **$L_{t-1}$**: Reverse Process $p_\theta$ ãŒçœŸã®åˆ†å¸ƒ $q$ ã«ã©ã‚Œã ã‘è¿‘ã„ã‹
- **$L_0$**: å†æ§‹æˆé … (VAEã®å†æ§‹æˆæå¤±ã«å¯¾å¿œ)

**ã“ã‚ŒãŒDDPMã®ç†è«–çš„åŸºç›¤ â€” å¤‰åˆ†æ¨è«– (ç¬¬9å›) ã®ç›´æ¥çš„å¿œç”¨ã§ã‚ã‚‹ã€‚**

### 3.5 æå¤±é–¢æ•°ã®3å½¢æ…‹: Îµ / xâ‚€ / v-prediction

**ç›®æ¨™**: $L_{t-1}$ ã‚’å…·ä½“çš„ãªè¨“ç·´æå¤±ã«è½ã¨ã—è¾¼ã‚€ã€‚

**KL divergence**: ä¸¡æ–¹ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãªã®ã§é–‰å½¢å¼:

$$
D_\text{KL}(\mathcal{N}(\boldsymbol{\mu}_1, \Sigma_1) \| \mathcal{N}(\boldsymbol{\mu}_2, \Sigma_2)) = \frac{1}{2} \left( \text{tr}(\Sigma_2^{-1} \Sigma_1) + (\boldsymbol{\mu}_2 - \boldsymbol{\mu}_1)^\top \Sigma_2^{-1} (\boldsymbol{\mu}_2 - \boldsymbol{\mu}_1) - k + \log \frac{|\Sigma_2|}{|\Sigma_1|} \right)
$$

åˆ†æ•£ã‚’å›ºå®š ($\Sigma_1 = \Sigma_2 = \sigma^2 \mathbf{I}$) ã™ã‚‹ã¨ã€å¹³å‡ã®å·®ã ã‘æ®‹ã‚‹:

$$
L_{t-1} \propto \|\tilde{\boldsymbol{\mu}}_t - \boldsymbol{\mu}_\theta\|^2
$$

**3ã¤ã®äºˆæ¸¬æ–¹å¼**:

#### 3.5.1 Îµ-prediction (DDPM [^1])

**$\tilde{\boldsymbol{\mu}}_t$ ã‚’ $\boldsymbol{\epsilon}$ ã§è¡¨ç¾**:

$\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}$ ã‚ˆã‚Š $\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} (\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon})$ã€‚ä»£å…¥:

$$
\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon} \right)
$$

ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒ $\boldsymbol{\epsilon}$ ã‚’äºˆæ¸¬:

$$
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right)
$$

æå¤±:

$$
L_{t-1}^\text{Îµ} = \frac{\beta_t^2}{2\sigma_t^2 \alpha_t (1-\bar{\alpha}_t)} \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2
$$

**ç°¡ç´ åŒ–**: é‡ã¿ $\frac{\beta_t^2}{2\sigma_t^2 \alpha_t (1-\bar{\alpha}_t)}$ ã‚’ç„¡è¦–:

$$
\boxed{L_\text{simple} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \right]}
$$

#### 3.5.2 xâ‚€-prediction

**$\tilde{\boldsymbol{\mu}}_t$ ã‚’ç›´æ¥ $\mathbf{x}_0$ ã§è¡¨ç¾** (Section 3.3):

$$
\tilde{\boldsymbol{\mu}}_t = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} \mathbf{x}_t
$$

ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒ $\mathbf{x}_0$ ã‚’äºˆæ¸¬:

$$
\boldsymbol{\mu}_\theta = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_\theta(\mathbf{x}_t, t) + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} \mathbf{x}_t
$$

æå¤±:

$$
L_{t-1}^{x_0} \propto \|\mathbf{x}_0 - \mathbf{x}_\theta(\mathbf{x}_t, t)\|^2
$$

#### 3.5.3 v-prediction (Progressive Distillation, Salimans & Ho 2022)

**Angular parameterization**: $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}$ ã‚’è§’åº¦ $\phi_t = \arctan(\sqrt{(1-\bar{\alpha}_t)/\bar{\alpha}_t})$ ã§å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã€‚

$$
\mathbf{v} = \sqrt{\bar{\alpha}_t} \boldsymbol{\epsilon} - \sqrt{1-\bar{\alpha}_t} \mathbf{x}_0
$$

æå¤±:

$$
L_t^\mathbf{v} = \|\mathbf{v} - \mathbf{v}_\theta(\mathbf{x}_t, t)\|^2
$$

**åˆ©ç‚¹**: $t$ å…¨ä½“ã§åˆ†æ•£ãŒå‡ä¸€ â†’ è¨“ç·´å®‰å®šã€‚

**3å½¢æ…‹ã®å¤‰æ›**:

$$
\begin{aligned}
\mathbf{x}_0 &= \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}}{\sqrt{\bar{\alpha}_t}} \\
\boldsymbol{\epsilon} &= \frac{\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0}{\sqrt{1-\bar{\alpha}_t}} \\
\mathbf{v} &= \sqrt{\bar{\alpha}_t} \boldsymbol{\epsilon} - \sqrt{1-\bar{\alpha}_t} \mathbf{x}_0
\end{aligned}
$$

```julia
# Conversion between Îµ, xâ‚€, v predictions
function predict_Îµ_from_xâ‚€(x_t::Vector{Float64}, xâ‚€::Vector{Float64}, á¾±_t::Float64)
    Îµ = (x_t - sqrt(á¾±_t) * xâ‚€) / sqrt(1 - á¾±_t)
    return Îµ
end

function predict_xâ‚€_from_Îµ(x_t::Vector{Float64}, Îµ::Vector{Float64}, á¾±_t::Float64)
    xâ‚€ = (x_t - sqrt(1 - á¾±_t) * Îµ) / sqrt(á¾±_t)
    return xâ‚€
end

function predict_v(xâ‚€::Vector{Float64}, Îµ::Vector{Float64}, á¾±_t::Float64)
    v = sqrt(á¾±_t) * Îµ - sqrt(1 - á¾±_t) * xâ‚€
    return v
end

# Test
xâ‚€ = [1.0, 2.0]
Îµ = randn(2)
á¾±_t = 0.5
x_t = sqrt(á¾±_t) * xâ‚€ + sqrt(1 - á¾±_t) * Îµ

Îµ_recon = predict_Îµ_from_xâ‚€(x_t, xâ‚€, á¾±_t)
xâ‚€_recon = predict_xâ‚€_from_Îµ(x_t, Îµ, á¾±_t)
v = predict_v(xâ‚€, Îµ, á¾±_t)

println("Original Îµ: $Îµ")
println("Reconstructed Îµ: $Îµ_recon")
println("Original xâ‚€: $xâ‚€")
println("Reconstructed xâ‚€: $xâ‚€_recon")
println("v: $v")
```

**ã©ã‚Œã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ**

| äºˆæ¸¬å¯¾è±¡ | è¨“ç·´å®‰å®šæ€§ | æ¨è«–å“è³ª | æ¨å¥¨ã‚·ãƒ¼ãƒ³ |
|:---------|:----------|:---------|:----------|
| **Îµ** | é«˜ | é«˜ | **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ** (DDPM [^1]) |
| **xâ‚€** | ä¸­ | ä¸­ | ä½ãƒã‚¤ã‚ºé ˜åŸŸã§æœ‰åŠ¹ |
| **v** | **æœ€é«˜** | é«˜ | **æœ€æ–°æ¨å¥¨** (v-prediction [^5]) |

### 3.6 ç°¡ç´ åŒ–æå¤± L_simple ã¨ VLB ã®é–¢ä¿‚

**DDPM [^1] ã®ç™ºè¦‹**: VLBæå¤± $L_\text{VLB}$ ã®é‡ã¿ä»˜ã‘ã‚’ç„¡è¦–ã—ãŸ $L_\text{simple}$ ã®æ–¹ãŒã€ã‚µãƒ³ãƒ—ãƒ«å“è³ªãŒé«˜ã„ã€‚

$$
L_\text{VLB} = L_T + \sum_{t=2}^T L_{t-1} + L_0, \quad L_\text{simple} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \right]
$$

**ãªãœ $L_\text{simple}$ ãŒå„ªã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ**

1. **é‡ã¿ä»˜ã‘ã®åŠ¹æœ**: $L_{t-1}$ ã®é‡ã¿ $\frac{\beta_t^2}{2\sigma_t^2 \alpha_t (1-\bar{\alpha}_t)}$ ã¯ã€å°ã•ãª $t$ (ä½ãƒã‚¤ã‚º) ã‚’å¼·èª¿ã™ã‚‹ã€‚ã“ã‚ŒãŒçŸ¥è¦šå“è³ªã«æœ‰å®³ã€‚
2. **å…¨æ™‚åˆ»ä¸€æ§˜ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: $L_\text{simple}$ ã¯ $t \sim \text{Uniform}(1, T)$ â†’ å…¨æ™‚åˆ»ã‚’å‡ç­‰ã«å­¦ç¿’ã€‚
3. **å‹¾é…ã®ãƒãƒ©ãƒ³ã‚¹**: VLB ã®é‡ã¿ã¯ç†è«–çš„ã«ã¯æ­£ã—ã„ãŒã€å®Ÿéš›ã«ã¯é«˜ãƒã‚¤ã‚ºé ˜åŸŸã‚’éå­¦ç¿’ã•ã›ã‚‹ã€‚

**çµŒé¨“å‰‡**: å°¤åº¦ (bits/dim) ã‚’æœ€é©åŒ–ã™ã‚‹ãªã‚‰ $L_\text{VLB}$ã€çŸ¥è¦šå“è³ª (FID) ã‚’æœ€é©åŒ–ã™ã‚‹ãªã‚‰ $L_\text{simple}$ã€‚

### 3.7 SNR (Signal-to-Noise Ratio) è¦–ç‚¹ã§ã®çµ±ä¸€çš„ç†è§£

**SNRå®šç¾©**:

$$
\text{SNR}(t) = \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}
$$

- $t=0$: $\text{SNR}(0) = \frac{1}{0}$ (ç„¡é™å¤§ã€ãƒã‚¤ã‚ºãªã—)
- $t=T$: $\text{SNR}(T) \approx 0$ (ä¿¡å·ãªã—)

**Noise Schedule ã®è¨­è¨ˆåŸå‰‡**: $\text{SNR}(t)$ ãŒå˜èª¿æ¸›å°‘ã—ã€$\text{SNR}(T) = 0$ (Zero Terminal SNR [^5])ã€‚

**SNRã¨Weighting ã®é–¢ä¿‚** (Ho+ 2020 [^1] Appendix):

$$
L_\text{VLB} = \mathbb{E}_t \left[ \lambda(t) \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta\|^2 \right], \quad \lambda(t) = \frac{1}{2\sigma_t^2} \frac{\beta_t^2}{\alpha_t (1-\bar{\alpha}_t)}
$$

$\lambda(t) \propto \text{SNR}(t)$ â†’ ä½SNR (é«˜ãƒã‚¤ã‚º) ã®æ™‚åˆ»ã‚’é‡è¦–ã€‚

**$L_\text{simple}$ ã®å†è§£é‡ˆ**: $\lambda(t) = 1$ â†’ SNRã«ä¾ã‚‰ãšå…¨æ™‚åˆ»ã‚’å‡ç­‰ã«é‡è¦–ã€‚

**æœ€æ–°ã®é‡ã¿ä»˜ã‘ã‚¹ã‚­ãƒ¼ãƒ ** (Min-SNR Weighting, Hang+ 2023):

$$
\lambda_\text{min-SNR}(t) = \min(\text{SNR}(t), \gamma)
$$

$\gamma = 5$ ãŒæ¨å¥¨ã€‚é«˜SNR (ä½ãƒã‚¤ã‚º) ã®æ™‚åˆ»ã®é‡ã¿ã‚’åˆ¶é™ â†’ è¨“ç·´å®‰å®šã€‚

```julia
# SNR computation
function compute_snr(á¾±::Vector{Float64})
    snr = á¾± ./ (1.0 .- á¾±)
    return snr
end

# Min-SNR weighting
function min_snr_weight(snr::Vector{Float64}, Î³::Float64=5.0)
    Î» = min.(snr, Î³)
    return Î»
end

Î²_cosine, á¾±_cosine = cosine_schedule(1000)
snr = compute_snr(á¾±_cosine)
Î»_min_snr = min_snr_weight(snr, 5.0)

println("SNR range: [$(minimum(snr)), $(maximum(snr))]")
println("Min-SNR weight range: [$(minimum(Î»_min_snr)), $(maximum(Î»_min_snr))]")
```

### 3.8 U-Net Architecture for DDPM

**U-Net** ã¯ DDPM [^1] ã®æ¨™æº–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚**Time Embedding**ã€**GroupNorm**ã€**Self-Attention** ãŒæ ¸å¿ƒã€‚

#### 3.8.1 Time Embedding

**å‹•æ©Ÿ**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ ã«æ™‚åˆ» $t$ ã‚’å…¥åŠ›ã™ã‚‹ã€‚

**Sinusoidal Position Encoding** (Transformer [Vaswani+ 2017] ã¨åŒã˜):

$$
\text{PE}(t, 2i) = \sin(t / 10000^{2i/d}), \quad \text{PE}(t, 2i+1) = \cos(t / 10000^{2i/d})
$$

$d$ ã¯åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ (é€šå¸¸ $d_\text{model} \times 4$)ã€‚

```julia
# Sinusoidal time embedding
function time_embedding(t::Int, d::Int)
    half_dim = d Ã· 2
    emb = log(10000) / (half_dim - 1)
    emb = exp.(-emb * (0:half_dim-1))
    emb = t * emb
    emb = [sin.(emb); cos.(emb)]
    return emb
end

t = 500
d = 128
t_emb = time_embedding(t, d)
println("Time embedding shape: $(length(t_emb))")
```

**çµ±åˆ**: Time Embedding ã‚’å„ Residual Block ã«åŠ ç®— (FiLM: Feature-wise Linear Modulation)ã€‚

$$
\mathbf{h} = \mathbf{h} + \text{MLP}(\text{TimeEmb}(t))
$$

#### 3.8.2 GroupNorm

**Batch Normalization ã®å•é¡Œ**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ä¾å­˜ â†’ å°ãƒãƒƒãƒã§ä¸å®‰å®šã€‚

**GroupNorm** (Wu & He 2018): ãƒãƒ£ãƒãƒ«ã‚’ $G$ å€‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²ã—ã€ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«æ­£è¦åŒ–ã€‚

$$
\text{GN}(\mathbf{x}) = \gamma \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

$\mu, \sigma$ ã¯ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«è¨ˆç®—ã€‚é€šå¸¸ $G = 32$ã€‚

```julia
# GroupNorm (simplified)
function group_norm(x::Matrix{Float64}, G::Int=32)
    C, N = size(x)  # C: channels, N: spatial
    @assert C % G == 0

    # Reshape: (C, N) â†’ (G, C/G, N)
    x_grouped = reshape(x, G, CÃ·G, N)

    # Normalize per group
    for g in 1:G
        Î¼ = mean(x_grouped[g, :, :])
        ÏƒÂ² = var(x_grouped[g, :, :])
        x_grouped[g, :, :] = (x_grouped[g, :, :] .- Î¼) ./ sqrt(ÏƒÂ² + 1e-5)
    end

    # Reshape back
    x_norm = reshape(x_grouped, C, N)
    return x_norm
end

x = randn(64, 100)  # 64 channels, 100 spatial
x_norm = group_norm(x, 32)
println("GroupNorm applied, mean: $(mean(x_norm)), std: $(std(x_norm))")
```

#### 3.8.3 Self-Attention

**å‹•æ©Ÿ**: ä½è§£åƒåº¦ã®ç‰¹å¾´ãƒãƒƒãƒ—ã§ **é•·è·é›¢ä¾å­˜** ã‚’æ•æ‰ã€‚

**Multi-Head Self-Attention** (ç¬¬14å›):

$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V
$$

U-Netã§ã¯ã€**è§£åƒåº¦ 16Ã—16 ä»¥ä¸‹** ã§ã®ã¿ Attention ã‚’é©ç”¨ (è¨ˆç®—é‡ $O(N^2)$ ã®ãŸã‚)ã€‚

```julia
# Simplified self-attention layer
function self_attention(x::Matrix{Float64}, d_k::Int)
    # x: (d_model, seq_len)
    d_model, seq_len = size(x)

    # Linear projections (simplified: using identity for demo)
    Q = x
    K = x
    V = x

    # Scaled dot-product attention
    scores = (Q' * K) / sqrt(d_k)  # (seq_len, seq_len)
    attn = softmax(scores, dims=2)  # row-wise softmax
    output = V * attn'  # (d_model, seq_len)

    return output
end

softmax(x; dims) = exp.(x .- maximum(x, dims=dims)) ./ sum(exp.(x .- maximum(x, dims=dims)), dims=dims)

x_feature = randn(256, 16*16)  # 256 channels, 16x16 spatial (flattened)
x_attn = self_attention(x_feature, 256)
println("Self-attention output shape: $(size(x_attn))")
```

#### 3.8.4 U-Net å…¨ä½“æ§‹é€ 

```mermaid
graph TD
    A["Input<br/>x_t + TimeEmb(t)"] --> B["DownBlock 1<br/>Conv + GN + SiLU"]
    B --> C["DownBlock 2<br/>+ Self-Attn (16x16)"]
    C --> D["DownBlock 3"]
    D --> E["Bottleneck<br/>+ Self-Attn"]
    E --> F["UpBlock 3<br/>+ Skip from D"]
    F --> G["UpBlock 2<br/>+ Self-Attn + Skip"]
    G --> H["UpBlock 1<br/>+ Skip"]
    H --> I["Output Conv<br/>Îµ_Î¸(x_t, t)"]

    style E fill:#fff9c4
    style I fill:#c8e6c9
```

**Skip Connection**: Encoder ã®ç‰¹å¾´ã‚’ Decoder ã«ç›´æ¥æ¥ç¶š (U-Net ã®åå‰ã®ç”±æ¥)ã€‚

### 3.9 DDIM: æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œå…¨ç‰ˆ

**DDPM [^1] ã®å•é¡Œ**: 1000ã‚¹ãƒ†ãƒƒãƒ— â†’ æ¨è«–ã«æ•°åˆ†ã‹ã‹ã‚‹ã€‚

**DDIM [^2] (Song+ 2020) ã®é©æ–°**: **Non-Markovian forward process** ã§ã€æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã€‚10-50ã‚¹ãƒ†ãƒƒãƒ—ã§åŒç­‰ã®å“è³ªã€‚

#### 3.9.1 Non-Markovian Forward Process

**DDPM**: $q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t \mid \mathbf{x}_{t-1})$ (ãƒãƒ«ã‚³ãƒ•)

**DDIM**: $q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)$ ã¯ **éãƒãƒ«ã‚³ãƒ•** â€” $\mathbf{x}_t$ ã¯ $\mathbf{x}_0$ ã«ç›´æ¥ä¾å­˜ã—ã€$\mathbf{x}_{t-1}$ ã‚’çµŒç”±ã—ãªã„ã€‚

$$
q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1} - \sigma_t^2} \cdot \frac{\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0}{\sqrt{1-\bar{\alpha}_t}}, \sigma_t^2 \mathbf{I})
$$

ã“ã“ã§ $\sigma_t$ ã¯ä»»æ„ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:

- **$\sigma_t = \sqrt{\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}} \sqrt{1-\frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}}$**: DDPM ã¨åŒã˜ (ç¢ºç‡çš„)
- **$\sigma_t = 0$**: æ±ºå®šè«–çš„ (DDIM)

#### 3.9.2 DDIM ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¼

**Reparameterize**: $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ ã‚ˆã‚Š:

$$
\mathbf{x}_0 \approx \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{\bar{\alpha}_t}}
$$

ä»£å…¥:

$$
\boxed{\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \underbrace{\frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{\bar{\alpha}_t}}}_{\text{predicted } \mathbf{x}_0} + \sqrt{1-\bar{\alpha}_{t-1} - \sigma_t^2} \cdot \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) + \sigma_t \boldsymbol{\epsilon}_t}
$$

ã“ã“ã§ $\boldsymbol{\epsilon}_t \sim \mathcal{N}(0, \mathbf{I})$ã€‚

**æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** ($\sigma_t = 0$):

$$
\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_\theta}{\sqrt{\bar{\alpha}_t}} + \sqrt{1-\bar{\alpha}_{t-1}} \boldsymbol{\epsilon}_\theta
$$

**åŠ é€Ÿ**: $T$ ã‚’ $S$ ã‚¹ãƒ†ãƒƒãƒ—ã«ã‚¹ã‚­ãƒƒãƒ— ($\tau_1, \dots, \tau_S \subset \{1, \dots, T\}$)ã€‚

```julia
# DDIM sampling step
function ddim_step(x_t::Vector{Float64}, Îµ_Î¸::Vector{Float64}, t::Int, t_prev::Int, á¾±::Vector{Float64}, Î·::Float64=0.0)
    # Î·: stochasticity parameter (0 = deterministic, 1 = DDPM-like)
    á¾±_t = á¾±[t]
    á¾±_prev = (t_prev > 0) ? á¾±[t_prev] : 1.0

    # Predicted xâ‚€
    xâ‚€_pred = (x_t - sqrt(1 - á¾±_t) * Îµ_Î¸) / sqrt(á¾±_t)

    # Variance
    Ïƒ_t = Î· * sqrt((1 - á¾±_prev) / (1 - á¾±_t)) * sqrt(1 - á¾±_t / á¾±_prev)

    # Direction pointing to x_t
    dir_xt = sqrt(1 - á¾±_prev - Ïƒ_t^2) * Îµ_Î¸

    # Random noise (zero if deterministic)
    noise = (Î· > 0) ? randn(length(x_t)) : zeros(length(x_t))

    # DDIM step
    x_prev = sqrt(á¾±_prev) * xâ‚€_pred + dir_xt + Ïƒ_t * noise

    return x_prev
end

# Test
x_t = randn(2)
Îµ_Î¸ = randn(2)
Î²_cosine, á¾±_cosine = cosine_schedule(1000)

# Deterministic (Î·=0)
x_prev_det = ddim_step(x_t, Îµ_Î¸, 1000, 500, á¾±_cosine, 0.0)
println("Deterministic DDIM: $x_prev_det")

# Stochastic (Î·=1, DDPM-like)
x_prev_sto = ddim_step(x_t, Îµ_Î¸, 1000, 500, á¾±_cosine, 1.0)
println("Stochastic DDIM: $x_prev_sto")
```

**DDIM ã®åˆ©ç‚¹**:

1. **é«˜é€Ÿ**: 50ã‚¹ãƒ†ãƒƒãƒ—ã§ DDPM 1000ã‚¹ãƒ†ãƒƒãƒ—ã¨åŒç­‰ã®å“è³ª
2. **æ±ºå®šè«–çš„**: åŒã˜ $\mathbf{x}_T$ ã‹ã‚‰å¸¸ã«åŒã˜ $\mathbf{x}_0$ (å†ç¾æ€§)
3. **æ½œåœ¨ç©ºé–“è£œé–“**: $\mathbf{x}_T$ ã‚’è£œé–“ â†’ $\mathbf{x}_0$ ã‚’è£œé–“ (Latent Consistency)

#### 3.9.3 DDIM ã¨ Probability Flow ODE ã®é–¢ä¿‚

**Probability Flow ODE** (Song+ 2020 score-based generative models, ç¬¬35å›):

$$
\frac{d\mathbf{x}}{dt} = f(\mathbf{x}, t) - \frac{1}{2} g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x})
$$

**DDIM ã¯ Probability Flow ODE ã® Euleræ³•é›¢æ•£åŒ–** ã«å¯¾å¿œ (ç¬¬38å› Flow Matching ã§è©³è¿°)ã€‚

$$
\mathbf{x}_{t-\Delta t} = \mathbf{x}_t - \Delta t \left[ f(\mathbf{x}_t, t) - \frac{1}{2} g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x}_t) \right]
$$

**ã“ã‚ŒãŒDDIM â†’ Flow Matching â†’ OTçµ±ä¸€ç†è«–ã¸ã®é“ç­‹ã§ã‚ã‚‹ã€‚**

### 3.10 Score-based è¦–ç‚¹ã§ã® DDPM å†è§£é‡ˆ

**Score Matching** (ç¬¬35å›) ã¨ã®ç­‰ä¾¡æ€§:

$$
\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \mid \mathbf{x}_0) = - \frac{\boldsymbol{\epsilon}}{\sqrt{1-\bar{\alpha}_t}}
$$

**è¨¼æ˜**:

$$
\begin{aligned}
\log q(\mathbf{x}_t \mid \mathbf{x}_0) &= \log \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I}) \\
&= -\frac{1}{2(1-\bar{\alpha}_t)} \|\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0\|^2 + \text{const}
\end{aligned}
$$

$$
\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \mid \mathbf{x}_0) = -\frac{1}{1-\bar{\alpha}_t} (\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0) = - \frac{\boldsymbol{\epsilon}}{\sqrt{1-\bar{\alpha}_t}}
$$

ã“ã“ã§ $\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0 = \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}$ã€‚â– 

**Score Network ã¨ã®å¯¾å¿œ**:

$$
\mathbf{s}_\theta(\mathbf{x}_t, t) = \nabla_{\mathbf{x}_t} \log p_\theta(\mathbf{x}_t) \approx - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1-\bar{\alpha}_t}}
$$

**ã¤ã¾ã‚Šã€ãƒã‚¤ã‚ºäºˆæ¸¬ = ã‚¹ã‚³ã‚¢äºˆæ¸¬ (rescaled)**ã€‚

**Denoising Score Matching** (ç¬¬35å›) ã®æå¤±:

$$
L_\text{DSM} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \lambda(t) \left\| \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \mid \mathbf{x}_0) - \mathbf{s}_\theta(\mathbf{x}_t, t) \right\|^2 \right]
$$

$\lambda(t) = (1-\bar{\alpha}_t)$ ã¨ã™ã‚‹ã¨:

$$
L_\text{DSM} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \right] = L_\text{simple}
$$

**çµè«–**: **DDPM = Denoising Score Matching**ã€‚DDPMã¯Score-based Generative Modelsã®ä¸€å½¢æ…‹ã§ã‚ã‚‹ã€‚

**Song & Ho ã®çµ±ä¸€ç†è«–** (ç¬¬38å›ã§å®Œå…¨è¨¼æ˜):

```mermaid
graph TD
    A[Score-based<br/>âˆ‡log p] --> B[DDPM<br/>Îµ-prediction]
    B --> C[DDIM<br/>PF-ODE]
    C --> D[Flow Matching<br/>OT-CFM]

    A -.Denoising Score.-> B
    B -.Deterministic.-> C
    C -.Continuous.-> D

    style A fill:#e3f2fd
    style B fill:#fff9c4
    style D fill:#c8e6c9
```

**ã“ã‚Œã§ Zone 3 å®Œäº† â€” DDPM ã®ç†è«–ã‚’å®Œå…¨å°å‡ºã—ãŸã€‚**

:::message
**é€²æ—: 50% å®Œäº†** Forward/Reverse/VLB/3å½¢æ…‹/SNR/U-Net/DDIM/Score-based ã‚’å®Œå…¨å°å‡ºã—ãŸã€‚Boss Battle æ’ƒç ´ã€‚Zone 4 ã§å®Ÿè£…ã¸ã€‚
:::

---
