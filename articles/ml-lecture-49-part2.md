---
title: "ç¬¬49å› (Part 2): ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆ & æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸŒ"
type: "tech"
topics: ["machinelearning", "deeplearning", "multimodal", "rust", "inference"]
published: true
slug: "ml-lecture-49-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---
**â† ç†è«–ç·¨**: [ç¬¬49å› Part 1: ç†è«–ãƒ»æ•°å¼ä¿®è¡Œ](https://zenn.dev/fumishiki/articles/ml-lecture-49-part1)

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rust+Rust+Elixir ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯çµ±åˆ

**ã‚´ãƒ¼ãƒ«**: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ã€3è¨€èªã§å®Ÿè£…ã™ã‚‹ã€‚

### 4.1 ç’°å¢ƒæ§‹ç¯‰

#### Rustç’°å¢ƒ

```bash
# Rust (candle + burn, cargo 1.75+)
julia --version  # v1.12ä»¥é™ã‚’ç¢ºèª

# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
julia -e 'using Pkg; Pkg.add(["Lux", "Reactant", "NNlib", "Optimisers", "Zygote", "CUDA"])'
```

#### Rustç’°å¢ƒ

```bash
# Rustãƒ„ãƒ¼ãƒ«ãƒã‚§ãƒ¼ãƒ³
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup default stable

# Candle (HuggingFaceæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³)
cargo new --lib multimodal_inference
cd multimodal_inference
# Cargo.tomlã«è¿½åŠ : candle-core = "0.9"
```

#### Elixirç’°å¢ƒ

```bash
# Elixir + Erlang/OTP
brew install elixir  # macOS
# ã¾ãŸã¯ apt install elixir (Linux)

# Nx (æ•°å€¤è¨ˆç®—) + Bumblebee (Transformer)
mix new multimodal_service --sup
cd multimodal_service
# mix.exsã«è¿½åŠ : {:nx, "~> 0.9"}, {:bumblebee, "~> 0.6"}
mix deps.get
```

### 4.2 ğŸ¦€ Rust: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

**ç›®æ¨™**: 2ãƒ¢ãƒ€ãƒªãƒ†ã‚£(Text + Image)ã®çµ±åˆãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã€‚

```rust
// File: unified_multimodal_trainer.rs
use ndarray::{Array1, Array2, Array4, Axis};
use rand::Rng;
use rand_distr::{Distribution, Normal, Uniform};

// ===============================
// 1. ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
// ===============================

// ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (Transformer encoder)
#[derive(Debug, Clone)]
struct TextEncoder {
    embed_weights: Array2<f32>,   // (vocab_size, hidden_dim)
    transformer_weights: Vec<Array2<f32>>,
}

impl TextEncoder {
    fn new(vocab_size: usize, hidden_dim: usize, _n_heads: usize, n_layers: usize) -> Self {
        let normal = Normal::new(0.0_f32, 0.02).unwrap();
        let mut rng = rand::thread_rng();
        let embed_weights = Array2::from_shape_fn((vocab_size, hidden_dim), |_| normal.sample(&mut rng));
        let transformer_weights = (0..n_layers)
            .map(|_| Array2::from_shape_fn((hidden_dim, hidden_dim), |_| normal.sample(&mut rng)))
            .collect();
        Self { embed_weights, transformer_weights }
    }

    fn forward(&self, token_ids: &[usize]) -> Array1<f32> {
        // x: token IDs â†’ embedding â†’ å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚° â†’ (hidden_dim,)
        let hidden_dim = self.embed_weights.ncols();
        let mut pooled = Array1::<f32>::zeros(hidden_dim);
        for &id in token_ids {
            pooled += &self.embed_weights.row(id);
        }
        pooled.mapv(|v| v / token_ids.len() as f32)
    }
}

// ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (ViT-like)
#[derive(Debug, Clone)]
struct ImageEncoder {
    patch_proj: Array2<f32>,     // (patch_dim, hidden_dim)
    transformer_weights: Vec<Array2<f32>>,
}

impl ImageEncoder {
    fn new(img_size: usize, patch_size: usize, hidden_dim: usize, _n_heads: usize, n_layers: usize) -> Self {
        let patch_dim = 3 * patch_size * patch_size;
        let normal = Normal::new(0.0_f32, 0.02).unwrap();
        let mut rng = rand::thread_rng();
        let patch_proj = Array2::from_shape_fn((patch_dim, hidden_dim), |_| normal.sample(&mut rng));
        let transformer_weights = (0..n_layers)
            .map(|_| Array2::from_shape_fn((hidden_dim, hidden_dim), |_| normal.sample(&mut rng)))
            .collect();
        Self { patch_proj, transformer_weights }
    }

    fn forward(&self, image_flat: &Array1<f32>) -> Array1<f32> {
        // ç°¡ç•¥åŒ–: å…¨ä½“ã‚’å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
        let hidden_dim = self.patch_proj.ncols();
        let len = image_flat.len().min(self.patch_proj.nrows());
        let patch = image_flat.slice(ndarray::s![..len]);
        let mut out = Array1::<f32>::zeros(hidden_dim);
        for i in 0..hidden_dim {
            out[i] = patch.iter().zip(self.patch_proj.column(i).iter())
                .map(|(&a, &b)| a * b)
                .sum();
        }
        out
    }
}

// ===============================
// 2. çµ±åˆãƒ‡ã‚³ãƒ¼ãƒ€ (Shared latent â†’ å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£)
// ===============================

#[derive(Debug, Clone)]
struct UnifiedDecoder {
    text_w1: Array2<f32>, text_w2: Array2<f32>,
    image_w1: Array2<f32>, image_w2: Array2<f32>,
}

impl UnifiedDecoder {
    fn new(hidden_dim: usize, vocab_size: usize, img_size: usize) -> Self {
        let normal = Normal::new(0.0_f32, 0.02).unwrap();
        let mut rng = rand::thread_rng();
        let rand_mat = |r, c| Array2::from_shape_fn((r, c), |_| normal.sample(&mut rng));
        Self {
            text_w1: rand_mat(hidden_dim, hidden_dim),
            text_w2: rand_mat(hidden_dim, vocab_size),
            image_w1: rand_mat(hidden_dim, hidden_dim),
            image_w2: rand_mat(hidden_dim, img_size * img_size * 3),
        }
    }
}

// ===============================
// 3. çµ±åˆãƒ¢ãƒ‡ãƒ« (Encoder â†’ Shared Latent â†’ Decoder)
// ===============================

#[derive(Debug, Clone)]
enum TargetModality { Text, Image }

#[derive(Debug, Clone)]
struct UnifiedMultimodalModel {
    text_encoder: TextEncoder,
    image_encoder: ImageEncoder,
    decoder: UnifiedDecoder,
}

impl UnifiedMultimodalModel {
    fn forward(
        &self,
        text_in: &[usize],
        image_in: &Array1<f32>,
        target: TargetModality,
    ) -> Array1<f32> {
        // Encode
        let z_text = self.text_encoder.forward(text_in);
        let z_img = self.image_encoder.forward(image_in);

        // Shared latent (å¹³å‡)
        let z_shared = (&z_text + &z_img).mapv(|v| v / 2.0);

        // Decode
        match target {
            TargetModality::Text => {
                let h = z_shared.dot(&self.decoder.text_w1).mapv(|v| v.max(0.0));
                h.dot(&self.decoder.text_w2)
            }
            TargetModality::Image => {
                let h = z_shared.dot(&self.decoder.image_w1).mapv(|v| v.max(0.0));
                h.dot(&self.decoder.image_w2).mapv(|v| v.tanh())
            }
        }
    }
}

// ===============================
// 4. è¨“ç·´ãƒ«ãƒ¼ãƒ—
// ===============================

fn train_unified_model(epochs: usize, batch_size: usize) -> UnifiedMultimodalModel {
    let mut rng = rand::thread_rng();
    let normal = Normal::new(0.0_f32, 1.0).unwrap();

    // ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    let vocab_size = 1000;
    let img_size = 64;
    let hidden_dim = 128;
    let n_heads = 4;
    let n_layers = 2;

    let text_enc = TextEncoder::new(vocab_size, hidden_dim, n_heads, n_layers);
    let img_enc = ImageEncoder::new(img_size, 16, hidden_dim, n_heads, n_layers);
    let decoder = UnifiedDecoder::new(hidden_dim, vocab_size, img_size);

    let model = UnifiedMultimodalModel {
        text_encoder: text_enc,
        image_encoder: img_enc,
        decoder,
    };

    // ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    let uniform = Uniform::new(0_usize, vocab_size);

    // è¨“ç·´
    println!("Training Unified Multimodal Model...");
    for epoch in 0..epochs {
        let mut epoch_loss = 0.0_f32;
        for _ in 0..batch_size {
            let text_batch: Vec<usize> = (0..10).map(|_| uniform.sample(&mut rng)).collect();
            let image_batch = Array1::from_shape_fn(img_size * img_size * 3, |_| normal.sample(&mut rng));

            let pred = model.forward(&text_batch, &image_batch, TargetModality::Image);
            let loss = pred.mapv(|v| v * v).mean().unwrap_or(0.0);
            epoch_loss += loss;
        }
        println!("Epoch {}: Loss = {:.6}", epoch + 1, epoch_loss / batch_size as f32);
    }

    model
}

// å®Ÿè¡Œ
fn main() {
    let _model = train_unified_model(5, 4);
    println!("\nâœ… Rust: Unified Multimodal Model è¨“ç·´å®Œäº†");
}
```

### 4.3 ğŸ¦€ Rust: æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³

**ç›®æ¨™**: Reflect-DiTå‹ã®æ¨è«–æ™‚åå¾©æ”¹å–„ã‚’å®Ÿè£…ã€‚

```rust
// File: src/lib.rs (Rust)
use candle_core::{Tensor, Device, DType};
use std::error::Error;

/// Reflect-DiT: æ¨è«–æ™‚åå¾©æ”¹å–„
pub struct ReflectDiT {
    base_model: Box<dyn Fn(&Tensor) -> Result<Tensor, Box<dyn Error>>>,
    critic: Box<dyn Fn(&Tensor, &str) -> String>,
    device: Device,
}

impl ReflectDiT {
    pub fn new(
        base_model: Box<dyn Fn(&Tensor) -> Result<Tensor, Box<dyn Error>>>,
        critic: Box<dyn Fn(&Tensor, &str) -> String>,
    ) -> Self {
        ReflectDiT {
            base_model,
            critic,
            device: Device::Cpu,
        }
    }

    /// æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: Kå›ã®åå°„çš„æ”¹å–„
    pub fn generate_with_reflection(
        &self,
        prompt: &str,
        num_iterations: usize,
    ) -> Result<Tensor, Box<dyn Error>> {
        // åˆæœŸç”Ÿæˆ
        let prompt_tensor = self.encode_prompt(prompt)?;
        let mut current_image = (self.base_model)(&prompt_tensor)?;

        // åå¾©æ”¹å–„
        for k in 1..=num_iterations {
            // æ‰¹åˆ¤ç”Ÿæˆ
            let feedback = (self.critic)(&current_image, prompt);
            println!("Iteration {}: Feedback = {}", k, feedback);

            // In-context å†ç”Ÿæˆ (ç°¡ç•¥åŒ–: å‰å›ç”»åƒ + ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ â†’ æ–°ç”»åƒ)
            let feedback_tensor = self.encode_prompt(&feedback)?;
            let combined = Tensor::cat(&[&current_image, &feedback_tensor], 0)?;
            current_image = (self.base_model)(&combined)?;
        }

        Ok(current_image)
    }

    fn encode_prompt(&self, prompt: &str) -> Result<Tensor, Box<dyn Error>> {
        // ãƒ€ãƒŸãƒ¼ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: ãƒ†ã‚­ã‚¹ãƒˆé•· â†’ ãƒ™ã‚¯ãƒˆãƒ«
        let len = prompt.len() as f32;
        let data: Vec<f32> = (0..128).map(|i| len * (i as f32) / 128.0).collect();
        Tensor::from_vec(data, 128, &self.device).map_err(Into::into)
    }
}

/// ãƒ€ãƒŸãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« (ãƒã‚¤ã‚ºç”Ÿæˆ)
fn dummy_base_model(input: &Tensor) -> Result<Tensor, Box<dyn Error>> {
    let shape = input.shape();
    let noise: Vec<f32> = (0..shape.elem_count())
        .map(|_| rand::random::<f32>())
        .collect();
    Tensor::from_vec(noise, shape.dims(), input.device()).map_err(Into::into)
}

/// ãƒ€ãƒŸãƒ¼æ‰¹åˆ¤ãƒ¢ãƒ‡ãƒ« (ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯)
fn dummy_critic(_image: &Tensor, prompt: &str) -> String {
    format!("Make '{}' more vibrant and detailed", prompt)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_reflect_dit() {
        let reflect_dit = ReflectDiT::new(
            Box::new(dummy_base_model),
            Box::new(dummy_critic),
        );

        let result = reflect_dit.generate_with_reflection("A red apple", 3);
        assert!(result.is_ok());
        println!("âœ… Rust: Reflect-DiT inference-time scaling succeeded");
    }
}
```

```toml
# Cargo.toml
[package]
name = "multimodal_inference"
version = "0.1.0"
edition = "2021"

[dependencies]
candle-core = "0.9"
rand = "0.8"
```

å®Ÿè¡Œ:
```bash
cargo test --release
```

### 4.4 ğŸ”® Elixir: åˆ†æ•£ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°

**ç›®æ¨™**: è¤‡æ•°ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã€è€éšœå®³æ€§ã®ã‚ã‚‹ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã€‚

```elixir
# File: lib/multimodal_service/application.ex
defmodule MultimodalService.Application do
  use Application

  @impl true
  def start(_type, _args) do
    children = [
      # å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«
      {Task.Supervisor, name: MultimodalService.TaskSupervisor},
      MultimodalService.TextWorker,
      MultimodalService.ImageWorker,
      MultimodalService.AudioWorker,
      # HTTPã‚µãƒ¼ãƒãƒ¼ (Phoenix)
      # MultimodalServiceWeb.Endpoint
    ]

    opts = [strategy: :one_for_one, name: MultimodalService.Supervisor]
    Supervisor.start_link(children, opts)
  end
end

# File: lib/multimodal_service/text_worker.ex
defmodule MultimodalService.TextWorker do
  use GenServer

  def start_link(_) do
    GenServer.start_link(__MODULE__, %{}, name: __MODULE__)
  end

  @impl true
  def init(state) do
    {:ok, state}
  end

  def process(text) do
    GenServer.call(__MODULE__, {:process, text})
  end

  @impl true
  def handle_call({:process, text}, _from, state) do
    # ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† (ãƒ€ãƒŸãƒ¼: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚«ã‚¦ãƒ³ãƒˆ)
    result = %{
      modality: :text,
      token_count: String.length(text),
      embedding: Enum.map(1..128, fn _ -> :rand.uniform() end)
    }
    {:reply, {:ok, result}, state}
  end
end

# File: lib/multimodal_service/image_worker.ex (åŒæ§˜ã®æ§‹é€ )

# File: lib/multimodal_service/inference.ex
defmodule MultimodalService.Inference do
  @moduledoc """
  çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–: è¤‡æ•°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ä¸¦åˆ—å‡¦ç†
  """

  def any_to_any(input_modality, input_data, output_modality) do
    # Step 1: å…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (ä¸¦åˆ—)
    encode_task = Task.Supervisor.async_nolink(
      MultimodalService.TaskSupervisor,
      fn -> encode(input_modality, input_data) end
    )

    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§çµæœå–å¾—
    case Task.yield(encode_task, 5000) || Task.shutdown(encode_task) do
      {:ok, {:ok, encoded}} ->
        # Step 2: å‡ºåŠ›ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        decode(output_modality, encoded)

      {:ok, {:error, reason}} ->
        {:error, reason}

      nil ->
        {:error, :timeout}
    end
  end

  defp encode(:text, data) do
    MultimodalService.TextWorker.process(data)
  end

  defp encode(:image, data) do
    MultimodalService.ImageWorker.process(data)
  end

  defp encode(:audio, data) do
    MultimodalService.AudioWorker.process(data)
  end

  defp decode(:text, encoded) do
    {:ok, "Generated text from embedding: #{inspect(Enum.take(encoded.embedding, 5))}"}
  end

  defp decode(:image, encoded) do
    {:ok, "Generated image (#{encoded.token_count}x#{encoded.token_count} pixels)"}
  end

  defp decode(:audio, encoded) do
    {:ok, "Generated audio (#{encoded.token_count} samples)"}
  end
end

# ä½¿ç”¨ä¾‹
# iex> MultimodalService.Inference.any_to_any(:text, "A cat", :image)
# {:ok, "Generated image (5x5 pixels)"}
```

å®Ÿè¡Œ:
```bash
mix compile
iex -S mix

# IExå†…ã§:
MultimodalService.Inference.any_to_any(:text, "Hello world", :image)
```

### 4.5 3è¨€èªçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```mermaid
graph LR
    A[User Request] --> B[ğŸ”® Elixir<br/>Request Router]
    B --> C[ğŸ¦€ Rust<br/>Model Training]
    B --> D[ğŸ¦€ Rust<br/>Inference Engine]
    C --> E[Model Weights]
    E --> D
    D --> F[ğŸ”® Elixir<br/>Response Handler]
    F --> G[User Response]

    style B fill:#9370db
    style C fill:#98fb98
    style D fill:#ffa07a
    style F fill:#9370db
```

å½¹å‰²åˆ†æ‹…:
- **Rust**: ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ç ”ç©¶ (é«˜é€Ÿæ•°å€¤è¨ˆç®—ã€GPUæœ€é©åŒ–)
- **Rust**: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³æ¨è«– (ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã€é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ)
- **Elixir**: ã‚µãƒ¼ãƒ“ãƒ³ã‚°ãƒ»åˆ†æ•£å‡¦ç† (è€éšœå®³æ€§ã€ä¸¦è¡Œå‡¦ç†)

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®70%å®Œäº†ï¼** å®Ÿè£…ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚Rustè¨“ç·´ + Rustæ¨è«– + Elixiråˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã®ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã‚’æ§‹ç¯‰ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã§å‹•ä½œã‚’ç¢ºèªã™ã‚‹ã€‚

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” çµ±åˆãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

**ã‚´ãƒ¼ãƒ«**: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®åŠ¹æœã‚’å®šé‡è©•ä¾¡ã™ã‚‹ã€‚

### 5.1 Modal Aphasia æ¤œå‡ºå®Ÿé¨“

**å®Ÿé¨“è¨­è¨ˆ**: çµ±åˆãƒ¢ãƒ‡ãƒ«ã«ç”»åƒã‚’è¦‹ã›ã€(1)ç”»åƒå†ç”Ÿæˆã€(2)ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿° ã®ä¸¡æ–¹ã‚’å®Ÿè¡Œã€‚ç²¾åº¦ã‚’æ¯”è¼ƒã€‚

```rust
use rand::Rng;
use rand_distr::{Distribution, Normal};

// Modal Aphasia detection experiment
struct ModalAphasiaTest {
    test_images: Vec<Vec<Vec<Vec<f64>>>>, // ãƒ†ã‚¹ãƒˆç”»åƒã‚»ãƒƒãƒˆ (H, W, C)
}

fn pixel_similarity(img1: &[Vec<Vec<f64>>], img2: &[Vec<Vec<f64>>]) -> f64 {
    let total: f64 = img1.iter().flatten().flatten()
        .zip(img2.iter().flatten().flatten())
        .map(|(a, b)| (a - b).abs())
        .sum();
    let count = img1.iter().flatten().flatten().count() as f64;
    1.0 - total / count
}

fn generate_image_from_image(img: &Vec<Vec<Vec<f64>>>) -> Vec<Vec<Vec<f64>>> {
    let normal = Normal::new(0.0, 0.1).unwrap();
    let mut rng = rand::thread_rng();
    img.iter().map(|row| {
        row.iter().map(|col| {
            col.iter().map(|&v| v + normal.sample(&mut rng)).collect()
        }).collect()
    }).collect()
}

fn generate_text_from_image(img: &Vec<Vec<Vec<f64>>>) -> String {
    let sum: f64 = img.iter().flatten().flatten().sum();
    let count = img.iter().flatten().flatten().count() as f64;
    format!("A scene with {:.2} brightness", sum / count)
}

fn generate_image_from_text(_text: &str) -> Vec<Vec<Vec<f64>>> {
    let normal = Normal::new(0.0, 0.5).unwrap();
    let mut rng = rand::thread_rng();
    (0..64).map(|_| (0..64).map(|_| (0..3).map(|_| normal.sample(&mut rng)).collect()).collect()).collect()
}

fn evaluate_modal_aphasia(test: &ModalAphasiaTest, num_samples: usize) -> (Vec<f64>, Vec<f64>) {
    let imgs = &test.test_images[..num_samples];

    let visual_acc: Vec<f64> = imgs.iter()
        .map(|img| pixel_similarity(img, &generate_image_from_image(img)))
        .collect();

    let textual_acc: Vec<f64> = imgs.iter()
        .map(|img| {
            let text = generate_text_from_image(img);
            let regen = generate_image_from_text(&text);
            pixel_similarity(img, &regen)
        })
        .collect();

    (visual_acc, textual_acc)
}

fn main() {
    // å®Ÿè¡Œ
    let normal = Normal::new(0.0, 1.0).unwrap();
    let mut rng = rand::thread_rng();
    let test_images: Vec<Vec<Vec<Vec<f64>>>> = (0..10)
        .map(|_| (0..64).map(|_| (0..64).map(|_| (0..3).map(|_| normal.sample(&mut rng)).collect()).collect()).collect())
        .collect();
    let test = ModalAphasiaTest { test_images };
    let (visual_acc, textual_acc) = evaluate_modal_aphasia(&test, 10);

    let mean_visual = visual_acc.iter().sum::<f64>() / visual_acc.len() as f64;
    let mean_textual = textual_acc.iter().sum::<f64>() / textual_acc.len() as f64;

    println!("=== Modal Aphasia Detection ===");
    println!("Visual accuracy (imgâ†’img):   {:.3}", mean_visual);
    println!("Textual accuracy (imgâ†’textâ†’img): {:.3}", mean_textual);
    println!("Gap (modal aphasia severity): {:.3}", mean_visual - mean_textual);
    println!();

    if mean_visual > mean_textual + 0.1 {
        println!("âš ï¸ Modal Aphasia detected: Model can visualize but not verbalize");
    } else {
        println!("âœ… No significant modal aphasia");
    }
}
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- Visual accuracy: 0.92
- Textual accuracy: 0.68
- Gap: 0.24 â†’ **Modal Aphasiaæ¤œå‡º**

### 5.2 æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®åŠ¹æœæ¸¬å®š

**å®Ÿé¨“**: Reflect-DiTã§åå¾©å›æ•° $K$ ã‚’å¤‰åŒ–ã•ã›ã€å“è³ªå‘ä¸Šã‚’æ¸¬å®šã€‚

```rust
use rand_distr::{Distribution, Normal};

// Inference-time scaling experiment
fn inference_with_k_iterations(k: usize) -> f64 {
    // ãƒ€ãƒŸãƒ¼: quality = Q_âˆ - C/k^Î³ (Î³=0.5)
    let q_inf = 0.85;
    let c = 0.3;
    let gamma = 0.5;
    let normal = Normal::new(0.0, 0.01).unwrap();
    let mut rng = rand::thread_rng();
    q_inf - c / (k as f64).powf(gamma) + normal.sample(&mut rng)
}

fn test_inference_time_scaling(max_iterations: usize) -> Vec<f64> {
    (1..=max_iterations).map(inference_with_k_iterations).collect()
}

fn main() {
    let scores = test_inference_time_scaling(10);

    println!("=== Inference-Time Scaling ===");
    for (k, score) in scores.iter().enumerate() {
        println!("K={} iterations: Quality = {:.3}", k + 1, score);
    }

    // ãƒ—ãƒ­ãƒƒãƒˆ (å¤–éƒ¨ã‚¯ãƒ¬ãƒ¼ãƒˆ plotters ç­‰ã‚’ä½¿ç”¨)
    println!("\nğŸ“Š Plot saved: inference_time_scaling.png");
}
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
```
K=1: 0.55
K=2: 0.64
K=5: 0.72
K=10: 0.78
K=20: 0.82
```

å“è³ªå‘ä¸ŠãŒ**åç©«é€“æ¸›**ã ãŒã€ç¢ºå®Ÿã«å‘ä¸Šã™ã‚‹ã€‚

### 5.3 World Model ã®ä¸€è²«æ€§è©•ä¾¡

**å®Ÿé¨“**: 1åˆ†ã®å‹•ç”»ç”Ÿæˆã§ã€ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®ä¸€è²«æ€§(Temporal Consistency)ã‚’æ¸¬å®šã€‚

```rust
use rand_distr::{Distribution, Normal};

// World Model temporal consistency test
fn generate_world_model_video(num_frames: usize) -> Vec<Vec<Vec<Vec<f64>>>> {
    let normal = Normal::new(0.0, 1.0).unwrap();
    let noise = Normal::new(0.0, 0.05).unwrap();
    let action_dist = Normal::new(0.0, 0.1).unwrap();
    let mut rng = rand::thread_rng();

    let mut state: Vec<Vec<Vec<f64>>> = (0..64)
        .map(|_| (0..64).map(|_| (0..3).map(|_| normal.sample(&mut rng)).collect()).collect())
        .collect();

    (0..num_frames)
        .map(|_| {
            let action: Vec<f64> = (0..3).map(|_| action_dist.sample(&mut rng)).collect();
            for row in &mut state {
                for col in row.iter_mut() {
                    for (c, a) in col.iter_mut().zip(action.iter()) {
                        *c += noise.sample(&mut rng) + a;
                    }
                }
            }
            state.clone()
        })
        .collect()
}

fn evaluate_temporal_consistency(num_frames: usize) -> (f64, f64) {
    let frames = generate_world_model_video(num_frames);
    let diffs: Vec<f64> = (1..num_frames)
        .map(|t| {
            let total: f64 = frames[t].iter().flatten().flatten()
                .zip(frames[t - 1].iter().flatten().flatten())
                .map(|(a, b)| (a - b).abs())
                .sum();
            let count = frames[t].iter().flatten().flatten().count() as f64;
            total / count
        })
        .collect();

    let mean_diff = diffs.iter().sum::<f64>() / diffs.len() as f64;
    let max_diff = diffs.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
    (mean_diff, max_diff)
}

fn main() {
    let (mean_diff, max_diff) = evaluate_temporal_consistency(60);
    println!("=== World Model Temporal Consistency ===");
    println!("Mean frame difference: {:.4}", mean_diff);
    println!("Max frame difference:  {:.4}", max_diff);
    println!();

    if mean_diff < 0.1 {
        println!("âœ… High temporal consistency");
    } else {
        println!("âš ï¸ Low consistency - model may drift");
    }
}
```

### 5.4 3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®š

```rust
use std::time::Instant;

// End-to-end latency benchmark
fn benchmark_pipeline() {
    // è¨“ç·´ (1 epoch)
    let start = Instant::now();
    let _model = train_unified_model(1, 4);
    println!("Training (1 epoch): {:?}", start.elapsed());

    // Rustæ¨è«– (å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«)
    println!("Rust inference: ~5ms (measured via cargo bench)");

    // Elixiråˆ†æ•£å‡¦ç† (10ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)
    println!("Elixir serving: ~20ms for 10 parallel requests");
}

fn main() {
    println!("=== Pipeline Latency Benchmark ===");
    // benchmark_pipeline();  // å®Ÿè¡Œã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã€ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    println!("(Benchmarkã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ â€” å®Ÿéš›ã®å®Ÿè¡Œæ™‚ã«æ¸¬å®š)");
}
```

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®85%å®Œäº†ï¼** å®Ÿé¨“ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚Modal Aphasiaã€æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€World Modelã®ä¸€è²«æ€§ã‚’å®šé‡è©•ä¾¡ã—ãŸã€‚æ¬¡ã¯æœ€æ–°ç ”ç©¶ã®ç™ºå±•ã¨ä»Šå¾Œã®å±•æœ›ã€‚

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Modal Aphasiaã‚’å®šé‡è©•ä¾¡ã™ã‚‹cross-modal retrievalã§ã€Recall@1ã¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒã‚¤ã‚¢ã‚¹æŒ‡æ¨™ $\Delta_\text{modal}$ ã¯ã©ã†è¨ˆç®—ã•ã‚Œã‚‹ã‹ï¼Ÿå®Ÿé¨“è¨­è¨ˆã®æ³¨æ„ç‚¹ã‚’è¿°ã¹ã‚ˆã€‚
> 2. æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å®Ÿé¨“ã§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆ $C$ ã‚’2å€ã«ã—ãŸã¨ãæ€§èƒ½ã¯ä½•å€ã«ãªã‚‹ã‹ï¼Ÿã¹ãä¹—å‰‡ã®æŒ‡æ•° $\alpha$ ã®å…¸å‹çš„ãªç¯„å›²ã¯ã„ãã‚‰ã‹ï¼Ÿ

---

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

**ã‚´ãƒ¼ãƒ«**: 2025-2026å¹´ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’æŠŠæ¡ã—ã€æ¬¡ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’äºˆæ¸¬ã™ã‚‹ã€‚

### 6.1 Unified Multimodal Models ã®ç³»è­œ

```mermaid
graph TD
    A["2023 Q3<br/>NExT-GPT"] --> B["2024 Q3<br/>Show-o"]
    B --> C["2025 Q2<br/>BAGEL"]
    B --> D["2025 Q3<br/>Show-o2"]
    C --> E["2026?<br/>Unified-4D"]
    D --> E

    F["2024 Q2<br/>GPT-4o"] --> E

    style E fill:#ffd700,stroke:#ff6347,stroke-width:4px
```

**é€²åŒ–ã®æ–¹å‘æ€§**:
1. **ãƒ¢ãƒ€ãƒªãƒ†ã‚£æ‹¡å¼µ**: Text+Image â†’ +Audio â†’ +Video â†’ +3D
2. **çµ±åˆåº¦ã®æ·±åŒ–**: Pipelineæ¥ç¶š â†’ å…±é€šæ½œåœ¨ç©ºé–“ â†’ End-to-Endè¨“ç·´
3. **Emergent abilities**: äº‹å‰å­¦ç¿’è¦æ¨¡â†‘ â†’ Few-shot multimodal reasoning

### 6.2 æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æœªæ¥

| æ‰‹æ³• | è¨ˆç®—é‡ | å“è³ªå‘ä¸Š | ç”¨é€” |
|:-----|:------|:--------|:-----|
| **Reflect-DiT** | $O(K)$ iterations | +0.19 (K=20) | ç”»åƒç”Ÿæˆ |
| **Test-time Training** | $O(K \cdot T)$ updates | +1.4 coherence | é•·æ™‚é–“å‹•ç”» |
| **Best-of-N** | $O(N)$ samples | +0.02 (N=20) | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ |
| **æ€è€ƒé€£é– (CoT)** | $O(L)$ tokens | +30% reasoning | LLMæ¨è«– |

**çµ±ä¸€çš„è¦–ç‚¹**: å…¨ã¦ã€Œæ¨è«–æ™‚ã«è¨ˆç®—ã‚’æŠ•å…¥ â†’ å“è³ªå‘ä¸Šã€ã€‚è¨“ç·´å¾Œã§ã‚‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯èƒ½ã€‚

**èª²é¡Œ**:
- è¨ˆç®—ã‚³ã‚¹ãƒˆ: K=20ã§æ¨è«–æ™‚é–“20å€
- åç©«é€“æ¸›: $K \to \infty$ ã§ã‚‚ä¸Šé™ã‚ã‚Š
- æœ€é©åœæ­¢: ã„ã¤åå¾©ã‚’æ­¢ã‚ã‚‹ã¹ãã‹ï¼Ÿ

### 6.3 Generative World Models ã®å¿œç”¨

**Genie 3 (Google DeepMind, 2026å¹´1æœˆå…¬é–‹)**[^7]:
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆ: 24fps @ 720p
- æŒç¶šæ€§: æ•°åˆ†é–“ã®ä¸€è²«æ€§
- ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–: ã‚«ãƒ¡ãƒ©ãƒ»ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ“ä½œ

**Runway GWM-1 (2025å¹´12æœˆå…¬é–‹)**[^8]:
- 3ã¤ã®ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«: Worlds / Avatars / Robotics
- ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹å¿œç”¨: åå®Ÿä»®æƒ³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- éŸ³å£°é§†å‹•ã‚¢ãƒã‚¿ãƒ¼: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¼šè©±

**å¿œç”¨åˆ†é‡**:
1. **ã‚²ãƒ¼ãƒ é–‹ç™º**: ãƒ—ãƒ­ã‚·ãƒ¼ã‚¸ãƒ£ãƒ«ç”Ÿæˆ â†’ ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è¡Œå‹•ã«å¿œã˜ãŸä¸–ç•Œå±•é–‹
2. **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹**: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨“ç·´ â†’ Sim-to-Realè»¢ç§»
3. **è‡ªå‹•é‹è»¢**: Waymo Ã— Genie 3[^9] â†’ æœªé­é‡ã‚·ãƒŠãƒªã‚ªã®ç”Ÿæˆ

[^9]: Waymo (2026). "Waymo Taps Google DeepMind Genie 3 for Self-Driving Simulation". https://winbuzzer.com/2026/02/07/waymo-google-deepmind-genie-3-autonomous-driving-simulation-xcxwbn/

### 6.4 Modal Aphasia ã®è§£æ±ºç­–(ä»®èª¬)

**å•é¡Œã®æœ¬è³ª**: ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®**è¡¨ç¾èƒ½åŠ›ã®ä¸å‡è¡¡**ã€‚

**ææ¡ˆã•ã‚Œã‚‹è§£æ±ºç­–**:

1. **å‹•çš„æ½œåœ¨æ¬¡å…ƒ**:
   - ãƒ†ã‚­ã‚¹ãƒˆ: ä½æ¬¡å…ƒ(512-d)
   - ç”»åƒ: é«˜æ¬¡å…ƒ(8192-d)
   - å…±é€šç©ºé–“ã§çµ±ä¸€ã›ãšã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–è¡¨ç¾ã‚’ä¿æŒ

2. **éšå±¤çš„ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ**:
   - ä½ãƒ¬ãƒ™ãƒ«(ãƒ”ã‚¯ã‚»ãƒ«â†”å˜èª): è©³ç´°å¯¾å¿œ
   - ä¸­ãƒ¬ãƒ™ãƒ«(ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆâ†”å¥): æ„å‘³çš„å¯¾å¿œ
   - é«˜ãƒ¬ãƒ™ãƒ«(ã‚·ãƒ¼ãƒ³â†”æ–‡ç« ): æŠ½è±¡çš„å¯¾å¿œ

3. **Contrastive + Generative ã®çµ±åˆ**:
   - CLIPå‹ã®å¯¾æ¯”å­¦ç¿’: ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®æ„å‘³çš„å¯¾å¿œ
   - VAEå‹ã®ç”Ÿæˆå­¦ç¿’: ãƒ¢ãƒ€ãƒªãƒ†ã‚£å†…ã®è©³ç´°ä¿æŒ

### 6.5 æœªè§£æ±ºå•é¡Œãƒªã‚¹ãƒˆ

| å•é¡Œ | ç¾çŠ¶ | é‡è¦åº¦ |
|:-----|:-----|:-------|
| **Modal Aphasia** | æ¤œå‡ºã•ã‚ŒãŸãŒã€æ ¹æœ¬è§£æ±ºãªã— | â˜…â˜…â˜…â˜…â˜… |
| **Long-context Multimodal** | 1åˆ†å‹•ç”»ãŒé™ç•Œ | â˜…â˜…â˜…â˜…â˜† |
| **Physical consistency** | World Modelsã§ã‚‚ç ´ç¶»ã‚ã‚Š | â˜…â˜…â˜…â˜…â˜… |
| **Inference cost** | Reflect-DiT: 20å€ã‚³ã‚¹ãƒˆ | â˜…â˜…â˜…â˜†â˜† |
| **Multimodal evaluation** | çµ±ä¸€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãªã— | â˜…â˜…â˜…â˜…â˜† |

### 6.6 æ¬¡ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼äºˆæ¸¬(2026-2027)

1. **Unified 4D Models**: ç©ºé–“(3D) + æ™‚é–“(1D) + ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®å®Œå…¨çµ±åˆ
2. **Compositional World Models**: ã‚·ãƒ¼ãƒ³åˆ†è§£ â†’ éƒ¨åˆ†ç”Ÿæˆ â†’ åˆæˆ (è¨ˆç®—åŠ¹ç‡åŒ–)
3. **Self-improving Multimodal Models**: æ¨è«–æ™‚ã«è‡ªå·±å­¦ç¿’ â†’ æ°¸ç¶šçš„æ”¹å–„
4. **Modal-agnostic Representations**: ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ä¾å­˜ã—ãªã„æ™®éè¡¨ç¾

### 6.7 ç ”ç©¶ãƒ†ãƒ¼ãƒã®è¦‹ã¤ã‘æ–¹

**Gap Analysis**:
1. æ—¢å­˜æ‰‹æ³•ã®å¼±ç‚¹ã‚’ç‰¹å®š (ä¾‹: Modal Aphasia)
2. ä»–åˆ†é‡ã®æ‰‹æ³•ã‚’å¿œç”¨ (ä¾‹: LLMã®CoT â†’ ç”»åƒç”Ÿæˆã®Reflect-DiT)
3. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã®æ¤œè¨¼ (ä¾‹: Inference-time scaling laws)

**å†ç¾å®Ÿé¨“**:
- Show-o / BAGEL / Reflect-DiT ã‚’å°è¦æ¨¡å®Ÿè£…
- è«–æ–‡ã®ä¸»å¼µã‚’æ¤œè¨¼ â†’ è¿½åŠ å®Ÿé¨“ã§æ–°ç™ºè¦‹

**ç†è«–æ‹¡å¼µ**:
- Modal Aphasiaã®æƒ…å ±ç†è«–çš„åˆ†æ â†’ æ–°ã—ã„è¨“ç·´æ‰‹æ³•ææ¡ˆ
- æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®åæŸä¿è¨¼ â†’ æœ€é©åœæ­¢æˆ¦ç•¥

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®100%å®Œäº†ï¼** ç™ºå±•ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚2025-2026å¹´ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’å®Œå…¨æŠŠæ¡ã—ã€æ¬¡ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’äºˆæ¸¬ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚

> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. 2025å¹´ã®çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§ã®é™ç•Œã‚’1ã¤æŒ™ã’ã€2026-2027å¹´ã«æƒ³å®šã•ã‚Œã‚‹è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã›ã‚ˆã€‚
> 2. Test-Time Trainingï¼ˆTTTï¼‰ã¨Fine-tuningã®é•ã„ã‚’ç›®çš„é–¢æ•° $\mathcal{L}_\text{TTT}(\theta; \mathbf{x}_\text{test})$ ã¨ $\mathcal{L}_\text{FT}(\theta; \mathcal{D}_\text{train})$ ã§è¡¨ã—ã€ãã‚Œãã‚Œã®é©ç”¨æ¡ä»¶ã‚’è¿°ã¹ã‚ˆã€‚

---



## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.8 æœ¬è¬›ç¾©ã®3ã¤ã®æŸ±

1. **Unified Multimodal Models**:
   - Show-o: Hybrid (AR + Diffusion)
   - BAGEL: Unified tokenization + Large-scale pretraining
   - NExT-GPT: LLM-centric modality bridging

2. **Modal Aphasia**:
   - çµ±åˆã®ä»£å„Ÿ: è¦–è¦šè¨˜æ†¶ã¯å®Œç’§ã€è¨€èªè¨˜è¿°ã¯ä¸æ­£ç¢º
   - åŸå› : ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®è¡¨ç¾èƒ½åŠ›æ ¼å·®
   - å½±éŸ¿: ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®è„†å¼±æ€§

3. **Inference-Time Scaling**:
   - Reflect-DiT: åå¾©çš„æ”¹å–„ã§å“è³ªå‘ä¸Š
   - Test-time Training: æ¨è«–æ™‚ã«ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã‚’å¾®èª¿æ•´
   - Scaling law: Quality $\propto K^{-\gamma}$ (åç©«é€“æ¸›)

### 6.9 Course V ã®å…¨ä½“æŒ¯ã‚Šè¿”ã‚Š

| è¬›ç¾© | ãƒ¢ãƒ€ãƒªãƒ†ã‚£ | ä¸»è¦æŠ€è¡“ | åˆ°é”ç‚¹ |
|:-----|:----------|:---------|:-------|
| ç¬¬43å› | ç”»åƒ | DiT, FLUX, SD3 | æ¬¡ä¸–ä»£ç”»åƒç”Ÿæˆ |
| ç¬¬44å› | éŸ³å£° | F5-TTS, Flow Matching | ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆTTS |
| ç¬¬45å› | å‹•ç”» | Sora 2, CogVideoX | æ™‚é–“çš„ä¸€è²«æ€§ |
| ç¬¬46å› | 3D | 3DGS, DreamFusion | ç©ºé–“è¡¨ç¾ |
| ç¬¬47å› | ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ | MotionGPT-3, 4DGS | å‹•çš„3D |
| ç¬¬48å› | ç§‘å­¦ | RFdiffusion3, MatterGen | åˆ¶ç´„ä»˜ãç”Ÿæˆ |
| **ç¬¬49å›** | **çµ±åˆ** | **Show-o, Reflect-DiT** | **å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆ** |

ç¬¬50å›(ç·æ‹¬+å’æ¥­åˆ¶ä½œ)ã§ã€å…¨ã¦ã‚’çµ±åˆã—ãŸ3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

### 6.10 FAQ

<details>
<summary>Q1: Modal Aphasiaã¯è§£æ±ºå¯èƒ½ã‹ï¼Ÿ</summary>

**A**: å®Œå…¨è§£æ±ºã¯å›°é›£ã ãŒã€è»½æ¸›ç­–ã¯å­˜åœ¨ã™ã‚‹:
1. **Modality-specific heads**: å…±é€šæ½œåœ¨ç©ºé–“ã®å¾Œã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã®ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’æ·±ãã™ã‚‹
2. **Multi-task learning**: ç”»åƒâ†’ãƒ†ã‚­ã‚¹ãƒˆã€ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒã‚’ä¸¡æ–¹è¨“ç·´
3. **Hierarchical alignment**: ä½ãƒ¬ãƒ™ãƒ«(è©³ç´°)ã¨é«˜ãƒ¬ãƒ™ãƒ«(æŠ½è±¡)ã‚’åˆ†é›¢

æ ¹æœ¬çš„ã«ã¯ã€**ãƒ†ã‚­ã‚¹ãƒˆã¯æŠ½è±¡åŒ–ãŒæœ¬è³ª**ãªã®ã§ã€ã‚ã‚‹ç¨‹åº¦ã®è©³ç´°å–ªå¤±ã¯é¿ã‘ã‚‰ã‚Œãªã„ã€‚
</details>

<details>
<summary>Q2: æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯è¨“ç·´æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ç½®ãæ›ãˆã‚‹ã‹ï¼Ÿ</summary>

**A**: ç½®ãæ›ãˆã§ã¯ãªã**è£œå®Œ**:
- è¨“ç·´æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: ãƒ¢ãƒ‡ãƒ«ã®åŸºç¤èƒ½åŠ›ã‚’å‘ä¸Š
- æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: ç‰¹å®šã‚¿ã‚¹ã‚¯ãƒ»ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§æœ€é©åŒ–

ä¸¡æ–¹ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã®ãŒæœ€é©ã€‚ãŸã ã—ã€è¨“ç·´ã‚³ã‚¹ãƒˆãŒé™ç•Œã«é”ã—ãŸå ´åˆã€æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå”¯ä¸€ã®é¸æŠè‚¢ã¨ãªã‚‹å¯èƒ½æ€§ã¯ã‚ã‚‹(Chinchilla Scaling Lawsã®å£)ã€‚
</details>

<details>
<summary>Q3: Genie 3 ã¨ Runway GWM-1 ã®é•ã„ã¯ï¼Ÿ</summary>

**A**: è¨­è¨ˆæ€æƒ³ãŒç•°ãªã‚‹:
- **Genie 3**: æ±ç”¨ä¸–ç•Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ (Google DeepMind, ç ”ç©¶å¿—å‘)
  - ã‚ªãƒ¼ãƒ—ãƒ³ãƒ¯ãƒ¼ãƒ«ãƒ‰æ¢ç´¢
  - ç‰©ç†æ³•å‰‡å­¦ç¿’
  - AGIã¸ã®ã‚¹ãƒ†ãƒƒãƒ—
- **Runway GWM-1**: ç‰¹åŒ–å‹World Models (Runway, å•†ç”¨å¿—å‘)
  - 3ã¤ã®ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«(Worlds/Avatars/Robotics)
  - å®Ÿç”¨çš„å¿œç”¨(ã‚²ãƒ¼ãƒ /æ˜ åƒåˆ¶ä½œ/ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹)
  - ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å“è³ª

Genie 3ã¯æ±ç”¨æ€§ã€GWM-1ã¯å®Ÿç”¨æ€§ã‚’å„ªå…ˆã€‚
</details>

<details>
<summary>Q4: ç¬¬50å›ã®å’æ¥­åˆ¶ä½œã§ã¯ä½•ã‚’ä½œã‚‹ã¹ãï¼Ÿ</summary>

**A**: Course V ã§å­¦ã‚“ã å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±åˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ã€‚ä¾‹:
1. **ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–3Dä¸–ç•Œç”Ÿæˆ**: ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â†’ Genie 3å‹World Model â†’ æ¢ç´¢å¯èƒ½3Dç’°å¢ƒ
2. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ**: éŸ³å£°å…¥åŠ› â†’ çµ±åˆç†è§£ â†’ å‹•ç”»+éŸ³å£°ã§å¿œç­”
3. **ç§‘å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿**: åˆ†å­æ§‹é€ å…¥åŠ› â†’ ç‰©æ€§äºˆæ¸¬ â†’ 3Då¯è¦–åŒ–

é‡è¦ãªã®ã¯ã€**Rustè¨“ç·´ + Rustæ¨è«– + Elixiråˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°**ã®3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã€‚
</details>

<details>
<summary>Q5: Show-o ã¨ Show-o2 ã®é•ã„ã¯ï¼Ÿ</summary>

**A**: Show-o2[^10]ã¯æ”¹è‰¯ç‰ˆ:
- **3D Causal VAE**: ç”»åƒ+å‹•ç”»ã‚’çµ±ä¸€çš„ã«æ‰±ã†
- **Dual-path fusion**: ç©ºé–“ã¨æ™‚é–“ã®èåˆ
- **Flow Matching**: Diffusionã¨Flow Matchingã‚’çµ±åˆ
- **2æ®µéšè¨“ç·´**: å°è¦æ¨¡äº‹å‰å­¦ç¿’ â†’ å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

Show-oã¯ç”»åƒä¸­å¿ƒã€Show-o2ã¯ç”»åƒ+å‹•ç”»ã‚’çµ±ä¸€ã€‚

[^10]: ShowLab (2025). "Show-o2: Improved Native Unified Multimodal Models". NeurIPS 2025. arXiv:2506.15564
</details>

### 6.11 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«(1é€±é–“)

| æ—¥ | å†…å®¹ | æ™‚é–“ |
|:---|:-----|:-----|
| Day 1 | Zone 0-2 (å°å…¥+ä½“é¨“+ç›´æ„Ÿ) | 2æ™‚é–“ |
| Day 2 | Zone 3.1-3.3 (çµ±åˆç†è«–+Modal Aphasia) | 3æ™‚é–“ |
| Day 3 | Zone 3.4-3.5 (æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°+World Models) | 3æ™‚é–“ |
| Day 4 | Zone 4 (Rustè¨“ç·´å®Ÿè£…) | 2æ™‚é–“ |
| Day 5 | Zone 4 (Rustæ¨è«–+Elixiråˆ†æ•£) | 2æ™‚é–“ |
| Day 6 | Zone 5 (å®Ÿé¨“ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯) | 2æ™‚é–“ |
| Day 7 | Zone 6-7 (ç™ºå±•+æŒ¯ã‚Šè¿”ã‚Š) | 2æ™‚é–“ |

**åˆè¨ˆ**: 16æ™‚é–“ (æœ¬æ°—ã§å–ã‚Šçµ„ã‚ã°1é€±é–“ã§ç¿’å¾—å¯èƒ½)

### 6.12 æ¬¡å›äºˆå‘Š: ç¬¬50å› â€” ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ç·æ‹¬ & å’æ¥­åˆ¶ä½œ

**å…¨50å›ã®é›†å¤§æˆ**:
- **ãƒ‘ãƒ¼ãƒˆ1: ç†è«–çµ±æ‹¬** (1500è¡Œ)
  - Score â†” Flow â†” Diffusion â†” ODE â†” EBM â†” OT â†” Unified Multimodal ã®å®Œå…¨çµ±ä¸€
  - 2025-2026ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆç·æ‹¬
  - æœªè§£æ±ºå•é¡Œã¨æ¬¡ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼äºˆæ¸¬

- **ãƒ‘ãƒ¼ãƒˆ2: å’æ¥­åˆ¶ä½œ** (1500è¡Œ)
  - 3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ç”ŸæˆAIã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆãƒ»å®Ÿè£…ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤
  - Rustè¨“ç·´ + Rustæ¨è«– + Elixiråˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã®å®Œå…¨çµ±åˆ
  - SmolVLM2(ç†è§£) + aMUSEd(ç”»åƒ) + LTX-Video(å‹•ç”»)ã®3ãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ‡ãƒ¢

**åˆ°é”ç›®æ¨™**: ã€Œ3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ç”ŸæˆAIã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆãƒ»å®Ÿè£…ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã€çµ±ä¸€ç†è«–ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹ã€

æº–å‚™ã—ã¦ãŠãã“ã¨:
1. ç¬¬1-49å›ã®å¾©ç¿’(ç‰¹ã«ç¬¬33-42å›ã®ç†è«–ç·¨)
2. å’æ¥­åˆ¶ä½œãƒ†ãƒ¼ãƒã®é¸å®š(ç”»åƒ/å‹•ç”»/3D/World Modelsã‹ã‚‰1ã¤)
3. GPUç’°å¢ƒã®æº–å‚™(å¯èƒ½ã§ã‚ã‚Œã°)

**æœ€çµ‚è¬›ç¾©ã§ä¼šãŠã†ã€‚å…¨50å›ã®æ—…ã‚’ã€å…±ã«å®Œèµ°ã—ã‚ˆã†ã€‚**

> **Note:** **ğŸ‰ ç¬¬49å› å®Œå…¨åˆ¶è¦‡ï¼** å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®é©å‘½ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯æœ€çµ‚å› â€” å…¨50å›ã®ç·æ‹¬ã¨å’æ¥­åˆ¶ä½œã ã€‚

---

## ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

### è¨“ç·´æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯çµ‚ã‚ã£ãŸã®ã‹ï¼Ÿ

OpenAI o1ã€DeepSeek-R1ã€Gemini 2.5 â€” ã“ã‚Œã‚‰ã¯å…¨ã¦**æ¨è«–æ™‚æ€è€ƒ**ã§æ€§èƒ½ã‚’ä¸Šã’ã‚‹ã€‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯å¢—ãˆç¶šã‘ã‚‰ã‚Œãªã„ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚æ•°å…†ã§é ­æ‰“ã¡ã€‚

**æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**ãŒæ¬¡ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã ã€‚Reflect-DiTã¯ç”»åƒç”Ÿæˆã§ã€Test-time Trainingã¯å‹•ç”»ç”Ÿæˆã§ã€ãã‚Œã‚’è¨¼æ˜ã—ãŸã€‚

ã—ã‹ã—ã€**æ¨è«–æ™‚è¨ˆç®—ã¯è¨“ç·´ã®20å€ã®ã‚³ã‚¹ãƒˆ**ã‚’ã‹ã‘ã‚‹ä¾¡å€¤ãŒã‚ã‚‹ã®ã‹ï¼Ÿ

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯å¾…ã¦ã‚‹ã®ã‹ï¼Ÿãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ï¼Ÿ

ã‚‚ã—ã‹ã™ã‚‹ã¨ã€æˆ‘ã€…ã¯**åŠ¹ç‡æ€§ã®ç½ **ã«é™¥ã£ã¦ã„ã‚‹ã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚ã€Œé€Ÿãã€å®‰ãã€ã‚’è¿½æ±‚ã™ã‚‹ã‚ã¾ã‚Šã€ã€Œæœ€é«˜å“è³ªã€ã‚’è«¦ã‚ã¦ã„ãªã„ã‹ï¼Ÿ

**å•ã„**:
1. æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯ã€è¨“ç·´æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Œå…¨ã«ç½®ãæ›ãˆã‚‹ã‹ï¼Ÿ
2. ãã‚Œã¨ã‚‚ã€ä¸¡è€…ã®**æœ€é©ãƒãƒ©ãƒ³ã‚¹**ãŒå­˜åœ¨ã™ã‚‹ã®ã‹ï¼Ÿ
3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€Œ1ç§’ã§70ç‚¹ã€ã¨ã€Œ20ç§’ã§95ç‚¹ã€ã®ã©ã¡ã‚‰ã‚’é¸ã¶ã®ã‹ï¼Ÿ

<details>
<summary>æ­´å²çš„æ–‡è„ˆ</summary>

**2017-2022**: "Scaling Laws"ã®æ™‚ä»£ â†’ ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã›ã°æ€§èƒ½å‘ä¸Š
- GPT-3 (175B)ã€PaLM (540B)ã€Chinchilla (70B with optimal data)

**2023-2024**: è¨“ç·´ã‚³ã‚¹ãƒˆã®é™ç•Œ â†’ æ•°å„„ãƒ‰ãƒ«è¦æ¨¡ã€ç’°å¢ƒè² è·ã€ãƒ‡ãƒ¼ã‚¿æ¯æ¸‡
- GPT-4è¨“ç·´: æ¨å®š$100M+
- é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã®æ¯æ¸‡å•é¡Œ

**2025-2026**: æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¸ã®ã‚·ãƒ•ãƒˆ â†’ è¨“ç·´å¾Œã§ã‚‚æ€§èƒ½å‘ä¸Šå¯èƒ½
- Reflect-DiT: +0.19 with K=20
- Test-time Training: +1.4 coherence for video
- o1/DeepSeek-R1: Reasoningæ™‚é–“ âˆ æ€§èƒ½

**æ¬¡ã®10å¹´**:
- è¨“ç·´ + æ¨è«–ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–ï¼Ÿ
- æ¨è«–æ™‚å­¦ç¿’ã®è‡ªå‹•åŒ–ï¼Ÿ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¨ˆç®—æ™‚é–“ã¨å“è³ªã‚’ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•é¸æŠï¼Ÿ
</details>

### çµ±åˆã¯æœ¬å½“ã«æ­£è§£ã‹ï¼Ÿ

Show-oã€BAGELã€GPT-4o â€” å…¨ã¦ã€Œçµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã€ã‚’ç›®æŒ‡ã™ã€‚

ã—ã‹ã—ã€**Modal Aphasia**ã¯çµ±åˆã®ä»£å„Ÿã ã€‚è¦–è¦šè¨˜æ†¶ã¯å®Œç’§ã€è¨€èªè¨˜è¿°ã¯ä¸æ­£ç¢ºã€‚

ã‚‚ã—ã‹ã™ã‚‹ã¨ã€**ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¯åˆ†é›¢ã™ã¹ã**ãªã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®å°‚é–€å®¶ã‚’å”èª¿ã•ã›ã‚‹(Mixture of Modality Experts)æ–¹ãŒã€çµ±ä¸€æ½œåœ¨ç©ºé–“ã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã®ã§ã¯ï¼Ÿ

**å•ã„**:
1. çµ±åˆã¯ç›®çš„ã‹ã€ãã‚Œã¨ã‚‚æ‰‹æ®µã‹ï¼Ÿ
2. Modal Aphasiaã‚’è§£æ±ºã§ããªã„ãªã‚‰ã€çµ±åˆã‚’è«¦ã‚ã‚‹ã¹ãã‹ï¼Ÿ
3. äººé–“ã®è„³ã¯çµ±åˆã‹åˆ†é›¢ã‹ï¼Ÿ(è¦–è¦šé‡ vs è´è¦šé‡ vs è¨€èªé‡ â€” ç‹¬ç«‹ã ãŒå”èª¿)

<details>
<summary>ç¥çµŒç§‘å­¦çš„ç¤ºå”†</summary>

äººé–“ã®è„³ã¯**ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å‹**:
- è¦–è¦šé‡(V1-V5): ç”»åƒå‡¦ç†ç‰¹åŒ–
- è´è¦šé‡(A1): éŸ³å£°å‡¦ç†ç‰¹åŒ–
- Wernickeé‡/Brocaé‡: è¨€èªå‡¦ç†ç‰¹åŒ–

ã—ã‹ã—ã€**é«˜æ¬¡çµ±åˆ**ã‚‚å­˜åœ¨:
- è§’å›(Angular gyrus): ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆ
- å‰é ­å‰é‡(Prefrontal cortex): æ¨è«–ãƒ»æ„æ€æ±ºå®š

ã¤ã¾ã‚Šã€**éšå±¤çš„çµ±åˆ**: ä½ãƒ¬ãƒ™ãƒ«=åˆ†é›¢ã€é«˜ãƒ¬ãƒ™ãƒ«=çµ±åˆã€‚

ç¾åœ¨ã®Unified Multimodal Modelsã¯ã€Œå…¨ã¦ã‚’å…±é€šæ½œåœ¨ç©ºé–“ã«æŠ¼ã—è¾¼ã‚€ã€=å¹³å¦ãªçµ±åˆã€‚éšå±¤çš„çµ±åˆã‚’æ¨¡å€£ã™ã¹ãã§ã¯ï¼Ÿ
</details>

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Wu, S., Fei, H., et al. (2023). "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation". *ICLR 2025*. arXiv:2408.12528
<https://arxiv.org/abs/2408.12528>

[^2]: Deng, C., et al. (2025). "Emerging Properties in Unified Multimodal Pretraining". arXiv:2505.14683
<https://arxiv.org/abs/2505.14683>

[^3]: Wu, S., Fei, H., et al. (2023). "NExT-GPT: Any-to-Any Multimodal LLM". arXiv:2309.05519
<https://arxiv.org/abs/2309.05519>

[^4]: Aerni, M., et al. (2025). "Modal Aphasia: Can Unified Multimodal Models Describe Images From Memory?". arXiv:2510.21842
<https://arxiv.org/abs/2510.21842>

[^5]: Li, S., et al. (2025). "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection". *ICCV 2025*. arXiv:2503.12271
<https://arxiv.org/abs/2503.12271>

[^6]: Dalal, K., et al. (2025). "One-Minute Video Generation with Test-Time Training". *CVPR 2025*. arXiv:2504.05298
<https://arxiv.org/abs/2504.05298>

[^10]: ShowLab (2025). "Show-o2: Improved Native Unified Multimodal Models". *NeurIPS 2025*. arXiv:2506.15564
<https://arxiv.org/abs/2506.15564>

Zhang, H., et al. (2025). "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities". arXiv:2505.02567
<https://arxiv.org/abs/2505.02567>

Zhang, L., et al. (2025). "The Art of Scaling Test-Time Compute for Large Language Models". arXiv:2512.02008
<https://arxiv.org/abs/2512.02008>

### æ•™ç§‘æ›¸ãƒ»ã‚µãƒ¼ãƒ™ã‚¤

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Link](https://probml.github.io/pml-book/book2.html)
- Goodfellow, I., Bengio, Y., Courville, A. (2016). *Deep Learning*. MIT Press. [Link](http://www.deeplearningbook.org/)
- Zhang, H., et al. (2025). "Unified Multimodal Models Survey". GitHub. [Link](https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models)

### Web Resources

[^7]: Google DeepMind (2026). "Genie 3: A New Frontier for World Models". https://deepmind.google/models/genie/
<https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/>

[^8]: Runway (2025). "Introducing Runway GWM-1". https://runwayml.com/research/introducing-runway-gwm-1
<https://runwayml.com/research/introducing-runway-gwm-1>

[^9]: Waymo (2026). "Waymo Taps Google DeepMind Genie 3 for Self-Driving Simulation". https://winbuzzer.com/2026/02/07/waymo-google-deepmind-genie-3-autonomous-driving-simulation-xcxwbn/

---

## ğŸ“š è£œéºA: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…è©³ç´°

### A.1 ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥ã®æ¯”è¼ƒ

çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ã€**ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥**ã«å¤§ããä¾å­˜ã™ã‚‹ã€‚

| ãƒ¢ãƒ€ãƒªãƒ†ã‚£ | æˆ¦ç•¥ | ãƒˆãƒ¼ã‚¯ãƒ³æ•° | åœ§ç¸®ç‡ | æƒ…å ±æå¤± |
|:----------|:-----|:----------|:-------|:--------|
| ãƒ†ã‚­ã‚¹ãƒˆ | BPE/SentencePiece | 100-500 | 1x (å…ƒã€…é›¢æ•£) | ãªã— |
| ç”»åƒ | VQ-VAE | 256-1024 | 100-1000x | ä¸­ |
| éŸ³å£° | EnCodec/WavTokenizer | 500-2000 | 50-100x | ä½ |
| å‹•ç”» | 3D-VAE | 1000-5000 | 500-2000x | é«˜ |

**VQ-VAEã®è©³ç´°è¨­å®š**:

```rust
use ndarray::{Array1, Array2, Axis};
use rand::Rng;

// VQ-VAE for image tokenization
#[derive(Debug, Clone)]
struct VQVAETokenizer {
    codebook: Array2<f32>, // (codebook_dim, num_codes)
}

fn vqvae_encode(vqvae: &VQVAETokenizer, z_continuous: &Array2<f32>) -> (Vec<usize>, Array2<f32>) {
    // ç”»åƒ â†’ é€£ç¶šæ½œåœ¨ â†’ é›¢æ•£ã‚³ãƒ¼ãƒ‰
    let (n_tokens, dim) = (z_continuous.nrows(), z_continuous.ncols());
    let num_codes = vqvae.codebook.ncols();

    // ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã¨ã®è·é›¢è¨ˆç®— â†’ æœ€è¿‘å‚ã‚³ãƒ¼ãƒ‰ã‚’é¸æŠ
    let codes: Vec<usize> = (0..n_tokens)
        .map(|i| {
            let token = z_continuous.row(i);
            (0..num_codes)
                .min_by(|&a, &b| {
                    let da: f32 = (0..dim).map(|d| (token[d] - vqvae.codebook[[d, a]]).powi(2)).sum();
                    let db: f32 = (0..dim).map(|d| (token[d] - vqvae.codebook[[d, b]]).powi(2)).sum();
                    da.partial_cmp(&db).unwrap()
                })
                .unwrap()
        })
        .collect();

    // Straight-through estimator ã§å‹¾é…ã‚’é€šã™
    let z_quantized = Array2::from_shape_fn((n_tokens, dim), |(i, d)| {
        vqvae.codebook[[d, codes[i]]]
    });

    (codes, z_quantized)
}

fn vqvae_decode(vqvae: &VQVAETokenizer, codes: &[usize], dim: usize) -> Array2<f32> {
    // é›¢æ•£ã‚³ãƒ¼ãƒ‰ â†’ é€£ç¶šæ½œåœ¨ â†’ ç”»åƒ
    Array2::from_shape_fn((codes.len(), dim), |(i, d)| {
        vqvae.codebook[[d, codes[i]]]
    })
}

fn main() {
    let mut rng = rand::thread_rng();

    // ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚µã‚¤ã‚ºã®å½±éŸ¿
    let codebook_sizes = [512, 1024, 2048, 4096, 8192, 16384];
    let mut reconstruction_errors = Vec::new();

    for &size in &codebook_sizes {
        // ãƒ€ãƒŸãƒ¼å®Ÿé¨“
        let error = 0.5 / (size as f64).sqrt()
            + (rng.gen::<f64>() - 0.5) * 0.02; // ç†è«–: error âˆ 1/âˆš|codebook|
        reconstruction_errors.push(error);
        println!("Codebook size {}: Reconstruction MSE = {:.4}", size, error);
    }

    println!("\nçµè«–: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚µã‚¤ã‚ºâ†‘ â†’ å†æ§‹æˆèª¤å·®â†“ (åç©«é€“æ¸›)");
}
```

### A.2 Attention Mechanism ã®è©³ç´°å®Ÿè£…

**Causal Attention vs Full Attention**:

```rust
use ndarray::Array2;
use rand_distr::{Distribution, Normal};

// Causal Attention (ãƒ†ã‚­ã‚¹ãƒˆç”¨)
fn causal_attention(q: &Array2<f64>, k: &Array2<f64>, v: &Array2<f64>) -> (Array2<f64>, Array2<f64>) {
    let (seq_len, head_dim) = (q.nrows(), q.ncols());

    // Attention scores
    let scale = (head_dim as f64).sqrt();
    let mut scores = q.dot(&k.t()) / scale; // (seq_len, seq_len)

    // Causal mask: æœªæ¥ã‚’è¦‹ãªã„
    for i in 0..seq_len {
        for j in (i + 1)..seq_len {
            scores[[i, j]] = -1e9; // -âˆ for masked positions
        }
    }

    // Softmax
    let mut attn_weights = Array2::<f64>::zeros((seq_len, seq_len));
    for i in 0..seq_len {
        let row = scores.row(i);
        let max_val = row.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
        let exp_sum: f64 = row.iter().map(|&v| (v - max_val).exp()).sum();
        for j in 0..seq_len {
            attn_weights[[i, j]] = (scores[[i, j]] - max_val).exp() / exp_sum;
        }
    }

    // Weighted sum
    let output = attn_weights.dot(v); // (seq_len, head_dim)
    (output, attn_weights)
}

// Full Attention (ç”»åƒç”¨)
fn full_attention(q: &Array2<f64>, k: &Array2<f64>, v: &Array2<f64>) -> (Array2<f64>, Array2<f64>) {
    let (num_patches, head_dim) = (q.nrows(), q.ncols());

    // Attention scores (å…¨çµåˆ)
    let scale = (head_dim as f64).sqrt();
    let scores = q.dot(&k.t()) / scale; // (num_patches, num_patches)

    // Softmax (ãƒã‚¹ã‚¯ãªã—)
    let mut attn_weights = Array2::<f64>::zeros((num_patches, num_patches));
    for i in 0..num_patches {
        let row = scores.row(i);
        let max_val = row.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
        let exp_sum: f64 = row.iter().map(|&v| (v - max_val).exp()).sum();
        for j in 0..num_patches {
            attn_weights[[i, j]] = (scores[[i, j]] - max_val).exp() / exp_sum;
        }
    }

    // Weighted sum
    let output = attn_weights.dot(v);
    (output, attn_weights)
}

fn main() {
    // æ¯”è¼ƒå®Ÿé¨“
    let seq_len = 10;
    let head_dim = 64;
    let normal = Normal::new(0.0, 1.0).unwrap();
    let mut rng = rand::thread_rng();
    let rand_mat = |r, c| Array2::from_shape_fn((r, c), |_| normal.sample(&mut rng));

    let q = rand_mat(seq_len, head_dim);
    let k = rand_mat(seq_len, head_dim);
    let v = rand_mat(seq_len, head_dim);

    let (_out_causal, attn_causal) = causal_attention(&q, &k, &v);
    let (_out_full, attn_full) = full_attention(&q, &k, &v);

    let nonzero_causal = attn_causal.iter().filter(|&&v| v > 1e-6).count();
    let nonzero_full = attn_full.iter().filter(|&&v| v > 1e-6).count();

    println!("Causal Attention:");
    println!("  Non-zero entries: {} / {}", nonzero_causal, seq_len * seq_len);
    println!("  Structure: Lower triangular");

    println!("\nFull Attention:");
    println!("  Non-zero entries: {} / {}", nonzero_full, seq_len * seq_len);
    println!("  Structure: Fully connected");
}
```

### A.3 ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–æå¤±é–¢æ•°

ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ã¯ç•°ãªã‚‹æå¤±é–¢æ•°ãŒé©ã—ã¦ã„ã‚‹:

```rust
use std::collections::HashMap;

// ãƒ†ã‚­ã‚¹ãƒˆ: Cross-entropy loss
fn text_loss(logits: &[Vec<f64>], targets: &[usize]) -> f64 {
    // logits: (seq_len, vocab_size)
    // targets: (seq_len,) token IDs
    let sum: f64 = targets.iter().enumerate()
        .map(|(i, &t)| -logits[i][t])
        .sum();
    sum / targets.len() as f64
}

// ç”»åƒ: Perceptual loss (VGGç‰¹å¾´é‡ã®å·®)
fn perceptual_loss(feat_pred: &[f64], feat_true: &[f64]) -> f64 {
    feat_pred.iter().zip(feat_true.iter())
        .map(|(p, t)| (p - t).powi(2))
        .sum::<f64>() / feat_pred.len() as f64
}

// éŸ³å£°: Multi-resolution STFT loss
fn multi_res_stft_loss(
    audio_pred: &[f64],
    audio_true: &[f64],
    fft_sizes: &[usize],
) -> f64 {
    let total_loss: f64 = fft_sizes.iter()
        .map(|&fft_size| {
            let stft_pred = stft(audio_pred, fft_size);
            let stft_true = stft(audio_true, fft_size);

            // Magnitude loss
            let mag_loss: f64 = stft_pred.iter().zip(stft_true.iter())
                .map(|(p, t)| (p.abs() - t.abs()).abs())
                .sum::<f64>() / stft_pred.len() as f64;

            // Phase loss (cosine distance)
            let phase_loss: f64 = 1.0 - stft_pred.iter().zip(stft_true.iter())
                .map(|(p, t)| (p.atan2(1.0) - t.atan2(1.0)).cos())
                .sum::<f64>() / stft_pred.len() as f64;

            mag_loss + 0.1 * phase_loss
        })
        .sum();
    total_loss / fft_sizes.len() as f64
}

// çµ±åˆæå¤± (é‡ã¿ä»˜ãå’Œ)
fn unified_multimodal_loss(
    preds: &HashMap<&str, Vec<f64>>,
    targets: &HashMap<&str, Vec<f64>>,
    modality_weights: &HashMap<&str, f64>,
) -> f64 {
    let mut total_loss = 0.0;

    if let (Some(pred), Some(target)) = (preds.get("text"), targets.get("text")) {
        total_loss += modality_weights["text"]
            * pred.iter().zip(target.iter()).map(|(p, t)| (p - t).powi(2)).sum::<f64>() / pred.len() as f64;
    }

    if let (Some(pred), Some(target)) = (preds.get("image"), targets.get("image")) {
        total_loss += modality_weights["image"] * perceptual_loss(pred, target);
    }

    if let (Some(pred), Some(target)) = (preds.get("audio"), targets.get("audio")) {
        total_loss += modality_weights["audio"]
            * multi_res_stft_loss(pred, target, &[512, 1024, 2048]);
    }

    total_loss
}

fn main() {
    println!("çµ±åˆæå¤±é–¢æ•°: ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«æœ€é©ãªæå¤±ã‚’é©ç”¨");
}
```

### A.4 æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å®Ÿè£…ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³

**Best-of-N vs Reflect-DiT vs Test-time Training**:

```rust
use rand_distr::{Distribution, Normal};

// 1. Best-of-N (ç‹¬ç«‹ç”Ÿæˆ â†’ æœ€è‰¯é¸æŠ)
fn best_of_n<F, G>(model: &F, prompt: &str, n: usize, quality_fn: &G) -> (Vec<f64>, f64)
where
    F: Fn(&str) -> Vec<f64>,
    G: Fn(&[f64]) -> f64,
{
    let samples: Vec<Vec<f64>> = (0..n).map(|_| model(prompt)).collect();
    let qualities: Vec<f64> = samples.iter().map(|s| quality_fn(s)).collect();
    let best_idx = qualities.iter()
        .enumerate()
        .max_by(|a, b| a.1.partial_cmp(b.1).unwrap())
        .unwrap().0;
    (samples[best_idx].clone(), qualities[best_idx])
}

// 2. Reflect-DiT (åå¾©æ”¹å–„)
fn reflect_dit<F, C>(model: &F, critic: &C, prompt: &str, k: usize) -> Vec<f64>
where
    F: Fn(&str) -> Vec<f64>,
    C: Fn(&[f64], &str) -> String,
{
    let mut x = model(prompt);

    for _ in 0..k {
        let _feedback = critic(&x, prompt);
        x = model(prompt); // In-context learning
    }

    x
}

// 3. Test-time Training (æ¨è«–æ™‚ã«ãƒ¢ãƒ‡ãƒ«å¾®èª¿æ•´)
fn test_time_training<F>(model: &F, prompt: &str, t: usize, _learning_rate: f64) -> Vec<f64>
where
    F: Fn(&str) -> Vec<f64>,
{
    // ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ (å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä¿æŒ)
    for _ in 0..t {
        // è‡ªå·±æ•™å¸«ã‚ã‚Šç›®æ¨™: ä¸€è²«æ€§æœ€å¤§åŒ–
        let x1 = model(prompt);
        let x2 = model(prompt);

        // ä¸€è²«æ€§æå¤±
        let _loss: f64 = x1.iter().zip(x2.iter())
            .map(|(a, b)| (a - b).powi(2))
            .sum::<f64>() / x1.len() as f64;

        // å‹¾é…é™ä¸‹ (ç°¡ç•¥åŒ–)
    }

    // æœ€çµ‚ç”Ÿæˆ
    model(prompt)
}

fn main() {
    let normal = Normal::new(0.0, 1.0).unwrap();
    let mut rng = rand::thread_rng();

    // æ¯”è¼ƒå®Ÿé¨“ (ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«)
    let dummy_model = |_p: &str| -> Vec<f64> {
        (0..64 * 64 * 3).map(|_| normal.sample(&mut rand::thread_rng())).collect()
    };
    let dummy_critic = |_x: &[f64], _p: &str| -> String { "Improve colors".to_string() };
    let quality_fn = |x: &[f64]| -> f64 { -x.iter().map(|v| v.abs()).sum::<f64>() / x.len() as f64 };

    println!("=== æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ‰‹æ³•ã®æ¯”è¼ƒ ===");
    println!();

    // Best-of-N
    let (_sample_bon, quality_bon) = best_of_n(&dummy_model, "prompt", 20, &quality_fn);
    println!("Best-of-N (N=20): Quality = {:.4}", quality_bon);

    // Reflect-DiT
    let _sample_reflect = reflect_dit(&dummy_model, &dummy_critic, "prompt", 10);
    println!("Reflect-DiT (K=10): Sample generated");

    // Test-time Training
    let _sample_ttt = test_time_training(&dummy_model, "prompt", 5, 1e-4);
    println!("Test-time Training (T=5): Sample generated");

    println!();
    println!("è¨ˆç®—ã‚³ã‚¹ãƒˆ: Best-of-N < Reflect-DiT < TTT");
    println!("å“è³ªå‘ä¸Š:   Best-of-N < Reflect-DiT â‰ˆ TTT");
}
```

### A.5 World Model ã®ç‰©ç†æ³•å‰‡å­¦ç¿’

**Implicit Physics via Data vs Explicit Physics Priors**:

```rust
// Implicit Physics (ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’)
#[derive(Debug, Clone)]
struct ImplicitPhysicsWorldModel {
    state_encoder_weights: Vec<Vec<f64>>,
    dynamics_predictor_weights: Vec<Vec<f64>>,
}

fn forward_implicit(model: &ImplicitPhysicsWorldModel, s_t: &[f64], a_t: &[f64]) -> Vec<f64> {
    // ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ãŸæš—é»™çš„ç‰©ç†æ³•å‰‡
    let z_t = apply_encoder(&model.state_encoder_weights, s_t);
    let mut input = z_t;
    input.extend_from_slice(a_t);
    let z_next = apply_predictor(&model.dynamics_predictor_weights, &input);
    decode_state(&z_next)
}

// Explicit Physics (æ˜ç¤ºçš„ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿)
#[derive(Debug, Clone)]
struct ExplicitPhysicsWorldModel {
    state_encoder_weights: Vec<Vec<f64>>,
    dt: f64,
}

fn forward_explicit(model: &ExplicitPhysicsWorldModel, s_t: &[f64], a_t: &[f64]) -> Vec<f64> {
    // ç‰©ç†æ³•å‰‡ã‚’æ˜ç¤ºçš„ã«é©ç”¨
    let z_t = apply_encoder(&model.state_encoder_weights, s_t);

    // ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³åŠ›å­¦: F = ma, v = v0 + at, x = x0 + vt
    let (position, velocity, mass) = extract_physics_state(&z_t);
    let force = action_to_force(a_t);

    let acceleration: Vec<f64> = force.iter().map(|&f| f / mass).collect();
    let velocity_new: Vec<f64> = velocity.iter().zip(acceleration.iter())
        .map(|(&v, &a)| v + a * model.dt).collect();
    let position_new: Vec<f64> = position.iter().zip(velocity_new.iter())
        .map(|(&x, &v)| x + v * model.dt).collect();

    let z_next = pack_physics_state(&position_new, &velocity_new, mass);
    render(&z_next)
}

// ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (Implicit + Explicit)
#[derive(Debug, Clone)]
struct HybridPhysicsWorldModel {
    implicit_model: ImplicitPhysicsWorldModel,
    explicit_model: ExplicitPhysicsWorldModel,
    blend_weight: f64, // 0=full explicit, 1=full implicit
}

fn forward_hybrid(model: &HybridPhysicsWorldModel, s_t: &[f64], a_t: &[f64]) -> Vec<f64> {
    let s_implicit = forward_implicit(&model.implicit_model, s_t, a_t);
    let s_explicit = forward_explicit(&model.explicit_model, s_t, a_t);

    // é‡ã¿ä»˜ãå’Œ
    let alpha = model.blend_weight;
    s_implicit.iter().zip(s_explicit.iter())
        .map(|(&si, &se)| alpha * si + (1.0 - alpha) * se)
        .collect()
}

fn main() {
    println!("=== World Model ã®ç‰©ç†æ³•å‰‡å­¦ç¿’æˆ¦ç•¥ ===");
    println!("Implicit: ãƒ‡ãƒ¼ã‚¿é§†å‹•ã€æŸ”è»Ÿã ãŒç‰©ç†çš„ã«ä¸æ­£ç¢ºã«ãªã‚Šå¾—ã‚‹");
    println!("Explicit: ç‰©ç†æ³•å‰‡ä¿è¨¼ã€ãŸã ã—æœªçŸ¥ã®ç¾è±¡ã«ã¯å¯¾å¿œä¸å¯");
    println!("Hybrid: ä¸¡æ–¹ã®åˆ©ç‚¹ã‚’çµ„ã¿åˆã‚ã›ã‚‹ (Î±=0.7 ç¨‹åº¦ãŒå®Ÿé¨“çš„ã«æœ€é©)");
}
```

---

## ğŸ“š è£œéºB: æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç†è«–çš„æ·±æ˜ã‚Š

### B.1 Inference-Time Scaling Laws ã®å°å‡º

**ä»®å®š**: æ¨è«–æ™‚è¨ˆç®— $C$(ã‚µãƒ³ãƒ—ãƒ«æ•°ã€åå¾©å›æ•°ã€å¾®èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—æ•°)ã¨å“è³ª $Q$ ã®é–¢ä¿‚ã€‚

çµŒé¨“çš„è¦³å¯Ÿã‚ˆã‚Šã€power law:

$$
Q(C) = Q_\infty - \frac{A}{C^\gamma}
$$

- $Q_\infty$: ç„¡é™è¨ˆç®—ã§ã®ç†è«–ä¸Šé™
- $A$: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
- $\gamma$: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æŒ‡æ•° ($\gamma \approx 0.3 \sim 0.7$)

**å°å‡º** (ç°¡ç•¥ç‰ˆ):

1. å“è³ªã¯ã€Œèª¤å·® $E$ ã®æ¸›å°‘ã€ã¨ã—ã¦å®šç¾©: $Q = 1 - E$
2. å„åå¾©ã§èª¤å·®ãŒç¢ºç‡çš„ã«æ¸›å°‘: $E_{k+1} = E_k \cdot (1 - p_k)$
3. $p_k \approx p$ (ä¸€å®šã®æ”¹å–„ç‡) ã¨ä»®å®š: $E_K = E_0 (1-p)^K$
4. $K$ ãŒå¤§ãã„ã¨ã: $(1-p)^K \approx e^{-pK}$
5. $Q(K) = 1 - E_0 e^{-pK}$
6. $K$ ãŒå°ã•ã„é ˜åŸŸã§Taylorå±•é–‹: $Q(K) \approx Q_\infty - A K^{-\gamma}$

**æ•°å€¤æ¤œè¨¼**:

```rust
// Power law fitting experiment

fn main() {
    // å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ (ãƒ€ãƒŸãƒ¼)
    let k_values: Vec<f64> = (1..=20).map(|k| k as f64).collect();
    let q_observed = vec![
        0.5, 0.62, 0.68, 0.72, 0.75, 0.77, 0.79, 0.80, 0.81, 0.82,
        0.83, 0.835, 0.84, 0.845, 0.85, 0.852, 0.854, 0.856, 0.858, 0.86,
    ];

    // ãƒ¢ãƒ‡ãƒ«: Q(K) = Q_âˆ - A / K^Î³
    let model = |k: f64, params: &[f64]| -> f64 {
        params[0] - params[1] / k.powf(params[2])
    };

    // ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚° (ç°¡æ˜“çš„ãªã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ)
    // å®Ÿéš›ã«ã¯nalgebra + levenberg-marquardt ã‚¯ãƒ¬ãƒ¼ãƒˆç­‰ã‚’ä½¿ç”¨
    let q_inf_fitted = 0.87;
    let a_fitted = 0.38;
    let gamma_fitted = 0.52;
    let params = [q_inf_fitted, a_fitted, gamma_fitted];

    println!("=== Inference-Time Scaling Law ===");
    println!("Fitted parameters:");
    println!("  Q_âˆ = {:.3}", q_inf_fitted);
    println!("  A   = {:.3}", a_fitted);
    println!("  Î³   = {:.3}", gamma_fitted);
    println!();

    // äºˆæ¸¬
    let k_test = [25, 50, 100];
    for k in k_test {
        let q_pred = model(k as f64, &params);
        println!("Predicted Q(K={}) = {:.4}", k, q_pred);
    }

    println!();
    println!("ç¤ºå”†: K=100ã§ã‚‚ Q_âˆ={:.3} ã«ã¯é”ã—ãªã„ â†’ åç©«é€“æ¸›", q_inf_fitted);
}
```

### B.2 æœ€é©åœæ­¢å•é¡Œ

æ¨è«–æ™‚è¨ˆç®—ã‚³ã‚¹ãƒˆ $C(K)$ ã¨å“è³ªå‘ä¸Š $\Delta Q(K)$ ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:

$$
\text{Value}(K) = Q(K) - \lambda \cdot C(K)
$$

$\lambda$: ã‚³ã‚¹ãƒˆé‡ã¿(ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¾å­˜)ã€‚

æœ€é©åœæ­¢: $\frac{d}{dK} \text{Value}(K) = 0$

$$
\frac{dQ}{dK} = \lambda \frac{dC}{dK}
$$

$Q(K) = Q_\infty - A K^{-\gamma}$ ã‚ˆã‚Š:

$$
\frac{dQ}{dK} = A \gamma K^{-\gamma - 1}
$$

$C(K) = c \cdot K$ (ç·šå½¢ã‚³ã‚¹ãƒˆ) ã®å ´åˆ:

$$
A \gamma K^{-\gamma - 1} = \lambda c
$$

$$
K^* = \left( \frac{A \gamma}{\lambda c} \right)^{\frac{1}{\gamma + 1}}
$$

**æ•°å€¤ä¾‹**:

```rust
// æœ€é©åœæ­¢ç‚¹ã®è¨ˆç®—
fn main() {
    let a = 0.3_f64;
    let gamma = 0.5_f64;
    let lambda_values = [0.001, 0.01, 0.1]; // ã‚³ã‚¹ãƒˆé‡ã¿
    let c = 1.0_f64; // åå¾©ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆ

    println!("=== æœ€é©åœæ­¢ç‚¹ ===");
    for &lambda in &lambda_values {
        let k_opt = (a * gamma / (lambda * c)).powf(1.0 / (gamma + 1.0));
        let q_opt = 0.86 - a / k_opt.powf(gamma);

        println!("Î» = {}:", lambda);
        println!("  K* = {:.2}", k_opt);
        println!("  Q(K*) = {:.4}", q_opt);
        println!();
    }

    println!("ç¤ºå”†: ã‚³ã‚¹ãƒˆã‚’é‡è¦– (Î»â†‘) â†’ K*â†“ (æ—©æœŸåœæ­¢)");
    println!("       å“è³ªã‚’é‡è¦– (Î»â†“) â†’ K*â†‘ (é•·æ™‚é–“è¨ˆç®—)");
}
```

### B.3 Test-Time Training ã®åæŸä¿è¨¼

**å®šç†** (ç°¡ç•¥ç‰ˆ): Test-time Trainingã¯ã€é©åˆ‡ãªå­¦ç¿’ç‡ $\eta$ ã®ä¸‹ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«æœ€é©è§£ã«åæŸã™ã‚‹ã€‚

è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ:

1. æå¤±é–¢æ•° $\mathcal{L}_{\text{TTT}}(\theta)$ ã¯æ»‘ã‚‰ã‹ã§å‡¸(å±€æ‰€çš„)
2. å‹¾é…é™ä¸‹: $\theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}_{\text{TTT}}(\theta_t)$
3. $\eta < \frac{2}{L}$ ($L$: Lipschitzå®šæ•°)ã®ã¨ãã€$\mathcal{L}(\theta_t)$ ã¯å˜èª¿æ¸›å°‘
4. $T \to \infty$ ã§ $\nabla \mathcal{L}(\theta_T) \to 0$ â†’ è‡¨ç•Œç‚¹

**å®Ÿé¨“çš„æ¤œè¨¼**:

```rust
use rand_distr::{Distribution, Normal};

// TTTåæŸå®Ÿé¨“
fn ttt_convergence_experiment(t: usize, eta: f64) -> Vec<f64> {
    let normal = Normal::new(0.0, 1.0).unwrap();
    let mut rng = rand::thread_rng();
    let mut theta: Vec<f64> = (0..10).map(|_| normal.sample(&mut rng)).collect();
    let mut losses = Vec::new();

    for _ in 0..t {
        // ãƒ€ãƒŸãƒ¼æå¤±: â€–Î¸ - Î¸*â€–Â² (Î¸* = 0ãŒæœ€é©)
        let loss: f64 = theta.iter().map(|x| x * x).sum();
        losses.push(loss);

        // å‹¾é…é™ä¸‹
        theta = theta.iter().map(|&x| x - eta * 2.0 * x).collect();
    }

    losses
}

fn main() {
    let losses = ttt_convergence_experiment(100, 1e-3);

    println!("=== Test-Time Training åæŸ ===");
    println!("Initial loss: {:.4}", losses[0]);
    println!("Final loss:   {:.8}", losses[losses.len() - 1]);
    println!("Converged: {}", losses[losses.len() - 1] < 1e-6);

    // å­¦ç¿’ç‡ã®å½±éŸ¿
    let eta_values = [1e-4, 1e-3, 1e-2, 1e-1];
    println!();
    println!("å­¦ç¿’ç‡ã®å½±éŸ¿:");
    for &eta in &eta_values {
        let losses_eta = ttt_convergence_experiment(100, eta);
        let final_loss = losses_eta[losses_eta.len() - 1];
        let converged = final_loss < 1e-4;
        println!("  Î·={}: Converged={}, Final loss={:.6}", eta, converged, final_loss);
    }

    println!();
    println!("ç¤ºå”†: Î· ãŒå¤§ãã™ãã‚‹ã¨ç™ºæ•£ã€å°ã•ã™ãã‚‹ã¨åæŸãŒé…ã„");
}
```

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
