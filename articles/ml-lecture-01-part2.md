---
title: "ç¬¬1å›: æ¦‚è«–: æ•°å¼ã¨è«–æ–‡ã®èª­ã¿æ–¹ â€” 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ§­"
type: "tech"
topics: ["machinelearning", "deeplearning", "math", "python"]
published: true
---

## ğŸ› ï¸ 4. ç’°å¢ƒãƒ»ãƒ„ãƒ¼ãƒ«ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” é–‹ç™ºç’°å¢ƒãƒ»LaTeXãƒ»è«–æ–‡èª­è§£è¡“

> **ç›®æ¨™**: Python ç’°å¢ƒã‚’æ•´ãˆã€LaTeX ã§æ•°å¼ã‚’æ›¸ã‘ã‚‹ã‚ˆã†ã«ã—ã€arXiv è«–æ–‡ã‚’æ§‹é€ çš„ã«èª­ã‚€æŠ€è¡“ã‚’èº«ã«ã¤ã‘ã‚‹ã€‚

### 4.1 é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— â€” Pythonãƒ»IDEãƒ»AI CLI

ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã€å®Ÿè¡Œã—ã€AI ã«åŠ©ã‘ã¦ã‚‚ã‚‰ã†ã€‚ã“ã®3ã¤ã®ç’°å¢ƒã‚’ä¸€æ°—ã«æ•´ãˆã‚‹ã€‚

#### Python ç’°å¢ƒæ§‹ç¯‰

æœ¬ã‚·ãƒªãƒ¼ã‚ºã® Course Iï¼ˆç¬¬1å›ã€œç¬¬8å›ï¼‰ã¯ Python 100% ã§é€²ã‚ã‚‹ã€‚ç’°å¢ƒæ§‹ç¯‰ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ä¿ã¤ã€‚

#### æ¨å¥¨ç’°å¢ƒ

| é …ç›® | æ¨å¥¨ | ç†ç”± |
|:---|:---|:---|
| Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | 3.11+ | match æ–‡ã€tomllibã€é€Ÿåº¦æ”¹å–„ |
| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç† | `uv` | pip ã®10å€é«˜é€Ÿã€lockfileå¯¾å¿œ |
| ä»®æƒ³ç’°å¢ƒ | `uv venv` | ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«åˆ†é›¢ |
| ã‚¨ãƒ‡ã‚£ã‚¿ | VSCode + Pylance | å‹æ¨è«–ã€Jupyter çµ±åˆ |
| ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ | Jupyter Lab or VSCode | å¯¾è©±çš„å®Ÿé¨“ |

```bash
# uv ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã¾ã ã®å ´åˆï¼‰
curl -LsSf https://astral.sh/uv/install.sh | sh

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
mkdir -p ~/ml-lectures && cd ~/ml-lectures
uv init
uv add numpy matplotlib jupyter

# ä»®æƒ³ç’°å¢ƒã®æœ‰åŠ¹åŒ–
source .venv/bin/activate
python -c "import numpy; print(f'NumPy {numpy.__version__} ready')"
```

#### æœ€å°é™ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```toml
# pyproject.toml
[project]
name = "ml-lectures"
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "numpy>=1.26",
    "matplotlib>=3.8",
    "jupyter>=1.0",
]

[project.optional-dependencies]
lecture01 = []  # ç¬¬1å›ã¯è¿½åŠ ä¾å­˜ãªã—
lecture02 = ["scipy>=1.12"]  # ç¬¬2å›ã§è¿½åŠ 
```

:::message
**æœ¬ã‚·ãƒªãƒ¼ã‚ºã®ãƒ«ãƒ¼ãƒ«**: å„è¬›ç¾©ã§å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ `[project.optional-dependencies]` ã§ç®¡ç†ã™ã‚‹ã€‚ç¬¬1å›ã¯ NumPy ã¨ Matplotlib ã®ã¿ã€‚PyTorch ã¯ç¬¬3å›ã‹ã‚‰ã€JAX ã¯ Course II ã‹ã‚‰ç™»å ´ã™ã‚‹ã€‚
:::

#### IDEï¼ˆçµ±åˆé–‹ç™ºç’°å¢ƒï¼‰ã®é¸ã³æ–¹

Python ã®ç’°å¢ƒãŒã§ããŸã‚‰ã€æ¬¡ã¯ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿æ›¸ãã™ã‚‹é“å…·ã ã€‚æ­£ç›´ã€ã©ã‚Œã‚’é¸ã‚“ã§ã‚‚å­¦ç¿’ã¯ã§ãã‚‹ã€‚ã ãŒé“å…·ã®å·®ã¯é•·æœŸçš„ã«åŠ¹ã„ã¦ãã‚‹ã€‚

#### 3å¤§ã‚¨ãƒ‡ã‚£ã‚¿æ¯”è¼ƒ

| | VSCode | Cursor | Zed |
|:---|:---|:---|:---|
| **ä¾¡æ ¼** | ç„¡æ–™ | ç„¡æ–™ã€œ$20/æœˆ | ç„¡æ–™ |
| **ç‰¹å¾´** | æ‹¡å¼µæ©Ÿèƒ½ãŒè±Šå¯Œ | AIçµ±åˆã‚¨ãƒ‡ã‚£ã‚¿ | Rustè£½ãƒ»è¶…é«˜é€Ÿ |
| **AIæ”¯æ´** | Copilotæ‹¡å¼µã§å¯¾å¿œ | ãƒã‚¤ãƒ†ã‚£ãƒ–AIçµ±åˆ | AIçµ±åˆã‚ã‚Š |
| **èµ·å‹•é€Ÿåº¦** | æ™®é€š | æ™®é€šï¼ˆVSCode forkï¼‰ | éå¸¸ã«é«˜é€Ÿ |
| **Jupyter** | çµ±åˆã‚µãƒãƒ¼ãƒˆ | çµ±åˆã‚µãƒãƒ¼ãƒˆ | æœªå¯¾å¿œ |
| **ãŠã™ã™ã‚å¯¾è±¡** | ä¸‡äººå‘ã‘ | AIæ´»ç”¨ã—ãŸã„äºº | é€Ÿåº¦é‡è¦–ã®äºº |

```mermaid
graph TD
    Q{"ã‚¨ãƒ‡ã‚£ã‚¿é¸ã³"} -->|åˆå¿ƒè€…ãƒ»å®‰å®š| V["VSCode<br/>æ‹¡å¼µè±Šå¯Œãƒ»æƒ…å ±å¤šã„"]
    Q -->|AIé‡è¦–| C["Cursor<br/>VSCode + AIçµ±åˆ"]
    Q -->|é€Ÿåº¦é‡è¦–| Z["Zed<br/>Rustè£½ãƒ»è»½é‡"]
    V --> J["Jupyterçµ±åˆOK"]
    C --> J
    style V fill:#e3f2fd
    style C fill:#fff3e0
    style Z fill:#e8f5e9
```

:::message
**æœ¬ã‚·ãƒªãƒ¼ã‚ºã®æ¨å¥¨**: è¿·ã£ãŸã‚‰ **VSCode** ã§å§‹ã‚ã‚‹ã€‚æ‹¡å¼µæ©Ÿèƒ½ãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãŒæœ€ã‚‚å……å®Ÿã—ã¦ãŠã‚Šã€å›°ã£ãŸã¨ãæ¤œç´¢ã§è§£æ±ºã—ã‚„ã™ã„ã€‚
:::

#### æœ€ä½é™å…¥ã‚Œã‚‹ã¹ãæ‹¡å¼µæ©Ÿèƒ½ï¼ˆVSCodeï¼‰

| æ‹¡å¼µæ©Ÿèƒ½ | ç”¨é€” |
|:---|:---|
| Python (ms-python) | Python è¨€èªã‚µãƒãƒ¼ãƒˆ |
| Pylance | å‹æ¨è«–ãƒ»è£œå®Œ |
| Jupyter | ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œ |
| GitLens | Git å±¥æ­´ã®å¯è¦–åŒ– |
| Markdown All in One | Markdown ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ |

```bash
# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
code --install-extension ms-python.python
code --install-extension ms-python.vscode-pylance
code --install-extension ms-toolsai.jupyter
code --install-extension eamodio.gitlens
code --install-extension yzhang.markdown-all-in-one
```

#### ã‚¿ãƒ¼ãƒŸãƒŠãƒ«çµ±åˆã¨ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒ‰

ã‚¨ãƒ‡ã‚£ã‚¿å†…ã§ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’é–‹ãã€ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œã¨ç·¨é›†ã‚’è¡Œãæ¥ã§ãã‚‹ã®ãŒ IDE ã®å¼·ã¿:

| æ“ä½œ | ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆï¼ˆVSCodeï¼‰ |
|:---|:---|
| ã‚¿ãƒ¼ãƒŸãƒŠãƒ«è¡¨ç¤º/éè¡¨ç¤º | `` Ctrl+` `` |
| ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ | `Ctrl+P` |
| ã‚³ãƒãƒ³ãƒ‰ãƒ‘ãƒ¬ãƒƒãƒˆ | `Ctrl+Shift+P` |
| å®šç¾©ã¸ã‚¸ãƒ£ãƒ³ãƒ— | `F12` |
| å‚ç…§ã‚’æ¤œç´¢ | `Shift+F12` |
| è¡Œã‚³ãƒ¡ãƒ³ãƒˆ | `Ctrl+/` |

:::details Cursor ã¨ Zed ã®è£œè¶³
**Cursor**: VSCode ã‚’ãƒ•ã‚©ãƒ¼ã‚¯ã—ãŸã‚¨ãƒ‡ã‚£ã‚¿ã§ã€AI ãƒãƒ£ãƒƒãƒˆãƒ»ã‚³ãƒ¼ãƒ‰è£œå®Œãƒ»ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ç†è§£ãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹ã€‚VSCode ã®æ‹¡å¼µæ©Ÿèƒ½ãŒãã®ã¾ã¾ä½¿ãˆã‚‹ã€‚æœˆé¡ $20 ã® Pro ãƒ—ãƒ©ãƒ³ã§ Claude / GPT-4 ã‚’ä½¿ã£ãŸã‚³ãƒ¼ãƒ‰ç”ŸæˆãŒå¯èƒ½ã€‚

**Zed**: Rust ã§æ›¸ã‹ã‚ŒãŸæ¬¡ä¸–ä»£ã‚¨ãƒ‡ã‚£ã‚¿ã€‚èµ·å‹•ã¨ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãŒåœ§å€’çš„ã«é€Ÿã„ã€‚ãƒãƒ«ãƒãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ç·¨é›†ï¼ˆãƒšã‚¢ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ï¼‰ãŒãƒã‚¤ãƒ†ã‚£ãƒ–å¯¾å¿œã€‚ãŸã ã—æ‹¡å¼µæ©Ÿèƒ½ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã¯ VSCode ã»ã©æˆç†Ÿã—ã¦ã„ãªã„ã€‚Jupyter æœªå¯¾å¿œã®ãŸã‚ã€æœ¬ã‚·ãƒªãƒ¼ã‚ºã®åºç›¤ã§ã¯è£œåŠ©ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ä½¿ã„ã€ãƒ¡ã‚¤ãƒ³ã¯ VSCode ãŒå®‰å…¨ã€‚
:::

#### AI CLI ãƒ„ãƒ¼ãƒ« â€” ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰AIã‚’ä½¿ã†

IDE ãŒæ•´ã£ãŸã‚‰ã€ã‚‚ã†ä¸€ã¤ã®æ­¦å™¨ã‚’æ‰‹ã«å…¥ã‚Œã‚ˆã†ã€‚2025å¹´ä»¥é™ã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰ç›´æ¥AIã«è³ªå•ãƒ»ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ»ãƒ‡ãƒãƒƒã‚°æ”¯æ´ã‚’å—ã‘ã‚‹ã®ãŒå½“ãŸã‚Šå‰ã«ãªã£ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ã‹ãšã«ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¸­ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰ãã®ã¾ã¾ AI ã‚’å‘¼ã¹ã‚‹ã€‚

#### ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒ

| ãƒ„ãƒ¼ãƒ« | ä¾¡æ ¼ | ç‰¹å¾´ | ãŠã™ã™ã‚åº¦ |
|:---|:---|:---|:---|
| **Gemini CLI** | ç„¡æ–™ | Googleè£½ãƒ»å°å…¥ãŒæœ€ã‚‚ç°¡å˜ | â˜…â˜…â˜…â˜…â˜… |
| **GitHub Copilot CLI** | $10/æœˆï¼ˆå­¦ç”Ÿç„¡æ–™ï¼‰ | GitHubçµ±åˆãƒ»å®‰å®š | â˜…â˜…â˜…â˜…â˜† |
| **Codex CLI** | APIå¾“é‡èª²é‡‘ | OpenAIè£½ãƒ»é«˜ç²¾åº¦ | â˜…â˜…â˜…â˜†â˜† |
| **Claude Code** | APIå¾“é‡èª²é‡‘ | Anthropicè£½ãƒ»æ·±ã„æ¨è«– | â˜…â˜…â˜…â˜†â˜† |

:::message alert
**èª²é‡‘ã®è½ã¨ã—ç©´**: Claude Code ã¨ Codex CLI ã¯ API å¾“é‡èª²é‡‘åˆ¶ã€‚1å›ã®è³ªå•ã§ $0.01ã€œ$0.50+ ã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚$20/æœˆã®ãƒ—ãƒ©ãƒ³ã«å…¥ã£ã¦ã‚‚ API åˆ©ç”¨åˆ†ã¯åˆ¥é€”è«‹æ±‚ã•ã‚Œã‚‹ãŸã‚ã€åˆå­¦è€…ã¯**ç„¡æ–™ã® Gemini CLI ã‹ã‚‰å§‹ã‚ã‚‹**ã®ãŒå®‰å…¨ã€‚æœˆã®è«‹æ±‚é¡ãŒæ€ã‚ã¬é‡‘é¡ã«ãªã£ãŸå ±å‘Šã¯å°‘ãªããªã„ã€‚
:::

#### Gemini CLI ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆæ¨å¥¨ï¼‰

```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install -g @anthropic-ai/gemini-cli
# ã¾ãŸã¯
npx @google/gemini-cli

# èªè¨¼ï¼ˆGoogle ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ãƒ­ã‚°ã‚¤ãƒ³ï¼‰
gemini auth login

# åŸºæœ¬çš„ãªä½¿ã„æ–¹
gemini "Softmaxé–¢æ•°ã‚’Pythonã§å®Ÿè£…ã—ã¦"
gemini "ã“ã®ã‚¨ãƒ©ãƒ¼ã®åŸå› ã‚’æ•™ãˆã¦: IndexError: index out of range"
gemini "numpy ã® einsum ã®ä½¿ã„æ–¹ã‚’æ•™ãˆã¦"
```

#### AI CLI ã®å®Ÿè·µçš„ãªä½¿ã„æ–¹

```bash
# ã‚³ãƒ¼ãƒ‰ã®èª¬æ˜ã‚’æ±‚ã‚ã‚‹
gemini "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ãŒä½•ã‚’ã—ã¦ã„ã‚‹ã‹èª¬æ˜ã—ã¦:
def attention(Q, K, V):
    d_k = Q.shape[-1]
    scores = Q @ K.T / np.sqrt(d_k)
    weights = softmax(scores)
    return weights @ V"

# ãƒ‡ãƒãƒƒã‚°æ”¯æ´
gemini "np.linalg.norm ãŒ nan ã‚’è¿”ã™ã€‚åŸå› ã¯ï¼Ÿ"

# æ•°å¼ã‚’ã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³
gemini "KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®å¼ã‚’NumPyã§å®Ÿè£…ã—ã¦"
```

:::details èª²é‡‘ãƒ„ãƒ¼ãƒ«ï¼ˆCodex / Claude Codeï¼‰ã‚’ä½¿ã†å ´åˆã®æ³¨æ„
- **åˆ©ç”¨é‡ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**: æ¯æ—¥ã®åˆ©ç”¨é¡ã‚’ç¢ºèªã™ã‚‹ç¿’æ…£ã‚’ã¤ã‘ã‚‹
- **ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã®è¨­å®š**: ç’°å¢ƒå¤‰æ•°ã‚„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§1å›ã‚ãŸã‚Šã®ä¸Šé™ã‚’è¨­å®š
- **ç°¡å˜ãªè³ªå•ã¯ç„¡æ–™ãƒ„ãƒ¼ãƒ«ã§**: Gemini CLI ã§ååˆ†ãªè³ªå•ã‚’èª²é‡‘ãƒ„ãƒ¼ãƒ«ã«æŠ•ã’ãªã„
- **æœ¬å½“ã«å¿…è¦ãªå ´é¢**: å¤§è¦æ¨¡ãªã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ç†è§£ã€è¤‡é›‘ãªãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã€æ·±ã„æ¨è«–ãŒå¿…è¦ãªã¨ã

```bash
# Claude Code ã®å ´åˆ
claude "è¤‡é›‘ãªè³ªå•ã‚’ã“ã“ã«"

# Codex CLI ã®å ´åˆ
codex "è¤‡é›‘ãªè³ªå•ã‚’ã“ã“ã«"
```

ã©ã¡ã‚‰ã‚‚é«˜ç²¾åº¦ã ãŒã€æ—¥å¸¸çš„ãªè³ªå•ã«ã¯ Gemini CLI ã§ååˆ†ã€‚èª²é‡‘ãƒ„ãƒ¼ãƒ«ã¯ã€Œã“ã“ãã€ã¨ã„ã†å ´é¢ã§ä½¿ã†ã®ãŒã‚³ã‚¹ãƒ‘æœ€è‰¯ã€‚
:::

### 4.2 ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ´»ç”¨è¡“ â€” GitHubãƒ»Hugging Faceãƒ»OSSãƒ©ã‚¤ã‚»ãƒ³ã‚¹

é–‹ç™ºç’°å¢ƒãŒæ•´ã£ãŸã‚‰ã€æ¬¡ã¯**å¤–ã®ä¸–ç•Œ**ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹æ–¹æ³•ã‚’å­¦ã¶ã€‚è«–æ–‡å®Ÿè£…ã‚’èª­ã¿ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã€æ³•çš„ãƒªã‚¹ã‚¯ã‚’é¿ã‘ã‚‹ â€” ã“ã®3ã¤ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚¹ã‚­ãƒ«ãŒã‚»ãƒƒãƒˆã§å¿…è¦ã«ãªã‚‹ã€‚

#### GitHubå…¥é–€ â€” ã‚³ãƒ¼ãƒ‰ã®å®åº«ã‚’èª­ã¿è§£ã

è«–æ–‡ã‚’èª­ã‚ã‚‹ã‚ˆã†ã«ãªã£ãŸã‚‰ã€æ¬¡ã¯**å®Ÿè£…ã‚’èª­ã‚€**ç•ªã ã€‚ä¸–ç•Œä¸­ã®ç ”ç©¶è€…ãƒ»ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã—ã¦ã„ã‚‹å ´æ‰€ã€ãã‚ŒãŒ GitHubã€‚

#### ãƒªãƒã‚¸ãƒˆãƒªã®èª­ã¿æ–¹

GitHub ãƒªãƒã‚¸ãƒˆãƒªã‚’é–‹ã„ãŸã¨ãã€æœ€åˆã«è¦‹ã‚‹ã¹ããƒ•ã‚¡ã‚¤ãƒ«ã¯3ã¤:

| ãƒ•ã‚¡ã‚¤ãƒ« | è¦‹ã‚‹ã¹ããƒã‚¤ãƒ³ãƒˆ |
|:---|:---|
| `README.md` | ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †ãƒ»ä½¿ã„æ–¹ |
| `requirements.txt` / `pyproject.toml` | ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆPyTorch? JAX? ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯?ï¼‰ |
| ãƒ¡ã‚¤ãƒ³ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ï¼ˆ`model.py` ç­‰ï¼‰ | è«–æ–‡ã®æ•°å¼ãŒã©ã“ã«å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã‹ |

```mermaid
graph LR
    R["README.md<br/>æ¦‚è¦æŠŠæ¡"] --> D["ä¾å­˜é–¢ä¿‚<br/>requirements.txt"]
    D --> S["ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰<br/>model.py"]
    S --> T["ãƒ†ã‚¹ãƒˆ<br/>tests/"]
    T --> I["Issue/PR<br/>è­°è«–ãƒ»ãƒã‚°"]
    style R fill:#e3f2fd
    style S fill:#fff3e0
```

#### è«–æ–‡å®Ÿè£…ã®æ¢ã—æ–¹

**Papers With Code** (paperswithcode.com) ãŒæœ€å¼·ã®ãƒ„ãƒ¼ãƒ«ã€‚è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢ã™ã‚‹ã¨ã€å…¬å¼ãƒ»éå…¬å¼ã®å®Ÿè£…ãŒä¸€è¦§ã§å‡ºã‚‹ã€‚

```bash
# GitHub ã§ã®ã‚³ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆä¾‹: Attention å®Ÿè£…ã‚’æ¢ã™ï¼‰
# github.com ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€æ¤œç´¢ãƒãƒ¼ã§:
# "scaled_dot_product_attention" language:python

# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/<user>/<repo>.git
cd <repo>

# ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
find . -name "*.py" | head -20

# ç‰¹å®šã®é–¢æ•°ã‚’æ¤œç´¢
grep -r "def attention" --include="*.py"
```

:::message
**Tips**: è«–æ–‡ã®å®Ÿè£…ã‚’èª­ã‚€ã¨ãã€ã¾ãš `forward` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¢ã›ã€‚PyTorch ãªã‚‰ `nn.Module` ã®ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã® `forward` ãŒè«–æ–‡ã®æ•°å¼ã«å¯¾å¿œã—ã¦ã„ã‚‹ã€‚
:::

#### Git åŸºæœ¬æ“ä½œ

ã‚³ãƒ¼ãƒ‰ã‚’æ‰‹å…ƒã«ã‚³ãƒ”ãƒ¼ã—ã¦å®Ÿé¨“ã™ã‚‹ãŸã‚ã®æœ€å°é™ã® Git:

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚³ãƒ”ãƒ¼
git clone <url>

# å¤‰æ›´ã®ç¢ºèª
git status
git diff

# å¤‰æ›´ã®ä¿å­˜
git add <file>
git commit -m "message"

# å±¥æ­´ã®ç¢ºèª
git log --oneline -10
```

#### jjï¼ˆJujutsuï¼‰â€” Git ã®ä¸Šä½äº’æ› VCS

æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯ **jj**ï¼ˆJujutsuï¼‰ã‚’æ¨å¥¨ã™ã‚‹ã€‚Git ã¨äº’æ›æ€§ã‚’ä¿ã¡ãªãŒã‚‰ã€æ“ä½œæ€§ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚Œã¦ã„ã‚‹ã€‚

| æ©Ÿèƒ½ | Git | jj |
|:---|:---|:---|
| ä½œæ¥­ã‚³ãƒ”ãƒ¼ | æ‰‹å‹• add/commit | **è‡ªå‹•è¿½è·¡**ï¼ˆå¸¸ã«è¨˜éŒ²ï¼‰ |
| undo | `reflog` + `reset --hard`ï¼ˆå±é™ºï¼‰ | **`jj undo`**ï¼ˆä½•å›ã§ã‚‚å®‰å…¨ï¼‰ |
| ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ | ãƒãƒ¼ã‚¸æ™‚ã«ç™ºç”Ÿãƒ»å³è§£æ±ºå¿…é ˆ | **è¨˜éŒ²ã—ã¦å¾Œã§è§£æ±ºå¯èƒ½** |
| ãƒ–ãƒ©ãƒ³ãƒ | å¿…é ˆï¼ˆHEADç®¡ç†ï¼‰ | **ä¸è¦**ï¼ˆåŒ¿åã‚³ãƒŸãƒƒãƒˆãŒåŸºæœ¬ï¼‰ |
| ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ | Git ç‹¬è‡ªå½¢å¼ | **Gitäº’æ›**ï¼ˆæ—¢å­˜ãƒªãƒã‚¸ãƒˆãƒªã«ãã®ã¾ã¾ä½¿ãˆã‚‹ï¼‰ |

```bash
# jj ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# macOS
brew install jj

# æ—¢å­˜ã® Git ãƒªãƒã‚¸ãƒˆãƒªã§ jj ã‚’ä½¿ã„å§‹ã‚ã‚‹
cd <git-repo>
jj git init --colocate

# åŸºæœ¬æ“ä½œ
jj status          # çŠ¶æ…‹ç¢ºèª
jj diff            # å·®åˆ†è¡¨ç¤º
jj describe -m "message"  # ã‚³ãƒŸãƒƒãƒˆã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
jj new             # æ–°ã—ã„å¤‰æ›´ã‚’é–‹å§‹
jj log             # å±¥æ­´ã‚’ã‚°ãƒ©ãƒ•è¡¨ç¤º
jj undo            # ç›´å‰ã®æ“ä½œã‚’å–ã‚Šæ¶ˆã—ï¼ˆä½•å›ã§ã‚‚ï¼‰
```

:::details Git vs jj â€” ã©ã¡ã‚‰ã‚’å­¦ã¶ã¹ãã‹ï¼Ÿ
çµè«–: **ä¸¡æ–¹ã®æ¦‚å¿µã‚’ç†è§£ã—ã€æ—¥å¸¸ã§ã¯ jj ã‚’ä½¿ã†**ã€‚

ç†ç”±:
1. jj ã¯ Git ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ã†ã®ã§ã€Git ã®çŸ¥è­˜ã¯ç„¡é§„ã«ãªã‚‰ãªã„
2. jj ã®æ“ä½œä½“ç³»ã¯ Git ã‚ˆã‚Šç›´æ„Ÿçš„ï¼ˆ`add/commit` ãŒä¸è¦ã€`undo` ãŒå®‰å…¨ï¼‰
3. æ—¢å­˜ã® GitHub ãƒªãƒã‚¸ãƒˆãƒªã«å¯¾ã—ã¦ãã®ã¾ã¾ `jj` ã‚’ä½¿ãˆã‚‹
4. Git ã‚’è¦æ±‚ã™ã‚‹ç’°å¢ƒï¼ˆCI/CDã€ãƒãƒ¼ãƒ é–‹ç™ºï¼‰ã§ã‚‚ jj ãŒè£ã§ Git æ“ä½œã‚’è¡Œã†

åˆå­¦è€…ã¯ jj ã‹ã‚‰å§‹ã‚ã¦ã€å¿…è¦ã«å¿œã˜ã¦ Git ã®æ¦‚å¿µã‚’å­¦ã¶ã®ãŒæœ€çŸ­çµŒè·¯ã ã€‚
:::

#### Hugging Faceå…¥é–€ â€” ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ–

GitHub ãŒã‚³ãƒ¼ãƒ‰ã®å®åº«ãªã‚‰ã€**Hugging Face** (huggingface.co) ã¯å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å®åº«ã ã€‚æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»ãƒ‡ãƒ¢ã®å…±æœ‰ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¨ã—ã¦ã€è«–æ–‡ã®å®Ÿè£…ã‚’ã€Œå‹•ã‹ã™ã€ã«ã¯ã€ã“ã“ã‚’ä½¿ã„ã“ãªã™ã®ãŒæœ€çŸ­çµŒè·¯ã€‚

#### 3ã¤ã®æŸ±

| ã‚µãƒ¼ãƒ“ã‚¹ | å†…å®¹ | URL |
|:---|:---|:---|
| **Models** | äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆ80ä¸‡+ï¼‰ | huggingface.co/models |
| **Datasets** | å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ15ä¸‡+ï¼‰ | huggingface.co/datasets |
| **Spaces** | ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ‡ãƒ¢ | huggingface.co/spaces |

#### Model Card ã®èª­ã¿æ–¹

ãƒ¢ãƒ‡ãƒ«ãƒšãƒ¼ã‚¸ã‚’é–‹ãã¨ã€ã€ŒModel Cardã€ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã€‚ã“ã‚Œã¯è«–æ–‡ã® Abstract ã«ç›¸å½“ã™ã‚‹:

| ã‚»ã‚¯ã‚·ãƒ§ãƒ³ | ç¢ºèªãƒã‚¤ãƒ³ãƒˆ |
|:---|:---|
| Model Description | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãƒ»å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ |
| Intended Use | æƒ³å®šç”¨é€”ã¨åˆ¶é™äº‹é … |
| Training Details | å­¦ç¿’è¨­å®šï¼ˆã‚¨ãƒãƒƒã‚¯æ•°ãƒ»ãƒãƒƒãƒã‚µã‚¤ã‚ºãƒ»lrï¼‰ |
| Evaluation | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ |
| Limitations | ãƒã‚¤ã‚¢ã‚¹ãƒ»å¤±æ•—ã‚±ãƒ¼ã‚¹ãƒ»å€«ç†çš„è€ƒæ…® |

:::message
**é‡è¦**: Model Card ã® **Limitations** ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…ãšèª­ã‚€ã“ã¨ã€‚ã€Œã“ã®ãƒ¢ãƒ‡ãƒ«ã¯è‹±èªã®ã¿ã€ã€Œæœ‰å®³ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã†ã‚‹ã€ç­‰ã®åˆ¶ç´„ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚ç„¡è¦–ã—ã¦æœ¬ç•ªæŠ•å…¥ã™ã‚‹ã¨äº‹æ•…ã«ãªã‚‹ã€‚
:::

#### transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®åŸºæœ¬

```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯ Course II ã‹ã‚‰æœ¬æ ¼ä½¿ç”¨ï¼‰
uv add transformers torch
```

```python
# æ„Ÿæƒ…åˆ†æã‚’3è¡Œã§ä½“é¨“ï¼ˆãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("I love machine learning!")
print(result)
# [{'label': 'POSITIVE', 'score': 0.9998}]
```

ã“ã®3è¡Œã®è£ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã‹:

```mermaid
graph LR
    T["ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›"] --> TK["Tokenizer<br/>æ–‡â†’ãƒˆãƒ¼ã‚¯ãƒ³ID"]
    TK --> M["Model<br/>BERTç­‰"]
    M --> L["åˆ†é¡ãƒ˜ãƒƒãƒ‰<br/>Softmax"]
    L --> R["çµæœ<br/>POSITIVE 0.99"]
    style T fill:#e3f2fd
    style R fill:#e8f5e9
```

#### ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨æ¨è«–

```python
from transformers import AutoTokenizer, AutoModel

# ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®šã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ– â†’ ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›
text = "Attention is all you need"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)

print(f"Token IDs: {inputs['input_ids'][0].tolist()}")
print(f"Output shape: {outputs.last_hidden_state.shape}")
# Output shape: torch.Size([1, 7, 768])
# â†’ 7ãƒˆãƒ¼ã‚¯ãƒ³ Ã— 768æ¬¡å…ƒã® hidden state
```

:::details Hugging Face Hub ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
ãƒ¢ãƒ‡ãƒ«ã¯ `~/.cache/huggingface/` ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã€‚å¤§ããªãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ã‚£ã‚¹ã‚¯ã‚’åœ§è¿«ã™ã‚‹ã®ã§:

```bash
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèª
du -sh ~/.cache/huggingface/

# ç‰¹å®šãƒ¢ãƒ‡ãƒ«ã®å‰Šé™¤
huggingface-cli delete-cache

# ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æŒ‡å®š
export HF_HOME=/path/to/large/disk/.cache/huggingface
```

BERT-base ã§ç´„ 440MBã€GPT-2 ã§ç´„ 500MBã€‚å¤§å‹ãƒ¢ãƒ‡ãƒ«ï¼ˆLLaMA ç­‰ï¼‰ã¯æ•°åGBå˜ä½ã«ãªã‚‹ãŸã‚ã€ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã«æ³¨æ„ã€‚
:::

#### OSSãƒ©ã‚¤ã‚»ãƒ³ã‚¹ â€” ä½¿ã†å‰ã«çŸ¥ã‚‹ã¹ãã“ã¨

GitHub ã‚„ Hugging Face ã§ã‚³ãƒ¼ãƒ‰ã‚„ãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ãŸã‚‰ã€ä½¿ã†å‰ã«å¿…ãšç¢ºèªã™ã¹ãã“ã¨ãŒã‚ã‚‹ â€” **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**ã ã€‚ã€ŒçŸ¥ã‚‰ãªã‹ã£ãŸã€ã¯é€šç”¨ã—ãªã„ã€‚

#### ä¸»è¦ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ä¸€è¦§

| ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ | å•†ç”¨åˆ©ç”¨ | æ”¹å¤‰ | å†é…å¸ƒæ¡ä»¶ | ã‚³ãƒ”ãƒ¼ãƒ¬ãƒ•ãƒˆ |
|:---|:---|:---|:---|:---|
| **MIT** | OK | OK | è‘—ä½œæ¨©è¡¨ç¤ºã®ã¿ | ãªã— |
| **Apache-2.0** | OK | OK | è‘—ä½œæ¨©è¡¨ç¤º + å¤‰æ›´ç‚¹æ˜è¨˜ | ãªã— |
| **BSD-2/3** | OK | OK | è‘—ä½œæ¨©è¡¨ç¤ºã®ã¿ | ãªã— |
| **MPL-2.0** | OK | OK | æ”¹å¤‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å…¬é–‹ | å¼±ã„ |
| **LGPL** | OK | OK | ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæ”¹å¤‰éƒ¨åˆ†ã‚’å…¬é–‹ | ä¸­ç¨‹åº¦ |
| **GPL** | OK | OK | **æ´¾ç”Ÿç‰©å…¨ä½“ã‚’å…¬é–‹** | å¼·ã„ |
| **CC BY** | OK | OK | ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆè¡¨ç¤º | ãªã— |
| **CC BY-NC** | **ä¸å¯** | OK | ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆè¡¨ç¤º | ãªã— |

#### ã‚³ãƒ”ãƒ¼ãƒ¬ãƒ•ãƒˆã®å¼·åº¦ã‚¹ãƒšã‚¯ãƒˆãƒ«

```mermaid
graph LR
    MIT["MIT / BSD<br/>åˆ¶ç´„ã»ã¼ãªã—"] --> APL["Apache-2.0<br/>ç‰¹è¨±æ¡é …è¿½åŠ "]
    APL --> MPL["MPL-2.0<br/>ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½"]
    MPL --> LGPL["LGPL<br/>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå˜ä½"]
    LGPL --> GPL["GPL<br/>æ´¾ç”Ÿç‰©å…¨ä½“"]
    GPL --> AGPL["AGPL<br/>ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµŒç”±ã‚‚"]
    style MIT fill:#e8f5e9
    style GPL fill:#ffcdd2
    style AGPL fill:#ef9a9a
```

**å·¦ã«è¡Œãã»ã©è‡ªç”±ã€å³ã«è¡Œãã»ã©åˆ¶ç´„ãŒå¼·ã„ã€‚** è‡ªåˆ†ã®ã‚³ãƒ¼ãƒ‰ã« GPL ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’çµ„ã¿è¾¼ã‚€ã¨ã€è‡ªåˆ†ã®ã‚³ãƒ¼ãƒ‰å…¨ä½“ã‚‚ GPL ã§å…¬é–‹ã™ã‚‹ç¾©å‹™ãŒç”Ÿã˜ã‚‹ï¼ˆæ„ŸæŸ“æ€§ï¼‰ã€‚

#### å•†ç”¨åˆ©ç”¨ã®åˆ¤æ–­ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```mermaid
graph TD
    S["ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ã„ãŸã„"] --> L{"ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¯ï¼Ÿ"}
    L -->|MIT/BSD/Apache| OK["âœ… è‘—ä½œæ¨©è¡¨ç¤ºã—ã¦ä½¿ç”¨OK"]
    L -->|MPL-2.0| MPL["âœ… æ”¹å¤‰ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘å…¬é–‹"]
    L -->|LGPL| LG["âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ã—ã¦ä½¿ã†ãªã‚‰OK<br/>æ”¹å¤‰ã—ãŸã‚‰æ”¹å¤‰éƒ¨åˆ†ã‚’å…¬é–‹"]
    L -->|GPL| GP{"çµ„ã¿è¾¼ã¿æ–¹ã¯ï¼Ÿ"}
    GP -->|ãƒªãƒ³ã‚¯ãƒ»import| WARN["âš ï¸ æ´¾ç”Ÿç‰© â†’ GPLå…¬é–‹ç¾©å‹™"]
    GP -->|CLIå‘¼ã³å‡ºã—ï¼ˆåˆ¥ãƒ—ãƒ­ã‚»ã‚¹ï¼‰| CLI["âœ… åˆ†é›¢ã•ã‚Œã¦ã„ã‚Œã°OK"]
    L -->|CC BY-NC| NC["âŒ å•†ç”¨åˆ©ç”¨ä¸å¯"]
    L -->|ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãªã—| NONE["âŒ ä½¿ç”¨ä¸å¯<br/>ï¼ˆè‘—è€…ã«è¨±å¯ã‚’æ±‚ã‚ã‚‹ï¼‰"]
    style OK fill:#e8f5e9
    style WARN fill:#fff3e0
    style NC fill:#ffcdd2
    style NONE fill:#ffcdd2
```

:::message alert
**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãªã— = ä½¿ç”¨ä¸å¯**ã€‚GitHub ã«ã‚³ãƒ¼ãƒ‰ãŒå…¬é–‹ã•ã‚Œã¦ã„ã¦ã‚‚ã€`LICENSE` ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°è‘—ä½œæ¨©è€…ã®è¨±å¯ãªãä½¿ç”¨ã§ããªã„ã€‚ã€Œå…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‹ã‚‰è‡ªç”±ã«ä½¿ãˆã‚‹ã€ã¯èª¤è§£ã€‚
:::

#### ãƒ©ã‚¤ã‚»ãƒ³ã‚¹äº’æ›æ€§ãƒãƒˆãƒªã‚¯ã‚¹

è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒ MIT ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®å ´åˆã€ã©ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã‚’å–ã‚Šè¾¼ã‚ã‚‹ã‹:

| å–ã‚Šè¾¼ã¿å…ƒ â†’ | MIT | Apache-2.0 | MPL-2.0 | LGPL | GPL |
|:---|:---|:---|:---|:---|:---|
| **MIT ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ** | OK | OK | æ¡ä»¶ä»˜ãOK | æ¡ä»¶ä»˜ãOK | **ä¸å¯** |
| **Apache-2.0** | OK | OK | æ¡ä»¶ä»˜ãOK | æ¡ä»¶ä»˜ãOK | **ä¸å¯** |
| **GPL ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ** | OK | OK | OK | OK | OK |

:::details ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç¢ºèªã®å®Ÿè·µæ‰‹é †
```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèª
cat LICENSE
# ã¾ãŸã¯
cat LICENSE.md

# GitHub API ã§ç¢ºèª
gh api repos/<owner>/<repo> --jq '.license.spdx_id'

# Python ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç¢ºèª
pip show numpy | grep License
# License: BSD License
```

**æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ä½¿ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**:

| ãƒ©ã‚¤ãƒ–ãƒ©ãƒª | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ | å•†ç”¨åˆ©ç”¨ |
|:---|:---|:---|
| NumPy | BSD-3-Clause | OK |
| Matplotlib | PSF (BSDäº’æ›) | OK |
| PyTorch | BSD-3-Clause | OK |
| JAX | Apache-2.0 | OK |
| Hugging Face transformers | Apache-2.0 | OK |

å…¨ã¦å•†ç”¨åˆ©ç”¨å¯èƒ½ã€‚å®‰å¿ƒã—ã¦ä½¿ãˆã‚‹ã€‚
:::

### 4.3 è«–æ–‡ã¨ã®å‘ãåˆã„æ–¹ â€” arXivãƒ»3ãƒ‘ã‚¹ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»çŸ¥è­˜ç®¡ç†

é–‹ç™ºç’°å¢ƒã¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®æº–å‚™ãŒã§ããŸã€‚ã“ã“ã‹ã‚‰ã¯**è«–æ–‡ã‚’èª­ã¿ã€ç†è§£ã—ã€è¨˜æ†¶ã«æ®‹ã™**ãŸã‚ã®æ–¹æ³•è«–ã«å…¥ã‚‹ã€‚arXiv ã§è«–æ–‡ã‚’è¦‹ã¤ã‘ã€æ§‹é€ çš„ã«èª­ã¿ã€çŸ¥è­˜ã‚’ã‚°ãƒ©ãƒ•åŒ–ã™ã‚‹ â€” ã“ã®ä¸€é€£ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’èº«ã«ã¤ã‘ã‚ˆã†ã€‚

#### arXiv ã®ä½¿ã„æ–¹ â€” è«–æ–‡ã®å®åº«

arXiv (https://arxiv.org) ã¯ç‰©ç†å­¦ãƒ»æ•°å­¦ãƒ»è¨ˆç®—æ©Ÿç§‘å­¦ã®ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆã‚µãƒ¼ãƒãƒ¼ã€‚æ©Ÿæ¢°å­¦ç¿’ã®æœ€æ–°è«–æ–‡ã¯ã»ã¼ã™ã¹ã¦ã“ã“ã«æŠ•ç¨¿ã•ã‚Œã‚‹ã€‚

#### arXiv ID ã®èª­ã¿æ–¹

| å½¢å¼ | ä¾‹ | æ„å‘³ |
|:---|:---|:---|
| æ–°å½¢å¼ | `2006.11239` | 2020å¹´6æœˆã®11239ç•ªç›® |
| æ—§å½¢å¼ | `1706.03762` | 2017å¹´6æœˆã®3762ç•ªç›® |
| ã‚«ãƒ†ã‚´ãƒªä»˜ã | `cs.LG/2006.11239` | cs.LG (Machine Learning) ã‚«ãƒ†ã‚´ãƒª |

**ä¸»è¦ã‚«ãƒ†ã‚´ãƒª**:
- `cs.LG` â€” Machine Learning
- `cs.CL` â€” Computation and Language (NLP)
- `cs.CV` â€” Computer Vision
- `cs.AI` â€” Artificial Intelligence
- `stat.ML` â€” Statistics: Machine Learning

#### åŠ¹ç‡çš„ãªè«–æ–‡ã®æ¢ã—æ–¹

1. **Semantic Scholar** (semanticscholar.org) â€” å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§é–¢é€£è«–æ–‡ã‚’æ¢ç´¢
2. **Papers With Code** (paperswithcode.com) â€” å®Ÿè£…ä»˜ãè«–æ–‡
3. **Connected Papers** (connectedpapers.com) â€” å¼•ç”¨ã‚°ãƒ©ãƒ•ã®å¯è¦–åŒ–
4. **Daily Papers** (huggingface.co/papers) â€” æ—¥æ¬¡ã®æ³¨ç›®è«–æ–‡
5. **arXiv Sanity** â€” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸæ–°ç€è«–æ–‡

:::message
**æœ¬ã‚·ãƒªãƒ¼ã‚ºã§å¼•ç”¨ã™ã‚‹è«–æ–‡ã¯ã€ã™ã¹ã¦ arXiv ID ã¾ãŸã¯DOIä»˜ãã§è¨˜è¼‰ã™ã‚‹ã€‚** ã€Œã€œã¨è¨€ã‚ã‚Œã¦ã„ã‚‹ã€ã®ã‚ˆã†ãªæ›–æ˜§ãªå¼•ç”¨ã¯ä¸€åˆ‡è¡Œã‚ãªã„ã€‚ã“ã‚ŒãŒå­¦è¡“çš„èª å®Ÿã•ã®åŸºæœ¬ã§ã‚ã‚Šã€èª­è€…ãŒåŸå…¸ã«å½“ãŸã‚Œã‚‹ç’°å¢ƒã‚’ä¿è¨¼ã™ã‚‹ã€‚
:::

#### 3ãƒ‘ã‚¹ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚° â€” è«–æ–‡ã®æ§‹é€ çš„èª­è§£æ³•

è«–æ–‡ã¯**3å›èª­ã‚€**ã®ãŒåŸºæœ¬æˆ¦ç•¥ã€‚S. Keshav ã® "How to Read a Paper" (2007) ã«åŸºã¥ãæ–¹æ³•è«–ã€‚

#### Pass 1: é³¥ç°ï¼ˆ5-10åˆ†ï¼‰

**èª­ã‚€ç®‡æ‰€**: ã‚¿ã‚¤ãƒˆãƒ« â†’ Abstract â†’ Introductionï¼ˆæœ€åˆã¨æœ€å¾Œã®æ®µè½ï¼‰â†’ å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã— â†’ Conclusion â†’ å›³è¡¨

**å¾—ã‚‹ã‚‚ã®**: ã€Œã“ã®è«–æ–‡ã¯ä½•ã‚’ã—ãŸã®ã‹ã€ã®1è¡Œè¦ç´„

```mermaid
graph LR
    T["Title"] --> A["Abstract"]
    A --> I["Intro<br/>(First+Last Â¶)"]
    I --> H["Section<br/>Headings"]
    H --> C["Conclusion"]
    C --> F["Figures &<br/>Tables"]
    style T fill:#e3f2fd
    style F fill:#e8f5e9
```

**Pass 1 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] ä½•ã®å•é¡Œã‚’è§£ã„ã¦ã„ã‚‹ã‹ï¼Ÿ
- [ ] æ—¢å­˜æ‰‹æ³•ã®é™ç•Œã¯ä½•ã‹ï¼Ÿ
- [ ] ææ¡ˆæ‰‹æ³•ã®æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢ã¯ï¼Ÿ
- [ ] ä¸»è¦ãªçµæœï¼ˆæ•°å€¤ï¼‰ã¯ï¼Ÿ
- [ ] è‡ªåˆ†ã®ç ”ç©¶/å­¦ç¿’ã«é–¢é€£ã™ã‚‹ã‹ï¼Ÿ

#### Pass 2: ç²¾èª­ï¼ˆ1-2æ™‚é–“ï¼‰

**èª­ã‚€ç®‡æ‰€**: å…¨æ–‡ã‚’é€šèª­ï¼ˆè¨¼æ˜ã¯é£›ã°ã—ã¦ã‚ˆã„ï¼‰

**å¾—ã‚‹ã‚‚ã®**: æ‰‹æ³•ã®è©³ç´°ç†è§£ã€è‡ªåˆ†ã®è¨€è‘‰ã§ã®èª¬æ˜

é‡è¦ãªã®ã¯**å›³è¡¨ã¨æ•°å¼ã‚’ã‚»ãƒƒãƒˆã§èª­ã‚€**ã“ã¨:
1. å›³ã‚’è¦‹ã‚‹ â†’ ä½•ã‚’è¡¨ã—ã¦ã„ã‚‹ã‹æ¨æ¸¬
2. å¯¾å¿œã™ã‚‹æ•°å¼ã‚’èª­ã‚€ â†’ å›³ã®å„è¦ç´ ã‚’æ•°å¼ã¨å¯¾å¿œã¥ã‘ã‚‹
3. æœ¬æ–‡ã®èª¬æ˜ã‚’èª­ã‚€ â†’ æ¨æ¸¬ã®ç­”ãˆåˆã‚ã›

:::details Pass 2 ã§ã®æ•°å¼ã®èª­ã¿æ–¹
Zone 3 ã§å­¦ã‚“ã æŠ€è¡“ã‚’ãƒ•ãƒ«æ´»ç”¨ã™ã‚‹:

1. **è¨˜å·ã®æ´—ã„å‡ºã—**: æ–°ã—ã„è¨˜å·ãŒå‡ºãŸã‚‰ã€å®šç¾©ã‚’æ¢ã™
2. **æ¬¡å…ƒã®ç¢ºèª**: å„å¤‰æ•°ã® shape ã‚’è¿½è·¡ã™ã‚‹
3. **ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã®ç¢ºèª**: $n=1$ ã‚„ $d=1$ ã§å¼ã‚’å˜ç´”åŒ–ã—ã¦æ„å‘³ã‚’ç¢ºèª
4. **ã‚³ãƒ¼ãƒ‰ã¨ã®å¯¾å¿œ**: æ•°å¼ã‚’ Python ã«ç¿»è¨³ã—ã¦ã¿ã‚‹

ä¾‹: VAE[^4] ã®å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯
$$
\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, I)
$$

â†’ Python: `z = mu + sigma * np.random.randn(*mu.shape)`
:::

#### Pass 3: å†ç¾ï¼ˆæ•°æ™‚é–“ã€œæ•°æ—¥ï¼‰

**ã‚„ã‚‹ã“ã¨**: è«–æ–‡ã®æ‰‹æ³•ã‚’å®Ÿè£…ã™ã‚‹ã€ã‚ã‚‹ã„ã¯è«–æ–‡ã®ä¸»å¼µã‚’è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼ã™ã‚‹

Pass 3 ã¯å…¨è«–æ–‡ã§è¡Œã†å¿…è¦ã¯ãªã„ã€‚è‡ªåˆ†ã®ç ”ç©¶ã«ç›´çµã™ã‚‹è«–æ–‡ã€ã¾ãŸã¯ã‚·ãƒªãƒ¼ã‚ºã®è¬›ç¾©ãƒ†ãƒ¼ãƒã¨ãªã‚‹è«–æ–‡ã«é™å®šã™ã‚‹ã€‚

#### å®Ÿè·µ: "Attention Is All You Need"[^1] ã® Pass 1

| é …ç›® | å†…å®¹ |
|:---|:---|
| ã‚¿ã‚¤ãƒˆãƒ« | "Attention Is All You Need" â€” Attention æ©Ÿæ§‹ã ã‘ã§ååˆ† |
| å•é¡Œ | æ©Ÿæ¢°ç¿»è¨³ã®ç³»åˆ—å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã€‚RNN/CNN ã®é€æ¬¡å‡¦ç†ãŒä¸¦åˆ—åŒ–ã‚’é˜»å®³ |
| ææ¡ˆ | Transformer: Self-Attention ã®ã¿ã§æ§‹æˆã€‚å†å¸°ãªã—ã€ç•³ã¿è¾¼ã¿ãªã— |
| æ ¸å¿ƒ | Scaled Dot-Product Attention + Multi-Head Attention + Positional Encoding |
| çµæœ | WMT 2014 è‹±ç‹¬ç¿»è¨³ã§ BLEU 28.4ï¼ˆå½“æ™‚SOTAï¼‰ã€‚è¨“ç·´æ™‚é–“ã¯1/10ä»¥ä¸‹ |
| å½±éŸ¿ | BERT, GPT, ViT, DALL-E, ... ç¾ä»£ã®ã»ã¼å…¨ãƒ¢ãƒ‡ãƒ«ã®åŸºç›¤ |

```python
"""è«–æ–‡ã® Pass 1 ã‚’æ§‹é€ åŒ–ã™ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""

pass1_template = {
    "title": "",
    "authors": "",
    "year": 0,
    "arxiv_id": "",
    "problem": "",          # ä½•ã®å•é¡Œã‚’è§£ã„ã¦ã„ã‚‹ã‹
    "limitation": "",       # æ—¢å­˜æ‰‹æ³•ã®é™ç•Œ
    "proposal": "",         # ææ¡ˆæ‰‹æ³•ã®æ ¸å¿ƒ
    "key_equation": "",     # æœ€ã‚‚é‡è¦ãªæ•°å¼ï¼ˆLaTeXï¼‰
    "main_result": "",      # ä¸»è¦ãªæ•°å€¤çµæœ
    "relevance": "",        # è‡ªåˆ†ã¨ã®é–¢é€£
    "pass2_needed": False,  # ç²¾èª­ã™ã¹ãã‹
}

# è¨˜å…¥ä¾‹: Attention Is All You Need
attention_paper = {
    "title": "Attention Is All You Need",
    "authors": "Vaswani, Shazeer, Parmar, et al.",
    "year": 2017,
    "arxiv_id": "1706.03762",
    "problem": "Sequence transduction (machine translation)",
    "limitation": "RNN/CNN require sequential computation, limiting parallelization",
    "proposal": "Transformer: pure attention-based architecture, no recurrence",
    "key_equation": r"Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V",
    "main_result": "BLEU 28.4 on WMT 2014 En-De (SOTA), 10x less training cost",
    "relevance": "Foundation of all modern LLMs and generative models",
    "pass2_needed": True,
}

for key, val in attention_paper.items():
    print(f"  {key:20s}: {val}")
```

#### è«–æ–‡ãƒ»çŸ¥è­˜ç®¡ç† â€” Obsidian ã§çŸ¥è­˜ã‚’ã‚°ãƒ©ãƒ•åŒ–ã™ã‚‹

è«–æ–‡ã‚’èª­ã‚€æŠ€è¡“ã‚’èº«ã«ã¤ã‘ãŸã‚‰ã€æ¬¡ã¯èª­ã‚“ã çŸ¥è­˜ã‚’**æ§‹é€ åŒ–ã—ã¦æ®‹ã™**ä»•çµ„ã¿ã ã€‚40å›ã®è¬›ç¾©ã‚’å—ã‘ã€æ•°åæœ¬ã®è«–æ–‡ã‚’èª­ã¿ã€ä½•ç™¾ã‚‚ã®æ•°å¼ã«è§¦ã‚Œã‚‹ã€‚ã“ã®çŸ¥è­˜ã‚’æ•´ç†ã—ãªã„ã¨ã€3ãƒ¶æœˆå¾Œã«ã¯ä½•ã‚‚è¦šãˆã¦ã„ãªã„ã€‚

#### æ¨å¥¨ãƒ„ãƒ¼ãƒ«: Obsidian

**Obsidian** (obsidian.md) ã¯ãƒ­ãƒ¼ã‚«ãƒ«å®Œçµã®Markdownã‚¨ãƒ‡ã‚£ã‚¿ã€‚æœ€å¤§ã®ç‰¹å¾´ã¯**åŒæ–¹å‘ãƒªãƒ³ã‚¯**ã¨**ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•**ã€‚

| ç‰¹å¾´ | èª¬æ˜ |
|:---|:---|
| ãƒ­ãƒ¼ã‚«ãƒ«å®Œçµ | ãƒ‡ãƒ¼ã‚¿ã¯å…¨ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã® `.md` ãƒ•ã‚¡ã‚¤ãƒ«ã€‚ã‚¯ãƒ©ã‚¦ãƒ‰ä¾å­˜ãªã— |
| åŒæ–¹å‘ãƒªãƒ³ã‚¯ | `[[ãƒãƒ¼ãƒˆå]]` ã§ãƒãƒ¼ãƒˆé–“ã‚’ãƒªãƒ³ã‚¯ã€‚è¢«ãƒªãƒ³ã‚¯ã‚‚è‡ªå‹•è¡¨ç¤º |
| ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ• | ãƒªãƒ³ã‚¯æ§‹é€ ã‚’è¦–è¦šåŒ–ã€‚çŸ¥è­˜ã®å…¨ä½“åƒãŒè¦‹ãˆã‚‹ |
| ãƒ—ãƒ©ã‚°ã‚¤ãƒ³è±Šå¯Œ | ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã§æ©Ÿèƒ½æ‹¡å¼µ |
| æ•°å¼å¯¾å¿œ | KaTeX/MathJax ã§æ•°å¼ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚° |

```mermaid
graph TD
    L1["ç¬¬1å›: æ¦‚è«–"] -->|"å‰æçŸ¥è­˜"| L2["ç¬¬2å›: ç·šå½¢ä»£æ•°"]
    L1 -->|"Softmax"| L3["ç¬¬3å›: å¾®åˆ†"]
    L2 -->|"è¡Œåˆ—"| L6["ç¬¬6å›: KLæƒ…å ±é‡"]
    L3 -->|"å‹¾é…"| L4["ç¬¬4å›: ç¢ºç‡"]
    L4 -->|"ãƒ™ã‚¤ã‚º"| L9["ç¬¬9å›: ELBO"]
    L6 -->|"KL"| L9
    style L1 fill:#e3f2fd
    style L9 fill:#fff3e0
```

#### ãƒ­ãƒ¼ã‚«ãƒ«å®Œçµã‚¹ã‚¿ãƒƒã‚¯

è«–æ–‡ç®¡ç†ã‹ã‚‰åŸ·ç­†ã¾ã§ã€å…¨ã¦ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Œçµã™ã‚‹ãƒ„ãƒ¼ãƒ«ç¾¤:

| ãƒ„ãƒ¼ãƒ« | å½¹å‰² | é€£æº |
|:---|:---|:---|
| **Zotero** | è«–æ–‡PDFç®¡ç†ãƒ»å¼•ç”¨ | Obsidian ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã§é€£æº |
| **Obsidian** | ãƒãƒ¼ãƒˆãƒ»çŸ¥è­˜ç®¡ç† | Markdown â†’ ã©ã“ã§ã‚‚ä½¿ãˆã‚‹ |
| **Longform** | é•·æ–‡åŸ·ç­†ï¼ˆObsidian ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ï¼‰ | ãƒãƒ£ãƒ—ã‚¿ãƒ¼ç®¡ç† |
| **Pandoc** | å‡ºåŠ›å¤‰æ› | Markdown â†’ PDF / LaTeX / DOCX |

```bash
# Zotero ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# https://www.zotero.org/ ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

# Pandoc ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install pandoc   # macOS
# or: sudo apt install pandoc  # Ubuntu

# Markdown â†’ PDF å¤‰æ›
pandoc lecture-notes.md -o lecture-notes.pdf --pdf-engine=lualatex
```

#### ã‚¯ãƒ©ã‚¦ãƒ‰å…±è‘—ãƒ„ãƒ¼ãƒ«: Prism

ãƒãƒ¼ãƒ ã§è«–æ–‡ã‚’æ›¸ãå ´åˆã¯ **Prism** (withprism.ai) ãŒé¸æŠè‚¢ã«å…¥ã‚‹ã€‚OpenAI ãŒé–‹ç™ºã—ãŸAIæ”¯æ´ä»˜ãå…±åŒåŸ·ç­†ãƒ„ãƒ¼ãƒ«ã§ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å…±åŒç·¨é›† + AI ã«ã‚ˆã‚‹æ–‡ç« æ”¹å–„ææ¡ˆãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹ã€‚ãŸã ã—æœ¬ã‚·ãƒªãƒ¼ã‚ºã®å­¦ç¿’ãƒãƒ¼ãƒˆã«ã¯ã‚ªãƒ¼ãƒãƒ¼ã‚¹ãƒšãƒƒã‚¯ â€” ã¾ãšã¯ Obsidian ã§å€‹äººã®çŸ¥è­˜ç®¡ç†ã‚’å›ºã‚ã‚‹ã®ãŒå…ˆæ±ºã€‚

#### è¬›ç¾©ãƒãƒ¼ãƒˆã®å–ã‚Šæ–¹ â€” å®Ÿè·µãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

æœ¬ã‚·ãƒªãƒ¼ã‚º40å›åˆ†ã‚’Obsidianã§ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•åŒ–ã™ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:

```markdown
---
tags: [ml-lecture, zone3, ç·šå½¢ä»£æ•°]
lecture: 2
date: 2025-xx-xx
---

# ç¬¬2å›: ç·šå½¢ä»£æ•° I

#### Key Concepts
- [[è¡Œåˆ—ç©]] â€” $C = AB$ where $C_{ij} = \sum_k A_{ik}B_{kj}$
- [[å›ºæœ‰å€¤åˆ†è§£]] â€” $A\mathbf{v} = \lambda\mathbf{v}$

#### Links
- å‰æ: [[ç¬¬1å›_æ¦‚è«–]]
- æ¬¡å›: [[ç¬¬3å›_å¾®åˆ†]]
- é–¢é€£: [[Attention]] uses [[è¡Œåˆ—ç©]]

#### Questions
- [ ] ãªãœå›ºæœ‰å€¤åˆ†è§£ãŒé‡è¦ï¼Ÿâ†’ [[PCA]] ã§ä½¿ã†ï¼ˆç¬¬5å›ï¼‰

#### Code Snippets
<!-- æ•°å¼ã¨ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œã‚’æ®‹ã™ -->
```

:::details Notion / Scrapbox ã§ã¯ãƒ€ãƒ¡ãªã®ã‹ï¼Ÿ
ä½¿ã£ã¦ã‚‚æ§‹ã‚ãªã„ãŒã€Obsidian ã‚’æ¨å¥¨ã™ã‚‹ç†ç”±:

1. **ãƒ­ãƒ¼ã‚«ãƒ«å®Œçµ**: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸è¦ã€‚ã‚µãƒ¼ãƒ“ã‚¹çµ‚äº†ãƒªã‚¹ã‚¯ã‚¼ãƒ­
2. **Markdown**: æ¨™æº–å½¢å¼ãªã®ã§ä»–ãƒ„ãƒ¼ãƒ«ã¸ã®ç§»è¡ŒãŒå®¹æ˜“
3. **åŒæ–¹å‘ãƒªãƒ³ã‚¯**: è¬›ç¾©é–“ã®é–¢ä¿‚æ€§ãŒè‡ªç„¶ã«æ§‹é€ åŒ–ã•ã‚Œã‚‹
4. **Git/jj ç®¡ç†å¯èƒ½**: `.md` ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã§ãã‚‹
5. **æ•°å¼**: KaTeX å¯¾å¿œã§æ•°å¼ãŒãã®ã¾ã¾ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚Œã‚‹

Notion ã¯ã‚¯ãƒ©ã‚¦ãƒ‰ä¾å­˜ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãŒé¢å€’ã€‚Scrapbox ã¯åŒæ–¹å‘ãƒªãƒ³ã‚¯ã¯å„ªç§€ã ãŒæ•°å¼å¯¾å¿œãŒå¼±ã„ã€‚
:::

### 4.4 LaTeX å…¥é–€ â€” æ•°å¼ã‚’ã€Œæ›¸ãã€åŠ›

æ•°å¼ã‚’ã€Œèª­ã‚€ã€ã ã‘ã§ãªãã€Œæ›¸ãã€åŠ›ã‚‚å¿…è¦ã ã€‚è«–æ–‡ã‚’æ›¸ãã¨ãã¯ã‚‚ã¡ã‚ã‚“ã€Zenn ã®è¨˜äº‹ã‚„ãƒãƒ¼ãƒˆã«æ•°å¼ã‚’æ®‹ã™ã¨ãã«ã‚‚ LaTeX ã‚’ä½¿ã†ã€‚

#### åŸºæœ¬è¨˜æ³•

| æ•°å¼ | LaTeX | å‡ºåŠ› |
|:---|:---|:---|
| åˆ†æ•° | `\frac{a}{b}` | $\frac{a}{b}$ |
| ä¸Šä»˜ã | `x^{2}` | $x^{2}$ |
| ä¸‹ä»˜ã | `x_{i}` | $x_{i}$ |
| å¹³æ–¹æ ¹ | `\sqrt{x}` | $\sqrt{x}$ |
| ç·å’Œ | `\sum_{i=1}^{n} x_i` | $\sum_{i=1}^{n} x_i$ |
| ç·ä¹— | `\prod_{i=1}^{n} x_i` | $\prod_{i=1}^{n} x_i$ |
| ç©åˆ† | `\int_{a}^{b} f(x) dx` | $\int_{a}^{b} f(x) dx$ |
| åå¾®åˆ† | `\frac{\partial f}{\partial x}` | $\frac{\partial f}{\partial x}$ |
| ãƒ™ã‚¯ãƒˆãƒ« | `\mathbf{x}` | $\mathbf{x}$ |
| è¡Œåˆ— | `\mathbf{A}` or `\mathbf{W}` | $\mathbf{A}$ |
| é›†åˆ | `\mathbb{R}^n` | $\mathbb{R}^n$ |
| æå¤±é–¢æ•° | `\mathcal{L}` | $\mathcal{L}$ |
| æœŸå¾…å€¤ | `\mathbb{E}[X]` | $\mathbb{E}[X]$ |

#### Zenn ã§ã®æ•°å¼è¨˜æ³•

Zenn ã¯ KaTeX ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã€‚ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã¯ `$...$`ã€ãƒ–ãƒ­ãƒƒã‚¯ã¯ `$$...$$`:

```markdown
<!-- ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ•°å¼ -->
Softmax ã¯ $\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$ ã§å®šç¾©ã•ã‚Œã‚‹ã€‚

<!-- ãƒ–ãƒ­ãƒƒã‚¯æ•°å¼ -->
$$
\mathcal{L}_{\text{CE}} = -\sum_{i=1}^{C} y_i \log \hat{y}_i
$$
```

:::details KaTeX ã§ä½¿ãˆãªã„ LaTeX ã‚³ãƒãƒ³ãƒ‰ï¼ˆæ³¨æ„ï¼‰
KaTeX ã¯ LaTeX ã®å®Œå…¨äº’æ›ã§ã¯ãªã„ã€‚ä»¥ä¸‹ã¯æ³¨æ„:

| ä½¿ãˆãªã„ | ä»£æ›¿ |
|:---|:---|
| `\text{}` å†…ã®æ—¥æœ¬èª | æ•°å¼å¤–ã«æ›¸ã |
| `\boldsymbol{}` | `\mathbf{}` |
| `\newcommand` | Zenn ã§ã¯ä½¿ãˆãªã„ |
| `aligned` ç’°å¢ƒ | `\begin{aligned}...\end{aligned}` ã¯ä½¿ãˆã‚‹ |

**Tips**: è¤‡é›‘ãªæ•°å¼ã¯ Overleaf ã‹ HackMD ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ã‹ã‚‰ Zenn ã«è²¼ã‚‹ã¨å®‰å…¨ã€‚
:::

#### ç·´ç¿’: Attention ã®å¼ã‚’ LaTeX ã§æ›¸ã

ä»¥ä¸‹ã®æ•°å¼ã‚’ LaTeX ã§æ›¸ã„ã¦ã¿ã‚ˆã†:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

:::details è§£ç­”
```latex
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
```

ãƒã‚¤ãƒ³ãƒˆ:
- `\text{Attention}` â€” é–¢æ•°åã¯ãƒ­ãƒ¼ãƒãƒ³ä½“
- `\left(` `\right)` â€” æ‹¬å¼§ã®ã‚µã‚¤ã‚ºè‡ªå‹•èª¿æ•´
- `K^\top` â€” è»¢ç½®ã€‚`K^T` ã§ã‚‚ã‚ˆã„ãŒ `\top` ãŒæ­£å¼
- `\sqrt{d_k}` â€” å¹³æ–¹æ ¹
- `\frac{}{}` â€” åˆ†æ•°
:::

### 4.5 æ•°å¼ â†” ã‚³ãƒ¼ãƒ‰ç¿»è¨³ â€” 7ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³

è«–æ–‡ã®æ•°å¼ã‚’ã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã™ã‚‹ã¨ãã€é »å‡ºã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ•´ç†ã™ã‚‹ã€‚ã“ã‚Œã‚’çŸ¥ã£ã¦ã„ã‚Œã°ã€åˆè¦‹ã®æ•°å¼ã§ã‚‚è¿·ã‚ãªã„ã€‚

#### Pattern 1: $\sum$ â†’ `np.sum()` / `sum()`

$$
\bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i
$$

```python
x_bar = np.mean(x)  # = np.sum(x) / len(x)
```

#### Pattern 2: $\prod$ â†’ `np.prod()` / å¯¾æ•°å’Œ

$$
p(\mathcal{D}) = \prod_{i=1}^{N} p(x^{(i)})
$$

```python
# ç›´æ¥è¨ˆç®—ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼æ³¨æ„ï¼‰
p_data = np.prod(p_xi)

# å¯¾æ•°ç©ºé–“ï¼ˆæ¨å¥¨ï¼‰
log_p = np.sum(np.log(p_xi))
```

#### Pattern 3: $\arg\max$ â†’ `np.argmax()`

$$
\hat{y} = \arg\max_c p(y = c \mid \mathbf{x})
$$

```python
y_hat = np.argmax(probs)
```

#### Pattern 4: $\mathbb{E}[\cdot]$ â†’ `np.mean()` (ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­)

$$
\mathbb{E}_{p(x)}[f(x)] \approx \frac{1}{N} \sum_{i=1}^{N} f(x^{(i)}), \quad x^{(i)} \sim p
$$

```python
samples = np.random.normal(0, 1, size=10000)  # x ~ p
E_fx = np.mean(f(samples))
```

#### Pattern 5: è¡Œåˆ—ç© $AB$ â†’ `A @ B`

$$
\mathbf{h} = W\mathbf{x} + \mathbf{b}
$$

```python
h = W @ x + b
```

#### Pattern 6: è¦ç´ ã”ã¨ã®æ¼”ç®— $\odot$ â†’ `*`

$$
\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}
$$

```python
z = mu + sigma * epsilon  # element-wise
```

#### Pattern 7: $\nabla_\theta \mathcal{L}$ â†’ è‡ªå‹•å¾®åˆ†

$$
\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta)
$$

```python
# NumPyï¼ˆæ‰‹å‹•ï¼‰
grad = compute_gradient(theta, loss_fn)
theta = theta - alpha * grad

# PyTorchï¼ˆè‡ªå‹•å¾®åˆ†ï¼‰â€” ç¬¬3å›ä»¥é™
# loss.backward()
# optimizer.step()
```

:::details ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œè¡¨ï¼ˆã¾ã¨ã‚ï¼‰
| æ•°å¼ | Python (NumPy) | å‚™è€ƒ |
|:---|:---|:---|
| $\sum_i x_i$ | `np.sum(x)` | axis æŒ‡å®šã§æ¬¡å…ƒåˆ¶å¾¡ |
| $\prod_i x_i$ | `np.prod(x)` | å¯¾æ•°ç©ºé–“æ¨å¥¨ |
| $\arg\max$ | `np.argmax(x)` | |
| $\mathbb{E}[f(x)]$ | `np.mean(f(samples))` | ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ |
| $AB$ | `A @ B` | è¡Œåˆ—ç© |
| $A \odot B$ | `A * B` | è¦ç´ ç© |
| $A^\top$ | `A.T` | è»¢ç½® |
| $\|x\|_2$ | `np.linalg.norm(x)` | |
| $\nabla f$ | æ‰‹å‹• or autograd | ç¬¬3å›ä»¥é™ |
| $\mathcal{N}(\mu, \sigma^2)$ | `np.random.normal(mu, sigma)` | |
| $\mathbb{1}[c]$ | `(condition).astype(int)` | æŒ‡ç¤ºé–¢æ•° |
:::

:::message
**é€²æ—: 75% å®Œäº†** é–‹ç™ºç’°å¢ƒã€ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ´»ç”¨ã€è«–æ–‡èª­è§£ãƒ»çŸ¥è­˜ç®¡ç†ã€LaTeXã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ã¾ã§ä¸€é€šã‚Šã‚«ãƒãƒ¼ã—ãŸã€‚æ®‹ã‚Šã¯è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆã¨ã¾ã¨ã‚ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

> **ç›®æ¨™**: Zone 3-4 ã®å†…å®¹ã‚’æœ¬å½“ã«ç†è§£ã—ã¦ã„ã‚‹ã‹ã€è‡ªåˆ†ã§ç¢ºèªã™ã‚‹ã€‚ã€Œã‚ã‹ã£ãŸã¤ã‚‚ã‚Šã€ã‚’æ’é™¤ã™ã‚‹ã€‚

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆï¼ˆ10å•ï¼‰

ä»¥ä¸‹ã®æ•°å¼ã‚’**æ—¥æœ¬èªã§**èª¬æ˜ã›ã‚ˆã€‚ç­”ãˆã‚’è¦‹ã‚‹å‰ã«ã€è‡ªåˆ†ã§æ›¸ã„ã¦ã¿ã‚‹ã“ã¨ã€‚

:::details Q1: $\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta)$
**A**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’ã€æå¤±é–¢æ•° $\mathcal{L}$ ã® $\theta$ ã«ã¤ã„ã¦ã®å‹¾é… $\nabla_\theta \mathcal{L}$ ã«å­¦ç¿’ç‡ $\alpha$ ã‚’æ›ã‘ãŸåˆ†ã ã‘æ›´æ–°ã™ã‚‹ã€‚ã“ã‚ŒãŒ**å‹¾é…é™ä¸‹æ³•**ï¼ˆGradient Descentï¼‰ã®1ã‚¹ãƒ†ãƒƒãƒ—ã€‚Rumelhart et al. (1986)[^2] ãŒèª¤å·®é€†ä¼æ’­æ³•ã¨çµ„ã¿åˆã‚ã›ã¦ææ¡ˆã—ãŸå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŸºæœ¬å½¢ã€‚
:::

:::details Q2: $p_\theta(\mathbf{x}) = \int p_\theta(\mathbf{x}, \mathbf{z}) \, d\mathbf{z}$
**A**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ã«å¯¾ã™ã‚‹ç¢ºç‡ã‚’ã€æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã«ã¤ã„ã¦å‘¨è¾ºåŒ–ï¼ˆç©åˆ†æ¶ˆå»ï¼‰ã—ã¦æ±‚ã‚ã‚‹ã€‚ã“ã‚ŒãŒ**å‘¨è¾ºå°¤åº¦**ï¼ˆmarginal likelihoodï¼‰ã€‚VAE[^4] ã§ã¯ã“ã®ç©åˆ†ãŒè§£æçš„ã«è¨ˆç®—ã§ããªã„ãŸã‚ã€å¤‰åˆ†ä¸‹ç•Œï¼ˆELBOï¼‰ã§è¿‘ä¼¼ã™ã‚‹ã€‚
:::

:::details Q3: $D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))$
**A**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ $q_\phi$ ãŒå‡ºåŠ›ã™ã‚‹äº‹å¾Œåˆ†å¸ƒã¨ã€äº‹å‰åˆ†å¸ƒ $p(\mathbf{z})$ï¼ˆé€šå¸¸ $\mathcal{N}(0, I)$ï¼‰ã®é–“ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€‚VAE[^4] ã®æ­£å‰‡åŒ–é …ã¨ã—ã¦æ©Ÿèƒ½ã—ã€æ½œåœ¨ç©ºé–“ãŒæ§‹é€ ã‚’æŒã¤ã‚ˆã†ã«åˆ¶ç´„ã™ã‚‹ã€‚
:::

:::details Q4: $\text{softmax}(z_i / \tau)$
**A**: ãƒ­ã‚¸ãƒƒãƒˆ $z_i$ ã‚’æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\tau$ ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãŸå¾Œã« Softmax ã‚’é©ç”¨ã€‚$\tau \to 0$ ã§ argmaxï¼ˆæœ€ã‚‚ç¢ºç‡ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã®ã¿1ï¼‰ã€$\tau \to \infty$ ã§ä¸€æ§˜åˆ†å¸ƒã«è¿‘ã¥ãã€‚Hinton et al. (2015)[^3] ãŒKnowledge Distillation ã§ä½¿ç”¨ã€‚
:::

:::details Q5: $\hat{y} = \arg\max_{c \in \{1,\ldots,C\}} p_\theta(y = c \mid \mathbf{x})$
**A**: å…¥åŠ› $\mathbf{x}$ ã«å¯¾ã—ã¦ã€$C$ å€‹ã®ã‚¯ãƒ©ã‚¹ã®ä¸­ã§äº‹å¾Œç¢ºç‡ $p_\theta(y = c | \mathbf{x})$ ãŒæœ€å¤§ã¨ãªã‚‹ã‚¯ãƒ©ã‚¹ $c$ ã‚’äºˆæ¸¬ãƒ©ãƒ™ãƒ« $\hat{y}$ ã¨ã™ã‚‹ã€‚åˆ†é¡å•é¡Œã®æ¨è«–æ™‚ã®æ“ä½œã€‚
:::

:::details Q6: $W_{ij}^{(l)} \in \mathbb{R}$
**A**: ç¬¬ $l$ å±¤ã®é‡ã¿è¡Œåˆ—ã® $(i, j)$ æˆåˆ†ã€‚å®Ÿæ•°å€¤ã‚¹ã‚«ãƒ©ãƒ¼ã€‚ä¸Šä»˜ãã® $(l)$ ã¯å±¤ç•ªå·ã€ä¸‹ä»˜ãã® $ij$ ã¯è¡Œåˆ—ã®è¡Œãƒ»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚
:::

:::details Q7: $f: \mathbb{R}^n \to \mathbb{R}^m$
**A**: é–¢æ•° $f$ ã¯ $n$ æ¬¡å…ƒå®Ÿæ•°ãƒ™ã‚¯ãƒˆãƒ«ã‚’å—ã‘å–ã‚Šã€$m$ æ¬¡å…ƒå®Ÿæ•°ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™å†™åƒã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å„å±¤ã¯ã“ã®å½¢ã®å†™åƒã€‚
:::

:::details Q8: $\mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(\mathbf{x})]$
**A**: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸ $\mathbf{x}$ ã«ã¤ã„ã¦ã€ãƒ¢ãƒ‡ãƒ« $p_\theta$ ã®å¯¾æ•°ç¢ºç‡ã®æœŸå¾…å€¤ã€‚ã“ã‚Œã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ãŒ**æœ€å°¤æ¨å®š**ï¼ˆMaximum Likelihood Estimationï¼‰ã«ç›¸å½“ã™ã‚‹ã€‚
:::

:::details Q9: $\epsilon_t \sim \mathcal{N}(0, I)$
**A**: æ™‚åˆ»ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ï¼‰$t$ ã®ãƒã‚¤ã‚º $\epsilon_t$ ã‚’ã€å¹³å‡0ã€å…±åˆ†æ•£ãŒå˜ä½è¡Œåˆ— $I$ ã®å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«[^5]ã®forward processã§å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒã‚¤ã‚ºã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
:::

:::details Q10: $\|\nabla_\theta \mathcal{L}\|_2$
**A**: æå¤±é–¢æ•° $\mathcal{L}$ ã® $\theta$ ã«ã¤ã„ã¦ã®å‹¾é…ãƒ™ã‚¯ãƒˆãƒ«ã®L2ãƒãƒ«ãƒ ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ãƒãƒ«ãƒ ï¼‰ã€‚**å‹¾é…ãƒãƒ«ãƒ **ã¨å‘¼ã°ã‚Œã€å­¦ç¿’ã®å®‰å®šæ€§ã®æŒ‡æ¨™ã¨ã—ã¦ç›£è¦–ã•ã‚Œã‚‹ã€‚ã“ã‚ŒãŒçˆ†ç™ºï¼ˆexplodingï¼‰ã™ã‚‹ã¨å­¦ç¿’ãŒç ´ç¶»ã—ã€æ¶ˆå¤±ï¼ˆvanishingï¼‰ã™ã‚‹ã¨å­¦ç¿’ãŒåœæ»ã™ã‚‹ã€‚
:::

### 5.2 LaTeX æ›¸ãå–ã‚Šãƒ†ã‚¹ãƒˆï¼ˆ5å•ï¼‰

ä»¥ä¸‹ã®æ•°å¼ã‚’ **LaTeX ã§æ›¸ã‘**ã€‚KaTeX ã§æ­£ã—ãè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã›ã‚ˆã€‚

:::details Q1: Cross-Entropy Loss
**ç›®æ¨™**:
$$
\mathcal{L}_{\text{CE}} = -\sum_{i=1}^{C} y_i \log \hat{y}_i
$$

**è§£ç­”**:
```latex
\mathcal{L}_{\text{CE}} = -\sum_{i=1}^{C} y_i \log \hat{y}_i
```
:::

:::details Q2: Scaled Dot-Product Attention
**ç›®æ¨™**:
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

**è§£ç­”**:
```latex
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
```
:::

:::details Q3: KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
**ç›®æ¨™**:
$$
D_{\text{KL}}(q \| p) = \sum_{x} q(x) \log \frac{q(x)}{p(x)}
$$

**è§£ç­”**:
```latex
D_{\text{KL}}(q \| p) = \sum_{x} q(x) \log \frac{q(x)}{p(x)}
```
:::

:::details Q4: å‹¾é…é™ä¸‹æ³•
**ç›®æ¨™**:
$$
\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla_\theta \mathcal{L}(\theta^{(t)})
$$

**è§£ç­”**:
```latex
\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla_\theta \mathcal{L}(\theta^{(t)})
```
:::

:::details Q5: VAE ã® ELBO
**ç›®æ¨™**:
$$
\mathcal{L}_{\text{ELBO}} = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x}|\mathbf{z}) \right] - D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))
$$

**è§£ç­”**:
```latex
\mathcal{L}_{\text{ELBO}} = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x}|\mathbf{z}) \right] - D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))
```
:::

### 5.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆï¼ˆ5å•ï¼‰

ä»¥ä¸‹ã®æ•°å¼ã‚’ **NumPy ã§å®Ÿè£…ã›ã‚ˆ**ã€‚

:::details Q1: Softmax
$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}
$$

```python
def softmax(z):
    """æ•°å€¤å®‰å®šãª Softmax"""
    e_z = np.exp(z - np.max(z))  # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
    return e_z / np.sum(e_z)

# ãƒ†ã‚¹ãƒˆ
z = np.array([2.0, 1.0, 0.1])
p = softmax(z)
print(f"softmax({z}) = {p.round(4)}")
print(f"sum = {p.sum():.6f}")  # 1.0
```
:::

:::details Q2: Cross-Entropy Loss
$$
\mathcal{L}_{\text{CE}} = -\sum_{i=1}^{C} y_i \log(\hat{y}_i + \epsilon)
$$

```python
def cross_entropy(y_true, y_pred, eps=1e-12):
    """Cross-Entropy Loss"""
    return -np.sum(y_true * np.log(y_pred + eps))

# ãƒ†ã‚¹ãƒˆ: æ­£è§£ãŒã‚¯ãƒ©ã‚¹2
y_true = np.array([0, 0, 1, 0])  # one-hot
y_pred = np.array([0.1, 0.05, 0.8, 0.05])  # Softmax å‡ºåŠ›

loss = cross_entropy(y_true, y_pred)
print(f"L_CE = {loss:.4f}")  # -log(0.8) â‰ˆ 0.2231
```
:::

:::details Q3: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
$$
\text{cos}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a}^\top \mathbf{b}}{\|\mathbf{a}\|_2 \|\mathbf{b}\|_2}
$$

```python
def cosine_similarity(a, b):
    """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# ãƒ†ã‚¹ãƒˆ
a = np.array([1.0, 2.0, 3.0])
b = np.array([1.0, 2.0, 3.0])
c = np.array([-1.0, -2.0, -3.0])

print(f"cos(a, b) = {cosine_similarity(a, b):.4f}")   # 1.0 (åŒæ–¹å‘)
print(f"cos(a, c) = {cosine_similarity(a, c):.4f}")   # -1.0 (é€†æ–¹å‘)
```
:::

:::details Q4: æ­£è¦åˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦
$$
\log \mathcal{N}(x; \mu, \sigma^2) = -\frac{1}{2}\left(\log(2\pi\sigma^2) + \frac{(x - \mu)^2}{\sigma^2}\right)
$$

```python
def log_normal_pdf(x, mu, sigma):
    """æ­£è¦åˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦"""
    return -0.5 * (np.log(2 * np.pi * sigma**2) + (x - mu)**2 / sigma**2)

# ãƒ†ã‚¹ãƒˆ: N(0,1) ã§ x=0 ã®å¯¾æ•°ç¢ºç‡å¯†åº¦
print(f"log N(0; 0, 1) = {log_normal_pdf(0, 0, 1):.4f}")  # -0.9189
print(f"ç†è«–å€¤: -0.5 * log(2Ï€) = {-0.5 * np.log(2 * np.pi):.4f}")
```
:::

:::details Q5: ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•
$$
\theta \leftarrow \theta - \frac{\alpha}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_\theta \ell(\theta; x^{(i)}, y^{(i)})
$$

```python
def sgd_step(theta, X_batch, y_batch, grad_fn, alpha=0.01):
    """
    ãƒŸãƒ‹ãƒãƒƒãƒ SGD ã®1ã‚¹ãƒ†ãƒƒãƒ—

    Parameters:
        theta: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        X_batch: ãƒŸãƒ‹ãƒãƒƒãƒå…¥åŠ› (batch_size, d)
        y_batch: ãƒŸãƒ‹ãƒãƒƒãƒãƒ©ãƒ™ãƒ« (batch_size,)
        grad_fn: å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
        alpha: å­¦ç¿’ç‡
    """
    batch_size = len(X_batch)
    # Î£ âˆ‡Î¸ â„“(Î¸; x^(i), y^(i)) / |B|
    grad_sum = np.zeros_like(theta)
    for i in range(batch_size):
        grad_sum += grad_fn(theta, X_batch[i], y_batch[i])
    avg_grad = grad_sum / batch_size

    # Î¸ â† Î¸ - Î± * avg_grad
    return theta - alpha * avg_grad
```
:::

### 5.4 è«–æ–‡èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®è«–æ–‡æƒ…å ±ã‚’èª­ã‚“ã§ã€Pass 1 ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’åŸ‹ã‚ã‚ˆã€‚

**å¯¾è±¡**: Ho et al. (2020) "Denoising Diffusion Probabilistic Models"[^5]

:::details ãƒ’ãƒ³ãƒˆ
arXiv ID: 2006.11239ã€‚Abstract ã‚’èª­ã‚€ã ã‘ã§ Pass 1 ã¯å®Œæˆã™ã‚‹ã€‚

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: diffusion process, denoising, variational inference, progressive lossy decompression
:::

:::details è§£ç­”ä¾‹
| é …ç›® | å†…å®¹ |
|:---|:---|
| ã‚¿ã‚¤ãƒˆãƒ« | Denoising Diffusion Probabilistic Models |
| è‘—è€… | Ho, Jain, Abbeel |
| å¹´ | 2020 |
| arXiv ID | 2006.11239 |
| å•é¡Œ | é«˜å“è³ªãªç”»åƒç”Ÿæˆ |
| æ—¢å­˜æ‰‹æ³•ã®é™ç•Œ | GAN[^8]ã¯è¨“ç·´ä¸å®‰å®šã€VAE[^4]ã¯ç”Ÿæˆå“è³ªã«é™ç•Œ |
| ææ¡ˆ | æ‹¡æ•£éç¨‹ï¼ˆãƒã‚¤ã‚ºä»˜åŠ â†’é™¤å»ï¼‰ã«ã‚ˆã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ« |
| æ ¸å¿ƒæ•°å¼ | $L_{\text{simple}} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \right]$ |
| ä¸»è¦çµæœ | FID 3.17 on CIFAR-10ï¼ˆå½“æ™‚ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§SOTAå“è³ªï¼‰ |
| é–¢é€£åº¦ | æœ¬ã‚·ãƒªãƒ¼ã‚ºç¬¬11-14å›ã§è©³è§£ |
:::

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: 3ã¤ã®ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

ã“ã‚Œã¾ã§ã®çŸ¥è­˜ã‚’çµ±åˆã™ã‚‹3ã¤ã®å®Ÿè£…èª²é¡Œã€‚æ‰€è¦æ™‚é–“ã¯åˆè¨ˆ1-2æ™‚é–“ã€‚

#### Challenge 1: æ•°å¼ãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆè¨˜å·â†’èª¬æ˜è¾æ›¸ï¼‰

```python
"""
Challenge: æ•°å¼ã®å„è¨˜å·ã‚’è‡ªå‹•çš„ã«è§£èª¬ã™ã‚‹ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½œã‚‹
å…¥åŠ›: LaTeX æ–‡å­—åˆ—ï¼ˆã®ç°¡æ˜“ç‰ˆï¼‰
å‡ºåŠ›: å„è¨˜å·ã®æ—¥æœ¬èªèª¬æ˜
"""
import re

# è¨˜å·è¾æ›¸
SYMBOL_DB = {
    r"\theta": ("ã‚·ãƒ¼ã‚¿", "ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"),
    r"\phi": ("ãƒ•ã‚¡ã‚¤", "ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€/å¤‰åˆ†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"),
    r"\mu": ("ãƒŸãƒ¥ãƒ¼", "å¹³å‡"),
    r"\sigma": ("ã‚·ã‚°ãƒ", "æ¨™æº–åå·®"),
    r"\nabla": ("ãƒŠãƒ–ãƒ©", "å‹¾é…æ¼”ç®—å­"),
    r"\mathcal{L}": ("ã‚¨ãƒ«", "æå¤±é–¢æ•°"),
    r"\mathbb{E}": ("ã‚¤ãƒ¼", "æœŸå¾…å€¤"),
    r"\sum": ("ã‚·ã‚°ãƒ", "ç·å’Œ"),
    r"\prod": ("ãƒ‘ã‚¤", "ç·ä¹—"),
    r"\partial": ("ãƒ‘ãƒ¼ã‚·ãƒ£ãƒ«", "åå¾®åˆ†"),
    r"\alpha": ("ã‚¢ãƒ«ãƒ•ã‚¡", "å­¦ç¿’ç‡"),
    r"\epsilon": ("ã‚¤ãƒ—ã‚·ãƒ­ãƒ³", "å¾®å°é‡/ãƒã‚¤ã‚º"),
    r"\lambda": ("ãƒ©ãƒ ãƒ€", "æ­£å‰‡åŒ–ä¿‚æ•°"),
    r"\mathbb{R}": ("ã‚¢ãƒ¼ãƒ«", "å®Ÿæ•°ã®é›†åˆ"),
    r"\in": ("å±ã™ã‚‹", "é›†åˆã®è¦ç´ "),
    r"\forall": ("ã™ã¹ã¦ã®", "å…¨ç§°é‡åŒ–å­"),
    r"\exists": ("å­˜åœ¨ã™ã‚‹", "å­˜åœ¨é‡åŒ–å­"),
    r"\sqrt": ("ãƒ«ãƒ¼ãƒˆ", "å¹³æ–¹æ ¹"),
    r"\frac": ("åˆ†æ•°", "åˆ†å­/åˆ†æ¯"),
    r"\log": ("ãƒ­ã‚°", "å¯¾æ•°é–¢æ•°"),
    r"\exp": ("ã‚¨ã‚¯ã‚¹ãƒ—", "æŒ‡æ•°é–¢æ•°"),
    r"\top": ("ãƒˆãƒƒãƒ—", "è»¢ç½®"),
    r"\text{softmax}": ("ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹", "ç¢ºç‡åˆ†å¸ƒã¸ã®å¤‰æ›"),
}

def parse_symbols(latex_str):
    """LaTeX æ–‡å­—åˆ—ã‹ã‚‰æ—¢çŸ¥ã®è¨˜å·ã‚’æŠ½å‡ºã—ã¦è§£èª¬"""
    found = []
    for symbol, (reading, meaning) in SYMBOL_DB.items():
        if symbol in latex_str:
            found.append((symbol, reading, meaning))
    return found

# ãƒ†ã‚¹ãƒˆ
formulas = [
    r"\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V",
    r"\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta)",
    r"\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x}|\mathbf{z}) \right]",
]

for formula in formulas:
    print(f"\næ•°å¼: {formula[:60]}...")
    symbols = parse_symbols(formula)
    for sym, reading, meaning in symbols:
        print(f"  {sym:25s} ({reading}) â†’ {meaning}")
```

#### Challenge 2: Attention ã®å¯è¦–åŒ–

```python
"""
Challenge: Attention weights ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–ã™ã‚‹
"""
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

def softmax(x, axis=-1):
    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return e_x / np.sum(e_x, axis=axis, keepdims=True)

def attention_with_viz(Q, K, V, labels_q=None, labels_k=None):
    """Attention ã‚’è¨ˆç®—ã—ã¦å¯è¦–åŒ–"""
    d_k = Q.shape[-1]
    scores = Q @ K.T / np.sqrt(d_k)
    weights = softmax(scores)
    output = weights @ V

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # ç”Ÿã®ã‚¹ã‚³ã‚¢
    im0 = axes[0].imshow(scores, cmap='RdBu_r', aspect='auto')
    axes[0].set_title("Raw scores (QK^T/âˆšdk)")
    axes[0].set_xlabel("Key")
    axes[0].set_ylabel("Query")
    plt.colorbar(im0, ax=axes[0])

    # Attention weights (softmax å¾Œ)
    im1 = axes[1].imshow(weights, cmap='Blues', aspect='auto', vmin=0, vmax=1)
    axes[1].set_title("Attention weights (after softmax)")
    axes[1].set_xlabel("Key")
    axes[1].set_ylabel("Query")
    plt.colorbar(im1, ax=axes[1])

    if labels_q:
        for ax in axes:
            ax.set_yticks(range(len(labels_q)))
            ax.set_yticklabels(labels_q)
    if labels_k:
        for ax in axes:
            ax.set_xticks(range(len(labels_k)))
            ax.set_xticklabels(labels_k, rotation=45, ha='right')

    plt.tight_layout()
    plt.savefig("attention_heatmap.png", dpi=100, bbox_inches='tight')
    print("â†’ attention_heatmap.png ã«ä¿å­˜")
    return output, weights

# ãƒ†ã‚¹ãƒˆ: å˜èªã®åŸ‹ã‚è¾¼ã¿ã‚’æ¨¡æ“¬
np.random.seed(42)
n_queries, n_keys, d_model = 4, 6, 64

Q = np.random.randn(n_queries, d_model)
K = np.random.randn(n_keys, d_model)
V = np.random.randn(n_keys, d_model)

# æ„å›³çš„ã« Q[0] ã¨ K[2] ã‚’é¡ä¼¼ã•ã›ã‚‹
K[2] = Q[0] + np.random.randn(d_model) * 0.1

labels_q = ["Query_0", "Query_1", "Query_2", "Query_3"]
labels_k = ["Key_0", "Key_1", "Key_2", "Key_3", "Key_4", "Key_5"]

output, weights = attention_with_viz(Q, K, V, labels_q, labels_k)
print(f"\nQuery_0 ã® Attention weights:")
for i, w in enumerate(weights[0]):
    bar = "â–ˆ" * int(w * 50)
    print(f"  Key_{i}: {w:.4f} {bar}")
```

#### Challenge 3: å­¦ç¿’æ›²ç·šã®å®Ÿè£…ã¨å¯è¦–åŒ–

```python
"""
Challenge: ç°¡å˜ãªç·šå½¢å›å¸°ã‚’å‹¾é…é™ä¸‹æ³•ã§è§£ã„ã¦å­¦ç¿’æ›²ç·šã‚’æã
"""
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# --- ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ ---
np.random.seed(42)
N = 100
x_true = np.random.uniform(-3, 3, N)
y_true = 2.5 * x_true + 1.0 + np.random.randn(N) * 0.5  # y = 2.5x + 1.0 + noise

# --- ãƒ¢ãƒ‡ãƒ«: y = wx + b ---
# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸ = (w, b)
w, b = 0.0, 0.0
alpha = 0.01  # å­¦ç¿’ç‡ Î±
n_epochs = 200

# --- å‹¾é…é™ä¸‹æ³• ---
history = {"epoch": [], "loss": [], "w": [], "b": []}

for epoch in range(n_epochs):
    # äºˆæ¸¬: Å· = wx + b
    y_pred = w * x_true + b

    # æå¤±: L = (1/N) Î£(Å· - y)Â²  (MSE)
    loss = np.mean((y_pred - y_true)**2)

    # å‹¾é…: âˆ‚L/âˆ‚w = (2/N) Î£(Å· - y)Â·x
    #        âˆ‚L/âˆ‚b = (2/N) Î£(Å· - y)
    residual = y_pred - y_true
    grad_w = 2 * np.mean(residual * x_true)
    grad_b = 2 * np.mean(residual)

    # æ›´æ–°: Î¸ â† Î¸ - Î±âˆ‡L
    w -= alpha * grad_w
    b -= alpha * grad_b

    history["epoch"].append(epoch)
    history["loss"].append(loss)
    history["w"].append(w)
    history["b"].append(b)

print(f"æœ€çµ‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: w = {w:.4f} (çœŸå€¤ 2.5), b = {b:.4f} (çœŸå€¤ 1.0)")
print(f"æœ€çµ‚æå¤±: L = {history['loss'][-1]:.4f}")

# --- å¯è¦–åŒ– ---
fig, axes = plt.subplots(1, 3, figsize=(14, 4))

# å­¦ç¿’æ›²ç·š
axes[0].plot(history["epoch"], history["loss"])
axes[0].set_xlabel("Epoch")
axes[0].set_ylabel("Loss (MSE)")
axes[0].set_title("Learning Curve")
axes[0].set_yscale('log')

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ
axes[1].plot(history["epoch"], history["w"], label="w (â†’ 2.5)")
axes[1].plot(history["epoch"], history["b"], label="b (â†’ 1.0)")
axes[1].axhline(y=2.5, color='C0', linestyle='--', alpha=0.5)
axes[1].axhline(y=1.0, color='C1', linestyle='--', alpha=0.5)
axes[1].set_xlabel("Epoch")
axes[1].set_ylabel("Parameter value")
axes[1].set_title("Parameter Convergence")
axes[1].legend()

# ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœ
axes[2].scatter(x_true, y_true, alpha=0.5, s=10, label="data")
x_line = np.linspace(-3, 3, 100)
axes[2].plot(x_line, w * x_line + b, 'r-', label=f"y = {w:.2f}x + {b:.2f}")
axes[2].plot(x_line, 2.5 * x_line + 1.0, 'g--', alpha=0.5, label="true")
axes[2].set_xlabel("x")
axes[2].set_ylabel("y")
axes[2].set_title("Linear Regression Fit")
axes[2].legend()

plt.tight_layout()
plt.savefig("learning_curve.png", dpi=100, bbox_inches='tight')
print("â†’ learning_curve.png ã«ä¿å­˜")
```

:::message
**Challenge ã‚¯ãƒªã‚¢åŸºæº–**: 3ã¤ã®ã†ã¡2ã¤ä»¥ä¸Šã‚’å®Ÿè¡Œã—ã¦çµæœã‚’ç¢ºèªã§ãã‚Œã°ã‚¯ãƒªã‚¢ã€‚ã‚³ãƒ¼ãƒ‰ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹ã“ã¨ï¼ˆå†™çµŒã§ã¯ãªãç†è§£ã—ã¦ã„ã‚‹ã“ã¨ï¼‰ãŒé‡è¦ã€‚
:::

### 5.6 ç·åˆè¨ºæ–­: ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

ä»¥ä¸‹ã®å…¨é …ç›®ã«ã€Œã¯ã„ã€ã¨ç­”ãˆã‚‰ã‚Œã‚Œã°ã€Zone 5 ã‚¯ãƒªã‚¢:

- [ ] ã‚®ãƒªã‚·ãƒ£æ–‡å­— $\theta, \phi, \mu, \sigma, \nabla, \alpha, \epsilon$ ã®æ„å‘³ã‚’å³ç­”ã§ãã‚‹
- [ ] $W_{ij}^{(l)}$ ã®æ·»å­—ã‚’ã€Œç¬¬$l$å±¤ã®$i$è¡Œ$j$åˆ—ã€ã¨èª­ã‚ã‚‹
- [ ] $\sum$ã€$\prod$ã€$\arg\max$ã€$\mathbb{E}$ ã‚’ Python ã«ç¿»è¨³ã§ãã‚‹
- [ ] $\mathbb{R}^n$ã€$\in$ã€$\forall$ã€$\exists$ ã®æ„å‘³ãŒã‚ã‹ã‚‹
- [ ] $f: \mathbb{R}^n \to \mathbb{R}^m$ ã‚’ã€Œ$n$æ¬¡å…ƒå…¥åŠ›ã‹ã‚‰$m$æ¬¡å…ƒå‡ºåŠ›ã¸ã®å†™åƒã€ã¨èª­ã‚ã‚‹
- [ ] Attention ã®å¼ $\text{softmax}(QK^\top / \sqrt{d_k})V$ ã‚’ä¸€æ–‡å­—æ®‹ã‚‰ãšèª¬æ˜ã§ãã‚‹
- [ ] LaTeX ã§åŸºæœ¬çš„ãªæ•°å¼ï¼ˆåˆ†æ•°ã€æ·»å­—ã€ç·å’Œï¼‰ã‚’æ›¸ã‘ã‚‹
- [ ] arXiv ã® ID ã‹ã‚‰è«–æ–‡ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹
- [ ] è«–æ–‡ã® Pass 1 ã‚’10åˆ†ä»¥å†…ã§å®Ÿè¡Œã§ãã‚‹
- [ ] æ•°å¼â†’Python ã®7ã¤ã®ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ãˆã‚‹

:::message
**é€²æ—: 85% å®Œäº†** è‡ªå·±è¨ºæ–­ã‚’é€šã˜ã¦ç†è§£ã®ç©´ã‚’åŸ‹ã‚ãŸã€‚ã‚ã¨ã¯å…¨ä½“ã®ã¾ã¨ã‚ã¨æ¬¡å›ã¸ã®æ©‹æ¸¡ã—ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 ç”¨èªé›†ï¼ˆæœ¬è¬›ç¾©ã§ç™»å ´ã—ãŸç”¨èªï¼‰

:::details ç”¨èªé›†ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹ï¼‰
| ç”¨èª | è‹±èª | å®šç¾© |
|:---|:---|:---|
| å‹¾é… | gradient | å¤šå¤‰æ•°é–¢æ•°ã®å„åå¾®åˆ†ã‚’ä¸¦ã¹ãŸãƒ™ã‚¯ãƒˆãƒ«ã€‚$\nabla f$ |
| å‹¾é…é™ä¸‹æ³• | gradient descent | å‹¾é…ã®é€†æ–¹å‘ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹æœ€é©åŒ–æ‰‹æ³• |
| èª¤å·®é€†ä¼æ’­æ³• | backpropagation | åˆæˆé–¢æ•°ã®é€£é–å¾‹ã‚’ç”¨ã„ã¦å‹¾é…ã‚’åŠ¹ç‡çš„ã«è¨ˆç®—ã™ã‚‹æ‰‹æ³•[^2] |
| æå¤±é–¢æ•° | loss function | ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã¨æ­£è§£ã®ä¹–é›¢ã‚’æ¸¬ã‚‹é–¢æ•°ã€‚$\mathcal{L}$ |
| äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | cross-entropy | 2ã¤ã®ç¢ºç‡åˆ†å¸ƒã®å·®ç•°ã‚’æ¸¬ã‚‹æå¤±é–¢æ•° |
| ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ | softmax | å®Ÿæ•°ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç¢ºç‡åˆ†å¸ƒã«å¤‰æ›ã™ã‚‹é–¢æ•°[^1] |
| æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | temperature | Softmax ã®ã‚·ãƒ£ãƒ¼ãƒ—ã•ã‚’åˆ¶å¾¡ã™ã‚‹ã‚¹ã‚«ãƒ©ãƒ¼[^3] |
| ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ | attention | ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®é¡ä¼¼åº¦ã§å€¤ã‚’é‡ã¿ä»˜ã‘é›†ç´„ã™ã‚‹æ©Ÿæ§‹[^1] |
| æ½œåœ¨å¤‰æ•° | latent variable | ãƒ‡ãƒ¼ã‚¿ã®èƒŒå¾Œã«ã‚ã‚‹è¦³æ¸¬ã•ã‚Œãªã„å¤‰æ•°ã€‚$\mathbf{z}$ |
| å¤‰åˆ†æ¨è«– | variational inference | äº‹å¾Œåˆ†å¸ƒã‚’è¿‘ä¼¼ã™ã‚‹ãŸã‚ã®æœ€é©åŒ–ãƒ™ãƒ¼ã‚¹ã®æ¨è«–æ‰‹æ³•[^4] |
| ELBO | evidence lower bound | å‘¨è¾ºå°¤åº¦ã®ä¸‹ç•Œã€‚VAEã®ç›®çš„é–¢æ•°[^4] |
| KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | KL divergence | 2ã¤ã®åˆ†å¸ƒé–“ã®éå¯¾ç§°ãªã€Œè·é›¢ã€ |
| æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« | diffusion model | ãƒ‡ãƒ¼ã‚¿ã«ãƒã‚¤ã‚ºã‚’åŠ ãˆã€é™¤å»ã™ã‚‹éç¨‹ã§å­¦ç¿’ã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«[^5] |
| ãƒ•ãƒ­ãƒ¼ãƒãƒƒãƒãƒ³ã‚° | flow matching | ç¢ºç‡çš„ãªãƒ•ãƒ­ãƒ¼ã§ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¸ã®å¤‰æ›ã‚’å­¦ç¿’[^6] |
| arXiv | arXiv | ç‰©ç†å­¦ãƒ»æ•°å­¦ãƒ»è¨ˆç®—æ©Ÿç§‘å­¦ã®ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆã‚µãƒ¼ãƒãƒ¼ |
| ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆ | preprint | æŸ»èª­å‰ã®è«–æ–‡ |
| å†™åƒ | mapping / function | å®šç¾©åŸŸã®å„è¦ç´ ã‚’å€¤åŸŸã®è¦ç´ ã«å¯¾å¿œã•ã›ã‚‹è¦å‰‡ |
| ç¢ºç‡å˜ä½“ | probability simplex | éè² ã§ç·å’Œ1ã®ãƒ™ã‚¯ãƒˆãƒ«ã®é›†åˆã€‚$\Delta^{C-1}$ |
:::

### 6.2 ç¬¬1å›ã®çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
mindmap
    root((ç¬¬1å›))
        æ•°å¼è¨˜å·
            ã‚®ãƒªã‚·ãƒ£æ–‡å­—
            æ·»å­—
            æ¼”ç®—å­
        æ•°å¼æ–‡æ³•
            é›†åˆè«–
            è«–ç†è¨˜å·
            é–¢æ•°è¨˜æ³•
        LLMåŸºç¤
            Softmax
            Attention
            Cross-Entropy
        å®Ÿè·µæŠ€è¡“
            Pythonç’°å¢ƒ
            LaTeXè¨˜æ³•
            è«–æ–‡èª­è§£
        ç”Ÿæˆãƒ¢ãƒ‡ãƒ«æ¦‚è¦³
            VAE
            GAN
            æ‹¡æ•£
            Flow
```

### 6.3 æœ¬è¬›ç¾©ã®ã¾ã¨ã‚

ç¬¬1å›ã§ã¯ä»¥ä¸‹ã‚’å­¦ã‚“ã :

**1. æ•°å¼ã¯ã€Œèª­ã‚€ã€ã‚‚ã®ã§ã‚ã‚Šã€Œæã‚Œã‚‹ã€ã‚‚ã®ã§ã¯ãªã„**
- ã‚®ãƒªã‚·ãƒ£æ–‡å­—ã¯ã€Œæ•°å¼ã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã€â€” è¦šãˆã‚Œã°èª­ã‚ã‚‹
- æ·»å­—ãƒ»æ¼”ç®—å­ãƒ»é›†åˆè¨˜æ³•ã¯ã€Œæ•°å¼ã®æ–‡æ³•ã€â€” ãƒ«ãƒ¼ãƒ«ã‚’ç†è§£ã™ã‚Œã°æ§‹æ–‡è§£æã§ãã‚‹
- è«–æ–‡ã®æ•°å¼ã¯è‘—è€…ã®æ€è€ƒã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ â€” ä¸€æ–‡å­—ãšã¤åˆ†è§£ã™ã‚Œã°å¿…ãšç†è§£ã§ãã‚‹

**2. Transformer ã® Attention å¼ã‚’å®Œå…¨ã«èª­è§£ã—ãŸ**
- Vaswani et al. (2017)[^1] ã® Scaled Dot-Product Attention
- å„è¨˜å·ã®æ„å‘³ã€æ¬¡å…ƒã®è¿½è·¡ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç†ç”±ã‚’ç†è§£ã—ãŸ
- Pythonï¼ˆNumPyï¼‰ã§å®Œå…¨ã«å†å®Ÿè£…ã—ãŸ

**3. è«–æ–‡ã®èª­ã¿æ–¹ã‚’æ§‹é€ åŒ–ã—ãŸ**
- 3ãƒ‘ã‚¹ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: é³¥ç° â†’ ç²¾èª­ â†’ å†ç¾
- arXiv ã®ä½¿ã„æ–¹ã€è«–æ–‡æ¤œç´¢ã®æ–¹æ³•
- æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ç¿»è¨³ã®7ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³

**4. æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å…¨ä½“åƒã‚’ä¿¯ç°ã—ãŸ**
- VAE[^4], GAN[^8], æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«[^5], Flow Matching[^6], DiT[^7] ã®ä½ç½®ã¥ã‘
- å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã®æ§‹æˆã‚’ç†è§£ã—ãŸ

### 6.4 ã‚ˆãã‚ã‚‹è³ªå•

:::details Q: æ•°å­¦ãŒè‹¦æ‰‹ã§ã‚‚ã¤ã„ã¦ã„ã‘ã¾ã™ã‹ï¼Ÿ
**A**: ã¯ã„ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ã€Œæ•°å­¦ãŒå¾—æ„ãªäººå‘ã‘ã€ã§ã¯ãªãã€Œæ•°å­¦ã‚’ã“ã‚Œã‹ã‚‰èº«ã«ã¤ã‘ãŸã„äººå‘ã‘ã€ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ã€‚ç¬¬1å›ã§è¨˜å·ä½“ç³»ã‚’ç¶²ç¾…ã—ãŸã®ã¯ãã®ãŸã‚ã€‚ä»¥é™ã®è¬›ç¾©ã§æ–°ã—ã„è¨˜å·ãŒå‡ºã¦ããŸã‚‰ã€Zone 3 ã«æˆ»ã‚Œã°è§£æ±ºã™ã‚‹ã€‚ãŸã ã—ã€ã€Œèª­ã¿é£›ã°ã™ã€ã®ã§ã¯ãªãã€Œã‚ã‹ã‚‰ãªã‹ã£ãŸã‚‰æˆ»ã‚‹ã€ã¨ã„ã†å§¿å‹¢ã¯å¿…è¦ã€‚
:::

:::details Q: Python ä»¥å¤–ã®è¨€èªã¯ä½¿ã„ã¾ã™ã‹ï¼Ÿ
**A**: Course Iï¼ˆç¬¬1-8å›ï¼‰ã¯ Python 100%ã€‚Course IIï¼ˆç¬¬9-16å›ï¼‰ã§ Julia ãŒç™»å ´ã—ã€Course III ä»¥é™ã§ Rust + Julia ã®å¤šè¨€èªæ§‹æˆã«ãªã‚‹ã€‚å„è¨€èªã®å°å…¥æ™‚ã«ä¸å¯§ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã®ã§å¿ƒé…ä¸è¦ã€‚
:::

:::details Q: è¬›ç¾©ã®é †ç•ªé€šã‚Šã«é€²ã‚ã‚‹ã¹ãã§ã™ã‹ï¼Ÿ
**A**: åŸºæœ¬çš„ã«ã¯é †ç•ªé€šã‚Šã‚’æ¨å¥¨ã€‚ç‰¹ã«ç¬¬1-4å›ã¯åŸºç¤ãªã®ã§é£›ã°ã•ãªã„ã“ã¨ã€‚ãŸã ã—ã€ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ï¼ˆä¾‹: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã ã‘çŸ¥ã‚ŠãŸã„ï¼‰ãŒã‚ã‚‹å ´åˆã¯ã€ç¬¬1-2å› â†’ ç¬¬11å›ã¨é£›ã‚“ã§ã‚‚ç†è§£ã§ãã‚‹ã‚ˆã†ã«è¨­è¨ˆã—ã¦ã‚ã‚‹ã€‚
:::

:::details Q: æ•°å¼ã‚’ã™ã¹ã¦æš—è¨˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ
**A**: **æš—è¨˜ã¯ä¸è¦ã€‚ç†è§£ãŒé‡è¦ã€‚** Softmax ã®å¼ã‚’æš—è¨˜ã—ã¦ã„ãªãã¦ã‚‚ã€ã€Œå®Ÿæ•°ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç¢ºç‡åˆ†å¸ƒã«å¤‰æ›ã™ã‚‹é–¢æ•°ã§ã€å„è¦ç´ ã®æŒ‡æ•°ã‚’å…¨ä½“ã®æŒ‡æ•°ã®å’Œã§å‰²ã‚‹ã€ã¨èª¬æ˜ã§ãã‚Œã°ååˆ†ã€‚å¼ã¯è«–æ–‡ã‚’è¦‹ã‚Œã°æ›¸ã„ã¦ã‚ã‚‹ã€‚æ„å‘³ã‚’ç†è§£ã—ã¦ã„ã‚Œã°ã€å¼ã‚’è¦‹ãŸç¬é–“ã«èª­ã‚ã‚‹ã€‚
:::

:::details Q: å‚è€ƒæ›¸ã¯è²·ã†ã¹ãã§ã™ã‹ï¼Ÿ
**A**: æœ€ä½é™ã¯ä¸è¦ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã§å¿…è¦ãªæ•°å­¦ã¯ã™ã¹ã¦è¬›ç¾©å†…ã§è§£èª¬ã™ã‚‹ã€‚ãŸã ã—ã€æ·±æ˜ã‚Šã—ãŸã„å ´åˆã¯ Zone 6 ã®æ¨è–¦æ›¸ç±ã‚’å‚ç…§ã€‚ç‰¹ã« "Mathematics for Machine Learning" (Deisenroth et al.) ã¯ç„¡æ–™PDFå…¬é–‹ã•ã‚Œã¦ãŠã‚Šã€æ‰‹å…ƒã«ç½®ã„ã¦ãŠãä¾¡å€¤ãŒã‚ã‚‹ã€‚
:::

:::details Q: æ©Ÿæ¢°å­¦ç¿’ã®çµŒé¨“ãŒã‚¼ãƒ­ã§ã‚‚å¤§ä¸ˆå¤«ã§ã™ã‹ï¼Ÿ
**A**: å¤§ä¸ˆå¤«ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ã€Œãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãŒã§ãã‚‹ãŒã€æ©Ÿæ¢°å­¦ç¿’ã¯åˆã‚ã¦ã€ã¨ã„ã†èª­è€…ã‚’æƒ³å®šã—ã¦ã„ã‚‹ã€‚Python ã®åŸºç¤ï¼ˆå¤‰æ•°ã€é–¢æ•°ã€ãƒ«ãƒ¼ãƒ—ã€ãƒªã‚¹ãƒˆï¼‰ãŒã§ãã‚Œã°ååˆ†ã€‚NumPy ã‚‚æœ¬è¬›ç¾©ã®ä¸­ã§å¿…è¦ãªæ“ä½œã‚’éƒ½åº¦è§£èª¬ã™ã‚‹ã€‚ãŸã ã—ã€å®Œå…¨ãªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã®å ´åˆã¯ã€å…ˆã« Python ã®å…¥é–€æ›¸ã‚’1å†Šèª­ã‚“ã§ãŠãã“ã¨ã‚’æ¨å¥¨ã™ã‚‹ã€‚
:::

:::details Q: GPU ã¯å¿…è¦ã§ã™ã‹ï¼Ÿ
**A**: Course Iï¼ˆç¬¬1-8å›ï¼‰ã¯ GPU ä¸è¦ã€‚CPU ã ã‘ã§å…¨ã‚³ãƒ¼ãƒ‰ãŒå‹•ãã€‚GPU ãŒå¿…è¦ã«ãªã‚‹ã®ã¯ç¬¬9å›ä»¥é™ã®è¨“ç·´å®Ÿé¨“ã‹ã‚‰ã€‚ãã®æ™‚ç‚¹ã§ Google Colabï¼ˆç„¡æ–™æ ï¼‰ã§ååˆ†ã€‚æœ¬æ ¼çš„ãªè¨“ç·´å®Ÿé¨“ã‚’ã—ãŸã„å ´åˆã¯ã€ç¬¬9å›ã§ GPU ç’°å¢ƒã®æ§‹ç¯‰æ–¹æ³•ã‚’è§£èª¬ã™ã‚‹ã€‚
:::

:::details Q: è«–æ–‡ã‚’è‹±èªã§èª­ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ
**A**: ã¯ã„ã€‚æ©Ÿæ¢°å­¦ç¿’ã®ä¸€æ¬¡æƒ…å ±ã¯ã»ã¼ã™ã¹ã¦è‹±èªã€‚ãŸã ã—ã€æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯é‡è¦è«–æ–‡ã®æ ¸å¿ƒéƒ¨åˆ†ã‚’æ—¥æœ¬èªã§è§£èª¬ã™ã‚‹ã®ã§ã€ã€Œè«–æ–‡ã‚’å®Œå…¨ã«èª­ã‚€ã€å¿…è¦ã¯ãªã„ã€‚ã¾ãšã¯æœ¬è¬›ç¾©ã® Pass 1 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ Abstract ã¨å›³è¡¨ã ã‘èª­ã‚€ç·´ç¿’ã‹ã‚‰å§‹ã‚ã‚ˆã†ã€‚è‹±èªåŠ›ã¯ç¹°ã‚Šè¿”ã—èª­ã‚€ã†ã¡ã«è‡ªç„¶ã«ä¸ŠãŒã‚‹ã€‚DeepL/GPT ã‚’è£œåŠ©çš„ã«ä½¿ã†ã®ã¯å•é¡Œãªã„ï¼ˆãŸã ã—æ•°å¼ã¯è‡ªåˆ†ã§èª­ã‚€ã“ã¨ï¼‰ã€‚
:::

:::details Q: ã“ã®è¬›ç¾©ã ã‘ã§ç ”ç©¶ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã‹ï¼Ÿ
**A**: å…¨50å›ã‚’ä¿®äº†ã™ã‚Œã°ã€æœ€æ–°ã®æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è«–æ–‡ã‚’èª­ã¿ã€ç†è§£ã—ã€å®Ÿè£…ã—ã€æ”¹è‰¯ã™ã‚‹ãŸã‚ã®åŸºç¤åŠ›ãŒèº«ã«ã¤ãã€‚ãŸã ã—ã€Œç ”ç©¶ã€ã«ã¯å•é¡Œè¨­å®šèƒ½åŠ›ã‚„å®Ÿé¨“è¨­è¨ˆåŠ›ãªã©ã€æœ¬ã‚·ãƒªãƒ¼ã‚ºã ã‘ã§ã¯ã‚«ãƒãƒ¼ã—ãã‚Œãªã„èƒ½åŠ›ã‚‚å¿…è¦ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ã‚ãã¾ã§ã€Œè«–æ–‡ãŒèª­ã‚ã€å®Ÿè£…ã§ãã‚‹ã€ã¨ã“ã‚ã¾ã§ã‚’ä¿è¨¼ã™ã‚‹ã€‚
:::

### 6.5 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ææ¡ˆ

ç¬¬1å›ã®å†…å®¹ã‚’åŠ¹ç‡çš„ã«æ¶ˆåŒ–ã™ã‚‹ãŸã‚ã®æ¨å¥¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«:

| æ—¥ | å†…å®¹ | æ‰€è¦æ™‚é–“ |
|:---|:---|:---|
| Day 1 | Zone 0-2: æ¦‚è¦ã¨å‹•æ©Ÿã¥ã‘ã€‚Softmax, Attention, Cross-Entropy ã‚’æ‰‹ã§è¨ˆç®— | 1.5h |
| Day 2 | Zone 3 (3.1-3.6): ã‚®ãƒªã‚·ãƒ£æ–‡å­—ã€æ·»å­—ã€æ¼”ç®—å­ã€é›†åˆã€è«–ç†ã€é–¢æ•° | 2h |
| Day 3 | Zone 3 (3.7-3.9): å¾®åˆ†ã€ç¢ºç‡ã€Boss Battle (Attention å®Œå…¨èª­è§£) | 2h |
| Day 4 | Zone 4: ç’°å¢ƒæ§‹ç¯‰ + LaTeX ç·´ç¿’ + è«–æ–‡èª­è§£ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ | 1.5h |
| Day 5 | Zone 5: è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ + å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ | 2h |
| Day 6 | Zone 6-7: å…¨ä½“åƒã®ç¢ºèª + å¾©ç¿’ | 1h |
| Day 7 | **å¾©ç¿’æ—¥**: Zone 3 ã®è‹¦æ‰‹ç®‡æ‰€ã‚’å†èª­ã€Pass 1 ã‚’1æœ¬å®Ÿè·µ | 1h |

**åˆè¨ˆ: ç´„11æ™‚é–“ / 1é€±é–“**

:::message
**ãƒšãƒ¼ã‚¹é…åˆ†ã®ã‚³ãƒ„**: 1æ—¥ã« Zone ã‚’2ã¤ä»¥ä¸Šé€²ã‚ã‚ˆã†ã¨ã—ãªã„ã“ã¨ã€‚ç‰¹ã« Zone 3 ã¯æ¶ˆåŒ–ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã€‚ã€Œã‚ã‹ã£ãŸã¤ã‚‚ã‚Šã€ã§å…ˆã«é€²ã‚€ã‚ˆã‚Šã€1ã¤ã® Zone ã‚’ç¢ºå®Ÿã«ç†è§£ã—ã¦ã‹ã‚‰æ¬¡ã«é€²ã‚€æ–¹ãŒã€çµæœçš„ã«é€Ÿã„ã€‚
:::

#### å¾©ç¿’ã®å…·ä½“çš„ãªæ–¹æ³•

1. **ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚«ãƒ¼ãƒ‰**: Zone 3 ã®ã‚®ãƒªã‚·ãƒ£æ–‡å­—ã¨è¨˜å·ã‚’ Anki ã«ç™»éŒ²ã€‚1æ—¥5åˆ†ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼
2. **å†™çµŒ + æ”¹é€ **: Zone 5 ã®ã‚³ãƒ¼ãƒ‰ã‚’å†™çµŒã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ãˆã¦å®Ÿé¨“ã™ã‚‹
3. **è«–æ–‡ Pass 1**: é€±1æœ¬ã® arXiv è«–æ–‡ã§ Pass 1 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’åŸ‹ã‚ã‚‹ç·´ç¿’
4. **æ•°å¼æ—¥è¨˜**: æ¯æ—¥1ã¤ã€æ–°ã—ã„æ•°å¼ã‚’è¦‹ã¤ã‘ã¦ã€Œæ—¥æœ¬èªã§èª¬æ˜ã™ã‚‹ã€ç·´ç¿’

```python
"""å­¦ç¿’é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼"""

progress = {
    "Zone 0: QuickStart": {"status": "done", "confidence": 5},
    "Zone 1: Intuition": {"status": "done", "confidence": 4},
    "Zone 2: Motivation": {"status": "done", "confidence": 5},
    "Zone 3.1: Greek Letters": {"status": "done", "confidence": 3},
    "Zone 3.2: Subscripts": {"status": "done", "confidence": 4},
    "Zone 3.3: Operators": {"status": "done", "confidence": 3},
    "Zone 3.4: Sets": {"status": "done", "confidence": 4},
    "Zone 3.5: Logic": {"status": "done", "confidence": 3},
    "Zone 3.6: Functions": {"status": "done", "confidence": 4},
    "Zone 3.7: Calculus": {"status": "done", "confidence": 3},
    "Zone 3.8: Probability": {"status": "done", "confidence": 3},
    "Zone 3.9: Boss Battle": {"status": "done", "confidence": 4},
    "Zone 4: Practical": {"status": "done", "confidence": 4},
    "Zone 5: Diagnosis": {"status": "done", "confidence": 3},
    "Zone 6: References": {"status": "done", "confidence": 5},
    "Zone 7: Summary": {"status": "done", "confidence": 5},
}

print("=== ç¬¬1å› å­¦ç¿’é€²æ— ===\n")
total_conf = 0
n_zones = len(progress)
for zone, info in progress.items():
    bar = "â˜…" * info["confidence"] + "â˜†" * (5 - info["confidence"])
    print(f"  [{info['status']:4s}] {bar} {zone}")
    total_conf += info["confidence"]

avg_conf = total_conf / n_zones
print(f"\nå¹³å‡è‡ªä¿¡åº¦: {avg_conf:.1f}/5.0")
if avg_conf >= 4.0:
    print("â†’ ç¬¬2å›ã«é€²ã‚“ã§OKï¼")
elif avg_conf >= 3.0:
    print("â†’ è‡ªä¿¡åº¦3ä»¥ä¸‹ã® Zone ã‚’å¾©ç¿’ã—ã¦ã‹ã‚‰ç¬¬2å›ã¸")
else:
    print("â†’ Zone 3 ã‚’é‡ç‚¹çš„ã«å¾©ç¿’ã—ã‚ˆã†")
```

### 6.6 æ¬¡å›äºˆå‘Š

**ç¬¬2å›: ç·šå½¢ä»£æ•° â€” ãƒ™ã‚¯ãƒˆãƒ«ã¨è¡Œåˆ—ã®ä¸–ç•Œ**

> ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã€Œè¨€èªã€ã¯ç·šå½¢ä»£æ•°ã ã€‚è¡Œåˆ—ã®æ›ã‘ç®—ãŒä¸–ç•Œã‚’å¤‰ãˆã‚‹ã€‚

æ¬¡å›ã®ã‚­ãƒ¼ãƒˆãƒ”ãƒƒã‚¯:
- ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã€åŸºåº•ã€æ¬¡å…ƒ
- è¡Œåˆ—æ¼”ç®—ã¨ãã®å¹¾ä½•å­¦çš„æ„å‘³
- å›ºæœ‰å€¤åˆ†è§£ã¨ç‰¹ç•°å€¤åˆ†è§£ï¼ˆSVDï¼‰
- PCA: æ¬¡å…ƒå‰Šæ¸›ã®åŸç†
- **Boss Battle**: Transformer ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ç·šå½¢ä»£æ•°çš„è§£é‡ˆ

:::message
**é€²æ—: 100% å®Œäº†** ãŠã‚ã§ã¨ã†ã€‚ç¬¬1å›ã€Œæ¦‚è«–: æ•°å¼ã¨è«–æ–‡ã®èª­ã¿æ–¹ã€ã‚’ä¿®äº†ã—ãŸã€‚ã“ã“ã§èº«ã«ã¤ã‘ãŸã€Œæ•°å¼ã‚’èª­ã‚€åŠ›ã€ã¯ã€æ®‹ã‚Š39å›ã™ã¹ã¦ã®è¬›ç¾©ã§ä½¿ã„ç¶šã‘ã‚‹åŸºç¤ä½“åŠ›ã ã€‚æ¬¡å›ã¯ç·šå½¢ä»£æ•°ã®ä¸–ç•Œã«è¸ã¿è¾¼ã‚€ã€‚
:::

### 6.7 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã‚‚ã—ã€Œæ•°å¼ã€ã¨ã„ã†è¡¨ç¾å½¢å¼ãŒå­˜åœ¨ã—ãªã‹ã£ãŸã‚‰ã€äººé¡ã¯æ·±å±¤å­¦ç¿’ã‚’ç™ºæ˜ã§ããŸã ã‚ã†ã‹ï¼Ÿ**

æ•°å¼ã¯ã€Œå³å¯†ã•ã€ã¨ã€Œæ±ç”¨æ€§ã€ã‚’ä¸¡ç«‹ã™ã‚‹å”¯ä¸€ã®è¨€èªã ã€‚ã€Œå…¥åŠ› $\mathbf{x}$ ã‚’é‡ã¿ $W$ ã§ç·šå½¢å¤‰æ›ã—ã€ãƒã‚¤ã‚¢ã‚¹ $\mathbf{b}$ ã‚’åŠ ãˆã€éç·šå½¢é–¢æ•° $\sigma$ ã‚’é€šã™ã€â€” ã“ã®æ“ä½œã‚’ $\sigma(W\mathbf{x} + \mathbf{b})$ ã¨ã„ã†7æ–‡å­—ã§è¡¨ç¾ã§ãã‚‹ã€‚è‡ªç„¶è¨€èªã§ã¯åŒã˜æƒ…å ±é‡ã«50æ–‡å­—ä»¥ä¸Šã‹ã‹ã‚‹ã€‚

ã—ã‹ã—ã€åˆ¥ã®å¯èƒ½æ€§ã‚‚ã‚ã‚‹:
- **ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ç›´æ¥å®šç¾©ã™ã‚‹ä¸–ç•Œ**: `h = relu(W @ x + b)` â€” å®Ÿéš›ã€å¤šãã®å®Ÿå‹™è€…ã¯ã‚³ãƒ¼ãƒ‰ã§è€ƒãˆã¦ã„ã‚‹
- **å›³çš„è¨€èªï¼ˆãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ï¼‰ã§å®šç¾©ã™ã‚‹ä¸–ç•Œ**: ã‚«ãƒ†ã‚´ãƒªç†è«–ã®string diagramã®ã‚ˆã†ã«
- **è‡ªç„¶è¨€èªã§å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ç”Ÿæˆã•ã›ã‚‹ä¸–ç•Œ**: 2024-2026ã®AIã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒç¤ºã™æ–¹å‘æ€§

æ•°å¼ã¯ã€Œç™ºè¦‹ã®é“å…·ã€ã ã£ãŸã®ã‹ã€ãã‚Œã¨ã‚‚ã€Œç™ºè¦‹ã‚’åˆ¶ç´„ã™ã‚‹æª»ã€ã ã£ãŸã®ã‹ã€‚Einstein ã®ã€Œæ•°å­¦ã¯è‡ªç„¶ã®è¨€èªã€ã¨ã„ã†ä¸»å¼µã¯ã€æ•°å­¦ãŒè‡ªç„¶ã‚’è¨˜è¿°ã™ã‚‹ã®ã«é©ã—ã¦ã„ã‚‹ã®ã‹ã€ãã‚Œã¨ã‚‚æ•°å­¦ã§è¨˜è¿°å¯èƒ½ãªè‡ªç„¶ã—ã‹æˆ‘ã€…ãŒèªè­˜ã§ããªã„ã®ã‹ â€” ã“ã®å•ã„ã«ç¬¬2å›ä»¥é™ã€ç¹°ã‚Šè¿”ã—ç«‹ã¡æˆ»ã‚‹ã“ã¨ã«ãªã‚‹ã€‚

**è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã“ã¨**:

1. **Transformer[^1] ã¯æ•°å¼ã‹ã‚‰ç”Ÿã¾ã‚ŒãŸã®ã‹ã€å®Ÿé¨“ã‹ã‚‰ç”Ÿã¾ã‚ŒãŸã®ã‹ï¼Ÿ** åŸè«–æ–‡ã‚’èª­ã‚€ã¨ã€Scaled Dot-Product Attention ã® $\sqrt{d_k}$ ã¨ã„ã†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å› å­ã¯ã€æ•°å­¦çš„è§£æï¼ˆå†…ç©ã®åˆ†æ•£ãŒ $d_k$ ã«æ¯”ä¾‹ã™ã‚‹ã¨ã„ã†è¦³å¯Ÿï¼‰ã‹ã‚‰å°ã‹ã‚Œã¦ã„ã‚‹ã€‚ä¸€æ–¹ã§ã€Multi-Head Attention ã®ãƒ˜ãƒƒãƒ‰æ•° $h=8$ ã¨ã„ã†é¸æŠã¯ã€å®Ÿé¨“çš„ã«æœ€è‰¯ã ã£ãŸæ•°å€¤ã§ã‚ã‚Šã€æ•°å­¦çš„ãªå¿…ç„¶æ€§ã¯ãªã„ã€‚

2. **æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«[^5] ã®ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**ã¯ã€ç‰©ç†å­¦ã®æ‹¡æ•£æ–¹ç¨‹å¼ã«ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¾—ã¦ã„ã‚‹ãŒã€å®Ÿéš›ã«æ©Ÿèƒ½ã™ã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆlinear, cosineï¼‰ã¯ç‰©ç†çš„ã«è‡ªç„¶ãªã‚‚ã®ã§ã¯ãªãã€å®Ÿé¨“çš„ã«ç™ºè¦‹ã•ã‚ŒãŸã‚‚ã®ã ã€‚ã€Œç‰©ç†ã®æ•°å¼ã‹ã‚‰AIã‚’è¨­è¨ˆã™ã‚‹ã€ã¨ã„ã†æ–¹å‘ã¯æœ¬å½“ã«æ­£ã—ã„ã®ã‹ã€ãã‚Œã¨ã‚‚ã€ŒAIã®æŒ¯ã‚‹èˆã„ã‚’äº‹å¾Œçš„ã«ç‰©ç†ã§è§£é‡ˆã™ã‚‹ã€æ–¹ãŒç”Ÿç”£çš„ãªã®ã‹ã€‚

3. **Flow Matching[^6]** ã¯æœ€é©è¼¸é€ç†è«–ã¨ã„ã†ç´”ç²‹æ•°å­¦ã‹ã‚‰ã®ç›´æ¥çš„ãªå¿œç”¨ã ã€‚ä¸€æ–¹ã€GAN[^8] ã¯ã‚²ãƒ¼ãƒ ç†è«–ï¼ˆã“ã‚Œã‚‚æ•°å­¦ï¼‰ã‹ã‚‰ç€æƒ³ã‚’å¾—ã¦ã„ã‚‹ã€‚ç•°ãªã‚‹æ•°å­¦çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒç•°ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”Ÿã‚€ â€” æ•°å­¦ã®é¸ã³æ–¹ãŒç™ºæ˜ã‚’æ±ºå®šã™ã‚‹ã®ã ã¨ã—ãŸã‚‰ã€ã¾ã è©¦ã•ã‚Œã¦ã„ãªã„æ•°å­¦çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ä¸­ã«ã€æ¬¡ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ãŒçœ ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

ã‚ãªãŸã®è€ƒãˆã‚’ã€æ¬¡å›ã®è¬›ç¾©ã®å‰ã«è¨€èªåŒ–ã—ã¦ãŠã„ã¦ã»ã—ã„ã€‚æ­£è§£ã¯å­˜åœ¨ã—ãªã„ã€‚è€ƒãˆã‚‹ã“ã¨è‡ªä½“ã«ä¾¡å€¤ãŒã‚ã‚‹ã€‚

:::details æ­´å²çš„ãªè¦–ç‚¹: è¡¨ç¾å½¢å¼ã¨ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã®é–¢ä¿‚
- **ãƒ©ã‚¤ãƒ—ãƒ‹ãƒƒãƒ„ã®å¾®ç©åˆ†è¨˜æ³•** ($\frac{dy}{dx}$) ãŒãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ã®è¨˜æ³• ($\dot{y}$) ã‚ˆã‚Šåºƒãæ™®åŠã—ãŸã®ã¯ã€é€£é–å¾‹ãŒæ©Ÿæ¢°çš„ã«é©ç”¨ã§ããŸã‹ã‚‰ã€‚è¨˜æ³•ãŒæ€è€ƒã‚’åŠ é€Ÿã—ãŸä¾‹ã€‚
- **ã‚¢ã‚¤ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³ã®ç¸®ç´„è¨˜æ³•** ($a_i b^i = \sum_i a_i b_i$) ãŒãƒ†ãƒ³ã‚½ãƒ«è¨ˆç®—ã‚’åŠ‡çš„ã«ç°¡ç•¥åŒ–ã—ã€ä¸€èˆ¬ç›¸å¯¾æ€§ç†è«–ã®ç™ºå±•ã‚’åŠ é€Ÿã—ãŸã€‚
- **Dirac ã®ãƒ–ãƒ©ã‚±ãƒƒãƒˆè¨˜æ³•** ($\langle \psi | \phi \rangle$) ãŒé‡å­åŠ›å­¦ã®è¨ˆç®—ã‚’ç›´æ„Ÿçš„ã«ã—ãŸã€‚
- ãã—ã¦ä»Šã€**PyTorch/JAX ã®è‡ªå‹•å¾®åˆ†** ãŒã€Œå‹¾é…ã‚’æ‰‹è¨ˆç®—ã§æ±‚ã‚ã‚‹ã€å¿…è¦ã‚’ãªãã—ã€æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å®Ÿé¨“ã‚³ã‚¹ãƒˆã‚’åŠ‡çš„ã«ä¸‹ã’ãŸã€‚

è¡¨ç¾å½¢å¼ã®é€²åŒ–ã¯ã€æ–°ã—ã„ç™ºè¦‹ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚æ•°å¼â†’ã‚³ãƒ¼ãƒ‰â†’è‡ªç„¶è¨€èªï¼ˆLLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã¨ã„ã†è¡¨ç¾å½¢å¼ã®é€²åŒ–ã¯ã€æ¬¡ã«ã©ã‚“ãªç™ºè¦‹ã‚’å¯èƒ½ã«ã™ã‚‹ã ã‚ã†ã‹ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. *Advances in Neural Information Processing Systems*, 30.
@[card](https://arxiv.org/abs/1706.03762)

[^2]: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. *Nature*, 323(6088), 533-536.
@[card](https://doi.org/10.1038/323533a0)

[^3]: Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network.
@[card](https://arxiv.org/abs/1503.02531)

[^4]: Kingma, D. P. & Welling, M. (2013). Auto-Encoding Variational Bayes.
@[card](https://arxiv.org/abs/1312.6114)

[^5]: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models.
@[card](https://arxiv.org/abs/2006.11239)

[^6]: Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., & Le, M. (2022). Flow Matching for Generative Modeling.
@[card](https://arxiv.org/abs/2210.02747)

[^7]: Peebles, W. & Xie, S. (2022). Scalable Diffusion Models with Transformers.
@[card](https://arxiv.org/abs/2212.09748)

[^8]: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative Adversarial Networks.
@[card](https://arxiv.org/abs/1406.2661)

[^9]: Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [deeplearningbook.org](https://www.deeplearningbook.org/)

### æ•™ç§‘æ›¸

- Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). *Mathematics for Machine Learning*. Cambridge University Press. [mml-book.github.io](https://mml-book.github.io/)
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [deeplearningbook.org](https://www.deeplearningbook.org/)
- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
- Axler, S. (2024). *Linear Algebra Done Right* (4th ed.). Springer.
- Prince, S. J. D. (2023). *Understanding Deep Learning*. MIT Press. [udlbook.github.io](https://udlbook.github.io/udlbook/)

---

## è¨˜æ³•è¦ç´„

æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ä½¿ç”¨ã™ã‚‹è¨˜æ³•è¦ç´„ï¼ˆå…¨50å›å…±é€šï¼‰:

| è¨˜æ³• | æ„å‘³ |
|:---|:---|
| $\mathbf{x}$ (å¤ªå­—å°æ–‡å­—) | ãƒ™ã‚¯ãƒˆãƒ« |
| $\mathbf{A}$, $W$ (å¤ªå­—/å¤§æ–‡å­—) | è¡Œåˆ— |
| $x$ (ã‚¤ã‚¿ãƒªãƒƒã‚¯å°æ–‡å­—) | ã‚¹ã‚«ãƒ©ãƒ¼ |
| $\mathcal{L}$ (ã‚«ãƒªã‚°ãƒ©ãƒ•ã‚£) | æå¤±é–¢æ•°ã€é›†åˆæ— |
| $\mathbb{R}, \mathbb{E}$ (é»’æ¿å¤ªå­—) | æ•°ã®é›†åˆã€æœŸå¾…å€¤æ¼”ç®—å­ |
| $\theta, \phi$ (ã‚®ãƒªã‚·ãƒ£å°æ–‡å­—) | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| $p(\cdot)$, $q(\cdot)$ | ç¢ºç‡åˆ†å¸ƒ/å¯†åº¦é–¢æ•° |
| $x_i$ | ãƒ™ã‚¯ãƒˆãƒ«ã® $i$ ç•ªç›®ã®è¦ç´  |
| $x^{(n)}$ | $n$ ç•ªç›®ã®ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ« |
| $W^{(l)}$ | $l$ ç•ªç›®ã®å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| $\nabla_\theta$ | $\theta$ ã«ã¤ã„ã¦ã®å‹¾é… |
| $\sim$ | ã€Œã€œã®åˆ†å¸ƒã«å¾“ã†ã€ |
| $:=$ | å®šç¾© |
| $\propto$ | æ¯”ä¾‹ |
| $\approx$ | è¿‘ä¼¼ |
| $\odot$ | è¦ç´ ã”ã¨ã®ç©ï¼ˆã‚¢ãƒ€ãƒãƒ¼ãƒ«ç©ï¼‰ |
| $\circ$ | é–¢æ•°åˆæˆ |
| $\|\cdot\|_2$ | L2ãƒãƒ«ãƒ ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ãƒãƒ«ãƒ ï¼‰ |
| $\langle \cdot, \cdot \rangle$ | å†…ç© |
| $\mathcal{N}(\mu, \sigma^2)$ | æ­£è¦åˆ†å¸ƒ |
| $D_{\text{KL}}(\cdot \| \cdot)$ | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ |
| $\mathbb{1}[\cdot]$ | æŒ‡ç¤ºé–¢æ•°ï¼ˆæ¡ä»¶ãŒçœŸã®ã¨ã1ã€å½ã®ã¨ã0ï¼‰ |
| $\mathcal{O}(\cdot)$ | è¨ˆç®—é‡ã®ã‚ªãƒ¼ãƒ€ãƒ¼ |
| $\Delta^{C-1}$ | $C$æ¬¡å…ƒç¢ºç‡å˜ä½“ |
| $\text{s.t.}$ | "subject to"ï¼ˆåˆ¶ç´„æ¡ä»¶ï¼‰ |

:::message
**è¨˜æ³•ã«ã¤ã„ã¦**: æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯ Goodfellow et al. "Deep Learning" (2016) ã®è¨˜æ³•è¦ç´„ã«æº–æ‹ ã™ã‚‹ã€‚è«–æ–‡ã«ã‚ˆã£ã¦ã¯ç•°ãªã‚‹è¨˜æ³•ã‚’ä½¿ã†ã“ã¨ãŒã‚ã‚‹ãŒã€ãã®å ´åˆã¯éƒ½åº¦æ³¨è¨˜ã™ã‚‹ã€‚è¨˜æ³•ã®ä¸çµ±ä¸€ã¯æ··ä¹±ã®åŸå› ã«ãªã‚‹ãŸã‚ã€è‡ªåˆ†ã®ãƒãƒ¼ãƒˆã§ã‚‚ä¸€è²«ã—ãŸè¨˜æ³•ã‚’ä½¿ã†ç¿’æ…£ã‚’ã¤ã‘ã‚ˆã†ã€‚
:::

---

**ç¬¬1å› å®Œ â€” æ¬¡å›ã€Œç¬¬2å›: ç·šå½¢ä»£æ•° â€” ãƒ™ã‚¯ãƒˆãƒ«ã¨è¡Œåˆ—ã®ä¸–ç•Œã€ã«ç¶šã**

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
