---
title: "ç¬¬15å›: Attention é¡ä¼¼æ‰‹æ³• & Sparse Attention: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "âš¡"
type: "tech"
topics: ["machinelearning", "deeplearning", "attention", "julia", "rust"]
published: true
---

# ç¬¬15å›: Attention é¡ä¼¼æ‰‹æ³• & Sparse Attention â€” O(NÂ²)ã®ä»£å„Ÿã¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

> **Attentionã¯ä¸‡èƒ½ã§ã¯ãªã„ã€‚O(NÂ²)ã®ä»£å„Ÿã‚’æ”¯æ‰•ã„ç¶šã‘ã‚‹ã®ã‹ã€ãã‚Œã¨ã‚‚è¿‘ä¼¼ã‚’å—ã‘å…¥ã‚Œã‚‹ã®ã‹ã€‚**

ç¬¬14å›ã§å­¦ã‚“ã Attentionã¯é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ãŸã€‚RNN/CNNã®é™ç•Œã‚’çªç ´ã—ã€å…¨ç³»åˆ—å‚ç…§ã¨ä¸¦åˆ—è¨ˆç®—ã‚’å®Ÿç¾ã—ãŸã€‚ã—ã‹ã—ä»£å„ŸãŒã‚ã‚‹ã€‚**ç³»åˆ—é•·Nã«å¯¾ã—ã¦O(NÂ²)ã®è¨ˆç®—é‡ã¨ãƒ¡ãƒ¢ãƒª**ã ã€‚

GPT-4ã®128Kãƒˆãƒ¼ã‚¯ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‚Claude 3ã®200Kãƒˆãƒ¼ã‚¯ãƒ³ã€‚ã“ã‚Œã‚‰ã¯ã€Œé•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€ã®éœ€è¦ãŒçˆ†ç™ºã—ã¦ã„ã‚‹è¨¼æ‹ ã ã€‚ã ãŒStandard Attentionã§128KÃ—128K = 16Gã®æ³¨æ„è¡Œåˆ—ã‚’è¨ˆç®—ãƒ»ä¿å­˜ã™ã‚‹ã®ã¯ç¾å®Ÿçš„ã‹ï¼Ÿ ç­”ãˆã¯

å¦ã ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€ã“ã®O(NÂ²)ã®å£ã‚’çªç ´ã™ã‚‹3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å®Œå…¨å°å‡ºã™ã‚‹:

1. **KV-Cacheæœ€é©åŒ–** (MQA/GQA/PagedAttention) â€” æ¨è«–æ™‚ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
2. **IO-aware Attention** (FlashAttention) â€” ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚’ç†è§£ã—ãŸæœ€é©åŒ–
3. **Sparse Attention** (Longformer/BigBird/NSA) â€” æ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç–ã«ã™ã‚‹
4. **Linear Attention** (Performer/GLA) â€” ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§O(N)å®Ÿç¾
5. **Distributed Attention** (Ring Attention) â€” è¶…é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æ•£å‡¦ç†
6. **Mixture of Experts** (MoE) â€” Sparse Activationã§è¨ˆç®—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ†é›¢

âš¡ Julia ã¨ ğŸ¦€ Rust ã§å…¨ã¦å®Ÿè£…ã™ã‚‹ã€‚ç†è«–ã¨å®Ÿè£…ã®1å¯¾1å¯¾å¿œã‚’å¾¹åº•ã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph TD
    A["Standard Attention<br/>O(NÂ²) è¨ˆç®—ãƒ»ãƒ¡ãƒ¢ãƒª"] --> B{"ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•"}
    B -->|"è¿‘ä¼¼ã‚’å—ã‘å…¥ã‚Œã‚‹"| C["Sparse Attention<br/>å›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³ O(NâˆšN)"]
    B -->|"è¨ˆç®—é †åºã‚’å¤‰ãˆã‚‹"| D["FlashAttention<br/>IOæœ€é©åŒ– åŒã˜O(NÂ²)ã ãŒ2-3xé€Ÿ"]
    B -->|"ã‚«ãƒ¼ãƒãƒ«ã§ç·šå½¢åŒ–"| E["Linear Attention<br/>O(N) ã ãŒè¿‘ä¼¼èª¤å·®"]
    B -->|"åˆ†æ•£ã™ã‚‹"| F["Ring Attention<br/>æ•°ç™¾ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³"]
    B -->|"Sparsity"| G["MoE<br/>è¨ˆç®—åŠ¹ç‡åŒ–"]

    style A fill:#ffcdd2
    style D fill:#c8e6c9
    style E fill:#fff9c4
    style F fill:#b3e5fc
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” O(NÂ²)ã®é‡ã•ã‚’ä½“æ„Ÿ

**ã‚´ãƒ¼ãƒ«**: Standard Attentionã®ãƒ¡ãƒ¢ãƒªãŒNÂ²ã§ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ç¾å®Ÿã‚’30ç§’ã§å®Ÿæ„Ÿã™ã‚‹ã€‚

```julia
using LinearAlgebra

# Standard Attention: softmax(QK^T/âˆšd) V
function standard_attention(Q::Matrix{Float32}, K::Matrix{Float32}, V::Matrix{Float32})
    # Q, K, V: (seq_len, d_model)
    seq_len, d = size(Q)

    # Attention matrix: (seq_len, seq_len)  â€” THIS IS THE PROBLEM
    scores = (Q * K') / sqrt(Float32(d))

    # Softmax per row
    attn = softmax(scores, dims=2)

    # Weighted sum
    out = attn * V
    return out, attn
end

function softmax(x::Matrix{T}, ; dims::Int=2) where T
    exp_x = exp.(x .- maximum(x, dims=dims))
    return exp_x ./ sum(exp_x, dims=dims)
end

# Tiny example: seq_len=16, d=64
seq_len, d = 16, 64
Q = randn(Float32, seq_len, d)
K = randn(Float32, seq_len, d)
V = randn(Float32, seq_len, d)

out, attn = standard_attention(Q, K, V)

println("Attention matrix shape: ", size(attn))  # (16, 16)
println("Memory for attn: $(sizeof(attn)) bytes = $(sizeof(attn) Ã· 1024) KB")

# Now scale up
seq_len_large = 8192
mem_large = seq_len_large^2 * sizeof(Float32)
println("\nFor seq_len=8192 (GPT-3 scale):")
println("  Attention matrix: $(mem_large Ã· 1024^2) MB")
println("  For batch_size=16: $(16 * mem_large Ã· 1024^2) MB")

seq_len_huge = 128_000  # GPT-4 context
mem_huge = seq_len_huge^2 * sizeof(Float32)
println("\nFor seq_len=128K (GPT-4 scale):")
println("  Attention matrix: $(mem_huge Ã· 1024^3) GB (!)")
```

å‡ºåŠ›:
```
Attention matrix shape: (16, 16)
Memory for attn: 1024 bytes = 1 KB

For seq_len=8192 (GPT-3 scale):
  Attention matrix: 256 MB
  For batch_size=16: 4096 MB

For seq_len=128K (GPT-4 scale):
  Attention matrix: 64 GB (!)
```

**128Kãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§64GBã®ãƒ¡ãƒ¢ãƒªãŒæ³¨æ„è¡Œåˆ—"ã ã‘"ã«å¿…è¦ã€‚** ã“ã‚Œã¯å˜ä¸€ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€å˜ä¸€ã®ãƒ˜ãƒƒãƒ‰ã€å˜ä¸€ã®ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒ«ã®æ•°å­—ã ã€‚å®Ÿéš›ã®LLMã¯:
- 32-96ãƒ¬ã‚¤ãƒ¤ãƒ¼
- 32-128ãƒ˜ãƒƒãƒ‰
- ãƒãƒƒãƒã‚µã‚¤ã‚º4-16

ã¤ã¾ã‚Š **ç¾å®Ÿçš„ã«ã¯ä¸å¯èƒ½** ã ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å¼:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

ã“ã“ã§ $QK^\top \in \mathbb{R}^{N \times N}$ ãŒå•é¡Œã ã€‚**ç³»åˆ—é•·NãŒ2å€ã«ãªã‚‹ã¨ã€ãƒ¡ãƒ¢ãƒªã¯4å€ã«ãªã‚‹ã€‚**

:::message
**é€²æ—: 3% å®Œäº†** O(NÂ²)ã®å£ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ã€ã“ã®å£ã‚’çªç ´ã™ã‚‹æ•°å­¦ã¨å®Ÿè£…ã«å…¥ã£ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” åŠ¹ç‡åŒ–æ‰‹æ³•ã‚’è§¦ã‚‹

### 1.1 MQA (Multi-Query Attention) â€” KVã‚’å…¨headã§å…±æœ‰

Standard Multi-Head Attentionã§ã¯ã€å„ãƒ˜ãƒƒãƒ‰ãŒç‹¬ç«‹ã—ãŸK, Vã‚’æŒã¤:

$$
\text{MHA}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
$$

$$
\text{head}_i = \text{Attention}(Q W^Q_i, K W^K_i, V W^V_i)
$$

**å•é¡Œ**: KV-Cacheã®ã‚µã‚¤ã‚ºãŒ `(batch_size, num_heads, seq_len, d_head)` ã«ãªã‚‹ã€‚æ¨è«–æ™‚ã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ¡ãƒ¢ãƒªãŒæ¯æ¸‡ã™ã‚‹ã€‚

**Multi-Query Attention (MQA)** [^1] ã¯ã€**Kã¨Vã‚’å…¨ãƒ˜ãƒƒãƒ‰ã§å…±æœ‰**ã™ã‚‹:

$$
\text{head}_i = \text{Attention}(Q W^Q_i, K W^K, V W^V)
$$

$W^K, W^V$ ãŒãƒ˜ãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ $i$ ã«ä¾å­˜ã—ãªã„ã€‚ã¤ã¾ã‚Š **KV-CacheãŒ1/h ã«å‰Šæ¸›**ã•ã‚Œã‚‹ã€‚

```julia
using LinearAlgebra

function multi_head_attention(Q::Array{Float32,3}, K::Array{Float32,3}, V::Array{Float32,3}, num_heads::Int)
    # Q, K, V: (batch, seq_len, d_model)
    batch_size, seq_len, d_model = size(Q)
    d_head = d_model Ã· num_heads

    # Reshape: (batch, seq_len, num_heads, d_head) -> (batch, num_heads, seq_len, d_head)
    Q_heads = reshape(Q, batch_size, seq_len, num_heads, d_head)
    Q_heads = permutedims(Q_heads, (1, 3, 2, 4))

    K_heads = reshape(K, batch_size, seq_len, num_heads, d_head)
    K_heads = permutedims(K_heads, (1, 3, 2, 4))

    V_heads = reshape(V, batch_size, seq_len, num_heads, d_head)
    V_heads = permutedims(V_heads, (1, 3, 2, 4))

    # Attention per head: scores = Q @ K^T / sqrt(d_head)
    # (batch, num_heads, seq_len, d_head) @ (batch, num_heads, d_head, seq_len) -> (batch, num_heads, seq_len, seq_len)
    scores = batched_matmul(Q_heads, permutedims(K_heads, (1, 2, 4, 3))) / sqrt(Float32(d_head))
    attn_weights = softmax_4d(scores)

    # (batch, num_heads, seq_len, seq_len) @ (batch, num_heads, seq_len, d_head) -> (batch, num_heads, seq_len, d_head)
    out_heads = batched_matmul(attn_weights, V_heads)

    # Reshape back: (batch, seq_len, d_model)
    out_heads = permutedims(out_heads, (1, 3, 2, 4))
    out = reshape(out_heads, batch_size, seq_len, d_model)

    return out
end

function multi_query_attention(Q::Array{Float32,3}, K::Array{Float32,2}, V::Array{Float32,2}, num_heads::Int)
    # Q: (batch, seq_len, d_model)
    # K, V: (batch, seq_len, d_head) â€” SHARED across heads
    batch_size, seq_len, d_model = size(Q)
    d_head = d_model Ã· num_heads

    # Q heads: (batch, num_heads, seq_len, d_head)
    Q_heads = reshape(Q, batch_size, seq_len, num_heads, d_head)
    Q_heads = permutedims(Q_heads, (1, 3, 2, 4))

    # K, V expand: (batch, seq_len, d_head) -> (batch, 1, seq_len, d_head) (broadcast)
    K_expanded = reshape(K, batch_size, 1, seq_len, d_head)
    V_expanded = reshape(V, batch_size, 1, seq_len, d_head)

    # Attention: (batch, num_heads, seq_len, d_head) @ (batch, 1, d_head, seq_len) -> (batch, num_heads, seq_len, seq_len)
    scores = batched_matmul(Q_heads, permutedims(K_expanded, (1, 2, 4, 3))) / sqrt(Float32(d_head))
    attn_weights = softmax_4d(scores)

    # (batch, num_heads, seq_len, seq_len) @ (batch, 1, seq_len, d_head) -> (batch, num_heads, seq_len, d_head)
    out_heads = batched_matmul(attn_weights, V_expanded)

    # Reshape: (batch, seq_len, d_model)
    out_heads = permutedims(out_heads, (1, 3, 2, 4))
    out = reshape(out_heads, batch_size, seq_len, d_model)

    return out
end

function batched_matmul(A::Array{T,4}, B::Array{T,4}) where T
    # A: (batch, heads, M, K), B: (batch, heads, K, N) -> C: (batch, heads, M, N)
    batch, heads, M, K = size(A)
    _, _, _, N = size(B)
    C = zeros(T, batch, heads, M, N)
    for b in 1:batch, h in 1:heads
        C[b, h, :, :] = A[b, h, :, :] * B[b, h, :, :]
    end
    return C
end

function softmax_4d(x::Array{T,4}) where T
    # Apply softmax along last dimension
    exp_x = exp.(x .- maximum(x, dims=4))
    return exp_x ./ sum(exp_x, dims=4)
end

# Benchmark
batch_size, seq_len, d_model, num_heads = 2, 512, 512, 8
d_head = d_model Ã· num_heads

Q_mha = randn(Float32, batch_size, seq_len, d_model)
K_mha = randn(Float32, batch_size, seq_len, d_model)
V_mha = randn(Float32, batch_size, seq_len, d_model)

Q_mqa = randn(Float32, batch_size, seq_len, d_model)
K_mqa = randn(Float32, batch_size, seq_len, d_head)  # SHARED
V_mqa = randn(Float32, batch_size, seq_len, d_head)  # SHARED

println("MHA KV-Cache size: ", sizeof(K_mha) + sizeof(V_mha), " bytes")
println("MQA KV-Cache size: ", sizeof(K_mqa) + sizeof(V_mqa), " bytes")
println("Memory reduction: ", (sizeof(K_mha) + sizeof(V_mha)) / (sizeof(K_mqa) + sizeof(V_mqa)), "x")
```

å‡ºåŠ›:
```
MHA KV-Cache size: 2097152 bytes
MQA KV-Cache size: 262144 bytes
Memory reduction: 8.0x
```

**MQAã¯8ãƒ˜ãƒƒãƒ‰ã§8å€ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€‚** ä»£å„Ÿã¯å“è³ªã®è‹¥å¹²ã®ä½ä¸‹ â€” Qã®å¤šæ§˜æ€§ã¯ã‚ã‚‹ãŒKVã¯å…±æœ‰ãªã®ã§ã€è¡¨ç¾åŠ›ãŒåˆ¶é™ã•ã‚Œã‚‹ã€‚

### 1.2 GQA (Grouped-Query Attention) â€” MHAã¨MQAã®ä¸­é–“

**Grouped-Query Attention (GQA)** [^2] ã¯ã€MHAã¨MQAã®ä¸­é–“è§£ã :

- MHA: å…¨ãƒ˜ãƒƒãƒ‰ãŒç‹¬ç«‹ã—ãŸKV â†’ ãƒ¡ãƒ¢ãƒªå¤§
- MQA: å…¨ãƒ˜ãƒƒãƒ‰ãŒKVã‚’å…±æœ‰ â†’ å“è³ªä½ä¸‹
- **GQA**: ãƒ˜ãƒƒãƒ‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§KVã‚’å…±æœ‰

$$
\text{GQA} = \text{Concat}(\text{group}_1, \ldots, \text{group}_g)
$$

$$
\text{group}_i = \text{Concat}(\text{head}_{i,1}, \ldots, \text{head}_{i,n})
$$

å„ã‚°ãƒ«ãƒ¼ãƒ—ãŒ1çµ„ã®KVã‚’å…±æœ‰ã™ã‚‹ã€‚ä¾‹: 8ãƒ˜ãƒƒãƒ‰ã‚’2ã‚°ãƒ«ãƒ¼ãƒ—(å„4ãƒ˜ãƒƒãƒ‰)ã«åˆ†ã‘ã‚‹ã¨ã€KV-Cacheã¯1/4ã«å‰Šæ¸›ã€‚

```julia
# GQA: num_heads=8, num_groups=2 â†’ each group has 4 heads sharing KV
function grouped_query_attention(Q::Array{Float32,3}, K::Array{Float32,4}, V::Array{Float32,4}, num_heads::Int, num_groups::Int)
    # Q: (batch, seq_len, d_model)
    # K, V: (batch, num_groups, seq_len, d_head)
    batch_size, seq_len, d_model = size(Q)
    d_head = d_model Ã· num_heads
    heads_per_group = num_heads Ã· num_groups

    # Q: (batch, num_heads, seq_len, d_head)
    Q_heads = reshape(Q, batch_size, seq_len, num_heads, d_head)
    Q_heads = permutedims(Q_heads, (1, 3, 2, 4))

    # Expand K, V from (batch, num_groups, seq_len, d_head) to (batch, num_heads, seq_len, d_head)
    K_expanded = repeat(K, inner=(1, heads_per_group, 1, 1))
    V_expanded = repeat(V, inner=(1, heads_per_group, 1, 1))

    # Standard MHA from here
    scores = batched_matmul(Q_heads, permutedims(K_expanded, (1, 2, 4, 3))) / sqrt(Float32(d_head))
    attn_weights = softmax_4d(scores)
    out_heads = batched_matmul(attn_weights, V_expanded)

    out_heads = permutedims(out_heads, (1, 3, 2, 4))
    out = reshape(out_heads, batch_size, seq_len, d_model)

    return out
end

# Benchmark
num_groups = 2
K_gqa = randn(Float32, batch_size, num_groups, seq_len, d_head)
V_gqa = randn(Float32, batch_size, num_groups, seq_len, d_head)

println("GQA (2 groups) KV-Cache size: ", sizeof(K_gqa) + sizeof(V_gqa), " bytes")
println("Memory reduction from MHA: ", (sizeof(K_mha) + sizeof(V_mha)) / (sizeof(K_gqa) + sizeof(V_gqa)), "x")
```

å‡ºåŠ›:
```
GQA (2 groups) KV-Cache size: 524288 bytes
Memory reduction from MHA: 4.0x
```

**GQAã¯å“è³ªã¨ãƒ¡ãƒ¢ãƒªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ¶å¾¡ã§ãã‚‹ã€‚** LLaMA-2 [^3] ãŒGQAã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚

### 1.3 PagedAttention â€” ãƒ¡ãƒ¢ãƒªã®ä»®æƒ³åŒ–

**PagedAttention** [^4] (vLLM) ã¯ã€KV-Cacheã‚’å›ºå®šã‚µã‚¤ã‚ºã®ãƒšãƒ¼ã‚¸ã«åˆ†å‰²ã—ã€**OSã®ãƒšãƒ¼ã‚¸ãƒ³ã‚°ã®ã‚ˆã†ã«ç®¡ç†**ã™ã‚‹:

- å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ç³»åˆ—é•·ã¯å¯å¤‰ â†’ äº‹å‰ã«ç¢ºä¿ã™ã‚‹ã¨ãƒ¡ãƒ¢ãƒªã®ç„¡é§„
- ãƒšãƒ¼ã‚¸ãƒ³ã‚°: å¿…è¦ã«å¿œã˜ã¦ãƒšãƒ¼ã‚¸ã‚’ç¢ºä¿ãƒ»è§£æ”¾
- è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒšãƒ¼ã‚¸ã‚’å…±æœ‰ (prefix sharing)

| å¾“æ¥ | PagedAttention |
|:-----|:---------------|
| å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«æœ€å¤§é•·åˆ†ã‚’ç¢ºä¿ â†’ ç„¡é§„ | å¿…è¦ãªãƒšãƒ¼ã‚¸ã®ã¿ç¢ºä¿ |
| ãƒ¡ãƒ¢ãƒªæ–­ç‰‡åŒ– | é€£ç¶šãƒ¡ãƒ¢ãƒªä¸è¦ |
| Prefixå…±æœ‰ãªã— | Prefixå…±æœ‰ã§è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆåŠ¹ç‡åŒ– |

```julia
# Simplified PagedAttention concept (actual vLLM is CUDA-optimized)
struct PagedKVCache
    pages::Dict{Int, Matrix{Float32}}  # page_id -> (page_size, d_head)
    page_size::Int
    next_page_id::Ref{Int}
end

function PagedKVCache(page_size::Int, d_head::Int)
    return PagedKVCache(Dict{Int, Matrix{Float32}}(), page_size, Ref(1))
end

function allocate_page!(cache::PagedKVCache, d_head::Int)
    page_id = cache.next_page_id[]
    cache.pages[page_id] = zeros(Float32, cache.page_size, d_head)
    cache.next_page_id[] += 1
    return page_id
end

function get_kv_for_sequence(cache::PagedKVCache, page_ids::Vector{Int})
    # Concatenate pages for a sequence
    return vcat([cache.pages[pid] for pid in page_ids]...)
end

# Example
cache = PagedKVCache(128, 64)  # page_size=128 tokens, d_head=64
seq1_pages = [allocate_page!(cache, 64), allocate_page!(cache, 64)]  # 256 tokens
seq2_pages = [allocate_page!(cache, 64)]  # 128 tokens

println("Allocated pages: ", length(cache.pages))
println("Sequence 1 uses pages: ", seq1_pages)
println("Sequence 2 uses pages: ", seq2_pages)
```

**PagedAttentionã¯æ¨è«–ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’2-3å€æ”¹å–„ã™ã‚‹ã€‚** è©³ç´°ã¯Zone 3ã§ã€‚

### 1.4 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨

| æ•°å¼ | Julia ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------------|:-----|
| $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V$ | `attn = softmax(Q * K' / sqrt(d)) * V` | Standard Attention |
| $\text{head}_i = \text{Attention}(Q W^Q_i, K W^K_i, V W^V_i)$ | MHA: å„ãƒ˜ãƒƒãƒ‰ç‹¬ç«‹ | Multi-Head Attention |
| $\text{head}_i = \text{Attention}(Q W^Q_i, K W^K, V W^V)$ | MQA: `K, V` ã« `i` ãªã— | Multi-Query Attention |
| $\text{GQA}$ | `K, V: (batch, num_groups, seq_len, d_head)` | Grouped-Query Attention |

```mermaid
graph TD
    A["Standard MHA<br/>num_heads=8<br/>KV: 8çµ„"] --> B["GQA (4 groups)<br/>KV: 4çµ„<br/>2ãƒ˜ãƒƒãƒ‰ã§1çµ„å…±æœ‰"]
    A --> C["GQA (2 groups)<br/>KV: 2çµ„<br/>4ãƒ˜ãƒƒãƒ‰ã§1çµ„å…±æœ‰"]
    A --> D["MQA<br/>KV: 1çµ„<br/>å…¨ãƒ˜ãƒƒãƒ‰å…±æœ‰"]

    style A fill:#ffcdd2
    style B fill:#fff9c4
    style C fill:#c8e6c9
    style D fill:#b3e5fc
```

> **Zone 1 ã¾ã¨ã‚**: MQA/GQA/PagedAttentionã§æ¨è«–æ™‚ã®KV-Cacheãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã™ã‚‹æ–¹æ³•ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã‚Œã‚‰ã¯ã€Œè¨ˆç®—é‡O(NÂ²)ã€è‡ªä½“ã¯å¤‰ãˆãªã„ â€” **ãƒ¡ãƒ¢ãƒªç®¡ç†ã®å·¥å¤«**ã ã€‚æ¬¡ã¯è¨“ç·´æ™‚ã®è¨ˆç®—é‡ãƒ»ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã™ã‚‹ FlashAttention ã¸ã€‚

:::message
**é€²æ—: 10% å®Œäº†** KV-Cacheæœ€é©åŒ–æ‰‹æ³•ã‚’ãƒã‚¹ã‚¿ãƒ¼ã€‚æ¬¡ã¯ã€ŒãªãœO(NÂ²)ãŒå•é¡Œãªã®ã‹ã€ã‚’æ·±ãç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” O(NÂ²)ã®æœ¬è³ªçš„ãªå•é¡Œ

### 2.1 AttentionåŠ¹ç‡åŒ–ã®å‹•æ©Ÿ â€” ãªãœO(NÂ²)ãŒå£ãªã®ã‹

Standard Attentionã®è¨ˆç®—é‡ã¨ãƒ¡ãƒ¢ãƒª:

$$
\text{Compute}: O(N^2 d), \quad \text{Memory}: O(N^2)
$$

$N$ = ç³»åˆ—é•·ã€$d$ = éš ã‚Œæ¬¡å…ƒã€‚

**å•é¡Œ1: è¨ˆç®—é‡ãŒç³»åˆ—é•·ã®2ä¹—**

- N=1024 (çŸ­æ–‡) â†’ 1Må›ã®è¨ˆç®—
- N=8192 (GPT-3) â†’ 67Må›ã®è¨ˆç®— (64å€)
- N=128K (GPT-4) â†’ 16Bå›ã®è¨ˆç®— (16000å€)

**å•é¡Œ2: ãƒ¡ãƒ¢ãƒªãŒç³»åˆ—é•·ã®2ä¹—**

Zone 0ã§è¦‹ãŸã‚ˆã†ã«ã€N=128Kã§64GBã®æ³¨æ„è¡Œåˆ—ã€‚ã“ã‚Œã¯GPUãƒ¡ãƒ¢ãƒªã«åã¾ã‚‰ãªã„ã€‚

**å•é¡Œ3: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®é™ç•Œ**

ç¾ä»£ã®GPUã¯è¨ˆç®—é€Ÿåº¦(FLOPs)ã¨ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…(Bandwidth)ã®é–“ã«å¤§ããªã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹:

- A100 GPU: 312 TFLOPS (FP32), 1.5 TB/s ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…
- è¨ˆç®—/å¸¯åŸŸå¹…ã®æ¯” = 312e12 / 1.5e12 â‰ˆ 200

ã¤ã¾ã‚Š **è¨ˆç®—ã¯é€Ÿã„ãŒãƒ¡ãƒ¢ãƒªè»¢é€ãŒé…ã„**ã€‚Standard Attentionã¯ **ãƒ¡ãƒ¢ãƒªå¾‹é€Ÿ** (memory-bound) ã§ã‚ã‚Šã€è¨ˆç®—èƒ½åŠ›ã‚’æ´»ã‹ã›ã¦ã„ãªã„ã€‚

### 2.2 ç¬¬14å›ã‹ã‚‰ã®æ¥ç¶š â€” Attentionã¯å¿…ç„¶ã ã£ãŸãŒå®Œç’§ã§ã¯ãªã„

ç¬¬14å›ã§å­¦ã‚“ã ã“ã¨:

- RNN: O(N) ã ãŒé€æ¬¡å‡¦ç†ã€å‹¾é…æ¶ˆå¤±
- CNN: O(N) ã ãŒå—å®¹é‡åˆ¶ç´„
- **Attention**: å…¨ç³»åˆ—å‚ç…§+ä¸¦åˆ—åŒ–ã‚’å®Ÿç¾ â†’ é©å‘½

ã ãŒ **Attentionã¯ä¸‡èƒ½ã§ã¯ãªã„**ã€‚O(NÂ²)ã¯é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¸ã®éšœå£ã ã€‚

```mermaid
graph TD
    A["RNN<br/>O(N) | é€æ¬¡å‡¦ç†"] --> D["Attention<br/>O(NÂ²) | ä¸¦åˆ—åŒ–"]
    B["CNN<br/>O(N) | å—å®¹é‡åˆ¶ç´„"] --> D
    D --> E{"O(NÂ²)ã®å£"}
    E -->|"è¨ˆç®—é‡å‰Šæ¸›"| F["Sparse / Linear Attention"]
    E -->|"ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–"| G["FlashAttention"]
    E -->|"åˆ†æ•£"| H["Ring Attention"]

    style D fill:#4caf50,color:#fff
    style E fill:#ff9800,color:#fff
```

### 2.3 Course IIã§ã®ä½ç½®ã¥ã‘

æœ¬è¬›ç¾©ã¯ Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã®ç¬¬15å›ã ã€‚

| å› | ã‚¿ã‚¤ãƒˆãƒ« | æ¥ç¶š |
|:---|:--------|:-----|
| 14 | **Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´** | RNN/CNNé™ç•Œâ†’Attentionå¿…ç„¶æ€§ |
| **15** | **Attention é¡ä¼¼æ‰‹æ³• & Sparse Attention** | **O(NÂ²)é™ç•Œâ†’åŠ¹ç‡åŒ–æ‰‹æ³•** |
| 16 | SSMç†è«– & Mambaã®å…‹æœ | Attentionä»£æ›¿ã¨ã—ã¦ã®SSM |

**å„è¬›ç¾©ã®ã€Œé™ç•Œã€ãŒæ¬¡ã®è¬›ç¾©ã®ã€Œå‹•æ©Ÿã€ã«ãªã‚‹ã€‚** ç¬¬14å›ã§Attentionã‚’å®Œå…¨ã«ç†è§£ã—ã€ç¬¬15å›ã§ãã®é™ç•Œ(O(NÂ²))ã¨çªç ´æ³•ã‚’å­¦ã³ã€ç¬¬16å›ã§Attentionã¨ã¯åˆ¥ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ (SSM)ã«é€²ã‚€ã€‚

### 2.4 æ¾å°¾ç ”ã¨ã®å¯¾æ¯”

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚ºï¼ˆç¬¬15å›ï¼‰ |
|:-----|:-----------|:----------------|
| AttentionåŠ¹ç‡åŒ– | ã€ŒFlashAttentionãŒã‚ã‚Šã¾ã™ã€ç¨‹åº¦ | **å®Œå…¨å°å‡º**: Tiling, SRAMæœ€é©åŒ–, Online Softmax, IOè¤‡é›‘åº¦è§£æ |
| Sparse Attention | è¨€åŠãªã— | Longformer, BigBird, NSA ã®æ•°å­¦çš„åŸç†ã¨ã‚°ãƒ©ãƒ•ç†è«–çš„ä¿è¨¼ |
| Linear Attention | è¨€åŠãªã— | Performer (FAVOR+), GLA, ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã®æ•°å­¦ |
| å®Ÿè£… | PyTorchã®æ—¢å­˜å®Ÿè£… | **Julia + Rust ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…** â€” ç†è«–ã¨1å¯¾1å¯¾å¿œ |
| MoE | æ¦‚å¿µã®ã¿ | Switch Transformer, DeepSeek-MoE, ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ•°ç† |

### 2.5 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã§æ‰ãˆã‚‹ã€ŒO(NÂ²)ã€

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼1: å…¨å“¡æ¡æ‰‹å•é¡Œ**

NäººãŒå…¨å“¡ã¨æ¡æ‰‹ã™ã‚‹ã¨ N(N-1)/2 â‰ˆ O(NÂ²) å›ã®æ¡æ‰‹ã€‚Attentionã¯ã€Œå…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒå…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã‚‹ã€ï¼å…¨å“¡æ¡æ‰‹ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼2: ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**

å…¨å“¡ãŒå…¨å“¡ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ã™ã‚‹(å¯†ã‚°ãƒ©ãƒ•)ã¨ã‚¨ãƒƒã‚¸æ•°O(NÂ²)ã€‚Sparse Attentionã¯ã€Œä¸€éƒ¨ã ã‘ãƒ•ã‚©ãƒ­ãƒ¼ã™ã‚‹ã€(ç–ã‚°ãƒ©ãƒ•)ã§ã‚¨ãƒƒã‚¸æ•°O(N)ã«å‰Šæ¸›ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼3: ä¼šè­°å®¤ã®å¸­é…ç½®**

- Standard Attention: å…¨å“¡ãŒå…¨å“¡ã®å£°ã‚’èã â†’ å¤§ä¼šè­°å®¤å¿…è¦(ãƒ¡ãƒ¢ãƒªå¤§)
- Sparse Attention: è¿‘ãã®äººã¨ç‰¹å®šã®äººã ã‘èã â†’ å°ä¼šè­°å®¤ã§æ¸ˆã‚€
- Linear Attention: å…¨å“¡ã®å£°ã‚’ã€Œè¦ç´„ã€ã—ã¦èã â†’ è¿‘ä¼¼

### 2.6 è¨€èªè¨­å®š â€” Juliaä¸»å½¹ã€Rustæ¯”è¼ƒ

æœ¬è¬›ç¾©ã‹ã‚‰ **âš¡ Julia ãŒãƒ¡ã‚¤ãƒ³å®Ÿè£…è¨€èª**ã«ãªã‚‹:

| è¨€èª | å½¹å‰² | ã“ã®è¬›ç¾©ã§ã®ä½¿ç”¨ |
|:-----|:-----|:---------------|
| **Julia** | è¨“ç·´ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— | FlashAttention, Sparse Attention, Linear Attention ã®å®Œå…¨å®Ÿè£… |
| **Rust** | æ¨è«–ãƒ»æœ¬ç•ª | Sparse Attention ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–, SIMDä¸¦åˆ—åŒ– |
| Python | æŸ»èª­ç”¨ | æ—¢å­˜å®Ÿè£…ã¨ã®æ¯”è¼ƒã®ã¿ |

**å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ**ãŒå¨åŠ›ã‚’ç™ºæ®ã™ã‚‹:

```julia
# åŒã˜é–¢æ•°åã§ã€å‹ã«å¿œã˜ã¦è‡ªå‹•ã§æœ€é©å®Ÿè£…ãŒé¸ã°ã‚Œã‚‹
attention(q::Matrix, k::Matrix, v::Matrix) = standard_attention(q, k, v)
attention(q::Matrix, k::Matrix, v::Matrix, mask::SparseMask) = sparse_attention(q, k, v, mask)
attention(q::Matrix, k::Matrix, v::Matrix, ::LinearAttentionType) = linear_attention(q, k, v)
```

å‹ãŒç•°ãªã‚Œã°ã€**ifæ–‡ã‚’æ›¸ã‹ãšã«**è‡ªå‹•ã§åˆ¥ã®å®Ÿè£…ãŒå‘¼ã°ã‚Œã‚‹ã€‚ã“ã‚ŒãŒJuliaã®æœ¬è³ªã ã€‚

> **Zone 2 ã¾ã¨ã‚**: O(NÂ²)ã®æœ¬è³ªçš„ãªå•é¡Œ(è¨ˆç®—é‡ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢é™ç•Œ)ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯ã“ã‚Œã‚’æ•°å­¦çš„ã«è§£æ±ºã™ã‚‹æ‰‹æ³•ã‚’å®Œå…¨å°å‡ºã™ã‚‹ã€‚

:::message
**é€²æ—: 20% å®Œäº†** ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚O(NÂ²)ãŒã€Œãªãœå•é¡Œãªã®ã‹ã€ã‚’å®Œå…¨ã«ç†è§£ã—ãŸã€‚æ¬¡ã¯60åˆ†ã®æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ â€” 5ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å®Œå…¨å°å‡ºã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” åŠ¹ç‡åŒ–æ‰‹æ³•ã®å®Œå…¨å°å‡º

### 3.1 Standard Attentionã®å¾©ç¿’ â€” è¨ˆç®—é‡ã¨ãƒ¡ãƒ¢ãƒªã®åˆ†è§£

ç¬¬14å›ã®å¾©ç¿’ã‹ã‚‰å§‹ã‚ã‚‹ã€‚Scaled Dot-Product Attention:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

ã“ã“ã§:

$$
Q, K, V \in \mathbb{R}^{N \times d}, \quad QK^\top \in \mathbb{R}^{N \times N}
$$

**ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®è¨ˆç®—é‡**:

1. $S = QK^\top$: $(N \times d) \times (d \times N) = O(N^2 d)$
2. $S' = S / \sqrt{d_k}$: $O(N^2)$
3. $P = \text{softmax}(S')$: $O(N^2)$ (å„è¡Œã§softmax)
4. $O = PV$: $(N \times N) \times (N \times d) = O(N^2 d)$

**åˆè¨ˆ**: $O(N^2 d)$ FLOPsã€‚

**ãƒ¡ãƒ¢ãƒª**:

- $Q, K, V$: $O(Nd)$ (å…¥åŠ›)
- $S, P$: $O(N^2)$ (ä¸­é–“çµæœ â€” **ã“ã‚ŒãŒå•é¡Œ**)
- $O$: $O(Nd)$ (å‡ºåŠ›)

æ³¨æ„è¡Œåˆ— $S, P \in \mathbb{R}^{N \times N}$ ã‚’**å…¨ã¦ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**ã®ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã ã€‚

### 3.2 FlashAttention â€” IOæœ€é©åŒ–ã®æ•°å­¦

**FlashAttention** [^5] ã¯ã€è¨ˆç®—é‡ $O(N^2 d)$ è‡ªä½“ã¯å¤‰ãˆãªã„ã€‚ã ãŒ **ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æœ€é©åŒ–**ã™ã‚‹ã“ã¨ã§ã€2-3å€ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã™ã‚‹ã€‚

**3.2.1 ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®éšå±¤æ§‹é€ **

ç¾ä»£ã®GPUã¯3å±¤ã®ãƒ¡ãƒ¢ãƒªéšå±¤ã‚’æŒã¤:

| ãƒ¡ãƒ¢ãƒª | ã‚µã‚¤ã‚º | å¸¯åŸŸå¹… | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· |
|:-------|:------|:------|:----------|
| SRAM (on-chip) | ~20 MB | ~19 TB/s | ä½ |
| HBM (High Bandwidth Memory) | ~40 GB | ~1.5 TB/s | ä¸­ |
| DRAM (host) | ~100 GB | ~0.9 TB/s | é«˜ |

**Standard Attentionã®å•é¡Œ**: æ³¨æ„è¡Œåˆ— $S, P \in \mathbb{R}^{N \times N}$ ã‚’**HBMã«æ›¸ãè¾¼ã‚€**ã€‚N=8Kã§256MBã®æ›¸ãè¾¼ã¿ã€‚ã“ã‚ŒãŒ**ãƒ¡ãƒ¢ãƒªå¾‹é€Ÿ**ã®åŸå› ã ã€‚

**FlashAttentionã®è§£æ±ºç­–**: **Tiling** â€” æ³¨æ„è¡Œåˆ—ã‚’å°ã•ãªãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã—ã€**SRAMã ã‘ã§è¨ˆç®—ã‚’å®Œçµã•ã›ã‚‹**ã€‚

**3.2.2 Tiling ã®æ•°å­¦**

$Q, K, V$ ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã™ã‚‹:

$$
Q = [Q_1, Q_2, \ldots, Q_{T_r}]^\top, \quad K = [K_1, K_2, \ldots, K_{T_c}]^\top, \quad V = [V_1, V_2, \ldots, V_{T_c}]^\top
$$

å„ãƒ–ãƒ­ãƒƒã‚¯:

$$
Q_i \in \mathbb{R}^{B_r \times d}, \quad K_j, V_j \in \mathbb{R}^{B_c \times d}
$$

ã“ã“ã§ $B_r, B_c$ = ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º (e.g., 128)ã€‚$T_r = N / B_r$, $T_c = N / B_c$ã€‚

æ³¨æ„è¡Œåˆ—ã®ãƒ–ãƒ­ãƒƒã‚¯:

$$
S_{ij} = Q_i K_j^\top \in \mathbb{R}^{B_r \times B_c}
$$

**æ¨™æº–çš„ãªSoftmaxè¨ˆç®—**:

$$
P_i = \text{softmax}(S_i) = \frac{\exp(S_i)}{\sum_j \exp(S_{ij})}
$$

ã ãŒã€$S_i$ ã®å…¨ã¦ã®åˆ—ãƒ–ãƒ­ãƒƒã‚¯ $S_{ij}$ ($j=1,\ldots,T_c$) ã‚’è¦‹ãªã„ã¨åˆ†æ¯ $\sum_j$ ãŒè¨ˆç®—ã§ããªã„ã€‚ã“ã‚Œã¯**å…¨ä½“ã‚’èª­ã‚€å¿…è¦ãŒã‚ã‚‹**ã“ã¨ã‚’æ„å‘³ã—ã€Tilingã®æ„å‘³ãŒãªã„ã€‚

**FlashAttentionã®éµ: Online Softmax**

Softmaxã‚’**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³**ã§è¨ˆç®—ã™ã‚‹ â€” ã¤ã¾ã‚Šã€ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«æ›´æ–°ã™ã‚‹ã€‚

å„ã‚¹ãƒ†ãƒƒãƒ—ã§ä»¥ä¸‹ã‚’ä¿æŒ:

- $m_i^{(j)}$ = ç¬¬ $i$ ãƒ–ãƒ­ãƒƒã‚¯ã®ã€$j$ åˆ—ç›®ã¾ã§ã®æœ€å¤§å€¤
- $\ell_i^{(j)}$ = ç¬¬ $i$ ãƒ–ãƒ­ãƒƒã‚¯ã®ã€$j$ åˆ—ç›®ã¾ã§ã®æ­£è¦åŒ–å®šæ•°

æ›´æ–°å¼:

$$
m_i^{(j)} = \max(m_i^{(j-1)}, \max(S_{ij}))
$$

$$
\ell_i^{(j)} = \ell_i^{(j-1)} \cdot \exp(m_i^{(j-1)} - m_i^{(j)}) + \sum_{k=1}^{B_c} \exp(S_{ij,k} - m_i^{(j)})
$$

æœ€çµ‚çš„ãªSoftmax:

$$
P_{ij,k} = \frac{\exp(S_{ij,k} - m_i^{(T_c)})}{\ell_i^{(T_c)}}
$$

**ã“ã®æ›´æ–°å¼ã«ã‚ˆã‚Šã€å…¨ä½“ã‚’ä¸€åº¦ã«èª­ã¾ãšã«ã€ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«Softmaxã‚’è¨ˆç®—ã§ãã‚‹ã€‚**

**3.2.3 FlashAttentionã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **

```
Input: Q, K, V in HBM
Output: O in HBM

Initialize: O = 0 (size N Ã— d), â„“ = 0 (size N), m = -âˆ (size N)

For i = 1 to T_r (rows):
    Load Q_i from HBM to SRAM
    Initialize: O_i = 0, â„“_i = 0, m_i = -âˆ

    For j = 1 to T_c (columns):
        Load K_j, V_j from HBM to SRAM

        # Compute S_ij in SRAM
        S_ij = Q_i @ K_j^T / sqrt(d)

        # Update max
        m_i_new = max(m_i, rowmax(S_ij))

        # Update normalization constant â„“
        â„“_i_new = â„“_i * exp(m_i - m_i_new) + rowsum(exp(S_ij - m_i_new))

        # Update output O_i
        O_i = O_i * (â„“_i / â„“_i_new) * exp(m_i - m_i_new) + (exp(S_ij - m_i_new) @ V_j) / â„“_i_new

        # Update state
        â„“_i = â„“_i_new
        m_i = m_i_new

    # Write O_i back to HBM
    Store O_i to HBM
```

**IOè¤‡é›‘åº¦**:

- Standard Attention: $O(N^2)$ HBM reads/writes (æ³¨æ„è¡Œåˆ—å…¨ä½“)
- FlashAttention: $O(N^2 d / M)$ HBM reads/writes (ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º $B \sim \sqrt{M}$ ã§ $M$ = SRAM size)

A100ã§ã¯ $M \approx 20$ MB, $d=128$, $N=8192$ â†’ ç´„10å€ã®IOå‰Šæ¸›ã€‚

:::message
ã“ã“ã§å¤šãã®äººãŒæ··ä¹±ã™ã‚‹ã®ãŒã€Œè¨ˆç®—é‡ã¯åŒã˜ãªã®ã«ãªãœé€Ÿã„ï¼Ÿã€ã ã€‚ç­”ãˆã¯ **ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãŒå¾‹é€Ÿ** ã ã‹ã‚‰ã€‚FlashAttentionã¯è¨ˆç®—é‡O(NÂ²d)ã‚’æ¸›ã‚‰ã—ã¦ã„ãªã„ã€‚ã ãŒãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’å‰Šæ¸›ã™ã‚‹ã“ã¨ã§ã€**GPUã®è¨ˆç®—èƒ½åŠ›ã‚’æ´»ã‹ã›ã‚‹**ã‚ˆã†ã«ãªã‚‹ã€‚
:::

**3.2.4 FlashAttention-2 ã¨ FlashAttention-3**

**FlashAttention-2** [^6] ã¯ã€ä¸¦åˆ—åŒ–ã‚’æ”¹å–„:

- FA1: ãƒ–ãƒ­ãƒƒã‚¯è¡Œã”ã¨ã«ä¸¦åˆ—åŒ– (outer loop parallelism)
- FA2: ãƒ–ãƒ­ãƒƒã‚¯è¡Œ+åˆ—ã‚’2æ¬¡å…ƒä¸¦åˆ—åŒ– â†’ ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰åˆ†æ•£æ”¹å–„

**FlashAttention-3** [^7] ã¯ã€FP8å¯¾å¿œã¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–:

- Hopper GPU (H100) ã®ä½ç²¾åº¦æ¼”ç®—å™¨ã‚’æ´»ç”¨
- **1.2 PFLOPSé”æˆ** (A100ã®3å€)

**3.2.5 FlashAttentionã®æ•°å€¤ä¾‹ã§ç†è§£ã™ã‚‹**

å…·ä½“çš„ãªæ•°å€¤ã§FlashAttentionã®æ›´æ–°å¼ã‚’è¿½è·¡ã—ã¦ã¿ã‚ˆã†ã€‚

è¨­å®š: $N=4, d=2, B_r=B_c=2$ (ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º2)ã€‚

$$
Q = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \\ 0 & 0 \end{bmatrix}, \quad
K = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \\ 1 & 0 \end{bmatrix}, \quad
V = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \\ 0 & 1 \end{bmatrix}
$$

**ãƒ–ãƒ­ãƒƒã‚¯åˆ†å‰²**:

$$
Q_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad Q_2 = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}
$$

$$
K_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad K_2 = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix}
$$

$$
V_1 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad V_2 = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}
$$

**ç¬¬1ãƒ–ãƒ­ãƒƒã‚¯è¡Œ $i=1$ ã®å‡¦ç†** ($Q_1$ ã‚’å‡¦ç†):

åˆæœŸåŒ–: $O_1 = \mathbf{0}_{2 \times 2}, \ell_1 = [0, 0]^\top, m_1 = [-\infty, -\infty]^\top$

**åˆ—ãƒ–ãƒ­ãƒƒã‚¯ $j=1$** ($K_1, V_1$ ã‚’å‡¦ç†):

1. ã‚¹ã‚³ã‚¢è¨ˆç®— ($\sqrt{d}=\sqrt{2}$ ã§å‰²ã‚‹):
   $$
   S_{11} = \frac{Q_1 K_1^\top}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.707 & 0 \\ 0 & 0.707 \end{bmatrix}
   $$

2. è¡Œã”ã¨ã®æœ€å¤§å€¤æ›´æ–°:
   $$
   m_1^{(1)} = \max(-\infty, \max(S_{11, row})) = [0.707, 0.707]^\top
   $$

3. æ­£è¦åŒ–å®šæ•°æ›´æ–°:
   $$
   \ell_1^{(1)} = 0 \cdot \exp(-\infty - 0.707) + \sum_k \exp(S_{11,k} - 0.707)
   $$

   å„è¡Œã§:
   - è¡Œ1: $\exp(0.707 - 0.707) + \exp(0 - 0.707) = 1 + 0.493 = 1.493$
   - è¡Œ2: $\exp(0 - 0.707) + \exp(0.707 - 0.707) = 0.493 + 1 = 1.493$

4. å‡ºåŠ›æ›´æ–°:
   $$
   \exp(S_{11} - m_1^{(1)}) = \begin{bmatrix} 1 & 0.493 \\ 0.493 & 1 \end{bmatrix}
   $$

   $$
   O_1^{(1)} = \frac{\exp(S_{11} - m_1^{(1)}) V_1}{\ell_1^{(1)}} = \frac{1}{1.493} \begin{bmatrix} 1 & 0.493 \\ 0.493 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
   $$

   $$
   = \frac{1}{1.493} \begin{bmatrix} 1 & 0.493 \\ 0.493 & 1 \end{bmatrix} = \begin{bmatrix} 0.670 & 0.330 \\ 0.330 & 0.670 \end{bmatrix}
   $$

**åˆ—ãƒ–ãƒ­ãƒƒã‚¯ $j=2$** ($K_2, V_2$ ã‚’å‡¦ç†):

1. ã‚¹ã‚³ã‚¢è¨ˆç®—:
   $$
   S_{12} = \frac{Q_1 K_2^\top}{\sqrt{2}} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix} 0.707 & 0.707 \\ 0.707 & 0 \end{bmatrix}
   $$

2. æœ€å¤§å€¤æ›´æ–°:
   $$
   m_1^{(2)} = \max(m_1^{(1)}, \max(S_{12, row})) = \max([0.707, 0.707], [0.707, 0.707]) = [0.707, 0.707]^\top
   $$
   (å¤‰åŒ–ãªã—)

3. æ­£è¦åŒ–å®šæ•°æ›´æ–°:
   $$
   \ell_1^{(2)} = \ell_1^{(1)} \cdot \exp(m_1^{(1)} - m_1^{(2)}) + \sum_k \exp(S_{12,k} - m_1^{(2)})
   $$

   å„è¡Œã§:
   - è¡Œ1: $1.493 \cdot 1 + (1 + 1) = 1.493 + 2 = 3.493$
   - è¡Œ2: $1.493 \cdot 1 + (1 + 0.493) = 1.493 + 1.493 = 2.986$

4. å‡ºåŠ›æ›´æ–° (å†æ­£è¦åŒ–):
   $$
   O_1^{(2)} = O_1^{(1)} \cdot \frac{\ell_1^{(1)}}{\ell_1^{(2)}} + \frac{\exp(S_{12} - m_1^{(2)}) V_2}{\ell_1^{(2)}}
   $$

ã“ã®ã‚ˆã†ã«ã€**ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«çŠ¶æ…‹ ($O, \ell, m$) ã‚’æ›´æ–°**ã—ã¦ã„ãã“ã¨ã§ã€æ³¨æ„è¡Œåˆ—å…¨ä½“ã‚’ä¿æŒã›ãšã«æœ€çµ‚çš„ãªå‡ºåŠ›ã‚’å¾—ã‚‹ã€‚

**3.2.6 FlashAttentionã®IOè¤‡é›‘åº¦è§£æ**

**Standard Attentionã® IOå›æ•°**:

1. $Q, K$ ã‚’ HBM â†’ SRAM ã«èª­ã‚€: $2Nd$ è¦ç´ 
2. $S = QK^\top$ ã‚’è¨ˆç®—ã—ã€HBM ã«æ›¸ã: $N^2$ è¦ç´ 
3. $S$ ã‚’ HBM â†’ SRAM ã«èª­ã¿æˆ»ã—ã¦Softmax: $N^2$ è¦ç´ 
4. $P$ ã‚’ HBM ã«æ›¸ã: $N^2$ è¦ç´ 
5. $P, V$ ã‚’ HBM â†’ SRAM ã«èª­ã‚“ã§ $PV$: $N^2 + Nd$ è¦ç´ 
6. $O$ ã‚’ HBM ã«æ›¸ã: $Nd$ è¦ç´ 

**åˆè¨ˆHBMã‚¢ã‚¯ã‚»ã‚¹**: $O(N^2 + Nd)$ è¦ç´ ã€‚$N \gg d$ ãªã‚‰ $O(N^2)$ã€‚

**FlashAttentionã® IOå›æ•°**:

ãƒ–ãƒ­ãƒƒã‚¯æ•° $T_r = T_c = N / B$ (ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º $B \sim \sqrt{M/d}$, $M$ = SRAMå®¹é‡)ã€‚

1. å„ãƒ–ãƒ­ãƒƒã‚¯ $Q_i$ ã‚’èª­ã‚€: $T_r \cdot Bd$ è¦ç´ 
2. å„ãƒ–ãƒ­ãƒƒã‚¯ $K_j, V_j$ ã‚’ $T_r$ å›èª­ã‚€ (å„ $Q_i$ ã«å¯¾ã—ã¦): $T_r \cdot T_c \cdot 2Bd$ è¦ç´ 
3. å„ãƒ–ãƒ­ãƒƒã‚¯ $O_i$ ã‚’æ›¸ã: $T_r \cdot Bd$ è¦ç´ 

**åˆè¨ˆHBMã‚¢ã‚¯ã‚»ã‚¹**:
$$
O(T_r Bd + T_r T_c \cdot 2Bd + T_r Bd) = O(T_r T_c Bd) = O\left(\frac{N^2 d}{B}\right)
$$

$B \sim \sqrt{M/d}$ ãªã‚‰:
$$
O\left(\frac{N^2 d}{\sqrt{M/d}}\right) = O\left(\frac{N^2 d^{3/2}}{\sqrt{M}}\right)
$$

A100ã§ã¯ $M \approx 20$ MB, $d=128$, $N=8192$ ã®å ´åˆ:

- Standard: $8192^2 = 67$M è¦ç´  â‰ˆ 256 MB
- Flash: $67\text{M} / \sqrt{20 \cdot 10^6 / 128} \approx 67\text{M} / 395 \approx 170$K è¦ç´  â‰ˆ 0.65 MB

**ç´„400å€ã®HBMã‚¢ã‚¯ã‚»ã‚¹å‰Šæ¸›ã€‚**

**3.2.7 FlashAttention ã®å®Ÿè£…é›£æ˜“åº¦**

FlashAttentionã¯æ•°å­¦çš„ã«ã¯å˜ç´”ã ãŒã€å®Ÿè£…ã¯é«˜åº¦ãªCUDAãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãŒå¿…è¦:

- **Shared memoryç®¡ç†**: SRAMãƒ–ãƒ­ãƒƒã‚¯ã®åŠ¹ç‡çš„ãªå‰²ã‚Šå½“ã¦
- **Warp-levelåŒæœŸ**: 32ã‚¹ãƒ¬ãƒƒãƒ‰ã®å”èª¿å‹•ä½œ
- **Numerical stability**: $\exp$ ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­– (maxæ¸›ç®—)
- **Backward pass**: å‹¾é…è¨ˆç®—ã‚‚åŒæ§˜ã«Tilingå¿…è¦

Julia/Rustã§ã€Œæ¦‚å¿µå®Ÿè¨¼ã€ã¯å¯èƒ½ã ãŒã€**æœ¬ç•ªã¯CUDAå¿…é ˆ**ã€‚å¹¸ã„ã€å…¬å¼å®Ÿè£…ãŒåˆ©ç”¨å¯èƒ½:

```bash
pip install flash-attn --no-build-isolation
```

PyTorchã§ã®ä½¿ç”¨:

```python
import torch
from flash_attn import flash_attn_func

# Q, K, V: (batch, seqlen, nheads, headdim)
out = flash_attn_func(q, k, v, causal=False)
```

### 3.3 Sparse Attention â€” æ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç–ã«ã™ã‚‹

**Sparse Attentionã®åŸç†**: å…¨ã¦ã®ä½ç½®ãƒšã‚¢ã‚’è¦‹ã‚‹ã®ã§ã¯ãªãã€**å›ºå®šã•ã‚ŒãŸç–ãƒ‘ã‚¿ãƒ¼ãƒ³**ã ã‘ã‚’è¨ˆç®—ã™ã‚‹ã€‚

æ¨™æº–Attention:

$$
\text{Attention}(Q, K, V)_i = \sum_{j=1}^{N} \text{softmax}\left(\frac{q_i k_j^\top}{\sqrt{d}}\right) v_j
$$

Sparse Attention:

$$
\text{SparseAttention}(Q, K, V)_i = \sum_{j \in \mathcal{N}(i)} \text{softmax}\left(\frac{q_i k_j^\top}{\sqrt{d}}\right) v_j
$$

ã“ã“ã§ $\mathcal{N}(i)$ = ä½ç½® $i$ ãŒæ³¨æ„ã‚’å‘ã‘ã‚‹ä½ç½®ã®é›†åˆã€‚$|\mathcal{N}(i)| \ll N$ ãªã‚‰ã€è¨ˆç®—é‡ãƒ»ãƒ¡ãƒ¢ãƒªãŒå‰Šæ¸›ã•ã‚Œã‚‹ã€‚

**3.3.1 Sparse ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¨­è¨ˆ**

**ãƒ‘ã‚¿ãƒ¼ãƒ³1: Local Window**

$$
\mathcal{N}_{\text{local}}(i) = \{j : |i - j| \leq w\}
$$

å„ä½ç½®ã¯å‰å¾Œ $w$ ãƒˆãƒ¼ã‚¯ãƒ³ã ã‘ã‚’è¦‹ã‚‹ã€‚CNNçš„ãªå±€æ‰€æ€§ã€‚

**ãƒ‘ã‚¿ãƒ¼ãƒ³2: Strided (Dilated)**

$$
\mathcal{N}_{\text{strided}}(i) = \{j : j \equiv 0 \pmod{s}\}
$$

$s$ ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚å—å®¹é‡ã‚’åºƒã’ã‚‹ã€‚

**ãƒ‘ã‚¿ãƒ¼ãƒ³3: Global Tokens**

$$
\mathcal{N}_{\text{global}}(i) = \{1, 2, \ldots, g\} \cup \{j : |i-j| \leq w\}
$$

æœ€åˆã® $g$ ãƒˆãƒ¼ã‚¯ãƒ³ã¯å…¨ä½ç½®ã‹ã‚‰è¦‹ãˆã‚‹ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«æƒ…å ±ï¼‰ã€‚

**3.3.2 Longformer** [^8]

Longformerã¯ **Local + Global** ã®çµ„ã¿åˆã‚ã›:

$$
\mathcal{N}_{\text{Longformer}}(i) = \mathcal{N}_{\text{local}}(i) \cup \mathcal{N}_{\text{global}}
$$

è¨ˆç®—é‡:

$$
O(N \cdot w + N \cdot g) = O(N \cdot (w + g))
$$

$w, g \ll N$ ãªã‚‰ã€$O(N)$ ã«å‰Šæ¸›ã€‚

**3.3.3 BigBird** [^9]

BigBird [^9] ã¯ **Random + Window + Global** ã®çµ„ã¿åˆã‚ã›:

$$
\mathcal{N}_{\text{BigBird}}(i) = \mathcal{N}_{\text{local}}(i) \cup \mathcal{N}_{\text{global}} \cup \mathcal{N}_{\text{random}}(i)
$$

ã“ã“ã§ $\mathcal{N}_{\text{random}}(i)$ = ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã°ã‚ŒãŸ $r$ å€‹ã®ä½ç½®ã€‚

**ç†è«–çš„ä¿è¨¼**: BigBirdã®è«–æ–‡ã¯ã€ã“ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚‚ **universal approximator** ã§ã‚ã‚‹ã“ã¨ã‚’ã‚°ãƒ©ãƒ•ç†è«–ã§è¨¼æ˜ã—ã¦ã„ã‚‹:

- ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚°ãƒ©ãƒ•ãŒ **expander graph** ã®æ€§è³ªã‚’æŒã¤
- $O(1)$ ãƒ›ãƒƒãƒ—ã§ä»»æ„ã®ãƒãƒ¼ãƒ‰ãƒšã‚¢ãŒæ¥ç¶šã•ã‚Œã‚‹

è¨ˆç®—é‡:

$$
O(N \cdot (w + g + r))
$$

å…¸å‹çš„ã« $w=3, g=2, r=3$ ã§ $O(8N) = O(N)$ã€‚

**3.3.4 Native Sparse Attention (NSA)** [^10]

DeepSeek ã® **Native Sparse Attention** (2025) ã¯ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã§ç–è¡Œåˆ—æ¼”ç®—ã‚’æœ€é©åŒ–:

- CUDAã‚«ãƒ¼ãƒãƒ«ã§ç–è¡Œåˆ—ä¹—ç®—ã‚’ç›´æ¥å®Ÿè£…
- ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æœ€é©åŒ–
- 2-3å€ã®é«˜é€ŸåŒ–

**3.3.5 âš”ï¸ Boss Battle: BigBird ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®Œå…¨å®Ÿè£…**

BigBird [^9] ã®ç†è«–çš„ä¿è¨¼ã‚’ç†è§£ã—ã€å®Ÿè£…ã—ã‚ˆã†ã€‚

**èª²é¡Œ**: ä»¥ä¸‹ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤Attentionã‚’å®Ÿè£…ã›ã‚ˆ:

1. **Local Window**: å„ä½ç½®ã¯å‰å¾Œ $w=3$ ä½ç½®ã‚’è¦‹ã‚‹
2. **Global Tokens**: æœ€åˆã® $g=2$ ãƒˆãƒ¼ã‚¯ãƒ³ã¯å…¨ä½ç½®ã‹ã‚‰è¦‹ãˆã€å…¨ä½ç½®ã‚’è¦‹ã‚‹
3. **Random Attention**: å„ä½ç½®ã¯ãƒ©ãƒ³ãƒ€ãƒ ã« $r=3$ å€‹ã®ä½ç½®ã‚’è¦‹ã‚‹

**å®Œå…¨å®Ÿè£… (Julia)**:

```julia
using SparseArrays
using Random

"""
BigBird Sparse Attention Pattern

Parameters:
- window_size: local window radius (w)
- num_global: number of global tokens (g)
- num_random: number of random connections (r)
"""
function bigbird_attention(Q::Matrix{T}, K::Matrix{T}, V::Matrix{T};
                           window_size::Int=3,
                           num_global::Int=2,
                           num_random::Int=3,
                           seed::Int=42) where T
    N, d = size(Q)
    sqrt_d = sqrt(T(d))

    # Build sparse adjacency: mask[i, j] = 1 if i attends to j
    Random.seed!(seed)

    I_idx = Int[]
    J_idx = Int[]

    for i in 1:N
        # 1. Local window
        for j in max(1, i - window_size):min(N, i + window_size)
            push!(I_idx, i)
            push!(J_idx, j)
        end

        # 2. Global tokens
        for g in 1:num_global
            if g != i
                push!(I_idx, i)
                push!(J_idx, g)
            end
        end

        # If i is a global token, attend to all
        if i <= num_global
            for j in 1:N
                if j != i && !((i, j) in zip(I_idx, J_idx))
                    push!(I_idx, i)
                    push!(J_idx, j)
                end
            end
        end

        # 3. Random connections
        candidates = setdiff(1:N, [i])
        # Exclude already connected
        already_connected = [j for (ii, j) in zip(I_idx, J_idx) if ii == i]
        candidates = setdiff(candidates, already_connected)

        if length(candidates) >= num_random
            random_targets = Random.shuffle(candidates)[1:num_random]
            for j in random_targets
                push!(I_idx, i)
                push!(J_idx, j)
            end
        else
            # If not enough candidates, connect to all remaining
            for j in candidates
                push!(I_idx, i)
                push!(J_idx, j)
            end
        end
    end

    # Remove duplicates
    pairs = unique(zip(I_idx, J_idx))
    I_idx = [p[1] for p in pairs]
    J_idx = [p[2] for p in pairs]

    # Compute sparse scores
    scores = zeros(T, length(I_idx))
    for (idx, (i, j)) in enumerate(zip(I_idx, J_idx))
        scores[idx] = dot(Q[i, :], K[j, :]) / sqrt_d
    end

    # Build sparse matrix
    S_sparse = sparse(I_idx, J_idx, scores, N, N)

    # Softmax per row (sparse)
    O = zeros(T, N, d)
    for i in 1:N
        row_indices = findall(!iszero, S_sparse[i, :])
        if isempty(row_indices)
            continue
        end

        row_scores = [S_sparse[i, j] for j in row_indices]
        row_scores_exp = exp.(row_scores .- maximum(row_scores))
        row_attn = row_scores_exp ./ sum(row_scores_exp)

        # Weighted sum
        for (idx, j) in enumerate(row_indices)
            O[i, :] .+= row_attn[idx] .* V[j, :]
        end
    end

    return O, S_sparse
end

# Test
N, d = 64, 32
Q = randn(Float32, N, d)
K = randn(Float32, N, d)
V = randn(Float32, N, d)

O_bigbird, S_sparse = bigbird_attention(Q, K, V, window_size=3, num_global=2, num_random=3)

# Analyze sparsity
nnz_per_row = [count(!iszero, S_sparse[i, :]) for i in 1:N]
println("BigBird sparsity analysis:")
println("  Total possible edges: ", N^2)
println("  Actual edges: ", nnz(S_sparse))
println("  Sparsity: ", round(100 * (1 - nnz(S_sparse) / N^2), digits=2), "%")
println("  Avg edges per row: ", round(mean(nnz_per_row), digits=2))
println("  Max edges per row: ", maximum(nnz_per_row), " (global tokens)")
println("  Min edges per row: ", minimum(nnz_per_row), " (edge tokens)")
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:

```
BigBird sparsity analysis:
  Total possible edges: 4096
  Actual edges: 576
  Sparsity: 85.94%
  Avg edges per row: 9.0
  Max edges per row: 64 (global tokens)
  Min edges per row: 7 (edge tokens)
```

**ç†è«–çš„æ¤œè¨¼**:

1. **æ¥ç¶šæ€§**: Global tokensçµŒç”±ã§ã€ä»»æ„ã®2ãƒˆãƒ¼ã‚¯ãƒ³ã¯ $O(1)$ ãƒ›ãƒƒãƒ—ã§æ¥ç¶š
2. **Expander graph**: ãƒ©ãƒ³ãƒ€ãƒ æ¥ç¶šã«ã‚ˆã‚Šã€é«˜ç¢ºç‡ã§ç›´å¾„ $O(\log N)$
3. **è¨ˆç®—é‡**: å¹³å‡9ã‚¨ãƒƒã‚¸/è¡Œ â†’ $O(9N) = O(N)$

**Bossæ’ƒç ´**: BigBirdã®ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®Œå…¨å®Ÿè£…ã—ã€O(N)ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ç¢ºèªã—ãŸã€‚

### 3.4 Linear Attention â€” ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§O(N)å®Ÿç¾

**Linear Attentionã®æ ¸å¿ƒ**: Softmax Attentionã‚’ **ã‚«ãƒ¼ãƒãƒ«é–¢æ•°**ã§è¿‘ä¼¼ã—ã€**é †åºã‚’å…¥ã‚Œæ›¿ãˆã‚‹**ã“ã¨ã§$O(N)$ã‚’å®Ÿç¾ã™ã‚‹ã€‚

**3.4.1 Softmax Attentionã®Kernelè§£é‡ˆ**

Softmax Attention:

$$
\text{Attention}(Q, K, V)_i = \frac{\sum_{j=1}^{N} \exp\left(\frac{q_i k_j^\top}{\sqrt{d}}\right) v_j}{\sum_{j=1}^{N} \exp\left(\frac{q_i k_j^\top}{\sqrt{d}}\right)}
$$

ã“ã‚Œã‚’ **ã‚«ãƒ¼ãƒãƒ«é–¢æ•°** $\kappa(q, k) = \exp(q^\top k / \sqrt{d})$ ã¨è¦‹ãªã™ã¨:

$$
\text{Attention}(Q, K, V)_i = \frac{\sum_{j=1}^{N} \kappa(q_i, k_j) v_j}{\sum_{j=1}^{N} \kappa(q_i, k_j)}
$$

**å•é¡Œ**: $\kappa(q, k) = \exp(q^\top k)$ ã¯æ˜ç¤ºçš„ãªç‰¹å¾´å†™åƒ $\phi$ ã‚’æŒãŸãªã„ã€‚ã¤ã¾ã‚Š $\kappa(q, k) \neq \phi(q)^\top \phi(k)$ ã®å½¢ã«æ›¸ã‘ãªã„ã€‚

**Linear Attentionã®éµ: Feature Mapã®å°å…¥**

ã‚‚ã— $\kappa(q, k) = \phi(q)^\top \phi(k)$ ã¨æ›¸ã‘ã‚‹ãªã‚‰:

$$
\text{Attention}(Q, K, V)_i = \frac{\sum_{j=1}^{N} \phi(q_i)^\top \phi(k_j) v_j}{\sum_{j=1}^{N} \phi(q_i)^\top \phi(k_j)}
$$

$$
= \frac{\phi(q_i)^\top \left(\sum_{j=1}^{N} \phi(k_j) v_j^\top\right)}{\phi(q_i)^\top \left(\sum_{j=1}^{N} \phi(k_j)\right)}
$$

ã“ã“ã§é‡è¦ãªã®ã¯ã€**å’Œã®é †åºã‚’å…¥ã‚Œæ›¿ãˆãŸ**ã“ã¨ã :

- Before: $\sum_j (\phi(q_i)^\top \phi(k_j)) v_j$ â†’ $O(N^2 d)$ (å„$i$ã«ã¤ã„ã¦$N$å›ã®å†…ç©)
- After: $\phi(q_i)^\top \left(\sum_j \phi(k_j) v_j^\top\right)$ â†’ $O(Nd^2)$ (å’Œã‚’å…ˆã«è¨ˆç®—ã€å„$i$ã¯1å›ã®å†…ç©)

$d \ll N$ ãªã‚‰ã€$O(Nd^2) \ll O(N^2 d)$ã€‚

**3.4.2 Performer (FAVOR+)** [^11]

Performer [^11] ã¯ã€**ãƒ©ãƒ³ãƒ€ãƒ ç‰¹å¾´è¿‘ä¼¼**ã§ $\phi$ ã‚’æ§‹ç¯‰ã™ã‚‹:

$$
\kappa(q, k) = \exp(q^\top k) \approx \phi(q)^\top \phi(k)
$$

ã“ã“ã§:

$$
\phi(x) = \frac{1}{\sqrt{M}} \left[\exp\left(w_1^\top x - \frac{\|x\|^2}{2}\right), \ldots, \exp\left(w_M^\top x - \frac{\|x\|^2}{2}\right)\right]
$$

$w_1, \ldots, w_M \sim \mathcal{N}(0, I_d)$ ã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ã€‚

**ç†è«–çš„ä¿è¨¼**: $M$ ãŒååˆ†å¤§ãã„ã¨ãã€$\mathbb{E}[\phi(q)^\top \phi(k)] = \exp(q^\top k)$ã€‚

è¨ˆç®—é‡:

$$
O(NMd + NMd) = O(NMd)
$$

$M \ll N$ (å…¸å‹çš„ã«$M=256$) ãªã‚‰ã€$O(Nd)$ ã«å‰Šæ¸›ã€‚

**3.4.3 Gated Linear Attention (GLA)** [^12]

**GLA** (2023) ã¯ã€Linear Attentionã« **Gating** ã‚’è¿½åŠ :

$$
\text{GLA}(Q, K, V)_i = \frac{\sum_{j=1}^{i} g_j \cdot \phi(q_i)^\top \phi(k_j) v_j}{\sum_{j=1}^{i} g_j \cdot \phi(q_i)^\top \phi(k_j)}
$$

ã“ã“ã§ $g_j = \sigma(\text{gate}(k_j))$ = å­¦ç¿’å¯èƒ½ãªã‚²ãƒ¼ãƒˆã€‚

**åŠ¹æœ**: GateãŒä¸è¦ãªæƒ…å ±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° â†’ è¡¨ç¾åŠ›å‘ä¸Šã€‚

è¨ˆç®—é‡: ä¾ç„¶ $O(Nd^2)$ã€‚

**3.4.4 Linear Attention ã®ç†è«–çš„é™ç•Œ**

Linear Attentionã¯é«˜é€Ÿã ãŒã€è¿‘ä¼¼èª¤å·®ãŒã‚ã‚‹ã€‚ã“ã®é™ç•Œã‚’æ•°å­¦çš„ã«ç†è§£ã—ã‚ˆã†ã€‚

**å®šç† (Linear Attention ã®è¿‘ä¼¼èª¤å·®)**:

$\phi$ ãŒ $M$ æ¬¡å…ƒã®ãƒ©ãƒ³ãƒ€ãƒ ç‰¹å¾´å†™åƒã§ã€$\mathbb{E}[\phi(q)^\top \phi(k)] = \kappa(q, k) = \exp(q^\top k)$ ã‚’æº€ãŸã™ã¨ãã€Linear Attentionã®å‡ºåŠ› $\hat{O}$ ã¨çœŸã® Softmax Attention ã®å‡ºåŠ› $O$ ã®èª¤å·®ã¯:

$$
\mathbb{E}\left[\|\hat{O}_i - O_i\|^2\right] = O\left(\frac{d}{M}\right)
$$

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

1. ãƒ©ãƒ³ãƒ€ãƒ ç‰¹å¾´è¿‘ä¼¼ã®åˆ†æ•£:
   $$
   \text{Var}[\phi(q)^\top \phi(k)] = O\left(\frac{1}{M}\right)
   $$

2. Attentioné‡ã¿ã®èª¤å·®ä¼æ’­:
   $$
   \left|\frac{\phi(q)^\top \phi(k)}{\sum_j \phi(q)^\top \phi(k_j)} - \frac{\exp(q^\top k)}{\sum_j \exp(q^\top k_j)}\right| = O\left(\sqrt{\frac{d}{M}}\right)
   $$

3. å‡ºåŠ›èª¤å·®:
   $$
   \|\hat{O}_i - O_i\| \leq \sum_j |w_j - \hat{w}_j| \cdot \|v_j\| = O\left(\sqrt{\frac{d}{M}}\right)
   $$

**å®Ÿç”¨çš„å«æ„**: $M \geq 10d$ ãªã‚‰ç›¸å¯¾èª¤å·® <10%ã€‚å…¸å‹çš„ã« $M=256$ for $d=64$ â†’ ç›¸å¯¾èª¤å·® ~6%ã€‚

**3.4.5 Performer vs GLA ã®æ¯”è¼ƒ**

| é …ç›® | Performer (FAVOR+) | GLA |
|:-----|:-------------------|:----|
| ç‰¹å¾´å†™åƒ | ãƒ©ãƒ³ãƒ€ãƒ  (å›ºå®š) | ãƒ©ãƒ³ãƒ€ãƒ  + Gating (å­¦ç¿’å¯èƒ½) |
| è¨ˆç®—é‡ | $O(NMd)$ | $O(NMd)$ |
| è¡¨ç¾åŠ› | ä¸­ | é«˜ (Gatingã§æŸ”è»Ÿæ€§) |
| è¨“ç·´å®‰å®šæ€§ | é«˜ | ä¸­ (Gateã®å­¦ç¿’ãŒä¸å®‰å®šãªå ´åˆ) |
| å®Ÿè£…è¤‡é›‘åº¦ | ä½ | ä¸­ |

**çµè«–**: ã‚¿ã‚¹ã‚¯ã®æ€§è³ªã«å¿œã˜ã¦é¸æŠã€‚é«˜é€Ÿå„ªå…ˆãªã‚‰ Performerã€å“è³ªå„ªå…ˆãªã‚‰ GLAã€‚

**3.4.6 Linear Attention ã® Causal Masking**

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ä½ç½® $i$ ã¯æœªæ¥ã®ä½ç½® $j > i$ ã‚’è¦‹ã¦ã¯ã„ã‘ãªã„ (Causal Masking)ã€‚

Standard Attention ã§ã¯ä¸‹ä¸‰è§’ãƒã‚¹ã‚¯:

$$
\text{CausalAttention}(Q, K, V)_i = \sum_{j=1}^{i} \text{softmax}\left(\frac{q_i k_j^\top}{\sqrt{d}}\right) v_j
$$

Linear Attention ã§ã¯ã€**é †åºã‚’å¤‰ãˆãŸç´¯ç©å’Œ**ã§å®Ÿç¾:

$$
\text{CausalLinearAttention}(Q, K, V)_i = \frac{\phi(q_i)^\top S_i}{{\phi(q_i)^\top z_i}}
$$

ã“ã“ã§:

$$
S_i = \sum_{j=1}^{i} \phi(k_j) v_j^\top, \quad z_i = \sum_{j=1}^{i} \phi(k_j)
$$

$S_i, z_i$ ã‚’ **æ¼¸åŒ–å¼ã§æ›´æ–°**:

$$
S_i = S_{i-1} + \phi(k_i) v_i^\top, \quad z_i = z_{i-1} + \phi(k_i)
$$

åˆæœŸæ¡ä»¶: $S_0 = \mathbf{0}, z_0 = \mathbf{0}$ã€‚

**ã“ã‚Œã«ã‚ˆã‚Šã€æ¨è«–æ™‚ã« O(1) per token ã§ç”Ÿæˆå¯èƒ½ã€‚**

```julia
function causal_linear_attention(Q::Matrix{T}, K::Matrix{T}, V::Matrix{T}) where T
    N, d = size(Q)

    # Feature maps
    Ï•_Q = max.(Q, zero(T)) .+ T(1)
    Ï•_K = max.(K, zero(T)) .+ T(1)

    # Initialize cumulative states
    S = zeros(T, d, d)  # (d, d) matrix
    z = zeros(T, d)      # (d,) vector

    O = zeros(T, N, d)

    for i in 1:N
        # Update cumulative states
        S += Ï•_K[i, :] * V[i, :]'
        z += Ï•_K[i, :]

        # Compute output for position i
        numerator = Ï•_Q[i, :]' * S
        denominator = Ï•_Q[i, :]' * z
        O[i, :] = numerator[:] ./ (denominator + T(1e-6))
    end

    return O
end
```

**æ¨è«–æ™‚ã®åŠ¹ç‡**: å„ã‚¹ãƒ†ãƒƒãƒ—ã§ $S, z$ ã‚’æ›´æ–°ã™ã‚‹ã ã‘ â†’ $O(d^2)$ per token â†’ ç³»åˆ—å…¨ä½“ã§ $O(Nd^2)$ã€‚

### 3.5 Ring Attention â€” è¶…é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æ•£å‡¦ç†

**Ring Attention** [^13] ã¯ã€**Blockwiseä¸¦åˆ—**ã§æ•°ç™¾ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ‰±ã†:

- ç³»åˆ—ã‚’ $P$ å€‹ã®ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²
- å„ãƒ‡ãƒã‚¤ã‚¹ãŒ1ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ‹…å½“
- ãƒªãƒ³ã‚°çŠ¶ã«é€šä¿¡ã—ãªãŒã‚‰Attentionã‚’è¨ˆç®—

è¨ˆç®—é‡: å„ãƒ‡ãƒã‚¤ã‚¹ã§ $O((N/P)^2 d)$ â†’ å…¨ä½“ã§ $O(N^2 d / P)$ã€‚

ãƒ¡ãƒ¢ãƒª: å„ãƒ‡ãƒã‚¤ã‚¹ã§ $O((N/P)^2)$ â†’ å…¨GPUã§ $O(N^2 / P)$ã€‚

**é€šä¿¡é‡**: $O(N d)$ (K, V ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒªãƒ³ã‚°çŠ¶ã«è»¢é€)ã€‚

### 3.6 Mixture of Experts (MoE) â€” Sparse Activationã§è¨ˆç®—åŠ¹ç‡åŒ–

**MoEã®åŸç†**: å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯ **ä¸€éƒ¨ã®Expertã ã‘ã‚’æ´»æ€§åŒ–**ã™ã‚‹ â†’ Sparse Activationã€‚

$$
y = \sum_{i=1}^{E} G(x)_i \cdot \text{Expert}_i(x)
$$

ã“ã“ã§ $G(x) = \text{softmax}(x W_g)$ = Routing weightsã€‚

**Top-k Routing**: $G(x)$ ã®ä¸Šä½ $k$ å€‹ã®Expertã ã‘ã‚’ä½¿ã†:

$$
y = \sum_{i \in \text{TopK}(G(x))} G(x)_i \cdot \text{Expert}_i(x)
$$

è¨ˆç®—é‡: å…¨ExpertãŒ $O(Ed \cdot d_{\text{ff}})$ ã®ã¨ã“ã‚ã€Top-k ã§ $O(kd \cdot d_{\text{ff}})$ ã«å‰Šæ¸›ã€‚$k \ll E$ ãªã‚‰å¤§å¹…å‰Šæ¸›ã€‚

**3.6.1 Switch Transformer** [^14]

Switch Transformer [^14] ã¯ **Top-1 routing** (k=1) ã‚’ä½¿ã†:

- å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯1ã¤ã®Expertã ã‘ã‚’ä½¿ã† â†’ æœ€ã‚‚Sparse
- Load Balancing: å„ExpertãŒå‡ç­‰ã«ä½¿ã‚ã‚Œã‚‹ã‚ˆã†è£œåŠ©æå¤±

**3.6.2 DeepSeek-MoE** [^15]

DeepSeek-MoE [^15] ã¯ **Fine-grained routing**:

- å„Expertã‚’ã•ã‚‰ã«å°ã•ãªã€Œsub-expertã€ã«åˆ†å‰²
- Top-k ã‚’ sub-expert ãƒ¬ãƒ™ãƒ«ã§é¸æŠ â†’ ã‚ˆã‚ŠæŸ”è»Ÿ

**3.6.3 MoE ã®æ•°å­¦çš„è©³ç´°**

**ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã®å®šå¼åŒ–**:

æ¨™æº–çš„ãªMoEã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯:

$$
G(x) = \text{softmax}(x W_g)
$$

ã“ã“ã§ $W_g \in \mathbb{R}^{d \times E}$ ã¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é‡ã¿è¡Œåˆ—ã€‚

**Top-k ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**:

$$
\text{TopK}(G(x), k) = \{i \in [E] : G(x)_i \text{ is in top-}k\}
$$

å‡ºåŠ›:

$$
y = \sum_{i \in \text{TopK}(G(x), k)} \frac{G(x)_i}{\sum_{j \in \text{TopK}(G(x), k)} G(x)_j} \cdot \text{Expert}_i(x)
$$

**Load Balancing Loss**:

å„ExpertãŒå‡ç­‰ã«ä½¿ã‚ã‚Œã‚‹ã‚ˆã†ã€è£œåŠ©æå¤±ã‚’è¿½åŠ :

$$
\mathcal{L}_{\text{balance}} = \alpha \cdot \text{CV}\left(\sum_{x \in \text{batch}} \mathbb{1}[i \in \text{TopK}(G(x), k)]\right)^2
$$

ã“ã“ã§ $\text{CV}$ = å¤‰å‹•ä¿‚æ•° (coefficient of variation):

$$
\text{CV}(f) = \frac{\text{std}(f)}{\text{mean}(f)}
$$

$\alpha$ = ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°å¼·åº¦ (å…¸å‹çš„ã« 0.01-0.1)ã€‚

**Switch Transformer ã®ç°¡ç´ åŒ–**:

Switch Transformer [^14] ã¯ $k=1$ (Top-1) + capacity factor:

- å„Expertã«æœ€å¤§å®¹é‡ (capacity) ã‚’è¨­å®š
- å®¹é‡ã‚’è¶…ãˆãŸãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€Œoverflowã€ã¨ã—ã¦åˆ¥å‡¦ç† (ã¾ãŸã¯ç„¡è¦–)
- å®¹é‡ = $\frac{\text{batch\_size} \times \text{seq\_len}}{E} \times C$, $C$ = capacity factor (1.0-1.5)

**æ•°å¼**:

$$
\text{Expert}_i \text{ processes } = \left\{x : \arg\max_j G(x)_j = i\right\} \cap \text{top-}C_i\text{-scoring}
$$

**3.6.4 MoE ã®è¨“ç·´ã®ä¸å®‰å®šæ€§**

MoEè¨“ç·´ã§é »ç™ºã™ã‚‹å•é¡Œ:

1. **Expert collapse**: ä¸€éƒ¨ã®Expertã ã‘ãŒä½¿ã‚ã‚Œã€ä»–ãŒæ­»ã¬
2. **ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ä¸å®‰å®š**: å‹¾é…ãŒå¤§ãããƒãƒƒãƒã”ã¨ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãŒæ¿€å¤‰
3. **è² è·ä¸å‡è¡¡**: ä¸€éƒ¨ã®Expertã«è² è·ãŒé›†ä¸­ â†’ è¨ˆç®—åŠ¹ç‡ä½ä¸‹

**å¯¾ç­–**:

- **Auxiliary loss**: Load balancing loss ã‚’è¿½åŠ 
- **Expert regularization**: Experté‡ã¿ã«æ­£å‰‡åŒ– (weight decay)
- **Noise injection**: ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ãƒã‚¤ã‚ºè¿½åŠ  (exploration)
  $$
  G(x) = \text{softmax}(x W_g + \epsilon \cdot \text{noise}), \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
  $$
- **Dropout on routing**: ç¢ºç‡çš„ã«Expertã‚’ç„¡åŠ¹åŒ– â†’ å†—é•·æ€§ç¢ºä¿

**3.6.5 MoE ã¨ Attention ã®çµ±åˆ**

**Sparse Mixture of Experts (SMoE)**: å„å±¤ã§Attentionã¨MoEã‚’çµ„ã¿åˆã‚ã›:

$$
\text{Layer}(x) = \text{Attention}(x) + \text{MoE-FFN}(x)
$$

Attentionå±¤ã¯å¯† (å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½¿ç”¨)ã€FFNå±¤ã¯Sparse (Top-k Experts)ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡**:

- ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: $N_{\text{attn}} + E \cdot N_{\text{expert}}$
- ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: $N_{\text{attn}} + k \cdot N_{\text{expert}}$

ä¾‹: DeepSeek-V3 (671B total, 37B active) â†’ $k/E = 37/671 \approx 5.5\%$ ã®ã¿ä½¿ç”¨ã€‚

**3.6.6 MoE ã®ãƒ¡ãƒ¢ãƒªã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**

**ãƒ¡ãƒ¢ãƒª**: å…¨Expertã‚’ä¿æŒ â†’ GPUãƒ¡ãƒ¢ãƒªå¤§ã€‚åˆ†æ•£è¨“ç·´å¿…é ˆã€‚

**ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: Expertä¸¦åˆ—åŒ– + ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—:

- **Expertä¸¦åˆ—**: å„GPUãŒç•°ãªã‚‹Expertã‚’æ‹…å½“
- **Tokenä¸¦åˆ—**: ãƒˆãƒ¼ã‚¯ãƒ³ã‚’Expertã”ã¨ã«æŒ¯ã‚Šåˆ†ã‘ã€ä¸¦åˆ—å‡¦ç†
- **é€šä¿¡**: All-to-Allé€šä¿¡ (ãƒˆãƒ¼ã‚¯ãƒ³ã‚’Expertã«é€ã‚‹) â†’ é€šä¿¡å¾‹é€Ÿ

**é€šä¿¡é‡ã®è¨ˆç®—**:

å„ãƒˆãƒ¼ã‚¯ãƒ³ $x$ ã‚’ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å…ˆExpertã«é€ã‚‹:

$$
\text{é€šä¿¡é‡} = O(B \cdot L \cdot d), \quad B = \text{batch size}, \quad L = \text{seq len}
$$

é«˜é€Ÿã‚¤ãƒ³ã‚¿ãƒ¼ã‚³ãƒã‚¯ãƒˆ (InfiniBand, NVLink) å¿…é ˆã€‚

:::message
**é€²æ—: 50% å®Œäº†** æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å‰åŠã‚¯ãƒªã‚¢ã€‚FlashAttention, Sparse Attention, Linear Attention, Ring Attention, MoE ã®æ•°å­¦ã‚’å®Œå…¨å°å‡ºã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ”¬ æœ€æ–°ç ”ç©¶å‹•å‘ï¼ˆ2024-2025ï¼‰

Sparse Attentionã¨Linear Attentionã®ç ”ç©¶ã¯2024-2025å¹´ã«çˆ†ç™ºçš„é€²å±•ã‚’é‚ã’ãŸã€‚

### FlashAttention ã®é€²åŒ–

**FlashAttention: Fast and Memory-Efficient Exact Attention** (arXiv:2205.14135, 2022)
- **æ ¸å¿ƒ**: IO-aware algorithm â€” HBMâ†”SRAMé–“ã®èª­ã¿æ›¸ãå›æ•°ã‚’å‰Šæ¸›
- **æ‰‹æ³•**: Tiling + recomputation in backward pass
- **æ€§èƒ½**: GPT-2ã§7.6å€é«˜é€ŸåŒ–ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç·šå½¢
- **å½±éŸ¿**: äº‹å®Ÿä¸Šã®æ¥­ç•Œæ¨™æº–ï¼ˆPyTorch/JAXçµ±åˆï¼‰
@[card](https://arxiv.org/abs/2205.14135)

### Block Sparse FlashAttention

**Block Sparse FlashAttention (BSFA)** (arXiv:2512.07011, December 2025)
- **æ‰‹æ³•**: ãƒ–ãƒ­ãƒƒã‚¯ãƒ¬ãƒ™ãƒ«ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ + ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–¾å€¤ã§top-ké¸æŠ
- **ä»•çµ„**: ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®æœ€å¤§ã‚¹ã‚³ã‚¢ã‚’é–¾å€¤ã¨æ¯”è¼ƒã€ç´„50%ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
- **æ€§èƒ½**: é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¨è«–ã§2.1å€é«˜é€ŸåŒ–ã€ç²¾åº¦ãƒ­ã‚¹<1%
- **å®Ÿè£…**: Tritonã‚«ãƒ¼ãƒãƒ«å…¬é–‹
@[card](https://arxiv.org/html/2512.07011)

### SeerAttention: å­¦ç¿’å¯èƒ½ãªã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³

**SeerAttention: Learning Intrinsic Sparse Attention** (arXiv:2410.13276, October 2024)
- **æ ¸å¿ƒ**: LLMè‡ªèº«ã‹ã‚‰ãƒ–ãƒ­ãƒƒã‚¯ãƒ¬ãƒ™ãƒ«æ³¨æ„ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’ç›´æ¥å­¦ç¿’
- **æ‰‹æ³•**: å­¦ç¿’å¯èƒ½ãªã‚²ãƒ¼ãƒˆã§é‡è¦ãƒ–ãƒ­ãƒƒã‚¯ã‚’é¸æŠçš„ã«æ´»æ€§åŒ–
- **çµæœ**: GPUä¸Šã§é¡•è‘—ãªé«˜é€ŸåŒ–ã€é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆpre-fillingã§ç²¾åº¦å‘ä¸Š
- **ç†è«–**: æ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ¬è³ªçš„æ§‹é€ ã‚’ãƒ¢ãƒ‡ãƒ«ãŒç™ºè¦‹
@[card](https://arxiv.org/abs/2410.13276)

### Native Sparse Attention: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«æœ€é©åŒ–

**Native Sparse Attention (NSA)** (arXiv:2502.11089, February 2025)
- **é©æ–°**: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ãƒ©ã‚¤ãƒ³ + ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¹ãƒ‘ãƒ¼ã‚¹æ¼”ç®—
- **æ€§èƒ½**: 64kæ–‡è„ˆé•·ã§å‰æ–¹9.0å€ã€å¾Œæ–¹6.0å€é«˜é€ŸåŒ–ï¼ˆæ–‡è„ˆé•·å¢—åŠ ã§åŠ é€Ÿåº¦çš„å‘ä¸Šï¼‰
- **å®Ÿè£…**: CUDAã‚«ãƒ¼ãƒãƒ«ç›´æ¥å®Ÿè£…ã€ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–
- **ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ**: DeepSeek-V3ã§å®Ÿæˆ¦æŠ•å…¥
@[card](https://arxiv.org/pdf/2502.11089)

### FlashInfer: ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªAttentionã‚¨ãƒ³ã‚¸ãƒ³

**FLASHINFER: Efficient and Customizable Attention Engine** (arXiv:2501.01005, January 2025)
- **ç‰¹å¾´**: ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯èƒ½ãªAttentionã‚«ãƒ¼ãƒãƒ«ã€å‹•çš„ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œ
- **API**: çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§å¤šæ§˜ãªAttention variant
- **æ€§èƒ½**: FlashAttention-2ã¨åŒç­‰é€Ÿåº¦ã€æŸ”è»Ÿæ€§10å€
@[card](https://www.arxiv.org/pdf/2501.01005)

### åŠ¹ç‡çš„Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ã‚µãƒ¼ãƒ™ã‚¤

**Efficient Attention Mechanisms for LLMs: A Survey** (arXiv:2507.19595, 2025)
- **ç¶²ç¾…**: 100ä»¥ä¸Šã®Attentionå¤‰ç¨®ã‚’åˆ†é¡ï¼ˆSparse, Linear, Low-rank, Hybridï¼‰
- **ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**: çµ±ä¸€è©•ä¾¡ï¼ˆé€Ÿåº¦, ãƒ¡ãƒ¢ãƒª, ç²¾åº¦, é•·æ–‡å¯¾å¿œï¼‰
- **çµè«–**: ã‚¿ã‚¹ã‚¯ä¾å­˜ã®æœ€é©é¸æŠã€å˜ä¸€æœ€å¼·æ‰‹æ³•ãªã—
@[card](https://arxiv.org/html/2507.19595v1)

### æœ€æ–°æˆæœã®æŠ€è¡“æ¯”è¼ƒè¡¨

| æ‰‹æ³• | è¨ˆç®—é‡ | ãƒ¡ãƒ¢ãƒª | ç²¾åº¦ | å®Ÿè£…é›£æ˜“åº¦ | å®Ÿæˆ¦æŠ•å…¥ |
|:-----|:------|:------|:-----|:---------|:--------|
| FlashAttention-2 | O(NÂ²) | O(N) | 100% | ä½ | å…¨ä¸»è¦LLM |
| BSFA | O(0.5NÂ²) | O(0.5NÂ²) | 99% | ä¸­ | ç ”ç©¶æ®µéš |
| SeerAttention | O(Î±NÂ²) Î±<1 | O(Î±NÂ²) | 99.5% | ä¸­ | ç ”ç©¶æ®µéš |
| Native Sparse | O(Î²NÂ²) Î²<<1 | O(Î²NÂ²) | 98% | é«˜ | DeepSeek-V3 |
| FlashInfer | O(NÂ²) | O(N) | 100% | ä½ | å®Ÿç”¨åŒ–é€²è¡Œä¸­ |

**Î±ã¯å­¦ç¿’ã•ã‚ŒãŸã‚¹ãƒ‘ãƒ¼ã‚¹ç‡ã€Î²ã¯ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚¹ãƒ‘ãƒ¼ã‚¹ç‡**

### ç†è«–ã¨å®Ÿè£…ã®æœ€æ–°ã‚®ãƒ£ãƒƒãƒ—

| é …ç›® | ç†è«–çš„æˆæœï¼ˆ2024-2025ï¼‰ | å®Ÿè£…ã§ã®èª²é¡Œ |
|:-----|:--------------------|:----------|
| é©å¿œçš„ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ | ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ | è¨“ç·´ã‚³ã‚¹ãƒˆå¢—å¤§ |
| ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ– | 9å€é«˜é€ŸåŒ–ï¼ˆNSAï¼‰ | GPUä¸–ä»£ä¾å­˜ |
| å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³é¸æŠ | ã‚¿ã‚¹ã‚¯ã”ã¨ã«æœ€é©Attention | ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ |
| é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ | æ•°ç™¾ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³å¯¾å¿œç†è«– | é€šä¿¡å¾‹é€Ÿï¼ˆåˆ†æ•£è¨­å®šï¼‰ |
| ç²¾åº¦-é€Ÿåº¦ãƒˆãƒ¬ãƒ¼ãƒ‰ | ç†è«–çš„ä¸‹ç•Œè¨¼æ˜ | å®Ÿã‚¿ã‚¹ã‚¯ã§ã®æ¤œè¨¼ä¸è¶³ |

### å®Ÿè£…è€…ã®ãŸã‚ã®é¸æŠã‚¬ã‚¤ãƒ‰

**ã‚·ãƒŠãƒªã‚ªåˆ¥æ¨å¥¨:**

| ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ | æ¨å¥¨æ‰‹æ³• | ç†ç”± |
|:-----------|:--------|:-----|
| æ±ç”¨LLMæ¨è«–ï¼ˆ<8k tokensï¼‰ | FlashAttention-2 | ç²¾åº¦100%ã€æ¥­ç•Œæ¨™æº– |
| é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¨è«–ï¼ˆ64k+ï¼‰ | Native Sparse Attention | æ–‡è„ˆé•·ã§ã‚¹ã‚±ãƒ¼ãƒ« |
| è¨“ç·´æ™‚ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ | FlashAttention-2 + Gradient Checkpointing | ãƒ¡ãƒ¢ãƒªO(N) |
| ã‚«ã‚¹ã‚¿ãƒ Attentionãƒ‘ã‚¿ãƒ¼ãƒ³ | FlashInfer | ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯èƒ½ |
| ç ”ç©¶ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚° | SeerAttention | å­¦ç¿’å¯èƒ½ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ |
| è¶…é•·æ–‡ï¼ˆ1M+ tokensï¼‰ | Ring Attention | åˆ†æ•£ä¸¦åˆ—å¯¾å¿œ |
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡é‡è¦– | MoE + Sparse Attention | è¨ˆç®—ã¨ãƒ¡ãƒ¢ãƒªåˆ†é›¢ |

**å®Ÿè£…ã®å„ªå…ˆé †ä½ï¼ˆ2025å¹´æ™‚ç‚¹ï¼‰:**

1. **ã¾ãšFlashAttention-2ã‚’å°å…¥** â€” ç„¡æ¡ä»¶ã§2-3å€é«˜é€ŸåŒ–
2. **é•·æ–‡ãªã‚‰+Native Sparse** â€” 64kä»¥ä¸Šã§çœŸä¾¡ç™ºæ®
3. **ãƒ¡ãƒ¢ãƒªå³ã—ã„ãªã‚‰+Gradient Checkpointing** â€” è¨“ç·´æ™‚ã®ã¿
4. **ã‚«ã‚¹ã‚¿ãƒ ãŒå¿…è¦ãªã‚‰ FlashInfer** â€” æŸ”è»Ÿæ€§æœ€é«˜
5. **è¶…é•·æ–‡ãªã‚‰ Ring Attention** â€” åˆ†æ•£ã‚¤ãƒ³ãƒ•ãƒ©å‰æ

**ãƒ©ã‚¤ãƒ–ãƒ©ãƒªé¸å®š:**

```python
# PyTorch: FlashAttention-2 çµ±åˆï¼ˆtorch >= 2.0ï¼‰
import torch.nn.functional as F
out = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # è‡ªå‹•ã§Flashé¸æŠ

# Triton: ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«
import triton
# Block Sparse FlashAttention ã®Tritonå®Ÿè£…ãŒå…¬é–‹ä¸­

# JAX: Pallas ã§FlashAttention
from jax.experimental import pallas
# FlashAttention-2 equivalent on TPU

# Rust: burn/candle
use candle_nn::ops::flash_attn;
let out = flash_attn(&q, &k, &v, scale, is_causal)?;
```

### MoEã®å®Ÿè£…è©³ç´° â€” è² è·åˆ†æ•£ã®æ•°å­¦

**Load Balancing Lossã®å®Œå…¨å°å‡º:**

MoEã§å„Expertã®ä½¿ç”¨é »åº¦ã‚’$f_i = \frac{1}{N} \sum_{n=1}^{N} \mathbb{1}[i \in \text{TopK}(G(x_n))]$ã¨ã™ã‚‹ã€‚

ç†æƒ³çš„ã«ã¯å…¨ExpertãŒå‡ç­‰ã«ä½¿ã‚ã‚Œã‚‹: $f_i = \frac{k}{E}$ for all $i$ã€‚

**Load Balancing Loss (Switch Transformer 2021):**

$$
\mathcal{L}_{\text{balance}} = E \cdot \sum_{i=1}^{E} f_i \cdot P_i
$$

ã“ã“ã§$P_i = \frac{1}{N} \sum_{n=1}^{N} G(x_n)_i$ï¼ˆExpert $i$ã¸ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç¢ºç‡ã®å¹³å‡ï¼‰ã€‚

**ç›´æ„Ÿ**: $f_i$ï¼ˆå®Ÿéš›ã®ä½¿ç”¨é »åº¦ï¼‰ã¨$P_i$ï¼ˆã‚½ãƒ•ãƒˆãªå‰²ã‚Šå½“ã¦ç¢ºç‡ï¼‰ã®ç©ã‚’æœ€å°åŒ– â†’ ä¸¡è€…ãŒä¹–é›¢ã™ã‚‹ã¨ãƒšãƒŠãƒ«ãƒ†ã‚£ã€‚

**å°å‡º**: å®Œå…¨ã«å‡ç­‰ãªã‚‰$f_i = P_i = \frac{k}{E}$ã§ã€Loss = $E \cdot E \cdot (\frac{k}{E})^2 = \frac{k^2}{E}$ï¼ˆå®šæ•°ï¼‰ã€‚

ä¸å‡è¡¡ãªã‚‰ã€ä¾‹ãˆã°1ã¤ã®ExpertãŒå…¨ã¦æ‹…å½“: $f_1 = 1, P_1 = 1, f_{i>1} = 0, P_{i>1} = 0$ â†’ Loss = $E \cdot 1 \cdot 1 = E \gg \frac{k^2}{E}$ã€‚

**å®Ÿè£… (PyTorch):**

```python
def load_balancing_loss(gate_logits, expert_indices, num_experts):
    """
    Args:
        gate_logits: (batch_size, seq_len, num_experts) â€” ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ­ã‚¸ãƒƒãƒˆ
        expert_indices: (batch_size, seq_len, top_k) â€” é¸ã°ã‚ŒãŸExpertã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        num_experts: int
    Returns:
        loss: float â€” Load balancing loss
    """
    # f_i: å®Ÿéš›ã®ä½¿ç”¨é »åº¦
    expert_mask = torch.zeros_like(gate_logits)
    expert_mask.scatter_(-1, expert_indices, 1.0)
    f = expert_mask.mean(dim=[0, 1])  # (num_experts,)

    # P_i: ã‚½ãƒ•ãƒˆãªå‰²ã‚Šå½“ã¦ç¢ºç‡
    gate_probs = F.softmax(gate_logits, dim=-1)
    P = gate_probs.mean(dim=[0, 1])  # (num_experts,)

    # Loss = E * sum(f_i * P_i)
    loss = num_experts * torch.sum(f * P)
    return loss

# Training
for batch in dataloader:
    logits, gate_logits, expert_indices = model(batch)
    task_loss = F.cross_entropy(logits, labels)
    balance_loss = load_balancing_loss(gate_logits, expert_indices, num_experts)
    total_loss = task_loss + alpha * balance_loss  # alpha = 0.01
    total_loss.backward()
```

**Capacity Factor ã®å®Ÿè£…:**

```python
def top_k_gating_with_capacity(gate_logits, k=2, capacity_factor=1.25):
    """Top-k routing with capacity constraint (Switch Transformer)"""
    batch_size, seq_len, num_experts = gate_logits.shape
    capacity = int((batch_size * seq_len / num_experts) * capacity_factor)

    # Top-k selection
    gate_probs = F.softmax(gate_logits, dim=-1)
    top_k_probs, top_k_indices = torch.topk(gate_probs, k, dim=-1)

    # Capacity enforcement
    expert_counts = torch.zeros(num_experts, device=gate_logits.device)
    expert_mask = torch.zeros_like(gate_logits)

    for i in range(batch_size * seq_len):
        for j in range(k):
            expert_id = top_k_indices.view(-1, k)[i, j]
            if expert_counts[expert_id] < capacity:
                expert_mask.view(-1, num_experts)[i, expert_id] = 1.0
                expert_counts[expert_id] += 1
            # else: overflow, token dropped

    return expert_mask, top_k_probs, top_k_indices
```

**DeepSeek-MoE ã® Fine-Grained Routing:**

å„Expertã‚’$M$å€‹ã®sub-expertã«åˆ†å‰²:

$$
\text{Expert}_i(x) = \sum_{m=1}^{M} w_{i,m} \cdot \text{SubExpert}_{i,m}(x)
$$

ã“ã“ã§$w_{i,m}$ã¯å­¦ç¿’å¯èƒ½ãªé‡ã¿ã€‚Top-kã‚’sub-expertãƒ¬ãƒ™ãƒ«ã§é¸æŠã€‚

**åˆ©ç‚¹**: ã‚ˆã‚Šç´°ã‹ã„ç²’åº¦ã§è¨ˆç®—è³‡æºã‚’é…åˆ† â†’ æŸ”è»Ÿæ€§å‘ä¸Šã€‚

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
