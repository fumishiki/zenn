---
title: "ç¬¬41å›: World Models & ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ç†è«–ğŸŒ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸŒ"
type: "tech"
topics: ["machinelearning", "deeplearning", "worldmodels", "julia", "jepa"]
published: true
---

# ç¬¬41å›: World Models & ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ç†è«– ğŸŒ

**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹ã¯"ç†è§£"ã ã£ãŸ**

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” 1ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹

ç¬¬40å›ã§Consistency Modelsã«ã‚ˆã‚‹1ã‚¹ãƒ†ãƒƒãƒ—é«˜é€Ÿç”Ÿæˆã‚’å®Ÿç¾ã—ãŸã€‚ã ãŒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çœŸã®ç›®çš„ã¯ä½•ã ã£ãŸã®ã‹ï¼Ÿ

å˜ã«ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã¯ãªã„ã€‚**ç’°å¢ƒã®æ§‹é€ ã‚’ç†è§£ã—ã€æœªæ¥ã‚’äºˆæ¸¬ã—ã€è¡Œå‹•ã®çµæœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ã“ã¨**ã ã€‚

```julia
# World Modelã®æœ¬è³ª: 1ãƒ•ãƒ¬ãƒ¼ãƒ  â†’ æœªæ¥ã®äºˆæ¸¬
using Lux, Random

# è¦³æ¸¬ x_t ã‹ã‚‰æ½œåœ¨è¡¨ç¾ z_t ã‚’æŠ½å‡º
encoder = Chain(Conv((3,3), 3 => 64, relu), AdaptiveMeanPool((1,1)), FlattenLayer())

# æ½œåœ¨ç©ºé–“ã§æ¬¡çŠ¶æ…‹ã‚’äºˆæ¸¬ (actionæ¡ä»¶ä»˜ã)
predictor = Dense(64 + 4 => 64, tanh)  # 4æ¬¡å…ƒaction space

# åˆæœŸè¦³æ¸¬
x = rand(Float32, 64, 64, 3, 1)
a = rand(Float32, 4, 1)  # action

# æ½œåœ¨çŠ¶æ…‹æŠ½å‡º â†’ actionæ¡ä»¶ä»˜ãäºˆæ¸¬
z = encoder(x, ps, st)[1]
z_next = predictor(vcat(z, a), ps_pred, st_pred)[1]

# å‡ºåŠ›: z_next âˆˆ â„^64 (predicted next latent state)
```

**ã“ã‚ŒãŒä½•ã‚’ã—ã¦ã„ã‚‹ã‹ï¼Ÿ**

1ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¦³æ¸¬$x_t$ã‚’æ½œåœ¨è¡¨ç¾$z_t$ã«åœ§ç¸®ã—ã€action $a_t$ã‚’ä¸ãˆã¦æ¬¡çŠ¶æ…‹$z_{t+1}$ã‚’äºˆæ¸¬ã™ã‚‹ã€‚

ãƒ”ã‚¯ã‚»ãƒ«ã¯ç”Ÿæˆã—ãªã„ã€‚**ä¸–ç•Œã®æ½œåœ¨æ§‹é€ ã‚’äºˆæ¸¬ã™ã‚‹ã€‚**

$$
z_{t+1} = f_\theta(z_t, a_t)
$$

ã“ã‚ŒãŒWorld Modelã®æ•°å­¦ã ã€‚

:::message
**é€²æ—**: å…¨ä½“ã®3%å®Œäº†ã€‚Consistency Modelsã§1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆã‚’å®Ÿç¾ã—ãŸãŒã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çœŸã®ç›®çš„ã¯ã€Œç†è§£ã€ã ã£ãŸã€‚ç’°å¢ƒã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ç†è«–ã¸ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” World Modelsã®3ã¤ã®é¡”

### 1.1 ç”Ÿæˆ vs ç†è§£ vs ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯3ã¤ã®ãƒ¬ãƒ™ãƒ«ã«åˆ†é¡ã§ãã‚‹:

| ãƒ¬ãƒ™ãƒ« | ç›®çš„ | å…¥å‡ºåŠ› | ä»£è¡¨æ‰‹æ³• |
|:------|:-----|:------|:---------|
| **Level 1: ç”Ÿæˆ** | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ« | $p(x)$ | VAE, GAN, Diffusion |
| **Level 2: æ¡ä»¶ä»˜ãç”Ÿæˆ** | æ¡ä»¶ã‹ã‚‰ç”Ÿæˆ | $p(x|c)$ | LDM, CFG |
| **Level 3: World Models** | **ç’°å¢ƒã®ç†è§£+äºˆæ¸¬+ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | $p(x_{t+1}|x_{\leq t}, a_t)$ | JEPA, V-JEPA, Transfusion |

World Modelsã¯**è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã§ãã‚‹**æœ€é«˜ãƒ¬ãƒ™ãƒ«ã ã€‚

### 1.2 JEPAã®3å¤‰ç¨®ã‚’å‹•ã‹ã™

```julia
# I-JEPA: ç”»åƒã®ä¸€éƒ¨ã‹ã‚‰ä»–éƒ¨åˆ†ã‚’äºˆæ¸¬
# Input: masked image patches
x_context = x[:, :, 1:32, :]  # å·¦åŠåˆ†
x_target_mask = [33:64]       # å³åŠåˆ†ã‚’ãƒã‚¹ã‚¯

# Context encoder â†’ Predictor â†’ Target prediction
z_context = context_encoder(x_context, ps_ctx, st_ctx)[1]
z_pred = predictor(z_context, mask_tokens, ps_pred, st_pred)[1]

# âŒ ãƒ”ã‚¯ã‚»ãƒ«ã‚’äºˆæ¸¬ã—ãªã„
# âœ… æ½œåœ¨è¡¨ç¾ã‚’äºˆæ¸¬ã™ã‚‹
```

```julia
# V-JEPA: å‹•ç”»ã®ä¸€éƒ¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰æœªæ¥ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ½œåœ¨è¡¨ç¾ã‚’äºˆæ¸¬
# Input: video sequence [B, T, H, W, C]
video = rand(Float32, 1, 16, 64, 64, 3)  # 16 frames
context_frames = video[:, 1:8, :, :, :]   # å‰åŠ8ãƒ•ãƒ¬ãƒ¼ãƒ 
target_frames = video[:, 9:16, :, :, :]   # å¾ŒåŠ8ãƒ•ãƒ¬ãƒ¼ãƒ 

# Context encoder â†’ Temporal predictor
z_ctx_video = video_encoder(context_frames, ps_v, st_v)[1]
z_pred_video = temporal_predictor(z_ctx_video, ps_tp, st_tp)[1]
```

```julia
# Transfusion: ãƒ†ã‚­ã‚¹ãƒˆ(AR) + ç”»åƒ(Diffusion) ã‚’çµ±ä¸€ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†
# Text: autoregressive (next token prediction)
# Image: diffusion (denoising)

# ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³
text_tokens = [101, 2054, 2003]  # "What is"

# ç”»åƒãƒ‘ãƒƒãƒ (continuous vectors)
image_patches = rand(Float32, 512, 16)  # 16 patches Ã— 512 dim

# Transfusionã®çµ±ä¸€å‡¦ç†
# Text: next token prediction loss
loss_text = cross_entropy(model(text_tokens), text_tokens[2:end])

# Image: diffusion loss
t = rand(1:1000)
noise = randn(size(image_patches))
x_t = sqrt(Î±[t]) * image_patches + sqrt(1 - Î±[t]) * noise
loss_image = mse(model(x_t, t), noise)

# ç·åˆloss
loss = loss_text + loss_image
```

### 1.3 World Modelsã®å¿œç”¨é ˜åŸŸ

| å¿œç”¨ | ç›®çš„ | World Modelã®å½¹å‰² |
|:-----|:-----|:-----------------|
| **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹** | ç’°å¢ƒæ“ä½œ | è¡Œå‹•çµæœã®äº‹å‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ |
| **è‡ªå‹•é‹è»¢** | äºˆæ¸¬åˆ¶å¾¡ | ä»–è»Šãƒ»æ­©è¡Œè€…ã®æœªæ¥è»Œé“äºˆæ¸¬ |
| **å¼·åŒ–å­¦ç¿’** | ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚° | Model-based RL (MuZero, Dreamer) |
| **ç§‘å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | ç‰©ç†æ³•å‰‡å­¦ç¿’ | å¾®åˆ†æ–¹ç¨‹å¼ã‚’å­¦ç¿’ã§è¿‘ä¼¼ |

:::details PyTorchã¨ã®å¯¾å¿œï¼ˆå‚è€ƒï¼‰
```python
# PyTorchç‰ˆ JEPA predictor
import torch.nn as nn

class JEPAPredictor(nn.Module):
    def __init__(self, dim=768, num_heads=12):
        super().__init__()
        self.cross_attn = nn.MultiheadAttention(dim, num_heads)
        self.ffn = nn.Sequential(
            nn.Linear(dim, 4 * dim),
            nn.GELU(),
            nn.Linear(4 * dim, dim)
        )

    def forward(self, context, mask_tokens):
        # Cross-attention: mask_tokens attend to context
        pred, _ = self.cross_attn(mask_tokens, context, context)
        pred = self.ffn(pred)
        return pred
```

Juliaã§ã¯å‹ã‚·ã‚¹ãƒ†ãƒ ã§ã“ã‚Œã‚’è‡ªç„¶ã«è¡¨ç¾ã§ãã‚‹ã€‚
:::

:::message
**é€²æ—**: å…¨ä½“ã®10%å®Œäº†ã€‚World Modelsã®3ãƒ¬ãƒ™ãƒ«åˆ†é¡ã‚’ç†è§£ã—ãŸã€‚JEPAã¯ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬ã™ã‚‹é©å‘½çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœWorld ModelsãŒæœ€çµ‚åˆ°é”ç‚¹ã‹

### 2.1 ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–ç³»è­œ

```mermaid
graph TD
    A[VAE: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«] --> B[GAN: æ•µå¯¾çš„å­¦ç¿’]
    B --> C[Diffusion: ã‚¹ã‚³ã‚¢é–¢æ•°]
    C --> D[LDM: æ½œåœ¨ç©ºé–“æ‹¡æ•£]
    D --> E[Consistency Models: 1-stepç”Ÿæˆ]
    E --> F[World Models: ç’°å¢ƒç†è§£]

    style F fill:#ff9,stroke:#333,stroke-width:4px
```

**ãªãœWorld ModelsãŒæœ€çµ‚å½¢æ…‹ã‹ï¼Ÿ**

1. **ç”Ÿæˆã¯æ‰‹æ®µã€ç†è§£ãŒç›®çš„**: ç”»åƒç”Ÿæˆã¯ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ä¸€éƒ¨ã‚’ã‚µãƒ³ãƒ—ãƒ«ã™ã‚‹ã ã‘ã€‚World Modelsã¯ç’°å¢ƒã®**å› æœæ§‹é€ **ã‚’ç†è§£ã™ã‚‹
2. **è¡Œå‹•æ¡ä»¶ä»˜ãäºˆæ¸¬**: $p(x_{t+1}|x_{\leq t}, a_t)$ â€” è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã§ãã‚‹
3. **ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒªãƒ¼**: ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’å›é¿ã—ã€æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬ã™ã‚‹åŠ¹ç‡æ€§

### 2.2 Course IVã§ã®ä½ç½®ã¥ã‘

| å› | ãƒ†ãƒ¼ãƒ | World Modelsã¸ã®æ¥ç¶š |
|:---|:------|:--------------------|
| **ç¬¬33å›** | Normalizing Flows | å¯é€†å¤‰æ› â†’ æ±ºå®šè«–çš„å†™åƒã®é™ç•Œ |
| **ç¬¬34å›** | EBM | ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° â†’ **Energy-based World Models** |
| **ç¬¬35å›** | Score Matching | ã‚¹ã‚³ã‚¢é–¢æ•° â†’ å‹•çš„éç¨‹ã®å­¦ç¿’ |
| **ç¬¬36å›** | DDPM | Forward/Reverse â†’ æ™‚ç³»åˆ—äºˆæ¸¬ã®åŸºç›¤ |
| **ç¬¬37å›** | SDE/ODE | é€£ç¶šæ™‚é–“ç¢ºç‡éç¨‹ â†’ ç‰©ç†æ³•å‰‡å­¦ç¿’ |
| **ç¬¬38å›** | Flow Matching | OTè¦–ç‚¹ â†’ **æœ€é©è¼¸é€ã¨ã—ã¦ã®World Models** |
| **ç¬¬39å›** | LDM | æ½œåœ¨ç©ºé–“æ‹¡æ•£ â†’ **æ½œåœ¨ç©ºé–“äºˆæ¸¬** |
| **ç¬¬40å›** | Consistency Models | 1-stepç”Ÿæˆ â†’ é«˜é€Ÿæ¨è«– |
| **ç¬¬41å›** | **World Models** | **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹** |

### 2.3 æ¾å°¾ç ”ã¨ã®æ±ºå®šçš„ãªé•ã„

| é …ç›® | æ¾å°¾ç ” | æœ¬è¬›ç¾© |
|:-----|:------|:------|
| **World Modelsæ‰±ã„** | è¨€åŠãªã— | **å®Œå…¨ç†è«–åŒ–** |
| **JEPA** | è§¦ã‚Œãªã„ | I-JEPA / V-JEPA / VL-JEPAå®Œå…¨è§£èª¬ |
| **Transfusion** | æ‰±ã‚ãªã„ | **AR+Diffusionçµ±ä¸€ç†è«–ã®æ•°å­¦** |
| **ç‰©ç†æ³•å‰‡å­¦ç¿’** | æ‰±ã‚ãªã„ | Physics-Informed World Modelsæ·±æ˜ã‚Š |
| **å®Ÿè£…** | ãªã— | Julia JEPAã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè£… |

### 2.4 å­¦ç¿’æˆ¦ç•¥

```mermaid
graph LR
    A[ç¬¬41å›: ç†è«–ç†è§£] --> B[ç¬¬42å›: çµ±ä¸€ç†è«–]
    A --> C[ç¬¬43å› DiT/FLUXå®Ÿè£…]
    A --> D[ç¬¬45å› Videoç”Ÿæˆ]
    A --> E[ç¬¬47å› Embodied AI]

    style A fill:#f9f,stroke:#333,stroke-width:2px
```

World Modelsã¯**å…¨ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆç”»åƒãƒ»å‹•ç”»ãƒ»ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ãƒ»ç§‘å­¦ï¼‰ã®çµ±ä¸€åŸºç›¤**ã ã€‚

:::details Trojan Horse â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®"æœ¬å½“ã®ç›®çš„"
ç¬¬1å›ã‹ã‚‰38å›ã¾ã§ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€Œç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã€æŠ€è¡“ã¨ã—ã¦å­¦ã‚“ã§ããŸã€‚

ã ãŒLeCunãŒæå”±ã™ã‚‹JEPAã¯**ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹**ã€‚

**ç”Ÿæˆã¯å‰¯ç”£ç‰©ã«éããªã‹ã£ãŸ**ã€‚çœŸã®ç›®çš„ã¯**ç’°å¢ƒã®å› æœæ§‹é€ ã‚’ç†è§£ã—ã€è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨**ã ã€‚

ã“ã‚ŒãŒTrojan Horseã®æœ€çµ‚å½¢æ…‹ã ã€‚ã€Œç”ŸæˆAIã€ã¯ã€Œç†è§£AIã€ã«é€²åŒ–ã™ã‚‹ã€‚
:::

:::message
**é€²æ—**: å…¨ä½“ã®20%å®Œäº†ã€‚ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–ç³»è­œã‚’ç†è§£ã—ãŸã€‚World Modelsã¯ç”Ÿæˆã®å…ˆã«ã‚ã‚‹ã€Œç†è§£+äºˆæ¸¬+ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ã®çµ±åˆæ¦‚å¿µã ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” World Modelsã®æ•°å­¦çš„åŸºç¤

### 3.1 World Modelã®å®šç¾©

**å®šç¾©**: World Model $\mathcal{M}$ã¯ç’°å¢ƒã®æ½œåœ¨è¡¨ç¾$z_t$ã¨é·ç§»é–¢æ•°$f_\theta$ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ç¢ºç‡çš„ã‚·ã‚¹ãƒ†ãƒ ã§ã‚ã‚‹ã€‚

$$
\begin{aligned}
\text{Encoder: } & z_t = \text{Enc}_\phi(x_t) \\
\text{Predictor: } & z_{t+1} = f_\theta(z_t, a_t) + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \Sigma) \\
\text{Decoder: } & \hat{x}_{t+1} = \text{Dec}_\psi(z_{t+1})
\end{aligned}
$$

**ãªãœæ½œåœ¨ç©ºé–“ã‹ï¼Ÿ**

- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ $x \in \mathbb{R}^{H \times W \times C}$ ã¯é«˜æ¬¡å…ƒï¼ˆ$H=256, W=256, C=3 \Rightarrow 196,608$æ¬¡å…ƒï¼‰
- æ½œåœ¨ç©ºé–“ $z \in \mathbb{R}^d$ ã¯ä½æ¬¡å…ƒï¼ˆ$d=256$ç¨‹åº¦ï¼‰ã§**æ§‹é€ çš„è¡¨ç¾**ã‚’ç²å¾—

**è¨“ç·´ç›®æ¨™**: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\{(x_t, a_t, x_{t+1})\}_{t=1}^T$ ã‹ã‚‰$\theta, \phi, \psi$ã‚’å­¦ç¿’

$$
\mathcal{L}(\theta, \phi, \psi) = \mathbb{E}_{(x_t, a_t, x_{t+1})} \left[ \| \text{Dec}_\psi(f_\theta(\text{Enc}_\phi(x_t), a_t)) - x_{t+1} \|_2^2 \right]
$$

### 3.2 JEPAç†è«–: Joint-Embedding Predictive Architecture

#### 3.2.1 I-JEPA (Image-based JEPA)

**è«–æ–‡**: Assran et al., "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture," CVPR 2023

**å‹•æ©Ÿ**: ç”»åƒç”Ÿæˆï¼ˆpixel reconstructionï¼‰ã¯ä½ãƒ¬ãƒ™ãƒ«è©³ç´°ã«éå‰°é©åˆã—ã€é«˜ãƒ¬ãƒ™ãƒ«æŠ½è±¡è¡¨ç¾ã‚’å­¦ç¿’ã—ã«ãã„ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

```mermaid
graph LR
    A[Context patches<br>x_ctx] --> B[Context Encoder<br>s_Î¸]
    B --> C[Latent z_ctx]
    C --> D[Predictor<br>f_Î¸]
    E[Target patches<br>x_tgt] --> F[Target Encoder<br>s_Î¸ EMA]
    F --> G[Latent z_tgt]
    D --> H[Predicted z_pred]
    H -.loss.-> G

    style D fill:#ff9,stroke:#333,stroke-width:2px
    style H fill:#f99,stroke:#333,stroke-width:2px
```

**æå¤±é–¢æ•°**:

$$
\mathcal{L}_{\text{I-JEPA}} = \mathbb{E}_{x, M} \left[ \| f_\theta(s_\theta(x_{\text{ctx}}), M) - \bar{s}_\theta(x_{\text{tgt}}) \|_2^2 \right]
$$

ã“ã“ã§:
- $x_{\text{ctx}}$: ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„context patches
- $x_{\text{tgt}}$: ãƒã‚¹ã‚¯ã•ã‚ŒãŸtarget patches
- $M$: mask tokens (positional encoding)
- $s_\theta$: context encoder (trainable)
- $\bar{s}_\theta$: target encoder (EMAæ›´æ–°)
- $f_\theta$: predictor

**é‡è¦ãªç‰¹æ€§**:

1. **ãƒ”ã‚¯ã‚»ãƒ«å†æ§‹æˆãªã—**: $x_{\text{tgt}}$ã‚’ç”Ÿæˆã›ãšã€æ½œåœ¨è¡¨ç¾$z_{\text{tgt}}$ã‚’äºˆæ¸¬
2. **EMA target encoder**: $\bar{\theta} \leftarrow \tau \bar{\theta} + (1-\tau)\theta$ ã§collapseå›é¿
3. **Mask strategy**: ãƒ©ãƒ³ãƒ€ãƒ ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¹ã‚¯ï¼ˆGrid-basedï¼‰ã§æ§‹é€ çš„äºˆæ¸¬ã‚’ä¿ƒé€²

**æ•°å€¤ä¾‹**:

```julia
# Context: ç”»åƒã®å·¦åŠåˆ† (32x64 patches)
x_ctx = x[:, 1:32, :, :]

# Target: ç”»åƒã®å³åŠåˆ† (32x64 patches) â€” Encoderé€šã™ãŒå‹¾é…ã¯æµã•ãªã„
x_tgt = x[:, 33:64, :, :]

# Context encoder (trainable)
z_ctx = s_Î¸(x_ctx)  # [B, 32, D]

# Predictor: mask tokens M ã‚’ä½¿ã£ã¦ targetä½ç½®ã®è¡¨ç¾ã‚’äºˆæ¸¬
M = mask_tokens[:, 33:64, :]  # [B, 32, D_mask]
z_pred = f_Î¸(z_ctx, M)  # [B, 32, D]

# Target encoder (EMA, no grad)
z_tgt = stopgradient(sÌ„_Î¸(x_tgt))  # [B, 32, D]

# Loss
loss = mean((z_pred - z_tgt).^2)
```

#### 3.2.2 V-JEPA (Video JEPA)

**è«–æ–‡**: Bardes et al., "Revisiting Feature Prediction for Learning Visual Representations from Video," arXiv:2404.08471, 2024 (V-JEPA 1.0)
**æœ€æ–°**: "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning," arXiv:2506.09985, 2025

**æ‹¡å¼µ**: ç”»åƒâ†’å‹•ç”»ï¼ˆæ™‚ç©ºé–“äºˆæ¸¬ï¼‰

$$
\begin{aligned}
\text{Context: } & \mathbf{x}_{\text{ctx}} \in \mathbb{R}^{T_c \times H \times W \times C} \\
\text{Target: } & \mathbf{x}_{\text{tgt}} \in \mathbb{R}^{T_t \times H \times W \times C}
\end{aligned}
$$

**Spatio-temporal masking**:

- **Temporal masking**: å‰åŠ8ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆcontextï¼‰â†’å¾ŒåŠ8ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆtargetï¼‰ã‚’äºˆæ¸¬
- **Spatial masking**: å„ãƒ•ãƒ¬ãƒ¼ãƒ å†…ã§ãƒ‘ãƒƒãƒã‚’ãƒã‚¹ã‚¯

**è¨“ç·´ç›®æ¨™**:

$$
\mathcal{L}_{\text{V-JEPA}} = \mathbb{E}_{\mathbf{x}, M_s, M_t} \left[ \| f_\theta(s_\theta(\mathbf{x}_{\text{ctx}}), M_s, M_t) - \bar{s}_\theta(\mathbf{x}_{\text{tgt}}) \|_2^2 \right]
$$

**æ€§èƒ½**:

- Kinetics-400 (action recognition): **81.9%** Top-1 accuracy (video pre-trainingã®ã¿)
- Something-Something v2: **72.2%**
- ImageNet: **77.9%** Top-1 (å‹•ç”»äº‹å‰å­¦ç¿’ã‹ã‚‰ç”»åƒã‚¿ã‚¹ã‚¯ã«è»¢ç§»)

#### 3.2.3 VL-JEPA (Vision-Language JEPA)

**è«–æ–‡**: Bardes et al., "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language," arXiv:2512.10942, 2024

**å‹•æ©Ÿ**: å¾“æ¥ã®VLMï¼ˆVision-Language Modelsï¼‰ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§autoregressiveã«ç”Ÿæˆã™ã‚‹ã€‚ã“ã‚Œã¯è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ãã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚‚è†¨å¤§ï¼ˆdecoderå±¤ãŒå¿…è¦ï¼‰ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

VL-JEPAã¯**ãƒ†ã‚­ã‚¹ãƒˆã®é€£ç¶šåŸ‹ã‚è¾¼ã¿ã‚’äºˆæ¸¬**ã—ã€token-by-tokenç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚

$$
\begin{aligned}
\text{Image encoder: } & z_v = \text{Enc}_v(x) \\
\text{Predictor: } & z_{\text{pred}} = f_\theta(z_v, \text{prompt}) \\
\text{Text encoder: } & z_t = \text{Enc}_t(\text{target text}) \\
\text{Loss: } & \mathcal{L} = \| z_{\text{pred}} - z_t \|_2^2
\end{aligned}
$$

**åˆ©ç‚¹**:

- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒæ¨™æº–VLMã®**50%å‰Šæ¸›**ï¼ˆdecoderãªã—ï¼‰
- **ã‚ˆã‚Šå¼·ã„æ€§èƒ½**: åŒã˜vision encoderã¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã—ãŸæ¨™æº–VLMã‚’ä¸Šå›ã‚‹

### 3.3 Transfusionç†è«–: AR + Diffusionçµ±ä¸€

**è«–æ–‡**: Zhou et al., "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model," arXiv:2408.11039, 2024 (Meta AI)

**å‹•æ©Ÿ**: ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã¨ç”»åƒï¼ˆé€£ç¶šãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’**å˜ä¸€Transformerã§çµ±ä¸€å‡¦ç†**ã—ãŸã„ã€‚

**å¾“æ¥æ‰‹æ³•ã®å•é¡Œ**:

- ç”»åƒã‚’VQ-VAEã§é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³åŒ– â†’ é‡å­åŒ–èª¤å·®ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯åˆ©ç”¨ç‡ä½ä¸‹
- åˆ¥ã€…ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆLM + Diffusionï¼‰â†’ çµ±åˆã§ããªã„

**Transfusionã®è§£æ±ºç­–**:

**åŒä¸€Transformerã§ç•°ãªã‚‹æå¤±é–¢æ•°**ã‚’ä½¿ã„åˆ†ã‘ã‚‹ã€‚

$$
\mathcal{L}_{\text{Transfusion}} = \mathcal{L}_{\text{LM}}(\text{text}) + \lambda \mathcal{L}_{\text{Diffusion}}(\text{image})
$$

#### 3.3.1 ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†: Autoregressive

ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ $\mathbf{t} = (t_1, t_2, \ldots, t_n)$ ã«å¯¾ã—ã¦:

$$
\mathcal{L}_{\text{LM}} = -\sum_{i=1}^n \log p_\theta(t_i | t_{<i})
$$

é€šå¸¸ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¨åŒã˜causal maskingã¨cross-entropy lossã€‚

#### 3.3.2 ç”»åƒéƒ¨åˆ†: Diffusion

ç”»åƒãƒ‘ãƒƒãƒ $\mathbf{x} = (x_1, \ldots, x_m) \in \mathbb{R}^{m \times d}$ ã«å¯¾ã—ã¦:

$$
\begin{aligned}
\text{Forward: } & x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I) \\
\text{Diffusion Loss: } & \mathcal{L}_{\text{Diffusion}} = \mathbb{E}_{t, \epsilon} \left[ \| \epsilon - \epsilon_\theta(\mathbf{x}_t, t, \mathbf{c}) \|_2^2 \right]
\end{aligned}
$$

ã“ã“ã§ $\mathbf{c}$ ã¯ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ï¼ˆcross-attentionçµŒç”±ã§Transformerã«æ³¨å…¥ï¼‰ã€‚

#### 3.3.3 çµ±åˆå‡¦ç†ã®æ•°å­¦

å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹:

$$
\text{seq} = [\text{text tokens } t_1, \ldots, t_n, \text{ image patches } x_1, \ldots, x_m]
$$

**Attention mask**:

- ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†: **causal mask**ï¼ˆæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„ï¼‰
- ç”»åƒéƒ¨åˆ†: **bidirectional mask**ï¼ˆå…¨ãƒ‘ãƒƒãƒã‚’è¦‹ã‚‹ï¼‰

**æå¤±è¨ˆç®—**:

```julia
# ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ (discrete)
text_logits = model(text_tokens)
loss_text = cross_entropy(text_logits, text_tokens[2:end])

# ç”»åƒãƒ‘ãƒƒãƒ (continuous)
t_diffusion = rand(1:T)
noise = randn(size(image_patches))
x_t = sqrt(á¾±[t_diffusion]) * image_patches + sqrt(1 - á¾±[t_diffusion]) * noise
pred_noise = model(x_t, t_diffusion, context=text_tokens)
loss_image = mean((pred_noise - noise).^2)

# çµ±åˆ
loss = loss_text + Î» * loss_image
```

**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°çµæœ**: 7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€2T multi-modal tokensã§è¨“ç·´ â†’ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨ç”»åƒç”Ÿæˆã®ä¸¡æ–¹ã§åŒè¦æ¨¡ã®å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰æ€§èƒ½ã€‚

### 3.4 ç‰©ç†æ³•å‰‡å­¦ç¿’ç†è«–

#### 3.4.1 Physics-Informed World Models

**å‹•æ©Ÿ**: æ¨™æº–çš„ãªWorld Modelsã¯ç‰©ç†æ³•å‰‡ï¼ˆä¿å­˜å‰‡ã€å¯¾ç§°æ€§ã€å¾®åˆ†æ–¹ç¨‹å¼ï¼‰ã‚’ç„¡è¦–ã—ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•ã§å­¦ç¿’ã™ã‚‹ã€‚ã“ã‚Œã¯:

- ç‰©ç†çš„ã«ä¸å¯èƒ½ãªäºˆæ¸¬ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜å‰‡é•åãªã©ï¼‰
- ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã®æ‚ªã•ï¼ˆç‰©ç†æ³•å‰‡ã‚’çŸ¥ã£ã¦ã„ã‚Œã°å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’å¯èƒ½ï¼‰

**Physics-Informed Neural Networks (PINNs)ã®åŸç†**:

å¾®åˆ†æ–¹ç¨‹å¼åˆ¶ç´„ã‚’æå¤±é–¢æ•°ã«åŸ‹ã‚è¾¼ã‚€ã€‚

ä¾‹: Navier-Stokesæ–¹ç¨‹å¼

$$
\frac{\partial \mathbf{u}}{\partial t} + (\mathbf{u} \cdot \nabla)\mathbf{u} = -\frac{1}{\rho}\nabla p + \nu \nabla^2 \mathbf{u}
$$

**PINNs loss**:

$$
\mathcal{L}_{\text{PINN}} = \mathcal{L}_{\text{data}} + \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}}
$$

$$
\mathcal{L}_{\text{PDE}} = \mathbb{E}_{x,t} \left[ \left\| \frac{\partial \mathbf{u}_\theta}{\partial t} + (\mathbf{u}_\theta \cdot \nabla)\mathbf{u}_\theta + \frac{1}{\rho}\nabla p_\theta - \nu \nabla^2 \mathbf{u}_\theta \right\|_2^2 \right]
$$

**World Modelsã¸ã®é©ç”¨**:

$$
\mathcal{L}_{\text{Physics-WM}} = \mathcal{L}_{\text{prediction}} + \lambda_{\text{conservation}} \mathcal{L}_{\text{conservation}}
$$

$$
\mathcal{L}_{\text{conservation}} = \mathbb{E} \left[ \| E(z_{t+1}) - E(z_t) \|_2^2 \right]
$$

ã“ã“ã§$E(z)$ã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ï¼ˆå­¦ç¿’ã¾ãŸã¯æ—¢çŸ¥ï¼‰ã€‚

#### 3.4.2 ä¿å­˜å‰‡ã®åŸ‹ã‚è¾¼ã¿

**é‹å‹•é‡ä¿å­˜**:

$$
\sum_{i=1}^N m_i \mathbf{v}_i(t) = \text{const}
$$

**Graph Neural Networkã§ã®å®Ÿè£…**:

ãƒãƒ¼ãƒ‰$i$ã®é€Ÿåº¦$\mathbf{v}_i$ã«å¯¾ã—ã¦ã€edge $(i,j)$ã®æ›´æ–°:

$$
\Delta \mathbf{v}_i = \sum_{j \in \mathcal{N}(i)} \text{MLP}(\mathbf{h}_i, \mathbf{h}_j, \mathbf{r}_{ij})
$$

**ä¿å­˜å‰‡åˆ¶ç´„**: å„edgeæ›´æ–°ãŒé‹å‹•é‡ä¿å­˜ã‚’æº€ãŸã™ã‚ˆã†ã«ã€**Newton's third law**ã‚’æ˜ç¤ºçš„ã«é©ç”¨:

$$
m_i \Delta \mathbf{v}_i = -m_j \Delta \mathbf{v}_j
$$

**å®Ÿè£…**:

```julia
# Edge-local reference frameã§ã®forceè¨ˆç®—
f_ij = MLP(h_i, h_j, r_ij)

# Newton's third lawã§å¯¾ç§°åŒ–
Î”v_i = f_ij / m_i
Î”v_j = -f_ij / m_j

# æ›´æ–°
v_i_new = v_i + Î”v_i
v_j_new = v_j + Î”v_j

# æ¤œè¨¼: ç³»å…¨ä½“ã®é‹å‹•é‡ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹
@assert sum(m_i * v_i_new for i in 1:N) â‰ˆ sum(m_i * v_i for i in 1:N)
```

#### 3.4.3 Hamiltonian Neural Networks

**HamiltonianåŠ›å­¦ç³»**:

$$
\begin{aligned}
\dot{q} &= \frac{\partial H}{\partial p} \\
\dot{p} &= -\frac{\partial H}{\partial q}
\end{aligned}
$$

ã“ã“ã§$H(q, p)$ã¯Hamiltonianï¼ˆç·ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰ã€‚

**HNNã®å­¦ç¿’**:

1. NNã§$H_\theta(q, p)$ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–
2. è‡ªå‹•å¾®åˆ†ã§$\partial H / \partial p, \partial H / \partial q$ã‚’è¨ˆç®—
3. æå¤±:

$$
\mathcal{L}_{\text{HNN}} = \mathbb{E} \left[ \left\| \left(\dot{q}, \dot{p}\right) - \left(\frac{\partial H_\theta}{\partial p}, -\frac{\partial H_\theta}{\partial q}\right) \right\|_2^2 \right]
$$

**åˆ©ç‚¹**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜å‰‡ãŒ**æ§‹é€ çš„ã«ä¿è¨¼**ã•ã‚Œã‚‹ï¼ˆHamiltonianã®æ™‚é–“å¾®åˆ†ãŒ0ï¼‰ã€‚

### 3.5 Energy-based World Models

**ç¬¬34å›EBMã¨ã®æ¥ç¶š**:

World Modelsã‚’**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°$E_\theta(z_t, a_t, z_{t+1})$**ã¨ã—ã¦å®šå¼åŒ–:

$$
p(z_{t+1} | z_t, a_t) = \frac{\exp(-E_\theta(z_t, a_t, z_{t+1}))}{Z(z_t, a_t)}
$$

**åˆ©ç‚¹**:

- ä»»æ„ã®åˆ†å¸ƒå½¢çŠ¶ã‚’è¡¨ç¾å¯èƒ½ï¼ˆGaussianã«åˆ¶ç´„ã•ã‚Œãªã„ï¼‰
- ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– = æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„æœªæ¥çŠ¶æ…‹

**è¨“ç·´**: Contrastive Divergence (CD-k) ã¾ãŸã¯ Score Matching

$$
\nabla_\theta \mathcal{L} = \mathbb{E}_{z_t, a_t, z_{t+1}^{+}} [\nabla_\theta E_\theta(z_t, a_t, z_{t+1}^{+})] - \mathbb{E}_{z_t, a_t, z_{t+1}^{-}} [\nabla_\theta E_\theta(z_t, a_t, z_{t+1}^{-})]
$$

ã“ã“ã§$z_{t+1}^{+}$ã¯ãƒ‡ãƒ¼ã‚¿ã€$z_{t+1}^{-}$ã¯Langevin dynamicsã§ã‚µãƒ³ãƒ—ãƒ«ã€‚

### 3.6 ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ç†è«–

#### 3.6.1 Action-conditionedäºˆæ¸¬

**ç›®çš„**: action $a_t$ã‚’ä¸ãˆã¦æ¬¡çŠ¶æ…‹$z_{t+1}$ã‚’äºˆæ¸¬

$$
z_{t+1} = f_\theta(z_t, a_t) + \epsilon_t
$$

**è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ãƒ­ãƒœãƒƒãƒˆè»Œè·¡ $(z_t, a_t, z_{t+1})$

**æå¤±**:

$$
\mathcal{L}_{\text{pred}} = \mathbb{E} \left[ \| f_\theta(z_t, a_t) - z_{t+1} \|_2^2 \right]
$$

**Stochastic dynamics**ã®å ´åˆ:

$$
p_\theta(z_{t+1} | z_t, a_t) = \mathcal{N}(f_\theta(z_t, a_t), \Sigma_\theta(z_t, a_t))
$$

$$
\mathcal{L}_{\text{NLL}} = -\mathbb{E} \left[ \log p_\theta(z_{t+1} | z_t, a_t) \right]
$$

#### 3.6.2 Reward Prediction

World Modelã‚’å¼·åŒ–å­¦ç¿’ã«çµ±åˆã™ã‚‹å ´åˆã€å ±é…¬é–¢æ•°$r_t$ã‚‚äºˆæ¸¬:

$$
r_t = g_\phi(z_t, a_t)
$$

**è¨“ç·´**:

$$
\mathcal{L}_{\text{reward}} = \mathbb{E} \left[ (g_\phi(z_t, a_t) - r_t)^2 \right]
$$

**Model-based RL**:

1. World Modelã§æœªæ¥ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ: $z_{t+1} = f_\theta(z_t, a_t)$
2. å ±é…¬ã‚’æ¨å®š: $\hat{r}_t = g_\phi(z_t, a_t)$
3. Policy $\pi_\psi(a|z)$ã‚’æœ€é©åŒ–:

$$
\mathcal{L}_{\text{policy}} = -\mathbb{E}_{\pi} \left[ \sum_{t=0}^H \gamma^t g_\phi(z_t, a_t) \right]
$$

#### 3.6.3 è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’

**Contrastive Learning**: æ­£ä¾‹ï¼ˆåŒä¸€å‹•ç”»ã®è¿‘æ¥ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰vs è² ä¾‹ï¼ˆç•°ãªã‚‹å‹•ç”»ï¼‰

$$
\mathcal{L}_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(z_t, z_{t+k}) / \tau)}{\sum_{j} \exp(\text{sim}(z_t, z_j^{-}) / \tau)}
$$

**Masked Autoencoding**: ä¸€éƒ¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒã‚¹ã‚¯ â†’ äºˆæ¸¬ï¼ˆJEPAã¨åŒã˜åŸç†ï¼‰

### 3.7 ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

#### 3.7.1 äºˆæ¸¬ç²¾åº¦

**Mean Squared Error (MSE)**:

$$
\text{MSE} = \frac{1}{N} \sum_{i=1}^N \| z_{t+1}^{(i)} - \hat{z}_{t+1}^{(i)} \|_2^2
$$

**Structural Similarity (SSIM)** (ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã§è©•ä¾¡ã™ã‚‹å ´åˆ):

$$
\text{SSIM}(x, \hat{x}) = \frac{(2\mu_x \mu_{\hat{x}} + C_1)(2\sigma_{x\hat{x}} + C_2)}{(\mu_x^2 + \mu_{\hat{x}}^2 + C_1)(\sigma_x^2 + \sigma_{\hat{x}}^2 + C_2)}
$$

#### 3.7.2 ç‰©ç†æ³•å‰‡éµå®ˆã‚¹ã‚³ã‚¢

**ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜èª¤å·®**:

$$
\text{Energy Error} = \frac{1}{T} \sum_{t=1}^T | E(z_t) - E(z_0) |
$$

**é‹å‹•é‡ä¿å­˜èª¤å·®**:

$$
\text{Momentum Error} = \frac{1}{T} \sum_{t=1}^T \left\| \sum_i m_i \mathbf{v}_i(t) - \sum_i m_i \mathbf{v}_i(0) \right\|_2
$$

#### 3.7.3 é•·æœŸä¸€è²«æ€§

**Frame Prediction Horizon**: ãƒ¢ãƒ‡ãƒ«ãŒä½•ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§æ­£ç¢ºã«äºˆæ¸¬ã§ãã‚‹ã‹

$$
T_{\text{horizon}} = \max\{t : \text{MSE}(t) < \epsilon\}
$$

**Video Quality Metrics**:

- **FVD (FrÃ©chet Video Distance)**: I3Dç‰¹å¾´é‡ã§ã®FrÃ©chetè·é›¢
- **LPIPS**: çŸ¥è¦šçš„é¡ä¼¼åº¦

### ğŸ¥Š Boss Battle: Transfusionã®å®Œå…¨åˆ†è§£

**èª²é¡Œ**: arXiv:2408.11039ã®Transfusionã®çµ±ä¸€æå¤±é–¢æ•°ã‚’ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¤ãƒ¡ãƒ¼ã‚¸ã®æ··åˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦å®Œå…¨å°å‡ºã›ã‚ˆã€‚

**Step 1**: å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹

ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ† $\mathbf{t} = (t_1, \ldots, t_n)$ ï¼ˆdiscrete tokensï¼‰
ç”»åƒéƒ¨åˆ† $\mathbf{x} = (x_1, \ldots, x_m)$ ï¼ˆcontinuous patch embeddingsï¼‰

çµ±åˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹:

$$
\mathbf{s} = [\mathbf{t}, \mathbf{x}] \in \mathbb{R}^{(n+m) \times d}
$$

**Step 2**: Attention Mask

$$
M_{\text{Transfusion}} = \begin{bmatrix}
M_{\text{causal}} & 0 \\
M_{\text{bi-dir}} & M_{\text{bi-dir}}
\end{bmatrix}
$$

- å·¦ä¸Š: ãƒ†ã‚­ã‚¹ãƒˆã®causal maskï¼ˆè‡ªå·±å›å¸°ï¼‰
- å³ä¸‹: ç”»åƒã®bidirectional maskï¼ˆå…¨ãƒ‘ãƒƒãƒç›¸äº’å‚ç…§ï¼‰
- å·¦ä¸‹: ç”»åƒãŒãƒ†ã‚­ã‚¹ãƒˆã‚’è¦‹ã‚‹ï¼ˆcross-modal attentionï¼‰
- å³ä¸Š: 0ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯ç”»åƒã‚’è¦‹ãªã„ï¼‰

**Step 3**: Loss Functions

**ãƒ†ã‚­ã‚¹ãƒˆloss** (next token prediction):

$$
\mathcal{L}_{\text{text}} = -\frac{1}{n} \sum_{i=1}^n \log p_\theta(t_i | t_{<i})
$$

Softmaxã§ç¢ºç‡åŒ–:

$$
p_\theta(t_i | t_{<i}) = \frac{\exp(z_{t_i}^\top e_{t_i})}{\sum_{j=1}^{|V|} \exp(z_{t_i}^\top e_j)}
$$

ã“ã“ã§$z_{t_i}$ã¯Transformerã®$i$ç•ªç›®å‡ºåŠ›ã€$e_j$ã¯token embeddingã®$j$ç•ªç›®ã€‚

**ç”»åƒloss** (diffusion):

$$
\mathcal{L}_{\text{image}} = \mathbb{E}_{t \sim [1,T], \epsilon \sim \mathcal{N}(0,I)} \left[ \| \epsilon - \epsilon_\theta(\mathbf{x}_t, t, \mathbf{c}) \|_2^2 \right]
$$

ã“ã“ã§:

$$
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon
$$

$\mathbf{c}$ã¯ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ï¼ˆcross-attentionã§æ³¨å…¥ï¼‰ã€‚

**Step 4**: çµ±åˆæå¤±

$$
\mathcal{L}_{\text{Transfusion}} = \mathcal{L}_{\text{text}} + \lambda \mathcal{L}_{\text{image}}
$$

$\lambda$ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè«–æ–‡ã§ã¯$\lambda=1$ã‚’ä½¿ç”¨ï¼‰ã€‚

**Step 5**: å®Ÿè£…ã‚³ãƒ¼ãƒ‰ï¼ˆJuliaï¼‰

```julia
using Lux, Random, Optimisers

# Transformer with mixed masking
struct TransfusionTransformer
    layers::Chain
    text_head::Dense
    image_head::Dense
end

function (m::TransfusionTransformer)(s, mask, t_diffusion=nothing)
    h = m.layers(s, mask)  # [B, n+m, D]

    # Split into text and image parts
    h_text = h[:, 1:n_text, :]
    h_image = h[:, n_text+1:end, :]

    # Text head: logits
    logits_text = m.text_head(h_text)  # [B, n_text, vocab_size]

    # Image head: noise prediction
    if !isnothing(t_diffusion)
        # Embed diffusion time step
        t_emb = sinusoidal_embedding(t_diffusion)
        pred_noise = m.image_head(cat(h_image, t_emb, dims=3))
    else
        pred_noise = nothing
    end

    return logits_text, pred_noise
end

# Loss function
function transfusion_loss(model, text_tokens, image_patches, ps, st)
    n_text = size(text_tokens, 2)

    # Forward diffusion on images
    t = rand(1:1000)
    noise = randn(size(image_patches))
    á¾±_t = alpha_bar(t)
    x_t = sqrt(á¾±_t) * image_patches + sqrt(1 - á¾±_t) * noise

    # Construct input sequence [text, image]
    s = cat(embed(text_tokens), x_t, dims=2)  # [B, n_text + m_image, D]

    # Construct mask
    mask = create_transfusion_mask(n_text, size(x_t, 2))

    # Forward
    logits_text, pred_noise = model(s, mask, t, ps, st)

    # Text loss (cross-entropy)
    loss_text = cross_entropy(logits_text, text_tokens[:, 2:end])

    # Image loss (diffusion)
    loss_image = mean((pred_noise - noise).^2)

    # Total loss
    return loss_text + Î» * loss_image
end
```

**Bossæ’ƒç ´ï¼** Transfusionã®çµ±ä¸€æå¤±é–¢æ•°ã‚’å®Œå…¨ã«å°å‡ºã—ã€å®Ÿè£…ã—ãŸã€‚

:::message alert
**ã“ã“ã§èº“ãäººãŒå¤šã„**: Transfusionã®Attention maskã¯**æ··åˆå‹**ã§ã‚ã‚‹ã€‚ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã¯causalã€ç”»åƒéƒ¨åˆ†ã¯bidirectionalã€ãã—ã¦ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«éƒ¨åˆ†ã¯**ç”»åƒâ†’ãƒ†ã‚­ã‚¹ãƒˆ**ã®ã¿è¨±å¯ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯ç”»åƒã‚’è¦‹ãªã„ï¼‰ã€‚ã“ã‚Œã‚’æ­£ã—ãå®Ÿè£…ã—ãªã„ã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã§ãƒªãƒ¼ã‚¯ãŒèµ·ãã‚‹ã€‚
:::

:::message
**é€²æ—**: å…¨ä½“ã®50%å®Œäº†ã€‚World Modelsã®æ•°å­¦çš„åŸºç¤ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚JEPAï¼ˆI/V/VLï¼‰ã®3å¤‰ç¨®ã€Transfusionã®çµ±ä¸€ç†è«–ã€ç‰©ç†æ³•å‰‡å­¦ç¿’ã€EBMè¦–ç‚¹ã€è¨“ç·´ãƒ»è©•ä¾¡æ‰‹æ³•ã‚’å°å‡ºã—ãŸã€‚æ•°å¼ä¿®è¡Œãƒœã‚¹æˆ¦ã‚’ã‚¯ãƒªã‚¢ã€‚
:::

### 3.8 JEPAã®æœ€æ–°ç™ºå±•ï¼ˆ2024-2026ï¼‰

#### 3.8.1 LeJEPA: ç†è«–çš„åŸºç›¤ã®ç¢ºç«‹

**è«–æ–‡**: "LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics," arXiv:2511.08544, 2024[^1]

å¾“æ¥ã®JEPAã¯çµŒé¨“çš„è¨­è¨ˆï¼ˆEMAã€ç‰¹å®šã®ãƒã‚¹ã‚¯æˆ¦ç•¥ãªã©ï¼‰ã«ä¾å­˜ã—ã¦ã„ãŸã€‚LeJEPAã¯**ç†è«–çš„ã«æ­£å½“åŒ–ã•ã‚ŒãŸè¨“ç·´ç›®çš„**ã‚’æç¤ºã™ã‚‹ã€‚

**æ ¸å¿ƒçš„æ´å¯Ÿ**: JEPAã®ç›®çš„é–¢æ•°ã¯**æ½œåœ¨å¤‰æ•°ã®ç›¸äº’æƒ…å ±é‡æœ€å¤§åŒ–**ã¨ã—ã¦å®šå¼åŒ–ã§ãã‚‹:

$$
\max_{\theta, \phi} I(Z_{\text{ctx}}; Z_{\text{tgt}}) = \mathbb{H}(Z_{\text{tgt}}) - \mathbb{H}(Z_{\text{tgt}} | Z_{\text{ctx}})
$$

ã“ã“ã§:
- $Z_{\text{ctx}} = s_\theta(x_{\text{ctx}})$: contextè¡¨ç¾
- $Z_{\text{tgt}} = s_\theta(x_{\text{tgt}})$: targetè¡¨ç¾
- $\mathbb{H}(\cdot)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼

**LeJEPAç›®çš„é–¢æ•°**:

$$
\mathcal{L}_{\text{LeJEPA}} = \mathbb{E}_{x, M} \left[ \| f_\theta(s_\theta(x_{\text{ctx}}), M) - s_\theta(x_{\text{tgt}}) \|_2^2 \right] + \lambda \mathbb{H}(Z_{\text{tgt}})
$$

ç¬¬2é …ã¯**è¡¨ç¾ã®å¤šæ§˜æ€§**ã‚’ä¿è¨¼ã—ã€collapseï¼ˆå…¨è¡¨ç¾ãŒåŒä¸€ã«ãªã‚‹ï¼‰ã‚’é˜²ãã€‚

**ç†è«–çš„ä¿è¨¼**:

1. **åæŸä¿è¨¼**: LeJEPAã¯é©åˆ‡ãª$\lambda$ã§å¤§åŸŸæœ€é©è§£ã«åæŸ
2. **EMAä¸è¦**: ç†è«–çš„ã«æ­£å½“åŒ–ã•ã‚ŒãŸç›®çš„é–¢æ•°ã«ã‚ˆã‚ŠEMAãªã—ã§è¨“ç·´å¯èƒ½
3. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: 10å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã§åŠ¹ç‡çš„ã«è¨“ç·´å¯èƒ½

#### 3.8.2 Causal-JEPA: å› æœçš„ä»‹å…¥å­¦ç¿’

**è«–æ–‡**: "Causal-JEPA: Learning World Models through Object-Level Latent Interventions," arXiv:2602.11389, 2025[^2]

å¾“æ¥ã®JEPAã¯**ç›¸é–¢**ã‚’å­¦ç¿’ã™ã‚‹ãŒã€**å› æœé–¢ä¿‚**ã¯å­¦ç¿’ã—ãªã„ã€‚Causal-JEPAï¼ˆC-JEPAï¼‰ã¯**ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ¬ãƒ™ãƒ«ã®ãƒã‚¹ã‚­ãƒ³ã‚°**ã¨**æ½œåœ¨ä»‹å…¥**ã‚’å°å…¥ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ‹¡å¼µ**:

1. **ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ†è§£**: ç”»åƒã‚’$K$å€‹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ½œåœ¨è¡¨ç¾ã«åˆ†è§£
   $$
   z = \{z_1, z_2, \ldots, z_K\}, \quad z_k \in \mathbb{R}^d
   $$

2. **ä»‹å…¥æ“ä½œ**: ç‰¹å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ$k$ã®è¡¨ç¾ã‚’å¤‰æ›´
   $$
   \text{do}(z_k = \tilde{z}_k)
   $$

3. **åäº‹å®Ÿäºˆæ¸¬**: ä»‹å…¥å¾Œã®æœªæ¥çŠ¶æ…‹ã‚’äºˆæ¸¬
   $$
   z_{t+1}' = f_\theta(z_t | \text{do}(z_k = \tilde{z}_k))
   $$

**è¨“ç·´ç›®çš„**:

$$
\mathcal{L}_{\text{C-JEPA}} = \mathbb{E} \left[ \| f_\theta(z_{\text{ctx}} | \text{do}(z_k)) - z_{\text{tgt}} \|_2^2 \right]
$$

**å¿œç”¨**: ãƒ­ãƒœãƒƒãƒˆãƒãƒ‹ãƒ”ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã€Œã“ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å‹•ã‹ã™ã¨ä½•ãŒèµ·ãã‚‹ã‹ï¼Ÿã€ï¼‰

#### 3.8.3 Value-guided Action Planning with JEPA

**è«–æ–‡**: "Value-guided action planning with JEPA world models," arXiv:2601.00844, 2025[^3]

JEPAã‚’**å¼·åŒ–å­¦ç¿’**ã«çµ±åˆã—ã€action planningã«ä½¿ç”¨ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

1. **JEPA world model**: $z_{t+1} = f_\theta(z_t, a_t)$
2. **Value network**: $V_\psi(z_t)$ â€” çŠ¶æ…‹ä¾¡å€¤é–¢æ•°
3. **Planning**: Model Predictive Control (MPC)é¢¨ã«æœªæ¥è»Œé“ã‚’æœ€é©åŒ–

**Planning objective**:

$$
a_{t:t+H}^* = \arg\max_{a_{t:t+H}} \sum_{k=0}^H \gamma^k V_\psi(z_{t+k})
$$

ã“ã“ã§$z_{t+k}$ã¯world modelã§äºˆæ¸¬ã€‚

**å®Ÿè£…ï¼ˆJuliaæ¦‚å¿µã‚³ãƒ¼ãƒ‰ï¼‰**:

```julia
# JEPA world model
function predict_latent(z_t, a_t, ps_wm, st_wm)
    z_next, st_new = world_model(vcat(z_t, a_t), ps_wm, st_wm)
    return z_next, st_new
end

# Value network
function estimate_value(z, ps_v, st_v)
    v, st_new = value_net(z, ps_v, st_v)
    return v, st_new
end

# Planning via gradient-based optimization
function plan_actions(z_0, horizon, ps_wm, ps_v, st_wm, st_v)
    # åˆæœŸaction sequence (learnable parameters)
    a_seq = rand(Float32, action_dim, horizon)

    # Optimize actions
    opt_state = Optimisers.setup(Adam(0.01), a_seq)

    for step in 1:50  # optimization steps
        # Rollout world model
        z_t = z_0
        total_value = 0.0

        for h in 1:horizon
            z_t, _ = predict_latent(z_t, a_seq[:, h], ps_wm, st_wm)
            v, _ = estimate_value(z_t, ps_v, st_v)
            total_value += Î³^(h-1) * v
        end

        # Gradient ascent on total value
        grad = gradient(a_seq -> compute_value(a_seq, z_0, ps_wm, ps_v), a_seq)
        opt_state, a_seq = Optimisers.update(opt_state, a_seq, grad)
    end

    return a_seq[:, 1]  # Execute first action only (MPC)
end
```

**å®Ÿé¨“çµæœ**: Atariã‚²ãƒ¼ãƒ ã§å¾“æ¥ã®model-free RLï¼ˆPPOï¼‰ã‚’ä¸Šå›ã‚‹æ€§èƒ½ï¼ˆsample efficiency 3xå‘ä¸Šï¼‰ã€‚

### 3.9 Physics-Informed World Modelsã®æœ€æ–°ç™ºå±•

#### 3.9.1 Separable PINNs (SPINN)

**è«–æ–‡**: Cho et al., "Separable Physics-Informed Neural Networks," arXiv:2306.15969, 2023[^4]

å¾“æ¥ã®PINNsã¯é«˜æ¬¡å…ƒPDEï¼ˆ$d \geq 4$ï¼‰ã§ãƒ¡ãƒ¢ãƒªçˆ†ç™ºã™ã‚‹ã€‚SPINNã¯**è»¸åˆ†é›¢å¯èƒ½**æ§‹é€ ã§æ¬¡å…ƒå‰Šæ¸›ã€‚

**æ ¸å¿ƒçš„ã‚¢ã‚¤ãƒ‡ã‚¢**: PDEè§£ã‚’å¤‰æ•°åˆ†é›¢å½¢å¼ã§è¿‘ä¼¼:

$$
u(x_1, \ldots, x_d) \approx \sum_{i=1}^R u_1^{(i)}(x_1) \cdot u_2^{(i)}(x_2) \cdots u_d^{(i)}(x_d)
$$

ã“ã“ã§å„$u_j^{(i)}: \mathbb{R} \to \mathbb{R}$ã¯1æ¬¡å…ƒNNã€‚

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: æ¨™æº–PINNsãŒ$O(N^d)$ã® collocation pointsã‚’å¿…è¦ã¨ã™ã‚‹ä¸€æ–¹ã€SPINNã¯$O(dN)$ã§æ¸ˆã‚€ã€‚

**å®Ÿè£…ä¾‹**ï¼ˆ2Dç†±æ–¹ç¨‹å¼ï¼‰:

$$
\frac{\partial u}{\partial t} = \alpha \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right)
$$

```julia
# åˆ†é›¢å¯èƒ½è¿‘ä¼¼
u_x = Chain(Dense(1 => 64, tanh), Dense(64 => 1))
u_y = Chain(Dense(1 => 64, tanh), Dense(64 => 1))
u_t = Chain(Dense(1 => 64, tanh), Dense(64 => 1))

# Combined solution
function u(x, y, t, ps_x, ps_y, ps_t, st_x, st_y, st_t)
    u_x_val, _ = u_x(x, ps_x, st_x)
    u_y_val, _ = u_y(y, ps_y, st_y)
    u_t_val, _ = u_t(t, ps_t, st_t)
    return u_x_val .* u_y_val .* u_t_val
end

# PDE residual
function pde_residual(x, y, t, ps, st)
    u_val = u(x, y, t, ps...)

    # è‡ªå‹•å¾®åˆ†ã§PDEé …ã‚’è¨ˆç®—
    âˆ‚u_âˆ‚t = gradient(t -> u(x, y, t, ps...), t)[1]
    âˆ‚Â²u_âˆ‚xÂ² = gradient(x -> gradient(x -> u(x, y, t, ps...), x)[1], x)[1]
    âˆ‚Â²u_âˆ‚yÂ² = gradient(y -> gradient(y -> u(x, y, t, ps...), y)[1], y)[1]

    # æ®‹å·®
    return âˆ‚u_âˆ‚t - Î± * (âˆ‚Â²u_âˆ‚xÂ² + âˆ‚Â²u_âˆ‚yÂ²)
end

# Loss
loss = mean(pde_residual(x_collocation, y_collocation, t_collocation, ps, st).^2)
```

**æ€§èƒ½**: 10^7 collocation pointsã§è¨“ç·´å¯èƒ½ï¼ˆå¾“æ¥PINNsã®1000å€ï¼‰ã€‚

#### 3.9.2 Conservation-Aware PINNs

**è«–æ–‡**: Cardoso-Bihlo & Bihlo, "Exactly Conservative Physics-Informed Neural Operators," 2025[^5]

ç‰©ç†æ³•å‰‡ï¼ˆè³ªé‡ãƒ»é‹å‹•é‡ãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ï¼‰ã‚’**é›¢æ•£ãƒ¬ãƒ™ãƒ«ã§å³å¯†ã«ä¿è¨¼**ã™ã‚‹ã€‚

**å•é¡Œè¨­å®š**: Navier-Stokesæ–¹ç¨‹å¼ã‚’è§£ãéš›ã€æ¨™æº–PINNsã¯è¿‘ä¼¼èª¤å·®ã«ã‚ˆã‚Šä¿å­˜å‰‡ã‚’ç ´ã‚‹ã€‚

**è§£æ±ºç­–**: **Learnable Adaptive Correction**

$$
u_{\text{corrected}} = u_\theta + \Delta u_{\text{conservation}}
$$

ã“ã“ã§$\Delta u_{\text{conservation}}$ã¯ä¿å­˜å‰‡ã‚’æº€ãŸã™ã‚ˆã†ã«è‡ªå‹•è¨ˆç®—ã€‚

**è³ªé‡ä¿å­˜ã®å ´åˆ**:

$$
\int_\Omega \nabla \cdot \mathbf{u} \, dV = 0
$$

**è£œæ­£é …**:

$$
\Delta \mathbf{u} = \nabla \phi, \quad \text{where } \nabla^2 \phi = -(\nabla \cdot \mathbf{u}_\theta)
$$

ã“ã®$\phi$ã‚’è§£ãã“ã¨ã§ã€$\nabla \cdot (\mathbf{u}_\theta + \nabla \phi) = 0$ãŒå³å¯†ã«æˆç«‹ã€‚

**å®Ÿè£…ã®éµ**: Poissonæ–¹ç¨‹å¼$\nabla^2 \phi = f$ã‚’é«˜é€Ÿã«è§£ãï¼ˆFFTã¾ãŸã¯ multigridæ³•ï¼‰ã€‚

```julia
using FFTW

function enforce_mass_conservation(u_theta, grid)
    # ç™ºæ•£ã‚’è¨ˆç®—
    div_u = compute_divergence(u_theta, grid)

    # Poisson equation: âˆ‡Â²Ï† = -div_u
    # FFTã§é«˜é€Ÿã«è§£ã
    Ï†_hat = fft(div_u) ./ laplacian_eigenvalues(grid)
    Ï† = real(ifft(Ï†_hat))

    # è£œæ­£
    âˆ‡Ï† = compute_gradient(Ï†, grid)
    u_corrected = u_theta + âˆ‡Ï†

    # æ¤œè¨¼
    @assert maximum(abs.(compute_divergence(u_corrected, grid))) < 1e-10

    return u_corrected
end
```

**çµæœ**: ä¿å­˜å‰‡èª¤å·®ãŒæ¨™æº–PINNsã®10^-3ã‹ã‚‰10^-12ã«æ”¹å–„ï¼ˆ9æ¡å‘ä¸Šï¼‰ã€‚

### 3.10 Energy-Based World Modelsã®ç†è«–

#### 3.10.1 EB-JEPA: Energy-Based JEPA Library

**è«–æ–‡**: "A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures," arXiv:2602.03604, 2025[^6]

JEPAã‚’**Energy-Based Model**ã¨ã—ã¦å†å®šå¼åŒ–ã€‚

**å‹•æ©Ÿ**: å¾“æ¥ã®JEPAã¯L2æå¤±ã§è¨“ç·´ â†’ å˜å³°æ€§Gaussianä»®å®šã€‚è¤‡é›‘ãªå¤šå³°æ€§åˆ†å¸ƒã‚’è¡¨ç¾ã§ããªã„ã€‚

**Energy-based formulation**:

$$
p(z_{\text{tgt}} | z_{\text{ctx}}) = \frac{\exp(-E_\theta(z_{\text{ctx}}, z_{\text{tgt}}))}{Z(z_{\text{ctx}})}
$$

ã“ã“ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°:

$$
E_\theta(z_{\text{ctx}}, z_{\text{tgt}}) = \| f_\theta(z_{\text{ctx}}) - z_{\text{tgt}} \|_2^2
$$

**è¨“ç·´**: Noise Contrastive Estimation (NCE)

$$
\mathcal{L}_{\text{NCE}} = -\mathbb{E}_{z^+} [\log \sigma(-E_\theta(z_{\text{ctx}}, z^+))] - \mathbb{E}_{z^-} [\log \sigma(E_\theta(z_{\text{ctx}}, z^-))]
$$

ã“ã“ã§$z^+$ã¯çœŸã® targetã€$z^-$ã¯è² ä¾‹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã€‚

**å®Ÿè£…**:

```julia
# Energy function
function energy(z_ctx, z_tgt, ps, st)
    z_pred, _ = predictor(z_ctx, ps, st)
    E = sum((z_pred - z_tgt).^2, dims=1)  # [B]
    return E
end

# NCE loss
function nce_loss(z_ctx, z_tgt_pos, z_tgt_neg, ps, st)
    E_pos = energy(z_ctx, z_tgt_pos, ps, st)
    E_neg = energy(z_ctx, z_tgt_neg, ps, st)

    # Binary classification: positive = low energy, negative = high energy
    loss = -mean(log.(Ïƒ.(-E_pos))) - mean(log.(Ïƒ.(E_neg)))
    return loss
end
```

**åˆ©ç‚¹**:

- **å¤šå³°æ€§**: è¤‡æ•°ã®å¯èƒ½ãªæœªæ¥ã‚’è¡¨ç¾ï¼ˆä¾‹: å‹•ç”»äºˆæ¸¬ã§è¤‡æ•°ã®è»Œé“å€™è£œï¼‰
- **ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®é«˜ã• = ä¸ç¢ºå®Ÿæ€§

#### 3.10.2 Cognitively Inspired Energy-Based World Models

**è«–æ–‡**: "Cognitively Inspired Energy-Based World Models," arXiv:2406.08862, 2024[^7]

èªçŸ¥ç§‘å­¦ã®**äºˆæ¸¬ç¬¦å·åŒ–ï¼ˆPredictive Codingï¼‰**ç†è«–ã‚’World Modelsã«çµ±åˆã€‚

**è„³ã®äºˆæ¸¬ç¬¦å·åŒ–**:

è„³ã¯å¸¸ã«**äºˆæ¸¬**ã‚’ç”Ÿæˆã—ã€**äºˆæ¸¬èª¤å·®**ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ã€‚

$$
\text{Prediction Error} = x_{\text{observed}} - x_{\text{predicted}}
$$

**Energy-Based World Modelã¨ã®å¯¾å¿œ**:

$$
E(x_t, a_t, x_{t+1}) = \| x_{t+1} - f_\theta(x_t, a_t) \|_2^2 + \text{Prior}(x_{t+1})
$$

**éšå±¤çš„äºˆæ¸¬**:

ãƒ¬ãƒ™ãƒ«1ï¼ˆä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ï¼‰â†’ ãƒ¬ãƒ™ãƒ«2ï¼ˆä¸­ãƒ¬ãƒ™ãƒ«ï¼‰â†’ ãƒ¬ãƒ™ãƒ«3ï¼ˆé«˜ãƒ¬ãƒ™ãƒ«æŠ½è±¡æ¦‚å¿µï¼‰

å„ãƒ¬ãƒ™ãƒ«ã§äºˆæ¸¬èª¤å·®ã‚’è¨ˆç®—:

$$
\epsilon_l = h_l - f_l(h_{l+1})
$$

**Total energy**:

$$
E_{\text{total}} = \sum_{l=1}^L \lambda_l \| \epsilon_l \|_2^2
$$

**è¨“ç·´**: ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– = éšå±¤çš„äºˆæ¸¬èª¤å·®æœ€å°åŒ–

**èªçŸ¥çš„åˆ©ç‚¹**:

- **æ³¨æ„æ©Ÿæ§‹**: é«˜ã‚¨ãƒãƒ«ã‚®ãƒ¼é ˜åŸŸï¼ˆäºˆæ¸¬èª¤å·®å¤§ï¼‰ã«æ³¨æ„ã‚’å‘ã‘ã‚‹
- **èƒ½å‹•æ¨è«–**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’æœ€å°åŒ–ã™ã‚‹action $a_t$ã‚’é¸æŠ
- **æ„è­˜**: é«˜ãƒ¬ãƒ™ãƒ«äºˆæ¸¬èª¤å·®ãŒé–¾å€¤ã‚’è¶…ãˆã‚‹ã¨ã€Œæ„è­˜ã€ã«ä¸Šã‚‹

```julia
# éšå±¤çš„äºˆæ¸¬ç¬¦å·åŒ–ãƒ¢ãƒ‡ãƒ«
struct HierarchicalPredictiveCoding
    encoders::Vector{Chain}
    predictors::Vector{Chain}
    num_levels::Int
end

function (m::HierarchicalPredictiveCoding)(x, a, ps, st)
    # Bottom-up pass (encoding)
    h = Vector{Any}(undef, m.num_levels)
    h[1] = x
    for l in 2:m.num_levels
        h[l], _ = m.encoders[l-1](h[l-1], ps.enc[l-1], st.enc[l-1])
    end

    # Top-down pass (prediction)
    pred_errors = Vector{Any}(undef, m.num_levels - 1)
    for l in (m.num_levels-1):-1:1
        # Predict lower level from higher level
        h_pred, _ = m.predictors[l](vcat(h[l+1], a), ps.pred[l], st.pred[l])
        pred_errors[l] = h[l] - h_pred
    end

    # Total energy = weighted sum of prediction errors
    E = sum(Î»[l] * sum(pred_errors[l].^2) for l in 1:(m.num_levels-1))

    return E, pred_errors
end
```

**å®Ÿé¨“çµæœ**: ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§ã€æ¨™æº–World Modelsã‚ˆã‚Š30%ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡å‘ä¸Šã€‚

#### 3.10.3 Autoregressive LMs as Energy-Based Models

**è«–æ–‡**: "Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction," arXiv:2512.15605, 2024[^8]

**é©šãã®ç™ºè¦‹**: Autoregressive LMsï¼ˆGPTç³»ï¼‰ã¯å®Ÿã¯**Energy-Based Models**ã¨ç­‰ä¾¡ï¼

**å®šç†**: ARMã¨EBMã®é–“ã«**æ˜ç¤ºçš„å…¨å˜å°„**ãŒå­˜åœ¨:

$$
p_{\text{ARM}}(x_{1:T}) = \prod_{t=1}^T p(x_t | x_{<t}) \iff p_{\text{EBM}}(x_{1:T}) = \frac{\exp(-E(x_{1:T}))}{Z}
$$

ã“ã“ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°:

$$
E(x_{1:T}) = -\sum_{t=1}^T \log p(x_t | x_{<t})
$$

**Soft Bellmanæ–¹ç¨‹å¼ã¨ã®æ¥ç¶š**:

$$
V(x_{<t}) = \log \sum_{x_t} \exp(r(x_t | x_{<t}) + V(x_{\leq t}))
$$

**Transfusionã¸ã®ç¤ºå”†**: ãƒ†ã‚­ã‚¹ãƒˆï¼ˆARï¼‰ã¨ç”»åƒï¼ˆDiffusionï¼‰ã®çµ±ä¸€ã¯ã€å®Ÿã¯**ä¸¡æ–¹ã¨ã‚‚EBM**ã¨ã„ã†è¦–ç‚¹ã‹ã‚‰è‡ªç„¶ã«ç†è§£ã§ãã‚‹ï¼

$$
E_{\text{Transfusion}}(x_{\text{text}}, x_{\text{image}}) = E_{\text{ARM}}(x_{\text{text}}) + E_{\text{Diffusion}}(x_{\text{image}})
$$

ã“ã‚Œã¯**å˜ä¸€ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**ã®ç•°ãªã‚‹åˆ†è§£ã«éããªã„ã€‚

:::message alert
**æ·±ã„æ´å¯Ÿ**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€ç†è«–ã¯ã€ŒEnergy-Based World Modelsã€ã«åæŸã—ã¦ã„ã‚‹ã€‚VAEã€GANã€Diffusionã€Transfusionã€JEPAã¯å…¨ã¦**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã®ç•°ãªã‚‹è¨“ç·´ãƒ»æ¨è«–æ–¹æ³•**ã¨ã—ã¦ç†è§£ã§ãã‚‹ã€‚

ç¬¬34å›ã§å­¦ã‚“ã EBMãŒã€å®Ÿã¯ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®**æœ€ã‚‚ä¸€èˆ¬çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ã ã£ãŸï¼
:::

:::message
**é€²æ—**: å…¨ä½“ã®70%å®Œäº†ã€‚æœ€æ–°ã®JEPAç™ºå±•ï¼ˆLeJEPAã€Causal-JEPAã€Value-guided planningï¼‰ã€Physics-Informed World Modelsï¼ˆSPINNã€Conservation-Aware PINNsï¼‰ã€Energy-Basedç†è«–ï¼ˆEB-JEPAã€Predictive Codingã€ARM-EBMåŒå€¤æ€§ï¼‰ã‚’å®Œå…¨ç¿’å¾—ã€‚2020-2025ã®æœ€å…ˆç«¯ç ”ç©¶ã‚’çµ±åˆã—ãŸã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” JEPA World Modelã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè£…

### 4.1 I-JEPAã®æœ€å°å®Ÿè£…

```julia
using Lux, Random, Optimisers, Zygote

# Context Encoder (trainable)
function create_context_encoder(input_dim, hidden_dim, output_dim)
    Chain(
        Dense(input_dim => hidden_dim, gelu),
        LayerNorm(hidden_dim),
        Dense(hidden_dim => hidden_dim, gelu),
        LayerNorm(hidden_dim),
        Dense(hidden_dim => output_dim)
    )
end

# Target Encoder (EMA updated)
function create_target_encoder(input_dim, hidden_dim, output_dim)
    # Same architecture as context encoder
    create_context_encoder(input_dim, hidden_dim, output_dim)
end

# Predictor (cross-attention based)
struct JEPAPredictor{C,F}
    cross_attn::C
    ffn::F
end

function JEPAPredictor(dim, num_heads)
    cross_attn = MultiHeadAttention(dim, num_heads)
    ffn = Chain(
        Dense(dim => 4 * dim, gelu),
        Dense(4 * dim => dim)
    )
    JEPAPredictor(cross_attn, ffn)
end

function (m::JEPAPredictor)(context, mask_tokens, ps, st)
    # Cross-attention: mask_tokens query context
    attn_out, st_attn = m.cross_attn(mask_tokens, context, context, ps.attn, st.attn)

    # Feed-forward
    pred, st_ffn = m.ffn(attn_out, ps.ffn, st.ffn)

    return pred, (attn=st_attn, ffn=st_ffn)
end

# Complete I-JEPA model
struct IJEPA{E_ctx, E_tgt, P}
    context_encoder::E_ctx
    target_encoder::E_tgt
    predictor::P
    ema_momentum::Float32
end

function IJEPA(input_dim, hidden_dim, latent_dim, num_heads; Ï„=0.996f0)
    context_encoder = create_context_encoder(input_dim, hidden_dim, latent_dim)
    target_encoder = create_target_encoder(input_dim, hidden_dim, latent_dim)
    predictor = JEPAPredictor(latent_dim, num_heads)
    IJEPA(context_encoder, target_encoder, predictor, Ï„)
end

# Forward pass
function (m::IJEPA)(x, mask, ps, st)
    # x: [B, H, W, C] input image
    # mask: [B, N_patches] boolean mask (true = masked)

    # Split into context and target patches
    x_flat = reshape(x, size(x, 1), :, size(x, 4))  # [B, N_patches, C]
    x_context = x_flat[:, .!mask, :]
    x_target = x_flat[:, mask, :]

    # Context encoder (trainable)
    z_context, st_ctx = m.context_encoder(x_context, ps.context_encoder, st.context_encoder)

    # Target encoder (EMA, stop gradient)
    z_target, st_tgt = m.target_encoder(x_target, ps.target_encoder, st.target_encoder)
    z_target = Zygote.@ignore z_target  # Stop gradient

    # Predictor
    mask_tokens = randn(Float32, size(z_target))  # Learnable mask tokens
    z_pred, st_pred = m.predictor(z_context, mask_tokens, ps.predictor, st.predictor)

    return z_pred, z_target, (context_encoder=st_ctx, target_encoder=st_tgt, predictor=st_pred)
end

# EMA update for target encoder
function update_ema!(ps_target, ps_context, Ï„)
    for (k, v_target) in pairs(ps_target)
        v_context = ps_context[k]
        ps_target[k] = Ï„ * v_target + (1 - Ï„) * v_context
    end
end

# Training step
function train_step!(model, ps, st, opt_state, x, mask)
    # Forward and loss
    loss, (grads, st_new) = Zygote.withgradient(ps) do p
        z_pred, z_target, st_out = model(x, mask, p, st)
        loss = mean((z_pred - z_target).^2)
        return loss, st_out
    end

    # Update context encoder and predictor
    opt_state, ps = Optimisers.update(opt_state, ps, grads)

    # EMA update for target encoder
    update_ema!(ps.target_encoder, ps.context_encoder, model.ema_momentum)

    return loss, ps, st_new, opt_state
end

# Example usage
rng = Random.default_rng()
Random.seed!(rng, 42)

# Model setup
input_dim = 64  # Patch embedding dimension
hidden_dim = 256
latent_dim = 128
num_heads = 8

model = IJEPA(input_dim, hidden_dim, latent_dim, num_heads)
ps, st = Lux.setup(rng, model)
opt_state = Optimisers.setup(Adam(1e-4), ps)

# Training loop (concept)
for epoch in 1:100
    # Sample batch
    x = rand(Float32, 32, 64, 64, 3)  # [B=32, H=64, W=64, C=3]

    # Generate random mask (mask 50% of patches)
    mask = rand(Float32, 32, 64) .< 0.5

    # Train step
    loss, ps, st, opt_state = train_step!(model, ps, st, opt_state, x, mask)

    if epoch % 10 == 0
        @info "Epoch $epoch: Loss = $loss"
    end
end
```

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**:

1. **EMAæ›´æ–°**: Target encoderã¯momentum $\tau=0.996$ã§ã‚†ã£ãã‚Šæ›´æ–° â†’ collapseå›é¿
2. **Stop gradient**: Target encoderã®å‡ºåŠ›ã«å‹¾é…ã‚’æµã•ãªã„ï¼ˆ`Zygote.@ignore`ï¼‰
3. **Mask strategy**: ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‘ãƒƒãƒã®50%ã‚’ãƒã‚¹ã‚¯ â†’ æ§‹é€ çš„äºˆæ¸¬ã‚’å­¦ç¿’

### 4.2 V-JEPAã®æ™‚ç©ºé–“æ‹¡å¼µ

```julia
# Video encoder (3D convolution)
function create_video_encoder(in_channels, hidden_dim, latent_dim)
    Chain(
        Conv((3, 3, 3), in_channels => 64, relu; stride=(1, 2, 2)),  # [T, H/2, W/2, 64]
        Conv((3, 3, 3), 64 => 128, relu; stride=(2, 2, 2)),          # [T/2, H/4, W/4, 128]
        Conv((3, 3, 3), 128 => hidden_dim, relu; stride=(2, 2, 2)),  # [T/4, H/8, W/8, hidden_dim]
        GlobalMeanPool(),  # [hidden_dim]
        Dense(hidden_dim => latent_dim)
    )
end

# Temporal predictor
struct TemporalPredictor{A,F}
    self_attn::A
    cross_attn::A
    ffn::F
    num_layers::Int
end

function TemporalPredictor(dim, num_heads, num_layers)
    self_attn = MultiHeadAttention(dim, num_heads)
    cross_attn = MultiHeadAttention(dim, num_heads)
    ffn = Chain(
        Dense(dim => 4 * dim, gelu),
        LayerNorm(4 * dim),
        Dense(4 * dim => dim)
    )
    TemporalPredictor(self_attn, cross_attn, ffn, num_layers)
end

function (m::TemporalPredictor)(context_seq, target_positions, ps, st)
    # context_seq: [B, T_ctx, D]
    # target_positions: [B, T_tgt, D] (positional encodings)

    h = target_positions
    st_layers = []

    for layer in 1:m.num_layers
        # Self-attention
        h_self, st_self = m.self_attn(h, h, h, ps.self_attn, st.self_attn)
        h = h + h_self

        # Cross-attention to context
        h_cross, st_cross = m.cross_attn(h, context_seq, context_seq, ps.cross_attn, st.cross_attn)
        h = h + h_cross

        # FFN
        h_ffn, st_ffn = m.ffn(h, ps.ffn, st.ffn)
        h = h + h_ffn

        push!(st_layers, (self_attn=st_self, cross_attn=st_cross, ffn=st_ffn))
    end

    return h, st_layers
end

# V-JEPA model
struct VJEPA{E_ctx, E_tgt, P}
    context_encoder::E_ctx
    target_encoder::E_tgt
    temporal_predictor::P
    ema_momentum::Float32
end

function VJEPA(in_channels, hidden_dim, latent_dim, num_heads, num_layers; Ï„=0.996f0)
    context_encoder = create_video_encoder(in_channels, hidden_dim, latent_dim)
    target_encoder = create_video_encoder(in_channels, hidden_dim, latent_dim)
    temporal_predictor = TemporalPredictor(latent_dim, num_heads, num_layers)
    VJEPA(context_encoder, target_encoder, temporal_predictor, Ï„)
end

# Example: predict future 8 frames from past 8 frames
function predict_future_frames(model, video, ps, st)
    # video: [B, T=16, H, W, C]
    B, T, H, W, C = size(video)
    T_ctx = T Ã· 2  # First 8 frames
    T_tgt = T - T_ctx  # Last 8 frames

    # Context frames
    video_ctx = video[:, 1:T_ctx, :, :, :]
    z_ctx, st_ctx = model.context_encoder(video_ctx, ps.context_encoder, st.context_encoder)

    # Target frames (for training)
    video_tgt = video[:, T_ctx+1:end, :, :, :]
    z_tgt, st_tgt = model.target_encoder(video_tgt, ps.target_encoder, st.target_encoder)
    z_tgt = Zygote.@ignore z_tgt

    # Predict target latents
    target_pos = positional_encoding(T_tgt, latent_dim)  # [1, T_tgt, D]
    z_pred, st_pred = model.temporal_predictor(z_ctx, target_pos, ps.temporal_predictor, st.temporal_predictor)

    return z_pred, z_tgt, (context_encoder=st_ctx, target_encoder=st_tgt, temporal_predictor=st_pred)
end
```

**V-JEPAã®ç‰¹å¾´**:

1. **3D Convolution**: æ™‚ç©ºé–“ç‰¹å¾´ã‚’åŒæ™‚ã«æŠ½å‡º
2. **Temporal Predictor**: Transformer-basedã§éå»ã‹ã‚‰æœªæ¥ã‚’äºˆæ¸¬
3. **Positional Encoding**: æ™‚é–“ä½ç½®æƒ…å ±ã‚’æ˜ç¤ºçš„ã«ä¸ãˆã‚‹

### 4.3 Physics-Informed World Modelå®Ÿè£…

```julia
# Hamiltonian Neural Network
struct HamiltonianNN{H}
    hamiltonian::H  # Neural network for H(q, p)
end

function HamiltonianNN(input_dim, hidden_dim)
    hamiltonian = Chain(
        Dense(input_dim => hidden_dim, tanh),
        Dense(hidden_dim => hidden_dim, tanh),
        Dense(hidden_dim => 1)  # Scalar energy
    )
    HamiltonianNN(hamiltonian)
end

function (m::HamiltonianNN)(q, p, ps, st)
    # q: positions [B, n]
    # p: momenta [B, n]
    qp = vcat(q, p)  # [B, 2n]

    # Hamiltonian energy
    H, st_new = m.hamiltonian(qp, ps, st)

    return H, st_new
end

# Hamiltonian dynamics
function hamiltonian_dynamics(model, q, p, ps, st)
    # Compute H(q, p)
    H, st_new = model(q, p, ps, st)

    # âˆ‚H/âˆ‚p and âˆ‚H/âˆ‚q via automatic differentiation
    dH_dp = gradient(p -> model(q, p, ps, st)[1], p)[1]
    dH_dq = gradient(q -> model(q, p, ps, st)[1], q)[1]

    # Hamiltonian equations
    qÌ‡ = dH_dp    # dq/dt = âˆ‚H/âˆ‚p
    á¹— = -dH_dq   # dp/dt = -âˆ‚H/âˆ‚q

    return qÌ‡, á¹—, st_new
end

# Training: match observed dynamics
function hnn_loss(model, q_t, p_t, qÌ‡_obs, á¹—_obs, ps, st)
    qÌ‡_pred, á¹—_pred, st_new = hamiltonian_dynamics(model, q_t, p_t, ps, st)

    # Prediction loss
    loss = mean((qÌ‡_pred - qÌ‡_obs).^2 + (á¹—_pred - á¹—_obs).^2)

    return loss, st_new
end

# Rollout in latent space
function rollout_hnn(model, q_0, p_0, num_steps, dt, ps, st)
    q_trajectory = [q_0]
    p_trajectory = [p_0]

    q_t, p_t = q_0, p_0
    st_t = st

    for step in 1:num_steps
        # Compute derivatives
        qÌ‡, á¹—, st_t = hamiltonian_dynamics(model, q_t, p_t, ps, st_t)

        # Symplectic Euler integration (energy-preserving)
        p_t = p_t + dt * á¹—
        q_t = q_t + dt * qÌ‡

        push!(q_trajectory, q_t)
        push!(p_trajectory, p_t)
    end

    return hcat(q_trajectory...), hcat(p_trajectory...)
end

# Example: Pendulum system
# H(q, p) = pÂ²/2m + mgl(1 - cos(q))
rng = Random.default_rng()
model = HamiltonianNN(2, 64)  # input = [q, p]
ps, st = Lux.setup(rng, model)

# Generate synthetic pendulum data
m, g, l = 1.0, 9.8, 1.0
function true_hamiltonian(q, p)
    return p^2 / (2m) + m * g * l * (1 - cos(q))
end

# Training data
q_data = rand(100) * 2Ï€ .- Ï€
p_data = randn(100)
qÌ‡_data = [gradient(p -> true_hamiltonian(q_data[i], p), p_data[i])[1] for i in 1:100]
á¹—_data = [-gradient(q -> true_hamiltonian(q, p_data[i]), q_data[i])[1] for i in 1:100]

# Train HNN
opt_state = Optimisers.setup(Adam(1e-3), ps)
for epoch in 1:1000
    loss, st = hnn_loss(model, q_data, p_data, qÌ‡_data, á¹—_data, ps, st)

    grads = gradient(ps -> hnn_loss(model, q_data, p_data, qÌ‡_data, á¹—_data, ps, st)[1], ps)[1]
    opt_state, ps = Optimisers.update(opt_state, ps, grads)

    if epoch % 100 == 0
        @info "Epoch $epoch: Loss = $loss"
    end
end

# Verify energy conservation
q_0, p_0 = Ï€/4, 0.0
q_traj, p_traj = rollout_hnn(model, [q_0], [p_0], 100, 0.01, ps, st)

# Compute energy at each step
energies = [model([q_traj[i]], [p_traj[i]], ps, st)[1] for i in 1:size(q_traj, 2)]
@info "Energy variance: $(std(energies))"  # Should be ~0 if conservation holds
```

**Physics-Informedå®Ÿè£…ã®éµ**:

1. **è‡ªå‹•å¾®åˆ†**: Hamiltonianã®åå¾®åˆ†ã‚’è‡ªå‹•è¨ˆç®—
2. **Symplecticç©åˆ†**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ã‚’æ•°å€¤çš„ã«ã‚‚ä¿è¨¼
3. **æ§‹é€ çš„åˆ¶ç´„**: Hamiltonianæ§‹é€ ã‚’å¼·åˆ¶ â†’ ç‰©ç†æ³•å‰‡ã‚’å­¦ç¿’

### 4.4 Energy-Based World Model with NCE

```julia
# Energy function
struct EnergyWorldModel{E}
    energy_net::E
end

function EnergyWorldModel(state_dim, action_dim, hidden_dim)
    energy_net = Chain(
        Dense(2 * state_dim + action_dim => hidden_dim, relu),
        Dense(hidden_dim => hidden_dim, relu),
        Dense(hidden_dim => 1)  # Scalar energy
    )
    EnergyWorldModel(energy_net)
end

function (m::EnergyWorldModel)(z_t, a_t, z_next, ps, st)
    # z_t: current state [B, D]
    # a_t: action [B, A]
    # z_next: next state [B, D]
    input = vcat(z_t, a_t, z_next)  # [B, 2D+A]

    E, st_new = m.energy_net(input, ps, st)
    return E, st_new
end

# Noise Contrastive Estimation loss
function nce_loss(model, z_t, a_t, z_next_pos, z_next_neg, ps, st)
    # Positive samples (real transitions)
    E_pos, st_pos = model(z_t, a_t, z_next_pos, ps, st)

    # Negative samples (random states)
    E_neg, st_neg = model(z_t, a_t, z_next_neg, ps, st)

    # NCE loss: positive = low energy, negative = high energy
    loss = -mean(log.(Ïƒ.(-E_pos))) - mean(log.(Ïƒ.(E_neg)))

    return loss, st_pos
end

# Inference: find most likely next state
function infer_next_state(model, z_t, a_t, ps, st; num_steps=100, lr=0.01)
    # Initialize random candidate
    z_next = randn(Float32, size(z_t))

    # Gradient descent on energy
    for step in 1:num_steps
        E, st = model(z_t, a_t, z_next, ps, st)

        # âˆ‡_{z_next} E
        grad_z = gradient(z -> model(z_t, a_t, z, ps, st)[1], z_next)[1]

        # Gradient descent
        z_next = z_next - lr * grad_z
    end

    return z_next
end

# Example usage
rng = Random.default_rng()
state_dim = 64
action_dim = 4
hidden_dim = 256

model = EnergyWorldModel(state_dim, action_dim, hidden_dim)
ps, st = Lux.setup(rng, model)
opt_state = Optimisers.setup(Adam(1e-4), ps)

# Training
for epoch in 1:100
    # Sample transitions
    z_t = randn(Float32, 32, state_dim)
    a_t = randn(Float32, 32, action_dim)
    z_next_pos = randn(Float32, 32, state_dim)  # Real next states
    z_next_neg = randn(Float32, 32, state_dim)  # Random negative samples

    # Compute loss
    loss, st = nce_loss(model, z_t, a_t, z_next_pos, z_next_neg, ps, st)

    # Update
    grads = gradient(ps -> nce_loss(model, z_t, a_t, z_next_pos, z_next_neg, ps, st)[1], ps)[1]
    opt_state, ps = Optimisers.update(opt_state, ps, grads)

    if epoch % 10 == 0
        @info "Epoch $epoch: Loss = $loss"
    end
end

# Predict next state
z_t_test = randn(Float32, 1, state_dim)
a_t_test = randn(Float32, 1, action_dim)
z_next_pred = infer_next_state(model, z_t_test, a_t_test, ps, st)
@info "Predicted next state: $z_next_pred"
```

**Energy-Basedæ¨è«–ã®ç‰¹å¾´**:

1. **Gradient-based inference**: ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ã§æœ€é©ãªæ¬¡çŠ¶æ…‹ã‚’æ¢ç´¢
2. **å¤šå³°æ€§è¡¨ç¾**: ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ãŒè¤‡æ•°ã®æ¥µå°å€¤ã‚’æŒã¦ã‚‹ â†’ è¤‡æ•°ã®å¯èƒ½ãªæœªæ¥
3. **Uncertainty**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®é«˜ã• = ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–

:::details å®Ÿè£…ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
âœ… **I-JEPA**: EMAæ›´æ–°ã€stop gradientã€mask strategy
âœ… **V-JEPA**: 3D convolutionã€temporal predictorã€positional encoding
âœ… **Hamiltonian NN**: è‡ªå‹•å¾®åˆ†ã€symplectic integrationã€energy conservation
âœ… **Energy-Based WM**: NCEè¨“ç·´ã€gradient-based inferenceã€å¤šå³°æ€§å¯¾å¿œ

å…¨ã¦æœ¬ç•ªæŠ•å…¥å¯èƒ½ãªã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè£…ï¼ˆProduction-readyã«ã™ã‚‹ã«ã¯ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–ã€distributedè¨“ç·´ã€checkpointingç­‰ãŒå¿…è¦ï¼‰ã€‚
:::

:::message
**é€²æ—**: å…¨ä½“ã®85%å®Œäº†ã€‚4ã¤ã®ä¸»è¦World Modelã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆI-JEPAã€V-JEPAã€Hamiltonian NNã€Energy-Based WMï¼‰ã‚’å®Œå…¨å®Ÿè£…ã—ãŸã€‚ç†è«–ã‹ã‚‰å®Ÿè£…ã¸ã®æ©‹æ¸¡ã—å®Œäº†ã€‚
:::

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Garrido, Q., et al. (2024). LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics. arXiv:2511.08544.
@[card](https://arxiv.org/abs/2511.08544)

[^2]: Biza, O., et al. (2025). Causal-JEPA: Learning World Models through Object-Level Latent Interventions. arXiv:2602.11389.
@[card](https://arxiv.org/abs/2602.11389)

[^3]: Venkatesh, R., et al. (2025). Value-guided action planning with JEPA world models. arXiv:2601.00844.
@[card](https://arxiv.org/abs/2601.00844)

[^4]: Cho, J., et al. (2023). Separable Physics-Informed Neural Networks. In: Koyejo, S., et al. (eds) Advances in Neural Information Processing Systems 36 (NeurIPS 2023).
@[card](https://arxiv.org/abs/2306.15969)

[^5]: Cardoso-Bihlo, E. & Bihlo, A. (2024). Exactly conservative physics-informed neural networks and deep operator networks for dynamical systems. Neural Networks, 182, 106826. arXiv:2311.14131.
@[card](https://arxiv.org/abs/2311.14131)

[^6]: Kumar, A., et al. (2025). A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures. arXiv:2602.03604.
@[card](https://arxiv.org/abs/2602.03604)

[^7]: Patel, M., et al. (2024). Cognitively Inspired Energy-Based World Models. arXiv:2406.08862.
@[card](https://arxiv.org/abs/2406.08862)

[^8]: Chen, Y., et al. (2024). Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction. arXiv:2512.15605.
@[card](https://arxiv.org/abs/2512.15605)

---

## ğŸ¯ 5. ã¾ã¨ã‚ â€” World Modelsã®æœ¬è³ª

### 5.1 Part 1ã§å­¦ã‚“ã ã“ã¨

æœ¬Partã§ã¯ã€World Modelsã®**ç†è«–çš„åŸºç›¤**ã‚’å®Œå…¨ã«æ§‹ç¯‰ã—ãŸ:

**æ ¸å¿ƒæ¦‚å¿µ**:
- ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹ã¯ã€Œç”»åƒç”Ÿæˆã€ã§ã¯ãªãã€Œç’°å¢ƒç†è§£+äºˆæ¸¬+ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€
- JEPAã¯ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬ã™ã‚‹é©å‘½çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- Physics-Informed World Modelsã¯ç‰©ç†æ³•å‰‡ã‚’åŸ‹ã‚è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã¨sim2realã‚’æ”¹å–„
- Energy-Basedå®šå¼åŒ–ã«ã‚ˆã‚Šã€å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆVAE/GAN/Diffusion/JEPAï¼‰ãŒçµ±ä¸€ç†è«–ã«åæŸ

**æ•°å­¦çš„æ­¦å™¨åº«**:
- I/V/VL-JEPAã€LeJEPAã€Causal-JEPAã®å®Œå…¨ç†è«–
- Transfusionã®çµ±ä¸€æå¤±é–¢æ•°ï¼ˆAR + Diffusionï¼‰
- Hamiltonian NNã¨SPINNã«ã‚ˆã‚‹ç‰©ç†æ³•å‰‡å­¦ç¿’
- EB-JEPAã¨Predictive Codingã«ã‚ˆã‚‹èªçŸ¥ç§‘å­¦çš„å®šå¼åŒ–

**å®Ÿè£…ã‚¹ã‚­ãƒ«**:
- 4ã¤ã®ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆI-JEPAã€V-JEPAã€HNNã€Energy-Based WMï¼‰ã®Juliaå®Ÿè£…
- EMAæ›´æ–°ã€Stop gradientã€NCEã€Gradient-based inferenceã®å®Ÿè·µ

### 5.2 Part 2ã¸ã®æ¥ç¶š

Part 2ã§ã¯ã€ã“ã‚Œã‚‰ã®ç†è«–ã‚’**å®Ÿä¸–ç•Œå¿œç”¨**ã«å±•é–‹ã™ã‚‹:

- å¼·åŒ–å­¦ç¿’çµ±åˆï¼ˆDreamerV3ã€MuZeroã€IRISï¼‰
- ãƒ­ãƒœãƒƒãƒˆãƒãƒ‹ãƒ”ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆRT-1/RT-2ã€GNMï¼‰
- å‹•ç”»ç”Ÿæˆï¼ˆSoraã€VideoPoetã€WALTï¼‰
- ç§‘å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆAlphaFold3ã€Climate modelingï¼‰

Part 1ã®ç†è«–ã¯**å…¨ã¦ã®å¿œç”¨ã®åŸºç›¤**ã¨ãªã‚‹ã€‚æ¬¡å›ã¯ã“ã‚Œã‚‰ã‚’å®Ÿè·µã™ã‚‹ã€‚

---


---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
