---
title: "ç¬¬41å›: World Models & ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ç†è«–ğŸŒ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸŒ"
type: "tech"
topics: ["machinelearning", "deeplearning", "worldmodels", "julia", "jepa"]
published: true
slug: "ml-lecture-41-part1"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

# ç¬¬41å›: World Models & ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ç†è«– ğŸŒ

**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹ã¯"ç†è§£"ã ã£ãŸ**

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” 1ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹

ç¬¬40å›ã§Consistency Modelsã«ã‚ˆã‚‹1ã‚¹ãƒ†ãƒƒãƒ—é«˜é€Ÿç”Ÿæˆã‚’å®Ÿç¾ã—ãŸã€‚ã ãŒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çœŸã®ç›®çš„ã¯ä½•ã ã£ãŸã®ã‹ï¼Ÿ

å˜ã«ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã¯ãªã„ã€‚**ç’°å¢ƒã®æ§‹é€ ã‚’ç†è§£ã—ã€æœªæ¥ã‚’äºˆæ¸¬ã—ã€è¡Œå‹•ã®çµæœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ã“ã¨**ã ã€‚

```julia
# World Modelã®æœ¬è³ª: 1ãƒ•ãƒ¬ãƒ¼ãƒ  â†’ æœªæ¥ã®äºˆæ¸¬
using Lux, Random

# è¦³æ¸¬ x_t ã‹ã‚‰æ½œåœ¨è¡¨ç¾ z_t ã‚’æŠ½å‡º
encoder = Chain(Conv((3,3), 3 => 64, relu), AdaptiveMeanPool((1,1)), FlattenLayer())

# æ½œåœ¨ç©ºé–“ã§æ¬¡çŠ¶æ…‹ã‚’äºˆæ¸¬ (actionæ¡ä»¶ä»˜ã)
predictor = Dense(64 + 4 => 64, tanh)  # 4æ¬¡å…ƒaction space

# åˆæœŸè¦³æ¸¬
x = rand(Float32, 64, 64, 3, 1)
a = rand(Float32, 4, 1)  # action

# æ½œåœ¨çŠ¶æ…‹æŠ½å‡º â†’ actionæ¡ä»¶ä»˜ãäºˆæ¸¬
z = encoder(x, ps, st)[1]
z_next = predictor(vcat(z, a), ps_pred, st_pred)[1]

# å‡ºåŠ›: z_next âˆˆ â„^64 (predicted next latent state)
```

**ã“ã‚ŒãŒä½•ã‚’ã—ã¦ã„ã‚‹ã‹ï¼Ÿ**

1ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¦³æ¸¬$x_t$ã‚’æ½œåœ¨è¡¨ç¾$z_t$ã«åœ§ç¸®ã—ã€action $a_t$ã‚’ä¸ãˆã¦æ¬¡çŠ¶æ…‹$z_{t+1}$ã‚’äºˆæ¸¬ã™ã‚‹ã€‚

ãƒ”ã‚¯ã‚»ãƒ«ã¯ç”Ÿæˆã—ãªã„ã€‚**ä¸–ç•Œã®æ½œåœ¨æ§‹é€ ã‚’äºˆæ¸¬ã™ã‚‹ã€‚**

$$
z_{t+1} = f_\theta(z_t, a_t)
$$

ã“ã‚ŒãŒWorld Modelã®æ•°å­¦ã ã€‚

> **Note:** **é€²æ—**: å…¨ä½“ã®3%å®Œäº†ã€‚Consistency Modelsã§1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆã‚’å®Ÿç¾ã—ãŸãŒã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çœŸã®ç›®çš„ã¯ã€Œç†è§£ã€ã ã£ãŸã€‚ç’°å¢ƒã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ç†è«–ã¸ã€‚

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” World Modelsã®3ã¤ã®é¡”

### 1.1 ç”Ÿæˆ vs ç†è§£ vs ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯3ã¤ã®ãƒ¬ãƒ™ãƒ«ã«åˆ†é¡ã§ãã‚‹:

| ãƒ¬ãƒ™ãƒ« | ç›®çš„ | å…¥å‡ºåŠ› | ä»£è¡¨æ‰‹æ³• |
|:------|:-----|:------|:---------|
| **Level 1: ç”Ÿæˆ** | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ« | $p(x)$ | VAE, GAN, Diffusion |
| **Level 2: æ¡ä»¶ä»˜ãç”Ÿæˆ** | æ¡ä»¶ã‹ã‚‰ç”Ÿæˆ | $p(x|c)$ | LDM, CFG |
| **Level 3: World Models** | **ç’°å¢ƒã®ç†è§£+äºˆæ¸¬+ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | $p(x_{t+1}|x_{\leq t}, a_t)$ | JEPA, V-JEPA, Transfusion |

World Modelsã¯**è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã§ãã‚‹**æœ€é«˜ãƒ¬ãƒ™ãƒ«ã ã€‚

### 1.2 JEPAã®3å¤‰ç¨®ã‚’å‹•ã‹ã™


### 1.3 World Modelsã®å¿œç”¨é ˜åŸŸ

| å¿œç”¨ | ç›®çš„ | World Modelã®å½¹å‰² |
|:-----|:-----|:-----------------|
| **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹** | ç’°å¢ƒæ“ä½œ | è¡Œå‹•çµæœã®äº‹å‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ |
| **è‡ªå‹•é‹è»¢** | äºˆæ¸¬åˆ¶å¾¡ | ä»–è»Šãƒ»æ­©è¡Œè€…ã®æœªæ¥è»Œé“äºˆæ¸¬ |
| **å¼·åŒ–å­¦ç¿’** | ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚° | Model-based RL (MuZero, Dreamer) |
| **ç§‘å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | ç‰©ç†æ³•å‰‡å­¦ç¿’ | å¾®åˆ†æ–¹ç¨‹å¼ã‚’å­¦ç¿’ã§è¿‘ä¼¼ |

<details><summary>PyTorchã¨ã®å¯¾å¿œï¼ˆå‚è€ƒï¼‰</summary>

Juliaã§ã¯å‹ã‚·ã‚¹ãƒ†ãƒ ã§ã“ã‚Œã‚’è‡ªç„¶ã«è¡¨ç¾ã§ãã‚‹ã€‚

</details>

> **Note:** **é€²æ—**: å…¨ä½“ã®10%å®Œäº†ã€‚World Modelsã®3ãƒ¬ãƒ™ãƒ«åˆ†é¡ã‚’ç†è§£ã—ãŸã€‚JEPAã¯ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬ã™ã‚‹é©å‘½çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã ã€‚

---


> Progress: 10%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $p(x)$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœWorld ModelsãŒæœ€çµ‚åˆ°é”ç‚¹ã‹

### 2.1 ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–ç³»è­œ

```mermaid
graph TD
    A[VAE: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«] --> B[GAN: æ•µå¯¾çš„å­¦ç¿’]
    B --> C[Diffusion: ã‚¹ã‚³ã‚¢é–¢æ•°]
    C --> D[LDM: æ½œåœ¨ç©ºé–“æ‹¡æ•£]
    D --> E[Consistency Models: 1-stepç”Ÿæˆ]
    E --> F[World Models: ç’°å¢ƒç†è§£]

    style F fill:#ff9,stroke:#333,stroke-width:4px
```

**ãªãœWorld ModelsãŒæœ€çµ‚å½¢æ…‹ã‹ï¼Ÿ**

1. **ç”Ÿæˆã¯æ‰‹æ®µã€ç†è§£ãŒç›®çš„**: ç”»åƒç”Ÿæˆã¯ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ä¸€éƒ¨ã‚’ã‚µãƒ³ãƒ—ãƒ«ã™ã‚‹ã ã‘ã€‚World Modelsã¯ç’°å¢ƒã®**å› æœæ§‹é€ **ã‚’ç†è§£ã™ã‚‹
2. **è¡Œå‹•æ¡ä»¶ä»˜ãäºˆæ¸¬**: $p(x_{t+1}|x_{\leq t}, a_t)$ â€” è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã§ãã‚‹
3. **ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒªãƒ¼**: ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’å›é¿ã—ã€æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬ã™ã‚‹åŠ¹ç‡æ€§

### 2.2 Course IVã§ã®ä½ç½®ã¥ã‘

| å› | ãƒ†ãƒ¼ãƒ | World Modelsã¸ã®æ¥ç¶š |
|:---|:------|:--------------------|
| **ç¬¬33å›** | Normalizing Flows | å¯é€†å¤‰æ› â†’ æ±ºå®šè«–çš„å†™åƒã®é™ç•Œ |
| **ç¬¬34å›** | EBM | ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° â†’ **Energy-based World Models** |
| **ç¬¬35å›** | Score Matching | ã‚¹ã‚³ã‚¢é–¢æ•° â†’ å‹•çš„éç¨‹ã®å­¦ç¿’ |
| **ç¬¬36å›** | DDPM | Forward/Reverse â†’ æ™‚ç³»åˆ—äºˆæ¸¬ã®åŸºç›¤ |
| **ç¬¬37å›** | SDE/ODE | é€£ç¶šæ™‚é–“ç¢ºç‡éç¨‹ â†’ ç‰©ç†æ³•å‰‡å­¦ç¿’ |
| **ç¬¬38å›** | Flow Matching | OTè¦–ç‚¹ â†’ **æœ€é©è¼¸é€ã¨ã—ã¦ã®World Models** |
| **ç¬¬39å›** | LDM | æ½œåœ¨ç©ºé–“æ‹¡æ•£ â†’ **æ½œåœ¨ç©ºé–“äºˆæ¸¬** |
| **ç¬¬40å›** | Consistency Models | 1-stepç”Ÿæˆ â†’ é«˜é€Ÿæ¨è«– |
| **ç¬¬41å›** | **World Models** | **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹** |

### 2.3 æ¾å°¾ç ”ã¨ã®æ±ºå®šçš„ãªé•ã„

| é …ç›® | æ¾å°¾ç ” | æœ¬è¬›ç¾© |
|:-----|:------|:------|
| **World Modelsæ‰±ã„** | è¨€åŠãªã— | **å®Œå…¨ç†è«–åŒ–** |
| **JEPA** | è§¦ã‚Œãªã„ | I-JEPA / V-JEPA / VL-JEPAå®Œå…¨è§£èª¬ |
| **Transfusion** | æ‰±ã‚ãªã„ | **AR+Diffusionçµ±ä¸€ç†è«–ã®æ•°å­¦** |
| **ç‰©ç†æ³•å‰‡å­¦ç¿’** | æ‰±ã‚ãªã„ | Physics-Informed World Modelsæ·±æ˜ã‚Š |
| **å®Ÿè£…** | ãªã— | Julia JEPAã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè£… |

### 2.4 å­¦ç¿’æˆ¦ç•¥

```mermaid
graph LR
    A[ç¬¬41å›: ç†è«–ç†è§£] --> B[ç¬¬42å›: çµ±ä¸€ç†è«–]
    A --> C[ç¬¬43å› DiT/FLUXå®Ÿè£…]
    A --> D[ç¬¬45å› Videoç”Ÿæˆ]
    A --> E[ç¬¬47å› Embodied AI]

    style A fill:#f9f,stroke:#333,stroke-width:2px
```

World Modelsã¯**å…¨ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆç”»åƒãƒ»å‹•ç”»ãƒ»ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ãƒ»ç§‘å­¦ï¼‰ã®çµ±ä¸€åŸºç›¤**ã ã€‚

<details><summary>Trojan Horse â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®"æœ¬å½“ã®ç›®çš„"</summary>

ç¬¬1å›ã‹ã‚‰38å›ã¾ã§ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€Œç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã€æŠ€è¡“ã¨ã—ã¦å­¦ã‚“ã§ããŸã€‚

ã ãŒLeCunãŒæå”±ã™ã‚‹JEPAã¯**ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹**ã€‚

**ç”Ÿæˆã¯å‰¯ç”£ç‰©ã«éããªã‹ã£ãŸ**ã€‚çœŸã®ç›®çš„ã¯**ç’°å¢ƒã®å› æœæ§‹é€ ã‚’ç†è§£ã—ã€è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨**ã ã€‚

ã“ã‚ŒãŒTrojan Horseã®æœ€çµ‚å½¢æ…‹ã ã€‚ã€Œç”ŸæˆAIã€ã¯ã€Œç†è§£AIã€ã«é€²åŒ–ã™ã‚‹ã€‚

</details>

> **Note:** **é€²æ—**: å…¨ä½“ã®20%å®Œäº†ã€‚ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–ç³»è­œã‚’ç†è§£ã—ãŸã€‚World Modelsã¯ç”Ÿæˆã®å…ˆã«ã‚ã‚‹ã€Œç†è§£+äºˆæ¸¬+ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ã®çµ±åˆæ¦‚å¿µã ã€‚

---


> Progress: 20%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $p(x)$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” World Modelsã®æ•°å­¦çš„åŸºç¤

### 3.1 World Modelã®å®šç¾©

**å®šç¾©**: World Model $\mathcal{M}$ã¯ç’°å¢ƒã®æ½œåœ¨è¡¨ç¾$z_t$ã¨é·ç§»é–¢æ•°$f_\theta$ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ç¢ºç‡çš„ã‚·ã‚¹ãƒ†ãƒ ã§ã‚ã‚‹ã€‚

$$
\begin{aligned}
\text{Encoder: } & z_t = \text{Enc}_\phi(x_t) \\
\text{Predictor: } & z_{t+1} = f_\theta(z_t, a_t) + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \Sigma) \\
\text{Decoder: } & \hat{x}_{t+1} = \text{Dec}_\psi(z_{t+1})
\end{aligned}
$$

**ãªãœæ½œåœ¨ç©ºé–“ã‹ï¼Ÿ**

- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ $x \in \mathbb{R}^{H \times W \times C}$ ã¯é«˜æ¬¡å…ƒï¼ˆ$H=256, W=256, C=3 \Rightarrow 196,608$æ¬¡å…ƒï¼‰
- æ½œåœ¨ç©ºé–“ $z \in \mathbb{R}^d$ ã¯ä½æ¬¡å…ƒï¼ˆ$d=256$ç¨‹åº¦ï¼‰ã§**æ§‹é€ çš„è¡¨ç¾**ã‚’ç²å¾—

**è¨“ç·´ç›®æ¨™**: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\{(x_t, a_t, x_{t+1})\}_{t=1}^T$ ã‹ã‚‰$\theta, \phi, \psi$ã‚’å­¦ç¿’

$$
\mathcal{L}(\theta, \phi, \psi) = \mathbb{E}_{(x_t, a_t, x_{t+1})} \left[ \| \text{Dec}_\psi(f_\theta(\text{Enc}_\phi(x_t), a_t)) - x_{t+1} \|_2^2 \right]
$$

### 3.2 JEPAç†è«–: Joint-Embedding Predictive Architecture

#### 3.2.1 I-JEPA (Image-based JEPA)

**è«–æ–‡**: Assran et al., "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture," CVPR 2023

**å‹•æ©Ÿ**: ç”»åƒç”Ÿæˆï¼ˆpixel reconstructionï¼‰ã¯ä½ãƒ¬ãƒ™ãƒ«è©³ç´°ã«éå‰°é©åˆã—ã€é«˜ãƒ¬ãƒ™ãƒ«æŠ½è±¡è¡¨ç¾ã‚’å­¦ç¿’ã—ã«ãã„ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

```mermaid
graph LR
    A[Context patches<br>x_ctx] --> B[Context Encoder<br>s_Î¸]
    B --> C[Latent z_ctx]
    C --> D[Predictor<br>f_Î¸]
    E[Target patches<br>x_tgt] --> F[Target Encoder<br>s_Î¸ EMA]
    F --> G[Latent z_tgt]
    D --> H[Predicted z_pred]
    H -.loss.-> G

    style D fill:#ff9,stroke:#333,stroke-width:2px
    style H fill:#f99,stroke:#333,stroke-width:2px
```

**æå¤±é–¢æ•°**:

$$
\mathcal{L}_{\text{I-JEPA}} = \mathbb{E}_{x, M} \left[ \| f_\theta(s_\theta(x_{\text{ctx}}), M) - \bar{s}_\theta(x_{\text{tgt}}) \|_2^2 \right]
$$

ã“ã“ã§:
- $x_{\text{ctx}}$: ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„context patches
- $x_{\text{tgt}}$: ãƒã‚¹ã‚¯ã•ã‚ŒãŸtarget patches
- $M$: mask tokens (positional encoding)
- $s_\theta$: context encoder (trainable)
- $\bar{s}_\theta$: target encoder (EMAæ›´æ–°)
- $f_\theta$: predictor

**é‡è¦ãªç‰¹æ€§**:

1. **ãƒ”ã‚¯ã‚»ãƒ«å†æ§‹æˆãªã—**: $x_{\text{tgt}}$ã‚’ç”Ÿæˆã›ãšã€æ½œåœ¨è¡¨ç¾$z_{\text{tgt}}$ã‚’äºˆæ¸¬
2. **EMA target encoder**: $\bar{\theta} \leftarrow \tau \bar{\theta} + (1-\tau)\theta$ ã§collapseå›é¿
3. **Mask strategy**: ãƒ©ãƒ³ãƒ€ãƒ ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¹ã‚¯ï¼ˆGrid-basedï¼‰ã§æ§‹é€ çš„äºˆæ¸¬ã‚’ä¿ƒé€²

**æ•°å€¤ä¾‹**:


#### 3.2.2 V-JEPA (Video JEPA)

**è«–æ–‡**: Bardes et al., "Revisiting Feature Prediction for Learning Visual Representations from Video," arXiv:2404.08471, 2024 (V-JEPA 1.0)
**æœ€æ–°**: "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning," arXiv:2506.09985, 2025

**æ‹¡å¼µ**: ç”»åƒâ†’å‹•ç”»ï¼ˆæ™‚ç©ºé–“äºˆæ¸¬ï¼‰

$$
\begin{aligned}
\text{Context: } & \mathbf{x}_{\text{ctx}} \in \mathbb{R}^{T_c \times H \times W \times C} \\
\text{Target: } & \mathbf{x}_{\text{tgt}} \in \mathbb{R}^{T_t \times H \times W \times C}
\end{aligned}
$$

**Spatio-temporal masking**:

- **Temporal masking**: å‰åŠ8ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆcontextï¼‰â†’å¾ŒåŠ8ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆtargetï¼‰ã‚’äºˆæ¸¬
- **Spatial masking**: å„ãƒ•ãƒ¬ãƒ¼ãƒ å†…ã§ãƒ‘ãƒƒãƒã‚’ãƒã‚¹ã‚¯

**è¨“ç·´ç›®æ¨™**:

$$
\mathcal{L}_{\text{V-JEPA}} = \mathbb{E}_{\mathbf{x}, M_s, M_t} \left[ \| f_\theta(s_\theta(\mathbf{x}_{\text{ctx}}), M_s, M_t) - \bar{s}_\theta(\mathbf{x}_{\text{tgt}}) \|_2^2 \right]
$$

**æ€§èƒ½**:

- Kinetics-400 (action recognition): **81.9%** Top-1 accuracy (video pre-trainingã®ã¿)
- Something-Something v2: **72.2%**
- ImageNet: **77.9%** Top-1 (å‹•ç”»äº‹å‰å­¦ç¿’ã‹ã‚‰ç”»åƒã‚¿ã‚¹ã‚¯ã«è»¢ç§»)

#### 3.2.3 VL-JEPA (Vision-Language JEPA)

**è«–æ–‡**: Bardes et al., "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language," arXiv:2512.10942, 2024

**å‹•æ©Ÿ**: å¾“æ¥ã®VLMï¼ˆVision-Language Modelsï¼‰ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§autoregressiveã«ç”Ÿæˆã™ã‚‹ã€‚ã“ã‚Œã¯è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ãã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚‚è†¨å¤§ï¼ˆdecoderå±¤ãŒå¿…è¦ï¼‰ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

VL-JEPAã¯**ãƒ†ã‚­ã‚¹ãƒˆã®é€£ç¶šåŸ‹ã‚è¾¼ã¿ã‚’äºˆæ¸¬**ã—ã€token-by-tokenç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚

$$
\begin{aligned}
\text{Image encoder: } & z_v = \text{Enc}_v(x) \\
\text{Predictor: } & z_{\text{pred}} = f_\theta(z_v, \text{prompt}) \\
\text{Text encoder: } & z_t = \text{Enc}_t(\text{target text}) \\
\text{Loss: } & \mathcal{L} = \| z_{\text{pred}} - z_t \|_2^2
\end{aligned}
$$

**åˆ©ç‚¹**:

- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒæ¨™æº–VLMã®**50%å‰Šæ¸›**ï¼ˆdecoderãªã—ï¼‰
- **ã‚ˆã‚Šå¼·ã„æ€§èƒ½**: åŒã˜vision encoderã¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã—ãŸæ¨™æº–VLMã‚’ä¸Šå›ã‚‹

### 3.3 Transfusionç†è«–: AR + Diffusionçµ±ä¸€

**è«–æ–‡**: Zhou et al., "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model," arXiv:2408.11039, 2024 (Meta AI)

**å‹•æ©Ÿ**: ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã¨ç”»åƒï¼ˆé€£ç¶šãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’**å˜ä¸€Transformerã§çµ±ä¸€å‡¦ç†**ã—ãŸã„ã€‚

**å¾“æ¥æ‰‹æ³•ã®å•é¡Œ**:

- ç”»åƒã‚’VQ-VAEã§é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³åŒ– â†’ é‡å­åŒ–èª¤å·®ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯åˆ©ç”¨ç‡ä½ä¸‹
- åˆ¥ã€…ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆLM + Diffusionï¼‰â†’ çµ±åˆã§ããªã„

**Transfusionã®è§£æ±ºç­–**:

**åŒä¸€Transformerã§ç•°ãªã‚‹æå¤±é–¢æ•°**ã‚’ä½¿ã„åˆ†ã‘ã‚‹ã€‚

$$
\mathcal{L}_{\text{Transfusion}} = \mathcal{L}_{\text{LM}}(\text{text}) + \lambda \mathcal{L}_{\text{Diffusion}}(\text{image})
$$

#### 3.3.1 ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†: Autoregressive

ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ $\mathbf{t} = (t_1, t_2, \ldots, t_n)$ ã«å¯¾ã—ã¦:

$$
\mathcal{L}_{\text{LM}} = -\sum_{i=1}^n \log p_\theta(t_i | t_{<i})
$$

é€šå¸¸ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¨åŒã˜causal maskingã¨cross-entropy lossã€‚

#### 3.3.2 ç”»åƒéƒ¨åˆ†: Diffusion

ç”»åƒãƒ‘ãƒƒãƒ $\mathbf{x} = (x_1, \ldots, x_m) \in \mathbb{R}^{m \times d}$ ã«å¯¾ã—ã¦:

$$
\begin{aligned}
\text{Forward: } & x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I) \\
\text{Diffusion Loss: } & \mathcal{L}_{\text{Diffusion}} = \mathbb{E}_{t, \epsilon} \left[ \| \epsilon - \epsilon_\theta(\mathbf{x}_t, t, \mathbf{c}) \|_2^2 \right]
\end{aligned}
$$

ã“ã“ã§ $\mathbf{c}$ ã¯ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ï¼ˆcross-attentionçµŒç”±ã§Transformerã«æ³¨å…¥ï¼‰ã€‚

#### 3.3.3 çµ±åˆå‡¦ç†ã®æ•°å­¦

å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹:

$$
\text{seq} = [\text{text tokens } t_1, \ldots, t_n, \text{ image patches } x_1, \ldots, x_m]
$$

**Attention mask**:

- ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†: **causal mask**ï¼ˆæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„ï¼‰
- ç”»åƒéƒ¨åˆ†: **bidirectional mask**ï¼ˆå…¨ãƒ‘ãƒƒãƒã‚’è¦‹ã‚‹ï¼‰

**æå¤±è¨ˆç®—**:


**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°çµæœ**: 7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€2T multi-modal tokensã§è¨“ç·´ â†’ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨ç”»åƒç”Ÿæˆã®ä¸¡æ–¹ã§åŒè¦æ¨¡ã®å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰æ€§èƒ½ã€‚

### 3.4 ç‰©ç†æ³•å‰‡å­¦ç¿’ç†è«–

#### 3.4.1 Physics-Informed World Models

**å‹•æ©Ÿ**: æ¨™æº–çš„ãªWorld Modelsã¯ç‰©ç†æ³•å‰‡ï¼ˆä¿å­˜å‰‡ã€å¯¾ç§°æ€§ã€å¾®åˆ†æ–¹ç¨‹å¼ï¼‰ã‚’ç„¡è¦–ã—ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•ã§å­¦ç¿’ã™ã‚‹ã€‚ã“ã‚Œã¯:

- ç‰©ç†çš„ã«ä¸å¯èƒ½ãªäºˆæ¸¬ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜å‰‡é•åãªã©ï¼‰
- ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã®æ‚ªã•ï¼ˆç‰©ç†æ³•å‰‡ã‚’çŸ¥ã£ã¦ã„ã‚Œã°å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’å¯èƒ½ï¼‰

**Physics-Informed Neural Networks (PINNs)ã®åŸç†**:

å¾®åˆ†æ–¹ç¨‹å¼åˆ¶ç´„ã‚’æå¤±é–¢æ•°ã«åŸ‹ã‚è¾¼ã‚€ã€‚

ä¾‹: Navier-Stokesæ–¹ç¨‹å¼

$$
\frac{\partial \mathbf{u}}{\partial t} + (\mathbf{u} \cdot \nabla)\mathbf{u} = -\frac{1}{\rho}\nabla p + \nu \nabla^2 \mathbf{u}
$$

**PINNs loss**:

$$
\mathcal{L}_{\text{PINN}} = \mathcal{L}_{\text{data}} + \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}}
$$

$$
\mathcal{L}_{\text{PDE}} = \mathbb{E}_{x,t} \left[ \left\| \frac{\partial \mathbf{u}_\theta}{\partial t} + (\mathbf{u}_\theta \cdot \nabla)\mathbf{u}_\theta + \frac{1}{\rho}\nabla p_\theta - \nu \nabla^2 \mathbf{u}_\theta \right\|_2^2 \right]
$$

**World Modelsã¸ã®é©ç”¨**:

$$
\mathcal{L}_{\text{Physics-WM}} = \mathcal{L}_{\text{prediction}} + \lambda_{\text{conservation}} \mathcal{L}_{\text{conservation}}
$$

$$
\mathcal{L}_{\text{conservation}} = \mathbb{E} \left[ \| E(z_{t+1}) - E(z_t) \|_2^2 \right]
$$

ã“ã“ã§$E(z)$ã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ï¼ˆå­¦ç¿’ã¾ãŸã¯æ—¢çŸ¥ï¼‰ã€‚

#### 3.4.2 ä¿å­˜å‰‡ã®åŸ‹ã‚è¾¼ã¿

**é‹å‹•é‡ä¿å­˜**:

$$
\sum_{i=1}^N m_i \mathbf{v}_i(t) = \text{const}
$$

**Graph Neural Networkã§ã®å®Ÿè£…**:

ãƒãƒ¼ãƒ‰$i$ã®é€Ÿåº¦$\mathbf{v}_i$ã«å¯¾ã—ã¦ã€edge $(i,j)$ã®æ›´æ–°:

$$
\Delta \mathbf{v}_i = \sum_{j \in \mathcal{N}(i)} \text{MLP}(\mathbf{h}_i, \mathbf{h}_j, \mathbf{r}_{ij})
$$

**ä¿å­˜å‰‡åˆ¶ç´„**: å„edgeæ›´æ–°ãŒé‹å‹•é‡ä¿å­˜ã‚’æº€ãŸã™ã‚ˆã†ã«ã€**Newton's third law**ã‚’æ˜ç¤ºçš„ã«é©ç”¨:

$$
m_i \Delta \mathbf{v}_i = -m_j \Delta \mathbf{v}_j
$$

**å®Ÿè£…**:

#### LagrangianåŠ›å­¦ã«ã‚ˆã‚‹ä¿å­˜å‰‡ã®å³å¯†åŸ‹ã‚è¾¼ã¿

é‹å‹•é‡ä¿å­˜ã‚’GNNã«çµ„ã¿è¾¼ã‚€ç†è«–çš„åŸºç›¤ã¨ã—ã¦ã€**LagrangianåŠ›å­¦**ã‹ã‚‰ã®å®šå¼åŒ–ãŒå¼·åŠ›ã§ã‚ã‚‹ã€‚ç³»ã®çŠ¶æ…‹ã‚’ä¸€èˆ¬åŒ–åº§æ¨™$\mathbf{q} = (q_1, \ldots, q_n)$ã¨ä¸€èˆ¬åŒ–é€Ÿåº¦$\dot{\mathbf{q}} = (\dot{q}_1, \ldots, \dot{q}_n)$ã§è¨˜è¿°ã™ã‚‹ã¨ãã€Lagrangian $L$ã¯

$$
L(\mathbf{q}, \dot{\mathbf{q}}) = T(\dot{\mathbf{q}}) - V(\mathbf{q})
$$

ã“ã“ã§$T$ã¯é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€$V$ã¯ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€‚é‹å‹•æ–¹ç¨‹å¼ã¯Euler-Lagrangeæ–¹ç¨‹å¼ã‹ã‚‰å°ã‹ã‚Œã‚‹:

$$
\frac{d}{dt}\frac{\partial L}{\partial \dot{q}_i} - \frac{\partial L}{\partial q_i} = 0, \quad i = 1, \ldots, n
$$

è³ªç‚¹ç³»ã§ã¯$T = \frac{1}{2} \sum_i m_i \|\dot{\mathbf{r}}_i\|^2$ã€$V = \sum_{i < j} V_{ij}(\|\mathbf{r}_i - \mathbf{r}_j\|)$ã¨ã™ã‚‹ã¨ã€ä¸Šå¼ã¯ç›´ã¡ã«é‹å‹•é‡ä¿å­˜ã¨ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ã‚’**ä»£æ•°çš„ã«å«æ„**ã™ã‚‹ã€‚

**StÃ¶rmer-Verletæ³•ã«ã‚ˆã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜æ•°å€¤ç©åˆ†**

é€šå¸¸ã®Euleræ³•ã§ã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã§$O(h^2)$ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼èª¤å·®ãŒè“„ç©ã—ã€é•·æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒç™ºæ•£ã™ã‚‹ã€‚StÃ¶rmer-Verletæ³•ã¯**ã‚·ãƒ³ãƒ—ãƒ¬ã‚¯ãƒ†ã‚£ãƒƒã‚¯ç©åˆ†å™¨**ã§ã‚ã‚Šã€é›¢æ•£è»Œé“ãŒé€£ç¶šLagrangianã®ä¿å­˜é‡ã‚’å³å¯†ã«ä¿æŒã™ã‚‹ã€‚æ›´æ–°å‰‡ã¯:

$$
\mathbf{q}_{t+1} = \mathbf{q}_t + h\dot{\mathbf{q}}_t + \frac{h^2}{2}\mathbf{M}^{-1}\mathbf{F}(\mathbf{q}_t)
$$

$$
\dot{\mathbf{q}}_{t+1} = \dot{\mathbf{q}}_t + \frac{h}{2}\mathbf{M}^{-1}\left[\mathbf{F}(\mathbf{q}_t) + \mathbf{F}(\mathbf{q}_{t+1})\right]
$$

ã“ã“ã§$\mathbf{M}$ã¯è³ªé‡è¡Œåˆ—ã€$\mathbf{F}(\mathbf{q}) = -\nabla_{\mathbf{q}} V(\mathbf{q})$ã¯ä¿å­˜åŠ›ã€‚ã“ã®æ–¹æ³•ã§ã¯é›¢æ•£ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³$\tilde{H}$ãŒ**é™°çš„ã«ä¿å­˜**ã•ã‚Œã‚‹ï¼ˆmodified Hamiltonianå®šç†ï¼‰:

$$
\left|\tilde{H}(\mathbf{q}_t, \dot{\mathbf{q}}_t) - \tilde{H}(\mathbf{q}_0, \dot{\mathbf{q}}_0)\right| \leq C h^2, \quad \forall t
$$

ã™ãªã‚ã¡ã‚¨ãƒãƒ«ã‚®ãƒ¼èª¤å·®ã¯æ™‚é–“ã«ä¾ã‚‰ãš$O(h^2)$ã«**æœ‰ç•Œ**ã§ã‚ã‚‹ï¼ˆEuleræ³•ã®$O(th^2)$ã¨ã¯æœ¬è³ªçš„ã«ç•°ãªã‚‹ï¼‰ã€‚

**E(3)-equivariant Graph Neural Networks**

ç‰©ç†ç³»ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«GNNã‚’ä½¿ã†å ´åˆã€åº§æ¨™å¤‰æ›ã«å¯¾ã™ã‚‹**ç­‰å¤‰æ€§ï¼ˆequivarianceï¼‰**ãŒç†è«–çš„ä¿è¨¼ã‚’ä¸ãˆã‚‹ã€‚$E(3)$ç¾¤ã¯3æ¬¡å…ƒãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰å¤‰æ›ï¼ˆå›è»¢$R \in SO(3)$ã€åè»¢$I$ã€å¹³è¡Œç§»å‹•$\mathbf{t}$ï¼‰ã®ç¾¤ã§ã‚ã‚‹ã€‚

ã‚¹ã‚«ãƒ©ãƒ¼ç‰¹å¾´é‡$h_i \in \mathbb{R}^d$ã¨ãƒ™ã‚¯ãƒˆãƒ«ç‰¹å¾´é‡$\mathbf{v}_i \in \mathbb{R}^3$ã‚’æŒã¤ã‚°ãƒ©ãƒ•ã«ãŠã„ã¦ã€E(3)-equivariant ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã¯:

$$
\mathbf{m}_{ij} = \phi_m\left(h_i, h_j, \|\mathbf{r}_{ij}\|^2, \mathbf{v}_i \cdot \mathbf{v}_j\right)
$$

$$
\mathbf{v}_i^{\text{new}} = \sum_{j \in \mathcal{N}(i)} \mathbf{r}_{ij} \cdot \phi_v(\|\mathbf{r}_{ij}\|^2, h_i, h_j)
$$

ã“ã“ã§$\mathbf{r}_{ij} = \mathbf{r}_i - \mathbf{r}_j$ã€$\phi_m, \phi_v$ã¯MLPã§ã‚ã‚‹ã€‚ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šå›è»¢å¤‰æ›$R$ã«å¯¾ã—ã¦:

$$
\text{GNN}(R\mathbf{r}_1, \ldots, R\mathbf{r}_N) = R \cdot \text{GNN}(\mathbf{r}_1, \ldots, \mathbf{r}_N)
$$

ãŒ**å³å¯†ã«æˆç«‹**ã—ã€åŠ›ã®äºˆæ¸¬ãŒåº§æ¨™ç³»ã®é¸æŠã«ä¾å­˜ã—ãªã„ã€‚å¹³è¡Œç§»å‹•ä¸å¤‰æ€§ã¯$\mathbf{r}_{ij}$ï¼ˆç›¸å¯¾åº§æ¨™ï¼‰ã®ã¿ã‚’å…¥åŠ›ã«ä½¿ã†ã“ã¨ã§è‡ªå‹•çš„ã«ä¿è¨¼ã•ã‚Œã‚‹ã€‚

**Steerable Equivariant GNNs (SEGNN) ã®é«˜æ¬¡ç­‰å¤‰æ€§**

EGNNï¼ˆE(3)-equivariant GNNï¼‰ã¯1éšãƒ†ãƒ³ã‚½ãƒ«ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã®ã¿ã‚’æ‰±ã†ãŒã€SEGNNã¯**ä»»æ„æ¬¡æ•°ã®çƒé¢èª¿å’Œé–¢æ•°**$Y_l^m$ï¼ˆ$l = 0, 1, 2, \ldots$ï¼‰ã‚’åŸºåº•ã¨ã™ã‚‹é«˜æ¬¡ç­‰å¤‰ç‰¹å¾´é‡ã‚’æ‰±ã†ã€‚æ¬¡æ•°$l$ã®ç­‰å¤‰ç‰¹å¾´é‡$\mathbf{f}^{(l)} \in \mathbb{R}^{2l+1}$ã¯$SO(3)$ã®æ—¢ç´„è¡¨ç¾$D^{(l)}(R)$ã§å¤‰æ›ã™ã‚‹:

$$
\mathbf{f}^{(l)} \mapsto D^{(l)}(R)\, \mathbf{f}^{(l)}
$$

ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯çƒé¢èª¿å’Œé–¢æ•°ã®**ãƒ†ãƒ³ã‚½ãƒ«ç©ï¼ˆCGç©ï¼‰**ã‚’ç”¨ã„ã¦æ§‹æˆã™ã‚‹:

$$
\mathbf{m}_{ij}^{(l_{\text{out}})} = \sum_{l_1, l_2} C^{l_{\text{out}}}_{l_1 l_2} \left(\mathbf{f}_i^{(l_1)} \otimes_{CG} \mathbf{f}_j^{(l_2)}\right) \cdot \phi\!\left(\|\mathbf{r}_{ij}\|\right)
$$

ã“ã“ã§$C^{l_{\text{out}}}_{l_1 l_2}$ã¯Clebsch-Gordanä¿‚æ•°ï¼ˆ$SO(3)$ã®æ—¢ç´„è¡¨ç¾ã®ç©å‰‡ï¼‰ã€‚åˆ†å­å‹•åŠ›å­¦ã«é©ç”¨ã™ã‚‹ã¨$l=2$ï¼ˆäºŒéšãƒ†ãƒ³ã‚½ãƒ«ã€å››æ¥µå­ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆç­‰ï¼‰ã¾ã§è€ƒæ…®ã™ã‚‹ã“ã¨ã§ã€å¾“æ¥ã®EGNNã¨æ¯”ã¹ã¦ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼é¢ã®ç²¾åº¦ãŒç´„30%å‘ä¸Šã™ã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚Œã¦ã„ã‚‹ï¼ˆThomas et al., 2018ï¼‰ã€‚

#### 3.4.3 Hamiltonian Neural Networks

**HamiltonianåŠ›å­¦ç³»**:

$$
\begin{aligned}
\dot{q} &= \frac{\partial H}{\partial p} \\
\dot{p} &= -\frac{\partial H}{\partial q}
\end{aligned}
$$

ã“ã“ã§$H(q, p)$ã¯Hamiltonianï¼ˆç·ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰ã€‚

**HNNã®å­¦ç¿’**:

1. NNã§$H_\theta(q, p)$ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–
2. è‡ªå‹•å¾®åˆ†ã§$\partial H / \partial p, \partial H / \partial q$ã‚’è¨ˆç®—
3. æå¤±:

$$
\mathcal{L}_{\text{HNN}} = \mathbb{E} \left[ \left\| \left(\dot{q}, \dot{p}\right) - \left(\frac{\partial H_\theta}{\partial p}, -\frac{\partial H_\theta}{\partial q}\right) \right\|_2^2 \right]
$$

**åˆ©ç‚¹**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜å‰‡ãŒ**æ§‹é€ çš„ã«ä¿è¨¼**ã•ã‚Œã‚‹ï¼ˆHamiltonianã®æ™‚é–“å¾®åˆ†ãŒ0ï¼‰ã€‚

**HNNã®ç†è«–çš„ä¿è¨¼: ã‚·ãƒ³ãƒ—ãƒ¬ã‚¯ãƒ†ã‚£ãƒƒã‚¯æ§‹é€ ã®ä¿æŒ**

HNNãŒç”Ÿæˆã™ã‚‹ãƒ•ãƒ­ãƒ¼ã¯$2n$æ¬¡å…ƒä½ç›¸ç©ºé–“ã«**ã‚·ãƒ³ãƒ—ãƒ¬ã‚¯ãƒ†ã‚£ãƒƒã‚¯æ§‹é€ **$\omega = \sum_i dq_i \wedge dp_i$ã‚’ä¿æŒã™ã‚‹ã€‚ã“ã‚Œã¯Liouvilleå®šç†ï¼ˆä½ç›¸ç©ºé–“ä½“ç©ã®ä¿å­˜ï¼‰ã®ä»£æ•°çš„è¡¨ç¾ã§ã‚ã‚Š:

$$
\frac{d}{dt}\int_{\Omega_0} d\mathbf{q}\, d\mathbf{p} = 0
$$

ãŒä»»æ„ã®é ˜åŸŸ$\Omega_0$ã«ã¤ã„ã¦æˆç«‹ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚æ•°å€¤çš„ã«ã¯ã‚·ãƒ³ãƒ—ãƒ¬ã‚¯ãƒ†ã‚£ãƒƒã‚¯ç©åˆ†å™¨ï¼ˆStÃ¶rmer-Verletç­‰ï¼‰ã¨ã®çµ„ã¿åˆã‚ã›ãŒå¿…é ˆã§ã‚ã‚Šã€Runge-Kuttaæ³•ã‚’ä½¿ã†ã¨ã‚·ãƒ³ãƒ—ãƒ¬ã‚¯ãƒ†ã‚£ãƒƒã‚¯æ€§ãŒç ´ã‚Œã¦ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒé•·æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã§ãƒ‰ãƒªãƒ•ãƒˆã™ã‚‹ã€‚

HNNãŒå­¦ç¿’ã™ã‚‹Hamiltonian $H_\theta(q, p)$ãŒçœŸã®Hamiltonian $H^*(q, p)$ã‚’æ­£ç¢ºã«è¿‘ä¼¼ã—ã¦ã„ã‚‹å ´åˆã€ç”Ÿæˆã•ã‚Œã‚‹è»Œé“ã¯ä»¥ä¸‹ã®æ„å‘³ã§æœ€é©ã§ã‚ã‚‹:

$$
\left\| z(t) - z^*(t) \right\| \leq C \cdot \left\| H_\theta - H^* \right\|_{\infty} \cdot t
$$

ã¤ã¾ã‚ŠHamiltonianã®è¿‘ä¼¼èª¤å·®ãŒ$\varepsilon$ã®ã¨ãã€è»Œé“èª¤å·®ã¯æ™‚é–“$t$ã«å¯¾ã—ã¦ç·šå½¢ã«ã—ã‹å¢—å¤§ã—ãªã„ã€‚ã“ã‚Œã¯ä¸€èˆ¬ã®å›å¸°ãƒ™ãƒ¼ã‚¹ã®world modelï¼ˆèª¤å·®ãŒæŒ‡æ•°å¢—å¤§ï¼‰ã¨æ¯”ã¹ã¦**æœ¬è³ªçš„ã«å„ªã‚ŒãŸé•·æœŸå®‰å®šæ€§**ã‚’ç¤ºã™ã€‚

**Symplectic World Modelsã¨ã®æ¯”è¼ƒ**

| ãƒ¢ãƒ‡ãƒ« | ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ | é•·æœŸå®‰å®šæ€§ | é©ç”¨ç³» |
|--------|--------------|-----------|--------|
| æ¨™æº–LSTM/RNN | ãªã— | æŒ‡æ•°çš„èª¤å·®å¢—å¤§ | ä»»æ„ |
| HNN | æ§‹é€ çš„ä¿è¨¼ | ç·šå½¢èª¤å·®å¢—å¤§ | ä¿å­˜åŠ›å­¦ç³» |
| LNN (3.9.3) | Noetherã§ä¿è¨¼ | ç·šå½¢èª¤å·®å¢—å¤§ | ä»»æ„ä¸€èˆ¬åŒ–åº§æ¨™ |
| Symplectic RNN | è¿‘ä¼¼çš„ä¿è¨¼ | å¤šé …å¼èª¤å·®å¢—å¤§ | ä¿å­˜åŠ›å­¦ç³» |

å®Ÿä¸–ç•Œã¸ã®é©ç”¨ã«ãŠã„ã¦ã¯ã€ç´”ç²‹ãªä¿å­˜åŠ›å­¦ç³»ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒå³å¯†ã«ä¿å­˜ï¼‰ã¯ç†æƒ³åŒ–ã§ã‚ã‚Šã€æ‘©æ“¦ãƒ»æ•£é€¸ãƒ»å¤–åŠ›ãŒå­˜åœ¨ã™ã‚‹ç³»ã§ã¯HNNã®ä»®å®šãŒç ´ã‚Œã‚‹ã€‚ã“ã†ã—ãŸç³»ã«ã¯**Port-Hamiltonian Systems (PHS)** ã¸ã®æ‹¡å¼µãŒæœ‰åŠ¹ã§ã‚ã‚Šã€æ•£é€¸é …$R(q,p) \geq 0$ã¨å¤–åŠ›å…¥åŠ›$u$ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹:

$$
\begin{pmatrix} \dot{q} \\ \dot{p} \end{pmatrix} = \left(J - R\right) \nabla H + B\, u
$$

ã“ã“ã§$J = \bigl(\begin{smallmatrix} 0 & I \\ -I & 0 \end{smallmatrix}\bigr)$ï¼ˆã‚·ãƒ³ãƒ—ãƒ¬ã‚¯ãƒ†ã‚£ãƒƒã‚¯è¡Œåˆ—ï¼‰ã€$R \succeq 0$ï¼ˆæ•£é€¸è¡Œåˆ—ï¼‰ã€$B$ã¯å…¥åŠ›è¡Œåˆ—ã€‚$u = 0$ã‹ã¤$R = 0$ã®ã¨ãå¤å…¸çš„HNNã«å¸°ç€ã™ã‚‹ã€‚Port-HNNï¼ˆPHNNï¼‰ã¯ãƒ­ãƒœãƒƒãƒˆã®é–¢ç¯€åˆ¶å¾¡ï¼ˆãƒãƒ-ãƒ€ãƒ³ãƒ‘ãƒ¼ç³»ï¼‰ã‚„æµä½“åŠ›å­¦ã«ãŠã‘ã‚‹ä¸–ç•Œãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã«å¿œç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚

### 3.5 Energy-based World Models

**ç¬¬34å›EBMã¨ã®æ¥ç¶š**:

World Modelsã‚’**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°$E_\theta(z_t, a_t, z_{t+1})$**ã¨ã—ã¦å®šå¼åŒ–:

$$
p(z_{t+1} | z_t, a_t) = \frac{\exp(-E_\theta(z_t, a_t, z_{t+1}))}{Z(z_t, a_t)}
$$

**åˆ©ç‚¹**:

- ä»»æ„ã®åˆ†å¸ƒå½¢çŠ¶ã‚’è¡¨ç¾å¯èƒ½ï¼ˆGaussianã«åˆ¶ç´„ã•ã‚Œãªã„ï¼‰
- ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– = æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„æœªæ¥çŠ¶æ…‹

**è¨“ç·´**: Contrastive Divergence (CD-k) ã¾ãŸã¯ Score Matching

$$
\nabla_\theta \mathcal{L} = \mathbb{E}_{z_t, a_t, z_{t+1}^{+}} [\nabla_\theta E_\theta(z_t, a_t, z_{t+1}^{+})] - \mathbb{E}_{z_t, a_t, z_{t+1}^{-}} [\nabla_\theta E_\theta(z_t, a_t, z_{t+1}^{-})]
$$

ã“ã“ã§$z_{t+1}^{+}$ã¯ãƒ‡ãƒ¼ã‚¿ã€$z_{t+1}^{-}$ã¯Langevin dynamicsã§ã‚µãƒ³ãƒ—ãƒ«ã€‚

#### Contrastive Divergence (CD-k) ã®å®Œå…¨å°å‡º

EBMã®è¨“ç·´ç›®çš„ã¯è² ã®å¯¾æ•°å°¤åº¦ã®æœ€å°åŒ–ã§ã‚ã‚‹:

$$
\mathcal{L}_{\text{EBM}} = -\mathbb{E}_{z^+ \sim p_{\text{data}}} \log p_\theta(z^+) = \mathbb{E}_{z^+}\left[E_\theta(z^+)\right] + \log Z_\theta
$$

åˆ†é…é–¢æ•°$Z_\theta = \int \exp(-E_\theta(z))\, dz$ã¯ä¸€èˆ¬ã«è¨ˆç®—ä¸èƒ½ã§ã‚ã‚‹ã€‚å‹¾é…ã¯:

$$
\nabla_\theta \mathcal{L}_{\text{EBM}} = \mathbb{E}_{z^+ \sim p_{\text{data}}}\left[\nabla_\theta E_\theta(z^+)\right] - \mathbb{E}_{z^- \sim p_\theta}\left[\nabla_\theta E_\theta(z^-)\right]
$$

ç¬¬2é …ï¼ˆnegative phaseï¼‰ã¯$p_\theta$ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«$z^-$ãŒå¿…è¦ã§ã‚ã‚‹ã€‚Contrastive Divergence (CD-k) ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ç‚¹$z^+$ã‹ã‚‰åˆæœŸåŒ–ã—ãŸMarkov Chainã‚’MCMCã§$k$ã‚¹ãƒ†ãƒƒãƒ—èµ°ã‚‰ã›ã¦$z^-$ã‚’å¾—ã‚‹:

$$
z^- = \text{MCMC}_k(z^+; E_\theta), \quad z_0 \leftarrow z^+
$$

å„MCMCã‚¹ãƒ†ãƒƒãƒ—ã§ã¯**Langevin dynamics**ã‚’ä½¿ç”¨:

$$
z_{s+1} = z_s - \frac{\delta}{2} \nabla_z E_\theta(z_s) + \sqrt{\delta}\, \varepsilon_s, \quad \varepsilon_s \sim \mathcal{N}(0, I)
$$

$k=1$ã®CD-1ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒä½ã„ãŒã€ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚‹ã€‚$k \to \infty$ã§ã¯çœŸã®$p_\theta$ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ã«è¿‘ã¥ãã€‚World Modelsã¸ã®é©ç”¨ã§ã¯$z = z_{t+1}$ã€æ¡ä»¶$z_t, a_t$ã‚’å›ºå®šã—ãŸä¸Šã§Langevin MCMCã‚’èµ°ã‚‰ã›ã‚‹:

$$
z_{t+1}^{(s+1)} = z_{t+1}^{(s)} - \frac{\delta}{2}\nabla_{z_{t+1}} E_\theta(z_t, a_t, z_{t+1}^{(s)}) + \sqrt{\delta}\, \varepsilon_s
$$

**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–**

ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°$E_\theta$ã®è¨­è¨ˆã¯è¡¨ç¾åŠ›ã¨è¨“ç·´å®‰å®šæ€§ã«ç›´çµã™ã‚‹ã€‚ä¸»ãªé¸æŠè‚¢:

1. **ã‚¹ã‚«ãƒ©ãƒ¼MLP**: $E_\theta(z_t, a_t, z_{t+1}) = \text{MLP}_\theta([z_t; a_t; z_{t+1}])$
   â€” æœ€ã‚‚æŸ”è»Ÿã ãŒè¨“ç·´ãŒä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„

2. **äºŒæ¬¡å‹ + æ®‹å·®**: $E_\theta = \frac{1}{2}\| z_{t+1} - \mu_\theta(z_t, a_t) \|_{\Sigma_\theta}^2 + r_\theta(z_t, a_t, z_{t+1})$
   â€” åˆæœŸåŒ–ãŒå®‰å®šã—ã€æ®‹å·®é …ãŒéGaussæ€§ã‚’æ‰ãˆã‚‹

3. **åŒç·šå‹**: $E_\theta = -z_{t+1}^\top W_\theta(z_t, a_t) z_{t+1}$
   â€” è¨ˆç®—åŠ¹ç‡ãŒé«˜ã„ãŒè¡¨ç¾åŠ›ãŒé™å®šçš„

**Score Matching: CDã®ä»£æ›¿è¨“ç·´æ³•**

Score Matchingã¯åˆ†é…é–¢æ•°$Z_\theta$ã‚’å›é¿ã™ã‚‹**Fisher divergenceæœ€å°åŒ–**ã«åŸºã¥ã:

$$
\mathcal{L}_{\text{SM}} = \mathbb{E}_{z \sim p_{\text{data}}} \left[ \frac{1}{2} \| \nabla_z \log p_\theta(z) - \nabla_z \log p_{\text{data}}(z) \|^2 \right]
$$

$\nabla_z \log p_\theta(z) = -\nabla_z E_\theta(z)$ã§ã‚ã‚‹ã‹ã‚‰ã€**éƒ¨åˆ†ç©åˆ†**ã«ã‚ˆã‚Š$p_{\text{data}}$ã®å¾®åˆ†ã‚’æ¶ˆå»ã§ãã‚‹ï¼ˆHyvÃ¤rinen, 2005ï¼‰:

$$
\mathcal{L}_{\text{SM}} = \mathbb{E}_{z \sim p_{\text{data}}} \left[ \text{tr}\!\left(\nabla_z^2 E_\theta(z)\right) + \frac{1}{2}\| \nabla_z E_\theta(z) \|^2 \right] + \text{const}
$$

ç¬¬1é …ã¯Hessianã®ãƒˆãƒ¬ãƒ¼ã‚¹ã§ã‚ã‚Šã€ã‚¹ãƒ©ã‚¤ã‚¹Score Matchingï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ–¹å‘ã¸ã®å°„å½±ï¼‰ã§è¿‘ä¼¼å¯èƒ½:

$$
\mathcal{L}_{\text{SSM}} = \mathbb{E}_{z, \mathbf{v}} \left[ \mathbf{v}^\top \nabla_z^2 E_\theta(z)\, \mathbf{v} + \frac{1}{2}\left(\mathbf{v}^\top \nabla_z E_\theta(z)\right)^2 \right]
$$

ã“ã“ã§$\mathbf{v} \sim \mathcal{N}(0, I)$ã¯ç¢ºç‡çš„å°„å½±ãƒ™ã‚¯ãƒˆãƒ«ã€‚MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä¸€åˆ‡å¿…è¦ã¨ã—ãªã„ç‚¹ãŒCD-kã¨æ¯”ã¹ãŸæœ€å¤§ã®åˆ©ç‚¹ã§ã‚ã‚‹ã€‚Denoising Score Matchingï¼ˆVincent, 2011ï¼‰ã¯å®Ÿç”¨çš„ãªå¤‰ç¨®ã§ã‚ã‚Šã€ãƒã‚¤ã‚ºä»˜ãè¦³æ¸¬$\tilde{z} = z + \sigma \varepsilon$ã«å¯¾ã™ã‚‹ã‚¹ã‚³ã‚¢é–¢æ•°$s_\theta(\tilde{z}, \sigma) \approx \nabla_{\tilde{z}} \log p_{\sigma}(\tilde{z})$ã‚’å­¦ç¿’ã™ã‚‹ã€‚ã“ã‚Œã¯Diffusion Modelsã®ç†è«–çš„åŸºç›¤ã§ã‚‚ã‚ã‚‹ã€‚

DSMã®è¨“ç·´ç›®çš„ã¯:

$$
\mathcal{L}_{\text{DSM}} = \mathbb{E}_{z \sim p_{\text{data}},\, \varepsilon \sim \mathcal{N}(0,I),\, \sigma} \left[ \sigma^2 \left\| s_\theta(z + \sigma\varepsilon,\, \sigma) + \frac{\varepsilon}{\sigma} \right\|_2^2 \right]
$$

Score Matchingã€Denoising Score Matchingã€CD-kã¯ã„ãšã‚Œã‚‚EBMã®è² ã®å¯¾æ•°å°¤åº¦æœ€å°åŒ–ã®ç•°ãªã‚‹è¿‘ä¼¼ã§ã‚ã‚Šã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¨çµ±è¨ˆçš„åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒå­˜åœ¨ã™ã‚‹ã€‚

**ã‚¨ãƒãƒ«ã‚®ãƒ¼æ™¯è¦³ã¨å¤šå³°æ€§æœªæ¥äºˆæ¸¬**

æ±ºå®šè«–çš„world model $z_{t+1} = f_\theta(z_t, a_t)$ã¯å˜ä¸€ã®æœªæ¥ã—ã‹äºˆæ¸¬ã§ããªã„ã€‚EBMã§ã¯$E_\theta(z_t, a_t, \cdot)$ãŒ**è¤‡æ•°ã®æ¥µå°å€¤**ã‚’æŒã¤ã“ã¨ã§ã€å†…åœ¨çš„ãªå¤šå³°æ€§ã‚’è¡¨ç¾ã§ãã‚‹:

$$
\mathcal{Z}_{\text{future}} = \{z^* : \nabla_{z_{t+1}} E_\theta(z_t, a_t, z^*) = 0,\; \nabla^2_{z_{t+1}} E_\theta(z_t, a_t, z^*) \succ 0\}
$$

å„æ¥µå°å€¤ã¯ã€Œã‚ã‚Šå¾—ã‚‹æœªæ¥çŠ¶æ…‹ã€ã«å¯¾å¿œã—ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æ·±ã•ãŒç¢ºç‡ï¼ˆå¯èƒ½æ€§ï¼‰ã«ç›¸å½“ã™ã‚‹ã€‚Langevin MCMCã®ã‚¹ãƒ†ãƒƒãƒ—æ•°$k$ã¨å­¦ç¿’ç‡$\delta$ã®ãƒãƒ©ãƒ³ã‚¹ãŒãƒ¢ãƒ¼ãƒ‰æ¢ç´¢èƒ½åŠ›ã‚’æ±ºå®šã™ã‚‹: $\delta$ãŒå¤§ãã™ãã‚‹ã¨ãƒ¢ãƒ¼ãƒ‰é–“ã®å¢ƒç•Œã‚’è¶Šãˆã«ããã€å°ã•ã™ãã‚‹ã¨æ··åˆãŒé…ã„ã€‚å®Ÿç”¨çš„ã«ã¯**Parallel Tempering**ï¼ˆç•°ãªã‚‹æ¸©åº¦$T$ã§ã®MCMCã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã¦äº¤æ›ï¼‰ãŒå¤šå³°æ€§ã‚¨ãƒãƒ«ã‚®ãƒ¼æ™¯è¦³ã®æ¢ç´¢ã«æœ‰åŠ¹ã§ã‚ã‚‹ã€‚

### 3.6 ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ç†è«–

#### 3.6.1 Action-conditionedäºˆæ¸¬

**ç›®çš„**: action $a_t$ã‚’ä¸ãˆã¦æ¬¡çŠ¶æ…‹$z_{t+1}$ã‚’äºˆæ¸¬

$$
z_{t+1} = f_\theta(z_t, a_t) + \epsilon_t
$$

**è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ãƒ­ãƒœãƒƒãƒˆè»Œè·¡ $(z_t, a_t, z_{t+1})$

**æå¤±**:

$$
\mathcal{L}_{\text{pred}} = \mathbb{E} \left[ \| f_\theta(z_t, a_t) - z_{t+1} \|_2^2 \right]
$$

**Stochastic dynamics**ã®å ´åˆ:

$$
p_\theta(z_{t+1} | z_t, a_t) = \mathcal{N}(f_\theta(z_t, a_t), \Sigma_\theta(z_t, a_t))
$$

$$
\mathcal{L}_{\text{NLL}} = -\mathbb{E} \left[ \log p_\theta(z_{t+1} | z_t, a_t) \right]
$$

#### 3.6.2 Reward Prediction

World Modelã‚’å¼·åŒ–å­¦ç¿’ã«çµ±åˆã™ã‚‹å ´åˆã€å ±é…¬é–¢æ•°$r_t$ã‚‚äºˆæ¸¬:

$$
r_t = g_\phi(z_t, a_t)
$$

**è¨“ç·´**:

$$
\mathcal{L}_{\text{reward}} = \mathbb{E} \left[ (g_\phi(z_t, a_t) - r_t)^2 \right]
$$

**Model-based RL**:

1. World Modelã§æœªæ¥ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ: $z_{t+1} = f_\theta(z_t, a_t)$
2. å ±é…¬ã‚’æ¨å®š: $\hat{r}_t = g_\phi(z_t, a_t)$
3. Policy $\pi_\psi(a|z)$ã‚’æœ€é©åŒ–:

$$
\mathcal{L}_{\text{policy}} = -\mathbb{E}_{\pi} \left[ \sum_{t=0}^H \gamma^t g_\phi(z_t, a_t) \right]
$$

#### 3.6.3 è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’

**Contrastive Learning**: æ­£ä¾‹ï¼ˆåŒä¸€å‹•ç”»ã®è¿‘æ¥ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰vs è² ä¾‹ï¼ˆç•°ãªã‚‹å‹•ç”»ï¼‰

$$
\mathcal{L}_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(z_t, z_{t+k}) / \tau)}{\sum_{j} \exp(\text{sim}(z_t, z_j^{-}) / \tau)}
$$

**Masked Autoencoding**: ä¸€éƒ¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒã‚¹ã‚¯ â†’ äºˆæ¸¬ï¼ˆJEPAã¨åŒã˜åŸç†ï¼‰

#### 3.6.4 RSSM: Recurrent State Space Model (DreamerV3)

DreamerV3ï¼ˆHafner et al., 2023ï¼‰ã®ä¸­æ ¸ã‚’ãªã™RSSMï¼ˆRecurrent State Space Modelï¼‰ã¯ã€**æ±ºå®šè«–çš„ãƒ‘ã‚¹**ã¨**ç¢ºç‡çš„ãƒ‘ã‚¹**ã‚’çµ„ã¿åˆã‚ã›ãŸæ½œåœ¨çŠ¶æ…‹ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚

**çŠ¶æ…‹é·ç§»ã®æ§‹é€ **:

$$
h_t = f_\phi(h_{t-1},\, z_{t-1},\, a_{t-1})
$$

$$
\hat{z}_t \sim p_\phi(\hat{z}_t \mid h_t) \quad \text{ï¼ˆprior: äº‹å‰åˆ†å¸ƒ)}
$$

$$
z_t \sim q_\phi(z_t \mid h_t, x_t) \quad \text{ï¼ˆposterior: è¦³æ¸¬$x_t$ã§æ›´æ–°ï¼‰}
$$

ã“ã“ã§$h_t \in \mathbb{R}^{d_h}$ã¯GRUã«ã‚ˆã‚‹**æ±ºå®šè«–çš„**éš ã‚ŒçŠ¶æ…‹ï¼ˆé•·æœŸä¾å­˜ã‚’ä¿æŒï¼‰ã€$z_t \in \mathbb{R}^{d_z}$ã¯**ç¢ºç‡çš„**æ½œåœ¨çŠ¶æ…‹ï¼ˆä¸ç¢ºå®Ÿæ€§ã‚’è¡¨ç¾ï¼‰ã€$a_{t-1}$ã¯å‰ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§ã‚ã‚‹ã€‚Prior $p_\phi$ã¯è¦³æ¸¬ãªã—ã§å°†æ¥ã‚’äºˆæ¸¬ï¼ˆimaginationç”¨ï¼‰ã€Posterior $q_\phi$ã¯è¦³æ¸¬$x_t$ã‚’å–ã‚Šè¾¼ã‚“ã§çŠ¶æ…‹ã‚’ç²¾ç·»åŒ–ã™ã‚‹ã€‚

**ELBOã«ã‚ˆã‚‹è¨“ç·´ç›®çš„**:

ELBOã‚’å…¨æ™‚åˆ»$t=1,\ldots,T$ã«ã‚ãŸã£ã¦å±•é–‹ã™ã‚‹ã¨:

$$
\mathcal{L}_{\text{RSSM}} = \sum_{t=1}^T \underbrace{\mathbb{E}_{q_\phi}[\log p_\phi(x_t \mid h_t, z_t)]}_{\text{å†æ§‹æˆæå¤±}} - \underbrace{D_{\text{KL}}[q_\phi(z_t \mid h_t, x_t) \,\|\, p_\phi(z_t \mid h_t)]}_{\text{KLæ­£å‰‡åŒ–}}
$$

å†æ§‹æˆæå¤±ã¯ãƒ‡ã‚³ãƒ¼ãƒ€$p_\phi(x_t \mid h_t, z_t)$ï¼ˆç”»åƒã®å ´åˆã¯ConvDecoderãªã©ï¼‰ãŒä¸ãˆã‚‹ã€‚KLæ­£å‰‡åŒ–ã¯posteriorã‚’ã€Œæ„å‘³ã®ã‚ã‚‹æƒ…å ±ã®ã¿ç¬¦å·åŒ–ã™ã‚‹ã€ã‚ˆã†ä¿ƒã—ã€ä¸å¿…è¦ãªæƒ…å ±ã®è¨˜æ†¶ã‚’é˜²ãæ­£å‰‡åŒ–ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã€‚ã¾ãŸã€$h_t$ãŒæ±ºå®šè«–çš„ãƒ‘ã‚¹ã§æƒ…å ±ã‚’ä¿æŒã™ã‚‹ãŸã‚ã€$z_t$ã¯ãã®æ™‚åˆ»ã«å›ºæœ‰ã®ç¢ºç‡çš„æƒ…å ±ã®ã¿ã‚’æ‹…ãˆã°ã‚ˆãã€ä¸¡ãƒ‘ã‚¹ã®å½¹å‰²åˆ†æ‹…ãŒELBOã‚’æœ€å¤§åŒ–ã™ã‚‹ä¸Šã§é‡è¦ãªè¨­è¨ˆä¸Šã®æ´å¯Ÿã§ã‚ã‚‹ã€‚

**KLãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°ã¨Free Bits Trick**ï¼ˆDreamerV3ï¼‰

å˜ç´”ãªKLæœ€å°åŒ–ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒäº‹å‰åˆ†å¸ƒ$p_\phi$ã‚’äº‹å¾Œåˆ†å¸ƒ$q_\phi$ã«ä¸€è‡´ã•ã›ã‚‹ã‚ˆã‚Šã‚‚é€†æ–¹å‘ï¼ˆ$q_\phi \to p_\phi$ï¼‰ã«åç¸®ã•ã›ã¦ã—ã¾ã†ï¼ˆposterior collapseï¼‰ã€‚DreamerV3ã§ã¯ä»¥ä¸‹ã®**KLãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°**ã‚’æ¡ç”¨:

$$
\mathcal{L}_{\text{KL}} = \alpha \cdot D_{\text{KL}}\!\left[\text{sg}(q_\phi) \,\|\, p_\phi\right] + (1-\alpha) \cdot D_{\text{KL}}\!\left[q_\phi \,\|\, \text{sg}(p_\phi)\right]
$$

ã“ã“ã§$\text{sg}(\cdot)$ã¯stop-gradientæ¼”ç®—å­ã€$\alpha = 0.8$ï¼ˆè«–æ–‡å€¤ï¼‰ã€‚ç¬¬1é …ã¯priorã®æ›´æ–°ã€ç¬¬2é …ã¯posteriorã®æ›´æ–°ã‚’æ‹…ã†ã€‚**Free Bits**åˆ¶ç´„ã«ã‚ˆã‚Šå„æ½œåœ¨å¤‰æ•°ã®KLã«ä¸‹é™ã‚’è¨­ã‘ã‚‹:

$$
\mathcal{L}_{\text{KL}}^{\text{free}} = \max\!\left(\mathcal{L}_{\text{KL}},\, \beta_{\text{free}}\right), \quad \beta_{\text{free}} = 1.0 \text{ nat}
$$

ã“ã‚Œã«ã‚ˆã‚Šæƒ…å ±é‡ãŒ$\beta_{\text{free}}$ä»¥ä¸‹ã®æ½œåœ¨å¤‰æ•°ã®KLã‚’0ã¨ã—ã¦æ‰±ã„ã€posterior collapseã‚’é˜²ãã€‚

**Imagination-basedå­¦ç¿’**

è¨“ç·´æ¸ˆã¿RSSMã‚’ç”¨ã„ã¦ã€å®Ÿç’°å¢ƒã‚’ä½¿ã‚ãšã«**å†…éƒ¨ã§è»Œè·¡ã‚’å±•é–‹**ï¼ˆimaginationï¼‰ã™ã‚‹:

1. ç¾åœ¨ã®æ½œåœ¨çŠ¶æ…‹$z_t, h_t$ã‹ã‚‰å‡ºç™ºã—ã€world modelã§$H$ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§äºˆæ¸¬:

$$
\hat{z}_{t+k} \sim p_\phi(\hat{z}_{t+k} \mid h_{t+k}), \quad h_{t+k} = f_\phi(h_{t+k-1}, \hat{z}_{t+k-1}, a_{t+k-1})
$$

2. å„è™šæƒ³çŠ¶æ…‹ã«ãŠã‘ã‚‹å ±é…¬ã¨ä¾¡å€¤ã‚’æ¨å®š:

$$
\hat{r}_{t+k} = r_\phi(h_{t+k}, \hat{z}_{t+k}), \quad \hat{v}_{t+k} = V_\psi(h_{t+k}, \hat{z}_{t+k})
$$

3. $\lambda$-return ã§ã‚¢ã‚¯ã‚¿ãƒ¼æå¤±ã‚’è¨ˆç®—:

$$
\mathcal{R}_t^\lambda = \hat{r}_t + \gamma \left[(1-\lambda)\hat{v}_{t+1} + \lambda \mathcal{R}_{t+1}^\lambda\right]
$$

$$
\mathcal{L}_{\text{actor}} = -\mathbb{E}_{\text{imagination}}\left[\sum_{k=0}^{H-1} \gamma^k \mathcal{R}_{t+k}^\lambda\right]
$$

å®Ÿç’°å¢ƒã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ãªãŒã‚‰ï¼ˆ100å€ä»¥ä¸Šã®ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ï¼‰ã€Atariãƒ»DMCãƒ»Craftaxãªã©åºƒç¯„ãªã‚¿ã‚¹ã‚¯ã§æœ€é«˜æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã‚‹ã€‚

**ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«æ½œåœ¨è¡¨ç¾ã¨ç›´ç·šçš„å‹¾é…æ¨å®š**

DreamerV3ã§ã¯é€£ç¶šæ½œåœ¨å¤‰æ•°$z_t \in \mathbb{R}^{d_z}$ã®ä»£ã‚ã‚Šã«**ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒã®ç©**ã‚’ä½¿ç”¨ã™ã‚‹ã€‚$K$å€‹ã®ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰$C$ã‚¯ãƒ©ã‚¹ã‚’é¸ã¶ç‹¬ç«‹ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ç©ã¨ã—ã¦:

$$
z_t = \text{onehot}(k_1) \oplus \ldots \oplus \text{onehot}(k_K), \quad k_i \sim \text{Categorical}(\pi_{i,1}, \ldots, \pi_{i,C})
$$

åˆè¨ˆ$KC$æ¬¡å…ƒã®é›¢æ•£æ½œåœ¨è¡¨ç¾ã¨ãªã‚‹ï¼ˆè«–æ–‡å€¤: $K=32, C=32$ã§1024æ¬¡å…ƒï¼‰ã€‚ã“ã®é›¢æ•£è¡¨ç¾ã¯**å†æ§‹æˆæå¤±ã®grading**ã«å„ªã‚Œã¦ãŠã‚Šã€é€£ç¶šè¡¨ç¾ã‚ˆã‚Šè¡¨ç¾åŠ›ãŒé«˜ã„ã“ã¨ãŒå®Ÿè¨¼ã•ã‚Œã¦ã„ã‚‹ã€‚

é›¢æ•£å¤‰æ•°ã®ELBOã¯ç›´æ¥å¾®åˆ†ã§ããªã„ãŸã‚ã€**Straight-Through Gradient**ã‚’ä½¿ç”¨:

$$
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\pi}} \approx \frac{\partial \mathcal{L}}{\partial z_t} \cdot \frac{\partial z_t}{\partial \boldsymbol{\pi}} \bigg|_{z_t \leftarrow \text{onehot}(\arg\max \boldsymbol{\pi})}
$$

å‰å‘ããƒ‘ã‚¹ã§ã¯$\arg\max$ã§ãƒãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ã„ã€å¾Œã‚å‘ããƒ‘ã‚¹ã§ã¯$\text{softmax}$ã®å‹¾é…ã‚’æµã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šé›¢æ•£ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã®è¡¨ç¾èƒ½åŠ›ã¨é€£ç¶šæœ€é©åŒ–ã®è¨ˆç®—åŠ¹ç‡ã‚’ä¸¡ç«‹ã™ã‚‹ã€‚

**ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æå¤±åˆ†è§£ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**

DreamerV3ã®å…¨æå¤±ã¯ä»¥ä¸‹ã®3æˆåˆ†ã«åˆ†è§£ã•ã‚Œã‚‹:

$$
\mathcal{L}_{\text{WM}} = \underbrace{\mathcal{L}_{\text{recon}}}_{\text{ãƒ‡ã‚³ãƒ¼ãƒ€æå¤±}} + \underbrace{\mathcal{L}_{\text{pred}}}_{\text{reward/cont}} + \underbrace{\mathcal{L}_{\text{dyn}}}_{\text{KLå‹•çš„ãƒãƒ©ãƒ³ã‚¹}}
$$

å„æˆåˆ†ã‚’**symlogå¤‰æ›**ã«ã‚ˆã‚Šã‚¹ã‚±ãƒ¼ãƒ«æ­£è¦åŒ–ã™ã‚‹:

$$
\text{symlog}(x) = \text{sign}(x) \cdot \log(|x| + 1)
$$

ã“ã‚Œã«ã‚ˆã‚Šå ±é…¬ãŒ$[-1000, 1000]$ã®åºƒç¯„å›²ã«åˆ†æ•£ã™ã‚‹ç’°å¢ƒï¼ˆMinecraftç­‰ï¼‰ã§ã‚‚æå¤±ã‚¹ã‚±ãƒ¼ãƒ«ã‚’$O(1)$ã«ä¿ã¡ã€å˜ä¸€ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã§ãã‚‹ã€‚

symlogã®é€†å¤‰æ›ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰æ™‚ã«ä½¿ç”¨ï¼‰ã¯:

$$
\text{symexp}(x) = \text{sign}(x) \cdot (\exp(|x|) - 1)
$$

å ±é…¬äºˆæ¸¬é ­éƒ¨ã¯symlogå¤‰æ›å¾Œã®å ±é…¬ã‚’äºˆæ¸¬ã—ã€å®Ÿéš›ã®å ±é…¬ã¨ã®æ¯”è¼ƒã«ã¯symexpå¾Œã®å€¤ã‚’ç”¨ã„ã‚‹ã€‚ã“ã®å¯¾ç§°çš„ãªå¯¾æ•°å¤‰æ›ã¯ã€æ­£è² ã®å¤§ããªå¤–ã‚Œå€¤ã«å¯¾ã—ã¦ã‚‚ãƒ­ãƒã‚¹ãƒˆã§ã‚ã‚Šã€å‹¾é…ã®çˆ†ç™ºã‚’æŠ‘åˆ¶ã™ã‚‹ã€‚ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«ãŠã„ã¦ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰ãªæå¤±è¨­è¨ˆã¯ã€ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ï¼ˆå ±é…¬$\sim 0.1$ï¼‰ã‹ã‚‰ã‚²ãƒ¼ãƒ ï¼ˆå ±é…¬$\sim 10^4$ï¼‰ã¾ã§çµ±ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§æ‰±ã†ãŸã‚ã«ä¸å¯æ¬ ã§ã‚ã‚‹ã€‚

### 3.7 ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

#### 3.7.1 äºˆæ¸¬ç²¾åº¦

**Mean Squared Error (MSE)**:

$$
\text{MSE} = \frac{1}{N} \sum_{i=1}^N \| z_{t+1}^{(i)} - \hat{z}_{t+1}^{(i)} \|_2^2
$$

**Structural Similarity (SSIM)** (ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã§è©•ä¾¡ã™ã‚‹å ´åˆ):

$$
\text{SSIM}(x, \hat{x}) = \frac{(2\mu_x \mu_{\hat{x}} + C_1)(2\sigma_{x\hat{x}} + C_2)}{(\mu_x^2 + \mu_{\hat{x}}^2 + C_1)(\sigma_x^2 + \sigma_{\hat{x}}^2 + C_2)}
$$

#### 3.7.2 ç‰©ç†æ³•å‰‡éµå®ˆã‚¹ã‚³ã‚¢

**ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜èª¤å·®**:

$$
\text{Energy Error} = \frac{1}{T} \sum_{t=1}^T | E(z_t) - E(z_0) |
$$

**é‹å‹•é‡ä¿å­˜èª¤å·®**:

$$
\text{Momentum Error} = \frac{1}{T} \sum_{t=1}^T \left\| \sum_i m_i \mathbf{v}_i(t) - \sum_i m_i \mathbf{v}_i(0) \right\|_2
$$

#### 3.7.3 é•·æœŸä¸€è²«æ€§

**Frame Prediction Horizon**: ãƒ¢ãƒ‡ãƒ«ãŒä½•ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§æ­£ç¢ºã«äºˆæ¸¬ã§ãã‚‹ã‹

$$
T_{\text{horizon}} = \max\{t : \text{MSE}(t) < \epsilon\}
$$

**Video Quality Metrics**:

- **FVD (FrÃ©chet Video Distance)**: I3Dç‰¹å¾´é‡ã§ã®FrÃ©chetè·é›¢
- **LPIPS**: çŸ¥è¦šçš„é¡ä¼¼åº¦

#### 3.7.4 FVD (FrÃ©chet Video Distance) ã®å³å¯†å®šå¼åŒ–

FVDã¯FrÃ©chet Inception Distance (FID) ã‚’å‹•ç”»ã«æ‹¡å¼µã—ãŸæŒ‡æ¨™ã§ã‚ã‚Šã€**I3Dï¼ˆInflated 3D ConvNetï¼‰**ã®ç‰¹å¾´ç©ºé–“ã§ãƒªã‚¢ãƒ«å‹•ç”»ã¨ç”Ÿæˆå‹•ç”»ã®åˆ†å¸ƒè·é›¢ã‚’æ¸¬å®šã™ã‚‹ã€‚

ã¾ãšI3Dã‚¨ãƒ³ã‚³ãƒ¼ãƒ€$\phi_{I3D}: \mathbb{R}^{T \times H \times W \times 3} \to \mathbb{R}^d$ã§å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã‚’ç‰¹å¾´é‡ã«å¤‰æ›ã—ã€ãƒªã‚¢ãƒ«åˆ†å¸ƒã¨ç”Ÿæˆåˆ†å¸ƒã‚’ãã‚Œãã‚ŒGaussianã§è¿‘ä¼¼ã™ã‚‹:

$$
\mu_r, \Sigma_r = \text{MeanCov}\left(\{\phi_{I3D}(v_i)\}_{i=1}^{N_r}\right)
$$

$$
\mu_g, \Sigma_g = \text{MeanCov}\left(\{\phi_{I3D}(\hat{v}_j)\}_{j=1}^{N_g}\right)
$$

FVDã¯ã“ã®2ã¤ã®Gaussiané–“ã®**FrÃ©chetè·é›¢**ï¼ˆWasserstein-2è·é›¢ã®äºŒä¹—ã®é–‰å½¢å¼ï¼‰:

$$
\text{FVD} = \|\mu_r - \mu_g\|_2^2 + \text{tr}\!\left(\Sigma_r + \Sigma_g - 2\left(\Sigma_r \Sigma_g\right)^{1/2}\right)
$$

$(\Sigma_r \Sigma_g)^{1/2}$ã¯è¡Œåˆ—ã®å¹³æ–¹æ ¹ï¼ˆæ­£å®šå€¤è¡Œåˆ—ã®å¹¾ä½•å¹³å‡ï¼‰ã§ã‚ã‚Šã€æ•°å€¤çš„ã«ã¯å›ºæœ‰å€¤åˆ†è§£ã§è¨ˆç®—ã™ã‚‹ã€‚FVD $= 0$ã¯ãƒªã‚¢ãƒ«ã¨ç”Ÿæˆã®åˆ†å¸ƒãŒå®Œå…¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚å…¸å‹çš„ãªè‰¯ã„world modelã¯FVD $< 100$ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®VideoLDMã¯ FVD $\approx 20$å°ã‚’é”æˆã—ã¦ã„ã‚‹ã€‚

**Expected Calibration Error (ECE): ç¢ºç‡çš„äºˆæ¸¬ã®ä¿¡é ¼åº¦è©•ä¾¡**

ç¢ºç‡çš„world model $p_\theta(z_{t+k} \mid z_t, a_{t:t+k-1})$ã®äºˆæ¸¬ä¿¡é ¼åº¦ãŒå®Ÿéš›ã®æ­£è§£ç‡ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ã€‚ä¿¡é ¼åº¦åŒºé–“ã‚’$M$å€‹ã®ãƒ“ãƒ³$B_m = [(m-1)/M, m/M)$ã«åˆ†å‰²ã—:

$$
\text{ECE} = \sum_{m=1}^M \frac{|B_m|}{N} \left|\text{acc}(B_m) - \text{conf}(B_m)\right|
$$

$$
\text{acc}(B_m) = \frac{1}{|B_m|}\sum_{i \in B_m} \mathbf{1}[\hat{z}_i = z_i^*], \quad \text{conf}(B_m) = \frac{1}{|B_m|}\sum_{i \in B_m} \hat{p}_i
$$

ã“ã“ã§$\hat{p}_i$ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã€‚ECE $= 0$ã¯å®Œå…¨ã«calibratedã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

**å¤šã‚¹ãƒ†ãƒƒãƒ—ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®èª¤å·®è“„ç©åˆ†æ**

World modelã‚’$k$ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§å±•é–‹ã—ãŸå ´åˆã®èª¤å·®ã¯ã€1ã‚¹ãƒ†ãƒƒãƒ—èª¤å·®$\epsilon_1$ã¨èª¤å·®ä¼æ’­ç‡$\rho$ã«ã‚ˆã£ã¦æŒ‡æ•°çš„ã«å¢—å¤§ã™ã‚‹:

$$
\text{MSE}(k) \leq \epsilon_1 \cdot \frac{\rho^k - 1}{\rho - 1} \approx \epsilon_1 \cdot e^{(\rho-1)k} \quad (\rho > 1 \text{ ã®å ´åˆ})
$$

ã‚ˆã‚Šç²¾å¯†ã«ã¯ã€Lipschitzå®šæ•°$L_f$ã‚’æŒã¤world model $f_\theta$ã«å¯¾ã—ã¦:

$$
\|z_{t+k} - \hat{z}_{t+k}\| \leq L_f^k \|z_t - \hat{z}_t\| + \sum_{j=0}^{k-1} L_f^j \epsilon_{t+k-j}
$$

ã“ã“ã§$\epsilon_t$ã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã®1ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬èª¤å·®ã€‚$L_f < 1$ï¼ˆåç¸®ãƒãƒƒãƒ—ï¼‰ãªã‚‰ã°èª¤å·®ãŒæœ‰ç•Œã§ã‚ã‚Šã€$L_f > 1$ãªã‚‰ã°æŒ‡æ•°çš„ã«ç™ºæ•£ã™ã‚‹ã€‚

**æƒ…å ±ç†è«–çš„ç²¾åº¦ä¸Šé™**

World modelã®äºˆæ¸¬ç²¾åº¦ã«ã¯ã€ç³»ã®**LyapunovæŒ‡æ•°**$\lambda_{\max}$ã«ã‚ˆã‚‹æƒ…å ±ç†è«–çš„é™ç•ŒãŒã‚ã‚‹ã€‚åˆæœŸçŠ¶æ…‹ã®ä¸ç¢ºå®Ÿæ€§$\sigma_0$ã«å¯¾ã—ã¦ã€æ™‚åˆ»$t$ã§ã®äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§ã¯:

$$
\sigma_t \geq \sigma_0 \cdot e^{\lambda_{\max} t}
$$

é€£ç¶šè¦³æ¸¬ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã§ã‚‚ã€Shannonæƒ…å ±é‡ã®è¦³ç‚¹ã‹ã‚‰:

$$
I(z_{t+k}; \{z_t, a_{t:t+k}\}) \leq I(z_{t+k}; z_{t+k-1}) + I(z_{t+k-1}; \{z_t, a_{t:t+k-1}\})
$$

ã¤ã¾ã‚Šäºˆæ¸¬å¯èƒ½ãªæƒ…å ±é‡ã¯Mandelbrot-Shannon-Khinchinå‰‡ã«å¾“ã„ã€Lyapunovæ™‚é–“$t \sim 1/\lambda_{\max}$ã‚’è¶…ãˆã‚‹ã¨æ€¥é€Ÿã«æ¸›è¡°ã™ã‚‹ã€‚ã“ã‚Œã¯ç‰©ç†çš„ã«æ„å‘³ã®ã‚ã‚‹é•·æœŸäºˆæ¸¬ã®å›°é›£ã•ã®æ ¹æœ¬çš„ãªèª¬æ˜ã‚’ä¸ãˆã‚‹ã€‚

å®Ÿéš›ã®è©•ä¾¡ã§ã¯ã€LyapunovæŒ‡æ•°ã‚’æ¨å®šã—ã¦world modelã®ã€Œç†è«–çš„äºˆæ¸¬é™ç•Œã‚¹ãƒ†ãƒƒãƒ—æ•°ã€$t^* = \kappa / \lambda_{\max}$ï¼ˆ$\kappa$ã¯è¨±å®¹æƒ…å ±æå¤±ã‚’æ±ºã‚ã‚‹å®šæ•°ï¼‰ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒ$t^*$ã‚¹ãƒ†ãƒƒãƒ—ä»¥å†…ã§é«˜ç²¾åº¦ã«äºˆæ¸¬ã§ãã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒå®Ÿç”¨çš„ãªè©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã¨ãªã‚‹ã€‚ç‰©ç†ç³»ï¼ˆåˆ†å­å‹•åŠ›å­¦$\lambda_{\max} \sim 10^{12}$ Hzï¼‰ã¨ãƒ­ãƒœãƒƒãƒˆç³»ï¼ˆ$\lambda_{\max} \sim 1$ Hzï¼‰ã§ã¯äºˆæ¸¬é™ç•ŒãŒ12æ¡ç•°ãªã‚‹ãŸã‚ã€ã‚¿ã‚¹ã‚¯ã«å¿œã˜ãŸè©•ä¾¡è¨­è¨ˆãŒä¸å¯æ¬ ã§ã‚ã‚‹ã€‚

### ğŸ¥Š Boss Battle: Transfusionã®å®Œå…¨åˆ†è§£

**èª²é¡Œ**: arXiv:2408.11039ã®Transfusionã®çµ±ä¸€æå¤±é–¢æ•°ã‚’ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¤ãƒ¡ãƒ¼ã‚¸ã®æ··åˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦å®Œå…¨å°å‡ºã›ã‚ˆã€‚

**Step 1**: å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹

ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ† $\mathbf{t} = (t_1, \ldots, t_n)$ ï¼ˆdiscrete tokensï¼‰
ç”»åƒéƒ¨åˆ† $\mathbf{x} = (x_1, \ldots, x_m)$ ï¼ˆcontinuous patch embeddingsï¼‰

çµ±åˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹:

$$
\mathbf{s} = [\mathbf{t}, \mathbf{x}] \in \mathbb{R}^{(n+m) \times d}
$$

**Step 2**: Attention Mask

$$
M_{\text{Transfusion}} = \begin{bmatrix}
M_{\text{causal}} & 0 \\
M_{\text{bi-dir}} & M_{\text{bi-dir}}
\end{bmatrix}
$$

- å·¦ä¸Š: ãƒ†ã‚­ã‚¹ãƒˆã®causal maskï¼ˆè‡ªå·±å›å¸°ï¼‰
- å³ä¸‹: ç”»åƒã®bidirectional maskï¼ˆå…¨ãƒ‘ãƒƒãƒç›¸äº’å‚ç…§ï¼‰
- å·¦ä¸‹: ç”»åƒãŒãƒ†ã‚­ã‚¹ãƒˆã‚’è¦‹ã‚‹ï¼ˆcross-modal attentionï¼‰
- å³ä¸Š: 0ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯ç”»åƒã‚’è¦‹ãªã„ï¼‰

**Step 3**: Loss Functions

**ãƒ†ã‚­ã‚¹ãƒˆloss** (next token prediction):

$$
\mathcal{L}_{\text{text}} = -\frac{1}{n} \sum_{i=1}^n \log p_\theta(t_i | t_{<i})
$$

Softmaxã§ç¢ºç‡åŒ–:

$$
p_\theta(t_i | t_{<i}) = \frac{\exp(z_{t_i}^\top e_{t_i})}{\sum_{j=1}^{|V|} \exp(z_{t_i}^\top e_j)}
$$

ã“ã“ã§$z_{t_i}$ã¯Transformerã®$i$ç•ªç›®å‡ºåŠ›ã€$e_j$ã¯token embeddingã®$j$ç•ªç›®ã€‚

**ç”»åƒloss** (diffusion):

$$
\mathcal{L}_{\text{image}} = \mathbb{E}_{t \sim [1,T], \epsilon \sim \mathcal{N}(0,I)} \left[ \| \epsilon - \epsilon_\theta(\mathbf{x}_t, t, \mathbf{c}) \|_2^2 \right]
$$

ã“ã“ã§:

$$
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon
$$

$\mathbf{c}$ã¯ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ï¼ˆcross-attentionã§æ³¨å…¥ï¼‰ã€‚

**Step 4**: çµ±åˆæå¤±

$$
\mathcal{L}_{\text{Transfusion}} = \mathcal{L}_{\text{text}} + \lambda \mathcal{L}_{\text{image}}
$$

$\lambda$ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè«–æ–‡ã§ã¯$\lambda=1$ã‚’ä½¿ç”¨ï¼‰ã€‚

**Step 5**: å®Ÿè£…ã‚³ãƒ¼ãƒ‰ï¼ˆJuliaï¼‰

**Step 6: Attention Mask æ§‹é€ ã®è¨¼æ˜**

Transfusionã®maskè¨­è¨ˆãŒãªãœã“ã®å½¢ã§ãªã‘ã‚Œã°ãªã‚‰ãªã„ã‹ã€æƒ…å ±ãƒ•ãƒ­ãƒ¼ã®è¦³ç‚¹ã‹ã‚‰å³å¯†ã«è«–ã˜ã‚‹ã€‚

ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åˆ—$\mathbf{t} = (t_1, \ldots, t_n)$ã¨ç”»åƒãƒ‘ãƒƒãƒåˆ—$\mathbf{x} = (x_1, \ldots, x_m)$ã‚’é€£çµã—ãŸçµ±åˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹$\mathbf{s} = [\mathbf{t}; \mathbf{x}]$ï¼ˆé•·ã•$n+m$ï¼‰ã«å¯¾ã—ã¦ã€attention mask $M \in \{0, 1\}^{(n+m) \times (n+m)}$ã®$(i,j)$æˆåˆ†ãŒ1ãªã‚‰tokenã‹ã‚‰$j$ãŒãƒˆãƒ¼ã‚¯ãƒ³$i$ã«attendã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

**ãƒ†ã‚­ã‚¹ãƒˆâ†’ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå·¦ä¸Š $n \times n$ ãƒ–ãƒ­ãƒƒã‚¯ï¼‰**: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚**causal mask**:

$$
M_{ij}^{tt} = \mathbf{1}[j \leq i], \quad 1 \leq i, j \leq n
$$

ã“ã‚Œã‚’ç ´ã‚‹ã¨$t_{i+1}, \ldots, t_n$ã®æƒ…å ±ãŒ$t_i$ã®äºˆæ¸¬ã«æ¼ã‚Œè¾¼ã¿ï¼ˆinformation leakï¼‰ã€$\mathcal{L}_{\text{text}}$ã®æœ€å°åŒ–ãŒ trivialï¼ˆ$p_\theta(t_i | t_{<i}) = 1$ï¼‰ã«ãªã‚‹ã€‚

**ç”»åƒâ†’ç”»åƒï¼ˆå³ä¸‹ $m \times m$ ãƒ–ãƒ­ãƒƒã‚¯ï¼‰**: Diffusion denoisingã§ã¯å„ãƒ‘ãƒƒãƒãŒä»–ã®å…¨ãƒ‘ãƒƒãƒã‚’å‚ç…§ã§ãã‚‹**bidirectional mask**:

$$
M_{ij}^{xx} = 1, \quad n+1 \leq i, j \leq n+m
$$

ç”»åƒã®ç©ºé–“ä¸€è²«æ€§ï¼ˆé ãé›¢ã‚ŒãŸãƒ‘ãƒƒãƒé–“ã®æ•´åˆæ€§ï¼‰ã«ã¯global attentionãŒä¸å¯æ¬ ã§ã‚ã‚Šã€causalã«ã™ã‚‹ã¨ä½å“è³ªãªç”Ÿæˆã¨ãªã‚‹ã€‚

**ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒï¼ˆå³ä¸Š $n \times m$ ãƒ–ãƒ­ãƒƒã‚¯ï¼‰**: ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã¯ç”»åƒãƒ‘ãƒƒãƒã‚’**attend ã—ãªã„**:

$$
M_{ij}^{tx} = 0, \quad 1 \leq i \leq n,\; n+1 \leq j \leq n+m
$$

ã“ã‚Œã¯è‡ªå·±å›å¸°çš„ãƒ†ã‚­ã‚¹ãƒˆäºˆæ¸¬ã«ç”»åƒã®å°†æ¥æƒ…å ±ãŒå…¥ã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã§ã‚ã‚‹ã€‚

**ç”»åƒâ†’ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå·¦ä¸‹ $m \times n$ ãƒ–ãƒ­ãƒƒã‚¯ï¼‰**: ç”»åƒç”Ÿæˆã®æ¡ä»¶ä»˜ã‘ã®ãŸã‚ã€ç”»åƒãƒ‘ãƒƒãƒã¯å…¨ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‚ç…§ã§ãã‚‹:

$$
M_{ij}^{xt} = 1, \quad n+1 \leq i \leq n+m,\; 1 \leq j \leq n
$$

ã“ã®éå¯¾ç§°æ€§ãŒTransfusionã®æœ¬è³ªã§ã‚ã‚Šã€ã€Œãƒ†ã‚­ã‚¹ãƒˆã¯ç”»åƒã‚’è¦‹ãšã€ç”»åƒã¯ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’è¦‹ã‚‹ã€è¨­è¨ˆã¨ãªã£ã¦ã„ã‚‹ã€‚

**Step 7: $\lambda$ã®æœ€é©é¸æŠã¨æå¤±ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**

ãƒ†ã‚­ã‚¹ãƒˆæå¤±$\mathcal{L}_{\text{text}}$ã¨ç”»åƒæå¤±$\mathcal{L}_{\text{image}}$ã®ã‚¹ã‚±ãƒ¼ãƒ«ã¯æœ¬è³ªçš„ã«ç•°ãªã‚‹ã€‚ãƒ†ã‚­ã‚¹ãƒˆæå¤±ã¯ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã§å…¸å‹çš„ã«$O(1)$ nat/tokenã€ç”»åƒæå¤±ã¯MSEãƒ™ãƒ¼ã‚¹ã§ãƒ‘ãƒƒãƒæ¬¡å…ƒ$d_{\text{patch}}$ã«æ¯”ä¾‹ã™ã‚‹ã€‚ã‚¹ã‚±ãƒ¼ãƒ«ä¸å‡è¡¡ã‚’è£œæ­£ã™ã‚‹æœ€é©$\lambda$ã¯:

$$
\lambda^* = \frac{\mathbb{E}[\|\nabla_\theta \mathcal{L}_{\text{text}}\|_2^2]}{\mathbb{E}[\|\nabla_\theta \mathcal{L}_{\text{image}}\|_2^2]}
$$

å„æå¤±ã®å‹¾é…ãƒãƒ«ãƒ ã®æ¯”ç‡ã‚’å‡ç­‰ã«ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ä¸¡æ–¹ã§å®‰å®šã—ãŸå­¦ç¿’ãŒå®Ÿç¾ã™ã‚‹ã€‚è«–æ–‡ã§ã¯$\lambda = 1$ãŒé¸ã°ã‚Œã¦ã„ã‚‹ãŒã€ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ1.4Bã€œ7Bï¼‰ã«ãŠã„ã¦ã“ã®æ¯”ãŒæ¦‚ã­1ã«è¿‘ããªã‚‹ãŸã‚ã§ã‚ã‚‹ã€‚å°ã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚„é«˜è§£åƒåº¦ç”»åƒã§ã¯$\lambda$ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé‡è¦ã«ãªã‚‹ã€‚


**Bossæ’ƒç ´ï¼** Transfusionã®çµ±ä¸€æå¤±é–¢æ•°ã‚’å®Œå…¨ã«å°å‡ºã—ã€å®Ÿè£…ã—ãŸã€‚

> **âš ï¸ Warning:** **ã“ã“ã§èº“ãäººãŒå¤šã„**: Transfusionã®Attention maskã¯**æ··åˆå‹**ã§ã‚ã‚‹ã€‚ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã¯causalã€ç”»åƒéƒ¨åˆ†ã¯bidirectionalã€ãã—ã¦ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«éƒ¨åˆ†ã¯**ç”»åƒâ†’ãƒ†ã‚­ã‚¹ãƒˆ**ã®ã¿è¨±å¯ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯ç”»åƒã‚’è¦‹ãªã„ï¼‰ã€‚ã“ã‚Œã‚’æ­£ã—ãå®Ÿè£…ã—ãªã„ã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã§ãƒªãƒ¼ã‚¯ãŒèµ·ãã‚‹ã€‚

> **Note:** **é€²æ—**: å…¨ä½“ã®50%å®Œäº†ã€‚World Modelsã®æ•°å­¦çš„åŸºç¤ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚JEPAï¼ˆI/V/VLï¼‰ã®3å¤‰ç¨®ã€Transfusionã®çµ±ä¸€ç†è«–ã€ç‰©ç†æ³•å‰‡å­¦ç¿’ã€EBMè¦–ç‚¹ã€è¨“ç·´ãƒ»è©•ä¾¡æ‰‹æ³•ã‚’å°å‡ºã—ãŸã€‚æ•°å¼ä¿®è¡Œãƒœã‚¹æˆ¦ã‚’ã‚¯ãƒªã‚¢ã€‚

### 3.8 JEPAã®æœ€æ–°ç™ºå±•ï¼ˆ2024-2026ï¼‰

#### 3.8.1 LeJEPA: ç†è«–çš„åŸºç›¤ã®ç¢ºç«‹

**è«–æ–‡**: "LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics," arXiv:2511.08544, 2024[^1]

å¾“æ¥ã®JEPAã¯çµŒé¨“çš„è¨­è¨ˆï¼ˆEMAã€ç‰¹å®šã®ãƒã‚¹ã‚¯æˆ¦ç•¥ãªã©ï¼‰ã«ä¾å­˜ã—ã¦ã„ãŸã€‚LeJEPAã¯**ç†è«–çš„ã«æ­£å½“åŒ–ã•ã‚ŒãŸè¨“ç·´ç›®çš„**ã‚’æç¤ºã™ã‚‹ã€‚

**æ ¸å¿ƒçš„æ´å¯Ÿ**: JEPAã®ç›®çš„é–¢æ•°ã¯**æ½œåœ¨å¤‰æ•°ã®ç›¸äº’æƒ…å ±é‡æœ€å¤§åŒ–**ã¨ã—ã¦å®šå¼åŒ–ã§ãã‚‹:

$$
\max_{\theta, \phi} I(Z_{\text{ctx}}; Z_{\text{tgt}}) = \mathbb{H}(Z_{\text{tgt}}) - \mathbb{H}(Z_{\text{tgt}} | Z_{\text{ctx}})
$$

ã“ã“ã§:
- $Z_{\text{ctx}} = s_\theta(x_{\text{ctx}})$: contextè¡¨ç¾
- $Z_{\text{tgt}} = s_\theta(x_{\text{tgt}})$: targetè¡¨ç¾
- $\mathbb{H}(\cdot)$: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼

**LeJEPAç›®çš„é–¢æ•°**:

$$
\mathcal{L}_{\text{LeJEPA}} = \mathbb{E}_{x, M} \left[ \| f_\theta(s_\theta(x_{\text{ctx}}), M) - s_\theta(x_{\text{tgt}}) \|_2^2 \right] + \lambda \mathbb{H}(Z_{\text{tgt}})
$$

ç¬¬2é …ã¯**è¡¨ç¾ã®å¤šæ§˜æ€§**ã‚’ä¿è¨¼ã—ã€collapseï¼ˆå…¨è¡¨ç¾ãŒåŒä¸€ã«ãªã‚‹ï¼‰ã‚’é˜²ãã€‚

**ç†è«–çš„ä¿è¨¼**:

1. **åæŸä¿è¨¼**: LeJEPAã¯é©åˆ‡ãª$\lambda$ã§å¤§åŸŸæœ€é©è§£ã«åæŸ
2. **EMAä¸è¦**: ç†è«–çš„ã«æ­£å½“åŒ–ã•ã‚ŒãŸç›®çš„é–¢æ•°ã«ã‚ˆã‚ŠEMAãªã—ã§è¨“ç·´å¯èƒ½
3. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: 10å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã§åŠ¹ç‡çš„ã«è¨“ç·´å¯èƒ½

#### 3.8.2 Causal-JEPA: å› æœçš„ä»‹å…¥å­¦ç¿’

**è«–æ–‡**: "Causal-JEPA: Learning World Models through Object-Level Latent Interventions," arXiv:2602.11389, 2025[^2]

å¾“æ¥ã®JEPAã¯**ç›¸é–¢**ã‚’å­¦ç¿’ã™ã‚‹ãŒã€**å› æœé–¢ä¿‚**ã¯å­¦ç¿’ã—ãªã„ã€‚Causal-JEPAï¼ˆC-JEPAï¼‰ã¯**ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ¬ãƒ™ãƒ«ã®ãƒã‚¹ã‚­ãƒ³ã‚°**ã¨**æ½œåœ¨ä»‹å…¥**ã‚’å°å…¥ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ‹¡å¼µ**:

1. **ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆ†è§£**: ç”»åƒã‚’$K$å€‹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ½œåœ¨è¡¨ç¾ã«åˆ†è§£
   $$
   z = \{z_1, z_2, \ldots, z_K\}, \quad z_k \in \mathbb{R}^d
   $$

2. **ä»‹å…¥æ“ä½œ**: ç‰¹å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ$k$ã®è¡¨ç¾ã‚’å¤‰æ›´
   $$
   \text{do}(z_k = \tilde{z}_k)
   $$

3. **åäº‹å®Ÿäºˆæ¸¬**: ä»‹å…¥å¾Œã®æœªæ¥çŠ¶æ…‹ã‚’äºˆæ¸¬
   $$
   z_{t+1}' = f_\theta(z_t | \text{do}(z_k = \tilde{z}_k))
   $$

**è¨“ç·´ç›®çš„**:

$$
\mathcal{L}_{\text{C-JEPA}} = \mathbb{E} \left[ \| f_\theta(z_{\text{ctx}} | \text{do}(z_k)) - z_{\text{tgt}} \|_2^2 \right]
$$

**å¿œç”¨**: ãƒ­ãƒœãƒƒãƒˆãƒãƒ‹ãƒ”ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã€Œã“ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å‹•ã‹ã™ã¨ä½•ãŒèµ·ãã‚‹ã‹ï¼Ÿã€ï¼‰

#### 3.8.3 Value-guided Action Planning with JEPA

**è«–æ–‡**: "Value-guided action planning with JEPA world models," arXiv:2601.00844, 2025[^3]

JEPAã‚’**å¼·åŒ–å­¦ç¿’**ã«çµ±åˆã—ã€action planningã«ä½¿ç”¨ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

1. **JEPA world model**: $z_{t+1} = f_\theta(z_t, a_t)$
2. **Value network**: $V_\psi(z_t)$ â€” çŠ¶æ…‹ä¾¡å€¤é–¢æ•°
3. **Planning**: Model Predictive Control (MPC)é¢¨ã«æœªæ¥è»Œé“ã‚’æœ€é©åŒ–

**Planning objective**:

$$
a_{t:t+H}^* = \arg\max_{a_{t:t+H}} \sum_{k=0}^H \gamma^k V_\psi(z_{t+k})
$$

ã“ã“ã§$z_{t+k}$ã¯world modelã§äºˆæ¸¬ã€‚

**å®Ÿè£…ï¼ˆJuliaæ¦‚å¿µã‚³ãƒ¼ãƒ‰ï¼‰**:


**å®Ÿé¨“çµæœ**: Atariã‚²ãƒ¼ãƒ ã§å¾“æ¥ã®model-free RLï¼ˆPPOï¼‰ã‚’ä¸Šå›ã‚‹æ€§èƒ½ï¼ˆsample efficiency 3xå‘ä¸Šï¼‰ã€‚

### 3.9 Physics-Informed World Modelsã®æœ€æ–°ç™ºå±•

#### 3.9.1 Separable PINNs (SPINN)

**è«–æ–‡**: Cho et al., "Separable Physics-Informed Neural Networks," arXiv:2306.15969, 2023[^4]

å¾“æ¥ã®PINNsã¯é«˜æ¬¡å…ƒPDEï¼ˆ$d \geq 4$ï¼‰ã§ãƒ¡ãƒ¢ãƒªçˆ†ç™ºã™ã‚‹ã€‚SPINNã¯**è»¸åˆ†é›¢å¯èƒ½**æ§‹é€ ã§æ¬¡å…ƒå‰Šæ¸›ã€‚

**æ ¸å¿ƒçš„ã‚¢ã‚¤ãƒ‡ã‚¢**: PDEè§£ã‚’å¤‰æ•°åˆ†é›¢å½¢å¼ã§è¿‘ä¼¼:

$$
u(x_1, \ldots, x_d) \approx \sum_{i=1}^R u_1^{(i)}(x_1) \cdot u_2^{(i)}(x_2) \cdots u_d^{(i)}(x_d)
$$

ã“ã“ã§å„$u_j^{(i)}: \mathbb{R} \to \mathbb{R}$ã¯1æ¬¡å…ƒNNã€‚

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: æ¨™æº–PINNsãŒ$O(N^d)$ã® collocation pointsã‚’å¿…è¦ã¨ã™ã‚‹ä¸€æ–¹ã€SPINNã¯$O(dN)$ã§æ¸ˆã‚€ã€‚

**å®Ÿè£…ä¾‹**ï¼ˆ2Dç†±æ–¹ç¨‹å¼ï¼‰:

$$
\frac{\partial u}{\partial t} = \alpha \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right)
$$


**æ€§èƒ½**: 10^7 collocation pointsã§è¨“ç·´å¯èƒ½ï¼ˆå¾“æ¥PINNsã®1000å€ï¼‰ã€‚

#### 3.9.2 Conservation-Aware PINNs

**è«–æ–‡**: Cardoso-Bihlo & Bihlo, "Exactly Conservative Physics-Informed Neural Operators," 2025[^5]

ç‰©ç†æ³•å‰‡ï¼ˆè³ªé‡ãƒ»é‹å‹•é‡ãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ï¼‰ã‚’**é›¢æ•£ãƒ¬ãƒ™ãƒ«ã§å³å¯†ã«ä¿è¨¼**ã™ã‚‹ã€‚

**å•é¡Œè¨­å®š**: Navier-Stokesæ–¹ç¨‹å¼ã‚’è§£ãéš›ã€æ¨™æº–PINNsã¯è¿‘ä¼¼èª¤å·®ã«ã‚ˆã‚Šä¿å­˜å‰‡ã‚’ç ´ã‚‹ã€‚

**è§£æ±ºç­–**: **Learnable Adaptive Correction**

$$
u_{\text{corrected}} = u_\theta + \Delta u_{\text{conservation}}
$$

ã“ã“ã§$\Delta u_{\text{conservation}}$ã¯ä¿å­˜å‰‡ã‚’æº€ãŸã™ã‚ˆã†ã«è‡ªå‹•è¨ˆç®—ã€‚

**è³ªé‡ä¿å­˜ã®å ´åˆ**:

$$
\int_\Omega \nabla \cdot \mathbf{u} \, dV = 0
$$

**è£œæ­£é …**:

$$
\Delta \mathbf{u} = \nabla \phi, \quad \text{where } \nabla^2 \phi = -(\nabla \cdot \mathbf{u}_\theta)
$$

ã“ã®$\phi$ã‚’è§£ãã“ã¨ã§ã€$\nabla \cdot (\mathbf{u}_\theta + \nabla \phi) = 0$ãŒå³å¯†ã«æˆç«‹ã€‚

**å®Ÿè£…ã®éµ**: Poissonæ–¹ç¨‹å¼$\nabla^2 \phi = f$ã‚’é«˜é€Ÿã«è§£ãï¼ˆFFTã¾ãŸã¯ multigridæ³•ï¼‰ã€‚


**çµæœ**: ä¿å­˜å‰‡èª¤å·®ãŒæ¨™æº–PINNsã®10^-3ã‹ã‚‰10^-12ã«æ”¹å–„ï¼ˆ9æ¡å‘ä¸Šï¼‰ã€‚

#### 3.9.3 Lagrangian Neural Networks (LNN)

Hamiltonian Neural Networks (HNN) ãŒä½ç›¸ç©ºé–“$(q, p)$ã®**æ­£æº–åº§æ¨™**ã‚’å¿…è¦ã¨ã™ã‚‹ã®ã«å¯¾ã—ã€**Lagrangian Neural Networks (LNN)** ã¯ä»»æ„ã®ä¸€èˆ¬åŒ–åº§æ¨™$(\mathbf{q}, \dot{\mathbf{q}})$ã§å‹•ä½œã™ã‚‹ã€‚ã“ã‚Œã¯é–¢ç¯€è§’åº¦ã‚„å››å…ƒæ•°ãªã©ã€æ­£æº–å¤‰æ›ãŒè‡ªæ˜ã§ãªã„ç³»ã¸ã®é©ç”¨ã‚’å¤§å¹…ã«ç°¡ç´ åŒ–ã™ã‚‹ã€‚

**LNNã®åŸºæœ¬å®šå¼åŒ–**

Lagrangian $L(\mathbf{q}, \dot{\mathbf{q}})$ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯$L_\theta(\mathbf{q}, \dot{\mathbf{q}})$ã§è¿‘ä¼¼ã—ã€Euler-Lagrangeæ–¹ç¨‹å¼ã‚’é€šã˜ã¦åŠ é€Ÿåº¦$\ddot{\mathbf{q}}$ã‚’å°ã:

$$
\frac{d}{dt}\frac{\partial L_\theta}{\partial \dot{\mathbf{q}}} = \frac{\partial L_\theta}{\partial \mathbf{q}}
$$

å·¦è¾ºã‚’é€£é–å¾‹ã§å±•é–‹ã™ã‚‹ã¨:

$$
\frac{\partial^2 L_\theta}{\partial \dot{\mathbf{q}}^2} \ddot{\mathbf{q}} + \frac{\partial^2 L_\theta}{\partial \dot{\mathbf{q}} \partial \mathbf{q}} \dot{\mathbf{q}} = \frac{\partial L_\theta}{\partial \mathbf{q}}
$$

ã“ã‚Œã‚’$\ddot{\mathbf{q}}$ã«ã¤ã„ã¦è§£ãã¨:

$$
\ddot{\mathbf{q}} = \left(\frac{\partial^2 L_\theta}{\partial \dot{\mathbf{q}}^2}\right)^{-1} \left(\frac{\partial L_\theta}{\partial \mathbf{q}} - \frac{\partial^2 L_\theta}{\partial \dot{\mathbf{q}} \partial \mathbf{q}} \dot{\mathbf{q}}\right)
$$

ã“ã‚ŒãŒLNNã®é‹å‹•æ–¹ç¨‹å¼ã§ã‚ã‚Šã€å…¨ã¦ã®å¾®åˆ†ã¯è‡ªå‹•å¾®åˆ†ï¼ˆforward-mode + reverse-modeï¼‰ã§è¨ˆç®—ã•ã‚Œã‚‹ã€‚

**HNNã¨LNNã®æ¯”è¼ƒ**

| æ€§è³ª | HNN | LNN |
|------|-----|-----|
| å¿…è¦ãªåº§æ¨™ | æ­£æº–åº§æ¨™$(q,p)$ | ä»»æ„ã®ä¸€èˆ¬åŒ–åº§æ¨™$(q,\dot{q})$ |
| ä¸»è¦è¨ˆç®— | $\partial H/\partial q$, $\partial H/\partial p$ | $\partial^2 L/\partial \dot{q}^2$ã®é€†è¡Œåˆ— |
| ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ | æ§‹é€ çš„ã«ä¿è¨¼ï¼ˆ$dH/dt=0$ï¼‰ | Noetherã®å®šç†ã§ä¿è¨¼ |
| è¨ˆç®—ã‚³ã‚¹ãƒˆ | $O(n)$ï¼ˆè‡ªå‹•å¾®åˆ†ï¼‰ | $O(n^3)$ï¼ˆé€†è¡Œåˆ—ï¼‰ã¾ãŸã¯$O(n^2)$ï¼ˆCGæ³•ï¼‰ |
| é©ç”¨ç¯„å›² | ä¿å­˜åŠ›å­¦ç³» | é€Ÿåº¦ä¾å­˜åŠ›ï¼ˆæ‘©æ“¦ãªã—ï¼‰ã®ä»»æ„åŠ›å­¦ç³» |

**äºŒé‡æŒ¯ã‚Šå­ã«ã‚ˆã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ã®æ¤œè¨¼**

äºŒé‡æŒ¯ã‚Šå­ï¼ˆ$n=2$ï¼‰ã‚’ä¾‹ã«å–ã‚‹ã€‚ä¸€èˆ¬åŒ–åº§æ¨™$\mathbf{q} = (\theta_1, \theta_2)$ã¨ã™ã‚‹ã¨ã€Lagrangianã¯:

$$
L(\mathbf{q}, \dot{\mathbf{q}}) = \frac{1}{2}(m_1 + m_2)l_1^2 \dot{\theta}_1^2 + \frac{1}{2}m_2 l_2^2 \dot{\theta}_2^2 + m_2 l_1 l_2 \dot{\theta}_1 \dot{\theta}_2 \cos(\theta_1 - \theta_2)$$

$$+ (m_1 + m_2)g l_1 \cos\theta_1 + m_2 g l_2 \cos\theta_2
$$

è¨“ç·´æå¤±ã¯LNNãŒäºˆæ¸¬ã™ã‚‹åŠ é€Ÿåº¦$\hat{\ddot{\mathbf{q}}}$ã‚’çœŸã®è»Œé“ã‹ã‚‰è¨ˆç®—ã—ãŸ$\ddot{\mathbf{q}}^*$ã«åˆã‚ã›ã‚‹MSE:

$$
\mathcal{L}_{\text{LNN}} = \mathbb{E}_{(\mathbf{q}, \dot{\mathbf{q}}, \ddot{\mathbf{q}}) \sim \mathcal{D}} \left[\| \hat{\ddot{\mathbf{q}}}_\theta(\mathbf{q}, \dot{\mathbf{q}}) - \ddot{\mathbf{q}} \|_2^2\right]
$$

ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ã®æ¤œè¨¼æŒ‡æ¨™ã¨ã—ã¦ã€Noetherã®å®šç†ã‹ã‚‰$L$ãŒæ™‚é–“ä¸¦é€²å¯¾ç§°æ€§ã‚’æŒã¤å ´åˆï¼ˆ$\partial L/\partial t = 0$ï¼‰ã€å…¨ã‚¨ãƒãƒ«ã‚®ãƒ¼$E = \dot{\mathbf{q}}^\top \nabla_{\dot{\mathbf{q}}} L_\theta - L_\theta$ãŒä¿å­˜ã•ã‚Œã‚‹ã€‚æ•°å€¤å®Ÿé¨“ã§ã¯ã€æ¨™æº–RNNãŒ1000ã‚¹ãƒ†ãƒƒãƒ—ã§$\Delta E / E_0 \approx 30\%$ã®èª¤å·®ã‚’ç¤ºã™ã®ã«å¯¾ã—ã€LNNã¯$\Delta E / E_0 < 0.1\%$ã‚’ç¶­æŒã™ã‚‹ã“ã¨ãŒ Cranmer et al.ï¼ˆ2020ï¼‰ã§å®Ÿè¨¼ã•ã‚Œã¦ã„ã‚‹ã€‚

### 3.10 Energy-Based World Modelsã®ç†è«–

#### 3.10.1 EB-JEPA: Energy-Based JEPA Library

**è«–æ–‡**: "A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures," arXiv:2602.03604, 2025[^6]

JEPAã‚’**Energy-Based Model**ã¨ã—ã¦å†å®šå¼åŒ–ã€‚

**å‹•æ©Ÿ**: å¾“æ¥ã®JEPAã¯L2æå¤±ã§è¨“ç·´ â†’ å˜å³°æ€§Gaussianä»®å®šã€‚è¤‡é›‘ãªå¤šå³°æ€§åˆ†å¸ƒã‚’è¡¨ç¾ã§ããªã„ã€‚

**Energy-based formulation**:

$$
p(z_{\text{tgt}} | z_{\text{ctx}}) = \frac{\exp(-E_\theta(z_{\text{ctx}}, z_{\text{tgt}}))}{Z(z_{\text{ctx}})}
$$

ã“ã“ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°:

$$
E_\theta(z_{\text{ctx}}, z_{\text{tgt}}) = \| f_\theta(z_{\text{ctx}}) - z_{\text{tgt}} \|_2^2
$$

**è¨“ç·´**: Noise Contrastive Estimation (NCE)

$$
\mathcal{L}_{\text{NCE}} = -\mathbb{E}_{z^+} [\log \sigma(-E_\theta(z_{\text{ctx}}, z^+))] - \mathbb{E}_{z^-} [\log \sigma(E_\theta(z_{\text{ctx}}, z^-))]
$$

ã“ã“ã§$z^+$ã¯çœŸã® targetã€$z^-$ã¯è² ä¾‹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã€‚

**å®Ÿè£…**:


**åˆ©ç‚¹**:

- **å¤šå³°æ€§**: è¤‡æ•°ã®å¯èƒ½ãªæœªæ¥ã‚’è¡¨ç¾ï¼ˆä¾‹: å‹•ç”»äºˆæ¸¬ã§è¤‡æ•°ã®è»Œé“å€™è£œï¼‰
- **ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®é«˜ã• = ä¸ç¢ºå®Ÿæ€§

#### 3.10.2 Cognitively Inspired Energy-Based World Models

**è«–æ–‡**: "Cognitively Inspired Energy-Based World Models," arXiv:2406.08862, 2024[^7]

èªçŸ¥ç§‘å­¦ã®**äºˆæ¸¬ç¬¦å·åŒ–ï¼ˆPredictive Codingï¼‰**ç†è«–ã‚’World Modelsã«çµ±åˆã€‚

**è„³ã®äºˆæ¸¬ç¬¦å·åŒ–**:

è„³ã¯å¸¸ã«**äºˆæ¸¬**ã‚’ç”Ÿæˆã—ã€**äºˆæ¸¬èª¤å·®**ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ã€‚

$$
\text{Prediction Error} = x_{\text{observed}} - x_{\text{predicted}}
$$

**Energy-Based World Modelã¨ã®å¯¾å¿œ**:

$$
E(x_t, a_t, x_{t+1}) = \| x_{t+1} - f_\theta(x_t, a_t) \|_2^2 + \text{Prior}(x_{t+1})
$$

**éšå±¤çš„äºˆæ¸¬**:

ãƒ¬ãƒ™ãƒ«1ï¼ˆä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ï¼‰â†’ ãƒ¬ãƒ™ãƒ«2ï¼ˆä¸­ãƒ¬ãƒ™ãƒ«ï¼‰â†’ ãƒ¬ãƒ™ãƒ«3ï¼ˆé«˜ãƒ¬ãƒ™ãƒ«æŠ½è±¡æ¦‚å¿µï¼‰

å„ãƒ¬ãƒ™ãƒ«ã§äºˆæ¸¬èª¤å·®ã‚’è¨ˆç®—:

$$
\epsilon_l = h_l - f_l(h_{l+1})
$$

**Total energy**:

$$
E_{\text{total}} = \sum_{l=1}^L \lambda_l \| \epsilon_l \|_2^2
$$

**è¨“ç·´**: ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– = éšå±¤çš„äºˆæ¸¬èª¤å·®æœ€å°åŒ–

**èªçŸ¥çš„åˆ©ç‚¹**:

- **æ³¨æ„æ©Ÿæ§‹**: é«˜ã‚¨ãƒãƒ«ã‚®ãƒ¼é ˜åŸŸï¼ˆäºˆæ¸¬èª¤å·®å¤§ï¼‰ã«æ³¨æ„ã‚’å‘ã‘ã‚‹
- **èƒ½å‹•æ¨è«–**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’æœ€å°åŒ–ã™ã‚‹action $a_t$ã‚’é¸æŠ
- **æ„è­˜**: é«˜ãƒ¬ãƒ™ãƒ«äºˆæ¸¬èª¤å·®ãŒé–¾å€¤ã‚’è¶…ãˆã‚‹ã¨ã€Œæ„è­˜ã€ã«ä¸Šã‚‹


**å®Ÿé¨“çµæœ**: ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§ã€æ¨™æº–World Modelsã‚ˆã‚Š30%ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡å‘ä¸Šã€‚

#### 3.10.3 Autoregressive LMs as Energy-Based Models

**è«–æ–‡**: "Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction," arXiv:2512.15605, 2024[^8]

**é©šãã®ç™ºè¦‹**: Autoregressive LMsï¼ˆGPTç³»ï¼‰ã¯å®Ÿã¯**Energy-Based Models**ã¨ç­‰ä¾¡ï¼

**å®šç†**: ARMã¨EBMã®é–“ã«**æ˜ç¤ºçš„å…¨å˜å°„**ãŒå­˜åœ¨:

$$
p_{\text{ARM}}(x_{1:T}) = \prod_{t=1}^T p(x_t | x_{<t}) \iff p_{\text{EBM}}(x_{1:T}) = \frac{\exp(-E(x_{1:T}))}{Z}
$$

ã“ã“ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°:

$$
E(x_{1:T}) = -\sum_{t=1}^T \log p(x_t | x_{<t})
$$

**Soft Bellmanæ–¹ç¨‹å¼ã¨ã®æ¥ç¶š**:

$$
V(x_{<t}) = \log \sum_{x_t} \exp(r(x_t | x_{<t}) + V(x_{\leq t}))
$$

**Transfusionã¸ã®ç¤ºå”†**: ãƒ†ã‚­ã‚¹ãƒˆï¼ˆARï¼‰ã¨ç”»åƒï¼ˆDiffusionï¼‰ã®çµ±ä¸€ã¯ã€å®Ÿã¯**ä¸¡æ–¹ã¨ã‚‚EBM**ã¨ã„ã†è¦–ç‚¹ã‹ã‚‰è‡ªç„¶ã«ç†è§£ã§ãã‚‹ï¼

$$
E_{\text{Transfusion}}(x_{\text{text}}, x_{\text{image}}) = E_{\text{ARM}}(x_{\text{text}}) + E_{\text{Diffusion}}(x_{\text{image}})
$$

ã“ã‚Œã¯**å˜ä¸€ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**ã®ç•°ãªã‚‹åˆ†è§£ã«éããªã„ã€‚

> **âš ï¸ Warning:** **æ·±ã„æ´å¯Ÿ**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€ç†è«–ã¯ã€ŒEnergy-Based World Modelsã€ã«åæŸã—ã¦ã„ã‚‹ã€‚VAEã€GANã€Diffusionã€Transfusionã€JEPAã¯å…¨ã¦**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã®ç•°ãªã‚‹è¨“ç·´ãƒ»æ¨è«–æ–¹æ³•**ã¨ã—ã¦ç†è§£ã§ãã‚‹ã€‚
>
> ç¬¬34å›ã§å­¦ã‚“ã EBMãŒã€å®Ÿã¯ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®**æœ€ã‚‚ä¸€èˆ¬çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ã ã£ãŸï¼

> **Note:** **é€²æ—**: å…¨ä½“ã®70%å®Œäº†ã€‚æœ€æ–°ã®JEPAç™ºå±•ï¼ˆLeJEPAã€Causal-JEPAã€Value-guided planningï¼‰ã€Physics-Informed World Modelsï¼ˆSPINNã€Conservation-Aware PINNsï¼‰ã€Energy-Basedç†è«–ï¼ˆEB-JEPAã€Predictive Codingã€ARM-EBMåŒå€¤æ€§ï¼‰ã‚’å®Œå…¨ç¿’å¾—ã€‚2020-2025ã®æœ€å…ˆç«¯ç ”ç©¶ã‚’çµ±åˆã—ãŸã€‚

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” JEPA World Modelã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè£…

### 4.1 I-JEPAã®æœ€å°å®Ÿè£…


**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**:

1. **EMAæ›´æ–°**: Target encoderã¯momentum $\tau=0.996$ã§ã‚†ã£ãã‚Šæ›´æ–° â†’ collapseå›é¿
2. **Stop gradient**: Target encoderã®å‡ºåŠ›ã«å‹¾é…ã‚’æµã•ãªã„ï¼ˆ`Zygote.@ignore`ï¼‰
3. **Mask strategy**: ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ‘ãƒƒãƒã®50%ã‚’ãƒã‚¹ã‚¯ â†’ æ§‹é€ çš„äºˆæ¸¬ã‚’å­¦ç¿’

### 4.2 V-JEPAã®æ™‚ç©ºé–“æ‹¡å¼µ


**V-JEPAã®ç‰¹å¾´**:

1. **3D Convolution**: æ™‚ç©ºé–“ç‰¹å¾´ã‚’åŒæ™‚ã«æŠ½å‡º
2. **Temporal Predictor**: Transformer-basedã§éå»ã‹ã‚‰æœªæ¥ã‚’äºˆæ¸¬
3. **Positional Encoding**: æ™‚é–“ä½ç½®æƒ…å ±ã‚’æ˜ç¤ºçš„ã«ä¸ãˆã‚‹

### 4.3 Physics-Informed World Modelå®Ÿè£…


**Physics-Informedå®Ÿè£…ã®éµ**:

1. **è‡ªå‹•å¾®åˆ†**: Hamiltonianã®åå¾®åˆ†ã‚’è‡ªå‹•è¨ˆç®—
2. **Symplecticç©åˆ†**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ã‚’æ•°å€¤çš„ã«ã‚‚ä¿è¨¼
3. **æ§‹é€ çš„åˆ¶ç´„**: Hamiltonianæ§‹é€ ã‚’å¼·åˆ¶ â†’ ç‰©ç†æ³•å‰‡ã‚’å­¦ç¿’

### 4.4 Energy-Based World Model with NCE


**Energy-Basedæ¨è«–ã®ç‰¹å¾´**:

1. **Gradient-based inference**: ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ã§æœ€é©ãªæ¬¡çŠ¶æ…‹ã‚’æ¢ç´¢
2. **å¤šå³°æ€§è¡¨ç¾**: ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ãŒè¤‡æ•°ã®æ¥µå°å€¤ã‚’æŒã¦ã‚‹ â†’ è¤‡æ•°ã®å¯èƒ½ãªæœªæ¥
3. **Uncertainty**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®é«˜ã• = ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–

<details><summary>å®Ÿè£…ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ</summary>

âœ… **I-JEPA**: EMAæ›´æ–°ã€stop gradientã€mask strategy
âœ… **V-JEPA**: 3D convolutionã€temporal predictorã€positional encoding
âœ… **Hamiltonian NN**: è‡ªå‹•å¾®åˆ†ã€symplectic integrationã€energy conservation
âœ… **Energy-Based WM**: NCEè¨“ç·´ã€gradient-based inferenceã€å¤šå³°æ€§å¯¾å¿œ

å…¨ã¦æœ¬ç•ªæŠ•å…¥å¯èƒ½ãªã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè£…ï¼ˆProduction-readyã«ã™ã‚‹ã«ã¯ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–ã€distributedè¨“ç·´ã€checkpointingç­‰ãŒå¿…è¦ï¼‰ã€‚

</details>

> **Note:** **é€²æ—**: å…¨ä½“ã®85%å®Œäº†ã€‚4ã¤ã®ä¸»è¦World Modelã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆI-JEPAã€V-JEPAã€Hamiltonian NNã€Energy-Based WMï¼‰ã‚’å®Œå…¨å®Ÿè£…ã—ãŸã€‚ç†è«–ã‹ã‚‰å®Ÿè£…ã¸ã®æ©‹æ¸¡ã—å®Œäº†ã€‚

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Balestriero, R., & LeCun, Y. (2024). LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics. arXiv:2511.08544.
<https://arxiv.org/abs/2511.08544>

[^2]: Nam, H., Le Lidec, Q., Maes, L., LeCun, Y., & Balestriero, R. (2025). Causal-JEPA: Learning World Models through Object-Level Latent Interventions. arXiv:2602.11389.
<https://arxiv.org/abs/2602.11389>

[^3]: Destrade, M., Bounou, O., Le Lidec, Q., Ponce, J., & LeCun, Y. (2025). Value-guided action planning with JEPA world models. arXiv:2601.00844.
<https://arxiv.org/abs/2601.00844>

[^4]: Cho, J., et al. (2023). Separable Physics-Informed Neural Networks. In: Koyejo, S., et al. (eds) Advances in Neural Information Processing Systems 36 (NeurIPS 2023).
<https://arxiv.org/abs/2306.15969>

[^5]: Cardoso-Bihlo, E. & Bihlo, A. (2024). Exactly conservative physics-informed neural networks and deep operator networks for dynamical systems. Neural Networks, 182, 106826. arXiv:2311.14131.
<https://arxiv.org/abs/2311.14131>

[^6]: Terver, B., Balestriero, R., Dervishi, M., Fan, D., Garrido, Q., Nagarajan, T., Sinha, K., Zhang, W., Rabbat, M., LeCun, Y., & Bar, A. (2025). A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures. arXiv:2602.03604.
<https://arxiv.org/abs/2602.03604>

[^7]: Gladstone, A., et al. (2024). Cognitively Inspired Energy-Based World Models. arXiv:2406.08862.
<https://arxiv.org/abs/2406.08862>

[^8]: Blondel, M., Sander, M. E., Vivier-Ardisson, G., Liu, T., & Roulet, V. (2024). Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction. arXiv:2512.15605.
<https://arxiv.org/abs/2512.15605>

---


> Progress: 50%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $epoch: Loss = $ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ¯ 5. ã¾ã¨ã‚ â€” World Modelsã®æœ¬è³ª

### 5.1 Part 1ã§å­¦ã‚“ã ã“ã¨

æœ¬Partã§ã¯ã€World Modelsã®**ç†è«–çš„åŸºç›¤**ã‚’å®Œå…¨ã«æ§‹ç¯‰ã—ãŸ:

**æ ¸å¿ƒæ¦‚å¿µ**:
- ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹ã¯ã€Œç”»åƒç”Ÿæˆã€ã§ã¯ãªãã€Œç’°å¢ƒç†è§£+äºˆæ¸¬+ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€
- JEPAã¯ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬ã™ã‚‹é©å‘½çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- Physics-Informed World Modelsã¯ç‰©ç†æ³•å‰‡ã‚’åŸ‹ã‚è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã¨sim2realã‚’æ”¹å–„
- Energy-Basedå®šå¼åŒ–ã«ã‚ˆã‚Šã€å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆVAE/GAN/Diffusion/JEPAï¼‰ãŒçµ±ä¸€ç†è«–ã«åæŸ

**æ•°å­¦çš„æ­¦å™¨åº«**:
- I/V/VL-JEPAã€LeJEPAã€Causal-JEPAã®å®Œå…¨ç†è«–
- Transfusionã®çµ±ä¸€æå¤±é–¢æ•°ï¼ˆAR + Diffusionï¼‰
- Hamiltonian NNã¨SPINNã«ã‚ˆã‚‹ç‰©ç†æ³•å‰‡å­¦ç¿’
- EB-JEPAã¨Predictive Codingã«ã‚ˆã‚‹èªçŸ¥ç§‘å­¦çš„å®šå¼åŒ–

**å®Ÿè£…ã‚¹ã‚­ãƒ«**:
- 4ã¤ã®ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆI-JEPAã€V-JEPAã€HNNã€Energy-Based WMï¼‰ã®Juliaå®Ÿè£…
- EMAæ›´æ–°ã€Stop gradientã€NCEã€Gradient-based inferenceã®å®Ÿè·µ

### 5.2 Part 2ã¸ã®æ¥ç¶š

Part 2ã§ã¯ã€ã“ã‚Œã‚‰ã®ç†è«–ã‚’**å®Ÿä¸–ç•Œå¿œç”¨**ã«å±•é–‹ã™ã‚‹:

- å¼·åŒ–å­¦ç¿’çµ±åˆï¼ˆDreamerV3ã€MuZeroã€IRISï¼‰
- ãƒ­ãƒœãƒƒãƒˆãƒãƒ‹ãƒ”ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆRT-1/RT-2ã€GNMï¼‰
- å‹•ç”»ç”Ÿæˆï¼ˆSoraã€VideoPoetã€WALTï¼‰
- ç§‘å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆAlphaFold3ã€Climate modelingï¼‰

Part 1ã®ç†è«–ã¯**å…¨ã¦ã®å¿œç”¨ã®åŸºç›¤**ã¨ãªã‚‹ã€‚æ¬¡å›ã¯ã“ã‚Œã‚‰ã‚’å®Ÿè·µã™ã‚‹ã€‚

---


---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
