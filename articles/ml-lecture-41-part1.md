---
title: "ç¬¬41å›: World Models & ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ç†è«–ğŸŒ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼""
emoji: "ğŸŒ"
type: "tech"
topics: ["machinelearning", "deeplearning", "worldmodels", "julia", "jepa"]
published: true
---

# ç¬¬41å›: World Models & ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ç†è«– ğŸŒ

**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹ã¯"ç†è§£"ã ã£ãŸ**

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” 1ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹

ç¬¬40å›ã§Consistency Modelsã«ã‚ˆã‚‹1ã‚¹ãƒ†ãƒƒãƒ—é«˜é€Ÿç”Ÿæˆã‚’å®Ÿç¾ã—ãŸã€‚ã ãŒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çœŸã®ç›®çš„ã¯ä½•ã ã£ãŸã®ã‹ï¼Ÿ

å˜ã«ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã¯ãªã„ã€‚**ç’°å¢ƒã®æ§‹é€ ã‚’ç†è§£ã—ã€æœªæ¥ã‚’äºˆæ¸¬ã—ã€è¡Œå‹•ã®çµæœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ã“ã¨**ã ã€‚

```julia
# World Modelã®æœ¬è³ª: 1ãƒ•ãƒ¬ãƒ¼ãƒ  â†’ æœªæ¥ã®äºˆæ¸¬
using Lux, Random

# è¦³æ¸¬ x_t ã‹ã‚‰æ½œåœ¨è¡¨ç¾ z_t ã‚’æŠ½å‡º
encoder = Chain(Conv((3,3), 3 => 64, relu), AdaptiveMeanPool((1,1)), FlattenLayer())

# æ½œåœ¨ç©ºé–“ã§æ¬¡çŠ¶æ…‹ã‚’äºˆæ¸¬ (actionæ¡ä»¶ä»˜ã)
predictor = Dense(64 + 4 => 64, tanh)  # 4æ¬¡å…ƒaction space

# åˆæœŸè¦³æ¸¬
x = rand(Float32, 64, 64, 3, 1)
a = rand(Float32, 4, 1)  # action

# æ½œåœ¨çŠ¶æ…‹æŠ½å‡º â†’ actionæ¡ä»¶ä»˜ãäºˆæ¸¬
z = encoder(x, ps, st)[1]
z_next = predictor(vcat(z, a), ps_pred, st_pred)[1]

# å‡ºåŠ›: z_next âˆˆ â„^64 (predicted next latent state)
```

**ã“ã‚ŒãŒä½•ã‚’ã—ã¦ã„ã‚‹ã‹ï¼Ÿ**

1ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¦³æ¸¬$x_t$ã‚’æ½œåœ¨è¡¨ç¾$z_t$ã«åœ§ç¸®ã—ã€action $a_t$ã‚’ä¸ãˆã¦æ¬¡çŠ¶æ…‹$z_{t+1}$ã‚’äºˆæ¸¬ã™ã‚‹ã€‚

ãƒ”ã‚¯ã‚»ãƒ«ã¯ç”Ÿæˆã—ãªã„ã€‚**ä¸–ç•Œã®æ½œåœ¨æ§‹é€ ã‚’äºˆæ¸¬ã™ã‚‹ã€‚**

$$
z_{t+1} = f_\theta(z_t, a_t)
$$

ã“ã‚ŒãŒWorld Modelã®æ•°å­¦ã ã€‚

:::message
**é€²æ—**: å…¨ä½“ã®3%å®Œäº†ã€‚Consistency Modelsã§1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆã‚’å®Ÿç¾ã—ãŸãŒã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çœŸã®ç›®çš„ã¯ã€Œç†è§£ã€ã ã£ãŸã€‚ç’°å¢ƒã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ç†è«–ã¸ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” World Modelsã®3ã¤ã®é¡”

### 1.1 ç”Ÿæˆ vs ç†è§£ vs ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯3ã¤ã®ãƒ¬ãƒ™ãƒ«ã«åˆ†é¡ã§ãã‚‹:

| ãƒ¬ãƒ™ãƒ« | ç›®çš„ | å…¥å‡ºåŠ› | ä»£è¡¨æ‰‹æ³• |
|:------|:-----|:------|:---------|
| **Level 1: ç”Ÿæˆ** | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ« | $p(x)$ | VAE, GAN, Diffusion |
| **Level 2: æ¡ä»¶ä»˜ãç”Ÿæˆ** | æ¡ä»¶ã‹ã‚‰ç”Ÿæˆ | $p(x|c)$ | LDM, CFG |
| **Level 3: World Models** | **ç’°å¢ƒã®ç†è§£+äºˆæ¸¬+ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | $p(x_{t+1}|x_{\leq t}, a_t)$ | JEPA, V-JEPA, Transfusion |

World Modelsã¯**è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã§ãã‚‹**æœ€é«˜ãƒ¬ãƒ™ãƒ«ã ã€‚

### 1.2 JEPAã®3å¤‰ç¨®ã‚’å‹•ã‹ã™

```julia
# I-JEPA: ç”»åƒã®ä¸€éƒ¨ã‹ã‚‰ä»–éƒ¨åˆ†ã‚’äºˆæ¸¬
# Input: masked image patches
x_context = x[:, :, 1:32, :]  # å·¦åŠåˆ†
x_target_mask = [33:64]       # å³åŠåˆ†ã‚’ãƒã‚¹ã‚¯

# Context encoder â†’ Predictor â†’ Target prediction
z_context = context_encoder(x_context, ps_ctx, st_ctx)[1]
z_pred = predictor(z_context, mask_tokens, ps_pred, st_pred)[1]

# âŒ ãƒ”ã‚¯ã‚»ãƒ«ã‚’äºˆæ¸¬ã—ãªã„
# âœ… æ½œåœ¨è¡¨ç¾ã‚’äºˆæ¸¬ã™ã‚‹
```

```julia
# V-JEPA: å‹•ç”»ã®ä¸€éƒ¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰æœªæ¥ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ½œåœ¨è¡¨ç¾ã‚’äºˆæ¸¬
# Input: video sequence [B, T, H, W, C]
video = rand(Float32, 1, 16, 64, 64, 3)  # 16 frames
context_frames = video[:, 1:8, :, :, :]   # å‰åŠ8ãƒ•ãƒ¬ãƒ¼ãƒ 
target_frames = video[:, 9:16, :, :, :]   # å¾ŒåŠ8ãƒ•ãƒ¬ãƒ¼ãƒ 

# Context encoder â†’ Temporal predictor
z_ctx_video = video_encoder(context_frames, ps_v, st_v)[1]
z_pred_video = temporal_predictor(z_ctx_video, ps_tp, st_tp)[1]
```

```julia
# Transfusion: ãƒ†ã‚­ã‚¹ãƒˆ(AR) + ç”»åƒ(Diffusion) ã‚’çµ±ä¸€ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†
# Text: autoregressive (next token prediction)
# Image: diffusion (denoising)

# ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³
text_tokens = [101, 2054, 2003]  # "What is"

# ç”»åƒãƒ‘ãƒƒãƒ (continuous vectors)
image_patches = rand(Float32, 512, 16)  # 16 patches Ã— 512 dim

# Transfusionã®çµ±ä¸€å‡¦ç†
# Text: next token prediction loss
loss_text = cross_entropy(model(text_tokens), text_tokens[2:end])

# Image: diffusion loss
t = rand(1:1000)
noise = randn(size(image_patches))
x_t = sqrt(Î±[t]) * image_patches + sqrt(1 - Î±[t]) * noise
loss_image = mse(model(x_t, t), noise)

# ç·åˆloss
loss = loss_text + loss_image
```

### 1.3 World Modelsã®å¿œç”¨é ˜åŸŸ

| å¿œç”¨ | ç›®çš„ | World Modelã®å½¹å‰² |
|:-----|:-----|:-----------------|
| **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹** | ç’°å¢ƒæ“ä½œ | è¡Œå‹•çµæœã®äº‹å‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ |
| **è‡ªå‹•é‹è»¢** | äºˆæ¸¬åˆ¶å¾¡ | ä»–è»Šãƒ»æ­©è¡Œè€…ã®æœªæ¥è»Œé“äºˆæ¸¬ |
| **å¼·åŒ–å­¦ç¿’** | ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚° | Model-based RL (MuZero, Dreamer) |
| **ç§‘å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | ç‰©ç†æ³•å‰‡å­¦ç¿’ | å¾®åˆ†æ–¹ç¨‹å¼ã‚’å­¦ç¿’ã§è¿‘ä¼¼ |

:::details PyTorchã¨ã®å¯¾å¿œï¼ˆå‚è€ƒï¼‰
```python
# PyTorchç‰ˆ JEPA predictor
import torch.nn as nn

class JEPAPredictor(nn.Module):
    def __init__(self, dim=768, num_heads=12):
        super().__init__()
        self.cross_attn = nn.MultiheadAttention(dim, num_heads)
        self.ffn = nn.Sequential(
            nn.Linear(dim, 4 * dim),
            nn.GELU(),
            nn.Linear(4 * dim, dim)
        )

    def forward(self, context, mask_tokens):
        # Cross-attention: mask_tokens attend to context
        pred, _ = self.cross_attn(mask_tokens, context, context)
        pred = self.ffn(pred)
        return pred
```

Juliaã§ã¯å‹ã‚·ã‚¹ãƒ†ãƒ ã§ã“ã‚Œã‚’è‡ªç„¶ã«è¡¨ç¾ã§ãã‚‹ã€‚
:::

:::message
**é€²æ—**: å…¨ä½“ã®10%å®Œäº†ã€‚World Modelsã®3ãƒ¬ãƒ™ãƒ«åˆ†é¡ã‚’ç†è§£ã—ãŸã€‚JEPAã¯ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬ã™ã‚‹é©å‘½çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœWorld ModelsãŒæœ€çµ‚åˆ°é”ç‚¹ã‹

### 2.1 ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–ç³»è­œ

```mermaid
graph TD
    A[VAE: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«] --> B[GAN: æ•µå¯¾çš„å­¦ç¿’]
    B --> C[Diffusion: ã‚¹ã‚³ã‚¢é–¢æ•°]
    C --> D[LDM: æ½œåœ¨ç©ºé–“æ‹¡æ•£]
    D --> E[Consistency Models: 1-stepç”Ÿæˆ]
    E --> F[World Models: ç’°å¢ƒç†è§£]

    style F fill:#ff9,stroke:#333,stroke-width:4px
```

**ãªãœWorld ModelsãŒæœ€çµ‚å½¢æ…‹ã‹ï¼Ÿ**

1. **ç”Ÿæˆã¯æ‰‹æ®µã€ç†è§£ãŒç›®çš„**: ç”»åƒç”Ÿæˆã¯ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ä¸€éƒ¨ã‚’ã‚µãƒ³ãƒ—ãƒ«ã™ã‚‹ã ã‘ã€‚World Modelsã¯ç’°å¢ƒã®**å› æœæ§‹é€ **ã‚’ç†è§£ã™ã‚‹
2. **è¡Œå‹•æ¡ä»¶ä»˜ãäºˆæ¸¬**: $p(x_{t+1}|x_{\leq t}, a_t)$ â€” è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã§ãã‚‹
3. **ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒªãƒ¼**: ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’å›é¿ã—ã€æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬ã™ã‚‹åŠ¹ç‡æ€§

### 2.2 Course IVã§ã®ä½ç½®ã¥ã‘

| å› | ãƒ†ãƒ¼ãƒ | World Modelsã¸ã®æ¥ç¶š |
|:---|:------|:--------------------|
| **ç¬¬33å›** | Normalizing Flows | å¯é€†å¤‰æ› â†’ æ±ºå®šè«–çš„å†™åƒã®é™ç•Œ |
| **ç¬¬34å›** | EBM | ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° â†’ **Energy-based World Models** |
| **ç¬¬35å›** | Score Matching | ã‚¹ã‚³ã‚¢é–¢æ•° â†’ å‹•çš„éç¨‹ã®å­¦ç¿’ |
| **ç¬¬36å›** | DDPM | Forward/Reverse â†’ æ™‚ç³»åˆ—äºˆæ¸¬ã®åŸºç›¤ |
| **ç¬¬37å›** | SDE/ODE | é€£ç¶šæ™‚é–“ç¢ºç‡éç¨‹ â†’ ç‰©ç†æ³•å‰‡å­¦ç¿’ |
| **ç¬¬38å›** | Flow Matching | OTè¦–ç‚¹ â†’ **æœ€é©è¼¸é€ã¨ã—ã¦ã®World Models** |
| **ç¬¬39å›** | LDM | æ½œåœ¨ç©ºé–“æ‹¡æ•£ â†’ **æ½œåœ¨ç©ºé–“äºˆæ¸¬** |
| **ç¬¬40å›** | Consistency Models | 1-stepç”Ÿæˆ â†’ é«˜é€Ÿæ¨è«– |
| **ç¬¬41å›** | **World Models** | **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹** |

### 2.3 æ¾å°¾ç ”ã¨ã®æ±ºå®šçš„ãªé•ã„

| é …ç›® | æ¾å°¾ç ” | æœ¬è¬›ç¾© |
|:-----|:------|:------|
| **World Modelsæ‰±ã„** | è¨€åŠãªã— | **å®Œå…¨ç†è«–åŒ–** |
| **JEPA** | è§¦ã‚Œãªã„ | I-JEPA / V-JEPA / VL-JEPAå®Œå…¨è§£èª¬ |
| **Transfusion** | æ‰±ã‚ãªã„ | **AR+Diffusionçµ±ä¸€ç†è«–ã®æ•°å­¦** |
| **ç‰©ç†æ³•å‰‡å­¦ç¿’** | æ‰±ã‚ãªã„ | Physics-Informed World Modelsæ·±æ˜ã‚Š |
| **å®Ÿè£…** | ãªã— | Julia JEPAã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè£… |

### 2.4 å­¦ç¿’æˆ¦ç•¥

```mermaid
graph LR
    A[ç¬¬41å›: ç†è«–ç†è§£] --> B[ç¬¬42å›: çµ±ä¸€ç†è«–]
    A --> C[ç¬¬43å› DiT/FLUXå®Ÿè£…]
    A --> D[ç¬¬45å› Videoç”Ÿæˆ]
    A --> E[ç¬¬47å› Embodied AI]

    style A fill:#f9f,stroke:#333,stroke-width:2px
```

World Modelsã¯**å…¨ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆç”»åƒãƒ»å‹•ç”»ãƒ»ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ãƒ»ç§‘å­¦ï¼‰ã®çµ±ä¸€åŸºç›¤**ã ã€‚

:::details Trojan Horse â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®"æœ¬å½“ã®ç›®çš„"
ç¬¬1å›ã‹ã‚‰38å›ã¾ã§ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€Œç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã€æŠ€è¡“ã¨ã—ã¦å­¦ã‚“ã§ããŸã€‚

ã ãŒLeCunãŒæå”±ã™ã‚‹JEPAã¯**ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹**ã€‚

**ç”Ÿæˆã¯å‰¯ç”£ç‰©ã«éããªã‹ã£ãŸ**ã€‚çœŸã®ç›®çš„ã¯**ç’°å¢ƒã®å› æœæ§‹é€ ã‚’ç†è§£ã—ã€è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨**ã ã€‚

ã“ã‚ŒãŒTrojan Horseã®æœ€çµ‚å½¢æ…‹ã ã€‚ã€Œç”ŸæˆAIã€ã¯ã€Œç†è§£AIã€ã«é€²åŒ–ã™ã‚‹ã€‚
:::

:::message
**é€²æ—**: å…¨ä½“ã®20%å®Œäº†ã€‚ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–ç³»è­œã‚’ç†è§£ã—ãŸã€‚World Modelsã¯ç”Ÿæˆã®å…ˆã«ã‚ã‚‹ã€Œç†è§£+äºˆæ¸¬+ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ã®çµ±åˆæ¦‚å¿µã ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” World Modelsã®æ•°å­¦çš„åŸºç¤

### 3.1 World Modelã®å®šç¾©

**å®šç¾©**: World Model $\mathcal{M}$ã¯ç’°å¢ƒã®æ½œåœ¨è¡¨ç¾$z_t$ã¨é·ç§»é–¢æ•°$f_\theta$ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ç¢ºç‡çš„ã‚·ã‚¹ãƒ†ãƒ ã§ã‚ã‚‹ã€‚

$$
\begin{aligned}
\text{Encoder: } & z_t = \text{Enc}_\phi(x_t) \\
\text{Predictor: } & z_{t+1} = f_\theta(z_t, a_t) + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \Sigma) \\
\text{Decoder: } & \hat{x}_{t+1} = \text{Dec}_\psi(z_{t+1})
\end{aligned}
$$

**ãªãœæ½œåœ¨ç©ºé–“ã‹ï¼Ÿ**

- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ $x \in \mathbb{R}^{H \times W \times C}$ ã¯é«˜æ¬¡å…ƒï¼ˆ$H=256, W=256, C=3 \Rightarrow 196,608$æ¬¡å…ƒï¼‰
- æ½œåœ¨ç©ºé–“ $z \in \mathbb{R}^d$ ã¯ä½æ¬¡å…ƒï¼ˆ$d=256$ç¨‹åº¦ï¼‰ã§**æ§‹é€ çš„è¡¨ç¾**ã‚’ç²å¾—

**è¨“ç·´ç›®æ¨™**: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\{(x_t, a_t, x_{t+1})\}_{t=1}^T$ ã‹ã‚‰$\theta, \phi, \psi$ã‚’å­¦ç¿’

$$
\mathcal{L}(\theta, \phi, \psi) = \mathbb{E}_{(x_t, a_t, x_{t+1})} \left[ \| \text{Dec}_\psi(f_\theta(\text{Enc}_\phi(x_t), a_t)) - x_{t+1} \|_2^2 \right]
$$

### 3.2 JEPAç†è«–: Joint-Embedding Predictive Architecture

#### 3.2.1 I-JEPA (Image-based JEPA)

**è«–æ–‡**: Assran et al., "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture," CVPR 2023

**å‹•æ©Ÿ**: ç”»åƒç”Ÿæˆï¼ˆpixel reconstructionï¼‰ã¯ä½ãƒ¬ãƒ™ãƒ«è©³ç´°ã«éå‰°é©åˆã—ã€é«˜ãƒ¬ãƒ™ãƒ«æŠ½è±¡è¡¨ç¾ã‚’å­¦ç¿’ã—ã«ãã„ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

```mermaid
graph LR
    A[Context patches<br>x_ctx] --> B[Context Encoder<br>s_Î¸]
    B --> C[Latent z_ctx]
    C --> D[Predictor<br>f_Î¸]
    E[Target patches<br>x_tgt] --> F[Target Encoder<br>s_Î¸ EMA]
    F --> G[Latent z_tgt]
    D --> H[Predicted z_pred]
    H -.loss.-> G

    style D fill:#ff9,stroke:#333,stroke-width:2px
    style H fill:#f99,stroke:#333,stroke-width:2px
```

**æå¤±é–¢æ•°**:

$$
\mathcal{L}_{\text{I-JEPA}} = \mathbb{E}_{x, M} \left[ \| f_\theta(s_\theta(x_{\text{ctx}}), M) - \bar{s}_\theta(x_{\text{tgt}}) \|_2^2 \right]
$$

ã“ã“ã§:
- $x_{\text{ctx}}$: ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„context patches
- $x_{\text{tgt}}$: ãƒã‚¹ã‚¯ã•ã‚ŒãŸtarget patches
- $M$: mask tokens (positional encoding)
- $s_\theta$: context encoder (trainable)
- $\bar{s}_\theta$: target encoder (EMAæ›´æ–°)
- $f_\theta$: predictor

**é‡è¦ãªç‰¹æ€§**:

1. **ãƒ”ã‚¯ã‚»ãƒ«å†æ§‹æˆãªã—**: $x_{\text{tgt}}$ã‚’ç”Ÿæˆã›ãšã€æ½œåœ¨è¡¨ç¾$z_{\text{tgt}}$ã‚’äºˆæ¸¬
2. **EMA target encoder**: $\bar{\theta} \leftarrow \tau \bar{\theta} + (1-\tau)\theta$ ã§collapseå›é¿
3. **Mask strategy**: ãƒ©ãƒ³ãƒ€ãƒ ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¹ã‚¯ï¼ˆGrid-basedï¼‰ã§æ§‹é€ çš„äºˆæ¸¬ã‚’ä¿ƒé€²

**æ•°å€¤ä¾‹**:

```julia
# Context: ç”»åƒã®å·¦åŠåˆ† (32x64 patches)
x_ctx = x[:, 1:32, :, :]

# Target: ç”»åƒã®å³åŠåˆ† (32x64 patches) â€” Encoderé€šã™ãŒå‹¾é…ã¯æµã•ãªã„
x_tgt = x[:, 33:64, :, :]

# Context encoder (trainable)
z_ctx = s_Î¸(x_ctx)  # [B, 32, D]

# Predictor: mask tokens M ã‚’ä½¿ã£ã¦ targetä½ç½®ã®è¡¨ç¾ã‚’äºˆæ¸¬
M = mask_tokens[:, 33:64, :]  # [B, 32, D_mask]
z_pred = f_Î¸(z_ctx, M)  # [B, 32, D]

# Target encoder (EMA, no grad)
z_tgt = stopgradient(sÌ„_Î¸(x_tgt))  # [B, 32, D]

# Loss
loss = mean((z_pred - z_tgt).^2)
```

#### 3.2.2 V-JEPA (Video JEPA)

**è«–æ–‡**: Bardes et al., "Revisiting Feature Prediction for Learning Visual Representations from Video," arXiv:2404.08471, 2024 (V-JEPA 1.0)
**æœ€æ–°**: "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning," arXiv:2506.09985, 2025

**æ‹¡å¼µ**: ç”»åƒâ†’å‹•ç”»ï¼ˆæ™‚ç©ºé–“äºˆæ¸¬ï¼‰

$$
\begin{aligned}
\text{Context: } & \mathbf{x}_{\text{ctx}} \in \mathbb{R}^{T_c \times H \times W \times C} \\
\text{Target: } & \mathbf{x}_{\text{tgt}} \in \mathbb{R}^{T_t \times H \times W \times C}
\end{aligned}
$$

**Spatio-temporal masking**:

- **Temporal masking**: å‰åŠ8ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆcontextï¼‰â†’å¾ŒåŠ8ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆtargetï¼‰ã‚’äºˆæ¸¬
- **Spatial masking**: å„ãƒ•ãƒ¬ãƒ¼ãƒ å†…ã§ãƒ‘ãƒƒãƒã‚’ãƒã‚¹ã‚¯

**è¨“ç·´ç›®æ¨™**:

$$
\mathcal{L}_{\text{V-JEPA}} = \mathbb{E}_{\mathbf{x}, M_s, M_t} \left[ \| f_\theta(s_\theta(\mathbf{x}_{\text{ctx}}), M_s, M_t) - \bar{s}_\theta(\mathbf{x}_{\text{tgt}}) \|_2^2 \right]
$$

**æ€§èƒ½**:

- Kinetics-400 (action recognition): **81.9%** Top-1 accuracy (video pre-trainingã®ã¿)
- Something-Something v2: **72.2%**
- ImageNet: **77.9%** Top-1 (å‹•ç”»äº‹å‰å­¦ç¿’ã‹ã‚‰ç”»åƒã‚¿ã‚¹ã‚¯ã«è»¢ç§»)

#### 3.2.3 VL-JEPA (Vision-Language JEPA)

**è«–æ–‡**: Bardes et al., "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language," arXiv:2512.10942, 2024

**å‹•æ©Ÿ**: å¾“æ¥ã®VLMï¼ˆVision-Language Modelsï¼‰ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§autoregressiveã«ç”Ÿæˆã™ã‚‹ã€‚ã“ã‚Œã¯è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ãã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚‚è†¨å¤§ï¼ˆdecoderå±¤ãŒå¿…è¦ï¼‰ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

VL-JEPAã¯**ãƒ†ã‚­ã‚¹ãƒˆã®é€£ç¶šåŸ‹ã‚è¾¼ã¿ã‚’äºˆæ¸¬**ã—ã€token-by-tokenç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚

$$
\begin{aligned}
\text{Image encoder: } & z_v = \text{Enc}_v(x) \\
\text{Predictor: } & z_{\text{pred}} = f_\theta(z_v, \text{prompt}) \\
\text{Text encoder: } & z_t = \text{Enc}_t(\text{target text}) \\
\text{Loss: } & \mathcal{L} = \| z_{\text{pred}} - z_t \|_2^2
\end{aligned}
$$

**åˆ©ç‚¹**:

- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒæ¨™æº–VLMã®**50%å‰Šæ¸›**ï¼ˆdecoderãªã—ï¼‰
- **ã‚ˆã‚Šå¼·ã„æ€§èƒ½**: åŒã˜vision encoderã¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã—ãŸæ¨™æº–VLMã‚’ä¸Šå›ã‚‹

### 3.3 Transfusionç†è«–: AR + Diffusionçµ±ä¸€

**è«–æ–‡**: Zhou et al., "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model," arXiv:2408.11039, 2024 (Meta AI)

**å‹•æ©Ÿ**: ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã¨ç”»åƒï¼ˆé€£ç¶šãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’**å˜ä¸€Transformerã§çµ±ä¸€å‡¦ç†**ã—ãŸã„ã€‚

**å¾“æ¥æ‰‹æ³•ã®å•é¡Œ**:

- ç”»åƒã‚’VQ-VAEã§é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³åŒ– â†’ é‡å­åŒ–èª¤å·®ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯åˆ©ç”¨ç‡ä½ä¸‹
- åˆ¥ã€…ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆLM + Diffusionï¼‰â†’ çµ±åˆã§ããªã„

**Transfusionã®è§£æ±ºç­–**:

**åŒä¸€Transformerã§ç•°ãªã‚‹æå¤±é–¢æ•°**ã‚’ä½¿ã„åˆ†ã‘ã‚‹ã€‚

$$
\mathcal{L}_{\text{Transfusion}} = \mathcal{L}_{\text{LM}}(\text{text}) + \lambda \mathcal{L}_{\text{Diffusion}}(\text{image})
$$

#### 3.3.1 ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†: Autoregressive

ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ $\mathbf{t} = (t_1, t_2, \ldots, t_n)$ ã«å¯¾ã—ã¦:

$$
\mathcal{L}_{\text{LM}} = -\sum_{i=1}^n \log p_\theta(t_i | t_{<i})
$$

é€šå¸¸ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¨åŒã˜causal maskingã¨cross-entropy lossã€‚

#### 3.3.2 ç”»åƒéƒ¨åˆ†: Diffusion

ç”»åƒãƒ‘ãƒƒãƒ $\mathbf{x} = (x_1, \ldots, x_m) \in \mathbb{R}^{m \times d}$ ã«å¯¾ã—ã¦:

$$
\begin{aligned}
\text{Forward: } & x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I) \\
\text{Diffusion Loss: } & \mathcal{L}_{\text{Diffusion}} = \mathbb{E}_{t, \epsilon} \left[ \| \epsilon - \epsilon_\theta(\mathbf{x}_t, t, \mathbf{c}) \|_2^2 \right]
\end{aligned}
$$

ã“ã“ã§ $\mathbf{c}$ ã¯ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ï¼ˆcross-attentionçµŒç”±ã§Transformerã«æ³¨å…¥ï¼‰ã€‚

#### 3.3.3 çµ±åˆå‡¦ç†ã®æ•°å­¦

å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹:

$$
\text{seq} = [\text{text tokens } t_1, \ldots, t_n, \text{ image patches } x_1, \ldots, x_m]
$$

**Attention mask**:

- ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†: **causal mask**ï¼ˆæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„ï¼‰
- ç”»åƒéƒ¨åˆ†: **bidirectional mask**ï¼ˆå…¨ãƒ‘ãƒƒãƒã‚’è¦‹ã‚‹ï¼‰

**æå¤±è¨ˆç®—**:

```julia
# ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ (discrete)
text_logits = model(text_tokens)
loss_text = cross_entropy(text_logits, text_tokens[2:end])

# ç”»åƒãƒ‘ãƒƒãƒ (continuous)
t_diffusion = rand(1:T)
noise = randn(size(image_patches))
x_t = sqrt(á¾±[t_diffusion]) * image_patches + sqrt(1 - á¾±[t_diffusion]) * noise
pred_noise = model(x_t, t_diffusion, context=text_tokens)
loss_image = mean((pred_noise - noise).^2)

# çµ±åˆ
loss = loss_text + Î» * loss_image
```

**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°çµæœ**: 7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€2T multi-modal tokensã§è¨“ç·´ â†’ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨ç”»åƒç”Ÿæˆã®ä¸¡æ–¹ã§åŒè¦æ¨¡ã®å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰æ€§èƒ½ã€‚

### 3.4 ç‰©ç†æ³•å‰‡å­¦ç¿’ç†è«–

#### 3.4.1 Physics-Informed World Models

**å‹•æ©Ÿ**: æ¨™æº–çš„ãªWorld Modelsã¯ç‰©ç†æ³•å‰‡ï¼ˆä¿å­˜å‰‡ã€å¯¾ç§°æ€§ã€å¾®åˆ†æ–¹ç¨‹å¼ï¼‰ã‚’ç„¡è¦–ã—ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•ã§å­¦ç¿’ã™ã‚‹ã€‚ã“ã‚Œã¯:

- ç‰©ç†çš„ã«ä¸å¯èƒ½ãªäºˆæ¸¬ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜å‰‡é•åãªã©ï¼‰
- ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã®æ‚ªã•ï¼ˆç‰©ç†æ³•å‰‡ã‚’çŸ¥ã£ã¦ã„ã‚Œã°å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’å¯èƒ½ï¼‰

**Physics-Informed Neural Networks (PINNs)ã®åŸç†**:

å¾®åˆ†æ–¹ç¨‹å¼åˆ¶ç´„ã‚’æå¤±é–¢æ•°ã«åŸ‹ã‚è¾¼ã‚€ã€‚

ä¾‹: Navier-Stokesæ–¹ç¨‹å¼

$$
\frac{\partial \mathbf{u}}{\partial t} + (\mathbf{u} \cdot \nabla)\mathbf{u} = -\frac{1}{\rho}\nabla p + \nu \nabla^2 \mathbf{u}
$$

**PINNs loss**:

$$
\mathcal{L}_{\text{PINN}} = \mathcal{L}_{\text{data}} + \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}}
$$

$$
\mathcal{L}_{\text{PDE}} = \mathbb{E}_{x,t} \left[ \left\| \frac{\partial \mathbf{u}_\theta}{\partial t} + (\mathbf{u}_\theta \cdot \nabla)\mathbf{u}_\theta + \frac{1}{\rho}\nabla p_\theta - \nu \nabla^2 \mathbf{u}_\theta \right\|_2^2 \right]
$$

**World Modelsã¸ã®é©ç”¨**:

$$
\mathcal{L}_{\text{Physics-WM}} = \mathcal{L}_{\text{prediction}} + \lambda_{\text{conservation}} \mathcal{L}_{\text{conservation}}
$$

$$
\mathcal{L}_{\text{conservation}} = \mathbb{E} \left[ \| E(z_{t+1}) - E(z_t) \|_2^2 \right]
$$

ã“ã“ã§$E(z)$ã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ï¼ˆå­¦ç¿’ã¾ãŸã¯æ—¢çŸ¥ï¼‰ã€‚

#### 3.4.2 ä¿å­˜å‰‡ã®åŸ‹ã‚è¾¼ã¿

**é‹å‹•é‡ä¿å­˜**:

$$
\sum_{i=1}^N m_i \mathbf{v}_i(t) = \text{const}
$$

**Graph Neural Networkã§ã®å®Ÿè£…**:

ãƒãƒ¼ãƒ‰$i$ã®é€Ÿåº¦$\mathbf{v}_i$ã«å¯¾ã—ã¦ã€edge $(i,j)$ã®æ›´æ–°:

$$
\Delta \mathbf{v}_i = \sum_{j \in \mathcal{N}(i)} \text{MLP}(\mathbf{h}_i, \mathbf{h}_j, \mathbf{r}_{ij})
$$

**ä¿å­˜å‰‡åˆ¶ç´„**: å„edgeæ›´æ–°ãŒé‹å‹•é‡ä¿å­˜ã‚’æº€ãŸã™ã‚ˆã†ã«ã€**Newton's third law**ã‚’æ˜ç¤ºçš„ã«é©ç”¨:

$$
m_i \Delta \mathbf{v}_i = -m_j \Delta \mathbf{v}_j
$$

**å®Ÿè£…**:

```julia
# Edge-local reference frameã§ã®forceè¨ˆç®—
f_ij = MLP(h_i, h_j, r_ij)

# Newton's third lawã§å¯¾ç§°åŒ–
Î”v_i = f_ij / m_i
Î”v_j = -f_ij / m_j

# æ›´æ–°
v_i_new = v_i + Î”v_i
v_j_new = v_j + Î”v_j

# æ¤œè¨¼: ç³»å…¨ä½“ã®é‹å‹•é‡ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹
@assert sum(m_i * v_i_new for i in 1:N) â‰ˆ sum(m_i * v_i for i in 1:N)
```

#### 3.4.3 Hamiltonian Neural Networks

**HamiltonianåŠ›å­¦ç³»**:

$$
\begin{aligned}
\dot{q} &= \frac{\partial H}{\partial p} \\
\dot{p} &= -\frac{\partial H}{\partial q}
\end{aligned}
$$

ã“ã“ã§$H(q, p)$ã¯Hamiltonianï¼ˆç·ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰ã€‚

**HNNã®å­¦ç¿’**:

1. NNã§$H_\theta(q, p)$ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–
2. è‡ªå‹•å¾®åˆ†ã§$\partial H / \partial p, \partial H / \partial q$ã‚’è¨ˆç®—
3. æå¤±:

$$
\mathcal{L}_{\text{HNN}} = \mathbb{E} \left[ \left\| \left(\dot{q}, \dot{p}\right) - \left(\frac{\partial H_\theta}{\partial p}, -\frac{\partial H_\theta}{\partial q}\right) \right\|_2^2 \right]
$$

**åˆ©ç‚¹**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜å‰‡ãŒ**æ§‹é€ çš„ã«ä¿è¨¼**ã•ã‚Œã‚‹ï¼ˆHamiltonianã®æ™‚é–“å¾®åˆ†ãŒ0ï¼‰ã€‚

### 3.5 Energy-based World Models

**ç¬¬34å›EBMã¨ã®æ¥ç¶š**:

World Modelsã‚’**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°$E_\theta(z_t, a_t, z_{t+1})$**ã¨ã—ã¦å®šå¼åŒ–:

$$
p(z_{t+1} | z_t, a_t) = \frac{\exp(-E_\theta(z_t, a_t, z_{t+1}))}{Z(z_t, a_t)}
$$

**åˆ©ç‚¹**:

- ä»»æ„ã®åˆ†å¸ƒå½¢çŠ¶ã‚’è¡¨ç¾å¯èƒ½ï¼ˆGaussianã«åˆ¶ç´„ã•ã‚Œãªã„ï¼‰
- ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– = æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„æœªæ¥çŠ¶æ…‹

**è¨“ç·´**: Contrastive Divergence (CD-k) ã¾ãŸã¯ Score Matching

$$
\nabla_\theta \mathcal{L} = \mathbb{E}_{z_t, a_t, z_{t+1}^{+}} [\nabla_\theta E_\theta(z_t, a_t, z_{t+1}^{+})] - \mathbb{E}_{z_t, a_t, z_{t+1}^{-}} [\nabla_\theta E_\theta(z_t, a_t, z_{t+1}^{-})]
$$

ã“ã“ã§$z_{t+1}^{+}$ã¯ãƒ‡ãƒ¼ã‚¿ã€$z_{t+1}^{-}$ã¯Langevin dynamicsã§ã‚µãƒ³ãƒ—ãƒ«ã€‚

### 3.6 ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ç†è«–

#### 3.6.1 Action-conditionedäºˆæ¸¬

**ç›®çš„**: action $a_t$ã‚’ä¸ãˆã¦æ¬¡çŠ¶æ…‹$z_{t+1}$ã‚’äºˆæ¸¬

$$
z_{t+1} = f_\theta(z_t, a_t) + \epsilon_t
$$

**è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ãƒ­ãƒœãƒƒãƒˆè»Œè·¡ $(z_t, a_t, z_{t+1})$

**æå¤±**:

$$
\mathcal{L}_{\text{pred}} = \mathbb{E} \left[ \| f_\theta(z_t, a_t) - z_{t+1} \|_2^2 \right]
$$

**Stochastic dynamics**ã®å ´åˆ:

$$
p_\theta(z_{t+1} | z_t, a_t) = \mathcal{N}(f_\theta(z_t, a_t), \Sigma_\theta(z_t, a_t))
$$

$$
\mathcal{L}_{\text{NLL}} = -\mathbb{E} \left[ \log p_\theta(z_{t+1} | z_t, a_t) \right]
$$

#### 3.6.2 Reward Prediction

World Modelã‚’å¼·åŒ–å­¦ç¿’ã«çµ±åˆã™ã‚‹å ´åˆã€å ±é…¬é–¢æ•°$r_t$ã‚‚äºˆæ¸¬:

$$
r_t = g_\phi(z_t, a_t)
$$

**è¨“ç·´**:

$$
\mathcal{L}_{\text{reward}} = \mathbb{E} \left[ (g_\phi(z_t, a_t) - r_t)^2 \right]
$$

**Model-based RL**:

1. World Modelã§æœªæ¥ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ: $z_{t+1} = f_\theta(z_t, a_t)$
2. å ±é…¬ã‚’æ¨å®š: $\hat{r}_t = g_\phi(z_t, a_t)$
3. Policy $\pi_\psi(a|z)$ã‚’æœ€é©åŒ–:

$$
\mathcal{L}_{\text{policy}} = -\mathbb{E}_{\pi} \left[ \sum_{t=0}^H \gamma^t g_\phi(z_t, a_t) \right]
$$

#### 3.6.3 è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’

**Contrastive Learning**: æ­£ä¾‹ï¼ˆåŒä¸€å‹•ç”»ã®è¿‘æ¥ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰vs è² ä¾‹ï¼ˆç•°ãªã‚‹å‹•ç”»ï¼‰

$$
\mathcal{L}_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(z_t, z_{t+k}) / \tau)}{\sum_{j} \exp(\text{sim}(z_t, z_j^{-}) / \tau)}
$$

**Masked Autoencoding**: ä¸€éƒ¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒã‚¹ã‚¯ â†’ äºˆæ¸¬ï¼ˆJEPAã¨åŒã˜åŸç†ï¼‰

### 3.7 ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

#### 3.7.1 äºˆæ¸¬ç²¾åº¦

**Mean Squared Error (MSE)**:

$$
\text{MSE} = \frac{1}{N} \sum_{i=1}^N \| z_{t+1}^{(i)} - \hat{z}_{t+1}^{(i)} \|_2^2
$$

**Structural Similarity (SSIM)** (ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã§è©•ä¾¡ã™ã‚‹å ´åˆ):

$$
\text{SSIM}(x, \hat{x}) = \frac{(2\mu_x \mu_{\hat{x}} + C_1)(2\sigma_{x\hat{x}} + C_2)}{(\mu_x^2 + \mu_{\hat{x}}^2 + C_1)(\sigma_x^2 + \sigma_{\hat{x}}^2 + C_2)}
$$

#### 3.7.2 ç‰©ç†æ³•å‰‡éµå®ˆã‚¹ã‚³ã‚¢

**ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜èª¤å·®**:

$$
\text{Energy Error} = \frac{1}{T} \sum_{t=1}^T | E(z_t) - E(z_0) |
$$

**é‹å‹•é‡ä¿å­˜èª¤å·®**:

$$
\text{Momentum Error} = \frac{1}{T} \sum_{t=1}^T \left\| \sum_i m_i \mathbf{v}_i(t) - \sum_i m_i \mathbf{v}_i(0) \right\|_2
$$

#### 3.7.3 é•·æœŸä¸€è²«æ€§

**Frame Prediction Horizon**: ãƒ¢ãƒ‡ãƒ«ãŒä½•ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§æ­£ç¢ºã«äºˆæ¸¬ã§ãã‚‹ã‹

$$
T_{\text{horizon}} = \max\{t : \text{MSE}(t) < \epsilon\}
$$

**Video Quality Metrics**:

- **FVD (FrÃ©chet Video Distance)**: I3Dç‰¹å¾´é‡ã§ã®FrÃ©chetè·é›¢
- **LPIPS**: çŸ¥è¦šçš„é¡ä¼¼åº¦

### ğŸ¥Š Boss Battle: Transfusionã®å®Œå…¨åˆ†è§£

**èª²é¡Œ**: arXiv:2408.11039ã®Transfusionã®çµ±ä¸€æå¤±é–¢æ•°ã‚’ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¤ãƒ¡ãƒ¼ã‚¸ã®æ··åˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦å®Œå…¨å°å‡ºã›ã‚ˆã€‚

**Step 1**: å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹

ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ† $\mathbf{t} = (t_1, \ldots, t_n)$ ï¼ˆdiscrete tokensï¼‰
ç”»åƒéƒ¨åˆ† $\mathbf{x} = (x_1, \ldots, x_m)$ ï¼ˆcontinuous patch embeddingsï¼‰

çµ±åˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹:

$$
\mathbf{s} = [\mathbf{t}, \mathbf{x}] \in \mathbb{R}^{(n+m) \times d}
$$

**Step 2**: Attention Mask

$$
M_{\text{Transfusion}} = \begin{bmatrix}
M_{\text{causal}} & 0 \\
M_{\text{bi-dir}} & M_{\text{bi-dir}}
\end{bmatrix}
$$

- å·¦ä¸Š: ãƒ†ã‚­ã‚¹ãƒˆã®causal maskï¼ˆè‡ªå·±å›å¸°ï¼‰
- å³ä¸‹: ç”»åƒã®bidirectional maskï¼ˆå…¨ãƒ‘ãƒƒãƒç›¸äº’å‚ç…§ï¼‰
- å·¦ä¸‹: ç”»åƒãŒãƒ†ã‚­ã‚¹ãƒˆã‚’è¦‹ã‚‹ï¼ˆcross-modal attentionï¼‰
- å³ä¸Š: 0ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯ç”»åƒã‚’è¦‹ãªã„ï¼‰

**Step 3**: Loss Functions

**ãƒ†ã‚­ã‚¹ãƒˆloss** (next token prediction):

$$
\mathcal{L}_{\text{text}} = -\frac{1}{n} \sum_{i=1}^n \log p_\theta(t_i | t_{<i})
$$

Softmaxã§ç¢ºç‡åŒ–:

$$
p_\theta(t_i | t_{<i}) = \frac{\exp(z_{t_i}^\top e_{t_i})}{\sum_{j=1}^{|V|} \exp(z_{t_i}^\top e_j)}
$$

ã“ã“ã§$z_{t_i}$ã¯Transformerã®$i$ç•ªç›®å‡ºåŠ›ã€$e_j$ã¯token embeddingã®$j$ç•ªç›®ã€‚

**ç”»åƒloss** (diffusion):

$$
\mathcal{L}_{\text{image}} = \mathbb{E}_{t \sim [1,T], \epsilon \sim \mathcal{N}(0,I)} \left[ \| \epsilon - \epsilon_\theta(\mathbf{x}_t, t, \mathbf{c}) \|_2^2 \right]
$$

ã“ã“ã§:

$$
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon
$$

$\mathbf{c}$ã¯ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ï¼ˆcross-attentionã§æ³¨å…¥ï¼‰ã€‚

**Step 4**: çµ±åˆæå¤±

$$
\mathcal{L}_{\text{Transfusion}} = \mathcal{L}_{\text{text}} + \lambda \mathcal{L}_{\text{image}}
$$

$\lambda$ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè«–æ–‡ã§ã¯$\lambda=1$ã‚’ä½¿ç”¨ï¼‰ã€‚

**Step 5**: å®Ÿè£…ã‚³ãƒ¼ãƒ‰ï¼ˆJuliaï¼‰

```julia
using Lux, Random, Optimisers

# Transformer with mixed masking
struct TransfusionTransformer
    layers::Chain
    text_head::Dense
    image_head::Dense
end

function (m::TransfusionTransformer)(s, mask, t_diffusion=nothing)
    h = m.layers(s, mask)  # [B, n+m, D]

    # Split into text and image parts
    h_text = h[:, 1:n_text, :]
    h_image = h[:, n_text+1:end, :]

    # Text head: logits
    logits_text = m.text_head(h_text)  # [B, n_text, vocab_size]

    # Image head: noise prediction
    if !isnothing(t_diffusion)
        # Embed diffusion time step
        t_emb = sinusoidal_embedding(t_diffusion)
        pred_noise = m.image_head(cat(h_image, t_emb, dims=3))
    else
        pred_noise = nothing
    end

    return logits_text, pred_noise
end

# Loss function
function transfusion_loss(model, text_tokens, image_patches, ps, st)
    n_text = size(text_tokens, 2)

    # Forward diffusion on images
    t = rand(1:1000)
    noise = randn(size(image_patches))
    á¾±_t = alpha_bar(t)
    x_t = sqrt(á¾±_t) * image_patches + sqrt(1 - á¾±_t) * noise

    # Construct input sequence [text, image]
    s = cat(embed(text_tokens), x_t, dims=2)  # [B, n_text + m_image, D]

    # Construct mask
    mask = create_transfusion_mask(n_text, size(x_t, 2))

    # Forward
    logits_text, pred_noise = model(s, mask, t, ps, st)

    # Text loss (cross-entropy)
    loss_text = cross_entropy(logits_text, text_tokens[:, 2:end])

    # Image loss (diffusion)
    loss_image = mean((pred_noise - noise).^2)

    # Total loss
    return loss_text + Î» * loss_image
end
```

**Bossæ’ƒç ´ï¼** Transfusionã®çµ±ä¸€æå¤±é–¢æ•°ã‚’å®Œå…¨ã«å°å‡ºã—ã€å®Ÿè£…ã—ãŸã€‚

:::message alert
**ã“ã“ã§èº“ãäººãŒå¤šã„**: Transfusionã®Attention maskã¯**æ··åˆå‹**ã§ã‚ã‚‹ã€‚ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã¯causalã€ç”»åƒéƒ¨åˆ†ã¯bidirectionalã€ãã—ã¦ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«éƒ¨åˆ†ã¯**ç”»åƒâ†’ãƒ†ã‚­ã‚¹ãƒˆ**ã®ã¿è¨±å¯ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯ç”»åƒã‚’è¦‹ãªã„ï¼‰ã€‚ã“ã‚Œã‚’æ­£ã—ãå®Ÿè£…ã—ãªã„ã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã§ãƒªãƒ¼ã‚¯ãŒèµ·ãã‚‹ã€‚
:::

:::message
**é€²æ—**: å…¨ä½“ã®50%å®Œäº†ã€‚World Modelsã®æ•°å­¦çš„åŸºç¤ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚JEPAï¼ˆI/V/VLï¼‰ã®3å¤‰ç¨®ã€Transfusionã®çµ±ä¸€ç†è«–ã€ç‰©ç†æ³•å‰‡å­¦ç¿’ã€EBMè¦–ç‚¹ã€è¨“ç·´ãƒ»è©•ä¾¡æ‰‹æ³•ã‚’å°å‡ºã—ãŸã€‚æ•°å¼ä¿®è¡Œãƒœã‚¹æˆ¦ã‚’ã‚¯ãƒªã‚¢ã€‚
:::

---

