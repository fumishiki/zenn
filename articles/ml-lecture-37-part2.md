---
title: "ç¬¬37å›: ğŸ² SDE/ODE & ç¢ºç‡éç¨‹è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ²"
type: "tech"
topics: ["machinelearning", "deeplearning", "sde", "rust", "stochasticprocesses"]
published: true
slug: "ml-lecture-37-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rust ode_solversã§SDEæ•°å€¤è§£æ³•

### 4.1 Rust ode_solverså…¥é–€ â€” SDEProblemã®å®šç¾©

Rustã®ode_solversã¯SDE/ODE/DAEã‚’çµ±ä¸€çš„ã«æ‰±ã†å¼·åŠ›ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã€‚

**åŸºæœ¬çš„ãªSDEå®šç¾©**:

```rust
// use rand_distr; // rand, rand_distr ã‚¯ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨

// SDE Model trait: defines drift f and diffusion g
// trait SdeModel {
//     fn drift(&self, x: f64, t: f64) -> f64;     // f(x,t)
//     fn diffusion(&self, t: f64) -> f64;           // g(t)
//     fn score(&self, x: f64, t: f64) -> f64;      // âˆ‡log p_t(x)
// }

use rand::Rng;
use rand_distr::StandardNormal;

// Forward SDE: dx = f(x,t)dt + g(t)dW  (ItÃ´)
// drift: f(x, t) = -0.5 * Î² * x
fn drift(x: f64, beta: f64) -> f64 { -0.5 * beta * x } // f(x,t) = -Â½Î²(t)Â·x

// diffusion: g(x, t) = âˆšÎ²
fn diffusion(beta: f64) -> f64 { beta.sqrt() } // g(t) = âˆšÎ²(t)

fn main() {
    let mut rng = rand::thread_rng();

    // åˆæœŸå€¤ã€æ™‚é–“ç¯„å›²ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    let mut x = 1.0_f64;
    let beta = 1.0_f64;
    let dt = 0.01_f64;
    let n_steps = (1.0 / dt) as usize;

    // Euler-Maruyama æ³•ã§ VP-SDE ã‚’è§£ã
    let mut trajectory = vec![x];
    for _ in 0..n_steps {
        let dw: f64 = rng.sample(StandardNormal);
        x += drift(x, beta) * dt + diffusion(beta) * dt.sqrt() * dw; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W
        trajectory.push(x);
    }

    println!("VP-SDE ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹: {} ã‚¹ãƒ†ãƒƒãƒ—", trajectory.len());
    println!("çµ‚ç«¯å€¤ X(1.0) = {:.4}", trajectory[trajectory.len() - 1]);
    // Plotting: use plotters crate for visualization
}
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:
- SDE: $dX_t = -\frac{1}{2}\beta X_t dt + \sqrt{\beta} dW_t$
- `drift(u, p, t)`: Drifté … $f(x, t) = -\frac{1}{2}\beta x$
- `diffusion(u, p, t)`: Diffusioné … $g(x, t) = \sqrt{\beta}$
- `EM()`: Euler-Maruyamaæ³•ï¼ˆ$\Delta t = 0.01$ï¼‰

### 4.2 VP-SDEå®Ÿè£… â€” ç·šå½¢/Cosineã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

DDPMå¯¾å¿œã®VP-SDEã‚’ç·šå½¢/Cosineã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å®Ÿè£…ã€‚

**ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**:
$$
\beta(t) = \beta_{\min} + t(\beta_{\max} - \beta_{\min})
$$

```rust
// VP-SDE with ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
use rand::Rng;
use rand_distr::StandardNormal;

fn beta_linear(t: f64, beta_min: f64, beta_max: f64) -> f64 { beta_min + t * (beta_max - beta_min) } // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)

// Drift: f(x, t) = -0.5 * Î²(t) * x
fn vp_drift_linear(x: f64, t: f64, beta_min: f64, beta_max: f64) -> f64 { -0.5 * beta_linear(t, beta_min, beta_max) * x } // f(x,t) = -Â½Î²(t)Â·x

// Diffusion: g(x, t) = âˆšÎ²(t)
fn vp_noise_linear(t: f64, beta_min: f64, beta_max: f64) -> f64 { beta_linear(t, beta_min, beta_max).sqrt() } // g(t) = âˆšÎ²(t)

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;

    // Euler-Maruyama ã§ VP-SDEï¼ˆç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰
    let mut x = 1.0_f64;
    let mut trajectory = vec![(0.0_f64, x)];
    for step in 0..n_steps {
        let t = step as f64 * dt;
        let dw: f64 = rng.sample(StandardNormal);
        x += vp_drift_linear(x, t, beta_min, beta_max) * dt
            + vp_noise_linear(t, beta_min, beta_max) * dt.sqrt() * dw; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W
        trajectory.push((t + dt, x));
    }

    println!("VP-SDE ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: {} ã‚¹ãƒ†ãƒƒãƒ—", trajectory.len());
    println!("çµ‚ç«¯å€¤ X(1.0) = {:.4}", trajectory.last().unwrap().1);
    // Plotting: use plotters crate â€” xlabel="t", ylabel="X(t)", title="VP-SDE ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"
}
```

**Cosineã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**ï¼ˆDDPM Improved, Nichol & Dhariwal 2021ï¼‰:
$$
\bar{\alpha}_t = \frac{\cos\left(\frac{t + s}{1 + s} \cdot \frac{\pi}{2}\right)^2}{\cos\left(\frac{s}{1 + s} \cdot \frac{\pi}{2}\right)^2}, \quad \beta(t) = -\frac{d \log \bar{\alpha}_t}{dt}
$$
ï¼ˆ$s = 0.008$ ã¯å°ã•ãªã‚ªãƒ•ã‚»ãƒƒãƒˆï¼‰

```rust
// Cosineã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
use rand::Rng;
use rand_distr::StandardNormal;
use std::f64::consts::PI;

fn alpha_bar_cosine(t: f64, s: f64) -> f64 {
    let num = ((t + s) / (1.0 + s) * PI / 2.0).cos().powi(2);
    let den = (s / (1.0 + s) * PI / 2.0).cos().powi(2);
    num / den
}

// Î²(t) = -d/dt log á¾±(t),  á¾±(t) = cosÂ²(Ï€t/(2+2s)) / cosÂ²(Ï€s/(2+2s))
fn beta_cosine(t: f64, s: f64) -> f64 {
    let h = 1e-6;
    -(alpha_bar_cosine(t + h, s).ln() - alpha_bar_cosine(t, s).ln()) / h // Î²(t) = -d/dt log á¾±(t)
}

fn vp_drift_cosine(x: f64, t: f64, s: f64) -> f64 { -0.5 * beta_cosine(t, s) * x } // f(x,t) = -Â½Î²(t)Â·x

fn vp_noise_cosine(t: f64, s: f64) -> f64 { beta_cosine(t, s).sqrt() } // g(t) = âˆšÎ²(t)

fn main() {
    let mut rng = rand::thread_rng();
    let s = 0.008_f64;
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;

    // ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    let mut x_linear = 1.0_f64;
    // Cosineã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    let mut x_cosine = 1.0_f64;

    let mut traj_linear = vec![(0.0_f64, x_linear)];
    let mut traj_cosine = vec![(0.0_f64, x_cosine)];

    for step in 0..n_steps {
        let t = step as f64 * dt;

        // ç·šå½¢
        let dw_l: f64 = rng.sample(StandardNormal);
        let b_l = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
        x_linear += -0.5 * b_l * x_linear * dt + b_l.sqrt() * dt.sqrt() * dw_l; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W
        traj_linear.push((t + dt, x_linear));

        // Cosine
        let dw_c: f64 = rng.sample(StandardNormal);
        x_cosine += vp_drift_cosine(x_cosine, t, s) * dt
            + vp_noise_cosine(t, s) * dt.sqrt() * dw_c; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W
        traj_cosine.push((t + dt, x_cosine));
    }

    println!("VP-SDE ç·šå½¢ çµ‚ç«¯å€¤: {:.4}", traj_linear.last().unwrap().1);
    println!("VP-SDE Cosine çµ‚ç«¯å€¤: {:.4}", traj_cosine.last().unwrap().1);
    // Plotting: use plotters crate â€” title="VP-SDE: ç·šå½¢ vs Cosine"
}
```

**ç·šå½¢ vs Cosine ã®é•ã„**:
- ç·šå½¢: çµ‚ç«¯ã§ãƒã‚¤ã‚ºãŒæ€¥å¢—ï¼ˆ$\beta_{\max} = 20$ï¼‰
- Cosine: æ»‘ã‚‰ã‹ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ç«¯ç‚¹ã§ã®æ€¥å¤‰ã‚’å›é¿

### 4.3 VE-SDEå®Ÿè£… â€” å¹¾ä½•ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

NCSNã®VE-SDEã‚’å¹¾ä½•ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å®Ÿè£…ã€‚

**å¹¾ä½•ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**:
$$
\sigma(t) = \sigma_{\min} \left(\frac{\sigma_{\max}}{\sigma_{\min}}\right)^t
$$

$$
\frac{d\sigma^2(t)}{dt} = 2\sigma(t) \log\left(\frac{\sigma_{\max}}{\sigma_{\min}}\right) \sigma(t) = 2\sigma^2(t) \log\left(\frac{\sigma_{\max}}{\sigma_{\min}}\right)
$$

```rust
// VE-SDE with å¹¾ä½•ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
use rand::Rng;
use rand_distr::StandardNormal;

// Drifté … = 0ï¼ˆVE-SDEã¯å¹³å‡ã‚’å¤‰åŒ–ã•ã›ãªã„ï¼‰
fn ve_drift(_x: f64) -> f64 { 0.0 } // f(x,t) = 0  (VE-SDE has no drift)

// Diffusion: g(t) = âˆš(2 ÏƒÂ²(t) log(Ïƒ_max / Ïƒ_min))
fn ve_noise(t: f64, sigma_min: f64, sigma_max: f64) -> f64 {
    let sigma_t = sigma_min * (sigma_max / sigma_min).powf(t); // Ïƒ(t) = Ïƒ_minÂ·(Ïƒ_max/Ïƒ_min)^t
    (2.0 * sigma_t.powi(2) * (sigma_max / sigma_min).ln()).sqrt() // g(t) = âˆš(2ÏƒÂ²(t)Â·log(Ïƒ_max/Ïƒ_min))
}

fn main() {
    let mut rng = rand::thread_rng();
    let sigma_min = 0.01_f64;
    let sigma_max = 50.0_f64;
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;

    // Euler-Maruyama ã§ VE-SDE ã‚’è§£ã
    let mut x = 1.0_f64;
    let mut trajectory = vec![(0.0_f64, x)];
    for step in 0..n_steps {
        let t = step as f64 * dt;
        let dw: f64 = rng.sample(StandardNormal);
        x += ve_drift(x) * dt + ve_noise(t, sigma_min, sigma_max) * dt.sqrt() * dw; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W
        trajectory.push((t + dt, x));
    }

    println!("VE-SDE å¹¾ä½•ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: {} ã‚¹ãƒ†ãƒƒãƒ—", trajectory.len());
    println!("çµ‚ç«¯å€¤ X(1.0) = {:.4}", trajectory.last().unwrap().1);
    // Plotting: use plotters crate â€” xlabel="t", ylabel="X(t)", title="VE-SDE å¹¾ä½•ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"
}
```

**ç‰¹å¾´**:
- Drifté …ãªã—ï¼ˆå¹³å‡å¤‰åŒ–ãªã—ï¼‰
- åˆ†æ•£ãŒæ™‚é–“ã¨ã¨ã‚‚ã«çˆ†ç™ºçš„ã«å¢—åŠ 

### 4.4 Reverse-time SDEå®Ÿè£… â€” Scoreé–¢æ•°è¿‘ä¼¼

Reverse-time SDEã‚’ç°¡æ˜“Scoreé–¢æ•°è¿‘ä¼¼ã§å®Ÿè£…ã€‚

**VP-SDE Reverse-time**:
$$
dX_t = \left[-\frac{1}{2}\beta(t) X_t - \beta(t) \nabla \log p_t(X_t)\right] dt + \sqrt{\beta(t)} d\bar{W}_t
$$

**Scoreé–¢æ•°è¿‘ä¼¼**ï¼ˆã‚¬ã‚¦ã‚¹ä»®å®šï¼‰:
å­¦ç¿’æ¸ˆã¿Scoreé–¢æ•° $s_\theta(x, t)$ ãŒãªã„å ´åˆã€ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ã§ $\nabla \log p_t(x) \approx -x / \sigma_t^2$ã€‚

```rust
// Reverse-time VP-SDEï¼ˆç°¡æ˜“Scoreè¿‘ä¼¼ï¼‰
use rand::Rng;
use rand_distr::StandardNormal;

// Scoreè¿‘ä¼¼ï¼ˆå®Ÿéš›ã¯NNã§å­¦ç¿’ï¼‰
// ç°¡æ˜“çš„ã« âˆ‡log p_t(x) â‰ˆ -xï¼ˆã‚¬ã‚¦ã‚¹ä»®å®šï¼‰
fn score_approx(x: f64) -> f64 { -x } // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)

// Reverse-time Drift = -0.5 * Î²(t) * x - Î²(t) * âˆ‡log p_t(x)
fn reverse_vp_drift(x: f64, t: f64, beta_min: f64, beta_max: f64) -> f64 {
    let beta_t = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
    -0.5 * beta_t * x - beta_t * score_approx(x) // f_rev(x,t) = f - gÂ²Â·âˆ‡log p_t  (Anderson 1982)
}

fn reverse_vp_noise(t: f64, beta_min: f64, beta_max: f64) -> f64 { (beta_min + t * (beta_max - beta_min)).sqrt() } // g(t) = âˆšÎ²(t)

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;

    // åˆæœŸå€¤: ãƒã‚¤ã‚ºåˆ†å¸ƒ N(0, 1)
    let mut x: f64 = rng.sample(StandardNormal);
    let mut trajectory = vec![(1.0_f64, x)];

    // é€†æ™‚é–“ï¼ˆt: 1 â†’ 0ï¼‰: è² ã®dt
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let dw: f64 = rng.sample(StandardNormal);
        // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
        x += reverse_vp_drift(x, t, beta_min, beta_max) * (-dt)
            + reverse_vp_noise(t, beta_min, beta_max) * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
        trajectory.push((t - dt, x));
    }

    println!("Reverse-time VP-SDE: {} ã‚¹ãƒ†ãƒƒãƒ—", trajectory.len());
    println!("çµ‚ç«¯å€¤ X(0.0) = {:.4}", trajectory.last().unwrap().1);
    // Plotting: use plotters crate â€” title="Reverse-time VP-SDEï¼ˆç°¡æ˜“Scoreï¼‰"
}
```

**æ³¨æ„**:
- å®Ÿéš›ã®Diffusion Modelã§ã¯ Scoreé–¢æ•° $s_\theta(x, t)$ ã‚’Neural Networkã§å­¦ç¿’
- ã“ã“ã§ã¯ $\nabla \log p_t(x) \approx -x$ ã®ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ï¼ˆãƒ‡ãƒ¢ç›®çš„ï¼‰

### 4.5 Probability Flow ODEå®Ÿè£… â€” æ±ºå®šè«–çš„è»Œé“

Probability Flow ODEã‚’`ODEProblem`ã§å®Ÿè£…ã€‚

**VP-SDE Probability Flow ODE**:
$$
\frac{dX_t}{dt} = -\frac{1}{2}\beta(t) X_t - \frac{1}{2}\beta(t) \nabla \log p_t(X_t)
$$

```rust
// Probability Flow ODE for VP-SDE
use rand::Rng;
use rand_distr::StandardNormal;

// PF-ODE: dx/dt = f - Â½gÂ²Â·âˆ‡log p_t  (Song+ 2021)
// Scoreè¿‘ä¼¼ï¼ˆå®Ÿéš›ã¯NNã§å­¦ç¿’ï¼‰: âˆ‡log p_t(x) â‰ˆ -x
fn pf_ode_rhs(x: f64, t: f64, beta_min: f64, beta_max: f64) -> f64 {
    let beta_t = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
    let score_approx = -x; // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)
    -0.5 * beta_t * x - 0.5 * beta_t * score_approx // PF-ODE: dx/dt = f - Â½gÂ²Â·âˆ‡log p_t
}

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;

    // åˆæœŸãƒã‚¤ã‚ºï¼ˆt=1 ã‹ã‚‰ t=0 ã¸é€†æ™‚é–“ï¼‰
    let mut x: f64 = rng.sample(StandardNormal);
    let mut trajectory = vec![(1.0_f64, x)];

    // Euleræ³•ã§ PF-ODE ã‚’é€†æ™‚é–“ã«è§£ã
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        x += pf_ode_rhs(x, t, beta_min, beta_max) * (-dt); // PF-ODE: dx/dt = f - Â½gÂ²Â·âˆ‡log p_t  (Song+ 2021)
        trajectory.push((t - dt, x));
    }

    println!("Probability Flow ODE: {} ã‚¹ãƒ†ãƒƒãƒ—", trajectory.len());
    println!("çµ‚ç«¯å€¤ X(0.0) = {:.4}", trajectory.last().unwrap().1);
    // Plotting: use plotters crate â€” title="Probability Flow ODE"
}
```

**Reverse-time SDE vs PF-ODE**:
```rust
// åŒã˜åˆæœŸå€¤ã§æ¯”è¼ƒ
use rand::Rng;
use rand_distr::StandardNormal;

// Reverse-time SDE: Euler-Maruyamaï¼ˆé€†æ™‚é–“ï¼‰
fn run_reverse_sde(x0: f64, beta_min: f64, beta_max: f64, dt: f64, n_steps: usize, rng: &mut impl Rng) -> Vec<f64> {
    let mut x = x0;
    let mut traj = vec![x];
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let beta_t = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
        let score = -x; // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)
        let dw: f64 = rng.sample(StandardNormal);
        x += (-0.5 * beta_t * x - beta_t * score) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
        traj.push(x);
    }
    traj
}

// PF-ODE: Euleræ³•ï¼ˆé€†æ™‚é–“ï¼‰
fn run_pf_ode(x0: f64, beta_min: f64, beta_max: f64, dt: f64, n_steps: usize) -> Vec<f64> {
    let mut x = x0;
    let mut traj = vec![x];
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let beta_t = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
        let score = -x; // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)
        x += (-0.5 * beta_t * x - 0.5 * beta_t * score) * (-dt); // PF-ODE: dx/dt = f - Â½gÂ²Â·âˆ‡log p_t  (Song+ 2021)
        traj.push(x);
    }
    traj
}

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let x0 = 0.5_f64; // å…±é€šã®åˆæœŸå€¤
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;

    // Reverse-time SDE
    let traj_sde = run_reverse_sde(x0, beta_min, beta_max, dt, n_steps, &mut rng);

    // PF-ODE
    let traj_ode = run_pf_ode(x0, beta_min, beta_max, dt, n_steps);

    println!("SDE çµ‚ç«¯å€¤: {:.4}", traj_sde.last().unwrap());
    println!("ODE çµ‚ç«¯å€¤: {:.4}", traj_ode.last().unwrap());
    // Plotting: use plotters crate â€” title="SDE vs ODE"
}
```

**çµæœ**:
- Reverse-time SDE: ç¢ºç‡çš„ï¼ˆè»Œé“ãŒæºã‚Œã‚‹ï¼‰
- PF-ODE: æ±ºå®šè«–çš„ï¼ˆæ»‘ã‚‰ã‹ãªè»Œé“ï¼‰

### 4.6 Predictor-Correctoræ³•å®Ÿè£… â€” ç²¾åº¦å‘ä¸Š

Predictor-Correctoræ³•ã§é«˜å“è³ªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
1. Predictor: Reverse-time SDEã§1ã‚¹ãƒ†ãƒƒãƒ—
2. Corrector: Langevin Dynamicsï¼ˆè¤‡æ•°å›åå¾©ï¼‰

```rust
// Predictor-Corrector ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
use rand::Rng;
use rand_distr::StandardNormal;

fn predictor_corrector_sampling(
    n_steps: usize,
    n_corrector: usize,
    eps_langevin: f64,
    beta_min: f64,
    beta_max: f64,
    rng: &mut impl Rng,
) -> Vec<f64> {
    let mut x: f64 = rng.sample(StandardNormal);
    let dt = 1.0 / n_steps as f64; // é€†æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—å¹…ï¼ˆæ­£ï¼‰

    let mut trajectory = vec![x];

    for step in 0..n_steps {
        let t = 1.0 - step as f64 / n_steps as f64;
        let beta_t = beta_min + t * (beta_max - beta_min);

        // Predictor (reverse SDE): xâ‚œâ‚‹â‚ = xâ‚œ + f_revÂ·dt + gÂ·âˆšdtÂ·Î”W
        let score = -x; // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)
        let dw: f64 = rng.sample(StandardNormal);
        x += (-0.5 * beta_t * x - beta_t * score) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // PC predictor: xâ‚œâ‚‹â‚ = xâ‚œ + f_revÂ·dt + gÂ·âˆšdtÂ·Î”W

        // Corrector (Langevin): x â† x + ÎµÂ·s + âˆš(2Îµ)Â·Î”W
        for _ in 0..n_corrector {
            let score_c = -x; // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)
            let dw_c: f64 = rng.sample(StandardNormal);
            x += eps_langevin * score_c + (2.0 * eps_langevin).sqrt() * dw_c; // Langevin: x â† x + ÎµÂ·âˆ‡log p + âˆš(2Îµ)Â·Î”W
        }

        trajectory.push(x);
    }

    trajectory // n_steps+1 è¦ç´ ã®ãƒ™ã‚¯ãƒˆãƒ«
}

fn main() {
    let mut rng = rand::thread_rng();
    // ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ
    let traj = predictor_corrector_sampling(100, 5, 0.01, 0.1, 20.0, &mut rng);

    println!("Predictor-Corrector: {} ã‚¹ãƒ†ãƒƒãƒ—", traj.len());
    // t_plot: 1.0 â†’ 0.0 (101ç‚¹)
    let t_plot: Vec<f64> = (0..=100).map(|i| 1.0 - i as f64 / 100.0).collect();
    for (t, x) in t_plot.iter().zip(traj.iter()).take(5) {
        println!("  t={:.2} x={:.4}", t, x);
    }
    // Plotting: use plotters crate â€” title="Predictor-Corrector ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"
}
```

**Predictor-Corrector vs Euler-Maruyama**:
```rust
// Euler-Maruyamaï¼ˆPredictor-onlyã¨ç­‰ä¾¡ï¼‰
use rand::Rng;
use rand_distr::StandardNormal;

// Euler-Maruyama ã§ãƒªãƒãƒ¼ã‚¹ VP-SDE ã‚’è§£ãï¼ˆscore â‰ˆ -xï¼‰
fn em_reverse_vp(x0: f64, beta_min: f64, beta_max: f64, dt: f64, n_steps: usize, rng: &mut impl Rng) -> Vec<f64> {
    let mut x = x0;
    let mut traj = vec![x];
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let beta_t = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
        let score = -x; // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)
        let dw: f64 = rng.sample(StandardNormal);
        x += (-0.5 * beta_t * x - beta_t * score) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
        traj.push(x);
    }
    traj
}

fn predictor_corrector_sampling(
    n_steps: usize, n_corrector: usize, eps: f64,
    beta_min: f64, beta_max: f64, rng: &mut impl Rng,
) -> Vec<f64> {
    let mut x: f64 = rng.sample(StandardNormal);
    let dt = 1.0 / n_steps as f64;
    let mut traj = vec![x];
    for step in 0..n_steps {
        let t = 1.0 - step as f64 / n_steps as f64;
        let beta_t = beta_min + t * (beta_max - beta_min);
        let dw: f64 = rng.sample(StandardNormal);
        x += (-0.5 * beta_t * x - beta_t * (-x)) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // PC predictor: xâ‚œâ‚‹â‚ = xâ‚œ + f_revÂ·dt + gÂ·âˆšdtÂ·Î”W
        for _ in 0..n_corrector {
            let dw_c: f64 = rng.sample(StandardNormal);
            x += eps * (-x) + (2.0 * eps).sqrt() * dw_c; // Corrector (Langevin): x â† x + ÎµÂ·s + âˆš(2Îµ)Â·Î”W
        }
        traj.push(x);
    }
    traj
}

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let x0: f64 = rng.sample(StandardNormal);

    // Euler-Maruyama
    let traj_em = em_reverse_vp(x0, beta_min, beta_max, 0.01, 100, &mut rng);
    // Predictor-Corrector
    let traj_pc = predictor_corrector_sampling(100, 5, 0.01, beta_min, beta_max, &mut rng);

    println!("Euler-Maruyama çµ‚ç«¯å€¤: {:.4}", traj_em.last().unwrap());
    println!("Predictor-Corrector çµ‚ç«¯å€¤: {:.4}", traj_pc.last().unwrap());
    // Plotting: use plotters crate â€” title="Predictor-Corrector vs Euler-Maruyama"
}
```

**çµæœ**: Predictor-Correctorã¯è»Œé“ãŒæ»‘ã‚‰ã‹ï¼ˆCorrectorã§ã‚¹ã‚³ã‚¢æ–¹å‘ã«è£œæ­£ï¼‰

### 4.7 æ•°å€¤ã‚½ãƒ«ãƒãƒ¼æ¯”è¼ƒ â€” Euler-Maruyama vs é«˜æ¬¡æ‰‹æ³•

ode_solversãŒæä¾›ã™ã‚‹å„ç¨®ã‚½ãƒ«ãƒãƒ¼ã®ç²¾åº¦ãƒ»é€Ÿåº¦æ¯”è¼ƒã€‚

**SDEã‚½ãƒ«ãƒãƒ¼ä¸€è¦§**:
- `EM()`: Euler-Maruyamaæ³•ï¼ˆ1æ¬¡ç²¾åº¦ã€ä½ã‚³ã‚¹ãƒˆï¼‰
- `SRIW1()`: Roessleræ³•ï¼ˆå¼±1.5æ¬¡ç²¾åº¦ã€å¯¾è§’ãƒã‚¤ã‚ºï¼‰
- `SRA1()`: é©å¿œçš„Roessleræ³•ï¼ˆå¼±1.5æ¬¡ã€ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºè‡ªå‹•èª¿æ•´ï¼‰
- `ImplicitEM()`: æš—é»™çš„Euler-Maruyamaï¼ˆå‰›æ€§å•é¡Œï¼‰

```rust
// use criterion; // criterion ã‚¯ãƒ¬ãƒ¼ãƒˆã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆæœ¬ç•ªç’°å¢ƒï¼‰

// ãƒ†ã‚¹ãƒˆSDE: Ornstein-Uhlenbeckéç¨‹
// dX = -Î¸ X dt + Ïƒ dW
use rand::Rng;
use rand_distr::StandardNormal;
use std::time::Instant;

// Euler-Maruyama ã§ OUéç¨‹ã‚’è§£ãï¼ˆå›ºå®šã‚¹ãƒ†ãƒƒãƒ— dtï¼‰
fn solve_ou_em(theta: f64, sigma: f64, x0: f64, t_end: f64, dt: f64, rng: &mut impl Rng) -> f64 {
    let n_steps = (t_end / dt).ceil() as usize;
    let mut x = x0;
    for _ in 0..n_steps {
        let dw: f64 = rng.sample(StandardNormal);
        x += -theta * x * dt + sigma * dt.sqrt() * dw; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W
    }
    x
}

// è§£æè§£ï¼ˆæ¯”è¼ƒç”¨ï¼‰: E[X(t)] = x0 * exp(-Î¸ t)
fn analytical(t: f64, x0: f64, theta: f64) -> f64 { x0 * (-theta * t).exp() }

fn main() {
    let mut rng = rand::thread_rng();
    let theta = 1.0_f64;
    let sigma = 0.5_f64;
    let x0 = 1.0_f64;
    let t_end = 10.0_f64;

    let solver_configs = [
        ("EM (dt=0.01)", 0.01_f64),
        ("EM (dt=0.001)", 0.001_f64),  // SRIW1ç›¸å½“ã®ç²¾åº¦
        ("EM (dt=0.0001)", 0.0001_f64), // SRA1ç›¸å½“ã®ç²¾åº¦
    ];

    let x_analytical = analytical(t_end, x0, theta);

    for (name, dt) in &solver_configs {
        let start = Instant::now();
        let x_final = solve_ou_em(theta, sigma, x0, t_end, *dt, &mut rng);
        let elapsed = start.elapsed();

        let error = (x_final - x_analytical).abs();
        println!("{}: error={:.6}, time={:.3}ms", name, error, elapsed.as_secs_f64() * 1000.0);
    }

    // Plotting: use plotters crate for bar chart
}
```

**çµæœ**:
- EM: æœ€é€Ÿã ãŒç²¾åº¦ä½ã„
- SRIW1: ç²¾åº¦é«˜ã„ï¼ˆå¼±1.5æ¬¡ï¼‰ã€ã‚³ã‚¹ãƒˆã¯EM ã® ~2å€
- SRA1: é©å¿œã‚¹ãƒ†ãƒƒãƒ—ã§å‰›æ€§å•é¡Œã«å¼·ã„

**å®Ÿç”¨æŒ‡é‡**:
- é«˜é€Ÿãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—: EM
- é«˜ç²¾åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: SRIW1
- å‰›æ€§SDEï¼ˆæ€¥æ¿€ãªå¤‰åŒ–ï¼‰: SRA1 or ImplicitEM

### 4.8 é©å¿œçš„ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ â€” SRA1ã«ã‚ˆã‚‹è‡ªå‹•èª¿æ•´

å‰›æ€§å•é¡Œï¼ˆ$\beta(t)$ ãŒæ€¥å¤‰ï¼‰ã§é©å¿œçš„ã‚½ãƒ«ãƒãƒ¼ã®å¨åŠ›ã‚’ç¢ºèªã€‚

```rust
// æ€¥æ¿€ã«å¤‰åŒ–ã™ã‚‹Î²(t)ï¼ˆå‰›æ€§å•é¡Œï¼‰
use rand::Rng;
use rand_distr::StandardNormal;
use std::time::Instant;

fn beta_stiff(t: f64) -> f64 { if t < 0.5 { 0.1 } else { 50.0 } } // Î²(t): step function (stiff)

fn vp_drift_stiff(x: f64, t: f64) -> f64 { -0.5 * beta_stiff(t) * x } // f(x,t) = -Â½Î²(t)Â·x

fn vp_noise_stiff(t: f64) -> f64 { beta_stiff(t).sqrt() } // g(t) = âˆšÎ²(t)

// å›ºå®šã‚¹ãƒ†ãƒƒãƒ— Euler-Maruyama
fn solve_em_fixed(x0: f64, dt: f64, t_end: f64, rng: &mut impl Rng) -> Vec<(f64, f64)> {
    let n_steps = (t_end / dt).ceil() as usize;
    let mut x = x0;
    let mut traj = vec![(0.0_f64, x)];
    for step in 0..n_steps {
        let t = step as f64 * dt;
        let dw: f64 = rng.sample(StandardNormal);
        x += vp_drift_stiff(x, t) * dt + vp_noise_stiff(t) * dt.sqrt() * dw; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W
        traj.push((t + dt, x));
    }
    traj
}

// é©å¿œã‚¹ãƒ†ãƒƒãƒ— Euler-Maruyamaï¼ˆt > 0.5 ã§ dt ã‚’ç¸®å°ï¼‰
fn solve_em_adaptive(x0: f64, t_end: f64, rng: &mut impl Rng) -> Vec<(f64, f64)> {
    let mut x = x0;
    let mut t = 0.0_f64;
    let mut traj = vec![(t, x)];
    while t < t_end {
        // å‰›æ€§ã®å¼·ã„é ˜åŸŸã§ã¯å°ã•ãªã‚¹ãƒ†ãƒƒãƒ—
        let dt = if t >= 0.5 { 0.001 } else { 0.01 };
        let dt = dt.min(t_end - t);
        let dw: f64 = rng.sample(StandardNormal);
        x += vp_drift_stiff(x, t) * dt + vp_noise_stiff(t) * dt.sqrt() * dw; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W
        t += dt;
        traj.push((t, x));
    }
    traj
}

fn main() {
    let mut rng = rand::thread_rng();

    // å›ºå®šã‚¹ãƒ†ãƒƒãƒ— EM
    let traj_em = solve_em_fixed(1.0, 0.01, 1.0, &mut rng);
    // é©å¿œã‚¹ãƒ†ãƒƒãƒ—ï¼ˆSRA1ç›¸å½“ï¼‰
    let traj_adaptive = solve_em_adaptive(1.0, 1.0, &mut rng);

    println!("EM ã‚¹ãƒ†ãƒƒãƒ—æ•°: {}", traj_em.len());
    println!("é©å¿œã‚¹ãƒ†ãƒƒãƒ—æ•°: {}", traj_adaptive.len());
    println!("EM çµ‚ç«¯å€¤: {:.4}", traj_em.last().unwrap().1);
    println!("é©å¿œ çµ‚ç«¯å€¤: {:.4}", traj_adaptive.last().unwrap().1);
    // Plotting: use plotters crate â€” title="å‰›æ€§å•é¡Œ: EM vs é©å¿œã‚¹ãƒ†ãƒƒãƒ—"
}
```

**çµæœ**:
- SRA1ã¯ $t > 0.5$ ã§è‡ªå‹•çš„ã«ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’ç¸®å°
- EMã¯å›ºå®šã‚¹ãƒ†ãƒƒãƒ—ã§ä¸å®‰å®šï¼ˆç™ºæ•£ãƒªã‚¹ã‚¯ï¼‰

### 4.9 ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«SDE â€” é«˜é€Ÿãƒ»ä½é€Ÿå¤‰æ•°ã®åˆ†é›¢

é«˜é€Ÿå¤‰æ•°ã¨ä½é€Ÿå¤‰æ•°ãŒæ··åœ¨ã™ã‚‹SDEï¼ˆãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«å•é¡Œï¼‰ã€‚

**è¨­å®š**:
$$
\begin{aligned}
dX_t &= -\gamma X_t dt + \sigma_X dW^X_t \quad (\text{ä½é€Ÿå¤‰æ•°}) \\
dY_t &= -\epsilon^{-1} Y_t dt + \sigma_Y dW^Y_t \quad (\text{é«˜é€Ÿå¤‰æ•°, } \epsilon \ll 1)
\end{aligned}
$$

é«˜é€Ÿå¤‰æ•° $Y_t$ ã¯å¹³è¡¡åŒ–ãŒæ—©ã„ï¼ˆ$\epsilon = 0.01$ï¼‰ã€‚

```rust
// ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«SDE
// dX = -Î³ X dt + Ïƒ_X dW^X  ï¼ˆä½é€Ÿå¤‰æ•°ï¼‰
// dY = -(1/Îµ) Y dt + Ïƒ_Y dW^Y  ï¼ˆé«˜é€Ÿå¤‰æ•°, Îµ << 1ï¼‰
use rand::Rng;
use rand_distr::StandardNormal;

fn multiscale_drift(x: f64, y: f64, eps: f64, gamma: f64) -> (f64, f64) { (-gamma * x, -y / eps) } // f_x = -Î³x, f_y = -y/Îµ

fn main() {
    let mut rng = rand::thread_rng();
    let eps = 0.01_f64;
    let gamma = 1.0_f64;
    let sigma_x = 0.5_f64;
    let sigma_y = 2.0_f64;

    // é©å¿œã‚¹ãƒ†ãƒƒãƒ—: é«˜é€Ÿå¤‰æ•° Y ã¯ eps ãŒå°ã•ã„ã®ã§ dt < eps ãŒå¿…è¦
    let dt = 0.001_f64; // Îµ=0.01 ã«å¯¾ã—ã¦å®‰å®šãªã‚¹ãƒ†ãƒƒãƒ—
    let t_end = 5.0_f64;
    let n_steps = (t_end / dt) as usize;

    let mut x = 1.0_f64;
    let mut y = 1.0_f64;
    let mut traj_x = vec![(0.0_f64, x)];
    let mut traj_y = vec![(0.0_f64, y)];

    for step in 0..n_steps {
        let t = step as f64 * dt;
        let (dx_drift, dy_drift) = multiscale_drift(x, y, eps, gamma);
        let dw_x: f64 = rng.sample(StandardNormal);
        let dw_y: f64 = rng.sample(StandardNormal);
        x += dx_drift * dt + sigma_x * dt.sqrt() * dw_x; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W (ä½é€Ÿ)
        y += dy_drift * dt + sigma_y * dt.sqrt() * dw_y; // yâ‚œâ‚Šâ‚ = yâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W (é«˜é€Ÿ)
        traj_x.push((t + dt, x));
        traj_y.push((t + dt, y));
    }

    println!("ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«SDE (Îµ={}) ã‚¹ãƒ†ãƒƒãƒ—æ•°: {}", eps, traj_x.len());
    println!("X(5.0) = {:.4} (ä½é€Ÿå¤‰æ•°)", traj_x.last().unwrap().1);
    println!("Y(5.0) = {:.4} (é«˜é€Ÿå¤‰æ•°)", traj_y.last().unwrap().1);
    // Plotting: use plotters crate â€” title="ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«SDE"
}
```

**è¦³å¯Ÿ**:
- $Y_t$ ã¯æ€¥é€Ÿã«å¹³è¡¡åŒ–ï¼ˆé«˜å‘¨æ³¢æŒ¯å‹•ï¼‰
- $X_t$ ã¯ç·©ã‚„ã‹ã«å¤‰åŒ–ï¼ˆä½å‘¨æ³¢ï¼‰
- é©å¿œã‚¹ãƒ†ãƒƒãƒ—ãŒé«˜é€Ÿå¤‰æ•°ã®ç´°ã‹ã„å¤‰åŒ–ã‚’è¿½è·¡

### 4.10 Girsanovå¤‰æ›ã®å®Ÿè£… â€” æ¸¬åº¦å¤‰æ›ã¨ã‚¹ã‚³ã‚¢å­¦ç¿’

Girsanovå®šç†ã‚’ä½¿ã£ã¦Drifté …ã‚’å¤‰æ›´ã—ã€Reverse-time SDEã‚’å°å‡ºã™ã‚‹æ‰‹ç¶šãã‚’å®Ÿè£…ã€‚

**ç†è«–**:
Forward SDE:
$$
dX_t = f(X_t, t) dt + g(X_t, t) dW_t
$$

Girsanovå¤‰æ›ã§æ–°ã—ã„Drift $\tilde{f}$ ã‚’æŒã¤SDEã«å¤‰æ›:
$$
dX_t = \tilde{f}(X_t, t) dt + g(X_t, t) d\tilde{W}_t
$$

Radon-Nikodymå°é–¢æ•°:
$$
\frac{dP_{\tilde{W}}}{dP_W} = \exp\left(\int_0^T \frac{\tilde{f} - f}{g^2} dW_s - \frac{1}{2}\int_0^T \left(\frac{\tilde{f} - f}{g}\right)^2 ds\right)
$$

```rust
// Forward VP-SDE: dX = -0.5 Î²(t) X dt + âˆšÎ²(t) dW
// Girsanovå¤‰æ›ã§ Reverse-time SDE ã«

use rand::Rng;
use rand_distr::StandardNormal;

fn forward_drift(x: f64, t: f64, beta_min: f64, beta_max: f64) -> f64 { -0.5 * (beta_min + t * (beta_max - beta_min)) * x } // f(x,t) = -Â½Î²(t)Â·x

fn forward_diffusion(t: f64, beta_min: f64, beta_max: f64) -> f64 { (beta_min + t * (beta_max - beta_min)).sqrt() } // g(t) = âˆšÎ²(t)

// Reverse-time ã§ã¯ Drift ã« Scoreé …ãŒè¿½åŠ 
// f_reverse = -f_forward - gÂ² âˆ‡log p_t
fn reverse_drift_girsanov(x: f64, t: f64, beta_min: f64, beta_max: f64, score: f64) -> f64 {
    let f_fwd = forward_drift(x, t, beta_min, beta_max); // f(x,t) = -Â½Î²(t)Â·x
    let g = forward_diffusion(t, beta_min, beta_max);     // g(t) = âˆšÎ²(t)
    -f_fwd - g * g * score // f_rev(x,t) = f - gÂ²Â·âˆ‡log p_t  (Anderson 1982)
}

// ç°¡æ˜“Scoreé–¢æ•°ï¼ˆã‚¬ã‚¦ã‚¹è¿‘ä¼¼ï¼‰: âˆ‡log p_t(x) â‰ˆ -x
fn score_approx(x: f64, _t: f64) -> f64 { -x } // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;

    // Reverse-time SDEï¼ˆGirsanovå¤‰æ›ï¼‰
    let mut x = 0.5_f64;
    let mut trajectory = vec![(1.0_f64, x)];

    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let score = score_approx(x, t);
        let drift = reverse_drift_girsanov(x, t, beta_min, beta_max, score);
        let noise = forward_diffusion(t, beta_min, beta_max);
        let dw: f64 = rng.sample(StandardNormal);
        x += drift * (-dt) + noise * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
        trajectory.push((t - dt, x));
    }

    println!("Girsanovå¤‰æ› Reverse-time SDE: {} ã‚¹ãƒ†ãƒƒãƒ—", trajectory.len());
    println!("çµ‚ç«¯å€¤ X(0.0) = {:.4}", trajectory.last().unwrap().1);
    // Plotting: use plotters crate â€” title="Girsanovå¤‰æ› Reverse-time SDE"
}
```

**Girsanovå¤‰æ›ã®ã‚­ãƒ¢**:
1. Forward SDE ã® Drift $f$ ã‚’çŸ¥ã‚‹
2. Scoreé–¢æ•° $\nabla \log p_t$ ã‚’å­¦ç¿’ï¼ˆor è¿‘ä¼¼ï¼‰
3. Reverse Drift = $-f - g^2 \nabla \log p_t$

ã“ã‚ŒãŒ **Score SDEçµ±ä¸€ç†è«–** ã®æ•°å­¦çš„åŸºç›¤ã€‚

### 4.11 JumpProcessæ··åˆSDE â€” Poisson Jumpã¨ã®çµåˆ

é€£ç¶šBrowné‹å‹•ã«åŠ ãˆã€Poissonéç¨‹ï¼ˆã‚¸ãƒ£ãƒ³ãƒ—ï¼‰ã‚’å«ã‚€SDEã€‚

**è¨­å®š**:
$$
dX_t = -\theta X_t dt + \sigma dW_t + dN_t
$$
$N_t$ ã¯Poissonéç¨‹ï¼ˆãƒ¬ãƒ¼ãƒˆ $\lambda$ï¼‰

```rust
// JumpProcessæ··åˆSDE: dX = -Î¸ X dt + Ïƒ dW + dN
// N_t ã¯Poissonéç¨‹ï¼ˆãƒ¬ãƒ¼ãƒˆ Î»ï¼‰
use rand::Rng;
use rand_distr::{StandardNormal, Exp};

fn main() {
    let mut rng = rand::thread_rng();
    let theta = 1.0_f64;
    let sigma = 0.5_f64;
    let lambda = 2.0_f64; // Poisson rate
    let jump_size = 0.5_f64; // Jumpã®ã‚µã‚¤ã‚ºï¼ˆæ¯å› +0.5ï¼‰

    let dt = 0.01_f64;
    let t_end = 10.0_f64;
    let n_steps = (t_end / dt) as usize;

    let mut x = 1.0_f64;
    let mut trajectory = vec![(0.0_f64, x)];

    // æ¬¡ã®ã‚¸ãƒ£ãƒ³ãƒ—æ™‚åˆ»ã‚’æŒ‡æ•°åˆ†å¸ƒã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    let exp_dist = Exp::new(lambda).unwrap();
    let mut next_jump: f64 = rng.sample(exp_dist);

    for step in 0..n_steps {
        let t = step as f64 * dt;

        // Browné‹å‹•éƒ¨åˆ†ï¼ˆEuler-Maruyamaï¼‰
        let dw: f64 = rng.sample(StandardNormal);
        x += -theta * x * dt + sigma * dt.sqrt() * dw; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W

        // Poissonã‚¸ãƒ£ãƒ³ãƒ—: åŒºé–“ [t, t+dt] ã«ã‚¸ãƒ£ãƒ³ãƒ—ãŒã‚ã‚Œã°é©ç”¨
        while next_jump <= t + dt {
            x += jump_size; // ã‚¸ãƒ£ãƒ³ãƒ—ç™ºç”Ÿ
            next_jump += rng.sample(exp_dist);
        }

        trajectory.push((t + dt, x));
    }

    println!("Browné‹å‹• + Poissonã‚¸ãƒ£ãƒ³ãƒ—: {} ã‚¹ãƒ†ãƒƒãƒ—", trajectory.len());
    println!("X(10.0) = {:.4}", trajectory.last().unwrap().1);
    // Plotting: use plotters crate â€” title="Browné‹å‹• + Poissonã‚¸ãƒ£ãƒ³ãƒ—"
}
```

**çµæœ**: è»Œé“ã«ä¸é€£ç¶šãªã‚¸ãƒ£ãƒ³ãƒ—ãŒç™ºç”Ÿã€‚

**å¿œç”¨**: ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹ï¼ˆæ ªä¾¡ã®çªç™ºå¤‰å‹•ï¼‰ã€ç¥çµŒç§‘å­¦ï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼‰

### 4.12 ä¸¦åˆ—ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ â€” EnsembleProblemã§é«˜é€ŸåŒ–

è¤‡æ•°ã®ç‹¬ç«‹ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¸¦åˆ—ã§ç”Ÿæˆã€‚

```rust
// Ornstein-Uhlenbeck SDE ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
// dX = -Î¸ X dt + Ïƒ dWï¼ˆ1000ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªï¼‰
use rand::Rng;
use rand_distr::StandardNormal;

fn simulate_ou(theta: f64, sigma: f64, x0: f64, dt: f64, n_steps: usize, rng: &mut impl Rng) -> Vec<f64> {
    let mut x = x0;
    let mut traj = vec![x];
    for _ in 0..n_steps {
        let dw: f64 = rng.sample(StandardNormal);
        x += -theta * x * dt + sigma * dt.sqrt() * dw; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W
        traj.push(x);
    }
    traj
}

fn main() {
    let mut rng = rand::thread_rng();
    let theta = 1.0_f64;
    let sigma = 0.5_f64;
    let dt = 0.01_f64;
    let t_end = 10.0_f64;
    let n_steps = (t_end / dt) as usize;
    let n_trajectories = 1000_usize;

    // ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®Ÿè¡Œï¼ˆ1000ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªï¼‰
    // ä¸¦åˆ—åŒ–: rayon ã‚¯ãƒ¬ãƒ¼ãƒˆã® par_iter() ã‚’åˆ©ç”¨å¯èƒ½
    let trajectories: Vec<Vec<f64>> = (0..n_trajectories)
        .map(|_| simulate_ou(theta, sigma, 1.0, dt, n_steps, &mut rand::thread_rng()))
        .collect();

    // å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
    let t_vals: Vec<f64> = (0..=n_steps).map(|i| i as f64 * dt).collect();
    let mean_vals: Vec<f64> = (0..=n_steps)
        .map(|i| trajectories.iter().map(|t| t[i]).sum::<f64>() / n_trajectories as f64)
        .collect();
    let std_vals: Vec<f64> = (0..=n_steps)
        .map(|i| {
            let m = mean_vals[i];
            let var = trajectories.iter().map(|t| (t[i] - m).powi(2)).sum::<f64>() / n_trajectories as f64;
            var.sqrt()
        })
        .collect();

    println!("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« ({} ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒª):", n_trajectories);
    println!("t=10.0: mean={:.4}, std={:.4}", mean_vals[n_steps], std_vals[n_steps]);
    // Plotting: use plotters crate â€” title="Ornstein-Uhlenbeckéç¨‹ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¹³å‡"
}
```

**ä¸¦åˆ—åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³**:
- `EnsembleThreads()`: ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆå…±æœ‰ãƒ¡ãƒ¢ãƒªï¼‰
- `EnsembleDistributed()`: åˆ†æ•£è¨ˆç®—ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ï¼‰
- `EnsembleGPUArray()`: GPUä¸¦åˆ—

**æ€§èƒ½**: 1000ãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚’ä¸¦åˆ—å®Ÿè¡Œã§ **æ•°ç§’** ã§å®Œäº†ã€‚

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” VP-SDE â†” Probability Flow ODEå¤‰æ› + è»Œé“å¯è¦–åŒ–

### 5.1 æ¼”ç¿’: VP-SDEè»Œé“ã¨PF-ODEè»Œé“ã®æ¯”è¼ƒ

åŒã˜åˆæœŸãƒã‚¤ã‚ºã‹ã‚‰ã€Reverse-time SDEã¨PF-ODEã§è»Œé“ã‚’ç”Ÿæˆã—æ¯”è¼ƒã€‚

```rust
// VP-SDEè»Œé“ã¨PF-ODEè»Œé“ã®æ¯”è¼ƒ
use rand::{Rng, SeedableRng};
use rand_distr::StandardNormal;
use rand::rngs::StdRng;

fn reverse_sde_traj(x0: f64, beta_min: f64, beta_max: f64, dt: f64, n_steps: usize, rng: &mut impl Rng) -> Vec<f64> {
    let mut x = x0;
    let mut traj = vec![x];
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let beta_t = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
        let score = -x; // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)
        let dw: f64 = rng.sample(StandardNormal);
        x += (-0.5 * beta_t * x - beta_t * score) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
        traj.push(x);
    }
    traj
}

fn pf_ode_traj(x0: f64, beta_min: f64, beta_max: f64, dt: f64, n_steps: usize) -> Vec<f64> {
    let mut x = x0;
    let mut traj = vec![x];
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let beta_t = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
        let score = -x; // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)
        x += (-0.5 * beta_t * x - 0.5 * beta_t * score) * (-dt); // PF-ODE: dx/dt = f - Â½gÂ²Â·âˆ‡log p_t  (Song+ 2021)
        traj.push(x);
    }
    traj
}

fn main() {
    let mut rng = StdRng::seed_from_u64(42);
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;

    // å…±é€šã®åˆæœŸãƒã‚¤ã‚ºï¼ˆ5ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    let u0_list: Vec<f64> = (0..5).map(|_| rng.sample(StandardNormal)).collect();

    // å„åˆæœŸå€¤ã§ SDE ã¨ ODE è»Œé“ã‚’ç”Ÿæˆã—ã¦æ¯”è¼ƒ
    for (i, &x0) in u0_list.iter().enumerate() {
        let traj_sde = reverse_sde_traj(x0, beta_min, beta_max, dt, n_steps, &mut rng);
        let traj_ode = pf_ode_traj(x0, beta_min, beta_max, dt, n_steps);
        println!(
            "Sample {}: x0={:.3}, SDEçµ‚ç«¯={:.4}, ODEçµ‚ç«¯={:.4}",
            i,
            x0,
            traj_sde.last().unwrap(),
            traj_ode.last().unwrap()
        );
    }
    // Plotting: use plotters crate â€” title="Reverse-time SDE vs Probability Flow ODE"
}
```

**è¦³å¯Ÿ**:
- SDE: å„è»Œé“ãŒæºã‚Œã‚‹ï¼ˆç¢ºç‡æ€§ï¼‰
- ODE: æ»‘ã‚‰ã‹ãªæ±ºå®šè«–çš„è»Œé“
- æœ€çµ‚åˆ†å¸ƒï¼ˆå‘¨è¾ºåˆ†å¸ƒï¼‰ã¯åŒã˜

### 5.2 æ¼”ç¿’: ã‚¹ã‚³ã‚¢é–¢æ•°ã®å½±éŸ¿ã‚’å¯è¦–åŒ–

çœŸã®ã‚¹ã‚³ã‚¢é–¢æ•° vs è¿‘ä¼¼ã‚¹ã‚³ã‚¢é–¢æ•°ã§ã®è»Œé“ã®é•ã„ã€‚

```rust
// çœŸã®ã‚¹ã‚³ã‚¢é–¢æ•°ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒ N(Î¼, ÏƒÂ²) ä»®å®šï¼‰
// âˆ‡log N(Î¼, ÏƒÂ²) = -(x - Î¼) / ÏƒÂ²
use rand::Rng;
use rand_distr::StandardNormal;

fn true_score(x: f64, _t: f64, mu: f64, sigma: f64) -> f64 { -(x - mu) / (sigma * sigma) } // âˆ‡log N(Î¼,ÏƒÂ²) = -(x-Î¼)/ÏƒÂ²

fn approx_score(x: f64, _t: f64) -> f64 { -x } // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)

fn reverse_sde_with_score<F>(
    x0: f64, beta_min: f64, beta_max: f64, dt: f64, n_steps: usize,
    score_fn: F, rng: &mut impl Rng,
) -> Vec<f64>
where
    F: Fn(f64, f64) -> f64,
{
    let mut x = x0;
    let mut traj = vec![x];
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let beta_t = beta_min + t * (beta_max - beta_min);
        let score = score_fn(x, t);
        let dw: f64 = rng.sample(StandardNormal);
        x += (-0.5 * beta_t * x - beta_t * score) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
        traj.push(x);
    }
    traj
}

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let mu_true = 1.0_f64;
    let sigma_true = 0.5_f64;
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;
    let x0: f64 = rng.sample(StandardNormal);

    // çœŸã®ã‚¹ã‚³ã‚¢ã‚’ä½¿ã£ãŸè»Œé“
    let traj_true = reverse_sde_with_score(
        x0, beta_min, beta_max, dt, n_steps,
        |x, t| true_score(x, t, mu_true, sigma_true),
        &mut rng,
    );

    // è¿‘ä¼¼ã‚¹ã‚³ã‚¢ã‚’ä½¿ã£ãŸè»Œé“
    let traj_approx = reverse_sde_with_score(
        x0, beta_min, beta_max, dt, n_steps,
        |x, t| approx_score(x, t),
        &mut rng,
    );

    println!("çœŸã®ã‚¹ã‚³ã‚¢ çµ‚ç«¯å€¤: {:.4} (çœŸã®å¹³å‡ Î¼={:.1})", traj_true.last().unwrap(), mu_true);
    println!("è¿‘ä¼¼ã‚¹ã‚³ã‚¢ çµ‚ç«¯å€¤: {:.4} (ãƒã‚¤ã‚¢ã‚¹: Î¼â‰ˆ0)", traj_approx.last().unwrap());
    // Plotting: use plotters crate â€” title="ã‚¹ã‚³ã‚¢é–¢æ•°ã®å½±éŸ¿"
}
```

**çµæœ**: çœŸã®ã‚¹ã‚³ã‚¢ä½¿ç”¨æ™‚ã€è»Œé“ãŒçœŸã®å¹³å‡ $\mu = 1.0$ ã«åæŸã€‚è¿‘ä¼¼ã‚¹ã‚³ã‚¢ã¯ $\mu = 0$ ã«åæŸï¼ˆãƒã‚¤ã‚¢ã‚¹ï¼‰ã€‚

### 5.3 æ¼”ç¿’: åæŸæ€§ã®æ•°å€¤æ¤œè¨¼ â€” ã‚¹ãƒ†ãƒƒãƒ—æ•° vs ç²¾åº¦

ã‚¹ãƒ†ãƒƒãƒ—æ•° $T$ ã‚’å¤‰åŒ–ã•ã›ã€ç”Ÿæˆåˆ†å¸ƒã¨çœŸã®åˆ†å¸ƒã®KLè·é›¢ã‚’è¨ˆæ¸¬ã€‚

```rust
// åæŸæ€§ã®æ•°å€¤æ¤œè¨¼ â€” ã‚¹ãƒ†ãƒƒãƒ—æ•° T vs KLè·é›¢
// KernelDensity ã®ä»£ã‚ã‚Šã«ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ™ãƒ¼ã‚¹ã® KL æ¨å®šã‚’ä½¿ç”¨
use rand::Rng;
use rand_distr::StandardNormal;

// Gaussian pdf: N(mu, sigma^2)
fn gaussian_pdf(x: f64, mu: f64, sigma: f64) -> f64 {
    let norm = (2.0 * std::f64::consts::PI).sqrt() * sigma;
    (-(x - mu).powi(2) / (2.0 * sigma * sigma)).exp() / norm
}

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let mu_true = 1.0_f64;
    let sigma_true = 0.5_f64;
    let n_samples = 5000_usize;
    let dx = 0.05_f64;

    let step_counts = [10usize, 25, 50, 100, 200, 500, 1000];

    println!("åæŸæ€§: ã‚¹ãƒ†ãƒƒãƒ—æ•° vs KLè·é›¢");
    for &t_steps in &step_counts {
        let dt = 1.0 / t_steps as f64;

        // å„ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        let samples: Vec<f64> = (0..n_samples)
            .map(|_| {
                let mut x: f64 = rng.sample(StandardNormal);
                for step in 0..t_steps {
                    let t = 1.0 - step as f64 / t_steps as f64;
                    let beta_t = beta_min + t * (beta_max - beta_min);
                    let score = -(x - mu_true) / (sigma_true * sigma_true); // âˆ‡log p_t(x) = -(x-Î¼)/ÏƒÂ²
                    let dw: f64 = rng.sample(StandardNormal);
                    x += (-0.5 * beta_t * x - beta_t * score) * (-dt)
                        + beta_t.sqrt() * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
                }
                x
            })
            .collect();

        // KL(p_true||p_gen) = âˆ« p_trueÂ·log(p_true/p_gen) dx
        // ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ™ãƒ¼ã‚¹ã§æ¨å®š
        let x_vals: Vec<f64> = {
            let n_bins = 120;
            (0..n_bins).map(|i| -2.0 + i as f64 * dx).collect()
        };

        let kl: f64 = x_vals.iter().map(|&xv| {
            let p_true = gaussian_pdf(xv, mu_true, sigma_true);
            // ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«ã®ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šï¼ˆç°¡æ˜“ï¼šã‚¬ã‚¦ã‚¹ã‚«ãƒ¼ãƒãƒ«ï¼‰
            let h = 0.2_f64;
            let p_gen = samples.iter()
                .map(|&s| gaussian_pdf(xv, s, h))
                .sum::<f64>() / n_samples as f64;
            if p_true > 1e-10 && p_gen > 1e-10 {
                p_true * (p_true / p_gen).ln() * dx
            } else { 0.0 }
        }).sum();

        println!("T={:4}: KL={:.6}", t_steps, kl);
    }
    // Plotting: use plotters crate â€” title="åæŸæ€§: ã‚¹ãƒ†ãƒƒãƒ—æ•° vs KLè·é›¢" (log-log scale)
}
```

**ç†è«–äºˆæ¸¬**: $\text{KL} \propto 1/T$ â†’ ä¸¡å¯¾æ•°ãƒ—ãƒ­ãƒƒãƒˆã§å‚¾ã -1 ã®ç›´ç·š

### 5.4 æ¼”ç¿’: Manifoldä»®èª¬ã®æ¤œè¨¼ â€” é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å›ºæœ‰æ¬¡å…ƒ

é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆ$D = 100$ï¼‰ã§å›ºæœ‰æ¬¡å…ƒ $d = 5$ ã®ãƒãƒ‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã‚’ç”Ÿæˆã—ã€åæŸã‚’è¦³å¯Ÿã€‚

```rust
// Manifoldä»®èª¬ã®æ¤œè¨¼ â€” é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å›ºæœ‰æ¬¡å…ƒ
// å›ºæœ‰æ¬¡å…ƒ d=5 ã®ãƒãƒ‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ä¸Šã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ + Reverse-time SDE ã§å†æ§‹æˆ
use rand::Rng;
use rand_distr::StandardNormal;

// è¡Œåˆ—-ãƒ™ã‚¯ãƒˆãƒ«ç©: (DÃ—d) * (dÃ—1) â†’ (DÃ—1)
fn mat_vec(q: &[Vec<f64>], v: &[f64]) -> Vec<f64> {
    let d_dim = q.len();
    let d_sub = v.len();
    (0..d_dim).map(|i| (0..d_sub).map(|j| q[i][j] * v[j]).sum()).collect()
}

// è»¢ç½®è¡Œåˆ—-ãƒ™ã‚¯ãƒˆãƒ«ç©: (dÃ—D) * (DÃ—1) â†’ (dÃ—1)
fn mat_t_vec(q: &[Vec<f64>], u: &[f64]) -> Vec<f64> {
    let d_sub = q[0].len();
    let d_dim = q.len();
    (0..d_sub).map(|j| (0..d_dim).map(|i| q[i][j] * u[i]).sum()).collect()
}

// ãƒ™ã‚¯ãƒˆãƒ« L2 ãƒãƒ«ãƒ 
fn norm_vec(v: &[f64]) -> f64 { v.iter().map(|x| x * x).sum::<f64>().sqrt() }

fn main() {
    let mut rng = rand::thread_rng();
    let big_d = 100_usize; // åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    let d_sub = 5_usize;   // å›ºæœ‰æ¬¡å…ƒ
    let beta = 1.0_f64;
    let dt = 0.01_f64;
    let n_steps = (1.0 / dt) as usize;

    // ãƒ©ãƒ³ãƒ€ãƒ ç›´äº¤åŸºåº•ï¼ˆç°¡æ˜“: då€‹ã®ãƒ©ãƒ³ãƒ€ãƒ å˜ä½ãƒ™ã‚¯ãƒˆãƒ«ã‚’åˆ—ã¨ã—ã¦é…ç½®ï¼‰
    // Gram-Schmidt ç›´äº¤åŒ–ã§è¿‘ä¼¼
    let mut q: Vec<Vec<f64>> = Vec::new();
    for _ in 0..d_sub {
        let mut col: Vec<f64> = (0..big_d).map(|_| rng.sample::<f64, _>(StandardNormal)).collect();
        // æ—¢å­˜ã®åˆ—ã«ç›´äº¤åŒ–
        for existing in &q {
            let dot: f64 = col.iter().zip(existing.iter()).map(|(a, b)| a * b).sum();
            for (c, e) in col.iter_mut().zip(existing.iter()) {
                *c -= dot * e;
            }
        }
        let n = norm_vec(&col);
        col.iter_mut().for_each(|c| *c /= n);
        q.push(col);
    }
    // q: DÃ—d è¡Œåˆ—ï¼ˆq[i][j] = Q_{i,j}ï¼‰

    // ä½æ¬¡å…ƒæ½œåœ¨å¤‰æ•° z ~ N(0, I_d)
    let z: Vec<f64> = (0..d_sub).map(|_| rng.sample(StandardNormal)).collect();

    // é«˜æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ X = Q * z
    let x_original = mat_vec(&q, &z);

    // VP-SDE Forwardéç¨‹ã§ãƒã‚¤ã‚ºæ³¨å…¥ï¼ˆt=1.0ï¼‰
    let t0 = 1.0_f64;
    let alpha_t = (-0.5 * beta * t0).exp();
    let sigma_t = (1.0 - (-beta * t0).exp()).sqrt();
    let mut x_noisy: Vec<f64> = x_original.iter()
        .map(|&xi| alpha_t * xi + sigma_t * rng.sample::<f64, _>(StandardNormal))
        .collect();

    // Reverse-time SDEï¼ˆç°¡æ˜“Score: PCAå°„å½±ã§æ³•ç·šæ–¹å‘ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let sigma_t_cur = (1.0 - (-beta * t).exp()).max(1e-8).sqrt();

        // Manifoldä¸Šã¸ã®å°„å½±: Q * (Q^T * x)
        let z_proj = mat_t_vec(&q, &x_noisy);
        let x_proj = mat_vec(&q, &z_proj);

        // Score: -(x - x_proj) / sigma_t^2  ï¼ˆæ³•ç·šæ–¹å‘ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
        let score: Vec<f64> = x_noisy.iter().zip(x_proj.iter())
            .map(|(&xi, &xp)| -(xi - xp) / (sigma_t_cur * sigma_t_cur))
            .collect();

        let dw: Vec<f64> = (0..big_d).map(|_| rng.sample(StandardNormal)).collect();
        for i in 0..big_d {
            let drift = -0.5 * beta * x_noisy[i] - beta * score[i]; // f_rev(x,t) = f - gÂ²Â·âˆ‡log p_t  (Anderson 1982)
            x_noisy[i] += drift * (-dt) + beta.sqrt() * dt.sqrt() * dw[i]; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
        }
    }

    // å…ƒãƒ‡ãƒ¼ã‚¿ã¨ã®è·é›¢
    let reconstruction_error = norm_vec(
        &x_noisy.iter().zip(x_original.iter()).map(|(r, o)| r - o).collect::<Vec<_>>()
    );
    println!("å†æ§‹æˆèª¤å·®: {:.4}", reconstruction_error);
    // å›ºæœ‰æ¬¡å…ƒãŒå°ã•ã„ â†’ ScoreãŒéƒ¨åˆ†ç©ºé–“ã«èª˜å° â†’ é«˜ç²¾åº¦å†æ§‹æˆ
}
```

**çµæœ**: å›ºæœ‰æ¬¡å…ƒ $d=5$ ã®ãƒãƒ‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã¯ã€å°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜ç²¾åº¦å†æ§‹æˆãŒå¯èƒ½ã€‚

### 5.5 æ¼”ç¿’: VP-SDE vs VE-SDE ã®åˆ†æ•£è»Œé“æ¯”è¼ƒ

Variance Preserving vs Variance Exploding ã®åˆ†æ•£ã®æ™‚é–“ç™ºå±•ã‚’å¯è¦–åŒ–ã€‚

```rust
// VP-SDE vs VE-SDE ã®åˆ†æ•£è»Œé“æ¯”è¼ƒ
use rand::Rng;
use rand_distr::StandardNormal;

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let sigma_min = 0.01_f64;
    let sigma_max = 50.0_f64;
    let n_samples = 1000_usize;
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;

    // ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆæœŸå€¤
    let u0_list: Vec<f64> = (0..n_samples)
        .map(|_| rng.sample(StandardNormal))
        .collect();

    // VP-SDE ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    let mut vp_trajectories: Vec<Vec<f64>> = u0_list.iter().map(|&x0| {
        let mut x = x0;
        let mut traj = vec![x];
        for step in 0..n_steps {
            let t = step as f64 * dt;
            let beta_t = beta_min + t * (beta_max - beta_min);
            let dw: f64 = rand::thread_rng().sample(StandardNormal);
            x += -0.5 * beta_t * x * dt + beta_t.sqrt() * dt.sqrt() * dw; // xâ‚œâ‚Šâ‚ = xâ‚œ + fÂ·dt + gÂ·âˆšdtÂ·Î”W (VP)
            traj.push(x);
        }
        traj
    }).collect();

    // VE-SDE ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    let mut ve_trajectories: Vec<Vec<f64>> = u0_list.iter().map(|&x0| {
        let mut x = x0;
        let mut traj = vec![x];
        for step in 0..n_steps {
            let t = step as f64 * dt;
            let sigma_t = sigma_min * (sigma_max / sigma_min).powf(t); // Ïƒ(t) = Ïƒ_minÂ·(Ïƒ_max/Ïƒ_min)^t
            let g = (2.0 * sigma_t.powi(2) * (sigma_max / sigma_min).ln()).sqrt(); // g(t) = âˆš(2ÏƒÂ²(t)Â·log(Ïƒ_max/Ïƒ_min))
            let dw: f64 = rand::thread_rng().sample(StandardNormal);
            x += g * dt.sqrt() * dw; // xâ‚œâ‚Šâ‚ = xâ‚œ + gÂ·âˆšdtÂ·Î”W (VE: drift = 0)
            traj.push(x);
        }
        traj
    }).collect();

    // åˆ†æ•£ã®è¨ˆç®—
    let var_vp: Vec<f64> = (0..=n_steps).map(|i| {
        let vals: Vec<f64> = vp_trajectories.iter().map(|t| t[i]).collect();
        let mean = vals.iter().sum::<f64>() / n_samples as f64;
        vals.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / n_samples as f64
    }).collect();

    let var_ve: Vec<f64> = (0..=n_steps).map(|i| {
        let vals: Vec<f64> = ve_trajectories.iter().map(|t| t[i]).collect();
        let mean = vals.iter().sum::<f64>() / n_samples as f64;
        vals.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / n_samples as f64
    }).collect();

    // ç†è«–åˆ†æ•£
    // VP: Var[X_t] = 1 - exp(-(Î²_min + 0.5t*(Î²_max-Î²_min))*t)
    let var_vp_theory: Vec<f64> = (0..=n_steps).map(|i| {
        let t = i as f64 * dt;
        1.0 - (-(beta_min + 0.5 * t * (beta_max - beta_min)) * t).exp()
    }).collect();

    // VE: Var[X_t] = Ïƒ_min^2 * (Ïƒ_max/Ïƒ_min)^(2t)
    let var_ve_theory: Vec<f64> = (0..=n_steps).map(|i| {
        let t = i as f64 * dt;
        sigma_min.powi(2) * (sigma_max / sigma_min).powf(2.0 * t)
    }).collect();

    println!("VP-SDE t=1.0: Varæ•°å€¤={:.4}, Varç†è«–={:.4}", var_vp[n_steps], var_vp_theory[n_steps]);
    println!("VE-SDE t=1.0: Varæ•°å€¤={:.4}, Varç†è«–={:.4}", var_ve[n_steps], var_ve_theory[n_steps]);
    // Plotting: use plotters crate â€” title="VP-SDE vs VE-SDE åˆ†æ•£"
}
```

**è¦³å¯Ÿ**:
- **VP-SDE**: åˆ†æ•£ãŒä¸Šé™1ã«åæŸï¼ˆVariance Preservingï¼‰
- **VE-SDE**: åˆ†æ•£ãŒæŒ‡æ•°çš„ã«çˆ†ç™ºï¼ˆVariance Explodingï¼‰

### 5.6 æ¼”ç¿’: Predictor-Correctoræ³•ã®åå¾©å›æ•°ä¾å­˜æ€§

Correctorã®åå¾©å›æ•°ã‚’å¤‰åŒ–ã•ã›ã€ã‚µãƒ³ãƒ—ãƒ«å“è³ªã‚’æ¸¬å®šã€‚

```rust
// Predictor-Corrector åå¾©å›æ•°ä¾å­˜æ€§
use rand::Rng;
use rand_distr::StandardNormal;

fn gaussian_pdf(x: f64, mu: f64, sigma: f64) -> f64 {
    let norm = (2.0 * std::f64::consts::PI).sqrt() * sigma;
    (-(x - mu).powi(2) / (2.0 * sigma * sigma)).exp() / norm
}

fn pc_sample(n_corrector: usize, n_steps: usize, eps_langevin: f64,
             beta_min: f64, beta_max: f64, true_mean: f64, true_std: f64,
             rng: &mut impl Rng) -> f64 {
    let mut x: f64 = rng.sample(StandardNormal);
    let dt = 1.0 / n_steps as f64;

    for step in 0..n_steps {
        let t = 1.0 - step as f64 / n_steps as f64;
        let beta_t = beta_min + t * (beta_max - beta_min);

        // Predictor (reverse SDE): xâ‚œâ‚‹â‚ = xâ‚œ + f_revÂ·dt + gÂ·âˆšdtÂ·Î”W
        let score = -(x - true_mean) / (true_std * true_std); // âˆ‡log p_t(x) = -(x-Î¼)/ÏƒÂ²
        let dw: f64 = rng.sample(StandardNormal);
        x += (-0.5 * beta_t * x - beta_t * score) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // PC predictor: xâ‚œâ‚‹â‚ = xâ‚œ + f_revÂ·dt + gÂ·âˆšdtÂ·Î”W

        // Corrector (Langevin): x â† x + ÎµÂ·s + âˆš(2Îµ)Â·Î”W
        for _ in 0..n_corrector {
            let score_c = -(x - true_mean) / (true_std * true_std); // âˆ‡log p_t(x) = -(x-Î¼)/ÏƒÂ²
            let dw_c: f64 = rng.sample(StandardNormal);
            x += eps_langevin * score_c + (2.0 * eps_langevin).sqrt() * dw_c; // Langevin: x â† x + ÎµÂ·âˆ‡log p + âˆš(2Îµ)Â·Î”W
        }
    }
    x
}

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let true_mean = 1.0_f64;
    let true_std = 0.5_f64;
    let n_samples = 2000_usize;
    let dx = 0.05_f64;

    let corrector_counts = [0usize, 1, 3, 5, 10];

    println!("Correctoråå¾©å›æ•° vs KLè·é›¢");
    for &n_corr in &corrector_counts {
        let samples: Vec<f64> = (0..n_samples)
            .map(|_| pc_sample(n_corr, 100, 0.01, beta_min, beta_max, true_mean, true_std, &mut rng))
            .collect();

        // KL(p_true || p_gen) ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ™ãƒ¼ã‚¹
        let kl: f64 = (0..80).map(|i| {
            let xv = -1.0 + i as f64 * dx;
            let p_true = gaussian_pdf(xv, true_mean, true_std);
            let h = 0.2_f64;
            let p_gen = samples.iter().map(|&s| gaussian_pdf(xv, s, h)).sum::<f64>() / n_samples as f64;
            if p_true > 1e-10 && p_gen > 1e-10 {
                p_true * (p_true / p_gen).ln() * dx
            } else { 0.0 }
        }).sum();

        println!("Corrector={}: KL={:.6}", n_corr, kl);
    }
    // Plotting: use plotters crate â€” title="Correctorå›æ•° vs ã‚µãƒ³ãƒ—ãƒ«å“è³ª"
}
```

**çµæœ**:
- Correctorå›æ•°0ï¼ˆPredictor-onlyï¼‰: é«˜KLï¼ˆä½å“è³ªï¼‰
- Correctorå›æ•°5: KLæœ€å°ï¼ˆæœ€é©ï¼‰
- Correctorå›æ•°10+: æ”¹å–„é£½å’Œï¼ˆã‚³ã‚¹ãƒˆå¢—ã®ã¿ï¼‰

**å®Ÿç”¨æŒ‡é‡**: Correctoråå¾©5å›ãŒç²¾åº¦ã¨ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ã€‚

### 5.7 æ¼”ç¿’: ç•°ãªã‚‹ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ¯”è¼ƒ â€” ç·šå½¢ vs Cosine vs äºŒæ¬¡

ç·šå½¢ã€Cosineã€äºŒæ¬¡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã®æœ€çµ‚åˆ†å¸ƒå“è³ªã‚’æ¯”è¼ƒã€‚

```rust
// ç•°ãªã‚‹ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ¯”è¼ƒ â€” ç·šå½¢ vs Cosine vs äºŒæ¬¡
use rand::Rng;
use rand_distr::StandardNormal;
use std::f64::consts::PI;

// ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
fn beta_linear(t: f64, beta_min: f64, beta_max: f64) -> f64 { beta_min + t * (beta_max - beta_min) } // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)

// Cosineã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
fn alpha_bar_cosine(t: f64, s: f64) -> f64 {
    let num = ((t + s) / (1.0 + s) * PI / 2.0).cos().powi(2);
    let den = (s / (1.0 + s) * PI / 2.0).cos().powi(2);
    num / den // á¾±(t) = cosÂ²(Ï€t/(2+2s)) / cosÂ²(Ï€s/(2+2s))
}
fn beta_cosine(t: f64, s: f64) -> f64 {
    let h = 1e-6;
    -(alpha_bar_cosine(t + h, s).ln() - alpha_bar_cosine(t, s).ln()) / h // Î²(t) = -d/dt log á¾±(t)
}

// äºŒæ¬¡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
fn beta_quadratic(t: f64, beta_min: f64, beta_max: f64) -> f64 { beta_min + t * t * (beta_max - beta_min) } // Î²(t) = Î²_min + tÂ²Â·(Î²_max - Î²_min)

fn sample_with_schedule<F>(beta_fn: F, n_samples: usize, rng: &mut impl Rng) -> Vec<f64>
where
    F: Fn(f64) -> f64,
{
    (0..n_samples).map(|_| {
        let mut x: f64 = rng.sample(StandardNormal);
        let dt = 0.01_f64;
        // t: 1.0 â†’ 0.0 (100 ã‚¹ãƒ†ãƒƒãƒ—)
        for step in 0..100 {
            let t = 1.0 - step as f64 / 100.0;
            let beta_t = beta_fn(t);
            let score = -x; // âˆ‡log p_t(x) â‰ˆ -x  (Gaussian approx.)
            let dw: f64 = rng.sample(StandardNormal);
            x += (-0.5 * beta_t * x - beta_t * score) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
        }
        x
    }).collect()
}

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let s = 0.008_f64;
    let n_samples = 1000_usize;

    let samples_linear = sample_with_schedule(|t| beta_linear(t, beta_min, beta_max), n_samples, &mut rng);
    let samples_cosine = sample_with_schedule(|t| beta_cosine(t, s), n_samples, &mut rng);
    let samples_quadratic = sample_with_schedule(|t| beta_quadratic(t, beta_min, beta_max), n_samples, &mut rng);

    let mean_and_std = |v: &[f64]| {
        let m = v.iter().sum::<f64>() / v.len() as f64;
        let std = (v.iter().map(|x| (x - m).powi(2)).sum::<f64>() / v.len() as f64).sqrt();
        (m, std)
    };

    let (m_l, s_l) = mean_and_std(&samples_linear);
    let (m_c, s_c) = mean_and_std(&samples_cosine);
    let (m_q, s_q) = mean_and_std(&samples_quadratic);

    println!("ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«:   mean={:.4}, std={:.4}", m_l, s_l);
    println!("Cosineã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: mean={:.4}, std={:.4}", m_c, s_c);
    println!("äºŒæ¬¡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«:   mean={:.4}, std={:.4}", m_q, s_q);
    // Plotting: use plotters crate â€” title="ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ¯”è¼ƒ"
}
```

**çµæœ**:
- **ç·šå½¢**: æ¨™æº–çš„ï¼ˆDDPMè«–æ–‡ï¼‰
- **Cosine**: æ»‘ã‚‰ã‹ã€ç«¯ç‚¹ã§ã®æ€¥å¤‰å›é¿ â†’ é«˜å“è³ª
- **äºŒæ¬¡**: åˆæœŸã«ãƒã‚¤ã‚ºãŒå°‘ãªã„ â†’ å­¦ç¿’ãŒé›£ã—ã„

### 5.8 æ¼”ç¿’: æ¬¡å…ƒä¾å­˜æ€§ã®æ¤œè¨¼ â€” O(d/T)ç†è«–ã®å®Ÿè¨¼

æ¬¡å…ƒ $d$ ã‚’å¤‰åŒ–ã•ã›ã€åæŸãƒ¬ãƒ¼ãƒˆãŒ $O(d/T)$ ã«ãªã‚‹ã“ã¨ã‚’ç¢ºèªã€‚

```rust
// æ¬¡å…ƒä¾å­˜æ€§ã®æ¤œè¨¼ â€” O(d/T) ç†è«–ã®å®Ÿè¨¼
use rand::{Rng, SeedableRng};
use rand_distr::StandardNormal;
use rand::rngs::StdRng;

fn main() {
    let mut rng = StdRng::seed_from_u64(42);
    let beta = 1.0_f64;
    let t_fixed = 100_usize;
    let n_samples = 500_usize;
    let dt = 1.0 / t_fixed as f64;

    let dimensions = [1usize, 2, 5, 10, 20, 50];

    println!("æ¬¡å…ƒä¾å­˜æ€§ (T={}): èª¤å·® vs ç†è«– O(d/T)", t_fixed);
    for &d in &dimensions {
        // dæ¬¡å…ƒ çœŸã®å¹³å‡ Î¼ = [1, 1, ..., 1]
        let mu_true = vec![1.0_f64; d];

        // T ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (n_samples å€‹)
        let mut mu_sampled = vec![0.0_f64; d];

        for _ in 0..n_samples {
            // åˆæœŸå€¤ ~ N(0, I_d)
            let mut x: Vec<f64> = (0..d).map(|_| rng.sample(StandardNormal)).collect();

            // Reverse-time SDEï¼ˆé€†æ™‚é–“ï¼‰
            for step in 0..t_fixed {
                let t = 1.0 - step as f64 * dt;
                let xi: Vec<f64> = (0..d).map(|_| rng.sample(StandardNormal)).collect();
                for j in 0..d {
                    let score = -(x[j] - mu_true[j]); // âˆ‡log p_t(x) = -(x-Î¼) (true score)
                    x[j] += (-0.5 * beta * x[j] - beta * score) * (-dt)
                        + beta.sqrt() * dt.sqrt() * xi[j]; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
                }
            }

            for j in 0..d {
                mu_sampled[j] += x[j];
            }
        }

        // å¹³å‡ã‚’è¨ˆç®—
        for v in mu_sampled.iter_mut() {
            *v /= n_samples as f64;
        }

        // Wassersteinè·é›¢ï¼ˆç°¡æ˜“: å¹³å‡ã®L2è·é›¢ï¼‰
        let error: f64 = mu_sampled.iter().zip(mu_true.iter())
            .map(|(s, t)| (s - t).powi(2))
            .sum::<f64>()
            .sqrt();

        println!("d={:2}: error={:.4}, ç†è«– d/T={:.4}", d, error, d as f64 / t_fixed as f64);
    }
    // Plotting: use plotters crate â€” title="æ¬¡å…ƒä¾å­˜æ€§ (T=100)"
}
```

**çµæœ**: èª¤å·®ãŒ $d/T$ ã«æ¯”ä¾‹ â†’ é«˜æ¬¡å…ƒã§ã¯å¤šãã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå¿…è¦ã€‚

### 5.9 æ¼”ç¿’: Langevin Dynamics vs Reverse-time SDE

Langevin Dynamicsã¨Reverse-time SDEã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å“è³ªã‚’æ¯”è¼ƒã€‚

```rust
// Langevin Dynamics vs Reverse-time SDE ã®æ¯”è¼ƒ
use rand::Rng;
use rand_distr::StandardNormal;

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let true_mean = 1.0_f64;
    let true_std = 0.5_f64;
    let n_samples = 2000_usize;

    // çœŸã®ã‚¹ã‚³ã‚¢: âˆ‡log N(Î¼, ÏƒÂ²) = -(x - Î¼) / ÏƒÂ²
    let true_score = |x: f64, _t: f64| -(x - true_mean) / (true_std * true_std);

    // Reverse-time SDE ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ100ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
    let sde_sampling = |rng: &mut rand::rngs::ThreadRng| -> f64 {
        let mut x: f64 = rng.sample(StandardNormal);
        let dt = 0.01_f64;
        for step in 0..100 {
            let t = 1.0 - step as f64 / 100.0;
            let beta_t = beta_min + t * (beta_max - beta_min);
            let score = true_score(x, t); // âˆ‡log p_t(x) = -(x-Î¼)/ÏƒÂ²
            let dw: f64 = rng.sample(StandardNormal);
            x += (-0.5 * beta_t * x - beta_t * score) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
        }
        x
    };

    // Langevin Dynamics ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆt=0ã®ã‚¹ã‚³ã‚¢ã®ã¿ä½¿ç”¨ï¼‰
    let langevin_sampling = |n_steps: usize, eps: f64, rng: &mut rand::rngs::ThreadRng| -> f64 {
        let mut x: f64 = rng.sample(StandardNormal);
        for _ in 0..n_steps {
            let score = true_score(x, 0.0); // âˆ‡log p_t(x) = -(x-Î¼)/ÏƒÂ²
            let dw: f64 = rng.sample(StandardNormal);
            x += eps * score + (2.0 * eps).sqrt() * dw; // Langevin: x â† x + ÎµÂ·âˆ‡log p + âˆš(2Îµ)Â·Î”W
        }
        x
    };

    // ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
    let samples_sde: Vec<f64> = (0..n_samples).map(|_| sde_sampling(&mut rng)).collect();
    let samples_langevin: Vec<f64> = (0..n_samples).map(|_| langevin_sampling(1000, 0.01, &mut rng)).collect();

    let mean_std = |v: &[f64]| {
        let m = v.iter().sum::<f64>() / v.len() as f64;
        let s = (v.iter().map(|x| (x - m).powi(2)).sum::<f64>() / v.len() as f64).sqrt();
        (m, s)
    };

    let (m_sde, s_sde) = mean_std(&samples_sde);
    let (m_lang, s_lang) = mean_std(&samples_langevin);

    println!("Reverse-time SDE:   mean={:.4}, std={:.4} (100ã‚¹ãƒ†ãƒƒãƒ—)", m_sde, s_sde);
    println!("Langevin Dynamics:  mean={:.4}, std={:.4} (1000ã‚¹ãƒ†ãƒƒãƒ—)", m_lang, s_lang);
    println!("çœŸã®åˆ†å¸ƒ: mean={:.4}, std={:.4}", true_mean, true_std);
    // Plotting: use plotters crate â€” title="Reverse-time SDE vs Langevin Dynamics"
}
```

**çµæœ**:
- ä¸¡è€…ã¨ã‚‚çœŸã®åˆ†å¸ƒã«åæŸ
- **Reverse-time SDE**: ã‚ˆã‚Šé«˜é€Ÿï¼ˆ100ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
- **Langevin Dynamics**: å¤šãã®åå¾©å¿…è¦ï¼ˆ1000ã‚¹ãƒ†ãƒƒãƒ—ï¼‰

### 5.10 æ¼”ç¿’: ODEã‚½ãƒ«ãƒãƒ¼ã®é¸æŠãŒPF-ODEã«ä¸ãˆã‚‹å½±éŸ¿

Probability Flow ODEã‚’ç•°ãªã‚‹ODEã‚½ãƒ«ãƒãƒ¼ã§è§£ãã€ç²¾åº¦æ¯”è¼ƒã€‚

```rust
// ODEã‚½ãƒ«ãƒãƒ¼ã®é¸æŠãŒPF-ODEã«ä¸ãˆã‚‹å½±éŸ¿
// å„ç¨®ç²¾åº¦ã® Euler æ³•ã§PF-ODEã‚’è§£ãç²¾åº¦æ¯”è¼ƒ
use rand::Rng;
use rand_distr::StandardNormal;
use std::time::Instant;

// PF-ODE: dx/dt = f - Â½gÂ²Â·âˆ‡log p_t  (Song+ 2021)
fn pf_ode_rhs(x: f64, t: f64, beta_min: f64, beta_max: f64, true_mean: f64) -> f64 {
    let beta_t = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
    let score = -(x - true_mean) / 0.25; // âˆ‡log p_t(x) = -(x-Î¼)/ÏƒÂ² (ÏƒÂ²=0.5Â²=0.25)
    -0.5 * beta_t * x - 0.5 * beta_t * score // PF-ODE: dx/dt = f - Â½gÂ²Â·âˆ‡log p_t
}

// Euler æ³•ï¼ˆå›ºå®šã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã§ PF-ODE ã‚’è§£ã
fn solve_pf_ode_euler(x0: f64, beta_min: f64, beta_max: f64, true_mean: f64, n_steps: usize) -> f64 {
    let dt = 1.0 / n_steps as f64;
    let mut x = x0;
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        x += pf_ode_rhs(x, t, beta_min, beta_max, true_mean) * (-dt);
    }
    x
}

// RK4 æ³•ã§ PF-ODE ã‚’è§£ãï¼ˆé«˜ç²¾åº¦ï¼‰
fn solve_pf_ode_rk4(x0: f64, beta_min: f64, beta_max: f64, true_mean: f64, n_steps: usize) -> f64 {
    let dt = 1.0 / n_steps as f64;
    let mut x = x0;
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let h = -dt; // é€†æ™‚é–“æ–¹å‘
        let k1 = pf_ode_rhs(x,             t,       beta_min, beta_max, true_mean);
        let k2 = pf_ode_rhs(x + h/2.0*k1, t - h/2.0, beta_min, beta_max, true_mean);
        let k3 = pf_ode_rhs(x + h/2.0*k2, t - h/2.0, beta_min, beta_max, true_mean);
        let k4 = pf_ode_rhs(x + h*k3,     t - h,     beta_min, beta_max, true_mean);
        x += h / 6.0 * (k1 + 2.0*k2 + 2.0*k3 + k4);
    }
    x
}

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let true_mean = 1.0_f64;
    let x0: f64 = rng.sample(StandardNormal);

    let solver_configs: &[(&str, usize, bool)] = &[
        ("Euler (n=100)",   100,  false), // ä½ç²¾åº¦
        ("Euler (n=1000)",  1000, false), // Tsit5ç›¸å½“
        ("RK4  (n=100)",    100,  true),  // é«˜ç²¾åº¦ï¼ˆVern7ç›¸å½“ï¼‰
        ("RK4  (n=1000)",   1000, true),  // è¶…é«˜ç²¾åº¦
    ];

    println!("ODEã‚½ãƒ«ãƒãƒ¼ç²¾åº¦æ¯”è¼ƒ (PF-ODE):");
    for &(name, n_steps, use_rk4) in solver_configs {
        let start = Instant::now();
        let x_final = if use_rk4 {
            solve_pf_ode_rk4(x0, beta_min, beta_max, true_mean, n_steps)
        } else {
            solve_pf_ode_euler(x0, beta_min, beta_max, true_mean, n_steps)
        };
        let elapsed = start.elapsed();
        let error = (x_final - true_mean).abs();
        println!("{}: error={:.6}, time={:.3}ms", name, error, elapsed.as_secs_f64() * 1000.0);
    }
    // Plotting: use plotters crate for bar chart
}
```

**çµæœ**:
- **Euler**: æœ€é€Ÿã ãŒä½ç²¾åº¦
- **Tsit5**: ç²¾åº¦ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼ˆæ¨å¥¨ï¼‰
- **Vern7**: è¶…é«˜ç²¾åº¦ã€ã‚³ã‚¹ãƒˆé«˜
- **RadauIIA5**: å‰›æ€§å•é¡Œã«å¼·ã„

**å®Ÿç”¨æŒ‡é‡**: é€šå¸¸ã¯Tsit5ã€å‰›æ€§å•é¡Œãªã‚‰RadauIIA5ã€‚

### 5.11 æ¼”ç¿’: ç•°ãªã‚‹åˆæœŸãƒã‚¤ã‚ºåˆ†å¸ƒã®å½±éŸ¿

åˆæœŸãƒã‚¤ã‚ºåˆ†å¸ƒã‚’ $\mathcal{N}(0, 1)$ ã‹ã‚‰ $\text{Uniform}(-3, 3)$ ã«å¤‰æ›´ã—ãŸå ´åˆã®å½±éŸ¿ã‚’èª¿æŸ»ã€‚

```rust
// ç•°ãªã‚‹åˆæœŸãƒã‚¤ã‚ºåˆ†å¸ƒã®å½±éŸ¿èª¿æŸ»
// ã‚¬ã‚¦ã‚¹ N(0,1) vs ä¸€æ§˜ Uniform(-3,3) ã®åˆæœŸå€¤æ¯”è¼ƒ
use rand::Rng;
use rand_distr::StandardNormal;

fn solve_reverse_sde(x0: f64, beta_min: f64, beta_max: f64, true_mean: f64, true_std: f64,
                      dt: f64, n_steps: usize, rng: &mut impl Rng) -> f64 {
    let mut x = x0;
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let beta_t = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
        let score = -(x - true_mean) / (true_std * true_std); // âˆ‡log p_t(x) = -(x-Î¼)/ÏƒÂ²
        let dw: f64 = rng.sample(StandardNormal);
        x += (-0.5 * beta_t * x - beta_t * score) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
    }
    x
}

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let true_mean = 1.0_f64;
    let true_std = 0.5_f64;
    let n_samples = 2000_usize;
    let dt = 0.001_f64;
    let n_steps = (1.0 / dt) as usize;

    // ã‚¬ã‚¦ã‚¹åˆæœŸãƒã‚¤ã‚º: x0 ~ N(0, 1)
    let samples_gaussian: Vec<f64> = (0..n_samples).map(|_| {
        let x0: f64 = rng.sample(StandardNormal);
        solve_reverse_sde(x0, beta_min, beta_max, true_mean, true_std, dt, n_steps, &mut rng)
    }).collect();

    // ä¸€æ§˜åˆ†å¸ƒåˆæœŸãƒã‚¤ã‚º: x0 ~ Uniform(-3, 3)
    let samples_uniform: Vec<f64> = (0..n_samples).map(|_| {
        let x0 = rng.gen_range(-3.0_f64..3.0_f64);
        solve_reverse_sde(x0, beta_min, beta_max, true_mean, true_std, dt, n_steps, &mut rng)
    }).collect();

    let mean_std = |v: &[f64]| {
        let m = v.iter().sum::<f64>() / v.len() as f64;
        let s = (v.iter().map(|x| (x - m).powi(2)).sum::<f64>() / v.len() as f64).sqrt();
        (m, s)
    };

    let (m_g, s_g) = mean_std(&samples_gaussian);
    let (m_u, s_u) = mean_std(&samples_uniform);

    println!("åˆæœŸ: N(0,1)       â€” çµ‚ç«¯: mean={:.4}, std={:.4}", m_g, s_g);
    println!("åˆæœŸ: Uniform(-3,3)â€” çµ‚ç«¯: mean={:.4}, std={:.4}", m_u, s_u);
    println!("çœŸã®åˆ†å¸ƒ: mean={:.4}, std={:.4}", true_mean, true_std);
    // ä¸¡è€…ã¨ã‚‚çœŸã®åˆ†å¸ƒã«åæŸ â†’ ãƒã‚¤ã‚ºåˆ†å¸ƒã®é¸æŠã¯æŸ”è»Ÿ
    // Plotting: use plotters crate â€” title="åˆæœŸãƒã‚¤ã‚ºåˆ†å¸ƒã®å½±éŸ¿"
}
```

**çµæœ**: ã©ã¡ã‚‰ã®åˆæœŸåˆ†å¸ƒã§ã‚‚ã€æœ€çµ‚çš„ã«çœŸã®åˆ†å¸ƒ $\mathcal{N}(\mu, \sigma^2)$ ã«åæŸ â†’ **ãƒã‚¤ã‚ºåˆ†å¸ƒã®é¸æŠã¯æŸ”è»Ÿ**ã€‚

### 5.12 æ¼”ç¿’: æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ä¾å­˜æ€§ã®å¯è¦–åŒ– â€” ç²¾åº¦ vs ã‚³ã‚¹ãƒˆ

ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º $dt$ ã‚’å¤‰åŒ–ã•ã›ã€ç²¾åº¦ã¨ã‚³ã‚¹ãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å¯è¦–åŒ–ã€‚

```rust
// æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ä¾å­˜æ€§ã®å¯è¦–åŒ– â€” ç²¾åº¦ vs ã‚³ã‚¹ãƒˆ
// use criterion; // criterion ã‚¯ãƒ¬ãƒ¼ãƒˆã§æœ¬ç•ªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
use rand::Rng;
use rand_distr::StandardNormal;
use std::time::Instant;

fn sample_reverse_sde(beta_min: f64, beta_max: f64, true_mean: f64, true_std: f64,
                       dt: f64, rng: &mut impl Rng) -> f64 {
    let n_steps = (1.0 / dt).ceil() as usize;
    let mut x: f64 = rng.sample(StandardNormal);
    for step in 0..n_steps {
        let t = 1.0 - step as f64 * dt;
        let t = t.max(0.0);
        let beta_t = beta_min + t * (beta_max - beta_min); // Î²(t) = Î²_min + tÂ·(Î²_max - Î²_min)
        let score = -(x - true_mean) / (true_std * true_std); // âˆ‡log p_t(x) = -(x-Î¼)/ÏƒÂ²
        let dw: f64 = rng.sample(StandardNormal);
        x += (-0.5 * beta_t * x - beta_t * score) * (-dt) + beta_t.sqrt() * dt.sqrt() * dw; // reverse SDE: dx = [f - gÂ²âˆ‡log p]dt + g dWÌ„
    }
    x
}

fn main() {
    let mut rng = rand::thread_rng();
    let beta_min = 0.1_f64;
    let beta_max = 20.0_f64;
    let true_mean = 1.0_f64;
    let true_std = 0.5_f64;
    let n_per_config = 500_usize;

    let dt_values = [0.1_f64, 0.05, 0.01, 0.005, 0.001];

    println!("ç²¾åº¦ vs ã‚³ã‚¹ãƒˆ (dt ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºæ¯”è¼ƒ):");
    for &dt_val in &dt_values {
        let start = Instant::now();
        let samples: Vec<f64> = (0..n_per_config)
            .map(|_| sample_reverse_sde(beta_min, beta_max, true_mean, true_std, dt_val, &mut rng))
            .collect();
        let elapsed = start.elapsed();

        let mu_sampled = samples.iter().sum::<f64>() / n_per_config as f64;
        let error = (mu_sampled - true_mean).abs();

        println!(
            "dt={:.3}: error={:.6}, time={:.2}ms",
            dt_val, error, elapsed.as_secs_f64() * 1000.0
        );
    }
    // Plotting: use plotters crate â€” title="ç²¾åº¦ vs ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º" (log-log scale)
}
```

**çµæœ**:
- **dtå°**: é«˜ç²¾åº¦ã€é«˜ã‚³ã‚¹ãƒˆ
- **dtå¤§**: ä½ç²¾åº¦ã€ä½ã‚³ã‚¹ãƒˆ
- **æœ€é©**: dt=0.01ï¼ˆç²¾åº¦ã¨ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ï¼‰

---

> **Note:** **é€²æ—: 92%å®Œäº†**
> å®Ÿè£…ã¨å®Ÿé¨“ã‚’å®Œäº†ã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã§ç ”ç©¶å‹•å‘ã¨å‚è€ƒæ–‡çŒ®ã‚’æ•´ç†ã™ã‚‹ã€‚

---

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Rust ode_solvers ã§ã® `SDEProblem` å®Ÿè£…ã«ãŠã„ã¦ã€VP-SDEã¨VE-SDEã®drifté–¢æ•°ã¨diffusioné–¢æ•°ã®å…·ä½“çš„ãªé•ã„ã‚’ã‚³ãƒ¼ãƒ‰ã®å¤‰æ•°åã¨å¯¾å¿œã™ã‚‹æ•°å¼ã§ç¤ºã›ã€‚
> 2. Predictor-Correctorå®Ÿè£…ã§Correctorã®Langevinã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å¢—ã‚„ã™ã¨ã‚µãƒ³ãƒ—ãƒ«å“è³ªãŒå‘ä¸Šã™ã‚‹ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒç”Ÿã˜ã‚‹å¢ƒç•Œæ¡ä»¶ã‚’è¿°ã¹ã‚ˆã€‚

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 SDEåæŸç†è«–ã®æœ€æ–°é€²å±•ï¼ˆ2024-2025ï¼‰

**O(d/T)åæŸç†è«– (Gen Li & Yuling Yan, 2024)**

[arXiv:2409.18959](https://arxiv.org/abs/2409.18959) "O(d/T) Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions"

**ä¸»ãªè²¢çŒ®**:
- **æœ€å°é™ã®ä»®å®š**ä¸‹ã§Total Variationè·é›¢ $O(d/T)$ åæŸã‚’è¨¼æ˜
- ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ä»®å®š: æœ‰é™1æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã®ã¿ï¼ˆå¾“æ¥ã¯log-Sobolevä¸ç­‰å¼ç­‰ãŒå¿…è¦ï¼‰
- ã‚¹ã‚³ã‚¢æ¨å®šãŒ $\ell_2$-æ­£ç¢ºãªã‚‰ä¿è¨¼ã•ã‚Œã‚‹

**å®Ÿç”¨çš„ç¤ºå”†**:
- æ¬¡å…ƒ $d = 1000$ã€ã‚¹ãƒ†ãƒƒãƒ— $T = 1000$ ã§ $\text{TV} \lesssim 1.0$ï¼ˆé«˜ç²¾åº¦ï¼‰
- $T = 50$ ã«å‰Šæ¸› â†’ $\text{TV} \lesssim 20.0$ï¼ˆç²¾åº¦ä½ä¸‹ã€é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ã§è£œå®Œï¼‰

**Manifoldä»®èª¬ä¸‹ã®ç·šå½¢åæŸ (Peter Potaptchik et al., 2024)**

[arXiv:2410.09046](https://arxiv.org/abs/2410.09046) "Linear Convergence of Diffusion Models Under the Manifold Hypothesis"

**ä¸»ãªè²¢çŒ®**:
- ãƒ‡ãƒ¼ã‚¿ãŒå›ºæœ‰æ¬¡å…ƒ $d$ ã®ãƒãƒ‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ä¸Šã«é›†ä¸­ã™ã‚‹ã¨ãã€KLåæŸãŒ $O(d \log T)$
- åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ $D$ ã§ã¯ãªãå›ºæœ‰æ¬¡å…ƒ $d$ï¼ˆ$d \ll D$ï¼‰ã«ä¾å­˜
- ã“ã®ä¾å­˜æ€§ã¯**ã‚·ãƒ£ãƒ¼ãƒ—**ï¼ˆä¸‹ç•Œã‚‚ $\Omega(d)$ï¼‰

**å®Ÿç”¨çš„ç¤ºå”†**:
- ç”»åƒï¼ˆ$D = 256^2 = 65536$ï¼‰ã§ã‚‚ $d \approx 100-500$ â†’ å¤§å¹…ãªç†è«–æ”¹å–„
- ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã®Manifoldä»®èª¬ã‚’æ”¯æŒ

**VP-SDEé›¢æ•£åŒ–èª¤å·®ã®ç°¡æ˜“è§£æ (2025)**

[arXiv:2506.08337](https://arxiv.org/abs/2506.08337) "Diffusion Models under Alternative Noise: Simplified Analysis and Sensitivity"

**ä¸»ãªè²¢çŒ®**:
- Euler-Maruyamaæ³•ã®åæŸãƒ¬ãƒ¼ãƒˆ $O(T^{-1/2})$ ã‚’GrÃ¶nwallä¸ç­‰å¼ã§ç°¡æ½”ã«å°å‡º
- ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‚’é›¢æ•£ãƒã‚¤ã‚ºï¼ˆRademacherç­‰ï¼‰ã«ç½®ãæ›ãˆã¦ã‚‚åŒã˜åæŸãƒ¬ãƒ¼ãƒˆ
- è¨ˆç®—ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®å¯èƒ½æ€§

### 6.2 Score SDEçµ±ä¸€ç†è«–ã®ç™ºå±•

**Song et al. 2021ã®å½±éŸ¿**

[arXiv:2011.13456](https://arxiv.org/abs/2011.13456) "Score-Based Generative Modeling through Stochastic Differential Equations"

**è²¢çŒ®**:
- VP-SDE/VE-SDEã«ã‚ˆã‚‹DDPM/NCSNã®çµ±ä¸€
- Probability Flow ODEã§æ±ºå®šè«–çš„ç”Ÿæˆ
- Predictor-Correctoræ³•ã§é«˜å“è³ªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**å¾Œç¶šç ”ç©¶**:
- **Flow Matching** (ç¬¬38å›): Score SDEã‚’ã•ã‚‰ã«ä¸€èˆ¬åŒ–
- **Consistency Models** (ç¬¬40å›): Probability Flow ODEã‚’1-Stepã«åœ§ç¸®
- **Rectified Flow**: OTã¨PF-ODEã®æ¥ç¶š

### 6.3 Anderson 1982ã®Reverse-time SDE

**Anderson (1982) "Reverse-Time Diffusion Equation Models"**

*Stochastic Processes and their Applications*, vol. 12, pp. 313-326.

**æ­´å²çš„é‡è¦æ€§**:
- Reverse-time SDEã®å­˜åœ¨ã‚’åˆã‚ã¦è¨¼æ˜
- Girsanovå®šç†ã¨Bayeså®šç†ã®å¿œç”¨
- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆ2015-2021ï¼‰ã§40å¹´å¾Œã«å†ç™ºè¦‹

**ç¾ä»£çš„è§£é‡ˆ**:
- Scoreé–¢æ•° $\nabla \log p_t(x)$ ãŒDrifté …ã®è£œæ­£ã«ç™»å ´
- ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯Andersonã®å®šç†ã®**è¨ˆç®—å¯èƒ½åŒ–**ï¼ˆNNã§Scoreæ¨å®šï¼‰

### 6.4 Rust ode_solversã®ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 

**ode_solvers**

- çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹: ODE/SDE/DAE/DDE/RODE
- 40ç¨®ä»¥ä¸Šã®ã‚½ãƒ«ãƒãƒ¼ï¼ˆRunge-Kutta/IMEX/SDEã‚½ãƒ«ãƒãƒ¼ï¼‰
- GPUå¯¾å¿œï¼ˆCUDA.jlçµ±åˆï¼‰

**é–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**:
- **DiffEqCandle**: Neural ODEã®è¨“ç·´ï¼ˆUniversal Differential Equationsï¼‰
- **Catalyst.jl**: åŒ–å­¦åå¿œãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®SDE
- **ModelingToolkit.jl**: è¨˜å·çš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚° â†’ è‡ªå‹•çš„ã«SDEã‚’ç”Ÿæˆ

**Diffusion Modelã¨ã®çµ±åˆ**:
- Candleï¼ˆDLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰ã§Scoreé–¢æ•° $s_\theta(x, t)$ ã‚’è¨“ç·´
- ode_solversã§Reverse-time SDE/PF-ODEã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
- Burnï¼ˆXLAã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼‰ã§GPUé«˜é€ŸåŒ–

### 6.5 SDEæ•°å€¤è§£æ³•ã®é«˜åº¦åŒ–

**é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ï¼ˆç¬¬40å›ã§è©³èª¬ï¼‰**:
- **DPM-Solver++**: PF-ODEã‚’Runge-Kuttaç³»ã§è§£ãã€$O(T^{-2})$åæŸ
- **UniPC**: çµ±ä¸€Predictor-Correctorãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- **EDM**: Elucidating Diffusion Modelsï¼ˆKarras et al. 2022ï¼‰ã€æœ€é©é›¢æ•£åŒ–

**Stochastic Runge-Kuttaæ³•**:
- Euler-Maruyamaã‚’è¶…ãˆã‚‹é«˜æ¬¡SDE solver
- Strong convergence $O(\Delta t^{3/2})$
- ode_solversã§å®Ÿè£…æ¸ˆã¿ï¼ˆ`SRIW1()`, `SRIW2()`ç­‰ï¼‰

> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. SDE â†’ Flow Matching ã¸ã®æ¥ç¶šã«ãŠã„ã¦ã€Fokker-Planckæ–¹ç¨‹å¼ã®é€£ç¶šæ€§æ–¹ç¨‹å¼ã¨ã—ã¦ã®è§£é‡ˆãŒæ¡ä»¶ä»˜ãé€Ÿåº¦å ´ $u_t(\mathbf{x}|\mathbf{x}_1)$ ã®è¨­è¨ˆã«ã©ã†å¯„ä¸ã™ã‚‹ã‹è¿°ã¹ã‚ˆã€‚
> 2. VP-SDEãƒ»VE-SDEãƒ»Sub-VP SDEãƒ»PF-ODEã®4å®šå¼åŒ–ãŒåŒä¸€ã®å‘¨è¾ºåˆ†å¸ƒ $p_t(\mathbf{x})$ ã‚’ç”Ÿæˆã§ãã‚‹æ¡ä»¶ã¨ã€ãã‚Œãã‚Œã®æ•°å€¤è§£æ³•ä¸Šã®æœ‰åˆ©ãªç‚¹ã‚’ä¸€è¡Œãšã¤è¿°ã¹ã‚ˆã€‚

## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 7.1 æœ¬å›ã®ã¾ã¨ã‚ â€” 3ã¤ã®æ ¸å¿ƒ

**æ ¸å¿ƒ1: é›¢æ•£DDPMã®é€£ç¶šæ™‚é–“æ¥µé™ãŒVP-SDE/VE-SDE**
- DDPM $q(x_t | x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t) \mathbf{I})$ â†’ VP-SDE
- NCSNï¼ˆãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\{\sigma_i\}$ï¼‰â†’ VE-SDE
- ç†è«–çš„æ ¹æ‹ ãŒæ˜ç¢ºåŒ–ï¼ˆFokker-Planckæ–¹ç¨‹å¼ã€åæŸæ€§è§£æï¼‰

**æ ¸å¿ƒ2: Reverse-time SDEã¨Probability Flow ODEã§ç”Ÿæˆ**
- Anderson 1982ã®Reverse-time SDE: ç¢ºç‡çš„ç”Ÿæˆ
- Song et al. 2021ã®PF-ODE: æ±ºå®šè«–çš„ç”Ÿæˆ
- åŒã˜å‘¨è¾ºåˆ†å¸ƒ $p_t(x)$ â†’ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã®é¸æŠè‚¢

**æ ¸å¿ƒ3: Score SDEçµ±ä¸€ç†è«–ãŒDDPM/NCSN/DDIMã‚’åŒ…æ‘‚**
- Forward SDEï¼ˆãƒã‚¤ã‚ºæ³¨å…¥ï¼‰
- Reverse-time SDEï¼ˆç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- Probability Flow ODEï¼ˆæ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- Scoreé–¢æ•° $\nabla \log p_t(x)$ ãŒã™ã¹ã¦ã®éµ

### 7.2 Course Iç¬¬5å›ã¨ã®æ¥ç¶š â€” æ—¢ç¿’çŸ¥è­˜ã®æ´»ç”¨

**ç¬¬5å›ã§å­¦ã‚“ã ã“ã¨**:
- Browné‹å‹•ã®å®šç¾©ã¨æ€§è³ªï¼ˆé€£ç¶šæ€§ã€éå¾®åˆ†å¯èƒ½æ€§ã€äºŒæ¬¡å¤‰åˆ†ï¼‰
- ä¼Šè—¤ç©åˆ†ã®å®šç¾©ï¼ˆéäºˆè¦‹æ€§ã€ä¼Šè—¤ç­‰è·é›¢æ€§ï¼‰
- ä¼Šè—¤ã®è£œé¡Œï¼ˆ$dW^2 = dt$ ã®å°å‡ºã€ç¢ºç‡å¾®åˆ†ã®é€£é–å¾‹ï¼‰
- åŸºæœ¬SDEï¼ˆ$dX = f dt + g dW$ ã®å½¢å¼ã€å­˜åœ¨ãƒ»ä¸€æ„æ€§ã®ç›´æ„Ÿï¼‰
- Euler-Maruyamaæ³•ï¼ˆSDEã®æ•°å€¤è§£æ³•åŸºç¤ï¼‰
- Fokker-Planckæ–¹ç¨‹å¼ã®ç›´æ„Ÿ

**æœ¬å›ã§æ·±æ˜ã‚Šã—ãŸã“ã¨**:
- VP-SDE/VE-SDEã®**å³å¯†å°å‡º**ï¼ˆä¼Šè—¤ã®è£œé¡Œã‚’é©ç”¨ï¼‰
- Fokker-Planckæ–¹ç¨‹å¼ã®**å³å¯†å°å‡º**ï¼ˆKramers-Moyalå±•é–‹ï¼‰
- Andersoné€†æ™‚é–“SDEå®šç†ï¼ˆGirsanovå®šç†ã®å¿œç”¨ï¼‰
- Probability Flow ODEï¼ˆé€£ç¶šæ–¹ç¨‹å¼ã¨ã®é–¢ä¿‚ï¼‰
- åæŸæ€§è§£æï¼ˆO(d/T)ã€Manifoldä»®èª¬ï¼‰
- Rust ode_solversã§ã®SDEå®Ÿè£…

**ç¬¬5å›ã®çŸ¥è­˜ãŒæœ¬å›ã§æ´»ãã‚‹ç¬é–“**:
- ä¼Šè—¤ã®è£œé¡Œã§ $dX_t^2$ ã‚’è¨ˆç®— â†’ VP-SDEåˆ†æ•£å°å‡ºï¼ˆ3.3ç¯€ï¼‰
- Fokker-Planckæ–¹ç¨‹å¼ã®ç›´æ„Ÿã‚’å³å¯†åŒ–ï¼ˆ3.6ç¯€ï¼‰
- Euler-Maruyamaæ³•ã‚’å‰æã«Predictor-Correctoræ³•ã¸ç™ºå±•ï¼ˆ3.13ç¯€ï¼‰

### 7.3 æ¬¡å›ï¼ˆç¬¬38å›ï¼‰ã¸ã®æ©‹æ¸¡ã— â€” Flow Matchingçµ±ä¸€ç†è«–

ç¬¬38å›ã€ŒFlow Matching & çµ±ä¸€ç†è«–ã€ã§å­¦ã¶ã“ã¨:
- **Conditional Flow Matching**: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒªãƒ¼è¨“ç·´
- **Optimal Transport ODE**: Rectified Flowï¼ˆç›´ç·šè¼¸é€ï¼‰
- **Stochastic Interpolants**: Flow/Diffusionã®çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- **DiffFlowçµ±ä¸€ç†è«–**: SDM + GANã‚’åŒä¸€SDEè¡¨ç¾
- **Wassersteinå‹¾é…æµ**: JKO schemeã¨Fokker-Planckã®ç­‰ä¾¡æ€§
- **Score â†” Flow â†” Diffusion â†” ODE ã®æ•°å­¦çš„ç­‰ä¾¡æ€§è¨¼æ˜**

**æœ¬å›ã¨ã®æ¥ç¶š**:
- Probability Flow ODE â†’ Flow Matchingã¸ã®è‡ªç„¶ãªæ‹¡å¼µ
- VP-SDE/VE-SDE â†’ ä¸€èˆ¬ç¢ºç‡ãƒ‘ã‚¹ã¸ã®ä¸€èˆ¬åŒ–
- Score SDEçµ±ä¸€ç†è«– â†’ ã•ã‚‰ãªã‚‹çµ±ä¸€ï¼ˆOTçµ±åˆï¼‰

### 7.4 FAQ â€” ã‚ˆãã‚ã‚‹è³ªå•

**Q1: VP-SDEã¨VE-SDEã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ**

A: ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚
- **VP-SDE**: DDPMãƒ™ãƒ¼ã‚¹ã€ç”»åƒç”Ÿæˆã§æ¨™æº–ã€åˆ†æ•£ä¿å­˜ã§æ•°å€¤å®‰å®š
- **VE-SDE**: NCSNãƒ™ãƒ¼ã‚¹ã€ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ãŒæ˜ç¤ºçš„ã€é«˜æ¬¡å…ƒæ½œåœ¨ç©ºé–“
- ç¬¬38å›ã§å­¦ã¶Flow MatchingãŒSDEã®åˆ¶ç´„ã‚’è¶…ãˆã‚‹

**Q2: Probability Flow ODEã®ã€ŒåŒã˜å‘¨è¾ºåˆ†å¸ƒã€ã®æ„å‘³ã¯ï¼Ÿ**

A: å„æ™‚åˆ» $t$ ã§ã®ç¢ºç‡åˆ†å¸ƒ $p_t(x)$ ãŒåŒã˜ã€‚
- Reverse-time SDE: ç¢ºç‡çš„è»Œé“ã€ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«ç•°ãªã‚‹çµŒè·¯
- PF-ODE: æ±ºå®šè«–çš„è»Œé“ã€åˆæœŸå€¤ãŒåŒã˜ãªã‚‰åŒã˜çµŒè·¯
- ã©ã¡ã‚‰ã‚‚å‘¨è¾ºåˆ†å¸ƒ $\{p_t\}_{t \in [0, T]}$ ã¯ä¸€è‡´

**Q3: Euler-Maruyamaæ³•ã§ååˆ†ï¼Ÿé«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ã¯å¿…é ˆï¼Ÿ**

A: ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚
- **Euler-Maruyama**: å®Ÿè£…ç°¡å˜ã€$T = 1000$ ã§ååˆ†ãªç²¾åº¦
- **é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼**: $T = 50$ ã«å‰Šæ¸›å¯èƒ½ã€æ¨è«–é«˜é€ŸåŒ–
- ç¬¬40å›ã§å­¦ã¶DPM-Solver++/UniPCãŒå®Ÿç”¨çš„

**Q4: ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla \log p_t(x)$ ã¯ã©ã†å­¦ç¿’ã™ã‚‹ï¼Ÿ**

A: Denoising Score Matchingï¼ˆç¬¬35å›ï¼‰ã€‚
- ãƒã‚¤ã‚ºä»˜ããƒ‡ãƒ¼ã‚¿ $x_t$ ã‹ã‚‰Score $\nabla \log p_t(x_t)$ ã‚’æ¨å®š
- Neural Network $s_\theta(x, t)$ ã‚’è¨“ç·´
- æœ¬å›ã¯ã€Œå­¦ç¿’æ¸ˆã¿Scoreé–¢æ•°ãŒä¸ãˆã‚‰ã‚ŒãŸã€ã¨ä»®å®š

**Q5: ode_solversã¯å¿…é ˆï¼ŸPyTorchã§å®Ÿè£…ã§ããªã„ï¼Ÿ**

A: PyTorchã§ã‚‚å¯èƒ½ã ãŒã€ode_solversãŒåœ§å€’çš„ã«å¼·åŠ›ã€‚
- PyTorch: è‡ªåŠ›ã§Euler-Maruyamaå®Ÿè£…ã€ã‚½ãƒ«ãƒãƒ¼é¸æŠè‚¢å°‘
- ode_solvers: 40ç¨®ã‚½ãƒ«ãƒãƒ¼ã€è‡ªå‹•ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºèª¿æ•´ã€GPUå¯¾å¿œ
- ç ”ç©¶ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ãªã‚‰Rustã€è«–æ–‡æŸ»èª­ç”¨ãªã‚‰PyTorch

**Q6: Anderson 1982è«–æ–‡ã¯èª­ã‚€ã¹ãï¼Ÿ**

A: ç†è«–æ´¾ãªã‚‰æ¨å¥¨ã€å®Ÿè£…æ´¾ãªã‚‰ä¸è¦ã€‚
- Song et al. 2021ãŒAndersonå®šç†ã‚’ç¾ä»£çš„ã«å†è§£é‡ˆ
- Reverse-time SDEã®å°å‡ºã‚¹ã‚±ãƒƒãƒï¼ˆæœ¬å›3.8ç¯€ï¼‰ã§ååˆ†
- å³å¯†è¨¼æ˜ï¼ˆGirsanovå®šç†ï¼‰ã¯å°‚é–€æ›¸ï¼ˆÃ˜ksendalç­‰ï¼‰å‚ç…§
### 7.6 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] Browné‹å‹•ã®äºŒæ¬¡å¤‰åˆ† $\langle W \rangle_t = t$ ã‚’å°å‡ºã§ãã‚‹
- [ ] ä¼Šè—¤ã®è£œé¡Œã‚’ä½¿ã£ã¦VP-SDEã®å¹³å‡ãƒ»åˆ†æ•£ã‚’å°å‡ºã§ãã‚‹
- [ ] Fokker-Planckæ–¹ç¨‹å¼ã‚’Kramers-Moyalå±•é–‹ã‹ã‚‰å°å‡ºã§ãã‚‹
- [ ] VP-SDE/VE-SDE/Sub-VP SDEã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Andersoné€†æ™‚é–“SDEå®šç†ã‚’è¿°ã¹ã‚‰ã‚Œã‚‹
- [ ] Probability Flow ODEã¨Reverse-time SDEã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Score SDEçµ±ä¸€ç†è«–ã®4è¦ç´ ï¼ˆForward/Reverse/Score/ODEï¼‰ã‚’åˆ—æŒ™ã§ãã‚‹
- [ ] O(d/T)åæŸç†è«–ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Manifoldä»®èª¬ä¸‹ã®ç·šå½¢åæŸã®æ„ç¾©ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Rust ode_solversã§VP-SDEã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Predictor-Correctoræ³•ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã§ãã‚‹

å…¨é …ç›®âœ“ãªã‚‰æ¬¡å›ã¸ï¼æœªé”æˆé …ç›®ã¯è©²å½“Zoneã‚’å¾©ç¿’ã€‚

### 7.7 æ¬¡å›äºˆå‘Š â€” ç¬¬38å›: Flow Matching & çµ±ä¸€ç†è«–

**ç¬¬38å›ã®æ ¸å¿ƒãƒˆãƒ”ãƒƒã‚¯**:
- Conditional Flow Matchingï¼ˆCFMï¼‰å®Œå…¨å°å‡º
- Optimal Transport ODE / Rectified Flowï¼ˆç›´ç·šè¼¸é€ï¼‰
- Stochastic Interpolantsçµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- DiffFlowçµ±ä¸€ç†è«–ï¼ˆSDM + GAN = åŒä¸€SDEï¼‰
- Wassersteinå‹¾é…æµï¼ˆJKO scheme / Fokker-Planckã¨ã®ç­‰ä¾¡æ€§ï¼‰
- **Score â†” Flow â†” Diffusion â†” ODE ã®æ•°å­¦çš„ç­‰ä¾¡æ€§è¨¼æ˜**

**ç¬¬37å›ï¼ˆæœ¬å›ï¼‰ã¨ã®æ¥ç¶š**:
- VP-SDE/VE-SDEã‚’**ä¸€èˆ¬ç¢ºç‡ãƒ‘ã‚¹**ã«æ‹¡å¼µ
- Probability Flow ODE â†’ Flow Matching ODEï¼ˆOptimal Transportçµ±åˆï¼‰
- Score SDE â†’ Flow Matchingçµ±ä¸€ç†è«–ã¸

> **Note:** **é€²æ—: 100%å®Œäº† â€” ç¬¬37å›èª­äº†ï¼**
> SDE/ODE & ç¢ºç‡éç¨‹è«–ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚VP-SDE/VE-SDEå°å‡ºã€Andersoné€†æ™‚é–“SDEã€Probability Flow ODEã€Score SDEçµ±ä¸€ç†è«–ã€åæŸæ€§è§£æã€Rustå®Ÿè£…ã‚’ä¿®å¾—ã€‚æ¬¡å›Flow Matchingã§å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€ç†è«–ã¸ã€‚

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**"é›¢æ•£ã‚¹ãƒ†ãƒƒãƒ—æ•° $T = 1000$ ã¯çµŒé¨“å‰‡ã€‚é€£ç¶šæ™‚é–“SDEã§ç†è«–åŒ–ã—ãŸã¨ãã€åˆã‚ã¦ã€Œãªãœ1000ã§ååˆ†ã‹ã€ã«ç­”ãˆã‚‰ã‚Œã‚‹ã€‚ç†è«–ãªãå®Ÿè£…ã¯æš—é—‡ã®èˆªæµ·ã§ã¯ï¼Ÿ"**

**è­°è«–ãƒã‚¤ãƒ³ãƒˆ**:
1. DDPMã®æˆåŠŸï¼ˆ2020ï¼‰ã¯çµŒé¨“çš„ã€‚ç†è«–çš„æ­£å½“åŒ–ï¼ˆScore SDEçµ±ä¸€ç†è«–ã€2021ï¼‰ã¯å¾Œè¿½ã„ã€‚å®Ÿå‹™ã§ã¯ã€Œå‹•ã‘ã°OKã€ã‹ã€ç†è«–çš„ç†è§£ã¯å¿…é ˆã‹ï¼Ÿ
2. O(d/T)åæŸç†è«–ï¼ˆ2024ï¼‰ã§ã€Œ$T = 1000$ ãŒååˆ†ãªç†ç”±ã€ãŒæ•°å­¦çš„ã«èª¬æ˜ã•ã‚ŒãŸã€‚ã ãŒå®Ÿè£…è€…ã®ä½•%ãŒã“ã‚Œã‚’çŸ¥ã‚‹ã¹ãã‹ï¼Ÿ
3. Probability Flow ODEã®ç™ºè¦‹ï¼ˆSong et al. 2021ï¼‰ã¯SDEã®é€£ç¶šæ™‚é–“å®šå¼åŒ–ãªã—ã«ã¯ä¸å¯èƒ½ã ã£ãŸã€‚é€£ç¶šç†è«–ãŒæ–°æ‰‹æ³•ã‚’ç”Ÿã‚€ä¾‹ã€‚ç†è«– vs å®Ÿè£…ã€ã©ã¡ã‚‰ãŒå…ˆã‹ï¼Ÿ

<details><summary>æ­´å²çš„æ–‡è„ˆ â€” SDEã¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®40å¹´ã‚®ãƒ£ãƒƒãƒ—</summary>

**Anderson 1982**: Reverse-time SDEã‚’è¨¼æ˜ã€‚å½“æ™‚ã¯ç†è«–çš„èˆˆå‘³ã®ã¿ã€å¿œç”¨ãªã—ã€‚

**2015 Sohl-Dickstein et al.**: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«åˆææ¡ˆã€‚Andersonã‚’å¼•ç”¨ã›ãšï¼ˆç‹¬ç«‹ã«ç™ºè¦‹ï¼‰ã€‚

**2020 Ho et al. DDPM**: é›¢æ•£æ™‚é–“å®šå¼åŒ–ã§å¤§æˆåŠŸã€‚SDEã¨ã®æ¥ç¶šã¯æ˜ç¤ºã›ãšã€‚

**2021 Song et al. Score SDE**: 40å¹´å‰ã®Andersonå®šç†ã‚’å†ç™ºè¦‹ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¨SDEçµ±ä¸€ã€‚Probability Flow ODEç™ºè¦‹ã€‚

**2024-2025 åæŸç†è«–**: Li & Yanã€Potaptchik et al.ãŒO(d/T)ã€Manifoldç·šå½¢åæŸã‚’è¨¼æ˜ã€‚ç†è«–ãŒå®Ÿè£…ã‚’é€†ç…§å°„ã€‚

**æ•™è¨“**: ç†è«–ã¨å®Ÿè£…ã®å¯¾è©±ãŒæ–°ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ç”Ÿã‚€ã€‚40å¹´ã®æ™‚ã‚’çµŒã¦ç†è«–ãŒå®Ÿè£…ã«å…‰ã‚’å½“ã¦ã‚‹ã€‚

</details>

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole (2021). "Score-Based Generative Modeling through Stochastic Differential Equations". *ICLR 2021 (Oral)*.
<https://arxiv.org/abs/2011.13456>

[^2]: Brian D. O. Anderson (1982). "Reverse-time diffusion equation models". *Stochastic Processes and their Applications*, vol. 12, pp. 313-326.
<https://www.sciencedirect.com/science/article/pii/0304414982900515>

[^3]: Gen Li and Yuling Yan (2024). "O(d/T) Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions". *arXiv preprint*.
<https://arxiv.org/abs/2409.18959>

[^4]: Peter Potaptchik, Iskander Azangulov, and George Deligiannidis (2024). "Linear Convergence of Diffusion Models Under the Manifold Hypothesis". *arXiv preprint*.
<https://arxiv.org/abs/2410.09046>

[^5]: Choi, J. & Fan, C. (2025). "Diffusion Models under Alternative Noise: Simplified Analysis and Sensitivity". *arXiv preprint*.
<https://arxiv.org/abs/2506.08337>

[^6]: Jonathan Ho, Ajay Jain, and Pieter Abbeel (2020). "Denoising Diffusion Probabilistic Models". *NeurIPS 2020*.
<https://arxiv.org/abs/2006.11239>

[^7]: Alex Nichol and Prafulla Dhariwal (2021). "Improved Denoising Diffusion Probabilistic Models". *ICML 2021*.
<https://arxiv.org/abs/2102.09672>

[^8]: Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli (2015). "Deep Unsupervised Learning using Nonequilibrium Thermodynamics". *ICML 2015*.
<https://arxiv.org/abs/1503.03585>

[^9]: Jiaming Song, Chenlin Meng, and Stefano Ermon (2020). "Denoising Diffusion Implicit Models". *ICLR 2021*.
<https://arxiv.org/abs/2010.02502>

[^10]: Yang Song and Stefano Ermon (2020). "Improved Techniques for Training Score-Based Generative Models". *NeurIPS 2020*.
<https://arxiv.org/abs/2006.09011>

### æ•™ç§‘æ›¸

- Bernt Ã˜ksendal (2003). *Stochastic Differential Equations: An Introduction with Applications* (6th ed.). Springer.
- Peter E. Kloeden and Eckhard Platen (1992). *Numerical Solution of Stochastic Differential Equations*. Springer.
- Olav Kallenberg (2002). *Foundations of Modern Probability* (2nd ed.). Springer.

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- Yang Song (2021). "Generative Modeling by Estimating Gradients of the Data Distribution". [Blog Post](https://yang-song.net/blog/2021/score/)
- MIT 6.S184 (2026). "Diffusion Models & Flow Matching". [Course Website](https://diffusion.csail.mit.edu/)
- ode_solvers Documentation. [docs.sciml.ai](https://docs.sciml.ai/DiffEqDocs/stable/)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
