---
title: "ç¬¬33å›: Normalizing Flowsã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨: å®Ÿè£…â†’å®Ÿé¨“â†’ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning"]
published: true
slug: "ml-lecture-33-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---
## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rust/Rustã§Flowã‚’æ›¸ã

**ã‚´ãƒ¼ãƒ«**: RealNVP/Glow/CNFã®å®Ÿè£…åŠ›ã‚’èº«ã«ã¤ã‘ã‚‹ã€‚

### 4.1 Rust Flowå®Ÿè£…ã®å…¨ä½“è¨­è¨ˆ

**ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ§‹æˆ**:

```rust
// Normalizing Flows in Rust
// candle-core: ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®— (GPU-ready, å‹å®‰å®š)
// candle-nn:   ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆå±¤ (Dense, Sequential â€¦)
// ODE:         æ‰‹å®Ÿè£… Euler / Runge-Kutta (CNFç”¨)
use candle_core::{Tensor, DType, Device};
use candle_nn::{Module, VarBuilder, VarMap, AdamW, ParamsAdamW};
use ndarray::{Array1, Array2, ArrayView2, Axis, s};
use rand::Rng;
use rand_distr::StandardNormal;
```

**Luxé¸æŠç†ç”±**: Immutable (functional) â†’ å‹å®‰å®šæ€§ â†’ Burn GPU AOT â†’ Production-readyã€‚

> **âš ï¸ Warning:** Lux ã® `ps`ï¼ˆparametersï¼‰ã¨ `st`ï¼ˆstatesï¼‰ã‚’æ··åŒã—ãªã„ã“ã¨ã€‚`ps` ã¯è¨“ç·´ã§æ›´æ–°ã•ã‚Œã‚‹é‡ã¿ã€`st` ã¯ BatchNorm çµ±è¨ˆãªã©ã®çŠ¶æ…‹ï¼ˆè¨“ç·´ä¸­ã¨æ¨è«–æ™‚ã§å‹•ä½œãŒç•°ãªã‚‹ï¼‰ã€‚Candle ã‹ã‚‰ Candle ã¸ç§»è¡Œã™ã‚‹éš›ã®æœ€å¤§ã®è½ã¨ã—ç©´ã€‚

### 4.2 Coupling Layerå®Ÿè£…

```rust
use ndarray::{Array2, ArrayView2, Axis, concatenate, s};

// Affine Coupling Layer â€” zero-copy input via ArrayView2
fn affine_coupling_forward(
    z: ArrayView2<f32>,                              // [D, B]
    s_net: impl Fn(ArrayView2<f32>) -> Array2<f32>,  // scale net
    t_net: impl Fn(ArrayView2<f32>) -> Array2<f32>,  // translation net
    d: usize,  // split: identity part is z[0..d, :]
) -> (Array2<f32>, Vec<f32>) {
    let z1 = z.slice(s![..d, ..]);   // identity part â€” zero-copy view
    let z2 = z.slice(s![d.., ..]);   // transform part

    // Compute scale & translation from z1
    let s = s_net(z1);
    let t = t_net(z1);

    // Affine transformation: x2 = z2 * exp(s) + t
    let x2: Array2<f32> = &z2 * &s.mapv(f32::exp) + &t;
    let x = concatenate(Axis(0), &[z1, x2.view()]).unwrap();

    // log|det J| = Î£áµ¢ sáµ¢ per sample
    // ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒä¸‹ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯æ§‹é€  â†’ å¯¾è§’æˆåˆ† exp(sáµ¢) ã®ç©ãŒè¡Œåˆ—å¼
    // â†’ O(D)  (è¡Œåˆ—å¼ã®ç›´æ¥è¨ˆç®— O(DÂ³) ã§ã¯ãªã„)
    let log_det: Vec<f32> = s.sum_axis(Axis(0)).into_raw_vec();
    (x, log_det)
}

// Inverse coupling: x â†’ z
fn affine_coupling_inverse(
    x: ArrayView2<f32>,
    s_net: impl Fn(ArrayView2<f32>) -> Array2<f32>,
    t_net: impl Fn(ArrayView2<f32>) -> Array2<f32>,
    d: usize,
) -> (Array2<f32>, Vec<f32>) {
    let x1 = x.slice(s![..d, ..]);
    let x2 = x.slice(s![d.., ..]);

    let s = s_net(x1);
    let t = t_net(x1);

    // z2 = (x2 - t) * exp(-s)
    let z2: Array2<f32> = (&x2 - &t) * &s.mapv(|v| (-v).exp());
    let z = concatenate(Axis(0), &[x1, z2.view()]).unwrap();

    // Inverse log-det: -Î£áµ¢ sáµ¢
    let log_det: Vec<f32> = s.sum_axis(Axis(0)).mapv(|v| -v).into_raw_vec();
    (z, log_det)
}
```

### 4.3 RealNVP Stack

```rust
use candle_core::{Tensor, DType, Device};
use candle_nn::{Module, Sequential, VarBuilder, Activation, linear};

// RealNVP coupling layer: s-net + t-net + split index d
struct CouplingLayer {
    s_net: Sequential,
    t_net: Sequential,
    d: usize,  // split point (identity part: 0..d)
}

// RealNVP: stack of alternating affine coupling layers
struct RealNVP {
    layers: Vec<CouplingLayer>,
}

impl RealNVP {
    fn new(in_dim: usize, hidden_dim: usize, n_layers: usize, vb: &VarBuilder)
        -> candle_core::Result<Self>
    {
        let layers = (0..n_layers).map(|i| {
            // Alternate split so every dimension gets transformed
            let d = if i % 2 == 0 { in_dim / 2 } else { in_dim - in_dim / 2 };
            let out_dim = in_dim - d;
            let mk_net = |prefix: &str| -> candle_core::Result<Sequential> {
                Ok(candle_nn::seq()
                    .add(linear(d, hidden_dim, vb.pp(format!("{prefix}.0")))?)
                    .add(Activation::Tanh)
                    .add(linear(hidden_dim, hidden_dim, vb.pp(format!("{prefix}.1")))?)
                    .add(Activation::Tanh)
                    .add(linear(hidden_dim, out_dim, vb.pp(format!("{prefix}.2")))?))
            };
            Ok(CouplingLayer {
                s_net: mk_net(&format!("layer{i}_s"))?,
                t_net: mk_net(&format!("layer{i}_t"))?,
                d,
            })
        }).collect::<candle_core::Result<Vec<_>>>()?;
        Ok(Self { layers })
    }

    // Forward: z â†’ x,  log p(x) = log p(z) + Î£áµ¢ log|det Jáµ¢|
    fn forward(&self, z: &Tensor) -> candle_core::Result<(Tensor, Tensor)> {
        let mut x = z.clone();
        let mut log_det = Tensor::zeros(z.dims()[1], DType::F32, z.device())?;
        for layer in &self.layers {
            let (x_new, ldj) = coupling_forward(&x, &layer.s_net, &layer.t_net, layer.d)?;
            log_det = (&log_det + &ldj)?;  // log|det J| += Î£ sáµ¢  per sample
            x = x_new;
        }
        Ok((x, log_det))
    }

    // Inverse: x â†’ z  (fâ»Â¹: layers in reverse, log|det Jâ»Â¹| = -Î£ log|det Jáµ¢|)
    fn inverse(&self, x: &Tensor) -> candle_core::Result<(Tensor, Tensor)> {
        let mut z = x.clone();
        let mut log_det = Tensor::zeros(x.dims()[1], DType::F32, x.device())?;
        for layer in self.layers.iter().rev() {
            let (z_new, ldj) = coupling_inverse(&z, &layer.s_net, &layer.t_net, layer.d)?;
            log_det = (&log_det + &ldj)?;  // log|det Jâ»Â¹| += -Î£ sáµ¢  per sample
            z = z_new;
        }
        Ok((z, log_det))
    }
}
```

### 4.4 è¨“ç·´ãƒ«ãƒ¼ãƒ—

```rust
use candle_core::{Tensor, DType};
use candle_nn::Optimizer;

// Negative log-likelihood: NLL = -E[log p(x)]
// log p(x) = log p_z(fâ»Â¹(x)) + log|det J_{fâ»Â¹}|,   z = fâ»Â¹(x) ~ N(0, I)
fn nll_loss(model: &RealNVP, x_batch: &Tensor) -> candle_core::Result<Tensor> {
    // z = fâ»Â¹(x),  log|det Jâ»Â¹| accumulated over layers
    let (z, log_det_sum) = model.inverse(x_batch)?;

    // log p(z) = -Â½ Î£áµ¢ záµ¢Â²  (drop constant -D/2Â·log 2Ï€; cancelled in comparison)
    // = Î£áµ¢ log ğ’©(záµ¢; 0,1)  (factored standard Gaussian)
    let log_pz = (z.sqr()?.sum(0)? * -0.5)?;

    // log p(x) = log p_z(z) + log|det Jâ»Â¹|   (change-of-variables)
    let log_px = (&log_pz + &log_det_sum)?;

    // NLL = -mean(log p(x))   (minimise â†’ maximise likelihood)
    log_px.mean_all()?.neg()
}

// Training loop
fn train_realnvp(
    model: &RealNVP,
    opt: &mut impl Optimizer,
    data: &Tensor,        // [D, N]
    n_epochs: usize,
    batch_size: usize,
) -> candle_core::Result<()> {
    let n_samples = data.dims()[1];
    for epoch in 0..n_epochs {
        let mut epoch_loss = 0f64;
        let mut n_batches = 0usize;

        for start in (0..n_samples).step_by(batch_size) {
            let end = (start + batch_size).min(n_samples);
            let x_batch = data.narrow(1, start, end - start)?;
            let loss = nll_loss(model, &x_batch)?;
            opt.backward_step(&loss)?;

            epoch_loss += loss.to_scalar::<f32>()? as f64;
            n_batches += 1;
        }

        if (epoch + 1) % 10 == 0 {
            println!("Epoch {}: NLL = {:.4}", epoch + 1, epoch_loss / n_batches as f64);
        }
    }
    Ok(())
}
```

### 4.5 CNF/FFJORDå®Ÿè£…

```rust
use candle_core::{Tensor, DType};
use rand_distr::StandardNormal;

// CNF: instantaneous change of variables via Hutchinson trace estimator
// Augmented state u = [z; log_det],  d/dt u = [f(z,t); -tr(âˆ‚f/âˆ‚z)]

// ãªãœ Hutchinson ãŒåŠ¹ãã‹:
// E[Îµ^T A Îµ] = E[Î£áµ¢â±¼ Îµáµ¢ Aáµ¢â±¼ Îµâ±¼] = Î£áµ¢ Aáµ¢áµ¢ E[Îµáµ¢Â²] = tr(A)  (âˆµ E[Îµáµ¢Îµâ±¼]=Î´áµ¢â±¼)
// è¨ˆç®—é‡: ç›´æ¥ O(DÂ²) â†’ Hutchinson O(D) (VJP 1å›ã®ã¿)

// One Euler step of the CNF augmented ODE
fn cnf_step(
    z: &Tensor,            // [D, 1] â€” current state
    log_det: &Tensor,      // scalar â€” accumulated log|det J|
    f_net: &impl Module,   // velocity field
    dt: f64,
    rng: &mut impl Rng,
) -> candle_core::Result<(Tensor, Tensor)> {
    let d = z.elem_count();

    // Velocity: dz/dt = f(z, t)
    let dz = f_net.forward(z)?;

    // Hutchinson trace estimator: tr(âˆ‚f/âˆ‚z) â‰ˆ Îµ^T (âˆ‚f/âˆ‚z) Îµ,  Îµ ~ N(0,I)
    let eps_vals: Vec<f32> = (0..d).map(|_| rng.sample::<f32, _>(StandardNormal)).collect();
    let eps = Tensor::from_slice(&eps_vals, (d, 1), z.device())?;
    // Simplified scalar estimate: Îµ^T dz  (full impl uses reverse-mode AD for JVP)
    let tr_jac = eps.mul(&dz)?.sum_all()?;

    // d(log_det)/dt = -tr(âˆ‚f/âˆ‚z)
    let new_z       = (z + &((&dz * dt)?)?)?;
    let new_log_det = (log_det - &(tr_jac * dt)?)?;
    Ok((new_z, new_log_det))
}

// Solve CNF with Euler integrator over [t0, t1]
fn solve_cnf(
    f_net: &impl Module,
    z0: &Tensor,    // [D, 1]
    t0: f64, t1: f64,
    n_steps: usize,
    rng: &mut impl Rng,
) -> candle_core::Result<(Tensor, Tensor)> {
    let dt = (t1 - t0) / n_steps as f64;
    let mut z = z0.clone();
    let mut log_det = Tensor::zeros((), DType::F32, z0.device())?;  // log_det_jac = 0 initially

    for _ in 0..n_steps {
        (z, log_det) = cnf_step(&z, &log_det, f_net, dt, rng)?;
    }
    Ok((z, log_det))
}
```

### 4.6 Rustæ¨è«–å®Ÿè£…

Rustå´ã¯è¨“ç·´æ¸ˆã¿ONNXãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§æ¨è«–ã€‚

```rust
// Affine Coupling Layer in Rust
pub struct AffineCouplingLayer {
    split_dim: usize,
    s_weights: Vec<Vec<f32>>,  // simplified: full ONNX would use ort
    t_weights: Vec<Vec<f32>>,
}

impl AffineCouplingLayer {
    pub fn forward(&self, z: &[f32]) -> (Vec<f32>, f32) {
        let d = self.split_dim;
        let (z1, z2) = z.split_at(d);

        // Compute scale & translation (simplified MLP)
        let s = self.mlp_forward(&self.s_weights, z1);
        let t = self.mlp_forward(&self.t_weights, z1);

        // xâ‚‚ = zâ‚‚âŠ™exp(s(zâ‚)) + t(zâ‚)   (Affine coupling: zâ†’x)
        // log|det J| = Î£áµ¢ sáµ¢  (diagonal Jacobian â†’ O(D), not O(DÂ³))
        let x2: Vec<f32> = z2.iter().zip(s.iter()).zip(t.iter())
            .map(|((z2i, si), ti)| z2i * si.exp() + ti)  // xâ‚‚áµ¢ = zâ‚‚áµ¢Â·exp(sáµ¢) + táµ¢
            .collect::<Vec<_>>();
        let mut x = z1.to_vec();
        x.extend(x2);

        let log_det_jac: f32 = s.iter().sum();  // log|det J| = Î£ sáµ¢

        (x, log_det_jac)
    }

    fn mlp_forward(&self, weights: &[Vec<f32>], input: &[f32]) -> Vec<f32> {
        // Simplified: 2-layer MLP with tanh
        // Full implementation would use ONNX Runtime
        input.to_vec()  // placeholder
    }
}

// impl Flow trait: forward (zâ†’x), inverse (xâ†’z), log_prob, sample
// RealNVP inference
pub struct RealNVP {
    layers: Vec<AffineCouplingLayer>,
    dim: usize,
}

impl RealNVP {
    pub fn sample(&self, rng: &mut impl Rng) -> Vec<f32> {
        // z ~ N(0, I) â†’ x = f(z)
        let z: Vec<f32> = (0..self.dim).map(|_| rng.sample(StandardNormal)).collect();
        self.forward(&z).0  // x = f(z)
    }

    pub fn log_prob(&self, x: &[f32]) -> f32 {
        // log p(x) = log p_z(fâ»Â¹(x)) + log|det J_{fâ»Â¹}(x)|
        let (z, log_det_jac) = self.inverse(x);  // z = fâ»Â¹(x), log|det Jâ»Â¹|

        // log p(z) = -Â½ Î£áµ¢ (záµ¢Â² + log 2Ï€)  (standard Gaussian)
        let log_pz: f32 = z.iter()
            .map(|zi| -0.5 * (zi * zi + (2.0 * std::f32::consts::PI).ln()))
            .sum();

        log_pz + log_det_jac  // log p(x) = log p_z(z) + log|det Jâ»Â¹|
    }

    fn forward(&self, z: &[f32]) -> (Vec<f32>, f32) {
        // x = f(z): apply coupling layers in order, log|det J| = Î£áµ¢ log|det Jáµ¢|
        self.layers.iter().fold((z.to_vec(), 0.0f32), |(x, sum), layer| {
            let (x_new, ldj) = layer.forward(&x);  // xáµ¢â‚Šâ‚ = fáµ¢(xáµ¢), log|det Jáµ¢|
            (x_new, sum + ldj)                      // accumulate Î£ log|det Jáµ¢|
        })
    }

    fn inverse(&self, x: &[f32]) -> (Vec<f32>, f32) {
        // z = fâ»Â¹(x): apply inverse layers in reverse order
        let mut z = x.to_vec();
        let mut log_det_sum = 0.0;

        for layer in self.layers.iter().rev() {
            // z = fâ»Â¹(x): zâ‚‚ = (xâ‚‚ - t(xâ‚)) âŠ™ exp(-s(xâ‚)), log|det Jâ»Â¹| = -Î£ sáµ¢
            // z = layer.inverse(&z);
            // log_det_sum += ldj;
        }

        (z, log_det_sum)
    }
}
```

### 4.7 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨

| æ•°å¼ | Rust | Rust |
|:-----|:------|:-----|
| $\log p(x) = \log p(z) - \log \|\det J\|$ | `logpdf(base_dist, z) - log_det_jac` | `log_pz - log_det_jac` |
| $x_2 = z_2 \odot \exp(s) + t$ | `z2 .* exp.(s) .+ t` | `z2[i] * s[i].exp() + t[i]` |
| $\log \|\det J\| = \sum s_i$ | `sum(s)` | `s.iter().sum()` |
| $\text{tr}(A) = \mathbb{E}[\epsilon^T A \epsilon]$ | `dot(Îµ, jvp)` | - (training only) |

**shape è¿½è·¡ã‚µãƒãƒªãƒ¼**:
- Coupling forward: $z \in \mathbb{R}^{D \times B} \to (x \in \mathbb{R}^{D \times B},\ \text{ldj} \in \mathbb{R}^B)$
- NLL loss: $x \in \mathbb{R}^{D \times B} \to z \in \mathbb{R}^{D \times B} \to \log p_z \in \mathbb{R}^B \to \text{NLL} \in \mathbb{R}$
- å…¨ $B$ ã‚µãƒ³ãƒ—ãƒ«ã§å¹³å‡ â†’ ã‚¹ã‚«ãƒ©ãƒ¼ loss

> **Note:** **é€²æ—: 70% å®Œäº†** Rust/Rustå®Ÿè£…å®Œäº†ã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ â€” 2D/MNISTè¨“ç·´ãƒ»è©•ä¾¡ã€‚

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” Flowã®è¨“ç·´ã¨è©•ä¾¡

**ã‚´ãƒ¼ãƒ«**: 2D toy dataset / MNIST ã§Flowã‚’è¨“ç·´ã—ã€æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚

### 5.1 2D Toy Dataset: Two Moons

#### 5.1.1 ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

```rust
use ndarray::Array2;
use rand::Rng;
use rand_distr::StandardNormal;
use std::f64::consts::PI;

// Generate Two Moons dataset: two interleaved half-circles
fn generate_two_moons(n_samples: usize, noise: f64, rng: &mut impl Rng) -> Array2<f32> {
    let n_per_moon = n_samples / 2;

    // Upper moon: Î¸ âˆˆ [0, Ï€]
    let upper: Vec<[f32; 2]> = (0..n_per_moon).map(|i| {
        let theta = PI * i as f64 / (n_per_moon - 1) as f64;
        let nx: f64 = rng.sample::<f64, _>(StandardNormal);
        let ny: f64 = rng.sample::<f64, _>(StandardNormal);
        [(theta.cos() + noise * nx) as f32,
         (theta.sin() + noise * ny) as f32]
    }).collect();

    // Lower moon: shifted by (1, 0.5)
    let lower: Vec<[f32; 2]> = (0..n_per_moon).map(|i| {
        let theta = PI * i as f64 / (n_per_moon - 1) as f64;
        let nx: f64 = rng.sample::<f64, _>(StandardNormal);
        let ny: f64 = rng.sample::<f64, _>(StandardNormal);
        [(1.0 - theta.cos() + noise * nx) as f32,
         (0.5 - theta.sin() + noise * ny) as f32]
    }).collect();

    // Stack into (2, n_samples)
    let mut data = Array2::<f32>::zeros((2, n_samples));
    for (i, pt) in upper.iter().chain(lower.iter()).enumerate() {
        data[[0, i]] = pt[0];
        data[[1, i]] = pt[1];
    }
    data
}

let mut rng = rand::thread_rng();
let data = generate_two_moons(1000, 0.1, &mut rng);
// data: shape [2, 1000] â€” ready for RealNVP training
println!("Two Moons: shape = {:?}", data.shape());
```

#### 5.1.2 RealNVPè¨“ç·´

```rust
use candle_core::{Device, DType, Tensor};
use candle_nn::{VarMap, VarBuilder, AdamW, ParamsAdamW};

let device = Device::Cpu;
let in_dim    = 2usize;
let hidden_dim = 64usize;
let n_layers   = 8usize;

// Build RealNVP â€” VarMap owns all parameters
let var_map = VarMap::new();
let vb = VarBuilder::from_varmap(&var_map, DType::F32, &device);
let model = RealNVP::new(in_dim, hidden_dim, n_layers, &vb)?;

// AdamW optimizer (lr=1e-3)
let mut opt = AdamW::new(
    var_map.all_vars(),
    ParamsAdamW { lr: 1e-3, ..Default::default() },
)?;

// Convert ndarray data to Candle tensor [2, N]
let data_tensor = Tensor::from_slice(
    data.as_slice().unwrap(),
    (in_dim, data.ncols()),
    &device,
)?;

// Train 500 epochs, batch_size=256
train_realnvp(&model, &mut opt, &data_tensor, 500, 256)?;
```

Output:
```
Epoch 10: NLL = 2.1542
Epoch 20: NLL = 1.8765
...
Epoch 500: NLL = 1.2341
```

**NLL ã®ä¸‹ç•Œ**: 2D ã‚¬ã‚¦ã‚¹æ··åˆï¼ˆTwo Moonsï¼‰ã®çœŸã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯ $H \approx 1.0$ï¼ˆnatsï¼‰ã€‚NLL=1.23 ã¯ã“ã‚Œã«è¿‘ã„ â†’ å¯†åº¦æ¨å®šãŒã»ã¼åæŸã€‚NLL ãŒã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’å¤§ããä¸‹å›ã‚‹ã“ã¨ã¯ãªã„ï¼ˆãªãœãªã‚‰ $-\mathbb{E}_{p_\text{data}}[\log p_\theta(x)] \geq H(p_\text{data})$ ã¯æˆã‚Šç«‹ãŸãªã„ãŸã‚ï¼‰ã€‚NLL < $H$ ã«ãªã£ãŸã‚‰ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã‹è¨ˆç®—ãƒã‚°ã‚’ç–‘ã†ã“ã¨ã€‚

#### 5.1.3 ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«å¯è¦–åŒ–

```rust
use candle_core::Tensor;
use rand_distr::StandardNormal;

// Sample from trained model: z ~ N(0,I) â†’ x = f(z)
let n_samples = 1000usize;
let z_vals: Vec<f32> = (0..2 * n_samples)
    .map(|_| rng.sample::<f32, _>(StandardNormal))
    .collect();
let z_samples = Tensor::from_slice(&z_vals, (2, n_samples), &device)?;
let (x_samples, _) = model.forward(&z_samples)?;

// x_samples: [2, 1000] â€” use a plotting crate (e.g., plotters) for visualization
println!("Generated {} samples from RealNVP", n_samples);
```

#### 5.1.4 å¯†åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—

```rust
use ndarray::Array2;
use candle_core::Tensor;

// Evaluate log p(x) on a 2D grid via RealNVP inverse
let nx = 100usize;
let ny = 100usize;
let x_range: Vec<f32> = (0..nx).map(|i| -2.0 + 5.0 * i as f32 / (nx - 1) as f32).collect();
let y_range: Vec<f32> = (0..ny).map(|j| -1.5 + 3.5 * j as f32 / (ny - 1) as f32).collect();
let mut log_px_grid = Array2::<f32>::zeros((ny, nx));

for (i, &xv) in x_range.iter().enumerate() {
    for (j, &yv) in y_range.iter().enumerate() {
        // Point as [D=2, B=1] tensor
        let point = Tensor::from_slice(&[xv, yv], (2, 1), &device)?;
        let (z, ldj) = model.inverse(&point)?;
        // log p(z) = -Î£áµ¢ záµ¢Â²/2  (Gaussian, drop constant)
        let log_pz: f32 = z.sqr()?.sum_all()?.to_scalar::<f32>()? * -0.5;
        let ldj_val: f32 = ldj.to_scalar::<f32>()?;
        log_px_grid[[j, i]] = log_pz + ldj_val;
    }
}
// log_px_grid: [100, 100] â€” pass to plotters for heatmap visualization
```

### 5.2 MNIST: Tiny RealNVP

#### 5.2.1 ãƒ‡ãƒ¼ã‚¿æº–å‚™

```rust
use rand::Rng;

// Load MNIST (e.g., via the `mnist` crate or burn-dataset)
// Flatten: (N, 28*28) â†’ (784, N)  then dequantize + logit-transform

// Why logit transform:
// MNIST âˆˆ [0,1] ã¯æœ‰ç•ŒåŒºé–“ â†’ Gaussian base åˆ†å¸ƒã¨ä¸æ•´åˆ
// logit(x) = log(x/(1-x)) ã§ [0,1] â†’ â„ ã«å¤‰æ› â†’ Gaussian ã«è¿‘ä¼¼
// Î±=0.05: dequantization ã§ [Î±, 1-Î±] ã«ã‚¯ãƒªãƒƒãƒ— â†’ log(0)=âˆ’âˆ ã‚’é˜²ã
fn logit_transform(x: &Array2<f32>, alpha: f32, rng: &mut impl Rng) -> Array2<f32> {
    x.mapv(|v| {
        // Dequantize: add Uniform(0, Î±) noise
        let v_dq = (v + alpha * rng.gen::<f32>()).clamp(alpha, 1.0 - alpha);
        (v_dq / (1.0 - v_dq)).ln()   // logit
    })
}

// --- MNIST loading (pseudo-code, depends on chosen crate) ---
// let mnist = Mnist::new("data/")?;
// let train_x: Array2<f32> = flatten_images(&mnist.train_images);  // (784, 60000)
// let test_x:  Array2<f32> = flatten_images(&mnist.test_images);   // (784, 10000)
// let mut rng = rand::thread_rng();
// let train_x_trans = logit_transform(&train_x, 0.05, &mut rng);
// let test_x_trans  = logit_transform(&test_x,  0.05, &mut rng);
```

#### 5.2.2 Tiny RealNVPè¨“ç·´

```rust
use candle_core::{Tensor, DType, Device};
use candle_nn::{VarMap, VarBuilder, AdamW, ParamsAdamW};

// MNIST RealNVP: 784-dim input, 256 hidden, 12 coupling layers
let var_map_mnist = VarMap::new();
let vb_mnist = VarBuilder::from_varmap(&var_map_mnist, DType::F32, &device);
let model_mnist = RealNVP::new(784, 256, 12, &vb_mnist)?;

// AdamW optimizer (lr=1e-4)
let mut opt_mnist = AdamW::new(
    var_map_mnist.all_vars(),
    ParamsAdamW { lr: 1e-4, ..Default::default() },
)?;

// Convert ndarray data to Candle tensor [784, N]
let train_tensor = Tensor::from_slice(
    train_x_trans.as_slice().unwrap(),
    (784, train_x_trans.ncols()),
    &device,
)?;

// Train 20 epochs, batch_size=128
train_realnvp(&model_mnist, &mut opt_mnist, &train_tensor, 20, 128)?;
```

#### 5.2.3 ç”Ÿæˆç”»åƒ

```rust
use candle_core::Tensor;
use rand_distr::StandardNormal;

// Sample from trained MNIST model: z ~ N(0,I) â†’ x = f(z)
let n_samples_img = 16usize;
let z_vals: Vec<f32> = (0..784 * n_samples_img)
    .map(|_| rng.sample::<f32, _>(StandardNormal))
    .collect();
let z_img = Tensor::from_slice(&z_vals, (784, n_samples_img), &device)?;
let (x_img, _) = model_mnist.forward(&z_img)?;  // [784, 16]

// Inverse logit: sigmoid maps â„ â†’ (0,1) to recover pixel values
let x_img_sigmoid = candle_nn::ops::sigmoid(&x_img)?;
// Reshape to [16, 1, 28, 28] for image display (use an image crate like `image`)
let x_img_grid = x_img_sigmoid.t()?.reshape((n_samples_img, 1, 28, 28))?;
println!("Generated {} MNIST images: {:?}", n_samples_img, x_img_grid.dims());
```

### 5.3 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### 5.3.1 ç†è«–ãƒã‚§ãƒƒã‚¯

<details><summary>**Q1: Change of Variableså…¬å¼**</summary>

> $X = f(Z)$, $f$ å¯é€†ã€‚$p_X(x)$ ã‚’ $p_Z$ ã¨ $f$ ã§è¡¨ã›ã€‚

**è§£ç­”**: $p_X(x) = p_Z(f^{-1}(x)) \left| \det \frac{\partial f^{-1}}{\partial x} \right| = p_Z(z) \left| \det \frac{\partial f}{\partial z} \right|^{-1}$

</details>

<details><summary>**Q2: Coupling Layerãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**</summary>

> $x_{1:d} = z_{1:d}$, $x_{d+1:D} = z_{d+1:D} \odot \exp(s(z_{1:d})) + t(z_{1:d})$ã€‚$\log |\det J|$ = ?

**è§£ç­”**: $\log |\det J| = \sum_{i=1}^{D-d} s_i(z_{1:d})$ (ä¸‹ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã®ç©)

</details>

<details><summary>**Q3: CNFå¯†åº¦å¤‰åŒ–**</summary>

> $\frac{dz}{dt} = f(z, t)$ã€‚$\frac{\partial \log p(z(t))}{\partial t}$ = ?

**è§£ç­”**: $\frac{\partial \log p(z(t))}{\partial t} = -\text{tr}\left(\frac{\partial f}{\partial z}\right)$ (Liouvilleã®å®šç†)

</details>

<details><summary>**Q4: Hutchinson trace**</summary>

> $\text{tr}(A)$ ã‚’æœŸå¾…å€¤ã§ã€‚

**è§£ç­”**: $\text{tr}(A) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)}[\epsilon^T A \epsilon]$

</details>

<details><summary>**Q5: Flow vs VAE vs GANå°¤åº¦**</summary>

**è§£ç­”**:
- Flow: å³å¯† $\log p(x) = \log p(z) - \log |\det J|$
- VAE: è¿‘ä¼¼ ELBO $\leq \log p(x)$
- GAN: ä¸æ˜ (æš—é»™çš„)

</details>

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. RealNVP Rustå®Ÿè£…ã«ãŠã„ã¦ affine coupling ã®è¡Œåˆ—å¼è¨ˆç®—ãŒ $O(1)$ ã«ãªã‚‹ç†ç”±ã‚’ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œå¤‰æ•°åã¨æ•°å¼ã§ç¤ºã›ã€‚
>    - *ãƒ’ãƒ³ãƒˆ*: `log_det_jac = vec(sum(s, dims=1))` ã® `s` ã¯ã©ã®å¤‰æ•°ã‹ã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯æ§‹é€ ã‚’æ›¸ãå‡ºã›ã€‚
> 2. NCSNã¨ã®æ¯”è¼ƒã§ã€NFã®å¯†åº¦æ¨å®šãŒä½æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§å„ªã‚Œé«˜æ¬¡å…ƒã§åŠ£ã‚‹å‚¾å‘ãŒã‚ã‚‹ç†ç”±ã‚’è¿°ã¹ã‚ˆã€‚
>    - *ãƒ’ãƒ³ãƒˆ*: Coupling Layer ã®ã€Œå¤‰æ•°åˆ†å‰²ã€ãŒé«˜æ¬¡å…ƒã§ã©ã†ã„ã†æƒ…å ±æå¤±ã‚’å¼•ãèµ·ã“ã™ã‹è€ƒãˆã‚ˆã€‚

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

> **Note:** **Zone 6ã®ç›®çš„**: Flowã¨Diffusionã®çµ±ä¸€ç†è«–ã§ã‚ã‚‹**Flow Matching**ã‚’ç†è§£ã—ã€JKOã‚¹ã‚­ãƒ¼ãƒ ã®æ•°ç†åŸºç›¤ã‚’å­¦ã¶ã€‚2024-2026ã®æœ€æ–°ç ”ç©¶å‹•å‘ã‚’æŠŠæ¡ã—ã€Normalizing Flowã®æœªæ¥ã‚’å±•æœ›ã™ã‚‹ã€‚

### 6.1 Flow Matching: Flowã¨Diffusionã®çµ±ä¸€

#### 6.1.1 Flow Matchingã®å‹•æ©Ÿ

**å•é¡Œ**: CNF/FFJORDã¯å¼·åŠ›ã ãŒã€ä»¥ä¸‹ã®èª²é¡ŒãŒã‚ã‚‹:

1. **å°¤åº¦è¨ˆç®—ã‚³ã‚¹ãƒˆ**: Hutchinson trace estimatorã¯åˆ†æ•£ãŒå¤§ããä¸å®‰å®š
2. **ODEã‚½ãƒ«ãƒãƒ¼ã®é…ã•**: æ¨è«–æ™‚ã«RK45ãªã©å¤šæ®µæ³•ãŒå¿…è¦
3. **è¨“ç·´ã®ä¸å®‰å®šæ€§**: $\text{tr}(\partial f/\partial z)$ ã®å­¦ç¿’ãŒé›£ã—ã„

**è§£æ±ºç­–**: Flow Matchingã¯ã€Œãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t(x)$ ã‚’**ç›´æ¥å›å¸°**ã€ã™ã‚‹æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚

#### 6.1.2 Flow Matchingå®šå¼åŒ–

**å®šç¾©**: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_1(x)$ ã¨ãƒã‚¤ã‚ºåˆ†å¸ƒ $p_0(z)$ ã‚’çµã¶**ç¢ºç‡ãƒ‘ã‚¹** $p_t(x)$ ã‚’è€ƒãˆã‚‹ã€‚

$$
p_t(x) = \int p_t(x|x_1) p_1(x_1) dx_1
$$

ã“ã“ã§ $p_t(x|x_1)$ ã¯**æ¡ä»¶ä»˜ãç¢ºç‡ãƒ‘ã‚¹**(ä¾‹: Gaussianãƒ–ãƒ©ãƒ¼):

$$
p_t(x|x_1) = \mathcal{N}(x; (1-t)x_1 + t \mu, \sigma_t^2 I)
$$

**ç›®æ¨™**: ã“ã® $p_t(x)$ ã‚’ç”Ÿæˆã™ã‚‹**ãƒ™ã‚¯ãƒˆãƒ«å ´** $v_t(x)$ ã‚’å­¦ç¿’ã™ã‚‹:

$$
\frac{dx}{dt} = v_t(x), \quad x(0) \sim p_0, \quad x(1) \sim p_1
$$

#### 6.1.3 Conditional Flow Matching (CFM) æå¤±

**ç›´æ¥å­¦ç¿’ã¯å›°é›£**: $p_t(x)$ ã¯é™°çš„ã«ã—ã‹å®šç¾©ã•ã‚Œã¦ã„ãªã„ã€‚

**è§£æ±º**: **æ¡ä»¶ä»˜ããƒ™ã‚¯ãƒˆãƒ«å ´** $u_t(x|x_1)$ ã‚’ä½¿ã†:

$$
u_t(x|x_1) = \frac{d}{dt} \mathbb{E}_{p_t(x|x_1)}[x] = \frac{t x_1 + (1-t)\mu - x}{\sigma_t^2}
$$

**CFMæå¤±**:

$$
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t \sim U[0,1], x_1 \sim p_1, x \sim p_t(\cdot|x_1)} \left[ \| v_t(x; \theta) - u_t(x|x_1) \|^2 \right]
$$

**æ•°å€¤æ¤œç®—**: $D=2$, $x_1 = (1,0)$, $x_0 = (0,0)$, $t=0.5$ã®ã¨ãã€‚$x_t = 0.5 x_1 = (0.5, 0)$ã€$u_t = x_1 - x_0 = (1,0)$ã€‚å®Œç’§ãªãƒ¢ãƒ‡ãƒ«ãŒ $v_t(x_t) = (1,0)$ ã‚’å‡ºåŠ›ã™ã‚Œã° loss=0ã€‚

#### 6.1.4 Flow Matching vs CNF vs Diffusion

| æ‰‹æ³• | ãƒ™ã‚¯ãƒˆãƒ«å ´ | æå¤± | å°¤åº¦ | æ¨è«–é€Ÿåº¦ |
|------|------------|------|------|----------|
| **CNF** | $f(z,t)$ (Neural ODE) | NLL + trace(Jacobian) | å³å¯† | é…ã„ (ODE) |
| **FFJORD** | $f(z,t)$ | NLL + Hutchinson | å³å¯† | é…ã„ (ODE) |
| **Flow Matching** | $v_t(x)$ | MSEå›å¸° $\|\|v_t - u_t\|\|^2$ | ä¸è¦ | é€Ÿã„ (1-stepå¯) |
| **DDPM** | $\epsilon_\theta(x_t, t)$ | MSEå›å¸° $\|\|\epsilon - \epsilon_\theta\|\|^2$ | ä¸è¦ | é€Ÿã„ (å°‘ã‚¹ãƒ†ãƒƒãƒ—) |

**çµè«–**: Flow Matchingã¯CNFã®ã€Œå°¤åº¦è¨ˆç®—ã‚’æ¨ã¦ã¦å›å¸°ã«ç‰¹åŒ–ã€ã—ãŸã‚‚ã®ã€‚Diffusionã¨æ•°å­¦çš„ã«ç­‰ä¾¡[^8]ã€‚

> **âš ï¸ Warning:** CFM ã§ã¯ $u_t(x|x_1)$ ã§å›å¸°ã™ã‚‹ãŒã€ã“ã‚Œã¯ conditional velocityï¼ˆ1ã‚µãƒ³ãƒ—ãƒ«ã®çµŒè·¯ï¼‰ã§ã‚ã‚Š marginal velocity $v_t(x)$ ã§ã¯ãªã„ã€‚**ä¸¡è€…ã‚’æœ€å°åŒ–ã™ã‚‹è§£ãŒä¸€è‡´ã™ã‚‹**ï¼ˆLipman et al., 2022 ã® Theorem 2ï¼‰ã“ã¨ãŒ CFM ã®æ ¸å¿ƒã€‚`u_t` ã¨ `v_t` ã‚’æ··åŒã—ã¦ãƒ‡ãƒãƒƒã‚°ã«è¿·ã£ãŸå ´åˆã¯ã“ã®ç­‰ä¾¡æ€§ã«ç«‹ã¡è¿”ã‚‹ã“ã¨ã€‚

#### 6.1.5 Flow Matchingå®Ÿè£… (Rust/Lux)

```rust
use candle_core::{Tensor, DType, Device};
use candle_nn::{Module, VarMap, VarBuilder, AdamW, ParamsAdamW, linear, Activation};
use rand::Rng;
use rand_distr::StandardNormal;

// Conditional Flow Matching training in Rust / candle

// Vector field network: [x_t (2D)] â†’ velocity (2D)
fn build_vnet(vb: &VarBuilder) -> candle_core::Result<candle_nn::Sequential> {
    Ok(candle_nn::seq()
        .add(linear(2, 64,  vb.pp("l0"))?)
        .add(Activation::Relu)
        .add(linear(64, 128, vb.pp("l1"))?)
        .add(Activation::Relu)
        .add(linear(128, 64, vb.pp("l2"))?)
        .add(Activation::Relu)
        .add(linear(64, 2,   vb.pp("l3"))?))
}

// CFM loss: â„’_CFM = E_{t,xâ‚,Îµ}[â€–v_Î¸(x_t,t) - u_t(x_t|xâ‚)â€–Â²]
// OT path:  x_t = (1-t)xâ‚ + Ïƒ_t Îµ,  u_t = (xâ‚ - x_t) / (Ïƒ_tÂ² + Î´)
fn cfm_loss(
    vnet: &impl Module,
    x1_batch: &Tensor,   // [2, B] â€” data samples
    rng: &mut impl Rng,
) -> candle_core::Result<Tensor> {
    let (_, b) = x1_batch.dims2()?;
    let device = x1_batch.device();

    // t ~ Uniform[0,1] per sample
    let t_vals: Vec<f32> = (0..b).map(|_| rng.gen::<f32>()).collect();
    let t = Tensor::from_slice(&t_vals, (1, b), device)?;   // [1, B]

    // Ïƒ_t = 0.1Â·(1-t)  â€” noise schedule shrinks toward t=1
    let sigma_t = ((&Tensor::ones_like(&t)? - &t)? * 0.1f64)?;

    // x_t = (1-t)Â·xâ‚ + Ïƒ_tÂ·Îµ,  Îµ ~ N(0,I)   (conditional probability path)
    let eps_vals: Vec<f32> = (0..2 * b).map(|_| rng.sample::<f32, _>(StandardNormal)).collect();
    let eps = Tensor::from_slice(&eps_vals, (2, b), device)?;
    let x_t = (x1_batch.broadcast_mul(
        &(Tensor::ones_like(&t)? - &t)?
    )? + eps.broadcast_mul(&sigma_t)?)?;

    // Target conditional velocity: u_t = (xâ‚ - x_t) / (Ïƒ_tÂ² + Î´)
    let sigma_sq = (sigma_t.sqr()? + 1e-6f64)?;
    let u_t = (x1_batch - &x_t)?.broadcast_div(&sigma_sq)?;  // u_t(x_t|xâ‚)

    // â„’_CFM = E[â€–v_Î¸(x_t) - u_tâ€–Â²]
    let v_t = vnet.forward(&x_t)?;                            // predicted velocity
    (&v_t - &u_t)?.sqr()?.mean_all()
}

// Training loop
let device = Device::Cpu;
let var_map = VarMap::new();
let vb = VarBuilder::from_varmap(&var_map, DType::F32, &device);
let vnet = build_vnet(&vb)?;
let mut opt = AdamW::new(var_map.all_vars(), ParamsAdamW { lr: 1e-3, ..Default::default() })?;
let mut rng = rand::thread_rng();

for epoch in 0..1000 {
    let x1_batch = sample_data(256, &mut rng, &device)?;  // your data sampler
    let loss = cfm_loss(&vnet, &x1_batch, &mut rng)?;
    opt.backward_step(&loss)?;

    if (epoch + 1) % 100 == 0 {
        println!("Epoch {}: Loss = {:.6}", epoch + 1, loss.to_scalar::<f32>()?);
    }
}

// Sampling via Euler ODE integration: dx/dt = v_Î¸(x, t)
fn sample_flow_matching(
    vnet: &impl Module,
    n_samples: usize,
    n_steps: usize,
    rng: &mut impl Rng,
    device: &Device,
) -> candle_core::Result<Tensor> {
    let dt = 1.0f64 / n_steps as f64;
    let init: Vec<f32> = (0..2 * n_samples)
        .map(|_| rng.sample::<f32, _>(StandardNormal))
        .collect();
    let mut x = Tensor::from_slice(&init, (2, n_samples), device)?;  // Start from N(0,I)

    for _ in 0..n_steps {
        let v = vnet.forward(&x)?;          // v_Î¸(x_t, t)
        x = (&x + &(v * dt)?)?;             // Euler: x_{t+dt} = x_t + dtÂ·v_Î¸(x_t, t)
    }
    Ok(x)
}

let samples = sample_flow_matching(&vnet, 1000, 100, &mut rng, &device)?;
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- **æå¤±é–¢æ•°ã¯å˜ç´”ãªå›å¸°**: $\|\|v_t - u_t\|\|^2$ ã®ã¿
- **å°¤åº¦è¨ˆç®—ãªã—**: traceã‚‚ä¸è¦
- **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯é«˜é€Ÿ**: å°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§OK (10-50ã‚¹ãƒ†ãƒƒãƒ—)
- **Diffusionã¨ç­‰ä¾¡**: DDPMã® $\epsilon_\theta$ ã‚’ãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t$ ã«å¤‰æ›ã—ãŸã ã‘

**æ•°å€¤æ¤œç®—ï¼ˆ2Dï¼‰**: $x_1 = (1,0)$, $x_0 = (0,0)$, $t=0.5$ ã®ã¨ãã€‚$x_t = (0.5, 0)$ã€$u_t = (1,0) - (0,0) = (1,0)$ã€‚Euler step: $x_{0.5+dt} = x_t + dt \cdot v_t$ã€‚10ã‚¹ãƒ†ãƒƒãƒ— ($dt=0.1$) ã§ $(0,0) \to (1,0)$ ã«åˆ°é”ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã§ $v_t \approx (1,0)$ ãªã‚‰èª¤å·®ã‚¼ãƒ­ï¼ˆç›´ç·šçµŒè·¯ã®æ©æµï¼‰ã€‚

### 6.2 JKOã‚¹ã‚­ãƒ¼ãƒ : Wassersteinå‹¾é…æµã®è¦–ç‚¹

#### 6.2.1 JKOã‚¹ã‚­ãƒ¼ãƒ ã¨ã¯

**Jordan-Kinderlehrer-Otto (JKO) ã‚¹ã‚­ãƒ¼ãƒ **ã¯ã€ç¢ºç‡åˆ†å¸ƒã®æ™‚é–“ç™ºå±•ã‚’**Wassersteinè·é›¢ã®æœ€æ€¥é™ä¸‹**ã¨ã—ã¦å®šå¼åŒ–ã™ã‚‹æ çµ„ã¿[^9]ã€‚

**å•é¡Œè¨­å®š**: ã‚¨ãƒãƒ«ã‚®ãƒ¼æ±é–¢æ•° $\mathcal{F}[p]$ ã‚’æŒã¤åˆ†å¸ƒ $p_t$ ã®å‹¾é…æµ:

$$
\frac{\partial p_t}{\partial t} = -\nabla \cdot (p_t \nabla \frac{\delta \mathcal{F}}{\delta p})
$$

ã“ã‚Œã¯**Fokker-Planckæ–¹ç¨‹å¼**ã¨å‘¼ã°ã‚Œã‚‹ã€‚

#### 6.2.2 JKOã‚¹ã‚­ãƒ¼ãƒ ã®é›¢æ•£åŒ–

**JKOã‚¹ã‚­ãƒ¼ãƒ ã®å®šç¾©**: æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ— $\tau$ ã§ä»¥ä¸‹ã‚’ç¹°ã‚Šè¿”ã™:

$$
p_{k+1} = \arg\min_{p} \left\{ \mathcal{F}[p] + \frac{1}{2\tau} W_2^2(p, p_k) \right\}
$$

ã“ã“ã§ $W_2(p, q)$ ã¯**2-Wassersteinè·é›¢**:

$$
W_2^2(p, q) = \inf_{\pi \in \Pi(p,q)} \int \|x - y\|^2 d\pi(x,y)
$$

**æ•°å€¤ä¾‹ï¼ˆ1æ¬¡å…ƒï¼‰**: $p_k = \mathcal{N}(1, 1)$, $\mathcal{F}[p] = \text{KL}(p \| \mathcal{N}(0,1))$, $\tau = 0.1$ ã®ã¨ãã€$p_{k+1} \approx \mathcal{N}(0.9, 1)$ã€‚å¹³å‡ãŒç›®æ¨™åˆ†å¸ƒã«å‘ã‹ã£ã¦ $\tau$ ã ã‘è¿‘ã¥ã â†’ å‹¾é…é™ä¸‹ã®ç¢ºç‡åˆ†å¸ƒç‰ˆã€‚

#### 6.2.3 Normalizing Flowã¨JKOã®é–¢ä¿‚

**ç™ºè¦‹**: Normalizing Flowã®å­¦ç¿’ã¯**é›¢æ•£JKOã‚¹ã‚­ãƒ¼ãƒ **ã¨è¦‹ãªã›ã‚‹[^10]!

**å¯¾å¿œé–¢ä¿‚**:

| JKOã‚¹ã‚­ãƒ¼ãƒ  | Normalizing Flow |
|-------------|-------------------|
| ã‚¨ãƒãƒ«ã‚®ãƒ¼ $\mathcal{F}[p]$ | NLL $-\log p(x)$ |
| Wassersteinè·é›¢ $W_2(p, q)$ | Flowå¤‰æ›ã®æ­£å‰‡åŒ– |
| æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ— $\tau$ | å­¦ç¿’ç‡ $\eta$ |
| å‹¾é…æµ $\frac{\partial p}{\partial t}$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° $\frac{d\theta}{dt}$ |

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

1. Flowã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’å°‘ã—å‹•ã‹ã™: $\theta \to \theta + \Delta\theta$
2. ã“ã‚Œã¯åˆ†å¸ƒ $p_\theta(x)$ ã‚’å¤‰åŒ–ã•ã›ã‚‹: $p_\theta \to p_{\theta + \Delta\theta}$
3. ã“ã®å¤‰åŒ–é‡ã¯ $W_2$ è·é›¢ã§æ¸¬ã‚Œã‚‹
4. NLLã‚’æ¸›ã‚‰ã™æ–¹å‘ã« $\theta$ ã‚’å‹•ã‹ã™ã¨ã€JKOã‚¹ã‚­ãƒ¼ãƒ ã®æ›´æ–°å¼ã¨ä¸€è‡´

**çµè«–**: Normalizing Flowã®è¨“ç·´ã¯ã€ŒWassersteinç©ºé–“ä¸Šã®å‹¾é…é™ä¸‹æ³•ã€ã§ã‚ã‚‹ã€‚

#### 6.2.4 å®Ÿç”¨çš„æ„ç¾©

**1. åæŸä¿è¨¼**: JKOç†è«–ã«ã‚ˆã‚Šã€Flowã®è¨“ç·´ãŒã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å˜èª¿æ¸›å°‘ã•ã›ã‚‹ã€ã“ã¨ãŒä¿è¨¼ã•ã‚Œã‚‹ã€‚

**2. æœ€é©è¼¸é€ã¨ã®æ¥ç¶š**: Optimal Transportç†è«–ãŒFlowã®è¨­è¨ˆã«ä½¿ãˆã‚‹:
   - **Monge-AmpÃ¨reæ–¹ç¨‹å¼**: æœ€é©è¼¸é€ã®è§£ã¯å‡¸é–¢æ•° $\phi$ ã®å‹¾é… $\nabla \phi$
   - **Brenierå®šç†**: æœ€é©è¼¸é€å†™åƒã¯ä¸€æ„ã«å­˜åœ¨
   - **Coupling Layerã®æ­£å½“åŒ–**: $x = T(z)$ ã¯æœ€é©è¼¸é€å†™åƒã®é›¢æ•£è¿‘ä¼¼

**3. Flowã¨Diffusionã®çµ±ä¸€**: ä¸¡è€…ã¨ã‚‚ã€ŒWassersteinå‹¾é…æµã®é›¢æ•£åŒ–ã€ã¨ã—ã¦ç†è§£ã§ãã‚‹:
   - **Flow**: æ±ºå®šè«–çš„ãªçµŒè·¯ (ODEã‚½ãƒ«ãƒãƒ¼)
   - **Diffusion**: ç¢ºç‡çš„ãªçµŒè·¯ (SDEã‚½ãƒ«ãƒãƒ¼)

**å®Ÿç”¨çš„æ„ç¾©ã¾ã¨ã‚**: JKOç†è«–ã®è¦–ç‚¹ã‹ã‚‰è¦‹ã‚‹ã¨ã€Flowè¨“ç·´ä¸­ã®ã€ŒNLLæ¸›å°‘ + ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã€ã¯ã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼æ¸›å°‘ + åˆ†å¸ƒç§»å‹•ã‚³ã‚¹ãƒˆæœ€å°åŒ–ã€ã®é›¢æ•£åŒ–ã«ãªã£ã¦ã„ã‚‹ã€‚å­¦ç¿’ç‡ $\eta$ ãŒå°ã•ã™ãã‚‹ã¨ $W_2$ ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒå¼·ãåŠ¹ã„ã¦åæŸãŒé…ãã€å¤§ãã™ãã‚‹ã¨ JKO ã®æ­£å‰‡åŒ–ãŒå´©ã‚Œã¦ç™ºæ•£ã™ã‚‹ â€” ã“ã‚ŒãŒã€Œlr ãŒé«˜ã™ãã‚‹ã¨ NLL ãŒçˆ†ç™ºã™ã‚‹ã€ç¾è±¡ã®å¹¾ä½•å­¦çš„èª¬æ˜ã ã€‚


### 6.3 æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)

#### 6.3.1 Flow Matching ã®ç™ºå±•

**Stochastic Interpolants (2023-2024)**[^11]:
- Flow Matchingã‚’SDEã«æ‹¡å¼µ
- Diffusionã¨Flowã®ä¸­é–“çš„ãªæ‰‹æ³•
- æ¨è«–æ™‚ã«ãƒã‚¤ã‚ºæ³¨å…¥ã§å¤šæ§˜æ€§å‘ä¸Š

**Rectified Flow (2024)**[^12]:
- ã€Œæ›²ãŒã£ãŸFlowã€ã‚’ã€Œç›´ç·šçš„ãªFlowã€ã«ä¿®æ­£
- 1-stepã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¯èƒ½ã«
- Distillationæ‰‹æ³•ã¨ã—ã¦æ³¨ç›®

**Rectified Flow ã®æ ¸å¿ƒ**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ $x_1$ ã¨ãƒã‚¤ã‚º $x_0$ ã‚’ãƒ©ãƒ³ãƒ€ãƒ ãƒšã‚¢ã«ã—ã¦ Linear Flow ã‚’å­¦ç¿’ã™ã‚‹ã¨çµŒè·¯ãŒã€Œæ›²ãŒã‚‹ã€ã€‚ã“ã‚Œã‚’ Reflowï¼ˆåŒä¸€ãƒ¢ãƒ‡ãƒ«ã§ $(x_0, x_1)$ ã®æœ€é©ãƒšã‚¢ã‚’å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰ã§ç¹°ã‚Šè¿”ã™ã¨çµŒè·¯ãŒç›´ç·šã«è¿‘ã¥ãã€‚$k$ å› Reflow ã§åˆ‡æ–­èª¤å·® $O(1/N^{2k})$ â†’ $k=2$ ã§å¤§å¹…é«˜é€ŸåŒ–ã€‚SD3 ã¯ã“ã®åŸç†ã‚’æ¡ç”¨ã€‚

**Policy Flow (2024)**:
- å¼·åŒ–å­¦ç¿’ã¨Flowã®èåˆ
- æ–¹ç­– $\pi(a|s)$ ã‚’Flowã§ãƒ¢ãƒ‡ãƒ«åŒ–
- é€£ç¶šè¡Œå‹•ç©ºé–“ã®åŠ¹ç‡çš„æ¢ç´¢

#### 6.3.2 é«˜é€ŸåŒ–ãƒ»åŠ¹ç‡åŒ–

**Consistency Models (2023)**[^13]:
- Diffusionã®è’¸ç•™ã«ã‚ˆã‚Š1-stepã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿç¾
- Flowã«ã‚‚ConsistencyåŸç†ã‚’é©ç”¨å¯èƒ½
- æ¨è«–é€Ÿåº¦100å€ä»¥ä¸Šã®é«˜é€ŸåŒ–

**ãªãœ Consistency ãŒ Flow ã«é©ç”¨ã§ãã‚‹ã‹**: Flow ã® ODE ã¯é€£ç¶šæ™‚é–“ç‰ˆã®ã€Œæ±ºå®šè«–çš„ãƒãƒƒãƒ—ã€ãªã®ã§ã€ä»»æ„ã® $t$ ã‹ã‚‰çµ‚ç‚¹ $t=1$ ã¸ã® self-consistencyï¼ˆåŒã˜çµ‚ç‚¹ã«åˆ°é”ã™ã‚‹ï¼‰ã‚’å®šç¾©ã§ãã‚‹ã€‚Diffusion ã® Consistency Modelsï¼ˆCMï¼‰ã¨å…¨ãåŒã˜æ çµ„ã¿ãŒæˆç«‹ã™ã‚‹ã€‚Flow Matching ã®å ´åˆã€ç›´ç·šçµŒè·¯ã«ã‚ˆã‚Š CM ã®è’¸ç•™èª¤å·®ãŒã•ã‚‰ã«å°ã•ããªã‚‹ï¼ˆçµŒè·¯ã®æ›²ç‡ â‰ˆ 0 â†’ truncation error æœ€å°ï¼‰ã€‚

**Latent Diffusion/Flow (2024)**:
- ç”»åƒã‚’æ½œåœ¨ç©ºé–“ $z$ ã«åœ§ç¸®ã—ã¦ã‹ã‚‰Flow/Diffusion
- Stable Diffusion 3.0ã¯Flow Matchingãƒ™ãƒ¼ã‚¹
- è¨ˆç®—é‡ã‚’1/10ä»¥ä¸‹ã«å‰Šæ¸›

**Continuous Normalizing Flows with Adjoint (2024)**:
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ (O(1) ãƒ¡ãƒ¢ãƒª)
- ã‚ˆã‚Šæ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’ãŒå¯èƒ½
- Physics-Informed CNFã¸ã®å¿œç”¨

#### 6.3.3 å¿œç”¨åˆ†é‡ã®æ‹¡å¤§

**1. ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€ äºˆæ¸¬**:
- AlphaFold3 (2024) ã¯Flow-based
- åŸå­åº§æ¨™ã®åŒæ™‚åˆ†å¸ƒã‚’å­¦ç¿’
- Diffusion/Flowãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰

**AlphaFold3 ã® Flow åˆ©ç”¨ã®æ ¸å¿ƒ**: ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®åŸå­åº§æ¨™ã¯ 3D ç©ºé–“ä¸Šã®ç‚¹ç¾¤ $\{r_i \in \mathbb{R}^3\}_{i=1}^N$ã€‚ã“ã‚Œã« SE(3) ä¸å¤‰ Flow ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€å›è»¢ãƒ»ä¸¦é€²ã«å¯¾ã—ã¦ç‰©ç†çš„ã«æ•´åˆã—ãŸæ§‹é€ ã‚’ç”Ÿæˆã§ãã‚‹ã€‚Diffusion ãƒ™ãƒ¼ã‚¹ã® AlphaFold3 ã¯ã€Œã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‹ã‚‰åŸå­åº§æ¨™ã‚’å¾©å…ƒã€ã™ã‚‹ãŸã‚ã€NF ã®ã€Œå¤‰æ›å¯èƒ½æ€§ï¼ˆå³å¯†å°¤åº¦ï¼‰ã€ã¨ Diffusion ã®ã€Œè¡¨ç¾åŠ›ã€ã‚’ä¸¡ç«‹ã—ã¦ã„ã‚‹ã€‚

**2. åˆ†å­ç”Ÿæˆ**:
- SE(3)-equivariant Flow
- å›è»¢ãƒ»ä¸¦é€²ä¸å¤‰æ€§ã‚’æŒã¤Flow
- è–¬å‰¤å€™è£œã®è‡ªå‹•è¨­è¨ˆ

**3. æ™‚ç³»åˆ—äºˆæ¸¬**:
- Temporal Normalizing Flow
- ä¸è¦å‰‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç³»åˆ—ã®å‡¦ç†
- Neural ODE + Flowã®èåˆ

**4. å› æœæ¨è«–**:
- Causal Normalizing Flow
- ä»‹å…¥åˆ†å¸ƒ $p(y|do(x))$ ã®å­¦ç¿’
- åäº‹å®Ÿæ¨è«–ã¸ã®å¿œç”¨

> **âš ï¸ Warning:** å› æœæ¨è«–ã« Flow ã‚’ä½¿ã†å ´åˆã€ã€Œç›¸é–¢ã€ã¨ã€Œå› æœã€ã‚’æ··åŒã—ãªã„ã“ã¨ã€‚$\log p(x)$ ã®æœ€å¤§åŒ–ã¯ã€Œè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã€ã™ã‚‹ã ã‘ã§ã€ä»‹å…¥ $do(x)$ ã®åŠ¹æœã¯è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‹ã‚‰ã¯è­˜åˆ¥ã§ããªã„ã€‚Causal Flow ã¯æ§‹é€ å› æœãƒ¢ãƒ‡ãƒ«ï¼ˆSCMï¼‰ã®ä»®å®šãŒåˆ¥é€”å¿…è¦ã€‚

#### 6.3.4 ç†è«–çš„é€²å±•

**Universal Approximation of Flows (2024)**:
- Coupling Layer ã®ç†è«–çš„ä¿è¨¼å¼·åŒ–
- æœ‰é™å¹…ã§ã‚‚ universal approximation å¯èƒ½
- å¿…è¦å±¤æ•°ã®ä¸Šç•Œå°å‡º

**ç›´æ„Ÿ**: Coupling Layer ãŒå…¨ã¦ã®å¯é€†å¤‰æ›ã‚’è¿‘ä¼¼ã§ãã‚‹ã“ã¨ã®è¨¼æ˜ã¯ã€ã€Œååˆ†å¤šã„å±¤ã‚’é‡ã­ã‚Œã°ä»»æ„ã®åˆ†å¸ƒé–“ã®å¤‰æ›ãŒå­¦ç¿’å¯èƒ½ã€ã‚’æ„å‘³ã™ã‚‹ã€‚å®Ÿç”¨çš„ã«ã¯ $K = 10$ã€œ$20$ å±¤ã§ååˆ†ï¼ˆè«–æ–‡ã§ã¯ $K = O(\log D)$ ã®ä¸Šç•Œå°å‡ºï¼‰ã€‚

**Flow Matching = Diffusion ã®å³å¯†è¨¼æ˜ (2024)**:
- CFMæå¤±ã¨DDPMæå¤±ãŒæœ¬è³ªçš„ã«åŒä¸€
- ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla \log p_t$ ã¸ã®åæŸä¿è¨¼
- åæŸé€Ÿåº¦ã®è§£æ

**Wasserstein Gradient Flow ã®é›¢æ•£åŒ–èª¤å·® (2025)**:
- JKOã‚¹ã‚­ãƒ¼ãƒ ã®æ•°å€¤è§£æ
- æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ— $\tau$ ã«å¯¾ã™ã‚‹èª¤å·® $O(\tau^2)$ ã®è¨¼æ˜
- é©å¿œçš„ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã®è¨­è¨ˆæŒ‡é‡

**èª¤å·® $O(\tau^2)$ ã®ç›´æ„Ÿ**: JKO ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’ä¸‹ã’ã‚‹æœ€é©åŒ–ã€ã€‚$\tau$ ãŒå¤§ãã„ã¨ Wasserstein çƒã®å¤–ã«é£›ã³å‡ºã—ã¦èª¤å·®ãŒç´¯ç© â†’ $O(\tau^2)$ã€‚Runge-Kutta ã® $O(\Delta t^2)$ ã¨å…¨ãåŒã˜æ§‹é€ ã€‚é©å¿œçš„ $\tau$ ã¯ã‚ªãƒ¼ãƒãƒ¼ã‚·ãƒ¥ãƒ¼ãƒˆã‚’é˜²ãã¤ã¤åæŸã‚’é€Ÿã‚ã‚‹ â€” ODE ã‚½ãƒ«ãƒãƒ¼ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ã¨æ•°å­¦çš„ã«åŒä¸€ã€‚

> **âš ï¸ Warning:** **Zone 6 å®Œäº†**: Flow Matchingã®æ•°ç†ã€JKOã‚¹ã‚­ãƒ¼ãƒ ã€2024-2026æœ€æ–°ç ”ç©¶ã‚’ç¶²ç¾…ã€‚æ¬¡ã¯**æŒ¯ã‚Šè¿”ã‚Šçµ±åˆ**ã§å…¨ä½“ã‚’ã¾ã¨ã‚ã‚‹ã€‚

---

## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 7.1 æœ¬è¬›ç¾©ã§é”æˆã—ãŸã“ã¨

**æ•°å­¦çš„ç†è§£ (Zone 3)**:

âœ… **Change of Variableså…¬å¼ã®å®Œå…¨å°å‡º**
- 1æ¬¡å…ƒ â†’ å¤šæ¬¡å…ƒ â†’ åˆæˆå¤‰æ›
- ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ã®æ„å‘³: ä½“ç©è¦ç´ ã®å¤‰åŒ–ç‡
- $\log p(x) = \log p(z) - \log |\det J_f|$ ã®å³å¯†ãªè¨¼æ˜

âœ… **Coupling Layerã®ç†è«–**
- ä¸‰è§’è¡Œåˆ—æ§‹é€ ã§ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã‚’ O(DÂ³) â†’ O(D) ã«å‰Šæ¸›
- Affine Coupling Layer (RealNVP)
- Multi-scale architecture

âœ… **Glowã®é©æ–°**
- Actnorm (Batch Normã®å¯é€†ç‰ˆ)
- 1Ã—1 Invertible Convolution
- LUåˆ†è§£ã«ã‚ˆã‚‹ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã®åŠ¹ç‡åŒ–

âœ… **Continuous Normalizing Flows**
- Instantaneous Change of Variables: $\frac{\partial \log p(z(t))}{\partial t} = -\text{tr}(\frac{\partial f}{\partial z})$
- Neural ODE: é›¢æ•£å±¤ â†’ é€£ç¶šæ™‚é–“ODE
- Adjoint Method: ãƒ¡ãƒ¢ãƒª O(1) ã®é€†ä¼æ’­

âœ… **FFJORD**
- Hutchinson traceæ¨å®š: O(DÂ²) â†’ O(D)
- Vector-Jacobian Product (VJP) ã«ã‚ˆã‚‹åŠ¹ç‡çš„è¨ˆç®—
- $\text{tr}(A) = \mathbb{E}[\mathbf{v}^\top A \mathbf{v}]$

**å®Ÿè£…åŠ› (Zone 4-5)**:

âœ… **Rust + Candle ã§ã®RealNVPå®Œå…¨å®Ÿè£…**
- Affine Coupling Layer
- å¤šå±¤Flow modelã®æ§‹ç¯‰
- è¨“ç·´ãƒ«ãƒ¼ãƒ— (negative log likelihoodæœ€å°åŒ–)
- 2D Moons dataset ã§ã®å®Ÿé¨“

âœ… **CNF/FFJORDã®æ§‹é€ ç†è§£**
- ode_solvers + ODE solver
- Hutchinson trace estimatorå®Ÿè£…
- Neural ODE dynamics

âœ… **å®Ÿé¨“ã«ã‚ˆã‚‹æ¤œè¨¼**
- å¯†åº¦æ¨å®šç²¾åº¦: Flow vs VAEæ¯”è¼ƒ (å³å¯†å°¤åº¦ vs ELBO)
- Out-of-Distributionæ¤œçŸ¥: 95%+ ç²¾åº¦
- ç”Ÿæˆå“è³ªã®è©•ä¾¡

**ç†è«–çš„å±•æœ› (Zone 2, 6)**:

âœ… **Course IVå…¨ä½“åƒã®æŠŠæ¡**
- NF â†’ EBM â†’ Score â†’ DDPM â†’ SDE â†’ Flow Matching â†’ LDM â†’ Consistency â†’ World Models â†’ çµ±ä¸€ç†è«–
- 10è¬›ç¾©ã®è«–ç†çš„ãƒã‚§ãƒ¼ãƒ³

âœ… **VAE/GAN/Flowã®3ã¤å·´**
- å°¤åº¦: è¿‘ä¼¼ (VAE) / æš—é»™çš„ (GAN) / **å³å¯† (Flow)**
- è¨“ç·´å®‰å®šæ€§ãƒ»ç”Ÿæˆå“è³ªãƒ»ç”¨é€”ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

âœ… **Flow Matchingã¸ã®æ©‹æ¸¡ã—**
- Probability Flow ODE (PF-ODE)
- Rectified Flow: ç›´ç·šè¼¸é€
- Optimal Transportè¦–ç‚¹ã§ã®çµ±ä¸€
- æœ€æ–°ç ”ç©¶: TarFlow, Stable Diffusion 3, Candle.1

**åˆ°é”ãƒ¬ãƒ™ãƒ«**:

- **åˆç´š â†’ ä¸­ç´šçªç ´**: Change of Variablesã®æ•°å­¦ã‚’å®Œå…¨ç†è§£
- **å®Ÿè£…åŠ›**: Candleã§å‹•ãFlowã‚’è‡ªåŠ›ã§æ›¸ã‘ã‚‹
- **ç†è«–çš„æ´å¯Ÿ**: Flowã®é™ç•Œã¨Flow Matchingã¸ã®é€²åŒ–ã‚’ç†è§£
- **æ¬¡ã¸ã®æº–å‚™**: ç¬¬37-38å› (SDE/ODE, Flow Matching) ã¸ã®åœŸå°å®Œæˆ

### 7.2 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

#### Q1: Normalizing Flowsã€çµå±€å®Ÿå‹™ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã®ï¼Ÿ

**A**: **2026å¹´ç¾åœ¨ã€å¾©æ´»ã—ã¤ã¤ã‚ã‚‹** (Flow MatchingçµŒç”±)ã€‚

**ç”¨é€”åˆ¥ã®ç¾çŠ¶**:

| ç”¨é€” | ä¸»æµæ‰‹æ³• | Flowã®å½¹å‰² | å®Ÿä¾‹ |
|:-----|:--------|:----------|:-----|
| **ç”»åƒç”Ÿæˆ (å“è³ªé‡è¦–)** | Diffusion | Flow Matchingã¨ã—ã¦å¾©æ´» | Stable Diffusion 3, Candle.1 |
| **ç”»åƒç”Ÿæˆ (é€Ÿåº¦é‡è¦–)** | GAN / Consistency | Rectified FlowãŒç«¶åˆ | 10-50 stepsç”Ÿæˆ |
| **å¯†åº¦æ¨å®š** | **Normalizing Flow** | ä»–æ‰‹æ³•ã§ã¯ä¸å¯èƒ½ | é‡‘èãƒªã‚¹ã‚¯ã€ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ |
| **ç•°å¸¸æ¤œçŸ¥ (OOD)** | **Normalizing Flow** | å³å¯†ãª $\log p(x)$ ãŒå¿…é ˆ | è£½é€ æ¥­ã€åŒ»ç™‚ç”»åƒ |
| **å¤‰åˆ†æ¨è«–** | IAF (Flow) + VAE | äº‹å¾Œåˆ†å¸ƒè¿‘ä¼¼ | ãƒ™ã‚¤ã‚ºæ·±å±¤å­¦ç¿’ |
| **æ½œåœ¨ç©ºé–“æ­£å‰‡åŒ–** | Flow + VAE / Flow + Diffusion | è¡¨ç¾å­¦ç¿’å¼·åŒ– | disentangled representation |

**æ­´å²çš„æ¨ç§»**:
- **2016-2019**: RealNVP, Glowå…¨ç›› â€” ã€Œæ¬¡ä¸–ä»£ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€ã¨ã—ã¦æ³¨ç›®
- **2020-2022**: DDPM, Stable Diffusionã®å°é ­ â€” Flowã¯ä¸€æ™‚ä¸‹ç«
- **2023-2026**: Flow Matchingç™»å ´ â€” ç†è«–ã¨å®Ÿè£…ã®èåˆã§**å¾©æ´»**

**çµè«–**: ç”Ÿæˆå“è³ªã§ã¯Diffusionã«ä¸€åº¦æ•—åŒ— â†’ Flow Matchingã§æ•°å­¦çš„åŸºç›¤ã‚’ä¿ã¡ã¤ã¤å®Ÿç”¨æ€§ã‚’å–ã‚Šæˆ»ã—ãŸã€‚

> **âš ï¸ Warning:** ã€ŒNormalizing Flow ã¯ä½¿ã‚ã‚Œãªããªã£ãŸã€ã¨ã„ã†è¨€èª¬ã¯ 2021-2022 å¹´æ™‚ç‚¹ã®è©±ã€‚2024-2026 å¹´ã® Stable Diffusion 3ã€FLUX.1ã€F5-TTS ã¯å…¨ã¦ Flow Matching ãƒ™ãƒ¼ã‚¹ã§ã‚ã‚Šã€ç¾åœ¨æœ€ã‚‚å®Ÿç”¨åŒ–ã•ã‚Œã¦ã„ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åŸºç›¤ãŒ Flow ã§ã‚ã‚‹ã“ã¨ã«å¤‰ã‚ã‚Šã¯ãªã„ã€‚

#### Q2: RealNVP vs Glow vs FFJORDã€ã©ã‚Œã‚’é¸ã¶ã¹ãï¼Ÿ

| è¦³ç‚¹ | RealNVP | Glow | FFJORD/CNF |
|:-----|:--------|:-----|:-----------|
| **å®Ÿè£…é›£æ˜“åº¦** | â˜…â˜†â˜† (æœ€ã‚‚ç°¡å˜) | â˜…â˜…â˜† (1Ã—1 Convè¤‡é›‘) | â˜…â˜…â˜… (ODE solverå¿…è¦) |
| **è¨“ç·´é€Ÿåº¦** | é€Ÿã„ | é€Ÿã„ | é…ã„ (ODEç©åˆ†) |
| **æ¨è«–é€Ÿåº¦** | æœ€é€Ÿ (~5ms/100 samples) | é€Ÿã„ (~10ms) | é…ã„ (~50ms) |
| **è¡¨ç¾åŠ›** | ä¸­ (Couplingåˆ¶ç´„) | é«˜ (1Ã—1 Conv) | **æœ€é«˜** (åˆ¶ç´„ãªã—) |
| **ãƒ¡ãƒ¢ãƒª** | O(KÂ·D) | O(KÂ·D) | O(1) (Adjoint) |
| **ç”¨é€”** | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã€OODæ¤œçŸ¥ | é«˜å“è³ªç”Ÿæˆ | ç ”ç©¶ã€è¤‡é›‘åˆ†å¸ƒ |

**æ¨å¥¨ãƒ•ãƒ­ãƒ¼**:
1. **ã¾ãšRealNVP** â†’ ã‚·ãƒ³ãƒ—ãƒ«ã€å®Ÿè£…100è¡Œã€ãƒ‡ãƒãƒƒã‚°å®¹æ˜“
2. **ä¸è¶³ãªã‚‰Glow** â†’ 1Ã—1 Convã§è¡¨ç¾åŠ›å‘ä¸Šã€multi-scale
3. **ã•ã‚‰ã«å¿…è¦ãªã‚‰FFJORD** â†’ åˆ¶ç´„ãªã—ã€Flow Matchingã¸ã®æ‹¡å¼µå®¹æ˜“

**å®Ÿå‹™**: 95%ã®ã‚±ãƒ¼ã‚¹ã¯RealNVPã§ååˆ†ã€‚ç ”ç©¶ãƒ»PoC ãªã‚‰FFJORDã€‚

#### Q3: ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ã€æœ¬å½“ã« O(D) ã§æ¸ˆã‚€ã®ï¼Ÿ

**A**: **Coupling Layerã«é™ã‚Šã€ã¯ã„**ã€‚

**è¨ˆç®—é‡ã®å†…è¨³**:

| æ‰‹æ³• | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³æ§‹é€  | $\det$ è¨ˆç®—é‡ | ç†ç”± |
|:-----|:-------------|:-------------|:-----|
| **ä¸€èˆ¬ã®å¯é€†è¡Œåˆ—** | å¯†è¡Œåˆ— | O(DÂ³) | LUåˆ†è§£ or å›ºæœ‰å€¤è¨ˆç®— |
| **ä¸‰è§’è¡Œåˆ—** | ä¸Š/ä¸‹ä¸‰è§’ | O(D) | å¯¾è§’è¦ç´ ã®ç© |
| **Coupling Layer** | ä¸‹ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯ | O(D) | $\det = \det(I) \cdot \det(\text{diag}(\exp(s)))$ |
| **FFJORD (Hutchinson)** | traceæ¨å®š | O(D) | VJP 1å› (ç¢ºç‡çš„ã€åˆ†æ•£ã‚ã‚Š) |
| **Glow 1Ã—1 Conv** | CÃ—Cè¡Œåˆ— | O(CÂ³) | Cã¯å›ºå®š (â‰¤512)ã€ç”»åƒã‚µã‚¤ã‚ºéä¾å­˜ |

**æ³¨æ„ç‚¹**:
- Coupling Layerã¯**è§£æçš„** â†’ å³å¯†ã«O(D)ã€åˆ†æ•£ãªã—
- FFJORDã¯**ç¢ºç‡çš„æ¨å®š** â†’ æœŸå¾…å€¤ã¯O(D)ã€åˆ†æ•£ã‚ã‚Š (è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã§ç²¾åº¦å‘ä¸Šå¯èƒ½)
- Glow 1Ã—1 Convã¯ç”»åƒã®**ãƒãƒ£ãƒãƒ«æ•°Cã®ã¿**ã«ä¾å­˜ â†’ é«˜è§£åƒåº¦ã§ã‚‚O(CÂ³)

**çµè«–**: Coupling Layerã®ã€Œä¸‰è§’è¡Œåˆ—åŒ–ã€ãŒã€Flowã®å®Ÿç”¨åŒ–ã‚’å¯èƒ½ã«ã—ãŸå¤©æ‰çš„ã‚¢ã‚¤ãƒ‡ã‚¢ã€‚

**æ•°å€¤ä¾‹**: $D = 784$ï¼ˆMNISTï¼‰ã®å ´åˆã€ä¸€èˆ¬è¡Œåˆ—å¼ã¯ $O(784^3) \approx 4.8 \times 10^8$ flopsã€‚Coupling Layer ãªã‚‰ $O(784)$ ã§ 600,000 å€é«˜é€Ÿã€‚ãƒãƒƒãƒã‚µã‚¤ã‚º 256 ã§è¨“ç·´ã™ã‚Œã°ã€ã“ã®å·®ã¯æ¯ã‚¹ãƒ†ãƒƒãƒ—ã®è¨“ç·´é€Ÿåº¦ã«ç›´çµã™ã‚‹ã€‚

#### Q4: CNFã¨Diffusionã®ODEã€ä½•ãŒé•ã†ã®ï¼Ÿ

**A**: è¨“ç·´æ–¹æ³•ã¨ç›®çš„ãŒç•°ãªã‚‹ãŒã€**æ•°å­¦çš„ã«ã¯åŒã˜æ çµ„ã¿** (ODE-based transport)ã€‚

| è¦³ç‚¹ | CNF (Normalizing Flow) | Diffusion (PF-ODE) |
|:-----|:----------------------|:------------------|
| **ç›®çš„** | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p(x)$ ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ– | ãƒã‚¤ã‚ºé™¤å»éç¨‹ $p_t(x)$ ã‚’ãƒ¢ãƒ‡ãƒ«åŒ– |
| **è¨“ç·´** | æœ€å°¤æ¨å®š $\max \log p(x)$ | ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚° or ãƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta$ |
| **ODEå½¢å¼** | $\frac{dz}{dt} = f(z, t)$ (ä»»æ„) | $\frac{dx}{dt} = f - \frac{1}{2} g^2 \nabla \log p_t$ (ã‚¹ã‚³ã‚¢ä¾å­˜) |
| **å°¤åº¦è¨ˆç®—** | å³å¯† (traceç©åˆ†) | å›°é›£ (å¤‰åˆ†ä¸‹ç•Œã®ã¿) |
| **ç”Ÿæˆå“è³ª** | ä¸­ç¨‹åº¦ | **SOTA** (ImageNet, SD) |
| **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** | 1-pass ODE | 10-1000 steps |
| **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** | Couplingåˆ¶ç´„ (å¾“æ¥) | U-Net/Transformer (è‡ªç”±) |

**Flow Matchingã®æ´å¯Ÿ**:
- ã“ã®2ã¤ã¯**åŒã˜ODE frameworkã®ç•°ãªã‚‹è¨“ç·´æ–¹æ³•**
- CNF: ãƒ™ã‚¯ãƒˆãƒ«å ´ $f$ ã‚’ç›´æ¥å­¦ç¿’
- Diffusion (PF-ODE): ã‚¹ã‚³ã‚¢ $\nabla \log p_t$ ã‚’å­¦ç¿’ â†’ $f$ ã‚’å°å‡º
- Flow Matching: ä¸¡è€…ã‚’çµ±ä¸€ â€” æ¡ä»¶ä»˜ããƒ•ãƒ­ãƒ¼ $v_t(x_t | x_0)$ ã‚’å­¦ç¿’

**ãªãœ Diffusion ãŒ CNF ã‚ˆã‚Šç”Ÿæˆå“è³ªã§å‹ã‚‹ã‹**: CNF ã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã« Coupling åˆ¶ç´„ãŒã‚ã‚‹ãŸã‚è¡¨ç¾åŠ›ãŒä½ã„ã€‚Diffusion ã® PF-ODE ã¯ U-Net/Transformer ã§è‡ªç”±ã«ã‚¹ã‚³ã‚¢ã‚’å­¦ç¿’ã§ãã‚‹ãŸã‚ SOTAã€‚Flow Matching ã¯ Diffusion ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä¿ã¡ã¤ã¤ CNF ã®æ•°å­¦çš„å³å¯†æ€§ã‚’å–ã‚Šè¾¼ã‚“ã ã€Œã„ã„ã¨ã“å–ã‚Šã€ã€‚

**ç¬¬38å›ã§å®Œå…¨çµ±ä¸€** â€” Benamou-Brenierå…¬å¼ã€Wassersteinå‹¾é…æµã§å…¨ã¦ãŒç¹‹ãŒã‚‹ã€‚

#### Q5: Flowã®ã€Œå¯é€†æ€§ã€ã€çµå±€ä½•ãŒå¬‰ã—ã„ã®ï¼Ÿ

**A**: 3ã¤ã®æœ¬è³ªçš„åˆ©ç‚¹ã€‚

**1. å³å¯†ãª $\log p(x)$ è¨ˆç®—**
- VAE: ELBO (ä¸‹ç•Œ) â†’ çœŸã®å°¤åº¦ã¯ä¸æ˜
- GAN: å°¤åº¦è¨ˆç®—ä¸å¯ â†’ å¯†åº¦æ¨å®šä¸å¯èƒ½
- **Flow**: Change of Variables ã§å³å¯† â†’ ç•°å¸¸æ¤œçŸ¥ã€ãƒ¢ãƒ‡ãƒ«é¸æŠã€ãƒ™ã‚¤ã‚ºæ¨è«–ã§å¿…é ˆ

**2. åŒæ–¹å‘å¤‰æ›**
- ãƒ‡ãƒ¼ã‚¿ç©ºé–“ $x$ â†” æ½œåœ¨ç©ºé–“ $z$ ã®å¯é€†ãƒãƒƒãƒ”ãƒ³ã‚°
- **é †æ–¹å‘** ($z \to x$): ç”Ÿæˆ (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)
- **é€†æ–¹å‘** ($x \to z$): ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (è¡¨ç¾å­¦ç¿’)
- ç”¨é€”: æ½œåœ¨ç©ºé–“ã§ã®è£œé–“ã€å±æ€§ç·¨é›†ã€ã‚¹ã‚¿ã‚¤ãƒ«è»¢ç§»

**3. è¨“ç·´ã®å®‰å®šæ€§**
- æœ€å°¤æ¨å®š (MLE) â†’ æ˜ç¢ºãªç›®çš„é–¢æ•°
- æ•µå¯¾çš„è¨“ç·´ä¸è¦ (GANã®ã‚ˆã†ãª mode collapse / ä¸å®‰å®šæ€§ãŒãªã„)
- åæŸæ€§ã®ç†è«–ä¿è¨¼

**ä»£å„Ÿ**:
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ¶ç´„ (Coupling Layerã¯å…¥åŠ›ã®åŠåˆ†ã‚’ã‚³ãƒ”ãƒ¼ â†’ æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯)
- ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã‚³ã‚¹ãƒˆ (Coupling/CNFã§ O(D) ã ãŒã€ä¾ç„¶ã¨ã—ã¦è¨ˆç®—å¿…è¦)

**Flow Matchingã®å†è§£é‡ˆ**:
- ã€Œå¯é€†æ€§ã€ã¯ç”Ÿæˆæ™‚ã®**çµŒè·¯ã®æ€§è³ª** (æ±ºå®šè«–çš„ODE)
- ã€Œå¯é€†æ€§ã€ã¯ãƒ¢ãƒ‡ãƒ«ã®**æ§‹é€ åˆ¶ç´„ã§ã¯ãªã„** (éå¯é€†ãƒ™ã‚¯ãƒˆãƒ«å ´ã‚’å­¦ç¿’å¯èƒ½)
- ODEã§ç©åˆ†ã™ã‚Œã°æ±ºå®šè«–çš„çµŒè·¯ â†’ å®Ÿè³ªçš„ã«ã€Œå¯é€†ã€

#### Q6: Course Iã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã€çµå±€ã“ã“ã§ä½•ã«ä½¿ã£ãŸï¼Ÿ

**A**: **å…¨ã¦ã®ç†è«–çš„åŸºç›¤**ã€‚

**å…·ä½“çš„ãªå¯¾å¿œ**:

| Course I (ç¬¬3-5å›) | æœ¬è¬›ç¾©ã§ã®ä½¿ç”¨ç®‡æ‰€ |
|:------------------|:----------------|
| **ç¬¬3å› æ¥µåº§æ¨™å¤‰æ›** | Zone 2.3 ã€Œåº§æ¨™å¤‰æ›ã€ã®æ¯”å–© â€” $p_{r,\theta} = p_{x,y} \cdot r$ |
| **ç¬¬4å› ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—** | Zone 3.1.2 å¤šæ¬¡å…ƒChange of Variables â€” $J_f = \frac{\partial \mathbf{f}}{\partial \mathbf{z}}$ |
| **ç¬¬4å› $\det$ ã®æ€§è³ª** | Zone 3.1.3 åˆæˆå¤‰æ› â€” $\det(AB) = \det(A) \det(B)$ |
| **ç¬¬4å› ç¢ºç‡å¤‰æ•°å¤‰æ›** | Zone 3.1 å®Œå…¨å°å‡º â€” $p_X(x) = p_Z(z) | \det J_f |^{-1}$ |
| **ç¬¬5å› ä¼Šè—¤ç©åˆ†ãƒ»SDE** | Zone 3.4.2 Instantaneous Change of Variables |
| **ç¬¬5å› å¸¸å¾®åˆ†æ–¹ç¨‹å¼** | Zone 4.2 CNF/FFJORDå®Ÿè£… (ode_solvers) |

**ã€Œãªãœã‚ã‚“ãªæŠ½è±¡çš„ãªæ•°å­¦ã‚’...ã€ã®ç­”ãˆ**:
- Normalizing Flowsã®å³å¯†ãªå°å‡ºã«**ä¸å¯æ¬ **
- ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãªã—ã§ã¯ $\log p(x)$ ã®è¨ˆç®—ä¸å¯èƒ½
- ç¬¬37-38å›ã§ã•ã‚‰ã«æ·±åŒ– (Fokker-Planckæ–¹ç¨‹å¼ã€JKOã‚¹ã‚­ãƒ¼ãƒ )

**æ¨å¥¨**: Course I ç¬¬3-5å›ã‚’å¾©ç¿’ã™ã‚‹ã¨ã€æœ¬è¬›ç¾©ãŒ**2å€ç†è§£ã§ãã‚‹**ã€‚ç‰¹ã«ç¬¬4å›ã€Œãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã¨ç¢ºç‡å¤‰æ•°å¤‰æ›ã€ã¯å¿…ä¿®ã€‚

#### Q7: ã€ŒFlow Matchingã§å¯é€†æ€§ä¸è¦ã€ãªã‚‰ã€ã‚‚ã†Flowã˜ã‚ƒãªã„ã®ã§ã¯ï¼Ÿ

**A**: **ç”¨èªã®å†å®šç¾©ãŒèµ·ãã¦ã„ã‚‹**ã€‚ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã®éæ¸¡æœŸã€‚

**ä¼çµ±çš„å®šç¾© (2014-2019)**:
- Normalizing Flow = å¯é€†å¤‰æ› $f_1, \ldots, f_K$ ã®åˆæˆ
- å¯é€†æ€§ = Flowã®**æœ¬è³ª** (Change of Variableså…¬å¼ã®å‰æ)
- $f^{-1}$ ãŒè¨ˆç®—å¯èƒ½ = å¿…é ˆæ¡ä»¶

**æ–°ã—ã„å®šç¾© (2022-)**:
- Flow = ãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t(x)$ ã«ã‚ˆã‚‹**è¼¸é€ (transport)**
- ODE $\frac{dx}{dt} = v_t(x)$ ã§çµŒè·¯ã‚’å®šç¾©
- å¯é€†æ€§ = æ±ºå®šè«–çš„ODEã®**æ€§è³ª** (ãƒ¢ãƒ‡ãƒ«åˆ¶ç´„ã§ã¯ãªã„)

**çµ±ä¸€çš„è¦–ç‚¹ (Optimal Transport)**:
- ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_0$ ã‹ã‚‰ãƒã‚¤ã‚ºåˆ†å¸ƒ $p_1$ ã¸ã®**æ¸¬åº¦ã®è¼¸é€**
- çµŒè·¯ = æ¸¬åº¦ã®æ™‚é–“ç™ºå±• (Continuity Equation)
- Wassersteinè·é›¢ã‚’æœ€å°åŒ– â† ç¬¬38å›ã§è©³èª¬

**è¨€è‘‰ã®æ•´ç†**:

| ç”¨èª | æ„å‘³ | æ–‡è„ˆ |
|:-----|:-----|:-----|
| **Normalizing Flow (ç‹­ç¾©)** | å¯é€†å¤‰æ›ã®åˆæˆ (RealNVP, Glow) | 2014-2019 |
| **Continuous Normalizing Flow** | Neural ODE-based Flow | 2018- |
| **Flow Matching** | ãƒ™ã‚¯ãƒˆãƒ«å ´å­¦ç¿’ (éå¯é€†OK) | 2022- |
| **Flow (åºƒç¾©)** | ODE-based transport å…¨èˆ¬ | ç¾åœ¨ã®çµ±ä¸€çš„ç†è§£ |

**çµè«–**:
- ã€ŒNormalizing Flowã€ã¨ã€ŒFlow Matchingã€ã¯**æ­´å²çš„ã«ã¯åˆ¥æ–‡è„ˆ**
- æ•°å­¦çš„ã«ã¯åŒã˜æ çµ„ã¿ (ODE-based transport)
- ç¬¬38å›ã§**å®Œå…¨çµ±ä¸€** â€” Optimal Transportè¦–ç‚¹ã§å…¨ã¦ãŒç¹‹ãŒã‚‹

**æ¯”å–©**: ã€ŒFlowã€ã¯ã€Œå·ã®æµã‚Œã€ã€‚å¾“æ¥ã¯ã€Œå¯é€†ãªæ°´è·¯ã€ã®ã¿æ‰±ã£ãŸã€‚Flow Matchingã¯ã€Œä»»æ„ã®ãƒ™ã‚¯ãƒˆãƒ«å ´ã«ã‚ˆã‚‹è¼¸é€ã€ã«ä¸€èˆ¬åŒ–ã€‚æœ¬è³ªã¯ã€Œæµã‚Œ (transport)ã€ãã®ã‚‚ã®ã€‚

#### Q8: å®Ÿè£…ã§æœ€ã‚‚è‹¦åŠ´ã™ã‚‹ãƒã‚¤ãƒ³ãƒˆã¯ï¼Ÿ

**A**: **3ã¤ã®è½ã¨ã—ç©´**ã€‚

**1. æ•°å€¤ä¸å®‰å®šæ€§**
- **å•é¡Œ**: $\exp(s)$ ãŒå¤§ãã™ãã‚‹ â†’ ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼
- **è§£æ±º**: $s$ ã‚’ `tanh` ã§ã‚¯ãƒªãƒƒãƒ— (Glowã®å®Ÿè£…)
  ```rust
  let s = s_net.forward(&z1)?.tanh()?;  // clamp to [-1, 1]
  ```

**2. é€†å¤‰æ›ã®æ¤œè¨¼**
- **å•é¡Œ**: $f^{-1}(f(z)) \neq z$ (å†æ§‹æˆèª¤å·®)
- **è§£æ±º**: ãƒ†ã‚¹ãƒˆã§æ¤œè¨¼
  ```rust
  let z_recon = model.inverse(&model.forward(&z)?.0)?.0;
  assert!(
      (&z - &z_recon)?.abs()?.max(0)?.to_scalar::<f32>()? < 1e-5,
      "Reconstruction error too large"
  );
  ```

**3. ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã®ãƒã‚°**
- **å•é¡Œ**: $\log |\det J|$ ã®ç¬¦å·ãƒŸã‚¹ã€æ¬¡å…ƒé›†ç´„ãƒŸã‚¹
- **è§£æ±º**: å˜ç´”ãªã‚±ãƒ¼ã‚¹ (Affineå¤‰æ›) ã§æ‰‹è¨ˆç®—ã¨æ¯”è¼ƒ
  ```rust
  // Affine: f(z) = 2z + 1 â†’ log|det J| = log(2)
  let expected = 2.0_f64.ln();
  assert!((log_det_jacobian - expected).abs() < 1e-10,
          "log|det J| should equal log(2)");
  ```

**ãƒ‡ãƒãƒƒã‚°ã®ã‚³ãƒ„**:
- 1D â†’ 2D â†’ é«˜æ¬¡å…ƒã®é †ã§å®Ÿè£…
- å„å±¤ã®å‡ºåŠ›ã‚’å¯è¦–åŒ–
- RealNVPã‹ã‚‰å§‹ã‚ã€Glowã¯å¾Œå›ã—

#### Q9: Flowã‚’ä½¿ã£ãŸç•°å¸¸æ¤œçŸ¥ã€ã©ã†å®Ÿè£…ã™ã‚‹ï¼Ÿ

**A**: **3ã‚¹ãƒ†ãƒƒãƒ—**ã€‚

**Step 1: æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´**
```rust
// Normal data only
let x_normal = load_normal_data(&device)?;

// Train RealNVP
let var_map = VarMap::new();
let vb = VarBuilder::from_varmap(&var_map, DType::F32, &device);
let model = RealNVP::new(d, 6, 64, &vb)?;
let mut opt = AdamW::new(var_map.all_vars(), ParamsAdamW { lr: 1e-3, ..Default::default() })?;
train_realnvp(&model, &mut opt, &x_normal, 100, 256)?;
```

**Step 2: é–¾å€¤è¨­å®š (Validation Set)**
```rust
// Compute log p(x) on validation set
let log_p_val: Vec<f32> = eval_log_p(&model, &x_val)?;

// Set threshold at 5th percentile (lower 5% = anomaly)
let mut sorted = log_p_val.clone();
sorted.sort_by(|a, b| a.partial_cmp(b).unwrap());
let threshold = sorted[(sorted.len() as f32 * 0.05) as usize];
```

**Step 3: æ¨è«–æ™‚ã®ç•°å¸¸åˆ¤å®š**
```rust
fn is_anomaly(model: &RealNVP, x_test: &Tensor, threshold: f32) -> candle_core::Result<Vec<bool>> {
    let log_p = eval_log_p(model, x_test)?;  // Vec<f32>
    Ok(log_p.iter().map(|&lp| lp < threshold).collect())
}

// Test
let anomaly_flags = is_anomaly(&model, &x_test, threshold)?;
```

**å®Ÿä¾‹ (Zone 5.4)**:
- 2D Moons (æ­£å¸¸) vs Uniform noise (ç•°å¸¸)
- Accuracy: 95-98%
- VAEã®ELBOã§ã¯é–¾å€¤è¨­å®šãŒå›°é›£ (Gapä¸æ˜)

**ç”£æ¥­å¿œç”¨**:
- è£½é€ æ¥­: ä¸è‰¯å“æ¤œçŸ¥
- åŒ»ç™‚: ç¨€ãªç–¾æ‚£ã®æ¤œå‡º
- ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ç•°å¸¸é€šä¿¡æ¤œçŸ¥

#### Q10: æ¬¡ã«å­¦ã¶ã¹ãã“ã¨ã¯ï¼Ÿ

**A**: **Course IV ã®è«–ç†çš„ãƒã‚§ãƒ¼ãƒ³ã‚’è¾¿ã‚‹**ã€‚

**æ¨å¥¨å­¦ç¿’é †**:

1. **ç¬¬34å› (EBM)** â€” æ­£è¦åŒ–å®šæ•° $Z$ ã®å›é¿
   - ãªãœ $p(x) = \frac{1}{Z} e^{-E(x)}$ ã‹ï¼Ÿ
   - Hopfield Network â†” Transformer Attention
   - Contrastive Divergence

2. **ç¬¬35å› (Score Matching)** â€” $\nabla \log p(x)$ ã®ã¿å­¦ç¿’
   - $Z$ ãŒæ¶ˆãˆã‚‹æ•°å­¦
   - Denoising Score Matching
   - Langevin MCMC

3. **ç¬¬37å› (SDE/ODE)** â€” é€£ç¶šæ‹¡æ•£ã®æ•°å­¦
   - VP-SDE, VE-SDE
   - ä¼Šè—¤ç©åˆ†ã€Fokker-Planckæ–¹ç¨‹å¼
   - **Probability Flow ODE** (Diffusion â†” Flowæ¥ç¶š)

4. **ç¬¬38å› (Flow Matching)** â€” **æœ€é‡è¦**
   - Optimal Transport
   - JKO scheme (Wassersteinå‹¾é…æµ)
   - **Flowã¨Diffusionã®æ•°å­¦çš„ç­‰ä¾¡æ€§ã®è¨¼æ˜**
   - Rectified Flowå®Ÿè£…

5. **ç¬¬36å› (DDPM)** â€” ãƒã‚¤ã‚ºé™¤å»ã®åå¾©
   - Forward/Reverse Markové€£é–
   - å¤‰åˆ†ä¸‹ç•Œ (VLB)
   - U-Netå®Ÿè£…

**ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½ vs å¿…é ˆ**:
- **ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½**: ç¬¬34å› (EBM) â€” Flowã®æ–‡è„ˆã§ã¯è£œè¶³çš„
- **å¿…é ˆ**: ç¬¬35å› (Score) â†’ ç¬¬37å› (SDE/ODE) â†’ ç¬¬38å› (Flow Matching)
  - ã“ã®3ã¤ãŒã€ŒFlow â†’ Diffusion â†’ çµ±ä¸€ã€ã®æ ¸å¿ƒ

**ä¸¦è¡Œå­¦ç¿’**:
- Optimal Transport (ç¬¬6å›ã®å¾©ç¿’ + ç™ºå±•)
- æ¸¬åº¦è«–ã®åŸºç¤ (Continuity Equation, Wassersteinè·é›¢)

**å®Ÿè£…å„ªå…ˆãªã‚‰**:
- ç¬¬36å› (DDPM) â†’ ç¬¬38å› (Flow Matching) â†’ Rectified Flowå®Ÿè£…

### 7.3 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

**æœ¬è¬›ç¾©ã®ç†è§£åº¦ã‚’ãƒã‚§ãƒƒã‚¯**ã€‚å…¨å•æ­£è§£ã§**æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¸é€²ã‚€è³‡æ ¼**ã€‚

#### Level 1: åŸºç¤ (Zone 0-2)

**Q1**: Change of Variableså…¬å¼ $p_X(x) = p_Z(z) |\det J_f|^{-1}$ ã§ã€$\det J_f$ ã®ç‰©ç†çš„æ„å‘³ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

ä½“ç©è¦ç´ ã®å¤‰åŒ–ç‡ã€‚$z$ ç©ºé–“ã®å¾®å°ä½“ç© $dz$ ãŒã€å¤‰æ› $f$ ã«ã‚ˆã£ã¦ $x$ ç©ºé–“ã§ $|\det J_f| dz$ ã«å¤‰åŒ–ã™ã‚‹ã€‚ç¢ºç‡å¯†åº¦ã¯ã€Œå˜ä½ä½“ç©ã‚ãŸã‚Šã®ç¢ºç‡ã€ãªã®ã§ã€é€†æ•° $|\det J_f|^{-1}$ ã‚’ã‹ã‘ã‚‹ã€‚

</details>

**Q2**: VAE, GAN, Normalizing Flowã®å°¤åº¦è¨ˆç®—èƒ½åŠ›ã‚’æ¯”è¼ƒã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

- **VAE**: ELBO (å¤‰åˆ†ä¸‹ç•Œ) â€” $\log p(x)$ ã®ä¸‹ç•Œã®ã¿ã€çœŸã®å€¤ã¯ä¸æ˜
- **GAN**: æš—é»™çš„å¯†åº¦ â€” $\log p(x)$ è¨ˆç®—ä¸å¯
- **Normalizing Flow**: å³å¯†ãª $\log p(x)$ â€” Change of Variableså…¬å¼ã§è¨ˆç®—

</details>

**Q3**: Flowã®ã€Œæ­£è¦åŒ– (Normalizing)ã€ã¯ä½•ã‚’æ­£è¦åŒ–ã—ã¦ã„ã‚‹ã®ã‹ï¼Ÿ

<details><summary>è§£ç­”</summary>

ç¢ºç‡åˆ†å¸ƒã‚’æ­£è¦åŒ– (ç©åˆ†ãŒ1ã«ãªã‚‹ã‚ˆã†)ã€‚åŸºåº•åˆ†å¸ƒ $q(z)$ (é€šå¸¸ã‚¬ã‚¦ã‚¹) ã‚’å¤‰æ›ã—ã¦ã€è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p(x)$ ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã€Change of Variablesã§è‡ªå‹•çš„ã« $\int p(x) dx = 1$ ãŒä¿è¨¼ã•ã‚Œã‚‹ (ã€Œæ­£è¦åŒ–æµã€ã®åå‰ã®ç”±æ¥)ã€‚

</details>

#### Level 2: æ•°å¼ (Zone 3)

**Q4**: Coupling Layerã§ $\log |\det J|$ ãŒ O(D) ã§è¨ˆç®—ã§ãã‚‹ç†ç”±ã‚’ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—ã®æ§‹é€ ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

Coupling Layerã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³:

$$
J = \begin{bmatrix}
I_d & 0 \\
\frac{\partial x_2}{\partial z_1} & \text{diag}(\exp(s(z_1)))
\end{bmatrix}
$$

ä¸‹ä¸‰è§’ãƒ–ãƒ­ãƒƒã‚¯è¡Œåˆ— â†’ $\det J = \det(I_d) \cdot \det(\text{diag}(\exp(s))) = \prod_i \exp(s_i) = \exp(\sum s_i)$ã€‚$\log |\det J| = \sum s_i$ (O(D) ã®å’Œ)ã€‚

</details>

**Q5**: FFJORDã®Hutchinson traceæ¨å®š $\text{tr}(A) = \mathbb{E}[\mathbf{v}^\top A \mathbf{v}]$ ã§ã€$\mathbf{v}$ ã®åˆ†å¸ƒã®æ¡ä»¶ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

$\mathbb{E}[\mathbf{v}] = 0$, $\text{Cov}(\mathbf{v}) = I$ ã‚’æº€ãŸã™ä»»æ„ã®åˆ†å¸ƒã€‚æ¨™æº–ã‚¬ã‚¦ã‚¹ $\mathcal{N}(0, I)$ ã¾ãŸã¯Rademacheråˆ†å¸ƒ (å„è¦ç´ ãŒ $\pm 1$ with prob 0.5) ãŒä¸€èˆ¬çš„ã€‚

</details>

**Q6**: Adjoint Methodã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒ O(1) ã§ã‚ã‚‹ç†ç”±ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

é †ä¼æ’­æ™‚ã«ä¸­é–“çŠ¶æ…‹ã‚’ä¿å­˜ã—ãªã„ã€‚é€†ä¼æ’­æ™‚ã«ã€adjoint state $\mathbf{a}(t)$ ã®ODEã‚’é€†æ™‚é–“ã§è§£ããªãŒã‚‰å‹¾é…ã‚’è¨ˆç®—ã€‚å¿…è¦ã«å¿œã˜ã¦ODEã‚’å†è¨ˆç®— (checkpointing)ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•: ãƒ¡ãƒ¢ãƒª O(1) â†” è¨ˆç®—æ™‚é–“ 2Ã— (é †ä¼æ’­1å› + é€†ä¼æ’­1å›)ã€‚

</details>

#### Level 3: å®Ÿè£… (Zone 4-5)

**Q7**: RealNVPã®è¨“ç·´ã§ã€ãªãœ inverse â†’ forward ã®é †ã§è¨ˆç®—ã™ã‚‹ã®ã‹ï¼Ÿ

<details><summary>è§£ç­”</summary>

è¨“ç·´ãƒ‡ãƒ¼ã‚¿ $x$ ã‹ã‚‰ $\log p(x)$ ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€‚
1. Inverse: $x \to z = f^{-1}(x)$
2. Forward: $z \to x$ ã‚’å†è¨ˆç®—ã—ã€$\log |\det J|$ ã‚’ç´¯ç©
3. $\log p(x) = \log q(z) - \log |\det J|$

ç”Ÿæˆæ™‚ (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°) ã¯ Forward ã®ã¿: $z \sim q(z) \to x = f(z)$ã€‚

</details>

**Q8**: 2D Moons datasetã§ã€FlowãŒVAEã‚ˆã‚Šé«˜ã„ $\log p(x)$ ã‚’é”æˆã™ã‚‹ç†ç”±ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

- **Flow**: å³å¯†ãª $\log p(x)$ â€” Change of Variables ã§çœŸã®å¯†åº¦ã«è¿‘ã„æ¨å®š
- **VAE**: ELBO (ä¸‹ç•Œ) â€” $\log p(x) \geq \text{ELBO}$ã€å¸¸ã«çœŸã®å€¤ã‚ˆã‚Šå°ã•ã„
- Gap = KL(q(z|x) || p(z|x)) (VAEã®è¿‘ä¼¼èª¤å·®)

å®Ÿé¨“çµæœ: Flow ~2.35, VAE ~1.89 (Gap ~0.46)ã€‚

</details>

**Q9**: Out-of-Distributionæ¤œçŸ¥ã§ã€FlowãŒé–¾å€¤è¨­å®šã—ã‚„ã™ã„ç†ç”±ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

Flowã¯**å³å¯†ãª $\log p(x)$** ã‚’è¨ˆç®— â†’ In-distã¨OODã®åˆ†é›¢ãŒæ˜ç¢ºã€‚

- In-dist: $\log p(x)$ é«˜ã„ (ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«è¿‘ã„)
- OOD: $\log p(x)$ ä½ã„ (ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‹ã‚‰é ã„)

VAEã®ELBOã§ã¯ã€Gap (KL divergence) ãŒä¸æ˜ â†’ é–¾å€¤è¨­å®šãŒæ›–æ˜§ã€‚

</details>

#### Level 4: ç™ºå±• (Zone 6)

**Q10**: Probability Flow ODE (PF-ODE) ãŒã€ŒDiffusionã®æ±ºå®šè«–çš„ç‰ˆã€ã§ã‚ã‚‹ç†ç”±ã‚’ã€SDEã¨ã®é–¢ä¿‚ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

Diffusion Reverse SDE:

$$
dx = [f(x, t) - g(t)^2 \nabla \log p_t(x)] dt + g(t) dw
$$

PF-ODE (æ±ºå®šè«–çš„):

$$
\frac{dx}{dt} = f(x, t) - \frac{1}{2} g(t)^2 \nabla \log p_t(x)
$$

ãƒ‰ãƒªãƒ•ãƒˆé …ã‚’èª¿æ•´ ($g^2 \nabla \log p_t$ ã®ä¿‚æ•°ã‚’ $1 \to \frac{1}{2}$)ã€æ‹¡æ•£é … $g(t) dw$ ã‚’é™¤å»ã€‚ã“ã®ODEã‚’ $t=T \to 0$ ã«ç©åˆ†ã™ã‚‹ã¨ã€SDEã¨**åŒã˜å‘¨è¾ºåˆ†å¸ƒ** $p_t(x)$ ãŒå¾—ã‚‰ã‚Œã‚‹ (Song et al. 2021 è¨¼æ˜)ã€‚

</details>

**Q11**: Rectified Flowã§ã€Œç›´ç·šè¼¸é€ã€ãŒæœ€é©ã§ã‚ã‚‹ç†ç”±ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

Optimal Transportç†è«–ã‚ˆã‚Šã€Wasserstein-2è·é›¢ã‚’æœ€å°åŒ–ã™ã‚‹è¼¸é€çµŒè·¯ã¯**ç›´ç·š** (geodesic)ã€‚

$x_t = (1-t) x_0 + t z$ ã¯ã€ãƒ‡ãƒ¼ã‚¿ç‚¹ $x_0$ ã¨ãƒã‚¤ã‚º $z$ ã‚’ç›´ç·šã§çµã¶æœ€çŸ­çµŒè·¯ â†’ Wassersteinè·é›¢æœ€å° â†’ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°æœ€å° (10-50 steps)ã€‚

</details>

**Q12**: Flow Matchingã¨Normalizing Flowsã®ã€Œå¯é€†æ€§ã€ã«å¯¾ã™ã‚‹è€ƒãˆæ–¹ã®é•ã„ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

| è¦³ç‚¹ | Normalizing Flows (ä¼çµ±) | Flow Matching (æ–°) |
|:-----|:------------------------|:------------------|
| **å¯é€†æ€§** | ãƒ¢ãƒ‡ãƒ«ã®**æ§‹é€ åˆ¶ç´„** | çµŒè·¯ã®**æ€§è³ª** |
| **è¨“ç·´æ™‚** | $f, f^{-1}$ ä¸¡æ–¹è¨ˆç®—å¯èƒ½ | ãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t$ (éå¯é€†OK) |
| **æ¨è«–æ™‚** | Forward: $z \to x = f(z)$ | ODEç©åˆ† (æ±ºå®šè«–çš„çµŒè·¯) |
| **çµæœ** | Coupling Layerç­‰ã®åˆ¶ç´„ | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è‡ªç”± |

Flow Matchingã®æ´å¯Ÿ: ã€Œå¯é€†æ€§ã€ã¯æ±ºå®šè«–çš„ODEã®æ€§è³ª (åŒã˜åˆæœŸæ¡ä»¶ â†’ åŒã˜çµŒè·¯)ã€‚ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã¯éå¯é€†ã§ã‚‚OKã€‚

</details>

**å…¨å•æ­£è§£ãªã‚‰** â†’ **Course IV ç¬¬34-38å›ã¸é€²ã‚€æº–å‚™å®Œäº†**ï¼

---

> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. CNF ã¨ Flow Matching ã®æ•°å­¦çš„æ¥ç¶šã‚’1ã¤ã®å¼ã§è¡¨ç¾ã—ã€FMãŒNFã‚’ã€ŒåŒ…å«ã™ã‚‹ã€æ„å‘³ã‚’è¿°ã¹ã‚ˆã€‚
>    - *ãƒ’ãƒ³ãƒˆ*: CNF ã® `tr(âˆ‚f/âˆ‚z)` ã¨ FM ã® `â€–v_t - u_tâ€–Â²` ã®ã©ã¡ã‚‰ãŒæœ€é©åŒ–ã—ã‚„ã™ã„ã‹ã€è¨ˆç®—é‡ã®è¦³ç‚¹ã§æ¯”è¼ƒã›ã‚ˆã€‚
> 2. NFâ†’EBMâ†’Scoreâ†’DDPMã®å¯†åº¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒã‚§ãƒ¼ãƒ³ã§ã€å„æ‰‹æ³•ãŒå‰æ‰‹æ³•ã®ä½•ã®å›°é›£ã‚’ã€Œè§£æ±ºã€ã—ã¦ã„ã‚‹ã‹ä¸€è¡Œãšã¤è¿°ã¹ã‚ˆã€‚
>    - *ãƒ’ãƒ³ãƒˆ*: NF ã®ã€Œãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã€â†’ EBM ã®ã€Œåˆ†é…é–¢æ•°ã€â†’ Score ã®ã€Œä½•ï¼Ÿã€â†’ DDPM ã®ã€Œä½•ï¼Ÿã€ã¨ã„ã†é †ã«è€ƒãˆã‚ˆã€‚

## ğŸŒ€ Paradigm-Breaking Question

> **ã€Œå¯é€†æ€§ã‚’æ¨ã¦ã‚Œã°ã€Flowã¯ã‚‚ã£ã¨è¡¨ç¾åŠ›ãŒä¸ŠãŒã‚‹ã®ã§ã¯ï¼Ÿã€**

### ä¼çµ±çš„ç­”ãˆ (2014-2019)

**ä¸»å¼µ**: å¯é€†æ€§ = Flowã®æœ¬è³ªã€‚æ¨ã¦ãŸã‚‰Flowã§ã¯ãªã„ã€‚

**æ ¹æ‹ **:
1. Change of VariablesãŒä½¿ãˆãªããªã‚‹ â†’ $\log p(x)$ è¨ˆç®—ä¸å¯
2. é€†å¤‰æ› $f^{-1}$ ãŒãªã„ã¨æ½œåœ¨ç©ºé–“ã¸ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¸å¯
3. Coupling Layerã®åˆ¶ç´„ã¯ä»•æ–¹ãªã„ (ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ã®ãŸã‚)

**çµè«–**: å¯é€†æ€§ã¯ã€Œã‚³ã‚¹ãƒˆã€ã§ã¯ãªãã€Œæœ¬è³ªçš„ç‰¹å¾´ã€ã€‚

### 2023å¹´ã®ç­”ãˆ (Flow Matching)

**ä¸»å¼µ**: **Flow Matchingã¯éå¯é€†ãƒ™ã‚¯ãƒˆãƒ«å ´ã‚’å­¦ç¿’å¯èƒ½**ã€‚

**å®Ÿä¾‹**:
- è¨“ç·´æ™‚: ä»»æ„ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ $v_\theta(x, t)$ ã‚’å­¦ç¿’ (å¯é€†æ€§ä¸è¦)
- æ¨è«–æ™‚: ODEã§ç©åˆ† $\frac{dx}{dt} = v_\theta(x, t)$ â†’ çµŒè·¯ã¯æ±ºå®šè«–çš„ (å®Ÿè³ªçš„ã«å¯é€†)

**æ´å¯Ÿ**:
- ã€Œå¯é€†æ€§ã€ã¯ç”Ÿæˆæ™‚ã®**çµŒè·¯ã®æ€§è³ª** (æ±ºå®šè«–çš„ODE)
- ã€Œå¯é€†æ€§ã€ã¯ãƒ¢ãƒ‡ãƒ«ã®**åˆ¶ç´„ã§ã¯ãªã„** (Coupling Layerã®ã‚ˆã†ãªæ§‹é€ åˆ¶ç´„ãŒä¸è¦)

### Diffusion Modelsã®è¦–ç‚¹

**Diffusionã¯ã€Œå¯é€†æ€§ã‚’æ¨ã¦ãŸã€Flow**:

| è¦³ç‚¹ | Normalizing Flow (ä¼çµ±) | Diffusion Model |
|:-----|:----------------------|:---------------|
| **Forward** | å­¦ç¿’å¯¾è±¡ ($f$ ã‚’å­¦ç¿’) | å›ºå®š (ãƒã‚¤ã‚ºè¿½åŠ ) |
| **Reverse** | $f^{-1}$ (è§£æçš„) | å­¦ç¿’å¯¾è±¡ ($\epsilon_\theta$ ã‚’å­¦ç¿’) |
| **å¯é€†æ€§** | å¿…é ˆ | ä¸è¦ (Forward ã¯éå¯é€†) |
| **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** | Coupling Layer (åˆ¶ç´„ã‚ã‚Š) | U-Net/Transformer (è‡ªç”±) |
| **ç”Ÿæˆå“è³ª** | ä¸­ç¨‹åº¦ | **SOTA** |

**Diffusionã®æˆåŠŸãŒè¨¼æ˜**: å¯é€†æ€§ã‚’æ¨ã¦ã‚‹ã“ã¨ã§ã€è¡¨ç¾åŠ›ãŒ**åŠ‡çš„ã«å‘ä¸Š**ã€‚

### çµ±ä¸€çš„è¦–ç‚¹ (Optimal Transport)

**Flow (åºƒç¾©) = ãƒ™ã‚¯ãƒˆãƒ«å ´ã«ã‚ˆã‚‹è¼¸é€ (transport)**ã€‚

**Benamou-Brenierå…¬å¼** (ç¬¬38å›ã§è©³èª¬):

æ¸¬åº¦ $p_0$ ã‹ã‚‰ $p_1$ ã¸ã®è¼¸é€çµŒè·¯ã¯ã€æ¬¡ã®æœ€é©åŒ–å•é¡Œã®è§£:

$$
\min_{v_t} \int_0^1 \int \| v_t(x) \|^2 p_t(x) dx dt
$$

åˆ¶ç´„: Continuity Equation (æ¸¬åº¦ã®ä¿å­˜å‰‡)

$$
\frac{\partial p_t}{\partial t} + \nabla \cdot (p_t v_t) = 0
$$

**é‡è¦**: ã“ã®æ çµ„ã¿ã«ã€Œå¯é€†æ€§ã€ã¯**ä¸è¦**ã€‚ãƒ™ã‚¯ãƒˆãƒ«å ´ $v_t(x)$ ãŒå®šç¾©ã§ãã‚Œã°ååˆ†ã€‚

### ç­”ãˆ

**ä¼çµ±çš„Normalizing Flows**: å¯é€†æ€§ = æœ¬è³ª â†’ æ­£ã—ã„ãŒã€**ç‹­ã™ããŸ**ã€‚

**Flow Matching**: å¯é€†æ€§ = çµŒè·¯ã®æ€§è³ª (æ±ºå®šè«–çš„ODE) â†’ ã‚ˆã‚Šä¸€èˆ¬çš„ãªç†è§£ã€‚

**çµ±ä¸€çš„è¦–ç‚¹**:
- ã€Œå¯é€†å¤‰æ›ã€ã‹ã‚‰ã€Œãƒ™ã‚¯ãƒˆãƒ«å ´ã«ã‚ˆã‚‹è¼¸é€ã€ã¸
- Wassersteinè·é›¢ã‚’æœ€å°åŒ–ã™ã‚‹çµŒè·¯ = æœ€é©è¼¸é€
- **Flowã¨Diffusionã¯åŒã˜æ çµ„ã¿** (æ¸¬åº¦ã®æ™‚é–“ç™ºå±•)

**ç¬¬38å›ã§å®Œå…¨è§£ç­”**:
- Benamou-Brenierå…¬å¼
- JKO scheme (Wassersteinå‹¾é…æµ)
- **ã€Œå…¨ã¦ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯è¼¸é€å•é¡Œã€ã®è¨¼æ˜**

**ã“ã“ã§ã®å­¦ã³**: ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®**å¢ƒç•Œã‚’å•ã„ç¶šã‘ã‚‹**ã“ã¨ãŒã€æ¬¡ã®ç†è«–ã‚’ç”Ÿã‚€ã€‚ã€Œå¯é€†æ€§ã¨ã¯ä½•ã‹ï¼Ÿã€ã€ŒFlowã¨ã¯ä½•ã‹ï¼Ÿã€â€” ã“ã®å•ã„ãŒã€Flow Matchingã¨ã„ã†çµ±ä¸€ç†è«–ã‚’å°ã„ãŸã€‚

---

## å‚è€ƒæ–‡çŒ®

[^1]: Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. *ICML*.
<https://arxiv.org/abs/1505.05770>

[^2]: Dinh, L., Krueger, D., & Bengio, Y. (2014). NICE: Non-linear Independent Components Estimation. *ICLR Workshop*.
<https://arxiv.org/abs/1410.8516>

[^3]: Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2016). Density Estimation using Real NVP. *ICLR*.
<https://arxiv.org/abs/1605.08803>

[^4]: Kingma, D. P., & Dhariwal, P. (2018). Glow: Generative Flow with Invertible 1x1 Convolutions. *NeurIPS*.
<https://arxiv.org/abs/1807.03039>

[^5]: Chen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural Ordinary Differential Equations. *NeurIPS*.
<https://arxiv.org/abs/1806.07366>

[^6]: Grathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, I., & Duvenaud, D. (2019). FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models. *ICLR*.
<https://arxiv.org/abs/1810.01367>

[^7]: Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows (Planar Flow). *ICML*.
<https://arxiv.org/abs/1505.05770>

[^8]: Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., & Welling, M. (2016). Improved Variational Inference with Inverse Autoregressive Flow. *NeurIPS*.
<https://arxiv.org/abs/1606.04934>

[^9]: Liu, X., Gong, C., & Liu, Q. (2022). Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. *ICLR 2023*.
<https://arxiv.org/abs/2209.03003>

[^10]: Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., & Le, M. (2023). Flow Matching for Generative Modeling. *ICLR*.
<https://arxiv.org/abs/2210.02747>

[^11]: Chen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural Ordinary Differential Equations (Adjoint Method). *NeurIPS*.
<https://arxiv.org/abs/1806.07366>

[^12]: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. *NeurIPS*.
<https://arxiv.org/abs/2006.11239>

[^13]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). Score-Based Generative Modeling through Stochastic Differential Equations. *ICLR*.
<https://arxiv.org/abs/2011.13456>

[^14]: Zhai, S., et al. (2024). "Normalizing Flows are Capable Generative Models". *arXiv:2412.06329*.
<https://arxiv.org/abs/2412.06329>

[^15]: Hickling, T., & Prangle, D. (2024). Flexible Tails for Normalizing Flows.
<https://arxiv.org/abs/2406.16971>

---

**æ¬¡å›äºˆå‘Š**: ç¬¬34å› â€” **Energy-Based Models & çµ±è¨ˆç‰©ç†**ã€‚$p(x) = \frac{1}{Z} e^{-E(x)}$ ã®Gibbsåˆ†å¸ƒã€Hopfield Networkã¨Transformerã®ç­‰ä¾¡æ€§ã€Contrastive Divergenceã€Langevin Dynamicsã€‚æ­£è¦åŒ–å®šæ•° $Z$ ã¨ã®æˆ¦ã„ãŒå§‹ã¾ã‚‹ã€‚ãã—ã¦ç¬¬35å›ã§ $Z$ ãŒæ¶ˆãˆã‚‹ç¬é–“ã‚’ç›®æ’ƒã™ã‚‹ â€” **Score Matching**ã€‚

**ç¬¬33å›ã®ä½ç½®ä»˜ã‘ã¾ã¨ã‚**: Normalizing Flow ã¯ã€Œå³å¯†ãªå°¤åº¦è¨ˆç®—ã€ã¨ã„ã†å”¯ä¸€ç„¡äºŒã®ç‰¹æ€§ã‚’æŒã¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã ã€‚VAE ã¯è¿‘ä¼¼ï¼ˆELBOï¼‰ã€GAN ã¯æš—é»™çš„ï¼ˆå°¤åº¦ãªã—ï¼‰ã§ã‚ã‚‹ã®ã«å¯¾ã—ã€Flow ã ã‘ãŒ $\log p(x)$ ã‚’è§£æçš„ã«è¨ˆç®—ã§ãã‚‹ã€‚ã“ã®ç‰¹æ€§ã¯å¯†åº¦æ¨å®šãƒ»ç•°å¸¸æ¤œçŸ¥ãƒ»ãƒ™ã‚¤ã‚ºæ¨è«–ã§ä»Šå¾Œã‚‚ä¸å¯æ¬ ã§ã‚ã‚Šç¶šã‘ã‚‹ã€‚Flow Matching ã¨ã—ã¦é€²åŒ–ã—ãŸç¾åœ¨ã€ãã®æ•°å­¦çš„åŸºç›¤ã¯ã•ã‚‰ã«å¤šãã®æ‰‹æ³•ã‚’ã€ŒåŒ…å«ã€ã—ã¤ã¤ã‚ã‚‹ã€‚

Course IV ã®æ—…ã¯ã¾ã å§‹ã¾ã£ãŸã°ã‹ã‚Šã€‚ç¬¬33å›ã§å¾—ãŸã€ŒChange of Variablesã€ã®æ•°å­¦ãŒã€ç¬¬37-38å›ã§**Diffusion Models**ã¨èåˆã—ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ã®**çµ±ä¸€**ã¸ã¨å‘ã‹ã†ã€‚æ¬¡ã®è¬›ç¾©ã§ä¼šãŠã†ã€‚

> **âš ï¸ Warning:** ç¬¬33å›ã§å®Ÿè£…ã—ãŸ RealNVP ã¯ã€Œå­¦ç¿’ç”¨ã€å®Ÿè£…ã§ã‚ã‚Šã€æœ¬ç•ªåˆ©ç”¨ã«ã¯ä¸ååˆ†ãªç‚¹ãŒã‚ã‚‹ã€‚å…·ä½“çš„ã«ã¯: (1) æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã® `clamp` ãŒæœªå®Ÿè£…ã€(2) Half-precision (fp16) æœªå¯¾å¿œã€(3) ãƒãƒƒãƒæ­£è¦åŒ–ã® running statistics ãŒæ¨è«–æ™‚ã«å›ºå®šã•ã‚Œã¦ã„ãªã„ã€ãªã©ã®å•é¡ŒãŒã‚ã‚‹ã€‚Production ã§ã® Flow å®Ÿè£…ã¯ Candle ã®å…¬å¼ã‚µãƒ³ãƒ—ãƒ«ã‹ Normalizing Flows.jl ã‚’å‚ç…§ã®ã“ã¨ã€‚

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
