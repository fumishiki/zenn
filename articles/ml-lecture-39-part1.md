---
title: "ç¬¬39å›: Latent Diffusion Models: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ–¼ï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "ldm", "julia", "stablediffusion"]
published: true
---

# ç¬¬39å›: ğŸ–¼ï¸ Latent Diffusion Models

:::message
**å‰å›ã®åˆ°é”ç‚¹**: ç¬¬38å›ã§Score/Flow/Diffusionã®æ•°å­¦çš„ç­‰ä¾¡æ€§ã‚’è¨¼æ˜ã—ã€çµ±ä¸€ç†è«–ãŒå®Œæˆã—ãŸã€‚ç†è«–ã ã‘ã§ã¯ç”»åƒã¯ç”Ÿæˆã§ããªã„ â€” ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“æ‹¡æ•£ã®è¨ˆç®—é™ç•Œã‚’è¶…ãˆã‚‹æ½œåœ¨ç©ºé–“æ‹¡æ•£ã¨ã€ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆã¸ã€‚
:::

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ãƒ”ã‚¯ã‚»ãƒ« vs æ½œåœ¨ç©ºé–“ã®è¡æ’ƒ

```julia
using Lux, Random

# ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“æ‹¡æ•£: 512x512x3 = 786,432æ¬¡å…ƒ
pixel_dim = 512 * 512 * 3
pixel_diffusion_params = pixel_dim * 1000  # 7å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿...

# VAE latent space: 64x64x4 = 16,384æ¬¡å…ƒ (48xåœ§ç¸®!)
latent_dim = 64 * 64 * 4
latent_diffusion_params = latent_dim * 1000  # 1640ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

compression_ratio = pixel_dim / latent_dim
speedup = compression_ratio^2  # è¨ˆç®—é‡ã¯O(NÂ²)

println("Compression: $(round(compression_ratio, digits=1))x")
println("Theoretical speedup: $(round(speedup, digits=1))x")
# Output:
# Compression: 48.0x
# Theoretical speedup: 2304.0x
```

**æ•°å¼ã®æ­£ä½“**:
$$
\begin{aligned}
\text{Pixel Diffusion: } &x \in \mathbb{R}^{512 \times 512 \times 3} \quad (\approx 786\text{Kæ¬¡å…ƒ}) \\
\text{Latent Diffusion: } &z \in \mathbb{R}^{64 \times 64 \times 4} \quad (\approx 16\text{Kæ¬¡å…ƒ}) \\
\text{Compression: } &f = \frac{512^2 \times 3}{64^2 \times 4} = 48\times \\
\text{Speedup: } &\mathcal{O}(f^2) \approx 2304\times
\end{aligned}
$$

**ã“ã®30ç§’ã§ä½“æ„Ÿã—ãŸã“ã¨**: æ¬¡å…ƒå‰Šæ¸›ãŒè¨ˆç®—é‡ã‚’ **2000å€** å‰Šæ¸›ã€‚Stable DiffusionãŒæ¶ˆè²»è€…GPUã§å‹•ãç†ç”±ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®3%å®Œäº†ï¼** ã“ã‚Œã‹ã‚‰æ½œåœ¨ç©ºé–“ã®æ•°å­¦çš„åŸºç›¤ã¨ã€ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆã®å®Œå…¨ç†è«–ã¸ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ãªãœæ½œåœ¨ç©ºé–“ã‹

### ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“æ‹¡æ•£ã®é™ç•Œ

ç¬¬36å›ã§å­¦ã‚“ã DDPMã¯ç¾ã—ã„ç†è«–ã ãŒã€è¨ˆç®—é™ç•ŒãŒã‚ã‚‹:

| é …ç›® | 256Ã—256 DDPM | 512Ã—512 DDPM | 1024Ã—1024 DDPM |
|:-----|:-------------|:-------------|:----------------|
| **å…¥åŠ›æ¬¡å…ƒ** | 196,608 | 786,432 | 3,145,728 |
| **U-Net params** | ~100M | ~500M | ~2B |
| **è¨“ç·´æ™‚é–“/iter** | ~1ç§’ | ~5ç§’ | ~20ç§’ |
| **V100 VRAM** | 12GB | 32GB | 80GB (ä¸å¯èƒ½) |
| **åæŸã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** | 500K | 1M | 2M+ |
| **ç·è¨“ç·´æ™‚é–“** | 6æ—¥ | 58æ—¥ | **å¹´å˜ä½** |

**å•é¡Œã®æœ¬è³ª**: ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®æ¬¡å…ƒ $d = H \times W \times C$ ãŒå¤§ãã™ãã‚‹ã€‚U-Netã®self-attentionã¯ $\mathcal{O}(d^2)$ ã®è¨ˆç®—é‡ â€” è§£åƒåº¦ã‚’2å€ã«ã™ã‚‹ã¨è¨ˆç®—é‡ã¯ **16å€**ã€‚

### æ½œåœ¨ç©ºé–“ã¸ã®å¿…ç„¶æ€§

**éµã¨ãªã‚‹è¦³å¯Ÿ**: è‡ªç„¶ç”»åƒã¯é«˜æ¬¡å…ƒç©ºé–“ã«åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹ãŒã€å®Ÿéš›ã«ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«åˆ†å¸ƒã—ã¦ã„ã‚‹ï¼ˆå¤šæ§˜ä½“ä»®èª¬ï¼‰ã€‚

$$
\begin{aligned}
\text{Pixel space: } &\mathbb{R}^{H \times W \times C} \quad \text{(é«˜æ¬¡å…ƒãƒ»å†—é•·)} \\
\text{Manifold: } &\mathcal{M} \subset \mathbb{R}^{H \times W \times C}, \quad \dim(\mathcal{M}) \ll H \times W \times C \\
\text{Latent space: } &\mathbb{R}^{h \times w \times c}, \quad h \ll H, w \ll W
\end{aligned}
$$

**è§£æ±ºç­–**: VAEã§ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ $z$ ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãã“ã§æ‹¡æ•£éç¨‹ã‚’å®Ÿè¡Œã€‚

```mermaid
graph LR
    X["Pixel x âˆˆ R^(HxWxC)"] -->|Encoder E| Z["Latent z âˆˆ R^(hxwxc)"]
    Z -->|Diffusion| Z2["Denoised z' âˆˆ R^(hxwxc)"]
    Z2 -->|Decoder D| X2["Pixel x' âˆˆ R^(HxWxC)"]

    style X fill:#ffcccc
    style Z fill:#ccffcc
    style Z2 fill:#ccffcc
    style X2 fill:#ffcccc
```

### LDM vs ãƒ”ã‚¯ã‚»ãƒ«æ‹¡æ•£: æ•°å€¤æ¯”è¼ƒ

Stable Diffusion 1.5ã®å®Ÿæ¸¬å€¤:

| ãƒ¡ãƒˆãƒªãƒƒã‚¯ | DDPM (512Â²) | LDM (SD 1.5) | æ”¹å–„ç‡ |
|:-----------|:------------|:-------------|:-------|
| **æ½œåœ¨ç©ºé–“æ¬¡å…ƒ** | 786,432 | 16,384 | **48xåœ§ç¸®** |
| **U-Net params** | ~500M | ~860M | 1.7x (ã§ã‚‚GPUã«ä¹—ã‚‹) |
| **è¨“ç·´æ™‚é–“/iter** | ~5ç§’ | ~0.8ç§’ | **6.25xé«˜é€ŸåŒ–** |
| **VRAM (fp16)** | 32GB | 10GB | **3.2xå‰Šæ¸›** |
| **åæŸã‚¹ãƒ†ãƒƒãƒ—** | 1M | 500K | **2xé«˜é€Ÿ** |
| **FID (COCO)** | 12.6 | **10.4** | å“è³ªå‘ä¸Š |

**ãªãœé«˜é€ŸåŒ–ã¨å“è³ªå‘ä¸ŠãŒä¸¡ç«‹ï¼Ÿ**

1. **Inductive bias**: æ½œåœ¨ç©ºé–“ã¯ã€Œæ„å‘³ã®ã‚ã‚‹ç‰¹å¾´ã€ã«åœ§ç¸®æ¸ˆã¿ â†’ æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ã‚„ã™ã„
2. **Perceptual compression**: VAEãŒçŸ¥è¦šçš„ã«é‡è¦ãªç‰¹å¾´ã‚’ä¿å­˜ â†’ å“è³ªç¶­æŒ
3. **Computational efficiency**: æ¬¡å…ƒå‰Šæ¸›ã§è¨ˆç®—é‡å‰Šæ¸› â†’ ã‚ˆã‚Šæ·±ã„U-Netãƒ»é•·æ™‚é–“è¨“ç·´ãŒå¯èƒ½

:::message alert
**ã‚ˆãã‚ã‚‹èª¤è§£**: ã€Œæ½œåœ¨ç©ºé–“ã§æ‹¡æ•£ã™ã‚‹ã‹ã‚‰å“è³ªãŒä¸‹ãŒã‚‹ã€â€” å®Ÿéš›ã¯VAEã®çŸ¥è¦šçš„æå¤±é–¢æ•°ã§ **å“è³ªã¯å‘ä¸Š**ã€‚
:::

### æ•°å¼ã§è¦‹ã‚‹LDM

ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“DDPMã®ç›®çš„é–¢æ•°ï¼ˆç¬¬36å›ã®å¾©ç¿’ï¼‰:
$$
\mathcal{L}_\text{DDPM} = \mathbb{E}_{x_0, \epsilon, t} \left[ \|\epsilon - \epsilon_\theta(x_t, t)\|^2 \right]
$$

Latent Diffusion Modelã®ç›®çš„é–¢æ•°:
$$
\mathcal{L}_\text{LDM} = \mathbb{E}_{z_0, \epsilon, t} \left[ \|\epsilon - \epsilon_\theta(z_t, t)\|^2 \right], \quad z_0 = \mathcal{E}(x_0)
$$

ã“ã“ã§:
- $\mathcal{E}: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^{h \times w \times c}$ ãŒVAE Encoder
- $z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$ ã¯æ½œåœ¨ç©ºé–“ã§ã®forward process
- $\epsilon_\theta(z_t, t)$ ã¯æ½œåœ¨ç©ºé–“ã§å‹•ä½œã™ã‚‹U-Net

**éµ**: $x$ ã‚’ $z$ ã«ç½®ãæ›ãˆãŸã ã‘ã€‚DDPMã®ç†è«–ãŒãã®ã¾ã¾ä½¿ãˆã‚‹!

```julia
# ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“DDPM
xâ‚€ = randn(512, 512, 3)  # 786Kæ¬¡å…ƒ
Îµâ‚œ = unet_pixel(xâ‚œ, t)    # 786Kæ¬¡å…ƒã®é€†æ‹¡æ•£

# æ½œåœ¨ç©ºé–“LDM
zâ‚€ = encoder(xâ‚€)         # 64Ã—64Ã—4 = 16Kæ¬¡å…ƒ
Îµâ‚œ = unet_latent(zâ‚œ, t)  # 16Kæ¬¡å…ƒã®é€†æ‹¡æ•£ (48xå‰Šæ¸›!)
```

### LDMã®è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

2æ®µéšã®è¨“ç·´:

```mermaid
graph TD
    A[Stage 1: VAEäº‹å‰è¨“ç·´] -->|å›ºå®š| B[Stage 2: æ½œåœ¨ç©ºé–“æ‹¡æ•£è¨“ç·´]

    A1[VAEè¨“ç·´<br>å†æ§‹æˆæå¤± + KLæ­£å‰‡åŒ–] --> A2[Encoder E & Decoder D]
    A2 --> B1[Diffusion U-Netè¨“ç·´<br>Îµ-prediction on z]
    B1 --> B2[å®Œæˆ: LDM]

    style A fill:#ffffcc
    style B fill:#ccffff
```

**Stage 1: VAEè¨“ç·´**
$$
\mathcal{L}_\text{VAE} = \underbrace{\mathbb{E}_{q(z|x)}[\log p(x|z)]}_{\text{å†æ§‹æˆé …}} - \underbrace{\beta \cdot \text{KL}[q(z|x) \| p(z)]}_{\text{æ­£å‰‡åŒ–é …}}
$$

- ç¬¬10å›ã§å­¦ã‚“ã VAEãã®ã‚‚ã®
- $\beta$-VAE ($\beta < 1$) ã§å†æ§‹æˆå“è³ªã‚’é‡è¦–
- ã¾ãŸã¯ VQ-VAE/FSQ ã§é›¢æ•£è¡¨ç¾å­¦ç¿’

**Stage 2: Diffusionè¨“ç·´**
$$
\mathcal{L}_\text{Diffusion} = \mathbb{E}_{z_0 \sim q(z_0), \epsilon \sim \mathcal{N}(0,I), t \sim \mathcal{U}(1,T)} \left[ \|\epsilon - \epsilon_\theta(z_t, t, c)\|^2 \right]
$$

- $c$ ã¯æ¡ä»¶ä»˜ã‘æƒ…å ±ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ç­‰ï¼‰
- VAEã¯ **å›ºå®š** ï¼ˆå‹¾é…ã‚’æµã•ãªã„ï¼‰
- U-Netã ã‘ã‚’è¨“ç·´

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®10%å®Œäº†ï¼** VAEã§åœ§ç¸®ã€æ½œåœ¨ç©ºé–“ã§æ‹¡æ•£ã¨ã„ã†2æ®µéšè¨­è¨ˆã®ç†è«–çš„æ ¹æ‹ ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” æ½œåœ¨ç©ºé–“ã®å¹¾ä½•å­¦

### ãªãœã“ã®è¬›ç¾©ã‚’å­¦ã¶ã‹

**åˆ°é”ç›®æ¨™**:
- Stable Diffusionã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Œå…¨ç†è§£
- Classifier-Free Guidanceã®æ•°å­¦çš„å°å‡ºã‚’è‡ªåŠ›ã§è¡Œãˆã‚‹
- FLUX/SD3ç­‰ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®ç†è«–çš„åŸºç›¤ã‚’æŠŠæ¡
- ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒç”Ÿæˆã®å…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã§ãã‚‹

**Course IVã§ã®ä½ç½®ã¥ã‘**:

```mermaid
graph LR
    L33[L33: Normalizing Flow] --> L34[L34: EBM]
    L34 --> L35[L35: Score Matching]
    L35 --> L36[L36: DDPM]
    L36 --> L37[L37: SDE/ODE]
    L37 --> L38[L38: Flow Matchingçµ±ä¸€ç†è«–]
    L38 --> L39[â˜…L39: Latent Diffusionâ˜…]
    L39 --> L40[L40: Consistency Models]
    L40 --> L41[L41: World Models]
    L41 --> L42[L42: çµ±ä¸€ç†è«–]

    style L39 fill:#ffcccc
```

**å‰æçŸ¥è­˜**:
- ç¬¬10å›: VAEç†è«–ï¼ˆELBOãƒ»å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ï¼‰
- ç¬¬36å›: DDPMå®Œå…¨å°å‡º
- ç¬¬38å›: Flow Matchingçµ±ä¸€ç†è«–

### æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã¨ã®å·®åˆ¥åŒ–

| ãƒˆãƒ”ãƒƒã‚¯ | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬è¬›ç¾© |
|:---------|:-------------|:-------|
| **LDMç†è«–** | æ¦‚è¦èª¬æ˜ | VAEåœ§ç¸®ç‡ã¨DDPMè¨ˆç®—é‡ã® **å®šé‡çš„é–¢ä¿‚** å°å‡º |
| **CFGå°å‡º** | ã‚¹ã‚­ãƒƒãƒ— | Îµ-prediction / score / æ¸©åº¦ã® **3è¦–ç‚¹ã‹ã‚‰å®Œå…¨å°å‡º** |
| **Text Conditioning** | CLIPç´¹ä»‹ | Cross-Attention / Self-Attention / Positional Encodingã® **å®Ÿè£…ãƒ¬ãƒ™ãƒ«è©³ç´°** |
| **FLUXè§£èª¬** | ãªã— | Rectified Flowçµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã® **æ•°å­¦çš„è§£æ** (2025) |
| **å®Ÿè£…** | ãªã— | âš¡Juliaè¨“ç·´ + ğŸ¦€Rustæ¨è«– + CFGå®Ÿé¨“ (3,000è¡Œè¶…) |

### LDMã®3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼1: åœ°å›³å¸³**
- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ = ä¸–ç•Œä¸­ã®è¡—ã®è©³ç´°åœ°å›³ï¼ˆè†¨å¤§ï¼‰
- æ½œåœ¨ç©ºé–“ = åœ°å›³å¸³ã®ç›®æ¬¡ï¼ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã ãŒä½ç½®é–¢ä¿‚ã¯ä¿å­˜ï¼‰
- æ‹¡æ•£éç¨‹ = ç›®æ¬¡ã‚’ã¼ã‹ã—ã¦å¾©å…ƒï¼ˆç›®æ¬¡ãƒ¬ãƒ™ãƒ«ã§ä½œæ¥­ï¼‰

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼2: åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«**
- VAE Encoder = ZIPåœ§ç¸®
- æ½œåœ¨ç©ºé–“ = .zip ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ48å€åœ§ç¸®ï¼‰
- æ‹¡æ•£U-Net = .zipå†…ã§ç›´æ¥ç·¨é›†
- VAE Decoder = è§£å‡

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼3: ã‚¹ã‚±ãƒƒãƒâ†’è©³ç´°åŒ–**
- æ½œåœ¨ç©ºé–“ = ãƒ©ãƒ•ã‚¹ã‚±ãƒƒãƒï¼ˆæ§‹å›³ãƒ»é…ç½®ã ã‘ï¼‰
- VAE Decoder = ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«è¿½åŠ ï¼ˆãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ»è‰²ãƒ»ç´°éƒ¨ï¼‰
- æ‹¡æ•£éç¨‹ = ã‚¹ã‚±ãƒƒãƒã®ãƒã‚¤ã‚ºé™¤å»

### Course I-IIIæ•°å­¦ã®æ´»ç”¨

| Course | æ´»ç”¨ç®‡æ‰€ |
|:-------|:---------|
| **ç¬¬10å› VAE** | Encoder/Decoderè¨­è¨ˆ / KLæ­£å‰‡åŒ– / å†æ§‹æˆæå¤± |
| **ç¬¬13å› OT** | Wassersteinè·é›¢ã§ã®å“è³ªè©•ä¾¡ / OT-CFMã¨ã®æ¥ç¶š |
| **ç¬¬36å› DDPM** | æ½œåœ¨ç©ºé–“ã§ã®æ‹¡æ•£éç¨‹ / Îµ-prediction / VLB |
| **ç¬¬38å› Flow Matching** | FLUX architecture / Rectified Flowçµ±åˆ |
| **ç¬¬14å› Attention** | Cross-Attention text conditioning / Self-Attention in U-Net |

### ãƒ¬ãƒ™ãƒ«ã‚¢ãƒƒãƒ—ãƒãƒƒãƒ—

```mermaid
graph TD
    A[Lv1: LDMã®å‹•æ©Ÿç†è§£<br>ãªãœæ½œåœ¨ç©ºé–“ã‹] --> B[Lv2: VAEåœ§ç¸®ã®æ•°å­¦<br>åœ§ç¸®ç‡vså“è³ª]
    B --> C[Lv3: æ½œåœ¨ç©ºé–“æ‹¡æ•£<br>DDPMã®æ‹¡å¼µ]
    C --> D[Lv4: Classifier Guidance<br>å‹¾é…ãƒ™ãƒ¼ã‚¹èª˜å°]
    D --> E[Lv5: Classifier-Free Guidance<br>CFGå®Œå…¨å°å‡º]
    E --> F[Lv6: Text Conditioning<br>Cross-Attention]
    F --> G[Lv7: FLUX Architecture<br>Flow Matchingçµ±åˆ]
    G --> H[Boss: Mini LDMå®Ÿè£…<br>Juliaè¨“ç·´+CFGå®Ÿé¨“]
```

### Trojan Horse â€” ğŸâ†’âš¡ğŸ¦€ğŸ”®ã®å¿…ç„¶æ€§

**ç¬¬39å›ã®è¨€èªæ§‹æˆ**: âš¡Julia 70% / ğŸ¦€Rust 20% / ğŸ”®Elixir 10%

**ãªãœJuliaä¸»å½¹ï¼Ÿ**
- VAEè¨“ç·´: Lux.jl + Reactant â†’ JAXä¸¦ã®é€Ÿåº¦
- Diffusionè¨“ç·´: å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§æå¤±é–¢æ•°ãŒè‡ªå‹•æœ€é©åŒ–
- CFGå®Ÿé¨“: Guidance scaleæƒå¼•ãŒ1è¡Œ

**ãªãœRustï¼Ÿ**
- ONNXæ¨è«–: Candle/Burn â†’ PyTorchæ¯”35-47%é«˜é€Ÿ
- ãƒãƒƒãƒå‡¦ç†: ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ + SIMDæœ€é©åŒ–

**ãªãœElixirï¼Ÿ**
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚µãƒ¼ãƒ“ãƒ³ã‚°: GenStage + Broadway
- åˆ†æ•£æ¨è«–: Supervisor Treeè€éšœå®³æ€§

ã“ã®3è¨€èªã‚¹ã‚¿ãƒƒã‚¯ã§ **è¨“ç·´â†’æ¨è«–â†’é…ä¿¡** ã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®20%å®Œäº†ï¼** ç›´æ„Ÿã‚’å›ºã‚ãŸã€‚æ¬¡ã¯æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã§å®Œå…¨ç†è«–ã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” LDMå®Œå…¨ç†è«–

### 3.1 VAE Encoder/Decoderã®æ•°å­¦

**Encoder $\mathcal{E}: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^{h \times w \times c}$**

| é …ç›® | è©³ç´° |
|:-----|:-----|
| **åœ§ç¸®æ¯”** | $f = \frac{H \times W}{h \times w}$ (å…¸å‹çš„ã« $f=8$ or $f=16$) |
| **ãƒãƒ£ãƒãƒ«æ•°** | $c$ ã¯é€šå¸¸4 or 8 (RGB 3chã‚ˆã‚Šå¢—ãˆã‚‹) |
| **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–** | ResNet blocks + Downsampling + Attention |
| **å‡ºåŠ›** | Deterministic $z = \mathcal{E}(x)$ or Stochastic $z \sim q(z\|x) = \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x))$ |

**Decoder $\mathcal{D}: \mathbb{R}^{h \times w \times c} \to \mathbb{R}^{H \times W \times C}$**

$$
\tilde{x} = \mathcal{D}(z), \quad \text{s.t.} \quad \|\tilde{x} - x\|_\text{perceptual} < \epsilon
$$

çŸ¥è¦šçš„æå¤±ï¼ˆPerceptual Lossï¼‰ã‚’ä½¿ã†:
$$
\mathcal{L}_\text{rec} = \|\Phi(x) - \Phi(\mathcal{D}(\mathcal{E}(x)))\|^2
$$

ã“ã“ã§ $\Phi$ ã¯VGGã‚„LPIPSç‰¹å¾´æŠ½å‡ºå™¨ã€‚

**2ã¤ã®VAEæ­£å‰‡åŒ–æ–¹å¼**:

| æ–¹å¼ | æ­£å‰‡åŒ–é … | ç‰¹å¾´ |
|:-----|:---------|:-----|
| **KL-regularization** | $\mathcal{L}_\text{KL} = \text{KL}[q(z\|x) \| \mathcal{N}(0,I)]$ | é€£ç¶šæ½œåœ¨ç©ºé–“ / $\beta$-VAE |
| **VQ-regularization** | Codebook loss + Commitment loss | é›¢æ•£æ½œåœ¨ç©ºé–“ / VQ-VAE/FSQ |

**Stable Diffusion 1.x/2.xã®é¸æŠ**: KL-reg VAE with $f=8$ compressionã€‚

**åœ§ç¸®ç‡ vs å†æ§‹æˆå“è³ªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:

| åœ§ç¸®ç‡ $f$ | Latent size (512Â² input) | LPIPSâ†“ | FIDâ†“ | è¨“ç·´é€Ÿåº¦ |
|:-----------|:------------------------|:-------|:-----|:---------|
| $f=4$ | 128Ã—128Ã—4 | 0.05 | 1.2 | 1x |
| $f=8$ | 64Ã—64Ã—4 | 0.08 | 2.1 | **4x** |
| $f=16$ | 32Ã—32Ã—4 | 0.15 | 5.6 | 16x |

**çµè«–**: $f=8$ ãŒå“è³ªã¨é€Ÿåº¦ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ã€‚

```julia
# VAE Encoder/Decoderå®šç¾© (Lux.jl)
function create_vae_encoder(; img_size=512, latent_size=64, latent_channels=4)
    # Downsampling path: 512 -> 256 -> 128 -> 64
    encoder = Chain(
        Conv((3, 3), 3 => 128, pad=1),
        ResBlock(128),
        Downsample(128 => 256),  # 512 -> 256
        ResBlock(256),
        Downsample(256 => 512),  # 256 -> 128
        ResBlock(512),
        Downsample(512 => 512),  # 128 -> 64
        SelfAttention(512),
        ResBlock(512),
        Conv((3, 3), 512 => latent_channels, pad=1)  # Output z
    )
    return encoder
end

# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
x = randn(Float32, 512, 512, 3, 1)  # [H, W, C, B]
z = encoder(x, ps, st)[1]           # [64, 64, 4, 1] (48xåœ§ç¸®!)
```

:::details VAE Decoderã®ãƒŸãƒ©ãƒ¼æ§‹é€ 
```julia
function create_vae_decoder(; latent_size=64, img_size=512, latent_channels=4)
    decoder = Chain(
        Conv((3, 3), latent_channels => 512, pad=1),
        ResBlock(512),
        SelfAttention(512),
        ResBlock(512),
        Upsample(512 => 512),  # 64 -> 128
        ResBlock(512),
        Upsample(512 => 256),  # 128 -> 256
        ResBlock(256),
        Upsample(256 => 128),  # 256 -> 512
        ResBlock(128),
        Conv((3, 3), 128 => 3, pad=1, activation=tanh)  # Output x
    )
    return decoder
end

xÌƒ = decoder(z, ps, st)[1]  # [512, 512, 3, 1] å†æ§‹æˆ
```
:::

### 3.2 æ½œåœ¨ç©ºé–“ã§ã®æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹

**Forward process** (DDPMç¬¬36å›ã®å¾©ç¿’):
$$
q(z_t | z_0) = \mathcal{N}(z_t; \sqrt{\bar{\alpha}_t} z_0, (1-\bar{\alpha}_t) I)
$$

**é–‰å½¢å¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**:
$$
z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
$$

**Reverse process**:
$$
p_\theta(z_{t-1} | z_t) = \mathcal{N}(z_{t-1}; \mu_\theta(z_t, t), \Sigma_\theta(z_t, t))
$$

**è¨“ç·´ç›®çš„é–¢æ•°** (Îµ-prediction):
$$
\mathcal{L}_\text{simple} = \mathbb{E}_{z_0, \epsilon, t} \left[ \|\epsilon - \epsilon_\theta(z_t, t)\|^2 \right]
$$

**éµ**: ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®DDPMã¨æ•°å¼ã¯ **å®Œå…¨ã«åŒã˜**ã€‚$x \to z$ ã®ç½®ãæ›ãˆã ã‘ã€‚

```julia
# Forward process: zâ‚€ã«ãƒã‚¤ã‚ºä»˜åŠ 
function forward_diffusion(zâ‚€, t, Î±â‚œ_bar, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    z_t = sqrt(Î±â‚œ_bar) .* zâ‚€ .+ sqrt(1 - Î±â‚œ_bar) .* Îµ
    return z_t, Îµ
end

# è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—
function train_step!(model, zâ‚€, t, Î±â‚œ_bar, ps, st, opt_state)
    z_t, Îµ_true = forward_diffusion(zâ‚€, t, Î±â‚œ_bar, rng)

    # Îµ-prediction
    Îµ_pred, st = model((z_t, t), ps, st)

    # MSE loss
    loss = mean((Îµ_pred .- Îµ_true).^2)

    # Backprop
    gs = gradient(ps -> loss, ps)[1]
    opt_state, ps = Optimisers.update(opt_state, ps, gs)

    return loss, ps, st, opt_state
end
```

**Noise scheduleã®é¸æŠ**:

ç¬¬36å›ã§å­¦ã‚“ã ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒãã®ã¾ã¾ä½¿ãˆã‚‹:
- Linear: $\beta_t = \beta_\text{start} + t \cdot (\beta_\text{end} - \beta_\text{start}) / T$
- Cosine: $\bar{\alpha}_t = \cos\left(\frac{t/T + s}{1+s} \cdot \frac{\pi}{2}\right)^2 / \cos\left(\frac{s}{1+s} \cdot \frac{\pi}{2}\right)^2$
- **Zero Terminal SNR**: $\bar{\alpha}_T = 0$ ã‚’å¼·åˆ¶ï¼ˆå¾Œè¿°ï¼‰

Stable Diffusion 1.x/2.xã¯ **Linear schedule** with 1000 stepsã€‚

### 3.3 LDMè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**2æ®µéšè¨“ç·´**:

```mermaid
sequenceDiagram
    participant Data as Dataset
    participant VAE as VAE (fixed)
    participant UNet as Diffusion U-Net
    participant Loss as Loss

    Note over Data,Loss: Stage 1: VAEè¨“ç·´ (åˆ¥é€”å®Œäº†)

    Note over Data,Loss: Stage 2: Diffusionè¨“ç·´
    Data->>VAE: xâ‚€ (512Ã—512Ã—3)
    VAE->>UNet: zâ‚€ = E(xâ‚€) (64Ã—64Ã—4)
    UNet->>UNet: Forward: z_t ~ q(z_t|zâ‚€)
    UNet->>UNet: Predict: Îµ_Î¸(z_t, t)
    UNet->>Loss: ||Îµ - Îµ_Î¸(z_t, t)||Â²
    Loss->>UNet: Backprop (VAEã¯å›ºå®š)
```

**Stage 2ã®å®Œå…¨ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—**:

$$
\begin{aligned}
&\text{for epoch in 1:N} \\
&\quad \text{for } (x_0, c) \in \text{DataLoader} \\
&\quad\quad z_0 \leftarrow \mathcal{E}(x_0) \quad \text{(Encoder forward, no grad)} \\
&\quad\quad t \sim \mathcal{U}(1, T) \\
&\quad\quad \epsilon \sim \mathcal{N}(0, I) \\
&\quad\quad z_t \leftarrow \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon \\
&\quad\quad \epsilon_\theta \leftarrow \text{UNet}(z_t, t, c) \\
&\quad\quad \mathcal{L} \leftarrow \|\epsilon - \epsilon_\theta\|^2 \\
&\quad\quad \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L} \quad \text{(UNetã®ã¿æ›´æ–°)}
\end{aligned}
$$

```julia
# å®Œå…¨ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—
function train_ldm!(unet, vae_encoder, dataloader, epochs; lr=1e-4)
    opt = Adam(lr)
    opt_state = Optimisers.setup(opt, ps_unet)

    for epoch in 1:epochs
        for (x, c) in dataloader
            # VAE encode (no grad)
            zâ‚€ = vae_encoder(x, ps_vae, st_vae)[1]  # å‹¾é…ãªã—

            # Random timestep
            t = rand(1:T)
            Î±â‚œ_bar = alpha_bar_schedule[t]

            # Forward diffusion
            z_t, Îµ = forward_diffusion(zâ‚€, t, Î±â‚œ_bar, rng)

            # Predict noise
            Îµ_pred, st_unet = unet((z_t, t, c), ps_unet, st_unet)

            # Loss & update
            loss = mse_loss(Îµ_pred, Îµ)
            gs = gradient(ps -> loss, ps_unet)[1]
            opt_state, ps_unet = Optimisers.update(opt_state, ps_unet, gs)
        end
    end
    return ps_unet
end
```

:::message alert
**ã‚ˆãã‚ã‚‹ãƒŸã‚¹**: VAEã«å‹¾é…ã‚’æµã—ã¦ã—ã¾ã†ã€‚Encoderã¯ **å®Œå…¨ã«å›ºå®š** ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚
:::

### 3.4 Classifier Guidanceå®Œå…¨ç‰ˆ

**å‹•æ©Ÿ**: æ¡ä»¶ä»˜ãç”Ÿæˆ $p(z_0 | c)$ ã§ã€æ¡ä»¶ $c$ (ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«)ã¸ã®å¿ å®Ÿåº¦ã‚’é«˜ã‚ãŸã„ã€‚

**ãƒ™ã‚¤ã‚ºã®å®šç†**:
$$
\nabla_{z_t} \log p(z_t | c) = \nabla_{z_t} \log p(z_t) + \nabla_{z_t} \log p(c | z_t)
$$

ã“ã“ã§:
- $\nabla_{z_t} \log p(z_t)$ ã¯ç„¡æ¡ä»¶ã‚¹ã‚³ã‚¢ï¼ˆU-NetãŒå­¦ç¿’ï¼‰
- $\nabla_{z_t} \log p(c | z_t)$ ã¯åˆ†é¡å™¨ã®å‹¾é…

**Classifier Guidanceã®ä¿®æ­£ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**:
$$
\tilde{\epsilon}_\theta(z_t, t, c) = \epsilon_\theta(z_t, t) - \sqrt{1-\bar{\alpha}_t} \cdot w \nabla_{z_t} \log p_\phi(c | z_t)
$$

ã“ã“ã§:
- $\epsilon_\theta(z_t, t)$ ã¯ç„¡æ¡ä»¶ãƒã‚¤ã‚ºäºˆæ¸¬
- $p_\phi(c | z_t)$ ã¯ **åˆ¥é€”è¨“ç·´ã—ãŸåˆ†é¡å™¨**
- $w$ ã¯guidance scale

**å•é¡Œç‚¹**:
1. åˆ†é¡å™¨ $p_\phi(c | z_t)$ ã‚’åˆ¥é€”è¨“ç·´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
2. å„timestep $t$ ã§ç•°ãªã‚‹åˆ†é¡å™¨ãŒå¿…è¦
3. è¨“ç·´ã‚³ã‚¹ãƒˆãŒ2å€

â†’ Classifier-Free Guidanceã§è§£æ±º!

```julia
# Classifier Guidance (å‚è€ƒå®Ÿè£…)
function classifier_guidance_sample(unet, classifier, z_T, c, w)
    z_t = z_T
    for t in T:-1:1
        # ç„¡æ¡ä»¶ã‚¹ã‚³ã‚¢
        Îµ_uncond = unet(z_t, t, nothing)

        # åˆ†é¡å™¨å‹¾é…
        grad_log_p_c = gradient(z -> log_prob(classifier(z), c), z_t)[1]

        # ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹é©ç”¨
        Îµ_guided = Îµ_uncond .- sqrt(1 - Î±_bar[t]) .* w .* grad_log_p_c

        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—
        z_t = reverse_step(z_t, Îµ_guided, t)
    end
    return decoder(z_t)
end
```

### 3.5 Classifier-Free Guidanceå®Œå…¨å°å‡º

**éµã¨ãªã‚‹ã‚¢ã‚¤ãƒ‡ã‚¢**: æ¡ä»¶ä»˜ããƒ¢ãƒ‡ãƒ« $\epsilon_\theta(z_t, t, c)$ ã¨ç„¡æ¡ä»¶ãƒ¢ãƒ‡ãƒ« $\epsilon_\theta(z_t, t, \emptyset)$ ã‚’ **åŒæ™‚ã«è¨“ç·´** ã—ã€æ¨è«–æ™‚ã«ç·šå½¢çµåˆã™ã‚‹ã€‚

**è¨“ç·´æ™‚**: ãƒ©ãƒ³ãƒ€ãƒ ã«conditionã‚’drop
$$
c_\text{input} = \begin{cases}
c & \text{with probability } p_\text{uncond} \text{ (e.g. 0.1)} \\
\emptyset & \text{with probability } 1 - p_\text{uncond}
\end{cases}
$$

ã“ã‚Œã§å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãŒæ¡ä»¶ä»˜ããƒ»ç„¡æ¡ä»¶ä¸¡æ–¹ã‚’å­¦ç¿’ã€‚

**æ¨è«–æ™‚**: 2ã¤ã®äºˆæ¸¬ã®ç·šå½¢çµåˆ
$$
\tilde{\epsilon}_\theta(z_t, t, c, w) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right)
$$

ã“ã“ã§:
- $w$ ã¯guidance scale ($w=0$: ç„¡æ¡ä»¶, $w=1$: æ¡ä»¶ä»˜ã, $w>1$: over-guidance)
- $\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset)$ ãŒæ¡ä»¶ã®"æ–¹å‘"

**ãªãœã“ã‚ŒãŒæ©Ÿèƒ½ã™ã‚‹ã‹ï¼Ÿã‚¹ã‚³ã‚¢ã®è¦–ç‚¹ã‹ã‚‰å°å‡º**:

ã‚¹ã‚³ã‚¢é–¢æ•° $s_\theta(z_t, t, c) = -\frac{\epsilon_\theta(z_t, t, c)}{\sqrt{1-\bar{\alpha}_t}}$ ã¨ã™ã‚‹ã¨:

$$
\begin{aligned}
\tilde{s}_\theta(z_t, t, c, w) &= -\frac{\tilde{\epsilon}_\theta(z_t, t, c, w)}{\sqrt{1-\bar{\alpha}_t}} \\
&= -\frac{1}{\sqrt{1-\bar{\alpha}_t}} \left[ \epsilon_\theta(z_t, t, \emptyset) + w(\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset)) \right] \\
&= s_\theta(z_t, t, \emptyset) + w \cdot (s_\theta(z_t, t, c) - s_\theta(z_t, t, \emptyset)) \\
&= (1-w) \cdot s_\theta(z_t, t, \emptyset) + w \cdot s_\theta(z_t, t, c)
\end{aligned}
$$

ã“ã‚Œã¯ **ç„¡æ¡ä»¶ã‚¹ã‚³ã‚¢ã¨æ¡ä»¶ä»˜ãã‚¹ã‚³ã‚¢ã®åŠ é‡å¹³å‡** !

**$w > 1$ ã®å ´åˆ**:
$$
\tilde{s}_\theta = s_\theta(z_t, t, \emptyset) + w \cdot (s_\theta(z_t, t, c) - s_\theta(z_t, t, \emptyset))
$$

æ¡ä»¶ã®æ–¹å‘ã« **$w$å€å¼·ã** æŠ¼ã™ â†’ mode-seekingè¡Œå‹•ã€‚

**åˆ¥ã®è¦–ç‚¹: æš—é»™çš„ãªæ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:

$w > 1$ ã®ã¨ãã€å®ŸåŠ¹çš„ãªç¢ºç‡åˆ†å¸ƒã¯:
$$
p_w(z_t | c) \propto p(z_t | c)^w \cdot p(z_t)^{1-w}
$$

ã“ã‚Œã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¦–ç‚¹ã§:
$$
E_w(z_t) = -w \log p(z_t | c) - (1-w) \log p(z_t)
$$

$w \to \infty$ ã§æ¡ä»¶ä»˜ãåˆ†å¸ƒã® **æœ€é »å€¤** (mode)ã«ãƒ”ãƒ¼ã‚¯ â†’ å“è³ªå‘ä¸Šã ãŒå¤šæ§˜æ€§ä½ä¸‹ã€‚

:::details CFGã®ç†è«–çš„ç†è§£: Mode-Seeking vs Mode-Covering
**$w$ã®åŠ¹æœ**:

| Guidance Scale $w$ | æŒ™å‹• | å“è³ª | å¤šæ§˜æ€§ |
|:-------------------|:-----|:-----|:-------|
| $w = 0$ | ç„¡æ¡ä»¶ç”Ÿæˆ | ä½ | é«˜ |
| $w = 1$ | æ¨™æº–æ¡ä»¶ä»˜ã | ä¸­ | ä¸­ |
| $w \in (1, 7]$ | Mode-seeking | **é«˜** | ä¸­ |
| $w > 7$ | Over-guidance | éé£½å’Œ | **ä½** |

**å…¸å‹çš„ãªå€¤**:
- Stable Diffusion 1.x: $w = 7.5$
- DALL-E 2: $w = 3.0$
- Imagen: $w = 5.0$

**æ•°å­¦çš„è¦–ç‚¹**:
- $w < 1$: Mode-covering (KL[qâ€–p]æœ€å°åŒ–é¢¨)
- $w > 1$: Mode-seeking (KL[pâ€–q]æœ€å°åŒ–é¢¨)
:::

```julia
# Classifier-Free Guidanceå®Ÿè£…
function cfg_sample(unet, z_T, c, w; steps=50)
    z_t = z_T
    timesteps = reverse(1:steps)

    for t in timesteps
        # 2å›ã®forward pass
        Îµ_uncond = unet((z_t, t, nothing), ps, st)[1]  # ç„¡æ¡ä»¶
        Îµ_cond = unet((z_t, t, c), ps, st)[1]           # æ¡ä»¶ä»˜ã

        # CFGçµåˆ
        Îµ_guided = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)

        # DDIMã‚¹ãƒ†ãƒƒãƒ— (é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)
        z_t = ddim_step(z_t, Îµ_guided, t, t-1, Î±â‚œ_bar)
    end

    return vae_decoder(z_t)
end

# è¨“ç·´æ™‚ã®Condition Drop
function train_step_with_cfg!(unet, zâ‚€, c, t, p_uncond=0.1)
    # Randomly drop condition
    if rand() < p_uncond
        c = nothing  # ç„¡æ¡ä»¶åŒ–
    end

    z_t, Îµ = forward_diffusion(zâ‚€, t, Î±â‚œ_bar, rng)
    Îµ_pred = unet((z_t, t, c), ps, st)[1]

    loss = mse_loss(Îµ_pred, Îµ)
    # ... backprop
end
```

### 3.6 CFGã®ç†è«–çš„ç†è§£

**ãªãœå“è³ªãŒå‘ä¸Šã™ã‚‹ã‹ï¼Ÿ**

1. **æ¡ä»¶ã®å¼·èª¿**: $w > 1$ ã§æ¡ä»¶æƒ…å ±ã‚’å¢—å¹… â†’ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¿ å®Ÿåº¦â†‘
2. **åˆ†æ•£å‰Šæ¸›**: ç„¡æ¡ä»¶ã¨ã®å·®åˆ†ãŒã€Œæ¡ä»¶ã®ç´”ç²‹ãªåŠ¹æœã€ â†’ ãƒã‚¤ã‚ºé™¤å»
3. **Mode-seeking**: é«˜ç¢ºç‡é ˜åŸŸã«é›†ä¸­ â†’ é®®æ˜ãªç”»åƒ

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:
$$
\text{Quality} \uparrow, \quad \text{Diversity} \downarrow, \quad \text{Compute} \times 2
$$

æ¨è«–æ™‚ã«2å›ã®U-Net forward passãŒå¿…è¦ â†’ é€Ÿåº¦åŠæ¸›ã€‚

**æœ€é©åŒ–**: Negative Promptï¼ˆæ¬¡ç¯€ï¼‰ã§1.5å›ã«å‰Šæ¸›å¯èƒ½ã€‚

### 3.7 Negative Prompt

**å‹•æ©Ÿ**: CFGã§ $\epsilon_\theta(z_t, t, \emptyset)$ ã®ä»£ã‚ã‚Šã«ã€Œé¿ã‘ãŸã„æ¦‚å¿µã€ã‚’æŒ‡å®šã—ãŸã„ã€‚

**å®šå¼åŒ–**:
$$
\tilde{\epsilon}_\theta(z_t, t, c_\text{pos}, c_\text{neg}, w) = \epsilon_\theta(z_t, t, c_\text{neg}) + w \cdot \left( \epsilon_\theta(z_t, t, c_\text{pos}) - \epsilon_\theta(z_t, t, c_\text{neg}) \right)
$$

ã“ã“ã§:
- $c_\text{pos}$ ã¯æ­£ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ("a beautiful mountain")
- $c_\text{neg}$ ã¯è² ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ("blurry, low quality")
- $w$ ã¯guidance scale

**è§£é‡ˆ**: ã€Œ$c_\text{neg}$ã‹ã‚‰é›¢ã‚Œã€$c_\text{pos}$ã«è¿‘ã¥ãã€æ–¹å‘ã«ã‚¬ã‚¤ãƒ‰ã€‚

**å…¸å‹çš„ãªNegative Prompt**:
```
"blurry, low quality, watermark, signature, jpeg artifacts,
 worst quality, low resolution, bad anatomy"
```

```julia
# Negative Promptå®Ÿè£…
function cfg_with_negative(unet, z_T, c_pos, c_neg, w)
    z_t = z_T
    for t in T:-1:1
        Îµ_neg = unet((z_t, t, c_neg), ps, st)[1]
        Îµ_pos = unet((z_t, t, c_pos), ps, st)[1]

        # Negative Prompté©ç”¨
        Îµ_guided = Îµ_neg .+ w .* (Îµ_pos .- Îµ_neg)

        z_t = reverse_step(z_t, Îµ_guided, t)
    end
    return decoder(z_t)
end
```

**åŠ¹æœ**:
- å“è³ªå‘ä¸Š: ã€Œblurryã€ã‚’é¿ã‘ã‚‹ â†’ é®®æ˜
- ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆå‰Šæ¸›: ã€Œwatermarkã€ã‚’é¿ã‘ã‚‹
- è§£å‰–å­¦çš„ã‚¨ãƒ©ãƒ¼å‰Šæ¸›: ã€Œbad anatomyã€ã‚’é¿ã‘ã‚‹

### 3.8 Text Conditioning

**å•é¡Œè¨­å®š**: ãƒ†ã‚­ã‚¹ãƒˆ $\mathbf{t}$ â†’ ç”»åƒ $x$ ã®ç”Ÿæˆã€‚æ¡ä»¶ $c = \text{Encoder}_\text{text}(\mathbf{t})$ã€‚

**Text Encoderé¸æŠ**:

| Encoder | æ¬¡å…ƒ | ç‰¹å¾´ | SDæ¡ç”¨ |
|:--------|:-----|:-----|:-------|
| **CLIP Text** | 768 | Vision-languageã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ | SD 1.x |
| **OpenCLIP** | 1024 | ã‚ˆã‚Šå¤§è¦æ¨¡CLIP | SD 2.x |
| **T5** | 4096 | ç´”ç²‹è¨€èªãƒ¢ãƒ‡ãƒ« | Imagen/SD3 |
| **CLIP+T5** | 768+4096 | ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ | SDXL/SD3 |

**CLIP vs T5ã®é•ã„**:

| é …ç›® | CLIP | T5 |
|:-----|:-----|:---|
| **è¨“ç·´** | Image-Text contrastive | Language modeling |
| **èªå½™** | 49K | 32K |
| **æ–‡ç†è§£** | æµ…ã„ | æ·±ã„ (Transformer) |
| **ç”»åƒã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ** | å¼·ã„ | å¼±ã„ |
| **é•·æ–‡** | 77 tokens max | 512 tokens |

**Stable Diffusion 3ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰**:
- CLIP: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæ„å‘³ï¼ˆã€ŒçŠ¬ã€ã€Œå±±ã€ï¼‰
- T5: è©³ç´°ãªé–¢ä¿‚æ€§ï¼ˆã€ŒçŠ¬ãŒå±±ã®ä¸Šã«åº§ã£ã¦ã„ã‚‹ã€ï¼‰

:::details CLIP Text Encoderã®ä»•çµ„ã¿
```python
# CLIPãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (PyTorchæ“¬ä¼¼ã‚³ãƒ¼ãƒ‰)
text = "A beautiful mountain landscape"
tokens = tokenizer(text)  # [BOS, 320, 1215, 5270, 5677, EOS, PAD, ...]  # 77 tokens

# Transformer Encoder
embeddings = text_embedding(tokens)  # [77, 768]
hidden = transformer_layers(embeddings)  # [77, 768]

# Pooling
pooled = hidden[0]  # [BOS]ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ (BERTé¢¨)
# ã¾ãŸã¯
pooled = hidden.mean(dim=0)  # å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°

# Output: [768] vector
```

SD 1.xã¯CLIPã®æœ€çµ‚å±¤hidden statesã‚’ **å…¨ã¦ä½¿ç”¨**:
- $c = [\mathbf{h}_0, \mathbf{h}_1, \ldots, \mathbf{h}_{76}]$ : shape [77, 768]
- ã“ã‚Œã‚’Cross-Attentionã«å…¥åŠ›
:::

### 3.9 Cross-Attention Text Conditioning

**U-Netã¸ã®ãƒ†ã‚­ã‚¹ãƒˆæ³¨å…¥æ–¹æ³•**:

```mermaid
graph TD
    T[Text: "mountain landscape"] --> TE[Text Encoder<br>CLIP/T5]
    TE --> C[c âˆˆ R^(LÃ—D)]

    Z[z_t âˆˆ R^(hÃ—wÃ—c)] --> SA[Self-Attention]
    SA --> CA[Cross-Attention]
    C --> CA
    CA --> FF[FeedForward]
    FF --> Out[Output]

    style CA fill:#ffcccc
```

**Cross-Attentionã®å®šå¼åŒ–**:

U-Netã®ä¸­é–“ç‰¹å¾´ $\mathbf{f} \in \mathbb{R}^{(h \times w) \times d}$ ã«å¯¾ã—ã¦:

$$
\begin{aligned}
Q &= W_Q \mathbf{f} \quad \in \mathbb{R}^{(h \times w) \times d_k} \\
K &= W_K \mathbf{c} \quad \in \mathbb{R}^{L \times d_k} \\
V &= W_V \mathbf{c} \quad \in \mathbb{R}^{L \times d_v} \\
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\end{aligned}
$$

ã“ã“ã§:
- $\mathbf{f}$: U-Netä¸­é–“ç‰¹å¾´ï¼ˆç”»åƒå´ï¼‰
- $\mathbf{c}$: ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæ¡ä»¶å´ï¼‰
- $Q$ from image, $K, V$ from text â†’ **Cross**-Attention

**Multi-Head Cross-Attention**:
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
$$

å„headãŒç•°ãªã‚‹æ„å‘³çš„å´é¢ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ï¼ˆä¾‹: è‰²ã€å½¢ã€é…ç½®ï¼‰ã€‚

```julia
# Cross-Attention Layer (Lux.jl)
struct CrossAttention
    num_heads::Int
    head_dim::Int
    W_Q::Dense
    W_K::Dense
    W_V::Dense
    W_O::Dense
end

function (ca::CrossAttention)(f, c)
    # f: [h*w, d], c: [L, d_text]
    Q = ca.W_Q(f)      # [h*w, num_heads * head_dim]
    K = ca.W_K(c)      # [L, num_heads * head_dim]
    V = ca.W_V(c)      # [L, num_heads * head_dim]

    # Reshape for multi-head
    Q = reshape(Q, :, ca.num_heads, ca.head_dim)  # [h*w, heads, dim]
    K = reshape(K, :, ca.num_heads, ca.head_dim)  # [L, heads, dim]
    V = reshape(V, :, ca.num_heads, ca.head_dim)

    # Attention
    scores = batched_mul(Q, permutedims(K, (2, 1, 3))) / sqrt(ca.head_dim)
    attn = softmax(scores, dims=2)  # [h*w, L, heads]
    out = batched_mul(attn, V)      # [h*w, heads, dim]

    # Concat + projection
    out = reshape(out, :, ca.num_heads * ca.head_dim)
    return ca.W_O(out)
end
```

**SpatialTransformer Block** (SD U-Net):
```
Input z_t
  â†“
GroupNorm
  â†“
Self-Attention (spatial)
  â†“
Cross-Attention (with text c)
  â†“
FeedForward
  â†“
Output
```

ã“ã‚Œã‚’å„è§£åƒåº¦ã§ç¹°ã‚Šè¿”ã—ã€‚

### 3.10 Stable Diffusion 1.x/2.x ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

**å…¨ä½“æ§‹æˆ**:

```mermaid
graph LR
    Input[Text Prompt] --> CLIP[CLIP Text Encoder]
    CLIP --> C[c: [77, 768]]

    Noise[Random Noise z_T] --> UNet[U-Net with<br>Cross-Attention]
    C --> UNet
    UNet --> z0[Denoised z_0]
    z0 --> VAE[VAE Decoder]
    VAE --> Output[Image 512Ã—512]
```

**U-Netè©³ç´°**:

| ãƒ¬ãƒ™ãƒ« | è§£åƒåº¦ | Channels | Blocks | Cross-Attn |
|:-------|:-------|:---------|:-------|:-----------|
| **Input** | 64Ã—64 | 4 | - | - |
| **Down 1** | 64â†’32 | 320 | ResBlockÃ—2 | âœ“ |
| **Down 2** | 32â†’16 | 640 | ResBlockÃ—2 | âœ“ |
| **Down 3** | 16â†’8 | 1280 | ResBlockÃ—2 | âœ“ |
| **Middle** | 8 | 1280 | ResBlockÃ—1 | âœ“ |
| **Up 1** | 8â†’16 | 1280 | ResBlockÃ—3 | âœ“ |
| **Up 2** | 16â†’32 | 640 | ResBlockÃ—3 | âœ“ |
| **Up 3** | 32â†’64 | 320 | ResBlockÃ—3 | âœ“ |
| **Output** | 64Ã—64 | 4 | - | - |

**ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: ~860M

**ResBlockæ§‹æˆ**:
```
Input
  â†“
GroupNorm â†’ SiLU â†’ Conv (3Ã—3)
  â†“
Timestep Embedding (broadcast add)
  â†“
GroupNorm â†’ SiLU â†’ Conv (3Ã—3)
  â†“
Residual Connection
  â†“
Output
```

**Timestep Embedding**:
$$
\gamma(t) = [\sin(t \omega_1), \cos(t \omega_1), \ldots, \sin(t \omega_{d/2}), \cos(t \omega_{d/2})]
$$

ã“ã“ã§ $\omega_k = 10000^{-2k/d}$ (Transformeré¢¨)ã€‚

```julia
# Timestep Embedding
function sinusoidal_embedding(t, dim)
    half_dim = dim Ã· 2
    emb = log(10000) / (half_dim - 1)
    emb = exp.(-emb .* (0:(half_dim-1)))
    emb = t .* emb'
    return hcat(sin.(emb), cos.(emb))
end
```

### 3.11 LDMå›ºæœ‰ã®U-Netæ‹¡å¼µ: SpatialTransformer

**æ¨™æº–U-Net vs LDM U-Net**:

| é …ç›® | æ¨™æº–U-Net | LDM U-Net |
|:-----|:----------|:----------|
| **Attention** | Self-Attention ã®ã¿ | Self + **Cross**-Attention |
| **æ¡ä»¶ä»˜ã‘** | Timestep $t$ ã®ã¿ | $t$ + **text $c$** |
| **å±¤æ§‹æˆ** | ResBlockâ†’Attn | ResBlockâ†’**SpatialTransformer** |

**SpatialTransformer Block**:
```
Input: [B, H, W, C]
  â†“
Reshape: [B, H*W, C]
  â†“
LayerNorm
  â†“
Self-Attention: Attention(Q,K,V) where Q=K=V=features
  â†“
LayerNorm
  â†“
Cross-Attention: Attention(Q_img, K_text, V_text)
  â†“
LayerNorm
  â†“
FeedForward (MLP)
  â†“
Reshape: [B, H, W, C]
  â†“
Residual Add
  â†“
Output
```

**ãªãœSpatial Transformerï¼Ÿ**

1. **Self-Attention**: ç”»åƒå†…ã®é•·è·é›¢ä¾å­˜ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£
2. **Cross-Attention**: ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’æ•´åˆ—
3. **FeedForward**: éç·šå½¢å¤‰æ›ã§è¡¨ç¾åŠ›å‘ä¸Š

**è¨ˆç®—é‡**: $\mathcal{O}((HW)^2 + HW \cdot L)$ where $L$ ã¯ãƒ†ã‚­ã‚¹ãƒˆé•·ã€‚

### 3.12 FLUX Architectureè©³è§£

**FLUXã®é©æ–°**: Rectified Flowï¼ˆç¬¬38å›ï¼‰+ DiTé¢¨Transformerè¨­è¨ˆã€‚

**FLUX vs Stable Diffusion**:

| é …ç›® | SD 1.x/2.x | FLUX.1 |
|:-----|:-----------|:-------|
| **ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³** | U-Net | **Transformer (DiTé¢¨)** |
| **Flow** | DDPM | **Rectified Flow** |
| **Text Encoder** | CLIP | **CLIP + T5** |
| **Latent Size** | 64Ã—64Ã—4 | 64Ã—64Ã—16 (é«˜æ¬¡å…ƒåŒ–) |
| **Params** | 860M | **12B** |
| **é€Ÿåº¦** | 50 steps | **20 steps** (ç›´ç·šåŒ–) |

**Rectified Flowçµ±åˆ**:

ç¬¬38å›ã§å­¦ã‚“ã ã‚ˆã†ã«ã€Rectified Flowã¯ODEã§ç›´ç·šçš„ãªè¼¸é€çµŒè·¯:
$$
\frac{dz_t}{dt} = v_\theta(z_t, t, c)
$$

FLUXã¯ $v_\theta$ ã‚’Transformerã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–:
$$
v_\theta(z_t, t, c) = \text{Transformer}(z_t, t, \text{CLIP}(c), \text{T5}(c))
$$

**DiT (Diffusion Transformer) é¢¨è¨­è¨ˆ**:

```mermaid
graph TD
    Z[z_t patches] --> PE[Positional Encoding]
    PE --> TB1[Transformer Block 1]
    TB1 --> TB2[Transformer Block 2]
    TB2 --> TBn[Transformer Block N]
    TBn --> Out[v_Î¸ prediction]

    T[Text: CLIP+T5] --> Cond[Conditioning]
    Cond --> TB1
    Cond --> TB2
    Cond --> TBn
```

**Transformer Blockã®æ§‹æˆ**:
```
Input: z_t patches + timestep t + text c
  â†“
Adaptive LayerNorm (conditioned on t, c)
  â†“
Self-Attention (å…¨patché–“)
  â†“
Adaptive LayerNorm
  â†“
Cross-Attention (patch â†” text)
  â†“
Adaptive LayerNorm
  â†“
MLP
  â†“
Output
```

**ãªãœFLUXãŒé€Ÿã„ï¼Ÿ**

1. **Rectified Flow**: ç›´ç·šçš„è¼¸é€ â†’ å°‘ã‚¹ãƒ†ãƒƒãƒ—ã§åæŸ
2. **é«˜æ¬¡å…ƒlatent**: 16ch â†’ ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾
3. **Transformer**: é•·è·é›¢ä¾å­˜ã‚’åŠ¹ç‡çš„ã«ã‚­ãƒ£ãƒ—ãƒãƒ£

**FLUX.1ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³**:

| ãƒ¢ãƒ‡ãƒ« | Params | é€Ÿåº¦ | å“è³ª | ç”¨é€” |
|:-------|:-------|:-----|:-----|:-----|
| **FLUX.1-pro** | 12B | 20 steps | æœ€é«˜ | APIå°‚ç”¨ |
| **FLUX.1-dev** | 12B | 20 steps | é«˜ | é–‹ç™ºç”¨ |
| **FLUX.1-schnell** | 12B | **4 steps** | ä¸­ | é«˜é€Ÿç”Ÿæˆ |

:::details FLUX Transformerã®å®Ÿè£…æ¦‚è¦
```julia
# FLUX Transformer Block (æ¦‚å¿µçš„)
struct FLUXTransformerBlock
    self_attn::MultiHeadAttention
    cross_attn::MultiHeadAttention
    mlp::MLP
    adaLN::AdaptiveLayerNorm
end

function (block::FLUXTransformerBlock)(z_patches, t_emb, text_emb)
    # Adaptive LayerNorm (timestep & text conditioned)
    z = block.adaLN(z_patches, t_emb, text_emb)

    # Self-Attention (å…¨patché–“)
    z = z + block.self_attn(z, z, z)

    # Cross-Attention (patch â†” text)
    z = block.adaLN(z, t_emb, text_emb)
    z = z + block.cross_attn(z, text_emb, text_emb)

    # MLP
    z = block.adaLN(z, t_emb, text_emb)
    z = z + block.mlp(z)

    return z
end
```
:::

### 3.13 å­¦ç¿’ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

**Noise Offset**:

Forward processã«ãƒã‚¤ã‚¢ã‚¹ã‚’åŠ ãˆã‚‹:
$$
z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} (\epsilon + \text{offset})
$$

**åŠ¹æœ**: æš—ã„ç”»åƒãƒ»æ˜ã‚‹ã„ç”»åƒã®ç”Ÿæˆå“è³ªå‘ä¸Šã€‚

**Min-SNR Weighting**:

Loss weightingã‚’SNRãƒ™ãƒ¼ã‚¹ã§èª¿æ•´:
$$
\mathcal{L}_\text{Min-SNR} = \mathbb{E}_{t} \left[ w(t) \|\epsilon - \epsilon_\theta(z_t, t)\|^2 \right]
$$

$$
w(t) = \min\left(\text{SNR}(t), \gamma\right), \quad \text{SNR}(t) = \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}
$$

å…¸å‹çš„ã« $\gamma = 5$ã€‚

**åŠ¹æœ**: 3.4å€è¨“ç·´é«˜é€ŸåŒ– [^min_snr]ã€‚

**v-prediction**:

Îµ-predictionã®ä»£ã‚ã‚Šã«velocity prediction:
$$
v_t = \sqrt{\bar{\alpha}_t} \epsilon - \sqrt{1-\bar{\alpha}_t} z_0
$$

$$
\mathcal{L}_v = \mathbb{E}_{t} \left[ \|v_t - v_\theta(z_t, t)\|^2 \right]
$$

**åŠ¹æœ**: æ•°å€¤å®‰å®šæ€§å‘ä¸Šãƒ»åæŸæ€§æ”¹å–„ã€‚

**Zero Terminal SNR**:

Noise scheduleã‚’å¼·åˆ¶çš„ã« $\bar{\alpha}_T = 0$ ã« rescale:
$$
\tilde{\alpha}_t = \frac{\alpha_t}{\alpha_T}
$$

**åŠ¹æœ**: éå¸¸ã«æ˜ã‚‹ã„/æš—ã„ç”»åƒã®ç”Ÿæˆå“è³ªå‘ä¸Š [^zero_snr]ã€‚

```julia
# Zero Terminal SNR rescaling
function rescale_to_zero_terminal_snr(alphas)
    alphas_cumprod = cumprod(alphas)
    sqrt_alphas_cumprod = sqrt.(alphas_cumprod)

    # Rescale
    sqrt_alphas_cumprod_final = sqrt_alphas_cumprod[end]
    sqrt_alphas_cumprod .= sqrt_alphas_cumprod ./ sqrt_alphas_cumprod_final

    return sqrt_alphas_cumprod
end
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®50%å®Œäº†ï¼ ãƒœã‚¹æˆ¦å‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€‚**

æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œäº†:
- VAE Encoder/Decoderåœ§ç¸®ã®æ•°å­¦
- æ½œåœ¨ç©ºé–“æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹
- Classifier Guidanceå®Œå…¨ç‰ˆ
- Classifier-Free Guidanceå®Œå…¨å°å‡º
- CFGç†è«–çš„ç†è§£ï¼ˆMode-Seeking / æ¸©åº¦ï¼‰
- Negative Prompt
- Text Conditioningï¼ˆCLIP / T5ï¼‰
- Cross-Attentionå®Œå…¨ç‰ˆ
- SD 1.x/2.x ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- SpatialTransformer
- FLUX Architectureè©³è§£
- å­¦ç¿’ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ï¼ˆNoise Offset / Min-SNR / v-prediction / Zero Terminal SNRï¼‰

æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ï¼
:::

### 3.14 SD vs FLUX: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°æ¯”è¼ƒ

**Stable Diffusion 1.x/2.x/SDXL Architecture**:

```
Input: Text prompt
  â†“
CLIP Text Encoder: [77, 768]
  â†“
Random Noise z_T: [64, 64, 4]
  â†“
U-Net (ResBlock + SpatialTransformer):
  - Down1: 64â†’32 (320ch, Cross-Attn)
  - Down2: 32â†’16 (640ch, Cross-Attn)
  - Down3: 16â†’8 (1280ch, Cross-Attn)
  - Middle: 8 (1280ch, Cross-Attn)
  - Up1: 8â†’16 (1280ch, Cross-Attn)
  - Up2: 16â†’32 (640ch, Cross-Attn)
  - Up3: 32â†’64 (320ch, Cross-Attn)
  â†“
Denoised z_0: [64, 64, 4]
  â†“
VAE Decoder (f=8)
  â†“
Image: [512, 512, 3]
```

**FLUX.1 Architecture**:

```
Input: Text prompt
  â†“
Dual Encoders:
  - CLIP ViT-L: [77, 768]
  - T5-XXL: [512, 4096]
  â†“
Random Noise z_T: [64, 64, 16]  # 4å€ã®ãƒãƒ£ãƒãƒ«!
  â†“
Patchify: [64, 64, 16] â†’ [1024, 768]  # 4Ã—4ãƒ‘ãƒƒãƒ
  â†“
Positional Encoding (RoPE)
  â†“
Transformer Blocks (N=24):
  - Adaptive LayerNorm (t, c conditioned)
  - Self-Attention (å…¨patché–“)
  - Cross-Attention (patch â†” CLIP+T5)
  - Gated FFN
  â†“
Unpatchify: [1024, 768] â†’ [64, 64, 16]
  â†“
VAE Decoder (f=8, 16ch input)
  â†“
Image: [512, 512, 3]
```

**è©³ç´°æ¯”è¼ƒè¡¨**:

| é …ç›® | SD 1.x/2.x | SDXL | FLUX.1 |
|:-----|:-----------|:-----|:-------|
| **Backbone** | U-Net | U-Net | **Transformer** |
| **Total Params** | 860M | 2.6B | **12B** |
| **Text Encoder** | CLIP (768) | CLIP+OpenCLIP (2048) | **CLIP+T5 (4864)** |
| **Latent Channels** | 4 | 4 | **16** |
| **Latent Res** | 64Ã—64 (f=8) | 128Ã—128 (f=8) | 64Ã—64 (f=8) |
| **Attention Type** | Cross-Attn in U-Net | Cross-Attn in U-Net | **Self+Cross in Transformer** |
| **Position Enc** | ãªã— (CNN) | ãªã— (CNN) | **RoPE** |
| **Flow Type** | DDPM | DDPM | **Rectified Flow** |
| **Timesteps** | 1000 | 1000 | 1000 (è¨“ç·´), 20-50 (æ¨è«–) |
| **CFG Default** | 7.5 | 7.5 | 3.5 (ä½ã‚ã§OK) |
| **Memory (fp16)** | 10GB | 16GB | **24GB** |
| **Speed (50 steps)** | ~5s (A100) | ~8s (A100) | **~3s (20 steps, A100)** |

### 3.15 Cross-Attentionè©³ç´°å®Ÿè£…

**Multi-Head Cross-Attentionå®Œå…¨ç‰ˆ**:

```julia
struct MultiHeadCrossAttention{F}
    num_heads::Int
    head_dim::Int
    qkv_dim::Int
    W_Q::Dense
    W_K::Dense
    W_V::Dense
    W_O::Dense
    dropout::Dropout
end

function MultiHeadCrossAttention(qkv_dim::Int, num_heads::Int; dropout_rate=0.1)
    head_dim = qkv_dim Ã· num_heads
    @assert qkv_dim == num_heads * head_dim "qkv_dim must be divisible by num_heads"

    return MultiHeadCrossAttention(
        num_heads,
        head_dim,
        qkv_dim,
        Dense(qkv_dim => qkv_dim),  # W_Q
        Dense(qkv_dim => qkv_dim),  # W_K
        Dense(qkv_dim => qkv_dim),  # W_V
        Dense(qkv_dim => qkv_dim),  # W_O
        Dropout(dropout_rate)
    )
end

function (mha::MultiHeadCrossAttention)(q, k, v, mask=nothing)
    # q: [N_q, d], k: [N_k, d], v: [N_k, d]
    batch_size = size(q, 1)

    # Linear projections
    Q = mha.W_Q(q)  # [N_q, d]
    K = mha.W_K(k)  # [N_k, d]
    V = mha.W_V(v)  # [N_k, d]

    # Reshape to multi-head: [N, d] â†’ [N, num_heads, head_dim]
    Q = reshape(Q, :, mha.num_heads, mha.head_dim)
    K = reshape(K, :, mha.num_heads, mha.head_dim)
    V = reshape(V, :, mha.num_heads, mha.head_dim)

    # Transpose for batch matrix multiply: [num_heads, N, head_dim]
    Q = permutedims(Q, (2, 1, 3))
    K = permutedims(K, (2, 1, 3))
    V = permutedims(V, (2, 1, 3))

    # Scaled dot-product attention
    scores = batched_mul(Q, batched_transpose(K)) / sqrt(Float32(mha.head_dim))
    # scores: [num_heads, N_q, N_k]

    # Apply mask if provided
    if mask !== nothing
        scores = scores .+ mask
    end

    # Softmax
    attn_weights = softmax(scores, dims=3)  # Over N_k
    attn_weights = mha.dropout(attn_weights)

    # Apply attention to values
    out = batched_mul(attn_weights, V)  # [num_heads, N_q, head_dim]

    # Transpose back: [num_heads, N_q, head_dim] â†’ [N_q, num_heads, head_dim]
    out = permutedims(out, (2, 1, 3))

    # Concat heads: [N_q, num_heads, head_dim] â†’ [N_q, d]
    out = reshape(out, :, mha.qkv_dim)

    # Final linear
    return mha.W_O(out)
end
```

**ä½¿ç”¨ä¾‹**:

```julia
# åˆæœŸåŒ–
d_model = 768
num_heads = 12
mha = MultiHeadCrossAttention(d_model, num_heads)

# U-Netä¸­é–“ç‰¹å¾´: [h*w, d_model]
f = randn(Float32, 64*64, d_model)

# Text embeddings: [77, d_model]
c = randn(Float32, 77, d_model)

# Cross-Attention
out = mha(f, c, c)  # Q from image, K/V from text
# out: [64*64, d_model]
```

### 3.16 VAEè¨“ç·´ã®è©³ç´°

**VAEæå¤±é–¢æ•°ã®å®Œå…¨å±•é–‹**:

$$
\begin{aligned}
\mathcal{L}_\text{VAE} &= \mathcal{L}_\text{rec} + \beta \cdot \mathcal{L}_\text{KL} \\
\mathcal{L}_\text{rec} &= \mathbb{E}_{q(z|x)}[-\log p_\theta(x|z)] \\
&\approx -\log p_\theta(x | \mathcal{E}(x)) \\
&= \text{MSE}(x, \mathcal{D}(\mathcal{E}(x))) \quad \text{or} \quad \text{LPIPS}(x, \mathcal{D}(\mathcal{E}(x))) \\
\mathcal{L}_\text{KL} &= \text{KL}[q_\phi(z|x) \| p(z)] \\
&= \text{KL}[\mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x)) \| \mathcal{N}(0, I)] \\
&= \frac{1}{2} \sum_{i=1}^d \left( \mu_i^2 + \sigma_i^2 - \log \sigma_i^2 - 1 \right)
\end{aligned}
$$

**Perceptual Loss (LPIPS)**:

LPIPSã¯VGGç‰¹å¾´ç©ºé–“ã§ã®è·é›¢:
$$
\mathcal{L}_\text{LPIPS}(x, \tilde{x}) = \sum_l w_l \|\Phi_l(x) - \Phi_l(\tilde{x})\|^2
$$

ã“ã“ã§ $\Phi_l$ ã¯VGGã®ç¬¬$l$å±¤ç‰¹å¾´ã€‚

```julia
# LPIPSæå¤± (ç°¡ç•¥ç‰ˆ)
function lpips_loss(x, x_recon, vgg_model, layers=[3, 8, 15, 22])
    loss = 0.0
    for layer in layers
        feat_x = vgg_model[1:layer](x)
        feat_recon = vgg_model[1:layer](x_recon)
        loss += mean((feat_x .- feat_recon).^2)
    end
    return loss / length(layers)
end

# VAEè¨“ç·´with LPIPS
function train_vae_lpips!(encoder, decoder, vgg, dataloader; Î²=0.1)
    for (x,) in dataloader
        # Encode
        Î¼, logÏƒÂ² = encoder(x)
        Ïƒ = exp.(0.5 .* logÏƒÂ²)
        Îµ = randn(size(Î¼))
        z = Î¼ .+ Ïƒ .* Îµ

        # Decode
        x_recon = decoder(z)

        # Losses
        recon_loss = lpips_loss(x, x_recon, vgg)
        kl_loss = 0.5 * mean(Î¼.^2 .+ Ïƒ.^2 .- logÏƒÂ² .- 1)

        loss = recon_loss + Î² * kl_loss
        # ... backprop
    end
end
```

### 3.17 Noise Scheduleã®è¨­è¨ˆç†è«–

**3ã¤ã®ä¸»è¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**:

**1. Linear Schedule** (DDPM original):
$$
\beta_t = \beta_\text{min} + \frac{t-1}{T-1} (\beta_\text{max} - \beta_\text{min})
$$

å…¸å‹å€¤: $\beta_\text{min} = 0.0001$, $\beta_\text{max} = 0.02$, $T = 1000$

**2. Cosine Schedule** (Improved DDPM):
$$
\bar{\alpha}_t = \frac{f(t)}{f(0)}, \quad f(t) = \cos\left( \frac{t/T + s}{1+s} \cdot \frac{\pi}{2} \right)^2
$$

$s = 0.008$ ãŒæ¨™æº–ã€‚

**3. Learned Schedule**:

$\beta_t$ ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã—ã¦å­¦ç¿’ã€‚

```julia
# 3ã¤ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè£…
function linear_beta_schedule(T::Int; Î²_start=1e-4, Î²_end=0.02)
    return range(Î²_start, Î²_end, length=T)
end

function cosine_alpha_bar_schedule(T::Int; s=0.008)
    t = 0:T
    f_t = cos.(((t ./ T) .+ s) ./ (1 + s) .* Ï€ ./ 2).^2
    Î±_bar = f_t ./ f_t[1]
    return Î±_bar[2:end]
end

function learned_beta_schedule(T::Int; init_Î²_start=1e-4, init_Î²_end=0.02)
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–Î²ã‚’å­¦ç¿’
    logit_Î² = range(logit(init_Î²_start), logit(init_Î²_end), length=T)
    return logit_Î²  # è¨“ç·´ä¸­ã«æœ€é©åŒ–
end

# Zero Terminal SNR rescaling
function rescale_zero_terminal_snr(Î±_bar)
    # Î±_bar[T] = 0 ã‚’å¼·åˆ¶
    return Î±_bar ./ Î±_bar[end]
end
```

**SNRã®å¯è¦–åŒ–**:

```julia
using Plots

T = 1000
betas_linear = linear_beta_schedule(T)
Î±_bar_cosine = cosine_alpha_bar_schedule(T)

# SNRè¨ˆç®—
snr_linear = cumprod(1 .- betas_linear) ./ (1 .- cumprod(1 .- betas_linear))
snr_cosine = Î±_bar_cosine ./ (1 .- Î±_bar_cosine)

# Plot
plot(1:T, snr_linear, label="Linear", yscale=:log10, xlabel="Timestep", ylabel="SNR")
plot!(1:T, snr_cosine, label="Cosine")
title!("SNR Schedule Comparison")
```

### 3.18 Sampling Algorithmså®Œå…¨æ¯”è¼ƒ

| ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | ã‚¹ãƒ†ãƒƒãƒ—æ•° | å“è³ª | é€Ÿåº¦ | æ±ºå®šè«–çš„ | å‚™è€ƒ |
|:-------------|:-----------|:-----|:-----|:---------|:-----|
| **DDPM** | 1000 | æœ€é«˜ | æœ€é… | âœ— | ç¢ºç‡çš„ã€å…¨ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ |
| **DDIM** | 50-100 | é«˜ | ä¸­ | âœ“ | æ±ºå®šè«–çš„ã€ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½ |
| **DPM-Solver++** | 20-30 | é«˜ | é«˜ | âœ“ | é«˜æ¬¡ODE solver |
| **UniPC** | 10-20 | ä¸­ | æœ€é«˜ | âœ“ | Predictor-Corrector |
| **PNDM** | 50 | é«˜ | ä¸­ | âœ— | Pseudo Numerical |
| **LMS** | 50 | ä¸­ | ä¸­ | âœ“ | Linear Multi-Step |

**DDIMå®Œå…¨ç‰ˆ**:

$$
\begin{aligned}
z_{t-\Delta t} &= \sqrt{\bar{\alpha}_{t-\Delta t}} \underbrace{\left( \frac{z_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(z_t, t)}{\sqrt{\bar{\alpha}_t}} \right)}_{\text{pred } x_0} \\
&\quad + \underbrace{\sqrt{1-\bar{\alpha}_{t-\Delta t} - \sigma_t^2} \cdot \epsilon_\theta(z_t, t)}_{\text{direction to } z_{t-\Delta t}} \\
&\quad + \underbrace{\sigma_t \epsilon}_{\text{random noise}}
\end{aligned}
$$

$\sigma_t = 0$ ã§å®Œå…¨æ±ºå®šè«–çš„ã€$\sigma_t = \sqrt{(1-\bar{\alpha}_{t-\Delta t})/(1-\bar{\alpha}_t)} \sqrt{1-\bar{\alpha}_t/\bar{\alpha}_{t-\Delta t}}$ ã§DDPMã¨åŒç­‰ã€‚

```julia
function ddim_step(z_t, Îµ_Î¸, t, t_prev, Î±_bar; Î·=0.0)
    # Predict xâ‚€
    Î±_t = Î±_bar[t]
    Î±_prev = t_prev > 0 ? Î±_bar[t_prev] : 1.0

    pred_xâ‚€ = (z_t .- sqrt(1 - Î±_t) .* Îµ_Î¸) ./ sqrt(Î±_t)

    # Direction
    Ïƒ_t = Î· * sqrt((1 - Î±_prev) / (1 - Î±_t)) * sqrt(1 - Î±_t / Î±_prev)
    dir_z = sqrt(1 - Î±_prev - Ïƒ_t^2) .* Îµ_Î¸

    # Noise
    noise = Ïƒ_t .* randn(Float32, size(z_t))

    # Combine
    z_prev = sqrt(Î±_prev) .* pred_xâ‚€ .+ dir_z .+ noise
    return z_prev
end
```

**DPM-Solver++ (æ¦‚è¦)**:

é«˜æ¬¡ODE solverã§DDIMã‚’æ”¹å–„:
$$
z_{t-\Delta t} = z_t + \int_t^{t-\Delta t} f(z_s, s) ds
$$

3æ¬¡Adams-Bashforthæ³•ã§è¿‘ä¼¼ â†’ 20ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ªã€‚

### âš”ï¸ Boss Battle: CFGã®å®Œå…¨åˆ†è§£

**èª²é¡Œ**: Classifier-Free Guidanceã®æ•°å¼ã‚’ã€3ã¤ã®è¦–ç‚¹ã‹ã‚‰å®Œå…¨ã«å°å‡ºã›ã‚ˆã€‚

**è¦–ç‚¹1: Îµ-prediction**

ç›®æ¨™: ä¿®æ­£ã•ã‚ŒãŸãƒã‚¤ã‚ºäºˆæ¸¬ $\tilde{\epsilon}_\theta(z_t, t, c, w)$ ã‚’æ±‚ã‚ã‚ˆã€‚

**è§£ç­”**:
$$
\begin{aligned}
\tilde{\epsilon}_\theta(z_t, t, c, w) &= \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right) \\
&= (1-w) \epsilon_\theta(z_t, t, \emptyset) + w \cdot \epsilon_\theta(z_t, t, c)
\end{aligned}
$$

**è¦–ç‚¹2: ã‚¹ã‚³ã‚¢é–¢æ•°**

ç›®æ¨™: ä¿®æ­£ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ $\tilde{s}_\theta(z_t, t, c, w)$ ã‚’æ±‚ã‚ã‚ˆã€‚

**è§£ç­”**:

ã‚¹ã‚³ã‚¢ $s_\theta = -\frac{\epsilon_\theta}{\sqrt{1-\bar{\alpha}_t}}$ ã‚ˆã‚Š:
$$
\begin{aligned}
\tilde{s}_\theta(z_t, t, c, w) &= -\frac{\tilde{\epsilon}_\theta}{\sqrt{1-\bar{\alpha}_t}} \\
&= -\frac{1}{\sqrt{1-\bar{\alpha}_t}} \left[ \epsilon_\theta(z_t, t, \emptyset) + w(\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset)) \right] \\
&= s_\theta(z_t, t, \emptyset) + w \cdot (s_\theta(z_t, t, c) - s_\theta(z_t, t, \emptyset)) \\
&= (1-w) s_\theta(z_t, t, \emptyset) + w \cdot s_\theta(z_t, t, c)
\end{aligned}
$$

**è¦–ç‚¹3: ç¢ºç‡åˆ†å¸ƒ**

ç›®æ¨™: $w > 1$ ã®ã¨ãã®å®ŸåŠ¹ç¢ºç‡åˆ†å¸ƒ $p_w(z_t | c)$ ã‚’æ±‚ã‚ã‚ˆã€‚

**è§£ç­”**:

ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã‚ˆã‚Š $s(z) = \nabla_z \log p(z)$ ãªã®ã§:
$$
\begin{aligned}
\tilde{s}_\theta(z_t, t, c, w) &= \nabla_{z_t} \log \tilde{p}(z_t | c) \\
&= (1-w) \nabla_{z_t} \log p(z_t) + w \nabla_{z_t} \log p(z_t | c) \\
&= \nabla_{z_t} \left[ (1-w) \log p(z_t) + w \log p(z_t | c) \right] \\
&= \nabla_{z_t} \log p_w(z_t | c)
\end{aligned}
$$

ã‚ˆã£ã¦:
$$
p_w(z_t | c) \propto p(z_t)^{1-w} \cdot p(z_t | c)^w
$$

$w > 1$ ã®ã¨ãã€æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’ **over-emphasize** â†’ mode-seekingã€‚

**æ•°å€¤æ¤œè¨¼**:
```julia
# CFGã®3è¦–ç‚¹æ¤œè¨¼
w = 7.5
Îµ_uncond = randn(Float32, 64, 64, 4)
Îµ_cond = randn(Float32, 64, 64, 4)

# è¦–ç‚¹1: Îµ-prediction
Îµ_cfg1 = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)
Îµ_cfg2 = (1 - w) .* Îµ_uncond .+ w .* Îµ_cond

@assert isapprox(Îµ_cfg1, Îµ_cfg2)  # ç­‰ä¾¡æ€§ç¢ºèª

# è¦–ç‚¹2: ã‚¹ã‚³ã‚¢
Î±_bar = 0.5
s_uncond = -Îµ_uncond ./ sqrt(1 - Î±_bar)
s_cond = -Îµ_cond ./ sqrt(1 - Î±_bar)
s_cfg = (1 - w) .* s_uncond .+ w .* s_cond

# è¦–ç‚¹3: ç¢ºç‡
log_p_uncond = -0.5 * sum(Îµ_uncond.^2)
log_p_cond = -0.5 * sum(Îµ_cond.^2)
log_p_w = (1 - w) * log_p_uncond + w * log_p_cond

println("CFG 3è¦–ç‚¹æ¤œè¨¼å®Œäº†ï¼")
```

**ãƒœã‚¹æ’ƒç ´ï¼** CFGã®æ•°å­¦çš„æ§‹é€ ã‚’å®Œå…¨ç†è§£ã—ãŸã€‚

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
