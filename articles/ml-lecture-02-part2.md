---
title: "ç¬¬2å›: ç·šå½¢ä»£æ•° I: ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»åŸºåº• â€” 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ“"
type: "tech"
topics: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç·šå½¢ä»£æ•°", "æ•°å­¦", "Python"]
published: true
slug: "ml-lecture-02-part2"
difficulty: "intermediate"
time_estimate: "90 minutes"
languages: ["Python"]
keywords: ["NumPy", "einsum", "å›ºæœ‰å€¤", "Cholesky", "æœ€å°äºŒä¹—æ³•"]
---

> **ğŸ“– ã“ã®è¨˜äº‹ã¯å¾Œç·¨ï¼ˆå®Ÿè£…ç·¨ï¼‰ã§ã™**
> ç†è«–ç·¨ã¯ [ã€å‰ç·¨ã€‘ç¬¬2å›: ç·šå½¢ä»£æ•° I â€” ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»åŸºåº•](/articles/ml-lecture-02-part1) ã‚’ã”è¦§ãã ã•ã„ã€‚

---

## ğŸ’» Z5. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” NumPyã§ç·šå½¢ä»£æ•°ã‚’æ“ã‚‹

### 4.1 NumPy ã®ç·šå½¢ä»£æ•°ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ

NumPyã® `np.linalg` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ç·šå½¢ä»£æ•°ã®ä¸»è¦ãªæ¼”ç®—ã‚’å…¨ã¦ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã€‚ã“ã“ã§ã¯å®Ÿç”¨ä¸Šæœ€ã‚‚é‡è¦ãªé–¢æ•°ã‚’æ•´ç†ã™ã‚‹ã€‚

| é–¢æ•° | æ•°å¼ | ç”¨é€” |
|:-----|:-----|:-----|
| `A @ B` | $AB$ | è¡Œåˆ—ç© |
| `np.linalg.inv(A)` | $A^{-1}$ | é€†è¡Œåˆ—ï¼ˆéæ¨å¥¨ã€solveã‚’ä½¿ãˆï¼‰ |
| `np.linalg.solve(A, b)` | $A^{-1}\mathbf{b}$ | é€£ç«‹æ–¹ç¨‹å¼ |
| `np.linalg.eigh(A)` | $A = Q\Lambda Q^\top$ | å¯¾ç§°è¡Œåˆ—ã®å›ºæœ‰å€¤åˆ†è§£ |
| `np.linalg.svd(A)` | $A = U\Sigma V^\top$ | ç‰¹ç•°å€¤åˆ†è§£ï¼ˆç¬¬3å›ï¼‰ |
| `np.linalg.qr(A)` | $A = QR$ | QRåˆ†è§£ |
| `np.linalg.cholesky(A)` | $A = LL^\top$ | Choleskyåˆ†è§£ |
| `np.linalg.norm(x)` | $\|\mathbf{x}\|$ | ãƒãƒ«ãƒ  |
| `np.linalg.det(A)` | $\det(A)$ | è¡Œåˆ—å¼ |
| `np.trace(A)` | $\text{tr}(A)$ | ãƒˆãƒ¬ãƒ¼ã‚¹ |
| `np.linalg.matrix_rank(A)` | $\text{rank}(A)$ | ãƒ©ãƒ³ã‚¯ |
| `np.linalg.lstsq(A, b)` | $\hat{\mathbf{x}} = \arg\min\|A\mathbf{x} - \mathbf{b}\|^2$ | æœ€å°äºŒä¹— |

> **âš ï¸ Warning:** Section 3.3 ã§è¿°ã¹ãŸã¨ãŠã‚Šã€é€†è¡Œåˆ—ã®ç›´æ¥è¨ˆç®—ã¯é¿ã‘ã¾ã—ã‚‡ã† [^8]ã€‚

è¡¨ã®é–¢æ•°ã‚’ä¸€é€šã‚Šå‹•ã‹ã—ã¦ã€ä½•ãŒè¿”ã£ã¦ãã‚‹ã‹æ‰‹ã§ç¢ºèªã—ã¦ãŠã“ã†ã€‚

$$
A = LL^\top = Q\Lambda Q^\top \quad (A: \text{æ­£å®šå€¤å¯¾ç§°è¡Œåˆ—})
$$

- `np.linalg.eigh` ã¯å›ºæœ‰å€¤ã‚’æ˜‡é †ã«è¿”ã™ï¼ˆ$\lambda_1 \leq \lambda_2 \leq \cdots$ï¼‰â€” é™é †ãŒæ¬²ã—ã‘ã‚Œã° `[::-1]` ã§åè»¢
- `np.linalg.cholesky` ã¯ä¸‹ä¸‰è§’è¡Œåˆ— $L$ ã‚’è¿”ã™ï¼ˆ$A = LL^\top$ã€$L^\top$ ã§ã¯ãªã $L$ï¼‰
- `np.linalg.norm(A)` ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯**Frobeniusãƒãƒ«ãƒ **ï¼ˆè¡Œåˆ—ã®å ´åˆï¼‰â€” ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã¯ `norm(A, 2)`

```python
import numpy as np

# æ­£å®šå€¤å¯¾ç§°è¡Œåˆ—ã®ä½œæˆ
np.random.seed(7)
B = np.random.randn(4, 4)
A = B.T @ B + np.eye(4) * 0.1   # B^T B + ÎµI ã¯å¿…ãšæ­£å®šå€¤
print("A =\n", np.round(A, 3))

# â”€â”€ é€£ç«‹æ–¹ç¨‹å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
b = np.array([1.0, -1.0, 2.0, 0.5])
x_solve = np.linalg.solve(A, b)         # æ¨å¥¨: LUåˆ†è§£ã‚’ä½¿ã†
# x_inv = np.linalg.inv(A) @ b          # éæ¨å¥¨: é…ãæ•°å€¤ä¸å®‰å®š
print(f"\nAx=b ã®è§£: {np.round(x_solve, 4)}")
print(f"æ®‹å·® ||Ax - b||: {np.linalg.norm(A @ x_solve - b):.2e}")

# â”€â”€ å›ºæœ‰å€¤åˆ†è§£ï¼ˆå¯¾ç§°è¡Œåˆ—å°‚ç”¨ eighï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
eigvals, Q = np.linalg.eigh(A)          # æ˜‡é †
print(f"\nå›ºæœ‰å€¤ (æ˜‡é †): {np.round(eigvals, 3)}")
print(f"ç›´äº¤æ€§ ||Q^T Q - I||_F: {np.linalg.norm(Q.T @ Q - np.eye(4)):.2e}")
print(f"å†æ§‹æˆ ||QÎ›Q^T - A||_F: {np.linalg.norm(Q @ np.diag(eigvals) @ Q.T - A):.2e}")

# â”€â”€ Choleskyåˆ†è§£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
L = np.linalg.cholesky(A)               # ä¸‹ä¸‰è§’è¡Œåˆ—
print(f"\nL ã®å¯¾è§’: {np.round(np.diag(L), 3)}")
print(f"å†æ§‹æˆ ||LL^T - A||_F: {np.linalg.norm(L @ L.T - A):.2e}")

# â”€â”€ ãƒãƒ«ãƒ ã¨æ¡ä»¶æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
frob   = np.linalg.norm(A)              # Frobenius (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
spec   = np.linalg.norm(A, 2)          # ã‚¹ãƒšã‚¯ãƒˆãƒ« (æœ€å¤§ç‰¹ç•°å€¤)
cond   = np.linalg.cond(A)             # æ¡ä»¶æ•° = Ïƒ_max / Ïƒ_min
print(f"\n||A||_F = {frob:.4f}, ||A||_2 = {spec:.4f}, Îº(A) = {cond:.2f}")
# æ­£å®šå€¤ãªã®ã§ Îº(A) = Î»_max / Î»_min
kappa_from_eig = eigvals[-1] / eigvals[0]
assert np.isclose(cond, kappa_from_eig, rtol=1e-5)
print(f"æ¡ä»¶æ•° (å›ºæœ‰å€¤æ¯”) = {kappa_from_eig:.2f}  âœ“")
```



`np.einsum` ã¯ Einstein è¨˜æ³•ï¼ˆæ·»å­—ã®ç¸®ç´„è¦å‰‡ï¼‰ã«åŸºã¥ãæ±ç”¨çš„ãªé…åˆ—æ¼”ç®—é–¢æ•°ã ã€‚ã“ã‚Œã‚’ä½¿ã„ã“ãªã™ã¨ã€è¤‡é›‘ãªè¡Œåˆ—æ¼”ç®—ã‚’ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼ã§æ›¸ã‘ã‚‹ã€‚

åŸºæœ¬ãƒ«ãƒ¼ãƒ«: **åŒã˜æ·»å­—ãŒ2å›ç¾ã‚ŒãŸã‚‰ã€ãã®æ·»å­—ã§ç·å’Œã‚’å–ã‚‹**ã€‚

| æ¼”ç®— | æ•°å¼ | einsum |
|:-----|:-----|:-------|
| å†…ç© | $\mathbf{a}^\top\mathbf{b} = \sum_i a_i b_i$ | `np.einsum('i,i->', a, b)` |
| å¤–ç© | $\mathbf{a}\mathbf{b}^\top$ | `np.einsum('i,j->ij', a, b)` |
| è¡Œåˆ—ç© | $C_{ij} = \sum_k A_{ik}B_{kj}$ | `np.einsum('ik,kj->ij', A, B)` |
| è¡Œåˆ—ã®ãƒˆãƒ¬ãƒ¼ã‚¹ | $\text{tr}(A) = \sum_i A_{ii}$ | `np.einsum('ii->', A)` |
| è¡Œåˆ—è»¢ç½® | $B_{ij} = A_{ji}$ | `np.einsum('ij->ji', A)` |
| ãƒãƒƒãƒè¡Œåˆ—ç© | $C_{bij} = \sum_k A_{bik}B_{bkj}$ | `np.einsum('bik,bkj->bij', A, B)` |
| äºŒæ¬¡å½¢å¼ | $\mathbf{x}^\top A \mathbf{x}$ | `np.einsum('i,ij,j->', x, A, x)` |

$$
\mathbf{a}^\top\mathbf{b}=\sum_i a_i b_i

C_{ij}=\sum_k A_{ik}B_{kj}

\mathbf{x}^\top M\mathbf{x}=\sum_{i,j}x_i M_{ij}x_j
$$

```python
import numpy as np

np.random.seed(42)
a = np.random.randn(5)
b = np.random.randn(5)

A = np.random.randn(3, 4)
B = np.random.randn(4, 2)

M = np.array([[2.0, 1.0], [1.0, 3.0]])
x = np.array([1.0, 2.0])

# dot: a^T b
dot_std = a @ b
dot_ein = np.einsum("i,i->", a, b)
assert np.allclose(dot_std, dot_ein)

# matmul: C = A B
C_std = A @ B
C_ein = np.einsum("ik,kj->ij", A, B)
assert np.allclose(C_std, C_ein)

# quadratic form: x^T M x
q_std = x @ M @ x
q_ein = np.einsum("i,ij,j->", x, M, x)
assert np.allclose(q_std, q_ein)

print("einsum sanity checks: ok")
```

<details><summary>einsum vs @ æ¼”ç®—å­ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹</summary>
å°ã•ãªè¡Œåˆ—ã§ã¯einsumã®æ–¹ãŒã‚ãšã‹ã«é…ã„ï¼ˆPythonå´ã®ãƒ‘ãƒ¼ã‚¹å‡¦ç†ãŒã‚ã‚‹ãŸã‚ï¼‰ã€‚å¤§ããªè¡Œåˆ—ã‚„ãƒãƒƒãƒæ¼”ç®—ã§ã¯å·®ã¯ã»ã¼æ¶ˆãˆã‚‹ã€‚å¯èª­æ€§ã‚’é‡è¦–ã™ã‚‹å ´åˆã¯einsumã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€å„ªå…ˆãªã‚‰`@`æ¼”ç®—å­ã‚’ä½¿ã†ã€‚

PyTorch ã§ã‚‚ `torch.einsum` ãŒä½¿ãˆã€åŒã˜æ·»å­—è¦å‰‡ã®ã¾ã¾è‡ªå‹•å¾®åˆ†ã«ä¹—ã‚‹ï¼ˆè©³ç´°ã¯ç¬¬3å›ï¼‰ã€‚
</details>

### 4.3 ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ â€” Row-major vs Column-major

è¡Œåˆ—ã®ãƒ¡ãƒ¢ãƒªä¸Šã§ã®æ ¼ç´é †åºãŒè¨ˆç®—é€Ÿåº¦ã«ç›´çµã™ã‚‹ã€‚

| æ–¹å¼ | è¡Œåˆ— $A_{ij}$ ã®æ ¼ç´é † | è¨€èª/ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
|:-----|:---------------------|:-------------|
| **Row-major (C order)** | $A_{00}, A_{01}, A_{02}, A_{10}, \ldots$ | C, Python/NumPy, PyTorch |
| **Column-major (Fortran order)** | $A_{00}, A_{10}, A_{20}, A_{01}, \ldots$ | Fortran, Julia, MATLAB, R |

**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡**: ãƒ¡ãƒ¢ãƒªã¯é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ãŒé€Ÿã„ã€‚Row-majorã§ã¯**è¡Œæ–¹å‘**ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒé«˜é€Ÿã€Column-majorã§ã¯**åˆ—æ–¹å‘**ãŒé«˜é€Ÿã€‚

> **Note:** **ãªãœã“ã‚ŒãŒé‡è¦ã‹**: è¡Œåˆ—ç© $C = AB$ ã‚’å®Ÿè£…ã™ã‚‹ã¨ãã€ãƒŠã‚¤ãƒ¼ãƒ–ãª3é‡ãƒ«ãƒ¼ãƒ—ã®é †åº (i, j, k) vs (i, k, j) ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãŒå¤§ããå¤‰ã‚ã‚Šã€æ€§èƒ½ãŒæ•°å€å¤‰ã‚ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚NumPy ã¯å†…éƒ¨ã§æœ€é©åŒ–ã•ã‚ŒãŸ BLASï¼ˆBasic Linear Algebra Subprogramsï¼‰ã‚’å‘¼ã‚“ã§ã„ã‚‹ã®ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ„è­˜ã™ã‚‹å¿…è¦ã¯å°‘ãªã„ãŒã€Juliaã‚„Rustç­‰ã§è‡ªå‰å®Ÿè£…ã™ã‚‹å ´åˆã¯å¿…é ˆã®çŸ¥è­˜ã ã€‚ç¬¬9å›ï¼ˆJuliaç™»å ´ï¼‰ã¨ç¬¬11å›ï¼ˆRustç™»å ´ï¼‰ã§æ”¹ã‚ã¦æ‰±ã†ã€‚

ã€Œæœ¬å½“ã«å·®ãŒå‡ºã‚‹ã®ã‹ã€ã¯è‡ªåˆ†ã®æ‰‹ã§æ¸¬ã‚‹ã®ãŒä¸€ç•ªé€Ÿã„ã€‚`axis=1`ï¼ˆè¡Œæ–¹å‘ï¼‰ã¨ `axis=0`ï¼ˆåˆ—æ–¹å‘ï¼‰ã§ `np.sum` ã®æ™‚é–“ã‚’æ¯”ã¹ã‚‹ã ã‘ã§ã‚‚ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±€æ‰€æ€§ã®å·®ãŒå‡ºã‚‹ã€‚

### 4.4 ç·šå½¢ä»£æ•°ã®è¨ˆç®—é‡

å„æ¼”ç®—ã®è¨ˆç®—é‡ã‚’çŸ¥ã£ã¦ãŠãã¨ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’åˆ¤æ–­ã§ãã‚‹ã€‚

| æ¼”ç®— | è¨ˆç®—é‡ | å‚™è€ƒ |
|:-----|:------|:-----|
| ãƒ™ã‚¯ãƒˆãƒ«å†…ç© | $O(n)$ | |
| è¡Œåˆ—-ãƒ™ã‚¯ãƒˆãƒ«ç© | $O(mn)$ | $A \in \mathbb{R}^{m \times n}$ |
| è¡Œåˆ—-è¡Œåˆ—ç© | $O(mnp)$ | $A \in \mathbb{R}^{m \times n}, B \in \mathbb{R}^{n \times p}$ |
| LUåˆ†è§£ | $O(\frac{2}{3}n^3)$ | é€£ç«‹æ–¹ç¨‹å¼ |
| Choleskyåˆ†è§£ | $O(\frac{1}{3}n^3)$ | æ­£å®šå€¤è¡Œåˆ— |
| QRåˆ†è§£ | $O(\frac{4}{3}n^3)$ | Householderæ³• |
| å›ºæœ‰å€¤åˆ†è§£ | $O(n^3)$ | QRã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
| SVD | $O(mn\min(m,n))$ | ç¬¬3å›ã§è©³èª¬ |
| Attention $QK^\top$ | $O(n^2 d)$ | ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·$n$ã®äºŒä¹—! |

<details><summary>Strassenã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ç†è«–é™ç•Œ</summary>
è¡Œåˆ—ç©ã®è¨ˆç®—é‡ã¯é•·ã‚‰ã $O(n^3)$ ãŒæœ€å–„ã¨è€ƒãˆã‚‰ã‚Œã¦ã„ãŸãŒã€1969å¹´ã«StrassenãŒ $O(n^{2.807})$ ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç™ºè¦‹ã—ãŸã€‚ç¾åœ¨ã®ç†è«–çš„æœ€å–„ã¯ $O(n^{2.3728\ldots})$ [Alman & Vassilevska Williams, 2021] ã ãŒã€å®šæ•°ãŒå¤§ããå®Ÿç”¨ã•ã‚Œã¦ã„ãªã„ã€‚

GPUä¸Šã®è¡Œåˆ—ç©ã¯ã€NVIDIA ã® cuBLAS ãŒæœ€é©åŒ–ã—ã¦ãŠã‚Šã€Tensor Core ã‚’ä½¿ãˆã°FP16ã§ç†è«–é™ç•Œã«è¿‘ã„æ€§èƒ½ãŒå‡ºã‚‹ã€‚Transformerã®è¨“ç·´é€Ÿåº¦ã¯ã€æœ¬è³ªçš„ã«ã“ã®è¡Œåˆ—ç©ã®é€Ÿåº¦ã§æ±ºã¾ã‚‹ã€‚
</details>

### 4.5 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³

ç·šå½¢ä»£æ•°ã®æ•°å¼ã‚’ã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã™ã‚‹7ã¤ã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³:

| # | æ•°å¼ãƒ‘ã‚¿ãƒ¼ãƒ³ | ã‚³ãƒ¼ãƒ‰ | ä¾‹ |
|:--|:-----------|:------|:---|
| 1 | $\mathbf{a}^\top\mathbf{b}$ | `np.dot(a, b)` or `a @ b` | å†…ç© |
| 2 | $AB$ | `A @ B` | è¡Œåˆ—ç© |
| 3 | $A^\top$ | `A.T` | è»¢ç½® |
| 4 | $A^{-1}\mathbf{b}$ | `np.linalg.solve(A, b)` | é€£ç«‹æ–¹ç¨‹å¼ |
| 5 | $\|x\|_2$ | `np.linalg.norm(x)` | L2ãƒãƒ«ãƒ  |
| 6 | $\text{diag}(\lambda_1, \ldots)$ | `np.diag(lambdas)` | å¯¾è§’è¡Œåˆ— |
| 7 | $\sum_{ij} A_{ij} B_{ij}$ | `np.einsum('ij,ij->', A, B)` | Frobeniuså†…ç© |
> **Note:** ã“ã‚Œã‚‰ã®ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€å¾Œã®è¬›ç¾©ï¼ˆç¬¬6å›ã®KL/CEã€VAEã®ELBOã€Diffusionã®ã‚¹ã‚³ã‚¢ç­‰ï¼‰ã§ã€Œãã®ã¾ã¾ã€å‡ºã¦ãã‚‹ã€‚ã“ã“ã§æ‰‹ã«é¦´æŸ“ã¾ã›ã‚‹ã¨ã€ä»¥é™ã®æ•°å¼ãŒæ€¥ã«èª­ã¿ã‚„ã™ããªã‚‹ã€‚

### 4.6 è¡Œåˆ—ã®æŒ‡æ•°é–¢æ•° $\exp(A)$

è¡Œåˆ—ã®æŒ‡æ•°é–¢æ•°ã¯ã€SSMï¼ˆState Space Modelsã€ç¬¬26å›ï¼‰ã®ä¸­æ ¸:

$$
\exp(A) = \sum_{k=0}^{\infty} \frac{A^k}{k!} = I + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \cdots
$$

$A$ ãŒå¯¾è§’åŒ–å¯èƒ½ãªã‚‰: $\exp(A) = V \exp(\Lambda) V^{-1} = V \text{diag}(e^{\lambda_1}, \ldots, e^{\lambda_n}) V^{-1}$

```python
import numpy as np
from scipy.linalg import expm

# 2Ã—2 è¡Œåˆ—ã®æŒ‡æ•°é–¢æ•°ã‚’3ã¤ã®æ–¹æ³•ã§è¨ˆç®—ã—ã¦æ¯”è¼ƒ
A = np.array([[0.0, -1.0],
              [1.0,  0.0]])   # 90åº¦å›è»¢ã®ç”Ÿæˆå­

# æ–¹æ³•1: scipy.linalg.expmï¼ˆPadÃ©è¿‘ä¼¼ã€æ•°å€¤çš„ã«å®‰å®šï¼‰
eA_scipy = expm(A)

# æ–¹æ³•2: å›ºæœ‰å€¤åˆ†è§£çµŒç”±  exp(A) = V exp(Î›) V^{-1}
eigvals, V = np.linalg.eig(A)
eA_eig = V @ np.diag(np.exp(eigvals)) @ np.linalg.inv(V)

# æ–¹æ³•3: ã¹ãç´šæ•°ï¼ˆtruncatedã€å‚è€ƒã®ã¿ï¼‰
eA_series = np.eye(2)
Ak = np.eye(2)
for k in range(1, 20):
    Ak = Ak @ A / k
    eA_series += Ak

print("exp(A) via scipy:\n", np.round(eA_scipy.real, 6))
print("exp(A) via eig:  \n", np.round(eA_eig.real, 6))
# exp([[0,-1],[1,0]]) = [[cos1, -sin1],[sin1, cos1]] (å›è»¢è¡Œåˆ—)
```

> **SSMã¸ã®äºˆå‘Š**: ç¬¬26å›ï¼ˆState Space Models / Mambaï¼‰ã§ã¯ã€$\exp(A\Delta t)$ ã®åŠ¹ç‡çš„ãªè¨ˆç®—ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å·¦å³ã™ã‚‹ã€‚é€£ç¶šæ™‚é–“ã®çŠ¶æ…‹æ–¹ç¨‹å¼ $\dot{\mathbf{x}} = A\mathbf{x} + B\mathbf{u}$ ã‚’é›¢æ•£åŒ–ã™ã‚‹éš›ã«ã“ã®è¡Œåˆ—æŒ‡æ•°é–¢æ•°ãŒç™»å ´ã™ã‚‹ã€‚è¦šãˆã¦ãŠã„ã¦ã»ã—ã„ã€‚

### 4.7 æ•°å€¤è¨ˆç®—ã®ãƒ”ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ«

ç·šå½¢ä»£æ•°ã®è¨ˆç®—ã¯ã€ç†è«–çš„ã«ã¯æ­£ã—ãã¦ã‚‚æ•°å€¤çš„ã«ç ´ç¶»ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚å®Ÿè£…è€…ã¯ä»¥ä¸‹ã®è½ã¨ã—ç©´ã‚’çŸ¥ã£ã¦ãŠãå¿…è¦ãŒã‚ã‚‹ã€‚

| ãƒ”ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ« | åŸå›  | å¯¾ç­– |
|:-------------|:-----|:-----|
| æµ®å‹•å°æ•°ç‚¹ã®ç­‰å·æ¯”è¼ƒ | ä¸¸ã‚èª¤å·® | `np.allclose(a, b, atol=1e-10)` ã‚’ä½¿ã† |
| é€†è¡Œåˆ—ã®æ˜ç¤ºè¨ˆç®— | æ¡ä»¶æ•°ãŒå¤§ãã„ã¨ä¸å®‰å®š | `np.linalg.solve` ã‚’ä½¿ã† |
| å¤§è¡Œåˆ—ã®è¡Œåˆ—å¼ | ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼/ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼ | `np.linalg.slogdet` ã§å¯¾æ•°ã‚’å–ã‚‹ |
| Gram-Schmidt ã®ç›´äº¤æ€§åŠ£åŒ– | æµ®å‹•å°æ•°ç‚¹èª¤å·®ã®è“„ç© | Modified Gram-Schmidt or QRåˆ†è§£ã‚’ä½¿ã† |
| å›ºæœ‰å€¤ã®é †åºä»®å®š | `eig` ã¯å›ºæœ‰å€¤ã‚’ã‚½ãƒ¼ãƒˆã—ãªã„ | `eigh` ã‚’ä½¿ã†ã€ã¾ãŸã¯æ˜ç¤ºçš„ã«ã‚½ãƒ¼ãƒˆ |
| å¯¾ç§°æ€§ã®ä»®å®šå´©ã‚Œ | ä¸¸ã‚èª¤å·®ã§ $A \neq A^\top$ | `A = (A + A.T) / 2` ã§å¼·åˆ¶å¯¾ç§°åŒ– |

```python
import numpy as np

np.random.seed(0)

# --- Pitfall 1: é€†è¡Œåˆ—ã®ç›´æ¥è¨ˆç®— vs solve ---
n = 200
A = np.random.randn(n, n)
b = np.random.randn(n)

x_inv   = np.linalg.inv(A) @ b          # éæ¨å¥¨: O(n^3) Ã— 2 å›
x_solve = np.linalg.solve(A, b)         # æ¨å¥¨: LUåˆ†è§£1å›
print(f"inv vs solve æ®‹å·®: {np.max(np.abs(x_inv - x_solve)):.2e}")

# --- Pitfall 2: ill-conditioned è¡Œåˆ— ---
eps = 1e-14
A_ill = np.array([[1.0, 1.0],
                  [1.0, 1.0 + eps]])    # æ¡ä»¶æ•° ~ 1/eps
cond = np.linalg.cond(A_ill)
print(f"æ¡ä»¶æ•°: {cond:.2e}")             # > 1e14 â†’ æ•°å€¤çš„ã«å±é™º

# --- Pitfall 3: å¤§è¡Œåˆ—ã®è¡Œåˆ—å¼ã¯log-detã§ ---
A_large = np.random.randn(500, 500)
sign, logdet = np.linalg.slogdet(A_large)
print(f"log|det(A)| = {logdet:.2f}, sign = {sign:.0f}")
# np.linalg.det(A_large) ã¯ã‚¢ãƒ³ãƒ€ãƒ¼/ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š

# --- Pitfall 4: ä¸¸ã‚èª¤å·®ã§å¤±ã‚ã‚Œã‚‹å¯¾ç§°æ€§ã®å›å¾© ---
S = A_large @ A_large.T                  # ç†è«–çš„ã«ã¯å¯¾ç§°
print(f"å¯¾ç§°æ€§èª¤å·®: {np.max(np.abs(S - S.T)):.2e}")
S_sym = (S + S.T) / 2                   # å¼·åˆ¶å¯¾ç§°åŒ–
print(f"å¼·åˆ¶å¯¾ç§°åŒ–å¾Œ: {np.max(np.abs(S_sym - S_sym.T)):.2e}")
```

### 4.8 è¡Œåˆ—åˆ†è§£ã®å®Ÿè£…æ¯”è¼ƒ â€” QRãƒ»Choleskyãƒ»å›ºæœ‰å€¤åˆ†è§£ã‚’ä½¿ã„åˆ†ã‘ã‚‹

ã€Œæ­£æ–¹è¡Œåˆ—ã‚’åˆ†è§£ã™ã‚‹æ‰‹æ³•ã€ã‚’3ã¤ä¸¦ã¹ã¦ã€**åŒã˜å•é¡Œã«å¯¾ã—ã¦ç•°ãªã‚‹æ‰‹æ³•ãŒãªãœç•°ãªã‚‹ç­”ãˆã‚’è¿”ã™ã‹**ã‚’æ•°å€¤ã§ç¢ºèªã™ã‚‹ã€‚

$$
A = QR \quad (Q^\top Q = I,\ R \text{ ä¸Šä¸‰è§’}) \quad \Leftarrow \text{æœ€å°äºŒä¹—æ³•ãƒ»ç›´äº¤åŸºåº•}
$$

$$
A = LL^\top \quad (L \text{ ä¸‹ä¸‰è§’}) \quad \Leftarrow \text{æ­£å®šå€¤è¡Œåˆ—ã®é€£ç«‹æ–¹ç¨‹å¼ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°}
$$

$$
A = Q\Lambda Q^\top \quad (Q^\top Q = I,\ \Lambda \text{ å¯¾è§’}) \quad \Leftarrow \text{ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ»PCA}
$$

- $A$ ã¯å…±é€š: $3 \times 3$ æ­£å®šå€¤å¯¾ç§°è¡Œåˆ—
- QR: å…¨è¡Œåˆ—ã«é©ç”¨å¯ï¼ˆãŸã ã—å¯¾ç§°æ€§ã‚’æ´»ç”¨ã—ãªã„ï¼‰
- Cholesky: æ­£å®šå€¤è¡Œåˆ—å°‚ç”¨ â€” LUã®åŠåˆ†ã®è¨ˆç®—é‡
- Eigendecomposition: å¯¾ç§°è¡Œåˆ—å°‚ç”¨ â€” ã‚¹ãƒšã‚¯ãƒˆãƒ«æƒ…å ±ã‚’å®Œå…¨ã«å–ã‚Šå‡ºã™

```python
import numpy as np

# å…±é€šã®æ­£å®šå€¤å¯¾ç§°è¡Œåˆ—ï¼ˆCholeskyãƒ†ã‚¹ãƒˆè¡Œåˆ—ï¼‰
A = np.array([[4.0, 2.0, -2.0],
              [2.0, 5.0,  4.0],
              [-2.0, 4.0, 14.0]])

print("=== 1. QRåˆ†è§£ ===")
Q_qr, R = np.linalg.qr(A)
print("Q (ç›´äº¤è¡Œåˆ—):\n", np.round(Q_qr, 4))
print("R (ä¸Šä¸‰è§’):\n", np.round(R, 4))
print(f"||Q^T Q - I||_F = {np.linalg.norm(Q_qr.T @ Q_qr - np.eye(3)):.2e}")
print(f"||QR - A||_F = {np.linalg.norm(Q_qr @ R - A):.2e}")

print("\n=== 2. Choleskyåˆ†è§£ ===")
L = np.linalg.cholesky(A)
print("L (ä¸‹ä¸‰è§’):\n", np.round(L, 4))
print(f"L[0,0] = sqrt(A[0,0]) = sqrt({A[0,0]}) = {np.sqrt(A[0,0]):.4f}")
print(f"||LL^T - A||_F = {np.linalg.norm(L @ L.T - A):.2e}")

print("\n=== 3. å›ºæœ‰å€¤åˆ†è§£ï¼ˆeighï¼‰===")
eigvals, Q_eig = np.linalg.eigh(A)
print("å›ºæœ‰å€¤ (æ˜‡é †):", np.round(eigvals, 4))
print("å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« (åˆ—):\n", np.round(Q_eig, 4))
print(f"ç›´äº¤æ€§ãƒã‚§ãƒƒã‚¯ ||Q^T Q - I||_F = {np.linalg.norm(Q_eig.T @ Q_eig - np.eye(3)):.2e}")
Lambda = np.diag(eigvals)
print(f"||Q Î› Q^T - A||_F = {np.linalg.norm(Q_eig @ Lambda @ Q_eig.T - A):.2e}")

print("\n=== 4. é€£ç«‹æ–¹ç¨‹å¼ Ax = b ã‚’3æ‰‹æ³•ã§è§£ã ===")
b = np.array([1.0, 2.0, 3.0])

# æ–¹æ³•1: QRåˆ†è§£ (Q^T b â†’ R backsolve)
x_qr = np.linalg.solve(R, Q_qr.T @ b)

# æ–¹æ³•2: Cholesky (L y = b â†’ L^T x = y)
y = np.linalg.solve(L, b)          # å‰é€²ä»£å…¥
x_chol = np.linalg.solve(L.T, y)   # å¾Œé€€ä»£å…¥

# æ–¹æ³•3: å›ºæœ‰å€¤åˆ†è§£ (x = Q Î›^{-1} Q^T b)
x_eig = Q_eig @ np.diag(1.0 / eigvals) @ Q_eig.T @ b

# å‚ç…§è§£
x_ref = np.linalg.solve(A, b)

print(f"QRè§£:     {np.round(x_qr, 6)}")
print(f"Cholesky: {np.round(x_chol, 6)}")
print(f"Eigè§£:    {np.round(x_eig, 6)}")
print(f"å‚ç…§è§£:   {np.round(x_ref, 6)}")
# 3æ‰‹æ³•å…¨ã¦ä¸€è‡´ã™ã‚‹ã¯ãš
assert np.allclose(x_qr, x_ref, atol=1e-10)
assert np.allclose(x_chol, x_ref, atol=1e-10)
assert np.allclose(x_eig, x_ref, atol=1e-10)
print("3æ‰‹æ³•å…¨ã¦ä¸€è‡´ âœ“")
```

> **ã©ã‚Œã‚’ä½¿ã†ã‹**:
> - æ­£å®šå€¤è¡Œåˆ—ã®é€£ç«‹æ–¹ç¨‹å¼ â†’ **Cholesky** (`np.linalg.solve` ãŒå†…éƒ¨ã§ä½¿ã†)
> - æœ€å°äºŒä¹—å•é¡Œ â†’ **QR** (`np.linalg.lstsq` ãŒå†…éƒ¨ã§ä½¿ã†)
> - ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ»PCA â†’ **å›ºæœ‰å€¤åˆ†è§£** (`np.linalg.eigh` ã‚’å¯¾ç§°è¡Œåˆ—ã«)

> Progress: 65%

---

## ğŸ”¬ Z6. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

<details><summary>Q1: $A \in \mathbb{R}^{m \times n}$</summary>
**èª­ã¿**: ã€Œ$A$ ã¯ $m$ è¡Œ $n$ åˆ—ã®å®Ÿæ•°è¡Œåˆ—ã€

**æ„å‘³**: $A$ ã¯ $m \times n$ å€‹ã®å®Ÿæ•°å€¤ã‚’æŒã¤è¡Œåˆ—ã€‚ç·šå½¢å†™åƒ $A: \mathbb{R}^n \to \mathbb{R}^m$ ã‚’è¡¨ç¾ã™ã‚‹ã€‚
</details>

<details><summary>Q2: $\mathbf{v} \in \ker(A) \iff A\mathbf{v} = \mathbf{0}$</summary>
**èª­ã¿**: ã€Œ$\mathbf{v}$ ãŒ $A$ ã®æ ¸ã«å±ã™ã‚‹ã“ã¨ã¨ã€$A\mathbf{v}$ ãŒã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚‹ã“ã¨ã¯åŒå€¤ã€

**æ„å‘³**: æ ¸ï¼ˆnull spaceï¼‰ã¯ã€$A$ ã§æ½°ã•ã‚Œã¦ã‚¼ãƒ­ã«ãªã‚‹ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ã®é›†åˆã€‚Rank-Nullityå®šç†ã§ $\dim(\ker(A)) = n - \text{rank}(A)$ã€‚
</details>

<details><summary>Q3: $\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)$</summary>
**èª­ã¿**: ã€Œ$ABC$ ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã¯ $BCA$ ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«ç­‰ã—ãã€$CAB$ ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«ã‚‚ç­‰ã—ã„ã€

**æ„å‘³**: ãƒˆãƒ¬ãƒ¼ã‚¹ã®å·¡å›æ€§ï¼ˆcyclic propertyï¼‰ã€‚è¡Œåˆ—ç©ã®é †åºã‚’å·¡å›çš„ã«å…¥ã‚Œæ›¿ãˆã¦ã‚‚ãƒˆãƒ¬ãƒ¼ã‚¹ã¯å¤‰ã‚ã‚‰ãªã„ã€‚è¡Œåˆ—å¾®åˆ†ã§é »å‡ºã€‚**æ³¨æ„**: $\text{tr}(ABC) \neq \text{tr}(ACB)$ â€” å·¡å›çš„ã§ãªã„ä¸¦ã¹æ›¿ãˆã§ã¯ãƒˆãƒ¬ãƒ¼ã‚¹ã¯å¤‰ã‚ã‚‹ã€‚
</details>

<details><summary>Q4: $A \succ 0$</summary>
**èª­ã¿**: ã€Œ$A$ ã¯æ­£å®šå€¤ã€

**æ„å‘³**: $\mathbf{x}^\top A \mathbf{x} > 0$ for all $\mathbf{x} \neq \mathbf{0}$ã€‚å…¨ã¦ã®å›ºæœ‰å€¤ãŒæ­£ã€‚Choleskyåˆ†è§£ãŒå¯èƒ½ã€‚å…±åˆ†æ•£è¡Œåˆ—ãŒæ­£å‰‡ãªã¨ãæˆç«‹ã€‚
</details>

<details><summary>Q5: $\hat{\mathbf{x}} = (A^\top A)^{-1} A^\top \mathbf{b}$</summary>
**èª­ã¿**: ã€Œ$\hat{\mathbf{x}}$ ã¯ $A^\top A$ ã®é€†è¡Œåˆ—ã¨ $A^\top \mathbf{b}$ ã®ç©ã€

**æ„å‘³**: æœ€å°äºŒä¹—è§£ã€‚$\|A\mathbf{x} - \mathbf{b}\|^2$ ã‚’æœ€å°ã«ã™ã‚‹ $\mathbf{x}$ã€‚æ­£è¦æ–¹ç¨‹å¼ $A^\top A\hat{\mathbf{x}} = A^\top\mathbf{b}$ ã®è§£ã€‚$A^\top A$ ãŒæ­£å‰‡ï¼ˆ$A$ ãŒãƒ•ãƒ«ãƒ©ãƒ³ã‚¯åˆ—ï¼‰ã®ã¨ãä¸€æ„ã€‚
</details>

<details><summary>Q6: $A = Q\Lambda Q^\top$, $Q^\top Q = I$</summary>
**èª­ã¿**: ã€Œ$A$ ã¯ç›´äº¤è¡Œåˆ— $Q$ ã¨å¯¾è§’è¡Œåˆ— $\Lambda$ ã§ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†è§£ã•ã‚Œã‚‹ã€

**æ„å‘³**: å¯¾ç§°è¡Œåˆ—ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†ã€‚$Q$ ã®åˆ—ãŒå›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã€$\Lambda$ ã®å¯¾è§’æˆåˆ†ãŒå›ºæœ‰å€¤ã€‚PCAã€å…±åˆ†æ•£è¡Œåˆ—ã®åˆ†æã§å¿…é ˆã€‚
</details>

<details><summary>Q7: $P = A(A^\top A)^{-1}A^\top$, $P^2 = P$</summary>
**èª­ã¿**: ã€Œ$P$ ã¯å°„å½±è¡Œåˆ—ã§ã€2å›é©ç”¨ã—ã¦ã‚‚çµæœãŒå¤‰ã‚ã‚‰ãªã„ï¼ˆå†ªç­‰ï¼‰ã€

**æ„å‘³**: $P$ ã¯ $A$ ã®åˆ—ç©ºé–“ã¸ã®ç›´äº¤å°„å½±ã€‚$P\mathbf{b}$ ã¯ $\mathbf{b}$ ã«æœ€ã‚‚è¿‘ã„ $\text{Col}(A)$ ä¸Šã®ç‚¹ã€‚
</details>

<details><summary>Q8: $\|\mathbf{u}\| \|\mathbf{v}\| \cos\theta = \langle \mathbf{u}, \mathbf{v} \rangle$</summary>
**èª­ã¿**: ã€Œ$\mathbf{u}$ ã¨ $\mathbf{v}$ ã®ãƒãƒ«ãƒ ã®ç©ã«ã‚³ã‚µã‚¤ãƒ³ã‚’ã‹ã‘ãŸã‚‚ã®ãŒå†…ç©ã€

**æ„å‘³**: å†…ç©ã®å¹¾ä½•å­¦çš„è§£é‡ˆã€‚$\cos\theta = 1$ï¼ˆå¹³è¡Œï¼‰â†’å†…ç©æœ€å¤§ã€$\cos\theta = 0$ï¼ˆç›´äº¤ï¼‰â†’å†…ç©ã‚¼ãƒ­ã€‚Attention[^1]ã®é¡ä¼¼åº¦è¨ˆç®—ã®æ•°å­¦çš„åŸºç›¤ã€‚
</details>

<details><summary>Q9: $(AB)^{-1} = B^{-1}A^{-1}$</summary>
**èª­ã¿**: ã€Œ$AB$ ã®é€†è¡Œåˆ—ã¯ $B$ ã®é€†è¡Œåˆ—ã¨ $A$ ã®é€†è¡Œåˆ—ã®ç©ï¼ˆé †åºåè»¢ï¼‰ã€

**æ„å‘³**: ã€Œé´ä¸‹ã‚’å±¥ã„ã¦ã‹ã‚‰é´ã‚’å±¥ãã€â†’ã€Œè„±ãã¨ãã¯é´ã‚’å…ˆã«è„±ãã€æ¬¡ã«é´ä¸‹ã€ã€‚é€†æ“ä½œã¯é †åºãŒé€†ã«ãªã‚‹ã€‚$(AB)^\top = B^\top A^\top$ ã¨åŒã˜åŸç†ã€‚
</details>

<details><summary>Q10: $R(\mathbf{x}) = \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}$, $\lambda_{\min} \leq R(\mathbf{x}) \leq \lambda_{\max}$</summary>
**èª­ã¿**: ã€ŒRayleighå•†ã¯æœ€å°å›ºæœ‰å€¤ã¨æœ€å¤§å›ºæœ‰å€¤ã®é–“ã«åã¾ã‚‹ã€

**æ„å‘³**: å¯¾ç§°è¡Œåˆ— $A$ ã®Rayleighå•†ã®æœ€å¤§åŒ–ãŒæœ€å¤§å›ºæœ‰å€¤ã¨ç¬¬1å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸ãˆã‚‹ã€‚PCA[^6][^7]ã®æ•°å­¦çš„åŸºç›¤ã€‚
</details>

### 5.2 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’NumPyã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã›ã‚ˆã€‚

<details><summary>Q1: $C = A^\top B$ ($A \in \mathbb{R}^{3 \times 2}, B \in \mathbb{R}^{3 \times 4}$)</summary>

$$
C = A^\top B, \quad C \in \mathbb{R}^{2 \times 4}
$$

- $A$ ã®è»¢ç½®: shape `(2, 3)` â†’ $B$: shape `(3, 4)` â†’ ç©: shape `(2, 4)`
- è¨˜å·å¯¾å¿œ: `A` â†’ `A`ã€`B` â†’ `B`ã€çµæœ `C` â†’ `C`

```python
import numpy as np
A = np.random.randn(3, 2)   # (3, 2)
B = np.random.randn(3, 4)   # (3, 4)
C = A.T @ B                 # (2, 3) @ (3, 4) â†’ (2, 4)
assert C.shape == (2, 4)
print("C.shape:", C.shape)  # (2, 4)
```

è½ã¨ã—ç©´: `A @ B` ã¯shapeä¸ä¸€è‡´ã§ã‚¨ãƒ©ãƒ¼ã€‚è»¢ç½®ã®å‘ãã‚’é–“é•ãˆã‚„ã™ã„ã€‚
</details>

<details><summary>Q2: Frobenius ãƒãƒ«ãƒ  $\|A\|_F = \sqrt{\text{tr}(A^\top A)}$</summary>

$$
\|A\|_F = \sqrt{\text{tr}(A^\top A)} = \sqrt{\sum_{i,j} A_{ij}^2}
$$

- shape: $A \in \mathbb{R}^{m \times n}$ â†’ ã‚¹ã‚«ãƒ©ãƒ¼
- è¨˜å·: `A` â†’ `A`ã€`tr` â†’ `np.trace`ã€å…¨è¦ç´ äºŒä¹—å’Œã®å¹³æ–¹æ ¹

```python
import numpy as np
A = np.random.randn(4, 3)

fro_trace = np.sqrt(np.trace(A.T @ A))   # å®šç¾©é€šã‚Š: sqrt(tr(A^T A))
fro_norm  = np.linalg.norm(A, 'fro')     # NumPyçµ„ã¿è¾¼ã¿
fro_elem  = np.sqrt(np.sum(A**2))        # è¦ç´ äºŒä¹—å’Œ

assert np.allclose(fro_trace, fro_norm)
assert np.allclose(fro_elem,  fro_norm)
print(f"||A||_F = {fro_norm:.6f}")       # 3ã¤å…¨ã¦ä¸€è‡´
```

è½ã¨ã—ç©´: `np.linalg.norm(A)` ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Frobeniusã§ã¯ãªãæœ€å¤§ç‰¹ç•°å€¤ï¼ˆSpectralãƒãƒ«ãƒ ï¼‰ã§ã¯ãªã„ã€‚å®Ÿã¯è¡Œåˆ—ã®å ´åˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Frobeniusã€‚ãƒ™ã‚¯ãƒˆãƒ«ã®å ´åˆã¯L2ã€‚æ··åŒã«æ³¨æ„ã€‚
</details>

<details><summary>Q3: äºŒæ¬¡å½¢å¼ $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top H \mathbf{x} - \mathbf{b}^\top\mathbf{x}$</summary>

$$
f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top H \mathbf{x} - \mathbf{b}^\top\mathbf{x}, \quad H \in \mathbb{R}^{n \times n},\ \mathbf{x},\mathbf{b} \in \mathbb{R}^n
$$

- $\frac{1}{2}\mathbf{x}^\top H \mathbf{x}$: ã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆäºŒæ¬¡é …ï¼‰
- $\mathbf{b}^\top\mathbf{x}$: ã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆç·šå½¢é …ï¼‰
- è¨˜å·: `x` â†’ `x`ã€`H` â†’ `H`ã€`b` â†’ `b`

```python
import numpy as np
n = 4
H = np.array([[4., 1., 0., 0.],
              [1., 3., 0., 0.],
              [0., 0., 2., 1.],
              [0., 0., 1., 2.]])  # æ­£å®šå€¤å¯¾ç§°è¡Œåˆ—
b = np.array([1., 2., 3., 4.])
x = np.array([1., 0., -1., 0.5])

f = 0.5 * x @ H @ x - b @ x     # æ•°å¼ã¨1:1å¯¾å¿œ
# âˆ‡f = Hx - b (æœ€é©è§£ã¯ Hx = b)
grad_f = H @ x - b
print(f"f(x) = {f:.4f}")
print(f"âˆ‡f(x) = {grad_f}")

# æ¤œç®—: f ã®æœ€å°å€¤ã¯ x* = H^{-1}b
x_star = np.linalg.solve(H, b)
f_min = 0.5 * x_star @ H @ x_star - b @ x_star
print(f"f(x*) = {f_min:.4f}")   # f(x*) = -b^T H^{-1} b / 2
```

ã“ã®å½¢ã¯æœ€å°äºŒä¹—æ³•ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³æ³•ãƒ»ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã§é »å‡ºã™ã‚‹ã€‚
</details>

<details><summary>Q4: PCAæ¬¡å…ƒå‰Šæ¸› $Z = \tilde{X} Q_k$</summary>

$$
Z = \tilde{X} Q_k, \quad \tilde{X} \in \mathbb{R}^{N \times d},\ Q_k \in \mathbb{R}^{d \times k},\ Z \in \mathbb{R}^{N \times k}
$$

- $\tilde{X}$: ä¸­å¿ƒåŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆå„åˆ—ã®å¹³å‡ã‚¼ãƒ­ï¼‰
- $Q_k$: å…±åˆ†æ•£è¡Œåˆ—ã®ä¸Šä½ $k$ å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’åˆ—ã«æŒã¤è¡Œåˆ—
- shape: `(N, d) @ (d, k)` â†’ `(N, k)`

```python
import numpy as np

np.random.seed(0)
N, d, k = 200, 10, 2
X = np.random.randn(N, d)
X[:, 0] *= 3.0   # ç¬¬0æˆåˆ†ã®åˆ†æ•£ã‚’å¤§ããã™ã‚‹

# ä¸­å¿ƒåŒ–
X_mean = X.mean(axis=0)        # shape (d,)
X_tilde = X - X_mean           # shape (N, d)

# å…±åˆ†æ•£è¡Œåˆ— + å›ºæœ‰å€¤åˆ†è§£
Sigma = X_tilde.T @ X_tilde / (N - 1)   # (d, d)
eigvals, Q = np.linalg.eigh(Sigma)       # æ˜‡é †

# ä¸Šä½kå€‹ï¼ˆé™é †ã«ã‚½ãƒ¼ãƒˆï¼‰
idx = np.argsort(eigvals)[::-1]
Q_k = Q[:, idx[:k]]                     # (d, k)

# å°„å½±
Z = X_tilde @ Q_k                       # (N, k)
assert Z.shape == (N, k)
print(f"Z.shape: {Z.shape}")
print(f"explained variance ratio: {eigvals[idx[:k]] / eigvals.sum()}")
```

è½ã¨ã—ç©´: `np.linalg.eigh` ã¯å›ºæœ‰å€¤ã‚’**æ˜‡é †**ã«è¿”ã™ã€‚PCAã§ã¯åˆ†æ•£å¤§ = å›ºæœ‰å€¤å¤§ ã®æˆåˆ†ã‹ã‚‰ä½¿ã†ã®ã§ã€`[::-1]` ã§é™é †ã«ã™ã‚‹ã€‚
</details>

<details><summary>Q5: Cholesky ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° $\mathbf{x} = \boldsymbol{\mu} + L\mathbf{z}$, $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, I)$</summary>

$$
\mathbf{x} = \boldsymbol{\mu} + L\mathbf{z}, \quad \mathbf{z} \sim \mathcal{N}(\mathbf{0}, I_d), \quad \Sigma = LL^\top
$$

- $\mathbb{E}[\mathbf{x}] = \boldsymbol{\mu}$ã€$\text{Cov}[\mathbf{x}] = L \cdot I \cdot L^\top = LL^\top = \Sigma$
- è¨˜å·: `mu` â†’ $\boldsymbol{\mu}$ã€`L` â†’ $L$ï¼ˆä¸‹ä¸‰è§’ï¼‰ã€`z` â†’ $\mathbf{z}$

```python
import numpy as np

d = 3
mu = np.array([1.0, 2.0, -1.0])
Sigma = np.array([[2.0, 0.8, 0.3],
                  [0.8, 1.5, 0.1],
                  [0.3, 0.1, 1.0]])   # æ­£å®šå€¤å¯¾ç§°è¡Œåˆ—

L = np.linalg.cholesky(Sigma)  # Î£ = LL^T, L ã¯ä¸‹ä¸‰è§’

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
n_samples = 10_000
Z = np.random.randn(d, n_samples)   # (d, N): å„åˆ—ãŒ z ~ N(0, I)
X = mu[:, None] + L @ Z             # (d, N)

# æ¤œç®—: æ¨™æœ¬å…±åˆ†æ•£ãŒ Î£ ã«è¿‘ã„ã‹
X_centered = X - X.mean(axis=1, keepdims=True)
Sigma_sample = X_centered @ X_centered.T / (n_samples - 1)
print("ç†è«–å€¤ Î£:\n", Sigma)
print("æ¨™æœ¬ Î£:\n", np.round(Sigma_sample, 2))
# è¡Œåˆ—ãƒãƒ«ãƒ ã®å·®
print(f"||Î£ - Î£_sample||_F = {np.linalg.norm(Sigma - Sigma_sample, 'fro'):.4f}")
```

ã“ã‚Œã¯ VAE ã®å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ï¼ˆç¬¬10å›ï¼‰ã¨åŒã˜æ§‹é€ ã€‚$L\mathbf{z}$ ãŒç¢ºç‡å¤‰æ•°ã®ã€Œãƒ«ãƒ¼ãƒ„ã€ã‚’åˆ†é›¢ã™ã‚‹ã€‚
</details>

### 5.3 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: PCA ã§ MNIST ã‚’å¯è¦–åŒ–ã™ã‚‹

sklearn ã® `load_digits`ï¼ˆ8Ã—8ãƒ”ã‚¯ã‚»ãƒ«ã®æ‰‹æ›¸ãæ•°å­—ã€64æ¬¡å…ƒï¼‰ã‚’PCAã§2æ¬¡å…ƒã«åœ§ç¸®ã—ã€ã‚¯ãƒ©ã‚¹ãŒç·šå½¢åˆ†é›¢ã§ãã‚‹ã‹ã‚’ç¢ºã‹ã‚ã‚‹ã€‚å›ºæœ‰å€¤åˆ†è§£ã‚’è‡ªåˆ†ã§å®Ÿè£…ã—ã¦ `sklearn.decomposition.PCA` ã¨çµæœãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã‚ˆã†ã€‚

**æ•°å­¦çš„èƒŒæ™¯:**

$$
\tilde{X} = X - \bar{\mathbf{x}}^\top \mathbf{1}_N^\top, \quad \Sigma = \frac{1}{N-1}\tilde{X}^\top\tilde{X} \in \mathbb{R}^{d \times d}
$$

$$
\Sigma = Q\Lambda Q^\top, \quad Q^\top Q = I, \quad \Lambda = \text{diag}(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_d)
$$

$$
Z = \tilde{X} Q_k \in \mathbb{R}^{N \times k}, \quad \text{explained ratio} = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{d} \lambda_i}
$$

- shape: $\tilde{X}$:`(N, d)=(1797, 64)` â†’ $\Sigma$:`(64, 64)` â†’ $Q_k$:`(64, 2)` â†’ $Z$:`(1797, 2)`
- è¨˜å·å¯¾å¿œ: `X_c` â†’ $\tilde{X}$ã€`eigvecs` â†’ $Q$ã€`Q_k` â†’ $Q_k$ã€`Z` â†’ $Z$
- æ•°å€¤å®‰å®šæ€§: `eigh` ã¯å¯¾ç§°è¡Œåˆ—å°‚ç”¨ã§ `eig` ã‚ˆã‚Šé«˜ç²¾åº¦ï¼ˆå›ºæœ‰å€¤ãŒå®Ÿæ•°ä¿è¨¼ï¼‰

```python
import numpy as np
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA as SklearnPCA

# â”€â”€ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
digits = load_digits()
X = digits.data.astype(float)   # shape (1797, 64)
y = digits.target               # 0-9 ã®ãƒ©ãƒ™ãƒ«
N, d = X.shape
print(f"X.shape = {X.shape}")   # (1797, 64)

# â”€â”€ Step 1: ä¸­å¿ƒåŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mu = X.mean(axis=0)             # shape (64,)
X_c = X - mu                   # shape (1797, 64)

# ä¸­å¿ƒåŒ–ã®ç¢ºèª: åˆ—æ–¹å‘ã®å¹³å‡ãŒã»ã¼ã‚¼ãƒ­ã«ãªã‚‹ã‹
assert np.allclose(X_c.mean(axis=0), 0, atol=1e-10)

# â”€â”€ Step 2: å…±åˆ†æ•£è¡Œåˆ— + å›ºæœ‰å€¤åˆ†è§£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sigma = X_c.T @ X_c / (N - 1)  # shape (64, 64), å¯¾ç§°è¡Œåˆ—
eigvals, Q = np.linalg.eigh(Sigma)  # æ˜‡é †ã«è¿”ã‚‹

# é™é †ã«ã‚½ãƒ¼ãƒˆï¼ˆå¤§ããªå›ºæœ‰å€¤ = é‡è¦ãªä¸»æˆåˆ† ã‹ã‚‰ä¸¦ã¹ã‚‹ï¼‰
idx = np.argsort(eigvals)[::-1]
eigvals = eigvals[idx]
Q = Q[:, idx]                  # shape (64, 64)

# â”€â”€ Step 3: å¯„ä¸ç‡ã®ç¢ºèª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
total_var = eigvals.sum()
explained_ratio = eigvals / total_var
cumulative_ratio = np.cumsum(explained_ratio)

print("=== ä¸»æˆåˆ†ã®å¯„ä¸ç‡ ===")
for i, (lam, r, cum) in enumerate(zip(eigvals[:5], explained_ratio[:5], cumulative_ratio[:5]), 1):
    print(f"PC{i}: Î»={lam:.2f}, ratio={r:.3f}, cumulative={cum:.3f}")
# ä¸Šä½2ä¸»æˆåˆ†ã§ã©ã‚Œãã‚‰ã„èª¬æ˜ã§ãã‚‹ã‹
print(f"\nä¸Šä½2ä¸»æˆåˆ†ã®ç´¯ç©å¯„ä¸ç‡: {cumulative_ratio[1]:.3f}")
print(f"ä¸Šä½10ä¸»æˆåˆ†ã®ç´¯ç©å¯„ä¸ç‡: {cumulative_ratio[9]:.3f}")

# â”€â”€ Step 4: 2æ¬¡å…ƒã«å°„å½± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
k = 2
Q_k = Q[:, :k]                 # shape (64, 2) â€” ä¸Šä½2å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«
Z = X_c @ Q_k                  # shape (1797, 2)
assert Z.shape == (N, k)

# â”€â”€ Step 5: sklearn ã¨ã®ç…§åˆï¼ˆç¬¦å·ã¯åè»¢ã—ã¦ã‚ˆã„ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pca_sk = SklearnPCA(n_components=k)
Z_sk = pca_sk.fit_transform(X_c)

# å„ä¸»æˆåˆ†ã®ç¬¦å·ã¯è‡ªç”±ï¼ˆ-1å€ã—ã¦ã‚‚åŒã˜å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ãªã®ã§absã§æ¯”è¼ƒ
for i in range(k):
    err = min(
        np.linalg.norm(Z[:, i] - Z_sk[:, i]),
        np.linalg.norm(Z[:, i] + Z_sk[:, i])   # ç¬¦å·åè»¢ã‚’è¨±å®¹
    )
    print(f"PC{i+1} å·®ã®ãƒãƒ«ãƒ : {err:.6f}")

# â”€â”€ Step 6: å†æ§‹æˆèª¤å·® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
X_reconstructed = Z @ Q_k.T + mu   # shape (1797, 64)
recon_error = np.linalg.norm(X - X_reconstructed, 'fro') / np.linalg.norm(X, 'fro')
print(f"\n2ä¸»æˆåˆ†ã§ã®å†æ§‹æˆèª¤å·® (ç›¸å¯¾Frobeniusãƒãƒ«ãƒ ): {recon_error:.4f}")

# 10ä¸»æˆåˆ†ã§ã®å†æ§‹æˆèª¤å·®
k10 = 10
Z10 = X_c @ Q[:, :k10]
X_rec10 = Z10 @ Q[:, :k10].T + mu
err10 = np.linalg.norm(X - X_rec10, 'fro') / np.linalg.norm(X, 'fro')
print(f"10ä¸»æˆåˆ†ã§ã®å†æ§‹æˆèª¤å·®:                       {err10:.4f}")
```

<details><summary>matplotlib ã§æ•£å¸ƒå›³ã‚’æãï¼ˆç’°å¢ƒã« matplotlib ãŒã‚ã‚‹å ´åˆï¼‰</summary>

```python
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8, 6))
colors = plt.cm.tab10(np.linspace(0, 1, 10))

for digit in range(10):
    mask = (y == digit)
    ax.scatter(Z[mask, 0], Z[mask, 1],
               c=[colors[digit]], label=str(digit),
               alpha=0.5, s=15)

ax.set_xlabel(f"PC1 ({explained_ratio[0]:.1%})")
ax.set_ylabel(f"PC2 ({explained_ratio[1]:.1%})")
ax.set_title("MNIST digits â€” PCA 2D projection")
ax.legend(title="Digit", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.savefig("pca_digits.png", dpi=120)
plt.show()
```

2Då›³ã§ã¯ã€Œ0ã€ã¨ã€Œ1ã€ãŒæ¯”è¼ƒçš„ãã‚Œã„ã«åˆ†é›¢ã•ã‚Œã‚‹ãŒã€ä»–ã‚¯ãƒ©ã‚¹ã¯æ··åœ¨ã™ã‚‹ã€‚ã“ã‚Œã¯64æ¬¡å…ƒç©ºé–“ã§ã®æƒ…å ±ã®å¤šããŒ2ä¸»æˆåˆ†ã§ã¯è¡¨ç¾ã—ãã‚Œãªã„ã‹ã‚‰ã€‚ç¬¬15å›ï¼ˆVAEï¼‰ã®æ½œåœ¨ç©ºé–“ $\mathbf{z} \in \mathbb{R}^{10}$ ã¨æ¯”ã¹ãŸã¨ãã€PCAã¨VAEã®ã€Œæ¬¡å…ƒåœ§ç¸®ã€ã®æœ¬è³ªçš„ãªé•ã„ãŒè¦‹ãˆã¦ãã‚‹ã€‚
</details>

> **Note:** PCA ã¯**ç·šå½¢**æ¬¡å…ƒå‰Šæ¸›ã€‚ã‚¯ãƒ©ã‚¹é–“ã®å¢ƒç•ŒãŒéç·šå½¢ãªå ´åˆã¯ã€Kernel PCAãƒ»Autoencoderï¼ˆç¬¬12å›ï¼‰ãƒ»UMAP ãŒæœ‰åŠ¹ã€‚PCA ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« $Q_k$ ã¯ã€Œãƒ‡ãƒ¼ã‚¿ãŒæœ€ã‚‚å¤‰åŒ–ã™ã‚‹æ–¹å‘ã€ã‚’è¦‹ã¤ã‘ã‚‹ãŒã€ã‚¯ãƒ©ã‚¹ã‚’åˆ†é›¢ã™ã‚‹æ–¹å‘ï¼ˆLDAï¼‰ã§ã¯ãªã„ã€‚



ä»¥ä¸‹ã®æ•°å¼ã‚’LaTeXã§æ›¸ã„ã¦ã¿ã‚ˆã†ã€‚ç­”ãˆã¯æŠ˜ã‚Šç•³ã¿ã®ä¸­ã€‚

<details><summary>Q1: å›ºæœ‰å€¤æ–¹ç¨‹å¼</summary>

$$
A\mathbf{v} = \lambda\mathbf{v}
$$
</details>

<details><summary>Q2: ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†è§£</summary>

$$
A = Q\Lambda Q^\top = \sum_{i=1}^{n} \lambda_i \mathbf{q}_i \mathbf{q}_i^\top
$$
</details>

<details><summary>Q3: Cauchy-Schwarz ä¸ç­‰å¼</summary>

$$
|\langle \mathbf{u}, \mathbf{v} \rangle| \leq \|\mathbf{u}\| \cdot \|\mathbf{v}\|
$$
</details>

<details><summary>Q4: æ­£è¦æ–¹ç¨‹å¼</summary>

$$
\hat{\mathbf{x}} = (A^\top A)^{-1} A^\top \mathbf{b}
$$
</details>

<details><summary>Q5: å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ</summary>

$$
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x}-\boldsymbol{\mu})\right)
$$
</details>

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: å‹¾é…é™ä¸‹æ³•ã§ç·šå½¢å›å¸°

æœ€å°äºŒä¹—æ³•ã¯é–‰å½¢å¼è§£ã‚’æŒã¤ãŒã€å‹¾é…é™ä¸‹æ³•ã§ã‚‚è§£ã‘ã‚‹ã€‚ã“ã“ã§ã¯å‹¾é…é™ä¸‹æ³•ã§ç·šå½¢å›å¸°ã‚’è§£ãã€é–‰å½¢å¼è§£ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèªã™ã‚‹ã€‚

$$
L(\mathbf{w})=\frac{1}{n}\|X\mathbf{w}-\mathbf{y}\|_2^2=\frac{1}{n}(X\mathbf{w}-\mathbf{y})^\top(X\mathbf{w}-\mathbf{y})

\nabla_{\mathbf{w}}L(\mathbf{w})=\frac{2}{n}X^\top(X\mathbf{w}-\mathbf{y})

\mathbf{w}\leftarrow \mathbf{w}-\\alpha\nabla_{\mathbf{w}}L(\mathbf{w})
$$

```python
import numpy as np

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
n, d = 100, 3
X = np.random.randn(n, d)
w_true = np.array([2.0, -1.5, 0.5])
y = X @ w_true + np.random.randn(n) * 0.3

# é–‰å½¢å¼è§£
w_closed = np.linalg.solve(X.T @ X, X.T @ y)

# å‹¾é…é™ä¸‹æ³•
w_gd = np.zeros(d)
lr = 0.01
n_iters = 500

losses = []
for t in range(n_iters):
    # å‹¾é…: âˆ‡L = (2/n) X^T (Xw - y)
    residual = X @ w_gd - y
    grad = (2 / n) * X.T @ residual
    w_gd -= lr * grad
    loss = np.mean(residual**2)
    losses.append(loss)

print("=== å‹¾é…é™ä¸‹æ³• vs é–‰å½¢å¼è§£ ===")
print(f"çœŸã®é‡ã¿:   {w_true}")
print(f"é–‰å½¢å¼è§£:   {np.round(w_closed, 4)}")
print(f"GD ({n_iters}å›): {np.round(w_gd, 4)}")
print(f"å·®ã®ãƒãƒ«ãƒ : {np.linalg.norm(w_gd - w_closed):.6f}")
print(f"æœ€çµ‚æå¤±:   {losses[-1]:.6f}")
```

<details><summary>ãƒãƒ£ãƒ¬ãƒ³ã‚¸: ãƒŸãƒ‹ãƒãƒƒãƒSGDã«æ”¹é€ ã™ã‚‹</summary>
ä¸Šã®ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ã¦ã€å…¨ãƒ‡ãƒ¼ã‚¿ã§ã¯ãªãæ¯å›ãƒ©ãƒ³ãƒ€ãƒ ã«32å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸ã‚“ã§å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ãƒŸãƒ‹ãƒãƒƒãƒSGDã«æ”¹é€ ã—ã¦ã¿ã‚ˆã†ã€‚

</details>

### 5.6 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: Power Iteration ã§æœ€å¤§å›ºæœ‰å€¤ã‚’æ±‚ã‚ã‚‹

å›ºæœ‰å€¤åˆ†è§£ã‚’ `np.linalg.eigh` ãªã—ã§å®Ÿè£…ã™ã‚‹ã€‚Power Iterationï¼ˆã¹ãä¹—æ³•ï¼‰ã¯ã€è¡Œåˆ—ã‚’ç¹°ã‚Šè¿”ã—ã‹ã‘ã‚‹ã“ã¨ã§æœ€å¤§å›ºæœ‰å€¤ã¨å¯¾å¿œã™ã‚‹å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ±‚ã‚ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã ã€‚

$$
\\mathbf{v}_{t+1}=\\frac{A\\mathbf{v}_t}{\\|A\\mathbf{v}_t\\|_2}

\\lambda_t=\\mathbf{v}_t^\\top A\\mathbf{v}_t
$$

```python
import numpy as np

def power_iteration(A: np.ndarray, n_iters: int = 100) -> tuple[float, np.ndarray]:
    """Power Iteration ã§æœ€å¤§å›ºæœ‰å€¤ã¨å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ±‚ã‚ã‚‹ã€‚

    Algorithm:
    1. ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ™ã‚¯ãƒˆãƒ« v ã‚’åˆæœŸåŒ–
    2. v â† Av / ||Av|| ã‚’ç¹°ã‚Šè¿”ã™
    3. Î» = v^T A v (Rayleighå•†) ãŒæœ€å¤§å›ºæœ‰å€¤ã«åæŸ
    """
    n = A.shape[0]
    v = np.random.randn(n)
    v = v / np.linalg.norm(v)

    for i in range(n_iters):
        Av = A @ v
        v_new = Av / np.linalg.norm(Av)
        # åæŸåˆ¤å®š
        if np.allclose(abs(np.dot(v_new, v)), 1.0, atol=1e-10):
            v = v_new
            break
        v = v_new

    eigenvalue = v @ A @ v  # Rayleighå•†
    return eigenvalue, v

# ãƒ†ã‚¹ãƒˆ
np.random.seed(42)
A = np.array([[4.0, 1.0, 0.5],
              [1.0, 3.0, 0.2],
              [0.5, 0.2, 2.0]])

lam_pi, v_pi = power_iteration(A)
lam_np, V_np = np.linalg.eigh(A)

print("=== Power Iteration vs np.linalg.eigh ===")
print(f"Power Iteration: Î»_max = {lam_pi:.6f}")
print(f"np.linalg.eigh:  Î»_max = {lam_np[-1]:.6f}")
print(f"å·®: {abs(lam_pi - lam_np[-1]):.10f}")
print(f"\nå›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« (PI):  {np.round(v_pi, 4)}")
print(f"å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« (eigh): {np.round(V_np[:, -1], 4)}")
```

<details><summary>Deflation ã§å…¨å›ºæœ‰å€¤ã‚’æ±‚ã‚ã‚‹</summary>
Power Iteration ã¯æœ€å¤§å›ºæœ‰å€¤ã®ã¿ã‚’è¿”ã™ã€‚å…¨å›ºæœ‰å€¤ã‚’æ±‚ã‚ã‚‹ã«ã¯ **Deflation**ï¼ˆæ¸›è¡°æ³•ï¼‰ã‚’ä½¿ã†:

1. æœ€å¤§å›ºæœ‰å€¤ $\lambda_1$ ã¨å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{v}_1$ ã‚’æ±‚ã‚ã‚‹
2. $A \leftarrow A - \lambda_1 \mathbf{v}_1 \mathbf{v}_1^\top$ï¼ˆãƒ©ãƒ³ã‚¯1ã®å¼•ãç®—ï¼‰
3. æ–°ã—ã„ $A$ ã«å¯¾ã—ã¦Power Iterationã‚’ç¹°ã‚Šè¿”ã™

</details>

### 5.7 è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

æœ¬è¬›ç¾©ã‚’ä¿®äº†ã—ãŸæ™‚ç‚¹ã§ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‹ç¢ºèªã—ã¦ã»ã—ã„:

- [ ] ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã®å…¬ç†ã‚’3ã¤ä»¥ä¸Šè¨€ãˆã‚‹
- [ ] ç·šå½¢ç‹¬ç«‹ã®å®šç¾©ã‚’ã‚³ãƒ¼ãƒ‰ã§ç¢ºèªã§ãã‚‹
- [ ] å†…ç©â†’ãƒãƒ«ãƒ â†’è·é›¢ã®å®šç¾©ã®é€£é–ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Cauchy-Schwarzä¸ç­‰å¼ã‚’è¿°ã¹ã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¨ã®é–¢ä¿‚ã‚’èª¬æ˜ã§ãã‚‹
- [ ] è¡Œåˆ—ç©ã®3ã¤ã®è¦‹æ–¹ï¼ˆè¦ç´ ãƒ»åˆ—ãƒ»è¡Œï¼‰ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
- [ ] è»¢ç½®ã®æ€§è³ª $(AB)^\top = B^\top A^\top$ ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] `np.linalg.solve` ã¨ `np.linalg.inv` ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] å›ºæœ‰å€¤åˆ†è§£ã‚’æ‰‹è¨ˆç®—ã§2Ã—2è¡Œåˆ—ã«é©ç”¨ã§ãã‚‹
- [ ] ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†ã®3ã¤ã®ä¸»å¼µã‚’è¿°ã¹ã‚‰ã‚Œã‚‹
- [ ] æ­£å®šå€¤è¡Œåˆ—ã®3ã¤ã®åˆ¤å®šæ¡ä»¶ã‚’åˆ—æŒ™ã§ãã‚‹
- [ ] Choleskyåˆ†è§£ã‚’ä½¿ã£ã¦ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãã‚‹
- [ ] æœ€å°äºŒä¹—æ³•ã®æ­£è¦æ–¹ç¨‹å¼ã‚’å°å‡ºã§ãã‚‹
- [ ] PCAã‚’å›ºæœ‰å€¤åˆ†è§£ã¨ã—ã¦å®Ÿè£…ã§ãã‚‹
- [ ] `np.einsum` ã§å†…ç©ãƒ»è¡Œåˆ—ç©ãƒ»ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’æ›¸ã‘ã‚‹
- [ ] Attention[^1]ã® $QK^\top$ ã‚’ç·šå½¢ä»£æ•°ã®è¨€è‘‰ã§èª¬æ˜ã§ãã‚‹
- [ ] Choleskyåˆ†è§£ã®æ§‹æˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆå¯¾è§’ãƒ»ä¸‹ä¸‰è§’ã®å…¬å¼ï¼‰ã‚’æ‰‹æ›¸ãã§å®Ÿè£…ã§ãã‚‹
- [ ] `np.linalg.solve`ãƒ»`np.linalg.qr`ãƒ»`np.linalg.eigh` ã‚’æ­£ã—ãä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
- [ ] æ¡ä»¶æ•° $\kappa(A) = \sigma_{\max}/\sigma_{\min}$ ãŒå¤§ãã„è¡Œåˆ—ã®é€£ç«‹æ–¹ç¨‹å¼ã§ä½•ãŒèµ·ãã‚‹ã‹ã‚’èª¬æ˜ã§ãã‚‹

### 5.8 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: Choleskyåˆ†è§£ã‚’æ‰‹æ›¸ãã§å®Ÿè£…ã™ã‚‹

Part1 Section 3.8 ã§å­¦ã‚“ã  Cholesky åˆ†è§£ã®æ§‹æˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ Python ã§å®Ÿè£…ã—ã€`np.linalg.cholesky` ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

$$
L_{jj} = \sqrt{A_{jj} - \sum_{k=1}^{j-1} L_{jk}^2}, \qquad L_{ij} = \frac{1}{L_{jj}}\left(A_{ij} - \sum_{k=1}^{j-1} L_{ik} L_{jk}\right),\ i > j
$$

- shape: å…¥åŠ› $A \in \mathbb{R}^{n \times n}$ï¼ˆæ­£å®šå€¤å¯¾ç§°ï¼‰â†’ å‡ºåŠ› $L \in \mathbb{R}^{n \times n}$ï¼ˆä¸‹ä¸‰è§’ï¼‰
- è¨˜å·å¯¾å¿œ: `A[i, j]` â†’ $A_{ij}$ã€`L[j, j]` â†’ $L_{jj}$
- æ•°å€¤å®‰å®šæ€§: å¹³æ–¹æ ¹ã®ä¸­ãŒè² ã«ãªã‚Œã° `A` ãŒæ­£å®šå€¤ã§ãªã„è¨¼æ‹  â†’ `ValueError` ã‚’å‡ºã™

```python
import numpy as np

def cholesky_manual(A: np.ndarray) -> np.ndarray:
    """Choleskyåˆ†è§£ã‚’é€æ¬¡å…¬å¼ã§å®Ÿè£…ã™ã‚‹ã€‚

    Returns L such that A = L @ L.T (lower triangular L).
    Raises ValueError if A is not positive definite.
    """
    n = A.shape[0]
    assert A.shape == (n, n), "square matrix required"
    L = np.zeros((n, n))

    for j in range(n):
        # å¯¾è§’æˆåˆ†: L_{jj} = sqrt(A_{jj} - sum_{k<j} L_{jk}^2)
        diag_sq = A[j, j] - np.sum(L[j, :j] ** 2)
        if diag_sq <= 0:
            raise ValueError(
                f"è¡Œåˆ—ã¯æ­£å®šå€¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“ (j={j}, diag_sq={diag_sq:.6e})"
            )
        L[j, j] = np.sqrt(diag_sq)

        # ä¸‹ä¸‰è§’æˆåˆ†: L_{ij} = (A_{ij} - sum_{k<j} L_{ik} L_{jk}) / L_{jj}
        for i in range(j + 1, n):
            L[i, j] = (A[i, j] - np.sum(L[i, :j] * L[j, :j])) / L[j, j]

    return L

# â”€â”€ ãƒ†ã‚¹ãƒˆ1: Part1 Section 3.8 ã®å…·ä½“ä¾‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A_test = np.array([[4.0, 2.0, -2.0],
                   [2.0, 5.0,  4.0],
                   [-2.0, 4.0, 14.0]])

L_manual = cholesky_manual(A_test)
L_numpy  = np.linalg.cholesky(A_test)

print("=== L (æ‰‹æ›¸ãå®Ÿè£…) ===")
print(np.round(L_manual, 4))
print("=== L (np.linalg.cholesky) ===")
print(np.round(L_numpy, 4))
print(f"\nå·® ||L_manual - L_numpy||_F = {np.linalg.norm(L_manual - L_numpy):.2e}")
print(f"å†æ§‹æˆ ||LL^T - A||_F = {np.linalg.norm(L_manual @ L_manual.T - A_test):.2e}")

# â”€â”€ ãƒ†ã‚¹ãƒˆ2: ãƒ©ãƒ³ãƒ€ãƒ æ­£å®šå€¤è¡Œåˆ—ã§æ¤œè¨¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
np.random.seed(42)
B = np.random.randn(5, 5)
A_rand = B.T @ B + np.eye(5)   # æ­£å®šå€¤ä¿è¨¼
L_rand = cholesky_manual(A_rand)
assert np.allclose(L_rand @ L_rand.T, A_rand, atol=1e-10)
print("\nãƒ©ãƒ³ãƒ€ãƒ 5x5æ­£å®šå€¤è¡Œåˆ—ã§ã‚‚ä¸€è‡´ âœ“")

# â”€â”€ ãƒ†ã‚¹ãƒˆ3: éæ­£å®šå€¤è¡Œåˆ—ã§ã‚¨ãƒ©ãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A_bad = np.array([[1.0, 2.0], [2.0, 1.0]])   # å›ºæœ‰å€¤: 3, -1 â†’ éæ­£å®šå€¤
try:
    cholesky_manual(A_bad)
    print("ã‚¨ãƒ©ãƒ¼ãŒå‡ºãªã‹ã£ãŸï¼ˆæƒ³å®šå¤–ï¼‰")
except ValueError as e:
    print(f"\néæ­£å®šå€¤è¡Œåˆ—ã« ValueError: {e}")
```

> Progress: 85%

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. è¡Œåˆ— $A \in \mathbb{R}^{m \times n}$ã€$B \in \mathbb{R}^{n \times p}$ ã®ç© $AB$ ã® shape ã‚’ç­”ãˆã‚ˆã€‚ç©ãŒå®šç¾©ã§ãã‚‹æ¡ä»¶ã¯ä½•ã‹ã€‚
> 2. å›ºæœ‰å€¤åˆ†è§£ $A = Q\Lambda Q^{-1}$ ã«ãŠã„ã¦ $\Lambda$ ã¯ä½•ã‚’è¡¨ã—ã€$Q$ ã®åˆ—ã¯ã©ã†è§£é‡ˆã•ã‚Œã‚‹ã‹ã€‚

---

## ğŸ“ Z7. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 NumPy / SciPy ã®ç·šå½¢ä»£æ•°é–¢æ•°ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

å®Ÿè£…æ™‚ã«é »ç¹ã«å‚ç…§ã™ã‚‹é–¢æ•°ã‚’ã¾ã¨ã‚ã¦ãŠãã€‚

| ç›®çš„ | NumPy | SciPy | æ³¨æ„ç‚¹ |
|:-----|:------|:------|:------|
| è¡Œåˆ—ç© | `A @ B` | â€” | BLAS Level 3 ã® dgemm ã‚’å‘¼ã¶ |
| å†…ç© | `np.dot(a, b)` | â€” | 1Dãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®ã¿ã€‚2Dä»¥ä¸Šã¯ `@` ã‚’ä½¿ã† |
| è»¢ç½® | `A.T` | â€” | ãƒ“ãƒ¥ãƒ¼ã‚’è¿”ã™ï¼ˆã‚³ãƒ”ãƒ¼ãªã—ï¼‰ |
| é€†è¡Œåˆ— | `np.linalg.inv(A)` | `scipy.linalg.inv(A)` | å¯èƒ½ãªé™ã‚Š `solve` ã‚’ä½¿ã† |
| é€£ç«‹æ–¹ç¨‹å¼ | `np.linalg.solve(A, b)` | `scipy.linalg.solve(A, b)` | $A\mathbf{x}=\mathbf{b}$ ã‚’è§£ã |
| å›ºæœ‰å€¤åˆ†è§£ï¼ˆå¯¾ç§°ï¼‰ | `np.linalg.eigh(A)` | `scipy.linalg.eigh(A)` | **å¯¾ç§°è¡Œåˆ—ã«ã¯å¿…ãš eigh** |
| å›ºæœ‰å€¤åˆ†è§£ï¼ˆä¸€èˆ¬ï¼‰ | `np.linalg.eig(A)` | `scipy.linalg.eig(A)` | éå¯¾ç§°è¡Œåˆ—ç”¨ã€‚è¤‡ç´ å›ºæœ‰å€¤ã‚ã‚Š |
| SVD | `np.linalg.svd(A)` | `scipy.linalg.svd(A)` | ç¬¬3å›ã§è©³ã—ã |
| QRåˆ†è§£ | `np.linalg.qr(A)` | `scipy.linalg.qr(A)` | `mode='reduced'` ã§economy QR |
| Choleskyåˆ†è§£ | `np.linalg.cholesky(A)` | `scipy.linalg.cholesky(A)` | NumPy: ä¸‹ä¸‰è§’ $L$ã€SciPy: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸Šä¸‰è§’ |
| è¡Œåˆ—å¼ | `np.linalg.det(A)` | â€” | å¤§è¡Œåˆ—ã§ã¯å¯¾æ•°è¡Œåˆ—å¼ `slogdet` ã‚’ä½¿ã† |
| ãƒ©ãƒ³ã‚¯ | `np.linalg.matrix_rank(A)` | â€” | æ•°å€¤ãƒ©ãƒ³ã‚¯ï¼ˆé–¾å€¤ä»˜ãï¼‰ |
| ãƒãƒ«ãƒ  | `np.linalg.norm(A, ord)` | â€” | `ord=2`: ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã€`ord='fro'`: Frobenius |
| einsum | `np.einsum('ij,jk->ik', A, B)` | â€” | Einsteinè¨˜æ³•ã€‚ãƒãƒƒãƒå‡¦ç†ã«ä¾¿åˆ© |


### 6.2 ç”¨èªé›†

<details><summary>ç”¨èªé›†</summary>
| è‹±èª | æ—¥æœ¬èª | è¨˜å· |
|:-----|:------|:-----|
| Vector space | ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ | $V$ |
| Linear independence | ç·šå½¢ç‹¬ç«‹ | |
| Basis | åŸºåº• | $\{\mathbf{e}_i\}$ |
| Dimension | æ¬¡å…ƒ | $\dim V$ |
| Inner product | å†…ç© | $\langle \cdot, \cdot \rangle$ |
| Norm | ãƒãƒ«ãƒ  | $\|\cdot\|$ |
| Orthogonal | ç›´äº¤ | $\perp$ |
| Eigenvalue | å›ºæœ‰å€¤ | $\lambda$ |
| Eigenvector | å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« | $\mathbf{v}$ |
| Positive definite | æ­£å®šå€¤ | $A \succ 0$ |
| Trace | ãƒˆãƒ¬ãƒ¼ã‚¹ | $\text{tr}(\cdot)$ |
| Determinant | è¡Œåˆ—å¼ | $\det(\cdot)$ |
| Rank | ãƒ©ãƒ³ã‚¯ | $\text{rank}(\cdot)$ |
| Projection | å°„å½± | $P$ |
| Least squares | æœ€å°äºŒä¹—æ³• | |
| QR decomposition | QRåˆ†è§£ | $A = QR$ |
| Cholesky decomposition | Choleskyåˆ†è§£ | $A = LL^\top$ |
| Spectral theorem | ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç† | |
| Cauchy-Schwarz inequality | Cauchy-Schwarzä¸ç­‰å¼ | |
| Rayleigh quotient | Rayleighå•† | $R(\mathbf{x})$ |
</details>

### 6.25 è£œéº â€” é«˜é€ŸåŒ–æŠ€è¡“ã¨ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

> **Note:** **è¨ˆç®—åŠ¹ç‡ã®é™ç•Œã¨çªç ´**: å¯†è¡Œåˆ—ã® SVD ã¯ $O(n^3)$ ã®è¨ˆç®—é‡ã ãŒ[^13]ã€ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã¨GPUæ´»ç”¨ã§å®Ÿç”¨çš„ãªé«˜é€ŸåŒ–ãŒå¯èƒ½ã«ã€‚æœ¬ç¯€ã§ã¯æœ€æ–°ç ”ç©¶ã«åŸºã¥ãå®Ÿè·µçš„æ‰‹æ³•ã‚’è§£èª¬ã€‚

#### ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD â€” å¤§è¦æ¨¡è¡Œåˆ—ã®ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼

é€šå¸¸ã® SVD ã¯ $O(\min(mn^2, m^2n))$ ã®è¨ˆç®—é‡ã‚’è¦ã™ã‚‹ãŒã€ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD[^14] ã¯ $O(mnk)$ï¼ˆ$k$ ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ³ã‚¯ï¼‰ã«å‰Šæ¸›ã§ãã‚‹ã€‚

##### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 


**ç†è«–çš„ä¿è¨¼**:

$$
\mathbb{E}\left[\|A - QQ^\top A\|_F\right] \leq \left(1 + \frac{k}{p-k-1}\right)^{1/2} \sigma_{k+1}
$$

ã“ã“ã§ $\sigma_{k+1}$ ã¯ $(k+1)$ ç•ªç›®ã®ç‰¹ç•°å€¤ã€‚ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° $p = k + 10$ ã§é«˜ç²¾åº¦ãªè¿‘ä¼¼ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

##### æ€§èƒ½æ¯”è¼ƒ

| æ‰‹æ³• | è¨ˆç®—é‡ | 1000Ã—1000 (k=50) | ç²¾åº¦ |
|:---|:---|:---:|:---|
| é€šå¸¸ SVD | $O(n^3)$ | 2.3ç§’ | Exact |
| ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD | $O(mnk)$ | 0.08ç§’ | ç›¸å¯¾èª¤å·® < 1% |

#### GPU åŠ é€Ÿã«ã‚ˆã‚‹è¡Œåˆ—åˆ†è§£ã®é«˜é€ŸåŒ–

2024-2025å¹´ã®ç ”ç©¶[^15][^16]ã«ã‚ˆã‚Šã€GPUå®Ÿè£…ã§å¾“æ¥æ‰‹æ³•ã® 10-1000å€ã®é«˜é€ŸåŒ–ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã‚‹ã€‚

##### QRåˆ†è§£ã®GPUå®Ÿè£…ï¼ˆCuPyï¼‰


##### æœ€æ–°ã® GPU-SVD ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

Ringoot et al. (2025)[^15] ã«ã‚ˆã‚‹ portable SVD å®Ÿè£…ã®ç‰¹å¾´:

- **2æ®µéš QR ç°¡ç´„**: bandå½¢å¼ â†’ 2å¯¾è§’å½¢å¼ã®æ®µéšçš„å¤‰æ›
- **GPUæœ€é©åŒ–**: Apple Metalã€CUDAã€ROCm ã«å¯¾å¿œ
- **åŠç²¾åº¦å¯¾å¿œ**: FP16 ã§ 2å€ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼ˆç²¾åº¦è¦ä»¶ãŒç·©ã„å ´åˆï¼‰

æ•°å¼çš„ã«ã¯ã€ä»¥ä¸‹ã®å¤‰æ›ã‚’ GPU ä¸Šã§å®Ÿè¡Œ:

$$
A \xrightarrow{\text{Householder}} B \xrightarrow{\text{Givens}} \text{Bidiag} \xrightarrow{\text{D\&C}} U\Sigma V^\top
$$

å„ã‚¹ãƒ†ãƒ¼ã‚¸ã§ GPU ãƒ¡ãƒ¢ãƒªéšå±¤ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«/å…±æœ‰/ãƒ¬ã‚¸ã‚¹ã‚¿ï¼‰ã‚’æœ€é©æ´»ç”¨ã™ã‚‹ã“ã¨ã§ 100-300å€ã®é«˜é€ŸåŒ–ã‚’é”æˆ[^16]ã€‚

#### ãƒ©ãƒ³ã‚¯é¡•åœ¨åŒ– QLP åˆ†è§£

Randomized Rank-Revealing QLP (RU-QLP) åˆ†è§£[^17] ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ unpivoted QR ã‚’çµ„ã¿åˆã‚ã›:

$$
A P = Q \begin{bmatrix} L_{11} & 0 \\ L_{21} & L_{22} \end{bmatrix} P^\top
$$

ã“ã“ã§ $L_{11}$ ã¯ $k \times k$ ã®ä¸‹ä¸‰è§’è¡Œåˆ—ã€$P$ ã¯ç½®æ›è¡Œåˆ—ã€‚

##### æ€§èƒ½:
- **CPU**: ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD ã® 7.1-8.5å€é«˜é€Ÿ
- **GPU**: ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD ã® 2.3-5.8å€é«˜é€Ÿ
- **èª¤å·®ä¿è¨¼**: $\|A - A_k\|_2 \leq (1+\epsilon)\sigma_{k+1}$


#### å®Ÿè·µçš„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

| è¡Œåˆ—ã‚µã‚¤ã‚º | ãƒ©ãƒ³ã‚¯ | æ¨å¥¨æ‰‹æ³• | ç†ç”± |
|:---|:---|:---|:---|
| $n < 1000$ | Full | `np.linalg.svd` | æ­£ç¢ºãƒ»ç°¡æ½” |
| $n \geq 1000$ | $k \ll n$ | ãƒ©ãƒ³ãƒ€ãƒ åŒ– SVD | $O(mnk)$ è¨ˆç®—é‡ |
| $n \geq 5000$ | Any | GPU (CuPy/JAX) | 10-100å€é«˜é€ŸåŒ– |
| ã‚¹ãƒ‘ãƒ¼ã‚¹ | å° $k$ | `scipy.sparse.linalg.svds` | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ |

> **âš ï¸ Warning:** **æ³¨æ„**: GPU ã¯åˆæœŸåŒ–ã‚³ã‚¹ãƒˆï¼ˆæ•°ç™¾msï¼‰ãŒã‚ã‚‹ãŸã‚ã€å°è¦æ¨¡è¡Œåˆ—ã§ã¯ CPU ã®æ–¹ãŒé€Ÿã„å ´åˆã‚‚ã‚ã‚‹ã€‚$n \geq 5000$ ãŒç›®å®‰ã€‚

#### ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

å¤§è¡Œåˆ—ã‚’æ‰±ã†éš›ã¯ã€Œãƒ¡ãƒ¢ãƒªã«å…¨éƒ¨ä¹—ã›ãªã„ã€è¨­è¨ˆãŒé‡è¦ã€‚

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | æ‰‹æ³• | ç”¨é€” |
|:---|:---|:---|
| **Chunked computation** | è¡Œã‚’åˆ†å‰²ã—ã¦å‡¦ç† | å·¨å¤§ãª Gram è¡Œåˆ— $X X^\top$ |
| **In-place ops** | `A += B`ï¼ˆ`A = A + B` ã‚ˆã‚ŠåŠ¹ç‡çš„ï¼‰ | å‹¾é…ç´¯ç© |
| **Sparse format** | `scipy.sparse.csr_matrix` | ç–è¡Œåˆ—ï¼ˆã‚¼ãƒ­ãŒ90%è¶…ï¼‰ |
| **Low-rank factor** | `U @ V.T` ã‚’åˆ†è§£ã®ã¾ã¾ä¿æŒ | LoRA[^10]ã®é‡ã¿æ›´æ–° |

LoRAã¯ã€Œãƒ©ãƒ³ã‚¯ $r$ ã®ç© $BA$ï¼ˆ$r \ll d$ï¼‰ã§å¤§è¡Œåˆ—ã®æ›´æ–°ã‚’è¿‘ä¼¼ã€ã™ã‚‹æŠ€æ³•ã€‚$W + \Delta W = W + BA$ ã®å½¢ã§å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šæ¸›ã™ã‚‹ã€‚ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—é‡ãŒ $O(d \cdot r)$ ã«è½ã¡ã‚‹ï¼ˆvs $O(d^2)$ï¼‰ã€‚
#### ã¾ã¨ã‚: ç·šå½¢ä»£æ•°ã®é«˜é€ŸåŒ–æŠ€è¡“ãƒãƒƒãƒ—

```mermaid
graph TD
    A[å¤§è¦æ¨¡è¡Œåˆ—ã®åˆ†è§£] --> B{ãƒ©ãƒ³ã‚¯}
    B -->|Full rank| C[GPUåŠ é€Ÿ<br/>CuPy/JAX]
    B -->|ä½ãƒ©ãƒ³ã‚¯ kâ‰ªn| D[ãƒ©ãƒ³ãƒ€ãƒ åŒ–æ‰‹æ³•]
    D --> E[Randomized SVD<br/>O mnk]
    D --> F[RU-QLP<br/>SVDã‚ˆã‚Šé«˜é€Ÿ]
    C --> G[2æ®µéšQRç°¡ç´„<br/>100-300xé«˜é€ŸåŒ–]
    E --> H[Halko 2011]
    F --> I[Feng 2022]
    G --> J[Ringoot 2025]
```

**References**:
- Halko, N., Martinsson, P. G., & Tropp, J. A. (2011). Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. *SIAM Review*, 53(2), 217-288.
- Martinsson, P. G., & Tropp, J. A. (2020). Randomized numerical linear algebra: Foundations and algorithms. *Acta Numerica*, 29, 403-572.

### 6.3 çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
mindmap
  root((ç·šå½¢ä»£æ•° I))
    ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“
      å…¬ç†
      ç·šå½¢ç‹¬ç«‹
      åŸºåº•ã¨æ¬¡å…ƒ
      ç·šå½¢å†™åƒ
    å†…ç©ã¨ç›´äº¤æ€§
      å†…ç©ã®å…¬ç†
      Cauchy-Schwarz
      ãƒãƒ«ãƒ 
      ç›´äº¤æ€§
    è¡Œåˆ—æ¼”ç®—
      ç©ã®3ã¤ã®è¦‹æ–¹
      è»¢ç½®
      é€†è¡Œåˆ—
      ãƒˆãƒ¬ãƒ¼ã‚¹
    å›ºæœ‰å€¤åˆ†è§£
      ç‰¹æ€§æ–¹ç¨‹å¼
      å¯¾è§’åŒ–
      ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†
      Rayleighå•†
    æ­£å®šå€¤è¡Œåˆ—
      åˆ¤å®šæ¡ä»¶
      Choleskyåˆ†è§£
      äºŒæ¬¡å½¢å¼
    å°„å½±ã¨æœ€å°äºŒä¹—
      æ­£è¦æ–¹ç¨‹å¼
      PCA
      Attention QK^T
```

### 6.35 æ•°å€¤å®‰å®šæ€§ã¨æ¡ä»¶æ•° â€” å®Ÿè£…ã§é™¥ã‚Šã‚„ã™ã„ç½ 

> **Note:** **æ•°å€¤è¨ˆç®—ã®ç¾å®Ÿ**: æ•°å­¦çš„ã«æ­£ã—ã„å¼ã§ã‚‚ã€æµ®å‹•å°æ•°ç‚¹æ¼”ç®—ã§ã¯ä¸å®‰å®šã«ãªã‚Šå¾—ã‚‹[^18]ã€‚æ¡ä»¶æ•° (condition number) ã¯ã€ã“ã®å®‰å®šæ€§ã‚’å®šé‡åŒ–ã™ã‚‹éµã¨ãªã‚‹æ¦‚å¿µã€‚

#### æ¡ä»¶æ•°ã®å®šç¾©ã¨æ„å‘³

è¡Œåˆ— $A \in \mathbb{R}^{n \times n}$ ã® **æ¡ä»¶æ•°** ã¯ä»¥ä¸‹ã§å®šç¾©ã•ã‚Œã‚‹:

$$
\kappa(A) = \|A\| \cdot \|A^{-1}\| = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}
$$

ã“ã“ã§ $\sigma_{\max}, \sigma_{\min}$ ã¯æœ€å¤§ãƒ»æœ€å°ç‰¹ç•°å€¤ã€‚

**ç›´æ„Ÿçš„è§£é‡ˆ**:
- $\kappa(A) = 1$: ç†æƒ³çš„ï¼ˆç›´äº¤è¡Œåˆ—ï¼‰
- $\kappa(A) \sim 10^2$: è‰¯å¥½
- $\kappa(A) \sim 10^{6}$: è­¦æˆ’ï¼ˆå˜ç²¾åº¦FP32ã§æ¡è½ã¡ç™ºç”Ÿï¼‰
- $\kappa(A) \sim 10^{14}$: å±é™ºï¼ˆå€ç²¾åº¦FP64ã§ã‚‚ç²¾åº¦å–ªå¤±ï¼‰
- $\kappa(A) = \infty$: ç‰¹ç•°è¡Œåˆ—ï¼ˆé€†è¡Œåˆ—ãªã—ï¼‰


#### æ¡ä»¶æ•°ãŒå¤§ãããªã‚‹å®Ÿä¾‹

##### 1. é«˜ç›¸é–¢ãªç‰¹å¾´é‡è¡Œåˆ—ï¼ˆæ©Ÿæ¢°å­¦ç¿’ã§ã®å…¸å‹ä¾‹ï¼‰


##### 2. Hilbert è¡Œåˆ—ï¼ˆæ•™ç§‘æ›¸çš„ãªç—…çš„è¡Œåˆ—ï¼‰

$$
H_{ij} = \frac{1}{i+j-1}, \quad i, j = 1, \ldots, n
$$


##### 3. æ·±å±¤å­¦ç¿’ã®é‡ã¿è¡Œåˆ—

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨“ç·´ä¸­ã€é‡ã¿è¡Œåˆ—ã®æ¡ä»¶æ•°ãŒå¢—å¤§ã™ã‚‹ã¨å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºãŒç™ºç”Ÿ[^18]ã€‚


**å®Ÿéš›ã®å¯¾ç­–**:
- **Batch Normalization**: å±¤ã”ã¨ã«æ­£è¦åŒ–ã—ã€æ¡ä»¶æ•°ã‚’æŠ‘åˆ¶
- **Residual Connections (ResNet)**: ç›´æ¥ãƒ‘ã‚¹ã§æ¡ä»¶æ•°ã®ç´¯ç©ã‚’å›é¿
- **Weight Normalization**: é‡ã¿ã‚’å˜ä½ãƒãƒ«ãƒ ã«æ­£è¦åŒ–

#### æ•°å€¤å®‰å®šãªå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

##### ãƒ‘ã‚¿ãƒ¼ãƒ³1: é€£ç«‹æ–¹ç¨‹å¼ã¯é€†è¡Œåˆ—ã§ã¯ãªãç›´æ¥æ³•ã§


**ç†è«–çš„æ ¹æ‹ **: $\kappa(A)$ ãŒå¤§ãã„ã¨ãã€$A^{-1}$ ã®è¨ˆç®—èª¤å·®ãŒè§£ $x$ ã«å¢—å¹…ã•ã‚Œã‚‹ã€‚ç›´æ¥æ³•ã¯å®‰å®šæ€§ãŒé«˜ã„ã€‚

##### ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ­£å®šå€¤è¡Œåˆ—ã«ã¯ Cholesky åˆ†è§£


##### ãƒ‘ã‚¿ãƒ¼ãƒ³3: SVD ã«ã‚ˆã‚‹å®‰å®šãªç–‘ä¼¼é€†è¡Œåˆ—

æ¡ä»¶æ•°ãŒå¤§ããã€ãƒ©ãƒ³ã‚¯ãŒä¸æ˜ç­ãªå ´åˆ:


#### æ¡ä»¶æ•°åˆ¶ç´„ä»˜ãå…±åˆ†æ•£è¡Œåˆ—è¿‘ä¼¼

Zhao et al. (2020)[^19] ã¯ã€é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å…±åˆ†æ•£è¡Œåˆ—æ¨å®šã«ãŠã„ã¦ã€æ¡ä»¶æ•°åˆ¶ç´„ã‚’èª²ã™ã“ã¨ã§æ•°å€¤å®‰å®šæ€§ã¨æ­£å®šå€¤æ€§ã‚’åŒæ™‚ã«ä¿è¨¼ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆ:

$$
\min_{S \succ 0} \|S - \hat{\Sigma}\|_F^2 \quad \text{s.t.} \quad \kappa(S) \leq \kappa_{\max}
$$

ã“ã“ã§ $\hat{\Sigma}$ ã¯ã‚µãƒ³ãƒ—ãƒ«å…±åˆ†æ•£è¡Œåˆ—ã€$\kappa_{\max}$ ã¯è¨±å®¹æ¡ä»¶æ•°ã€‚


ã“ã®æ‰‹æ³•ã¯ã€Ridgeå›å¸°ãƒ»æ­£å‰‡åŒ–å…±åˆ†æ•£æ¨å®šãƒ»ã‚«ãƒ¼ãƒãƒ«æ³•ãªã©ã®ç†è«–çš„åŸºç›¤ã¨ãªã£ã¦ã„ã‚‹ã€‚

#### å®Ÿè·µçš„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³: æ¡ä»¶æ•°è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

| çŠ¶æ³ | æ¡ä»¶æ•°ç¯„å›² | æ¨å¥¨å¯¾ç­– |
|:---|:---|:---|
| ç·šå½¢å›å¸°ï¼ˆé«˜ç›¸é–¢ç‰¹å¾´ï¼‰ | $\kappa \geq 10^6$ | Ridge / Lasso / PCA ã§æ¬¡å…ƒå‰Šæ¸› |
| å…±åˆ†æ•£è¡Œåˆ—ï¼ˆ$n < p$ï¼‰ | $\kappa = \infty$ | æ­£å‰‡åŒ– or Ledoit-Wolf æ¨å®š |
| ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆè¨“ç·´ | å±¤æ•°ã«å¿œã˜ã¦å¢—å¤§ | Batch Norm / Layer Norm / ResNet |
| æ•°å€¤æœ€é©åŒ–ï¼ˆHessianï¼‰ | $\kappa \geq 10^4$ | Preconditioner / Adam / 2æ¬¡æ‰‹æ³• |
| GPU ã§ã® FP16 è¨ˆç®— | $\kappa \geq 10^3$ | Mixed precision trainingï¼ˆFP32 accumulationï¼‰ |


#### ã¾ã¨ã‚: æ•°å€¤å®‰å®šæ€§ã®åŸå‰‡

1. **é€†è¡Œåˆ—ã¯é¿ã‘ã‚‹** â€” `solve()` ã‚’ä½¿ã†
2. **æ­£å®šå€¤è¡Œåˆ—ã«ã¯ Cholesky** â€” é«˜é€Ÿ + å®‰å®š
3. **æ¡ä»¶æ•°ã‚’ç›£è¦–** â€” `np.linalg.cond()` ã§å®šæœŸãƒã‚§ãƒƒã‚¯
4. **æ­£å‰‡åŒ–ã¯ä¸‡èƒ½è–¬** â€” $\lambda \sim \sigma_{\min}$ ãŒç›®å®‰
5. **SVD ã¯æœ€å¾Œã®ç ¦** â€” ç–‘ä¼¼é€†è¡Œåˆ—ã§é ‘å¥ã«è§£ã

```mermaid
graph TD
    A[ç·šå½¢ã‚·ã‚¹ãƒ†ãƒ  Ax=b] --> B{A ã¯æ­£å®šå€¤?}
    B -->|Yes| C[Choleskyåˆ†è§£<br/>cho_solve]
    B -->|No| D{Îº A ?}
    D -->|Îº < 10^6| E[LUåˆ†è§£<br/>np.linalg.solve]
    D -->|Îº â‰¥ 10^6| F{ãƒ©ãƒ³ã‚¯ä¸è¶³?}
    F -->|Yes| G[SVDç–‘ä¼¼é€†è¡Œåˆ—<br/>pinv rcond=1e-6]
    F -->|No| H[æ­£å‰‡åŒ–<br/>Ridge Î»~1e-4]
```

### 6.4 æœ¬è¬›ç¾©ã®3ã¤ã®ãƒã‚¤ãƒ³ãƒˆ

**1. å†…ç© = é¡ä¼¼åº¦ã®æ•°å­¦çš„åŸºç›¤**

$$
\langle \mathbf{q}_i, \mathbf{k}_j \rangle = \mathbf{q}_i^\top \mathbf{k}_j
$$

Attention[^1]ã®æ ¸å¿ƒã¯å†…ç©ã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—ã€‚Cauchy-Schwarzä¸ç­‰å¼ãŒã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®å€¤åŸŸ $[-1, 1]$ ã‚’ä¿è¨¼ã™ã‚‹ã€‚

**2. å›ºæœ‰å€¤åˆ†è§£ = è¡Œåˆ—ã®ã€ŒXç·šå†™çœŸã€**

$$
A = Q\Lambda Q^\top
$$

å¯¾ç§°è¡Œåˆ—ã¯å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã§å¯¾è§’åŒ–ã§ãã€å›ºæœ‰å€¤ãŒè¡Œåˆ—ã®æœ¬è³ªçš„ãªæƒ…å ±ï¼ˆåˆ†æ•£ã®å¤§ãã•ã€å®‰å®šæ€§ã€å‡¸æ€§ï¼‰ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚PCA[^6][^7]ã¯ã“ã®ç›´æ¥çš„ãªå¿œç”¨ã€‚

**3. æ­£å®šå€¤æ€§ = å®‰å…¨è£…ç½®**

$$
\mathbf{x}^\top A \mathbf{x} > 0 \quad \forall \mathbf{x} \neq \mathbf{0}
$$

å…±åˆ†æ•£è¡Œåˆ—ã®æ­£å®šå€¤æ€§ã€ãƒ˜ã‚·ã‚¢ãƒ³ã®æ­£å®šå€¤æ€§ã«ã‚ˆã‚‹å‡¸æ€§ä¿è¨¼ã€Choleskyåˆ†è§£ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªè¨ˆç®—ã€‚

### 6.5 FAQ

<details><summary>Q: ç·šå½¢ä»£æ•°ã¯ã©ã“ã¾ã§æ·±ãã‚„ã‚‹ã¹ãï¼Ÿ</summary>
ã“ã®è¬›ç¾©ã¨æ¬¡ã®ç¬¬3å›ã§æ‰±ã†ç¯„å›²ã‚’ã—ã£ã‹ã‚Šç†è§£ã™ã‚Œã°ã€Course IIï¼ˆç¬¬9-16å›ï¼‰ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ•°å¼ã¯å…¨ã¦èª­ã‚ã‚‹ã€‚è¨¼æ˜ã‚’æš—è¨˜ã™ã‚‹å¿…è¦ã¯ãªã„ã€‚ã€Œãªãœã“ã†ãªã‚‹ã‹ã€ã®ç›´æ„Ÿã‚’æŒã£ã¦ã„ã‚Œã°ååˆ†ã€‚

ãŸã ã—ã€ç ”ç©¶ã§ä½¿ã†å ´åˆã¯ Golub & Van Loan[^8] ã®é–¢é€£ç« ã‚’èª­ã‚€ã“ã¨ã‚’å‹§ã‚ã‚‹ã€‚æ•°å€¤å®‰å®šæ€§ã‚„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠè‚¢ã«é–¢ã™ã‚‹çŸ¥è­˜ã¯ã€å®Ÿè£…ã®å“è³ªã«ç›´çµã™ã‚‹ã€‚
</details>

<details><summary>Q: eigh ã¨ eig ã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ</summary>
å¯¾ç§°è¡Œåˆ—ï¼ˆå…±åˆ†æ•£è¡Œåˆ—ã€ãƒ˜ã‚·ã‚¢ãƒ³ç­‰ï¼‰ã«ã¯å¿…ãš `eigh` ã‚’ä½¿ã†ã€‚ä¸€èˆ¬è¡Œåˆ—ã«ã¯ `eig`ã€‚`eigh` ã¯å¯¾ç§°æ€§ã‚’åˆ©ç”¨ã™ã‚‹ã®ã§ç´„2å€é€Ÿãã€å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã®ç›´äº¤æ€§ãŒæ•°å€¤çš„ã«ã‚‚ä¿è¨¼ã•ã‚Œã‚‹ã€‚
</details>

<details><summary>Q: é€†è¡Œåˆ—ã®è¨ˆç®—ã¯ã©ã®ãã‚‰ã„é¿ã‘ã‚‹ã¹ãï¼Ÿ</summary>
æ˜ç¤ºçš„ã« $A^{-1}$ ãŒå¿…è¦ãªå ´é¢ã¯ã»ã¼ãªã„ã€‚$A^{-1}\mathbf{b}$ â†’ `solve(A, b)`ã€$A^{-1}B$ â†’ `solve(A, B)`ã€$\det(A^{-1})$ â†’ `1/det(A)`ã€‚$A^{-1}$ è‡ªä½“ãŒå¿…è¦ãªã®ã¯ã€å°„å½±è¡Œåˆ— $P = A(A^\top A)^{-1}A^\top$ ã®å¯è¦–åŒ–ãã‚‰ã„ã€‚
</details>

<details><summary>Q: PCA ã§æ¬¡å…ƒã‚’ã„ãã¤ã«è½ã¨ã™ã¹ãï¼Ÿ</summary>
ç´¯ç©å¯„ä¸ç‡ï¼ˆcumulative explained variance ratioï¼‰ãŒ 90-95% ã«ãªã‚‹æ¬¡å…ƒæ•°ãŒä¸€èˆ¬çš„ãªç›®å®‰ã€‚ãŸã ã—ã€å¯è¦–åŒ–ç›®çš„ãªã‚‰2-3æ¬¡å…ƒã€‚ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ã§æ±ºã‚ã‚‹ã®ãŒæœ€å–„ã€‚
</details>

<details><summary>Q: einsum ã¯è¦šãˆã‚‹å¿…è¦ãŒã‚ã‚‹ï¼Ÿ</summary>
å¿…é ˆã§ã¯ãªã„ãŒã€è«–æ–‡ã®ã‚³ãƒ¼ãƒ‰ã§ã‚ˆãè¦‹ã‹ã‘ã‚‹ã€‚ç‰¹ã«Transformerç³»ã®å®Ÿè£…ã§ã¯ `einsum` ãŒå¤šç”¨ã•ã‚Œã‚‹ã€‚æœ€ä½é™ã€å†…ç© `'i,i->'`ã€è¡Œåˆ—ç© `'ik,kj->ij'`ã€ãƒãƒƒãƒè¡Œåˆ—ç© `'bik,bkj->bij'` ã®3ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦šãˆã¦ãŠã‘ã°å›°ã‚‰ãªã„ã€‚
</details>

<details><summary>Q: ç·šå½¢ä»£æ•°ã¨å¾®ç©åˆ†ã€ã©ã¡ã‚‰ãŒå…ˆã«å¿…è¦ï¼Ÿ</summary>
ç·šå½¢ä»£æ•°ãŒå…ˆã€‚ç†ç”±: (1) æ©Ÿæ¢°å­¦ç¿’ã®å¤šãã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯è¡Œåˆ—æ¼”ç®—ã§è¨˜è¿°ã•ã‚Œã‚‹ã€(2) å‹¾é…ã¯ã€Œãƒ™ã‚¯ãƒˆãƒ«å€¤é–¢æ•°ã®å¾®åˆ†ã€ãªã®ã§ç·šå½¢ä»£æ•°ã®è¨€è‘‰ã§å®šç¾©ã•ã‚Œã‚‹ã€(3) é€†ä¼æ’­æ³•ã¯ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®é€£é–å¾‹ã§ã‚ã‚Šã€è¡Œåˆ—å¾®åˆ†ã¯ç·šå½¢ä»£æ•°ã®ä¸Šã«æ§‹ç¯‰ã•ã‚Œã‚‹ã€‚

æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯ç¬¬2-3å›ã§ç·šå½¢ä»£æ•°ã€ç¬¬4å›ã§ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ã€ç¬¬5å›ã§æ¸¬åº¦è«–çš„ç¢ºç‡è«–ãƒ»ç¢ºç‡éç¨‹ã€ç¬¬6å›ã§æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–ã®é †ç•ªã‚’å–ã£ã¦ã„ã‚‹ã€‚
</details>

<details><summary>Q: å¤§ããªè¡Œåˆ—ã®å›ºæœ‰å€¤åˆ†è§£ã¯é…ã„ã®ã§ã¯ï¼Ÿ</summary>
ãã®é€šã‚Šã€‚$n \times n$ è¡Œåˆ—ã®å®Œå…¨ãªå›ºæœ‰å€¤åˆ†è§£ã¯ $O(n^3)$ ã§ã€$n > 10000$ ã§ã¯å®Ÿç”¨çš„ã§ãªã„ã€‚å®Ÿå‹™ã§ã¯:

1. **Power Iteration / Lanczosæ³•**: ä¸Šä½ $k$ å€‹ã®å›ºæœ‰å€¤ãƒ»å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã ã‘ã‚’ $O(kn^2)$ ã§è¨ˆç®—
2. **Randomized SVD**: ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§æ¬¡å…ƒã‚’è½ã¨ã—ã¦ã‹ã‚‰SVDã€‚scikit-learn ã® PCA ã¯ã“ã‚Œã‚’ä½¿ã†
3. **Sparse solver**: ç–è¡Œåˆ—ãªã‚‰ `scipy.sparse.linalg.eigsh` ã§å¤§è¦æ¨¡å•é¡Œã«å¯¾å¿œ
4. **GPUè¨ˆç®—**: cuSOLVER ã§ GPUä¸Šã®å¤§è¦æ¨¡å›ºæœ‰å€¤åˆ†è§£

ç¬¬3å›ã§SVDã®åŠ¹ç‡çš„ãªè¨ˆç®—æ³•ã‚’è©³ã—ãæ‰±ã†ã€‚
</details>

<details><summary>Q: PyTorch ã§ã‚‚ç·šå½¢ä»£æ•°é–¢æ•°ã‚’ä½¿ãˆã‚‹ã‹ï¼Ÿ</summary>
ä½¿ãˆã‚‹ã€‚`torch.linalg` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒ NumPy ã® `np.linalg` ã¨ã»ã¼åŒã˜APIã‚’æä¾›ã™ã‚‹ã€‚è‡ªå‹•å¾®åˆ†å¯¾å¿œãªã®ã§ã€å›ºæœ‰å€¤åˆ†è§£ã‚„SVDã®çµæœã‚’é€šã˜ã¦å‹¾é…ã‚’é€†ä¼æ’­ã§ãã‚‹ã€‚


ãŸã ã—ã€å›ºæœ‰å€¤ãŒé‡è¤‡ï¼ˆdegenerateï¼‰ã—ã¦ã„ã‚‹å ´åˆã®å‹¾é…ã¯ä¸å®‰å®šãªã®ã§æ³¨æ„ã€‚
</details>

### 6.6 ã‚ˆãã‚ã‚‹é–“é•ã„ãƒ»å‹˜é•ã„

ç·šå½¢ä»£æ•°ã®å­¦ç¿’ã§é »å‡ºã™ã‚‹é–“é•ã„ã‚’å…ˆã«çŸ¥ã£ã¦ãŠãã“ã¨ã§ã€ç„¡é§„ãªèº“ãã‚’é¿ã‘ã‚‰ã‚Œã‚‹ã€‚

#### é–“é•ã„1: è¡Œåˆ—ç©ã¯äº¤æ›å¯èƒ½

$$
AB \neq BA \quad \text{ï¼ˆä¸€èˆ¬ã«ã¯æˆã‚Šç«‹ãŸãªã„ï¼‰}
$$


**æ­£ã—ã„ç†è§£**: è¡Œåˆ—ç©ã¯ä¸€èˆ¬ã«éå¯æ›ã€‚$AB = BA$ ãŒæˆã‚Šç«‹ã¤ã®ã¯ç‰¹æ®Šãªå ´åˆï¼ˆ$B = \alpha I$ã€$A$ ã¨ $B$ ãŒåŒæ™‚å¯¾è§’åŒ–å¯èƒ½ãªå ´åˆãªã©ï¼‰ã®ã¿ã€‚ãŸã ã—ã€ãƒˆãƒ¬ãƒ¼ã‚¹ã«ã¤ã„ã¦ã¯ $\text{tr}(AB) = \text{tr}(BA)$ï¼ˆå·¡å›æ€§ï¼‰ãŒ**å¸¸ã«**æˆã‚Šç«‹ã¤ã€‚

#### é–“é•ã„2: é€†è¡Œåˆ—ã§é€£ç«‹æ–¹ç¨‹å¼ã‚’è§£ã


**æ­£ã—ã„ç†è§£**: `solve` ã¯å†…éƒ¨ã§LUåˆ†è§£ã‚’ä½¿ã„ã€é€†è¡Œåˆ—ã‚’æ˜ç¤ºçš„ã«è¨ˆç®—ã—ãªã„ã€‚è¨ˆç®—é‡ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã¯åŒã˜ã ãŒã€æ•°å€¤å®‰å®šæ€§ãŒå¤§ããç•°ãªã‚‹ã€‚æ¡ä»¶æ•°ãŒå¤§ãã„è¡Œåˆ—ã§ã¯ã€`inv` ã®çµæœã¯ä¿¡ç”¨ã§ããªã„ã€‚

#### é–“é•ã„3: å›ºæœ‰å€¤åˆ†è§£ã¯ã©ã®è¡Œåˆ—ã§ã‚‚ã§ãã‚‹

**æ­£ã—ã„ç†è§£**: å…¨ã¦ã® $n \times n$ è¡Œåˆ—ãŒå¯¾è§’åŒ–å¯èƒ½ãªã‚ã‘ã§ã¯ãªã„ã€‚å¯¾è§’åŒ–å¯èƒ½æ€§ã®æ¡ä»¶ã¯ã€Œ$n$ å€‹ã®ç·šå½¢ç‹¬ç«‹ãªå›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã€ã€‚å¯¾ç§°è¡Œåˆ—ã¯å¸¸ã«å¯¾è§’åŒ–å¯èƒ½ï¼ˆã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†ï¼‰ã ãŒã€ä¸€èˆ¬ã®è¡Œåˆ—ã§ã¯ä¿è¨¼ã•ã‚Œãªã„ã€‚

$$
A = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}
$$

ã“ã®è¡Œåˆ—ã¯å›ºæœ‰å€¤ $\lambda = 0$ï¼ˆé‡è¤‡åº¦2ï¼‰ã‚’æŒã¤ãŒã€å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã¯1ã¤ã—ã‹ãªã„ã€‚å¯¾è§’åŒ–ä¸å¯èƒ½ã€‚

#### é–“é•ã„4: eig ã¨ eigh ã®æ··åŒ

`np.linalg.eig`ï¼ˆä¸€èˆ¬è¡Œåˆ—ç”¨ï¼‰ã¨ `np.linalg.eigh`ï¼ˆå¯¾ç§°/ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆè¡Œåˆ—å°‚ç”¨ï¼‰ã¯**åˆ¥ç‰©**ã€‚

| | `eig` | `eigh` |
|:---|:---|:---|
| å¯¾è±¡ | ä¸€èˆ¬æ­£æ–¹è¡Œåˆ— | å¯¾ç§°/ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆè¡Œåˆ— |
| å›ºæœ‰å€¤ | è¤‡ç´ æ•°ã«ãªã‚Šã†ã‚‹ | å®Ÿæ•°ä¿è¨¼ |
| å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« | ç›´äº¤æ€§ãªã— | ç›´äº¤æ€§ä¿è¨¼ |
| é€Ÿåº¦ | é…ã„ï¼ˆ$O(n^3)$ï¼‰ | é€Ÿã„ï¼ˆä¿‚æ•°ãŒå°ã•ã„ï¼‰ |

å…±åˆ†æ•£è¡Œåˆ— $\Sigma$ï¼ˆå¯¾ç§°æ­£å®šå€¤ï¼‰ã« `eig` ã‚’ä½¿ã†ã¨ã€æ•°å€¤èª¤å·®ã§å›ºæœ‰å€¤ã«è™šéƒ¨ãŒä¹—ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚`eigh` ã‚’ä½¿ãˆã°å®Ÿæ•°ã‹ã¤æ˜‡é †ã‚½ãƒ¼ãƒˆã§è¿”ã£ã¦ãã‚‹ã€‚**å¯¾ç§°è¡Œåˆ—ã«ã¯å¿…ãš `eigh`**ã€‚

#### é–“é•ã„5: è¡Œåˆ—ã®ãƒ©ãƒ³ã‚¯ã¨é€†è¡Œåˆ—ã®é–¢ä¿‚ã®èª¤è§£

| æ¡ä»¶ | $\text{rank}(A) = n$ | $\text{rank}(A) < n$ |
|:-----|:---------------------|:--------------------|
| é€†è¡Œåˆ— | å­˜åœ¨ã™ã‚‹ï¼ˆ$A$ ã¯æ­£å‰‡ï¼‰ | å­˜åœ¨ã—ãªã„ï¼ˆ$A$ ã¯ç‰¹ç•°ï¼‰ |
| é€£ç«‹æ–¹ç¨‹å¼ $A\mathbf{x} = \mathbf{b}$ | å”¯ä¸€è§£ | è§£ãªã— or ç„¡é™ã«è§£ãŒã‚ã‚‹ |
| å›ºæœ‰å€¤ | $0$ ã¯å›ºæœ‰å€¤ã§ãªã„ | $0$ ãŒå›ºæœ‰å€¤ã«å«ã¾ã‚Œã‚‹ |
| è¡Œåˆ—å¼ | $\det(A) \neq 0$ | $\det(A) = 0$ |

<details><summary>é–“é•ã„6: ãƒ™ã‚¯ãƒˆãƒ«ã®ç·šå½¢ç‹¬ç«‹æ€§ã®èª¤åˆ¤å®š</summary>
ã€Œã©ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚‚å¹³è¡Œã§ãªã‘ã‚Œã°ç·šå½¢ç‹¬ç«‹ã€ã¯**2æ¬¡å…ƒã§ã®ã¿æ­£ã—ã„**ã€‚3æ¬¡å…ƒä»¥ä¸Šã§ã¯ã€ã©ã®2æœ¬ã‚‚å¹³è¡Œã§ãªãã¦ã‚‚ç·šå½¢å¾“å±ã«ãªã‚Šå¾—ã‚‹ã€‚

$$
\mathbf{v}_1 = \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix}, \quad
\mathbf{v}_2 = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}, \quad
\mathbf{v}_3 = \begin{pmatrix} 1 \\ 1 \\ 2 \end{pmatrix}
$$

ã©ã®2æœ¬ã‚‚å¹³è¡Œã§ãªã„ãŒã€$\mathbf{v}_3 = \mathbf{v}_1 + \mathbf{v}_2$ ãªã®ã§ç·šå½¢å¾“å±ã€‚

æ­£ã—ã„åˆ¤å®šæ–¹æ³•ã¯ãƒ©ãƒ³ã‚¯ã‚’è¦‹ã‚‹ã“ã¨:

</details>

### 6.7 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ‰€è¦æ™‚é–“ |
|:---|:-----|:--------|
| Day 1 | Zone 0-2 é€šèª­ | 30åˆ† |
| Day 2 | Zone 3 å‰åŠï¼ˆ3.1-3.5ï¼‰ | 45åˆ† |
| Day 3 | Zone 3 å¾ŒåŠï¼ˆ3.6-3.9ï¼‰ | 45åˆ† |
| Day 4 | Zone 4ï¼ˆå®Ÿè£…ï¼‰ | 45åˆ† |
| Day 5 | Zone 5ï¼ˆãƒ†ã‚¹ãƒˆï¼‰ | 30åˆ† |
| Day 6 | å¾©ç¿’: 2Ã—2è¡Œåˆ—ã®å›ºæœ‰å€¤åˆ†è§£ã‚’æ‰‹è¨ˆç®— | 30åˆ† |
| Day 7 | ç¬¬3å›ã‚’å…ˆèª­ã¿ + æœ¬è¬›ç¾©ã®æŒ¯ã‚Šè¿”ã‚Š | 30åˆ† |

### 6.8 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

æœ¬è¬›ç¾©ã®ç†è§£åº¦ã‚’è‡ªå·±æ¡ç‚¹ã™ã‚‹ï¼ˆå„ãƒã‚§ãƒƒã‚¯ãŒã€Œã‚¹ãƒ©ã‚¹ãƒ©è¨€ãˆã‚‹ã€ãƒ¬ãƒ™ãƒ«ã§ âœ“ï¼‰:

- [ ] å†…ç© $\mathbf{a}^\top\mathbf{b} = \|\mathbf{a}\|\|\mathbf{b}\|\cos\theta$ ã®æ„å‘³ã‚’å›³ã§èª¬æ˜ã§ãã‚‹
- [ ] $A \in \mathbb{R}^{m \times n}$, $B \in \mathbb{R}^{n \times p}$ â†’ $AB \in \mathbb{R}^{m \times p}$ ã‚’å³ç­”ã§ãã‚‹
- [ ] $QK^\top$ ãŒã€Œå…¨ãƒˆãƒ¼ã‚¯ãƒ³å¯¾ã®é¡ä¼¼åº¦è¡Œåˆ—ã€ã ã¨è¨€ãˆã‚‹
- [ ] å›ºæœ‰å€¤åˆ†è§£ $A = Q\Lambda Q^\top$ï¼ˆå¯¾ç§°è¡Œåˆ—ï¼‰ã®æ„å‘³ã‚’ä¸€æ–‡ã§è¨€ãˆã‚‹
- [ ] æ­£å®šå€¤è¡Œåˆ—ã®æ¡ä»¶ï¼ˆå…¨å›ºæœ‰å€¤ > 0ï¼‰ã¨ã€Œãªãœé‡è¦ã‹ã€ã‚’è¨€ãˆã‚‹
- [ ] `np.linalg.solve(A, b)` vs `np.linalg.inv(A) @ b` ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] `eig` ã¨ `eigh` ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹

7é …ç›®ä¸­5é …ç›®ä»¥ä¸Šã§ç¬¬3å›ã«é€²ã‚“ã§ã‚ˆã„ã€‚3é …ç›®ä»¥ä¸‹ãªã‚‰ Z4 ã‚’ã‚‚ã†ä¸€å‘¨ã€‚

### 6.9 æ¬¡å›äºˆå‘Š: ç¬¬3å›ã€Œç·šå½¢ä»£æ•° II: SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«ã€

ç¬¬3å›ã§ã¯ã€æœ¬è¬›ç¾©ã§ç¯‰ã„ãŸåŸºç›¤ã®ä¸Šã«3ã¤ã®å¼·åŠ›ãªé“å…·ã‚’ç©ã¿ä¸Šã’ã‚‹:

1. **SVD**ï¼ˆç‰¹ç•°å€¤åˆ†è§£ï¼‰â€” è¡Œåˆ—ã®ã€Œä¸‡èƒ½ãƒŠã‚¤ãƒ•ã€ã€‚PCAã‚‚ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã‚‚æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚‚LoRA[^10]ã‚‚ã€å…¨ã¦SVDã®å¿œç”¨
2. **è¡Œåˆ—å¾®åˆ†** â€” ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤ã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³ãƒ»é€£é–å¾‹ã®è¡Œåˆ—ç‰ˆ
3. **è‡ªå‹•å¾®åˆ†** â€” PyTorchã® `loss.backward()` ã®ä¸­ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã‹ã€‚Forward mode vs Reverse mode ã®å®Œå…¨ç†è§£

**SVDã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**: ä»»æ„ã®è¡Œåˆ— $A \in \mathbb{R}^{m \times n}$ ã¯æ¬¡ã®ã‚ˆã†ã«åˆ†è§£ã§ãã‚‹:

$$
A = U\Sigma V^\top, \quad U \in \mathbb{R}^{m \times m},\ \Sigma \in \mathbb{R}^{m \times n},\ V \in \mathbb{R}^{n \times n}
$$

$U$ã€$V$ ã¯ç›´äº¤è¡Œåˆ—ã€$\Sigma$ ã¯éè² ã®å¯¾è§’æˆåˆ†ï¼ˆç‰¹ç•°å€¤ $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$ï¼‰ã‚’æŒã¤å¯¾è§’è¡Œåˆ—ã€‚

æœ¬è¬›ç¾©ã§å­¦ã‚“ã å›ºæœ‰å€¤åˆ†è§£ã¯ã€Œå¯¾ç§°è¡Œåˆ—å°‚ç”¨ã€ã€‚SVDã¯**ä»»æ„ã®è¡Œåˆ—**ã«é©ç”¨ã§ãã‚‹ã€Œå›ºæœ‰å€¤åˆ†è§£ã®ä¸€èˆ¬åŒ–ã€ã ã€‚ä¸¡è€…ã®æ¥ç¶š:

$$
A^\top A = V \Sigma^\top \Sigma V^\top \quad \text{ï¼ˆå³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ãŒ } A^\top A \text{ ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ï¼‰}
$$

$$
A A^\top = U \Sigma \Sigma^\top U^\top \quad \text{ï¼ˆå·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ãŒ } AA^\top \text{ ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ï¼‰}
$$

**è¡Œåˆ—å¾®åˆ†ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**: ã‚¹ã‚«ãƒ©ãƒ¼æå¤± $L$ ã‚’è¡Œåˆ— $W$ ã§å¾®åˆ†ã™ã‚‹ã¨ã€åŒã˜å½¢ã®è¡Œåˆ—ãŒè¿”ã£ã¦ãã‚‹:

$$
\frac{\partial L}{\partial W} \in \mathbb{R}^{m \times n} \quad \text{if } W \in \mathbb{R}^{m \times n}
$$

åŸºæœ¬å…¬å¼ï¼ˆ**Numerator layout**ã€Matrix Cookbook[^9]è¨˜æ³•ï¼‰:

$$
\frac{\partial}{\partial W} \text{tr}(W^\top A) = A, \qquad \frac{\partial}{\partial W} \text{tr}(W^\top A W B) = AWB + A^\top W B^\top
$$

$$
\frac{\partial}{\partial \mathbf{w}} \|\mathbf{w}\|_2^2 = 2\mathbf{w}, \qquad \frac{\partial}{\partial \mathbf{w}} \|A\mathbf{w} - \mathbf{b}\|_2^2 = 2A^\top(A\mathbf{w} - \mathbf{b})
$$

ä»Šã®è‡ªåˆ†ãŒ $\partial L / \partial W$ ã‚’ã©ã†è¨ˆç®—ã™ã‚‹ã‹æƒ³åƒã—ãªãŒã‚‰æ¬¡å›ã‚’å¾…ã¨ã†ã€‚

**ã‚­ãƒ¼ã¨ãªã‚‹LLM/Transformeræ¥ç‚¹**:
- ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ â†’ Flow Modelï¼ˆç¬¬25å›ï¼‰
- å‹¾é… â†’ Backpropagation[^2]ï¼ˆç¬¬3å›ã§å®Œå…¨å°å‡ºï¼‰
- é€£é–å¾‹ â†’ Transformer ã®å„å±¤ã‚’é€šã˜ãŸå‹¾é…ä¼æ’­

> **ç¬¬2å›ã®é™ç•Œ**: è¡Œåˆ—ã‚’ã€Œæ‰±ãˆã‚‹ã€ã‚ˆã†ã«ãªã£ãŸã€‚ã ãŒã€Œåˆ†è§£ã—ã¦æ§‹é€ ã‚’è¦‹æŠœãã€ã«ã¯SVDãŒå¿…è¦ã€‚ã€Œè¡Œåˆ—ã®é–¢æ•°ã‚’å¾®åˆ†ã™ã‚‹ã€ã«ã¯è¡Œåˆ—å¾®åˆ†ãŒå¿…è¦ã€‚ãã®2ã¤ã‚’ç¬¬3å›ã§å®Œå…¨æ­¦è£…ã™ã‚‹ã€‚

> Progress: 100%

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. æ­£è¦ç›´äº¤è¡Œåˆ— $Q$ ãŒ $Q^\top Q = I$ ã‚’æº€ãŸã™ã¨ãã€$Q$ ã§å¤‰æ›ã—ã¦ã‚‚é•·ã•ãŒä¿ãŸã‚Œã‚‹ç†ç”±ã‚’å¹¾ä½•å­¦çš„ã«èª¬æ˜ã›ã‚ˆã€‚
> 2. ç‰¹ç•°å€¤åˆ†è§£ $A = U\Sigma V^\top$ ã® $\Sigma$ ã®å¯¾è§’æˆåˆ†ãŒéè² ã«ãªã‚‹ç†ç”±ã¯ä½•ã‹ã€‚


---

### 6.10 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **GPUã¯è¡Œåˆ—æ¼”ç®—ãƒã‚·ãƒ³ã€‚ç·šå½¢ä»£æ•°ã‚’"åˆ¶ã™ã‚‹è€…"ãŒAIã‚’åˆ¶ã™ã‚‹ã®ã§ã¯ï¼Ÿ**

ã“ã®å•ã„ã®æ„å‘³ã‚’è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚

ç¾ä»£ã®AIã®é€²æ­©ã¯ã€ä¸€è¦‹ã™ã‚‹ã¨ãƒ‡ãƒ¼ã‚¿é‡ã‚„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å¢—å¤§ã«ã‚ˆã‚‹ã‚‚ã®ã«è¦‹ãˆã‚‹ã€‚ã ãŒã€ãã®è£å´ã§èµ·ãã¦ã„ã‚‹ã“ã¨ã¯ã€Œã„ã‹ã«åŠ¹ç‡ã‚ˆãè¡Œåˆ—ç©ã‚’è¨ˆç®—ã™ã‚‹ã‹ã€ã®æœ€é©åŒ–ã ã€‚

Flash Attention[^12]ã¯ã€Attention ã®è¨ˆç®—ã‚’è¡Œåˆ—ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§å†æ§‹æˆã—ã¦ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’æœ€é©åŒ–ã—ãŸã€‚ã“ã‚Œã¯ç·šå½¢ä»£æ•°ã®çŸ¥è­˜ãªã—ã«ã¯ç™ºæƒ³ã§ããªã„ã€‚LoRA[^10]ã¯é‡ã¿è¡Œåˆ—ã®æ›´æ–°ã‚’ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®ç©ã§è¿‘ä¼¼ã—ãŸã€‚ã“ã‚Œã‚‚SVDçš„ãªç™ºæƒ³ã®ç›´æ¥çš„ãªå¿œç”¨ã ã€‚

è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚GPT-4ã®æ¨è«–ã¯ã€çµå±€ã®ã¨ã“ã‚ä½•ã‚’ã—ã¦ã„ã‚‹ã®ã‹ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ï¼ˆè¡Œåˆ—ã®è¡Œé¸æŠï¼‰ã€Queryã¨ Keyã®å†…ç©ã‚’è¨ˆç®—ã—ï¼ˆè¡Œåˆ—ç© $QK^\top$ï¼‰ã€Softmaxã§æ­£è¦åŒ–ã—ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ï¼‰ã€Valueã®åŠ é‡å’Œã‚’å–ã‚Šï¼ˆè¡Œåˆ—ç© $AV$ï¼‰ã€ç·šå½¢å°„å½±ã™ã‚‹ï¼ˆè¡Œåˆ—ç© $W_O$ï¼‰ã€‚**å…¨ã¦ãŒè¡Œåˆ—æ¼”ç®—ã ã€‚**

ã“ã®äº‹å®Ÿã¯ã€AIã®ç†è§£ã‚’æ ¹æœ¬ã‹ã‚‰å¤‰ãˆã‚‹ã€‚AIã¯ã€ŒçŸ¥èƒ½ã®æ¨¡å€£ã€ã§ã¯ãªãã€Œé«˜æ¬¡å…ƒç·šå½¢ä»£æ•°ã®å¤§è¦æ¨¡ä¸¦åˆ—å®Ÿè¡Œã€ã ã€‚ç·šå½¢ä»£æ•°ã®ç†è«–çš„é™ç•ŒãŒAIã®é™ç•Œã‚’è¦å®šã—ã€ç·šå½¢ä»£æ•°ã®è¨ˆç®—åŠ¹ç‡ãŒAIã®å®Ÿç”¨æ€§ã‚’æ±ºå®šã™ã‚‹ã€‚

<details><summary>è­°è«–ãƒã‚¤ãƒ³ãƒˆ</summary>

1. **ã‚‚ã—GPUãŒè¡Œåˆ—ç©ä»¥å¤–ã®è¨ˆç®—ã‚‚å¾—æ„ã ã£ãŸã‚‰ã€AIã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å¤‰ã‚ã£ã¦ã„ãŸã‹ï¼Ÿ** â€” TransformerãŒæ”¯é…çš„ã«ãªã£ãŸç†ç”±ã®ä¸€ã¤ã¯ã€ãã®ã‚³ã‚¢è¨ˆç®—ãŒè¡Œåˆ—ç©ã§ã‚ã‚Šã€GPUã¨ç›¸æ€§ãŒè‰¯ã„ã“ã¨ã«ã‚ã‚‹ã€‚RNNã¯é€æ¬¡çš„ãªè¨ˆç®—ãŒå¿…è¦ã§GPUã®ä¸¦åˆ—æ€§ã‚’æ´»ã‹ã—ãã‚Œãªã‹ã£ãŸã€‚
2. **ç·šå½¢ä»£æ•°ã®é™ç•Œã¯ã©ã“ã«ã‚ã‚‹ã‹ï¼Ÿ** â€” éç·šå½¢æ€§ï¼ˆæ´»æ€§åŒ–é–¢æ•°ï¼‰ãªã—ã«ã¯ä»»æ„ã®é–¢æ•°ã‚’è¿‘ä¼¼ã§ããªã„ã€‚ç·šå½¢ä»£æ•°ã¯ã€ŒåœŸå°ã€ã§ã‚ã£ã¦ã€Œå…¨ã¦ã€ã§ã¯ãªã„ã€‚ãŸã ã—ã€ReLU ã¯åŒºåˆ†ç·šå½¢é–¢æ•°ã§ã‚ã‚Šã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ã€ŒåŒºåˆ†çš„ã«ç·šå½¢ãªã€å†™åƒã ã€‚
3. **é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ç·šå½¢ä»£æ•°ã‚’åŠ é€Ÿã™ã‚‹ã‹ï¼Ÿ** â€” é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ç‰¹å®šã®ç·šå½¢ä»£æ•°æ¼”ç®—ï¼ˆHHL algorithmï¼‰ã§æŒ‡æ•°é–¢æ•°çš„ãªé«˜é€ŸåŒ–ã‚’é”æˆã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚é‡å­æ©Ÿæ¢°å­¦ç¿’ã®ç†è«–çš„åŸºç›¤ã‚‚ç·šå½¢ä»£æ•°ã ã€‚
4. **ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã¯ã©ã“ã¾ã§æœ‰åŠ¹ã‹ï¼Ÿ** â€” LoRA[^10]ã¯é‡ã¿æ›´æ–°ã‚’ rank-$r$ è¿‘ä¼¼ã™ã‚‹ã“ã¨ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ $O(d^2) \to O(dr)$ ã«å‰Šæ¸›ã—ãŸã€‚ã“ã‚Œã¯ã€Œé‡ã¿æ›´æ–°ãŒæœ¬è³ªçš„ã«ä½ãƒ©ãƒ³ã‚¯ã§ã‚ã‚‹ã€ã¨ã„ã†çµŒé¨“çš„ç™ºè¦‹ã«åŸºã¥ãã€‚ã ãŒã€ã“ã®ä»®å®šã¯å¸¸ã«æ­£ã—ã„ã®ã‹ï¼Ÿ ã©ã®ã‚¿ã‚¹ã‚¯ã§ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ãŒå¤±æ•—ã™ã‚‹ã‹ã¯ã€ã¾ã å®Œå…¨ã«ã¯ç†è§£ã•ã‚Œã¦ã„ãªã„ã€‚
</details>

---

> **ğŸ“– å‰ç·¨ã‚‚ã‚ã‚ã›ã¦ã”è¦§ãã ã•ã„**
> [ã€å‰ç·¨ã€‘ç¬¬2å›: ç·šå½¢ä»£æ•° I â€” ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»åŸºåº•](/articles/ml-lecture-02-part1) ã§ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ãƒ»å†…ç©ãƒ»å›ºæœ‰å€¤åˆ†è§£ãƒ»å°„å½±ã®ç†è«–ã‚’å­¦ã³ã¾ã—ãŸã€‚

---
## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). Attention Is All You Need. *NeurIPS 2017*.
<https://arxiv.org/abs/1706.03762>
[^2]: Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2018). Automatic differentiation in machine learning: a survey.
<https://arxiv.org/abs/1502.05767>
[^6]: Shlens, J. (2014). A Tutorial on Principal Component Analysis.
<https://arxiv.org/abs/1404.1100>
[^7]: Halko, N., Martinsson, P. G., & Tropp, J. A. (2011). Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions.
<https://arxiv.org/abs/0909.4061>
[^10]: Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. *ICLR 2022*.
<https://arxiv.org/abs/2106.09685>
[^12]: Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & RÃ©, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. *NeurIPS 2022*.
<https://arxiv.org/abs/2205.14135>
[^13]: Martinsson, P. G., & Tropp, J. A. (2020). Randomized numerical linear algebra: Foundations and algorithms. *Acta Numerica*, 29, 403-572.
<https://arxiv.org/abs/2002.01387>
[^14]: Halko, N., Martinsson, P. G., & Tropp, J. A. (2011). Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. *SIAM Review*, 53(2), 217-288. arXiv:0909.4061.

[^15]: Ringoot, E., Alomairy, R., Churavy, V., & Edelman, A. (2025). Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision.
<https://arxiv.org/abs/2508.06339>
[^16]: Liu, S., Li, H., et al. (2025). Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method.
<https://arxiv.org/abs/2508.11467>
[^17]: Feng, Y., Xiang, H., & Saad, Y. (2022). Randomized Rank-Revealing QLP for Low-Rank Matrix Approximation.
<https://arxiv.org/abs/2209.12464>
[^18]: Nenov, R., Haider, D., & Balazs, P. (2024). (Almost) Smooth Sailing: Towards Numerical Stability of Neural Networks.
<https://arxiv.org/abs/2410.00169>
[^19]: Zhao, Y., Anandkumar, A., & Yu, Y. (2020). An efficient numerical method for condition number constrained covariance matrix approximation.
<https://arxiv.org/abs/2008.06851>

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
