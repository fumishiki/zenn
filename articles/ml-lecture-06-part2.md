---
title: "ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ“¡"
type: "tech"
topics: ["machinelearning", "deeplearning", "informationtheory", "python"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” æ•°å¼ã‚’ã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã™ã‚‹æŠ€è¡“

### 4.1 ç’°å¢ƒæ§‹ç¯‰

```bash
pip install numpy matplotlib
```

æœ¬è¬›ç¾©ã¯ Python 90% ã§é€²ã‚€ã€‚NumPy ã®ã¿ã§å…¨ã¦å®Ÿè£…ã™ã‚‹ã€‚PyTorch ã¯ `:::details` ã§æ¯”è¼ƒç”¨ã«ç¤ºã™ã€‚

### 4.2 æƒ…å ±ç†è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…

Zone 3 ã§å°å‡ºã—ãŸå…¨ã¦ã®æƒ…å ±é‡ã‚’ã€1ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦å®Ÿè£…ã™ã‚‹ã€‚

```python
import numpy as np
from typing import Optional

class InfoTheory:
    """Information theory toolkit â€” all formulas from Zone 3."""

    @staticmethod
    def entropy(p: np.ndarray, base: float = np.e) -> float:
        """Shannon entropy: H(X) = -Î£ p(x) log p(x)

        Definition 3.1. Returns in nats (base=e) or bits (base=2).
        """
        p = np.asarray(p, dtype=np.float64)
        p = p[p > 0]
        return -np.sum(p * np.log(p)) / np.log(base)

    @staticmethod
    def cross_entropy(p: np.ndarray, q: np.ndarray, base: float = np.e) -> float:
        """Cross-entropy: H(p, q) = -Î£ p(x) log q(x)

        Definition 3.4. H(p,q) = H(p) + KL(p||q).
        """
        p, q = np.asarray(p, dtype=np.float64), np.asarray(q, dtype=np.float64)
        mask = p > 0
        return -np.sum(p[mask] * np.log(q[mask])) / np.log(base)

    @staticmethod
    def kl_divergence(p: np.ndarray, q: np.ndarray) -> float:
        """KL divergence: D_KL(p || q) = Î£ p(x) log(p(x)/q(x))

        Definition 3.3. Always >= 0 (Gibbs inequality, Theorem 3.2).
        """
        p, q = np.asarray(p, dtype=np.float64), np.asarray(q, dtype=np.float64)
        mask = (p > 0) & (q > 0)
        return np.sum(p[mask] * np.log(p[mask] / q[mask]))

    @staticmethod
    def mutual_information(pxy: np.ndarray) -> float:
        """Mutual information: I(X;Y) = H(X) + H(Y) - H(X,Y)

        Definition 3.6. I(X;Y) = KL(p(x,y) || p(x)p(y)).
        """
        pxy = np.asarray(pxy, dtype=np.float64)
        px = pxy.sum(axis=1)
        py = pxy.sum(axis=0)
        h_x = InfoTheory.entropy(px)
        h_y = InfoTheory.entropy(py)
        h_xy = InfoTheory.entropy(pxy.flatten())
        return h_x + h_y - h_xy

    @staticmethod
    def js_divergence(p: np.ndarray, q: np.ndarray) -> float:
        """Jensen-Shannon divergence: JSD(p||q) = 0.5*KL(p||m) + 0.5*KL(q||m)

        where m = 0.5*(p+q). Symmetric, bounded [0, log2].
        """
        m = 0.5 * (p + q)
        return 0.5 * InfoTheory.kl_divergence(p, m) + 0.5 * InfoTheory.kl_divergence(q, m)

    @staticmethod
    def perplexity(p: np.ndarray, q: np.ndarray) -> float:
        """Perplexity: PPL = exp(H(p, q)) = 2^(H(p,q) in bits)

        Zone 0 connection: PPL = average branching factor.
        """
        ce = InfoTheory.cross_entropy(p, q)
        return np.exp(ce)

# Verification: all Zone 3 formulas
it = InfoTheory()
p = np.array([0.4, 0.3, 0.2, 0.1])
q = np.array([0.25, 0.25, 0.25, 0.25])

print("=== Information Theory Verification ===")
print(f"H(p)          = {it.entropy(p):.6f} nats")
print(f"H(p,q)        = {it.cross_entropy(p, q):.6f} nats")
print(f"KL(p||q)      = {it.kl_divergence(p, q):.6f} nats")
print(f"H(p) + KL     = {it.entropy(p) + it.kl_divergence(p, q):.6f} nats")
print(f"Match CE:       {np.isclose(it.entropy(p) + it.kl_divergence(p, q), it.cross_entropy(p, q))}")
print(f"JSD(p,q)      = {it.js_divergence(p, q):.6f} nats")
print(f"PPL(p,q)      = {it.perplexity(p, q):.4f}")
print(f"\nKL non-negativity: KL(p||q) = {it.kl_divergence(p, q):.6f} >= 0 âœ“")
print(f"KL(p||p) = {it.kl_divergence(p, p):.10f} â‰ˆ 0 âœ“")
```

### 4.3 æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®çµ±ä¸€å®Ÿè£…

SGDã€Momentumã€Adamã€AdamW ã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§å®Ÿè£…ã™ã‚‹ã€‚

```python
import numpy as np
from abc import ABC, abstractmethod

class Optimizer(ABC):
    """Base optimizer interface."""

    def __init__(self, lr: float = 0.001):
        self.lr = lr
        self.t = 0

    @abstractmethod
    def step(self, params: np.ndarray, grads: np.ndarray) -> np.ndarray:
        pass

class SGD(Optimizer):
    """Vanilla SGD: Î¸ = Î¸ - Î·âˆ‡L (Robbins & Monro, 1951)"""

    def step(self, params, grads):
        self.t += 1
        return params - self.lr * grads

class MomentumSGD(Optimizer):
    """SGD with Momentum (Polyak, 1964): v = Î²v + g; Î¸ = Î¸ - Î·v"""

    def __init__(self, lr=0.001, beta=0.9):
        super().__init__(lr)
        self.beta = beta
        self.v = None

    def step(self, params, grads):
        self.t += 1
        if self.v is None:
            self.v = np.zeros_like(params)
        self.v = self.beta * self.v + grads
        return params - self.lr * self.v

class AdamOptimizer(Optimizer):
    """Adam (Kingma & Ba, 2014)"""

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        super().__init__(lr)
        self.beta1, self.beta2, self.eps = beta1, beta2, eps
        self.m = None
        self.v = None

    def step(self, params, grads):
        self.t += 1
        if self.m is None:
            self.m = np.zeros_like(params)
            self.v = np.zeros_like(params)
        self.m = self.beta1 * self.m + (1 - self.beta1) * grads
        self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2
        m_hat = self.m / (1 - self.beta1**self.t)
        v_hat = self.v / (1 - self.beta2**self.t)
        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)

class AdamWOptimizer(AdamOptimizer):
    """AdamW (Loshchilov & Hutter, 2017): decoupled weight decay"""

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, wd=0.01):
        super().__init__(lr, beta1, beta2, eps)
        self.wd = wd

    def step(self, params, grads):
        params = params * (1 - self.lr * self.wd)  # decoupled weight decay
        return super().step(params, grads)
```

### 4.4 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ7ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

| # | æ•°å¼ãƒ‘ã‚¿ãƒ¼ãƒ³ | Python ãƒ‘ã‚¿ãƒ¼ãƒ³ | ä¾‹ |
|:--|:-----------|:--------------|:---|
| 1 | $\sum_{x} p(x) f(x)$ | `np.sum(p * f(x))` | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ |
| 2 | $\log \frac{a}{b}$ | `np.log(a / b)` or `np.log(a) - np.log(b)` | KL |
| 3 | $\mathbb{E}_{x \sim p}[f(x)]$ | `np.mean(f(samples))` | Monte Carlo æ¨å®š |
| 4 | $\frac{\partial}{\partial \theta} f$ | æ•°å€¤å¾®åˆ†: `(f(Î¸+Îµ) - f(Î¸-Îµ))/(2Îµ)` | å‹¾é…æ¤œè¨¼ |
| 5 | $\beta v + (1-\beta) g$ | `v = beta * v + (1-beta) * g` | æŒ‡æ•°ç§»å‹•å¹³å‡ |
| 6 | $\frac{m}{1 - \beta^t}$ | `m / (1 - beta**t)` | ãƒã‚¤ã‚¢ã‚¹è£œæ­£ |
| 7 | $\frac{a}{\sqrt{b} + \epsilon}$ | `a / (np.sqrt(b) + eps)` | Adam æ›´æ–° |

:::details PyTorch ã¨ã®å¯¾å¿œ
```python
import torch
import torch.nn.functional as F

# Pattern 1: Entropy
p_pt = torch.tensor([0.4, 0.3, 0.2, 0.1])
entropy_pt = -torch.sum(p_pt * torch.log(p_pt))

# Pattern 2: KL divergence
q_pt = torch.tensor([0.25, 0.25, 0.25, 0.25])
kl_pt = F.kl_div(q_pt.log(), p_pt, reduction='sum')
# NOTE: PyTorch's kl_div expects log(q) as first arg, p as second

# Pattern 3: Cross-Entropy Loss
logits = torch.randn(1, 100)  # model output
target = torch.tensor([42])   # correct token
ce_pt = F.cross_entropy(logits, target)

# Optimizers
model = torch.nn.Linear(10, 10)
opt_sgd = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
opt_adam = torch.optim.Adam(model.parameters(), lr=0.001)
opt_adamw = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
```
:::

### 4.5 æœ€é©åŒ–ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã®å¯è¦–åŒ–

```python
import numpy as np

def visualize_optimization_landscape():
    """Compare optimizer trajectories on Beale's function."""
    # Beale function: f(x,y) = (1.5-x+xy)^2 + (2.25-x+xy^2)^2 + (2.625-x+xy^3)^2
    def beale(xy):
        x, y = xy[0], xy[1]
        return ((1.5 - x + x*y)**2 +
                (2.25 - x + x*y**2)**2 +
                (2.625 - x + x*y**3)**2)

    def beale_grad(xy):
        x, y = xy[0], xy[1]
        t1 = 1.5 - x + x*y
        t2 = 2.25 - x + x*y**2
        t3 = 2.625 - x + x*y**3
        dx = 2*t1*(-1+y) + 2*t2*(-1+y**2) + 2*t3*(-1+y**3)
        dy = 2*t1*x + 2*t2*2*x*y + 2*t3*3*x*y**2
        return np.array([dx, dy])

    x0 = np.array([0.0, 0.0])
    optimizers = {
        "SGD(lr=0.0001)": SGD(lr=0.0001),
        "Momentum(Î²=0.9)": MomentumSGD(lr=0.0001, beta=0.9),
        "Adam(lr=0.01)": AdamOptimizer(lr=0.01),
    }

    print("=== Optimizer Comparison on Beale Function ===")
    print(f"Optimal: (3.0, 0.5), f* = 0.0\n")

    for name, opt in optimizers.items():
        x = x0.copy()
        for step in range(2000):
            g = beale_grad(x)
            # Clip gradient to prevent divergence
            g = np.clip(g, -10, 10)
            x = opt.step(x, g)

        print(f"{name:25s}: x=({x[0]:+.4f}, {x[1]:+.4f}), "
              f"f={beale(x):.6f}, dist={np.linalg.norm(x - np.array([3,0.5])):.4f}")

visualize_optimization_landscape()
```

### 4.6 Python ã®é…ã•ã‚’ä½“æ„Ÿã™ã‚‹ â€” `%timeit` ã®è¡æ’ƒ

ã“ã“ã§ä¸ç©ãªè¨ˆæ¸¬ã‚’è¡Œã†ã€‚

```python
import numpy as np
import time

def train_loop_python(n_params, n_steps):
    """Pure Python training loop (SGD + gradient computation)."""
    params = np.random.randn(n_params)
    lr = 0.001
    loss_history = []

    start = time.perf_counter()
    for step in range(n_steps):
        # Simulate gradient computation (quadratic loss)
        grad = 2.0 * params + np.random.randn(n_params) * 0.01
        # SGD update
        params = params - lr * grad
        # Compute loss
        loss = float(np.sum(params**2))
        loss_history.append(loss)
    elapsed = time.perf_counter() - start
    return elapsed, loss_history

# Benchmark
for n_params in [100, 1000, 10000]:
    elapsed, losses = train_loop_python(n_params, 1000)
    print(f"Params={n_params:6d}, Steps=1000: {elapsed:.3f}s "
          f"({elapsed/1000*1e6:.0f} us/step) "
          f"final_loss={losses[-1]:.6f}")

print("\n--- Consider ---")
print("GPT-3: 175B parameters, ~300B tokens")
print("Each step: forward + backward + update on 175B params")
print("Pure Python? Impossible. Even NumPy is not enough.")
print("This is why we need Julia (Lec 9) and Rust (Lec 11).")
```

:::message alert
ã“ã“ã§ `%timeit` ã®çµæœã‚’è¦³å¯Ÿã—ã¦ã»ã—ã„ã€‚10,000ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã® SGD ãƒ«ãƒ¼ãƒ—ãŒ Python ã§ã©ã‚Œã ã‘é…ã„ã‹ã€‚å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã¯æ•°ç™¾ä¸‡ã€œæ•°åå„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã€‚ã“ã®ã€Œé…ã•ã€ã¯ç¬¬7å›ã§ MLE ã®åå¾©è¨ˆç®—ã§å¢—å¹…ã—ã€ç¬¬8å›ã® EM ç®—æ³•ã§ã€Œ**é…ã™ããªã„ï¼Ÿ**ã€ã¨ã„ã†å•ã„ãŒç¢ºä¿¡ã«å¤‰ã‚ã‚‹ã€‚ç¬¬9å›ã§ Julia ãŒç™»å ´ã™ã‚‹ä¼ç·šãŒã“ã“ã«ã‚ã‚‹ã€‚
:::

### 4.7 å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã¨æ•°å€¤å®‰å®šæ€§

å®Ÿéš›ã®è¨“ç·´ã§ã¯å‹¾é…ãŒçˆ†ç™ºã™ã‚‹å•é¡Œã«å¯¾å‡¦ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

```python
import numpy as np

def gradient_clipping(grads: np.ndarray, max_norm: float = 1.0) -> np.ndarray:
    """Gradient clipping by global norm.

    If ||g|| > max_norm, scale g so that ||g|| = max_norm.
    Used in LLM training to prevent gradient explosion.
    """
    grad_norm = np.linalg.norm(grads)
    if grad_norm > max_norm:
        grads = grads * (max_norm / grad_norm)
    return grads

# Demonstrate gradient clipping
np.random.seed(42)
normal_grads = np.random.randn(10) * 0.5
exploding_grads = np.random.randn(10) * 100

print("Normal gradients:")
print(f"  Before: norm = {np.linalg.norm(normal_grads):.4f}")
clipped = gradient_clipping(normal_grads, max_norm=1.0)
print(f"  After:  norm = {np.linalg.norm(clipped):.4f}")

print("\nExploding gradients:")
print(f"  Before: norm = {np.linalg.norm(exploding_grads):.4f}")
clipped = gradient_clipping(exploding_grads, max_norm=1.0)
print(f"  After:  norm = {np.linalg.norm(clipped):.4f}")
print(f"  Scale factor: {1.0 / np.linalg.norm(exploding_grads):.6f}")
```

**å‹¾é…çˆ†ç™ºã®åŸå› **: æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã¯é€†ä¼æ’­ä¸­ã«å‹¾é…ãŒä¹—ç®—ã•ã‚Œã¦ã„ãã€‚å„å±¤ã®å‹¾é…ãŒ1ã‚ˆã‚Šå¤§ãã„ã¨æŒ‡æ•°çš„ã«å¢—å¤§ï¼ˆçˆ†ç™ºï¼‰ã€1ã‚ˆã‚Šå°ã•ã„ã¨æŒ‡æ•°çš„ã«æ¸›å°‘ï¼ˆæ¶ˆå¤±ï¼‰ã™ã‚‹ã€‚ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã¯çˆ†ç™ºã‚’é˜²ãå¯¾ç—‡ç™‚æ³•ã§ã‚ã‚Šã€æ¶ˆå¤±ã«ã¯åˆ¥ã®å¯¾ç­–ï¼ˆæ®‹å·®æ¥ç¶šã€æ­£è¦åŒ–ï¼‰ãŒå¿…è¦ã€‚

**æ··åˆç²¾åº¦è¨“ç·´ã®æ¦‚è¦ï¼ˆfp16/bf16/fp8ï¼‰**:

| ç²¾åº¦ | ãƒ“ãƒƒãƒˆæ•° | ç¯„å›² | ç”¨é€” |
|:-----|:---------|:-----|:-----|
| fp32 | 32 | $\pm 3.4 \times 10^{38}$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜ï¼ˆãƒã‚¹ã‚¿ãƒ¼ã‚³ãƒ”ãƒ¼ï¼‰ |
| fp16 | 16 | $\pm 65504$ | é †ä¼æ’­ãƒ»é€†ä¼æ’­ã®é«˜é€ŸåŒ– |
| bf16 | 16 | $\pm 3.4 \times 10^{38}$ | fp32åŒæ§˜ã®ç¯„å›²ã€ç²¾åº¦ã¯ä½ã„ |
| fp8 | 8 | é™å®š | Transformer Engine (H100+) |

æ··åˆç²¾åº¦è¨“ç·´ã¯ fp32 ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒã‚¹ã‚¿ãƒ¼ã‚³ãƒ”ãƒ¼ã‚’ä¿æŒã—ã¤ã¤ã€é †ä¼æ’­ã¨é€†ä¼æ’­ã‚’ fp16/bf16 ã§è¡Œã†ã€‚è¨ˆç®—é€Ÿåº¦ãŒ2-3å€ã«ãªã‚Šã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåŠæ¸›ã™ã‚‹ã€‚Loss scalingï¼ˆæå¤±ã«å¤§ããªå®šæ•°ã‚’æ›ã‘ã¦ã‹ã‚‰é€†ä¼æ’­ã—ã€å‹¾é…æ›´æ–°æ™‚ã«æˆ»ã™ï¼‰ã§ fp16 ã®ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼å•é¡Œã‚’å›é¿ã™ã‚‹ã€‚

### 4.8 ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³• â€” åˆ¶ç´„ä»˜ãæœ€é©åŒ–

æ©Ÿæ¢°å­¦ç¿’ã§é »å‡ºã™ã‚‹åˆ¶ç´„ä»˜ãæœ€é©åŒ–ã®åŸºæœ¬ã‚’æŠ¼ã•ãˆã‚‹ã€‚

**å•é¡Œè¨­å®š**: $g(\theta) = 0$ ã®åˆ¶ç´„ä¸‹ã§ $f(\theta)$ ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

$$
\min_\theta f(\theta) \quad \text{s.t.} \quad g(\theta) = 0
$$

**ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ã‚¢ãƒ³**:

$$
\mathcal{L}(\theta, \lambda) = f(\theta) + \lambda g(\theta)
$$

**æœ€é©æ€§æ¡ä»¶ï¼ˆKKTæ¡ä»¶ã®ç­‰å¼åˆ¶ç´„ç‰ˆï¼‰**:

$$
\nabla_\theta \mathcal{L} = 0, \quad \nabla_\lambda \mathcal{L} = 0
$$

```python
import numpy as np

def lagrangian_example():
    """Example: Maximize entropy H(p) subject to Î£p_i = 1, Î£p_i x_i = mu

    This shows why maximum entropy distribution is exponential family.
    """
    # Maximum entropy for a discrete distribution with mean constraint
    # Result: p(x) = exp(lambda_0 + lambda_1 * x) / Z
    # For mean=3.5 on {1,2,3,4,5,6} (fair die): uniform distribution

    x = np.arange(1, 7, dtype=float)

    # Uniform (maximum entropy with just normalization constraint)
    p_uniform = np.ones(6) / 6
    h_uniform = -np.sum(p_uniform * np.log2(p_uniform))

    # Constrained to mean=2.5 (biased toward lower numbers)
    # Solve via Lagrange multiplier (numerical approximation)
    best_h, best_p = -np.inf, None
    for lam in np.linspace(-2, 2, 10000):
        p = np.exp(lam * x)
        p /= p.sum()
        mean = np.sum(p * x)
        if abs(mean - 2.5) < 0.01:
            h = -np.sum(p * np.log2(p))
            if h > best_h:
                best_h, best_p = h, p.copy()

    print(f"Uniform (mean=3.5): H = {h_uniform:.4f} bits")
    if best_p is not None:
        print(f"MaxEnt (mean=2.5):  H = {best_h:.4f} bits")
        print(f"  p = {np.round(best_p, 4)}")
        print(f"  mean = {np.sum(best_p * x):.4f}")

lagrangian_example()
```

ã“ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åŸç†ã¯çµ±è¨ˆç‰©ç†ã® Boltzmann åˆ†å¸ƒã¨åŒä¸€ã§ã‚ã‚Šã€ç¬¬27å›ï¼ˆEBMï¼‰ã§å†ç™»å ´ã™ã‚‹ã€‚åˆ¶ç´„ãªã—ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = ä¸€æ§˜åˆ†å¸ƒã€å¹³å‡ã®åˆ¶ç´„ã¤ã = æŒ‡æ•°åˆ†å¸ƒæ—ã€‚**æƒ…å ±ç†è«–ã¨çµ±è¨ˆç‰©ç†ã¯åŒã˜æ•°å­¦ã§ç¹‹ãŒã£ã¦ã„ã‚‹**ã€‚

### 4.9 è«–æ–‡èª­è§£ã®æƒ…å ±ç†è«–çš„è¦–ç‚¹

æƒ…å ±ç†è«–ã®é“å…·ã‚’ä½¿ã£ã¦è«–æ–‡ã‚’èª­ã‚€éš›ã®è¦–ç‚¹ã‚’æ•´ç†ã™ã‚‹ã€‚

**3ãƒ‘ã‚¹ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæƒ…å ±ç†è«–ç‰ˆï¼‰**:

```mermaid
graph TD
    P1["Pass 1: Survey<br/>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›<br/>H(ç†è§£) ã‚’å¤§ããä¸‹ã’ã‚‹"] --> P2["Pass 2: Grasp<br/>ç›¸äº’æƒ…å ±é‡<br/>å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã® I(X;Y) ã‚’æ´ã‚€"]
    P2 --> P3["Pass 3: Verify<br/>KL æœ€å°åŒ–<br/>è‡ªåˆ†ã®ç†è§£ã¨è‘—è€…ã®æ„å›³ã® KL ã‚’ 0 ã«"]
```

| ãƒ‘ã‚¹ | ç›®çš„ | æ‰€è¦æ™‚é–“ | èª­ã‚€ç®‡æ‰€ |
|:-----|:-----|:---------|:---------|
| 1 | å…¨ä½“åƒã®æŠŠæ¡ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¤§å¹…å‰Šæ¸›ï¼‰ | 10åˆ† | Title, Abstract, Conclusion, Figures |
| 2 | æ§‹é€ ã®ç†è§£ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ä¾å­˜é–¢ä¿‚ï¼‰ | 30åˆ† | Method, Results, key equations |
| 3 | è©³ç´°ã®æ¤œè¨¼ï¼ˆè‡ªåˆ†ã®ç†è§£ã® KL â†’ 0ï¼‰ | 60åˆ† | å…¨ãƒšãƒ¼ã‚¸ã€å¼ã®å°å‡ºè¿½è·¡ |

:::message
**é€²æ—: 70% å®Œäº†** æƒ…å ±ç†è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã—ã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã—ãŸã€‚Python ã®é…ã•ã‚‚ä½“æ„Ÿã—ãŸã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®è¨˜å·ãƒ»æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

:::details Q1: $H(X) = -\sum_{x} p(x) \log p(x)$
**èª­ã¿æ–¹**: ã‚¨ã‚¤ãƒ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ ã‚·ã‚°ãƒ ã‚¨ãƒƒã‚¯ã‚¹ ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹

**æ„å‘³**: ç¢ºç‡å¤‰æ•° $X$ ã® Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€‚ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ã€‚$p(x)$ ãŒä¸€æ§˜åˆ†å¸ƒã®ã¨ãæœ€å¤§ã€ç¢ºå®šçš„ã®ã¨ã 0ã€‚Shannon (1948) [^1] ãŒæƒ…å ±ç†è«–ã®åŸºç¤ã¨ã—ã¦å®šç¾©ã€‚
:::

:::details Q2: $D_\text{KL}(p \| q) = \mathbb{E}_{p}\left[\log \frac{p(x)}{q(x)}\right]$
**èª­ã¿æ–¹**: ãƒ‡ã‚£ãƒ¼ ã‚±ãƒ¼ã‚¨ãƒ« ãƒ”ãƒ¼ ãƒ‘ãƒ©ãƒ¬ãƒ« ã‚­ãƒ¥ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¤ãƒ¼ ã‚µãƒ– ãƒ”ãƒ¼ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ã‚­ãƒ¥ãƒ¼

**æ„å‘³**: $p$ ã‹ã‚‰ $q$ ã¸ã® KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€‚$p$ ã§ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ $q$ ã§ç¬¦å·åŒ–ã—ãŸã¨ãã®ä½™åˆ†ãªæƒ…å ±é‡ã€‚éå¯¾ç§°ï¼ˆ$D_\text{KL}(p\|q) \neq D_\text{KL}(q\|p)$ï¼‰ã€‚Kullback & Leibler (1951) [^2]ã€‚
:::

:::details Q3: $H(p, q) = H(p) + D_\text{KL}(p \| q)$
**èª­ã¿æ–¹**: ã‚¨ã‚¤ãƒ ãƒ”ãƒ¼ ã‚­ãƒ¥ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ã‚¤ãƒ ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ‡ã‚£ãƒ¼ ã‚±ãƒ¼ã‚¨ãƒ« ãƒ”ãƒ¼ ãƒ‘ãƒ©ãƒ¬ãƒ« ã‚­ãƒ¥ãƒ¼

**æ„å‘³**: Cross-Entropy ã®åˆ†è§£å®šç†ã€‚Cross-Entropy = ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ + ãƒ¢ãƒ‡ãƒ«ã®ä¸å®Œå…¨æ€§ã€‚LLM è¨“ç·´ã§ Cross-Entropy ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¯ KL ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¨ç­‰ä¾¡ã€‚
:::

:::details Q4: $I(X; Y) = H(X) - H(X \mid Y)$
**èª­ã¿æ–¹**: ã‚¢ã‚¤ ã‚¨ãƒƒã‚¯ã‚¹ ã‚»ãƒŸã‚³ãƒ­ãƒ³ ãƒ¯ã‚¤ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ã‚¤ãƒ ã‚¨ãƒƒã‚¯ã‚¹ ãƒã‚¤ãƒŠã‚¹ ã‚¨ã‚¤ãƒ ã‚¨ãƒƒã‚¯ã‚¹ ãƒãƒ¼ ãƒ¯ã‚¤

**æ„å‘³**: $X$ ã¨ $Y$ ã®ç›¸äº’æƒ…å ±é‡ã€‚$Y$ ã‚’çŸ¥ã‚‹ã“ã¨ã§ $X$ ã®ä¸ç¢ºå®Ÿæ€§ãŒã©ã‚Œã ã‘æ¸›ã‚‹ã‹ã€‚è¡¨ç¾å­¦ç¿’ã§å…¥åŠ›ã¨æ½œåœ¨è¡¨ç¾ã®ä¾å­˜é–¢ä¿‚ã‚’æ¸¬ã‚‹ã€‚
:::

:::details Q5: $\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)$
**èª­ã¿æ–¹**: ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ãƒã‚¤ãƒŠã‚¹ ã‚¤ãƒ¼ã‚¿ ãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ ã‚¨ãƒ« ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼

**æ„å‘³**: å‹¾é…é™ä¸‹æ³•ã®æ›´æ–°å‰‡ã€‚å­¦ç¿’ç‡ $\eta$ ã§å‹¾é…æ–¹å‘ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã€‚Robbins & Monro (1951) [^3] ã«é¡ã‚‹ã€‚
:::

:::details Q6: $\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$
**èª­ã¿æ–¹**: ã‚¨ãƒ  ãƒãƒƒãƒˆ ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒ  ãƒ†ã‚£ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ™ãƒ¼ã‚¿ ãƒ¯ãƒ³ ãƒ ãƒ†ã‚£ãƒ¼ä¹—

**æ„å‘³**: Adam ã®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã€‚åˆæœŸåŒ– $m_0 = 0$ ã‹ã‚‰ã®ãƒã‚¤ã‚¢ã‚¹ã‚’ $1 - \beta_1^t$ ã§è£œæ­£ã€‚$t$ ãŒå¤§ãããªã‚‹ã¨è£œæ­£é‡ã¯æ¸›å°‘ã€‚Kingma & Ba (2014) [^4]ã€‚
:::

:::details Q7: $D_f(p \| q) = \sum_x q(x) f\left(\frac{p(x)}{q(x)}\right)$
**èª­ã¿æ–¹**: ãƒ‡ã‚£ãƒ¼ ã‚¨ãƒ• ãƒ”ãƒ¼ ãƒ‘ãƒ©ãƒ¬ãƒ« ã‚­ãƒ¥ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚·ã‚°ãƒ ã‚­ãƒ¥ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¨ãƒ• ãƒ”ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ã‚­ãƒ¥ãƒ¼

**æ„å‘³**: f-Divergenceã€‚ç”Ÿæˆé–¢æ•° $f$ ã‚’å¤‰ãˆã‚‹ã“ã¨ã§ KLã€$\chi^2$ã€Hellingerã€TVã€JS ãªã©ã‚’çµ±ä¸€çš„ã«è¡¨ç¾ã€‚Csiszar (1967) [^6]ã€‚
:::

:::details Q8: $\text{PPL} = 2^{H(p, q)}$ï¼ˆãŸã ã— $H$ ã¯ bitsï¼‰
**èª­ã¿æ–¹**: ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ‹ ãƒ ã‚¨ã‚¤ãƒ ãƒ”ãƒ¼ ã‚­ãƒ¥ãƒ¼ ä¹—

**æ„å‘³**: Perplexityã€‚ãƒ¢ãƒ‡ãƒ«ãŒå„æ™‚ç‚¹ã§å¹³å‡ä½•å€‹ã®é¸æŠè‚¢ã«è¿·ã£ã¦ã„ã‚‹ã‹ã€‚$H(p, q)$ ãŒå°ã•ã„ã»ã© PPL ãŒä½ãã€ã‚ˆã„äºˆæ¸¬ã€‚LLM è©•ä¾¡ã®æ¨™æº–æŒ‡æ¨™ã€‚
:::

:::details Q9: $v_{t+1} = \beta v_t + \nabla_\theta \mathcal{L}(\theta_t)$
**èª­ã¿æ–¹**: ãƒ–ã‚¤ ãƒ†ã‚£ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ™ãƒ¼ã‚¿ ãƒ–ã‚¤ ãƒ†ã‚£ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ ã‚¨ãƒ« ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼

**æ„å‘³**: Momentum ã®é€Ÿåº¦æ›´æ–°ã€‚éå»ã®å‹¾é…ã®æŒ‡æ•°ç§»å‹•å¹³å‡ã€‚$\beta = 0.9$ ãªã‚‰éå»10ã‚¹ãƒ†ãƒƒãƒ—ã®å‹¾é…ãŒå½±éŸ¿ã€‚Polyak (1964) [^8]ã€‚
:::

:::details Q10: $\eta_t = \eta_\text{min} + \frac{1}{2}(\eta_\text{max} - \eta_\text{min})(1 + \cos(\pi t / T))$
**èª­ã¿æ–¹**: ã‚¤ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¤ãƒ¼ã‚¿ ãƒŸãƒ³ ãƒ—ãƒ©ã‚¹ äºŒåˆ†ã®ä¸€ ã‚¤ãƒ¼ã‚¿ ãƒãƒƒã‚¯ã‚¹ ãƒã‚¤ãƒŠã‚¹ ã‚¤ãƒ¼ã‚¿ ãƒŸãƒ³ ã‚«ãƒƒã‚³ ãƒ¯ãƒ³ ãƒ—ãƒ©ã‚¹ ã‚³ã‚µã‚¤ãƒ³ ãƒ‘ã‚¤ ãƒ†ã‚£ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼

**æ„å‘³**: Cosine Annealing å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚ã‚³ã‚µã‚¤ãƒ³æ›²ç·šã«æ²¿ã£ã¦å­¦ç¿’ç‡ã‚’ $\eta_\text{max}$ ã‹ã‚‰ $\eta_\text{min}$ ã«æ¸›è¡°ã€‚
:::

### 5.2 LaTeX ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’ LaTeX ã§æ›¸ã‘ã€‚

:::details Q1: Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å®šç¾©
```latex
H(X) = -\sum_{x \in \mathcal{X}} p(x) \log p(x)
```
$$H(X) = -\sum_{x \in \mathcal{X}} p(x) \log p(x)$$
:::

:::details Q2: KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®éè² æ€§
```latex
D_\text{KL}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)} \geq 0
```
$$D_\text{KL}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)} \geq 0$$
:::

:::details Q3: Adam ã®æ›´æ–°å‰‡ï¼ˆãƒã‚¤ã‚¢ã‚¹è£œæ­£è¾¼ã¿ï¼‰
```latex
\theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}, \quad
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
```
:::

:::details Q4: Cross-Entropy ã®åˆ†è§£
```latex
H(p, q) = H(p) + D_\text{KL}(p \| q)
```
$$H(p, q) = H(p) + D_\text{KL}(p \| q)$$
:::

:::details Q5: f-Divergence ã®å®šç¾©
```latex
D_f(p \| q) = \sum_x q(x) f\left(\frac{p(x)}{q(x)}\right)
```
$$D_f(p \| q) = \sum_x q(x) f\left(\frac{p(x)}{q(x)}\right)$$
:::

### 5.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

æ•°å¼ã‚’ Python ã«ç¿»è¨³ã›ã‚ˆã€‚

:::details Q1: æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $H(Y \mid X) = -\sum_{x,y} p(x,y) \log p(y \mid x)$
```python
def conditional_entropy(pxy: np.ndarray) -> float:
    """H(Y|X) = -Î£ p(x,y) log p(y|x)"""
    px = pxy.sum(axis=1, keepdims=True)
    # p(y|x) = p(x,y) / p(x)
    p_y_given_x = np.where(px > 0, pxy / px, 0)
    mask = (pxy > 0) & (p_y_given_x > 0)
    return -np.sum(pxy[mask] * np.log(p_y_given_x[mask]))
```
:::

:::details Q2: Nesterov Momentum ã®æ›´æ–°
```python
def nesterov_step(params, velocity, grad_fn, lr, beta):
    """Nesterov accelerated gradient: look-ahead gradient."""
    look_ahead = params - lr * beta * velocity
    g = grad_fn(look_ahead)
    velocity_new = beta * velocity + g
    params_new = params - lr * velocity_new
    return params_new, velocity_new
```
:::

:::details Q3: ç›¸äº’æƒ…å ±é‡ã‚’ KL ã¨ã—ã¦è¨ˆç®—
```python
def mi_via_kl(pxy: np.ndarray) -> float:
    """I(X;Y) = KL(p(x,y) || p(x)p(y))"""
    px = pxy.sum(axis=1)
    py = pxy.sum(axis=0)
    independent = np.outer(px, py)
    mask = (pxy > 0) & (independent > 0)
    return np.sum(pxy[mask] * np.log(pxy[mask] / independent[mask]))
```
:::

:::details Q4: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¾®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
```python
def gaussian_entropy(sigma: float) -> float:
    """h(X) = 0.5 * log(2*pi*e*sigma^2) for X ~ N(mu, sigma^2)"""
    return 0.5 * np.log(2 * np.pi * np.e * sigma**2)
```
:::

:::details Q5: Cosine Annealing ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
```python
def cosine_annealing(step, total_steps, lr_max, lr_min=0):
    """Î·_t = Î·_min + 0.5*(Î·_max - Î·_min)*(1 + cos(Ï€*t/T))"""
    return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(np.pi * step / total_steps))
```
:::

### 5.4 è«–æ–‡èª­è§£ãƒ†ã‚¹ãƒˆ â€” Kingma & Ba (2014) "Adam" [^4]

Adam ã®åŸè«–æ–‡ã‚’ Pass 1 ã§èª­ã‚“ã§ã¿ã‚ˆã†ã€‚

**ã‚¿ã‚¹ã‚¯**: ä»¥ä¸‹ã®å•ã„ã«ç­”ãˆã‚ˆï¼ˆè«–æ–‡ arXiv:1412.6980ï¼‰ã€‚

```python
pass1_template = {
    "title": "Adam: A Method for Stochastic Optimization",
    "authors": "Diederik P. Kingma, Jimmy Ba",
    "year": 2014,
    "venue": "ICLR 2015",
    "category": "Optimization",
    "main_contribution": "???",  # Q1: fill this
    "key_equation": "???",       # Q2: equation number
    "compared_to": "???",        # Q3: baselines
    "limitations": "???",        # Q4: acknowledged
    "relevance_to_lec6": "???",  # Q5: connection
}
```

:::details è§£ç­”
```python
pass1_template = {
    "title": "Adam: A Method for Stochastic Optimization",
    "authors": "Diederik P. Kingma, Jimmy Ba",
    "year": 2014,
    "venue": "ICLR 2015",
    "category": "Optimization / First-order gradient methods",
    "main_contribution": "Adaptive learning rates using 1st & 2nd moment estimates with bias correction",
    "key_equation": "Algorithm 1 (Adam update rule)",
    "compared_to": "SGD, AdaGrad, RMSProp, SGD+Nesterov",
    "limitations": "May not converge for some convex problems (later fixed by AMSGrad)",
    "relevance_to_lec6": "Definition 3.10 â€” Adam is the standard optimizer for LLM training",
}
```
:::

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” KL æ¨å®šã® Monte Carlo æ³•

è§£æçš„ã« KL ãŒè¨ˆç®—ã§ããªã„å ´åˆã€ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰æ¨å®šã™ã‚‹æ–¹æ³•ã‚’å®Ÿè£…ã™ã‚‹ã€‚

```python
import numpy as np

def kl_monte_carlo(log_p_fn, log_q_fn, samples_p, n_samples=10000):
    """Estimate KL(p||q) via Monte Carlo: E_p[log p(x) - log q(x)]

    When we can sample from p and evaluate log-densities.
    """
    log_p = log_p_fn(samples_p)
    log_q = log_q_fn(samples_p)
    return np.mean(log_p - log_q)

# Example: KL between two Gaussians
# KL(N(mu1,s1^2) || N(mu2,s2^2)) = log(s2/s1) + (s1^2 + (mu1-mu2)^2)/(2*s2^2) - 1/2
mu1, s1 = 1.0, 0.5
mu2, s2 = 0.0, 1.0

# Analytical KL
kl_analytical = np.log(s2/s1) + (s1**2 + (mu1-mu2)**2)/(2*s2**2) - 0.5
print(f"Analytical KL(N({mu1},{s1**2}) || N({mu2},{s2**2})) = {kl_analytical:.6f}")

# Monte Carlo KL
np.random.seed(42)
samples = np.random.normal(mu1, s1, size=100000)
log_p = lambda x: -0.5*np.log(2*np.pi*s1**2) - (x-mu1)**2/(2*s1**2)
log_q = lambda x: -0.5*np.log(2*np.pi*s2**2) - (x-mu2)**2/(2*s2**2)

for n in [100, 1000, 10000, 100000]:
    kl_mc = kl_monte_carlo(log_p, log_q, samples[:n])
    error = abs(kl_mc - kl_analytical)
    print(f"  MC (n={n:6d}): KL = {kl_mc:.6f}, error = {error:.6f}")
```

### 5.6 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” æœ€é©åŒ–å™¨ã®åæŸæ¯”è¼ƒ

```python
import numpy as np

def rosenbrock(x):
    """f(x,y) = (1-x)^2 + 100(y-x^2)^2, minimum at (1,1)"""
    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2

def rosenbrock_grad(x):
    dx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)
    dy = 200*(x[1] - x[0]**2)
    return np.array([dx, dy])

def run_optimizer(opt, grad_fn, x0, n_steps=5000):
    x = x0.copy()
    trajectory = [x.copy()]
    for _ in range(n_steps):
        g = grad_fn(x)
        g = np.clip(g, -100, 100)
        x = opt.step(x, g)
        trajectory.append(x.copy())
    return np.array(trajectory)

x0 = np.array([-1.0, 1.0])
optimizers = {
    "SGD(lr=0.0001)": SGD(lr=0.0001),
    "Momentum(0.9)": MomentumSGD(lr=0.0001, beta=0.9),
    "Adam(lr=0.001)": AdamOptimizer(lr=0.001),
    "AdamW(lr=0.001)": AdamWOptimizer(lr=0.001, wd=0.01),
}

print("=== Rosenbrock Function Optimization ===")
print(f"Optimal: (1.0, 1.0), f* = 0.0\n")
for name, opt in optimizers.items():
    traj = run_optimizer(opt, rosenbrock_grad, x0, n_steps=5000)
    final = traj[-1]
    dist = np.linalg.norm(final - np.array([1.0, 1.0]))
    print(f"{name:20s}: final=({final[0]:+.4f}, {final[1]:.4f}), "
          f"f={rosenbrock(final):.6f}, dist={dist:.4f}")
```

### 5.7 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” AdamW ã§ç°¡å˜ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’è¨“ç·´

å­¦ã‚“ã æœ€é©åŒ–å™¨ã‚’ä½¿ã£ã¦ã€2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’ XOR å•é¡Œã§è¨“ç·´ã™ã‚‹ã€‚

```python
import numpy as np

class TwoLayerNet:
    """2-layer neural network for XOR classification."""

    def __init__(self, d_in=2, d_hidden=4, d_out=1):
        # Xavier initialization
        self.W1 = np.random.randn(d_in, d_hidden) * np.sqrt(2.0 / d_in)
        self.b1 = np.zeros(d_hidden)
        self.W2 = np.random.randn(d_hidden, d_out) * np.sqrt(2.0 / d_hidden)
        self.b2 = np.zeros(d_out)

    def forward(self, X):
        """Forward pass with ReLU activation."""
        self.z1 = X @ self.W1 + self.b1
        self.a1 = np.maximum(0, self.z1)  # ReLU
        self.z2 = self.a1 @ self.W2 + self.b2
        self.out = 1 / (1 + np.exp(-self.z2))  # sigmoid
        return self.out

    def backward(self, X, y):
        """Backward pass â€” compute gradients."""
        m = X.shape[0]
        # Output layer
        dz2 = self.out - y.reshape(-1, 1)  # BCE gradient
        dW2 = self.a1.T @ dz2 / m
        db2 = np.mean(dz2, axis=0)
        # Hidden layer
        da1 = dz2 @ self.W2.T
        dz1 = da1 * (self.z1 > 0)  # ReLU derivative
        dW1 = X.T @ dz1 / m
        db1 = np.mean(dz1, axis=0)
        return {"W1": dW1, "b1": db1, "W2": dW2, "b2": db2}

    def loss(self, y):
        """Binary cross-entropy: -1/N Î£ [y log(p) + (1-y) log(1-p)]"""
        y = y.reshape(-1, 1)
        eps = 1e-7
        return -np.mean(y * np.log(self.out + eps) + (1 - y) * np.log(1 - self.out + eps))

# XOR dataset
X = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=float)
y = np.array([0, 1, 1, 0], dtype=float)

# Train with AdamW
np.random.seed(42)
net = TwoLayerNet(d_in=2, d_hidden=8, d_out=1)
opts = {
    name: AdamWOptimizer(lr=0.01, wd=0.001)
    for name in ["W1", "b1", "W2", "b2"]
}
# Reset optimizers to match parameter shapes
for name in opts:
    param = getattr(net, name)
    opts[name] = AdamWOptimizer(lr=0.01, wd=0.001)
    opts[name].m = np.zeros_like(param)
    opts[name].v = np.zeros_like(param)

for epoch in range(1000):
    pred = net.forward(X)
    loss_val = net.loss(y)
    grads = net.backward(X, y)

    for name in ["W1", "b1", "W2", "b2"]:
        param = getattr(net, name)
        param = opts[name].step(param, grads[name])
        setattr(net, name, param)

    if epoch % 200 == 0:
        acc = np.mean((pred.flatten() > 0.5) == y)
        print(f"Epoch {epoch:4d}: loss = {loss_val:.4f}, accuracy = {acc:.2f}")

# Final predictions
pred_final = net.forward(X)
print(f"\nFinal predictions:")
for i in range(4):
    print(f"  Input: {X[i]} â†’ Pred: {pred_final[i,0]:.4f} (target: {y[i]:.0f})")
```

### 5.8 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®ã‚¬ã‚¦ã‚¹é–‰å½¢å¼

2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã® KL ã¯é–‰å½¢å¼ã§è¨ˆç®—ã§ãã‚‹ã€‚å°å‡ºã—ã€Monte Carlo ã¨æ¯”è¼ƒã›ã‚ˆã€‚

```python
import numpy as np

def kl_gaussians(mu1, sigma1, mu2, sigma2):
    """Analytical KL(N(mu1,s1^2) || N(mu2,s2^2))

    KL = log(s2/s1) + (s1^2 + (mu1-mu2)^2)/(2*s2^2) - 1/2
    """
    return (np.log(sigma2 / sigma1) +
            (sigma1**2 + (mu1 - mu2)**2) / (2 * sigma2**2) - 0.5)

def kl_gaussians_mc(mu1, sigma1, mu2, sigma2, n=100000):
    """Monte Carlo estimate of KL(N(mu1,s1^2) || N(mu2,s2^2))"""
    samples = np.random.normal(mu1, sigma1, n)
    log_p = -0.5*np.log(2*np.pi*sigma1**2) - (samples-mu1)**2/(2*sigma1**2)
    log_q = -0.5*np.log(2*np.pi*sigma2**2) - (samples-mu2)**2/(2*sigma2**2)
    return np.mean(log_p - log_q)

# Test cases relevant to VAE (Lec 10)
print("KL between Gaussians â€” relevant to VAE prior matching")
print("=" * 60)
cases = [
    (0, 1, 0, 1, "Same distribution"),
    (0, 1, 0, 2, "Wider q"),
    (0, 1, 1, 1, "Shifted mean"),
    (2, 0.5, 0, 1, "VAE-like: learned vs prior"),
]

np.random.seed(42)
for mu1, s1, mu2, s2, desc in cases:
    kl_exact = kl_gaussians(mu1, s1, mu2, s2)
    kl_mc = kl_gaussians_mc(mu1, s1, mu2, s2)
    print(f"  {desc:35s}: exact={kl_exact:.4f}, MC={kl_mc:.4f}, "
          f"error={abs(kl_exact-kl_mc):.4f}")
```

ã“ã®é–‰å½¢å¼ KL ã¯ VAE ã®æå¤±é–¢æ•°ã§ç›´æ¥ä½¿ã‚ã‚Œã‚‹ï¼ˆç¬¬10å›ï¼‰ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒå‡ºåŠ›ã™ã‚‹ $q(z|x) = \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x))$ ã¨äº‹å‰åˆ†å¸ƒ $p(z) = \mathcal{N}(0, I)$ ã® KL ãŒã“ã‚Œã ã€‚

### 5.9 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” Source Coding Theorem ã®ä½“é¨“

Shannon ã® Source Coding Theoremï¼ˆæƒ…å ±æºç¬¦å·åŒ–å®šç†ï¼‰ã‚’ä½“é¨“ã™ã‚‹ã€‚ã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ã®ç†è«–çš„é™ç•Œã‚’ç¤ºã™ã€‚

**å®šç†** (Shannon, 1948 [^1]): æƒ…å ±æº $X$ ã®å‡ºåŠ›ã‚’ç¬¦å·åŒ–ã™ã‚‹ã¨ãã€å¹³å‡ç¬¦å·é•· $L$ ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä»¥ä¸Š:

$$
L \geq H(X)
$$

ç­‰å·ã¯æœ€é©ãªç¬¦å·ï¼ˆHuffman ç¬¦å·ãªã©ï¼‰ã§è¿‘ä¼¼çš„ã«é”æˆã•ã‚Œã‚‹ã€‚

```python
import numpy as np
from collections import Counter

def huffman_avg_length(probs):
    """Approximate average code length for Huffman coding.

    Exact Huffman requires tree construction, but:
    H(X) <= L_huffman < H(X) + 1
    """
    # Shannon-Fano lower bound: optimal code length for symbol i â‰ˆ -log2(pi)
    p = np.array(probs)
    p = p[p > 0]
    entropy = -np.sum(p * np.log2(p))
    # Individual optimal lengths (ceil to integers)
    lengths = np.ceil(-np.log2(p))
    avg_length = np.sum(p * lengths)
    return entropy, avg_length

# Example: English letter frequencies (approximate)
letters = "etaoinshrdlcumwfgypbvkjxqz"
freqs = np.array([12.7, 9.06, 8.17, 7.51, 6.97, 6.75, 6.33, 6.09, 5.99,
                   4.25, 4.03, 2.78, 2.76, 2.41, 2.36, 2.23, 2.02, 1.97,
                   1.93, 1.49, 0.98, 0.77, 0.15, 0.15, 0.10, 0.05])
freqs = freqs / freqs.sum()

entropy, avg_len = huffman_avg_length(freqs)
print("English letter frequencies:")
print(f"  Entropy H(X)     = {entropy:.4f} bits/letter")
print(f"  Avg code length  = {avg_len:.4f} bits/letter")
print(f"  Fixed-length     = {np.log2(26):.4f} bits/letter")
print(f"  Compression ratio: {avg_len / np.log2(26):.2%} of fixed-length")
print(f"\nShannon bound: {entropy:.4f} <= L = {avg_len:.4f} < {entropy + 1:.4f}")

# Connection to LLM
print(f"\nLLM analogy:")
print(f"  Vocabulary size V = 50000")
print(f"  Fixed-length encoding: {np.log2(50000):.1f} bits/token")
print(f"  GPT-4 PPL ~10 â†’ H â‰ˆ {np.log2(10):.2f} bits/token")
print(f"  Compression: {np.log2(10)/np.log2(50000):.1%} of fixed-length")
print(f"  â†’ LLM is an extremely efficient 'compressor' of language")
```

**LLM = åœ§ç¸®å™¨**: ã“ã®è¦–ç‚¹ã¯æ·±ã„ã€‚LLM ãŒ Perplexity ã‚’ä¸‹ã’ã‚‹ã“ã¨ã¯ã€è¨€èªã®åŠ¹ç‡çš„ãªç¬¦å·åŒ–ã‚’å­¦ã¶ã“ã¨ã¨ç­‰ä¾¡ã ã€‚GPT-4 ã® Perplexity ãŒ 10 ã¨ã„ã†ã“ã¨ã¯ã€å¹³å‡ $\log_2 10 \approx 3.32$ bits/token ã§è‹±èªã‚’ç¬¦å·åŒ–ã§ãã‚‹ã¨ã„ã†ã“ã¨ã€‚å›ºå®šé•· $\log_2 50000 \approx 15.6$ bits ã«å¯¾ã—ã¦ç´„ 21% ã®åŠ¹ç‡ã€‚**LLM ã¯æœ¬è³ªçš„ã«ç¢ºç‡çš„ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®å™¨ãªã®ã **ã€‚

### 5.10 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” æœ€é©åŒ–ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã®æ¡ä»¶æ•°ã¨åæŸé€Ÿåº¦

æ¡ä»¶æ•°ï¼ˆcondition numberï¼‰ãŒæœ€é©åŒ–ã®é›£ã—ã•ã‚’æ±ºã‚ã‚‹ã€‚

```python
import numpy as np

def quadratic_convergence(eigenvalues, lr, n_steps):
    """Track convergence of GD on f(x) = 0.5 * x^T diag(eig) x.

    Convergence rate depends on condition number kappa = max_eig / min_eig.
    """
    d = len(eigenvalues)
    x = np.random.randn(d)
    losses = []
    for _ in range(n_steps):
        grad = eigenvalues * x
        x = x - lr * grad
        losses.append(0.5 * np.sum(eigenvalues * x**2))
    return losses

print("Effect of condition number on GD convergence:")
print(f"{'kappa':>8s} {'Steps to 1e-6':>15s} {'Optimal LR':>12s}")
print("-" * 40)

for kappa in [1, 10, 100, 1000]:
    eigs = np.array([1.0, float(kappa)])
    # Optimal learning rate for GD: 2 / (lambda_max + lambda_min)
    lr_opt = 2.0 / (eigs.max() + eigs.min())
    losses = quadratic_convergence(eigs, lr_opt, 10000)
    # Find steps to reach 1e-6
    steps_needed = next((i for i, l in enumerate(losses) if l < 1e-6), 10000)
    print(f"{kappa:8d} {steps_needed:15d} {lr_opt:12.6f}")

print(f"\nInsight: kappa 10x â†’ convergence ~2-3x slower")
print(f"Adam mitigates this via per-parameter adaptive learning rates")
```

ã“ã®æ¡ä»¶æ•°ã®å•é¡Œã“ã Adam ãŒè§£æ±ºã™ã‚‹èª²é¡Œã ã€‚å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ–¹å‘ã«ç‹¬ç«‹ã—ãŸå­¦ç¿’ç‡ã‚’æŒã¤ã“ã¨ã§ã€æ¡ä»¶æ•°ãŒå¤§ãã„ï¼ˆ= æ–¹å‘ã«ã‚ˆã£ã¦æ›²ç‡ãŒç•°ãªã‚‹ï¼‰å•é¡Œã§ã‚‚åŠ¹ç‡çš„ã«åæŸã™ã‚‹ã€‚

### 5.11 è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å®šç¾©ã‚’æ›¸ã‘ã‚‹
- [ ] KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®éè² æ€§ã‚’ Jensen ã®ä¸ç­‰å¼ã‹ã‚‰è¨¼æ˜ã§ãã‚‹
- [ ] Cross-Entropy = H(p) + KL(p||q) ã‚’å°å‡ºã§ãã‚‹
- [ ] å‰å‘ã KL ã¨é€†å‘ã KL ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] ç›¸äº’æƒ…å ±é‡ã‚’ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã—ã¦æ›¸ã‘ã‚‹
- [ ] f-Divergence ã®å®šç¾©ã¨ä¸»è¦ãªç‰¹æ®Šã‚±ãƒ¼ã‚¹ã‚’åˆ—æŒ™ã§ãã‚‹
- [ ] SGD ã®æ›´æ–°å‰‡ã‚’æ›¸ã‘ã‚‹
- [ ] Momentum ã®ç‰©ç†çš„ç›´æ„Ÿã‚’èª¬æ˜ã§ãã‚‹
- [ ] Adam ã®å…¨æ›´æ–°å‰‡ï¼ˆãƒã‚¤ã‚¢ã‚¹è£œæ­£è¾¼ã¿ï¼‰ã‚’æ›¸ã‘ã‚‹
- [ ] AdamW ã¨ Adam + L2 ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Cosine Annealing ã®æ•°å¼ã‚’æ›¸ã‘ã‚‹
- [ ] Cross-Entropy Loss ã®æƒ…å ±ç†è«–çš„åˆ†è§£ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Perplexity = 2^H ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹

:::message
**é€²æ—: 85% å®Œäº†** è¨˜å·èª­è§£ãƒ»LaTeXãƒ»ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ»è«–æ–‡èª­è§£ã®å…¨ãƒ†ã‚¹ãƒˆã‚’å®Œäº†ã€‚è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã§å¼±ç‚¹ã‚’ç¢ºèªã›ã‚ˆã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.2 æœ¬è¬›ç¾©ã®æ¦‚å¿µé–“ã®æ¥ç¶šãƒãƒƒãƒ—

æœ¬è¬›ç¾©ã§å­¦ã‚“ã æ¦‚å¿µã¯å­¤ç«‹ã—ã¦ã„ãªã„ã€‚ä»¥ä¸‹ã®ãƒãƒƒãƒ—ã§å…¨ã¦ã®ç¹‹ãŒã‚Šã‚’ç¢ºèªã™ã‚‹ã€‚

```mermaid
graph TD
    Shannon["Shannon Entropy<br/>H(X) 1948"] -->|"+å®šæ•°"| CE["Cross-Entropy<br/>H(p,q)"]
    Shannon -->|"æ¡ä»¶ä»˜ã"| CndH["H(Y|X)"]
    CE -->|"åˆ†è§£"| KL["KL Divergence<br/>D_KL 1951"]
    Shannon -->|"å·®åˆ†"| MI["Mutual Info<br/>I(X;Y)"]
    CndH -->|"å·®åˆ†"| MI

    KL -->|"ç‰¹æ®Šã‚±ãƒ¼ã‚¹"| fDiv["f-Divergence<br/>Csiszar 1967"]
    fDiv -->|"f=t*log(t)"| KL
    fDiv -->|"JSD"| JSD["Jensen-Shannon"]
    fDiv -->|"å¤‰åˆ†è¡¨ç¾"| NWJ["NWJ 2010"]

    KL -->|"æœ€å°åŒ–"| MLE["MLE<br/>ç¬¬7å›"]
    CE -->|"æå¤±é–¢æ•°"| LLM["LLM è¨“ç·´"]
    Shannon -->|"2^H"| PPL["Perplexity"]

    SGD["SGD<br/>Robbins 1951"] -->|"+æ…£æ€§"| Mom["Momentum<br/>Polyak 1964"]
    Mom -->|"+é©å¿œLR"| Adam["Adam<br/>Kingma 2014"]
    Adam -->|"+åˆ†é›¢WD"| AdamW["AdamW<br/>Loshchilov 2017"]
    Adam -->|"æ›´æ–°"| LLM

    fDiv -->|"GANæå¤±"| GAN["GAN ç¬¬12å›"]
    KL -->|"ELBO"| VAE["VAE ç¬¬10å›"]
    NWJ -->|"f-GAN"| GAN

    style Shannon fill:#e3f2fd
    style Adam fill:#e8f5e9
    style LLM fill:#fff3e0
```

### 6.3 æƒ…å ±ç†è«–ã¨ç‰©ç†å­¦ã®æ¥ç¶š

Shannon ãŒã€Œã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€ã®åã‚’ç†±åŠ›å­¦ã‹ã‚‰å€Ÿã‚ŠãŸã®ã¯å¶ç„¶ã§ã¯ãªã„ã€‚

| æƒ…å ±ç†è«– | çµ±è¨ˆç‰©ç†å­¦ | å¯¾å¿œ |
|:---------|:---------|:-----|
| Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $H$ | Gibbs ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $S$ | $S = -k_B \sum p \ln p$ |
| æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒ | Boltzmann åˆ†å¸ƒ | $p \propto e^{-E/k_BT}$ |
| KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼å·® | $F = E - TS$ |
| Cross-Entropy æœ€å°åŒ– | è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– | å¤‰åˆ†æ¨è«– |
| ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸ç­‰å¼ | ç†±åŠ›å­¦ç¬¬2æ³•å‰‡ | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¢—å¤§ |

ã“ã®å¯¾å¿œã¯å¶ç„¶ã§ã¯ãªãæ•°å­¦çš„ã«å³å¯†ã ã€‚Boltzmann åˆ†å¸ƒ $p(x) \propto \exp(-E(x)/T)$ ã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆ¶ç´„ä»˜ãã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒã§ã‚ã‚Šã€Zone 4 ã®ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã§å°ã‘ã‚‹ã€‚ã“ã®ã€Œæƒ…å ± = ç‰©ç†ã€ã®è¦–ç‚¹ã¯ç¬¬27å›ï¼ˆEnergy-Based Modelsï¼‰ã§æœ¬æ ¼çš„ã«å±•é–‹ã™ã‚‹ã€‚

### 6.4 æ¬¡ä¸–ä»£æœ€é©åŒ–å™¨ã®å‹•å‘ï¼ˆ2024-2026ï¼‰

Adam ã¯2014å¹´ã‹ã‚‰10å¹´ä»¥ä¸Šã«ã‚ãŸã‚Šæ¨™æº–çš„ãªæœ€é©åŒ–å™¨ã§ã‚ã‚Šç¶šã‘ã¦ã„ã‚‹ã€‚ã ãŒè¿‘å¹´ã€ä»¥ä¸‹ã®ä»£æ›¿æ¡ˆãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ã€‚

| æœ€é©åŒ–å™¨ | è‘—è€…/å¹´ | ç‰¹å¾´ | Adam ã¨ã®æ¯”è¼ƒ |
|:---------|:-------|:-----|:-----------|
| Lion | Google, 2023 | sign-basedæ›´æ–°ã€ãƒ¡ãƒ¢ãƒªåŠæ¸› | åŒ¹æ•µã™ã‚‹æ€§èƒ½ã§çœãƒ¡ãƒ¢ãƒª |
| Sophia | Stanford, 2023 | 2æ¬¡æƒ…å ±ï¼ˆãƒ˜ãƒƒã‚»å¯¾è§’ï¼‰åˆ©ç”¨ | è¨“ç·´ãƒˆãƒ¼ã‚¯ãƒ³50%å‰Šæ¸› |
| Muon | MIT, 2024 | ç›´äº¤å°„å½±ã«åŸºã¥ã | Transformerç‰¹åŒ– |
| Schedule-Free | Meta, 2024 | ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ä¸è¦ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸› |
| ADOPT | Taniguchi+, 2024 | ä»»æ„ã® $\beta_2$ ã§åæŸä¿è¨¼ | Adam ã®ç†è«–çš„æ¬ é™¥ã‚’ä¿®æ­£ |

ã“ã‚Œã‚‰ã®æœ€é©åŒ–å™¨ãŒ AdamW ã‚’æœ¬å½“ã«ç½®ãæ›ãˆã‚‹ã‹ã¯ã¾ã æ±ºç€ãŒã¤ã„ã¦ã„ãªã„ã€‚LLM ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®æ¤œè¨¼ã«ã¯ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ãŸã‚ã€çµæœã®å†ç¾ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã®ãŒç¾çŠ¶ã ã€‚

:::details ç”¨èªé›†ï¼ˆæœ¬è¬›ç¾©ã®å…¨ç”¨èªï¼‰
| ç”¨èªï¼ˆè‹±ï¼‰ | ç”¨èªï¼ˆæ—¥ï¼‰ | å®šç¾©ã®å ´æ‰€ |
|:-----------|:---------|:---------|
| Shannon Entropy | ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.1 |
| Differential Entropy | å¾®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.2 |
| KL Divergence | KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç¾© 3.3 |
| Cross-Entropy | äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.4 |
| Conditional Entropy | æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.5 |
| Mutual Information | ç›¸äº’æƒ…å ±é‡ | å®šç¾© 3.6 |
| f-Divergence | f ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç¾© 3.7 |
| Jensen's Inequality | ã‚¤ã‚§ãƒ³ã‚»ãƒ³ã®ä¸ç­‰å¼ | å®šç† 3.4 |
| SGD | ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³• | å®šç¾© 3.8 |
| Momentum | ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  | å®šç¾© 3.9 |
| Adam | ã‚¢ãƒ€ãƒ  | å®šç¾© 3.10 |
| AdamW | ã‚¢ãƒ€ãƒ ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ | 3.10 |
| Perplexity | ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ | Zone 0 |
| Mode-covering | ãƒ¢ãƒ¼ãƒ‰ã‚«ãƒãƒªãƒ³ã‚° | 3.3 |
| Mode-seeking | ãƒ¢ãƒ¼ãƒ‰ã‚·ãƒ¼ã‚­ãƒ³ã‚° | 3.3 |
| Fenchel Conjugate | ãƒ•ã‚§ãƒ³ã‚·ã‚§ãƒ«å…±å½¹ | 3.6 details |
| Cosine Annealing | ã‚³ã‚µã‚¤ãƒ³ç„¼ããªã¾ã— | 3.11 |
| Warmup | ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— | 3.11 |
| Bias Correction | ãƒã‚¤ã‚¢ã‚¹è£œæ­£ | 3.10 |
| Weight Decay | é‡ã¿æ¸›è¡° | 3.10 |
| Data Processing Inequality | ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸ç­‰å¼ | å®šç† 3.5 |
| Fano's Inequality | ãƒ•ã‚¡ãƒã®ä¸ç­‰å¼ | 3.5b |
| Chain Rule (Entropy) | é€£é–å¾‹ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰ | 3.5c |
| Convex Set | å‡¸é›†åˆ | å®šç¾© 3.6 |
| Convex Function | å‡¸é–¢æ•° | å®šç¾© 3.7 |
| Strong Convexity | å¼·å‡¸æ€§ | å®šç¾© 3.8 |
| KKT Conditions | KKT æ¡ä»¶ | å®šç† 3.9 |
| Lagrangian Dual | ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥åŒå¯¾ | å®šç† 3.10 |
| Strong Duality | å¼·åŒå¯¾æ€§ | å®šç† 3.11 |
| Lipschitz Continuity | ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šæ€§ | å®šç¾© 3.9 |
| Spectral Normalization | ã‚¹ãƒšã‚¯ãƒˆãƒ«æ­£è¦åŒ– | 3.11c |
| Jensen-Shannon Divergence | JSD | 3.11d |
| Wasserstein Distance | ãƒ¯ãƒƒã‚µãƒ¼ã‚¹ã‚¿ã‚¤ãƒ³è·é›¢ | å®šç¾© 3.10 |
| Kantorovich-Rubinstein Duality | KR åŒå¯¾æ€§ | 3.11d |
| Optimal Transport | æœ€é©è¼¸é€ | 3.11d |
| WGAN | ãƒ¯ãƒƒã‚µãƒ¼ã‚¹ã‚¿ã‚¤ãƒ³ GAN | 3.11d |
| Gradient Clipping | å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚° | 4.7 |
| Mixed Precision | æ··åˆç²¾åº¦ | 4.7 |
| Lagrangian | ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ã‚¢ãƒ³ | 4.8 |
| Maximum Entropy | æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | 4.8 |
| Fisher Information | ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±é‡ | 6.3 |
| Natural Gradient | è‡ªç„¶å‹¾é… | 6.3 |
| Rate-Distortion | ãƒ¬ãƒ¼ãƒˆæ­ªã¿ | 6.2 |
| Source Coding Theorem | æƒ…å ±æºç¬¦å·åŒ–å®šç† | 5.9 |
| Condition Number | æ¡ä»¶æ•° | 5.10 |
| Gibbs Inequality | ã‚®ãƒ–ã‚¹ã®ä¸ç­‰å¼ | å®šç† 3.2 |
| Bregman Divergence | ãƒ–ãƒ¬ã‚°ãƒãƒ³ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | 3.7 details |
| Information Bottleneck | æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ | 3.5b |
| WSD Schedule | ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®‰å®šæ¸›è¡° | 3.11 |
:::

:::details ä¸»è¦ãªä¸ç­‰å¼ã¾ã¨ã‚
| ä¸ç­‰å¼ | æ•°å¼ | æ„å‘³ | è¨¼æ˜ |
|:-------|:-----|:-----|:-----|
| KL ã®éè² æ€§ | $D_\text{KL}(p \| q) \geq 0$ | ç•°ãªã‚‹åˆ†å¸ƒãªã‚‰è·é›¢ã¯æ­£ | Jensen |
| Jensen ã®ä¸ç­‰å¼ | $f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]$ | å‡¸é–¢æ•°ã®æœŸå¾…å€¤ | æ”¯æŒè¶…å¹³é¢ |
| Gibbs ã®ä¸ç­‰å¼ | $H(p, q) \geq H(p)$ | Cross-Entropy â‰¥ Entropy | KL â‰¥ 0 |
| ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸ç­‰å¼ | $I(X;Z) \leq I(X;Y)$ if $X \to Y \to Z$ | å‡¦ç†ã§æƒ…å ±ã¯å¢—ãˆãªã„ | Chain Rule |
| Fano ã®ä¸ç­‰å¼ | $H(X|\hat{X}) \leq H_b(P_e) + P_e \log(|\mathcal{X}|-1)$ | æ¨å®šç²¾åº¦ã®ä¸‹é™ | â€” |
| Source Coding | $L \geq H(X)$ | ç¬¦å·é•·ã®ä¸‹é™ = ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | â€” |
| $H$ ã®ä¸Šç•Œ | $H(X) \leq \log |\mathcal{X}|$ | ç­‰å·ã¯ä¸€æ§˜åˆ†å¸ƒ | Jensen |
| ã‚¬ã‚¦ã‚¹ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | $h(X) \leq \frac{1}{2}\log(2\pi e \sigma^2)$ | åˆ†æ•£å›ºå®šã§ã‚¬ã‚¦ã‚¹ãŒæœ€å¤§ | Lagrange |
:::

:::details ä¸»è¦ãªç­‰å¼ã¾ã¨ã‚
| ç­‰å¼ | æ•°å¼ | æ„å‘³ |
|:-----|:-----|:-----|
| Cross-Entropy åˆ†è§£ | $H(p,q) = H(p) + D_\text{KL}(p \| q)$ | CE = Entropy + KL |
| ç›¸äº’æƒ…å ±é‡ (1) | $I(X;Y) = H(X) - H(X|Y)$ | MI = Entropy reduction |
| ç›¸äº’æƒ…å ±é‡ (2) | $I(X;Y) = D_\text{KL}(p(x,y) \| p(x)p(y))$ | MI = KL from independence |
| Entropy Chain Rule | $H(X,Y) = H(X) + H(Y|X)$ | Joint = Marginal + Conditional |
| ã‚¬ã‚¦ã‚¹ KL | $D_\text{KL}(\mathcal{N}_1 \| \mathcal{N}_2) = \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}$ | Closed form |
| ã‚¬ã‚¦ã‚¹å¾®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | $h(X) = \frac{1}{2}\log(2\pi e \sigma^2)$ | Depends only on $\sigma$ |
:::

### 6.0 æœ¬è¬›ç¾©ã®çŸ¥è­˜ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—

```mermaid
mindmap
  root((ç¬¬6å›))
    æƒ…å ±ç†è«–
      ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        Shannon 1948
        é›¢æ•£ H_X
        å¾®åˆ† h_X
        Perplexity = 2^H
      KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
        Kullback-Leibler 1951
        éå¯¾ç§°æ€§
        å‰å‘ã vs é€†å‘ã
        éè² æ€§ Gibbs
      Cross-Entropy
        H_p,q = H_p + KL
        LLM æå¤±é–¢æ•°
      ç›¸äº’æƒ…å ±é‡
        I_X;Y = H_X - H_X|Y
        è¡¨ç¾å­¦ç¿’
      f-Divergence
        Csiszar 1967
        KL, chi2, Hellinger
        å¤‰åˆ†è¡¨ç¾ NWJ
    æœ€é©åŒ–ç†è«–
      å‹¾é…é™ä¸‹æ³•
        Robbins-Monro 1951
        åæŸ O_1/T
      Momentum
        Polyak 1964
        Nesterov 1983
        åæŸ O_1/T^2
      Adam
        Kingma-Ba 2014
        é©å¿œçš„å­¦ç¿’ç‡
        ãƒã‚¤ã‚¢ã‚¹è£œæ­£
      AdamW
        Loshchilov-Hutter 2017
        åˆ†é›¢å‹é‡ã¿æ¸›è¡°
      ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
        Cosine Annealing
        Warmup
        WSD
```

---

### 6.5 æœ¬è¬›ç¾©ã®ã‚­ãƒ¼ãƒ†ã‚¤ã‚¯ã‚¢ã‚¦ã‚§ã‚¤

1. **Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**ã¯ä¸ç¢ºå®Ÿæ€§ã®å”¯ä¸€ã®åˆç†çš„å°ºåº¦ã§ã‚ã‚Šã€LLM ã® Perplexity $= 2^H$ ã®æ­£ä½“
2. **KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹**ã¯éå¯¾ç§°ãªåˆ†å¸ƒé–“è·é›¢ã€‚å‰å‘ã KL ã¯ mode-coveringã€é€†å‘ã KL ã¯ mode-seeking â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æå¤±è¨­è¨ˆã«ç›´çµ
3. **Cross-Entropy ã®æœ€å°åŒ– = KL ã®æœ€å°åŒ–** â€” LLM è¨“ç·´ã®æå¤±é–¢æ•°ãŒæƒ…å ±ç†è«–çš„ã«å¿…ç„¶ã§ã‚ã‚‹ã“ã¨ã®è¨¼æ˜
4. **Adam** ã¯ Momentum + é©å¿œçš„å­¦ç¿’ç‡ + ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã®åˆã‚ã›æŠ€ã€‚AdamW ãŒæ­£ã—ã„é‡ã¿æ¸›è¡°

### 6.6 FAQ

:::details Q1: KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯è·é›¢ã˜ã‚ƒãªã„ãªã‚‰ã€ãªãœä½¿ã†ã®ã‹ï¼Ÿ
KL ã¯è·é›¢ã®å…¬ç†ï¼ˆå¯¾ç§°æ€§ã€ä¸‰è§’ä¸ç­‰å¼ï¼‰ã‚’æº€ãŸã•ãªã„ã€‚ã ãŒæ©Ÿæ¢°å­¦ç¿’ã§é‡è¦ãªã®ã¯ã€Œæœ€å°åŒ–å¯èƒ½ã‹ã€ã§ã‚ã‚Šã€KL ã«ã¯ä»¥ä¸‹ã®åˆ©ç‚¹ãŒã‚ã‚‹: (1) æœ€å°åŒ–ãŒ MLE ã¨ç­‰ä¾¡ã€(2) å‹¾é…è¨ˆç®—ãŒå®¹æ˜“ã€(3) æƒ…å ±ç†è«–çš„æ„å‘³ãŒæ˜ç¢ºã€‚çœŸã®ã€Œè·é›¢ã€ãŒæ¬²ã—ã‘ã‚Œã° JS ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚„ Wasserstein è·é›¢ã‚’ä½¿ã†ï¼ˆç¬¬13å›ï¼‰ã€‚
:::

:::details Q2: Adam ã¨ SGDã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ
ä¸€èˆ¬å‰‡: **Adam ã§å§‹ã‚ã¦ã€å¿…è¦ãªã‚‰ SGD ã«åˆ‡ã‚Šæ›¿ãˆã‚‹**ã€‚Adam ã¯å­¦ç¿’ç‡ã®èª¿æ•´ãŒæ¥½ã§åæŸãŒé€Ÿã„ãŒã€æ±åŒ–æ€§èƒ½ã§ã¯ SGD+Momentum ã«åŠ£ã‚‹å ´åˆãŒã‚ã‚‹ï¼ˆç‰¹ã«ç”»åƒåˆ†é¡ï¼‰ã€‚LLM è¨“ç·´ã§ã¯ AdamW ãŒã»ã¼æ¨™æº–ã€‚æœ€è¿‘ã®ç ”ç©¶ã§ã¯ Lion ã‚„ Sophia ãªã©æ¬¡ä¸–ä»£æœ€é©åŒ–å™¨ã‚‚ææ¡ˆã•ã‚Œã¦ã„ã‚‹ãŒã€AdamW ã®ãƒ­ãƒã‚¹ãƒˆæ€§ã¯ã¾ã æºã‚‹ãŒãªã„ã€‚
:::

:::details Q3: Perplexity ã¯ã©ã“ã¾ã§ä¸‹ãŒã‚‹ï¼Ÿ
ç†è«–çš„ä¸‹é™ã¯ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $H(\hat{p})$ã€‚è‡ªç„¶è¨€èªã¯å†—é•·æ€§ãŒé«˜ã„ãŸã‚ã€è‹±èªãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯1-2 bits/character ç¨‹åº¦ã€‚GPT-4 ã® Perplexity ãŒéå¸¸ã«ä½ã„ã®ã¯ã€äººé–“ã®è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç²¾ç·»ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¦ã„ã‚‹ã‹ã‚‰ã€‚ãŸã ã— Perplexity = 1 ã¯ä¸å¯èƒ½ â€” ãã‚Œã¯ãƒ‡ãƒ¼ã‚¿ã«ä¸ç¢ºå®Ÿæ€§ãŒãªã„ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚
:::

:::details Q4: f-Divergence ã¯å®Ÿéš›ã«ã©ã“ã§ä½¿ã‚ã‚Œã‚‹ï¼Ÿ
f-GAN [Nowozin+ 2016] ã¯ f-Divergence ã®å¤‰åˆ†è¡¨ç¾ã‚’ç›´æ¥ä½¿ã£ã¦ GAN ã‚’è¨“ç·´ã™ã‚‹ã€‚$f$ ã®é¸æŠã«ã‚ˆã‚Š KL-GANã€Pearson-GANã€Hellinger-GAN ãªã©ãŒçµ±ä¸€çš„ã«å°å‡ºã§ãã‚‹ã€‚ã¾ãŸã€å¯†åº¦æ¯”æ¨å®šï¼ˆdensity ratio estimationï¼‰ã«ã‚‚ f-Divergence ã®å¤‰åˆ†è¡¨ç¾ãŒä½¿ã‚ã‚Œã‚‹ã€‚ç¬¬12-14å›ã§å®Ÿè£…ã™ã‚‹ã€‚
:::

:::details Q5: æ•°å­¦ãŒè‹¦æ‰‹ã§ã‚‚å¤§ä¸ˆå¤«ï¼Ÿ
Zone 3 ã®å…¨å°å‡ºãŒç†è§£ã§ããªãã¦ã‚‚ã€ä»¥ä¸‹ã®ã€Œæœ€ä½é™ã€ã‚’æŠ¼ã•ãˆã‚Œã°å…ˆã«é€²ã‚ã‚‹: (1) ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = ä¸ç¢ºå®Ÿæ€§ã€(2) KL = åˆ†å¸ƒã®è·é›¢ï¼ˆéå¯¾ç§°ï¼‰ã€(3) Cross-Entropy æœ€å°åŒ– = KL æœ€å°åŒ–ã€(4) Adam = è³¢ã„ SGDã€‚æ•°å¼ã¯ç¹°ã‚Šè¿”ã—è§¦ã‚‹ã“ã¨ã§èº«ä½“ã«æŸ“ã¿ã‚‹ã€‚ç¬¬7-16å›ã§åŒã˜é“å…·ã‚’ä½•åº¦ã‚‚ä½¿ã†ã‹ã‚‰ã€ä»Šå®Œå…¨ã«ç†è§£ã™ã‚‹å¿…è¦ã¯ãªã„ã€‚
:::

:::details Q6: ãªãœ MSEï¼ˆå¹³å‡äºŒä¹—èª¤å·®ï¼‰ã§ã¯ãªã Cross-Entropy ã‚’ä½¿ã†ã®ã‹ï¼Ÿ
åˆ†é¡å•é¡Œã§ã¯å‡ºåŠ›ãŒç¢ºç‡åˆ†å¸ƒãªã®ã§ã€åˆ†å¸ƒé–“ã®è·é›¢ã‚’æ¸¬ã‚‹ Cross-Entropy ãŒè‡ªç„¶ãªé¸æŠã€‚MSE ã¯å›å¸°å•é¡Œå‘ãã§ã€ç¢ºç‡åˆ†å¸ƒã®æ¯”è¼ƒã«ã¯æƒ…å ±ç†è«–çš„ã«æ ¹æ‹ ãŒãªã„ã€‚å…·ä½“çš„ã«ã¯ã€Cross-Entropy ã¯å‡ºåŠ›ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¯¾ã—ã¦é©åˆ‡ãªå‹¾é…ã‚’ä¸ãˆã‚‹ãŒã€MSE + Sigmoid ã¯å‡ºåŠ›ãŒ 0 ã‚„ 1 ã«è¿‘ã„ã¨ãã«å‹¾é…ãŒæ¶ˆå¤±ã™ã‚‹ï¼ˆsigmoid ã® saturation å•é¡Œï¼‰ã€‚
:::

:::details Q7: KL ã¯è·é›¢ã®ä¸‰è§’ä¸ç­‰å¼ã‚’æº€ãŸã•ãªã„ãŒã€å›°ã‚‰ãªã„ã®ã‹ï¼Ÿ
å®Ÿç”¨ä¸Šã¯å›°ã‚‰ãªã„ã€‚ä¸‰è§’ä¸ç­‰å¼ $D(p, r) \leq D(p, q) + D(q, r)$ ã¯ã€Œä¸­é–“ç‚¹ã‚’çµŒç”±ã—ã¦ã‚‚è·é›¢ãŒå¢—ãˆãªã„ã€ã¨ã„ã†æ€§è³ªã ãŒã€æœ€é©åŒ–ã§ã¯ç›´æ¥ $D(p_\text{data}, q_\theta)$ ã‚’æœ€å°åŒ–ã™ã‚‹ã®ã§ä¸­é–“ç‚¹ã¯ä¸è¦ã€‚ãŸã ã—ã€ç†è«–çš„ãªè§£æï¼ˆåæŸãƒ¬ãƒ¼ãƒˆã®è¨¼æ˜ãªã©ï¼‰ã§ã¯ä¸‰è§’ä¸ç­‰å¼ãŒä¾¿åˆ©ãªã®ã§ã€ãã†ã„ã†å ´é¢ã§ã¯ TV è·é›¢ã‚„ Wasserstein è·é›¢ã‚’ä½¿ã†ã€‚
:::

:::details Q8: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒæœ€å¤§ã®ã¨ãã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œä½•ã‚‚å­¦ã‚“ã§ã„ãªã„ã€ã®ã‹ï¼Ÿ
ãã®é€šã‚Šã€‚ä¸€æ§˜åˆ†å¸ƒã¯æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒã§ã‚ã‚Šã€ã€Œå…¨ã¦ã®é¸æŠè‚¢ãŒç­‰ç¢ºç‡ã€= ã€Œä½•ã®æƒ…å ±ã‚‚ä½¿ãˆã¦ã„ãªã„ã€çŠ¶æ…‹ã€‚å­¦ç¿’ãŒé€²ã‚€ã¨å‡ºåŠ›åˆ†å¸ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒä¸‹ãŒã‚Šã€ç‰¹å®šã®é¸æŠè‚¢ã«ç¢ºç‡ãŒé›†ä¸­ã™ã‚‹ã€‚ã“ã‚ŒãŒ Perplexity ã®æ¸›å°‘ã¨ã—ã¦è¦³æ¸¬ã•ã‚Œã‚‹ã€‚ãŸã ã—ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒä½ã™ãã‚‹ï¼ˆ= 1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ç¢ºç‡ãŒé›†ä¸­ã—ã™ãã‚‹ï¼‰ã®ã‚‚å•é¡Œã§ã€ç”Ÿæˆã®å¤šæ§˜æ€§ãŒå¤±ã‚ã‚Œã‚‹ã€‚ã“ã‚ŒãŒ Temperature ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å­˜åœ¨ç†ç”±ã€‚
:::

:::details Q9: å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã¯ãªãœ max_norm = 1.0 ãŒä¸€èˆ¬çš„ï¼Ÿ
ç†è«–çš„ãªæ ¹æ‹ ã¯è–„ã„ã€‚çµŒé¨“çš„ã«ã€å‹¾é…ã®ãƒãƒ«ãƒ ãŒ1ç¨‹åº¦ãªã‚‰å­¦ç¿’ãŒå®‰å®šã™ã‚‹ã¨ã„ã†ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã ã€‚å®Ÿéš›ã«ã¯å­¦ç¿’ç‡ã‚„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦èª¿æ•´ã™ã‚‹ã€‚GPT-3 ã§ã¯ max_norm = 1.0ã€Llama 2 ã§ã¯ max_norm = 1.0 ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚é‡è¦ãªã®ã¯å…·ä½“çš„ãªå€¤ã‚ˆã‚Šã€Œçˆ†ç™ºã‚’é˜²ãå®‰å…¨å¼ãŒã‚ã‚‹ã€ã“ã¨ã€‚
:::

:::details Q10: Adam ã®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã¯æœ¬å½“ã«å¿…è¦ï¼Ÿ
å¿…è¦ã€‚ãƒã‚¤ã‚¢ã‚¹è£œæ­£ãªã—ã® Adamï¼ˆ= RMSProp + Momentumï¼‰ã¯å­¦ç¿’åˆæœŸã«$m_0 = 0, v_0 = 0$ ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã€åˆæœŸã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¨å®šãŒã‚¼ãƒ­æ–¹å‘ã«åã‚‹ã€‚ä¾‹ãˆã° $\beta_2 = 0.999$ ã§ step 1 ã§ã¯ $v_1 = 0.001 \cdot g_1^2$ ã¨æ¥µç«¯ã«å°ã•ãã€$\hat{v}_1 = v_1 / (1 - 0.999^1) = g_1^2$ ã¨è£œæ­£ã•ã‚Œã‚‹ã€‚è£œæ­£ãŒãªã„ã¨å­¦ç¿’ç‡ãŒå®ŸåŠ¹çš„ã«å¤§ãããªã‚Šã™ãã¦ä¸å®‰å®šã«ãªã‚‹ã€‚
:::

### 6.7 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | ç›®å®‰æ™‚é–“ |
|:---|:-----|:---------|
| Day 1 | Zone 0-2 ã‚’é€šèª­ + ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—ç·´ç¿’ | 45åˆ† |
| Day 2 | Zone 3 ã® 3.1-3.5ï¼ˆæƒ…å ±ç†è«–ãƒ‘ãƒ¼ãƒˆï¼‰ã‚’ç´™ã§å°å‡º | 90åˆ† |
| Day 3 | Zone 3 ã® 3.7-3.11ï¼ˆæœ€é©åŒ–ãƒ‘ãƒ¼ãƒˆï¼‰ã‚’ç´™ã§å°å‡º | 90åˆ† |
| Day 4 | Zone 4 ã®ã‚³ãƒ¼ãƒ‰ã‚’å…¨ã¦å®Ÿè¡Œ + æ”¹é€  | 60åˆ† |
| Day 5 | Zone 5 ã®è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ + Adam è«–æ–‡ Pass 1 | 60åˆ† |
| Day 6 | ãƒœã‚¹æˆ¦ã® Cross-Entropy åˆ†è§£ã‚’ç´™ã§å†ç¾ | 45åˆ† |
| Day 7 | ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆæœ€çµ‚ç¢ºèª + Zone 6 ã§èª­æ›¸è¨ˆç”» | 30åˆ† |

### 6.8 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```python
lecture6_progress = {
    "zone0_quickstart": True,
    "zone1_experience": True,
    "zone2_intuition": True,
    "zone3_math": {
        "entropy": False,        # Can you derive H(X)?
        "kl_divergence": False,  # Can you prove KL >= 0?
        "cross_entropy": False,  # Can you show H(p,q) = H(p) + KL?
        "mutual_info": False,    # Can you compute I(X;Y)?
        "f_divergence": False,   # Can you list 5 f-divergences?
        "jensen": False,         # Can you state and prove Jensen?
        "sgd": False,            # Can you write SGD update?
        "momentum": False,       # Can you explain momentum?
        "adam": False,            # Can you write full Adam?
        "boss_battle": False,    # Can you decompose CE loss?
    },
    "zone4_implementation": False,
    "zone5_experiment": False,
}

completed = sum(1 for v in lecture6_progress["zone3_math"].values() if v)
total = len(lecture6_progress["zone3_math"])
print(f"Zone 3 progress: {completed}/{total} ({completed/total:.0%})")
print(f"Mark each as True when you can do it WITHOUT looking at notes.")
```

### 6.9 æ¬¡å›äºˆå‘Š â€” ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–

ç¬¬6å›ã§æƒ…å ±ç†è«–ã¨æœ€é©åŒ–ã®æ­¦å™¨ãŒæƒã£ãŸã€‚æ¬¡å›ã¯ã„ã‚ˆã„ã‚ˆ**æœ€å°¤æ¨å®šã®æ•°å­¦æ§‹é€ **ã«æ­£é¢ã‹ã‚‰å‘ãåˆã†ã€‚

- **MLE ã®æ•°å­¦**: Cross-Entropy æœ€å°åŒ– = KL æœ€å°åŒ– = MLE ã®ä¸‰ä½ä¸€ä½“è¨¼æ˜ï¼ˆæœ¬è¬›ç¾©ã®ç›´æ¥çš„ç¶šãï¼‰
- **æ¨å®šé‡ã®åˆ†é¡ä½“ç³»**: æ˜ç¤ºçš„å°¤åº¦ / æš—é»™çš„æ¨å®š / ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°
- **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€ãƒãƒƒãƒ—**: VAE / GAN / Flow / Diffusion ã‚’ã€Œæ¨å®šé‡ã®è¨­è¨ˆã€ã¨ã—ã¦é³¥ç°
- **è©•ä¾¡æŒ‡æ¨™**: FID / KID / CMMD â€” çµ±è¨ˆçš„è·é›¢ã®å¿œç”¨
- **Python ã®é…ã•åŠ é€Ÿ**: MLE ã®åå¾©è¨ˆç®—ã§ã€Œé…ã™ããªã„ï¼Ÿã€ã®ä¸æº€ãŒå¢—å¹…

ç¬¬5å›ã¾ã§ã®æ•°å­¦åŸºç›¤ã¨ã€æœ¬è¬›ç¾©ã®æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ãŒã€ã“ã“ã‹ã‚‰å…ˆã®å…¨ã¦ã‚’æ”¯ãˆã‚‹ã€‚**6è¬›ç¾©ã®æ•°å­¦çš„æ­¦è£…ãŒå®Œäº†ã—ãŸ**ã€‚KL ã¯æå¤±é–¢æ•°ã«ã€SGD ã¯å­¦ç¿’ã«ã€Wasserstein è·é›¢ã¯è©•ä¾¡ã«ç›´çµã™ã‚‹ â€” ã“ã®å…¨ã¦ãŒç„¡ã‘ã‚Œã°ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åœ°å›³ã¯èª­ã‚ãªã„ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ç¬¬6å›ã€Œæƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–ã€å®Œäº†ã€‚Course I ã®æ•°å­¦çš„æ­¦è£…ã¯ã“ã‚Œã§6/8ã€‚æ¬¡å›ã‹ã‚‰ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ä¸–ç•Œã«è¶³ã‚’è¸ã¿å…¥ã‚Œã‚‹ã€‚
:::

---


### 6.10 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯å¯¾ç§°ã§ã™ã‚‰ãªã„"è·é›¢"ã€‚ãªãœã“ã‚ŒãŒæœ€é©è§£ï¼Ÿ**

ã“ã®å•ã„ã‚’3ã¤ã®è¦–ç‚¹ã‹ã‚‰è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚

1. **MLE ã¨ã®ç­‰ä¾¡æ€§**: KL æœ€å°åŒ–ã¨æœ€å°¤æ¨å®šãŒæ•°å­¦çš„ã«ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ï¼ˆç¬¬7å›ã§è¨¼æ˜ï¼‰ã€‚MLE ã¯çµ±è¨ˆå­¦ã§150å¹´ä»¥ä¸Šã®æ­´å²ã‚’æŒã¤æ¨å®šæ³•ã§ã‚ã‚Šã€ä¸€è‡´æ€§ãƒ»æ¼¸è¿‘æ­£è¦æ€§ãƒ»æ¼¸è¿‘æœ‰åŠ¹æ€§ã‚’æŒã¤ã€‚KL ã¯ã“ã®ã€Œç”±ç·’æ­£ã—ã„æ¨å®šæ³•ã€ã®æƒ…å ±ç†è«–çš„ãªé¡”ã«éããªã„ã€‚

2. **è¨ˆç®—å¯èƒ½æ€§**: Wasserstein è·é›¢ã¯å¯¾ç§°ã§ä¸‰è§’ä¸ç­‰å¼ã‚‚æº€ãŸã™ã€Œæ­£ã—ã„è·é›¢ã€ã ãŒã€é«˜æ¬¡å…ƒã§ã¯è¨ˆç®—ãŒå›°é›£ã€‚KL ã¯æœŸå¾…å€¤ã¨ã—ã¦æ›¸ã‘ã‚‹ãŸã‚ã€Monte Carlo æ¨å®šãŒå®¹æ˜“ã€‚å®Ÿç”¨ä¸Šã€è¨ˆç®—å¯èƒ½ãªã€Œä¸å®Œå…¨ãªè·é›¢ã€ã®æ–¹ãŒã€è¨ˆç®—ä¸èƒ½ãªã€Œå®Œå…¨ãªè·é›¢ã€ã‚ˆã‚Šæœ‰ç”¨ã ã€‚

3. **éå¯¾ç§°æ€§ã¯æ©Ÿèƒ½**: éå¯¾ç§°æ€§ã¯æ¬ ç‚¹ã§ã¯ãªãç‰¹å¾´ã€‚å‰å‘ã KL ã¨é€†å‘ã KL ãŒç•°ãªã‚‹æŒ¯ã‚‹èˆã„ã‚’ã™ã‚‹ã‹ã‚‰ã“ãã€ç›®çš„ã«å¿œã˜ãŸæå¤±é–¢æ•°è¨­è¨ˆãŒã§ãã‚‹ã€‚VAEï¼ˆå‰å‘ãï¼‰ã¨ GANï¼ˆé€†å‘ãï¼‰ã®å“è³ªã®é•ã„ã¯ã€ã¾ã•ã« KL ã®éå¯¾ç§°æ€§ã«èµ·å› ã™ã‚‹ã€‚

:::details æ­´å²çš„æ–‡è„ˆ: Shannon ã®ã€Œè³­ã‘ã€
Shannon ã¯1948å¹´ã®è«–æ–‡ [^1] ã§ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®åå‰ã‚’ç‰©ç†å­¦ã®ç†±åŠ›å­¦ã‹ã‚‰å€Ÿã‚ŠãŸã€‚von Neumann ã«åå‰ã®ç›¸è«‡ã‚’ã—ãŸã¨ã“ã‚ã€Œã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨å‘¼ã¹ã€‚èª°ã‚‚ç†è§£ã—ã¦ã„ãªã„ã‹ã‚‰è­°è«–ã§æœ‰åˆ©ã«ãªã‚‹ã€ã¨è¨€ã‚ã‚ŒãŸã¨ã„ã†é€¸è©±ãŒã‚ã‚‹ã€‚75å¹´å¾Œã€ã“ã®ã€Œèª°ã‚‚ç†è§£ã—ã¦ã„ãªã„ã€é‡ãŒ AI ã®è¨“ç·´ã‚’æ”¯é…ã—ã¦ã„ã‚‹ã€‚Shannon ã®ç›´æ„Ÿã¯æ­£ã—ã‹ã£ãŸ â€” æƒ…å ±ã®æœ¬è³ªã¯ã€Œé©šãã€ã®å®šé‡åŒ–ã«ã‚ã£ãŸã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Shannon, C. E. (1948). "A Mathematical Theory of Communication." *Bell System Technical Journal*, 27(3), 379-423.

[^2]: Kullback, S. & Leibler, R. A. (1951). "On Information and Sufficiency." *Annals of Mathematical Statistics*, 22(1), 79-86.

[^3]: Robbins, H. & Monro, S. (1951). "A Stochastic Approximation Method." *Annals of Mathematical Statistics*, 22(3), 400-407.

[^4]: Kingma, D. P. & Ba, J. (2014). "Adam: A Method for Stochastic Optimization." *ICLR 2015*.
@[card](https://arxiv.org/abs/1412.6980)

[^5]: Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). "Attention Is All You Need." *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1706.03762)

[^6]: Csiszar, I. (1967). "Information-Type Measures of Difference of Probability Distributions and Indirect Observations." *Studia Scientiarum Mathematicarum Hungarica*, 2, 299-318.

[^7]: Nguyen, X., Wainwright, M. J. & Jordan, M. I. (2010). "Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization." *IEEE Transactions on Information Theory*, 56(11), 5847-5861.
@[card](https://arxiv.org/abs/0809.0853)

[^8]: Polyak, B. T. (1964). "Some Methods of Speeding up the Convergence of Iteration Methods." *USSR Computational Mathematics and Mathematical Physics*, 4(5), 1-17.

[^9]: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. (2014). "Generative Adversarial Networks." *NeurIPS 2014*.
@[card](https://arxiv.org/abs/1406.2661)

[^11]: Boyd, S. & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.

[^12]: Miyato, T., Kataoka, T., Koyama, M. & Yoshida, Y. (2018). "Spectral Normalization for Generative Adversarial Networks." *ICLR 2018*.
@[card](https://arxiv.org/abs/1802.05957)

[^13]: Arjovsky, M., Chintala, S. & Bottou, L. (2017). "Wasserstein Generative Adversarial Networks." *ICML 2017*.
@[card](https://arxiv.org/abs/1701.07875)

### æ•™ç§‘æ›¸

- Cover, T. M. & Thomas, J. A. (2006). *Elements of Information Theory*. 2nd ed. Wiley.
- Boyd, S. & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press. [Free PDF: stanford.edu/~boyd/cvxbook/]
- Villani, C. (2008). *Optimal Transport: Old and New*. Springer. [Fields Medal å—è³è€…ã«ã‚ˆã‚‹æ±ºå®šç‰ˆã€‚ç¬¬13å›ã§æœ¬æ ¼ä½¿ç”¨]
- MacKay, D. J. C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. [Free PDF: inference.org.uk/mackay/itila/]

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | èª­ã¿æ–¹ | æ„å‘³ | åˆå‡º |
|:-----|:-------|:-----|:-----|
| $H(X)$ | ã‚¨ã‚¤ãƒ ã‚¨ãƒƒã‚¯ã‚¹ | Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.1 |
| $h(X)$ | ã‚¹ãƒ¢ãƒ¼ãƒ« ã‚¨ã‚¤ãƒ | å¾®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.2 |
| $D_\text{KL}(p \| q)$ | ã‚±ãƒ¼ã‚¨ãƒ« ãƒ”ãƒ¼ ãƒ‘ãƒ©ãƒ¬ãƒ« ã‚­ãƒ¥ãƒ¼ | KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç¾© 3.3 |
| $H(p, q)$ | ã‚¨ã‚¤ãƒ ãƒ”ãƒ¼ ã‚­ãƒ¥ãƒ¼ | Cross-Entropy | å®šç¾© 3.4 |
| $H(Y \mid X)$ | ã‚¨ã‚¤ãƒ ãƒ¯ã‚¤ ãƒãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ | æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.5 |
| $I(X; Y)$ | ã‚¢ã‚¤ ã‚¨ãƒƒã‚¯ã‚¹ ãƒ¯ã‚¤ | ç›¸äº’æƒ…å ±é‡ | å®šç¾© 3.6 |
| $D_f(p \| q)$ | ãƒ‡ã‚£ãƒ¼ ã‚¨ãƒ• | f-Divergence | å®šç¾© 3.7 |
| $\eta$ | ã‚¤ãƒ¼ã‚¿ | å­¦ç¿’ç‡ | å®šç¾© 3.8 |
| $\nabla_\theta$ | ãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ | $\theta$ ã«é–¢ã™ã‚‹å‹¾é… | å®šç¾© 3.8 |
| $\beta_1, \beta_2$ | ãƒ™ãƒ¼ã‚¿ | Adam ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¸›è¡°ç‡ | å®šç¾© 3.10 |
| $m_t, v_t$ | ã‚¨ãƒ , ãƒ–ã‚¤ | 1æ¬¡/2æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¨å®š | å®šç¾© 3.10 |
| $\hat{m}_t, \hat{v}_t$ | ã‚¨ãƒ ãƒãƒƒãƒˆ, ãƒ–ã‚¤ãƒãƒƒãƒˆ | ãƒã‚¤ã‚¢ã‚¹è£œæ­£æ¸ˆã¿æ¨å®š | å®šç¾© 3.10 |
| $\epsilon$ | ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ | æ•°å€¤å®‰å®šåŒ–é … | å®šç¾© 3.10 |
| $\text{PPL}$ | ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ | $2^{H}$ | Zone 0 |
| $D_\text{JS}(p \| q)$ | ã‚¸ã‚§ãƒ¼ã‚¨ã‚¹ | Jensen-Shannon ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | 3.11d |
| $W_1(p, q)$ | ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ ãƒ¯ãƒ³ | Wasserstein-1 è·é›¢ | 3.11d |
| $\Pi(\mu, \nu)$ | ãƒ‘ã‚¤ | ã‚«ãƒƒãƒ—ãƒªãƒ³ã‚°ã®é›†åˆ | 3.11d |
| $\text{Lip}(f)$ | ãƒªãƒ—ã‚·ãƒƒãƒ„ | Lipschitz å®šæ•° | 3.11c |
| $\sigma_\max(W)$ | ã‚·ã‚°ãƒ ãƒãƒƒã‚¯ã‚¹ | ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ï¼ˆæœ€å¤§ç‰¹ç•°å€¤ï¼‰ | 3.11c |
| $\nabla^2 f$ | ãƒŠãƒ–ãƒ© ãƒ‹ã‚¸ãƒ§ã‚¦ | ãƒ˜ã‚·ã‚¢ãƒ³è¡Œåˆ— | 3.11b |
| $D_f(p \| q)$ | ãƒ‡ã‚£ãƒ¼ã‚¨ãƒ• | f-Divergence ã®ä¸€èˆ¬å½¢ | å®šç¾© 3.7 |
| $f^*$ | ã‚¨ãƒ•ã‚¹ã‚¿ãƒ¼ | å‡¸å…±å½¹ï¼ˆFenchel å…±å½¹ï¼‰ | 3.6 |
| $\lambda$ | ãƒ©ãƒ ãƒ€ | ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•° / é‡ã¿æ¸›è¡°ä¿‚æ•° | 4.8 |
| $\kappa(A)$ | ã‚«ãƒƒãƒ‘ | æ¡ä»¶æ•° | 5.10 |
| $\mathcal{F}(p)$ | ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼ | Fisher æƒ…å ±é‡ | 6.3 |
| $g_\text{ij}$ | ã‚¸ãƒ¼ã‚¢ã‚¤ã‚¸ã‚§ãƒ¼ | Fisher æƒ…å ±è¡Œåˆ—ã® $(i,j)$ æˆåˆ† | 6.3 |
| $R(D)$ | ã‚¢ãƒ¼ãƒ« ãƒ‡ã‚£ãƒ¼ | Rate-Distortion é–¢æ•° | 6.2 |
| $T$ | ãƒ†ã‚£ãƒ¼ | Temperatureï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦ï¼‰ | Q8 |

---

## å®Ÿè·µãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

:::details æƒ…å ±ç†è«–ã®è¨ˆç®—ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆï¼ˆå°åˆ·ç”¨ï¼‰

**åŸºæœ¬å…¬å¼**

$$H(X) = -\sum_x p(x) \log_2 p(x)$$

$$D_\text{KL}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)} \geq 0$$

$$H(p, q) = H(p) + D_\text{KL}(p \| q)$$

$$I(X; Y) = H(X) + H(Y) - H(X, Y) = H(X) - H(X|Y)$$

**ç‰¹æ®Šã‚±ãƒ¼ã‚¹ï¼ˆè¦šãˆã‚‹ã¹ãå€¤ï¼‰**

| åˆ†å¸ƒ | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ |
|:-----|:------------|
| å…¬å¹³ãªã‚³ã‚¤ãƒ³ | $H = 1$ bit |
| å…¬å¹³ãª6é¢ãƒ€ã‚¤ã‚¹ | $H = \log_2 6 \approx 2.585$ bits |
| ç¢ºå®šçš„ï¼ˆ$p=1$ï¼‰ | $H = 0$ bits |
| $n$ å€‹ã®ç­‰ç¢ºç‡ | $H = \log_2 n$ bits |
| ã‚¬ã‚¦ã‚¹ $\mathcal{N}(\mu, \sigma^2)$ | $h = \frac{1}{2}\log_2(2\pi e \sigma^2)$ bits |

**æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ—©è¦‹è¡¨**

| ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | æ›´æ–°å‰‡ï¼ˆç°¡ç•¥ï¼‰ | ãƒ¡ãƒ¢ãƒª | æ¨å¥¨å ´é¢ |
|:------------|:--------------|:-------|:---------|
| SGD | $\theta \leftarrow \theta - \eta g$ | $O(d)$ | å‡¸å•é¡Œã€ç†è«–è§£æ |
| Momentum | $v \leftarrow \beta v + g$; $\theta \leftarrow \theta - \eta v$ | $O(2d)$ | ç”»åƒåˆ†é¡ï¼ˆSGD+Mï¼‰ |
| Adam | $m, v$ ã®æŒ‡æ•°ç§»å‹•å¹³å‡ + ãƒã‚¤ã‚¢ã‚¹è£œæ­£ | $O(3d)$ | æ±ç”¨ã€LLM è¨“ç·´ |
| AdamW | Adam + åˆ†é›¢å‹é‡ã¿æ¸›è¡° | $O(3d)$ | **LLM è¨“ç·´ã®æ¨™æº–** |

**KL ã®æ–¹å‘ï¼šè¦šãˆæ–¹**

```
å‰å‘ã KL: D_KL(p_data || q_model) â†’ mode-coveringï¼ˆå®‰å…¨ã ãŒæ›–æ˜§ï¼‰
é€†å‘ã KL: D_KL(q_model || p_data) â†’ mode-seekingï¼ˆé®®æ˜ã ãŒåã‚Šï¼‰

VAE = å‰å‘ã KL â†’ ã¼ã‚„ã‘ã‚‹ãŒå…¨ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼
GAN = é€†å‘ã KL â†’ é®®æ˜ã ãŒãƒ¢ãƒ¼ãƒ‰å´©å£Šã®ãƒªã‚¹ã‚¯
```

**Python ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼é›†**

```python
# ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
H = -np.sum(p * np.log2(p + 1e-12))

# KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
KL = np.sum(p * np.log(p / (q + 1e-12) + 1e-12))

# Cross-Entropy
CE = -np.sum(p * np.log2(q + 1e-12))

# ç›¸äº’æƒ…å ±é‡ï¼ˆé›¢æ•£ï¼‰
MI = np.sum(pxy * np.log(pxy / (px[:, None] * py[None, :]) + 1e-12))

# Perplexity
PPL = 2 ** H
```
:::

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
