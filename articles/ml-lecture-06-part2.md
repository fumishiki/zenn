---
title: "ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ“¡"
type: "tech"
topics: ["machinelearning", "deeplearning", "informationtheory", "statistics", "python"]
published: true
time_estimate: "90 minutes"
slug: "ml-lecture-06-part2"
difficulty: "intermediate"
languages: ["Python"]
keywords: ["ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼", "KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹", "Adam", "å‹¾é…é™ä¸‹æ³•", "æƒ…å ±ç†è«–"]
---

> **Note:** **ğŸ“˜ æœ¬è¨˜äº‹ã¯å¾Œç·¨ï¼ˆå®Ÿè£…ç·¨ï¼‰ã§ã™**: [å‰ç·¨ï¼ˆç†è«–ç·¨ï¼‰ã¯ã“ã¡ã‚‰](/articles/ml-lecture-06-part1)

---

## ğŸ’» Z5. è©¦ç·´ï¼ˆ60åˆ†ï¼‰â€” æ•°å¼ã‚’ã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã™ã‚‹æŠ€è¡“

### Z5.1 ç’°å¢ƒæ§‹ç¯‰

```bash
pip install numpy matplotlib
```

æœ¬è¬›ç¾©ã¯ Python 90% ã§é€²ã‚€ã€‚NumPy ã®ã¿ã§å…¨ã¦å®Ÿè£…ã™ã‚‹ã€‚æ¯”è¼ƒï¼ˆPyTorch ç­‰ï¼‰ã¯æŠ˜ã‚Šç•³ã¿ã§è£œè¶³ã™ã‚‹ã€‚

### Z5.2 æƒ…å ±ç†è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…

Z4 ã§å°å‡ºã—ãŸå…¨ã¦ã®æƒ…å ±é‡ã‚’ã€1ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦å®Ÿè£…ã™ã‚‹ã€‚

```math
H(p)=-\\sum_i p_i\\log p_i

H(p,q)=-\\sum_i p_i\\log q_i

D_\\mathrm{KL}(p\\|q)=\\sum_i p_i\\log\\frac{p_i}{q_i}

H(p,q)=H(p)+D_\\mathrm{KL}(p\\|q)
```

```python
import numpy as np

def entropy(p: np.ndarray) -> float:
    p = np.asarray(p, dtype=np.float64)
    p = p[p > 0]
    return float(-np.sum(p * np.log(p)))

def cross_entropy(p: np.ndarray, q: np.ndarray) -> float:
    p = np.asarray(p, dtype=np.float64)
    q = np.asarray(q, dtype=np.float64)
    mask = p > 0
    return float(-np.sum(p[mask] * np.log(q[mask])))

def kl_divergence(p: np.ndarray, q: np.ndarray) -> float:
    p = np.asarray(p, dtype=np.float64)
    q = np.asarray(q, dtype=np.float64)
    mask = (p > 0) & (q > 0)
    return float(np.sum(p[mask] * np.log(p[mask] / q[mask])))

p = np.array([0.4, 0.3, 0.2, 0.1])
q = np.array([0.25, 0.25, 0.25, 0.25])

H = entropy(p)
CE = cross_entropy(p, q)
KL = kl_divergence(p, q)

print(f"H(p)={H:.6f}  CE={CE:.6f}  KL={KL:.6f}")
assert np.isclose(H + KL, CE)
assert kl_divergence(p, p) >= -1e-12
```

**ç›¸äº’æƒ…å ±é‡ â€” ç‹¬ç«‹æ€§ã®å®šé‡åŒ–**

```math
I(X;Y) = D_{KL}(p(x,y) \| p(x)p(y)) = H(X) + H(Y) - H(X,Y)
```

è¨˜å·â†”å¤‰æ•°: åŒæ™‚åˆ†å¸ƒ `$p(x,y)$` = `joint`, å‘¨è¾ºåˆ†å¸ƒ `$p(x)$` = `px = joint.sum(axis=1)`, `$H(X,Y)$` = `entropy(joint.ravel())`ã€‚

**shape**: `joint` ã¯ `(|X|, |Y|)` ã®ç¢ºç‡è¡Œåˆ—ï¼ˆç·å’Œ=1ï¼‰ã€‚`px` ã¯ `(|X|,)`, `py` ã¯ `(|Y|,)`ã€‚

```python
def mutual_information(joint: np.ndarray) -> float:
    """
    I(X;Y) = H(X) + H(Y) - H(X,Y)
    joint: (|X|, |Y|) joint distribution, sums to 1
    """
    px = joint.sum(axis=1)  # shape: (|X|,)
    py = joint.sum(axis=0)  # shape: (|Y|,)
    # H(X) = -sum px * log px
    Hx = entropy(px)
    Hy = entropy(py)
    Hxy = entropy(joint.ravel())  # H(X,Y)
    return float(Hx + Hy - Hxy)

# æ¤œç®—1: ç‹¬ç«‹åˆ†å¸ƒã§ã¯ I(X;Y) = 0
joint_indep = np.outer([0.5, 0.5], [0.3, 0.7])  # p(x)p(y)
mi_indep = mutual_information(joint_indep)
print(f"I(X;Y) independent: {mi_indep:.8f}")  # â‰ˆ 0
assert abs(mi_indep) < 1e-10

# æ¤œç®—2: å®Œå…¨ç›¸é–¢ã§ã¯ I(X;Y) = H(X)
joint_corr = np.array([[0.5, 0.0], [0.0, 0.5]])  # X = Y
mi_corr = mutual_information(joint_corr)
px = joint_corr.sum(axis=1)
print(f"I(X;Y) perfect corr: {mi_corr:.6f}, H(X): {entropy(px):.6f}")  # must match
assert np.isclose(mi_corr, entropy(px))
```

è½ã¨ã—ç©´: `joint.ravel()` ã§ `H(X,Y)` ã‚’è¨ˆç®—ã™ã‚‹éš›ã€`joint` ã®è¦ç´ ãŒ 0 ã§ã‚‚ `entropy()` ã® `+ eps` ãŒæ©Ÿèƒ½ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã›ã‚ˆã€‚è¡Œåˆ—ãŒç–ãªå ´åˆï¼ˆ0 æˆåˆ†ãŒå¤šã„ï¼‰ã¯ log(0) ãŒå‡ºã‚„ã™ã„ã€‚

### Z5.3 æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®çµ±ä¸€å®Ÿè£…

SGDã€Momentumã€Adamã€AdamW ã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§å®Ÿè£…ã™ã‚‹ã€‚

```math
g_t=\\nabla_\\theta L(\\theta_t)

m_t=\\beta_1 m_{t-1}+(1-\\beta_1)g_t
\\quad
v_t=\\beta_2 v_{t-1}+(1-\\beta_2)g_t^2

\\hat m_t=\\frac{m_t}{1-\\beta_1^t}
\\quad
\\hat v_t=\\frac{v_t}{1-\\beta_2^t}

\\theta_{t+1}=\\theta_t-\\eta\\,\\frac{\\hat m_t}{\\sqrt{\\hat v_t}+\\epsilon}
```

```python
import numpy as np

def adam_step(theta: np.ndarray, g: np.ndarray, state: dict, lr: float = 1e-3,
              beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8) -> tuple[np.ndarray, dict]:
    t = state.get('t', 0) + 1
    m = state.get('m', np.zeros_like(theta))
    v = state.get('v', np.zeros_like(theta))

    m = beta1 * m + (1 - beta1) * g
    v = beta2 * v + (1 - beta2) * (g * g)

    m_hat = m / (1 - beta1**t)
    v_hat = v / (1 - beta2**t)
    theta = theta - lr * m_hat / (np.sqrt(v_hat) + eps)

    return theta, {'t': t, 'm': m, 'v': v}

theta = np.zeros(3)
state = {}
g = np.array([1.0, -2.0, 0.5])  # pretend gradient
theta2, state = adam_step(theta, g, state, lr=1e-2)
print('adam step:', theta2)
```

**AdamW â€” é‡ã¿æ¸›è¡°ã®åˆ†é›¢**

Adam ã®æœ‰åãªè½ã¨ã—ç©´: `$L_2$` æ­£å‰‡åŒ–ï¼ˆ`weight_decay`ï¼‰ã‚’å‹¾é…ã«ä¹—ã›ã‚‹ã¨é©å¿œå­¦ç¿’ç‡ã«ã‚ˆã£ã¦æ­£å‰‡åŒ–ãŒä¸å‡ä¸€ã«ãªã‚‹ã€‚AdamW ã¯ã“ã‚Œã‚’ä¿®æ­£ã™ã‚‹ã€‚

```math
\text{Adam + L2:} \quad g_t \leftarrow g_t + \lambda \theta_t \quad\text{ï¼ˆå­¦ç¿’ç‡ã«ä¾å­˜ã™ã‚‹æ­£å‰‡åŒ–ï¼‰}
```

```math
\text{AdamW:} \quad \theta_{t+1} = \theta_t - \eta\,\frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon} - \eta \lambda \theta_t \quad\text{ï¼ˆå­¦ç¿’ç‡ã«æ¯”ä¾‹ã™ã‚‹æ­£å‰‡åŒ–ï¼‰}
```

è¨˜å·â†”å¤‰æ•°: `$\lambda$` = `weight_decay`, `$\theta$` = `theta`, ç¬¬2é …ãŒ pure weight decayã€‚

**shape**: å…¨ãƒ™ã‚¯ãƒˆãƒ«ã¯ `(d,)` â€” Adam step ã¨åŒã˜ã€‚AdamW ã®å·®åˆ†ã¯æœ€å¾Œã® `-lr * wd * theta` ã®1è¡Œã ã‘ã€‚

```python
def adamw_step(theta: np.ndarray, g: np.ndarray, state: dict,
               lr: float = 1e-3, beta1: float = 0.9, beta2: float = 0.999,
               eps: float = 1e-8, weight_decay: float = 1e-2) -> tuple[np.ndarray, dict]:
    # weight decay BEFORE gradient accumulation (decoupled)
    theta = theta * (1.0 - lr * weight_decay)  # pure weight decay term
    t = state.get('t', 0) + 1
    m = state.get('m', np.zeros_like(theta))
    v = state.get('v', np.zeros_like(theta))
    m = beta1 * m + (1 - beta1) * g          # g has NO weight decay added
    v = beta2 * v + (1 - beta2) * (g * g)
    mh = m / (1 - beta1**t)
    vh = v / (1 - beta2**t)
    theta = theta - lr * mh / (np.sqrt(vh) + eps)
    return theta, {'t': t, 'm': m, 'v': v}

# æ¤œç®—: weight_decay=0 ã®å ´åˆã¯ adam ã¨ç­‰ä¾¡
theta = np.zeros(3); state = {}
g = np.array([1.0, -2.0, 0.5])
theta_wd0, _ = adamw_step(theta.copy(), g, {}, weight_decay=0.0)
theta_adam2, _ = adam_step(theta.copy(), g, {}, lr=1e-3)
assert np.allclose(theta_wd0, theta_adam2), "AdamW with wd=0 should match Adam"
print("AdamW wd=0 matches Adam âœ“")
```

è½ã¨ã—ç©´: PyTorch ã® `AdamW` ã¯ `weight_decay` ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãŒ 0ã€‚LLM è¨“ç·´ã§ã¯é€šå¸¸ `0.1` ã‚’è¨­å®šã™ã‚‹ã€‚å¿˜ã‚Œã‚‹ã¨æ­£å‰‡åŒ–ãªã—ã§è¨“ç·´ã—ã€æ±åŒ–æ€§èƒ½ãŒä¸‹ãŒã‚‹ã€‚

### Z5.4 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ7ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

| # | æ•°å¼ãƒ‘ã‚¿ãƒ¼ãƒ³ | Python ãƒ‘ã‚¿ãƒ¼ãƒ³ | ä¾‹ |
|:--|:-----------|:--------------|:---|
| 1 | `$\sum_{x} p(x) f(x)$` | `np.sum(p * f(x))` | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ |
| 2 | `$\log \frac{a}{b}$` | `np.log(a / b)` or `np.log(a) - np.log(b)` | KL |
| 3 | `$\mathbb{E}_{x \sim p}[f(x)]$` | `np.mean(f(samples))` | Monte Carlo æ¨å®š |
| 4 | `$\frac{\partial}{\partial \theta} f$` | æ•°å€¤å¾®åˆ†: `(f(Î¸+Îµ) - f(Î¸-Îµ))/(2Îµ)` | å‹¾é…æ¤œè¨¼ |
| 5 | `$\beta v + (1-\beta) g$` | `v = beta * v + (1-beta) * g` | æŒ‡æ•°ç§»å‹•å¹³å‡ |
| 6 | `$\frac{m}{1 - \beta^t}$` | `m / (1 - beta**t)` | ãƒã‚¤ã‚¢ã‚¹è£œæ­£ |
| 7 | `$\frac{a}{\sqrt{b} + \epsilon}$` | `a / (np.sqrt(b) + eps)` | Adam æ›´æ–° |

<details><summary>PyTorch ã¨ã®å¯¾å¿œ</summary>

æœ¬å®Ÿè£…ã®ã‚³ãƒ¼ãƒ‰ã¨ PyTorch ã®å¯¾å¿œ:

| æœ¬å®Ÿè£… | PyTorch | å‚™è€ƒ |
|:-------|:--------|:-----|
| `entropy(p)` | `torch.distributions.Categorical(probs).entropy()` | nats ã§ã¯ãªã bits ã®å ´åˆã¯ `/math.log(2)` |
| `kl_divergence(p, q)` | `torch.nn.functional.kl_div(q.log(), p, reduction='sum')` | å¼•æ•°é †æ³¨æ„: `kl_div(input, target)` |
| `mutual_information(joint)` | æ‰‹å®Ÿè£…ãŒå¿…è¦ï¼ˆPyTorch æ¨™æº–ãªã—ï¼‰ | `torchinfo` ã‚„ `dit` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†ã“ã¨ã‚‚ |
| `adam_step(...)` | `torch.optim.Adam(params, lr=1e-3)` | å†…éƒ¨å®Ÿè£…ã¯ã»ã¼åŒã˜; `amsgrad` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚ã‚Š |
| `adamw_step(...)` | `torch.optim.AdamW(params, lr=1e-3, weight_decay=0.01)` | PyTorch 1.14 ä»¥é™ã¯ `fused=True` ã§é«˜é€ŸåŒ– |
| `clip_grad_norm(g, 1.0)` | `torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)` | in-place æ“ä½œ |

</details>

### Z5.5 æœ€é©åŒ–ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã®å¯è¦–åŒ–

äºŒæ¬¡æå¤±é–¢æ•° `$L(\theta_1, \theta_2) = \theta_1^2 + 100\theta_2^2$`ï¼ˆæ¡ä»¶æ•° = 100ï¼‰ã®ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã‚’å¯è¦–åŒ–ã—ã€SGD vs Adam ã®åæŸçµŒè·¯ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

```math
L(\theta) = \theta_1^2 + \kappa\,\theta_2^2, \qquad \kappa = \frac{\lambda_{\max}}{\lambda_{\min}} = 100
```

è¨˜å·â†”å¤‰æ•°: `$\theta_1$` = `theta1`, `$\theta_2$` = `theta2`, `$\kappa$` = `kappa`, å­¦ç¿’ç‡ `$\eta$` = `lr`

**shape**: `theta` ã¯ `(2,)` ã®ãƒ™ã‚¯ãƒˆãƒ«ã€‚`grad` ã‚‚åŒã˜ shapeã€‚

```python
import numpy as np

def loss(theta: np.ndarray, kappa: float = 100.0) -> float:
    return float(theta[0]**2 + kappa * theta[1]**2)

def grad_loss(theta: np.ndarray, kappa: float = 100.0) -> np.ndarray:
    # grad shape: (2,) â€” same as theta
    return np.array([2 * theta[0], 2 * kappa * theta[1]])

# SGD trajectory
theta_sgd = np.array([1.0, 1.0])
lr_sgd = 0.009  # must be < 1/kappa = 0.01
history_sgd = [theta_sgd.copy()]
for _ in range(100):
    theta_sgd = theta_sgd - lr_sgd * grad_loss(theta_sgd)
    history_sgd.append(theta_sgd.copy())

# Adam trajectory
theta_adam = np.array([1.0, 1.0])
m, v, t = np.zeros(2), np.zeros(2), 0
history_adam = [theta_adam.copy()]
for _ in range(100):
    t += 1
    g = grad_loss(theta_adam)
    m = 0.9 * m + 0.1 * g
    v = 0.999 * v + 0.001 * g**2
    mh = m / (1 - 0.9**t)
    vh = v / (1 - 0.999**t)
    theta_adam = theta_adam - 0.1 * mh / (np.sqrt(vh) + 1e-8)
    history_adam.append(theta_adam.copy())

print(f"SGD final loss:  {loss(np.array(history_sgd[-1])):.6f}")
print(f"Adam final loss: {loss(np.array(history_adam[-1])):.6f}")
# assert Adam converges faster
assert loss(np.array(history_adam[-1])) < loss(np.array(history_sgd[-1]))
```

æ¤œç®—: æ¡ä»¶æ•° = 100 ã®å•é¡Œã§ã¯ã€SGD ãŒæœ€é©å­¦ç¿’ç‡ `$\eta < 1/100$` ã§åæŸã™ã‚‹ãŒé…ã„ã€‚Adam ã¯å„æ–¹å‘ã«ç‹¬ç«‹ãªå­¦ç¿’ç‡ã§ `$\theta_2$` æ–¹å‘ã‚‚é€ŸãåæŸã™ã‚‹ã€‚

### Z5.6 Python ã®é…ã•ã‚’ä½“æ„Ÿã™ã‚‹ â€” `%timeit` ã®è¡æ’ƒ

ã“ã“ã§ä¸ç©ãªè¨ˆæ¸¬ã‚’è¡Œã†ã€‚

```python
import time
import numpy as np

# 10,000 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã® SGD â€” Python ãƒ«ãƒ¼ãƒ—ç‰ˆ vs NumPy ç‰ˆ
d = 10_000
theta = np.random.randn(d)
grad = np.random.randn(d)
lr = 0.01
n_steps = 1000

# Python ãƒ«ãƒ¼ãƒ— (æ•…æ„ã«é…ã„å®Ÿè£…)
start = time.perf_counter()
for _ in range(n_steps):
    for i in range(d):
        theta[i] -= lr * grad[i]  # è¦ç´ ã”ã¨ã®ãƒ«ãƒ¼ãƒ—
elapsed_python = time.perf_counter() - start

# NumPy ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—
theta2 = np.random.randn(d)
start = time.perf_counter()
for _ in range(n_steps):
    theta2 -= lr * grad  # ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—
elapsed_numpy = time.perf_counter() - start

print(f"Python loop: {elapsed_python:.3f}s")
print(f"NumPy:       {elapsed_numpy:.4f}s")
print(f"Speedup:     {elapsed_python / elapsed_numpy:.0f}x")
# å…¸å‹çš„ãªå‡ºåŠ›: Python = 3-10s, NumPy = 0.01-0.05s, Speedup = 100-500x
```

ã“ã®å·® 100-500x ã‚’è¦‹ã¦ã€ŒNumPy ã§ã„ã„ã˜ã‚ƒãªã„ã‹ã€ã¨æ€ã†ã‹ã‚‚ã—ã‚Œãªã„ã€‚ã ãŒå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã¯:
- GPT-3: 1,750 å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ`$d = 1.75 \times 10^{11}$`ï¼‰
- 1ã‚¹ãƒ†ãƒƒãƒ—ã®è¡Œåˆ—ç©: `$O(d^2)$`

NumPy ã§ã•ãˆ100å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç„¡ç†ã ã€‚GPU + CUDA ãŒå¿…è¦ã«ãªã‚‹ç†ç”±ãŒã“ã“ã«ã‚ã‚‹ã€‚

> **âš ï¸ Warning:** ã“ã“ã§ `%timeit` ã®çµæœã‚’è¦³å¯Ÿã—ã¦ã»ã—ã„ã€‚10,000ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã® SGD ãƒ«ãƒ¼ãƒ—ãŒ Python ã§ã©ã‚Œã ã‘é…ã„ã‹ã€‚å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã¯æ•°ç™¾ä¸‡ã€œæ•°åå„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã€‚ã“ã®ã€Œé…ã•ã€ã¯ç¬¬7å›ã§ MLE ã®åå¾©è¨ˆç®—ã§å¢—å¹…ã—ã€ç¬¬8å›ã® EM ç®—æ³•ã§ã€Œ**é…ã™ããªã„ï¼Ÿ**ã€ã¨ã„ã†å•ã„ãŒç¢ºä¿¡ã«å¤‰ã‚ã‚‹ã€‚ç¬¬9å›ã§ Julia ãŒç™»å ´ã™ã‚‹ä¼ç·šãŒã“ã“ã«ã‚ã‚‹ã€‚

### Z5.7 å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã¨æ•°å€¤å®‰å®šæ€§

å®Ÿéš›ã®è¨“ç·´ã§ã¯å‹¾é…ãŒçˆ†ç™ºã™ã‚‹å•é¡Œã«å¯¾å‡¦ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚


**å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®æ•°å¼**:

```math
g \leftarrow \min\left(1, \frac{\tau}{\|g\|}\right) g, \quad \|g\| = \sqrt{\sum_i g_i^2}
```

è¨˜å·â†”å¤‰æ•°: `$g$` = `grad`ï¼ˆå‹¾é…ãƒ™ã‚¯ãƒˆãƒ«ï¼‰, `$\tau$` = `max_norm`ï¼ˆé–¾å€¤ï¼‰, `$\|g\|$` = `norm = np.linalg.norm(grad)`ã€‚ã‚¯ãƒªãƒƒãƒ—ä¿‚æ•° `$\min(1, \tau/\|g\|)$` ã¯ `$\|g\| \leq \tau$` ãªã‚‰ 1ï¼ˆç„¡å¤‰åŒ–ï¼‰ã€è¶…ãˆãŸã¨ãã®ã¿ç¸®å°ã€‚

```python
def clip_grad_norm(grad: np.ndarray, max_norm: float = 1.0) -> np.ndarray:
    # grad shape: (d,) â€” any 1D gradient vector
    norm = float(np.linalg.norm(grad))
    if norm > max_norm:
        grad = grad * (max_norm / norm)  # scale down
    return grad

# æ¤œç®—: ãƒãƒ«ãƒ  10 ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ max_norm=1 ã§ã‚¯ãƒªãƒƒãƒ—
g = np.array([6.0, 8.0])  # ||g|| = 10
g_clipped = clip_grad_norm(g, max_norm=1.0)
print(f"Before: ||g||={np.linalg.norm(g):.1f}")
print(f"After:  ||g||={np.linalg.norm(g_clipped):.1f}")  # should be 1.0
assert np.isclose(np.linalg.norm(g_clipped), 1.0)
```

**å‹¾é…çˆ†ç™ºã®åŸå› **: æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã¯é€†ä¼æ’­ä¸­ã«å‹¾é…ãŒä¹—ç®—ã•ã‚Œã¦ã„ãã€‚å„å±¤ã®å‹¾é…ãŒ1ã‚ˆã‚Šå¤§ãã„ã¨æŒ‡æ•°çš„ã«å¢—å¤§ï¼ˆçˆ†ç™ºï¼‰ã€1ã‚ˆã‚Šå°ã•ã„ã¨æŒ‡æ•°çš„ã«æ¸›å°‘ï¼ˆæ¶ˆå¤±ï¼‰ã™ã‚‹ã€‚ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã¯çˆ†ç™ºã‚’é˜²ãå¯¾ç—‡ç™‚æ³•ã§ã‚ã‚Šã€æ¶ˆå¤±ã«ã¯åˆ¥ã®å¯¾ç­–ï¼ˆæ®‹å·®æ¥ç¶šã€æ­£è¦åŒ–ï¼‰ãŒå¿…è¦ã€‚

**æ··åˆç²¾åº¦è¨“ç·´ã®æ¦‚è¦ï¼ˆfp16/bf16/fp8ï¼‰**:

| ç²¾åº¦ | ãƒ“ãƒƒãƒˆæ•° | ç¯„å›² | ç”¨é€” |
|:-----|:---------|:-----|:-----|
| fp32 | 32 | `$\pm 3.4 \times 10^{38}$` | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜ï¼ˆãƒã‚¹ã‚¿ãƒ¼ã‚³ãƒ”ãƒ¼ï¼‰ |
| fp16 | 16 | `$\pm 65504$` | é †ä¼æ’­ãƒ»é€†ä¼æ’­ã®é«˜é€ŸåŒ– |
| bf16 | 16 | `$\pm 3.4 \times 10^{38}$` | fp32åŒæ§˜ã®ç¯„å›²ã€ç²¾åº¦ã¯ä½ã„ |
| fp8 | 8 | é™å®š | Transformer Engine (H100+) |

æ··åˆç²¾åº¦è¨“ç·´ã¯ fp32 ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒã‚¹ã‚¿ãƒ¼ã‚³ãƒ”ãƒ¼ã‚’ä¿æŒã—ã¤ã¤ã€é †ä¼æ’­ã¨é€†ä¼æ’­ã‚’ fp16/bf16 ã§è¡Œã†ã€‚è¨ˆç®—é€Ÿåº¦ãŒ2-3å€ã«ãªã‚Šã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåŠæ¸›ã™ã‚‹ã€‚Loss scalingï¼ˆæå¤±ã«å¤§ããªå®šæ•°ã‚’æ›ã‘ã¦ã‹ã‚‰é€†ä¼æ’­ã—ã€å‹¾é…æ›´æ–°æ™‚ã«æˆ»ã™ï¼‰ã§ fp16 ã®ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼å•é¡Œã‚’å›é¿ã™ã‚‹ã€‚

### Z5.8 ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³• â€” åˆ¶ç´„ä»˜ãæœ€é©åŒ–

æ©Ÿæ¢°å­¦ç¿’ã§é »å‡ºã™ã‚‹åˆ¶ç´„ä»˜ãæœ€é©åŒ–ã®åŸºæœ¬ã‚’æŠ¼ã•ãˆã‚‹ã€‚

**å•é¡Œè¨­å®š**: `$g(\theta) = 0$` ã®åˆ¶ç´„ä¸‹ã§ `$f(\theta)$` ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

```math
\min_\theta f(\theta) \quad \text{s.t.} \quad g(\theta) = 0
```

**ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ã‚¢ãƒ³**:

```math
\mathcal{L}(\theta, \lambda) = f(\theta) + \lambda g(\theta)
```

**æœ€é©æ€§æ¡ä»¶ï¼ˆKKTæ¡ä»¶ã®ç­‰å¼åˆ¶ç´„ç‰ˆï¼‰**:

```math
\nabla_\theta \mathcal{L} = 0, \quad \nabla_\lambda \mathcal{L} = 0
```


ã“ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åŸç†ã¯çµ±è¨ˆç‰©ç†ã® Boltzmann åˆ†å¸ƒã¨åŒä¸€ã§ã‚ã‚Šã€ç¬¬27å›ï¼ˆEBMï¼‰ã§å†ç™»å ´ã™ã‚‹ã€‚åˆ¶ç´„ãªã—ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = ä¸€æ§˜åˆ†å¸ƒã€å¹³å‡ã®åˆ¶ç´„ã¤ã = æŒ‡æ•°åˆ†å¸ƒæ—ã€‚**æƒ…å ±ç†è«–ã¨çµ±è¨ˆç‰©ç†ã¯åŒã˜æ•°å­¦ã§ç¹‹ãŒã£ã¦ã„ã‚‹**ã€‚

**æ•°å€¤å®Ÿè£…: æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒã®æ¤œè¨¼**

2ç‚¹åˆ†å¸ƒ `$p = (p_1, 1-p_1)$` ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å•é¡Œ (`$\sum p_i = 1$`) ã‚’è§£ãã¨ã€è§£æè§£ã¯ `$p_1 = p_2 = 0.5$` ï¼ˆä¸€æ§˜åˆ†å¸ƒï¼‰ã€‚æ•°å€¤æœ€é©åŒ–ã§ç¢ºèª:

```math
\max_{p} H(p) = -\sum_{i=1}^{n} p_i \log p_i \quad \text{s.t.} \quad \sum_{i=1}^n p_i = 1
```

è¨˜å·â†”å¤‰æ•°: `$p_i$` = `p[i]`, ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•° `$\lambda$` = `lambda_` (äºˆç´„èªå›é¿), ç›®çš„é–¢æ•°ã¯æœ€å°åŒ–ãªã®ã§ç¬¦å·åè»¢ã€‚

```python
import numpy as np
from scipy.optimize import minimize

def neg_entropy(p: np.ndarray) -> float:
    # minimize -H(p), i.e., maximize H(p)
    p = np.clip(p, 1e-12, None)
    return float(np.sum(p * np.log(p)))  # -H(p)

def neg_entropy_grad(p: np.ndarray) -> np.ndarray:
    return np.log(np.clip(p, 1e-12, None)) + 1.0  # d(-H)/dp_i = log(p_i) + 1

n = 4
p0 = np.random.dirichlet(np.ones(n))  # feasible starting point
constraint = {'type': 'eq', 'fun': lambda p: p.sum() - 1.0,
               'jac': lambda p: np.ones(n)}

result = minimize(neg_entropy, p0, jac=neg_entropy_grad,
                  constraints=[constraint],
                  bounds=[(0, 1)] * n, method='SLSQP')

print(f"æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒ: {result.x.round(4)}")
print(f"ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å€¤: {-result.fun:.6f}")
print(f"ç†è«–å€¤ log({n}): {np.log(n):.6f}")
assert np.allclose(result.x, 1/n, atol=1e-5), "æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = ä¸€æ§˜åˆ†å¸ƒ âœ“"
```

è½ã¨ã—ç©´: `scipy.optimize.minimize` ã® `SLSQP` ã¯ç­‰å¼åˆ¶ç´„ã‚’æ‰±ãˆã‚‹ã€‚`jacobian` ã‚’æä¾›ã—ãªã„ã¨æ•°å€¤å¾®åˆ†ã«ãªã‚Šé…ã„ã€‚ä¸Šã®å®Ÿè£…ã¯è§£æçš„ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã‚’ä½¿ç”¨ã€‚

### Z5.9 è«–æ–‡èª­è§£ã®æƒ…å ±ç†è«–çš„è¦–ç‚¹

æƒ…å ±ç†è«–ã®é“å…·ã‚’ä½¿ã£ã¦è«–æ–‡ã‚’èª­ã‚€éš›ã®è¦–ç‚¹ã‚’æ•´ç†ã™ã‚‹ã€‚

**3ãƒ‘ã‚¹ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæƒ…å ±ç†è«–ç‰ˆï¼‰**:

```mermaid
graph TD
    P1["Pass 1: Survey<br/>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›<br/>H(ç†è§£) ã‚’å¤§ããä¸‹ã’ã‚‹"] --> P2["Pass 2: Grasp<br/>ç›¸äº’æƒ…å ±é‡<br/>å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã® I(X;Y) ã‚’æ´ã‚€"]
    P2 --> P3["Pass 3: Verify<br/>KL æœ€å°åŒ–<br/>è‡ªåˆ†ã®ç†è§£ã¨è‘—è€…ã®æ„å›³ã® KL ã‚’ 0 ã«"]
```

| ãƒ‘ã‚¹ | ç›®çš„ | æ‰€è¦æ™‚é–“ | èª­ã‚€ç®‡æ‰€ |
|:-----|:-----|:---------|:---------|
| 1 | å…¨ä½“åƒã®æŠŠæ¡ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¤§å¹…å‰Šæ¸›ï¼‰ | 10åˆ† | Title, Abstract, Conclusion, Figures |
| 2 | æ§‹é€ ã®ç†è§£ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ä¾å­˜é–¢ä¿‚ï¼‰ | 30åˆ† | Method, Results, key equations |
| 3 | è©³ç´°ã®æ¤œè¨¼ï¼ˆè‡ªåˆ†ã®ç†è§£ã® KL â†’ 0ï¼‰ | 60åˆ† | å…¨ãƒšãƒ¼ã‚¸ã€å¼ã®å°å‡ºè¿½è·¡ |

**å®Ÿè·µä¾‹: "Adam" è«–æ–‡ã®æƒ…å ±ç†è«–çš„èª­ã¿æ–¹**

Kingma & Ba (2014) [^4] ã‚’3ãƒ‘ã‚¹ã§èª­ã‚€ã€‚

- **Pass 1 (10åˆ†)**: Abstract ã‹ã‚‰ã€Œé©å¿œå­¦ç¿’ç‡ã€ã€Œãƒã‚¤ã‚¢ã‚¹è£œæ­£ã€ã€ŒåæŸè¨¼æ˜ã€ã®3ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã€‚ç†è§£ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒä¸‹ãŒã‚‹ã€‚
- **Pass 2 (30åˆ†)**: Algorithm 1ï¼ˆæ›´æ–°å‰‡ï¼‰ã¨ Theorem 1ï¼ˆåæŸä¿è¨¼ï¼‰ã«é›†ä¸­ã€‚æ•°å¼ã®æ§‹é€ ï¼ˆ`$m_t, v_t, \hat{m}_t, \hat{v}_t, \theta_t$`ï¼‰ã‚’è¿½ã†ã€‚
- **Pass 3 (60åˆ†)**: ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã®å°å‡ºï¼ˆSection 2ï¼‰ã‚’å…¨ã¦æ‰‹ã§è¿½ã†ã€‚`$\mathbb{E}[m_t] = (1-\beta_1^t) \cdot g$` ã®è¨¼æ˜ã‚’è‡ªåˆ†ã§å†ç¾ã€‚

**æƒ…å ±ç†è«–è¦–ç‚¹ã®æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ**: Adam ã®å­¦ç¿’ç‡ `$\eta/(\sqrt{\hat{v}_t}+\epsilon)$` ã¯ Fisher æƒ…å ±è¡Œåˆ—ã®å¯¾è§’è¿‘ä¼¼ `$\mathcal{I}(\theta)^{-1/2}$` ã¨ã—ã¦è§£é‡ˆã§ãã‚‹ï¼ˆPart1 Z4 å‚ç…§ï¼‰ã€‚è«–æ–‡ã«ã“ã®è¨˜è¿°ã¯ãªã„ãŒã€Amari ã®è‡ªç„¶å‹¾é… [^13] ã¨ã®æ¥ç¶šã¯ç†è§£ã«æ·±ã¿ã‚’ä¸ãˆã‚‹ã€‚


### Z5.9b Rate-Distortion ã¨ Î²-VAE ã®æ¥ç¶šæ¼”ç¿’

Part1 ã§æ‰±ã£ãŸ Rate-Distortion ã®ç†è«– `$R(D) = \min_{q(z|x): \mathbb{E}[d(x,z)] \leq D} I(X;Z)$` ãŒã€Î²-VAE ã®ç›®çš„é–¢æ•°ã«ç›´æ¥ç™»å ´ã™ã‚‹ã€‚

```math
\mathcal{L}_{\beta\text{-VAE}} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta D_{KL}(q_\phi(z|x) \| p(z))
```

`$\beta > 1$` ã®å ´åˆ: KL é …ã‚’å¼·ãç½°ã—ã¦ `$I(X;Z)$` ã‚’æœ€å°åŒ– â†’ **è¡¨ç¾ã®åœ§ç¸®**ã‚’å¼·åˆ¶ã€‚
`$\beta = 1$` ã®å ´åˆ: æ¨™æº– VAEï¼ˆç¬¬10å›ï¼‰ã€‚

**æ¼”ç¿’å•é¡Œ**:

1. `$\beta \to \infty$` ã®ã¨ãã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ `$q_\phi(z|x)$` ã¯ã©ã†ãªã‚‹ã‹ï¼Ÿï¼ˆãƒ’ãƒ³ãƒˆ: `$D_{KL}(q \| p) = 0$` ãŒæˆã‚Šç«‹ã¤ã¨ã `$q = p$`ï¼‰
2. `$\beta = 0$` ã®ã¨ãã€ç›®çš„é–¢æ•°ã¯ã©ã†ãªã‚‹ã‹ï¼Ÿï¼ˆãƒ’ãƒ³ãƒˆ: å†æ§‹æˆèª¤å·®ã®ã¿ï¼‰
3. Rate-Distortion ã® `$D$` ã¯ Î²-VAE ã®ã©ã®é …ã«å¯¾å¿œã™ã‚‹ã‹ï¼Ÿ

<details><summary>è§£ç­”</summary>

1. `$\beta \to \infty$`: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ prior `$p(z) = \mathcal{N}(0,I)$` ã«ä¸€è‡´ã—ã€`$q_\phi(z|x)$` ãŒ `$x$` ã«ä¾å­˜ã—ãªããªã‚‹ã€‚æ½œåœ¨å¤‰æ•°ã¯å®Œå…¨ã«åœ§ç¸®ã•ã‚Œã€å†æ§‹æˆã¯ä¸å¯èƒ½ã€‚æƒ…å ±ãŒå…¨ã¦æ¨ã¦ã‚‰ã‚ŒãŸçŠ¶æ…‹ã€‚
2. `$\beta = 0$`: KL é …ãªã—ã€å†æ§‹æˆèª¤å·®ã®ã¿æœ€å¤§åŒ–ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨ãƒ‡ã‚³ãƒ¼ãƒ€ã¯å˜ãªã‚‹ Autoencoder ã«ãªã‚‹ã€‚æ½œåœ¨ç©ºé–“ã«ã¯æ§‹é€ ãŒç”Ÿã¾ã‚Œãªã„ã€‚
3. `$D$` ã¯å†æ§‹æˆèª¤å·® `$\mathbb{E}[\|x - \hat{x}\|^2]$` ã«å¯¾å¿œã€‚`$\beta$` ã¯ Rate-Distortion ã® Lagrange ä¹—æ•° `$\lambda$` ã«å¯¾å¿œã€‚`$\beta$` ã‚’å¢—ã‚„ã™ã“ã¨ã¯ã€Œã‚ˆã‚Šå°‘ãªã„æƒ…å ±ï¼ˆRateï¼‰ã§ç¬¦å·åŒ–ã›ã‚ˆã€ã¨ã„ã†åˆ¶ç´„ã‚’å¼·ãã™ã‚‹ã“ã¨ã¨åŒå€¤ã€‚

</details>

### Z5.10 Gaussian KL é–‰å½¢å¼ã®æ•°å€¤æ¤œè¨¼

Z4ï¼ˆå‰ç·¨ï¼‰ã§å°å‡ºã—ãŸ Gaussian KL ã®é–‰å½¢å¼ã‚’æ•°å€¤çš„ã«ç¢ºèªã™ã‚‹ã€‚

**å¯¾å¿œã™ã‚‹æ•°å¼**:

```math
D_{KL}(\mathcal{N}(\mu_1, \sigma_1^2) \| \mathcal{N}(\mu_2, \sigma_2^2)) = \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}
```

è¨˜å·â†”å¤‰æ•°: `$\mu_1$` = `mu1`, `$\mu_2$` = `mu2`, `$\sigma_1$` = `sigma1`, `$\sigma_2$` = `sigma2`

**shape**: ã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆ1å¤‰é‡ã‚¬ã‚¦ã‚¹ï¼‰ã€‚å¤šå¤‰é‡ã¸ã®æ‹¡å¼µ: `$\sigma^2$` â†’ `$\Sigma$`ï¼ˆå…±åˆ†æ•£è¡Œåˆ—ï¼‰ã€‚

**è½ã¨ã—ç©´**: `$\sigma = 0$` ã®ã¨ã `$\log(0) = -\infty$` ã§ç™ºæ•£ã€‚VAE ã®å®Ÿè£…ã§ã¯ `$\log\sigma^2$` ã‚’ç›´æ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã—ã¦æ•°å€¤å®‰å®šåŒ–ã™ã‚‹ï¼ˆ`sigma2 = exp(log_var)`ï¼‰ã€‚

```python
import numpy as np

def kl_gaussian_closed(mu1: float, sigma1: float,
                        mu2: float, sigma2: float) -> float:
    # Closed-form KL(N(mu1, sigma1^2) || N(mu2, sigma2^2))
    # shape: scalar
    return (np.log(sigma2 / sigma1)
            + (sigma1**2 + (mu1 - mu2)**2) / (2 * sigma2**2)
            - 0.5)

def kl_gaussian_mc(mu1: float, sigma1: float,
                   mu2: float, sigma2: float, n: int = 100_000) -> float:
    # Monte Carlo approximation for verification
    # shape: (n,) samples
    x = np.random.normal(mu1, sigma1, n)
    log_p = -0.5 * ((x - mu1) / sigma1)**2 - np.log(sigma1)
    log_q = -0.5 * ((x - mu2) / sigma2)**2 - np.log(sigma2)
    return float(np.mean(log_p - log_q))

np.random.seed(42)
mu1, sigma1 = 1.0, 2.0
mu2, sigma2 = 0.0, 1.0
closed = kl_gaussian_closed(mu1, sigma1, mu2, sigma2)
mc     = kl_gaussian_mc(mu1, sigma1, mu2, sigma2)
print(f"Closed-form: {closed:.4f}")
print(f"Monte Carlo: {mc:.4f}")
# Must be non-negative and close to each other
assert closed >= 0
assert abs(closed - mc) < 0.05, f"Mismatch: {closed:.4f} vs {mc:.4f}"
```

**VAE ã® KL æ­£å‰‡åŒ–é …**: `$\mu_2 = 0, \sigma_2 = 1$`ï¼ˆæ¨™æº–æ­£è¦äº‹å‰åˆ†å¸ƒï¼‰ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹:

```math
D_{KL}(\mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(0, 1)) = \frac{\mu^2 + \sigma^2 - 1 - \log\sigma^2}{2}
```

ã“ã‚ŒãŒ VAE ã® ELBO æå¤±ã®æ­£å‰‡åŒ–é …ã¨ã—ã¦ç›´æ¥ä½¿ã‚ã‚Œã‚‹ï¼ˆç¬¬9å›ï¼‰ã€‚

### Z5 Quick Check

**ãƒã‚§ãƒƒã‚¯ 1**: Gaussian KL `$D_{KL}(\mathcal{N}(1,4) \| \mathcal{N}(0,1))$` ã®å€¤ã‚’æ‰‹è¨ˆç®—ã›ã‚ˆã€‚

<details><summary>ç­”ãˆ</summary>

`$\mu_1=1, \sigma_1=2, \mu_2=0, \sigma_2=1$` ã‚’å…¬å¼ã«ä»£å…¥:

```math
D_{KL} = \log\frac{1}{2} + \frac{4 + 1}{2 \cdot 1} - \frac{1}{2} = -\log 2 + 2.5 - 0.5 = 2 - \log 2 \approx 1.307
```

ã‚³ãƒ¼ãƒ‰ã§ç¢ºèª: `kl_gaussian_closed(1.0, 2.0, 0.0, 1.0)` â†’ `1.3068...`ï¼ˆ`$2 - \ln 2 = 1.3069$`ï¼‰âœ…
</details>

**ãƒã‚§ãƒƒã‚¯ 2**: Adam ã®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã‚’çœç•¥ã—ãŸã¨ãã€å­¦ç¿’åˆæœŸï¼ˆ`$t=1$`ï¼‰ã®æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—ã«ã©ã‚“ãªå½±éŸ¿ãŒã‚ã‚‹ã‹ï¼Ÿ

<details><summary>ç­”ãˆ</summary>

`$t=1$` ã§ã¯ `$m_1 = (1-\beta_1)g_1$`, `$v_1 = (1-\beta_2)g_1^2$`ã€‚è£œæ­£ãªã—ã®æ›´æ–°é‡:

```math
\frac{m_1}{\sqrt{v_1} + \epsilon} = \frac{(1-\beta_1)g_1}{\sqrt{(1-\beta_2)g_1^2} + \epsilon} \approx \frac{1-\beta_1}{\sqrt{1-\beta_2}} \cdot \text{sign}(g_1)
```

`$\beta_1=0.9, \beta_2=0.999$` ã§ã¯ `$(1-0.9)/\sqrt{1-0.999} \approx 0.1/0.0316 \approx 3.16$`ã€‚è£œæ­£å¾Œï¼ˆ= 1.0 ã«è¿‘ã„ç¬¦å·ãƒ™ãƒ¼ã‚¹æ›´æ–°ï¼‰ã¨æ¯”è¼ƒã—ã¦ã€**è£œæ­£ãªã—ã§ã¯å­¦ç¿’åˆæœŸã«ç´„3å€å¤§ããªã‚¹ãƒ†ãƒƒãƒ—**ã«ãªã‚‹ã€‚ã“ã‚ŒãŒå­¦ç¿’ä¸å®‰å®šã®åŸå› ã€‚
</details>

**ãƒã‚§ãƒƒã‚¯ 3**: Î²-VAE ã®æå¤±é–¢æ•° `$\mathcal{L}_{\beta} = \mathbb{E}[\log p(x|z)] - \beta D_{KL}(q||p)$` ã«ãŠã„ã¦ã€`$\beta = 1$` ã¨ `$\beta = 10$` ã§ä½•ãŒå¤‰ã‚ã‚‹ã‹ï¼ŸRate-Distortion ã®è¦³ç‚¹ã§ç­”ãˆã‚ˆã€‚

<details><summary>ç­”ãˆ</summary>

`$\beta$` ã¯ Rate-Distortion ã® Lagrange ä¹—æ•°ï¼ˆPart1 Z4 å‚ç…§ï¼‰ã€‚

- `$\beta = 1$`: æ¨™æº– VAEã€‚å†æ§‹æˆç²¾åº¦ã¨æƒ…å ±åœ§ç¸®ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒå‡ç­‰ã€‚
- `$\beta = 10$`: KL é …ã‚’ 10 å€ç½°ã™ã‚‹ã€‚æ½œåœ¨å¤‰æ•° `$z$` ãŒæŒã¦ã‚‹æƒ…å ±é‡ `$I(X;Z)$` ãŒæ¿€æ¸› â†’ æ½œåœ¨ç©ºé–“ãŒã‚ˆã‚Šã€Œæ•´ç†ã•ã‚ŒãŸã€æ§‹é€ ã‚’æŒã¤ï¼ˆè§£ãã»ãã—è¡¨ç¾ï¼‰ãŒã€å†æ§‹æˆç²¾åº¦ã¯ä¸‹ãŒã‚‹ã€‚

Rate-Distortion æ›²ç·šä¸Šã®å‹•ã: `$\beta$` ã‚’å¢—ã‚„ã™ã¨ Rate `$= I(X;Z)$` ãŒä¸‹ãŒã‚Šã€Distortion `$= \mathbb{E}[\|x - \hat{x}\|^2]$` ãŒä¸ŠãŒã‚‹ã€‚`$\beta$` ã¯æ›²ç·šä¸Šã®å‹•ä½œç‚¹ã‚’åˆ¶å¾¡ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚
</details>


### Z5.11 Softmax + æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ + Temperature ã®ä¸‰ä½ä¸€ä½“

Softmax ã¯ã€Œæ¸©åº¦ `$T$` ä»˜ãæœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒã€ã®è§£ã¨ã—ã¦å°å‡ºã§ãã‚‹ã€‚Part1 Z4 ã§è¨¼æ˜ã—ãŸçµè«–ã‚’æ•°å€¤ã§ç¢ºèªã™ã‚‹ã€‚

åˆ¶ç´„: `$\mathbb{E}[x] = \mu$`ï¼ˆæœŸå¾…å€¤å›ºå®šï¼‰ã®ä¸‹ã§ `$H(p)$` ã‚’æœ€å¤§åŒ–ã™ã‚‹ã¨è§£ã¯æŒ‡æ•°æ—ã€‚ãƒ­ã‚¸ãƒƒãƒˆã‚’ `$z_i$`ã€æ¸©åº¦ã‚’ `$T$` ã¨ã™ã‚Œã°:

```math
p_i(T) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
```

`$T \to \infty$`ï¼ˆé«˜æ¸©ï¼‰: ä¸€æ§˜åˆ†å¸ƒã€`$H \to \log n$`ï¼ˆæœ€å¤§ï¼‰ã€‚`$T \to 0$`ï¼ˆä½æ¸©ï¼‰: æœ€å¤§ãƒ­ã‚¸ãƒƒãƒˆã«ç¢ºç‡é›†ä¸­ã€`$H \to 0$`ã€‚

è¨˜å·â†”å¤‰æ•°: `$z_i$` = `logits[i]`, `$T$` = `temperature`, `$p_i$` = `probs[i]`ã€‚

```python
def softmax_with_temperature(logits: np.ndarray, temperature: float = 1.0) -> np.ndarray:
    # Numerically stable: subtract max before exp
    z = (logits - logits.max()) / temperature  # shape: (n,)
    e = np.exp(z)
    return e / e.sum()

logits = np.array([2.0, 1.0, 0.5, 0.1])
for T in [0.1, 0.5, 1.0, 2.0, 10.0]:
    p = softmax_with_temperature(logits, T)
    H = -np.sum(p * np.log(p + 1e-12))
    print(f"T={T:4.1f}: max_p={p.max():.3f}, H={H:.4f}")
# T=0.1: almost deterministic, H â‰ˆ 0
# T=10:  almost uniform, H â‰ˆ log(4) â‰ˆ 1.386
```

æ•°å€¤å®‰å®šæ€§: `logits.max()` ã‚’å¼•ã„ã¦ã‚‚ `$\text{softmax}$` ã®å€¤ã¯å¤‰ã‚ã‚‰ãªã„ï¼ˆåˆ†å­ãƒ»åˆ†æ¯ã‚’åŒã˜å®šæ•°ã§å‰²ã‚‹ã“ã¨ã¨åŒã˜ï¼‰ã€‚ã“ã®ã€Œå¼•ãç®—ã€ã‚’å¿˜ã‚Œã‚‹ã¨ `exp(100)` ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã€‚

**ãƒã‚§ãƒƒã‚¯ 4**: Temperature `$T = 0.01$` ã® Softmax ã®å‡ºåŠ› `$p$` ã‚’ `logits = [2, 1, 0, -1]` ã§è¨ˆç®—ã—ã€Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ `$H(p)$` ã‚’æ±‚ã‚ã‚ˆã€‚

<details><summary>ç­”ãˆ</summary>

`$T = 0.01$` ã¯æ¥µä½æ¸©ã€‚`logits / T = [200, 100, 0, -100]`ã€‚Softmax ã®åˆ†å­æ¯”ã¯ `$\exp(0) : \exp(-100) : \exp(-200) : \exp(-300) \approx 1 : 0 : 0 : 0$`ï¼ˆæ¸›ç®—å¾Œï¼‰ã€‚

çµæœ: `$p \approx (1, 0, 0, 0)$`ï¼ˆæœ€å¤§ãƒ­ã‚¸ãƒƒãƒˆã«ç¢ºç‡ 1 é›†ä¸­ï¼‰ã€`$H(p) \approx 0$`ã€‚

```python
logits = np.array([2.0, 1.0, 0.0, -1.0])
p = softmax_with_temperature(logits, T=0.01)
H = -np.sum(p * np.log(p + 1e-12))
print(f"p={p.round(4)}, H={H:.6f}")  # H â‰ˆ 0
```
</details>

> **Note:** **é€²æ—: 70% å®Œäº†** æƒ…å ±ç†è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã—ã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã—ãŸã€‚Python ã®é…ã•ã‚‚ä½“æ„Ÿã—ãŸã€‚

---

## ğŸ”¬ Z5b. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### Z5b.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®è¨˜å·ãƒ»æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

<details><summary>Q1: `$H(X) = -\sum_{x} p(x) \log p(x)$`</summary>
**èª­ã¿æ–¹**: ã‚¨ã‚¤ãƒ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ ã‚·ã‚°ãƒ ã‚¨ãƒƒã‚¯ã‚¹ ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹

**æ„å‘³**: ç¢ºç‡å¤‰æ•° `$X$` ã® Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€‚ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ã€‚`$p(x)$` ãŒä¸€æ§˜åˆ†å¸ƒã®ã¨ãæœ€å¤§ã€ç¢ºå®šçš„ã®ã¨ã 0ã€‚Shannon (1948) [^1] ãŒæƒ…å ±ç†è«–ã®åŸºç¤ã¨ã—ã¦å®šç¾©ã€‚
</details>

<details><summary>Q2: `$D_\text{KL}(p \| q) = \mathbb{E}_{p}\left[\log \frac{p(x)}{q(x)}\right]$`</summary>
**èª­ã¿æ–¹**: ãƒ‡ã‚£ãƒ¼ ã‚±ãƒ¼ã‚¨ãƒ« ãƒ”ãƒ¼ ãƒ‘ãƒ©ãƒ¬ãƒ« ã‚­ãƒ¥ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¤ãƒ¼ ã‚µãƒ– ãƒ”ãƒ¼ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ã‚­ãƒ¥ãƒ¼

**æ„å‘³**: `$p$` ã‹ã‚‰ `$q$` ã¸ã® KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€‚`$p$` ã§ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ `$q$` ã§ç¬¦å·åŒ–ã—ãŸã¨ãã®ä½™åˆ†ãªæƒ…å ±é‡ã€‚éå¯¾ç§°ï¼ˆ`$D_\text{KL}(p\|q) \neq D_\text{KL}(q\|p)$`ï¼‰ã€‚Kullback & Leibler (1951) [^2]ã€‚
</details>

<details><summary>Q3: `$H(p, q) = H(p) + D_\text{KL}(p \| q)$`</summary>
**èª­ã¿æ–¹**: ã‚¨ã‚¤ãƒ ãƒ”ãƒ¼ ã‚­ãƒ¥ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ã‚¤ãƒ ãƒ”ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ‡ã‚£ãƒ¼ ã‚±ãƒ¼ã‚¨ãƒ« ãƒ”ãƒ¼ ãƒ‘ãƒ©ãƒ¬ãƒ« ã‚­ãƒ¥ãƒ¼

**æ„å‘³**: Cross-Entropy ã®åˆ†è§£å®šç†ã€‚Cross-Entropy = ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ + ãƒ¢ãƒ‡ãƒ«ã®ä¸å®Œå…¨æ€§ã€‚LLM è¨“ç·´ã§ Cross-Entropy ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¯ KL ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¨ç­‰ä¾¡ã€‚
</details>

<details><summary>Q4: `$I(X; Y) = H(X) - H(X \mid Y)$`</summary>
**èª­ã¿æ–¹**: ã‚¢ã‚¤ ã‚¨ãƒƒã‚¯ã‚¹ ã‚»ãƒŸã‚³ãƒ­ãƒ³ ãƒ¯ã‚¤ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ã‚¤ãƒ ã‚¨ãƒƒã‚¯ã‚¹ ãƒã‚¤ãƒŠã‚¹ ã‚¨ã‚¤ãƒ ã‚¨ãƒƒã‚¯ã‚¹ ãƒãƒ¼ ãƒ¯ã‚¤

**æ„å‘³**: `$X$` ã¨ `$Y$` ã®ç›¸äº’æƒ…å ±é‡ã€‚`$Y$` ã‚’çŸ¥ã‚‹ã“ã¨ã§ `$X$` ã®ä¸ç¢ºå®Ÿæ€§ãŒã©ã‚Œã ã‘æ¸›ã‚‹ã‹ã€‚è¡¨ç¾å­¦ç¿’ã§å…¥åŠ›ã¨æ½œåœ¨è¡¨ç¾ã®ä¾å­˜é–¢ä¿‚ã‚’æ¸¬ã‚‹ã€‚
</details>

<details><summary>Q5: `$\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)$`</summary>
**èª­ã¿æ–¹**: ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ãƒã‚¤ãƒŠã‚¹ ã‚¤ãƒ¼ã‚¿ ãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ ã‚¨ãƒ« ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼

**æ„å‘³**: å‹¾é…é™ä¸‹æ³•ã®æ›´æ–°å‰‡ã€‚å­¦ç¿’ç‡ `$\eta$` ã§å‹¾é…æ–¹å‘ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã€‚Robbins & Monro (1951) [^3] ã«é¡ã‚‹ã€‚
</details>

<details><summary>Q6: `$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$`</summary>
**èª­ã¿æ–¹**: ã‚¨ãƒ  ãƒãƒƒãƒˆ ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒ  ãƒ†ã‚£ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒ¯ãƒ³ ãƒã‚¤ãƒŠã‚¹ ãƒ™ãƒ¼ã‚¿ ãƒ¯ãƒ³ ãƒ ãƒ†ã‚£ãƒ¼ä¹—

**æ„å‘³**: Adam ã®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã€‚åˆæœŸåŒ– `$m_0 = 0$` ã‹ã‚‰ã®ãƒã‚¤ã‚¢ã‚¹ã‚’ `$1 - \beta_1^t$` ã§è£œæ­£ã€‚`$t$` ãŒå¤§ãããªã‚‹ã¨è£œæ­£é‡ã¯æ¸›å°‘ã€‚Kingma & Ba (2014) [^4]ã€‚
</details>

<details><summary>Q7: `$D_f(p \| q) = \sum_x q(x) f\left(\frac{p(x)}{q(x)}\right)$`</summary>
**èª­ã¿æ–¹**: ãƒ‡ã‚£ãƒ¼ ã‚¨ãƒ• ãƒ”ãƒ¼ ãƒ‘ãƒ©ãƒ¬ãƒ« ã‚­ãƒ¥ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚·ã‚°ãƒ ã‚­ãƒ¥ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¨ãƒ• ãƒ”ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ã‚­ãƒ¥ãƒ¼

**æ„å‘³**: f-Divergenceã€‚ç”Ÿæˆé–¢æ•° `$f$` ã‚’å¤‰ãˆã‚‹ã“ã¨ã§ KLã€`$\chi^2$`ã€Hellingerã€TVã€JS ãªã©ã‚’çµ±ä¸€çš„ã«è¡¨ç¾ã€‚Csiszar (1967) [^6]ã€‚
</details>

<details><summary>Q8: `$\text{PPL} = 2^{H(p, q)}$`ï¼ˆãŸã ã— `$H$` ã¯ bitsï¼‰</summary>
**èª­ã¿æ–¹**: ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ‹ ãƒ ã‚¨ã‚¤ãƒ ãƒ”ãƒ¼ ã‚­ãƒ¥ãƒ¼ ä¹—

**æ„å‘³**: Perplexityã€‚ãƒ¢ãƒ‡ãƒ«ãŒå„æ™‚ç‚¹ã§å¹³å‡ä½•å€‹ã®é¸æŠè‚¢ã«è¿·ã£ã¦ã„ã‚‹ã‹ã€‚`$H(p, q)$` ãŒå°ã•ã„ã»ã© PPL ãŒä½ãã€ã‚ˆã„äºˆæ¸¬ã€‚LLM è©•ä¾¡ã®æ¨™æº–æŒ‡æ¨™ã€‚
</details>

<details><summary>Q9: `$v_{t+1} = \beta v_t + \nabla_\theta \mathcal{L}(\theta_t)$`</summary>
**èª­ã¿æ–¹**: ãƒ–ã‚¤ ãƒ†ã‚£ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ™ãƒ¼ã‚¿ ãƒ–ã‚¤ ãƒ†ã‚£ãƒ¼ ãƒ—ãƒ©ã‚¹ ãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ ã‚¨ãƒ« ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼

**æ„å‘³**: Momentum ã®é€Ÿåº¦æ›´æ–°ã€‚éå»ã®å‹¾é…ã®æŒ‡æ•°ç§»å‹•å¹³å‡ã€‚`$\beta = 0.9$` ãªã‚‰éå»10ã‚¹ãƒ†ãƒƒãƒ—ã®å‹¾é…ãŒå½±éŸ¿ã€‚Polyak (1964) [^8]ã€‚
</details>

<details><summary>Q10: `$\eta_t = \eta_\text{min} + \frac{1}{2}(\eta_\text{max} - \eta_\text{min})(1 + \cos(\pi t / T))$`</summary>
**èª­ã¿æ–¹**: ã‚¤ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¤ãƒ¼ã‚¿ ãƒŸãƒ³ ãƒ—ãƒ©ã‚¹ äºŒåˆ†ã®ä¸€ ã‚¤ãƒ¼ã‚¿ ãƒãƒƒã‚¯ã‚¹ ãƒã‚¤ãƒŠã‚¹ ã‚¤ãƒ¼ã‚¿ ãƒŸãƒ³ ã‚«ãƒƒã‚³ ãƒ¯ãƒ³ ãƒ—ãƒ©ã‚¹ ã‚³ã‚µã‚¤ãƒ³ ãƒ‘ã‚¤ ãƒ†ã‚£ãƒ¼ ã‚ªãƒ¼ãƒãƒ¼ ãƒ†ã‚£ãƒ¼

**æ„å‘³**: Cosine Annealing å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚ã‚³ã‚µã‚¤ãƒ³æ›²ç·šã«æ²¿ã£ã¦å­¦ç¿’ç‡ã‚’ `$\eta_\text{max}$` ã‹ã‚‰ `$\eta_\text{min}$` ã«æ¸›è¡°ã€‚
</details>

### Z5b.2 LaTeX ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’ LaTeX ã§æ›¸ã‘ã€‚

<details><summary>Q1: Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å®šç¾©</summary>

```math
H(X) = -\sum_{x \in \mathcal{X}} p(x) \log p(x)
```
</details>

<details><summary>Q2: KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®éè² æ€§</summary>

```math
D_\text{KL}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)} \geq 0
```
</details>

<details><summary>Q3: Adam ã®æ›´æ–°å‰‡ï¼ˆãƒã‚¤ã‚¢ã‚¹è£œæ­£è¾¼ã¿ï¼‰</summary>

</details>

<details><summary>Q4: Cross-Entropy ã®åˆ†è§£</summary>

```math
H(p, q) = H(p) + D_\text{KL}(p \| q)
```
</details>

<details><summary>Q5: f-Divergence ã®å®šç¾©</summary>

```math
D_f(p \| q) = \sum_x q(x) f\left(\frac{p(x)}{q(x)}\right)
```
</details>

<details><summary>Q6: ç›¸äº’æƒ…å ±é‡ (3ã¤ã®ç­‰ä¾¡ãªè¡¨ç¾)</summary>

```math
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)
```

```math
I(X;Y) = D_{KL}(p(x,y) \| p(x)p(y)) \geq 0
```
</details>

<details><summary>Q7: è‡ªç„¶å‹¾é…ã®æ›´æ–°å‰‡</summary>

```math
\theta_{t+1} = \theta_t - \eta \mathcal{I}(\theta_t)^{-1} \nabla_\theta \mathcal{L}(\theta_t)
```

ã“ã“ã§ `$\mathcal{I}(\theta) = \mathbb{E}_{p_\theta(x)}\left[\nabla \log p_\theta \cdot (\nabla \log p_\theta)^\top\right]$` ã¯ Fisher æƒ…å ±è¡Œåˆ—ã€‚
</details>

<details><summary>Q8: Rate-Distortion é–¢æ•°ã®å®šç¾©</summary>

```math
R(D) = \min_{q(z|x): \mathbb{E}_{p(x)}\mathbb{E}_{q(z|x)}[d(x,z)] \leq D} I(X;Z)
```
</details>

### Z5b.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

æ•°å¼ã‚’ Python ã«ç¿»è¨³ã›ã‚ˆã€‚

<details><summary>Q1: æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ `$H(Y \mid X) = -\sum_{x,y} p(x,y) \log p(y \mid x)$`</summary>

```python
# joint: (|X|, |Y|) matrix
def conditional_entropy(joint: np.ndarray) -> float:
    px = joint.sum(axis=1, keepdims=True)
    # p(y|x) = joint / px; avoid 0/0 with eps
    p_y_given_x = joint / (px + 1e-12)
    # H(Y|X) = -sum_{x,y} p(x,y) * log p(y|x)
    log_cond = np.log(p_y_given_x + 1e-12)
    return float(-np.sum(joint * log_cond))
```
æ¤œç®—: `H(Y|X) = H(X,Y) - H(X)` ã§ç¢ºèªã€‚
</details>

<details><summary>Q2: Nesterov Momentum ã®æ›´æ–°</summary>

æ•°å¼: `$v_{t+1} = \mu v_t - \eta \nabla f(\theta_t + \mu v_t)$`, `$\theta_{t+1} = \theta_t + v_{t+1}$`

```python
def nesterov_step(theta, v, grad_fn, lr=0.01, mu=0.9):
    # Evaluate gradient at lookahead position
    grad = grad_fn(theta + mu * v)
    v = mu * v - lr * grad
    theta = theta + v
    return theta, v
```
é€šå¸¸ Momentum ã¨ã®é•ã„: å‹¾é…è©•ä¾¡ç‚¹ãŒ `$\theta + \mu v$`ï¼ˆå…ˆèª­ã¿ï¼‰ã«ãªã‚‹ã€‚
</details>

<details><summary>Q3: ç›¸äº’æƒ…å ±é‡ã‚’ KL ã¨ã—ã¦è¨ˆç®—</summary>

æ•°å¼: `$I(X;Y) = D_{KL}(p(x,y) \| p(x)p(y))$`

```python
def mi_via_kl(joint: np.ndarray) -> float:
    px = joint.sum(axis=1, keepdims=True)  # shape: (|X|, 1)
    py = joint.sum(axis=0, keepdims=True)  # shape: (1, |Y|)
    independent = px * py                  # outer product: p(x)p(y)
    # KL(joint || independent)
    mask = joint > 0
    return float(np.sum(joint[mask] * np.log(joint[mask] / independent[mask])))
```
</details>

<details><summary>Q4: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¾®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼</summary>

æ•°å¼: `$h(\mathcal{N}(\mu, \sigma^2)) = \frac{1}{2} \ln(2\pi e \sigma^2)$`

```python
def gaussian_differential_entropy(sigma: float) -> float:
    # h = 0.5 * ln(2*pi*e*sigma^2)
    return 0.5 * np.log(2 * np.pi * np.e * sigma**2)
# æ¤œç®—: sigma=1 â†’ 0.5*ln(2*pi*e) â‰ˆ 1.4189
print(gaussian_differential_entropy(1.0))  # â‰ˆ 1.4189
```
</details>

<details><summary>Q5: Cosine Annealing ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©</summary>

æ•°å¼: `$\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\frac{\pi t}{T}\right)$`

```python
def cosine_annealing(t: int, T: int, lr_max: float = 0.1, lr_min: float = 1e-6) -> float:
    return lr_min + 0.5 * (lr_max - lr_min) * (1 + np.cos(np.pi * t / T))
# æ¤œç®—: t=0 â†’ lr_max, t=T â†’ lr_min
assert np.isclose(cosine_annealing(0, 100), 0.1)
assert np.isclose(cosine_annealing(100, 100), 1e-6)
```
</details>

### Z5b.4 è«–æ–‡èª­è§£ãƒ†ã‚¹ãƒˆ â€” Kingma & Ba (2014) "Adam" [^4]

Adam ã®åŸè«–æ–‡ã‚’ Pass 1 ã§èª­ã‚“ã§ã¿ã‚ˆã†ã€‚

**ã‚¿ã‚¹ã‚¯**: ä»¥ä¸‹ã®å•ã„ã«ç­”ãˆã‚ˆï¼ˆè«–æ–‡ arXiv:1412.6980ï¼‰ã€‚

1. Adam ã®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ãŒå¿…è¦ãªç†ç”±ã‚’ Abstract ã‹ã‚‰ä¸€æ–‡ã§ç­”ãˆã‚ˆ
2. Algorithm 1 ã«å‡ºã¦ãã‚‹ `$\alpha$` ã®æ¨å¥¨å€¤ã¯ä½•ã‹
3. Section 3 ã§è¨¼æ˜ã•ã‚Œã¦ã„ã‚‹åæŸç‡ã¯ `$O(?)$` ã‹

<details><summary>è§£ç­”</summary>

1. `m_t` ã¨ `$v_t$` ã¯åˆæœŸå€¤ 0 ã§å§‹ã¾ã‚‹ãŸã‚å­¦ç¿’åˆæœŸã«ã‚¼ãƒ­æ–¹å‘ã¸ã®ãƒã‚¤ã‚¢ã‚¹ãŒç”Ÿã˜ã‚‹ã€‚Abstract ã«ã¯ "counteract these biases" ã¨è¨˜è¼‰ï¼ˆp.1ï¼‰ã€‚
2. Algorithm 1: `$\alpha = 0.001$`ï¼ˆ= 1e-3ï¼‰ãŒæ¨å¥¨å€¤ã¨ã—ã¦è¨˜è¼‰ã€‚ãŸã ã—è«–æ–‡è‡ªä½“ã¯ `$\alpha$` ã‚’ tunable ã¨ã—ã€ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚
3. Theorem 1: Regretï¼ˆå¾Œæ‚”ï¼‰ã®ä¸Šç•ŒãŒ `$O(\sqrt{T})$` ã§ä¸ãˆã‚‰ã‚Œã‚‹ã€‚ã“ã‚Œã¯ Adagrad [^5] ã¨åŒæ§˜ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã ãŒã€Adam ã¯ã‚ˆã‚Šå°ã•ãªå®šæ•°ã§é”æˆã•ã‚Œã‚‹ï¼ˆSection 3 ã® Remark 1ï¼‰ã€‚

</details>

### Z5b.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” KL æ¨å®šã® Monte Carlo æ³•

è§£æçš„ã« KL ãŒè¨ˆç®—ã§ããªã„å ´åˆã€ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰æ¨å®šã™ã‚‹æ–¹æ³•ã‚’å®Ÿè£…ã™ã‚‹ã€‚

**ç›®æ¨™**: `$D_{KL}(\mathcal{N}(1, 2^2) \| \mathcal{N}(0, 1^2))$` ã‚’ Monte Carlo æ¨å®šã—ã€é–‰å½¢å¼ã¨æ¯”è¼ƒã€‚

```math
D_{KL}(p \| q) = \mathbb{E}_{x \sim p}\left[\log \frac{p(x)}{q(x)}\right] \approx \frac{1}{N} \sum_{i=1}^N \log \frac{p(x_i)}{q(x_i)}
```

**å®Ÿè£…æ–¹é‡**:
1. `np.random.normal(mu1, sigma1, N)` ã§ `$p$` ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
2. `scipy.stats.norm.logpdf(x, mu1, sigma1)` ã§ `$\log p(x_i)$` è¨ˆç®—
3. å·®ã‚’å¹³å‡ â†’ MC æ¨å®šå€¤
4. é–‰å½¢å¼ `kl_gaussian_closed` ã¨æ¯”è¼ƒ

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**: MC æ¨å®šå€¤ã¨é–‰å½¢å¼ã®å·®ãŒ `$N=10000$` ã§æ¦‚ã­ `Â±0.05` ä»¥å†…ã€‚`$N \to \infty$` ã§ä¸€è‡´ã™ã‚‹ã€‚ã“ã‚ŒãŒç¢ºèªã§ãã‚Œã° Z5.10 ã®é–‰å½¢å¼å°å‡ºãŒæ­£ã—ã„ã“ã¨ã®æ•°å€¤çš„è¨¼æ‹ ã«ãªã‚‹ã€‚

### Z5b.6 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” æœ€é©åŒ–å™¨ã®åæŸæ¯”è¼ƒ

Z5.5 ã® `adam_step` ã¨è‡ªåˆ†ã§å®Ÿè£…ã—ãŸ SGD+Momentum ã‚’ä½¿ã„ã€åŒã˜äºŒæ¬¡æå¤±é–¢æ•°ã§åæŸé€Ÿåº¦ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

**å•ã„**: æ¡ä»¶æ•° `$\kappa = 100$` ã®å ´åˆã€SGD ãŒ 0.01 ä»¥ä¸‹ã®æå¤±ã«é”ã™ã‚‹ã®ã«ä½•ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ã‹ï¼ŸAdam ã¯ä½•ã‚¹ãƒ†ãƒƒãƒ—ã‹ï¼Ÿ

**å®Ÿè£…æ–¹é‡**:
1. æå¤± `$L(\theta) = \theta_1^2 + 100\theta_2^2$` ã‚’ä½¿ç”¨ï¼ˆZ5.5ã¨åŒã˜ï¼‰
2. `theta = [1.0, 1.0]` ã‹ã‚‰å‡ºç™º
3. SGD: `lr=0.009` ã§ 200 ã‚¹ãƒ†ãƒƒãƒ—
4. Adam: `lr=0.1` ã§ 200 ã‚¹ãƒ†ãƒƒãƒ—
5. æå¤±ã‚’ `history_loss_sgd`, `history_loss_adam` ã«è¨˜éŒ²ã—ã€æœ€åˆã« `1e-4` ã‚’ä¸‹å›ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ¯”è¼ƒ

**ç¢ºèª**: `ç†è«–çš„ã« SGD ã®åæŸã¯ O(Îº) = O(100) ã‚¹ãƒ†ãƒƒãƒ—, Adam ã¯æ¡ä»¶æ•°ã«ä¾å­˜ã—ã«ãã„ã®ã§ãšã£ã¨é€Ÿã„ã€‚

### Z5b.7 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” AdamW ã§ç°¡å˜ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’è¨“ç·´

å­¦ã‚“ã æœ€é©åŒ–å™¨ã‚’ä½¿ã£ã¦ã€2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’ XOR å•é¡Œã§è¨“ç·´ã™ã‚‹ã€‚


### Z5b.8 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®ã‚¬ã‚¦ã‚¹é–‰å½¢å¼

2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã® KL ã¯é–‰å½¢å¼ã§è¨ˆç®—ã§ãã‚‹ã€‚å°å‡ºã—ã€Monte Carlo ã¨æ¯”è¼ƒã›ã‚ˆã€‚


ã“ã®é–‰å½¢å¼ KL ã¯ VAE ã®æå¤±é–¢æ•°ã§ç›´æ¥ä½¿ã‚ã‚Œã‚‹ï¼ˆç¬¬10å›ï¼‰ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒå‡ºåŠ›ã™ã‚‹ `$q(z|x) = \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x))$` ã¨äº‹å‰åˆ†å¸ƒ `$p(z) = \mathcal{N}(0, I)$` ã® KL ãŒã“ã‚Œã ã€‚

### Z5b.9 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” Source Coding Theorem ã®ä½“é¨“

Shannon ã® Source Coding Theoremï¼ˆæƒ…å ±æºç¬¦å·åŒ–å®šç†ï¼‰ã‚’ä½“é¨“ã™ã‚‹ã€‚ã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ã®ç†è«–çš„é™ç•Œã‚’ç¤ºã™ã€‚

**å®šç†** (Shannon, 1948 [^1]): æƒ…å ±æº `$X$` ã®å‡ºåŠ›ã‚’ç¬¦å·åŒ–ã™ã‚‹ã¨ãã€å¹³å‡ç¬¦å·é•· `$L$` ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä»¥ä¸Š:

```math
L \geq H(X)
```

ç­‰å·ã¯æœ€é©ãªç¬¦å·ï¼ˆHuffman ç¬¦å·ãªã©ï¼‰ã§è¿‘ä¼¼çš„ã«é”æˆã•ã‚Œã‚‹ã€‚


**LLM = åœ§ç¸®å™¨**: ã“ã®è¦–ç‚¹ã¯æ·±ã„ã€‚LLM ãŒ Perplexity ã‚’ä¸‹ã’ã‚‹ã“ã¨ã¯ã€è¨€èªã®åŠ¹ç‡çš„ãªç¬¦å·åŒ–ã‚’å­¦ã¶ã“ã¨ã¨ç­‰ä¾¡ã ã€‚GPT-4 ã® Perplexity ãŒ 10 ã¨ã„ã†ã“ã¨ã¯ã€å¹³å‡ `$\log_2 10 \approx 3.32$` bits/token ã§è‹±èªã‚’ç¬¦å·åŒ–ã§ãã‚‹ã¨ã„ã†ã“ã¨ã€‚å›ºå®šé•· `$\log_2 50000 \approx 15.6$` bits ã«å¯¾ã—ã¦ç´„ 21% ã®åŠ¹ç‡ã€‚**LLM ã¯æœ¬è³ªçš„ã«ç¢ºç‡çš„ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®å™¨ãªã®ã **ã€‚

### Z5b.10 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” æœ€é©åŒ–ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã®æ¡ä»¶æ•°ã¨åæŸé€Ÿåº¦

æ¡ä»¶æ•°ï¼ˆcondition numberï¼‰ãŒæœ€é©åŒ–ã®é›£ã—ã•ã‚’æ±ºã‚ã‚‹ã€‚


ã“ã®æ¡ä»¶æ•°ã®å•é¡Œã“ã Adam ãŒè§£æ±ºã™ã‚‹èª²é¡Œã ã€‚å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ–¹å‘ã«ç‹¬ç«‹ã—ãŸå­¦ç¿’ç‡ã‚’æŒã¤ã“ã¨ã§ã€æ¡ä»¶æ•°ãŒå¤§ãã„ï¼ˆ= æ–¹å‘ã«ã‚ˆã£ã¦æ›²ç‡ãŒç•°ãªã‚‹ï¼‰å•é¡Œã§ã‚‚åŠ¹ç‡çš„ã«åæŸã™ã‚‹ã€‚

### Z5b.11 è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å®šç¾©ã‚’æ›¸ã‘ã‚‹
- [ ] KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®éè² æ€§ã‚’ Jensen ã®ä¸ç­‰å¼ã‹ã‚‰è¨¼æ˜ã§ãã‚‹
- [ ] Cross-Entropy = H(p) + KL(p||q) ã‚’å°å‡ºã§ãã‚‹
- [ ] å‰å‘ã KL ã¨é€†å‘ã KL ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] ç›¸äº’æƒ…å ±é‡ã‚’ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã—ã¦æ›¸ã‘ã‚‹
- [ ] f-Divergence ã®å®šç¾©ã¨ä¸»è¦ãªç‰¹æ®Šã‚±ãƒ¼ã‚¹ã‚’åˆ—æŒ™ã§ãã‚‹
- [ ] SGD ã®æ›´æ–°å‰‡ã‚’æ›¸ã‘ã‚‹
- [ ] Momentum ã®ç‰©ç†çš„ç›´æ„Ÿã‚’èª¬æ˜ã§ãã‚‹
- [ ] Adam ã®å…¨æ›´æ–°å‰‡ï¼ˆãƒã‚¤ã‚¢ã‚¹è£œæ­£è¾¼ã¿ï¼‰ã‚’æ›¸ã‘ã‚‹
- [ ] AdamW ã¨ Adam + L2 ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Cosine Annealing ã®æ•°å¼ã‚’æ›¸ã‘ã‚‹
- [ ] Cross-Entropy Loss ã®æƒ…å ±ç†è«–çš„åˆ†è§£ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Perplexity = 2^H ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹
- [ ] f-GAN ãŒ f-Divergence ã‚’ä½¿ã£ã¦ GAN ã®ç›®çš„é–¢æ•°ã‚’å°å‡ºã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã§ãã‚‹
- [ ] è‡ªç„¶å‹¾é…ã®å‹•æ©Ÿï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰å‹¾é…ã®æ¬ ç‚¹ï¼‰ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Fisher æƒ…å ±è¡Œåˆ—ã¨ Adam ã®å¯¾è§’è¿‘ä¼¼ã®é–¢ä¿‚ã‚’èª¬æ˜ã§ãã‚‹
- [ ] ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã¨ KKT æ¡ä»¶ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Rate-Distortion ç†è«–ãŒ Î²-VAE ã«æ¥ç¶šã™ã‚‹ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ `$\min I(X;Z) - \beta I(Z;Y)$` ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹
- [ ] ç›¸äº’æƒ…å ±é‡ã®æ’ç­‰å¼ `$I(X;Y) = H(X) + H(Y) - H(X,Y)$` ã‚’æ•°å€¤ä¾‹ã§ç¢ºèªã§ãã‚‹

**æ¡ç‚¹åŸºæº–**:
- 15å•ä»¥ä¸Š: æœ¬è¬›ç¾©å®Œå…¨ç¿’å¾— ï¿½ï¿½
- 10-14å•: åŸºç¤ã¯å›ºã¾ã£ãŸã€‚å¼±ç‚¹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¾©ç¿’ âœ…
- 7-9å•: Part1 ã‚’å†èª­å¾Œã€Part2 ã«æˆ»ã‚‹
- 6å•ä»¥ä¸‹: Z2 ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‹ã‚‰å†ã‚¹ã‚¿ãƒ¼ãƒˆ

> **Note:** **é€²æ—: 85% å®Œäº†** è¨˜å·èª­è§£ãƒ»LaTeXãƒ»ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ»è«–æ–‡èª­è§£ã®å…¨ãƒ†ã‚¹ãƒˆã‚’å®Œäº†ã€‚è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã§å¼±ç‚¹ã‚’ç¢ºèªã›ã‚ˆã€‚

---

> Progress: 85%

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆ20åˆ†ï¼‰â€” æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ã®ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢

æƒ…å ±ç†è«–ã¨æœ€é©åŒ–ç†è«–ã®ç¾åœ¨åœ°ã¨ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¸ã®æ¥ç¶šãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã€‚æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å…¨å¼•ç”¨ã¯ arXiv è«–æ–‡ã®ã¿ã€‚

```mermaid
graph LR
    A["Shannon 1948<br/>æƒ…å ±ç†è«–ã®èª•ç”Ÿ"] --> B["KL / f-Divergence<br/>1951-1967"]
    B --> C["å¤‰åˆ†æ¨è«–ãƒ»EM<br/>1970-2000"]
    C --> D["æ·±å±¤å­¦ç¿’ã¨ã®çµåˆ<br/>2014-2020<br/>GAN, VAE, Flow"]
    D --> E["ç”Ÿæˆãƒ¢ãƒ‡ãƒ«çµ±ä¸€ç†è«–<br/>2020-2026<br/>Diffusion, Score, OT"]
    style E fill:#c8e6c9
```

### Z6.1 æ¬¡ä¸–ä»£æœ€é©åŒ–å™¨ã®å‹•å‘ï¼ˆ2024-2026ï¼‰

Adam ã¯2014å¹´ã‹ã‚‰10å¹´ä»¥ä¸Šã«ã‚ãŸã‚Šæ¨™æº–çš„ãªæœ€é©åŒ–å™¨ã§ã‚ã‚Šç¶šã‘ã¦ã„ã‚‹ã€‚ã ãŒè¿‘å¹´ã€ä»¥ä¸‹ã®ä»£æ›¿æ¡ˆãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ã€‚

| æœ€é©åŒ–å™¨ | è‘—è€…/å¹´ | ç‰¹å¾´ | Adam ã¨ã®æ¯”è¼ƒ |
|:---------|:-------|:-----|:-----------|
| Lion | Google, 2023 | sign-basedæ›´æ–°ã€ãƒ¡ãƒ¢ãƒªåŠæ¸› | åŒ¹æ•µã™ã‚‹æ€§èƒ½ã§çœãƒ¡ãƒ¢ãƒª |
| Sophia | Stanford, 2023 | 2æ¬¡æƒ…å ±ï¼ˆãƒ˜ãƒƒã‚»å¯¾è§’ï¼‰åˆ©ç”¨ | è¨“ç·´ãƒˆãƒ¼ã‚¯ãƒ³50%å‰Šæ¸› |
| Muon | MIT, 2024 | ç›´äº¤å°„å½±ã«åŸºã¥ã | Transformerç‰¹åŒ– |
| Schedule-Free | Meta, 2024 | ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ä¸è¦ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸› |
| ADOPT | Taniguchi+, 2024 | ä»»æ„ã® `$\beta_2$` ã§åæŸä¿è¨¼ | Adam ã®ç†è«–çš„æ¬ é™¥ã‚’ä¿®æ­£ |

ã“ã‚Œã‚‰ã®æœ€é©åŒ–å™¨ãŒ AdamW ã‚’æœ¬å½“ã«ç½®ãæ›ãˆã‚‹ã‹ã¯ã¾ã æ±ºç€ãŒã¤ã„ã¦ã„ãªã„ã€‚LLM ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®æ¤œè¨¼ã«ã¯ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ãŸã‚ã€çµæœã®å†ç¾ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã®ãŒç¾çŠ¶ã ã€‚

<details><summary>ç”¨èªé›†ï¼ˆæœ¬è¬›ç¾©ã®å…¨ç”¨èªï¼‰</summary>
| ç”¨èªï¼ˆè‹±ï¼‰ | ç”¨èªï¼ˆæ—¥ï¼‰ | å®šç¾©ã®å ´æ‰€ |
|:-----------|:---------|:---------|
| Shannon Entropy | ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.1 |
| Differential Entropy | å¾®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.2 |
| KL Divergence | KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç¾© 3.3 |
| Cross-Entropy | äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.4 |
| Conditional Entropy | æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å®šç¾© 3.5 |
| Mutual Information | ç›¸äº’æƒ…å ±é‡ | å®šç¾© 3.6 |
| f-Divergence | f ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç¾© 3.7 |
| Jensen's Inequality | ã‚¤ã‚§ãƒ³ã‚»ãƒ³ã®ä¸ç­‰å¼ | å®šç† 3.4 |
| SGD | ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³• | å®šç¾© 3.8 |
| Momentum | ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  | å®šç¾© 3.9 |
| Adam | ã‚¢ãƒ€ãƒ  | å®šç¾© 3.10 |
| AdamW | ã‚¢ãƒ€ãƒ ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ | 3.10 |
| Perplexity | ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ | Zone 0 |
| Mode-covering | ãƒ¢ãƒ¼ãƒ‰ã‚«ãƒãƒªãƒ³ã‚° | 3.3 |
| Mode-seeking | ãƒ¢ãƒ¼ãƒ‰ã‚·ãƒ¼ã‚­ãƒ³ã‚° | 3.3 |
| Fenchel Conjugate | ãƒ•ã‚§ãƒ³ã‚·ã‚§ãƒ«å…±å½¹ | 3.6 details |
| Cosine Annealing | ã‚³ã‚µã‚¤ãƒ³ç„¼ããªã¾ã— | 3.11 |
| Warmup | ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— | 3.11 |
| Bias Correction | ãƒã‚¤ã‚¢ã‚¹è£œæ­£ | 3.10 |
| Weight Decay | é‡ã¿æ¸›è¡° | 3.10 |
| Data Processing Inequality | ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸ç­‰å¼ | å®šç† 3.5 |
| Fano's Inequality | ãƒ•ã‚¡ãƒã®ä¸ç­‰å¼ | 3.5b |
| Chain Rule (Entropy) | é€£é–å¾‹ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰ | 3.5c |
| Convex Set | å‡¸é›†åˆ | å®šç¾© 3.6 |
| Convex Function | å‡¸é–¢æ•° | å®šç¾© 3.7 |
| Strong Convexity | å¼·å‡¸æ€§ | å®šç¾© 3.8 |
| KKT Conditions | KKT æ¡ä»¶ | å®šç† 3.9 |
| Lagrangian Dual | ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥åŒå¯¾ | å®šç† 3.10 |
| Strong Duality | å¼·åŒå¯¾æ€§ | å®šç† 3.11 |
| Lipschitz Continuity | ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šæ€§ | å®šç¾© 3.9 |
| Spectral Normalization | ã‚¹ãƒšã‚¯ãƒˆãƒ«æ­£è¦åŒ– | 3.11c |
| Jensen-Shannon Divergence | JSD | 3.11d |
| Wasserstein Distance | ãƒ¯ãƒƒã‚µãƒ¼ã‚¹ã‚¿ã‚¤ãƒ³è·é›¢ | å®šç¾© 3.10 |
| Kantorovich-Rubinstein Duality | KR åŒå¯¾æ€§ | 3.11d |
| Optimal Transport | æœ€é©è¼¸é€ | 3.11d |
| WGAN | ãƒ¯ãƒƒã‚µãƒ¼ã‚¹ã‚¿ã‚¤ãƒ³ GAN | 3.11d |
| Gradient Clipping | å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚° | 4.7 |
| Mixed Precision | æ··åˆç²¾åº¦ | 4.7 |
| Lagrangian | ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ã‚¢ãƒ³ | 4.8 |
| Maximum Entropy | æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | 4.8 |
| Fisher Information | ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±é‡ | 6.3 |
| Natural Gradient | è‡ªç„¶å‹¾é… | 6.3 |
| Rate-Distortion | ãƒ¬ãƒ¼ãƒˆæ­ªã¿ | 6.2 |
| Source Coding Theorem | æƒ…å ±æºç¬¦å·åŒ–å®šç† | 5.9 |
| Condition Number | æ¡ä»¶æ•° | 5.10 |
| Gibbs Inequality | ã‚®ãƒ–ã‚¹ã®ä¸ç­‰å¼ | å®šç† 3.2 |
| Bregman Divergence | ãƒ–ãƒ¬ã‚°ãƒãƒ³ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | 3.7 details |
| Information Bottleneck | æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ | 3.5b |
| WSD Schedule | ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®‰å®šæ¸›è¡° | 3.11 |
</details>

<details><summary>ä¸»è¦ãªä¸ç­‰å¼ã¾ã¨ã‚</summary>
| ä¸ç­‰å¼ | æ•°å¼ | æ„å‘³ | è¨¼æ˜ |
|:-------|:-----|:-----|:-----|
| KL ã®éè² æ€§ | `$D_\text{KL}(p \| q) \geq 0$` | ç•°ãªã‚‹åˆ†å¸ƒãªã‚‰è·é›¢ã¯æ­£ | Jensen |
| Jensen ã®ä¸ç­‰å¼ | `$f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]$` | å‡¸é–¢æ•°ã®æœŸå¾…å€¤ | æ”¯æŒè¶…å¹³é¢ |
| Gibbs ã®ä¸ç­‰å¼ | `$H(p, q) \geq H(p)$` | Cross-Entropy â‰¥ Entropy | KL â‰¥ 0 |
| ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸ç­‰å¼ | `$I(X;Z) \leq I(X;Y)$` if `$X \to Y \to Z$` | å‡¦ç†ã§æƒ…å ±ã¯å¢—ãˆãªã„ | Chain Rule |
| Fano ã®ä¸ç­‰å¼ | `$H(X|\hat{X}) \leq H_b(P_e) + P_e \log(|\mathcal{X}|-1)$` | æ¨å®šç²¾åº¦ã®ä¸‹é™ | â€” |
| Source Coding | `$L \geq H(X)$` | ç¬¦å·é•·ã®ä¸‹é™ = ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | â€” |
| `$H$` ã®ä¸Šç•Œ | `$H(X) \leq \log |\mathcal{X}|$` | ç­‰å·ã¯ä¸€æ§˜åˆ†å¸ƒ | Jensen |
| ã‚¬ã‚¦ã‚¹ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | `$h(X) \leq \frac{1}{2}\log(2\pi e \sigma^2)$` | åˆ†æ•£å›ºå®šã§ã‚¬ã‚¦ã‚¹ãŒæœ€å¤§ | Lagrange |
</details>

<details><summary>ä¸»è¦ãªç­‰å¼ã¾ã¨ã‚</summary>
| ç­‰å¼ | æ•°å¼ | æ„å‘³ |
|:-----|:-----|:-----|
| Cross-Entropy åˆ†è§£ | `$H(p,q) = H(p) + D_\text{KL}(p \| q)$` | CE = Entropy + KL |
| ç›¸äº’æƒ…å ±é‡ (1) | `$I(X;Y) = H(X) - H(X|Y)$` | MI = Entropy reduction |
| ç›¸äº’æƒ…å ±é‡ (2) | `$I(X;Y) = D_\text{KL}(p(x,y) \| p(x)p(y))$` | MI = KL from independence |
| Entropy Chain Rule | `$H(X,Y) = H(X) + H(Y|X)$` | Joint = Marginal + Conditional |
| ã‚¬ã‚¦ã‚¹ KL | `$D_\text{KL}(\mathcal{N}_1 \| \mathcal{N}_2) = \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}$` | Closed form |
| ã‚¬ã‚¦ã‚¹å¾®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | `$h(X) = \frac{1}{2}\log(2\pi e \sigma^2)$` | Depends only on `$\sigma$` |
</details>


### Z6.2 æƒ…å ±ç†è«–ã®æœ€æ–°ç ”ç©¶ (2020-2026)

#### 6.9.1 Î±-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ãƒ™ã‚¤ã‚ºæœ€é©åŒ–

KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ `$f$`-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã ãŒã€2024å¹´ã®ç ”ç©¶[^14]ã¯ `$\alpha$`-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’ç”¨ã„ãŸæ–°ã—ã„ãƒ™ã‚¤ã‚ºæœ€é©åŒ–æ‰‹æ³•ã‚’ææ¡ˆã—ãŸã€‚

**Î±-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®å®šç¾©**:

```math
D_\alpha(p \| q) = \frac{1}{\alpha(\alpha-1)} \left( \int p(x)^\alpha q(x)^{1-\alpha} dx - 1 \right), \quad \alpha \neq 0, 1
```

ç‰¹æ®Šã‚±ãƒ¼ã‚¹:
- `$\alpha \to 1$`: KL divergence `$D_{\text{KL}}(p \| q)$`
- `$\alpha \to 0$`: Reverse KL divergence `$D_{\text{KL}}(q \| p)$`
- `$\alpha = 1/2$`: Hellinger distance

**Alpha Entropy Search (AES)**: ç²å¾—é–¢æ•°ã¨ã—ã¦ã€æ¬¡ã®è©•ä¾¡ç‚¹ã§ã®ç›®çš„é–¢æ•°å€¤ `$y^*$` ã¨å¤§åŸŸçš„æœ€å¤§å€¤ `$f^*$` ã®ã€Œä¾å­˜åº¦ã€ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚ã“ã®ä¾å­˜åº¦ã‚’ `$\alpha$`-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã§æ¸¬ã‚‹ã“ã¨ã§ã€æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’æŸ”è»Ÿã«åˆ¶å¾¡ã§ãã‚‹ã€‚


#### 6.9.2 Jensen-Shannonã¨KLã®æœ€é©ä¸‹ç•Œ

2025å¹´ã®è«–æ–‡[^15]ã¯ã€Jensen-Shannon (JS) ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®é–¢ä¿‚ã‚’å®šé‡åŒ–ã™ã‚‹æœ€é©ãªä¸‹ç•Œã‚’ç¢ºç«‹ã—ãŸã€‚

**Jensen-Shannonãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹Math**:

```math
\text{JS}(p \| q) = \frac{1}{2} D_{\text{KL}}(p \| m) + \frac{1}{2} D_{\text{KL}}(q \| m), \quad m = \frac{p + q}{2}
```

**æ–°ã—ã„ä¸‹ç•Œ**:

```math
D_{\text{KL}}(p \| q) \geq \phi(\text{JS}(p \| q))
```

ã“ã“ã§ `$\phi$` ã¯å˜èª¿å¢—åŠ é–¢æ•°ã§ã€æœ€é©ãª `$\phi$` ãŒé–‰ã˜ãŸå½¢ã§æ±‚ã¾ã‚‹ã€‚ã“ã®çµæœã¯ã€GANã®ç›®çš„é–¢æ•°ï¼ˆJSã‚’æœ€å°åŒ–ï¼‰ã¨VAEã®ç›®çš„é–¢æ•°ï¼ˆKLã‚’æœ€å°åŒ–ï¼‰ã®é–¢ä¿‚ã‚’æ˜ç¢ºåŒ–ã—ãŸã€‚

**f-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹å¤‰åˆ†ä¸‹ç•Œ**:

JSãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã® `$f$`-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹å¤‰åˆ†ä¸‹ç•Œã¯ã€ç‰¹å®šã®è­˜åˆ¥å™¨ã®Cross-Entropyæå¤±ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã€‚ã“ã‚Œã¯GANã®è¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç†è«–çš„è£ä»˜ã‘ã ã€‚


#### 6.9.3 å¹¾ä½•å­¦çš„æƒ…å ±ç†è«– (GAIT)

2019å¹´ã®è«–æ–‡[^16]ã¯ã€ç¢ºç‡åˆ†å¸ƒé–“ã®ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«ã€Œå¹¾ä½•å­¦çš„æ§‹é€ ã€ã‚’çµ„ã¿è¾¼ã‚€æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ãŸã€‚

å¾“æ¥ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ç¢ºç‡åˆ†å¸ƒã‚’ã€Œç‚¹ã€ã¨ã—ã¦æ‰±ã†ãŒã€åˆ†å¸ƒã®å°ï¼ˆsupportï¼‰ã®å¹¾ä½•å­¦çš„è·é›¢ã‚’ç„¡è¦–ã™ã‚‹ã€‚GAITã¯æœ€é©è¼¸é€ç†è«–ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’æƒ…å ±ç†è«–ã«å°å…¥ã—ã€åˆ†å¸ƒé–“ã®ã€Œç§»å‹•ã‚³ã‚¹ãƒˆã€ã‚’è€ƒæ…®ã—ãŸãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’å®šç¾©ã™ã‚‹ã€‚

**Geometric Informationã®å®šç¾©**:

```math
\text{GI}(p, q) = D_{\text{KL}}(p \| q) + \lambda \cdot W_2(p, q)
```

ã“ã“ã§ `$W_2$` ã¯2-Wassersteinè·é›¢ï¼ˆæœ€é©è¼¸é€è·é›¢ï¼‰ã€`$\lambda$` ã¯ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚ã“ã‚Œã¯åˆ†å¸ƒã®ã€Œå½¢çŠ¶ã€ã¨ã€Œä½ç½®ã€ã®ä¸¡æ–¹ã‚’è€ƒæ…®ã™ã‚‹ã€‚

**å¿œç”¨**: Wasserstein GANã®ç†è«–çš„åŸºç¤ã¨ãªã‚Šã€mode collapseã®è»½æ¸›ã«è²¢çŒ®ã—ãŸã€‚

#### 6.9.4 æƒ…å ±ç†è«–çš„æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤

2024å¹´ã®åŒ…æ‹¬çš„ãƒ¬ãƒ“ãƒ¥ãƒ¼[^17]ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹æƒ…å ±ç†è«–çš„æ‰‹æ³•ã®çµ±ä¸€çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ãŸã€‚

**ä¸»è¦ãªå®šç†**:

1. **PAC-Bayeså¢ƒç•Œ**: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨æ±åŒ–èª¤å·®ã®é–¢ä¿‚
```math
   \mathbb{E}_{S, \theta \sim Q}[L(\theta)] \leq \mathbb{E}_{S, \theta \sim Q}[\hat{L}(\theta)] + \sqrt{\frac{D_{\text{KL}}(Q \| P) + \log(2n/\delta)}{2n}}
```

2. **ç›¸äº’æƒ…å ±é‡ã¨æ±åŒ–**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ `$S$` ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ `$\theta$` ã®ç›¸äº’æƒ…å ±é‡ `$I(S; \theta)$` ãŒå°ã•ã„ã»ã©ã€æ±åŒ–æ€§èƒ½ãŒé«˜ã„ã€‚

3. **æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: æœ€é©ãªè¡¨ç¾ `$Z$` ã¯ `$I(X; Z)$` ã‚’æœ€å°åŒ–ã—ã¤ã¤ `$I(Y; Z)$` ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚


#### 6.9.5 æœ€é©åŒ–ç†è«–ã®é€²å±•

**Adaptive Optimizersã®ç†è«–çš„ä¿è¨¼**

Adamã®åæŸä¿è¨¼ã¯é•·å¹´ä¸æ˜ã ã£ãŸãŒã€2018å¹´ã®Reddi et al. [^AMSGrad]ã¯åä¾‹ã‚’ç¤ºã—ã€ä¿®æ­£ç‰ˆAMSGradã‚’ææ¡ˆã—ãŸã€‚2021å¹´ã®Defazio & Jelassi [^AdamW]ã¯AdamWã®ç†è«–çš„æ€§è³ªã‚’è§£æ˜ã—ãŸã€‚

**Sharpness-Aware Minimization (SAM)**

2020å¹´ã®Foret et al. [^SAM]ã¯ã€æå¤±é–¢æ•°ã®ã€Œå¹³å¦ãªæ¥µå°ã€ã‚’æ¢ç´¢ã™ã‚‹SAMã‚’ææ¡ˆã—ãŸã€‚ã“ã‚Œã¯æ•µå¯¾çš„å­¦ç¿’ã®è¦–ç‚¹ã‹ã‚‰æœ€é©åŒ–ã‚’æ‰ãˆç›´ã—ãŸã‚‚ã®ã :

```math
\min_\theta \max_{\|\boldsymbol{\epsilon}\| \leq \rho} L(\theta + \boldsymbol{\epsilon})
```

SAMã¯æ±åŒ–æ€§èƒ½ã‚’å¤§å¹…ã«æ”¹å–„ã—ã€æƒ…å ±ç†è«–çš„ã«ã¯ã€ŒFisheræƒ…å ±é‡ãŒå°ã•ã„é ˜åŸŸã€ã‚’æ¢ç´¢ã—ã¦ã„ã‚‹ã¨è§£é‡ˆã§ãã‚‹ã€‚

#### Z6.2.6 æƒ…å ±å¹¾ä½•å­¦çš„æ·±å±¤å­¦ç¿’ (2023-2025)

Amari ã®è‡ªç„¶å‹¾é…æ³• (1998) ã¯é•·ã‚‰ãè¨ˆç®—ã‚³ã‚¹ãƒˆã®å•é¡ŒãŒã‚ã£ãŸãŒã€è¿‘å¹´ã®è¿‘ä¼¼æ‰‹æ³•ã«ã‚ˆã‚Šå®Ÿç”¨åŒ–ãŒé€²ã‚“ã§ã„ã‚‹ã€‚

**K-FAC (Kronecker-Factored Approximate Curvature)** [^16]:

```math
F \approx \bigotimes_{\ell} A_{\ell-1} \otimes G_\ell
```

ã“ã“ã§ `$A_{\ell-1}$` ã¯ `$\ell$` å±¤ã¸ã®å…¥åŠ›ã®å…±åˆ†æ•£ã€`$G_\ell$` ã¯å‡ºåŠ›ã®å‹¾é…å…±åˆ†æ•£ã€‚Fisher è¡Œåˆ—ã‚’ã‚¯ãƒ­ãƒãƒƒã‚«ãƒ¼ç©ã§è¿‘ä¼¼ã™ã‚‹ã“ã¨ã§ `$O(d^3)$` â†’ `$O(d)$` ï½ `$O(d^2)$` ã«å‰Šæ¸›ã€‚

**Muon (2024)**: ã‚¹ãƒ†ãƒ•ã‚©ãƒ³ãƒ»ãƒã‚¹ãƒ¯ãƒ‹ãƒ¼ (MIT) ãŒææ¡ˆã—ãŸæœ€é©åŒ–å™¨ã€‚SGD ã® Nesterov æ›´æ–°ã«ã‚°ãƒ©ãƒ -ã‚·ãƒ¥ãƒŸãƒƒãƒˆç›´äº¤åŒ–ã‚’çµ„ã¿åˆã‚ã›ã‚‹:

```math
G_t = \text{Nesterov}(g_t) \quad \to \quad \theta_{t+1} = \theta_t - \eta \cdot \text{orth}(G_t)
```

`$\text{orth}(\cdot)$` ã¯ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³-ã‚·ãƒ¥ãƒ«ãƒ„åå¾©ã«ã‚ˆã‚‹è¡Œåˆ—ç›´äº¤åŒ–ã€‚è¡Œåˆ— `$W \in \mathbb{R}^{m \times n}$` ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°ãŒã€Œã‚¹ãƒšã‚¯ãƒˆãƒ«ä¸Šã§å‡ç­‰ã€ã«ãªã‚Šã€Transformer ã® weight matrix å­¦ç¿’ã«ç‰¹ã«æœ‰åŠ¹ã€‚

**æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ (2020-2024)**:

SimCLR, BYOL, VICReg ãªã©ã®è‡ªå·±æ•™å¸«ã‚ã‚Šè¡¨ç¾å­¦ç¿’ã¯ã€æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®åŸç†ã‚’æš—é»™çš„ã«å®Ÿè£…ã—ã¦ã„ã‚‹:

| æ‰‹æ³• | æƒ…å ±ç†è«–çš„è§£é‡ˆ | ç›®çš„ |
|:-----|:-------------|:-----|
| SimCLR | Positive pairã® `$I(Z_1; Z_2)$` ã‚’æœ€å¤§åŒ– | è¡¨ç¾ã®ä¸€è‡´ |
| VICReg | åˆ†æ•£ + ä¸å¤‰æ€§ + å…±åˆ†æ•£ã®æœ€é©åŒ– | Collapseé˜²æ­¢ |
| BYOL | éå¯¾ç§° student-teacher = `$I(Z_\text{online}; Z_\text{target})$` | å´©å£Šãªã— |

ã“ã‚Œã‚‰ã®æ‰‹æ³•ã§å­¦ç¿’ã—ãŸç‰¹å¾´é‡ã¯ä¸‹æµã‚¿ã‚¹ã‚¯ã§é«˜ã„æ±åŒ–æ€§èƒ½ã‚’ç¤ºã—ã€ã€Œæœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ + å¯¾ç§°æ€§ã€ã¨ã„ã†æƒ…å ±ç†è«–çš„åŸç†ã§çµ±ä¸€çš„ã«ç†è§£ã§ãã‚‹ (Wang & Isola, 2020)ã€‚

#### Z6.2.7 Flow Matching ã¨æœ€é©è¼¸é€ (2022-2025)

Lipman ã‚‰ (2022) ã® Flow Matching [^17] ã¯ã€æƒ…å ±ç†è«–çš„æœ€é©è¼¸é€ã‚’æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«çµ„ã¿è¾¼ã‚“ã é©æ–°çš„æ‰‹æ³•:

```math
u_t(x) = \frac{x_1 - x_0}{1} = x_1 - x_0 \quad (\text{Optimal Transport path})
```

ã‚½ãƒ¼ã‚¹åˆ†å¸ƒ `$p_0$` ã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ `$p_1$` ã¸ã®ç›´ç·šãƒ‘ã‚¹ãŒæœ€çŸ­ï¼ˆWasserstein è·é›¢æœ€å°ï¼‰ã€‚Score-based modelï¼ˆç¬¬5å›ï¼‰ã®è¤‡é›‘ãª SDE ã¨æ¯”ã¹ã€ã‚·ãƒ³ãƒ—ãƒ«ã§è®­ç·´ãŒå®‰å®šã€‚2025å¹´ç¾åœ¨ Stable Diffusion 3, FLUX.1 ç­‰ã®å¤§å‹ãƒ¢ãƒ‡ãƒ«ã§æ¡ç”¨ã€‚

> Progress: 95%

## ğŸ¯ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆ10åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### Z7.0 æœ¬è¬›ç¾©ã®çŸ¥è­˜ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—

```mermaid
mindmap
  root((ç¬¬6å›))
    æƒ…å ±ç†è«–
      ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        Shannon 1948
        é›¢æ•£ H_X
        å¾®åˆ† h_X
        Perplexity = 2^H
      KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
        Kullback-Leibler 1951
        éå¯¾ç§°æ€§
        å‰å‘ã vs é€†å‘ã
        éè² æ€§ Gibbs
      Cross-Entropy
        H_p,q = H_p + KL
        LLM æå¤±é–¢æ•°
      ç›¸äº’æƒ…å ±é‡
        I_X;Y = H_X - H_X|Y
        è¡¨ç¾å­¦ç¿’
      f-Divergence
        Csiszar 1967
        KL, chi2, Hellinger
        å¤‰åˆ†è¡¨ç¾ NWJ
    æœ€é©åŒ–ç†è«–
      å‹¾é…é™ä¸‹æ³•
        Robbins-Monro 1951
        åæŸ O_1/T
      Momentum
        Polyak 1964
        Nesterov 1983
        åæŸ O_1/T^2
      Adam
        Kingma-Ba 2014
        é©å¿œçš„å­¦ç¿’ç‡
        ãƒã‚¤ã‚¢ã‚¹è£œæ­£
      AdamW
        Loshchilov-Hutter 2017
        åˆ†é›¢å‹é‡ã¿æ¸›è¡°
      ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
        Cosine Annealing
        Warmup
        WSD
```

---


### Z7.2 æœ¬è¬›ç¾©ã®æ¦‚å¿µé–“ã®æ¥ç¶šãƒãƒƒãƒ—

æœ¬è¬›ç¾©ã§å­¦ã‚“ã æ¦‚å¿µã¯å­¤ç«‹ã—ã¦ã„ãªã„ã€‚ä»¥ä¸‹ã®ãƒãƒƒãƒ—ã§å…¨ã¦ã®ç¹‹ãŒã‚Šã‚’ç¢ºèªã™ã‚‹ã€‚

```mermaid
graph TD
    Shannon["Shannon Entropy<br/>H(X) 1948"] -->|"+å®šæ•°"| CE["Cross-Entropy<br/>H(p,q)"]
    Shannon -->|"æ¡ä»¶ä»˜ã"| CndH["H(Y|X)"]
    CE -->|"åˆ†è§£"| KL["KL Divergence<br/>D_KL 1951"]
    Shannon -->|"å·®åˆ†"| MI["Mutual Info<br/>I(X;Y)"]
    CndH -->|"å·®åˆ†"| MI

    KL -->|"ç‰¹æ®Šã‚±ãƒ¼ã‚¹"| fDiv["f-Divergence<br/>Csiszar 1967"]
    fDiv -->|"f=t*log(t)"| KL
    fDiv -->|"JSD"| JSD["Jensen-Shannon"]
    fDiv -->|"å¤‰åˆ†è¡¨ç¾"| NWJ["NWJ 2010"]

    KL -->|"æœ€å°åŒ–"| MLE["MLE<br/>ç¬¬7å›"]
    CE -->|"æå¤±é–¢æ•°"| LLM["LLM è¨“ç·´"]
    Shannon -->|"2^H"| PPL["Perplexity"]

    SGD["SGD<br/>Robbins 1951"] -->|"+æ…£æ€§"| Mom["Momentum<br/>Polyak 1964"]
    Mom -->|"+é©å¿œLR"| Adam["Adam<br/>Kingma 2014"]
    Adam -->|"+åˆ†é›¢WD"| AdamW["AdamW<br/>Loshchilov 2017"]
    Adam -->|"æ›´æ–°"| LLM

    fDiv -->|"GANæå¤±"| GAN["GAN ç¬¬12å›"]
    KL -->|"ELBO"| VAE["VAE ç¬¬10å›"]
    NWJ -->|"f-GAN"| GAN

    style Shannon fill:#e3f2fd
    style Adam fill:#e8f5e9
    style LLM fill:#fff3e0
```

### Z7.3 æƒ…å ±ç†è«–ã¨ç‰©ç†å­¦ã®æ¥ç¶š

Shannon ãŒã€Œã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€ã®åã‚’ç†±åŠ›å­¦ã‹ã‚‰å€Ÿã‚ŠãŸã®ã¯å¶ç„¶ã§ã¯ãªã„ã€‚

| æƒ…å ±ç†è«– | çµ±è¨ˆç‰©ç†å­¦ | å¯¾å¿œ |
|:---------|:---------|:-----|
| Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ `$H$` | Gibbs ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ `$S$` | `$S = -k_B \sum p \ln p$` |
| æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒ | Boltzmann åˆ†å¸ƒ | `$p \propto e^{-E/k_BT}$` |
| KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼å·® | `$F = E - TS$` |
| Cross-Entropy æœ€å°åŒ– | è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– | å¤‰åˆ†æ¨è«– |
| ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸ç­‰å¼ | ç†±åŠ›å­¦ç¬¬2æ³•å‰‡ | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¢—å¤§ |

ã“ã®å¯¾å¿œã¯å¶ç„¶ã§ã¯ãªãæ•°å­¦çš„ã«å³å¯†ã ã€‚Boltzmann åˆ†å¸ƒ `$p(x) \propto \exp(-E(x)/T)$` ã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆ¶ç´„ä»˜ãã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒã§ã‚ã‚Šã€Zone 4 ã®ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°æ³•ã§å°ã‘ã‚‹ã€‚ã“ã®ã€Œæƒ…å ± = ç‰©ç†ã€ã®è¦–ç‚¹ã¯ç¬¬27å›ï¼ˆEnergy-Based Modelsï¼‰ã§æœ¬æ ¼çš„ã«å±•é–‹ã™ã‚‹ã€‚

**Landauer ã®åŸç†**: æƒ…å ±ã®æ¶ˆå»ï¼ˆãƒ“ãƒƒãƒˆã®0ãƒªã‚»ãƒƒãƒˆï¼‰ã¯æœ€å°ã§ã‚‚ `$k_B T \ln 2$` ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’æ¶ˆè²»ã™ã‚‹ã€‚ã“ã‚Œã¯æƒ…å ±ã¨ç‰©ç†çš„ãªã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒäº¤æ›å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¯ã€ã“ã®åŸç†ãŒç¤ºã™é€šã‚Šå·¨å¤§ãªã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’æ¶ˆè²»ã™ã‚‹ â€” æƒ…å ±çš„ã«è¦‹ã‚Œã°ã€Œç¢ºç‡åˆ†å¸ƒ `$p_{\theta}$` ã‹ã‚‰ä¸ç¢ºå®Ÿæ€§ã‚’é™¤å»ã™ã‚‹ã‚³ã‚¹ãƒˆã€ã ã€‚

**Maxwell's demon ã¨å­¦ç¿’**: Maxwell ã®æ‚ªé­”ã¯åˆ†å­ã®é€Ÿåº¦æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã“ã¨ã§ç†±åŠ›å­¦ç¬¬2æ³•å‰‡ã‚’ç ´ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ãŒã€Szilard (1929) ã¨ Landauer (1961) ã®åˆ†æãŒç¤ºã™ã‚ˆã†ã«ã€æƒ…å ±ã®æ¶ˆå»ãŒã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¢—å¤§ã‚’è£œå„Ÿã™ã‚‹ã€‚æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€Œæƒ…å ±ã€ã‚’æŠ½å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«é‡ã¿ã«è¨˜éŒ²ã™ã‚‹ã€‚ã“ã®ã€Œè¨˜éŒ²ã€ã®éç¨‹ãŒæœ€å¤§åŒ–ã™ã‚‹ã®ã¯ `$I(\text{data}; \theta)$` â€” ã¾ã•ã«ç›¸äº’æƒ…å ±é‡ã ã€‚

**ç†±åŠ›å­¦çš„è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¨ ELBO**:

```math
F(q) \equiv \mathbb{E}_{q(z)}[\log q(z) - \log p(x,z)] = D_{KL}(q \| p) - \log p(x)
```

ç‰©ç†ã§ã¯è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ `$F = E - TS$` ãŒç³»ã®åˆ©ç”¨å¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¡¨ã™ã€‚å¤‰åˆ†æ¨è«–ã® ELBO `$= -F(q)$` ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã¯ã€ç†±åŠ›å­¦çš„æ„å‘³ã§ã€Œè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’æœ€å°åŒ–ã™ã‚‹ã€ã“ã¨ã¨åŒå€¤ã€‚VAE ã‚’è¨“ç·´ã™ã‚‹ã¨ã¯ã€æƒ…å ±ã®æŒã¤ã€Œåˆ©ç”¨å¯èƒ½ãªã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ã‚’æœ€å¤§é™ã«å¼•ãå‡ºã™éç¨‹ã ï¼ˆç¬¬10å›ã§æ•°å­¦çš„ã«å³å¯†åŒ–ï¼‰ã€‚


### Z7.4 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾ç…§è¡¨

æœ¬è¬›ç¾©ã§ç™»å ´ã—ãŸæ•°å¼ã¨ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œã‚’ä¸€è¦§ã«ã™ã‚‹ã€‚

| æ•°å¼ | Python | æ³¨æ„ç‚¹ |
|:-----|:-------|:-------|
| `$H(p) = -\sum_i p_i \log p_i$` | `-np.sum(p * np.log(p + eps))` | `eps` ã§log(0)å›é¿ |
| `$D_{KL}(p\|q) = \sum_i p_i \log(p_i/q_i)$` | `np.sum(p * np.log(p / q))` | `p > 0` ã®ã¿è¨ˆç®— |
| `$H(p,q) = H(p) + D_{KL}(p\|q)$` | `entropy(p) + kl_divergence(p,q)` | æ’ç­‰å¼ã§æ¤œç®—å¯ |
| `$m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$` | `m = beta1*m + (1-beta1)*g` | in-placeæ›´æ–° |
| `$\hat{m}_t = m_t/(1-\beta_1^t)$` | `mh = m / (1 - beta1**t)` | `t` ã¯æ•´æ•°ã§ç®¡ç† |
| `$\theta \leftarrow \theta - \eta \hat{m}/(\sqrt{\hat{v}}+\epsilon)$` | `theta -= lr * mh / (np.sqrt(vh) + eps)` | `eps=1e-8` ãŒæ¨™æº– |
| `$W_1(\mu,\nu) = \sup_{f:\text{1-Lip}} [\mathbb{E}_\mu f - \mathbb{E}_\nu f]$` | `scipy.stats.wasserstein_distance(p, q)` | 1æ¬¡å…ƒã®ã¿ç›´æ¥è¨ˆç®—å¯ |
| `$D_{KL}(\mathcal{N}_1\|\mathcal{N}_2)$` | `kl_gaussian_closed(mu1, sig1, mu2, sig2)` | é–‰å½¢å¼ãƒ»é«˜é€Ÿ |
| `$\sigma_{\max}(W) = \|W\|_2$` | `np.linalg.svd(W, compute_uv=False)[0]` | SVDã®æœ€å¤§ç‰¹ç•°å€¤ |
| `$\text{Perplexity} = 2^H$` | `np.exp2(entropy(p))` | `np.exp2` = `2**x` |

### Z7.5 æœ¬è¬›ç¾©ã®ã‚­ãƒ¼ãƒ†ã‚¤ã‚¯ã‚¢ã‚¦ã‚§ã‚¤

1. **Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**ã¯ä¸ç¢ºå®Ÿæ€§ã®å”¯ä¸€ã®åˆç†çš„å°ºåº¦ã§ã‚ã‚Šã€LLM ã® Perplexity `$= 2^H$` ã®æ­£ä½“
2. **KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹**ã¯éå¯¾ç§°ãªåˆ†å¸ƒé–“è·é›¢ã€‚å‰å‘ã KL ã¯ mode-coveringã€é€†å‘ã KL ã¯ mode-seeking â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æå¤±è¨­è¨ˆã«ç›´çµ
3. **Cross-Entropy ã®æœ€å°åŒ– = KL ã®æœ€å°åŒ–** â€” LLM è¨“ç·´ã®æå¤±é–¢æ•°ãŒæƒ…å ±ç†è«–çš„ã«å¿…ç„¶ã§ã‚ã‚‹ã“ã¨ã®è¨¼æ˜
4. **Adam** ã¯ Momentum + é©å¿œçš„å­¦ç¿’ç‡ + ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã®åˆã‚ã›æŠ€ã€‚AdamW ãŒæ­£ã—ã„é‡ã¿æ¸›è¡°

### Z7.6 FAQ

<details><summary>Q1: KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯è·é›¢ã˜ã‚ƒãªã„ãªã‚‰ã€ãªãœä½¿ã†ã®ã‹ï¼Ÿ</summary>
KL ã¯è·é›¢ã®å…¬ç†ï¼ˆå¯¾ç§°æ€§ã€ä¸‰è§’ä¸ç­‰å¼ï¼‰ã‚’æº€ãŸã•ãªã„ã€‚ã ãŒæ©Ÿæ¢°å­¦ç¿’ã§é‡è¦ãªã®ã¯ã€Œæœ€å°åŒ–å¯èƒ½ã‹ã€ã§ã‚ã‚Šã€KL ã«ã¯ä»¥ä¸‹ã®åˆ©ç‚¹ãŒã‚ã‚‹: (1) æœ€å°åŒ–ãŒ MLE ã¨ç­‰ä¾¡ã€(2) å‹¾é…è¨ˆç®—ãŒå®¹æ˜“ã€(3) æƒ…å ±ç†è«–çš„æ„å‘³ãŒæ˜ç¢ºã€‚çœŸã®ã€Œè·é›¢ã€ãŒæ¬²ã—ã‘ã‚Œã° JS ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚„ Wasserstein è·é›¢ã‚’ä½¿ã†ï¼ˆç¬¬13å›ï¼‰ã€‚
</details>

<details><summary>Q2: Adam ã¨ SGDã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ</summary>
ä¸€èˆ¬å‰‡: **Adam ã§å§‹ã‚ã¦ã€å¿…è¦ãªã‚‰ SGD ã«åˆ‡ã‚Šæ›¿ãˆã‚‹**ã€‚Adam ã¯å­¦ç¿’ç‡ã®èª¿æ•´ãŒæ¥½ã§åæŸãŒé€Ÿã„ãŒã€æ±åŒ–æ€§èƒ½ã§ã¯ SGD+Momentum ã«åŠ£ã‚‹å ´åˆãŒã‚ã‚‹ï¼ˆç‰¹ã«ç”»åƒåˆ†é¡ï¼‰ã€‚LLM è¨“ç·´ã§ã¯ AdamW ãŒã»ã¼æ¨™æº–ã€‚æœ€è¿‘ã®ç ”ç©¶ã§ã¯ Lion ã‚„ Sophia ãªã©æ¬¡ä¸–ä»£æœ€é©åŒ–å™¨ã‚‚ææ¡ˆã•ã‚Œã¦ã„ã‚‹ãŒã€AdamW ã®ãƒ­ãƒã‚¹ãƒˆæ€§ã¯ã¾ã æºã‚‹ãŒãªã„ã€‚
</details>

<details><summary>Q3: Perplexity ã¯ã©ã“ã¾ã§ä¸‹ãŒã‚‹ï¼Ÿ</summary>
ç†è«–çš„ä¸‹é™ã¯ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ `$H(\hat{p})$`ã€‚è‡ªç„¶è¨€èªã¯å†—é•·æ€§ãŒé«˜ã„ãŸã‚ã€è‹±èªãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯1-2 bits/character ç¨‹åº¦ã€‚GPT-4 ã® Perplexity ãŒéå¸¸ã«ä½ã„ã®ã¯ã€äººé–“ã®è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç²¾ç·»ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¦ã„ã‚‹ã‹ã‚‰ã€‚ãŸã ã— Perplexity = 1 ã¯ä¸å¯èƒ½ â€” ãã‚Œã¯ãƒ‡ãƒ¼ã‚¿ã«ä¸ç¢ºå®Ÿæ€§ãŒãªã„ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚
</details>

<details><summary>Q4: f-Divergence ã¯å®Ÿéš›ã«ã©ã“ã§ä½¿ã‚ã‚Œã‚‹ï¼Ÿ</summary>
f-GAN [Nowozin+ 2016] ã¯ f-Divergence ã®å¤‰åˆ†è¡¨ç¾ã‚’ç›´æ¥ä½¿ã£ã¦ GAN ã‚’è¨“ç·´ã™ã‚‹ã€‚`$f$` ã®é¸æŠã«ã‚ˆã‚Š KL-GANã€Pearson-GANã€Hellinger-GAN ãªã©ãŒçµ±ä¸€çš„ã«å°å‡ºã§ãã‚‹ã€‚ã¾ãŸã€å¯†åº¦æ¯”æ¨å®šï¼ˆdensity ratio estimationï¼‰ã«ã‚‚ f-Divergence ã®å¤‰åˆ†è¡¨ç¾ãŒä½¿ã‚ã‚Œã‚‹ã€‚ç¬¬12-14å›ã§å®Ÿè£…ã™ã‚‹ã€‚
</details>

<details><summary>Q5: æ•°å­¦ãŒè‹¦æ‰‹ã§ã‚‚å¤§ä¸ˆå¤«ï¼Ÿ</summary>
Zone 3 ã®å…¨å°å‡ºãŒç†è§£ã§ããªãã¦ã‚‚ã€ä»¥ä¸‹ã®ã€Œæœ€ä½é™ã€ã‚’æŠ¼ã•ãˆã‚Œã°å…ˆã«é€²ã‚ã‚‹: (1) ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = ä¸ç¢ºå®Ÿæ€§ã€(2) KL = åˆ†å¸ƒã®è·é›¢ï¼ˆéå¯¾ç§°ï¼‰ã€(3) Cross-Entropy æœ€å°åŒ– = KL æœ€å°åŒ–ã€(4) Adam = è³¢ã„ SGDã€‚æ•°å¼ã¯ç¹°ã‚Šè¿”ã—è§¦ã‚‹ã“ã¨ã§èº«ä½“ã«æŸ“ã¿ã‚‹ã€‚ç¬¬7-16å›ã§åŒã˜é“å…·ã‚’ä½•åº¦ã‚‚ä½¿ã†ã‹ã‚‰ã€ä»Šå®Œå…¨ã«ç†è§£ã™ã‚‹å¿…è¦ã¯ãªã„ã€‚
</details>

<details><summary>Q6: ãªãœ MSEï¼ˆå¹³å‡äºŒä¹—èª¤å·®ï¼‰ã§ã¯ãªã Cross-Entropy ã‚’ä½¿ã†ã®ã‹ï¼Ÿ</summary>
åˆ†é¡å•é¡Œã§ã¯å‡ºåŠ›ãŒç¢ºç‡åˆ†å¸ƒãªã®ã§ã€åˆ†å¸ƒé–“ã®è·é›¢ã‚’æ¸¬ã‚‹ Cross-Entropy ãŒè‡ªç„¶ãªé¸æŠã€‚MSE ã¯å›å¸°å•é¡Œå‘ãã§ã€ç¢ºç‡åˆ†å¸ƒã®æ¯”è¼ƒã«ã¯æƒ…å ±ç†è«–çš„ã«æ ¹æ‹ ãŒãªã„ã€‚å…·ä½“çš„ã«ã¯ã€Cross-Entropy ã¯å‡ºåŠ›ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¯¾ã—ã¦é©åˆ‡ãªå‹¾é…ã‚’ä¸ãˆã‚‹ãŒã€MSE + Sigmoid ã¯å‡ºåŠ›ãŒ 0 ã‚„ 1 ã«è¿‘ã„ã¨ãã«å‹¾é…ãŒæ¶ˆå¤±ã™ã‚‹ï¼ˆsigmoid ã® saturation å•é¡Œï¼‰ã€‚
</details>

<details><summary>Q7: KL ã¯è·é›¢ã®ä¸‰è§’ä¸ç­‰å¼ã‚’æº€ãŸã•ãªã„ãŒã€å›°ã‚‰ãªã„ã®ã‹ï¼Ÿ</summary>
å®Ÿç”¨ä¸Šã¯å›°ã‚‰ãªã„ã€‚ä¸‰è§’ä¸ç­‰å¼ `$D(p, r) \leq D(p, q) + D(q, r)$` ã¯ã€Œä¸­é–“ç‚¹ã‚’çµŒç”±ã—ã¦ã‚‚è·é›¢ãŒå¢—ãˆãªã„ã€ã¨ã„ã†æ€§è³ªã ãŒã€æœ€é©åŒ–ã§ã¯ç›´æ¥ `$D(p_\text{data}, q_\theta)$` ã‚’æœ€å°åŒ–ã™ã‚‹ã®ã§ä¸­é–“ç‚¹ã¯ä¸è¦ã€‚ãŸã ã—ã€ç†è«–çš„ãªè§£æï¼ˆåæŸãƒ¬ãƒ¼ãƒˆã®è¨¼æ˜ãªã©ï¼‰ã§ã¯ä¸‰è§’ä¸ç­‰å¼ãŒä¾¿åˆ©ãªã®ã§ã€ãã†ã„ã†å ´é¢ã§ã¯ TV è·é›¢ã‚„ Wasserstein è·é›¢ã‚’ä½¿ã†ã€‚
</details>

<details><summary>Q8: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒæœ€å¤§ã®ã¨ãã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œä½•ã‚‚å­¦ã‚“ã§ã„ãªã„ã€ã®ã‹ï¼Ÿ</summary>
ãã®é€šã‚Šã€‚ä¸€æ§˜åˆ†å¸ƒã¯æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†å¸ƒã§ã‚ã‚Šã€ã€Œå…¨ã¦ã®é¸æŠè‚¢ãŒç­‰ç¢ºç‡ã€= ã€Œä½•ã®æƒ…å ±ã‚‚ä½¿ãˆã¦ã„ãªã„ã€çŠ¶æ…‹ã€‚å­¦ç¿’ãŒé€²ã‚€ã¨å‡ºåŠ›åˆ†å¸ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒä¸‹ãŒã‚Šã€ç‰¹å®šã®é¸æŠè‚¢ã«ç¢ºç‡ãŒé›†ä¸­ã™ã‚‹ã€‚ã“ã‚ŒãŒ Perplexity ã®æ¸›å°‘ã¨ã—ã¦è¦³æ¸¬ã•ã‚Œã‚‹ã€‚ãŸã ã—ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒä½ã™ãã‚‹ï¼ˆ= 1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ç¢ºç‡ãŒé›†ä¸­ã—ã™ãã‚‹ï¼‰ã®ã‚‚å•é¡Œã§ã€ç”Ÿæˆã®å¤šæ§˜æ€§ãŒå¤±ã‚ã‚Œã‚‹ã€‚ã“ã‚ŒãŒ Temperature ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å­˜åœ¨ç†ç”±ã€‚
</details>

<details><summary>Q9: å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã¯ãªãœ max_norm = 1.0 ãŒä¸€èˆ¬çš„ï¼Ÿ</summary>
ç†è«–çš„ãªæ ¹æ‹ ã¯è–„ã„ã€‚çµŒé¨“çš„ã«ã€å‹¾é…ã®ãƒãƒ«ãƒ ãŒ1ç¨‹åº¦ãªã‚‰å­¦ç¿’ãŒå®‰å®šã™ã‚‹ã¨ã„ã†ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã ã€‚å®Ÿéš›ã«ã¯å­¦ç¿’ç‡ã‚„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦èª¿æ•´ã™ã‚‹ã€‚GPT-3 ã§ã¯ max_norm = 1.0ã€Llama 2 ã§ã¯ max_norm = 1.0 ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚é‡è¦ãªã®ã¯å…·ä½“çš„ãªå€¤ã‚ˆã‚Šã€Œçˆ†ç™ºã‚’é˜²ãå®‰å…¨å¼ãŒã‚ã‚‹ã€ã“ã¨ã€‚
</details>

<details><summary>Q10: Adam ã®ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã¯æœ¬å½“ã«å¿…è¦ï¼Ÿ</summary>
å¿…è¦ã€‚ãƒã‚¤ã‚¢ã‚¹è£œæ­£ãªã—ã® Adamï¼ˆ= RMSProp + Momentumï¼‰ã¯å­¦ç¿’åˆæœŸã«`$m_0 = 0, v_0 = 0$` ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã€åˆæœŸã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¨å®šãŒã‚¼ãƒ­æ–¹å‘ã«åã‚‹ã€‚ä¾‹ãˆã° `$\beta_2 = 0.999$` ã§ step 1 ã§ã¯ `$v_1 = 0.001 \cdot g_1^2$` ã¨æ¥µç«¯ã«å°ã•ãã€`$\hat{v}_1 = v_1 / (1 - 0.999^1) = g_1^2$` ã¨è£œæ­£ã•ã‚Œã‚‹ã€‚è£œæ­£ãŒãªã„ã¨å­¦ç¿’ç‡ãŒå®ŸåŠ¹çš„ã«å¤§ãããªã‚Šã™ãã¦ä¸å®‰å®šã«ãªã‚‹ã€‚
</details>

<details><summary>Q8: æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯å®Ÿéš›ã«æ·±å±¤å­¦ç¿’ã§æˆç«‹ã™ã‚‹ã‹ï¼Ÿ</summary>

Tishby & Schwartz-Ziv (2017) ã®ä¸»å¼µã¯è«–äº‰ä¸­ã ã€‚å½¼ã‚‰ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒè¨“ç·´ä¸­ã«æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®åŸç†ã«å¾“ã£ã¦è¡¨ç¾ã‚’åœ§ç¸®ã™ã‚‹ã¨ä¸»å¼µã—ãŸãŒã€Saxe ã‚‰ (2018) ã¯ã€Œç·šå½¢ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã¯æˆç«‹ã—ãªã„ã€ã€Œæ¸¬å®šæ–¹æ³•ã«ä¾å­˜ã™ã‚‹ã€ã¨åè«–ã—ãŸã€‚ç¾åœ¨ã®ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ã¯:

- **éç·šå½¢æ´»æ€§åŒ–é–¢æ•°**ï¼ˆReLU ç­‰ï¼‰ãŒã‚ã‚Œã°éƒ¨åˆ†çš„ã«æˆç«‹ã™ã‚‹è¨¼æ‹ ãŒã‚ã‚‹
- **æœ€å¾Œã®å±¤**ã®è¡¨ç¾ã¯ç¢ºã‹ã« `$I(Z;Y)$` ãŒé«˜ã„å‚¾å‘
- ã€Œæƒ…å ±åœ§ç¸®ãŒ**å­¦ç¿’ã«æœ‰åˆ©**ã‹ã€ã¯ã¾ã ä¸æ˜

ç¢ºå®Ÿã«è¨€ãˆã‚‹ã“ã¨: æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯è¡¨ç¾å­¦ç¿’ã®**ç›®æ¨™**ã¨ã—ã¦æ„å‘³ã®ã‚ã‚‹å®šç¾©ã‚’ä¸ãˆã‚‹ã€‚å®Ÿè£…ï¼ˆSimCLR, VICRegï¼‰ã¯ã“ã®åŸç†ã‹ã‚‰ç€æƒ³ã‚’å¾—ã¦å®Ÿéš›ã«æ©Ÿèƒ½ã—ã¦ã„ã‚‹ï¼ˆç¬¬21å›ã§è©³è¿°ï¼‰ã€‚
</details>

<details><summary>Q9: ãªãœ LLM ã®æå¤±ã« Cross-Entropy ã‚’ä½¿ã„ã€MSE ã‚’ä½¿ã‚ãªã„ã®ã‹ï¼Ÿ</summary>

è¨€èªã¯**é›¢æ•£çš„**ï¼ˆãƒ¯ãƒ³ãƒ›ãƒƒãƒˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã ã€‚MSE ã‚’ä½¿ã†ã¨:

```math
\text{MSE} = \|y - \hat{y}\|^2 \quad y \in \{0,1\}^V, \hat{y} \in [0,1]^V
```

å•é¡Œ: (1) MSE ã¯ç¢ºç‡åˆ†å¸ƒã®æ€§è³ªã‚’ç„¡è¦–ã™ã‚‹ï¼ˆç¢ºç‡ã®å’Œ = 1 ã‚’å¼·åˆ¶ã—ãªã„ï¼‰ã€(2) MSE ã®å‹¾é…ã¯ Softmax ã®é£½å’Œé ˜åŸŸã§æ¶ˆå¤±ã—ã‚„ã™ã„ã€(3) Cross-Entropy ã¯æœ€å°¤æ¨å®šã¨ã—ã¦ç†è«–çš„ä¿è¨¼ãŒã‚ã‚‹ï¼ˆç¬¬7å›ï¼‰ã€‚

é€£ç¶šå€¤å‡ºåŠ›ï¼ˆç”»åƒç”Ÿæˆã€éŸ³å£°åˆæˆï¼‰ã§ã¯ MSEï¼ˆ= ã‚¬ã‚¦ã‚¹å°¤åº¦ï¼‰ã‚‚ä½¿ã‚ã‚Œã‚‹ã€‚Diffusion ãƒ¢ãƒ‡ãƒ«ã®æå¤± `$\|\epsilon - \hat{\epsilon}\|^2$` ã¯ã¾ã•ã« MSE â€” ã“ã‚Œã¯ã€Œãƒã‚¤ã‚ºã‚’ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¨ã—ã¦æœ€å°¤æ¨å®šã™ã‚‹ã€ã¨è§£é‡ˆã§ãã‚‹ï¼ˆç¬¬5å›ï¼‰ã€‚
</details>

### Z7.7 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | ç›®å®‰æ™‚é–“ |
|:---|:-----|:---------|
| Day 1 | Zone 0-2 ã‚’é€šèª­ + ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—ç·´ç¿’ | 45åˆ† |
| Day 2 | Zone 3 ã® 3.1-3.5ï¼ˆæƒ…å ±ç†è«–ãƒ‘ãƒ¼ãƒˆï¼‰ã‚’ç´™ã§å°å‡º | 90åˆ† |
| Day 3 | Zone 3 ã® 3.7-3.11ï¼ˆæœ€é©åŒ–ãƒ‘ãƒ¼ãƒˆï¼‰ã‚’ç´™ã§å°å‡º | 90åˆ† |
| Day 4 | Zone 4 ã®ã‚³ãƒ¼ãƒ‰ã‚’å…¨ã¦å®Ÿè¡Œ + æ”¹é€  | 60åˆ† |
| Day 5 | Zone 5 ã®è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ + Adam è«–æ–‡ Pass 1 | 60åˆ† |
| Day 6 | ãƒœã‚¹æˆ¦ã® Cross-Entropy åˆ†è§£ã‚’ç´™ã§å†ç¾ | 45åˆ† |
| Day 7 | ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆæœ€çµ‚ç¢ºèª + Zone 6 ã§èª­æ›¸è¨ˆç”» | 30åˆ† |

### Z7.8 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

ç¬¬6å›ã®å­¦ç¿’ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨˜éŒ²ã›ã‚ˆã€‚

| é …ç›® | å®Œäº† | ãƒ¡ãƒ¢ |
|:-----|:----:|:-----|
| Part1 Z1: ãƒ—ãƒ­ãƒ­ãƒ¼ã‚° (entropy è¨ˆç®—ç¢ºèª) | â˜ | |
| Part1 Z2: 5ãƒˆãƒ”ãƒƒã‚¯æ¦‚è¦³ (æ¯”è¼ƒè¡¨) | â˜ | |
| Part1 Z3: å‹•æ©Ÿãƒ»å…¨ä½“åƒã®ç†è§£ | â˜ | |
| Part1 Z4: Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å…¨å°å‡º | â˜ | |
| Part1 Z4: KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ & éå¯¾ç§°æ€§ | â˜ | |
| Part1 Z4: f-Divergence & GAN ç›®çš„é–¢æ•° | â˜ | |
| Part1 Z4: SGD / Adam / AdamW æ›´æ–°å‰‡ | â˜ | |
| Part1 Z4: å‡¸æœ€é©åŒ–åŒå¯¾æ€§ & KKT æ¡ä»¶ | â˜ | |
| Part2 Z5: æƒ…å ±ç†è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå®Ÿè£… | â˜ | |
| Part2 Z5: Adam / AdamW ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£… | â˜ | |
| Part2 Z5: æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ•°å€¤æ¤œè¨¼ | â˜ | |
| Part2 Z5: Gaussian KL é–‰å½¢å¼æ¤œè¨¼ | â˜ | |
| Part2 Z5: Î²-VAE Quick Check å®Œç­” | â˜ | |
| Part2 Z5b: è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ 10å•ä»¥ä¸Š | â˜ | |
| Part2 Z5b: ã‚³ãƒ¼ãƒ‰ç¿»è¨³ 5å• | â˜ | |
| Part2 Z6: arXiv è«–æ–‡ 3æœ¬èª­ã‚“ã  | â˜ | |
| Part2 Z7: FAQ å…¨èª­ã¿ | â˜ | |
| PB: ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„ã«å¯¾ã—ã¦è‡ªåˆ†ã®ç­”ãˆã‚’æ›¸ã„ãŸ | â˜ | |

**å®Œäº†ç‡**: `__/18 é …ç›®`

### Z7.9 æ¬¡å›äºˆå‘Š â€” ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–

ç¬¬6å›ã§æƒ…å ±ç†è«–ã¨æœ€é©åŒ–ã®æ­¦å™¨ãŒæƒã£ãŸã€‚æ¬¡å›ã¯ã„ã‚ˆã„ã‚ˆ**æœ€å°¤æ¨å®šã®æ•°å­¦æ§‹é€ **ã«æ­£é¢ã‹ã‚‰å‘ãåˆã†ã€‚

- **MLE ã®æ•°å­¦**: Cross-Entropy æœ€å°åŒ– = KL æœ€å°åŒ– = MLE ã®ä¸‰ä½ä¸€ä½“è¨¼æ˜ï¼ˆæœ¬è¬›ç¾©ã®ç›´æ¥çš„ç¶šãï¼‰
- **æ¨å®šé‡ã®åˆ†é¡ä½“ç³»**: æ˜ç¤ºçš„å°¤åº¦ / æš—é»™çš„æ¨å®š / ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°
- **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€ãƒãƒƒãƒ—**: VAE / GAN / Flow / Diffusion ã‚’ã€Œæ¨å®šé‡ã®è¨­è¨ˆã€ã¨ã—ã¦é³¥ç°
- **è©•ä¾¡æŒ‡æ¨™**: FID / KID / CMMD â€” çµ±è¨ˆçš„è·é›¢ã®å¿œç”¨
- **Python ã®é…ã•åŠ é€Ÿ**: MLE ã®åå¾©è¨ˆç®—ã§ã€Œé…ã™ããªã„ï¼Ÿã€ã®ä¸æº€ãŒå¢—å¹…

ç¬¬5å›ã¾ã§ã®æ•°å­¦åŸºç›¤ã¨ã€æœ¬è¬›ç¾©ã®æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ãŒã€ã“ã“ã‹ã‚‰å…ˆã®å…¨ã¦ã‚’æ”¯ãˆã‚‹ã€‚**6è¬›ç¾©ã®æ•°å­¦çš„æ­¦è£…ãŒå®Œäº†ã—ãŸ**ã€‚KL ã¯æå¤±é–¢æ•°ã«ã€SGD ã¯å­¦ç¿’ã«ã€Wasserstein è·é›¢ã¯è©•ä¾¡ã«ç›´çµã™ã‚‹ â€” ã“ã®å…¨ã¦ãŒç„¡ã‘ã‚Œã°ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åœ°å›³ã¯èª­ã‚ãªã„ã€‚

**æœ¬è¬›ç¾©ã‹ã‚‰ç¬¬7å›ã¸ã®æ¥ç¶š**:

| æœ¬è¬›ç¾© (ç¬¬6å›) | ç¬¬7å›ã¸ã®æ©‹æ¸¡ã— |
|:-------------|:-------------|
| `$D_{KL}(p_{\text{data}} \| p_\theta)$` ã‚’æœ€å°åŒ– | ã“ã‚ŒãŒ MLE ã®å®šç¾©: `$\hat{\theta}_{MLE} = \arg\min_\theta D_{KL}$` |
| Cross-Entropy `$H(p, q_\theta) = H(p) + D_{KL}$` | LLM è¨“ç·´ = MLE ã®å®Ÿè£…ã€`$H(p)$` ã¯å®šæ•° |
| Softmax + æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ | å¤šé …åˆ†å¸ƒã® MLE ã¯ Softmax å‡ºåŠ›ã¨ã—ã¦è‡ªç„¶ã«å°å‡º |
| Fisher æƒ…å ±è¡Œåˆ— `$\mathcal{I}(\theta)$` | CramÃ©r-Rao ä¸‹é™: `$\text{Var}(\hat{\theta}) \geq \mathcal{I}(\theta)^{-1}$` |
| SGD ã®ç¢ºç‡çš„æ›´æ–° | ã‚¹ã‚³ã‚¢é–¢æ•° `$\nabla_\theta \log p_\theta(x)$` ãŒå‹¾é…ã®æœ¬è³ª |
| å¤‰åˆ†ä¸‹ç•Œ ELBO | MLE ãŒå›°é›£ â†’ è¿‘ä¼¼: VAE ã®å‹•æ©Ÿ |

**æº–å‚™**: ç¬¬7å›ã§ã¯ `$\log p_\theta(x)$` ã®å…·ä½“çš„ãªå½¢ï¼ˆã‚¬ã‚¦ã‚¹ / ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ« / Bernoulliï¼‰ã‚’æ‰±ã†ã€‚æœ¬è¬›ç¾©ã® KL ã¨ Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒå…¨ã¦å‰æçŸ¥è­˜ã¨ã—ã¦ç™»å ´ã™ã‚‹ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ç¬¬6å›ã€Œæƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–ã€å®Œäº†ã€‚Course I ã®æ•°å­¦çš„æ­¦è£…ã¯ã“ã‚Œã§6/8ã€‚æ¬¡å›ã‹ã‚‰ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ä¸–ç•Œã«è¶³ã‚’è¸ã¿å…¥ã‚Œã‚‹ã€‚

> Progress: 100%

---


### PB ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯å¯¾ç§°ã§ã™ã‚‰ãªã„"è·é›¢"ã€‚ãªãœã“ã‚ŒãŒæœ€é©è§£ï¼Ÿ**

ã“ã®å•ã„ã‚’3ã¤ã®è¦–ç‚¹ã‹ã‚‰è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚

1. **MLE ã¨ã®ç­‰ä¾¡æ€§**: KL æœ€å°åŒ–ã¨æœ€å°¤æ¨å®šãŒæ•°å­¦çš„ã«ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ï¼ˆç¬¬7å›ã§è¨¼æ˜ï¼‰ã€‚MLE ã¯çµ±è¨ˆå­¦ã§150å¹´ä»¥ä¸Šã®æ­´å²ã‚’æŒã¤æ¨å®šæ³•ã§ã‚ã‚Šã€ä¸€è‡´æ€§ãƒ»æ¼¸è¿‘æ­£è¦æ€§ãƒ»æ¼¸è¿‘æœ‰åŠ¹æ€§ã‚’æŒã¤ã€‚KL ã¯ã“ã®ã€Œç”±ç·’æ­£ã—ã„æ¨å®šæ³•ã€ã®æƒ…å ±ç†è«–çš„ãªé¡”ã«éããªã„ã€‚

2. **è¨ˆç®—å¯èƒ½æ€§**: Wasserstein è·é›¢ã¯å¯¾ç§°ã§ä¸‰è§’ä¸ç­‰å¼ã‚‚æº€ãŸã™ã€Œæ­£ã—ã„è·é›¢ã€ã ãŒã€é«˜æ¬¡å…ƒã§ã¯è¨ˆç®—ãŒå›°é›£ã€‚KL ã¯æœŸå¾…å€¤ã¨ã—ã¦æ›¸ã‘ã‚‹ãŸã‚ã€Monte Carlo æ¨å®šãŒå®¹æ˜“ã€‚å®Ÿç”¨ä¸Šã€è¨ˆç®—å¯èƒ½ãªã€Œä¸å®Œå…¨ãªè·é›¢ã€ã®æ–¹ãŒã€è¨ˆç®—ä¸èƒ½ãªã€Œå®Œå…¨ãªè·é›¢ã€ã‚ˆã‚Šæœ‰ç”¨ã ã€‚

3. **éå¯¾ç§°æ€§ã¯æ©Ÿèƒ½**: éå¯¾ç§°æ€§ã¯æ¬ ç‚¹ã§ã¯ãªãç‰¹å¾´ã€‚å‰å‘ã KL ã¨é€†å‘ã KL ãŒç•°ãªã‚‹æŒ¯ã‚‹èˆã„ã‚’ã™ã‚‹ã‹ã‚‰ã“ãã€ç›®çš„ã«å¿œã˜ãŸæå¤±é–¢æ•°è¨­è¨ˆãŒã§ãã‚‹ã€‚VAEï¼ˆå‰å‘ãï¼‰ã¨ GANï¼ˆé€†å‘ãï¼‰ã®å“è³ªã®é•ã„ã¯ã€ã¾ã•ã« KL ã®éå¯¾ç§°æ€§ã«èµ·å› ã™ã‚‹ã€‚

<details><summary>æ­´å²çš„æ–‡è„ˆ: Shannon ã®ã€Œè³­ã‘ã€</summary>

Shannon ã¯1948å¹´ã®è«–æ–‡ [^1] ã§ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®åå‰ã‚’ç‰©ç†å­¦ã®ç†±åŠ›å­¦ã‹ã‚‰å€Ÿã‚ŠãŸã€‚von Neumann ã«åå‰ã®ç›¸è«‡ã‚’ã—ãŸã¨ã“ã‚ã€Œã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨å‘¼ã¹ã€‚èª°ã‚‚ç†è§£ã—ã¦ã„ãªã„ã‹ã‚‰è­°è«–ã§æœ‰åˆ©ã«ãªã‚‹ã€ã¨è¨€ã‚ã‚ŒãŸã¨ã„ã†é€¸è©±ãŒã‚ã‚‹ã€‚75å¹´å¾Œã€ã“ã®ã€Œèª°ã‚‚ç†è§£ã—ã¦ã„ãªã„ã€é‡ãŒ AI ã®è¨“ç·´ã‚’æ”¯é…ã—ã¦ã„ã‚‹ã€‚Shannon ã®ç›´æ„Ÿã¯æ­£ã—ã‹ã£ãŸ â€” æƒ…å ±ã®æœ¬è³ªã¯ã€Œé©šãã€ã®å®šé‡åŒ–ã«ã‚ã£ãŸã€‚

KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒã€Œæœ€é©è§£ã€ã¨ã—ã¦æ¡ç”¨ã•ã‚ŒãŸçµŒç·¯ã‚‚ã¾ãŸå¶ç„¶ã«è¿‘ã„ã€‚Kullback ã¨ Leibler [^8] ãŒ1951å¹´ã«ç™ºè¡¨ã—ãŸè«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯ "Information and Sufficiency" â€” çµ±è¨ˆçš„ååˆ†é‡ã®ç ”ç©¶ã ã£ãŸã€‚å½¼ã‚‰ã¯ KL ã‚’ã€Œç¢ºç‡åˆ†å¸ƒã®åˆ†é›¢åº¦ã€ã¨ã—ã¦å°å…¥ã—ãŸãŒã€æ©Ÿæ¢°å­¦ç¿’ãŒãã‚Œã‚’æå¤±é–¢æ•°ã¨ã—ã¦æ¡ç”¨ã™ã‚‹ã®ã¯æ•°åå¹´å¾Œã®ã“ã¨ã ã€‚

**å¯¾è©±**: æ•°å­¦çš„ã«ã€Œä¸å®Œå…¨ãªé“å…·ã€ãŒã€ãªãœå®Ÿç”¨ä¸Šã€Œæœ€é©ãªé“å…·ã€ã«ãªã‚‹ã®ã‹ã€‚æ­´å²ãŒç¤ºã™ç­”ãˆã¯ã€Œè¨ˆç®—å¯èƒ½æ€§ãŒæ”¯é…ã™ã‚‹ã€ã ã€‚ç¾ã—ã„å…¬ç†ç³»ã‚ˆã‚Šã€å‹•ãã‚³ãƒ¼ãƒ‰ã®æ–¹ãŒå¼·ã„ã€‚ã‚ãªãŸã¯ã©ã¡ã‚‰å´ã«ç«‹ã¤ï¼Ÿ
</details>

<details><summary>ç™ºå±•: æƒ…å ±å¹¾ä½•å­¦ã‹ã‚‰ã®è¦–ç‚¹</summary>

Amari ã®æƒ…å ±å¹¾ä½•å­¦ã¯ã€ç¢ºç‡åˆ†å¸ƒã®ç©ºé–“ã«ã€Œæ›²ãŒã£ãŸå¹¾ä½•å­¦ã€ã‚’ä¸ãˆã‚‹ã€‚`$\alpha$`-ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æ—:

```math
D_\alpha(p \| q) = \frac{4}{1-\alpha^2} \left(1 - \int p^{(1-\alpha)/2} q^{(1+\alpha)/2} \, dx \right)
```

`$\alpha = 1$` ãŒå‰å‘ã KLã€`$\alpha = -1$` ãŒé€†å‘ã KLã€‚`$\alpha = 0$` ãŒ Fisher-Rao æ¸¬åœ°ç·šè·é›¢ã«ç›¸å½“ã™ã‚‹ã€‚

ã“ã®è¦–ç‚¹ã§ã¯ã€KL ã¯ã€Œe-æ¸¬åœ°ç·šã€ï¼ˆæŒ‡æ•°æ—ã«æ²¿ã£ãŸæœ€çŸ­çµŒè·¯ï¼‰ã¨ã€Œm-æ¸¬åœ°ç·šã€ï¼ˆæ··åˆæ—ã«æ²¿ã£ãŸæœ€çŸ­çµŒè·¯ï¼‰ã®2ç¨®é¡ã®ã€Œç›´ç·šã€ã‚’å®šç¾©ã™ã‚‹ã€‚VAE ã¨ GAN ã®é•ã„ã¯ã€ã“ã®2ç¨®é¡ã®æ¸¬åœ°ç·šã®é•ã„ã¨ã—ã¦çµ±ä¸€çš„ã«ç†è§£ã§ãã‚‹ [^13]ã€‚

è‡ªç„¶å‹¾é…æ³•ã¯ã“ã® Riemannian å¹¾ä½•å­¦ã®ä¸­ã§ã€Fisher è¨ˆé‡ã‚’ä½¿ã£ã¦ã€Œæœ€ã‚‚æ€¥ãªæ–¹å‘ã€ã«é€²ã‚€ã€‚Adam ã¯ãã®å¯¾è§’è¿‘ä¼¼ã«éããªã„ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆ `$O(d)$` ã§å®Ÿç”¨çš„ã  â€” ã“ã“ã§ã‚‚ã€Œè¿‘ä¼¼å¯èƒ½æ€§ãŒæ”¯é…ã™ã‚‹ã€æ³•å‰‡ãŒåƒã„ã¦ã„ã‚‹ã€‚
</details>

---


---
## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Shannon, C. E. (1948). "A Mathematical Theory of Communication." *Bell System Technical Journal*, 27(3), 379-423.

[^2]: Kullback, S. & Leibler, R. A. (1951). "On Information and Sufficiency." *Annals of Mathematical Statistics*, 22(1), 79-86.

[^3]: Robbins, H. & Monro, S. (1951). "A Stochastic Approximation Method." *Annals of Mathematical Statistics*, 22(3), 400-407.

[^4]: Kingma, D. P. & Ba, J. (2014). "Adam: A Method for Stochastic Optimization." *ICLR 2015*.
<https://arxiv.org/abs/1412.6980>
[^5]: Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). "Attention Is All You Need." *NeurIPS 2017*.
<https://arxiv.org/abs/1706.03762>
[^6]: Csiszar, I. (1967). "Information-Type Measures of Difference of Probability Distributions and Indirect Observations." *Studia Scientiarum Mathematicarum Hungarica*, 2, 299-318.

[^7]: Nguyen, X., Wainwright, M. J. & Jordan, M. I. (2010). "Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization." *IEEE Transactions on Information Theory*, 56(11), 5847-5861.
<https://arxiv.org/abs/0809.0853>
[^8]: Polyak, B. T. (1964). "Some Methods of Speeding up the Convergence of Iteration Methods." *USSR Computational Mathematics and Mathematical Physics*, 4(5), 1-17.

[^9]: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. (2014). "Generative Adversarial Networks." *NeurIPS 2014*.
<https://arxiv.org/abs/1406.2661>
[^11]: Boyd, S. & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.

[^12]: Miyato, T., Kataoka, T., Koyama, M. & Yoshida, Y. (2018). "Spectral Normalization for Generative Adversarial Networks." *ICLR 2018*.
<https://arxiv.org/abs/1802.05957>
[^13]: Arjovsky, M., Chintala, S. & Bottou, L. (2017). "Wasserstein Generative Adversarial Networks." *ICML 2017*.
<https://arxiv.org/abs/1701.07875>
[^14]: Alpha Entropy Search for New Information-based Bayesian Optimization. (2024). *arXiv preprint*.
<https://arxiv.org/abs/2411.16586>
[^15]: Connecting Jensen-Shannon and Kullback-Leibler. (2025). *arXiv preprint*.
<https://arxiv.org/abs/2510.20644>
[^16]: GAIT: A Geometric Approach to Information Theory. (2019). *arXiv preprint*.
<https://arxiv.org/abs/1906.08325>
[^17]: Information-Theoretic Foundations for Machine Learning. (2024). *arXiv preprint*.
<https://arxiv.org/abs/2407.12288>
[^AMSGrad]: Reddi, S. J., Kale, S., & Kumar, S. (2018). "On the Convergence of Adam and Beyond." *ICLR 2018*.
<https://arxiv.org/abs/1904.09237>
[^AdamW]: Loshchilov, I. & Hutter, F. (2017). "Decoupled Weight Decay Regularization." *ICLR 2019*.
<https://arxiv.org/abs/1711.05101>
[^SAM]: Foret, P., Kleiner, A., Mobahi, H., & Neyshabur, B. (2020). "Sharpness-Aware Minimization for Efficiently Improving Generalization." *ICLR 2021*.
<https://arxiv.org/abs/2010.01412>

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
