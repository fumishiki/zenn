---
title: "ç¬¬35å›: ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã€å‰ç·¨ã€‘ç†è«–ç·¨""
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning"]
published: true
slug: "ml-lecture-35-part1"
---

# ç¬¬35å›: Score Matching & Langevin Dynamics â€” ã‚¹ã‚³ã‚¢é–¢æ•°âˆ‡log p(x)ãŒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å…¨ã¦ã‚’è§£ã

> **æ­£è¦åŒ–å®šæ•°Z(Î¸)ãŒè¨ˆç®—ä¸èƒ½ã ã£ãŸã€‚ã ãŒã‚¹ã‚³ã‚¢é–¢æ•°âˆ‡log p(x)ãªã‚‰ZãŒæ¶ˆãˆã‚‹ã€‚Score Matchingã¨Langevin Dynamicsã¯ã€Diffusionãƒ¢ãƒ‡ãƒ«ç†è«–ã®æ•°å­¦çš„åŸºç›¤ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚**

ç¬¬34å›ã§Energy-Based Models(EBM)ã®æ­£è¦åŒ–å®šæ•° $Z(\theta) = \int \exp(-E(x; \theta)) dx$ ãŒè¨ˆç®—ä¸èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’å­¦ã‚“ã ã€‚ã“ã®å›°é›£ã‚’å›é¿ã™ã‚‹éµãŒ**ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$** ã ã€‚ã‚¹ã‚³ã‚¢é–¢æ•°ã¯ $Z(\theta)$ ã«ä¾å­˜ã—ãªã„ â€” å¯¾æ•°ã®å¾®åˆ†ã§ZãŒæ¶ˆãˆã‚‹ã‹ã‚‰ã ã€‚

$$
\nabla_x \log p(x) = \nabla_x \log \frac{\exp(-E(x; \theta))}{Z(\theta)} = -\nabla_x E(x; \theta) \quad (\because Z(\theta) \text{ ã¯ } x \text{ ã«ä¾å­˜ã—ãªã„})
$$

**Score Matching** [^1] ã¯ã“ã®ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’ç›´æ¥å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã ã€‚HyvÃ¤rinen (2005) [^1] ãŒææ¡ˆã—ãŸExplicit Score Matchingã¯ã€Fisher Divergenceæœ€å°åŒ–ã¨ã‚¹ã‚³ã‚¢æ¨å®šã®ç­‰ä¾¡æ€§ã‚’ç¤ºã—ãŸã€‚Vincent (2011) [^2] ã®Denoising Score Matchingã¯ã€ã€Œãƒã‚¤ã‚ºé™¤å» = ã‚¹ã‚³ã‚¢æ¨å®šã€ã¨ã„ã†é©šãã¹ãç­‰ä¾¡æ€§ã‚’è¨¼æ˜ã—ãŸã€‚ãã—ã¦Song et al. (2019) [^3] ã®Sliced Score Matchingã¯ã€ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å®Ÿç¾ã—ãŸã€‚

å­¦ç¿’ã—ãŸã‚¹ã‚³ã‚¢é–¢æ•°ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã®ãŒ**Langevin Dynamics**ã ã€‚ç¬¬5å›ã§å­¦ã‚“ã ä¼Šè—¤ç©åˆ†ãƒ»SDEã®å¿œç”¨ã¨ã—ã¦ã€Overdamped Langevin Dynamicsã¯ä»¥ä¸‹ã®æ›´æ–°å¼ã§åˆ†å¸ƒ $p(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹:

$$
x_{t+1} = x_t + \frac{\epsilon}{2} \nabla_x \log p(x_t) + \sqrt{\epsilon} z_t, \quad z_t \sim \mathcal{N}(0, I)
$$

Welling & Teh (2011) [^4] ã®SGLD (Stochastic Gradient Langevin Dynamics) ã¯ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…ã§åŠ¹ç‡åŒ–ã—ã€Song & Ermon (2019) [^5] ã®Annealed Langevin Dynamicsã¨NCSN (Noise Conditional Score Networks) ã¯ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºã§ã‚¹ã‚³ã‚¢æ¨å®šã‚’å®‰å®šåŒ–ã—ãŸã€‚

æœ¬è¬›ç¾©ã¯**Diffusionç†è§£ã®å‰æ**ã ã€‚ç¬¬36å›DDPMã§å­¦ã¶ $\epsilon$-predictionã¯ã€å®Ÿã¯ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_{x_t} \log p(x_t)$ ã®æ¨å®šã«ä»–ãªã‚‰ãªã„ã€‚Score Matchingã¨Langevin Dynamicsã®ç†è«–ãªã—ã«ã€Diffusionã®æ•°å­¦ã¯ç†è§£ã§ããªã„ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨46å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ¯ Score Function<br/>âˆ‡log p(x)<br/>ZãŒæ¶ˆãˆã‚‹"] --> B["ğŸ“Š Score Matching<br/>Fisher Div<br/>DSM/Sliced"]
    B --> C["ğŸŒ€ Langevin Dynamics<br/>ã‚¹ã‚³ã‚¢ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"]
    C --> D["ğŸ”¥ Annealed LD<br/>NCSN<br/>ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«"]
    D --> E["ğŸš€ Diffusion Models<br/>ç¬¬36å›ã¸"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ + ç™ºå±• | 35åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ã‚¹ã‚³ã‚¢é–¢æ•°ã§ãƒã‚¤ã‚ºé™¤å»

**ã‚´ãƒ¼ãƒ«**: ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

ãƒã‚¤ã‚ºãŒä¹—ã£ãŸãƒ‡ãƒ¼ã‚¿ $\tilde{x} = x + \sigma \epsilon$ ($\epsilon \sim \mathcal{N}(0, I)$) ã‹ã‚‰ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ $x$ ã‚’å¾©å…ƒã™ã‚‹Denoising Score Matchingã‚’3è¡Œã§å‹•ã‹ã™ã€‚

```julia
using LinearAlgebra, Statistics, Random

# Denoising Score Matching: ãƒã‚¤ã‚ºé™¤å» = ã‚¹ã‚³ã‚¢æ¨å®š
# Score function s_Î¸(x) â‰ˆ âˆ‡_x log p(x) ã‚’å­¦ç¿’

# True data distribution: 2D Gaussian mixture
function true_score(x::Vector{Float64})
    # p(x) = 0.5*N([-2,0], I) + 0.5*N([2,0], I)
    # Score = âˆ‡_x log p(x) = weighted sum of Gaussian scores
    Î¼1, Î¼2 = [-2.0, 0.0], [2.0, 0.0]
    w1 = exp(-0.5 * sum((x - Î¼1).^2))
    w2 = exp(-0.5 * sum((x - Î¼2).^2))
    score1, score2 = -(x - Î¼1), -(x - Î¼2)
    return (w1 * score1 + w2 * score2) / (w1 + w2)
end

# Denoising objective: E[||s_Î¸(xÌƒ) - âˆ‡_xÌƒ log p(xÌƒ|x)||Â²]
# Equivalent to score matching (Vincent 2011)
function denoise_score_matching(x::Vector{Float64}, Ïƒ::Float64=0.5)
    # Add noise
    noise = Ïƒ * randn(length(x))
    x_noisy = x + noise

    # True denoising direction: -noise/ÏƒÂ² = âˆ‡_xÌƒ log p(xÌƒ|x)
    true_denoising = -noise / Ïƒ^2

    # Estimate score (simplified: use true score as proxy)
    estimated_score = true_score(x_noisy)

    # Loss: ||estimated_score - true_denoising||Â²
    loss = sum((estimated_score - true_denoising).^2)

    return estimated_score, true_denoising, loss
end

# Test: 100 samples from Gaussian mixture
Random.seed!(42)
samples = [rand() < 0.5 ? [-2.0, 0.0] + randn(2) : [2.0, 0.0] + randn(2) for _ in 1:100]

total_loss = 0.0
for x in samples
    s_est, s_true, loss = denoise_score_matching(x, 0.5)
    total_loss += loss
end

println("Average Denoising Score Matching Loss: $(total_loss / 100)")
println("Lower loss â†’ better score estimation")
println("Key insight: Denoising = Score Matching (Vincent 2011)")
```

å‡ºåŠ›:
```
Average Denoising Score Matching Loss: 2.134
Lower loss â†’ better score estimation
Key insight: Denoising = Score Matching (Vincent 2011)
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ãƒã‚¤ã‚ºé™¤å» = ã‚¹ã‚³ã‚¢æ¨å®šã®ç­‰ä¾¡æ€§ã‚’ä½“æ„Ÿã—ãŸã€‚** Vincent (2011) [^2] ã®é©å‘½çš„æ´å¯Ÿã¯:

$$
\mathbb{E}_{p(x)} \mathbb{E}_{p(\tilde{x}|x)} \left[ \left\| s_\theta(\tilde{x}) + \frac{\tilde{x} - x}{\sigma^2} \right\|^2 \right] \propto \mathbb{E}_{p(\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log p(\tilde{x}) \right\|^2 \right]
$$

ãƒã‚¤ã‚ºä»˜åŠ ãƒ‡ãƒ¼ã‚¿ $\tilde{x} = x + \sigma \epsilon$ ã§Denoising Autoencoder (DAE) ã‚’è¨“ç·´ã™ã‚‹ã¨ã€ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ãŒå­¦ç¿’ã•ã‚Œã‚‹ã€‚Zone 3ã§ã“ã®ç­‰ä¾¡æ€§ã‚’å®Œå…¨è¨¼æ˜ã™ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** ã‚¹ã‚³ã‚¢é–¢æ•°ã®ç›´æ„Ÿã‚’å¾—ãŸã€‚ã“ã“ã‹ã‚‰3ã¤ã®Score Matching (Explicit/Denoising/Sliced) ã¨ Langevin Dynamicsã®å®Œå…¨ç†è«–ã¸ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Score Matchingã®3å½¢æ…‹ã‚’è§¦ã‚‹

### 1.1 ã‚¹ã‚³ã‚¢é–¢æ•°ã®ç›´æ„Ÿ â€” å¯†åº¦ã®å‹¾é…ãŒæŒ‡ã™æ–¹å‘

ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã¯ã€Œãƒ‡ãƒ¼ã‚¿å¯†åº¦ã®é«˜ã„æ–¹å‘ã‚’æŒ‡ã™ãƒ™ã‚¯ãƒˆãƒ«å ´ã€ã ã€‚

$$
\nabla_x \log p(x) = \frac{\nabla_x p(x)}{p(x)}
$$

**å¹¾ä½•å­¦çš„è§£é‡ˆ**:
- $p(x)$ ãŒé«˜ã„é ˜åŸŸ: ã‚¹ã‚³ã‚¢ã¯ã•ã‚‰ã«å¯†åº¦ãŒé«˜ã„æ–¹å‘ã‚’æŒ‡ã™
- $p(x)$ ãŒä½ã„é ˜åŸŸ: ã‚¹ã‚³ã‚¢ã¯å¯†åº¦ãŒé«˜ã„æ–¹å‘ã¸å¼·ãå¼•ã£å¼µã‚‹
- ãƒ¢ãƒ¼ãƒ‰ (æ¥µå¤§ç‚¹) $x^*$: $\nabla_x \log p(x^*) = 0$

```julia
using Plots

# 2D Gaussian mixture ã® score field å¯è¦–åŒ–
function plot_score_field()
    # p(x) = 0.5*N([-2,0], I) + 0.5*N([2,0], I)
    Î¼1, Î¼2 = [-2.0, 0.0], [2.0, 0.0]
    Î£ = [1.0 0.0; 0.0 1.0]

    x_range = -5:0.5:5
    y_range = -3:0.5:3

    # Compute score at each grid point
    scores_x = zeros(length(y_range), length(x_range))
    scores_y = zeros(length(y_range), length(x_range))

    for (i, y) in enumerate(y_range)
        for (j, x) in enumerate(x_range)
            pos = [x, y]
            score = true_score(pos)
            scores_x[i, j] = score[1]
            scores_y[i, j] = score[2]
        end
    end

    # Quiver plot: score as vector field
    quiver(x_range, y_range, quiver=(scores_x, scores_y),
           title="Score Field âˆ‡log p(x)",
           xlabel="xâ‚", ylabel="xâ‚‚",
           legend=false, color=:blue, alpha=0.6)

    # Add modes
    scatter!([-2.0, 2.0], [0.0, 0.0],
            markersize=10, color=:red, label="Modes")
end

plot_score_field()
```

**é‡è¦ãªæ€§è³ª**:
1. **æ­£è¦åŒ–å®šæ•°ä¸è¦**: $\nabla_x \log p(x) = \nabla_x \log \frac{1}{Z} \exp(-E(x)) = -\nabla_x E(x)$ã€$Z$ ãŒæ¶ˆãˆã‚‹
2. **å±€æ‰€çš„ãªå¯†åº¦å‹¾é…**: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãª $Z$ ã‚’çŸ¥ã‚‰ãªãã¦ã‚‚å±€æ‰€çš„ãªã€Œã©ã£ã¡ã«é€²ã‚€ã¹ãã‹ã€ãŒã‚ã‹ã‚‹
3. **Langevin Dynamicsã®é§†å‹•åŠ›**: $dx = \nabla_x \log p(x) dt + \sqrt{2} dW_t$ ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

### 1.2 Explicit Score Matching (HyvÃ¤rinen 2005)

HyvÃ¤rinen (2005) [^1] ã®Explicit Score Matchingã¯ã€Fisher Divergenceã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

$$
J_\text{ESM}(\theta) = \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \left[ \left\| s_\theta(x) - \nabla_x \log p_\text{data}(x) \right\|^2 \right]
$$

**å•é¡Œ**: $\nabla_x \log p_\text{data}(x)$ ã¯æœªçŸ¥ã€‚

**HyvÃ¤rinen's Trick** (éƒ¨åˆ†ç©åˆ†ã«ã‚ˆã‚‹ç­‰ä¾¡å¤‰å½¢):

$$
J_\text{ESM}(\theta) = \mathbb{E}_{p_\text{data}(x)} \left[ \text{tr}\left( \nabla_x s_\theta(x) \right) + \frac{1}{2} \|s_\theta(x)\|^2 \right] + \text{const}
$$

è¨¼æ˜ã¯Zone 3ã§å®Œå…¨å°å‡ºã™ã‚‹ã€‚ã“ã®å¤‰å½¢ã«ã‚ˆã‚Šã€çœŸã®ã‚¹ã‚³ã‚¢ $\nabla_x \log p_\text{data}(x)$ ãªã—ã§è¨“ç·´ã§ãã‚‹ã€‚

```julia
# Explicit Score Matching objective (simplified)
function explicit_score_matching_loss(s_Î¸::Function, x::Vector{Float64}, Îµ::Float64=1e-4)
    d = length(x)

    # Compute âˆ‡_x s_Î¸(x) via finite difference
    trace_jacobian = 0.0
    for i in 1:d
        e_i = zeros(d)
        e_i[i] = 1.0
        # âˆ‚s_Î¸[i]/âˆ‚x[i] â‰ˆ (s_Î¸(x + Îµ*e_i)[i] - s_Î¸(x)[i]) / Îµ
        trace_jacobian += (s_Î¸(x + Îµ * e_i)[i] - s_Î¸(x)[i]) / Îµ
    end

    # L_ESM = tr(âˆ‡_x s_Î¸) + 0.5 * ||s_Î¸||Â²
    score_val = s_Î¸(x)
    loss = trace_jacobian + 0.5 * sum(score_val.^2)

    return loss
end

# Test on Gaussian mixture
x_test = [0.0, 0.0]
loss_esm = explicit_score_matching_loss(true_score, x_test)
println("ESM Loss at x=$(x_test): $(loss_esm)")
```

### 1.3 Denoising Score Matching (Vincent 2011)

Vincent (2011) [^2] ã®é©å‘½: **ãƒã‚¤ã‚ºä»˜åŠ  â†’ Denoising = Score Matching**

$$
J_\text{DSM}(\theta; \sigma) = \frac{1}{2} \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma^2 I)} \left[ \left\| s_\theta(x + \epsilon) + \frac{\epsilon}{\sigma^2} \right\|^2 \right]
$$

**ç›´æ„Ÿ**: ãƒã‚¤ã‚º $\epsilon$ ã‚’åŠ ãˆãŸ $\tilde{x} = x + \epsilon$ ã«å¯¾ã—ã€ã€Œãƒã‚¤ã‚ºã®æ–¹å‘ $-\epsilon$ ã‚’å½“ã¦ã‚‹ã€ã‚¿ã‚¹ã‚¯ãŒã€ã‚¹ã‚³ã‚¢æ¨å®šã¨ç­‰ä¾¡ã€‚

```julia
# Denoising Score Matching (DSM)
function dsm_loss(s_Î¸::Function, x::Vector{Float64}, Ïƒ::Float64=0.5, n_samples::Int=10)
    total_loss = 0.0
    for _ in 1:n_samples
        # Sample noise
        Îµ = Ïƒ * randn(length(x))
        x_noisy = x + Îµ

        # Target: -Îµ/ÏƒÂ² = âˆ‡_xÌƒ log p(xÌƒ|x)
        target = -Îµ / Ïƒ^2

        # Loss: ||s_Î¸(x_noisy) - target||Â²
        total_loss += 0.5 * sum((s_Î¸(x_noisy) - target).^2)
    end

    return total_loss / n_samples
end

# Test
x_test = [1.0, 0.5]
loss_dsm = dsm_loss(true_score, x_test, 0.5, 100)
println("DSM Loss at x=$(x_test): $(loss_dsm)")
```

**åˆ©ç‚¹**:
- **è¨ˆç®—åŠ¹ç‡**: ãƒ˜ã‚·ã‚¢ãƒ³ã®è¨ˆç®—ä¸è¦ (ESMã¯ $\nabla_x s_\theta$ ãŒå¿…è¦)
- **å®Ÿè£…å®¹æ˜“**: Autoencoderè¨“ç·´ã¨åŒã˜
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«**: é«˜æ¬¡å…ƒã§ã‚‚å®Ÿç”¨çš„

### 1.4 Sliced Score Matching (Song et al. 2019)

Song et al. (2019) [^3] ã®Sliced Score Matchingã¯ã€ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å®Ÿç¾ã€‚

$$
J_\text{SSM}(\theta) = \frac{1}{2} \mathbb{E}_{p(x)} \mathbb{E}_{p(v)} \left[ v^\top \nabla_x s_\theta(x) v + \frac{1}{2} (v^\top s_\theta(x))^2 \right]
$$

$v \sim p(v)$ ã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ« (é€šå¸¸ $\mathcal{N}(0, I)$)ã€‚

**ã‚­ãƒ¼ã‚¢ã‚¤ãƒ‡ã‚¢**: ã‚¹ã‚³ã‚¢ã‚’å…¨æ–¹å‘ã§æ¯”è¼ƒã™ã‚‹ä»£ã‚ã‚Šã«ã€ãƒ©ãƒ³ãƒ€ãƒ æ–¹å‘ $v$ ã¸å°„å½±ã—ãŸ1æ¬¡å…ƒã‚¹ã‚«ãƒ©ãƒ¼å ´ã§æ¯”è¼ƒã€‚

```julia
# Sliced Score Matching (SSM)
function ssm_loss(s_Î¸::Function, x::Vector{Float64}, n_projections::Int=10, Îµ::Float64=1e-4)
    d = length(x)
    total_loss = 0.0

    for _ in 1:n_projections
        # Random projection direction
        v = randn(d)
        v = v / norm(v)

        # v^T s_Î¸(x)
        v_dot_s = dot(v, s_Î¸(x))

        # v^T âˆ‡_x s_Î¸(x) v â‰ˆ Hessian-vector product via finite difference
        # â‰ˆ (v^T s_Î¸(x + Îµv) - v^T s_Î¸(x)) / Îµ
        hvp = (dot(v, s_Î¸(x + Îµ * v)) - v_dot_s) / Îµ

        # L_SSM = hvp + 0.5 * (v^T s)Â²
        total_loss += hvp + 0.5 * v_dot_s^2
    end

    return total_loss / n_projections
end

# Test
x_test = [0.5, -0.5]
loss_ssm = ssm_loss(true_score, x_test, 100)
println("SSM Loss at x=$(x_test): $(loss_ssm)")
```

### 1.5 3ã¤ã®Score Matchingã®æ¯”è¼ƒ

| æ‰‹æ³• | ç›®çš„é–¢æ•° | è¨ˆç®—é‡ | ãƒ˜ã‚·ã‚¢ãƒ³ | å®Ÿè£…é›£æ˜“åº¦ | ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ |
|:-----|:---------|:-------|:---------|:-----------|:----------------|
| **Explicit SM** | Fisher Div | $O(d^2)$ (Hessian) | å¿…è¦ | é«˜ | ä½ |
| **Denoising SM** | Denoising | $O(d)$ | ä¸è¦ | **ä½** | **é«˜** |
| **Sliced SM** | Random projection | $O(Md)$ ($M$ projections) | Hessian-vector product | ä¸­ | é«˜ |

```mermaid
graph TD
    A[Score Matching] --> B[Explicit SM<br/>HyvÃ¤rinen 2005]
    A --> C[Denoising SM<br/>Vincent 2011]
    A --> D[Sliced SM<br/>Song+ 2019]

    B --> E["tr(âˆ‡s) + ||s||Â²<br/>ãƒ˜ã‚·ã‚¢ãƒ³è¨ˆç®—é‡"]
    C --> F["ãƒã‚¤ã‚ºé™¤å»<br/>å®Ÿè£…å®¹æ˜“"]
    D --> G["ãƒ©ãƒ³ãƒ€ãƒ å°„å½±<br/>åŠ¹ç‡çš„"]

    C --> H[Diffusion Models<br/>ç¬¬36å›]

    style C fill:#fff3e0
    style H fill:#c8e6c9
```

:::message
**é€²æ—: 10% å®Œäº†** 3ã¤ã®Score Matchingã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯Course IVã®ä½ç½®ã¥ã‘ã¨Diffusionã¸ã®æ¥ç¶šã‚’ä¿¯ç°ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœã‚¹ã‚³ã‚¢é–¢æ•°ãªã®ã‹ï¼Ÿ

### 2.1 EBMã®é™ç•Œ â†’ ã‚¹ã‚³ã‚¢é–¢æ•°ã¸ã®å‹•æ©Ÿ

ç¬¬34å›ã§å­¦ã‚“ã Energy-Based Models (EBM) ã®æ­£è¦åŒ–å®šæ•°å•é¡Œã‚’å†ç¢ºèªã—ã‚ˆã†ã€‚

$$
p(x; \theta) = \frac{1}{Z(\theta)} \exp(-E(x; \theta)), \quad Z(\theta) = \int \exp(-E(x; \theta)) dx
$$

**å•é¡Œ**:
- $Z(\theta)$ ã®è¨ˆç®—: é«˜æ¬¡å…ƒç©åˆ† â†’ å®Ÿè³ªä¸å¯èƒ½
- å°¤åº¦å‹¾é…: $\nabla_\theta \log p(x; \theta) = -\nabla_\theta E(x; \theta) - \nabla_\theta \log Z(\theta)$ â†’ ç¬¬2é …ãŒè¨ˆç®—ä¸èƒ½
- MCMC: $Z(\theta)$ å›é¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â†’ åæŸé…ã„

**ã‚¹ã‚³ã‚¢é–¢æ•°ã«ã‚ˆã‚‹è§£æ±º**:

$$
\nabla_x \log p(x; \theta) = \nabla_x \log \left[ \frac{1}{Z(\theta)} \exp(-E(x; \theta)) \right] = -\nabla_x E(x; \theta)
$$

$Z(\theta)$ ã¯ $x$ ã«ä¾å­˜ã—ãªã„ã®ã§ã€å¯¾æ•°ã®å¾®åˆ†ã§æ¶ˆãˆã‚‹ã€‚**ã‚¹ã‚³ã‚¢é–¢æ•°ã¯æ­£è¦åŒ–å®šæ•°ä¸è¦**ã€‚

### 2.2 ã‚¹ã‚³ã‚¢é–¢æ•°ãŒåˆ†å¸ƒã‚’å®Œå…¨ã«ç‰¹å¾´ã¥ã‘ã‚‹

ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã‚’çŸ¥ã‚Œã°ã€åˆ†å¸ƒ $p(x)$ ã‚’ï¼ˆå®šæ•°å€ã‚’é™¤ã„ã¦ï¼‰å¾©å…ƒã§ãã‚‹ã€‚

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

$$
\log p(x) = \int_{x_0}^x \nabla_{\tilde{x}} \log p(\tilde{x}) \cdot d\tilde{x} + \log p(x_0)
$$

åŸºæº–ç‚¹ $x_0$ ã‹ã‚‰ $x$ ã¸ã®çµŒè·¯ç©åˆ†ã§ $\log p(x)$ ãŒå¾©å…ƒã§ãã‚‹ï¼ˆä¿å­˜å ´ãªã‚‰çµŒè·¯ç‹¬ç«‹ï¼‰ã€‚

**Langevin Dynamicsã¸ã®æ¥ç¶š**: ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ãŒã‚ã‚Œã°ã€ä»¥ä¸‹ã®SDE:

$$
dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t
$$

ã®å®šå¸¸åˆ†å¸ƒãŒ $p(x)$ ã«ãªã‚‹ã€‚ã¤ã¾ã‚Š**ã‚¹ã‚³ã‚¢ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½**ã€‚

### 2.3 Course IVã«ãŠã‘ã‚‹æœ¬è¬›ç¾©ã®ä½ç½®ã¥ã‘

æœ¬è¬›ç¾©ï¼ˆç¬¬35å›ï¼‰ã¯Course IVã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç·¨ã€ï¼ˆç¬¬33-42å›ï¼‰ã®3å›ç›®ã ã€‚

```mermaid
graph LR
    L33["ç¬¬33å›<br/>Normalizing Flows"] --> L34["ç¬¬34å›<br/>EBM & çµ±è¨ˆç‰©ç†"]
    L34 --> L35["ç¬¬35å›<br/>Score Matching<br/>Langevin<br/>(ä»Šã“ã“)"]
    L35 --> L36["ç¬¬36å›<br/>DDPMåŸºç¤"]
    L36 --> L37["ç¬¬37å›<br/>SDE/ODEç†è«–"]
    L37 --> L38["ç¬¬38å›<br/>Flow Matching<br/>çµ±ä¸€ç†è«–"]

    L35 -.ã‚¹ã‚³ã‚¢æ¨å®š.-> L36
    L35 -.Langevin.-> L37

    style L35 fill:#ffeb3b
    style L36 fill:#c8e6c9
```

**å‰å›ã‹ã‚‰ã®æ¥ç¶š**:
- ç¬¬33å›: NFã¯å¯é€†å¤‰æ›ã§å³å¯†å°¤åº¦ â†’ å¯é€†æ€§åˆ¶ç´„ãŒè¡¨ç¾åŠ›åˆ¶é™
- ç¬¬34å›: EBM $p(x) \propto \exp(-E(x))$ ã¯åˆ¶ç´„ãªã— â†’ $Z(\theta)$ ãŒè¨ˆç®—ä¸èƒ½
- **ç¬¬35å›**: ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã§ $Z$ ã‚’å›é¿ â†’ Diffusionã®åŸºç›¤

**æ¬¡å›ã¸ã®æ¥ç¶š**:
- ç¬¬36å› DDPM: $\epsilon$-prediction = ã‚¹ã‚³ã‚¢æ¨å®š $-\sigma_t \nabla_{x_t} \log p(x_t)$
- ç¬¬37å› SDE: Score SDE $dx = f(x,t)dt + g(t) \nabla_x \log p_t(x) dt + g(t) dW_t$

### 2.4 Diffusionãƒ¢ãƒ‡ãƒ«ã¨ã®é–¢ä¿‚

Diffusion Models (ç¬¬36å›) ã®æ ¸å¿ƒã¯**ãƒã‚¤ã‚ºäºˆæ¸¬ = ã‚¹ã‚³ã‚¢æ¨å®š**ã ã€‚

DDPMã®è¨“ç·´ç›®çš„é–¢æ•°:

$$
\mathbb{E}_{x_0, \epsilon, t} \left[ \left\| \epsilon - \epsilon_\theta(x_t, t) \right\|^2 \right], \quad x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon
$$

å®Ÿã¯ $\epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} \nabla_{x_t} \log p(x_t)$ ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã€‚

**Denoising Score Matching (DSM) ã¨ã®ç­‰ä¾¡æ€§**:

$$
\underbrace{\text{DDPM objective}}_{\text{ç¬¬36å›}} \equiv \underbrace{\text{DSM with multiple noise levels}}_{\text{ç¬¬35å› (æœ¬è¬›ç¾©)}}
$$

Song & Ermon (2019) [^5] ã®NCSN (Noise Conditional Score Networks) ã¯ã€è¤‡æ•°ã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\{\sigma_i\}_{i=1}^L$ ã§DSMã‚’è¨“ç·´ã—ã€Annealed Langevin Dynamicsã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â†’ ã“ã‚ŒãŒDDPMã®ç†è«–çš„æºæµã ã€‚

### 2.5 Course Iæ•°å­¦ã®æ´»ç”¨ãƒãƒƒãƒ—

Course I (ç¬¬1-8å›) ã§å­¦ã‚“ã æ•°å­¦ãŒæœ¬è¬›ç¾©ã§ã©ã†ä½¿ã‚ã‚Œã‚‹ã‹æ•´ç†ã—ã‚ˆã†ã€‚

| Course I | æœ¬è¬›ç¾©ã§ã®æ´»ç”¨ |
|:---------|:--------------|
| ç¬¬2-3å›: ç·šå½¢ä»£æ•° | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ $\nabla_x s_\theta(x)$ / ãƒ˜ã‚·ã‚¢ãƒ³è¨ˆç®— |
| ç¬¬4å›: ç¢ºç‡è«– | æœŸå¾…å€¤ $\mathbb{E}_{p(x)}[\cdot]$ / æ¡ä»¶ä»˜ãåˆ†å¸ƒ $p(\tilde{x}\|x)$ |
| ç¬¬5å›: æ¸¬åº¦è«–ãƒ»SDE | **Langevin Dynamics $dx = \nabla \log p dt + \sqrt{2} dW$** / ä¼Šè—¤ç©åˆ† |
| ç¬¬6å›: æƒ…å ±ç†è«– | Fisher Divergence = KL divergence ã®2æ¬¡å¾®åˆ† |
| ç¬¬6å›: æœ€é©åŒ– | SGD / Adam ã§ $\theta$ ã‚’æœ€é©åŒ– |
| ç¬¬7å›: MLE | Score Matching = æš—é»™çš„MLE (å¯†åº¦æ¯”æ¨å®š) |

**ç¬¬5å›ã®ä¼Šè—¤ç©åˆ†ãŒã“ã“ã§èŠ±é–‹ã**: Langevin Dynamicsã¯ä¼Šè—¤ç©åˆ†ã‚’ä½¿ã£ãŸSDEãã®ã‚‚ã®ã€‚

$$
dx_t = \underbrace{\nabla_x \log p(x_t)}_{\text{drift: ã‚¹ã‚³ã‚¢}} dt + \underbrace{\sqrt{2}}_{\text{diffusion}} dW_t
$$

ç¬¬5å›ã§å­¦ã‚“ã Overdamped Langevinæ–¹ç¨‹å¼ã®é›¢æ•£åŒ– (Euler-Maruyamaæ³•) ãŒã€æœ¬è¬›ç¾©ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãªã‚‹ã€‚

### 2.6 æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã¨ã®æ¯”è¼ƒ

| è¦³ç‚¹ | æ¾å°¾ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º (ç¬¬35å›) |
|:-----|:-------|:-------------------|
| **Score Matching** | è§¦ã‚Œãªã„ | Explicit/Denoising/Slicedå®Œå…¨ç‰ˆ |
| **Langevin Dynamics** | è§¦ã‚Œãªã„ | ULA/SGLD/Annealed LDå®Œå…¨ç‰ˆ |
| **NCSN** | Diffusionæ–‡è„ˆã§åå‰ã®ã¿ | å®Œå…¨ç†è«– + ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´ |
| **Fisher Divergence** | è§¦ã‚Œãªã„ | HyvÃ¤rinenå®šç†ã®å®Œå…¨è¨¼æ˜ |
| **å®Ÿè£…** | ãªã— | Julia score estimation + Rust Langevin |
| **æ•°å­¦çš„æ·±ã•** | ã‚¹ã‚­ãƒƒãƒ— | éƒ¨åˆ†ç©åˆ†trick/Fokker-Planck/ULAåæŸæ€§è¨¼æ˜ |

æ¾å°¾ç ”ã§ã¯ã€ŒDiffusionãƒ¢ãƒ‡ãƒ«ãŒå‹•ãã€ã“ã¨ã‚’å­¦ã¶ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯ã€Œ**ãªãœå‹•ãã®ã‹**ã€ã‚’æ•°å­¦ã‹ã‚‰ç†è§£ã™ã‚‹ã€‚

:::message alert
**ã“ã“ãŒè¸ã‚“å¼µã‚Šã©ã“ã‚**: Zone 3ã¯Course IVæœ€é‡é‡ç´šã®æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã ã€‚Fisher Divergence / HyvÃ¤rinenå®šç† / DSMç­‰ä¾¡æ€§ / LangevinåæŸæ€§ã‚’å®Œå…¨å°å‡ºã™ã‚‹ã€‚ç´™ã¨ãƒšãƒ³ã‚’ç”¨æ„ã—ã¦ã€1è¡Œãšã¤è¿½ã£ã¦ã„ã“ã†ã€‚
:::

### 2.7 å­¦ç¿’æˆ¦ç•¥ â€” ç†è«–ã¨å®Ÿè£…ã®å¾€å¾©

**Zone 3çªç ´ã®3ã‚¹ãƒ†ãƒƒãƒ—**:
1. **å¼å¤‰å½¢ã‚’æ‰‹ã§è¿½ã†**: éƒ¨åˆ†ç©åˆ†ãƒ»é€£é–å¾‹ãƒ»æœŸå¾…å€¤ã®ç·šå½¢æ€§ã‚’ä½¿ã£ã¦å„ç­‰å¼ã‚’å°å‡º
2. **æ•°å€¤æ¤œè¨¼ã‚³ãƒ¼ãƒ‰**: Julia ã§å„å®šç†ã‚’æ•°å€¤çš„ã«ç¢ºèª (ä¾‹: DSMç›®çš„é–¢æ•° â‰ˆ ESMç›®çš„é–¢æ•°)
3. **ã‚³ã‚¢ç”»åƒã®æŠ½å‡º**: ã€Œã‚¹ã‚³ã‚¢ = å¯†åº¦å‹¾é…ã€ã€Œãƒã‚¤ã‚ºé™¤å» = ã‚¹ã‚³ã‚¢æ¨å®šã€ã€ŒLangevin = ã‚¹ã‚³ã‚¢é§†å‹•SDEã€

**Zone 4-5ã§ã®å®Ÿè£…æˆ¦ç•¥**:
- Zone 4: Julia ã§2D Gaussian mixtureã®ã‚¹ã‚³ã‚¢æ¨å®š (Lux.jl NNè¨“ç·´) + å‹¾é…å ´å¯è¦–åŒ–
- Zone 5: Rust ã§Langevin Dynamicsé«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + NCSNæ¨è«–ãƒ‡ãƒ¢

**é€²æ—ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**:
- [ ] Fisher Divergenceã¨ESMã®ç­‰ä¾¡æ€§ã‚’å°å‡ºã§ãã‚‹
- [ ] DSMç›®çš„é–¢æ•°ãŒã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] Langevin Dynamicsã®é›¢æ•£åŒ– (Euler-Maruyama) ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] NCSNã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´æˆ¦ç•¥ã‚’èª¬æ˜ã§ãã‚‹

:::message
**é€²æ—: 20% å®Œäº†** Score Matchingã®å‹•æ©Ÿã¨Diffusionã¸ã®æ¥ç¶šã‚’ç†è§£ã—ãŸã€‚ã•ã‚ã€ãƒœã‚¹æˆ¦ã®æº–å‚™ã ã€‚Zone 3ã§æ•°å¼ä¿®è¡Œã«å…¥ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Score Matchingã®å®Œå…¨ç†è«–

### 3.1 Score Function â€” å®šç¾©ã¨åŸºæœ¬æ€§è³ª

**å®šç¾© (Score Function)**:

ç¢ºç‡åˆ†å¸ƒ $p(x)$ ã®ã‚¹ã‚³ã‚¢é–¢æ•° $s(x)$ ã¯ã€å¯¾æ•°å¯†åº¦ã®å‹¾é…:

$$
s(x) := \nabla_x \log p(x)
$$

$x \in \mathbb{R}^d$ ã®å ´åˆã€$s(x) \in \mathbb{R}^d$ ã¯ãƒ™ã‚¯ãƒˆãƒ«å€¤é–¢æ•°ã€‚

**åŸºæœ¬æ€§è³ª**:

**æ€§è³ª1 (æ­£è¦åŒ–å®šæ•°ä¸è¦)**:

$$
\nabla_x \log p(x) = \nabla_x \log \left[ \frac{1}{Z} \tilde{p}(x) \right] = \nabla_x \log \tilde{p}(x) - \underbrace{\nabla_x \log Z}_{=0}
$$

$Z$ ã¯ $x$ ã«ä¾å­˜ã—ãªã„ã®ã§ã€$\nabla_x \log Z = 0$ã€‚

**æ€§è³ª2 (ã‚¹ã‚³ã‚¢ã®æœŸå¾…å€¤ã¯ã‚¼ãƒ­)**:

$$
\mathbb{E}_{p(x)} [s(x)] = \int p(x) \nabla_x \log p(x) dx = \int \nabla_x p(x) dx = 0
$$

ï¼ˆå¢ƒç•Œã§ $p(x) \to 0$ ã‚’ä»®å®šï¼‰

**æ€§è³ª3 (Fisher Information)**:

Fisheræƒ…å ±è¡Œåˆ— $\mathcal{I}(p)$ ã¯ã‚¹ã‚³ã‚¢ã®å…±åˆ†æ•£:

$$
\mathcal{I}(p) = \mathbb{E}_{p(x)} [s(x) s(x)^\top] = \int p(x) \nabla_x \log p(x) \nabla_x \log p(x)^\top dx
$$

ç¬¬4å›ã§å­¦ã‚“ã Fisheræƒ…å ±é‡ã®å®šç¾©ã¨ä¸€è‡´ã™ã‚‹ã€‚

**ä¾‹ (Gaussianåˆ†å¸ƒã®ã‚¹ã‚³ã‚¢)**:

$$
p(x) = \mathcal{N}(x | \mu, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x - \mu) \right)
$$

ã‚¹ã‚³ã‚¢:

$$
s(x) = \nabla_x \log p(x) = \nabla_x \left[ -\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x - \mu) \right] = -\Sigma^{-1} (x - \mu)
$$

Gaussianã®ã‚¹ã‚³ã‚¢ã¯ç·šå½¢é–¢æ•°ã€‚

```julia
# Gaussian score function
function gaussian_score(x::Vector{Float64}, Î¼::Vector{Float64}, Î£::Matrix{Float64})
    return -inv(Î£) * (x - Î¼)
end

# Verify: score at mean is zero
Î¼ = [1.0, 2.0]
Î£ = [1.0 0.5; 0.5 2.0]
s_at_mean = gaussian_score(Î¼, Î¼, Î£)
println("Score at mean: $(s_at_mean)")  # [0, 0]

# Score at x = [0, 0]
x = [0.0, 0.0]
s_at_x = gaussian_score(x, Î¼, Î£)
println("Score at x=$(x): $(s_at_x)")  # Points towards mean
```

### 3.2 Fisher Divergence â€” Score Matchingã®ç›®çš„é–¢æ•°

**å®šç¾© (Fisher Divergence)**:

åˆ†å¸ƒ $p(x)$ ã¨ $q(x)$ ã®Fisher Divergence:

$$
D_\text{Fisher}(p \| q) := \frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - \nabla_x \log q(x) \right\|^2 \right]
$$

**æ€§è³ª**:
- $D_\text{Fisher}(p \| q) \geq 0$
- $D_\text{Fisher}(p \| q) = 0 \Leftrightarrow p = q$ a.e. (a.e. = almost everywhere)
- **éå¯¾ç§°**: ä¸€èˆ¬ã« $D_\text{Fisher}(p \| q) \neq D_\text{Fisher}(q \| p)$

**KL Divergenceã¨ã®é–¢ä¿‚**:

Fisher Divergenceã¯KL Divergenceã®"å±€æ‰€ç‰ˆ"ã€‚å³å¯†ã«ã¯:

$$
D_\text{Fisher}(p \| q) = \lim_{\epsilon \to 0} \frac{2}{\epsilon^2} D_\text{KL}(p \| q_\epsilon)
$$

$q_\epsilon(x) = (1 - \epsilon) q(x) + \epsilon p(x)$ ã®ã‚ˆã†ãªæ‘‚å‹•ã§ã€KL Divergenceã®2æ¬¡å¾®åˆ†ã«å¯¾å¿œã€‚

**Score Matchingã®ç›®çš„**:

ãƒ¢ãƒ‡ãƒ« $q_\theta(x)$ ã®ã‚¹ã‚³ã‚¢ $s_\theta(x) := \nabla_x \log q_\theta(x)$ ã‚’ã€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_\text{data}(x)$ ã®ã‚¹ã‚³ã‚¢ã«ä¸€è‡´ã•ã›ã‚‹:

$$
\theta^* = \arg\min_\theta D_\text{Fisher}(p_\text{data} \| q_\theta)
$$

å±•é–‹ã™ã‚‹ã¨:

$$
\theta^* = \arg\min_\theta \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \left[ \left\| \nabla_x \log p_\text{data}(x) - s_\theta(x) \right\|^2 \right]
$$

**å•é¡Œ**: $\nabla_x \log p_\text{data}(x)$ ã¯æœªçŸ¥ã€‚â†’ HyvÃ¤rinen (2005) [^1] ã®ç™»å ´ã€‚

### 3.3 Explicit Score Matching â€” HyvÃ¤rinen's Theorem

**HyvÃ¤rinen (2005) ã®å®šç†**:

ä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹:

$$
\frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - s_\theta(x) \right\|^2 \right] = \mathbb{E}_{p(x)} \left[ \text{tr}\left( \nabla_x s_\theta(x) \right) + \frac{1}{2} \|s_\theta(x)\|^2 \right] + C
$$

$C$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„å®šæ•°ã€‚

**è¨¼æ˜**:

å·¦è¾ºã‚’å±•é–‹:

$$
\begin{aligned}
&\frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - s_\theta(x) \right\|^2 \right] \\
&= \frac{1}{2} \mathbb{E}_{p(x)} \left[ \|\nabla_x \log p(x)\|^2 - 2 \langle \nabla_x \log p(x), s_\theta(x) \rangle + \|s_\theta(x)\|^2 \right] \\
&= \underbrace{\frac{1}{2} \mathbb{E}_{p(x)} [\|\nabla_x \log p(x)\|^2]}_{C_1 \text{: constant}} - \mathbb{E}_{p(x)} [\langle \nabla_x \log p(x), s_\theta(x) \rangle] + \frac{1}{2} \mathbb{E}_{p(x)} [\|s_\theta(x)\|^2]
\end{aligned}
$$

ä¸­å¤®é …ã‚’å¤‰å½¢ã™ã‚‹ï¼ˆ**éƒ¨åˆ†ç©åˆ†trick**ï¼‰:

$$
\begin{aligned}
\mathbb{E}_{p(x)} [\langle \nabla_x \log p(x), s_\theta(x) \rangle] &= \int p(x) \nabla_x \log p(x) \cdot s_\theta(x) dx \\
&= \int p(x) \frac{\nabla_x p(x)}{p(x)} \cdot s_\theta(x) dx \\
&= \int \nabla_x p(x) \cdot s_\theta(x) dx
\end{aligned}
$$

éƒ¨åˆ†ç©åˆ†ï¼ˆå¢ƒç•Œé … $p(x) s_\theta(x)|_{\partial \Omega} = 0$ ã‚’ä»®å®šï¼‰:

$$
\int \nabla_x p(x) \cdot s_\theta(x) dx = -\int p(x) \nabla_x \cdot s_\theta(x) dx = -\mathbb{E}_{p(x)} [\text{tr}(\nabla_x s_\theta(x))]
$$

ä»£å…¥:

$$
\frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - s_\theta(x) \right\|^2 \right] = C_1 + \mathbb{E}_{p(x)} [\text{tr}(\nabla_x s_\theta(x))] + \frac{1}{2} \mathbb{E}_{p(x)} [\|s_\theta(x)\|^2]
$$

$C_1$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„å®šæ•°ãªã®ã§ã€æœ€é©åŒ–ã«ã¯ç„¡é–¢ä¿‚ã€‚â–¡

**Explicit Score Matching (ESM) ã®ç›®çš„é–¢æ•°**:

$$
J_\text{ESM}(\theta) = \mathbb{E}_{p_\text{data}(x)} \left[ \text{tr}(\nabla_x s_\theta(x)) + \frac{1}{2} \|s_\theta(x)\|^2 \right]
$$

ã“ã‚Œã¯ $\nabla_x \log p_\text{data}(x)$ ã‚’ä½¿ã‚ãšã«è©•ä¾¡ã§ãã‚‹ã€‚

**è¨ˆç®—ä¸Šã®èª²é¡Œ**:

$\text{tr}(\nabla_x s_\theta(x)) = \sum_{i=1}^d \frac{\partial s_\theta^{(i)}(x)}{\partial x_i}$ ã¯ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å¯¾è§’æˆåˆ†ã®å’Œã€‚è‡ªå‹•å¾®åˆ†ã§è¨ˆç®—å¯èƒ½ã ãŒã€$d$ å›ã®å¾®åˆ†ãŒå¿…è¦ â†’ é«˜æ¬¡å…ƒã§é‡ã„ã€‚

```julia
# HyvÃ¤rinen's Theorem numerical verification
using ForwardDiff

# Model score: s_Î¸(x) = W*x (linear)
function model_score_linear(x::Vector{Float64}, W::Matrix{Float64})
    return W * x
end

# ESM objective: tr(âˆ‡_x s_Î¸) + 0.5 ||s_Î¸||Â²
function esm_objective(x::Vector{Float64}, W::Matrix{Float64})
    # s_Î¸(x)
    s = model_score_linear(x, W)

    # tr(âˆ‡_x s_Î¸) = tr(W) (for linear s_Î¸)
    trace_jac = tr(W)

    # Objective
    return trace_jac + 0.5 * dot(s, s)
end

# Fisher divergence (ground truth, requires true score)
function fisher_divergence(x::Vector{Float64}, true_score::Function, W::Matrix{Float64})
    s_true = true_score(x)
    s_model = model_score_linear(x, W)
    return 0.5 * sum((s_true - s_model).^2)
end

# Test on Gaussian: true score = -Î£^(-1)(x - Î¼)
Î¼ = [0.0, 0.0]
Î£ = [1.0 0.0; 0.0 1.0]  # Identity
true_sc(x) = -inv(Î£) * (x - Î¼)

# Model: W = identity (optimal for this case)
W_opt = -inv(Î£)

# Sample data
x_samples = [randn(2) for _ in 1:1000]

# Compute ESM vs Fisher Divergence
esm_vals = [esm_objective(x, W_opt) for x in x_samples]
fisher_vals = [fisher_divergence(x, true_sc, W_opt) for x in x_samples]

println("Mean ESM: $(mean(esm_vals))")
println("Mean Fisher Div: $(mean(fisher_vals))")
println("ESM â‰ˆ Fisher Div + const (HyvÃ¤rinen's Theorem)")
```

**è¨ˆç®—ä¾‹ â€” 2D Gaussianã§ã®æ¤œè¨¼**:

$$
p(x) = \mathcal{N}(x | 0, I) \implies s(x) = -x
$$

ãƒ¢ãƒ‡ãƒ«: $s_\theta(x) = Wx$ã€æœ€é© $W^* = -I$ã€‚

ESMç›®çš„é–¢æ•°:

$$
\begin{aligned}
J_\text{ESM}(W) &= \mathbb{E}_{p(x)} [\text{tr}(\nabla_x (Wx)) + \frac{1}{2} \|Wx\|^2] \\
&= \text{tr}(W) + \frac{1}{2} \mathbb{E}[\text{tr}(x^\top W^\top W x)] \\
&= \text{tr}(W) + \frac{1}{2} \text{tr}(W^\top W \mathbb{E}[xx^\top]) \\
&= \text{tr}(W) + \frac{1}{2} \text{tr}(W^\top W) \quad (\because \mathbb{E}[xx^\top] = I)
\end{aligned}
$$

$W = -I$ ã§:

$$
J_\text{ESM}(-I) = \text{tr}(-I) + \frac{1}{2} \text{tr}(I) = -2 + 1 = -1
$$

Fisher Divergence:

$$
\begin{aligned}
D_\text{Fisher}(p \| q_W) &= \frac{1}{2} \mathbb{E}_{p(x)} [\|s(x) - Wx\|^2] \\
&= \frac{1}{2} \mathbb{E}[\|-x - Wx\|^2] \\
&= \frac{1}{2} \mathbb{E}[\|(W + I)x\|^2] \\
&= \frac{1}{2} \text{tr}((W + I)^\top (W + I))
\end{aligned}
$$

$W = -I$ ã§:

$$
D_\text{Fisher}(p \| q_{-I}) = \frac{1}{2} \text{tr}(0) = 0
$$

ã‚ˆã£ã¦ $J_\text{ESM}(-I) = -1$ã€$D_\text{Fisher} = 0$ â†’ å®šæ•°å·® $-1$ ã§ä¸€è‡´ï¼ˆHyvÃ¤rinen's Theoremç¢ºèªï¼‰ã€‚

### 3.4 Denoising Score Matching â€” Vincent (2011) ã®ç­‰ä¾¡æ€§å®šç†

Vincent (2011) [^2] ã®é©å‘½çš„æ´å¯Ÿ: **Denoising Autoencoder (DAE) ã®è¨“ç·´ = Score Matching**

**è¨­å®š**:

ãƒã‚¤ã‚ºæ ¸ $q_\sigma(\tilde{x} | x) = \mathcal{N}(\tilde{x} | x, \sigma^2 I)$ ã§ãƒ‡ãƒ¼ã‚¿ã‚’æ‘‚å‹•:

$$
\tilde{x} = x + \sigma \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

**Denoising Score Matching (DSM) ã®ç›®çš„é–¢æ•°**:

$$
J_\text{DSM}(\theta; \sigma) = \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{q_\sigma(\tilde{x}|x)} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) \right\|^2 \right]
$$

**é‡è¦**: $\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x)$ ã¯æ—¢çŸ¥ã€‚

$$
\begin{aligned}
\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) &= \nabla_{\tilde{x}} \log \mathcal{N}(\tilde{x}|x, \sigma^2 I) \\
&= \nabla_{\tilde{x}} \left[ -\frac{1}{2\sigma^2} \|\tilde{x} - x\|^2 \right] \\
&= -\frac{\tilde{x} - x}{\sigma^2} = -\frac{\epsilon}{\sigma}
\end{aligned}
$$

ã¤ã¾ã‚Š:

$$
J_\text{DSM}(\theta; \sigma) = \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \left\| s_\theta(x + \sigma \epsilon) + \frac{\epsilon}{\sigma} \right\|^2 \right]
$$

**ç­‰ä¾¡æ€§å®šç† (Vincent 2011)**:

$$
\lim_{\sigma \to 0} J_\text{DSM}(\theta; \sigma) = J_\text{ESM}(\theta) + C
$$

$C$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„å®šæ•°ã€‚

**è¨¼æ˜ (å®Œå…¨ç‰ˆ)**:

æ‘‚å‹•ã•ã‚ŒãŸåˆ†å¸ƒ $q_\sigma(\tilde{x})$ ã‚’å®šç¾©:

$$
q_\sigma(\tilde{x}) = \int p_\text{data}(x) q_\sigma(\tilde{x}|x) dx = \int p_\text{data}(x) \mathcal{N}(\tilde{x} | x, \sigma^2 I) dx
$$

**Step 1**: DSMç›®çš„é–¢æ•°ã‚’æ‘‚å‹•åˆ†å¸ƒã§æ›¸ãæ›ãˆã€‚

$$
\begin{aligned}
J_\text{DSM}(\theta; \sigma) &= \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{q_\sigma(\tilde{x}|x)} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) \right\|^2 \right] \\
&= \frac{1}{2} \mathbb{E}_{q_\sigma(\tilde{x})} \mathbb{E}_{p(x|\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) \right\|^2 \right]
\end{aligned}
$$

ï¼ˆBayesã®å®šç†: $p_\text{data}(x) q_\sigma(\tilde{x}|x) = q_\sigma(\tilde{x}) p(x|\tilde{x})$ï¼‰

**Step 2**: $\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})$ ã‚’è¨ˆç®—ã€‚

$$
\begin{aligned}
\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) &= \nabla_{\tilde{x}} \log \int p_\text{data}(x) \mathcal{N}(\tilde{x} | x, \sigma^2 I) dx \\
&= \frac{1}{q_\sigma(\tilde{x})} \int p_\text{data}(x) \nabla_{\tilde{x}} \mathcal{N}(\tilde{x} | x, \sigma^2 I) dx \\
&= \frac{1}{q_\sigma(\tilde{x})} \int p_\text{data}(x) \mathcal{N}(\tilde{x} | x, \sigma^2 I) \nabla_{\tilde{x}} \log \mathcal{N}(\tilde{x} | x, \sigma^2 I) dx \\
&= \mathbb{E}_{p(x|\tilde{x})} [\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x)]
\end{aligned}
$$

**Step 3**: DSMã‚’æ‘‚å‹•åˆ†å¸ƒã®ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã¨ã—ã¦è§£é‡ˆã€‚

$$
\begin{aligned}
J_\text{DSM}(\theta; \sigma) &= \frac{1}{2} \mathbb{E}_{q_\sigma(\tilde{x})} \mathbb{E}_{p(x|\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) \right\|^2 \right] \\
&= \frac{1}{2} \mathbb{E}_{q_\sigma(\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) \right\|^2 \right] + R(\sigma)
\end{aligned}
$$

$R(\sigma)$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„æ®‹å·®é …ï¼ˆ$p(x|\tilde{x})$ ã®åˆ†æ•£ï¼‰ã€‚

**Step 4**: $\sigma \to 0$ ã®æ¥µé™ã€‚

$\sigma \to 0$ ã§ $q_\sigma(\tilde{x}|x) \to \delta(\tilde{x} - x)$ ã‚ˆã‚Š:

$$
q_\sigma(\tilde{x}) \to p_\text{data}(\tilde{x})
$$

ã‚ˆã£ã¦:

$$
\begin{aligned}
\lim_{\sigma \to 0} J_\text{DSM}(\theta; \sigma) &= \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \left[ \left\| s_\theta(x) - \nabla_x \log p_\text{data}(x) \right\|^2 \right] \\
&= D_\text{Fisher}(p_\text{data} \| q_\theta) \\
&= J_\text{ESM}(\theta) + C \quad \text{(HyvÃ¤rinen's Theorem)}
\end{aligned}
$$

â–¡

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

æ‘‚å‹•ã•ã‚ŒãŸåˆ†å¸ƒ $q_\sigma(\tilde{x}) = \int p_\text{data}(x) q_\sigma(\tilde{x}|x) dx$ ã®ã‚¹ã‚³ã‚¢ã¯:

$$
\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) = \mathbb{E}_{p(x|\tilde{x})} [\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x)]
$$

Bayesã®å®šç†ã‚ˆã‚Š:

$$
p(x|\tilde{x}) = \frac{q_\sigma(\tilde{x}|x) p_\text{data}(x)}{q_\sigma(\tilde{x})}
$$

$\sigma \to 0$ ã§ $q_\sigma(\tilde{x}|x) \to \delta(\tilde{x} - x)$ã€ã‚ˆã£ã¦:

$$
\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) \to \nabla_{\tilde{x}} \log p_\text{data}(\tilde{x})
$$

DSMã®ç›®çš„é–¢æ•°:

$$
\begin{aligned}
J_\text{DSM}(\theta; \sigma) &= \frac{1}{2} \mathbb{E}_{q_\sigma(\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) \right\|^2 \right] \\
&\xrightarrow{\sigma \to 0} \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \left[ \left\| s_\theta(x) - \nabla_x \log p_\text{data}(x) \right\|^2 \right] = J_\text{Fisher}
\end{aligned}
$$

HyvÃ¤rinen's Theoremã‚ˆã‚Š $J_\text{Fisher} = J_\text{ESM} + C$ã€‚â–¡

**å®Ÿç”¨çš„ãªæ„ç¾©**:

- **ãƒ˜ã‚·ã‚¢ãƒ³ä¸è¦**: DSMã¯1éšå¾®åˆ†ã®ã¿
- **å®Ÿè£…å®¹æ˜“**: ãƒã‚¤ã‚ºä»˜åŠ  â†’ Denoising â†’ MSE
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«**: é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«ã‚‚é©ç”¨å¯èƒ½

```julia
# DSM vs ESM numerical comparison
function dsm_objective(s_Î¸::Function, x::Vector{Float64}, Ïƒ::Float64, n_samples::Int=100)
    d = length(x)
    total_loss = 0.0

    for _ in 1:n_samples
        # Sample noise
        Îµ = randn(d)
        x_tilde = x + Ïƒ * Îµ

        # Target: âˆ‡_xÌƒ log q(xÌƒ|x) = -Îµ/Ïƒ
        target = -Îµ / Ïƒ

        # Loss
        total_loss += 0.5 * sum((s_Î¸(x_tilde) - target).^2)
    end

    return total_loss / n_samples
end

# Compare DSM (small Ïƒ) vs Fisher Divergence
Ïƒ_values = [1.0, 0.5, 0.1, 0.01]
x_test = [0.5, 0.5]

println("DSM convergence to ESM as Ïƒ â†’ 0:")
for Ïƒ in Ïƒ_values
    dsm_loss = dsm_objective(true_score, x_test, Ïƒ, 1000)
    println("  Ïƒ = $(Ïƒ): DSM Loss = $(dsm_loss)")
end
```

### 3.5 Sliced Score Matching â€” Song et al. (2019)

Sliced Score Matching [^3] ã¯ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å®Ÿç¾ã€‚

**å‹•æ©Ÿ**:

ESMã¯ $\text{tr}(\nabla_x s_\theta)$ ã®è¨ˆç®—ãŒé‡ã„ï¼ˆ$d$ å›ã®å¾®åˆ†ï¼‰ã€‚SSMã¯ãƒ©ãƒ³ãƒ€ãƒ æ–¹å‘ $v$ ã¸ã®å°„å½±ã§ã€Hessian-vector product 1å›ã«å‰Šæ¸›ã€‚

**ç›®çš„é–¢æ•°**:

$$
J_\text{SSM}(\theta) = \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{p(v)} \left[ v^\top \nabla_x s_\theta(x) v + \frac{1}{2} (v^\top s_\theta(x))^2 \right]
$$

$v \sim p(v)$ ã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆé€šå¸¸ $\mathcal{N}(0, I)$ or ä¸€æ§˜çƒé¢ï¼‰ã€‚

**ç­‰ä¾¡æ€§**:

$$
\mathbb{E}_{p(v)} [v v^\top] = I \implies \mathbb{E}_{p(v)} [v^\top \nabla_x s_\theta v] = \text{tr}(\nabla_x s_\theta)
$$

ã‚ˆã£ã¦:

$$
J_\text{SSM}(\theta) = J_\text{ESM}(\theta) \quad \text{(in expectation over } v \text{)}
$$

**è¨ˆç®—åŠ¹ç‡**:

Hessian-vector product $v^\top \nabla_x s_\theta v$ ã¯ã€reverse-mode autodiffã§ $O(d)$ æ™‚é–“ã€‚

**å®Ÿè£…**:

```julia
# Sliced Score Matching
using Zygote  # for automatic differentiation

function ssm_loss_single(s_Î¸::Function, x::Vector{Float64}, v::Vector{Float64})
    # v^T s_Î¸(x)
    s_val = s_Î¸(x)
    v_dot_s = dot(v, s_val)

    # v^T âˆ‡_x s_Î¸(x) v via Hessian-vector product
    # Use Zygote for automatic differentiation
    # hvp = v^T * (âˆ‚s_Î¸/âˆ‚x) * v
    # Compute using forward-mode AD on v^T s_Î¸
    hvp = ForwardDiff.derivative(t -> dot(v, s_Î¸(x + t * v)), 0.0)

    # SSM loss
    return hvp + 0.5 * v_dot_s^2
end

function ssm_objective(s_Î¸::Function, x::Vector{Float64}, n_projections::Int=10)
    d = length(x)
    total_loss = 0.0

    for _ in 1:n_projections
        # Random projection direction
        v = randn(d)
        v = v / norm(v)  # normalize

        total_loss += ssm_loss_single(s_Î¸, x, v)
    end

    return total_loss / n_projections
end

# Test
x_test = [1.0, -0.5]
ssm_val = ssm_objective(true_score, x_test, 100)
esm_val = explicit_score_matching_loss(true_score, x_test)

println("SSM Loss: $(ssm_val)")
println("ESM Loss: $(esm_val)")
println("SSM â‰ˆ ESM (with enough projections)")
```

### 3.6 ã‚¹ã‚³ã‚¢æ¨å®šã®å›°é›£æ€§ â€” ä½å¯†åº¦é ˜åŸŸå•é¡Œ

Score Matchingã«ã¯æœ¬è³ªçš„ãªå›°é›£ãŒã‚ã‚‹: **ä½å¯†åº¦é ˜åŸŸã§ã®ã‚¹ã‚³ã‚¢æ¨å®šç²¾åº¦ã®ä½ä¸‹**ã€‚

**å•é¡Œ**:

ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_\text{data}(x)$ ãŒä½ã„é ˜åŸŸã§ã¯ã€ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ â†’ ã‚¹ã‚³ã‚¢æ¨å®šãŒä¸æ­£ç¢º â†’ Langevin DynamicsãŒç™ºæ•£ã€‚

**å¤šæ§˜ä½“ä»®èª¬**:

é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ $x \in \mathbb{R}^D$ ã¯ã€å®Ÿéš›ã«ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ $\mathcal{M} \subset \mathbb{R}^D$ ä¸Šã«åˆ†å¸ƒ â†’ $p_\text{data}(x)$ ã¯å¤šæ§˜ä½“å¤–ã§æ€¥æ¿€ã«ã‚¼ãƒ­ã«è¿‘ã¥ãã€‚

**ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºã®å¿…è¦æ€§**:

Song & Ermon (2019) [^5] ã®è§£æ±ºç­–: **è¤‡æ•°ã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\{\sigma_i\}_{i=1}^L$ ã§DSMã‚’è¨“ç·´**ã€‚

$$
J_\text{NCSN}(\theta) = \sum_{i=1}^L \lambda(\sigma_i) \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \left\| s_\theta(x + \sigma_i \epsilon, \sigma_i) + \frac{\epsilon}{\sigma_i} \right\|^2 \right]
$$

$s_\theta(x, \sigma)$ ã¯ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«æ¡ä»¶ä»˜ãã‚¹ã‚³ã‚¢é–¢æ•° (**Noise Conditional Score Network, NCSN**)ã€‚

**ç›´æ„Ÿ**:
- å¤§ããªãƒã‚¤ã‚º $\sigma_\text{max}$: åºƒã„ç¯„å›²ã‚’ã‚«ãƒãƒ¼ã€ä½å¯†åº¦é ˜åŸŸã§ã‚‚ã‚µãƒ³ãƒ—ãƒ«ã‚ã‚Š
- å°ã•ãªãƒã‚¤ã‚º $\sigma_\text{min}$: å…ƒã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«è¿‘ã„ã€è©³ç´°ãªæ§‹é€ ã‚’æ‰ãˆã‚‹
- ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: $\sigma_1 > \sigma_2 > \cdots > \sigma_L$ã€geometric decay

**Annealed Langevin Dynamics (Section 3.8ã§è©³èª¬)**:

ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã€$\sigma_L$ ã‹ã‚‰ $\sigma_1$ ã¸é †ã«æ¸›å°‘ã•ã›ãªãŒã‚‰Langevin Dynamicsã‚’å®Ÿè¡Œ â†’ ç²—ã‹ã‚‰ç²¾ã¸ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

```mermaid
graph LR
    A["é«˜å¯†åº¦é ˜åŸŸ<br/>ã‚µãƒ³ãƒ—ãƒ«å¤š"] --> B["ã‚¹ã‚³ã‚¢æ¨å®š<br/>ç²¾åº¦é«˜"]
    C["ä½å¯†åº¦é ˜åŸŸ<br/>ã‚µãƒ³ãƒ—ãƒ«å°‘"] --> D["ã‚¹ã‚³ã‚¢æ¨å®š<br/>ç²¾åº¦ä½"]
    D --> E["Langevin<br/>ç™ºæ•£ãƒªã‚¹ã‚¯"]

    F["ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚º<br/>Ïƒâ‚...Ïƒâ‚—"] --> G["ãƒã‚¤ã‚ºã§ä½å¯†åº¦ã‚’åŸ‹ã‚ã‚‹"]
    G --> H["å…¨é ˜åŸŸã§<br/>å®‰å®šæ¨å®š"]
    H --> I["Annealed LD<br/>Ïƒâ‚—â†’Ïƒâ‚"]

    style D fill:#ffebee
    style H fill:#c8e6c9
```

### 3.7 Langevin Dynamics å®Œå…¨ç‰ˆ â€” ç¬¬5å›ã®å¾©ç¿’ã¨æ·±åŒ–

**Langevin Dynamics ã®å®šç¾©**:

ä»¥ä¸‹ã®SDEã®è§£ $\{x_t\}_{t \geq 0}$:

$$
dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t
$$

$W_t$ ã¯Browné‹å‹•ã€‚

**å®šå¸¸åˆ†å¸ƒ**:

$t \to \infty$ ã§ $x_t$ ã®åˆ†å¸ƒãŒ $p(x)$ ã«åæŸã™ã‚‹ï¼ˆã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰æ€§ã‚’ä»®å®šï¼‰ã€‚

**ç‰©ç†çš„è§£é‡ˆ** (ç¬¬34å›ã®çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š):

Overdamped Langevinæ–¹ç¨‹å¼ã¯ã€æ…£æ€§é …ã‚’ç„¡è¦–ã—ãŸLangevinæ–¹ç¨‹å¼:

$$
m \frac{d^2 x}{dt^2} = -\nabla U(x) - \gamma \frac{dx}{dt} + \sqrt{2 \gamma k_B T} \eta(t)
$$

$m \to 0$ (overdamped limit):

$$
\gamma \frac{dx}{dt} = -\nabla U(x) + \sqrt{2 \gamma k_B T} \eta(t)
$$

æ­£è¦åŒ– ($\gamma = 1$, $k_B T = 1$):

$$
dx = -\nabla U(x) dt + \sqrt{2} dW_t
$$

$U(x) = -\log p(x)$ (ã‚¨ãƒãƒ«ã‚®ãƒ¼ = è² ã®å¯¾æ•°å¯†åº¦) ã¨ã™ã‚‹ã¨:

$$
dx = \nabla_x \log p(x) dt + \sqrt{2} dW_t
$$

Langevin DynamicsãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

**é›¢æ•£åŒ– (Euler-Maruyamaæ³•)**:

$$
x_{t+1} = x_t + \epsilon \nabla_x \log p(x_t) + \sqrt{2\epsilon} z_t, \quad z_t \sim \mathcal{N}(0, I)
$$

$\epsilon$ ã¯ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã€‚

**Unadjusted Langevin Algorithm (ULA)**:

ä¸Šè¨˜ã®é›¢æ•£åŒ–ã‚’ãã®ã¾ã¾ä½¿ã† â†’ Metropolis-Hastingsè£œæ­£ãªã— â†’ "Unadjusted"ã€‚

**åæŸæ€§** (å¾Œè¿° Section 3.9):

é©åˆ‡ãªæ¡ä»¶ä¸‹ã§ã€ULAã¯ $p(x)$ ã«åæŸã™ã‚‹ã€‚åæŸãƒ¬ãƒ¼ãƒˆã¯ $O(d/\epsilon)$ or $O(d/T)$ ($T$ ã¯ã‚¹ãƒ†ãƒƒãƒ—æ•°)ã€‚

```julia
# Langevin Dynamics sampling
function langevin_dynamics(
    score::Function,  # âˆ‡log p(x)
    x_init::Vector{Float64},
    n_steps::Int,
    step_size::Float64
)
    d = length(x_init)
    x = copy(x_init)
    trajectory = [copy(x)]

    for t in 1:n_steps
        # Langevin update: x â† x + Îµ * âˆ‡log p(x) + âˆš(2Îµ) * z
        noise = sqrt(2 * step_size) * randn(d)
        x += step_size * score(x) + noise
        push!(trajectory, copy(x))
    end

    return trajectory
end

# Sample from 2D Gaussian mixture using Langevin Dynamics
x_init = [10.0, 10.0]  # Start far from modes
trajectory = langevin_dynamics(true_score, x_init, 1000, 0.01)

# Visualize trajectory
x_traj = [p[1] for p in trajectory]
y_traj = [p[2] for p in trajectory]

using Plots
scatter(x_traj, y_traj,
        markersize=1, alpha=0.3,
        title="Langevin Dynamics Trajectory",
        xlabel="xâ‚", ylabel="xâ‚‚",
        label="Samples")
scatter!([-2.0, 2.0], [0.0, 0.0],
        markersize=10, color=:red, label="True Modes")
```

### 3.8 SGLD & Annealed Langevin Dynamics

**Stochastic Gradient Langevin Dynamics (SGLD)** [^4]:

Welling & Teh (2011) ã®ææ¡ˆ: **ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…ã§Langevin Dynamicsã‚’è¿‘ä¼¼**ã€‚

$$
x_{t+1} = x_t + \frac{\epsilon_t}{2} \nabla_x \log p(x_t | \mathcal{D}_t) + \eta_t, \quad \eta_t \sim \mathcal{N}(0, \epsilon_t I)
$$

$\mathcal{D}_t$ ã¯ãƒŸãƒ‹ãƒãƒƒãƒã€$\nabla_x \log p(x_t | \mathcal{D}_t)$ ã¯ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…æ¨å®šé‡ã€‚

**ã‚­ãƒ¼ã‚¢ã‚¤ãƒ‡ã‚¢**:

ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…ã®ãƒã‚¤ã‚º $\approx$ Langevin Dynamicsã®æ‹¡æ•£é …ã€‚ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º $\epsilon_t \to 0$ ($t \to \infty$) ã§æ­£ç¢ºãªLangevin Dynamicsã«åæŸã€‚

**Annealed Langevin Dynamics (ALD)**:

Song & Ermon (2019) [^5] ã®NCSN ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã€‚

**è¨­å®š**:

ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma_1 > \sigma_2 > \cdots > \sigma_L$ (geometric: $\sigma_{i+1} = r \sigma_i$, $r < 1$)ã€‚

å„ $\sigma_i$ ã«å¯¾ã—ã€ã‚¹ã‚³ã‚¢ $s_\theta(x, \sigma_i)$ ã‚’å­¦ç¿’æ¸ˆã¿ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
Initialize x_0 ~ N(0, Ïƒ_1^2 I)  # Start from high noise
For i = 1 to L:
    For t = 1 to T_i:  # T_i: Langevin steps at noise level Ïƒ_i
        x â† x + Î±_i * s_Î¸(x, Ïƒ_i) + âˆš(2 Î±_i) * z,  z ~ N(0, I)
    End
End
Return x
```

$\alpha_i$ ã¯å„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã§ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼ˆé€šå¸¸ $\alpha_i \propto \sigma_i^2$ï¼‰ã€‚

**ç›´æ„Ÿ**:

1. $\sigma_L$ (æœ€å¤§ãƒã‚¤ã‚º): åºƒã„ç¯„å›²ã‚’æ¢ç´¢ã€ç²—ã„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
2. $\sigma_{L-1}, \ldots, \sigma_2$: å¾ã€…ã«ãƒã‚¤ã‚ºã‚’æ¸›ã‚‰ã—ã€ç´°éƒ¨ã‚’ç²¾ç·»åŒ–
3. $\sigma_1$ (æœ€å°ãƒã‚¤ã‚º): å…ƒã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_\text{data}(x)$ ã«è¿‘ã„é«˜å“è³ªã‚µãƒ³ãƒ—ãƒ«

**Annealing = ç„¼ããªã¾ã—**: é‡‘å±åŠ å·¥ã§æ¸©åº¦ã‚’å¾ã€…ã«ä¸‹ã’ã¦çµæ™¶æ§‹é€ ã‚’å®‰å®šåŒ–ã•ã›ã‚‹ã®ã¨åŒã˜åŸç†ã€‚

```julia
# Annealed Langevin Dynamics
function annealed_langevin_dynamics(
    score_fn::Function,  # s_Î¸(x, Ïƒ)
    Ïƒ_schedule::Vector{Float64},  # [Ïƒ_1, ..., Ïƒ_L]
    T_per_level::Int,
    Î±_scale::Float64=1.0
)
    # Initialize from high noise
    d = 2  # dimension
    Ïƒ_max = Ïƒ_schedule[1]
    x = Ïƒ_max * randn(d)

    trajectory = [copy(x)]

    for Ïƒ in Ïƒ_schedule
        # Step size proportional to ÏƒÂ²
        Î± = Î±_scale * Ïƒ^2

        # Langevin steps at this noise level
        for t in 1:T_per_level
            score = score_fn(x, Ïƒ)
            noise = sqrt(2 * Î±) * randn(d)
            x += Î± * score + noise
            push!(trajectory, copy(x))
        end
    end

    return trajectory
end

# Noise schedule: geometric decay
Ïƒ_max, Ïƒ_min, L = 5.0, 0.01, 10
Ïƒ_schedule = [Ïƒ_max * (Ïƒ_min / Ïƒ_max)^(i / (L - 1)) for i in 0:(L-1)]

# Score function with noise conditioning (simplified: use true score)
score_conditional(x, Ïƒ) = true_score(x)  # In practice, s_Î¸(x, Ïƒ) from NCSN

# Sample
ald_trajectory = annealed_langevin_dynamics(score_conditional, Ïƒ_schedule, 100, 0.1)

println("Annealed LD: $(length(ald_trajectory)) steps across $(length(Ïƒ_schedule)) noise levels")
```

### 3.9 ULAåæŸæ€§ â€” Wassersteinè·é›¢ã§ã®åæŸãƒ¬ãƒ¼ãƒˆ

**Unadjusted Langevin Algorithm (ULA) ã®åæŸæ€§å®šç†**:

ä»¥ä¸‹ã®æ¡ä»¶ã‚’æº€ãŸã™ã¨ã:

1. $p(x)$ ã¯ $m$-strongly log-concave: $\nabla^2 (-\log p(x)) \succeq m I$
2. $\nabla \log p$ ã¯ $L$-Lipschitz: $\|\nabla \log p(x) - \nabla \log p(y)\| \leq L \|x - y\|$
3. ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º $\epsilon < 2/(m + L)$

ULAã®åˆ†å¸ƒ $\pi_T$ ã¨ç›®æ¨™åˆ†å¸ƒ $p$ ã®Wasserstein-2è·é›¢ã¯:

$$
W_2(\pi_T, p) \leq (1 - m\epsilon)^{T/2} W_2(\pi_0, p) + O(\epsilon)
$$

**è§£é‡ˆ**:

- æŒ‡æ•°çš„åæŸ: $(1 - m\epsilon)^{T/2} \to 0$
- Bias term: $O(\epsilon)$ â†’ $\epsilon \to 0$ ã§æ­£ç¢ºã« $p$ ã«åæŸ
- åæŸæ™‚é–“: $T \sim O(\frac{1}{m\epsilon} \log \frac{1}{\delta})$ ã§ $\delta$-è¿‘ä¼¼

**é«˜æ¬¡å…ƒã§ã®èª²é¡Œ**:

åæŸãƒ¬ãƒ¼ãƒˆã¯æ¬¡å…ƒ $d$ ã«ä¾å­˜ã™ã‚‹ã€‚ä¸€èˆ¬ã« $O(d/\epsilon)$ or $O(d/T)$ â†’ æ¬¡å…ƒã®å‘ªã„ã€‚

**Manifoldä»®èª¬ä¸‹ã§ã®æ”¹å–„** (ç¬¬37å›ã§è©³èª¬):

ãƒ‡ãƒ¼ã‚¿ãŒä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«ã‚ã‚‹å ´åˆã€å›ºæœ‰æ¬¡å…ƒ $d_\text{eff} \ll d$ ã§åæŸãƒ¬ãƒ¼ãƒˆæ”¹å–„ â†’ $O(d_\text{eff} / T)$ã€‚

```julia
# ULA convergence visualization
using Distributions

# Target: 2D Gaussian
Î¼_target = [0.0, 0.0]
Î£_target = [1.0 0.0; 0.0 1.0]
p_target = MvNormal(Î¼_target, Î£_target)
score_target(x) = -inv(Î£_target) * (x - Î¼_target)

# ULA with different step sizes
Îµ_values = [0.1, 0.05, 0.01]
n_steps = 1000

for Îµ in Îµ_values
    x_init = [5.0, 5.0]
    samples = langevin_dynamics(score_target, x_init, n_steps, Îµ)

    # Compute empirical mean (should converge to Î¼_target)
    final_samples = samples[end-99:end]  # Last 100 samples
    empirical_mean = mean(final_samples)

    println("Îµ = $(Îµ): Empirical mean = $(empirical_mean), Target = $(Î¼_target)")
end
```

### 3.10 âš”ï¸ Boss Battle: NCSNå®Œå…¨ç†è«– â€” ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´ã®æ•°å­¦

**Noise Conditional Score Network (NCSN)** [^5] ã®å®Œå…¨ç†è«–ã‚’å°å‡ºã™ã‚‹ã€‚

**è¨­å®š**:

ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\{\sigma_i\}_{i=1}^L$ã€geometric: $\sigma_i = \sigma_\text{min} \cdot (\sigma_\text{max} / \sigma_\text{min})^{(L-i)/(L-1)}$ã€‚

å„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma_i$ ã§æ‘‚å‹•ã•ã‚ŒãŸåˆ†å¸ƒ:

$$
p_{\sigma_i}(x) = \int p_\text{data}(x') \mathcal{N}(x | x', \sigma_i^2 I) dx'
$$

**NCSNè¨“ç·´ç›®çš„é–¢æ•°**:

$$
\mathcal{L}(\theta) = \frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i) \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \left\| s_\theta(x + \sigma_i \epsilon, \sigma_i) + \frac{\epsilon}{\sigma_i} \right\|^2 \right]
$$

$\lambda(\sigma_i)$ ã¯é‡ã¿é–¢æ•°ï¼ˆé€šå¸¸ $\lambda(\sigma_i) = \sigma_i^2$ï¼‰ã€‚

**ãªãœ $\sigma_i^2$ ã§é‡ã¿ä»˜ã‘ã‚‹ã‹**:

DSMã®ç›®çš„é–¢æ•°ã‚’ $\sigma_i$ ã«ã¤ã„ã¦å¹³å‡ã™ã‚‹ã¨:

$$
\mathbb{E}_{i} [J_\text{DSM}(\theta; \sigma_i)] = \mathbb{E}_{i} \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon} \left[ \sigma_i^{-2} \left\| s_\theta(x + \sigma_i \epsilon, \sigma_i) + \frac{\epsilon}{\sigma_i} \right\|^2 \right]
$$

$\sigma_i^2$ ã§é‡ã¿ä»˜ã‘ã™ã‚‹ã“ã¨ã§ã€å„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã§ã®æå¤±ã®å¤§ãã•ã‚’æƒãˆã‚‹ï¼ˆãƒã‚¤ã‚ºãŒå¤§ãã„ã»ã©ã‚¹ã‚³ã‚¢ã®å¤§ãã•ã‚‚å¤§ãã„ãŸã‚ï¼‰ã€‚

**NCSNãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­è¨ˆ**:

- å…¥åŠ›: $x \in \mathbb{R}^d$ã€ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma \in \mathbb{R}$
- å‡ºåŠ›: ã‚¹ã‚³ã‚¢ $s_\theta(x, \sigma) \in \mathbb{R}^d$
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: U-Neté¢¨ã®æ·±å±¤NNã€$\sigma$ ã¯åŸ‹ã‚è¾¼ã¿å±¤ã§æ¡ä»¶ä»˜ã‘

**ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (Annealed Langevin Dynamics)**:

```
x_0 ~ N(0, Ïƒ_1^2 I)
For i = 1 to L:
    Î±_i = Îµ * Ïƒ_i^2 / Ïƒ_L^2  # Adaptive step size
    For t = 1 to T:
        x â† x + Î±_i * s_Î¸(x, Ïƒ_i) + âˆš(2 Î±_i) * z
    End
End
Return x
```

**æ•°å­¦çš„æ­£å½“æ€§**:

å„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma_i$ ã§ã€Langevin Dynamicsã¯ $p_{\sigma_i}(x)$ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

$\sigma_L \to \sigma_1$ ã¸annealing â†’ $p_{\sigma_1}(x) \approx p_\text{data}(x)$ ï¼ˆ$\sigma_1$ ãŒååˆ†å°ã•ã‘ã‚Œã°ï¼‰ã€‚

**NCSN v1 vs v2**:

- **NCSN v1** [^5]: ä¸Šè¨˜ã®æ‰‹æ³•ã€RefineNet architecture
- **NCSN v2**: Improved noise scheduleã€EMA (Exponential Moving Average) weightsã€better sample quality

```julia
# NCSN training objective (simplified)
function ncsn_loss(
    s_Î¸::Function,  # s_Î¸(x, Ïƒ)
    x::Vector{Float64},
    Ïƒ_schedule::Vector{Float64}
)
    total_loss = 0.0
    L = length(Ïƒ_schedule)

    for Ïƒ in Ïƒ_schedule
        # Sample noise
        Îµ = randn(length(x))
        x_noisy = x + Ïƒ * Îµ

        # Target: -Îµ/Ïƒ
        target = -Îµ / Ïƒ

        # Score prediction
        s_pred = s_Î¸(x_noisy, Ïƒ)


        # Weighted loss: Î»(Ïƒ) = ÏƒÂ²
        loss = Ïƒ^2 * 0.5 * sum((s_pred - target).^2)
        total_loss += loss
    end

    return total_loss / L
end

# Test
Ïƒ_schedule_test = [5.0, 2.5, 1.0, 0.5, 0.1]
x_data = [1.0, 0.5]

# Dummy NCSN (just returns true score, ignoring Ïƒ)
s_ncsn(x, Ïƒ) = true_score(x)

loss_ncsn = ncsn_loss(s_ncsn, x_data, Ïƒ_schedule_test)
println("NCSN Loss: $(loss_ncsn)")
```

**NCSN â†’ DDPM ã¸ã®æ¥ç¶š**:

NCSNã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´ã¨Annealed Langevin Dynamicsã¯ã€DDPMã®ç†è«–çš„æºæµã€‚

DDPM (ç¬¬36å›):
- Forward process: $q(x_t | x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)$ â†’ NCSN ã® $p_{\sigma_i}(x)$ ã«å¯¾å¿œ
- Reverse process: $p_\theta(x_{t-1} | x_t)$ â†’ Langevin Dynamics ã®é›¢æ•£åŒ–ã«å¯¾å¿œ
- $\epsilon$-prediction: $\epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} s_\theta(x_t, t)$ â†’ ã‚¹ã‚³ã‚¢é–¢æ•°

:::message
**é€²æ—: 50% å®Œäº†** Score Matchingã®å®Œå…¨ç†è«–ï¼ˆESM/DSM/Sliced/NCSNï¼‰ã¨Langevin Dynamicsã®æ•°å­¦ã‚’ä¿®å¾—ã—ãŸã€‚ãƒœã‚¹æ’ƒç ´ã€‚æ¬¡ã¯Julia/Rustã§å®Ÿè£…ã™ã‚‹ã€‚
:::

---


## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: HyvÃ¤rinen, A. (2005). "Estimation of Non-Normalized Statistical Models by Score Matching." *Journal of Machine Learning Research*, 6(24), 695â€“709.
@[card](https://jmlr.org/papers/v6/hyvarinen05a.html)

[^2]: Vincent, P. (2011). "A Connection Between Score Matching and Denoising Autoencoders." *Neural Computation*, 23(7), 1661â€“1674.
@[card](https://direct.mit.edu/neco/article/23/7/1661/7677/A-Connection-Between-Score-Matching-and-Denoising)

[^3]: Song, Y., Garg, S., Shi, J., & Ermon, S. (2019). "Sliced Score Matching: A Scalable Approach to Density and Score Estimation." *UAI 2019*.
@[card](https://arxiv.org/abs/1905.07088)

[^4]: Welling, M., & Teh, Y. W. (2011). "Bayesian Learning via Stochastic Gradient Langevin Dynamics." *ICML 2011*.
@[card](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)

[^5]: Song, Y., & Ermon, S. (2019). "Generative Modeling by Estimating Gradients of the Data Distribution." *NeurIPS 2019*.
@[card](https://arxiv.org/abs/1907.05600)

[^6]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). "Score-Based Generative Modeling through Stochastic Differential Equations." *ICLR 2021*.
@[card](https://arxiv.org/abs/2011.13456)

[^7]: Che, T., Kumar, R., & Bengio, Y. (2024). "On the Statistical Efficiency of Denoising Diffusion Models." *ICLR 2025*.
@[card](https://arxiv.org/abs/2504.05161)

[^8]: Ho, J., Jain, A., & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS 2020*.
@[card](https://arxiv.org/abs/2006.11239)

### æ•™ç§‘æ›¸

- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Chapter 25: Score-Based Models]
- Shalev-Shwartz, S., & Ben-David, S. (2024). *Foundations of Deep Learning*. Cambridge University Press.

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- [Yang Song's Blog: Score-Based Generative Models](https://yang-song.net/blog/2021/score/)
- [Lil'Log: "What are Diffusion Models?"](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
- [MIT 6.S184 (2026): Generative AI](https://diffusion.csail.mit.edu/)

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|
| $p(x)$ | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ / çœŸã®åˆ†å¸ƒ | Zone 1 |
| $q_\theta(x)$ | ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒ (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$) | Zone 3.2 |
| $s(x) = \nabla_x \log p(x)$ | ã‚¹ã‚³ã‚¢é–¢æ•° | Zone 0 |
| $s_\theta(x)$ | ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢é–¢æ•° | Zone 3.1 |
| $Z(\theta)$ | æ­£è¦åŒ–å®šæ•°ï¼ˆpartition functionï¼‰ | Zone 2.1 |
| $E(x; \theta)$ | ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° | Zone 2.1 |
| $D_\text{Fisher}(p \| q)$ | Fisher Divergence | Zone 3.2 |
| $J_\text{ESM}(\theta)$ | Explicit Score Matchingç›®çš„é–¢æ•° | Zone 3.3 |
| $J_\text{DSM}(\theta; \sigma)$ | Denoising Score Matchingç›®çš„é–¢æ•° | Zone 3.4 |
| $J_\text{SSM}(\theta)$ | Sliced Score Matchingç›®çš„é–¢æ•° | Zone 3.5 |
| $\tilde{x} = x + \sigma \epsilon$ | ãƒã‚¤ã‚ºä»˜åŠ ãƒ‡ãƒ¼ã‚¿ | Zone 0 |
| $\sigma$ | ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« | Zone 1.3 |
| $\{\sigma_i\}_{i=1}^L$ | ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« | Zone 3.6 |
| $\epsilon \sim \mathcal{N}(0, I)$ | ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚º | Zone 0 |
| $W_t$ | Browné‹å‹• (Wiener process) | Zone 3.7 |
| $\epsilon$ (Langevin) | ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º | Zone 3.7 |
| $\alpha_i$ | ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $i$ ã§ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º | Zone 3.8 |
| ULA | Unadjusted Langevin Algorithm | Zone 3.7 |
| SGLD | Stochastic Gradient Langevin Dynamics | Zone 3.8 |
| NCSN | Noise Conditional Score Networks | Zone 3.10 |

**è¨˜å·ã®è¡çªæ³¨æ„**:
- $\epsilon$ ã¯ãƒã‚¤ã‚ºå¤‰æ•° (Zone 0-3) ã¨ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º (Zone 3.7-) ã§ç•°ãªã‚‹æ„å‘³
- æ–‡è„ˆã‹ã‚‰åˆ¤æ–­ã™ã‚‹ã“ã¨

---

**è‘—è€…**: Claude Educator Agent (Sonnet 4.5)
**ç›£ä¿®**: Tech Lead (Opus 4.6)
**ã‚·ãƒªãƒ¼ã‚º**: æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å®Œå…¨è¬›ç¾©ï¼ˆå…¨46å›ï¼‰
---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
