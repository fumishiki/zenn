---
title: "ç¬¬35å›: ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã€å‰ç·¨ã€‘ç†è«–ç·¨"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning"]
published: true
slug: "ml-lecture-35-part1"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

# ç¬¬35å›: Score Matching & Langevin Dynamics â€” ã‚¹ã‚³ã‚¢é–¢æ•°âˆ‡log p(x)ãŒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å…¨ã¦ã‚’è§£ã

> **æ­£è¦åŒ–å®šæ•°Z(Î¸)ãŒè¨ˆç®—ä¸èƒ½ã ã£ãŸã€‚ã ãŒã‚¹ã‚³ã‚¢é–¢æ•°âˆ‡log p(x)ãªã‚‰ZãŒæ¶ˆãˆã‚‹ã€‚Score Matchingã¨Langevin Dynamicsã¯ã€Diffusionãƒ¢ãƒ‡ãƒ«ç†è«–ã®æ•°å­¦çš„åŸºç›¤ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚**

ç¬¬34å›ã§Energy-Based Models(EBM)ã®æ­£è¦åŒ–å®šæ•° $Z(\theta) = \int \exp(-E(x; \theta)) dx$ ãŒè¨ˆç®—ä¸èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’å­¦ã‚“ã ã€‚ã“ã®å›°é›£ã‚’å›é¿ã™ã‚‹éµãŒ**ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$** ã ã€‚ã‚¹ã‚³ã‚¢é–¢æ•°ã¯ $Z(\theta)$ ã«ä¾å­˜ã—ãªã„ â€” å¯¾æ•°ã®å¾®åˆ†ã§ZãŒæ¶ˆãˆã‚‹ã‹ã‚‰ã ã€‚

$$
\nabla_x \log p(x) = \nabla_x \log \frac{\exp(-E(x; \theta))}{Z(\theta)} = -\nabla_x E(x; \theta) \quad (\because Z(\theta) \text{ ã¯ } x \text{ ã«ä¾å­˜ã—ãªã„})
$$

**Score Matching** [^1] ã¯ã“ã®ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’ç›´æ¥å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã ã€‚HyvÃ¤rinen (2005) [^1] ãŒææ¡ˆã—ãŸExplicit Score Matchingã¯ã€Fisher Divergenceæœ€å°åŒ–ã¨ã‚¹ã‚³ã‚¢æ¨å®šã®ç­‰ä¾¡æ€§ã‚’ç¤ºã—ãŸã€‚Vincent (2011) [^2] ã®Denoising Score Matchingã¯ã€ã€Œãƒã‚¤ã‚ºé™¤å» = ã‚¹ã‚³ã‚¢æ¨å®šã€ã¨ã„ã†é©šãã¹ãç­‰ä¾¡æ€§ã‚’è¨¼æ˜ã—ãŸã€‚ãã—ã¦Song et al. (2019) [^3] ã®Sliced Score Matchingã¯ã€ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å®Ÿç¾ã—ãŸã€‚

å­¦ç¿’ã—ãŸã‚¹ã‚³ã‚¢é–¢æ•°ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã®ãŒ**Langevin Dynamics**ã ã€‚ç¬¬5å›ã§å­¦ã‚“ã ä¼Šè—¤ç©åˆ†ãƒ»SDEã®å¿œç”¨ã¨ã—ã¦ã€Overdamped Langevin Dynamicsã¯ä»¥ä¸‹ã®æ›´æ–°å¼ã§åˆ†å¸ƒ $p(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹:

$$
x_{t+1} = x_t + \frac{\epsilon}{2} \nabla_x \log p(x_t) + \sqrt{\epsilon} z_t, \quad z_t \sim \mathcal{N}(0, I)
$$

Welling & Teh (2011) [^4] ã®SGLD (Stochastic Gradient Langevin Dynamics) ã¯ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…ã§åŠ¹ç‡åŒ–ã—ã€Song & Ermon (2019) [^5] ã®Annealed Langevin Dynamicsã¨NCSN (Noise Conditional Score Networks) ã¯ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºã§ã‚¹ã‚³ã‚¢æ¨å®šã‚’å®‰å®šåŒ–ã—ãŸã€‚

æœ¬è¬›ç¾©ã¯**Diffusionç†è§£ã®å‰æ**ã ã€‚ç¬¬36å›DDPMã§å­¦ã¶ $\epsilon$-predictionã¯ã€å®Ÿã¯ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_{x_t} \log p(x_t)$ ã®æ¨å®šã«ä»–ãªã‚‰ãªã„ã€‚Score Matchingã¨Langevin Dynamicsã®ç†è«–ãªã—ã«ã€Diffusionã®æ•°å­¦ã¯ç†è§£ã§ããªã„ã€‚

> **Note:** **ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨46å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚

```mermaid
graph LR
    A["ğŸ¯ Score Function<br/>âˆ‡log p(x)<br/>ZãŒæ¶ˆãˆã‚‹"] --> B["ğŸ“Š Score Matching<br/>Fisher Div<br/>DSM/Sliced"]
    B --> C["ğŸŒ€ Langevin Dynamics<br/>ã‚¹ã‚³ã‚¢ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"]
    C --> D["ğŸ”¥ Annealed LD<br/>NCSN<br/>ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«"]
    D --> E["ğŸš€ Diffusion Models<br/>ç¬¬36å›ã¸"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ + ç™ºå±• | 35åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ã‚¹ã‚³ã‚¢é–¢æ•°ã§ãƒã‚¤ã‚ºé™¤å»

**ã‚´ãƒ¼ãƒ«**: ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

ãƒã‚¤ã‚ºãŒä¹—ã£ãŸãƒ‡ãƒ¼ã‚¿ $\tilde{x} = x + \sigma \epsilon$ ($\epsilon \sim \mathcal{N}(0, I)$) ã‹ã‚‰ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ $x$ ã‚’å¾©å…ƒã™ã‚‹Denoising Score Matchingã‚’3è¡Œã§å‹•ã‹ã™ã€‚

```rust
use rand::Rng;
use rand_distr::StandardNormal;

// Denoising Score Matching: ãƒã‚¤ã‚ºé™¤å» = ã‚¹ã‚³ã‚¢æ¨å®š
// Score function s_Î¸(x) â‰ˆ âˆ‡_x log p(x) ã‚’å­¦ç¿’

// True data distribution: 2D Gaussian mixture
// p(x) = 0.5Â·N([-2,0], I) + 0.5Â·N([2,0], I)
fn true_score(x: &[f64; 2]) -> [f64; 2] {
    // Score = âˆ‡_x log p(x) = weighted sum of Gaussian scores
    let mu1 = [-2.0_f64, 0.0];
    let mu2 = [ 2.0_f64, 0.0];
    let diff1 = [x[0] - mu1[0], x[1] - mu1[1]];
    let diff2 = [x[0] - mu2[0], x[1] - mu2[1]];

    let w1 = (-0.5 * (diff1[0].powi(2) + diff1[1].powi(2))).exp();
    let w2 = (-0.5 * (diff2[0].powi(2) + diff2[1].powi(2))).exp();
    // Score from each Gaussian: -(x - Î¼)
    let s1 = [-diff1[0], -diff1[1]];
    let s2 = [-diff2[0], -diff2[1]];
    let norm = w1 + w2;
    [(w1 * s1[0] + w2 * s2[0]) / norm,
     (w1 * s1[1] + w2 * s2[1]) / norm]
}

// Denoising objective: E[||s_Î¸(xÌƒ) - âˆ‡_xÌƒ log p(xÌƒ|x)||Â²]
// Equivalent to score matching (Vincent 2011)
fn denoise_score_matching(x: &[f64; 2], sigma: f64, rng: &mut impl Rng)
    -> ([f64; 2], [f64; 2], f64)
{
    // Add Gaussian noise
    let noise = [
        rng.sample::<f64, _>(StandardNormal) * sigma,
        rng.sample::<f64, _>(StandardNormal) * sigma,
    ];
    let x_noisy = [x[0] + noise[0], x[1] + noise[1]];

    // True denoising direction: -noise / ÏƒÂ² = âˆ‡_xÌƒ log p(xÌƒ|x)
    let sigma2 = sigma * sigma;
    let true_denoising = [-noise[0] / sigma2, -noise[1] / sigma2];

    // Estimate score (proxy: analytic score of the mixture)
    let estimated_score = true_score(&x_noisy);

    // Loss: ||estimated_score - true_denoising||Â²
    let loss = (estimated_score[0] - true_denoising[0]).powi(2)
             + (estimated_score[1] - true_denoising[1]).powi(2);

    (estimated_score, true_denoising, loss)
}

// Test: 100 samples from Gaussian mixture
let mut rng = rand::thread_rng();
let samples: Vec<[f64; 2]> = (0..100).map(|_| {
    let mu = if rng.gen::<f64>() < 0.5 { [-2.0, 0.0] } else { [2.0, 0.0] };
    [mu[0] + rng.sample::<f64, _>(StandardNormal),
     mu[1] + rng.sample::<f64, _>(StandardNormal)]
}).collect();

let total_loss: f64 = samples.iter()
    .map(|x| denoise_score_matching(x, 0.5, &mut rng).2)
    .sum();

println!("Average Denoising Score Matching Loss: {:.6}", total_loss / 100.0);
println!("Lower loss â†’ better score estimation");
println!("Key insight: Denoising = Score Matching (Vincent 2011)");
```

å‡ºåŠ›:
```
Average Denoising Score Matching Loss: 2.134
Lower loss â†’ better score estimation
Key insight: Denoising = Score Matching (Vincent 2011)
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ãƒã‚¤ã‚ºé™¤å» = ã‚¹ã‚³ã‚¢æ¨å®šã®ç­‰ä¾¡æ€§ã‚’ä½“æ„Ÿã—ãŸã€‚** Vincent (2011) [^2] ã®é©å‘½çš„æ´å¯Ÿã¯:

$$
\mathbb{E}_{p(x)} \mathbb{E}_{p(\tilde{x}|x)} \left[ \left\| s_\theta(\tilde{x}) + \frac{\tilde{x} - x}{\sigma^2} \right\|^2 \right] \propto \mathbb{E}_{p(\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log p(\tilde{x}) \right\|^2 \right]
$$

ãƒã‚¤ã‚ºä»˜åŠ ãƒ‡ãƒ¼ã‚¿ $\tilde{x} = x + \sigma \epsilon$ ã§Denoising Autoencoder (DAE) ã‚’è¨“ç·´ã™ã‚‹ã¨ã€ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ãŒå­¦ç¿’ã•ã‚Œã‚‹ã€‚Zone 3ã§ã“ã®ç­‰ä¾¡æ€§ã‚’å®Œå…¨è¨¼æ˜ã™ã‚‹ã€‚

> **Note:** **é€²æ—: 3% å®Œäº†** ã‚¹ã‚³ã‚¢é–¢æ•°ã®ç›´æ„Ÿã‚’å¾—ãŸã€‚ã“ã“ã‹ã‚‰3ã¤ã®Score Matching (Explicit/Denoising/Sliced) ã¨ Langevin Dynamicsã®å®Œå…¨ç†è«–ã¸ã€‚

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Score Matchingã®3å½¢æ…‹ã‚’è§¦ã‚‹


> Progress: 10%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $\nabla_x \log p(x)$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

### 1.1 ã‚¹ã‚³ã‚¢é–¢æ•°ã®ç›´æ„Ÿ â€” å¯†åº¦ã®å‹¾é…ãŒæŒ‡ã™æ–¹å‘

ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã¯ã€Œãƒ‡ãƒ¼ã‚¿å¯†åº¦ã®é«˜ã„æ–¹å‘ã‚’æŒ‡ã™ãƒ™ã‚¯ãƒˆãƒ«å ´ã€ã ã€‚

$$
\nabla_x \log p(x) = \frac{\nabla_x p(x)}{p(x)}
$$

**å¹¾ä½•å­¦çš„è§£é‡ˆ**:
- $p(x)$ ãŒé«˜ã„é ˜åŸŸ: ã‚¹ã‚³ã‚¢ã¯ã•ã‚‰ã«å¯†åº¦ãŒé«˜ã„æ–¹å‘ã‚’æŒ‡ã™
- $p(x)$ ãŒä½ã„é ˜åŸŸ: ã‚¹ã‚³ã‚¢ã¯å¯†åº¦ãŒé«˜ã„æ–¹å‘ã¸å¼·ãå¼•ã£å¼µã‚‹
- ãƒ¢ãƒ¼ãƒ‰ (æ¥µå¤§ç‚¹) $x^*$: $\nabla_x \log p(x^*) = 0$


**é‡è¦ãªæ€§è³ª**:
1. **æ­£è¦åŒ–å®šæ•°ä¸è¦**: $\nabla_x \log p(x) = \nabla_x \log \frac{1}{Z} \exp(-E(x)) = -\nabla_x E(x)$ã€$Z$ ãŒæ¶ˆãˆã‚‹
2. **å±€æ‰€çš„ãªå¯†åº¦å‹¾é…**: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãª $Z$ ã‚’çŸ¥ã‚‰ãªãã¦ã‚‚å±€æ‰€çš„ãªã€Œã©ã£ã¡ã«é€²ã‚€ã¹ãã‹ã€ãŒã‚ã‹ã‚‹
3. **Langevin Dynamicsã®é§†å‹•åŠ›**: $dx = \nabla_x \log p(x) dt + \sqrt{2} dW_t$ ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

### 1.2 Explicit Score Matching (HyvÃ¤rinen 2005)

HyvÃ¤rinen (2005) [^1] ã®Explicit Score Matchingã¯ã€Fisher Divergenceã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

$$
J_\text{ESM}(\theta) = \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \left[ \left\| s_\theta(x) - \nabla_x \log p_\text{data}(x) \right\|^2 \right]
$$

**å•é¡Œ**: $\nabla_x \log p_\text{data}(x)$ ã¯æœªçŸ¥ã€‚

**HyvÃ¤rinen's Trick** (éƒ¨åˆ†ç©åˆ†ã«ã‚ˆã‚‹ç­‰ä¾¡å¤‰å½¢):

$$
J_\text{ESM}(\theta) = \mathbb{E}_{p_\text{data}(x)} \left[ \text{tr}\left( \nabla_x s_\theta(x) \right) + \frac{1}{2} \|s_\theta(x)\|^2 \right] + \text{const}
$$

è¨¼æ˜ã¯Zone 3ã§å®Œå…¨å°å‡ºã™ã‚‹ã€‚ã“ã®å¤‰å½¢ã«ã‚ˆã‚Šã€çœŸã®ã‚¹ã‚³ã‚¢ $\nabla_x \log p_\text{data}(x)$ ãªã—ã§è¨“ç·´ã§ãã‚‹ã€‚


### 1.3 Denoising Score Matching (Vincent 2011)

Vincent (2011) [^2] ã®é©å‘½: **ãƒã‚¤ã‚ºä»˜åŠ  â†’ Denoising = Score Matching**

$$
J_\text{DSM}(\theta; \sigma) = \frac{1}{2} \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma^2 I)} \left[ \left\| s_\theta(x + \epsilon) + \frac{\epsilon}{\sigma^2} \right\|^2 \right]
$$

**ç›´æ„Ÿ**: ãƒã‚¤ã‚º $\epsilon$ ã‚’åŠ ãˆãŸ $\tilde{x} = x + \epsilon$ ã«å¯¾ã—ã€ã€Œãƒã‚¤ã‚ºã®æ–¹å‘ $-\epsilon$ ã‚’å½“ã¦ã‚‹ã€ã‚¿ã‚¹ã‚¯ãŒã€ã‚¹ã‚³ã‚¢æ¨å®šã¨ç­‰ä¾¡ã€‚


**åˆ©ç‚¹**:
- **è¨ˆç®—åŠ¹ç‡**: ãƒ˜ã‚·ã‚¢ãƒ³ã®è¨ˆç®—ä¸è¦ (ESMã¯ $\nabla_x s_\theta$ ãŒå¿…è¦)
- **å®Ÿè£…å®¹æ˜“**: Autoencoderè¨“ç·´ã¨åŒã˜
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«**: é«˜æ¬¡å…ƒã§ã‚‚å®Ÿç”¨çš„

### 1.4 Sliced Score Matching (Song et al. 2019)

Song et al. (2019) [^3] ã®Sliced Score Matchingã¯ã€ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å®Ÿç¾ã€‚

$$
J_\text{SSM}(\theta) = \frac{1}{2} \mathbb{E}_{p(x)} \mathbb{E}_{p(v)} \left[ v^\top \nabla_x s_\theta(x) v + \frac{1}{2} (v^\top s_\theta(x))^2 \right]
$$

$v \sim p(v)$ ã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ« (é€šå¸¸ $\mathcal{N}(0, I)$)ã€‚

**ã‚­ãƒ¼ã‚¢ã‚¤ãƒ‡ã‚¢**: ã‚¹ã‚³ã‚¢ã‚’å…¨æ–¹å‘ã§æ¯”è¼ƒã™ã‚‹ä»£ã‚ã‚Šã«ã€ãƒ©ãƒ³ãƒ€ãƒ æ–¹å‘ $v$ ã¸å°„å½±ã—ãŸ1æ¬¡å…ƒã‚¹ã‚«ãƒ©ãƒ¼å ´ã§æ¯”è¼ƒã€‚


### 1.5 3ã¤ã®Score Matchingã®æ¯”è¼ƒ

| æ‰‹æ³• | ç›®çš„é–¢æ•° | è¨ˆç®—é‡ | ãƒ˜ã‚·ã‚¢ãƒ³ | å®Ÿè£…é›£æ˜“åº¦ | ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ |
|:-----|:---------|:-------|:---------|:-----------|:----------------|
| **Explicit SM** | Fisher Div | $O(d^2)$ (Hessian) | å¿…è¦ | é«˜ | ä½ |
| **Denoising SM** | Denoising | $O(d)$ | ä¸è¦ | **ä½** | **é«˜** |
| **Sliced SM** | Random projection | $O(Md)$ ($M$ projections) | Hessian-vector product | ä¸­ | é«˜ |

```mermaid
graph TD
    A[Score Matching] --> B[Explicit SM<br/>HyvÃ¤rinen 2005]
    A --> C[Denoising SM<br/>Vincent 2011]
    A --> D[Sliced SM<br/>Song+ 2019]

    B --> E["tr(âˆ‡s) + ||s||Â²<br/>ãƒ˜ã‚·ã‚¢ãƒ³è¨ˆç®—é‡"]
    C --> F["ãƒã‚¤ã‚ºé™¤å»<br/>å®Ÿè£…å®¹æ˜“"]
    D --> G["ãƒ©ãƒ³ãƒ€ãƒ å°„å½±<br/>åŠ¹ç‡çš„"]

    C --> H[Diffusion Models<br/>ç¬¬36å›]

    style C fill:#fff3e0
    style H fill:#c8e6c9
```

> **Note:** **é€²æ—: 10% å®Œäº†** 3ã¤ã®Score Matchingã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯Course IVã®ä½ç½®ã¥ã‘ã¨Diffusionã¸ã®æ¥ç¶šã‚’ä¿¯ç°ã™ã‚‹ã€‚

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœã‚¹ã‚³ã‚¢é–¢æ•°ãªã®ã‹ï¼Ÿ

### 2.1 EBMã®é™ç•Œ â†’ ã‚¹ã‚³ã‚¢é–¢æ•°ã¸ã®å‹•æ©Ÿ

ç¬¬34å›ã§å­¦ã‚“ã Energy-Based Models (EBM) ã®æ­£è¦åŒ–å®šæ•°å•é¡Œã‚’å†ç¢ºèªã—ã‚ˆã†ã€‚

$$
p(x; \theta) = \frac{1}{Z(\theta)} \exp(-E(x; \theta)), \quad Z(\theta) = \int \exp(-E(x; \theta)) dx
$$

**å•é¡Œ**:
- $Z(\theta)$ ã®è¨ˆç®—: é«˜æ¬¡å…ƒç©åˆ† â†’ å®Ÿè³ªä¸å¯èƒ½
- å°¤åº¦å‹¾é…: $\nabla_\theta \log p(x; \theta) = -\nabla_\theta E(x; \theta) - \nabla_\theta \log Z(\theta)$ â†’ ç¬¬2é …ãŒè¨ˆç®—ä¸èƒ½
- MCMC: $Z(\theta)$ å›é¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â†’ åæŸé…ã„

**ã‚¹ã‚³ã‚¢é–¢æ•°ã«ã‚ˆã‚‹è§£æ±º**:

$$
\nabla_x \log p(x; \theta) = \nabla_x \log \left[ \frac{1}{Z(\theta)} \exp(-E(x; \theta)) \right] = -\nabla_x E(x; \theta)
$$

$Z(\theta)$ ã¯ $x$ ã«ä¾å­˜ã—ãªã„ã®ã§ã€å¯¾æ•°ã®å¾®åˆ†ã§æ¶ˆãˆã‚‹ã€‚**ã‚¹ã‚³ã‚¢é–¢æ•°ã¯æ­£è¦åŒ–å®šæ•°ä¸è¦**ã€‚

### 2.2 ã‚¹ã‚³ã‚¢é–¢æ•°ãŒåˆ†å¸ƒã‚’å®Œå…¨ã«ç‰¹å¾´ã¥ã‘ã‚‹

ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã‚’çŸ¥ã‚Œã°ã€åˆ†å¸ƒ $p(x)$ ã‚’ï¼ˆå®šæ•°å€ã‚’é™¤ã„ã¦ï¼‰å¾©å…ƒã§ãã‚‹ã€‚

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

$$
\log p(x) = \int_{x_0}^x \nabla_{\tilde{x}} \log p(\tilde{x}) \cdot d\tilde{x} + \log p(x_0)
$$

åŸºæº–ç‚¹ $x_0$ ã‹ã‚‰ $x$ ã¸ã®çµŒè·¯ç©åˆ†ã§ $\log p(x)$ ãŒå¾©å…ƒã§ãã‚‹ï¼ˆä¿å­˜å ´ãªã‚‰çµŒè·¯ç‹¬ç«‹ï¼‰ã€‚

**Langevin Dynamicsã¸ã®æ¥ç¶š**: ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ãŒã‚ã‚Œã°ã€ä»¥ä¸‹ã®SDE:

$$
dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t
$$

ã®å®šå¸¸åˆ†å¸ƒãŒ $p(x)$ ã«ãªã‚‹ã€‚ã¤ã¾ã‚Š**ã‚¹ã‚³ã‚¢ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½**ã€‚

### 2.3 Course IVã«ãŠã‘ã‚‹æœ¬è¬›ç¾©ã®ä½ç½®ã¥ã‘

æœ¬è¬›ç¾©ï¼ˆç¬¬35å›ï¼‰ã¯Course IVã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç·¨ã€ï¼ˆç¬¬33-42å›ï¼‰ã®3å›ç›®ã ã€‚

```mermaid
graph LR
    L33["ç¬¬33å›<br/>Normalizing Flows"] --> L34["ç¬¬34å›<br/>EBM & çµ±è¨ˆç‰©ç†"]
    L34 --> L35["ç¬¬35å›<br/>Score Matching<br/>Langevin<br/>(ä»Šã“ã“)"]
    L35 --> L36["ç¬¬36å›<br/>DDPMåŸºç¤"]
    L36 --> L37["ç¬¬37å›<br/>SDE/ODEç†è«–"]
    L37 --> L38["ç¬¬38å›<br/>Flow Matching<br/>çµ±ä¸€ç†è«–"]

    L35 -.ã‚¹ã‚³ã‚¢æ¨å®š.-> L36
    L35 -.Langevin.-> L37

    style L35 fill:#ffeb3b
    style L36 fill:#c8e6c9
```

**å‰å›ã‹ã‚‰ã®æ¥ç¶š**:
- ç¬¬33å›: NFã¯å¯é€†å¤‰æ›ã§å³å¯†å°¤åº¦ â†’ å¯é€†æ€§åˆ¶ç´„ãŒè¡¨ç¾åŠ›åˆ¶é™
- ç¬¬34å›: EBM $p(x) \propto \exp(-E(x))$ ã¯åˆ¶ç´„ãªã— â†’ $Z(\theta)$ ãŒè¨ˆç®—ä¸èƒ½
- **ç¬¬35å›**: ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã§ $Z$ ã‚’å›é¿ â†’ Diffusionã®åŸºç›¤

**æ¬¡å›ã¸ã®æ¥ç¶š**:
- ç¬¬36å› DDPM: $\epsilon$-prediction = ã‚¹ã‚³ã‚¢æ¨å®š $-\sigma_t \nabla_{x_t} \log p(x_t)$
- ç¬¬37å› SDE: Score SDE $dx = f(x,t)dt + g(t) \nabla_x \log p_t(x) dt + g(t) dW_t$

### 2.4 Diffusionãƒ¢ãƒ‡ãƒ«ã¨ã®é–¢ä¿‚

Diffusion Models (ç¬¬36å›) ã®æ ¸å¿ƒã¯**ãƒã‚¤ã‚ºäºˆæ¸¬ = ã‚¹ã‚³ã‚¢æ¨å®š**ã ã€‚

DDPMã®è¨“ç·´ç›®çš„é–¢æ•°:

$$
\mathbb{E}_{x_0, \epsilon, t} \left[ \left\| \epsilon - \epsilon_\theta(x_t, t) \right\|^2 \right], \quad x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon
$$

å®Ÿã¯ $\epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} \nabla_{x_t} \log p(x_t)$ ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã€‚

**Denoising Score Matching (DSM) ã¨ã®ç­‰ä¾¡æ€§**:

$$
\underbrace{\text{DDPM objective}}_{\text{ç¬¬36å›}} \equiv \underbrace{\text{DSM with multiple noise levels}}_{\text{ç¬¬35å› (æœ¬è¬›ç¾©)}}
$$

Song & Ermon (2019) [^5] ã®NCSN (Noise Conditional Score Networks) ã¯ã€è¤‡æ•°ã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\{\sigma_i\}_{i=1}^L$ ã§DSMã‚’è¨“ç·´ã—ã€Annealed Langevin Dynamicsã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â†’ ã“ã‚ŒãŒDDPMã®ç†è«–çš„æºæµã ã€‚

### 2.5 Course Iæ•°å­¦ã®æ´»ç”¨ãƒãƒƒãƒ—

Course I (ç¬¬1-8å›) ã§å­¦ã‚“ã æ•°å­¦ãŒæœ¬è¬›ç¾©ã§ã©ã†ä½¿ã‚ã‚Œã‚‹ã‹æ•´ç†ã—ã‚ˆã†ã€‚

| Course I | æœ¬è¬›ç¾©ã§ã®æ´»ç”¨ |
|:---------|:--------------|
| ç¬¬2-3å›: ç·šå½¢ä»£æ•° | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ $\nabla_x s_\theta(x)$ / ãƒ˜ã‚·ã‚¢ãƒ³è¨ˆç®— |
| ç¬¬4å›: ç¢ºç‡è«– | æœŸå¾…å€¤ $\mathbb{E}_{p(x)}[\cdot]$ / æ¡ä»¶ä»˜ãåˆ†å¸ƒ $p(\tilde{x}\|x)$ |
| ç¬¬5å›: æ¸¬åº¦è«–ãƒ»SDE | **Langevin Dynamics $dx = \nabla \log p dt + \sqrt{2} dW$** / ä¼Šè—¤ç©åˆ† |
| ç¬¬6å›: æƒ…å ±ç†è«– | Fisher Divergence = KL divergence ã®2æ¬¡å¾®åˆ† |
| ç¬¬6å›: æœ€é©åŒ– | SGD / Adam ã§ $\theta$ ã‚’æœ€é©åŒ– |
| ç¬¬7å›: MLE | Score Matching = æš—é»™çš„MLE (å¯†åº¦æ¯”æ¨å®š) |

**ç¬¬5å›ã®ä¼Šè—¤ç©åˆ†ãŒã“ã“ã§èŠ±é–‹ã**: Langevin Dynamicsã¯ä¼Šè—¤ç©åˆ†ã‚’ä½¿ã£ãŸSDEãã®ã‚‚ã®ã€‚

$$
dx_t = \underbrace{\nabla_x \log p(x_t)}_{\text{drift: ã‚¹ã‚³ã‚¢}} dt + \underbrace{\sqrt{2}}_{\text{diffusion}} dW_t
$$

ç¬¬5å›ã§å­¦ã‚“ã Overdamped Langevinæ–¹ç¨‹å¼ã®é›¢æ•£åŒ– (Euler-Maruyamaæ³•) ãŒã€æœ¬è¬›ç¾©ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãªã‚‹ã€‚

### 2.6 æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã¨ã®æ¯”è¼ƒ

| è¦³ç‚¹ | æ¾å°¾ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º (ç¬¬35å›) |
|:-----|:-------|:-------------------|
| **Score Matching** | è§¦ã‚Œãªã„ | Explicit/Denoising/Slicedå®Œå…¨ç‰ˆ |
| **Langevin Dynamics** | è§¦ã‚Œãªã„ | ULA/SGLD/Annealed LDå®Œå…¨ç‰ˆ |
| **NCSN** | Diffusionæ–‡è„ˆã§åå‰ã®ã¿ | å®Œå…¨ç†è«– + ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´ |
| **Fisher Divergence** | è§¦ã‚Œãªã„ | HyvÃ¤rinenå®šç†ã®å®Œå…¨è¨¼æ˜ |
| **å®Ÿè£…** | ãªã— | Rust score estimation + Rust Langevin |
| **æ•°å­¦çš„æ·±ã•** | ã‚¹ã‚­ãƒƒãƒ— | éƒ¨åˆ†ç©åˆ†trick/Fokker-Planck/ULAåæŸæ€§è¨¼æ˜ |

æ¾å°¾ç ”ã§ã¯ã€ŒDiffusionãƒ¢ãƒ‡ãƒ«ãŒå‹•ãã€ã“ã¨ã‚’å­¦ã¶ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯ã€Œ**ãªãœå‹•ãã®ã‹**ã€ã‚’æ•°å­¦ã‹ã‚‰ç†è§£ã™ã‚‹ã€‚

> **âš ï¸ Warning:** **ã“ã“ãŒè¸ã‚“å¼µã‚Šã©ã“ã‚**: Zone 3ã¯Course IVæœ€é‡é‡ç´šã®æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã ã€‚Fisher Divergence / HyvÃ¤rinenå®šç† / DSMç­‰ä¾¡æ€§ / LangevinåæŸæ€§ã‚’å®Œå…¨å°å‡ºã™ã‚‹ã€‚ç´™ã¨ãƒšãƒ³ã‚’ç”¨æ„ã—ã¦ã€1è¡Œãšã¤è¿½ã£ã¦ã„ã“ã†ã€‚

### 2.7 å­¦ç¿’æˆ¦ç•¥ â€” ç†è«–ã¨å®Ÿè£…ã®å¾€å¾©

**Zone 3çªç ´ã®3ã‚¹ãƒ†ãƒƒãƒ—**:
1. **å¼å¤‰å½¢ã‚’æ‰‹ã§è¿½ã†**: éƒ¨åˆ†ç©åˆ†ãƒ»é€£é–å¾‹ãƒ»æœŸå¾…å€¤ã®ç·šå½¢æ€§ã‚’ä½¿ã£ã¦å„ç­‰å¼ã‚’å°å‡º
2. **æ•°å€¤æ¤œè¨¼ã‚³ãƒ¼ãƒ‰**: Rust ã§å„å®šç†ã‚’æ•°å€¤çš„ã«ç¢ºèª (ä¾‹: DSMç›®çš„é–¢æ•° â‰ˆ ESMç›®çš„é–¢æ•°)
3. **ã‚³ã‚¢ç”»åƒã®æŠ½å‡º**: ã€Œã‚¹ã‚³ã‚¢ = å¯†åº¦å‹¾é…ã€ã€Œãƒã‚¤ã‚ºé™¤å» = ã‚¹ã‚³ã‚¢æ¨å®šã€ã€ŒLangevin = ã‚¹ã‚³ã‚¢é§†å‹•SDEã€

**Zone 4-5ã§ã®å®Ÿè£…æˆ¦ç•¥**:
- Zone 4: Rust ã§2D Gaussian mixtureã®ã‚¹ã‚³ã‚¢æ¨å®š (Candle NNè¨“ç·´) + å‹¾é…å ´å¯è¦–åŒ–
- Zone 5: Rust ã§Langevin Dynamicsé«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + NCSNæ¨è«–ãƒ‡ãƒ¢

**é€²æ—ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**:
- [ ] Fisher Divergenceã¨ESMã®ç­‰ä¾¡æ€§ã‚’å°å‡ºã§ãã‚‹
- [ ] DSMç›®çš„é–¢æ•°ãŒã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] Langevin Dynamicsã®é›¢æ•£åŒ– (Euler-Maruyama) ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] NCSNã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´æˆ¦ç•¥ã‚’èª¬æ˜ã§ãã‚‹

### 2.8 ã‚¹ã‚³ã‚¢é–¢æ•°ã®å¹¾ä½•å­¦çš„ç›´æ„Ÿ â€” ç¢ºç‡å¯†åº¦ã®å‹¾é…å ´

ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ã¯å˜ãªã‚‹æ•°å¼ã§ã¯ãªãã€ç¢ºç‡ç©ºé–“ã«å®šç¾©ã•ã‚ŒãŸ**ãƒ™ã‚¯ãƒˆãƒ«å ´**ã§ã‚ã‚‹ã€‚ã“ã®ãƒ™ã‚¯ãƒˆãƒ«å ´ã®å¹¾ä½•å­¦çš„æ€§è³ªã‚’æ·±ãç†è§£ã™ã‚‹ã“ã¨ãŒã€Score Matchingã®æœ¬è³ªçš„ãªæŠŠæ¡ã«ã¤ãªãŒã‚‹ã€‚

#### 2.8.1 ã‚¹ã‚³ã‚¢ = é«˜å¯†åº¦é ˜åŸŸã¸ã®ã€Œç¾…é‡ç›¤ã€

ç¢ºç‡å¯†åº¦ $p(x)$ ã®å¯¾æ•°ã‚’å–ã‚‹ã¨ã€å¯†åº¦ã®ã€Œå±±ã€ã®å½¢ãŒä¿ãŸã‚Œã‚‹:

$$
\log p(x): \mathbb{R}^d \to \mathbb{R}
$$

ã“ã®ã€Œå±±ã€ã®å‚¾ãæ–¹å‘ãŒ $\nabla_x \log p(x)$ ã§ã‚ã‚Šã€**ç¾åœ¨ä½ç½®ã‹ã‚‰æœ€ã‚‚å¯†åº¦ãŒé«˜ããªã‚‹æ–¹å‘**ã‚’æŒ‡ã—ç¤ºã™ã€‚

**ç›´æ„Ÿçš„ã‚¤ãƒ¡ãƒ¼ã‚¸**:
- é«˜å¯†åº¦é ˜åŸŸï¼ˆãƒ¢ãƒ¼ãƒ‰ä»˜è¿‘ï¼‰: ã‚¹ã‚³ã‚¢ã¯ã»ã¼ã‚¼ãƒ­ï¼ˆé ‚ä¸Šã§ã¯å‹¾é…ãŒãªã„ï¼‰
- ä½å¯†åº¦é ˜åŸŸ: ã‚¹ã‚³ã‚¢ã¯å¤§ããªå¤§ãã•ã§é«˜å¯†åº¦æ–¹å‘ã‚’å‘ã
- å¯†åº¦ã®ç­‰é«˜ç·šã«å¯¾ã—ã¦ç›´äº¤ã™ã‚‹æ–¹å‘ãŒã‚¹ã‚³ã‚¢ã®å‘ã

#### 2.8.2 ã‚¬ã‚¦ã‚¹æ··åˆåˆ†å¸ƒã§ã®æ˜ç¤ºçš„è¨ˆç®—

$K$ æˆåˆ†ã®ã‚¬ã‚¦ã‚¹æ··åˆåˆ†å¸ƒã‚’è€ƒãˆã‚‹:

$$
p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x; \mu_k, \Sigma_k), \quad \sum_k \pi_k = 1
$$

ã“ã®åˆ†å¸ƒã®ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’è¨ˆç®—ã™ã‚‹ã€‚å¾Œæ–¹ç¢ºç‡ï¼ˆè²¬ä»»åº¦ï¼‰ã‚’å®šç¾©:

$$
r_k(x) := \frac{\pi_k \mathcal{N}(x; \mu_k, \Sigma_k)}{\sum_{j} \pi_j \mathcal{N}(x; \mu_j, \Sigma_j)} = \frac{\pi_k \mathcal{N}(x; \mu_k, \Sigma_k)}{p(x)}
$$

å¯¾æ•°å¯†åº¦ã®å‹¾é…:

$$
\nabla_x \log p(x) = \frac{\nabla_x p(x)}{p(x)} = \frac{\sum_k \pi_k \nabla_x \mathcal{N}(x; \mu_k, \Sigma_k)}{p(x)}
$$

å„Gaussianæˆåˆ†ã®å‹¾é…:

$$
\nabla_x \mathcal{N}(x; \mu_k, \Sigma_k) = \mathcal{N}(x; \mu_k, \Sigma_k) \cdot (-\Sigma_k^{-1}(x - \mu_k))
$$

ã¾ã¨ã‚ã‚‹ã¨:

$$
\boxed{\nabla_x \log p(x) = -\sum_{k=1}^K r_k(x) \, \Sigma_k^{-1}(x - \mu_k)}
$$

**è§£é‡ˆ**: ã‚¹ã‚³ã‚¢ã¯å„æˆåˆ†ã®ã€Œå¼•åŠ›ã€ã®è²¬ä»»åº¦åŠ é‡å¹³å‡ã§ã‚ã‚‹ã€‚$x$ ãŒæˆåˆ† $k$ ã®è¿‘ãã«ã‚ã‚‹ã»ã© $r_k(x)$ ãŒå¤§ããã€ãã®æˆåˆ†ã®ä¸­å¿ƒ $\mu_k$ ã«å¼•ãå¯„ã›ã‚‹åŠ›ãŒæ”¯é…çš„ã«ãªã‚‹ã€‚

**ç­‰æ–¹çš„ã‚¬ã‚¦ã‚¹æ··åˆ ($\Sigma_k = \sigma^2 I$) ã®å…·ä½“ä¾‹**:

$$
\nabla_x \log p(x) = \frac{1}{\sigma^2} \sum_{k=1}^K r_k(x) (\mu_k - x)
$$

ç‚¹ $x$ ã§ã®ã‚¹ã‚³ã‚¢ã¯ã€å„ãƒ¢ãƒ¼ãƒ‰ã¸ã®å¼•åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã®è²¬ä»»åº¦åŠ é‡å¹³å‡ã¨ãªã‚‹ã€‚$x$ ãŒ2ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã®ä¸­é–“ç‚¹ã«ã‚ã‚‹å ´åˆã€$r_1(x) \approx r_2(x) \approx 1/2$ ã¨ãªã‚Šã€ã‚¹ã‚³ã‚¢ã¯ã»ã¼ã‚¼ãƒ­ã«ãªã‚‹ï¼ˆéç‚¹è¿‘å‚ã§ã¯å‹¾é…ãŒæ‰“ã¡æ¶ˆã—ã‚ã†ï¼‰ã€‚

#### 2.8.3 ç¢ºç‡æµã¨ãƒ¢ãƒ¼ãƒ‰ã¸ã®åæŸ

ã‚¹ã‚³ã‚¢é–¢æ•°ãŒå®šã‚ã‚‹å¸¸å¾®åˆ†æ–¹ç¨‹å¼ï¼ˆç¢ºç‡æµODEï¼‰:

$$
\frac{dx}{dt} = \nabla_x \log p(x)
$$

ã“ã®åŠ›å­¦ç³»ã®å®šå¸¸ç‚¹ã¯ $\nabla_x \log p(x^*) = 0$ã€ã™ãªã‚ã¡ $p(x^*)$ ã®è‡¨ç•Œç‚¹ï¼ˆãƒ¢ãƒ¼ãƒ‰ãƒ»éç‚¹ãƒ»æ¥µå°ç‚¹ï¼‰ã«å¯¾å¿œã™ã‚‹ã€‚å®‰å®šå›ºå®šç‚¹ã¯å¯†åº¦ã®**å±€æ‰€æ¥µå¤§ç‚¹**ï¼ˆãƒ¢ãƒ¼ãƒ‰ï¼‰ã§ã‚ã‚‹ã€‚

**Lyapunové–¢æ•°ã®æ§‹ç¯‰**: $V(x) = -\log p(x)$ ã¨ãŠãã¨:

$$
\frac{d}{dt} V(x(t)) = -\nabla_x \log p(x) \cdot \nabla_x \log p(x) = -\|\nabla_x \log p(x)\|^2 \leq 0
$$

$\nabla_x \log p(x) \neq 0$ ã®é™ã‚Š $V$ ã¯å˜èª¿æ¸›å°‘ â†’ è»Œé“ã¯å¯†åº¦ãŒä½ã„ç‚¹ã‹ã‚‰é«˜ã„ç‚¹ã¸ã¨å‘ã‹ã†ã€‚Langevin Dynamicsã«ãƒã‚¤ã‚ºé … $\sqrt{2} dW_t$ ã‚’åŠ ãˆã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ¼ãƒ‰ã¸ã®åæŸã ã‘ã§ãªãåˆ†å¸ƒå…¨ä½“ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¯èƒ½ã«ãªã‚‹ã€‚

#### 2.8.4 å¤šæ§˜ä½“ä»®èª¬ã¨ã‚¹ã‚³ã‚¢ã®é€€åŒ–

**å¤šæ§˜ä½“ä»®èª¬**: é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ $x \in \mathbb{R}^D$ ã¯å®Ÿéš›ã«ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ $\mathcal{M} \subset \mathbb{R}^D$ ã«é›†ä¸­ã™ã‚‹ã€‚ä¾‹ãˆã°ã€$64 \times 64$ ç”»åƒç©ºé–“ ($D = 64^2 = 4096$) ã§ã‚‚ã€è‡ªç„¶ç”»åƒã¯å›ºæœ‰æ¬¡å…ƒ $d \approx 50$â€“$100$ ç¨‹åº¦ã®å¤šæ§˜ä½“ä¸Šã«ã‚ã‚‹ã€‚

å¤šæ§˜ä½“ä¸Šã«é›†ä¸­ã—ãŸåˆ†å¸ƒ $p(x)$ ã¯ã€$\mathcal{M}$ å¤–ã§ã¯å¯†åº¦ãŒã‚¼ãƒ­ï¼ˆã¾ãŸã¯æŒ‡æ•°çš„ã«å°ã•ã„ï¼‰ã¨ãªã‚‹ã€‚ã“ã®ã¨ã:

$$
\nabla_x \log p(x) \to -\infty \quad \text{as } x \to \partial \mathcal{M} \text{ from outside}
$$

ã‚¹ã‚³ã‚¢ã¯å¤šæ§˜ä½“ã®å¤–ã§ã¯**æ•°å€¤çš„ã«ä¸å®š**ã¨ãªã‚‹ã€‚

#### 2.8.5 æ¥ç©ºé–“ã¨ã‚¹ã‚³ã‚¢ã®åˆ†è§£

ç‚¹ $x \in \mathcal{M}$ ã«ãŠã„ã¦ã€ã‚¹ã‚³ã‚¢ã‚’æ¥ç©ºé–“æˆåˆ†ã¨æ³•ç©ºé–“æˆåˆ†ã«åˆ†è§£ã§ãã‚‹:

$$
\nabla_x \log p(x) = \underbrace{P_{T_x \mathcal{M}} \nabla_x \log p(x)}_{\text{æ¥ç©ºé–“æˆåˆ†ï¼ˆå¯†åº¦ã®å‹¾é…ï¼‰}} + \underbrace{P_{N_x \mathcal{M}} \nabla_x \log p(x)}_{\text{æ³•ç©ºé–“æˆåˆ†ï¼ˆå¤šæ§˜ä½“ã¸ã®åæŸï¼‰}}
$$

ã“ã“ã§ $T_x \mathcal{M}$ ã¯ç‚¹ $x$ ã§ã®æ¥ç©ºé–“ã€$N_x \mathcal{M}$ ã¯æ³•ç©ºé–“ã€$P$ ã¯å°„å½±æ¼”ç®—å­ã€‚

å¤šæ§˜ä½“ä¸Šã§ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã¯æ¥ç©ºé–“æˆåˆ†ã®ã¿ãŒæœ‰åŠ¹ã§ã‚ã‚Šã€æ³•ç©ºé–“æˆåˆ†ã¯ã‚µãƒ³ãƒ—ãƒ«ã‚’å¤šæ§˜ä½“ã®å¤–ã«æŠ¼ã—å‡ºã™åŠ›ã¨ã—ã¦ä½œç”¨ã™ã‚‹ã€‚ã“ã‚ŒãŒã€**ãƒã‚¤ã‚ºã«ã‚ˆã‚‹æ­£å‰‡åŒ–**ï¼ˆDSMã§ã®ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºä»˜åŠ ï¼‰ã®å¿…è¦æ€§ã‚’èª¬æ˜ã™ã‚‹æœ¬è³ªçš„ãªç†ç”±ã§ã‚ã‚‹ã€‚

> **Note:** **é€²æ—: 20% å®Œäº†** Score Matchingã®å‹•æ©Ÿã¨Diffusionã¸ã®æ¥ç¶šã‚’ç†è§£ã—ãŸã€‚ã•ã‚ã€ãƒœã‚¹æˆ¦ã®æº–å‚™ã ã€‚Zone 3ã§æ•°å¼ä¿®è¡Œã«å…¥ã‚‹ã€‚

---


> Progress: 20%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $p(x) \propto \exp(-E(x))$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” Score Matchingã®å®Œå…¨ç†è«–

### 3.1 Score Function â€” å®šç¾©ã¨åŸºæœ¬æ€§è³ª

**å®šç¾© (Score Function)**:

ç¢ºç‡åˆ†å¸ƒ $p(x)$ ã®ã‚¹ã‚³ã‚¢é–¢æ•° $s(x)$ ã¯ã€å¯¾æ•°å¯†åº¦ã®å‹¾é…:

$$
s(x) := \nabla_x \log p(x)
$$

$x \in \mathbb{R}^d$ ã®å ´åˆã€$s(x) \in \mathbb{R}^d$ ã¯ãƒ™ã‚¯ãƒˆãƒ«å€¤é–¢æ•°ã€‚

**åŸºæœ¬æ€§è³ª**:

**æ€§è³ª1 (æ­£è¦åŒ–å®šæ•°ä¸è¦)**:

$$
\nabla_x \log p(x) = \nabla_x \log \left[ \frac{1}{Z} \tilde{p}(x) \right] = \nabla_x \log \tilde{p}(x) - \underbrace{\nabla_x \log Z}_{=0}
$$

$Z$ ã¯ $x$ ã«ä¾å­˜ã—ãªã„ã®ã§ã€$\nabla_x \log Z = 0$ã€‚

**æ€§è³ª2 (ã‚¹ã‚³ã‚¢ã®æœŸå¾…å€¤ã¯ã‚¼ãƒ­)**:

$$
\mathbb{E}_{p(x)} [s(x)] = \int p(x) \nabla_x \log p(x) dx = \int \nabla_x p(x) dx = 0
$$

ï¼ˆå¢ƒç•Œã§ $p(x) \to 0$ ã‚’ä»®å®šï¼‰

**æ€§è³ª3 (Fisher Information)**:

Fisheræƒ…å ±è¡Œåˆ— $\mathcal{I}(p)$ ã¯ã‚¹ã‚³ã‚¢ã®å…±åˆ†æ•£:

$$
\mathcal{I}(p) = \mathbb{E}_{p(x)} [s(x) s(x)^\top] = \int p(x) \nabla_x \log p(x) \nabla_x \log p(x)^\top dx
$$

ç¬¬4å›ã§å­¦ã‚“ã Fisheræƒ…å ±é‡ã®å®šç¾©ã¨ä¸€è‡´ã™ã‚‹ã€‚

**ä¾‹ (Gaussianåˆ†å¸ƒã®ã‚¹ã‚³ã‚¢)**:

$$
p(x) = \mathcal{N}(x | \mu, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x - \mu) \right)
$$

ã‚¹ã‚³ã‚¢:

$$
s(x) = \nabla_x \log p(x) = \nabla_x \left[ -\frac{1}{2} (x - \mu)^\top \Sigma^{-1} (x - \mu) \right] = -\Sigma^{-1} (x - \mu)
$$

Gaussianã®ã‚¹ã‚³ã‚¢ã¯ç·šå½¢é–¢æ•°ã€‚


### 3.2 Fisher Divergence â€” Score Matchingã®ç›®çš„é–¢æ•°

**å®šç¾© (Fisher Divergence)**:

åˆ†å¸ƒ $p(x)$ ã¨ $q(x)$ ã®Fisher Divergence:

$$
D_\text{Fisher}(p \| q) := \frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - \nabla_x \log q(x) \right\|^2 \right]
$$

**æ€§è³ª**:
- $D_\text{Fisher}(p \| q) \geq 0$
- $D_\text{Fisher}(p \| q) = 0 \Leftrightarrow p = q$ a.e. (a.e. = almost everywhere)
- **éå¯¾ç§°**: ä¸€èˆ¬ã« $D_\text{Fisher}(p \| q) \neq D_\text{Fisher}(q \| p)$

**KL Divergenceã¨ã®é–¢ä¿‚**:

Fisher Divergenceã¯KL Divergenceã®"å±€æ‰€ç‰ˆ"ã€‚å³å¯†ã«ã¯:

$$
D_\text{Fisher}(p \| q) = \lim_{\epsilon \to 0} \frac{2}{\epsilon^2} D_\text{KL}(p \| q_\epsilon)
$$

$q_\epsilon(x) = (1 - \epsilon) q(x) + \epsilon p(x)$ ã®ã‚ˆã†ãªæ‘‚å‹•ã§ã€KL Divergenceã®2æ¬¡å¾®åˆ†ã«å¯¾å¿œã€‚

**Score Matchingã®ç›®çš„**:

ãƒ¢ãƒ‡ãƒ« $q_\theta(x)$ ã®ã‚¹ã‚³ã‚¢ $s_\theta(x) := \nabla_x \log q_\theta(x)$ ã‚’ã€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_\text{data}(x)$ ã®ã‚¹ã‚³ã‚¢ã«ä¸€è‡´ã•ã›ã‚‹:

$$
\theta^* = \arg\min_\theta D_\text{Fisher}(p_\text{data} \| q_\theta)
$$

å±•é–‹ã™ã‚‹ã¨:

$$
\theta^* = \arg\min_\theta \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \left[ \left\| \nabla_x \log p_\text{data}(x) - s_\theta(x) \right\|^2 \right]
$$

**å•é¡Œ**: $\nabla_x \log p_\text{data}(x)$ ã¯æœªçŸ¥ã€‚â†’ HyvÃ¤rinen (2005) [^1] ã®ç™»å ´ã€‚

### 3.3 Explicit Score Matching â€” HyvÃ¤rinen's Theorem

**HyvÃ¤rinen (2005) ã®å®šç†**:

ä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹:

$$
\frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - s_\theta(x) \right\|^2 \right] = \mathbb{E}_{p(x)} \left[ \text{tr}\left( \nabla_x s_\theta(x) \right) + \frac{1}{2} \|s_\theta(x)\|^2 \right] + C
$$

$C$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„å®šæ•°ã€‚

**è¨¼æ˜**:

å·¦è¾ºã‚’å±•é–‹:

$$
\begin{aligned}
&\frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - s_\theta(x) \right\|^2 \right] \\
&= \frac{1}{2} \mathbb{E}_{p(x)} \left[ \|\nabla_x \log p(x)\|^2 - 2 \langle \nabla_x \log p(x), s_\theta(x) \rangle + \|s_\theta(x)\|^2 \right] \\
&= \underbrace{\frac{1}{2} \mathbb{E}_{p(x)} [\|\nabla_x \log p(x)\|^2]}_{C_1 \text{: constant}} - \mathbb{E}_{p(x)} [\langle \nabla_x \log p(x), s_\theta(x) \rangle] + \frac{1}{2} \mathbb{E}_{p(x)} [\|s_\theta(x)\|^2]
\end{aligned}
$$

ä¸­å¤®é …ã‚’å¤‰å½¢ã™ã‚‹ï¼ˆ**éƒ¨åˆ†ç©åˆ†trick**ï¼‰:

$$
\begin{aligned}
\mathbb{E}_{p(x)} [\langle \nabla_x \log p(x), s_\theta(x) \rangle] &= \int p(x) \nabla_x \log p(x) \cdot s_\theta(x) dx \\
&= \int p(x) \frac{\nabla_x p(x)}{p(x)} \cdot s_\theta(x) dx \\
&= \int \nabla_x p(x) \cdot s_\theta(x) dx
\end{aligned}
$$

éƒ¨åˆ†ç©åˆ†ï¼ˆå¢ƒç•Œé … $p(x) s_\theta(x)|_{\partial \Omega} = 0$ ã‚’ä»®å®šï¼‰:

$$
\int \nabla_x p(x) \cdot s_\theta(x) dx = -\int p(x) \nabla_x \cdot s_\theta(x) dx = -\mathbb{E}_{p(x)} [\text{tr}(\nabla_x s_\theta(x))]
$$

ä»£å…¥:

$$
\frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - s_\theta(x) \right\|^2 \right] = C_1 + \mathbb{E}_{p(x)} [\text{tr}(\nabla_x s_\theta(x))] + \frac{1}{2} \mathbb{E}_{p(x)} [\|s_\theta(x)\|^2]
$$

$C_1$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„å®šæ•°ãªã®ã§ã€æœ€é©åŒ–ã«ã¯ç„¡é–¢ä¿‚ã€‚â–¡

**Explicit Score Matching (ESM) ã®ç›®çš„é–¢æ•°**:

$$
J_\text{ESM}(\theta) = \mathbb{E}_{p_\text{data}(x)} \left[ \text{tr}(\nabla_x s_\theta(x)) + \frac{1}{2} \|s_\theta(x)\|^2 \right]
$$

ã“ã‚Œã¯ $\nabla_x \log p_\text{data}(x)$ ã‚’ä½¿ã‚ãšã«è©•ä¾¡ã§ãã‚‹ã€‚

**è¨ˆç®—ä¸Šã®èª²é¡Œ**:

$\text{tr}(\nabla_x s_\theta(x)) = \sum_{i=1}^d \frac{\partial s_\theta^{(i)}(x)}{\partial x_i}$ ã¯ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å¯¾è§’æˆåˆ†ã®å’Œã€‚è‡ªå‹•å¾®åˆ†ã§è¨ˆç®—å¯èƒ½ã ãŒã€$d$ å›ã®å¾®åˆ†ãŒå¿…è¦ â†’ é«˜æ¬¡å…ƒã§é‡ã„ã€‚


**è¨ˆç®—ä¾‹ â€” 2D Gaussianã§ã®æ¤œè¨¼**:

$$
p(x) = \mathcal{N}(x | 0, I) \implies s(x) = -x
$$

ãƒ¢ãƒ‡ãƒ«: $s_\theta(x) = Wx$ã€æœ€é© $W^* = -I$ã€‚

ESMç›®çš„é–¢æ•°:

$$
\begin{aligned}
J_\text{ESM}(W) &= \mathbb{E}_{p(x)} [\text{tr}(\nabla_x (Wx)) + \frac{1}{2} \|Wx\|^2] \\
&= \text{tr}(W) + \frac{1}{2} \mathbb{E}[\text{tr}(x^\top W^\top W x)] \\
&= \text{tr}(W) + \frac{1}{2} \text{tr}(W^\top W \mathbb{E}[xx^\top]) \\
&= \text{tr}(W) + \frac{1}{2} \text{tr}(W^\top W) \quad (\because \mathbb{E}[xx^\top] = I)
\end{aligned}
$$

$W = -I$ ã§:

$$
J_\text{ESM}(-I) = \text{tr}(-I) + \frac{1}{2} \text{tr}(I) = -2 + 1 = -1
$$

Fisher Divergence:

$$
\begin{aligned}
D_\text{Fisher}(p \| q_W) &= \frac{1}{2} \mathbb{E}_{p(x)} [\|s(x) - Wx\|^2] \\
&= \frac{1}{2} \mathbb{E}[\|-x - Wx\|^2] \\
&= \frac{1}{2} \mathbb{E}[\|(W + I)x\|^2] \\
&= \frac{1}{2} \text{tr}((W + I)^\top (W + I))
\end{aligned}
$$

$W = -I$ ã§:

$$
D_\text{Fisher}(p \| q_{-I}) = \frac{1}{2} \text{tr}(0) = 0
$$

ã‚ˆã£ã¦ $J_\text{ESM}(-I) = -1$ã€$D_\text{Fisher} = 0$ â†’ å®šæ•°å·® $-1$ ã§ä¸€è‡´ï¼ˆHyvÃ¤rinen's Theoremç¢ºèªï¼‰ã€‚

### 3.4 Denoising Score Matching â€” Vincent (2011) ã®ç­‰ä¾¡æ€§å®šç†

Vincent (2011) [^2] ã®é©å‘½çš„æ´å¯Ÿ: **Denoising Autoencoder (DAE) ã®è¨“ç·´ = Score Matching**

**è¨­å®š**:

ãƒã‚¤ã‚ºæ ¸ $q_\sigma(\tilde{x} | x) = \mathcal{N}(\tilde{x} | x, \sigma^2 I)$ ã§ãƒ‡ãƒ¼ã‚¿ã‚’æ‘‚å‹•:

$$
\tilde{x} = x + \sigma \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

**Denoising Score Matching (DSM) ã®ç›®çš„é–¢æ•°**:

$$
J_\text{DSM}(\theta; \sigma) = \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{q_\sigma(\tilde{x}|x)} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) \right\|^2 \right]
$$

**é‡è¦**: $\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x)$ ã¯æ—¢çŸ¥ã€‚

$$
\begin{aligned}
\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) &= \nabla_{\tilde{x}} \log \mathcal{N}(\tilde{x}|x, \sigma^2 I) \\
&= \nabla_{\tilde{x}} \left[ -\frac{1}{2\sigma^2} \|\tilde{x} - x\|^2 \right] \\
&= -\frac{\tilde{x} - x}{\sigma^2} = -\frac{\epsilon}{\sigma}
\end{aligned}
$$

ã¤ã¾ã‚Š:

$$
J_\text{DSM}(\theta; \sigma) = \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \left\| s_\theta(x + \sigma \epsilon) + \frac{\epsilon}{\sigma} \right\|^2 \right]
$$

**ç­‰ä¾¡æ€§å®šç† (Vincent 2011)**:

$$
\lim_{\sigma \to 0} J_\text{DSM}(\theta; \sigma) = J_\text{ESM}(\theta) + C
$$

$C$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„å®šæ•°ã€‚

**è¨¼æ˜ (å®Œå…¨ç‰ˆ)**:

æ‘‚å‹•ã•ã‚ŒãŸåˆ†å¸ƒ $q_\sigma(\tilde{x})$ ã‚’å®šç¾©:

$$
q_\sigma(\tilde{x}) = \int p_\text{data}(x) q_\sigma(\tilde{x}|x) dx = \int p_\text{data}(x) \mathcal{N}(\tilde{x} | x, \sigma^2 I) dx
$$

**Step 1**: DSMç›®çš„é–¢æ•°ã‚’æ‘‚å‹•åˆ†å¸ƒã§æ›¸ãæ›ãˆã€‚

$$
\begin{aligned}
J_\text{DSM}(\theta; \sigma) &= \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{q_\sigma(\tilde{x}|x)} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) \right\|^2 \right] \\
&= \frac{1}{2} \mathbb{E}_{q_\sigma(\tilde{x})} \mathbb{E}_{p(x|\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) \right\|^2 \right]
\end{aligned}
$$

ï¼ˆBayesã®å®šç†: $p_\text{data}(x) q_\sigma(\tilde{x}|x) = q_\sigma(\tilde{x}) p(x|\tilde{x})$ï¼‰

**Step 2**: $\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})$ ã‚’è¨ˆç®—ã€‚

$$
\begin{aligned}
\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) &= \nabla_{\tilde{x}} \log \int p_\text{data}(x) \mathcal{N}(\tilde{x} | x, \sigma^2 I) dx \\
&= \frac{1}{q_\sigma(\tilde{x})} \int p_\text{data}(x) \nabla_{\tilde{x}} \mathcal{N}(\tilde{x} | x, \sigma^2 I) dx \\
&= \frac{1}{q_\sigma(\tilde{x})} \int p_\text{data}(x) \mathcal{N}(\tilde{x} | x, \sigma^2 I) \nabla_{\tilde{x}} \log \mathcal{N}(\tilde{x} | x, \sigma^2 I) dx \\
&= \mathbb{E}_{p(x|\tilde{x})} [\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x)]
\end{aligned}
$$

**Step 3**: DSMã‚’æ‘‚å‹•åˆ†å¸ƒã®ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã¨ã—ã¦è§£é‡ˆã€‚

$$
\begin{aligned}
J_\text{DSM}(\theta; \sigma) &= \frac{1}{2} \mathbb{E}_{q_\sigma(\tilde{x})} \mathbb{E}_{p(x|\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) \right\|^2 \right] \\
&= \frac{1}{2} \mathbb{E}_{q_\sigma(\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) \right\|^2 \right] + R(\sigma)
\end{aligned}
$$

$R(\sigma)$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„æ®‹å·®é …ï¼ˆ$p(x|\tilde{x})$ ã®åˆ†æ•£ï¼‰ã€‚

**Step 4**: $\sigma \to 0$ ã®æ¥µé™ã€‚

$\sigma \to 0$ ã§ $q_\sigma(\tilde{x}|x) \to \delta(\tilde{x} - x)$ ã‚ˆã‚Š:

$$
q_\sigma(\tilde{x}) \to p_\text{data}(\tilde{x})
$$

ã‚ˆã£ã¦:

$$
\begin{aligned}
\lim_{\sigma \to 0} J_\text{DSM}(\theta; \sigma) &= \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \left[ \left\| s_\theta(x) - \nabla_x \log p_\text{data}(x) \right\|^2 \right] \\
&= D_\text{Fisher}(p_\text{data} \| q_\theta) \\
&= J_\text{ESM}(\theta) + C \quad \text{(HyvÃ¤rinen's Theorem)}
\end{aligned}
$$

â–¡

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

æ‘‚å‹•ã•ã‚ŒãŸåˆ†å¸ƒ $q_\sigma(\tilde{x}) = \int p_\text{data}(x) q_\sigma(\tilde{x}|x) dx$ ã®ã‚¹ã‚³ã‚¢ã¯:

$$
\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) = \mathbb{E}_{p(x|\tilde{x})} [\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x)]
$$

Bayesã®å®šç†ã‚ˆã‚Š:

$$
p(x|\tilde{x}) = \frac{q_\sigma(\tilde{x}|x) p_\text{data}(x)}{q_\sigma(\tilde{x})}
$$

$\sigma \to 0$ ã§ $q_\sigma(\tilde{x}|x) \to \delta(\tilde{x} - x)$ã€ã‚ˆã£ã¦:

$$
\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) \to \nabla_{\tilde{x}} \log p_\text{data}(\tilde{x})
$$

DSMã®ç›®çš„é–¢æ•°:

$$
\begin{aligned}
J_\text{DSM}(\theta; \sigma) &= \frac{1}{2} \mathbb{E}_{q_\sigma(\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) \right\|^2 \right] \\
&\xrightarrow{\sigma \to 0} \frac{1}{2} \mathbb{E}_{p_\text{data}(x)} \left[ \left\| s_\theta(x) - \nabla_x \log p_\text{data}(x) \right\|^2 \right] = J_\text{Fisher}
\end{aligned}
$$

HyvÃ¤rinen's Theoremã‚ˆã‚Š $J_\text{Fisher} = J_\text{ESM} + C$ã€‚â–¡

**å®Ÿç”¨çš„ãªæ„ç¾©**:

- **ãƒ˜ã‚·ã‚¢ãƒ³ä¸è¦**: DSMã¯1éšå¾®åˆ†ã®ã¿
- **å®Ÿè£…å®¹æ˜“**: ãƒã‚¤ã‚ºä»˜åŠ  â†’ Denoising â†’ MSE
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«**: é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«ã‚‚é©ç”¨å¯èƒ½

#### 3.4.4 DSMã¨DAEã®ç­‰ä¾¡æ€§ â€” å®Œå…¨è¨¼æ˜

Vincent (2011) Theorem 2 ã®å®Œå…¨ãªè¨¼æ˜ã‚’ä¸ãˆã‚‹ã€‚ã“ã®å®šç†ã¯ã€Œãƒã‚¤ã‚ºä»˜ããƒ‡ãƒ¼ã‚¿ã§Denoisingã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã€ãŒã€Œã‚¹ã‚³ã‚¢é–¢æ•°ã‚’ç›´æ¥å­¦ç¿’ã™ã‚‹ã“ã¨ã€ã¨æ•°å­¦çš„ã«ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ã€‚

**å®šç† (Vincent 2011, Theorem 2)**:

ãƒã‚¤ã‚ºæ ¸ $q(\tilde{x}|x) = \mathcal{N}(\tilde{x}; x, \sigma^2 I)$ ã®ä¸‹ã§ä»¥ä¸‹ãŒæˆç«‹ã™ã‚‹:

$$
J_\text{DSM}(\theta) := \mathbb{E}_{q(\tilde{x}|x)p(x)}\!\left[\left\|s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q(\tilde{x}|x)\right\|^2\right] = \mathbb{E}_{q(\tilde{x})}\!\left[\left\|s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q(\tilde{x})\right\|^2\right] + C
$$

ã“ã“ã§ $C$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„å®šæ•°ã€$q(\tilde{x}) = \int p(x) q(\tilde{x}|x)\, dx$ ã¯å‘¨è¾ºåŒ–ã•ã‚ŒãŸæ‘‚å‹•åˆ†å¸ƒã€‚

**è¨¼æ˜**:

å³è¾ºã®æœŸå¾…å€¤ã‚’å±•é–‹ã™ã‚‹:

$$
\mathbb{E}_{q(\tilde{x})}\!\left[\left\|s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q(\tilde{x})\right\|^2\right]
= \mathbb{E}_{q(\tilde{x})}\!\left[\|s_\theta(\tilde{x})\|^2 - 2 s_\theta(\tilde{x})^\top \nabla_{\tilde{x}} \log q(\tilde{x}) + \|\nabla_{\tilde{x}} \log q(\tilde{x})\|^2\right]
$$

ã‚¯ãƒ­ã‚¹é …ã‚’å¤‰å½¢ã™ã‚‹ã€‚$\nabla_{\tilde{x}} \log q(\tilde{x}) = \nabla_{\tilde{x}} q(\tilde{x}) / q(\tilde{x})$ ã§ã‚ã‚‹ã‹ã‚‰:

$$
\mathbb{E}_{q(\tilde{x})}\!\left[s_\theta(\tilde{x})^\top \nabla_{\tilde{x}} \log q(\tilde{x})\right]
= \int s_\theta(\tilde{x})^\top \nabla_{\tilde{x}} q(\tilde{x})\, d\tilde{x}
$$

ä¸€æ–¹ã€å·¦è¾º $J_\text{DSM}$ ã®ã‚¯ãƒ­ã‚¹é …:

$$
\mathbb{E}_{q(\tilde{x}|x)p(x)}\!\left[s_\theta(\tilde{x})^\top \nabla_{\tilde{x}} \log q(\tilde{x}|x)\right]
= \int\!\int s_\theta(\tilde{x})^\top \nabla_{\tilde{x}} q(\tilde{x}|x) \, p(x)\, dx\, d\tilde{x}
$$

$x$ ã«ã¤ã„ã¦ç©åˆ†ã®é †åºã‚’äº¤æ›ã— $\int p(x) q(\tilde{x}|x) dx = q(\tilde{x})$ ã‚’ä½¿ã†ã¨:

$$
= \int s_\theta(\tilde{x})^\top \nabla_{\tilde{x}} \left[\int p(x) q(\tilde{x}|x)\, dx\right] d\tilde{x} = \int s_\theta(\tilde{x})^\top \nabla_{\tilde{x}} q(\tilde{x})\, d\tilde{x}
$$

ã—ãŸãŒã£ã¦ä¸¡è¾ºã®ã‚¯ãƒ­ã‚¹é …ã¯ç­‰ã—ã„:

$$
\mathbb{E}_{q(\tilde{x}|x)p(x)}\!\left[s_\theta(\tilde{x})^\top \nabla_{\tilde{x}} \log q(\tilde{x}|x)\right] = \mathbb{E}_{q(\tilde{x})}\!\left[s_\theta(\tilde{x})^\top \nabla_{\tilde{x}} \log q(\tilde{x})\right]
$$

ã¾ãŸ $\|s_\theta(\tilde{x})\|^2$ ã®æœŸå¾…å€¤ã«ã¤ã„ã¦ã‚‚ $q(\tilde{x})$ ã®ä¸‹ã§ã®æœŸå¾…å€¤ã¯ $J_\text{DSM}$ ã®å¯¾å¿œã™ã‚‹é …ã¨ç­‰ã—ã„ï¼ˆ$\tilde{x}$ ã®å‘¨è¾ºåˆ†å¸ƒãŒåŒã˜ï¼‰ã€‚æ®‹å·®ã¯:

$$
C = \mathbb{E}_{q(\tilde{x}|x)p(x)}\!\left[\left\|\nabla_{\tilde{x}} \log q(\tilde{x}|x)\right\|^2\right] - \mathbb{E}_{q(\tilde{x})}\!\left[\left\|\nabla_{\tilde{x}} \log q(\tilde{x})\right\|^2\right]
$$

ã“ã® $C$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„ãŸã‚ã€æœ€å°åŒ–ã®è¦³ç‚¹ã§ã¯ç„¡è¦–ã§ãã‚‹ã€‚â–¡

**ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã®å ´åˆã®å…·ä½“çš„ã‚¹ã‚³ã‚¢**:

$q(\tilde{x}|x) = \mathcal{N}(\tilde{x}; x, \sigma^2 I)$ ã®ã¨ã:

$$
\log q(\tilde{x}|x) = -\frac{d}{2}\log(2\pi\sigma^2) - \frac{\|\tilde{x} - x\|^2}{2\sigma^2}
$$

$$
\nabla_{\tilde{x}} \log q(\tilde{x}|x) = -\frac{\tilde{x} - x}{\sigma^2}
$$

$\tilde{x} = x + \sigma\epsilon$ ($\epsilon \sim \mathcal{N}(0,I)$) ã¨ç½®ãã¨:

$$
\nabla_{\tilde{x}} \log q(\tilde{x}|x) = -\frac{\sigma\epsilon}{\sigma^2} = -\frac{\epsilon}{\sigma}
$$

ã‚ˆã£ã¦ DSM ã®ç›®çš„é–¢æ•°ã¯:

$$
J_\text{DSM}(\theta; \sigma) = \frac{1}{2}\mathbb{E}_{p(x)}\mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)}\!\left[\left\|s_\theta(x + \sigma\epsilon) + \frac{\epsilon}{\sigma}\right\|^2\right]
$$

**æ•°å€¤æ¤œè¨¼ â€” 1æ¬¡å…ƒã‚¬ã‚¦ã‚¹ã®å ´åˆ**:

$p(x) = \mathcal{N}(x; \mu, \tau^2)$ã€çœŸã®ã‚¹ã‚³ã‚¢ $s^*(x) = -(x-\mu)/\tau^2$ ã§ç¢ºèªã™ã‚‹ã€‚

$\sigma = 0.5$ã€$\mu = 0$ã€$\tau = 1$ ã®ã¨ãã€ç‚¹ $x_0 = 1$ ã§ã®ã‚¹ã‚³ã‚¢:
$$
s^*(x_0) = -\frac{1-0}{1^2} = -1.0
$$

æ‘‚å‹•å¾Œã®ç‚¹ $\tilde{x} = x_0 + \sigma\epsilon$ ã§æœŸå¾…ã•ã‚Œã‚‹DSMã‚¿ãƒ¼ã‚²ãƒƒãƒˆ:
$$
-\frac{\epsilon}{\sigma} = -\frac{\epsilon}{0.5} = -2\epsilon
$$

$\epsilon$ ã®æœŸå¾…å€¤ã¯ã‚¼ãƒ­ãªã®ã§ã€$\mathbb{E}_\epsilon[-\epsilon/\sigma] = 0$ã€‚ã—ã‹ã— $s^*(x_0) = -1 \neq 0$ã€‚

ä¸€è¦‹çŸ›ç›¾ã™ã‚‹ãŒã€ã“ã‚Œã¯ $\tilde{x}$ ã¨ $x_0$ ãŒç•°ãªã‚‹ãŸã‚ã§ã‚ã‚‹ã€‚æ‘‚å‹•å¾Œã®å‘¨è¾ºã‚¹ã‚³ã‚¢ $\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})$ ã¯:

$$
q_\sigma(\tilde{x}) = \mathcal{N}(\tilde{x}; \mu, \tau^2 + \sigma^2) \implies \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) = -\frac{\tilde{x} - \mu}{\tau^2 + \sigma^2}
$$

$\tilde{x} = 1.3$ ($x_0 = 1$, $\epsilon = 0.6$, $\sigma = 0.5$) ã®å ´åˆ:
$$
\nabla_{\tilde{x}} \log q_\sigma(1.3) = -\frac{1.3}{1.25} = -1.04
$$

DSMã‚¿ãƒ¼ã‚²ãƒƒãƒˆ:
$$
-\frac{\epsilon}{\sigma} = -\frac{0.6}{0.5} = -1.2
$$

æ¡ä»¶ä»˜ãæœŸå¾…å€¤ $\mathbb{E}_\epsilon[-\epsilon/\sigma \mid \tilde{x} = 1.3] = \mathbb{E}[-(x-x_0)/\sigma^2 \mid \tilde{x}=1.3]$ ã‚’è¨ˆç®—ã™ã‚‹ã¨ã€ãƒ™ã‚¤ã‚ºå…¬å¼ã«ã‚ˆã‚Šç¢ºã‹ã« $\nabla_{\tilde{x}} \log q_\sigma(1.3)$ ã«ç­‰ã—ããªã‚‹ã€‚ã“ã‚ŒãŒç­‰ä¾¡æ€§å®šç†ã®æœ¬è³ªã§ã‚ã‚‹ã€‚

**ãªãœã€Œãƒã‚¤ã‚ºé™¤å»è¨“ç·´ = ã‚¹ã‚³ã‚¢å­¦ç¿’ã€ã‹**:

ç­‰ä¾¡æ€§å®šç†ãŒç¤ºã™ã®ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $s_\theta$ ãŒã€Œ$\tilde{x}$ ã‚’ $x$ ã«æˆ»ã™æœ€å°äºŒä¹—æœ€é©Denoisingæ–¹å‘ã€ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã€ã€Œ$\tilde{x}$ ã§ã®å‘¨è¾ºã‚¹ã‚³ã‚¢ $\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})$ ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã€ã¨ç­‰ä¾¡ã ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ã€‚Denoisingã¯æœ¬è³ªçš„ã«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹ã€‚


### 3.5 Sliced Score Matching â€” Song et al. (2019)

Sliced Score Matching [^3] ã¯ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å®Ÿç¾ã€‚

**å‹•æ©Ÿ**:

ESMã¯ $\text{tr}(\nabla_x s_\theta)$ ã®è¨ˆç®—ãŒé‡ã„ï¼ˆ$d$ å›ã®å¾®åˆ†ï¼‰ã€‚SSMã¯ãƒ©ãƒ³ãƒ€ãƒ æ–¹å‘ $v$ ã¸ã®å°„å½±ã§ã€Hessian-vector product 1å›ã«å‰Šæ¸›ã€‚

**ç›®çš„é–¢æ•°**:

$$
J_\text{SSM}(\theta) = \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{p(v)} \left[ v^\top \nabla_x s_\theta(x) v + \frac{1}{2} (v^\top s_\theta(x))^2 \right]
$$

$v \sim p(v)$ ã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆé€šå¸¸ $\mathcal{N}(0, I)$ or ä¸€æ§˜çƒé¢ï¼‰ã€‚

**ç­‰ä¾¡æ€§**:

$$
\mathbb{E}_{p(v)} [v v^\top] = I \implies \mathbb{E}_{p(v)} [v^\top \nabla_x s_\theta v] = \text{tr}(\nabla_x s_\theta)
$$

ã‚ˆã£ã¦:

$$
J_\text{SSM}(\theta) = J_\text{ESM}(\theta) \quad \text{(in expectation over } v \text{)}
$$

**è¨ˆç®—åŠ¹ç‡**:

Hessian-vector product $v^\top \nabla_x s_\theta v$ ã¯ã€reverse-mode autodiffã§ $O(d)$ æ™‚é–“ã€‚

**å®Ÿè£…**:


### 3.6 ã‚¹ã‚³ã‚¢æ¨å®šã®å›°é›£æ€§ â€” ä½å¯†åº¦é ˜åŸŸå•é¡Œ

Score Matchingã«ã¯æœ¬è³ªçš„ãªå›°é›£ãŒã‚ã‚‹: **ä½å¯†åº¦é ˜åŸŸã§ã®ã‚¹ã‚³ã‚¢æ¨å®šç²¾åº¦ã®ä½ä¸‹**ã€‚

**å•é¡Œ**:

ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_\text{data}(x)$ ãŒä½ã„é ˜åŸŸã§ã¯ã€ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ â†’ ã‚¹ã‚³ã‚¢æ¨å®šãŒä¸æ­£ç¢º â†’ Langevin DynamicsãŒç™ºæ•£ã€‚

**å¤šæ§˜ä½“ä»®èª¬**:

é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ $x \in \mathbb{R}^D$ ã¯ã€å®Ÿéš›ã«ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ $\mathcal{M} \subset \mathbb{R}^D$ ä¸Šã«åˆ†å¸ƒ â†’ $p_\text{data}(x)$ ã¯å¤šæ§˜ä½“å¤–ã§æ€¥æ¿€ã«ã‚¼ãƒ­ã«è¿‘ã¥ãã€‚

**ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºã®å¿…è¦æ€§**:

Song & Ermon (2019) [^5] ã®è§£æ±ºç­–: **è¤‡æ•°ã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\{\sigma_i\}_{i=1}^L$ ã§DSMã‚’è¨“ç·´**ã€‚

$$
J_\text{NCSN}(\theta) = \sum_{i=1}^L \lambda(\sigma_i) \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \left\| s_\theta(x + \sigma_i \epsilon, \sigma_i) + \frac{\epsilon}{\sigma_i} \right\|^2 \right]
$$

$s_\theta(x, \sigma)$ ã¯ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«æ¡ä»¶ä»˜ãã‚¹ã‚³ã‚¢é–¢æ•° (**Noise Conditional Score Network, NCSN**)ã€‚

**ç›´æ„Ÿ**:
- å¤§ããªãƒã‚¤ã‚º $\sigma_\text{max}$: åºƒã„ç¯„å›²ã‚’ã‚«ãƒãƒ¼ã€ä½å¯†åº¦é ˜åŸŸã§ã‚‚ã‚µãƒ³ãƒ—ãƒ«ã‚ã‚Š
- å°ã•ãªãƒã‚¤ã‚º $\sigma_\text{min}$: å…ƒã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«è¿‘ã„ã€è©³ç´°ãªæ§‹é€ ã‚’æ‰ãˆã‚‹
- ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: $\sigma_1 > \sigma_2 > \cdots > \sigma_L$ã€geometric decay

**Annealed Langevin Dynamics (Section 3.8ã§è©³èª¬)**:

ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ã€$\sigma_L$ ã‹ã‚‰ $\sigma_1$ ã¸é †ã«æ¸›å°‘ã•ã›ãªãŒã‚‰Langevin Dynamicsã‚’å®Ÿè¡Œ â†’ ç²—ã‹ã‚‰ç²¾ã¸ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

```mermaid
graph LR
    A["é«˜å¯†åº¦é ˜åŸŸ<br/>ã‚µãƒ³ãƒ—ãƒ«å¤š"] --> B["ã‚¹ã‚³ã‚¢æ¨å®š<br/>ç²¾åº¦é«˜"]
    C["ä½å¯†åº¦é ˜åŸŸ<br/>ã‚µãƒ³ãƒ—ãƒ«å°‘"] --> D["ã‚¹ã‚³ã‚¢æ¨å®š<br/>ç²¾åº¦ä½"]
    D --> E["Langevin<br/>ç™ºæ•£ãƒªã‚¹ã‚¯"]

    F["ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚º<br/>Ïƒâ‚...Ïƒâ‚—"] --> G["ãƒã‚¤ã‚ºã§ä½å¯†åº¦ã‚’åŸ‹ã‚ã‚‹"]
    G --> H["å…¨é ˜åŸŸã§<br/>å®‰å®šæ¨å®š"]
    H --> I["Annealed LD<br/>Ïƒâ‚—â†’Ïƒâ‚"]

    style D fill:#ffebee
    style H fill:#c8e6c9
```

#### 3.6.3 å¤šæ§˜ä½“ä»®èª¬ã¨ã‚¹ã‚³ã‚¢ã®é€€åŒ– â€” å³å¯†ãªå®šå¼åŒ–

Score Matchingã®åæŸç†è«–ã¯ $p(x)$ ãŒ $\mathbb{R}^d$ ä¸Šã®ååˆ†ã«æ»‘ã‚‰ã‹ãªå¯†åº¦ã‚’æŒã¤ã“ã¨ã‚’å‰æã¨ã™ã‚‹ã€‚ã—ã‹ã—å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ã«é›†ä¸­ã—ã¦ãŠã‚Šã€ã“ã®å‰æãŒæˆç«‹ã—ãªã„ã€‚ã“ã®å•é¡Œã‚’å³å¯†ã«å®šå¼åŒ–ã™ã‚‹ã€‚

**å¤šæ§˜ä½“ã®è¨­å®š**:

ãƒ‡ãƒ¼ã‚¿å¤šæ§˜ä½“ $\mathcal{M} \subset \mathbb{R}^d$ ã‚’å›ºæœ‰æ¬¡å…ƒ $k \ll d$ ã®ãƒªãƒ¼ãƒãƒ³å¤šæ§˜ä½“ã¨ã™ã‚‹ã€‚å…·ä½“çš„ã«ã¯:

$$
\mathcal{M} = \{x \in \mathbb{R}^d : f_1(x) = 0, \ldots, f_{d-k}(x) = 0\}
$$

ã“ã“ã§ $f_1, \ldots, f_{d-k}: \mathbb{R}^d \to \mathbb{R}$ ã¯æ»‘ã‚‰ã‹ãªåˆ¶ç´„é–¢æ•°ã€‚å¤šæ§˜ä½“ $\mathcal{M}$ ä¸Šã®å±€æ‰€åº§æ¨™ $(u_1, \ldots, u_k)$ ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯è¡¨ç¾ $\phi: \mathbb{R}^k \supset U \to \mathcal{M}$ ãŒå­˜åœ¨ã™ã‚‹ã€‚

**ãƒ«ãƒ™ãƒ¼ã‚°æ¸¬åº¦ã‚¼ãƒ­ã®å•é¡Œ**:

$k < d$ ã®ã¨ãã€$\mathcal{M}$ ã® $d$ æ¬¡å…ƒãƒ«ãƒ™ãƒ¼ã‚°æ¸¬åº¦ã¯ã‚¼ãƒ­:

$$
\text{Vol}_d(\mathcal{M}) = \int_{\mathbb{R}^d} \mathbf{1}_{x \in \mathcal{M}}\, dx = 0
$$

ã—ãŸãŒã£ã¦ $\mathbb{R}^d$ ä¸Šã®ç¢ºç‡å¯†åº¦é–¢æ•° $p: \mathbb{R}^d \to \mathbb{R}_{\geq 0}$ ã¨ã—ã¦ $p_\text{data}$ ã‚’å®šç¾©ã§ããªã„ï¼ˆãƒ«ãƒ™ãƒ¼ã‚°æ¸¬åº¦ã‚¼ãƒ­ã®é›†åˆä¸Šã«å…¨ç¢ºç‡è³ªé‡ãŒé›†ä¸­ã™ã‚‹ãŸã‚ï¼‰ã€‚ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p_\text{data}(x)$ ã¯ $\mathcal{M}$ ä¸Šã§ã‚‚ã€ãã®å¤–ã§ã‚‚å®šç¾©ã•ã‚Œãªã„ã€‚

**é€€åŒ–ã®æ•°å­¦çš„æå†™**:

$\mathcal{M}$ ã« $\delta$ è¿‘å‚ãƒãƒ¥ãƒ¼ãƒ–ã‚’è€ƒãˆã‚‹:

$$
\mathcal{M}_\delta = \{x \in \mathbb{R}^d : \text{dist}(x, \mathcal{M}) < \delta\}
$$

$p_\text{data}$ ã‚’è¿‘ä¼¼ã™ã‚‹åšã• $\delta$ ã®ã€Œã‚¹ãƒ©ãƒ–åˆ†å¸ƒã€$p^\delta$ ã‚’å®šç¾©ã™ã‚‹ã¨:

$$
\|\nabla_x \log p^\delta(x)\| = O\!\left(\frac{1}{\delta}\right) \quad \text{for } x \notin \mathcal{M}_\delta
$$

$\delta \to 0$ ã§ã‚¹ã‚³ã‚¢ã®ãƒãƒ«ãƒ ãŒç™ºæ•£ â†’ å¤šæ§˜ä½“å¤–ã§ã®ã‚¹ã‚³ã‚¢ã¯ç„¡é™å¤§ã«çˆ†ç™ºã™ã‚‹ã€‚

**ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã«ã‚ˆã‚‹æ­£å‰‡åŒ–**:

ãƒã‚¤ã‚ºæ¨™æº–åå·® $\sigma > 0$ ã§æ‘‚å‹•ã—ãŸå¯†åº¦:

$$
p_\sigma(x) = \int_{\mathbb{M}} p_\text{data}(y) \mathcal{N}(x; y, \sigma^2 I)\, d\mu_{\mathcal{M}}(y)
$$

ã“ã“ã§ $d\mu_{\mathcal{M}}$ ã¯å¤šæ§˜ä½“ä¸Šã®æ¸¬åº¦ï¼ˆ$k$ æ¬¡å…ƒãƒã‚¦ã‚¹ãƒ‰ãƒ«ãƒ•æ¸¬åº¦ï¼‰ã€‚ã“ã®æ‘‚å‹•åˆ†å¸ƒã¯ $\mathbb{R}^d$ ä¸Šã®çœŸã®ç¢ºç‡å¯†åº¦ã§ã‚ã‚Š:

$$
\int_{\mathbb{R}^d} p_\sigma(x)\, dx = 1, \quad p_\sigma(x) > 0 \text{ for all } x \in \mathbb{R}^d
$$

ã‚¹ã‚³ã‚¢ãŒ well-defined ã«ãªã‚‹:

$$
\nabla_x \log p_\sigma(x) = \frac{\int p_\text{data}(y)\, \nabla_x \mathcal{N}(x; y, \sigma^2 I)\, d\mu_{\mathcal{M}}(y)}{p_\sigma(x)}
$$

**$\sigma$ ã®å½¹å‰²ã¨ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:

- $\sigma$ ãŒå¤§ãã„: $p_\sigma(x)$ ã¯ $\mathbb{R}^d$ å…¨ä½“ã«åºƒãŒã‚‹ â†’ ã‚¹ã‚³ã‚¢æ¨å®šãŒå®‰å®šã€ã—ã‹ã—å…ƒã®åˆ†å¸ƒ $p_\text{data}$ ã‹ã‚‰ã®ä¹–é›¢ãŒå¤§ãã„
- $\sigma$ ãŒå°ã•ã„: $p_\sigma(x) \approx p_\text{data}$ â†’ ã‚ˆã‚Šæ­£ç¢ºã€ã—ã‹ã—ä½å¯†åº¦é ˜åŸŸã§ã‚¹ã‚³ã‚¢ãŒä¸å®‰å®š

ã“ã®æœ¬è³ªçš„ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã€**NCSNã«ãŠã‘ã‚‹ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºã®ç†è«–çš„å¿…ç„¶æ€§**ã‚’èª¬æ˜ã™ã‚‹ã€‚

**NCSNã¨ã®æ¥ç¶š**:

ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\sigma_1 > \sigma_2 > \cdots > \sigma_L$ ã«ãŠã„ã¦:

- $\sigma_1$ (æœ€å¤§): $p_{\sigma_1}(x)$ ã¯ $\mathcal{M}$ ã‹ã‚‰é ã„é ˜åŸŸã‚‚ã‚«ãƒãƒ¼ â†’ å…¨ç©ºé–“ã§ã‚¹ã‚³ã‚¢å®‰å®š
- $\sigma_L$ (æœ€å°): $p_{\sigma_L}(x) \approx p_\text{data}$ â†’ é«˜å¿ å®Ÿåº¦ã‚µãƒ³ãƒ—ãƒ«

å„ã‚¹ã‚±ãƒ¼ãƒ«ã§ç‹¬ç«‹ã«ã‚¹ã‚³ã‚¢ã‚’å­¦ç¿’ã—ã€Annealed Langevin Dynamicsã§Annealingã™ã‚‹ã“ã¨ãŒã€å¤šæ§˜ä½“ä»®èª¬ä¸‹ã§ã®å”¯ä¸€ã®ç†è«–çš„ã« justified ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚‹ã€‚


### 3.7 Langevin Dynamics å®Œå…¨ç‰ˆ â€” ç¬¬5å›ã®å¾©ç¿’ã¨æ·±åŒ–

**Langevin Dynamics ã®å®šç¾©**:

ä»¥ä¸‹ã®SDEã®è§£ $\{x_t\}_{t \geq 0}$:

$$
dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t
$$

$W_t$ ã¯Browné‹å‹•ã€‚

**å®šå¸¸åˆ†å¸ƒ**:

$t \to \infty$ ã§ $x_t$ ã®åˆ†å¸ƒãŒ $p(x)$ ã«åæŸã™ã‚‹ï¼ˆã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰æ€§ã‚’ä»®å®šï¼‰ã€‚

**ç‰©ç†çš„è§£é‡ˆ** (ç¬¬34å›ã®çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š):

Overdamped Langevinæ–¹ç¨‹å¼ã¯ã€æ…£æ€§é …ã‚’ç„¡è¦–ã—ãŸLangevinæ–¹ç¨‹å¼:

$$
m \frac{d^2 x}{dt^2} = -\nabla U(x) - \gamma \frac{dx}{dt} + \sqrt{2 \gamma k_B T} \eta(t)
$$

$m \to 0$ (overdamped limit):

$$
\gamma \frac{dx}{dt} = -\nabla U(x) + \sqrt{2 \gamma k_B T} \eta(t)
$$

æ­£è¦åŒ– ($\gamma = 1$, $k_B T = 1$):

$$
dx = -\nabla U(x) dt + \sqrt{2} dW_t
$$

$U(x) = -\log p(x)$ (ã‚¨ãƒãƒ«ã‚®ãƒ¼ = è² ã®å¯¾æ•°å¯†åº¦) ã¨ã™ã‚‹ã¨:

$$
dx = \nabla_x \log p(x) dt + \sqrt{2} dW_t
$$

Langevin DynamicsãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

**é›¢æ•£åŒ– (Euler-Maruyamaæ³•)**:

$$
x_{t+1} = x_t + \epsilon \nabla_x \log p(x_t) + \sqrt{2\epsilon} z_t, \quad z_t \sim \mathcal{N}(0, I)
$$

$\epsilon$ ã¯ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã€‚

**Unadjusted Langevin Algorithm (ULA)**:

ä¸Šè¨˜ã®é›¢æ•£åŒ–ã‚’ãã®ã¾ã¾ä½¿ã† â†’ Metropolis-Hastingsè£œæ­£ãªã— â†’ "Unadjusted"ã€‚

**åæŸæ€§** (å¾Œè¿° Section 3.9):

é©åˆ‡ãªæ¡ä»¶ä¸‹ã§ã€ULAã¯ $p(x)$ ã«åæŸã™ã‚‹ã€‚åæŸãƒ¬ãƒ¼ãƒˆã¯ $O(d/\epsilon)$ or $O(d/T)$ ($T$ ã¯ã‚¹ãƒ†ãƒƒãƒ—æ•°)ã€‚


### 3.8 SGLD & Annealed Langevin Dynamics

**Stochastic Gradient Langevin Dynamics (SGLD)** [^4]:

Welling & Teh (2011) ã®ææ¡ˆ: **ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…ã§Langevin Dynamicsã‚’è¿‘ä¼¼**ã€‚

$$
x_{t+1} = x_t + \frac{\epsilon_t}{2} \nabla_x \log p(x_t | \mathcal{D}_t) + \eta_t, \quad \eta_t \sim \mathcal{N}(0, \epsilon_t I)
$$

$\mathcal{D}_t$ ã¯ãƒŸãƒ‹ãƒãƒƒãƒã€$\nabla_x \log p(x_t | \mathcal{D}_t)$ ã¯ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…æ¨å®šé‡ã€‚

**ã‚­ãƒ¼ã‚¢ã‚¤ãƒ‡ã‚¢**:

ãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…ã®ãƒã‚¤ã‚º $\approx$ Langevin Dynamicsã®æ‹¡æ•£é …ã€‚ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º $\epsilon_t \to 0$ ($t \to \infty$) ã§æ­£ç¢ºãªLangevin Dynamicsã«åæŸã€‚

**Annealed Langevin Dynamics (ALD)**:

Song & Ermon (2019) [^5] ã®NCSN ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã€‚

**è¨­å®š**:

ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma_1 > \sigma_2 > \cdots > \sigma_L$ (geometric: $\sigma_{i+1} = r \sigma_i$, $r < 1$)ã€‚

å„ $\sigma_i$ ã«å¯¾ã—ã€ã‚¹ã‚³ã‚¢ $s_\theta(x, \sigma_i)$ ã‚’å­¦ç¿’æ¸ˆã¿ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:


$\alpha_i$ ã¯å„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã§ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼ˆé€šå¸¸ $\alpha_i \propto \sigma_i^2$ï¼‰ã€‚

**ç›´æ„Ÿ**:

1. $\sigma_L$ (æœ€å¤§ãƒã‚¤ã‚º): åºƒã„ç¯„å›²ã‚’æ¢ç´¢ã€ç²—ã„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
2. $\sigma_{L-1}, \ldots, \sigma_2$: å¾ã€…ã«ãƒã‚¤ã‚ºã‚’æ¸›ã‚‰ã—ã€ç´°éƒ¨ã‚’ç²¾ç·»åŒ–
3. $\sigma_1$ (æœ€å°ãƒã‚¤ã‚º): å…ƒã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_\text{data}(x)$ ã«è¿‘ã„é«˜å“è³ªã‚µãƒ³ãƒ—ãƒ«

**Annealing = ç„¼ããªã¾ã—**: é‡‘å±åŠ å·¥ã§æ¸©åº¦ã‚’å¾ã€…ã«ä¸‹ã’ã¦çµæ™¶æ§‹é€ ã‚’å®‰å®šåŒ–ã•ã›ã‚‹ã®ã¨åŒã˜åŸç†ã€‚


### 3.9 ULAåæŸæ€§ â€” Wassersteinè·é›¢ã§ã®åæŸãƒ¬ãƒ¼ãƒˆ

**Unadjusted Langevin Algorithm (ULA) ã®åæŸæ€§å®šç†**:

ä»¥ä¸‹ã®æ¡ä»¶ã‚’æº€ãŸã™ã¨ã:

1. $p(x)$ ã¯ $m$-strongly log-concave: $\nabla^2 (-\log p(x)) \succeq m I$
2. $\nabla \log p$ ã¯ $L$-Lipschitz: $\|\nabla \log p(x) - \nabla \log p(y)\| \leq L \|x - y\|$
3. ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º $\epsilon < 2/(m + L)$

ULAã®åˆ†å¸ƒ $\pi_T$ ã¨ç›®æ¨™åˆ†å¸ƒ $p$ ã®Wasserstein-2è·é›¢ã¯:

$$
W_2(\pi_T, p) \leq (1 - m\epsilon)^{T/2} W_2(\pi_0, p) + O(\epsilon)
$$

**è§£é‡ˆ**:

- æŒ‡æ•°çš„åæŸ: $(1 - m\epsilon)^{T/2} \to 0$
- Bias term: $O(\epsilon)$ â†’ $\epsilon \to 0$ ã§æ­£ç¢ºã« $p$ ã«åæŸ
- åæŸæ™‚é–“: $T \sim O(\frac{1}{m\epsilon} \log \frac{1}{\delta})$ ã§ $\delta$-è¿‘ä¼¼

**é«˜æ¬¡å…ƒã§ã®èª²é¡Œ**:

åæŸãƒ¬ãƒ¼ãƒˆã¯æ¬¡å…ƒ $d$ ã«ä¾å­˜ã™ã‚‹ã€‚ä¸€èˆ¬ã« $O(d/\epsilon)$ or $O(d/T)$ â†’ æ¬¡å…ƒã®å‘ªã„ã€‚

**Manifoldä»®èª¬ä¸‹ã§ã®æ”¹å–„** (ç¬¬37å›ã§è©³èª¬):

ãƒ‡ãƒ¼ã‚¿ãŒä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«ã‚ã‚‹å ´åˆã€å›ºæœ‰æ¬¡å…ƒ $d_\text{eff} \ll d$ ã§åæŸãƒ¬ãƒ¼ãƒˆæ”¹å–„ â†’ $O(d_\text{eff} / T)$ã€‚

#### 3.9.3 Wassersteinè·é›¢ã§ã®åæŸè¨¼æ˜ â€” è©³ç´°

Overdamped Langevin SDEã®å®šå¸¸åˆ†å¸ƒãŒ $p(x)$ ã§ã‚ã‚‹ã“ã¨ã‚’Fokker-Planckæ–¹ç¨‹å¼ã‹ã‚‰ç¤ºã—ã€ULAã®é›¢æ•£åŒ–èª¤å·®ã¨Log-Sobolevä¸ç­‰å¼ã«ã‚ˆã‚‹æŒ‡æ•°åæŸã‚’è©³è¿°ã™ã‚‹ã€‚

**Overdamped Langevin SDE ã¨ Fokker-Planck æ–¹ç¨‹å¼**:

ç¢ºç‡éç¨‹ $X_t$ ãŒå¾“ã†SDEã‚’å†æ²ã™ã‚‹:

$$
dX_t = \nabla \log p(X_t)\, dt + \sqrt{2}\, dW_t
$$

$X_t$ ã®ç¢ºç‡å¯†åº¦ $\rho_t(x)$ ã®æ™‚é–“ç™ºå±•ã¯ Fokker-Planck æ–¹ç¨‹å¼ã§è¨˜è¿°ã•ã‚Œã‚‹:

$$
\frac{\partial \rho_t}{\partial t} = -\nabla \cdot \left(\rho_t \nabla \log p\right) + \Delta \rho_t = -\nabla \cdot \left(\rho_t \nabla \log p - \nabla \rho_t\right)
$$

æ‹¬å¼§å†…ã‚’æ•´ç†ã™ã‚‹ã¨:

$$
\rho_t \nabla \log p - \nabla \rho_t = \rho_t \cdot \frac{\nabla p}{p} - \nabla \rho_t
$$

**å®šå¸¸è§£ã®ç¢ºèª**: $\rho_t = p$ ã‚’ä»£å…¥ã™ã‚‹ã¨:

$$
p \cdot \frac{\nabla p}{p} - \nabla p = \nabla p - \nabla p = 0
$$

ã—ãŸãŒã£ã¦ $\partial \rho_t / \partial t = 0$ã€ã™ãªã‚ã¡ $p(x)$ ã¯ Fokker-Planck æ–¹ç¨‹å¼ã®å®šå¸¸è§£ã§ã‚ã‚‹ã€‚â–¡

ã•ã‚‰ã«ã“ã®å®šå¸¸è§£ã¸ã®åæŸã¯ã€è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ $\mathcal{F}[\rho] = \int \rho \log(\rho/p)\, dx = D_\text{KL}(\rho \| p) \geq 0$ ã®å˜èª¿æ¸›å°‘ã‹ã‚‰å¾“ã†:

$$
\frac{d}{dt} D_\text{KL}(\rho_t \| p) = -\int \rho_t \left\|\nabla \log \frac{\rho_t}{p}\right\|^2 dx \leq 0
$$

**ULAé›¢æ•£åŒ–ã¨ $O(h)$ ãƒã‚¤ã‚¢ã‚¹**:

é€£ç¶šæ™‚é–“SDEã‚’Euler-Maruyamaã§é›¢æ•£åŒ–:

$$
X_{n+1} = X_n + h \nabla \log p(X_n) + \sqrt{2h}\, \xi_n, \quad \xi_n \sim \mathcal{N}(0, I)
$$

ã“ã®é›¢æ•£åŒ–ã¯ã€ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º $h > 0$ ã«æ¯”ä¾‹ã™ã‚‹ãƒã‚¤ã‚¢ã‚¹ã‚’å°å…¥ã™ã‚‹ã€‚$\nabla \log p$ ãŒ $L$-Lipschitzã®ä¸‹ã§ã€ULAã®ä¸å¤‰æ¸¬åº¦ $\pi_h$ ã¨çœŸã®åˆ†å¸ƒ $p$ ã®Wasserstein-2è·é›¢ã¯:

$$
W_2(\pi_h, p) = O\!\left(\sqrt{h}\right)
$$

ã‚ˆã‚Šç²¾å¯†ãªä¸Šç•Œï¼ˆDalalyan 2017ï¼‰:

$$
W_2(\pi_h, p)^2 \leq \frac{dLh}{2m}
$$

ã“ã“ã§ $d$ ã¯æ¬¡å…ƒã€$m$ ã¯å¼·å¯¾æ•°å‡¹æ€§å®šæ•°ã€$L$ ã¯Lipschitzå®šæ•°ã€‚

**Log-Sobolev ä¸ç­‰å¼ (LSI)**:

$p$ ãŒä»¥ä¸‹ã®Log-Sobolevä¸ç­‰å¼ã‚’ $\rho > 0$ ã§æº€ãŸã™ã¨ã™ã‚‹:

$$
\int \rho_t \log \frac{\rho_t}{p}\, dx \leq \frac{1}{2\rho} \int \rho_t \left\|\nabla \log \frac{\rho_t}{p}\right\|^2 dx
$$

ã“ã‚Œã¯ $D_\text{KL}(\rho_t \| p) \leq \frac{1}{2\rho} I(\rho_t \| p)$ ã¨æ›¸ã‘ã‚‹ï¼ˆ$I$ ã¯Fisheræƒ…å ±é‡ï¼‰ã€‚

**LSIä¸‹ã§ã®é€£ç¶šæ™‚é–“åæŸ**:

å…ˆã»ã©ã®è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æ¸›å°‘å¼ $\frac{d}{dt} D_\text{KL}(\rho_t \| p) = -I(\rho_t \| p)$ ã¨LSIã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨:

$$
\frac{d}{dt} D_\text{KL}(\rho_t \| p) \leq -2\rho \cdot D_\text{KL}(\rho_t \| p)
$$

GrÃ¶nwall ã®è£œé¡Œã‚ˆã‚Š:

$$
D_\text{KL}(\rho_t \| p) \leq e^{-2\rho t} D_\text{KL}(\rho_0 \| p)
$$

ã•ã‚‰ã« Talagrand ã®è¼¸é€ä¸ç­‰å¼ $W_2^2(\rho, p) \leq \frac{2}{\rho} D_\text{KL}(\rho \| p)$ï¼ˆLSIã‹ã‚‰å°å‡ºå¯èƒ½ï¼‰ã‚’ä½¿ã†ã¨:

$$
\boxed{W_2(\rho_t, p)^2 \leq e^{-2\rho t} W_2(\rho_0, p)^2}
$$

ã“ã‚ŒãŒ**æŒ‡æ•°çš„åæŸ**ã®å®šé‡çš„ä¿è¨¼ã§ã‚ã‚‹ã€‚

**Gaussianåˆ†å¸ƒã§ã®LSIå®šæ•°ã®è¨ˆç®—**:

$p(x) = \mathcal{N}(x; \mu, \Sigma)$ ã®å ´åˆã€LSIå®šæ•°ã¯ç²¾åº¦è¡Œåˆ— $\Sigma^{-1}$ ã®æœ€å°å›ºæœ‰å€¤:

$$
\rho = \lambda_{\min}(\Sigma^{-1}) = \frac{1}{\lambda_{\max}(\Sigma)}
$$

ä¾‹ãˆã° $\Sigma = \text{diag}(\sigma_1^2, \ldots, \sigma_d^2)$ ãªã‚‰ã° $\rho = 1/\max_i \sigma_i^2$ã€‚æœ€å¤§åˆ†æ•£ã®æ–¹å‘ãŒåæŸé€Ÿåº¦ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹ã€‚

æ¡ä»¶æ•° $\kappa = \lambda_{\max}(\Sigma) / \lambda_{\min}(\Sigma) = \lambda_{\max}(\Sigma^{-1})^{-1} / \lambda_{\min}(\Sigma^{-1})^{-1}$ ãŒå¤§ãã„ï¼ˆåˆ†å¸ƒãŒæ­ªã‚“ã§ã„ã‚‹ï¼‰ã»ã©åæŸãŒé…ããªã‚‹ã€‚ã“ã‚ŒãŒå®Ÿç”¨çš„ãªPreconditioningï¼ˆ$\Sigma^{-1}$ ã®æ¨å®šã¨ãã®é€†è¡Œåˆ—ã«ã‚ˆã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰ã®å¿…è¦æ€§ã‚’æ­£å½“åŒ–ã™ã‚‹ã€‚

**ULAã®å®Ÿç”¨çš„åæŸä¿è¨¼ã®ã¾ã¨ã‚**:

åˆæœŸåˆ†å¸ƒ $\rho_0$ã€ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º $h$ã€$T$ ã‚¹ãƒ†ãƒƒãƒ—å¾Œã®ULAåˆ†å¸ƒ $\pi_T$ ã«å¯¾ã—ã¦:

$$
W_2(\pi_T, p)^2 \leq \underbrace{e^{-2\rho Th} W_2(\pi_0, p)^2}_{\text{åˆæœŸåŒ–èª¤å·®}} + \underbrace{O(dLh/m)}_{\text{é›¢æ•£åŒ–ãƒã‚¤ã‚¢ã‚¹}}
$$

$\varepsilon$-ç²¾åº¦ã‚’é”æˆã™ã‚‹ãŸã‚ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°: $T = O\!\left(\frac{1}{\rho h} \log \frac{W_2(\rho_0,p)^2}{\varepsilon}\right)$ã€ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º $h = O(\varepsilon m / dL)$ã€‚


### 3.10 âš”ï¸ Boss Battle: NCSNå®Œå…¨ç†è«– â€” ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´ã®æ•°å­¦

**Noise Conditional Score Network (NCSN)** [^5] ã®å®Œå…¨ç†è«–ã‚’å°å‡ºã™ã‚‹ã€‚

**è¨­å®š**:

ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\{\sigma_i\}_{i=1}^L$ã€geometric: $\sigma_i = \sigma_\text{min} \cdot (\sigma_\text{max} / \sigma_\text{min})^{(L-i)/(L-1)}$ã€‚

å„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma_i$ ã§æ‘‚å‹•ã•ã‚ŒãŸåˆ†å¸ƒ:

$$
p_{\sigma_i}(x) = \int p_\text{data}(x') \mathcal{N}(x | x', \sigma_i^2 I) dx'
$$

**NCSNè¨“ç·´ç›®çš„é–¢æ•°**:

$$
\mathcal{L}(\theta) = \frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i) \mathbb{E}_{p_\text{data}(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \left\| s_\theta(x + \sigma_i \epsilon, \sigma_i) + \frac{\epsilon}{\sigma_i} \right\|^2 \right]
$$

$\lambda(\sigma_i)$ ã¯é‡ã¿é–¢æ•°ï¼ˆé€šå¸¸ $\lambda(\sigma_i) = \sigma_i^2$ï¼‰ã€‚

**ãªãœ $\sigma_i^2$ ã§é‡ã¿ä»˜ã‘ã‚‹ã‹**:

DSMã®ç›®çš„é–¢æ•°ã‚’ $\sigma_i$ ã«ã¤ã„ã¦å¹³å‡ã™ã‚‹ã¨:

$$
\mathbb{E}_{i} [J_\text{DSM}(\theta; \sigma_i)] = \mathbb{E}_{i} \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon} \left[ \sigma_i^{-2} \left\| s_\theta(x + \sigma_i \epsilon, \sigma_i) + \frac{\epsilon}{\sigma_i} \right\|^2 \right]
$$

$\sigma_i^2$ ã§é‡ã¿ä»˜ã‘ã™ã‚‹ã“ã¨ã§ã€å„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã§ã®æå¤±ã®å¤§ãã•ã‚’æƒãˆã‚‹ï¼ˆãƒã‚¤ã‚ºãŒå¤§ãã„ã»ã©ã‚¹ã‚³ã‚¢ã®å¤§ãã•ã‚‚å¤§ãã„ãŸã‚ï¼‰ã€‚

**NCSNãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­è¨ˆ**:

- å…¥åŠ›: $x \in \mathbb{R}^d$ã€ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma \in \mathbb{R}$
- å‡ºåŠ›: ã‚¹ã‚³ã‚¢ $s_\theta(x, \sigma) \in \mathbb{R}^d$
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: U-Neté¢¨ã®æ·±å±¤NNã€$\sigma$ ã¯åŸ‹ã‚è¾¼ã¿å±¤ã§æ¡ä»¶ä»˜ã‘

**ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (Annealed Langevin Dynamics)**:


**æ•°å­¦çš„æ­£å½“æ€§**:

å„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma_i$ ã§ã€Langevin Dynamicsã¯ $p_{\sigma_i}(x)$ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

$\sigma_L \to \sigma_1$ ã¸annealing â†’ $p_{\sigma_1}(x) \approx p_\text{data}(x)$ ï¼ˆ$\sigma_1$ ãŒååˆ†å°ã•ã‘ã‚Œã°ï¼‰ã€‚

**NCSN v1 vs v2**:

- **NCSN v1** [^5]: ä¸Šè¨˜ã®æ‰‹æ³•ã€RefineNet architecture
- **NCSN v2**: Improved noise scheduleã€EMA (Exponential Moving Average) weightsã€better sample quality


**NCSN â†’ DDPM ã¸ã®æ¥ç¶š**:

NCSNã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´ã¨Annealed Langevin Dynamicsã¯ã€DDPMã®ç†è«–çš„æºæµã€‚

DDPM (ç¬¬36å›):
- Forward process: $q(x_t | x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)$ â†’ NCSN ã® $p_{\sigma_i}(x)$ ã«å¯¾å¿œ
- Reverse process: $p_\theta(x_{t-1} | x_t)$ â†’ Langevin Dynamics ã®é›¢æ•£åŒ–ã«å¯¾å¿œ
- $\epsilon$-prediction: $\epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} s_\theta(x_t, t)$ â†’ ã‚¹ã‚³ã‚¢é–¢æ•°

> **Note:** **é€²æ—: 50% å®Œäº†** Score Matchingã®å®Œå…¨ç†è«–ï¼ˆESM/DSM/Sliced/NCSNï¼‰ã¨Langevin Dynamicsã®æ•°å­¦ã‚’ä¿®å¾—ã—ãŸã€‚ãƒœã‚¹æ’ƒç ´ã€‚æ¬¡ã¯Rust/Rustã§å®Ÿè£…ã™ã‚‹ã€‚

### 3.11 æœ€æ–°ç†è«– (2025) â€” Score Matchingã®çµ±è¨ˆçš„æœ€é©æ€§

**2025å¹´ã®æœ€æ–°çµæœ**: Che et al. (2025) [^7] ã¯ã€Denoising Diffusion Modelsã®Score MatchingãŒ**FisheråŠ¹ç‡çš„**ï¼ˆçµ±è¨ˆçš„ã«æœ€é©ï¼‰ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ãŸã€‚

**å®šç† (Statistical Efficiency of DDPM)**:

é©åˆ‡ãªãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\{\sigma_i\}$ ã¨ååˆ†ãªãƒ¢ãƒ‡ãƒ«å®¹é‡ã®ä¸‹ã§ã€DDPMè¨“ç·´ã®ã‚¹ã‚³ã‚¢æ¨å®šé‡ã¯ä»¥ä¸‹ã‚’æº€ãŸã™:

$$
\mathbb{E}\left[\|\nabla_x \log p(x) - s_\theta^*(x)\|^2\right] = O(n^{-1})
$$

ã“ã“ã§ $n$ ã¯ã‚µãƒ³ãƒ—ãƒ«æ•°ã€$s_\theta^*$ ã¯æœ€é©åŒ–å¾Œã®ã‚¹ã‚³ã‚¢é–¢æ•°ã€‚ã“ã®åæŸãƒ¬ãƒ¼ãƒˆã¯**CramÃ©r-Raoä¸‹ç•Œã‚’é”æˆ**ã—ã€çµ±è¨ˆçš„ã«æœ€é©ã€‚

**Dimension-Free Annealed Langevin** (2025 arXiv:2602.01449):

å¾“æ¥ã®Langevin DynamicsåæŸãƒ¬ãƒ¼ãƒˆã¯ $O(d/\epsilon)$ ã§æ¬¡å…ƒ $d$ ã«ä¾å­˜ã€‚æœ€æ–°ç ”ç©¶ã§ã¯ã€**æ¬¡å…ƒã«ä¾å­˜ã—ãªã„åæŸ**ã‚’é”æˆ:

$$
W_2(\pi_T, p) \leq C \exp(-\lambda T) + O(\epsilon)
$$

$C, \lambda$ ã¯ $d$ ã«ç‹¬ç«‹ã€‚æ¡ä»¶: Gaussian mixtureè¿‘ä¼¼å¯èƒ½ + Preconditioned Langevin (é©å¿œçš„ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º)ã€‚

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

ãƒ—ãƒ¬ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°è¡Œåˆ— $M_t$ ã‚’å°å…¥:

$$
dx_t = M_t^{-1} \nabla_x \log p(x_t) dt + \sqrt{2 M_t^{-1}} dW_t
$$

$M_t \approx -\nabla^2 \log p(x_t)$ (å±€æ‰€Hessianè¿‘ä¼¼) ã¨ã™ã‚‹ã¨ã€Wassersteinè·é›¢ã®æ¸›å°‘ç‡:

$$
\frac{d}{dt} W_2^2(\rho_t, p) \leq -2 \lambda_{\min}(M_t) W_2^2(\rho_t, p)
$$

Gaussian mixtureä»®å®šä¸‹ã§ $\lambda_{\min}(M_t) \geq \lambda > 0$ ãŒ $d$ ã«ç‹¬ç«‹ â†’ æŒ‡æ•°åæŸã€‚

**Manifold-Aware Posterior Sampling** (2025 arXiv:2510.26324):

ãƒ‡ãƒ¼ã‚¿ãŒä½æ¬¡å…ƒå¤šæ§˜ä½“ $\mathcal{M} \subset \mathbb{R}^D$ ($\dim \mathcal{M} = d \ll D$) ã«é›†ä¸­ã™ã‚‹å ´åˆã€ã‚¹ã‚³ã‚¢æ¨å®šã¯å¤šæ§˜ä½“æ¥ç©ºé–“ã«åˆ¶é™:

$$
s_\theta(x) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma^2 I)} \left[ -\frac{\epsilon}{\sigma} \mid x + \epsilon \in \mathcal{M} \right]
$$

**Manifold Score Matching**:

$$
\mathcal{L}_{\text{manifold}} = \mathbb{E}_{x \sim p_\mathcal{M}} \mathbb{E}_{\epsilon \perp T_x \mathcal{M}} \left[ \left\| s_\theta(x + \epsilon) + \frac{\epsilon}{\sigma} \right\|^2 \right]
$$

$T_x \mathcal{M}$: å¤šæ§˜ä½“ã®æ¥ç©ºé–“ã€$\epsilon \perp T_x \mathcal{M}$: æ³•ç·šæ–¹å‘ãƒã‚¤ã‚ºã€‚

**åˆ©ç‚¹**: å›ºæœ‰æ¬¡å…ƒ $d$ ã§ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¤‡é›‘åº¦ â†’ é«˜æ¬¡å…ƒ $D$ ã§ã‚‚åŠ¹ç‡çš„ã€‚

### 3.12 Score Matching â†’ Diffusion ã¸ã®ç†è«–çš„æ©‹æ¸¡ã—

Score Matchingã®ç†è«–ä½“ç³»ï¼ˆESMãƒ»DSMãƒ»NCSNï¼‰ã¨DDPMï¼ˆDenoising Diffusion Probabilistic Modelsï¼‰ã¯è¡¨é¢ä¸Šç•°ãªã‚‹å®šå¼åŒ–ã«è¦‹ãˆã‚‹ãŒã€å®Ÿã¯åŒä¸€ã®æ•°å­¦çš„æ§‹é€ ã‚’æŒã¤ã€‚ã“ã®æ¥ç¶šã‚’å³å¯†ã«å°å‡ºã™ã‚‹ã€‚

#### 3.12.1 DDPMã®ã‚¹ã‚³ã‚¢é–¢æ•°ã¨DSMã®ç­‰ä¾¡æ€§

DDPMã®å‰å‘ãéç¨‹:

$$
q(x_t | x_0) = \mathcal{N}\!\left(x_t;\, \sqrt{\bar\alpha_t}\, x_0,\, (1 - \bar\alpha_t) I\right)
$$

ã“ã“ã§ $\bar\alpha_t = \prod_{s=1}^t \alpha_s$ã€$\alpha_s = 1 - \beta_s$ï¼ˆ$\beta_s$ ã¯ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰ã€‚

ã“ã®åˆ†å¸ƒã®ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’è¨ˆç®—ã™ã‚‹:

$$
\log q(x_t | x_0) = -\frac{d}{2}\log\!\left(2\pi(1-\bar\alpha_t)\right) - \frac{\|x_t - \sqrt{\bar\alpha_t} x_0\|^2}{2(1-\bar\alpha_t)}
$$

$$
\nabla_{x_t} \log q(x_t | x_0) = -\frac{x_t - \sqrt{\bar\alpha_t} x_0}{1 - \bar\alpha_t}
$$

ãƒªãƒ‘ãƒ©ãƒ¡ãƒˆãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ $x_t = \sqrt{\bar\alpha_t} x_0 + \sqrt{1-\bar\alpha_t}\, \epsilon$ï¼ˆ$\epsilon \sim \mathcal{N}(0,I)$ï¼‰ã‚’ä»£å…¥ã™ã‚‹ã¨:

$$
x_t - \sqrt{\bar\alpha_t} x_0 = \sqrt{1-\bar\alpha_t}\, \epsilon
$$

$$
\boxed{\nabla_{x_t} \log q(x_t | x_0) = -\frac{\sqrt{1-\bar\alpha_t}\,\epsilon}{1-\bar\alpha_t} = -\frac{\epsilon}{\sqrt{1-\bar\alpha_t}}}
$$

ã“ã‚Œã¯DSMã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ $-\epsilon/\sigma$ ã®å½¢ï¼ˆ$\sigma = \sqrt{1-\bar\alpha_t}$ ã¨ãŠã„ãŸå ´åˆï¼‰ã«å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã€‚DDPMã®$\epsilon$-äºˆæ¸¬ã¯DSMã®ã‚¹ã‚³ã‚¢æ¨å®šã«ä»–ãªã‚‰ãªã„ã€‚

#### 3.12.2 $\epsilon$-äºˆæ¸¬ã¨ã‚¹ã‚³ã‚¢é–¢æ•°ã®å¤‰æ›

DDPMã§ã¯ $\epsilon_\theta(x_t, t)$ ã‚’å­¦ç¿’ã—ã€ã‚¹ã‚³ã‚¢é–¢æ•°ã¨ã®é–¢ä¿‚:

$$
s_\theta(x_t, t) = -\frac{\epsilon_\theta(x_t, t)}{\sqrt{1-\bar\alpha_t}}
$$

ãŒæˆç«‹ã™ã‚‹ã€‚ã¤ã¾ã‚ŠDDPMã® $\epsilon$-äºˆæ¸¬ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’é™¤ã„ã¦NCSNã®ã‚¹ã‚³ã‚¢ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $s_\theta(x, \sigma_t)$ ã¨ç­‰ä¾¡ã§ã‚ã‚Šã€$\sigma_t = \sqrt{1-\bar\alpha_t}$ ãŒãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å¯¾å¿œã™ã‚‹ã€‚

#### 3.12.3 ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«DSMã¨ã—ã¦ã®DDPMè¨“ç·´ç›®çš„é–¢æ•°

DDPMã®è¨“ç·´æå¤±ï¼ˆsimplifiedï¼‰:

$$
\mathcal{L}_\text{simple} = \mathbb{E}_{t, x_0, \epsilon}\!\left[\left\|\epsilon - \epsilon_\theta(x_t, t)\right\|^2\right]
$$

NCSNã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«DSMç›®çš„é–¢æ•°ã¨æ¯”è¼ƒã™ã‚‹:

$$
\mathcal{L}_\text{NCSN} = \sum_{i=1}^L \lambda(\sigma_i)\, \mathbb{E}_{p(x_0)}\mathbb{E}_{\epsilon}\!\left[\left\|s_\theta(x_0 + \sigma_i \epsilon,\, \sigma_i) + \frac{\epsilon}{\sigma_i}\right\|^2\right]
$$

$s_\theta = -\epsilon_\theta / \sigma_t$ã€$\sigma_i = \sqrt{1-\bar\alpha_t}$ ã®ç½®ãæ›ãˆã‚’è¡Œã„ã€$\lambda(\sigma_i) = \sigma_i^2 = 1 - \bar\alpha_t$ ã‚’é¸æŠã™ã‚‹ã¨:

$$
\mathcal{L}_\text{NCSN} = \sum_t (1-\bar\alpha_t) \cdot \frac{1}{1-\bar\alpha_t} \mathbb{E}\!\left[\left\|\epsilon_\theta(x_t,t) - \epsilon\right\|^2\right] = \sum_t \mathbb{E}\!\left[\left\|\epsilon - \epsilon_\theta(x_t,t)\right\|^2\right]
$$

é€£ç¶šæ™‚é–“æ¥µé™ $T \to \infty$ ã§ã¯å’ŒãŒç©åˆ†ã«ãªã‚Š:

$$
\mathcal{L} = \int_0^1 \mathbb{E}\!\left[\left\|\epsilon - \epsilon_\theta(x_t, t)\right\|^2\right] dt
$$

ã“ã‚Œã¯DDPMã®simplified lossã®é€£ç¶šç‰ˆã«ä¸€è‡´ã™ã‚‹ã€‚**DDPMã¨NCSNã¯åŒä¸€ã®ç›®çš„é–¢æ•°ã®ç•°ãªã‚‹é›¢æ•£åŒ–ã§ã‚ã‚‹**ã€‚

#### 3.12.4 Tweedie ã®å…¬å¼

$x_t = \sqrt{\bar\alpha_t} x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon$ ã¨ã—ã¦ã€$x_0$ ã®äº‹å¾ŒæœŸå¾…å€¤ã‚’æ±‚ã‚ã‚‹ã€‚

ä¸€èˆ¬ã«Gaussianãƒã‚¤ã‚ºä¸‹ã§ã®Denoisingæ¨å®šé‡ã¯**Tweedie ã®å…¬å¼**ã§ä¸ãˆã‚‰ã‚Œã‚‹:

$$
\mathbb{E}[x_0 | x_t] = \frac{x_t + (1-\bar\alpha_t)\nabla_{x_t} \log q(x_t)}{\sqrt{\bar\alpha_t}}
$$

**å°å‡º**: $q(x_t) = \int p(x_0) q(x_t|x_0) dx_0$ ã®ä¸¡è¾ºã‚’ $x_t$ ã§å¾®åˆ†ã—ã€$\nabla_{x_t} \log q(x_t)$ ã‚’ $x_0$ ã®æ¡ä»¶ä»˜ãæœŸå¾…å€¤ã§è¡¨ã™:

$$
\nabla_{x_t} \log q(x_t) = \frac{\nabla_{x_t} q(x_t)}{q(x_t)} = \frac{\int p(x_0)\, \nabla_{x_t} q(x_t|x_0)\, dx_0}{q(x_t)}
$$

$$
= \mathbb{E}_{p(x_0|x_t)}\!\left[\nabla_{x_t} \log q(x_t|x_0)\right] = \mathbb{E}_{p(x_0|x_t)}\!\left[-\frac{x_t - \sqrt{\bar\alpha_t} x_0}{1-\bar\alpha_t}\right]
$$

æ•´ç†ã™ã‚‹ã¨:

$$
(1-\bar\alpha_t)\nabla_{x_t} \log q(x_t) = -x_t + \sqrt{\bar\alpha_t}\,\mathbb{E}[x_0|x_t]
$$

$$
\mathbb{E}[x_0|x_t] = \frac{x_t + (1-\bar\alpha_t)\nabla_{x_t}\log q(x_t)}{\sqrt{\bar\alpha_t}}
$$

ã‚¹ã‚³ã‚¢æ¨å®šé‡ $s_\theta \approx \nabla_{x_t}\log q(x_t)$ ã‚’ç”¨ã„ãŸè¿‘ä¼¼Denoising:

$$
\hat{x}_0 = \frac{x_t + (1-\bar\alpha_t) s_\theta(x_t, t)}{\sqrt{\bar\alpha_t}} = \frac{x_t - \sqrt{1-\bar\alpha_t}\,\epsilon_\theta(x_t, t)}{\sqrt{\bar\alpha_t}}
$$

ã“ã‚Œã¯DDPMã®é€†éç¨‹ã®ã‚¹ãƒ†ãƒƒãƒ— $\mu_\theta(x_t, t)$ ã®è¨ˆç®—å¼ã«å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã€‚

#### 3.12.5 VP-SDE â€” é€£ç¶šæ™‚é–“æ¥µé™ã§ã®ç†è«–çš„çµ±ä¸€

$T \to \infty$ ã®é€£ç¶šæ™‚é–“æ¥µé™ã§ã¯ã€DDPMã®å‰å‘ãéç¨‹ã¯**Variance Preserving SDE (VP-SDE)**:

$$
dX_t = -\frac{\beta(t)}{2} X_t\, dt + \sqrt{\beta(t)}\, dW_t
$$

ã«åæŸã™ã‚‹ï¼ˆ$\beta(t)$ ã¯ $\beta_s$ ã®é€£ç¶šç‰ˆãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰ã€‚ã“ã®å‰å‘ãSDEã®**æ™‚é–“åè»¢SDE**ï¼ˆAnderson 1982ï¼‰ã¯:

$$
dX_t = \left[-\frac{\beta(t)}{2} X_t - \beta(t)\nabla_{x}\log q_t(X_t)\right] dt + \sqrt{\beta(t)}\, d\bar{W}_t
$$

ã“ã“ã§ $\bar{W}_t$ ã¯é€†æ™‚é–“Browné‹å‹•ã€$q_t(x)$ ã¯æ™‚åˆ» $t$ ã§ã®å‘¨è¾ºå¯†åº¦ã€‚ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log q_t(x)$ ãŒé€†SDEã®ãƒ‰ãƒªãƒ•ãƒˆã«ç›´æ¥ç¾ã‚Œã‚‹ã€‚

$$
\text{é€†SDEã®ãƒ‰ãƒªãƒ•ãƒˆ} = -\frac{\beta(t)}{2} x - \beta(t) s_\theta(x, t)
$$

Score Matchingã§å­¦ç¿’ã—ãŸ $s_\theta$ ã‚’é€†SDEã«ä»£å…¥ã™ã‚‹ã“ã¨ã§ã€æ‹¡æ•£éç¨‹ã‚’æ™‚é–“åè»¢ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒã§ãã‚‹ã€‚ã“ã‚ŒãŒ**Song et al. (2021) Score SDE**ã®æ ¸å¿ƒã§ã‚ã‚Šã€Score Matchingã¨Diffusionã®ç†è«–çš„çµ±ä¸€ç‚¹ã§ã‚ã‚‹ã€‚

```mermaid
graph TB
    A["Score Matching<br/>HyvÃ¤rinen 2005"] --> B["DSM<br/>Vincent 2011"]
    B --> C["NCSN<br/>Song & Ermon 2019"]
    C --> D["Score SDE<br/>Song et al. 2021"]
    
    E["DDPM<br/>Ho et al. 2020"] --> D
    
    B -->|"Ïƒ_t = âˆš(1-á¾±_t)"| E
    C -->|"multi-scale = time steps"| E
    
    D --> F["VP-SDE / VE-SDE<br/>çµ±ä¸€ç†è«–"]
    F --> G["Part 2: å®Ÿè£…<br/>Zone 4-5"]

    style D fill:#fff3cd
    style F fill:#c8e6c9
```

---




> Progress: 50%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $(Îµ): Empirical mean = $ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: HyvÃ¤rinen, A. (2005). "Estimation of Non-Normalized Statistical Models by Score Matching." *Journal of Machine Learning Research*, 6(24), 695â€“709.
<https://jmlr.org/papers/v6/hyvarinen05a.html>

[^2]: Vincent, P. (2011). "A Connection Between Score Matching and Denoising Autoencoders." *Neural Computation*, 23(7), 1661â€“1674.
<https://direct.mit.edu/neco/article/23/7/1661/7677/A-Connection-Between-Score-Matching-and-Denoising>

[^3]: Song, Y., Garg, S., Shi, J., & Ermon, S. (2019). "Sliced Score Matching: A Scalable Approach to Density and Score Estimation." *UAI 2019*.
<https://arxiv.org/abs/1905.07088>

[^4]: Welling, M., & Teh, Y. W. (2011). "Bayesian Learning via Stochastic Gradient Langevin Dynamics." *ICML 2011*.
<https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf>

[^5]: Song, Y., & Ermon, S. (2019). "Generative Modeling by Estimating Gradients of the Data Distribution." *NeurIPS 2019*.
<https://arxiv.org/abs/1907.05600>

[^6]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). "Score-Based Generative Modeling through Stochastic Differential Equations." *ICLR 2021*.
<https://arxiv.org/abs/2011.13456>

[^7]: Chewi, S., Kalavasis, A., Mehrotra, A., & Montasser, O. (2025). DDPM Score Matching and Distribution Learning.
<https://arxiv.org/abs/2504.05161>

[^8]: Ho, J., Jain, A., & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS 2020*.
<https://arxiv.org/abs/2006.11239>

### æ•™ç§‘æ›¸

- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Chapter 25: Score-Based Models]
- Shalev-Shwartz, S., & Ben-David, S. (2024). *Foundations of Deep Learning*. Cambridge University Press.

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- [Yang Song's Blog: Score-Based Generative Models](https://yang-song.net/blog/2021/score/)
- [Lil'Log: "What are Diffusion Models?"](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
- [MIT 6.S184 (2026): Generative AI](https://diffusion.csail.mit.edu/)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
