---
title: "ç¬¬20å›: VAE/GAN/Transformerãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯å®Ÿè£… & åˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”¥"
type: "tech"
topics: ["machinelearning", "deeplearning", "julia", "rust", "elixir"]
published: true
---

:::message
**å‰æçŸ¥è­˜**: ç¬¬19å›ã§3è¨€èªç’°å¢ƒã¨FFIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰æ¸ˆã¿ã€‚Course IIã§VAE/GAN/Transformerã®ç†è«–ã‚’ç¿’å¾—æ¸ˆã¿ã€‚
**ç›®æ¨™**: ç†è«–ã‚’3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆJuliaè¨“ç·´â†’Rustæ¨è«–â†’Elixiré…ä¿¡ï¼‰ã§å®Ÿè£…ã™ã‚‹ã€‚
**é€²æ—**: å…¨ä½“ã®80%å®Œäº†
:::

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” ç†è«–â†’å®Ÿè£…ã®1è¡Œå¯¾å¿œ

ç¬¬19å›ã§ç’°å¢ƒã‚’æ•´ãˆãŸã€‚ç¬¬10å›ã§VAEã€ç¬¬12å›ã§GANã€ç¬¬16å›ã§Transformerã®**ç†è«–**ã‚’å­¦ã‚“ã ã€‚ä»Šå›ã¯ãã‚Œã‚’**å‹•ã‹ã™**ã€‚

ç†è«–ã¨å®Ÿè£…ã®å¯¾å¿œã‚’ä½“æ„Ÿã—ã‚ˆã†ã€‚VAEã®ELBOã‚’1è¡Œã§ï¼š

```julia
using Lux, Optimisers, Random

# VAE ELBO = å†æ§‹æˆé … - KLæ­£å‰‡åŒ–é …
function elbo_loss(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x)
    # Encoder: q_Ï†(z|x) â†’ (Î¼, log_ÏƒÂ²)
    (Î¼, logÏƒÂ²), st_enc = encoder(x, ps_enc, st_enc)

    # Reparameterization: z = Î¼ + ÏƒâŠ™Îµ
    Îµ = randn(Float32, size(Î¼)...)
    Ïƒ = exp.(logÏƒÂ² ./ 2)
    z = Î¼ .+ Ïƒ .* Îµ

    # Decoder: p_Î¸(x|z) â†’ xÌ‚
    xÌ‚, st_dec = decoder(z, ps_dec, st_dec)

    # ELBO = ğ”¼[log p(x|z)] - KL[q(z|x) || p(z)]
    recon = -sum((x .- xÌ‚).^2) / size(x, 2)  # å†æ§‹æˆé …ï¼ˆã‚¬ã‚¦ã‚¹å°¤åº¦ï¼‰
    kl = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²)) / size(x, 2)  # KLç™ºæ•£

    return -(recon - kl), (st_enc, st_dec)  # ELBOã‚’æœ€å¤§åŒ– = è² ã®ELBOã‚’æœ€å°åŒ–
end
```

**ã“ã®30è¡ŒãŒç¬¬10å›ã®æ•°å¼ã‚’ã™ã¹ã¦å«ã‚€**ï¼š

$$
\mathcal{L}_{\text{ELBO}} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{\text{KL}}[q_\phi(z|x) \| p(z)]
$$

- å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯: $z = \mu + \sigma \odot \epsilon$ï¼ˆ23è¡Œç›®ï¼‰
- ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼: $-\frac{1}{2}\sum(1 + \log\sigma^2 - \mu^2 - \sigma^2)$ï¼ˆ28è¡Œç›®ï¼‰
- æ•°å¼ã®å„é …ãŒã‚³ãƒ¼ãƒ‰ã®å„è¡Œã«**1:1å¯¾å¿œ**

ã“ã‚ŒãŒJuliaã®å¨åŠ›ã€‚æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ã®è·é›¢ãŒã‚¼ãƒ­ã€‚

:::message
**é€²æ—**: å…¨ä½“ã®3%å®Œäº†ã€‚ç†è«–ã‚’å®Ÿè£…ã«ç¿»è¨³ã™ã‚‹æº–å‚™ãŒã§ããŸã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” 3ãƒ¢ãƒ‡ãƒ«ã‚’è§¦ã‚‹

ç†è«–ã‚’å¾©ç¿’ã—ãªãŒã‚‰ã€3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã™ã€‚æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œã‚’ä½“æ„Ÿã™ã‚‹ã€‚

### 1.1 VAE â€” æ½œåœ¨ç©ºé–“ã§ç”»åƒã‚’åœ§ç¸®ãƒ»å†æ§‹æˆ

ç¬¬10å›ã§å­¦ã‚“ã VAEã®æ ¸å¿ƒï¼š**è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $x$ ã‚’ä½æ¬¡å…ƒæ½œåœ¨å¤‰æ•° $z$ ã«åœ§ç¸®ã—ã€ãã“ã‹ã‚‰å†æ§‹æˆã™ã‚‹**ã€‚

```julia
using Lux, MLUtils, MLDatasets, Optimisers

# MNIST ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
train_data = MNIST(split=:train)
x_train = Float32.(train_data.features) |> flatten_images  # (784, 60000)

# VAE ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
encoder = Chain(
    Dense(784 => 400, tanh),
    Dense(400 => 200, tanh),
    Dense(200 => 40)  # â†’ [Î¼(20æ¬¡å…ƒ), log_ÏƒÂ²(20æ¬¡å…ƒ)]
)

decoder = Chain(
    Dense(20 => 200, tanh),
    Dense(200 => 400, tanh),
    Dense(400 => 784, sigmoid)  # sigmoid for pixel values [0,1]
)

# è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
opt = Adam(0.001f0)
ps_enc, st_enc = Lux.setup(Random.default_rng(), encoder)
ps_dec, st_dec = Lux.setup(Random.default_rng(), decoder)

for epoch in 1:10
    for batch in DataLoader((x_train,), batchsize=128, shuffle=true)
        x = batch[1]
        loss, grads = Lux.Training.compute_gradients(
            AutoZygote(), elbo_loss, encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x
        )
        ps_enc, ps_dec = Optimisers.update!(opt, (ps_enc, ps_dec), grads)
    end
    println("Epoch $epoch: loss = $(loss)")
end

# æ½œåœ¨ç©ºé–“ã§ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
z_random = randn(Float32, 20, 10)  # 10å€‹ã®ãƒ©ãƒ³ãƒ€ãƒ æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«
x_generated, _ = decoder(z_random, ps_dec, st_dec)
# â†’ æ–°ã—ã„æ•°å­—ç”»åƒãŒç”Ÿæˆã•ã‚Œã‚‹
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------|:-----|
| $q_\phi(z\|x) = \mathcal{N}(z; \mu_\phi(x), \sigma^2_\phi(x)\mathbf{I})$ | `(Î¼, logÏƒÂ²) = encoder(x)` | EncoderãŒå¹³å‡ã¨åˆ†æ•£ã‚’å‡ºåŠ› |
| $z = \mu + \sigma \odot \epsilon, \epsilon \sim \mathcal{N}(0, \mathbf{I})$ | `z = Î¼ .+ Ïƒ .* randn(...)` | å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ |
| $p_\theta(x\|z) = \mathcal{N}(x; \mu_\theta(z), \mathbf{I})$ | `xÌ‚ = decoder(z)` | DecoderãŒå†æ§‹æˆç”»åƒã‚’å‡ºåŠ› |
| $D_{\text{KL}}[q_\phi(z\|x) \| \mathcal{N}(0, \mathbf{I})]$ | `-0.5 * sum(1 + logÏƒÂ² - Î¼Â² - exp(logÏƒÂ²))` | ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼ |

**ä½“æ„Ÿ**ï¼šæ½œåœ¨ç©ºé–“ $z \in \mathbb{R}^{20}$ ã§784æ¬¡å…ƒç”»åƒã‚’è¡¨ç¾ã€‚ç¬¬10å›ã®æ•°å¼ãŒãã®ã¾ã¾å‹•ãã€‚

---

### 1.2 GAN â€” ç”Ÿæˆå™¨ã¨è­˜åˆ¥å™¨ã®å¯¾æ±º

ç¬¬12å›ã§å­¦ã‚“ã GANã®æ ¸å¿ƒï¼š**Generator $G$ ãŒãƒã‚¤ã‚º $z$ ã‹ã‚‰å½ç”»åƒã‚’ç”Ÿæˆã—ã€Criticï¼ˆè­˜åˆ¥å™¨ï¼‰ $D$ ãŒæœ¬ç‰©/å½ç‰©ã‚’è¦‹åˆ†ã‘ã‚‹ç«¶äº‰**ã€‚

WGANã®æå¤±é–¢æ•°ï¼ˆç¬¬13å›ã§å­¦ã‚“ã Wassersteinè·é›¢ãƒ™ãƒ¼ã‚¹ï¼‰ï¼š

$$
\mathcal{L}_D = \mathbb{E}_{x \sim p_r}[D(x)] - \mathbb{E}_{z \sim p_z}[D(G(z))] - \lambda \mathbb{E}_{\hat{x}}[(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2]
$$

$$
\mathcal{L}_G = -\mathbb{E}_{z \sim p_z}[D(G(z))]
$$

```julia
# Generator: z (100æ¬¡å…ƒãƒã‚¤ã‚º) â†’ ç”»åƒ (28Ã—28)
generator = Chain(
    Dense(100 => 256, relu),
    Dense(256 => 512, relu),
    Dense(512 => 784, tanh)  # tanh for [-1, 1] pixel range
)

# Critic (WGAN-GPã§ã¯è­˜åˆ¥å™¨ã‚’"Critic"ã¨å‘¼ã¶)
critic = Chain(
    Dense(784 => 512, leakyrelu),
    Dense(512 => 256, leakyrelu),
    Dense(256 => 1)  # ã‚¹ã‚³ã‚¢å‡ºåŠ›ï¼ˆç¢ºç‡ã§ã¯ãªã„ï¼‰
)

# WGAN-GPè¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
function train_wgan_gp!(generator, critic, real_data, epochs=100, Î»_gp=10.0f0)
    opt_g = Adam(0.0001f0, (0.5f0, 0.9f0))  # Generator optimizer
    opt_c = Adam(0.0001f0, (0.5f0, 0.9f0))  # Critic optimizer

    for epoch in 1:epochs
        for batch in DataLoader((real_data,), batchsize=64, shuffle=true)
            x_real = batch[1]
            batch_size = size(x_real, 2)

            # --- Criticã‚’5å›æ›´æ–° ---
            for _ in 1:5
                z = randn(Float32, 100, batch_size)
                x_fake = generator(z, ps_g, st_g)[1]

                # Gradient Penalty è¨ˆç®—
                Î± = rand(Float32, 1, batch_size)
                x_interp = Î± .* x_real .+ (1 .- Î±) .* x_fake
                grad_interp = gradient(x -> sum(critic(x, ps_c, st_c)[1]), x_interp)[1]
                gp = mean((sqrt.(sum(grad_interp.^2, dims=1)) .- 1).^2)

                # Critic loss
                loss_c = mean(critic(x_fake, ps_c, st_c)[1]) - mean(critic(x_real, ps_c, st_c)[1]) + Î»_gp * gp
                ps_c = update!(opt_c, ps_c, gradient(loss_c, ps_c)[1])
            end

            # --- Generatorã‚’1å›æ›´æ–° ---
            z = randn(Float32, 100, batch_size)
            loss_g = -mean(critic(generator(z, ps_g, st_g)[1], ps_c, st_c)[1])
            ps_g = update!(opt_g, ps_g, gradient(loss_g, ps_g)[1])
        end
        println("Epoch $epoch: D_loss=$(loss_c), G_loss=$(loss_g)")
    end
end
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------|:-----|
| $G(z)$ | `generator(z)` | ãƒã‚¤ã‚ºâ†’å½ç”»åƒ |
| $D(x)$ | `critic(x)` | ç”»åƒâ†’ã‚¹ã‚³ã‚¢ |
| $\hat{x} = \alpha x + (1-\alpha)G(z)$ | `x_interp = Î± .* x_real .+ (1 .- Î±) .* x_fake` | æœ¬ç‰©ã¨å½ç‰©ã®è£œé–“ |
| $\|\nabla_{\hat{x}} D(\hat{x})\|_2$ | `sqrt(sum(grad_interp.^2, dims=1))` | å‹¾é…ãƒãƒ«ãƒ  |
| $(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2$ | `(sqrt(...) .- 1).^2` | Gradient Penalty |

**ä½“æ„Ÿ**ï¼šCriticã‚’5å›ã€Generatorã‚’1å›æ›´æ–°ï¼ˆWGAN-GPæ¨å¥¨æ¯”ç‡ï¼‰ã€‚ç¬¬12å›ãƒ»ç¬¬13å›ã®æ•°å¼ãŒãã®ã¾ã¾å‹•ãã€‚

---

### 1.3 Transformer â€” Attentionã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬

ç¬¬16å›ã§å­¦ã‚“ã Transformerã®æ ¸å¿ƒï¼š**Multi-Head Attentionã§æ–‡è„ˆã‚’ä¸¦åˆ—å‡¦ç†ã—ã€æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬**ã€‚

Scaled Dot-Product Attentionã®æ•°å¼ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

```julia
using Lux, NNlib

# Multi-Head Attention ãƒ¬ã‚¤ãƒ¤ãƒ¼
struct MultiHeadAttention <: Lux.AbstractExplicitLayer
    num_heads::Int
    d_model::Int
    d_k::Int
    q_proj::Dense
    k_proj::Dense
    v_proj::Dense
    o_proj::Dense
end

function MultiHeadAttention(d_model::Int, num_heads::Int)
    d_k = d_model Ã· num_heads
    return MultiHeadAttention(
        num_heads, d_model, d_k,
        Dense(d_model => d_model),  # Q projection
        Dense(d_model => d_model),  # K projection
        Dense(d_model => d_model),  # V projection
        Dense(d_model => d_model)   # Output projection
    )
end

function (mha::MultiHeadAttention)(x, ps, st)
    batch_size, seq_len, _ = size(x)

    # Q, K, V projection
    Q, st_q = mha.q_proj(x, ps.q_proj, st.q_proj)
    K, st_k = mha.k_proj(x, ps.k_proj, st.k_proj)
    V, st_v = mha.v_proj(x, ps.v_proj, st.v_proj)

    # Reshape for multi-head: (batch, seq_len, d_model) â†’ (batch, num_heads, seq_len, d_k)
    Q = reshape(Q, batch_size, mha.num_heads, seq_len, mha.d_k) |> permutedims([1,2,4,3])
    K = reshape(K, batch_size, mha.num_heads, seq_len, mha.d_k) |> permutedims([1,2,4,3])
    V = reshape(V, batch_size, mha.num_heads, seq_len, mha.d_k) |> permutedims([1,2,4,3])

    # Scaled Dot-Product Attention: softmax(QK^T / âˆšd_k) V
    scores = batched_mul(Q, batched_transpose(K)) ./ sqrt(Float32(mha.d_k))  # (batch, heads, seq, seq)
    attn_weights = softmax(scores, dims=4)  # Softmax over key dimension
    out = batched_mul(attn_weights, V)  # (batch, heads, d_k, seq)

    # Concatenate heads and project
    out = permutedims(out, [1,4,2,3]) |> x -> reshape(x, batch_size, seq_len, mha.d_model)
    out, st_o = mha.o_proj(out, ps.o_proj, st.o_proj)

    return out, (st_q=st_q, st_k=st_k, st_v=st_v, st_o=st_o)
end

# Causal Maskï¼ˆæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã›ãªã„ï¼‰
function causal_mask(seq_len)
    mask = triu(ones(Float32, seq_len, seq_len), 1)  # ä¸Šä¸‰è§’è¡Œåˆ—
    return mask .* -Inf32  # Softmaxå‰ã«åŠ ç®— â†’ æœªæ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®é‡ã¿ã‚’0ã«
end

# ä½¿ç”¨ä¾‹
x = randn(Float32, 2, 10, 512)  # (batch=2, seq_len=10, d_model=512)
mha = MultiHeadAttention(512, 8)
ps, st = Lux.setup(Random.default_rng(), mha)
y, st = mha(x, ps, st)  # y: (2, 10, 512) â€” å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æ–°ã—ã„è¡¨ç¾
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------|:-----|
| $Q, K, V = xW_Q, xW_K, xW_V$ | `Q = mha.q_proj(x)` | ç·šå½¢å¤‰æ› |
| $\frac{QK^\top}{\sqrt{d_k}}$ | `scores = Q @ K.T / sqrt(d_k)` | ã‚¹ã‚³ã‚¢è¨ˆç®— |
| $\text{softmax}(\cdot)$ | `softmax(scores, dims=4)` | æ³¨æ„é‡ã¿æ­£è¦åŒ– |
| $\text{softmax}(\cdot)V$ | `attn_weights @ V` | åŠ é‡å’Œ |
| Causal Mask | `scores + causal_mask` | æœªæ¥ã‚’è¦‹ã›ãªã„ |

**ä½“æ„Ÿ**ï¼šMulti-Head AttentionãŒä¸¦åˆ—ã«è¤‡æ•°ã®è¦–ç‚¹ã§æ–‡è„ˆã‚’æ‰ãˆã‚‹ã€‚ç¬¬16å›ã®æ•°å¼ãŒãã®ã¾ã¾å‹•ãã€‚

---

### 1.4 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œã®å®Œå…¨æ€§

3ãƒ¢ãƒ‡ãƒ«ã§å…±é€šã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼š

```julia
# æ•°å¼: ğ”¼[f(z)] where z ~ q(z)
# ã‚³ãƒ¼ãƒ‰: mean(f(z) for z in sample(q, n_samples))

# æ•°å¼: âˆ‡_Î¸ L(Î¸)
# ã‚³ãƒ¼ãƒ‰: gradient(Î¸ -> L(Î¸), Î¸)

# æ•°å¼: Î¸ â† Î¸ - Î·âˆ‡_Î¸ L
# ã‚³ãƒ¼ãƒ‰: Î¸ = update!(optimizer, Î¸, grads)
```

Juliaã®åˆ©ç‚¹ï¼š
- `.=` broadcastæ¼”ç®—å­ â†’ è¦ç´ ã”ã¨ã®æ¼”ç®—ã‚’1è¡Œã§
- `|>` pipeæ¼”ç®—å­ â†’ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ˜ç¤º
- å‹å®‰å®šæ€§ â†’ `@code_warntype`ã§å‹æ¨è«–ãƒã‚§ãƒƒã‚¯ â†’ è‡ªå‹•æœ€é©åŒ–

æ¬¡ã®Zone 2ã§ã€ãªãœã“ã®3ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã™ã‚‹ã®ã‹ã€å…¨ä½“åƒã‚’è¦‹ã‚‹ã€‚

:::message
**é€²æ—**: å…¨ä½“ã®10%å®Œäº†ã€‚3ãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œã‚’ä½“æ„Ÿã—ãŸã€‚
:::

---
## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœã“ã®3ãƒ¢ãƒ‡ãƒ«ã‹

### 2.1 Course IIIã®ä½ç½®ã¥ã‘ â€” ç†è«–â†’å®Ÿè£…ã®æ©‹æ¸¡ã—

```mermaid
graph TD
    A[Course I: æ•°å­¦åŸºç¤<br>ç¬¬1-8å›] --> B[Course II: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–<br>ç¬¬9-16å›]
    B --> C[Course III: å®Ÿè£…ç·¨<br>ç¬¬17-24å›]
    C --> D[Course IV: Diffusionç†è«–<br>ç¬¬25-32å›]
    D --> E[Course V: å¿œç”¨ãƒ»æœ€å‰ç·š<br>ç¬¬33-40å›]

    B -->|ç†è«–| F[VAE ç¬¬10å›]
    B -->|ç†è«–| G[GAN ç¬¬12å›]
    B -->|ç†è«–| H[Transformer ç¬¬16å›]

    F --> I[VAEå®Ÿè£… ç¬¬20å›<br>ä»Šã‚³ã‚³]
    G --> I
    H --> I

    style I fill:#ff6b6b,stroke:#333,stroke-width:4px
```

**Course IIã§å­¦ã‚“ã ã“ã¨**ï¼ˆç†è«–ï¼‰ï¼š
- ç¬¬10å›ï¼šVAEã®ELBOå°å‡ºã€å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ã€ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼
- ç¬¬12å›ï¼šGANã®Minimaxæå¤±ã€JSDã€Mode Collapseå•é¡Œ
- ç¬¬13å›ï¼šOptimal Transportã¨Wassersteinè·é›¢ã€WGAN-GP
- ç¬¬16å›ï¼šTransformerã®Attentionæ©Ÿæ§‹ã€Positional Encodingã€Causal Mask

**Course IIIã§å­¦ã¶ã“ã¨**ï¼ˆå®Ÿè£…ï¼‰ï¼š
- ç¬¬19å›ï¼š3è¨€èªç’°å¢ƒæ§‹ç¯‰ï¼ˆJulia/Rust/Elixirï¼‰ã€FFIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ
- **ç¬¬20å›ï¼ˆä»Šå›ï¼‰**ï¼šVAE/GAN/Transformerã®å®Œå…¨å®Ÿè£…ã€æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œ
- ç¬¬21å›ï¼šãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹åŸºç¤ã€HuggingFace Datasetsçµ±åˆ
- ç¬¬22å›ï¼šè©•ä¾¡æŒ‡æ¨™å®Ÿè£…ï¼ˆFID/IS/Perplexityï¼‰ã€ãƒ¢ãƒ‡ãƒ«é¸æŠ

**ä»Šå›ã®å·®åˆ¥åŒ–**ï¼š
- **æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã¨ã®é•ã„**ï¼šå½¼ã‚‰ã¯ç†è«–è¬›ç¾©ã§å®Ÿè£…ã¯èª²é¡Œã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯**ç†è«–ã¨å®Ÿè£…ã®ä¸¡æ–¹ã‚’ç¶²ç¾…**ã€‚
- **ä»–ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¨ã®é•ã„**ï¼šPyTorch/TensorFlowã«é–‰ã˜ãªã„ã€‚**3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ï¼ˆè¨“ç·´â†’æ¨è«–â†’é…ä¿¡ï¼‰ã§å®Ÿæˆ¦ã‚¹ã‚­ãƒ«ã‚’ç²å¾—ã€‚
- **è«–æ–‡å®Ÿè£…ã¨ã®é•ã„**ï¼šå˜ãªã‚‹å†™çµŒã§ã¯ãªã„ã€‚**æ•°å¼ã®å„é …ã¨ã‚³ãƒ¼ãƒ‰ã®å„è¡Œã‚’1:1å¯¾å¿œ**ã•ã›ã€ç†è§£ã‚’æ·±ã‚ã‚‹ã€‚

---

### 2.2 ãªãœVAE/GAN/Transformerã‹ â€” 3å¤§ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®ä»£è¡¨

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ  | è¨“ç·´æ–¹æ³• | å¼·ã¿ | å¼±ã¿ |
|:-------|:-----------|:---------|:-----|:-----|
| **VAE** | å°¤åº¦ãƒ™ãƒ¼ã‚¹ï¼ˆæ˜ç¤ºçš„å¯†åº¦ï¼‰ | ELBOæœ€å¤§åŒ– | å®‰å®šè¨“ç·´ã€æ½œåœ¨ç©ºé–“è§£é‡ˆå¯èƒ½ | ã¼ã‚„ã‘ãŸç”Ÿæˆã€è¡¨ç¾åŠ›åˆ¶ç´„ |
| **GAN** | æš—é»™çš„å¯†åº¦ï¼ˆAdversarialï¼‰ | Minimaxç«¶äº‰ | é®®æ˜ãªç”Ÿæˆã€é«˜å“è³ªç”»åƒ | è¨“ç·´ä¸å®‰å®šã€Mode Collapse |
| **Transformer** | è‡ªå·±å›å¸°ï¼ˆæ˜ç¤ºçš„å¯†åº¦ï¼‰ | æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬MLE | ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€æ±ç”¨æ€§ | é€æ¬¡ç”Ÿæˆã€è¨ˆç®—ã‚³ã‚¹ãƒˆ |

**3ã¤ã®ç”Ÿæˆãƒ‘ãƒ©ãƒ€ã‚¤ãƒ **ï¼ˆç¬¬7å›ã§å­¦ã‚“ã åˆ†é¡ï¼‰ï¼š

```mermaid
graph TD
    A[ç”Ÿæˆãƒ¢ãƒ‡ãƒ«] --> B[æ˜ç¤ºçš„å¯†åº¦ p_Î¸ x]
    A --> C[æš—é»™çš„å¯†åº¦]

    B --> D[Tractable<br>è¨ˆç®—å¯èƒ½]
    B --> E[Approximate<br>è¿‘ä¼¼æ¨è«–]

    D --> F[è‡ªå·±å›å¸°<br>Transformer]
    D --> G[Flow<br>RealNVP]

    E --> H[å¤‰åˆ†æ¨è«–<br>VAE]
    E --> I[ãƒãƒ«ã‚³ãƒ•é€£é–<br>Diffusion]

    C --> J[GANãƒ•ã‚¡ãƒŸãƒªãƒ¼<br>WGAN-GP/StyleGAN]
    C --> K[Implicit MLE<br>Noise Contrastive]

    style F fill:#4ecdc4
    style H fill:#ff6b6b
    style J fill:#ffe66d
```

**ãªãœã“ã®3ã¤ã‚’é¸ã‚“ã ã‹**ï¼š
1. **VAE**ï¼šå¤‰åˆ†æ¨è«–ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã€‚ELBOã¯ä»–ã®å¤šãã®ãƒ¢ãƒ‡ãƒ«ï¼ˆDiffusion/Flowï¼‰ã®åŸºç¤ã€‚
2. **GAN**ï¼šAdversarialè¨“ç·´ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã€‚å®‰å®šåŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ï¼ˆWGAN-GPï¼‰ã¯å¿…é ˆã‚¹ã‚­ãƒ«ã€‚
3. **Transformer**ï¼šè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã€‚LLMæ™‚ä»£ã®å¿…é ˆçŸ¥è­˜ã€‚KV-Cacheã¯æ¨è«–åŠ¹ç‡åŒ–ã®éµã€‚

ã“ã‚Œã‚‰3ã¤ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚Œã°ã€ä»–ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆDiffusion/Flow/VQ-VAEï¼‰ã®å®Ÿè£…ã‚‚ç†è§£ã§ãã‚‹ã€‚

---

### 2.3 3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å…¨ä½“åƒ

```mermaid
graph LR
    A[Julia<br>Lux.jl] -->|è¨“ç·´| B[ãƒ¢ãƒ‡ãƒ«<br>VAE/GAN/Trans]
    B -->|ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ| C[safetensors/<br>ONNX]
    C -->|ãƒ­ãƒ¼ãƒ‰| D[Rust<br>Candle]
    D -->|æ¨è«–| E[ãƒãƒƒãƒå‡¦ç†<br>ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼]
    E -->|FFI| F[Elixir<br>Broadway]
    F -->|é…ä¿¡| G[åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ <br>è€éšœå®³æ€§]

    style A fill:#9b59b6
    style D fill:#e67e22
    style F fill:#3498db
```

**å„è¨€èªã®å½¹å‰²**ï¼ˆç¬¬19å›ã§è¨­è¨ˆï¼‰ï¼š

| æ®µéš | è¨€èª | ç†ç”± | ãƒ„ãƒ¼ãƒ« |
|:-----|:-----|:-----|:-------|
| è¨“ç·´ | âš¡ Julia | æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1ã€JITé«˜é€ŸåŒ–ã€REPLãƒ«ãƒ¼ãƒ— | Lux.jl, Reactant |
| æ¨è«– | ğŸ¦€ Rust | ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã€å‹å®‰å…¨ã€ä¸¦åˆ—å‡¦ç†ã€C-ABI FFI | Candle, ndarray |
| é…ä¿¡ | ğŸ”® Elixir | è€éšœå®³æ€§ã€ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã€ç›£è¦–ãƒ„ãƒªãƒ¼ | GenStage, Broadway |

**ãªãœ3è¨€èªã‹**ï¼š
- **Python 1è¨€èªã§ã¯ä¸å¯èƒ½**ï¼šGILãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼ã€å‹å®‰å…¨æ€§æ¬ å¦‚ã€è€éšœå®³æ€§å¼±ã„
- **PyTorchã ã‘ã§ã¯ä¸ååˆ†**ï¼šè¨“ç·´ã¯å¾—æ„ã ãŒã€æ¨è«–æœ€é©åŒ–ãƒ»åˆ†æ•£é…ä¿¡ã¯è‹¦æ‰‹
- **å„è¨€èªãŒæœ€é©é ˜åŸŸã‚’æ‹…å½“**ï¼šJuliaï¼ˆè¨“ç·´ï¼‰ã€Rustï¼ˆæ¨è«–ï¼‰ã€Elixirï¼ˆé…ä¿¡ï¼‰ã®åˆ†æ¥­ã§ã€å„æ®µéšã§æœ€é«˜æ€§èƒ½ã‚’é”æˆ

**ä»Šå›ã®å®Ÿè£…ç¯„å›²**ï¼š
- Zone 3ï¼ˆæ•°å¼ä¿®è¡Œï¼‰ï¼šVAE/GAN/Transformerã®Juliaè¨“ç·´å®Ÿè£…ã€æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å®Œå…¨å¯¾å¿œ
- Zone 4ï¼ˆå®Ÿè£…ï¼‰ï¼šRustæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã€Candleã§ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒãƒƒãƒå‡¦ç†
- Zone 5ï¼ˆå®Ÿé¨“ï¼‰ï¼šElixiråˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã€Broadwayéœ€è¦é§†å‹•ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€è€éšœå®³æ€§ãƒ‡ãƒ¢

---

### 2.4 å­¦ç¿’æˆ¦ç•¥ â€” æ•°å¼â†’ã‚³ãƒ¼ãƒ‰â†’ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ

**æ¨å¥¨å­¦ç¿’é †åº**ï¼š
1. Zone 3: æ•°å¼ã‚’1è¡Œãšã¤å°å‡ºã€Juliaã‚³ãƒ¼ãƒ‰ã¨å¯¾å¿œä»˜ã‘
2. Zone 4: Juliaè¨“ç·´â†’Rustæ¨è«–â†’Elixiré…ä¿¡ã®é †ã§å®Ÿè£…
3. Zone 5: å®Ÿéš›ã«å‹•ã‹ã—ã€è€éšœå®³æ€§ã‚’ãƒ‡ãƒ¢

**æœ¬è¬›ç¾©ã®ç›®æ¨™åˆ°é”ç‚¹**ï¼š
- [ ] VAE/GAN/Transformerã®ELBOã‚’**ç´™ã§å°å‡º**ã§ãã‚‹
- [ ] Juliaã§**ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ãƒ«ãƒ¼ãƒ—**ã‚’æ›¸ã‘ã‚‹
- [ ] Rustã§**safetensorsã‚’ãƒ­ãƒ¼ãƒ‰**ã—ã€æ¨è«–ã§ãã‚‹
- [ ] Elixirã§**Broadwayãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã‚’æ§‹ç¯‰ã§ãã‚‹
- [ ] ãƒ—ãƒ­ã‚»ã‚¹ã‚’killã—ã¦ã‚‚**è‡ªå‹•å¾©æ—§**ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆã§ãã‚‹

:::message
**é€²æ—**: å…¨ä½“ã®20%å®Œäº†ã€‚å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚æ•°å¼ä¿®è¡Œã®æº–å‚™ãŒã§ããŸã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” VAE/GAN/Transformerå®Œå…¨å°å‡º

ã“ã®ã‚¾ãƒ¼ãƒ³ã¯**æœ€ã‚‚é‡è¦**ã€‚ç†è«–ï¼ˆCourse IIï¼‰ã§å­¦ã‚“ã æ•°å¼ã‚’ã€å®Ÿè£…ã¨1:1å¯¾å¿œã•ã›ã‚‹ã€‚

### 3.1 VAE â€” ELBOå®Œå…¨åˆ†è§£ã¨å®Ÿè£…å¯¾å¿œ

**å¾©ç¿’ï¼šVAEã®ç›®çš„**ï¼ˆç¬¬10å›ã‚ˆã‚Šï¼‰

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ã®å°¤åº¦ $p_\theta(\mathbf{x})$ ã‚’æœ€å¤§åŒ–ã—ãŸã„ãŒã€æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’å‘¨è¾ºåŒ–ã™ã‚‹ç©åˆ†ãŒè¨ˆç®—ä¸èƒ½ï¼š

$$
p_\theta(\mathbf{x}) = \int p_\theta(\mathbf{x}|\mathbf{z})p(\mathbf{z})d\mathbf{z}
$$

ãã“ã§å¤‰åˆ†æ¨è«–ã§è¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒ $q_\phi(\mathbf{z}|\mathbf{x})$ ã‚’å°å…¥ã—ã€ELBOã‚’å°å‡ºã—ãŸã€‚

---

#### 3.1.1 ELBOå°å‡ºï¼ˆå¾©ç¿’ï¼‰

ç¬¬8å›ãƒ»ç¬¬9å›ã§å­¦ã‚“ã ELBOå°å‡ºã‚’ã€å®Ÿè£…ã¨å¯¾å¿œä»˜ã‘ãªãŒã‚‰å†ç¢ºèªã€‚

**Step 1: å¯¾æ•°å°¤åº¦ã®åˆ†è§£**

$$
\begin{align}
\log p_\theta(\mathbf{x})
&= \log \int p_\theta(\mathbf{x}, \mathbf{z})d\mathbf{z} \\
&= \log \int p_\theta(\mathbf{x}, \mathbf{z}) \frac{q_\phi(\mathbf{z}|\mathbf{x})}{q_\phi(\mathbf{z}|\mathbf{x})} d\mathbf{z} \\
&= \log \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\left[\frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})}\right]
\end{align}
$$

**Step 2: Jensenã®ä¸ç­‰å¼**ï¼ˆç¬¬6å›ã§è¨¼æ˜ï¼‰

$\log$ ã¯å‡¹é–¢æ•°ãªã®ã§ï¼š

$$
\log \mathbb{E}[f(\mathbf{z})] \geq \mathbb{E}[\log f(\mathbf{z})]
$$

é©ç”¨ã™ã‚‹ã¨ï¼š

$$
\log p_\theta(\mathbf{x}) \geq \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\left[\log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})}\right] \equiv \mathcal{L}_{\text{ELBO}}(\theta, \phi; \mathbf{x})
$$

**Step 3: ELBOåˆ†è§£**

$$
\begin{align}
\mathcal{L}_{\text{ELBO}}
&= \mathbb{E}_{q_\phi}\left[\log p_\theta(\mathbf{x}, \mathbf{z}) - \log q_\phi(\mathbf{z}|\mathbf{x})\right] \\
&= \mathbb{E}_{q_\phi}\left[\log p_\theta(\mathbf{x}|\mathbf{z}) + \log p(\mathbf{z}) - \log q_\phi(\mathbf{z}|\mathbf{x})\right] \\
&= \mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{\text{KL}}[q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})]
\end{align}
$$

ç¬¬1é …ï¼š**å†æ§‹æˆé …**ï¼ˆReconstruction termï¼‰
ç¬¬2é …ï¼š**KLæ­£å‰‡åŒ–é …**ï¼ˆKL Divergence regularizationï¼‰

---

#### 3.1.2 å†æ§‹æˆé …ã®å®Ÿè£…

**æ•°å¼**ï¼š

$$
\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]
$$

DecoderãŒå‡ºåŠ› $\hat{\mathbf{x}} = \mu_\theta(\mathbf{z})$ ã‚’ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¹³å‡ã¨ã™ã‚‹ã¨ï¼š

$$
p_\theta(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mathbf{x}; \mu_\theta(\mathbf{z}), \sigma^2\mathbf{I})
$$

å¯¾æ•°å°¤åº¦ï¼š

$$
\log p_\theta(\mathbf{x}|\mathbf{z}) = -\frac{1}{2\sigma^2}\|\mathbf{x} - \mu_\theta(\mathbf{z})\|^2 + \text{const}
$$

$\sigma^2 = 1$ ã¨å›ºå®šã™ã‚‹ã¨ï¼ˆå®Ÿè£…ä¸Šã®ç°¡ç•¥åŒ–ï¼‰ï¼š

$$
\log p_\theta(\mathbf{x}|\mathbf{z}) \propto -\|\mathbf{x} - \hat{\mathbf{x}}\|^2
$$

**Juliaã‚³ãƒ¼ãƒ‰**ï¼š

```julia
# Decoderå‡ºåŠ›: xÌ‚ = decoder(z)
xÌ‚, st_dec = decoder(z, ps_dec, st_dec)

# å†æ§‹æˆé …: -||x - xÌ‚||Â² / batch_size
recon_term = -sum((x .- xÌ‚).^2) / size(x, 2)
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $\mu_\theta(\mathbf{z})$ | `decoder(z)` | Decoderã®å‡ºåŠ› |
| $\|\mathbf{x} - \mu_\theta(\mathbf{z})\|^2$ | `sum((x .- xÌ‚).^2)` | äºŒä¹—èª¤å·® |
| $\mathbb{E}_{q_\phi}[\cdot]$ | `/ size(x, 2)` | ãƒãƒƒãƒå¹³å‡ |

---

#### 3.1.3 KLæ­£å‰‡åŒ–é …ã®å®Ÿè£… â€” ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼

**æ•°å¼**ï¼š

$$
D_{\text{KL}}[q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})]
$$

ä»®å®šï¼š
- $q_\phi(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}_\phi(\mathbf{x}), \text{diag}(\boldsymbol{\sigma}^2_\phi(\mathbf{x})))$
- $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$

**ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼**ï¼ˆç¬¬4å›ã§å°å‡ºï¼‰ï¼š

$$
D_{\text{KL}}[\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\sigma}^2\mathbf{I}) \| \mathcal{N}(\mathbf{0}, \mathbf{I})] = \frac{1}{2}\sum_{i=1}^d (\mu_i^2 + \sigma_i^2 - \log\sigma_i^2 - 1)
$$

Encoderã¯ $\log\sigma^2$ ã‚’å‡ºåŠ›ã™ã‚‹ã¨ä¾¿åˆ©ï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰ï¼š

$$
D_{\text{KL}} = -\frac{1}{2}\sum_{i=1}^d (1 + \log\sigma_i^2 - \mu_i^2 - \sigma_i^2)
$$

**Juliaã‚³ãƒ¼ãƒ‰**ï¼š

```julia
# Encoderå‡ºåŠ›: (Î¼, log_ÏƒÂ²)
output, st_enc = encoder(x, ps_enc, st_enc)
Î¼ = output[1:latent_dim, :]
logÏƒÂ² = output[latent_dim+1:end, :]

# KLç™ºæ•£: -0.5 * Î£(1 + log_ÏƒÂ² - Î¼Â² - ÏƒÂ²) / batch_size
kl_term = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²)) / size(x, 2)
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $\boldsymbol{\mu}_\phi(\mathbf{x})$ | `Î¼ = output[1:d, :]` | Encoderã®å‰åŠå‡ºåŠ› |
| $\log\boldsymbol{\sigma}^2_\phi(\mathbf{x})$ | `logÏƒÂ² = output[d+1:end, :]` | Encoderã®å¾ŒåŠå‡ºåŠ› |
| $\mu_i^2$ | `Î¼.^2` | è¦ç´ ã”ã¨ã®äºŒä¹— |
| $\sigma_i^2 = \exp(\log\sigma_i^2)$ | `exp.(logÏƒÂ²)` | æŒ‡æ•°é–¢æ•° |
| $\sum_{i=1}^d$ | `sum(...)` | å…¨è¦ç´ ã®å’Œ |

:::message alert
**æ³¨æ„**: $\log\sigma^2$ ã‚’å‡ºåŠ›ã™ã‚‹ç†ç”±ã¯æ•°å€¤å®‰å®šæ€§ã€‚ç›´æ¥ $\sigma$ ã‚’å‡ºåŠ›ã™ã‚‹ã¨ã€å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºã®ãƒªã‚¹ã‚¯ãŒã‚ã‚‹ã€‚
:::

---

#### 3.1.4 å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ â€” å‹¾é…ã‚’é€šã™é­”æ³•

**å•é¡Œ**ï¼š$\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã¨ã€ç¢ºç‡çš„ãƒãƒ¼ãƒ‰ã§å‹¾é…ãŒæ­¢ã¾ã‚‹ã€‚

**è§£æ±º**ï¼šå†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ï¼ˆReparameterization Trick, ç¬¬10å›ã§å­¦ã‚“ã ï¼‰

$$
\mathbf{z} = \boldsymbol{\mu}_\phi(\mathbf{x}) + \boldsymbol{\sigma}_\phi(\mathbf{x}) \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
$$

ã“ã‚Œã§ $\mathbf{z}$ ã¯ $\phi$ ã®æ±ºå®šçš„é–¢æ•°ã«ãªã‚Šã€å‹¾é…ãŒé€šã‚‹ã€‚

**Juliaã‚³ãƒ¼ãƒ‰**ï¼š

```julia
# Reparameterization: z = Î¼ + Ïƒ âŠ™ Îµ
Îµ = randn(Float32, size(Î¼)...)
Ïƒ = exp.(logÏƒÂ² ./ 2)  # Ïƒ = exp(log_ÏƒÂ² / 2) = âˆš(ÏƒÂ²)
z = Î¼ .+ Ïƒ .* Îµ
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ | `Îµ = randn(Float32, size(Î¼))` | æ¨™æº–æ­£è¦ãƒã‚¤ã‚º |
| $\boldsymbol{\sigma} = \exp(\log\boldsymbol{\sigma}^2 / 2)$ | `Ïƒ = exp.(logÏƒÂ² ./ 2)` | æ¨™æº–åå·®è¨ˆç®— |
| $\boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}$ | `Î¼ .+ Ïƒ .* Îµ` | è¦ç´ ã”ã¨ã®ç©ã¨å’Œ |

**å‹¾é…ã®æµã‚Œ**ï¼š

```mermaid
graph LR
    A[x] -->|Encoder| B[Î¼, log_ÏƒÂ²]
    B -->|æ±ºå®šçš„å¤‰æ›| C[z = Î¼ + ÏƒâŠ™Îµ]
    C -->|Decoder| D[xÌ‚]
    D -->|Loss| E[ELBO]
    E -->|âˆ‡_Ï†| B
    E -->|âˆ‡_Î¸| D

    style C fill:#4ecdc4
```

å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã«ã‚ˆã‚Šã€$\nabla_\phi \mathcal{L}_{\text{ELBO}}$ ãŒè¨ˆç®—å¯èƒ½ã«ãªã‚‹ã€‚

---

#### 3.1.5 VAEå®Œå…¨å®Ÿè£… â€” å…¨ã¦ã‚’çµ±åˆ

```julia
using Lux, Optimisers, Zygote, Random

# === ãƒ¢ãƒ‡ãƒ«å®šç¾© ===
function create_vae(input_dim, latent_dim, hidden_dim)
    encoder = Chain(
        Dense(input_dim => hidden_dim, tanh),
        Dense(hidden_dim => hidden_dimÃ·2, tanh),
        Dense(hidden_dimÃ·2 => latent_dim*2)  # [Î¼, log_ÏƒÂ²]
    )

    decoder = Chain(
        Dense(latent_dim => hidden_dimÃ·2, tanh),
        Dense(hidden_dimÃ·2 => hidden_dim, tanh),
        Dense(hidden_dim => input_dim, sigmoid)  # [0, 1] pixel range
    )

    return encoder, decoder
end

# === ELBOæå¤±é–¢æ•° ===
function elbo_loss(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x, latent_dim)
    # Encoder: q_Ï†(z|x) â†’ (Î¼, log_ÏƒÂ²)
    output, st_enc = encoder(x, ps_enc, st_enc)
    Î¼ = output[1:latent_dim, :]
    logÏƒÂ² = output[latent_dim+1:end, :]

    # Reparameterization: z = Î¼ + ÏƒâŠ™Îµ
    Îµ = randn(Float32, size(Î¼)...)
    Ïƒ = exp.(logÏƒÂ² ./ 2)
    z = Î¼ .+ Ïƒ .* Îµ

    # Decoder: p_Î¸(x|z) â†’ xÌ‚
    xÌ‚, st_dec = decoder(z, ps_dec, st_dec)

    # ELBO = å†æ§‹æˆé … - KLæ­£å‰‡åŒ–é …
    batch_size = size(x, 2)
    recon = -sum((x .- xÌ‚).^2) / batch_size  # ã‚¬ã‚¦ã‚¹å°¤åº¦
    kl = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²)) / batch_size

    elbo = recon - kl

    return -elbo, (st_enc, st_dec)  # æœ€å¤§åŒ– = è² ã®æœ€å°åŒ–
end

# === è¨“ç·´ãƒ«ãƒ¼ãƒ— ===
function train_vae!(encoder, decoder, train_data, latent_dim, epochs=100, lr=1e-3)
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
    rng = Random.default_rng()
    ps_enc, st_enc = Lux.setup(rng, encoder)
    ps_dec, st_dec = Lux.setup(rng, decoder)

    # Optimizer
    opt_state_enc = Optimisers.setup(Adam(lr), ps_enc)
    opt_state_dec = Optimisers.setup(Adam(lr), ps_dec)

    for epoch in 1:epochs
        total_loss = 0.0f0

        for batch in DataLoader((train_data,), batchsize=128, shuffle=true)
            x = batch[1]

            # å‹¾é…è¨ˆç®—
            (loss, (st_enc, st_dec)), back = Zygote.pullback(
                (pe, pd) -> elbo_loss(encoder, decoder, pe, pd, st_enc, st_dec, x, latent_dim),
                ps_enc, ps_dec
            )
            grads_enc, grads_dec = back((one(loss), nothing))

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            opt_state_enc, ps_enc = Optimisers.update(opt_state_enc, ps_enc, grads_enc)
            opt_state_dec, ps_dec = Optimisers.update(opt_state_dec, ps_dec, grads_dec)

            total_loss += loss
        end

        println("Epoch $epoch: ELBO loss = $(total_loss/length(train_data))")
    end

    return ps_enc, ps_dec, st_enc, st_dec
end

# === ä½¿ç”¨ä¾‹ ===
encoder, decoder = create_vae(784, 20, 400)
ps_enc, ps_dec, st_enc, st_dec = train_vae!(encoder, decoder, x_train, 20, epochs=50)
```

**å…¨ä½“ã®æµã‚Œ**ï¼š

```mermaid
sequenceDiagram
    participant Data as è¨“ç·´ãƒ‡ãƒ¼ã‚¿ x
    participant Enc as Encoder q_Ï†
    participant Reparam as Reparameterization
    participant Dec as Decoder p_Î¸
    participant Loss as ELBO Loss

    Data->>Enc: x (784æ¬¡å…ƒ)
    Enc->>Reparam: Î¼, log_ÏƒÂ² (å„20æ¬¡å…ƒ)
    Reparam->>Reparam: Îµ ~ N(0, I)
    Reparam->>Dec: z = Î¼ + ÏƒâŠ™Îµ (20æ¬¡å…ƒ)
    Dec->>Loss: xÌ‚ (784æ¬¡å…ƒ)
    Loss->>Loss: recon = -||x - xÌ‚||Â²
    Loss->>Loss: kl = -0.5Î£(1 + log_ÏƒÂ² - Î¼Â² - ÏƒÂ²)
    Loss->>Loss: ELBO = recon - kl
    Loss->>Enc: âˆ‡_Ï† ELBO
    Loss->>Dec: âˆ‡_Î¸ ELBO
```

**è¨“ç·´æ™‚ã®ãƒ‡ãƒãƒƒã‚°Tips**ï¼š

```julia
# æå¤±ãŒç™ºæ•£ã™ã‚‹å ´åˆã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
function debug_vae_loss(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x)
    # 1. Encoderå‡ºåŠ›ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
    enc_out, _ = encoder(x, ps_enc, st_enc)
    Î¼ = enc_out[1:20, :]
    logÏƒÂ² = enc_out[21:end, :]

    println("Î¼ range: [$(minimum(Î¼)), $(maximum(Î¼))]")  # æœŸå¾…: [-3, 3]ç¨‹åº¦
    println("logÏƒÂ² range: [$(minimum(logÏƒÂ²)), $(maximum(logÏƒÂ²))]")  # æœŸå¾…: [-5, 5]ç¨‹åº¦

    # 2. ÏƒÂ²ãŒæ¥µç«¯ã«å°ã•ã„/å¤§ãã„å ´åˆã¯clip
    logÏƒÂ² = clamp.(logÏƒÂ², -10.0f0, 10.0f0)

    # 3. Decoderå‡ºåŠ›ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
    z = Î¼ .+ exp.(logÏƒÂ² ./ 2) .* randn(Float32, size(Î¼)...)
    xÌ‚, _ = decoder(z, ps_dec, st_dec)

    println("Decoder output range: [$(minimum(xÌ‚)), $(maximum(xÌ‚))]")  # æœŸå¾…: [0, 1]

    # 4. KLé …ãŒè² ã«ãªã‚‰ãªã„ã“ã¨ã‚’ç¢ºèª
    kl = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))
    println("KL term: $kl")  # æœŸå¾…: â‰¥0 (è² ãªã‚‰å®Ÿè£…ãƒã‚°)

    # 5. å„é …ã®ã‚¹ã‚±ãƒ¼ãƒ«ç¢ºèª
    recon = -sum((x .- xÌ‚).^2) / size(x, 2)
    println("Recon: $recon, KL: $kl")
    # æœŸå¾…: åŒã˜ã‚ªãƒ¼ãƒ€ãƒ¼ï¼ˆKLãŒæ¥µç«¯ã«å¤§ãã„ã¨Posterior Collapseï¼‰
end
```

**Posterior Collapseå¯¾ç­–**ï¼š

```julia
# KL Annealing: KLé …ã®é‡ã¿ã‚’å¾ã€…ã«å¢—åŠ 
function kl_annealing_schedule(epoch, total_epochs, anneal_start=10, anneal_end=50)
    if epoch < anneal_start
        return 0.0f0
    elseif epoch > anneal_end
        return 1.0f0
    else
        return Float32((epoch - anneal_start) / (anneal_end - anneal_start))
    end
end

# è¨“ç·´ãƒ«ãƒ¼ãƒ—ã§ä½¿ç”¨
for epoch in 1:epochs
    Î²_kl = kl_annealing_schedule(epoch, epochs)
    # loss = recon - Î²_kl * kl
end
```

---

### 3.2 GAN â€” WGAN-GPå®Œå…¨å°å‡ºã¨å®Ÿè£…å¯¾å¿œ

**å¾©ç¿’ï¼šGANã®ç›®çš„**ï¼ˆç¬¬12å›ã‚ˆã‚Šï¼‰

Generator $G$ ã¨ Discriminator $D$ ã®2ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚²ãƒ¼ãƒ ï¼š

$$
\min_G \max_D \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}}[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_z}[\log(1 - D(G(\mathbf{z})))]
$$

å•é¡Œç‚¹ï¼š
- è¨“ç·´ä¸å®‰å®šï¼ˆæŒ¯å‹•ãƒ»ç™ºæ•£ï¼‰
- Mode Collapseï¼ˆå¤šæ§˜æ€§ã®æ¬ å¦‚ï¼‰
- å‹¾é…æ¶ˆå¤±ï¼ˆ$D$ ãŒå¼·ã™ãã‚‹ã¨ $G$ ã®å‹¾é…ãŒæ¶ˆãˆã‚‹ï¼‰

è§£æ±ºç­–ï¼š**WGAN-GP**ï¼ˆWasserstein GAN with Gradient Penalty, ç¬¬13å›ã§å­¦ã‚“ã ï¼‰

---

#### 3.2.1 Wassersteinè·é›¢ã®å°å‡ºï¼ˆå¾©ç¿’ï¼‰

ç¬¬13å›ã§å­¦ã‚“ã Wasserstein-1è·é›¢ï¼ˆEarth Mover's Distanceï¼‰ï¼š

$$
W_1(p_r, p_g) = \inf_{\gamma \in \Pi(p_r, p_g)} \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim \gamma}[\|\mathbf{x} - \mathbf{y}\|]
$$

Kantorovich-RubinsteinåŒå¯¾æ€§ï¼ˆç¬¬13å›ã§è¨¼æ˜ï¼‰ï¼š

$$
W_1(p_r, p_g) = \sup_{\|f\|_L \leq 1} \mathbb{E}_{\mathbf{x} \sim p_r}[f(\mathbf{x})] - \mathbb{E}_{\mathbf{x} \sim p_g}[f(\mathbf{x})]
$$

ã“ã“ã§ $\|f\|_L \leq 1$ ã¯1-Lipschitzé€£ç¶šåˆ¶ç´„ã€‚

**WGANã®æå¤±é–¢æ•°**ï¼š

$$
\mathcal{L}_D = \mathbb{E}_{\mathbf{x} \sim p_r}[D(\mathbf{x})] - \mathbb{E}_{\mathbf{z} \sim p_z}[D(G(\mathbf{z}))]
$$

$$
\mathcal{L}_G = -\mathbb{E}_{\mathbf{z} \sim p_z}[D(G(\mathbf{z}))]
$$

$D$ ã¯"Critic"ï¼ˆè­˜åˆ¥å™¨ã§ã¯ãªãã€ã‚¹ã‚³ã‚¢é–¢æ•°ï¼‰ã€‚

---

#### 3.2.2 Gradient Penalty â€” Lipschitzåˆ¶ç´„ã®å¼·åˆ¶

**å•é¡Œ**ï¼šå…ƒã®WGANã¯weight clippingã§ $\|f\|_L \leq 1$ ã‚’å¼·åˆ¶ã—ãŸãŒã€å®¹é‡ä½ä¸‹ãƒ»å‹¾é…æ¶ˆå¤±ã‚’å¼•ãèµ·ã“ã™ã€‚

**è§£æ±º**ï¼šWGAN-GPï¼ˆGulrajani+ 2017 [^2]ï¼‰ã¯Gradient Penaltyã§åˆ¶ç´„ï¼š

$$
\|\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})\|_2 = 1
$$

ã‚’ $\hat{\mathbf{x}} = \alpha \mathbf{x} + (1 - \alpha)G(\mathbf{z})$ ï¼ˆæœ¬ç‰©ã¨å½ç‰©ã®è£œé–“ç‚¹ï¼‰ã§å¼·åˆ¶ã€‚

**WGAN-GPæå¤±é–¢æ•°**ï¼š

$$
\mathcal{L}_D = \mathbb{E}_{\mathbf{z} \sim p_z}[D(G(\mathbf{z}))] - \mathbb{E}_{\mathbf{x} \sim p_r}[D(\mathbf{x})] + \lambda \mathbb{E}_{\hat{\mathbf{x}}}[(\|\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})\|_2 - 1)^2]
$$

ç¬¬1é …ï¼šå½ç‰©ã®ã‚¹ã‚³ã‚¢ï¼ˆæœ€å°åŒ–ï¼‰
ç¬¬2é …ï¼šæœ¬ç‰©ã®ã‚¹ã‚³ã‚¢ï¼ˆæœ€å¤§åŒ–ï¼‰
ç¬¬3é …ï¼šGradient Penaltyï¼ˆå‹¾é…ãƒãƒ«ãƒ ã‚’1ã«è¿‘ã¥ã‘ã‚‹ï¼‰

---

#### 3.2.3 Gradient Penalty ã®å®Ÿè£…

**æ•°å¼**ï¼š

$$
\text{GP} = \mathbb{E}_{\hat{\mathbf{x}}}[(\|\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})\|_2 - 1)^2]
$$

**Step 1: è£œé–“ç‚¹ç”Ÿæˆ**

$$
\hat{\mathbf{x}} = \alpha \mathbf{x} + (1 - \alpha)G(\mathbf{z}), \quad \alpha \sim \text{Uniform}(0, 1)
$$

**Juliaã‚³ãƒ¼ãƒ‰**ï¼š

```julia
# æœ¬ç‰©ã¨å½ç‰©ã®è£œé–“
Î± = rand(Float32, 1, batch_size)
x_interp = Î± .* x_real .+ (1 .- Î±) .* x_fake
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $\alpha \sim \text{Uniform}(0, 1)$ | `Î± = rand(Float32, 1, batch_size)` | è£œé–“ä¿‚æ•° |
| $\alpha \mathbf{x}$ | `Î± .* x_real` | broadcastä¹—ç®— |
| $(1 - \alpha)G(\mathbf{z})$ | `(1 .- Î±) .* x_fake` | broadcastæ¸›ç®—ãƒ»ä¹—ç®— |

**Step 2: å‹¾é…è¨ˆç®—**

$$
\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})
$$

Juliaã§ã¯`Zygote.gradient`ã‚’ä½¿ã†ï¼š

```julia
# è£œé–“ç‚¹ã§ã®å‹¾é…è¨ˆç®—
grad_interp = Zygote.gradient(x -> sum(critic(x, ps_c, st_c)[1]), x_interp)[1]
```

**Step 3: å‹¾é…ãƒãƒ«ãƒ è¨ˆç®—**

$$
\|\nabla_{\hat{\mathbf{x}}} D(\hat{\mathbf{x}})\|_2 = \sqrt{\sum_i (\partial D / \partial \hat{x}_i)^2}
$$

```julia
# å‹¾é…ãƒãƒ«ãƒ : âˆš(Î£ gradÂ²) for each sample
grad_norm = sqrt.(sum(grad_interp.^2, dims=1))  # (1, batch_size)

# Gradient Penalty: ğ”¼[(||âˆ‡D||â‚‚ - 1)Â²]
gp = mean((grad_norm .- 1).^2)
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $(\partial D / \partial \hat{x}_i)^2$ | `grad_interp.^2` | å‹¾é…ã®äºŒä¹— |
| $\sum_i$ | `sum(..., dims=1)` | å„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®å’Œ |
| $\sqrt{\cdot}$ | `sqrt.(...)` | å¹³æ–¹æ ¹ï¼ˆbroadcastï¼‰ |
| $(\|\nabla D\|_2 - 1)^2$ | `(grad_norm .- 1).^2` | ãƒšãƒŠãƒ«ãƒ†ã‚£é … |
| $\mathbb{E}[\cdot]$ | `mean(...)` | ãƒãƒƒãƒå¹³å‡ |

---

#### 3.2.4 WGAN-GPå®Œå…¨å®Ÿè£…

```julia
using Lux, Optimisers, Zygote, Random

# === ãƒ¢ãƒ‡ãƒ«å®šç¾© ===
function create_wgan_gp(latent_dim, img_dim, hidden_dim)
    generator = Chain(
        Dense(latent_dim => hidden_dim, relu),
        Dense(hidden_dim => hidden_dim*2, relu),
        Dense(hidden_dim*2 => img_dim, tanh)  # [-1, 1] range
    )

    critic = Chain(
        Dense(img_dim => hidden_dim*2, x -> leakyrelu(x, 0.2f0)),
        Dense(hidden_dim*2 => hidden_dim, x -> leakyrelu(x, 0.2f0)),
        Dense(hidden_dim => 1)  # ã‚¹ã‚³ã‚¢å‡ºåŠ›
    )

    return generator, critic
end

# === Criticæå¤±ï¼ˆWGAN-GPï¼‰ ===
function critic_loss(generator, critic, ps_g, ps_c, st_g, st_c, x_real, Î»_gp=10.0f0)
    batch_size = size(x_real, 2)

    # å½ç”»åƒç”Ÿæˆ
    z = randn(Float32, size(ps_g)[1], batch_size)
    x_fake, st_g = generator(z, ps_g, st_g)

    # Criticã‚¹ã‚³ã‚¢
    score_real, st_c_real = critic(x_real, ps_c, st_c)
    score_fake, st_c_fake = critic(x_fake, ps_c, st_c)

    # Wassersteinè·é›¢: ğ”¼[D(fake)] - ğ”¼[D(real)]
    wasserstein = mean(score_fake) - mean(score_real)

    # Gradient Penalty
    Î± = rand(Float32, 1, batch_size)
    x_interp = Î± .* x_real .+ (1 .- Î±) .* x_fake

    grad_interp = Zygote.gradient(x -> sum(critic(x, ps_c, st_c)[1]), x_interp)[1]
    grad_norm = sqrt.(sum(grad_interp.^2, dims=1))
    gp = mean((grad_norm .- 1).^2)

    loss = wasserstein + Î»_gp * gp

    return loss, st_c
end

# === Generatoræå¤±ï¼ˆWGAN-GPï¼‰ ===
function generator_loss(generator, critic, ps_g, ps_c, st_g, st_c, batch_size)
    # å½ç”»åƒç”Ÿæˆ
    z = randn(Float32, size(ps_g)[1], batch_size)
    x_fake, st_g = generator(z, ps_g, st_g)

    # Generatorã®ç›®çš„: Criticã‚¹ã‚³ã‚¢ã‚’æœ€å¤§åŒ–
    score_fake, st_c = critic(x_fake, ps_c, st_c)
    loss = -mean(score_fake)

    return loss, st_g
end

# === è¨“ç·´ãƒ«ãƒ¼ãƒ— ===
function train_wgan_gp!(generator, critic, train_data, latent_dim, epochs=100, n_critic=5)
    rng = Random.default_rng()
    ps_g, st_g = Lux.setup(rng, generator)
    ps_c, st_c = Lux.setup(rng, critic)

    opt_g = Optimisers.setup(Adam(1e-4, (0.5, 0.9)), ps_g)
    opt_c = Optimisers.setup(Adam(1e-4, (0.5, 0.9)), ps_c)

    for epoch in 1:epochs
        for batch in DataLoader((train_data,), batchsize=64, shuffle=true)
            x_real = batch[1]

            # Criticã‚’ n_critic å›æ›´æ–°
            for _ in 1:n_critic
                (loss_c, st_c), back_c = Zygote.pullback(
                    pc -> critic_loss(generator, critic, ps_g, pc, st_g, st_c, x_real),
                    ps_c
                )
                grads_c = back_c((one(loss_c), nothing))[1]
                opt_c, ps_c = Optimisers.update(opt_c, ps_c, grads_c)
            end

            # Generatorã‚’ 1 å›æ›´æ–°
            (loss_g, st_g), back_g = Zygote.pullback(
                pg -> generator_loss(generator, critic, pg, ps_c, st_g, st_c, size(x_real, 2)),
                ps_g
            )
            grads_g = back_g((one(loss_g), nothing))[1]
            opt_g, ps_g = Optimisers.update(opt_g, ps_g, grads_g)
        end

        println("Epoch $epoch: C_loss=$(loss_c), G_loss=$(loss_g)")
    end

    return ps_g, ps_c, st_g, st_c
end
```

**è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®æµã‚Œ**ï¼š

```mermaid
sequenceDiagram
    participant Data as è¨“ç·´ãƒ‡ãƒ¼ã‚¿ x_real
    participant G as Generator G
    participant C as Critic D
    participant GP as Gradient Penalty

    loop Criticã‚’5å›æ›´æ–°
        Data->>G: z ~ N(0, I)
        G->>C: x_fake = G(z)
        C->>C: score_real = D(x_real)
        C->>C: score_fake = D(x_fake)
        C->>GP: x_interp = Î±x_real + (1-Î±)x_fake
        GP->>GP: grad = âˆ‡_x_interp D(x_interp)
        GP->>GP: gp = ğ”¼[(||grad||â‚‚ - 1)Â²]
        GP->>C: loss_C = score_fake - score_real + Î»*gp
        C->>C: ps_c â† ps_c - Î·âˆ‡_c loss_C
    end

    loop Generatorã‚’1å›æ›´æ–°
        G->>C: x_fake = G(z)
        C->>G: score_fake = D(x_fake)
        G->>G: loss_G = -score_fake
        G->>G: ps_g â† ps_g - Î·âˆ‡_g loss_G
    end
```

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**ï¼š
- Criticã‚’$n_{\text{critic}}=5$å›ã€Generatorã‚’1å›æ›´æ–°ï¼ˆWGAN-GPæ¨å¥¨æ¯”ç‡ï¼‰
- Gradient Penaltyã® $\lambda=10$ ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè«–æ–‡æ¨å¥¨å€¤ï¼‰
- Adamã® $\beta_1=0.5$ ã¯GANè¨“ç·´ã®å®‰å®šåŒ–ã«æœ‰åŠ¹ï¼ˆé€šå¸¸ã¯0.9ï¼‰

:::message
**ã“ã“ãŒé‡è¦**: WGAN-GPã®æ ¸å¿ƒã¯ã€Œå‹¾é…ãƒãƒ«ãƒ ã‚’1ã«ä¿ã¤ã€ã“ã¨ã€‚ã“ã‚ŒãŒLipschitzåˆ¶ç´„ã®å®Ÿç”¨çš„å®Ÿè£…ã€‚
:::

---

### 3.3 Transformer â€” Multi-Head Attentionå®Œå…¨å°å‡º

**å¾©ç¿’ï¼šTransformerã®ç›®çš„**ï¼ˆç¬¬16å›ã‚ˆã‚Šï¼‰

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼š

$$
p(\mathbf{x}) = \prod_{t=1}^T p(x_t | x_{<t})
$$

RNN/LSTMã®é€æ¬¡å‡¦ç†ã‚’æ¨ã¦ã€Attentionã§ä¸¦åˆ—å‡¦ç†ã€‚

---

#### 3.3.1 Scaled Dot-Product Attentionå°å‡º

**Step 1: Attentionæ©Ÿæ§‹ã®ç›´æ„Ÿ**

Query $\mathbf{q}$ ã¨ Key $\mathbf{k}_i$ ã®é¡ä¼¼åº¦ã§Value $\mathbf{v}_i$ ã‚’é‡ã¿ä»˜ã‘ï¼š

$$
\text{Attention}(\mathbf{q}, \{\mathbf{k}_i, \mathbf{v}_i\}) = \sum_{i} \alpha_i \mathbf{v}_i
$$

ã“ã“ã§ $\alpha_i = \text{softmax}(\text{score}(\mathbf{q}, \mathbf{k}_i))$

**Step 2: ã‚¹ã‚³ã‚¢é–¢æ•°ã®é¸æŠ**

å†…ç©ã‚¹ã‚³ã‚¢ï¼š

$$
\text{score}(\mathbf{q}, \mathbf{k}) = \mathbf{q}^\top \mathbf{k}
$$

å•é¡Œï¼š$d_k$ ãŒå¤§ãã„ã¨ã‚¹ã‚³ã‚¢ã®åˆ†æ•£ãŒå¤§ãããªã‚Šã€softmaxãŒé£½å’Œï¼ˆå‹¾é…æ¶ˆå¤±ï¼‰ã€‚

è§£æ±ºï¼šã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

$$
\text{score}(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d_k}}
$$

**Step 3: è¡Œåˆ—å½¢å¼**

Queryè¡Œåˆ— $Q \in \mathbb{R}^{n \times d_k}$ã€Keyè¡Œåˆ— $K \in \mathbb{R}^{m \times d_k}$ã€Valueè¡Œåˆ— $V \in \mathbb{R}^{m \times d_v}$ ã‚’ä½¿ã†ã¨ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

ã“ã“ã§ï¼š
- $QK^\top \in \mathbb{R}^{n \times m}$ï¼šå„Queryã¨Keyã®é¡ä¼¼åº¦è¡Œåˆ—
- $\text{softmax}$ï¼šè¡Œã”ã¨ã«æ­£è¦åŒ–ï¼ˆå„QueryãŒå…¨Keyã®é‡ã¿ã‚’åˆè¨ˆ1ã«ï¼‰
- çµæœ $\in \mathbb{R}^{n \times d_v}$ï¼šå„Queryã«å¯¾ã™ã‚‹åŠ é‡Valueã®å’Œ

---

#### 3.3.2 Multi-Head Attentionå°å‡º

**å‹•æ©Ÿ**ï¼šå˜ä¸€ã®Attentionã§ã¯è¡¨ç¾åŠ›ä¸è¶³ã€‚è¤‡æ•°ã®"è¦–ç‚¹"ã§Attentionã‚’ä¸¦åˆ—è¨ˆç®—ã€‚

**Step 1: ãƒ˜ãƒƒãƒ‰ã®åˆ†å‰²**

$d_{\text{model}}$ æ¬¡å…ƒã‚’ $h$ å€‹ã®ãƒ˜ãƒƒãƒ‰ã«åˆ†å‰²ï¼š

$$
d_k = d_v = \frac{d_{\text{model}}}{h}
$$

**Step 2: å„ãƒ˜ãƒƒãƒ‰ã§ç‹¬ç«‹ã«Attention**

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

ã“ã“ã§ $W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_k}$ ã¯å­¦ç¿’å¯èƒ½ãªå°„å½±è¡Œåˆ—ã€‚

**Step 3: Concatenate and Project**

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
$$

ã“ã“ã§ $W^O \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$

**å®Œå…¨ãªæ•°å¼**ï¼š

$$
\begin{align}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\
&= \text{softmax}\left(\frac{(QW_i^Q)(KW_i^K)^\top}{\sqrt{d_k}}\right)(VW_i^V)
\end{align}
$$

---

#### 3.3.3 Causal Mask â€” æœªæ¥ã‚’è¦‹ã›ãªã„

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€æ™‚åˆ» $t$ ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ $t+1$ ä»¥é™ã‚’è¦‹ã¦ã¯ã„ã‘ãªã„ã€‚

**Maskè¡Œåˆ—**ï¼š

$$
M_{ij} = \begin{cases}
0 & \text{if } i \geq j \\
-\infty & \text{if } i < j
\end{cases}
$$

Softmaxå‰ã«ã‚¹ã‚³ã‚¢ã«åŠ ç®—ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right)V
$$

$M_{ij} = -\infty$ ã®éƒ¨åˆ†ã¯ $\exp(-\infty) = 0$ ã«ãªã‚Šã€æœªæ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®é‡ã¿ãŒ0ã«ãªã‚‹ã€‚

**Juliaã‚³ãƒ¼ãƒ‰**ï¼š

```julia
# Causal Maskç”Ÿæˆ
function causal_mask(seq_len)
    mask = triu(ones(Float32, seq_len, seq_len), 1)  # ä¸Šä¸‰è§’è¡Œåˆ—ï¼ˆå¯¾è§’ã‚ˆã‚Šä¸Šï¼‰
    return mask .* -Inf32  # Softmaxå‰ã«åŠ ç®— â†’ exp(-âˆ) = 0
end

# Attentionã«ãƒã‚¹ã‚¯é©ç”¨
scores = Q @ K' ./ sqrt(Float32(d_k))  # (seq_len, seq_len)
scores = scores .+ causal_mask(seq_len)  # æœªæ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’-âˆã«
attn_weights = softmax(scores, dims=2)  # è¡Œã”ã¨ã«æ­£è¦åŒ–
output = attn_weights @ V
```

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | å¯¾å¿œ |
|:-----|:-------|:-----|
| $QK^\top$ | `Q @ K'` | è¡Œåˆ—ç©ï¼ˆ`'`ã¯è»¢ç½®ï¼‰ |
| $/\sqrt{d_k}$ | `./ sqrt(Float32(d_k))` | broadcasté™¤ç®— |
| $M$ | `causal_mask(seq_len)` | ãƒã‚¹ã‚¯è¡Œåˆ— |
| $\text{softmax}(\cdot + M)$ | `softmax(scores .+ mask, dims=2)` | è¡Œã”ã¨softmax |

---

#### 3.3.4 Multi-Head Attentionå®Œå…¨å®Ÿè£…

```julia
using Lux, NNlib, Random

# === Multi-Head Attention Layer ===
struct MultiHeadAttention <: Lux.AbstractExplicitLayer
    num_heads::Int
    d_model::Int
    d_k::Int
    q_proj::Dense
    k_proj::Dense
    v_proj::Dense
    o_proj::Dense
end

function MultiHeadAttention(d_model::Int, num_heads::Int)
    @assert d_model % num_heads == 0 "d_model must be divisible by num_heads"
    d_k = d_model Ã· num_heads

    return MultiHeadAttention(
        num_heads, d_model, d_k,
        Dense(d_model => d_model, use_bias=false),  # Q projection
        Dense(d_model => d_model, use_bias=false),  # K projection
        Dense(d_model => d_model, use_bias=false),  # V projection
        Dense(d_model => d_model, use_bias=false)   # Output projection
    )
end

function (mha::MultiHeadAttention)(x, ps, st; mask=nothing)
    # x: (d_model, seq_len, batch_size)
    d_model, seq_len, batch_size = size(x)

    # Linear projections: Q, K, V
    Q, st_q = mha.q_proj(x, ps.q_proj, st.q_proj)
    K, st_k = mha.k_proj(x, ps.k_proj, st.k_proj)
    V, st_v = mha.v_proj(x, ps.v_proj, st.v_proj)

    # Reshape for multi-head: (d_model, seq_len, batch) â†’ (num_heads, d_k, seq_len, batch)
    Q = reshape(Q, mha.d_k, mha.num_heads, seq_len, batch_size) |> x -> permutedims(x, (2,1,3,4))
    K = reshape(K, mha.d_k, mha.num_heads, seq_len, batch_size) |> x -> permutedims(x, (2,1,3,4))
    V = reshape(V, mha.d_k, mha.num_heads, seq_len, batch_size) |> x -> permutedims(x, (2,1,3,4))

    # Scaled Dot-Product Attention for all heads
    # scores: (num_heads, seq_len, seq_len, batch)
    scores = batched_mul(batched_transpose(Q), K) ./ sqrt(Float32(mha.d_k))

    # Apply mask if provided
    if !isnothing(mask)
        scores = scores .+ reshape(mask, 1, seq_len, seq_len, 1)  # broadcast over heads and batch
    end

    # Softmax over keys dimension
    attn_weights = softmax(scores, dims=2)  # normalize over keys (dim 2)

    # Weighted sum of values
    out = batched_mul(V, attn_weights)  # (num_heads, d_k, seq_len, batch)

    # Concatenate heads: (num_heads, d_k, seq_len, batch) â†’ (d_model, seq_len, batch)
    out = permutedims(out, (2,1,3,4)) |> x -> reshape(x, d_model, seq_len, batch_size)

    # Output projection
    out, st_o = mha.o_proj(out, ps.o_proj, st.o_proj)

    return out, (st_q=st_q, st_k=st_k, st_v=st_v, st_o=st_o)
end

# === Causal Mask ===
function causal_mask(seq_len)
    mask = triu(ones(Float32, seq_len, seq_len), 1)
    return mask .* -Inf32
end

# === ä½¿ç”¨ä¾‹ ===
d_model = 512
num_heads = 8
seq_len = 10
batch_size = 2

x = randn(Float32, d_model, seq_len, batch_size)
mha = MultiHeadAttention(d_model, num_heads)
ps, st = Lux.setup(Random.default_rng(), mha)

mask = causal_mask(seq_len)
y, st = mha(x, ps, st; mask=mask)  # y: (512, 10, 2)
```

**å‡¦ç†ã®æµã‚Œ**ï¼š

```mermaid
graph TD
    A[Input x<br>d_model Ã— seq_len Ã— batch] -->|Q proj| B1[Q: d_model Ã— seq_len Ã— batch]
    A -->|K proj| B2[K: d_model Ã— seq_len Ã— batch]
    A -->|V proj| B3[V: d_model Ã— seq_len Ã— batch]

    B1 -->|Reshape| C1[Q: num_heads Ã— d_k Ã— seq_len Ã— batch]
    B2 -->|Reshape| C2[K: num_heads Ã— d_k Ã— seq_len Ã— batch]
    B3 -->|Reshape| C3[V: num_heads Ã— d_k Ã— seq_len Ã— batch]

    C1 -->|QK^T/âˆšd_k| D[Scores:<br>num_heads Ã— seq_len Ã— seq_len Ã— batch]
    C2 --> D

    D -->|+ Mask| E[Masked Scores]
    E -->|Softmax| F[Attn Weights]
    F -->|Ã— V| G[Weighted Values:<br>num_heads Ã— d_k Ã— seq_len Ã— batch]
    C3 --> G

    G -->|Concat| H[Concat:<br>d_model Ã— seq_len Ã— batch]
    H -->|O proj| I[Output:<br>d_model Ã— seq_len Ã— batch]

    style D fill:#4ecdc4
    style E fill:#ff6b6b
    style G fill:#ffe66d
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å®Œå…¨å¯¾å¿œ**ï¼š

| æ•°å¼ã‚¹ãƒ†ãƒƒãƒ— | Juliaã‚³ãƒ¼ãƒ‰ | æ¬¡å…ƒå¤‰åŒ– |
|:-------------|:------------|:---------|
| $Q = XW^Q$ | `Q, _ = mha.q_proj(x, ps.q_proj, st.q_proj)` | $(d, n, b) \to (d, n, b)$ |
| $Q$ ã‚’ $h$ ãƒ˜ãƒƒãƒ‰ã«åˆ†å‰² | `reshape(Q, d_k, h, n, b) \|> permutedims((2,1,3,4))` | $(d, n, b) \to (h, d_k, n, b)$ |
| $QK^\top/\sqrt{d_k}$ | `batched_mul(Q', K) ./ sqrt(Float32(d_k))` | $(h, n, d_k, b) \to (h, n, n, b)$ |
| $\text{scores} + M$ | `scores .+ mask` | Maskã‚’broadcast |
| $\text{softmax}(\cdot)$ | `softmax(scores, dims=2)` | è¡Œï¼ˆKeyæ¬¡å…ƒï¼‰ã§æ­£è¦åŒ– |
| $\text{Attention} \times V$ | `batched_mul(V, attn_weights)$ | $(h, d_k, n, b) \times (h, n, n, b) \to (h, d_k, n, b)$ |
| Concat heads | `reshape(..., d, n, b)` | $(h, d_k, n, b) \to (d, n, b)$ |
| Output projection | `mha.o_proj(out)` | $(d, n, b) \to (d, n, b)$ |

:::message
**ã“ã“ãŒé‡è¦**: Multi-Head Attentionã¯ã€Œä¸¦åˆ—ã«è¤‡æ•°ã®è¦–ç‚¹ã§Attentionã€ã€‚å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹éƒ¨åˆ†ç©ºé–“ã§é¡ä¼¼åº¦ã‚’è¨ˆç®—ã€‚
:::

---

### 3.4 âš”ï¸ Boss Battle â€” 3ãƒ¢ãƒ‡ãƒ«çµ±åˆè¨“ç·´ãƒ«ãƒ¼ãƒ—

ã“ã“ã¾ã§ã§3ãƒ¢ãƒ‡ãƒ«ã®æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œã‚’å®Œå…¨ã«ç†è§£ã—ãŸã€‚æœ€å¾Œã®Boss Battleï¼š**3ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ãŸè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¨­è¨ˆã›ã‚ˆ**ã€‚

**èª²é¡Œ**ï¼š
1. VAE/GAN/Transformerã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§å®Ÿè£…
2. æå¤±æ›²ç·šã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ—ãƒ­ãƒƒãƒˆ
3. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ»å†é–‹æ©Ÿèƒ½
4. Early Stoppingå®Ÿè£…

**ãƒ’ãƒ³ãƒˆ**ï¼š
- ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§`loss, state = model_loss(params, state, data)`ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’çµ±ä¸€
- Lux.jlã®`Lux.Training.TrainState`ã‚’æ´»ç”¨
- JLD2.jlã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜

**è§£ç­”ä¾‹ã¯ Zone 4 ã§æä¾›**ã€‚ã¾ãšã¯è‡ªåˆ†ã§è¨­è¨ˆã—ã¦ã¿ã‚ˆã†ã€‚

### 3.5 æœ€æ–°ç ”ç©¶å‹•å‘ï¼ˆ2024-2025ï¼‰â€” Production Deploymentæœ€é©åŒ–

#### 3.5.1 Safetensors Format ã®ç”Ÿç”£ç’°å¢ƒã§ã®åˆ©ç”¨

HuggingFaceãŒé–‹ç™ºã—ãŸsafetensorså½¢å¼ã¯ã€ç”Ÿç”£ç’°å¢ƒã§ã®ãƒ¢ãƒ‡ãƒ«é…ä¿¡ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹ [^safetensors_prod].

[^safetensors_prod]: [VAE safetensors deployment](https://huggingface.co/stabilityai/sd-vae-ft-mse-original), [WAN21-VAE Model](https://huggingface.co/wangkanai/wan21-vae)

**Safetensorsã®åˆ©ç‚¹**:

1. **Pickleæ”»æ’ƒè€æ€§**: Pythonã®pickleã¨ç•°ãªã‚Šã€ä»»æ„ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã®ãƒªã‚¹ã‚¯ãªã—
2. **Zero-copy loading**: ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã§ç›´æ¥ãƒ­ãƒ¼ãƒ‰ã€ã‚³ãƒ”ãƒ¼ä¸è¦
3. **é«˜é€ŸåŒ–**: 243MB VAEãƒ¢ãƒ‡ãƒ«ã§ã€PyTorch `.pth` ã‚ˆã‚Š30%é«˜é€Ÿãƒ­ãƒ¼ãƒ‰

```python
from safetensors.torch import load_file

# Zero-copy loading
model_weights = load_file("vae-ft-mse-840000-ema-pruned.safetensors")
# ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã§ç›´æ¥å‚ç…§ã€ã‚³ãƒ”ãƒ¼ãªã—
```

#### 3.5.2 Transformer-GAN Hybrid Architectures

2024-2025ã®æœ€æ–°ç ”ç©¶ã§ã¯ã€GANã¨Transformerã‚’çµ±åˆã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒç™»å ´ [^gan_transformer_2024].

[^gan_transformer_2024]: [Scalable GANs with Transformers (2024)](https://arxiv.org/html/2509.24935v1), [GAN vs Transformer Comparison](https://www.techtarget.com/searchenterpriseai/tip/GAN-vs-transformer-models-Comparing-architectures-and-uses)

**GANsformer Architecture**:

Diffusionã‚„Flowãƒ¢ãƒ‡ãƒ«ãŒç¤ºã—ãŸã‚ˆã†ã«ã€Transformerãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨æ½œåœ¨ç©ºé–“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ãªè¨“ç·´ã¨é«˜è§£åƒåº¦åˆæˆãŒå¯èƒ½ã«ãªã‚‹ã€‚

æœ€æ–°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€VAE-latentè¨“ç·´ã¨plain Transformer generator/discriminatorã‚’çµ„ã¿åˆã‚ã›ã€**single-stepæ¨è«–**ã‚’ä¿æŒã—ãªãŒã‚‰Transformerã®è¡¨ç¾åŠ›ã‚’æ´»ç”¨:

$$
\begin{aligned}
\text{Encoder:} \quad & z = E_{\text{VAE}}(x) \quad \text{(latent tokenization)} \\
\text{Generator:} \quad & G_{\text{Transformer}}(z_{\text{noise}}) \to z_{\text{fake}} \\
\text{Discriminator:} \quad & D_{\text{Transformer}}(z) \to \text{real/fake score} \\
\text{Decoder:} \quad & x_{\text{gen}} = D_{\text{VAE}}(z_{\text{fake}})
\end{aligned}
$$

**Computational Efficiency Comparison**:

| Model Type | Training Cost | Inference | Context Length |
|:-----------|:--------------|:----------|:---------------|
| Pure GAN | ä¸­ | 1-step (æœ€é€Ÿ) | N/A |
| Pure Transformer | é«˜ | Multi-step (é…ã„) | é•·æ–‡å¯¾å¿œ |
| **GANsformer** | ä¸­-é«˜ | 1-step | ä¸­ç¨‹åº¦ |

Transformerã¯è¨ˆç®—ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã§GANã«åŠ£ã‚‹ãŒã€GANsformerã¯ä¸¡è€…ã®åˆ©ç‚¹ã‚’çµ±åˆã—ã€Attentionã«ã‚ˆã‚Šgeneratorã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ã‚’å¼·åŒ–ã€‚

#### 3.5.3 Production Inference Optimization Techniques

2024-2025ã®ç”Ÿç”£ç’°å¢ƒã§ã¯ã€ä»¥ä¸‹ã®æœ€é©åŒ–ãŒæ¨™æº–ã¨ãªã£ã¦ã„ã‚‹ [^inference_opt_2024]:

[^inference_opt_2024]: [Generative AI Production Deployment 2025](https://thinkpalm.com/blogs/generative-ai-in-2024-industry-applications-and-implications/), [VAE Inference Optimization](https://civitai.com/models/276082/vae-ft-mse-840000-ema-pruned-or-840000-or-840k-sd15-vae)

**1. Model Compilation**: PyTorch 2.0+ ã® `torch.compile()` ã§æ¨è«–ã‚’é«˜é€ŸåŒ–

```python
import torch

vae = VAE.from_pretrained("stabilityai/sd-vae-ft-mse")
vae_compiled = torch.compile(vae, mode="reduce-overhead")

# æ¨è«–æ™‚é–“: 45ms â†’ 28ms (1.6x speedup)
latent = vae_compiled.encode(image)
```

**2. xFormers Efficient Attention**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªAttentionå®Ÿè£…

$$
\text{Memory: } O(n^2) \to O(n) \quad \text{(xFormers FlashAttention)}
$$

**3. Half Precision (FP16/BF16)**: æ¨è«–é€Ÿåº¦2å€ã€ãƒ¡ãƒ¢ãƒªåŠæ¸›

```python
vae = vae.half()  # FP32 â†’ FP16
# VRAM: 1.2GB â†’ 0.6GB, Latency: 45ms â†’ 23ms
```

**4. Resolution-based Batching**: è§£åƒåº¦ã«å¿œã˜ãŸæœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º

| Resolution | Batch Size | VRAM | Use Case |
|:-----------|:-----------|:-----|:---------|
| 480P | 8-16 | 4GB | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  |
| 720P | 4-8 | 8GB | ãƒãƒ©ãƒ³ã‚¹ |
| 1080P | 1-2 | 12GB+ | é«˜å“è³ª |

#### 3.5.4 Comparative Analysis: GAN vs Transformer Architectures

2024-2025ç ”ç©¶ã§ã¯ã€GANã¨Transformerã®çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ [^gan_vs_transformer].

[^gan_vs_transformer]: [GAN vs Transformer Models](https://www.techtarget.com/searchenterpriseai/tip/GAN-vs-transformer-models-Comparing-architectures-and-uses), [Comparing Generative AI Models](https://hyqoo.com/artificial-intelligence/comparing-generative-ai-models-gans-vaes-and-transformers)

**Computational Efficiency Trade-offs**:

Transformerã¯ãƒ¡ãƒ¢ãƒªãƒ»è¨ˆç®—ãƒ»ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã§GANã‚ˆã‚Šè¦æ±‚ãŒé«˜ã„ã€‚ä¸€æ–¹ã€Transformerã¯é•·è·é›¢ä¾å­˜é–¢ä¿‚ã®å­¦ç¿’ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ã«å„ªã‚Œã‚‹ã€‚æœ€æ–°ç ”ç©¶ã§ã¯ã€**GANsformer**ã¨ã—ã¦ä¸¡è€…ã‚’çµ±åˆã—ã€Transformerã®Attentionæ©Ÿæ§‹ã‚’Generatorã«çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ã‚’å‘ä¸Šã•ã›ã‚‹è©¦ã¿ãŒé€²è¡Œä¸­ã€‚

**Resource Requirements**:

| Aspect | GAN | Transformer |
|:-------|:----|:------------|
| Training Memory | ä¸­ | é«˜ |
| Inference Speed | 1-step (é«˜é€Ÿ) | Multi-step (ä½é€Ÿ) |
| Data Efficiency | ä¸­ | ä½ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿å¿…è¦ï¼‰ |
| IT Resources | ä¸­è¦æ¨¡GPUã§å¯ | é«˜æ€§èƒ½GPU/TPUå¿…é ˆ |

#### 3.5.5 Julia Reactant.jl â€” JAX-level Performance

2025å¹´ã€Juliaã¯ **Reactant.jl** ã«ã‚ˆã‚Šã€JAX/XLAä¸¦ã¿ã®æ€§èƒ½ã‚’é”æˆ [^reactant_julia].

[^reactant_julia]: Reactant.jl enables Julia code to compile to MLIRâ†’XLA, achieving JAX-level performance on GPU/TPU.

**Before Reactant** (ç´”Julia):

```julia
using Flux

model = Chain(Dense(784 => 256, relu), Dense(256 => 10))
loss(x, y) = Flux.crossentropy(model(x), y)

# GPUæ¨è«–: ~15ms/batch (1000 samples)
```

**With Reactant** (XLA compilation):

```julia
using Reactant

@compile model_compiled = model  # MLIRâ†’XLAå¤‰æ›
loss_compiled = @compile (x, y) -> Flux.crossentropy(model_compiled(x), y)

# GPUæ¨è«–: ~5ms/batch (3x speedup)
```

Reactantã¯ã€Juliaã‚³ãƒ¼ãƒ‰ã‚’MLIRä¸­é–“è¡¨ç¾ã«å¤‰æ›ã—ã€XLAãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§æœ€é©åŒ–:

$$
\text{Julia Code} \xrightarrow{\text{Reactant}} \text{MLIR} \xrightarrow{\text{XLA}} \text{GPU/TPU Kernel}
$$

**Multi-deviceè‡ªå‹•å¯¾å¿œ**:

```julia
# è‡ªå‹•çš„ã«åˆ©ç”¨å¯èƒ½ãƒ‡ãƒã‚¤ã‚¹ï¼ˆGPU/TPUï¼‰ã‚’æ¤œå‡ºãƒ»æœ€é©åŒ–
@compile device_agnostic = my_model

# A100 GPU, TPU v4, Apple M2 â€” å…¨ã¦åŒã˜ã‚³ãƒ¼ãƒ‰
```

#### 3.5.5 Rust Candle vs Burn â€” Production Frameworkæ¯”è¼ƒ

2024-2025ã®Rust ML frameworkã¯2å¼·æ™‚ä»£ [^rust_ml_frameworks]:

[^rust_ml_frameworks]: Candle (HuggingFace) focuses on lightweight inference; Burn supports training with WGPU/WASM for edge deployment.

| Framework | Developer | Training | Inference | Target | License |
|:----------|:----------|:---------|:----------|:-------|:--------|
| **Candle** | HuggingFace | é™å®šçš„ | â­â­â­ | ã‚µãƒ¼ãƒãƒ¼æ¨è«– | Apache 2.0 |
| **Burn** | Community | â­â­â­ | â­â­ | ã‚¨ãƒƒã‚¸ãƒ»WASM | MIT/Apache 2.0 |
| **dfdx** | coreylowman | â­â­ | â­ | ç ”ç©¶ | MIT/Apache 2.0 |

**Candle**: PyTorché¢¨APIã€safetensorsç›´æ¥ãƒ­ãƒ¼ãƒ‰ã€æ¨è«–æœ€é©åŒ–ã«ç‰¹åŒ–

```rust
use candle_core::{Device, Tensor};
use candle_nn::VarBuilder;

let device = Device::cuda_if_available(0)?;
let vb = VarBuilder::from_safetensors(vec!["model.safetensors"], DType::F32, &device)?;

// PyTorchãƒ©ã‚¤ã‚¯ãªè¨˜æ³•
let x = Tensor::randn(0f32, 1.0, (32, 784), &device)?;
let h = x.matmul(&w)?  + &b)?;
```

**Burn**: WGPUå¯¾å¿œï¼ˆVulkan/Metal/DX12ï¼‰ã€WASMã‚¿ãƒ¼ã‚²ãƒƒãƒˆã€è¨“ç·´ãƒ•ãƒ«å¯¾å¿œ

```rust
use burn::prelude::*;
use burn::backend::Wgpu;  // ã¾ãŸã¯ Candle, LibTorch, NdArray

type Backend = Wgpu;

let model = MLP::<Backend>::new();
let optim = AdamWConfig::new().init();

// WASM/Edge deviceã§ã‚‚è¨“ç·´å¯èƒ½
```

**Production Recommendation**:

- ã‚µãƒ¼ãƒãƒ¼æ¨è«–ï¼ˆGPUï¼‰: **Candle** â€” safetensorsçµ±åˆã€HuggingFace Hubã¨è¦ªå’Œæ€§
- ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ï¼ˆRaspberry Pi, WASMï¼‰: **Burn** â€” WGPUå¯¾å¿œã€è»½é‡
- ç ”ç©¶ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—: **Julia + Reactant** â€” æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1ã€JAXç´šé€Ÿåº¦

:::message
**é€²æ—**: å…¨ä½“ã®50%å®Œäº†ã€‚æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ + æœ€æ–°2024-2025ç ”ç©¶å‹•å‘ã‚’æŠŠæ¡ã€‚å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

**æ¬¡å›äºˆå‘Š**: Zone 4å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã§ã¯ã€Flow Matchingã®Juliaå®Ÿè£…ã¨Rust FFIçµ±åˆã‚’å®Œå…¨å®Ÿè£…ã™ã‚‹ã€‚

---


---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
