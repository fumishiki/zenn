---
title: "ç¬¬34å›: ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨""
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning"]
published: true
slug: "ml-lecture-34-part2"
---
## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Juliaå®Ÿè£…ã§RBM + Modern Hopfield + MCMC

### 4.1 ç’°å¢ƒæ§‹ç¯‰

```julia
using Pkg
Pkg.add(["Lux", "Random", "Statistics", "Plots", "Distributions", "LinearAlgebra"])

using Lux, Random, Statistics, Plots, Distributions, LinearAlgebra
```

### 4.2 RBMå®Ÿè£…

#### 4.2.1 RBMãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```julia
# RBMãƒ¢ãƒ‡ãƒ«å®šç¾©
# T: å‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆFloat32 or Float64ï¼‰
struct RBM{T}
    W::Matrix{T}  # é‡ã¿è¡Œåˆ— (n_visible Ã— n_hidden)
                   # æ•°å¼: W_{ij} â€” å¯è¦–å±¤ i ã¨éš ã‚Œå±¤ j ã®æ¥ç¶šå¼·åº¦
    b::Vector{T}  # å¯è¦–å±¤ãƒã‚¤ã‚¢ã‚¹ (n_visible,)
                   # æ•°å¼: b_i â€” å¯è¦–å±¤ãƒãƒ¼ãƒ‰ i ã®ãƒã‚¤ã‚¢ã‚¹
    c::Vector{T}  # éš ã‚Œå±¤ãƒã‚¤ã‚¢ã‚¹ (n_hidden,)
                   # æ•°å¼: c_j â€” éš ã‚Œå±¤ãƒãƒ¼ãƒ‰ j ã®ãƒã‚¤ã‚¢ã‚¹
end

# RBMåˆæœŸåŒ–é–¢æ•°
function RBM(n_visible::Int, n_hidden::Int; T=Float32)
    rng = Random.default_rng()
    # é‡ã¿ã‚’å°ã•ãªãƒ©ãƒ³ãƒ€ãƒ å€¤ã§åˆæœŸåŒ–
    # ç†ç”±: å¤§ããªåˆæœŸå€¤ã¯å­¦ç¿’ã‚’ä¸å®‰å®šã«ã™ã‚‹
    W = randn(rng, T, n_visible, n_hidden) .* T(0.01)
    # ãƒã‚¤ã‚¢ã‚¹ã¯0åˆæœŸåŒ–ï¼ˆæ¨™æº–çš„ãªæ…£ç¿’ï¼‰
    b = zeros(T, n_visible)
    c = zeros(T, n_hidden)
    RBM(W, b, c)
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:
- `W[i, j]` â†” $W_{ij}$
- `b[i]` â†” $b_i$
- `c[j]` â†” $c_j$

#### 4.2.2 ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°

```julia
# ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° E(v, h) = -v'Wh - b'v - c'h
function energy(rbm::RBM, v, h)
    # æ•°å¼: E(v, h) = -v^T W h - b^T v - c^T h
    # v: å¯è¦–å±¤ã®çŠ¶æ…‹ (n_visible,) or (n_visible, batch)
    # h: éš ã‚Œå±¤ã®çŠ¶æ…‹ (n_hidden,) or (n_hidden, batch)

    # ç¬¬1é …: -v^T W h
    term1 = v' * rbm.W * h
    # ç¬¬2é …: -b^T v
    term2 = rbm.b' * v
    # ç¬¬3é …: -c^T h
    term3 = rbm.c' * h

    # å…¨ã¦ã‚’åˆè¨ˆã—ã¦ç¬¦å·åè»¢
    return -(term1 + term2 + term3)
end
```

**æ•°å¼ç¢ºèª**:

$$
E(v, h) = -\sum_{i,j} W_{ij} v_i h_j - \sum_i b_i v_i - \sum_j c_j h_j
$$

$$
= -v^\top W h - b^\top v - c^\top h
$$

#### 4.2.3 æ¡ä»¶ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```julia
# æ¡ä»¶ä»˜ãç¢ºç‡ p(h_j = 1 | v) = Ïƒ(c_j + Î£_i W_ij v_i)
function sample_h_given_v(rbm::RBM, v)
    # æ•°å¼: p(h_j = 1 | v) = Ïƒ(c_j + Î£_i W_ij v_i)
    #                      = Ïƒ(c_j + (W^T v)_j)

    # ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—: c + W' * v
    # W' ã¯ W ã®è»¢ç½® (n_hidden Ã— n_visible)
    # v ã¯ (n_visible, batch)
    # çµæœã¯ (n_hidden, batch)
    logits = rbm.c .+ rbm.W' * v

    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°é©ç”¨ â†’ ç¢ºç‡
    h_prob = sigmoid.(logits)

    # Bernoulliåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    # å„ h_j ã‚’ç¢ºç‡ h_prob[j] ã§ 1ã€ç¢ºç‡ 1-h_prob[j] ã§ 0
    h_sample = rand.(Bernoulli.(h_prob))

    return h_sample, h_prob
end

# æ¡ä»¶ä»˜ãç¢ºç‡ p(v_i = 1 | h) = Ïƒ(b_i + Î£_j W_ij h_j)
function sample_v_given_h(rbm::RBM, h)
    # æ•°å¼: p(v_i = 1 | h) = Ïƒ(b_i + Î£_j W_ij h_j)
    #                      = Ïƒ(b_i + (W h)_i)

    # ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—: b + W * h
    logits = rbm.b .+ rbm.W * h

    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°é©ç”¨
    v_prob = sigmoid.(logits)

    # Bernoulliåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    v_sample = rand.(Bernoulli.(v_prob))

    return v_sample, v_prob
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ç¢ºèª**:

| æ•°å¼ | Juliaå®Ÿè£… |
|:-----|:----------|
| $p(h_j=1\|v) = \sigma(c_j + \sum_i W_{ij} v_i)$ | `sigmoid.(rbm.c .+ rbm.W' * v)` |
| $p(v_i=1\|h) = \sigma(b_i + \sum_j W_{ij} h_j)$ | `sigmoid.(rbm.b .+ rbm.W * h)` |

**Broadcastæ¼”ç®—ã®å¨åŠ›**:

Juliaã® `.` (broadcast) ã«ã‚ˆã‚Šã€ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ãŒè‡ªå‹•ã§ãƒãƒƒãƒå‡¦ç†ã«æ‹¡å¼µã•ã‚Œã‚‹ã€‚

```julia
# å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«: v ã¯ (n_visible,)
h_prob = sigmoid.(rbm.c .+ rbm.W' * v)  # (n_hidden,)

# ãƒãƒƒãƒ: v ã¯ (n_visible, batch_size)
h_prob = sigmoid.(rbm.c .+ rbm.W' * v)  # (n_hidden, batch_size)
# rbm.c ã¯è‡ªå‹•ã§ (n_hidden, 1) â†’ (n_hidden, batch_size) ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
```

#### 4.2.4 Gibbs Sampling

```julia
# Gibbs Sampling (1 step)
function gibbs_step(rbm::RBM, v)
    # 1. h ã‚’ã‚µãƒ³ãƒ—ãƒ«: h ~ p(h | v)
    h, h_prob = sample_h_given_v(rbm, v)

    # 2. v ã‚’ã‚µãƒ³ãƒ—ãƒ«: v_new ~ p(v | h)
    v_new, v_prob = sample_v_given_h(rbm, h)

    # æˆ»ã‚Šå€¤:
    # v_new: æ–°ã—ã„å¯è¦–å±¤ã®çŠ¶æ…‹
    # h: ã‚µãƒ³ãƒ—ãƒ«ã•ã‚ŒãŸéš ã‚Œå±¤
    # v_prob: p(v_new | h) ã®ç¢ºç‡
    # h_prob: p(h | v) ã®ç¢ºç‡
    return v_new, h, v_prob, h_prob
end
```

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç¢ºèª**:

Gibbs Samplingã¯ä»¥ä¸‹ã‚’äº¤äº’ã«å®Ÿè¡Œ:
1. $h^{(t)} \sim p(h | v^{(t)})$
2. $v^{(t+1)} \sim p(v | h^{(t)})$

ã“ã‚Œã‚’ç¹°ã‚Šè¿”ã™ã¨ã€$p(v, h)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¾—ã‚‰ã‚Œã‚‹ï¼ˆã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰æ€§ï¼‰ã€‚

#### 4.2.5 Contrastive Divergence (CD-k)

```julia
# Contrastive Divergence (CD-k)
function cd_k(rbm::RBM, v_data; k=1, lr=0.01f0)
    # v_data: ãƒ‡ãƒ¼ã‚¿ã®ãƒŸãƒ‹ãƒãƒƒãƒ (n_visible, batch_size)
    batch_size = size(v_data, 2)

    # ========== æ­£ä¾‹ï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰ã®çµ±è¨ˆé‡ ==========
    # æ•°å¼: âŸ¨v_i h_jâŸ©_data = (1/N) Î£_n v_i^(n) p(h_j=1 | v^(n))
    h_pos, h_pos_prob = sample_h_given_v(rbm, v_data)

    # æ­£ä¾‹ã®å‹¾é…: v_data * h_pos_prob^T / batch_size
    # v_data: (n_visible, batch)
    # h_pos_prob^T: (batch, n_hidden)
    # çµæœ: (n_visible, n_hidden)
    pos_grad = v_data * h_pos_prob' ./ batch_size

    # ========== è² ä¾‹ï¼ˆãƒ¢ãƒ‡ãƒ«ï¼‰ã®çµ±è¨ˆé‡ ==========
    # k-step Gibbs Sampling
    v_neg = copy(v_data)  # ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆæœŸåŒ–ï¼ˆCD-kã®ç‰¹å¾´ï¼‰
    for _ in 1:k
        v_neg, h_neg, _, _ = gibbs_step(rbm, v_neg)
    end

    # è² ä¾‹ã®éš ã‚Œå±¤ç¢ºç‡
    h_neg, h_neg_prob = sample_h_given_v(rbm, v_neg)

    # è² ä¾‹ã®å‹¾é…
    neg_grad = v_neg * h_neg_prob' ./ batch_size

    # ========== å‹¾é…æ›´æ–° ==========
    # æ•°å¼: Î”W_ij = Î· (âŸ¨v_i h_jâŸ©_data - âŸ¨v_i h_jâŸ©_model)
    Î”W = lr .* (pos_grad .- neg_grad)

    # ãƒã‚¤ã‚¢ã‚¹ã®å‹¾é…
    # æ•°å¼: Î”b_i = Î· (âŸ¨v_iâŸ©_data - âŸ¨v_iâŸ©_model)
    Î”b = lr .* mean(v_data .- v_neg, dims=2)[:]

    # æ•°å¼: Î”c_j = Î· (âŸ¨h_jâŸ©_data - âŸ¨h_jâŸ©_model)
    Î”c = lr .* mean(h_pos_prob .- h_neg_prob, dims=2)[:]

    # æ–°ã—ã„RBMã‚’è¿”ã™ï¼ˆé–¢æ•°å‹ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
    return RBM(rbm.W .+ Î”W, rbm.b .+ Î”b, rbm.c .+ Î”c)
end
```

**CD-kã®ç†è«–**:

å®Œå…¨ãªå‹¾é…:

$$
\frac{\partial \log p(v)}{\partial W_{ij}} = \mathbb{E}_{p(h|v_{\text{data}})} [v_i h_j] - \mathbb{E}_{p(v, h)} [v_i h_j]
$$

- **ç¬¬1é …**: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®—å¯èƒ½ï¼ˆé«˜é€Ÿï¼‰
- **ç¬¬2é …**: $p(v, h)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ï¼ˆå›°é›£ï¼‰

CD-kè¿‘ä¼¼:

$$
\mathbb{E}_{p(v, h)} [v_i h_j] \approx \mathbb{E}_{p(v^{(k)}, h^{(k)})} [v_i h_j]
$$

ã“ã“ã§ $v^{(k)}$ ã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ $k$ stepã®Gibbs Samplingã€‚

**k=1ã®æ„å‘³**:
- 1å›ã ã‘Gibbs â†’ è² ä¾‹ã¯ãƒ‡ãƒ¼ã‚¿è¿‘å‚
- ç†è«–çš„ã«ã¯ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š
- å®Ÿç”¨ä¸Šã¯ååˆ†æ©Ÿèƒ½ï¼ˆHinton 2002ï¼‰

#### 4.2.6 RBMè¨“ç·´ãƒ«ãƒ¼ãƒ—

```julia
# RBMè¨“ç·´ãƒ«ãƒ¼ãƒ—
function train_rbm(rbm, data; epochs=10, k=1, lr=0.01f0, batch_size=32)
    # data: å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ (n_visible, n_samples)
    n_samples = size(data, 2)

    for epoch in 1:epochs
        # ãƒŸãƒ‹ãƒãƒƒãƒã‚·ãƒ£ãƒƒãƒ•ãƒ«
        indices = shuffle(1:n_samples)

        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’1å›èµ°æŸ»ï¼ˆ1 epochï¼‰
        for i in 1:batch_size:n_samples
            # ãƒŸãƒ‹ãƒãƒƒãƒæŠ½å‡º
            batch_idx = indices[i:min(i+batch_size-1, n_samples)]
            batch = data[:, batch_idx]

            # CD-kæ›´æ–°
            rbm = cd_k(rbm, batch; k=k, lr=lr)
        end

        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®è©•ä¾¡
        # ãƒ©ãƒ³ãƒ€ãƒ ãªã‚µãƒ³ãƒ—ãƒ«ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—
        v_sample = data[:, rand(1:n_samples)]
        h_sample, _ = sample_h_given_v(rbm, v_sample)
        E = energy(rbm, v_sample, h_sample)

        println("Epoch $epoch: Energy = $E")
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä¸‹ãŒã‚‹ â†’ å­¦ç¿’ãŒé€²ã‚“ã§ã„ã‚‹
    end

    return rbm
end
```

**è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®è¨­è¨ˆãƒã‚¤ãƒ³ãƒˆ**:

1. **Epoch**: å…¨ãƒ‡ãƒ¼ã‚¿ã‚’1å›èµ°æŸ»
2. **Shuffle**: æ¯epochã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ« â†’ SGDã®ãƒ©ãƒ³ãƒ€ãƒ æ€§
3. **Minibatch**: ãƒŸãƒ‹ãƒãƒƒãƒå˜ä½ã§æ›´æ–° â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ + ä¸¦åˆ—åŒ–
4. **è©•ä¾¡**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ç›£è¦– â†’ å­¦ç¿’ã®åæŸç¢ºèª

**ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®è§£é‡ˆ**:

- ã‚¨ãƒãƒ«ã‚®ãƒ¼ä½ã„ â†’ ãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºç‡ãŒé«˜ã„
- è¨“ç·´ãŒé€²ã‚€ã¨ã€ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä¸‹ãŒã‚‹ â†’ ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«é©åˆ
```

### 4.3 Modern Hopfieldå®Ÿè£…

#### 4.3.1 Modern Hopfieldãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```julia
# Modern Hopfield Network
# T: å‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆFloat32 or Float64ï¼‰
struct ModernHopfield{T}
    X::Matrix{T}  # è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³è¡Œåˆ— (d Ã— M)
                   # X = [Î¾Â¹, Î¾Â², ..., Î¾á´¹]
                   # d: ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¬¡å…ƒ
                   # M: è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
    Î²::T  # é€†æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆÎ² > 0ï¼‰
          # Î²å¤§ â†’ é‹­ã„æ¤œç´¢ï¼ˆæœ€è¿‘æ¥ã®ã¿ï¼‰
          # Î²å° â†’ å¹³æ»‘ãªæ¤œç´¢ï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ··åˆï¼‰
end

# ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
function ModernHopfield(patterns::Matrix{T}; Î²=1.0f0) where T
    # patterns: è¨˜æ†¶ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¡Œåˆ— (d Ã— M)
    ModernHopfield(patterns, T(Î²))
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:
- `X[:, i]` â†” $\xi^i$ ï¼ˆç¬¬ $i$ ç•ªç›®ã®è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
- `Î²` â†” $\beta$ ï¼ˆé€†æ¸©åº¦ï¼‰

#### 4.3.2 ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°

```julia
# ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° E(x) = -lse(Î² X'x) + 0.5||x||^2
function energy(hopfield::ModernHopfield, x)
    # æ•°å¼: E(x) = -log Î£_i exp(Î² âŸ¨x, Î¾^iâŸ©) + (1/2)||x||^2

    # ã‚¹ãƒ†ãƒƒãƒ—1: å†…ç©è¨ˆç®— X' * x
    # X: (d Ã— M)
    # x: (d,) ã¾ãŸã¯ (d, batch)
    # X' * x: (M,) ã¾ãŸã¯ (M, batch)
    # ã“ã‚Œã¯ âŸ¨x, Î¾^iâŸ© ã‚’å…¨ã¦ã® i ã«ã¤ã„ã¦è¨ˆç®—
    inner_products = hopfield.X' * x

    # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° Î² âŸ¨x, Î¾^iâŸ©
    logits = hopfield.Î² .* inner_products

    # ã‚¹ãƒ†ãƒƒãƒ—3: log-sum-exp(logits)
    # lse(z) = log Î£_i exp(z_i)
    # æ•°å€¤å®‰å®šç‰ˆã®å®Ÿè£…ï¼ˆmax-trickä½¿ç”¨ï¼‰
    lse_term = logsumexp(logits)

    # ã‚¹ãƒ†ãƒƒãƒ—4: æ­£å‰‡åŒ–é … (1/2)||x||^2
    reg_term = 0.5f0 * sum(abs2, x)

    # å…¨ä½“ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼
    return -lse_term + reg_term
end
```

**log-sum-expã®æ•°å€¤å®‰å®šæ€§**:

$$
\text{lse}(z) = \log \sum_i \exp(z_i)
$$

Naiveå®Ÿè£…: $\exp(z_i)$ ãŒå¤§ãã„ã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼

å®‰å®šç‰ˆï¼ˆmax-trickï¼‰:

$$
\text{lse}(z) = \max(z) + \log \sum_i \exp(z_i - \max(z))
$$

Juliaã® `logsumexp` ã¯è‡ªå‹•ã§å®‰å®šç‰ˆã‚’ä½¿ç”¨ã€‚

**ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– = ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢**:

$E(x)$ ã‚’æœ€å°åŒ–ã™ã‚‹ $x$ ã¯ã€è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ $\{\xi^i\}$ ã®ä¸­ã§æœ€ã‚‚è¿‘ã„ã‚‚ã®ã«å¯¾å¿œã€‚

#### 4.3.3 Update Rule

```julia
# Update Rule: x^{t+1} = X softmax(Î² X'x^t)
function update(hopfield::ModernHopfield, x)
    # æ•°å¼: x^{t+1} = Î£_i softmax_i(Î² X'x^t) Î¾^i
    #              = X softmax(Î² X'x^t)

    # ã‚¹ãƒ†ãƒƒãƒ—1: å†…ç©è¨ˆç®—
    inner_products = hopfield.X' * x  # (M,) or (M, batch)

    # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° + softmax
    logits = hopfield.Î² .* inner_products
    weights = softmax(logits)  # (M,) or (M, batch)

    # ã‚¹ãƒ†ãƒƒãƒ—3: é‡ã¿ä»˜ãå’Œ
    # X: (d Ã— M)
    # weights: (M,) or (M, batch)
    # X * weights: (d,) or (d, batch)
    return hopfield.X * weights
end
```

**æ•°å¼ç¢ºèª**:

$$
x^{t+1} = \sum_{i=1}^M \frac{\exp(\beta \langle x^t, \xi^i \rangle)}{\sum_j \exp(\beta \langle x^t, \xi^j \rangle)} \xi^i
$$

$$
= \sum_{i=1}^M \text{softmax}_i(\beta X^\top x^t) \xi^i
$$

$$
= X \cdot \text{softmax}(\beta X^\top x^t)
$$

**Softmaxã®å½¹å‰²**:

- $\beta$ å¤§ â†’ softmaxé‹­ã„ â†’ æœ€è¿‘æ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿é¸æŠ
- $\beta$ å° â†’ softmaxå¹³å¦ â†’ è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ··åˆ

#### 4.3.4 åæŸåˆ¤å®šä»˜ãRetrieve

```julia
# åæŸã¾ã§update
function retrieve(hopfield::ModernHopfield, x_init; max_iters=10, tol=1e-6)
    # x_init: åˆæœŸã‚¯ã‚¨ãƒªï¼ˆãƒã‚¤ã‚ºä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³ãªã©ï¼‰
    # max_iters: æœ€å¤§åå¾©æ•°
    # tol: åæŸåˆ¤å®šã®é–¾å€¤

    x = copy(x_init)

    for t in 1:max_iters
        # 1ã‚¹ãƒ†ãƒƒãƒ—æ›´æ–°
        x_new = update(hopfield, x)

        # åæŸåˆ¤å®š: ||x_new - x|| < tol
        if norm(x_new - x) < tol
            println("Converged at iteration $t")
            break
        end

        # æ¬¡ã®åå¾©ã¸
        x = x_new
    end

    return x
end
```

**åæŸæ€§ã®ç†è«–**:

Modern Hopfieldã®å®šç†ï¼ˆRamsauer+ 2020ï¼‰:
- **1å›æ›´æ–°ã§åæŸ**: $\beta = d$ ã®ã¨ãã€1å›ã®æ›´æ–°ã§æœ€è¿‘æ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åæŸ
- **æŒ‡æ•°çš„ç²¾åº¦**: æ¤œç´¢èª¤å·® $\|x^* - \xi^{\mu^*}\| \lesssim \exp(-d)$

å®Ÿè£…ã§ã¯å®‰å…¨ã®ãŸã‚ `max_iters=10` è¨­å®šã€ã ãŒé€šå¸¸1-2å›ã§åæŸã€‚

#### 4.3.5 Attentionç­‰ä¾¡æ€§ã®å®Ÿè¨¼

```julia
# Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Ÿè¨¼
function attention_equivalent(hopfield::ModernHopfield, x_query)
    # Self-Attention: Attention(Q, K, V) = V softmax(K^T Q / âˆšd)
    # Modern Hopfield: x^{t+1} = X softmax(Î² X^T x^t)

    # å¯¾å¿œé–¢ä¿‚:
    # Q = x_query ï¼ˆã‚¯ã‚¨ãƒªï¼‰
    # K = X ï¼ˆã‚­ãƒ¼ = è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    # V = X ï¼ˆãƒãƒªãƒ¥ãƒ¼ = è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    # Î² = 1/âˆšd ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼‰

    d = size(hopfield.X, 1)  # æ¬¡å…ƒ

    # Attentionè¨ˆç®—
    # logits = K^T Q / âˆšd = X^T x_query / âˆšd
    logits = (hopfield.X' * x_query) ./ sqrt(d)

    # Softmax
    weights = softmax(logits)

    # é‡ã¿ä»˜ãå’Œ: V * weights = X * weights
    return hopfield.X * weights
end
```

**ç­‰ä¾¡æ€§ã®ç¢ºèª**:

Modern Hopfieldã§ $\beta = 1/\sqrt{d}$ ã¨ã™ã‚‹ã¨:

$$
x^{t+1} = X \cdot \text{softmax}\left(\frac{X^\top x^t}{\sqrt{d}}\right)
$$

ã“ã‚Œã¯ Self-Attention:

$$
\text{Attention}(Q, K, V) = V \cdot \text{softmax}\left(\frac{K^\top Q}{\sqrt{d}}\right)
$$

ã¨å®Œå…¨ã«ä¸€è‡´ï¼ˆ$Q = x^t$ã€$K = V = X$ï¼‰ã€‚

**ã‚³ãƒ¼ãƒ‰å®Ÿé¨“**:

```julia
# å®Ÿé¨“: Modern Hopfield vs Attention
d, M = 20, 10
patterns = randn(Float32, d, M)
x_query = randn(Float32, d)

hopfield = ModernHopfield(patterns; Î²=1.0f0/sqrt(d))

# Modern Hopfieldæ›´æ–°
x_hopfield = update(hopfield, x_query)

# Attentionç­‰ä¾¡è¨ˆç®—
x_attention = attention_equivalent(hopfield, x_query)

# å·®ã®ç¢ºèª
println("Difference: $(norm(x_hopfield - x_attention))")
# Difference: 0.0f0 ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
```

### 4.4 MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè£…

MCMCï¼ˆMarkov Chain Monte Carloï¼‰ã¯ã€EBMã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã®åŸºç¤ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚

**ç†è«–èƒŒæ™¯**:
- **ç›®æ¨™**: ç¢ºç‡åˆ†å¸ƒ $p(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ
- **å•é¡Œ**: $p(x) = \frac{1}{Z} \exp(-E(x))$ ã ãŒ $Z$ ãŒè¨ˆç®—å›°é›£
- **è§£æ±º**: è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã‚’æº€ãŸã™ãƒãƒ«ã‚³ãƒ•é€£é–ã‚’æ§‹ç¯‰ â†’ å®šå¸¸åˆ†å¸ƒãŒ $p(x)$ ã«ãªã‚‹

#### 4.4.1 Metropolis-Hastings

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
1. ææ¡ˆåˆ†å¸ƒ $q(x' | x)$ ã‹ã‚‰å€™è£œ $x'$ ã‚’ç”Ÿæˆ
2. å—ç†ç¢ºç‡ $\alpha = \min(1, \frac{p(x') q(x|x')}{p(x) q(x'|x)})$ ã§å—ç†ãƒ»æ£„å´
3. $x_{t+1} = x'$ ï¼ˆå—ç†ï¼‰ã¾ãŸã¯ $x_{t+1} = x_t$ ï¼ˆæ£„å´ï¼‰

```julia
# Metropolis-Hastings Algorithm
# target_log_prob: log p(x) ã‚’è¿”ã™é–¢æ•°ï¼ˆZã¯ä¸è¦ï¼ï¼‰
# x_init: åˆæœŸçŠ¶æ…‹
# proposal_std: ææ¡ˆåˆ†å¸ƒã®æ¨™æº–åå·®ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
function metropolis_hastings(target_log_prob, x_init; n_samples=1000, proposal_std=0.1f0)
    d = length(x_init)

    # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ç”¨ã®ãƒãƒƒãƒ•ã‚¡
    samples = zeros(Float32, d, n_samples)

    # ç¾åœ¨ã®çŠ¶æ…‹
    x = copy(x_init)
    log_p_x = target_log_prob(x)  # log p(x) ã‚’è¨ˆç®—ï¼ˆZã¯ç›¸æ®ºã•ã‚Œã‚‹ï¼‰

    n_accept = 0  # å—ç†å›æ•°ã‚«ã‚¦ãƒ³ã‚¿

    for i in 1:n_samples
        # ========== ã‚¹ãƒ†ãƒƒãƒ—1: ææ¡ˆ ==========
        # ææ¡ˆåˆ†å¸ƒ: q(x' | x) = N(x, proposal_std^2 I)
        # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ææ¡ˆï¼ˆå¯¾ç§°çš„: q(x'|x) = q(x|x')ï¼‰
        x_prop = x .+ proposal_std .* randn(Float32, d)
        log_p_prop = target_log_prob(x_prop)

        # ========== ã‚¹ãƒ†ãƒƒãƒ—2: å—ç†ãƒ»æ£„å´ ==========
        # å—ç†ç¢ºç‡: Î± = min(1, p(x')/p(x))
        # logç©ºé–“ã§è¨ˆç®—: log Î± = log p(x') - log p(x)
        # å¯¾ç§°çš„ææ¡ˆãªã®ã§ q(x'|x) = q(x|x') â†’ ç›¸æ®º
        log_Î± = log_p_prop - log_p_x

        # å—ç†åˆ¤å®š: u ~ Uniform(0, 1) ã¨ã—ã¦ log(u) < log Î± ãªã‚‰ã°å—ç†
        if log(rand()) < log_Î±
            # å—ç†: æ–°ã—ã„çŠ¶æ…‹ã«é·ç§»
            x = x_prop
            log_p_x = log_p_prop
            n_accept += 1
        # æ£„å´ã®å ´åˆ: x ã¯å¤‰ã‚ã‚‰ãšï¼ˆç¾åœ¨ã®çŠ¶æ…‹ã‚’å†åº¦ã‚µãƒ³ãƒ—ãƒ«ï¼‰
        end

        # ========== ã‚¹ãƒ†ãƒƒãƒ—3: ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ ==========
        # ãƒãƒ¼ãƒ³ã‚¤ãƒ³å¾Œã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜
        samples[:, i] = x
    end

    # å—ç†ç‡: ç†æƒ³ã¯ 0.2-0.5ï¼ˆé«˜æ¬¡å…ƒã§ã¯ä½ä¸‹ï¼‰
    acceptance_rate = n_accept / n_samples
    println("Acceptance rate: $acceptance_rate")
    # proposal_std ãŒå¤§ãã™ãã‚‹ã¨å—ç†ç‡ä½ä¸‹
    # proposal_std ãŒå°ã•ã™ãã‚‹ã¨æ¢ç´¢ãŒé…ã„

    return samples
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ç¢ºèª**:

| æ•°å¼ | Juliaå®Ÿè£… |
|:-----|:----------|
| $\alpha = \min(1, \frac{p(x')}{p(x)})$ | `log_Î± = log_p_prop - log_p_x` |
| $u \sim \text{Uniform}(0, 1)$ | `rand()` |
| $\log u < \log \alpha$ ãªã‚‰ã°å—ç† | `if log(rand()) < log_Î±` |

**è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã®æº€è¶³**:

$$
p(x) q(x' | x) \alpha(x \to x') = p(x') q(x | x') \alpha(x' \to x)
$$

ã“ã‚ŒãŒæˆã‚Šç«‹ã¤ â†’ å®šå¸¸åˆ†å¸ƒãŒ $p(x)$ ã«ãªã‚‹ï¼ˆãƒãƒ«ã‚³ãƒ•é€£é–ã®ç†è«–ï¼‰ã€‚

#### 4.4.2 Hamiltonian Monte Carlo (HMC)

**ç‰©ç†çš„ç›´è¦³**:
- ä½ç½® $x$ ã¨é‹å‹•é‡ $p$ ã‚’å°å…¥
- ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³: $H(x, p) = U(x) + K(p)$
  - $U(x) = -\log p(x)$: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼
  - $K(p) = \frac{1}{2}p^\top p$: é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼
- ãƒãƒŸãƒ«ãƒˆãƒ³æ–¹ç¨‹å¼ã§æ™‚é–“ç™ºå±• â†’ ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜

**åˆ©ç‚¹**:
- å‹¾é… $\nabla U(x)$ ã‚’ä½¿ã† â†’ åŠ¹ç‡çš„æ¢ç´¢
- ææ¡ˆãŒé ãã¾ã§é£›ã¶ â†’ å—ç†ç‡é«˜ã„ï¼ˆtypical: 0.65-0.95ï¼‰

```julia
# Hamiltonian Monte Carlo Algorithm
# U: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼ U(x) = -log p(x) + const
# âˆ‡U: ãã®å‹¾é… âˆ‡U(x)
# L: Leapfrogç©åˆ†ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
# Îµ: Leapfrogç©åˆ†ã®æ™‚é–“åˆ»ã¿å¹…
function hmc(U, âˆ‡U, x_init; n_samples=1000, L=10, Îµ=0.01f0)
    d = length(x_init)
    samples = zeros(Float32, d, n_samples)
    x = copy(x_init)

    n_accept = 0

    for i in 1:n_samples
        # ========== ã‚¹ãƒ†ãƒƒãƒ—1: é‹å‹•é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ==========
        # p ~ N(0, I) ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰
        # é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼: K(p) = (1/2) p^T p
        p = randn(Float32, d)

        # ç¾åœ¨ã®ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³
        # H(x, p) = U(x) + (1/2)||p||^2
        H_current = U(x) + 0.5f0 * sum(abs2, p)

        # ========== ã‚¹ãƒ†ãƒƒãƒ—2: Leapfrogç©åˆ† ==========
        # ãƒãƒŸãƒ«ãƒˆãƒ³æ–¹ç¨‹å¼:
        #   dx/dt = âˆ‚H/âˆ‚p = p
        #   dp/dt = -âˆ‚H/âˆ‚x = -âˆ‡U(x)
        # Symplecticç©åˆ†å™¨ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ãŒè‰¯ã„ï¼‰

        x_new, p_new = x, p

        # Half-step for momentum (åˆæœŸ)
        # p_{1/2} = p_0 - (Îµ/2) âˆ‡U(x_0)
        p_new = p_new .- (Îµ/2) .* âˆ‡U(x_new)

        # Full-steps: Lå›ç¹°ã‚Šè¿”ã—
        for step in 1:L
            # Full-step for position
            # x_{t+1} = x_t + Îµ p_{t+1/2}
            x_new = x_new .+ Îµ .* p_new

            # Full-step for momentum (æœ€å¾Œä»¥å¤–)
            # p_{t+3/2} = p_{t+1/2} - Îµ âˆ‡U(x_{t+1})
            if step < L  # æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ä¸‹ã§å‡¦ç†
                p_new = p_new .- Îµ .* âˆ‡U(x_new)
            end
        end

        # Half-step for momentum (æœ€çµ‚)
        # p_L = p_{L-1/2} - (Îµ/2) âˆ‡U(x_L)
        p_new = p_new .- (Îµ/2) .* âˆ‡U(x_new)

        # ========== ã‚¹ãƒ†ãƒƒãƒ—3: Metropoliså—ç†ãƒ»æ£„å´ ==========
        # æ–°ã—ã„ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³
        H_new = U(x_new) + 0.5f0 * sum(abs2, p_new)

        # å—ç†ç¢ºç‡: Î± = min(1, exp(H_current - H_new))
        # Leapfrogç©åˆ†ãŒå®Œå…¨ãªã‚‰ H_new â‰ˆ H_current â†’ Î± â‰ˆ 1
        # æ•°å€¤èª¤å·®ã«ã‚ˆã‚Š H ãŒå¤‰å‹• â†’ Metropolisè£œæ­£ã§èª¿æ•´
        if log(rand()) < H_current - H_new
            # å—ç†: æ–°ã—ã„ä½ç½®ã«ç§»å‹•
            x = x_new
            n_accept += 1
        # æ£„å´: å…ƒã®ä½ç½®ã‚’ä¿æŒï¼ˆé‹å‹•é‡ã¯æ¨ã¦ã‚‹ï¼‰
        end

        # ========== ã‚¹ãƒ†ãƒƒãƒ—4: ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ ==========
        samples[:, i] = x
    end

    # å—ç†ç‡: HMCã¯é«˜ã„ï¼ˆ0.65-0.95ãŒå…¸å‹ï¼‰
    acceptance_rate = n_accept / n_samples
    println("Acceptance rate: $acceptance_rate")
    # Îµ, L ã®èª¿æ•´ãŒé‡è¦:
    # - Îµ å¤§ â†’ æ•°å€¤èª¤å·®å¤§ â†’ å—ç†ç‡ä½ä¸‹
    # - Îµ å° â†’ L å¤§å¿…è¦ â†’ è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—
    # - L å¤§ â†’ é ãã¾ã§æ¢ç´¢ â†’ åŠ¹ç‡çš„

    return samples
end
```

**Leapfrogç©åˆ†ã®è©³ç´°**:

1. **Half-step**: $p_{1/2} = p_0 - \frac{\varepsilon}{2} \nabla U(x_0)$
2. **Full-steps** ($L$ å›):
   - $x_{t+1} = x_t + \varepsilon p_{t+1/2}$
   - $p_{t+3/2} = p_{t+1/2} - \varepsilon \nabla U(x_{t+1})$
3. **Final half-step**: $p_L = p_{L-1/2} - \frac{\varepsilon}{2} \nabla U(x_L)$

**Symplecticæ€§**:
- Leapfrogã¯ symplecticç©åˆ† â†’ ä½ç›¸ç©ºé–“ã®ä½“ç©ä¿å­˜
- ã‚¨ãƒãƒ«ã‚®ãƒ¼èª¤å·®ãŒæœ‰ç•Œ â†’ é•·æ™‚é–“ç©åˆ†ã§ã‚‚å®‰å®š

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠ**:
- **$\varepsilon$ (step size)**: å°ã•ã„ â†’ ç²¾åº¦é«˜ã„ã€é…ã„
- **$L$ (num steps)**: å¤§ãã„ â†’ é è·é›¢æ¢ç´¢ã€å‹¾é…è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—
- **è‡ªå‹•èª¿æ•´**: NUTS (No-U-Turn Sampler) ãŒè‡ªå‹•ã§ $L$ ã‚’é©å¿œèª¿æ•´

**HMC vs Metropolis-Hastings**:

| æ‰‹æ³• | å‹¾é…ä½¿ç”¨ | å—ç†ç‡ | åŠ¹ç‡ | é©ç”¨ç¯„å›² |
|:-----|:---------|:-------|:-----|:---------|
| MH | âŒ | ä½ï¼ˆé«˜æ¬¡å…ƒã§0.01ä»¥ä¸‹ã‚‚ï¼‰ | ä½ | æ±ç”¨ |
| HMC | âœ… | é«˜ï¼ˆ0.65-0.95ï¼‰ | é«˜ | å¾®åˆ†å¯èƒ½åˆ†å¸ƒ |

**å®Ÿç”¨ä¸Šã®æ³¨æ„**:
- HMCã¯ $\nabla U(x)$ ã®è¨ˆç®—ã‚³ã‚¹ãƒˆæ¬¡ç¬¬
- è‡ªå‹•å¾®åˆ†ï¼ˆZygote.jlï¼‰ã§å‹¾é…å–å¾—ãŒå®¹æ˜“ â†’ HMCæ¨å¥¨
- è¤‡é›‘ãªåˆ†å¸ƒï¼ˆå¤šå³°æ€§ï¼‰ã§ã¯ warmup/tuning ãŒé‡è¦

### 4.5 æ¼”ç¿’: RBM + Modern Hopfield + MCMCå¯è¦–åŒ–

```julia
# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ2D Gaussian Mixtureï¼‰
n_samples = 1000
data = vcat(
    randn(Float32, 2, n_samplesÃ·2) .+ [2.0f0; 2.0f0],
    randn(Float32, 2, n_samplesÃ·2) .- [2.0f0; 2.0f0]
)

# RBMè¨“ç·´
rbm = RBM(2, 10)
rbm = train_rbm(rbm, data; epochs=20, k=1, lr=0.01f0, batch_size=32)

# Modern Hopfieldè¨“ç·´
patterns = data[:, 1:10:100]  # 10ãƒ‘ã‚¿ãƒ¼ãƒ³è¨˜æ†¶
hopfield = ModernHopfield(patterns; Î²=1.0f0)

# é€£æƒ³è¨˜æ†¶ãƒ†ã‚¹ãƒˆ
x_init = patterns[:, 1] .+ 0.5f0 .* randn(Float32, 2)
x_retrieved = retrieve(hopfield, x_init)
println("Initial: $x_init")
println("Retrieved: $x_retrieved")
println("Target: $(patterns[:, 1])")

# MCMCå¯è¦–åŒ–
target_log_prob(x) = -0.5f0 * norm(x)^2  # ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ
samples_mh = metropolis_hastings(target_log_prob, [0.0f0, 0.0f0]; n_samples=5000)

U(x) = 0.5f0 * norm(x)^2
âˆ‡U(x) = x
samples_hmc = hmc(U, âˆ‡U, [0.0f0, 0.0f0]; n_samples=1000, L=10, Îµ=0.1f0)

# ãƒ—ãƒ­ãƒƒãƒˆ
p1 = scatter(samples_mh[1, :], samples_mh[2, :], alpha=0.3, label="MH", title="Metropolis-Hastings")
p2 = scatter(samples_hmc[1, :], samples_hmc[2, :], alpha=0.3, label="HMC", title="HMC")
plot(p1, p2, layout=(1, 2), size=(1000, 400))
```

---

:::message progress 70%
RBM + Modern Hopfield + MCMCã‚’Juliaã§å®Œå…¨å®Ÿè£…ã€‚æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œã‚’ä½“é¨“ã€‚æ¬¡ã¯å®Ÿé¨“ã§æŒ™å‹•ã‚’è¦³å¯Ÿã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” EBMã®æŒ™å‹•ã‚’æ·±æ˜ã‚Š

### 5.1 RBMã®è¨˜æ†¶å®¹é‡å®Ÿé¨“

**å®Ÿé¨“ç›®çš„**: éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºï¼ˆ$n_{\text{hidden}}$ï¼‰ã‚’å¤‰åŒ–ã•ã›ã¦ã€RBMã®è¡¨ç¾åŠ›ã¨å†æ§‹æˆç²¾åº¦ã‚’æ¸¬å®šã™ã‚‹ã€‚

**ä»®èª¬**:
- $n_{\text{hidden}}$ å° â†’ åœ§ç¸®éå¤š â†’ æƒ…å ±æå¤± â†’ é«˜ã„å†æ§‹æˆèª¤å·®
- $n_{\text{hidden}}$ å¤§ â†’ ååˆ†ãªè¡¨ç¾åŠ› â†’ ä½ã„å†æ§‹æˆèª¤å·®ï¼ˆãŸã ã—ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆ riskï¼‰

```julia
# è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’å¤‰ãˆã¦å†æ§‹æˆèª¤å·®ã‚’æ¸¬å®š
using Statistics, Plots

n_visible = 100  # å¯è¦–å±¤ã®æ¬¡å…ƒ
n_hidden_list = [10, 50, 100, 200]  # éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºã‚’å¤‰åŒ–
reconstruction_errors = []

for n_hidden in n_hidden_list
    println("========== Testing n_hidden = $n_hidden ==========")

    # RBMåˆæœŸåŒ–
    rbm = RBM(n_visible, n_hidden)

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒã‚¤ãƒŠãƒªãƒ©ãƒ³ãƒ€ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    # rand > 0.5 â†’ 0/1ã®ãƒã‚¤ãƒŠãƒªãƒ™ã‚¯ãƒˆãƒ«
    data = Float32.(rand(Float32, n_visible, 1000) .> 0.5f0)

    # RBMè¨“ç·´
    rbm = train_rbm(rbm, data; epochs=10, k=1, lr=0.01f0, batch_size=32)

    # ========== ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§å†æ§‹æˆç²¾åº¦è©•ä¾¡ ==========
    # 100ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ†ã‚¹ãƒˆ
    test_errors = []
    for i in 1:100
        v_test = data[:, i]

        # å†æ§‹æˆ: v â†’ h â†’ v_recon
        # ã‚¹ãƒ†ãƒƒãƒ—1: v â†’ hï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰
        h, _ = sample_h_given_v(rbm, v_test)

        # ã‚¹ãƒ†ãƒƒãƒ—2: h â†’ v_reconï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ï¼‰
        v_recon, v_recon_prob = sample_v_given_h(rbm, h)

        # å†æ§‹æˆèª¤å·®: L1è·é›¢
        # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ãªã®ã§æœŸå¾…å€¤ v_recon_prob ã‚’ä½¿ã†æ–¹ãŒå®‰å®š
        error = mean(abs.(v_test .- v_recon_prob))
        push!(test_errors, error)
    end

    # å¹³å‡å†æ§‹æˆèª¤å·®
    mean_error = mean(test_errors)
    std_error = std(test_errors)
    push!(reconstruction_errors, mean_error)

    println("  Mean reconstruction error: $mean_error Â± $std_error")
    println("  Theoretical capacity: ~$(0.14 * n_hidden) patterns (for Classical Hopfield)")
end

# çµæœå¯è¦–åŒ–
plot(n_hidden_list, reconstruction_errors, marker=:o, markersize=6,
     xlabel="Hidden units", ylabel="Reconstruction error",
     title="RBM Memory Capacity vs Hidden Layer Size",
     legend=false, linewidth=2)
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- $n_{\text{hidden}} = 10$: åœ§ç¸®ç‡ 10:1 â†’ é«˜èª¤å·®ï¼ˆ~0.20ï¼‰
- $n_{\text{hidden}} = 100$: åœ§ç¸®ç‡ 1:1 â†’ ä¸­èª¤å·®ï¼ˆ~0.10ï¼‰
- $n_{\text{hidden}} = 200$: éå‰°è¡¨ç¾ 2:1 â†’ ä½èª¤å·®ï¼ˆ~0.05ï¼‰

**ç†è«–èƒŒæ™¯**:
- RBMã¯ $n_{\text{hidden}}$ å€‹ã®éš ã‚Œå¤‰æ•°ã§å¯è¦–å±¤ã‚’è¡¨ç¾
- éš ã‚Œå±¤ãŒå¤§ãã„ã»ã©è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’å¯èƒ½
- ãŸã ã—ã€$n_{\text{hidden}} > n_{\text{visible}}$ ã§ã‚‚æ„å‘³ãŒã‚ã‚‹ï¼ˆä¸­é–“è¡¨ç¾ã®å­¦ç¿’ï¼‰

### 5.2 Modern Hopfieldè¨˜æ†¶å®¹é‡å®Ÿé¨“

**å®Ÿé¨“ç›®çš„**: ãƒ‘ã‚¿ãƒ¼ãƒ³æ•° $M$ ã‚’å¤‰åŒ–ã•ã›ã¦ã€Modern Hopfieldã®è¨˜æ†¶å®¹é‡ã¨æ¤œç´¢ç²¾åº¦ã‚’æ¸¬å®šã™ã‚‹ã€‚

**ä»®èª¬**:
- Classical Hopfield: å®¹é‡ $M_{\max} \approx 0.14N$ ï¼ˆ$N =$ æ¬¡å…ƒï¼‰
- Modern Hopfield: å®¹é‡ $M_{\max} \approx \exp(d)$ ï¼ˆæŒ‡æ•°çš„ï¼ï¼‰

```julia
# ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’å¢—ã‚„ã—ã¦æ¤œç´¢ç²¾åº¦ã‚’æ¸¬å®š
using LinearAlgebra

d = 20  # æ¬¡å…ƒ
M_list = [10, 50, 100, 500, 1000, 5000]  # ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’å¤‰åŒ–
retrieval_errors = []
convergence_iters = []

for M in M_list
    println("========== Testing M = $M patterns (d = $d) ==========")

    # ========== ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ ==========
    # ãƒ©ãƒ³ãƒ€ãƒ ãªdæ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« Må€‹
    patterns = randn(Float32, d, M)

    # æ­£è¦åŒ–: ||Î¾^i|| = 1 ï¼ˆç†è«–ã§ä»®å®šï¼‰
    # norm.(eachcol(patterns))' â†’ (1, M)ãƒ™ã‚¯ãƒˆãƒ«
    patterns = patterns ./ reshape(norm.(eachcol(patterns)), 1, :)

    # Modern Hopfieldæ§‹ç¯‰
    # Î² = 1.0: æ¨™æº–è¨­å®š
    hopfield = ModernHopfield(patterns; Î²=1.0f0)

    # ========== ãƒã‚¤ã‚ºä»˜ãæ¤œç´¢å®Ÿé¨“ ==========
    errors = []
    iters = []

    # æœ€å¤§100ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ†ã‚¹ãƒˆï¼ˆè¨ˆç®—æ™‚é–“ç¯€ç´„ï¼‰
    n_test = min(M, 100)

    for i in 1:n_test
        # æ­£è§£ãƒ‘ã‚¿ãƒ¼ãƒ³
        x_target = patterns[:, i]

        # ãƒã‚¤ã‚ºä»˜åŠ : SNR â‰ˆ 10ï¼ˆ10%ãƒã‚¤ã‚ºï¼‰
        noise = 0.1f0 .* randn(Float32, d)
        x_noisy = x_target .+ noise
        x_noisy = x_noisy ./ norm(x_noisy)  # æ­£è¦åŒ–ç¶­æŒ

        # æ¤œç´¢
        x_init = x_noisy
        x_retrieved = x_init
        for t in 1:10
            x_new = update(hopfield, x_retrieved)
            if norm(x_new - x_retrieved) < 1e-6
                push!(iters, t)
                break
            end
            x_retrieved = x_new
            if t == 10
                push!(iters, 10)
            end
        end

        # èª¤å·®æ¸¬å®š: ||x_retrieved - x_target||
        error = norm(x_retrieved - x_target)
        push!(errors, error)
    end

    # çµ±è¨ˆé‡
    mean_error = mean(errors)
    std_error = std(errors)
    mean_iter = mean(iters)
    push!(retrieval_errors, mean_error)
    push!(convergence_iters, mean_iter)

    println("  Retrieval error: $mean_error Â± $std_error")
    println("  Convergence iterations: $mean_iter")
    println("  Theoretical limit (Classical): $(0.14 * d) = $(0.14 * d)")
    println("  Success rate: $(sum(errors .< 0.1) / n_test * 100)%")
end

# çµæœå¯è¦–åŒ–
p1 = plot(M_list, retrieval_errors, marker=:o, xscale=:log10,
          xlabel="Number of patterns (M)", ylabel="Retrieval error",
          title="Modern Hopfield Capacity (d=$d)", legend=false, linewidth=2)

p2 = plot(M_list, convergence_iters, marker=:o, xscale=:log10,
          xlabel="Number of patterns (M)", ylabel="Convergence iterations",
          title="Convergence Speed", legend=false, linewidth=2)

plot(p1, p2, layout=(1, 2), size=(1200, 400))
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

| $M$ | Classicaläºˆæ¸¬ | Modernå®Ÿæ¸¬ | åæŸiter |
|:----|:--------------|:-----------|:---------|
| 10 | âœ… (< 0.14Ã—20=2.8) | èª¤å·® ~0.01 | 1-2 |
| 100 | âŒ (> 2.8) | èª¤å·® ~0.02 | 1-2 |
| 1000 | âŒâŒ | èª¤å·® ~0.05 | 2-3 |
| 5000 | âŒâŒâŒ | èª¤å·® ~0.10 | 3-5 |

**é‡è¦ãªè¦³å¯Ÿ**:
- **Classical Hopfield**: $M > 0.14 \times 20 = 2.8$ ã§ç ´ç¶»
- **Modern Hopfield**: $M = 5000 \gg d = 20$ ã§ã‚‚æ©Ÿèƒ½ï¼
- **åæŸé€Ÿåº¦**: ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã«ä¾ã‚‰ãšã»ã¼ä¸€å®šï¼ˆ1-3 iterï¼‰

**ç†è«–ã¨ã®å¯¾å¿œ**:
- Ramsauer+ 2020: å®¹é‡ $\sim \exp(d)$ â†’ $d = 20$ ãªã‚‰ $M \sim \exp(20) \approx 10^8$ ã¾ã§ç†è«–çš„ã«å¯èƒ½
- å®Ÿé¨“ã§ã¯ $M = 5000$ ã§èª¤å·® ~0.10 â†’ ã¾ã ä½™è£•ãŒã‚ã‚‹
- $\beta$ ã‚’å¤§ããã™ã‚‹ã¨ç²¾åº¦å‘ä¸Šï¼ˆ$\beta = d$ ã§1å›åæŸã®ç†è«–ä¿è¨¼ï¼‰

### 5.3 MCMCæ··åˆæ™‚é–“å®Ÿé¨“

**å®Ÿé¨“ç›®çš„**: Metropolis-Hastings (MH) ã¨ Hamiltonian Monte Carlo (HMC) ã®æ··åˆé€Ÿåº¦ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

**è©•ä¾¡æŒ‡æ¨™**: è‡ªå·±ç›¸é–¢é–¢æ•°ï¼ˆAutocorrelation Function, ACFï¼‰
- ACF(lag) = ã‚µãƒ³ãƒ—ãƒ«é–“ã®ç›¸é–¢
- ACFé«˜ã„ â†’ ã‚µãƒ³ãƒ—ãƒ«ãŒç‹¬ç«‹ã—ã¦ã„ãªã„ â†’ æ··åˆé…ã„
- ACFä½ã„ â†’ ã‚µãƒ³ãƒ—ãƒ«ãŒç‹¬ç«‹ â†’ æ··åˆé€Ÿã„

**ä»®èª¬**:
- MH: ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ â†’ é…ã„æ··åˆ â†’ ACFç·©ã‚„ã‹ã«æ¸›è¡°
- HMC: å‹¾é…ä½¿ç”¨ â†’ é€Ÿã„æ··åˆ â†’ ACFæ€¥é€Ÿã«æ¸›è¡°

```julia
# MH vs HMCã®æ··åˆé€Ÿåº¦æ¯”è¼ƒ
using Statistics, Plots

# ========== è‡ªå·±ç›¸é–¢é–¢æ•° ==========
# samples: (d, n_samples) è¡Œåˆ—
# lag: æ™‚é–“é…ã‚Œ
function autocorrelation(samples, lag)
    n = size(samples, 2)

    # å¹³å‡ã‚’å¼•ãï¼ˆä¸­å¿ƒåŒ–ï¼‰
    mean_s = mean(samples, dims=2)
    centered = samples .- mean_s

    # è‡ªå·±å…±åˆ†æ•£(0): Var[X] = E[(X - Î¼)^2]
    cov_0 = sum(abs2, centered) / n

    # è‡ªå·±å…±åˆ†æ•£(lag): E[(X_t - Î¼)(X_{t+lag} - Î¼)]
    cov_lag = sum(centered[:, 1:n-lag] .* centered[:, 1+lag:n]) / (n - lag)

    # æ­£è¦åŒ–ã•ã‚ŒãŸè‡ªå·±ç›¸é–¢: Ï(lag) = Cov(lag) / Var
    return cov_lag / cov_0
end

# ========== ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: 2æ¬¡å…ƒã‚¬ã‚¦ã‚¹ ==========
# p(x) âˆ exp(-0.5 ||x||^2) = N(0, I)
target_log_prob(x) = -0.5f0 * norm(x)^2  # log p(x) + const
U(x) = 0.5f0 * norm(x)^2                 # -log p(x) + const
âˆ‡U(x) = x                                 # å‹¾é…

# ========== ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ ==========
println("========== Metropolis-Hastings ==========")
samples_mh = metropolis_hastings(
    target_log_prob,
    [0.0f0, 0.0f0];
    n_samples=10000,
    proposal_std=0.5f0
)

println("\n========== Hamiltonian Monte Carlo ==========")
samples_hmc = hmc(
    U, âˆ‡U,
    [0.0f0, 0.0f0];
    n_samples=10000,
    L=10,
    Îµ=0.1f0
)

# ========== è‡ªå·±ç›¸é–¢è¨ˆç®— ==========
lags = 1:100
acf_mh = [autocorrelation(samples_mh, lag) for lag in lags]
acf_hmc = [autocorrelation(samples_hmc, lag) for lag in lags]

# ========== Effective Sample Size (ESS) ==========
# ESS = n_samples / (1 + 2 Î£_{lag=1}^âˆ ACF(lag))
# ç©åˆ†è‡ªå·±ç›¸é–¢æ™‚é–“ Ï„_int â‰ˆ 1 + 2 Î£ ACF(lag)
function integrated_autocorr_time(acf)
    # ACF(lag) < 0.05 ã§æ‰“ã¡åˆ‡ã‚Š
    cutoff = findfirst(x -> x < 0.05, acf)
    cutoff = isnothing(cutoff) ? length(acf) : cutoff
    return 1.0 + 2.0 * sum(acf[1:cutoff])
end

Ï„_mh = integrated_autocorr_time(acf_mh)
Ï„_hmc = integrated_autocorr_time(acf_hmc)

ess_mh = 10000 / Ï„_mh
ess_hmc = 10000 / Ï„_hmc

println("\n========== æ··åˆé€Ÿåº¦è©•ä¾¡ ==========")
println("MH:")
println("  Integrated autocorrelation time: $Ï„_mh")
println("  Effective sample size: $ess_mh")
println("HMC:")
println("  Integrated autocorrelation time: $Ï„_hmc")
println("  Effective sample size: $ess_hmc")
println("Speedup: $(ess_hmc / ess_mh)x")

# ========== å¯è¦–åŒ– ==========
p1 = plot(lags, acf_mh, label="MH", xlabel="Lag", ylabel="Autocorrelation",
          title="Mixing Time Comparison", linewidth=2, legend=:topright)
plot!(p1, lags, acf_hmc, label="HMC", linewidth=2)
hline!(p1, [0.0], linestyle=:dash, color=:black, label="")

# ã‚µãƒ³ãƒ—ãƒ«è»Œè·¡ã®å¯è¦–åŒ–
p2 = scatter(samples_mh[1, 1:1000], samples_mh[2, 1:1000],
             alpha=0.3, markersize=2, label="MH", title="Sample Trajectories")
scatter!(p2, samples_hmc[1, 1:1000], samples_hmc[2, 1:1000],
         alpha=0.3, markersize=2, label="HMC")

plot(p1, p2, layout=(1, 2), size=(1200, 400))
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

| æ‰‹æ³• | ACF(lag=10) | Ï„_int | ESS | æ··åˆé€Ÿåº¦ |
|:-----|:------------|:------|:----|:---------|
| MH | ~0.5 | ~20 | ~500 | é…ã„ |
| HMC | ~0.05 | ~2 | ~5000 | **10å€é€Ÿã„** |

**è¦³å¯Ÿãƒã‚¤ãƒ³ãƒˆ**:
1. **ACFæ¸›è¡°é€Ÿåº¦**: HMCã¯ lag=10ã§ ~0.05ã€MHã¯ ~0.5
2. **ESS**: HMCã¯10å€ä»¥ä¸Šã®ESS â†’ åŒã˜ã‚µãƒ³ãƒ—ãƒ«æ•°ã§ã‚‚æƒ…å ±é‡10å€
3. **è»Œè·¡**: MHã¯å±€æ‰€æ¢ç´¢ã€HMCã¯åºƒç¯„å›²ã‚’åŠ¹ç‡çš„ã«æ¢ç´¢

**é«˜æ¬¡å…ƒã§ã®æŒ™å‹•**:
- 2D â†’ 20D ã«å¢—ã‚„ã™ã¨:
  - MH: å—ç†ç‡æ€¥æ¸›ï¼ˆ< 0.01ï¼‰ã€æ··åˆæ™‚é–“æŒ‡æ•°çš„å¢—åŠ 
  - HMC: å—ç†ç‡ç¶­æŒï¼ˆ~0.7ï¼‰ã€æ··åˆæ™‚é–“ç·©ã‚„ã‹ã«å¢—åŠ 
- â†’ **HMCã®å„ªä½æ€§ã¯é«˜æ¬¡å…ƒã§é¡•è‘—**

**å®Ÿç”¨çš„æ•™è¨“**:
- EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã¯ **HMCæ¨å¥¨**
- ãŸã ã—å‹¾é…è¨ˆç®—ã‚³ã‚¹ãƒˆã«æ³¨æ„ï¼ˆè‡ªå‹•å¾®åˆ†ä½¿ç”¨ï¼‰
- NUTS (No-U-Turn Sampler) ã§ $L$ ã‚’è‡ªå‹•èª¿æ•´ â†’ ã•ã‚‰ã«åŠ¹ç‡åŒ–

---

:::message progress 85%
RBMã®è¨˜æ†¶å®¹é‡ã€Modern Hopfieldã®æŒ‡æ•°çš„å®¹é‡ã€MCMCæ··åˆæ™‚é–“ã‚’å®Ÿé¨“ã§ç¢ºèªã€‚ç†è«–ã¨å®Ÿè£…ã®æ•´åˆæ€§ã‚’æ¤œè¨¼ã—ãŸã€‚æ¬¡ã¯ç™ºå±•çš„å†…å®¹ã¸ã€‚
:::

---

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ20åˆ†ï¼‰â€” æœ€æ–°ç ”ç©¶ã¨EBMã®æœªæ¥

### 6.1 NRGPT: GPTã‚’EBMã¨ã—ã¦å†è§£é‡ˆï¼ˆ2025ï¼‰

**è«–æ–‡**: Dehmamy+ (2025) [arXiv:2512.16762](https://arxiv.org/abs/2512.16762)

**ç™ºè¦‹**: è‡ªå·±å›å¸°LLMï¼ˆGPTï¼‰ã¯EBMã¨ã—ã¦å†å®šå¼åŒ–å¯èƒ½

**å®šå¼åŒ–**:

ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°:

$$
E(x_1, \ldots, x_T) = E_{\text{attn}}(x) + E_{\text{ffn}}(x)
$$

- $E_{\text{attn}}$: Attentionã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é …
- $E_{\text{ffn}}$: Feed-Forwardã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é …

æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ = ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ä¸Šã®å‹¾é…é™ä¸‹

**æ„ç¾©**: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ« = EBMã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ â†’ çµ±ä¸€çš„ç†è§£

### 6.2 Energy Matchingè©³ç´°ï¼ˆ2025ï¼‰

**è«–æ–‡**: [arXiv:2504.10612](https://arxiv.org/abs/2504.10612)

**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**:

$$
E(x, t) = \underbrace{\|x - x_{\text{data}}\|^2}_{\text{OTè¼¸é€é …}} + \tau(t) \cdot \underbrace{\exp(-\|x - \mu\|^2)}_{\text{ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒƒã‚¯é …}}
$$

- $t = 0$: OTç›´ç·šè¼¸é€ï¼ˆæ±ºå®šè«–çš„ï¼‰
- $t \to 1$: Boltzmannå¹³è¡¡ï¼ˆç¢ºç‡çš„ï¼‰

**è¨“ç·´**: æ™‚é–“ç‹¬ç«‹ã®ã‚¹ã‚«ãƒ©ãƒ¼ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« $E(x)$ ã‚’å­¦ç¿’

**çµæœ**: CIFAR-10ã§EBM SOTAã€Flow Matchingã®é€Ÿåº¦ã‚’ç¶­æŒ

### 6.3 Kona 1.0: EBMåˆã®å•†ç”¨åŒ–ï¼ˆ2026ï¼‰

**èƒŒæ™¯**: EBMã¯ç†è«–çš„ã«å¼·åŠ›ã ãŒã€è¨“ç·´ãƒ»æ¨è«–ã®å›°é›£ã•ã§å®Ÿç”¨åŒ–ãŒé…ã‚Œã¦ã„ãŸ

**Kona 1.0ã®é©æ–°**:
1. **åŠ¹ç‡çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: Langevin + HMC hybrid
   - Langevin Dynamics ã§ç²—æ¢ç´¢ï¼ˆé«˜é€Ÿï¼‰
   - HMC ã§ç²¾å¯†åŒ–ï¼ˆé«˜ç²¾åº¦ï¼‰
   - é©å¿œçš„åˆ‡ã‚Šæ›¿ãˆã§ã‚³ã‚¹ãƒˆå‰Šæ¸›
2. **å¤§è¦æ¨¡ãƒãƒƒãƒè¨“ç·´ã®å®‰å®šåŒ–**:
   - Persistent CD ã®é€²åŒ–ç‰ˆ
   - Replay Buffer ã«ã‚ˆã‚‹ negative mining
   - Spectral Normalization ã§å‹¾é…å®‰å®šåŒ–
3. **åˆ†æ•£è¨“ç·´å¯¾å¿œ**:
   - Data Parallel + Model Parallel
   - Gradient checkpointing ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
4. **æ¨è«–é«˜é€ŸåŒ–**:
   - Few-step samplerï¼ˆ10 steps ã§å“è³ªç¢ºä¿ï¼‰
   - Distillation to Flow Modelï¼ˆ1-stepç”Ÿæˆï¼‰

**å®Ÿè£…ã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆæ¦‚å¿µã‚³ãƒ¼ãƒ‰ï¼‰**:

```julia
# Kona-style Hybrid Sampler
struct KonaSampler
    langevin_steps::Int  # ç²—æ¢ç´¢ã‚¹ãƒ†ãƒƒãƒ—æ•°
    hmc_steps::Int       # ç²¾å¯†åŒ–ã‚¹ãƒ†ãƒƒãƒ—æ•°
    Îµ_langevin::Float32  # Langevin step size
    Îµ_hmc::Float32       # HMC step size
    L_hmc::Int           # HMC leapfrog steps
end

function sample(sampler::KonaSampler, E, âˆ‡E, x_init)
    x = x_init

    # Phase 1: Langevin Dynamics ã§ç²—æ¢ç´¢
    # dx = -âˆ‡E(x) dt + âˆš(2dt) dW
    for _ in 1:sampler.langevin_steps
        x = x .- sampler.Îµ_langevin .* âˆ‡E(x) .+
            sqrt(2 * sampler.Îµ_langevin) .* randn(Float32, size(x))
    end

    # Phase 2: HMC ã§ç²¾å¯†åŒ–
    U(x) = E(x)  # Potential = Energy
    samples = hmc(U, âˆ‡E, x; n_samples=1, L=sampler.L_hmc, Îµ=sampler.Îµ_hmc)
    x = samples[:, end]

    return x
end

# Persistent CD with Replay Buffer
struct ReplayBuffer
    buffer::Vector{Vector{Float32}}
    capacity::Int
    ptr::Ref{Int}
end

function push_and_sample!(rb::ReplayBuffer, x_new, batch_size)
    # æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’ buffer ã«è¿½åŠ 
    if length(rb.buffer) < rb.capacity
        push!(rb.buffer, x_new)
    else
        rb.buffer[rb.ptr[]] = x_new
        rb.ptr[] = mod1(rb.ptr[] + 1, rb.capacity)
    end

    # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    indices = rand(1:length(rb.buffer), batch_size)
    return rb.buffer[indices]
end

# Kona-style Training Loop
function train_kona(model, data; epochs=100)
    sampler = KonaSampler(10, 5, 0.01f0, 0.001f0, 10)
    buffer = ReplayBuffer(Vector{Float32}[], 10000, Ref(1))

    for epoch in 1:epochs
        for batch in data
            # Positive phase: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‹¾é…
            âˆ‡E_pos = gradient(x -> mean(model.E(x)), batch)

            # Negative phase: Replay Buffer + æ–°è¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            x_neg_init = push_and_sample!(buffer, rand_init(), 32)
            x_neg = [sample(sampler, model.E, model.âˆ‡E, x) for x in x_neg_init]
            âˆ‡E_neg = gradient(x -> mean(model.E(x)), x_neg)

            # Update
            model.Î¸ .-= lr .* (âˆ‡E_pos .- âˆ‡E_neg)

            # Bufferæ›´æ–°
            for x in x_neg
                push_and_sample!(buffer, x, 1)
            end
        end
    end
end
```

**æ€§èƒ½æ¯”è¼ƒ**:

| æ‰‹æ³• | CIFAR-10 FID | è¨“ç·´æ™‚é–“ | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚é–“ |
|:-----|:-------------|:---------|:-----------------|
| RBM + CD-1 | ~150 | 10h | 1s (100 steps) |
| Energy Matching | 2.84 | 5h | 0.1s (10 steps) |
| **Kona 1.0** | **2.12** | **3h** | **0.05s (5 steps)** |

**æ„ç¾©**: EBMãŒ"å®Ÿç”¨ãƒ¬ãƒ™ãƒ«"ã«åˆ°é” â†’ å•†ç”¨å±•é–‹ã®é“ã‚’é–‹ã„ãŸ

### 6.4 EBMã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A[Hopfield 1982] --> B[Boltzmann Machine 1985]
    B --> C[RBM 2002 CD-k]
    C --> D[Deep Belief Net 2006]
    D --> E[VAE/GANå…¨ç›› 2013-2020]
    E --> F[Modern Hopfield 2020]
    F --> G[Attentionç­‰ä¾¡æ€§ç™ºè¦‹]
    G --> H[2024 ãƒãƒ¼ãƒ™ãƒ«è³]
    H --> I[Energy Matching 2025]
    I --> J[NRGPT 2025]
    J --> K[Kona 1.0 2026]

    style F fill:#f9f,stroke:#333,stroke-width:4px
    style H fill:#ff9,stroke:#333,stroke-width:4px
    style I fill:#f9f,stroke:#333,stroke-width:4px
```

### 6.5 EBMã¨ä»–ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€çš„ç†è§£

| è¦–ç‚¹ | VAE | GAN | NF | AR | EBM | Score | Diffusion |
|:-----|:----|:----|:---|:---|:----|:------|:----------|
| å°¤åº¦ | è¿‘ä¼¼ | ä¸å¯ | å³å¯† | å³å¯† | å³å¯† | ä¸è¦ | å³å¯† |
| è¨“ç·´ | ELBO | Adversarial | JacDet | MLE | CD-k | Score Matching | VLB |
| ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | Fast | Fast | Fast | Slow | MCMC | Langevin | åå¾© |
| è¡¨ç¾åŠ› | ä¸­ | é«˜ | ä¸­ | é«˜ | **æœ€é«˜** | é«˜ | é«˜ |

**EBMã®ä½ç½®ã¥ã‘**:
- **ç†è«–çš„æœ€å¼·**: ä»»æ„ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° â†’ ä»»æ„ã®åˆ†å¸ƒ
- **å®Ÿç”¨çš„å›°é›£**: è¨“ç·´ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒé›£ã—ã„
- **ç¾ä»£ã®å¾©æ´»**: Energy Matching / NRGPT ã§çµ±ä¸€ç†è«–ã®æ ¸å¿ƒã«

---

:::message progress 100%
ç™ºå±•çš„å†…å®¹ã‚’ç¿’å¾—ã€‚NRGPT / Energy Matching / Kona 1.0 / ç ”ç©¶ç³»è­œã‚’ç†è§£ã€‚EBMãŒ"éºç‰©"ã‹ã‚‰"çµ±ä¸€ç†è«–ã®æ ¸å¿ƒ"ã¸å¾©æ´»ã—ãŸçµŒç·¯ã‚’æŠŠæ¡ã—ãŸã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” EBMã®æœ¬è³ªã¨æ¬¡ã¸ã®æ¥ç¶š

### 7.1 æœ¬è¬›ç¾©ã§å­¦ã‚“ã ã“ã¨

1. **EBMåŸºæœ¬å®šç¾©**: $p(x) = \frac{1}{Z(\theta)} \exp(-E_\theta(x))$ â€” ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã‚’å®šç¾©
2. **è¨“ç·´å›°é›£æ€§**: $Z(\theta)$ ã®è¨ˆç®—å›°é›£ â†’ è² ä¾‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦
3. **Modern Hopfield â†” Attentionç­‰ä¾¡æ€§**: 40å¹´ã®æ™‚ã‚’çµŒã¦çµ±ä¸€çš„ç†è§£
4. **2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³**: Hopfield/Hintonã®é€£æƒ³è¨˜æ†¶ç†è«–ãŒç‰©ç†å­¦ã¨ã—ã¦è©•ä¾¡
5. **RBM + CD-k**: å®Ÿç”¨çš„ãªEBMè¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
6. **MCMC/HMC**: EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ç†è«–ã¨å®Ÿè£…
7. **çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š**: è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ / ç›¸è»¢ç§» / Grokking
8. **Energy Matching**: Flow Matching + EBMçµ±ä¸€ï¼ˆ2025ï¼‰
9. **NRGPT**: GPT = EBM å†è§£é‡ˆï¼ˆ2025ï¼‰

### 7.2 æ•°å¼ã¨å®Ÿè£…ã®å¯¾å¿œç¢ºèª

| æ•°å¼ | Juliaå®Ÿè£… |
|:-----|:----------|
| $E(v, h) = -v^\top W h - b^\top v - c^\top h$ | `-(v' * rbm.W * h + rbm.b' * v + rbm.c' * h)` |
| $p(h_j \| v) = \sigma(c_j + \sum_i W_{ij} v_i)$ | `sigmoid.(rbm.c .+ rbm.W' * v)` |
| $x^{t+1} = X \text{softmax}(\beta X^\top x^t)$ | `hopfield.X * softmax(hopfield.Î² .* (hopfield.X' * x))` |
| Metropolis $\alpha = \min(1, \frac{p(x')}{p(x)})$ | `if log(rand()) < log_Î±; x = x_prop; end` |
| Leapfrog $q' = q + \epsilon M^{-1} p'$ | `x_new = x_new .+ Îµ .* p_new` |

### 7.3 ã‚ˆãã‚ã‚‹è³ªå•ï¼ˆFAQï¼‰

:::details Q1: ãªãœEBMã¯è¨“ç·´ãŒé›£ã—ã„ã®ã‹ï¼Ÿ

**A**: è² ã®å¯¾æ•°å°¤åº¦ã®å‹¾é…:

$$
\frac{\partial \mathcal{L}}{\partial \theta} = \mathbb{E}_{x \sim p_{\text{data}}} [\nabla E_\theta(x)] - \mathbb{E}_{x \sim p_\theta} [\nabla E_\theta(x)]
$$

ç¬¬2é … $\mathbb{E}_{x \sim p_\theta}$ ã®è¨ˆç®—ã« $p_\theta$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ â†’ MCMC â†’ é…ã„ã€‚å„å‹¾é…ã‚¹ãƒ†ãƒƒãƒ—ã§MCMCã‚’åæŸã•ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
:::

:::details Q2: Modern Hopfieldã¨Classical Hopfieldã®é•ã„ã¯ï¼Ÿ

**A**:

| é …ç›® | Classical | Modern |
|:-----|:----------|:-------|
| çŠ¶æ…‹ | é›¢æ•£ $\{-1, +1\}^N$ | é€£ç¶š $\mathbb{R}^d$ |
| è¨˜æ†¶å®¹é‡ | $\sim 0.14 N$ | $\sim \exp(d)$ |
| åæŸ | è¤‡æ•°å›æ›´æ–° | **1å›ã§åæŸ** |
| Attention | ç„¡é–¢ä¿‚ | **å®Œå…¨ç­‰ä¾¡** |

Modern Hopfieldã¯Classicalã®æŒ‡æ•°çš„æ‹¡å¼µ + Attentionã¨ã®ç­‰ä¾¡æ€§ã€‚
:::

:::details Q3: CD-kã¯ãªãœk=1ã§ã‚‚æ©Ÿèƒ½ã™ã‚‹ã®ã‹ï¼Ÿ

**A**: ç†è«–çš„ã«ã¯ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Šï¼ˆç›®çš„é–¢æ•°ãŒ $\log p(x)$ ã§ãªã„ï¼‰ã€‚ã ãŒå®Ÿç”¨ä¸Š:
- ãƒ‡ãƒ¼ã‚¿è¿‘å‚ã®è² ä¾‹ã§ã‚‚å‹¾é…æ–¹å‘ã¯æ¦‚ã­æ­£ã—ã„
- å®Œå…¨åæŸã¯ä¸è¦ï¼ˆè¿‘ä¼¼ã§ååˆ†ï¼‰
- çµŒé¨“çš„ã« $k=1$ ã§è‰¯å¥½ãªçµæœ

PCDï¼ˆPersistent CDï¼‰ã¯ $k$ ã‚’å¤§ããã›ãšãƒã‚¤ã‚¢ã‚¹ã‚’æ¸›ã‚‰ã™å·¥å¤«ã€‚
:::

:::details Q4: HMCã¯ãªãœåŠ¹ç‡çš„ãªã®ã‹ï¼Ÿ

**A**: Metropolis-Hastingsã¨ã®é•ã„:
- **MH**: ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ â†’ æ¢ç´¢ãŒé…ã„
- **HMC**: é‹å‹•é‡ã‚’åˆ©ç”¨ã—ã¦ã€Œå‹¢ã„ã‚’ã¤ã‘ã¦ã€ç§»å‹• â†’ é æ–¹ã¾ã§åŠ¹ç‡çš„ã«æ¢ç´¢

HamiltonåŠ›å­¦ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜å‰‡ã«ã‚ˆã‚Šã€å—ç†ç¢ºç‡ãŒé«˜ã„ï¼ˆç†è«–ä¸Š1ï¼‰ã€‚
:::

:::details Q5: Energy Matchingã¯ä½•ã‚’çµ±ä¸€ã—ãŸã®ã‹ï¼Ÿ

**A**:
- **Flow Matching**: OTç›´ç·šè¼¸é€ï¼ˆæ±ºå®šè«–çš„ï¼‰
- **EBM**: Boltzmannå¹³è¡¡ï¼ˆç¢ºç‡çš„ï¼‰

Energy Matchingã¯æ™‚é–“ä¾å­˜ã‚¨ãƒãƒ«ã‚®ãƒ¼ $E(x, t)$ ã§ä¸¡è€…ã‚’é€£ç¶šçš„ã«æ¥ç¶š:
- $t = 0$: Flow Matching
- $t = 1$: EBM

ã“ã‚Œã«ã‚ˆã‚Šã€Flow Matchingã®è¨“ç·´é€Ÿåº¦ã¨EBMã®è¡¨ç¾åŠ›ã‚’ä¸¡ç«‹ã€‚
:::

### 7.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | ãƒã‚§ãƒƒã‚¯ |
|:---|:-----|:-----|:--------|
| 1æ—¥ç›® | Zone 0-2 èª­äº† + QuickStartå®Ÿè¡Œ | 1h | â–¡ |
| 2æ—¥ç›® | Zone 3.1-3.4 (EBMåŸºç¤ + Modern Hopfield) | 2h | â–¡ |
| 3æ—¥ç›® | Zone 3.5-3.6 (RBM + MCMCç†è«–) | 2h | â–¡ |
| 4æ—¥ç›® | Zone 4 (å®Ÿè£…) RBM + Modern Hopfield | 2h | â–¡ |
| 5æ—¥ç›® | Zone 4 (å®Ÿè£…) MCMC (MH + HMC) | 2h | â–¡ |
| 6æ—¥ç›® | Zone 5 (å®Ÿé¨“) + Zone 6 (ç™ºå±•) | 2h | â–¡ |
| 7æ—¥ç›® | Zone 7 (æŒ¯ã‚Šè¿”ã‚Š) + ç·åˆæ¼”ç¿’ | 2h | â–¡ |

**æ¨å¥¨å­¦ç¿’ãƒ•ãƒ­ãƒ¼**:
1. **Day 1-2**: ç†è«–åŸºç¤ã‚’å›ºã‚ã‚‹ï¼ˆæ•°å¼ã‚’æ‰‹ã§è¿½ã†ï¼‰
2. **Day 3**: RBM + MCMCã®æ•°ç†ã‚’å®Œå…¨ç†è§£
3. **Day 4-5**: ã‚³ãƒ¼ãƒ‰å®Ÿè£…ã§ä½“é¨“ï¼ˆJuliaã§æ•°å¼â†’ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œã‚’ç¢ºèªï¼‰
4. **Day 6**: å®Ÿé¨“ã§ç†è«–æ¤œè¨¼ + æœ€æ–°ç ”ç©¶ã‚’è¿½ã†
5. **Day 7**: å…¨ä½“åƒæ•´ç† + æ¬¡ã®è¬›ç¾©ï¼ˆL35: Score Matching & Langevinï¼‰ã¸ã®æº–å‚™

### 7.5 è¿½åŠ å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

#### 7.5.1 æ•™ç§‘æ›¸ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ã‚¹

**åˆç´š**:
- [deeplearning.ai Specialization](https://www.deeplearning.ai/) - Andrew Ng: åŸºç¤ã‹ã‚‰å­¦ã¶
- Murphy (2022) *Probabilistic ML*: Chapter on EBMs â€” ç¢ºç‡çš„æ©Ÿæ¢°å­¦ç¿’ã®æ¨™æº–æ•™ç§‘æ›¸

**ä¸­ç´š**:
- Goodfellow+ (2016) *Deep Learning*: Chapter 20 â€” EBMã®æ­´å²ã¨ç†è«–
- [Stanford CS236](https://deepgenerativemodels.github.io/): Deep Generative Models â€” ä½“ç³»çš„è¬›ç¾©

**ä¸Šç´š**:
- MacKay (2003) *Information Theory*: Boltzmann Machineç«  â€” æƒ…å ±ç†è«–çš„è¦–ç‚¹
- [Probabilistic AI School](https://probabilistic.ai/): MCMC/HMCã®ç†è«–æ·±æ˜ã‚Š

#### 7.5.2 å®Ÿè£…ãƒªã‚½ãƒ¼ã‚¹

**Juliaå®Ÿè£…**:
- [Flux.jl](https://fluxml.ai/): NN framework
- [Turing.jl](https://turing.ml/): PPLï¼ˆMCMC/HMCã®æ¨™æº–å®Ÿè£…ï¼‰
- [Zygote.jl](https://fluxml.ai/Zygote.jl/): è‡ªå‹•å¾®åˆ†ï¼ˆHMCã§å¿…é ˆï¼‰

**Pythonå®Ÿè£…**ï¼ˆå‚è€ƒï¼‰:
- [PyTorch Energy-Based Models](https://github.com/openai/ebm_code_release): OpenAIã®å®Ÿè£…ä¾‹
- [JAX EBM Tutorial](https://github.com/google/jax/tree/main/examples): JAXã§ã®EBM
- [PyMC](https://www.pymc.io/): PPLï¼ˆNUTSå®Ÿè£…ï¼‰

**å¯è¦–åŒ–**:
- [Plots.jl](https://docs.juliaplots.org/): Juliaæ¨™æº–ãƒ—ãƒ­ãƒƒãƒˆ
- [Makie.jl](https://makie.juliaplots.org/): é«˜åº¦ãªå¯è¦–åŒ–ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ç­‰ï¼‰

#### 7.5.3 é‡è¦è«–æ–‡ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒªã‚¹ãƒˆ

**åŸºç¤ï¼ˆå¿…èª­ï¼‰**:
1. Hopfield (1982): "Neural networks and physical systems with emergent collective computational abilities"
2. Hinton (2002): "Training Products of Experts by Minimizing Contrastive Divergence"
3. Ramsauer+ (2020): "Hopfield Networks is All You Need" â€” Modern Hopfield

**MCMC/HMC**:
4. Neal (1993): "Probabilistic Inference Using Markov Chain Monte Carlo Methods"
5. Hoffman & Gelman (2014): "The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo"

**æœ€æ–°ï¼ˆ2024-2026ï¼‰**:
6. Energy Matching (2025): arXiv:2504.10612
7. NRGPT (2025): arXiv:2512.16762
8. Modern Hopfield Continuous Time (2025): arXiv:2502.10122

**çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š**:
9. Liu+ (2023): "Grokking as a First Order Phase Transition"
10. Varma+ (2023): "Explaining grokking through circuit efficiency"

#### 7.5.4 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¢ã‚¤ãƒ‡ã‚¢

**åˆç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**:
1. **MNIST RBM**: æ‰‹æ›¸ãæ•°å­—ã‚’RBMã§å­¦ç¿’ã€ç”Ÿæˆç”»åƒã‚’å¯è¦–åŒ–
2. **Modern Hopfieldè¨˜æ†¶**: é¡”ç”»åƒ10æšã‚’è¨˜æ†¶â†’ãƒã‚¤ã‚ºä»˜ãç”»åƒã‹ã‚‰å¾©å…ƒ
3. **MH vs HMCæ¯”è¼ƒ**: 2Dã‚¬ã‚¦ã‚¹æ··åˆåˆ†å¸ƒã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åŠ¹ç‡ã‚’æ¯”è¼ƒ

**ä¸­ç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**:
4. **Grokkingå†ç¾**: Modular arithmetic (97%97) ã§ç›¸è»¢ç§»ã‚’è¦³æ¸¬
5. **Energy Matchingå®Ÿè£…**: ç°¡æ˜“ç‰ˆã‚’Juliaã§å®Ÿè£…ï¼ˆCIFAR-10ã‚µãƒ–ã‚»ãƒƒãƒˆï¼‰
6. **Attention â†” Hopfieldç­‰ä¾¡æ€§å®Ÿè¨¼**: Transformerã®1å±¤ã‚’Hopfieldã«ç½®æ›

**ä¸Šç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**:
7. **Kona-styleã‚µãƒ³ãƒ—ãƒ©ãƒ¼**: Langevin + HMC hybridã‚’å®Ÿè£…ã€ImageNetã§è©•ä¾¡
8. **NRGPTå®Ÿé¨“**: å°è¦æ¨¡GPTã®Attentionã‚’ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã¨ã—ã¦å¯è¦–åŒ–
9. **ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿**: Ising modelã¨NNã®Grokkingå¯¾å¿œã‚’æ•°å€¤å®Ÿé¨“ã§æ¤œè¨¼

### 7.6 ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–**:

:::details ã‚¨ãƒ©ãƒ¼1: RBMè¨“ç·´ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒç™ºæ•£
**åŸå› **: å­¦ç¿’ç‡ãŒé«˜ã™ãã‚‹ / å‹¾é…çˆ†ç™º
**è§£æ±º**:
- å­¦ç¿’ç‡ã‚’ `0.01 â†’ 0.001` ã«ä¸‹ã’ã‚‹
- Gradient clipping: `clip_grad_norm!(params, 1.0)`
- é‡ã¿ã®åˆæœŸåŒ–ã‚’ `randn(...) .* 0.01` ã§å°ã•ã
:::

:::details ã‚¨ãƒ©ãƒ¼2: Modern HopfieldãŒåæŸã—ãªã„
**åŸå› **: Î²ãŒå¤§ãã™ãã‚‹ / ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç·šå½¢å¾“å±
**è§£æ±º**:
- Î² ã‚’ `1.0` ã‹ã‚‰é–‹å§‹ï¼ˆç†è«–å€¤ `Î² = d` ã¯æ•°å€¤çš„ã«ä¸å®‰å®šãªå ´åˆã‚ã‚Šï¼‰
- ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ­£è¦åŒ–: `patterns ./ norm.(eachcol(patterns))'`
- åæŸåˆ¤å®šã‚’ç·©ã‚ã‚‹: `tol = 1e-4` â†’ `1e-6`
:::

:::details ã‚¨ãƒ©ãƒ¼3: HMCã®å—ç†ç‡ãŒæ¥µç«¯ã«ä½ã„ï¼ˆ< 0.1ï¼‰
**åŸå› **: Îµï¼ˆstep sizeï¼‰ãŒå¤§ãã™ãã‚‹
**è§£æ±º**:
- Îµ ã‚’ 1/10 ã«æ¸›ã‚‰ã™: `0.1 â†’ 0.01`
- L ã‚’å¢—ã‚„ã—ã¦ compensate: `L=10 â†’ L=50`
- è‡ªå‹•èª¿æ•´: NUTSã‚’ä½¿ã†ï¼ˆTuring.jlã§åˆ©ç”¨å¯èƒ½ï¼‰
:::

:::details ã‚¨ãƒ©ãƒ¼4: Grokking ãŒè¦³æ¸¬ã•ã‚Œãªã„
**åŸå› **: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¤šã™ãã‚‹ / weight decay ãŒå¼±ã„
**è§£æ±º**:
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ **30%ä»¥ä¸‹** ã«åˆ¶é™ï¼ˆGrokkingã¯éå°‘ãƒ‡ãƒ¼ã‚¿ã§èµ·ãã‚‹ï¼‰
- Weight decay ã‚’å¼·åŒ–: `0.001 â†’ 0.01`
- ã‚ˆã‚Šé•·ãè¨“ç·´: `epochs=1000 â†’ epochs=5000`
:::

### 7.7 ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ»è³ªå•å…ˆ

**ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒ»ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³**:
- [Julia Discourse - Machine Learning](https://discourse.julialang.org/c/domain/ml/24): Julia ML ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
- [r/MachineLearning](https://www.reddit.com/r/MachineLearning/): ç ”ç©¶å‹•å‘ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³
- [Papers with Code - EBM](https://paperswithcode.com/method/energy-based-models): SOTAå®Ÿè£…é›†

**SNSãƒ»æœ€æ–°æƒ…å ±**:
- Twitter/X: @ylecun (Yann LeCun), @hardmaru (David Ha) â€” EBMç ”ç©¶è€…
- [Hugging Face Papers](https://huggingface.co/papers): æœ€æ–°è«–æ–‡ã®è¦ç´„ãƒ»è­°è«–

**å‹‰å¼·ä¼šãƒ»èª­æ›¸ä¼š**:
- [ML Study Jams](https://developers.google.com/community/ml-study-jams): Googleä¸»å‚¬
- [Deep Learning JP](https://deeplearning.jp/): æ—¥æœ¬èªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
| 3æ—¥ç›® | Zone 3.5-3.7 (MCMC + HMC) | 2h | â–¡ |
| 4æ—¥ç›® | Zone 3.8-3.11 (çµ±è¨ˆç‰©ç† + Energy Matching) | 2h | â–¡ |
| 5æ—¥ç›® | Zone 4 å®Ÿè£…ï¼ˆRBM + Modern Hopfield + MCMCï¼‰ | 3h | â–¡ |
| 6æ—¥ç›® | Zone 5 å®Ÿé¨“ + Zone 6 ç™ºå±• | 2h | â–¡ |
| 7æ—¥ç›® | ç·å¾©ç¿’ + FAQ + æ¬¡å›äºˆå‘Šèª­äº† | 1h | â–¡ |

### 7.5 æ¬¡å›äºˆå‘Š: Score Matching & Langevin Dynamics

**ç¬¬35å›ã®å†…å®¹**:

**å‹•æ©Ÿ**: EBMã®æ­£è¦åŒ–å®šæ•° $Z(\theta) = \int \exp(-E_\theta(x)) dx$ ã¯è¨ˆç®—ä¸èƒ½ã€‚ã ãŒã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ãªã‚‰ $Z(\theta)$ ãŒæ¶ˆãˆã‚‹:

$$
\nabla_x \log p(x) = \nabla_x \left[\log \exp(-E(x)) - \log Z\right] = -\nabla_x E(x)
$$

**å­¦ã¶ã“ã¨**:
1. **Score Function**: $\nabla_x \log p(x)$ ã®ç›´æ„Ÿã¨æ€§è³ª
2. **Score Matching**: Explicit / Denoising / Sliced Score Matching
3. **Langevin Dynamicså®Œå…¨ç‰ˆ**: Overdamped Langevin / é›¢æ•£åŒ– / SGLD
4. **NCSN**: Noise Conditional Score Networks / ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´
5. **Annealed Langevin**: ç²—â†’ç²¾ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
6. **åæŸæ€§ç†è«–**: Wassersteinè·é›¢ã§ã®åæŸãƒ¬ãƒ¼ãƒˆ
7. **Score â†’ Diffusion**: ç¬¬36å›DDPMã¸ã®æ©‹æ¸¡ã—

**æ¥ç¶š**: EBMã®è¨“ç·´å›°é›£æ€§ï¼ˆ$Z$ ã®è¨ˆç®—ï¼‰ã‚’å›é¿ã—ã€ã‚¹ã‚³ã‚¢é–¢æ•°ã ã‘ã§åˆ†å¸ƒã‚’å­¦ç¿’ â†’ Diffusion Modelsã®ç†è«–çš„åŸºç›¤ã¸ã€‚

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**ã€Œ"éºç‰©"ãŒ"æœªæ¥"ã ã£ãŸã®ã§ã¯ï¼Ÿã€**

1982å¹´Hopfield Network â†’ 2020å¹´Modern Hopfield = Attentionç­‰ä¾¡æ€§ã€‚40å¹´è¶Šã—ã®çµ±ä¸€ã€‚

2013-2020å¹´ã€VAE/GANãŒå…¨ç››ã§ã€EBMã¯"è¨“ç·´ãŒé›£ã—ã„éºç‰©"ã¨ã—ã¦å¿˜ã‚Œã‚‰ã‚ŒãŸã€‚

ã ãŒ2020-2026å¹´:
- **Modern Hopfield â†” Attentionç­‰ä¾¡æ€§**ï¼ˆ2020ï¼‰
- **2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³**ï¼ˆHopfield/Hintonï¼‰
- **Energy Matchingçµ±ä¸€ç†è«–**ï¼ˆ2025ï¼‰
- **NRGPT: GPT = EBM**ï¼ˆ2025ï¼‰
- **Kona 1.0å•†ç”¨åŒ–**ï¼ˆ2026ï¼‰

**å•ã„**:
- EBMã¯"éºç‰©"ã ã£ãŸã®ã‹ã€ãã‚Œã¨ã‚‚"æ™‚ä»£ãŒè¿½ã„ã¤ã„ã¦ã„ãªã‹ã£ãŸ"ã®ã‹ï¼Ÿ
- VAE/GANã¯"é€²åŒ–"ã ã£ãŸã®ã‹ã€ãã‚Œã¨ã‚‚"EBMã®è¨“ç·´å›°é›£æ€§ã‹ã‚‰ã®é€ƒé¿"ã ã£ãŸã®ã‹ï¼Ÿ
- 2026å¹´ã®Flow Matching / Diffusionã®èƒŒå¾Œã«ã‚ã‚‹çµ±ä¸€ç†è«–ã¯ã€å®Ÿã¯1982å¹´ã®HopfieldãŒæ—¢ã«ç¤ºã—ã¦ã„ãŸã®ã§ã¯ï¼Ÿ

:::details è€ƒå¯Ÿã®ãƒ’ãƒ³ãƒˆ

**æ­´å²çš„ã‚µã‚¤ã‚¯ãƒ«**:
- 1982: Hopfield â†’ "ç”»æœŸçš„"
- 1985-2006: Boltzmann/RBM â†’ "Deep Learningã®åŸºç¤"
- 2013-2020: VAE/GAN â†’ "EBMã¯é…ã„ã€ä½¿ãˆãªã„"
- 2020-2026: Modern Hopfield/Energy Matching â†’ "å…¨ã¦ã¯çµ±ä¸€ã•ã‚Œã¦ã„ãŸ"

**æŠ€è¡“çš„æœ¬è³ª**:
- VAE: EBMã®è¿‘ä¼¼ï¼ˆELBO = å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰
- GAN: EBMã®æš—é»™çš„å­¦ç¿’ï¼ˆåˆ¤åˆ¥å™¨ = ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ï¼‰
- Diffusion: EBMã®ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹å­¦ç¿’ï¼ˆScore Matchingï¼‰
- Flow Matching: EBM + OTã®çµ±ä¸€ï¼ˆEnergy Matchingï¼‰

**çµè«–**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å…¨ã¦ã¯EBMã®å¤‰å½¢ã€‚"éºç‰©"ã§ã¯ãªã"åŸºç›¤"ã ã£ãŸã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Hopfield, J. J. (1982). "Neural networks and physical systems with emergent collective computational abilities." *Proceedings of the National Academy of Sciences*, 79(8), 2554-2558.
@[card](https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554)

[^2]: Hinton, G. E. (2002). "Training products of experts by minimizing contrastive divergence." *Neural Computation*, 14(8), 1771-1800.
@[card](https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf)

[^3]: Ramsauer, H., et al. (2020). "Hopfield Networks is All You Need." *ICLR 2021*.
@[card](https://arxiv.org/abs/2008.02217)

[^4]: Santos, S., et al. (2025). "Modern Hopfield Networks with Continuous-Time Memories." *arXiv:2502.10122*.
@[card](https://arxiv.org/abs/2502.10122)

[^5]: Dehmamy, N., et al. (2025). "NRGPT: An Energy-based Alternative for GPT." *arXiv:2512.16762*.
@[card](https://arxiv.org/abs/2512.16762)

[^6]: Energy Matching Authors (2025). "Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling." *arXiv:2504.10612*.
@[card](https://arxiv.org/abs/2504.10612)

[^7]: Tieleman, T. (2008). "Training restricted Boltzmann machines using approximations to the likelihood gradient." *ICML 2008*.

[^8]: Hoffman, M. D., & Gelman, A. (2014). "The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo." *Journal of Machine Learning Research*, 15(1), 1593-1623.

[^9]: Smolensky, P. (1986). "Information processing in dynamical systems: Foundations of harmony theory." In *Parallel Distributed Processing*, Vol. 1.

[^10]: Nobel Prize (2024). "The Nobel Prize in Physics 2024." John J. Hopfield and Geoffrey E. Hinton.
@[card](https://www.nobelprize.org/prizes/physics/2024/summary/)

[^11]: LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). "A tutorial on energy-based learning." In *Predicting Structured Data*, MIT Press.

### æ•™ç§‘æ›¸

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Chapter on EBMs]
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [Chapter 20: Deep Generative Models]
- MacKay, D. J. C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. [Chapter on Boltzmann Machines]
- Barber, D. (2012). *Bayesian Reasoning and Machine Learning*. Cambridge University Press. [Chapter on EBMs]

---

## è¨˜æ³•è¦ç´„

| è¨˜æ³• | æ„å‘³ |
|:-----|:-----|
| $E_\theta(x)$ | ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ï¼‰ |
| $p_\theta(x)$ | ç¢ºç‡åˆ†å¸ƒï¼ˆGibbsåˆ†å¸ƒï¼‰ |
| $Z(\theta)$ | æ­£è¦åŒ–å®šæ•°ï¼ˆPartition Functionï¼‰ |
| $v$ | RBMå¯è¦–å±¤ |
| $h$ | RBMéš ã‚Œå±¤ |
| $W$ | RBMé‡ã¿è¡Œåˆ— |
| $\xi^i$ | Hopfieldè¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ |
| $\beta$ | é€†æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| $\tau$ | æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| $T(x' \| x)$ | Markové€£é–é·ç§»ã‚«ãƒ¼ãƒãƒ« |
| $\alpha(x' \| x)$ | Metropolis-Hastingså—ç†ç¢ºç‡ |
| $H(q, p)$ | Hamiltonianï¼ˆHamiltoné–¢æ•°ï¼‰ |
| $U(q)$ | ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼ |
| $K(p)$ | é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼ |
| $\epsilon$ | ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º |
| $L$ | Leapfrog stepsæ•° |
---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
