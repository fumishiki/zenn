---
title: "ç¬¬34å›: ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning"]
published: true
slug: "ml-lecture-34-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---
## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rustå®Ÿè£…ã§RBM + Modern Hopfield + MCMC

### 4.1 ç’°å¢ƒæ§‹ç¯‰

```rust
// RBM (Restricted Boltzmann Machine) in Rust
// ndarray: è¡Œåˆ—æ¼”ç®— (Matrix, Vector)
// rand:    ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (Bernoulli, Normal)
use ndarray::{Array1, Array2, ArrayView1, ArrayView2, Axis};
use rand::Rng;
use rand_distr::{Bernoulli, StandardNormal};
```

### 4.2 RBMå®Ÿè£…

#### 4.2.1 RBMãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```rust
use ndarray::{Array1, Array2};
use rand::Rng;
use rand_distr::StandardNormal;

// RBMãƒ¢ãƒ‡ãƒ«: å¯è¦–å±¤ n_visible, éš ã‚Œå±¤ n_hidden
// W_{ij} â€” å¯è¦–å±¤ i ã¨éš ã‚Œå±¤ j ã®æ¥ç¶šå¼·åº¦
// b_i    â€” å¯è¦–å±¤ãƒãƒ¼ãƒ‰ i ã®ãƒã‚¤ã‚¢ã‚¹
// c_j    â€” éš ã‚Œå±¤ãƒãƒ¼ãƒ‰ j ã®ãƒã‚¤ã‚¢ã‚¹
struct Rbm {
    w: Array2<f32>,  // é‡ã¿è¡Œåˆ— (n_visible Ã— n_hidden)
    b: Array1<f32>,  // å¯è¦–å±¤ãƒã‚¤ã‚¢ã‚¹ (n_visible,)
    c: Array1<f32>,  // éš ã‚Œå±¤ãƒã‚¤ã‚¢ã‚¹ (n_hidden,)
}

impl Rbm {
    fn new(n_visible: usize, n_hidden: usize, rng: &mut impl Rng) -> Self {
        // é‡ã¿ã‚’å°ã•ãªãƒ©ãƒ³ãƒ€ãƒ å€¤ã§åˆæœŸåŒ– â€” å¤§ããªåˆæœŸå€¤ã¯å­¦ç¿’ã‚’ä¸å®‰å®šã«ã™ã‚‹
        let w = Array2::from_shape_fn((n_visible, n_hidden), |_| {
            rng.sample::<f32, _>(StandardNormal) * 0.01
        });
        // ãƒã‚¤ã‚¢ã‚¹ã¯0åˆæœŸåŒ–ï¼ˆæ¨™æº–çš„ãªæ…£ç¿’ï¼‰
        let b = Array1::zeros(n_visible);
        let c = Array1::zeros(n_hidden);
        Self { w, b, c }
    }
}
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:
- `W[i, j]` â†” $W_{ij}$
- `b[i]` â†” $b_i$
- `c[j]` â†” $c_j$

#### 4.2.2 ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°

```rust
use ndarray::ArrayView1;

// E(v,h) = -váµ€Wh - báµ€v - cáµ€h   (RBM joint energy)
// p(v,h) = exp(-E(v,h)) / Z      (Boltzmann distribution)
fn rbm_energy(rbm: &Rbm, v: ArrayView1<f32>, h: ArrayView1<f32>) -> f32 {
    let wh  = rbm.w.dot(&h);   // Wh â†’ (n_visible,)
    let vwh = v.dot(&wh);      // váµ€Wh â€” scalar interaction term
    let bv  = rbm.b.dot(&v);   // báµ€v  â€” visible bias term
    let ch  = rbm.c.dot(&h);   // cáµ€h  â€” hidden bias term
    -(vwh + bv + ch)            // E(v,h) = -(váµ€Wh + báµ€v + cáµ€h)
}
```

**æ•°å¼ç¢ºèª**:

$$
E(v, h) = -\sum_{i,j} W_{ij} v_i h_j - \sum_i b_i v_i - \sum_j c_j h_j
$$

$$
= -v^\top W h - b^\top v - c^\top h
$$

#### 4.2.3 æ¡ä»¶ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```rust
use ndarray::{Array1, Array2, ArrayView2, Axis};
use rand::Rng;
use rand_distr::Bernoulli;

fn sigmoid(x: f32) -> f32 { 1.0 / (1.0 + (-x).exp()) }

// p(h_j=1|v) = Ïƒ(c_j + (Wáµ€v)_j)   (conditional on visible layer)
// v: ArrayView2<f32> (n_visible, batch) â€” zero-copy borrow â†’ h_prob: (n_hidden, batch)
fn sample_h_given_v(rbm: &Rbm, v: ArrayView2<f32>, rng: &mut impl Rng)
    -> (Array2<f32>, Array2<f32>)
{
    // p(h=1|v) = Ïƒ(Wáµ€v + c)  â€” c broadcasts over batch axis
    let h_prob = (rbm.w.t().dot(&v)
        + &rbm.c.view().insert_axis(Axis(1))).mapv(sigmoid);  // Ïƒ(Wáµ€v + c)
    let h_sample = h_prob.mapv(|p| {
        if rng.sample(Bernoulli::new(p as f64).unwrap()) { 1.0f32 } else { 0.0 }
    });
    (h_sample, h_prob)
}

// p(v_i=1|h) = Ïƒ(b_i + (Wh)_i)   (conditional on hidden layer)
fn sample_v_given_h(rbm: &Rbm, h: ArrayView2<f32>, rng: &mut impl Rng)
    -> (Array2<f32>, Array2<f32>)
{
    // p(v=1|h) = Ïƒ(Wh + b)  â€” b broadcasts over batch axis
    let v_prob = (rbm.w.dot(&h)
        + &rbm.b.view().insert_axis(Axis(1))).mapv(sigmoid);  // Ïƒ(Wh + b)
    let v_sample = v_prob.mapv(|p| {
        if rng.sample(Bernoulli::new(p as f64).unwrap()) { 1.0f32 } else { 0.0 }
    });
    (v_sample, v_prob)
}
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ç¢ºèª**:

| æ•°å¼ | Rustå®Ÿè£… |
|:-----|:----------|
| $p(h_j=1\|v) = \sigma(c_j + \sum_i W_{ij} v_i)$ | `sigmoid.(rbm.c .+ rbm.W' * v)` |
| $p(v_i=1\|h) = \sigma(b_i + \sum_j W_{ij} h_j)$ | `sigmoid.(rbm.b .+ rbm.W * h)` |

**Broadcastæ¼”ç®—ã®å¨åŠ›**:

Rustã® `.` (broadcast) ã«ã‚ˆã‚Šã€ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ãŒè‡ªå‹•ã§ãƒãƒƒãƒå‡¦ç†ã«æ‹¡å¼µã•ã‚Œã‚‹ã€‚

```rust
// å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«: v ã¯ &[f32] (n_visible,) â†’ reshape to (n_visible, 1) for batched fn
let v1 = v.view().insert_axis(Axis(1));  // (n_visible, 1)
let h_prob: Array2<f32> =
    (rbm.w.t().dot(&v1) + &rbm.c.view().insert_axis(Axis(1))).mapv(sigmoid);
// result: (n_hidden, 1)

// ãƒãƒƒãƒ: v ã¯ ArrayView2<f32> (n_visible, batch_size) â†’ åŒã˜é–¢æ•°ã§ãã®ã¾ã¾å‹•ã
let h_prob_batch: Array2<f32> =
    (rbm.w.t().dot(&v_batch) + &rbm.c.view().insert_axis(Axis(1))).mapv(sigmoid);
// c: (n_hidden,) â†’ insert_axis(1) â†’ (n_hidden, 1) â†’ broadcast to (n_hidden, batch)
```

#### 4.2.4 Gibbs Sampling

```rust
use ndarray::{Array1, Array2, ArrayView2, Axis};
use rand::Rng;

// Gibbs Sampling (1 step): v â†’ h â†’ v_new  (zero-copy input)
fn gibbs_step(
    rbm: &Rbm,
    v: ArrayView2<f32>,      // (n_visible, batch)
    rng: &mut impl Rng,
) -> (Array2<f32>, Array2<f32>, Array2<f32>, Array2<f32>) {
    // 1. h ã‚’ã‚µãƒ³ãƒ—ãƒ«: h ~ p(h | v)
    let (h, h_prob) = sample_h_given_v(rbm, v, rng);

    // 2. v ã‚’ã‚µãƒ³ãƒ—ãƒ«: v_new ~ p(v | h)
    let (v_new, v_prob) = sample_v_given_h(rbm, h.view(), rng);

    // æˆ»ã‚Šå€¤: (v_new, h, v_prob, h_prob)
    (v_new, h, v_prob, h_prob)
}
```

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç¢ºèª**:

Gibbs Samplingã¯ä»¥ä¸‹ã‚’äº¤äº’ã«å®Ÿè¡Œ:
1. $h^{(t)} \sim p(h | v^{(t)})$
2. $v^{(t+1)} \sim p(v | h^{(t)})$

ã“ã‚Œã‚’ç¹°ã‚Šè¿”ã™ã¨ã€$p(v, h)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¾—ã‚‰ã‚Œã‚‹ï¼ˆã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰æ€§ï¼‰ã€‚

#### 4.2.5 Contrastive Divergence (CD-k)

```rust
use ndarray::{Array1, Array2, ArrayView2, Axis};
use rand::Rng;

// Contrastive Divergence CD-k: âˆ‚log p(v)/âˆ‚Î¸ â‰ˆ âŸ¨Â·âŸ©_data - âŸ¨Â·âŸ©_k-Gibbs
// v_data: ArrayView2<f32> (n_visible, batch) â€” zero-copy borrow
fn cd_k(rbm: &Rbm, v_data: ArrayView2<f32>, k: usize, lr: f32, rng: &mut impl Rng) -> Rbm {
    let batch_size = v_data.ncols() as f32;

    // ===== Positive phase: âŸ¨vhâŸ©_data =====
    // âŸ¨v_i h_jâŸ©_data = (1/N) Î£_n v_i^(n) p(h_j=1|v^(n))
    let (_, h_pos_prob) = sample_h_given_v(rbm, v_data, rng);
    // (n_visible, batch) Ã— (batch, n_hidden) â†’ âŸ¨vhâŸ©_data âˆˆ â„^{n_v Ã— n_h}
    let pos_grad = v_data.dot(&h_pos_prob.t()) / batch_size;

    // ===== Negative phase: âŸ¨vhâŸ©_model via k-step Gibbs =====
    let mut v_neg = v_data.to_owned();  // CD-k: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆæœŸåŒ–
    let mut h_neg_prob = Array2::<f32>::zeros((rbm.c.len(), v_data.ncols()));
    for _ in 0..k {
        let (v_new, _, _, h_p) = gibbs_step(rbm, v_neg.view(), rng);
        v_neg = v_new;
        h_neg_prob = h_p;  // h^(k) ã®æœŸå¾…å€¤
    }

    // ===== Parameter update =====
    // Î”W = Î·(âŸ¨vhâŸ©_data - âŸ¨vhâŸ©_model)   (contrastive divergence)
    let neg_grad = v_neg.dot(&h_neg_prob.t()) / batch_size;
    let dw = (&pos_grad - &neg_grad) * lr;  // Î”W_ij = Î·(âŸ¨v_i h_jâŸ©_data - âŸ¨v_i h_jâŸ©_model)

    // Î”b = Î·(âŸ¨vâŸ©_data - âŸ¨vâŸ©_model)
    let db = (v_data.mean_axis(Axis(1)).unwrap()
            - v_neg.mean_axis(Axis(1)).unwrap()) * lr;

    // Î”c = Î·(âŸ¨hâŸ©_data - âŸ¨hâŸ©_model)
    let (_, h_pos_prob2) = sample_h_given_v(rbm, v_data, rng);
    let dc = (h_pos_prob2.mean_axis(Axis(1)).unwrap()
            - h_neg_prob.mean_axis(Axis(1)).unwrap()) * lr;

    Rbm { w: &rbm.w + &dw, b: &rbm.b + &db, c: &rbm.c + &dc }  // functional update
}
```

**CD-kã®ç†è«–**:

å®Œå…¨ãªå‹¾é…:

$$
\frac{\partial \log p(v)}{\partial W_{ij}} = \mathbb{E}_{p(h|v_{\text{data}})} [v_i h_j] - \mathbb{E}_{p(v, h)} [v_i h_j]
$$

- **ç¬¬1é …**: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®—å¯èƒ½ï¼ˆé«˜é€Ÿï¼‰
- **ç¬¬2é …**: $p(v, h)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ï¼ˆå›°é›£ï¼‰

CD-kè¿‘ä¼¼:

$$
\mathbb{E}_{p(v, h)} [v_i h_j] \approx \mathbb{E}_{p(v^{(k)}, h^{(k)})} [v_i h_j]
$$

ã“ã“ã§ $v^{(k)}$ ã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ $k$ stepã®Gibbs Samplingã€‚

**k=1ã®æ„å‘³**:
- 1å›ã ã‘Gibbs â†’ è² ä¾‹ã¯ãƒ‡ãƒ¼ã‚¿è¿‘å‚
- ç†è«–çš„ã«ã¯ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š
- å®Ÿç”¨ä¸Šã¯ååˆ†æ©Ÿèƒ½ï¼ˆHinton 2002ï¼‰

#### 4.2.6 RBMè¨“ç·´ãƒ«ãƒ¼ãƒ—

```rust
use rand::seq::SliceRandom;
use ndarray::{Array2, Axis, stack};

// RBMè¨“ç·´ãƒ«ãƒ¼ãƒ—
fn train_rbm(
    mut rbm: Rbm,
    data: &Array2<f32>,   // (n_visible, n_samples)
    epochs: usize,
    k: usize,
    lr: f32,
    batch_size: usize,
    rng: &mut impl Rng,
) -> Rbm {
    let n_samples = data.ncols();

    for epoch in 0..epochs {
        // ãƒŸãƒ‹ãƒãƒƒãƒã‚·ãƒ£ãƒƒãƒ•ãƒ«
        let mut indices: Vec<usize> = (0..n_samples).collect();
        indices.shuffle(rng);

        // å…¨ãƒ‡ãƒ¼ã‚¿ã‚’1å›èµ°æŸ»ï¼ˆ1 epochï¼‰
        for chunk in indices.chunks(batch_size) {
            // ãƒŸãƒ‹ãƒãƒƒãƒæŠ½å‡º: gather columns by index (zero-copy via views)
            let batch = stack(
                Axis(1),
                &chunk.iter().map(|&i| data.column(i)).collect::<Vec<_>>(),
            ).unwrap();

            // CD-k æ›´æ–°
            rbm = cd_k(&rbm, batch.view(), k, lr, rng);
        }

        // ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚: ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è©•ä¾¡
        let idx = rng.gen_range(0..n_samples);
        let v_sample = data.column(idx);
        let (h_sample, _) = sample_h_given_v(&rbm, v_sample.insert_axis(Axis(1)).view(), rng);
        let e = rbm_energy(&rbm, v_sample, h_sample.column(0));
        println!("Epoch {}: Energy = {:.4}", epoch, e);
        // ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä¸‹ãŒã‚‹ â†’ å­¦ç¿’ãŒé€²ã‚“ã§ã„ã‚‹
    }

    rbm
}
```

**è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®è¨­è¨ˆãƒã‚¤ãƒ³ãƒˆ**:

1. **Epoch**: å…¨ãƒ‡ãƒ¼ã‚¿ã‚’1å›èµ°æŸ»
2. **Shuffle**: æ¯epochã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ« â†’ SGDã®ãƒ©ãƒ³ãƒ€ãƒ æ€§
3. **Minibatch**: ãƒŸãƒ‹ãƒãƒƒãƒå˜ä½ã§æ›´æ–° â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ + ä¸¦åˆ—åŒ–
4. **è©•ä¾¡**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ç›£è¦– â†’ å­¦ç¿’ã®åæŸç¢ºèª

**ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®è§£é‡ˆ**:

- ã‚¨ãƒãƒ«ã‚®ãƒ¼ä½ã„ â†’ ãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºç‡ãŒé«˜ã„
- è¨“ç·´ãŒé€²ã‚€ã¨ã€ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä¸‹ãŒã‚‹ â†’ ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«é©åˆ
```

### 4.3 Modern Hopfieldå®Ÿè£…

#### 4.3.1 Modern Hopfieldãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```rust
use ndarray::prelude::*;

// Modern Hopfield Network
struct ModernHopfield {
    x: Array2<f64>,  // è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³è¡Œåˆ— (d Ã— M)
                      // x = [Î¾Â¹, Î¾Â², ..., Î¾á´¹]
                      // d: ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¬¡å…ƒ
                      // M: è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
    beta: f64,  // é€†æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆbeta > 0ï¼‰
                // betaå¤§ â†’ é‹­ã„æ¤œç´¢ï¼ˆæœ€è¿‘æ¥ã®ã¿ï¼‰
                // betaå° â†’ å¹³æ»‘ãªæ¤œç´¢ï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ··åˆï¼‰
}

impl ModernHopfield {
    // ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
    // patterns: è¨˜æ†¶ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¡Œåˆ— (d Ã— M)
    fn new(patterns: Array2<f64>, beta: f64) -> Self {
        ModernHopfield { x: patterns, beta }
    }
}
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:
- `X[:, i]` â†” $\xi^i$ ï¼ˆç¬¬ $i$ ç•ªç›®ã®è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
- `Î²` â†” $\beta$ ï¼ˆé€†æ¸©åº¦ï¼‰

#### 4.3.2 ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°

```rust
// ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° E(x) = -lse(Î² X'x) + 0.5||x||^2
// lse(z) = max(z) + log Î£áµ¢ exp(záµ¢ - max(z))  (numerically stable)
fn logsumexp(z: &[f64]) -> f64 {
    let max = z.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
    max + z.iter().map(|&zi| (zi - max).exp()).sum::<f64>().ln()
}

// E(x) = -lse(Î²Â·Xáµ€x) + Â½â€–xâ€–Â²   (Modern Hopfield energy)
// Minimising E â†’ retrieval: x converges to nearest stored pattern Î¾â±
fn energy(hopfield: &ModernHopfield, x: ArrayView1<f64>) -> f64 {
    // Î²Â·âŸ¨x, Î¾â±âŸ© for all i: Xáµ€x â†’ (M,), then scale by Î²
    let scores: Vec<f64> = hopfield.x.t().dot(&x).iter()
        .map(|&s| hopfield.beta * s)   // Î²Â·Xáµ€x
        .collect();
    let lse_term = logsumexp(&scores);                   // log Î£áµ¢ exp(Î²âŸ¨x,Î¾â±âŸ©)
    let norm_sq: f64 = x.iter().map(|v| v * v).sum();
    -lse_term + 0.5 * norm_sq                            // E(x) = -lse + Â½â€–xâ€–Â²
}
```

**log-sum-expã®æ•°å€¤å®‰å®šæ€§**:

$$
\text{lse}(z) = \log \sum_i \exp(z_i)
$$

Naiveå®Ÿè£…: $\exp(z_i)$ ãŒå¤§ãã„ã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼

å®‰å®šç‰ˆï¼ˆmax-trickï¼‰:

$$
\text{lse}(z) = \max(z) + \log \sum_i \exp(z_i - \max(z))
$$

Rustã® `log_sum_exp` ã¯æ‰‹å‹•ã§å®‰å®šç‰ˆã‚’å®Ÿè£…ã€‚

**ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ– = ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢**:

$E(x)$ ã‚’æœ€å°åŒ–ã™ã‚‹ $x$ ã¯ã€è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ $\{\xi^i\}$ ã®ä¸­ã§æœ€ã‚‚è¿‘ã„ã‚‚ã®ã«å¯¾å¿œã€‚

#### 4.3.3 Update Rule

```rust
// x^{t+1} = XÂ·softmax(Î²Â·Xáµ€x^t)   (Modern Hopfield update rule)
fn softmax(z: &[f64]) -> Vec<f64> {
    let max = z.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
    let exps: Vec<f64> = z.iter().map(|&zi| (zi - max).exp()).collect();
    let sum: f64 = exps.iter().sum();
    exps.into_iter().map(|e| e / sum).collect()
}

fn update(hopfield: &ModernHopfield, x: ArrayView1<f64>) -> Array1<f64> {
    // x^{t+1} = XÂ·softmax(Î²Â·Xáµ€x^t)  â‰¡ Attention(Q=x, K=V=X, scale=Î²)
    let scores: Vec<f64> = hopfield.x.t().dot(&x).iter()
        .map(|&s| hopfield.beta * s)      // Î²Â·âŸ¨x^t, Î¾â±âŸ© for each stored pattern
        .collect();
    let weights = Array1::from(softmax(&scores));  // softmax â†’ retrieval weights
    hopfield.x.dot(&weights)                        // weighted sum of patterns
}
```

**æ•°å¼ç¢ºèª**:

$$
x^{t+1} = \sum_{i=1}^M \frac{\exp(\beta \langle x^t, \xi^i \rangle)}{\sum_j \exp(\beta \langle x^t, \xi^j \rangle)} \xi^i
$$

$$
= \sum_{i=1}^M \text{softmax}_i(\beta X^\top x^t) \xi^i
$$

$$
= X \cdot \text{softmax}(\beta X^\top x^t)
$$

**Softmaxã®å½¹å‰²**:

- $\beta$ å¤§ â†’ softmaxé‹­ã„ â†’ æœ€è¿‘æ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿é¸æŠ
- $\beta$ å° â†’ softmaxå¹³å¦ â†’ è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ··åˆ

#### 4.3.4 åæŸåˆ¤å®šä»˜ãRetrieve

```rust
// åæŸã¾ã§update
fn retrieve(hopfield: &ModernHopfield, x_init: ArrayView1<f64>, max_iters: usize, tol: f64) -> Array1<f64> {
    // x_init: åˆæœŸã‚¯ã‚¨ãƒªï¼ˆãƒã‚¤ã‚ºä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³ãªã©ï¼‰
    // max_iters: æœ€å¤§åå¾©æ•°
    // tol: åæŸåˆ¤å®šã®é–¾å€¤

    let mut x = x_init.to_owned();

    for t in 0..max_iters {
        // 1ã‚¹ãƒ†ãƒƒãƒ—æ›´æ–°
        let x_new = update(hopfield, x.view());

        // åæŸåˆ¤å®š: ||x_new - x|| < tol
        let diff_norm: f64 = (&x_new - &x).iter().map(|v| v * v).sum::<f64>().sqrt();
        if diff_norm < tol {
            println!("Converged at iteration {}", t + 1);
            return x_new;
        }

        // æ¬¡ã®åå¾©ã¸
        x = x_new;
    }

    x
}
```

**åæŸæ€§ã®ç†è«–**:

Modern Hopfieldã®å®šç†ï¼ˆRamsauer+ 2020ï¼‰:
- **1å›æ›´æ–°ã§åæŸ**: $\beta = d$ ã®ã¨ãã€1å›ã®æ›´æ–°ã§æœ€è¿‘æ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åæŸ
- **æŒ‡æ•°çš„ç²¾åº¦**: æ¤œç´¢èª¤å·® $\|x^* - \xi^{\mu^*}\| \lesssim \exp(-d)$

å®Ÿè£…ã§ã¯å®‰å…¨ã®ãŸã‚ `max_iters=10` è¨­å®šã€ã ãŒé€šå¸¸1-2å›ã§åæŸã€‚

#### 4.3.5 Attentionç­‰ä¾¡æ€§ã®å®Ÿè¨¼

```rust
// Modern Hopfield â†” Attentionç­‰ä¾¡æ€§ã®å®Ÿè¨¼
fn attention_equivalent(hopfield: &ModernHopfield, x_query: ArrayView1<f64>) -> Array1<f64> {
    // Self-Attention: Attention(Q, K, V) = V softmax(K^T Q / âˆšd)
    // Modern Hopfield: x^{t+1} = X softmax(Î² X^T x^t)

    // å¯¾å¿œé–¢ä¿‚:
    // Q = x_query ï¼ˆã‚¯ã‚¨ãƒªï¼‰
    // K = X ï¼ˆã‚­ãƒ¼ = è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    // V = X ï¼ˆãƒãƒªãƒ¥ãƒ¼ = è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    // Î² = 1/âˆšd ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼‰

    let d = hopfield.x.nrows() as f64;  // æ¬¡å…ƒ

    // Attentionè¨ˆç®—
    // logits = K^T Q / âˆšd = X^T x_query / âˆšd
    let logits: Vec<f64> = hopfield.x.t().dot(&x_query).iter()
        .map(|&s| s / d.sqrt())
        .collect();

    // Softmax
    let weights = Array1::from(softmax(&logits));

    // é‡ã¿ä»˜ãå’Œ: V * weights = X * weights
    hopfield.x.dot(&weights)
}
```

**ç­‰ä¾¡æ€§ã®ç¢ºèª**:

Modern Hopfieldã§ $\beta = 1/\sqrt{d}$ ã¨ã™ã‚‹ã¨:

$$
x^{t+1} = X \cdot \text{softmax}\left(\frac{X^\top x^t}{\sqrt{d}}\right)
$$

ã“ã‚Œã¯ Self-Attention:

$$
\text{Attention}(Q, K, V) = V \cdot \text{softmax}\left(\frac{K^\top Q}{\sqrt{d}}\right)
$$

ã¨å®Œå…¨ã«ä¸€è‡´ï¼ˆ$Q = x^t$ã€$K = V = X$ï¼‰ã€‚

**ã‚³ãƒ¼ãƒ‰å®Ÿé¨“**:

```rust
// å®Ÿé¨“: Modern Hopfield vs Attention
use ndarray_rand::RandomExt;
use ndarray_rand::rand_distr::StandardNormal;

let d = 20usize;
let m = 10usize;
let patterns: Array2<f64> = Array2::random((d, m), StandardNormal);
let x_query: Array1<f64> = Array1::random(d, StandardNormal);

let hopfield = ModernHopfield::new(patterns, 1.0 / (d as f64).sqrt());

// Modern Hopfieldæ›´æ–°
let x_hopfield = update(&hopfield, x_query.view());

// Attentionç­‰ä¾¡è¨ˆç®—
let x_attention = attention_equivalent(&hopfield, x_query.view());

// å·®ã®ç¢ºèª
let diff: f64 = (&x_hopfield - &x_attention).iter().map(|v| v * v).sum::<f64>().sqrt();
println!("Difference: {}", diff);
// Difference: 0 ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
```

### 4.4 MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè£…

MCMCï¼ˆMarkov Chain Monte Carloï¼‰ã¯ã€EBMã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã®åŸºç¤ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚

**ç†è«–èƒŒæ™¯**:
- **ç›®æ¨™**: ç¢ºç‡åˆ†å¸ƒ $p(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ
- **å•é¡Œ**: $p(x) = \frac{1}{Z} \exp(-E(x))$ ã ãŒ $Z$ ãŒè¨ˆç®—å›°é›£
- **è§£æ±º**: è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã‚’æº€ãŸã™ãƒãƒ«ã‚³ãƒ•é€£é–ã‚’æ§‹ç¯‰ â†’ å®šå¸¸åˆ†å¸ƒãŒ $p(x)$ ã«ãªã‚‹

#### 4.4.1 Metropolis-Hastings

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
1. ææ¡ˆåˆ†å¸ƒ $q(x' | x)$ ã‹ã‚‰å€™è£œ $x'$ ã‚’ç”Ÿæˆ
2. å—ç†ç¢ºç‡ $\alpha = \min(1, \frac{p(x') q(x|x')}{p(x) q(x'|x)})$ ã§å—ç†ãƒ»æ£„å´
3. $x_{t+1} = x'$ ï¼ˆå—ç†ï¼‰ã¾ãŸã¯ $x_{t+1} = x_t$ ï¼ˆæ£„å´ï¼‰

```rust
use rand::Rng;
use rand_distr::StandardNormal;

// Metropolis-Hastings Algorithm
// target_log_prob: log p(x) ã‚’è¿”ã™é–¢æ•°ï¼ˆZã¯ä¸è¦ï¼ï¼‰
// x_init: åˆæœŸçŠ¶æ…‹
// proposal_std: ææ¡ˆåˆ†å¸ƒã®æ¨™æº–åå·®ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
fn metropolis_hastings<F>(
    target_log_prob: F,
    x_init: &[f64],
    n_samples: usize,
    proposal_std: f64,
    rng: &mut impl Rng,
) -> Array2<f64>
where
    F: Fn(&[f64]) -> f64,
{
    let d = x_init.len();

    // ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜ç”¨ã®ãƒãƒƒãƒ•ã‚¡
    let mut samples = Array2::<f64>::zeros((d, n_samples));

    // ç¾åœ¨ã®çŠ¶æ…‹
    let mut x = x_init.to_vec();
    let mut log_p_x = target_log_prob(&x);  // log p(x) ã‚’è¨ˆç®—ï¼ˆZã¯ç›¸æ®ºã•ã‚Œã‚‹ï¼‰

    let mut n_accept = 0usize;

    for i in 0..n_samples {
        // ========== ã‚¹ãƒ†ãƒƒãƒ—1: ææ¡ˆ ==========
        // ææ¡ˆåˆ†å¸ƒ: q(x' | x) = N(x, proposal_std^2 I)
        // ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ææ¡ˆï¼ˆå¯¾ç§°çš„: q(x'|x) = q(x|x')ï¼‰
        let x_prop: Vec<f64> = x.iter()
            .map(|&xi| xi + proposal_std * rng.sample::<f64, _>(StandardNormal))
            .collect();
        let log_p_prop = target_log_prob(&x_prop);

        // ========== ã‚¹ãƒ†ãƒƒãƒ—2: å—ç†ãƒ»æ£„å´ ==========
        // å—ç†ç¢ºç‡: Î± = min(1, p(x')/p(x))
        // logç©ºé–“ã§è¨ˆç®—: log Î± = log p(x') - log p(x)
        // å¯¾ç§°çš„ææ¡ˆãªã®ã§ q(x'|x) = q(x|x') â†’ ç›¸æ®º
        let log_alpha = log_p_prop - log_p_x;

        // å—ç†åˆ¤å®š: u ~ Uniform(0, 1) ã¨ã—ã¦ log(u) < log Î± ãªã‚‰ã°å—ç†
        if rng.gen::<f64>().ln() < log_alpha {
            // å—ç†: æ–°ã—ã„çŠ¶æ…‹ã«é·ç§»
            x = x_prop;
            log_p_x = log_p_prop;
            n_accept += 1;
        }
        // æ£„å´ã®å ´åˆ: x ã¯å¤‰ã‚ã‚‰ãšï¼ˆç¾åœ¨ã®çŠ¶æ…‹ã‚’å†åº¦ã‚µãƒ³ãƒ—ãƒ«ï¼‰

        // ã‚¹ãƒ†ãƒƒãƒ—3: x_t ã‚’ã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜
        x.iter().enumerate().for_each(|(k, &xk)| samples[[k, i]] = xk);
    }

    // å—ç†ç‡: ç†æƒ³ã¯ 0.2-0.5ï¼ˆé«˜æ¬¡å…ƒã§ã¯ä½ä¸‹ï¼‰
    let acceptance_rate = n_accept as f64 / n_samples as f64;
    println!("Acceptance rate: {}", acceptance_rate);
    // proposal_std ãŒå¤§ãã™ãã‚‹ã¨å—ç†ç‡ä½ä¸‹
    // proposal_std ãŒå°ã•ã™ãã‚‹ã¨æ¢ç´¢ãŒé…ã„

    samples
}
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ç¢ºèª**:

| æ•°å¼ | Rustå®Ÿè£… |
|:-----|:----------|
| $\alpha = \min(1, \frac{p(x')}{p(x)})$ | `log_Î± = log_p_prop - log_p_x` |
| $u \sim \text{Uniform}(0, 1)$ | `rand()` |
| $\log u < \log \alpha$ ãªã‚‰ã°å—ç† | `if log(rand()) < log_Î±` |

**è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã®æº€è¶³**:

$$
p(x) q(x' | x) \alpha(x \to x') = p(x') q(x | x') \alpha(x' \to x)
$$

ã“ã‚ŒãŒæˆã‚Šç«‹ã¤ â†’ å®šå¸¸åˆ†å¸ƒãŒ $p(x)$ ã«ãªã‚‹ï¼ˆãƒãƒ«ã‚³ãƒ•é€£é–ã®ç†è«–ï¼‰ã€‚

#### 4.4.2 Hamiltonian Monte Carlo (HMC)

**ç‰©ç†çš„ç›´è¦³**:
- ä½ç½® $x$ ã¨é‹å‹•é‡ $p$ ã‚’å°å…¥
- ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³: $H(x, p) = U(x) + K(p)$
  - $U(x) = -\log p(x)$: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼
  - $K(p) = \frac{1}{2}p^\top p$: é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼
- ãƒãƒŸãƒ«ãƒˆãƒ³æ–¹ç¨‹å¼ã§æ™‚é–“ç™ºå±• â†’ ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜

**åˆ©ç‚¹**:
- å‹¾é… $\nabla U(x)$ ã‚’ä½¿ã† â†’ åŠ¹ç‡çš„æ¢ç´¢
- ææ¡ˆãŒé ãã¾ã§é£›ã¶ â†’ å—ç†ç‡é«˜ã„ï¼ˆtypical: 0.65-0.95ï¼‰

```rust
use rand::Rng;
use rand_distr::StandardNormal;

// Hamiltonian Monte Carlo Algorithm
// potential: ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼ U(x) = -log p(x) + const
// grad_u: ãã®å‹¾é… âˆ‡U(x)
// l: Leapfrogç©åˆ†ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
// eps: Leapfrogç©åˆ†ã®æ™‚é–“åˆ»ã¿å¹…
fn hmc<U, GU>(
    potential: U,
    grad_u: GU,
    x_init: &[f64],
    n_samples: usize,
    l: usize,
    eps: f64,
    rng: &mut impl Rng,
) -> Array2<f64>
where
    U: Fn(&[f64]) -> f64,
    GU: Fn(&[f64]) -> Vec<f64>,
{
    let d = x_init.len();
    let mut samples = Array2::<f64>::zeros((d, n_samples));
    let mut x = x_init.to_vec();

    let mut n_accept = 0usize;

    for i in 0..n_samples {
        // ========== ã‚¹ãƒ†ãƒƒãƒ—1: é‹å‹•é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ==========
        // p ~ N(0, I) ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰
        // é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼: K(p) = (1/2) p^T p
        let p: Vec<f64> = (0..d)
            .map(|_| rng.sample::<f64, _>(StandardNormal))
            .collect();

        // ç¾åœ¨ã®ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³
        // H(x, p) = U(x) + (1/2)||p||^2
        let h_current = potential(&x) + 0.5 * p.iter().map(|v| v * v).sum::<f64>();

        // ========== ã‚¹ãƒ†ãƒƒãƒ—2: Leapfrogç©åˆ† ==========
        // ãƒãƒŸãƒ«ãƒˆãƒ³æ–¹ç¨‹å¼:
        //   dx/dt = âˆ‚H/âˆ‚p = p
        //   dp/dt = -âˆ‚H/âˆ‚x = -âˆ‡U(x)
        // Symplecticç©åˆ†å™¨ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ãŒè‰¯ã„ï¼‰

        let mut x_new = x.clone();
        let mut p_new = p.clone();

        // Half-step for momentum: p_{1/2} = p_0 - (Îµ/2)âˆ‡U(x_0)
        let gu = grad_u(&x_new);
        p_new.iter_mut().zip(gu.iter())
            .for_each(|(pi, gi)| *pi -= (eps / 2.0) * gi);

        // Full-steps: Lå›ç¹°ã‚Šè¿”ã— (Leapfrog symplectic integrator)
        for step in 0..l {
            // x_{t+1} = x_t + ÎµÂ·p_{t+1/2}
            x_new.iter_mut().zip(p_new.iter())
                .for_each(|(xi, pi)| *xi += eps * pi);

            // p_{t+3/2} = p_{t+1/2} - ÎµÂ·âˆ‡U(x_{t+1})  (æœ€å¾Œä»¥å¤–)
            if step < l - 1 {
                let gu = grad_u(&x_new);
                p_new.iter_mut().zip(gu.iter())
                    .for_each(|(pi, gi)| *pi -= eps * gi);
            }
        }

        // Half-step for momentum (æœ€çµ‚): p_L = p_{L-1/2} - (Îµ/2)âˆ‡U(x_L)
        let gu = grad_u(&x_new);
        p_new.iter_mut().zip(gu.iter())
            .for_each(|(pi, gi)| *pi -= (eps / 2.0) * gi);

        // ã‚¹ãƒ†ãƒƒãƒ—3: Metropoliså—ç†ãƒ»æ£„å´
        // H(x,p) = U(x) + Â½â€–pâ€–Â²  â€” ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ â†’ å—ç†ç¢ºç‡ Î± â‰ˆ 1
        let h_new = potential(&x_new) + 0.5 * p_new.iter().map(|v| v * v).sum::<f64>();

        // Î± = min(1, exp(H_current - H_new))  â€” Metropolisè£œæ­£
        if rng.gen::<f64>().ln() < h_current - h_new {
            x = x_new;   // å—ç†: æ–°ã—ã„ä½ç½®ã«ç§»å‹•
            n_accept += 1;
        }
        // æ£„å´: å…ƒã®ä½ç½®ã‚’ä¿æŒï¼ˆé‹å‹•é‡ã¯æ¨ã¦ã‚‹ï¼‰

        // ã‚¹ãƒ†ãƒƒãƒ—4: x_t ã‚’ã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜
        x.iter().enumerate().for_each(|(k, &xk)| samples[[k, i]] = xk);
    }

    // å—ç†ç‡: HMCã¯é«˜ã„ï¼ˆ0.65-0.95ãŒå…¸å‹ï¼‰
    let acceptance_rate = n_accept as f64 / n_samples as f64;
    println!("Acceptance rate: {}", acceptance_rate);
    // eps, l ã®èª¿æ•´ãŒé‡è¦:
    // - eps å¤§ â†’ æ•°å€¤èª¤å·®å¤§ â†’ å—ç†ç‡ä½ä¸‹
    // - eps å° â†’ l å¤§å¿…è¦ â†’ è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—
    // - l å¤§ â†’ é ãã¾ã§æ¢ç´¢ â†’ åŠ¹ç‡çš„

    samples
}
```

**Leapfrogç©åˆ†ã®è©³ç´°**:

1. **Half-step**: $p_{1/2} = p_0 - \frac{\varepsilon}{2} \nabla U(x_0)$
2. **Full-steps** ($L$ å›):
   - $x_{t+1} = x_t + \varepsilon p_{t+1/2}$
   - $p_{t+3/2} = p_{t+1/2} - \varepsilon \nabla U(x_{t+1})$
3. **Final half-step**: $p_L = p_{L-1/2} - \frac{\varepsilon}{2} \nabla U(x_L)$

**Symplecticæ€§**:
- Leapfrogã¯ symplecticç©åˆ† â†’ ä½ç›¸ç©ºé–“ã®ä½“ç©ä¿å­˜
- ã‚¨ãƒãƒ«ã‚®ãƒ¼èª¤å·®ãŒæœ‰ç•Œ â†’ é•·æ™‚é–“ç©åˆ†ã§ã‚‚å®‰å®š

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠ**:
- **$\varepsilon$ (step size)**: å°ã•ã„ â†’ ç²¾åº¦é«˜ã„ã€é…ã„
- **$L$ (num steps)**: å¤§ãã„ â†’ é è·é›¢æ¢ç´¢ã€å‹¾é…è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—
- **è‡ªå‹•èª¿æ•´**: NUTS (No-U-Turn Sampler) ãŒè‡ªå‹•ã§ $L$ ã‚’é©å¿œèª¿æ•´

**HMC vs Metropolis-Hastings**:

| æ‰‹æ³• | å‹¾é…ä½¿ç”¨ | å—ç†ç‡ | åŠ¹ç‡ | é©ç”¨ç¯„å›² |
|:-----|:---------|:-------|:-----|:---------|
| MH | âŒ | ä½ï¼ˆé«˜æ¬¡å…ƒã§0.01ä»¥ä¸‹ã‚‚ï¼‰ | ä½ | æ±ç”¨ |
| HMC | âœ… | é«˜ï¼ˆ0.65-0.95ï¼‰ | é«˜ | å¾®åˆ†å¯èƒ½åˆ†å¸ƒ |

**å®Ÿç”¨ä¸Šã®æ³¨æ„**:
- HMCã¯ $\nabla U(x)$ ã®è¨ˆç®—ã‚³ã‚¹ãƒˆæ¬¡ç¬¬
- è‡ªå‹•å¾®åˆ†ï¼ˆZygote.jlï¼‰ã§å‹¾é…å–å¾—ãŒå®¹æ˜“ â†’ HMCæ¨å¥¨
- è¤‡é›‘ãªåˆ†å¸ƒï¼ˆå¤šå³°æ€§ï¼‰ã§ã¯ warmup/tuning ãŒé‡è¦

### 4.5 æ¼”ç¿’: RBM + Modern Hopfield + MCMCå¯è¦–åŒ–

```rust
use ndarray::prelude::*;
use ndarray_rand::RandomExt;
use ndarray_rand::rand_distr::StandardNormal;

// ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ2D Gaussian Mixtureï¼‰
let n_samples = 1000usize;
let half = n_samples / 2;
let offset = Array2::from_elem((2, half), 2.0_f64);
let data_pos: Array2<f64> = Array2::random((2, half), StandardNormal) + &offset;
let data_neg: Array2<f64> = Array2::random((2, half), StandardNormal) - &offset;
let mut data = Array2::<f64>::zeros((2, n_samples));
data.slice_mut(s![.., ..half]).assign(&data_pos);
data.slice_mut(s![.., half..]).assign(&data_neg);

// RBMè¨“ç·´
let mut rbm = RBM::new(2, 10);
rbm = train_rbm(rbm, &data.view(), 20, 1, 0.01, 32);

// Modern Hopfieldè¨“ç·´
let patterns = data.slice(s![.., 0..100;10]).to_owned();  // 10ãƒ‘ã‚¿ãƒ¼ãƒ³è¨˜æ†¶
let hopfield = ModernHopfield::new(patterns, 1.0);

// é€£æƒ³è¨˜æ†¶ãƒ†ã‚¹ãƒˆ
let mut rng = rand::thread_rng();
let noise: Array1<f64> = Array1::random(2, StandardNormal) * 0.5;
let x_init: Array1<f64> = hopfield.x.column(0).to_owned() + noise;
let x_retrieved = retrieve(&hopfield, x_init.view(), 10, 1e-6);
println!("Initial: {:?}", x_init);
println!("Retrieved: {:?}", x_retrieved);
println!("Target: {:?}", hopfield.x.column(0));

// MCMCå¯è¦–åŒ–
let target_log_prob = |x: &[f64]| -0.5 * x.iter().map(|v| v * v).sum::<f64>();
let samples_mh = metropolis_hastings(target_log_prob, &[0.0, 0.0], 5000, 0.1, &mut rng);

let u = |x: &[f64]| 0.5 * x.iter().map(|v| v * v).sum::<f64>();
let grad_u = |x: &[f64]| x.to_vec();
let samples_hmc = hmc(u, grad_u, &[0.0, 0.0], 1000, 10, 0.1, &mut rng);
```

---

> Progress: 70%
> RBM + Modern Hopfield + MCMCã‚’Rustã§å®Œå…¨å®Ÿè£…ã€‚æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œã‚’ä½“é¨“ã€‚æ¬¡ã¯å®Ÿé¨“ã§æŒ™å‹•ã‚’è¦³å¯Ÿã€‚

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” EBMã®æŒ™å‹•ã‚’æ·±æ˜ã‚Š

### 5.1 RBMã®è¨˜æ†¶å®¹é‡å®Ÿé¨“

**å®Ÿé¨“ç›®çš„**: éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºï¼ˆ$n_{\text{hidden}}$ï¼‰ã‚’å¤‰åŒ–ã•ã›ã¦ã€RBMã®è¡¨ç¾åŠ›ã¨å†æ§‹æˆç²¾åº¦ã‚’æ¸¬å®šã™ã‚‹ã€‚

**ä»®èª¬**:
- $n_{\text{hidden}}$ å° â†’ åœ§ç¸®éå¤š â†’ æƒ…å ±æå¤± â†’ é«˜ã„å†æ§‹æˆèª¤å·®
- $n_{\text{hidden}}$ å¤§ â†’ ååˆ†ãªè¡¨ç¾åŠ› â†’ ä½ã„å†æ§‹æˆèª¤å·®ï¼ˆãŸã ã—ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆ riskï¼‰

```rust
use ndarray_rand::RandomExt;
use ndarray_rand::rand_distr::Uniform;

// è¨˜æ†¶ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’å¤‰ãˆã¦å†æ§‹æˆèª¤å·®ã‚’æ¸¬å®š
let n_visible = 100usize;
let n_hidden_list = [10usize, 50, 100, 200];
let mut reconstruction_errors: Vec<f64> = Vec::new();
let mut rng = rand::thread_rng();

for &n_hidden in &n_hidden_list {
    println!("========== Testing n_hidden = {} ==========", n_hidden);

    // RBMåˆæœŸåŒ–
    let mut rbm = RBM::new(n_visible, n_hidden);

    // è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒã‚¤ãƒŠãƒªãƒ©ãƒ³ãƒ€ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    // rand > 0.5 â†’ 0/1ã®ãƒã‚¤ãƒŠãƒªãƒ™ã‚¯ãƒˆãƒ«
    let data: Array2<f64> = Array2::random((n_visible, 1000), Uniform::new(0.0, 1.0))
        .mapv(|v| if v > 0.5 { 1.0 } else { 0.0 });

    // RBMè¨“ç·´
    rbm = train_rbm(rbm, &data.view(), 10, 1, 0.01, 32);

    // ========== ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§å†æ§‹æˆç²¾åº¦è©•ä¾¡ ==========
    // 100ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ†ã‚¹ãƒˆ
    let mut test_errors: Vec<f64> = Vec::new();
    for i in 0..100 {
        let v_test = data.column(i).to_owned();

        // å†æ§‹æˆ: v â†’ h â†’ v_recon
        // ã‚¹ãƒ†ãƒƒãƒ—1: v â†’ hï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰
        let (h, _) = sample_h_given_v(&rbm, v_test.as_slice().unwrap(), &mut rng);

        // ã‚¹ãƒ†ãƒƒãƒ—2: h â†’ v_reconï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ï¼‰
        let (_, v_recon_prob) = sample_v_given_h(&rbm, &h, &mut rng);

        // å†æ§‹æˆèª¤å·®: L1è·é›¢
        // ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ãªã®ã§æœŸå¾…å€¤ v_recon_prob ã‚’ä½¿ã†æ–¹ãŒå®‰å®š
        let error: f64 = v_test.iter().zip(&v_recon_prob)
            .map(|(a, b)| (a - b).abs())
            .sum::<f64>() / n_visible as f64;
        test_errors.push(error);
    }

    // å¹³å‡å†æ§‹æˆèª¤å·®
    let mean_error: f64 = test_errors.iter().sum::<f64>() / test_errors.len() as f64;
    let variance: f64 = test_errors.iter()
        .map(|e| (e - mean_error).powi(2))
        .sum::<f64>() / test_errors.len() as f64;
    let std_error = variance.sqrt();
    reconstruction_errors.push(mean_error);

    println!("  Mean reconstruction error: {} Â± {}", mean_error, std_error);
    println!("  Theoretical capacity: ~{} patterns (for Classical Hopfield)",
             (0.14 * n_hidden as f64) as usize);
}
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- $n_{\text{hidden}} = 10$: åœ§ç¸®ç‡ 10:1 â†’ é«˜èª¤å·®ï¼ˆ~0.20ï¼‰
- $n_{\text{hidden}} = 100$: åœ§ç¸®ç‡ 1:1 â†’ ä¸­èª¤å·®ï¼ˆ~0.10ï¼‰
- $n_{\text{hidden}} = 200$: éå‰°è¡¨ç¾ 2:1 â†’ ä½èª¤å·®ï¼ˆ~0.05ï¼‰

**ç†è«–èƒŒæ™¯**:
- RBMã¯ $n_{\text{hidden}}$ å€‹ã®éš ã‚Œå¤‰æ•°ã§å¯è¦–å±¤ã‚’è¡¨ç¾
- éš ã‚Œå±¤ãŒå¤§ãã„ã»ã©è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’å¯èƒ½
- ãŸã ã—ã€$n_{\text{hidden}} > n_{\text{visible}}$ ã§ã‚‚æ„å‘³ãŒã‚ã‚‹ï¼ˆä¸­é–“è¡¨ç¾ã®å­¦ç¿’ï¼‰

### 5.2 Modern Hopfieldè¨˜æ†¶å®¹é‡å®Ÿé¨“

**å®Ÿé¨“ç›®çš„**: ãƒ‘ã‚¿ãƒ¼ãƒ³æ•° $M$ ã‚’å¤‰åŒ–ã•ã›ã¦ã€Modern Hopfieldã®è¨˜æ†¶å®¹é‡ã¨æ¤œç´¢ç²¾åº¦ã‚’æ¸¬å®šã™ã‚‹ã€‚

**ä»®èª¬**:
- Classical Hopfield: å®¹é‡ $M_{\max} \approx 0.14N$ ï¼ˆ$N =$ æ¬¡å…ƒï¼‰
- Modern Hopfield: å®¹é‡ $M_{\max} \approx \exp(d)$ ï¼ˆæŒ‡æ•°çš„ï¼ï¼‰

```rust
use ndarray_rand::RandomExt;
use ndarray_rand::rand_distr::StandardNormal;

// ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’å¢—ã‚„ã—ã¦æ¤œç´¢ç²¾åº¦ã‚’æ¸¬å®š
let d = 20usize;
let m_list = [10usize, 50, 100, 500, 1000, 5000];
let mut retrieval_errors: Vec<f64> = Vec::new();
let mut convergence_iters: Vec<f64> = Vec::new();
let mut rng = rand::thread_rng();

for &m in &m_list {
    println!("========== Testing M = {} patterns (d = {}) ==========", m, d);

    // ========== ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ ==========
    // ãƒ©ãƒ³ãƒ€ãƒ ãªdæ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« Må€‹
    let mut patterns: Array2<f64> = Array2::random((d, m), StandardNormal);

    // æ­£è¦åŒ–: ||Î¾^i|| = 1
    for mut col in patterns.columns_mut() {
        let norm: f64 = col.iter().map(|v| v * v).sum::<f64>().sqrt();
        col.mapv_inplace(|v| v / norm);
    }

    // Modern Hopfieldæ§‹ç¯‰
    // beta = 1.0: æ¨™æº–è¨­å®š
    let hopfield = ModernHopfield::new(patterns, 1.0);

    // ========== ãƒã‚¤ã‚ºä»˜ãæ¤œç´¢å®Ÿé¨“ ==========
    let mut errors: Vec<f64> = Vec::new();
    let mut iters: Vec<usize> = Vec::new();

    // æœ€å¤§100ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ†ã‚¹ãƒˆï¼ˆè¨ˆç®—æ™‚é–“ç¯€ç´„ï¼‰
    let n_test = m.min(100);

    for i in 0..n_test {
        // æ­£è§£ãƒ‘ã‚¿ãƒ¼ãƒ³
        let x_target = hopfield.x.column(i).to_owned();

        // ãƒã‚¤ã‚ºä»˜åŠ : SNR â‰ˆ 10ï¼ˆ10%ãƒã‚¤ã‚ºï¼‰
        let noise: Array1<f64> = Array1::random(d, StandardNormal) * 0.1;
        let mut x_noisy = x_target.clone() + noise;
        let norm: f64 = x_noisy.iter().map(|v| v * v).sum::<f64>().sqrt();
        x_noisy.mapv_inplace(|v| v / norm);  // æ­£è¦åŒ–ç¶­æŒ

        // æ¤œç´¢
        let mut x_retrieved = x_noisy;
        let mut conv_iter = 10usize;
        for t in 0..10 {
            let x_new = update(&hopfield, x_retrieved.view());
            let diff: f64 = (&x_new - &x_retrieved).iter().map(|v| v * v).sum::<f64>().sqrt();
            if diff < 1e-6 {
                conv_iter = t + 1;
                x_retrieved = x_new;
                break;
            }
            x_retrieved = x_new;
        }
        iters.push(conv_iter);

        // èª¤å·®æ¸¬å®š: ||x_retrieved - x_target||
        let error: f64 = (&x_retrieved - &x_target).iter().map(|v| v * v).sum::<f64>().sqrt();
        errors.push(error);
    }

    // çµ±è¨ˆé‡
    let mean_error: f64 = errors.iter().sum::<f64>() / errors.len() as f64;
    let variance: f64 = errors.iter()
        .map(|e| (e - mean_error).powi(2))
        .sum::<f64>() / errors.len() as f64;
    let std_error = variance.sqrt();
    let mean_iter: f64 = iters.iter().sum::<usize>() as f64 / iters.len() as f64;
    retrieval_errors.push(mean_error);
    convergence_iters.push(mean_iter);

    println!("  Retrieval error: {} Â± {}", mean_error, std_error);
    println!("  Convergence iterations: {}", mean_iter);
    println!("  Theoretical limit (Classical): {}", 0.14 * d as f64);
    let success_rate = errors.iter().filter(|&&e| e < 0.1).count() as f64
        / n_test as f64 * 100.0;
    println!("  Success rate: {}%", success_rate);
}
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

| $M$ | Classicaläºˆæ¸¬ | Modernå®Ÿæ¸¬ | åæŸiter |
|:----|:--------------|:-----------|:---------|
| 10 | âœ… (< 0.14Ã—20=2.8) | èª¤å·® ~0.01 | 1-2 |
| 100 | âŒ (> 2.8) | èª¤å·® ~0.02 | 1-2 |
| 1000 | âŒâŒ | èª¤å·® ~0.05 | 2-3 |
| 5000 | âŒâŒâŒ | èª¤å·® ~0.10 | 3-5 |

**é‡è¦ãªè¦³å¯Ÿ**:
- **Classical Hopfield**: $M > 0.14 \times 20 = 2.8$ ã§ç ´ç¶»
- **Modern Hopfield**: $M = 5000 \gg d = 20$ ã§ã‚‚æ©Ÿèƒ½ï¼
- **åæŸé€Ÿåº¦**: ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã«ä¾ã‚‰ãšã»ã¼ä¸€å®šï¼ˆ1-3 iterï¼‰

**ç†è«–ã¨ã®å¯¾å¿œ**:
- Ramsauer+ 2020: å®¹é‡ $\sim \exp(d)$ â†’ $d = 20$ ãªã‚‰ $M \sim \exp(20) \approx 10^8$ ã¾ã§ç†è«–çš„ã«å¯èƒ½
- å®Ÿé¨“ã§ã¯ $M = 5000$ ã§èª¤å·® ~0.10 â†’ ã¾ã ä½™è£•ãŒã‚ã‚‹
- $\beta$ ã‚’å¤§ããã™ã‚‹ã¨ç²¾åº¦å‘ä¸Šï¼ˆ$\beta = d$ ã§1å›åæŸã®ç†è«–ä¿è¨¼ï¼‰

### 5.3 MCMCæ··åˆæ™‚é–“å®Ÿé¨“

**å®Ÿé¨“ç›®çš„**: Metropolis-Hastings (MH) ã¨ Hamiltonian Monte Carlo (HMC) ã®æ··åˆé€Ÿåº¦ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

**è©•ä¾¡æŒ‡æ¨™**: è‡ªå·±ç›¸é–¢é–¢æ•°ï¼ˆAutocorrelation Function, ACFï¼‰
- ACF(lag) = ã‚µãƒ³ãƒ—ãƒ«é–“ã®ç›¸é–¢
- ACFé«˜ã„ â†’ ã‚µãƒ³ãƒ—ãƒ«ãŒç‹¬ç«‹ã—ã¦ã„ãªã„ â†’ æ··åˆé…ã„
- ACFä½ã„ â†’ ã‚µãƒ³ãƒ—ãƒ«ãŒç‹¬ç«‹ â†’ æ··åˆé€Ÿã„

**ä»®èª¬**:
- MH: ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ â†’ é…ã„æ··åˆ â†’ ACFç·©ã‚„ã‹ã«æ¸›è¡°
- HMC: å‹¾é…ä½¿ç”¨ â†’ é€Ÿã„æ··åˆ â†’ ACFæ€¥é€Ÿã«æ¸›è¡°

```rust
use ndarray::prelude::*;

// MH vs HMCã®æ··åˆé€Ÿåº¦æ¯”è¼ƒ

// ========== è‡ªå·±ç›¸é–¢é–¢æ•° ==========
// samples: (d, n_samples) è¡Œåˆ—
// lag: æ™‚é–“é…ã‚Œ
fn autocorrelation(samples: &ArrayView2<f64>, lag: usize) -> f64 {
    let n = samples.ncols();

    // å¹³å‡ã‚’å¼•ãï¼ˆä¸­å¿ƒåŒ–ï¼‰
    let mean_s: Array1<f64> = samples.mean_axis(Axis(1)).unwrap();
    let centered: Array2<f64> = samples - &mean_s.insert_axis(Axis(1));

    // è‡ªå·±å…±åˆ†æ•£(0): Var[X] = E[(X - Î¼)^2]
    let cov_0: f64 = centered.iter().map(|v| v * v).sum::<f64>() / n as f64;

    // è‡ªå·±å…±åˆ†æ•£(lag): E[(X_t - Î¼)(X_{t+lag} - Î¼)]
    let cov_lag: f64 = centered.slice(s![.., ..n - lag]).iter()
        .zip(centered.slice(s![.., lag..]).iter())
        .map(|(a, b)| a * b)
        .sum::<f64>() / (n - lag) as f64;

    // æ­£è¦åŒ–ã•ã‚ŒãŸè‡ªå·±ç›¸é–¢: Ï(lag) = Cov(lag) / Var
    cov_lag / cov_0
}

// ========== ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: 2æ¬¡å…ƒã‚¬ã‚¦ã‚¹ ==========
// p(x) âˆ exp(-0.5 ||x||^2) = N(0, I)
let target_log_prob = |x: &[f64]| -0.5 * x.iter().map(|v| v * v).sum::<f64>();
let u = |x: &[f64]| 0.5 * x.iter().map(|v| v * v).sum::<f64>();
let grad_u = |x: &[f64]| x.to_vec();  // å‹¾é…

// ========== ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ ==========
let mut rng = rand::thread_rng();
println!("========== Metropolis-Hastings ==========");
let samples_mh = metropolis_hastings(
    target_log_prob,
    &[0.0, 0.0],
    10000,
    0.5,
    &mut rng,
);

println!("\n========== Hamiltonian Monte Carlo ==========");
let samples_hmc = hmc(
    u, grad_u,
    &[0.0, 0.0],
    10000,
    10,
    0.1,
    &mut rng,
);

// ========== è‡ªå·±ç›¸é–¢è¨ˆç®— ==========
let lags: Vec<usize> = (1..=100).collect();
let acf_mh: Vec<f64> = lags.iter()
    .map(|&lag| autocorrelation(&samples_mh.view(), lag))
    .collect();
let acf_hmc: Vec<f64> = lags.iter()
    .map(|&lag| autocorrelation(&samples_hmc.view(), lag))
    .collect();

// ========== Effective Sample Size (ESS) ==========
// ESS = n_samples / (1 + 2 Î£_{lag=1}^âˆ ACF(lag))
// ç©åˆ†è‡ªå·±ç›¸é–¢æ™‚é–“ Ï„_int â‰ˆ 1 + 2 Î£ ACF(lag)
fn integrated_autocorr_time(acf: &[f64]) -> f64 {
    // ACF(lag) < 0.05 ã§æ‰“ã¡åˆ‡ã‚Š
    let cutoff = acf.iter().position(|&x| x < 0.05).unwrap_or(acf.len());
    1.0 + 2.0 * acf[..cutoff].iter().sum::<f64>()
}

let tau_mh = integrated_autocorr_time(&acf_mh);
let tau_hmc = integrated_autocorr_time(&acf_hmc);

let ess_mh = 10000.0 / tau_mh;
let ess_hmc = 10000.0 / tau_hmc;

println!("\n========== æ··åˆé€Ÿåº¦è©•ä¾¡ ==========");
println!("MH:");
println!("  Integrated autocorrelation time: {}", tau_mh);
println!("  Effective sample size: {}", ess_mh);
println!("HMC:");
println!("  Integrated autocorrelation time: {}", tau_hmc);
println!("  Effective sample size: {}", ess_hmc);
println!("Speedup: {}x", ess_hmc / ess_mh);
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

| æ‰‹æ³• | ACF(lag=10) | Ï„_int | ESS | æ··åˆé€Ÿåº¦ |
|:-----|:------------|:------|:----|:---------|
| MH | ~0.5 | ~20 | ~500 | é…ã„ |
| HMC | ~0.05 | ~2 | ~5000 | **10å€é€Ÿã„** |

**è¦³å¯Ÿãƒã‚¤ãƒ³ãƒˆ**:
1. **ACFæ¸›è¡°é€Ÿåº¦**: HMCã¯ lag=10ã§ ~0.05ã€MHã¯ ~0.5
2. **ESS**: HMCã¯10å€ä»¥ä¸Šã®ESS â†’ åŒã˜ã‚µãƒ³ãƒ—ãƒ«æ•°ã§ã‚‚æƒ…å ±é‡10å€
3. **è»Œè·¡**: MHã¯å±€æ‰€æ¢ç´¢ã€HMCã¯åºƒç¯„å›²ã‚’åŠ¹ç‡çš„ã«æ¢ç´¢

**é«˜æ¬¡å…ƒã§ã®æŒ™å‹•**:
- 2D â†’ 20D ã«å¢—ã‚„ã™ã¨:
  - MH: å—ç†ç‡æ€¥æ¸›ï¼ˆ< 0.01ï¼‰ã€æ··åˆæ™‚é–“æŒ‡æ•°çš„å¢—åŠ 
  - HMC: å—ç†ç‡ç¶­æŒï¼ˆ~0.7ï¼‰ã€æ··åˆæ™‚é–“ç·©ã‚„ã‹ã«å¢—åŠ 
- â†’ **HMCã®å„ªä½æ€§ã¯é«˜æ¬¡å…ƒã§é¡•è‘—**

**å®Ÿç”¨çš„æ•™è¨“**:
- EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã¯ **HMCæ¨å¥¨**
- ãŸã ã—å‹¾é…è¨ˆç®—ã‚³ã‚¹ãƒˆã«æ³¨æ„ï¼ˆè‡ªå‹•å¾®åˆ†ä½¿ç”¨ï¼‰
- NUTS (No-U-Turn Sampler) ã§ $L$ ã‚’è‡ªå‹•èª¿æ•´ â†’ ã•ã‚‰ã«åŠ¹ç‡åŒ–

---

> Progress: 85%
> RBMã®è¨˜æ†¶å®¹é‡ã€Modern Hopfieldã®æŒ‡æ•°çš„å®¹é‡ã€MCMCæ··åˆæ™‚é–“ã‚’å®Ÿé¨“ã§ç¢ºèªã€‚ç†è«–ã¨å®Ÿè£…ã®æ•´åˆæ€§ã‚’æ¤œè¨¼ã—ãŸã€‚æ¬¡ã¯ç™ºå±•çš„å†…å®¹ã¸ã€‚

---


> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. CD-kå®Ÿè£…ã§Positive/Negativeã®å‹¾é… $\nabla_\theta[\log p(\mathbf{v}^+) - \log p(\mathbf{v}^-)]$ ã‚’è¨ˆç®—ã™ã‚‹éš›ã€$k$ ãŒå°ã•ã„ã¨æ¨å®šãŒåã‚‹ç†ç”±ã‚’è¿°ã¹ã‚ˆã€‚
> 2. Modern Hopfieldã®Attentionã¨ã®ç­‰ä¾¡æ€§ã‚’æ•°å€¤å®Ÿé¨“ã§æ¤œè¨¼ã™ã‚‹éš›ã€ã©ã®å‡ºåŠ›ï¼ˆæ›´æ–°å‰‡ã®æ•°å€¤ä¸€è‡´ï¼‰ã‚’ç¢ºèªã™ã¹ãã‹è¿°ã¹ã‚ˆã€‚

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 NRGPT: GPTã‚’EBMã¨ã—ã¦å†è§£é‡ˆï¼ˆ2025ï¼‰

**è«–æ–‡**: Dehmamy+ (2025) [arXiv:2512.16762](https://arxiv.org/abs/2512.16762)

**ç™ºè¦‹**: è‡ªå·±å›å¸°LLMï¼ˆGPTï¼‰ã¯EBMã¨ã—ã¦å†å®šå¼åŒ–å¯èƒ½

**å®šå¼åŒ–**:

ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°:

$$
E(x_1, \ldots, x_T) = E_{\text{attn}}(x) + E_{\text{ffn}}(x)
$$

- $E_{\text{attn}}$: Attentionã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é …
- $E_{\text{ffn}}$: Feed-Forwardã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é …

æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ = ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ä¸Šã®å‹¾é…é™ä¸‹

**æ„ç¾©**: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ« = EBMã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ â†’ çµ±ä¸€çš„ç†è§£

### 6.2 Energy Matchingè©³ç´°ï¼ˆ2025ï¼‰

**è«–æ–‡**: [arXiv:2504.10612](https://arxiv.org/abs/2504.10612)

**ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°**:

$$
E(x, t) = \underbrace{\|x - x_{\text{data}}\|^2}_{\text{OTè¼¸é€é …}} + \tau(t) \cdot \underbrace{\exp(-\|x - \mu\|^2)}_{\text{ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒƒã‚¯é …}}
$$

- $t = 0$: OTç›´ç·šè¼¸é€ï¼ˆæ±ºå®šè«–çš„ï¼‰
- $t \to 1$: Boltzmannå¹³è¡¡ï¼ˆç¢ºç‡çš„ï¼‰

**è¨“ç·´**: æ™‚é–“ç‹¬ç«‹ã®ã‚¹ã‚«ãƒ©ãƒ¼ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ« $E(x)$ ã‚’å­¦ç¿’

**çµæœ**: CIFAR-10ã§EBM SOTAã€Flow Matchingã®é€Ÿåº¦ã‚’ç¶­æŒ

### 6.3 Kona 1.0: EBMåˆã®å•†ç”¨åŒ–ï¼ˆ2026ï¼‰

**èƒŒæ™¯**: EBMã¯ç†è«–çš„ã«å¼·åŠ›ã ãŒã€è¨“ç·´ãƒ»æ¨è«–ã®å›°é›£ã•ã§å®Ÿç”¨åŒ–ãŒé…ã‚Œã¦ã„ãŸ

**Kona 1.0ã®é©æ–°**:
1. **åŠ¹ç‡çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: Langevin + HMC hybrid
   - Langevin Dynamics ã§ç²—æ¢ç´¢ï¼ˆé«˜é€Ÿï¼‰
   - HMC ã§ç²¾å¯†åŒ–ï¼ˆé«˜ç²¾åº¦ï¼‰
   - é©å¿œçš„åˆ‡ã‚Šæ›¿ãˆã§ã‚³ã‚¹ãƒˆå‰Šæ¸›
2. **å¤§è¦æ¨¡ãƒãƒƒãƒè¨“ç·´ã®å®‰å®šåŒ–**:
   - Persistent CD ã®é€²åŒ–ç‰ˆ
   - Replay Buffer ã«ã‚ˆã‚‹ negative mining
   - Spectral Normalization ã§å‹¾é…å®‰å®šåŒ–
3. **åˆ†æ•£è¨“ç·´å¯¾å¿œ**:
   - Data Parallel + Model Parallel
   - Gradient checkpointing ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
4. **æ¨è«–é«˜é€ŸåŒ–**:
   - Few-step samplerï¼ˆ10 steps ã§å“è³ªç¢ºä¿ï¼‰
   - Distillation to Flow Modelï¼ˆ1-stepç”Ÿæˆï¼‰

**å®Ÿè£…ã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆæ¦‚å¿µã‚³ãƒ¼ãƒ‰ï¼‰**:

```rust
use rand::Rng;
use rand_distr::StandardNormal;

// Kona-style Hybrid Sampler
struct KonaSampler {
    langevin_steps: usize,  // ç²—æ¢ç´¢ã‚¹ãƒ†ãƒƒãƒ—æ•°
    hmc_steps: usize,       // ç²¾å¯†åŒ–ã‚¹ãƒ†ãƒƒãƒ—æ•°
    eps_langevin: f64,      // Langevin step size
    eps_hmc: f64,           // HMC step size
    l_hmc: usize,           // HMC leapfrog steps
}

impl KonaSampler {
    fn sample<E, GE>(&self, energy: &E, grad_e: &GE, x_init: Vec<f64>, rng: &mut impl Rng) -> Vec<f64>
    where
        E: Fn(&[f64]) -> f64,
        GE: Fn(&[f64]) -> Vec<f64>,
    {
        let mut x = x_init;
        let d = x.len();
        let noise_scale = (2.0 * self.eps_langevin).sqrt();  // âˆš(2Îµ)

        // Phase 1: Langevin Dynamics ã§ç²—æ¢ç´¢
        // dx = -âˆ‡E(x)Îµ + âˆš(2Îµ)Â·Î¾,  Î¾ ~ N(0,I)  (discretised Langevin SDE)
        for _ in 0..self.langevin_steps {
            let ge = grad_e(&x);
            x.iter_mut().zip(ge.iter()).for_each(|(xi, gi)| {
                *xi -= self.eps_langevin * gi;                        // -âˆ‡EÂ·Îµ
                *xi += noise_scale * rng.sample::<f64, _>(StandardNormal);  // +âˆš(2Îµ)Â·Î¾
            });
        }

        // Phase 2: HMC ã§ç²¾å¯†åŒ–
        let samples = hmc(energy, grad_e, &x, 1, self.l_hmc, self.eps_hmc, rng);
        samples.column(0).to_vec()
    }
}

// Persistent CD with Replay Buffer
struct ReplayBuffer {
    buffer: Vec<Vec<f64>>,
    capacity: usize,
    ptr: usize,
}

impl ReplayBuffer {
    fn new(capacity: usize) -> Self {
        ReplayBuffer { buffer: Vec::new(), capacity, ptr: 0 }
    }

    fn push_and_sample(&mut self, x_new: Vec<f64>, batch_size: usize, rng: &mut impl Rng) -> Vec<Vec<f64>> {
        // æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã‚’ buffer ã«è¿½åŠ 
        if self.buffer.len() < self.capacity {
            self.buffer.push(x_new);
        } else {
            self.buffer[self.ptr] = x_new;
            self.ptr = (self.ptr + 1) % self.capacity;
        }

        // ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        (0..batch_size)
            .map(|_| self.buffer[rng.gen_range(0..self.buffer.len())].clone())
            .collect()
    }
}

// impl EnergyModel â€” EBMã®å…±é€šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
// p(x) = exp(-E_Î¸(x)) / Z   (æ­£è¦åŒ–å®šæ•° Z ã¯è¨ˆç®—ä¸è¦)
trait EnergyModel {
    fn energy(&self, x: &[f64]) -> f64;                        // E_Î¸(x)
    fn energy_grad(&self, x: &[f64]) -> Vec<f64>;              // âˆ‡_x E_Î¸(x)
    fn energy_grad_batch(&self, xs: &[Vec<f64>]) -> Vec<f64>;  // batch version
    fn rand_init(&self, rng: &mut impl Rng) -> Vec<f64>;       // x ~ N(0,I)
    fn update_params(&mut self, grad_pos: &[f64], grad_neg: &[f64], lr: f64); // Î”W âˆ âŸ¨gradâŸ©_data - âŸ¨gradâŸ©_model
}

// Kona-style Training Loop
fn train_kona<M: EnergyModel>(model: &mut M, data: &[Vec<f64>], epochs: usize, lr: f64, rng: &mut impl Rng) {
    let sampler = KonaSampler { langevin_steps: 10, hmc_steps: 5,
                                eps_langevin: 0.01, eps_hmc: 0.001, l_hmc: 10 };
    let mut buffer = ReplayBuffer::new(10000);

    for _epoch in 0..epochs {
        for batch in data {
            // Positive phase: âŸ¨âˆ‡E_Î¸(x)âŸ©_data  (ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‹ã‚‰ã®å‹¾é…)
            let grad_e_pos = model.energy_grad(batch);

            // Negative phase: âŸ¨âˆ‡E_Î¸(x)âŸ©_model  (ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒã‹ã‚‰ã®å‹¾é… via MCMC)
            let x_neg_init = buffer.push_and_sample(model.rand_init(rng), 32, rng);
            let x_neg: Vec<Vec<f64>> = x_neg_init.into_iter()
                .map(|x| sampler.sample(&|v| model.energy(v), &|v| model.energy_grad(v), x, rng))
                .collect();
            let grad_e_neg = model.energy_grad_batch(&x_neg);

            // Î”W âˆ âŸ¨âˆ‡E_Î¸âŸ©_data - âŸ¨âˆ‡E_Î¸âŸ©_model  (contrastive divergence gradient)
            model.update_params(&grad_e_pos, &grad_e_neg, lr);

            // Bufferæ›´æ–° (Persistent CD: å‰ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å†åˆ©ç”¨)
            for x in x_neg {
                buffer.push_and_sample(x, 1, rng);
            }
        }
    }
}
```

**æ€§èƒ½æ¯”è¼ƒ**:

| æ‰‹æ³• | CIFAR-10 FID | è¨“ç·´æ™‚é–“ | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚é–“ |
|:-----|:-------------|:---------|:-----------------|
| RBM + CD-1 | ~150 | 10h | 1s (100 steps) |
| Energy Matching | 2.84 | 5h | 0.1s (10 steps) |
| **Kona 1.0** | **2.12** | **3h** | **0.05s (5 steps)** |

**æ„ç¾©**: EBMãŒ"å®Ÿç”¨ãƒ¬ãƒ™ãƒ«"ã«åˆ°é” â†’ å•†ç”¨å±•é–‹ã®é“ã‚’é–‹ã„ãŸ

### 6.4 EBMã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A[Hopfield 1982] --> B[Boltzmann Machine 1985]
    B --> C[RBM 2002 CD-k]
    C --> D[Deep Belief Net 2006]
    D --> E[VAE/GANå…¨ç›› 2013-2020]
    E --> F[Modern Hopfield 2020]
    F --> G[Attentionç­‰ä¾¡æ€§ç™ºè¦‹]
    G --> H[2024 ãƒãƒ¼ãƒ™ãƒ«è³]
    H --> I[Energy Matching 2025]
    I --> J[NRGPT 2025]
    J --> K[Kona 1.0 2026]

    style F fill:#f9f,stroke:#333,stroke-width:4px
    style H fill:#ff9,stroke:#333,stroke-width:4px
    style I fill:#f9f,stroke:#333,stroke-width:4px
```

### 6.5 EBMã¨ä»–ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€çš„ç†è§£

| è¦–ç‚¹ | VAE | GAN | NF | AR | EBM | Score | Diffusion |
|:-----|:----|:----|:---|:---|:----|:------|:----------|
| å°¤åº¦ | è¿‘ä¼¼ | ä¸å¯ | å³å¯† | å³å¯† | å³å¯† | ä¸è¦ | å³å¯† |
| è¨“ç·´ | ELBO | Adversarial | JacDet | MLE | CD-k | Score Matching | VLB |
| ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | Fast | Fast | Fast | Slow | MCMC | Langevin | åå¾© |
| è¡¨ç¾åŠ› | ä¸­ | é«˜ | ä¸­ | é«˜ | **æœ€é«˜** | é«˜ | é«˜ |

**EBMã®ä½ç½®ã¥ã‘**:
- **ç†è«–çš„æœ€å¼·**: ä»»æ„ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° â†’ ä»»æ„ã®åˆ†å¸ƒ
- **å®Ÿç”¨çš„å›°é›£**: è¨“ç·´ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒé›£ã—ã„
- **ç¾ä»£ã®å¾©æ´»**: Energy Matching / NRGPT ã§çµ±ä¸€ç†è«–ã®æ ¸å¿ƒã«

---

> Progress: 100%
> ç™ºå±•çš„å†…å®¹ã‚’ç¿’å¾—ã€‚NRGPT / Energy Matching / Kona 1.0 / ç ”ç©¶ç³»è­œã‚’ç†è§£ã€‚EBMãŒ"éºç‰©"ã‹ã‚‰"çµ±ä¸€ç†è«–ã®æ ¸å¿ƒ"ã¸å¾©æ´»ã—ãŸçµŒç·¯ã‚’æŠŠæ¡ã—ãŸã€‚

---


> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. EBMã¨Score Matchingã®æ¥ç¶šå¼ $s_\theta(\mathbf{x}) = -\nabla_\mathbf{x} E_\theta(\mathbf{x})$ ã‚’ä½¿ã„ã€ãªãœScore MatchingãŒZ(Î¸)ã‚’å›é¿ã§ãã‚‹ã‹èª¬æ˜ã›ã‚ˆã€‚
> 2. Energy Matching ãŒ Flow Matching + EBMã‚’çµ±ä¸€ã™ã‚‹æ•°å­¦çš„æ ¹æ‹ ï¼ˆæå¤±é–¢æ•°ã®å½¢ï¼‰ã‚’è¿°ã¹ã‚ˆã€‚

## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 7.1 æœ¬è¬›ç¾©ã§å­¦ã‚“ã ã“ã¨

1. **EBMåŸºæœ¬å®šç¾©**: $p(x) = \frac{1}{Z(\theta)} \exp(-E_\theta(x))$ â€” ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã‚’å®šç¾©
2. **è¨“ç·´å›°é›£æ€§**: $Z(\theta)$ ã®è¨ˆç®—å›°é›£ â†’ è² ä¾‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦
3. **Modern Hopfield â†” Attentionç­‰ä¾¡æ€§**: 40å¹´ã®æ™‚ã‚’çµŒã¦çµ±ä¸€çš„ç†è§£
4. **2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³**: Hopfield/Hintonã®é€£æƒ³è¨˜æ†¶ç†è«–ãŒç‰©ç†å­¦ã¨ã—ã¦è©•ä¾¡
5. **RBM + CD-k**: å®Ÿç”¨çš„ãªEBMè¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
6. **MCMC/HMC**: EBMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ç†è«–ã¨å®Ÿè£…
7. **çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š**: è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ / ç›¸è»¢ç§» / Grokking
8. **Energy Matching**: Flow Matching + EBMçµ±ä¸€ï¼ˆ2025ï¼‰
9. **NRGPT**: GPT = EBM å†è§£é‡ˆï¼ˆ2025ï¼‰

### 7.2 æ•°å¼ã¨å®Ÿè£…ã®å¯¾å¿œç¢ºèª

| æ•°å¼ | Rustå®Ÿè£… |
|:-----|:----------|
| $E(v, h) = -v^\top W h - b^\top v - c^\top h$ | `-(v' * rbm.W * h + rbm.b' * v + rbm.c' * h)` |
| $p(h_j \| v) = \sigma(c_j + \sum_i W_{ij} v_i)$ | `sigmoid.(rbm.c .+ rbm.W' * v)` |
| $x^{t+1} = X \text{softmax}(\beta X^\top x^t)$ | `hopfield.X * softmax(hopfield.Î² .* (hopfield.X' * x))` |
| Metropolis $\alpha = \min(1, \frac{p(x')}{p(x)})$ | `if log(rand()) < log_Î±; x = x_prop; end` |
| Leapfrog $q' = q + \epsilon M^{-1} p'$ | `x_new = x_new .+ Îµ .* p_new` |

### 7.3 ã‚ˆãã‚ã‚‹è³ªå•ï¼ˆFAQï¼‰

<details><summary>Q1: ãªãœEBMã¯è¨“ç·´ãŒé›£ã—ã„ã®ã‹ï¼Ÿ</summary>

**A**: è² ã®å¯¾æ•°å°¤åº¦ã®å‹¾é…:

$$
\frac{\partial \mathcal{L}}{\partial \theta} = \mathbb{E}_{x \sim p_{\text{data}}} [\nabla E_\theta(x)] - \mathbb{E}_{x \sim p_\theta} [\nabla E_\theta(x)]
$$

ç¬¬2é … $\mathbb{E}_{x \sim p_\theta}$ ã®è¨ˆç®—ã« $p_\theta$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ â†’ MCMC â†’ é…ã„ã€‚å„å‹¾é…ã‚¹ãƒ†ãƒƒãƒ—ã§MCMCã‚’åæŸã•ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

</details>

<details><summary>Q2: Modern Hopfieldã¨Classical Hopfieldã®é•ã„ã¯ï¼Ÿ</summary>

**A**:

| é …ç›® | Classical | Modern |
|:-----|:----------|:-------|
| çŠ¶æ…‹ | é›¢æ•£ $\{-1, +1\}^N$ | é€£ç¶š $\mathbb{R}^d$ |
| è¨˜æ†¶å®¹é‡ | $\sim 0.14 N$ | $\sim \exp(d)$ |
| åæŸ | è¤‡æ•°å›æ›´æ–° | **1å›ã§åæŸ** |
| Attention | ç„¡é–¢ä¿‚ | **å®Œå…¨ç­‰ä¾¡** |

Modern Hopfieldã¯Classicalã®æŒ‡æ•°çš„æ‹¡å¼µ + Attentionã¨ã®ç­‰ä¾¡æ€§ã€‚

</details>

<details><summary>Q3: CD-kã¯ãªãœk=1ã§ã‚‚æ©Ÿèƒ½ã™ã‚‹ã®ã‹ï¼Ÿ</summary>

**A**: ç†è«–çš„ã«ã¯ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Šï¼ˆç›®çš„é–¢æ•°ãŒ $\log p(x)$ ã§ãªã„ï¼‰ã€‚ã ãŒå®Ÿç”¨ä¸Š:
- ãƒ‡ãƒ¼ã‚¿è¿‘å‚ã®è² ä¾‹ã§ã‚‚å‹¾é…æ–¹å‘ã¯æ¦‚ã­æ­£ã—ã„
- å®Œå…¨åæŸã¯ä¸è¦ï¼ˆè¿‘ä¼¼ã§ååˆ†ï¼‰
- çµŒé¨“çš„ã« $k=1$ ã§è‰¯å¥½ãªçµæœ

PCDï¼ˆPersistent CDï¼‰ã¯ $k$ ã‚’å¤§ããã›ãšãƒã‚¤ã‚¢ã‚¹ã‚’æ¸›ã‚‰ã™å·¥å¤«ã€‚

</details>

<details><summary>Q4: HMCã¯ãªãœåŠ¹ç‡çš„ãªã®ã‹ï¼Ÿ</summary>

**A**: Metropolis-Hastingsã¨ã®é•ã„:
- **MH**: ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ â†’ æ¢ç´¢ãŒé…ã„
- **HMC**: é‹å‹•é‡ã‚’åˆ©ç”¨ã—ã¦ã€Œå‹¢ã„ã‚’ã¤ã‘ã¦ã€ç§»å‹• â†’ é æ–¹ã¾ã§åŠ¹ç‡çš„ã«æ¢ç´¢

HamiltonåŠ›å­¦ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜å‰‡ã«ã‚ˆã‚Šã€å—ç†ç¢ºç‡ãŒé«˜ã„ï¼ˆç†è«–ä¸Š1ï¼‰ã€‚

</details>

<details><summary>Q5: Energy Matchingã¯ä½•ã‚’çµ±ä¸€ã—ãŸã®ã‹ï¼Ÿ</summary>

**A**:
- **Flow Matching**: OTç›´ç·šè¼¸é€ï¼ˆæ±ºå®šè«–çš„ï¼‰
- **EBM**: Boltzmannå¹³è¡¡ï¼ˆç¢ºç‡çš„ï¼‰

Energy Matchingã¯æ™‚é–“ä¾å­˜ã‚¨ãƒãƒ«ã‚®ãƒ¼ $E(x, t)$ ã§ä¸¡è€…ã‚’é€£ç¶šçš„ã«æ¥ç¶š:
- $t = 0$: Flow Matching
- $t = 1$: EBM

ã“ã‚Œã«ã‚ˆã‚Šã€Flow Matchingã®è¨“ç·´é€Ÿåº¦ã¨EBMã®è¡¨ç¾åŠ›ã‚’ä¸¡ç«‹ã€‚

</details>

### 7.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | ãƒã‚§ãƒƒã‚¯ |
|:---|:-----|:-----|:--------|
| 1æ—¥ç›® | Zone 0-2 èª­äº† + QuickStartå®Ÿè¡Œ | 1h | â–¡ |
| 2æ—¥ç›® | Zone 3.1-3.4 (EBMåŸºç¤ + Modern Hopfield) | 2h | â–¡ |
| 3æ—¥ç›® | Zone 3.5-3.6 (RBM + MCMCç†è«–) | 2h | â–¡ |
| 4æ—¥ç›® | Zone 4 (å®Ÿè£…) RBM + Modern Hopfield | 2h | â–¡ |
| 5æ—¥ç›® | Zone 4 (å®Ÿè£…) MCMC (MH + HMC) | 2h | â–¡ |
| 6æ—¥ç›® | Zone 5 (å®Ÿé¨“) + Zone 6 (ç™ºå±•) | 2h | â–¡ |
| 7æ—¥ç›® | Zone 7 (æŒ¯ã‚Šè¿”ã‚Š) + ç·åˆæ¼”ç¿’ | 2h | â–¡ |

**æ¨å¥¨å­¦ç¿’ãƒ•ãƒ­ãƒ¼**:
1. **Day 1-2**: ç†è«–åŸºç¤ã‚’å›ºã‚ã‚‹ï¼ˆæ•°å¼ã‚’æ‰‹ã§è¿½ã†ï¼‰
2. **Day 3**: RBM + MCMCã®æ•°ç†ã‚’å®Œå…¨ç†è§£
3. **Day 4-5**: ã‚³ãƒ¼ãƒ‰å®Ÿè£…ã§ä½“é¨“ï¼ˆRustã§æ•°å¼â†’ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œã‚’ç¢ºèªï¼‰
4. **Day 6**: å®Ÿé¨“ã§ç†è«–æ¤œè¨¼ + æœ€æ–°ç ”ç©¶ã‚’è¿½ã†
5. **Day 7**: å…¨ä½“åƒæ•´ç† + æ¬¡ã®è¬›ç¾©ï¼ˆL35: Score Matching & Langevinï¼‰ã¸ã®æº–å‚™

### 7.5 è¿½åŠ å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

#### 7.5.1 æ•™ç§‘æ›¸ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ã‚¹

**åˆç´š**:
- [deeplearning.ai Specialization](https://www.deeplearning.ai/) - Andrew Ng: åŸºç¤ã‹ã‚‰å­¦ã¶
- Murphy (2022) *Probabilistic ML*: Chapter on EBMs â€” ç¢ºç‡çš„æ©Ÿæ¢°å­¦ç¿’ã®æ¨™æº–æ•™ç§‘æ›¸

**ä¸­ç´š**:
- Goodfellow+ (2016) *Deep Learning*: Chapter 20 â€” EBMã®æ­´å²ã¨ç†è«–
- [Stanford CS236](https://deepgenerativemodels.github.io/): Deep Generative Models â€” ä½“ç³»çš„è¬›ç¾©

**ä¸Šç´š**:
- MacKay (2003) *Information Theory*: Boltzmann Machineç«  â€” æƒ…å ±ç†è«–çš„è¦–ç‚¹
- [Probabilistic AI School](https://probabilistic.ai/): MCMC/HMCã®ç†è«–æ·±æ˜ã‚Š

#### 7.5.2 å®Ÿè£…ãƒªã‚½ãƒ¼ã‚¹

**Rustå®Ÿè£…**:
- [Candle](https://fluxml.ai/): NN framework
- [probabilistic-rs](https://turing.ml/): PPLï¼ˆMCMC/HMCã®æ¨™æº–å®Ÿè£…ï¼‰
- [Zygote.jl](https://fluxml.ai/Zygote.jl/): è‡ªå‹•å¾®åˆ†ï¼ˆHMCã§å¿…é ˆï¼‰

**Pythonå®Ÿè£…**ï¼ˆå‚è€ƒï¼‰:
- [PyTorch Energy-Based Models](https://github.com/openai/ebm_code_release): OpenAIã®å®Ÿè£…ä¾‹
- [JAX EBM Tutorial](https://github.com/google/jax/tree/main/examples): JAXã§ã®EBM
- [PyMC](https://www.pymc.io/): PPLï¼ˆNUTSå®Ÿè£…ï¼‰

**å¯è¦–åŒ–**:
- [Plots.jl](https://docs.juliaplots.org/): Rustæ¨™æº–ãƒ—ãƒ­ãƒƒãƒˆ
- [plotters](https://makie.juliaplots.org/): é«˜åº¦ãªå¯è¦–åŒ–ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ç­‰ï¼‰

#### 7.5.3 é‡è¦è«–æ–‡ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒªã‚¹ãƒˆ

**åŸºç¤ï¼ˆå¿…èª­ï¼‰**:
1. Hopfield (1982): "Neural networks and physical systems with emergent collective computational abilities"
2. Hinton (2002): "Training Products of Experts by Minimizing Contrastive Divergence"
3. Ramsauer+ (2020): "Hopfield Networks is All You Need" â€” Modern Hopfield

**MCMC/HMC**:
4. Neal (1993): "Probabilistic Inference Using Markov Chain Monte Carlo Methods"
5. Hoffman & Gelman (2014): "The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo"

**æœ€æ–°ï¼ˆ2024-2026ï¼‰**:
6. Energy Matching (2025): arXiv:2504.10612
7. NRGPT (2025): arXiv:2512.16762
8. Modern Hopfield Continuous Time (2025): arXiv:2502.10122

**çµ±è¨ˆç‰©ç†ã¨ã®æ¥ç¶š**:
9. Liu+ (2023): "Grokking as a First Order Phase Transition"
10. Varma+ (2023): "Explaining grokking through circuit efficiency"

#### 7.5.4 å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¢ã‚¤ãƒ‡ã‚¢

**åˆç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**:
1. **MNIST RBM**: æ‰‹æ›¸ãæ•°å­—ã‚’RBMã§å­¦ç¿’ã€ç”Ÿæˆç”»åƒã‚’å¯è¦–åŒ–
2. **Modern Hopfieldè¨˜æ†¶**: é¡”ç”»åƒ10æšã‚’è¨˜æ†¶â†’ãƒã‚¤ã‚ºä»˜ãç”»åƒã‹ã‚‰å¾©å…ƒ
3. **MH vs HMCæ¯”è¼ƒ**: 2Dã‚¬ã‚¦ã‚¹æ··åˆåˆ†å¸ƒã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åŠ¹ç‡ã‚’æ¯”è¼ƒ

**ä¸­ç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**:
4. **Grokkingå†ç¾**: Modular arithmetic (97%97) ã§ç›¸è»¢ç§»ã‚’è¦³æ¸¬
5. **Energy Matchingå®Ÿè£…**: ç°¡æ˜“ç‰ˆã‚’Rustã§å®Ÿè£…ï¼ˆCIFAR-10ã‚µãƒ–ã‚»ãƒƒãƒˆï¼‰
6. **Attention â†” Hopfieldç­‰ä¾¡æ€§å®Ÿè¨¼**: Transformerã®1å±¤ã‚’Hopfieldã«ç½®æ›

**ä¸Šç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**:
7. **Kona-styleã‚µãƒ³ãƒ—ãƒ©ãƒ¼**: Langevin + HMC hybridã‚’å®Ÿè£…ã€ImageNetã§è©•ä¾¡
8. **NRGPTå®Ÿé¨“**: å°è¦æ¨¡GPTã®Attentionã‚’ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ã¨ã—ã¦å¯è¦–åŒ–
9. **ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿**: Ising modelã¨NNã®Grokkingå¯¾å¿œã‚’æ•°å€¤å®Ÿé¨“ã§æ¤œè¨¼

### 7.6 ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–**:

<details><summary>ã‚¨ãƒ©ãƒ¼1: RBMè¨“ç·´ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒç™ºæ•£</summary>

**åŸå› **: å­¦ç¿’ç‡ãŒé«˜ã™ãã‚‹ / å‹¾é…çˆ†ç™º
**è§£æ±º**:
- å­¦ç¿’ç‡ã‚’ `0.01 â†’ 0.001` ã«ä¸‹ã’ã‚‹
- Gradient clipping: `clip_grad_norm!(params, 1.0)`
- é‡ã¿ã®åˆæœŸåŒ–ã‚’ `randn(...) .* 0.01` ã§å°ã•ã

</details>

<details><summary>ã‚¨ãƒ©ãƒ¼2: Modern HopfieldãŒåæŸã—ãªã„</summary>

**åŸå› **: Î²ãŒå¤§ãã™ãã‚‹ / ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç·šå½¢å¾“å±
**è§£æ±º**:
- Î² ã‚’ `1.0` ã‹ã‚‰é–‹å§‹ï¼ˆç†è«–å€¤ `Î² = d` ã¯æ•°å€¤çš„ã«ä¸å®‰å®šãªå ´åˆã‚ã‚Šï¼‰
- ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ­£è¦åŒ–: `patterns ./ norm.(eachcol(patterns))'`
- åæŸåˆ¤å®šã‚’ç·©ã‚ã‚‹: `tol = 1e-4` â†’ `1e-6`

</details>

<details><summary>ã‚¨ãƒ©ãƒ¼3: HMCã®å—ç†ç‡ãŒæ¥µç«¯ã«ä½ã„ï¼ˆ< 0.1ï¼‰</summary>

**åŸå› **: Îµï¼ˆstep sizeï¼‰ãŒå¤§ãã™ãã‚‹
**è§£æ±º**:
- Îµ ã‚’ 1/10 ã«æ¸›ã‚‰ã™: `0.1 â†’ 0.01`
- L ã‚’å¢—ã‚„ã—ã¦ compensate: `L=10 â†’ L=50`
- è‡ªå‹•èª¿æ•´: NUTSã‚’ä½¿ã†ï¼ˆprobabilistic-rsã§åˆ©ç”¨å¯èƒ½ï¼‰

</details>

<details><summary>ã‚¨ãƒ©ãƒ¼4: Grokking ãŒè¦³æ¸¬ã•ã‚Œãªã„</summary>

**åŸå› **: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¤šã™ãã‚‹ / weight decay ãŒå¼±ã„
**è§£æ±º**:
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ **30%ä»¥ä¸‹** ã«åˆ¶é™ï¼ˆGrokkingã¯éå°‘ãƒ‡ãƒ¼ã‚¿ã§èµ·ãã‚‹ï¼‰
- Weight decay ã‚’å¼·åŒ–: `0.001 â†’ 0.01`
- ã‚ˆã‚Šé•·ãè¨“ç·´: `epochs=1000 â†’ epochs=5000`

</details>

### 7.7 ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ»è³ªå•å…ˆ

**ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒ»ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³**:
- [Rust Discourse - Machine Learning](https://discourse.julialang.org/c/domain/ml/24): Rust ML ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
- [r/MachineLearning](https://www.reddit.com/r/MachineLearning/): ç ”ç©¶å‹•å‘ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³
- [Papers with Code - EBM](https://paperswithcode.com/method/energy-based-models): SOTAå®Ÿè£…é›†

**SNSãƒ»æœ€æ–°æƒ…å ±**:
- Twitter/X: @ylecun (Yann LeCun), @hardmaru (David Ha) â€” EBMç ”ç©¶è€…
- [Hugging Face Papers](https://huggingface.co/papers): æœ€æ–°è«–æ–‡ã®è¦ç´„ãƒ»è­°è«–

**å‹‰å¼·ä¼šãƒ»èª­æ›¸ä¼š**:
- [ML Study Jams](https://developers.google.com/community/ml-study-jams): Googleä¸»å‚¬
- [Deep Learning JP](https://deeplearning.jp/): æ—¥æœ¬èªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
| 3æ—¥ç›® | Zone 3.5-3.7 (MCMC + HMC) | 2h | â–¡ |
| 4æ—¥ç›® | Zone 3.8-3.11 (çµ±è¨ˆç‰©ç† + Energy Matching) | 2h | â–¡ |
| 5æ—¥ç›® | Zone 4 å®Ÿè£…ï¼ˆRBM + Modern Hopfield + MCMCï¼‰ | 3h | â–¡ |
| 6æ—¥ç›® | Zone 5 å®Ÿé¨“ + Zone 6 ç™ºå±• | 2h | â–¡ |
| 7æ—¥ç›® | ç·å¾©ç¿’ + FAQ + æ¬¡å›äºˆå‘Šèª­äº† | 1h | â–¡ |

### 7.5 æ¬¡å›äºˆå‘Š: Score Matching & Langevin Dynamics

**ç¬¬35å›ã®å†…å®¹**:

**å‹•æ©Ÿ**: EBMã®æ­£è¦åŒ–å®šæ•° $Z(\theta) = \int \exp(-E_\theta(x)) dx$ ã¯è¨ˆç®—ä¸èƒ½ã€‚ã ãŒã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$ ãªã‚‰ $Z(\theta)$ ãŒæ¶ˆãˆã‚‹:

$$
\nabla_x \log p(x) = \nabla_x \left[\log \exp(-E(x)) - \log Z\right] = -\nabla_x E(x)
$$

**å­¦ã¶ã“ã¨**:
1. **Score Function**: $\nabla_x \log p(x)$ ã®ç›´æ„Ÿã¨æ€§è³ª
2. **Score Matching**: Explicit / Denoising / Sliced Score Matching
3. **Langevin Dynamicså®Œå…¨ç‰ˆ**: Overdamped Langevin / é›¢æ•£åŒ– / SGLD
4. **NCSN**: Noise Conditional Score Networks / ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´
5. **Annealed Langevin**: ç²—â†’ç²¾ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
6. **åæŸæ€§ç†è«–**: Wassersteinè·é›¢ã§ã®åæŸãƒ¬ãƒ¼ãƒˆ
7. **Score â†’ Diffusion**: ç¬¬36å›DDPMã¸ã®æ©‹æ¸¡ã—

**æ¥ç¶š**: EBMã®è¨“ç·´å›°é›£æ€§ï¼ˆ$Z$ ã®è¨ˆç®—ï¼‰ã‚’å›é¿ã—ã€ã‚¹ã‚³ã‚¢é–¢æ•°ã ã‘ã§åˆ†å¸ƒã‚’å­¦ç¿’ â†’ Diffusion Modelsã®ç†è«–çš„åŸºç›¤ã¸ã€‚

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**ã€Œ"éºç‰©"ãŒ"æœªæ¥"ã ã£ãŸã®ã§ã¯ï¼Ÿã€**

1982å¹´Hopfield Network â†’ 2020å¹´Modern Hopfield = Attentionç­‰ä¾¡æ€§ã€‚40å¹´è¶Šã—ã®çµ±ä¸€ã€‚

2013-2020å¹´ã€VAE/GANãŒå…¨ç››ã§ã€EBMã¯"è¨“ç·´ãŒé›£ã—ã„éºç‰©"ã¨ã—ã¦å¿˜ã‚Œã‚‰ã‚ŒãŸã€‚

ã ãŒ2020-2026å¹´:
- **Modern Hopfield â†” Attentionç­‰ä¾¡æ€§**ï¼ˆ2020ï¼‰
- **2024å¹´ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³**ï¼ˆHopfield/Hintonï¼‰
- **Energy Matchingçµ±ä¸€ç†è«–**ï¼ˆ2025ï¼‰
- **NRGPT: GPT = EBM**ï¼ˆ2025ï¼‰
- **Kona 1.0å•†ç”¨åŒ–**ï¼ˆ2026ï¼‰

**å•ã„**:
- EBMã¯"éºç‰©"ã ã£ãŸã®ã‹ã€ãã‚Œã¨ã‚‚"æ™‚ä»£ãŒè¿½ã„ã¤ã„ã¦ã„ãªã‹ã£ãŸ"ã®ã‹ï¼Ÿ
- VAE/GANã¯"é€²åŒ–"ã ã£ãŸã®ã‹ã€ãã‚Œã¨ã‚‚"EBMã®è¨“ç·´å›°é›£æ€§ã‹ã‚‰ã®é€ƒé¿"ã ã£ãŸã®ã‹ï¼Ÿ
- 2026å¹´ã®Flow Matching / Diffusionã®èƒŒå¾Œã«ã‚ã‚‹çµ±ä¸€ç†è«–ã¯ã€å®Ÿã¯1982å¹´ã®HopfieldãŒæ—¢ã«ç¤ºã—ã¦ã„ãŸã®ã§ã¯ï¼Ÿ

<details><summary>è€ƒå¯Ÿã®ãƒ’ãƒ³ãƒˆ</summary>

**æ­´å²çš„ã‚µã‚¤ã‚¯ãƒ«**:
- 1982: Hopfield â†’ "ç”»æœŸçš„"
- 1985-2006: Boltzmann/RBM â†’ "Deep Learningã®åŸºç¤"
- 2013-2020: VAE/GAN â†’ "EBMã¯é…ã„ã€ä½¿ãˆãªã„"
- 2020-2026: Modern Hopfield/Energy Matching â†’ "å…¨ã¦ã¯çµ±ä¸€ã•ã‚Œã¦ã„ãŸ"

**æŠ€è¡“çš„æœ¬è³ª**:
- VAE: EBMã®è¿‘ä¼¼ï¼ˆELBO = å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰
- GAN: EBMã®æš—é»™çš„å­¦ç¿’ï¼ˆåˆ¤åˆ¥å™¨ = ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ï¼‰
- Diffusion: EBMã®ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹å­¦ç¿’ï¼ˆScore Matchingï¼‰
- Flow Matching: EBM + OTã®çµ±ä¸€ï¼ˆEnergy Matchingï¼‰

**çµè«–**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å…¨ã¦ã¯EBMã®å¤‰å½¢ã€‚"éºç‰©"ã§ã¯ãªã"åŸºç›¤"ã ã£ãŸã€‚

</details>

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Hopfield, J. J. (1982). "Neural networks and physical systems with emergent collective computational abilities." *Proceedings of the National Academy of Sciences*, 79(8), 2554-2558.
<https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554>

[^2]: Hinton, G. E. (2002). "Training products of experts by minimizing contrastive divergence." *Neural Computation*, 14(8), 1771-1800.
<https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf>

[^3]: Ramsauer, H., et al. (2020). "Hopfield Networks is All You Need." *ICLR 2021*.
<https://arxiv.org/abs/2008.02217>

[^4]: Santos, S., et al. (2025). "Modern Hopfield Networks with Continuous-Time Memories." *arXiv:2502.10122*.
<https://arxiv.org/abs/2502.10122>

[^5]: Dehmamy, N., et al. (2025). "NRGPT: An Energy-based Alternative for GPT." *arXiv:2512.16762*.
<https://arxiv.org/abs/2512.16762>

[^6]: Balcerak, M., et al. (2025). "Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling." *arXiv:2504.10612*.
<https://arxiv.org/abs/2504.10612>

[^7]: Tieleman, T. (2008). "Training restricted Boltzmann machines using approximations to the likelihood gradient." *ICML 2008*.

[^8]: Hoffman, M. D., & Gelman, A. (2014). "The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo." *Journal of Machine Learning Research*, 15(1), 1593-1623.

[^9]: Smolensky, P. (1986). "Information processing in dynamical systems: Foundations of harmony theory." In *Parallel Distributed Processing*, Vol. 1.

[^10]: Nobel Prize (2024). "The Nobel Prize in Physics 2024." John J. Hopfield and Geoffrey E. Hinton.
<https://www.nobelprize.org/prizes/physics/2024/summary/>

[^11]: LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). "A tutorial on energy-based learning." In *Predicting Structured Data*, MIT Press.

### æ•™ç§‘æ›¸

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Chapter on EBMs]
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [Chapter 20: Deep Generative Models]
- MacKay, D. J. C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. [Chapter on Boltzmann Machines]
- Barber, D. (2012). *Bayesian Reasoning and Machine Learning*. Cambridge University Press. [Chapter on EBMs]

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
