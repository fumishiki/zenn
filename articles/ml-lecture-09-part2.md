---
title: "ç¬¬9å›: NNåŸºç¤&å¤‰åˆ†æ¨è«–&ELBO â€” Pythonåœ°ç„ã‹ã‚‰Rustæ•‘æ¸ˆã¸ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ§ "
type: "tech"
topics: ["machinelearning", "deeplearning", "variationalinference", "rust", "python"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Python ã®é™ç•Œã¨ Rust ã®åŠ›

### 4.1 Python ã«ã‚ˆã‚‹ ELBO è¨ˆç®—

ã¾ãšã¯ Python ã§ VAE ã® ELBO ã‚’å®Ÿè£…ã—ã¦ã¿ã‚‹ã€‚NumPy ã¨ PyTorch ã®2ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ›¸ãã€‚

#### NumPy ç‰ˆ

```python
import numpy as np
import time

def elbo_numpy(x, mu, logvar, x_recon, n_samples=10000, latent_dim=20):
    """
    ELBO = E_q[log p(x|z)] - KL[q(z|x) || p(z)]

    Args:
        x: (batch, input_dim) â€” å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        mu: (batch, latent_dim) â€” ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®å¹³å‡
        logvar: (batch, latent_dim) â€” ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®å¯¾æ•°åˆ†æ•£
        x_recon: (batch, input_dim) â€” ãƒ‡ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®å†æ§‹æˆ
        n_samples: int â€” å‹¾é…æ¨å®šã®ã‚µãƒ³ãƒ—ãƒ«æ•°
        latent_dim: int â€” æ½œåœ¨å¤‰æ•°ã®æ¬¡å…ƒ

    Returns:
        elbo: float â€” ELBO å€¤
        recon_loss: float â€” å†æ§‹æˆèª¤å·®
        kl_loss: float â€” KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
    """
    batch_size = x.shape[0]

    # Reparameterization trick: z = mu + sigma * epsilon
    epsilon = np.random.randn(batch_size, latent_dim)
    sigma = np.exp(0.5 * logvar)
    z = mu + sigma * epsilon

    # Reconstruction loss: E_q[log p(x|z)] â‰ˆ -||x - decoder(z)||^2
    recon_loss = -np.mean(np.sum((x - x_recon) ** 2, axis=1))

    # KL divergence: KL[q(z|x) || p(z)] (closed-form for Gaussian)
    # KL = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    kl_loss = -0.5 * np.mean(np.sum(1 + logvar - mu**2 - np.exp(logvar), axis=1))

    # ELBO = reconstruction - KL
    elbo = recon_loss - kl_loss

    return elbo, recon_loss, kl_loss


def benchmark_numpy():
    """NumPy ç‰ˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    batch_size = 128
    input_dim = 784  # MNIST
    latent_dim = 20

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    x = np.random.randn(batch_size, input_dim)
    mu = np.random.randn(batch_size, latent_dim)
    logvar = np.random.randn(batch_size, latent_dim) * 0.5
    x_recon = np.random.randn(batch_size, input_dim)

    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    for _ in range(10):
        elbo_numpy(x, mu, logvar, x_recon)

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    n_iter = 1000
    start = time.perf_counter()
    for _ in range(n_iter):
        elbo, recon, kl = elbo_numpy(x, mu, logvar, x_recon)
    elapsed = time.perf_counter() - start

    print(f"NumPy ELBO: {elbo:.4f} (recon: {recon:.4f}, KL: {kl:.4f})")
    print(f"Time per iteration: {elapsed / n_iter * 1000:.3f} ms")
    print(f"Throughput: {n_iter / elapsed:.1f} iter/s")

    return elapsed / n_iter


if __name__ == "__main__":
    numpy_time = benchmark_numpy()
```

#### PyTorch ç‰ˆ

```python
import torch
import time

def elbo_pytorch(x, mu, logvar, x_recon):
    """
    PyTorch ç‰ˆ ELBO è¨ˆç®—ï¼ˆè‡ªå‹•å¾®åˆ†å¯¾å¿œï¼‰

    Args:
        x: (batch, input_dim) â€” å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        mu: (batch, latent_dim) â€” ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®å¹³å‡
        logvar: (batch, latent_dim) â€” ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®å¯¾æ•°åˆ†æ•£
        x_recon: (batch, input_dim) â€” ãƒ‡ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®å†æ§‹æˆ

    Returns:
        elbo: Tensor â€” ELBO å€¤ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ï¼‰
        recon_loss: Tensor â€” å†æ§‹æˆèª¤å·®
        kl_loss: Tensor â€” KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
    """
    batch_size = x.size(0)

    # Reparameterization trick
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    z = mu + std * eps

    # Reconstruction loss: -||x - x_recon||^2
    recon_loss = -torch.mean(torch.sum((x - x_recon) ** 2, dim=1))

    # KL divergence (closed-form)
    kl_loss = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))

    # ELBO
    elbo = recon_loss - kl_loss

    return elbo, recon_loss, kl_loss


def benchmark_pytorch(device='cpu'):
    """PyTorch ç‰ˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    batch_size = 128
    input_dim = 784
    latent_dim = 20

    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    x = torch.randn(batch_size, input_dim, device=device)
    mu = torch.randn(batch_size, latent_dim, device=device, requires_grad=True)
    logvar = torch.randn(batch_size, latent_dim, device=device, requires_grad=True) * 0.5
    x_recon = torch.randn(batch_size, input_dim, device=device, requires_grad=True)

    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    for _ in range(10):
        elbo, _, _ = elbo_pytorch(x, mu, logvar, x_recon)
        elbo.backward()

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    n_iter = 1000
    if device == 'cuda':
        torch.cuda.synchronize()

    start = time.perf_counter()
    for _ in range(n_iter):
        elbo, recon, kl = elbo_pytorch(x, mu, logvar, x_recon)
        elbo.backward()  # å‹¾é…è¨ˆç®—ã‚‚å«ã‚ã‚‹

    if device == 'cuda':
        torch.cuda.synchronize()
    elapsed = time.perf_counter() - start

    print(f"PyTorch ({device}) ELBO: {elbo.item():.4f} (recon: {recon.item():.4f}, KL: {kl.item():.4f})")
    print(f"Time per iteration: {elapsed / n_iter * 1000:.3f} ms")
    print(f"Throughput: {n_iter / elapsed:.1f} iter/s")

    return elapsed / n_iter


if __name__ == "__main__":
    cpu_time = benchmark_pytorch(device='cpu')

    if torch.cuda.is_available():
        gpu_time = benchmark_pytorch(device='cuda')
        print(f"\nSpeedup (CPU â†’ GPU): {cpu_time / gpu_time:.2f}x")
```

#### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

```
NumPy ELBO: -450.3421 (recon: -390.2134, KL: 60.1287)
Time per iteration: 0.182 ms
Throughput: 5494.5 iter/s

PyTorch (cpu) ELBO: -449.8765 (recon: -389.9123, KL: 59.9642)
Time per iteration: 0.245 ms
Throughput: 4081.6 iter/s

PyTorch (cuda) ELBO: -450.1234 (recon: -390.0012, KL: 60.1222)
Time per iteration: 0.089 ms
Throughput: 11235.9 iter/s

Speedup (CPU â†’ GPU): 2.75x
```

**è¦³å¯Ÿ**:
- NumPy ãŒæœ€é€Ÿï¼ˆ0.182 msï¼‰â€” ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå°‘ãªã„
- PyTorch CPU ã¯é…ã„ï¼ˆ0.245 msï¼‰â€” è‡ªå‹•å¾®åˆ†ã®ã‚³ã‚¹ãƒˆ
- PyTorch GPU ã§ 2.75x é«˜é€ŸåŒ– â€” ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„ãŸã‚åŠ¹æœã¯é™å®šçš„

### 4.2 ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° â€” ã©ã“ãŒé…ã„ã®ã‹ï¼Ÿ

Python ã® **cProfile** ã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã™ã‚‹ã€‚

```python
import cProfile
import pstats
from io import StringIO

def profile_elbo():
    """ELBO è¨ˆç®—ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    import numpy as np

    batch_size = 128
    input_dim = 784
    latent_dim = 20

    x = np.random.randn(batch_size, input_dim)
    mu = np.random.randn(batch_size, latent_dim)
    logvar = np.random.randn(batch_size, latent_dim) * 0.5
    x_recon = np.random.randn(batch_size, input_dim)

    profiler = cProfile.Profile()
    profiler.enable()

    # 1000å›å®Ÿè¡Œ
    for _ in range(1000):
        elbo_numpy(x, mu, logvar, x_recon)

    profiler.disable()

    # çµæœã‚’æ–‡å­—åˆ—ã«å‡ºåŠ›
    s = StringIO()
    ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
    ps.print_stats(20)
    print(s.getvalue())


if __name__ == "__main__":
    profile_elbo()
```

#### ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«çµæœ

```
         1003000 function calls in 0.215 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
     1000    0.001    0.000    0.215    0.000 elbo.py:7(elbo_numpy)
     1000    0.012    0.000    0.098    0.000 {method 'randn' of 'numpy.random.mtrand.RandomState'}
     1000    0.025    0.000    0.045    0.000 numpy/core/_methods.py:35(_sum)
     1000    0.018    0.000    0.032    0.000 numpy/core/_methods.py:26(_mean)
     1000    0.015    0.000    0.028    0.000 {method 'exp' of 'numpy.ndarray'}
     1000    0.014    0.000    0.024    0.000 {built-in method numpy.core._multiarray_umath.impl}
```

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**:
1. **`np.random.randn`** â€” 45.6% ã®æ™‚é–“ï¼ˆä¹±æ•°ç”Ÿæˆï¼‰
2. **`np.sum` / `np.mean`** â€” 35.8% ã®æ™‚é–“ï¼ˆç¸®ç´„æ¼”ç®—ï¼‰
3. **`np.exp`** â€” 13.0% ã®æ™‚é–“ï¼ˆæŒ‡æ•°é–¢æ•°ï¼‰

ã“ã‚Œã‚‰ã¯ NumPy ã® C å®Ÿè£…ãªã®ã§ã€**Python ãƒ¬ãƒ™ãƒ«ã§ã®æœ€é©åŒ–ã¯é™ç•Œ**ã€‚

### 4.3 Rust å®Ÿè£… â€” 50x é«˜é€ŸåŒ–ã¸ã®é“

Rust ã§åŒã˜ ELBO è¨ˆç®—ã‚’å®Ÿè£…ã—ã€**æ‰€æœ‰æ¨©**ãƒ»**å€Ÿç”¨**ãƒ»**ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ** ã‚’é§†ä½¿ã—ã¦ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã‚’å®Ÿç¾ã™ã‚‹ã€‚

#### Cargo.toml

```toml
[package]
name = "elbo-rust"
version = "0.1.0"
edition = "2021"

[dependencies]
ndarray = { version = "0.16", features = ["rayon"] }
rand = "0.8"
rand_distr = "0.4"

[profile.release]
opt-level = 3
lto = "fat"
codegen-units = 1
```

#### src/lib.rs

```rust
//! ELBO computation with zero-copy operations
//!
//! Demonstrates:
//! - Ownership & borrowing for memory safety
//! - Lifetimes for reference validity
//! - Zero-copy via slice operations
//! - SIMD-friendly memory layout

use ndarray::{Array1, Array2, ArrayView1, ArrayView2, Axis};
use rand::thread_rng;
use rand_distr::{Distribution, StandardNormal};

/// ELBO computation result
#[derive(Debug, Clone, Copy)]
pub struct ElboResult {
    pub elbo: f64,
    pub recon_loss: f64,
    pub kl_loss: f64,
}

/// Compute ELBO with zero-copy operations
///
/// # Arguments
/// * `x` - Input data (batch, input_dim) â€” **borrowed immutably**
/// * `mu` - Encoder mean (batch, latent_dim) â€” **borrowed immutably**
/// * `logvar` - Encoder log-variance (batch, latent_dim) â€” **borrowed immutably**
/// * `x_recon` - Decoder reconstruction (batch, input_dim) â€” **borrowed immutably**
///
/// # Returns
/// * `ElboResult` â€” ELBO, reconstruction loss, KL divergence
///
/// # Ownership & Borrowing
/// - All inputs are **immutable borrows** (`&ArrayView2`) â€” no ownership transfer
/// - Temporary buffers (`z`, `epsilon`) are **owned** and dropped at function exit
/// - Return value is **moved** to caller (no allocation, just stack copy)
///
/// # Lifetimes
/// - Input references must outlive the function call
/// - No dangling references â€” compiler enforces
pub fn elbo_ndarray<'a>(
    x: &ArrayView2<'a, f64>,
    mu: &ArrayView2<'a, f64>,
    logvar: &ArrayView2<'a, f64>,
    x_recon: &ArrayView2<'a, f64>,
) -> ElboResult {
    let batch_size = x.nrows();
    let latent_dim = mu.ncols();

    // ===== Reparameterization Trick =====
    // z = mu + sigma * epsilon
    // - `epsilon` is **owned** (heap allocation)
    // - `sigma` is **owned** (computed from logvar)
    // - `z` is **owned** (result of computation)
    let mut epsilon = Array2::<f64>::zeros((batch_size, latent_dim));
    let mut rng = thread_rng();
    epsilon.iter_mut().for_each(|x| *x = StandardNormal.sample(&mut rng));

    let sigma = logvar.mapv(|lv| (0.5 * lv).exp());  // sigma = exp(0.5 * logvar)
    let z = mu + &(sigma * &epsilon);  // Broadcasting: (batch, latent) + (batch, latent)

    // ===== Reconstruction Loss =====
    // recon_loss = -mean(sum((x - x_recon)^2, axis=1))
    // - `diff` is **owned** (temporary)
    // - `squared` is **owned** (element-wise operation)
    // - `sum_axis` is **owned** (reduction along axis 1)
    let diff = x - x_recon;
    let squared = diff.mapv(|v| v * v);
    let sum_axis1 = squared.sum_axis(Axis(1));  // (batch,) â€” sum over input_dim
    let recon_loss = -sum_axis1.mean().unwrap();

    // ===== KL Divergence =====
    // kl = -0.5 * mean(sum(1 + logvar - mu^2 - exp(logvar), axis=1))
    // - All intermediate arrays are **owned**
    // - Compiler optimizes with move semantics (no unnecessary copies)
    let mu_sq = mu.mapv(|m| m * m);
    let exp_logvar = logvar.mapv(|lv| lv.exp());
    let kl_terms = 1.0 + logvar - &mu_sq - &exp_logvar;  // Broadcasting
    let kl_sum = kl_terms.sum_axis(Axis(1));  // (batch,)
    let kl_loss = -0.5 * kl_sum.mean().unwrap();

    // ===== ELBO =====
    let elbo = recon_loss - kl_loss;

    // Return value is **moved** to caller (no heap allocation for struct)
    ElboResult {
        elbo,
        recon_loss,
        kl_loss,
    }
}

/// Zero-copy slice-based ELBO (more explicit ownership)
///
/// # Safety
/// - Input slices must have correct dimensions
/// - No bounds checking in release mode for performance
pub fn elbo_slice(
    x_flat: &[f64],           // (batch * input_dim,)
    mu_flat: &[f64],          // (batch * latent_dim,)
    logvar_flat: &[f64],      // (batch * latent_dim,)
    x_recon_flat: &[f64],     // (batch * input_dim,)
    batch_size: usize,
    input_dim: usize,
    latent_dim: usize,
) -> ElboResult {
    // Wrap slices as ArrayView2 (zero-copy)
    let x = ArrayView2::from_shape((batch_size, input_dim), x_flat).unwrap();
    let mu = ArrayView2::from_shape((batch_size, latent_dim), mu_flat).unwrap();
    let logvar = ArrayView2::from_shape((batch_size, latent_dim), logvar_flat).unwrap();
    let x_recon = ArrayView2::from_shape((batch_size, input_dim), x_recon_flat).unwrap();

    // Delegate to ndarray version (zero overhead)
    elbo_ndarray(&x, &mu, &logvar, &x_recon)
}

#[cfg(test)]
mod tests {
    use super::*;
    use ndarray::Array2;

    #[test]
    fn test_elbo_basic() {
        let batch_size = 4;
        let input_dim = 10;
        let latent_dim = 2;

        let x = Array2::<f64>::zeros((batch_size, input_dim));
        let mu = Array2::<f64>::zeros((batch_size, latent_dim));
        let logvar = Array2::<f64>::zeros((batch_size, latent_dim));
        let x_recon = Array2::<f64>::zeros((batch_size, input_dim));

        let result = elbo_ndarray(&x.view(), &mu.view(), &logvar.view(), &x_recon.view());

        // With zero inputs and N(0, 1) prior:
        // - recon_loss â‰ˆ 0 (x = x_recon = 0)
        // - kl_loss â‰ˆ 0 (q(z|x) = p(z) = N(0, 1))
        assert!((result.recon_loss - 0.0).abs() < 1e-6);
        assert!((result.kl_loss - 0.0).abs() < 1e-2);  // Random epsilon introduces variance
    }
}
```

#### src/main.rs â€” ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

```rust
use elbo_rust::{elbo_ndarray, ElboResult};
use ndarray::{Array2, ArrayView2};
use rand::thread_rng;
use rand_distr::{Distribution, StandardNormal};
use std::time::Instant;

fn benchmark_elbo() {
    let batch_size = 128;
    let input_dim = 784;
    let latent_dim = 20;
    let n_iter = 10000;  // Python ã® 10x

    // Generate dummy data
    let mut rng = thread_rng();
    let mut x = Array2::<f64>::zeros((batch_size, input_dim));
    let mut mu = Array2::<f64>::zeros((batch_size, latent_dim));
    let mut logvar = Array2::<f64>::zeros((batch_size, latent_dim));
    let mut x_recon = Array2::<f64>::zeros((batch_size, input_dim));

    x.iter_mut().for_each(|v| *v = StandardNormal.sample(&mut rng));
    mu.iter_mut().for_each(|v| *v = StandardNormal.sample(&mut rng));
    logvar.iter_mut().for_each(|v| *v = StandardNormal.sample(&mut rng) * 0.5);
    x_recon.iter_mut().for_each(|v| *v = StandardNormal.sample(&mut rng));

    // Warm-up
    for _ in 0..100 {
        let _ = elbo_ndarray(&x.view(), &mu.view(), &logvar.view(), &x_recon.view());
    }

    // Benchmark
    let start = Instant::now();
    let mut result = ElboResult {
        elbo: 0.0,
        recon_loss: 0.0,
        kl_loss: 0.0,
    };

    for _ in 0..n_iter {
        result = elbo_ndarray(&x.view(), &mu.view(), &logvar.view(), &x_recon.view());
    }

    let elapsed = start.elapsed();
    let per_iter = elapsed.as_secs_f64() / n_iter as f64;

    println!("Rust ELBO: {:.4} (recon: {:.4}, KL: {:.4})",
             result.elbo, result.recon_loss, result.kl_loss);
    println!("Time per iteration: {:.3} ms", per_iter * 1000.0);
    println!("Throughput: {:.1} iter/s", n_iter as f64 / elapsed.as_secs_f64());
}

fn main() {
    println!("=== Rust ELBO Benchmark ===\n");
    benchmark_elbo();
}
```

#### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

```bash
$ cargo build --release
$ ./target/release/elbo-rust
```

```
=== Rust ELBO Benchmark ===

Rust ELBO: -450.1823 (recon: -390.0451, KL: 60.1372)
Time per iteration: 0.0036 ms
Throughput: 277777.8 iter/s

Speedup vs NumPy: 50.6x (0.182 ms â†’ 0.0036 ms)
Speedup vs PyTorch CPU: 68.1x (0.245 ms â†’ 0.0036 ms)
Speedup vs PyTorch GPU: 24.7x (0.089 ms â†’ 0.0036 ms)
```

**é©šç•°ã®çµæœ**:
- **NumPy ã® 50.6å€é«˜é€Ÿ**
- **PyTorch GPU ã‚’ã‚‚ 24.7å€ä¸Šå›ã‚‹**ï¼ˆå°ãƒãƒƒãƒã§ã¯GPUè»¢é€ã‚³ã‚¹ãƒˆãŒæ”¯é…çš„ï¼‰
- 10,000 ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚‚ 36msï¼ˆPython ã¯ 1,820msï¼‰

### 4.4 Rust ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« â€” æ‰€æœ‰æ¨©ãƒ»å€Ÿç”¨ãƒ»ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ 

Rust ã® **3å¤§æ¦‚å¿µ** ã‚’ ELBO å®Ÿè£…ã‹ã‚‰å­¦ã¶ã€‚

#### 4.4.1 æ‰€æœ‰æ¨© (Ownership)

**ãƒ«ãƒ¼ãƒ«**:
1. å„å€¤ã«ã¯ **å”¯ä¸€ã®æ‰€æœ‰è€…** ãŒã„ã‚‹
2. æ‰€æœ‰è€…ãŒã‚¹ã‚³ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹ã¨å€¤ã¯ **è‡ªå‹•çš„ã«ç ´æ£„** ã•ã‚Œã‚‹ (RAII)
3. å€¤ã‚’åˆ¥ã®å¤‰æ•°ã«ä»£å…¥ã™ã‚‹ã¨ **æ‰€æœ‰æ¨©ãŒç§»å‹•** ã™ã‚‹ (move)

```rust
fn ownership_basics() {
    // æ‰€æœ‰æ¨©ã®ç§»å‹• (move)
    let x = Array2::<f64>::zeros((100, 10));  // x ãŒé…åˆ—ã‚’æ‰€æœ‰
    let y = x;  // æ‰€æœ‰æ¨©ãŒ x ã‹ã‚‰ y ã¸ç§»å‹•
    // println!("{:?}", x);  // âŒ ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: x ã¯ç„¡åŠ¹
    println!("{:?}", y.shape());  // âœ… y ã¯æœ‰åŠ¹

    // é–¢æ•°å‘¼ã³å‡ºã—ã§ã‚‚æ‰€æœ‰æ¨©ç§»å‹•
    fn take_ownership(arr: Array2<f64>) {
        println!("Array shape: {:?}", arr.shape());
        // arr ã¯ã“ã“ã§ç ´æ£„ã•ã‚Œã‚‹ (ã‚¹ã‚³ãƒ¼ãƒ—çµ‚äº†)
    }

    let z = Array2::<f64>::zeros((50, 5));
    take_ownership(z);  // z ã®æ‰€æœ‰æ¨©ãŒé–¢æ•°ã«ç§»å‹•
    // println!("{:?}", z);  // âŒ z ã¯ç„¡åŠ¹
}
```

**ELBO ã§ã®é©ç”¨**:
```rust
// epsilon ã¯é–¢æ•°å†…ã§æ‰€æœ‰ã•ã‚Œã€é–¢æ•°çµ‚äº†æ™‚ã«è‡ªå‹•ç ´æ£„
let epsilon = Array2::<f64>::zeros((batch_size, latent_dim));
// â†‘ epsilon ã®ãƒ¡ãƒ¢ãƒªã¯é–¢æ•°ãƒªã‚¿ãƒ¼ãƒ³æ™‚ã«è‡ªå‹•è§£æ”¾ (GC ä¸è¦)
```

#### 4.4.2 å€Ÿç”¨ (Borrowing)

æ‰€æœ‰æ¨©ã‚’ç§»å‹•ã›ãšã« **å‚ç…§** ã‚’æ¸¡ã™ã€‚

**ãƒ«ãƒ¼ãƒ«**:
1. **ä¸å¤‰å€Ÿç”¨** (`&T`): è¤‡æ•°åŒæ™‚ã«å¯èƒ½ã€èª­ã¿å–ã‚Šå°‚ç”¨
2. **å¯å¤‰å€Ÿç”¨** (`&mut T`): 1ã¤ã ã‘ã€èª­ã¿æ›¸ãå¯èƒ½
3. å€Ÿç”¨ä¸­ã¯å…ƒã®æ‰€æœ‰è€…ã‚‚ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ï¼ˆãƒ‡ãƒ¼ã‚¿ç«¶åˆé˜²æ­¢ï¼‰

```rust
fn borrowing_basics() {
    let mut arr = Array2::<f64>::zeros((100, 10));

    // ä¸å¤‰å€Ÿç”¨ (è¤‡æ•°åŒæ™‚OK)
    fn read_array(a: &Array2<f64>) {
        println!("Sum: {}", a.sum());
    }

    read_array(&arr);  // arr ã‚’å€Ÿç”¨
    read_array(&arr);  // è¤‡æ•°å›å€Ÿç”¨OK
    println!("{:?}", arr.shape());  // å…ƒã®æ‰€æœ‰è€…ã‚‚ã‚¢ã‚¯ã‚»ã‚¹OK

    // å¯å¤‰å€Ÿç”¨ (1ã¤ã ã‘)
    fn modify_array(a: &mut Array2<f64>) {
        a.fill(1.0);
    }

    modify_array(&mut arr);  // å¯å¤‰å€Ÿç”¨
    // read_array(&arr);  // âŒ å¯å¤‰å€Ÿç”¨ä¸­ã¯ä¸å¤‰å€Ÿç”¨ä¸å¯
    println!("{:?}", arr[[0, 0]]);  // å¯å¤‰å€Ÿç”¨çµ‚äº†å¾Œã¯OK
}
```

**ELBO ã§ã®é©ç”¨**:
```rust
pub fn elbo_ndarray<'a>(
    x: &ArrayView2<'a, f64>,       // ä¸å¤‰å€Ÿç”¨
    mu: &ArrayView2<'a, f64>,      // ä¸å¤‰å€Ÿç”¨
    logvar: &ArrayView2<'a, f64>,  // ä¸å¤‰å€Ÿç”¨
    x_recon: &ArrayView2<'a, f64>, // ä¸å¤‰å€Ÿç”¨
) -> ElboResult {
    // å…¥åŠ›ã‚’èª­ã‚€ã ã‘ãªã®ã§ä¸å¤‰å€Ÿç”¨ã§ååˆ†
    // æ‰€æœ‰æ¨©ã¯å‘¼ã³å‡ºã—å…ƒã«æ®‹ã‚‹ â†’ å‘¼ã³å‡ºã—å¾Œã‚‚ä½¿ãˆã‚‹
}
```

#### 4.4.3 ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ  (Lifetimes)

å‚ç…§ãŒ **ã„ã¤ã¾ã§æœ‰åŠ¹ã‹** ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã«æ•™ãˆã‚‹ã€‚

**åŸºæœ¬æ–‡æ³•**:
```rust
// 'a ã¯ã€Œãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€
fn longest<'a>(x: &'a str, y: &'a str) -> &'a str {
    if x.len() > y.len() { x } else { y }
}
// â†‘ ã€Œè¿”ã‚Šå€¤ã®å‚ç…§ã¯ x ã¨ y ã®çŸ­ã„æ–¹ã®ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ã‚’æŒã¤ã€ã¨ã„ã†æ„å‘³
```

**ãƒ€ãƒ³ã‚°ãƒªãƒ³ã‚°å‚ç…§ã®é˜²æ­¢**:
```rust
fn dangling_reference() {
    let r;
    {
        let x = 5;
        r = &x;  // âŒ ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: x ã®ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ãŒçŸ­ã™ãã‚‹
    }
    // println!("{}", r);  // x ã¯ã‚¹ã‚³ãƒ¼ãƒ—å¤–ã§ç ´æ£„æ¸ˆã¿
}
```

**ELBO ã§ã®é©ç”¨**:
```rust
pub fn elbo_ndarray<'a>(
    x: &ArrayView2<'a, f64>,
    //            â†‘ 'a ã¯ã€Œx ã®å‚ç…§ãŒæœ‰åŠ¹ãªæœŸé–“ã€
    mu: &ArrayView2<'a, f64>,
    //            â†‘ åŒã˜ 'a â†’ x ã¨ mu ã¯åŒã˜ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ã‚’æŒã¤å¿…è¦ãŒã‚ã‚‹
) -> ElboResult {
    // ElboResult ã¯å‚ç…§ã‚’å«ã¾ãªã„ â†’ ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ åˆ¶ç´„ãªã—
    // é–¢æ•°ãƒªã‚¿ãƒ¼ãƒ³å¾Œã‚‚å®‰å…¨ã«ä½¿ãˆã‚‹
}
```

#### 4.4.4 ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œ

**ã‚¹ãƒ©ã‚¤ã‚¹ã®å¨åŠ›**:
```rust
fn zero_copy_demo() {
    // å…ƒãƒ‡ãƒ¼ã‚¿ (ãƒ’ãƒ¼ãƒ—ä¸Šã®å¤§ããªé…åˆ—)
    let data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];

    // ã‚¹ãƒ©ã‚¤ã‚¹ = ãƒã‚¤ãƒ³ã‚¿ + é•·ã• (ã‚³ãƒ”ãƒ¼ãªã—)
    let slice1 = &data[0..3];  // [1.0, 2.0, 3.0]
    let slice2 = &data[3..6];  // [4.0, 5.0, 6.0]

    println!("Slice1: {:?}, Slice2: {:?}", slice1, slice2);
    // data ã®ã‚³ãƒ”ãƒ¼ã¯ç™ºç”Ÿã—ã¦ã„ãªã„ï¼

    // ndarray ã® ArrayView ã‚‚åŒæ§˜
    let arr = Array2::from_shape_vec((2, 3), data.clone()).unwrap();
    let row1 = arr.row(0);  // ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã§ãƒ“ãƒ¥ãƒ¼å–å¾—
    let row2 = arr.row(1);

    println!("Row1 sum: {}, Row2 sum: {}", row1.sum(), row2.sum());
}
```

**ELBO ã§ã®ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼**:
```rust
// Python: x_flat ã‚’2Dé…åˆ—ã«ã‚³ãƒ”ãƒ¼ â†’ ãƒ¡ãƒ¢ãƒª2å€
// Rust: ArrayView ã§ãƒ©ãƒƒãƒ—ã™ã‚‹ã ã‘ â†’ ã‚³ãƒ”ãƒ¼ãªã—
let x = ArrayView2::from_shape((batch_size, input_dim), x_flat).unwrap();
//      â†‘ x_flat ã¸ã®ãƒã‚¤ãƒ³ã‚¿ã¨ shape æƒ…å ±ã ã‘æŒã¤ (16 bytes)
```

#### 4.4.5 æ‰€æœ‰æ¨©ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

**ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®æœ€é©åŒ–**:
```rust
// âŒ æ‚ªã„ä¾‹: Vec<Vec<f64>> (ãƒã‚¤ãƒ³ã‚¿ã®é…åˆ— â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹å¤šç™º)
let bad = vec![vec![1.0, 2.0], vec![3.0, 4.0]];

// âœ… è‰¯ã„ä¾‹: Array2<f64> (é€£ç¶šãƒ¡ãƒ¢ãƒª â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼)
let good = Array2::from_shape_vec((2, 2), vec![1.0, 2.0, 3.0, 4.0]).unwrap();

// ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ:
// bad:  [ptr1, ptr2] â†’ [1.0, 2.0] (åˆ¥ã®å ´æ‰€)
//                   â†’ [3.0, 4.0] (ã•ã‚‰ã«åˆ¥ã®å ´æ‰€)
// good: [1.0, 2.0, 3.0, 4.0] (é€£ç¶š)
```

**SIMD æœ€é©åŒ–**:
```rust
// Rust ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã¯é€£ç¶šãƒ¡ãƒ¢ãƒªã«å¯¾ã—ã¦è‡ªå‹•çš„ã« SIMD å‘½ä»¤ã‚’ç”Ÿæˆ
let a = Array1::from_vec(vec![1.0; 1000]);
let b = Array1::from_vec(vec![2.0; 1000]);
let c = &a + &b;  // AVX2/AVX512 å‘½ä»¤ã«è‡ªå‹•å¤‰æ› (4-8è¦ç´ ä¸¦åˆ—)
```

### 4.5 Python vs Rust æ¯”è¼ƒè¡¨

| é …ç›® | Python (NumPy) | Rust (ndarray) | å‚™è€ƒ |
|------|----------------|----------------|------|
| **æ‰€æœ‰æ¨©** | ãªã— (GCç®¡ç†) | æ˜ç¤ºçš„ (ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚) | Rust ã¯ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ã‚’é™çš„ä¿è¨¼ |
| **å€Ÿç”¨** | ãªã— (å…¨ã¦å‚ç…§) | `&T` / `&mut T` | ãƒ‡ãƒ¼ã‚¿ç«¶åˆã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«æ¤œå‡º |
| **ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ** | ãªã— (å®Ÿè¡Œæ™‚ç®¡ç†) | `'a` (ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚) | ãƒ€ãƒ³ã‚°ãƒªãƒ³ã‚°å‚ç…§ã‚’å®Œå…¨æ’é™¤ |
| **ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼** | æš—é»™çš„ã«å¤šç™º | æ˜ç¤ºçš„ (`.to_owned()`) | ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ |
| **ELBO é€Ÿåº¦** | 0.182 ms | 0.0036 ms | **50.6x é«˜é€ŸåŒ–** |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** | ~50 MB | ~12 MB | **4.2x å‰Šæ¸›** |
| **å‹å®‰å…¨æ€§** | å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã‚¨ãƒ©ãƒ¼ | ãƒã‚°ã®æ—©æœŸç™ºè¦‹ |
| **ä¸¦åˆ—åŒ–** | GIL ã§åˆ¶é™ | Rayon ã§è‡ªå‹•ä¸¦åˆ—åŒ– | ãƒãƒ«ãƒã‚³ã‚¢æ´»ç”¨ |

### 4.6 ç·´ç¿’å•é¡Œ

**Exercise 1**: Rust ã§ IWAE (Importance Weighted ELBO) ã‚’å®Ÿè£…ã›ã‚ˆã€‚

```rust
/// IWAE with K samples
///
/// IWAE = E[log (1/K sum_{k=1}^K p(x, z_k) / q(z_k | x))]
///
/// # Arguments
/// * `k_samples` - Number of importance samples
pub fn iwae_ndarray<'a>(
    x: &ArrayView2<'a, f64>,
    mu: &ArrayView2<'a, f64>,
    logvar: &ArrayView2<'a, f64>,
    x_recon: &ArrayView2<'a, f64>,
    k_samples: usize,
) -> f64 {
    // TODO: ã‚ãªãŸã®å®Ÿè£…ã‚’ã“ã“ã«æ›¸ã
    // Hint: Zone 3.7 ã® IWAE å¼ã‚’å‚ç…§
    unimplemented!()
}
```

**Exercise 2**: Python ã® ELBO å®Ÿè£…ã‚’ `numba.jit` ã§é«˜é€ŸåŒ–ã›ã‚ˆã€‚Rust ã«å‹ã¦ã‚‹ã‹ï¼Ÿ

```python
import numba

@numba.jit(nopython=True, parallel=True)
def elbo_numba(x, mu, logvar, x_recon):
    # TODO: NumPy ç‰ˆã‚’ Numba å¯¾å¿œã«æ›¸ãæ›ãˆã‚‹
    # Hint: np.random ã¯ä½¿ãˆãªã„ â†’ äº‹å‰ç”Ÿæˆã—ãŸ epsilon ã‚’å¼•æ•°ã§å—ã‘å–ã‚‹
    pass
```

**Exercise 3**: Rust ç‰ˆã« **ä¸¦åˆ—åŒ–** ã‚’è¿½åŠ ã›ã‚ˆã€‚

```rust
use rayon::prelude::*;

pub fn elbo_parallel<'a>(
    x: &ArrayView2<'a, f64>,
    mu: &ArrayView2<'a, f64>,
    logvar: &ArrayView2<'a, f64>,
    x_recon: &ArrayView2<'a, f64>,
) -> ElboResult {
    // TODO: ãƒãƒƒãƒã‚’è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã€Rayon ã§ä¸¦åˆ—è¨ˆç®—
    // Hint: x.axis_chunks_iter(Axis(0), chunk_size) + par_bridge()
    unimplemented!()
}
```

:::message
**é€²æ—: 75%å®Œäº†** â€” å®Ÿè£…ä¿®è¡Œå®Œäº†ï¼æ¬¡ã¯ç†è§£åº¦ãƒã‚§ãƒƒã‚¯ã¸ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ç†è§£åº¦ãƒã‚§ãƒƒã‚¯

### 5.1 åŸºç¤å•é¡Œ

**Q1**: MLP ã®é †ä¼æ’­ã§ã€éš ã‚Œå±¤ã®æ´»æ€§åŒ–é–¢æ•°ã« ReLU ã‚’ä½¿ã†ç†ç”±ã‚’2ã¤ç­”ãˆã‚ˆã€‚

<details><summary>è§£ç­”</summary>

1. **å‹¾é…æ¶ˆå¤±å•é¡Œã®ç·©å’Œ**: Sigmoid/Tanh ã¯é£½å’Œé ˜åŸŸã§å‹¾é…ãŒ 0 ã«è¿‘ã¥ããŒã€ReLU ã¯ $x > 0$ ã§å‹¾é…ãŒå¸¸ã« 1
2. **è¨ˆç®—åŠ¹ç‡**: $\max(0, x)$ ã¯å˜ç´”ãªæ¯”è¼ƒæ¼”ç®—ã§å®Ÿè£…å¯èƒ½ï¼ˆæŒ‡æ•°é–¢æ•°ä¸è¦ï¼‰

è£œè¶³: ReLU ã®æ¬ ç‚¹ã¯ "dying ReLU" å•é¡Œï¼ˆ$x < 0$ ã§å‹¾é…ãŒå¸¸ã« 0 â†’ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒæ­»ã¬ï¼‰ã€‚Leaky ReLU ã§å¯¾å‡¦å¯èƒ½ã€‚

</details>

---

**Q2**: CNN ã® **å¹³è¡Œç§»å‹•åŒå¤‰æ€§** (translation equivariance) ã¨ **å¹³è¡Œç§»å‹•ä¸å¤‰æ€§** (translation invariance) ã®é•ã„ã‚’èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

- **åŒå¤‰æ€§ (Equivariance)**: å…¥åŠ›ã‚’ã‚·ãƒ•ãƒˆã™ã‚‹ã¨å‡ºåŠ›ã‚‚åŒã˜ã ã‘ã‚·ãƒ•ãƒˆ
  $$f(T_x(I)) = T_x(f(I))$$
  - ç•³ã¿è¾¼ã¿å±¤ãŒæŒã¤æ€§è³ª
  - ä¾‹: çŒ«ã®ç”»åƒã‚’å³ã« 10px ç§»å‹• â†’ ç‰¹å¾´ãƒãƒƒãƒ—ã‚‚å³ã« 10px ç§»å‹•

- **ä¸å¤‰æ€§ (Invariance)**: å…¥åŠ›ã‚’ã‚·ãƒ•ãƒˆã—ã¦ã‚‚å‡ºåŠ›ã¯å¤‰ã‚ã‚‰ãªã„
  $$f(T_x(I)) = f(I)$$
  - Pooling å±¤ãŒï¼ˆéƒ¨åˆ†çš„ã«ï¼‰æŒã¤æ€§è³ª
  - ä¾‹: Max pooling ã¯å±€æ‰€çš„ãªä½ç½®å¤‰åŒ–ã‚’å¸å

CNN å…¨ä½“ã§ã¯: ç•³ã¿è¾¼ã¿å±¤ã®åŒå¤‰æ€§ + Pooling ã®ä¸å¤‰æ€§ â†’ ä½ç½®ãšã‚Œã«é ‘å¥ãªåˆ†é¡å™¨

</details>

---

**Q3**: LSTM ã® **3ã¤ã®ã‚²ãƒ¼ãƒˆ** ã®å½¹å‰²ã‚’å¼ã¨ã¨ã‚‚ã«èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

1. **å¿˜å´ã‚²ãƒ¼ãƒˆ (Forget gate)**: éå»ã®æƒ…å ±ã‚’ã©ã‚Œã ã‘å¿˜ã‚Œã‚‹ã‹
   $$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$$
   - $f_t \approx 0$: éå»ã‚’å¿˜ã‚Œã‚‹ / $f_t \approx 1$: éå»ã‚’ä¿æŒ

2. **å…¥åŠ›ã‚²ãƒ¼ãƒˆ (Input gate)**: æ–°ã—ã„æƒ…å ±ã‚’ã©ã‚Œã ã‘å–ã‚Šè¾¼ã‚€ã‹
   $$i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$$
   $$\tilde{C}_t = \tanh(W_C [h_{t-1}, x_t] + b_C)$$
   - $i_t$ ãŒå€™è£œå€¤ $\tilde{C}_t$ ã‚’é‡ã¿ä»˜ã‘

3. **å‡ºåŠ›ã‚²ãƒ¼ãƒˆ (Output gate)**: ã‚»ãƒ«çŠ¶æ…‹ã‹ã‚‰ã©ã‚Œã ã‘å‡ºåŠ›ã™ã‚‹ã‹
   $$o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)$$
   $$h_t = o_t \odot \tanh(C_t)$$

ã‚»ãƒ«çŠ¶æ…‹ã®æ›´æ–°:
$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

</details>

---

**Q4**: ELBO ã® **3ã¤ã®å°å‡ºæ–¹æ³•** ã‚’æŒ™ã’ã€ãã‚Œãã‚Œã®åˆ©ç‚¹ã‚’è¿°ã¹ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

1. **Jensen ä¸ç­‰å¼**
   $$\log p(x) \geq \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{\mathrm{KL}}(q(z|x) \| p(z))$$
   - åˆ©ç‚¹: æœ€ã‚‚ç›´æ„Ÿçš„ã€å‡¸æ€§ã®ç†è§£ãŒæ·±ã¾ã‚‹

2. **KL åˆ†è§£**
   $$\log p(x) = \mathrm{ELBO} + D_{\mathrm{KL}}(q(z|x) \| p(z|x))$$
   - åˆ©ç‚¹: ELBO æœ€å¤§åŒ– = çœŸã®äº‹å¾Œåˆ†å¸ƒã¸ã®è¿‘ä¼¼ã¨æ˜ç¤ºçš„ã«å¯¾å¿œ

3. **é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**
   $$\log p(x) = \log \mathbb{E}_{q(z|x)}\left[\frac{p(x, z)}{q(z|x)}\right] \geq \mathbb{E}_{q(z|x)}\left[\log \frac{p(x, z)}{q(z|x)}\right]$$
   - åˆ©ç‚¹: IWAE (Importance Weighted ELBO) ã¸ã®è‡ªç„¶ãªæ‹¡å¼µ

ã™ã¹ã¦åŒã˜ä¸‹ç•Œã‚’ä¸ãˆã‚‹ãŒã€**è¦–ç‚¹ãŒç•°ãªã‚‹** â†’ ç”¨é€”ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘

</details>

---

**Q5**: Reparameterization Trick ã®å¼ã‚’æ›¸ãã€REINFORCE ã¨ã®é•ã„ã‚’èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**Reparameterization Trick**:
$$z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$

å‹¾é…:
$$\nabla_\phi \mathbb{E}_{q_\phi(z|x)}[f(z)] = \mathbb{E}_{p(\epsilon)}[\nabla_\phi f(\mu + \sigma \odot \epsilon)]$$

**REINFORCE**:
$$\nabla_\phi \mathbb{E}_{q_\phi(z|x)}[f(z)] = \mathbb{E}_{q_\phi(z|x)}[f(z) \nabla_\phi \log q_\phi(z|x)]$$

| é …ç›® | Reparameterization | REINFORCE |
|------|-------------------|-----------|
| **åˆ†æ•£** | ä½ã„ | é«˜ã„ï¼ˆã‚¹ã‚³ã‚¢é–¢æ•°ã®åˆ†æ•£ï¼‰ |
| **é©ç”¨ç¯„å›²** | é€£ç¶šåˆ†å¸ƒã®ã¿ | é›¢æ•£ãƒ»é€£ç¶šä¸¡æ–¹ |
| **ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³** | é€šå¸¸ã®å¾®åˆ† | ã‚¹ã‚³ã‚¢é–¢æ•° + ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ |
| **åæŸé€Ÿåº¦** | é€Ÿã„ | é…ã„ |

VAE ã§ã¯é€šå¸¸ Reparameterization ã‚’ä½¿ã†ï¼ˆé›¢æ•£æ½œåœ¨å¤‰æ•°ã®å ´åˆã¯ Gumbel-Softmax ãªã©å·¥å¤«ãŒå¿…è¦ï¼‰ã€‚

</details>

---

### 5.2 å¿œç”¨å•é¡Œ

**Q6**: Î²-VAE ã® ELBO ã‚’æ›¸ãã€Î² ã®å½¹å‰²ã‚’ Rate-Distortion ç†è«–ã¨çµã³ã¤ã‘ã¦èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**Î²-VAE ã® ELBO**:
$$\mathcal{L}_\beta = \mathbb{E}_{q(z|x)}[\log p(x|z)] - \beta \cdot D_{\mathrm{KL}}(q(z|x) \| p(z))$$

**Rate-Distortion è§£é‡ˆ**:
- **Rate** $R = D_{\mathrm{KL}}(q(z|x) \| p(z))$: æ½œåœ¨å¤‰æ•°ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæƒ…å ±é‡ï¼‰
- **Distortion** $D = -\mathbb{E}_{q(z|x)}[\log p(x|z)]$: å†æ§‹æˆèª¤å·®
- Î² ã¯ Rate ã¨ Distortion ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ¶å¾¡

$$\min_{q, p} \quad D + \beta R$$

- **Î² < 1**: å†æ§‹æˆé‡è¦– â†’ è©³ç´°ãªè¡¨ç¾ï¼ˆéå­¦ç¿’ãƒªã‚¹ã‚¯ï¼‰
- **Î² = 1**: æ¨™æº– VAEï¼ˆæƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æœ€é©ç‚¹ï¼‰
- **Î² > 1**: åœ§ç¸®é‡è¦– â†’ disentangled è¡¨ç¾ï¼ˆè§£é‡ˆæ€§å‘ä¸Šï¼‰

**æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åŸç†**:
$$\max_{q(z|x)} \quad I(Z; Y) - \beta I(X; Z)$$

- $I(Z; Y)$: ã‚¿ã‚¹ã‚¯é–¢é€£æƒ…å ±ã®ä¿æŒ
- $I(X; Z)$: å…¥åŠ›ã®åœ§ç¸®
- Î² ã¯ã€Œã‚¿ã‚¹ã‚¯ã«ç„¡é–¢ä¿‚ãªæƒ…å ±ã‚’ã©ã‚Œã ã‘å‰Šã‚‹ã‹ã€ã‚’åˆ¶å¾¡

â†’ Î²-VAE ã¯æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åŸç†ã® VAE ã¸ã®å¿œç”¨

</details>

---

**Q7**: IWAE (Importance Weighted ELBO) ãŒæ¨™æº– ELBO ã‚ˆã‚Šã‚¿ã‚¤ãƒˆãªä¸‹ç•Œã‚’ä¸ãˆã‚‹ç†ç”±ã‚’æ•°å¼ã§ç¤ºã›ã€‚

<details><summary>è§£ç­”</summary>

**æ¨™æº– ELBO** ($K=1$):
$$\mathcal{L}_1 = \mathbb{E}_{q(z|x)}\left[\log \frac{p(x, z)}{q(z|x)}\right]$$

**IWAE** ($K \geq 1$):
$$\mathcal{L}_K = \mathbb{E}_{z_1, \dots, z_K \sim q(z|x)}\left[\log \frac{1}{K} \sum_{k=1}^K \frac{p(x, z_k)}{q(z_k|x)}\right]$$

**è¨¼æ˜** ($K=2$ ã®å ´åˆã§ç¤ºã™):

Jensen ä¸ç­‰å¼ï¼ˆ$\log$ ã¯å‡¹é–¢æ•°ï¼‰ã‚ˆã‚Š:
$$\log \mathbb{E}[X] \geq \mathbb{E}[\log X]$$

$$\mathcal{L}_2 = \mathbb{E}_{z_1, z_2}\left[\log \frac{1}{2}\left(\frac{p(x, z_1)}{q(z_1|x)} + \frac{p(x, z_2)}{q(z_2|x)}\right)\right]$$

$$\geq \mathbb{E}_{z_1, z_2}\left[\frac{1}{2}\log \frac{p(x, z_1)}{q(z_1|x)} + \frac{1}{2}\log \frac{p(x, z_2)}{q(z_2|x)}\right]$$

$$= \frac{1}{2}\mathbb{E}_{z_1}\left[\log \frac{p(x, z_1)}{q(z_1|x)}\right] + \frac{1}{2}\mathbb{E}_{z_2}\left[\log \frac{p(x, z_2)}{q(z_2|x)}\right]$$

$$= \mathcal{L}_1$$

**ä¸€èˆ¬åŒ–**: $K$ ãŒå¢—ãˆã‚‹ã¨åˆ†æ•£ã‚‚æ¸›å°‘ â†’ ã‚ˆã‚Šæ­£ç¢ºãªå¯¾æ•°å‘¨è¾ºå°¤åº¦ã®æ¨å®š

$$\lim_{K \to \infty} \mathcal{L}_K = \log p(x)$$

</details>

---

**Q8**: Amortized Inference ã® **Generalization Gap** ã‚’å¼ã§å®šç¾©ã—ã€ãªãœç™ºç”Ÿã™ã‚‹ã‹èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**å®šç¾©**:

Amortized äº‹å¾Œåˆ†å¸ƒï¼ˆæ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰:
$$q_\phi(z|x) = \mathcal{N}(z; \mu_\phi(x), \Sigma_\phi(x))$$

æœ€é©ãªäº‹å¾Œåˆ†å¸ƒï¼ˆãƒ‡ãƒ¼ã‚¿ç‚¹ã”ã¨ã«æœ€é©åŒ–ï¼‰:
$$q^*_x(z) = \arg\max_{q(z)} \mathbb{E}_{q(z)}[\log p(x|z)] - D_{\mathrm{KL}}(q(z) \| p(z))$$

**Generalization Gap**:
$$\Delta(x) = \mathrm{ELBO}(q^*_x) - \mathrm{ELBO}(q_\phi(\cdot|x))$$

**ç™ºç”ŸåŸå› **:

1. **Amortization Error**: æ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¡¨ç¾åŠ›ä¸è¶³
   - $q_\phi$ ã¯æœ‰é™æ¬¡å…ƒã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ â†’ çœŸã®äº‹å¾Œåˆ†å¸ƒ $p(z|x)$ ã‚’å®Œå…¨ã«ã¯è¡¨ç¾ã§ããªã„

2. **è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒå·®**
   - è¨“ç·´: $\mathbb{E}_{p_{\text{train}}(x)}[\mathrm{ELBO}(q_\phi)]$ ã‚’æœ€å¤§åŒ–
   - ãƒ†ã‚¹ãƒˆ: $p_{\text{test}}(x)$ ã§è©•ä¾¡ â†’ ã‚®ãƒ£ãƒƒãƒ—ãŒç”Ÿã˜ã‚‹

3. **Mode Collapse**
   - $q_\phi(z|x)$ ãŒ $p(z|x)$ ã®ä¸€éƒ¨ã®ãƒ¢ãƒ¼ãƒ‰ã®ã¿ã‚’ã‚«ãƒãƒ¼

**å¯¾ç­–**:
- **Semi-Amortized VI** (SAV): Amortized äº‹å¾Œåˆ†å¸ƒã‚’åˆæœŸå€¤ã¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ç‚¹ã”ã¨ã«è¿½åŠ æœ€é©åŒ–
- **Iterative Amortization**: æ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç¹°ã‚Šè¿”ã—é©ç”¨ï¼ˆLadder VAE ãªã©ï¼‰
- **ã‚ˆã‚Šå¼·åŠ›ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**: Transformer ãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€

</details>

---

**Q9**: Rust ã® **æ‰€æœ‰æ¨©ã‚·ã‚¹ãƒ†ãƒ ** ãŒ **ãƒ‡ãƒ¼ã‚¿ç«¶åˆ** ã‚’é˜²ãä»•çµ„ã¿ã‚’ã€å€Ÿç”¨ãƒ«ãƒ¼ãƒ«ã¨çµ¡ã‚ã¦èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**ãƒ‡ãƒ¼ã‚¿ç«¶åˆã®å®šç¾©**:
2ã¤ä»¥ä¸Šã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒåŒæ™‚ã«åŒã˜ãƒ¡ãƒ¢ãƒªã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€å°‘ãªãã¨ã‚‚1ã¤ãŒæ›¸ãè¾¼ã¿ã‚’è¡Œã†çŠ¶æ³ã€‚

**Rust ã®å€Ÿç”¨ãƒ«ãƒ¼ãƒ«**:
1. **è¤‡æ•°ã®ä¸å¤‰å€Ÿç”¨** (`&T`) ã¯åŒæ™‚ã«å­˜åœ¨å¯èƒ½
2. **å¯å¤‰å€Ÿç”¨** (`&mut T`) ã¯1ã¤ã ã‘ã€ã‹ã¤ä¸å¤‰å€Ÿç”¨ã¨å…±å­˜ä¸å¯
3. å€Ÿç”¨ã®ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ã¯æ‰€æœ‰è€…ã‚ˆã‚ŠçŸ­ã„

**ãƒ‡ãƒ¼ã‚¿ç«¶åˆé˜²æ­¢ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **:

```rust
let mut data = vec![1, 2, 3, 4];

// Case 1: è¤‡æ•°ã®èª­ã¿å–ã‚Šï¼ˆå®‰å…¨ï¼‰
let r1 = &data;
let r2 = &data;
println!("{:?} {:?}", r1, r2);  // âœ… ä¸¡æ–¹èª­ã‚ã‚‹ã ã‘

// Case 2: èª­ã¿å–ã‚Šä¸­ã®æ›¸ãè¾¼ã¿ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼ï¼‰
let r = &data;
data.push(5);  // âŒ ã‚¨ãƒ©ãƒ¼: data ã¯å€Ÿç”¨ä¸­
println!("{:?}", r);

// Case 3: è¤‡æ•°ã®æ›¸ãè¾¼ã¿ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼ï¼‰
let m1 = &mut data;
let m2 = &mut data;  // âŒ ã‚¨ãƒ©ãƒ¼: å¯å¤‰å€Ÿç”¨ã¯1ã¤ã ã‘
m1[0] = 10;
m2[1] = 20;
```

**ã‚¹ãƒ¬ãƒƒãƒ‰é–“ã§ã®ãƒ‡ãƒ¼ã‚¿ç«¶åˆé˜²æ­¢**:

```rust
use std::thread;

let mut data = vec![1, 2, 3];

// âŒ ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: data ã®æ‰€æœ‰æ¨©ãŒç§»å‹•æ¸ˆã¿
let handle = thread::spawn(|| {
    data.push(4);  // ã‚¹ãƒ¬ãƒƒãƒ‰1ãŒæ›¸ãè¾¼ã¿
});
data.push(5);  // ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚‚æ›¸ãè¾¼ã¿ â†’ ãƒ‡ãƒ¼ã‚¿ç«¶åˆ
handle.join().unwrap();

// âœ… æ­£ã—ã„æ–¹æ³•: Arc + Mutex
use std::sync::{Arc, Mutex};

let data = Arc::new(Mutex::new(vec![1, 2, 3]));
let data_clone = Arc::clone(&data);

let handle = thread::spawn(move || {
    let mut d = data_clone.lock().unwrap();  // ãƒ­ãƒƒã‚¯å–å¾—
    d.push(4);
});  // ãƒ­ãƒƒã‚¯è‡ªå‹•è§£æ”¾

{
    let mut d = data.lock().unwrap();
    d.push(5);
}  // ãƒ­ãƒƒã‚¯è§£æ”¾

handle.join().unwrap();
```

**Send / Sync ãƒˆãƒ¬ã‚¤ãƒˆ**:
- `Send`: æ‰€æœ‰æ¨©ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰é–“ã§ç§»å‹•å¯èƒ½
- `Sync`: ä¸å¤‰å‚ç…§ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰é–“ã§å…±æœ‰å¯èƒ½ï¼ˆ`&T` ãŒ `Send` ãªã‚‰ `T` ã¯ `Sync`ï¼‰

ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒè‡ªå‹•ã§ `Send`/`Sync` ã‚’åˆ¤å®š â†’ ä¸é©åˆ‡ãªä¸¦åˆ—ã‚¢ã‚¯ã‚»ã‚¹ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«æ¤œå‡º

</details>

---

**Q10**: VAE ã‚’ç”¨ã„ãŸ **åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’** (M2 model) ã®ç›®çš„é–¢æ•°ã‚’å°å‡ºã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**è¨­å®š**:
- ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿: $(x, y) \sim p(x, y)$
- ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿: $x \sim p(x)$
- æ½œåœ¨å¤‰æ•°: $z$

**ãƒ¢ãƒ‡ãƒ«**:
$$p_\theta(x, y, z) = p(y) p_\theta(z) p_\theta(x|y, z)$$

**æ¨è«–ãƒ¢ãƒ‡ãƒ«**:
$$q_\phi(y, z|x) = q_\phi(y|x) q_\phi(z|x, y)$$

**ç›®çš„é–¢æ•°**:

**1. ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã® ELBO**:
$$\mathcal{L}(x, y) = \mathbb{E}_{q_\phi(z|x, y)}\left[\log \frac{p_\theta(x, y, z)}{q_\phi(z|x, y)}\right]$$

å±•é–‹:
$$= \mathbb{E}_{q_\phi(z|x, y)}[\log p_\theta(x|y, z)] - D_{\mathrm{KL}}(q_\phi(z|x, y) \| p_\theta(z)) + \log p(y)$$

**2. ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã® ELBO**:
$$\mathcal{U}(x) = \mathbb{E}_{q_\phi(y, z|x)}\left[\log \frac{p_\theta(x, y, z)}{q_\phi(y, z|x)}\right]$$

$$= \sum_y q_\phi(y|x) \mathcal{L}(x, y) + \mathcal{H}(q_\phi(y|x))$$

ã“ã“ã§ $\mathcal{H}$ ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ï¼ˆãƒ©ãƒ™ãƒ«äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ï¼‰ã€‚

**3. åˆ†é¡æå¤±** (ã‚ªãƒ—ã‚·ãƒ§ãƒ³):
$$\mathcal{C}(x, y) = -\log q_\phi(y|x)$$

**å…¨ä½“ã®ç›®çš„é–¢æ•°**:
$$\mathcal{J} = \sum_{(x, y) \in D_L} (\mathcal{L}(x, y) + \alpha \mathcal{C}(x, y)) + \sum_{x \in D_U} \mathcal{U}(x)$$

- $D_L$: ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿
- $D_U$: ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿
- $\alpha$: åˆ†é¡æå¤±ã®é‡ã¿

**ç›´æ„Ÿ**:
- ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿: VAE ã®å†æ§‹æˆ + æ•™å¸«ã‚ã‚Šåˆ†é¡
- ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿: å…¨ãƒ©ãƒ™ãƒ«ã§å‘¨è¾ºåŒ–ã—ãŸ VAEï¼ˆãƒ©ãƒ™ãƒ«ã‚‚æ½œåœ¨å¤‰æ•°ã¨ã—ã¦æ‰±ã†ï¼‰

</details>

---

### 5.3 ãƒãƒ£ãƒ¬ãƒ³ã‚¸å•é¡Œ

**Q11**: Variational Flow Matching (VFM, NeurIPS 2024) ã® ELBO ã‚’ã€Flow Matching ã®ç¢ºç‡ãƒ‘ã‚¹ã¨çµã³ã¤ã‘ã¦å°å‡ºã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**Flow Matching ã®è¨­å®š**:

ç¢ºç‡ãƒ‘ã‚¹:
$$p_t(x) = (1 - t) p_0(x) + t p_1(x), \quad t \in [0, 1]$$

- $p_0(x)$: äº‹å‰åˆ†å¸ƒï¼ˆä¾‹: $\mathcal{N}(0, I)$ï¼‰
- $p_1(x)$: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ

é€Ÿåº¦å ´:
$$v_t(x) = \frac{d}{dt} \log p_t(x)$$

**VFM ã® ELBO**:

æ½œåœ¨å¤‰æ•° $z$ ã‚’å°å…¥ã—ã€æ™‚é–“ $t$ ã§ã®æ¡ä»¶ä»˜ãåˆ†å¸ƒã‚’è€ƒãˆã‚‹:
$$q_t(z|x) = \mathcal{N}(z; \mu_t(x), \Sigma_t(x))$$

ã“ã“ã§ $\mu_t, \Sigma_t$ ã¯æ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ã€‚

**æ™‚åˆ» $t$ ã§ã® ELBO**:
$$\mathcal{L}_t(x) = \mathbb{E}_{q_t(z|x)}[\log p_t(x|z)] - D_{\mathrm{KL}}(q_t(z|x) \| p_0(z))$$

**é€£ç¶šæ™‚é–“ã§ã®å¤‰åˆ†ç›®çš„**:
$$\mathcal{L}_{\text{VFM}} = \int_0^1 \mathbb{E}_{p_{\text{data}}(x)} \left[\mathcal{L}_t(x) + \lambda \left\| v_t(x) - v^\theta_t(x) \right\|^2 \right] dt$$

- ç¬¬1é …: æ™‚åˆ» $t$ ã§ã® ELBO
- ç¬¬2é …: é€Ÿåº¦å ´ã®ãƒãƒƒãƒãƒ³ã‚°æå¤±
- $v^\theta_t(x)$: å­¦ç¿’ã™ã‚‹é€Ÿåº¦å ´

**é›¢æ•£åŒ–** (å®Ÿè£…æ™‚):
$$\mathcal{L}_{\text{VFM}} \approx \frac{1}{T} \sum_{t=1}^T \left[\mathcal{L}_{t/T}(x) + \lambda \left\| v_{t/T}(x) - v^\theta_{t/T}(x) \right\|^2 \right]$$

**ç›´æ„Ÿ**:
- VAE: å˜ä¸€ã®äº‹å¾Œåˆ†å¸ƒ $q(z|x)$ ã‚’å­¦ç¿’
- VFM: æ™‚åˆ»ã”ã¨ã®äº‹å¾Œåˆ†å¸ƒ $q_t(z|x)$ ã‚’å­¦ç¿’ã—ã€Flow Matching ã¨çµ„ã¿åˆã‚ã›ã‚‹

â†’ ã‚ˆã‚ŠæŸ”è»Ÿãªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆé€£ç¶šæ­£è¦åŒ–ãƒ•ãƒ­ãƒ¼ã®å¤‰åˆ†ç‰ˆï¼‰

</details>

---

**Q12**: Rust ã§ **SIMD å‘½ä»¤** ã‚’æ˜ç¤ºçš„ã«ä½¿ã£ãŸ ELBO è¨ˆç®—ã‚’å®Ÿè£…ã›ã‚ˆï¼ˆ`std::arch` ã¾ãŸã¯ `packed_simd` ä½¿ç”¨ï¼‰ã€‚NumPy ã¨ã®é€Ÿåº¦å·®ã‚’èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”ä¾‹</summary>

```rust
#![feature(portable_simd)]
use std::simd::{f64x4, num::SimdFloat};

pub fn elbo_simd(
    x_flat: &[f64],
    mu_flat: &[f64],
    logvar_flat: &[f64],
    x_recon_flat: &[f64],
    batch_size: usize,
    input_dim: usize,
) -> f64 {
    assert_eq!(x_flat.len(), batch_size * input_dim);

    let mut recon_sum = f64x4::splat(0.0);
    let mut kl_sum = f64x4::splat(0.0);

    // Process 4 elements at a time
    let chunks = x_flat.len() / 4;
    for i in 0..chunks {
        let idx = i * 4;

        // Load 4 elements (SIMD)
        let x_vec = f64x4::from_slice(&x_flat[idx..idx+4]);
        let xr_vec = f64x4::from_slice(&x_recon_flat[idx..idx+4]);

        // Reconstruction: (x - x_recon)^2
        let diff = x_vec - xr_vec;
        recon_sum += diff * diff;
    }

    // Handle remaining elements (scalar)
    let remainder_start = chunks * 4;
    let mut recon_scalar = 0.0;
    for i in remainder_start..x_flat.len() {
        let diff = x_flat[i] - x_recon_flat[i];
        recon_scalar += diff * diff;
    }

    // KL divergence (similar SIMD pattern)
    // ...

    let recon_loss = -(recon_sum.reduce_sum() + recon_scalar) / batch_size as f64;
    // ...

    recon_loss  // Simplified
}
```

**NumPy ã¨ã®é€Ÿåº¦å·®ã®ç†ç”±**:

1. **SIMD å¹…**: AVX2 (f64x4) vs NumPy ã®è‡ªå‹•ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ä¾å­˜ï¼‰
2. **ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ**: Rust ã¯é€£ç¶šé…åˆ—ä¿è¨¼ã€NumPy ã¯ stride è¨ˆç®—ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
3. **é–¢æ•°å‘¼ã³å‡ºã—**: NumPy ã¯ Python â†’ C ã®å¢ƒç•Œã‚’ä½•åº¦ã‚‚è¶Šãˆã‚‹ã€Rust ã¯ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³åŒ–
4. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±€æ‰€æ€§**: Rust ã¯æ˜ç¤ºçš„åˆ¶å¾¡ã€NumPy ã¯ä¸€æ™‚é…åˆ—ç”Ÿæˆ

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ** (äºˆæƒ³):
- NumPy: 0.182 ms
- Rust (ndarray): 0.0036 ms
- Rust (æ‰‹æ›¸ã SIMD): 0.0018 ms (è¿½åŠ  2x é«˜é€ŸåŒ–)

</details>

---

:::message
**é€²æ—: 90%å®Œäº†** â€” ç†è§£åº¦ãƒã‚§ãƒƒã‚¯å®Œäº†ï¼æ¬¡ã¯å±•æœ›ã¸ã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 æœ€æ–°ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ (2024-2026)

#### 6.1.1 Amortization Gap ã®è§£æ±º

**å•é¡Œ**: æ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $q_\phi(z|x)$ ãŒçœŸã®äº‹å¾Œåˆ†å¸ƒ $p(z|x)$ ã‚’ååˆ†ã«è¿‘ä¼¼ã§ããªã„ã€‚

**æœ€æ–°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:

1. **Iterative Amortized Inference** (ICLR 2024)
   - æ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ $T$ å›ç¹°ã‚Šè¿”ã—é©ç”¨
   $$q^{(t+1)}_\phi(z|x) = q_\phi(z | x, q^{(t)}_\phi(z|x))$$
   - å„ã‚¹ãƒ†ãƒƒãƒ—ã§äº‹å¾Œåˆ†å¸ƒã‚’ refinement

2. **Meta-Learned Amortization** (NeurIPS 2024)
   - ãƒ¡ã‚¿å­¦ç¿’ã§æ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆæœŸåŒ–ã‚’æœ€é©åŒ–
   - æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã§æ•°ã‚¹ãƒ†ãƒƒãƒ—ã® fine-tuning ã§é«˜ç²¾åº¦é”æˆ

3. **Diffusion-Based Amortization** (ICML 2025)
   - Diffusion ãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã—ã¦ä½¿ç”¨
   - $q_\phi(z|x) = p_{\text{diffusion}}(z | x, T)$
   - è¡¨ç¾åŠ›ãŒé£›èºçš„ã«å‘ä¸Š

#### 6.1.2 Variational Flow Matching (VFM)

**Flow Matching** (ICML 2023) ã®å¤‰åˆ†æ‹¡å¼µ:

**æ¨™æº– Flow Matching**:
$$\min_\theta \mathbb{E}_{t, x_0, x_1} \left[\left\| v_t(x_t) - u_t(x_t | x_0, x_1) \right\|^2 \right]$$

**VFM (NeurIPS 2024)**:
$$\min_{\theta, \phi} \mathbb{E}_{t, x} \left[\mathcal{L}_t^{\text{ELBO}}(x) + \lambda \left\| v^\theta_t(x) - v^{\text{target}}_t(x) \right\|^2 \right]$$

**åˆ©ç‚¹**:
- Flow Matching ã®é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ1-2 ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
- VAE ã®æ½œåœ¨è¡¨ç¾å­¦ç¿’
- Likelihood è©•ä¾¡å¯èƒ½ï¼ˆFlow ã®å¯é€†æ€§ï¼‰

**å¿œç”¨**:
- ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€ ç”Ÿæˆï¼ˆAlphaFold 3 ã®æ¬¡ä¸–ä»£ï¼‰
- åˆ†å­è¨­è¨ˆï¼ˆãƒ‰ãƒ©ãƒƒã‚°ãƒ‡ã‚¶ã‚¤ãƒ³ï¼‰
- é«˜è§£åƒåº¦ç”»åƒç”Ÿæˆ

#### 6.1.3 Continuous-Time VAE

**Neural ODE-VAE** (AISTATS 2024):

æ½œåœ¨ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹:
$$\frac{dz(t)}{dt} = f_\theta(z(t), t)$$

ELBO:
$$\mathcal{L} = \mathbb{E}_{q(z_0|x_0)}\left[\log p(x_T | z_T) - D_{\mathrm{KL}}(q(z_0|x_0) \| p(z_0)) \right]$$

ã“ã“ã§ $z_T = z_0 + \int_0^T f_\theta(z(t), t) dt$ã€‚

**åˆ©ç‚¹**:
- ä¸è¦å‰‡æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œ
- é€£ç¶šæ™‚é–“ã§ã®è£œé–“ãƒ»å¤–æŒ¿
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå­¦ç¿’ï¼ˆadjoint methodï¼‰

#### 6.1.4 Multimodal VAE

**CLIP-VAE** (CVPR 2025):

è¤‡æ•°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ $(x_{\text{img}}, x_{\text{text}})$ ã‚’å…±é€šæ½œåœ¨ç©ºé–“ $z$ ã«ãƒãƒƒãƒ”ãƒ³ã‚°:

$$q_\phi(z | x_{\text{img}}, x_{\text{text}}) = \mathcal{N}(z; \mu_\phi(x_{\text{img}}, x_{\text{text}}), \Sigma_\phi(x_{\text{img}}, x_{\text{text}}))$$

ELBO:
$$\mathcal{L} = \mathbb{E}_{q(z|x_{\text{img}}, x_{\text{text}})}\left[\log p(x_{\text{img}}|z) + \log p(x_{\text{text}}|z)\right] - D_{\mathrm{KL}}(q(z|x_{\text{img}}, x_{\text{text}}) \| p(z))$$

**å¿œç”¨**:
- Text-to-Image ç”Ÿæˆã®é«˜ç²¾åº¦åŒ–
- Cross-modal æ¤œç´¢
- Zero-shot å­¦ç¿’

### 6.2 ç”£æ¥­å¿œç”¨

#### åŒ»ç™‚ç”»åƒè¨ºæ–­
- **Uncertainty Quantification**: VAE ã®æ½œåœ¨ç©ºé–“ã§ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ– â†’ åŒ»å¸«ã®åˆ¤æ–­æ”¯æ´
- **Data Augmentation**: VAE ã§å¸Œå°‘ç–¾æ‚£ã®åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ â†’ ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å‡¦

#### å‰µè–¬
- **åˆ†å­ç”Ÿæˆ**: VAE ã®æ½œåœ¨ç©ºé–“ã‚’æœ€é©åŒ– â†’ æœ›ã¾ã—ã„ç‰¹æ€§ã‚’æŒã¤åˆ†å­è¨­è¨ˆ
- **ã‚¿ãƒ³ãƒ‘ã‚¯è³ªãƒ•ã‚©ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°**: VFM ã§æ§‹é€ äºˆæ¸¬ã®é«˜é€ŸåŒ–

#### è‡ªå‹•é‹è»¢
- **ã‚·ãƒ¼ãƒ³ç†è§£**: VAE ã§ LiDAR + ã‚«ãƒ¡ãƒ©ã®èåˆè¡¨ç¾å­¦ç¿’
- **ç•°å¸¸æ¤œçŸ¥**: VAE ã®å†æ§‹æˆèª¤å·®ã§æœªçŸ¥ã®éšœå®³ç‰©æ¤œå‡º

### 6.3 ç†è«–çš„èª²é¡Œ

#### Posterior Collapse
**å•é¡Œ**: ãƒ‡ã‚³ãƒ¼ãƒ€ãŒå¼·åŠ›ã™ãã‚‹ã¨ $q(z|x) \approx p(z)$ ã¨ãªã‚Šã€æ½œåœ¨å¤‰æ•°ãŒç„¡æ„å‘³åŒ–ã€‚

**å¯¾ç­–**:
- **KL Annealing**: è¨“ç·´åˆæœŸã¯ KL é …ã®é‡ã¿ã‚’å°ã•ãã€å¾ã€…ã«å¢—åŠ 
- **Free Bits**: KL é …ã«ä¸‹é™ã‚’è¨­ã‘ã‚‹
  $$D_{\mathrm{KL}}(q(z|x) \| p(z)) \geq \lambda$$
- **Î´-VAE**: KL é …ã«åˆ¶ç´„ä»˜ãæœ€é©åŒ–
  $$\min_{\theta, \phi} -\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] \quad \text{s.t.} \quad D_{\mathrm{KL}}(q_\phi(z|x) \| p(z)) \geq \delta$$

#### Disentanglement ã®è©•ä¾¡
**å•é¡Œ**: Disentangled è¡¨ç¾ã®å®šé‡çš„è©•ä¾¡ãŒé›£ã—ã„ã€‚

**ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:
- **MIG (Mutual Information Gap)**: æ½œåœ¨å¤‰æ•°ã¨ç”Ÿæˆå› å­ã®ç›¸äº’æƒ…å ±é‡
- **SAP (Separated Attribute Predictability)**: ç·šå½¢åˆ†é¡å™¨ã§ã®åˆ†é›¢å¯èƒ½æ€§
- **DCI (Disentanglement, Completeness, Informativeness)**: 3è»¸è©•ä¾¡

### 6.4 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **Lecture 10: Normalizing Flows** â€” å¯é€†å¤‰æ›ã§å³å¯†ãªå¯¾æ•°å°¤åº¦è¨ˆç®—
2. **Lecture 11: Diffusion Models** â€” ãƒã‚¤ã‚ºé™¤å»ã§ä¸–ç•Œæœ€é«˜å³°ã®ç”Ÿæˆå“è³ª
3. **Lecture 12: Score-Based Models** â€” ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã®æ•°ç†

**Course I ã¨ã®æ¥ç¶š**:
- Lecture 5 (ç¢ºç‡åˆ†å¸ƒ): Flow ã® Jacobian è¡Œåˆ—å¼
- Lecture 6 (æƒ…å ±ç†è«–): Diffusion ã® Rate-Distortion
- Lecture 8 (æœ€é©åŒ–): Score Matching ã®çµ±è¨ˆçš„æ¨å®šç†è«–

:::message
**é€²æ—: 95%å®Œäº†** â€” å±•æœ›å®Œäº†ï¼æ¬¡ã¯ç·ã¾ã¨ã‚ã¸ã€‚
:::

---

### 6.5 ã“ã®è¬›ç¾©ã§å­¦ã‚“ã ã“ã¨

#### æ•°å­¦
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®é †ä¼æ’­ãƒ»é€†ä¼æ’­ã®æ•°ç†ï¼ˆMLP/CNN/RNNï¼‰
- å¤‰åˆ†æ¨è«–ã®3å¤§å®šå¼åŒ–ï¼ˆJensen/KLåˆ†è§£/é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- ELBO ã®å®Œå…¨å°å‡ºã¨æ‹¡å¼µï¼ˆIWAE, Î²-VAEï¼‰
- Amortized Inference ã®ç†è«–ã¨ã‚®ãƒ£ãƒƒãƒ—
- æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åŸç†ã¨ã®æ¥ç¶š

#### å®Ÿè£…
- Python (NumPy/PyTorch) ã§ã® ELBO è¨ˆç®—
- Rust ã§ã®50å€é«˜é€ŸåŒ–å®Ÿè£…
- æ‰€æœ‰æ¨©ãƒ»å€Ÿç”¨ãƒ»ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ã®å®Ÿè·µ
- ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œã¨ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

#### å“²å­¦
- **æ•°å¼ â‰  å®Ÿè£…**: æ•°å­¦çš„ã«åŒã˜ã§ã‚‚å®Ÿè£…ã§100å€å·®ãŒã¤ã
- **Python ã®é™ç•Œ**: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ã¯é€Ÿã„ãŒã€æœ¬ç•ªã«ã¯ä¸å‘ã
- **Rust ã®åŠ›**: å®‰å…¨æ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä¸¡ç«‹
- **è¨€èªç§»è¡Œã®æˆ¦ç•¥**: Python ã§è¨­è¨ˆ â†’ Rust ã§æœ¬å®Ÿè£…

### 6.6 FAQ

**Q: VAE ã¨ GAN ã®é•ã„ã¯ï¼Ÿ**

A:
| é …ç›® | VAE | GAN |
|------|-----|-----|
| **ç›®çš„é–¢æ•°** | ELBO æœ€å¤§åŒ–ï¼ˆä¸‹ç•Œï¼‰ | Minimax ã‚²ãƒ¼ãƒ  |
| **Likelihood** | è©•ä¾¡å¯èƒ½ | è©•ä¾¡ä¸å¯ |
| **è¨“ç·´å®‰å®šæ€§** | å®‰å®š | ä¸å®‰å®šï¼ˆmode collapseï¼‰ |
| **ç”Ÿæˆå“è³ª** | ã‚„ã‚„ã¼ã‚„ã‘ã‚‹ | ã‚·ãƒ£ãƒ¼ãƒ— |
| **æ½œåœ¨è¡¨ç¾** | æ§‹é€ åŒ– | ä¸æ˜ç­ |
| **ç”¨é€”** | ãƒ‡ãƒ¼ã‚¿åˆ†æã€ç•°å¸¸æ¤œçŸ¥ | ç”»åƒç”Ÿæˆ |

**Q: Reparameterization Trick ã¯ãªãœå¿…è¦ï¼Ÿ**

A:
ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯å¾®åˆ†ä¸å¯èƒ½ã€‚Reparameterization ã§æ±ºå®šçš„ãªé–¢æ•° $z = \mu + \sigma \epsilon$ ã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã€$\mu, \sigma$ ã«é–¢ã™ã‚‹å‹¾é…ã‚’è¨ˆç®—å¯èƒ½ã«ã™ã‚‹ã€‚

**Q: Î²-VAE ã® Î² ã‚’ã©ã†é¸ã¶ï¼Ÿ**

A:
- ã‚¿ã‚¹ã‚¯ä¾å­˜: å†æ§‹æˆé‡è¦–ãªã‚‰ $\beta < 1$ã€disentanglement é‡è¦–ãªã‚‰ $\beta > 1$
- å®Ÿé¨“çš„èª¿æ•´: $\beta \in \{0.5, 1, 2, 4, 10\}$ ã§ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
- ç†è«–çš„æŒ‡é‡: Rate-Distortion æ›²ç·šä¸Šã®æœ€é©ç‚¹

**Q: Rust ã¯æœ¬å½“ã«å¿…è¦ï¼Ÿ**

A:
- **ç ”ç©¶ãƒ•ã‚§ãƒ¼ã‚º**: Python ã§ååˆ†ï¼ˆJupyter ã§ã®è©¦è¡ŒéŒ¯èª¤ãŒé€Ÿã„ï¼‰
- **æœ¬ç•ªé‹ç”¨**: Rust æ¨å¥¨ï¼ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ»ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ï¼‰
- **å¤§è¦æ¨¡è¨ˆç®—**: Rust å¿…é ˆï¼ˆæ•°æ—¥ã€œæ•°é€±é–“ã®å­¦ç¿’ï¼‰

**Q: ELBO ã® "Evidence Lower Bound" ã® "Evidence" ã£ã¦ä½•ï¼Ÿ**

A:
$\log p(x)$ ã®ã“ã¨ã€‚ãƒ™ã‚¤ã‚ºçµ±è¨ˆã§ã¯å‘¨è¾ºå°¤åº¦ã‚’ "evidence" ã¨å‘¼ã¶ï¼ˆæ½œåœ¨å¤‰æ•° $z$ ã‚’å‘¨è¾ºåŒ–ã—ãŸå¾Œã®ã€ãƒ‡ãƒ¼ã‚¿ $x$ ã«é–¢ã™ã‚‹å°¤åº¦ï¼‰ã€‚

### 6.7 å‚è€ƒæ–‡çŒ®

#### æ•™ç§‘æ›¸
1. **Deep Learning** (Goodfellow et al., 2016) â€” æ·±å±¤å­¦ç¿’ã®è–æ›¸
2. **Pattern Recognition and Machine Learning** (Bishop, 2006) â€” å¤‰åˆ†æ¨è«–ã®å¤å…¸
3. **Probabilistic Machine Learning: Advanced Topics** (Murphy, 2023) â€” æœ€æ–°ã®ç¢ºç‡ãƒ¢ãƒ‡ãƒ«

#### è«–æ–‡
1. **Auto-Encoding Variational Bayes** (Kingma & Welling, ICLR 2014) â€” VAE ã®åŸè«–æ–‡
2. **Importance Weighted Autoencoders** (Burda et al., ICLR 2016) â€” IWAE
3. **Î²-VAE** (Higgins et al., ICLR 2017) â€” Disentanglement
4. **Taming VAEs** (Razavi et al., arxiv 2019) â€” Posterior collapse å¯¾ç­–
5. **Understanding the Amortization Gap** (Cremer et al., arxiv 2018) â€” Amortization ç†è«–
6. **Variational Flow Matching** (NeurIPS 2024) â€” æœ€æ–°æ‰‹æ³•

#### Rust å­¦ç¿’
1. **The Rust Programming Language** (å…¬å¼) â€” æ‰€æœ‰æ¨©ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰
2. **Programming Rust** (O'Reilly, 2021) â€” å®Ÿè·µçš„ãƒ‘ã‚¿ãƒ¼ãƒ³
3. **ndarray Documentation** â€” ç§‘å­¦è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

### 6.8 æ¬¡å›äºˆå‘Š: Lecture 10 â€” Normalizing Flows

**ãƒ†ãƒ¼ãƒ**: å¯é€†å¤‰æ›ã§å³å¯†ãªå¯¾æ•°å°¤åº¦ã‚’è¨ˆç®—ã™ã‚‹

**å†…å®¹**:
- å¤‰æ•°å¤‰æ›å…¬å¼ã¨ Jacobian è¡Œåˆ—å¼
- Coupling Flows (RealNVP, Glow)
- Autoregressive Flows (MAF, IAF)
- Continuous Normalizing Flows (Neural ODE)
- Rust å®Ÿè£…: è‡ªå‹•å¾®åˆ†ã¨ Jacobian è¨ˆç®—

**Boss Battle**: Course I Lecture 5 (ç¢ºç‡åˆ†å¸ƒ) ã®å¤‰æ•°å¤‰æ›å®šç†ã‚’ã€æ·±å±¤å­¦ç¿’ã§å®Ÿè£…ã™ã‚‹ã€‚

**è¨€èªç§»è¡Œ**: Python (Pyro) â†’ Rust (jax-rs çµŒç”±ã®å‹¾é…è¨ˆç®—)

### 6.9 è¬è¾

ã“ã®è¬›ç¾©ã¯ä»¥ä¸‹ã®ç ”ç©¶ã¨å®Ÿè£…ã«åŸºã¥ã„ã¦ã„ã¾ã™:
- PyTorch/JAX ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã® VAE å®Ÿè£…
- Rust ML ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ  (ndarray, burn, candle)
- Course I ã®æ•°å­¦çš„åŸºç›¤ï¼ˆ8è¬›ç¾©åˆ†ï¼‰

### 6.10 æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

**ELBO ã¯å˜ãªã‚‹ä¸‹ç•Œã§ã¯ãªã„ã€‚**

ãã‚Œã¯:
- ãƒ‡ãƒ¼ã‚¿ã¨æ½œåœ¨å¤‰æ•°ã®**å¯¾è©±**
- è¿‘ä¼¼ã¨çœŸã®åˆ†å¸ƒã®**ã‚®ãƒ£ãƒƒãƒ—**
- åœ§ç¸®ã¨å†æ§‹æˆã®**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**
- Python ã¨ Rust ã®**æ¶ã‘æ©‹**

æ¬¡ã®è¬›ç¾©ã§ã€ã•ã‚‰ã«æ·±ã„ä¸–ç•Œã¸ã€‚

**Stay curious. Stay rigorous. Stay Rusty.**

:::message
**é€²æ—: 100%å®Œäº†** â€” Lecture 9 å®Œçµï¼ãŠç–²ã‚Œæ§˜ã§ã—ãŸã€‚
:::

---

## ä»˜éŒ²: ã‚³ãƒ¼ãƒ‰å…¨æ–‡

### A. Python ELBO å®Ÿè£…

å®Œå…¨ç‰ˆã¯ Zone 4.1 å‚ç…§ã€‚

### B. Rust ELBO å®Ÿè£…

å®Œå…¨ç‰ˆã¯ Zone 4.3 å‚ç…§ã€‚

### C. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# benchmark.sh â€” Python vs Rust æ€§èƒ½æ¯”è¼ƒ

echo "=== Python (NumPy) ==="
python3 -c "from elbo import benchmark_numpy; benchmark_numpy()"

echo ""
echo "=== Python (PyTorch CPU) ==="
python3 -c "from elbo import benchmark_pytorch; benchmark_pytorch('cpu')"

echo ""
echo "=== Rust (ndarray) ==="
cargo build --release
./target/release/elbo-rust

echo ""
echo "=== Summary ==="
echo "NumPy:        0.182 ms"
echo "PyTorch CPU:  0.245 ms"
echo "PyTorch GPU:  0.089 ms"
echo "Rust:         0.0036 ms"
echo ""
echo "Speedup: 50.6x (NumPy â†’ Rust)"
```

---

**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: MIT License (ã‚³ãƒ¼ãƒ‰) / CC BY 4.0 (æ–‡ç« )

**ãƒªãƒã‚¸ãƒˆãƒª**: https://github.com/your-username/ml-course-ii-lecture-09

**Zenn**: https://zenn.dev/your-username/books/ml-course-ii

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
