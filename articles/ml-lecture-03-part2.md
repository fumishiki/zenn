---
title: "ç¬¬3å›: ç·šå½¢ä»£æ•° II: SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ« â€” ä¸‡èƒ½ãƒŠã‚¤ãƒ•SVDã¨é€†ä¼æ’­ã®æ•°å­¦ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ”¬"
type: "tech"
topics: ["machinelearning", "deeplearning", "linearalgebra", "math", "python"]
published: true
slug: "ml-lecture-03-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Python"]
keywords: ["SVD", "ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼", "è¡Œåˆ—å¾®åˆ†", "è‡ªå‹•å¾®åˆ†", "LoRA"]
---

# ç¬¬3å›: ç·šå½¢ä»£æ•° II â€” SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«ã€å¾Œç·¨ï¼šå®Ÿè£…ç·¨ã€‘

> **ç†è«–ç·¨ã¸ã®ãƒªãƒ³ã‚¯**: [ç¬¬3å› Part1ï¼ˆç†è«–ç·¨ï¼‰](/articles/ml-lecture-03-part1)

## Learning Objectives

- [ ] truncated SVD ã® shape contract ã‚’ç ´ã‚‰ãšã«å®Ÿè£…ã—ã€èª¤å·®ãŒã€Œæ¨ã¦ãŸç‰¹ç•°å€¤ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ã«ç­‰ã—ã„ã“ã¨ã‚’æ•°å€¤æ¤œè¨¼ã§ãã‚‹
- [ ] Randomized SVD ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è¿½ã„ã‹ã‘ã€ãªãœãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§æ­£ç¢ºãªéƒ¨åˆ†ç©ºé–“ãŒå–ã‚Œã‚‹ã®ã‹ã‚’èª¬æ˜ã§ãã‚‹
- [ ] LoRA ã®è¡Œåˆ—åˆæœŸåŒ–æˆ¦ç•¥ï¼ˆKaiming normal + ã‚¼ãƒ­åˆæœŸåŒ–ï¼‰ã®æ•°å­¦çš„æ ¹æ‹ ã‚’è¿°ã¹ã‚‰ã‚Œã‚‹
- [ ] einsum ã®æ·»å­—ãƒ«ãƒ¼ãƒ«ã‚’ãƒ‘ã‚¿ãƒ¼ãƒ³è¡¨ã‹ã‚‰é€†å¼•ãã—ã€ä»»æ„ã®ãƒ†ãƒ³ã‚½ãƒ«ç¸®ç´„ã‚’æ›¸ã‘ã‚‹
- [ ] è§£æå‹¾é…ã‚’æ•°å€¤å¾®åˆ†ã§æ¤œç®—ã—ã€ç›¸å¯¾èª¤å·®ã®åˆ¤å®šåŸºæº–ã‚’æ­£ã—ãé©ç”¨ã§ãã‚‹
- [ ] Dual Numbers ã«ã‚ˆã‚‹ Forward Mode AD ã‚’å®Ÿè£…ã—ã€æ©Ÿæ¢°å¾®åˆ†ã¨ã®ç­‰ä¾¡æ€§ã‚’ç¢ºèªã§ãã‚‹
- [ ] Reverse Mode AD ã® Wengert tape ã®æ§‹é€ ã‚’èª¬æ˜ã—ã€ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è¿°ã¹ã‚‰ã‚Œã‚‹
- [ ] 2å±¤NNã®å…¨ã¦ã®å‹¾é…ã‚’æ‰‹ã§å°å‡ºã—ã€shape ã‚’æ­£ã—ãè¿½è·¡ã§ãã‚‹

---

## ğŸ’» Z5. è©¦ç·´ï¼ˆ75åˆ†ï¼‰â€” æ•°å­¦ã‚’å®Ÿè£…ã«è½ã¨ã™

ç·šå½¢ä»£æ•°ã®å®Ÿè£…ãŒå£Šã‚Œã‚‹ç¬é–“ã¯ã€ã„ã¤ã‚‚åŒã˜ã ã€‚

- shape ã‚’ã€Œã‚ã‹ã£ã¦ã‚‹ã€ã¨æ€ã„è¾¼ã‚€ï¼ˆ`(m, n)` ã¨ `(n,)` ã®åŒºåˆ¥ãŒæº¶ã‘ã‚‹ï¼‰
- ã€Œæ•°å¼ã¨å®Ÿè£…ãŒ1:1ã€ã®ã¤ã‚‚ã‚ŠãŒã€æ·»å­—ãŒãšã‚Œã¦ã„ã‚‹
- æ•°å€¤çš„ãªåŒä¸€æ€§ã‚’å³å¯†ä¸€è‡´ã§æ¯”è¼ƒã™ã‚‹ï¼ˆSVD ã®ç¬¦å·è‡ªç”±åº¦ã§æ­»ã¬ï¼‰
- è§£æå‹¾é…ã‚’å®Ÿè£…ã—ã¦ãƒã‚°ãŒã‚ã£ã¦ã‚‚ã€Œå‹•ã„ã¦ã„ã‚‹ã€ã‚ˆã†ã«è¦‹ãˆã‚‹

ã‚³ãƒ¼ãƒ‰ã¯ã€Œå¼ãŒå˜˜ã‚’ã¤ã„ã¦ã„ãªã„ã‹ã€ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã‚ã‚‹â€”â€”é€Ÿåº¦ã®ãŸã‚ã§ã‚‚ã€ä¾¿åˆ©ã•ã®ãŸã‚ã§ã‚‚ãªã„ã€‚



```mermaid
graph TD
    A["Z5.1 SVDå®Œå…¨å®Ÿè£…<br/>truncated + Randomized + LoRA"] --> B["Z5.2 einsumå®Œå…¨å®Ÿè£…<br/>ãƒ‘ã‚¿ãƒ¼ãƒ³é›† + è¨ˆç®—é‡"]
    B --> C["Z5.3 è¡Œåˆ—å¾®åˆ†å®Ÿè£…<br/>æ•°å€¤å‹¾é…æ¤œè¨¼"]
    C --> D["Z5.4 è‡ªå‹•å¾®åˆ†å®Ÿè£…<br/>Dual Numbers"]
    D --> E["Z5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸<br/>SVDãƒã‚¤ã‚ºé™¤å» + 2å±¤NN"]
    style A fill:#e1f5fe
    style D fill:#c8e6c9
```

---

### 5.1 SVDå®Œå…¨å®Ÿè£…

#### 5.1.1 truncated SVD ã®æ•°å­¦çš„æ§‹é€ 

Compact SVD ã®è¨˜æ³•ã‚’å†ç¢ºèªã™ã‚‹ã€‚`$A \in \mathbb{R}^{m \times n}$`ã€`$r = \text{rank}(A)$` ã¨ã—ã¦:

```math
A = U \Sigma V^\top, \quad U \in \mathbb{R}^{m \times r},\ \Sigma = \text{diag}(\sigma_1, \ldots, \sigma_r),\ V \in \mathbb{R}^{n \times r}
```

rank-`$k$` è¿‘ä¼¼ï¼ˆ`$k \leq r$`ï¼‰ã¯ä¸Šä½ `$k$` æˆåˆ†ã ã‘ã‚’å–ã‚‹:

```math
A_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top = U_{:,1:k}\ \Sigma_{1:k}\ V^\top_{1:k,:}
```

**shape ã®å¥‘ç´„**:
- `$U_{:,1:k} \in \mathbb{R}^{m \times k}$` ï¼ˆ`mÃ—k`ï¼‰
- `$\Sigma_{1:k} = \text{diag}(\sigma_1, \ldots, \sigma_k) \in \mathbb{R}^{k \times k}$` ï¼ˆ`kÃ—k`ï¼‰
- `$V^\top_{1:k,:} \in \mathbb{R}^{k \times n}$` ï¼ˆ`kÃ—n`ï¼‰
- ç© `$A_k \in \mathbb{R}^{m \times n}$` âœ“

`np.linalg.svd(A, full_matrices=False)` ã¯ `$U$`ï¼ˆ`mÃ—min(m,n)`ï¼‰ã€`$s$`ï¼ˆ`min(m,n),`ï¼‰ã€`$V^\top$`ï¼ˆ`min(m,n)Ã—n`ï¼‰ã‚’è¿”ã™ã®ã§ã€sliceã¯ `[:, :k]`, `[:k]`, `[:k, :]`ã€‚**`diag(s)` ã‚’ä½œã‚‰ãªã„**ã®ãŒé‡è¦ã ã€‚`s[:k, None] * Vt[:k, :]` ã¯ broadcasting ã§ `kÃ—n` ã‚’ç›´æ¥ä½œã‚‹ã€‚

**èª¤å·®ã®ä¿è¨¼**ï¼ˆEckart-Youngå®šç†ï¼‰:

```math
\|A - A_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2
```

ã¤ã¾ã‚Šç›¸å¯¾èª¤å·®ã¯:

```math
\frac{\|A - A_k\|_F}{\|A\|_F} = \sqrt{\frac{\sum_{i=k+1}^r \sigma_i^2}{\sum_{i=1}^r \sigma_i^2}}
```

ã“ã®ç­‰å·ãŒå®Ÿè£…ã®ã€Œæ¤œç®—å¼ã€ã«ãªã‚‹ã€‚æ•°å€¤çš„ã«ã¯ `assert abs(err - bound) < 1e-6` ãŒé€šã‚‰ãªã‘ã‚Œã°ã€å®Ÿè£…ã‹Eckart-Youngå®šç†ã®è§£é‡ˆã«èª¤ã‚ŠãŒã‚ã‚‹ã€‚

**ç¬¦å·è‡ªç”±åº¦ã¸ã®æ³¨æ„**: `$(\mathbf{u}_i, \mathbf{v}_i)$` ã¯ `$(-\mathbf{u}_i, -\mathbf{v}_i)$` ã¨äº¤æ›ã—ã¦ã‚‚ `$\sigma_i \mathbf{u}_i \mathbf{v}_i^\top$` ã¯ä¸å¤‰ã€‚ã—ãŸãŒã£ã¦ `$U$` ã®åˆ—ã‚’ç›´æ¥æ¯”è¼ƒã—ã¦ã¯ã„ã‘ãªã„ã€‚å†æ§‹æˆèª¤å·® `$\|A - A_k\|_F$` ã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã€‚

**SVDã¨PCAã®é–¢ä¿‚**: ãƒ‡ãƒ¼ã‚¿è¡Œåˆ— `$X \in \mathbb{R}^{n \times d}$`ï¼ˆè¡Œ=ã‚µãƒ³ãƒ—ãƒ«ã€åˆ—=ç‰¹å¾´ï¼‰ã‚’ä¸­å¿ƒåŒ–ã—ãŸå¾Œ `$\tilde{X} = X - \bar{X}$`ã€PCAã®ä¸»æˆåˆ† `$V_k$` ã¯ `$\tilde{X}` ã®SVDã®å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ã¨ä¸€è‡´ã™ã‚‹:

```math
\tilde{X} = U\Sigma V^\top \Rightarrow \text{PC}_{k} = V_{:,1:k}
```

å…±åˆ†æ•£è¡Œåˆ— `$C = \frac{1}{n-1}\tilde{X}^\top \tilde{X} = \frac{1}{n-1}V\Sigma^2 V^\top$` ã®å›ºæœ‰å€¤ `$\frac{\sigma_i^2}{n-1}$` ãŒå„ä¸»æˆåˆ†ã®åˆ†æ•£ã€‚

**å®Ÿè£…ã®è½ã¨ã—ç©´**: `np.linalg.svd` vs `np.linalg.eig(X.T @ X)` â€” ã©ã¡ã‚‰ã§ã‚‚PCAã¯ã§ãã‚‹ãŒã€å‰è€…ã®æ–¹ãŒæ•°å€¤å®‰å®šæ€§ãŒé«˜ã„ï¼ˆå¾Œè€…ã¯æ¡ä»¶æ•°ãŒäºŒä¹—ã•ã‚Œã‚‹ï¼‰ã€‚

```math
A = U\Sigma V^\top,\quad \Sigma=\mathrm{diag}(\sigma_1,\dots,\sigma_r),\ r=\min(m,n)

A_k = U_{[:,1:k]}\,\Sigma_{1:k,1:k}\,V^\top_{[1:k,:]}

\|A-A_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2
```
```python
import numpy as np


def svd_rank_k(A: np.ndarray, k: int) -> np.ndarray:
    # A: (m,n)
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    # U: (m,r), s: (r,), Vt: (r,n)
    return U[:, :k] @ (s[:k, None] * Vt[:k, :])


def rel_fro_error(A: np.ndarray, B: np.ndarray) -> float:
    return float(np.linalg.norm(A - B, ord='fro') / np.linalg.norm(A, ord='fro'))


def tail_energy_bound(s: np.ndarray, k: int) -> float:
    num = float(np.sum(s[k:] ** 2))
    den = float(np.sum(s ** 2)) + 1e-12
    return float(np.sqrt(num / den))


rng = np.random.default_rng(0)
A = rng.normal(size=(128, 96))
U, s, Vt = np.linalg.svd(A, full_matrices=False)

prev = 1.0
for k in [1, 5, 10, 20, 40, 80]:
    Ak = svd_rank_k(A, k)
    err = rel_fro_error(A, Ak)
    bound = tail_energy_bound(s, k)
    assert err <= prev + 1e-10
    assert abs(err - bound) < 1e-6
    prev = err
    print(f'k={k:3d}  rel_fro_err={err:.6f}')
```

**ã‚³ãƒ¼ãƒ‰ã®æ¤œç®—å‡ºåŠ›ä¾‹**:
```
k=  1  rel_fro_err=0.964799
k=  5  rel_fro_err=0.913498
k= 10  rel_fro_err=0.859134
k= 20  rel_fro_err=0.745211
k= 40  rel_fro_err=0.551398
k= 80  rel_fro_err=0.249764
```

`assert abs(err - bound) < 1e-6` ãŒå…¨ã¦ã® `k` ã§é€šã‚‹ã€‚Eckart-Youngå®šç†ã¯**æ•°å€¤çš„ã«å³å¯†ã«æˆç«‹ã™ã‚‹**ã€‚

#### 5.1.2 Randomized SVD â€” ãªãœãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã§éƒ¨åˆ†ç©ºé–“ãŒå–ã‚Œã‚‹ã®ã‹

Halko, Martinsson, Tropp[^1]ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ•°å­¦çš„ç›´æ„Ÿã‹ã‚‰å…¥ã‚‹ã€‚

`$A$` ã®ãƒ©ãƒ³ã‚¯-`$k$` éƒ¨åˆ†ç©ºé–“ã‚’æ±‚ã‚ãŸã„ã€‚ç›´æ¥ `$\text{range}(A)$` ã‚’è¨ˆç®—ã™ã‚‹ã®ã¯ `$O(mn^2)$` ã ã€‚ä»£ã‚ã‚Šã«:

**éµã¨ãªã‚‹è¦³å¯Ÿ**: ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ« `$\boldsymbol{\omega} \in \mathbb{R}^n$` ã‚’ `$A$` ã«ä½œç”¨ã•ã›ã‚‹ã¨ã€`$A\boldsymbol{\omega}$` ã¯ `$\text{range}(A)$` ã®ä¸­ã«è½ã¡ã‚‹ã€‚

```math
A\boldsymbol{\omega} = U\Sigma V^\top \boldsymbol{\omega} = \sum_{i=1}^r \sigma_i (\mathbf{v}_i^\top \boldsymbol{\omega}) \mathbf{u}_i
```

ä¿‚æ•° `$c_i = \mathbf{v}_i^\top \boldsymbol{\omega}$` ã¯æ¨™æº–æ­£è¦ã‹ã‚‰å–ã£ãŸãƒ©ãƒ³ãƒ€ãƒ ã‚¹ã‚«ãƒ©ãƒ¼ã€‚å¤§ãã„ `$\sigma_i$` ã«å¯¾å¿œã™ã‚‹ `$c_i$` ã®ç›¸å¯¾çš„å¯„ä¸ãŒå¤§ãã„ãŸã‚ã€`$A\boldsymbol{\omega}$` ã¯ä¸Šä½ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ã§å¼µã‚‰ã‚Œã‚‹éƒ¨åˆ†ç©ºé–“ã«ã€Œè‡ªç„¶ã«é›†ã¾ã‚‹ã€ã€‚

`$l = k + p$` æœ¬ï¼ˆã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° `$p \approx 5{-}10$`ï¼‰ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ã£ãŸè¡Œåˆ— `$\Omega \in \mathbb{R}^{n \times l}$`:

```math
Y = A\Omega \in \mathbb{R}^{m \times l}
```

`$Y$` ã®åˆ—ç©ºé–“ã¯ `$\text{range}(A)$` ã®ä¸Šä½éƒ¨åˆ†ã‚’æ‰ãˆã¦ã„ã‚‹ã€‚`$Y = QR$`ï¼ˆQRåˆ†è§£ï¼‰ã§ `$Q \in \mathbb{R}^{m \times l}$` ã‚’å¾—ãŸå¾Œ:

```math
B = Q^\top A \in \mathbb{R}^{l \times n} \quad (l \ll m)
```

`$B$` ã®SVD `$B = \tilde{U}\Sigma V^\top$` ã‚’è¨ˆç®—ã—ã€`$U = Q\tilde{U}$`ã€‚å…¨ä½“ã®è¨ˆç®—é‡ã¯ `$O(mn \cdot l)$` â€” é€šå¸¸SVDã‚ˆã‚Š `$\min(m,n)/l$ å€é«˜é€Ÿã€‚

**èª¤å·®ä¿è¨¼**[^1]ï¼ˆæœŸå¾…å€¤å¢ƒç•Œï¼‰:

```math
\mathbb{E}\left[\|A - QQ^\top A\|_F\right] \leq \left(1 + \frac{k}{p-1}\right)^{1/2} \sigma_{k+1}
```

`$p=10$` ã§ã¯ä½™å‰°å› å­ãŒ `$\approx 1.06$` ã¨å°ã•ãã€æœ€é©è¿‘ä¼¼ï¼ˆ`$\sigma_{k+1}$`ï¼‰ã¨ã»ã¼åŒç­‰ã€‚

ãƒ‘ãƒ¯ãƒ¼ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ`$(AA^\top)^q \Omega$` ã‚’ä½¿ã†æ‹¡å¼µï¼‰ã§ã•ã‚‰ã«ç²¾åº¦ãŒä¸ŠãŒã‚‹ã€‚ç‰¹ç•°å€¤ã‚¹ãƒšã‚¯ãƒˆãƒ«ãŒç·©ã‚„ã‹ã«æ¸›è¡°ã™ã‚‹è¡Œåˆ—ï¼ˆä½ãƒ©ãƒ³ã‚¯æ§‹é€ ãŒå¼±ã„ï¼‰ã«æœ‰åŠ¹:

```math
Y = (AA^\top)^q A \Omega
```

`$q=1$` ã§æœ€å¤§ç‰¹ç•°å€¤ã¨ `$k+1$` ç•ªç›®ã®æ¯”ãŒ `$(\sigma_1 / \sigma_{k+1})^{2q+1}$` å€ã«å¼·èª¿ã•ã‚Œã€ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã®ã€Œæ´©ã‚Œã€ãŒæ¸›ã‚‹ã€‚

#### 5.1.3 LoRA ã®åˆæœŸåŒ–æˆ¦ç•¥ã¨æ•°å­¦çš„æ ¹æ‹ 

LoRA[^2]ã®æ ¸ã¯ `$\Delta W = BA$`ï¼ˆ`$B \in \mathbb{R}^{d \times r}$`, `$A \in \mathbb{R}^{r \times k}$`ï¼‰ã ãŒã€åˆæœŸåŒ–ãŒé‡è¦ã ã€‚

**è¨“ç·´é–‹å§‹æ™‚ã®æ¡ä»¶**: `$\Delta W = 0$`ï¼ˆPre-trainedãƒ¢ãƒ‡ãƒ«ã¨åŒã˜å‡ºåŠ›ã‹ã‚‰é–‹å§‹ï¼‰

ã“ã‚Œã‚’å®Ÿç¾ã™ã‚‹åˆæœŸåŒ–:
- `$A$`ï¼šKaiming normalï¼ˆ`$\mathcal{N}(0, 2/r)$`ï¼‰
- `$B$`ï¼šã‚¼ãƒ­åˆæœŸåŒ–

`$B = 0 \Rightarrow \Delta W = B A = 0$` âœ“

ãªãœ `$A$` ã‚’ã‚¼ãƒ­ã«ã—ã¦ `$B$` ã‚’ Kaiming normal ã«ã—ãªã„ã®ã‹ï¼Ÿã€€ç­”ãˆã¯å‹¾é…ã®æµã‚Œã«ã‚ã‚‹ã€‚

Forward pass: `$y = Wx + \Delta W x = Wx + B(Ax)$`

```math
\frac{\partial \mathcal{L}}{\partial A} = B^\top \frac{\partial \mathcal{L}}{\partial (BAx)} \cdot x^\top
```

è¨“ç·´é–‹å§‹æ™‚ã« `$B=0$` ãªã‚‰ `$\partial \mathcal{L}/\partial A = 0$` ã¨ãªã‚Šã€`$A$` ã¸ã®å‹¾é…ãŒã‚¼ãƒ­ã€‚ã“ã‚Œã¯å›°ã‚‹ã€‚é€†ã« `$A=0$` ãªã‚‰ `$\partial \mathcal{L}/\partial B = 0$` ã¨ãªã‚Šã€`$B$` ãŒå­¦ç¿’ã—ãªã„ã€‚

ã©ã¡ã‚‰ã‚’ã‚¼ãƒ­ã«ã—ã¦ã‚‚ç‰‡æ–¹ãŒå­¦ç¿’ã—ãªã„å•é¡ŒãŒç”Ÿã˜ã‚‹â€”â€”ã“ã‚Œã¯å¯¾ç§°æ€§ã®å•é¡Œã§ã¯ãªãã€ä¹—ç®—ã®**å‹¾é…ã®æµã‚Œ**ã®å•é¡Œã ã€‚

å®Ÿéš›ã«ã¯ `$B=0$`ã€`$A=\text{Kaiming normal}$` ã¨ã™ã‚‹ã€‚æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã§ `$B$` ãŒéã‚¼ãƒ­ã«ãªã‚Œã°ï¼ˆå‹¾é…ã¯ `$A`ã‚’é€šã˜ã¦æ¥ã‚‹ã®ã§éã‚¼ãƒ­ï¼‰ã€ä»¥é™ã¯ä¸¡æ–¹ãŒæ›´æ–°ã•ã‚Œã‚‹ã€‚`$B$` ã‚’ã‚¼ãƒ­ã«ã™ã‚Œã°ã€Œå‡ºåŠ›ã¸ã®å½±éŸ¿ã‚¼ãƒ­ã€ãŒç¢ºä¿ã•ã‚Œã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•° `$\alpha/r$` ã¨çµ„ã¿åˆã‚ã›ã¦å­¦ç¿’ç‡ã®åŠ¹æœãŒå®‰å®šã™ã‚‹ã€‚

ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: å‡ºåŠ›ã¯ `$\frac{\alpha}{r} BA x$`ï¼ˆ`$\alpha$` ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã€‚`$r$` ã‚’å¤‰ãˆã¦ã‚‚æœ‰åŠ¹å­¦ç¿’ç‡ãŒä¸€å®šã«ãªã‚‹è¨­è¨ˆã€‚

#### 5.1.4 Tikhonovæ­£å‰‡åŒ–ã®SVDè§£æè§£

å•é¡Œ:

```math
\mathbf{x}^* = \arg\min_{\mathbf{x}} \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2
```

è§£ã®é–‰å½¢å¼ã¯æ­£è¦æ–¹ç¨‹å¼ `$(A^\top A + \lambda I)\mathbf{x}^* = A^\top \mathbf{b}$` ã‹ã‚‰æ¥ã‚‹ã€‚SVD `$A = U\Sigma V^\top$` ã‚’ä»£å…¥ã™ã‚‹ã¨:

```math
(V\Sigma^\top U^\top U \Sigma V^\top + \lambda VV^\top)\mathbf{x}^* = V\Sigma^\top U^\top \mathbf{b}
```

```math
V(\Sigma^\top\Sigma + \lambda I)V^\top \mathbf{x}^* = V\Sigma^\top U^\top \mathbf{b}
```

```math
\mathbf{x}^* = V(\Sigma^\top\Sigma + \lambda I)^{-1} \Sigma^\top U^\top \mathbf{b} = \sum_{i=1}^r \frac{\sigma_i}{\sigma_i^2 + \lambda} (\mathbf{u}_i^\top \mathbf{b})\, \mathbf{v}_i
```

ãƒ•ã‚£ãƒ«ã‚¿ä¿‚æ•° `$f_i(\lambda) = \frac{\sigma_i}{\sigma_i^2 + \lambda}$` ã®æŒ™å‹•:
- `$\sigma_i \gg \sqrt{\lambda}$`: `$f_i \approx 1/\sigma_i$`ï¼ˆé€šå¸¸ã®ç–‘ä¼¼é€†è¡Œåˆ—çš„è§£ï¼‰
- `$\sigma_i \ll \sqrt{\lambda}$`: `$f_i \approx \sigma_i/\lambda \to 0$`ï¼ˆå°ã•ã„ç‰¹ç•°å€¤ã®æˆåˆ†ã‚’æŠ‘åˆ¶ï¼‰

Truncated SVDï¼ˆå°ã•ã„ç‰¹ç•°å€¤ã‚’å®Œå…¨ã«ã‚«ãƒƒãƒˆï¼‰ã¨Tikhonovæ­£å‰‡åŒ–ï¼ˆæ»‘ã‚‰ã‹ã«ã‚«ãƒƒãƒˆï¼‰ã®é•ã„ã¯ã€ã“ã®ãƒ•ã‚£ãƒ«ã‚¿ä¿‚æ•°ã®ã€Œå´–vsæ›²ç·šã€ã«ç¾ã‚Œã‚‹ã€‚Tikhonovã®æ–¹ãŒæ»‘ã‚‰ã‹ã§æ•°å€¤å®‰å®šæ€§ã«å„ªã‚Œã‚‹ã€‚

```mermaid
graph LR
    A["A = UÎ£Váµ€"] --> B["ãƒ•ã‚£ãƒ«ã‚¿ä¿‚æ•°<br/>Ïƒáµ¢/(Ïƒáµ¢Â²+Î»)"]
    B --> C["å¤§ãã„Ïƒáµ¢<br/>1/Ïƒáµ¢ã«è¿‘ä¼¼"]
    B --> D["å°ã•ã„Ïƒáµ¢<br/>Ïƒáµ¢/Î» â†’ 0"]
    C --> E["é€šå¸¸ã®æœ€å°äºŒä¹—"]
    D --> F["ãƒã‚¤ã‚ºæˆåˆ†ã‚’æŠ‘åˆ¶"]
```

#### 5.1.5 ç”»åƒåœ§ç¸®ã¨ãƒã‚¤ã‚ºé™¤å»ã®åŸç†

**SVDåœ§ç¸®æ¯”**:

`$m \times n$` ç”»åƒã‚’rank-`$k$`ã§åœ§ç¸®ã™ã‚‹ã¨:

```math
\text{åœ§ç¸®ç‡} = \frac{k(m + n + 1)}{mn}
```

`$m=n=512$`, `$k=50$` ãªã‚‰ `$50 \times 1025 / 262144 \approx 19.5\%`ã€‚

**SVDãƒã‚¤ã‚ºé™¤å»**: ãƒã‚¤ã‚º `$N$`ï¼ˆè¦ç´ ãŒç‹¬ç«‹ `$\mathcal{N}(0, \sigma^2)$`ï¼‰ãŒåŠ ã‚ã£ãŸè¡Œåˆ— `$\tilde{A} = A + N$`ã€‚

Marchenko-Pasturå‰‡[^3]ã«ã‚ˆã‚Œã°ã€ç´”ç²‹ãƒã‚¤ã‚ºè¡Œåˆ—ã®ç‰¹ç•°å€¤åˆ†å¸ƒã¯åŒºé–“ `$[\sigma(\sqrt{m} - \sqrt{n}), \sigma(\sqrt{m} + \sqrt{n})]$` ã«é›†ä¸­ã™ã‚‹ï¼ˆ`$m \geq n$`ï¼‰ã€‚ä¸Šé™ `$\sigma_{\text{thresh}} = \sigma(\sqrt{m} + \sqrt{n})$` ã‚ˆã‚Šå¤§ãã„ç‰¹ç•°å€¤ã®ã¿ä¿æŒã™ã‚‹ã“ã¨ãŒãƒã‚¤ã‚ºé™¤å»ã®æ•°å­¦çš„æ ¹æ‹ ã ã€‚

å®Ÿç”¨çš„ãªã—ãã„å€¤ï¼ˆUniversal Singular Value Thresholdingï¼‰:

```math
\lambda^* = \frac{4}{\sqrt{3}} \sigma \sqrt{n} \quad (m \gg n)
```

---

### 5.2 einsumå®Œå…¨å®Ÿè£…

#### 5.2.1 æ·»å­—ãƒ«ãƒ¼ãƒ«ã®å®Œå…¨è¨˜è¿°

einsum ã®è¦å‰‡ã¯3ã¤ã ã‘:

1. åŒã˜æ·»å­—ãŒ2å›ç¾ã‚ŒãŸã‚‰ç¸®ç´„ï¼ˆå’Œã‚’å–ã‚‹ï¼‰
2. `->` ã®å³è¾ºã«æ›¸ã„ãŸæ·»å­—ãŒå‡ºåŠ›ã«æ®‹ã‚‹
3. `->` ã‚’çœç•¥ã™ã‚‹ã¨ã€1å›ã—ã‹ç¾ã‚Œãªã„æ·»å­—ãŒå…¨ã¦å‡ºåŠ›ã«ãªã‚‹

ã“ã‚Œã§å…¨ã¦ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå°å‡ºã§ãã‚‹ã€‚

**å®Œå…¨ãƒ‘ã‚¿ãƒ¼ãƒ³è¡¨**:

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | æ•°å¼ | einsumæ–‡å­—åˆ— | å‡ºåŠ›shape |
|:---------|:-----|:-------------|:---------|
| å†…ç© | `$\mathbf{a}^\top \mathbf{b}$` | `'i,i->'` | ã‚¹ã‚«ãƒ©ãƒ¼ |
| å¤–ç© | `$\mathbf{a}\mathbf{b}^\top$` | `'i,j->ij'` | `(n,m)` |
| è¡Œåˆ—-ãƒ™ã‚¯ãƒˆãƒ« | `$A\mathbf{x}$` | `'ij,j->i'` | `(m,)` |
| è¡Œåˆ—ç© | `$AB$` | `'ik,kj->ij'` | `(m,n)` |
| ãƒˆãƒ¬ãƒ¼ã‚¹ | `$\text{tr}(A)$` | `'ii->'` | ã‚¹ã‚«ãƒ©ãƒ¼ |
| Hadamard | `$A \odot B$` | `'ij,ij->ij'` | `(m,n)` |
| ç¸®å°å’Œ | `$\sum_j A_{ij}$` | `'ij->i'` | `(m,)` |
| ãƒãƒƒãƒè¡Œåˆ—ç© | `$C_{bij}$` | `'bik,bkj->bij'` | `(B,m,n)` |
| ãƒ†ãƒ³ã‚½ãƒ«ç¸®ç´„ | `$C_{ijl} = A_{ikm}B_{mjl}$` | `'ikm,mjl->ijl'` | `(I,J,L)` |

#### 5.2.2 Multi-Head Attention ã® einsumå±•é–‹

Attentionæ©Ÿæ§‹[^4]ã®4æ®µéšã‚’einsumã§æ›¸ãã¨ã€æ·»å­—ã®æ„å‘³ãŒè‡ªç„¶ã«æ˜ç¤ºã•ã‚Œã‚‹:

**Step 1: ã‚¹ã‚³ã‚¢è¨ˆç®—**

```math
S_{bhqk} = \frac{1}{\sqrt{d_h}} \sum_d Q_{bhqd} K_{bhkd}
```

einsum: `'bhqd,bhkd->bhqk'`ã€‚ç¸®ç´„æ·»å­— `$d$`ï¼ˆãƒ˜ãƒƒãƒ‰å†…æ¬¡å…ƒï¼‰ãŒæ¶ˆãˆã‚‹ã€‚

**Step 2: Softmax**ï¼ˆè¡Œåˆ—æ¼”ç®—ã§ã¯ãªã„ãŒæ·»å­—è¨˜æ³•ã§æ›¸ã‘ã‚‹ï¼‰

```math
P_{bhqk} = \frac{\exp(S_{bhqk})}{\sum_{k'}\exp(S_{bhqk'})}
```

`$k$` è»¸ã§softmaxã€‚`$q$` ã”ã¨ã€`$b, h$` ã”ã¨ã«ç‹¬ç«‹ã€‚

**Step 3: åŠ é‡å¹³å‡**

```math
Y_{bhqv} = \sum_k P_{bhqk} V_{bhkv}
```

einsum: `'bhqk,bhkv->bhqv'`ã€‚ç¸®ç´„æ·»å­— `$k$`ï¼ˆã‚­ãƒ¼ä½ç½®ï¼‰ãŒæ¶ˆãˆã‚‹ã€‚

**Step 4: ãƒ˜ãƒƒãƒ‰çµ±åˆ**ï¼ˆ`$H \cdot d_h = d$`ï¼‰

```math
O_{bqd} = \sum_h \sum_{d_h} Y_{bh,q,d_h}\, W^O_{h \cdot d_h,\, d}
```

ã“ã‚Œã¯ `Y.reshape(B, T, H*dh) @ W_O` ã¨ç­‰ä¾¡ã€‚æ·»å­—ã§ã„ã†ã¨ `'bqhv,hvd->bqd'`ï¼ˆ`$h$` ã¨ `$v$` ã®2ã¤ãŒç¸®ç´„ï¼‰ã€‚

#### 5.2.3 è¨ˆç®—é‡ã¨æ·»å­—ã®æœ€é©åŒ–

einsum ã®è¨ˆç®—é‡ã¯ã€Œç¸®ç´„å¾Œã®æ·»å­—æ¬¡å…ƒã®ç©ã€ã«æ¯”ä¾‹ã™ã‚‹ã€‚

ä¾‹: `$A_{ijk} B_{jkl} C_{lmn}$` ã®3é …ç¸®ç´„

- **é †åº1**: `$(AB)C$`
  - `$AB$`: ç¸®ç´„ `$jk$`ã€è¨ˆç®—é‡ `$O(I J K L)$`
  - `$(AB)C$`: ç¸®ç´„ `$l$`ã€è¨ˆç®—é‡ `$O(I L M N)$`
  - åˆè¨ˆ: `$O(IJKL + ILMN)$`

- **é †åº2**: `$A(BC)$`
  - `$BC$`: ç¸®ç´„ `$l$`ã€è¨ˆç®—é‡ `$O(J K L M N)$`
  - `$A(BC)$`: ç¸®ç´„ `$jk$`ã€è¨ˆç®—é‡ `$O(I J K M N)$`
  - åˆè¨ˆ: `$O(JKLMN + IJKMN)$`

`$J, K \gg L$` ãªã‚‰é †åº1ãŒæœ‰åˆ©ã€‚`opt_einsum` ã¯å‹•çš„è¨ˆç”»æ³•ã§ã“ã®æœ€é©çµŒè·¯ã‚’ `$O((\text{é …æ•°})^3)$` ã§ç™ºè¦‹ã™ã‚‹ã€‚

**ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±€æ‰€æ€§**: einsumå†…éƒ¨ã§ã¯ç¸®ç´„æ¬¡å…ƒã‚’ innermost ã«ã™ã‚‹è»¢ç½®ãŒè‡ªå‹•ã§è¡Œã‚ã‚Œã‚‹ã€‚è¡Œåˆ—ç©ã¯ innermostæ¬¡å…ƒãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¹—ã‚Šã‚„ã™ãã€BLASå‘¼ã³å‡ºã—ã®æ©æµã‚’æœ€å¤§åŒ–ã§ãã‚‹ã€‚

#### 5.2.4 ç†è«–ã¨æ¤œç®—

einsum ã®æ­£ã—ã•ã¯**shape assertion**ã§å®ˆã‚‹:

```math
\text{einsum}(\texttt{'bhqd,bhkd->bhqk'}, Q, K).\text{shape} = (B, H, T, T)
```

å®Ÿè£…ã§ã¯ã“ã‚Œã‚’ `assert` ã«è½ã¨ã™ã€‚ç¸®ç´„ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé–“é•ã£ã¦ã„ã‚Œã° shape ãŒå¤‰ã‚ã‚‹ã®ã§ã€ã“ã‚ŒãŒæœ€é€Ÿã®é–“é•ã„æ¤œçŸ¥ã«ãªã‚‹ã€‚

#### 5.2.5 å®Ÿè£…ã®è½ã¨ã—ç©´ã¾ã¨ã‚

| è½ã¨ã—ç©´ | ç—‡çŠ¶ | å¯¾ç­– |
|:---------|:-----|:-----|
| ç¸®ç´„æ·»å­—ã®ã‚µã‚¤ã‚ºä¸ä¸€è‡´ | shape error ã¾ãŸã¯ä¸æ­£ãªçµæœ | å„å…¥åŠ›ã®å¯¾å¿œæ¬¡å…ƒãŒç­‰ã—ã„ã“ã¨ã‚’ assert |
| æš—é»™ã®è»¢ç½® | `'ij,ji->ij'` vs `'ij,ij->ij'` ã®æ··åŒ | æ·»å­—ã‚’æ˜ç¤ºçš„ã«æ›¸ã„ã¦ Wolfram Alphaç­‰ã§ç¢ºèª |
| `->` ãªã—çœç•¥ | æœŸå¾…å¤–ã®ç¸®ç´„ãŒèµ·ãã‚‹ | å¸¸ã« `->` ã‚’æ›¸ã |
| `optimize=True` ã®å‰¯ä½œç”¨ | éæ±ºå®šçš„ãªæµ®å‹•å°æ•°ç‚¹é †åº | æ•°å€¤ãƒ†ã‚¹ãƒˆã¯ `optimize=False` ã§è¡Œã† |

---

### 5.3 è¡Œåˆ—å¾®åˆ†å®Ÿè£…

#### 5.3.1 æ•°å€¤å¾®åˆ†ã®ç²¾åº¦ç†è«–

ä¸­å¤®å·®åˆ†ã®èª¤å·®ã‚’å®šé‡åŒ–ã™ã‚‹ã€‚`$f$` ã‚’ `$x_i$` ã§åå¾®åˆ†ã™ã‚‹ä¸­å¤®å·®åˆ†:

```math
\frac{f(\mathbf{x} + h\mathbf{e}_i) - f(\mathbf{x} - h\mathbf{e}_i)}{2h} = \frac{\partial f}{\partial x_i} + \frac{h^2}{6}\frac{\partial^3 f}{\partial x_i^3} + O(h^4)
```

æ‰“ã¡åˆ‡ã‚Šèª¤å·®ã¯ `$O(h^2)$`ï¼ˆå‰é€²å·®åˆ†ã® `$O(h)$` ã‚ˆã‚Šå„ªã‚Œã‚‹ï¼‰ã€‚ä¸€æ–¹ã€æµ®å‹•å°æ•°ç‚¹ä¸¸ã‚èª¤å·®ã¯å„ `$f$` è©•ä¾¡ã« `$\epsilon_{\text{mach}} |f|$` ã®èª¤å·®ãŒã‚ã‚‹ãŸã‚ã€å·®åˆ†ã§ã¯:

```math
\varepsilon_{\text{round}} \approx \frac{2\epsilon_{\text{mach}} |f|}{2h} = \frac{\epsilon_{\text{mach}} |f|}{h}
```

ç·èª¤å·®:

```math
\varepsilon_{\text{total}} \approx \frac{h^2}{6}\left|\frac{\partial^3 f}{\partial x_i^3}\right| + \frac{\epsilon_{\text{mach}} |f|}{h}
```

`$h$` ã«ã¤ã„ã¦ã®æœ€å°åŒ–: `$h^* \approx \left(\frac{3\epsilon_{\text{mach}} |f|}{|\partial^3 f / \partial x_i^3|}\right)^{1/3}$`

`$f \sim O(1)$`ã€ä¸‰éšå¾®åˆ† `$\sim O(1)$` ã®å ´åˆ: `$h^* \approx (3 \times 2.2 \times 10^{-16})^{1/3} \approx 10^{-5}$`

#### 5.3.3 è¡Œåˆ—å¾®åˆ†ã®ä¸»è¦å…¬å¼ï¼ˆæ•°å€¤æ¤œè¨¼ä»˜ãï¼‰

åŸºæœ¬å…¬å¼ã‚’æ•°å€¤çš„ã«ç¢ºèªã§ãã‚‹å½¢ã§æ•´ç†ã™ã‚‹ã€‚

**ç·šå½¢å¤‰æ›ã®å‹¾é…**:
```math
\frac{\partial}{\partial W}(W\mathbf{x}) = \mathbf{x}^\top \otimes I \quad (\text{4éšãƒ†ãƒ³ã‚½ãƒ«å½¢å¼})
```
ã‚¹ã‚«ãƒ©ãƒ¼åˆæˆ `$f(W\mathbf{x})$` ã§ã¯:
```math
\frac{\partial \mathcal{L}}{\partial W} = \frac{\partial \mathcal{L}}{\partial (W\mathbf{x})} \mathbf{x}^\top \in \mathbb{R}^{m \times n}
```

**Frobenius ãƒãƒ«ãƒ ã®å‹¾é…**:
```math
\frac{\partial}{\partial A}\|A\|_F^2 = 2A
```

**è¡Œåˆ—å¼ã®å‹¾é…**ï¼ˆæ­£å®šå€¤ `$A$`ï¼‰:
```math
\frac{\partial}{\partial A}\log\det(A) = (A^{-1})^\top = A^{-1} \quad (\text{å¯¾ç§°ãªã‚‰})
```

**trace ã®å‹¾é…**:
```math
\frac{\partial}{\partial A}\text{tr}(BA) = B^\top
```

ã“ã‚Œã‚‰ã¯å…¨ã¦ä¸­å¤®å·®åˆ†ã§ `$< 10^{-6}$` ã®ç›¸å¯¾èª¤å·®ã§æ¤œè¨¼ã§ãã‚‹ã€‚ç–‘ã‚ã—ã„ã¨ãã¯2Ã—2ã®å°è¡Œåˆ—ã§æ‰‹è¨ˆç®—ã—ã¦ã‹ã‚‰ã€ä¸€èˆ¬ã‚µã‚¤ã‚ºã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã€‚



äºŒæ¬¡å½¢å¼ `$f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top A \mathbf{x}$` ã®è§£æå‹¾é…:

```math
\nabla_\mathbf{x} f = \frac{1}{2}(A + A^\top)\mathbf{x}
```

`$A$` ãŒå¯¾ç§°ãªã‚‰ `$\nabla_\mathbf{x} f = A\mathbf{x}$`ã€‚éå¯¾ç§°ã®å ´åˆã‚‚ä¸Šå¼ãŒæ­£ç¢ºã€‚

**è¨˜å·â†”å¤‰æ•°åã®å¯¾å¿œ**:
- `$\mathbf{x} \in \mathbb{R}^d$` â†” `x: np.ndarray (d,)`
- `$A \in \mathbb{R}^{d \times d}$` â†” `A: np.ndarray (d, d)`
- `$f \in \mathbb{R}$` â†” `float`
- `$\nabla_\mathbf{x} f \in \mathbb{R}^d$` â†” `g: np.ndarray (d,)`

**è½ã¨ã—ç©´**: éå¯¾ç§° `$A$` ã§ `$A\mathbf{x}$` ã ã‘ã‚’è¿”ã™å®Ÿè£…ã¯ãƒã‚°ã€‚`$\frac{1}{2}(A+A^\top)\mathbf{x}$` ãŒæ­£ã—ã„ã€‚æ¤œç®—ã™ã‚‹ã¾ã§ã¯è¦‹ã¤ã‹ã‚‰ãªã„ã€‚

ã¾ãŸã€Attention einsum `$S_{nm} = \sum_d Q_{nd}K_{md}/\sqrt{d_k}$` ã§ã¯ softmax ã®æ•°å€¤å®‰å®šåŒ–ï¼ˆmax-shiftï¼‰ã‚’å¿˜ã‚Œã‚‹ã¨ã€å¤§ããª `$d_k$` ã§ã‚¹ã‚³ã‚¢ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã™ã‚‹ã€‚å®‰å®šç‰ˆã¯ä»¥ä¸‹:

```math
S' = S - \max_m S, \quad P = \frac{\exp(S')}{\sum_m \exp(S')}
```



```math
f(x) = \frac{1}{2}x^\top A x,\qquad
\nabla_x f(x) = \frac{1}{2}(A + A^\top) x

S = \frac{1}{\sqrt{d_k}}QK^\top,\quad P=\mathrm{softmax}(S),\quad Y=PV
```
```python
import numpy as np


def f_quadratic(x: np.ndarray, A: np.ndarray) -> float:
    return float(0.5 * x.T @ A @ x)


def grad_x_analytic(x: np.ndarray, A: np.ndarray) -> np.ndarray:
    return 0.5 * (A + A.T) @ x


def grad_x_numeric(x: np.ndarray, A: np.ndarray, eps: float = 1e-6) -> np.ndarray:
    g = np.zeros_like(x)
    for i in range(x.shape[0]):
        xp = x.copy(); xm = x.copy()
        xp[i] += eps; xm[i] -= eps
        g[i] = (f_quadratic(xp, A) - f_quadratic(xm, A)) / (2.0 * eps)
    return g


rng = np.random.default_rng(1)
d = 8
x = rng.normal(size=(d,))
A = rng.normal(size=(d, d))

g_a = grad_x_analytic(x, A)
g_n = grad_x_numeric(x, A)
rel = np.linalg.norm(g_a - g_n) / (np.linalg.norm(g_a) + 1e-12)
print('grad check (relative error)=', float(rel))
assert rel < 1e-6


# einsum: contract indices explicitly (shape contract)
N, d_k, d_v = 4, 6, 5
Q = rng.normal(size=(N, d_k))
K = rng.normal(size=(N, d_k))
V = rng.normal(size=(N, d_v))

S = np.einsum('nd,md->nm', Q, K) / np.sqrt(float(d_k))
S = S - S.max(axis=1, keepdims=True)
P = np.exp(S); P = P / P.sum(axis=1, keepdims=True)
Y = np.einsum('nm,mv->nv', P, V)

assert S.shape == (N, N) and P.shape == (N, N) and Y.shape == (N, d_v)
print('attention shapes:', S.shape, P.shape, Y.shape)
```

**æ¤œç®—å‡ºåŠ›ä¾‹**:
```
grad check (relative error)= 3.2e-10
attention shapes: (4, 4) (4, 4) (4, 5)
```

`rel < 1e-6` ã® assert ãŒé€šã‚‹ã€‚è§£æå‹¾é…ã®ç²¾åº¦ã¯æ•°å€¤å¾®åˆ†ã‚ˆã‚Š `$10^6$` å€æ­£ç¢ºã ï¼ˆæ•°å€¤å¾®åˆ†ã¯ `$h=10^{-6}$` ãªã®ã§ç›¸å¯¾ç²¾åº¦ `$\sim 10^{-6}$` ãŒä¸Šé™ï¼‰ã€‚

```mermaid
flowchart LR
  x[x] --> Ax[A x]
  A[A] --> Ax
  Ax --> xtAx["x^T(Ax)"]
  x --> xtAx
  xtAx --> f["f = 1/2 x^T A x"]
  Q[Q: NÃ—d_k] --> S["S: NÃ—N"]
  K[K: NÃ—d_k] --> S
  S --> P["P: NÃ—N"]
  P --> Y["Y: NÃ—d_v"]
  V[V: NÃ—d_v] --> Y
```

---

### 5.4 è‡ªå‹•å¾®åˆ†å®Ÿè£… â€” Dual Numbersã§å‰ã‹ã‚‰å¾®åˆ†ã‚’æµã™

Reverse Modeï¼ˆbackpropï¼‰ã¯ã€Œå¾Œã‚ã‹ã‚‰æµã™ã€ã ã£ãŸã€‚Forward Mode ã¯ã€Œå‰ã‹ã‚‰æµã™ã€â€”â€” å€¤ã¨å¾®åˆ†ã‚’**åŒæ™‚ã«**è¨ˆç®—ã™ã‚‹ã€‚

**åŒå¯¾æ•°ã®ä»£æ•°æ§‹é€ **:

åŒå¯¾æ•° `$\mathbb{D} = \{a + b\varepsilon \mid a, b \in \mathbb{R},\ \varepsilon^2 = 0\}$` ã¯ `$\mathbb{R}$` ã®æ‹¡å¼µç’°ã ã€‚

```math
a + b\varepsilon, \quad \varepsilon^2 = 0,\quad \varepsilon \neq 0
```

`$\varepsilon$` ã¯ã€Œç„¡é™å°ã®æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã€ã¨æ€ãˆã°ã‚ˆã„ã€‚æ¼”ç®—è¦å‰‡:

```math
(a + b\varepsilon) + (c + d\varepsilon) = (a+c) + (b+d)\varepsilon
```

```math
(a + b\varepsilon)(c + d\varepsilon) = ac + (ad + bc)\varepsilon + \underbrace{bd\varepsilon^2}_{=0} = ac + (ad+bc)\varepsilon
```

`$\varepsilon^2 = 0$` ã®ãŠã‹ã’ã§ã€2æ¬¡ã®é …ãŒæ¶ˆãˆã‚‹ã€‚ã“ã‚ŒãŒã¾ã•ã«ã€Œå¾®åˆ†ã®ç·šå½¢è¿‘ä¼¼ã€ã€‚

**åŒå¯¾æ•°ã®ä¸»ãªåˆç­‰é–¢æ•°**:

```math
\sin(a + b\varepsilon) = \sin a + b\cos a \cdot \varepsilon
```
```math
\exp(a + b\varepsilon) = e^a + be^a \varepsilon
```
```math
\log(a + b\varepsilon) = \log a + \frac{b}{a}\varepsilon \quad (a > 0)
```
```math
(a + b\varepsilon)^n = a^n + n a^{n-1} b \varepsilon
```

å„å¼ã® `$\varepsilon$` ä¿‚æ•°ãŒå¾®åˆ† `$f'(x)$` ã®å…¬å¼ãã®ã‚‚ã®ã«ãªã£ã¦ã„ã‚‹ã€‚ã“ã‚Œã¯Pythonã® dunder method ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ­ãƒ¼ãƒ‰ã™ã‚Œã°è‡ªå‹•çš„ã«å…¨ã¦ã®åˆæˆé–¢æ•°ã®å¾®åˆ†ãŒè¨ˆç®—ã§ãã‚‹ã€‚

**ãªãœã“ã‚ŒãŒå¾®åˆ†ã«ãªã‚‹ã®ã‹ï¼Ÿ** é–¢æ•° `$f$` ã« `$x + \varepsilon$` ã‚’å…¥ã‚Œã‚‹ã¨:

```math
f(x + \varepsilon) = f(x) + f'(x)\varepsilon \quad (\varepsilon^2=0 \text{ ãªã®ã§é«˜æ¬¡æ¶ˆæ»…})
```

`$\varepsilon$` ã®ä¿‚æ•°ãŒ `$f'(x)$` ã«ãªã‚‹ã€‚ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã®1æ¬¡é …ãŒãã®ã¾ã¾æŠ½å‡ºã•ã‚Œã‚‹ã€‚

**Forward Mode ã®è¨ˆç®—è¡¨ â€” `$f(x) = \sin(x^2 + x)$`, `$x=1$`:**

| ã‚¹ãƒ†ãƒƒãƒ— | å®Ÿéƒ¨ï¼ˆå€¤ï¼‰ | `$\varepsilon$`ä¿‚æ•°ï¼ˆå¾®åˆ†ï¼‰ |
|:---------|:---------|:--------------------------|
| `$v_0 = x$` | `$1$` | `$\dot{v}_0 = 1$`ï¼ˆ`$dx/dx=1$`ï¼‰ |
| `$v_1 = v_0^2$` | `$1$` | `$\dot{v}_1 = 2v_0\dot{v}_0 = 2$` |
| `$v_2 = v_1 + v_0$` | `$2$` | `$\dot{v}_2 = \dot{v}_1 + \dot{v}_0 = 3$` |
| `$v_3 = \sin(v_2)$` | `$\sin 2 \approx 0.909$` | `$\dot{v}_3 = \cos(v_2)\dot{v}_2 = 3\cos 2 \approx -1.248$` |

**Forward vs Reverse ã®ä½¿ã„åˆ†ã‘**:

```math
\text{å…¥åŠ›æ¬¡å…ƒ} = n,\quad \text{å‡ºåŠ›æ¬¡å…ƒ} = m
```

| ãƒ¢ãƒ¼ãƒ‰ | è¨ˆç®—ã‚³ã‚¹ãƒˆ | å‘ã„ã¦ã„ã‚‹å ´é¢ |
|:-------|:---------|:-------------|
| Forward | `$O(n)$` ãƒ‘ã‚¹ | `$n \ll m$` ï¼ˆç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ï¼‰ |
| Reverse | `$O(m)$` ãƒ‘ã‚¹ | `$m \ll n$` ï¼ˆDLã®æå¤±æœ€å°åŒ–: `$m=1$`ï¼‰ |

LLMè¨“ç·´ã¯ `$n=10^9$`, `$m=1$` â†’ Reverse ModeãŒ `$10^9$` å€æœ‰åˆ©ã€‚

**Forward ModeãŒè¼ãå ´é¢**: ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—ã®**åˆ—**ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨ãï¼ˆ`$m > n$`ï¼‰ã€‚ä¾‹ãˆã° Jacobian-vector product `$J\mathbf{v}$` ã¯ Forward Modeã§1ãƒ‘ã‚¹ã§æ¸ˆã‚€ã€‚

```mermaid
flowchart LR
  X[x + Îµv] --> F["f(x+Îµv)"]
  F --> JV["f(x) + JÂ·v Îµ"]
  JV --> EX["ä¿‚æ•° â†’ Jv"]
```

**Dual Numbersã®å®Ÿè£…**ã¯ã€Pythonã®dunder methodã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ã‚ˆã„ã€‚æ•°å­¦ã®ä»£æ•°æ§‹é€ ã‚’ç›´æ¥ã‚³ãƒ¼ãƒ‰ã«å†™ã™ä¾‹ã¨ã—ã¦å®Œçµã—ã¦ã„ã‚‹ã€‚

```math
\frac{d}{dx}\sin(x) = \cos(x),\quad \frac{d}{dx}(u \cdot v) = u'v + uv'
```

è¨˜å·â†”å¤‰æ•°åã®å¯¾å¿œ:
- `$a + b\varepsilon$` â†” `Dual(a, b)` ï¼ˆ`a` = å®Ÿéƒ¨, `b` = `$\varepsilon$` ä¿‚æ•° = å¾®åˆ†ï¼‰
- `$f(x)$` â†” `real` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
- `$f'(x)$` â†” `dual` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

```python
from __future__ import annotations
import math


class Dual:
    """Dual number: a + b*eps, eps^2 = 0."""

    def __init__(self, real: float, dual: float = 0.0) -> None:
        self.real = float(real)
        self.dual = float(dual)

    def __add__(self, other: Dual | float) -> Dual:
        o = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.real + o.real, self.dual + o.dual)

    def __radd__(self, other: float) -> Dual:
        return Dual(other + self.real, self.dual)

    def __mul__(self, other: Dual | float) -> Dual:
        o = other if isinstance(other, Dual) else Dual(other)
        # (a+bÎµ)(c+dÎµ) = ac + (ad+bc)Îµ
        return Dual(self.real * o.real, self.real * o.dual + self.dual * o.real)

    def __rmul__(self, other: float) -> Dual:
        return Dual(other * self.real, other * self.dual)

    def __pow__(self, n: int) -> Dual:
        # d/dx x^n = n x^{n-1}
        return Dual(self.real ** n, n * self.real ** (n - 1) * self.dual)

    def sin(self) -> Dual:
        return Dual(math.sin(self.real), math.cos(self.real) * self.dual)

    def cos(self) -> Dual:
        return Dual(math.cos(self.real), -math.sin(self.real) * self.dual)

    def __repr__(self) -> str:
        return f"Dual({self.real:.6f}, {self.dual:.6f})"


def diff(f, x: float) -> float:
    """Forward-mode AD: compute f'(x) via Dual numbers."""
    return f(Dual(x, 1.0)).dual


# --- check 1: f(x) = sin(x^2 + x)  at  x = 1.0 ---
def f1(x: Dual) -> Dual:
    return (x ** 2 + x).sin()

x0 = 1.0
val = f1(Dual(x0, 0.0)).real
deriv_ad = diff(f1, x0)
# analytical: f'(x) = cos(x^2+x) * (2x+1)
deriv_analytic = math.cos(x0**2 + x0) * (2*x0 + 1)
err = abs(deriv_ad - deriv_analytic)

print(f"f(1.0)      = {val:.6f}")
print(f"f'(1.0) AD  = {deriv_ad:.6f}")
print(f"f'(1.0) ana = {deriv_analytic:.6f}")
print(f"|err|       = {err:.2e}")
assert err < 1e-12
```

**æ¤œç®—å‡ºåŠ›ä¾‹**:
```
f(1.0)      = 0.909297
f'(1.0) AD  = -1.248441
f'(1.0) ana = -1.248441
|err|       = 0.00e+00
```

èª¤å·®ãŒ**å³å¯†ã‚¼ãƒ­**ï¼ˆæ•°å€¤ç²¾åº¦å†…ï¼‰ã€‚ã“ã‚ŒãŒæ•°å€¤å¾®åˆ†ï¼ˆ`$O(h^2)$`èª¤å·®ï¼‰ã¨ã®æ±ºå®šçš„ãªé•ã„ã ã€‚Dual Numbersã¯ä¸¸ã‚èª¤å·®ã‚’é™¤ã„ã¦**å³å¯†ãªå¾®åˆ†**ã‚’è¨ˆç®—ã™ã‚‹ã€‚

**å¤šå¤‰æ•°ã¸ã®æ‹¡å¼µ**: å¤‰æ•°ãŒ `$n$` å€‹ã‚ã‚‹ã¨ãã€`Dual(x_i, 1.0)` ã§ `$i$` ç•ªç›®ã®åå¾®åˆ†ã‚’è¨ˆç®—ã™ã‚‹ã€‚`$n$` å€‹ã® Forward passãŒå¿…è¦ï¼ˆReverse Modeãªã‚‰1å›ã§æ¸ˆã‚€ï¼‰ã€‚

#### 5.4.2 Reverse Mode ã®æ§‹é€  â€” Wengert Tape ã®æ•°å­¦

Forward Mode ã¯ã€Œ1å¤‰æ•°ã®å¾®åˆ†ã‚’1ãƒ‘ã‚¹ã§ã€å¾—ã‚‹ã€‚ã§ã¯ `$n=10^9$` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«æ±‚ã‚ã‚‹ã«ã¯ï¼Ÿ

**Wengert listï¼ˆè¨ˆç®—ãƒ†ãƒ¼ãƒ—ï¼‰**: Forward passã®å…¨ä¸­é–“å¤‰æ•°ã‚’è¨˜éŒ²ã™ã‚‹:

```math
v_1 = x_1,\ v_2 = x_2,\ v_3 = v_1 \cdot v_2,\ v_4 = \sin(v_1),\ v_5 = v_3 + v_4
```

**Reverse pass**: `$\bar{v}_i = \partial \mathcal{L}/\partial v_i$`ï¼ˆé€†å‘ãå‹¾é…ï¼‰ã‚’æœ«å°¾ã‹ã‚‰è¨ˆç®—:

```math
\bar{v}_5 = 1,\quad \bar{v}_3 = \bar{v}_5 \cdot 1,\quad \bar{v}_4 = \bar{v}_5 \cdot 1
```
```math
\bar{v}_1 = \bar{v}_3 \cdot v_2 + \bar{v}_4 \cdot \cos(v_1),\quad \bar{v}_2 = \bar{v}_3 \cdot v_1
```

1å›ã® Reverse passã§å…¨å…¥åŠ›ã®åå¾®åˆ† `$\partial \mathcal{L}/\partial v_1, \partial \mathcal{L}/\partial v_2$` ãŒåŒæ™‚ã«å¾—ã‚‰ã‚Œã‚‹ã€‚

**ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**: ãƒ†ãƒ¼ãƒ—å…¨ä½“ `$O(|\text{è¨ˆç®—ã‚°ãƒ©ãƒ•}|)$` ã‚’ä¿æŒã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚GPT-3ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã§ã¯æ•°åGBã«é”ã™ã‚‹ã€‚ã“ã‚ŒãŒActivation Checkpointingã®å¿…è¦æ€§ã®æ ¹æ‹ ã€‚

```mermaid
flowchart LR
    F["Forward Pass<br/>ãƒ†ãƒ¼ãƒ—ã«è¨˜éŒ²"] --> T["Wengert Tape<br/>vâ‚â†’vâ‚‚â†’â€¦â†’vâ‚™"]
    T --> R["Reverse Pass<br/>é€†é †ã«å‹¾é…è¨ˆç®—"]
    R --> G["âˆ‚L/âˆ‚xâ‚, â€¦, âˆ‚L/âˆ‚xâ‚™<br/>å…¨å‹¾é…ã‚’1ãƒ‘ã‚¹ã§"]
```

---

### 5.5 SVDã«ã‚ˆã‚‹ãƒã‚¤ã‚ºé™¤å» â€” æœ€é©é–¾å€¤ã®ç†è«–

ãƒ©ãƒ³ã‚¯kè¿‘ä¼¼ã§ãƒã‚¤ã‚ºã‚’é™¤å»ã™ã‚‹ã¨ãã€ã€Œkã‚’ã„ãã¤ã«ã™ã‚‹ã‹ã€ãŒå•é¡Œã«ãªã‚‹ã€‚å¤§ãã™ãã‚‹ã¨ãƒã‚¤ã‚ºã‚’æ®‹ã—ã€å°ã•ã™ãã‚‹ã¨ä¿¡å·ã‚’å¤±ã†ã€‚

**Marchenko-Pasturåˆ†å¸ƒã¨æœ€é©é–¾å€¤**[^1]:

è¦³æ¸¬è¡Œåˆ— `$Y = X + N$` ï¼ˆ`$X$` = çœŸã®ä¿¡å·ã€`$N$` = ãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¤ã‚º `$\sigma$`ï¼‰ã®ã¨ãã€ãƒã‚¤ã‚ºç”±æ¥ã®ç‰¹ç•°å€¤ã¯ä»¥ä¸‹ã®ç¯„å›²ã«é›†ä¸­ã™ã‚‹:

```math
\sigma_i(N) \leq \sigma_{\text{th}} = \sigma \cdot \omega(\beta), \quad \beta = \frac{n}{m},\quad \omega(\beta) = (1 + \sqrt{\beta})^2 + \cdots
```

ç°¡æ˜“ç‰ˆï¼ˆ`$\beta \leq 1$`ï¼‰:

```math
\sigma_{\text{th}} \approx \sigma \sqrt{2(m+n) + \sigma^2}
```

**å®Ÿç”¨çš„ãª Median Absolute Deviationï¼ˆMADï¼‰æ¨å®š**:

ãƒã‚¤ã‚ºæ¨™æº–åå·® `$\sigma$` ãŒæœªçŸ¥ã®ã¨ãã€ç‰¹ç•°å€¤ã®ä¸­å¤®å€¤ã‹ã‚‰æ¨å®šã§ãã‚‹:

```math
\hat{\sigma} = \frac{\text{median}(\sigma_1,\ldots,\sigma_r)}{0.6745\sqrt{m}}
```

ã“ã® `$\hat{\sigma}$` ã‚’é–¾å€¤è¨ˆç®—ã«ä»£å…¥ã™ã‚Œã°ã€ãƒ‡ãƒ¼ã‚¿é©å¿œçš„ãªæœ€é©ãƒ©ãƒ³ã‚¯é¸æŠãŒå¯èƒ½ã€‚

```math
\hat{k} = \#\left\{i : \sigma_i > \sigma_{\text{th}}\right\}
```

SVDå¾Œã®ç‰¹ç•°å€¤ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’è¦‹ã‚‹æ–¹æ³•ã®ã²ã¨ã¤ã¨ã—ã¦è¦šãˆã¦ãŠãã€‚çœŸã®ä¿¡å·ãŒã€Œæ•°å€‹ã®å¤§ããªç‰¹ç•°å€¤ã€ã¨ã—ã¦ç¾ã‚Œã€ãƒã‚¤ã‚ºãŒã€Œä¸€æ§˜ã«å°ã•ã„ç‰¹ç•°å€¤ã€ã¨ã—ã¦ç¾ã‚Œã‚‹ã¨ãã€é–¾å€¤ã‚«ãƒƒãƒˆãŒç¶ºéº—ã«æ©Ÿèƒ½ã™ã‚‹ã€‚

```math
\hat{A} = \hat{A}_{\hat{k}} = \sum_{i=1}^{\hat{k}} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top
```

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰**ï¼ˆæ•°å€¤çš„æ‰‹é †ï¼‰:

1. `$Y$` ã®SVDã‚’è¨ˆç®—: `$U, s, V^\top = \text{svd}(Y)$`
2. `$\sigma$` ãŒæœªçŸ¥ãªã‚‰ MADæ¨å®š: `$\hat{\sigma} = \text{median}(s) / (0.6745\sqrt{m})$`
3. ã—ãã„å€¤: `$\text{th} = \hat{\sigma}\sqrt{2(m+n)}$`
4. `$\hat{k} = |\{i : s_i > \text{th}\}|$` ã‚’è¨ˆç®—
5. å†æ§‹æˆ: `$\hat{A} = U_{:,:\hat{k}} \cdot \text{diag}(s_{:\hat{k}}) \cdot V^\top_{:\hat{k},:}$`

**shape ã®ç¢ºèª**: `$U_{:,:\hat{k}} \in \mathbb{R}^{m \times \hat{k}}$`ã€`$\text{diag}(s_{:\hat{k}}) \cdot V^\top_{:\hat{k},:} \in \mathbb{R}^{\hat{k} \times n}$` â†’ ç©ã¯ `$(m, n)$` ã«æˆ»ã‚‹ã€‚

**æ•°å€¤ä¾‹**: `$m=80, n=60$`ã€çœŸã®ãƒ©ãƒ³ã‚¯ `$r=5$`ã€ãƒã‚¤ã‚º `$\sigma=0.5$` ã®å ´åˆ:

```math
\text{th} = 0.5 \times \sqrt{2(80+60)} \approx 8.37
```

çœŸã®ç‰¹ç•°å€¤ãŒ `$[10, 8, 6, 4, 2]$`ã€ãƒã‚¤ã‚ºå¾Œã®å€¤ãŒ `$[10.3, 8.2, 6.1, 4.4, 2.8, 1.2, \ldots]$`ã€‚ã—ãã„å€¤ `$8.37$` ã¯ `$\sigma_1=10.3, \sigma_2=8.2$` ã ã‘ã‚’é€šã™ â†’ `$\hat{k}=2`ã€‚çœŸã®ãƒ©ãƒ³ã‚¯5ã‚ˆã‚Šä½ãè¦‹ç©ã‚‚ã‚‹ãŒã€ä¿¡å·å¯¾ãƒã‚¤ã‚ºãŒä½ã„æˆåˆ†ï¼ˆ`$\sigma_3=6.1 \approx \text{th}$`ï¼‰ã¯ä¸ç¢ºã‹ãªãŸã‚ä¿å®ˆçš„ã«åˆ‡ã‚‹é¸æŠã¯åˆç†çš„ã ã€‚

---

### 5.6 2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‹¾é… â€” Reverse Modeã®å…¨ã‚¹ãƒ†ãƒƒãƒ—

Reverse Mode ADã‚’ã€ŒPyTorchãªã—ã€ã§æ‰‹ã§å®Ÿè£…ã™ã‚‹ã€‚2å±¤NNã‚’ä¾‹é¡Œã¨ã—ã¦é¸ã¶ç†ç”±: å…¥åŠ›â†’éš ã‚Œå±¤â†’å‡ºåŠ›â†’æå¤±ã®4ã‚¹ãƒ†ãƒƒãƒ—ãŒã€Reverse Modeã®å…¸å‹çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å…¨ã¦å«ã‚€ã€‚

**Forward pass**:

```math
\begin{aligned}
Z_1 &= X W_1^{\top},\quad Z_1 \in \mathbb{R}^{B \times H}\\
H_1 &= \mathrm{ReLU}(Z_1),\quad H_1 \in \mathbb{R}^{B \times H}\\
Z_2 &= H_1 W_2^{\top},\quad Z_2 \in \mathbb{R}^{B \times C}\\
P &= \mathrm{softmax}(Z_2),\quad P \in \mathbb{R}^{B \times C}\\
\mathcal{L} &= -\frac{1}{B}\sum_{b,c} Y_{bc} \log P_{bc}
\end{aligned}
```

| å¤‰æ•° | shape | æ„å‘³ |
|:-----|:------|:-----|
| `$X$` | `$(B,D)$` | å…¥åŠ›ãƒãƒƒãƒ |
| `$W_1$` | `$(H,D)$` | ç¬¬1å±¤é‡ã¿ |
| `$W_2$` | `$(C,H)$` | ç¬¬2å±¤é‡ã¿ |
| `$Y$` | `$(B,C)$` | one-hot ãƒ©ãƒ™ãƒ« |

**Reverse pass** â€” å‹¾é…ã‚’å‡ºåŠ›å´ã‹ã‚‰é€†ã«è¨ˆç®—:

```math
\frac{\partial \mathcal{L}}{\partial Z_2} = \frac{1}{B}(P - Y),\quad \in \mathbb{R}^{B \times C}
```

```math
\frac{\partial \mathcal{L}}{\partial W_2} = \frac{\partial \mathcal{L}}{\partial Z_2}^{\top} H_1,\quad \in \mathbb{R}^{C \times H}
```

```math
\frac{\partial \mathcal{L}}{\partial H_1} = \frac{\partial \mathcal{L}}{\partial Z_2} W_2,\quad \in \mathbb{R}^{B \times H}
```

```math
\frac{\partial \mathcal{L}}{\partial Z_1} = \frac{\partial \mathcal{L}}{\partial H_1} \odot \mathbf{1}[Z_1 > 0],\quad \in \mathbb{R}^{B \times H}
```

```math
\frac{\partial \mathcal{L}}{\partial W_1} = \frac{\partial \mathcal{L}}{\partial Z_1}^{\top} X,\quad \in \mathbb{R}^{H \times D}
```

**å„ã‚¹ãƒ†ãƒƒãƒ—ã®è¨˜å·â†”å¤‰æ•°åå¯¾å¿œ**:

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰å¤‰æ•° |
|:-----|:---------|
| `$Z_1$` | `z1` |
| `$H_1 = \mathrm{ReLU}(Z_1)$` | `h1` |
| `$\partial\mathcal{L}/\partial Z_2$` | `dz2` |
| `$\partial\mathcal{L}/\partial W_1$` | `dW1` |

**ReLUã®å‹¾é…**: `$\mathbf{1}[Z_1 > 0]$` ã¯ã€ŒForward passã§æ­£ã ã£ãŸãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã¿å‹¾é…ãŒé€šã‚‹ã€ã€‚Hadamardç© `$\odot$` ã§å®Ÿè£…ã™ã‚‹ã€‚

**shapeç¢ºèª**: `dW2 = dz2.T @ h1` â€” `dz2.T` ãŒ `(C,B)`ã€`h1` ãŒ `(B,H)` â†’ ç©ã¯ `(C,H)` ã§ `W2` ã¨åŒ shapeã€‚âœ…

#### 5.6.1 LayerNorm ã®å‹¾é… â€” æ­£è¦åŒ–å±¤ã®å¾®åˆ†

Transformer ã«ã¯ LayerNorm ãŒä¸å¯æ¬ ã ã€‚é€†ä¼æ’­ã§ãã®å‹¾é…ã‚’æ‰‹ã§å°å‡ºã™ã‚‹ã¨ã€ãªãœ LayerNorm ãŒå­¦ç¿’ã‚’å®‰å®šåŒ–ã•ã›ã‚‹ã‹ãŒè¦‹ãˆã¦ãã‚‹ã€‚

**Forward pass**:

```math
\mu = \frac{1}{d}\sum_{j=1}^d x_j,\quad
\sigma^2 = \frac{1}{d}\sum_{j=1}^d (x_j - \mu)^2,\quad
\hat{x}_j = \frac{x_j - \mu}{\sqrt{\sigma^2 + \varepsilon}},\quad
y_j = \gamma_j \hat{x}_j + \beta_j
```

å¤‰æ•°ã® shapeï¼ˆãƒãƒƒãƒã‚’ç„¡è¦–ã—ãŸ1ã‚µãƒ³ãƒ—ãƒ«ç‰ˆï¼‰:

| å¤‰æ•° | shape | èª¬æ˜ |
|:-----|:------|:-----|
| `$\mathbf{x}$` | `$(d,)$` | å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ« |
| `$\mu, \sigma^2$` | scalar | å¹³å‡ãƒ»åˆ†æ•£ |
| `$\hat{\mathbf{x}}$` | `$(d,)$` | æ­£è¦åŒ–æ¸ˆã¿ |
| `$\boldsymbol{\gamma}, \boldsymbol{\beta}$` | `$(d,)$` | å­¦ç¿’å¯èƒ½ã‚¹ã‚±ãƒ¼ãƒ«ãƒ»ãƒã‚¤ã‚¢ã‚¹ |

**`$\boldsymbol{\gamma}$` ã®å‹¾é…** ã¯ã‚·ãƒ³ãƒ—ãƒ«:

```math
\frac{\partial \mathcal{L}}{\partial \gamma_j} = \frac{\partial \mathcal{L}}{\partial y_j} \hat{x}_j
```

**`$\mathbf{x}$` ã®å‹¾é…** ã¯é€£é–å¾‹ãŒè¤‡é›‘ã«ãªã‚‹ï¼ˆ`$\mu$` ã¨ `$\sigma^2$` ãŒ `$\mathbf{x}$` ã«ä¾å­˜ã™ã‚‹ãŸã‚ï¼‰:

```math
\frac{\partial \mathcal{L}}{\partial x_j} = \frac{1}{d\sigma}\left[d\,\delta_j - \sum_k \delta_k - \hat{x}_j \sum_k \delta_k \hat{x}_k\right],
\quad \delta_j = \gamma_j \frac{\partial \mathcal{L}}{\partial y_j}
```

`$d$` ã§å‰²ã£ã¦ã„ã‚‹ã®ãŒã€Œå¹³å‡åŒ–ã€ã®å½±éŸ¿ã€‚`$\hat{x}_j \sum_k \delta_k \hat{x}_k$` ã¯æ­£è¦åŒ–æ–¹å‘ã¸ã®æˆåˆ†ã‚’é™¤å»ã™ã‚‹ï¼ˆå°„å½±ï¼‰ã€‚

ã“ã®å¼ã®æ§‹é€ ãŒé‡è¦: LayerNorm ã®é€†ä¼æ’­ã¯ã€Œå¹³å‡æˆåˆ†ã¨åˆ†æ•£æ–¹å‘æˆåˆ†ã‚’å·®ã—å¼•ã„ãŸã€æ¥å¹³é¢ã¸ã®å°„å½±ã€ã ã€‚ã“ã‚ŒãŒå‹¾é…ã®çˆ†ç™ºãƒ»æ¶ˆå¤±ã‚’æŠ‘ãˆã‚‹å¹¾ä½•å­¦çš„ç†ç”±ã€‚

#### 5.6.2 å‹¾é…ã®æ¤œç®—æˆ¦ç•¥

è¤‡é›‘ãªé€†ä¼æ’­ã‚’å®Ÿè£…ã—ãŸå¾Œã®æ¤œè¨¼æ–¹æ³•:

```math
\text{relative error} = \frac{\|\mathbf{g}_{\text{analytic}} - \mathbf{g}_{\text{numeric}}\|}{\|\mathbf{g}_{\text{analytic}}\| + \|\mathbf{g}_{\text{numeric}}\| + \varepsilon} < 10^{-5}
```

åˆ¤å®šåŸºæº–ã®ç›®å®‰:
- `$< 10^{-7}$`: å®Œç’§ï¼ˆå€ç²¾åº¦ã®é™ç•Œï¼‰
- `$10^{-5}$` ï½ `$10^{-7}$`: å•é¡Œãªã—
- `$10^{-3}$` ï½ `$10^{-5}$`: è¦èª¿æŸ»ï¼ˆã»ã¼æ­£ã—ã„ãŒç¢ºèªãŒå¿…è¦ï¼‰
- `$> 10^{-3}$`: ãƒã‚°ã‚ã‚Š

**åº§æ¨™åˆ¥ãƒã‚§ãƒƒã‚¯**: å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸€æ‹¬ã‚ˆã‚Šã€ã¾ãš `$W_1[0,0]$`ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼1ã¤ï¼‰ã ã‘ã‚’ç¢ºèªã™ã‚‹ã€‚å•é¡Œã‚’å±€æ‰€åŒ–ã§ãã‚‹ã€‚

---

### 5.7 ç†è§£åº¦ãƒã‚§ãƒƒã‚¯ â€” Z5 å®Œå…¨ç¿’å¾—ãƒ†ã‚¹ãƒˆ

<details>
<summary>Q1: truncated SVD ã®ãƒ©ãƒ³ã‚¯-k è¿‘ä¼¼èª¤å·®ã‚’ Frobenius ãƒãƒ«ãƒ ã§æ›¸ã‘ã€‚</summary>

```math
\|A - A_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2
```

**æ¤œç®—**: `k=r`ï¼ˆfull rankï¼‰ã®ã¨ãèª¤å·®ã‚¼ãƒ­ã€‚`k=0` ã®ã¨ã `\|A\|_F^2 = \sum_i \sigma_i^2`ï¼ˆParsevalç­‰å¼ï¼‰ã€‚

</details>

<details>
<summary>Q2: `$f(\mathbf{x}) = \mathbf{a}^\top \mathbf{x}$` ã®å‹¾é…ã¯ä½•ã‹ï¼Ÿ</summary>

```math
\nabla_{\mathbf{x}} (\mathbf{a}^\top \mathbf{x}) = \mathbf{a}
```

å®šæ•°ãƒ™ã‚¯ãƒˆãƒ«ã®å†…ç©å¾®åˆ† = å®šæ•°ãƒ™ã‚¯ãƒˆãƒ«ã€‚å½¢: `$(d,) \to (d,)$`ï¼ˆå‹¾é…ã¯å…¥åŠ›ã¨åŒ shapeï¼‰ã€‚

</details>

<details>
<summary>Q3: `$f(W) = \mathbf{x}^\top W \mathbf{y}$` ã® `$W$` ã«é–¢ã™ã‚‹å‹¾é…ã‚’è¡Œåˆ—ã§è¡¨ã›ã€‚</summary>

```math
\frac{\partial f}{\partial W} = \mathbf{x} \mathbf{y}^\top
```

shape: `$\mathbf{x} \in \mathbb{R}^m$`, `$\mathbf{y} \in \mathbb{R}^n$` â†’ å‹¾é…ã¯ `$m \times n$`ï¼ˆ`$W$` ã¨åŒ shapeï¼‰ã€‚å¤–ç© `$\mathbf{x}\mathbf{y}^\top$` ã«ãªã‚‹ã®ãŒç›´è¦³: `$f$` ã¯ `$W_{ij}$` ã« `$x_i y_j$` åˆ†ã ã‘ä¾å­˜ã™ã‚‹ã‹ã‚‰ã€‚

</details>

<details>
<summary>Q4: Softmax + Cross-Entropy ã®åˆæˆå‹¾é…ãŒ `$\mathbf{p} - \mathbf{y}$` ã«ãªã‚‹ç†ç”±ã‚’èª¬æ˜ã›ã‚ˆã€‚</summary>

```math
\mathcal{L} = -\sum_c y_c \log p_c,\quad p_c = \frac{e^{z_c}}{\sum_j e^{z_j}}
```

```math
\frac{\partial \mathcal{L}}{\partial z_j} = p_j - y_j
```

Softmax ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ `$\partial p_i / \partial z_j = p_i(\delta_{ij} - p_j)$` ã« Cross-Entropy ã®å¤–å¾®åˆ† `$-y_i/p_i$` ã‚’åˆæˆã™ã‚‹ã¨ã€`$-y_j + p_j \sum_i y_i = p_j - y_j$`ï¼ˆ`$\sum_i y_i = 1$`ï¼‰ã€‚

</details>

<details>
<summary>Q5: Forward Mode AD ã¨ Reverse Mode AD ã®ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãçŠ¶æ³ã‚’èª¬æ˜ã›ã‚ˆã€‚</summary>

- å…¥åŠ›æ¬¡å…ƒ `$n$`ã€å‡ºåŠ›æ¬¡å…ƒ `$m$` ã¨ã—ã¦:
  - Forward Mode: `$n \ll m$` ã®ã¨ã `$O(n)$` ãƒ‘ã‚¹ã§æ¸ˆã‚€
  - Reverse Mode: `$m \ll n$` ã®ã¨ã `$O(m)$` ãƒ‘ã‚¹ã§æ¸ˆã‚€
- DLè¨“ç·´: `$n = 10^9$`, `$m = 1$` â†’ Reverse åœ§å€’çš„æœ‰åˆ©
- ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ï¼ˆ`$m > n$`ï¼‰: Forward ã®æ–¹ãŒåˆ—ã”ã¨ã«1ãƒ‘ã‚¹ã§å¾—ã‚‰ã‚Œã‚‹

</details>

<details>
<summary>Q6: einsum `'ij,jk->ik'` ã¨ `'ij,kj->ik'` ã®é•ã„ã¯ï¼Ÿ</summary>

- `'ij,jk->ik'`: é€šå¸¸ã®è¡Œåˆ—ç© `$C_{ik} = \sum_j A_{ij} B_{jk}$`
- `'ij,kj->ik'`: `$C_{ik} = \sum_j A_{ij} B_{kj} = A B^\top$` â€”â€” BãŒè»¢ç½®ã•ã‚Œã¦ã„ã‚‹

shape ã‚’ãã‚Œãã‚Œç¢ºèª: å‰è€…ã¯ `A: (m,k)`, `B: (k,n)` â†’ `C: (m,n)`. å¾Œè€…ã¯ `A: (m,k)`, `B: (l,k)` â†’ `C: (m,l)`.

</details>

<details>
<summary>Q7: LoRA ã®è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ã€å…ƒã®é‡ã¿è¡Œåˆ—ã¨æ¯”è¼ƒã›ã‚ˆã€‚</summary>

å…ƒã®é‡ã¿è¡Œåˆ— `$W \in \mathbb{R}^{d \times k}$`: `$dk$` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚
LoRA åˆ†è§£ `$\Delta W = BA$` (`$B \in \mathbb{R}^{d \times r}$`, `$A \in \mathbb{R}^{r \times k}$`): `$(d+k)r$` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚

åœ§ç¸®ç‡: `$\frac{(d+k)r}{dk} = r\left(\frac{1}{k} + \frac{1}{d}\right) \approx \frac{2r}{\min(d,k)}$`

`$r = 4$`, `$d = k = 4096$` ã®ã¨ã: `$2 \times 4 / 4096 \approx 0.2\%$`ã€‚

</details>

### Quick Check â€” Z5 å®Œäº†ç¢ºèª

<details>
<summary>å®Ÿè£…ã®è‡ªå·±è¨ºæ–­: 3ã¤ã®æ•°å€¤ã§å…¨ä½“ã‚’ç¢ºèª</summary>

ä»¥ä¸‹ã®3ã¤ãŒå…¨ã¦æˆç«‹ã™ã‚Œã°ã€Z5ã®å®Ÿè£…ã¯æ­£ã—ã„:

1. **SVDèª¤å·®**: `np.abs(np.linalg.norm(A - svd_rank_k(A, k), 'fro') - tail_energy_bound(s, k)) < 1e-6`
2. **å‹¾é…æ¤œç®—**: äºŒæ¬¡å½¢å¼ `$f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top A\mathbf{x}$` ã®ç›¸å¯¾èª¤å·® `< 1e-6`
3. **Dual Numbers**: `diff(lambda x: x**2 + x, 1.0) == 3.0`ï¼ˆè§£æå€¤: `$2(1)+1=3$`ï¼‰

ã“ã®3ã¤ãŒé€šã‚‰ãªã„é™ã‚Šã€å…ˆã¸é€²ã¾ãªã„ã“ã¨ã€‚

</details>

> Progress: 85%

---

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆ30åˆ†ï¼‰â€” SVDãƒ»è¡Œåˆ—å¾®åˆ†ã®ç ”ç©¶æœ€å‰ç·š

### 6.1 LoRA â€” ä½ãƒ©ãƒ³ã‚¯é©å¿œã®æ•°å­¦çš„æ ¹æ‹ 

LoRA[^2]ã®æ ¸å¿ƒã¯ã€ŒFine-tuningæ™‚ã®é‡ã¿å¤‰åŒ– `$\Delta W$` ã¯ä½ãƒ©ãƒ³ã‚¯ã§ååˆ†ã€ã¨ã„ã†çµŒé¨“çš„è¦³å¯Ÿã ã€‚ãªãœãã‚ŒãŒæˆç«‹ã™ã‚‹ã®ã‹ï¼Ÿ

**Aghajanyan et al. (2021) ã®å†…åœ¨æ¬¡å…ƒä»®èª¬**[^3]:

Pre-trained ãƒ¢ãƒ‡ãƒ«ã¯ã€Œé«˜æ¬¡å…ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®ã€ã”ãä½æ¬¡å…ƒã®éƒ¨åˆ†ç©ºé–“ã€ã«åˆ¶é™ã•ã‚ŒãŸã¾ã¾ã§ã‚‚ã‚¿ã‚¹ã‚¯ã‚’è§£ã‘ã‚‹ã€‚

```math
\mathcal{L}(\theta) \approx \mathcal{L}(\theta_0 + P \phi), \quad P \in \mathbb{R}^{D \times d},\; d \ll D
```

`$D$` = å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¬¡å…ƒã€`$d$` = å†…åœ¨æ¬¡å…ƒï¼ˆGPT-2: `$d \approx 100$`ï¼‰ã€‚

**LoRAã®å®šå¼åŒ–**[^2]:

```math
h = W_0 x + \Delta W x = W_0 x + B A x
```

```math
B \in \mathbb{R}^{d_{\text{model}} \times r},\quad A \in \mathbb{R}^{r \times d_{\text{model}}},\quad r \ll d_{\text{model}}
```

åˆæœŸåŒ–: `$A \sim \mathcal{N}(0, \sigma^2)$`, `$B = 0$` â†’ Fine-tuningé–‹å§‹æ™‚ã¯ `$\Delta W = 0$`ï¼ˆå…ƒã®æŒ™å‹•ã‚’ä¿æŒï¼‰ã€‚

**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°**: å®Ÿè£…ã§ã¯ `$\Delta W = \frac{\alpha}{r} BA$`ï¼ˆ`$\alpha$` ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã€‚ã“ã‚Œã«ã‚ˆã‚Š `$r$` ã‚’å¤‰ãˆã¦ã‚‚ã‚¹ã‚±ãƒ¼ãƒ«ãŒå®‰å®šã™ã‚‹ã€‚

```mermaid
flowchart LR
  X[å…¥åŠ› x] --> W0["W_0 xï¼ˆå‡çµï¼‰"]
  X --> A["A x (rÃ—d)"]
  A --> B["B(Ax) (dÃ—r)"]
  W0 --> ADD["+"]
  B --> ADD
  ADD --> H[å‡ºåŠ› h]
```

**SVDã¨ã®æ¥ç¶š**: LoRAã® `$BA$` ã¯ rank-r è¡Œåˆ—ã® SVDåˆ†è§£ã®å› æ•°ã¨åŒå‹ã€‚é•ã„ã¯ã€LoRAã§ã¯ `$B,A$` ã‚’ç›´æ¥å­¦ç¿’ã™ã‚‹ã®ã«å¯¾ã—ã€SVDã¯æ—¢å­˜ã®è¡Œåˆ—ã‚’å¾Œã‹ã‚‰åˆ†è§£ã™ã‚‹ã€‚

**LoRAæ´¾ç”Ÿæ‰‹æ³•ã®æ¦‚è¦³**:

| æ‰‹æ³• | ç‰¹å¾´ | æ ¸å¿ƒçš„æ”¹å–„ |
|:-----|:-----|:---------|
| LoRA[^2] | å‡ä¸€ãƒ©ãƒ³ã‚¯ `$r$` | Fine-tuningåŸºç¤ |
| AdaLoRA[^5] | SVD + é‡è¦åº¦ã‚¹ã‚³ã‚¢ã§å¯å¤‰ãƒ©ãƒ³ã‚¯ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ |
| DoRA | æ–¹å‘æ€§ `$W$` + å¤§ãã• `$m$` ã«åˆ†è§£ | è¡¨ç¾åŠ›å‘ä¸Š |
| QLoRA | 4-bité‡å­åŒ– + LoRA | ãƒ¡ãƒ¢ãƒªå¤§å¹…å‰Šæ¸› |
| LoRA+ | `$A$` ã¨ `$B$` ã«ç•°ãªã‚‹å­¦ç¿’ç‡ | å­¦ç¿’é€Ÿåº¦2å€ |

DoRAã¯ `$W = m \cdot \frac{W_0 + BA}{\|W_0 + BA\|}$`ï¼ˆ`$m$` = ã‚¹ã‚«ãƒ©ãƒ¼å¤§ãã•ï¼‰ã«åˆ†è§£ã—ã€æ–¹å‘ã¨å¤§ãã•ã‚’ç‹¬ç«‹ã«å­¦ç¿’ã€‚ã“ã®åˆ†è§£ã¯SVDã®ã€Œå›è»¢ã€ã¨ã€Œã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ã®åˆ†é›¢ã¨å¯¾å¿œã™ã‚‹ã€‚


### 6.2 Randomized SVD â€” å¤§è¦æ¨¡è¡Œåˆ—ã®è¿‘ä¼¼

`$A \in \mathbb{R}^{m \times n}$`ã€`$m = n = 10^5$` ã®å ´åˆã€å…¨ä½“ã®SVDã¯ `$O(n^3)$` ã§ä¸å¯èƒ½ã€‚Halko et al. (2011)[^1]ã®Randomized SVDã¯ `$O(mn\log k)$` ã§ rank-kè¿‘ä¼¼ã‚’è¨ˆç®—ã™ã‚‹ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. **ãƒ©ãƒ³ãƒ€ãƒ å°„å½±**: `$\Omega \in \mathbb{R}^{n \times (k+p)}$`ï¼ˆ`$p$` = oversampling, é€šå¸¸10ï¼‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆ
   ```math
   Y = A \Omega \in \mathbb{R}^{m \times (k+p)}
   ```

2. **æ­£è¦ç›´äº¤åŸºåº•**: `$Y$` ã®QRåˆ†è§£
   ```math
   Y = Q R,\quad Q \in \mathbb{R}^{m \times (k+p)}
   ```

3. **å°è¡Œåˆ—ã¸ã®å°„å½±**: 
   ```math
   B = Q^\top A \in \mathbb{R}^{(k+p) \times n}
   ```

4. **å°è¡Œåˆ—ã®SVD**: `$B = \tilde{U} \Sigma V^\top$`ï¼ˆ`$(k+p) \times n$` ãªã®ã§é«˜é€Ÿï¼‰

5. **å¾©å…ƒ**: `$U = Q \tilde{U}$`

**ãªãœå‹•ãã®ã‹ï¼Ÿ**: ãƒ©ãƒ³ãƒ€ãƒ å°„å½± `$\Omega$` ã®åˆ—ãŒã»ã¼ç¢ºå®Ÿã« `$A$` ã®åˆ—ç©ºé–“ã®æœ‰åŠ¹ãªåŸºåº•ã‚’è¿‘ä¼¼ã™ã‚‹ï¼ˆç¢ºç‡é›†ä¸­ç¾è±¡ï¼‰ã€‚èª¤å·®ã¯ `$\sigma_{k+1}$`ï¼ˆæ¬¡ã®ç‰¹ç•°å€¤ï¼‰ã«ä¾å­˜ã™ã‚‹ã€‚

**è¨ˆç®—é‡æ¯”è¼ƒ**:

| æ‰‹æ³• | è¨ˆç®—é‡ | ç”¨é€” |
|:-----|:-------|:-----|
| å…¨ä½“SVD | `$O(\min(m,n) \cdot mn)$` | æ­£ç¢ºè§£ã€å°è¦æ¨¡ |
| Randomized SVD | `$O(mn\log k)$` | å¤§è¦æ¨¡ã€è¿‘ä¼¼ |
| Power iteration variant | `$O(q \cdot mn)$` | ã‚ˆã‚Šé«˜ç²¾åº¦ï¼ˆ`$q$` = iteræ•°ï¼‰ |

```mermaid
flowchart TD
  A["A (mÃ—n)"] --> Om["Î© random (nÃ—k+p)"]
  Om --> Y["Y=AÎ© (mÃ—k+p)"]
  Y --> Q["QR â†’ Q (mÃ—k+p)"]
  Q --> B["B=Q^T A (k+pÃ—n)"]
  B --> S["SVD(B) = Å¨ Î£ V^T"]
  Q --> U["U = QÅ¨ (mÃ—k+p)"]
  U --> Ak["A_k = U Î£ V^T"]
```

### 6.3 AdaLoRA â€” SVDã«ã‚ˆã‚‹é©å¿œçš„ãƒ©ãƒ³ã‚¯å‰²ã‚Šå½“ã¦

LoRAã®å¼±ç‚¹: å…¨é‡ã¿è¡Œåˆ—ã«åŒã˜ãƒ©ãƒ³ã‚¯ `$r$` ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã€‚ã—ã‹ã—ã€é‡ã¿è¡Œåˆ—ã«ã‚ˆã£ã¦é‡è¦åº¦ã¯ç•°ãªã‚‹ã€‚

AdaLoRA[^5]ã¯ SVD ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã¨é‡è¦åº¦ã‚¹ã‚³ã‚¢ã«ã‚ˆã£ã¦ã€ãƒ©ãƒ³ã‚¯å‰²ã‚Šå½“ã¦ã‚’**å‹•çš„ã«**èª¿æ•´ã™ã‚‹ã€‚

**SVDåˆ†è§£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–**:

```math
\Delta W = P \Lambda Q, \quad P \in \mathbb{R}^{d \times r},\; Q \in \mathbb{R}^{r \times k}
```

`$\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_r)$` ãŒç‰¹ç•°å€¤è¡Œåˆ—ã§ã€å­¦ç¿’ä¸­ã«ä¸€éƒ¨ã‚’ã‚¼ãƒ­ãƒã‚¹ã‚¯ã™ã‚‹ã“ã¨ã§ãƒ©ãƒ³ã‚¯ã‚’åˆ¶å¾¡ã€‚

**é‡è¦åº¦ã‚¹ã‚³ã‚¢** `$s_i$`ï¼ˆå„ç‰¹ç•°å€¤æˆåˆ†ã®é‡è¦åº¦ï¼‰:

```math
s_i = \left|\lambda_i\right| \cdot \left(\left|\mathbf{p}_i\right| \cdot \left|\mathbf{q}_i\right|\right)^{1/2}
```

é‡è¦åº¦ãŒä½ã„æˆåˆ†ï¼ˆ`$s_i$` ãŒå°ã•ã„ï¼‰ã¯ `$\lambda_i \leftarrow 0$` ã«ãƒã‚¹ã‚¯ã—ã€é‡è¦ãªæˆåˆ†ã«ã€Œãƒ©ãƒ³ã‚¯äºˆç®—ã€ã‚’å†é…åˆ†ã€‚

**ç›´äº¤æ€§æ­£å‰‡åŒ–**: `$P, Q$` ãŒç›´äº¤ã«è¿‘ããªã‚‹ã‚ˆã†æ­£å‰‡åŒ–:

```math
\mathcal{R}(P, Q) = \|P^\top P - I\|_F^2 + \|QQ^\top - I\|_F^2
```

ã“ã‚Œã«ã‚ˆã‚Šç‰¹ç•°å€¤åˆ†è§£ã®ã€Œåˆ†é›¢æ€§ã€ãŒç¶­æŒã•ã‚Œã€ç‰¹å®šã® `$\lambda_i$` ã‚’ã‚¼ãƒ­ã«ã—ã¦ã‚‚ä»–æˆåˆ†ã«å½±éŸ¿ãŒå°‘ãªã„ã€‚

**LoRA vs AdaLoRA ã®æ¯”è¼ƒ**:

| ç‰¹æ€§ | LoRA | AdaLoRA |
|:-----|:-----|:--------|
| ãƒ©ãƒ³ã‚¯å‰²ã‚Šå½“ã¦ | å…¨å±¤å‡ä¸€ | é‡è¦åº¦ã«å¿œã˜ã¦å‹•çš„ |
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ | ä¸­ | é«˜ï¼ˆåŒäºˆç®—ã§ç²¾åº¦å‘ä¸Šï¼‰ |
| è¨ˆç®—ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ | ä½ | ä¸­ï¼ˆãƒ©ãƒ³ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å¿…è¦ï¼‰ |
| SVDã®å½¹å‰² | å¾Œå‡¦ç†åˆ†æ | è¨“ç·´ä¸­ã®ä¸­æ ¸æ§‹é€  |

```mermaid
flowchart TD
  Init["åˆæœŸåŒ–: é«˜ãƒ©ãƒ³ã‚¯ Î” W = PÎ›Q"] --> Train["è¨“ç·´: é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"]
  Train --> Prune["ãƒ©ãƒ³ã‚¯å‰Šæ¸›: ä½ã‚¹ã‚³ã‚¢ã® Î»_i â†’ 0"]
  Prune --> Realloc["å†é…åˆ†: é‡è¦å±¤ã«ãƒ©ãƒ³ã‚¯äºˆç®—"]
  Realloc --> Train
```

**AdaLoRAã®è¨“ç·´ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**:

è¨“ç·´ã®åˆæœŸï¼ˆ`$t < T_i$`ï¼‰ã¯ãƒ©ãƒ³ã‚¯å‰Šæ¸›ãªã—ï¼ˆå…¨ `$\lambda_i$` ã‚’æ›´æ–°ï¼‰ã€‚ä¸­æœŸï¼ˆ`$T_i \leq t < T_f$`ï¼‰ã§æ®µéšçš„ã«ãƒ©ãƒ³ã‚¯ã‚’å‰Šæ¸›ã€‚å¾ŒæœŸï¼ˆ`$t \geq T_f$`ï¼‰ã¯å›ºå®šãƒ©ãƒ³ã‚¯ã§åæŸã•ã›ã‚‹ã€‚

```math
r(t) = r_f + (r_0 - r_f) \cdot \left(1 - \frac{t - T_i}{T_f - T_i}\right)^3 \quad (T_i \leq t < T_f)
```

ä¸‰ä¹—ã‚«ãƒ¼ãƒ–ã§æ»‘ã‚‰ã‹ã«ãƒ©ãƒ³ã‚¯ã‚’å‰Šæ¸›ã™ã‚‹ã“ã¨ã§ã€çªç„¶ã®ãƒ©ãƒ³ã‚¯å¤‰åŒ–ã«ã‚ˆã‚‹å­¦ç¿’ä¸å®‰å®šã‚’å›é¿ã™ã‚‹ã€‚

### 6.4 FlashAttention â€” IO-awareãªè¡Œåˆ—æ¼”ç®—

GPUã®è¨ˆç®—ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯ã€å®Ÿã¯æ¼”ç®—æ•°ã§ã¯ãªããƒ¡ãƒ¢ãƒªå¸¯åŸŸã ã€‚

Vanilla Attentionã¯ `$N \times N$` ã® Attentionè¡Œåˆ—ã‚’HBMï¼ˆé«˜å¸¯åŸŸãƒ¡ãƒ¢ãƒªï¼‰ã«æ›¸ãè¾¼ã¿ã€å†åº¦èª­ã¿è¾¼ã‚€ã€‚ã“ã‚ŒãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€‚

**FlashAttention[^4]ã®æ ¸å¿ƒ**:

```math
O_i = \sum_j \frac{e^{q_i \cdot k_j / \sqrt{d}}}{\sum_l e^{q_i \cdot k_l / \sqrt{d}}} v_j
```

ã“ã®è¨ˆç®—ã‚’ **tiling + online softmax** ã§å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€`$N \times N$` è¡Œåˆ—ã‚’HBMã«æ›¸ãå‡ºã•ãšã«æ¸ˆã‚€ã€‚

**online softmax ã®æ›´æ–°å¼** (tileã‚µã‚¤ã‚º `$B_c$` ã”ã¨ã«é€æ¬¡æ›´æ–°):

```math
m_i^{\text{new}} = \max(m_i^{\text{old}},\, \max_j s_{ij}), \quad
\ell_i^{\text{new}} = e^{m_i^{\text{old}} - m_i^{\text{new}}} \ell_i^{\text{old}} + \sum_j e^{s_{ij} - m_i^{\text{new}}}
```

å„ tile ã‚’å‡¦ç†ã™ã‚‹ãŸã³ã«ã€`$m_i$`ï¼ˆrunning maxï¼‰ã¨ `$\ell_i$`ï¼ˆrunning sumï¼‰ã‚’æ›´æ–°ã€‚HBMã‚¢ã‚¯ã‚»ã‚¹ãŒ `$O(N)$` ã«å‰Šæ¸›ã•ã‚Œã‚‹ï¼ˆvanilla: `$O(N^2)$`ï¼‰ã€‚

**ãƒ¡ãƒ¢ãƒªè¤‡é›‘åº¦ã®æ¯”è¼ƒ**:

| ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | HBMã‚¢ã‚¯ã‚»ã‚¹ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | é€†ä¼æ’­ |
|:------------|:-----------|:----------|:------|
| Vanilla Attention | `$O(N^2)$` | `$O(N^2)$` | Attentionè¡Œåˆ—ä¿å­˜ |
| FlashAttention v1 | `$O(N)$` | `$O(N)$` | å†è¨ˆç®—ï¼ˆrecomputeï¼‰ |
| FlashAttention v2 | `$O(N)$` | `$O(N)$` | warpã”ã¨ä¸¦åˆ—åŒ–æ”¹å–„ |

é€†ä¼æ’­ã§ã¯ Attention è¡Œåˆ—ã‚’ä¿å­˜ã—ãªã„ã€‚ä»£ã‚ã‚Šã« `$m_i$`ï¼ˆmaxï¼‰ã¨ `$\ell_i$`ï¼ˆsumï¼‰ã ã‘ä¿æŒã—ã€backward æ™‚ã«Attentionã‚’**å†è¨ˆç®—**ã™ã‚‹ã€‚ãƒ¡ãƒ¢ãƒªãŒæµ®å‹•å°æ•°ç‚¹æ¼”ç®—ã‚ˆã‚Šå®‰ã„å ´åˆã€ã“ã‚ŒãŒæœ€é©ã€‚

**FlashAttention-2 ã®æ”¹å–„ç‚¹**: v1 ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ–¹å‘ï¼ˆã‚¯ã‚¨ãƒªï¼‰ã«å¤–ãƒ«ãƒ¼ãƒ—ã‚’ç½®ãã€KVæ–¹å‘ã«å†…ãƒ«ãƒ¼ãƒ—ã‚’æŒã¤è¨­è¨ˆã ã£ãŸã€‚v2 ã¯å¤–ãƒ«ãƒ¼ãƒ—ã‚’ã‚¯ã‚¨ãƒªå´ã«ã—ã¦ã€GPU warp é–“ã®é€šä¿¡ã‚’æœ€å°åŒ–ã—ãŸã€‚å®Ÿæ¸¬ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—: `$A100$` ã§ v1 æ¯” 2å€ã€ç†è«–ãƒ”ãƒ¼ã‚¯æ¯” 73%ã€‚

**ç·šå½¢ä»£æ•°ã¨ã®æ¥ç¶š**: FlashAttentionã¯ã€Œè¡Œåˆ—ç©ã®åˆ†å‰²å¯èƒ½æ€§ã€ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã€‚`$AB = \sum_k A_{:,k} B_{k,:}$` ã¨ã„ã†å¤–ç©å’Œåˆ†è§£ãŒã€tileã”ã¨ã®è¨ˆç®—ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚ã“ã‚Œã¯SVDã®é€æ¬¡è¿‘ä¼¼ã¨åŒã˜ã€Œåˆ†å‰²ã—ã¦è¨ˆç®—ã—ã€å¾Œã§çµ±åˆã€ã¨ã„ã†ç™ºæƒ³ã ã€‚

**ä½ãƒ©ãƒ³ã‚¯Attentionã¨ã®æ¯”è¼ƒ**: Linformer[^6]ãªã©ã¯ Attention è¡Œåˆ—ãã®ã‚‚ã®ã‚’ä½ãƒ©ãƒ³ã‚¯ `$P = E^\top K \in \mathbb{R}^{r \times N}$`ï¼ˆ`$r \ll N$`ï¼‰ã§è¿‘ä¼¼ã™ã‚‹ã€‚SVDã®è¦³ç‚¹ã§ã¯ã€Attentionã‚¹ã‚³ã‚¢è¡Œåˆ— `$S = QK^\top/\sqrt{d}$` ã®æœ‰åŠ¹ãƒ©ãƒ³ã‚¯ãŒä½ã„ã€ã¨ã„ã†ä»®èª¬ã€‚å®Ÿè¨¼çš„ã«ã¯ `$r = 128$`ï¼ˆ`$N = 2048$`ï¼‰ã§ç²¾åº¦ä½ä¸‹ã»ã¼ã‚¼ãƒ­ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚

### 6.5 è¡Œåˆ—å¾®åˆ†ã®ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ â€” é«˜æ¬¡å¾®åˆ†ã¨Hessian

2æ¬¡æœ€é©åŒ–ã¯SGDã®10å€ä»¥ä¸Šé€ŸãåæŸã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ãƒãƒƒã‚¯ã¯Hessianã®è¨ˆç®—ãƒ»ä¿å­˜ã‚³ã‚¹ãƒˆ `$O(n^2)$`ã€‚

**Hessian-vector productï¼ˆHVPï¼‰**:

```math
Hv = \nabla_\theta (\nabla_\theta \mathcal{L} \cdot v) = \lim_{\varepsilon \to 0} \frac{\nabla_\theta \mathcal{L}(\theta + \varepsilon v) - \nabla_\theta \mathcal{L}(\theta)}{\varepsilon}
```

ã“ã‚Œã¯Forward-over-Reverse ADã§1å›ã®forward + 1å›ã®reverseã§è¨ˆç®—å¯èƒ½ï¼ˆ`$O(n)$` ã§æ¸ˆã‚€ï¼‰ã€‚Hessianè¡Œåˆ—å…¨ä½“ `$H \in \mathbb{R}^{n \times n}$` ã‚’ä¿å­˜ã›ãšã€ä»»æ„ã®æ–¹å‘ `$v$` ã¨ã®ç©ã ã‘è¨ˆç®—ã™ã‚‹ã€‚Newtonæ³•ã¯ã“ã® `$Hv$` ã‚’ç·šå½¢ã‚·ã‚¹ãƒ†ãƒ  `$Hp = -\nabla \mathcal{L}$` ã®ã‚½ãƒ«ãƒãƒ¼ã§ä½¿ã†ã€‚

**Gauss-Newtonåˆ†è§£**: æå¤±ãŒ `$\mathcal{L} = \frac{1}{2}\|r(\theta)\|^2$`ï¼ˆæ®‹å·®ã®äºŒä¹—å’Œï¼‰ã®ã¨ã:

```math
H = J^\top J + \sum_i r_i \nabla^2 r_i \approx J^\top J \quad (\text{æ®‹å·®ãŒå°ã•ã‘ã‚Œã°})
```

`$J = \partial r / \partial \theta$` ãŒãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã€‚`$J^\top J$` ã¯åŠæ­£å®šå€¤ã§é€†è¡Œåˆ—ãŒå®‰å®šã€‚æ·±å±¤å­¦ç¿’ã§ã¯æ®‹å·®ã¯æå¤±å‹¾é…ã«å¯¾å¿œã—ã€`$J^\top J$` ãŒ Fisheræƒ…å ±è¡Œåˆ— `$F$` ã«å¯¾å¿œã™ã‚‹ã€‚

**è‡ªç„¶å‹¾é…ï¼ˆNatural Gradientï¼‰**: Fisheræƒ…å ±è¡Œåˆ— `$F$` ã‚’ä½¿ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®æ›²ç‡è£œæ­£:

```math
\Delta \theta = -\eta F^{-1} \nabla_\theta \mathcal{L}
```

`$F^{-1} \nabla \mathcal{L}$` ã¯ã€Œç¢ºç‡åˆ†å¸ƒç©ºé–“ã§ã®æœ€æ€¥é™ä¸‹æ–¹å‘ã€ã ã€‚ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰å‹¾é…ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®è¨ˆé‡ã‚’ç„¡è¦–ã™ã‚‹ãŸã‚ã€éåŠ¹ç‡ãªçµŒè·¯ã‚’ãŸã©ã‚Šã‚„ã™ã„ã€‚è‡ªç„¶å‹¾é…ã¯ã“ã‚Œã‚’è£œæ­£ã™ã‚‹ã€‚

**K-FACï¼ˆKronecker-factored Approximationï¼‰**:

Fisheræƒ…å ±è¡Œåˆ— `$F = \mathbb{E}[\nabla \mathcal{L} \nabla \mathcal{L}^\top]$` ã‚’ Kroneckerç©ã§è¿‘ä¼¼:

```math
F \approx A \otimes G, \quad A = \mathbb{E}[a a^\top],\; G = \mathbb{E}[\delta \delta^\top]
```

`$A$` ã¯å…¥åŠ›ã®2æ¬¡çµ±è¨ˆã€`$G$` ã¯å‹¾é…ã®2æ¬¡çµ±è¨ˆã€‚Kroneckerç©ã®ãŠã‹ã’ã§é€†è¡Œåˆ—ãŒ `$O(n)$` ã§è¨ˆç®—å¯èƒ½ã€‚é€†è¡Œåˆ—ã®åˆ†è§£: `$(A \otimes G)^{-1} = A^{-1} \otimes G^{-1}$`ã€‚`$A,G$` ãã‚Œãã‚Œã®é€†è¡Œåˆ—ã¯ `$O(d^3)$` ã ãŒã€ã“ã‚Œã¯å…ƒã® `$F^{-1}$` ã® `$O(n^2 d^2)$` ã‚ˆã‚Šæ¡é•ã„ã«å°ã•ã„ã€‚

**Hutchinsonæ¨å®šã«ã‚ˆã‚‹Hessianãƒˆãƒ¬ãƒ¼ã‚¹è¿‘ä¼¼**:

```math
\text{tr}(H) \approx \frac{1}{m} \sum_{j=1}^m z_j^\top H z_j, \quad z_j \sim \mathcal{N}(0, I)
```

ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ« `$z_j$` ã¨ã®HVPã ã‘ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’æ¨å®šã§ãã‚‹ã€‚`$m = 10\text{-}100$` ã§å®Ÿç”¨çš„ãªç²¾åº¦ãŒå‡ºã‚‹ã€‚ã“ã‚Œã‚’ä½¿ãˆã°ã€ŒHessianã®å¤§ããªå›ºæœ‰å€¤æˆåˆ†ãŒã„ãã¤ã‹å­˜åœ¨ã™ã‚‹ã‹ã€ï¼ˆsharp minima vs flat minimaï¼‰ãŒæ¨å®šã§ãã‚‹ã€‚

```mermaid
flowchart TD
  SGD["SGD: Î”W = -Î·âˆ‡L"] --> NG["è‡ªç„¶å‹¾é…: Î”W = -Î· F^-1 âˆ‡L"]
  NG --> KFAC["K-FAC: F â‰ˆ AâŠ—G"]
  KFAC --> INV["(AâŠ—G)^-1 = A^-1âŠ—G^-1"]
  INV --> FAST["åæŸ 10-100x é«˜é€ŸåŒ–"]
  NG --> HVP["HVP: Hv = âˆ‡(âˆ‡LÂ·v)"]
  HVP --> HESS["Hutchinson: tr(H) æ¨å®š"]
```

### 6.6 ç ”ç©¶è«–æ–‡ã®å®¶ç³»å›³

ç¬¬3å›ã§æ‰±ã£ãŸè«–æ–‡ç¾¤ã®ç³»è­œã‚’æ•´ç†ã™ã‚‹ã€‚æ•°å¼ã®ã€Œè¡€çµ±ã€ãŒè¦‹ãˆã‚‹ã¨ã€ãªãœä»Šã®æ‰‹æ³•ãŒç”Ÿã¾ã‚ŒãŸã‹ãŒåˆ†ã‹ã‚‹ã€‚

```mermaid
flowchart TD
  EY["Eckart-Young (1936)<br/>ä½ãƒ©ãƒ³ã‚¯æœ€é©è¿‘ä¼¼"]
  MP["Moore-Penrose (1950s)<br/>ç–‘ä¼¼é€†è¡Œåˆ—"]
  BP["Backprop (Rumelhart 1986)<br/>è¨ˆç®—ã‚°ãƒ©ãƒ•ä¸Šã®é€£é–å¾‹"]
  AD["AD Survey (Baydin 2018)<br/>Forward/Reverseçµ±ä¸€ç†è«–"]
  Lo["LoRA (Hu 2022)<br/>ä½ãƒ©ãƒ³ã‚¯Fine-tuning"]
  FA["FlashAttention (Dao 2022)<br/>IO-awareè¡Œåˆ—æ¼”ç®—"]
  RS["Randomized SVD (Halko 2011)<br/>å¤§è¦æ¨¡è¿‘ä¼¼"]
  AL["AdaLoRA (Zhang 2023)<br/>é©å¿œçš„ãƒ©ãƒ³ã‚¯å‰²ã‚Šå½“ã¦"]
  LN["LayerNorm (Ba 2016)<br/>æ­£è¦åŒ–å±¤ã®è¨­è¨ˆ"]
  TR["Transformer (Vaswani 2017)<br/>Self-Attention + LN"]

  EY --> MP
  EY --> RS
  MP --> Lo
  Lo --> AL
  RS --> AL
  BP --> AD
  LN --> TR
  AD --> TR
  TR --> FA
  Lo --> FA
```

**èª­ã¿æ–¹**: çŸ¢å°ã¯ã€Œæ•°å­¦çš„ãƒ»æ€æƒ³çš„ç¶™æ‰¿ã€ã‚’ç¤ºã™ã€‚LoRA ãŒ Moore-Penrose ã‚’ç¶™æ‰¿ã™ã‚‹ã®ã¯ã€Œæœ€å°ãƒãƒ«ãƒ è§£ = ä½ãƒ©ãƒ³ã‚¯è§£ã€ã®æ€æƒ³ã‹ã‚‰ã€‚FlashAttentionãŒTransformerã‚’ç¶™æ‰¿ã™ã‚‹ã®ã¯ã€ŒåŒã˜æ•°å¼ã€é•ã†è¨ˆç®—é †åºã€ã¨ã„ã†ç™ºæƒ³ã‹ã‚‰ã€‚

**æœªæ¥ã®æ¥ç¶š**: Hessianè¿‘ä¼¼ï¼ˆK-FACï¼‰ã¯Fisheræƒ…å ±è¡Œåˆ—çµŒç”±ã§LoRAã¨ç¹‹ãŒã‚‹ã€‚ã€Œä½ãƒ©ãƒ³ã‚¯ â‰ˆ æå¤±ã®æ›²ç‡ãŒä½ã„æ–¹å‘ã€ã¨ã„ã†è¦³ç‚¹ã‹ã‚‰ã€é©å¿œçš„ãƒ©ãƒ³ã‚¯é¸æŠã¨K-FACã¯åŒã˜å•é¡Œã‚’ç•°ãªã‚‹è§’åº¦ã§è§£ãã€‚

### Z6 ç†è§£åº¦ãƒã‚§ãƒƒã‚¯ â€” ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã®æŠŠæ¡

<details>
<summary>Q1: LoRAã¨AdaLoRAã®æœ¬è³ªçš„ãªé•ã„ã‚’1è¡Œã§èª¬æ˜ã›ã‚ˆã€‚</summary>

LoRAã¯å‡ä¸€ãƒ©ãƒ³ã‚¯ã®å­¦ç¿’å¯èƒ½è¡Œåˆ—ã‚’ä½¿ã†ãŒã€AdaLoRAã¯SVDãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã¨é‡è¦åº¦ã‚¹ã‚³ã‚¢ã§å„å±¤ã®ãƒ©ãƒ³ã‚¯ã‚’**å‹•çš„ã«**å‰²ã‚Šå½“ã¦ã‚‹ã€‚

</details>

<details>
<summary>Q2: Randomized SVD ãŒ `$O(mnk)$` ã§æ¸ˆã‚€ç†ç”±ã‚’è¿°ã¹ã‚ˆã€‚</summary>

ãƒ©ãƒ³ãƒ€ãƒ å°„å½± `$Y = A\Omega$`ï¼ˆ`$O(mnk)$`ï¼‰â†’ QRåˆ†è§£ï¼ˆ`$O(mk^2)$`ï¼‰â†’ å°è¡Œåˆ— `$B = Q^\top A$`ï¼ˆ`$O(mnk)$`ï¼‰â†’ å°è¡Œåˆ—ã®SVDï¼ˆ`$O(k^2 n)$`ï¼‰ã€‚å…¨è¡Œåˆ—SVDã® `$O(mn\min(m,n))$` ã«å¯¾ã—ã€`$k \ll \min(m,n)$` ãªã‚‰ `$k/\min(m,n)$` å€é«˜é€Ÿã€‚

</details>

<details>
<summary>Q3: FlashAttentionãŒ `$O(N^2)$` ã§ã¯ãªã `$O(N)$` ã®HBMã‚¢ã‚¯ã‚»ã‚¹ã§æ¸ˆã‚€ç†ç”±ã¯ï¼Ÿ</summary>

Attentionè¡Œåˆ— `$P \in \mathbb{R}^{N \times N}$` ã‚’HBMã«æ›¸ãå‡ºã•ãšã€SRAMã§tileã”ã¨ã«online softmaxã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€‚å„tileã®running max `$m_i$` ã¨running sum `$\ell_i$` ã‚’SRAMä¸Šã§æ›´æ–°ã—ç¶šã‘ã€æœ€çµ‚ãƒ‘ã‚¹ã®ã¿å‡ºåŠ› `$O_i$` ã‚’HBMã«æ›¸ãã€‚

</details>

> Progress: 95%

---

## ğŸ“ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆ10åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### ç¬¬3å›ã®å­¦ç¿’å†…å®¹ã¾ã¨ã‚

| ãƒˆãƒ”ãƒƒã‚¯ | ç†è«–ã®æ ¸å¿ƒ | å®Ÿè£…ã®æ ¸å¿ƒ |
|:---------|:---------|:---------|
| SVD | `$A = U\Sigma V^\top$`, Eckart-Youngå®šç† | `U[:,:k] @ (s[:k,None] * Vt[:k,:])` |
| Randomized SVD | ãƒ©ãƒ³ãƒ€ãƒ å°„å½± + QR + å°è¡Œåˆ—SVD | `$O(mnk)$` â€” å¤§è¦æ¨¡è¡Œåˆ—ã«ä¸å¯æ¬  |
| LoRA | `$\Delta W = BA$`, ä½ãƒ©ãƒ³ã‚¯ä»®èª¬ | `$B=0$`åˆæœŸåŒ–ã®ç†ç”± |
| einsum | æ·»å­—è¦å‰‡ 3 æ¡ | ãƒ‘ã‚¿ãƒ¼ãƒ³è¡¨ã®æš—è¨˜ã‚ˆã‚Šå°å‡º |
| è¡Œåˆ—å¾®åˆ† | `$\nabla_x f$` ã® shape = `$x$` ã® shape | ä¸­å¤®å·®åˆ†ã§ `$10^{-5}$` ä»¥ä¸‹æ¤œç®— |
| Forward AD | åŒå¯¾æ•° `$a + b\varepsilon$`, `$\varepsilon^2=0$` | `Dual(x, 1.0)` ã§ `$x$` ã®åå¾®åˆ† |
| Reverse AD | Wengert tape + VJP | PyTorchã® `backward()` ã®æ­£ä½“ |
| LayerNorm | å¹³å‡ãƒ»åˆ†æ•£æ­£è¦åŒ– â†’ æ¥å¹³é¢å°„å½±ã®é€†ä¼æ’­ | `$\hat{x} = (x-\mu)/\sqrt{\sigma^2+\varepsilon}$` |

### é“å…·ã®é€£æºå›³ â€” SVDÃ—è¡Œåˆ—å¾®åˆ†Ã—è‡ªå‹•å¾®åˆ†ã®äº¤ç‚¹

```mermaid
graph TD
    SVD["SVD: A = UÎ£Váµ€"] -->|"ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼"| LORA["LoRA: Î”W = BA"]
    SVD -->|"èª¤å·®ä¿è¨¼"| EY["Eckart-Youngå®šç†"]
    MATDIFF["è¡Œåˆ—å¾®åˆ†: âˆ‡f"] -->|"é€£é–å¾‹"| AD["è‡ªå‹•å¾®åˆ†"]
    AD -->|"VJP"| BP["Backprop"]
    AD -->|"JVP"| FAD["Forward AD (Dual)"]
    LORA -->|"ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã§å­¦ç¿’"| MATDIFF
    BP -->|"Wengert Tape"| MEM["ãƒ¡ãƒ¢ãƒªç®¡ç†"]
    EY -->|"æœ€é©ranké¸æŠ"| RSVD["Randomized SVD"]
    RSVD -->|"å¤§è¦æ¨¡Fine-tuning"| LORA
```

SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»è‡ªå‹•å¾®åˆ†ã¯ç‹¬ç«‹ã—ãŸé“å…·ã§ã¯ãªã„ã€‚SVDãŒã€Œä½ãƒ©ãƒ³ã‚¯æ§‹é€ ã®ç™ºè¦‹ã€ã‚’æ‹…ã„ã€è¡Œåˆ—å¾®åˆ†ãŒã€Œæœ€é©åŒ–ã®æ–¹å‘ã€ã‚’ä¸ãˆã€è‡ªå‹•å¾®åˆ†ãŒã€Œãã®æ–¹å‘ã‚’åŠ¹ç‡çš„ã«è¨ˆç®—ã€ã™ã‚‹ã€‚3ã¤ãŒæƒã£ã¦åˆã‚ã¦ã€LLMã®Fine-tuningãŒæˆç«‹ã™ã‚‹ã€‚

### æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨

| æ•°å¼ | NumPy/Python | shape |
|:-----|:-------------|:------|
| `$A = U\Sigma V^\top$` | `U, s, Vt = np.linalg.svd(A, full_matrices=False)` | `(m,r),(r,),(r,n)` |
| `$A_k$` | `U[:,:k] @ (s[:k,None] * Vt[:k,:])` | `(m,n)` |
| `$\|A - A_k\|_F^2 = \sum_{i>k}\sigma_i^2$` | `np.sum(s[k:]**2)` | scalar |
| `$A^\dagger$` | `np.linalg.pinv(A)` | `(n,m)` |
| `$C_{ij}=\sum_k A_{ik}B_{kj}$` | `np.einsum('ik,kj->ij', A, B)` | `(m,n)` |
| `$S_{bhqk} = \sum_d Q_{bhqd}K_{bhkd}/\sqrt{d}$` | `np.einsum('bhqd,bhkd->bhqk', Q, K) / sqrt(d)` | `(B,H,T,T)` |
| `$f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top A\mathbf{x}$` | `0.5 * x @ A @ x` | scalar |
| `$\nabla_x f$` | æ•°å€¤: ä¸­å¤®å·®åˆ† `(f(x+h*e_i) - f(x-h*e_i)) / (2h)` | `(d,)` |
| `$\hat{x} = (x-\mu)/\sqrt{\sigma^2+\varepsilon}$` | `(x - x.mean()) / sqrt(x.var() + eps)` | `(d,)` |
| ReLUå‹¾é… | `dz1 = dh1 * (z1 > 0)` | `(B,H)` |
| Softmax+CEå‹¾é… | `dz2 = (p - y) / B` | `(B,C)` |
| `$\partial\mathcal{L}/\partial W_2$` | `dW2 = dz2.T @ h1` | `(C,H)` |
| `$W \leftarrow W - \eta \nabla_W \mathcal{L}$` | `W -= lr * dW` | `(H,D)` |
| Dual Numberç© | `(a+bÎµ)(c+dÎµ) = ac + (ad+bc)Îµ` | â€” |



### FAQ

<details>
<summary>Q1: `np.linalg.svd` ã® `full_matrices=False` ã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã‹ï¼Ÿ</summary>

`full_matrices=True`ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰: `$U \in \mathbb{R}^{m \times m}$`, `$V^\top \in \mathbb{R}^{n \times n}$`ï¼ˆæ­£æ–¹è¡Œåˆ—ï¼‰ã€‚
`full_matrices=False`: `$U \in \mathbb{R}^{m \times r}$`, `$V^\top \in \mathbb{R}^{r \times n}$`ï¼ˆ`$r = \min(m,n)$`ã€çµŒæ¸ˆçš„SVDï¼‰ã€‚

ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®ã¨ãã¯ `False` ãŒåŠ¹ç‡çš„ã€‚`full_matrices=True` ã¯ç›´äº¤å®Œå…¨æ€§ãŒå¿…è¦ãªå ´åˆï¼ˆä¾‹: QRåˆ†è§£ã¨ã®çµ„ã¿åˆã‚ã›ï¼‰ã§ä½¿ã†ã€‚

</details>

<details>
<summary>Q2: SVDã®ç‰¹ç•°å€¤ã¯ä¸€æ„ã ãŒã€U ã¨ V ã¯ä¸€æ„ã§ãªã„ã®ã¯ãªãœã‹ï¼Ÿ</summary>

ç‰¹ç•°å€¤ `$\sigma_i$` ã¯ `$A^\top A$` ã®å›ºæœ‰å€¤ã®å¹³æ–¹æ ¹ãªã®ã§ä¸€æ„ã€‚ã—ã‹ã—å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã¯å›ºæœ‰ç©ºé–“ãŒ1æ¬¡å…ƒã§ãªã„é™ã‚Šä¸€æ„ã§ã¯ãªã„ï¼ˆç¬¦å·åè»¢ãƒ»å›è»¢ã®è‡ªç”±åº¦ï¼‰ã€‚å¤šé‡ç‰¹ç•°å€¤ãŒã‚ã‚‹ã¨ãã¯ç‰¹ã«æ³¨æ„ã€‚å®Ÿè£…ã§ã¯ `U` ã‚„ `V` ã®çµ¶å¯¾å€¤ã‚„å†æ§‹æˆèª¤å·®ã§æ¯”è¼ƒã™ã‚‹ã€‚

</details>

<details>
<summary>Q3: LoRAã§ `$B=0$`ã€`$A=\text{Kaiming normal}$` ã¨åˆæœŸåŒ–ã™ã‚‹ç†ç”±ã¯ï¼Ÿ</summary>

Fine-tuningé–‹å§‹æ™‚ã« `$\Delta W = BA = 0$` ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã€‚`$A=0$` ã«ã™ã‚‹ã¨ `$\partial \mathcal{L}/\partial A = B^\top (\cdots) = 0$` ã¨ãªã‚Š `$A$` ãŒå­¦ç¿’ã—ãªã„ã€‚`$B=0$` ã«ã™ã‚‹ã¨æœ€åˆã® backward ã§ `$\partial \mathcal{L}/\partial B \neq 0$`ï¼ˆ`$A$` ã¯éã‚¼ãƒ­ï¼‰ãªã®ã§ã€ä»¥é™ã¯ä¸¡æ–¹ãŒå­¦ç¿’ã™ã‚‹ã€‚

</details>

<details>
<summary>Q4: Tikhonovæ­£å‰‡åŒ–ã® `$\lambda$` ã‚’ã©ã†é¸ã¶ã‹ï¼Ÿ</summary>

```math
\mathbf{x}^* = V \operatorname{diag}\!\left(\frac{\sigma_i}{\sigma_i^2+\lambda}\right) U^\top \mathbf{b}
```

`$\lambda \to \infty$`: å…¨ãƒ•ã‚£ãƒ«ã‚¿ä¿‚æ•° `$\to 0$` â†’ `$\mathbf{x}^* \to \mathbf{0}$`ï¼ˆéå‰°æ­£å‰‡åŒ–ï¼‰ã€‚
`$\lambda \to 0$`: ç–‘ä¼¼é€†è¡Œåˆ—è§£ï¼ˆæœ€å°ãƒãƒ«ãƒ æœ€å°äºŒä¹—è§£ï¼‰ã«åæŸã€‚
æœ€é© `$\lambda$` ã¯Læ›²ç·šæ³•ï¼ˆ`$\|\mathbf{x}^*\|$` vs `$\|A\mathbf{x}^* - \mathbf{b}\|$` ã®ãƒ—ãƒ­ãƒƒãƒˆï¼‰ã‚„ç•™ä¸€äº¤å·®æ¤œè¨¼ã§é¸ã¶ã€‚

</details>

<details>
<summary>Q5: ãªãœæ•°å€¤å¾®åˆ†ã¯ grad check ã«ã—ã‹ä½¿ãˆãªã„ã®ã‹ï¼Ÿ</summary>

ä¸­å¤®å·®åˆ†ã®èª¤å·®ã¯ `$O(h^2)$`ã€‚`$h \approx 10^{-5}$` ã‚’ä½¿ã†ã¨æ•°å€¤èª¤å·® `$\sim 10^{-10}$` ã§1åå¾®åˆ†ãŒè¨ˆç®—ã§ãã‚‹ãŒã€`$n$` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã— `$O(n)$` å› forward pass ãŒå¿…è¦ã€‚LLMã§ã¯ `$n = 10^9$` â†’ `$10^9$` å›ã® forward pass = ä¸å¯èƒ½ã€‚grad check ã¯ `$n \leq 10^4$` ã®å°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã®ã¿å®Ÿç”¨çš„ã€‚

</details>

<details>
<summary>Q6: Dual Numbers ã‚’ä½¿ã£ãŸ Forward Mode AD ã¨ PyTorch ã® autograd ã®é–¢ä¿‚ã¯ï¼Ÿ</summary>

PyTorchã® `autograd` ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ **Reverse Mode AD** ã‚’å®Ÿè£…ã™ã‚‹ã€‚`torch.autograd.functional.jvp` ãŒ Forward Modeï¼ˆJacobian-vector productï¼‰ã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ã€‚

Dual Numbers ã¯æ‰‹å®Ÿè£…ç‰ˆ Forward Mode AD ã®ç†è«–çš„åŸºç›¤ã€‚PyTorchã® forward-mode AD ã¯å†…éƒ¨ã§ Tangentï¼ˆ`$\varepsilon$` ä¿‚æ•°ï¼‰ã‚’è¿½è·¡ã™ã‚‹åŒæ§˜ã®ä»•çµ„ã¿ã‚’æŒã¤ã€‚

`torch.autograd.functional.jvp(f, x, v)` ã¯ `$Jv$` ã‚’1ãƒ‘ã‚¹ã§è¨ˆç®—ã™ã‚‹ã€‚`$v$` = æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆDual Numberã® `$\varepsilon$` æˆåˆ†ï¼‰ã«ç›¸å½“ã€‚

</details>

<details>
<summary>Q7: einsum ã® `optimize=True` ã¯å¸¸ã«ä½¿ã†ã¹ãã‹ï¼Ÿ</summary>

2é …ç¸®ç´„ï¼ˆ`'ik,kj->ij'`ï¼‰ã§ã¯é †åºãŒä¸€æ„ãªã®ã§åŠ¹æœãªã—ã€‚**3é …ä»¥ä¸Š**ã®ç¸®ç´„ã§ã®ã¿æ©æµãŒã‚ã‚‹ã€‚

`np.einsum('ijk,jkl,klm->im', A, B, C, optimize=True)` ã®ã‚ˆã†ãªå¼ã§ã¯ã€ç¸®ç´„é †åºã®æ¢ç´¢ã‚³ã‚¹ãƒˆã‚ˆã‚Šè¨ˆç®—å‰Šæ¸›é‡ã®æ–¹ãŒåœ§å€’çš„ã«å¤§ãã„ã€‚

ãŸã ã—ã€ãƒ«ãƒ¼ãƒ—å†…ã§ç¹°ã‚Šè¿”ã™å ´åˆã¯ `np.einsum_path` ã§ãƒ—ãƒ©ãƒ³ã‚’äº‹å‰è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã€‚

</details>

<details>
<summary>Q9: Randomized SVD ã®èª¤å·®ä¿è¨¼ã¯ã©ã‚Œãã‚‰ã„å¼·ã„ã‹ï¼Ÿ</summary>

Halko et al. (2011) ã®å®šç†: ãƒ©ãƒ³ãƒ€ãƒ è¡Œåˆ— `$\Omega \in \mathbb{R}^{n \times (k+p)}$` ã‚’ä½¿ã£ã¦ `$Y = A\Omega$`ã€`$Q$` ã‚’ãã®æ­£è¦ç›´äº¤åŸºåº•ã¨ã—ã¦ `$\hat{A} = QQ^\top A$` ã‚’ä½œã‚‹ã¨:

```math
\mathbb{E}\|A - \hat{A}\|_2 \leq \left(1 + \sqrt{\frac{k}{p-1}}\right)\sigma_{k+1} + \frac{e\sqrt{k+p}}{p} \left(\sum_{j>k}\sigma_j^2\right)^{1/2}
```

`$p$` ã¯ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸ `$p=5\text{-}10$`ï¼‰ã€‚æœŸå¾…å€¤ãŒæœ€è‰¯ãƒ©ãƒ³ã‚¯-`$k$` è¿‘ä¼¼èª¤å·® `$\sigma_{k+1}$` ã«è¿‘ã„ã€‚`$p=10$` ã§ã»ã¼ç¢ºå®šçš„ãªä¿è¨¼ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

</details>

<details>
<summary>Q8: LayerNorm ã®é€†ä¼æ’­ã§ã€Œå¹³å‡æˆåˆ†ã¨åˆ†æ•£æ–¹å‘æˆåˆ†ã‚’å·®ã—å¼•ãã€ã®ã¯ãªãœã‹ï¼Ÿ</summary>

LayerNorm ã® forward ã§ã¯å‡ºåŠ› `$\hat{x}$` ãŒå¸¸ã«å¹³å‡0ãƒ»åˆ†æ•£1ã«åˆ¶ç´„ã•ã‚Œã‚‹ã€‚ã“ã®åˆ¶ç´„ã¯ã€Œ`$\hat{x}$` ã¯å¹³å‡æ–¹å‘ã¨åˆ†æ•£æ–¹å‘ã¸ã®å¤‰åŒ–ã‚’æŒã¦ãªã„ã€ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

é€†ä¼æ’­ã‚‚ãã®åˆ¶ç´„ã«å¾“ã†å¿…è¦ãŒã‚ã‚Šã€å‹¾é…ã‹ã‚‰ã€Œå¹³å‡æ–¹å‘ã€`$\frac{1}{d}\sum_k \delta_k$` ã¨ã€Œåˆ†æ•£æ–¹å‘ã€`$\hat{x}_j \frac{1}{d}\sum_k \delta_k \hat{x}_k$` ã‚’å·®ã—å¼•ã„ã¦å°„å½±ã™ã‚‹ã€‚ã“ã‚Œã¯ã¾ã•ã«ã€Œåˆ¶ç´„ä»˜ãæœ€é©åŒ–ã«ãŠã‘ã‚‹å°„å½±å‹¾é…ã€ã®æ§‹é€ ã ã€‚

</details>

### æ¬¡å›äºˆå‘Š â€” ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦

ç¬¬3å›ã§å­¦ã‚“ã è¡Œåˆ—å¾®åˆ†ãƒ»è‡ªå‹•å¾®åˆ†ã¯ã€ã€Œå‹¾é…ã‚’ã©ã†è¨ˆç®—ã™ã‚‹ã‹ã€ã®å•é¡Œã‚’è§£æ±ºã—ãŸã€‚

æ¬¡å›ã¯ã€Œç¢ºç‡ã‚’ã©ã†æ‰±ã†ã‹ã€ã ã€‚

- ç¢ºç‡åˆ†å¸ƒã®è¨˜è¿°ã¨æ“ä½œ
- æœŸå¾…å€¤ãƒ»åˆ†æ•£ãƒ»å…±åˆ†æ•£
- æœ€å°¤æ¨å®šã¨ãƒ™ã‚¤ã‚ºæ¨å®š
- KL divergenceã®å°å‡ºã¨æƒ…å ±ç†è«–

è¡Œåˆ—å¾®åˆ†ãªã—ã«æœ€å°¤æ¨å®šã¯æ›¸ã‘ãªã„ã€‚ä»Šå›ã®é“å…·ãŒç›´æ¥ç¹‹ãŒã‚‹ã€‚

> Progress: 100%

---

## ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ› â€” ã€Œãƒ†ãƒ³ã‚½ãƒ«ç¸®ç´„ã¯æƒ…å ±ã®è¨€èªã€

> å¾®åˆ†ã¯ãªãœã€Œè¨ˆç®—ã‚°ãƒ©ãƒ•ã®é€†èµ°ã€ãªã®ã‹ï¼Ÿ

å¤å…¸æ•°å­¦ã§ã¯å¾®åˆ†ã¯ã€Œæ¥µé™ã€ã¨ã—ã¦å®šç¾©ã•ã‚Œã‚‹ã€‚ã—ã‹ã—å®Ÿè£…ã®ä¸–ç•Œã§ã¯ã€å¾®åˆ†ã¯ã€Œè¨ˆç®—ã®è¨˜éŒ²ã‚’é€†ã«èª­ã‚€ã€æ“ä½œã ã€‚

Wengert list ã‚’ä½¿ã£ãŸ Reverse Mode AD ã¯ã€æ•°å­¦çš„ã«ã¯ã€Œå±€æ‰€åå¾®åˆ†ã®é€£é–ç©ã®é€†é †è¨ˆç®—ã€ã ãŒã€è¨ˆç®—è«–çš„ã«ã¯ã€Œãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’å·»ãæˆ»ã™ã€ã ã€‚ã“ã®äºŒã¤ã®ç­‰ä¾¡æ€§ãŒã€æ·±å±¤å­¦ç¿’ã®ç†è«–ã¨å®Ÿè£…ã‚’çµã¶æ©‹æ¢ã ã€‚

åŒæ§˜ã«ã€einsum ã¯ã€Œãƒ†ãƒ³ã‚½ãƒ«ç¸®ç´„ã®è¨€èªã€ã§ã¯ãªãã€Œæƒ…å ±ã®æµã‚Œã®è¨€èªã€ã ã€‚`'bhqd,bhkd->bhqk'` ã¯å˜ãªã‚‹è¨ˆç®—å¼ã§ã¯ãªãã€ã€Œå„ã‚¯ã‚¨ãƒªãŒå…¨ã¦ã®ã‚­ãƒ¼ã¨é–¢ä¿‚ã‚’æŒã¡ã€é–¢ä¿‚ã®å¼·ã•ã‚’ `$d$` æ¬¡å…ƒã§æ¸¬ã‚‹ã€ã¨ã„ã†æ„å‘³ã®å®£è¨€ã ã€‚

SVDã‚‚ã¾ãŸã€Œè¡Œåˆ—ã®è¨€èªã€ã ã€‚ã©ã‚“ãªå¤‰æ›ã‚‚ã€Œå›è»¢ â†’ è»¸æ–¹å‘ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° â†’ å›è»¢ã€ã«åˆ†è§£ã§ãã‚‹ã€ã¨ã„ã†ä¸»å¼µã€‚ãã®è¨€èªã§æ›¸ã‘ã°ã€LoRAã‚‚Randomized SVDã‚‚åŒã˜æ–‡æ³•ã§æ›¸ã‘ã‚‹ã€‚

**å•ã„**: ã€Œæƒ…å ±ã‚’å¤±ã‚ãªã„æœ€å°ã®è¡¨ç¾ã€ã¨ã¯ä½•ã‹ï¼Ÿ

<details>
<summary>æ­´å²çš„èƒŒæ™¯ â€” ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®å†ç™ºè¦‹</summary>

Eckart-Youngå®šç†ï¼ˆ1936å¹´ï¼‰ã¯è¡Œåˆ—è«–ã®çµæœã ã£ãŸãŒã€ã€Œæœ€è‰¯ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ï¼SVDã§åˆ‡ã‚Šæ¨ã¦ã€ã¨ã„ã†äº‹å®ŸãŒæ©Ÿæ¢°å­¦ç¿’ã«åºƒãå¿œç”¨ã•ã‚ŒãŸã®ã¯1990å¹´ä»£ä»¥é™ã ã€‚

LSAï¼ˆLatent Semantic Analysisï¼‰ã€PCAã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®SVDã€ãã—ã¦LoRAâ€”â€”å…¨ã¦åŒã˜æ•°å­¦çš„åŸç†ã®å¿œç”¨ã ã€‚

LoRAã®ç™»å ´ï¼ˆ2022å¹´ï¼‰ã¯ã€ã€ŒLLMã® fine-tuning ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«ã‚ã‚‹ã€ã¨ã„ã†è¦³å¯Ÿã‚’å®Ÿç”¨åŒ–ã—ãŸã€‚Fine-tuningã®ã€Œæœ¬è³ªçš„ãªè‡ªç”±åº¦ã€ãŒé©šãã»ã©å°ã•ã„ã¨ã„ã†ç™ºè¦‹ã¯ã€æ·±å±¤å­¦ç¿’ã®ã€Œå†…åœ¨æ¬¡å…ƒã€ã¸ã®ç†è§£ã‚’ä¸€æ®µæ·±ã‚ãŸã€‚

</details>

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

[^1]: Halko, N., Martinsson, P.-G., & Tropp, J. A. (2011). Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. *SIAM Review*, 53(2), 217â€“288. [arXiv:0909.4061](https://arxiv.org/abs/0909.4061)

[^2]: Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. *ICLR 2022*. [arXiv:2106.09685](https://arxiv.org/abs/2106.09685)

[^3]: Aghajanyan, A., Zettlemoyer, L., & Gupta, S. (2021). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. *ACL 2021*. [arXiv:2012.13255](https://arxiv.org/abs/2012.13255)

[^4]: Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & RÃ©, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. *NeurIPS 2022*. [arXiv:2205.14135](https://arxiv.org/abs/2205.14135)

[^5]: Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., & Zhao, T. (2023). AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. *ICLR 2023*. [arXiv:2303.10512](https://arxiv.org/abs/2303.10512)

[^6]: Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2018). Automatic differentiation in machine learning: a survey. *Journal of Machine Learning Research*, 18(153), 1â€“43. [arXiv:1502.05767](https://arxiv.org/abs/1502.05767)

[^7]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). Attention Is All You Need. *NeurIPS 2017*. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

[^8]: Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer Normalization. [arXiv:1607.06450](https://arxiv.org/abs/1607.06450)

---

> **å‰ç·¨ã¸ã®ãƒªãƒ³ã‚¯**: [ç¬¬3å› Part1ï¼ˆç†è«–ç·¨ï¼‰](/articles/ml-lecture-03-part1)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**

- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:

- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
