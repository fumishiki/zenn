---
title: "ç¬¬27å›: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å‰ç·¨ã€‘ç†è«–ç·¨""
slug: "ml-lecture-27-part1"
emoji: "ğŸ“Š"
type: "tech"
topics: ["machinelearning", "evaluation", "julia", "rust", "statistics"]
published: true
---

# ç¬¬27å›: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ â€” æ•°å€¤ãŒæ”¹å–„ã™ã‚Œã°"è‰¯ã„"ãƒ¢ãƒ‡ãƒ«ã‹ï¼Ÿ

> **ç¬¬26å›ã§æ¨è«–ã‚’é«˜é€ŸåŒ–ã—Productionå“è³ªã‚’ç¢ºä¿ã—ãŸã€‚ã ãŒ"è‰¯ã„"ãƒ¢ãƒ‡ãƒ«ã¨ã¯ä½•ã‹ï¼Ÿå®šé‡è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚**

ã€ŒFIDãŒ3.2ã‹ã‚‰2.8ã«æ”¹å–„ã—ãŸï¼ã€â€” å¬‰ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã ã€‚ã ãŒã€ãã‚Œã¯æœ¬å½“ã«"è‰¯ã„"ã®ã‹ï¼Ÿäººé–“ã®ç›®ã«ã¯ã©ã†è¦‹ãˆã‚‹ã®ã‹ï¼Ÿå“è³ªã¨å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã¯ï¼Ÿã‚µãƒ³ãƒ—ãƒ«æ•°ã¯ååˆ†ã‹ï¼Ÿçµ±è¨ˆçš„ã«æœ‰æ„ã‹ï¼Ÿ

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¯æ•°å€¤ã ã‘ã§ã¯å®Œçµã—ãªã„ã€‚FID [^1], IS [^2], LPIPS [^3], Precision-Recall [^4], CMMD [^5] â€” å„æŒ‡æ¨™ã¯ç•°ãªã‚‹å´é¢ã‚’æ¸¬å®šã—ã€äº’ã„ã‚’è£œå®Œã™ã‚‹ã€‚2024å¹´ã€FIDã®é™ç•ŒãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€CMMD [^5] ã‚„FLD+ [^7] ãŒç™»å ´ã—ãŸã€‚

æœ¬è¬›ç¾©ã§ã¯ã€**æ•°å¼å®Œå…¨å°å‡ºâ†’å®Ÿè£…â†’çµ±è¨ˆæ¤œå®šçµ±åˆâ†’è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚è©•ä¾¡æŒ‡æ¨™ã®ç†è«–çš„åŸºç›¤ã‚’ç†è§£ã—ã€Productionç’°å¢ƒã§ä½¿ãˆã‚‹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºï¼ˆå…¨5ã‚³ãƒ¼ã‚¹ï¼‰ã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚

**Course III: å®Ÿè·µãƒ»æ©‹æ¸¡ã—ç·¨ï¼ˆç¬¬19-32å›ï¼‰**: æœ¬è¬›ç¾©ã¯ç¬¬27å› â€” è©•ä¾¡ã®ç†è«–ã¨å®Ÿè£…ã€‚ç¬¬24å›ã®çµ±è¨ˆå­¦ã‚’è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«çµ±åˆã—ã€ç¬¬32å›ã®ç·åˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸æ¥ç¶šã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ–¼ï¸ ç”Ÿæˆç”»åƒ"] --> B["ğŸ“ è·é›¢è¨ˆç®—"]
    A2["ğŸ“¸ çœŸç”»åƒ"] --> B
    B --> C1["FID<br/>åˆ†å¸ƒè·é›¢"]
    B --> C2["IS<br/>å“è³ª+å¤šæ§˜æ€§"]
    B --> C3["LPIPS<br/>çŸ¥è¦šè·é›¢"]
    B --> C4["P&R<br/>å“è³ªvså¤šæ§˜æ€§"]
    B --> C5["CMMD<br/>CLIPåŸ‹ã‚è¾¼ã¿"]
    C1 & C2 & C3 & C4 & C5 --> D["ğŸ“Š çµ±è¨ˆæ¤œå®š"]
    D --> E["âœ… ç·åˆè©•ä¾¡"]
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style E fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 7 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” FIDã‚’3è¡Œã§è¨ˆç®—

**ã‚´ãƒ¼ãƒ«**: FrÃ©chet Inception Distance (FID) ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

FIDã¯2ã¤ã®ç”»åƒã‚»ãƒƒãƒˆé–“ã®åˆ†å¸ƒè·é›¢ã‚’æ¸¬å®šã™ã‚‹ã€‚çœŸç”»åƒã¨ç”Ÿæˆç”»åƒã®ç‰¹å¾´é‡ï¼ˆInceptionç‰¹å¾´ï¼‰ã‚’æŠ½å‡ºã—ã€ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¨ã—ã¦è¿‘ä¼¼ã—ã€ãƒ•ãƒ¬ã‚·ã‚§è·é›¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚

```julia
using LinearAlgebra, Statistics

# Simplified FID: FrÃ©chet distance between two Gaussians
# Real images: Î¼_r, Î£_r (mean, covariance of Inception features)
# Generated images: Î¼_g, Î£_g
function fid_simplified(Î¼_r::Vector{Float64}, Î£_r::Matrix{Float64},
                         Î¼_g::Vector{Float64}, Î£_g::Matrix{Float64})
    # FID = ||Î¼_r - Î¼_g||Â² + Tr(Î£_r + Î£_g - 2(Î£_r Î£_g)^{1/2})
    mean_diff = sum((Î¼_r .- Î¼_g).^2)

    # Matrix square root: (Î£_r Î£_g)^{1/2}
    # Use eigen decomposition: A = V Î› V^T â†’ A^{1/2} = V Î›^{1/2} V^T
    product = Î£_r * Î£_g
    eigen_decomp = eigen(product)
    sqrt_product = eigen_decomp.vectors * Diagonal(sqrt.(abs.(eigen_decomp.values))) * eigen_decomp.vectors'

    trace_term = tr(Î£_r) + tr(Î£_g) - 2*tr(sqrt_product)

    return mean_diff + trace_term
end

# Test: 4-dim features, simulated real/generated distributions
Î¼_real = [0.5, 0.3, 0.7, 0.2]
Î£_real = [1.0 0.1 0.05 0.0; 0.1 0.8 0.0 0.05; 0.05 0.0 0.9 0.1; 0.0 0.05 0.1 1.1]
Î¼_gen = [0.52, 0.28, 0.72, 0.19]  # slightly different
Î£_gen = [0.95 0.12 0.04 0.0; 0.12 0.85 0.0 0.06; 0.04 0.0 0.88 0.09; 0.0 0.06 0.09 1.08]

fid_score = fid_simplified(Î¼_real, Î£_real, Î¼_gen, Î£_gen)
println("FID score: $(round(fid_score, digits=4))")
println("Lower is better â€” 0.0 = identical distributions")
```

å‡ºåŠ›:
```
FID score: 0.0523
Lower is better â€” 0.0 = identical distributions
```

**3è¡Œã§FIDã®æ ¸å¿ƒã‚’å‹•ã‹ã—ãŸã€‚** å®Ÿéš›ã®FIDã¯:
1. Inception-v3ã§ç‰¹å¾´æŠ½å‡ºï¼ˆ2048æ¬¡å…ƒï¼‰
2. 2ã¤ã®ç”»åƒã‚»ãƒƒãƒˆã‹ã‚‰ $\mu, \Sigma$ ã‚’è¨ˆç®—
3. ãƒ•ãƒ¬ã‚·ã‚§è·é›¢ = $\|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})$

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å­¦:

$$
\begin{aligned}
&\text{FID}(\mathcal{N}(\mu_r, \Sigma_r), \mathcal{N}(\mu_g, \Sigma_g)) \\
&= \|\mu_r - \mu_g\|_2^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)
\end{aligned}
$$

- ç¬¬1é … $\|\mu_r - \mu_g\|^2$: å¹³å‡ã®ãšã‚Œï¼ˆåˆ†å¸ƒã®ä¸­å¿ƒãŒåˆã£ã¦ã„ã‚‹ã‹ï¼Ÿï¼‰
- ç¬¬2é … $\text{Tr}(\Sigma_r + \Sigma_g - 2\sqrt{\Sigma_r \Sigma_g})$: å…±åˆ†æ•£ã®ãšã‚Œï¼ˆåˆ†å¸ƒã®åºƒãŒã‚Šæ–¹ãŒä¼¼ã¦ã„ã‚‹ã‹ï¼Ÿï¼‰

FIDãŒå°ã•ã„ã»ã©ã€ç”Ÿæˆç”»åƒã®åˆ†å¸ƒãŒçœŸç”»åƒã«è¿‘ã„ã€‚ã ãŒã€**FIDã ã‘ã§åˆ¤æ–­ã—ã¦ã¯ã„ã‘ãªã„ç†ç”±**ãŒã‚ã‚‹ï¼ˆâ†’ Zone 3ã§å®Œå…¨è§£èª¬ï¼‰ã€‚

:::message
**é€²æ—: 3% å®Œäº†** FIDã®è¨ˆç®—å¼ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ä»–ã®5ã¤ã®æŒ‡æ¨™ï¼ˆIS/LPIPS/P&R/CMMD/MMDï¼‰ã‚’è§¦ã‚Šã€æ•°å¼ã‚’å®Œå…¨å°å‡ºã—ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” 5ã¤ã®è©•ä¾¡æŒ‡æ¨™ã‚’è§¦ã‚‹

### 1.1 è©•ä¾¡æŒ‡æ¨™ã®å…¨ä½“åƒ

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™ã¯ã€**æ¸¬å®šå¯¾è±¡**ã¨**ä¾å­˜ã™ã‚‹ä»®å®š**ã«ã‚ˆã£ã¦åˆ†é¡ã§ãã‚‹ã€‚

| æŒ‡æ¨™ | æ¸¬å®šå¯¾è±¡ | ä¾å­˜ã™ã‚‹ã‚‚ã® | ä»®å®š | é•·æ‰€ | çŸ­æ‰€ |
|:-----|:---------|:------------|:-----|:-----|:-----|
| **FID** [^1] | åˆ†å¸ƒè·é›¢ | Inception-v3 | ã‚¬ã‚¦ã‚¹æ€§ | æ¨™æº–åŒ–ã•ã‚Œã¦ã„ã‚‹ | æ­£è¦æ€§ä»®å®šã€ImageNetãƒã‚¤ã‚¢ã‚¹ |
| **IS** [^2] | å“è³ª+å¤šæ§˜æ€§ | Inception-v3 | ImageNetåˆ†é¡ | å˜ä¸€ã‚¹ã‚³ã‚¢ | KLç™ºæ•£ã®è§£é‡ˆå›°é›£ã€ImageNetãƒã‚¤ã‚¢ã‚¹ |
| **LPIPS** [^3] | çŸ¥è¦šè·é›¢ | VGG/AlexNet | æ·±å±¤ç‰¹å¾´ | äººé–“ã®çŸ¥è¦šã¨ç›¸é–¢é«˜ã„ | ãƒšã‚¢å˜ä½ã€åˆ†å¸ƒãƒ¬ãƒ™ãƒ«è©•ä¾¡ä¸å¯ |
| **Precision-Recall** [^4] | å“è³ªvså¤šæ§˜æ€§ | ç‰¹å¾´æŠ½å‡ºå™¨ | å¤šæ§˜ä½“è¿‘ä¼¼ | å“è³ªã¨å¤šæ§˜æ€§ã‚’åˆ†é›¢ | è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ |
| **CMMD** [^5] | åˆ†å¸ƒè·é›¢ | CLIP | ä»®å®šãªã—ï¼ˆMMDï¼‰ | æ­£è¦æ€§ä¸è¦ã€ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œ | CLIPä¾å­˜ |
| **MMD** [^6] | åˆ†å¸ƒè·é›¢ | ã‚«ãƒ¼ãƒãƒ« | RKHSã§ã®è·é›¢ | ä»®å®šãªã— | ã‚«ãƒ¼ãƒãƒ«é¸æŠã«ä¾å­˜ |

#### 1.1.1 FID (FrÃ©chet Inception Distance)

```julia
# FID: Inceptionç‰¹å¾´ â†’ ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ â†’ ãƒ•ãƒ¬ã‚·ã‚§è·é›¢
function inception_features_dummy(images::Vector{Matrix{Float64}})
    # Real impl: Inception-v3 pre-pool layer (2048-dim)
    # Here: random projection to 64-dim for demo
    n_samples = length(images)
    d_features = 64
    return randn(n_samples, d_features)  # (n_samples, 64)
end

function compute_fid(real_images::Vector{Matrix{Float64}},
                      gen_images::Vector{Matrix{Float64}})
    # Extract features
    feats_r = inception_features_dummy(real_images)
    feats_g = inception_features_dummy(gen_images)

    # Compute Î¼, Î£
    Î¼_r = vec(mean(feats_r, dims=1))
    Î¼_g = vec(mean(feats_g, dims=1))
    Î£_r = cov(feats_r)
    Î£_g = cov(feats_g)

    # FrÃ©chet distance
    mean_diff = sum((Î¼_r .- Î¼_g).^2)
    product = Î£_r * Î£_g
    eig_decomp = eigen(product)
    sqrt_product = eig_decomp.vectors * Diagonal(sqrt.(abs.(eig_decomp.values))) * eig_decomp.vectors'
    trace_term = tr(Î£_r) + tr(Î£_g) - 2*tr(sqrt_product)

    return mean_diff + trace_term
end

# Test
real_imgs = [randn(32, 32) for _ in 1:50]  # 50 images
gen_imgs = [randn(32, 32) for _ in 1:50]
fid = compute_fid(real_imgs, gen_imgs)
println("FID: $(round(fid, digits=2))")
```

**è§£é‡ˆ**: FID â‰ˆ 0 ãªã‚‰åˆ†å¸ƒãŒä¸€è‡´ã€‚å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§ã¯ FID < 10 ãŒé«˜å“è³ªã€FID > 50 ã¯ä½å“è³ªã¨ã•ã‚Œã‚‹ï¼ˆImageNetåŸºæº–ï¼‰ã€‚

#### 1.1.2 IS (Inception Score)

```julia
# IS: Inceptionåˆ†é¡ â†’ KL divergence
function inception_classify_dummy(images::Vector{Matrix{Float64}})
    # Real impl: Inception-v3 â†’ softmax over 1000 ImageNet classes
    # Here: 10 classes for demo
    n_samples = length(images)
    n_classes = 10
    # Random softmax probs
    logits = randn(n_samples, n_classes)
    return exp.(logits) ./ sum(exp.(logits), dims=2)  # (n_samples, 10)
end

function inception_score(images::Vector{Matrix{Float64}})
    # p(y|x) for each image
    p_yx = inception_classify_dummy(images)  # (n, k)

    # p(y) = E_x[p(y|x)] = marginal over dataset
    p_y = vec(mean(p_yx, dims=1))  # (k,)

    # IS = exp(E_x[KL(p(y|x) || p(y))])
    # KL(p||q) = Î£ p log(p/q)
    kl_divs = zeros(size(p_yx, 1))
    for i in 1:size(p_yx, 1)
        for j in 1:length(p_y)
            if p_yx[i,j] > 0 && p_y[j] > 0
                kl_divs[i] += p_yx[i,j] * log(p_yx[i,j] / p_y[j])
            end
        end
    end

    mean_kl = mean(kl_divs)
    return exp(mean_kl)
end

is_score = inception_score(gen_imgs)
println("Inception Score: $(round(is_score, digits=2))")
println("Range: [1.0, n_classes]. Higher = better quality + diversity")
```

**è§£é‡ˆ**: IS âˆˆ [1, 1000]ï¼ˆImageNet 1000ã‚¯ãƒ©ã‚¹ã®å ´åˆï¼‰ã€‚IS > 30 ãŒé«˜å“è³ªï¼ˆCIFAR-10ã§ã¯ IS > 8ï¼‰ã€‚

#### 1.1.3 LPIPS (Learned Perceptual Image Patch Similarity)

```julia
# LPIPS: VGGç‰¹å¾´ â†’ L2è·é›¢
function vgg_features_dummy(image::Matrix{Float64})
    # Real impl: VGG-16 layers â†’ multiple scales
    # Here: 3 scales Ã— 32-dim = 96-dim
    return randn(96)
end

function lpips_distance(img1::Matrix{Float64}, img2::Matrix{Float64})
    # Extract features
    feat1 = vgg_features_dummy(img1)
    feat2 = vgg_features_dummy(img2)

    # L2 distance in feature space
    return sqrt(sum((feat1 .- feat2).^2))
end

# Test: compare 2 images
img_a = randn(64, 64)
img_b = randn(64, 64)
img_c = img_a .+ 0.1 .* randn(64, 64)  # similar to A
lpips_ab = lpips_distance(img_a, img_b)
lpips_ac = lpips_distance(img_a, img_c)
println("LPIPS(A, B): $(round(lpips_ab, digits=4))")
println("LPIPS(A, C): $(round(lpips_ac, digits=4))")
println("Lower = more perceptually similar")
```

**è§£é‡ˆ**: LPIPS âˆˆ [0, âˆ)ã€‚LPIPS < 0.1 ã¯çŸ¥è¦šçš„ã«è¿‘ã„ã€‚äººé–“ã®åˆ¤æ–­ã¨ Pearson ç›¸é–¢ ~0.8 [^3]ã€‚

#### 1.1.4 Precision-Recall (P&R)

```julia
# P&R: å¤šæ§˜ä½“ãƒ™ãƒ¼ã‚¹
function precision_recall_manifold(real_feats::Matrix{Float64},
                                    gen_feats::Matrix{Float64}, k::Int=5)
    # Precision: ç”Ÿæˆç”»åƒãŒçœŸç”»åƒå¤šæ§˜ä½“ã«ã©ã‚Œã ã‘è¿‘ã„ã‹
    # Recall: çœŸç”»åƒå¤šæ§˜ä½“ã‚’ã©ã‚Œã ã‘ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã‹

    # k-NN distance to define manifold
    n_real = size(real_feats, 1)
    n_gen = size(gen_feats, 1)

    # Precision: for each generated sample, check if it's near real manifold
    precision_count = 0
    for i in 1:n_gen
        dists = [sqrt(sum((gen_feats[i,:] .- real_feats[j,:]).^2)) for j in 1:n_real]
        if minimum(dists) < quantile(dists, 0.1)  # simplified threshold
            precision_count += 1
        end
    end
    precision = precision_count / n_gen

    # Recall: for each real sample, check if generated manifold covers it
    recall_count = 0
    for i in 1:n_real
        dists = [sqrt(sum((real_feats[i,:] .- gen_feats[j,:]).^2)) for j in 1:n_gen]
        if minimum(dists) < quantile(dists, 0.1)
            recall_count += 1
        end
    end
    recall = recall_count / n_real

    return precision, recall
end

# Test
real_f = randn(100, 64)
gen_f = randn(100, 64)
prec, rec = precision_recall_manifold(real_f, gen_f)
println("Precision: $(round(prec, digits=3)), Recall: $(round(rec, digits=3))")
println("Precision â‰ˆ quality, Recall â‰ˆ diversity")
```

**è§£é‡ˆ**: Precision = 1.0 ãªã‚‰ç”Ÿæˆç”»åƒã¯å…¨ã¦é«˜å“è³ªã€‚Recall = 1.0 ãªã‚‰çœŸç”»åƒåˆ†å¸ƒã‚’å®Œå…¨ã‚«ãƒãƒ¼ã€‚

#### 1.1.5 CMMD (CLIP-MMD)

```julia
# CMMD: CLIPåŸ‹ã‚è¾¼ã¿ â†’ MMD (RBF kernel)
function clip_embeddings_dummy(images::Vector{Matrix{Float64}})
    # Real impl: CLIP image encoder â†’ 512-dim
    n_samples = length(images)
    return randn(n_samples, 512)  # (n, 512)
end

function rbf_kernel(x::Vector{Float64}, y::Vector{Float64}, Ïƒ::Float64=1.0)
    # k(x, y) = exp(-||x - y||Â² / (2ÏƒÂ²))
    return exp(-sum((x .- y).^2) / (2*Ïƒ^2))
end

function cmmd(real_images::Vector{Matrix{Float64}},
              gen_images::Vector{Matrix{Float64}}, Ïƒ::Float64=1.0)
    # CLIP embeddings
    emb_r = clip_embeddings_dummy(real_images)  # (n, 512)
    emb_g = clip_embeddings_dummy(gen_images)   # (m, 512)

    n = size(emb_r, 1)
    m = size(emb_g, 1)

    # MMDÂ² = E[k(x,x')] + E[k(y,y')] - 2E[k(x,y)]
    # x,x' ~ P_real, y,y' ~ P_gen

    # E[k(x, x')]
    kxx = 0.0
    for i in 1:n, j in 1:n
        kxx += rbf_kernel(emb_r[i,:], emb_r[j,:], Ïƒ)
    end
    kxx /= (n * n)

    # E[k(y, y')]
    kyy = 0.0
    for i in 1:m, j in 1:m
        kyy += rbf_kernel(emb_g[i,:], emb_g[j,:], Ïƒ)
    end
    kyy /= (m * m)

    # E[k(x, y)]
    kxy = 0.0
    for i in 1:n, j in 1:m
        kxy += rbf_kernel(emb_r[i,:], emb_g[j,:], Ïƒ)
    end
    kxy /= (n * m)

    mmd_squared = kxx + kyy - 2*kxy
    return sqrt(max(0, mmd_squared))  # max(0, ...) for numerical stability
end

cmmd_score = cmmd(real_imgs[1:20], gen_imgs[1:20])  # subset for speed
println("CMMD: $(round(cmmd_score, digits=4))")
println("Lower = more similar distributions (0 = identical)")
```

**è§£é‡ˆ**: CMMD â‰ˆ 0 ãªã‚‰åˆ†å¸ƒãŒä¸€è‡´ã€‚CMMD ã¯ FID ã¨ç•°ãªã‚Š**æ­£è¦æ€§ã‚’ä»®å®šã—ãªã„** [^5]ã€‚

### 1.2 æŒ‡æ¨™é–“ã®é–¢ä¿‚

```mermaid
graph TD
    A[è©•ä¾¡æŒ‡æ¨™] --> B[åˆ†å¸ƒãƒ¬ãƒ™ãƒ«]
    A --> C[ãƒšã‚¢ãƒ¬ãƒ™ãƒ«]
    B --> D[FID<br/>ã‚¬ã‚¦ã‚¹ä»®å®š]
    B --> E[CMMD<br/>MMD, ä»®å®šãªã—]
    B --> F[IS<br/>KL, å˜ä¸€ã‚¹ã‚³ã‚¢]
    B --> G[P&R<br/>å“è³ªvså¤šæ§˜æ€§]
    C --> H[LPIPS<br/>çŸ¥è¦šè·é›¢]

    D --> I[Inceptionä¾å­˜]
    E --> J[CLIPä¾å­˜]
    F --> I
    G --> I
    H --> K[VGG/AlexNetä¾å­˜]

    style D fill:#ffe0b2
    style E fill:#c8e6c9
    style H fill:#e1bee7
```

**è¨­è¨ˆæ€æƒ³ã®é•ã„**:

- **FID**: 2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã®ãƒ•ãƒ¬ã‚·ã‚§è·é›¢ã€‚é«˜é€Ÿã ãŒæ­£è¦æ€§ä»®å®šãŒå¼·ã„ã€‚
- **CMMD**: MMDãƒ™ãƒ¼ã‚¹ã§ä»®å®šãªã—ã€‚CLIPç‰¹å¾´ã§ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œã‚‚å¯èƒ½ã€‚
- **LPIPS**: ãƒšã‚¢ç”»åƒã®çŸ¥è¦šè·é›¢ã€‚åˆ†å¸ƒå…¨ä½“ã¯è©•ä¾¡ã§ããªã„ãŒã€äººé–“ã®åˆ¤æ–­ã¨ç›¸é–¢ãŒé«˜ã„ã€‚
- **Precision-Recall**: å“è³ªï¼ˆprecisionï¼‰ã¨å¤šæ§˜æ€§ï¼ˆrecallï¼‰ã‚’åˆ†é›¢è©•ä¾¡ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ã€‚
- **IS**: å“è³ªã¨å¤šæ§˜æ€§ã‚’å˜ä¸€ã‚¹ã‚³ã‚¢ã«é›†ç´„ã€‚è§£é‡ˆãŒå›°é›£ã€‚

**ãƒ¡ãƒˆãƒªã‚¯ã‚¹é¸æŠã®æŒ‡é‡**:

| çŠ¶æ³ | æ¨å¥¨æŒ‡æ¨™ | ç†ç”± |
|:-----|:---------|:-----|
| æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ | FID + IS | æ¯”è¼ƒå¯èƒ½æ€§ |
| 2024å¹´ä»¥é™ã®ç ”ç©¶ | CMMD + FID | FIDã®é™ç•Œã‚’è£œå®Œ [^5] |
| ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆ | CMMD (CLIP) | ãƒ†ã‚­ã‚¹ãƒˆ-ç”»åƒå¯¾å¿œ |
| ãƒšã‚¢wiseæ¯”è¼ƒ | LPIPS | äººé–“ã®çŸ¥è¦šã¨ç›¸é–¢ |
| å“è³ªvså¤šæ§˜æ€§ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ• | P&R | ä¸¡è€…ã‚’åˆ†é›¢æ¸¬å®š |
| å°‘ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ<1000ï¼‰ | FLD+ [^7] | æ•°ç™¾ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®š |

:::message alert
**ã“ã“ãŒå¼•ã£ã‹ã‹ã‚Šã‚„ã™ã„**: FIDãŒæ”¹å–„ã—ã¦ã‚‚ISãŒæ‚ªåŒ–ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚å„æŒ‡æ¨™ã¯ç•°ãªã‚‹å´é¢ã‚’æ¸¬å®šã™ã‚‹ â€” **è¤‡æ•°ã®æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›ã¦ç·åˆåˆ¤æ–­**ã™ã‚‹ã“ã¨ã€‚
:::

:::message
**é€²æ—: 10% å®Œäº†** 5ã¤ã®æŒ‡æ¨™ã‚’è§¦ã£ãŸã€‚ã“ã“ã‹ã‚‰ãªãœè©•ä¾¡ãŒé›£ã—ã„ã®ã‹ã€å„æŒ‡æ¨™ã®é™ç•Œã‚’ç›´æ„Ÿçš„ã«ç†è§£ã—ã¦ã„ãã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœè©•ä¾¡ã¯é›£ã—ã„ã®ã‹

### 2.1 è©•ä¾¡ã®3ã¤ã®å›°é›£

#### 2.1.1 å›°é›£1: å®šç¾©ã®æ›–æ˜§ã•

ã€Œè‰¯ã„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€ã¨ã¯ä½•ã‹ï¼Ÿ3ã¤ã®ç›¸åã™ã‚‹è¦æ±‚ãŒã‚ã‚‹:

1. **å“è³ª (Quality)**: ç”Ÿæˆç”»åƒã¯é«˜å“è³ªã‹ï¼Ÿã¼ã‚„ã‘ã¦ã„ãªã„ã‹ï¼Ÿç¾å®Ÿçš„ã‹ï¼Ÿ
2. **å¤šæ§˜æ€§ (Diversity)**: ç”Ÿæˆç”»åƒã¯å¤šæ§˜ã‹ï¼Ÿãƒ¢ãƒ¼ãƒ‰å´©å£Šã—ã¦ã„ãªã„ã‹ï¼Ÿ
3. **å¿ å®Ÿæ€§ (Fidelity)**: çœŸç”»åƒã®åˆ†å¸ƒã‚’æ­£ç¢ºã«å†ç¾ã—ã¦ã„ã‚‹ã‹ï¼Ÿ

ã“ã‚Œã‚‰ã¯**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**ã®é–¢ä¿‚ã«ã‚ã‚‹:

```mermaid
graph TD
    A[å“è³ª] -->|é«˜å“è³ªã«é›†ä¸­| B[å¤šæ§˜æ€§â†“<br/>Mode Collapse]
    C[å¤šæ§˜æ€§] -->|å…¨ã¦ã‚«ãƒãƒ¼| D[å“è³ªâ†“<br/>ã¼ã‚„ã‘ãŸç”»åƒ]
    E[å¿ å®Ÿæ€§] -->|åˆ†å¸ƒä¸€è‡´| F[å“è³ªãƒ»å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹]
    B -.->|æ¤œå‡º| G[Precision-Recall]
    D -.->|æ¤œå‡º| G
    F -.->|æ¸¬å®š| H[FID / CMMD]

    style A fill:#ffccbc
    style C fill:#c5e1a5
    style E fill:#b3e5fc
```

**å…·ä½“ä¾‹**: GANã®StyleGANã¯å“è³ªã¯é«˜ã„ãŒã€è¨“ç·´ãŒä¸å®‰å®šã§å¤šæ§˜æ€§ãŒä½ä¸‹ã—ã‚„ã™ã„ã€‚VAEã¯å¤šæ§˜æ€§ã¯é«˜ã„ãŒã¼ã‚„ã‘ãŸå‡ºåŠ›ã«ãªã‚Šã‚„ã™ã„ã€‚

#### 2.1.2 å›°é›£2: æŒ‡æ¨™ã®é™ç•Œã¨åã‚Š

**FIDã®3ã¤ã®é™ç•Œ** [^5]:

1. **æ­£è¦æ€§ã®ä»®å®š**: ç‰¹å¾´åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«å¾“ã†ã¨ä»®å®šã€‚å®Ÿéš›ã¯å¤šå³°åˆ†å¸ƒã€‚
2. **Inception-v3ãƒã‚¤ã‚¢ã‚¹**: ImageNetã§è¨“ç·´ â†’ è‡ªç„¶ç”»åƒä»¥å¤–ï¼ˆåŒ»ç™‚ç”»åƒã€è¡›æ˜Ÿç”»åƒï¼‰ã§ä¸é©åˆ‡ã€‚
3. **ã‚µãƒ³ãƒ—ãƒ«æ•°ä¾å­˜**: æ¨å®šç²¾åº¦ãŒä½ã„ã¨ä¸å®‰å®šï¼ˆæœ€ä½2000-5000ã‚µãƒ³ãƒ—ãƒ«å¿…è¦ï¼‰ã€‚

**ISã®2ã¤ã®é™ç•Œ** [^2]:

1. **ImageNetåˆ†é¡ã¸ã®ä¾å­˜**: åˆ†é¡ç²¾åº¦ãŒé«˜ã„ â‰  ç”»åƒå“è³ªãŒé«˜ã„ã€‚
2. **KLç™ºæ•£ã®è§£é‡ˆå›°é›£**: ã‚¹ã‚³ã‚¢ãŒé«˜ã„ = è‰¯ã„ï¼Ÿ ä½•ã¨æ¯”è¼ƒã—ã¦ã„ã‚‹ã®ã‹ä¸æ˜ç­ã€‚

**LPIPSã®é™ç•Œ**:

- ãƒšã‚¢wiseæ¯”è¼ƒã®ã¿ â†’ åˆ†å¸ƒå…¨ä½“ã®è©•ä¾¡ä¸å¯ã€‚
- VGG/AlexNetä¾å­˜ â†’ ç‰¹å¾´ç©ºé–“ã®ãƒã‚¤ã‚¢ã‚¹ã€‚

**2024å¹´ã®è§£æ±ºç­–**: CMMD [^5] â€” CLIPåŸ‹ã‚è¾¼ã¿ + MMDï¼ˆä»®å®šãªã—ï¼‰ã€‚

| æŒ‡æ¨™ | ä»®å®š | ãƒã‚¤ã‚¢ã‚¹ | ã‚µãƒ³ãƒ—ãƒ«æ•° | è§£æ±ºç­– |
|:-----|:-----|:---------|:----------|:-------|
| FID | ã‚¬ã‚¦ã‚¹æ€§ | ImageNet | 2000+ | CMMD, FLD+ |
| IS | ImageNetåˆ†é¡ | ImageNet | 1000+ | â€” |
| LPIPS | æ·±å±¤ç‰¹å¾´ | ImageNet/VGG | 1ãƒšã‚¢ | â€” |
| P&R | k-NNå¤šæ§˜ä½“ | ç‰¹å¾´æŠ½å‡ºå™¨ | 1000+ | â€” |
| CMMD | ãªã— | CLIP | 500+ | â€” |
| FLD+ | Normalizing Flow | å­¦ç¿’ä¾å­˜ | 200+ | â€” |

#### 2.1.3 å›°é›£3: äººé–“è©•ä¾¡ã¨ã®ä¹–é›¢

**å®šé‡æŒ‡æ¨™ã¨äººé–“è©•ä¾¡ã®ç›¸é–¢** [^5]:

| æŒ‡æ¨™ | Pearsonç›¸é–¢ï¼ˆäººé–“è©•ä¾¡ï¼‰ | å‚™è€ƒ |
|:-----|:-----------------------|:-----|
| FID | 0.56-0.68 | ãƒ¢ãƒ‡ãƒ«é–“ã§ä¸ä¸€è‡´ |
| IS | 0.34-0.52 | ç›¸é–¢ä½ã„ |
| LPIPS | 0.78-0.82 | ãƒšã‚¢wiseæ¯”è¼ƒã§é«˜ç›¸é–¢ |
| CMMD | **0.72-0.79** | FIDã‚ˆã‚Šäººé–“è©•ä¾¡ã«è¿‘ã„ [^5] |

**ãªãœä¹–é›¢ã™ã‚‹ã®ã‹ï¼Ÿ**

1. **çŸ¥è¦šçš„å“è³ª vs çµ±è¨ˆçš„å“è³ª**: çµ±è¨ˆçš„ã«è¿‘ãã¦ã‚‚ã€äººé–“ãŒè¦‹ã¦é•å’Œæ„ŸãŒã‚ã‚‹ã€‚
2. **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¾å­˜**: ã€Œè‰¯ã„ã€ç”»åƒã®åŸºæº–ã¯ã‚¿ã‚¹ã‚¯ä¾å­˜ï¼ˆå†™å®Ÿ vs èŠ¸è¡“ï¼‰ã€‚
3. **å¤šå³°æ€§**: FIDã¯ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ â†’ è¤‡æ•°ã®ãƒ¢ãƒ¼ãƒ‰ã‚’æŒã¤åˆ†å¸ƒã§å¤±æ•—ã€‚

**æ•™è¨“**: å®šé‡æŒ‡æ¨™ã¯**ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°**ã«ã¯æœ‰åŠ¹ã€‚æœ€çµ‚åˆ¤æ–­ã¯äººé–“è©•ä¾¡ãŒå¿…è¦ã€‚

### 2.2 æœ¬è¬›ç¾©ã®ä½ç½®ã¥ã‘ â€” Course IIIã®è©•ä¾¡åŸºç›¤

```mermaid
graph LR
    A["ç¬¬24å›<br/>çµ±è¨ˆå­¦"] --> B["ç¬¬27å›<br/>è©•ä¾¡æŒ‡æ¨™"]
    C["ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–"] --> B
    B --> D["ç¬¬28å›<br/>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"]
    B --> E["ç¬¬32å›<br/>çµ±åˆPJ"]
    D --> E
    style B fill:#fff3e0
    style E fill:#c8e6c9
```

**ç¬¬24å›ï¼ˆçµ±è¨ˆå­¦ï¼‰ã¨ã®æ¥ç¶š**:
- ä»®èª¬æ¤œå®š â†’ FIDã®æœ‰æ„å·®æ¤œå®š
- ä¿¡é ¼åŒºé–“ â†’ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–
- åŠ¹æœé‡ â†’ å®Ÿè³ªçš„ãªæ”¹å–„åº¦åˆã„
- å¤šé‡æ¯”è¼ƒè£œæ­£ â†’ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒæ™‚ã®Bonferroni/FDR

**ç¬¬32å›ï¼ˆçµ±åˆPJï¼‰ã¸ã®æ©‹æ¸¡ã—**:
- è‡ªå‹•è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ â†’ CI/CDçµ±åˆ
- A/Bãƒ†ã‚¹ãƒˆ â†’ Productionç’°å¢ƒã§ã®è©•ä¾¡
- äººé–“è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ« â†’ ã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚·ãƒ³ã‚°è¨­è¨ˆ

**æœ¬è¬›ç¾©ã®ç‹¬è‡ªæ€§** â€” æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ–:

| é …ç›® | æ¾å°¾ç ”ï¼ˆ2026Springï¼‰ | æœ¬è¬›ç¾©ï¼ˆä¸Šä½äº’æ›ï¼‰ |
|:-----|:--------------------|:------------------|
| ç†è«– | FID/ISã®ç´¹ä»‹ | **æ•°å¼å®Œå…¨å°å‡º** + çµ±ä¸€ç†è«– |
| å®Ÿè£… | PyTorchå®Ÿè£… | **Juliaçµ±è¨ˆåˆ†æ + Rust Criterion** |
| æœ€æ–° | FIDä¸­å¿ƒ | **CMMD/FLD+ (2024)** + çµ±è¨ˆæ¤œå®šçµ±åˆ |
| è©•ä¾¡ | ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®— | **è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** |

### 2.3 å­¦ç¿’æˆ¦ç•¥ â€” 3ã¤ã®ãƒ¬ãƒ™ãƒ«

**ãƒ¬ãƒ™ãƒ«1: ä½¿ãˆã‚‹** (Zone 0-2, 4-5)
- FID/IS/LPIPSã‚’è¨ˆç®—ã§ãã‚‹
- æ—¢å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆ`torch-fidelity`, `lpips`ï¼‰ã‚’ä½¿ç”¨
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ„å‘³ã‚’ç†è§£

**ãƒ¬ãƒ™ãƒ«2: ç†è§£ã—ã¦ã„ã‚‹** (Zone 3, 6)
- å„æŒ‡æ¨™ã®æ•°å¼ã‚’å®Œå…¨å°å‡ºã§ãã‚‹
- ä»®å®šã¨é™ç•Œã‚’èª¬æ˜ã§ãã‚‹
- é©åˆ‡ãªæŒ‡æ¨™ã‚’é¸æŠã§ãã‚‹

**ãƒ¬ãƒ™ãƒ«3: è¨­è¨ˆã§ãã‚‹** (Zone 4-7, æ¼”ç¿’)
- è‡ªå‹•è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã§ãã‚‹
- çµ±è¨ˆæ¤œå®šã¨çµ±åˆã§ãã‚‹
- äººé–“è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’è¨­è¨ˆã§ãã‚‹

```mermaid
graph TD
    A[Level 1: ä½¿ãˆã‚‹] --> B[Level 2: ç†è§£]
    B --> C[Level 3: è¨­è¨ˆ]
    A --> D["Zone 0-2, 4-5<br/>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨"]
    B --> E["Zone 3, 6<br/>æ•°å¼å°å‡º"]
    C --> F["Zone 4-7<br/>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰"]
    style C fill:#c8e6c9
```

:::message
**é€²æ—: 20% å®Œäº†** è©•ä¾¡ã®å›°é›£ã•ã‚’ç†è§£ã—ãŸã€‚ã“ã“ã‹ã‚‰æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã¸ã€‚FID/IS/LPIPS/MMDã®å®Œå…¨å°å‡ºã«æŒ‘ã‚€ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” è©•ä¾¡æŒ‡æ¨™ã®å®Œå…¨ç†è«–

### 3.1 å‰æçŸ¥è­˜ã®ç¢ºèª

æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ä½¿ã†æ•°å­¦ï¼ˆCourse Iã§å­¦ç¿’æ¸ˆã¿ï¼‰:

| æ¦‚å¿µ | åˆå‡º | æœ¬è¬›ç¾©ã§ã®å½¹å‰² |
|:-----|:-----|:-------------|
| **ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ** | ç¬¬4å› | FIDã®ã‚¬ã‚¦ã‚¹ä»®å®š |
| **KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹** | ç¬¬6å› | ISã®å®šç¾© |
| **ãƒ•ãƒ¬ã‚·ã‚§è·é›¢** | ç¬¬5å› | FIDã®è·é›¢å®šç¾© |
| **è¡Œåˆ—å¹³æ–¹æ ¹** | ç¬¬2-3å› | FIDã®å…±åˆ†æ•£é … |
| **ã‚«ãƒ¼ãƒãƒ«æ³•** | ç¬¬6å› | MMDã®RKHS |
| **æœŸå¾…å€¤ãƒ»åˆ†æ•£** | ç¬¬4å› | çµ±è¨ˆé‡ã®è¨ˆç®— |

### 3.2 FID (FrÃ©chet Inception Distance) å®Œå…¨å°å‡º

#### 3.2.1 ãƒ•ãƒ¬ã‚·ã‚§è·é›¢ã®å®šç¾©

**å•é¡Œè¨­å®š**: 2ã¤ã®ç¢ºç‡åˆ†å¸ƒ $P_r$ï¼ˆçœŸç”»åƒï¼‰, $P_g$ï¼ˆç”Ÿæˆç”»åƒï¼‰ã®è·é›¢ã‚’æ¸¬ã‚ŠãŸã„ã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ä¸¡åˆ†å¸ƒã‚’ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}(\mu, \Sigma)$ ã§è¿‘ä¼¼ã—ã€2ã¤ã®ã‚¬ã‚¦ã‚¹é–“ã®è·é›¢ã‚’æ¸¬ã‚‹ã€‚

**å®šç¾©** (FrÃ©chet distance between two Gaussians):

$$
d_F^2(\mathcal{N}(\mu_1, \Sigma_1), \mathcal{N}(\mu_2, \Sigma_2)) = \|\mu_1 - \mu_2\|_2^2 + \text{Tr}(\Sigma_1 + \Sigma_2 - 2(\Sigma_1 \Sigma_2)^{1/2})
$$

**å„é …ã®æ„å‘³**:
- $\|\mu_1 - \mu_2\|^2$: å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã®äºŒä¹— â†’ åˆ†å¸ƒã®ä¸­å¿ƒãŒã©ã‚Œã ã‘ãšã‚Œã¦ã„ã‚‹ã‹
- $\text{Tr}(\Sigma_1 + \Sigma_2 - 2\sqrt{\Sigma_1 \Sigma_2})$: å…±åˆ†æ•£è¡Œåˆ—ã®å·® â†’ åˆ†å¸ƒã®åºƒãŒã‚Šæ–¹ãŒã©ã‚Œã ã‘ç•°ãªã‚‹ã‹

#### 3.2.2 ãªãœã“ã®å¼ãªã®ã‹ â€” 2-Wassersteinè·é›¢ã¨ã®é–¢ä¿‚

ãƒ•ãƒ¬ã‚·ã‚§è·é›¢ã¯ã€**2-Wassersteinè·é›¢** $W_2$ ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã§ã®é–‰å½¢å¼è§£ã§ã‚ã‚‹ã€‚

**Wassersteinè·é›¢ã®å®šç¾©** (ç¬¬13å›ã§å­¦ç¿’):

$$
W_2^2(P, Q) = \inf_{\gamma \in \Gamma(P,Q)} \mathbb{E}_{(x,y) \sim \gamma}[\|x - y\|^2]
$$

ã“ã“ã§ $\Gamma(P,Q)$ ã¯ $P$ ã¨ $Q$ ã‚’ãƒãƒ¼ã‚¸ãƒŠãƒ«ã«æŒã¤çµåˆåˆ†å¸ƒã®é›†åˆï¼ˆè¼¸é€è¨ˆç”»ï¼‰ã€‚

**å®šç†** (Dowson & Landau 1982): $P = \mathcal{N}(\mu_1, \Sigma_1)$, $Q = \mathcal{N}(\mu_2, \Sigma_2)$ ã®ã¨ãã€

$$
W_2^2(P, Q) = \|\mu_1 - \mu_2\|^2 + \text{Tr}(\Sigma_1 + \Sigma_2 - 2(\Sigma_1 \Sigma_2)^{1/2})
$$

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ** (å®Œå…¨è¨¼æ˜ã¯ [Recalled, not fully derived â€” verify]):

1. ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã®æœ€é©è¼¸é€ã¯**ç·šå½¢å†™åƒ** $T(x) = Ax + b$ ã§é”æˆã•ã‚Œã‚‹ã€‚
2. $P$-almost surely ã« $T_\#P = Q$ ã‚’æº€ãŸã™ $T$ ã‚’æ±‚ã‚ã‚‹ã€‚
3. $T$ ã®å½¢ã‚’æ±‚ã‚ã‚‹ã¨ã€$A = \Sigma_1^{-1/2}(\Sigma_1^{1/2}\Sigma_2\Sigma_1^{1/2})^{1/2}\Sigma_1^{-1/2}$, $b = \mu_2 - A\mu_1$ã€‚
4. è¼¸é€ã‚³ã‚¹ãƒˆ $\mathbb{E}[\|x - T(x)\|^2]$ ã‚’è¨ˆç®—ã™ã‚‹ã¨ä¸Šå¼ã‚’å¾—ã‚‹ã€‚

:::details ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“Wassersteinè·é›¢ã®è©³ç´°å°å‡ºï¼ˆç™ºå±•ï¼‰

**Step 1**: æœ€é©è¼¸é€ãƒãƒƒãƒ— $T$ ã®å½¢ã‚’ä»®å®šã€‚

ç·šå½¢å†™åƒ $T(x) = Ax + b$ ã‚’è€ƒãˆã‚‹ã€‚$T_\#\mathcal{N}(\mu_1, \Sigma_1) = \mathcal{N}(\mu_2, \Sigma_2)$ ã¨ãªã‚‹æ¡ä»¶:
- å¹³å‡: $A\mu_1 + b = \mu_2$ â†’ $b = \mu_2 - A\mu_1$
- å…±åˆ†æ•£: $A\Sigma_1 A^\top = \Sigma_2$

**Step 2**: $A$ ã®é¸æŠã€‚

$A\Sigma_1 A^\top = \Sigma_2$ ã‚’æº€ãŸã™ $A$ ã¯ä¸€æ„ã§ã¯ãªã„ã€‚Monge-Kantorovichç†è«–ã‚ˆã‚Šã€æœ€é©ãª $A$ ã¯:

$$
A = \Sigma_1^{-1/2}(\Sigma_1^{1/2}\Sigma_2\Sigma_1^{1/2})^{1/2}\Sigma_1^{-1/2}
$$

**Step 3**: è¼¸é€ã‚³ã‚¹ãƒˆã®è¨ˆç®—ã€‚

$$
\begin{aligned}
W_2^2 &= \mathbb{E}_{x \sim P}[\|x - T(x)\|^2] \\
&= \mathbb{E}[\|x - Ax - b\|^2] \\
&= \mathbb{E}[\|(I - A)x - b\|^2] \\
&= \mathbb{E}[\|(I - A)(x - \mu_1) + (I - A)\mu_1 - b\|^2]
\end{aligned}
$$

$b = \mu_2 - A\mu_1$ ã‚ˆã‚Š $(I - A)\mu_1 - b = \mu_1 - \mu_2$ã€‚

$$
W_2^2 = \text{Tr}((I - A)\Sigma_1(I - A)^\top) + \|\mu_1 - \mu_2\|^2
$$

$(I - A)\Sigma_1(I - A)^\top$ ã‚’å±•é–‹ã—ã€$A\Sigma_1 A^\top = \Sigma_2$ ã‚’ä»£å…¥:

$$
\begin{aligned}
\text{Tr}((I - A)\Sigma_1(I - A)^\top) &= \text{Tr}(\Sigma_1 - A\Sigma_1 - \Sigma_1 A^\top + A\Sigma_1 A^\top) \\
&= \text{Tr}(\Sigma_1) + \text{Tr}(\Sigma_2) - 2\text{Tr}(A\Sigma_1)
\end{aligned}
$$

$A$ ã®å½¢ã‚’ä»£å…¥ã—ã€$\text{Tr}(A\Sigma_1) = \text{Tr}((\Sigma_1 \Sigma_2)^{1/2})$ ã‚’ç¤ºã›ã‚‹ï¼ˆç·šå½¢ä»£æ•°ã®ãƒˆãƒªãƒƒã‚¯ï¼‰:

$$
W_2^2 = \|\mu_1 - \mu_2\|^2 + \text{Tr}(\Sigma_1 + \Sigma_2 - 2(\Sigma_1\Sigma_2)^{1/2})
$$

:::

#### 3.2.3 è¡Œåˆ—å¹³æ–¹æ ¹ $(\Sigma_1\Sigma_2)^{1/2}$ ã®è¨ˆç®—

**å•é¡Œ**: 2ã¤ã®æ­£å®šå€¤è¡Œåˆ— $\Sigma_1, \Sigma_2$ ã®ç© $\Sigma_1\Sigma_2$ ã®å¹³æ–¹æ ¹ã‚’è¨ˆç®—ã—ãŸã„ã€‚

**æ³¨æ„**: $\Sigma_1\Sigma_2$ ã¯ä¸€èˆ¬ã«å¯¾ç§°è¡Œåˆ—ã§ã¯ãªã„ â†’ å›ºæœ‰å€¤åˆ†è§£ãŒéå¯¾ç§°ã€‚

**è¨ˆç®—æ–¹æ³•**: å›ºæœ‰å€¤åˆ†è§£ã‚’ä½¿ã†ã€‚

$$
\Sigma_1\Sigma_2 = V\Lambda V^{-1}
$$

ã“ã“ã§ $V$ ã¯å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—ã€$\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_d)$ ã¯å›ºæœ‰å€¤ã®å¯¾è§’è¡Œåˆ—ã€‚

$$
(\Sigma_1\Sigma_2)^{1/2} = V\Lambda^{1/2}V^{-1} = V\text{diag}(\sqrt{\lambda_1}, \ldots, \sqrt{\lambda_d})V^{-1}
$$

**å®Ÿè£…ä¸Šã®æ³¨æ„**:
1. $\Sigma_1, \Sigma_2$ ãŒæ­£å®šå€¤ã§ã‚‚ã€$\Sigma_1\Sigma_2$ ã¯æ­£å®šå€¤ã¨ã¯é™ã‚‰ãªã„ â†’ å›ºæœ‰å€¤ãŒè² ã«ãªã‚‹å¯èƒ½æ€§ã€‚
2. æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã€$\lambda_i < 0$ ã®å ´åˆã¯ $|\lambda_i|$ ã‚’ä½¿ã†ï¼ˆor small positive value ã§ clippingï¼‰ã€‚

```julia
# Matrix square root via eigen decomposition
function matrix_sqrt(A::Matrix{Float64})
    eig = eigen(A)
    # Handle numerical errors: negative eigenvalues â†’ abs
    Î»_sqrt = sqrt.(Complex.(eig.values))  # complex sqrt for negative Î»
    return real(eig.vectors * Diagonal(Î»_sqrt) * inv(eig.vectors))
end

# Test
Î£1 = [2.0 0.5; 0.5 1.5]
Î£2 = [1.8 0.3; 0.3 1.2]
prod = Î£1 * Î£2
sqrt_prod = matrix_sqrt(prod)
println("(Î£1*Î£2)^{1/2} computed")
println("Verification: (sqrt)^2 â‰ˆ original? ", isapprox(sqrt_prod^2, prod, atol=1e-6))
```

#### 3.2.4 FIDã®å®Ÿè£…ã¨Inceptionç‰¹å¾´æŠ½å‡º

**FIDè¨ˆç®—ã®å…¨ä½“ãƒ•ãƒ­ãƒ¼**:

1. **Inception-v3ã§ç‰¹å¾´æŠ½å‡º**: ç”»åƒ â†’ 2048æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ï¼ˆpre-poolå±¤ï¼‰
2. **çµ±è¨ˆé‡è¨ˆç®—**: $\mu_r, \Sigma_r$ (çœŸç”»åƒ), $\mu_g, \Sigma_g$ (ç”Ÿæˆç”»åƒ)
3. **ãƒ•ãƒ¬ã‚·ã‚§è·é›¢è¨ˆç®—**: ä¸Šè¨˜ã®å¼

```julia
# FID implementation (with dummy Inception features)
using LinearAlgebra, Statistics

function extract_inception_features(images::Vector{Matrix{Float64}})
    # Real impl: load pre-trained Inception-v3, extract pool3 layer
    # Here: simulate with random projection
    n = length(images)
    d_feat = 2048  # Inception pool3 dimension
    return randn(n, d_feat)
end

function compute_statistics(features::Matrix{Float64})
    # features: (n_samples, d_features)
    Î¼ = vec(mean(features, dims=1))  # (d_features,)
    Î£ = cov(features)  # (d_features, d_features)
    return Î¼, Î£
end

function frechet_distance(Î¼1::Vector{Float64}, Î£1::Matrix{Float64},
                           Î¼2::Vector{Float64}, Î£2::Matrix{Float64})
    # Mean difference
    diff = Î¼1 .- Î¼2
    mean_term = sum(diff.^2)

    # Covariance term: Tr(Î£1 + Î£2 - 2(Î£1*Î£2)^{1/2})
    # Matrix square root
    product = Î£1 * Î£2
    eig = eigen(product)
    # Use abs for numerical stability
    sqrt_eig = sqrt.(abs.(eig.values))
    sqrt_product = real(eig.vectors * Diagonal(sqrt_eig) * eig.vectors')

    trace_term = tr(Î£1) + tr(Î£2) - 2 * tr(sqrt_product)

    return mean_term + trace_term
end

function fid_score(real_images::Vector{Matrix{Float64}},
                    gen_images::Vector{Matrix{Float64}})
    # Extract features
    feats_real = extract_inception_features(real_images)
    feats_gen = extract_inception_features(gen_images)

    # Compute statistics
    Î¼_r, Î£_r = compute_statistics(feats_real)
    Î¼_g, Î£_g = compute_statistics(feats_gen)

    # Compute FrÃ©chet distance
    return frechet_distance(Î¼_r, Î£_r, Î¼_g, Î£_g)
end

# Test with synthetic data
n_samples = 100
real_imgs = [randn(64, 64) for _ in 1:n_samples]
gen_imgs = [randn(64, 64) for _ in 1:n_samples]  # random images

fid = fid_score(real_imgs, gen_imgs)
println("FID: $(round(fid, digits=2))")
println("Expected range: 0 (identical) to ~400 (completely different)")
```

**æ•°å€¤æ¤œè¨¼**: $\mu_1 = \mu_2$, $\Sigma_1 = \Sigma_2$ ã®ã¨ã FID = 0 ã«ãªã‚‹ã‹ç¢ºèªã€‚

```julia
# Sanity check: identical distributions â†’ FID = 0
Î¼_test = randn(10)
Î£_test = randn(10, 10); Î£_test = Î£_test * Î£_test' + I  # ensure PD
fid_identical = frechet_distance(Î¼_test, Î£_test, Î¼_test, Î£_test)
println("FID (identical distributions): $(round(fid_identical, digits=10))")
# Should be ~0 (machine precision errors ~1e-10)
```

#### 3.2.5 FIDã®é™ç•Œã¨å¯¾ç­–

**é™ç•Œ1: ã‚¬ã‚¦ã‚¹æ€§ã®ä»®å®š**

å®Ÿéš›ã®ç‰¹å¾´åˆ†å¸ƒã¯**å¤šå³°æ€§**ã‚’æŒã¤ â†’ å˜ä¸€ã‚¬ã‚¦ã‚¹ã§è¿‘ä¼¼ã™ã‚‹ã¨æƒ…å ±ã‚’å¤±ã†ã€‚

**å¯¾ç­–**:
- Gaussian Mixture Model (GMM) ã§è¿‘ä¼¼ â†’ è¨ˆç®—è¤‡é›‘åº¦å¢—
- MMDãƒ™ãƒ¼ã‚¹ã®æŒ‡æ¨™ï¼ˆCMMD [^5]ï¼‰â†’ ä»®å®šãªã—

**é™ç•Œ2: ã‚µãƒ³ãƒ—ãƒ«æ•°ä¾å­˜**

$\Sigma$ ã®æ¨å®šã«ã¯ $O(d^2)$ ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå¿…è¦ï¼ˆ$d$ = ç‰¹å¾´æ¬¡å…ƒï¼‰ã€‚Inceptionç‰¹å¾´ã¯2048æ¬¡å…ƒ â†’ ç†è«–ä¸Š $2048^2 \approx 4M$ ã‚µãƒ³ãƒ—ãƒ«å¿…è¦ã€‚

å®Ÿéš›ã¯2000-5000ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®šã™ã‚‹ãŒã€å°‘ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ä¸å®‰å®šã€‚

**å¯¾ç­–**:
- ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å¢—ã‚„ã™
- æ¬¡å…ƒå‰Šæ¸›ï¼ˆPCAï¼‰â†’ æƒ…å ±æå¤±
- FLD+ [^7]: Normalizing Flowã§200ã‚µãƒ³ãƒ—ãƒ«ã§ã‚‚å®‰å®š

**é™ç•Œ3: ImageNetãƒã‚¤ã‚¢ã‚¹**

Inception-v3ã¯ImageNetã§è¨“ç·´ â†’ è‡ªç„¶ç”»åƒä»¥å¤–ã§ä¸é©åˆ‡ï¼ˆåŒ»ç™‚ç”»åƒã€è¡›æ˜Ÿç”»åƒã€ã‚¢ãƒ¼ãƒˆï¼‰ã€‚

**å¯¾ç­–**:
- ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®ç‰¹å¾´æŠ½å‡ºå™¨ï¼ˆä¾‹: åŒ»ç™‚ç”»åƒç”¨ResNetï¼‰
- CLIPåŸ‹ã‚è¾¼ã¿ï¼ˆCMMD [^5]ï¼‰â†’ ã‚ˆã‚Šæ±ç”¨çš„

:::message alert
**æ•°å¼ä¿®è¡Œã®ã‚³ãƒ„**: FIDã®å¼ã‚’**æš—è¨˜ã™ã‚‹ãªã€‚å°å‡ºã—ã‚**ã€‚Wassersteinè·é›¢ â†’ ã‚¬ã‚¦ã‚¹é–“ã®é–‰å½¢å¼ â†’ è¡Œåˆ—å¹³æ–¹æ ¹ã®è¨ˆç®—ã€ã¨ã„ã†æµã‚Œã‚’è¿½ãˆã°ã€å¼ã®æ„å‘³ãŒç†è§£ã§ãã‚‹ã€‚
:::

### 3.3 IS (Inception Score) å®Œå…¨å°å‡º

#### 3.3.1 å®šç¾©ã¨å‹•æ©Ÿ

**Inception Score** [^2] ã¯ã€ç”Ÿæˆç”»åƒã®å“è³ªã¨å¤šæ§˜æ€§ã‚’å˜ä¸€ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**:
1. **å“è³ª**: å„ç”Ÿæˆç”»åƒ $x$ ã‚’ Inception-v3 ã§åˆ†é¡ â†’ äºˆæ¸¬åˆ†å¸ƒ $p(y|x)$ ãŒã‚·ãƒ£ãƒ¼ãƒ—ï¼ˆé«˜confidenceï¼‰ãªã‚‰é«˜å“è³ª
2. **å¤šæ§˜æ€§**: å…¨ç”»åƒã®äºˆæ¸¬åˆ†å¸ƒã®å¹³å‡ $p(y) = \mathbb{E}_x[p(y|x)]$ ãŒå‡ä¸€ï¼ˆå…¨ã‚¯ãƒ©ã‚¹ã‚’ã‚«ãƒãƒ¼ï¼‰ãªã‚‰å¤šæ§˜

**å®šç¾©**:

$$
\text{IS}(G) = \exp\left(\mathbb{E}_{x \sim p_g}[\text{KL}(p(y|x) \| p(y))]\right)
$$

ã“ã“ã§:
- $p_g$: ç”Ÿæˆãƒ¢ãƒ‡ãƒ« $G$ ã®åˆ†å¸ƒ
- $p(y|x)$: ç”»åƒ $x$ ã«å¯¾ã™ã‚‹Inception-v3ã®äºˆæ¸¬åˆ†å¸ƒï¼ˆsoftmax outputï¼‰
- $p(y) = \mathbb{E}_{x \sim p_g}[p(y|x)]$: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã§ã®äºˆæ¸¬åˆ†å¸ƒã®å¹³å‡ï¼ˆå‘¨è¾ºåˆ†å¸ƒï¼‰
- $\text{KL}(p(y|x) \| p(y))$: æ¡ä»¶ä»˜ãåˆ†å¸ƒã¨å‘¨è¾ºåˆ†å¸ƒã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹

#### 3.3.2 KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®å¾©ç¿’

**å®šç¾©** (ç¬¬6å›ã§å­¦ç¿’):

$$
\text{KL}(P \| Q) = \sum_y P(y) \log\frac{P(y)}{Q(y)} = \mathbb{E}_{y \sim P}\left[\log\frac{P(y)}{Q(y)}\right]
$$

**æ€§è³ª**:
- $\text{KL}(P \| Q) \geq 0$ï¼ˆéè² æ€§ï¼‰
- $\text{KL}(P \| Q) = 0 \iff P = Q$
- éå¯¾ç§°: $\text{KL}(P \| Q) \neq \text{KL}(Q \| P)$

#### 3.3.3 ISãŒé«˜ã„ã¨ã = è‰¯ã„ãƒ¢ãƒ‡ãƒ«ï¼Ÿ

**ISãŒé«˜ã„ã‚±ãƒ¼ã‚¹**:

1. $p(y|x)$ ãŒã‚·ãƒ£ãƒ¼ãƒ—ï¼ˆpeakyï¼‰â†’ $\text{KL}(p(y|x) \| p(y))$ ãŒå¤§ãã„
2. $p(y)$ ãŒå‡ä¸€ï¼ˆuniformï¼‰â†’ å¤šæ§˜ãªã‚¯ãƒ©ã‚¹ã‚’ã‚«ãƒãƒ¼

**å…·ä½“ä¾‹**:

- **æœ€è‰¯ã®ã‚±ãƒ¼ã‚¹**: $p(y|x) = \delta(y - y^*)$ï¼ˆ1ã¤ã®ã‚¯ãƒ©ã‚¹ã«ç¢ºç‡1ï¼‰ã‹ã¤ $p(y) = \text{Uniform}(1/K)$ï¼ˆå…¨ã‚¯ãƒ©ã‚¹å‡ç­‰ï¼‰
  - $\text{KL}(p(y|x) \| p(y)) = \log K$ ï¼ˆæœ€å¤§ï¼‰
  - $\text{IS} = \exp(\log K) = K$ ï¼ˆã‚¯ãƒ©ã‚¹æ•°ï¼‰

- **æœ€æ‚ªã®ã‚±ãƒ¼ã‚¹**: $p(y|x) = p(y)$ ï¼ˆæ¡ä»¶ä»˜ã = å‘¨è¾ºï¼‰
  - $\text{KL}(p(y|x) \| p(y)) = 0$
  - $\text{IS} = \exp(0) = 1$

**ã‚¹ã‚³ã‚¢ã®ç¯„å›²**:

$$
\text{IS} \in [1, K]
$$

ã“ã“ã§ $K$ ã¯Inceptionã®åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°ï¼ˆImageNetã§ã¯1000ï¼‰ã€‚

#### 3.3.4 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ 1:1å¯¾å¿œ

```julia
# Inception Score implementation
using Statistics

function inception_score(images::Vector{Matrix{Float64}}, n_splits::Int=10)
    # Step 1: Inception-v3 classification â†’ p(y|x) for each image
    # Real impl: forward pass through Inception-v3
    # Here: random softmax for demo
    n_samples = length(images)
    n_classes = 1000  # ImageNet classes

    # Simulate Inception predictions
    logits = randn(n_samples, n_classes)
    p_yx = exp.(logits) ./ sum(exp.(logits), dims=2)  # (n_samples, n_classes)

    # Step 2: Compute p(y) = E_x[p(y|x)] (marginal distribution)
    p_y = vec(mean(p_yx, dims=1))  # (n_classes,)

    # Step 3: Compute KL(p(y|x) || p(y)) for each image
    kl_divs = zeros(n_samples)
    for i in 1:n_samples
        for j in 1:n_classes
            if p_yx[i,j] > 1e-10 && p_y[j] > 1e-10  # avoid log(0)
                kl_divs[i] += p_yx[i,j] * log(p_yx[i,j] / p_y[j])
            end
        end
    end

    # Step 4: IS = exp(E[KL])
    mean_kl = mean(kl_divs)
    is_score = exp(mean_kl)

    # Optional: compute IS over multiple splits for stability
    # (split dataset into n_splits parts, compute IS for each, average)
    # Here: simplified version with single split

    return is_score, mean_kl
end

# Test
test_imgs = [randn(64, 64) for _ in 1:1000]
is, kl = inception_score(test_imgs)
println("Inception Score: $(round(is, digits=2))")
println("Mean KL: $(round(kl, digits=4))")
println("Expected range: [1.0, 1000.0] for ImageNet")
```

**æ•°å€¤æ¤œè¨¼**: æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã§ç¢ºèªã€‚

```julia
# Case 1: perfect quality + diversity (maximum IS)
# p(y|x) = one-hot, p(y) = uniform â†’ IS = K
n = 1000
k = 100  # simplified: 100 classes
p_yx_perfect = zeros(n, k)
for i in 1:n
    p_yx_perfect[i, mod(i-1, k)+1] = 1.0  # one-hot, cyclic
end
p_y_perfect = vec(mean(p_yx_perfect, dims=1))  # should be uniform

kl_perfect = zeros(n)
for i in 1:n, j in 1:k
    if p_yx_perfect[i,j] > 0 && p_y_perfect[j] > 0
        kl_perfect[i] += p_yx_perfect[i,j] * log(p_yx_perfect[i,j] / p_y_perfect[j])
    end
end
is_perfect = exp(mean(kl_perfect))
println("IS (perfect case): $(round(is_perfect, digits=2)) â‰ˆ $k")

# Case 2: p(y|x) = p(y) (worst case) â†’ IS = 1
p_yx_worst = repeat(p_y_perfect', n, 1)  # all images have same p(y|x) = p(y)
kl_worst = zeros(n)
for i in 1:n, j in 1:k
    if p_yx_worst[i,j] > 0
        kl_worst[i] += p_yx_worst[i,j] * log(p_yx_worst[i,j] / p_y_perfect[j])
    end
end
is_worst = exp(mean(kl_worst))
println("IS (worst case): $(round(is_worst, digits=4)) â‰ˆ 1.0")
```

#### 3.3.5 ISã®é™ç•Œ

**é™ç•Œ1: ImageNetåˆ†é¡ã¸ã®ä¾å­˜**

Inception-v3ã®åˆ†é¡ç²¾åº¦ãŒé«˜ã„ â‰  ç”»åƒå“è³ªãŒé«˜ã„ã€‚

**ä¾‹**: çŠ¬ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚å…¨ã¦åŒã˜çŠ¬ç¨®ï¼ˆmode collapseï¼‰ã§ã‚‚ã€InceptionãŒã€ŒçŠ¬ã€ã¨é«˜ç¢ºä¿¡ã§åˆ†é¡ã™ã‚Œã°ISã¯é«˜ã„ã€‚

**é™ç•Œ2: KLç™ºæ•£ã®è§£é‡ˆå›°é›£**

$\text{KL}(p(y|x) \| p(y))$ ãŒå¤§ãã„ â†’ è‰¯ã„ï¼Ÿä½•ã¨æ¯”è¼ƒã—ã¦ã„ã‚‹ã®ã‹ä¸æ˜ç­ã€‚

**é™ç•Œ3: ã‚¹ã‚³ã‚¢ã®çµ¶å¯¾å€¤ã«æ„å‘³ãŒãªã„**

IS = 30 vs 35 ã®å·®ã¯å®Ÿè³ªçš„ã«ã©ã‚Œãã‚‰ã„ï¼Ÿå®šé‡çš„ãªè§£é‡ˆãŒå›°é›£ã€‚

**å¯¾ç­–**:
- FIDã¨ä½µç”¨ â†’ ç›¸è£œçš„ãªæƒ…å ±
- Precision-Recall â†’ å“è³ªã¨å¤šæ§˜æ€§ã‚’åˆ†é›¢æ¸¬å®š
- äººé–“è©•ä¾¡ â†’ æœ€çµ‚åˆ¤æ–­

:::message
**ãƒœã‚¹æˆ¦ã¸ã®æº–å‚™ 30% å®Œäº†**: FIDã¨ISã®æ•°å¼ã‚’å®Œå…¨å°å‡ºã—ãŸã€‚ã“ã“ã‹ã‚‰LPIPS, Precision-Recall, MMD/CMMDã‚’å°å‡ºã™ã‚‹ã€‚
:::

### 3.4 LPIPS (Learned Perceptual Image Patch Similarity) å®Œå…¨å°å‡º

#### 3.4.1 å‹•æ©Ÿã¨è¨­è¨ˆæ€æƒ³

**å•é¡Œ**: ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®è·é›¢ï¼ˆL2, SSIMï¼‰ã¯äººé–“ã®çŸ¥è¦šã¨ç›¸é–¢ãŒä½ã„ã€‚

**ä¾‹**:
- ç”»åƒAã‚’1ãƒ”ã‚¯ã‚»ãƒ«ãšã‚‰ã™ â†’ L2è·é›¢ã¯å¤§ãã„ãŒã€äººé–“ã«ã¯åŒã˜ã«è¦‹ãˆã‚‹
- ç”»åƒBã®è‰²ã‚’å°‘ã—å¤‰ãˆã‚‹ â†’ L2è·é›¢ã¯å°ã•ã„ãŒã€äººé–“ã«ã¯é•ã£ã¦è¦‹ãˆã‚‹

**ã‚¢ã‚¤ãƒ‡ã‚¢**: æ·±å±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç‰¹å¾´ç©ºé–“ã§è·é›¢ã‚’æ¸¬ã‚‹ â†’ äººé–“ã®çŸ¥è¦šã«è¿‘ã„ [^3]ã€‚

#### 3.4.2 å®šç¾©

**LPIPSè·é›¢** (Zhang et al. 2018 [^3]):

$$
d_{\text{LPIPS}}(x, x_0) = \sum_{\ell} w_\ell \frac{1}{H_\ell W_\ell} \sum_{h,w} \|f_\ell^h(x) - f_\ell^h(x_0)\|_2^2
$$

ã“ã“ã§:
- $f_\ell$: VGG/AlexNetã®ç¬¬$\ell$å±¤ã®ç‰¹å¾´ãƒãƒƒãƒ—ï¼ˆè¤‡æ•°å±¤ã‚’ä½¿ç”¨ï¼‰
- $f_\ell^h$: channel-wise normalizationï¼ˆå„ãƒãƒ£ãƒãƒ«ã‚’æ­£è¦åŒ–ï¼‰
- $w_\ell$: å±¤ã”ã¨ã®é‡ã¿ï¼ˆå­¦ç¿’ã•ã‚Œã‚‹ï¼‰
- $H_\ell, W_\ell$: ç¬¬$\ell$å±¤ã®ç‰¹å¾´ãƒãƒƒãƒ—ã®é«˜ã•ãƒ»å¹…

**ç›´æ„Ÿ**:
- æµ…ã„å±¤ï¼ˆedge, textureï¼‰+ æ·±ã„å±¤ï¼ˆsemantic contentï¼‰ã®ä¸¡æ–¹ã‚’ä½¿ã†
- å¤šå±¤ã®ç‰¹å¾´ã‚’ weighted sum â†’ äººé–“ã®çŸ¥è¦šã‚’è¿‘ä¼¼

#### 3.4.3 Channel-wise Normalization ã®æ„å‘³

**ãªãœæ­£è¦åŒ–ã™ã‚‹ã®ã‹ï¼Ÿ**

æ·±å±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç‰¹å¾´ã¯ã€ãƒãƒ£ãƒãƒ«ã”ã¨ã«å¤§ãã•ãŒç•°ãªã‚‹ï¼ˆä¾‹: ãƒãƒ£ãƒãƒ«1ã¯å¹³å‡100, ãƒãƒ£ãƒãƒ«2ã¯å¹³å‡0.1ï¼‰ã€‚ãã®ã¾ã¾è·é›¢ã‚’æ¸¬ã‚‹ã¨ã€å¤§ãã„ãƒãƒ£ãƒãƒ«ãŒæ”¯é…çš„ã«ãªã‚‹ã€‚

**æ­£è¦åŒ–**:

$$
f_\ell^h(x) = \frac{f_\ell(x) - \mu_\ell}{\sigma_\ell}
$$

ã“ã“ã§ $\mu_\ell, \sigma_\ell$ ã¯ãƒãƒ£ãƒãƒ«ã”ã¨ã®å¹³å‡ãƒ»æ¨™æº–åå·®ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—ï¼‰ã€‚

#### 3.4.4 å®Ÿè£…ã¨æ•°å¼å¯¾å¿œ

```julia
# LPIPS implementation (simplified)
using Statistics

# Dummy VGG feature extractor (real impl: pre-trained VGG-16)
function vgg_features(image::Matrix{Float64})
    # Real impl: extract features from VGG layers: conv1_2, conv2_2, conv3_3, conv4_3, conv5_3
    # Here: simulate with 5 scales Ã— 64 channels
    n_layers = 5
    features = []
    for â„“ in 1:n_layers
        # Simulate feature map: (H_â„“, W_â„“, C_â„“)
        h_size = 64 Ã· (2^(â„“-1))  # decreasing spatial size
        c_size = 64 * (2^(â„“-1))  # increasing channels
        feat = randn(h_size, h_size, c_size)
        push!(features, feat)
    end
    return features
end

function channel_normalize(feat::Array{Float64,3})
    # feat: (H, W, C)
    # Normalize each channel
    H, W, C = size(feat)
    feat_norm = zeros(H, W, C)
    for c in 1:C
        channel = feat[:,:,c]
        Î¼ = mean(channel)
        Ïƒ = std(channel) + 1e-10  # avoid division by zero
        feat_norm[:,:,c] = (channel .- Î¼) ./ Ïƒ
    end
    return feat_norm
end

function lpips(img1::Matrix{Float64}, img2::Matrix{Float64}, weights::Vector{Float64}=[1.0, 1.0, 1.0, 1.0, 1.0])
    # Extract multi-scale features
    feats1 = vgg_features(img1)
    feats2 = vgg_features(img2)

    # Compute distance per layer
    distance = 0.0
    for (â„“, (f1, f2)) in enumerate(zip(feats1, feats2))
        # Channel-wise normalization
        f1_norm = channel_normalize(f1)
        f2_norm = channel_normalize(f2)

        # L2 distance, averaged over spatial dimensions
        diff = f1_norm .- f2_norm
        layer_dist = sum(diff.^2) / (size(f1, 1) * size(f1, 2))

        # Weighted sum
        distance += weights[â„“] * layer_dist
    end

    return sqrt(distance)  # or distance (squared)
end

# Test
img_a = randn(128, 128)
img_b = randn(128, 128)
img_c = img_a .+ 0.05 .* randn(128, 128)  # similar to A

lpips_ab = lpips(img_a, img_b)
lpips_ac = lpips(img_a, img_c)
println("LPIPS(A, B): $(round(lpips_ab, digits=4))")
println("LPIPS(A, C): $(round(lpips_ac, digits=4))")
println("Expected: LPIPS(A, C) < LPIPS(A, B)")
```

#### 3.4.5 LPIPSã¨äººé–“è©•ä¾¡ã®ç›¸é–¢

**Berkeley-Adobe Perceptual Patch Similarity (BAPPS) dataset** [^3]:

- äººé–“ã®çŸ¥è¦šåˆ¤æ–­ vs å„ç¨®è·é›¢æŒ‡æ¨™ã®ç›¸é–¢ã‚’æ¸¬å®š
- LPIPS vs L2 vs SSIM vs æ—¢å­˜æ‰‹æ³•

**çµæœ** [^3]:

| æŒ‡æ¨™ | Pearsonç›¸é–¢ï¼ˆäººé–“è©•ä¾¡ï¼‰ |
|:-----|:----------------------|
| L2 (pixel-wise) | 0.45 |
| SSIM | 0.52 |
| LPIPS (VGG) | **0.78** |
| LPIPS (AlexNet) | **0.80** |

LPIPS ã¯æ—¢å­˜æ‰‹æ³•ã‚’å¤§ããä¸Šå›ã‚‹ã€‚

#### 3.4.6 LPIPSã®é™ç•Œ

**é™ç•Œ1: ãƒšã‚¢wiseæ¯”è¼ƒã®ã¿**

LPIPSã¯2ç”»åƒé–“ã®è·é›¢ â†’ åˆ†å¸ƒå…¨ä½“ã®è©•ä¾¡ã«ã¯ä½¿ãˆãªã„ï¼ˆFID/ISã®è£œå®Œï¼‰ã€‚

**é™ç•Œ2: ç‰¹å¾´æŠ½å‡ºå™¨ã¸ã®ä¾å­˜**

VGG/AlexNetã¯ImageNetã§è¨“ç·´ â†’ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚¤ã‚¢ã‚¹ã€‚

**å¯¾ç­–**:
- ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®ç‰¹å¾´æŠ½å‡ºå™¨ã‚’è¨“ç·´
- è¤‡æ•°ã®ç‰¹å¾´æŠ½å‡ºå™¨ã§ensemble

:::message
**ãƒœã‚¹æˆ¦ã¸ã®æº–å‚™ 50% å®Œäº†**: LPIPSå®Œäº†ã€‚ã“ã“ã‹ã‚‰Precision-Recall, MMD/CMMDã®æ•°å¼ã¸ã€‚
:::

### 3.5 Precision-Recall for Generative Models å®Œå…¨å°å‡º

#### 3.5.1 å‹•æ©Ÿ â€” å“è³ªã¨å¤šæ§˜æ€§ã®åˆ†é›¢

**å•é¡Œ**: FID/ISã¯å“è³ªã¨å¤šæ§˜æ€§ã‚’å˜ä¸€ã‚¹ã‚³ã‚¢ã«é›†ç´„ â†’ ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒè¦‹ãˆãªã„ã€‚

**ä¾‹**:
- ãƒ¢ãƒ‡ãƒ«A: é«˜å“è³ªã ãŒå¤šæ§˜æ€§ä½ã„ï¼ˆmode collapseï¼‰
- ãƒ¢ãƒ‡ãƒ«B: å¤šæ§˜æ€§é«˜ã„ãŒã¼ã‚„ã‘ãŸç”»åƒ

FIDã ã‘ã§ã¯ã€ã©ã¡ã‚‰ãŒ"è‰¯ã„"ã‹åˆ¤æ–­ã§ããªã„ã€‚

**Precision-Recall** [^4] ã¯ã€**å“è³ªï¼ˆPrecisionï¼‰ã¨å¤šæ§˜æ€§ï¼ˆRecallï¼‰ã‚’åˆ†é›¢æ¸¬å®š**ã™ã‚‹ã€‚

#### 3.5.2 å®šç¾©ï¼ˆå¤šæ§˜ä½“ãƒ™ãƒ¼ã‚¹ï¼‰

**è¨­å®š**:
- çœŸç”»åƒã®ç‰¹å¾´: $\{f_r^{(i)}\}_{i=1}^{N_r}$ ï¼ˆInceptionç‰¹å¾´ï¼‰
- ç”Ÿæˆç”»åƒã®ç‰¹å¾´: $\{f_g^{(i)}\}_{i=1}^{N_g}$

**å¤šæ§˜ä½“ã®è¿‘ä¼¼**:

å„ã‚µãƒ³ãƒ—ãƒ« $f^{(i)}$ ã®å‘¨ã‚Šã« $k$-NN çƒã‚’æ§‹ç¯‰ â†’ å¤šæ§˜ä½“ã‚’è¿‘ä¼¼ã€‚

$$
\mathcal{M}_r = \bigcup_{i=1}^{N_r} B(f_r^{(i)}, r_k^{(i)})
$$

ã“ã“ã§ $r_k^{(i)}$ ã¯ $f_r^{(i)}$ ã® $k$-æœ€è¿‘å‚ã¾ã§ã®è·é›¢ã€‚

**Precision** (å“è³ª):

$$
\text{Precision} = \frac{1}{N_g} \sum_{i=1}^{N_g} \mathbb{1}[f_g^{(i)} \in \mathcal{M}_r]
$$

ã€Œç”Ÿæˆç”»åƒã®ã†ã¡ã€çœŸç”»åƒã®å¤šæ§˜ä½“ã«å«ã¾ã‚Œã‚‹å‰²åˆã€â†’ å“è³ªãŒé«˜ã„ã»ã©1ã«è¿‘ã„ã€‚

**Recall** (å¤šæ§˜æ€§):

$$
\text{Recall} = \frac{1}{N_r} \sum_{i=1}^{N_r} \mathbb{1}[f_r^{(i)} \in \mathcal{M}_g]
$$

ã€ŒçœŸç”»åƒã®ã†ã¡ã€ç”Ÿæˆç”»åƒã®å¤šæ§˜ä½“ã«å«ã¾ã‚Œã‚‹å‰²åˆã€â†’ å¤šæ§˜æ€§ãŒé«˜ã„ï¼ˆçœŸåˆ†å¸ƒã‚’ã‚«ãƒãƒ¼ï¼‰ã»ã©1ã«è¿‘ã„ã€‚

#### 3.5.3 ç›´æ„Ÿçš„ç†è§£

```mermaid
graph TD
    A[çœŸç”»åƒå¤šæ§˜ä½“ M_r] --> B[Precision<br/>ç”Ÿæˆç”»åƒãŒM_rã«ã©ã‚Œã ã‘å«ã¾ã‚Œã‚‹ã‹]
    C[ç”Ÿæˆç”»åƒå¤šæ§˜ä½“ M_g] --> D[Recall<br/>M_gãŒçœŸç”»åƒã‚’ã©ã‚Œã ã‘ã‚«ãƒãƒ¼ã™ã‚‹ã‹]
    B --> E["é«˜Precision<br/>= é«˜å“è³ª"]
    D --> F["é«˜Recall<br/>= é«˜å¤šæ§˜æ€§"]
    E & F --> G[ç†æƒ³: P=1, R=1]
    style E fill:#ffccbc
    style F fill:#c5e1a5
    style G fill:#b3e5fc
```

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:
- Precisionâ†‘, Recallâ†“: é«˜å“è³ªã ãŒå¤šæ§˜æ€§ä½ã„ï¼ˆmode collapseï¼‰
- Precisionâ†“, Recallâ†‘: å¤šæ§˜æ€§é«˜ã„ãŒå“è³ªä½ã„ï¼ˆã¼ã‚„ã‘ãŸç”»åƒï¼‰
- ç†æƒ³: Precision = Recall = 1

#### 3.5.4 å®Ÿè£…ã¨æ•°å¼å¯¾å¿œ

```julia
# Precision-Recall for generative models
using NearestNeighbors

function precision_recall(feats_real::Matrix{Float64},
                           feats_gen::Matrix{Float64}, k::Int=5)
    # feats: (n_samples, d_features)
    n_real = size(feats_real, 1)
    n_gen = size(feats_gen, 1)

    # Build k-NN trees
    tree_real = KDTree(feats_real')  # NearestNeighbors expects (d, n)
    tree_gen = KDTree(feats_gen')

    # Compute k-th nearest neighbor distances for manifold radius
    # Real manifold: r_k^(i) = distance to k-th NN in real data
    radii_real = zeros(n_real)
    for i in 1:n_real
        idxs, dists = knn(tree_real, feats_real[i,:], k+1)  # k+1 to exclude self
        radii_real[i] = dists[end]  # k-th NN distance
    end

    # Gen manifold
    radii_gen = zeros(n_gen)
    for i in 1:n_gen
        idxs, dists = knn(tree_gen, feats_gen[i,:], k+1)
        radii_gen[i] = dists[end]
    end

    # Precision: fraction of gen samples within real manifold
    precision_count = 0
    for i in 1:n_gen
        # Find nearest real sample
        idxs, dists = knn(tree_real, feats_gen[i,:], 1)
        nearest_idx = idxs[1]
        if dists[1] <= radii_real[nearest_idx]
            precision_count += 1
        end
    end
    precision = precision_count / n_gen

    # Recall: fraction of real samples within gen manifold
    recall_count = 0
    for i in 1:n_real
        idxs, dists = knn(tree_gen, feats_real[i,:], 1)
        nearest_idx = idxs[1]
        if dists[1] <= radii_gen[nearest_idx]
            recall_count += 1
        end
    end
    recall = recall_count / n_real

    return precision, recall
end

# Test with synthetic data
n_real = 200
n_gen = 200
d = 64

# Case 1: high quality, low diversity (mode collapse)
# Gen samples concentrated around a subset of real samples
feats_real_1 = randn(n_real, d)
feats_gen_1 = feats_real_1[1:50,:] .+ 0.1 .* randn(50, d)  # only 50 modes
feats_gen_1 = vcat(feats_gen_1, feats_gen_1[rand(1:50, 150),:])  # replicate to 200

p1, r1 = precision_recall(feats_real_1, feats_gen_1)
println("Case 1 (mode collapse): Precision=$(round(p1, digits=3)), Recall=$(round(r1, digits=3))")
println("Expected: high P, low R")

# Case 2: high diversity, low quality (noisy samples)
feats_gen_2 = feats_real_1 .+ 2.0 .* randn(n_real, d)  # far from real manifold but diverse
p2, r2 = precision_recall(feats_real_1, feats_gen_2)
println("Case 2 (noisy): Precision=$(round(p2, digits=3)), Recall=$(round(r2, digits=3))")
println("Expected: low P, high R (if noise covers broadly)")

# Case 3: ideal (perfect match)
feats_gen_3 = feats_real_1 .+ 0.01 .* randn(n_real, d)  # very close to real
p3, r3 = precision_recall(feats_real_1, feats_gen_3)
println("Case 3 (ideal): Precision=$(round(p3, digits=3)), Recall=$(round(r3, digits=3))")
println("Expected: high P, high R")
```

#### 3.5.5 Precision-Recallã®å¯è¦–åŒ–

**P-Ræ›²ç·š**: ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¾‹: temperature, truncationï¼‰ã‚’å¤‰ãˆãªãŒã‚‰Precision-Recallã‚’ãƒ—ãƒ­ãƒƒãƒˆã€‚

```julia
# Visualize P-R tradeoff (conceptual)
# Vary generation temperature â†’ observe P-R tradeoff
temperatures = [0.5, 0.7, 0.9, 1.0, 1.2, 1.5]
precisions = Float64[]
recalls = Float64[]

feats_real = randn(200, 64)

for temp in temperatures
    # Simulate: lower temp â†’ higher quality, lower diversity
    if temp < 1.0
        # Mode collapse simulation
        n_modes = Int(round(50 * temp))
        feats_gen = feats_real[1:n_modes,:] .+ (0.1/temp) .* randn(n_modes, 64)
        feats_gen = vcat(feats_gen, feats_gen[rand(1:n_modes, 200-n_modes),:])
    else
        # Higher diversity, lower quality
        feats_gen = feats_real .+ (temp-0.5) .* randn(200, 64)
    end

    p, r = precision_recall(feats_real, feats_gen)
    push!(precisions, p)
    push!(recalls, r)
end

println("Temperature vs Precision-Recall:")
for (i, temp) in enumerate(temperatures)
    println("T=$temp: P=$(round(precisions[i], digits=3)), R=$(round(recalls[i], digits=3))")
end
```

**è§£é‡ˆ**: P-Rå¹³é¢ä¸Šã§å³ä¸Šï¼ˆP=1, R=1ï¼‰ã«è¿‘ã„ã»ã©è‰¯ã„ã€‚

#### 3.5.6 Precision-Recallã®é™ç•Œ

**é™ç•Œ1: è¨ˆç®—ã‚³ã‚¹ãƒˆ**

k-NNæ¢ç´¢ã‚’å…¨ã‚µãƒ³ãƒ—ãƒ«ã§å®Ÿè¡Œ â†’ $O(N^2)$ or $O(N \log N)$ï¼ˆKD-treeä½¿ç”¨æ™‚ï¼‰ã€‚å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§é…ã„ã€‚

**é™ç•Œ2: $k$ ã®é¸æŠ**

$k$ï¼ˆæœ€è¿‘å‚æ•°ï¼‰ã«ã‚ˆã£ã¦çµæœãŒå¤‰ã‚ã‚‹ã€‚è«–æ–‡ [^4] ã§ã¯ $k=5$ ã‚’æ¨å¥¨ã€‚

**é™ç•Œ3: ç‰¹å¾´æŠ½å‡ºå™¨ã¸ã®ä¾å­˜**

Inceptionç‰¹å¾´ã«ä¾å­˜ â†’ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚¤ã‚¢ã‚¹ï¼ˆFIDã¨åŒã˜å•é¡Œï¼‰ã€‚

:::message
**ãƒœã‚¹æˆ¦ã¸ã®æº–å‚™ 70% å®Œäº†**: Precision-Recallå®Œäº†ã€‚æ®‹ã‚ŠMMD/CMMD â†’ ãƒœã‚¹æˆ¦ã¸ã€‚
:::

### 3.6 MMD (Maximum Mean Discrepancy) & CMMD å®Œå…¨å°å‡º

#### 3.6.1 MMDã®å‹•æ©Ÿ â€” ä»®å®šã®ãªã„åˆ†å¸ƒè·é›¢

**å•é¡Œ**: FIDã¯ã‚¬ã‚¦ã‚¹æ€§ã‚’ä»®å®š â†’ å¤šå³°åˆ†å¸ƒã§å¤±æ•—ã€‚ä»®å®šãªã—ã®åˆ†å¸ƒè·é›¢ãŒæ¬²ã—ã„ã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ã‚«ãƒ¼ãƒãƒ«æ³•ï¼ˆRKHS: Reproducing Kernel Hilbert Spaceï¼‰ã‚’ä½¿ã„ã€2ã¤ã®åˆ†å¸ƒã®**å¹³å‡åŸ‹ã‚è¾¼ã¿**ã®è·é›¢ã‚’æ¸¬ã‚‹ [^6]ã€‚

#### 3.6.2 RKHSã¨å¹³å‡åŸ‹ã‚è¾¼ã¿

**RKHS** (Reproducing Kernel Hilbert Space):

ã‚«ãƒ¼ãƒãƒ«é–¢æ•° $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ ã‹ã‚‰å®šç¾©ã•ã‚Œã‚‹ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆç©ºé–“ $\mathcal{H}$ã€‚

**ä»£è¡¨çš„ãªã‚«ãƒ¼ãƒãƒ«**:
- RBFã‚«ãƒ¼ãƒãƒ«ï¼ˆã‚¬ã‚¦ã‚·ã‚¢ãƒ³ã‚«ãƒ¼ãƒãƒ«ï¼‰: $k(x, y) = \exp(-\|x - y\|^2 / (2\sigma^2))$
- å¤šé …å¼ã‚«ãƒ¼ãƒãƒ«: $k(x, y) = (x^\top y + c)^d$

**å¹³å‡åŸ‹ã‚è¾¼ã¿** (Mean Embedding):

åˆ†å¸ƒ $P$ ã®å¹³å‡åŸ‹ã‚è¾¼ã¿ $\mu_P \in \mathcal{H}$ ã¯:

$$
\mu_P = \mathbb{E}_{x \sim P}[\phi(x)]
$$

ã“ã“ã§ $\phi: \mathcal{X} \to \mathcal{H}$ ã¯ã‚«ãƒ¼ãƒãƒ«ã«ã‚ˆã‚‹ç‰¹å¾´å†™åƒï¼ˆé€šå¸¸ã¯é™½ã«è¨ˆç®—ã—ãªã„ â€” kernel trickã§å†…ç©ã®ã¿è¨ˆç®—ï¼‰ã€‚

#### 3.6.3 MMDã®å®šç¾©

**å®šç¾©**:

$$
\text{MMD}^2(P, Q) = \|\mu_P - \mu_Q\|_{\mathcal{H}}^2
$$

**å±•é–‹** (kernel trick):

$$
\begin{aligned}
\text{MMD}^2(P, Q) &= \|\mu_P - \mu_Q\|^2 \\
&= \langle \mu_P - \mu_Q, \mu_P - \mu_Q \rangle_{\mathcal{H}} \\
&= \langle \mu_P, \mu_P \rangle + \langle \mu_Q, \mu_Q \rangle - 2\langle \mu_P, \mu_Q \rangle \\
&= \mathbb{E}_{x,x' \sim P}[k(x, x')] + \mathbb{E}_{y,y' \sim Q}[k(y, y')] - 2\mathbb{E}_{x \sim P, y \sim Q}[k(x, y)]
\end{aligned}
$$

**å®Ÿç”¨çš„ãªæ¨å®š** (empirical MMD):

$$
\widehat{\text{MMD}}^2 = \frac{1}{n^2}\sum_{i,j=1}^n k(x_i, x_j) + \frac{1}{m^2}\sum_{i,j=1}^m k(y_i, y_j) - \frac{2}{nm}\sum_{i=1}^n\sum_{j=1}^m k(x_i, y_j)
$$

ã“ã“ã§ $\{x_i\}_{i=1}^n \sim P$, $\{y_j\}_{j=1}^m \sim Q$ã€‚

#### 3.6.4 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰ 1:1å¯¾å¿œ (MMD)

```julia
# MMD implementation with RBF kernel
using Statistics

function rbf_kernel(x::Vector{Float64}, y::Vector{Float64}, Ïƒ::Float64=1.0)
    # k(x, y) = exp(-||x - y||Â² / (2ÏƒÂ²))
    return exp(-sum((x .- y).^2) / (2*Ïƒ^2))
end

function mmd_squared(X::Matrix{Float64}, Y::Matrix{Float64}, Ïƒ::Float64=1.0)
    # X: (n, d), Y: (m, d)
    n = size(X, 1)
    m = size(Y, 1)

    # E_{x,x'}[k(x, x')]
    kxx = 0.0
    for i in 1:n, j in 1:n
        kxx += rbf_kernel(X[i,:], X[j,:], Ïƒ)
    end
    kxx /= (n * n)

    # E_{y,y'}[k(y, y')]
    kyy = 0.0
    for i in 1:m, j in 1:m
        kyy += rbf_kernel(Y[i,:], Y[j,:], Ïƒ)
    end
    kyy /= (m * m)

    # E_{x,y}[k(x, y)]
    kxy = 0.0
    for i in 1:n, j in 1:m
        kxy += rbf_kernel(X[i,:], Y[j,:], Ïƒ)
    end
    kxy /= (n * m)

    # MMDÂ²
    mmd_sq = kxx + kyy - 2*kxy
    return max(0, mmd_sq)  # numerical stability
end

function mmd(X::Matrix{Float64}, Y::Matrix{Float64}, Ïƒ::Float64=1.0)
    return sqrt(mmd_squared(X, Y, Ïƒ))
end

# Test: identical distributions â†’ MMD â‰ˆ 0
X_test = randn(100, 32)
Y_test = randn(100, 32)
Y_test_same = X_test .+ 0.01 .* randn(100, 32)  # very similar

mmd_diff = mmd(X_test, Y_test)
mmd_same = mmd(X_test, Y_test_same)
println("MMD (different): $(round(mmd_diff, digits=4))")
println("MMD (similar): $(round(mmd_same, digits=6))")
println("Expected: MMD(similar) â‰ˆ 0")
```

#### 3.6.5 CMMD (CLIP-MMD) â€” FIDã®ä»£æ›¿ [^5]

**CMMD** (Jayasumana et al. 2024 [^5]) = MMD + CLIPåŸ‹ã‚è¾¼ã¿ã€‚

**å‹•æ©Ÿ**:
1. FIDã®æ­£è¦æ€§ä»®å®šã‚’æ’é™¤
2. CLIPç‰¹å¾´ â†’ ImageNetãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸›ã€ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆã«å¯¾å¿œ

**å®šç¾©**: CMMD = MMD over CLIP embeddings

$$
\text{CMMD}^2(P_r, P_g) = \text{MMD}^2(\text{CLIP}(P_r), \text{CLIP}(P_g))
$$

**CLIPã®åˆ©ç‚¹**:
- Vision-Languageäº‹å‰è¨“ç·´ â†’ ã‚ˆã‚Šæ±ç”¨çš„
- Text-to-Imageç”Ÿæˆã®è©•ä¾¡ã«ç›´æ¥å¯¾å¿œ
- äººé–“è©•ä¾¡ã¨ã®ç›¸é–¢ãŒFIDã‚ˆã‚Šé«˜ã„ [^5]

**å®Ÿé¨“çµæœ** [^5]:

| æŒ‡æ¨™ | Pearsonç›¸é–¢ï¼ˆäººé–“è©•ä¾¡ï¼‰ | ã‚µãƒ³ãƒ—ãƒ«æ•°ä¾å­˜æ€§ |
|:-----|:-----------------------|:----------------|
| FID | 0.56 | é«˜ï¼ˆ2000+å¿…è¦ï¼‰ |
| CMMD | **0.72** | ä½ï¼ˆ500ã§å®‰å®šï¼‰ |

#### 3.6.6 å®Ÿè£… (CMMD)

```julia
# CMMD implementation (with dummy CLIP embeddings)
function clip_embed_dummy(images::Vector{Matrix{Float64}})
    # Real impl: CLIP image encoder â†’ 512-dim
    n = length(images)
    return randn(n, 512)  # (n, 512)
end

function cmmd(real_images::Vector{Matrix{Float64}},
              gen_images::Vector{Matrix{Float64}}, Ïƒ::Float64=10.0)
    # Extract CLIP embeddings
    emb_real = clip_embed_dummy(real_images)  # (n, 512)
    emb_gen = clip_embed_dummy(gen_images)    # (m, 512)

    # Compute MMD
    return mmd(emb_real, emb_gen, Ïƒ)
end

# Test
real_imgs_cmmd = [randn(64, 64) for _ in 1:100]
gen_imgs_cmmd = [randn(64, 64) for _ in 1:100]
cmmd_score = cmmd(real_imgs_cmmd, gen_imgs_cmmd)
println("CMMD: $(round(cmmd_score, digits=4))")
println("Lower = more similar distributions")
```

#### 3.6.7 ã‚«ãƒ¼ãƒãƒ«é¸æŠã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\sigma$

**RBFã‚«ãƒ¼ãƒãƒ«ã® $\sigma$**:

$\sigma$ ãŒå°ã•ã„ â†’ å±€æ‰€çš„ãªé•ã„ã«æ•æ„Ÿ
$\sigma$ ãŒå¤§ãã„ â†’ å¤§åŸŸçš„ãªé•ã„ã®ã¿æ¤œå‡º

**ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯**: ãƒ‡ãƒ¼ã‚¿ã®ä¸­å¤®å€¤è·é›¢ï¼ˆmedian trickï¼‰ [^6]

$$
\sigma = \text{median}(\{\|x_i - x_j\| : i,j\})
$$

```julia
# Median heuristic for Ïƒ
function median_heuristic(X::Matrix{Float64})
    n = size(X, 1)
    dists = Float64[]
    # Subsample for efficiency
    n_samples = min(1000, n*(n-1)Ã·2)
    for _ in 1:n_samples
        i, j = rand(1:n, 2)
        if i != j
            push!(dists, sqrt(sum((X[i,:] .- X[j,:]).^2)))
        end
    end
    return median(dists)
end

# Test
X_test2 = randn(200, 64)
Ïƒ_auto = median_heuristic(X_test2)
println("Auto-selected Ïƒ (median heuristic): $(round(Ïƒ_auto, digits=2))")
```

:::message
**ãƒœã‚¹æˆ¦ã‚¯ãƒªã‚¢ æº–å‚™ 90% å®Œäº†**: MMD/CMMDã®ç†è«–ã‚’å®Œå…¨å°å‡ºã—ãŸã€‚ã“ã‚Œã§å…¨æŒ‡æ¨™ï¼ˆFID/IS/LPIPS/P&R/CMMDï¼‰ã®æ•°å¼åŸºç›¤ãŒæ•´ã£ãŸã€‚ã“ã“ã‹ã‚‰å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

### 3.7 âš”ï¸ Boss Battle: è«–æ–‡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¼ã‚’å®Œå…¨èª­è§£

**èª²é¡Œ**: CMMDè«–æ–‡ [^5] ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã‚’å®Œå…¨ç†è§£ã—ã€Juliaã§å†å®Ÿè£…ã›ã‚ˆã€‚

**è«–æ–‡æŠœç²‹** (Jayasumana et al. 2024 [^5], Algorithm 1 simplified):

```
Algorithm: CMMD Computation
Input: Real images I_r, Generated images I_g, CLIP model C, kernel bandwidth Ïƒ
Output: CMMD score

1. Extract CLIP embeddings:
   E_r = [C(img) for img in I_r]  # (n_r, 512)
   E_g = [C(img) for img in I_g]  # (n_g, 512)

2. Compute kernel matrices:
   K_rr[i,j] = k(E_r[i], E_r[j]; Ïƒ)
   K_gg[i,j] = k(E_g[i], E_g[j]; Ïƒ)
   K_rg[i,j] = k(E_r[i], E_g[j]; Ïƒ)

3. Compute MMDÂ²:
   MMDÂ² = mean(K_rr) + mean(K_gg) - 2*mean(K_rg)

4. Return CMMD = sqrt(max(0, MMDÂ²))
```

**å®Ÿè£…**:

```julia
# Boss Battle: Full CMMD implementation following paper
using LinearAlgebra, Statistics

function cmmd_paper(real_imgs::Vector{Matrix{Float64}},
                     gen_imgs::Vector{Matrix{Float64}})
    # Step 1: CLIP embeddings (dummy)
    E_r = clip_embed_dummy(real_imgs)  # (n_r, 512)
    E_g = clip_embed_dummy(gen_imgs)   # (n_g, 512)

    # Step 2: Auto-select Ïƒ via median heuristic
    Ïƒ = median_heuristic(vcat(E_r, E_g))

    # Step 3: Compute kernel matrices
    n_r, n_g = size(E_r, 1), size(E_g, 1)

    K_rr = zeros(n_r, n_r)
    for i in 1:n_r, j in 1:n_r
        K_rr[i,j] = rbf_kernel(E_r[i,:], E_r[j,:], Ïƒ)
    end

    K_gg = zeros(n_g, n_g)
    for i in 1:n_g, j in 1:n_g
        K_gg[i,j] = rbf_kernel(E_g[i,:], E_g[j,:], Ïƒ)
    end

    K_rg = zeros(n_r, n_g)
    for i in 1:n_r, j in 1:n_g
        K_rg[i,j] = rbf_kernel(E_r[i,:], E_g[j,:], Ïƒ)
    end

    # Step 4: MMDÂ²
    mmd_sq = mean(K_rr) + mean(K_gg) - 2*mean(K_rg)

    # Step 5: CMMD
    cmmd_val = sqrt(max(0, mmd_sq))

    return cmmd_val, Ïƒ
end

# Test
imgs_r_boss = [randn(64, 64) for _ in 1:50]
imgs_g_boss = [randn(64, 64) for _ in 1:50]
cmmd_boss, Ïƒ_boss = cmmd_paper(imgs_r_boss, imgs_g_boss)
println("âš”ï¸ Boss Battle: CMMD = $(round(cmmd_boss, digits=4)), Ïƒ = $(round(Ïƒ_boss, digits=2))")
println("âœ… Boss ã‚¯ãƒªã‚¢ï¼")
```

**æ¤œè¨¼**: åŒä¸€åˆ†å¸ƒã§ CMMD â‰ˆ 0 ã«ãªã‚‹ã‹ã€‚

```julia
# Sanity check: identical â†’ CMMD â‰ˆ 0
imgs_same = [randn(64, 64) for _ in 1:50]
imgs_same2 = [img .+ 0.01.*randn(64,64) for img in imgs_same]  # very similar
cmmd_same, _ = cmmd_paper(imgs_same, imgs_same2)
println("CMMD (near-identical): $(round(cmmd_same, digits=6)) â‰ˆ 0")
```

:::message
**ãƒœã‚¹æˆ¦ã‚¯ãƒªã‚¢ï¼ğŸ‰** å…¨æŒ‡æ¨™ã®æ•°å¼ã‚’å®Œå…¨å°å‡ºã—ã€è«–æ–‡ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã‚’å†å®Ÿè£…ã—ãŸã€‚
**é€²æ—: 50% å®Œäº†** æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚ã“ã“ã‹ã‚‰å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã¸ â€” Juliaçµ±è¨ˆåˆ†æ + Rust Criterion ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚
:::

---
