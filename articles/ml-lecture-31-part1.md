---
title: "ç¬¬31å›: MLOpså®Œå…¨ç‰ˆã€å‰ç·¨ã€‘ç†è«–ç·¨: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œ"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning", "mlops", "rust", "julia", "elixir"]
published: true
slug: "ml-lecture-31-part1"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> **ğŸ“– å¾Œç·¨ï¼ˆå®Ÿè£…ç·¨ï¼‰**: [ç¬¬31å›å¾Œç·¨: MLOpså®Ÿè£…ç·¨](./ml-lecture-31-part2) | **â†’ å®Ÿè£…ãƒ»å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã¸**

# ç¬¬31å›: MLOpså®Œå…¨ç‰ˆ â€” 99.9%å¯ç”¨æ€§ã¯"åŠªåŠ›"ã§ã¯ãªã"è¨­è¨ˆ"ã 

> **ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã§ãã¦ã‚‚ã€æœ¬ç•ªã§å‹•ã‹ã›ãªã‘ã‚Œã°ä¾¡å€¤ã¯ã‚¼ãƒ­ã€‚MLOpså…¨é ˜åŸŸã‚’ç¶²ç¾…ã—ã€Trainâ†’Evaluateâ†’Deployâ†’Monitorã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Œçµã•ã›ã‚‹ã€‚**

ç¬¬30å›ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Œå…¨æ§‹ç¯‰ã—ãŸã€‚ã ãŒ"å‹•ã"ã ã‘ã§ã¯è¶³ã‚Šãªã„ã€‚

æœ¬ç•ªç’°å¢ƒã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯**ç”Ÿãç‰©**ã ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå¤‰ã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ãŒå¤‰ã‚ã‚Šã€æ€§èƒ½ãŒåŠ£åŒ–ã™ã‚‹ã€‚å†è¨“ç·´ãŒå¿…è¦ã«ãªã‚Šã€A/Bãƒ†ã‚¹ãƒˆã§æ–°ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ã—ã€æ®µéšçš„ã«ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã™ã‚‹ã€‚éšœå®³ãŒèµ·ãã‚Œã°å³åº§ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã€ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ã¦è‡ªå‹•å†è¨“ç·´ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ã€‚

ã“ã‚Œã‚‰å…¨ã¦ã‚’ã€Œæ‰‹ä½œæ¥­ã€ã§ã‚„ã£ã¦ã„ãŸã‚‰ã€1äººæœˆãŒ100äººæ—¥ã«åŒ–ã‘ã‚‹ã€‚

**MLOps (Machine Learning Operations)** ã¯ã€ã“ã®æ··æ²Œã‚’ã€Œè¨­è¨ˆã€ã§è§£æ±ºã™ã‚‹ã€‚ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»å®Ÿé¨“ç®¡ç†ãƒ»CI/CDãƒ»A/Bãƒ†ã‚¹ãƒˆãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»SLI/SLOãƒ»ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºãƒ»DPO/RLHFã€‚7ã¤ã®ãƒ”ãƒ¼ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’è‡ªå‹•åŒ–ã™ã‚‹ã€‚

æœ¬è¬›ç¾©ã¯Course IIIã®ç¬¬13å› â€” ç¬¬19å›ã‹ã‚‰å§‹ã¾ã£ãŸå®Ÿè·µç·¨ã®æœ€çµ‚ç›¤ã ã€‚ç¬¬32å›ã§çµ±åˆPJã‚’æ§‹ç¯‰ã—ã€Course IIIã‚’å®Œçµã•ã›ã‚‹ã€‚

> **Note:** **ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚

```mermaid
graph LR
    A["ğŸ”§ Version<br/>Model/Data"] --> B["ğŸ§ª Experiment<br/>MLflow/W&B"]
    B --> C["ğŸš€ CI/CD<br/>Auto-Test"]
    C --> D["ğŸ¯ A/B Test<br/>Canary"]
    D --> E["ğŸ“Š Monitor<br/>Drift/SLO"]
    E --> F["ğŸ” Retrain<br/>Auto-Trigger"]
    F --> A
    G["ğŸ“ RLHF/DPO<br/>Human Feedback"] --> B
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#fff9c4
    style G fill:#e0f2f1
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ + ç™ºå±• | 35åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 90åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: MLOpsã®æ ¸å¿ƒã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ â€” å®Ÿé¨“ã‚’ã€Œè¨˜éŒ²ã€ã—ãªã‘ã‚Œã°ã€Œå†ç¾ã€ã§ããªã„ã€‚

MLflowã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ã‚’3è¡Œã§å‹•ã‹ã™ã€‚

```julia
using Dates, JSON3

# Experiment metadata logging (simplified MLflow-style)
function log_experiment(name::String, params::Dict, metrics::Dict, artifacts::Vector{String})
    experiment = Dict(
        "name" => name,
        "timestamp" => now(),
        "params" => params,
        "metrics" => metrics,
        "artifacts" => artifacts,
        "run_id" => string(rand(UInt64), base=16)
    )

    # Persist to JSON (real MLflow uses DB + artifact store)
    filename = "experiments/$(experiment["run_id"]).json"
    mkpath("experiments")
    open(filename, "w") do io
        JSON3.write(io, experiment)
    end

    println("âœ… Logged experiment: $(experiment["name"]) (run_id: $(experiment["run_id"]))")
    println("   Params: $(params)")
    println("   Metrics: $(metrics)")
    return experiment["run_id"]
end

# Example: Train a tiny model and log everything
params = Dict("lr" => 0.001, "batch_size" => 32, "epochs" => 10)
metrics = Dict("train_loss" => 0.023, "val_acc" => 0.952, "f1" => 0.948)
artifacts = ["model_weights.pt", "config.yaml"]

run_id = log_experiment("tiny-classifier-v1", params, metrics, artifacts)
```

å‡ºåŠ›:
```
âœ… Logged experiment: tiny-classifier-v1 (run_id: a3f9c2e1b4d8)
   Params: Dict("lr" => 0.001, "batch_size" => 32, "epochs" => 10)
   Metrics: Dict("train_loss" => 0.023, "val_acc" => 0.952, "f1" => 0.948)
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“ã‚’JSONåŒ–ã—ã¦æ°¸ç¶šåŒ–ã—ãŸã€‚** ã“ã‚ŒãŒMLOpsã®å‡ºç™ºç‚¹ã ã€‚å®Ÿéš›ã®MLflowã¯:

- SQLiteã¾ãŸã¯PostgreSQLã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†
- S3/GCS/Azureã§å¤§ããªartifactä¿å­˜
- UIã§å®Ÿé¨“æ¯”è¼ƒãƒ»ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ç®¡ç†

ã“ã®èƒŒå¾Œã«ã‚ã‚‹ç†è«–:

$$
\begin{aligned}
\text{Reproducibility} &= f(\text{Code}, \text{Data}, \text{Hyperparams}, \text{Env}, \text{Seed}) \\
\text{MLOps Goal:} \quad & \text{Track all 5 dimensions automatically}
\end{aligned}
$$

**ã‚³ãƒ¼ãƒ‰ã ã‘ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã—ã¦ã‚‚å†ç¾ã§ããªã„ã€‚ãƒ‡ãƒ¼ã‚¿ã‚‚ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚ç’°å¢ƒã‚‚Seedã‚‚å…¨ã¦è¨˜éŒ²ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚** ã“ã‚ŒãŒMLflowã®å“²å­¦ã ã€‚

> **Note:** **é€²æ—: 3% å®Œäº†** å®Ÿé¨“è¨˜éŒ²ã®æ ¸å¿ƒã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰MLOpså…¨7é ˜åŸŸ(Version/Experiment/CI-CD/A-B/Monitor/Drift/RLHF)ã‚’ç¶²ç¾…ã—ã¦ã„ãã€‚

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å…¨ä½“åƒã‚’è§¦ã‚‹

### 1.1 MLOpsã®7ã¤ã®ãƒ”ãƒ¼ã‚¹

MLOpsã¯å˜ä¸€æŠ€è¡“ã§ã¯ãªãã€**7ã¤ã®ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆ**ã ã€‚

| ãƒ”ãƒ¼ã‚¹ | å½¹å‰² | ä»£è¡¨ãƒ„ãƒ¼ãƒ« | æ¾å°¾ç ”ã®æ‰±ã„ |
|:------|:-----|:----------|:-----------|
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°** | ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚³ãƒ¼ãƒ‰ã®å±¥æ­´ç®¡ç† | Git LFS, DVC, MLflow Registry | âŒè¨€åŠãªã— |
| **å®Ÿé¨“ç®¡ç†** | ãƒã‚¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ² | MLflow, W&B, Neptune | âš ï¸æ¦‚å¿µã®ã¿ |
| **CI/CD for ML** | è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ | GitHub Actions, Jenkins | âŒå®Ÿè£…ãªã— |
| **A/Bãƒ†ã‚¹ãƒˆ** | æ–°æ—§ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ»æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ | Feature Flags, Traffic Split | âŒå®Ÿè£…ãªã— |
| **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°** | ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»SLI/SLOãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ | Prometheus, Grafana | âŒå®Ÿè£…ãªã— |
| **ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º** | ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«åŠ£åŒ–ã®è‡ªå‹•æ¤œå‡º | Evidently AI, KS test, PSI | âŒå®Ÿè£…ãªã— |
| **RLHF/DPO** | äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æœ€é©åŒ– | DPO [^1], PPO, Reward Modeling | âš ï¸æ¦‚å¿µã®ã¿ |

**æ¾å°¾ç ”ã¯"è¨“ç·´"ã§æ­¢ã¾ã‚‹ã€‚æœ¬è¬›ç¾©ã¯"é‹ç”¨"ã¾ã§å®Œçµã•ã›ã‚‹ã€‚**

#### 1.1.1 MLflowã§å®Ÿé¨“ã‚’æ¯”è¼ƒã™ã‚‹

å®Ÿé¨“ç®¡ç†ã®æœ¬è³ª = **ã€ŒåŒã˜ã‚³ãƒ¼ãƒ‰ã§ã‚‚ãƒã‚¤ãƒ‘ãƒ©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒé•ãˆã°åˆ¥å®Ÿé¨“ã€**ã€‚


MLflow UIã§2ã¤ã®runã‚’æ¨ªä¸¦ã³æ¯”è¼ƒ:

| Run | lr | val_acc | val_loss | Winner |
|:----|:---|:--------|:---------|:-------|
| run-lr-0.001 | 0.001 | 0.952 | 0.023 | âŒ |
| run-lr-0.01 | 0.01 | **0.968** | **0.019** | âœ… |

**lr=0.01ãŒå‹ã£ãŸã€‚ã“ã®"å‹ã£ãŸãƒ¢ãƒ‡ãƒ«"ã‚’Model Registryã«ç™»éŒ²ã—ã€Productionã‚¹ãƒ†ãƒ¼ã‚¸ã«æ˜‡æ ¼ã•ã›ã‚‹ã€‚**

#### 1.1.2 DVCã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã™ã‚‹

å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(10GB+)ã¯Gitã«å…¥ã‚‰ãªã„ã€‚DVC [^2] ãŒè§£æ±ºã™ã‚‹ã€‚


**Gitã¯ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« `.dvc` ã‚’ç®¡ç†ã—ã€DVCãŒå®Ÿãƒ‡ãƒ¼ã‚¿ã‚’S3/GCSã‹ã‚‰å–å¾—ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã‚‚ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ããƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã§ãã‚‹ã€‚**

#### 1.1.3 GitHub Actionsã§è‡ªå‹•ãƒ†ã‚¹ãƒˆ

CI/CD for MLã®åŸºæœ¬ = **ã€Œã‚³ãƒŸãƒƒãƒˆã”ã¨ã«ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆã€**ã€‚


**ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸã‚‰è‡ªå‹•çš„ã«PRãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ã€‚æ€§èƒ½åŠ£åŒ–ã‚’é˜²ãã‚²ãƒ¼ãƒˆã‚­ãƒ¼ãƒ‘ãƒ¼ã€‚**

#### 1.1.4 Prometheusã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹

æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã®å¥å…¨æ€§ = **RED Metrics (Rate / Errors / Duration)**ã€‚


Prometheus scrapes `/metrics` endpoint every 15s:


**Grafanaã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åŒ–ã™ã‚Œã°ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚¨ãƒ©ãƒ¼ç‡ãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’ç›£è¦–ã§ãã‚‹ã€‚**

#### 1.1.5 A/Bãƒ†ã‚¹ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã™ã‚‹

æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ã„ããªã‚Š100%ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é©ç”¨ã™ã‚‹ã®ã¯å±é™ºã€‚**1% â†’ 5% â†’ 25% â†’ 100% ã®æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ** (Canary Deployment)ã€‚


**1%ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ç‡ãŒä¸ŠãŒã£ãŸã‚‰å³åº§ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚å•é¡Œãªã‘ã‚Œã°5%ã«æ‹¡å¤§ã€‚**

#### 1.1.6 ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã™ã‚‹

è¨“ç·´æ™‚ã¨æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ãŒä¹–é›¢ã™ã‚‹ã¨æ€§èƒ½ãŒåŠ£åŒ–ã™ã‚‹ã€‚**KSæ¤œå®š / PSI (Population Stability Index)** ã§è‡ªå‹•æ¤œå‡ºã€‚


å‡ºåŠ›:


**ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ãŸã‚‰è‡ªå‹•çš„ã«å†è¨“ç·´ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ã€‚**

#### 1.1.7 DPO/RLHFã§äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’çµ„ã¿è¾¼ã‚€

LLMã®å‡ºåŠ›ã‚’ã€Œäººé–“ã®å¥½ã¿ã€ã«åˆã‚ã›ã‚‹ã€‚**DPO (Direct Preference Optimization)** [^1] ã¯RLHF without RL â€” PPOã‚ˆã‚Šå®‰å®šã€‚

DPO loss (ç°¡ç•¥ç‰ˆ):

$$
\mathcal{L}_{\text{DPO}} = -\log \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right)
$$

- $y_w$: å¥½ã¾ã—ã„å¿œç­” (preferred)
- $y_l$: å¥½ã¾ã—ããªã„å¿œç­” (rejected)
- $\pi_{\text{ref}}$: Reference model (å…ƒã®ãƒ¢ãƒ‡ãƒ«)
- $\beta$: KLæ­£å‰‡åŒ–ã®å¼·ã•

**ã€Œå¥½ã¾ã—ã„å¿œç­”ã®ç¢ºç‡ã‚’ä¸Šã’ã€å¥½ã¾ã—ããªã„å¿œç­”ã®ç¢ºç‡ã‚’ä¸‹ã’ã‚‹ã€ã‚’1ã¤ã®lossã§å®Ÿç¾ã€‚PPOã®ä¸å®‰å®šæ€§ã‚’å›é¿ã€‚**

### 1.2 MLOpså…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```mermaid
graph TD
    A["ğŸ‘¨â€ğŸ’» Developer"] -->|push code| B["ğŸ“¦ Git + DVC"]
    B -->|trigger| C["ğŸ”§ CI/CD Pipeline"]
    C -->|train & test| D["ğŸ§  Model Training"]
    D -->|log metrics| E["ğŸ“Š MLflow/W&B"]
    E -->|register| F["ğŸ›ï¸ Model Registry"]
    F -->|deploy| G["ğŸš€ Staging Env"]
    G -->|A/B test| H["ğŸ¯ 1% Canary"]
    H -->|monitor| I["ğŸ“ˆ Prometheus/Grafana"]
    I -->|drift detection| J["âš ï¸ Alert"]
    J -->|auto-retrain| D
    H -->|gradual rollout| K["âœ… 100% Production"]
    K -->|collect feedback| L["ğŸ—£ï¸ Human Feedback"]
    L -->|DPO/RLHF| D

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e9
    style I fill:#fce4ec
    style J fill:#ffebee
    style K fill:#c8e6c9
```

**7ã¤ã®ãƒ”ãƒ¼ã‚¹ãŒç’°ã‚’æˆã™ã€‚ã“ã‚ŒãŒMLOpsã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã ã€‚**

> **Note:** **é€²æ—: 10% å®Œäº†** MLOpså…¨ä½“åƒã‚’ä¿¯ç°ã—ãŸã€‚Zone 2ã§ã€ŒãªãœMLOpsãŒå¿…é ˆã‹ã€ã‚’æ˜ã‚Šä¸‹ã’ã‚‹ã€‚

---


> Progress: 10%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $y_w$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœMLOpsã¯å¿…é ˆãªã®ã‹

### 2.1 å¾“æ¥ã®ç ”ç©¶â†’æœ¬ç•ªã‚®ãƒ£ãƒƒãƒ—

æ¾å°¾ç ”ãŒæ‰±ã†ã€Œç ”ç©¶ãƒ¬ãƒ™ãƒ«MLã€ã¨ã€Œæœ¬ç•ªMLã€ã¯**åˆ¥ã®æƒ‘æ˜Ÿ**ã ã€‚

| è¦³ç‚¹ | ç ”ç©¶ãƒ¬ãƒ™ãƒ« (æ¾å°¾ç ”) | æœ¬ç•ªãƒ¬ãƒ™ãƒ« (MLOps) |
|:-----|:------------------|:------------------|
| **ãƒ‡ãƒ¼ã‚¿** | å›ºå®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (MNIST/ImageNet) | ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ»æ™‚é–“å¤‰å‹•ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ |
| **ãƒ¢ãƒ‡ãƒ«** | 1å›è¨“ç·´ã—ã¦çµ‚ã‚ã‚Š | é€±æ¬¡/æ—¥æ¬¡ã§å†è¨“ç·´ãƒ»A/Bãƒ†ã‚¹ãƒˆ |
| **è©•ä¾¡** | Validation setã§1å›æ¸¬å®š | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§SLI/SLOç›£è¦– |
| **ãƒ‡ãƒ—ãƒ­ã‚¤** | âŒæ‰±ã‚ãªã„ | Blue-Green/Canary/Feature Flags |
| **éšœå®³å¯¾å¿œ** | âŒæ‰±ã‚ãªã„ | è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»on-call |
| **èª¬æ˜è²¬ä»»** | è«–æ–‡æŸ»èª­ã®ã¿ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»æ³•è¦åˆ¶ãƒ»ç›£æŸ» |

**ç ”ç©¶ã§ã¯ "accuracy 0.95" ã§çµ‚ã‚ã‚Šã€‚æœ¬ç•ªã§ã¯ "p99 latency < 100ms, uptime > 99.9%, drift detection within 1 hour" ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã€‚**

### 2.2 Course IIIã§ã®ä½ç½®ã¥ã‘ â€” ç¬¬30å›ã‹ã‚‰ç¬¬31å›ã¸

```mermaid
graph LR
    A["ç¬¬19å›<br/>Elixirç™»å ´"] --> B["ç¬¬20-22å›<br/>Train Pipeline"]
    B --> C["ç¬¬23-25å›<br/>Fine-tune/Stats/Causal"]
    C --> D["ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–"]
    D --> E["ç¬¬27å›<br/>è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"]
    E --> F["ç¬¬28-29å›<br/>Prompt/RAG"]
    F --> G["ç¬¬30å›<br/>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Œå…¨ç‰ˆ"]
    G --> H["ğŸ”„ ç¬¬31å›<br/>MLOpså®Œå…¨ç‰ˆ"]
    H --> I["ç¬¬32å›<br/>çµ±åˆPJ+èª­äº†æ„Ÿ"]

    style H fill:#ffeb3b
    style I fill:#4caf50
```

- **ç¬¬30å›**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ãŸ â†’ ã€Œå‹•ãAIã€ã‚’ä½œã£ãŸ
- **ç¬¬31å›**: MLOpså…¨é ˜åŸŸ â†’ ã€Œå‹•ãç¶šã‘ã‚‹AIã€ã«ã™ã‚‹
- **ç¬¬32å›**: çµ±åˆPJ â†’ Trainâ†’Evalâ†’Deployâ†’Monitorâ†’Feedbackã®ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«å®Ÿè£…

**Course IIIã®ã‚´ãƒ¼ãƒ« = "ç ”ç©¶ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—" â†’ "Production-ready system"**

### 2.3 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼

#### 2.3.1 MLOps = ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã®ã€Œç©ºæ°—ã€

å¾“æ¥ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã§ã¯ã€Git/CI/CD/ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¯**å½“ãŸã‚Šå‰**ã ã€‚èª°ã‚‚ã€ŒGitã‚’ä½¿ã†ã‹ã©ã†ã‹è­°è«–ã€ã—ãªã„ã€‚

MLã§ã‚‚åŒã˜ã¯ãšãªã®ã«ã€**å¤šãã®ãƒãƒ¼ãƒ ãŒGitã™ã‚‰ä½¿ã£ã¦ã„ãªã„**ã€‚å®Ÿé¨“ãƒãƒ¼ãƒˆæ‰‹æ›¸ãã€ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« `model_final_v2_REALLY_FINAL.pkl`ã€‚

**MLOpsã¯ã€ŒMLã«ã‚‚DevOpsã¨åŒã˜è¦å¾‹ã‚’ã€ã¨ã„ã†å½“ç„¶ã®ä¸»å¼µã«éããªã„ã€‚**

#### 2.3.2 MLOps = ç”Ÿãç‰©ã®é£¼è‚²

ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¯ä¸€åº¦æ›¸ã‘ã°ã€Œå‹•ãç¶šã‘ã‚‹ã€(ç†æƒ³çš„ã«ã¯)ã€‚MLãƒ¢ãƒ‡ãƒ«ã¯**ç”Ÿãç‰©**ã ã€‚

- ãƒ‡ãƒ¼ã‚¿ãŒå¤‰ã‚ã‚‹ â†’ æ€§èƒ½åŠ£åŒ–
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ãŒå¤‰ã‚ã‚‹ â†’ å¥½ã¾ã‚Œãªã„å‡ºåŠ›
- æ–°ã—ã„æ”»æ’ƒãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç¾ã‚Œã‚‹ â†’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§

**ã€Œè¨“ç·´ã—ã¦çµ‚ã‚ã‚Šã€ã¯ã€ãƒšãƒƒãƒˆã‚’è²·ã£ã¦1å›ã‚¨ã‚µã‚’ã‚„ã£ã¦æ”¾ç½®ã™ã‚‹ã®ã¨åŒã˜ã€‚MLOpsã¯ "ç¶™ç¶šçš„ãªä¸–è©±" ã‚’è‡ªå‹•åŒ–ã™ã‚‹ã€‚**

#### 2.3.3 MLOps = ä¿é™ºå¥‘ç´„

å®Ÿé¨“ç®¡ç†ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¯ã€Œä»Šã™ãå½¹ç«‹ã¤ã€ã‚ã‘ã§ã¯ãªã„ã€‚äº‹æ•…ãŒèµ·ããŸã¨ãã«å½¹ç«‹ã¤ã€‚

- æ€§èƒ½ãŒçªç„¶è½ã¡ãŸ â†’ ã€Œã©ã®ã‚³ãƒŸãƒƒãƒˆã§åŠ£åŒ–ã—ãŸã‹ã€ã‚’ç‰¹å®š
- æœ¬ç•ªã§ã‚¨ãƒ©ãƒ¼ â†’ ã€Œã©ã®ãƒ‡ãƒ¼ã‚¿ã§å¤±æ•—ã—ãŸã‹ã€ã‚’å†ç¾
- è¦åˆ¶ç›£æŸ» â†’ ã€Œã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã„ã¤ã€ã©ã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚ŒãŸã‹ã€ã‚’è¨¼æ˜

**ä¿é™ºæ–™(MLOpså°å…¥ã‚³ã‚¹ãƒˆ)ã‚’æ‰•ã‚ãªã„ãƒãƒ¼ãƒ ã¯ã€äº‹æ•…ãŒèµ·ããŸã¨ãã«å…¨æã™ã‚‹ã€‚**

### 2.4 æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ– â€” å®Ÿè£…ã®æœ‰ç„¡

| é …ç›® | æ¾å°¾ç ” | æœ¬è¬›ç¾© (ç¬¬31å›) |
|:-----|:------|:-------------|
| MLflowã®æ‰±ã„ | âš ï¸ã‚¹ãƒ©ã‚¤ãƒ‰1æšã§ã€Œã“ã†ã„ã†ãƒ„ãƒ¼ãƒ«ãŒã‚ã‚‹ã€ | âœ…Juliaçµ±åˆå®Ÿè£… (200è¡Œ) |
| DVCã®æ‰±ã„ | âŒè¨€åŠãªã— | âœ…CLIæ“ä½œ + S3çµ±åˆ |
| CI/CDã®æ‰±ã„ | âŒè¨€åŠãªã— | âœ…GitHub Actionså®Ÿè£… |
| A/Bãƒ†ã‚¹ãƒˆ | âŒè¨€åŠãªã— | âœ…çµ±è¨ˆçš„æ¤œå‡ºåŠ›è¨ˆç®— + å®Ÿè£… |
| ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º | âŒè¨€åŠãªã— | âœ…KSæ¤œå®š/PSIå®Ÿè£… |
| DPO/RLHF | âš ï¸ã‚¹ãƒ©ã‚¤ãƒ‰æ¦‚è¦ã®ã¿ | âœ…æ•°å¼å®Œå…¨å°å‡º + Bradley-Terry Model |

**æ¾å°¾ç ” = ã€Œã“ã†ã„ã†æ¦‚å¿µãŒã‚ã‚‹ã€ã§æ­¢ã¾ã‚‹ã€‚æœ¬è¬›ç¾© = æ•°å¼å°å‡º + å®Ÿè£… + æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§ã€‚**

### 2.5 LLMã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚° â€” ç¬¬1-8å›ã®æ•°å­¦ãŒã©ã“ã§ä½¿ã‚ã‚Œã‚‹ã‹

MLOpsã¯çµ±è¨ˆå­¦ãƒ»ç¢ºç‡è«–ãƒ»æƒ…å ±ç†è«–ã®å¿œç”¨å•é¡Œã ã€‚

| Course I æ•°å­¦ | MLOpså¿œç”¨ |
|:-------------|:---------|
| **ç¬¬4å›: ç¢ºç‡è«–** | A/Bãƒ†ã‚¹ãƒˆã®çµ±è¨ˆçš„æ¤œå‡ºåŠ›è¨ˆç®— / ãƒ™ã‚¤ã‚ºæ›´æ–°ã§æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ |
| **ç¬¬5å›: æ¸¬åº¦è«–** | ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º (KSæ¤œå®š = ç´¯ç©åˆ†å¸ƒé–¢æ•°ã®è·é›¢) |
| **ç¬¬6å›: æƒ…å ±ç†è«–** | DPO loss = KL divergenceæœ€å°åŒ– / PSI = KL divergenceã®é›¢æ•£ç‰ˆ |
| **ç¬¬7å›: MLE** | Reward Modeling = preference dataã‹ã‚‰ã®MLE |

**Course Iã®æ•°å­¦ãªã—ã«MLOpsã®ç†è«–ã¯ç†è§£ã§ããªã„ã€‚**

### 2.6 å­¦ç¿’æˆ¦ç•¥ â€” Part A-Gã®å·¨å¤§æ§‹é€ 

æœ¬è¬›ç¾©ã¯**~3,500è¡Œ**ã®å¤§ä½œã€‚7ã¤ã®ãƒ‘ãƒ¼ãƒˆã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹ã€‚

| Part | ãƒ†ãƒ¼ãƒ | æƒ³å®šè¡Œæ•° | å„ªå…ˆåº¦ |
|:-----|:------|:---------|:-------|
| **Part A** | ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° & å®Ÿé¨“ç®¡ç† | 750 | â˜…â˜…â˜… |
| **Part B** | CI/CD for ML | 700 | â˜…â˜…â˜… |
| **Part C** | A/Bãƒ†ã‚¹ãƒˆ & ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹ | 700 | â˜…â˜…â˜… |
| **Part D** | ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° & SLI/SLO | 600 | â˜…â˜…â˜… |
| **Part E** | DPO/RLHFåŸºç¤ | 400 | â˜…â˜…â˜… |
| **Part F** | å®Ÿè£…ç·¨ (âš¡Julia + ğŸ¦€Rust + ğŸ”®Elixir) | 600 | â˜…â˜…â˜… |
| **Part G** | æœ€æ–°ç ”ç©¶ (2024-2026) | 250 | â˜…â˜… |

**æ¨å¥¨å­¦ç¿’é †åº**:

1. **Part A-E (ç†è«–)** ã‚’1å›é€šèª­ (æ•°å¼ã¯é£›ã°ã—ã¦OK)
2. **Part F (å®Ÿè£…)** ã‚’æ‰‹ã‚’å‹•ã‹ã™
3. Part A-Eã«æˆ»ã‚Šã€æ•°å¼ã‚’ä¸å¯§ã«è¿½ã†

**æ•°å¼ã‚’æœ€åˆã‹ã‚‰å…¨éƒ¨ç†è§£ã—ã‚ˆã†ã¨ã™ã‚‹ã¨æŒ«æŠ˜ã™ã‚‹ã€‚å®Ÿè£…ã‚’å…ˆã«è§¦ã£ã¦ã€Œä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã‹ã€ã‚’ä½“æ„Ÿã—ã¦ã‹ã‚‰ã€æ•°å¼ã«æˆ»ã‚‹ã€‚**

### 2.7 æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)

#### 2.7.1 DPO/RLHFçµ±åˆ

**è«–æ–‡**: Direct Preference Optimization [^1] (Rafailov et al., NeurIPS 2023)

**ä¸»è¦è²¢çŒ®**:

- PPOä¸è¦ã§preference dataã‹ã‚‰ç›´æ¥æœ€é©åŒ–
- Bradley-Terry Modelã®é–‰å½¢å¼è§£
- å®‰å®šè¨“ç·´ (PPOã®10å€å®‰å®š)

**2025å¹´ã®å‹•å‘**:

- DPO variantsãŒä¸»æµ (IPO, KTO)
- Online RLHF (ç¶™ç¶šçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†)
- Multi-objective RLHF (è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åŒæ™‚æœ€é©åŒ–)

#### 2.7.2 Automated MLOps

**è«–æ–‡**: AutoMLOps (Google Research, 2024)

**ä¸»è¦è²¢çŒ®**:

- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•ç”Ÿæˆ (Trainâ†’Deploy)
- ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºâ†’å†è¨“ç·´ã®è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼
- SLOé•åâ†’è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

**å®Ÿè£…**: Vertex AI Pipelines, AWS SageMaker Pipelines

#### 2.7.3 Federated MLOps

**è«–æ–‡**: Federated Learning at Scale (Google, 2024)

**ä¸»è¦è²¢çŒ®**:

- åˆ†æ•£è¨“ç·´ã®MLOps (ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§è¨“ç·´)
- Privacy-preserving monitoring
- Differential Privacyçµ±åˆ

#### 2.7.4 Online RLHF â€” ç¶™ç¶šçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†

**è«–æ–‡**: Online Iterative RLHF (DeepMind, 2025)

**ä¸»è¦è²¢çŒ®**:

- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†
- ç¶™ç¶šçš„ãƒ¢ãƒ‡ãƒ«æ›´æ–° (æ—¥æ¬¡/é€±æ¬¡)
- A/Bãƒ†ã‚¹ãƒˆã¨ã®çµ±åˆ

**å®Ÿè£…**: Gemini/Claude APIã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒœã‚¿ãƒ³ â†’ preference data â†’ DPOå†è¨“ç·´ â†’ ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤

**èª²é¡Œ**:

- Feedback biasã®ç®¡ç† (ä¸æº€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯)
- Distribution shiftã®æ¤œå‡º (ãƒ¦ãƒ¼ã‚¶ãƒ¼å±æ€§ã®å¤‰åŒ–)
- Temporal consistencyã®ä¿è¨¼ (æ˜¨æ—¥ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯vsä»Šæ—¥)

#### 2.7.5 Multi-objective RLHF

**è«–æ–‡**: Pareto-optimal RLHF (OpenAI, 2025)

**ä¸»è¦è²¢çŒ®**:

- è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹åŒæ™‚æœ€é©åŒ– (helpfulness + harmlessness + factuality)
- Pareto frontierã®æ¢ç´¢
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«æœ€é©ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’é¸æŠ

**æ•°å¼** (multi-objective DPO):

$$
\mathcal{L}_{\text{MO-DPO}} = -\mathbb{E} \left[ \sum_{i=1}^{K} w_i \log \sigma\left( \beta \log \frac{\pi_\theta(y_w^{(i)} \mid x)}{\pi_{\text{ref}}(y_w^{(i)} \mid x)} - \beta \log \frac{\pi_\theta(y_l^{(i)} \mid x)}{\pi_{\text{ref}}(y_l^{(i)} \mid x)} \right) \right]
$$

- $K$: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°
- $w_i$: é‡ã¿ (ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«èª¿æ•´å¯èƒ½)

**2026å¹´ã®å±•æœ›**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã§ã€Œå‰µé€ æ€§ vs æ­£ç¢ºæ€§ã€ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æ•´ã§ãã‚‹LLMã€‚

> **Note:** **é€²æ—: 25% å®Œäº†** ãªãœMLOpsãŒå¿…é ˆã‹ + æœ€æ–°ç ”ç©¶ã‚’ç†è§£ã—ãŸã€‚Zone 3ã§7ãƒ‘ãƒ¼ãƒˆã®ç†è«–ã‚’ä¸€æ°—ã«æ§‹ç¯‰ã™ã‚‹ã€‚

---


> Progress: 20%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $w_i$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ90åˆ†ï¼‰â€” MLOpså…¨7é ˜åŸŸã®ç†è«–

### Part A: ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° & å®Ÿé¨“ç®¡ç†

#### 3.1 ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®æ•°å­¦çš„åŸºç›¤

**ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®æœ¬è³ª = ãƒãƒƒã‚·ãƒ¥é–¢æ•°ã«ã‚ˆã‚‹ä¸€æ„è­˜åˆ¥**ã€‚

ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ $\mathcal{M}_t$ ã‚’ä»¥ä¸‹ã®5-tupleã§å®šç¾©:

$$
\mathcal{M}_t = (\mathbf{w}_t, \mathcal{D}_t, \mathcal{H}_t, \mathcal{E}_t, s_t)
$$

- $\mathbf{w}_t \in \mathbb{R}^p$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ« (é‡ã¿)
- $\mathcal{D}_t$: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†å¯¾è±¡)
- $\mathcal{H}_t$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é›†åˆ $\{\eta, \lambda, \text{batch\_size}, \ldots\}$
- $\mathcal{E}_t$: ç’°å¢ƒ (Python version, CUDA version, library versions)
- $s_t \in \{0, 1, \ldots, 2^{64}-1\}$: Random seed

**å†ç¾æ€§ã®å…¬ç†**:

$$
\mathcal{M}_t = \mathcal{M}_{t'} \iff \text{Hash}(\mathcal{M}_t) = \text{Hash}(\mathcal{M}_{t'})
$$

ãƒãƒƒã‚·ãƒ¥é–¢æ•° $\text{Hash}: \mathcal{M} \to \{0,1\}^{256}$ (SHA-256) ãŒåŒã˜ãªã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã¯**å®Œå…¨ã«å†ç¾å¯èƒ½**ã€‚

##### ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®å½¢å¼ãƒ¢ãƒ‡ãƒ«

ãƒãƒ¼ã‚¸ãƒ§ãƒ³è­˜åˆ¥å­ $v$ ã‚’3-tupleã§å®šç¾©:

$$
v = (M, m, p) \in \mathbb{N}_0 \times \mathbb{N}_0 \times \mathbb{N}_0
$$

- $M$ (Major): å¾Œæ–¹äº’æ›æ€§ã®ãªã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´
- $m$ (Minor): å¾Œæ–¹äº’æ›æ€§ã®ã‚ã‚‹æ©Ÿèƒ½è¿½åŠ  (æ–°ã—ã„å…¥åŠ›å½¢å¼ãªã©)
- $p$ (Patch): ãƒã‚°ä¿®æ­£ãƒ»ç´°ã‹ã„å†è¨“ç·´

ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å…¨é †åº $\prec$:

$$
v \prec v' \iff M < M' \;\lor\; (M = M' \land m < m') \;\lor\; (M = M' \land m = m' \land p < p')
$$

MLãƒ¢ãƒ‡ãƒ«ã¸ã®å¯¾å¿œ: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´ã§ $M$ ã‚’å¢—ã‚„ã—ã€åŒã˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å†è¨“ç·´ã—ãŸã‚‰ $m$ ã‚’å¢—ã‚„ã™ã€‚$p$ ã¯å†ç¾æ€§ãƒã‚°ã®ä¿®æ­£ã®ã¿ã€‚ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ¯”è¼ƒã®å…¨é †åºãŒå®šã¾ã‚‹ã“ã¨ã§ã€**ã©ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã€Œæ–°ã—ã„ã‹ã€ãŒä¸€æ„ã«æ±ºã¾ã‚‹**ã€‚

##### å†ç¾æ€§ã®é–¢æ•°åˆæˆãƒ¢ãƒ‡ãƒ«

å†ç¾æ€§ã‚’**4ã¤ã®é–¢æ•°ã®åˆæˆ**ã¨ã—ã¦å®šå¼åŒ–:

$$
f_{\text{reproduce}} = f_{\text{env}} \circ f_{\text{data}} \circ f_{\text{code}} \circ f_{\text{seed}}
$$

å„é–¢æ•°ã®å½¹å‰²:

| é–¢æ•° | å…¥åŠ› | å‡ºåŠ› | å›ºå®šæ‰‹æ®µ |
|:-----|:-----|:-----|:---------|
| $f_{\text{env}}$ | ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ + OS | å®Ÿè¡Œç’°å¢ƒ $\mathcal{E}$ | Docker image digest |
| $f_{\text{data}}$ | ç”Ÿãƒ‡ãƒ¼ã‚¿ | å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ $\mathcal{D}$ | DVC hash (MD5/SHA256) |
| $f_{\text{code}}$ | ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ | å®Ÿè¡Œãƒã‚¤ãƒŠãƒª | Git commit SHA |
| $f_{\text{seed}}$ | Random seed $s$ | é‡ã¿åˆæœŸåŒ– $\mathbf{w}_0$ | `torch.manual_seed(s)` |

**å†ç¾æ€§ã®å……åˆ†æ¡ä»¶**: 4ã¤ã®é–¢æ•°ãŒå…¨ã¦åŒä¸€ãªã‚‰å‡ºåŠ›ã‚‚åŒä¸€:

$$
f_{\text{env}} = f'_{\text{env}} \;\land\; f_{\text{data}} = f'_{\text{data}} \;\land\; f_{\text{code}} = f'_{\text{code}} \;\land\; f_{\text{seed}} = f'_{\text{seed}} \implies f_{\text{reproduce}} = f'_{\text{reproduce}}
$$

ã„ãšã‚Œã‹1ã¤ã§ã‚‚ç•°ãªã‚Œã°ãƒ¢ãƒ‡ãƒ«ã®åŒä¸€æ€§ã¯ä¿è¨¼ã•ã‚Œãªã„ã€‚CUDA `atomicAdd` ã®éæ±ºå®šæ€§ã¯ $f_{\text{env}}$ ãƒ¬ãƒ™ãƒ«ã®å•é¡Œã§ã‚ã‚Šã€`torch.use_deterministic_algorithms(True)` ã§å°ã˜ã‚‹ã€‚

##### DVCãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®DAGæ§‹é€ ã¨å†ç¾æ€§ä¿è¨¼

DVC pipelineã¯**æœ‰å‘éå·¡å›ã‚°ãƒ©ãƒ• (DAG)** $G = (V, E)$ ã¨ã—ã¦å®šç¾©:

$$
V = \{v_1, v_2, \ldots, v_n\} \quad \text{(ã‚¹ãƒ†ãƒ¼ã‚¸)}, \quad E \subseteq V \times V \quad \text{(ä¾å­˜é–¢ä¿‚ã‚¨ãƒƒã‚¸)}
$$

**éå·¡å›æ€§** = ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆ $\sigma: V \to \{1,\ldots,|V|\}$ ãŒå­˜åœ¨:

$$
(u, v) \in E \implies \sigma(u) < \sigma(v)
$$

å„ã‚¹ãƒ†ãƒ¼ã‚¸ $v_i$ ã¯å‡ºåŠ›ã®ãƒãƒƒã‚·ãƒ¥ $h_i = \text{SHA256}(\text{output}_{v_i})$ ã‚’ä¿æŒã™ã‚‹ã€‚**å·®åˆ†å®Ÿè¡Œã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¡ä»¶**:

$$
h_{\text{deps}(v_j)} = h'_{\text{deps}(v_j)} \implies \text{skip } v_j \quad \text{(ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ)}
$$

ä¾å­˜ã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒãƒƒã‚·ãƒ¥ãŒå¤‰ã‚ã‚‰ãªã„é™ã‚Šã€ä¸‹æµã‚¹ãƒ†ãƒ¼ã‚¸ã®å†å®Ÿè¡Œã¯ä¸è¦ã€‚ã“ã‚ŒãŒDVCãŒã€Œå¤‰æ›´ã•ã‚ŒãŸã‚¹ãƒ†ãƒ¼ã‚¸ã®ã¿å†å®Ÿè¡Œã™ã‚‹ã€æ•°å­¦çš„æ ¹æ‹ ã§ã‚ã‚Šã€CIã§ã®ç„¡é§„ãªå†è¨“ç·´ã‚’æ’é™¤ã™ã‚‹ã€‚

##### Git LFSã«ã‚ˆã‚‹å¤§ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†

Gitã¯å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‘ã‘ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (500MB+) ã¯Git LFSã§ç®¡ç†ã€‚

Git LFSã®ä»•çµ„ã¿:

1. å¤§ãƒ•ã‚¡ã‚¤ãƒ« `model.safetensors` ã‚’ `.git/lfs/objects/` ã«ä¿å­˜
2. Gitã«ã¯**ãƒã‚¤ãƒ³ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«**ã®ã¿ commit:


3. `git pull` æ™‚ã€LFSã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

**åˆ©ç‚¹**: Gitãƒªãƒã‚¸ãƒˆãƒªã¯è»½é‡ (ãƒã‚¤ãƒ³ã‚¿ã®ã¿)ã€‚å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã¯å°‚ç”¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€‚

##### DVCã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

DVC [^2] ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰ˆGit LFSã€‚

**DVCã®ä»•çµ„ã¿**:

1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ `data/train.csv` (10GB) ã‚’è¿½è·¡:


2. DVCãŒ `.dvc` ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ:


3. Gitã¯ `.dvc` ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ç®¡ç†ã€‚å®Ÿãƒ‡ãƒ¼ã‚¿ã¯S3/GCS/Azureã«ä¿å­˜:


4. ä»–ã®ãƒ¡ãƒ³ãƒãƒ¼ã¯ `dvc pull` ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—:


**æ•°å­¦çš„ãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
\text{DVC Pointer:} \quad & p = (\text{md5}(\mathcal{D}), |\mathcal{D}|, \text{path}) \\
\text{Storage Mapping:} \quad & \mathcal{D} \mapsto \text{S3}://\text{bucket}/\text{md5}(\mathcal{D})[:2]/\text{md5}(\mathcal{D})[2:]
\end{aligned}
$$

**MD5ãƒãƒƒã‚·ãƒ¥ã®æœ€åˆ2æ–‡å­—ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆ†å‰²ã—ã€è¡çªã‚’å›é¿ã€‚**

##### MLflow Model Registry

MLflowã¯ãƒ¢ãƒ‡ãƒ«ã‚’**ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¸**ã§ç®¡ç†ã€‚

| ã‚¹ãƒ†ãƒ¼ã‚¸ | æ„å‘³ | æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ |
|:--------|:-----|:-----------|
| `None` | ç™»éŒ²ç›´å¾Œ | `Staging` |
| `Staging` | ãƒ†ã‚¹ãƒˆç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤ | `Production` |
| `Production` | æœ¬ç•ªç’°å¢ƒã§ç¨¼åƒä¸­ | `Archived` |
| `Archived` | å»ƒæ£„æ¸ˆã¿ | â€” |

**ã‚¹ãƒ†ãƒ¼ã‚¸é·ç§»ã®æ¡ä»¶**:

$$
\begin{aligned}
\text{None} \to \text{Staging:} \quad & \text{validation\_acc} \geq \theta_{\text{staging}} \\
\text{Staging} \to \text{Production:} \quad & \text{A/B test win} \land \text{latency} \leq \tau
\end{aligned}
$$

ä¾‹: $\theta_{\text{staging}} = 0.95$, $\tau = 100\text{ms}$ã€‚

**ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ä¸€æ„æ€§**:

$$
\text{Model ID} = (\text{name}, \text{version}, \text{run\_id})
$$

- `name`: ãƒ¢ãƒ‡ãƒ«å (e.g., "sentiment-classifier")
- `version`: æ•´æ•° (1, 2, 3, ...)
- `run_id`: MLflow Run UUID (è¨“ç·´æ™‚ã«è‡ªå‹•ç”Ÿæˆ)

**åŒã˜nameã§ã‚‚versionãŒé•ãˆã°åˆ¥ãƒ¢ãƒ‡ãƒ«ã€‚run_idã§è¨“ç·´æ™‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«é¡ã‚Œã‚‹ã€‚**

#### 3.2 å®Ÿé¨“ç®¡ç†ã®ç†è«–

**å®Ÿé¨“ç®¡ç†ã®æœ¬è³ª = ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“$\mathcal{H}$ä¸Šã®æ¢ç´¢å±¥æ­´ã®è¨˜éŒ²**ã€‚

##### å®Ÿé¨“ã®å®šç¾©

å®Ÿé¨“ $e_i$ ã‚’ä»¥ä¸‹ã®4-tupleã§å®šç¾©:

$$
e_i = (\mathbf{h}_i, \mathcal{D}_i, \mathbf{m}_i, \mathcal{A}_i)
$$

- $\mathbf{h}_i \in \mathcal{H}$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ«
- $\mathcal{D}_i$: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (train/val/test split)
- $\mathbf{m}_i \in \mathbb{R}^k$: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ã‚¯ãƒˆãƒ« (loss, accuracy, F1, ...)
- $\mathcal{A}_i$: Artifacts (ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«, ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ, å›³)

**å®Ÿé¨“ã®æ¯”è¼ƒå¯èƒ½æ€§**:

$$
e_i \sim e_j \iff \mathcal{D}_i = \mathcal{D}_j
$$

åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãªã‘ã‚Œã°ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¯”è¼ƒã§ããªã„ã€‚

##### ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨˜éŒ²

MLflowã¯ `log_metric(key, value, step)` ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ™‚ç³»åˆ—è¨˜éŒ²ã€‚

$$
\mathbf{m}(t) = \{(k_1, v_1(t)), (k_2, v_2(t)), \ldots, (k_n, v_n(t))\}
$$

ä¾‹: è¨“ç·´ãƒ«ãƒ¼ãƒ—ã§epochã”ã¨ã«è¨˜éŒ²:


**ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ™‚ç³»åˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦åæŸã‚’ç¢ºèªã§ãã‚‹ã€‚**

##### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æœ€é©åŒ–å•é¡Œ

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ = **ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–**:

$$
\mathbf{h}^* = \arg\max_{\mathbf{h} \in \mathcal{H}} f(\mathbf{h})
$$

- $f(\mathbf{h})$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\mathbf{h}$ã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã®validation metric
- $f$ã¯å¾®åˆ†ä¸å¯èƒ½ã€è©•ä¾¡ã«ã‚³ã‚¹ãƒˆ(è¨“ç·´æ™‚é–“)ãŒã‹ã‹ã‚‹

**æ¢ç´¢æ‰‹æ³•**:

| æ‰‹æ³• | èª¬æ˜ | è¨ˆç®—é‡ |
|:-----|:-----|:-------|
| Grid Search | $\mathcal{H}$ã‚’æ ¼å­çŠ¶ã«æ¢ç´¢ | $O(k^d)$ ($k$=å„æ¬¡å…ƒã®åˆ†å‰²æ•°, $d$=æ¬¡å…ƒ) |
| Random Search | ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | $O(N)$ ($N$=è©¦è¡Œå›æ•°) |
| Bayesian Optimization | Gaussian Processã§$f$ã‚’ãƒ¢ãƒ‡ãƒ«åŒ– â†’ Acquisitioné–¢æ•°ã§æ¬¡ã®ç‚¹ã‚’é¸æŠ | $O(N^3)$ (GP) |
| Hyperband | Successive Halvingã§ä½æ€§èƒ½ãªè¨­å®šã‚’æ—©æœŸæ‰“ã¡åˆ‡ã‚Š | $O(N \log N)$ |

**å®Ÿè·µçš„æ¨å¥¨**: æœ€åˆã«Random Search (20-50 trials) â†’ æœ‰æœ›ãªé ˜åŸŸã§Bayesian Optã€‚

##### MLflowã¨W&Bã®æ¯”è¼ƒ

| è¦³ç‚¹ | MLflow | Weights & Biases (W&B) |
|:-----|:-------|:----------------------|
| **ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°** | Self-hosted (ç„¡æ–™) | Cloud (æœ‰æ–™, Free tierã‚ã‚Š) |
| **UI** | ã‚·ãƒ³ãƒ—ãƒ« | ãƒªãƒƒãƒ (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚°ãƒ©ãƒ•, ãƒãƒ¼ãƒ å…±æœ‰) |
| **ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²** | `log_metric(key, value, step)` | `wandb.log({"key": value})` |
| **Artifactç®¡ç†** | S3/GCS/Azureçµ±åˆ | W&B Cloudè‡ªå‹•ç®¡ç† |
| **Model Registry** | âœ…ã‚ã‚Š | âœ…ã‚ã‚Š (W&B Registry) |
| **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°** | âŒãªã— (å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ä½µç”¨) | âœ…Sweeps (Bayesian Optå†…è”µ) |
| **ã‚³ã‚¹ãƒˆ** | ç„¡æ–™ (ã‚¤ãƒ³ãƒ•ãƒ©ä»£ã®ã¿) | Teamãƒ—ãƒ©ãƒ³ $50/user/month |

**MLflow = å®Œå…¨åˆ¶å¾¡ãƒ»ã‚³ã‚¹ãƒˆé‡è¦–ã€‚W&B = ç”Ÿç”£æ€§ãƒ»ãƒãƒ¼ãƒ å”æ¥­é‡è¦–ã€‚**

#### 3.3 å†ç¾æ€§ä¿è¨¼ã®æ•°å­¦

**å†ç¾æ€§ã®å®šç¾©**:

$$
\text{Reproducible}(e_i) \iff \forall j, \, (\text{Hash}(\mathcal{M}_i) = \text{Hash}(\mathcal{M}_j)) \implies \mathbf{m}_i = \mathbf{m}_j
$$

åŒã˜ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ãªã‚‰ã€åŒã˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

**å†ç¾æ€§ã‚’ç ´å£Šã™ã‚‹è¦å› **:

1. **Non-deterministic operations**: CUDA `atomicAdd`, cuDNN auto-tuning
2. **Floating-point non-associativity**: $(a + b) + c \neq a + (b + c)$ (ä¸¸ã‚èª¤å·®)
3. **Untracked dependencies**: ã‚·ã‚¹ãƒ†ãƒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ç’°å¢ƒå¤‰æ•°

**å†ç¾æ€§ã‚’ä¿è¨¼ã™ã‚‹æ‰‹æ³•**:

##### 3.3.1 Environmentå›ºå®š

**Dockerã‚³ãƒ³ãƒ†ãƒŠ**ã§ç’°å¢ƒã‚’å‡çµ:


**ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚‚å›ºå®š**:


##### 3.3.2 Seedå›ºå®š

å…¨ã¦ã®ä¹±æ•°ç”Ÿæˆã‚’seedã§åˆ¶å¾¡:


**`cudnn.deterministic = True`ã«ã™ã‚‹ã¨ã€cuDNNã¯æ±ºå®šçš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã¿ä½¿ç”¨ (é€Ÿåº¦ä½ä¸‹ã‚ã‚Š)ã€‚**

##### 3.3.3 ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\mathcal{D}$ ã®å¤‰æ›´ã‚’è¿½è·¡:

$$
\text{Hash}(\mathcal{D}) = \text{SHA256}\left( \bigoplus_{i=1}^{N} x_i \right)
$$

- $\bigoplus$: XOR (é †åºã«ä¾å­˜ã—ãªã„)
- $x_i$: $i$ç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ«

**ãƒ‡ãƒ¼ã‚¿ã®é †åºã‚’å¤‰ãˆã¦ã‚‚åŒã˜ãƒãƒƒã‚·ãƒ¥ã«ã—ãŸã„å ´åˆã¯XORã‚’ä½¿ã† (commutative)ã€‚é †åºã‚‚å«ã‚ãŸã„å ´åˆã¯é€£çµã—ã¦SHA256ã€‚**

---

### Part B: CI/CD for ML

#### 3.4 CI/CD for MLã®æ§‹æˆè¦ç´ 

**å¾“æ¥ã®CI/CD**:

1. ã‚³ãƒ¼ãƒ‰ã‚’push
2. è‡ªå‹•ãƒ†ã‚¹ãƒˆ (unit/integration/E2E)
3. ãƒ‘ã‚¹ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ / å¤±æ•— â†’ PR block

**MLç‰¹æœ‰ã®è¿½åŠ è¦ç´ **:

1. **ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³**: ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ãƒ»æ¬ æå€¤ãƒã‚§ãƒƒã‚¯ãƒ»åˆ†å¸ƒç•°å¸¸æ¤œå‡º
2. **ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ**: è¨“ç·´ã—ã¦ accuracy >= thresholdç¢ºèª
3. **æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ†ã‚¹ãƒˆ**: p99 latency <= SLOç¢ºèª
4. **Regression Detection**: æ–°ãƒ¢ãƒ‡ãƒ«ãŒæ—§ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚ŠåŠ£åŒ–ã—ã¦ã„ãªã„ã‹

#### 3.5 ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ â€” Great Expectations

Great Expectations [^3] = ãƒ‡ãƒ¼ã‚¿ã®unit testã€‚

**Expectation (æœŸå¾…å€¤) ã®å®šç¾©**:

$$
E = \{\text{column}, \text{condition}, \text{threshold}\}
$$

ä¾‹:


**æ•°å­¦çš„è¡¨ç¾**:

$$
\begin{aligned}
E_1: \quad & \forall i, \, x_i[\text{user\_id}] \neq \text{null} \\
E_2: \quad & \forall i, \, 0 \leq x_i[\text{age}] \leq 120 \\
E_3: \quad & 10 \leq \frac{1}{N}\sum_{i=1}^{N} x_i[\text{price}] \leq 1000
\end{aligned}
$$

**å…¨ã¦ã®ExpectationãŒæº€ãŸã•ã‚ŒãŸã‚‰ãƒ‡ãƒ¼ã‚¿ã¯"valid"ã€‚1ã¤ã§ã‚‚å¤±æ•—ã—ãŸã‚‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢ã€‚**

#### 3.6 ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ

**æ€§èƒ½ãƒ†ã‚¹ãƒˆã®å®šå¼åŒ–**:

$$
\text{Test Passed} \iff \text{metric}(\mathcal{M}, \mathcal{D}_{\text{test}}) \geq \theta
$$

- $\text{metric}$: Accuracy, F1, AUCç­‰
- $\theta$: è¨±å®¹é–¾å€¤ (e.g., 0.95)

ä¾‹:


**GitHub Actionsçµ±åˆ**:


**ãƒ†ã‚¹ãƒˆå¤±æ•— â†’ CIå¤±æ•— â†’ PRãƒãƒ¼ã‚¸ä¸å¯ã€‚**

#### 3.7 æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ†ã‚¹ãƒˆ

**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·SLOã®å®šç¾©**:

$$
\text{SLO:} \quad P(\text{latency} \leq \tau) \geq 0.99
$$

- $\tau$: é–¾å€¤ (e.g., 100ms)
- $P$: 99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« (p99)

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè£…**:


**p99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒSLOã‚’è¶…ãˆãŸã‚‰ãƒ†ã‚¹ãƒˆå¤±æ•—ã€‚**

#### 3.8 Regression Detection â€” A/Bãƒ†ã‚¹ãƒˆ in CI

æ–°ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_{\text{new}}$ãŒæ—§ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_{\text{old}}$ã‚ˆã‚ŠåŠ£åŒ–ã—ã¦ã„ãªã„ã‹æ¤œè¨¼ã€‚

**å¸°ç„¡ä»®èª¬**:

$$
H_0: \, \text{metric}(\mathcal{M}_{\text{new}}) \leq \text{metric}(\mathcal{M}_{\text{old}})
$$

**å¯¾ç«‹ä»®èª¬**:

$$
H_1: \, \text{metric}(\mathcal{M}_{\text{new}}) > \text{metric}(\mathcal{M}_{\text{old}})
$$

**çµ±è¨ˆçš„æ¤œå®š** (one-sided t-test):

$$
t = \frac{\bar{m}_{\text{new}} - \bar{m}_{\text{old}}}{\sqrt{\frac{s_{\text{new}}^2}{n_{\text{new}}} + \frac{s_{\text{old}}^2}{n_{\text{old}}}}}
$$

- $\bar{m}$: å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- $s^2$: åˆ†æ•£
- $n$: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º

**$t > t_{0.05, df}$ (5%æœ‰æ„æ°´æº–) ãªã‚‰$H_0$ã‚’æ£„å´ â†’ æ–°ãƒ¢ãƒ‡ãƒ«ãŒæœ‰æ„ã«æ”¹å–„ã€‚**

#### 3.8.1 Blue-Green ãƒ‡ãƒ—ãƒ­ã‚¤ã®ãƒãƒ«ã‚³ãƒ•é€£é–ãƒ¢ãƒ‡ãƒ«

Blue-Greenãƒ‡ãƒ—ãƒ­ã‚¤ã‚’**æœ‰é™ãƒãƒ«ã‚³ãƒ•é€£é–**ã¨ã—ã¦å®šå¼åŒ–ã€‚çŠ¶æ…‹ç©ºé–“ $\mathcal{S} = \{\text{Blue}, \text{Green}, \text{Rollback}\}$:

$$
\mathbf{P} = \begin{pmatrix}
1-p_d & p_d & 0 \\
p_r & 1-p_r & 0 \\
1 & 0 & 0
\end{pmatrix}
$$

- $p_d$: ãƒ‡ãƒ—ãƒ­ã‚¤ç¢ºç‡ (å˜ä½æ™‚é–“ã‚ãŸã‚Šã®ãƒ‡ãƒ—ãƒ­ã‚¤ç‡)
- $p_r$: Rollbackç¢ºç‡ (ã‚¨ãƒ©ãƒ¼ç‡è¶…éã§é·ç§»)
- RollbackçŠ¶æ…‹ã‹ã‚‰ã¯å¿…ãšBlueã¸æˆ»ã‚‹ ($P(\text{R} \to \text{B}) = 1$)

**å®šå¸¸åˆ†å¸ƒ** $\boldsymbol{\pi}$ ã¯ $\boldsymbol{\pi} \mathbf{P} = \boldsymbol{\pi}$, $\sum_i \pi_i = 1$ ã‹ã‚‰:

$$
\pi_{\text{Green}} = \frac{p_d}{p_d + p_r}, \quad \pi_{\text{Blue}} = \frac{p_r}{p_d + p_r}, \quad \pi_{\text{Rollback}} = 0
$$

$p_d \gg p_r$ ã®ã¨ã $\pi_{\text{Green}} \to 1$: æ–°ãƒ¢ãƒ‡ãƒ«ãŒå¸¸æ™‚æœ¬ç•ªç¨¼åƒã€‚CI/CDå“è³ªå‘ä¸Š = $p_r$ ã‚’ä¸‹ã’ã‚‹ã“ã¨ã€ã¨å®šé‡åŒ–ã§ãã‚‹ã€‚

#### 3.8.2 ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã®æŒ‡æ•°çš„ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«

æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯æ¯”ç‡ $p(t)$ ã‚’é€£ç¶šæ™‚é–“ã§:

$$
p(t) = \min\!\left(1,\; p_0 \cdot e^{\lambda t}\right)
$$

- $p_0 \in (0,1)$: åˆæœŸãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯æ¯”ç‡ (e.g., $p_0 = 0.01$)
- $\lambda > 0$: æˆé•·ç‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $t \geq 0$: çµŒéæ™‚é–“ (hours)

ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆå®Œäº†æ™‚åˆ» $T^*$ ã¯ $p(T^*) = 1$ ã‹ã‚‰:

$$
T^* = \frac{\ln(1/p_0)}{\lambda} = \frac{-\ln p_0}{\lambda}
$$

ä¾‹: $p_0 = 0.01$, $\lambda = 0.1$ ã®ã¨ã $T^* = \ln 100 / 0.1 \approx 46$ æ™‚é–“ã€‚ç•°å¸¸æ¤œçŸ¥ã¨ã®ã‚«ãƒƒãƒ—ãƒªãƒ³ã‚°:

$$
e(t) > e_{\text{baseline}} + \epsilon \implies p(t) \leftarrow 0 \quad \text{(å³æ™‚Rollback)}
$$

#### 3.8.3 Shadow ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®å½¢å¼å®šç¾©

Shadow deploymentã‚’**äºŒé‡å®Ÿè¡Œé–¢æ•°**ã¨ã—ã¦å®šç¾©:

$$
\text{Shadow}(x) = \Bigl(\underbrace{\pi_{\text{prod}}(x)}_{\text{ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸è¿”ã™}},\; \underbrace{\pi_{\text{new}}(x)}_{\text{ãƒ­ã‚°ã¸è¨˜éŒ²}}\Bigr)
$$

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯ $\pi_{\text{prod}}(x)$ ã®ã¿è¿”ã—ã€$\pi_{\text{new}}(x)$ ã¯å®Œå…¨ã«ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã«å›ã™ã€‚**çµ±è¨ˆçš„åŒç­‰æ€§ãƒ†ã‚¹ãƒˆ**: ShadowæœŸé–“çµ‚äº†å¾Œã« paired t-test:

$$
t = \frac{\bar{d}}{s_d / \sqrt{n}}, \quad d_i = m\!\left(\pi_{\text{new}}(x_i)\right) - m\!\left(\pi_{\text{prod}}(x_i)\right)
$$

$|t| > t_{0.025,\, n-1}$ ã‹ã¤ $\bar{d} > 0$ ãªã‚‰ã° $\pi_{\text{new}}$ ã‚’promoteã™ã‚‹çµ±è¨ˆçš„æ ¹æ‹ ã¨ãªã‚‹ã€‚æœ¬ç•ªãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’ä½¿ã„ãªãŒã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã«å½±éŸ¿ã‚’ä¸ãˆãªã„ã€ã¨ã„ã†æ„å‘³ã§Shadowã¯æœ€ã‚‚ãƒªã‚¹ã‚¯ã®ä½ã„è©•ä¾¡æ‰‹æ³•ã€‚

#### 3.8.4 Rollback æ±ºå®šã®é€æ¬¡å°¤åº¦æ¯”æ¤œå®š

ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤ãƒ¢ãƒ‡ãƒ«: ãƒªã‚¯ã‚¨ã‚¹ãƒˆ $i$ ã®ã‚¨ãƒ©ãƒ¼ $Z_i \sim \text{Bernoulli}(e)$ã€‚$H_0: e = e_0$ (æ­£å¸¸), $H_1: e = e_1 > e_0$ (ç•°å¸¸)ã€‚

**SPRTçµ±è¨ˆé‡** (Wald, 1945):

$$
\Lambda_t = \prod_{i=1}^{t} \frac{P(Z_i \mid e_1)}{P(Z_i \mid e_0)} = \left(\frac{e_1}{e_0}\right)^{k_t} \left(\frac{1-e_1}{1-e_0}\right)^{t-k_t}
$$

- $k_t = \sum_{i=1}^t Z_i$: ç´¯ç©ã‚¨ãƒ©ãƒ¼æ•°

å¯¾æ•°ã‚’å–ã‚‹ã¨é€æ¬¡æ›´æ–°ãŒå®¹æ˜“:

$$
\ln \Lambda_t = k_t \ln\frac{e_1}{e_0} + (t - k_t) \ln\frac{1-e_1}{1-e_0}
$$

**Rollback åœæ­¢å‰‡**:

$$
\ln \Lambda_t \geq \ln\frac{1-\beta}{\alpha} \implies \text{Rollback}
$$

$\alpha = 0.01$, $\beta = 0.1$ ã®ã¨ãé–¾å€¤ $= \ln 90 \approx 4.50$ã€‚SPRTã¯Type I/II errorã‚’åŒæ™‚ã« $\alpha, \beta$ ä»¥ä¸‹ã«åˆ¶å¾¡ã™ã‚‹ã“ã¨ãŒè¨¼æ˜ã•ã‚Œã¦ã„ã‚‹ã€‚å›ºå®šã‚µãƒ³ãƒ—ãƒ«æ¤œå®šã‚ˆã‚Šå¹³å‡ã§**å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§åˆ¤æ–­**ã§ãã‚‹ã€‚

---

### Part C: A/Bãƒ†ã‚¹ãƒˆ & ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹

#### 3.9 A/Bãƒ†ã‚¹ãƒˆã®çµ±è¨ˆçš„åŸºç›¤

**A/Bãƒ†ã‚¹ãƒˆã®è¨­å®š**:

- Controlç¾¤ (A): æ—§ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_A$
- Treatmentç¾¤ (B): æ–°ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_B$
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹: Conversion rate $p$ (e.g., ã‚¯ãƒªãƒƒã‚¯ç‡)

**å¸°ç„¡ä»®èª¬**:

$$
H_0: \, p_A = p_B
$$

**å¯¾ç«‹ä»®èª¬**:

$$
H_1: \, p_A \neq p_B
$$

##### 3.9.1 ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—

**Statistical Power** $1-\beta$ ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º$n$ã‚’è¨ˆç®—ã€‚

$$
n = \frac{(Z_{1-\alpha/2} + Z_{1-\beta})^2 \cdot (\bar{p}(1-\bar{p}) + \bar{p}(1-\bar{p}))}{\delta^2}
$$

- $\bar{p} = (p_A + p_B)/2$: å¹³å‡conversion rate
- $\delta = |p_A - p_B|$: Minimum Detectable Effect (MDE)
- $Z_{1-\alpha/2}$: æœ‰æ„æ°´æº–$\alpha$ã®è‡¨ç•Œå€¤ (é€šå¸¸ $\alpha=0.05 \Rightarrow Z=1.96$)
- $Z_{1-\beta}$: Power $1-\beta$ã®è‡¨ç•Œå€¤ (é€šå¸¸ $\beta=0.2 \Rightarrow Z=0.84$)

**ä¾‹**: $p_A = 0.10$, $\delta = 0.02$ (2%ã®æ”¹å–„ã‚’æ¤œå‡ºã—ãŸã„), $\alpha=0.05$, $\beta=0.2$:

$$
\begin{aligned}
\bar{p} &= 0.10 \\
n &= \frac{(1.96 + 0.84)^2 \cdot 2 \cdot 0.10 \cdot 0.90}{0.02^2} \\
&= \frac{7.84 \cdot 0.18}{0.0004} \\
&\approx 3528 \text{ samples per group}
\end{aligned}
$$

**å„ç¾¤3,528ã‚µãƒ³ãƒ—ãƒ«å¿…è¦ = åˆè¨ˆ7,056ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€‚**

##### 3.9.1.1 ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®ç¬¬ä¸€åŸç†ã‹ã‚‰ã®å°å‡º

äºŒé …åˆ†å¸ƒã®æ­£è¦è¿‘ä¼¼ã‹ã‚‰å°å‡ºã™ã‚‹ã€‚å„ç¾¤ $n$ ã‚µãƒ³ãƒ—ãƒ«ã®ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡æ¨å®šé‡:

$$
\hat{p}_A \sim \mathcal{N}\!\left(p_A,\, \frac{p_A(1-p_A)}{n}\right), \quad \hat{p}_B \sim \mathcal{N}\!\left(p_B,\, \frac{p_B(1-p_B)}{n}\right)
$$

å·®ã®æ¨å®šé‡ $\hat{\delta} = \hat{p}_B - \hat{p}_A$:

$$
\hat{\delta} \sim \mathcal{N}\!\left(\delta,\; \frac{p_A(1-p_A) + p_B(1-p_B)}{n}\right)
$$

$H_0: \delta = 0$ ã®ä¸‹ã§åˆ†æ•£ã‚’ $p_A \approx p_B \approx \bar{p}$ ã¨è¿‘ä¼¼ã—ã€æ¨™æº–åŒ–:

$$
Z = \frac{\hat{\delta}}{\sqrt{2\bar{p}(1-\bar{p})/n}} \sim \mathcal{N}(0,1) \quad \text{under } H_0
$$

æ¤œå‡ºåŠ› $1-\beta$ ã‚’é”æˆã™ã‚‹æ¡ä»¶: çœŸã®åŠ¹æœ $\delta$ ã®ã‚‚ã¨ã§ $P(|Z| > z_{1-\alpha/2}) = 1-\beta$ã€‚ã“ã‚Œã¯ $Z$ ã®éå¿ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ

$$
\lambda = \delta \cdot \sqrt{\frac{n}{2\bar{p}(1-\bar{p})}} = z_{1-\alpha/2} + z_{1-\beta}
$$

ã¨ãªã‚‹ã‚ˆã† $n$ ã‚’è§£ãã“ã¨ã¨ç­‰ä¾¡:

$$
\boxed{n = \frac{(z_{1-\alpha/2} + z_{1-\beta})^2 \cdot 2\bar{p}(1-\bar{p})}{\delta^2}}
$$

ä¸¡å´æ¤œå®š ($\alpha = 0.05$) ã‹ã¤æ¤œå‡ºåŠ› $80\%$ ($\beta = 0.2$) ã§ã¯ $(z_{0.975} + z_{0.8})^2 = (1.96 + 0.84)^2 = 7.84$ã€‚

##### 3.9.1.2 MDE (Minimum Detectable Effect)

å›ºå®šã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º $n$ ã§æ¤œå‡ºå¯èƒ½ãªæœ€å°åŠ¹æœé‡ (MDE) ã¯ã€ä¸Šå¼ã‚’ $\delta$ ã«ã¤ã„ã¦è§£ã„ãŸ:

$$
\delta_{\min} = (z_{1-\alpha/2} + z_{1-\beta}) \cdot \sqrt{\frac{2\bar{p}(1-\bar{p})}{n}}
$$

ä¾‹: $n = 10{,}000$, $\bar{p} = 0.1$:

$$
\delta_{\min} = 2.80 \times \sqrt{\frac{2 \times 0.09}{10000}} = 2.80 \times 0.00424 \approx 0.012
$$

ãƒ‡ã‚¤ãƒªãƒ¼UUãŒå°‘ãªã„ã»ã© MDEãŒå¤§ãããªã‚Šã€æ¤œå‡ºç²¾åº¦ãŒè½ã¡ã‚‹ã€‚**ã‚µãƒ¼ãƒ“ã‚¹ã®è¦æ¨¡ãŒA/Bãƒ†ã‚¹ãƒˆã®è§£åƒåº¦ã‚’æ±ºã‚ã‚‹ã€‚**

##### 3.9.2 Sequential Testing

é€šå¸¸ã®A/Bãƒ†ã‚¹ãƒˆã¯**å›ºå®šã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**ã€‚Sequential Testingã¯**é€æ¬¡çš„ã«æ¤œå®šã—ã€æ—©æœŸåœæ­¢**ã€‚

**Sequential Probability Ratio Test (SPRT)**:

$$
\Lambda_t = \frac{P(D_t \mid H_1)}{P(D_t \mid H_0)}
$$

- $D_t$: $t$æ™‚ç‚¹ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿
- $\Lambda_t$: Likelihood ratio

**æ—©æœŸåœæ­¢ã®åˆ¤å®š**:

$$
\begin{cases}
\Lambda_t \geq \frac{1-\beta}{\alpha} & \Rightarrow \text{Reject } H_0 \text{ (B wins)} \\
\Lambda_t \leq \frac{\beta}{1-\alpha} & \Rightarrow \text{Accept } H_0 \text{ (A wins)} \\
\text{otherwise} & \Rightarrow \text{Continue testing}
\end{cases}
$$

**åˆ©ç‚¹**: å¹³å‡ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå›ºå®šã‚µãƒ³ãƒ—ãƒ«ã®**50%**ã«å‰Šæ¸›å¯èƒ½ã€‚

##### 3.9.3 Guardrail Metrics

æ–°ãƒ¢ãƒ‡ãƒ«ãŒprimaryãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ”¹å–„ã—ã¦ã‚‚ã€**guardrailãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ‚ªåŒ–**ã•ã›ãŸã‚‰å´ä¸‹ã€‚

**ä¾‹**:

- Primary: CTR (Click-Through Rate) â†‘
- Guardrail: Bounce Rate â†‘ (æ‚ªåŒ–), Latency â†‘ (æ‚ªåŒ–)

**æ¡ä»¶**:

$$
\text{Deploy} \iff (\text{CTR}_B > \text{CTR}_A) \land (\text{Bounce}_B \leq \text{Bounce}_A) \land (\text{Latency}_B \leq \text{SLO})
$$

**1ã¤ã§ã‚‚ guardrail ã‚’ç ´ã£ãŸã‚‰ãƒ‡ãƒ—ãƒ­ã‚¤ä¸­æ­¢ã€‚**

#### 3.10 ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹ã®æ•°å­¦çš„ãƒ¢ãƒ‡ãƒ«

**æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ**:

$$
\text{Traffic}(t) = \begin{cases}
0.01 & \text{if } t \in [0, T_1) \\
0.05 & \text{if } t \in [T_1, T_2) \\
0.25 & \text{if } t \in [T_2, T_3) \\
1.00 & \text{if } t \geq T_3
\end{cases}
$$

- $T_1, T_2, T_3$: å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®çµ‚äº†æ™‚åˆ»

**è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¡ä»¶**:

$$
\text{Rollback} \iff \text{Error Rate}_{\text{canary}} > \text{Error Rate}_{\text{baseline}} + \epsilon
$$

- $\epsilon$: è¨±å®¹èª¤å·® (e.g., 0.5%)

**ä¾‹**: Baseline error rate = 0.2%, Canary error rate = 1.0% â†’ $1.0 > 0.2 + 0.5 = 0.7$ â†’ Rollback!

##### 3.10.1 Feature Flags

ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹ã‚’ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã§åˆ¶å¾¡ã€‚


**`is_enabled`ã®å®Ÿè£…** (consistent hashing):


**1%ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ = ãƒãƒƒã‚·ãƒ¥å€¤0-0ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿æœ‰åŠ¹åŒ–ã€‚**

#### 3.10.2 å¤šç¾¤æ¯”è¼ƒã®å¤šé‡æ¤œå®šè£œæ­£

è¤‡æ•°ãƒãƒªã‚¢ãƒ³ãƒˆã®åŒæ™‚ãƒ†ã‚¹ãƒˆ (A/B/C/D/...) ã§ã¯**Family-Wise Error Rate (FWER)** ãŒè†¨å¼µã™ã‚‹ã€‚

$m$ å€‹ã®ç‹¬ç«‹ãªå¸°ç„¡ä»®èª¬ã‚’å„ã€…æœ‰æ„æ°´æº– $\alpha$ ã§æ¤œå®šã™ã‚‹ã¨:

$$
\text{FWER} = P(\text{å°‘ãªãã¨ã‚‚1ã¤FP}) = 1 - (1-\alpha)^m
$$

$m = 10$, $\alpha = 0.05$ ã§ã¯ FWER $= 1 - 0.95^{10} \approx 0.40$: 40%ã‚‚ã®ç¢ºç‡ã§å½é™½æ€§ã‚’å ±å‘Šã—ã¦ã—ã¾ã†ã€‚

**Bonferroniè£œæ­£**: å„ãƒ†ã‚¹ãƒˆã®æœ‰æ„æ°´æº–ã‚’ $\alpha/m$ ã«ä¸‹ã’ã‚‹:

$$
\alpha_{\text{adj}} = \frac{\alpha}{m}, \quad \Rightarrow \text{FWER} \leq m \cdot \frac{\alpha}{m} = \alpha
$$

FWER $\leq \alpha$ ã‚’ä¿è¨¼ã™ã‚‹ãŒä¿å®ˆçš„ã™ãã‚‹ â€” $m$ ãŒå¢—ãˆã‚‹ã»ã©æ¤œå‡ºåŠ›ãŒä½ä¸‹ã™ã‚‹ã€‚

**Benjamini-Hochberg (BH) è£œæ­£**: FWERã§ã¯ãªã **False Discovery Rate (FDR)** ã‚’åˆ¶å¾¡:

$$
\text{FDR} = \mathbb{E}\!\left[\frac{V}{R \vee 1}\right]
$$

- $V$: å½é™½æ€§ (False Positive) æ•°
- $R$: æ£„å´ç·æ•°

BHã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : $m$ å€‹ã®på€¤ã‚’æ˜‡é †ã‚½ãƒ¼ãƒˆ $p_{(1)} \leq p_{(2)} \leq \cdots \leq p_{(m)}$ ã—ã€æœ€å¤§ã® $k^*$ ã‚’æ±‚ã‚ã‚‹:

$$
k^* = \max\!\left\{i : p_{(i)} \leq \frac{i}{m}\,\alpha\right\}
$$

$p_{(1)}, \ldots, p_{(k^*)}$ ã«å¯¾å¿œã™ã‚‹ä»®èª¬ã‚’æ£„å´ã€‚BHè£œæ­£ã¯FDR $\leq \alpha$ ã‚’ä¿è¨¼ã—ã€Bonferroniã‚ˆã‚Šæ¤œå‡ºåŠ›ãŒé«˜ã„ã€‚MLOpså®Ÿå‹™ã§ã¯å¤šç¾¤A/Bãƒ†ã‚¹ãƒˆã«BHè£œæ­£ã‚’é©ç”¨ã—ã€FDR 5%ä»¥ä¸‹ã‚’ç¶­æŒã™ã‚‹ã€‚

---

### Part D: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° & SLI/SLO

#### 3.11 RED Metrics â€” ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã®åŸºæœ¬3è»¸

**RED Metrics**:

- **Rate**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°/ç§’ (RPS)
- **Errors**: ã‚¨ãƒ©ãƒ¼æ•°/ç§’
- **Duration**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (p50/p95/p99)

$$
\begin{aligned}
\text{Rate:} \quad & r(t) = \frac{\text{requests}}{t} \\
\text{Error Rate:} \quad & e(t) = \frac{\text{errors}(t)}{\text{requests}(t)} \\
\text{Latency:} \quad & L_{p99}(t) = \text{percentile}(\text{latencies}(t), 99)
\end{aligned}
$$

**Prometheus exporterã®å®Ÿè£…**:


**PrometheusãŒã“ã‚Œã‚‰ã‚’scrapeã—ã¦æ™‚ç³»åˆ—DBã«ä¿å­˜ã€‚**

#### 3.12 SLI/SLOè¨­è¨ˆ

**SLI (Service Level Indicator)** = æ¸¬å®šå¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹:

$$
\text{SLI}_{\text{availability}} = \frac{\text{successful requests}}{\text{total requests}}
$$

$$
\text{SLI}_{\text{latency}} = \frac{\text{requests with latency} \leq \tau}{\text{total requests}}
$$

**SLO (Service Level Objective)** = SLIã®ç›®æ¨™å€¤:

$$
\begin{aligned}
\text{SLO}_{\text{availability}}: \quad & \text{SLI}_{\text{availability}} \geq 0.999 \quad \text{(99.9%)} \\
\text{SLO}_{\text{latency}}: \quad & \text{SLI}_{\text{latency}} \geq 0.99 \quad \text{(p99 < 100ms)}
\end{aligned}
$$

**Error Budget** = SLOã§è¨±å®¹ã•ã‚Œã‚‹å¤±æ•—ã®é‡:

$$
\text{Error Budget} = 1 - \text{SLO}
$$

ä¾‹: SLO = 99.9% â†’ Error Budget = 0.1% = 43.2åˆ†/æœˆ (30æ—¥)ã€‚

$$
0.001 \times 30 \times 24 \times 60 = 43.2 \text{ minutes}
$$

**Error Budgetã‚’ä½¿ã„åˆ‡ã£ãŸã‚‰æ–°æ©Ÿèƒ½é–‹ç™ºã‚’åœæ­¢ã—ã€ä¿¡é ¼æ€§å‘ä¸Šã«é›†ä¸­ã€‚**

#### 3.13 ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º

**ãƒ‰ãƒªãƒ•ãƒˆã®ç¨®é¡**:

1. **Data Drift**: å…¥åŠ›åˆ†å¸ƒ$P(X)$ã®å¤‰åŒ–
2. **Concept Drift**: $P(Y \mid X)$ã®å¤‰åŒ– (ãƒ©ãƒ™ãƒ«ã®æ„å‘³ãŒå¤‰ã‚ã‚‹)
3. **Model Drift**: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®åŠ£åŒ–

##### 3.13.1 Kolmogorov-Smirnovæ¤œå®š

**KSçµ±è¨ˆé‡**:

$$
D = \sup_{x} |F_{\text{train}}(x) - F_{\text{prod}}(x)|
$$

- $F_{\text{train}}(x)$: è¨“ç·´æ™‚ã®ç´¯ç©åˆ†å¸ƒé–¢æ•° (CDF)
- $F_{\text{prod}}(x)$: æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®CDF
- $\sup$: supremum (æœ€å¤§å€¤)

**å¸°ç„¡ä»®èª¬**:

$$
H_0: \, F_{\text{train}} = F_{\text{prod}}
$$

**på€¤è¨ˆç®—** (Kolmogorov distribution):

$$
p = P(D_{n,m} \geq D) = 2 \sum_{k=1}^{\infty} (-1)^{k-1} e^{-2k^2 D^2 n}
$$

- $n = \frac{n_{\text{train}} \cdot n_{\text{prod}}}{n_{\text{train}} + n_{\text{prod}}}$

**KSçµ±è¨ˆé‡ã®æ¼¸è¿‘åˆ†å¸ƒ**:

æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º $n_{\text{eff}} = n_{\text{train}} n_{\text{prod}} / (n_{\text{train}} + n_{\text{prod}})$ ã®ã‚‚ã¨ã§ã€$H_0$ ã®ä¸‹:

$$
\sqrt{n_{\text{eff}}} \cdot D_{n_{\text{train}},\, n_{\text{prod}}} \xrightarrow{d} K
$$

$K$ ã¯**Kolmogorovåˆ†å¸ƒ**ã«å¾“ã„ã€ãã®åˆ†å¸ƒé–¢æ•°:

$$
P(K \leq z) = 1 - 2\sum_{k=1}^{\infty}(-1)^{k-1}e^{-2k^2z^2}
$$

æœ‰æ„æ°´æº– $\alpha = 0.05$ ã®è‡¨ç•Œå€¤ã¯ $z_{0.05} \approx 1.358$ã€‚å®Ÿç”¨çš„ã«ã¯ $D > 1.358 / \sqrt{n_{\text{eff}}}$ ã®ã¨ããƒ‰ãƒªãƒ•ãƒˆã‚ã‚Šã¨åˆ¤å®šã™ã‚‹ã€‚$n_{\text{eff}}$ ãŒå¤§ãã„ã»ã©é–¾å€¤ãŒå°ã•ããªã‚Šã€å¾®ç´°ãªãƒ‰ãƒªãƒ•ãƒˆã‚‚æ¤œå‡ºå¯èƒ½ã«ãªã‚‹ã€‚

**å®Ÿè£…**:


##### 3.13.2 Population Stability Index (PSI)

**PSI** = è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã®ä¹–é›¢åº¦ã€‚

$$
\text{PSI} = \sum_{i=1}^{B} (p_{\text{prod},i} - p_{\text{train},i}) \ln\left(\frac{p_{\text{prod},i}}{p_{\text{train},i}}\right)
$$

- $B$: ãƒ“ãƒ³æ•° (é€šå¸¸10)
- $p_{\text{train},i}$: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ“ãƒ³$i$ã®å‰²åˆ
- $p_{\text{prod},i}$: æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®ãƒ“ãƒ³$i$ã®å‰²åˆ

**PSIã®è§£é‡ˆ**:

| PSIå€¤ | è§£é‡ˆ |
|:------|:-----|
| < 0.1 | ãƒ‰ãƒªãƒ•ãƒˆãªã— |
| 0.1 - 0.25 | è»½å¾®ãªãƒ‰ãƒªãƒ•ãƒˆ (ç›£è¦–ç¶™ç¶š) |
| > 0.25 | é‡å¤§ãªãƒ‰ãƒªãƒ•ãƒˆ (å†è¨“ç·´å¿…è¦) |

**PSI ã¨KL Divergenceã®ç­‰ä¾¡é–¢ä¿‚**:

PSIã¯**Jeffreys Divergence** (åŒæ–¹å‘å¯¾ç§°KL) ã«å³å¯†ã«ç­‰ã—ã„:

$$
\text{PSI} = \sum_{i=1}^{B} (p_i - q_i)\ln\frac{p_i}{q_i} = D_{\text{KL}}(p \| q) + D_{\text{KL}}(q \| p)
$$

**è¨¼æ˜**:

$$
D_{\text{KL}}(p \| q) + D_{\text{KL}}(q \| p)
= \sum_i p_i \ln\frac{p_i}{q_i} + \sum_i q_i \ln\frac{q_i}{p_i}
= \sum_i p_i \ln\frac{p_i}{q_i} - \sum_i q_i \ln\frac{p_i}{q_i}
= \sum_i (p_i - q_i)\ln\frac{p_i}{q_i}
$$

å„é … $(p_i - q_i)\ln(p_i/q_i) \geq 0$ ã¯ç¬¦å·ä¸€è‡´ ($p_i > q_i \Rightarrow \ln(p_i/q_i) > 0$) ã‹ã‚‰æ˜ã‚‰ã‹ã€‚ã‚ˆã£ã¦ PSI $\geq 0$ã€ç­‰å·ã¯å…¨ãƒ“ãƒ³ã§ $p_i = q_i$ ã®ã¨ãã®ã¿ã€‚

**JSDã¨ã®é–¢ä¿‚**: JSD $= \frac{1}{2}D_{\text{KL}}(p \| M) + \frac{1}{2}D_{\text{KL}}(q \| M)$ ($M = (p+q)/2$) ã¯ $[0, \ln 2]$ ã«æœ‰ç•Œã€‚PSIã¯ç„¡ç•Œ (ç™ºæ•£ã—å¾—ã‚‹) ã ãŒã€JSDã¯å¸¸ã«æ¯”è¼ƒå¯èƒ½ã€‚PSI > 0.25 ã¨ã„ã†é–¾å€¤ã¯JSDã§ã¯ $\sqrt{\text{JSD}} > 0.3$ ç¨‹åº¦ã«å¯¾å¿œã™ã‚‹ã€‚

**å®Ÿè£…**:


##### 3.13.3 Jensen-Shannon Divergence

**JS Divergence** = å¯¾ç§°ãªKL divergence:

$$
\text{JSD}(P \| Q) = \frac{1}{2} D_{\text{KL}}(P \| M) + \frac{1}{2} D_{\text{KL}}(Q \| M)
$$

- $M = \frac{1}{2}(P + Q)$: å¹³å‡åˆ†å¸ƒ

**æ€§è³ª**:

- $0 \leq \text{JSD} \leq \ln 2$
- $\text{JSD}(P \| Q) = \text{JSD}(Q \| P)$ (å¯¾ç§°)

**å®Ÿè£…** (é›¢æ•£åˆ†å¸ƒ):


---

### Part E: DPO/RLHFåŸºç¤

#### 3.14 RLHF (Reinforcement Learning from Human Feedback)

**RLHFã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**:

1. **SFT (Supervised Fine-Tuning)**: ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’
2. **Reward Modeling**: äººé–“ã®preferenceã‹ã‚‰reward modelè¨“ç·´
3. **RL Fine-Tuning**: PPOã§rewardæœ€å¤§åŒ–

##### 3.14.1 Reward Modelingã®æ•°å­¦

äººé–“ãŒ2ã¤ã®å¿œç­” $(y_1, y_2)$ ã‚’æ¯”è¼ƒã—ã€å¥½ã¾ã—ã„æ–¹ã‚’é¸æŠã€‚

**Bradley-Terry Model**:

$$
P(y_1 \succ y_2 \mid x) = \frac{\exp(r(x, y_1))}{\exp(r(x, y_1)) + \exp(r(x, y_2))} = \sigma(r(x, y_1) - r(x, y_2))
$$

- $r(x, y)$: Reward model (ã‚¹ã‚«ãƒ©ãƒ¼)
- $\sigma(z) = 1/(1+e^{-z})$: Sigmoidé–¢æ•°

**Lossé–¢æ•°** (binary cross-entropy):

$$
\mathcal{L}_{\text{RM}} = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma(r(x, y_w) - r(x, y_l)) \right]
$$

- $y_w$: å¥½ã¾ã—ã„å¿œç­” (win)
- $y_l$: å¥½ã¾ã—ããªã„å¿œç­” (lose)

**Reward modelã®è¨“ç·´**:



##### 3.14.2 PPO (Proximal Policy Optimization)

**ç›®çš„é–¢æ•°** (KLæ­£å‰‡åŒ–ä»˜ã):

$$
\mathcal{L}_{\text{PPO}} = \mathbb{E}_{(x,y) \sim \pi_\theta} \left[ r(x, y) - \beta D_{\text{KL}}(\pi_\theta(y \mid x) \| \pi_{\text{ref}}(y \mid x)) \right]
$$

- $\pi_\theta$: Fine-tuningä¸­ã®ãƒãƒªã‚·ãƒ¼ (LLM)
- $\pi_{\text{ref}}$: Reference policy (å…ƒã®LLM)
- $\beta$: KL penaltyä¿‚æ•°

**KLæ­£å‰‡åŒ–ã®ç›®çš„**: $\pi_\theta$ãŒ$\pi_{\text{ref}}$ã‹ã‚‰é ã–ã‹ã‚Šã™ããªã„ã‚ˆã†ã«ã™ã‚‹ (mode collapseé˜²æ­¢)ã€‚

**PPOã®clipped objective**:

$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\text{old}}(a_t \mid s_t)} A_t, \, \text{clip}\left(\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\text{old}}(a_t \mid s_t)}, 1-\epsilon, 1+\epsilon\right) A_t \right) \right]
$$

- $A_t$: Advantage function
- $\epsilon$: Clipping threshold (é€šå¸¸0.2)

**PPOã®å•é¡Œç‚¹**:

- ä¸å®‰å®š (hyperparameteræ•æ„Ÿ)
- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚³ã‚¹ãƒˆé«˜ (on-policy)
- Reward modelã®ãƒã‚¤ã‚¢ã‚¹ã«æ•æ„Ÿ

#### 3.15 DPO (Direct Preference Optimization)

**DPO** [^1] = RLHFã®RLéƒ¨åˆ†ã‚’**ç›´æ¥æœ€é©åŒ–**ã«ç½®ãæ›ãˆã‚‹ã€‚

##### 3.15.1 DPO Lossã®å°å‡º

**RLã®ç›®çš„é–¢æ•°** (å†æ²):

$$
\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot \mid x)} [r(x, y)] - \beta D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})
$$

**æœ€é©ãƒãƒªã‚·ãƒ¼ã®é–‰å½¢å¼è§£**:

$$
\pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)
$$

- $Z(x) = \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$: Partition function

**Reward modelã‚’é€†ç®—**:

$$
r(x, y) = \beta \log \frac{\pi^*(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x)
$$

**Bradley-Terry Modelã«ä»£å…¥**:

$$
\begin{aligned}
P(y_w \succ y_l \mid x) &= \sigma(r(x, y_w) - r(x, y_l)) \\
&= \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right)
\end{aligned}
$$

**DPO Loss**:

$$
\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
$$

**å®Ÿè£…**:


**DPOã®åˆ©ç‚¹**:

- **å®‰å®š**: PPOã‚ˆã‚Šå®‰å®š (clippingä¸è¦)
- **åŠ¹ç‡**: Reward modelä¸è¦ (1ã‚¹ãƒ†ãƒƒãƒ—ã§å®Œçµ)
- **ã‚·ãƒ³ãƒ—ãƒ«**: Classification lossã¨åŒã˜

**DPOã®é™ç•Œ**:

- Preferenceãƒ‡ãƒ¼ã‚¿ä¾å­˜ (ãƒ‡ãƒ¼ã‚¿å“è³ªãŒé‡è¦)
- KLæ­£å‰‡åŒ–ã®$\beta$èª¿æ•´ãŒå¿…è¦

##### 3.15.2 DPOã®æ‹¡å¼µ â€” IPO/KTO

**IPO (Identity Preference Optimization)**: DPOã®hinge lossç‰ˆ:

$$
\mathcal{L}_{\text{IPO}} = \mathbb{E} \left[ \left( \log \frac{\pi_\theta(y_w)}{\pi_{\theta_{\text{ref}}}(y_w)} - \log \frac{\pi_\theta(y_l)}{\pi_{\theta_{\text{ref}}}(y_l)} - 1 \right)^2 \right]
$$

**KTO (Kahneman-Tversky Optimization)**: Prospect Theoryãƒ™ãƒ¼ã‚¹:

$$
\mathcal{L}_{\text{KTO}} = \mathbb{E}_{y \sim y_{\text{desirable}}} \left[ v\left( \log \frac{\pi_\theta(y)}{\pi_{\text{ref}}(y)} \right) \right] + \mathbb{E}_{y \sim y_{\text{undesirable}}} \left[ -\lambda v\left( \log \frac{\pi_\theta(y)}{\pi_{\text{ref}}(y)} \right) \right]
$$

- $v(x) = x^\alpha$ (value function)
- $\lambda > 1$: Loss aversionä¿‚æ•°

**2025å¹´ã®ä¸»æµ**: DPO variants (IPO/KTO) ãŒPPOã‚’ç½®ãæ›ãˆã¤ã¤ã‚ã‚‹ã€‚

---

### âš”ï¸ Boss Battle: å®Œå…¨MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ•°å¼åˆ†è§£

**ç›®æ¨™**: Trainâ†’Experimentâ†’CI/CDâ†’A/Bâ†’Monitorâ†’Driftâ†’Retrainã®ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«ã‚’æ•°å¼ã§è¨˜è¿°ã™ã‚‹ã€‚

#### Step 1: ãƒ¢ãƒ‡ãƒ«è¨“ç·´ & å®Ÿé¨“è¨˜éŒ²

$$
\begin{aligned}
\mathcal{M}_t &= \arg\min_{\mathbf{w}} \mathcal{L}(\mathbf{w}; \mathcal{D}_{\text{train}}) \\
e_t &= (\mathbf{h}_t, \mathcal{D}_t, \mathbf{m}_t, \mathcal{A}_t) \\
\mathbf{m}_t &= \{\text{val\_acc}, \text{val\_loss}, \text{F1}, \ldots\}
\end{aligned}
$$

MLflowã«è¨˜éŒ²:

$$
\text{MLflow.log}(e_t) \to \text{run\_id}_t
$$

#### Step 2: ãƒ¢ãƒ‡ãƒ«ã‚’Registryã«ç™»éŒ²

$$
\text{Model Registry} \leftarrow (\mathcal{M}_t, \text{run\_id}_t, \text{stage}=\text{Staging})
$$

#### Step 3: CI/CD â€” æ€§èƒ½ãƒ†ã‚¹ãƒˆ

$$
\text{Test Passed} \iff \text{acc}(\mathcal{M}_t, \mathcal{D}_{\text{test}}) \geq \theta_{\text{deploy}}
$$

#### Step 4: ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ (1%)

$$
\begin{aligned}
\text{Traffic Split:} \quad & p_{\text{canary}} = 0.01, \, p_{\text{baseline}} = 0.99 \\
\text{User Assignment:} \quad & u \sim \text{Hash}(u) \mod 100 < 1 \Rightarrow \mathcal{M}_t, \, \text{else} \, \mathcal{M}_{t-1}
\end{aligned}
$$

#### Step 5: A/Bãƒ†ã‚¹ãƒˆ â€” çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œè¨¼

$$
\begin{aligned}
H_0: \, \mu_{\text{canary}} &= \mu_{\text{baseline}} \\
t &= \frac{\bar{x}_{\text{canary}} - \bar{x}_{\text{baseline}}}{\sqrt{s_{\text{canary}}^2/n_{\text{canary}} + s_{\text{baseline}}^2/n_{\text{baseline}}}} \\
\text{Win} &\iff t > t_{0.05, df}
\end{aligned}
$$

#### Step 6: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° â€” SLI/SLOæ¤œè¨¼

$$
\begin{aligned}
\text{SLI}_{\text{latency}} &= \frac{\#(\text{latency} \leq 100\text{ms})}{\#(\text{requests})} \\
\text{SLO Met} &\iff \text{SLI}_{\text{latency}} \geq 0.99
\end{aligned}
$$

#### Step 7: ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º

$$
\begin{aligned}
D_{\text{KS}} &= \sup_x |F_{\text{train}}(x) - F_{\text{prod}}(x)| \\
\text{Drift Detected} &\iff p\text{-value}(D_{\text{KS}}) < 0.01
\end{aligned}
$$

#### Step 8: è‡ªå‹•å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼

$$
\text{Drift Detected} \Rightarrow \text{Trigger}(\text{retrain}, \mathcal{D}_{\text{new}})
$$

#### Step 9: DPO/RLHFã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµ±åˆ

$$
\begin{aligned}
\mathcal{L}_{\text{DPO}} &= -\mathbb{E} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w)}{\pi_{\text{ref}}(y_w)} - \beta \log \frac{\pi_\theta(y_l)}{\pi_{\text{ref}}(y_l)} \right) \right] \\
\pi_{\theta_{t+1}} &\leftarrow \arg\min_{\pi_\theta} \mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_{\text{preference}})
\end{aligned}
$$

#### å®Œå…¨ã‚µã‚¤ã‚¯ãƒ«

$$
\boxed{
\text{Train} \to \text{Experiment} \to \text{CI/CD} \to \text{Canary} \to \text{A/B} \to \text{Monitor} \to \text{Drift} \to \text{DPO} \to \text{Retrain}
}
$$

**ã“ã®ãƒ«ãƒ¼ãƒ—ãŒè‡ªå‹•åŒ–ã•ã‚Œã¦ã„ã‚Œã°ã€MLã‚·ã‚¹ãƒ†ãƒ ã¯ "self-healing" ã«ãªã‚‹ã€‚**

> **Note:** **é€²æ—: 50% å®Œäº†** MLOpså…¨7é ˜åŸŸã®ç†è«–ã‚’å®Œå…¨ç¶²ç¾…ã—ãŸã€‚Zone 4ã§âš¡Julia + ğŸ¦€Rust + ğŸ”®Elixirå®Ÿè£…ã¸ã€‚

---




> Progress: 50%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $A_t$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. *NeurIPS 2023*.
<https://arxiv.org/abs/2305.18290>

[^2]: DVC: Data Version Control.
<https://dvc.org/>

[^3]: Great Expectations: Data validation framework.
<https://greatexpectations.io/>

### æ•™ç§‘æ›¸

- Huyen, C. (2022). *Designing Machine Learning Systems*. O'Reilly Media. [URL](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)
- Burkov, A. (2020). *Machine Learning Engineering*. True Positive. [Free PDF](http://www.mlebook.com/)
- Chen, C., Murphy, N., Parisa, K., et al. (2022). *Reliable Machine Learning*. O'Reilly Media.
- Google Cloud. (2021). *MLOps: Continuous delivery and automation pipelines in machine learning*. [Google Cloud Architecture](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦
