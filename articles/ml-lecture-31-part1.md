---
title: "ç¬¬31å›: MLOpså®Œå…¨ç‰ˆã€å‰ç·¨ã€‘ç†è«–ç·¨: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œ""
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning", "mlops", "rust", "julia", "elixir"]
published: true
slug: "ml-lecture-31-part1"
---

# ç¬¬31å›: MLOpså®Œå…¨ç‰ˆ â€” 99.9%å¯ç”¨æ€§ã¯"åŠªåŠ›"ã§ã¯ãªã"è¨­è¨ˆ"ã 

> **ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã§ãã¦ã‚‚ã€æœ¬ç•ªã§å‹•ã‹ã›ãªã‘ã‚Œã°ä¾¡å€¤ã¯ã‚¼ãƒ­ã€‚MLOpså…¨é ˜åŸŸã‚’ç¶²ç¾…ã—ã€Trainâ†’Evaluateâ†’Deployâ†’Monitorã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Œçµã•ã›ã‚‹ã€‚**

ç¬¬30å›ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Œå…¨æ§‹ç¯‰ã—ãŸã€‚ã ãŒ"å‹•ã"ã ã‘ã§ã¯è¶³ã‚Šãªã„ã€‚

æœ¬ç•ªç’°å¢ƒã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯**ç”Ÿãç‰©**ã ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå¤‰ã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ãŒå¤‰ã‚ã‚Šã€æ€§èƒ½ãŒåŠ£åŒ–ã™ã‚‹ã€‚å†è¨“ç·´ãŒå¿…è¦ã«ãªã‚Šã€A/Bãƒ†ã‚¹ãƒˆã§æ–°ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ã—ã€æ®µéšçš„ã«ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã™ã‚‹ã€‚éšœå®³ãŒèµ·ãã‚Œã°å³åº§ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã€ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ã¦è‡ªå‹•å†è¨“ç·´ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ã€‚

ã“ã‚Œã‚‰å…¨ã¦ã‚’ã€Œæ‰‹ä½œæ¥­ã€ã§ã‚„ã£ã¦ã„ãŸã‚‰ã€1äººæœˆãŒ100äººæ—¥ã«åŒ–ã‘ã‚‹ã€‚

**MLOps (Machine Learning Operations)** ã¯ã€ã“ã®æ··æ²Œã‚’ã€Œè¨­è¨ˆã€ã§è§£æ±ºã™ã‚‹ã€‚ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»å®Ÿé¨“ç®¡ç†ãƒ»CI/CDãƒ»A/Bãƒ†ã‚¹ãƒˆãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»SLI/SLOãƒ»ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºãƒ»DPO/RLHFã€‚7ã¤ã®ãƒ”ãƒ¼ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’è‡ªå‹•åŒ–ã™ã‚‹ã€‚

æœ¬è¬›ç¾©ã¯Course IIIã®ç¬¬13å› â€” ç¬¬19å›ã‹ã‚‰å§‹ã¾ã£ãŸå®Ÿè·µç·¨ã®æœ€çµ‚ç›¤ã ã€‚ç¬¬32å›ã§çµ±åˆPJã‚’æ§‹ç¯‰ã—ã€Course IIIã‚’å®Œçµã•ã›ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ”§ Version<br/>Model/Data"] --> B["ğŸ§ª Experiment<br/>MLflow/W&B"]
    B --> C["ğŸš€ CI/CD<br/>Auto-Test"]
    C --> D["ğŸ¯ A/B Test<br/>Canary"]
    D --> E["ğŸ“Š Monitor<br/>Drift/SLO"]
    E --> F["ğŸ” Retrain<br/>Auto-Trigger"]
    F --> A
    G["ğŸ“ RLHF/DPO<br/>Human Feedback"] --> B
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
    style F fill:#fff9c4
    style G fill:#e0f2f1
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ + ç™ºå±• | 35åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 90åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: MLOpsã®æ ¸å¿ƒã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ â€” å®Ÿé¨“ã‚’ã€Œè¨˜éŒ²ã€ã—ãªã‘ã‚Œã°ã€Œå†ç¾ã€ã§ããªã„ã€‚

MLflowã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ã‚’3è¡Œã§å‹•ã‹ã™ã€‚

```julia
using Dates, JSON3

# Experiment metadata logging (simplified MLflow-style)
function log_experiment(name::String, params::Dict, metrics::Dict, artifacts::Vector{String})
    experiment = Dict(
        "name" => name,
        "timestamp" => now(),
        "params" => params,
        "metrics" => metrics,
        "artifacts" => artifacts,
        "run_id" => string(rand(UInt64), base=16)
    )

    # Persist to JSON (real MLflow uses DB + artifact store)
    filename = "experiments/$(experiment["run_id"]).json"
    mkpath("experiments")
    open(filename, "w") do io
        JSON3.write(io, experiment)
    end

    println("âœ… Logged experiment: $(experiment["name"]) (run_id: $(experiment["run_id"]))")
    println("   Params: $(params)")
    println("   Metrics: $(metrics)")
    return experiment["run_id"]
end

# Example: Train a tiny model and log everything
params = Dict("lr" => 0.001, "batch_size" => 32, "epochs" => 10)
metrics = Dict("train_loss" => 0.023, "val_acc" => 0.952, "f1" => 0.948)
artifacts = ["model_weights.pt", "config.yaml"]

run_id = log_experiment("tiny-classifier-v1", params, metrics, artifacts)
```

å‡ºåŠ›:
```
âœ… Logged experiment: tiny-classifier-v1 (run_id: a3f9c2e1b4d8)
   Params: Dict("lr" => 0.001, "batch_size" => 32, "epochs" => 10)
   Metrics: Dict("train_loss" => 0.023, "val_acc" => 0.952, "f1" => 0.948)
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§å®Ÿé¨“ã‚’JSONåŒ–ã—ã¦æ°¸ç¶šåŒ–ã—ãŸã€‚** ã“ã‚ŒãŒMLOpsã®å‡ºç™ºç‚¹ã ã€‚å®Ÿéš›ã®MLflowã¯:

- SQLiteã¾ãŸã¯PostgreSQLã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†
- S3/GCS/Azureã§å¤§ããªartifactä¿å­˜
- UIã§å®Ÿé¨“æ¯”è¼ƒãƒ»ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ç®¡ç†

ã“ã®èƒŒå¾Œã«ã‚ã‚‹ç†è«–:

$$
\begin{aligned}
\text{Reproducibility} &= f(\text{Code}, \text{Data}, \text{Hyperparams}, \text{Env}, \text{Seed}) \\
\text{MLOps Goal:} \quad & \text{Track all 5 dimensions automatically}
\end{aligned}
$$

**ã‚³ãƒ¼ãƒ‰ã ã‘ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã—ã¦ã‚‚å†ç¾ã§ããªã„ã€‚ãƒ‡ãƒ¼ã‚¿ã‚‚ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚ç’°å¢ƒã‚‚Seedã‚‚å…¨ã¦è¨˜éŒ²ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚** ã“ã‚ŒãŒMLflowã®å“²å­¦ã ã€‚

:::message
**é€²æ—: 3% å®Œäº†** å®Ÿé¨“è¨˜éŒ²ã®æ ¸å¿ƒã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰MLOpså…¨7é ˜åŸŸ(Version/Experiment/CI-CD/A-B/Monitor/Drift/RLHF)ã‚’ç¶²ç¾…ã—ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å…¨ä½“åƒã‚’è§¦ã‚‹

### 1.1 MLOpsã®7ã¤ã®ãƒ”ãƒ¼ã‚¹

MLOpsã¯å˜ä¸€æŠ€è¡“ã§ã¯ãªãã€**7ã¤ã®ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆ**ã ã€‚

| ãƒ”ãƒ¼ã‚¹ | å½¹å‰² | ä»£è¡¨ãƒ„ãƒ¼ãƒ« | æ¾å°¾ç ”ã®æ‰±ã„ |
|:------|:-----|:----------|:-----------|
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°** | ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚³ãƒ¼ãƒ‰ã®å±¥æ­´ç®¡ç† | Git LFS, DVC, MLflow Registry | âŒè¨€åŠãªã— |
| **å®Ÿé¨“ç®¡ç†** | ãƒã‚¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ² | MLflow, W&B, Neptune | âš ï¸æ¦‚å¿µã®ã¿ |
| **CI/CD for ML** | è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ | GitHub Actions, Jenkins | âŒå®Ÿè£…ãªã— |
| **A/Bãƒ†ã‚¹ãƒˆ** | æ–°æ—§ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ»æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ | Feature Flags, Traffic Split | âŒå®Ÿè£…ãªã— |
| **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°** | ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»SLI/SLOãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ | Prometheus, Grafana | âŒå®Ÿè£…ãªã— |
| **ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º** | ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«åŠ£åŒ–ã®è‡ªå‹•æ¤œå‡º | Evidently AI, KS test, PSI | âŒå®Ÿè£…ãªã— |
| **RLHF/DPO** | äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æœ€é©åŒ– | DPO [^1], PPO, Reward Modeling | âš ï¸æ¦‚å¿µã®ã¿ |

**æ¾å°¾ç ”ã¯"è¨“ç·´"ã§æ­¢ã¾ã‚‹ã€‚æœ¬è¬›ç¾©ã¯"é‹ç”¨"ã¾ã§å®Œçµã•ã›ã‚‹ã€‚**

#### 1.1.1 MLflowã§å®Ÿé¨“ã‚’æ¯”è¼ƒã™ã‚‹

å®Ÿé¨“ç®¡ç†ã®æœ¬è³ª = **ã€ŒåŒã˜ã‚³ãƒ¼ãƒ‰ã§ã‚‚ãƒã‚¤ãƒ‘ãƒ©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒé•ãˆã°åˆ¥å®Ÿé¨“ã€**ã€‚

```python
import mlflow

# Run 1: lr=0.001
with mlflow.start_run(run_name="run-lr-0.001"):
    mlflow.log_param("lr", 0.001)
    mlflow.log_param("batch_size", 32)
    mlflow.log_metric("val_acc", 0.952)
    mlflow.log_metric("val_loss", 0.023)

# Run 2: lr=0.01 (higher LR)
with mlflow.start_run(run_name="run-lr-0.01"):
    mlflow.log_param("lr", 0.01)
    mlflow.log_param("batch_size", 32)
    mlflow.log_metric("val_acc", 0.968)  # Better!
    mlflow.log_metric("val_loss", 0.019)

# UI: mlflow ui --backend-store-uri sqlite:///mlflow.db
```

MLflow UIã§2ã¤ã®runã‚’æ¨ªä¸¦ã³æ¯”è¼ƒ:

| Run | lr | val_acc | val_loss | Winner |
|:----|:---|:--------|:---------|:-------|
| run-lr-0.001 | 0.001 | 0.952 | 0.023 | âŒ |
| run-lr-0.01 | 0.01 | **0.968** | **0.019** | âœ… |

**lr=0.01ãŒå‹ã£ãŸã€‚ã“ã®"å‹ã£ãŸãƒ¢ãƒ‡ãƒ«"ã‚’Model Registryã«ç™»éŒ²ã—ã€Productionã‚¹ãƒ†ãƒ¼ã‚¸ã«æ˜‡æ ¼ã•ã›ã‚‹ã€‚**

#### 1.1.2 DVCã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã™ã‚‹

å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(10GB+)ã¯Gitã«å…¥ã‚‰ãªã„ã€‚DVC [^2] ãŒè§£æ±ºã™ã‚‹ã€‚

```bash
# Initialize DVC
dvc init

# Track large dataset (stores pointer in Git, actual data in remote storage)
dvc add data/train.csv
git add data/train.csv.dvc .gitignore
git commit -m "Track train.csv with DVC"

# Push data to remote (S3/GCS/Azure)
dvc remote add -d myremote s3://my-bucket/dvc-store
dvc push

# Checkout data version (like git checkout)
git checkout experiment-v2
dvc checkout  # Downloads data/train.csv version from experiment-v2
```

**Gitã¯ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« `.dvc` ã‚’ç®¡ç†ã—ã€DVCãŒå®Ÿãƒ‡ãƒ¼ã‚¿ã‚’S3/GCSã‹ã‚‰å–å¾—ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã‚‚ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ããƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã§ãã‚‹ã€‚**

#### 1.1.3 GitHub Actionsã§è‡ªå‹•ãƒ†ã‚¹ãƒˆ

CI/CD for MLã®åŸºæœ¬ = **ã€Œã‚³ãƒŸãƒƒãƒˆã”ã¨ã«ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆã€**ã€‚

```yaml
# .github/workflows/ml-ci.yml
name: ML CI/CD

on: [push, pull_request]

jobs:
  test-model:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run data validation tests
        run: pytest tests/test_data_quality.py

      - name: Train model and test performance
        run: |
          python train.py --config configs/test.yaml
          python evaluate.py --threshold 0.95  # Fail if accuracy < 95%

      - name: Test inference latency
        run: |
          python benchmark_latency.py --max-p99 100  # Fail if p99 > 100ms
```

**ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸã‚‰è‡ªå‹•çš„ã«PRãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ã€‚æ€§èƒ½åŠ£åŒ–ã‚’é˜²ãã‚²ãƒ¼ãƒˆã‚­ãƒ¼ãƒ‘ãƒ¼ã€‚**

#### 1.1.4 Prometheusã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹

æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ã®å¥å…¨æ€§ = **RED Metrics (Rate / Errors / Duration)**ã€‚

```python
from prometheus_client import Counter, Histogram, start_http_server
import time

# Define metrics
REQUEST_COUNT = Counter('model_requests_total', 'Total requests')
ERROR_COUNT = Counter('model_errors_total', 'Total errors')
LATENCY = Histogram('model_latency_seconds', 'Inference latency')

# Expose metrics on :8000/metrics
start_http_server(8000)

def predict(input_data):
    REQUEST_COUNT.inc()  # Increment request count
    start_time = time.time()

    try:
        # Model inference
        result = model.predict(input_data)
        LATENCY.observe(time.time() - start_time)  # Record latency
        return result
    except Exception as e:
        ERROR_COUNT.inc()  # Increment error count
        raise e
```

Prometheus scrapes `/metrics` endpoint every 15s:

```
# HELP model_requests_total Total requests
# TYPE model_requests_total counter
model_requests_total 15234.0

# HELP model_errors_total Total errors
# TYPE model_errors_total counter
model_errors_total 12.0

# HELP model_latency_seconds Inference latency
# TYPE model_latency_seconds histogram
model_latency_seconds_bucket{le="0.05"} 12000.0
model_latency_seconds_bucket{le="0.1"} 14800.0
model_latency_seconds_bucket{le="+Inf"} 15234.0
model_latency_seconds_sum 876.3
model_latency_seconds_count 15234.0
```

**Grafanaã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åŒ–ã™ã‚Œã°ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚¨ãƒ©ãƒ¼ç‡ãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’ç›£è¦–ã§ãã‚‹ã€‚**

#### 1.1.5 A/Bãƒ†ã‚¹ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã™ã‚‹

æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ã„ããªã‚Š100%ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é©ç”¨ã™ã‚‹ã®ã¯å±é™ºã€‚**1% â†’ 5% â†’ 25% â†’ 100% ã®æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ** (Canary Deployment)ã€‚

```python
import random

def route_traffic(user_id):
    # Hash user_id for consistent assignment
    hash_val = hash(user_id) % 100

    if hash_val < 1:  # 1% to canary (new model)
        return "model_v2"
    else:  # 99% to baseline (old model)
        return "model_v1"

def predict_with_ab(user_id, input_data):
    model_version = route_traffic(user_id)

    if model_version == "model_v2":
        result = model_v2.predict(input_data)
        log_metric("model_v2_requests", 1)
    else:
        result = model_v1.predict(input_data)
        log_metric("model_v1_requests", 1)

    return result
```

**1%ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ç‡ãŒä¸ŠãŒã£ãŸã‚‰å³åº§ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚å•é¡Œãªã‘ã‚Œã°5%ã«æ‹¡å¤§ã€‚**

#### 1.1.6 ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã™ã‚‹

è¨“ç·´æ™‚ã¨æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ãŒä¹–é›¢ã™ã‚‹ã¨æ€§èƒ½ãŒåŠ£åŒ–ã™ã‚‹ã€‚**KSæ¤œå®š / PSI (Population Stability Index)** ã§è‡ªå‹•æ¤œå‡ºã€‚

```python
from scipy.stats import ks_2samp
import numpy as np

# Training data distribution (baseline)
train_feature = np.random.normal(0, 1, 10000)

# Production data distribution (could drift over time)
prod_feature = np.random.normal(0.5, 1.2, 1000)  # Mean shift + variance increase

# Kolmogorov-Smirnov test
statistic, p_value = ks_2samp(train_feature, prod_feature)

if p_value < 0.01:  # Significant drift detected
    print(f"âš ï¸ Data drift detected! KS statistic: {statistic:.4f}, p-value: {p_value:.4e}")
    trigger_retraining()
else:
    print(f"âœ… No drift. KS statistic: {statistic:.4f}, p-value: {p_value:.4f}")
```

å‡ºåŠ›:
```
âš ï¸ Data drift detected! KS statistic: 0.2341, p-value: 3.42e-12
```

**ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ãŸã‚‰è‡ªå‹•çš„ã«å†è¨“ç·´ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ã€‚**

#### 1.1.7 DPO/RLHFã§äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’çµ„ã¿è¾¼ã‚€

LLMã®å‡ºåŠ›ã‚’ã€Œäººé–“ã®å¥½ã¿ã€ã«åˆã‚ã›ã‚‹ã€‚**DPO (Direct Preference Optimization)** [^1] ã¯RLHF without RL â€” PPOã‚ˆã‚Šå®‰å®šã€‚

DPO loss (ç°¡ç•¥ç‰ˆ):

$$
\mathcal{L}_{\text{DPO}} = -\log \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right)
$$

- $y_w$: å¥½ã¾ã—ã„å¿œç­” (preferred)
- $y_l$: å¥½ã¾ã—ããªã„å¿œç­” (rejected)
- $\pi_{\text{ref}}$: Reference model (å…ƒã®ãƒ¢ãƒ‡ãƒ«)
- $\beta$: KLæ­£å‰‡åŒ–ã®å¼·ã•

**ã€Œå¥½ã¾ã—ã„å¿œç­”ã®ç¢ºç‡ã‚’ä¸Šã’ã€å¥½ã¾ã—ããªã„å¿œç­”ã®ç¢ºç‡ã‚’ä¸‹ã’ã‚‹ã€ã‚’1ã¤ã®lossã§å®Ÿç¾ã€‚PPOã®ä¸å®‰å®šæ€§ã‚’å›é¿ã€‚**

### 1.2 MLOpså…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```mermaid
graph TD
    A["ğŸ‘¨â€ğŸ’» Developer"] -->|push code| B["ğŸ“¦ Git + DVC"]
    B -->|trigger| C["ğŸ”§ CI/CD Pipeline"]
    C -->|train & test| D["ğŸ§  Model Training"]
    D -->|log metrics| E["ğŸ“Š MLflow/W&B"]
    E -->|register| F["ğŸ›ï¸ Model Registry"]
    F -->|deploy| G["ğŸš€ Staging Env"]
    G -->|A/B test| H["ğŸ¯ 1% Canary"]
    H -->|monitor| I["ğŸ“ˆ Prometheus/Grafana"]
    I -->|drift detection| J["âš ï¸ Alert"]
    J -->|auto-retrain| D
    H -->|gradual rollout| K["âœ… 100% Production"]
    K -->|collect feedback| L["ğŸ—£ï¸ Human Feedback"]
    L -->|DPO/RLHF| D

    style A fill:#e3f2fd
    style D fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e9
    style I fill:#fce4ec
    style J fill:#ffebee
    style K fill:#c8e6c9
```

**7ã¤ã®ãƒ”ãƒ¼ã‚¹ãŒç’°ã‚’æˆã™ã€‚ã“ã‚ŒãŒMLOpsã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã ã€‚**

:::message
**é€²æ—: 10% å®Œäº†** MLOpså…¨ä½“åƒã‚’ä¿¯ç°ã—ãŸã€‚Zone 2ã§ã€ŒãªãœMLOpsãŒå¿…é ˆã‹ã€ã‚’æ˜ã‚Šä¸‹ã’ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœMLOpsã¯å¿…é ˆãªã®ã‹

### 2.1 å¾“æ¥ã®ç ”ç©¶â†’æœ¬ç•ªã‚®ãƒ£ãƒƒãƒ—

æ¾å°¾ç ”ãŒæ‰±ã†ã€Œç ”ç©¶ãƒ¬ãƒ™ãƒ«MLã€ã¨ã€Œæœ¬ç•ªMLã€ã¯**åˆ¥ã®æƒ‘æ˜Ÿ**ã ã€‚

| è¦³ç‚¹ | ç ”ç©¶ãƒ¬ãƒ™ãƒ« (æ¾å°¾ç ”) | æœ¬ç•ªãƒ¬ãƒ™ãƒ« (MLOps) |
|:-----|:------------------|:------------------|
| **ãƒ‡ãƒ¼ã‚¿** | å›ºå®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (MNIST/ImageNet) | ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ»æ™‚é–“å¤‰å‹•ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ |
| **ãƒ¢ãƒ‡ãƒ«** | 1å›è¨“ç·´ã—ã¦çµ‚ã‚ã‚Š | é€±æ¬¡/æ—¥æ¬¡ã§å†è¨“ç·´ãƒ»A/Bãƒ†ã‚¹ãƒˆ |
| **è©•ä¾¡** | Validation setã§1å›æ¸¬å®š | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§SLI/SLOç›£è¦– |
| **ãƒ‡ãƒ—ãƒ­ã‚¤** | âŒæ‰±ã‚ãªã„ | Blue-Green/Canary/Feature Flags |
| **éšœå®³å¯¾å¿œ** | âŒæ‰±ã‚ãªã„ | è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»on-call |
| **èª¬æ˜è²¬ä»»** | è«–æ–‡æŸ»èª­ã®ã¿ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»æ³•è¦åˆ¶ãƒ»ç›£æŸ» |

**ç ”ç©¶ã§ã¯ "accuracy 0.95" ã§çµ‚ã‚ã‚Šã€‚æœ¬ç•ªã§ã¯ "p99 latency < 100ms, uptime > 99.9%, drift detection within 1 hour" ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã€‚**

### 2.2 Course IIIã§ã®ä½ç½®ã¥ã‘ â€” ç¬¬30å›ã‹ã‚‰ç¬¬31å›ã¸

```mermaid
graph LR
    A["ç¬¬19å›<br/>Elixirç™»å ´"] --> B["ç¬¬20-22å›<br/>Train Pipeline"]
    B --> C["ç¬¬23-25å›<br/>Fine-tune/Stats/Causal"]
    C --> D["ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–"]
    D --> E["ç¬¬27å›<br/>è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"]
    E --> F["ç¬¬28-29å›<br/>Prompt/RAG"]
    F --> G["ç¬¬30å›<br/>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Œå…¨ç‰ˆ"]
    G --> H["ğŸ”„ ç¬¬31å›<br/>MLOpså®Œå…¨ç‰ˆ"]
    H --> I["ç¬¬32å›<br/>çµ±åˆPJ+èª­äº†æ„Ÿ"]

    style H fill:#ffeb3b
    style I fill:#4caf50
```

- **ç¬¬30å›**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ãŸ â†’ ã€Œå‹•ãAIã€ã‚’ä½œã£ãŸ
- **ç¬¬31å›**: MLOpså…¨é ˜åŸŸ â†’ ã€Œå‹•ãç¶šã‘ã‚‹AIã€ã«ã™ã‚‹
- **ç¬¬32å›**: çµ±åˆPJ â†’ Trainâ†’Evalâ†’Deployâ†’Monitorâ†’Feedbackã®ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«å®Ÿè£…

**Course IIIã®ã‚´ãƒ¼ãƒ« = "ç ”ç©¶ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—" â†’ "Production-ready system"**

### 2.3 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼

#### 2.3.1 MLOps = ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã®ã€Œç©ºæ°—ã€

å¾“æ¥ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã§ã¯ã€Git/CI/CD/ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¯**å½“ãŸã‚Šå‰**ã ã€‚èª°ã‚‚ã€ŒGitã‚’ä½¿ã†ã‹ã©ã†ã‹è­°è«–ã€ã—ãªã„ã€‚

MLã§ã‚‚åŒã˜ã¯ãšãªã®ã«ã€**å¤šãã®ãƒãƒ¼ãƒ ãŒGitã™ã‚‰ä½¿ã£ã¦ã„ãªã„**ã€‚å®Ÿé¨“ãƒãƒ¼ãƒˆæ‰‹æ›¸ãã€ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« `model_final_v2_REALLY_FINAL.pkl`ã€‚

**MLOpsã¯ã€ŒMLã«ã‚‚DevOpsã¨åŒã˜è¦å¾‹ã‚’ã€ã¨ã„ã†å½“ç„¶ã®ä¸»å¼µã«éããªã„ã€‚**

#### 2.3.2 MLOps = ç”Ÿãç‰©ã®é£¼è‚²

ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¯ä¸€åº¦æ›¸ã‘ã°ã€Œå‹•ãç¶šã‘ã‚‹ã€(ç†æƒ³çš„ã«ã¯)ã€‚MLãƒ¢ãƒ‡ãƒ«ã¯**ç”Ÿãç‰©**ã ã€‚

- ãƒ‡ãƒ¼ã‚¿ãŒå¤‰ã‚ã‚‹ â†’ æ€§èƒ½åŠ£åŒ–
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å—œå¥½ãŒå¤‰ã‚ã‚‹ â†’ å¥½ã¾ã‚Œãªã„å‡ºåŠ›
- æ–°ã—ã„æ”»æ’ƒãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç¾ã‚Œã‚‹ â†’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§

**ã€Œè¨“ç·´ã—ã¦çµ‚ã‚ã‚Šã€ã¯ã€ãƒšãƒƒãƒˆã‚’è²·ã£ã¦1å›ã‚¨ã‚µã‚’ã‚„ã£ã¦æ”¾ç½®ã™ã‚‹ã®ã¨åŒã˜ã€‚MLOpsã¯ "ç¶™ç¶šçš„ãªä¸–è©±" ã‚’è‡ªå‹•åŒ–ã™ã‚‹ã€‚**

#### 2.3.3 MLOps = ä¿é™ºå¥‘ç´„

å®Ÿé¨“ç®¡ç†ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¯ã€Œä»Šã™ãå½¹ç«‹ã¤ã€ã‚ã‘ã§ã¯ãªã„ã€‚äº‹æ•…ãŒèµ·ããŸã¨ãã«å½¹ç«‹ã¤ã€‚

- æ€§èƒ½ãŒçªç„¶è½ã¡ãŸ â†’ ã€Œã©ã®ã‚³ãƒŸãƒƒãƒˆã§åŠ£åŒ–ã—ãŸã‹ã€ã‚’ç‰¹å®š
- æœ¬ç•ªã§ã‚¨ãƒ©ãƒ¼ â†’ ã€Œã©ã®ãƒ‡ãƒ¼ã‚¿ã§å¤±æ•—ã—ãŸã‹ã€ã‚’å†ç¾
- è¦åˆ¶ç›£æŸ» â†’ ã€Œã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã„ã¤ã€ã©ã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚ŒãŸã‹ã€ã‚’è¨¼æ˜

**ä¿é™ºæ–™(MLOpså°å…¥ã‚³ã‚¹ãƒˆ)ã‚’æ‰•ã‚ãªã„ãƒãƒ¼ãƒ ã¯ã€äº‹æ•…ãŒèµ·ããŸã¨ãã«å…¨æã™ã‚‹ã€‚**

### 2.4 æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ– â€” å®Ÿè£…ã®æœ‰ç„¡

| é …ç›® | æ¾å°¾ç ” | æœ¬è¬›ç¾© (ç¬¬31å›) |
|:-----|:------|:-------------|
| MLflowã®æ‰±ã„ | âš ï¸ã‚¹ãƒ©ã‚¤ãƒ‰1æšã§ã€Œã“ã†ã„ã†ãƒ„ãƒ¼ãƒ«ãŒã‚ã‚‹ã€ | âœ…Juliaçµ±åˆå®Ÿè£… (200è¡Œ) |
| DVCã®æ‰±ã„ | âŒè¨€åŠãªã— | âœ…CLIæ“ä½œ + S3çµ±åˆ |
| CI/CDã®æ‰±ã„ | âŒè¨€åŠãªã— | âœ…GitHub Actionså®Ÿè£… |
| A/Bãƒ†ã‚¹ãƒˆ | âŒè¨€åŠãªã— | âœ…çµ±è¨ˆçš„æ¤œå‡ºåŠ›è¨ˆç®— + å®Ÿè£… |
| ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º | âŒè¨€åŠãªã— | âœ…KSæ¤œå®š/PSIå®Ÿè£… |
| DPO/RLHF | âš ï¸ã‚¹ãƒ©ã‚¤ãƒ‰æ¦‚è¦ã®ã¿ | âœ…æ•°å¼å®Œå…¨å°å‡º + Bradley-Terry Model |

**æ¾å°¾ç ” = ã€Œã“ã†ã„ã†æ¦‚å¿µãŒã‚ã‚‹ã€ã§æ­¢ã¾ã‚‹ã€‚æœ¬è¬›ç¾© = æ•°å¼å°å‡º + å®Ÿè£… + æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã¾ã§ã€‚**

### 2.5 LLMã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚° â€” ç¬¬1-8å›ã®æ•°å­¦ãŒã©ã“ã§ä½¿ã‚ã‚Œã‚‹ã‹

MLOpsã¯çµ±è¨ˆå­¦ãƒ»ç¢ºç‡è«–ãƒ»æƒ…å ±ç†è«–ã®å¿œç”¨å•é¡Œã ã€‚

| Course I æ•°å­¦ | MLOpså¿œç”¨ |
|:-------------|:---------|
| **ç¬¬4å›: ç¢ºç‡è«–** | A/Bãƒ†ã‚¹ãƒˆã®çµ±è¨ˆçš„æ¤œå‡ºåŠ›è¨ˆç®— / ãƒ™ã‚¤ã‚ºæ›´æ–°ã§æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ |
| **ç¬¬5å›: æ¸¬åº¦è«–** | ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º (KSæ¤œå®š = ç´¯ç©åˆ†å¸ƒé–¢æ•°ã®è·é›¢) |
| **ç¬¬6å›: æƒ…å ±ç†è«–** | DPO loss = KL divergenceæœ€å°åŒ– / PSI = KL divergenceã®é›¢æ•£ç‰ˆ |
| **ç¬¬7å›: MLE** | Reward Modeling = preference dataã‹ã‚‰ã®MLE |

**Course Iã®æ•°å­¦ãªã—ã«MLOpsã®ç†è«–ã¯ç†è§£ã§ããªã„ã€‚**

### 2.6 å­¦ç¿’æˆ¦ç•¥ â€” Part A-Gã®å·¨å¤§æ§‹é€ 

æœ¬è¬›ç¾©ã¯**~3,500è¡Œ**ã®å¤§ä½œã€‚7ã¤ã®ãƒ‘ãƒ¼ãƒˆã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹ã€‚

| Part | ãƒ†ãƒ¼ãƒ | æƒ³å®šè¡Œæ•° | å„ªå…ˆåº¦ |
|:-----|:------|:---------|:-------|
| **Part A** | ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° & å®Ÿé¨“ç®¡ç† | 750 | â˜…â˜…â˜… |
| **Part B** | CI/CD for ML | 700 | â˜…â˜…â˜… |
| **Part C** | A/Bãƒ†ã‚¹ãƒˆ & ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹ | 700 | â˜…â˜…â˜… |
| **Part D** | ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° & SLI/SLO | 600 | â˜…â˜…â˜… |
| **Part E** | DPO/RLHFåŸºç¤ | 400 | â˜…â˜…â˜… |
| **Part F** | å®Ÿè£…ç·¨ (âš¡Julia + ğŸ¦€Rust + ğŸ”®Elixir) | 600 | â˜…â˜…â˜… |
| **Part G** | æœ€æ–°ç ”ç©¶ (2024-2026) | 250 | â˜…â˜… |

**æ¨å¥¨å­¦ç¿’é †åº**:

1. **Part A-E (ç†è«–)** ã‚’1å›é€šèª­ (æ•°å¼ã¯é£›ã°ã—ã¦OK)
2. **Part F (å®Ÿè£…)** ã‚’æ‰‹ã‚’å‹•ã‹ã™
3. Part A-Eã«æˆ»ã‚Šã€æ•°å¼ã‚’ä¸å¯§ã«è¿½ã†

**æ•°å¼ã‚’æœ€åˆã‹ã‚‰å…¨éƒ¨ç†è§£ã—ã‚ˆã†ã¨ã™ã‚‹ã¨æŒ«æŠ˜ã™ã‚‹ã€‚å®Ÿè£…ã‚’å…ˆã«è§¦ã£ã¦ã€Œä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã‹ã€ã‚’ä½“æ„Ÿã—ã¦ã‹ã‚‰ã€æ•°å¼ã«æˆ»ã‚‹ã€‚**

### 2.7 æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)

#### 2.7.1 DPO/RLHFçµ±åˆ

**è«–æ–‡**: Direct Preference Optimization [^1] (Rafailov et al., NeurIPS 2023)

**ä¸»è¦è²¢çŒ®**:

- PPOä¸è¦ã§preference dataã‹ã‚‰ç›´æ¥æœ€é©åŒ–
- Bradley-Terry Modelã®é–‰å½¢å¼è§£
- å®‰å®šè¨“ç·´ (PPOã®10å€å®‰å®š)

**2025å¹´ã®å‹•å‘**:

- DPO variantsãŒä¸»æµ (IPO, KTO)
- Online RLHF (ç¶™ç¶šçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†)
- Multi-objective RLHF (è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åŒæ™‚æœ€é©åŒ–)

#### 2.7.2 Automated MLOps

**è«–æ–‡**: AutoMLOps (Google Research, 2024)

**ä¸»è¦è²¢çŒ®**:

- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•ç”Ÿæˆ (Trainâ†’Deploy)
- ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºâ†’å†è¨“ç·´ã®è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼
- SLOé•åâ†’è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

**å®Ÿè£…**: Vertex AI Pipelines, AWS SageMaker Pipelines

#### 2.7.3 Federated MLOps

**è«–æ–‡**: Federated Learning at Scale (Google, 2024)

**ä¸»è¦è²¢çŒ®**:

- åˆ†æ•£è¨“ç·´ã®MLOps (ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§è¨“ç·´)
- Privacy-preserving monitoring
- Differential Privacyçµ±åˆ

#### 2.7.4 Online RLHF â€” ç¶™ç¶šçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†

**è«–æ–‡**: Online Iterative RLHF (DeepMind, 2025)

**ä¸»è¦è²¢çŒ®**:

- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†
- ç¶™ç¶šçš„ãƒ¢ãƒ‡ãƒ«æ›´æ–° (æ—¥æ¬¡/é€±æ¬¡)
- A/Bãƒ†ã‚¹ãƒˆã¨ã®çµ±åˆ

**å®Ÿè£…**: Gemini/Claude APIã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒœã‚¿ãƒ³ â†’ preference data â†’ DPOå†è¨“ç·´ â†’ ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤

**èª²é¡Œ**:

- Feedback biasã®ç®¡ç† (ä¸æº€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯)
- Distribution shiftã®æ¤œå‡º (ãƒ¦ãƒ¼ã‚¶ãƒ¼å±æ€§ã®å¤‰åŒ–)
- Temporal consistencyã®ä¿è¨¼ (æ˜¨æ—¥ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯vsä»Šæ—¥)

#### 2.7.5 Multi-objective RLHF

**è«–æ–‡**: Pareto-optimal RLHF (OpenAI, 2025)

**ä¸»è¦è²¢çŒ®**:

- è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹åŒæ™‚æœ€é©åŒ– (helpfulness + harmlessness + factuality)
- Pareto frontierã®æ¢ç´¢
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«æœ€é©ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’é¸æŠ

**æ•°å¼** (multi-objective DPO):

$$
\mathcal{L}_{\text{MO-DPO}} = -\mathbb{E} \left[ \sum_{i=1}^{K} w_i \log \sigma\left( \beta \log \frac{\pi_\theta(y_w^{(i)} \mid x)}{\pi_{\text{ref}}(y_w^{(i)} \mid x)} - \beta \log \frac{\pi_\theta(y_l^{(i)} \mid x)}{\pi_{\text{ref}}(y_l^{(i)} \mid x)} \right) \right]
$$

- $K$: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°
- $w_i$: é‡ã¿ (ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«èª¿æ•´å¯èƒ½)

**2026å¹´ã®å±•æœ›**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã§ã€Œå‰µé€ æ€§ vs æ­£ç¢ºæ€§ã€ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æ•´ã§ãã‚‹LLMã€‚

:::message
**é€²æ—: 25% å®Œäº†** ãªãœMLOpsãŒå¿…é ˆã‹ + æœ€æ–°ç ”ç©¶ã‚’ç†è§£ã—ãŸã€‚Zone 3ã§7ãƒ‘ãƒ¼ãƒˆã®ç†è«–ã‚’ä¸€æ°—ã«æ§‹ç¯‰ã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ90åˆ†ï¼‰â€” MLOpså…¨7é ˜åŸŸã®ç†è«–

### Part A: ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° & å®Ÿé¨“ç®¡ç†

#### 3.1 ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®æ•°å­¦çš„åŸºç›¤

**ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®æœ¬è³ª = ãƒãƒƒã‚·ãƒ¥é–¢æ•°ã«ã‚ˆã‚‹ä¸€æ„è­˜åˆ¥**ã€‚

ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ $\mathcal{M}_t$ ã‚’ä»¥ä¸‹ã®5-tupleã§å®šç¾©:

$$
\mathcal{M}_t = (\mathbf{w}_t, \mathcal{D}_t, \mathcal{H}_t, \mathcal{E}_t, s_t)
$$

- $\mathbf{w}_t \in \mathbb{R}^p$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ« (é‡ã¿)
- $\mathcal{D}_t$: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†å¯¾è±¡)
- $\mathcal{H}_t$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é›†åˆ $\{\eta, \lambda, \text{batch\_size}, \ldots\}$
- $\mathcal{E}_t$: ç’°å¢ƒ (Python version, CUDA version, library versions)
- $s_t \in \{0, 1, \ldots, 2^{64}-1\}$: Random seed

**å†ç¾æ€§ã®å…¬ç†**:

$$
\mathcal{M}_t = \mathcal{M}_{t'} \iff \text{Hash}(\mathcal{M}_t) = \text{Hash}(\mathcal{M}_{t'})
$$

ãƒãƒƒã‚·ãƒ¥é–¢æ•° $\text{Hash}: \mathcal{M} \to \{0,1\}^{256}$ (SHA-256) ãŒåŒã˜ãªã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã¯**å®Œå…¨ã«å†ç¾å¯èƒ½**ã€‚

##### Git LFSã«ã‚ˆã‚‹å¤§ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†

Gitã¯å°ã•ãªãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‘ã‘ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (500MB+) ã¯Git LFSã§ç®¡ç†ã€‚

Git LFSã®ä»•çµ„ã¿:

1. å¤§ãƒ•ã‚¡ã‚¤ãƒ« `model.safetensors` ã‚’ `.git/lfs/objects/` ã«ä¿å­˜
2. Gitã«ã¯**ãƒã‚¤ãƒ³ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«**ã®ã¿ commit:

```
version https://git-lfs.github.com/spec/v1
oid sha256:4d7a214614ab2935c1f0e1c69a0d3e82a5bb9e6e8e1e3a0c9f5d4c3b2a1b0c1d
size 524288000
```

3. `git pull` æ™‚ã€LFSã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

**åˆ©ç‚¹**: Gitãƒªãƒã‚¸ãƒˆãƒªã¯è»½é‡ (ãƒã‚¤ãƒ³ã‚¿ã®ã¿)ã€‚å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã¯å°‚ç”¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€‚

##### DVCã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

DVC [^2] ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰ˆGit LFSã€‚

**DVCã®ä»•çµ„ã¿**:

1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ `data/train.csv` (10GB) ã‚’è¿½è·¡:

```bash
dvc add data/train.csv
```

2. DVCãŒ `.dvc` ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ:

```yaml
# data/train.csv.dvc
outs:
- md5: a3f9c2e1b4d87f3a9c2e1b4d87f3a9c2
  size: 10737418240
  path: train.csv
```

3. Gitã¯ `.dvc` ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ç®¡ç†ã€‚å®Ÿãƒ‡ãƒ¼ã‚¿ã¯S3/GCS/Azureã«ä¿å­˜:

```bash
dvc remote add -d myremote s3://my-bucket/dvc-store
dvc push
```

4. ä»–ã®ãƒ¡ãƒ³ãƒãƒ¼ã¯ `dvc pull` ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—:

```bash
git checkout experiment-v2
dvc checkout  # Downloads data version from experiment-v2
```

**æ•°å­¦çš„ãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
\text{DVC Pointer:} \quad & p = (\text{md5}(\mathcal{D}), |\mathcal{D}|, \text{path}) \\
\text{Storage Mapping:} \quad & \mathcal{D} \mapsto \text{S3}://\text{bucket}/\text{md5}(\mathcal{D})[:2]/\text{md5}(\mathcal{D})[2:]
\end{aligned}
$$

**MD5ãƒãƒƒã‚·ãƒ¥ã®æœ€åˆ2æ–‡å­—ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆ†å‰²ã—ã€è¡çªã‚’å›é¿ã€‚**

##### MLflow Model Registry

MLflowã¯ãƒ¢ãƒ‡ãƒ«ã‚’**ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¸**ã§ç®¡ç†ã€‚

| ã‚¹ãƒ†ãƒ¼ã‚¸ | æ„å‘³ | æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ |
|:--------|:-----|:-----------|
| `None` | ç™»éŒ²ç›´å¾Œ | `Staging` |
| `Staging` | ãƒ†ã‚¹ãƒˆç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤ | `Production` |
| `Production` | æœ¬ç•ªç’°å¢ƒã§ç¨¼åƒä¸­ | `Archived` |
| `Archived` | å»ƒæ£„æ¸ˆã¿ | â€” |

**ã‚¹ãƒ†ãƒ¼ã‚¸é·ç§»ã®æ¡ä»¶**:

$$
\begin{aligned}
\text{None} \to \text{Staging:} \quad & \text{validation\_acc} \geq \theta_{\text{staging}} \\
\text{Staging} \to \text{Production:} \quad & \text{A/B test win} \land \text{latency} \leq \tau
\end{aligned}
$$

ä¾‹: $\theta_{\text{staging}} = 0.95$, $\tau = 100\text{ms}$ã€‚

**ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ä¸€æ„æ€§**:

$$
\text{Model ID} = (\text{name}, \text{version}, \text{run\_id})
$$

- `name`: ãƒ¢ãƒ‡ãƒ«å (e.g., "sentiment-classifier")
- `version`: æ•´æ•° (1, 2, 3, ...)
- `run_id`: MLflow Run UUID (è¨“ç·´æ™‚ã«è‡ªå‹•ç”Ÿæˆ)

**åŒã˜nameã§ã‚‚versionãŒé•ãˆã°åˆ¥ãƒ¢ãƒ‡ãƒ«ã€‚run_idã§è¨“ç·´æ™‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«é¡ã‚Œã‚‹ã€‚**

#### 3.2 å®Ÿé¨“ç®¡ç†ã®ç†è«–

**å®Ÿé¨“ç®¡ç†ã®æœ¬è³ª = ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“$\mathcal{H}$ä¸Šã®æ¢ç´¢å±¥æ­´ã®è¨˜éŒ²**ã€‚

##### å®Ÿé¨“ã®å®šç¾©

å®Ÿé¨“ $e_i$ ã‚’ä»¥ä¸‹ã®4-tupleã§å®šç¾©:

$$
e_i = (\mathbf{h}_i, \mathcal{D}_i, \mathbf{m}_i, \mathcal{A}_i)
$$

- $\mathbf{h}_i \in \mathcal{H}$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ«
- $\mathcal{D}_i$: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (train/val/test split)
- $\mathbf{m}_i \in \mathbb{R}^k$: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ã‚¯ãƒˆãƒ« (loss, accuracy, F1, ...)
- $\mathcal{A}_i$: Artifacts (ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«, ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ, å›³)

**å®Ÿé¨“ã®æ¯”è¼ƒå¯èƒ½æ€§**:

$$
e_i \sim e_j \iff \mathcal{D}_i = \mathcal{D}_j
$$

åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãªã‘ã‚Œã°ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¯”è¼ƒã§ããªã„ã€‚

##### ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨˜éŒ²

MLflowã¯ `log_metric(key, value, step)` ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ™‚ç³»åˆ—è¨˜éŒ²ã€‚

$$
\mathbf{m}(t) = \{(k_1, v_1(t)), (k_2, v_2(t)), \ldots, (k_n, v_n(t))\}
$$

ä¾‹: è¨“ç·´ãƒ«ãƒ¼ãƒ—ã§epochã”ã¨ã«è¨˜éŒ²:

```python
for epoch in range(num_epochs):
    train_loss = train_one_epoch()
    val_acc = validate()

    mlflow.log_metric("train_loss", train_loss, step=epoch)
    mlflow.log_metric("val_acc", val_acc, step=epoch)
```

**ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ™‚ç³»åˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦åæŸã‚’ç¢ºèªã§ãã‚‹ã€‚**

##### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æœ€é©åŒ–å•é¡Œ

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ = **ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–**:

$$
\mathbf{h}^* = \arg\max_{\mathbf{h} \in \mathcal{H}} f(\mathbf{h})
$$

- $f(\mathbf{h})$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\mathbf{h}$ã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã®validation metric
- $f$ã¯å¾®åˆ†ä¸å¯èƒ½ã€è©•ä¾¡ã«ã‚³ã‚¹ãƒˆ(è¨“ç·´æ™‚é–“)ãŒã‹ã‹ã‚‹

**æ¢ç´¢æ‰‹æ³•**:

| æ‰‹æ³• | èª¬æ˜ | è¨ˆç®—é‡ |
|:-----|:-----|:-------|
| Grid Search | $\mathcal{H}$ã‚’æ ¼å­çŠ¶ã«æ¢ç´¢ | $O(k^d)$ ($k$=å„æ¬¡å…ƒã®åˆ†å‰²æ•°, $d$=æ¬¡å…ƒ) |
| Random Search | ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | $O(N)$ ($N$=è©¦è¡Œå›æ•°) |
| Bayesian Optimization | Gaussian Processã§$f$ã‚’ãƒ¢ãƒ‡ãƒ«åŒ– â†’ Acquisitioné–¢æ•°ã§æ¬¡ã®ç‚¹ã‚’é¸æŠ | $O(N^3)$ (GP) |
| Hyperband | Successive Halvingã§ä½æ€§èƒ½ãªè¨­å®šã‚’æ—©æœŸæ‰“ã¡åˆ‡ã‚Š | $O(N \log N)$ |

**å®Ÿè·µçš„æ¨å¥¨**: æœ€åˆã«Random Search (20-50 trials) â†’ æœ‰æœ›ãªé ˜åŸŸã§Bayesian Optã€‚

##### MLflowã¨W&Bã®æ¯”è¼ƒ

| è¦³ç‚¹ | MLflow | Weights & Biases (W&B) |
|:-----|:-------|:----------------------|
| **ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°** | Self-hosted (ç„¡æ–™) | Cloud (æœ‰æ–™, Free tierã‚ã‚Š) |
| **UI** | ã‚·ãƒ³ãƒ—ãƒ« | ãƒªãƒƒãƒ (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚°ãƒ©ãƒ•, ãƒãƒ¼ãƒ å…±æœ‰) |
| **ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²** | `log_metric(key, value, step)` | `wandb.log({"key": value})` |
| **Artifactç®¡ç†** | S3/GCS/Azureçµ±åˆ | W&B Cloudè‡ªå‹•ç®¡ç† |
| **Model Registry** | âœ…ã‚ã‚Š | âœ…ã‚ã‚Š (W&B Registry) |
| **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°** | âŒãªã— (å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ä½µç”¨) | âœ…Sweeps (Bayesian Optå†…è”µ) |
| **ã‚³ã‚¹ãƒˆ** | ç„¡æ–™ (ã‚¤ãƒ³ãƒ•ãƒ©ä»£ã®ã¿) | Teamãƒ—ãƒ©ãƒ³ $50/user/month |

**MLflow = å®Œå…¨åˆ¶å¾¡ãƒ»ã‚³ã‚¹ãƒˆé‡è¦–ã€‚W&B = ç”Ÿç”£æ€§ãƒ»ãƒãƒ¼ãƒ å”æ¥­é‡è¦–ã€‚**

#### 3.3 å†ç¾æ€§ä¿è¨¼ã®æ•°å­¦

**å†ç¾æ€§ã®å®šç¾©**:

$$
\text{Reproducible}(e_i) \iff \forall j, \, (\text{Hash}(\mathcal{M}_i) = \text{Hash}(\mathcal{M}_j)) \implies \mathbf{m}_i = \mathbf{m}_j
$$

åŒã˜ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ãªã‚‰ã€åŒã˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

**å†ç¾æ€§ã‚’ç ´å£Šã™ã‚‹è¦å› **:

1. **Non-deterministic operations**: CUDA `atomicAdd`, cuDNN auto-tuning
2. **Floating-point non-associativity**: $(a + b) + c \neq a + (b + c)$ (ä¸¸ã‚èª¤å·®)
3. **Untracked dependencies**: ã‚·ã‚¹ãƒ†ãƒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ç’°å¢ƒå¤‰æ•°

**å†ç¾æ€§ã‚’ä¿è¨¼ã™ã‚‹æ‰‹æ³•**:

##### 3.3.1 Environmentå›ºå®š

**Dockerã‚³ãƒ³ãƒ†ãƒŠ**ã§ç’°å¢ƒã‚’å‡çµ:

```dockerfile
FROM python:3.11-slim

# Pin library versions exactly
RUN pip install torch==2.1.0 transformers==4.35.0

# Copy code
COPY . /app
WORKDIR /app

CMD ["python", "train.py"]
```

**ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚‚å›ºå®š**:

```dockerfile
FROM python@sha256:a3f9c2e1b4d87f3a9c2e1b4d87f3a9c2e1b4d87f3a9c2e1b4d87f3a9c2
```

##### 3.3.2 Seedå›ºå®š

å…¨ã¦ã®ä¹±æ•°ç”Ÿæˆã‚’seedã§åˆ¶å¾¡:

```python
import torch
import numpy as np
import random

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # Deterministic CUDA operations (slower but reproducible)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)
```

**`cudnn.deterministic = True`ã«ã™ã‚‹ã¨ã€cuDNNã¯æ±ºå®šçš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã¿ä½¿ç”¨ (é€Ÿåº¦ä½ä¸‹ã‚ã‚Š)ã€‚**

##### 3.3.3 ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\mathcal{D}$ ã®å¤‰æ›´ã‚’è¿½è·¡:

$$
\text{Hash}(\mathcal{D}) = \text{SHA256}\left( \bigoplus_{i=1}^{N} x_i \right)
$$

- $\bigoplus$: XOR (é †åºã«ä¾å­˜ã—ãªã„)
- $x_i$: $i$ç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ«

**ãƒ‡ãƒ¼ã‚¿ã®é †åºã‚’å¤‰ãˆã¦ã‚‚åŒã˜ãƒãƒƒã‚·ãƒ¥ã«ã—ãŸã„å ´åˆã¯XORã‚’ä½¿ã† (commutative)ã€‚é †åºã‚‚å«ã‚ãŸã„å ´åˆã¯é€£çµã—ã¦SHA256ã€‚**

---

### Part B: CI/CD for ML

#### 3.4 CI/CD for MLã®æ§‹æˆè¦ç´ 

**å¾“æ¥ã®CI/CD**:

1. ã‚³ãƒ¼ãƒ‰ã‚’push
2. è‡ªå‹•ãƒ†ã‚¹ãƒˆ (unit/integration/E2E)
3. ãƒ‘ã‚¹ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ / å¤±æ•— â†’ PR block

**MLç‰¹æœ‰ã®è¿½åŠ è¦ç´ **:

1. **ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³**: ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ãƒ»æ¬ æå€¤ãƒã‚§ãƒƒã‚¯ãƒ»åˆ†å¸ƒç•°å¸¸æ¤œå‡º
2. **ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ**: è¨“ç·´ã—ã¦ accuracy >= thresholdç¢ºèª
3. **æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ†ã‚¹ãƒˆ**: p99 latency <= SLOç¢ºèª
4. **Regression Detection**: æ–°ãƒ¢ãƒ‡ãƒ«ãŒæ—§ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚ŠåŠ£åŒ–ã—ã¦ã„ãªã„ã‹

#### 3.5 ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ â€” Great Expectations

Great Expectations [^3] = ãƒ‡ãƒ¼ã‚¿ã®unit testã€‚

**Expectation (æœŸå¾…å€¤) ã®å®šç¾©**:

$$
E = \{\text{column}, \text{condition}, \text{threshold}\}
$$

ä¾‹:

```python
import great_expectations as gx

# Initialize context
context = gx.get_context()

# Create expectation suite
suite = context.add_expectation_suite("data_quality_suite")

# Define expectations
suite.expect_column_values_to_not_be_null("user_id")
suite.expect_column_values_to_be_between("age", min_value=0, max_value=120)
suite.expect_column_mean_to_be_between("price", min_value=10, max_value=1000)

# Validate data
batch = context.get_batch({"path": "data/train.csv"})
result = context.run_validation(batch, expectation_suite_name="data_quality_suite")

if not result["success"]:
    raise ValueError("Data validation failed!")
```

**æ•°å­¦çš„è¡¨ç¾**:

$$
\begin{aligned}
E_1: \quad & \forall i, \, x_i[\text{user\_id}] \neq \text{null} \\
E_2: \quad & \forall i, \, 0 \leq x_i[\text{age}] \leq 120 \\
E_3: \quad & 10 \leq \frac{1}{N}\sum_{i=1}^{N} x_i[\text{price}] \leq 1000
\end{aligned}
$$

**å…¨ã¦ã®ExpectationãŒæº€ãŸã•ã‚ŒãŸã‚‰ãƒ‡ãƒ¼ã‚¿ã¯"valid"ã€‚1ã¤ã§ã‚‚å¤±æ•—ã—ãŸã‚‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢ã€‚**

#### 3.6 ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ

**æ€§èƒ½ãƒ†ã‚¹ãƒˆã®å®šå¼åŒ–**:

$$
\text{Test Passed} \iff \text{metric}(\mathcal{M}, \mathcal{D}_{\text{test}}) \geq \theta
$$

- $\text{metric}$: Accuracy, F1, AUCç­‰
- $\theta$: è¨±å®¹é–¾å€¤ (e.g., 0.95)

ä¾‹:

```python
# train.py
model = train_model(train_data)
val_acc = evaluate(model, val_data)

if val_acc < 0.95:
    raise ValueError(f"Model accuracy {val_acc:.4f} < 0.95")
```

**GitHub Actionsçµ±åˆ**:

```yaml
- name: Train and test model
  run: |
    python train.py --config configs/ci.yaml
    python test_model.py --threshold 0.95
```

**ãƒ†ã‚¹ãƒˆå¤±æ•— â†’ CIå¤±æ•— â†’ PRãƒãƒ¼ã‚¸ä¸å¯ã€‚**

#### 3.7 æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ†ã‚¹ãƒˆ

**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·SLOã®å®šç¾©**:

$$
\text{SLO:} \quad P(\text{latency} \leq \tau) \geq 0.99
$$

- $\tau$: é–¾å€¤ (e.g., 100ms)
- $P$: 99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« (p99)

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè£…**:

```python
import time
import numpy as np

latencies = []
for _ in range(1000):  # 1000 requests
    start = time.time()
    model.predict(input_data)
    latencies.append(time.time() - start)

p99 = np.percentile(latencies, 99)
if p99 > 0.1:  # 100ms
    raise ValueError(f"p99 latency {p99*1000:.2f}ms > 100ms")
```

**p99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒSLOã‚’è¶…ãˆãŸã‚‰ãƒ†ã‚¹ãƒˆå¤±æ•—ã€‚**

#### 3.8 Regression Detection â€” A/Bãƒ†ã‚¹ãƒˆ in CI

æ–°ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_{\text{new}}$ãŒæ—§ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_{\text{old}}$ã‚ˆã‚ŠåŠ£åŒ–ã—ã¦ã„ãªã„ã‹æ¤œè¨¼ã€‚

**å¸°ç„¡ä»®èª¬**:

$$
H_0: \, \text{metric}(\mathcal{M}_{\text{new}}) \leq \text{metric}(\mathcal{M}_{\text{old}})
$$

**å¯¾ç«‹ä»®èª¬**:

$$
H_1: \, \text{metric}(\mathcal{M}_{\text{new}}) > \text{metric}(\mathcal{M}_{\text{old}})
$$

**çµ±è¨ˆçš„æ¤œå®š** (one-sided t-test):

$$
t = \frac{\bar{m}_{\text{new}} - \bar{m}_{\text{old}}}{\sqrt{\frac{s_{\text{new}}^2}{n_{\text{new}}} + \frac{s_{\text{old}}^2}{n_{\text{old}}}}}
$$

- $\bar{m}$: å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- $s^2$: åˆ†æ•£
- $n$: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º

**$t > t_{0.05, df}$ (5%æœ‰æ„æ°´æº–) ãªã‚‰$H_0$ã‚’æ£„å´ â†’ æ–°ãƒ¢ãƒ‡ãƒ«ãŒæœ‰æ„ã«æ”¹å–„ã€‚**

---

### Part C: A/Bãƒ†ã‚¹ãƒˆ & ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹

#### 3.9 A/Bãƒ†ã‚¹ãƒˆã®çµ±è¨ˆçš„åŸºç›¤

**A/Bãƒ†ã‚¹ãƒˆã®è¨­å®š**:

- Controlç¾¤ (A): æ—§ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_A$
- Treatmentç¾¤ (B): æ–°ãƒ¢ãƒ‡ãƒ«$\mathcal{M}_B$
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹: Conversion rate $p$ (e.g., ã‚¯ãƒªãƒƒã‚¯ç‡)

**å¸°ç„¡ä»®èª¬**:

$$
H_0: \, p_A = p_B
$$

**å¯¾ç«‹ä»®èª¬**:

$$
H_1: \, p_A \neq p_B
$$

##### 3.9.1 ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—

**Statistical Power** $1-\beta$ ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º$n$ã‚’è¨ˆç®—ã€‚

$$
n = \frac{(Z_{1-\alpha/2} + Z_{1-\beta})^2 \cdot (\bar{p}(1-\bar{p}) + \bar{p}(1-\bar{p}))}{\delta^2}
$$

- $\bar{p} = (p_A + p_B)/2$: å¹³å‡conversion rate
- $\delta = |p_A - p_B|$: Minimum Detectable Effect (MDE)
- $Z_{1-\alpha/2}$: æœ‰æ„æ°´æº–$\alpha$ã®è‡¨ç•Œå€¤ (é€šå¸¸ $\alpha=0.05 \Rightarrow Z=1.96$)
- $Z_{1-\beta}$: Power $1-\beta$ã®è‡¨ç•Œå€¤ (é€šå¸¸ $\beta=0.2 \Rightarrow Z=0.84$)

**ä¾‹**: $p_A = 0.10$, $\delta = 0.02$ (2%ã®æ”¹å–„ã‚’æ¤œå‡ºã—ãŸã„), $\alpha=0.05$, $\beta=0.2$:

$$
\begin{aligned}
\bar{p} &= 0.10 \\
n &= \frac{(1.96 + 0.84)^2 \cdot 2 \cdot 0.10 \cdot 0.90}{0.02^2} \\
&= \frac{7.84 \cdot 0.18}{0.0004} \\
&\approx 3528 \text{ samples per group}
\end{aligned}
$$

**å„ç¾¤3,528ã‚µãƒ³ãƒ—ãƒ«å¿…è¦ = åˆè¨ˆ7,056ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€‚**

##### 3.9.2 Sequential Testing

é€šå¸¸ã®A/Bãƒ†ã‚¹ãƒˆã¯**å›ºå®šã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**ã€‚Sequential Testingã¯**é€æ¬¡çš„ã«æ¤œå®šã—ã€æ—©æœŸåœæ­¢**ã€‚

**Sequential Probability Ratio Test (SPRT)**:

$$
\Lambda_t = \frac{P(D_t \mid H_1)}{P(D_t \mid H_0)}
$$

- $D_t$: $t$æ™‚ç‚¹ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿
- $\Lambda_t$: Likelihood ratio

**æ—©æœŸåœæ­¢ã®åˆ¤å®š**:

$$
\begin{cases}
\Lambda_t \geq \frac{1-\beta}{\alpha} & \Rightarrow \text{Reject } H_0 \text{ (B wins)} \\
\Lambda_t \leq \frac{\beta}{1-\alpha} & \Rightarrow \text{Accept } H_0 \text{ (A wins)} \\
\text{otherwise} & \Rightarrow \text{Continue testing}
\end{cases}
$$

**åˆ©ç‚¹**: å¹³å‡ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå›ºå®šã‚µãƒ³ãƒ—ãƒ«ã®**50%**ã«å‰Šæ¸›å¯èƒ½ã€‚

##### 3.9.3 Guardrail Metrics

æ–°ãƒ¢ãƒ‡ãƒ«ãŒprimaryãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ”¹å–„ã—ã¦ã‚‚ã€**guardrailãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ‚ªåŒ–**ã•ã›ãŸã‚‰å´ä¸‹ã€‚

**ä¾‹**:

- Primary: CTR (Click-Through Rate) â†‘
- Guardrail: Bounce Rate â†‘ (æ‚ªåŒ–), Latency â†‘ (æ‚ªåŒ–)

**æ¡ä»¶**:

$$
\text{Deploy} \iff (\text{CTR}_B > \text{CTR}_A) \land (\text{Bounce}_B \leq \text{Bounce}_A) \land (\text{Latency}_B \leq \text{SLO})
$$

**1ã¤ã§ã‚‚ guardrail ã‚’ç ´ã£ãŸã‚‰ãƒ‡ãƒ—ãƒ­ã‚¤ä¸­æ­¢ã€‚**

#### 3.10 ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹ã®æ•°å­¦çš„ãƒ¢ãƒ‡ãƒ«

**æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ**:

$$
\text{Traffic}(t) = \begin{cases}
0.01 & \text{if } t \in [0, T_1) \\
0.05 & \text{if } t \in [T_1, T_2) \\
0.25 & \text{if } t \in [T_2, T_3) \\
1.00 & \text{if } t \geq T_3
\end{cases}
$$

- $T_1, T_2, T_3$: å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®çµ‚äº†æ™‚åˆ»

**è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¡ä»¶**:

$$
\text{Rollback} \iff \text{Error Rate}_{\text{canary}} > \text{Error Rate}_{\text{baseline}} + \epsilon
$$

- $\epsilon$: è¨±å®¹èª¤å·® (e.g., 0.5%)

**ä¾‹**: Baseline error rate = 0.2%, Canary error rate = 1.0% â†’ $1.0 > 0.2 + 0.5 = 0.7$ â†’ Rollback!

##### 3.10.1 Feature Flags

ã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹ã‚’ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã§åˆ¶å¾¡ã€‚

```python
from feature_flags import is_enabled

def predict(input_data, user_id):
    if is_enabled("use_model_v2", user_id):
        return model_v2.predict(input_data)
    else:
        return model_v1.predict(input_data)
```

**`is_enabled`ã®å®Ÿè£…** (consistent hashing):

```python
def is_enabled(flag_name, user_id, rollout_percentage=0.01):
    hash_val = hash(f"{flag_name}:{user_id}") % 100
    return hash_val < rollout_percentage * 100
```

**1%ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ = ãƒãƒƒã‚·ãƒ¥å€¤0-0ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿æœ‰åŠ¹åŒ–ã€‚**

---

### Part D: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° & SLI/SLO

#### 3.11 RED Metrics â€” ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã®åŸºæœ¬3è»¸

**RED Metrics**:

- **Rate**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°/ç§’ (RPS)
- **Errors**: ã‚¨ãƒ©ãƒ¼æ•°/ç§’
- **Duration**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (p50/p95/p99)

$$
\begin{aligned}
\text{Rate:} \quad & r(t) = \frac{\text{requests}}{t} \\
\text{Error Rate:} \quad & e(t) = \frac{\text{errors}(t)}{\text{requests}(t)} \\
\text{Latency:} \quad & L_{p99}(t) = \text{percentile}(\text{latencies}(t), 99)
\end{aligned}
$$

**Prometheus exporterã®å®Ÿè£…**:

```python
from prometheus_client import Counter, Histogram

REQUEST_COUNT = Counter('model_requests_total', 'Total requests')
ERROR_COUNT = Counter('model_errors_total', 'Total errors')
LATENCY = Histogram('model_latency_seconds', 'Latency',
                    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0])

def predict_with_metrics(input_data):
    REQUEST_COUNT.inc()
    start = time.time()

    try:
        result = model.predict(input_data)
        LATENCY.observe(time.time() - start)
        return result
    except Exception:
        ERROR_COUNT.inc()
        raise
```

**PrometheusãŒã“ã‚Œã‚‰ã‚’scrapeã—ã¦æ™‚ç³»åˆ—DBã«ä¿å­˜ã€‚**

#### 3.12 SLI/SLOè¨­è¨ˆ

**SLI (Service Level Indicator)** = æ¸¬å®šå¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹:

$$
\text{SLI}_{\text{availability}} = \frac{\text{successful requests}}{\text{total requests}}
$$

$$
\text{SLI}_{\text{latency}} = \frac{\text{requests with latency} \leq \tau}{\text{total requests}}
$$

**SLO (Service Level Objective)** = SLIã®ç›®æ¨™å€¤:

$$
\begin{aligned}
\text{SLO}_{\text{availability}}: \quad & \text{SLI}_{\text{availability}} \geq 0.999 \quad \text{(99.9%)} \\
\text{SLO}_{\text{latency}}: \quad & \text{SLI}_{\text{latency}} \geq 0.99 \quad \text{(p99 < 100ms)}
\end{aligned}
$$

**Error Budget** = SLOã§è¨±å®¹ã•ã‚Œã‚‹å¤±æ•—ã®é‡:

$$
\text{Error Budget} = 1 - \text{SLO}
$$

ä¾‹: SLO = 99.9% â†’ Error Budget = 0.1% = 43.2åˆ†/æœˆ (30æ—¥)ã€‚

$$
0.001 \times 30 \times 24 \times 60 = 43.2 \text{ minutes}
$$

**Error Budgetã‚’ä½¿ã„åˆ‡ã£ãŸã‚‰æ–°æ©Ÿèƒ½é–‹ç™ºã‚’åœæ­¢ã—ã€ä¿¡é ¼æ€§å‘ä¸Šã«é›†ä¸­ã€‚**

#### 3.13 ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º

**ãƒ‰ãƒªãƒ•ãƒˆã®ç¨®é¡**:

1. **Data Drift**: å…¥åŠ›åˆ†å¸ƒ$P(X)$ã®å¤‰åŒ–
2. **Concept Drift**: $P(Y \mid X)$ã®å¤‰åŒ– (ãƒ©ãƒ™ãƒ«ã®æ„å‘³ãŒå¤‰ã‚ã‚‹)
3. **Model Drift**: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®åŠ£åŒ–

##### 3.13.1 Kolmogorov-Smirnovæ¤œå®š

**KSçµ±è¨ˆé‡**:

$$
D = \sup_{x} |F_{\text{train}}(x) - F_{\text{prod}}(x)|
$$

- $F_{\text{train}}(x)$: è¨“ç·´æ™‚ã®ç´¯ç©åˆ†å¸ƒé–¢æ•° (CDF)
- $F_{\text{prod}}(x)$: æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®CDF
- $\sup$: supremum (æœ€å¤§å€¤)

**å¸°ç„¡ä»®èª¬**:

$$
H_0: \, F_{\text{train}} = F_{\text{prod}}
$$

**på€¤è¨ˆç®—** (Kolmogorov distribution):

$$
p = P(D_{n,m} \geq D) = 2 \sum_{k=1}^{\infty} (-1)^{k-1} e^{-2k^2 D^2 n}
$$

- $n = \frac{n_{\text{train}} \cdot n_{\text{prod}}}{n_{\text{train}} + n_{\text{prod}}}$

**å®Ÿè£…**:

```python
from scipy.stats import ks_2samp

train_feature = train_data["feature_1"]
prod_feature = prod_data["feature_1"]

statistic, p_value = ks_2samp(train_feature, prod_feature)

if p_value < 0.01:  # 1% significance level
    print("âš ï¸ Data drift detected!")
    trigger_retraining()
```

##### 3.13.2 Population Stability Index (PSI)

**PSI** = è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã®ä¹–é›¢åº¦ã€‚

$$
\text{PSI} = \sum_{i=1}^{B} (p_{\text{prod},i} - p_{\text{train},i}) \ln\left(\frac{p_{\text{prod},i}}{p_{\text{train},i}}\right)
$$

- $B$: ãƒ“ãƒ³æ•° (é€šå¸¸10)
- $p_{\text{train},i}$: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ“ãƒ³$i$ã®å‰²åˆ
- $p_{\text{prod},i}$: æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®ãƒ“ãƒ³$i$ã®å‰²åˆ

**PSIã®è§£é‡ˆ**:

| PSIå€¤ | è§£é‡ˆ |
|:------|:-----|
| < 0.1 | ãƒ‰ãƒªãƒ•ãƒˆãªã— |
| 0.1 - 0.25 | è»½å¾®ãªãƒ‰ãƒªãƒ•ãƒˆ (ç›£è¦–ç¶™ç¶š) |
| > 0.25 | é‡å¤§ãªãƒ‰ãƒªãƒ•ãƒˆ (å†è¨“ç·´å¿…è¦) |

**å®Ÿè£…**:

```python
import numpy as np

def calculate_psi(train_data, prod_data, bins=10):
    # Bin data
    min_val = min(train_data.min(), prod_data.min())
    max_val = max(train_data.max(), prod_data.max())
    bin_edges = np.linspace(min_val, max_val, bins+1)

    train_hist, _ = np.histogram(train_data, bins=bin_edges)
    prod_hist, _ = np.histogram(prod_data, bins=bin_edges)

    # Normalize
    p_train = train_hist / train_hist.sum()
    p_prod = prod_hist / prod_hist.sum()

    # Avoid log(0)
    p_train = np.where(p_train == 0, 0.0001, p_train)
    p_prod = np.where(p_prod == 0, 0.0001, p_prod)

    # Calculate PSI
    psi = np.sum((p_prod - p_train) * np.log(p_prod / p_train))
    return psi

psi = calculate_psi(train_feature, prod_feature)
if psi > 0.25:
    print(f"âš ï¸ Significant drift detected! PSI = {psi:.4f}")
```

##### 3.13.3 Jensen-Shannon Divergence

**JS Divergence** = å¯¾ç§°ãªKL divergence:

$$
\text{JSD}(P \| Q) = \frac{1}{2} D_{\text{KL}}(P \| M) + \frac{1}{2} D_{\text{KL}}(Q \| M)
$$

- $M = \frac{1}{2}(P + Q)$: å¹³å‡åˆ†å¸ƒ

**æ€§è³ª**:

- $0 \leq \text{JSD} \leq \ln 2$
- $\text{JSD}(P \| Q) = \text{JSD}(Q \| P)$ (å¯¾ç§°)

**å®Ÿè£…** (é›¢æ•£åˆ†å¸ƒ):

```python
from scipy.spatial.distance import jensenshannon

p = np.histogram(train_feature, bins=20, density=True)[0]
q = np.histogram(prod_feature, bins=20, density=True)[0]

js_div = jensenshannon(p, q)
if js_div > 0.3:  # Threshold (0-1 range after sqrt)
    print(f"âš ï¸ Drift detected! JSD = {js_div:.4f}")
```

---

### Part E: DPO/RLHFåŸºç¤

#### 3.14 RLHF (Reinforcement Learning from Human Feedback)

**RLHFã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**:

1. **SFT (Supervised Fine-Tuning)**: ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’
2. **Reward Modeling**: äººé–“ã®preferenceã‹ã‚‰reward modelè¨“ç·´
3. **RL Fine-Tuning**: PPOã§rewardæœ€å¤§åŒ–

##### 3.14.1 Reward Modelingã®æ•°å­¦

äººé–“ãŒ2ã¤ã®å¿œç­” $(y_1, y_2)$ ã‚’æ¯”è¼ƒã—ã€å¥½ã¾ã—ã„æ–¹ã‚’é¸æŠã€‚

**Bradley-Terry Model**:

$$
P(y_1 \succ y_2 \mid x) = \frac{\exp(r(x, y_1))}{\exp(r(x, y_1)) + \exp(r(x, y_2))} = \sigma(r(x, y_1) - r(x, y_2))
$$

- $r(x, y)$: Reward model (ã‚¹ã‚«ãƒ©ãƒ¼)
- $\sigma(z) = 1/(1+e^{-z})$: Sigmoidé–¢æ•°

**Lossé–¢æ•°** (binary cross-entropy):

$$
\mathcal{L}_{\text{RM}} = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma(r(x, y_w) - r(x, y_l)) \right]
$$

- $y_w$: å¥½ã¾ã—ã„å¿œç­” (win)
- $y_l$: å¥½ã¾ã—ããªã„å¿œç­” (lose)

**Reward modelã®è¨“ç·´**:

```python
def reward_model_loss(r_win, r_lose):
    return -torch.log(torch.sigmoid(r_win - r_lose)).mean()

# Training loop
for x, y_win, y_lose in dataloader:
    r_win = reward_model(x, y_win)
    r_lose = reward_model(x, y_lose)
    loss = reward_model_loss(r_win, r_lose)
    loss.backward()
    optimizer.step()
```


##### 3.14.2 PPO (Proximal Policy Optimization)

**ç›®çš„é–¢æ•°** (KLæ­£å‰‡åŒ–ä»˜ã):

$$
\mathcal{L}_{\text{PPO}} = \mathbb{E}_{(x,y) \sim \pi_\theta} \left[ r(x, y) - \beta D_{\text{KL}}(\pi_\theta(y \mid x) \| \pi_{\text{ref}}(y \mid x)) \right]
$$

- $\pi_\theta$: Fine-tuningä¸­ã®ãƒãƒªã‚·ãƒ¼ (LLM)
- $\pi_{\text{ref}}$: Reference policy (å…ƒã®LLM)
- $\beta$: KL penaltyä¿‚æ•°

**KLæ­£å‰‡åŒ–ã®ç›®çš„**: $\pi_\theta$ãŒ$\pi_{\text{ref}}$ã‹ã‚‰é ã–ã‹ã‚Šã™ããªã„ã‚ˆã†ã«ã™ã‚‹ (mode collapseé˜²æ­¢)ã€‚

**PPOã®clipped objective**:

$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\text{old}}(a_t \mid s_t)} A_t, \, \text{clip}\left(\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\text{old}}(a_t \mid s_t)}, 1-\epsilon, 1+\epsilon\right) A_t \right) \right]
$$

- $A_t$: Advantage function
- $\epsilon$: Clipping threshold (é€šå¸¸0.2)

**PPOã®å•é¡Œç‚¹**:

- ä¸å®‰å®š (hyperparameteræ•æ„Ÿ)
- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚³ã‚¹ãƒˆé«˜ (on-policy)
- Reward modelã®ãƒã‚¤ã‚¢ã‚¹ã«æ•æ„Ÿ

#### 3.15 DPO (Direct Preference Optimization)

**DPO** [^1] = RLHFã®RLéƒ¨åˆ†ã‚’**ç›´æ¥æœ€é©åŒ–**ã«ç½®ãæ›ãˆã‚‹ã€‚

##### 3.15.1 DPO Lossã®å°å‡º

**RLã®ç›®çš„é–¢æ•°** (å†æ²):

$$
\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot \mid x)} [r(x, y)] - \beta D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})
$$

**æœ€é©ãƒãƒªã‚·ãƒ¼ã®é–‰å½¢å¼è§£**:

$$
\pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)
$$

- $Z(x) = \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$: Partition function

**Reward modelã‚’é€†ç®—**:

$$
r(x, y) = \beta \log \frac{\pi^*(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x)
$$

**Bradley-Terry Modelã«ä»£å…¥**:

$$
\begin{aligned}
P(y_w \succ y_l \mid x) &= \sigma(r(x, y_w) - r(x, y_l)) \\
&= \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right)
\end{aligned}
$$

**DPO Loss**:

$$
\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
$$

**å®Ÿè£…**:

```python
def dpo_loss(pi_theta, pi_ref, x, y_win, y_lose, beta=0.1):
    log_ratio_win = pi_theta.log_prob(y_win, x) - pi_ref.log_prob(y_win, x)
    log_ratio_lose = pi_theta.log_prob(y_lose, x) - pi_ref.log_prob(y_lose, x)

    loss = -torch.log(torch.sigmoid(beta * (log_ratio_win - log_ratio_lose)))
    return loss.mean()
```

**DPOã®åˆ©ç‚¹**:

- **å®‰å®š**: PPOã‚ˆã‚Šå®‰å®š (clippingä¸è¦)
- **åŠ¹ç‡**: Reward modelä¸è¦ (1ã‚¹ãƒ†ãƒƒãƒ—ã§å®Œçµ)
- **ã‚·ãƒ³ãƒ—ãƒ«**: Classification lossã¨åŒã˜

**DPOã®é™ç•Œ**:

- Preferenceãƒ‡ãƒ¼ã‚¿ä¾å­˜ (ãƒ‡ãƒ¼ã‚¿å“è³ªãŒé‡è¦)
- KLæ­£å‰‡åŒ–ã®$\beta$èª¿æ•´ãŒå¿…è¦

##### 3.15.2 DPOã®æ‹¡å¼µ â€” IPO/KTO

**IPO (Identity Preference Optimization)**: DPOã®hinge lossç‰ˆ:

$$
\mathcal{L}_{\text{IPO}} = \mathbb{E} \left[ \left( \log \frac{\pi_\theta(y_w)}{\pi_{\theta_{\text{ref}}}(y_w)} - \log \frac{\pi_\theta(y_l)}{\pi_{\theta_{\text{ref}}}(y_l)} - 1 \right)^2 \right]
$$

**KTO (Kahneman-Tversky Optimization)**: Prospect Theoryãƒ™ãƒ¼ã‚¹:

$$
\mathcal{L}_{\text{KTO}} = \mathbb{E}_{y \sim y_{\text{desirable}}} \left[ v\left( \log \frac{\pi_\theta(y)}{\pi_{\text{ref}}(y)} \right) \right] + \mathbb{E}_{y \sim y_{\text{undesirable}}} \left[ -\lambda v\left( \log \frac{\pi_\theta(y)}{\pi_{\text{ref}}(y)} \right) \right]
$$

- $v(x) = x^\alpha$ (value function)
- $\lambda > 1$: Loss aversionä¿‚æ•°

**2025å¹´ã®ä¸»æµ**: DPO variants (IPO/KTO) ãŒPPOã‚’ç½®ãæ›ãˆã¤ã¤ã‚ã‚‹ã€‚

---

### âš”ï¸ Boss Battle: å®Œå…¨MLOpsãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ•°å¼åˆ†è§£

**ç›®æ¨™**: Trainâ†’Experimentâ†’CI/CDâ†’A/Bâ†’Monitorâ†’Driftâ†’Retrainã®ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«ã‚’æ•°å¼ã§è¨˜è¿°ã™ã‚‹ã€‚

#### Step 1: ãƒ¢ãƒ‡ãƒ«è¨“ç·´ & å®Ÿé¨“è¨˜éŒ²

$$
\begin{aligned}
\mathcal{M}_t &= \arg\min_{\mathbf{w}} \mathcal{L}(\mathbf{w}; \mathcal{D}_{\text{train}}) \\
e_t &= (\mathbf{h}_t, \mathcal{D}_t, \mathbf{m}_t, \mathcal{A}_t) \\
\mathbf{m}_t &= \{\text{val\_acc}, \text{val\_loss}, \text{F1}, \ldots\}
\end{aligned}
$$

MLflowã«è¨˜éŒ²:

$$
\text{MLflow.log}(e_t) \to \text{run\_id}_t
$$

#### Step 2: ãƒ¢ãƒ‡ãƒ«ã‚’Registryã«ç™»éŒ²

$$
\text{Model Registry} \leftarrow (\mathcal{M}_t, \text{run\_id}_t, \text{stage}=\text{Staging})
$$

#### Step 3: CI/CD â€” æ€§èƒ½ãƒ†ã‚¹ãƒˆ

$$
\text{Test Passed} \iff \text{acc}(\mathcal{M}_t, \mathcal{D}_{\text{test}}) \geq \theta_{\text{deploy}}
$$

#### Step 4: ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ (1%)

$$
\begin{aligned}
\text{Traffic Split:} \quad & p_{\text{canary}} = 0.01, \, p_{\text{baseline}} = 0.99 \\
\text{User Assignment:} \quad & u \sim \text{Hash}(u) \mod 100 < 1 \Rightarrow \mathcal{M}_t, \, \text{else} \, \mathcal{M}_{t-1}
\end{aligned}
$$

#### Step 5: A/Bãƒ†ã‚¹ãƒˆ â€” çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œè¨¼

$$
\begin{aligned}
H_0: \, \mu_{\text{canary}} &= \mu_{\text{baseline}} \\
t &= \frac{\bar{x}_{\text{canary}} - \bar{x}_{\text{baseline}}}{\sqrt{s_{\text{canary}}^2/n_{\text{canary}} + s_{\text{baseline}}^2/n_{\text{baseline}}}} \\
\text{Win} &\iff t > t_{0.05, df}
\end{aligned}
$$

#### Step 6: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° â€” SLI/SLOæ¤œè¨¼

$$
\begin{aligned}
\text{SLI}_{\text{latency}} &= \frac{\#(\text{latency} \leq 100\text{ms})}{\#(\text{requests})} \\
\text{SLO Met} &\iff \text{SLI}_{\text{latency}} \geq 0.99
\end{aligned}
$$

#### Step 7: ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º

$$
\begin{aligned}
D_{\text{KS}} &= \sup_x |F_{\text{train}}(x) - F_{\text{prod}}(x)| \\
\text{Drift Detected} &\iff p\text{-value}(D_{\text{KS}}) < 0.01
\end{aligned}
$$

#### Step 8: è‡ªå‹•å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼

$$
\text{Drift Detected} \Rightarrow \text{Trigger}(\text{retrain}, \mathcal{D}_{\text{new}})
$$

#### Step 9: DPO/RLHFã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµ±åˆ

$$
\begin{aligned}
\mathcal{L}_{\text{DPO}} &= -\mathbb{E} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w)}{\pi_{\text{ref}}(y_w)} - \beta \log \frac{\pi_\theta(y_l)}{\pi_{\text{ref}}(y_l)} \right) \right] \\
\pi_{\theta_{t+1}} &\leftarrow \arg\min_{\pi_\theta} \mathcal{L}_{\text{DPO}}(\pi_\theta; \mathcal{D}_{\text{preference}})
\end{aligned}
$$

#### å®Œå…¨ã‚µã‚¤ã‚¯ãƒ«

$$
\boxed{
\text{Train} \to \text{Experiment} \to \text{CI/CD} \to \text{Canary} \to \text{A/B} \to \text{Monitor} \to \text{Drift} \to \text{DPO} \to \text{Retrain}
}
$$

**ã“ã®ãƒ«ãƒ¼ãƒ—ãŒè‡ªå‹•åŒ–ã•ã‚Œã¦ã„ã‚Œã°ã€MLã‚·ã‚¹ãƒ†ãƒ ã¯ "self-healing" ã«ãªã‚‹ã€‚**

:::message
**é€²æ—: 50% å®Œäº†** MLOpså…¨7é ˜åŸŸã®ç†è«–ã‚’å®Œå…¨ç¶²ç¾…ã—ãŸã€‚Zone 4ã§âš¡Julia + ğŸ¦€Rust + ğŸ”®Elixirå®Ÿè£…ã¸ã€‚
:::

---


## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. *NeurIPS 2023*.
@[card](https://arxiv.org/abs/2305.18290)

[^2]: DVC: Data Version Control.
@[card](https://dvc.org/)

[^3]: Great Expectations: Data validation framework.
@[card](https://greatexpectations.io/)

### æ•™ç§‘æ›¸

- Huyen, C. (2022). *Designing Machine Learning Systems*. O'Reilly Media. [URL](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)
- Burkov, A. (2020). *Machine Learning Engineering*. True Positive. [Free PDF](http://www.mlebook.com/)
- Chen, C., Murphy, N., Parisa, K., et al. (2022). *Reliable Machine Learning*. O'Reilly Media.
- Google Cloud. (2021). *MLOps: Continuous delivery and automation pipelines in machine learning*. [Google Cloud Architecture](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)

---

## è¨˜æ³•è¦ç´„

| è¨˜æ³• | æ„å‘³ |
|:-----|:-----|
| $\mathcal{M}_t$ | æ™‚åˆ»$t$ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ (5-tuple) |
| $\mathbf{w}_t$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ« |
| $\mathcal{D}_t$ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ |
| $\mathcal{H}_t$ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é›†åˆ |
| $\mathcal{E}_t$ | ç’°å¢ƒ (Python/CUDA version) |
| $s_t$ | Random seed |
| $e_i$ | å®Ÿé¨“ $i$ (4-tuple: $\mathbf{h}, \mathcal{D}, \mathbf{m}, \mathcal{A}$) |
| $\text{SLI}$ | Service Level Indicator (æ¸¬å®šå¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹) |
| $\text{SLO}$ | Service Level Objective (SLIã®ç›®æ¨™å€¤) |
| $\text{Error Budget}$ | $1 - \text{SLO}$ (è¨±å®¹ã•ã‚Œã‚‹å¤±æ•—ã®é‡) |
| $D_{\text{KL}}(P \| Q)$ | Kullback-Leibler divergence |
| $\text{JSD}(P \| Q)$ | Jensen-Shannon Divergence |
| $D_{\text{KS}}$ | Kolmogorov-Smirnovçµ±è¨ˆé‡ |
| $\text{PSI}$ | Population Stability Index |
| $r(x, y)$ | Reward model |
| $\pi_\theta(y \mid x)$ | Policy (LLM) |
| $\pi_{\text{ref}}(y \mid x)$ | Reference policy |
| $\beta$ | KLæ­£å‰‡åŒ–ä¿‚æ•° |
| $y_w$ | å¥½ã¾ã—ã„å¿œç­” (win) |
| $y_l$ | å¥½ã¾ã—ããªã„å¿œç­” (lose) |
| $\mathcal{L}_{\text{DPO}}$ | Direct Preference Optimization loss |
| $\mathcal{L}_{\text{RM}}$ | Reward Modeling loss (Bradley-Terry) |
| $\alpha$ | æœ‰æ„æ°´æº– (Type I error rate, é€šå¸¸0.05) |
| $\beta$ | Type II error rate (é€šå¸¸0.2 â†’ power = 0.8) |
| $\delta$ | Minimum Detectable Effect (MDE) |
| $n$ | ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦
