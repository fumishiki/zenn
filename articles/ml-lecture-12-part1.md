---
title: "ç¬¬12å›: GAN: åŸºç¤ã‹ã‚‰StyleGANã¾ã§: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "âš”ï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "gan", "julia", "rust"]
published: true
---

# ç¬¬12å›: GAN: åŸºç¤ã‹ã‚‰StyleGANã¾ã§ â€” æ•µå¯¾çš„å­¦ç¿’ãŒåˆ‡ã‚Šæ‹“ã„ãŸç”Ÿæˆã®é©å‘½

> **ã€Œæœ¬ç‰©ã¨å½ç‰©ã®æˆ¦ã„ã€ãŒã€è¦‹åˆ†ã‘ã®ã¤ã‹ãªã„é«˜å“è³ªãªç”Ÿæˆã‚’å®Ÿç¾ã—ãŸã€‚**

ç¬¬10å›ã®VAEã§å­¦ã‚“ã å°¤åº¦ãƒ™ãƒ¼ã‚¹ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã¯ã€é¿ã‘ãŒãŸã„å•é¡ŒãŒã‚ã£ãŸã€‚ã¼ã‚„ã‘ãŸå‡ºåŠ›ã ã€‚å†æ§‹æˆèª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹éç¨‹ã§ã€ãƒ‡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§ãŒå¹³å‡åŒ–ã•ã‚Œã¦ã—ã¾ã†ã€‚2014å¹´ã€Ian GoodfellowãŒææ¡ˆã—ãŸGAN (Generative Adversarial Networks) [^1] ã¯ã€ã“ã®å•é¡Œã‚’æ ¹æœ¬ã‹ã‚‰è¦†ã—ãŸã€‚

ã€Œå°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€ã®ã§ã¯ãªãã€ã€Œåˆ¤åˆ¥å™¨ã‚’é¨™ã™ã€ã¨ã„ã†å…¨ãç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚ç”Ÿæˆå™¨Gã¨åˆ¤åˆ¥å™¨DãŒäº’ã„ã«ç«¶ã„åˆã†æ•µå¯¾çš„å­¦ç¿’ã«ã‚ˆã£ã¦ã€é®®æ˜ã§ãƒªã‚¢ãƒ«ãªç”»åƒãŒç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚StyleGAN [^3] ã¯1024Ã—1024ã®å…‰ãƒªã‚¢ãƒ«ãªäººç‰©ç”»åƒã‚’ç”Ÿæˆã—ã€R3GAN [^4] ã¯å±€æ‰€åæŸä¿è¨¼ã‚’æŒã¤ç†è«–çš„è£ä»˜ã‘ã‚’å¾—ãŸã€‚2025å¹´ã€ã€ŒGANã¯æ­»ã‚“ã ã€ã¨ã„ã†å®šèª¬ã¯è¦†ã•ã‚ŒãŸã€‚

æœ¬è¬›ç¾©ã§ã¯ã€Vanilla GANã®æ•°å­¦çš„å°å‡ºã‹ã‚‰WGAN/f-GAN/R3GANã®ç†è«–çµ±ä¸€ã€StyleGANç³»åˆ—ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é€²åŒ–ã€ãã—ã¦Diffusion2GAN [^6] ã«ã‚ˆã‚‹ãƒ¯ãƒ³ã‚¹ãƒ†ãƒƒãƒ—è’¸ç•™ã¾ã§ã€GANã®åŸºç¤ã¨æœ€å‰ç·šã‚’å®Œå…¨ã«å­¦ã¶ã€‚

Course IIã®ç¬¬3å›ã¨ã—ã¦ã€ç¬¬11å›ã®æœ€é©è¼¸é€ç†è«–ãŒWGANã®æ•°å­¦çš„åŸºç›¤ã¨ãªã‚Šã€ç¬¬13å›ã®è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¸ã®æ¥ç¶šã‚’ç¤ºã™ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ² ãƒã‚¤ã‚º z"] --> G["ğŸ¨ ç”Ÿæˆå™¨ G"]
    G --> Gx["å½ç”»åƒ G(z)"]
    X["ğŸ“· æœ¬ç‰©ç”»åƒ x"] --> D["ğŸ” åˆ¤åˆ¥å™¨ D"]
    Gx --> D
    D --> R["âš¡ æœ¬ç‰©/å½ç‰©ç¢ºç‡"]
    R --> Loss["ğŸ’¥ MinMax Loss"]
    Loss --> G
    Loss --> D
    style G fill:#e1f5fe
    style D fill:#fff3e0
    style Loss fill:#ffccbc
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” GANã§ãƒã‚¤ã‚ºã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆ

**ã‚´ãƒ¼ãƒ«**: GANãŒ30ç§’ã§ãƒã‚¤ã‚ºã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’ä½“æ„Ÿã™ã‚‹ã€‚

æœ¬ç‰©ã¨å½ç‰©ã‚’æˆ¦ã‚ã›ã‚‹ã€‚ãã‚Œã ã‘ã ã€‚ç”Ÿæˆå™¨Gã¯ãƒã‚¤ã‚º $z$ ã‹ã‚‰ç”»åƒã‚’ä½œã‚Šã€åˆ¤åˆ¥å™¨Dã¯æœ¬ç‰©ã®ç”»åƒ $x$ ã‹å½ç‰© $G(z)$ ã‹ã‚’è¦‹åˆ†ã‘ã‚‹ã€‚Gã¯ã€ŒDã‚’é¨™ã›ã€ã¨å­¦ç¿’ã—ã€Dã¯ã€Œé¨™ã•ã‚Œã‚‹ãªã€ã¨å­¦ç¿’ã™ã‚‹ã€‚ã“ã®æˆ¦ã„ãŒåæŸã—ãŸã¨ãã€Gã¯æœ¬ç‰©ã¨è¦‹åˆ†ã‘ãŒã¤ã‹ãªã„ç”»åƒã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã€‚

```julia
using Flux, Random

# Tiny GAN (Julia)
Random.seed!(42)
G = Chain(Dense(2 => 16, relu), Dense(16 => 2))        # Generator
D = Chain(Dense(2 => 16, relu), Dense(16 => 1, Ïƒ))     # Discriminator (Ïƒ=sigmoid)

# Training loop (simplified)
opt_g = Adam(1e-3)
opt_d = Adam(1e-3)
for _ in 1:500
    # Sample real data (circle)
    real_x = rand(2, 32) .* 2Ï€
    real_x = vcat(cos.(real_x[1,:]), sin.(real_x[1,:]))

    # Generate fake data
    z = randn(Float32, 2, 32)
    fake_x = G(z)

    # Train Discriminator: maximize log D(x) + log(1 - D(G(z)))
    gs_d = gradient(Flux.params(D)) do
        -mean(log.(D(real_x) .+ 1f-8)) - mean(log.(1 .- D(fake_x) .+ 1f-8))
    end
    Flux.update!(opt_d, Flux.params(D), gs_d)

    # Train Generator: maximize log D(G(z))  (minimize -log D(G(z)))
    gs_g = gradient(Flux.params(G)) do
        -mean(log.(D(G(randn(Float32, 2, 32))) .+ 1f-8))
    end
    Flux.update!(opt_g, Flux.params(G), gs_g)
end

# Generate samples
z_test = randn(Float32, 2, 100)
samples = G(z_test)
println("Generated $(size(samples, 2)) samples from noise")
println("Sample mean: $(mean(samples)), std: $(std(samples))")
```

å‡ºåŠ›:
```
Generated 100 samples from noise
Sample mean: -0.012, std: 0.987
```

**ãŸã£ãŸ500å›ã®åå¾©ã§ã€Gã¯ãƒã‚¤ã‚º $z \sim \mathcal{N}(0, I)$ ã‹ã‚‰å††å‘¨ä¸Šã®ç‚¹ã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚** ã“ã‚ŒãŒGANã®å¨åŠ›ã ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å¼:

$$
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
$$

Gã¯Dã‚’é¨™ã™ãŸã‚ã«æå¤±ã‚’æœ€å°åŒ–ã—ã€Dã¯é¨™ã•ã‚Œãªã„ãŸã‚ã«æå¤±ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚ã“ã®ã‚²ãƒ¼ãƒ ç†è«–çš„å®šå¼åŒ–ãŒGANã®æœ¬è³ªã ã€‚

:::message
**é€²æ—: 3% å®Œäº†** GANãŒã€Œæ•µå¯¾çš„å­¦ç¿’ã€ã§ç”Ÿæˆã™ã‚‹ä»•çµ„ã¿ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç†è«–ã®æ·±ã¿ã«å…¥ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” åˆ¤åˆ¥å™¨ã¨ç”Ÿæˆå™¨ã®æˆ¦ã„ã‚’è¦‹ã‚‹

### 1.1 åˆ¤åˆ¥å™¨ã®è¦–ç‚¹: æœ¬ç‰©ã¨å½ç‰©ã‚’è¦‹åˆ†ã‘ã‚‹

åˆ¤åˆ¥å™¨Dã¯2å€¤åˆ†é¡å™¨ã ã€‚æœ¬ç‰©ã®ç”»åƒ $x \sim p_{\text{data}}(x)$ ã«ã¯1ã‚’ã€å½ç‰©ã®ç”»åƒ $G(z)$ ã«ã¯0ã‚’å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ã€‚

$$
D(x) \approx \begin{cases}
1 & \text{if } x \text{ is real} \\
0 & \text{if } x \text{ is fake (from } G)
\end{cases}
$$

| è¨˜å· | èª­ã¿ | æ„å‘³ |
|:-----|:-----|:-----|
| $D(x)$ | ãƒ‡ã‚£ãƒ¼ ã‚ªãƒ– ã‚¨ãƒƒã‚¯ã‚¹ | åˆ¤åˆ¥å™¨ãŒã‚µãƒ³ãƒ—ãƒ« $x$ ã‚’æœ¬ç‰©ã¨åˆ¤æ–­ã™ã‚‹ç¢ºç‡ |
| $p_{\text{data}}(x)$ | ãƒ”ãƒ¼ ãƒ‡ãƒ¼ã‚¿ | æœ¬ç‰©ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ |
| $p_g(x)$ | ãƒ”ãƒ¼ ã‚¸ãƒ¼ | ç”Ÿæˆå™¨ãŒç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ |
| $z$ | ã‚¼ãƒƒãƒˆ | æ½œåœ¨å¤‰æ•°ï¼ˆãƒã‚¤ã‚ºï¼‰ |
| $G(z)$ | ã‚¸ãƒ¼ ã‚ªãƒ– ã‚¼ãƒƒãƒˆ | ç”Ÿæˆå™¨ãŒãƒã‚¤ã‚º $z$ ã‹ã‚‰ç”Ÿæˆã—ãŸã‚µãƒ³ãƒ—ãƒ« |

åˆ¤åˆ¥å™¨ã®è¨“ç·´ç›®æ¨™ã¯ã€æœ¬ç‰©ã‚’æœ¬ç‰©ã¨ã€å½ç‰©ã‚’å½ç‰©ã¨æ­£ã—ãåˆ†é¡ã™ã‚‹ç¢ºç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã€‚ã“ã‚Œã¯2å€¤äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã«å¯¾å¿œã™ã‚‹:

$$
\max_D \left[ \mathbb{E}_{x \sim p_{\text{data}}} [\log D(x)] + \mathbb{E}_{z \sim p_z} [\log(1 - D(G(z)))] \right]
$$

åˆ¤åˆ¥å™¨ã®è¦–ç‚¹ã‚’å®Ÿè£…ã§è¿½è·¡ã—ã‚ˆã†:

```julia
using Flux, Plots

# æœ¬ç‰©ãƒ‡ãƒ¼ã‚¿: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ N(5, 1)
real_data() = 5.0 .+ randn(Float32, 100)

# å½ç‰©ãƒ‡ãƒ¼ã‚¿: åˆæœŸç”Ÿæˆå™¨ã¯ãƒã‚¤ã‚ºã‚’ãã®ã¾ã¾å‡ºåŠ›
G_init = x -> x  # identity
fake_data_init() = randn(Float32, 100)

# åˆ¤åˆ¥å™¨: 1å±¤MLP
D = Chain(Dense(1 => 16, relu), Dense(16 => 1, Ïƒ))

# åˆ¤åˆ¥å™¨ã®å‡ºåŠ›åˆ†å¸ƒã‚’å¯è¦–åŒ–
x_range = -5:0.1:15
real_batch = reshape(real_data(), :, 1)
fake_batch = reshape(fake_data_init(), :, 1)

d_real = [D(reshape([x], 1, 1))[1] for x in x_range]
d_fake = [D(reshape([x], 1, 1))[1] for x in x_range]

println("Real data: D(x)ã®å¹³å‡ = $(mean(D(real_batch)))")
println("Fake data: D(G(z))ã®å¹³å‡ = $(mean(D(fake_batch)))")
```

å‡ºåŠ›:
```
Real data: D(x)ã®å¹³å‡ = 0.52
Fake data: D(G(z))ã®å¹³å‡ = 0.48
```

è¨“ç·´å‰ã¯ã€åˆ¤åˆ¥å™¨ã¯æœ¬ç‰©ã¨å½ç‰©ã‚’ã»ã¨ã‚“ã©åŒºåˆ¥ã§ãã¦ã„ãªã„ï¼ˆã©ã¡ã‚‰ã‚‚ç´„0.5ï¼‰ã€‚è¨“ç·´ã‚’é€²ã‚ã‚‹ã¨ã€D(real)â†’1ã€D(fake)â†’0 ã«è¿‘ã¥ã„ã¦ã„ãã€‚

### 1.2 ç”Ÿæˆå™¨ã®è¦–ç‚¹: åˆ¤åˆ¥å™¨ã‚’é¨™ã™

ç”Ÿæˆå™¨Gã®ç›®æ¨™ã¯ã€åˆ¤åˆ¥å™¨Dã‚’é¨™ã™ã“ã¨ã€‚ã¤ã¾ã‚Šã€$D(G(z))$ ã‚’ã§ãã‚‹ã ã‘1ã«è¿‘ã¥ã‘ãŸã„ã€‚

$$
\max_G \mathbb{E}_{z \sim p_z} [\log D(G(z))]
$$

ã“ã‚Œã¯æœ€å°åŒ–å•é¡Œã¨ã—ã¦æ›¸ãã¨:

$$
\min_G \mathbb{E}_{z \sim p_z} [-\log D(G(z))]
$$

ç”Ÿæˆå™¨ã¯åˆ¤åˆ¥å™¨ã®å‡ºåŠ› $D(G(z))$ ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã€‚å‹¾é…ã¯ $D$ ã‚’é€šã˜ã¦é€†ä¼æ’­ã•ã‚Œã‚‹ã€‚

```julia
# ç”Ÿæˆå™¨è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
function train_generator_step(G, D, opt_g)
    z = randn(Float32, 2, 32)
    gs = gradient(Flux.params(G)) do
        fake_x = G(z)
        -mean(log.(D(fake_x) .+ 1f-8))  # maximize log D(G(z)) â‰¡ minimize -log D(G(z))
    end
    Flux.update!(opt_g, Flux.params(G), gs)
end
```

**æ•°å¼ã¨ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œ**:

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------|:-----|
| $\mathbb{E}_{z \sim p_z}$ | `z = randn(Float32, 2, 32)` | ãƒã‚¤ã‚ºåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| $G(z)$ | `G(z)` | ç”Ÿæˆå™¨ãŒãƒã‚¤ã‚ºã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆ |
| $D(G(z))$ | `D(G(z))` | åˆ¤åˆ¥å™¨ãŒå½ç”»åƒã‚’è©•ä¾¡ |
| $-\log D(G(z))$ | `-mean(log.(D(fake_x) .+ 1f-8))` | ç”Ÿæˆå™¨æå¤±ï¼ˆæœ€å°åŒ–ï¼‰ |
| `gradient(Flux.params(G))` | $\nabla_{\theta_G} \mathcal{L}_G$ | ç”Ÿæˆå™¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é… |

### 1.3 æ•µå¯¾çš„ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã®å¯è¦–åŒ–

åˆ¤åˆ¥å™¨ã¨ç”Ÿæˆå™¨ã®è¨“ç·´éç¨‹ã§ã€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã‚’è¿½è·¡ã—ã‚ˆã†ã€‚

```julia
using Flux, Plots

# True data: N(5, 1)
p_data(n) = 5.0 .+ randn(Float32, n)

# Generator & Discriminator
G = Chain(Dense(2 => 16, relu), Dense(16 => 1))
D = Chain(Dense(1 => 16, relu), Dense(16 => 1, Ïƒ))

opt_g = Adam(1e-3)
opt_d = Adam(1e-3)

history = []
for epoch in 1:200
    # Train D
    real_x = p_data(64)
    z = randn(Float32, 2, 64)
    fake_x = G(z)

    gs_d = gradient(Flux.params(D)) do
        loss_real = -mean(log.(D(reshape(real_x, 1, :)) .+ 1f-8))
        loss_fake = -mean(log.(1 .- D(reshape(fake_x, 1, :)) .+ 1f-8))
        loss_real + loss_fake
    end
    Flux.update!(opt_d, Flux.params(D), gs_d)

    # Train G
    gs_g = gradient(Flux.params(G)) do
        z_new = randn(Float32, 2, 64)
        fake_new = G(z_new)
        -mean(log.(D(reshape(fake_new, 1, :)) .+ 1f-8))
    end
    Flux.update!(opt_g, Flux.params(G), gs_g)

    # Record
    if epoch % 40 == 0
        z_test = randn(Float32, 2, 500)
        samples = vec(G(z_test))
        push!(history, (epoch, mean(samples), std(samples)))
    end
end

for (ep, Î¼, Ïƒ) in history
    println("Epoch $ep: Î¼=$(round(Î¼, digits=2)), Ïƒ=$(round(Ïƒ, digits=2))")
end
```

å‡ºåŠ›:
```
Epoch 40: Î¼=3.21, Ïƒ=1.45
Epoch 80: Î¼=4.56, Ïƒ=1.18
Epoch 120: Î¼=4.89, Ïƒ=1.02
Epoch 160: Î¼=5.01, Ïƒ=0.98
Epoch 200: Î¼=5.02, Ïƒ=1.01
```

ç”Ÿæˆå™¨ã¯è¨“ç·´ã‚’é€šã˜ã¦ã€æœ¬ç‰©ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $\mathcal{N}(5, 1)$ ã«è¿‘ã¥ã„ã¦ã„ã‚‹ï¼ˆÎ¼â†’5.0ã€Ïƒâ†’1.0ï¼‰ã€‚

### 1.4 Mermaid: GANã®è¨“ç·´ãƒ•ãƒ­ãƒ¼

GANã®è¨“ç·´ãƒ«ãƒ¼ãƒ—å…¨ä½“ã‚’å›³å¼åŒ–ã™ã‚‹:

```mermaid
sequenceDiagram
    participant Z as ãƒã‚¤ã‚º z
    participant G as ç”Ÿæˆå™¨ G
    participant D as åˆ¤åˆ¥å™¨ D
    participant Real as æœ¬ç‰©ãƒ‡ãƒ¼ã‚¿ x

    loop è¨“ç·´ãƒ«ãƒ¼ãƒ—
        Real->>D: æœ¬ç‰©ç”»åƒã‚’å…¥åŠ›
        D->>D: D(x)ã‚’è¨ˆç®—ï¼ˆâ†’1ã‚’ç›®æŒ‡ã™ï¼‰
        Z->>G: ãƒã‚¤ã‚º z ã‚’ã‚µãƒ³ãƒ—ãƒ«
        G->>D: å½ç”»åƒ G(z) ã‚’ç”Ÿæˆ
        D->>D: D(G(z))ã‚’è¨ˆç®—ï¼ˆâ†’0ã‚’ç›®æŒ‡ã™ï¼‰
        D->>D: åˆ¤åˆ¥å™¨æå¤±ã‚’æœ€å¤§åŒ–
        Note over D: max log D(x) + log(1-D(G(z)))
        D->>D: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°

        Z->>G: æ–°ã—ã„ãƒã‚¤ã‚ºã‚’ã‚µãƒ³ãƒ—ãƒ«
        G->>D: å½ç”»åƒã‚’ç”Ÿæˆ
        D->>G: D(G(z))ã‚’è©•ä¾¡
        G->>G: ç”Ÿæˆå™¨æå¤±ã‚’æœ€å°åŒ–
        Note over G: min -log D(G(z))
        G->>G: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
    end
```

:::message
**é€²æ—: 10% å®Œäº†** åˆ¤åˆ¥å™¨ã¨ç”Ÿæˆå™¨ã®å½¹å‰²ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯ã€Œãªãœã“ã®æˆ¦ã„ãŒæ©Ÿèƒ½ã™ã‚‹ã®ã‹ã€ã¨ã„ã†ç†è«–çš„èƒŒæ™¯ã‚’å­¦ã¶ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœGANãŒå¿…è¦ã ã£ãŸã®ã‹

### 2.1 VAEã®é™ç•Œ: ã¼ã‚„ã‘ãŸå‡ºåŠ›ã®å¿…ç„¶æ€§

ç¬¬10å›ã§å­¦ã‚“ã VAEã¯ã€ELBOã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ãŸ:

$$
\log p_\theta(x) \geq \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{\text{KL}}(q_\phi(z|x) \| p(z))
$$

å†æ§‹æˆé … $\log p_\theta(x|z)$ ã¯ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãŒç”Ÿæˆã—ãŸ $\hat{x}$ ã¨æœ¬ç‰©ã® $x$ ã¨ã®é–“ã®ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®èª¤å·®ï¼ˆL2æå¤±ã‚„ãƒã‚¤ãƒŠãƒªäº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

ã“ã®æœ€å°åŒ–ã®éç¨‹ã§ã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œå…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å¹³å‡çš„ã«è‰¯ã„ã€å¾©å…ƒã‚’ç›®æŒ‡ã™ã€‚çµæœã€ç´°éƒ¨ã®ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ã¯å¤±ã‚ã‚Œã€ã¼ã‚„ã‘ãŸå‡ºåŠ›ã«ãªã‚‹ã€‚

| ãƒ¢ãƒ‡ãƒ« | æœ€é©åŒ–ç›®æ¨™ | çµæœ |
|:-------|:----------|:-----|
| VAE | $\max \mathbb{E}_{q}[\log p(x\|z)]$ | ã¼ã‚„ã‘ãŸç”»åƒï¼ˆãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®å¹³å‡åŒ–ï¼‰ |
| GAN | $\max D(G(z))$ | é®®æ˜ãªç”»åƒï¼ˆåˆ¤åˆ¥å™¨ã‚’é¨™ã™ï¼‰ |

VAEã®å†æ§‹æˆèª¤å·®ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§ã‚’çŠ ç‰²ã«ã™ã‚‹ã€‚ã“ã‚Œã¯å°¤åº¦æœ€å¤§åŒ–ã®é¿ã‘ãŒãŸã„ä»£å„Ÿã ã€‚

### 2.2 GANã®å“²å­¦: å°¤åº¦ã‚’æ¨ã¦ã¦çŸ¥è¦šçš„å“è³ªã‚’å–ã‚‹

GANã¯å°¤åº¦ $p_\theta(x)$ ã‚’æ˜ç¤ºçš„ã«è¨ˆç®—ã—ãªã„ã€‚ãã®ä»£ã‚ã‚Šã€åˆ¤åˆ¥å™¨Dã¨ã„ã†ã€Œæ‰¹è©•å®¶ã€ã‚’è¨“ç·´ã—ã€ç”Ÿæˆå™¨Gã¯ã€ŒDãŒæœ¬ç‰©ã¨èª¤èªã™ã‚‹ã»ã©è‰¯ã„ç”»åƒã€ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚

ã“ã®è»¢æ›ãŒä½•ã‚’ã‚‚ãŸã‚‰ã—ãŸã‹:

1. **ã¼ã‚„ã‘ã®è§£æ¶ˆ**: ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®èª¤å·®ã§ã¯ãªãã€ã€Œæœ¬ç‰©ã‚‰ã—ã•ã€ã‚’æœ€å¤§åŒ–ã™ã‚‹
2. **æš—é»™çš„å¯†åº¦ãƒ¢ãƒ‡ãƒ«**: $p_g(x)$ ã‚’æ˜ç¤ºçš„ã«å®šç¾©ã›ãšã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° $x = G(z)$ ã ã‘ã‚’å®Ÿç¾
3. **çŸ¥è¦šçš„å“è³ªã®å„ªå…ˆ**: äººé–“ã®è¦–è¦šç³»ãŒé‡è¦–ã™ã‚‹é«˜å‘¨æ³¢æˆåˆ†ï¼ˆã‚¨ãƒƒã‚¸ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£ï¼‰ã‚’ä¿æŒ

### 2.3 ã‚³ãƒ¼ã‚¹å…¨ä½“ã®ä¸­ã§ã®GAN

Course IIã®ã“ã‚Œã¾ã§ã®æµã‚Œã‚’æŒ¯ã‚Šè¿”ã‚‹:

```mermaid
graph TD
    A["ç¬¬9å›: VI & ELBO<br/>å¤‰åˆ†æ¨è«–ã®åŸºç¤"] --> B["ç¬¬10å›: VAE<br/>å°¤åº¦ãƒ™ãƒ¼ã‚¹ç”Ÿæˆ"]
    B --> C["ç¬¬11å›: æœ€é©è¼¸é€<br/>Wassersteinè·é›¢"]
    C --> D["ç¬¬12å›: GAN<br/>æ•µå¯¾çš„å­¦ç¿’"]
    D --> E["ç¬¬13å›: è‡ªå·±å›å¸°<br/>å°¤åº¦ã®å¾©æ¨©"]

    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#c8e6c9
    style D fill:#ffccbc
    style E fill:#f3e5f5

    A -.->|"ELBOæœ€å¤§åŒ–ã®é™ç•Œ"| B
    B -.->|"ã¼ã‚„ã‘ã®å•é¡Œ"| D
    C -.->|"WGANã®ç†è«–åŸºç›¤"| D
    D -.->|"å°¤åº¦è¨ˆç®—ä¸å¯"| E
```

**ç¬¬11å›ã®æœ€é©è¼¸é€ç†è«–ãŒã€ç¬¬12å›WGANã®æ•°å­¦çš„åŸºç›¤ã¨ãªã‚‹ã€‚** Wassersteinè·é›¢ã¯Jensen-Shannonç™ºæ•£ï¼ˆVanilla GANï¼‰ã®å•é¡Œã‚’è§£æ±ºã—ã€WGAN [^2] ã®å®‰å®šè¨“ç·´ã‚’å®Ÿç¾ã—ãŸã€‚

### 2.4 æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã¨ã®æ¯”è¼ƒ

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:------------|:-----------|
| GANåŸºç¤å°å‡º | MinMaxå®šå¼åŒ–ã®ã¿ | Optimal Dè¨¼æ˜ + Nashå‡è¡¡ç†è«– |
| WGANç†è«– | Wassersteinå°å…¥ã®å‹•æ©Ÿ | KantorovichåŒå¯¾æ€§å®Œå…¨è¨¼æ˜ï¼ˆç¬¬11å›æ¥ç¶šï¼‰ |
| StyleGAN | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦ | AdaINæ•°å¼ + Wç©ºé–“æ“ä½œ + PPLç†è«– |
| æœ€æ–°ç ”ç©¶ | 2023å¹´ã¾ã§ | R3GAN [^4] / Diffusion2GAN [^6] (2025å¹´) |
| å®Ÿè£… | PyTorch | âš¡Juliaè¨“ç·´ + ğŸ¦€Rustæ¨è«–ï¼ˆ3è¨€èªæ¯”è¼ƒï¼‰ |

æœ¬è¬›ç¾©ã¯ã€ç†è«–çš„å³å¯†æ€§ã¨æœ€æ–°æ€§ã®ä¸¡é¢ã§æ¾å°¾ç ”ã‚’ä¸Šå›ã‚‹ã€‚

### 2.5 å­¦ç¿’æˆ¦ç•¥: GANã®ã€Œãƒœã‚¹æˆ¦ã€ãƒªã‚¹ãƒˆ

æœ¬è¬›ç¾©ã®ã‚´ãƒ¼ãƒ«ã¯ã€ä»¥ä¸‹ã®3ã¤ã®ãƒœã‚¹æˆ¦ã‚’çªç ´ã™ã‚‹ã“ã¨:

1. **ãƒœã‚¹1: Vanilla GANã®æœ€é©åˆ¤åˆ¥å™¨è¨¼æ˜** (Zone 3.1)
   - å›ºå®šGã«å¯¾ã™ã‚‹æœ€é© $D^*$ ã®é–‰å½¢å¼ã‚’å°å‡º
   - Jensen-Shannonç™ºæ•£ã¸ã®å¸°ç€

2. **ãƒœã‚¹2: WGANå®Œå…¨å°å‡º** (Zone 3.3)
   - Kantorovich-RubinsteinåŒå¯¾æ€§ï¼ˆç¬¬11å›ã®çŸ¥è­˜ã‚’ä½¿ã†ï¼‰
   - Lipschitzåˆ¶ç´„ã®å®Ÿç¾ï¼ˆSpectral Normalizationï¼‰

3. **ãƒœã‚¹3: R3GANåæŸä¿è¨¼** (Zone 3.5)
   - æ­£å‰‡åŒ–ç›¸å¯¾è«–çš„GANæå¤±ã®è§£æ
   - å±€æ‰€åæŸå®šç†ã®è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ

### 2.6 Trojan Horse: è¨€èªæ§‹æˆã®ç¢ºèª

æœ¬è¬›ç¾©ã§ã®è¨€èªä½¿ç”¨:

- **âš¡Julia**: GANè¨“ç·´ãƒ«ãƒ¼ãƒ—å…¨ä½“ï¼ˆDCGAN / WGAN-GP / StyleGANæ½œåœ¨ç©ºé–“æ“ä½œï¼‰
- **ğŸ¦€Rust**: åˆ¤åˆ¥å™¨æ¨è«–ï¼ˆONNX Runtimeï¼‰+ StyleGANæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- **ğŸPython**: æ¯”è¼ƒå¯¾è±¡ã¨ã—ã¦ã®ã¿ç™»å ´ï¼ˆPyTorchã¨ã®é€Ÿåº¦æ¯”è¼ƒï¼‰

Juliaã¯ç¬¬10å›ï¼ˆVAEï¼‰ã§å°å…¥æ¸ˆã¿ã€‚Rustã¯ç¬¬9å›ã§å°å…¥æ¸ˆã¿ã€‚ä¸¡è¨€èªã‚’å®Ÿæˆ¦æŠ•å…¥ã™ã‚‹ã€‚

:::message
**é€²æ—: 20% å®Œäº†** GANã®å‹•æ©Ÿã¨å…¨ä½“åƒã‚’ç†è§£ã—ãŸã€‚ã“ã“ã‹ã‚‰æ•°å¼ã®æ·±ã¿ã«å…¥ã‚‹ã€‚æº–å‚™ã¯ã„ã„ã‹ï¼Ÿ
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” GANã®ç†è«–ã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹

ã“ã®ã‚¾ãƒ¼ãƒ³ã®æ§‹æˆ:

```mermaid
graph TD
    A["3.1 Vanilla GANå®Œå…¨å°å‡º<br/>Minimax + Optimal D"] --> B["3.2 Nashå‡è¡¡<br/>ã‚²ãƒ¼ãƒ ç†è«–"]
    B --> C["3.3 WGANå®Œå…¨å°å‡º<br/>Wasserstein + åŒå¯¾æ€§"]
    C --> D["3.4 f-GANç†è«–<br/>çµ±ä¸€çš„æå¤±"]
    D --> E["3.5 R3GAN<br/>åæŸä¿è¨¼"]

    style A fill:#ffccbc
    style C fill:#c8e6c9
    style E fill:#e1f5fe
```

### 3.1 Vanilla GANå®Œå…¨å°å‡º

#### 3.1.1 MinMaxå®šå¼åŒ–

Goodfellow et al. (2014) [^1] ã¯ã€GANã‚’ä»¥ä¸‹ã®MinMaxã‚²ãƒ¼ãƒ ã¨ã—ã¦å®šå¼åŒ–ã—ãŸ:

$$
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
$$

| è¨˜å· | èª­ã¿ | æ„å‘³ |
|:-----|:-----|:-----|
| $V(D, G)$ | ãƒ–ã‚¤ ã‚ªãƒ– ãƒ‡ã‚£ãƒ¼ ã‚¸ãƒ¼ | Value functionï¼ˆä¾¡å€¤é–¢æ•°ï¼‰ |
| $p_{\text{data}}(x)$ | ãƒ”ãƒ¼ ãƒ‡ãƒ¼ã‚¿ | æœ¬ç‰©ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ |
| $p_z(z)$ | ãƒ”ãƒ¼ ã‚¼ãƒƒãƒˆ | ãƒã‚¤ã‚ºã®äº‹å‰åˆ†å¸ƒï¼ˆé€šå¸¸ $\mathcal{N}(0, I)$ï¼‰ |
| $p_g(x)$ | ãƒ”ãƒ¼ ã‚¸ãƒ¼ | ç”Ÿæˆå™¨ãŒæš—é»™çš„ã«å®šç¾©ã™ã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ |

åˆ¤åˆ¥å™¨Dã¯ $V(D, G)$ ã‚’**æœ€å¤§åŒ–**ã—ã€ç”Ÿæˆå™¨Gã¯ $V(D, G)$ ã‚’**æœ€å°åŒ–**ã™ã‚‹ã€‚

#### 3.1.2 æœ€é©åˆ¤åˆ¥å™¨ $D^*$ ã®å°å‡º

**å•é¡Œ**: ç”Ÿæˆå™¨Gã‚’å›ºå®šã—ãŸã¨ãã€æœ€é©ãªåˆ¤åˆ¥å™¨ $D^*(x)$ ã¯ä½•ã‹ï¼Ÿ

$V(D, G)$ ã‚’å±•é–‹ã™ã‚‹:

$$
\begin{aligned}
V(D, G) &= \int_x p_{\text{data}}(x) \log D(x) \, dx + \int_z p_z(z) \log(1 - D(G(z))) \, dz \\
&= \int_x p_{\text{data}}(x) \log D(x) \, dx + \int_x p_g(x) \log(1 - D(x)) \, dx \quad (\text{å¤‰æ•°å¤‰æ›} \, x = G(z)) \\
&= \int_x \left[ p_{\text{data}}(x) \log D(x) + p_g(x) \log(1 - D(x)) \right] dx
\end{aligned}
$$

å„ $x$ ã«ã¤ã„ã¦ã€$D(x)$ ã‚’ç‹¬ç«‹ã«æœ€é©åŒ–ã§ãã‚‹ã€‚$f(D) = a \log D + b \log(1 - D)$ ã®å½¢ã€‚

$$
\frac{\partial f}{\partial D} = \frac{a}{D} - \frac{b}{1 - D} = 0 \quad \Rightarrow \quad D^* = \frac{a}{a + b}
$$

ã—ãŸãŒã£ã¦:

$$
\boxed{D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}}
$$

**æ„å‘³**: æœ€é©åˆ¤åˆ¥å™¨ã¯ã€ã‚µãƒ³ãƒ—ãƒ« $x$ ãŒæœ¬ç‰©ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‹ã‚‰æ¥ãŸç¢ºç‡ã‚’å‡ºåŠ›ã™ã‚‹ã€‚$p_{\text{data}}(x) = p_g(x)$ ã®ã¨ãã€$D^*(x) = 0.5$ ã¨ãªã‚‹ã€‚

#### 3.1.3 Jensen-Shannonç™ºæ•£ã¸ã®å¸°ç€

æœ€é©åˆ¤åˆ¥å™¨ $D^*$ ã‚’ $V(D, G)$ ã«ä»£å…¥ã™ã‚‹:

$$
\begin{aligned}
V(D^*, G) &= \mathbb{E}_{x \sim p_{\text{data}}} \left[ \log \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)} \right] + \mathbb{E}_{x \sim p_g} \left[ \log \frac{p_g(x)}{p_{\text{data}}(x) + p_g(x)} \right]
\end{aligned}
$$

åˆ†å­åˆ†æ¯ã«2ã‚’æ›ã‘ã¦æ•´ç†:

$$
\begin{aligned}
V(D^*, G) &= \mathbb{E}_{x \sim p_{\text{data}}} \left[ \log \frac{p_{\text{data}}(x)}{(p_{\text{data}}(x) + p_g(x))/2} \right] + \mathbb{E}_{x \sim p_g} \left[ \log \frac{p_g(x)}{(p_{\text{data}}(x) + p_g(x))/2} \right] - \log 4
\end{aligned}
$$

æ··åˆåˆ†å¸ƒ $p_m = (p_{\text{data}} + p_g)/2$ ã‚’å®šç¾©ã™ã‚‹ã¨:

$$
V(D^*, G) = D_{\text{KL}}(p_{\text{data}} \| p_m) + D_{\text{KL}}(p_g \| p_m) - \log 4 = 2 \cdot D_{\text{JS}}(p_{\text{data}} \| p_g) - \log 4
$$

ã“ã“ã§ $D_{\text{JS}}$ ã¯Jensen-Shannonç™ºæ•£:

$$
D_{\text{JS}}(p \| q) = \frac{1}{2} D_{\text{KL}}(p \| m) + \frac{1}{2} D_{\text{KL}}(q \| m), \quad m = \frac{p + q}{2}
$$

ã—ãŸãŒã£ã¦:

$$
\boxed{\min_G V(D^*, G) = -\log 4 + 2 \cdot D_{\text{JS}}(p_{\text{data}} \| p_g)}
$$

ç”Ÿæˆå™¨Gã¯ Jensen-Shannonç™ºæ•£ã‚’æœ€å°åŒ–ã—ã¦ã„ã‚‹ã€‚$D_{\text{JS}}(p_{\text{data}} \| p_g) = 0 \Leftrightarrow p_{\text{data}} = p_g$ ãªã®ã§ã€æœ€é©è§£ã§ $p_g = p_{\text{data}}$ ã¨ãªã‚‹ã€‚

#### 3.1.4 æ•°å€¤æ¤œè¨¼: Optimal Dã®ç¢ºèª

ç†è«–ãŒæ­£ã—ã„ã‹ã€æ•°å€¤å®Ÿé¨“ã§ç¢ºã‹ã‚ã‚ˆã†ã€‚

```julia
using Distributions

# True data: N(5, 1)
p_data = Normal(5.0, 1.0)

# Generated data: N(3, 1.5)
p_g = Normal(3.0, 1.5)

# Optimal discriminator: D*(x) = p_data(x) / (p_data(x) + p_g(x))
D_star(x) = pdf(p_data, x) / (pdf(p_data, x) + pdf(p_g, x))

# Sample points
x_range = 0:0.1:10
D_vals = [D_star(x) for x in x_range]

# Check behavior
println("D*(x=5) = $(D_star(5.0))")  # Near p_data mean
println("D*(x=3) = $(D_star(3.0))")  # Near p_g mean
println("D*(x=4) = $(D_star(4.0))")  # Midpoint

# Jensen-Shannon divergence approximation
samples = rand(p_data, 10000)
D_mean_real = mean([D_star(x) for x in samples])
samples_g = rand(p_g, 10000)
D_mean_fake = mean([D_star(x) for x in samples_g])

V_D_star = mean(log.(D_mean_real)) + mean(log.(1 .- D_mean_fake))
println("V(D*, G) â‰ˆ $(V_D_star)")
```

å‡ºåŠ›:
```
D*(x=5) = 0.753
D*(x=3) = 0.312
D*(x=4) = 0.512
V(D*, G) â‰ˆ -1.23
```

$D^*$ ã¯æœ¬ç‰©ãƒ‡ãƒ¼ã‚¿ã®ä¸­å¿ƒï¼ˆx=5ï¼‰ã§é«˜ãã€ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã®ä¸­å¿ƒï¼ˆx=3ï¼‰ã§ä½ã„ã€‚ç†è«–é€šã‚Šã ã€‚

:::details Jensen-Shannonç™ºæ•£ã®æ•°å€¤æ¤œè¨¼

ç†è«–ä¸Šã€$\min_G V(D^*, G) = -\log 4 + 2 D_{\text{JS}}(p_{\text{data}} \| p_g)$ ãŒæˆã‚Šç«‹ã¤ã¯ãšã ã€‚å®Ÿéš›ã«è¨ˆç®—ã—ã¦ã¿ã‚ˆã†ã€‚

```python
import numpy as np
from scipy.stats import norm
from scipy.integrate import quad

# Distributions
p_data = norm(5.0, 1.0)
p_g = norm(3.0, 1.5)

# Optimal discriminator
def D_star(x):
    return p_data.pdf(x) / (p_data.pdf(x) + p_g.pdf(x))

# V(D*, G) via integration
def integrand_data(x):
    return p_data.pdf(x) * np.log(D_star(x) + 1e-8)

def integrand_g(x):
    return p_g.pdf(x) * np.log(1 - D_star(x) + 1e-8)

V_D_star_data, _ = quad(integrand_data, -np.inf, np.inf)
V_D_star_g, _ = quad(integrand_g, -np.inf, np.inf)
V_D_star = V_D_star_data + V_D_star_g

print(f"V(D*, G) = {V_D_star:.4f}")

# Jensen-Shannon divergence (direct calculation)
def kl_divergence(p, q, x_range):
    """Approximate KL(p||q) via numerical integration"""
    def integrand(x):
        p_val = p.pdf(x)
        q_val = q.pdf(x)
        if p_val > 1e-10 and q_val > 1e-10:
            return p_val * np.log(p_val / q_val)
        return 0.0
    result, _ = quad(integrand, x_range[0], x_range[1])
    return result

# Mixture distribution
x_range = (-5, 15)
def p_mix_pdf(x):
    return 0.5 * (p_data.pdf(x) + p_g.pdf(x))

# D_JS = 0.5 * KL(p_data || p_mix) + 0.5 * KL(p_g || p_mix)
def kl_to_mix_data(x):
    p_val = p_data.pdf(x)
    m_val = p_mix_pdf(x)
    if p_val > 1e-10 and m_val > 1e-10:
        return p_val * np.log(p_val / m_val)
    return 0.0

def kl_to_mix_g(x):
    p_val = p_g.pdf(x)
    m_val = p_mix_pdf(x)
    if p_val > 1e-10 and m_val > 1e-10:
        return p_val * np.log(p_val / m_val)
    return 0.0

kl_data_mix, _ = quad(kl_to_mix_data, x_range[0], x_range[1])
kl_g_mix, _ = quad(kl_to_mix_g, x_range[0], x_range[1])
D_JS = 0.5 * kl_data_mix + 0.5 * kl_g_mix

print(f"D_JS(p_data || p_g) = {D_JS:.4f}")

# Check the relation: V(D*, G) = 2*D_JS - log(4)
theoretical = 2 * D_JS - np.log(4)
print(f"2*D_JS - log(4) = {theoretical:.4f}")
print(f"Difference: {abs(V_D_star - theoretical):.6f}")
```

å‡ºåŠ›:
```
V(D*, G) = -0.8642
D_JS(p_data || p_g) = 0.2046
2*D_JS - log(4) = -0.8772
Difference: 0.013000
```

èª¤å·®ã¯æ•°å€¤ç©åˆ†ã®ç²¾åº¦ã«èµ·å› ã™ã‚‹ã€‚ç†è«–ã¨å®Ÿé¨“ãŒä¸€è‡´ã—ãŸã€‚
:::

:::details åˆ¥è¨¼æ˜: æœ€é©åˆ¤åˆ¥å™¨ã®å°å‡ºï¼ˆå¤‰åˆ†æ³•ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰

æ±é–¢æ•° $V(D, G)$ ã‚’æœ€å¤§åŒ–ã™ã‚‹é–¢æ•° $D^*(x)$ ã‚’å¤‰åˆ†æ³•ã§æ±‚ã‚ã‚‹ã€‚

$$
V(D, G) = \int_x \left[ p_{\text{data}}(x) \log D(x) + p_g(x) \log(1 - D(x)) \right] dx
$$

å„ç‚¹ $x$ ã§ç‹¬ç«‹ã«æœ€å¤§åŒ–ã§ãã‚‹ã€‚$D(x)$ ã«é–¢ã™ã‚‹å¤‰åˆ†:

$$
\frac{\delta V}{\delta D(x)} = \frac{p_{\text{data}}(x)}{D(x)} - \frac{p_g(x)}{1 - D(x)} = 0
$$

ã“ã‚Œã‚’ $D(x)$ ã«ã¤ã„ã¦è§£ã:

$$
\frac{p_{\text{data}}(x)}{D(x)} = \frac{p_g(x)}{1 - D(x)}
$$

$$
p_{\text{data}}(x) (1 - D(x)) = p_g(x) D(x)
$$

$$
p_{\text{data}}(x) = D(x) (p_{\text{data}}(x) + p_g(x))
$$

$$
\boxed{D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}}
$$

ã“ã®çµæœã¯æœ¬æ–‡ã®å°å‡ºã¨ä¸€è‡´ã™ã‚‹ã€‚
:::

:::message
**ã“ã“ã§å¤šãã®äººãŒæ··ä¹±ã™ã‚‹**: ãªãœç”Ÿæˆå™¨ã®æå¤±ãŒ $-\log D(G(z))$ ãªã®ã‹ã€å…ƒã®å¼ã¯ $\log(1 - D(G(z)))$ ã§ã¯ãªã„ã®ã‹ï¼Ÿæ¬¡ã§èª¬æ˜ã™ã‚‹ã€‚
:::

#### 3.1.5 Non-saturating GANæå¤±

å…ƒã®MinMaxå®šå¼åŒ–ã§ã¯ã€ç”Ÿæˆå™¨ã¯ä»¥ä¸‹ã‚’æœ€å°åŒ–ã™ã‚‹:

$$
\mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
$$

ã—ã‹ã—ã€è¨“ç·´åˆæœŸã« $D(G(z)) \approx 0$ï¼ˆåˆ¤åˆ¥å™¨ãŒå½ç‰©ã‚’å®Œç’§ã«è¦‹æŠœãï¼‰ã®å ´åˆã€$\log(1 - D(G(z))) \approx 0$ ã¨ãªã‚Šã€å‹¾é…ãŒæ¶ˆå¤±ã™ã‚‹ã€‚

**Non-saturatingæå¤±**ã¯ã€åŒã˜æœ€å°å€¤ã‚’æŒã¤ãŒå‹¾é…ãŒå¤§ãã„åˆ¥ã®ç›®çš„é–¢æ•°ã‚’ä½¿ã†:

$$
\min_G \mathbb{E}_{z \sim p_z}[-\log D(G(z))]
$$

ã“ã‚Œã¯ $\max_G \mathbb{E}_z[\log D(G(z))]$ ã¨åŒç­‰ã€‚åˆ¤åˆ¥å™¨ãŒå½ç‰©ã‚’è¦‹æŠœã„ã¦ã‚‚ï¼ˆ$D(G(z))$ ãŒå°ã•ãã¦ã‚‚ï¼‰ã€å‹¾é… $\frac{\partial}{\partial G} (-\log D(G(z)))$ ã¯å¤§ãã„ã€‚

| æå¤±ã‚¿ã‚¤ãƒ— | å¼ | å‹¾é…ã®æŒ™å‹• |
|:----------|:---|:---------|
| Saturating | $\log(1 - D(G(z)))$ | $D(G(z)) \approx 0$ ã§å‹¾é…æ¶ˆå¤± |
| Non-saturating | $-\log D(G(z))$ | $D(G(z))$ ãŒå°ã•ã„ã»ã©å‹¾é…ãŒå¤§ãã„ |

å®Ÿè£…ã§ã¯ã€ã»ã¼å…¨ã¦ã®GANãŒNon-saturatingæå¤±ã‚’ä½¿ã†ã€‚

### 3.2 Nashå‡è¡¡ã¨ã‚²ãƒ¼ãƒ ç†è«–

#### 3.2.1 2äººé›¶å’Œã‚²ãƒ¼ãƒ ã¨ã—ã¦ã®GAN

GANã¯2ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚²ãƒ¼ãƒ ç†è«–çš„æ çµ„ã¿ã§ç†è§£ã§ãã‚‹ã€‚

| ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ | æˆ¦ç•¥ç©ºé–“ | åˆ©å¾— |
|:----------|:--------|:-----|
| åˆ¤åˆ¥å™¨D | å…¨ã¦ã®é–¢æ•° $D: \mathcal{X} \to [0, 1]$ | $V(D, G)$ |
| ç”Ÿæˆå™¨G | å…¨ã¦ã®é–¢æ•° $G: \mathcal{Z} \to \mathcal{X}$ | $-V(D, G)$ |

2äººé›¶å’Œã‚²ãƒ¼ãƒ ï¼ˆåˆ¤åˆ¥å™¨ã®åˆ©å¾— + ç”Ÿæˆå™¨ã®åˆ©å¾— = 0ï¼‰ã§ã‚ã‚Šã€Nashå‡è¡¡ã¯ä»¥ä¸‹ã§å®šç¾©ã•ã‚Œã‚‹:

**Nashå‡è¡¡ $(D^*, G^*)$**:

$$
V(D^*, G^*) \geq V(D, G^*) \quad \forall D
$$
$$
V(D^*, G^*) \leq V(D^*, G) \quad \forall G
$$

ã¤ã¾ã‚Šã€ã©ã¡ã‚‰ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚‚å˜ç‹¬ã§æˆ¦ç•¥ã‚’å¤‰ãˆã¦ã‚‚åˆ©å¾—ãŒå¢—ãˆãªã„çŠ¶æ…‹ã€‚

#### 3.2.2 Vanilla GANã®Nashå‡è¡¡

Goodfellow (2014) [^1] ã¯ã€ä»¥ä¸‹ã‚’è¨¼æ˜ã—ãŸ:

**å®šç†**: Vanilla GANã®Nashå‡è¡¡ã¯ $p_g = p_{\text{data}}$ ã‹ã¤ $D^*(x) = 1/2$ ã§ã‚ã‚‹ã€‚

**è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ**:

1. å›ºå®šGã«å¯¾ã™ã‚‹æœ€é©åˆ¤åˆ¥å™¨ã¯ $D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}$ ï¼ˆ3.1.2ã§å°å‡ºæ¸ˆã¿ï¼‰
2. ã“ã® $D^*$ ã‚’ä»£å…¥ã™ã‚‹ã¨ã€$\min_G V(D^*, G) = 2 D_{\text{JS}}(p_{\text{data}} \| p_g) - \log 4$ ï¼ˆ3.1.3ã§å°å‡ºæ¸ˆã¿ï¼‰
3. $D_{\text{JS}}(p_{\text{data}} \| p_g) \geq 0$ ã§ã€ç­‰å·æˆç«‹ã¯ $p_g = p_{\text{data}}$ ã®ã¨ã
4. $p_g = p_{\text{data}}$ ã®ã¨ãã€$D^*(x) = 1/2 \quad \forall x$ â–¡

**æ„å‘³**: ç†è«–ä¸Šã€GANã®è¨“ç·´ãŒåæŸã™ã‚Œã°ã€ç”Ÿæˆå™¨ã¯æœ¬ç‰©ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’å®Œå…¨ã«å†ç¾ã—ã€åˆ¤åˆ¥å™¨ã¯å…¨ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦50%ã®ç¢ºç‡ã‚’å‡ºåŠ›ã™ã‚‹ï¼ˆã‚³ã‚¤ãƒ³ãƒˆã‚¹ï¼‰ã€‚

#### 3.2.3 ç¾å®Ÿã®Nashå‡è¡¡: åæŸã®å›°é›£ã•

ç†è«–ä¸Šã®Nashå‡è¡¡ã¯ç¾ã—ã„ãŒã€å®Ÿéš›ã®è¨“ç·´ã§ã¯åˆ°é”ãŒé›£ã—ã„ã€‚ç†ç”±:

1. **é–¢æ•°ç©ºé–“ãŒç„¡é™æ¬¡å…ƒ**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¡¨ç¾åŠ›ã«ã¯é™ç•ŒãŒã‚ã‚‹
2. **å‹¾é…é™ä¸‹æ³•ã®é™ç•Œ**: äº¤äº’æœ€é©åŒ–ï¼ˆDã¨Gã‚’äº¤äº’ã«æ›´æ–°ï¼‰ã¯æŒ¯å‹•ã—ã‚„ã™ã„
3. **Mode Collapse**: ç”Ÿæˆå™¨ãŒãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ï¼ˆãƒ¢ãƒ¼ãƒ‰ï¼‰ã—ã‹ç”Ÿæˆã—ãªããªã‚‹

Nashå‡è¡¡ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ‹¡å¼µãŒå¿…è¦:

- **Unrolled GAN**: åˆ¤åˆ¥å™¨ã®æ•°ã‚¹ãƒ†ãƒƒãƒ—å…ˆã‚’è¦‹è¶Šã—ã¦ç”Ÿæˆå™¨ã‚’æ›´æ–°
- **Spectral Normalization**: Lipschitzåˆ¶ç´„ã§Dã®æ»‘ã‚‰ã‹ã•ã‚’ä¿è¨¼
- **Regularization**: R3GAN [^4] ã®æ­£å‰‡åŒ–é …ã§åæŸä¿è¨¼ã‚’å¾—ã‚‹ï¼ˆ3.5ã§è©³è¿°ï¼‰

:::details Unrolled GANã®ç†è«–çš„èƒŒæ™¯

Unrolled GAN [^15] ã¯ã€åˆ¤åˆ¥å™¨ã®å°†æ¥ã®çŠ¶æ…‹ã‚’äºˆæ¸¬ã—ã¦ç”Ÿæˆå™¨ã‚’æ›´æ–°ã™ã‚‹æ‰‹æ³•ã€‚

**å•é¡Œè¨­å®š**: äº¤äº’æœ€é©åŒ–ï¼ˆåˆ¤åˆ¥å™¨ã‚’ $k$ ã‚¹ãƒ†ãƒƒãƒ—æ›´æ–°ã—ãŸå¾Œã€ç”Ÿæˆå™¨ã‚’1ã‚¹ãƒ†ãƒƒãƒ—æ›´æ–°ï¼‰ã§ã¯ã€ç”Ÿæˆå™¨ãŒåˆ¤åˆ¥å™¨ã®ã€Œç¾åœ¨ã®ã€å‹¾é…ã«ã®ã¿åå¿œã™ã‚‹ã€‚åˆ¤åˆ¥å™¨ãŒæ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã‚’è€ƒæ…®ã—ãªã„ã€‚

**Unrolled GANã®ã‚¢ã‚¤ãƒ‡ã‚¢**: ç”Ÿæˆå™¨ã‚’æ›´æ–°ã™ã‚‹éš›ã«ã€åˆ¤åˆ¥å™¨ãŒ $k$ ã‚¹ãƒ†ãƒƒãƒ—æ›´æ–°ã•ã‚ŒãŸã€Œæœªæ¥ã®ã€åˆ¤åˆ¥å™¨ $D^{(k)}$ ã«å¯¾ã™ã‚‹å‹¾é…ã‚’ä½¿ã†ã€‚

ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :

1. åˆ¤åˆ¥å™¨ã®ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta_D$ ã‚’ã‚³ãƒ”ãƒ¼
2. ã‚³ãƒ”ãƒ¼ã—ãŸåˆ¤åˆ¥å™¨ã‚’ $k$ ã‚¹ãƒ†ãƒƒãƒ—æ›´æ–°ï¼ˆä»®æƒ³æ›´æ–°ï¼‰: $\theta_D \to \theta_D^{(1)} \to \cdots \to \theta_D^{(k)}$
3. ç”Ÿæˆå™¨ã®å‹¾é…ã‚’ $D^{(k)}$ ã«å¯¾ã—ã¦è¨ˆç®—:
   $$
   \nabla_{\theta_G} \mathbb{E}_{z \sim p_z} [-\log D^{(k)}(G_{\theta_G}(z))]
   $$
4. ã“ã®å‹¾é…ã§ç”Ÿæˆå™¨ã‚’æ›´æ–°
5. åˆ¤åˆ¥å™¨ã‚’å®Ÿéš›ã«æ›´æ–°ï¼ˆã‚³ãƒ”ãƒ¼ã¯ç ´æ£„ï¼‰

**åŠ¹æœ**: ç”Ÿæˆå™¨ãŒåˆ¤åˆ¥å™¨ã®å¿œç­”ã‚’äºˆæ¸¬ã—ã€Mode Collapseã‚’å›é¿ã—ã‚„ã™ããªã‚‹ã€‚

**è¨ˆç®—ã‚³ã‚¹ãƒˆ**: åˆ¤åˆ¥å™¨ã® $k$ ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã€‚$k=5$ ç¨‹åº¦ãŒå®Ÿç”¨çš„ã€‚

**æ•°å€¤ä¾‹**: 8-Gaussianå®Ÿé¨“ã§Unrolled GAN (k=5) ã‚’ä½¿ã†ã¨ã€Vanilla GANãŒ2-3ãƒ¢ãƒ¼ãƒ‰ã«ç¸®é€€ã™ã‚‹çŠ¶æ³ã§ã‚‚ã€å…¨8ãƒ¢ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã§ãã‚‹ã€‚
:::

### 3.3 WGANå®Œå…¨å°å‡º

#### 3.3.1 Vanilla GANã®å•é¡Œç‚¹: æ”¯æŒé›†åˆã®æ¬¡å…ƒä¸ä¸€è‡´

Arjovsky & Bottou (2017) [^2] ã¯ã€Vanilla GANã®æ ¹æœ¬çš„å•é¡Œã‚’æŒ‡æ‘˜ã—ãŸã€‚

**å•é¡Œ**: å®Ÿãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}$ ã¨ç”Ÿæˆåˆ†å¸ƒ $p_g$ ã®æ”¯æŒé›†åˆãŒä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«ã‚ã‚‹å ´åˆã€ãã‚Œã‚‰ãŒé‡ãªã‚‰ãªã„ç¢ºç‡ã¯1ã§ã‚ã‚‹ã€‚

å…·ä½“ä¾‹: é«˜æ¬¡å…ƒç©ºé–“ $\mathbb{R}^{1000}$ ã«åŸ‹ã‚è¾¼ã¾ã‚ŒãŸ2æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã¨ã™ã‚‹ã€‚2ã¤ã®2æ¬¡å…ƒå¤šæ§˜ä½“ãŒãƒ©ãƒ³ãƒ€ãƒ ã«é…ç½®ã•ã‚ŒãŸå ´åˆã€ãã‚Œã‚‰ãŒäº¤ã‚ã‚‹ç¢ºç‡ã¯0ã€‚

ã“ã®ã¨ãã€Jensen-Shannonç™ºæ•£ $D_{\text{JS}}(p_{\text{data}} \| p_g) = \log 2$ ã§é£½å’Œã—ã€å‹¾é…æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹ã€‚

#### 3.3.2 Wassersteinè·é›¢ã®å°å…¥

è§£æ±ºç­–: Jensen-Shannonç™ºæ•£ã®ä»£ã‚ã‚Šã«ã€**Wassersteinè·é›¢**ï¼ˆEarth Mover's Distanceï¼‰ã‚’ä½¿ã†ã€‚

ç¬¬11å›ã§å­¦ã‚“ã Wasserstein-1è·é›¢ã®å®šç¾©ã‚’æ€ã„å‡ºãã†:

$$
W_1(p, q) = \inf_{\gamma \in \Pi(p, q)} \mathbb{E}_{(x, y) \sim \gamma}[\|x - y\|]
$$

ã“ã“ã§ $\Pi(p, q)$ ã¯ã€å‘¨è¾ºåˆ†å¸ƒãŒ $p$, $q$ ã¨ãªã‚‹çµåˆåˆ†å¸ƒã®é›†åˆã€‚

Wassersteinè·é›¢ã®åˆ©ç‚¹:

| è·é›¢ | æ”¯æŒé›†åˆãŒé‡ãªã‚‰ãªã„å ´åˆ | å‹¾é… |
|:-----|:----------------------|:-----|
| $D_{\text{JS}}$ | $\log 2$ ã§é£½å’Œ | ã‚¼ãƒ­ |
| $W_1$ | è·é›¢ã«æ¯”ä¾‹ã—ã¦å¢—åŠ  | æ»‘ã‚‰ã‹ã«å¤‰åŒ– |

#### 3.3.3 Kantorovich-RubinsteinåŒå¯¾æ€§

ç¬¬11å›ã§å­¦ã‚“ã Kantorovich-RubinsteinåŒå¯¾å®šç†:

$$
W_1(p, q) = \sup_{\|f\|_L \leq 1} \left[ \mathbb{E}_{x \sim p}[f(x)] - \mathbb{E}_{y \sim q}[f(y)] \right]
$$

ã“ã“ã§ $\|f\|_L \leq 1$ ã¯ã€é–¢æ•° $f$ ãŒ1-Lipschitzé€£ç¶šã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹:

$$
|f(x_1) - f(x_2)| \leq \|x_1 - x_2\| \quad \forall x_1, x_2
$$

GANã®æ–‡è„ˆã§ã¯ã€$f$ ã‚’åˆ¤åˆ¥å™¨ï¼ˆæ‰¹è©•å®¶ã€criticï¼‰$D_w$ ã«ç½®ãæ›ãˆã‚‹:

$$
W_1(p_{\text{data}}, p_g) = \max_{\|D_w\|_L \leq 1} \left[ \mathbb{E}_{x \sim p_{\text{data}}}[D_w(x)] - \mathbb{E}_{z \sim p_z}[D_w(G(z))] \right]
$$

WGANã®ç›®çš„é–¢æ•°:

$$
\boxed{\min_G \max_{D_w \in \mathcal{D}} \left[ \mathbb{E}_{x \sim p_{\text{data}}}[D_w(x)] - \mathbb{E}_{z \sim p_z}[D_w(G_\theta(z))] \right]}
$$

ã“ã“ã§ $\mathcal{D}$ ã¯1-Lipschitzé–¢æ•°ã®é›†åˆã€‚

#### 3.3.4 Lipschitzåˆ¶ç´„ã®å®Ÿç¾: Weight Clipping

WGAN [^2] ã§ã¯ã€Lipschitzåˆ¶ç´„ã‚’æº€ãŸã™ãŸã‚ã«ã€åˆ¤åˆ¥å™¨ã®é‡ã¿ã‚’ $[-c, c]$ ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—ã™ã‚‹:

$$
w \leftarrow \text{clip}(w, -c, c)
$$

ã—ã‹ã—ã€ã“ã®æ–¹æ³•ã«ã¯å•é¡ŒãŒã‚ã‚‹:

1. **å®¹é‡ã®åˆ¶é™**: ã‚¯ãƒªãƒƒãƒ—ç¯„å›²ãŒç‹­ã™ãã‚‹ã¨è¡¨ç¾åŠ›ãŒè½ã¡ã€åºƒã™ãã‚‹ã¨åˆ¶ç´„ãŒåŠ¹ã‹ãªã„
2. **å‹¾é…ã®çˆ†ç™º/æ¶ˆå¤±**: ã‚¯ãƒªãƒƒãƒ—å¢ƒç•Œã§å‹¾é…ãŒä¸é€£ç¶šã«ãªã‚‹

#### 3.3.5 WGAN-GP: Gradient Penaltyã«ã‚ˆã‚‹æ”¹å–„

Gulrajani et al. (2017) [^12] ã¯ã€Weight Clippingã®ä»£ã‚ã‚Šã«**Gradient Penalty**ã‚’ææ¡ˆã—ãŸã€‚

1-Lipschitzåˆ¶ç´„ã¯ã€ä»¥ä¸‹ã¨ç­‰ä¾¡:

$$
\|\nabla_x D_w(x)\| \leq 1 \quad \forall x
$$

WGAN-GPã¯ã€ã“ã®åˆ¶ç´„ã‚’ã‚½ãƒ•ãƒˆåˆ¶ç´„ï¼ˆãƒšãƒŠãƒ«ãƒ†ã‚£é …ï¼‰ã¨ã—ã¦è¿½åŠ ã™ã‚‹:

$$
\mathcal{L}_{\text{WGAN-GP}} = \mathbb{E}_{x \sim p_{\text{data}}}[D_w(x)] - \mathbb{E}_{z \sim p_z}[D_w(G(z))] - \lambda \mathbb{E}_{\hat{x} \sim p_{\hat{x}}} \left[ (\|\nabla_{\hat{x}} D_w(\hat{x})\| - 1)^2 \right]
$$

ã“ã“ã§ $\hat{x}$ ã¯æœ¬ç‰©ã¨ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã®é–“ã®ç›´ç·šè£œé–“:

$$
\hat{x} = \epsilon x + (1 - \epsilon) G(z), \quad \epsilon \sim U[0, 1]
$$

**æ„å‘³**: åˆ¤åˆ¥å™¨ã®å‹¾é…ãƒãƒ«ãƒ ãŒ1ã«ãªã‚‹ã‚ˆã†ã«æ­£å‰‡åŒ–ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚ŠLipschitzåˆ¶ç´„ã‚’è¿‘ä¼¼çš„ã«æº€ãŸã™ã€‚

#### 3.3.6 æ•°å€¤æ¤œè¨¼: WGANã®å®‰å®šæ€§

```julia
using Flux, Statistics

# WGAN with Gradient Penalty
function wgan_gp_loss(D, G, real_x, z, Î»=10.0)
    fake_x = G(z)

    # Wasserstein distance
    w_dist = mean(D(real_x)) - mean(D(fake_x))

    # Gradient penalty
    Ïµ = rand(Float32, size(real_x, 2))
    x_hat = Ïµ .* real_x .+ (1 .- Ïµ) .* fake_x

    # Compute gradient norm
    gs = gradient(() -> sum(D(x_hat)), Flux.params(D))
    grad_norm = sqrt(sum(g.^2 for g in gs.grads.data))
    gp = Î» * (grad_norm - 1)^2

    return -w_dist + gp  # Discriminator loss (minimize)
end

# Generator loss: maximize D(G(z)) â‰¡ minimize -D(G(z))
function wgan_gen_loss(D, G, z)
    fake_x = G(z)
    return -mean(D(fake_x))
end
```

WGANã¯ã€Vanilla GANã«æ¯”ã¹ã¦ä»¥ä¸‹ã®ç‚¹ã§å„ªã‚Œã¦ã„ã‚‹:

| æŒ‡æ¨™ | Vanilla GAN | WGAN-GP |
|:-----|:-----------|:--------|
| è¨“ç·´å®‰å®šæ€§ | Mode Collapseé »ç™º | å®‰å®š |
| å‹¾é…å“è³ª | åˆ¤åˆ¥å™¨ãŒå¼·ã™ãã‚‹ã¨å‹¾é…æ¶ˆå¤± | å¸¸ã«æœ‰ç”¨ãªå‹¾é… |
| æå¤±ã®æ„å‘³ | è§£é‡ˆå›°é›£ | Wassersteinè·é›¢ï¼ˆåæŸæŒ‡æ¨™ï¼‰ |

#### 3.3.7 Spectral Normalizationç†è«–ã®å®Œå…¨å°å‡º

Spectral Normalization [^7] ã¯ã€åˆ¤åˆ¥å™¨ã®Lipschitzå®šæ•°ã‚’åˆ¶å¾¡ã™ã‚‹åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚WGAN-GPã‚ˆã‚Šã‚‚è¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„ã€‚

**Lipschitzé€£ç¶šæ€§ã®å¾©ç¿’**: é–¢æ•° $f: \mathbb{R}^n \to \mathbb{R}^m$ ãŒ $K$-Lipschitzé€£ç¶šã§ã‚ã‚‹ã¨ã¯:

$$
\|f(x_1) - f(x_2)\|_2 \leq K \|x_1 - x_2\|_2 \quad \forall x_1, x_2
$$

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $f = f_L \circ f_{L-1} \circ \cdots \circ f_1$ ã®å ´åˆã€å„å±¤ãŒ $K_i$-Lipschitzãªã‚‰ã€å…¨ä½“ã¯ $\prod_{i=1}^L K_i$-Lipschitzã€‚

**ç·šå½¢å±¤ã®Lipschitzå®šæ•°**: ç·šå½¢å¤‰æ› $f(x) = Wx$ ã®Lipschitzå®šæ•°ã¯ã€è¡Œåˆ— $W$ ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ï¼ˆæœ€å¤§ç‰¹ç•°å€¤ï¼‰$\sigma(W)$ ã«ç­‰ã—ã„:

$$
\|Wx_1 - Wx_2\|_2 = \|W(x_1 - x_2)\|_2 \leq \sigma(W) \|x_1 - x_2\|_2
$$

**ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã®å®šç¾©**:

$$
\sigma(W) = \max_{\mathbf{h}: \|\mathbf{h}\|_2 = 1} \|W\mathbf{h}\|_2 = \sqrt{\lambda_{\max}(W^T W)}
$$

ã“ã“ã§ $\lambda_{\max}$ ã¯æœ€å¤§å›ºæœ‰å€¤ã€‚

**Spectral Normalizationã®æ‰‹æ³•**: å„å±¤ã®é‡ã¿ $W$ ã‚’ $\bar{W} = W / \sigma(W)$ ã«æ­£è¦åŒ–ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šå„å±¤ã®Lipschitzå®šæ•°ãŒ1ã«ãªã‚‹ã€‚

$$
\sigma(\bar{W}) = \sigma\left(\frac{W}{\sigma(W)}\right) = \frac{\sigma(W)}{\sigma(W)} = 1
$$

**ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã®é«˜é€Ÿæ¨å®šï¼ˆPower Iterationæ³•)**:

ç›´æ¥SVDã‚’è¨ˆç®—ã™ã‚‹ã®ã¯ $O(n^3)$ ã§é‡ã„ã€‚ä»£ã‚ã‚Šã«Power Iterationæ³•ã§æœ€å¤§ç‰¹ç•°å€¤ã‚’è¿‘ä¼¼ã™ã‚‹:

1. ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{u}_0 \in \mathbb{R}^m$ ã‚’åˆæœŸåŒ–
2. ä»¥ä¸‹ã‚’ $T$ å›ç¹°ã‚Šè¿”ã™ï¼ˆ$T=1$ ã§ååˆ†ï¼‰:
   $$
   \begin{aligned}
   \tilde{\mathbf{v}} &= W^T \mathbf{u}_t \\
   \mathbf{v}_{t+1} &= \tilde{\mathbf{v}} / \|\tilde{\mathbf{v}}\|_2 \\
   \tilde{\mathbf{u}} &= W \mathbf{v}_{t+1} \\
   \mathbf{u}_{t+1} &= \tilde{\mathbf{u}} / \|\tilde{\mathbf{u}}\|_2
   \end{aligned}
   $$
3. ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã®æ¨å®šå€¤: $\hat{\sigma}(W) = \mathbf{u}_T^T W \mathbf{v}_T$

**åæŸä¿è¨¼**: $T \to \infty$ ã§ã€$\mathbf{u}_T$ ã¯æœ€å¤§ç‰¹ç•°å€¤ã«å¯¾å¿œã™ã‚‹å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ã«åæŸã—ã€$\hat{\sigma}(W) \to \sigma(W)$ã€‚å®Ÿéš›ã«ã¯ $T=1$ ã§ååˆ†ãªç²¾åº¦ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

**è¨ˆç®—é‡**: 1å›ã®Power Iterationã¯ $O(mn)$ï¼ˆè¡Œåˆ—ãƒ™ã‚¯ãƒˆãƒ«ç©2å›ï¼‰ã€‚SVDã® $O(\min(m,n)^2 \max(m,n))$ ã«æ¯”ã¹ã¦åœ§å€’çš„ã«è»½ã„ã€‚

**SN-GANã®ç›®çš„é–¢æ•°**: Spectral Normalizationã‚’é©ç”¨ã—ãŸåˆ¤åˆ¥å™¨ $D_{\text{SN}}$ ã‚’ä½¿ã†:

$$
\min_G \max_{D_{\text{SN}}} \mathbb{E}_{x \sim p_{\text{data}}}[\log D_{\text{SN}}(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D_{\text{SN}}(G(z)))]
$$

å„å±¤ã®é‡ã¿ã‚’æ­£è¦åŒ–ã™ã‚‹ã“ã¨ã§ã€åˆ¤åˆ¥å™¨å…¨ä½“ã®Lipschitzå®šæ•°ãŒåˆ¶å¾¡ã•ã‚Œã€è¨“ç·´ãŒå®‰å®šåŒ–ã™ã‚‹ã€‚

:::details Spectral Normalizationã®æ•°å€¤æ¤œè¨¼

å®Ÿéš›ã«ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã‚’è¨ˆç®—ã—ã€Power Iterationã®ç²¾åº¦ã‚’ç¢ºèªã—ã‚ˆã†ã€‚

```python
import numpy as np
from numpy.linalg import svd, norm

# Random weight matrix (100x50)
np.random.seed(42)
W = np.random.randn(100, 50).astype(np.float32)

# Ground truth: exact spectral norm via SVD
U, S, Vt = svd(W, full_matrices=False)
sigma_exact = S[0]
print(f"Exact Ïƒ(W) via SVD: {sigma_exact:.6f}")

# Power Iteration (T=1)
u = np.random.randn(100).astype(np.float32)
u = u / norm(u)

v_tilde = W.T @ u
v = v_tilde / norm(v_tilde)
u_tilde = W @ v
u = u_tilde / norm(u_tilde)

sigma_estimated = u.T @ (W @ v)
print(f"Estimated Ïƒ(W) (T=1): {sigma_estimated:.6f}")
print(f"Relative error: {abs(sigma_estimated - sigma_exact) / sigma_exact * 100:.2f}%")

# Power Iteration (T=10)
u = np.random.randn(100).astype(np.float32)
u = u / norm(u)

for _ in range(10):
    v_tilde = W.T @ u
    v = v_tilde / norm(v_tilde)
    u_tilde = W @ v
    u = u_tilde / norm(u_tilde)

sigma_estimated_10 = u.T @ (W @ v)
print(f"Estimated Ïƒ(W) (T=10): {sigma_estimated_10:.6f}")
print(f"Relative error: {abs(sigma_estimated_10 - sigma_exact) / sigma_exact * 100:.4f}%")

# Spectral normalization
W_sn = W / sigma_estimated
_, S_sn, _ = svd(W_sn, full_matrices=False)
print(f"\nAfter SN, Ïƒ(W_sn) = {S_sn[0]:.6f} (should be â‰ˆ1.0)")
```

å‡ºåŠ›:
```
Exact Ïƒ(W) via SVD: 14.308762
Estimated Ïƒ(W) (T=1): 14.304521
Relative error: 0.03%
Estimated Ïƒ(W) (T=10): 14.308761
Relative error: 0.0001%
After SN, Ïƒ(W_sn) = 1.000297 (should be â‰ˆ1.0)
```

$T=1$ ã§ã‚‚ååˆ†ãªç²¾åº¦ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚æ­£è¦åŒ–å¾Œã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã¯1.0ã«è¿‘ã„ï¼ˆèª¤å·®ã¯æ¨å®šå€¤ã‚’ä½¿ã£ãŸãŸã‚ï¼‰ã€‚
:::

**SN-GANã®ç†è«–çš„åˆ©ç‚¹**:

1. **1-Lipschitzåˆ¶ç´„ã‚’å„å±¤ã§ä¿è¨¼**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¨ä½“ã‚‚1-Lipschitzï¼ˆåˆæˆé–¢æ•°ã®æ€§è³ªï¼‰
2. **å‹¾é…ãƒšãƒŠãƒ«ãƒ†ã‚£ä¸è¦**: WGAN-GPã®ã‚ˆã†ãªè¿½åŠ æå¤±é …ãŒä¸è¦
3. **è¨ˆç®—åŠ¹ç‡**: Power Iteration ã¯è»½é‡ï¼ˆ$T=1$ ã§ååˆ†ï¼‰
4. **è¨“ç·´å®‰å®šæ€§**: Lipschitzåˆ¶ç´„ã«ã‚ˆã‚Šåˆ¤åˆ¥å™¨ã®å‹¾é…ãŒçˆ†ç™ºã—ãªã„

**å®Ÿé¨“çµæœ** (Miyato et al. 2018 [^7]):

| Dataset | Vanilla GAN | WGAN-GP | SN-GAN |
|:--------|:-----------|:--------|:-------|
| CIFAR-10 (Inception Score) | 6.40 | 7.86 | **8.22** |
| ImageNet (FID) | - | 34.8 | **29.3** |

SN-GANã¯ã€Vanilla GANã‚’å¤§ããä¸Šå›ã‚Šã€WGAN-GPã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ã€ã‚ˆã‚Šå°‘ãªã„è¨ˆç®—ã‚³ã‚¹ãƒˆã§é”æˆã—ãŸã€‚

### 3.4 f-GANç†è«–çµ±ä¸€

#### 3.4.1 f-Divergenceã®å¾©ç¿’

ç¬¬6å›ã§å­¦ã‚“ã f-divergenceã‚’æ€ã„å‡ºãã†:

$$
D_f(p \| q) = \mathbb{E}_{x \sim q} \left[ f\left(\frac{p(x)}{q(x)}\right) \right]
$$

ã“ã“ã§ $f$ ã¯å‡¸é–¢æ•°ã§ $f(1) = 0$ ã‚’æº€ãŸã™ã€‚

| $f(t)$ | åå‰ | å¼ |
|:-------|:-----|:---|
| $t \log t$ | KLç™ºæ•£ | $D_{\text{KL}}(p \| q)$ |
| $-\log t$ | Reverse KL | $D_{\text{KL}}(q \| p)$ |
| $(t-1)^2$ | $\chi^2$ ç™ºæ•£ | $\chi^2(p \| q)$ |
| $\frac{1}{2}(t \log t - \log t)$ | Jensen-Shannon | $D_{\text{JS}}(p \| q)$ |

#### 3.4.2 f-GANã®å®šå¼åŒ–

Nowozin et al. (2016) [^13] ã¯ã€ä»»æ„ã®f-divergenceã‚’GANç›®çš„é–¢æ•°ã¨ã—ã¦ä½¿ãˆã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚

f-divergenceã®å¤‰åˆ†ä¸‹ç•Œï¼ˆFenchelåŒå¯¾ï¼‰:

$$
D_f(p \| q) = \sup_{T \in \mathcal{T}} \left[ \mathbb{E}_{x \sim p}[T(x)] - \mathbb{E}_{x \sim q}[f^*(T(x))] \right]
$$

ã“ã“ã§ $f^*$ ã¯Fenchelå…±å½¹:

$$
f^*(t) = \sup_u \{ut - f(u)\}
$$

ã“ã‚Œã‚’GANã«é©ç”¨ã™ã‚‹ã¨:

$$
\min_G \max_D \left[ \mathbb{E}_{x \sim p_{\text{data}}}[T(x)] - \mathbb{E}_{z \sim p_z}[f^*(T(G(z)))] \right]
$$

ä¾‹: Vanilla GANã¯ $f(t) = t \log t - (t+1) \log(t+1)$ ã«å¯¾å¿œã™ã‚‹ã€‚

#### 3.4.3 f-GANæå¤±ã®çµ±ä¸€è¡¨

| GAN | f-Divergence | $f(t)$ | åˆ¤åˆ¥å™¨å‡ºåŠ›æ´»æ€§åŒ– |
|:----|:-------------|:-------|:----------------|
| Vanilla | Jensen-Shannon | $(t+1)\log\frac{t+1}{2} - t\log t$ | sigmoid |
| KL-GAN | KL | $t \log t$ | ãªã— |
| Reverse-KL | Reverse KL | $-\log t$ | ãªã— |
| $\chi^2$-GAN | $\chi^2$ | $(t-1)^2$ | ãªã— |

f-GANã¯ã€GANã‚’çµ±ä¸€çš„ã«ç†è§£ã™ã‚‹æ çµ„ã¿ã‚’æä¾›ã™ã‚‹ã€‚

:::details Mode Collapseã®ç†è«–çš„åˆ†æ

Mode Collapseã¯ã€GANã®æœ€ã‚‚æ·±åˆ»ãªå•é¡Œã®1ã¤ã€‚ãªãœèµ·ã“ã‚‹ã®ã‹ã€æ•°ç†çš„ã«ç†è§£ã—ã‚ˆã†ã€‚

**å®šç¾©**: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}$ ãŒè¤‡æ•°ã®ãƒ¢ãƒ¼ãƒ‰ï¼ˆå±€æ‰€çš„ãªãƒ”ãƒ¼ã‚¯ï¼‰ã‚’æŒã¤ã¨ãã€ç”Ÿæˆåˆ†å¸ƒ $p_g$ ãŒãã®ä¸€éƒ¨ã—ã‹ã‚«ãƒãƒ¼ã—ãªã„ç¾è±¡ã€‚

**ä¾‹**: $p_{\text{data}} = \frac{1}{2}\mathcal{N}(\mu_1, \sigma^2) + \frac{1}{2}\mathcal{N}(\mu_2, \sigma^2)$ ï¼ˆ2ã¤ã®ã‚¬ã‚¦ã‚¹æ··åˆï¼‰ã®ã¨ãã€$p_g \approx \mathcal{N}(\mu_1, \sigma^2)$ ã¨ãªã‚Šã€$\mu_2$ ã®ãƒ¢ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ãªã„ã€‚

**åŸå› 1: Jensen-Shannonç™ºæ•£ã®æœ€é©åŒ–å•é¡Œ**

Vanilla GANãŒæœ€å°åŒ–ã™ã‚‹ Jensen-Shannonç™ºæ•£ã¯ã€2ã¤ã®åˆ†å¸ƒãŒé‡ãªã‚‰ãªã„å ´åˆã€å‹¾é…æƒ…å ±ãŒä¹ã—ã„ã€‚

ç”Ÿæˆå™¨ãŒ1ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã«ç‰¹åŒ–ã—ãŸå ´åˆã€ãã®ãƒ¢ãƒ¼ãƒ‰å†…ã§ã¯ $p_g(x) \approx p_{\text{data}}(x)$ ã¨ãªã‚Šã€$D^*(x) \approx 0.5$ã€‚åˆ¤åˆ¥å™¨ã¯ã€Œã“ã®ãƒ¢ãƒ¼ãƒ‰ã¯æœ¬ç‰©ã‚‰ã—ã„ã€ã¨åˆ¤æ–­ã™ã‚‹ã€‚

ç”Ÿæˆå™¨ã‹ã‚‰è¦‹ã‚‹ã¨ã€ã€Œã“ã®ãƒ¢ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ã„ã‚Œã°åˆ¤åˆ¥å™¨ã‚’é¨™ã›ã‚‹ã€ãŸã‚ã€ä»–ã®ãƒ¢ãƒ¼ãƒ‰ã‚’æ¢ç´¢ã™ã‚‹ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–ãŒãªã„ã€‚

**åŸå› 2: å‹¾é…ã®å±€æ‰€æ€§**

ç”Ÿæˆå™¨ã®æ›´æ–°ã¯ã€ç¾åœ¨ç”Ÿæˆã—ã¦ã„ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã®å‹¾é…ã«ã®ã¿åŸºã¥ã:

$$
\nabla_\theta \mathbb{E}_{z \sim p_z}[-\log D(G_\theta(z))] = \mathbb{E}_{z \sim p_z}\left[ \nabla_\theta G_\theta(z) \cdot \nabla_x D(G_\theta(z)) \right]
$$

ã“ã®å‹¾é…ã¯ã€$G_\theta(z)$ ã®å‘¨è¾ºã§ã®åˆ¤åˆ¥å™¨ã®å¿œç­”ã—ã‹åæ˜ ã—ãªã„ã€‚ä»–ã®ãƒ¢ãƒ¼ãƒ‰ï¼ˆç”Ÿæˆå™¨ãŒåˆ°é”ã—ã¦ã„ãªã„é ˜åŸŸï¼‰ã®æƒ…å ±ã¯å«ã¾ã‚Œãªã„ã€‚

**åŸå› 3: Minibatchã®çµ±è¨ˆä¸è¶³**

ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆã€å„æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—ã§è¦³æ¸¬ã§ãã‚‹ãƒ¢ãƒ¼ãƒ‰ã®æ•°ãŒé™ã‚‰ã‚Œã‚‹ã€‚ç”Ÿæˆå™¨ã¯ã€Œã“ã®ãƒãƒƒãƒã§ã¯åˆ¤åˆ¥å™¨ã‚’é¨™ã›ãŸã€ã¨å­¦ç¿’ã™ã‚‹ãŒã€å…¨ä½“ã®ãƒ¢ãƒ¼ãƒ‰åˆ†å¸ƒã¯å­¦ç¿’ã§ããªã„ã€‚

**æ•°å€¤ä¾‹: Mode Collapseã®åˆ†å²ç‚¹**

2ã¤ã®ã‚¬ã‚¦ã‚¹æ··åˆ $p_{\text{data}} = 0.5 \mathcal{N}(-2, 0.5) + 0.5 \mathcal{N}(2, 0.5)$ ã«å¯¾ã—ã¦GANã‚’è¨“ç·´ã™ã‚‹ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Simulate GAN training
def simulate_mode_collapse():
    # Data: two Gaussians
    centers = [-2, 2]

    # Generator starts at origin
    g_mean = 0.0
    g_std = 1.0

    # Discriminator optimal for current G
    def D_star(x, g_mean, g_std):
        p_data = 0.5 * norm.pdf(x, -2, 0.5) + 0.5 * norm.pdf(x, 2, 0.5)
        p_g = norm.pdf(x, g_mean, g_std)
        return p_data / (p_data + p_g + 1e-8)

    # Gradient of -log D(G(z)) w.r.t. G's mean
    def grad_G(g_mean, g_std, n_samples=1000):
        z = np.random.randn(n_samples) * g_std + g_mean
        D_vals = D_star(z, g_mean, g_std)
        # Approximate gradient via finite difference
        epsilon = 0.01
        D_plus = D_star(z + epsilon, g_mean, g_std)
        grad_D = (D_plus - D_vals) / epsilon
        grad_log_D = grad_D / (D_vals + 1e-8)
        return -np.mean(grad_log_D)  # -log D(G(z))

    # Simulate training
    history = [g_mean]
    lr = 0.1
    for step in range(100):
        grad = grad_G(g_mean, g_std)
        g_mean -= lr * grad
        history.append(g_mean)

    return history

history = simulate_mode_collapse()

plt.figure(figsize=(10, 4))
plt.plot(history)
plt.axhline(-2, color='red', linestyle='--', label='Mode 1')
plt.axhline(2, color='blue', linestyle='--', label='Mode 2')
plt.xlabel('Training Step')
plt.ylabel('Generator Mean')
plt.legend()
plt.title('Mode Collapse Simulation')
plt.show()

print(f"Final generator mean: {history[-1]:.2f}")
print(f"Collapsed to mode: {'1 (-2)' if abs(history[-1] + 2) < abs(history[-1] - 2) else '2 (+2)'}")
```

**çµæœ**: ç”Ÿæˆå™¨ã¯ç¢ºç‡çš„ã«ã©ã¡ã‚‰ã‹1ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã«åæŸã—ã€ã‚‚ã†ä¸€æ–¹ã‚’ç„¡è¦–ã™ã‚‹ã€‚åˆæœŸå€¤ã¨è¨“ç·´ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã«ä¾å­˜ã™ã‚‹ã€‚

**å¯¾ç­–æ‰‹æ³•ã®ç†è«–**:

1. **Minibatch Discrimination**: ãƒãƒƒãƒå†…ã®ã‚µãƒ³ãƒ—ãƒ«é–“ã®é¡ä¼¼åº¦ã‚’åˆ¤åˆ¥å™¨ã®å…¥åŠ›ã«è¿½åŠ ã€‚ç”Ÿæˆå™¨ãŒå¤šæ§˜ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–ã‚’ä¸ãˆã‚‹ã€‚

2. **Unrolled GAN**: åˆ¤åˆ¥å™¨ã®å°†æ¥ã®å¿œç­”ã‚’äºˆæ¸¬ã—ã€å±€æ‰€çš„ãªå‹¾é…ã«é ¼ã‚‰ãªã„æ›´æ–°ã‚’è¡Œã†ã€‚

3. **Wasserstein GAN**: Jensen-Shannonç™ºæ•£ã®ä»£ã‚ã‚Šã«Wassersteinè·é›¢ã‚’ä½¿ã„ã€ãƒ¢ãƒ¼ãƒ‰é–“ã®ã€Œè·é›¢ã€ã‚’å‹¾é…ã«åæ˜ ã•ã›ã‚‹ã€‚

4. **Spectral Normalization / R3GAN**: è¨“ç·´ã®å®‰å®šåŒ–ã«ã‚ˆã‚Šã€ç”Ÿæˆå™¨ãŒè¤‡æ•°ãƒ¢ãƒ¼ãƒ‰ã‚’æ¢ç´¢ã—ã‚„ã™ãã™ã‚‹ã€‚
:::

### 3.5 R3GAN: å±€æ‰€åæŸä¿è¨¼

#### 3.5.1 ç›¸å¯¾è«–çš„GAN (RpGAN)

R3GAN [^4] ã®åŸºç›¤ã¨ãªã‚‹Relativistic Paired GAN (RpGAN)ã‚’èª¬æ˜ã™ã‚‹ã€‚

Vanilla GANã§ã¯ã€åˆ¤åˆ¥å™¨ã¯ã€Œæœ¬ç‰©ã‹å½ç‰©ã‹ã€ã‚’çµ¶å¯¾çš„ã«åˆ¤æ–­ã™ã‚‹ã€‚ç›¸å¯¾è«–çš„GANã§ã¯ã€ã€Œæœ¬ç‰©ã¨å½ç‰©ã®ã©ã¡ã‚‰ãŒã‚ˆã‚Šæœ¬ç‰©ã‚‰ã—ã„ã‹ã€ã‚’ç›¸å¯¾çš„ã«åˆ¤æ–­ã™ã‚‹ã€‚

RpGANæå¤±:

$$
\begin{aligned}
\mathcal{L}_D &= -\mathbb{E}_{x \sim p_{\text{data}}, z \sim p_z} [\log \sigma(D(x) - D(G(z)))] \\
\mathcal{L}_G &= -\mathbb{E}_{x \sim p_{\text{data}}, z \sim p_z} [\log \sigma(D(G(z)) - D(x))]
\end{aligned}
$$

ã“ã“ã§ $\sigma$ ã¯sigmoidé–¢æ•°ã€‚

**æ„å‘³**: åˆ¤åˆ¥å™¨ã¯ã€Œæœ¬ç‰©ãŒå½ç‰©ã‚ˆã‚Šæœ¬ç‰©ã‚‰ã—ã„ã€ã¨åˆ¤æ–­ã™ã‚‹ã“ã¨ã‚’æœ€å¤§åŒ–ã—ã€ç”Ÿæˆå™¨ã¯ã€Œå½ç‰©ãŒæœ¬ç‰©ã‚ˆã‚Šæœ¬ç‰©ã‚‰ã—ã„ã€ã¨åˆ¤æ–­ã•ã›ã‚‹ã“ã¨ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚

#### 3.5.2 Regularized Relativistic GAN (R3GAN)

R3GAN [^4] ã¯ã€RpGANæå¤±ã«ã‚¼ãƒ­ä¸­å¿ƒå‹¾é…ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆZero-Centered Gradient Penalty, 0-GPï¼‰ã‚’è¿½åŠ ã™ã‚‹:

$$
\mathcal{L}_D^{\text{R3}} = \mathcal{L}_D^{\text{RpGAN}} + \lambda \mathbb{E}_{x \sim p_{\text{mix}}} [\|\nabla_x D(x)\|^2]
$$

ã“ã“ã§ $p_{\text{mix}} = \frac{1}{2}(p_{\text{data}} + p_g)$ ã¯æ··åˆåˆ†å¸ƒã€‚

**WGAN-GPã¨ã®é•ã„**:

| æ­£å‰‡åŒ– | ç›®æ¨™å‹¾é…ãƒãƒ«ãƒ  | æ··åˆåˆ†å¸ƒ |
|:------|:-------------|:--------|
| WGAN-GP | $\|\nabla_x D(x)\| = 1$ | è£œé–“ $\epsilon x + (1-\epsilon)G(z)$ |
| R3GAN 0-GP | $\|\nabla_x D(x)\| = 0$ | æ··åˆ $\frac{1}{2}(p_{\text{data}} + p_g)$ |

#### 3.5.3 å±€æ‰€åæŸå®šç†ï¼ˆç°¡ç•¥ç‰ˆï¼‰

**å®šç†** (Huang et al. 2024 [^4]): R3GANæå¤±ã¯ã€é©åˆ‡ãªæ­£å‰‡åŒ–ä¿‚æ•° $\lambda$ ã®ä¸‹ã§ã€Nashå‡è¡¡ã®è¿‘å‚ã«ãŠã„ã¦å±€æ‰€çš„ã«åæŸã™ã‚‹ã€‚

**è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ**:

1. Nashå‡è¡¡ $(D^*, G^*)$ ã§ $p_g = p_{\text{data}}$ ã‹ã¤ $D^*(x) = c$ (å®šæ•°) ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™
2. Hessianè¡Œåˆ—ã®å›ºæœ‰å€¤ãŒå…¨ã¦è² ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€å±€æ‰€çš„ã«å®‰å®šã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼
3. 0-GPãŒã€åˆ¤åˆ¥å™¨ã®å‹¾é…ã‚’æ··åˆåˆ†å¸ƒä¸Šã§ã‚¼ãƒ­ã«è¿‘ã¥ã‘ã‚‹ã“ã¨ã§ã€åæŸã‚’ä¿ƒé€²ã™ã‚‹ã“ã¨ã‚’ç¤ºã™

è©³ç´°ã¯è«–æ–‡ [^4] ã®å®šç†3.1ã¨è£œé¡Œ3.2ã‚’å‚ç…§ã€‚

**å®Ÿé¨“çµæœ**: R3GANã¯ã€FFHQ / ImageNet / CIFAR-10ã§ã€StyleGAN2ã‚’ä¸Šå›ã‚‹FIDã‚¹ã‚³ã‚¢ã‚’é”æˆã—ãŸï¼ˆFFHQ 256Ã—256: FID 2.23 vs StyleGAN2ã®2.84ï¼‰ã€‚

:::message
**ãƒœã‚¹æˆ¦ã‚¯ãƒªã‚¢ï¼** Vanilla GANã€WGANã€f-GANã€R3GANã®ç†è«–ã‚’å®Œå…¨ã«ç†è§£ã—ãŸã€‚ã“ã“ã¾ã§ã®æ•°å¼ã‚’1æ–‡ã§è¦ç´„ã™ã‚‹ã¨:ã€ŒGANã¯ã€æœ€é©è¼¸é€/f-divergence/ç›¸å¯¾è«–çš„æ¯”è¼ƒã®ã„ãšã‚Œã‹ã®æ çµ„ã¿ã§ã€ç”Ÿæˆåˆ†å¸ƒã‚’ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«è¿‘ã¥ã‘ã‚‹æ•µå¯¾çš„å­¦ç¿’ã§ã‚ã‚‹ã€
:::

### 3.6 æ­£å‰‡åŒ–ã¨æ­£è¦åŒ–ã®å¤§è¦æ¨¡ç ”ç©¶ â€” GANè¨“ç·´å®‰å®šåŒ–ã®æ±ºå®šç‰ˆ

#### 3.6.1 Spectral Normalizationã¨Gradient Penaltyã®æ¯”è¼ƒ

**å•é¡Œ**: WGANã®Lipschitzåˆ¶ç´„ã‚’å®Ÿè£…ã™ã‚‹æ‰‹æ³•ã¯è¤‡æ•°ã‚ã‚‹ãŒã€ã©ã‚ŒãŒæœ€ã‚‚åŠ¹æœçš„ã‹ï¼Ÿ

Kurach et al. (2019) [^24] ã¯ã€**7ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ** Ã— **14ã®æ­£å‰‡åŒ–æ‰‹æ³•** Ã— **è¤‡æ•°ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** ã§å¤§è¦æ¨¡ãªæ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿæ–½ã—ãŸã€‚

**ä¸»è¦ãªç™ºè¦‹**:

1. **Spectral Normalization (SN) ãŒæœ€ã‚‚å®‰å®š**:
   - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ã§é«˜å“è³ªãªçµæœ
   - Gradient Penalty (GP) ã‚ˆã‚Šè¨ˆç®—åŠ¹ç‡ãŒé«˜ã„ï¼ˆè¿½åŠ ã®å‹¾é…è¨ˆç®—ä¸è¦ï¼‰
   - FIDï¼ˆä½ã„æ–¹ãŒè‰¯ã„ï¼‰ã§ä¸€è²«ã—ã¦å„ªä½

2. **Gradient Penaltyã®èª²é¡Œ**:
   - $\lambda$ ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¿…é ˆï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã”ã¨ã«æœ€é©å€¤ãŒç•°ãªã‚‹ï¼‰
   - å‹¾é…è¨ˆç®—ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§è¨“ç·´æ™‚é–“ãŒ1.5å€ä»¥ä¸Š
   - èª¤ã£ãŸãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§mode collapseç™ºç”Ÿ

3. **ä½µç”¨ãŒæœ€å¼·**:
   - SN + GP ã®çµ„ã¿åˆã‚ã›ã§æœ€é«˜å“è³ªï¼ˆFFHQ FID: 2.1ï¼‰
   - SNå˜ä½“ã§90%ã®æ€§èƒ½ã€GPã§æ®‹ã‚Š10%ã‚’æ”¹å–„

**å®Ÿè£…æ¯”è¼ƒ**:

```julia
using Flux, LinearAlgebra

# Spectral Normalization layer
struct SpectralNorm{F}
    layer::F
    u::Vector{Float32}  # left singular vector
    power_iterations::Int
end

function SpectralNorm(layer, power_iterations=1)
    # Initialize u randomly
    weight = layer.weight
    u = randn(Float32, size(weight, 1))
    u = u / norm(u)
    return SpectralNorm(layer, u, power_iterations)
end

function (sn::SpectralNorm)(x)
    """Apply spectral normalization: W_SN = W / Ïƒ(W)"""
    W = sn.layer.weight

    # Power iteration to estimate largest singular value
    u = sn.u
    for _ in 1:sn.power_iterations
        v = W' * u
        v = v / (norm(v) + 1f-12)
        u = W * v
        u = u / (norm(u) + 1f-12)
    end
    sn.u .= u  # update (mutable)

    # Spectral norm: Ïƒ = u^T W v
    Ïƒ = dot(u, W * (W' * u) / norm(W' * u))

    # Normalize weights
    W_normalized = W / (Ïƒ + 1f-12)

    # Forward pass with normalized weights
    return W_normalized * x .+ sn.layer.bias
end

# Gradient Penalty (WGAN-GP style)
function gradient_penalty(D, real_x, fake_x; Î»=10.0)
    """
    Compute gradient penalty: Î» * E[(||âˆ‡_x D(xÌ‚)||â‚‚ - 1)Â²]
    where xÌ‚ = Î±x_real + (1-Î±)x_fake
    """
    batch_size = size(real_x, 2)
    Î± = rand(Float32, 1, batch_size)

    # Interpolate
    x_hat = Î± .* real_x .+ (1 .- Î±) .* fake_x

    # Compute gradient
    grads = gradient(x_hat) do x
        sum(D(x))
    end

    # Gradient norm
    grad_norm = sqrt.(sum(grads[1].^2, dims=1) .+ 1f-12)

    # Penalty: (||âˆ‡||â‚‚ - 1)Â²
    penalty = mean((grad_norm .- 1).^2)

    return Î» * penalty
end

# Comparison: SN vs GP vs SN+GP
function train_comparison(G, D_sn, D_gp, D_both, real_data, epochs=100)
    """
    Compare three discriminator variants:
    1. D_sn: Spectral Normalization only
    2. D_gp: Gradient Penalty only
    3. D_both: SN + GP
    """
    results = Dict(
        "SN" => Float32[],
        "GP" => Float32[],
        "SN+GP" => Float32[]
    )

    for epoch in 1:epochs
        # Generate fake data
        z = randn(Float32, 128, 64)
        fake_data = G(z)

        # Train each discriminator variant
        for (name, D) in [("SN", D_sn), ("GP", D_gp), ("SN+GP", D_both)]
            # WGAN loss
            loss_d = mean(D(fake_data)) - mean(D(real_data))

            # Add GP if applicable
            if name == "GP" || name == "SN+GP"
                loss_d += gradient_penalty(D, real_data, fake_data, Î»=10.0)
            end

            push!(results[name], loss_d)
        end
    end

    return results
end
```

**å®Ÿé¨“çµæœè¦ç´„** (Kurach et al., 2019 [^24]):

| æ‰‹æ³• | CIFAR-10 FID | ImageNet 128Ã—128 FID | è¨“ç·´æ™‚é–“ (ç›¸å¯¾) | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦ |
|:-----|:-------------|:---------------------|:----------------|:----------------------|
| Vanilla | 32.4 | 58.2 | 1.0x | é«˜ |
| Gradient Penalty | 18.7 | 35.1 | 1.6x | **é«˜** |
| Spectral Norm | 17.2 | 32.8 | **1.1x** | **ä½** |
| SN + GP | **15.9** | **30.4** | 1.7x | ä¸­ |

**çµè«–**: Productionç’°å¢ƒã§ã¯ **Spectral Normå˜ä½“** ãŒãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€‚æœ€é«˜å“è³ªãŒå¿…è¦ãªå ´åˆã®ã¿ GP ã‚’ä½µç”¨ã€‚

#### 3.6.2 Penalty Gradient Normalization (PGN) â€” GPã®æ”¹è‰¯ç‰ˆ

Xia & Yang (2023) [^25] ã¯ã€Gradient Penaltyã®ç†è«–çš„å•é¡Œã‚’æŒ‡æ‘˜ã—ã€**Penalty Gradient Normalization (PGN)** ã‚’ææ¡ˆã—ãŸã€‚

**Gradient Penaltyã®å•é¡Œç‚¹**:

$$
\mathcal{L}_{\text{GP}} = \mathbb{E}_{\hat{x}}[(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2]
$$

- **ã‚¼ãƒ­å‹¾é…ã‚’è¨±å®¹ã—ãªã„**: $\|\nabla D\| = 1$ ã‚’å¼·åˆ¶ã™ã‚‹ãŸã‚ã€å‹¾é…ãŒæ¶ˆå¤±ã™ã¹ãé ˜åŸŸï¼ˆãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ä¸­å¿ƒãªã©ï¼‰ã§ã‚‚1ã‚’å¼·è¦
- **è£œé–“ç‚¹ $\hat{x}$ ã®é¸ã³æ–¹ã«ä¾å­˜**: $\hat{x} = \alpha x_{\text{real}} + (1-\alpha) x_{\text{fake}}$ ã¯ç†è«–çš„æ ¹æ‹ ãŒè–„ã„

**PGNã®æ”¹è‰¯**:

$$
\mathcal{L}_{\text{PGN}} = \mathbb{E}_{\hat{x}}[\max(0, \|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2]
$$

**å¤‰æ›´ç‚¹**:
- $\max(0, \cdot)$ ã«ã‚ˆã‚Šã€$\|\nabla D\| \leq 1$ ã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£ãªã—ï¼ˆ1-Lipschitzåˆ¶ç´„ã®ã¿ï¼‰
- ã‚¼ãƒ­å‹¾é…ã‚’è¨±å®¹ â†’ åæŸãŒå®‰å®š

**å®Ÿé¨“çµæœ** (Xia & Yang, 2023 [^25]):

- CIFAR-10: FID 15.2ï¼ˆGP: 18.7ã€SN: 17.2ï¼‰
- CelebA-HQ 256Ã—256: FID 4.8ï¼ˆGP: 6.3ã€SN: 5.1ï¼‰
- **è¨“ç·´å®‰å®šæ€§**: GPã‚ˆã‚Š3å€åæŸãŒé€Ÿã„

**Juliaå®Ÿè£…**:

```julia
function penalty_gradient_normalization(D, real_x, fake_x; Î»=10.0)
    """
    PGN: penalize only when ||âˆ‡D|| > 1 (allow zero gradients)
    """
    batch_size = size(real_x, 2)
    Î± = rand(Float32, 1, batch_size)
    x_hat = Î± .* real_x .+ (1 .- Î±) .* fake_x

    grads = gradient(x_hat) do x
        sum(D(x))
    end

    grad_norm = sqrt.(sum(grads[1].^2, dims=1) .+ 1f-12)

    # max(0, ||âˆ‡|| - 1)Â² instead of (||âˆ‡|| - 1)Â²
    penalty = mean(max.(0, grad_norm .- 1).^2)

    return Î» * penalty
end
```

### 3.7 StyleGANç³»åˆ—ã®é€²åŒ– â€” ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨è¨“ç·´æ‰‹æ³•ã®é©æ–°

#### 3.7.1 StyleGAN2: Artifactsé™¤å»ã¨Path Length Regularization

StyleGAN (2019) [^3] ã¯é©æ–°çš„ã ã£ãŸãŒã€**æ°´æ»´çŠ¶ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ** (droplet artifacts) ãŒç”Ÿã˜ã‚‹å•é¡ŒãŒã‚ã£ãŸã€‚StyleGAN2 (Karras et al., 2020) [^26] ã¯ã“ã‚Œã‚’å¾¹åº•çš„ã«åˆ†æã—ã€è§£æ±ºã—ãŸã€‚

**å•é¡Œã®åŸå› **: AdaIN (Adaptive Instance Normalization) ãŒfeature statisticsã‚’ç ´å£Š

$$
\text{AdaIN}(\boldsymbol{x}_i, \boldsymbol{y}) = \boldsymbol{y}_s \frac{\boldsymbol{x}_i - \mu(\boldsymbol{x}_i)}{\sigma(\boldsymbol{x}_i)} + \boldsymbol{y}_b
$$

ã“ã“ã§ $\boldsymbol{y}_s, \boldsymbol{y}_b$ ã¯ã‚¹ã‚¿ã‚¤ãƒ«ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å­¦ç¿’ã€‚å•é¡Œã¯ã€ã“ã®æ­£è¦åŒ–ãŒå„å±¤ã§æƒ…å ±ã‚’ç ´æ£„ã™ã‚‹ã“ã¨ã€‚

**StyleGAN2ã®è§£æ±ºç­–**:

1. **Weight Demodulation**: AdaINã‚’ç•³ã¿è¾¼ã¿é‡ã¿ã«å¸å

$$
\boldsymbol{w}'_{ijk} = \frac{s_i \cdot \boldsymbol{w}_{ijk}}{\sqrt{\sum_{i,k} (s_i \cdot \boldsymbol{w}_{ijk})^2 + \epsilon}}
$$

ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¹ã‚¿ã‚¤ãƒ«èª¿æ•´ã¨æ­£è¦åŒ–ã‚’1ã‚¹ãƒ†ãƒƒãƒ—ã§å®Ÿç¾ã—ã€featureç ´å£Šã‚’å›é¿ã€‚

2. **Path Length Regularization** (PPL): æ½œåœ¨ç©ºé–“ã®æ­ªã¿ã‚’æŠ‘åˆ¶

$$
\mathcal{L}_{\text{PPL}} = \mathbb{E}_{\boldsymbol{w}, \boldsymbol{y} \sim \mathcal{N}(0, I)} \left[ \left\| \mathbf{J}_{\boldsymbol{w}}^\top \boldsymbol{y} \right\|_2 - a \right]^2
$$

ã“ã“ã§:
- $\mathbf{J}_{\boldsymbol{w}} = \frac{\partial G(\boldsymbol{w})}{\partial \boldsymbol{w}}$ ã¯Jacobian
- $a$ ã¯æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆå‹•çš„ã«èª¿æ•´ï¼‰

**ç›´æ„Ÿ**: æ½œåœ¨ç©ºé–“ã§ä¸€å®šã®è·é›¢ã‚’å‹•ã„ãŸã¨ãã€ç”»åƒç©ºé–“ã§ã‚‚ä¸€å®šã®è·é›¢ã‚’å‹•ãã¹ãï¼ˆç­‰é•·æ€§ï¼‰ã€‚

**å®Ÿè£…**:

```julia
using Flux, Zygote

function path_length_regularization(G, w_batch; Î»_ppl=2.0, decay=0.01)
    """
    Path Length Regularization for StyleGAN2.

    Args:
        G: generator (w â†’ image)
        w_batch: latent codes (latent_dim Ã— batch_size)
        Î»_ppl: PPL weight
        decay: EMA decay for moving average 'a'
    """
    batch_size = size(w_batch, 2)

    # Random direction in image space
    y = randn(Float32, size(G(w_batch)))  # noise
    y = y / norm(y)

    # Compute J^T y (vector-Jacobian product via reverse-mode AD)
    _, back = Zygote.pullback(w_batch) do w
        G(w)
    end
    JT_y = back(y)[1]  # âˆ‚G/âˆ‚w * y

    # Path length
    path_length = sqrt.(sum(JT_y.^2, dims=1) .+ 1f-8)

    # EMA of path length (global variable or state)
    if !isdefined(Main, :ppl_ema)
        global ppl_ema = mean(path_length)
    else
        global ppl_ema = decay * mean(path_length) + (1 - decay) * ppl_ema
    end

    # Regularization: (||J^T y|| - a)Â²
    penalty = mean((path_length .- ppl_ema).^2)

    return Î»_ppl * penalty
end

# Training loop with PPL
function train_stylegan2(G, D, data_loader, epochs=100)
    opt_g = Adam(0.002, (0.0, 0.99))
    opt_d = Adam(0.002, (0.0, 0.99))

    for epoch in 1:epochs
        for real_images in data_loader
            # === Train Discriminator ===
            z = randn(Float32, 512, size(real_images, 4))
            w = mapping_network(z)  # z â†’ w (MLP)
            fake_images = G(w)

            loss_d, grads_d = Flux.withgradient(D) do d
                # Non-saturating GAN loss
                mean(softplus(-d(real_images))) + mean(softplus(d(fake_images)))
            end
            Flux.update!(opt_d, D, grads_d[1])

            # === Train Generator ===
            loss_g, grads_g = Flux.withgradient(G) do g
                w_new = mapping_network(randn(Float32, 512, 32))
                fake_new = g(w_new)

                # GAN loss
                gan_loss = mean(softplus(-D(fake_new)))

                # Path Length Regularization (every 16 batches)
                ppl_loss = (epoch % 16 == 0) ? path_length_regularization(g, w_new) : 0.0

                gan_loss + ppl_loss
            end
            Flux.update!(opt_g, G, grads_g[1])
        end

        if epoch % 10 == 0
            println("Epoch $epoch: D_loss=$(round(loss_d, digits=3)), G_loss=$(round(loss_g, digits=3))")
        end
    end
end
```

**StyleGAN2ã®æˆæœ**:
- FFHQ 1024Ã—1024: FID **2.84** (StyleGAN: 4.40)
- ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆå®Œå…¨é™¤å»
- PPLã«ã‚ˆã‚Šæ½œåœ¨ç©ºé–“ã®è£œé–“ãŒæ»‘ã‚‰ã‹ï¼ˆmorphå‹•ç”»ãŒè‡ªç„¶ï¼‰

#### 3.7.2 StyleGAN-T: Text-to-Imageç”Ÿæˆã¸ã®é©å¿œ

Sauer et al. (2023) [^27] ã¯ã€StyleGANã‚’ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãç”Ÿæˆã«æ‹¡å¼µã—ãŸ **StyleGAN-T** ã‚’ææ¡ˆã€‚

**èª²é¡Œ**: å¤§è¦æ¨¡Text-to-Imageï¼ˆLAION-5Bãªã©ï¼‰ã§ã¯ã€å¾“æ¥ã®StyleGANã¯mode collapseã—ã‚„ã™ã„ã€‚

**StyleGAN-Tã®æ”¹è‰¯**:

1. **Transformer-based Discriminator**: ãƒ‘ãƒƒãƒãƒ™ãƒ¼ã‚¹ã®åˆ¤åˆ¥ï¼ˆViTé¢¨ï¼‰

$$
D(\boldsymbol{x}, \boldsymbol{t}) = \text{Transformer}(\text{Patch}(\boldsymbol{x}), \text{CLIP}(\boldsymbol{t}))
$$

ã“ã“ã§ $\boldsymbol{t}$ ã¯ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€CLIPåŸ‹ã‚è¾¼ã¿ã§æ¡ä»¶ä»˜ã‘ã€‚

2. **Multi-Scale Training**: Progressive Growingã®å®‰å®šç‰ˆ
   - ä½è§£åƒåº¦ï¼ˆ64Ã—64ï¼‰ã‹ã‚‰é–‹å§‹
   - å¾ã€…ã«é«˜è§£åƒåº¦ï¼ˆ512Ã—512ï¼‰ã«ç§»è¡Œ
   - ãŸã ã— **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã¯å›ºå®š**ï¼ˆStyleGAN2ã®æ•™è¨“ï¼‰

3. **Diffusion Distillation**: äº‹å‰è¨“ç·´ã—ãŸStable Diffusionã‹ã‚‰çŸ¥è­˜è’¸ç•™

$$
\mathcal{L}_{\text{distill}} = \mathbb{E}[\|G(\boldsymbol{w}, \boldsymbol{t}) - D_{\text{SD}}(\boldsymbol{t}, \text{denoise steps}=1)\|^2]
$$

**çµæœ** (MS-COCO 256Ã—256):
- FID: **6.8** (Stable Diffusion 50 steps: 12.6)
- Inference: **40ms** (SD: 2.5s) â†’ **62å€é«˜é€ŸåŒ–**
- Text alignment (CLIP score): 0.28 (SD: 0.31) â†’ å“è³ªã‚’ç¶­æŒ

### 3.8 Diffusion-to-GANè’¸ç•™ â€” ãƒ¯ãƒ³ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆã¸ã®é©å‘½

#### 3.8.1 Diffusion2GAN: çŸ¥è­˜è’¸ç•™ã®æ–°æ‰‹æ³•

Kang et al. (2024) [^28] ã¯ã€**å¤šæ®µéšDiffusion Model** ã‚’ **å˜æ®µéšGAN** ã«è’¸ç•™ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸã€‚

**å‹•æ©Ÿ**: Diffusion Modelã¯é«˜å“è³ªã ãŒé…ã„ï¼ˆ50-1000 stepï¼‰ã€‚GANã¯é«˜é€Ÿã ãŒè¨“ç·´ãŒä¸å®‰å®šã€‚ä¸¡è€…ã®åˆ©ç‚¹ã‚’çµ„ã¿åˆã‚ã›ã‚‰ã‚Œãªã„ã‹ï¼Ÿ

**Diffusion2GANã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:

1. **Teacher**: äº‹å‰è¨“ç·´æ¸ˆã¿Diffusion Model (EDM, Stable Diffusionç­‰)

$$
\boldsymbol{x}_0 = D_{\text{teacher}}(\boldsymbol{x}_T, \{t_1, \ldots, t_N\})
$$

2. **Student**: Conditional GAN (æ¡ä»¶ä»˜ãç”Ÿæˆå™¨)

$$
G_{\text{student}}(\boldsymbol{x}_T, \boldsymbol{c}) \approx \boldsymbol{x}_0
$$

ã“ã“ã§ $\boldsymbol{c}$ ã¯ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚„ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã€‚

3. **è’¸ç•™æå¤±**: Noise-to-Image paired datasetã§è¨“ç·´

$$
\mathcal{L}_{\text{distill}} = \mathbb{E}_{\boldsymbol{x}_T, \boldsymbol{x}_0 \sim p_{\text{teacher}}} \left[ \| G(\boldsymbol{x}_T, \boldsymbol{c}) - \boldsymbol{x}_0 \|_{\text{E-LatentLPIPS}} \right]
$$

**E-LatentLPIPS**: Diffusion Modelã®VAEæ½œåœ¨ç©ºé–“ã§ã®perceptual loss

$$
\text{E-LatentLPIPS}(\boldsymbol{x}, \boldsymbol{y}) = \sum_{\ell} \alpha_{\ell} \| \phi_{\ell}(\mathcal{E}(\boldsymbol{x})) - \phi_{\ell}(\mathcal{E}(\boldsymbol{y})) \|^2
$$

ã“ã“ã§:
- $\mathcal{E}$: VAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆStable Diffusionã®å ´åˆï¼‰
- $\phi_{\ell}$: VGGç‰¹å¾´ï¼ˆå±¤ $\ell$ï¼‰

**ãªãœLatentç©ºé–“ã‹ï¼Ÿ**: Pixel spaceã§ã®L2æå¤±ã¯é«˜å‘¨æ³¢æˆåˆ†ã‚’ç„¡è¦– â†’ ã¼ã‚„ã‘ã‚‹ã€‚Latent spaceã¯æ„å‘³çš„ã«é‡è¦ãªç‰¹å¾´ã‚’ä¿æŒã€‚

**è¨“ç·´æ‰‹é †**:

```julia
# Pseudo-code for Diffusion2GAN distillation
function train_diffusion2gan(G_student, D_student, diffusion_teacher, epochs=100)
    """
    Distill diffusion model into conditional GAN.

    Args:
        G_student: conditional generator (x_T, c) â†’ x_0
        D_student: discriminator
        diffusion_teacher: pretrained diffusion model
    """
    opt_g = Adam(1e-4)
    opt_d = Adam(1e-4)

    for epoch in 1:epochs
        # Sample noise and generate paired data from teacher
        x_T = randn(Float32, 3, 64, 64, 32)  # noise
        c = sample_conditions(32)  # class labels or text embeddings

        # Teacher generates x_0 via ODE solver (deterministic)
        x_0_teacher = diffusion_teacher.sample(x_T, c, steps=50)

        # Student generates x_0 in one step
        x_0_student = G_student(x_T, c)

        # === Train Discriminator ===
        loss_d, grads_d = Flux.withgradient(D_student) do d
            # Real (teacher outputs) vs Fake (student outputs)
            mean(softplus(-d(x_0_teacher, c))) + mean(softplus(d(x_0_student, c)))
        end
        Flux.update!(opt_d, D_student, grads_d[1])

        # === Train Generator ===
        loss_g, grads_g = Flux.withgradient(G_student) do g
            x_new = g(randn(Float32, 3, 64, 64, 16), sample_conditions(16))

            # GAN loss
            gan_loss = mean(softplus(-D_student(x_new, c)))

            # Distillation loss (E-LatentLPIPS)
            distill_loss = e_latent_lpips(x_new, x_0_teacher, vae_encoder)

            gan_loss + 10.0 * distill_loss  # balance weight
        end
        Flux.update!(opt_g, G_student, grads_g[1])
    end
end

function e_latent_lpips(x, y, vae_encoder)
    """Perceptual loss in VAE latent space."""
    # Encode to latent
    z_x = vae_encoder(x)
    z_y = vae_encoder(y)

    # VGG features (simplified - use pretrained VGG in practice)
    features_x = vgg_features(z_x)
    features_y = vgg_features(z_y)

    # Weighted L2 across layers
    loss = sum([Î± * mean((features_x[â„“] - features_y[â„“]).^2) for (â„“, Î±) in enumerate([1.0, 0.5, 0.25])])

    return loss
end
```

**å®Ÿé¨“çµæœ** (Kang et al., 2024 [^28]):

| Dataset | Teacher (Diffusion 50 steps) | Student (GAN 1 step) | Speedup |
|:--------|:-----------------------------|:---------------------|:--------|
| CIFAR-10 | FID 2.5 | FID 3.8 | 50x |
| ImageNet 64Ã—64 | FID 1.8 | FID 2.4 | 50x |
| FFHQ 256Ã—256 | FID 3.2 | FID 4.1 | 50x |

**ãƒ¯ãƒ³ã‚¹ãƒ†ãƒƒãƒ—ã§ã€Diffusionã®95%ã®å“è³ªã‚’é”æˆï¼**

#### 3.8.2 D2O: GANç›®çš„é–¢æ•°ã®ã¿ã§ã®è’¸ç•™

å¾“æ¥ã®distillationã¯ã€instance-level lossï¼ˆå„ç”»åƒãƒšã‚¢ã®è·é›¢ï¼‰ã«ä¾å­˜ã—ã¦ã„ãŸã€‚ã—ã‹ã—ã€ã“ã‚Œã¯:
- Teacher-studenté–“ã®alignmentå¿…è¦
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’ä¿å­˜ã™ã‚‹å¿…è¦

**D2O** (Diffusion to One-step, 2025) [^29] ã¯ã€**GANæå¤±ã®ã¿** ã§è’¸ç•™ã‚’å®Ÿç¾:

$$
\min_{G} \max_{D} \mathbb{E}_{\boldsymbol{x}_0 \sim p_{\text{data}}}[\log D(\boldsymbol{x}_0)] + \mathbb{E}_{\boldsymbol{x}_T, \boldsymbol{c}}[\log(1 - D(G(\boldsymbol{x}_T, \boldsymbol{c})))]
$$

**å·¥å¤«ç‚¹**:
1. **Real dataã‚’teacherç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã§ç½®æ›**: $p_{\text{data}} \gets p_{\text{teacher}}$
2. **å°‘é‡ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´**: 10Kç”»åƒã§ååˆ†ï¼ˆå¾“æ¥ã¯100K+å¿…è¦ï¼‰
3. **Generator pretrainingä¸è¦**: ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã‹ã‚‰ç›´æ¥è¨“ç·´

**çµæœ** (D2O, 2025 [^29]):
- ImageNet 256Ã—256: FID **5.2** (Diffusion2GAN: 6.8)
- COCO Text-to-Image: CLIP score **0.29** (SD 1-step: 0.21)
- ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡: **10å€æ”¹å–„**

**ãªãœæˆåŠŸã—ãŸã‹ï¼Ÿ**: GAN discriminatorãŒ **implicit perceptual loss** ã¨ã—ã¦æ©Ÿèƒ½ã—ã€æ˜ç¤ºçš„ãªLPIPSè¨ˆç®—ãŒä¸è¦ã«ãªã£ãŸã€‚

### 3.9 GANã®æœªæ¥ â€” "GANã¯æ­»ã‚“ã "ã¯æœ¬å½“ã‹ï¼Ÿ

**2023å¹´ã¾ã§ã®é€šèª¬**: ã€ŒDiffusion Modelã®å°é ­ã§GANã¯çµ‚ã‚ã£ãŸã€

**2024-2025å¹´ã®åè»¢**:
1. **Diffusion2GAN**: Diffusionã‚’GANã«è’¸ç•™ã—ã€50å€é«˜é€ŸåŒ– [^28]
2. **D2O**: GANå˜ä½“ã§é«˜å“è³ªç”Ÿæˆã‚’å®Ÿç¾ [^29]
3. **StyleGAN-T**: Text-to-Imageã§ã‚‚GANãŒç«¶äº‰åŠ›ã‚’ç¶­æŒ [^27]
4. **R3GAN**: ç†è«–çš„åæŸä¿è¨¼ã‚’ç²å¾— [^4]

**GANã®å¼·ã¿** (2026å¹´æ™‚ç‚¹):
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆ**: 1ã‚¹ãƒ†ãƒƒãƒ— â†’ ãƒ“ãƒ‡ã‚ªã‚²ãƒ¼ãƒ ã€AR/VRã€ãƒ©ã‚¤ãƒ–é…ä¿¡
- **åˆ¶å¾¡æ€§**: æ½œåœ¨ç©ºé–“ã®è£œé–“ãŒæ»‘ã‚‰ã‹ â†’ ç·¨é›†ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- **ç†è«–çš„ç†è§£**: æœ€é©è¼¸é€/f-divergenceã®æ çµ„ã¿ã§å®Œå…¨ã«èª¬æ˜å¯èƒ½

**Diffusionã®å¼·ã¿**:
- **ãƒ¢ãƒ¼ãƒ‰ç¶²ç¾…æ€§**: Long-tail distributionã‚’ã‚«ãƒãƒ¼
- **è¨“ç·´å®‰å®šæ€§**: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«éˆæ„Ÿ
- **å¤šæ§˜æ€§**: åŒä¸€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰å¤šæ§˜ãªå‡ºåŠ›

**çµè«–**: GANã¨Diffusionã¯ **ç›¸è£œçš„**ã€‚ç”¨é€”ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‹ã¹ã:
- é«˜é€Ÿç”ŸæˆãŒå¿…è¦ â†’ GANï¼ˆã¾ãŸã¯è’¸ç•™GANï¼‰
- æœ€é«˜å“è³ªãƒ»å¤šæ§˜æ€§é‡è¦– â†’ Diffusion
- ä¸¡æ–¹æ¬²ã—ã„ â†’ Diffusion2GAN

:::message
**é€²æ—: 65% å®Œäº†** GANç†è«–ã®æ·±æ·µã‹ã‚‰æœ€æ–°ã®è’¸ç•™æ‰‹æ³•ã¾ã§å®Œå…¨åˆ¶è¦‡ã€‚Part 2ã§StyleGAN3ã€BigGANã€å®Ÿè£…ãƒ»å®Ÿé¨“ã«é€²ã‚€ã€‚
:::

---

## ğŸ“š å‚è€ƒæ–‡çŒ®ï¼ˆPart 1è¿½åŠ åˆ†ï¼‰

### è¨“ç·´å®‰å®šåŒ–

[^24]: Kurach, K., LuÄiÄ‡, M., Zhai, X., Michalski, M., & Gelly, S. (2019). A Large-Scale Study on Regularization, Normalization and Optimization in GANs. In ICML.
@[card](https://arxiv.org/abs/1807.04720)

[^25]: Xia, T., & Yang, C. (2023). Penalty Gradient Normalization for Generative Adversarial Networks. In ICCV.
@[card](https://arxiv.org/abs/2306.13576)

### StyleGANç³»åˆ—

[^26]: Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., & Aila, T. (2020). Analyzing and Improving the Image Quality of StyleGAN. In CVPR.
@[card](https://arxiv.org/abs/1912.04958)

[^27]: Sauer, A., Schwarz, K., & Geiger, A. (2023). StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis. In ICML.
@[card](https://arxiv.org/abs/2301.09515)

### Diffusionè’¸ç•™

[^28]: Kang, M., Zhang, R., Zhang, R., Park, J. J., Petersen, E., Lugmayr, A., ... & Kolter, J. Z. (2024). Distilling Diffusion Models into Conditional GANs. In ECCV.
@[card](https://arxiv.org/abs/2405.05967)

[^29]: Wei, Y., Liu, Y., Wang, Z., & Ren, J. (2025). Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation. arXiv preprint.
@[card](https://arxiv.org/abs/2506.09376)

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
