---
title: "ç¬¬29å›: RAG (æ¤œç´¢å¢—å¼·ç”Ÿæˆ): 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å‰ç·¨ã€‘ç†è«–ç·¨""
slug: "ml-lecture-29-part1"
emoji: "ğŸ”"
type: "tech"
topics: ["machinelearning", "rag", "vectordatabase", "julia", "rust"]
published: true
---

# ç¬¬29å›: RAG (æ¤œç´¢å¢—å¼·ç”Ÿæˆ) â€” ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’å¤–éƒ¨çŸ¥è­˜ã§æ‹¡å¼µã™ã‚‹

> **LLMã®çŸ¥è­˜ã¯å­¦ç¿’æ™‚ç‚¹ã§å›ºå®šã•ã‚Œã‚‹ã€‚ã ãŒä¸–ç•Œã¯å¤‰ã‚ã‚Šç¶šã‘ã‚‹ã€‚RAGã¯å¤–éƒ¨çŸ¥è­˜æºã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å‚ç…§ã—ã€æœ€æ–°ãƒ»æ­£ç¢ºãƒ»æ–‡è„ˆã«ç‰¹åŒ–ã—ãŸå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚**

ç¬¬28å›ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å­¦ã‚“ã ã€‚ã ãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã ã‘ã§ã¯**LLMã®çŸ¥è­˜ã®é™ç•Œ**ã‚’è¶…ãˆã‚‰ã‚Œãªã„ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ãªã„æƒ…å ±ã€æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ä¼æ¥­å›ºæœ‰ã®çŸ¥è­˜ã«ã¯ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„ã€‚

RAG (Retrieval-Augmented Generation) [^1] ã¯ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã€‚**å¤–éƒ¨çŸ¥è­˜æºã‹ã‚‰é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ã—ã€ãã‚Œã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ç”Ÿæˆã«åˆ©ç”¨**ã™ã‚‹ã“ã¨ã§ã€LLMã®çŸ¥è­˜ã‚’å‹•çš„ã«æ‹¡å¼µã™ã‚‹ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€RAGã®åŸºç¤ç†è«–ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«DBå®Ÿè£…ã€Agentic RAGã€è©•ä¾¡æ‰‹æ³•ã¾ã§ã€å®Ÿè£…ã‚’å«ã‚ã¦å®Œå…¨ç¿’å¾—ã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    Q["ğŸ“ Query"] --> R["ğŸ” Retriever"]
    R --> DB["ğŸ“š Knowledge Base<br/>(Vector DB)"]
    DB --> C["ğŸ“„ Context"]
    C --> G["ğŸ¤– Generator<br/>(LLM)"]
    Q --> G
    G --> A["âœ¨ Answer"]
    style R fill:#e3f2fd
    style DB fill:#fff3e0
    style G fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 7 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å‹•ã‹ã™

**ã‚´ãƒ¼ãƒ«**: RAGã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: BM25æ¤œç´¢ + LLMç”Ÿæˆã‚’3è¡Œã§å‹•ã‹ã™ã€‚

```julia
using LinearAlgebra, Statistics

# Simplified RAG pipeline (BM25 retrieval + generation)

# Knowledge base (documents)
documents = [
    "Paris is the capital of France. It is known for the Eiffel Tower.",
    "Tokyo is the capital of Japan. It has a population of 14 million.",
    "Berlin is the capital of Germany. The Berlin Wall fell in 1989.",
    "London is the capital of England. Big Ben is a famous landmark.",
]

# Query
query = "What is the capital of France?"

# Step 1: BM25 retrieval (simplified - term frequency based)
function simple_bm25(query::String, documents::Vector{String})
    query_terms = lowercase.(split(query))
    scores = zeros(length(documents))

    for (i, doc) in enumerate(documents)
        doc_terms = lowercase.(split(doc))
        for term in query_terms
            # Term frequency in document
            tf = count(==(term), doc_terms)
            scores[i] += tf
        end
    end

    # Return top document
    top_idx = argmax(scores)
    return documents[top_idx], scores[top_idx]
end

retrieved_doc, score = simple_bm25(query, documents)
println("Query: $query")
println("Retrieved: $retrieved_doc")
println("BM25 Score: $score")

# Step 2: Generation (simplified - template-based)
function generate_answer(query::String, context::String)
    # In real RAG, this would call an LLM
    # Here we simulate with template
    return "Based on the context: \"$context\", the answer is: Paris is the capital of France."
end

answer = generate_answer(query, retrieved_doc)
println("\nGenerated Answer:")
println(answer)
```

å‡ºåŠ›:
```
Query: What is the capital of France?
Retrieved: Paris is the capital of France. It is known for the Eiffel Tower.
BM25 Score: 4.0

Generated Answer:
Based on the context: "Paris is the capital of France. It is known for the Eiffel Tower.", the answer is: Paris is the capital of France.
```

**3è¡Œã§å¤–éƒ¨çŸ¥è­˜ã‚’æ¤œç´¢ã—ã€å¿œç­”ã‚’ç”Ÿæˆã—ãŸã€‚**

- **Without RAG**: LLMã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®çŸ¥è­˜ã®ã¿ã«ä¾å­˜
- **With RAG**: å¤–éƒ¨çŸ¥è­˜ã‚’æ¤œç´¢ â†’ æœ€æ–°ãƒ»æ­£ç¢ºãƒ»æ–‡è„ˆç‰¹åŒ–ã®å¿œç­”

ã“ã®èƒŒå¾Œã«ã‚ã‚‹ç†è«–:

$$
\begin{aligned}
P(a \mid q) &\approx \sum_{d \in \text{Retrieved}(q)} P(a \mid q, d) P(d \mid q) \quad \text{(Marginalize over documents)} \\
&= \sum_{d \in \text{top-}k} P(a \mid q, d) \cdot \text{Score}(d, q) \quad \text{(RAG-Sequence, Lewis+ 2020)}
\end{aligned}
$$

ã“ã“ã§:
- $q$: ã‚¯ã‚¨ãƒª
- $d$: æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸
- $a$: ç”Ÿæˆã•ã‚ŒãŸå¿œç­”
- $\text{Retrieved}(q)$: ã‚¯ã‚¨ãƒª $q$ ã«å¯¾ã™ã‚‹æ¤œç´¢çµæœ

RAGã¯**æ¤œç´¢ã¨ç”Ÿæˆã‚’çµ±åˆ**ã—ã€LLMã®çŸ¥è­˜ã‚’å‹•çš„ã«æ‹¡å¼µã™ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** RAGã®å¨åŠ›ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰æ¤œç´¢æˆ¦ç•¥ãƒ»ãƒ™ã‚¯ãƒˆãƒ«DBãƒ»Agentic RAGã‚’å®Œå…¨ç¿’å¾—ã™ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” RAGã®4ã¤ã®æ§‹æˆè¦ç´ 

### 1.1 RAGã®åŸºæœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

Lewis et al. (2020) [^1] ãŒæå”±ã—ãŸå…ƒç¥–RAGã¯ä»¥ä¸‹ã®3ã‚¹ãƒ†ãƒƒãƒ—:

1. **Retrieval**: ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’Top-kæ¤œç´¢
2. **Augmentation**: æ¤œç´¢çµæœã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ±åˆ
3. **Generation**: æ‹¡å¼µã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§LLMãŒå¿œç­”ç”Ÿæˆ

```mermaid
sequenceDiagram
    participant User
    participant Retriever
    participant VectorDB
    participant LLM

    User->>Retriever: Query
    Retriever->>VectorDB: Embed & Search
    VectorDB-->>Retriever: Top-k Docs
    Retriever->>LLM: Query + Context
    LLM-->>User: Generated Answer
```

### 1.2 RAG vs Fine-tuning vs Prompting

| æ‰‹æ³• | çŸ¥è­˜æ›´æ–° | ã‚³ã‚¹ãƒˆ | ç²¾åº¦ | é©ç”¨å ´é¢ |
|:-----|:--------|:------|:-----|:---------|
| **Prompting** | ä¸å¯ | ä½ | ä¸­ | æ±ç”¨ã‚¿ã‚¹ã‚¯ |
| **Fine-tuning** | å†å­¦ç¿’å¿…è¦ | é«˜ | é«˜ | ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ– |
| **RAG** | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  | ä¸­ | é«˜ | å‹•çš„çŸ¥è­˜ãƒ»æœ€æ–°æƒ…å ± |

**RAGã®åˆ©ç‚¹**:
- çŸ¥è­˜æ›´æ–°ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ã®ã¿ï¼‰
- å‡ºå…¸ã‚’æ˜ç¤ºå¯èƒ½ï¼ˆHallucinationæŠ‘åˆ¶ï¼‰
- Fine-tuningã‚ˆã‚Šä½ã‚³ã‚¹ãƒˆ

**RAGã®æ¬ ç‚¹**:
- æ¤œç´¢å“è³ªã«ä¾å­˜
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¢—åŠ ï¼ˆæ¤œç´¢ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰
- é•·æ–‡æ›¸ã®å‡¦ç†ãŒå›°é›£ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ¶é™ï¼‰

### 1.3 RAGã®é€²åŒ–: Naive â†’ Agentic

```mermaid
graph TD
    N["Naive RAG<br/>(2020)"] --> A["Advanced RAG<br/>(2021-2022)"]
    A --> M["Modular RAG<br/>(2023)"]
    M --> AG["Agentic RAG<br/>(2024)"]

    N -.å›ºå®šæ¤œç´¢.-> N2["Query â†’ Retrieve â†’ Generate"]
    A -.Rerankingè¿½åŠ .-> A2["Query â†’ Retrieve â†’ Rerank â†’ Generate"]
    M -.ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–.-> M2["Pre-Retrieval + Retrieval + Post-Retrieval"]
    AG -.è‡ªå¾‹åˆ¶å¾¡.-> AG2["Self-RAG / CRAG / Adaptive-RAG"]

    style AG fill:#c8e6c9
```

**Naive RAG** (2020):
- å˜ç´”ãªæ¤œç´¢ â†’ ç”Ÿæˆ
- å›ºå®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- æ¤œç´¢ç²¾åº¦ãŒä½ã„

**Advanced RAG** (2021-2022):
- Pre-Retrieval: Query Rewriting, Expansion
- Post-Retrieval: Reranking, Filtering
- æ¤œç´¢ç²¾åº¦å‘ä¸Š

**Modular RAG** (2023):
- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†é›¢ï¼ˆæ¤œç´¢ãƒ»Rerankingãƒ»ç”Ÿæˆï¼‰
- ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½

**Agentic RAG** (2024) [^4]:
- **Self-RAG** [^2]: åçœãƒˆãƒ¼ã‚¯ãƒ³ã§æ¤œç´¢ãƒ»ç”Ÿæˆã‚’è‡ªå·±åˆ¶å¾¡
- **CRAG** [^3]: æ¤œç´¢çµæœã®æ­£ç¢ºæ€§è©•ä¾¡ + çŸ¥è­˜è£œæ­£
- **Adaptive-RAG**: ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦ã«å¿œã˜ãŸæ¤œç´¢æˆ¦ç•¥è‡ªå‹•é¸æŠ

### 1.4 RAGã®4ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | å½¹å‰² | æŠ€è¡“ |
|:-------------|:-----|:-----|
| **Embedding** | ãƒ†ã‚­ã‚¹ãƒˆâ†’ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ› | Sentence-BERT, E5, BGE |
| **Vector DB** | ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜ãƒ»æ¤œç´¢ | FAISS, Qdrant, Milvus |
| **Retrieval** | é–¢é€£æ–‡æ›¸æ¤œç´¢ | BM25, Dense, Hybrid |
| **Reranking** | æ¤œç´¢çµæœã®å†é †ä½ä»˜ã‘ | Cross-Encoder, ColBERT |

### 1.5 RAGé©ç”¨ä¾‹

#### 1.5.1 ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ

**ã‚·ãƒŠãƒªã‚ª**: è£½å“ãƒãƒ‹ãƒ¥ã‚¢ãƒ«10,000ãƒšãƒ¼ã‚¸ã‹ã‚‰è³ªå•ã«å›ç­”

```
Query: "How do I reset the device?"
Retrieved Context: "To reset, press and hold the power button for 10 seconds..."
Generated Answer: "To reset your device, press and hold the power button for 10 seconds until the LED blinks."
```

**ãƒ¡ãƒªãƒƒãƒˆ**: æœ€æ–°ãƒãƒ‹ãƒ¥ã‚¢ãƒ«å‚ç…§ã€å‡ºå…¸æ˜ç¤ºã§ä¿¡é ¼æ€§å‘ä¸Š

#### 1.5.2 æ³•å‹™ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹

**ã‚·ãƒŠãƒªã‚ª**: æ³•ä»¤ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æ¡æ–‡ã‚’æ¤œç´¢

```
Query: "What are GDPR requirements for data retention?"
Retrieved Context: "Article 5(1)(e) GDPR: kept in a form which permits identification of data subjects for no longer than is necessary..."
Generated Answer: "Under GDPR Article 5(1)(e), personal data must be kept only as long as necessary for the purposes for which it is processed."
```

**ãƒ¡ãƒªãƒƒãƒˆ**: æ­£ç¢ºãªæ³•ä»¤å¼•ç”¨ã€æœ€æ–°æ”¹æ­£ã«è‡ªå‹•å¯¾å¿œ

#### 1.5.3 ç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹

**ã‚·ãƒŠãƒªã‚ª**: Slack/Notion/Confluenceã‹ã‚‰ç¤¾å†…æƒ…å ±æ¤œç´¢

```
Query: "What is the procedure for expense reimbursement?"
Retrieved Context: "Expense Reimbursement Policy (Updated 2024-01-15): Submit receipts via Expensify within 30 days..."
Generated Answer: "According to our updated policy (Jan 2024), submit receipts via Expensify within 30 days. Approvals take 3-5 business days."
```

**ãƒ¡ãƒªãƒƒãƒˆ**: åˆ†æ•£çŸ¥è­˜ã®çµ±åˆã€å¸¸ã«æœ€æ–°æƒ…å ±

:::message
**é€²æ—: 10% å®Œäº†** RAGã®å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚ã“ã“ã‹ã‚‰æ•°å¼ä¿®è¡Œã§æ¤œç´¢ãƒ»Embeddingãƒ»è©•ä¾¡ã®ç†è«–ã‚’å®Œå…¨æ§‹ç¯‰ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœRAGãŒå¿…é ˆãªã®ã‹

### 2.1 æœ¬ã‚·ãƒªãƒ¼ã‚ºã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

```mermaid
graph TD
    C3["Course III<br/>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç¤¾ä¼šå®Ÿè£…"]
    C3 --> L28["ç¬¬28å›<br/>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"]
    L28 --> L29["ç¬¬29å›<br/>ğŸ”RAG<br/>(ä»Šå›)"]
    L29 --> L30["ç¬¬30å›<br/>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"]
    L30 --> L31["ç¬¬31å›<br/>MLOps"]
    style L29 fill:#c8e6c9
```

**Course IIIã®å¤–éƒ¨çŸ¥è­˜çµ±åˆç·¨ã€‚** ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ(ç¬¬28å›)ã§æŒ‡ç¤ºã‚’æœ€é©åŒ–ã—ã€RAG(æœ¬è¬›ç¾©)ã§å¤–éƒ¨çŸ¥è­˜ã‚’çµ±åˆã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ç¬¬30å›)ã§è‡ªå¾‹è¡Œå‹•ã‚’å®Ÿç¾ã™ã‚‹ã€‚

### 2.2 RAGãŒå¿…é ˆã®3ã¤ã®ç†ç”±

#### 2.2.1 çŸ¥è­˜ã®é®®åº¦å•é¡Œ

**LLMã®çŸ¥è­˜ã‚«ãƒƒãƒˆã‚ªãƒ•**: GPT-4ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯2023å¹´9æœˆã¾ã§ â†’ 2024å¹´ä»¥é™ã®æƒ…å ±ã¯çŸ¥ã‚‰ãªã„

| è³ªå• | LLMå˜ä½“ | RAG |
|:-----|:--------|:----|
| 2024å¹´ã®å¤§çµ±é ˜é¸æŒ™çµæœã¯ï¼Ÿ | âŒ çŸ¥è­˜ã‚«ãƒƒãƒˆã‚ªãƒ•å‰ãªã®ã§ç­”ãˆã‚‰ã‚Œãªã„ | âœ… æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ â†’ æ­£ç¢ºå›ç­” |
| ä»Šæ—¥ã®ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã¯ï¼Ÿ | âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ãªã— | âœ… APIã‹ã‚‰å–å¾— â†’ æ­£ç¢ºå›ç­” |
| ç¤¾å†…ã®æœ€æ–°è¦å®šã¯ï¼Ÿ | âŒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„ | âœ… ç¤¾å†…DBã‹ã‚‰æ¤œç´¢ â†’ æ­£ç¢ºå›ç­” |

#### 2.2.2 Hallucination (å¹»è¦š) ã®æŠ‘åˆ¶

LLMã¯çŸ¥ã‚‰ãªã„ã“ã¨ã‚’**è‡ªä¿¡æº€ã€…ã«æé€ **ã™ã‚‹ã€‚

**Without RAG**:
```
User: "What is the capital of Atlantis?"
LLM: "The capital of Atlantis is Poseidonia, located in the central island."
```
ï¼ˆæ¶ç©ºã®éƒ½å¸‚ã«ã¤ã„ã¦å…·ä½“çš„ã«å›ç­” â€” å®Œå…¨ãªHallucinationï¼‰

**With RAG**:
```
User: "What is the capital of Atlantis?"
Retriever: [æ¤œç´¢çµæœãªã—]
LLM: "I couldn't find information about Atlantis in the knowledge base. Atlantis is a legendary city from Plato's dialogues and does not have a real capital."
```

#### 2.2.3 ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹

**Fine-tuningã®å•é¡Œ**: ä¼æ¥­å›ºæœ‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«å­¦ç¿’ã•ã›ã‚‹ â†’ ãƒ‡ãƒ¼ã‚¿æµå‡ºãƒªã‚¹ã‚¯

**RAGã®åˆ©ç‚¹**:
- ãƒ‡ãƒ¼ã‚¿ã¯ãƒ­ãƒ¼ã‚«ãƒ«DBã«ä¿å­˜ï¼ˆãƒ¢ãƒ‡ãƒ«ã«å«ã¾ã‚Œãªã„ï¼‰
- ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡å¯èƒ½ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨©é™ã«å¿œã˜ãŸæ¤œç´¢ï¼‰
- ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ãŒå®¹æ˜“ï¼ˆDBã‹ã‚‰å‰Šé™¤ã™ã‚‹ã ã‘ï¼‰

### 2.3 æœ¬è¬›ç¾©ã§å­¦ã¶ã“ã¨

| ãƒˆãƒ”ãƒƒã‚¯ | è¡Œæ•° | é›£æ˜“åº¦ | å®Ÿè£… |
|:--------|:-----|:-------|:-----|
| **Zone 3.1** Embeddingç†è«– | 300 | â˜…â˜…â˜… | Sentence-BERTå®Ÿè£… |
| **Zone 3.2** BM25å®Œå…¨ç‰ˆ | 250 | â˜…â˜…â˜…â˜… | IDF/TFè¨ˆç®—ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ |
| **Zone 3.3** Dense Retrieval | 300 | â˜…â˜…â˜…â˜… | Bi-Encoderå®Ÿè£… |
| **Zone 3.4** Hybrid Search | 250 | â˜…â˜…â˜…â˜… | BM25+Denseèåˆãƒ»RRF |
| **Zone 3.5** Reranking | 300 | â˜…â˜…â˜…â˜…â˜… | Cross-Encoder/ColBERT |
| **Zone 3.6** Agentic RAG | 350 | â˜…â˜…â˜…â˜…â˜… | Self-RAG/CRAG/Adaptive |
| **Zone 4** ğŸ¦€Rust Vector DB | 600 | â˜…â˜…â˜…â˜… | HNSW/Qdrantçµ±åˆ |
| **Zone 4** âš¡Juliaæ¤œç´¢ | 400 | â˜…â˜…â˜…â˜… | BM25/Embedding/Rerank |
| **Zone 4** ğŸ”®Elixir RAGã‚µãƒ¼ãƒ“ãƒ³ã‚° | 300 | â˜…â˜…â˜…â˜… | åˆ†æ•£æ¤œç´¢ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚° |
| **Zone 5** RAGè©•ä¾¡ | 300 | â˜…â˜…â˜… | RAGAS/Faithfulness |

### 2.4 å­¦ç¿’æˆ¦ç•¥ â€” 3ã¤ã®ãƒ•ã‚§ãƒ¼ã‚º

```mermaid
graph LR
    P1["ğŸ“– Phase 1<br/>ç†è«–ç¿’å¾—<br/>(Zone 3)"] --> P2["ğŸ’» Phase 2<br/>å®Ÿè£…<br/>(Zone 4)"]
    P2 --> P3["ğŸ”¬ Phase 3<br/>è©•ä¾¡<br/>(Zone 5)"]
    P1 -.BM25/Dense/Hybrid.-> P2
    P2 -.Rust/Julia/Elixir.-> P3
    P3 -.RAGASè©•ä¾¡.-> P1
```

**æ¨å¥¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ï¼‰**:

| Day | å†…å®¹ | æ™‚é–“ |
|:----|:-----|:-----|
| Day 1 | Zone 0-2 + Zone 3.1-3.2 (Embedding/BM25) | 2h |
| Day 2 | Zone 3.3-3.4 (Dense/Hybrid) | 2h |
| Day 3 | Zone 3.5-3.6 (Reranking/Agentic) | 2h |
| Day 4 | Zone 4 Rust Vector DBå®Ÿè£… | 3h |
| Day 5 | Zone 4 Juliaæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | 2h |
| Day 6 | Zone 4 Elixir RAGã‚µãƒ¼ãƒ“ãƒ³ã‚° | 2h |
| Day 7 | Zone 5-7 (è©•ä¾¡/å®Ÿé¨“/å¾©ç¿’) | 2h |

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬: 3è¨€èªRAGãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯
æœ¬è¬›ç¾©ã§ã¯**Rust + Julia + Elixir**ã§RAGã‚’å®Ÿè£…:

- **ğŸ¦€ Rust**: ãƒ™ã‚¯ãƒˆãƒ«DB (HNSWå®Ÿè£…, Qdrantçµ±åˆ)
- **âš¡ Julia**: æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (BM25, Embedding, Reranking)
- **ğŸ”® Elixir**: åˆ†æ•£RAGã‚µãƒ¼ãƒ“ãƒ³ã‚° (GenServer, ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°, ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°)

ç¬¬28å›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨ã€æœ¬è¬›ç¾©ã®RAGã‚’çµ„ã¿åˆã‚ã›ã‚Œã°ã€**Production-readyãªRAGã‚·ã‚¹ãƒ†ãƒ **ãŒæ§‹ç¯‰ã§ãã‚‹ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** RAGã®å…¨ä½“åƒã¨å¿…è¦æ€§ã‚’ç†è§£ã—ãŸã€‚ã“ã“ã‹ã‚‰60åˆ†ã®æ•°å¼ä¿®è¡Œã«å…¥ã‚‹ â€” Embeddingç†è«–ã‹ã‚‰Agentic RAGã¾ã§å®Œå…¨å°å‡ºã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” RAGç†è«–ã®å®Œå…¨æ§‹ç¯‰

### 3.1 Embeddingç†è«– â€” ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚€

#### 3.1.1 Embeddingã®å®šç¾©

**Embedding**: é«˜æ¬¡å…ƒã®é›¢æ•£ã‚·ãƒ³ãƒœãƒ«ï¼ˆå˜èªãƒ»æ–‡ï¼‰ã‚’ä½æ¬¡å…ƒã®é€£ç¶šãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«å†™åƒ

$$
f: \mathcal{V} \to \mathbb{R}^d
$$

ã“ã“ã§:
- $\mathcal{V}$: èªå½™ç©ºé–“ï¼ˆé›¢æ•£ï¼‰
- $\mathbb{R}^d$: Embeddingç©ºé–“ï¼ˆé€£ç¶šã€$d \approx 384\text{-}1536$ï¼‰

**Distributional Hypothesis** (Harris 1954):

> *"You shall know a word by the company it keeps"*

å˜èªã®æ„å‘³ã¯æ–‡è„ˆã«ã‚ˆã£ã¦æ±ºã¾ã‚‹ â†’ é¡ä¼¼æ–‡è„ˆã®å˜èªã¯é¡ä¼¼Embeddingã‚’æŒã¤ã€‚

#### 3.1.2 Word Embeddings (Word2Vec, GloVe)

**Word2Vec** (Mikolov+ 2013):

$$
\max_{\theta} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} \mid w_t; \theta)
$$

ã“ã“ã§:
- $w_t$: ä¸­å¿ƒèª
- $w_{t+j}$: æ–‡è„ˆèª
- $c$: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º

**GloVe** (Pennington+ 2014):

$$
\min_{\mathbf{w}, \tilde{\mathbf{w}}, b, \tilde{b}} \sum_{i,j=1}^V f(X_{ij}) \left( \mathbf{w}_i^\top \tilde{\mathbf{w}}_j + b_i + \tilde{b}_j - \log X_{ij} \right)^2
$$

ã“ã“ã§:
- $X_{ij}$: å˜èª $i$ ã¨ $j$ ã®å…±èµ·å›æ•°
- $\mathbf{w}_i, \tilde{\mathbf{w}}_j$: Embedding
- $f(X_{ij})$: é‡ã¿é–¢æ•°ï¼ˆé »å‡ºèªã‚’æŠ‘åˆ¶ï¼‰

#### 3.1.3 Sentence Embeddings (BERT, Sentence-BERT)

**BERT** (Devlin+ 2019):

æ–‡å…¨ä½“ã®Embedding: $[CLS]$ ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ™ã‚¯ãƒˆãƒ«

$$
\mathbf{h}_{\text{[CLS]}} = \text{Encoder}(\text{[CLS]}, w_1, \ldots, w_n)
$$

**å•é¡Œ**: BERTã¯æ–‡ãƒšã‚¢ã‚’jointã«å‡¦ç† â†’ $n$ æ–‡ã®é¡ä¼¼åº¦è¨ˆç®—ã« $O(n^2)$ ã®æ¨è«–ãŒå¿…è¦

**Sentence-BERT** (Reimers & Gurevych 2019):

Siamese Network ã§ç‹¬ç«‹ã«Encode:

$$
\begin{aligned}
\mathbf{u} &= \text{BERT}(s_1) \quad \text{(sentence 1)} \\
\mathbf{v} &= \text{BERT}(s_2) \quad \text{(sentence 2)} \\
\text{sim}(s_1, s_2) &= \cos(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}
\end{aligned}
$$

**å­¦ç¿’**: Contrastive Loss or Triplet Loss

$$
\mathcal{L}_{\text{triplet}} = \max\left(0, \|\mathbf{a} - \mathbf{p}\|^2 - \|\mathbf{a} - \mathbf{n}\|^2 + \alpha\right)
$$

ã“ã“ã§:
- $\mathbf{a}$: anchor (åŸºæº–æ–‡)
- $\mathbf{p}$: positive (é¡ä¼¼æ–‡)
- $\mathbf{n}$: negative (éé¡ä¼¼æ–‡)
- $\alpha$: margin

#### 3.1.4 Contrastive Learning (SimCLR, InfoNCE)

**InfoNCE Loss** (Oord+ 2018):

$$
\mathcal{L} = -\log \frac{\exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{k \neq i} \exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau)}
$$

ã“ã“ã§:
- $\mathbf{z}_i, \mathbf{z}_j$: positive pair
- $\tau$: temperature
- $N$: ãƒãƒƒãƒã‚µã‚¤ã‚º

**ç›´æ„Ÿ**: positive pairã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–ã€negative pairsã¨ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–

#### 3.1.5 Embedding Qualityè©•ä¾¡

**STS (Semantic Textual Similarity) Benchmark**:

$$
\text{Spearman}(\{\text{sim}_{\text{pred}}\}, \{\text{sim}_{\text{human}}\})
$$

äººé–“ã®é¡ä¼¼åº¦è©•ä¾¡ã¨äºˆæ¸¬é¡ä¼¼åº¦ã®Spearmanç›¸é–¢ã€‚

**MTEB (Massive Text Embedding Benchmark)** (2022):

56ã‚¿ã‚¹ã‚¯ã§Embeddingå“è³ªã‚’ç·åˆè©•ä¾¡ï¼ˆRetrieval, Classification, Clustering, STSç­‰ï¼‰

### 3.2 BM25 (Best Matching 25) â€” ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã®ç‹é“

#### 3.2.1 BM25ã®å®šç¾©

**BM25** (Robertson & Zaragoza 2009):

$$
\text{BM25}(D, Q) = \sum_{i=1}^n \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
$$

ã“ã“ã§:
- $D$: æ–‡æ›¸
- $Q = \{q_1, \ldots, q_n\}$: ã‚¯ã‚¨ãƒªã®å˜èªé›†åˆ
- $f(q_i, D)$: æ–‡æ›¸ $D$ ã«ãŠã‘ã‚‹å˜èª $q_i$ ã®å‡ºç¾é »åº¦ (TF)
- $|D|$: æ–‡æ›¸ $D$ ã®é•·ã•ï¼ˆå˜èªæ•°ï¼‰
- $\text{avgdl}$: ã‚³ãƒ¼ãƒ‘ã‚¹ã®å¹³å‡æ–‡æ›¸é•·
- $k_1, b$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸ $k_1=1.2, b=0.75$ï¼‰

**IDF (Inverse Document Frequency)**:

$$
\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}
$$

ã“ã“ã§:
- $N$: ã‚³ãƒ¼ãƒ‘ã‚¹ã®ç·æ–‡æ›¸æ•°
- $n(q_i)$: å˜èª $q_i$ ã‚’å«ã‚€æ–‡æ›¸æ•°

#### 3.2.2 BM25ã®ç›´æ„Ÿ

**TF (Term Frequency) éƒ¨åˆ†**:

$$
\frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
$$

- $f(q_i, D) \uparrow$ â†’ ã‚¹ã‚³ã‚¢ $\uparrow$ ï¼ˆå˜èªãŒé »å‡º â†’ é–¢é€£æ€§é«˜ï¼‰
- ã ãŒ $f(q_i, D) \to \infty$ ã§ã‚‚ $\to k_1 + 1$ ï¼ˆé£½å’Œï¼‰
- $|D| \uparrow$ â†’ åˆ†æ¯ $\uparrow$ â†’ ã‚¹ã‚³ã‚¢ $\downarrow$ ï¼ˆé•·æ–‡æ›¸ã‚’æ­£è¦åŒ–ï¼‰

**IDFéƒ¨åˆ†**:

$$
\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}
$$

- $n(q_i) \downarrow$ â†’ IDF $\uparrow$ ï¼ˆãƒ¬ã‚¢å˜èª â†’ é‡è¦ï¼‰
- $n(q_i) \uparrow$ â†’ IDF $\downarrow$ ï¼ˆé »å‡ºå˜èª â†’ é‡è¦åº¦ä½ï¼‰

#### 3.2.3 BM25ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

**$k_1$**: TFã®é£½å’Œåº¦ã‚’åˆ¶å¾¡

- $k_1 = 0$: TFã‚’ç„¡è¦–ï¼ˆIDF onlyï¼‰
- $k_1 \to \infty$: TFã®é£½å’Œãªã—ï¼ˆç”Ÿã®TFï¼‰
- æ¨å¥¨: $k_1 \in [1.2, 2.0]$

**$b$**: æ–‡æ›¸é•·æ­£è¦åŒ–ã®å¼·åº¦

- $b = 0$: æ­£è¦åŒ–ãªã—ï¼ˆçŸ­æ–‡æ›¸ã¨é•·æ–‡æ›¸ã‚’åŒç­‰ã«æ‰±ã†ï¼‰
- $b = 1$: å®Œå…¨æ­£è¦åŒ–ï¼ˆé•·æ–‡æ›¸ã‚’å³ã—ããƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
- æ¨å¥¨: $b \in [0.75, 0.85]$

#### 3.2.4 æ•°å€¤æ¤œè¨¼: BM25è¨ˆç®—

```julia
# BM25 calculation example
function bm25_score(query_terms::Vector{String}, doc_terms::Vector{String},
                    doc_freq::Dict{String, Int}, n_docs::Int, avg_doc_len::Float64,
                    k1::Float64=1.2, b::Float64=0.75)
    score = 0.0
    doc_len = length(doc_terms)

    for term in query_terms
        # TF: term frequency in document
        tf = count(==(term), doc_terms)

        # DF: number of documents containing term
        df = get(doc_freq, term, 0)

        # IDF
        idf = log((n_docs - df + 0.5) / (df + 0.5))

        # BM25 formula
        numerator = tf * (k1 + 1)
        denominator = tf + k1 * (1 - b + b * (doc_len / avg_doc_len))

        score += idf * (numerator / denominator)
    end

    return score
end

# Example
query = ["capital", "france"]
doc1 = ["paris", "is", "the", "capital", "of", "france"]
doc2 = ["london", "is", "the", "capital", "of", "england"]
doc_freq = Dict("capital" => 2, "france" => 1, "paris" => 1, "london" => 1, "england" => 1)
n_docs = 2
avg_doc_len = 6.0

score1 = bm25_score(query, doc1, doc_freq, n_docs, avg_doc_len)
score2 = bm25_score(query, doc2, doc_freq, n_docs, avg_doc_len)

println("BM25 Score (Doc1): $(round(score1, digits=3))")
println("BM25 Score (Doc2): $(round(score2, digits=3))")
```

### 3.3 Dense Retrieval â€” Neural Embeddingç©ºé–“ã§ã®æ¤œç´¢

#### 3.3.1 Bi-Encoder Architecture

**Bi-Encoder**: ã‚¯ã‚¨ãƒªã¨æ–‡æ›¸ã‚’ç‹¬ç«‹ã«Encode

$$
\begin{aligned}
\mathbf{q} &= f_Q(\text{Query}; \theta_Q) \quad \in \mathbb{R}^d \\
\mathbf{d} &= f_D(\text{Document}; \theta_D) \quad \in \mathbb{R}^d \\
\text{sim}(Q, D) &= \mathbf{q}^\top \mathbf{d} = \cos(\mathbf{q}, \mathbf{d}) \cdot \|\mathbf{q}\| \cdot \|\mathbf{d}\|
\end{aligned}
$$

é€šå¸¸ $\|\mathbf{q}\| = \|\mathbf{d}\| = 1$ ã«æ­£è¦åŒ– â†’ $\text{sim} = \cos(\mathbf{q}, \mathbf{d})$

**åˆ©ç‚¹**:
- æ–‡æ›¸ã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§Encodeå¯èƒ½ â†’ Vector DBã«ä¿å­˜
- ã‚¯ã‚¨ãƒªæ™‚ã¯ $\mathbf{q}$ ã®ã¿Encode â†’ é«˜é€Ÿ

**å­¦ç¿’**: In-batch Negatives (InfoNCE)

$$
\mathcal{L} = -\log \frac{\exp(\mathbf{q}^\top \mathbf{d}^+ / \tau)}{\exp(\mathbf{q}^\top \mathbf{d}^+ / \tau) + \sum_{i=1}^{B-1} \exp(\mathbf{q}^\top \mathbf{d}_i^- / \tau)}
$$

ã“ã“ã§:
- $\mathbf{d}^+$: positive document
- $\mathbf{d}_i^-$: negative documents (åŒä¸€ãƒãƒƒãƒå†…ã®ä»–ã®æ–‡æ›¸)
- $B$: ãƒãƒƒãƒã‚µã‚¤ã‚º

#### 3.3.2 Dense Passage Retrieval (DPR)

**DPR** (Karpukhin+ 2020):

$$
\text{sim}(q, d) = \mathbf{E}_Q(q)^\top \mathbf{E}_D(d)
$$

$\mathbf{E}_Q, \mathbf{E}_D$: BERT-based encoders

**Hard Negative Mining**:

ãƒ©ãƒ³ãƒ€ãƒ ãªnegativeã§ã¯ãªãã€**BM25ã§Top-kã ãŒGold labelã§ãªã„ã‚‚ã®**ã‚’negativeã¨ã—ã¦ä½¿ç”¨ â†’ å­¦ç¿’åŠ¹ç‡å‘ä¸Š

$$
\mathcal{L} = -\log \frac{\exp(\mathbf{q}^\top \mathbf{d}^+)}{\exp(\mathbf{q}^\top \mathbf{d}^+) + \sum_{d^- \in \text{HardNeg}} \exp(\mathbf{q}^\top \mathbf{d}^-)}
$$

#### 3.3.3 Approximate Nearest Neighbor (ANN) Search

**å•é¡Œ**: $N$ æ–‡æ›¸ã‹ã‚‰ Top-k ã‚’æ¢ã™ã®ã« $O(Nd)$ ã®è¨ˆç®— â†’ $N=10^9$ ã§éç¾å®Ÿçš„

**è§£æ±º**: Approximate Nearest Neighbor (ANN)

| æ‰‹æ³• | åŸç† | è¨ˆç®—é‡ | ç²¾åº¦ |
|:-----|:-----|:-------|:-----|
| **HNSW** | éšå±¤ã‚°ãƒ©ãƒ• | $O(\log N)$ | é«˜ |
| **IVF** | ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° | $O(\sqrt{N})$ | ä¸­ |
| **Product Quantization** | ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ– | $O(N/m)$ | ä½ |

**HNSW (Hierarchical Navigable Small World)**:

éšå±¤çš„ãªã‚°ãƒ©ãƒ•æ§‹é€ ã§è¿‘å‚æ¢ç´¢ã‚’é«˜é€ŸåŒ–ã€‚

$$
\begin{aligned}
&\text{Layer 0 (densest): å…¨ãƒãƒ¼ãƒ‰} \\
&\text{Layer 1: ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒ«} \\
&\text{Layer } L\text{: ç²—ã„ã‚°ãƒ©ãƒ•} \\
&\text{Search: Layer } L \to 0 \text{ ã«é™ã‚ŠãªãŒã‚‰è¿‘å‚æ¢ç´¢}
\end{aligned}
$$

**è¨ˆç®—é‡**: $O(\log N)$ (å¹³å‡)ã€ç²¾åº¦: 95-99%

### 3.4 Hybrid Retrieval â€” Sparse + Dense ã®çµ±åˆ

#### 3.4.1 Hybrid Search ã®å‹•æ©Ÿ

**BM25 (Sparse)ã®å¼·ã¿**:
- ãƒ¬ã‚¢å˜èªãƒ»å›ºæœ‰åè©ã«å¼·ã„
- å®Œå…¨ä¸€è‡´ã«å¼·ã„
- é«˜é€Ÿ

**Dense (Neural)ã®å¼·ã¿**:
- æ„å‘³çš„é¡ä¼¼æ€§ã«å¼·ã„
- è¨€ã„æ›ãˆãƒ»åŒç¾©èªã«å¼·ã„
- å¤šè¨€èªå¯¾å¿œ

**ä¸¡è€…ã¯ç›¸è£œçš„** â†’ çµ±åˆã™ã‚‹ã¨ç²¾åº¦å‘ä¸Š

#### 3.4.2 Reciprocal Rank Fusion (RRF)

**RRF** (Cormack+ 2009):

BM25ã¨Denseã®æ¤œç´¢çµæœã‚’çµ±åˆã€‚

$$
\text{RRF}(d) = \sum_{r \in \{r_{\text{BM25}}, r_{\text{Dense}}\}} \frac{1}{k + \text{rank}_r(d)}
$$

ã“ã“ã§:
- $\text{rank}_r(d)$: æ¤œç´¢æ‰‹æ³• $r$ ã«ãŠã‘ã‚‹æ–‡æ›¸ $d$ ã®ãƒ©ãƒ³ã‚¯
- $k$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸ $k=60$ï¼‰

**ç›´æ„Ÿ**: ä¸¡æ–¹ã§ä¸Šä½ã«ãƒ©ãƒ³ã‚¯ã•ã‚ŒãŸæ–‡æ›¸ãŒé«˜ã‚¹ã‚³ã‚¢

**ä¾‹**:

| Document | BM25 Rank | Dense Rank | RRF Score |
|:---------|:----------|:-----------|:----------|
| Doc A | 1 | 3 | $\frac{1}{60+1} + \frac{1}{60+3} = 0.032$ |
| Doc B | 2 | 1 | $\frac{1}{60+2} + \frac{1}{60+1} = 0.032$ |
| Doc C | 3 | 2 | $\frac{1}{60+3} + \frac{1}{60+2} = 0.032$ |

#### 3.4.3 Weighted Fusion

**Weighted Sum**:

$$
\text{Score}(d) = \alpha \cdot \text{Score}_{\text{BM25}}(d) + (1 - \alpha) \cdot \text{Score}_{\text{Dense}}(d)
$$

$\alpha$: BM25ã¨Denseã®é‡ã¿ï¼ˆé€šå¸¸ $\alpha \in [0.3, 0.7]$ï¼‰

**å•é¡Œ**: ã‚¹ã‚³ã‚¢ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒç•°ãªã‚‹ â†’ æ­£è¦åŒ–ãŒå¿…è¦

**Min-Maxæ­£è¦åŒ–**:

$$
\text{Score}_{\text{norm}}(d) = \frac{\text{Score}(d) - \min_i \text{Score}(d_i)}{\max_i \text{Score}(d_i) - \min_i \text{Score}(d_i)}
$$

### 3.5 Reranking â€” æ¤œç´¢çµæœã®ç²¾åº¦å‘ä¸Š

#### 3.5.1 Cross-Encoder

**Bi-Encoder vs Cross-Encoder**:

| | Bi-Encoder | Cross-Encoder |
|:--|:-----------|:--------------|
| **Input** | Query, Document ã‚’ç‹¬ç«‹ã«Encode | $[\text{CLS}] Q [\text{SEP}] D [\text{SEP}]$ ã‚’ä¸€ç·’ã«Encode |
| **Interaction** | ãªã—ï¼ˆãƒ‰ãƒƒãƒˆç©ã®ã¿ï¼‰ | ã‚ã‚Šï¼ˆAttentionå±¤ã§ç›¸äº’ä½œç”¨ï¼‰ |
| **ç²¾åº¦** | ä¸­ | é«˜ |
| **é€Ÿåº¦** | é€Ÿï¼ˆãƒ™ã‚¯ãƒˆãƒ«DBæ´»ç”¨ï¼‰ | é…ï¼ˆå„ãƒšã‚¢ã§æ¨è«–å¿…è¦ï¼‰ |

**Cross-Encoder Score**:

$$
\text{Score}(Q, D) = \sigma(\mathbf{W} \cdot \text{BERT}([Q; D])_{\text{[CLS]}})
$$

$\sigma$: sigmoid

**ä½¿ã„åˆ†ã‘**:
1. **Retrieval**: Bi-Encoder ã§ Top-100 ã‚’å–å¾—ï¼ˆé«˜é€Ÿï¼‰
2. **Reranking**: Cross-Encoder ã§ Top-100 ã‚’ Top-10 ã«çµã‚Šè¾¼ã¿ï¼ˆé«˜ç²¾åº¦ï¼‰

#### 3.5.2 ColBERT (Late Interaction)

**ColBERT** (Khattab & Zaharia 2020):

Bi-Encoderã®é€Ÿåº¦ + Cross-Encoderã®ç²¾åº¦ã‚’ä¸¡ç«‹ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

$$
\begin{aligned}
\mathbf{E}_Q &= \text{BERT}(Q) \quad \in \mathbb{R}^{|Q| \times d} \quad \text{(token-level embeddings)} \\
\mathbf{E}_D &= \text{BERT}(D) \quad \in \mathbb{R}^{|D| \times d} \\
\text{Score}(Q, D) &= \sum_{i=1}^{|Q|} \max_{j=1}^{|D|} \mathbf{E}_Q[i] \cdot \mathbf{E}_D[j]^\top
\end{aligned}
$$

**MaxSim**: å„ã‚¯ã‚¨ãƒªãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã€æœ€ã‚‚é¡ä¼¼ã™ã‚‹æ–‡æ›¸ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã¤ã‘ã¦ã‚¹ã‚³ã‚¢åŒ–

**åˆ©ç‚¹**:
- æ–‡æ›¸ã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§Encodeå¯èƒ½ï¼ˆBi-EncoderåŒæ§˜ï¼‰
- Token-levelã®ç›¸äº’ä½œç”¨ï¼ˆCross-Encoderçš„ï¼‰
- é€Ÿåº¦: Bi-Encoderã®2-3å€é…ã„ãŒã€Cross-Encoderã®10å€é€Ÿ

### 3.6 Agentic RAG â€” è‡ªå¾‹çš„æ¤œç´¢åˆ¶å¾¡

#### 3.6.1 Self-RAG (Self-Reflective RAG)

**Self-RAG** (Asai+ 2024) [^2]:

LLMãŒ**åçœãƒˆãƒ¼ã‚¯ãƒ³**ã‚’ç”Ÿæˆã—ã€æ¤œç´¢ãƒ»ç”Ÿæˆã‚’è‡ªå·±åˆ¶å¾¡ã€‚

**åçœãƒˆãƒ¼ã‚¯ãƒ³ã®ç¨®é¡**:

| ãƒˆãƒ¼ã‚¯ãƒ³ | æ„å‘³ | ä¾‹ |
|:--------|:-----|:---|
| **[Retrieval]** | æ¤œç´¢ãŒå¿…è¦ã‹ | Yes/No |
| **[IsRel]** | æ¤œç´¢çµæœãŒé–¢é€£ã—ã¦ã„ã‚‹ã‹ | Relevant/Irrelevant |
| **[IsSup]** | ç”ŸæˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ”¯æŒã•ã‚Œã¦ã„ã‚‹ã‹ | Fully/Partially/No |
| **[IsUse]** | ç”ŸæˆãŒã‚¯ã‚¨ãƒªã«æœ‰ç”¨ã‹ | 5/4/3/2/1 |

**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**:

```
1. Query â†’ LLM generates [Retrieval] token
2. If [Retrieval]=Yes â†’ Retrieve documents
3. LLM generates answer + [IsRel], [IsSup], [IsUse] tokens
4. If [IsSup]=No â†’ Re-retrieve or generate from memory
5. Return best answer based on reflection scores
```

**å­¦ç¿’**:

$$
\mathcal{L} = \mathcal{L}_{\text{LM}} + \lambda \mathcal{L}_{\text{Reflection}}
$$

åçœãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å­¦ç¿’ã€‚

#### 3.6.2 CRAG (Corrective RAG)

**CRAG** (Yan+ 2024) [^3]:

æ¤œç´¢çµæœã®**æ­£ç¢ºæ€§ã‚’è©•ä¾¡**ã—ã€ä¸æ­£ç¢ºãªã‚‰è£œæ­£ã€‚

**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**:

```
1. Query â†’ Retrieve top-k documents
2. Evaluator: Score each document â†’ {Correct, Ambiguous, Incorrect}
3. If all Correct â†’ Generate
4. If some Ambiguous â†’ Re-retrieve with query refinement
5. If Incorrect â†’ Use web search to augment knowledge
6. Generate answer from corrected context
```

**Evaluator**:

è»½é‡LM (T5-baseç­‰) ã§æ–‡æ›¸ã®æ­£ç¢ºæ€§ã‚’ã‚¹ã‚³ã‚¢åŒ–:

$$
p_{\text{correct}} = \sigma(\mathbf{W} \cdot \text{Encoder}(Q, D))
$$

**Knowledge Refinement**:

ä¸æ­£ç¢ºãªæ–‡æ›¸ã‹ã‚‰é–¢é€£éƒ¨åˆ†ã®ã¿æŠ½å‡ºï¼ˆæ–‡å˜ä½ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰ã€‚

#### 3.6.3 Adaptive-RAG

**Adaptive-RAG** (Jeong+ 2024):

ã‚¯ã‚¨ãƒªã®**è¤‡é›‘åº¦ã«å¿œã˜ã¦æ¤œç´¢æˆ¦ç•¥ã‚’å‹•çš„é¸æŠ**ã€‚

**æˆ¦ç•¥**:

| ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ— | æˆ¦ç•¥ | ä¾‹ |
|:-----------|:-----|:---|
| **Simple** | LLMã®ã¿ï¼ˆæ¤œç´¢ä¸è¦ï¼‰ | "What is 2+2?" |
| **Single-hop** | 1å›æ¤œç´¢ | "What is the capital of France?" |
| **Multi-hop** | åå¾©æ¤œç´¢ | "Who is the spouse of the director of Inception?" |

**Complexity Classifier**:

$$
p_{\text{complexity}} = \text{Classifier}(Q) \quad \in \{\text{Simple, Single, Multi}\}
$$

**Multi-hop Reasoning**:

```
1. Query â†’ Classify as Multi-hop
2. Retrieve documents for sub-query 1
3. Extract intermediate answer
4. Generate sub-query 2 using intermediate answer
5. Retrieve documents for sub-query 2
6. Generate final answer
```

:::message alert
**ãƒœã‚¹æˆ¦: RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œå…¨å®Ÿè£…**

ä»¥ä¸‹ã®RAGã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã›ã‚ˆ:

1. **Embedding**: Sentence-BERTã§æ–‡æ›¸ã‚’Embedding
2. **Vector DB**: HNSW indexã§Top-kæ¤œç´¢
3. **Hybrid Retrieval**: BM25ã¨Dense retrieval ã‚’RRFã§çµ±åˆ
4. **Reranking**: Cross-Encoderã§å†é †ä½ä»˜ã‘
5. **Agentic RAG**: Self-RAGã§åçœãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
6. **è©•ä¾¡**: RAGAS metricsã§è©•ä¾¡ï¼ˆFaithfulness, Context Relevanceï¼‰

**ã‚¿ã‚¹ã‚¯**:
- å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’Rust/Julia/Elixirã§å®Ÿè£…
- 1,000æ–‡æ›¸ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã§æ¤œç´¢ç²¾åº¦ã‚’æ¸¬å®š
- Latency/Throughputã‚’æœ€é©åŒ–

ã“ã‚ŒãŒã§ãã‚Œã°æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œå…¨ã‚¯ãƒªã‚¢ï¼
:::

:::message
**é€²æ—: 50% å®Œäº†** RAGç†è«–ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚Embedding/BM25/Dense/Hybrid/Reranking/Agentic RAGã‚’æ•°å¼ã‹ã‚‰å°å‡ºã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã§Rust/Julia/Elixirã§å…¨æ‰‹æ³•ã‚’å®Ÿè£…ã™ã‚‹ã€‚
:::

---
