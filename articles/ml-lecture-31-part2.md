---
title: "ç¬¬31å›: MLOpså®Œå…¨ç‰ˆã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨: Julia/Rust/Elixirå®Ÿè£…â†’ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning", "mlops", "rust", "julia", "elixir"]
published: true
slug: "ml-lecture-31-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---
> **ğŸ“– å‰ç·¨ï¼ˆç†è«–ç·¨ï¼‰**: [ç¬¬31å›å‰ç·¨: MLOpsç†è«–ç·¨](./ml-lecture-31-part1) | **â† ç†è«–ãƒ»æ•°å¼ã‚¾ãƒ¼ãƒ³ã¸**

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” âš¡Juliaå®Ÿé¨“ç®¡ç† + ğŸ¦€Rust MLOpsãƒ„ãƒ¼ãƒ« + ğŸ”®Elixirç›£è¦–

### Part F: å®Ÿè£…ç·¨

#### 4.1 âš¡ Juliaå®Ÿé¨“ç®¡ç† â€” MLflowçµ±åˆ

Juliaã§å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹ã€‚`MLFlowClient.jl`ã‚’ä½¿ã£ã¦MLflow APIã¨é€šä¿¡ã€‚

```julia
using HTTP, JSON3, Dates

# MLflow tracking server URL
const MLFLOW_URI = "http://localhost:5000"

"""
Log parameters to MLflow
"""
function log_params(run_id::String, params::Dict{String, Any})
    url = "$MLFLOW_URI/api/2.0/mlflow/runs/log-parameter"

    for (key, value) in params
        body = JSON3.write(Dict(
            "run_id" => run_id,
            "key" => key,
            "value" => string(value)
        ))

        HTTP.post(url, ["Content-Type" => "application/json"], body)
    end
end

"""
Log metrics to MLflow with step
"""
function log_metrics(run_id::String, metrics::Dict{String, Float64}, step::Int)
    url = "$MLFLOW_URI/api/2.0/mlflow/runs/log-metric"

    for (key, value) in metrics
        body = JSON3.write(Dict(
            "run_id" => run_id,
            "key" => key,
            "value" => value,
            "timestamp" => round(Int, datetime2unix(now()) * 1000),
            "step" => step
        ))

        HTTP.post(url, ["Content-Type" => "application/json"], body)
    end
end

"""
Create MLflow run
"""
function create_run(experiment_id::String, run_name::String)
    url = "$MLFLOW_URI/api/2.0/mlflow/runs/create"

    body = JSON3.write(Dict(
        "experiment_id" => experiment_id,
        "run_name" => run_name,
        "start_time" => round(Int, datetime2unix(now()) * 1000)
    ))

    response = HTTP.post(url, ["Content-Type" => "application/json"], body)
    result = JSON3.read(String(response.body))

    return result["run"]["info"]["run_id"]
end

"""
Complete MLflow run
"""
function end_run(run_id::String, status::String="FINISHED")
    url = "$MLFLOW_URI/api/2.0/mlflow/runs/update"

    body = JSON3.write(Dict(
        "run_id" => run_id,
        "status" => status,
        "end_time" => round(Int, datetime2unix(now()) * 1000)
    ))

    HTTP.post(url, ["Content-Type" => "application/json"], body)
end

# Example: Track a training run
function train_and_log()
    # Create run
    experiment_id = "0"  # Default experiment
    run_id = create_run(experiment_id, "julia-training-run")

    # Log hyperparameters
    params = Dict(
        "learning_rate" => 0.001,
        "batch_size" => 32,
        "epochs" => 10,
        "optimizer" => "Adam"
    )
    log_params(run_id, params)

    # Simulate training loop
    for epoch in 1:10
        train_loss = 1.0 / (1 + epoch * 0.1)  # Decreasing loss
        val_acc = 0.8 + epoch * 0.02  # Increasing accuracy

        # Log metrics with step
        metrics = Dict(
            "train_loss" => train_loss,
            "val_acc" => val_acc
        )
        log_metrics(run_id, metrics, epoch)

        println("Epoch $epoch: loss=$train_loss, acc=$val_acc")
    end

    # End run
    end_run(run_id)
    println("âœ… Run completed: $run_id")

    return run_id
end

# Run experiment
run_id = train_and_log()
```

å‡ºåŠ›:
```
Epoch 1: loss=0.9090909090909091, acc=0.82
Epoch 2: loss=0.8333333333333334, acc=0.84
...
Epoch 10: loss=0.5, acc=1.0
âœ… Run completed: a3f9c2e1b4d87f3a9c2e1b4d87f3a9c2
```

**MLflow UI** (`mlflow ui`) ã§å¯è¦–åŒ–:

- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒ
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•
- Runé–“ã®æ¯”è¼ƒ

**Juliaã®åˆ©ç‚¹**:

- è¨“ç·´ãƒ«ãƒ¼ãƒ—ãŒé«˜é€Ÿ (C/Fortranãƒ¬ãƒ™ãƒ«)
- MLflow APIã¯å˜ãªã‚‹HTTP POST (è¨€èªéä¾å­˜)
- å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§å‹ã«å¿œã˜ãŸæœ€é©åŒ–

#### 4.2 ğŸ¦€ Rust MLOpsãƒ„ãƒ¼ãƒ« â€” ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° & ãƒ¡ãƒˆãƒªã‚¯ã‚¹

Rustã§é«˜é€ŸãªMLOpsãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’æ§‹ç¯‰ã€‚

##### 4.2.1 ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®— (SHA-256)

```rust
use sha2::{Sha256, Digest};
use std::fs::File;
use std::io::Read;

/// Calculate SHA-256 hash of model file
pub fn hash_model_file(path: &str) -> Result<String, std::io::Error> {
    let mut file = File::open(path)?;
    let mut hasher = Sha256::new();
    let mut buffer = [0u8; 8192];

    loop {
        let n = file.read(&mut buffer)?;
        if n == 0 { break; }
        hasher.update(&buffer[..n]);
    }

    let result = hasher.finalize();
    Ok(format!("{:x}", result))
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_hash_model_file() {
        // Create a test file
        std::fs::write("test_model.bin", b"dummy model weights").unwrap();

        let hash = hash_model_file("test_model.bin").unwrap();
        assert_eq!(hash.len(), 64);  // SHA-256 = 256 bits = 64 hex chars

        std::fs::remove_file("test_model.bin").unwrap();
    }
}
```

##### 4.2.2 Prometheus Exporter (æ¨è«–ãƒ¡ãƒˆãƒªã‚¯ã‚¹)

```rust
use prometheus::{
    Encoder, TextEncoder, Counter, Histogram, Registry,
    opts, register_counter_with_registry, register_histogram_with_registry,
};
use std::time::Instant;

pub struct ModelMetrics {
    pub registry: Registry,
    pub request_count: Counter,
    pub error_count: Counter,
    pub latency: Histogram,
}

impl ModelMetrics {
    pub fn new() -> Self {
        let registry = Registry::new();

        let request_count = register_counter_with_registry!(
            opts!("model_requests_total", "Total inference requests"),
            registry
        ).unwrap();

        let error_count = register_counter_with_registry!(
            opts!("model_errors_total", "Total inference errors"),
            registry
        ).unwrap();

        let latency = register_histogram_with_registry!(
            "model_latency_seconds",
            "Inference latency in seconds",
            vec![0.01, 0.05, 0.1, 0.5, 1.0],
            registry
        ).unwrap();

        Self {
            registry,
            request_count,
            error_count,
            latency,
        }
    }

    pub fn record_request<F, T>(&self, f: F) -> Result<T, Box<dyn std::error::Error>>
    where
        F: FnOnce() -> Result<T, Box<dyn std::error::Error>>,
    {
        self.request_count.inc();
        let start = Instant::now();

        let result = f();

        let elapsed = start.elapsed().as_secs_f64();
        self.latency.observe(elapsed);

        if result.is_err() {
            self.error_count.inc();
        }

        result
    }

    pub fn export_metrics(&self) -> String {
        let encoder = TextEncoder::new();
        let metric_families = self.registry.gather();
        let mut buffer = Vec::new();
        encoder.encode(&metric_families, &mut buffer).unwrap();
        String::from_utf8(buffer).unwrap()
    }
}

// Example usage
fn main() {
    let metrics = ModelMetrics::new();

    // Simulate inference requests
    for _ in 0..100 {
        let result = metrics.record_request(|| {
            // Simulate model inference
            std::thread::sleep(std::time::Duration::from_millis(50));
            Ok(())
        });

        if let Err(e) = result {
            eprintln!("Error: {}", e);
        }
    }

    // Export metrics in Prometheus format
    println!("{}", metrics.export_metrics());
}
```

å‡ºåŠ› (Prometheus format):
```
# HELP model_requests_total Total inference requests
# TYPE model_requests_total counter
model_requests_total 100

# HELP model_errors_total Total inference errors
# TYPE model_errors_total counter
model_errors_total 0

# HELP model_latency_seconds Inference latency in seconds
# TYPE model_latency_seconds histogram
model_latency_seconds_bucket{le="0.01"} 0
model_latency_seconds_bucket{le="0.05"} 100
model_latency_seconds_bucket{le="0.1"} 100
model_latency_seconds_bucket{le="0.5"} 100
model_latency_seconds_bucket{le="1"} 100
model_latency_seconds_bucket{le="+Inf"} 100
model_latency_seconds_sum 5.0
model_latency_seconds_count 100
```

**Prometheusã‚µãƒ¼ãƒãƒ¼ãŒã“ã‚Œã‚’scrapeã—ã¦æ™‚ç³»åˆ—DBã«ä¿å­˜ã€‚**

#### 4.3 ğŸ”® Elixirç›£è¦–ã‚·ã‚¹ãƒ†ãƒ  â€” Telemetryçµ±åˆ & ã‚¢ãƒ©ãƒ¼ãƒˆ

Elixirã§åˆ†æ•£ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã€‚`:telemetry`ã§ã‚¤ãƒ™ãƒ³ãƒˆã‚’åé›†ã—ã€`:gen_statem`ã§ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ã€‚

##### 4.3.1 Telemetryçµ±åˆ

```elixir
defmodule MLOps.Telemetry do
  require Logger

  @doc """
  Attach telemetry handlers
  """
  def setup do
    :telemetry.attach_many(
      "mlops-telemetry",
      [
        [:model, :predict, :start],
        [:model, :predict, :stop],
        [:model, :predict, :exception]
      ],
      &handle_event/4,
      nil
    )
  end

  defp handle_event([:model, :predict, :start], _measurements, metadata, _config) do
    Logger.debug("Prediction started: #{inspect(metadata)}")
  end

  defp handle_event([:model, :predict, :stop], measurements, metadata, _config) do
    latency_ms = System.convert_time_unit(measurements.duration, :native, :millisecond)
    Logger.info("Prediction completed in #{latency_ms}ms: #{inspect(metadata)}")

    # Send to Prometheus
    :prometheus_histogram.observe(:model_latency_milliseconds, latency_ms)
  end

  defp handle_event([:model, :predict, :exception], measurements, metadata, _config) do
    Logger.error("Prediction failed: #{inspect(metadata)}")
    :prometheus_counter.inc(:model_errors_total)
  end
end

defmodule MLOps.Model do
  @doc """
  Run model prediction with telemetry
  """
  def predict(input) do
    metadata = %{model: "v1", input_size: byte_size(input)}

    :telemetry.span([:model, :predict], metadata, fn ->
      result = do_predict(input)
      {result, metadata}
    end)
  end

  defp do_predict(input) do
    # Simulate model inference
    Process.sleep(50)
    {:ok, "prediction for #{input}"}
  end
end

# Usage
MLOps.Telemetry.setup()

1..100 |> Enum.each(fn i -> MLOps.Model.predict("input_#{i}") end)
```

##### 4.3.2 SLOç›£è¦– & è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆ

```elixir
defmodule MLOps.SLOMonitor do
  use GenServer
  require Logger

  @slo_latency_ms 100
  @slo_availability 0.999

  def start_link(_) do
    GenServer.start_link(__MODULE__, %{}, name: __MODULE__)
  end

  def init(_) do
    # Check SLO every minute
    :timer.send_interval(60_000, :check_slo)

    state = %{
      total_requests: 0,
      successful_requests: 0,
      latencies: []
    }

    {:ok, state}
  end

  def record_request(latency_ms, success) do
    GenServer.cast(__MODULE__, {:record, latency_ms, success})
  end

  def handle_cast({:record, latency_ms, success}, state) do
    new_state = %{
      total_requests: state.total_requests + 1,
      successful_requests: state.successful_requests + (if success, do: 1, else: 0),
      latencies: [latency_ms | Enum.take(state.latencies, 999)]  # Keep last 1000
    }

    {:noreply, new_state}
  end

  def handle_info(:check_slo, state) do
    availability = state.successful_requests / max(state.total_requests, 1)
    p99_latency = if length(state.latencies) > 0 do
      Enum.sort(state.latencies) |> Enum.at(round(length(state.latencies) * 0.99))
    else
      0
    end

    Logger.info("SLO Check: availability=#{Float.round(availability, 4)}, p99_latency=#{p99_latency}ms")

    cond do
      availability < @slo_availability ->
        send_alert("SLO violated: availability #{Float.round(availability * 100, 2)}% < #{@slo_availability * 100}%")

      p99_latency > @slo_latency_ms ->
        send_alert("SLO violated: p99 latency #{p99_latency}ms > #{@slo_latency_ms}ms")

      true ->
        Logger.info("âœ… SLO met")
    end

    {:noreply, state}
  end

  defp send_alert(message) do
    Logger.warn("ğŸš¨ ALERT: #{message}")
    # In production: send to PagerDuty/Slack/etc
  end
end

# Usage
{:ok, _} = MLOps.SLOMonitor.start_link([])

# Simulate requests
Enum.each(1..1000, fn _ ->
  latency = :rand.uniform(150)
  success = latency < 120
  MLOps.SLOMonitor.record_request(latency, success)
  Process.sleep(10)
end)
```

å‡ºåŠ› (1åˆ†ã”ã¨):
```
[info] SLO Check: availability=0.9820, p99_latency=148ms
[warn] ğŸš¨ ALERT: SLO violated: p99 latency 148ms > 100ms
```

**Elixirã®åˆ©ç‚¹**:

- OTP supervisorã§éšœå®³æ™‚è‡ªå‹•å†èµ·å‹•
- åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã§ãƒãƒ¼ãƒ‰é–“ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†ç´„
- Telemetryã§å…¨ã¦ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’çµ±ä¸€çš„ã«è¨˜éŒ²

##### 4.3.3 åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚° â€” OpenTelemetryçµ±åˆ

Elixirã§åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’å®Ÿè£…ã—ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å…¨çµŒè·¯ã‚’å¯è¦–åŒ–ã€‚

```elixir
defmodule MLOps.Tracer do
  require OpenTelemetry.Tracer, as: Tracer

  @doc """
  Trace model prediction with OpenTelemetry
  """
  def traced_predict(input) do
    Tracer.with_span "model.predict" do
      # Add span attributes
      Tracer.set_attributes([
        {"input.size", byte_size(input)},
        {"model.version", "v1"}
      ])

      # Child span: preprocessing
      result = Tracer.with_span "preprocessing" do
        preprocess(input)
      end

      # Child span: inference
      prediction = Tracer.with_span "inference" do
        do_inference(result)
      end

      # Child span: postprocessing
      Tracer.with_span "postprocessing" do
        postprocess(prediction)
      end
    end
  end

  defp preprocess(input), do: String.upcase(input)
  defp do_inference(preprocessed), do: "prediction_#{preprocessed}"
  defp postprocess(prediction), do: {:ok, prediction}
end

# Usage with trace propagation across services
MLOps.Tracer.traced_predict("test_input")
```

**OpenTelemetry Collector**ã§å…¨ã¦ã®traceã‚’åé›†ã—ã€Jaeger/Zipkinã§å¯è¦–åŒ–:

```
Span: model.predict [12.5ms]
â”œâ”€ Span: preprocessing [1.2ms]
â”œâ”€ Span: inference [10.0ms]
â””â”€ Span: postprocessing [1.3ms]
```

**åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã©ã“ã§é…å»¶ã—ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã§ãã‚‹ã€‚**

#### 4.4 3è¨€èªæ¯”è¼ƒ â€” âš¡Julia vs ğŸ¦€Rust vs ğŸ”®Elixir

| è¦³ç‚¹ | âš¡Julia | ğŸ¦€Rust | ğŸ”®Elixir |
|:-----|:-------|:-------|:---------|
| **å½¹å‰²** | å®Ÿé¨“ç®¡ç†ãƒ»è¨“ç·´ãƒ«ãƒ¼ãƒ— | ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ãƒ»æ¨è«–æœ€é©åŒ– | ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ  |
| **é€Ÿåº¦** | â­â­â­â­ (JIT) | â­â­â­â­â­ (AOT) | â­â­â­ (BEAM VM) |
| **ä¸¦è¡Œæ€§** | `Threads.@threads` | Tokio async | Actor model (OTP) |
| **å‹å®‰å…¨** | å‹•çš„å‹ (opt-iné™çš„) | é™çš„å‹ (å³æ ¼) | å‹•çš„å‹ |
| **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ** | Lux.jl, MLJ.jl | `prometheus`, `tonic` | Phoenix, Ecto, Telemetry |
| **å­¦ç¿’æ›²ç·š** | ä¸­ (Pythonã‹ã‚‰å®¹æ˜“) | é«˜ (æ‰€æœ‰æ¨©å­¦ç¿’) | ä¸­ (é–¢æ•°å‹+OTP) |
| **é©ç”¨ä¾‹** | MLflowçµ±åˆ, ãƒã‚¤ãƒ‘ãƒ©ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | Prometheus exporter, é«˜é€Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®— | SLOç›£è¦–, åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚° |

**çµ„ã¿åˆã‚ã›ã®å¨åŠ›**:

- âš¡Julia: å®Ÿé¨“ç®¡ç†ãƒ»è¨“ç·´ (é«˜é€Ÿ+æ•°å¼ç¾)
- ğŸ¦€Rust: ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ãƒ»æ¨è«–ã‚µãƒ¼ãƒãƒ¼ (ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–)
- ğŸ”®Elixir: ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ  (OTP fault-tolerance)

**1ã¤ã®è¨€èªã§ã¯è¶³ã‚Šãªã„ã€‚é©æé©æ‰€ã§3è¨€èªã‚’ä½¿ã„åˆ†ã‘ã‚‹ã€‚**

---

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Julia + MLflowã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†ã§ã€`log_metric` ã¨ `log_param` ã‚’ä½¿ã„åˆ†ã‘ã‚‹è¨­è¨ˆåŸå‰‡ã¨ã€Artifactç®¡ç†ã«ã‚ˆã‚‹å†ç¾æ€§ä¿è¨¼ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. PSIï¼ˆPopulation Stability Indexï¼‰ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã«ãŠã„ã¦ã€é–¾å€¤ï¼ˆPSI > 0.2 = Significant Shiftï¼‰ã®çµ±è¨ˆçš„æ ¹æ‹ ã¨ã€KSæ¤œå®šã¨ã®ä½¿ã„åˆ†ã‘ã‚’èª¬æ˜ã›ã‚ˆã€‚

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ & ãƒŸãƒ‹PJ

### 5.1 MLOpsçŸ¥è­˜ãƒã‚§ãƒƒã‚¯ (10å•)

<details><summary>å•é¡Œ1: ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã®5-tuple</summary>

ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ $\mathcal{M}_t$ ã‚’æ§‹æˆã™ã‚‹5ã¤ã®è¦ç´ ã¯ï¼Ÿ

**ç­”ãˆ**: $(\mathbf{w}_t, \mathcal{D}_t, \mathcal{H}_t, \mathcal{E}_t, s_t)$

- $\mathbf{w}_t$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ«
- $\mathcal{D}_t$: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- $\mathcal{H}_t$: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $\mathcal{E}_t$: ç’°å¢ƒ (Python/CUDA version)
- $s_t$: Random seed

**å†ç¾æ€§ = 5ã¤å…¨ã¦ä¸€è‡´**

</details>

<details><summary>å•é¡Œ2: Error Budgetã®è¨ˆç®—</summary>

SLO = 99.9% (uptime) ã®å ´åˆã€30æ—¥é–“ã®Error Budgetã¯ä½•åˆ†ï¼Ÿ

**ç­”ãˆ**:

$$
\text{Error Budget} = (1 - 0.999) \times 30 \times 24 \times 60 = 43.2 \text{ minutes}
$$

**æœˆã«43.2åˆ†ã¾ã§ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ OKã€‚è¶…ãˆãŸã‚‰æ–°æ©Ÿèƒ½é–‹ç™ºåœæ­¢ã€‚**

</details>

<details><summary>å•é¡Œ3: A/Bãƒ†ã‚¹ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º</summary>

$p_A = 0.10$, MDE = 0.02, $\alpha=0.05$, power = 0.8 ã®å ´åˆã€å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã¯ï¼Ÿ

**ç­”ãˆ**:

$$
n = \frac{(1.96 + 0.84)^2 \cdot 2 \cdot 0.10 \cdot 0.90}{0.02^2} \approx 3528 \text{ per group}
$$

**åˆè¨ˆ 7,056 ãƒ¦ãƒ¼ã‚¶ãƒ¼å¿…è¦ã€‚**

</details>

<details><summary>å•é¡Œ4: KSæ¤œå®šã®på€¤è§£é‡ˆ</summary>

KSæ¤œå®šã§ $p = 0.001$ ãŒå¾—ã‚‰ã‚ŒãŸã€‚æœ‰æ„æ°´æº– $\alpha=0.01$ ã§å¸°ç„¡ä»®èª¬ã‚’æ£„å´ã§ãã‚‹ã‹ï¼Ÿ

**ç­”ãˆ**: **Yes**

$$
p = 0.001 < \alpha = 0.01 \Rightarrow \text{Reject } H_0
$$

**ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡º â†’ å†è¨“ç·´ã‚’ãƒˆãƒªã‚¬ãƒ¼**

</details>

<details><summary>å•é¡Œ5: PSIã®é–¾å€¤</summary>

PSI = 0.18 ãŒå¾—ã‚‰ã‚ŒãŸã€‚å†è¨“ç·´ã¯å¿…è¦ã‹ï¼Ÿ

**ç­”ãˆ**: **è»½å¾®ãªãƒ‰ãƒªãƒ•ãƒˆã€ç›£è¦–ç¶™ç¶š**

| PSI | è§£é‡ˆ |
|:----|:-----|
| < 0.1 | ãƒ‰ãƒªãƒ•ãƒˆãªã— |
| 0.1 - 0.25 | è»½å¾®ãªãƒ‰ãƒªãƒ•ãƒˆ (ç›£è¦–) |
| > 0.25 | é‡å¤§ãªãƒ‰ãƒªãƒ•ãƒˆ (å†è¨“ç·´) |

**0.18ã¯ç›£è¦–ç¶™ç¶šã‚¾ãƒ¼ãƒ³ã€‚**

</details>

<details><summary>å•é¡Œ6: DPO lossã®å¼</summary>

DPO lossã‚’æ›¸ã‘ã€‚

**ç­”ãˆ**:

$$
\mathcal{L}_{\text{DPO}} = -\mathbb{E} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
$$

**Bradley-Terry Model + KLæ­£å‰‡åŒ–ã®é–‰å½¢å¼è§£ã€‚**

</details>

<details><summary>å•é¡Œ7: Canary Deploymentã®æ®µéš</summary>

1% â†’ 5% â†’ ? â†’ 100% ã® ? ã¯ä½•%ï¼Ÿ

**ç­”ãˆ**: **25%**

æ¨™æº–çš„ãªã‚«ãƒŠãƒªã‚¢ãƒªãƒªãƒ¼ã‚¹: 1% â†’ 5% â†’ 25% â†’ 100%

**å„ã‚¹ãƒ†ãƒ¼ã‚¸ã§ã‚¨ãƒ©ãƒ¼ç‡ã‚’ç›£è¦–ã€‚ç•°å¸¸ãªã‚‰å³ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚**

</details>

<details><summary>å•é¡Œ8: RED Metricsã®3è¦ç´ </summary>

REDã®3è¦ç´ ã¯ï¼Ÿ

**ç­”ãˆ**:

- **Rate**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°/ç§’
- **Errors**: ã‚¨ãƒ©ãƒ¼æ•°/ç§’
- **Duration**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (p50/p95/p99)

**å…¨ã¦ã®ã‚µãƒ¼ãƒ“ã‚¹ã§æœ€ä½é™ç›£è¦–ã™ã¹ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‚**

</details>

<details><summary>å•é¡Œ9: Reward Modelingã®æå¤±é–¢æ•°</summary>

Bradley-Terry Modelã®æå¤±é–¢æ•°ã‚’æ›¸ã‘ã€‚

**ç­”ãˆ**:

$$
\mathcal{L}_{\text{RM}} = -\mathbb{E} \left[ \log \sigma(r(x, y_w) - r(x, y_l)) \right]
$$

**å¥½ã¾ã—ã„å¿œç­” $y_w$ ã®rewardã‚’ä¸Šã’ã€å¥½ã¾ã—ããªã„å¿œç­” $y_l$ ã®rewardã‚’ä¸‹ã’ã‚‹ã€‚**

</details>

<details><summary>å•é¡Œ10: Git LFSã¨DVCã®é•ã„</summary>

Git LFSã¨DVCã®ä¸»ãªé•ã„ã¯ï¼Ÿ

**ç­”ãˆ**:

| è¦³ç‚¹ | Git LFS | DVC |
|:-----|:--------|:----|
| **ç”¨é€”** | ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (ãƒã‚¤ãƒŠãƒª) | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (CSV/ç”»åƒ) |
| **ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰** | GitHub LFS / S3 | S3/GCS/Azure/SSH |
| **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** | âŒãªã— | âœ…ã‚ã‚Š (dvc.yaml) |
| **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿** | `.gitattributes` | `.dvc` ãƒ•ã‚¡ã‚¤ãƒ« |

**DVC = ãƒ‡ãƒ¼ã‚¿ç‰ˆGit + ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†ã€‚**

</details>

### 5.2 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1: ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ã‚·ã‚¹ãƒ†ãƒ 

**ç›®æ¨™**: âš¡Juliaã§è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’MLflowã«è¨˜éŒ²ã€‚

```julia
using HTTP, JSON3

# (4.1ã®MLflowé–¢æ•°ã‚’ä½¿ç”¨)

function train_tiny_model(lr::Float64, epochs::Int)
    experiment_id = "0"
    run_id = create_run(experiment_id, "tiny-model-lr-$lr")

    # Log hyperparameters
    params = Dict("lr" => lr, "epochs" => epochs)
    log_params(run_id, params)

    # Training loop
    for epoch in 1:epochs
        # Simulate training
        train_loss = 1.0 / (1 + epoch * lr)
        val_acc = 0.7 + epoch * 0.03

        # Log metrics
        metrics = Dict("train_loss" => train_loss, "val_acc" => val_acc)
        log_metrics(run_id, metrics, epoch)
    end

    end_run(run_id)
    return run_id
end

# Run hyperparameter sweep
for lr in [0.001, 0.01, 0.1]
    run_id = train_tiny_model(lr, 10)
    println("Completed run: $run_id with lr=$lr")
end
```

**MLflow UI** ã§3ã¤ã®runã‚’æ¯”è¼ƒ:

| Run | lr | Final val_acc | Winner |
|:----|:---|:--------------|:-------|
| 1 | 0.001 | 0.985 | âŒ |
| 2 | 0.01 | 0.994 | âœ… |
| 3 | 0.1 | 0.976 | âŒ |

**lr=0.01ãŒæœ€è‰¯ã€‚ã“ã®runã‚’Model Registryã«ç™»éŒ²ã€‚**

### 5.3 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2: ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º

**ç›®æ¨™**: ğŸ¦€Rustã§è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã®KSæ¤œå®šã€‚

```rust
use statrs::distribution::{ContinuousCDF, Normal};
use statrs::statistics::OrderStatistics;

/// Kolmogorov-Smirnov test
pub fn ks_test(sample1: &[f64], sample2: &[f64]) -> (f64, f64) {
    let n1 = sample1.len() as f64;
    let n2 = sample2.len() as f64;

    let mut sorted1 = sample1.to_vec();
    let mut sorted2 = sample2.to_vec();
    sorted1.sort_unstable_by(f64::total_cmp);
    sorted2.sort_unstable_by(f64::total_cmp);

    // Merge and calculate CDFs
    let mut all_values: Vec<f64> = sorted1.iter().chain(sorted2.iter()).copied().collect();
    all_values.sort_unstable_by(f64::total_cmp);
    all_values.dedup();

    let mut max_diff = 0.0_f64;

    for &value in &all_values {
        let cdf1 = sorted1.iter().filter(|&&x| x <= value).count() as f64 / n1;
        let cdf2 = sorted2.iter().filter(|&&x| x <= value).count() as f64 / n2;

        let diff = (cdf1 - cdf2).abs();
        max_diff = max_diff.max(diff);
    }

    // Compute p-value (approximation)
    let n_eff = (n1 * n2) / (n1 + n2);
    let lambda = (n_eff.sqrt() + 0.12 + 0.11 / n_eff.sqrt()) * max_diff;

    // Kolmogorov distribution approximation
    let p_value = if lambda < 0.1 {
        1.0
    } else {
        2.0 * (-2.0 * lambda * lambda).exp()
    };

    (max_diff, p_value)
}

fn main() {
    // Training data: N(0, 1)
    let train: Vec<f64> = (0..1000).map(|_| rand::random::<f64>()).collect();

    // Production data: N(0.5, 1.2) â€” shifted mean and variance
    let prod: Vec<f64> = (0..1000).map(|_| rand::random::<f64>() * 1.2 + 0.5).collect();

    let (statistic, p_value) = ks_test(&train, &prod);

    println!("KS statistic: {:.4}", statistic);
    println!("p-value: {:.4e}", p_value);

    if p_value < 0.01 {
        println!("âš ï¸ Data drift detected! Trigger retraining.");
    } else {
        println!("âœ… No drift detected.");
    }
}
```

å‡ºåŠ›:
```
KS statistic: 0.2341
p-value: 3.42e-12
âš ï¸ Data drift detected! Trigger retraining.
```

### 5.4 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ3: A/Bãƒ†ã‚¹ãƒˆçµ±è¨ˆçš„æ¤œå‡ºåŠ›è¨ˆç®—

**ç›®æ¨™**: âš¡Juliaã§ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®— + ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€‚

```julia
using Distributions, Statistics

"""
Calculate required sample size for A/B test
"""
function calculate_sample_size(p_baseline::Float64, mde::Float64;
                                Î±::Float64=0.05, power::Float64=0.8)
    z_Î± = quantile(Normal(), 1 - Î±/2)  # 1.96 for Î±=0.05
    z_Î² = quantile(Normal(), power)    # 0.84 for power=0.8

    pÌ„ = p_baseline
    n = ((z_Î± + z_Î²)^2 * 2pÌ„ * (1 - pÌ„)) / mde^2

    return ceil(Int, n)
end

"""
Simulate A/B test
"""
function simulate_ab_test(p_a::Float64, p_b::Float64, n::Int; Î±::Float64=0.05)
    a_successes = rand(Binomial(n, p_a))
    b_successes = rand(Binomial(n, p_b))

    pÌ‚_a = a_successes / n
    pÌ‚_b = b_successes / n

    p_pool = (a_successes + b_successes) / (2n)

    se  = sqrt(2p_pool * (1 - p_pool) / n)
    z   = (pÌ‚_b - pÌ‚_a) / se
    p_val = 2(1 - cdf(Normal(), abs(z)))

    return p_val < Î±
end

# Example
p_baseline = 0.10
mde = 0.02  # Want to detect 2% improvement
n = calculate_sample_size(p_baseline, mde)
println("Required sample size per group: $n")

# Run 1000 simulations
p_a = 0.10
p_b = 0.12  # True improvement = 2%
n_sims = 1000
wins = sum(simulate_ab_test(p_a, p_b, n) for _ in 1:n_sims)

println("Power (empirical): $(wins / n_sims)")  # Should be ~0.8
```

å‡ºåŠ›:
```
Required sample size per group: 3528
Power (empirical): 0.812
```

**ç†è«–å€¤ (power=0.8) ã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœãŒä¸€è‡´ã€‚**

### 5.5 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] MLflowã§å®Ÿé¨“ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã§ãã‚‹
- [ ] DVCã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã§ãã‚‹
- [ ] GitHub Actionsã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆã‚’è‡ªå‹•åŒ–ã§ãã‚‹
- [ ] ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã®æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’è¨­è¨ˆã§ãã‚‹
- [ ] A/Bãƒ†ã‚¹ãƒˆã®å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã§ãã‚‹
- [ ] KSæ¤œå®š / PSI ã§ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã§ãã‚‹
- [ ] Prometheusã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†ã§ãã‚‹
- [ ] SLI/SLOã‚’è¨­è¨ˆã—ã€Error Budgetã‚’è¨ˆç®—ã§ãã‚‹
- [ ] DPO lossã‚’å°å‡ºã§ãã‚‹
- [ ] âš¡Julia + ğŸ¦€Rust + ğŸ”®Elixir ã§ MLOps ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè£…ã§ãã‚‹

**10å€‹ãƒã‚§ãƒƒã‚¯ã§ããŸã‚‰MLOpså®Œå…¨ç‰ˆã‚¯ãƒªã‚¢ã€‚**

> **Note:** **é€²æ—: 85% å®Œäº†** å®Ÿè£…ã¨å®Ÿé¨“ã‚’å®Œäº†ã€‚Zone 6ã§ç ”ç©¶ç³»è­œã¨ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒã¸ã€‚

---

> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. MLOps Level 2ï¼ˆç¶™ç¶šçš„è‡ªå‹•å†è¨“ç·´ï¼‰ã«ãŠã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œçŸ¥â†’è‡ªå‹•å†è¨“ç·´â†’ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã®è‡ªå‹•åŒ–ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®æœ€å°æ§‹æˆã‚’è¿°ã¹ã‚ˆã€‚
> 2. DPOæå¤±ã®å®Ÿè£…ã§ã€`log_ratio_chosen - log_ratio_rejected` ã‚’è¨ˆç®—ã™ã‚‹éš›ã€æ•°å€¤å®‰å®šåŒ–ã®ãŸã‚ã«æ³¨æ„ã™ã¹ãç‚¹ï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼/ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ï¼‰ã¨å¯¾ç­–ã‚’èª¬æ˜ã›ã‚ˆã€‚

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” MLOpså®Œå…¨ç‰ˆã¾ã¨ã‚ & ãƒ„ãƒ¼ãƒ«

### 7.1 3ã¤ã®æ ¸å¿ƒ

#### 1. MLOps = ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã®è¦å¾‹ã‚’MLã«é©ç”¨

å¾“æ¥ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã§ã¯ã€Git/CI/CD/ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¯**å½“ãŸã‚Šå‰**ã€‚

MLã§ã‚‚åŒã˜ã¯ãš â€” ã ãŒå¤šãã®ãƒãƒ¼ãƒ ãŒæ‰‹ä½œæ¥­ã§å®Ÿé¨“ãƒãƒ¼ãƒˆã€‚

**MLOps = "MLã«ã‚‚DevOpsã¨åŒã˜è¦å¾‹ã‚’" ã¨ã„ã†å½“ç„¶ã®ä¸»å¼µ**ã€‚

#### 2. 7ã¤ã®ãƒ”ãƒ¼ã‚¹ãŒç’°ã‚’æˆã™

1. **ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°** (Git LFS/DVC) â†’ ã‚³ãƒ¼ãƒ‰ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«ã‚’è¿½è·¡
2. **å®Ÿé¨“ç®¡ç†** (MLflow/W&B) â†’ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
3. **CI/CD** (GitHub Actions) â†’ è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤
4. **A/Bãƒ†ã‚¹ãƒˆ** â†’ æ–°æ—§ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
5. **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°** (Prometheus/Grafana) â†’ SLI/SLOç›£è¦–
6. **ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º** (KS/PSI) â†’ è‡ªå‹•å†è¨“ç·´ãƒˆãƒªã‚¬ãƒ¼
7. **DPO/RLHF** â†’ äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµ±åˆ

**ã“ã®7ã¤ãŒæƒã£ã¦åˆã‚ã¦ "Production-ready ML system"**ã€‚

#### 3. 99.9%å¯ç”¨æ€§ã¯"åŠªåŠ›"ã§ã¯ãªã"è¨­è¨ˆ"

SLO = 99.9% ã¯ã€Œé ‘å¼µã‚‹ã€ã§ã¯é”æˆã§ããªã„ã€‚

**Error Budget (43.2åˆ†/æœˆ) ã‚’è¨­è¨ˆã«çµ„ã¿è¾¼ã‚€**:

- ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã§æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ
- è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¡ä»¶ã‚’äº‹å‰è¨­å®š
- ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã§å†è¨“ç·´ã‚’è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼

**è¨­è¨ˆã§"äº‹æ•…ãŒèµ·ããªã„"ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã‚‹ã€‚**

### 7.2 å­¦ç¿’åˆ°é”ç‚¹ãƒã‚§ãƒƒã‚¯

- [ ] MLflowã§å®Ÿé¨“ã‚’è¨˜éŒ²ã—ã€UIã§æ¯”è¼ƒã§ãã‚‹
- [ ] DVCã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ã—ã€ãƒãƒ¼ãƒ ã§å…±æœ‰ã§ãã‚‹
- [ ] GitHub Actionsã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆã‚’è‡ªå‹•åŒ–ã§ãã‚‹
- [ ] A/Bãƒ†ã‚¹ãƒˆã®å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã§ãã‚‹
- [ ] ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã®æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’è¨­è¨ˆã§ãã‚‹
- [ ] Prometheusã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†ã—ã€Grafanaã§å¯è¦–åŒ–ã§ãã‚‹
- [ ] SLI/SLOã‚’è¨­è¨ˆã—ã€Error Budgetã‚’è¨ˆç®—ã§ãã‚‹
- [ ] KSæ¤œå®š/PSIã§ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã§ãã‚‹
- [ ] DPO lossã‚’å°å‡ºã—ã€RLHFã¨ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] âš¡Julia + ğŸ¦€Rust + ğŸ”®Elixir ã§MLOpsãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè£…ã§ãã‚‹

**å…¨ã¦ãƒã‚§ãƒƒã‚¯ã§ããŸã‚‰ã€ã‚ãªãŸã¯MLOpså®Œå…¨ç‰ˆã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚**

### 7.3 FAQ

<details><summary>Q1: MLflowã¨W&Bã©ã¡ã‚‰ã‚’é¸ã¶ã¹ãï¼Ÿ</summary>

**A**: ã‚³ã‚¹ãƒˆ vs ç”Ÿç”£æ€§ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€‚

- **MLflow**: ç„¡æ–™ãƒ»Self-hosted â†’ å®Œå…¨åˆ¶å¾¡ãƒ»ã‚³ã‚¹ãƒˆé‡è¦–
- **W&B**: æœ‰æ–™ãƒ»Cloud â†’ UIæœ€å¼·ãƒ»ãƒãƒ¼ãƒ å”æ¥­

**æ¨å¥¨**:

- ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ»å€‹äººç ”ç©¶: MLflow
- ãƒãƒ¼ãƒ é–‹ç™ºãƒ»ä¼æ¥­: W&B (åˆæœŸã¯Free tierã§è©¦ã™)

</details>

<details><summary>Q2: DVCã¨Git LFSã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ</summary>

**A**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ€§è³ªã§æ±ºã‚ã‚‹ã€‚

| ç”¨é€” | ãƒ„ãƒ¼ãƒ« |
|:-----|:------|
| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (CSV/ç”»åƒ/å‹•ç”») | DVC |
| ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (ãƒã‚¤ãƒŠãƒª) | Git LFS |
| ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†ã‚‚å¿…è¦ | DVC (dvc.yaml) |

**DVC = ãƒ‡ãƒ¼ã‚¿ç‰ˆGit + ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã€‚Git LFSã‚ˆã‚Šé«˜æ©Ÿèƒ½ã ãŒå­¦ç¿’æ›²ç·šã¯é«˜ã„ã€‚

</details>

<details><summary>Q3: ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã®å„ã‚¹ãƒ†ãƒ¼ã‚¸ã¯ä½•%ãŒé©åˆ‡ï¼Ÿ</summary>

**A**: æ¨™æº–ã¯ 1% â†’ 5% â†’ 25% â†’ 100%ã€‚

- **1%**: æ—©æœŸç•°å¸¸æ¤œå‡º (æ•°ç™¾ãƒ¦ãƒ¼ã‚¶ãƒ¼)
- **5%**: çµ±è¨ˆçš„æœ‰æ„æ€§ç¢ºä¿ (æ•°åƒãƒ¦ãƒ¼ã‚¶ãƒ¼)
- **25%**: æœ¬æ ¼çš„æ€§èƒ½æ¤œè¨¼
- **100%**: å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼

**å„ã‚¹ãƒ†ãƒ¼ã‚¸ã§ç›£è¦– (1-24æ™‚é–“)ã€‚ç•°å¸¸ãªã‚‰å³ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚**

</details>

<details><summary>Q4: SLO 99.9% ã¨ 99.99% ã®é•ã„ã¯ï¼Ÿ</summary>

**A**: ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã®è¨±å®¹é‡ãŒ10å€é•ã†ã€‚

| SLO | æœˆé–“ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ  | å¹´é–“ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ  |
|:----|:---------------|:---------------|
| 99% | 7.2æ™‚é–“ | 3.65æ—¥ |
| 99.9% | 43.2åˆ† | 8.76æ™‚é–“ |
| 99.99% | 4.32åˆ† | 52.6åˆ† |
| 99.999% | 26ç§’ | 5.26åˆ† |

**99.99%ä»¥ä¸Šã¯é‡‘èãƒ»åŒ»ç™‚ãƒ¬ãƒ™ãƒ«ã€‚é€šå¸¸ã®MLã‚µãƒ¼ãƒ“ã‚¹ã¯99.9%ã§ååˆ†ã€‚**

</details>

<details><summary>Q5: ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡ºã—ãŸã‚‰å¿…ãšå†è¨“ç·´ã™ã¹ãï¼Ÿ</summary>

**A**: **No**ã€‚ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã¯"lead"ã§ã‚ã‚Š"verdict"ã§ã¯ãªã„ã€‚

**æ¤œè¨¼ã™ã¹ã**:

1. **æ€§èƒ½åŠ£åŒ–ã®æœ‰ç„¡**: ãƒ‰ãƒªãƒ•ãƒˆãŒã‚ã£ã¦ã‚‚æ€§èƒ½ãŒç¶­æŒã•ã‚Œã¦ã„ã‚Œã°OK
2. **ãƒ‰ãƒªãƒ•ãƒˆã®åŸå› **: ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œï¼Ÿãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•å¤‰åŒ–ï¼Ÿ
3. **å†è¨“ç·´ã®ã‚³ã‚¹ãƒˆ**: è¨“ç·´ã«1é€±é–“ã‹ã‹ã‚‹ãªã‚‰æ…é‡ã«åˆ¤æ–­

**Evidently AIã®æ¨å¥¨**: ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º â†’ æ€§èƒ½ç¢ºèª â†’ åŠ£åŒ–ã—ã¦ã„ãŸã‚‰å†è¨“ç·´ã€‚

</details>

### 7.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (1é€±é–“)

| æ—¥ | å­¦ç¿’å†…å®¹ | æ™‚é–“ | ã‚¿ã‚¹ã‚¯ |
|:---|:--------|:-----|:-------|
| 1æ—¥ç›® | Zone 0-2 é€šèª­ | 30åˆ† | MLOpså…¨ä½“åƒæŠŠæ¡ |
| 2æ—¥ç›® | Part A-B (ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»CI/CD) | 2æ™‚é–“ | æ•°å¼è¿½ã† |
| 3æ—¥ç›® | Part C-D (A/Bãƒ»ç›£è¦–) | 2æ™‚é–“ | ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®— |
| 4æ—¥ç›® | Part E (DPO/RLHF) | 1.5æ™‚é–“ | DPO losså°å‡º |
| 5æ—¥ç›® | Part F (å®Ÿè£…ç·¨) | 2æ™‚é–“ | âš¡ğŸ¦€ğŸ”®å®Ÿè£… |
| 6æ—¥ç›® | Zone 5 (å®Ÿé¨“) | 2æ™‚é–“ | ãƒŸãƒ‹PJ 3ã¤ |
| 7æ—¥ç›® | å¾©ç¿’ãƒ»Boss Battle | 2æ™‚é–“ | å®Œå…¨ã‚µã‚¤ã‚¯ãƒ«æ•°å¼ |

**åˆè¨ˆ: 12æ™‚é–“**ã€‚é›†ä¸­ã™ã‚Œã°1é€±é–“ã§ãƒã‚¹ã‚¿ãƒ¼å¯èƒ½ã€‚

### 6.5 ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒãƒãƒˆãƒªã‚¯ã‚¹

#### å®Ÿé¨“ç®¡ç†ãƒ„ãƒ¼ãƒ«

| ãƒ„ãƒ¼ãƒ« | ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚° | UIå“è³ª | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª | ã‚³ã‚¹ãƒˆ |
|:------|:-----------|:-------|:----------------------------|:---------------|:------|
| **MLflow** | Self-hosted | â­â­â­ | âŒ (å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ä½µç”¨) | âœ… | ç„¡æ–™ (ã‚¤ãƒ³ãƒ•ãƒ©ä»£ã®ã¿) |
| **W&B** | Cloud | â­â­â­â­â­ | âœ… Sweeps (Bayesian Opt) | âœ… | $50/user/month |
| **Neptune** | Cloud | â­â­â­â­ | âœ… | âœ… | $39/user/month |
| **Comet** | Cloud | â­â­â­â­ | âœ… | âœ… | $49/user/month |

**æ¨å¥¨**:

- **ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—**: MLflow (ç„¡æ–™ãƒ»Self-hosted)
- **ãƒãƒ¼ãƒ å”æ¥­**: W&B (UIæœ€å¼·ãƒ»Sweepsä¾¿åˆ©)
- **ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º**: MLflow on Databricks

#### ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

| ãƒ„ãƒ¼ãƒ« | Gitçµ±åˆ | ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ | å­¦ç¿’æ›²ç·š |
|:------|:--------|:-----------|:----------------|:---------|
| **DVC** | âœ… | âœ… (dvc.yaml) | S3/GCS/Azure/SSH | ä¸­ |
| **Git LFS** | âœ… | âŒ | GitHub LFS / S3 | ä½ |
| **LakeFS** | âœ… (Git-like) | âœ… | S3/Azure/GCS | é«˜ |

**æ¨å¥¨**:

- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ < 100GB**: DVC
- **ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿**: Git LFS
- **Data Lakeã‚¹ã‚±ãƒ¼ãƒ«**: LakeFS

#### ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° & ã‚¢ãƒ©ãƒ¼ãƒˆ

| ãƒ„ãƒ¼ãƒ« | ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›† | å¯è¦–åŒ– | ã‚¢ãƒ©ãƒ¼ãƒˆ | MLç‰¹åŒ– | ã‚³ã‚¹ãƒˆ |
|:------|:------------|:------|:--------|:-------|:------|
| **Prometheus + Grafana** | âœ… | âœ… | âœ… | âŒ | ç„¡æ–™ (Self-hosted) |
| **Datadog** | âœ… | âœ… | âœ… | â­â­ | $15/host/month |
| **New Relic** | âœ… | âœ… | âœ… | â­â­ | $99/user/month |
| **Evidently AI** | âŒ | âœ… (drift only) | âœ… | â­â­â­â­â­ | ç„¡æ–™ (OSS) + Cloud |

**æ¨å¥¨**:

- **æ±ç”¨ç›£è¦–**: Prometheus + Grafana
- **MLç‰¹åŒ– (ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º)**: Evidently AI
- **çµ±åˆç›£è¦–**: Datadog (APM + ã‚¤ãƒ³ãƒ•ãƒ© + ML)

### 6.6 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **99.9%å¯ç”¨æ€§ã¯"åŠªåŠ›"ã§ã¯ãªã"è¨­è¨ˆ"ã§ã¯ï¼Ÿ**

ã€Œé ‘å¼µã£ã¦ç›£è¦–ã—ã¾ã™ã€ã€Œéšœå®³ãŒèµ·ããŸã‚‰å¯¾å¿œã—ã¾ã™ã€ â€” ã“ã‚Œã¯**è¨­è¨ˆã§ã¯ãªãé‹ç”¨**ã ã€‚

**è¨­è¨ˆã¨ã¯**:

- Error Budget (43.2åˆ†/æœˆ) ã‚’**è¨­è¨ˆæ®µéšã§**çµ„ã¿è¾¼ã‚€
- ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ã§æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’**è‡ªå‹•åŒ–**
- ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºâ†’å†è¨“ç·´ã‚’**è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼**
- SLOé•åâ†’è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°/ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

**"äº‹æ•…ãŒèµ·ããŸã‚‰å¯¾å¿œ"ã§ã¯ãªãã€"äº‹æ•…ãŒèµ·ããªã„"ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆã™ã‚‹ã€‚**

å¾“æ¥ã®é–‹ç™º:

```
ãƒ¢ãƒ‡ãƒ«è¨“ç·´ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ â†’ (éšœå®³ç™ºç”Ÿ) â†’ æ‰‹ä½œæ¥­ã§å¯¾å¿œ
```

MLOps:

```
ãƒ¢ãƒ‡ãƒ«è¨“ç·´ â†’ CI/CDè‡ªå‹•ãƒ†ã‚¹ãƒˆ â†’ ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤ â†’ ç›£è¦– â†’ (ç•°å¸¸æ¤œå‡º) â†’ è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ â†’ ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º â†’ è‡ªå‹•å†è¨“ç·´
```

**å…¨ã¦è‡ªå‹•åŒ–ã•ã‚Œã¦ã„ã‚‹ = è¨­è¨ˆã§"äº‹æ•…ãŒèµ·ããªã„"ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚**

<details><summary>è­°è«–ã®å‡ºç™ºç‚¹</summary>

1. **ã‚ãªãŸã®ãƒãƒ¼ãƒ ã¯ã€ŒåŠªåŠ›ã€ã«é ¼ã£ã¦ã„ãªã„ã‹ï¼Ÿ** "é ‘å¼µã£ã¦ç›£è¦–" vs "è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆ+ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"
2. **Error Budgetã‚’è¨­è¨ˆã«çµ„ã¿è¾¼ã‚“ã§ã„ã‚‹ã‹ï¼Ÿ** æœˆã«ä½•åˆ†ã¾ã§ã®ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã‚’è¨±å®¹ã™ã‚‹ã‹ã‚’æ±ºã‚ã¦ã„ã‚‹ã‹ï¼Ÿ
3. **"å‹•ã"ã¨"å‹•ãç¶šã‘ã‚‹"ã®é•ã„ã¯ä½•ã‹ï¼Ÿ** ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦çµ‚ã‚ã‚Šã‹ã€ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã§å†è¨“ç·´ã¾ã§ã‚µã‚¤ã‚¯ãƒ«ãŒå›ã‚‹ã‹ï¼Ÿ

**99.9%å¯ç”¨æ€§ã¯ã€è¨­è¨ˆã®çµæœã¨ã—ã¦"è‡ªç„¶ã«é”æˆã•ã‚Œã‚‹"ã‚‚ã®ã ã€‚**

</details>

### 6.7 æ¬¡å›äºˆå‘Š â€” ç¬¬32å›: Production & ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ— + çµ±åˆPJ

**ç¬¬32å›ãŒCourse IIIæœ€çµ‚å›**ã€‚

**ãƒ†ãƒ¼ãƒ**: Trainâ†’Evaluateâ†’Deployâ†’Monitorâ†’Feedbackã®**ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«çµ±åˆPJ**

- AIã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆå°å…¥
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†ãƒ»åˆ†æ
- ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«
- Human-in-the-loop
- E2Eã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
- **Course IIIèª­äº†æ„Ÿ**

ç¬¬31å›ã§MLOpså…¨é ˜åŸŸã‚’ç†è«–ãƒ»å®Ÿè£…ã§ç¶²ç¾…ã—ãŸã€‚ç¬¬32å›ã§çµ±åˆPJã‚’æ§‹ç¯‰ã—ã€**"ç ”ç©¶ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—" â†’ "Production-ready system"** ã®å¤‰æ›ã‚’å®Œçµã•ã›ã‚‹ã€‚

Course IIIã®ã‚´ãƒ¼ãƒ«ã¾ã§ã‚ã¨1å›ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ MLOpså®Œå…¨ç‰ˆã‚¯ãƒªã‚¢ï¼æ¬¡å›ã§çµ±åˆPJæ§‹ç¯‰ â†’ Course IIIå®Œçµã¸ã€‚

---

### 6.8 Advanced MLOps Frameworks & Tools (2020-2026)

#### 6.8.1 Feature Store â€” ç‰¹å¾´é‡ã®ä¸€å…ƒç®¡ç†

**èª²é¡Œ**: è¨“ç·´æ™‚ã¨æ¨è«–æ™‚ã§ç‰¹å¾´é‡è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ãŒä¸ä¸€è‡´ â†’ Training-Serving Skew

**Feature Store**: ç‰¹å¾´é‡ã‚’ä¸­å¤®ãƒªãƒã‚¸ãƒˆãƒªã§ç®¡ç†ãƒ»é…ä¿¡

**ä¸»è¦ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ**:

| Tool | Provider | Key Features |
|:-----|:---------|:------------|
| **Feast** | Open-source | Offline (batch) + Online (low-latency) |
| **Tecton** | Commercial | Real-time features + monitoring |
| **Hopsworks** | Open-source | End-to-end ML platform |

**Feast Architecture**:

```julia
# Feature definition (feast.yaml)
"""
features:
  - name: user_avg_purchase_7d
    entity: user_id
    type: float
    source: data_warehouse
    freshness: 1 hour
"""

# Offline retrieval (training)
using PyCall
feast = pyimport("feast")
store = feast.FeatureStore(".")

entity_df = DataFrame(
    user_id = [1001, 1002, 1003],
    event_timestamp = [now(), now(), now()]
)

training_df = store.get_historical_features(
    entity_df = entity_df,
    features = ["user_features:avg_purchase_7d", "user_features:total_sessions"]
).to_df()

# Online retrieval (inference, <10ms latency)
features = store.get_online_features(
    features = ["user_features:avg_purchase_7d"],
    entity_rows = [Dict("user_id" => 1001)]
).to_dict()
```

**åˆ©ç‚¹**:
- Training-Servingä¸€è²«æ€§ä¿è¨¼
- ç‰¹å¾´é‡å†åˆ©ç”¨ (ãƒãƒ¼ãƒ é–“å…±æœ‰)
- Point-in-time correctness (æ™‚åˆ»æ•´åˆæ€§)

#### 6.8.2 Model Registry â€” ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†

**MLflow Model Registry** [^4]:

**ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¸**:

```
None â†’ Staging â†’ Production â†’ Archived
```

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† + ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**:

```julia
using PyCall
mlflow = pyimport("mlflow")

# Register model
mlflow.register_model(
    model_uri = "runs:/abc123/model",
    name = "fraud_detector_v2"
)

# Transition to production
client = mlflow.tracking.MlflowClient()
client.transition_model_version_stage(
    name = "fraud_detector_v2",
    version = 3,
    stage = "Production"
)

# Load production model
model_uri = "models:/fraud_detector_v2/Production"
model = mlflow.pyfunc.load_model(model_uri)
```

**Governanceæ©Ÿèƒ½**:
- **Approval Workflow**: Staging â†’ Production ã«æ‰¿èªå¿…é ˆ
- **Lineage Tracking**: ãƒ‡ãƒ¼ã‚¿ â†’ è¨“ç·´ â†’ ãƒ¢ãƒ‡ãƒ« ã®ç³»è­œ
- **Model Card**: æ€§èƒ½ãƒ»å…¬å¹³æ€§ãƒ»åˆ¶ç´„ã®æ–‡æ›¸åŒ–

#### 6.8.3 Experiment Tracking at Scale

**Weights & Biases (W&B)** vs **MLflow**:

| æ©Ÿèƒ½ | MLflow | W&B |
|:-----|:-------|:----|
| **UI** | Basic | Rich (interactive charts) |
| **Hyperparameter Sweep** | Manual | Automated (Bayesian) |
| **Collaboration** | Limited | Team-centric |
| **Artifact Storage** | Local/S3 | Cloud-native |
| **Cost** | Free (self-host) | Free tier + Paid |

**W&B Sweep (Bayesian Optimization)**:

```julia
using PyCall
wandb = pyimport("wandb")

# Sweep configuration
sweep_config = Dict(
    "method" => "bayes",
    "metric" => Dict("name" => "val_loss", "goal" => "minimize"),
    "parameters" => Dict(
        "learning_rate" => Dict("min" => 1e-5, "max" => 1e-2),
        "batch_size" => Dict("values" => [16, 32, 64, 128]),
        "dropout" => Dict("min" => 0.1, "max" => 0.5)
    )
)

sweep_id = wandb.sweep(sweep_config, project="my_project")

# Training function
function train()
    wandb.init()
    config = wandb.config

    # Train with config.learning_rate, config.batch_size, etc.
    for epoch in 1:10
        loss = train_one_epoch(config)
        wandb.log(Dict("loss" => loss, "epoch" => epoch))
    end
end

# Run sweep
wandb.agent(sweep_id, function=train, count=50)  # 50 trials
```

**åŠ¹æœ**: Manual grid search â†’ Bayesian optimization ã§æ¢ç´¢åŠ¹ç‡**10å€å‘ä¸Š**

#### 6.8.4 Data Quality Monitoring â€” Great Expectations Integration

**Great Expectations** [^3]: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ†ã‚¹ãƒˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**Expectation Suite**:

```julia
using PyCall
ge = pyimport("great_expectations")

# Create expectation suite
context = ge.data_context.DataContext()
suite = context.create_expectation_suite("transaction_data_suite")

# Define expectations
validator = context.get_validator(
    batch_request = batch_request,
    expectation_suite_name = "transaction_data_suite"
)

# Expectations (assertions on data)
validator.expect_column_values_to_be_between("amount", min_value=0, max_value=1e6)
validator.expect_column_values_to_not_be_null("user_id")
validator.expect_column_values_to_be_in_set("status", ["pending", "completed", "failed"])
validator.expect_column_values_to_match_regex("email", r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$")

# Save suite
validator.save_expectation_suite(discard_failed_expectations=false)
```

**Validation in Pipeline**:

```julia
# Run validation
checkpoint_config = Dict(
    "name" => "daily_data_checkpoint",
    "config_version" => 1,
    "class_name" => "SimpleCheckpoint",
    "validations" => [
        Dict(
            "batch_request" => batch_request,
            "expectation_suite_name" => "transaction_data_suite"
        )
    ]
)

results = context.run_checkpoint(checkpoint_config)

if !results["success"]
    error("Data validation failed! $(results["statistics"])")
end
```

**Production Integration** (Airflow DAG):

Airflow ã§ã¯ DAG ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ï¼ˆ`PythonOperator` / `BashOperator`ï¼‰ã‚’è¿½åŠ ã—ã€`validate >> train` ã®ãƒ“ãƒƒãƒˆã‚·ãƒ•ãƒˆæ§‹æ–‡ã§æœ‰å‘ä¾å­˜é–¢ä¿‚ã‚’å®£è¨€ã™ã‚‹ã€‚ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒä¾å­˜ã‚°ãƒ©ãƒ•ã‚’è§£æã—ã€ä¸ŠæµãŒæˆåŠŸã—ãŸå ´åˆã®ã¿ä¸‹æµã‚¿ã‚¹ã‚¯ã‚’èµ·å‹•ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼â†’ãƒ¢ãƒ‡ãƒ«è¨“ç·´â†’ãƒ‡ãƒ—ãƒ­ã‚¤ã®ç›´åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®£è¨€çš„ã«è¨˜è¿°ã§ãã‚‹ã€‚

#### 6.8.5 CI/CD for ML â€” GitHub Actions + DVC

**GitHub Actions Workflow**:

```yaml
# .github/workflows/ml_ci.yml
name: ML CI/CD Pipeline

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: iterative/setup-dvc@v1

      - name: Pull data with DVC
        run: dvc pull

      - name: Run unit tests
        run: pytest tests/unit/

      - name: Run data validation
        run: |
          python -m great_expectations checkpoint run data_validation

      - name: Train model (smoke test)
        run: |
          python train.py --epochs 1 --smoke-test

      - name: Run model tests
        run: pytest tests/model/

  deploy:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to staging
        run: |
          mlflow models serve -m "models:/my_model/Staging" -p 5001

      - name: Run integration tests
        run: pytest tests/integration/

      - name: Promote to production
        run: |
          python scripts/promote_model.py --version ${{ github.sha }}
```

**CML (Continuous Machine Learning)** [^5]:

```yaml
# .github/workflows/cml.yml
name: Model Performance Report

on: [push]

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: iterative/setup-cml@v1

      - name: Train model
        run: python train.py

      - name: Generate metrics report
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Create report
          cat metrics.json | jq -r '.accuracy' > report.txt
          echo "Accuracy: $(cat report.txt)" >> report.md

          # Plot
          python plot_metrics.py
          cml-publish confusion_matrix.png --md >> report.md

          # Send comment to PR
          cml-send-comment report.md
```

**åŠ¹æœ**: PRã”ã¨ã«è‡ªå‹•ã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ â†’ ãƒ¬ãƒ“ãƒ¥ãƒ¼æ™‚ã«å¯è¦–åŒ–

### 6.9 Scalable Training Infrastructure

#### 6.9.1 Distributed Training â€” Ray + DeepSpeed

**Ray Train** (åˆ†æ•£è¨“ç·´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯):

```julia
using PyCall
ray = pyimport("ray")
train = pyimport("ray.train")

# Define training function
function train_func(config)
    model = create_model(config["lr"])

    # Distributed data loading
    train_dataset = train.get_dataset_shard("train")

    for epoch in 1:config["epochs"]
        for batch in train_dataset.iter_batches(batch_size=32)
            loss = train_step(model, batch)
            train.report(Dict("loss" => loss))
        end
    end
end

# Scale to 4 GPUs
trainer = train.TorchTrainer(
    train_func,
    scaling_config = train.ScalingConfig(num_workers=4, use_gpu=true),
    datasets = Dict("train" => ray.data.read_parquet("s3://data/train/"))
)

result = trainer.fit()
```

**DeepSpeed ZeRO-3** (ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–):

| Stage | Parameter Partitioning | Gradient Partitioning | Optimizer State Partitioning | Memory Reduction |
|:------|:----------------------|:---------------------|:----------------------------|:----------------|
| ZeRO-1 | âŒ | âŒ | âœ… | 4x |
| ZeRO-2 | âŒ | âœ… | âœ… | 8x |
| **ZeRO-3** | âœ… | âœ… | âœ… | **15-60x** |

**åŠ¹æœ**: 175B parameter model ã‚’ 16x V100 (16GB) ã§è¨“ç·´å¯èƒ½

#### 6.9.2 Serverless Inference â€” AWS Lambda + SageMaker

**AWS Lambda (< 15MB model)**:

```rust
// Rust Lambda function for inference
use lambda_runtime::{service_fn, LambdaEvent, Error};
use serde::{Deserialize, Serialize};
use ort::{Environment, SessionBuilder, Value};

#[derive(Deserialize)]
struct Request {
    features: Vec<f32>,
}

#[derive(Serialize)]
struct Response {
    prediction: f32,
}

async fn handler(event: LambdaEvent<Request>) -> Result<Response, Error> {
    // Load ONNX model (embedded in Lambda)
    let environment = Environment::builder().build()?;
    let session = SessionBuilder::new(&environment)?
        .with_model_from_memory(include_bytes!("model.onnx"))?;

    // Run inference
    let input = ndarray::arr1(&event.payload.features);
    let outputs = session.run(vec![Value::from_array(input)?])?;
    let prediction = outputs[0].extract::<f32>()?.view()[0];

    Ok(Response { prediction })
}

#[tokio::main]
async fn main() -> Result<(), Error> {
    lambda_runtime::run(service_fn(handler)).await
}
```

**ç‰¹å¾´**:
- **Cold start**: 100-500ms (Rust), 500-3000ms (Python)
- **Cost**: $0.20 per 1M requests (128MB, 100ms execution)
- **Auto-scaling**: 0 â†’ 10,000 concurrentç„¡é™ã‚¹ã‚±ãƒ¼ãƒ«

**SageMaker Serverless Inference** (> 15MB model):

```julia
using PyCall
sagemaker = pyimport("sagemaker")

# Deploy model as serverless endpoint
predictor = model.deploy(
    endpoint_type = "serverless",
    serverless_inference_config = sagemaker.serverless.ServerlessInferenceConfig(
        memory_size_in_mb = 2048,
        max_concurrency = 20
    )
)

# Inference
result = predictor.predict(data)
```

**Cost comparison** (1M requests/month):

| Service | Fixed Cost | Variable Cost | Total |
|:--------|:----------|:-------------|:------|
| EC2 (t3.medium 24/7) | $30/month | $0 | **$30** |
| Lambda (100ms avg) | $0 | $0.20/1M | **$0.20** |
| SageMaker Serverless | $0 | $0.20/1M + $0.10/GB-hr | **$0.30** |

ä½ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯æ™‚ã¯ServerlessãŒ**100å€å®‰ã„**

### 6.10 Production Best Practices (Industry Standard)

#### 6.10.1 Model Governance â€” Audit Trail & Compliance

**å¿…é ˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°é …ç›®** (è¦åˆ¶å¯¾å¿œ):

| Item | Requirement | Tool |
|:-----|:-----------|:-----|
| **Training Data Lineage** | ãƒ‡ãƒ¼ã‚¿ã®å‡ºæ‰€ãƒ»å¤‰æ›å±¥æ­´ | DVC + Pachyderm |
| **Model Versioning** | å…¨ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† | MLflow Registry |
| **Prediction Logging** | å…¨æ¨è«–çµæœã®è¨˜éŒ² (90æ—¥ä¿æŒ) | CloudWatch Logs |
| **Bias Monitoring** | äººç¨®ãƒ»æ€§åˆ¥ç­‰ã§ã®æ€§èƒ½å·® | AWS SageMaker Clarify |
| **Explainability** | å€‹åˆ¥äºˆæ¸¬ã®èª¬æ˜ | SHAP / LIME |

**Audit Log Example**:

```json
{
  "timestamp": "2026-02-15T10:30:00Z",
  "model_id": "fraud_detector_v3.2",
  "model_version": "sha256:abc123...",
  "input": {"user_id": 1001, "amount": 500},
  "output": {"fraud_score": 0.82, "decision": "flag"},
  "explanation": {
    "top_features": [
      {"feature": "transaction_velocity", "contribution": 0.35},
      {"feature": "geolocation_mismatch", "contribution": 0.28}
    ]
  },
  "data_lineage": {
    "training_data": "s3://data/fraud/2026-01-15/",
    "training_commit": "git-sha:def456"
  }
}
```

#### 6.10.2 Security â€” Secrets Management & Access Control

**AWS Secrets Manager** (credentialsä¿ç®¡):

```rust
use aws_sdk_secretsmanager::Client;

async fn get_db_password() -> Result<String, Box<dyn std::error::Error>> {
    let config = aws_config::load_from_env().await;
    let client = Client::new(&config);

    let response = client
        .get_secret_value()
        .secret_id("prod/mlops/db_password")
        .send()
        .await?;

    Ok(response.secret_string().unwrap().to_string())
}
```

**IAM Role-based Access**:

```yaml
# Kubernetes ServiceAccount (EKS)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ml-inference-sa
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::123456789:role/MLInferenceRole

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-api
spec:
  template:
    spec:
      serviceAccountName: ml-inference-sa  # Inherits IAM permissions
      containers:
      - name: api
        image: my-inference:latest
```

**Network Isolation**:

```
Internet â†’ ALB (HTTPS) â†’ API Gateway â†’ Private Subnet (Inference) â†’ VPC Endpoint â†’ S3 (models)
                                             â†“
                                    Security Group (port 8080 only)
```

> **Note:** **é€²æ—: å®Œå…¨åˆ¶è¦‡!** Advanced MLOps toolsã€åˆ†æ•£è¨“ç·´ã€Serverlessæ¨è«–ã€Governanceã€Securityã¾ã§å…¨ã¦ç¿’å¾—ã€‚Production-readyã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ã®å®Œå…¨çŸ¥è­˜ã‚’ç²å¾—ï¼

---

### 6.11 Emerging Trends (2025-2026)

#### MLOps + LLMOps Convergence

**LLMOpsç‰¹æœ‰ã®èª²é¡Œ**:
- **Prompt Versioning**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ç®¡ç†
- **Few-shot Example Management**: In-context learningç”¨ã‚µãƒ³ãƒ—ãƒ«
- **Token Cost Optimization**: APIå‘¼ã³å‡ºã—ã‚³ã‚¹ãƒˆæœ€å°åŒ–

**çµ±åˆãƒ„ãƒ¼ãƒ«**: LangChain + LangSmith â€” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ç²¾åº¦ãƒ»ã‚³ã‚¹ãƒˆã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ã€‚

#### Edge MLOps â€” On-device Inference

**TensorFlow Lite** + **ONNX Runtime Mobile**:

- Model quantization (FP32 â†’ INT8): **4å€å°å‹åŒ–**
- On-device training: Federated Learning
- OTA (Over-The-Air) model updates

**å…¸å‹çš„ãªEdge Pipeline**:

```
Cloud Training â†’ Quantization â†’ ONNX â†’ Edge Device (ARM) â†’ Telemetry â†’ Cloud Retraining
```

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. *NeurIPS 2023*.
<https://arxiv.org/abs/2305.18290>

[^2]: DVC: Data Version Control.
<https://dvc.org/>

[^3]: Great Expectations: Data validation framework.
<https://greatexpectations.io/>

[^4]: MLflow: Open source platform for the machine learning lifecycle.
<https://mlflow.org/>

[^5]: CML (Continuous Machine Learning): CI/CD for Machine Learning Projects.
<https://cml.dev/>

---

> **ğŸ“– å‰ç·¨ï¼ˆç†è«–ç·¨ï¼‰**: [ç¬¬31å›å‰ç·¨: MLOpsç†è«–ç·¨](./ml-lecture-31-part1) | **â† ç†è«–ãƒ»æ•°å¼ã‚¾ãƒ¼ãƒ³ã¸**

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
