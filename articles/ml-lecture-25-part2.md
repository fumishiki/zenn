---
title: "ç¬¬25å›: å› æœæ¨è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
slug: "ml-lecture-25-part2"
emoji: "ğŸ”—"
type: "tech"
topics: ["machinelearning", "causalinference", "rust", "statistics", "experiment"]
published: true
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> **ç¬¬25å›ã€å‰ç·¨ã€‘**: [ç¬¬25å›ã€å‰ç·¨ã€‘](https://zenn.dev/fumishiki/ml-lecture-25-part1)


## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rustå› æœæ¨è«–ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯

### 4.1 ndarray ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```rust
// Cargo.toml dependencies:
// petgraph = "0.6"       -- DAG manipulation
// ndarray = "0.16"       -- matrices / arrays
// statrs = "0.17"        -- statistical distributions
// rand = "0.8"
// rand_distr = "0.4"
// serde = { version = "1", features = ["derive"] }
// serde_json = "1"

use std::collections::HashMap;
// use petgraph::graph::DiGraph;
// use ndarray::{Array1, Array2};
// use rand_distr::{Bernoulli, Normal, Distribution};
```

### 4.2 Pearl DAG + do-æ¼”ç®—å®Ÿè£…

#### 4.2.1 DAGæ§‹ç¯‰ã¨å¯è¦–åŒ–

```rust
// DAG construction: Smoking â†’ Cancer, Gene â†’ Smoking, Gene â†’ Cancer
// use petgraph::graph::DiGraph;
// use petgraph::algo::has_path_connecting;

fn build_smoking_cancer_dag() -> Vec<(usize, usize, &'static str, &'static str)> {
    // Nodes: 0=Gene, 1=Smoking, 2=Cancer
    vec![
        (0, 1, "Gene",    "Smoking"),  // Gene â†’ Smoking
        (0, 2, "Gene",    "Cancer"),   // Gene â†’ Cancer
        (1, 2, "Smoking", "Cancer"),   // Smoking â†’ Cancer
    ]
}

fn main_dag_demo() {
    let edges = build_smoking_cancer_dag();
    println!("DAG edges:");
    for (_, _, src, dst) in &edges {
        println!("  {} â†’ {}", src, dst);
    }

    // d-separation checks (requires petgraph or custom implementation)
    // Smoking âŠ¥ Cancer | Gene?  false (direct path Smokingâ†’Cancer remains)
    println!("Smoking âŠ¥ Cancer | Gene? false");
    // Gene âŠ¥ Cancer | Smoking?  false (Geneâ†’Cancer direct path remains)
    println!("Gene âŠ¥ Cancer | Smoking? false");
}
```

#### 4.2.2 ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã®æ¤œè¨¼

```rust
// Check if adjustment_set satisfies backdoor criterion for (treatment â†’ outcome)
//
// Backdoor criterion (Pearl):
//   1. No node in adjustment_set is a descendant of treatment
//   2. adjustment_set blocks all backdoor paths from treatment to outcome
//
// In our DAG: backdoor path is Smoking â† Gene â†’ Cancer
// Adjusting for Gene blocks this path â†’ satisfies backdoor criterion
fn check_backdoor_manual(
    treatment: usize,
    outcome: usize,
    adjustment_set: &[usize],
    edges: &[(usize, usize)],
) -> bool {
    // Check condition 1: no node in adjustment_set is a descendant of treatment
    let descendants_of_treatment: Vec<usize> = edges.iter()
        .filter(|(src, _)| *src == treatment)
        .map(|(_, dst)| *dst)
        .collect();
    let no_descendant = adjustment_set.iter()
        .all(|z| !descendants_of_treatment.contains(z));
    // Check condition 2: simplified â€” trust domain knowledge for this DAG
    no_descendant // full d-sep check requires graph traversal (petgraph)
}

// Does {Gene} satisfy backdoor criterion for (Smoking=1, Cancer=2)?
// true â€” Gene blocks the only backdoor path Smoking â† Gene â†’ Cancer
// let is_valid = check_backdoor_manual(1, 2, &[0], &dag_edges);
// println!("Does {{Gene}} satisfy backdoor criterion? {}", is_valid);
```

#### 4.2.3 do-æ¼”ç®—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```rust
// Simulate observational data from the DAG (Smoking â†’ Cancer, Gene â†’ both)
// use rand_distr::{Bernoulli, Distribution};

fn logistic(x: f64) -> f64 { 1.0 / (1.0 + (-x).exp()) }
fn mean_f(v: &[f64]) -> f64 { v.iter().sum::<f64>() / v.len() as f64 }

#[derive(Clone)]
struct DagRecord {
    gene: bool,
    smoking: bool,
    cancer: bool,
}

fn simulate_from_dag(n: usize) -> Vec<DagRecord> {
    // use rand::thread_rng;
    // let mut rng = thread_rng();
    (0..n).map(|_| {
        // Gene ~ Bernoulli(0.3)
        let gene = /* Bernoulli::new(0.3).unwrap().sample(&mut rng) */ false;

        // Smoking | Gene ~ Bernoulli(Ïƒ(0.5*Gene - 0.2))
        let smoking_p = logistic(0.5 * gene as u8 as f64 - 0.2);
        let smoking = /* Bernoulli::new(smoking_p).unwrap().sample(&mut rng) */ smoking_p > 0.5;

        // Cancer | Smoking, Gene ~ Bernoulli(Ïƒ(1.5*Smoking + 0.8*Gene - 1.0))
        let cancer_p = logistic(1.5 * smoking as u8 as f64 + 0.8 * gene as u8 as f64 - 1.0);
        let cancer = /* Bernoulli::new(cancer_p).unwrap().sample(&mut rng) */ cancer_p > 0.5;

        DagRecord { gene, smoking, cancer }
    }).collect()
}

// Backdoor adjustment: P(Y | do(X=x)) = Î£_z P(Y|X=x, Z=z) * P(Z=z)
fn backdoor_adjustment(data: &[DagRecord], treatment_val: bool) -> f64 {
    // Adjust for Gene (adjustment set = {Gene})
    let mut p_y = 0.0;
    for gene_val in [false, true] {
        let subset: Vec<&DagRecord> = data.iter()
            .filter(|r| r.smoking == treatment_val && r.gene == gene_val)
            .collect();
        let p_y_given_xz = if subset.is_empty() { 0.0 } else {
            subset.iter().filter(|r| r.cancer).count() as f64 / subset.len() as f64
        };
        let p_z = data.iter().filter(|r| r.gene == gene_val).count() as f64 / data.len() as f64;
        p_y += p_y_given_xz * p_z;
    }
    p_y
}

// Usage:
// let data = simulate_from_dag(5000);
// let obs_effect = ...;  // observational: P(Cancer|Smoking=1) - P(Cancer|Smoking=0)
// let do_effect = backdoor_adjustment(&data, true) - backdoor_adjustment(&data, false);
// println!("Interventional (do-calc): {:.3}", do_effect);
```

### 4.3 å‚¾å‘ã‚¹ã‚³ã‚¢å®Ÿè£…

#### 4.3.1 å‚¾å‘ã‚¹ã‚³ã‚¢æ¨å®š (Logistic Regression)

```rust
// Logistic regression for propensity score estimation
// use ndarray::{Array1, Array2, ArrayView2};

fn logistic(x: f64) -> f64 { 1.0 / (1.0 + (-x).exp()) }

/// Estimate propensity scores e(X) = Ïƒ(XÂ·w) using precomputed weights.
/// For production use, fit logistic regression with a crate like linfa.
fn estimate_propensity_scores(features: &[Vec<f64>], weights: &[f64], bias: f64) -> Vec<f64> {
    features.iter().map(|x| {
        let dot: f64 = x.iter().zip(weights).map(|(xi, wi)| xi * wi).sum::<f64>();
        logistic(dot + bias)
    }).collect()
}

// Generate propensity-score data
struct PsRecord {
    treatment: bool,
    age: f64,
    income: f64,
    outcome: f64,
    propensity: f64,
}

fn generate_ps_data(n: usize) -> Vec<PsRecord> {
    // use rand_distr::{Normal, Distribution};
    // let mut rng = rand::thread_rng();
    // let age_dist = Normal::new(40.0, 10.0).unwrap();
    // let inc_dist = Normal::new(50.0, 15.0).unwrap();
    (0..n).map(|i| {
        let age = 40.0 + ((i as f64) * 0.01).sin() * 10.0;    // placeholder
        let income = 50.0 + ((i as f64) * 0.013).cos() * 15.0; // placeholder
        let propensity = logistic(0.05 * age + 0.03 * income - 3.5);
        let treatment = propensity > 0.5; // use Bernoulli in real code
        let outcome = 2.0 * treatment as u8 as f64 + 0.5 * age + 0.3 * income;
        PsRecord { treatment, age, income, outcome, propensity }
    }).collect()
}

// Usage:
// let data = generate_ps_data(2000);
// let feats: Vec<Vec<f64>> = data.iter().map(|r| vec![r.age, r.income]).collect();
// let e_x = estimate_propensity_scores(&feats, &[0.05, 0.03], -3.5);
// println!("Propensity score range: [{:.3}, {:.3}]", e_x.iter().cloned().fold(f64::INFINITY, f64::min),
//          e_x.iter().cloned().fold(f64::NEG_INFINITY, f64::max));
```

#### 4.3.2 IPWæ¨å®š

```rust
/// IPW (Horvitz-Thompson) ATE estimator with propensity trimming.
/// Returns (ate, standard_error).
fn ipw_estimator(d: &[bool], y: &[f64], e: &[f64]) -> (f64, f64) {
    let eps = 0.05;

    // Trimming: exclude extreme propensity scores
    let valid: Vec<(bool, f64, f64)> = d.iter().zip(y).zip(e)
        .filter(|((_, _), ei)| **ei > eps && **ei < 1.0 - eps)
        .map(|((di, yi), ei)| (*di, *yi, *ei))
        .collect();
    let n = valid.len() as f64;

    // IPW ATE = E[D*Y/e(X)] - E[(1-D)*Y/(1-e(X))]
    let score: Vec<f64> = valid.iter().map(|(di, yi, ei)| {
        if *di { yi / ei } else { -yi / (1.0 - ei) }
    }).collect();

    let ate = score.iter().sum::<f64>() / n;

    // Variance: Var(score) / n  (Horvitz-Thompson)
    let mean_score = ate;
    let var_score = score.iter().map(|s| (s - mean_score).powi(2)).sum::<f64>() / (n - 1.0);
    let se = (var_score / n).sqrt();

    (ate, se)
}

// Usage:
// let d: Vec<bool> = data.iter().map(|r| r.treatment).collect();
// let y: Vec<f64> = data.iter().map(|r| r.outcome).collect();
// let e: Vec<f64> = data.iter().map(|r| r.propensity).collect();
// let (ate_ipw, se_ipw) = ipw_estimator(&d, &y, &e);
// println!("IPW ATE: {:.3} Â± {:.3} (95% CI)", ate_ipw, 1.96 * se_ipw);
// println!("True ATE: 2.0");
```

#### 4.3.3 Doubly Robustæ¨å®š

```rust
/// Doubly Robust ATE estimator.
/// Requires outcome regression predictions Î¼_1(X), Î¼_0(X) and propensity e(X).
/// Consistent if either the outcome model or propensity model is correctly specified.
fn doubly_robust_estimator(
    d: &[bool],
    y: &[f64],
    e: &[f64],
    mu1: &[f64],  // E[Y | D=1, X] predictions for all observations
    mu0: &[f64],  // E[Y | D=0, X] predictions for all observations
) -> (f64, f64) {
    let n = d.len() as f64;

    // DR score for each observation
    let dr_scores: Vec<f64> = d.iter().zip(y).zip(e).zip(mu1).zip(mu0)
        .map(|((((di, yi), ei), m1), m0)| {
            let term1 = *di as u8 as f64 * (yi - m1) / ei + m1;
            let term0 = (1.0 - *di as u8 as f64) * (yi - m0) / (1.0 - ei) + m0;
            term1 - term0
        })
        .collect();

    let ate_dr = dr_scores.iter().sum::<f64>() / n;

    // Variance estimation
    let var_dr = dr_scores.iter().map(|s| (s - ate_dr).powi(2)).sum::<f64>() / (n * (n - 1.0));
    let se_dr = var_dr.sqrt();

    (ate_dr, se_dr)
}

// Usage:
// let (ate_dr, se_dr) = doubly_robust_estimator(&d, &y, &e, &mu1, &mu0);
// println!("Doubly Robust ATE: {:.3} Â± {:.3} (95% CI)", ate_dr, 1.96 * se_dr);
```

#### 4.3.4 ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯

```rust
/// Standardized Mean Difference (SMD) balance check for a single covariate.
/// Returns (smd_before, smd_after_ipw).
fn balance_check_covariate(x: &[f64], d: &[bool], e: &[f64]) -> (f64, f64) {
    let mean_f = |v: &[f64]| v.iter().sum::<f64>() / v.len() as f64;
    let var_f = |v: &[f64]| {
        let m = mean_f(v);
        v.iter().map(|xi| (xi - m).powi(2)).sum::<f64>() / (v.len() as f64 - 1.0)
    };

    let x1: Vec<f64> = d.iter().zip(x).filter(|(di, _)| **di).map(|(_, xi)| *xi).collect();
    let x0: Vec<f64> = d.iter().zip(x).filter(|(di, _)| !*di).map(|(_, xi)| *xi).collect();
    let std_pooled = ((var_f(&x1) + var_f(&x0)) / 2.0).sqrt();
    let smd_before = (mean_f(&x1) - mean_f(&x0)).abs() / std_pooled;

    // After IPW weighting
    let w1: Vec<f64> = d.iter().zip(e).filter(|(di, _)| **di).map(|(_, ei)| 1.0 / ei).collect();
    let x1_w: Vec<f64> = d.iter().zip(x).zip(e).filter(|((di, _), _)| **di)
        .map(|((_, xi), _)| *xi).collect();
    let w0: Vec<f64> = d.iter().zip(e).filter(|(di, _)| !*di).map(|(_, ei)| 1.0 / (1.0 - ei)).collect();
    let x0_w: Vec<f64> = d.iter().zip(x).zip(e).filter(|((di, _), _)| !**di)
        .map(|((_, xi), _)| *xi).collect();

    let weighted_mean = |xs: &[f64], ws: &[f64]| {
        xs.iter().zip(ws).map(|(xi, wi)| xi * wi).sum::<f64>() / ws.iter().sum::<f64>()
    };
    let m1w = weighted_mean(&x1_w, &w1);
    let m0w = weighted_mean(&x0_w, &w0);
    let var1w = x1_w.iter().zip(&w1).map(|(xi, wi)| wi * (xi - m1w).powi(2)).sum::<f64>()
        / w1.iter().sum::<f64>();
    let var0w = x0_w.iter().zip(&w0).map(|(xi, wi)| wi * (xi - m0w).powi(2)).sum::<f64>()
        / w0.iter().sum::<f64>();
    let std_pooled_w = ((var1w + var0w) / 2.0).sqrt();
    let smd_after = (m1w - m0w).abs() / std_pooled_w;

    (smd_before, smd_after)
}

fn balance_check(xs: &[(&str, &[f64])], d: &[bool], e: &[f64]) {
    println!("\n=== Balance Check ===");
    for (name, x) in xs {
        let (before, after) = balance_check_covariate(x, d, e);
        let status = if after < 0.1 { "âœ…" } else { "âŒ" };
        println!("{}: SMD before={:.3}, after={:.3} {}", name, before, after, status);
    }
}
```

### 4.4 æ“ä½œå¤‰æ•°æ³• (2SLS) å®Ÿè£…

```rust
/// Two-Stage Least Squares (2SLS) estimator using matrix algebra.
/// Stage 1: DÌ‚ = ZÎ³ (project treatment onto instrument)
/// Stage 2: Y = DÌ‚Î² + Îµ (use predicted treatment)
///
/// For production: use the `linfa` crate or `nalgebra` for OLS.
fn two_stage_least_squares_simple(
    y: &[f64],   // outcome
    d: &[f64],   // treatment (endogenous)
    z: &[f64],   // instrument
) -> (f64, f64) {
    let n = y.len() as f64;

    // Stage 1: OLS of D on Z â†’ DÌ‚
    let z_mean = z.iter().sum::<f64>() / n;
    let d_mean = d.iter().sum::<f64>() / n;
    let cov_zd: f64 = z.iter().zip(d).map(|(zi, di)| (zi - z_mean) * (di - d_mean)).sum::<f64>();
    let var_z: f64 = z.iter().map(|zi| (zi - z_mean).powi(2)).sum::<f64>();
    let gamma = cov_zd / var_z;
    let d_hat: Vec<f64> = z.iter().map(|zi| gamma * zi).collect();

    // Stage 2: OLS of Y on DÌ‚ â†’ Î²_2SLS
    let d_hat_mean = d_hat.iter().sum::<f64>() / n;
    let y_mean = y.iter().sum::<f64>() / n;
    let cov_dhat_y: f64 = d_hat.iter().zip(y).map(|(di, yi)| (di - d_hat_mean) * (yi - y_mean)).sum::<f64>();
    let var_dhat: f64 = d_hat.iter().map(|di| (di - d_hat_mean).powi(2)).sum::<f64>();
    let beta_2sls = cov_dhat_y / var_dhat;

    // Approximate SE (sandwich estimator, simplified)
    let residuals: Vec<f64> = y.iter().zip(d).map(|(yi, di)| yi - beta_2sls * di).collect();
    let sigma2 = residuals.iter().map(|r| r.powi(2)).sum::<f64>() / (n - 2.0);
    let se = (sigma2 / var_dhat).sqrt();

    // First-stage F-statistic check
    let f_stat = (gamma * gamma * var_z) / (sigma2 / n);
    if f_stat < 10.0 {
        eprintln!("Warning: Weak IV detected (F = {:.2} < 10)", f_stat);
    } else {
        println!("First-stage F-statistic: {:.2}", f_stat);
    }

    (beta_2sls, se)
}

// Usage:
// let (beta, se) = two_stage_least_squares_simple(&y, &d, &z);
// println!("2SLS estimate: {:.3} Â± {:.3} (95% CI)", beta, 1.96 * se);
// println!("True causal effect: 2.0");
```

### 4.5 RDDå®Ÿè£…

```rust
/// Regression Discontinuity Design estimator.
/// Local linear regression: Y ~ D + X_c + D*X_c (within bandwidth).
/// Returns (rdd_effect, standard_error).
fn regression_discontinuity(
    x: &[f64],   // running variable
    y: &[f64],   // outcome
    cutoff: f64,
    bandwidth: f64,
) -> (f64, f64) {
    // Filter to bandwidth window
    let local: Vec<(f64, f64, f64)> = x.iter().zip(y)
        .filter(|(xi, _)| (*xi - cutoff).abs() <= bandwidth)
        .map(|(xi, yi)| {
            let d = if *xi >= cutoff { 1.0 } else { 0.0 };
            let xc = xi - cutoff;  // centered running variable
            (*yi, d, xc)
        })
        .collect();

    let n = local.len();
    if n < 4 { return (0.0, f64::INFINITY); }

    // Design matrix: [1, D, X_c, D*X_c]
    // OLS: Î² = (X'X)^{-1} X'y  â€” coefficient Î²[1] = RDD effect
    // Simplified: use orthogonal projection for the treatment indicator
    let y_vec: Vec<f64> = local.iter().map(|(yi, _, _)| *yi).collect();
    let d_vec: Vec<f64> = local.iter().map(|(_, di, _)| *di).collect();
    let xc_vec: Vec<f64> = local.iter().map(|(_, _, xi)| *xi).collect();
    let dxc_vec: Vec<f64> = local.iter().map(|(_, di, xi)| di * xi).collect();

    // 4-column OLS via normal equations (nalgebra recommended in production)
    // Simplified: local linear regression Î²_D via two separate local means
    let y1: Vec<f64> = local.iter().filter(|(_, d, _)| *d > 0.5).map(|(y, _, _)| *y).collect();
    let y0: Vec<f64> = local.iter().filter(|(_, d, _)| *d <= 0.5).map(|(y, _, _)| *y).collect();
    let x1: Vec<f64> = local.iter().filter(|(_, d, _)| *d > 0.5).map(|(_, _, x)| *x).collect();
    let x0: Vec<f64> = local.iter().filter(|(_, d, _)| *d <= 0.5).map(|(_, _, x)| *x).collect();

    let ols_slope = |ys: &[f64], xs: &[f64]| -> (f64, f64) {
        let n = ys.len() as f64;
        let mx = xs.iter().sum::<f64>() / n;
        let my = ys.iter().sum::<f64>() / n;
        let cov: f64 = xs.iter().zip(ys).map(|(xi, yi)| (xi - mx) * (yi - my)).sum();
        let var: f64 = xs.iter().map(|xi| (xi - mx).powi(2)).sum();
        let slope = if var.abs() < 1e-10 { 0.0 } else { cov / var };
        (slope, my - slope * mx) // (slope, intercept)
    };

    let (_, intercept1) = ols_slope(&y1, &x1);
    let (_, intercept0) = ols_slope(&y0, &x0);
    let rdd_effect = intercept1 - intercept0;  // jump at cutoff

    // Approximate SE from pooled residual variance
    let nf = n as f64;
    let residuals: Vec<f64> = y_vec.iter().zip(d_vec.iter().zip(xc_vec.iter().zip(&dxc_vec)))
        .map(|(yi, (di, (xi, dxi)))| yi - rdd_effect * di - 0.5 * xi - 0.1 * dxi)
        .collect();
    let sigma2 = residuals.iter().map(|r| r.powi(2)).sum::<f64>() / (nf - 4.0);
    let var_d: f64 = d_vec.iter().map(|di| {
        let m = d_vec.iter().sum::<f64>() / nf;
        (di - m).powi(2)
    }).sum::<f64>();
    let se = (sigma2 / var_d).sqrt();

    (rdd_effect, se)
}

// Usage:
// let data = generate_rdd_data(2000, 18.0);
// let (effect, se) = regression_discontinuity(&data.age, &data.outcome, 18.0, 2.0);
// println!("RDD estimate (h=2): {:.3} Â± {:.3} (95% CI)", effect, 1.96 * se);
// println!("True effect: 3.0");
// for h in [1.0_f64, 1.5, 2.0, 2.5, 3.0] {
//     let (eff, _) = regression_discontinuity(&data.age, &data.outcome, 18.0, h);
//     println!("  h={}: RDD effect = {:.3}", h, eff);
// }
```

### 4.6 DiDå®Ÿè£…

```rust
/// Difference-in-Differences estimator using the 2Ã—2 DiD formula.
/// DiD = (È²_{treated,post} - È²_{treated,pre}) - (È²_{control,post} - È²_{control,pre})
/// For regression-based DiD (with controls), use linfa or nalgebra in production.
fn difference_in_differences(
    y: &[f64],
    treated: &[bool],  // group indicator: true = treated group
    post: &[bool],     // time indicator: true = post period
) -> (f64, f64) {
    let mean_f = |v: &[f64]| v.iter().sum::<f64>() / v.len() as f64;

    let cell = |t: bool, p: bool| -> Vec<f64> {
        y.iter().zip(treated).zip(post)
            .filter(|((_, ti), pi)| **ti == t && **pi == p)
            .map(|((yi, _), _)| *yi)
            .collect()
    };

    let y11 = cell(true, true);    // treated, post
    let y10 = cell(true, false);   // treated, pre
    let y01 = cell(false, true);   // control, post
    let y00 = cell(false, false);  // control, pre

    let did_effect = (mean_f(&y11) - mean_f(&y10)) - (mean_f(&y01) - mean_f(&y00));

    // SE: simplified pooled variance
    let n = y.len() as f64;
    let residuals: Vec<f64> = y.iter().zip(treated).zip(post)
        .map(|((yi, ti), pi)| {
            let group_mean = if *ti { mean_f(&y10) } else { mean_f(&y00) };
            yi - group_mean - did_effect * (*ti && *pi) as u8 as f64
        })
        .collect();
    let sigma2 = residuals.iter().map(|r| r.powi(2)).sum::<f64>() / (n - 4.0);
    let se = (sigma2 / n).sqrt();

    (did_effect, se)
}

// Usage:
// let (did_effect, se_did) = difference_in_differences(&outcome, &treated, &post);
// println!("DiD estimate: {:.3} Â± {:.3} (95% CI)", did_effect, 1.96 * se_did);
// println!("True effect: 3.0");

fn event_study_placeholder() {
    // Full implementation requires panel data with multiple pre/post periods.
    // Each period's effect is estimated relative to the base period.
    println!("Event study plot would show pre-treatment trends here");
}
```

### 4.7 Causal Forestå®Ÿè£… (ç°¡æ˜“ç‰ˆ)

```rust
// Simplified Causal Forest (T-Learner) implementation.
// For production: use the `smartcore` crate or bindings to R's grf package.

struct HteRecord {
    x1: f64,
    x2: f64,
    treatment: bool,
    outcome: f64,
    true_cate: f64,
}

/// T-Learner: fit separate outcome models for treated/control,
/// then CATE(x) = Î¼Ì‚â‚(x) - Î¼Ì‚â‚€(x).
fn t_learner_ate(
    x1: &[f64],
    x2: &[f64],
    y: &[f64],
    d: &[bool],
) -> (f64, Vec<f64>) {
    let mean_f = |v: &[f64]| v.iter().sum::<f64>() / v.len() as f64;

    // Fit linear Î¼â‚(X) = a + b*x1 + c*x2 on treated subset
    let treated: Vec<(f64, f64, f64)> = d.iter().zip(x1).zip(x2).zip(y)
        .filter(|(((di, _), _), _)| **di)
        .map(|(((_, xi1), xi2), yi)| (*xi1, *xi2, *yi))
        .collect();
    let control: Vec<(f64, f64, f64)> = d.iter().zip(x1).zip(x2).zip(y)
        .filter(|(((di, _), _), _)| !**di)
        .map(|(((_, xi1), xi2), yi)| (*xi1, *xi2, *yi))
        .collect();

    // Simplified: use group mean + covariate means as Î¼Ì‚
    let mu1_mean = mean_f(&treated.iter().map(|(_, _, y)| *y).collect::<Vec<_>>());
    let mu0_mean = mean_f(&control.iter().map(|(_, _, y)| *y).collect::<Vec<_>>());

    // CATE for each observation (simplified linear model)
    let cate: Vec<f64> = x1.iter().zip(x2)
        .map(|(xi1, xi2)| (mu1_mean + 2.0 * xi1 + xi2) - (mu0_mean + 2.0 * xi1 + xi2))
        .collect();

    let ate_cf = mean_f(&cate);
    (ate_cf, cate)
}

fn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {
    let n = a.len() as f64;
    let ma = a.iter().sum::<f64>() / n;
    let mb = b.iter().sum::<f64>() / n;
    let cov: f64 = a.iter().zip(b).map(|(ai, bi)| (ai - ma) * (bi - mb)).sum();
    let sa: f64 = a.iter().map(|ai| (ai - ma).powi(2)).sum::<f64>().sqrt();
    let sb: f64 = b.iter().map(|bi| (bi - mb).powi(2)).sum::<f64>().sqrt();
    cov / (sa * sb)
}

// Usage:
// let (ate_cf, cate_cf) = t_learner_ate(&x1, &x2, &outcome, &treatment);
// println!("Causal Forest ATE: {:.3}", ate_cf);
// let corr = pearson_correlation(&cate_cf, &true_cate);
// println!("Correlation(estimated CATE, true CATE): {:.3}", corr);
```

### 4.8 çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ â€” è¤‡æ•°æ‰‹æ³•ã®æ¯”è¼ƒ

```rust
#[derive(Debug, Clone, Copy, PartialEq)]
enum Scenario { Propensity, Iv, Rdd, Did }

fn causal_inference_pipeline(scenario: Scenario,
    y: &[f64], d: &[bool], e: &[f64],
    mu1: Option<&[f64]>, mu0: Option<&[f64]>,
    z: Option<&[f64]>, x: Option<&[f64]>,
    cutoff: Option<f64>, bandwidth: Option<f64>,
    post: Option<&[bool]>,
) {
    match scenario {
        Scenario::Propensity => {
            println!("\n=== Causal Inference Pipeline: Propensity ===\n");
            let (ate_ipw, se_ipw) = ipw_estimator(d, y, e);
            println!("IPW ATE: {:.3} Â± {:.3}", ate_ipw, 1.96 * se_ipw);
            if let (Some(m1), Some(m0)) = (mu1, mu0) {
                let (ate_dr, se_dr) = doubly_robust_estimator(d, y, e, m1, m0);
                println!("DR  ATE: {:.3} Â± {:.3}", ate_dr, 1.96 * se_dr);
            }
        }
        Scenario::Iv => {
            println!("\n=== Causal Inference Pipeline: IV ===\n");
            let d_f64: Vec<f64> = d.iter().map(|&di| di as u8 as f64).collect();
            let (beta, se) = two_stage_least_squares_simple(y, &d_f64, z.unwrap());
            println!("2SLS estimate: {:.3} Â± {:.3}", beta, 1.96 * se);
        }
        Scenario::Rdd => {
            println!("\n=== Causal Inference Pipeline: RDD ===\n");
            let (eff, se) = regression_discontinuity(x.unwrap(), y, cutoff.unwrap_or(0.0), bandwidth.unwrap_or(2.0));
            println!("RDD estimate: {:.3} Â± {:.3}", eff, 1.96 * se);
        }
        Scenario::Did => {
            println!("\n=== Causal Inference Pipeline: DiD ===\n");
            let (eff, se) = difference_in_differences(y, d, post.unwrap());
            println!("DiD estimate: {:.3} Â± {:.3}", eff, 1.96 * se);
        }
    }
}
```

> **Note:** **é€²æ—: 70% å®Œäº†** Rustå› æœæ¨è«–ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã‚’å®Ÿè£…ã—ãŸã€‚DAG/do-æ¼”ç®—/å‚¾å‘ã‚¹ã‚³ã‚¢/IV/RDD/DiD/Causal Forestã®å…¨æ‰‹æ³•ã‚’ndarrayã§å®Ÿè£…ã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã«é©ç”¨ã™ã‚‹ã€‚

---


> Progress: [85%]
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°å¾Œã®ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ã§æ¨™æº–åŒ–å·®ï¼ˆSMDï¼‰ãŒ0.1æœªæº€ã‚’ç›®å®‰ã«ã™ã‚‹ç†ç”±ã¯ï¼Ÿ
> 2. 2SLSæ¨å®šé‡ã®ç¬¬ä¸€æ®µéšFçµ±è¨ˆé‡ãŒ10æœªæº€ã®ã¨ãã€Œå¼±æ“ä½œå¤‰æ•°ã€ã¨åˆ¤å®šã•ã‚Œã‚‹æ ¹æ‹ ã¯ï¼Ÿ

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” å®Ÿãƒ‡ãƒ¼ã‚¿å› æœæ¨è«–ãƒãƒ£ãƒ¬ãƒ³ã‚¸

### 5.1 ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã§å…¨æ‰‹æ³•æ¯”è¼ƒ

```rust
// Generate comprehensive causal inference test data (true ATE = 5.0)
struct CausalRecord {
    treatment: bool,
    outcome: f64,
    age: f64,
    income: f64,
    instrument: bool,
    propensity: f64,
}

fn logistic(x: f64) -> f64 { 1.0 / (1.0 + (-x).exp()) }
fn mean_f(v: &[f64]) -> f64 { v.iter().sum::<f64>() / v.len() as f64 }

fn comprehensive_causal_data(n: usize) -> Vec<CausalRecord> {
    // use rand_distr::{Normal, Bernoulli, Distribution};
    (0..n).map(|i| {
        let age = 40.0 + ((i as f64) * 0.01).sin() * 12.0;
        let income = 50.0 + ((i as f64) * 0.007).cos() * 20.0;
        let propensity = logistic(0.05 * age + 0.03 * income - 3.0);
        let treatment = propensity > 0.5;
        let instrument = i % 2 == 0;
        let outcome = 5.0 * treatment as u8 as f64 + 0.3 * age + 0.2 * income;
        CausalRecord { treatment, outcome, age, income, instrument, propensity }
    }).collect()
}

// Method comparison: true ATE = 5.0
fn compare_methods(data: &[CausalRecord]) {
    let y: Vec<f64> = data.iter().map(|r| r.outcome).collect();
    let d: Vec<bool> = data.iter().map(|r| r.treatment).collect();
    let e: Vec<f64> = data.iter().map(|r| r.propensity).collect();

    // Method 1: Naive comparison
    let y1: Vec<f64> = data.iter().filter(|r| r.treatment).map(|r| r.outcome).collect();
    let y0: Vec<f64> = data.iter().filter(|r| !r.treatment).map(|r| r.outcome).collect();
    let ate_naive = mean_f(&y1) - mean_f(&y0);

    // Method 2: IPW
    let (ate_ipw, se_ipw) = ipw_estimator(&d, &y, &e);

    println!("\n=== Method Comparison ===");
    println!("True ATE: 5.0");
    println!("Naive: {:.3}", ate_naive);
    println!("IPW: {:.3} Â± {:.3}", ate_ipw, 1.96 * se_ipw);
    // Method 3: Doubly Robust (requires fitted Î¼â‚, Î¼â‚€)
    // let (ate_dr, se_dr) = doubly_robust_estimator(&d, &y, &e, &mu1, &mu0);
    // println!("Doubly Robust: {:.3} Â± {:.3}", ate_dr, 1.96 * se_dr);
}
```

### 5.2 æ„Ÿåº¦åˆ†æ â€” æœªæ¸¬å®šäº¤çµ¡ã¸ã®é ‘å¥æ€§

```rust
/// Rosenbaum's Î“ sensitivity analysis (simplified).
/// For each Î“, widens the confidence interval by the odds ratio of hidden bias.
/// If CI still excludes 0, estimate is robust to that level of confounding.
fn sensitivity_analysis_gamma(ate: f64, se: f64, gamma_range: &[f64]) {
    println!("\n=== Sensitivity Analysis (Rosenbaum's Î“) ===");
    println!("Î“ = odds ratio of differential treatment assignment due to unobserved confounder");

    for &gamma in gamma_range {
        // Scale SE by Î“ to approximate bounds under hidden confounding
        let ci_lower = ate - 1.96 * se * gamma;
        let ci_upper = ate + 1.96 * se * gamma;
        let significant = ci_lower > 0.0 || ci_upper < 0.0;
        let status = if significant { "âœ… Still significant" } else { "âŒ Not significant" };
        println!("Î“={:.1}: CI = [{:.2}, {:.2}] {}", gamma, ci_lower, ci_upper, status);
    }
}

// Usage:
// sensitivity_analysis_gamma(ate_dr, se_dr, &[1.0, 1.5, 2.0, 2.5, 3.0]);
```

### 5.3 A/Bãƒ†ã‚¹ãƒˆçµ±åˆ â€” Sample Ratio Mismatchæ¤œå‡º

```rust
/// Sample Ratio Mismatch (SRM) test via chi-square goodness-of-fit.
/// Returns (chi_sq, approximate_p_value).
fn sample_ratio_mismatch_test(treated: &[bool], expected_ratio: f64) -> (f64, f64) {
    let n_total = treated.len() as f64;
    let n_treated = treated.iter().filter(|&&t| t).count() as f64;
    let n_control = n_total - n_treated;
    let observed_ratio = n_treated / n_total;

    // Chi-square test
    let expected_treated = n_total * expected_ratio;
    let expected_control = n_total * (1.0 - expected_ratio);
    let chi_sq = (n_treated - expected_treated).powi(2) / expected_treated
               + (n_control - expected_control).powi(2) / expected_control;

    // Approximate p-value for Ï‡Â²(1) using Wilson-Hilferty approximation
    let p_value = (-chi_sq / 2.0).exp(); // rough approximation

    println!("\n=== Sample Ratio Mismatch Test ===");
    println!("Expected ratio: {:.4}", expected_ratio);
    println!("Observed ratio: {:.4}", observed_ratio);
    println!("Ï‡Â² = {:.3}, p â‰ˆ {:.4}", chi_sq, p_value);
    if p_value < 0.05 {
        println!("âš ï¸ SRM detected! Treatment assignment may be biased.");
    } else {
        println!("âœ… No SRM detected.");
    }

    (chi_sq, p_value)
}
```

### 5.4 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### ãƒ†ã‚¹ãƒˆ1: è¨˜æ³•ç†è§£ï¼ˆ10å•ï¼‰

<details><summary>Q1: $\mathbb{E}[Y^1 - Y^0]$ ã¯ä½•ã‚’è¡¨ã™ã‹ï¼Ÿ</summary>

**Answer**: ATE (Average Treatment Effect) â€” å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ

$$
\text{ATE} = \mathbb{E}[Y^1 - Y^0] = \mathbb{E}[Y^1] - \mathbb{E}[Y^0]
$$

**è£œè¶³**: ã“ã‚Œã¯å€‹ä½“ãƒ¬ãƒ™ãƒ«ã®å‡¦ç½®åŠ¹æœ $\tau_i = Y_i^1 - Y_i^0$ ã®æœŸå¾…å€¤ã€‚å€‹ä½“ãƒ¬ãƒ™ãƒ«ã¯è¦³æ¸¬ä¸èƒ½ï¼ˆæ ¹æœ¬çš„å› æœæ¨è«–å•é¡Œï¼‰ã ãŒã€é›†å›£å¹³å‡ãªã‚‰æ¨å®šå¯èƒ½ã€‚

</details>

<details><summary>Q2: $P(Y \mid do(X=x))$ ã¨ $P(Y \mid X=x)$ ã®é•ã„ã¯ï¼Ÿ</summary>

**Answer**:
- $P(Y \mid do(X=x))$: **ä»‹å…¥ç¢ºç‡** â€” $X$ ã‚’å¤–éƒ¨ã‹ã‚‰å¼·åˆ¶çš„ã« $x$ ã«å›ºå®šã—ãŸå ´åˆã® $Y$ ã®åˆ†å¸ƒ
- $P(Y \mid X=x)$: **æ¡ä»¶ä»˜ãç¢ºç‡** â€” $X=x$ ã‚’è¦³æ¸¬ã—ãŸå ´åˆã® $Y$ ã®åˆ†å¸ƒï¼ˆäº¤çµ¡ã‚ã‚Šï¼‰

ä»‹å…¥ç¢ºç‡ã¯å› æœåŠ¹æœã€æ¡ä»¶ä»˜ãç¢ºç‡ã¯ç›¸é–¢ã‚’è¡¨ã™ã€‚

**ä¾‹**: å–«ç…™ã¨ãŒã‚“
- $P(\text{ãŒã‚“} \mid \text{å–«ç…™}=1)$: å–«ç…™è€…ã®ãŒã‚“ç‡ï¼ˆéºä¼ã®äº¤çµ¡ã‚ã‚Šï¼‰
- $P(\text{ãŒã‚“} \mid do(\text{å–«ç…™}=1))$: å¼·åˆ¶çš„ã«å–«ç…™ã•ã›ãŸå ´åˆã®ãŒã‚“ç‡ï¼ˆå› æœåŠ¹æœï¼‰

å‰è€…ã¯ç›¸é–¢ã€å¾Œè€…ã¯å› æœã€‚Simpson's Paradoxã§ã¯ä¸¡è€…ãŒé€†è»¢ã™ã‚‹ã“ã¨ã™ã‚‰ã‚ã‚‹ã€‚

</details>

<details><summary>Q3: $e(X) = P(D=1 \mid X)$ ã®åå‰ã¨å½¹å‰²ã¯ï¼Ÿ</summary>

**Answer**: **å‚¾å‘ã‚¹ã‚³ã‚¢ (Propensity Score)**

é«˜æ¬¡å…ƒã®å…±å¤‰é‡ $X$ ã‚’1æ¬¡å…ƒã®ã‚¹ã‚«ãƒ©ãƒ¼ã«åœ§ç¸®ã€‚$(Y^1, Y^0) \perp\!\!\!\perp D \mid X$ ãªã‚‰ $(Y^1, Y^0) \perp\!\!\!\perp D \mid e(X)$ ã‚‚æˆç«‹ï¼ˆæ¬¡å…ƒå‰Šæ¸›ï¼‰ã€‚

**å®Ÿç”¨ä¸Šã®ãƒ¡ãƒªãƒƒãƒˆ**:
- $X$ ãŒ10æ¬¡å…ƒã§ã‚‚ $e(X)$ ã¯1æ¬¡å…ƒ â†’ ãƒãƒƒãƒãƒ³ã‚°ãŒå®¹æ˜“
- å…±é€šã‚µãƒãƒ¼ãƒˆ $0 < e(X) < 1$ ã®ç¢ºèªãŒç°¡å˜
- IPWæ¨å®šã§ $1/e(X)$ ã®é‡ã¿ã‚’ä½¿ã†ã ã‘ã§å› æœåŠ¹æœæ¨å®šå¯èƒ½

</details>

<details><summary>Q4: SUTVAã®2ã¤ã®ä»®å®šã‚’è¿°ã¹ã‚ˆ</summary>

**Answer**:
1. **å‡¦ç½®ã®ä¸€æ„æ€§**: å€‹ä½“ $i$ ã®å‡¦ç½®ãŒ $d$ ã®ã¨ãã€çµæœã¯ $Y_i^d$ ã®1ã¤ã®ã¿
2. **å¹²æ¸‰ãªã— (No Interference)**: å€‹ä½“ $i$ ã®çµæœã¯ä»–ã®å€‹ä½“ã®å‡¦ç½®ã«ä¾å­˜ã—ãªã„

$$
Y_i^d = Y_i^{d_i} \quad \forall d_{-i}
$$

**ç ´ã‚Œã‚‹ä¾‹**:
- ãƒ¯ã‚¯ãƒãƒ³æ¥ç¨®: ä»–äººãŒæ¥ç¨®ã™ã‚‹ã¨è‡ªåˆ†ã®æ„ŸæŸ“ãƒªã‚¹ã‚¯ä½ä¸‹ï¼ˆå¹²æ¸‰ã‚ã‚Šï¼‰
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åºƒå‘Š: å‹äººãŒã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨è‡ªåˆ†ã‚‚ã‚¯ãƒªãƒƒã‚¯ï¼ˆspilloveråŠ¹æœï¼‰
- æ•™å®¤å†…ã®å‡¦ç½®: åŒã˜ã‚¯ãƒ©ã‚¹ã®å­¦ç”Ÿé–“ã§ç›¸äº’å½±éŸ¿

SUTVAãŒç ´ã‚Œã‚‹å ´åˆã¯ã€**Spillover Effects** ã‚„ **Network Effects** ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

</details>

<details><summary>Q5: ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã‚’æº€ãŸã™å¤‰æ•°é›†åˆ $Z$ ã®æ¡ä»¶ã¯ï¼Ÿ</summary>

**Answer**:
1. $Z$ ã®ã©ã®å¤‰æ•°ã‚‚ $X$ ã®å­å­«ã§ãªã„
2. $Z$ ãŒ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ã‚’é®æ–­ã™ã‚‹

æº€ãŸã›ã°:

$$
P(Y \mid do(X=x)) = \sum_z P(Y \mid X=x, Z=z) P(Z=z)
$$

**ç›´æ„Ÿ**:
- æ¡ä»¶1: $X$ ã®çµæœ ($X$ ã®å­å­«) ã§æ¡ä»¶ã¥ã‘ã‚‹ã¨ã€Collider BiasãŒç™ºç”Ÿã™ã‚‹
- æ¡ä»¶2: ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ï¼ˆ$X \leftarrow \cdots \to Y$ï¼‰ã‚’é®æ–­ã—ãªã„ã¨äº¤çµ¡ãŒæ®‹ã‚‹

**ä¾‹**: å–«ç…™â†’ãŒã‚“ã€ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹: å–«ç…™â†éºä¼â†’ãŒã‚“
- $Z = \{\text{éºä¼}\}$ ã§æ¡ä»¶ã¥ã‘ã‚‹ã¨ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ãŒé®æ–­ã•ã‚Œã‚‹
- $Z = \{\text{ã‚¿ãƒ¼ãƒ«æ²ˆç€}\}$ (å–«ç…™ã®çµæœ) ã§æ¡ä»¶ã¥ã‘ã‚‹ã¨Collider BiasãŒç™ºç”Ÿ

</details>

<details><summary>Q6: d-åˆ†é›¢ã¨ã¯ä½•ã‹ï¼Ÿ</summary>

**Answer**: DAGä¸Šã§å¤‰æ•°é›†åˆ $Z$ ãŒ $X$ ã¨ $Y$ ã‚’ d-åˆ†é›¢ã™ã‚‹ $\iff$ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒ‘ã‚¹ãŒ $Z$ ã«ã‚ˆã£ã¦é®æ–­ã•ã‚Œã‚‹ã€‚

**ãƒ‘ã‚¹é®æ–­æ¡ä»¶**:
- **Chain** $X \to Z \to Y$: $Z \in \mathcal{Z}$ ãªã‚‰é®æ–­
- **Fork** $X \leftarrow Z \to Y$: $Z \in \mathcal{Z}$ ãªã‚‰é®æ–­
- **Collider** $X \to Z \leftarrow Y$: $Z \notin \mathcal{Z}$ ã‹ã¤ $\text{DE}(Z) \cap \mathcal{Z} = \emptyset$ ãªã‚‰é®æ–­

**d-åˆ†é›¢ã®é‡è¦æ€§**: $X \perp_d Y \mid Z$ (d-åˆ†é›¢) $\Rightarrow$ $X \perp\!\!\!\perp Y \mid Z$ (æ¡ä»¶ä»˜ãç‹¬ç«‹)

</details>

<details><summary>Q7: Colliderã§æ¡ä»¶ã¥ã‘ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ï¼Ÿ</summary>

**Answer**: **é¸æŠãƒã‚¤ã‚¢ã‚¹** â€” ç‹¬ç«‹ã ã£ãŸå¤‰æ•°ãŒæ¡ä»¶ä»˜ãã§ç›¸é–¢ã™ã‚‹

**ä¾‹**: æ‰èƒ½ã¨åŠªåŠ›

```mermaid
graph TD
    T["æ‰èƒ½"] --> A["åˆæ ¼"]
    E["åŠªåŠ›"] --> A
```

æ‰èƒ½ã¨åŠªåŠ›ã¯ç‹¬ç«‹ $T \perp\!\!\!\perp E$ ã ãŒã€åˆæ ¼è€… $A=1$ ã‚’æ¡ä»¶ã¥ã‘ã‚‹ã¨:

$$
T \not\perp\!\!\!\perp E \mid A=1
$$

åˆæ ¼è€…ã®ä¸­ã§ã¯ã€ŒåŠªåŠ›ãŒå°‘ãªã„â†’æ‰èƒ½ãŒé«˜ã„ã€ã¨ã„ã†è² ã®ç›¸é–¢ãŒç”Ÿã¾ã‚Œã‚‹ã€‚ã“ã‚ŒãŒ**Berkson's Paradox**ã€‚

**å®Ÿç”¨ä¾‹**: ç—…é™¢æ‚£è€…ãƒ‡ãƒ¼ã‚¿ã§ç–¾æ‚£Aã¨ç–¾æ‚£BãŒè² ã®ç›¸é–¢ â†’ å…¥é™¢ï¼ˆColliderï¼‰ã§æ¡ä»¶ã¥ã‘ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚

</details>

<details><summary>Q8: Unconfoundednessä»®å®šã¨ã¯ï¼Ÿ</summary>

**Answer**: $(Y^1, Y^0) \perp\!\!\!\perp D \mid X$

å…±å¤‰é‡ $X$ ã‚’æ‰€ä¸ã¨ã™ã‚Œã°ã€æ½œåœ¨çš„çµæœã¨å‡¦ç½®å‰²ã‚Šå½“ã¦ãŒç‹¬ç«‹ã€‚

**æ„å‘³**: $X$ ã‚’åˆ¶å¾¡ã™ã‚Œã°ã€å‡¦ç½®ã¯ãƒ©ãƒ³ãƒ€ãƒ å‰²ã‚Šå½“ã¦ã¨åŒç­‰ï¼ˆselection on observablesï¼‰ã€‚

**æˆã‚Šç«‹ã¤æ¡ä»¶**:
- ã™ã¹ã¦ã®äº¤çµ¡å› å­ $X$ ã‚’æ¸¬å®šã—ã¦ã„ã‚‹
- æœªæ¸¬å®šäº¤çµ¡ $U$ ãŒå­˜åœ¨ã—ãªã„

**ç ´ã‚Œã‚‹ä¾‹**: èƒ½åŠ› $U$ ãŒæœªæ¸¬å®šã§ã€$U \to D$ ã‹ã¤ $U \to Y$ ãªã‚‰ Unconfoundedness ã¯æˆã‚Šç«‹ãŸãªã„ â†’ IV/RDD/DiDãªã©ä»–ã®æ‰‹æ³•ãŒå¿…è¦

</details>

<details><summary>Q9: LATEã¨ATEã®é•ã„ã¯ï¼Ÿ</summary>

**Answer**:
- **ATE**: $\mathbb{E}[Y^1 - Y^0]$ â€” å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ
- **LATE**: $\mathbb{E}[Y^1 - Y^0 \mid \text{Complier}]$ â€” ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ï¼ˆæ“ä½œå¤‰æ•°ã«å¾“ã†äººï¼‰ã®å‡¦ç½®åŠ¹æœ

**IVã§æ¨å®šã•ã‚Œã‚‹ã®ã¯LATE**:

$$
\text{LATE} = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0]}
$$

**4ã¤ã®ã‚¿ã‚¤ãƒ—**:
- Always-Taker: å¸¸ã«å‡¦ç½®ã‚’å—ã‘ã‚‹ï¼ˆIVã«ç„¡é–¢ä¿‚ï¼‰
- Never-Taker: å¸¸ã«å‡¦ç½®ã‚’å—ã‘ãªã„ï¼ˆIVã«ç„¡é–¢ä¿‚ï¼‰
- **Complier**: IVã«å¾“ã†ï¼ˆLATEã®å¯¾è±¡ï¼‰
- Defier: IVã«é€†ã‚‰ã†ï¼ˆMonotonicityä»®å®šã§æ’é™¤ï¼‰

**ATE vs LATE**: LATEã¯ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ã®ã¿ã®åŠ¹æœãªã®ã§ã€ATEã‚ˆã‚Šå±€æ‰€çš„ã€‚å¤–éƒ¨å¦¥å½“æ€§ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

</details>

<details><summary>Q10: ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®šã¨ã¯ï¼Ÿ</summary>

**Answer**: DiDã®è­˜åˆ¥ä»®å®š

$$
\mathbb{E}[Y_{01} - Y_{00} \mid G=1] = \mathbb{E}[Y_{01} - Y_{00} \mid G=0]
$$

å‡¦ç½®ãŒãªã‹ã£ãŸå ´åˆã€å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯å¹³è¡Œã€‚

**ç›´æ„Ÿ**: å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã¯å‡¦ç½®å‰ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãŒåŒã˜ â†’ å‡¦ç½®å¾Œã®å·®åˆ†ã¯å‡¦ç½®åŠ¹æœ

**æ¤œè¨¼æ–¹æ³•**:
- Event Study: å‡¦ç½®å‰ã®è¤‡æ•°æœŸé–“ã§ãƒˆãƒ¬ãƒ³ãƒ‰ãŒå¹³è¡Œã‹ç¢ºèª
- Placebo Test: å‡¦ç½®å‰æœŸé–“ã§ã€Œå½ã®å‡¦ç½®ã€ã‚’è¨­å®šã—ã€åŠ¹æœãŒã‚¼ãƒ­ã‹ç¢ºèª

**ç ´ã‚Œã‚‹ä¾‹**: å‡¦ç½®ç¾¤ãŒé«˜æˆé•·ä¼æ¥­ã€å¯¾ç…§ç¾¤ãŒä½æˆé•·ä¼æ¥­ â†’ ã‚‚ã¨ã‚‚ã¨ãƒˆãƒ¬ãƒ³ãƒ‰ãŒç•°ãªã‚‹ â†’ DiDã¯é©ç”¨ä¸å¯

</details>

#### ãƒ†ã‚¹ãƒˆ2: æ•°å¼å°å‡ºï¼ˆ5å•ï¼‰

<details><summary>Q1: IPWæ¨å®šé‡ãŒä¸åã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã›</summary>

**Proof**:

$$
\begin{aligned}
\mathbb{E}\left[\frac{D Y}{e(X)}\right] &= \mathbb{E}\left[\mathbb{E}\left[\frac{D Y}{e(X)} \mid X\right]\right] \\
&= \mathbb{E}\left[\frac{\mathbb{E}[D Y \mid X]}{e(X)}\right] \\
&= \mathbb{E}\left[\frac{P(D=1 \mid X) \mathbb{E}[Y \mid D=1, X]}{e(X)}\right] \\
&= \mathbb{E}\left[\frac{e(X) \mathbb{E}[Y^1 \mid X]}{e(X)}\right] \quad \text{(unconfoundedness)} \\
&= \mathbb{E}[Y^1]
\end{aligned}
$$

åŒæ§˜ã« $\mathbb{E}\left[\frac{(1-D) Y}{1-e(X)}\right] = \mathbb{E}[Y^0]$ã€‚ã‚ˆã£ã¦:

$$
\mathbb{E}[\hat{\text{ATE}}_{\text{IPW}}] = \mathbb{E}[Y^1] - \mathbb{E}[Y^0] = \text{ATE}
$$

**Key Stepè§£èª¬**:
- Step 3â†’4: Unconfoundedness $(Y^1, Y^0) \perp\!\!\!\perp D \mid X$ ã«ã‚ˆã‚Š $\mathbb{E}[Y \mid D=1, X] = \mathbb{E}[Y^1 \mid X]$
- Step 4â†’5: $e(X) = P(D=1 \mid X)$ ãªã®ã§ç´„åˆ†
- Overlapä»®å®š $0 < e(X) < 1$ ãŒå¿…é ˆï¼ˆåˆ†æ¯ãŒã‚¼ãƒ­ã«ãªã‚‰ãªã„ï¼‰

</details>

<details><summary>Q2: 2SLSæ¨å®šé‡ã‚’å°å‡ºã›ã‚ˆï¼ˆWaldæ¨å®šé‡å½¢å¼ï¼‰</summary>

**Derivation**:

æ§‹é€ æ–¹ç¨‹å¼:

$$
\begin{aligned}
D &= \pi_0 + \pi_1 Z + \nu \\
Y &= \beta_0 + \beta_1 D + U
\end{aligned}
$$

$U$ ã¨ $Z$ ãŒç„¡ç›¸é–¢ï¼ˆå¤–ç”Ÿæ€§ï¼‰ã€$Z$ ã¨ $D$ ãŒç›¸é–¢ï¼ˆé–¢é€£æ€§ï¼‰ã‚’ä»®å®šã€‚

$$
\begin{aligned}
\text{Cov}(Y, Z) &= \text{Cov}(\beta_0 + \beta_1 D + U, Z) \\
&= \beta_1 \text{Cov}(D, Z) + \text{Cov}(U, Z) \\
&= \beta_1 \text{Cov}(D, Z) \quad \text{(å¤–ç”Ÿæ€§: } \text{Cov}(U,Z)=0)
\end{aligned}
$$

$$
\hat{\beta}_1 = \frac{\text{Cov}(Y, Z)}{\text{Cov}(D, Z)} = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0]}
$$

ã“ã‚ŒãŒ2SLSæ¨å®šé‡ï¼ˆWaldæ¨å®šé‡ï¼‰ã€‚

**ç›´æ„Ÿ**:
- åˆ†å­: IVãŒ $Y$ ã«ä¸ãˆã‚‹ç·åŠ¹æœï¼ˆreduced formï¼‰
- åˆ†æ¯: IVãŒ $D$ ã«ä¸ãˆã‚‹åŠ¹æœï¼ˆfirst stageï¼‰
- æ¯”: $D$ ãŒ $Y$ ã«ä¸ãˆã‚‹å› æœåŠ¹æœï¼ˆstructural effectï¼‰

**æ¡ä»¶**:
- å¤–ç”Ÿæ€§: $\text{Cov}(U, Z) = 0$
- é–¢é€£æ€§: $\text{Cov}(D, Z) \neq 0$ (å¼±IVãªã‚‰åˆ†æ¯ãŒå°ã•ããƒã‚¤ã‚¢ã‚¹å¤§)
- æ’é™¤åˆ¶ç´„: $Z \to Y$ ã®ç›´æ¥ãƒ‘ã‚¹ãªã—

</details>

<details><summary>Q3: DiDæ¨å®šé‡ã‚’å°å‡ºã›ã‚ˆ</summary>

**Setup**: 2æœŸé–“ $t \in \{0, 1\}$, 2ã‚°ãƒ«ãƒ¼ãƒ— $G \in \{0, 1\}$

æ½œåœ¨çš„çµæœ:
- $Y_{it}^0$: å‡¦ç½®ãªã—ã®çµæœ
- $Y_{it}^1$: å‡¦ç½®ã‚ã‚Šã®çµæœ

è¦³æ¸¬çµæœ:

$$
Y_{it} = \begin{cases}
Y_{it}^0 & \text{if } G=0 \text{ or } t=0 \\
Y_{it}^1 & \text{if } G=1 \text{ and } t=1
\end{cases}
$$

**DiDæ¨å®šé‡**:

$$
\begin{aligned}
\hat{\tau}_{\text{DiD}} &= (\mathbb{E}[Y_{11}] - \mathbb{E}[Y_{10}]) - (\mathbb{E}[Y_{01}] - \mathbb{E}[Y_{00}]) \\
&= (\mathbb{E}[Y_{11}^1 \mid G=1] - \mathbb{E}[Y_{10}^0 \mid G=1]) \\
&\quad - (\mathbb{E}[Y_{01}^0 \mid G=0] - \mathbb{E}[Y_{00}^0 \mid G=0])
\end{aligned}
$$

**ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®š**:

$$
\mathbb{E}[Y_{11}^0 - Y_{10}^0 \mid G=1] = \mathbb{E}[Y_{01}^0 - Y_{00}^0 \mid G=0]
$$

å‡¦ç½®ãŒãªã‹ã£ãŸå ´åˆã®ãƒˆãƒ¬ãƒ³ãƒ‰ãŒå¹³è¡Œ â†’ ã“ã‚Œã‚’ä½¿ã†ã¨:

$$
\begin{aligned}
\hat{\tau}_{\text{DiD}} &= \mathbb{E}[Y_{11}^1 - Y_{10}^0 \mid G=1] - (\mathbb{E}[Y_{11}^0 - Y_{10}^0 \mid G=1]) \\
&= \mathbb{E}[Y_{11}^1 - Y_{11}^0 \mid G=1] \\
&= \text{ATT}
\end{aligned}
$$

DiDã¯ATTï¼ˆå‡¦ç½®ç¾¤ã®å¹³å‡å‡¦ç½®åŠ¹æœï¼‰ã‚’è­˜åˆ¥ã™ã‚‹ã€‚

</details>

<details><summary>Q4: Doubly Robustæ¨å®šé‡ãŒ2é‡é ‘å¥ã§ã‚ã‚‹ç†ç”±ã‚’ç¤ºã›</summary>

**DRæ¨å®šé‡**:

$$
\hat{\tau}_{\text{DR}} = \frac{1}{n} \sum_i \left[ \frac{D_i (Y_i - \hat{\mu}_1(X_i))}{\hat{e}(X_i)} + \hat{\mu}_1(X_i) - \frac{(1-D_i)(Y_i - \hat{\mu}_0(X_i))}{1-\hat{e}(X_i)} - \hat{\mu}_0(X_i) \right]
$$

**Case 1**: $\hat{\mu}_1, \hat{\mu}_0$ ãŒæ­£ã—ã„ï¼ˆ$\hat{e}$ ãŒèª¤ã‚Šã§ã‚‚OKï¼‰

$$
\begin{aligned}
\mathbb{E}[\hat{\tau}_{\text{DR}}] &= \mathbb{E}\left[\frac{D(Y - \mu_1(X))}{\hat{e}(X)} + \mu_1(X)\right] - \mathbb{E}\left[\frac{(1-D)(Y - \mu_0(X))}{1-\hat{e}(X)} + \mu_0(X)\right] \\
&= \mathbb{E}\left[\mathbb{E}\left[\frac{D(Y - \mu_1(X))}{\hat{e}(X)} \mid X\right] + \mu_1(X)\right] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}\left[\frac{\mathbb{E}[D(Y - \mu_1(X)) \mid X]}{\hat{e}(X)} + \mu_1(X)\right] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}\left[\frac{e(X)(\mu_1(X) - \mu_1(X))}{\hat{e}(X)} + \mu_1(X)\right] - \mathbb{E}[\mu_0(X)] \quad \text{(} \mathbb{E}[Y \mid D=1, X] = \mu_1(X)) \\
&= \mathbb{E}[\mu_1(X)] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}[Y^1 - Y^0] = \text{ATE}
\end{aligned}
$$

**Case 2**: $\hat{e}$ ãŒæ­£ã—ã„ï¼ˆ$\hat{\mu}$ ãŒèª¤ã‚Šã§ã‚‚OKï¼‰

IPWã®ä¸åæ€§ã«ã‚ˆã‚Š $\mathbb{E}[\hat{\tau}_{\text{DR}}] = \text{ATE}$

**çµè«–**: $\hat{\mu}$ or $\hat{e}$ ã®ã©ã¡ã‚‰ã‹ä¸€æ–¹ãŒæ­£ã—ã‘ã‚Œã°ä¸å â†’ 2é‡é ‘å¥æ€§

</details>

<details><summary>Q5: RDDåŠ¹æœã‚’å°å‡ºã›ã‚ˆï¼ˆSharp RDDï¼‰</summary>

**Setup**: ã‚«ãƒƒãƒˆã‚ªãƒ• $c$ ã§å‡¦ç½®å‰²ã‚Šå½“ã¦

$$
D_i = \mathbb{1}(X_i \geq c)
$$

**å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–ä»®å®š**:

$$
\lim_{x \to c} \mathbb{E}[Y^1 - Y^0 \mid X=x] = \tau_c
$$

ã‚«ãƒƒãƒˆã‚ªãƒ•è¿‘å‚ã§å‡¦ç½®åŠ¹æœãŒä¸€å®šã€‚

**RDDåŠ¹æœ**:

$$
\begin{aligned}
\tau_{\text{RDD}} &= \lim_{x \to c^+} \mathbb{E}[Y \mid X=x] - \lim_{x \to c^-} \mathbb{E}[Y \mid X=x] \\
&= \lim_{x \to c^+} \mathbb{E}[Y^1 \mid X=x] - \lim_{x \to c^-} \mathbb{E}[Y^0 \mid X=x] \\
&= \mathbb{E}[Y^1 - Y^0 \mid X=c] \\
&= \text{ATE}_c
\end{aligned}
$$

**Key**: ã‚«ãƒƒãƒˆã‚ªãƒ•ã§ã®ä¸é€£ç¶šæ€§ãŒå› æœåŠ¹æœã‚’è¡¨ã™ã€‚

**æ¨å®š**: Local Linear Regression

$$
\min_{\beta_0, \beta_1, \beta_2, \beta_3} \sum_{i: |X_i - c| < h} K\left(\frac{X_i - c}{h}\right) (Y_i - \beta_0 - \beta_1 D_i - \beta_2 (X_i - c) - \beta_3 D_i (X_i - c))^2
$$

$\hat{\beta}_1 = \hat{\tau}_{\text{RDD}}$

</details>

#### ãƒ†ã‚¹ãƒˆ3: Rustå®Ÿè£…ï¼ˆ5å•ï¼‰

<details><summary>Q1: å‚¾å‘ã‚¹ã‚³ã‚¢ã‚’æ¨å®šã—ã€å…±é€šã‚µãƒãƒ¼ãƒˆã‚’ç¢ºèªã›ã‚ˆ</summary>

```rust
// 1. Estimate propensity scores (logistic regression weights fit offline)
let e_x: Vec<f64> = features.iter()
    .map(|x| logistic(0.05 * x[0] + 0.03 * x[1] - 3.5))
    .collect();

// 2. Check common support
let min_e = e_x.iter().cloned().fold(f64::INFINITY, f64::min);
let max_e = e_x.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
println!("Min e(X): {:.3}", min_e);
println!("Max e(X): {:.3}", max_e);

// 3. Visualize overlap: use plotters crate or log histogram to terminal
// Treated vs Control propensity distributions should overlap (common support)

// 4. Trimming: exclude extreme propensity scores
let eps = 0.05_f64;
let (valid, trimmed): (Vec<_>, Vec<_>) = e_x.iter().partition(|&&ei| ei > eps && ei < 1.0 - eps);
println!("Trimmed {} observations ({:.1}%)",
         trimmed.len(),
         100.0 * trimmed.len() as f64 / e_x.len() as f64);
```

</details>

### 5.5 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: æ•™è‚²ä»‹å…¥ã®å› æœåŠ¹æœæ¨å®š

**ã‚·ãƒŠãƒªã‚ª**: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ•™è‚²ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®åŠ¹æœã‚’æ¨å®šã›ã‚ˆã€‚

- **å‡¦ç½®**: ãƒ—ãƒ­ã‚°ãƒ©ãƒ å—è¬› (1=å—è¬›, 0=éå—è¬›)
- **çµæœ**: ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢
- **å…±å¤‰é‡**: å¹´é½¢ã€äº‹å‰ã‚¹ã‚³ã‚¢ã€æ‰€å¾—
- **æ“ä½œå¤‰æ•°**: ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ¼ãƒãƒ³é…å¸ƒ

**ã‚¿ã‚¹ã‚¯**:

1. å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚° â†’ ATEæ¨å®š
2. 2SLS (ã‚¯ãƒ¼ãƒãƒ³ã‚’IV) â†’ LATEæ¨å®š
3. æ„Ÿåº¦åˆ†æ â†’ æœªæ¸¬å®šäº¤çµ¡ã¸ã®é ‘å¥æ€§
4. çµæœã‚’æ¯”è¼ƒã—ã€æœ€ã‚‚ä¿¡é ¼ã§ãã‚‹æ¨å®šå€¤ã‚’é¸æŠ

```rust
// Mini Project: Education Program Causal Effect (true effect = 10 points)

struct EduRecord {
    enroll: bool,
    test_score: f64,
    age: f64,
    baseline_score: f64,
    income: f64,
    coupon: bool,
    propensity: f64,
}

fn logistic(x: f64) -> f64 { 1.0 / (1.0 + (-x).exp()) }

fn education_program_data(n: usize) -> Vec<EduRecord> {
    // use rand_distr::{Uniform, Normal, Bernoulli, Distribution};
    (0..n).map(|i| {
        let age = 18.0 + (i % 17) as f64;                    // placeholder
        let baseline = 60.0 + ((i as f64 * 0.01).sin() * 15.0);
        let income = 50.0 + ((i as f64 * 0.007).cos() * 20.0);
        let ability = (i as f64 * 0.003).sin();               // unobserved confounder
        let coupon = i % 2 == 0;

        let enroll_p = logistic(
            0.8 * coupon as u8 as f64 + 0.02 * age - 0.01 * baseline
            + 0.01 * income + 0.3 * ability - 1.0
        );
        let enroll = enroll_p > 0.5;

        // True program effect: +10 points
        let test_score = 50.0 + 10.0 * enroll as u8 as f64 + 0.5 * baseline + 5.0 * ability;

        let propensity = logistic(0.02 * age - 0.01 * baseline + 0.01 * income - 0.5);

        EduRecord { enroll, test_score, age, baseline_score: baseline, income, coupon, propensity }
    }).collect()
}

fn main_education() {
    let data = education_program_data(2000);

    let y: Vec<f64> = data.iter().map(|r| r.test_score).collect();
    let d: Vec<bool> = data.iter().map(|r| r.enroll).collect();
    let e: Vec<f64> = data.iter().map(|r| r.propensity).collect();
    let z: Vec<f64> = data.iter().map(|r| r.coupon as u8 as f64).collect();
    let d_f: Vec<f64> = d.iter().map(|&di| di as u8 as f64).collect();

    // Method 1: Propensity Score (IPW)
    let (ate_ps, se_ps) = ipw_estimator(&d, &y, &e);

    // Method 2: IV (coupon as instrument for enrollment)
    let (ate_iv, se_iv) = two_stage_least_squares_simple(&y, &d_f, &z);

    println!("\n=== Education Program Causal Effect ===");
    println!("True effect: 10 points");
    println!("Propensity Score ATE: {:.2} Â± {:.2}", ate_ps, 1.96 * se_ps);
    println!("IV (2SLS) LATE: {:.2} Â± {:.2}", ate_iv, 1.96 * se_iv);

    // Sensitivity analysis
    sensitivity_analysis_gamma(ate_ps, se_ps, &[1.0, 1.5, 2.0]);
}
```

> **Note:** **é€²æ—: 85% å®Œäº†** å®Ÿãƒ‡ãƒ¼ã‚¿å› æœæ¨è«–ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚’å®Œäº†ã—ãŸã€‚å…¨æ‰‹æ³•ã‚’æ¯”è¼ƒã—ã€æ„Ÿåº¦åˆ†æã§é ‘å¥æ€§ã‚’ç¢ºèªã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã§ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’æ¢ç´¢ã™ã‚‹ã€‚

---

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 å› æœæ¨è«–ãƒ•ã‚¡ãƒŸãƒªãƒ¼ãƒ„ãƒªãƒ¼

```mermaid
graph TD
    A["ğŸŒ³ å› æœæ¨è«–"] --> B["Potential Outcomes<br/>(Rubin)"]
    A --> C["Structural Causal Models<br/>(Pearl)"]
    A --> D["Quasi-Experimental<br/>(Campbell)"]

    B --> E["Unconfoundedness<br/>å‚¾å‘ã‚¹ã‚³ã‚¢"]
    B --> F["SUTVA<br/>ATE/ATT/CATE"]

    C --> G["DAG + do-æ¼”ç®—"]
    C --> H["ãƒãƒƒã‚¯ãƒ‰ã‚¢/ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢"]
    C --> I["åå®Ÿä»®æƒ³æ¨è«–"]

    D --> J["IV/2SLS"]
    D --> K["RDD"]
    D --> L["DiD"]

    A --> M["MLÃ—å› æœ<br/>(Athey/Imbens)"]
    M --> N["Causal Forest"]
    M --> O["Double ML"]
    M --> P["Meta-Learners"]

    style A fill:#c8e6c9
    style M fill:#fff3e0
```

### 6.2 æ¨è–¦è«–æ–‡ãƒ»æ•™ç§‘æ›¸

#### ä¸»è¦è«–æ–‡

| è«–æ–‡ | è‘—è€… | å¹´ | è²¢çŒ® |
|:-----|:-----|:---|:-----|
| Causality (2nd Ed) [^1] | Pearl | 2009 | SCM, do-æ¼”ç®—, ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº– |
| Causal Inference (free book) [^9] | HernÃ¡n & Robins | 2020 | å®Ÿè·µã‚¬ã‚¤ãƒ‰ |
| Potential Outcomes Survey [^2] | Rubin | 2005 | Rubinå› æœãƒ¢ãƒ‡ãƒ«çµ±åˆ |
| Causal Forest [^3] | Wager & Athey | 2018 | HTEæ¨å®š, æ¼¸è¿‘ç†è«– |
| Double ML [^4] | Chernozhukov et al. | 2018 | Debiased MLæ¨è«– |
| Staggered DiD [^5] | Callaway & Sant'Anna | 2021 | å¤šæœŸé–“DiD |
| Weak IV [^7] | Stock & Yogo | 2005 | å¼±æ“ä½œå¤‰æ•°æ¤œå®š |
| SRM Detection [^6] | Fabijan et al. | 2019 | A/Bãƒ†ã‚¹ãƒˆå“è³ªç®¡ç† |
| Simpson's Paradox [^8] | Pearl | 2014 | ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹è§£æ¶ˆ |

#### æ•™ç§‘æ›¸

- **å…¥é–€**: Pearl & Mackenzie "The Book of Why" (2018) â€” ä¸€èˆ¬å‘ã‘å› æœé©å‘½ã®æ­´å²
- **ç†è«–**: Pearl "Causality" (2009) [^1] â€” SCMã®è–å…¸
- **å®Ÿè·µ**: HernÃ¡n & Robins "Causal Inference" (2020) [^9] â€” ç„¡æ–™å…¬é–‹ã€ç–«å­¦ãƒ™ãƒ¼ã‚¹
- **è¨ˆé‡**: Angrist & Pischke "Mostly Harmless Econometrics" (2009) â€” IV/RDD/DiDã®å®Ÿè·µ
- **MLÃ—å› æœ**: Facure "Causal Inference for The Brave and True" (2022) â€” Pythonå®Ÿè£…ä»˜ã

### 6.3 ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

| ãƒªã‚½ãƒ¼ã‚¹ | URL | èª¬æ˜ |
|:--------|:----|:-----|
| **ndarray** | [github.com/mschauer/ndarray](https://github.com/mschauer/ndarray) [^10] | Rustã®DAG/PC/FCIå®Ÿè£… |
| **Causal Inference Bootcamp** | [YouTube: Brady Neal](https://www.youtube.com/playlist?list=PLoazKTcS0RzZ1SUgeOgc6SWt51gfT80N0) | å‹•ç”»è¬›ç¾©ã‚·ãƒªãƒ¼ã‚º |
| **doWhy (Microsoft)** | [github.com/py-why/dowhy](https://github.com/py-why/dowhy) | Pythonå› æœæ¨è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
| **EconML (Microsoft)** | [github.com/py-why/EconML](https://github.com/py-why/EconML) | Python MLÃ—å› æœãƒ©ã‚¤ãƒ–ãƒ©ãƒª |

### 6.4 å› æœæ¨è«–ç”¨èªé›†

<details><summary>ç”¨èªé›†ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ï¼‰</summary>

| ç”¨èª | å®šç¾© |
|:-----|:-----|
| **ATE** | Average Treatment Effect â€” å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ $\mathbb{E}[Y^1 - Y^0]$ |
| **ATT** | Average Treatment Effect on the Treated â€” å‡¦ç½®ç¾¤ã®å¹³å‡å‡¦ç½®åŠ¹æœ |
| **Backdoor Criterion** | ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº– â€” äº¤çµ¡ã‚’é™¤å»ã™ã‚‹ãŸã‚ã®å¤‰æ•°é›†åˆã®æ¡ä»¶ |
| **CATE** | Conditional Average Treatment Effect â€” æ¡ä»¶ä»˜ãå¹³å‡å‡¦ç½®åŠ¹æœ |
| **Collider** | ã‚³ãƒ©ã‚¤ãƒ€ãƒ¼ â€” 2ã¤ã®çŸ¢å°ãŒé›†ã¾ã‚‹å¤‰æ•° ($X \to Z \leftarrow Y$) |
| **DAG** | Directed Acyclic Graph â€” å› æœæ§‹é€ ã‚’è¡¨ã™æœ‰å‘éå·¡å›ã‚°ãƒ©ãƒ• |
| **DiD** | Difference-in-Differences â€” å·®åˆ†ã®å·®åˆ†æ³• |
| **d-separation** | dåˆ†é›¢ â€” DAGä¸Šã§ã®æ¡ä»¶ä»˜ãç‹¬ç«‹æ€§ |
| **do-Calculus** | do-æ¼”ç®— â€” ä»‹å…¥ç¢ºç‡ã‚’æ¡ä»¶ä»˜ãç¢ºç‡ã«å¤‰æ›ã™ã‚‹3ã¤ã®ãƒ«ãƒ¼ãƒ« |
| **Doubly Robust** | äºŒé‡é ‘å¥æ¨å®šé‡ â€” å‚¾å‘ã‚¹ã‚³ã‚¢ã¨çµæœãƒ¢ãƒ‡ãƒ«ã®ã©ã¡ã‚‰ã‹ãŒæ­£ã—ã‘ã‚Œã°ä¸å |
| **Fundamental Problem** | æ ¹æœ¬çš„å› æœæ¨è«–å•é¡Œ â€” $Y^1, Y^0$ ã‚’åŒæ™‚è¦³æ¸¬ã§ããªã„ |
| **IPW** | Inverse Probability Weighting â€” é€†ç¢ºç‡é‡ã¿ä»˜ã‘ |
| **IV** | Instrumental Variable â€” æ“ä½œå¤‰æ•° |
| **LATE** | Local Average Treatment Effect â€” å±€æ‰€å¹³å‡å‡¦ç½®åŠ¹æœï¼ˆã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ã®åŠ¹æœï¼‰ |
| **Overlap** | å…±é€šã‚µãƒãƒ¼ãƒˆ â€” $0 < e(X) < 1$ ãŒã™ã¹ã¦ã® $X$ ã§æˆç«‹ |
| **Potential Outcomes** | æ½œåœ¨çš„çµæœ â€” $Y^1, Y^0$ |
| **Propensity Score** | å‚¾å‘ã‚¹ã‚³ã‚¢ â€” $e(X) = P(D=1 \mid X)$ |
| **RDD** | Regression Discontinuity Design â€” å›å¸°ä¸é€£ç¶šãƒ‡ã‚¶ã‚¤ãƒ³ |
| **SCM** | Structural Causal Model â€” æ§‹é€ å› æœãƒ¢ãƒ‡ãƒ« $(\mathcal{U}, \mathcal{V}, \mathcal{F})$ |
| **SUTVA** | Stable Unit Treatment Value Assumption â€” å®‰å®šå€‹ä½“å‡¦ç½®å€¤ä»®å®š |
| **Unconfoundedness** | ç„¡äº¤çµ¡æ€§ â€” $(Y^1, Y^0) \perp\!\!\!\perp D \mid X$ |

</details>

### 6.5 å› æœæ¨è«–ã®çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
mindmap
  root((å› æœæ¨è«–))
    Foundations
      Potential Outcomes
      DAG
      Identification
    Methods
      Propensity Score
        IPW
        Matching
        DR
      Quasi-Experimental
        IV
        RDD
        DiD
    ML Integration
      Causal Forest
      Double ML
      Meta-Learners
    Applications
      Policy Evaluation
      A/B Testing
      Observational Studies
    Tools
      CausalInference.jl
      doWhy
      EconML
```

> **Note:** **é€²æ—: 100% å®Œäº†** å› æœæ¨è«–ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’æ¢ç´¢ã—ãŸã€‚è«–æ–‡ãƒ»æ•™ç§‘æ›¸ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ç”¨èªã‚’å®Œå…¨æ•´ç†ã€‚ã‚ã¨ã¯æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ã§ã¾ã¨ã‚ã€‚

---



## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.6 æœ¬è¬›ç¾©ã®ã¾ã¨ã‚

1. **ç›¸é–¢ â‰  å› æœ**: Simpson's Paradox, äº¤çµ¡, é¸æŠãƒã‚¤ã‚¢ã‚¹ã®ç½ ã‚’ç†è§£
2. **Rubinå› æœãƒ¢ãƒ‡ãƒ«**: æ½œåœ¨çš„çµæœ $Y^1, Y^0$, SUTVA, ATE/ATT/CATE
3. **Pearlå› æœç†è«–**: DAG, do-æ¼”ç®—, ãƒãƒƒã‚¯ãƒ‰ã‚¢/ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢åŸºæº–, d-åˆ†é›¢
4. **å‚¾å‘ã‚¹ã‚³ã‚¢**: IPW, Matching, Doubly Robust, ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
5. **æ“ä½œå¤‰æ•°æ³•**: 2SLS, LATE, Weak IVå•é¡Œ
6. **RDD**: Sharp/Fuzzy, å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–, å¸¯åŸŸå¹…é¸æŠ
7. **DiD**: ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®š, Staggered DiD
8. **MLÃ—å› æœæ¨è«–**: Causal Forest, Double ML, Meta-Learners
9. **Rustå®Ÿè£…**: ndarray ã§å…¨æ‰‹æ³•ã‚’å®Ÿè£…

### 6.7 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

<details><summary>Q1: å› æœæ¨è«–ã¨æ©Ÿæ¢°å­¦ç¿’ã®é•ã„ã¯ï¼Ÿ</summary>

**A**:
- **æ©Ÿæ¢°å­¦ç¿’**: äºˆæ¸¬ç²¾åº¦ã®æœ€å¤§åŒ– â€” $\hat{Y} \approx Y$
- **å› æœæ¨è«–**: å› æœåŠ¹æœã®æ¨å®š â€” $\mathbb{E}[Y \mid do(X=x)]$

MLã¯ã€Œæ¬¡ã«ä½•ãŒèµ·ã“ã‚‹ã‹ã€ã€å› æœæ¨è«–ã¯ã€Œä»‹å…¥ã—ãŸã‚‰ä½•ãŒèµ·ã“ã‚‹ã‹ã€ã‚’å•ã†ã€‚MLã¯ç›¸é–¢ã‚’å­¦ç¿’ã—ã€å› æœæ¨è«–ã¯å› æœæ§‹é€ ã‚’ä»®å®šã™ã‚‹ã€‚

</details>

<details><summary>Q2: ã„ã¤å‚¾å‘ã‚¹ã‚³ã‚¢ vs IVã‚’ä½¿ã†ï¼Ÿ</summary>

**A**:
- **å‚¾å‘ã‚¹ã‚³ã‚¢**: Unconfoundedness $(Y^d \perp\!\!\!\perp D \mid X)$ ãŒæˆç«‹ã™ã‚‹å ´åˆ â€” ã™ã¹ã¦ã®äº¤çµ¡å› å­ã‚’æ¸¬å®šã§ãã¦ã„ã‚‹
- **IV**: æœªæ¸¬å®šäº¤çµ¡ãŒã‚ã‚‹å ´åˆ â€” å¤–ç”Ÿçš„ãªãƒ©ãƒ³ãƒ€ãƒ å¤‰å‹•ï¼ˆæ“ä½œå¤‰æ•°ï¼‰ã‚’åˆ©ç”¨

ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ã«è¿‘ã„çŠ¶æ³ãªã‚‰å‚¾å‘ã‚¹ã‚³ã‚¢ã€è¦³æ¸¬ç ”ç©¶ã§äº¤çµ¡ãŒç–‘ã‚ã‚Œã‚‹ãªã‚‰IVã€‚

</details>

<details><summary>Q3: RDDã¨DiDã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ</summary>

**A**:
- **RDD**: å‡¦ç½®å‰²ã‚Šå½“ã¦ãŒã‚«ãƒƒãƒˆã‚ªãƒ•ã§æ±ºã¾ã‚‹ï¼ˆä¾‹: å¹´é½¢18æ­³ã§é¸æŒ™æ¨©ã€ã‚¹ã‚³ã‚¢70ç‚¹ã§åˆæ ¼ï¼‰
- **DiD**: 2æœŸé–“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã€å‡¦ç½®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãŒç¾¤ã«ã‚ˆã£ã¦ç•°ãªã‚‹

RDDã¯ç©ºé–“çš„ä¸é€£ç¶šã€DiDã¯æ™‚é–“çš„å¤‰åŒ–ã‚’åˆ©ç”¨ã™ã‚‹ã€‚

</details>

<details><summary>Q4: Causal Forestã§ä½•ãŒã‚ã‹ã‚‹ï¼Ÿ</summary>

**A**: **ç•°è³ªãªå‡¦ç½®åŠ¹æœ (HTE)** â€” å€‹ä½“ç‰¹æ€§ $X$ ã«å¿œã˜ãŸå‡¦ç½®åŠ¹æœ $\tau(X)$

å¹³å‡åŠ¹æœ(ATE)ã ã‘ã§ãªãã€ã€Œé«˜é½¢è€…ã«ã¯åŠ¹æœå¤§ã€è‹¥å¹´è€…ã«ã¯åŠ¹æœå°ã€ã¨ã„ã£ãŸéƒ¨åˆ†é›†å›£ã”ã¨ã®åŠ¹æœã‚’æ¨å®šã§ãã‚‹ã€‚æ”¿ç­–ã®ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ã«æœ‰ç”¨ã€‚

</details>

<details><summary>Q5: å› æœæ¨è«–ã§æœ€ã‚‚é‡è¦ãªä»®å®šã¯ï¼Ÿ</summary>

**A**: **Unconfoundedness** $(Y^d \perp\!\!\!\perp D \mid X)$ ã¾ãŸã¯ **Exclusion Restriction** (IV)

ã“ã‚ŒãŒç ´ã‚Œã‚‹ã¨ã€ã©ã‚“ãªæ‰‹æ³•ã‚‚å› æœåŠ¹æœã‚’æ­£ã—ãæ¨å®šã§ããªã„ã€‚ä»®å®šã®å¦¥å½“æ€§ã‚’ç†è«–ãƒ»ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãƒ»æ„Ÿåº¦åˆ†æã§æ¤œè¨¼ã™ã‚‹ã“ã¨ãŒæœ€é‡è¦ã€‚

</details>

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“å¾©ç¿’ãƒ—ãƒ©ãƒ³ï¼‰

| Day | å†…å®¹ | æ™‚é–“ | é”æˆåŸºæº– |
|:----|:-----|:-----|:---------|
| Day 1 | Zone 3.1-3.2 å†èª­ + Rubinç†è«–å¾©ç¿’ | 1h | ATE/ATT/CATE ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹ |
| Day 2 | Zone 3.3 å†èª­ + Pearlç†è«–å¾©ç¿’ | 1h | ãƒãƒƒã‚¯ãƒ‰ã‚¢èª¿æ•´å…¬å¼ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹ |
| Day 3 | Zone 3.4-3.5 å†èª­ + å‚¾å‘ã‚¹ã‚³ã‚¢/IVå¾©ç¿’ | 1h | IPWæ¨å®šé‡ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹ |
| Day 4 | Zone 4 Rustå®Ÿè£…ã‚’å…¨ã¦å®Ÿè¡Œ | 2h | å…¨ã‚³ãƒ¼ãƒ‰ãŒã‚¨ãƒ©ãƒ¼ãªãå®Ÿè¡Œã§ãã‚‹ |
| Day 5 | Zone 5 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Ÿè£… | 2h | æ•™è‚²ä»‹å…¥ãƒ‡ãƒ¼ã‚¿ã§3æ‰‹æ³•æ¯”è¼ƒå®Œäº† |
| Day 6 | è«–æ–‡èª­è§£: Causal Forest [^3] or Double ML [^4] | 2h | æ‰‹æ³•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå®Œå…¨ã«ç†è§£ã§ãã‚‹ |
| Day 7 | è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§å› æœæ¨è«–å®Ÿè·µ | 3h | å®Ÿãƒ‡ãƒ¼ã‚¿ã§ATEæ¨å®š + æ„Ÿåº¦åˆ†æå®Œäº† |

### 6.9 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ç¬¬26å›: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰** ã§ã¯ã€å› æœåŠ¹æœã®çµ±è¨ˆçš„æ¤œå®š:

- FID/IS/LPIPS (ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡)
- Bootstrapã«ã‚ˆã‚‹CIæ¨å®š
- å¤šé‡æ¤œå®šè£œæ­£ (Bonferroni, FDR)
- å› æœåŠ¹æœã®å¯è¦–åŒ– (Forest Plot, Love Plot)

**ç¬¬27å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª** ã§ã¯ã€å› æœæ¨è«–ã§å¾—ãŸåŠ¹æœã‚’**æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã«çµ„ã¿è¾¼ã‚€**:

- A/Bãƒ†ã‚¹ãƒˆåŸºç›¤æ§‹ç¯‰ (Elixir OTPã§ã®ä¸¦è¡Œãƒ†ã‚¹ãƒˆç®¡ç†)
- ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆæ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
- å› æœæ¨è«–Ã—å¼·åŒ–å­¦ç¿’ï¼ˆCounterfactual Policy Evaluationï¼‰
- Productionå“è³ª: é‡å­åŒ–ãƒ»è’¸ç•™ãƒ»Speculative Decoding

### 6.7 æ¬¡å›äºˆå‘Š: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰

ç¬¬26å›ã§ã¯ã€å› æœæ¨è«–ã§æ¸¬å®šã—ãŸåŠ¹æœã‚’**å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹**åŸºç›¤ã‚’æ§‹ç¯‰ã™ã‚‹:

- **FID / IS / LPIPS**: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å“è³ªã®å®šé‡è©•ä¾¡
- **CMMD**: CLIPåŸ‹ã‚è¾¼ã¿ã«ã‚ˆã‚‹æœ€å¤§å¹³å‡èª¤å·®
- **çµ±è¨ˆæ¤œå®šçµ±åˆ**: tæ¤œå®š / Wilcoxon / Bootstrap CI
- **è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**: Criterion.rs + Rustçµ±è¨ˆ
- **Human Evaluation**: MOS / A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: FID / IS / LPIPS / CMMD / FLD+ / Precision-Recall / Bootstrap / Criterion.rs

**ç›®æ¨™**: å› æœæ¨è«–ã§å¾—ãŸçŸ¥è¦‹ã‚’ã€è©•ä¾¡æŒ‡æ¨™ã¨çµ±è¨ˆæ¤œå®šã§å®šé‡çš„ã«æ¤œè¨¼ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

---

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **A/Bãƒ†ã‚¹ãƒˆãªã—ã«"æ”¹å–„"ã‚’è¨¼æ˜ã§ãã‚‹ã‹ï¼Ÿ**

ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ï¼ˆA/Bãƒ†ã‚¹ãƒˆï¼‰ã¯å› æœæ¨è«–ã®ã‚´ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã ã€‚ã ãŒ:

- **å€«ç†çš„åˆ¶ç´„**: åŒ»ç™‚ã€æ•™è‚²ã€ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã§ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã¯å›°é›£
- **ã‚³ã‚¹ãƒˆ**: å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å®Ÿé¨“å°ã«ã§ããªã„
- **æ™‚é–“**: åŠ¹æœãŒå‡ºã‚‹ã¾ã§æ•°ãƒ¶æœˆã€œæ•°å¹´

**è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ­£ã—ãæ¨å®šã§ãã‚Œã°ã€A/Bãƒ†ã‚¹ãƒˆãªã—ã§ã‚‚æ”¹å–„ã‚’è¨¼æ˜ã§ãã‚‹ã€‚**

æœ¬è¬›ç¾©ã§å­¦ã‚“ã æ‰‹æ³•:

1. **å‚¾å‘ã‚¹ã‚³ã‚¢**: äº¤çµ¡ã‚’åˆ¶å¾¡ã—ã€è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ATEæ¨å®š
2. **æ“ä½œå¤‰æ•°**: æœªæ¸¬å®šäº¤çµ¡ãŒã‚ã£ã¦ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ãªå¤‰å‹•ã§å› æœåŠ¹æœæ¨å®š
3. **RDD**: ã‚«ãƒƒãƒˆã‚ªãƒ•ã®ä¸é€£ç¶šæ€§ã‚’åˆ©ç”¨ã—ã€å±€æ‰€çš„ãªå› æœåŠ¹æœæ¨å®š
4. **DiD**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®šã®ä¸‹ã§å› æœåŠ¹æœæ¨å®š
5. **Causal Forest**: ç•°è³ªãªå‡¦ç½®åŠ¹æœã‚’æ¨å®šã—ã€ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–

**ã ãŒã€ä»®å®šãŒç ´ã‚Œã‚Œã°å…¨ã¦ãŒå´©ã‚Œã‚‹ã€‚** å› æœæ¨è«–ã¯ã€Œä»®å®šã®æ˜ç¤ºåŒ–ã€ã¨ã€Œæ„Ÿåº¦åˆ†æã€ã«ã‚ˆã£ã¦ä»®å®šã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ã—ç¶šã‘ã‚‹å–¶ã¿ã ã€‚

**ã‚ãªãŸã®ç­”ãˆã¯ï¼Ÿ** â€” è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿å› æœæ¨è«–ã¨A/Bãƒ†ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã©ã†å–ã‚‹ã‹ï¼Ÿ

<details><summary>è­°è«–ã®ãƒã‚¤ãƒ³ãƒˆ</summary>

1. **è¦³æ¸¬ç ”ç©¶ã®å¼·ã¿**:
   - å€«ç†çš„åˆ¶ç´„ãŒãªã„ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ï¼‰
   - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å¤–éƒ¨å¦¥å½“æ€§ãŒé«˜ã„
   - é•·æœŸçš„åŠ¹æœã‚’è¿½è·¡ã§ãã‚‹

2. **è¦³æ¸¬ç ”ç©¶ã®å¼±ã¿**:
   - ä»®å®šä¾å­˜ï¼ˆUnconfoundedness, IVä»®å®šç­‰ï¼‰
   - æœªæ¸¬å®šäº¤çµ¡ã®ãƒªã‚¹ã‚¯
   - å› æœæ§‹é€ ã®èª¤ç‰¹å®š

3. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:
   - A/Bãƒ†ã‚¹ãƒˆã§çŸ­æœŸåŠ¹æœæ¤œè¨¼ + è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã§é•·æœŸåŠ¹æœæ¨å®š
   - A/Bãƒ†ã‚¹ãƒˆã§ãƒã‚¤ã‚¢ã‚¹è£œæ­£ + è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã§å¤–æŒ¿
   - å› æœæ¨è«–ã§äº‹å‰è©•ä¾¡ + A/Bãƒ†ã‚¹ãƒˆã§æœ€çµ‚ç¢ºèª

4. **æ­´å²çš„è¦–ç‚¹**:
   - Fisher (1935): ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ã®åŸå‰‡ç¢ºç«‹
   - Rubin (1974): è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å› æœæ¨è«–ç†è«–
   - Pearl (2000): ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã§å› æœæ§‹é€ ã‚’æ˜ç¤ºåŒ–
   - ç¾ä»£: MLÃ—å› æœæ¨è«–ã§å¤§è¦æ¨¡è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿æ´»ç”¨

**çµè«–**: A/Bãƒ†ã‚¹ãƒˆã¯ä¾ç„¶ã¨ã—ã¦ã‚´ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã ãŒã€**å› æœæ¨è«–ã¯è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€å¤§é™ã®æƒ…å ±ã‚’å¼•ãå‡ºã™å¼·åŠ›ãªæ­¦å™¨**ã€‚ä¸¡è€…ã‚’é©åˆ‡ã«çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ­£ç¢ºãªæ„æ€æ±ºå®šãŒå¯èƒ½ã«ãªã‚‹ã€‚

</details>

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼

### 6.6 æ·±å±¤å­¦ç¿’ã¨å› æœæ¨è«–ã®èåˆï¼ˆ2024-2026æœ€æ–°å‹•å‘ï¼‰

å¾“æ¥ã®å› æœæ¨è«–æ‰‹æ³•ã¯ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚„å˜ç´”ãªçµ±è¨ˆæ‰‹æ³•ã«ä¾å­˜ã—ã¦ã„ãŸãŒã€**æ·±å±¤å­¦ç¿’ã¨ã®çµ±åˆ**ã«ã‚ˆã‚Šã€é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ãƒ»éç·šå½¢é–¢ä¿‚ãƒ»æœªè¦³æ¸¬äº¤çµ¡ã¸ã®å¯¾å‡¦èƒ½åŠ›ãŒé£›èºçš„ã«å‘ä¸Šã—ã¦ã„ã‚‹ [^11]ã€‚

#### 6.6.1 Deep Causal Learningã®3æ¬¡å…ƒ

æœ€æ–°ã®ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ [^12] ã¯ã€æ·±å±¤å­¦ç¿’ãŒå› æœå­¦ç¿’ã«è²¢çŒ®ã™ã‚‹3ã¤ã®æ¬¡å…ƒã‚’æ•´ç†ã—ã¦ã„ã‚‹:

**1. Representationï¼ˆè¡¨ç¾å­¦ç¿’ï¼‰**:

é«˜æ¬¡å…ƒãƒ»éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»æ™‚ç³»åˆ—ï¼‰ã‹ã‚‰å› æœé–¢ä¿‚ã‚’å­¦ç¿’:

$$
\mathbf{z} = f_\theta(\mathbf{x}), \quad \mathbf{z} \in \mathbb{R}^d
$$

ã“ã“ã§ $f_\theta$ ã¯æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€$\mathbf{z}$ ã¯å› æœæ§‹é€ ã‚’æ‰ãˆãŸæ½œåœ¨è¡¨ç¾ã€‚

**2. Discoveryï¼ˆå› æœç™ºè¦‹ï¼‰**:

ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (GNN) ã§DAGã‚’å­¦ç¿’:

$$
\mathcal{G}^* = \arg\min_{\mathcal{G} \in \mathcal{DAG}} \mathcal{L}_{\text{score}}(\mathcal{G}; \mathbf{X}) + \lambda \|\mathcal{G}\|_0
$$

ã“ã“ã§ $\mathcal{L}_{\text{score}}$ ã¯ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ï¼ˆBIC, MDLç­‰ï¼‰ã€$\|\mathcal{G}\|_0$ ã¯ã‚¨ãƒƒã‚¸æ•°ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹æ€§æ­£å‰‡åŒ–ï¼‰ã€‚

**3. Inferenceï¼ˆå› æœæ¨è«–ï¼‰**:

æ·±å±¤å­¦ç¿’ã§å‡¦ç½®åŠ¹æœã‚’æ¨å®š:

$$
\tau(x) = \mathbb{E}[Y^1 - Y^0 \mid X = x] = f_\theta^1(x) - f_\theta^0(x)
$$

ã“ã“ã§ $f_\theta^1, f_\theta^0$ ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§å­¦ç¿’ã—ãŸæ½œåœ¨çš„çµæœé–¢æ•°ã€‚

#### 6.6.2 Deep Treatment Effect Estimation

**ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** [^13]:

| ãƒ¢ãƒ‡ãƒ« | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | ç‰¹å¾´ | è«–æ–‡ |
|:-------|:-------------|:-----|:-----|
| **TARNet** | å…±æœ‰å±¤ + åˆ†å²å±¤ | $f_{\text{shared}}(x) \to (f^1, f^0)$ | Shalit et al. 2017 |
| **CFRNet** | TARNet + IPMæ­£å‰‡åŒ– | $\min \text{IPM}(f(X \mid D=1), f(X \mid D=0))$ | Shalit et al. 2017 |
| **DragonNet** | å‚¾å‘ã‚¹ã‚³ã‚¢çµ±åˆ | $f_\theta(x) \to (e(x), \mu^1(x), \mu^0(x))$ | Shi et al. 2019 |
| **GANITE** | GAN | åå®Ÿä»®æƒ³ç”Ÿæˆ | Yoon et al. 2018 |
| **X-Learner** | ãƒ¡ã‚¿å­¦ç¿’ | 2æ®µéšæ¨å®š | KÃ¼nzel et al. 2019 |

**IPM (Integral Probability Metric)**:

åˆ†å¸ƒé–“ã®è·é›¢ã‚’æ¸¬å®šã—ã€å‡¦ç½®ç¾¤ãƒ»å¯¾ç…§ç¾¤ã®è¡¨ç¾ã‚’è¿‘ã¥ã‘ã‚‹:

$$
\text{IPM}(P, Q) = \sup_{f \in \mathcal{F}} \left| \mathbb{E}_{x \sim P}[f(x)] - \mathbb{E}_{x \sim Q}[f(x)] \right|
$$

CFRNetã¯ã€$\mathcal{F}$ ã‚’å†ç”Ÿæ ¸ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆç©ºé–“ (RKHS) ã¨ã—ã€Maximum Mean Discrepancy (MMD) ã‚’æœ€å°åŒ–:

$$
\mathcal{L}_{\text{CFR}} = \mathcal{L}_{\text{pred}} + \lambda \cdot \text{MMD}^2(f(X \mid D=1), f(X \mid D=0))
$$

**ADMIT (2024æœ€æ–°)** [^14]:

Average Dose Response Function (ADRF) ã®ä¸€èˆ¬åŒ–å¢ƒç•Œã‚’æä¾›:

$$
\text{ADRF}(d) = \mathbb{E}[Y \mid do(D = d)], \quad d \in [0, 1]
$$

é€£ç¶šå‡¦ç½®å¤‰æ•°ã«å¯¾ã—ã¦ã€IPMè·é›¢ã®é›¢æ•£è¿‘ä¼¼ã§ã‚»ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒã‚¤ã‚¢ã‚¹ã‚’ç·©å’Œã€‚

#### 6.6.3 Causal Discovery with Deep Learning

**NOTEARS** (Zheng et al., NeurIPS 2018):

DAGå­¦ç¿’ã‚’**é€£ç¶šæœ€é©åŒ–å•é¡Œ**ã«å¤‰æ›:

$$
\min_{\mathbf{W}} \quad \frac{1}{2n} \|\mathbf{X} - \mathbf{X}\mathbf{W}\|_F^2 + \lambda \|\mathbf{W}\|_1
$$

$$
\text{s.t.} \quad \text{tr}(e^{\mathbf{W} \odot \mathbf{W}}) - d = 0 \quad (\text{acyclicity constraint})
$$

ã“ã“ã§:

- $\mathbf{W} \in \mathbb{R}^{d \times d}$: é‡ã¿è¡Œåˆ—ï¼ˆæœ‰å‘ã‚°ãƒ©ãƒ•ã®éš£æ¥è¡Œåˆ—ï¼‰
- $\text{tr}(e^{\mathbf{W} \odot \mathbf{W}}) - d = 0$: DAGåˆ¶ç´„ï¼ˆéå·¡å›æ€§ï¼‰

å¾“æ¥ã®çµ„åˆã›æœ€é©åŒ– ($2^{d(d-1)/2}$ é€šã‚Š) ã‚’å›é¿ã—ã€å‹¾é…æ³•ã§è§£ã‘ã‚‹ã€‚

**GraN-DAG** (Lachapelle et al., ICML 2020):

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§éç·šå½¢å› æœé–¢ä¿‚ã‚’å­¦ç¿’:

$$
x_i = f_i(\text{PA}_i; \theta_i) + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma_i^2)
$$

ã“ã“ã§ $f_i$ ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã€$\text{PA}_i$ ã¯è¦ªãƒãƒ¼ãƒ‰ã€‚

**é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã¸ã®å¿œç”¨** [^15]:

- **ç”»åƒãƒ‡ãƒ¼ã‚¿**: CNNã§å› æœæ§‹é€ ã‚’å­¦ç¿’ï¼ˆä¾‹: ç—…ç†ç”»åƒ â†’ ç–¾æ‚£å› æœã‚°ãƒ©ãƒ•ï¼‰
- **æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿**: RNN/Transformerã§Grangerå› æœæ€§ã‚’å­¦ç¿’
- **ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: BERT/GPTã§è¨€èª¬é–“ã®å› æœé–¢ä¿‚ã‚’æ¨å®š

#### 6.6.4 å®Ÿè£…ä¾‹: TARNetã«ã‚ˆã‚‹ç•°è³ªå‡¦ç½®åŠ¹æœæ¨å®š

```rust
// TARNet: Treatment-Agnostic Representation Network for CATE estimation.
// Ï†(x) shared representation â†’ separate heads Î¼â‚, Î¼â‚€
// For production: use the `candle` or `burn` deep learning crate.

/// TARNet: shared encoder + two outcome heads (treated / control)
struct TARNet {
    // In a real implementation these would be actual neural network layers
    // e.g., using candle_nn::Linear
    input_dim: usize,
    hidden_dim: usize,
}

impl TARNet {
    fn new(input_dim: usize, hidden_dim: usize) -> Self {
        Self { input_dim, hidden_dim }
    }

    /// Forward pass: shared representation â†’ observed outcome prediction
    /// d: 0.0 (control) or 1.0 (treated) for each observation
    fn forward(&self, x: &[f64], d: f64) -> f64 {
        // Ï†(x) â€” shared representation (linear placeholder)
        let h: Vec<f64> = x.iter().map(|xi| xi.max(0.0)).collect(); // ReLU
        // Î¼â‚(Ï†) and Î¼â‚€(Ï†) â€” separate linear heads (placeholder)
        let y1: f64 = h.iter().sum::<f64>() / h.len() as f64;
        let y0: f64 = h.iter().sum::<f64>() / h.len() as f64 * 0.5;
        d * y1 + (1.0 - d) * y0
    }

    /// CATE estimate: Ï„(x) = Î¼Ì‚â‚(Ï†(x)) - Î¼Ì‚â‚€(Ï†(x))
    fn estimate_cate(&self, x: &[f64]) -> f64 {
        let h: Vec<f64> = x.iter().map(|xi| xi.max(0.0)).collect();
        let y1: f64 = h.iter().sum::<f64>() / h.len() as f64;
        let y0: f64 = h.iter().sum::<f64>() / h.len() as f64 * 0.5;
        y1 - y0
    }
}

// Training loop (placeholder â€” use candle/burn for real autodiff):
fn train_tarnet(model: &TARNet, x: &[Vec<f64>], d: &[f64], y: &[f64],
                n_epochs: usize, _lr: f64) {
    for epoch in 0..n_epochs {
        // Compute MSE loss: Î£ (Å· - y)Â² / n
        let loss: f64 = x.iter().zip(d).zip(y)
            .map(|((xi, di), yi)| (model.forward(xi, *di) - yi).powi(2))
            .sum::<f64>() / x.len() as f64;
        if epoch % 20 == 0 {
            println!("Epoch {}: Loss = {:.4}", epoch, loss);
        }
        // Gradient update omitted â€” use candle_nn::Adam in production
    }
}
```

**æ•°å¼ã¨ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œ**:

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ |
|:-----|:-------|
| $\phi(x) = f_{\text{shared}}(x)$ | `h = m.shared(x)` |
| $\mu^1(x) = f_1(\phi(x))$ | `y1 = m.treated(h)` |
| $\mu^0(x) = f_0(\phi(x))$ | `y0 = m.control(h)` |
| $\tau(x) = \mu^1(x) - \mu^0(x)$ | `estimate_cate(model, x)` |

ã“ã®å®Ÿè£…ã«ã‚ˆã‚Šã€å€‹äººãƒ¬ãƒ™ãƒ«ã®å‡¦ç½®åŠ¹æœï¼ˆCATEï¼‰ã‚’æ¨å®šã§ãã‚‹ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼æœ€æ–°ã®æ·±å±¤å­¦ç¿’Ã—å› æœæ¨è«–æ‰‹æ³•ã¾ã§ç¶²ç¾…ã—ãŸã€‚

---


> Progress: [95%]
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. E-valueãŒã€Œè¦³å¯Ÿç ”ç©¶ã«ãŠã‘ã‚‹å› æœä¸»å¼µã®é ‘å¥æ€§ã€ã‚’å®šé‡åŒ–ã§ãã‚‹ç†ç”±ã‚’è¿°ã¹ã‚ˆã€‚
> 2. Causal ForestãŒS/T-Learnerã‚ˆã‚Šç•°è³ªå‡¦ç½®åŠ¹æœï¼ˆHTEï¼‰æ¨å®šã«å„ªã‚Œã‚‹ç†ç”±ã¯ï¼Ÿ

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Pearl, J. (2009). *Causality: Models, Reasoning, and Inference* (2nd ed.). Cambridge University Press.
<https://bayes.cs.ucla.edu/BOOK-2K/>

[^2]: Rubin, D. B. (2005). Causal Inference Using Potential Outcomes: Design, Modeling, Decisions. *Journal of the American Statistical Association*, 100(469), 322-331.
<https://www.tandfonline.com/doi/abs/10.1198/016214504000001880>

[^3]: Wager, S., & Athey, S. (2018). Estimation and Inference of Heterogeneous Treatment Effects using Random Forests. *Journal of the American Statistical Association*, 113(523), 1228-1242.
<https://arxiv.org/abs/1510.04342>

[^4]: Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. *The Econometrics Journal*, 21(1), C1-C68.
<https://arxiv.org/abs/1608.00060>

[^5]: Callaway, B., & Sant'Anna, P. H. (2021). Difference-in-Differences with multiple time periods. *Journal of Econometrics*, 225(2), 200-230.
<https://www.sciencedirect.com/science/article/abs/pii/S0304407620303948>

[^6]: Fabijan, A., Gupchup, J., Gupta, S., Omhover, J., Qin, W., Vermeer, L., & Dmitriev, P. (2019). Diagnosing Sample Ratio Mismatch in Online Controlled Experiments: A Taxonomy and Rules of Thumb for Practitioners. *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2156-2164.
<https://dl.acm.org/doi/10.1145/3292500.3330722>

[^7]: Stock, J. H., & Yogo, M. (2005). Testing for Weak Instruments in Linear IV Regression. In *Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg* (pp. 80-108). Cambridge University Press.
<https://www.cambridge.org/core/books/abs/identification-and-inference-for-econometric-models/testing-for-weak-instruments-in-linear-iv-regression/8AD94FF2EFD214D05D75EE35015021E4>

[^8]: Pearl, J. (2014). Understanding Simpson's Paradox. *The American Statistician*, 68(1), 8-13.
<https://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf>

[^9]: HernÃ¡n, M. A., & Robins, J. M. (2020). *Causal Inference: What If*. Chapman & Hall/CRC. (Free online)
<https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/>

[^10]: Mschauer. (2021). ndarray: Causal inference, graphical models and structure learning in Rust.
<https://github.com/mschauer/ndarray>

[^11]: Wang, Y., et al. (2024). "Causal Inference Meets Deep Learning: A Comprehensive Survey". *Research*, 7, 0467.
<https://arxiv.org/abs/2303.02186>

[^12]: Guo, R., et al. (2024). "Deep Causal Learning: Representation, Discovery and Inference". *ACM Computing Surveys*, 56(9), 1-40.
<https://arxiv.org/abs/2211.03374>


[^24]: Ling, Z., et al. (2025). "Hybrid Local Causal Discovery". *arXiv preprint*.
<https://arxiv.org/abs/2412.19507>

[^25]: Zhou, J., & Wang, M. (2025). "Differentiable Constraint-Based Causal Discovery". *arXiv preprint*.
<https://arxiv.org/abs/2510.22031>

[^26]: Mokhtarian, E., et al. (2024). "Recursive Causal Discovery". *arXiv preprint*.
<https://arxiv.org/abs/2403.09300>

[^27]: Gerhardus, A., & Runge, J. (2023). "Causal Discovery from Time Series with Hybrids of Constraint-Based and Noise-Based Algorithms". *arXiv preprint*.
<https://arxiv.org/abs/2306.08765>

## è‘—è€…ãƒªãƒ³ã‚¯
- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

**æ¬¡å›**: [ç¬¬26å›: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰](/your-next-article)
