---
title: "ç¬¬25å›: å› æœæ¨è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
slug: "ml-lecture-25-part2"
emoji: "ğŸ”—"
type: "tech"
topics: ["machinelearning", "causalinference", "julia", "statistics", "experiment"]
published: true
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> **ç¬¬25å›ã€å‰ç·¨ã€‘**: [ç¬¬25å›ã€å‰ç·¨ã€‘](https://zenn.dev/fumishiki/ml-lecture-25-part1)


## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Juliaå› æœæ¨è«–ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯

### 4.1 CausalInference.jl ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```julia
# Package installation
using Pkg
Pkg.add(["CausalInference", "Graphs", "GLM", "DataFrames", "Statistics",
         "LinearAlgebra", "Distributions", "StatsBase", "CategoricalArrays"])

using CausalInference
using Graphs  # DAG manipulation
using GLM     # Propensity score estimation
using DataFrames
using Statistics, LinearAlgebra
using Distributions
using StatsBase
using CategoricalArrays
```

### 4.2 Pearl DAG + do-æ¼”ç®—å®Ÿè£…

#### 4.2.1 DAGæ§‹ç¯‰ã¨å¯è¦–åŒ–

```julia
# DAG construction: Smoking â†’ Cancer, Gene â†’ Smoking, Gene â†’ Cancer
function build_smoking_cancer_dag()
    # Create directed graph
    # Nodes: 1=Gene, 2=Smoking, 3=Cancer
    dag = SimpleDiGraph(3)
    add_edge!(dag, 1, 2)  # Gene â†’ Smoking
    add_edge!(dag, 1, 3)  # Gene â†’ Cancer
    add_edge!(dag, 2, 3)  # Smoking â†’ Cancer

    node_names = ["Gene", "Smoking", "Cancer"]
    return dag, node_names
end

dag, names = build_smoking_cancer_dag()
println("DAG edges:")
for edge in edges(dag)
    println("  $(names[src(edge)]) â†’ $(names[dst(edge)])")
end

# d-separation check
using CausalInference: dsep

# Are Smoking and Cancer d-separated by Gene?
# dsep(dag, [2], [3], [1])  # false (Gene doesn't block the direct path Smokingâ†’Cancer)
println("Smoking âŠ¥ Cancer | Gene? $(dsep(dag, [2], [3], [1]))")

# Are Gene and Cancer d-separated by Smoking?
# dsep(dag, [1], [3], [2])  # false (Geneâ†’Cancer direct path remains)
println("Gene âŠ¥ Cancer | Smoking? $(dsep(dag, [1], [3], [2]))")
```

#### 4.2.2 ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã®æ¤œè¨¼

```julia
using CausalInference: backdoor_criterion

# Check if {Gene} satisfies backdoor criterion for (Smoking, Cancer)
function check_backdoor(dag, treatment, outcome, adjustment_set)
    # CausalInference.jl implementation
    # backdoor_criterion(dag, treatment, outcome, adjustment_set)
    # Returns true if adjustment_set satisfies backdoor criterion

    # Manual check:
    # 1. No node in adjustment_set is descendant of treatment
    # 2. adjustment_set blocks all backdoor paths from treatment to outcome

    # In our DAG: Smoking(2) â†’ Cancer(3), backdoor path: Smoking â† Gene â†’ Cancer
    # Adjusting for Gene(1) blocks this path

    result = CausalInference.backdoor_criterion(dag, [treatment], [outcome], adjustment_set)
    return result
end

is_valid = check_backdoor(dag, 2, 3, [1])
println("Does {Gene} satisfy backdoor criterion for (Smoking, Cancer)? $is_valid")
```

#### 4.2.3 do-æ¼”ç®—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```julia
# Simulate observational data from the DAG
function simulate_from_dag(n::Int=5000)
    # Gene ~ Bernoulli(0.3)
    gene = rand(Bernoulli(0.3), n)

    # Smoking | Gene ~ Bernoulli(logistic(0.5 * Gene))
    smoking_prob = @. 1 / (1 + exp(-(0.5gene - 0.2)))
    smoking = rand.(Bernoulli.(smoking_prob))

    # Cancer | Smoking, Gene ~ Bernoulli(logistic(1.5 * Smoking + 0.8 * Gene))
    cancer_prob = @. 1 / (1 + exp(-(1.5smoking + 0.8gene - 1.0)))
    cancer = rand.(Bernoulli.(cancer_prob))

    return DataFrame(Gene=gene, Smoking=smoking, Cancer=cancer)
end

data = simulate_from_dag(5000)

# Observational: P(Cancer | Smoking)
obs_cancer_smoking = mean(data[data.Smoking .== 1, :Cancer])
obs_cancer_nonsmoking = mean(data[data.Smoking .== 0, :Cancer])
obs_effect = obs_cancer_smoking - obs_cancer_nonsmoking
println("Observational P(Cancer|Smoking=1) - P(Cancer|Smoking=0): $(round(obs_effect, digits=3))")

# Interventional: P(Cancer | do(Smoking)) via backdoor adjustment
function backdoor_adjustment(data, treatment, outcome, adjustment)
    # P(Y | do(X=x)) = Î£_z P(Y|X=x, Z=z) P(Z=z)
    result = Dict()
    for x in [0, 1]
        prob_y = 0.0
        for z in unique(data[:, adjustment])
            # P(Y=1 | X=x, Z=z)
            subset = data[(data[:, treatment] .== x) .& (data[:, adjustment] .== z), :]
            if nrow(subset) > 0
                p_y_given_xz = mean(subset[:, outcome])
            else
                p_y_given_xz = 0.0
            end

            # P(Z=z)
            p_z = mean(data[:, adjustment] .== z)

            prob_y += p_y_given_xz * p_z
        end
        result[x] = prob_y
    end
    return result
end

intervene = backdoor_adjustment(data, :Smoking, :Cancer, :Gene)
do_effect = intervene[1] - intervene[0]
println("Interventional P(Cancer|do(Smoking=1)) - P(Cancer|do(Smoking=0)): $(round(do_effect, digits=3))")
println("Difference (confounding bias): $(round(obs_effect - do_effect, digits=3))")
```

### 4.3 å‚¾å‘ã‚¹ã‚³ã‚¢å®Ÿè£…

#### 4.3.1 å‚¾å‘ã‚¹ã‚³ã‚¢æ¨å®š (Logistic Regression)

```julia
using GLM

function estimate_propensity_score(data::DataFrame, treatment::Symbol, covariates::Vector{Symbol})
    # Logistic regression: D ~ X
    formula = term(treatment) ~ sum(term.(covariates))
    model = glm(formula, data, Binomial(), LogitLink())

    # Predict propensity scores
    e_X = predict(model, data)

    return e_X, model
end

# Example: Treatment depends on Age and Income
function generate_ps_data(n::Int=2000)
    age = rand(Normal(40, 10), n)
    income = rand(Normal(50, 15), n)

    # Treatment assignment depends on age and income
    propensity = @. 1 / (1 + exp(-(0.05age + 0.03income - 3.5)))
    treatment = rand(n) .< propensity

    # Outcome depends on treatment + confounders
    outcome = 2.0 .* treatment .+ 0.5 .* age .+ 0.3 .* income .+ randn(n) .* 5

    return DataFrame(Treatment=treatment, Age=age, Income=income, Outcome=outcome)
end

ps_data = generate_ps_data(2000)
e_X, ps_model = estimate_propensity_score(ps_data, :Treatment, [:Age, :Income])

# Add to dataframe
ps_data.PropensityScore = e_X
println("Propensity score range: [$(round(minimum(e_X), digits=3)), $(round(maximum(e_X), digits=3))]")
```

#### 4.3.2 IPWæ¨å®š

```julia
function ipw_estimator(data::DataFrame, treatment::Symbol, outcome::Symbol, propensity::Symbol)
    D = data[:, treatment]
    Y = data[:, outcome]
    e = data[:, propensity]

    # Trimming: exclude extreme propensity scores
    Îµ = 0.05
    valid = (e .> Îµ) .& (e .< (1 - Îµ))
    D_trim = D[valid]
    Y_trim = Y[valid]
    e_trim = e[valid]

    # IPW ATE estimator
    ate = mean(D_trim .* Y_trim ./ e_trim) - mean((1 .- D_trim) .* Y_trim ./ (1 .- e_trim))

    # Variance estimation (Horvitz-Thompson)
    n = length(D_trim)
    var_ipw = var(D_trim .* Y_trim ./ e_trim .- (1 .- D_trim) .* Y_trim ./ (1 .- e_trim)) / n
    se = sqrt(var_ipw)

    return ate, se
end

ate_ipw, se_ipw = ipw_estimator(ps_data, :Treatment, :Outcome, :PropensityScore)
println("IPW ATE: $(round(ate_ipw, digits=3)) Â± $(round(1.96*se_ipw, digits=3)) (95% CI)")

# Compare with naive
ate_naive = mean(ps_data[ps_data.Treatment .== 1, :Outcome]) - mean(ps_data[ps_data.Treatment .== 0, :Outcome])
println("Naive ATE: $(round(ate_naive, digits=3))")
println("True ATE: 2.0")
```

#### 4.3.3 Doubly Robustæ¨å®š

```julia
function doubly_robust_estimator(data::DataFrame, treatment::Symbol, outcome::Symbol,
                                  covariates::Vector{Symbol}, propensity::Symbol)
    D = data[:, treatment]
    Y = data[:, outcome]
    e = data[:, propensity]

    # Outcome regression models
    # Î¼â‚(X) = E[Y | D=1, X]
    data_treated = data[data[:, treatment] .== 1, :]
    formula_1 = term(outcome) ~ sum(term.(covariates))
    model_1 = lm(formula_1, data_treated)
    Î¼_1 = predict(model_1, data)

    # Î¼â‚€(X) = E[Y | D=0, X]
    data_control = data[data[:, treatment] .== 0, :]
    model_0 = lm(formula_1, data_control)
    Î¼_0 = predict(model_0, data)

    # DR estimator
    dr_term_1 = @. D * (Y - Î¼_1) / e + Î¼_1
    dr_term_0 = @. (1 - D) * (Y - Î¼_0) / (1 - e) + Î¼_0
    ate_dr = mean(dr_term_1 .- dr_term_0)

    var_dr = var(dr_term_1 .- dr_term_0) / nrow(data)
    se_dr = sqrt(var_dr)

    return ate_dr, se_dr
end

ate_dr, se_dr = doubly_robust_estimator(ps_data, :Treatment, :Outcome, [:Age, :Income], :PropensityScore)
println("Doubly Robust ATE: $(round(ate_dr, digits=3)) Â± $(round(1.96*se_dr, digits=3)) (95% CI)")
```

#### 4.3.4 ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯

```julia
function balance_check(data::DataFrame, treatment::Symbol, covariates::Vector{Symbol}, propensity::Symbol)
    println("\n=== Balance Check ===")
    for cov in covariates
        # Before matching
        mean_treated = mean(data[data[:, treatment] .== 1, cov])
        mean_control = mean(data[data[:, treatment] .== 0, cov])
        std_pooled = sqrt((var(data[data[:, treatment] .== 1, cov]) +
                           var(data[data[:, treatment] .== 0, cov])) / 2)
        smd_before = abs(mean_treated - mean_control) / std_pooled

        # After IPW weighting
        D = data[:, treatment]
        X = data[:, cov]
        e = data[:, propensity]

        weights_1 = D ./ e
        weights_0 = (1 .- D) ./ (1 .- e)

        mean_1_weighted = sum(weights_1 .* X) / sum(weights_1)
        mean_0_weighted = sum(weights_0 .* X) / sum(weights_0)

        var_1_weighted = sum(weights_1 .* (X .- mean_1_weighted).^2) / sum(weights_1)
        var_0_weighted = sum(weights_0 .* (X .- mean_0_weighted).^2) / sum(weights_0)

        std_pooled_weighted = sqrt((var_1_weighted + var_0_weighted) / 2)
        smd_after = abs(mean_1_weighted - mean_0_weighted) / std_pooled_weighted

        status = smd_after < 0.1 ? "âœ…" : "âŒ"
        println("$cov: SMD before=$(round(smd_before, digits=3)), after=$(round(smd_after, digits=3)) $status")
    end
end

balance_check(ps_data, :Treatment, [:Age, :Income], :PropensityScore)
```

### 4.4 æ“ä½œå¤‰æ•°æ³• (2SLS) å®Ÿè£…

```julia
using GLM

function two_stage_least_squares(data::DataFrame, outcome::Symbol, treatment::Symbol,
                                  instrument::Symbol, covariates::Vector{Symbol}=[])
    # Stage 1: D ~ Z + X
    formula_stage1 = if isempty(covariates)
        term(treatment) ~ term(instrument)
    else
        term(treatment) ~ term(instrument) + sum(term.(covariates))
    end

    model_stage1 = lm(formula_stage1, data)
    D_hat = predict(model_stage1, data)

    # Check first-stage F-statistic
    f_stat = ftest(model_stage1.model).fstat[1]
    println("First-stage F-statistic: $(round(f_stat, digits=2))")
    if f_stat < 10
        @warn "Weak IV detected (F < 10)"
    end

    # Stage 2: Y ~ D_hat + X
    data_stage2 = copy(data)
    data_stage2[!, :D_hat] = D_hat

    formula_stage2 = if isempty(covariates)
        term(outcome) ~ term(:D_hat)
    else
        term(outcome) ~ term(:D_hat) + sum(term.(covariates))
    end

    model_stage2 = lm(formula_stage2, data_stage2)

    # 2SLS coefficient
    Î²_2sls = coef(model_stage2)[2]  # coefficient on D_hat
    se_2sls = stderror(model_stage2)[2]

    return Î²_2sls, se_2sls, f_stat
end

# Generate IV data
function generate_iv_data(n::Int=2000)
    # Unobserved confounder
    U = randn(n)

    # Instrument Z (independent of U)
    Z = rand(Bernoulli(0.5), n)

    # Treatment D depends on Z and U (endogenous)
    D = Z .+ 0.5 .* U .+ randn(n) .* 0.3
    D = D .> median(D)  # binarize

    # Outcome Y depends on D and U (confounded)
    # True causal effect of D: 2.0
    Y = 2.0 .* D .+ U .+ randn(n) .* 0.5

    return DataFrame(Outcome=Y, Treatment=D, Instrument=Z)
end

iv_data = generate_iv_data(2000)

# 2SLS estimation
Î²_2sls, se_2sls, f_stat = two_stage_least_squares(iv_data, :Outcome, :Treatment, :Instrument)
println("2SLS estimate: $(round(Î²_2sls, digits=3)) Â± $(round(1.96*se_2sls, digits=3)) (95% CI)")
println("True causal effect: 2.0")

# Compare with naive OLS (biased)
ols_model = lm(@formula(Outcome ~ Treatment), iv_data)
Î²_ols = coef(ols_model)[2]
println("Naive OLS estimate: $(round(Î²_ols, digits=3)) (biased upward due to U)")
```

### 4.5 RDDå®Ÿè£…

```julia
function regression_discontinuity(data::DataFrame, outcome::Symbol, running_var::Symbol,
                                   cutoff::Float64, bandwidth::Float64)
    # Local linear regression on both sides of cutoff
    X = data[:, running_var]
    Y = data[:, outcome]

    # Filter data within bandwidth
    in_bandwidth = abs.(X .- cutoff) .<= bandwidth
    X_local = X[in_bandwidth]
    Y_local = Y[in_bandwidth]

    # Treatment indicator
    D_local = X_local .>= cutoff

    # Centered running variable
    X_centered = X_local .- cutoff

    # Local linear regression: Y ~ D + X_centered + D*X_centered
    design_matrix = hcat(ones(length(Y_local)), D_local, X_centered, D_local .* X_centered)
    Î² = design_matrix \ Y_local

    # RDD effect = coefficient on D
    rdd_effect = Î²[2]

    # Standard error (simplified - use robust SE in practice)
    residuals = Y_local .- design_matrix * Î²
    se = sqrt(sum(residuals.^2) / (length(Y_local) - 4)) *
         sqrt((design_matrix' * design_matrix)[2, 2]^(-1))

    return rdd_effect, se
end

# Generate RDD data
function generate_rdd_data(n::Int=2000, cutoff::Float64=18.0)
    # Running variable (e.g., age)
    X = rand(Uniform(15, 21), n)

    # Treatment assignment (sharp RDD)
    D = X .>= cutoff

    # Outcome (discontinuity at cutoff)
    # True effect: 3.0
    Y = 10 .+ 0.5 .* X .+ 3.0 .* D .+ randn(n) .* 0.8

    return DataFrame(Age=X, Treatment=D, Outcome=Y)
end

rdd_data = generate_rdd_data(2000, 18.0)

# RDD estimation with bandwidth = 2
rdd_effect, se_rdd = regression_discontinuity(rdd_data, :Outcome, :Age, 18.0, 2.0)
println("RDD estimate (h=2): $(round(rdd_effect, digits=3)) Â± $(round(1.96*se_rdd, digits=3)) (95% CI)")
println("True effect: 3.0")

# Sensitivity to bandwidth
for h in [1.0, 1.5, 2.0, 2.5, 3.0]
    eff, _ = regression_discontinuity(rdd_data, :Outcome, :Age, 18.0, h)
    println("  h=$h: RDD effect = $(round(eff, digits=3))")
end
```

### 4.6 DiDå®Ÿè£…

```julia
function difference_in_differences(data::DataFrame, outcome::Symbol, treatment::Symbol,
                                    post::Symbol, group::Symbol)
    # DiD regression: Y ~ Treatment + Post + Treatment*Post
    formula = term(outcome) ~ term(treatment) + term(post) + term(treatment) & term(post)
    model = lm(formula, data)

    # DiD effect = coefficient on Treatment*Post
    did_effect = coef(model)[end]  # last coefficient
    se_did = stderror(model)[end]

    return did_effect, se_did, model
end

# Generate DiD data
function generate_did_data(n_group::Int=500, n_period::Int=2)
    # 2 groups Ã— 2 periods
    groups = repeat([0, 1], inner=n_group*n_period)
    periods = repeat(repeat([0, 1], inner=n_group), outer=2)
    treatment = (groups .== 1) .& (periods .== 1)

    # Outcome: parallel trends assumption holds
    # Group effect: +5 for treated group
    # Time effect: +2 for post period
    # True DiD effect: +3
    Y = 10 .+ 5 .* groups .+ 2 .* periods .+ 3 .* treatment .+ randn(length(groups))

    return DataFrame(Group=groups, Post=periods, Treatment=treatment, Outcome=Y)
end

did_data = generate_did_data(500, 2)

# DiD estimation
did_effect, se_did, did_model = difference_in_differences(did_data, :Outcome, :Treatment, :Post, :Group)
println("DiD estimate: $(round(did_effect, digits=3)) Â± $(round(1.96*se_did, digits=3)) (95% CI)")
println("True effect: 3.0")

# Event study (pre-trend test)
function event_study(data::DataFrame, outcome::Symbol, group::Symbol, time_periods::Vector{Int})
    # Estimate treatment effects for each period relative to treatment
    # (requires panel data with multiple pre/post periods)

    # Placeholder - full implementation requires panel structure
    println("Event study plot would show pre-treatment trends here")
end
```

### 4.7 Causal Forestå®Ÿè£… (ç°¡æ˜“ç‰ˆ)

```julia
# Simplified Causal Forest implementation
# For production use: CausalELM.jl or R's grf package via RCall.jl

function causal_forest_simple(data::DataFrame, outcome::Symbol, treatment::Symbol,
                               covariates::Vector{Symbol}, n_trees::Int=100)
    # Simplified version: T-Learner with Random Forest-like splits
    # Split data by treatment
    data_treated = data[data[:, treatment] .== 1, :]
    data_control = data[data[:, treatment] .== 0, :]

    # Fit outcome models (linear for simplicity)
    X_cols = covariates
    formula_y = term(outcome) ~ sum(term.(X_cols))

    model_1 = lm(formula_y, data_treated)
    model_0 = lm(formula_y, data_control)

    # Predict CATE for each observation
    Î¼_1 = predict(model_1, data)
    Î¼_0 = predict(model_0, data)

    cate = Î¼_1 .- Î¼_0

    # ATE = mean(CATE)
    ate_cf = mean(cate)

    return ate_cf, cate
end

# Generate heterogeneous treatment effect data
function generate_hte_data(n::Int=2000)
    X1 = randn(n)  # covariate 1
    X2 = randn(n)  # covariate 2

    # Treatment assignment (random)
    D = rand(Bernoulli(0.5), n)

    # Heterogeneous treatment effect: Ï„(X) = 2 + X1
    # Y^1 = 10 + 2*X1 + X2 + (2 + X1)
    # Y^0 = 10 + 2*X1 + X2
    Y1 = 10 .+ 2 .* X1 .+ X2 .+ (2 .+ X1) .+ randn(n) .* 0.5
    Y0 = 10 .+ 2 .* X1 .+ X2 .+ randn(n) .* 0.5
    Y = D .* Y1 .+ (1 .- D) .* Y0

    true_cate = 2 .+ X1  # ground truth

    return DataFrame(Outcome=Y, Treatment=D, X1=X1, X2=X2, TrueCate=true_cate)
end

hte_data = generate_hte_data(2000)

ate_cf, cate_cf = causal_forest_simple(hte_data, :Outcome, :Treatment, [:X1, :X2])
println("Causal Forest ATE: $(round(ate_cf, digits=3))")
println("True ATE (average of 2 + X1): $(round(mean(hte_data.TrueCate), digits=3))")

# Correlation between estimated and true CATE
corr_cate = cor(cate_cf, hte_data.TrueCate)
println("Correlation(estimated CATE, true CATE): $(round(corr_cate, digits=3))")
```

### 4.8 çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ â€” è¤‡æ•°æ‰‹æ³•ã®æ¯”è¼ƒ

```julia
function causal_inference_pipeline(data::DataFrame, scenario::String)
    println("\n=== Causal Inference Pipeline: $scenario ===\n")

    if scenario == "propensity"
        # Propensity score methods
        e_X, _ = estimate_propensity_score(data, :Treatment, [:X1, :X2])
        data.PropensityScore = e_X

        ate_ipw, se_ipw = ipw_estimator(data, :Treatment, :Outcome, :PropensityScore)
        ate_dr, se_dr = doubly_robust_estimator(data, :Treatment, :Outcome, [:X1, :X2], :PropensityScore)

        println("IPW ATE: $(round(ate_ipw, digits=3)) Â± $(round(1.96*se_ipw, digits=3))")
        println("DR ATE: $(round(ate_dr, digits=3)) Â± $(round(1.96*se_dr, digits=3))")

        balance_check(data, :Treatment, [:X1, :X2], :PropensityScore)

    elseif scenario == "iv"
        # Instrumental variables
        Î²_2sls, se_2sls, f_stat = two_stage_least_squares(data, :Outcome, :Treatment, :Instrument)
        println("2SLS estimate: $(round(Î²_2sls, digits=3)) Â± $(round(1.96*se_2sls, digits=3))")
        println("First-stage F: $(round(f_stat, digits=2))")

    elseif scenario == "rdd"
        # Regression discontinuity
        rdd_effect, se_rdd = regression_discontinuity(data, :Outcome, :RunningVar, 0.0, 2.0)
        println("RDD estimate: $(round(rdd_effect, digits=3)) Â± $(round(1.96*se_rdd, digits=3))")

    elseif scenario == "did"
        # Difference-in-differences
        did_effect, se_did, _ = difference_in_differences(data, :Outcome, :Treatment, :Post, :Group)
        println("DiD estimate: $(round(did_effect, digits=3)) Â± $(round(1.96*se_did, digits=3))")

    end
end

# Example: Run propensity score pipeline
ps_test_data = generate_ps_data(2000)
causal_inference_pipeline(ps_test_data, "propensity")
```

> **Note:** **é€²æ—: 70% å®Œäº†** Juliaå› æœæ¨è«–ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã‚’å®Ÿè£…ã—ãŸã€‚DAG/do-æ¼”ç®—/å‚¾å‘ã‚¹ã‚³ã‚¢/IV/RDD/DiD/Causal Forestã®å…¨æ‰‹æ³•ã‚’CausalInference.jlã§å®Ÿè£…ã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã«é©ç”¨ã™ã‚‹ã€‚

---


> Progress: [85%]
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°å¾Œã®ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ã§æ¨™æº–åŒ–å·®ï¼ˆSMDï¼‰ãŒ0.1æœªæº€ã‚’ç›®å®‰ã«ã™ã‚‹ç†ç”±ã¯ï¼Ÿ
> 2. 2SLSæ¨å®šé‡ã®ç¬¬ä¸€æ®µéšFçµ±è¨ˆé‡ãŒ10æœªæº€ã®ã¨ãã€Œå¼±æ“ä½œå¤‰æ•°ã€ã¨åˆ¤å®šã•ã‚Œã‚‹æ ¹æ‹ ã¯ï¼Ÿ

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” å®Ÿãƒ‡ãƒ¼ã‚¿å› æœæ¨è«–ãƒãƒ£ãƒ¬ãƒ³ã‚¸

### 5.1 ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã§å…¨æ‰‹æ³•æ¯”è¼ƒ

```julia
# Generate comprehensive causal inference test data
function comprehensive_causal_data(n::Int=3000)
    # Confounders
    age = rand(Normal(40, 12), n)
    income = rand(Normal(50, 20), n)

    # Propensity score (selection on observables)
    e_X = @. 1 / (1 + exp(-(0.05age + 0.03income - 3.0)))
    treatment = rand(n) .< e_X

    # Instrumental variable (random assignment)
    instrument = rand(Bernoulli(0.5), n)

    # Outcome (true effect = 5.0)
    outcome = 5.0 .* treatment .+ 0.3 .* age .+ 0.2 .* income .+ randn(n) .* 3.0

    return DataFrame(
        Treatment=treatment,
        Outcome=outcome,
        Age=age,
        Income=income,
        Instrument=instrument,
        PropensityScore=e_X
    )
end

test_data = comprehensive_causal_data(3000)

# Method 1: Naive comparison
ate_naive = mean(test_data[test_data.Treatment .== 1, :Outcome]) -
            mean(test_data[test_data.Treatment .== 0, :Outcome])

# Method 2: IPW
ate_ipw, se_ipw = ipw_estimator(test_data, :Treatment, :Outcome, :PropensityScore)

# Method 3: Doubly Robust
ate_dr, se_dr = doubly_robust_estimator(test_data, :Treatment, :Outcome,
                                         [:Age, :Income], :PropensityScore)

# Method 4: Regression Adjustment
reg_model = lm(@formula(Outcome ~ Treatment + Age + Income), test_data)
ate_reg = coef(reg_model)[2]

println("\n=== Method Comparison ===")
println("True ATE: 5.0")
println("Naive: $(round(ate_naive, digits=3))")
println("IPW: $(round(ate_ipw, digits=3)) Â± $(round(1.96*se_ipw, digits=3))")
println("Doubly Robust: $(round(ate_dr, digits=3)) Â± $(round(1.96*se_dr, digits=3))")
println("Regression Adjustment: $(round(ate_reg, digits=3))")
```

### 5.2 æ„Ÿåº¦åˆ†æ â€” æœªæ¸¬å®šäº¤çµ¡ã¸ã®é ‘å¥æ€§

```julia
# Rosenbaum's Î“ sensitivity analysis (simplified)
function sensitivity_analysis_gamma(ate_estimated::Float64, se::Float64, gamma_range::Vector{Float64})
    println("\n=== Sensitivity Analysis (Rosenbaum's Î“) ===")
    println("Î“ = odds ratio of differential treatment assignment due to unobserved confounder")

    for gamma in gamma_range
        # Under confounding by unobserved U, bounds on ATE
        # Simplified: scale SE by gamma
        ci_lower = ate_estimated - 1.96 * se * gamma
        ci_upper = ate_estimated + 1.96 * se * gamma

        significant = (ci_lower > 0) || (ci_upper < 0)
        status = significant ? "âœ… Still significant" : "âŒ Not significant"

        println("Î“=$gamma: CI = [$(round(ci_lower, digits=2)), $(round(ci_upper, digits=2))] $status")
    end
end

sensitivity_analysis_gamma(ate_dr, se_dr, [1.0, 1.5, 2.0, 2.5, 3.0])
```

### 5.3 A/Bãƒ†ã‚¹ãƒˆçµ±åˆ â€” Sample Ratio Mismatchæ¤œå‡º

```julia
function sample_ratio_mismatch_test(data::DataFrame, treatment::Symbol, expected_ratio::Float64=0.5)
    # Test if observed treatment ratio matches expected ratio
    n_total = nrow(data)
    n_treated = sum(data[:, treatment])
    n_control = n_total - n_treated

    observed_ratio = n_treated / n_total

    # Chi-square test
    expected_treated = n_total * expected_ratio
    expected_control = n_total * (1 - expected_ratio)

    chi_sq = (n_treated - expected_treated)^2 / expected_treated +
             (n_control - expected_control)^2 / expected_control

    p_value = 1 - cdf(Chisq(1), chi_sq)

    println("\n=== Sample Ratio Mismatch Test ===")
    println("Expected ratio: $expected_ratio")
    println("Observed ratio: $(round(observed_ratio, digits=4))")
    println("Ï‡Â² = $(round(chi_sq, digits=3)), p = $(round(p_value, digits=4))")

    if p_value < 0.05
        println("âš ï¸ SRM detected! Treatment assignment may be biased.")
    else
        println("âœ… No SRM detected.")
    end

    return chi_sq, p_value
end

sample_ratio_mismatch_test(test_data, :Treatment, 0.5)
```

### 5.4 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

#### ãƒ†ã‚¹ãƒˆ1: è¨˜æ³•ç†è§£ï¼ˆ10å•ï¼‰

<details><summary>Q1: $\mathbb{E}[Y^1 - Y^0]$ ã¯ä½•ã‚’è¡¨ã™ã‹ï¼Ÿ</summary>

**Answer**: ATE (Average Treatment Effect) â€” å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ

$$
\text{ATE} = \mathbb{E}[Y^1 - Y^0] = \mathbb{E}[Y^1] - \mathbb{E}[Y^0]
$$

**è£œè¶³**: ã“ã‚Œã¯å€‹ä½“ãƒ¬ãƒ™ãƒ«ã®å‡¦ç½®åŠ¹æœ $\tau_i = Y_i^1 - Y_i^0$ ã®æœŸå¾…å€¤ã€‚å€‹ä½“ãƒ¬ãƒ™ãƒ«ã¯è¦³æ¸¬ä¸èƒ½ï¼ˆæ ¹æœ¬çš„å› æœæ¨è«–å•é¡Œï¼‰ã ãŒã€é›†å›£å¹³å‡ãªã‚‰æ¨å®šå¯èƒ½ã€‚

</details>

<details><summary>Q2: $P(Y \mid do(X=x))$ ã¨ $P(Y \mid X=x)$ ã®é•ã„ã¯ï¼Ÿ</summary>

**Answer**:
- $P(Y \mid do(X=x))$: **ä»‹å…¥ç¢ºç‡** â€” $X$ ã‚’å¤–éƒ¨ã‹ã‚‰å¼·åˆ¶çš„ã« $x$ ã«å›ºå®šã—ãŸå ´åˆã® $Y$ ã®åˆ†å¸ƒ
- $P(Y \mid X=x)$: **æ¡ä»¶ä»˜ãç¢ºç‡** â€” $X=x$ ã‚’è¦³æ¸¬ã—ãŸå ´åˆã® $Y$ ã®åˆ†å¸ƒï¼ˆäº¤çµ¡ã‚ã‚Šï¼‰

ä»‹å…¥ç¢ºç‡ã¯å› æœåŠ¹æœã€æ¡ä»¶ä»˜ãç¢ºç‡ã¯ç›¸é–¢ã‚’è¡¨ã™ã€‚

**ä¾‹**: å–«ç…™ã¨ãŒã‚“
- $P(\text{ãŒã‚“} \mid \text{å–«ç…™}=1)$: å–«ç…™è€…ã®ãŒã‚“ç‡ï¼ˆéºä¼ã®äº¤çµ¡ã‚ã‚Šï¼‰
- $P(\text{ãŒã‚“} \mid do(\text{å–«ç…™}=1))$: å¼·åˆ¶çš„ã«å–«ç…™ã•ã›ãŸå ´åˆã®ãŒã‚“ç‡ï¼ˆå› æœåŠ¹æœï¼‰

å‰è€…ã¯ç›¸é–¢ã€å¾Œè€…ã¯å› æœã€‚Simpson's Paradoxã§ã¯ä¸¡è€…ãŒé€†è»¢ã™ã‚‹ã“ã¨ã™ã‚‰ã‚ã‚‹ã€‚

</details>

<details><summary>Q3: $e(X) = P(D=1 \mid X)$ ã®åå‰ã¨å½¹å‰²ã¯ï¼Ÿ</summary>

**Answer**: **å‚¾å‘ã‚¹ã‚³ã‚¢ (Propensity Score)**

é«˜æ¬¡å…ƒã®å…±å¤‰é‡ $X$ ã‚’1æ¬¡å…ƒã®ã‚¹ã‚«ãƒ©ãƒ¼ã«åœ§ç¸®ã€‚$(Y^1, Y^0) \perp\!\!\!\perp D \mid X$ ãªã‚‰ $(Y^1, Y^0) \perp\!\!\!\perp D \mid e(X)$ ã‚‚æˆç«‹ï¼ˆæ¬¡å…ƒå‰Šæ¸›ï¼‰ã€‚

**å®Ÿç”¨ä¸Šã®ãƒ¡ãƒªãƒƒãƒˆ**:
- $X$ ãŒ10æ¬¡å…ƒã§ã‚‚ $e(X)$ ã¯1æ¬¡å…ƒ â†’ ãƒãƒƒãƒãƒ³ã‚°ãŒå®¹æ˜“
- å…±é€šã‚µãƒãƒ¼ãƒˆ $0 < e(X) < 1$ ã®ç¢ºèªãŒç°¡å˜
- IPWæ¨å®šã§ $1/e(X)$ ã®é‡ã¿ã‚’ä½¿ã†ã ã‘ã§å› æœåŠ¹æœæ¨å®šå¯èƒ½

</details>

<details><summary>Q4: SUTVAã®2ã¤ã®ä»®å®šã‚’è¿°ã¹ã‚ˆ</summary>

**Answer**:
1. **å‡¦ç½®ã®ä¸€æ„æ€§**: å€‹ä½“ $i$ ã®å‡¦ç½®ãŒ $d$ ã®ã¨ãã€çµæœã¯ $Y_i^d$ ã®1ã¤ã®ã¿
2. **å¹²æ¸‰ãªã— (No Interference)**: å€‹ä½“ $i$ ã®çµæœã¯ä»–ã®å€‹ä½“ã®å‡¦ç½®ã«ä¾å­˜ã—ãªã„

$$
Y_i^d = Y_i^{d_i} \quad \forall d_{-i}
$$

**ç ´ã‚Œã‚‹ä¾‹**:
- ãƒ¯ã‚¯ãƒãƒ³æ¥ç¨®: ä»–äººãŒæ¥ç¨®ã™ã‚‹ã¨è‡ªåˆ†ã®æ„ŸæŸ“ãƒªã‚¹ã‚¯ä½ä¸‹ï¼ˆå¹²æ¸‰ã‚ã‚Šï¼‰
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åºƒå‘Š: å‹äººãŒã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨è‡ªåˆ†ã‚‚ã‚¯ãƒªãƒƒã‚¯ï¼ˆspilloveråŠ¹æœï¼‰
- æ•™å®¤å†…ã®å‡¦ç½®: åŒã˜ã‚¯ãƒ©ã‚¹ã®å­¦ç”Ÿé–“ã§ç›¸äº’å½±éŸ¿

SUTVAãŒç ´ã‚Œã‚‹å ´åˆã¯ã€**Spillover Effects** ã‚„ **Network Effects** ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

</details>

<details><summary>Q5: ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº–ã‚’æº€ãŸã™å¤‰æ•°é›†åˆ $Z$ ã®æ¡ä»¶ã¯ï¼Ÿ</summary>

**Answer**:
1. $Z$ ã®ã©ã®å¤‰æ•°ã‚‚ $X$ ã®å­å­«ã§ãªã„
2. $Z$ ãŒ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ã‚’é®æ–­ã™ã‚‹

æº€ãŸã›ã°:

$$
P(Y \mid do(X=x)) = \sum_z P(Y \mid X=x, Z=z) P(Z=z)
$$

**ç›´æ„Ÿ**:
- æ¡ä»¶1: $X$ ã®çµæœ ($X$ ã®å­å­«) ã§æ¡ä»¶ã¥ã‘ã‚‹ã¨ã€Collider BiasãŒç™ºç”Ÿã™ã‚‹
- æ¡ä»¶2: ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ï¼ˆ$X \leftarrow \cdots \to Y$ï¼‰ã‚’é®æ–­ã—ãªã„ã¨äº¤çµ¡ãŒæ®‹ã‚‹

**ä¾‹**: å–«ç…™â†’ãŒã‚“ã€ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹: å–«ç…™â†éºä¼â†’ãŒã‚“
- $Z = \{\text{éºä¼}\}$ ã§æ¡ä»¶ã¥ã‘ã‚‹ã¨ãƒãƒƒã‚¯ãƒ‰ã‚¢ãƒ‘ã‚¹ãŒé®æ–­ã•ã‚Œã‚‹
- $Z = \{\text{ã‚¿ãƒ¼ãƒ«æ²ˆç€}\}$ (å–«ç…™ã®çµæœ) ã§æ¡ä»¶ã¥ã‘ã‚‹ã¨Collider BiasãŒç™ºç”Ÿ

</details>

<details><summary>Q6: d-åˆ†é›¢ã¨ã¯ä½•ã‹ï¼Ÿ</summary>

**Answer**: DAGä¸Šã§å¤‰æ•°é›†åˆ $Z$ ãŒ $X$ ã¨ $Y$ ã‚’ d-åˆ†é›¢ã™ã‚‹ $\iff$ $X$ ã‹ã‚‰ $Y$ ã¸ã®ã™ã¹ã¦ã®ãƒ‘ã‚¹ãŒ $Z$ ã«ã‚ˆã£ã¦é®æ–­ã•ã‚Œã‚‹ã€‚

**ãƒ‘ã‚¹é®æ–­æ¡ä»¶**:
- **Chain** $X \to Z \to Y$: $Z \in \mathcal{Z}$ ãªã‚‰é®æ–­
- **Fork** $X \leftarrow Z \to Y$: $Z \in \mathcal{Z}$ ãªã‚‰é®æ–­
- **Collider** $X \to Z \leftarrow Y$: $Z \notin \mathcal{Z}$ ã‹ã¤ $\text{DE}(Z) \cap \mathcal{Z} = \emptyset$ ãªã‚‰é®æ–­

**d-åˆ†é›¢ã®é‡è¦æ€§**: $X \perp_d Y \mid Z$ (d-åˆ†é›¢) $\Rightarrow$ $X \perp\!\!\!\perp Y \mid Z$ (æ¡ä»¶ä»˜ãç‹¬ç«‹)

</details>

<details><summary>Q7: Colliderã§æ¡ä»¶ã¥ã‘ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ï¼Ÿ</summary>

**Answer**: **é¸æŠãƒã‚¤ã‚¢ã‚¹** â€” ç‹¬ç«‹ã ã£ãŸå¤‰æ•°ãŒæ¡ä»¶ä»˜ãã§ç›¸é–¢ã™ã‚‹

**ä¾‹**: æ‰èƒ½ã¨åŠªåŠ›

```mermaid
graph TD
    T["æ‰èƒ½"] --> A["åˆæ ¼"]
    E["åŠªåŠ›"] --> A
```

æ‰èƒ½ã¨åŠªåŠ›ã¯ç‹¬ç«‹ $T \perp\!\!\!\perp E$ ã ãŒã€åˆæ ¼è€… $A=1$ ã‚’æ¡ä»¶ã¥ã‘ã‚‹ã¨:

$$
T \not\perp\!\!\!\perp E \mid A=1
$$

åˆæ ¼è€…ã®ä¸­ã§ã¯ã€ŒåŠªåŠ›ãŒå°‘ãªã„â†’æ‰èƒ½ãŒé«˜ã„ã€ã¨ã„ã†è² ã®ç›¸é–¢ãŒç”Ÿã¾ã‚Œã‚‹ã€‚ã“ã‚ŒãŒ**Berkson's Paradox**ã€‚

**å®Ÿç”¨ä¾‹**: ç—…é™¢æ‚£è€…ãƒ‡ãƒ¼ã‚¿ã§ç–¾æ‚£Aã¨ç–¾æ‚£BãŒè² ã®ç›¸é–¢ â†’ å…¥é™¢ï¼ˆColliderï¼‰ã§æ¡ä»¶ã¥ã‘ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚

</details>

<details><summary>Q8: Unconfoundednessä»®å®šã¨ã¯ï¼Ÿ</summary>

**Answer**: $(Y^1, Y^0) \perp\!\!\!\perp D \mid X$

å…±å¤‰é‡ $X$ ã‚’æ‰€ä¸ã¨ã™ã‚Œã°ã€æ½œåœ¨çš„çµæœã¨å‡¦ç½®å‰²ã‚Šå½“ã¦ãŒç‹¬ç«‹ã€‚

**æ„å‘³**: $X$ ã‚’åˆ¶å¾¡ã™ã‚Œã°ã€å‡¦ç½®ã¯ãƒ©ãƒ³ãƒ€ãƒ å‰²ã‚Šå½“ã¦ã¨åŒç­‰ï¼ˆselection on observablesï¼‰ã€‚

**æˆã‚Šç«‹ã¤æ¡ä»¶**:
- ã™ã¹ã¦ã®äº¤çµ¡å› å­ $X$ ã‚’æ¸¬å®šã—ã¦ã„ã‚‹
- æœªæ¸¬å®šäº¤çµ¡ $U$ ãŒå­˜åœ¨ã—ãªã„

**ç ´ã‚Œã‚‹ä¾‹**: èƒ½åŠ› $U$ ãŒæœªæ¸¬å®šã§ã€$U \to D$ ã‹ã¤ $U \to Y$ ãªã‚‰ Unconfoundedness ã¯æˆã‚Šç«‹ãŸãªã„ â†’ IV/RDD/DiDãªã©ä»–ã®æ‰‹æ³•ãŒå¿…è¦

</details>

<details><summary>Q9: LATEã¨ATEã®é•ã„ã¯ï¼Ÿ</summary>

**Answer**:
- **ATE**: $\mathbb{E}[Y^1 - Y^0]$ â€” å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ
- **LATE**: $\mathbb{E}[Y^1 - Y^0 \mid \text{Complier}]$ â€” ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ï¼ˆæ“ä½œå¤‰æ•°ã«å¾“ã†äººï¼‰ã®å‡¦ç½®åŠ¹æœ

**IVã§æ¨å®šã•ã‚Œã‚‹ã®ã¯LATE**:

$$
\text{LATE} = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0]}
$$

**4ã¤ã®ã‚¿ã‚¤ãƒ—**:
- Always-Taker: å¸¸ã«å‡¦ç½®ã‚’å—ã‘ã‚‹ï¼ˆIVã«ç„¡é–¢ä¿‚ï¼‰
- Never-Taker: å¸¸ã«å‡¦ç½®ã‚’å—ã‘ãªã„ï¼ˆIVã«ç„¡é–¢ä¿‚ï¼‰
- **Complier**: IVã«å¾“ã†ï¼ˆLATEã®å¯¾è±¡ï¼‰
- Defier: IVã«é€†ã‚‰ã†ï¼ˆMonotonicityä»®å®šã§æ’é™¤ï¼‰

**ATE vs LATE**: LATEã¯ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ã®ã¿ã®åŠ¹æœãªã®ã§ã€ATEã‚ˆã‚Šå±€æ‰€çš„ã€‚å¤–éƒ¨å¦¥å½“æ€§ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

</details>

<details><summary>Q10: ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®šã¨ã¯ï¼Ÿ</summary>

**Answer**: DiDã®è­˜åˆ¥ä»®å®š

$$
\mathbb{E}[Y_{01} - Y_{00} \mid G=1] = \mathbb{E}[Y_{01} - Y_{00} \mid G=0]
$$

å‡¦ç½®ãŒãªã‹ã£ãŸå ´åˆã€å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯å¹³è¡Œã€‚

**ç›´æ„Ÿ**: å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã¯å‡¦ç½®å‰ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãŒåŒã˜ â†’ å‡¦ç½®å¾Œã®å·®åˆ†ã¯å‡¦ç½®åŠ¹æœ

**æ¤œè¨¼æ–¹æ³•**:
- Event Study: å‡¦ç½®å‰ã®è¤‡æ•°æœŸé–“ã§ãƒˆãƒ¬ãƒ³ãƒ‰ãŒå¹³è¡Œã‹ç¢ºèª
- Placebo Test: å‡¦ç½®å‰æœŸé–“ã§ã€Œå½ã®å‡¦ç½®ã€ã‚’è¨­å®šã—ã€åŠ¹æœãŒã‚¼ãƒ­ã‹ç¢ºèª

**ç ´ã‚Œã‚‹ä¾‹**: å‡¦ç½®ç¾¤ãŒé«˜æˆé•·ä¼æ¥­ã€å¯¾ç…§ç¾¤ãŒä½æˆé•·ä¼æ¥­ â†’ ã‚‚ã¨ã‚‚ã¨ãƒˆãƒ¬ãƒ³ãƒ‰ãŒç•°ãªã‚‹ â†’ DiDã¯é©ç”¨ä¸å¯

</details>

#### ãƒ†ã‚¹ãƒˆ2: æ•°å¼å°å‡ºï¼ˆ5å•ï¼‰

<details><summary>Q1: IPWæ¨å®šé‡ãŒä¸åã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã›</summary>

**Proof**:

$$
\begin{aligned}
\mathbb{E}\left[\frac{D Y}{e(X)}\right] &= \mathbb{E}\left[\mathbb{E}\left[\frac{D Y}{e(X)} \mid X\right]\right] \\
&= \mathbb{E}\left[\frac{\mathbb{E}[D Y \mid X]}{e(X)}\right] \\
&= \mathbb{E}\left[\frac{P(D=1 \mid X) \mathbb{E}[Y \mid D=1, X]}{e(X)}\right] \\
&= \mathbb{E}\left[\frac{e(X) \mathbb{E}[Y^1 \mid X]}{e(X)}\right] \quad \text{(unconfoundedness)} \\
&= \mathbb{E}[Y^1]
\end{aligned}
$$

åŒæ§˜ã« $\mathbb{E}\left[\frac{(1-D) Y}{1-e(X)}\right] = \mathbb{E}[Y^0]$ã€‚ã‚ˆã£ã¦:

$$
\mathbb{E}[\hat{\text{ATE}}_{\text{IPW}}] = \mathbb{E}[Y^1] - \mathbb{E}[Y^0] = \text{ATE}
$$

**Key Stepè§£èª¬**:
- Step 3â†’4: Unconfoundedness $(Y^1, Y^0) \perp\!\!\!\perp D \mid X$ ã«ã‚ˆã‚Š $\mathbb{E}[Y \mid D=1, X] = \mathbb{E}[Y^1 \mid X]$
- Step 4â†’5: $e(X) = P(D=1 \mid X)$ ãªã®ã§ç´„åˆ†
- Overlapä»®å®š $0 < e(X) < 1$ ãŒå¿…é ˆï¼ˆåˆ†æ¯ãŒã‚¼ãƒ­ã«ãªã‚‰ãªã„ï¼‰

</details>

<details><summary>Q2: 2SLSæ¨å®šé‡ã‚’å°å‡ºã›ã‚ˆï¼ˆWaldæ¨å®šé‡å½¢å¼ï¼‰</summary>

**Derivation**:

æ§‹é€ æ–¹ç¨‹å¼:

$$
\begin{aligned}
D &= \pi_0 + \pi_1 Z + \nu \\
Y &= \beta_0 + \beta_1 D + U
\end{aligned}
$$

$U$ ã¨ $Z$ ãŒç„¡ç›¸é–¢ï¼ˆå¤–ç”Ÿæ€§ï¼‰ã€$Z$ ã¨ $D$ ãŒç›¸é–¢ï¼ˆé–¢é€£æ€§ï¼‰ã‚’ä»®å®šã€‚

$$
\begin{aligned}
\text{Cov}(Y, Z) &= \text{Cov}(\beta_0 + \beta_1 D + U, Z) \\
&= \beta_1 \text{Cov}(D, Z) + \text{Cov}(U, Z) \\
&= \beta_1 \text{Cov}(D, Z) \quad \text{(å¤–ç”Ÿæ€§: } \text{Cov}(U,Z)=0)
\end{aligned}
$$

$$
\hat{\beta}_1 = \frac{\text{Cov}(Y, Z)}{\text{Cov}(D, Z)} = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[D \mid Z=1] - \mathbb{E}[D \mid Z=0]}
$$

ã“ã‚ŒãŒ2SLSæ¨å®šé‡ï¼ˆWaldæ¨å®šé‡ï¼‰ã€‚

**ç›´æ„Ÿ**:
- åˆ†å­: IVãŒ $Y$ ã«ä¸ãˆã‚‹ç·åŠ¹æœï¼ˆreduced formï¼‰
- åˆ†æ¯: IVãŒ $D$ ã«ä¸ãˆã‚‹åŠ¹æœï¼ˆfirst stageï¼‰
- æ¯”: $D$ ãŒ $Y$ ã«ä¸ãˆã‚‹å› æœåŠ¹æœï¼ˆstructural effectï¼‰

**æ¡ä»¶**:
- å¤–ç”Ÿæ€§: $\text{Cov}(U, Z) = 0$
- é–¢é€£æ€§: $\text{Cov}(D, Z) \neq 0$ (å¼±IVãªã‚‰åˆ†æ¯ãŒå°ã•ããƒã‚¤ã‚¢ã‚¹å¤§)
- æ’é™¤åˆ¶ç´„: $Z \to Y$ ã®ç›´æ¥ãƒ‘ã‚¹ãªã—

</details>

<details><summary>Q3: DiDæ¨å®šé‡ã‚’å°å‡ºã›ã‚ˆ</summary>

**Setup**: 2æœŸé–“ $t \in \{0, 1\}$, 2ã‚°ãƒ«ãƒ¼ãƒ— $G \in \{0, 1\}$

æ½œåœ¨çš„çµæœ:
- $Y_{it}^0$: å‡¦ç½®ãªã—ã®çµæœ
- $Y_{it}^1$: å‡¦ç½®ã‚ã‚Šã®çµæœ

è¦³æ¸¬çµæœ:

$$
Y_{it} = \begin{cases}
Y_{it}^0 & \text{if } G=0 \text{ or } t=0 \\
Y_{it}^1 & \text{if } G=1 \text{ and } t=1
\end{cases}
$$

**DiDæ¨å®šé‡**:

$$
\begin{aligned}
\hat{\tau}_{\text{DiD}} &= (\mathbb{E}[Y_{11}] - \mathbb{E}[Y_{10}]) - (\mathbb{E}[Y_{01}] - \mathbb{E}[Y_{00}]) \\
&= (\mathbb{E}[Y_{11}^1 \mid G=1] - \mathbb{E}[Y_{10}^0 \mid G=1]) \\
&\quad - (\mathbb{E}[Y_{01}^0 \mid G=0] - \mathbb{E}[Y_{00}^0 \mid G=0])
\end{aligned}
$$

**ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®š**:

$$
\mathbb{E}[Y_{11}^0 - Y_{10}^0 \mid G=1] = \mathbb{E}[Y_{01}^0 - Y_{00}^0 \mid G=0]
$$

å‡¦ç½®ãŒãªã‹ã£ãŸå ´åˆã®ãƒˆãƒ¬ãƒ³ãƒ‰ãŒå¹³è¡Œ â†’ ã“ã‚Œã‚’ä½¿ã†ã¨:

$$
\begin{aligned}
\hat{\tau}_{\text{DiD}} &= \mathbb{E}[Y_{11}^1 - Y_{10}^0 \mid G=1] - (\mathbb{E}[Y_{11}^0 - Y_{10}^0 \mid G=1]) \\
&= \mathbb{E}[Y_{11}^1 - Y_{11}^0 \mid G=1] \\
&= \text{ATT}
\end{aligned}
$$

DiDã¯ATTï¼ˆå‡¦ç½®ç¾¤ã®å¹³å‡å‡¦ç½®åŠ¹æœï¼‰ã‚’è­˜åˆ¥ã™ã‚‹ã€‚

</details>

<details><summary>Q4: Doubly Robustæ¨å®šé‡ãŒ2é‡é ‘å¥ã§ã‚ã‚‹ç†ç”±ã‚’ç¤ºã›</summary>

**DRæ¨å®šé‡**:

$$
\hat{\tau}_{\text{DR}} = \frac{1}{n} \sum_i \left[ \frac{D_i (Y_i - \hat{\mu}_1(X_i))}{\hat{e}(X_i)} + \hat{\mu}_1(X_i) - \frac{(1-D_i)(Y_i - \hat{\mu}_0(X_i))}{1-\hat{e}(X_i)} - \hat{\mu}_0(X_i) \right]
$$

**Case 1**: $\hat{\mu}_1, \hat{\mu}_0$ ãŒæ­£ã—ã„ï¼ˆ$\hat{e}$ ãŒèª¤ã‚Šã§ã‚‚OKï¼‰

$$
\begin{aligned}
\mathbb{E}[\hat{\tau}_{\text{DR}}] &= \mathbb{E}\left[\frac{D(Y - \mu_1(X))}{\hat{e}(X)} + \mu_1(X)\right] - \mathbb{E}\left[\frac{(1-D)(Y - \mu_0(X))}{1-\hat{e}(X)} + \mu_0(X)\right] \\
&= \mathbb{E}\left[\mathbb{E}\left[\frac{D(Y - \mu_1(X))}{\hat{e}(X)} \mid X\right] + \mu_1(X)\right] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}\left[\frac{\mathbb{E}[D(Y - \mu_1(X)) \mid X]}{\hat{e}(X)} + \mu_1(X)\right] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}\left[\frac{e(X)(\mu_1(X) - \mu_1(X))}{\hat{e}(X)} + \mu_1(X)\right] - \mathbb{E}[\mu_0(X)] \quad \text{(} \mathbb{E}[Y \mid D=1, X] = \mu_1(X)) \\
&= \mathbb{E}[\mu_1(X)] - \mathbb{E}[\mu_0(X)] \\
&= \mathbb{E}[Y^1 - Y^0] = \text{ATE}
\end{aligned}
$$

**Case 2**: $\hat{e}$ ãŒæ­£ã—ã„ï¼ˆ$\hat{\mu}$ ãŒèª¤ã‚Šã§ã‚‚OKï¼‰

IPWã®ä¸åæ€§ã«ã‚ˆã‚Š $\mathbb{E}[\hat{\tau}_{\text{DR}}] = \text{ATE}$

**çµè«–**: $\hat{\mu}$ or $\hat{e}$ ã®ã©ã¡ã‚‰ã‹ä¸€æ–¹ãŒæ­£ã—ã‘ã‚Œã°ä¸å â†’ 2é‡é ‘å¥æ€§

</details>

<details><summary>Q5: RDDåŠ¹æœã‚’å°å‡ºã›ã‚ˆï¼ˆSharp RDDï¼‰</summary>

**Setup**: ã‚«ãƒƒãƒˆã‚ªãƒ• $c$ ã§å‡¦ç½®å‰²ã‚Šå½“ã¦

$$
D_i = \mathbb{1}(X_i \geq c)
$$

**å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–ä»®å®š**:

$$
\lim_{x \to c} \mathbb{E}[Y^1 - Y^0 \mid X=x] = \tau_c
$$

ã‚«ãƒƒãƒˆã‚ªãƒ•è¿‘å‚ã§å‡¦ç½®åŠ¹æœãŒä¸€å®šã€‚

**RDDåŠ¹æœ**:

$$
\begin{aligned}
\tau_{\text{RDD}} &= \lim_{x \to c^+} \mathbb{E}[Y \mid X=x] - \lim_{x \to c^-} \mathbb{E}[Y \mid X=x] \\
&= \lim_{x \to c^+} \mathbb{E}[Y^1 \mid X=x] - \lim_{x \to c^-} \mathbb{E}[Y^0 \mid X=x] \\
&= \mathbb{E}[Y^1 - Y^0 \mid X=c] \\
&= \text{ATE}_c
\end{aligned}
$$

**Key**: ã‚«ãƒƒãƒˆã‚ªãƒ•ã§ã®ä¸é€£ç¶šæ€§ãŒå› æœåŠ¹æœã‚’è¡¨ã™ã€‚

**æ¨å®š**: Local Linear Regression

$$
\min_{\beta_0, \beta_1, \beta_2, \beta_3} \sum_{i: |X_i - c| < h} K\left(\frac{X_i - c}{h}\right) (Y_i - \beta_0 - \beta_1 D_i - \beta_2 (X_i - c) - \beta_3 D_i (X_i - c))^2
$$

$\hat{\beta}_1 = \hat{\tau}_{\text{RDD}}$

</details>

#### ãƒ†ã‚¹ãƒˆ3: Juliaå®Ÿè£…ï¼ˆ5å•ï¼‰

<details><summary>Q1: å‚¾å‘ã‚¹ã‚³ã‚¢ã‚’æ¨å®šã—ã€å…±é€šã‚µãƒãƒ¼ãƒˆã‚’ç¢ºèªã›ã‚ˆ</summary>

```julia
# 1. Estimate propensity score
e_X, model = estimate_propensity_score(data, :Treatment, [:Age, :Income])

# 2. Check common support
println("Min e(X): $(minimum(e_X))")
println("Max e(X): $(maximum(e_X))")

# 3. Visualize overlap
using Plots
histogram([e_X[data.Treatment .== 0], e_X[data.Treatment .== 1]],
          label=["Control" "Treated"],
          alpha=0.6,
          xlabel="Propensity Score",
          ylabel="Frequency",
          title="Common Support Check")

# 4. Trimming
Îµ = 0.05
trimmed = (e_X .> Îµ) .& (e_X .< (1 - Îµ))
println("Trimmed $(sum(.!trimmed)) observations ($(round(100*mean(.!trimmed), digits=1))%)")
```

</details>

### 5.5 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: æ•™è‚²ä»‹å…¥ã®å› æœåŠ¹æœæ¨å®š

**ã‚·ãƒŠãƒªã‚ª**: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ•™è‚²ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®åŠ¹æœã‚’æ¨å®šã›ã‚ˆã€‚

- **å‡¦ç½®**: ãƒ—ãƒ­ã‚°ãƒ©ãƒ å—è¬› (1=å—è¬›, 0=éå—è¬›)
- **çµæœ**: ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢
- **å…±å¤‰é‡**: å¹´é½¢ã€äº‹å‰ã‚¹ã‚³ã‚¢ã€æ‰€å¾—
- **æ“ä½œå¤‰æ•°**: ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ¼ãƒãƒ³é…å¸ƒ

**ã‚¿ã‚¹ã‚¯**:

1. å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚° â†’ ATEæ¨å®š
2. 2SLS (ã‚¯ãƒ¼ãƒãƒ³ã‚’IV) â†’ LATEæ¨å®š
3. æ„Ÿåº¦åˆ†æ â†’ æœªæ¸¬å®šäº¤çµ¡ã¸ã®é ‘å¥æ€§
4. çµæœã‚’æ¯”è¼ƒã—ã€æœ€ã‚‚ä¿¡é ¼ã§ãã‚‹æ¨å®šå€¤ã‚’é¸æŠ

```julia
# Mini Project: Education Program Causal Effect

# Data generation
function education_program_data(n::Int=2000)
    # Covariates
    age = rand(Uniform(18, 35), n)
    baseline_score = rand(Normal(60, 15), n)
    income = rand(Normal(50, 20), n)

    # Unobserved ability (confounder)
    ability = randn(n)

    # Instrument: random coupon
    coupon = rand(Bernoulli(0.5), n)

    # Treatment: program enrollment (endogenous)
    # Depends on: coupon, covariates, ability
    enroll_prob = @. 1 / (1 + exp(-(0.8coupon + 0.02age - 0.01baseline_score +
                                   0.01income + 0.3ability - 1.0)))
    enroll = rand(n) .< enroll_prob

    # Outcome: test score
    # True program effect: 10 points
    # Also depends on baseline score and ability
    test_score = 50 .+ 10 .* enroll .+ 0.5 .* baseline_score .+ 5 .* ability .+ randn(n) .* 8

    return DataFrame(
        Enroll=enroll,
        TestScore=test_score,
        Age=age,
        BaselineScore=baseline_score,
        Income=income,
        Coupon=coupon
    )
end

edu_data = education_program_data(2000)

# Method 1: Propensity Score
edu_data.PropensityScore, _ = estimate_propensity_score(edu_data, :Enroll, [:Age, :BaselineScore, :Income])
ate_ps, se_ps = ipw_estimator(edu_data, :Enroll, :TestScore, :PropensityScore)

# Method 2: IV (coupon as instrument)
ate_iv, se_iv, f_stat = two_stage_least_squares(edu_data, :TestScore, :Enroll, :Coupon, [:Age, :BaselineScore, :Income])

# Results
println("\n=== Education Program Causal Effect ===")
println("True effect: 10 points")
println("Propensity Score ATE: $(round(ate_ps, digits=2)) Â± $(round(1.96*se_ps, digits=2))")
println("IV (2SLS) LATE: $(round(ate_iv, digits=2)) Â± $(round(1.96*se_iv, digits=2))")
println("First-stage F: $(round(f_stat, digits=2))")

# Sensitivity
sensitivity_analysis_gamma(ate_ps, se_ps, [1.0, 1.5, 2.0])
```

> **Note:** **é€²æ—: 85% å®Œäº†** å®Ÿãƒ‡ãƒ¼ã‚¿å› æœæ¨è«–ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚’å®Œäº†ã—ãŸã€‚å…¨æ‰‹æ³•ã‚’æ¯”è¼ƒã—ã€æ„Ÿåº¦åˆ†æã§é ‘å¥æ€§ã‚’ç¢ºèªã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã§ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’æ¢ç´¢ã™ã‚‹ã€‚

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æœ€æ–°ç ”ç©¶å‹•å‘

### 6.1 å› æœæ¨è«–ãƒ•ã‚¡ãƒŸãƒªãƒ¼ãƒ„ãƒªãƒ¼

```mermaid
graph TD
    A["ğŸŒ³ å› æœæ¨è«–"] --> B["Potential Outcomes<br/>(Rubin)"]
    A --> C["Structural Causal Models<br/>(Pearl)"]
    A --> D["Quasi-Experimental<br/>(Campbell)"]

    B --> E["Unconfoundedness<br/>å‚¾å‘ã‚¹ã‚³ã‚¢"]
    B --> F["SUTVA<br/>ATE/ATT/CATE"]

    C --> G["DAG + do-æ¼”ç®—"]
    C --> H["ãƒãƒƒã‚¯ãƒ‰ã‚¢/ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢"]
    C --> I["åå®Ÿä»®æƒ³æ¨è«–"]

    D --> J["IV/2SLS"]
    D --> K["RDD"]
    D --> L["DiD"]

    A --> M["MLÃ—å› æœ<br/>(Athey/Imbens)"]
    M --> N["Causal Forest"]
    M --> O["Double ML"]
    M --> P["Meta-Learners"]

    style A fill:#c8e6c9
    style M fill:#fff3e0
```

### 6.2 æ¨è–¦è«–æ–‡ãƒ»æ•™ç§‘æ›¸

#### ä¸»è¦è«–æ–‡

| è«–æ–‡ | è‘—è€… | å¹´ | è²¢çŒ® |
|:-----|:-----|:---|:-----|
| Causality (2nd Ed) [^1] | Pearl | 2009 | SCM, do-æ¼”ç®—, ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº– |
| Causal Inference (free book) [^9] | HernÃ¡n & Robins | 2020 | å®Ÿè·µã‚¬ã‚¤ãƒ‰ |
| Potential Outcomes Survey [^2] | Rubin | 2005 | Rubinå› æœãƒ¢ãƒ‡ãƒ«çµ±åˆ |
| Causal Forest [^3] | Wager & Athey | 2018 | HTEæ¨å®š, æ¼¸è¿‘ç†è«– |
| Double ML [^4] | Chernozhukov et al. | 2018 | Debiased MLæ¨è«– |
| Staggered DiD [^5] | Callaway & Sant'Anna | 2021 | å¤šæœŸé–“DiD |
| Weak IV [^7] | Stock & Yogo | 2005 | å¼±æ“ä½œå¤‰æ•°æ¤œå®š |
| SRM Detection [^6] | Fabijan et al. | 2019 | A/Bãƒ†ã‚¹ãƒˆå“è³ªç®¡ç† |
| Simpson's Paradox [^8] | Pearl | 2014 | ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹è§£æ¶ˆ |

#### æ•™ç§‘æ›¸

- **å…¥é–€**: Pearl & Mackenzie "The Book of Why" (2018) â€” ä¸€èˆ¬å‘ã‘å› æœé©å‘½ã®æ­´å²
- **ç†è«–**: Pearl "Causality" (2009) [^1] â€” SCMã®è–å…¸
- **å®Ÿè·µ**: HernÃ¡n & Robins "Causal Inference" (2020) [^9] â€” ç„¡æ–™å…¬é–‹ã€ç–«å­¦ãƒ™ãƒ¼ã‚¹
- **è¨ˆé‡**: Angrist & Pischke "Mostly Harmless Econometrics" (2009) â€” IV/RDD/DiDã®å®Ÿè·µ
- **MLÃ—å› æœ**: Facure "Causal Inference for The Brave and True" (2022) â€” Pythonå®Ÿè£…ä»˜ã

### 6.3 ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

| ãƒªã‚½ãƒ¼ã‚¹ | URL | èª¬æ˜ |
|:--------|:----|:-----|
| **CausalInference.jl** | [github.com/mschauer/CausalInference.jl](https://github.com/mschauer/CausalInference.jl) [^10] | Juliaã®DAG/PC/FCIå®Ÿè£… |
| **Causal Inference Bootcamp** | [YouTube: Brady Neal](https://www.youtube.com/playlist?list=PLoazKTcS0RzZ1SUgeOgc6SWt51gfT80N0) | å‹•ç”»è¬›ç¾©ã‚·ãƒªãƒ¼ã‚º |
| **doWhy (Microsoft)** | [github.com/py-why/dowhy](https://github.com/py-why/dowhy) | Pythonå› æœæ¨è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
| **EconML (Microsoft)** | [github.com/py-why/EconML](https://github.com/py-why/EconML) | Python MLÃ—å› æœãƒ©ã‚¤ãƒ–ãƒ©ãƒª |

### 6.4 å› æœæ¨è«–ç”¨èªé›†

<details><summary>ç”¨èªé›†ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ï¼‰</summary>

| ç”¨èª | å®šç¾© |
|:-----|:-----|
| **ATE** | Average Treatment Effect â€” å…¨ä½“ã®å¹³å‡å‡¦ç½®åŠ¹æœ $\mathbb{E}[Y^1 - Y^0]$ |
| **ATT** | Average Treatment Effect on the Treated â€” å‡¦ç½®ç¾¤ã®å¹³å‡å‡¦ç½®åŠ¹æœ |
| **Backdoor Criterion** | ãƒãƒƒã‚¯ãƒ‰ã‚¢åŸºæº– â€” äº¤çµ¡ã‚’é™¤å»ã™ã‚‹ãŸã‚ã®å¤‰æ•°é›†åˆã®æ¡ä»¶ |
| **CATE** | Conditional Average Treatment Effect â€” æ¡ä»¶ä»˜ãå¹³å‡å‡¦ç½®åŠ¹æœ |
| **Collider** | ã‚³ãƒ©ã‚¤ãƒ€ãƒ¼ â€” 2ã¤ã®çŸ¢å°ãŒé›†ã¾ã‚‹å¤‰æ•° ($X \to Z \leftarrow Y$) |
| **DAG** | Directed Acyclic Graph â€” å› æœæ§‹é€ ã‚’è¡¨ã™æœ‰å‘éå·¡å›ã‚°ãƒ©ãƒ• |
| **DiD** | Difference-in-Differences â€” å·®åˆ†ã®å·®åˆ†æ³• |
| **d-separation** | dåˆ†é›¢ â€” DAGä¸Šã§ã®æ¡ä»¶ä»˜ãç‹¬ç«‹æ€§ |
| **do-Calculus** | do-æ¼”ç®— â€” ä»‹å…¥ç¢ºç‡ã‚’æ¡ä»¶ä»˜ãç¢ºç‡ã«å¤‰æ›ã™ã‚‹3ã¤ã®ãƒ«ãƒ¼ãƒ« |
| **Doubly Robust** | äºŒé‡é ‘å¥æ¨å®šé‡ â€” å‚¾å‘ã‚¹ã‚³ã‚¢ã¨çµæœãƒ¢ãƒ‡ãƒ«ã®ã©ã¡ã‚‰ã‹ãŒæ­£ã—ã‘ã‚Œã°ä¸å |
| **Fundamental Problem** | æ ¹æœ¬çš„å› æœæ¨è«–å•é¡Œ â€” $Y^1, Y^0$ ã‚’åŒæ™‚è¦³æ¸¬ã§ããªã„ |
| **IPW** | Inverse Probability Weighting â€” é€†ç¢ºç‡é‡ã¿ä»˜ã‘ |
| **IV** | Instrumental Variable â€” æ“ä½œå¤‰æ•° |
| **LATE** | Local Average Treatment Effect â€” å±€æ‰€å¹³å‡å‡¦ç½®åŠ¹æœï¼ˆã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ¼ã®åŠ¹æœï¼‰ |
| **Overlap** | å…±é€šã‚µãƒãƒ¼ãƒˆ â€” $0 < e(X) < 1$ ãŒã™ã¹ã¦ã® $X$ ã§æˆç«‹ |
| **Potential Outcomes** | æ½œåœ¨çš„çµæœ â€” $Y^1, Y^0$ |
| **Propensity Score** | å‚¾å‘ã‚¹ã‚³ã‚¢ â€” $e(X) = P(D=1 \mid X)$ |
| **RDD** | Regression Discontinuity Design â€” å›å¸°ä¸é€£ç¶šãƒ‡ã‚¶ã‚¤ãƒ³ |
| **SCM** | Structural Causal Model â€” æ§‹é€ å› æœãƒ¢ãƒ‡ãƒ« $(\mathcal{U}, \mathcal{V}, \mathcal{F})$ |
| **SUTVA** | Stable Unit Treatment Value Assumption â€” å®‰å®šå€‹ä½“å‡¦ç½®å€¤ä»®å®š |
| **Unconfoundedness** | ç„¡äº¤çµ¡æ€§ â€” $(Y^1, Y^0) \perp\!\!\!\perp D \mid X$ |

</details>

### 6.5 å› æœæ¨è«–ã®çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
mindmap
  root((å› æœæ¨è«–))
    Foundations
      Potential Outcomes
      DAG
      Identification
    Methods
      Propensity Score
        IPW
        Matching
        DR
      Quasi-Experimental
        IV
        RDD
        DiD
    ML Integration
      Causal Forest
      Double ML
      Meta-Learners
    Applications
      Policy Evaluation
      A/B Testing
      Observational Studies
    Tools
      CausalInference.jl
      doWhy
      EconML
```

> **Note:** **é€²æ—: 100% å®Œäº†** å› æœæ¨è«–ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’æ¢ç´¢ã—ãŸã€‚è«–æ–‡ãƒ»æ•™ç§‘æ›¸ãƒ»ãƒ„ãƒ¼ãƒ«ãƒ»ç”¨èªã‚’å®Œå…¨æ•´ç†ã€‚ã‚ã¨ã¯æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ã§ã¾ã¨ã‚ã€‚

---


### 6.6 æœ¬è¬›ç¾©ã®ã¾ã¨ã‚

1. **ç›¸é–¢ â‰  å› æœ**: Simpson's Paradox, äº¤çµ¡, é¸æŠãƒã‚¤ã‚¢ã‚¹ã®ç½ ã‚’ç†è§£
2. **Rubinå› æœãƒ¢ãƒ‡ãƒ«**: æ½œåœ¨çš„çµæœ $Y^1, Y^0$, SUTVA, ATE/ATT/CATE
3. **Pearlå› æœç†è«–**: DAG, do-æ¼”ç®—, ãƒãƒƒã‚¯ãƒ‰ã‚¢/ãƒ•ãƒ­ãƒ³ãƒˆãƒ‰ã‚¢åŸºæº–, d-åˆ†é›¢
4. **å‚¾å‘ã‚¹ã‚³ã‚¢**: IPW, Matching, Doubly Robust, ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
5. **æ“ä½œå¤‰æ•°æ³•**: 2SLS, LATE, Weak IVå•é¡Œ
6. **RDD**: Sharp/Fuzzy, å±€æ‰€ãƒ©ãƒ³ãƒ€ãƒ åŒ–, å¸¯åŸŸå¹…é¸æŠ
7. **DiD**: ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®š, Staggered DiD
8. **MLÃ—å› æœæ¨è«–**: Causal Forest, Double ML, Meta-Learners
9. **Juliaå®Ÿè£…**: CausalInference.jl ã§å…¨æ‰‹æ³•ã‚’å®Ÿè£…

### 6.7 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

<details><summary>Q1: å› æœæ¨è«–ã¨æ©Ÿæ¢°å­¦ç¿’ã®é•ã„ã¯ï¼Ÿ</summary>

**A**:
- **æ©Ÿæ¢°å­¦ç¿’**: äºˆæ¸¬ç²¾åº¦ã®æœ€å¤§åŒ– â€” $\hat{Y} \approx Y$
- **å› æœæ¨è«–**: å› æœåŠ¹æœã®æ¨å®š â€” $\mathbb{E}[Y \mid do(X=x)]$

MLã¯ã€Œæ¬¡ã«ä½•ãŒèµ·ã“ã‚‹ã‹ã€ã€å› æœæ¨è«–ã¯ã€Œä»‹å…¥ã—ãŸã‚‰ä½•ãŒèµ·ã“ã‚‹ã‹ã€ã‚’å•ã†ã€‚MLã¯ç›¸é–¢ã‚’å­¦ç¿’ã—ã€å› æœæ¨è«–ã¯å› æœæ§‹é€ ã‚’ä»®å®šã™ã‚‹ã€‚

</details>

<details><summary>Q2: ã„ã¤å‚¾å‘ã‚¹ã‚³ã‚¢ vs IVã‚’ä½¿ã†ï¼Ÿ</summary>

**A**:
- **å‚¾å‘ã‚¹ã‚³ã‚¢**: Unconfoundedness $(Y^d \perp\!\!\!\perp D \mid X)$ ãŒæˆç«‹ã™ã‚‹å ´åˆ â€” ã™ã¹ã¦ã®äº¤çµ¡å› å­ã‚’æ¸¬å®šã§ãã¦ã„ã‚‹
- **IV**: æœªæ¸¬å®šäº¤çµ¡ãŒã‚ã‚‹å ´åˆ â€” å¤–ç”Ÿçš„ãªãƒ©ãƒ³ãƒ€ãƒ å¤‰å‹•ï¼ˆæ“ä½œå¤‰æ•°ï¼‰ã‚’åˆ©ç”¨

ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ã«è¿‘ã„çŠ¶æ³ãªã‚‰å‚¾å‘ã‚¹ã‚³ã‚¢ã€è¦³æ¸¬ç ”ç©¶ã§äº¤çµ¡ãŒç–‘ã‚ã‚Œã‚‹ãªã‚‰IVã€‚

</details>

<details><summary>Q3: RDDã¨DiDã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ</summary>

**A**:
- **RDD**: å‡¦ç½®å‰²ã‚Šå½“ã¦ãŒã‚«ãƒƒãƒˆã‚ªãƒ•ã§æ±ºã¾ã‚‹ï¼ˆä¾‹: å¹´é½¢18æ­³ã§é¸æŒ™æ¨©ã€ã‚¹ã‚³ã‚¢70ç‚¹ã§åˆæ ¼ï¼‰
- **DiD**: 2æœŸé–“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã€å‡¦ç½®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãŒç¾¤ã«ã‚ˆã£ã¦ç•°ãªã‚‹

RDDã¯ç©ºé–“çš„ä¸é€£ç¶šã€DiDã¯æ™‚é–“çš„å¤‰åŒ–ã‚’åˆ©ç”¨ã™ã‚‹ã€‚

</details>

<details><summary>Q4: Causal Forestã§ä½•ãŒã‚ã‹ã‚‹ï¼Ÿ</summary>

**A**: **ç•°è³ªãªå‡¦ç½®åŠ¹æœ (HTE)** â€” å€‹ä½“ç‰¹æ€§ $X$ ã«å¿œã˜ãŸå‡¦ç½®åŠ¹æœ $\tau(X)$

å¹³å‡åŠ¹æœ(ATE)ã ã‘ã§ãªãã€ã€Œé«˜é½¢è€…ã«ã¯åŠ¹æœå¤§ã€è‹¥å¹´è€…ã«ã¯åŠ¹æœå°ã€ã¨ã„ã£ãŸéƒ¨åˆ†é›†å›£ã”ã¨ã®åŠ¹æœã‚’æ¨å®šã§ãã‚‹ã€‚æ”¿ç­–ã®ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ã«æœ‰ç”¨ã€‚

</details>

<details><summary>Q5: å› æœæ¨è«–ã§æœ€ã‚‚é‡è¦ãªä»®å®šã¯ï¼Ÿ</summary>

**A**: **Unconfoundedness** $(Y^d \perp\!\!\!\perp D \mid X)$ ã¾ãŸã¯ **Exclusion Restriction** (IV)

ã“ã‚ŒãŒç ´ã‚Œã‚‹ã¨ã€ã©ã‚“ãªæ‰‹æ³•ã‚‚å› æœåŠ¹æœã‚’æ­£ã—ãæ¨å®šã§ããªã„ã€‚ä»®å®šã®å¦¥å½“æ€§ã‚’ç†è«–ãƒ»ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãƒ»æ„Ÿåº¦åˆ†æã§æ¤œè¨¼ã™ã‚‹ã“ã¨ãŒæœ€é‡è¦ã€‚

</details>

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“å¾©ç¿’ãƒ—ãƒ©ãƒ³ï¼‰

| Day | å†…å®¹ | æ™‚é–“ | é”æˆåŸºæº– |
|:----|:-----|:-----|:---------|
| Day 1 | Zone 3.1-3.2 å†èª­ + Rubinç†è«–å¾©ç¿’ | 1h | ATE/ATT/CATE ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹ |
| Day 2 | Zone 3.3 å†èª­ + Pearlç†è«–å¾©ç¿’ | 1h | ãƒãƒƒã‚¯ãƒ‰ã‚¢èª¿æ•´å…¬å¼ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹ |
| Day 3 | Zone 3.4-3.5 å†èª­ + å‚¾å‘ã‚¹ã‚³ã‚¢/IVå¾©ç¿’ | 1h | IPWæ¨å®šé‡ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹ |
| Day 4 | Zone 4 Juliaå®Ÿè£…ã‚’å…¨ã¦å®Ÿè¡Œ | 2h | å…¨ã‚³ãƒ¼ãƒ‰ãŒã‚¨ãƒ©ãƒ¼ãªãå®Ÿè¡Œã§ãã‚‹ |
| Day 5 | Zone 5 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Ÿè£… | 2h | æ•™è‚²ä»‹å…¥ãƒ‡ãƒ¼ã‚¿ã§3æ‰‹æ³•æ¯”è¼ƒå®Œäº† |
| Day 6 | è«–æ–‡èª­è§£: Causal Forest [^3] or Double ML [^4] | 2h | æ‰‹æ³•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå®Œå…¨ã«ç†è§£ã§ãã‚‹ |
| Day 7 | è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§å› æœæ¨è«–å®Ÿè·µ | 3h | å®Ÿãƒ‡ãƒ¼ã‚¿ã§ATEæ¨å®š + æ„Ÿåº¦åˆ†æå®Œäº† |

### 6.9 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ç¬¬26å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª** ã§ã¯ã€å› æœæ¨è«–ã§å¾—ãŸåŠ¹æœã‚’**æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã«çµ„ã¿è¾¼ã‚€**:

- A/Bãƒ†ã‚¹ãƒˆåŸºç›¤æ§‹ç¯‰ (Elixir OTPã§ã®ä¸¦è¡Œãƒ†ã‚¹ãƒˆç®¡ç†)
- ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆæ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
- å› æœæ¨è«–Ã—å¼·åŒ–å­¦ç¿’ï¼ˆCounterfactual Policy Evaluationï¼‰
- Productionå“è³ª: é‡å­åŒ–ãƒ»è’¸ç•™ãƒ»Speculative Decoding

**ç¬¬27å›: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** ã§ã¯ã€å› æœåŠ¹æœã®çµ±è¨ˆçš„æ¤œå®š:

- FID/IS/LPIPS (ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡)
- Bootstrapã«ã‚ˆã‚‹CIæ¨å®š
- å¤šé‡æ¤œå®šè£œæ­£ (Bonferroni, FDR)
- å› æœåŠ¹æœã®å¯è¦–åŒ– (Forest Plot, Love Plot)

### 6.7 æ¬¡å›äºˆå‘Š: æ¨è«–æœ€é©åŒ– & Productionå“è³ª

ç¬¬26å›ã§ã¯ã€å› æœæ¨è«–ã§æ¸¬å®šã—ãŸåŠ¹æœã‚’**æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã§æ´»ã‹ã™**æŠ€è¡“ã‚’å­¦ã¶:

- **A/Bãƒ†ã‚¹ãƒˆåŸºç›¤**: Elixir OTPã§ã®ä¸¦è¡Œãƒ†ã‚¹ãƒˆç®¡ç†ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã€SRMæ¤œå‡º
- **ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆ**: æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€Thompson Sampling, UCB
- **å› æœæ¨è«–Ã—RL**: Counterfactual Policy Evaluation, Off-Policy Evaluation
- **æ¨è«–æœ€é©åŒ–**: é‡å­åŒ– (INT8/FP16), è’¸ç•™, Speculative Decoding, KVã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
- **Productionå“è³ª**: Rustæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³, Elixiråˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚°, ç›£è¦–ãƒ»ãƒ­ã‚®ãƒ³ã‚°ã€ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Multi-Armed Bandit / Contextual Bandit / Thompson Sampling / Speculative Decoding / GGUFé‡å­åŒ– / KV-Cache / OTP Supervision

**ç›®æ¨™**: å› æœæ¨è«–ã§å¾—ãŸçŸ¥è¦‹ã‚’ã€å®Ÿæˆ¦ã§ä½¿ãˆã‚‹é«˜é€Ÿãƒ»é ‘å¥ãªã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆã™ã‚‹ã€‚

---

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **A/Bãƒ†ã‚¹ãƒˆãªã—ã«"æ”¹å–„"ã‚’è¨¼æ˜ã§ãã‚‹ã‹ï¼Ÿ**

ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ï¼ˆA/Bãƒ†ã‚¹ãƒˆï¼‰ã¯å› æœæ¨è«–ã®ã‚´ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã ã€‚ã ãŒ:

- **å€«ç†çš„åˆ¶ç´„**: åŒ»ç™‚ã€æ•™è‚²ã€ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã§ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã¯å›°é›£
- **ã‚³ã‚¹ãƒˆ**: å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å®Ÿé¨“å°ã«ã§ããªã„
- **æ™‚é–“**: åŠ¹æœãŒå‡ºã‚‹ã¾ã§æ•°ãƒ¶æœˆã€œæ•°å¹´

**è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ­£ã—ãæ¨å®šã§ãã‚Œã°ã€A/Bãƒ†ã‚¹ãƒˆãªã—ã§ã‚‚æ”¹å–„ã‚’è¨¼æ˜ã§ãã‚‹ã€‚**

æœ¬è¬›ç¾©ã§å­¦ã‚“ã æ‰‹æ³•:

1. **å‚¾å‘ã‚¹ã‚³ã‚¢**: äº¤çµ¡ã‚’åˆ¶å¾¡ã—ã€è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ATEæ¨å®š
2. **æ“ä½œå¤‰æ•°**: æœªæ¸¬å®šäº¤çµ¡ãŒã‚ã£ã¦ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ãªå¤‰å‹•ã§å› æœåŠ¹æœæ¨å®š
3. **RDD**: ã‚«ãƒƒãƒˆã‚ªãƒ•ã®ä¸é€£ç¶šæ€§ã‚’åˆ©ç”¨ã—ã€å±€æ‰€çš„ãªå› æœåŠ¹æœæ¨å®š
4. **DiD**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ä¸¦è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®šã®ä¸‹ã§å› æœåŠ¹æœæ¨å®š
5. **Causal Forest**: ç•°è³ªãªå‡¦ç½®åŠ¹æœã‚’æ¨å®šã—ã€ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–

**ã ãŒã€ä»®å®šãŒç ´ã‚Œã‚Œã°å…¨ã¦ãŒå´©ã‚Œã‚‹ã€‚** å› æœæ¨è«–ã¯ã€Œä»®å®šã®æ˜ç¤ºåŒ–ã€ã¨ã€Œæ„Ÿåº¦åˆ†æã€ã«ã‚ˆã£ã¦ä»®å®šã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ã—ç¶šã‘ã‚‹å–¶ã¿ã ã€‚

**ã‚ãªãŸã®ç­”ãˆã¯ï¼Ÿ** â€” è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿å› æœæ¨è«–ã¨A/Bãƒ†ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã©ã†å–ã‚‹ã‹ï¼Ÿ

<details><summary>è­°è«–ã®ãƒã‚¤ãƒ³ãƒˆ</summary>

1. **è¦³æ¸¬ç ”ç©¶ã®å¼·ã¿**:
   - å€«ç†çš„åˆ¶ç´„ãŒãªã„ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ï¼‰
   - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å¤–éƒ¨å¦¥å½“æ€§ãŒé«˜ã„
   - é•·æœŸçš„åŠ¹æœã‚’è¿½è·¡ã§ãã‚‹

2. **è¦³æ¸¬ç ”ç©¶ã®å¼±ã¿**:
   - ä»®å®šä¾å­˜ï¼ˆUnconfoundedness, IVä»®å®šç­‰ï¼‰
   - æœªæ¸¬å®šäº¤çµ¡ã®ãƒªã‚¹ã‚¯
   - å› æœæ§‹é€ ã®èª¤ç‰¹å®š

3. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:
   - A/Bãƒ†ã‚¹ãƒˆã§çŸ­æœŸåŠ¹æœæ¤œè¨¼ + è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã§é•·æœŸåŠ¹æœæ¨å®š
   - A/Bãƒ†ã‚¹ãƒˆã§ãƒã‚¤ã‚¢ã‚¹è£œæ­£ + è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã§å¤–æŒ¿
   - å› æœæ¨è«–ã§äº‹å‰è©•ä¾¡ + A/Bãƒ†ã‚¹ãƒˆã§æœ€çµ‚ç¢ºèª

4. **æ­´å²çš„è¦–ç‚¹**:
   - Fisher (1935): ãƒ©ãƒ³ãƒ€ãƒ åŒ–å®Ÿé¨“ã®åŸå‰‡ç¢ºç«‹
   - Rubin (1974): è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å› æœæ¨è«–ç†è«–
   - Pearl (2000): ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã§å› æœæ§‹é€ ã‚’æ˜ç¤ºåŒ–
   - ç¾ä»£: MLÃ—å› æœæ¨è«–ã§å¤§è¦æ¨¡è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿æ´»ç”¨

**çµè«–**: A/Bãƒ†ã‚¹ãƒˆã¯ä¾ç„¶ã¨ã—ã¦ã‚´ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã ãŒã€**å› æœæ¨è«–ã¯è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€å¤§é™ã®æƒ…å ±ã‚’å¼•ãå‡ºã™å¼·åŠ›ãªæ­¦å™¨**ã€‚ä¸¡è€…ã‚’é©åˆ‡ã«çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ­£ç¢ºãªæ„æ€æ±ºå®šãŒå¯èƒ½ã«ãªã‚‹ã€‚

</details>

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼

### 6.6 æ·±å±¤å­¦ç¿’ã¨å› æœæ¨è«–ã®èåˆï¼ˆ2024-2026æœ€æ–°å‹•å‘ï¼‰

å¾“æ¥ã®å› æœæ¨è«–æ‰‹æ³•ã¯ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚„å˜ç´”ãªçµ±è¨ˆæ‰‹æ³•ã«ä¾å­˜ã—ã¦ã„ãŸãŒã€**æ·±å±¤å­¦ç¿’ã¨ã®çµ±åˆ**ã«ã‚ˆã‚Šã€é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ãƒ»éç·šå½¢é–¢ä¿‚ãƒ»æœªè¦³æ¸¬äº¤çµ¡ã¸ã®å¯¾å‡¦èƒ½åŠ›ãŒé£›èºçš„ã«å‘ä¸Šã—ã¦ã„ã‚‹ [^11]ã€‚

#### 6.6.1 Deep Causal Learningã®3æ¬¡å…ƒ

æœ€æ–°ã®ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ [^12] ã¯ã€æ·±å±¤å­¦ç¿’ãŒå› æœå­¦ç¿’ã«è²¢çŒ®ã™ã‚‹3ã¤ã®æ¬¡å…ƒã‚’æ•´ç†ã—ã¦ã„ã‚‹:

**1. Representationï¼ˆè¡¨ç¾å­¦ç¿’ï¼‰**:

é«˜æ¬¡å…ƒãƒ»éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»æ™‚ç³»åˆ—ï¼‰ã‹ã‚‰å› æœé–¢ä¿‚ã‚’å­¦ç¿’:

$$
\mathbf{z} = f_\theta(\mathbf{x}), \quad \mathbf{z} \in \mathbb{R}^d
$$

ã“ã“ã§ $f_\theta$ ã¯æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€$\mathbf{z}$ ã¯å› æœæ§‹é€ ã‚’æ‰ãˆãŸæ½œåœ¨è¡¨ç¾ã€‚

**2. Discoveryï¼ˆå› æœç™ºè¦‹ï¼‰**:

ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (GNN) ã§DAGã‚’å­¦ç¿’:

$$
\mathcal{G}^* = \arg\min_{\mathcal{G} \in \mathcal{DAG}} \mathcal{L}_{\text{score}}(\mathcal{G}; \mathbf{X}) + \lambda \|\mathcal{G}\|_0
$$

ã“ã“ã§ $\mathcal{L}_{\text{score}}$ ã¯ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ï¼ˆBIC, MDLç­‰ï¼‰ã€$\|\mathcal{G}\|_0$ ã¯ã‚¨ãƒƒã‚¸æ•°ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹æ€§æ­£å‰‡åŒ–ï¼‰ã€‚

**3. Inferenceï¼ˆå› æœæ¨è«–ï¼‰**:

æ·±å±¤å­¦ç¿’ã§å‡¦ç½®åŠ¹æœã‚’æ¨å®š:

$$
\tau(x) = \mathbb{E}[Y^1 - Y^0 \mid X = x] = f_\theta^1(x) - f_\theta^0(x)
$$

ã“ã“ã§ $f_\theta^1, f_\theta^0$ ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§å­¦ç¿’ã—ãŸæ½œåœ¨çš„çµæœé–¢æ•°ã€‚

#### 6.6.2 Deep Treatment Effect Estimation

**ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** [^13]:

| ãƒ¢ãƒ‡ãƒ« | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | ç‰¹å¾´ | è«–æ–‡ |
|:-------|:-------------|:-----|:-----|
| **TARNet** | å…±æœ‰å±¤ + åˆ†å²å±¤ | $f_{\text{shared}}(x) \to (f^1, f^0)$ | Shalit et al. 2017 |
| **CFRNet** | TARNet + IPMæ­£å‰‡åŒ– | $\min \text{IPM}(f(X \mid D=1), f(X \mid D=0))$ | Shalit et al. 2017 |
| **DragonNet** | å‚¾å‘ã‚¹ã‚³ã‚¢çµ±åˆ | $f_\theta(x) \to (e(x), \mu^1(x), \mu^0(x))$ | Shi et al. 2019 |
| **GANITE** | GAN | åå®Ÿä»®æƒ³ç”Ÿæˆ | Yoon et al. 2018 |
| **X-Learner** | ãƒ¡ã‚¿å­¦ç¿’ | 2æ®µéšæ¨å®š | KÃ¼nzel et al. 2019 |

**IPM (Integral Probability Metric)**:

åˆ†å¸ƒé–“ã®è·é›¢ã‚’æ¸¬å®šã—ã€å‡¦ç½®ç¾¤ãƒ»å¯¾ç…§ç¾¤ã®è¡¨ç¾ã‚’è¿‘ã¥ã‘ã‚‹:

$$
\text{IPM}(P, Q) = \sup_{f \in \mathcal{F}} \left| \mathbb{E}_{x \sim P}[f(x)] - \mathbb{E}_{x \sim Q}[f(x)] \right|
$$

CFRNetã¯ã€$\mathcal{F}$ ã‚’å†ç”Ÿæ ¸ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆç©ºé–“ (RKHS) ã¨ã—ã€Maximum Mean Discrepancy (MMD) ã‚’æœ€å°åŒ–:

$$
\mathcal{L}_{\text{CFR}} = \mathcal{L}_{\text{pred}} + \lambda \cdot \text{MMD}^2(f(X \mid D=1), f(X \mid D=0))
$$

**ADMIT (2024æœ€æ–°)** [^14]:

Average Dose Response Function (ADRF) ã®ä¸€èˆ¬åŒ–å¢ƒç•Œã‚’æä¾›:

$$
\text{ADRF}(d) = \mathbb{E}[Y \mid do(D = d)], \quad d \in [0, 1]
$$

é€£ç¶šå‡¦ç½®å¤‰æ•°ã«å¯¾ã—ã¦ã€IPMè·é›¢ã®é›¢æ•£è¿‘ä¼¼ã§ã‚»ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒã‚¤ã‚¢ã‚¹ã‚’ç·©å’Œã€‚

#### 6.6.3 Causal Discovery with Deep Learning

**NOTEARS** (Zheng et al., NeurIPS 2018):

DAGå­¦ç¿’ã‚’**é€£ç¶šæœ€é©åŒ–å•é¡Œ**ã«å¤‰æ›:

$$
\min_{\mathbf{W}} \quad \frac{1}{2n} \|\mathbf{X} - \mathbf{X}\mathbf{W}\|_F^2 + \lambda \|\mathbf{W}\|_1
$$

$$
\text{s.t.} \quad \text{tr}(e^{\mathbf{W} \odot \mathbf{W}}) - d = 0 \quad (\text{acyclicity constraint})
$$

ã“ã“ã§:

- $\mathbf{W} \in \mathbb{R}^{d \times d}$: é‡ã¿è¡Œåˆ—ï¼ˆæœ‰å‘ã‚°ãƒ©ãƒ•ã®éš£æ¥è¡Œåˆ—ï¼‰
- $\text{tr}(e^{\mathbf{W} \odot \mathbf{W}}) - d = 0$: DAGåˆ¶ç´„ï¼ˆéå·¡å›æ€§ï¼‰

å¾“æ¥ã®çµ„åˆã›æœ€é©åŒ– ($2^{d(d-1)/2}$ é€šã‚Š) ã‚’å›é¿ã—ã€å‹¾é…æ³•ã§è§£ã‘ã‚‹ã€‚

**GraN-DAG** (Lachapelle et al., ICML 2020):

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§éç·šå½¢å› æœé–¢ä¿‚ã‚’å­¦ç¿’:

$$
x_i = f_i(\text{PA}_i; \theta_i) + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma_i^2)
$$

ã“ã“ã§ $f_i$ ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã€$\text{PA}_i$ ã¯è¦ªãƒãƒ¼ãƒ‰ã€‚

**é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã¸ã®å¿œç”¨** [^15]:

- **ç”»åƒãƒ‡ãƒ¼ã‚¿**: CNNã§å› æœæ§‹é€ ã‚’å­¦ç¿’ï¼ˆä¾‹: ç—…ç†ç”»åƒ â†’ ç–¾æ‚£å› æœã‚°ãƒ©ãƒ•ï¼‰
- **æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿**: RNN/Transformerã§Grangerå› æœæ€§ã‚’å­¦ç¿’
- **ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: BERT/GPTã§è¨€èª¬é–“ã®å› æœé–¢ä¿‚ã‚’æ¨å®š

#### 6.6.4 å®Ÿè£…ä¾‹: TARNetã«ã‚ˆã‚‹ç•°è³ªå‡¦ç½®åŠ¹æœæ¨å®š

```julia
using Flux

# TARNet architecture
struct TARNet
    shared::Chain
    treated::Chain
    control::Chain
end

function TARNet(input_dim::Int, hidden_dim::Int, output_dim::Int=1)
    shared = Chain(
        Dense(input_dim => hidden_dim, relu),
        Dense(hidden_dim => hidden_dim, relu)
    )
    treated = Dense(hidden_dim => output_dim)
    control = Dense(hidden_dim => output_dim)
    return TARNet(shared, treated, control)
end

# Forward pass
function (m::TARNet)(x, d)
    h = m.shared(x)  # shared representation
    y1 = m.treated(h)
    y0 = m.control(h)
    # Return observed outcome
    return d .* y1 .+ (1 .- d) .* y0
end

# Training
function train_tarnet!(model, X, D, Y, n_epochs=100, lr=0.001)
    opt = Flux.Adam(lr)
    params = Flux.params(model.shared, model.treated, model.control)

    for epoch in 1:n_epochs
        loss = Flux.mse(model(X, D), Y)
        grads = Flux.gradient(() -> loss, params)
        Flux.update!(opt, params, grads)

        if epoch % 20 == 0
            println("Epoch $epoch: Loss = $(round(loss, digits=4))")
        end
    end
end

# CATE estimation
function estimate_cate(model, x)
    h = model.shared(x)
    return model.treated(h) .- model.control(h)
end
```

**æ•°å¼ã¨ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œ**:

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ |
|:-----|:-------|
| $\phi(x) = f_{\text{shared}}(x)$ | `h = m.shared(x)` |
| $\mu^1(x) = f_1(\phi(x))$ | `y1 = m.treated(h)` |
| $\mu^0(x) = f_0(\phi(x))$ | `y0 = m.control(h)` |
| $\tau(x) = \mu^1(x) - \mu^0(x)$ | `estimate_cate(model, x)` |

ã“ã®å®Ÿè£…ã«ã‚ˆã‚Šã€å€‹äººãƒ¬ãƒ™ãƒ«ã®å‡¦ç½®åŠ¹æœï¼ˆCATEï¼‰ã‚’æ¨å®šã§ãã‚‹ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼æœ€æ–°ã®æ·±å±¤å­¦ç¿’Ã—å› æœæ¨è«–æ‰‹æ³•ã¾ã§ç¶²ç¾…ã—ãŸã€‚

---


> Progress: [95%]
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. E-valueãŒã€Œè¦³å¯Ÿç ”ç©¶ã«ãŠã‘ã‚‹å› æœä¸»å¼µã®é ‘å¥æ€§ã€ã‚’å®šé‡åŒ–ã§ãã‚‹ç†ç”±ã‚’è¿°ã¹ã‚ˆã€‚
> 2. Causal ForestãŒS/T-Learnerã‚ˆã‚Šç•°è³ªå‡¦ç½®åŠ¹æœï¼ˆHTEï¼‰æ¨å®šã«å„ªã‚Œã‚‹ç†ç”±ã¯ï¼Ÿ

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Pearl, J. (2009). *Causality: Models, Reasoning, and Inference* (2nd ed.). Cambridge University Press.
<https://bayes.cs.ucla.edu/BOOK-2K/>

[^2]: Rubin, D. B. (2005). Causal Inference Using Potential Outcomes: Design, Modeling, Decisions. *Journal of the American Statistical Association*, 100(469), 322-331.
<https://www.tandfonline.com/doi/abs/10.1198/016214504000001880>

[^3]: Wager, S., & Athey, S. (2018). Estimation and Inference of Heterogeneous Treatment Effects using Random Forests. *Journal of the American Statistical Association*, 113(523), 1228-1242.
<https://arxiv.org/abs/1510.04342>

[^4]: Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. *The Econometrics Journal*, 21(1), C1-C68.
<https://arxiv.org/abs/1608.00060>

[^5]: Callaway, B., & Sant'Anna, P. H. (2021). Difference-in-Differences with multiple time periods. *Journal of Econometrics*, 225(2), 200-230.
<https://www.sciencedirect.com/science/article/abs/pii/S0304407620303948>

[^6]: Fabijan, A., Gupchup, J., Gupta, S., Omhover, J., Qin, W., Vermeer, L., & Dmitriev, P. (2019). Diagnosing Sample Ratio Mismatch in Online Controlled Experiments: A Taxonomy and Rules of Thumb for Practitioners. *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2156-2164.
<https://dl.acm.org/doi/10.1145/3292500.3330722>

[^7]: Stock, J. H., & Yogo, M. (2005). Testing for Weak Instruments in Linear IV Regression. In *Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg* (pp. 80-108). Cambridge University Press.
<https://www.cambridge.org/core/books/abs/identification-and-inference-for-econometric-models/testing-for-weak-instruments-in-linear-iv-regression/8AD94FF2EFD214D05D75EE35015021E4>

[^8]: Pearl, J. (2014). Understanding Simpson's Paradox. *The American Statistician*, 68(1), 8-13.
<https://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf>

[^9]: HernÃ¡n, M. A., & Robins, J. M. (2020). *Causal Inference: What If*. Chapman & Hall/CRC. (Free online)
<https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/>

[^10]: Mschauer. (2021). CausalInference.jl: Causal inference, graphical models and structure learning in Julia.
<https://github.com/mschauer/CausalInference.jl>

[^11]: Wang, Y., et al. (2024). "Causal Inference Meets Deep Learning: A Comprehensive Survey". *Research*, 7, 0467.
<https://arxiv.org/abs/2303.02186>

[^12]: Guo, R., et al. (2024). "Deep Causal Learning: Representation, Discovery and Inference". *ACM Computing Surveys*, 56(9), 1-40.
<https://arxiv.org/abs/2211.03374>


[^24]: Ling, Z., et al. (2025). "Hybrid Local Causal Discovery". *arXiv preprint*.
<https://arxiv.org/abs/2412.19507>

[^25]: Zhou, J., & Wang, M. (2025). "Differentiable Constraint-Based Causal Discovery". *arXiv preprint*.
<https://arxiv.org/abs/2510.22031>

[^26]: Mokhtarian, E., et al. (2024). "Recursive Causal Discovery". *arXiv preprint*.
<https://arxiv.org/abs/2403.09300>

[^27]: Gerhardus, A., & Runge, J. (2023). "Causal Discovery from Time Series with Hybrids of Constraint-Based and Noise-Based Algorithms". *arXiv preprint*.
<https://arxiv.org/abs/2306.08765>

## è‘—è€…ãƒªãƒ³ã‚¯
- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

**æ¬¡å›**: [ç¬¬26å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª](/your-next-article)
