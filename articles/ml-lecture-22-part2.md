---
title: "ç¬¬22å›: ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
slug: "ml-lecture-22-part2"
emoji: "ğŸ‘ï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "multimodal", "rust", "rust"]
published: true
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> ğŸ“Œ **å‰ç·¨ï¼ˆç†è«–ï¼‰**: [ç¬¬22å› å‰ç·¨](./ml-lecture-22-part1)

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rust CLIP + Rust SmolVLM2

ç†è«–ã‚’ç†è§£ã—ãŸã ã‘ã§ã¯ä¸ååˆ†ã ã€‚å®Ÿè£…ã—ã¦ã“ãã€**çœŸã®ç†è§£**ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

ã“ã®Zoneã§ã¯ã€3ã¤ã®å®Ÿè£…ã‚’å®Œèµ°ã™ã‚‹:
1. **ğŸ¦€Rust CLIPå®Ÿè£…** â€” Dual Encoderè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
2. **ğŸ¦€Rust ViTå®Ÿè£…** â€” Vision Transformerã®å®Œå…¨å®Ÿè£…
3. **ğŸ¦€Rust SmolVLM2æ¨è«–** â€” GGUF/Candleçµ±åˆã§ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–

### 4.1 ğŸ¦€Rust CLIPå®Ÿè£…

#### 4.1.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ

CLIPã¯**Dual Encoder**æ§‹é€ ã ã€‚ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç‹¬ç«‹ã«å‡¦ç†ã—ã€æœ€å¾Œã«é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ã€‚

```mermaid
graph TD
    Img[ç”»åƒãƒãƒƒãƒ<br>BÃ—HÃ—WÃ—C] --> VisionEnc[Vision Encoder<br>ViT-B/32]
    Text[ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒ<br>BÃ—L] --> TextEnc[Text Encoder<br>Transformer]
    VisionEnc --> VEmb[ç”»åƒåŸ‹ã‚è¾¼ã¿<br>BÃ—d]
    TextEnc --> TEmb[ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿<br>BÃ—d]
    VEmb --> InfoNCE[InfoNCE Loss]
    TEmb --> InfoNCE
    InfoNCE --> Grad[å‹¾é…è¨ˆç®—]
    Grad --> Update[ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°]
```

#### 4.1.2 Vision Encoderã®å®Ÿè£…

```rust
use candle_core::{Result, Tensor};
use candle_nn::{self as nn, LayerNorm, Linear, Module, VarBuilder};

// Vision Transformer for CLIP
pub struct VisionTransformer {
    patch_embed: PatchEmbed,
    pos_embed: Tensor,
    cls_token: Tensor,
    transformer_blocks: Vec<TransformerBlock>,
    norm: LayerNorm,
    proj: Linear,  // åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã¸ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
}

impl VisionTransformer {
    pub fn new(
        img_size: usize,    // = 224
        patch_size: usize,  // = 32
        in_channels: usize, // = 3
        embed_dim: usize,   // = 768
        depth: usize,       // = 12
        num_heads: usize,   // = 12
        mlp_ratio: usize,   // = 4
        out_dim: usize,     // = 512
        vb: VarBuilder,
    ) -> Result<Self> {
        let num_patches = (img_size / patch_size).pow(2);

        // Patch Embedding
        let patch_embed = PatchEmbed::new(img_size, patch_size, embed_dim, in_channels, vb.pp("patch_embed"))?;

        // Positional Encoding + CLS token
        let pos_embed = vb.get_with_hints(
            (1, num_patches + 1, embed_dim), "pos_embed",
            nn::init::Init::Randn { mean: 0.0, stdev: 0.02 },
        )?;
        let cls_token = vb.get_with_hints(
            (1, 1, embed_dim), "cls_token",
            nn::init::Init::Randn { mean: 0.0, stdev: 0.02 },
        )?;

        // Transformer Blocks
        let transformer_blocks = (0..depth)
            .map(|i| TransformerBlock::new(embed_dim, num_heads, mlp_ratio, vb.pp(format!("blocks.{i}"))))
            .collect::<Result<Vec<_>>>()?;

        // Layer Norm + Projection
        let norm = nn::layer_norm(embed_dim, 1e-6, vb.pp("norm"))?;
        let proj = nn::linear(embed_dim, out_dim, vb.pp("proj"))?;

        Ok(Self { patch_embed, pos_embed, cls_token, transformer_blocks, norm, proj })
    }
}

impl Module for VisionTransformer {
    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, C, H, W)
        let b = x.dim(0)?;

        // Patch Embedding: (B, C, H, W) â†’ (B, N, embed_dim)
        let patches = self.patch_embed.forward(x)?;

        // CLS tokenã‚’å„ãƒãƒƒãƒã«è¿½åŠ ã—ã€Positional Encodingã‚’ä¸€æ‹¬åŠ ç®—
        let cls_tokens = self.cls_token.expand((b, 1, self.cls_token.dim(2)?))?;
        let tokens = Tensor::cat(&[&cls_tokens, &patches], 1)?;  // (B, N+1, embed_dim)
        let mut tokens = tokens.broadcast_add(&self.pos_embed)?;

        // Transformer Blocks
        for block in &self.transformer_blocks {
            tokens = block.forward(&tokens)?;
        }

        // CLS tokenã®å‡ºåŠ›ã‚’å–å¾— â†’ Layer Norm â†’ Projection
        let cls_out = tokens.i((.., 0, ..))?;  // (B, embed_dim)
        self.proj.forward(&self.norm.forward(&cls_out)?)  // (B, out_dim)
    }
}

// Transformer Block
pub struct TransformerBlock {
    attn: MultiHeadSelfAttention,
    mlp: nn::Sequential,
    norm1: LayerNorm,
    norm2: LayerNorm,
}

impl TransformerBlock {
    pub fn new(embed_dim: usize, num_heads: usize, mlp_ratio: usize, vb: VarBuilder) -> Result<Self> {
        let attn = MultiHeadSelfAttention::new(embed_dim, num_heads, vb.pp("attn"))?;
        let mlp = nn::seq()
            .add(nn::linear(embed_dim, embed_dim * mlp_ratio, vb.pp("mlp.fc1"))?)
            .add(nn::Activation::Gelu)
            .add(nn::linear(embed_dim * mlp_ratio, embed_dim, vb.pp("mlp.fc2"))?);
        let norm1 = nn::layer_norm(embed_dim, 1e-6, vb.pp("norm1"))?;
        let norm2 = nn::layer_norm(embed_dim, 1e-6, vb.pp("norm2"))?;
        Ok(Self { attn, mlp, norm1, norm2 })
    }
}

impl Module for TransformerBlock {
    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // Pre-Norm: Norm â†’ Attention â†’ Residual
        let x = (x + self.attn.forward(&self.norm1.forward(x)?)?)?;
        // Pre-Norm: Norm â†’ MLP â†’ Residual
        let x = (&x + self.mlp.forward(&self.norm2.forward(&x)?)?)?;
        Ok(x)
    }
}
```

#### 4.1.3 Text Encoderã®å®Ÿè£…

```rust
// Text Transformer for CLIP
pub struct TextTransformer {
    token_embed: nn::Embedding,
    pos_embed: Tensor,
    transformer_blocks: Vec<TransformerBlock>,
    norm: LayerNorm,
    proj: Linear,
}

impl TextTransformer {
    pub fn new(
        vocab_size: usize,  // = 49408 â€” CLIPã®vocabã‚µã‚¤ã‚º
        max_len: usize,     // = 77
        embed_dim: usize,   // = 512
        depth: usize,       // = 12
        num_heads: usize,   // = 8
        mlp_ratio: usize,   // = 4
        out_dim: usize,     // = 512
        vb: VarBuilder,
    ) -> Result<Self> {
        let token_embed = nn::embedding(vocab_size, embed_dim, vb.pp("token_embed"))?;
        let pos_embed = vb.get_with_hints(
            (1, max_len, embed_dim), "pos_embed",
            nn::init::Init::Randn { mean: 0.0, stdev: 0.02 },
        )?;
        let transformer_blocks = (0..depth)
            .map(|i| TransformerBlock::new(embed_dim, num_heads, mlp_ratio, vb.pp(format!("blocks.{i}"))))
            .collect::<Result<Vec<_>>>()?;
        let norm = nn::layer_norm(embed_dim, 1e-6, vb.pp("norm"))?;
        let proj = nn::linear(embed_dim, out_dim, vb.pp("proj"))?;
        Ok(Self { token_embed, pos_embed, transformer_blocks, norm, proj })
    }

    pub fn forward(&self, tokens: &Tensor) -> Result<Tensor> {
        // tokens: (B, L) â€” ãƒˆãƒ¼ã‚¯ãƒ³IDåˆ—
        let (_b, l) = tokens.dims2()?;

        // Token Embedding + Positional Encodingï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã‚¹ãƒ©ã‚¤ã‚¹ï¼‰
        let mut x = (self.token_embed.forward(tokens)?
            + self.pos_embed.i((.., ..l, ..))?)?;  // (B, L, embed_dim)

        // Transformer Blocks
        for block in &self.transformer_blocks {
            x = block.forward(&x)?;
        }

        // EOT (End of Text) tokenã®å‡ºåŠ›ã‚’å–å¾— â†’ Layer Norm â†’ Projection
        // ä»®å®š: EOT tokenã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¾Œ
        let eot_out = x.i((.., l - 1, ..))?;  // (B, embed_dim)
        self.proj.forward(&self.norm.forward(&eot_out)?)  // (B, out_dim)
    }
}
```

#### 4.1.4 CLIPãƒ¢ãƒ‡ãƒ«å…¨ä½“

```rust
// CLIP: Vision + Text Dual Encoder
pub struct Clip {
    pub vision: VisionTransformer,
    pub text: TextTransformer,
    pub temperature: Tensor,  // æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
}

impl Clip {
    pub fn new(vb: VarBuilder) -> Result<Self> {
        let vision = VisionTransformer::new(
            224, 32, 3, 768, 12, 12, 4, 512, vb.pp("vision"),
        )?;
        let text = TextTransformer::new(
            49408, 77, 512, 12, 8, 4, 512, vb.pp("text"),
        )?;
        // åˆæœŸæ¸©åº¦
        let temperature = vb.get_with_hints(1, "temperature", nn::init::Init::Const(0.07))?;
        Ok(Self { vision, text, temperature })
    }

    pub fn forward(&self, images: &Tensor, tokens: &Tensor) -> Result<(Tensor, Tensor, Tensor)> {
        // ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
        let v_embeds = self.vision.forward(images)?;  // (B, out_dim)
        let t_embeds = self.text.forward(tokens)?;    // (B, out_dim)

        // InfoNCE loss
        let tau = self.temperature.to_scalar::<f64>()?;
        let loss = info_nce(&v_embeds, &t_embeds, tau)?;

        Ok((loss, v_embeds, t_embeds))
    }
}

// InfoNCE loss â€” zero-copy computation
fn info_nce(img_emb: &Tensor, txt_emb: &Tensor, temp: f64) -> Result<Tensor> {
    let logits = img_emb.matmul(&txt_emb.t()?)?.affine(1.0 / temp, 0.0)?;
    let n = logits.dim(0)?;
    let labels = Tensor::arange(0u32, n as u32, &Device::Cpu)?;
    candle_nn::loss::cross_entropy(&logits, &labels)
}
```

#### 4.1.5 è¨“ç·´ãƒ«ãƒ¼ãƒ—

```rust
use candle_nn::{AdamW, Optimizer, ParamsAdamW};

fn train_clip(
    clip: &mut Clip,
    train_loader: &[(Tensor, Tensor)],  // (images, tokens)
    epochs: usize,                      // = 10
    lr: f64,                            // = 1e-4
) -> Result<()> {
    // ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    let varmap = nn::VarMap::new();
    let mut opt = AdamW::new(varmap.all_vars(), ParamsAdamW { lr, ..Default::default() })?;

    for epoch in 0..epochs {
        let mut total_loss = 0.0_f64;
        for (images, tokens) in train_loader {
            // å‹¾é…è¨ˆç®— & æ›´æ–°
            let (loss, _v, _t) = clip.forward(images, tokens)?;
            opt.backward_step(&loss)?;
            total_loss += loss.to_scalar::<f64>()?;
        }

        let avg_loss = total_loss / train_loader.len() as f64;
        println!("Epoch {}: Loss = {:.4}", epoch, avg_loss);
    }
    Ok(())
}
```

#### 4.1.6 Zero-shotæ¨è«–

```rust
fn zero_shot_classify(
    clip: &Clip,
    image: &Tensor,           // (C, H, W)
    text_candidates: &[&str],
    tokenize: &impl Fn(&str) -> Result<Tensor>,
) -> Result<(Tensor, usize)> {
    // ç”»åƒåŸ‹ã‚è¾¼ã¿ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã‚¹ãƒ©ã‚¤ã‚¹ï¼‰
    let v_embed = clip.vision.forward(&image.unsqueeze(0)?)?;  // (1, out_dim)

    // ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆå„å€™è£œï¼‰
    let t_embeds: Vec<Tensor> = text_candidates
        .iter()
        .map(|t| clip.text.forward(&tokenize(t)?))
        .collect::<Result<_>>()?;
    let t_embeds = Tensor::cat(&t_embeds, 0)?;  // (N, out_dim)

    // é¡ä¼¼åº¦è¨ˆç®—ï¼ˆæ­£è¦åŒ–ãƒ™ã‚¯ãƒˆãƒ«ã®dotç©ã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰
    let v_norm = l2_normalize(&v_embed)?;
    let t_norm = l2_normalize(&t_embeds)?;
    let tau = clip.temperature.to_scalar::<f64>()?;
    let similarities = v_norm.matmul(&t_norm.t()?)?.affine(1.0 / tau, 0.0)?;

    // Softmaxç¢ºç‡
    let probs = candle_nn::ops::softmax(&similarities.squeeze(0)?, 0)?;
    let best = probs.argmax(0)?.to_scalar::<u32>()? as usize;

    Ok((probs, best))
}
```

#### 4.1.7 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å®Œå…¨å¯¾å¿œè¡¨

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ |
|:-----|:-------|
| $\mathbf{v} = f_v(\mathbf{x}^v)$ | `v_embeds = clip.vision(images)` |
| $\mathbf{t} = f_t(\mathbf{x}^t)$ | `t_embeds = clip.text(tokens)` |
| $s_{ij} = \frac{\mathbf{v}_i \cdot \mathbf{t}_j}{\|\mathbf{v}_i\| \|\mathbf{t}_j\|}$ | `S = v_embeds' * t_embeds` (æ­£è¦åŒ–å¾Œ) |
| $\mathcal{L}_i^{v \to t} = -\log \frac{\exp(s_{ii}/\tau)}{\sum_j \exp(s_{ij}/\tau)}$ | `logitcrossentropy(S ./ Ï„, labels)` |
| $\mathbf{Z}_p = W_{\text{proj}} \cdot \text{vec}(\mathbf{x}_p)$ | `pe.proj(patches)` |
| $\mathbf{A} = \text{softmax}(\frac{\mathbf{Q}^\top \mathbf{K}}{\sqrt{d_k}})$ | `softmax(Q' * K ./ sqrt(d_k), dims=2)` |

---

### 4.2 ğŸ¦€Rust ViTå®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰

Zone 3.2ã§ViTã®ç†è«–ã‚’å­¦ã‚“ã ã€‚ã“ã“ã§ã¯ã€**è¨“ç·´å¯èƒ½ãªViT**ã‚’å®Œå…¨å®Ÿè£…ã™ã‚‹ã€‚

#### 4.2.1 Multi-Head Self-Attentionã®å®Ÿè£…

```rust
// Multi-Head Self-Attention
pub struct MultiHeadSelfAttention {
    num_heads: usize,
    head_dim: usize,
    qkv: Linear,   // Query, Key, Valueã‚’ä¸€åº¦ã«è¨ˆç®—
    proj: Linear,
}

impl MultiHeadSelfAttention {
    pub fn new(embed_dim: usize, num_heads: usize, vb: VarBuilder) -> Result<Self> {
        assert_eq!(embed_dim % num_heads, 0);
        let head_dim = embed_dim / num_heads;
        let qkv = nn::linear(embed_dim, 3 * embed_dim, vb.pp("qkv"))?;  // Q, K, V
        let proj = nn::linear(embed_dim, embed_dim, vb.pp("proj"))?;
        Ok(Self { num_heads, head_dim, qkv, proj })
    }
}

impl Module for MultiHeadSelfAttention {
    fn forward(&self, x: &Tensor) -> Result<Tensor> {
        // x: (B, N, embed_dim)
        let (b, n, d) = x.dims3()?;
        let h = self.num_heads;
        let d_h = self.head_dim;

        // Q, K, Vè¨ˆç®—
        let qkv = self.qkv.forward(x)?;  // (B, N, 3*embed_dim)
        let q = qkv.i((.., .., ..d))?.reshape((b, n, h, d_h))?.transpose(1, 2)?;        // (B, h, N, d_h)
        let k = qkv.i((.., .., d..2 * d))?.reshape((b, n, h, d_h))?.transpose(1, 2)?;  // (B, h, N, d_h)
        let v = qkv.i((.., .., 2 * d..))?.reshape((b, n, h, d_h))?.transpose(1, 2)?;   // (B, h, N, d_h)

        // Multi-headå½¢çŠ¶ã«å¤‰æ›: (B, h, N, d_h)
        // Attentionè¨ˆç®—ï¼ˆå„ãƒ˜ãƒƒãƒ‰ç‹¬ç«‹ï¼‰
        // scores: (B, h, N, N)
        let scale = (d_h as f64).sqrt();
        let scores = q.matmul(&k.transpose(2, 3)?)?.affine(1.0 / scale, 0.0)?;
        let attn = candle_nn::ops::softmax(&scores, 3)?;

        // Attentioné©ç”¨: (B, h, N, d_h)
        let out = attn.matmul(&v)?;

        // Multi-headã‚’çµåˆ: (B, h, N, d_h) â†’ (B, N, embed_dim)
        let out = out.transpose(1, 2)?.reshape((b, n, d))?;

        // å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
        self.proj.forward(&out)
    }
}
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:

$$
\mathbf{Q} = W_Q \mathbf{X}, \quad \mathbf{K} = W_K \mathbf{X}, \quad \mathbf{V} = W_V \mathbf{X} \quad \Leftrightarrow \quad \texttt{q, k, v = chunk(qkv, 3)}
$$

$$
\mathbf{A} = \text{softmax}\left(\frac{\mathbf{Q}^\top \mathbf{K}}{\sqrt{d_h}}\right) \quad \Leftrightarrow \quad \texttt{attn = softmax(scores ./ sqrt(d\_h))}
$$

#### 4.2.2 ViTè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```rust
use candle_core::{DType, Device, Result, Tensor};
use candle_nn::{AdamW, Optimizer, ParamsAdamW};

// ImageNetãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰
fn imagenet_loader(batch_size: usize) -> Vec<(Tensor, Tensor)> {
    // å®Ÿéš›ã¯ImageNet-1kã‚’ä½¿ç”¨
    // ã“ã“ã§ã¯æ“¬ä¼¼ãƒ‡ãƒ¼ã‚¿
    let dev = Device::Cpu;
    (0..1000 / batch_size)
        .map(|_| {
            let images = Tensor::randn(0f32, 1.0, (batch_size, 3, 224, 224), &dev).unwrap();
            let labels = Tensor::zeros((batch_size,), DType::U32, &dev).unwrap();
            (images, labels)
        })
        .collect()
}

// ViTè¨“ç·´
fn train_vit(
    vit: &mut VisionTransformer,
    train_loader: &[(Tensor, Tensor)],
    test_loader: &[(Tensor, Tensor)],
    epochs: usize,  // = 30
    lr: f64,        // = 3e-4
) -> Result<()> {
    let varmap = nn::VarMap::new();
    let mut opt = AdamW::new(varmap.all_vars(), ParamsAdamW { lr, ..Default::default() })?;

    for epoch in 0..epochs {
        for (images, labels) in train_loader {
            let logits = vit.forward(images)?;  // (B, num_classes)
            let loss = candle_nn::loss::cross_entropy(&logits, labels)?;
            opt.backward_step(&loss)?;
        }

        // è©•ä¾¡
        let acc = evaluate_vit(vit, test_loader)?;
        println!("Epoch {}: Accuracy = {:.4}", epoch, acc);
    }
    Ok(())
}

fn evaluate_vit(vit: &VisionTransformer, test_loader: &[(Tensor, Tensor)]) -> Result<f64> {
    let mut correct = 0usize;
    let mut total = 0usize;
    for (images, labels) in test_loader {
        let logits = vit.forward(images)?;
        let preds = logits.argmax(1)?;
        let matches = preds.eq(labels)?.to_vec1::<u8>()?;
        correct += matches.iter().filter(|&&x| x != 0).count();
        total += labels.dim(0)?;
    }
    Ok(correct as f64 / total as f64)
}
```

---

### 4.3 ğŸ¦€Rust SmolVLM2æ¨è«–

Rustã§CLIPã‚’è¨“ç·´ã—ãŸã€‚æ¬¡ã¯ã€**Rustã§æ¨è«–**ã‚’å®Ÿè£…ã™ã‚‹ã€‚SmolVLM2-256Mã¯ã€Rustã®`candle`ã‚¯ãƒ¬ãƒ¼ãƒˆã§æ¨è«–ã§ãã‚‹ã€‚

#### 4.3.1 Rustãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
cargo new smolvlm2_inference
cd smolvlm2_inference
```

**Cargo.toml**:

```toml
[package]
name = "smolvlm2_inference"
version = "0.1.0"
edition = "2021"

[dependencies]
candle-core = "0.4"
candle-nn = "0.4"
candle-transformers = "0.4"
tokenizers = "0.15"
image = "0.25"
anyhow = "1.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
```

#### 4.3.2 ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›å‡¦ç†

```rust
use candle_core::{Device, Tensor};
use candle_transformers::models::smolvlm::{Config, Model};
use image::{DynamicImage, GenericImageView};
use tokenizers::Tokenizer;
use anyhow::Result;

/// ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›: ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆ
pub struct MultimodalInput {
    pub image: DynamicImage,
    pub text: String,
}

/// ç”»åƒã‚’å‰å‡¦ç†ã—ã¦ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
pub fn preprocess_image(image: &DynamicImage, device: &Device) -> Result<Tensor> {
    let (width, height) = image.dimensions();
    let img = image.resize_exact(224, 224, image::imageops::FilterType::Triangle);
    let img_rgb = img.to_rgb8();

    // (H, W, C) â†’ (C, H, W) â†’ æ­£è¦åŒ–
    let data: Vec<f32> = img_rgb
        .pixels()
        .flat_map(|p| {
            let r = (p[0] as f32 / 255.0 - 0.485) / 0.229;
            let g = (p[1] as f32 / 255.0 - 0.456) / 0.224;
            let b = (p[2] as f32 / 255.0 - 0.406) / 0.225;
            [r, g, b]
        })
        .collect::<Vec<_>>();

    let tensor = Tensor::from_vec(data, (3, 224, 224), device)?;
    Ok(tensor.unsqueeze(0)?) // (1, 3, 224, 224)
}

/// ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
pub fn tokenize_text(tokenizer: &Tokenizer, text: &str) -> Result<Tensor> {
    let encoding = tokenizer.encode(text, true)?;
    let ids = encoding.get_ids();
    let tensor = Tensor::new(ids, &Device::Cpu)?;
    Ok(tensor.unsqueeze(0)?) // (1, L)
}
```

#### 4.3.3 SmolVLM2ãƒ¢ãƒ‡ãƒ«æ¨è«–

```rust
/// SmolVLM2æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
pub struct SmolVLM2Inference {
    model: Model,
    tokenizer: Tokenizer,
    device: Device,
}

impl SmolVLM2Inference {
    /// ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    pub fn load(model_path: &str, tokenizer_path: &str) -> Result<Self> {
        let device = Device::cuda_if_available(0)?;
        let config = Config::smolvlm2_256m(); // 256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        let vb = candle_nn::VarBuilder::from_pth(model_path, candle_core::DType::F32, &device)?;
        let model = Model::new(&config, vb)?;
        let tokenizer = Tokenizer::from_file(tokenizer_path)?;

        Ok(Self { model, tokenizer, device })
    }

    /// ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–
    pub fn infer(&self, input: &MultimodalInput) -> Result<String> {
        // ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
        let image_tensor = preprocess_image(&input.image, &self.device)?;
        let text_tensor = tokenize_text(&self.tokenizer, &input.text)?;

        // ãƒ¢ãƒ‡ãƒ«æ¨è«–
        let output = self.model.forward(&image_tensor, &text_tensor)?;

        // ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆargmax â†’ ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        let logits = output.squeeze(0)?; // (vocab_size,)
        let token_id = logits.argmax(0)?.to_scalar::<u32>()?;
        let decoded = self.tokenizer.decode(&[token_id], false)?;

        Ok(decoded)
    }

    /// ãƒãƒƒãƒæ¨è«–
    pub fn infer_batch(&self, inputs: &[MultimodalInput]) -> Result<Vec<String>> {
        inputs.iter().map(|input| self.infer(input)).collect()
    }
}
```

#### 4.3.4 ä½¿ç”¨ä¾‹

```rust
fn main() -> Result<()> {
    // ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    let inference = SmolVLM2Inference::load(
        "models/smolvlm2-256m.pth",
        "models/tokenizer.json",
    )?;

    // ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›
    let image = image::open("cat.jpg")?;
    let input = MultimodalInput {
        image,
        text: "What is in this image?".to_string(),
    };

    // æ¨è«–
    let result = inference.infer(&input)?;
    println!("å›ç­”: {}", result);

    Ok(())
}
```

**å‡ºåŠ›ä¾‹**:
```
å›ç­”: A cat sitting on a sofa.
```

#### 4.3.5 FFIçµŒç”±ã§Rustã‹ã‚‰å‘¼ã³å‡ºã—

```rust
// FFIç”¨ã®C-ABIé–¢æ•°
#[no_mangle]
pub extern "C" fn smolvlm2_infer(
    image_path: *const c_char,
    text: *const c_char,
    output_buf: *mut c_char,
    buf_len: usize,
) -> i32 {
    // SAFETY: Cæ–‡å­—åˆ—ã‹ã‚‰Rust &strã«å¤‰æ›
    let image_path_str = unsafe { CStr::from_ptr(image_path).to_str().unwrap() };
    let text_str = unsafe { CStr::from_ptr(text).to_str().unwrap() };

    // æ¨è«–
    let inference = SmolVLM2Inference::load("models/smolvlm2-256m.pth", "models/tokenizer.json").unwrap();
    let image = image::open(image_path_str).unwrap();
    let input = MultimodalInput {
        image,
        text: text_str.to_string(),
    };
    let result = inference.infer(&input).unwrap();

    // çµæœã‚’Cæ–‡å­—åˆ—ã«ã‚³ãƒ”ãƒ¼
    let result_cstr = CString::new(result).unwrap();
    let result_bytes = result_cstr.as_bytes_with_nul();
    if result_bytes.len() > buf_len {
        return -1; // ãƒãƒƒãƒ•ã‚¡ä¸è¶³
    }
    unsafe {
        std::ptr::copy_nonoverlapping(result_bytes.as_ptr(), output_buf as *mut u8, result_bytes.len());
    }

    0 // æˆåŠŸ
}
```

**Rustã‹ã‚‰å‘¼ã³å‡ºã—**:

```rust
// Rustãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’åŒãƒ—ãƒ­ã‚»ã‚¹å†…ã§ç›´æ¥å‘¼ã³å‡ºã—
fn rust_smolvlm2_infer(image_path: &str, text: &str) -> anyhow::Result<String> {
    let inference = SmolVLM2Inference::load(
        "models/smolvlm2-256m.pth",
        "models/tokenizer.json",
    )?;
    let image = image::open(image_path)?;
    let input = MultimodalInput {
        image,
        text: text.to_string(),
    };
    inference.infer(&input)
}

fn main() -> anyhow::Result<()> {
    // ä½¿ç”¨ä¾‹
    let result = rust_smolvlm2_infer("cat.jpg", "What is in this image?")?;
    println!("å›ç­”: {}", result);
    Ok(())
}
```

---

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®70%å®Œäº†ï¼** Zone 5ã§ã¯ã€å®Ÿè£…ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ã€‚VQAã€Captioningã€Zero-shotåˆ†é¡ã€Retrievalã®4ã¤ã®ã‚¿ã‚¹ã‚¯ã§æ€§èƒ½ã‚’æ¸¬å®šã™ã‚‹ã€‚

---

> **Progress: 85%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. CLIPã®å¯¾ç…§æå¤±è¡Œåˆ— $\text{sim} = (I_{\text{emb}} \cdot T_{\text{emb}}^T) / \tau$ ã§ã€å¯¾è§’æˆåˆ†ã¨éå¯¾è§’æˆåˆ†ãŒè¡¨ã™ã‚‚ã®ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. SmolVLM2-256Mã®ã‚ˆã†ãªå°å‹VLMãŒå¤§å‹ãƒ¢ãƒ‡ãƒ«ã¨ç«¶äº‰ã§ãã‚‹ç†ç”±ã‚’ã€è’¸ç•™ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åŠ¹ç‡åŒ–ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” è©•ä¾¡å®Ÿè£…

å®Ÿè£…ã—ãŸCLIPã¨SmolVLM2ã®æ€§èƒ½ã‚’ã€**4ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**ã§è©•ä¾¡ã™ã‚‹ã€‚

### 5.1 VQA (Visual Question Answering) è©•ä¾¡

#### 5.1.1 VQAv2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

VQAv2[^14]ã¯ã€Visual Question Answeringã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚

**æ§‹æˆ**:
- è¨“ç·´: 214Kè³ªå•
- æ¤œè¨¼: 104Kè³ªå•
- å„è³ªå•ã«10å€‹ã®äººé–“ã«ã‚ˆã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å›ç­”

**è©•ä¾¡æŒ‡æ¨™**: Accuracy

$$
\text{Accuracy} = \frac{1}{N} \sum_{i=1}^N \min\left(1, \frac{\text{num\_annotators\_agree}(a_i)}{3}\right)
$$

ã“ã“ã§ $a_i$ ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å›ç­”ã€‚3äººä»¥ä¸Šã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãŒåŒæ„ã™ã‚Œã°ã€ã‚¹ã‚³ã‚¢ã¯1ã€‚

#### 5.1.2 VQAè©•ä¾¡å®Ÿè£…ï¼ˆRustï¼‰

```rust
use std::collections::HashMap;

// VQAv2ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
pub struct VqaDataset {
    pub images: Vec<String>,       // ç”»åƒãƒ‘ã‚¹
    pub questions: Vec<String>,
    pub answers: Vec<Vec<String>>, // å„è³ªå•ã«10å€‹ã®å›ç­”
}

pub fn load_vqav2(json_path: &str) -> anyhow::Result<VqaDataset> {
    let raw = std::fs::read_to_string(json_path)?;
    let data: serde_json::Value = serde_json::from_str(&raw)?;
    let images = data["questions"].as_array().unwrap()
        .iter().map(|q| q["image_id"].as_str().unwrap().to_string()).collect();
    let questions = data["questions"].as_array().unwrap()
        .iter().map(|q| q["question"].as_str().unwrap().to_string()).collect();
    let answers = data["annotations"].as_array().unwrap()
        .iter()
        .map(|a| a["answers"].as_array().unwrap()
            .iter().map(|ans| ans["answer"].as_str().unwrap().to_string()).collect())
        .collect();
    Ok(VqaDataset { images, questions, answers })
}

// VQA Accuracyè¨ˆç®—ï¼ˆã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ãƒã‚§ãƒ¼ãƒ³ã§ã‚¼ãƒ­å‰²ã‚Šå½“ã¦ï¼‰
fn vqa_accuracy(predictions: &[String], ground_truths: &[Vec<String>]) -> f64 {
    let scores: Vec<f64> = predictions
        .iter()
        .zip(ground_truths.iter())
        .map(|(pred, gts)| {
            let count = gts.iter()
                .filter(|gt| gt.to_lowercase() == pred.to_lowercase())
                .count();
            1.0f64.min(count as f64 / 3.0)
        })
        .collect();
    scores.iter().sum::<f64>() / scores.len() as f64
}

// SmolVLM2ã§VQAè©•ä¾¡
fn evaluate_vqa(smolvlm2: &SmolVLM2Inference, dataset: &VqaDataset) -> anyhow::Result<f64> {
    let predictions: Vec<String> = dataset.images.iter()
        .zip(dataset.questions.iter())
        .map(|(img_path, question)| {
            let image = image::open(img_path)?;
            smolvlm2.infer(&MultimodalInput { image, text: question.clone() })
        })
        .collect::<anyhow::Result<_>>()?;
    let acc = vqa_accuracy(&predictions, &dataset.answers);
    println!("VQAv2 Accuracy: {:.1}%", acc * 100.0);
    Ok(acc)
}
```

#### 5.1.3 VQAè©•ä¾¡çµæœï¼ˆä¾‹ï¼‰

```rust
// æ“¬ä¼¼è©•ä¾¡çµæœ
let vqa_dataset = load_vqav2("vqav2_val.json")?;
let smolvlm2 = SmolVLM2Inference::load("models/smolvlm2-256m.pth", "models/tokenizer.json")?;
let _acc = evaluate_vqa(&smolvlm2, &vqa_dataset)?;
```

**å‡ºåŠ›ä¾‹**:
```
VQAv2 Accuracy: 68.3%
```

SmolVLM2-256Mã¯ã€ã‚ãšã‹256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§68.3%ã®ç²¾åº¦ã‚’é”æˆã€‚ã“ã‚Œã¯ã€Idefics-80Bï¼ˆ17ãƒ¶æœˆå‰ã®ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã ã€‚

#### 5.1.4 VQAå¤±æ•—ä¾‹ã®åˆ†æ

VQAãƒ¢ãƒ‡ãƒ«ã®**å¼±ç‚¹**ã‚’ç†è§£ã™ã‚‹ãŸã‚ã€å¤±æ•—ä¾‹ã‚’è¦‹ã¦ã¿ã‚ˆã†ã€‚

**ä¾‹1: æ•°å€¤ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°**

```rust
# è³ªå•: "How many cats are in the image?"
# æ­£è§£: "3"
# SmolVLM2äºˆæ¸¬: "several"
```

**åŸå› **: å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¯**æ­£ç¢ºãªã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°**ãŒè‹¦æ‰‹ã€‚ã€Œseveralã€ã€Œmanyã€ã®ã‚ˆã†ãª**æ›–æ˜§ãªè¡¨ç¾**ã«é€ƒã’ã‚‹ã€‚

**è§£æ±ºç­–**: ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°å°‚ç”¨ã®ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ ã™ã‚‹ã‹ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã§å¼·åŒ–ã™ã‚‹ã€‚

**ä¾‹2: ç´°ã‹ã„ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿å–ã‚Š**

```rust
# è³ªå•: "What does the sign say?"
# æ­£è§£: "Stop"
# SmolVLM2äºˆæ¸¬: "traffic sign"
```

**åŸå› **: ç”»åƒè§£åƒåº¦ï¼ˆ224Ã—224ï¼‰ãŒä½ã™ãã¦ã€ç´°ã‹ã„ãƒ†ã‚­ã‚¹ãƒˆãŒèª­ã‚ãªã„ã€‚

**è§£æ±ºç­–**: Qwen-VLã®ã‚ˆã†ã«**Dynamic Resolution**ã‚’å°å…¥ã—ã€é«˜è§£åƒåº¦å…¥åŠ›ã‚’è¨±å¯ã™ã‚‹ã€‚

**ä¾‹3: æ¨è«–ãŒå¿…è¦ãªè³ªå•**

```rust
# è³ªå•: "Is it likely to rain soon?"
# ç”»åƒ: æ›‡ã‚Šç©º
# æ­£è§£: "yes"
# SmolVLM2äºˆæ¸¬: "cloudy"
```

**åŸå› **: è³ªå•ã¯ã€Œé›¨ãŒé™ã‚‹ã‹ã€ã‚’èã„ã¦ã„ã‚‹ãŒã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œæ›‡ã£ã¦ã„ã‚‹ã€ã¨ã„ã†**è¦³å¯Ÿäº‹å®Ÿ**ã ã‘ã‚’ç­”ãˆã‚‹ã€‚**æ¨è«–èƒ½åŠ›**ãŒä¸è¶³ã€‚

**è§£æ±ºç­–**: Chain-of-Thought (CoT) ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã‚’å°å…¥ã—ã€ã€Œæ›‡ã£ã¦ã„ã‚‹ â†’ é›¨ãŒé™ã‚Šãã†ã€ã¨ã„ã†æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ˜ç¤ºçš„ã«å­¦ç¿’ã•ã›ã‚‹ã€‚

---

### 5.2 Image Captioningè©•ä¾¡

#### 5.2.1 COCO Captionsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

COCO Captions[^15]ã¯ã€Image Captioningã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚

**æ§‹æˆ**:
- è¨“ç·´: 82Kç”»åƒã€å„ç”»åƒã«5ã¤ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³
- æ¤œè¨¼: 40Kç”»åƒ

**è©•ä¾¡æŒ‡æ¨™**: BLEUã€METEORã€CIDErã€SPICE

**è©•ä¾¡æŒ‡æ¨™ã®ç‰¹å¾´**:

| æŒ‡æ¨™ | æ¸¬å®šå†…å®¹ | ç‰¹å¾´ | ç¯„å›² |
|:-----|:---------|:-----|:-----|
| **BLEU-4** | n-gramä¸€è‡´ï¼ˆn=1,2,3,4ï¼‰ | æ©Ÿæ¢°ç¿»è¨³ã‹ã‚‰å€Ÿç”¨ã€‚ç°¡æ½”ãªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å¥½ã‚€ | 0-1 |
| **METEOR** | Unigramä¸€è‡´ + åŒç¾©èª + stemming | å˜èªã®æŸ”è»Ÿæ€§ã‚’è€ƒæ…® | 0-1 |
| **CIDEr** | TF-IDFé‡ã¿ä»˜ãn-gramé¡ä¼¼åº¦ | äººé–“ã®åˆ¤æ–­ã¨æœ€ã‚‚ç›¸é–¢ãŒé«˜ã„ | 0-10 |
| **SPICE** | Scene Graphä¸€è‡´ | æ„å‘³çš„æ­£ç¢ºæ€§ã‚’æ¸¬å®šï¼ˆç‰©ä½“ãƒ»å±æ€§ãƒ»é–¢ä¿‚ï¼‰ | 0-1 |
| **ROUGE-L** | æœ€é•·å…±é€šéƒ¨åˆ†åˆ— | æ–‡æ§‹é€ ã®é¡ä¼¼æ€§ | 0-1 |

#### 5.2.2 CIDErå®Ÿè£…ï¼ˆRustï¼‰

```rust
use std::collections::HashMap;

// CIDEr: Consensus-based Image Description Evaluation
fn cider_score(candidate: &str, references: &[&str]) -> f64 {
    // n-gramã®TF-IDFé‡ã¿ã‚’è¨ˆç®—
    let candidate_ngrams = extract_ngrams(candidate, 4);
    let ref_ngrams: Vec<_> = references.iter().map(|r| extract_ngrams(r, 4)).collect();

    // TF-IDFè¨ˆç®—
    let candidate_tfidf = compute_tfidf(&candidate_ngrams);
    let ref_tfidfs: Vec<_> = ref_ngrams.iter().map(compute_tfidf).collect();

    // ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®å¹³å‡
    let similarities: Vec<f64> = ref_tfidfs.iter()
        .map(|rt| cosine_similarity(&candidate_tfidf, rt))
        .collect();
    similarities.iter().sum::<f64>() / similarities.len() as f64
}

fn extract_ngrams(text: &str, n: usize) -> HashMap<String, usize> {
    let tokens: Vec<&str> = text.to_lowercase().split_whitespace().collect();
    let mut ngrams = HashMap::new();
    for i in 0..tokens.len().saturating_sub(n - 1) {
        let ng = tokens[i..i + n].join(" ");
        *ngrams.entry(ng).or_insert(0) += 1;
    }
    ngrams
}

fn compute_tfidf(ngrams: &HashMap<String, usize>) -> HashMap<String, f64> {
    // ç°¡æ˜“TF-IDFï¼ˆå®Ÿéš›ã¯ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã®IDFã‚’ä½¿ç”¨ï¼‰
    ngrams.iter()
        .map(|(k, &v)| {
            let idf = (1.0 + 1.0 / v as f64).ln();
            (k.clone(), v as f64 * idf)
        })
        .collect()
}

fn cosine_similarity(vec1: &HashMap<String, f64>, vec2: &HashMap<String, f64>) -> f64 {
    let all_keys: std::collections::HashSet<_> = vec1.keys().chain(vec2.keys()).collect();
    let dot_prod: f64 = all_keys.iter()
        .map(|k| vec1.get(*k).unwrap_or(&0.0) * vec2.get(*k).unwrap_or(&0.0))
        .sum();
    let norm1: f64 = vec1.values().map(|v| v * v).sum::<f64>().sqrt();
    let norm2: f64 = vec2.values().map(|v| v * v).sum::<f64>().sqrt();
    dot_prod / (norm1 * norm2 + 1e-8)
}
```

#### 5.2.3 SPICEå®Ÿè£…ï¼ˆå¤–éƒ¨ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ï¼‰

SPICEã¯ã€**Scene Graphãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡**ãªã®ã§ã€å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ï¼ˆStanford Scene Graph Parserï¼‰ã‚’ä½¿ã†ã€‚

```rust
// SPICEè©•ä¾¡ï¼ˆå¤–éƒ¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆçµŒç”±ï¼‰
fn spice_score(candidate: &str, references: &[&str]) -> anyhow::Result<f64> {
    // Pythonã®SPICEå®Ÿè£…ã‚’å‘¼ã³å‡ºã—
    let refs_arg = references.join("|");
    let output = std::process::Command::new("python")
        .args(["spice.py", "--candidate", candidate, "--references", &refs_arg])
        .output()?;
    let result = String::from_utf8(output.stdout)?;
    Ok(result.trim().parse::<f64>()?)
}
```

---

### 5.3 Zero-shotåˆ†é¡è©•ä¾¡

#### 5.3.1 ImageNetã§ã®è©•ä¾¡

CLIPã®Zero-shotåˆ†é¡ç²¾åº¦ã‚’ã€ImageNet-1kã§æ¸¬å®šã™ã‚‹ã€‚

```rust
// ImageNet-1kè©•ä¾¡
fn evaluate_zero_shot_imagenet(
    clip: &Clip,
    imagenet_val: &[(Tensor, u32)],
    tokenize: &impl Fn(&str) -> Result<Tensor>,
) -> Result<f64> {
    // ImageNetã‚¯ãƒ©ã‚¹åï¼ˆ1000ã‚¯ãƒ©ã‚¹ï¼‰
    let class_names = load_imagenet_class_names();
    let candidates: Vec<&str> = class_names.iter().map(|s| s.as_str()).collect();

    let mut correct = 0usize;
    for (img, label) in imagenet_val {
        let (_, pred) = zero_shot_classify(clip, img, &candidates, tokenize)?;
        if pred as u32 == *label {
            correct += 1;
        }
    }
    let acc = correct as f64 / imagenet_val.len() as f64;
    println!("ImageNet Zero-shot Accuracy: {:.1}%", acc * 100.0);
    Ok(acc)
}
```

**CLIP-ViT-L/14ã®çµæœ** (è«–æ–‡å€¤)[^1]:
```
ImageNet Zero-shot Accuracy: 75.5%
```

---

### 5.4 Image-Text Retrievalè©•ä¾¡

#### 5.4.1 Recall@Kå®Ÿè£…

```rust
// Image-to-Text Retrieval
fn image_to_text_retrieval(
    clip: &Clip,
    images: &[Tensor],
    texts: &[&str],
    k: usize,  // = 5
    tokenize: &impl Fn(&str) -> Result<Tensor>,
) -> Result<f64> {
    let mut recall_at_k = 0usize;

    for (i, img) in images.iter().enumerate() {
        // ç”»åƒåŸ‹ã‚è¾¼ã¿ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã‚¹ãƒ©ã‚¤ã‚¹ï¼‰
        let img_emb = clip.vision.forward(&img.unsqueeze(0)?)?;  // (1, out_dim)

        // å…¨ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
        let text_embeds: Vec<Tensor> = texts.iter()
            .map(|t| clip.text.forward(&tokenize(t)?))
            .collect::<Result<_>>()?;
        let text_embeds = Tensor::cat(&text_embeds, 0)?;  // (N, out_dim)

        // é¡ä¼¼åº¦è¨ˆç®—ï¼ˆbroadcastï¼‰
        let img_norm = l2_normalize(&img_emb)?;
        let txt_norm = l2_normalize(&text_embeds)?;
        let similarities = img_norm.matmul(&txt_norm.t()?)?.squeeze(0)?;  // (N,)

        // Top-Kå–å¾—ã—æ­£è§£ã‚’å«ã‚€ã‹åˆ¤å®š
        let sims_vec = similarities.to_vec1::<f32>()?;
        let mut indices: Vec<usize> = (0..sims_vec.len()).collect();
        indices.sort_by(|&a, &b| sims_vec[b].partial_cmp(&sims_vec[a]).unwrap());
        if indices[..k].contains(&i) {
            recall_at_k += 1;
        }
    }

    Ok(recall_at_k as f64 / images.len() as f64)
}
```

**COCO Captionsã§ã®çµæœ** (CLIPè«–æ–‡å€¤)[^1]:
```
Image-to-Text Recall@5: 88.0%
Text-to-Image Recall@5: 68.7%
```

---

### 5.5 Self-check Checklist

ä»¥ä¸‹ã®é …ç›®ã‚’ç¢ºèªã—ã¦ã€å®Ÿè£…ã¨è©•ä¾¡ãŒæ­£ã—ãè¡Œã‚ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯ã—ã‚ˆã†ã€‚

- [ ] InfoNCE lossãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ï¼ˆå¯¾è§’æˆåˆ†ãŒæœ€å¤§ã«ãªã£ã¦ã„ã‚‹ã‹ï¼‰
- [ ] Vision Encoderã¨Text Encoderã®å‡ºåŠ›æ¬¡å…ƒãŒä¸€è‡´ã—ã¦ã„ã‚‹
- [ ] Zero-shotåˆ†é¡ã®ç²¾åº¦ãŒè«–æ–‡å€¤ã«è¿‘ã„ï¼ˆÂ±3%ä»¥å†…ï¼‰
- [ ] VQA Accuracyã®è¨ˆç®—å¼ãŒæ­£ã—ã„ï¼ˆ3äººä»¥ä¸Šã®åˆæ„ã§1ã‚¹ã‚³ã‚¢ï¼‰
- [ ] CIDErãŒn-gramã®TF-IDFã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹
- [ ] Image-Text Retrievalã§åŒæ–¹å‘ï¼ˆImageâ†’Text, Textâ†’Imageï¼‰ã‚’è©•ä¾¡ã—ã¦ã„ã‚‹
- [ ] Rustæ¨è«–ãŒRustã‹ã‚‰æ­£ã—ãå‘¼ã³å‡ºã›ã‚‹ï¼ˆFFIçµŒç”±ï¼‰

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®85%å®Œäº†ï¼** Zone 6ã§ã¯ã€æœ€æ–°ç ”ç©¶ã¨å…¨ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’ä¿¯ç°ã™ã‚‹ã€‚

---

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¸–ç•Œã¯æ€¥é€Ÿã«é€²åŒ–ã—ã¦ã„ã‚‹ã€‚ã“ã“ã§ã¯ã€**7ã¤ã®ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ã‚’ä¿¯ç°ã—ã€æœ€æ–°ç ”ç©¶ã‚’ç´¹ä»‹ã™ã‚‹ã€‚

### 6.1 Vision-Languageãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ãƒ„ãƒªãƒ¼

```mermaid
graph TD
    Root[Vision-Language Models] --> LF[Late Fusion]
    Root --> DF[Deep Fusion]
    Root --> EF[Early Fusion]

    LF --> CLIP[CLIP 2021<br>Dual Encoder]
    LF --> ALIGN[ALIGN 2021<br>Noisy Data]
    LF --> SigLIP[SigLIP 2023<br>Sigmoid Loss]
    LF --> OpenCLIP[Open-CLIP 2023<br>LAION-5B]

    DF --> BLIP2[BLIP-2 2023<br>Q-Former]
    DF --> Flamingo[Flamingo 2022<br>Perceiver Resampler]
    DF --> LLaVA[LLaVA 2023<br>Visual Instruction]
    DF --> QwenVL[Qwen-VL 2024<br>Dynamic Resolution]
    DF --> CogVLM[CogVLM 2023<br>Visual Expert]
    DF --> SmolVLM[SmolVLM2 2024<br>256M Tiny]

    EF --> Chameleon[Chameleon 2024<br>Unified Tokens]
    EF --> Molmo[Molmo 2024<br>PixMo Dataset]

    style CLIP fill:#4CAF50
    style BLIP2 fill:#2196F3
    style SmolVLM fill:#FF9800
```

### 6.2 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒè¡¨

| ãƒ¢ãƒ‡ãƒ« | å¹´ | Fusion | Vision Enc | Text Enc | ç‰¹å¾´ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ä¸»è¦è«–æ–‡ |
|:-------|:---|:-------|:----------|:---------|:-----|:---------|:---------|
| **CLIP** | 2021 | Late | ViT/ResNet | Transformer | Contrastiveå­¦ç¿’ã€Zero-shot | 151M-428M | [^1] |
| **ALIGN** | 2021 | Late | EfficientNet | BERT | ãƒã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿è€æ€§ | 1B | Google |
| **Flamingo** | 2022 | Deep | NFNet | Chinchilla | Perceiver Resamplerã€Few-shot | 80B | [^5] |
| **BLIP-2** | 2023 | Deep | ViT | OPT/FlanT5 | Q-Formerã€Frozen LLM | 2.7B-13B | [^4] |
| **LLaVA** | 2023 | Deep | CLIP ViT | Vicuna | Visual Instruction Tuning | 7B-13B | [^6] |
| **SigLIP** | 2023 | Late | ViT | Transformer | Sigmoid lossã€ãƒãƒƒãƒéä¾å­˜ | 149M-986M | [^12] |
| **Open-CLIP** | 2023 | Late | ViT | Transformer | LAION-5Bè¨“ç·´ã€OSS | 149M-986M | [^11] |
| **CogVLM** | 2023 | Deep | ViT | Vicuna | Visual Expertã€Deep Fusion | 17B | [^8] |
| **Qwen-VL** | 2024 | Deep | ViT | Qwen | Dynamic Resolutionã€RoPE 2D | 7B-72B | [^7] |
| **Molmo** | 2024 | Deep | ViT | OLMo | PixMo 1Mé«˜å“è³ªãƒ‡ãƒ¼ã‚¿ | 7B | [^13] |
| **SmolVLM2** | 2024 | Deep | ViT | SmolLM2 | æ¥µå°256Mã€3ãƒ¢ãƒ€ãƒªãƒ†ã‚£ | 256M-2.2B | [^9] |
| **Chameleon** | 2024 | Early | ViT | Unified | ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆçµ±ä¸€Token | 7B-34B | Meta |

### 6.3 BLIP-2å®Œå…¨è§£å‰–

BLIP-2[^4]ã¯ã€**Q-Former**ã¨ã„ã†ç‹¬è‡ªã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å°å…¥ã—ãŸã€‚Frozen Vision Encoderã¨Frozen LLMã®é–“ã‚’æ©‹æ¸¡ã—ã™ã‚‹ã€**æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**ã®å½¹å‰²ã‚’æœãŸã™ã€‚

#### 6.3.1 Q-Formerã®è¨­è¨ˆåŸç†

**å‹•æ©Ÿ**: å¤§è¦æ¨¡ãªVision Encoderã¨LLMã‚’**ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´**ã™ã‚‹ã®ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒè†¨å¤§ã€‚æ—¢å­˜ã®äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸã„ã€‚

**èª²é¡Œ**:
1. Vision Encoderã®å‡ºåŠ›ï¼ˆ196 tokensãªã©ï¼‰ã¯**é•·ã™ãã‚‹** â†’ LLMã®å…¥åŠ›ã¨ã—ã¦éåŠ¹ç‡
2. Vision Encoderã¨LLMã¯**ç‹¬ç«‹ã«è¨“ç·´**ã•ã‚Œã¦ã„ã‚‹ â†’ åŸ‹ã‚è¾¼ã¿ç©ºé–“ãŒç•°ãªã‚‹
3. LLMã‚’**Fine-tuning**ã™ã‚‹ã¨ã€å…ƒã®è¨€èªèƒ½åŠ›ãŒåŠ£åŒ–ã™ã‚‹ï¼ˆCatastrophic Forgettingï¼‰

**è§£æ±ºç­–: Q-Former**

Q-Formerã¯ã€**å­¦ç¿’å¯èƒ½ãªã‚¯ã‚¨ãƒª**ã‚’ä½¿ã£ã¦ã€ç”»åƒç‰¹å¾´ã‚’**å›ºå®šé•·**ï¼ˆ32 tokensï¼‰ã«åœ§ç¸®ã™ã‚‹ã€‚

```mermaid
graph TD
    ImgEnc[Frozen<br>Vision Encoder] --> ImgFeats[ç”»åƒç‰¹å¾´<br>N tokens]
    Queries[Learnable Queries<br>32 tokens] --> QFormer[Q-Former<br>Cross-Attention]
    ImgFeats --> QFormer
    QFormer --> VFeats[è¦–è¦šç‰¹å¾´<br>32 tokens]
    VFeats --> LLM[Frozen LLM]
    TextPrompt[ãƒ†ã‚­ã‚¹ãƒˆPrompt] --> LLM
    LLM --> Output[ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ]
```

**Q-Formerã®å½¹å‰²**:
1. **æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: å¯å¤‰é•·ã®ç”»åƒç‰¹å¾´ï¼ˆ196 tokensï¼‰ã‚’å›ºå®šé•·ï¼ˆ32 tokensï¼‰ã«åœ§ç¸®ã€‚
2. **Vision-Language Bridge**: Frozen Vision Encoderã¨Frozen LLMã®é–“ã‚’æ©‹æ¸¡ã—ã€‚
3. **Cross-Attention**: QueryãŒç”»åƒç‰¹å¾´ã«Cross-Attentionã—ã¦ã€é‡è¦ãªè¦–è¦šæƒ…å ±ã‚’æŠ½å‡ºã€‚

**æ•°å¼**:

$$
\mathbf{Q} = \text{LearnableQueries} \in \mathbb{R}^{d \times 32}
$$

$$
\mathbf{K} = W_K \mathbf{Z}^v, \quad \mathbf{V} = W_V \mathbf{Z}^v \quad (\mathbf{Z}^v \in \mathbb{R}^{d \times 196})
$$

$$
\mathbf{Z}_{\text{visual}} = \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) \in \mathbb{R}^{d \times 32}
$$

#### 6.3.2 Two-stage Pre-training

**Stage 1: Vision-Language Representation Learning**

3ã¤ã®æå¤±ã‚’åŒæ™‚æœ€é©åŒ–:

1. **ITC (Image-Text Contrastive)**: CLIPã¨åŒã˜InfoNCE loss
2. **ITG (Image-grounded Text Generation)**: ç”»åƒã‚’æ¡ä»¶ã¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
3. **ITM (Image-Text Matching)**: ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã®ãƒãƒƒãƒãƒ³ã‚°ï¼ˆBinaryåˆ†é¡ï¼‰

**Stage 2: Vision-to-Language Generative Learning**

Q-Formerã‚’Frozen LLMã«æ¥ç¶šã—ã€**Language Modeling Loss**ã§è¨“ç·´:

$$
\mathcal{L}_{\text{LM}} = -\sum_{t=1}^T \log p(w_t \mid w_{<t}, \mathbf{Z}_{\text{visual}})
$$

### 6.4 LLaVA: Visual Instruction Tuning

LLaVA[^6]ã¯ã€**Visual Instruction Tuning**ã‚’å°å…¥ã—ãŸã€‚

**ã‚³ãƒ³ã‚»ãƒ—ãƒˆ**: GPT-4ã«Image Captionã‚’è¦‹ã›ã¦ã€**Instruction-Following ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ**ã•ã›ã‚‹ã€‚

**ä¾‹**:
```
ç”»åƒ: [çŒ«ãŒã‚½ãƒ•ã‚¡ã§å¯ã¦ã„ã‚‹å†™çœŸ]
Instruction: "ã“ã®ç”»åƒã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
GPT-4ç”Ÿæˆå›ç­”: "ã“ã®ç”»åƒã«ã¯ã€ã‚°ãƒ¬ãƒ¼ã®çŒ«ãŒé’ã„ã‚½ãƒ•ã‚¡ã®ä¸Šã§ä¸¸ã¾ã£ã¦å¯ã¦ã„ã‚‹æ§˜å­ãŒæ˜ ã£ã¦ã„ã¾ã™ã€‚..."
```

ã“ã®ãƒ‡ãƒ¼ã‚¿ã§LLaVAã‚’è¨“ç·´ã™ã‚‹ã¨ã€**GPT-4ã®85.1%ã®æ€§èƒ½**ã‚’é”æˆï¼ˆåˆæˆãƒ‡ãƒ¼ã‚¿ã§ã®æ¯”è¼ƒï¼‰ã€‚

#### 6.4.1 LLaVAã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

```mermaid
graph LR
    Img[ç”»åƒ] --> CLIP[CLIP ViT-L/14<br>Frozen]
    CLIP --> ImgFeats[ç”»åƒç‰¹å¾´<br>256 tokens]
    ImgFeats --> Proj[Projection MLP<br>Trainable]
    Proj --> VisTokens[è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³<br>256â†’32 tokens]
    TextPrompt[ãƒ†ã‚­ã‚¹ãƒˆPrompt] --> Concat[Concatenate]
    VisTokens --> Concat
    Concat --> Vicuna[Vicuna-7B<br>Frozen or LoRA]
    Vicuna --> Output[ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ]
```

**Projection MLPã®å½¹å‰²**:

LLaVAã¯ã€CLIP ViTã®å‡ºåŠ›ï¼ˆ256 tokensï¼‰ã‚’**å˜ç´”ãªMLP**ã§32 tokensã«åœ§ç¸®ã™ã‚‹ã€‚BLIP-2ã®Q-Formerã»ã©è¤‡é›‘ã§ã¯ãªã„ãŒã€**è¨“ç·´ãŒç°¡å˜**ã§åŠ¹æœçš„ã€‚

**æ•°å¼**:

$$
\mathbf{Z}_{\text{visual}} = \text{MLP}(\mathbf{Z}_{\text{CLIP}}) \in \mathbb{R}^{d \times 32}
$$

$$
\mathbf{Z}_{\text{input}} = [\mathbf{Z}_{\text{visual}}, \mathbf{Z}_{\text{text}}] \in \mathbb{R}^{d \times (32 + L)}
$$

#### 6.4.2 LLaVAã®è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆ2æ®µéšï¼‰

**Stage 1: Pre-training (Feature Alignment)**

- ãƒ‡ãƒ¼ã‚¿: CC3Mï¼ˆ3M image-caption pairsï¼‰
- ç›®æ¨™: è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®**åŸ‹ã‚è¾¼ã¿ç©ºé–“ã‚’æƒãˆã‚‹**
- è¨“ç·´å¯¾è±¡: Projection MLPã®ã¿ï¼ˆCLIP ViT + Vicunaã¯å‡çµï¼‰
- æå¤±: Language Modeling Loss

$$
\mathcal{L}_{\text{LM}} = -\sum_{t=1}^T \log p(w_t \mid w_{<t}, \mathbf{Z}_{\text{visual}})
$$

**Stage 2: Fine-tuning (Instruction Tuning)**

- ãƒ‡ãƒ¼ã‚¿: LLaVA-Instruct-150Kï¼ˆGPT-4ç”Ÿæˆï¼‰
- ç›®æ¨™: Instruction-Followingã‚’å­¦ç¿’
- è¨“ç·´å¯¾è±¡: Projection MLP + Vicunaï¼ˆLoRAï¼‰
- æå¤±: åŒã˜Language Modeling Loss

**LLaVA-1.5ã®æ”¹å–„ç‚¹**:
1. **é«˜è§£åƒåº¦å¯¾å¿œ**: 336Ã—336 å…¥åŠ›ï¼ˆå…ƒã¯224Ã—224ï¼‰
2. **ShareGPT4Vè¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ã‚ˆã‚Šå¤šæ§˜ã§é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
3. **Multi-turnå¯¾è©±**: è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®å¯¾è©±ã‚’å­¦ç¿’

#### 6.4.3 LLaVAã®Productionå®Ÿè£…ï¼ˆRustï¼‰

```rust
use candle_core::{Result, Tensor};
use candle_nn::{self as nn, Module, VarBuilder};

// LLaVA: Visual Instruction Tuning
pub struct LLaVA {
    clip_vit: VisionTransformer,  // Frozen
    projection: nn::Sequential,   // Trainable MLP
    llm: Vicuna,                  // Frozen or LoRA
}

impl LLaVA {
    pub fn new(vb: VarBuilder) -> Result<Self> {
        let clip_vit = VisionTransformer::load_pretrained(
            "openai/clip-vit-large-patch14", vb.pp("clip_vit"),
        )?;
        let projection = nn::seq()
            .add(nn::linear(1024, 4096, vb.pp("projection.0"))?)
            .add(nn::Activation::Gelu)
            .add(nn::linear(4096, 4096, vb.pp("projection.2"))?);
        let llm = Vicuna::load_pretrained("lmsys/vicuna-7b-v1.5", vb.pp("llm"))?;
        Ok(Self { clip_vit, projection, llm })
    }

    pub fn forward(&self, image: &Tensor, text_tokens: &Tensor) -> Result<Tensor> {
        // ç”»åƒç‰¹å¾´æŠ½å‡ºï¼ˆFrozenï¼‰â†’ Projection â†’ ãƒ†ã‚­ã‚¹ãƒˆã¨é€£çµ â†’ LLM
        let vis_tokens = self.projection.forward(&self.clip_vit.forward(image)?)?;  // (B, 32, 4096)
        let combined = Tensor::cat(&[&vis_tokens, text_tokens], 1)?;               // (B, L+32, 4096)
        self.llm.forward(&combined)
    }
}

// è¨“ç·´ï¼ˆStage 2: Instruction Tuningï¼‰
fn train_llava_stage2(
    llava: &mut LLaVA,
    instruct_data: &[(Tensor, Tensor, Tensor)],  // (image, prompt_tokens, answer_tokens)
    epochs: usize,                               // = 3
) -> Result<()> {
    // LoRAã‚’é©ç”¨
    apply_lora(&mut llava.llm, 8)?;

    let varmap = nn::VarMap::new();
    let mut opt = nn::AdamW::new(
        varmap.all_vars(),  // projection + LLM LoRA params (CLIP ViTã¯é™¤å¤–)
        nn::ParamsAdamW { lr: 1e-4, ..Default::default() },
    )?;

    for _epoch in 0..epochs {
        for (image, prompt, answer) in instruct_data {
            let output = llava.forward(image, prompt)?;
            // Language Modeling Loss
            let loss = nn::loss::cross_entropy(
                &output.reshape(((), output.dim(2)?))?,
                answer,
            )?;
            opt.backward_step(&loss)?;
        }
    }
    Ok(())
}
```

### 6.5 Qwen-VL: Dynamic Resolution

Qwen-VL[^7]ã¯ã€**Dynamic Resolution**ã‚’å°å…¥ã€‚

**å•é¡Œ**: å¾“æ¥ã®ViTã¯å›ºå®šè§£åƒåº¦ï¼ˆ224Ã—224ï¼‰ã«åˆ¶é™ã•ã‚Œã‚‹ãŸã‚ã€é«˜è§£åƒåº¦ç”»åƒã®è©³ç´°ãŒå¤±ã‚ã‚Œã‚‹ã€‚

**è§£æ±ºç­–**: å…¥åŠ›ç”»åƒã‚’**å¯å¤‰ã‚µã‚¤ã‚ºã®ãƒ‘ãƒƒãƒ**ã«åˆ†å‰²ã—ã€**2D RoPE** (Rotary Position Embedding) ã§ä½ç½®ã‚’è¡¨ç¾ã€‚

#### 6.5.1 2D RoPEã®æ•°å­¦çš„åŸºç¤

**1D RoPEï¼ˆå¾©ç¿’ï¼‰**: ç¬¬16å›ã§å­¦ã‚“ã Rotary Position Embeddingã¯ã€1æ¬¡å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’å›è»¢è¡Œåˆ—ã§è¡¨ç¾ã—ãŸ:

$$
\mathbf{q}_m = \begin{bmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{bmatrix} \begin{bmatrix} q_0 \\ q_1 \end{bmatrix}
$$

**2D RoPEï¼ˆQwen-VLï¼‰**: ç”»åƒãƒ‘ãƒƒãƒã¯2æ¬¡å…ƒã®ä½ç½® $(x, y)$ ã‚’æŒã¤ãŸã‚ã€**2ã¤ã®ç‹¬ç«‹ãªå›è»¢**ã‚’é©ç”¨:

$$
\mathbf{e}_{\text{pos}}(x, y) = [\underbrace{\cos(x\theta_1), \sin(x\theta_1)}_{\text{xæ–¹å‘}}, \underbrace{\cos(y\theta_2), \sin(y\theta_2)}_{\text{yæ–¹å‘}}, \ldots]
$$

ã“ã“ã§ $\theta_i = 10000^{-2i/d}$ ã¯RoPEã®åŸºæœ¬å‘¨æ³¢æ•°ã€‚

**Attentionã¸ã®é©ç”¨**:

$$
\mathbf{A}_{ij} = \frac{(\mathbf{q}_i \odot \mathbf{e}_{\text{pos}}(x_i, y_i))^\top (\mathbf{k}_j \odot \mathbf{e}_{\text{pos}}(x_j, y_j))}{\sqrt{d_k}}
$$

$\odot$ ã¯è¦ç´ ã”ã¨ã®ç©ï¼ˆHadamardç©ï¼‰ã€‚

**åˆ©ç‚¹**:
1. **ä»»æ„ã®è§£åƒåº¦ã«å¯¾å¿œ**: è¨“ç·´æ™‚ã«è¦‹ã¦ã„ãªã„è§£åƒåº¦ã§ã‚‚æ¨è«–å¯èƒ½ã€‚
2. **ç›¸å¯¾ä½ç½®ã®å­¦ç¿’**: $(x_i - x_j, y_i - y_j)$ ã®ç›¸å¯¾ä½ç½®ãŒè‡ªå‹•ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã€‚
3. **å¤–æŒ¿æ€§**: è¨“ç·´æ™‚ã‚ˆã‚Šã‚‚å¤§ããªè§£åƒåº¦ã§ã‚‚æ€§èƒ½åŠ£åŒ–ãŒå°‘ãªã„ã€‚

#### 6.5.2 Qwen2-VLã®æ”¹å–„ç‚¹ï¼ˆNaive Deduplicationï¼‰

**å•é¡Œ**: Webã‹ã‚‰åé›†ã—ãŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã¯**é‡è¤‡ç”»åƒ**ãŒå¤šã„ï¼ˆåŒã˜ç”»åƒãŒè¤‡æ•°ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã§ç™»å ´ï¼‰ã€‚

**è§£æ±ºç­–: Naive Deduplication**

1. **ç”»åƒãƒãƒƒã‚·ãƒ¥**: å„ç”»åƒã®perceptual hashï¼ˆpHashï¼‰ã‚’è¨ˆç®—
2. **é‡è¤‡æ¤œå‡º**: ãƒãƒƒã‚·ãƒ¥ãŒé¡ä¼¼ã—ã¦ã„ã‚‹ç”»åƒï¼ˆHammingè·é›¢ < 5ï¼‰ã‚’é‡è¤‡ã¨ã¿ãªã™
3. **ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³çµ±åˆ**: é‡è¤‡ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å…¨ã¦çµ±åˆã—ã€æœ€ã‚‚è©³ç´°ãªã‚‚ã®ã‚’æ®‹ã™

**åŠ¹æœ**:
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: 500M â†’ 350Mï¼ˆ30%å‰Šæ¸›ï¼‰
- è¨“ç·´æ™‚é–“: 20%çŸ­ç¸®
- æ€§èƒ½: VQAv2 75.3% â†’ 77.8%ï¼ˆé‡è¤‡é™¤å»ã§ç²¾åº¦å‘ä¸Šï¼‰

#### 6.5.3 Qwen-VLã®å®Ÿè£…ï¼ˆRustï¼‰

```rust
use candle_core::{Device, DType, Result, Tensor};

// 2D RoPEã®å®Ÿè£…ï¼ˆã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ãƒã‚§ãƒ¼ãƒ³ã§ç°¡æ½”ã«ï¼‰
fn rope_2d(x: usize, y: usize, d: usize, device: &Device) -> Result<Tensor> {
    let half = d / 4;
    let theta: Vec<f64> = (0..half)
        .map(|i| 10000_f64.powf(-2.0 * i as f64 / d as f64))
        .collect();
    let theta = Tensor::from_vec(theta, (half,), device)?;
    let x_theta = (Tensor::full(x as f64, (half,), device)? * &theta)?;
    let y_theta = (Tensor::full(y as f64, (half,), device)? * &theta)?;

    let x_emb = Tensor::cat(&[x_theta.cos()?, x_theta.sin()?], 0)?;  // xæ–¹å‘ã®å›è»¢
    let y_emb = Tensor::cat(&[y_theta.cos()?, y_theta.sin()?], 0)?;  // yæ–¹å‘ã®å›è»¢
    Tensor::cat(&[x_emb, y_emb], 0)  // (d,)
}

// Dynamic Resolutionå¯¾å¿œã®Patch Embedding
fn dynamic_patch_embed(img: &Tensor, patch_size: usize) -> Result<(Tensor, Vec<(usize, usize)>)> {
    // img: (C, H, W)
    let (_, h, w) = img.dims3()?;
    let num_patches_h = h / patch_size;
    let num_patches_w = w / patch_size;

    let mut patches = Vec::new();
    let mut positions = Vec::new();

    for i in 0..num_patches_h {
        for j in 0..num_patches_w {
            // ãƒ‘ãƒƒãƒåˆ‡ã‚Šå‡ºã—ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ï¼‰
            let patch = img.i((
                ..,
                i * patch_size..(i + 1) * patch_size,
                j * patch_size..(j + 1) * patch_size,
            ))?;
            patches.push(patch.flatten_all()?);
            positions.push((i + 1, j + 1));
        }
    }

    let patch_tensor = Tensor::stack(&patches, 0)?;  // (N, PÂ²C)
    Ok((patch_tensor, positions))  // (N, PÂ²C), [(1,1), (1,2), ...]
}

// Attentionã«2D RoPEã‚’é©ç”¨
fn attention_with_2d_rope(
    q: &Tensor,  // (d, N)
    k: &Tensor,  // (d, N)
    v: &Tensor,  // (d, N)
    positions: &[(usize, usize)],
    d_k: usize,
    device: &Device,
) -> Result<Tensor> {
    let d = q.dim(0)?;
    let rope_embs: Vec<Tensor> = positions.iter()
        .map(|&(px, py)| rope_2d(px, py, d, device))
        .collect::<Result<_>>()?;
    let rope_matrix = Tensor::stack(&rope_embs, 1)?;  // (d, N)

    // Q, K ã«2D RoPEã‚’æ›ã‘åˆã‚ã›ã‚‹
    let q_rope = (q * &rope_matrix)?;
    let k_rope = (k * &rope_matrix)?;

    // Attentionè¨ˆç®—
    let scale = (d_k as f64).sqrt();
    let attn = candle_nn::ops::softmax(
        &q_rope.t()?.matmul(&k_rope)?.affine(1.0 / scale, 0.0)?,
        1,
    )?;
    v.matmul(&attn.t()?)
}
```

#### 6.5.4 Dynamic Resolutionã®åŠ¹æœï¼ˆå®Ÿé¨“çµæœï¼‰

| è§£åƒåº¦ | å¾“æ¥ViT (å›ºå®š224Ã—224) | Qwen-VL (Dynamic) | æ”¹å–„ç‡ |
|:-------|:---------------------|:------------------|:------|
| 224Ã—224 | 72.3% | 72.5% | +0.2% |
| 336Ã—336 | 70.1% | 75.8% | **+5.7%** |
| 448Ã—448 | 65.4% | 78.2% | **+12.8%** |
| 672Ã—672 | 58.9% | 79.6% | **+20.7%** |

**è¦³å¯Ÿ**:
- å¾“æ¥ViTã¯ã€è¨“ç·´è§£åƒåº¦ï¼ˆ224Ã—224ï¼‰ã‹ã‚‰é›¢ã‚Œã‚‹ã¨æ€§èƒ½ãŒæ€¥æ¿€ã«ä½ä¸‹ã€‚
- Qwen-VLã¯ã€é«˜è§£åƒåº¦ã«ãªã‚‹ã»ã©æ€§èƒ½ãŒ**å‘ä¸Š**ï¼ˆç´°ã‹ã„è©³ç´°ã‚’æ‰ãˆã‚‰ã‚Œã‚‹ï¼‰ã€‚

### 6.6 CogVLM: Visual Expert

CogVLM[^8]ã¯ã€**Visual Expert**ã‚’å„Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ã«æŒ¿å…¥ã€‚

**é€šå¸¸ã®Transformer**:

$$
\mathbf{h}' = \mathbf{h} + \text{Attention}(\mathbf{h}) + \text{FFN}(\mathbf{h})
$$

**CogVLMã®Visual Expert**:

$$
\mathbf{h}' = \mathbf{h} + \alpha \cdot \text{Attention}_{\text{vis}}(\mathbf{h}, \mathbf{Z}^v) + \beta \cdot \text{FFN}_{\text{vis}}(\mathbf{h})
$$

$\alpha, \beta$ ã¯å­¦ç¿’å¯èƒ½ãªã‚²ãƒ¼ãƒˆã€‚é€šå¸¸ã®FFNã¨Visual FFNã‚’**ä¸¦åˆ—**ã«å®Ÿè¡Œã—ã€é‡ã¿ä»˜ãå’Œã‚’å–ã‚‹ã€‚

**åˆ©ç‚¹**: Frozen LMã®æ€§èƒ½ã‚’ä¿ã¡ã¤ã¤ã€è¦–è¦šæƒ…å ±ã‚’æ·±ãçµ±åˆã€‚

### 6.7 SmolVLM2: æ¥µå°256Mãƒ¢ãƒ‡ãƒ«

SmolVLM2[^9]ã¯ã€**256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ã§3ãƒ¢ãƒ€ãƒªãƒ†ã‚£ï¼ˆç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªï¼‰ã‚’å®Ÿç¾ã€‚

**åŠ¹ç‡åŒ–æŠ€è¡“**:
1. **Distillation**: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆIdefics2-8Bï¼‰ã‹ã‚‰çŸ¥è­˜ã‚’è’¸ç•™ã€‚
2. **Connectoråœ§ç¸®**: Vision Encoderã®å‡ºåŠ›ã‚’**16 tokens**ã«åœ§ç¸®ï¼ˆé€šå¸¸ã¯32-64 tokensï¼‰ã€‚
3. **Small LM**: SmolLM2-135Mï¼ˆGPT-2ã‚µã‚¤ã‚ºï¼‰ã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«ä½¿ç”¨ã€‚

**æ€§èƒ½**: Idefics-80Bï¼ˆ17ãƒ¶æœˆå‰ï¼‰ã‚’ä¸Šå›ã‚‹ã€‚

### 6.8 æœ€æ–°ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ2024-2026ï¼‰

#### 6.8.1 Molmo & PixMo

Molmo[^13]ã¯ã€Allen AIã«ã‚ˆã‚‹**å®Œå…¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹VLM**ã€‚

**PixMo Dataset**:
- **PixMo-Cap**: 1Mé«˜å“è³ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ï¼ˆéŸ³å£°å…¥åŠ›ã§äººé–“ãŒè¨˜è¿°ï¼‰
- **PixMo-Points**: 2D Pointing annotations â€” éè¨€èªçš„ãªã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°

**é©æ–°**: ãƒ¢ãƒ‡ãƒ«ãŒ**ç”»åƒä¸Šã®åº§æ¨™ã‚’å‡ºåŠ›**ã§ãã‚‹ã€‚ã€ŒçŒ«ã¯ã©ã“ï¼Ÿã€â†’ `(342, 189)` ã®ã‚ˆã†ã«å›ç­”ã€‚

#### 6.8.2 EVA-CLIP

EVA-CLIPï¼ˆ2023ï¼‰ã¯ã€**5B Vision Encoder**ã‚’ä½¿ç”¨ã€‚

**è¨“ç·´æˆ¦ç•¥**:
1. **MIM (Masked Image Modeling)** ã§Vision Encoderã‚’äº‹å‰è¨“ç·´
2. CLIPã®Contrastiveå­¦ç¿’ã§Fine-tuning

**çµæœ**: ImageNet Zero-shot 80.4%ï¼ˆCLIP-ViT-L/14ã¯75.5%ï¼‰ã€‚

### 6.9 æ¨å¥¨æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹

| æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹ | è‘—è€…/æ©Ÿé–¢ | å†…å®¹ | URL |
|:-------------|:---------|:-----|:----|
| **CLIPè«–æ–‡** | Radford et al., OpenAI | CLIPã®åŸè«–æ–‡ | [arXiv:2103.00020](https://arxiv.org/abs/2103.00020) |
| **BLIP-2è«–æ–‡** | Li et al., Salesforce | Q-Formerã®è©³ç´° | [arXiv:2301.12597](https://arxiv.org/abs/2301.12597) |
| **Flamingoè«–æ–‡** | Alayrac et al., DeepMind | Perceiver Resampler | [arXiv:2204.14198](https://arxiv.org/abs/2204.14198) |
| **HuggingFace Transformers** | HuggingFace | VLMå®Ÿè£…é›† | [github.com/huggingface/transformers](https://github.com/huggingface/transformers) |
| **Open-CLIP** | LAION | CLIPã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å®Ÿè£… | [github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip) |

<details><summary>ç”¨èªé›†</summary>

| ç”¨èª | æ„å‘³ |
|:-----|:-----|
| **Dual Encoder** | ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¥ã€…ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§å‡¦ç†ã™ã‚‹æ§‹é€  |
| **Contrastive Learning** | æ­£ä¾‹ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–ã€è² ä¾‹ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–ã™ã‚‹å­¦ç¿’ |
| **InfoNCE Loss** | Noise Contrastive Estimationã«åŸºã¥ãå¯¾æ¯”æå¤± |
| **Q-Former** | BLIP-2ã®Query-based Transformerã€‚ç”»åƒç‰¹å¾´ã‚’å›ºå®šé•·ã«åœ§ç¸® |
| **Perceiver Resampler** | Flamingoã®å¯å¤‰é•·â†’å›ºå®šé•·å¤‰æ›ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« |
| **Visual Expert** | CogVLMã®è¦–è¦šå°‚ç”¨FFN |
| **Dynamic Resolution** | Qwen-VLã®å¯å¤‰è§£åƒåº¦å¯¾å¿œ |
| **Visual Instruction Tuning** | LLaVAã®Instruction-Followingè¨“ç·´æ‰‹æ³• |
| **Frozen LLM** | é‡ã¿ã‚’å›ºå®šã—ãŸLarge Language Model |
| **Modality Gap** | ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®åŸ‹ã‚è¾¼ã¿åˆ†å¸ƒã®ã‚®ãƒ£ãƒƒãƒ— |
| **Hard Negative** | é¡ä¼¼åº¦ãŒé«˜ã„è² ä¾‹ï¼ˆè­˜åˆ¥ãŒé›£ã—ã„ï¼‰ |
| **Zero-shotåˆ†é¡** | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãªã—ã§ã®åˆ†é¡ |
| **VQA** | Visual Question Answering |
| **CIDEr** | Consensus-based Image Description Evaluation |
| **SPICE** | Semantic Propositional Image Caption Evaluation |

</details>

### 6.10 çŸ¥è­˜ãƒãƒƒãƒ—ï¼ˆmermaidï¼‰

```mermaid
graph TD
    A[Vision-Language Models] --> B[ç†è«–åŸºç¤]
    A --> C[ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
    A --> D[è©•ä¾¡]

    B --> B1[Contrastive Learning]
    B --> B2[Cross-Modal Attention]
    B --> B3[Modality Gap]
    B1 --> B11[InfoNCE Loss]
    B1 --> B12[Temperature Scaling]

    C --> C1[Late Fusion]
    C --> C2[Deep Fusion]
    C --> C3[Early Fusion]
    C1 --> C11[CLIP]
    C1 --> C12[SigLIP]
    C2 --> C21[BLIP-2]
    C2 --> C22[LLaVA]
    C2 --> C23[CogVLM]
    C3 --> C31[Chameleon]

    D --> D1[VQA]
    D --> D2[Captioning]
    D --> D3[Zero-shot]
    D --> D4[Retrieval]

    style A fill:#FF6B6B
    style B fill:#4ECDC4
    style C fill:#45B7D1
    style D fill:#FFA07A
```

### 6.6 ä¸»è¦ãªå­¦ã³ï¼ˆ4ã¤ã®Takeawayï¼‰

3,000è¡Œã®é•·ã„æ—…ã ã£ãŸãŒã€ã“ã“ã¾ã§æ¥ãŸã‚ãªãŸã¯**Vision-Languageãƒ¢ãƒ‡ãƒ«ã®å…¨é ˜åŸŸ**ã‚’ç†è§£ã—ãŸã€‚

1. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« = Late/Deep/Early Fusionã®3æˆ¦ç•¥**
   - Late Fusion (CLIP): ç‹¬ç«‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ + é¡ä¼¼åº¦è¨ˆç®—
   - Deep Fusion (BLIP-2): ä¸­é–“å±¤ã§Cross-Attention
   - Early Fusion (Chameleon): å…¥åŠ›ãƒ¬ãƒ™ãƒ«ã§çµ±ä¸€Token

2. **InfoNCE lossã®æœ¬è³ª = ç›¸äº’æƒ…å ±é‡ã®ä¸‹ç•Œæœ€å¤§åŒ–**
   - æ­£ä¾‹ãƒšã‚¢ $(v_i, t_i)$ ã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–
   - è² ä¾‹ãƒšã‚¢ $(v_i, t_j)$ ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–
   - æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\tau$ ã§åˆ†å¸ƒã®é‹­ã•ã‚’åˆ¶å¾¡

3. **Vision Transformer = Self-Attentionã§ç”»åƒã‚’å‡¦ç†**
   - Patch Embedding: ç”»åƒã‚’ $P \times P$ ãƒ‘ãƒƒãƒã«åˆ†å‰²
   - Positional Encoding: 2Dä½ç½®æƒ…å ±ã‚’ä»˜ä¸
   - Global Attention: å…¨ãƒ‘ãƒƒãƒé–“ã§Attentionï¼ˆCNNã‚ˆã‚Šåºƒã„å—å®¹é‡ï¼‰

4. **å®Ÿè£…ã®ç¾å®Ÿ: ğŸ¦€Rustè¨“ç·´ + ğŸ¦€Rustæ¨è«–**
   - Rustã§CLIPè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆInfoNCE losså®Ÿè£…ï¼‰
   - Rustã§SmolVLM2æ¨è«–ï¼ˆGGUF/Candleçµ±åˆï¼‰
   - FFIçµŒç”±ã§ç›¸äº’é‹ç”¨ï¼ˆProduction-readyï¼‰


## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.7 FAQ

<details><summary>Q1: CLIPã¨BLIP-2ã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ</summary>

**A**: ã‚¿ã‚¹ã‚¯æ¬¡ç¬¬ã€‚

- **Zero-shotåˆ†é¡ãƒ»Retrieval**: CLIPï¼ˆLate Fusionï¼‰ãŒæœ€é©ã€‚è¨“ç·´ãŒç°¡å˜ã§ã€æ¨è«–ã‚‚é€Ÿã„ã€‚
- **VQAãƒ»Captioning**: BLIP-2ï¼ˆDeep Fusionï¼‰ãŒæœ€é©ã€‚Q-FormerãŒç”»åƒã®è©³ç´°ã‚’æ‰ãˆã‚‹ã€‚
- **Instruction-Following**: LLaVAã€CogVLMï¼ˆDeep Fusion + Frozen LLMï¼‰ãŒæœ€é©ã€‚

**ã‚³ã‚¹ãƒˆ vs æ€§èƒ½**:
- CLIP: è¨“ç·´ã‚³ã‚¹ãƒˆä½ã€æ¨è«–é€Ÿåº¦é€Ÿã€æ€§èƒ½ä¸­
- BLIP-2: è¨“ç·´ã‚³ã‚¹ãƒˆä¸­ã€æ¨è«–é€Ÿåº¦ä¸­ã€æ€§èƒ½é«˜
- CogVLM: è¨“ç·´ã‚³ã‚¹ãƒˆé«˜ã€æ¨è«–é€Ÿåº¦é…ã€æ€§èƒ½æœ€é«˜

</details>

<details><summary>Q2: InfoNCE lossã®æ¸©åº¦ $\tau$ ã‚’ã©ã†æ±ºã‚ã‚‹ï¼Ÿ</summary>

**A**: å®Ÿé¨“çš„ã«æ±ºå®šã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã€‚

**çµŒé¨“å‰‡**:
- $\tau = 0.07$: CLIPã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€‚ã»ã¨ã‚“ã©ã®ã‚±ãƒ¼ã‚¹ã§ã“ã‚Œã§OKã€‚
- $\tau$ ãŒå°ã•ã„ï¼ˆ0.01ã€œ0.05ï¼‰: Hard Negativeã‚’å¼·ãç½°ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå¤šæ§˜ãªã‚‰æœ‰åŠ¹ã€‚
- $\tau$ ãŒå¤§ãã„ï¼ˆ0.1ã€œ0.5ï¼‰: åˆ†å¸ƒãŒãªã ã‚‰ã‹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã«éå­¦ç¿’ã‚’é˜²ãã€‚

**è‡ªå‹•èª¿æ•´**: $\tau$ ã‚’å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã—ã¦ã€è¨“ç·´ä¸­ã«æœ€é©åŒ–ã™ã‚‹æ‰‹æ³•ã‚‚ã‚ã‚‹ï¼ˆCLIPè«–æ–‡ã§ã¯å›ºå®šï¼‰ã€‚

</details>

<details><summary>Q3: SmolVLM2-256Mã¯å®Ÿç”¨çš„ï¼Ÿ</summary>

**A**: ç”¨é€”æ¬¡ç¬¬ã ãŒã€**ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹**ã§ã¯éå¸¸ã«æœ‰åŠ¹ã€‚

**åˆ©ç‚¹**:
- æ¨è«–ãŒè¶…é«˜é€Ÿï¼ˆ1ç”»åƒ<100ms on CPUï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå°ã•ã„ï¼ˆ<1GB RAMï¼‰
- 3ãƒ¢ãƒ€ãƒªãƒ†ã‚£å¯¾å¿œï¼ˆç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªï¼‰

**æ¬ ç‚¹**:
- è¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã§ã¯å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«åŠ£ã‚‹
- Fine-tuningã®ä½™åœ°ãŒé™å®šçš„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ï¼‰

**æ¨å¥¨ç”¨é€”**: ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒèªè­˜ã€IoTãƒ‡ãƒã‚¤ã‚¹ã€‚

</details>

<details><summary>Q4: Rustã§VLMè¨“ç·´ã¯ã§ããªã„ï¼Ÿ</summary>

**A**: æŠ€è¡“çš„ã«ã¯å¯èƒ½ã ãŒã€**ç¾æ™‚ç‚¹ã§ã¯éæ¨å¥¨**ã€‚

**ç†ç”±**:
1. **è‡ªå‹•å¾®åˆ†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æœªæˆç†Ÿ**: PyTorchã‚„JAXã«æ¯”ã¹ã€Rustã®è‡ªå‹•å¾®åˆ†ï¼ˆburn, dfdxï¼‰ã¯ã¾ã ç™ºå±•é€”ä¸Šã€‚
2. **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®æ¬ å¦‚**: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€åˆ†æ•£è¨“ç·´ãƒ„ãƒ¼ãƒ«ãŒä¸è¶³ã€‚
3. **é–‹ç™ºé€Ÿåº¦**: Rustã¯å‹å®‰å…¨ã ãŒã€å®Ÿé¨“ã®åå¾©é€Ÿåº¦ã¯Rustã‚„Pythonã«åŠ£ã‚‹ã€‚

**Rustã®å½¹å‰²**: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®**æ¨è«–**ã«ç‰¹åŒ–ã€‚GGUF/Candleã§é«˜é€Ÿæ¨è«–ã‚’å®Ÿç¾ã€‚

</details>

<details><summary>Q5: ç¬¬23å›ï¼ˆFine-tuningï¼‰ã§å­¦ã¶ã“ã¨ã¯ï¼Ÿ</summary>

**A**: LoRAã€QLoRAã€Adapterãªã©ã®PEFTæŠ€è¡“ã€‚

**äºˆç¿’ãƒã‚¤ãƒ³ãƒˆ**:
- LoRAã®æ•°å¼: ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—åˆ†è§£ $W' = W + AB$ ï¼ˆ$A \in \mathbb{R}^{d \times r}$, $B \in \mathbb{R}^{r \times d}$ï¼‰
- QLoRAã®é‡å­åŒ–: 4-bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- Adapterã®æŒ¿å…¥ä½ç½®: ã©ã“ã«Adapterå±¤ã‚’å…¥ã‚Œã‚‹ã‹

ç¬¬23å›ã§ã¯ã€ã“ã‚Œã‚‰ã‚’ğŸ¦€Rustã§å®Ÿè£…ã—ã€CLIPã‚„LLaVAã‚’Fine-tuningã™ã‚‹ã€‚

</details>

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ | å†…å®¹ |
|:---|:------|:-----|:-----|
| **Day 1** | Zone 0-2 | 1æ™‚é–“ | Quick Start + ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®æ¦‚è¦ã‚’æ´ã‚€ |
| **Day 2** | Zone 3.1-3.2 | 2æ™‚é–“ | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç¤ + ViTç†è«–ã€‚æ•°å¼ã‚’ç´™ã«æ›¸ããªãŒã‚‰ç†è§£ |
| **Day 3** | Zone 3.3-3.4 | 2æ™‚é–“ | Cross-Modal Attention + InfoNCE losså°å‡ºï¼ˆBoss Battleï¼‰ |
| **Day 4** | Zone 4.1-4.2 | 2æ™‚é–“ | Rust CLIPå®Ÿè£… + ViTå®Ÿè£…ã€‚å®Ÿéš›ã«ã‚³ãƒ¼ãƒ‰ã‚’å‹•ã‹ã™ |
| **Day 5** | Zone 4.3 | 1.5æ™‚é–“ | Rust SmolVLM2æ¨è«– + FFIçµ±åˆ |
| **Day 6** | Zone 5 | 2æ™‚é–“ | è©•ä¾¡å®Ÿè£…ï¼ˆVQA/Captioning/Zero-shot/Retrievalï¼‰ |
| **Day 7** | Zone 6 | 1.5æ™‚é–“ | æŒ¯ã‚Šè¿”ã‚Š + æœ€æ–°ç ”ç©¶ã€‚å…¨ä½“ã‚’ä¿¯ç° |

**Total**: 12æ™‚é–“

### 6.9 æ¬¡ã®è¬›ç¾©ã¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼

**ç¬¬23å›: Fine-tuning & PEFT** ã§ã¯ã€ä»¥ä¸‹ã‚’å­¦ã¶:

1. **LoRA (Low-Rank Adaptation)**
   - ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—åˆ†è§£ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’1%ã«å‰Šæ¸›
   - CLIPã®Vision Encoderã«LoRAã‚’é©ç”¨

2. **QLoRA (Quantized LoRA)**
   - 4-bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’75%å‰Šæ¸›
   - LLaVA-7Bã‚’QLoRAã§Fine-tuning

3. **Adapter**
   - å„Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ã«Adapterå±¤ã‚’æŒ¿å…¥
   - Frozen LMã‚’ä¿ã¡ã¤ã¤ã€ã‚¿ã‚¹ã‚¯ç‰¹åŒ–

4. **DreamBooth**
   - ã€ŒSksã¨ã„ã†çŒ«ã€ã‚’å­¦ç¿’ã•ã›ã‚‹ï¼ˆFew-shot Personalizationï¼‰

**å®Ÿè£…è¨€èª**: ğŸ¦€Rust (LoRA/QLoRAè¨“ç·´) + ğŸ¦€Rust (é‡å­åŒ–æ¨è«–)

æº–å‚™ã¯ã„ã„ã‹ï¼Ÿ æ¬¡å›ã‚‚æ¥½ã—ã¿ã«ã—ã¦ã„ã¦ã»ã—ã„ã€‚

### 6.10 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼ï¼ˆRustå®Ÿè£…ï¼‰

```rust
use std::collections::HashMap;

// ç¬¬22å›ã®é€²æ—ã‚’è¨˜éŒ²
pub struct Progress {
    pub lecture_num: u32,
    pub zones_completed: Vec<String>,
    pub implementations: HashMap<String, bool>,
    pub evaluations: HashMap<String, f64>,
}

fn track_progress() -> Progress {
    let progress = Progress {
        lecture_num: 22,
        zones_completed: vec![
            "Zone 0", "Zone 1", "Zone 2", "Zone 3",
            "Zone 4", "Zone 5", "Zone 6", "Zone 7",
        ].into_iter().map(String::from).collect(),
        implementations: [
            ("CLIP Rust",       true),
            ("ViT Rust",        true),
            ("SmolVLM2 Rust",   true),
            ("InfoNCE Loss",    true),
            ("VQA Eval",        true),
            ("Captioning Eval", true),
            ("Zero-shot Eval",  true),
            ("Retrieval Eval",  true),
        ].iter().map(|&(k, v)| (k.to_string(), v)).collect(),
        evaluations: [
            ("InfoNCE Lossç†è§£åº¦", 0.95),
            ("CLIPå®Ÿè£…å®Œæˆåº¦",     0.90),
            ("Rustæ¨è«–æˆåŠŸç‡",     0.88),
            ("è©•ä¾¡å®Ÿè£…å®Œæˆåº¦",     0.85),
        ].iter().map(|&(k, v)| (k.to_string(), v)).collect(),
    };

    println!("=== ç¬¬{}å›é€²æ— ===", progress.lecture_num);
    println!("å®Œäº†Zone: {}", progress.zones_completed.join(", "));
    println!("\nå®Ÿè£…çŠ¶æ³:");
    for (impl_name, status) in &progress.implementations {
        println!("  {}: {}", impl_name, if *status { "âœ“" } else { "âœ—" });
    }
    println!("\nè©•ä¾¡æŒ‡æ¨™:");
    for (metric, score) in &progress.evaluations {
        println!("  {}: {:.1}%", metric, score * 100.0);
    }
    let overall: f64 = progress.evaluations.values().sum::<f64>()
        / progress.evaluations.len() as f64;
    println!("\nç·åˆç†è§£åº¦: {:.1}%", overall * 100.0);

    progress
}

// å®Ÿè¡Œ
fn main() {
    track_progress();
}
```

**å‡ºåŠ›ä¾‹**:
```
=== ç¬¬22å›é€²æ— ===
å®Œäº†Zone: Zone 0, Zone 1, Zone 2, Zone 3, Zone 4, Zone 5, Zone 6, Zone 7

å®Ÿè£…çŠ¶æ³:
  CLIP Rust: âœ“
  ViT Rust: âœ“
  SmolVLM2 Rust: âœ“
  InfoNCE Loss: âœ“
  VQA Eval: âœ“
  Captioning Eval: âœ“
  Zero-shot Eval: âœ“
  Retrieval Eval: âœ“

è©•ä¾¡æŒ‡æ¨™:
  InfoNCE Lossç†è§£åº¦: 95.0%
  CLIPå®Ÿè£…å®Œæˆåº¦: 90.0%
  Rustæ¨è«–æˆåŠŸç‡: 88.0%
  è©•ä¾¡å®Ÿè£…å®Œæˆåº¦: 85.0%

ç·åˆç†è§£åº¦: 89.5%
```

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**å•ã„**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã¯ã€Œå¿œç”¨æŠ€è¡“ã€ã§ã¯ãªãã€Œæ¨™æº–ã€ã§ã¯ï¼Ÿ

**èƒŒæ™¯**:
æˆ‘ã€…ã¯é•·ã„é–“ã€ã€Œãƒ†ã‚­ã‚¹ãƒˆã®AIã€ã€Œç”»åƒã®AIã€ã€ŒéŸ³å£°ã®AIã€ã‚’**åˆ¥ã€…ã®æŠ€è¡“**ã¨ã—ã¦æ‰±ã£ã¦ããŸã€‚ã—ã‹ã—ã€äººé–“ã®çŸ¥èƒ½ã¯**æœ¬è³ªçš„ã«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«**ã ã€‚

- å­ä¾›ã¯ã€Œã‚Šã‚“ã”ã€ã¨ã„ã†å˜èªã‚’å­¦ã¶ã¨ãã€**å®Ÿç‰©ã‚’è¦‹ãªãŒã‚‰**èãã€‚
- æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’èª­ã‚€ã¨ãã€**å†™çœŸã‚’è¦‹ãªãŒã‚‰**æ‰‹é †ã‚’ç†è§£ã™ã‚‹ã€‚
- éŸ³æ¥½ã‚’è´ãã¨ãã€**æ­Œè©ã‚’èª­ã¿ãªãŒã‚‰**æ„Ÿæƒ…ã‚’æ·±ã‚ã‚‹ã€‚

ã§ã¯ã€ãªãœAIã¯ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’**åˆ†é›¢**ã—ã¦ããŸã®ã‹ï¼Ÿ

**ç­”ãˆ**: **æŠ€è¡“çš„åˆ¶ç´„**ãŒã‚ã£ãŸã‹ã‚‰ã€‚

- 1950-1990å¹´ä»£: è¨ˆç®—è³‡æºã®åˆ¶ç´„ã§ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ç‰¹åŒ–ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã€‚
- 2000-2010å¹´ä»£: Deep Learningã®å°é ­ã§ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆCNN for Vision, RNN for Textï¼‰ã€‚
- 2020å¹´ä»£: Transformerã®ç™»å ´ã§ã€**çµ±ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ãŒå¯èƒ½ã«ã€‚

**ä»Šå¾Œã®æ–¹å‘æ€§**:

1. **ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãŒæ¨™æº–ã«ãªã‚‹** â€” å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ã¯ã€Œç‰¹æ®Šç”¨é€”ã€ã«ã€‚
2. **å…¨ã¦ã®AIãŒãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã«** â€” LLMã«ã€Œç›®ã€ã€Œè€³ã€ã€Œæ‰‹ã€ãŒä»˜ãï¼ˆGPT-4o, Gemini Ultraã®æ–¹å‘æ€§ï¼‰ã€‚
3. **æ–°ã—ã„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®çµ±åˆ** â€” è§¦è¦šã€å—…è¦šã€å‘³è¦šã‚‚AIã®å…¥åŠ›ã«ï¼Ÿ

**è­°è«–ãƒã‚¤ãƒ³ãƒˆ**:

- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãŒæ¨™æº–ã«ãªã‚‹ã¨ã€**ã©ã‚“ãªæ–°ã—ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³**ãŒç”Ÿã¾ã‚Œã‚‹ã‹ï¼Ÿ
- å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ã®**å­˜åœ¨æ„ç¾©**ã¯æ®‹ã‚‹ã‹ï¼Ÿï¼ˆä¾‹: ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®LLMï¼‰
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®**å€«ç†çš„èª²é¡Œ**ã¯ï¼Ÿï¼ˆDeepfakeã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ï¼‰

**æ­´å²çš„æ–‡è„ˆ**:

20ä¸–ç´€åˆé ­ã€**ãƒ©ã‚¸ã‚ª**ãŒç™»å ´ã—ãŸã¨ãã€äººã€…ã¯ã€ŒéŸ³å£°ã ã‘ã§ååˆ†ã€ã¨è€ƒãˆãŸã€‚ã—ã‹ã—ã€**ãƒ†ãƒ¬ãƒ“**ãŒç™»å ´ã™ã‚‹ã¨ã€æ˜ åƒã¨éŸ³å£°ã®çµ„ã¿åˆã‚ã›ãŒ**æ¨™æº–**ã«ãªã£ãŸã€‚ä»Šã€AIã‚‚åŒã˜è»¢æ›ç‚¹ã«ã„ã‚‹ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ ç¬¬22å›ã€Œãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆã€å®Œèµ°ï¼ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆã‚’å®Œå…¨ã«ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚æ¬¡ã¯Fine-tuningã§ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹ã€‚

---

> **Progress: 95%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. SigLIPãŒCLIPã®æå¤±é–¢æ•°ã‚’SigmoidåŒ–ã—ãŸæ•°å­¦çš„ãªå‹•æ©Ÿã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. Flamingoã®gated cross-attentionãŒè¦–è¦šæƒ…å ±ã‚’LLMã«æ³¨å…¥ã™ã‚‹ã¨ãã€ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ãŒé‡è¦ãªç†ç”±ã¯ä½•ã‹ï¼Ÿ

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. *International Conference on Machine Learning (ICML)*.
<https://arxiv.org/abs/2103.00020>

[^2]: van den Oord, A., Li, Y., & Vinyals, O. (2018). Representation Learning with Contrastive Predictive Coding. *arXiv preprint*.
<https://arxiv.org/abs/1807.03748>

[^3]: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *International Conference on Learning Representations (ICLR) 2021*.
<https://arxiv.org/abs/2010.11929>

[^4]: Li, J., Li, D., Savarese, S., & Hoi, S. (2023). BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. *International Conference on Machine Learning (ICML)*.
<https://arxiv.org/abs/2301.12597>

[^5]: Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., & Simonyan, K. (2022). Flamingo: a Visual Language Model for Few-Shot Learning. *Advances in Neural Information Processing Systems (NeurIPS)*.
<https://arxiv.org/abs/2204.14198>

[^6]: Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Visual Instruction Tuning. *Advances in Neural Information Processing Systems (NeurIPS)*.
<https://arxiv.org/abs/2304.08485>

[^7]: Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, C., Wang, L., Ge, Y., Song, Y., Li, H., Dang, K., Ouyang, S., Ren, X., Yan, D., Zhang, X., Qin, Y., Lin, Z., Huang, F., Liu, J., & Zhou, J. (2024). Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution. *arXiv preprint*.
<https://arxiv.org/abs/2409.12191>

[^8]: Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., & Tang, J. (2023). CogVLM: Visual Expert for Pretrained Language Models. *arXiv preprint*.
<https://arxiv.org/abs/2311.03079>

[^9]: HuggingFace (2024). SmolVLM2-256M-Instruct.
<https://huggingface.co/HuggingFaceTB/SmolVLM2-256M-Instruct>

[^11]: Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., & Jitsev, J. (2023). Reproducible scaling laws for contrastive language-image learning. *Computer Vision and Pattern Recognition (CVPR)*.
<https://arxiv.org/abs/2212.07143>

[^12]: Zhai, X., Mustafa, B., Kolesnikov, A., & Beyer, L. (2023). Sigmoid Loss for Language Image Pre-Training. *arXiv preprint*.
<https://arxiv.org/abs/2303.15343>

[^13]: Deitke, M., Clark, C., Lee, S., Tripathi, R., Yang, Y., Park, J. S., Salehi, M., Muennighoff, N., Lo, K., Soldaini, L., Lu, J., Anderson, T., Bransom, E., Ehsani, K., Ngo, H., Chen, Y. H., Patel, A., Yatskar, M., Callison-Burch, C., Head, A., Hendrix, R., Bastani, F., VanderBilt, E., Lambert, N., Kim, Y.-J., Choudhury, S., Chasins, S., & Farhadi, A. (2024). Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models. *arXiv preprint*.
<https://arxiv.org/abs/2409.17146>

[^14]: Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., & Parikh, D. (2017). Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. *Computer Vision and Pattern Recognition (CVPR)*.

[^15]: Anderson, P., Fernando, B., Johnson, M., & Gould, S. (2016). SPICE: Semantic Propositional Image Caption Evaluation. *European Conference on Computer Vision (ECCV)*.
<https://panderson.me/spice/>

[^20]: Lin, F. (2025). "Vision Language Models: A Survey of 26K Papers". *arXiv preprint*.
<https://arxiv.org/abs/2510.09586>

[^21]: Li, J., Li, D., Savarese, S., & Hoi, S. (2023). "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models". *International Conference on Machine Learning (ICML)*.
<https://arxiv.org/abs/2301.12597>

[^22]: Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2024). "Improved Baselines with Visual Instruction Tuning". *arXiv preprint*.
<https://arxiv.org/abs/2310.03744>

[^24]: Wang, Y., et al. (2022). "Multimodal Token Fusion for Vision Transformers". *Computer Vision and Pattern Recognition (CVPR)*.
<https://arxiv.org/abs/2204.08721>

[^25]: Jia, D., et al. (2024). "GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer". *Computer Vision and Pattern Recognition (CVPR)*.
<https://arxiv.org/abs/2406.01210>

[^26]: Chen, X., et al. (2024). "Heterogeneous Contrastive Learning for Foundation Models and Beyond". *arXiv preprint*.
<https://arxiv.org/abs/2404.00225>

[^27]: Mohsin, M. T., et al. (2024). "Multimodal Foundation Models for Early Disease Detection". *arXiv preprint*.
<https://arxiv.org/abs/2510.01899>

[^28]: Han, X., Chen, S., Fu, Z., Feng, Z., Fan, L., et al. (2025). "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision". *arXiv preprint*.
<https://arxiv.org/abs/2504.02477>

[^29]: Dufumier, B., et al. (2024). "What to align in multimodal contrastive learning?" *ICLR 2025*.
<https://arxiv.org/abs/2409.07402>

### æ•™ç§‘æ›¸

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [https://probml.github.io/pml-book/book2.html](https://probml.github.io/pml-book/book2.html)
- Prince, S. J. D. (2023). *Understanding Deep Learning*. MIT Press. [https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)
- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2023). *Dive into Deep Learning*. Cambridge University Press. [https://d2l.ai/](https://d2l.ai/)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
