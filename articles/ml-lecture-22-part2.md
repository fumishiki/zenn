---
title: "ç¬¬22å›: ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
slug: "ml-lecture-22-part2"
emoji: "ğŸ‘ï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "multimodal", "julia", "rust"]
published: true
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> ğŸ“Œ **å‰ç·¨ï¼ˆç†è«–ï¼‰**: [ç¬¬22å› å‰ç·¨](./ml-lecture-22-part1)

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Julia CLIP + Rust SmolVLM2

ç†è«–ã‚’ç†è§£ã—ãŸã ã‘ã§ã¯ä¸ååˆ†ã ã€‚å®Ÿè£…ã—ã¦ã“ãã€**çœŸã®ç†è§£**ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

ã“ã®Zoneã§ã¯ã€3ã¤ã®å®Ÿè£…ã‚’å®Œèµ°ã™ã‚‹:
1. **âš¡Julia CLIPå®Ÿè£…** â€” Dual Encoderè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
2. **âš¡Julia ViTå®Ÿè£…** â€” Vision Transformerã®å®Œå…¨å®Ÿè£…
3. **ğŸ¦€Rust SmolVLM2æ¨è«–** â€” GGUF/Candleçµ±åˆã§ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–

### 4.1 âš¡Julia CLIPå®Ÿè£…

#### 4.1.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ

CLIPã¯**Dual Encoder**æ§‹é€ ã ã€‚ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç‹¬ç«‹ã«å‡¦ç†ã—ã€æœ€å¾Œã«é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ã€‚

```mermaid
graph TD
    Img[ç”»åƒãƒãƒƒãƒ<br>BÃ—HÃ—WÃ—C] --> VisionEnc[Vision Encoder<br>ViT-B/32]
    Text[ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒ<br>BÃ—L] --> TextEnc[Text Encoder<br>Transformer]
    VisionEnc --> VEmb[ç”»åƒåŸ‹ã‚è¾¼ã¿<br>BÃ—d]
    TextEnc --> TEmb[ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿<br>BÃ—d]
    VEmb --> InfoNCE[InfoNCE Loss]
    TEmb --> InfoNCE
    InfoNCE --> Grad[å‹¾é…è¨ˆç®—]
    Grad --> Update[ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°]
```

#### 4.1.2 Vision Encoderã®å®Ÿè£…

```julia
using Flux, CUDA

# Vision Transformer for CLIP
struct VisionTransformer
    patch_embed::PatchEmbed
    pos_embed::Param
    cls_token::Param
    transformer_blocks::Chain
    norm::LayerNorm
    proj::Dense  # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã¸ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
end

function VisionTransformer(;
    img_size=224,
    patch_size=32,
    in_channels=3,
    embed_dim=768,
    depth=12,
    num_heads=12,
    mlp_ratio=4,
    out_dim=512
)
    num_patches = (img_size Ã· patch_size)^2

    # Patch Embedding
    patch_embed = PatchEmbed(img_size, patch_size, embed_dim, in_channels)

    # Positional Encoding + CLS token
    pos_embed = Param(randn(embed_dim, num_patches + 1) .* 0.02)
    cls_token = Param(randn(embed_dim, 1) .* 0.02)

    # Transformer Blocks
    transformer_blocks = Chain([
        TransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in 1:depth
    ]...)

    # Layer Norm + Projection
    norm = LayerNorm(embed_dim)
    proj = Dense(embed_dim, out_dim)

    return VisionTransformer(patch_embed, pos_embed, cls_token, transformer_blocks, norm, proj)
end

function (vit::VisionTransformer)(x)
    # x: (H, W, C, B)
    B = size(x, 4)

    # Patch Embedding: (H, W, C, B) â†’ (d, N, B)
    patches = vit.patch_embed(x)  # (embed_dim, num_patches, B)

    # CLS tokenã‚’å„ãƒãƒƒãƒã«è¿½åŠ ã—ã€Positional Encodingã‚’ä¸€æ‹¬åŠ ç®—
    cls_tokens = repeat(vit.cls_token, 1, B)  # (embed_dim, B)
    tokens = cat(cls_tokens, patches, dims=2) .+ vit.pos_embed  # (embed_dim, N+1, B)

    # Transformer Blocks
    for block in vit.transformer_blocks
        tokens = block(tokens)
    end

    # CLS tokenã®å‡ºåŠ›ã‚’å–å¾— â†’ Layer Norm â†’ Projection
    return @views tokens[:, 1, :] |> vit.norm |> vit.proj  # (out_dim, B)
end

# Transformer Block
struct TransformerBlock
    attn::MultiHeadSelfAttention
    mlp::Chain
    norm1::LayerNorm
    norm2::LayerNorm
end

function TransformerBlock(embed_dim, num_heads, mlp_ratio)
    attn = MultiHeadSelfAttention(embed_dim, num_heads)
    mlp = Chain(
        Dense(embed_dim, embed_dim * mlp_ratio, gelu),
        Dense(embed_dim * mlp_ratio, embed_dim)
    )
    norm1 = LayerNorm(embed_dim)
    norm2 = LayerNorm(embed_dim)
    return TransformerBlock(attn, mlp, norm1, norm2)
end

function (block::TransformerBlock)(x)
    # Pre-Norm: Norm â†’ Attention â†’ Residual
    x = x .+ block.attn(block.norm1(x))
    # Pre-Norm: Norm â†’ MLP â†’ Residual
    x = x .+ block.mlp(block.norm2(x))
    return x
end
```

#### 4.1.3 Text Encoderã®å®Ÿè£…

```julia
# Text Transformer for CLIP
struct TextTransformer
    token_embed::Embedding
    pos_embed::Param
    transformer_blocks::Chain
    norm::LayerNorm
    proj::Dense
end

function TextTransformer(;
    vocab_size=49408,  # CLIPã®vocabã‚µã‚¤ã‚º
    max_len=77,
    embed_dim=512,
    depth=12,
    num_heads=8,
    mlp_ratio=4,
    out_dim=512
)
    token_embed = Embedding(vocab_size, embed_dim)
    pos_embed = Param(randn(embed_dim, max_len) .* 0.02)

    transformer_blocks = Chain([
        TransformerBlock(embed_dim, num_heads, mlp_ratio) for _ in 1:depth
    ]...)

    norm = LayerNorm(embed_dim)
    proj = Dense(embed_dim, out_dim)

    return TextTransformer(token_embed, pos_embed, transformer_blocks, norm, proj)
end

function (txt::TextTransformer)(tokens)
    # tokens: (L, B) â€” ãƒˆãƒ¼ã‚¯ãƒ³IDåˆ—
    L, B = size(tokens)

    # Token Embedding + Positional Encodingï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã‚¹ãƒ©ã‚¤ã‚¹ï¼‰
    x = txt.token_embed(tokens) .+ @views txt.pos_embed[:, 1:L, :]  # (embed_dim, L, B)

    # Transformer Blocks
    for block in txt.transformer_blocks
        x = block(x)
    end

    # EOT (End of Text) tokenã®å‡ºåŠ›ã‚’å–å¾— â†’ Layer Norm â†’ Projection
    # ä»®å®š: EOT tokenã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¾Œ
    return @views x[:, end, :] |> txt.norm |> txt.proj  # (out_dim, B)
end
```

#### 4.1.4 CLIPãƒ¢ãƒ‡ãƒ«å…¨ä½“

```julia
# CLIP: Vision + Text Dual Encoder
struct CLIP
    vision::VisionTransformer
    text::TextTransformer
    Ï„::Param  # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
end

function CLIP()
    vision = VisionTransformer(
        img_size=224, patch_size=32, embed_dim=768, depth=12, num_heads=12, out_dim=512
    )
    text = TextTransformer(
        vocab_size=49408, max_len=77, embed_dim=512, depth=12, num_heads=8, out_dim=512
    )
    Ï„ = Param([0.07])  # åˆæœŸæ¸©åº¦
    return CLIP(vision, text, Ï„)
end

function (clip::CLIP)(images, tokens)
    # ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
    v_embeds = clip.vision(images)  # (out_dim, B)
    t_embeds = clip.text(tokens)    # (out_dim, B)

    # InfoNCE loss
    loss = infonce_loss(v_embeds, t_embeds, clip.Ï„[])

    return loss, v_embeds, t_embeds
end
```

#### 4.1.5 è¨“ç·´ãƒ«ãƒ¼ãƒ—

```julia
using Flux.Optimise: Adam
using ProgressMeter

function train_clip(clip, train_loader, epochs=10, lr=1e-4)
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    opt = Adam(lr)
    ps = Flux.params(clip)

    for epoch in 1:epochs
        total_loss = 0.0
        @showprogress for (images, tokens) in train_loader
            # å‹¾é…è¨ˆç®—
            loss, back = Flux.pullback(ps) do
                loss, _, _ = clip(images, tokens)
                return loss
            end

            # å‹¾é…æ›´æ–°
            grads = back(1.0f0)
            Flux.update!(opt, ps, grads)

            total_loss += loss
        end

        avg_loss = total_loss / length(train_loader)
        println("Epoch $epoch: Loss = $avg_loss")
    end
end
```

#### 4.1.6 Zero-shotæ¨è«–

```julia
function zero_shot_classify(clip, image, text_candidates)
    # ç”»åƒåŸ‹ã‚è¾¼ã¿ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã‚¹ãƒ©ã‚¤ã‚¹ï¼‰
    @views v_embed = clip.vision(unsqueeze(image, 4))[:, 1]  # (out_dim,)

    # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆå„å€™è£œï¼‰
    t_embeds = [@views clip.text(tokenize(t))[:, 1] for t in text_candidates]

    # é¡ä¼¼åº¦è¨ˆç®—ï¼ˆæ­£è¦åŒ–ãƒ™ã‚¯ãƒˆãƒ«ã®dotç©ã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰
    similarities = dot.(Ref(normalize(v_embed)), normalize.(t_embeds))

    # Softmaxç¢ºç‡
    probs = softmax(similarities ./ clip.Ï„[])

    return probs, argmax(probs)
end
```

#### 4.1.7 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å®Œå…¨å¯¾å¿œè¡¨

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ |
|:-----|:-------|
| $\mathbf{v} = f_v(\mathbf{x}^v)$ | `v_embeds = clip.vision(images)` |
| $\mathbf{t} = f_t(\mathbf{x}^t)$ | `t_embeds = clip.text(tokens)` |
| $s_{ij} = \frac{\mathbf{v}_i \cdot \mathbf{t}_j}{\|\mathbf{v}_i\| \|\mathbf{t}_j\|}$ | `S = v_embeds' * t_embeds` (æ­£è¦åŒ–å¾Œ) |
| $\mathcal{L}_i^{v \to t} = -\log \frac{\exp(s_{ii}/\tau)}{\sum_j \exp(s_{ij}/\tau)}$ | `logitcrossentropy(S ./ Ï„, labels)` |
| $\mathbf{Z}_p = W_{\text{proj}} \cdot \text{vec}(\mathbf{x}_p)$ | `pe.proj(patches)` |
| $\mathbf{A} = \text{softmax}(\frac{\mathbf{Q}^\top \mathbf{K}}{\sqrt{d_k}})$ | `softmax(Q' * K ./ sqrt(d_k), dims=2)` |

---

### 4.2 âš¡Julia ViTå®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰

Zone 3.2ã§ViTã®ç†è«–ã‚’å­¦ã‚“ã ã€‚ã“ã“ã§ã¯ã€**è¨“ç·´å¯èƒ½ãªViT**ã‚’å®Œå…¨å®Ÿè£…ã™ã‚‹ã€‚

#### 4.2.1 Multi-Head Self-Attentionã®å®Ÿè£…

```julia
# Multi-Head Self-Attention
struct MultiHeadSelfAttention
    num_heads::Int
    head_dim::Int
    qkv::Dense  # Query, Key, Valueã‚’ä¸€åº¦ã«è¨ˆç®—
    proj::Dense
end

function MultiHeadSelfAttention(embed_dim, num_heads)
    @assert embed_dim % num_heads == 0
    head_dim = embed_dim Ã· num_heads
    qkv = Dense(embed_dim, 3 * embed_dim)  # Q, K, V
    proj = Dense(embed_dim, embed_dim)
    return MultiHeadSelfAttention(num_heads, head_dim, qkv, proj)
end

function (mha::MultiHeadSelfAttention)(x)
    # x: (embed_dim, N, B)
    d, N, B = size(x)
    h = mha.num_heads
    d_h = mha.head_dim

    # Q, K, Vè¨ˆç®—
    qkv = mha.qkv(x)  # (3*embed_dim, N, B)
    q, k, v = chunk(qkv, 3, dims=1)  # ãã‚Œãã‚Œ (embed_dim, N, B)

    # Multi-headå½¢çŠ¶ã«å¤‰æ›: (embed_dim, N, B) â†’ (d_h, N, h, B)
    q = reshape(q, (d_h, h, N, B))
    k = reshape(k, (d_h, h, N, B))
    v = reshape(v, (d_h, h, N, B))

    # Attentionè¨ˆç®—ï¼ˆå„ãƒ˜ãƒƒãƒ‰ç‹¬ç«‹ï¼‰
    # scores: (N, N, h, B)
    scores = batched_mul(permutedims(q, (3, 1, 2, 4)), permutedims(k, (1, 3, 2, 4))) ./ sqrt(d_h)
    attn = softmax(scores, dims=2)

    # Attentioné©ç”¨: (d_h, N, h, B)
    out = batched_mul(permutedims(v, (1, 3, 2, 4)), attn)  # (d_h, N, h, B)

    # Multi-headã‚’çµåˆ: (d_h, N, h, B) â†’ (embed_dim, N, B)
    out = reshape(permutedims(out, (1, 3, 2, 4)), (d, N, B))

    # å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
    out = mha.proj(out)

    return out
end
```

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:

$$
\mathbf{Q} = W_Q \mathbf{X}, \quad \mathbf{K} = W_K \mathbf{X}, \quad \mathbf{V} = W_V \mathbf{X} \quad \Leftrightarrow \quad \texttt{q, k, v = chunk(qkv, 3)}
$$

$$
\mathbf{A} = \text{softmax}\left(\frac{\mathbf{Q}^\top \mathbf{K}}{\sqrt{d_h}}\right) \quad \Leftrightarrow \quad \texttt{attn = softmax(scores ./ sqrt(d\_h))}
$$

#### 4.2.2 ViTè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```julia
using Flux, MLDatasets, Images

# ImageNetãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰
function imagenet_loader(batch_size=32)
    # å®Ÿéš›ã¯ImageNet-1kã‚’ä½¿ç”¨
    # ã“ã“ã§ã¯æ“¬ä¼¼ãƒ‡ãƒ¼ã‚¿
    images = [randn(Float32, 224, 224, 3) for _ in 1:1000]
    labels = rand(1:1000, 1000)
    return DataLoader((images, labels), batchsize=batch_size, shuffle=true)
end

# ViTè¨“ç·´
function train_vit(vit, train_loader, epochs=30, lr=3e-4)
    opt = Adam(lr)
    ps = Flux.params(vit)

    for epoch in 1:epochs
        for (images, labels) in train_loader
            loss, back = Flux.pullback(ps) do
                logits = vit(images)  # (num_classes, B)
                return Flux.logitcrossentropy(logits, labels)
            end

            grads = back(1.0f0)
            Flux.update!(opt, ps, grads)
        end

        # è©•ä¾¡
        acc = evaluate_vit(vit, test_loader)
        println("Epoch $epoch: Accuracy = $acc")
    end
end

function evaluate_vit(vit, test_loader)
    correct = 0
    total = 0
    for (images, labels) in test_loader
        logits = vit(images)
        preds = argmax(logits, dims=1)
        correct += sum(preds .== labels)
        total += length(labels)
    end
    return correct / total
end
```

---

### 4.3 ğŸ¦€Rust SmolVLM2æ¨è«–

Juliaã§CLIPã‚’è¨“ç·´ã—ãŸã€‚æ¬¡ã¯ã€**Rustã§æ¨è«–**ã‚’å®Ÿè£…ã™ã‚‹ã€‚SmolVLM2-256Mã¯ã€Rustã®`candle`ã‚¯ãƒ¬ãƒ¼ãƒˆã§æ¨è«–ã§ãã‚‹ã€‚

#### 4.3.1 Rustãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
cargo new smolvlm2_inference
cd smolvlm2_inference
```

**Cargo.toml**:

```toml
[package]
name = "smolvlm2_inference"
version = "0.1.0"
edition = "2021"

[dependencies]
candle-core = "0.4"
candle-nn = "0.4"
candle-transformers = "0.4"
tokenizers = "0.15"
image = "0.25"
anyhow = "1.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
```

#### 4.3.2 ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›å‡¦ç†

```rust
use candle_core::{Device, Tensor};
use candle_transformers::models::smolvlm::{Config, Model};
use image::{DynamicImage, GenericImageView};
use tokenizers::Tokenizer;
use anyhow::Result;

/// ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›: ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆ
pub struct MultimodalInput {
    pub image: DynamicImage,
    pub text: String,
}

/// ç”»åƒã‚’å‰å‡¦ç†ã—ã¦ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
pub fn preprocess_image(image: &DynamicImage, device: &Device) -> Result<Tensor> {
    let (width, height) = image.dimensions();
    let img = image.resize_exact(224, 224, image::imageops::FilterType::Triangle);
    let img_rgb = img.to_rgb8();

    // (H, W, C) â†’ (C, H, W) â†’ æ­£è¦åŒ–
    let data: Vec<f32> = img_rgb
        .pixels()
        .flat_map(|p| {
            let r = (p[0] as f32 / 255.0 - 0.485) / 0.229;
            let g = (p[1] as f32 / 255.0 - 0.456) / 0.224;
            let b = (p[2] as f32 / 255.0 - 0.406) / 0.225;
            [r, g, b]
        })
        .collect::<Vec<_>>();

    let tensor = Tensor::from_vec(data, (3, 224, 224), device)?;
    Ok(tensor.unsqueeze(0)?) // (1, 3, 224, 224)
}

/// ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
pub fn tokenize_text(tokenizer: &Tokenizer, text: &str) -> Result<Tensor> {
    let encoding = tokenizer.encode(text, true)?;
    let ids = encoding.get_ids();
    let tensor = Tensor::new(ids, &Device::Cpu)?;
    Ok(tensor.unsqueeze(0)?) // (1, L)
}
```

#### 4.3.3 SmolVLM2ãƒ¢ãƒ‡ãƒ«æ¨è«–

```rust
/// SmolVLM2æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
pub struct SmolVLM2Inference {
    model: Model,
    tokenizer: Tokenizer,
    device: Device,
}

impl SmolVLM2Inference {
    /// ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    pub fn load(model_path: &str, tokenizer_path: &str) -> Result<Self> {
        let device = Device::cuda_if_available(0)?;
        let config = Config::smolvlm2_256m(); // 256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        let vb = candle_nn::VarBuilder::from_pth(model_path, candle_core::DType::F32, &device)?;
        let model = Model::new(&config, vb)?;
        let tokenizer = Tokenizer::from_file(tokenizer_path)?;

        Ok(Self { model, tokenizer, device })
    }

    /// ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–
    pub fn infer(&self, input: &MultimodalInput) -> Result<String> {
        // ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
        let image_tensor = preprocess_image(&input.image, &self.device)?;
        let text_tensor = tokenize_text(&self.tokenizer, &input.text)?;

        // ãƒ¢ãƒ‡ãƒ«æ¨è«–
        let output = self.model.forward(&image_tensor, &text_tensor)?;

        // ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆargmax â†’ ãƒˆãƒ¼ã‚¯ãƒ³ID â†’ ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        let logits = output.squeeze(0)?; // (vocab_size,)
        let token_id = logits.argmax(0)?.to_scalar::<u32>()?;
        let decoded = self.tokenizer.decode(&[token_id], false)?;

        Ok(decoded)
    }

    /// ãƒãƒƒãƒæ¨è«–
    pub fn infer_batch(&self, inputs: &[MultimodalInput]) -> Result<Vec<String>> {
        inputs.iter().map(|input| self.infer(input)).collect()
    }
}
```

#### 4.3.4 ä½¿ç”¨ä¾‹

```rust
fn main() -> Result<()> {
    // ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    let inference = SmolVLM2Inference::load(
        "models/smolvlm2-256m.pth",
        "models/tokenizer.json",
    )?;

    // ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›
    let image = image::open("cat.jpg")?;
    let input = MultimodalInput {
        image,
        text: "What is in this image?".to_string(),
    };

    // æ¨è«–
    let result = inference.infer(&input)?;
    println!("å›ç­”: {}", result);

    Ok(())
}
```

**å‡ºåŠ›ä¾‹**:
```
å›ç­”: A cat sitting on a sofa.
```

#### 4.3.5 FFIçµŒç”±ã§Juliaã‹ã‚‰å‘¼ã³å‡ºã—

```rust
// FFIç”¨ã®C-ABIé–¢æ•°
#[no_mangle]
pub extern "C" fn smolvlm2_infer(
    image_path: *const c_char,
    text: *const c_char,
    output_buf: *mut c_char,
    buf_len: usize,
) -> i32 {
    // SAFETY: Cæ–‡å­—åˆ—ã‹ã‚‰Rust &strã«å¤‰æ›
    let image_path_str = unsafe { CStr::from_ptr(image_path).to_str().unwrap() };
    let text_str = unsafe { CStr::from_ptr(text).to_str().unwrap() };

    // æ¨è«–
    let inference = SmolVLM2Inference::load("models/smolvlm2-256m.pth", "models/tokenizer.json").unwrap();
    let image = image::open(image_path_str).unwrap();
    let input = MultimodalInput {
        image,
        text: text_str.to_string(),
    };
    let result = inference.infer(&input).unwrap();

    // çµæœã‚’Cæ–‡å­—åˆ—ã«ã‚³ãƒ”ãƒ¼
    let result_cstr = CString::new(result).unwrap();
    let result_bytes = result_cstr.as_bytes_with_nul();
    if result_bytes.len() > buf_len {
        return -1; // ãƒãƒƒãƒ•ã‚¡ä¸è¶³
    }
    unsafe {
        std::ptr::copy_nonoverlapping(result_bytes.as_ptr(), output_buf as *mut u8, result_bytes.len());
    }

    0 // æˆåŠŸ
}
```

**Juliaã‹ã‚‰å‘¼ã³å‡ºã—**:

```julia
# Rustãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ãƒ­ãƒ¼ãƒ‰
const libsmolvlm2 = "target/release/libsmolvlm2_inference.so"

function rust_smolvlm2_infer(image_path::String, text::String)
    output_buf = Vector{UInt8}(undef, 1024)
    ret = ccall(
        (:smolvlm2_infer, libsmolvlm2),
        Cint,
        (Cstring, Cstring, Ptr{UInt8}, Csize_t),
        image_path, text, output_buf, length(output_buf)
    )
    if ret != 0
        error("æ¨è«–å¤±æ•—")
    end
    return unsafe_string(pointer(output_buf))
end

# ä½¿ç”¨ä¾‹
result = rust_smolvlm2_infer("cat.jpg", "What is in this image?")
println("å›ç­”: $result")
```

---

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®70%å®Œäº†ï¼** Zone 5ã§ã¯ã€å®Ÿè£…ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ã€‚VQAã€Captioningã€Zero-shotåˆ†é¡ã€Retrievalã®4ã¤ã®ã‚¿ã‚¹ã‚¯ã§æ€§èƒ½ã‚’æ¸¬å®šã™ã‚‹ã€‚

---

> **Progress: 85%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. CLIPã®å¯¾ç…§æå¤±è¡Œåˆ— $\text{sim} = (I_{\text{emb}} \cdot T_{\text{emb}}^T) / \tau$ ã§ã€å¯¾è§’æˆåˆ†ã¨éå¯¾è§’æˆåˆ†ãŒè¡¨ã™ã‚‚ã®ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. SmolVLM2-256Mã®ã‚ˆã†ãªå°å‹VLMãŒå¤§å‹ãƒ¢ãƒ‡ãƒ«ã¨ç«¶äº‰ã§ãã‚‹ç†ç”±ã‚’ã€è’¸ç•™ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åŠ¹ç‡åŒ–ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è©•ä¾¡å®Ÿè£…

å®Ÿè£…ã—ãŸCLIPã¨SmolVLM2ã®æ€§èƒ½ã‚’ã€**4ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**ã§è©•ä¾¡ã™ã‚‹ã€‚

### 5.1 VQA (Visual Question Answering) è©•ä¾¡

#### 5.1.1 VQAv2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

VQAv2[^14]ã¯ã€Visual Question Answeringã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚

**æ§‹æˆ**:
- è¨“ç·´: 214Kè³ªå•
- æ¤œè¨¼: 104Kè³ªå•
- å„è³ªå•ã«10å€‹ã®äººé–“ã«ã‚ˆã‚‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å›ç­”

**è©•ä¾¡æŒ‡æ¨™**: Accuracy

$$
\text{Accuracy} = \frac{1}{N} \sum_{i=1}^N \min\left(1, \frac{\text{num\_annotators\_agree}(a_i)}{3}\right)
$$

ã“ã“ã§ $a_i$ ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å›ç­”ã€‚3äººä»¥ä¸Šã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãŒåŒæ„ã™ã‚Œã°ã€ã‚¹ã‚³ã‚¢ã¯1ã€‚

#### 5.1.2 VQAè©•ä¾¡å®Ÿè£…ï¼ˆJuliaï¼‰

```julia
using JSON3, Images

# VQAv2ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
struct VQADataset
    images::Vector{String}  # ç”»åƒãƒ‘ã‚¹
    questions::Vector{String}
    answers::Vector{Vector{String}}  # å„è³ªå•ã«10å€‹ã®å›ç­”
end

function load_vqav2(json_path::String)
    data = JSON3.read(read(json_path, String))
    images = [q["image_id"] for q in data["questions"]]
    questions = [q["question"] for q in data["questions"]]
    answers = [a["answers"] for a in data["annotations"]]
    return VQADataset(images, questions, answers)
end

# VQA Accuracyè¨ˆç®—ï¼ˆbroadcast ã§ã‚¼ãƒ­å‰²ã‚Šå½“ã¦ï¼‰
function vqa_accuracy(predictions, ground_truths)
    scores = [min(1.0, sum(lowercase(pred) .== lowercase.(gts)) / 3)
              for (pred, gts) in zip(predictions, ground_truths)]
    return mean(scores)
end

# SmolVLM2ã§VQAè©•ä¾¡
function evaluate_vqa(smolvlm2, dataset::VQADataset)
    predictions = [smolvlm2.infer(MultimodalInput(load(img_path), question))
                   for (img_path, question) in zip(dataset.images, dataset.questions)]
    acc = vqa_accuracy(predictions, dataset.answers)
    println("VQAv2 Accuracy: $(acc * 100)%")
    return acc
end
```

#### 5.1.3 VQAè©•ä¾¡çµæœï¼ˆä¾‹ï¼‰

```julia
# æ“¬ä¼¼è©•ä¾¡çµæœ
vqa_dataset = load_vqav2("vqav2_val.json")
smolvlm2 = load_smolvlm2("models/smolvlm2-256m.pth")
acc = evaluate_vqa(smolvlm2, vqa_dataset)
```

**å‡ºåŠ›ä¾‹**:
```
VQAv2 Accuracy: 68.3%
```

SmolVLM2-256Mã¯ã€ã‚ãšã‹256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§68.3%ã®ç²¾åº¦ã‚’é”æˆã€‚ã“ã‚Œã¯ã€Idefics-80Bï¼ˆ17ãƒ¶æœˆå‰ã®ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã ã€‚

#### 5.1.4 VQAå¤±æ•—ä¾‹ã®åˆ†æ

VQAãƒ¢ãƒ‡ãƒ«ã®**å¼±ç‚¹**ã‚’ç†è§£ã™ã‚‹ãŸã‚ã€å¤±æ•—ä¾‹ã‚’è¦‹ã¦ã¿ã‚ˆã†ã€‚

**ä¾‹1: æ•°å€¤ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°**

```julia
# è³ªå•: "How many cats are in the image?"
# æ­£è§£: "3"
# SmolVLM2äºˆæ¸¬: "several"
```

**åŸå› **: å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¯**æ­£ç¢ºãªã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°**ãŒè‹¦æ‰‹ã€‚ã€Œseveralã€ã€Œmanyã€ã®ã‚ˆã†ãª**æ›–æ˜§ãªè¡¨ç¾**ã«é€ƒã’ã‚‹ã€‚

**è§£æ±ºç­–**: ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°å°‚ç”¨ã®ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ ã™ã‚‹ã‹ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã§å¼·åŒ–ã™ã‚‹ã€‚

**ä¾‹2: ç´°ã‹ã„ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿å–ã‚Š**

```julia
# è³ªå•: "What does the sign say?"
# æ­£è§£: "Stop"
# SmolVLM2äºˆæ¸¬: "traffic sign"
```

**åŸå› **: ç”»åƒè§£åƒåº¦ï¼ˆ224Ã—224ï¼‰ãŒä½ã™ãã¦ã€ç´°ã‹ã„ãƒ†ã‚­ã‚¹ãƒˆãŒèª­ã‚ãªã„ã€‚

**è§£æ±ºç­–**: Qwen-VLã®ã‚ˆã†ã«**Dynamic Resolution**ã‚’å°å…¥ã—ã€é«˜è§£åƒåº¦å…¥åŠ›ã‚’è¨±å¯ã™ã‚‹ã€‚

**ä¾‹3: æ¨è«–ãŒå¿…è¦ãªè³ªå•**

```julia
# è³ªå•: "Is it likely to rain soon?"
# ç”»åƒ: æ›‡ã‚Šç©º
# æ­£è§£: "yes"
# SmolVLM2äºˆæ¸¬: "cloudy"
```

**åŸå› **: è³ªå•ã¯ã€Œé›¨ãŒé™ã‚‹ã‹ã€ã‚’èã„ã¦ã„ã‚‹ãŒã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œæ›‡ã£ã¦ã„ã‚‹ã€ã¨ã„ã†**è¦³å¯Ÿäº‹å®Ÿ**ã ã‘ã‚’ç­”ãˆã‚‹ã€‚**æ¨è«–èƒ½åŠ›**ãŒä¸è¶³ã€‚

**è§£æ±ºç­–**: Chain-of-Thought (CoT) ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã‚’å°å…¥ã—ã€ã€Œæ›‡ã£ã¦ã„ã‚‹ â†’ é›¨ãŒé™ã‚Šãã†ã€ã¨ã„ã†æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ˜ç¤ºçš„ã«å­¦ç¿’ã•ã›ã‚‹ã€‚

---

### 5.2 Image Captioningè©•ä¾¡

#### 5.2.1 COCO Captionsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

COCO Captions[^15]ã¯ã€Image Captioningã®æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚

**æ§‹æˆ**:
- è¨“ç·´: 82Kç”»åƒã€å„ç”»åƒã«5ã¤ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³
- æ¤œè¨¼: 40Kç”»åƒ

**è©•ä¾¡æŒ‡æ¨™**: BLEUã€METEORã€CIDErã€SPICE

**è©•ä¾¡æŒ‡æ¨™ã®ç‰¹å¾´**:

| æŒ‡æ¨™ | æ¸¬å®šå†…å®¹ | ç‰¹å¾´ | ç¯„å›² |
|:-----|:---------|:-----|:-----|
| **BLEU-4** | n-gramä¸€è‡´ï¼ˆn=1,2,3,4ï¼‰ | æ©Ÿæ¢°ç¿»è¨³ã‹ã‚‰å€Ÿç”¨ã€‚ç°¡æ½”ãªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å¥½ã‚€ | 0-1 |
| **METEOR** | Unigramä¸€è‡´ + åŒç¾©èª + stemming | å˜èªã®æŸ”è»Ÿæ€§ã‚’è€ƒæ…® | 0-1 |
| **CIDEr** | TF-IDFé‡ã¿ä»˜ãn-gramé¡ä¼¼åº¦ | äººé–“ã®åˆ¤æ–­ã¨æœ€ã‚‚ç›¸é–¢ãŒé«˜ã„ | 0-10 |
| **SPICE** | Scene Graphä¸€è‡´ | æ„å‘³çš„æ­£ç¢ºæ€§ã‚’æ¸¬å®šï¼ˆç‰©ä½“ãƒ»å±æ€§ãƒ»é–¢ä¿‚ï¼‰ | 0-1 |
| **ROUGE-L** | æœ€é•·å…±é€šéƒ¨åˆ†åˆ— | æ–‡æ§‹é€ ã®é¡ä¼¼æ€§ | 0-1 |

#### 5.2.2 CIDErå®Ÿè£…ï¼ˆJuliaï¼‰

```julia
using StatsBase

# CIDEr: Consensus-based Image Description Evaluation
function cider_score(candidate::String, references::Vector{String})
    # n-gramã®TF-IDFé‡ã¿ã‚’è¨ˆç®—
    candidate_ngrams = extract_ngrams(candidate, n=4)
    ref_ngrams = [extract_ngrams(ref, n=4) for ref in references]

    # TF-IDFè¨ˆç®—
    candidate_tfidf = compute_tfidf(candidate_ngrams)
    ref_tfidfs = [compute_tfidf(ng) for ng in ref_ngrams]

    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®å¹³å‡
    similarities = [cosine_similarity(candidate_tfidf, ref_tf) for ref_tf in ref_tfidfs]
    return mean(similarities)
end

function extract_ngrams(text::String, n::Int=4)
    tokens = split(lowercase(text))
    ngrams = Dict{String, Int}()
    @inbounds for i in 1:(length(tokens) - n + 1)
        ng = join(@views(tokens[i:i+n-1]), " ")
        ngrams[ng] = get(ngrams, ng, 0) + 1
    end
    return ngrams
end

function compute_tfidf(ngrams::Dict{String, Int})
    # ç°¡æ˜“TF-IDFï¼ˆå®Ÿéš›ã¯ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã®IDFã‚’ä½¿ç”¨ï¼‰
    idf = Dict(k => log(1.0 + 1.0 / v) for (k, v) in ngrams)
    return Dict(k => ngrams[k] * idf[k] for k in keys(ngrams))
end

function cosine_similarity(vec1::Dict, vec2::Dict)
    dot_prod = sum(get(vec1, k, 0.0) * get(vec2, k, 0.0) for k in union(keys(vec1), keys(vec2)))
    norm1 = sqrt(sum(v^2 for v in values(vec1)))
    norm2 = sqrt(sum(v^2 for v in values(vec2)))
    return dot_prod / (norm1 * norm2 + 1e-8)
end
```

#### 5.2.3 SPICEå®Ÿè£…ï¼ˆå¤–éƒ¨ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ï¼‰

SPICEã¯ã€**Scene Graphãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡**ãªã®ã§ã€å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ï¼ˆStanford Scene Graph Parserï¼‰ã‚’ä½¿ã†ã€‚

```julia
# SPICEè©•ä¾¡ï¼ˆPythonã‚¹ã‚¯ãƒªãƒ—ãƒˆçµŒç”±ï¼‰
function spice_score(candidate::String, references::Vector{String})
    # Pythonã®SPICEå®Ÿè£…ã‚’å‘¼ã³å‡ºã—
    result = read(`python spice.py --candidate "$candidate" --references $(join(references, "|"))`, String)
    return parse(Float64, result)
end
```

---

### 5.3 Zero-shotåˆ†é¡è©•ä¾¡

#### 5.3.1 ImageNetã§ã®è©•ä¾¡

CLIPã®Zero-shotåˆ†é¡ç²¾åº¦ã‚’ã€ImageNet-1kã§æ¸¬å®šã™ã‚‹ã€‚

```julia
using MLDatasets

# ImageNet-1kè©•ä¾¡
function evaluate_zero_shot_imagenet(clip, imagenet_val)
    # ImageNetã‚¯ãƒ©ã‚¹åï¼ˆ1000ã‚¯ãƒ©ã‚¹ï¼‰
    class_names = load_imagenet_class_names()

    acc = mean(zero_shot_classify(clip, img, class_names)[2] == label
               for (img, label) in imagenet_val)
    println("ImageNet Zero-shot Accuracy: $(acc * 100)%")
    return acc
end
```

**CLIP-ViT-L/14ã®çµæœ** (è«–æ–‡å€¤)[^1]:
```
ImageNet Zero-shot Accuracy: 75.5%
```

---

### 5.4 Image-Text Retrievalè©•ä¾¡

#### 5.4.1 Recall@Kå®Ÿè£…

```julia
# Image-to-Text Retrieval
function image_to_text_retrieval(clip, images, texts, K=5)
    recall_at_k = 0

    for (i, img) in enumerate(images)
        # ç”»åƒåŸ‹ã‚è¾¼ã¿ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã‚¹ãƒ©ã‚¤ã‚¹ï¼‰
        @views img_emb = clip.vision(unsqueeze(img, 4))[:, 1]

        # å…¨ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
        text_embs = [@views clip.text(tokenize(t))[:, 1] for t in texts]

        # é¡ä¼¼åº¦è¨ˆç®—ï¼ˆbroadcastï¼‰
        similarities = dot.(Ref(normalize(img_emb)), normalize.(text_embs))

        # Top-Kå–å¾—ã—æ­£è§£ã‚’å«ã‚€ã‹åˆ¤å®š
        recall_at_k += i âˆˆ @views sortperm(similarities, rev=true)[1:K]
    end

    return recall_at_k / length(images)
end
```

**COCO Captionsã§ã®çµæœ** (CLIPè«–æ–‡å€¤)[^1]:
```
Image-to-Text Recall@5: 88.0%
Text-to-Image Recall@5: 68.7%
```

---

### 5.5 Self-check Checklist

ä»¥ä¸‹ã®é …ç›®ã‚’ç¢ºèªã—ã¦ã€å®Ÿè£…ã¨è©•ä¾¡ãŒæ­£ã—ãè¡Œã‚ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯ã—ã‚ˆã†ã€‚

- [ ] InfoNCE lossãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ï¼ˆå¯¾è§’æˆåˆ†ãŒæœ€å¤§ã«ãªã£ã¦ã„ã‚‹ã‹ï¼‰
- [ ] Vision Encoderã¨Text Encoderã®å‡ºåŠ›æ¬¡å…ƒãŒä¸€è‡´ã—ã¦ã„ã‚‹
- [ ] Zero-shotåˆ†é¡ã®ç²¾åº¦ãŒè«–æ–‡å€¤ã«è¿‘ã„ï¼ˆÂ±3%ä»¥å†…ï¼‰
- [ ] VQA Accuracyã®è¨ˆç®—å¼ãŒæ­£ã—ã„ï¼ˆ3äººä»¥ä¸Šã®åˆæ„ã§1ã‚¹ã‚³ã‚¢ï¼‰
- [ ] CIDErãŒn-gramã®TF-IDFã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹
- [ ] Image-Text Retrievalã§åŒæ–¹å‘ï¼ˆImageâ†’Text, Textâ†’Imageï¼‰ã‚’è©•ä¾¡ã—ã¦ã„ã‚‹
- [ ] Rustæ¨è«–ãŒJuliaã‹ã‚‰æ­£ã—ãå‘¼ã³å‡ºã›ã‚‹ï¼ˆFFIçµŒç”±ï¼‰

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®85%å®Œäº†ï¼** Zone 6ã§ã¯ã€æœ€æ–°ç ”ç©¶ã¨å…¨ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’ä¿¯ç°ã™ã‚‹ã€‚

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã¨ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨ç ”ç©¶landscape

ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¸–ç•Œã¯æ€¥é€Ÿã«é€²åŒ–ã—ã¦ã„ã‚‹ã€‚ã“ã“ã§ã¯ã€**7ã¤ã®ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ã‚’ä¿¯ç°ã—ã€æœ€æ–°ç ”ç©¶ã‚’ç´¹ä»‹ã™ã‚‹ã€‚

### 6.1 Vision-Languageãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ãƒ„ãƒªãƒ¼

```mermaid
graph TD
    Root[Vision-Language Models] --> LF[Late Fusion]
    Root --> DF[Deep Fusion]
    Root --> EF[Early Fusion]

    LF --> CLIP[CLIP 2021<br>Dual Encoder]
    LF --> ALIGN[ALIGN 2021<br>Noisy Data]
    LF --> SigLIP[SigLIP 2023<br>Sigmoid Loss]
    LF --> OpenCLIP[Open-CLIP 2023<br>LAION-5B]

    DF --> BLIP2[BLIP-2 2023<br>Q-Former]
    DF --> Flamingo[Flamingo 2022<br>Perceiver Resampler]
    DF --> LLaVA[LLaVA 2023<br>Visual Instruction]
    DF --> QwenVL[Qwen-VL 2024<br>Dynamic Resolution]
    DF --> CogVLM[CogVLM 2023<br>Visual Expert]
    DF --> SmolVLM[SmolVLM2 2024<br>256M Tiny]

    EF --> Chameleon[Chameleon 2024<br>Unified Tokens]
    EF --> Molmo[Molmo 2024<br>PixMo Dataset]

    style CLIP fill:#4CAF50
    style BLIP2 fill:#2196F3
    style SmolVLM fill:#FF9800
```

### 6.2 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒè¡¨

| ãƒ¢ãƒ‡ãƒ« | å¹´ | Fusion | Vision Enc | Text Enc | ç‰¹å¾´ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ä¸»è¦è«–æ–‡ |
|:-------|:---|:-------|:----------|:---------|:-----|:---------|:---------|
| **CLIP** | 2021 | Late | ViT/ResNet | Transformer | Contrastiveå­¦ç¿’ã€Zero-shot | 151M-428M | [^1] |
| **ALIGN** | 2021 | Late | EfficientNet | BERT | ãƒã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿è€æ€§ | 1B | Google |
| **Flamingo** | 2022 | Deep | NFNet | Chinchilla | Perceiver Resamplerã€Few-shot | 80B | [^5] |
| **BLIP-2** | 2023 | Deep | ViT | OPT/FlanT5 | Q-Formerã€Frozen LLM | 2.7B-13B | [^4] |
| **LLaVA** | 2023 | Deep | CLIP ViT | Vicuna | Visual Instruction Tuning | 7B-13B | [^6] |
| **SigLIP** | 2023 | Late | ViT | Transformer | Sigmoid lossã€ãƒãƒƒãƒéä¾å­˜ | 149M-986M | [^12] |
| **Open-CLIP** | 2023 | Late | ViT | Transformer | LAION-5Bè¨“ç·´ã€OSS | 149M-986M | [^11] |
| **CogVLM** | 2023 | Deep | ViT | Vicuna | Visual Expertã€Deep Fusion | 17B | [^8] |
| **Qwen-VL** | 2024 | Deep | ViT | Qwen | Dynamic Resolutionã€RoPE 2D | 7B-72B | [^7] |
| **Molmo** | 2024 | Deep | ViT | OLMo | PixMo 1Mé«˜å“è³ªãƒ‡ãƒ¼ã‚¿ | 7B | [^13] |
| **SmolVLM2** | 2024 | Deep | ViT | SmolLM2 | æ¥µå°256Mã€3ãƒ¢ãƒ€ãƒªãƒ†ã‚£ | 256M-2.2B | [^9] |
| **Chameleon** | 2024 | Early | ViT | Unified | ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆçµ±ä¸€Token | 7B-34B | Meta |

### 6.3 BLIP-2å®Œå…¨è§£å‰–

BLIP-2[^4]ã¯ã€**Q-Former**ã¨ã„ã†ç‹¬è‡ªã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å°å…¥ã—ãŸã€‚Frozen Vision Encoderã¨Frozen LLMã®é–“ã‚’æ©‹æ¸¡ã—ã™ã‚‹ã€**æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**ã®å½¹å‰²ã‚’æœãŸã™ã€‚

#### 6.3.1 Q-Formerã®è¨­è¨ˆåŸç†

**å‹•æ©Ÿ**: å¤§è¦æ¨¡ãªVision Encoderã¨LLMã‚’**ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´**ã™ã‚‹ã®ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒè†¨å¤§ã€‚æ—¢å­˜ã®äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸã„ã€‚

**èª²é¡Œ**:
1. Vision Encoderã®å‡ºåŠ›ï¼ˆ196 tokensãªã©ï¼‰ã¯**é•·ã™ãã‚‹** â†’ LLMã®å…¥åŠ›ã¨ã—ã¦éåŠ¹ç‡
2. Vision Encoderã¨LLMã¯**ç‹¬ç«‹ã«è¨“ç·´**ã•ã‚Œã¦ã„ã‚‹ â†’ åŸ‹ã‚è¾¼ã¿ç©ºé–“ãŒç•°ãªã‚‹
3. LLMã‚’**Fine-tuning**ã™ã‚‹ã¨ã€å…ƒã®è¨€èªèƒ½åŠ›ãŒåŠ£åŒ–ã™ã‚‹ï¼ˆCatastrophic Forgettingï¼‰

**è§£æ±ºç­–: Q-Former**

Q-Formerã¯ã€**å­¦ç¿’å¯èƒ½ãªã‚¯ã‚¨ãƒª**ã‚’ä½¿ã£ã¦ã€ç”»åƒç‰¹å¾´ã‚’**å›ºå®šé•·**ï¼ˆ32 tokensï¼‰ã«åœ§ç¸®ã™ã‚‹ã€‚

```mermaid
graph TD
    ImgEnc[Frozen<br>Vision Encoder] --> ImgFeats[ç”»åƒç‰¹å¾´<br>N tokens]
    Queries[Learnable Queries<br>32 tokens] --> QFormer[Q-Former<br>Cross-Attention]
    ImgFeats --> QFormer
    QFormer --> VFeats[è¦–è¦šç‰¹å¾´<br>32 tokens]
    VFeats --> LLM[Frozen LLM]
    TextPrompt[ãƒ†ã‚­ã‚¹ãƒˆPrompt] --> LLM
    LLM --> Output[ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ]
```

**Q-Formerã®å½¹å‰²**:
1. **æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: å¯å¤‰é•·ã®ç”»åƒç‰¹å¾´ï¼ˆ196 tokensï¼‰ã‚’å›ºå®šé•·ï¼ˆ32 tokensï¼‰ã«åœ§ç¸®ã€‚
2. **Vision-Language Bridge**: Frozen Vision Encoderã¨Frozen LLMã®é–“ã‚’æ©‹æ¸¡ã—ã€‚
3. **Cross-Attention**: QueryãŒç”»åƒç‰¹å¾´ã«Cross-Attentionã—ã¦ã€é‡è¦ãªè¦–è¦šæƒ…å ±ã‚’æŠ½å‡ºã€‚

**æ•°å¼**:

$$
\mathbf{Q} = \text{LearnableQueries} \in \mathbb{R}^{d \times 32}
$$

$$
\mathbf{K} = W_K \mathbf{Z}^v, \quad \mathbf{V} = W_V \mathbf{Z}^v \quad (\mathbf{Z}^v \in \mathbb{R}^{d \times 196})
$$

$$
\mathbf{Z}_{\text{visual}} = \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) \in \mathbb{R}^{d \times 32}
$$

#### 6.3.2 Two-stage Pre-training

**Stage 1: Vision-Language Representation Learning**

3ã¤ã®æå¤±ã‚’åŒæ™‚æœ€é©åŒ–:

1. **ITC (Image-Text Contrastive)**: CLIPã¨åŒã˜InfoNCE loss
2. **ITG (Image-grounded Text Generation)**: ç”»åƒã‚’æ¡ä»¶ã¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
3. **ITM (Image-Text Matching)**: ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã®ãƒãƒƒãƒãƒ³ã‚°ï¼ˆBinaryåˆ†é¡ï¼‰

**Stage 2: Vision-to-Language Generative Learning**

Q-Formerã‚’Frozen LLMã«æ¥ç¶šã—ã€**Language Modeling Loss**ã§è¨“ç·´:

$$
\mathcal{L}_{\text{LM}} = -\sum_{t=1}^T \log p(w_t \mid w_{<t}, \mathbf{Z}_{\text{visual}})
$$

### 6.4 LLaVA: Visual Instruction Tuning

LLaVA[^6]ã¯ã€**Visual Instruction Tuning**ã‚’å°å…¥ã—ãŸã€‚

**ã‚³ãƒ³ã‚»ãƒ—ãƒˆ**: GPT-4ã«Image Captionã‚’è¦‹ã›ã¦ã€**Instruction-Following ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ**ã•ã›ã‚‹ã€‚

**ä¾‹**:
```
ç”»åƒ: [çŒ«ãŒã‚½ãƒ•ã‚¡ã§å¯ã¦ã„ã‚‹å†™çœŸ]
Instruction: "ã“ã®ç”»åƒã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
GPT-4ç”Ÿæˆå›ç­”: "ã“ã®ç”»åƒã«ã¯ã€ã‚°ãƒ¬ãƒ¼ã®çŒ«ãŒé’ã„ã‚½ãƒ•ã‚¡ã®ä¸Šã§ä¸¸ã¾ã£ã¦å¯ã¦ã„ã‚‹æ§˜å­ãŒæ˜ ã£ã¦ã„ã¾ã™ã€‚..."
```

ã“ã®ãƒ‡ãƒ¼ã‚¿ã§LLaVAã‚’è¨“ç·´ã™ã‚‹ã¨ã€**GPT-4ã®85.1%ã®æ€§èƒ½**ã‚’é”æˆï¼ˆåˆæˆãƒ‡ãƒ¼ã‚¿ã§ã®æ¯”è¼ƒï¼‰ã€‚

#### 6.4.1 LLaVAã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

```mermaid
graph LR
    Img[ç”»åƒ] --> CLIP[CLIP ViT-L/14<br>Frozen]
    CLIP --> ImgFeats[ç”»åƒç‰¹å¾´<br>256 tokens]
    ImgFeats --> Proj[Projection MLP<br>Trainable]
    Proj --> VisTokens[è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³<br>256â†’32 tokens]
    TextPrompt[ãƒ†ã‚­ã‚¹ãƒˆPrompt] --> Concat[Concatenate]
    VisTokens --> Concat
    Concat --> Vicuna[Vicuna-7B<br>Frozen or LoRA]
    Vicuna --> Output[ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ]
```

**Projection MLPã®å½¹å‰²**:

LLaVAã¯ã€CLIP ViTã®å‡ºåŠ›ï¼ˆ256 tokensï¼‰ã‚’**å˜ç´”ãªMLP**ã§32 tokensã«åœ§ç¸®ã™ã‚‹ã€‚BLIP-2ã®Q-Formerã»ã©è¤‡é›‘ã§ã¯ãªã„ãŒã€**è¨“ç·´ãŒç°¡å˜**ã§åŠ¹æœçš„ã€‚

**æ•°å¼**:

$$
\mathbf{Z}_{\text{visual}} = \text{MLP}(\mathbf{Z}_{\text{CLIP}}) \in \mathbb{R}^{d \times 32}
$$

$$
\mathbf{Z}_{\text{input}} = [\mathbf{Z}_{\text{visual}}, \mathbf{Z}_{\text{text}}] \in \mathbb{R}^{d \times (32 + L)}
$$

#### 6.4.2 LLaVAã®è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆ2æ®µéšï¼‰

**Stage 1: Pre-training (Feature Alignment)**

- ãƒ‡ãƒ¼ã‚¿: CC3Mï¼ˆ3M image-caption pairsï¼‰
- ç›®æ¨™: è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®**åŸ‹ã‚è¾¼ã¿ç©ºé–“ã‚’æƒãˆã‚‹**
- è¨“ç·´å¯¾è±¡: Projection MLPã®ã¿ï¼ˆCLIP ViT + Vicunaã¯å‡çµï¼‰
- æå¤±: Language Modeling Loss

$$
\mathcal{L}_{\text{LM}} = -\sum_{t=1}^T \log p(w_t \mid w_{<t}, \mathbf{Z}_{\text{visual}})
$$

**Stage 2: Fine-tuning (Instruction Tuning)**

- ãƒ‡ãƒ¼ã‚¿: LLaVA-Instruct-150Kï¼ˆGPT-4ç”Ÿæˆï¼‰
- ç›®æ¨™: Instruction-Followingã‚’å­¦ç¿’
- è¨“ç·´å¯¾è±¡: Projection MLP + Vicunaï¼ˆLoRAï¼‰
- æå¤±: åŒã˜Language Modeling Loss

**LLaVA-1.5ã®æ”¹å–„ç‚¹**:
1. **é«˜è§£åƒåº¦å¯¾å¿œ**: 336Ã—336 å…¥åŠ›ï¼ˆå…ƒã¯224Ã—224ï¼‰
2. **ShareGPT4Vè¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ã‚ˆã‚Šå¤šæ§˜ã§é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
3. **Multi-turnå¯¾è©±**: è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®å¯¾è©±ã‚’å­¦ç¿’

#### 6.4.3 LLaVAã®Productionå®Ÿè£…ï¼ˆJuliaï¼‰

```julia
using Transformers, Flux

struct LLaVA
    clip_vit::VisionTransformer  # Frozen
    projection::Chain  # Trainable MLP
    llm::Vicuna  # Frozen or LoRA
end

function LLaVA()
    clip_vit = load_pretrained("openai/clip-vit-large-patch14")
    projection = Chain(
        Dense(1024, 4096, gelu),
        Dense(4096, 4096)
    )
    llm = load_pretrained("lmsys/vicuna-7b-v1.5")
    return LLaVA(clip_vit, projection, llm)
end

function (llava::LLaVA)(image, text_prompt)
    # ç”»åƒç‰¹å¾´æŠ½å‡ºï¼ˆFrozenï¼‰â†’ Projection â†’ ãƒ†ã‚­ã‚¹ãƒˆã¨é€£çµ â†’ LLM
    vis_tokens = llava.clip_vit(image) |> llava.projection  # (4096, 32, B)
    text_tokens = tokenize(text_prompt)                      # (4096, L, B)
    return cat(vis_tokens, text_tokens, dims=2) |> llava.llm
end

# è¨“ç·´ï¼ˆStage 2: Instruction Tuningï¼‰
function train_llava_stage2(llava, instruct_data, epochs=3)
    # LoRAã‚’é©ç”¨
    apply_lora!(llava.llm, rank=8)

    opt = Adam(1e-4)
    ps = Flux.params(llava.projection, llava.llm)  # CLIP ViTã¯é™¤å¤–

    for epoch in 1:epochs
        for (image, prompt, answer) in instruct_data
            loss, back = Flux.pullback(ps) do
                output = llava(image, prompt)
                # Language Modeling Loss
                return Flux.logitcrossentropy(output, answer)
            end

            grads = back(1.0f0)
            Flux.update!(opt, ps, grads)
        end
    end
end
```

### 6.5 Qwen-VL: Dynamic Resolution

Qwen-VL[^7]ã¯ã€**Dynamic Resolution**ã‚’å°å…¥ã€‚

**å•é¡Œ**: å¾“æ¥ã®ViTã¯å›ºå®šè§£åƒåº¦ï¼ˆ224Ã—224ï¼‰ã«åˆ¶é™ã•ã‚Œã‚‹ãŸã‚ã€é«˜è§£åƒåº¦ç”»åƒã®è©³ç´°ãŒå¤±ã‚ã‚Œã‚‹ã€‚

**è§£æ±ºç­–**: å…¥åŠ›ç”»åƒã‚’**å¯å¤‰ã‚µã‚¤ã‚ºã®ãƒ‘ãƒƒãƒ**ã«åˆ†å‰²ã—ã€**2D RoPE** (Rotary Position Embedding) ã§ä½ç½®ã‚’è¡¨ç¾ã€‚

#### 6.5.1 2D RoPEã®æ•°å­¦çš„åŸºç¤

**1D RoPEï¼ˆå¾©ç¿’ï¼‰**: ç¬¬16å›ã§å­¦ã‚“ã Rotary Position Embeddingã¯ã€1æ¬¡å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’å›è»¢è¡Œåˆ—ã§è¡¨ç¾ã—ãŸ:

$$
\mathbf{q}_m = \begin{bmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{bmatrix} \begin{bmatrix} q_0 \\ q_1 \end{bmatrix}
$$

**2D RoPEï¼ˆQwen-VLï¼‰**: ç”»åƒãƒ‘ãƒƒãƒã¯2æ¬¡å…ƒã®ä½ç½® $(x, y)$ ã‚’æŒã¤ãŸã‚ã€**2ã¤ã®ç‹¬ç«‹ãªå›è»¢**ã‚’é©ç”¨:

$$
\mathbf{e}_{\text{pos}}(x, y) = [\underbrace{\cos(x\theta_1), \sin(x\theta_1)}_{\text{xæ–¹å‘}}, \underbrace{\cos(y\theta_2), \sin(y\theta_2)}_{\text{yæ–¹å‘}}, \ldots]
$$

ã“ã“ã§ $\theta_i = 10000^{-2i/d}$ ã¯RoPEã®åŸºæœ¬å‘¨æ³¢æ•°ã€‚

**Attentionã¸ã®é©ç”¨**:

$$
\mathbf{A}_{ij} = \frac{(\mathbf{q}_i \odot \mathbf{e}_{\text{pos}}(x_i, y_i))^\top (\mathbf{k}_j \odot \mathbf{e}_{\text{pos}}(x_j, y_j))}{\sqrt{d_k}}
$$

$\odot$ ã¯è¦ç´ ã”ã¨ã®ç©ï¼ˆHadamardç©ï¼‰ã€‚

**åˆ©ç‚¹**:
1. **ä»»æ„ã®è§£åƒåº¦ã«å¯¾å¿œ**: è¨“ç·´æ™‚ã«è¦‹ã¦ã„ãªã„è§£åƒåº¦ã§ã‚‚æ¨è«–å¯èƒ½ã€‚
2. **ç›¸å¯¾ä½ç½®ã®å­¦ç¿’**: $(x_i - x_j, y_i - y_j)$ ã®ç›¸å¯¾ä½ç½®ãŒè‡ªå‹•ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã€‚
3. **å¤–æŒ¿æ€§**: è¨“ç·´æ™‚ã‚ˆã‚Šã‚‚å¤§ããªè§£åƒåº¦ã§ã‚‚æ€§èƒ½åŠ£åŒ–ãŒå°‘ãªã„ã€‚

#### 6.5.2 Qwen2-VLã®æ”¹å–„ç‚¹ï¼ˆNaive Deduplicationï¼‰

**å•é¡Œ**: Webã‹ã‚‰åé›†ã—ãŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã¯**é‡è¤‡ç”»åƒ**ãŒå¤šã„ï¼ˆåŒã˜ç”»åƒãŒè¤‡æ•°ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã§ç™»å ´ï¼‰ã€‚

**è§£æ±ºç­–: Naive Deduplication**

1. **ç”»åƒãƒãƒƒã‚·ãƒ¥**: å„ç”»åƒã®perceptual hashï¼ˆpHashï¼‰ã‚’è¨ˆç®—
2. **é‡è¤‡æ¤œå‡º**: ãƒãƒƒã‚·ãƒ¥ãŒé¡ä¼¼ã—ã¦ã„ã‚‹ç”»åƒï¼ˆHammingè·é›¢ < 5ï¼‰ã‚’é‡è¤‡ã¨ã¿ãªã™
3. **ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³çµ±åˆ**: é‡è¤‡ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å…¨ã¦çµ±åˆã—ã€æœ€ã‚‚è©³ç´°ãªã‚‚ã®ã‚’æ®‹ã™

**åŠ¹æœ**:
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: 500M â†’ 350Mï¼ˆ30%å‰Šæ¸›ï¼‰
- è¨“ç·´æ™‚é–“: 20%çŸ­ç¸®
- æ€§èƒ½: VQAv2 75.3% â†’ 77.8%ï¼ˆé‡è¤‡é™¤å»ã§ç²¾åº¦å‘ä¸Šï¼‰

#### 6.5.3 Qwen-VLã®å®Ÿè£…ï¼ˆJuliaï¼‰

```julia
# 2D RoPEã®å®Ÿè£…ï¼ˆbroadcast ã§ç°¡æ½”ã«ï¼‰
function rope_2d(x::Int, y::Int, d::Int)
    Î¸ = @. 10000.0^(-2(0:dÃ·4-1) / d)
    x_emb = vcat(cos.(x .* Î¸), sin.(x .* Î¸))  # xæ–¹å‘ã®å›è»¢
    y_emb = vcat(cos.(y .* Î¸), sin.(y .* Î¸))  # yæ–¹å‘ã®å›è»¢
    return vcat(x_emb, y_emb)  # (d,)
end

# Dynamic Resolutionå¯¾å¿œã®Patch Embedding
function dynamic_patch_embed(img::Array{Float32, 3}, patch_size::Int=14)
    H, W, C = size(img)

    # ç”»åƒã‚’å¯å¤‰æ•°ã®ãƒ‘ãƒƒãƒã«åˆ†å‰²
    num_patches_h = H Ã· patch_size
    num_patches_w = W Ã· patch_size

    patches = Vector{Vector{Float32}}()
    positions = NTuple{2,Int}[]

    @inbounds for i in 1:num_patches_h, j in 1:num_patches_w
        # ãƒ‘ãƒƒãƒåˆ‡ã‚Šå‡ºã—ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ï¼‰
        @views patch = img[(i-1)*patch_size+1:i*patch_size,
                    (j-1)*patch_size+1:j*patch_size, :]
        push!(patches, vec(patch))
        push!(positions, (i, j))
    end

    return hcat(patches...), positions  # (PÂ²C, N), [(1,1), (1,2), ...]
end

# Attentionã«2D RoPEã‚’é©ç”¨
function attention_with_2d_rope(Q, K, V, positions, d_k)
    Q_rope = copy(Q)
    K_rope = copy(K)
    @inbounds for (i, (x, y)) in enumerate(positions)
        rope_emb = rope_2d(x, y, size(Q, 1))
        @views Q_rope[:, i] .= Q[:, i] .* rope_emb
        @views K_rope[:, i] .= K[:, i] .* rope_emb
    end

    # Attentionè¨ˆç®—
    attn = softmax(Q_rope' * K_rope ./ sqrt(d_k), dims=2)
    return V * attn'
end
```

#### 6.5.4 Dynamic Resolutionã®åŠ¹æœï¼ˆå®Ÿé¨“çµæœï¼‰

| è§£åƒåº¦ | å¾“æ¥ViT (å›ºå®š224Ã—224) | Qwen-VL (Dynamic) | æ”¹å–„ç‡ |
|:-------|:---------------------|:------------------|:------|
| 224Ã—224 | 72.3% | 72.5% | +0.2% |
| 336Ã—336 | 70.1% | 75.8% | **+5.7%** |
| 448Ã—448 | 65.4% | 78.2% | **+12.8%** |
| 672Ã—672 | 58.9% | 79.6% | **+20.7%** |

**è¦³å¯Ÿ**:
- å¾“æ¥ViTã¯ã€è¨“ç·´è§£åƒåº¦ï¼ˆ224Ã—224ï¼‰ã‹ã‚‰é›¢ã‚Œã‚‹ã¨æ€§èƒ½ãŒæ€¥æ¿€ã«ä½ä¸‹ã€‚
- Qwen-VLã¯ã€é«˜è§£åƒåº¦ã«ãªã‚‹ã»ã©æ€§èƒ½ãŒ**å‘ä¸Š**ï¼ˆç´°ã‹ã„è©³ç´°ã‚’æ‰ãˆã‚‰ã‚Œã‚‹ï¼‰ã€‚

### 6.6 CogVLM: Visual Expert

CogVLM[^8]ã¯ã€**Visual Expert**ã‚’å„Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ã«æŒ¿å…¥ã€‚

**é€šå¸¸ã®Transformer**:

$$
\mathbf{h}' = \mathbf{h} + \text{Attention}(\mathbf{h}) + \text{FFN}(\mathbf{h})
$$

**CogVLMã®Visual Expert**:

$$
\mathbf{h}' = \mathbf{h} + \alpha \cdot \text{Attention}_{\text{vis}}(\mathbf{h}, \mathbf{Z}^v) + \beta \cdot \text{FFN}_{\text{vis}}(\mathbf{h})
$$

$\alpha, \beta$ ã¯å­¦ç¿’å¯èƒ½ãªã‚²ãƒ¼ãƒˆã€‚é€šå¸¸ã®FFNã¨Visual FFNã‚’**ä¸¦åˆ—**ã«å®Ÿè¡Œã—ã€é‡ã¿ä»˜ãå’Œã‚’å–ã‚‹ã€‚

**åˆ©ç‚¹**: Frozen LMã®æ€§èƒ½ã‚’ä¿ã¡ã¤ã¤ã€è¦–è¦šæƒ…å ±ã‚’æ·±ãçµ±åˆã€‚

### 6.7 SmolVLM2: æ¥µå°256Mãƒ¢ãƒ‡ãƒ«

SmolVLM2[^9]ã¯ã€**256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ã§3ãƒ¢ãƒ€ãƒªãƒ†ã‚£ï¼ˆç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªï¼‰ã‚’å®Ÿç¾ã€‚

**åŠ¹ç‡åŒ–æŠ€è¡“**:
1. **Distillation**: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆIdefics2-8Bï¼‰ã‹ã‚‰çŸ¥è­˜ã‚’è’¸ç•™ã€‚
2. **Connectoråœ§ç¸®**: Vision Encoderã®å‡ºåŠ›ã‚’**16 tokens**ã«åœ§ç¸®ï¼ˆé€šå¸¸ã¯32-64 tokensï¼‰ã€‚
3. **Small LM**: SmolLM2-135Mï¼ˆGPT-2ã‚µã‚¤ã‚ºï¼‰ã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«ä½¿ç”¨ã€‚

**æ€§èƒ½**: Idefics-80Bï¼ˆ17ãƒ¶æœˆå‰ï¼‰ã‚’ä¸Šå›ã‚‹ã€‚

### 6.8 æœ€æ–°ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ2024-2026ï¼‰

#### 6.8.1 Molmo & PixMo

Molmo[^13]ã¯ã€Allen AIã«ã‚ˆã‚‹**å®Œå…¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹VLM**ã€‚

**PixMo Dataset**:
- **PixMo-Cap**: 1Mé«˜å“è³ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ï¼ˆéŸ³å£°å…¥åŠ›ã§äººé–“ãŒè¨˜è¿°ï¼‰
- **PixMo-Points**: 2D Pointing annotations â€” éè¨€èªçš„ãªã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°

**é©æ–°**: ãƒ¢ãƒ‡ãƒ«ãŒ**ç”»åƒä¸Šã®åº§æ¨™ã‚’å‡ºåŠ›**ã§ãã‚‹ã€‚ã€ŒçŒ«ã¯ã©ã“ï¼Ÿã€â†’ `(342, 189)` ã®ã‚ˆã†ã«å›ç­”ã€‚

#### 6.8.2 EVA-CLIP

EVA-CLIPï¼ˆ2023ï¼‰ã¯ã€**5B Vision Encoder**ã‚’ä½¿ç”¨ã€‚

**è¨“ç·´æˆ¦ç•¥**:
1. **MIM (Masked Image Modeling)** ã§Vision Encoderã‚’äº‹å‰è¨“ç·´
2. CLIPã®Contrastiveå­¦ç¿’ã§Fine-tuning

**çµæœ**: ImageNet Zero-shot 80.4%ï¼ˆCLIP-ViT-L/14ã¯75.5%ï¼‰ã€‚

### 6.9 æ¨å¥¨æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹

| æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹ | è‘—è€…/æ©Ÿé–¢ | å†…å®¹ | URL |
|:-------------|:---------|:-----|:----|
| **CLIPè«–æ–‡** | Radford et al., OpenAI | CLIPã®åŸè«–æ–‡ | [arXiv:2103.00020](https://arxiv.org/abs/2103.00020) |
| **BLIP-2è«–æ–‡** | Li et al., Salesforce | Q-Formerã®è©³ç´° | [arXiv:2301.12597](https://arxiv.org/abs/2301.12597) |
| **Flamingoè«–æ–‡** | Alayrac et al., DeepMind | Perceiver Resampler | [arXiv:2204.14198](https://arxiv.org/abs/2204.14198) |
| **HuggingFace Transformers** | HuggingFace | VLMå®Ÿè£…é›† | [github.com/huggingface/transformers](https://github.com/huggingface/transformers) |
| **Open-CLIP** | LAION | CLIPã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å®Ÿè£… | [github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip) |

<details><summary>ç”¨èªé›†</summary>

| ç”¨èª | æ„å‘³ |
|:-----|:-----|
| **Dual Encoder** | ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¥ã€…ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§å‡¦ç†ã™ã‚‹æ§‹é€  |
| **Contrastive Learning** | æ­£ä¾‹ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–ã€è² ä¾‹ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–ã™ã‚‹å­¦ç¿’ |
| **InfoNCE Loss** | Noise Contrastive Estimationã«åŸºã¥ãå¯¾æ¯”æå¤± |
| **Q-Former** | BLIP-2ã®Query-based Transformerã€‚ç”»åƒç‰¹å¾´ã‚’å›ºå®šé•·ã«åœ§ç¸® |
| **Perceiver Resampler** | Flamingoã®å¯å¤‰é•·â†’å›ºå®šé•·å¤‰æ›ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« |
| **Visual Expert** | CogVLMã®è¦–è¦šå°‚ç”¨FFN |
| **Dynamic Resolution** | Qwen-VLã®å¯å¤‰è§£åƒåº¦å¯¾å¿œ |
| **Visual Instruction Tuning** | LLaVAã®Instruction-Followingè¨“ç·´æ‰‹æ³• |
| **Frozen LLM** | é‡ã¿ã‚’å›ºå®šã—ãŸLarge Language Model |
| **Modality Gap** | ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®åŸ‹ã‚è¾¼ã¿åˆ†å¸ƒã®ã‚®ãƒ£ãƒƒãƒ— |
| **Hard Negative** | é¡ä¼¼åº¦ãŒé«˜ã„è² ä¾‹ï¼ˆè­˜åˆ¥ãŒé›£ã—ã„ï¼‰ |
| **Zero-shotåˆ†é¡** | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãªã—ã§ã®åˆ†é¡ |
| **VQA** | Visual Question Answering |
| **CIDEr** | Consensus-based Image Description Evaluation |
| **SPICE** | Semantic Propositional Image Caption Evaluation |

</details>

### 6.10 çŸ¥è­˜ãƒãƒƒãƒ—ï¼ˆmermaidï¼‰

```mermaid
graph TD
    A[Vision-Language Models] --> B[ç†è«–åŸºç¤]
    A --> C[ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
    A --> D[è©•ä¾¡]

    B --> B1[Contrastive Learning]
    B --> B2[Cross-Modal Attention]
    B --> B3[Modality Gap]
    B1 --> B11[InfoNCE Loss]
    B1 --> B12[Temperature Scaling]

    C --> C1[Late Fusion]
    C --> C2[Deep Fusion]
    C --> C3[Early Fusion]
    C1 --> C11[CLIP]
    C1 --> C12[SigLIP]
    C2 --> C21[BLIP-2]
    C2 --> C22[LLaVA]
    C2 --> C23[CogVLM]
    C3 --> C31[Chameleon]

    D --> D1[VQA]
    D --> D2[Captioning]
    D --> D3[Zero-shot]
    D --> D4[Retrieval]

    style A fill:#FF6B6B
    style B fill:#4ECDC4
    style C fill:#45B7D1
    style D fill:#FFA07A
```

### 6.6 ä¸»è¦ãªå­¦ã³ï¼ˆ4ã¤ã®Takeawayï¼‰

3,000è¡Œã®é•·ã„æ—…ã ã£ãŸãŒã€ã“ã“ã¾ã§æ¥ãŸã‚ãªãŸã¯**Vision-Languageãƒ¢ãƒ‡ãƒ«ã®å…¨é ˜åŸŸ**ã‚’ç†è§£ã—ãŸã€‚

1. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« = Late/Deep/Early Fusionã®3æˆ¦ç•¥**
   - Late Fusion (CLIP): ç‹¬ç«‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ + é¡ä¼¼åº¦è¨ˆç®—
   - Deep Fusion (BLIP-2): ä¸­é–“å±¤ã§Cross-Attention
   - Early Fusion (Chameleon): å…¥åŠ›ãƒ¬ãƒ™ãƒ«ã§çµ±ä¸€Token

2. **InfoNCE lossã®æœ¬è³ª = ç›¸äº’æƒ…å ±é‡ã®ä¸‹ç•Œæœ€å¤§åŒ–**
   - æ­£ä¾‹ãƒšã‚¢ $(v_i, t_i)$ ã®é¡ä¼¼åº¦ã‚’æœ€å¤§åŒ–
   - è² ä¾‹ãƒšã‚¢ $(v_i, t_j)$ ã®é¡ä¼¼åº¦ã‚’æœ€å°åŒ–
   - æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\tau$ ã§åˆ†å¸ƒã®é‹­ã•ã‚’åˆ¶å¾¡

3. **Vision Transformer = Self-Attentionã§ç”»åƒã‚’å‡¦ç†**
   - Patch Embedding: ç”»åƒã‚’ $P \times P$ ãƒ‘ãƒƒãƒã«åˆ†å‰²
   - Positional Encoding: 2Dä½ç½®æƒ…å ±ã‚’ä»˜ä¸
   - Global Attention: å…¨ãƒ‘ãƒƒãƒé–“ã§Attentionï¼ˆCNNã‚ˆã‚Šåºƒã„å—å®¹é‡ï¼‰

4. **å®Ÿè£…ã®ç¾å®Ÿ: âš¡Juliaè¨“ç·´ + ğŸ¦€Rustæ¨è«–**
   - Juliaã§CLIPè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆInfoNCE losså®Ÿè£…ï¼‰
   - Rustã§SmolVLM2æ¨è«–ï¼ˆGGUF/Candleçµ±åˆï¼‰
   - FFIçµŒç”±ã§ç›¸äº’é‹ç”¨ï¼ˆProduction-readyï¼‰

### 6.7 FAQ

<details><summary>Q1: CLIPã¨BLIP-2ã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ</summary>

**A**: ã‚¿ã‚¹ã‚¯æ¬¡ç¬¬ã€‚

- **Zero-shotåˆ†é¡ãƒ»Retrieval**: CLIPï¼ˆLate Fusionï¼‰ãŒæœ€é©ã€‚è¨“ç·´ãŒç°¡å˜ã§ã€æ¨è«–ã‚‚é€Ÿã„ã€‚
- **VQAãƒ»Captioning**: BLIP-2ï¼ˆDeep Fusionï¼‰ãŒæœ€é©ã€‚Q-FormerãŒç”»åƒã®è©³ç´°ã‚’æ‰ãˆã‚‹ã€‚
- **Instruction-Following**: LLaVAã€CogVLMï¼ˆDeep Fusion + Frozen LLMï¼‰ãŒæœ€é©ã€‚

**ã‚³ã‚¹ãƒˆ vs æ€§èƒ½**:
- CLIP: è¨“ç·´ã‚³ã‚¹ãƒˆä½ã€æ¨è«–é€Ÿåº¦é€Ÿã€æ€§èƒ½ä¸­
- BLIP-2: è¨“ç·´ã‚³ã‚¹ãƒˆä¸­ã€æ¨è«–é€Ÿåº¦ä¸­ã€æ€§èƒ½é«˜
- CogVLM: è¨“ç·´ã‚³ã‚¹ãƒˆé«˜ã€æ¨è«–é€Ÿåº¦é…ã€æ€§èƒ½æœ€é«˜

</details>

<details><summary>Q2: InfoNCE lossã®æ¸©åº¦ $\tau$ ã‚’ã©ã†æ±ºã‚ã‚‹ï¼Ÿ</summary>

**A**: å®Ÿé¨“çš„ã«æ±ºå®šã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã€‚

**çµŒé¨“å‰‡**:
- $\tau = 0.07$: CLIPã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€‚ã»ã¨ã‚“ã©ã®ã‚±ãƒ¼ã‚¹ã§ã“ã‚Œã§OKã€‚
- $\tau$ ãŒå°ã•ã„ï¼ˆ0.01ã€œ0.05ï¼‰: Hard Negativeã‚’å¼·ãç½°ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå¤šæ§˜ãªã‚‰æœ‰åŠ¹ã€‚
- $\tau$ ãŒå¤§ãã„ï¼ˆ0.1ã€œ0.5ï¼‰: åˆ†å¸ƒãŒãªã ã‚‰ã‹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã«éå­¦ç¿’ã‚’é˜²ãã€‚

**è‡ªå‹•èª¿æ•´**: $\tau$ ã‚’å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã—ã¦ã€è¨“ç·´ä¸­ã«æœ€é©åŒ–ã™ã‚‹æ‰‹æ³•ã‚‚ã‚ã‚‹ï¼ˆCLIPè«–æ–‡ã§ã¯å›ºå®šï¼‰ã€‚

</details>

<details><summary>Q3: SmolVLM2-256Mã¯å®Ÿç”¨çš„ï¼Ÿ</summary>

**A**: ç”¨é€”æ¬¡ç¬¬ã ãŒã€**ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹**ã§ã¯éå¸¸ã«æœ‰åŠ¹ã€‚

**åˆ©ç‚¹**:
- æ¨è«–ãŒè¶…é«˜é€Ÿï¼ˆ1ç”»åƒ<100ms on CPUï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå°ã•ã„ï¼ˆ<1GB RAMï¼‰
- 3ãƒ¢ãƒ€ãƒªãƒ†ã‚£å¯¾å¿œï¼ˆç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªï¼‰

**æ¬ ç‚¹**:
- è¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã§ã¯å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«åŠ£ã‚‹
- Fine-tuningã®ä½™åœ°ãŒé™å®šçš„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ï¼‰

**æ¨å¥¨ç”¨é€”**: ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒèªè­˜ã€IoTãƒ‡ãƒã‚¤ã‚¹ã€‚

</details>

<details><summary>Q4: Rustã§VLMè¨“ç·´ã¯ã§ããªã„ï¼Ÿ</summary>

**A**: æŠ€è¡“çš„ã«ã¯å¯èƒ½ã ãŒã€**ç¾æ™‚ç‚¹ã§ã¯éæ¨å¥¨**ã€‚

**ç†ç”±**:
1. **è‡ªå‹•å¾®åˆ†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æœªæˆç†Ÿ**: PyTorchã‚„JAXã«æ¯”ã¹ã€Rustã®è‡ªå‹•å¾®åˆ†ï¼ˆburn, dfdxï¼‰ã¯ã¾ã ç™ºå±•é€”ä¸Šã€‚
2. **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®æ¬ å¦‚**: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€ã‚ªãƒ¼ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€åˆ†æ•£è¨“ç·´ãƒ„ãƒ¼ãƒ«ãŒä¸è¶³ã€‚
3. **é–‹ç™ºé€Ÿåº¦**: Rustã¯å‹å®‰å…¨ã ãŒã€å®Ÿé¨“ã®åå¾©é€Ÿåº¦ã¯Juliaã‚„Pythonã«åŠ£ã‚‹ã€‚

**Rustã®å½¹å‰²**: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®**æ¨è«–**ã«ç‰¹åŒ–ã€‚GGUF/Candleã§é«˜é€Ÿæ¨è«–ã‚’å®Ÿç¾ã€‚

</details>

<details><summary>Q5: ç¬¬23å›ï¼ˆFine-tuningï¼‰ã§å­¦ã¶ã“ã¨ã¯ï¼Ÿ</summary>

**A**: LoRAã€QLoRAã€Adapterãªã©ã®PEFTæŠ€è¡“ã€‚

**äºˆç¿’ãƒã‚¤ãƒ³ãƒˆ**:
- LoRAã®æ•°å¼: ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—åˆ†è§£ $W' = W + AB$ ï¼ˆ$A \in \mathbb{R}^{d \times r}$, $B \in \mathbb{R}^{r \times d}$ï¼‰
- QLoRAã®é‡å­åŒ–: 4-bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- Adapterã®æŒ¿å…¥ä½ç½®: ã©ã“ã«Adapterå±¤ã‚’å…¥ã‚Œã‚‹ã‹

ç¬¬23å›ã§ã¯ã€ã“ã‚Œã‚‰ã‚’âš¡Juliaã§å®Ÿè£…ã—ã€CLIPã‚„LLaVAã‚’Fine-tuningã™ã‚‹ã€‚

</details>

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ | å†…å®¹ |
|:---|:------|:-----|:-----|
| **Day 1** | Zone 0-2 | 1æ™‚é–“ | Quick Start + ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®æ¦‚è¦ã‚’æ´ã‚€ |
| **Day 2** | Zone 3.1-3.2 | 2æ™‚é–“ | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç¤ + ViTç†è«–ã€‚æ•°å¼ã‚’ç´™ã«æ›¸ããªãŒã‚‰ç†è§£ |
| **Day 3** | Zone 3.3-3.4 | 2æ™‚é–“ | Cross-Modal Attention + InfoNCE losså°å‡ºï¼ˆBoss Battleï¼‰ |
| **Day 4** | Zone 4.1-4.2 | 2æ™‚é–“ | Julia CLIPå®Ÿè£… + ViTå®Ÿè£…ã€‚å®Ÿéš›ã«ã‚³ãƒ¼ãƒ‰ã‚’å‹•ã‹ã™ |
| **Day 5** | Zone 4.3 | 1.5æ™‚é–“ | Rust SmolVLM2æ¨è«– + FFIçµ±åˆ |
| **Day 6** | Zone 5 | 2æ™‚é–“ | è©•ä¾¡å®Ÿè£…ï¼ˆVQA/Captioning/Zero-shot/Retrievalï¼‰ |
| **Day 7** | Zone 6 | 1.5æ™‚é–“ | æŒ¯ã‚Šè¿”ã‚Š + æœ€æ–°ç ”ç©¶ã€‚å…¨ä½“ã‚’ä¿¯ç° |

**Total**: 12æ™‚é–“

### 6.9 æ¬¡ã®è¬›ç¾©ã¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼

**ç¬¬23å›: Fine-tuning & PEFT** ã§ã¯ã€ä»¥ä¸‹ã‚’å­¦ã¶:

1. **LoRA (Low-Rank Adaptation)**
   - ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—åˆ†è§£ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’1%ã«å‰Šæ¸›
   - CLIPã®Vision Encoderã«LoRAã‚’é©ç”¨

2. **QLoRA (Quantized LoRA)**
   - 4-bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’75%å‰Šæ¸›
   - LLaVA-7Bã‚’QLoRAã§Fine-tuning

3. **Adapter**
   - å„Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ã«Adapterå±¤ã‚’æŒ¿å…¥
   - Frozen LMã‚’ä¿ã¡ã¤ã¤ã€ã‚¿ã‚¹ã‚¯ç‰¹åŒ–

4. **DreamBooth**
   - ã€ŒSksã¨ã„ã†çŒ«ã€ã‚’å­¦ç¿’ã•ã›ã‚‹ï¼ˆFew-shot Personalizationï¼‰

**å®Ÿè£…è¨€èª**: âš¡Julia (LoRA/QLoRAè¨“ç·´) + ğŸ¦€Rust (é‡å­åŒ–æ¨è«–)

æº–å‚™ã¯ã„ã„ã‹ï¼Ÿ æ¬¡å›ã‚‚æ¥½ã—ã¿ã«ã—ã¦ã„ã¦ã»ã—ã„ã€‚

### 6.10 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼ï¼ˆJuliaå®Ÿè£…ï¼‰

```julia
# ç¬¬22å›ã®é€²æ—ã‚’è¨˜éŒ²
struct Progress
    lecture_num::Int
    zones_completed::Vector{String}
    implementations::Dict{String, Bool}
    evaluations::Dict{String, Float64}
end

function track_progress()
    progress = Progress(
        22,
        ["Zone 0", "Zone 1", "Zone 2", "Zone 3", "Zone 4", "Zone 5", "Zone 6", "Zone 7"],
        Dict(
            "CLIP Julia" => true,
            "ViT Julia" => true,
            "SmolVLM2 Rust" => true,
            "InfoNCE Loss" => true,
            "VQA Eval" => true,
            "Captioning Eval" => true,
            "Zero-shot Eval" => true,
            "Retrieval Eval" => true
        ),
        Dict(
            "InfoNCE Lossç†è§£åº¦" => 0.95,
            "CLIPå®Ÿè£…å®Œæˆåº¦" => 0.90,
            "Rustæ¨è«–æˆåŠŸç‡" => 0.88,
            "è©•ä¾¡å®Ÿè£…å®Œæˆåº¦" => 0.85
        )
    )

    println("=== ç¬¬$(progress.lecture_num)å›é€²æ— ===")
    println("å®Œäº†Zone: $(join(progress.zones_completed, ", "))")
    println("\nå®Ÿè£…çŠ¶æ³:")
    for (impl, status) in progress.implementations
        println("  $impl: $(status ? "âœ“" : "âœ—")")
    end
    println("\nè©•ä¾¡æŒ‡æ¨™:")
    for (metric, score) in progress.evaluations
        println("  $metric: $(round(score * 100, digits=1))%")
    end

    overall = mean(values(progress.evaluations))
    println("\nç·åˆç†è§£åº¦: $(round(overall * 100, digits=1))%")

    return progress
end

# å®Ÿè¡Œ
track_progress()
```

**å‡ºåŠ›ä¾‹**:
```
=== ç¬¬22å›é€²æ— ===
å®Œäº†Zone: Zone 0, Zone 1, Zone 2, Zone 3, Zone 4, Zone 5, Zone 6, Zone 7

å®Ÿè£…çŠ¶æ³:
  CLIP Julia: âœ“
  ViT Julia: âœ“
  SmolVLM2 Rust: âœ“
  InfoNCE Loss: âœ“
  VQA Eval: âœ“
  Captioning Eval: âœ“
  Zero-shot Eval: âœ“
  Retrieval Eval: âœ“

è©•ä¾¡æŒ‡æ¨™:
  InfoNCE Lossç†è§£åº¦: 95.0%
  CLIPå®Ÿè£…å®Œæˆåº¦: 90.0%
  Rustæ¨è«–æˆåŠŸç‡: 88.0%
  è©•ä¾¡å®Ÿè£…å®Œæˆåº¦: 85.0%

ç·åˆç†è§£åº¦: 89.5%
```

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**å•ã„**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã¯ã€Œå¿œç”¨æŠ€è¡“ã€ã§ã¯ãªãã€Œæ¨™æº–ã€ã§ã¯ï¼Ÿ

**èƒŒæ™¯**:
æˆ‘ã€…ã¯é•·ã„é–“ã€ã€Œãƒ†ã‚­ã‚¹ãƒˆã®AIã€ã€Œç”»åƒã®AIã€ã€ŒéŸ³å£°ã®AIã€ã‚’**åˆ¥ã€…ã®æŠ€è¡“**ã¨ã—ã¦æ‰±ã£ã¦ããŸã€‚ã—ã‹ã—ã€äººé–“ã®çŸ¥èƒ½ã¯**æœ¬è³ªçš„ã«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«**ã ã€‚

- å­ä¾›ã¯ã€Œã‚Šã‚“ã”ã€ã¨ã„ã†å˜èªã‚’å­¦ã¶ã¨ãã€**å®Ÿç‰©ã‚’è¦‹ãªãŒã‚‰**èãã€‚
- æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’èª­ã‚€ã¨ãã€**å†™çœŸã‚’è¦‹ãªãŒã‚‰**æ‰‹é †ã‚’ç†è§£ã™ã‚‹ã€‚
- éŸ³æ¥½ã‚’è´ãã¨ãã€**æ­Œè©ã‚’èª­ã¿ãªãŒã‚‰**æ„Ÿæƒ…ã‚’æ·±ã‚ã‚‹ã€‚

ã§ã¯ã€ãªãœAIã¯ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’**åˆ†é›¢**ã—ã¦ããŸã®ã‹ï¼Ÿ

**ç­”ãˆ**: **æŠ€è¡“çš„åˆ¶ç´„**ãŒã‚ã£ãŸã‹ã‚‰ã€‚

- 1950-1990å¹´ä»£: è¨ˆç®—è³‡æºã®åˆ¶ç´„ã§ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ç‰¹åŒ–ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã€‚
- 2000-2010å¹´ä»£: Deep Learningã®å°é ­ã§ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆCNN for Vision, RNN for Textï¼‰ã€‚
- 2020å¹´ä»£: Transformerã®ç™»å ´ã§ã€**çµ±ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ãŒå¯èƒ½ã«ã€‚

**ä»Šå¾Œã®æ–¹å‘æ€§**:

1. **ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãŒæ¨™æº–ã«ãªã‚‹** â€” å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ã¯ã€Œç‰¹æ®Šç”¨é€”ã€ã«ã€‚
2. **å…¨ã¦ã®AIãŒãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã«** â€” LLMã«ã€Œç›®ã€ã€Œè€³ã€ã€Œæ‰‹ã€ãŒä»˜ãï¼ˆGPT-4o, Gemini Ultraã®æ–¹å‘æ€§ï¼‰ã€‚
3. **æ–°ã—ã„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®çµ±åˆ** â€” è§¦è¦šã€å—…è¦šã€å‘³è¦šã‚‚AIã®å…¥åŠ›ã«ï¼Ÿ

**è­°è«–ãƒã‚¤ãƒ³ãƒˆ**:

- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãŒæ¨™æº–ã«ãªã‚‹ã¨ã€**ã©ã‚“ãªæ–°ã—ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³**ãŒç”Ÿã¾ã‚Œã‚‹ã‹ï¼Ÿ
- å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ã®**å­˜åœ¨æ„ç¾©**ã¯æ®‹ã‚‹ã‹ï¼Ÿï¼ˆä¾‹: ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®LLMï¼‰
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®**å€«ç†çš„èª²é¡Œ**ã¯ï¼Ÿï¼ˆDeepfakeã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ï¼‰

**æ­´å²çš„æ–‡è„ˆ**:

20ä¸–ç´€åˆé ­ã€**ãƒ©ã‚¸ã‚ª**ãŒç™»å ´ã—ãŸã¨ãã€äººã€…ã¯ã€ŒéŸ³å£°ã ã‘ã§ååˆ†ã€ã¨è€ƒãˆãŸã€‚ã—ã‹ã—ã€**ãƒ†ãƒ¬ãƒ“**ãŒç™»å ´ã™ã‚‹ã¨ã€æ˜ åƒã¨éŸ³å£°ã®çµ„ã¿åˆã‚ã›ãŒ**æ¨™æº–**ã«ãªã£ãŸã€‚ä»Šã€AIã‚‚åŒã˜è»¢æ›ç‚¹ã«ã„ã‚‹ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ ç¬¬22å›ã€Œãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å®Œå…¨ç‰ˆã€å®Œèµ°ï¼ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆã‚’å®Œå…¨ã«ãƒã‚¹ã‚¿ãƒ¼ã—ãŸã€‚æ¬¡ã¯Fine-tuningã§ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹ã€‚

---

> **Progress: 95%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. SigLIPãŒCLIPã®æå¤±é–¢æ•°ã‚’SigmoidåŒ–ã—ãŸæ•°å­¦çš„ãªå‹•æ©Ÿã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. Flamingoã®gated cross-attentionãŒè¦–è¦šæƒ…å ±ã‚’LLMã«æ³¨å…¥ã™ã‚‹ã¨ãã€ã‚²ãƒ¼ãƒˆæ©Ÿæ§‹ãŒé‡è¦ãªç†ç”±ã¯ä½•ã‹ï¼Ÿ

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. *International Conference on Machine Learning (ICML)*.
<https://arxiv.org/abs/2103.00020>

[^2]: van den Oord, A., Li, Y., & Vinyals, O. (2018). Representation Learning with Contrastive Predictive Coding. *arXiv preprint*.
<https://arxiv.org/abs/1807.03748>

[^3]: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *International Conference on Learning Representations (ICLR) 2021*.
<https://arxiv.org/abs/2010.11929>

[^4]: Li, J., Li, D., Savarese, S., & Hoi, S. (2023). BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. *International Conference on Machine Learning (ICML)*.
<https://arxiv.org/abs/2301.12597>

[^5]: Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., & Simonyan, K. (2022). Flamingo: a Visual Language Model for Few-Shot Learning. *Advances in Neural Information Processing Systems (NeurIPS)*.
<https://arxiv.org/abs/2204.14198>

[^6]: Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Visual Instruction Tuning. *Advances in Neural Information Processing Systems (NeurIPS)*.
<https://arxiv.org/abs/2304.08485>

[^7]: Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, C., Wang, L., Ge, Y., Song, Y., Li, H., Dang, K., Ouyang, S., Ren, X., Yan, D., Zhang, X., Qin, Y., Lin, Z., Huang, F., Liu, J., & Zhou, J. (2024). Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution. *arXiv preprint*.
<https://arxiv.org/abs/2409.12191>

[^8]: Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., & Tang, J. (2023). CogVLM: Visual Expert for Pretrained Language Models. *arXiv preprint*.
<https://arxiv.org/abs/2311.03079>

[^9]: HuggingFace (2024). SmolVLM2-256M-Instruct.
<https://huggingface.co/HuggingFaceTB/SmolVLM2-256M-Instruct>

[^11]: Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., & Jitsev, J. (2023). Reproducible scaling laws for contrastive language-image learning. *Computer Vision and Pattern Recognition (CVPR)*.
<https://arxiv.org/abs/2212.07143>

[^12]: Zhai, X., Mustafa, B., Kolesnikov, A., & Beyer, L. (2023). Sigmoid Loss for Language Image Pre-Training. *arXiv preprint*.
<https://arxiv.org/abs/2303.15343>

[^13]: Deitke, M., Clark, C., Lee, S., Tripathi, R., Yang, Y., Park, J. S., Salehi, M., Muennighoff, N., Lo, K., Soldaini, L., Lu, J., Anderson, T., Bransom, E., Ehsani, K., Ngo, H., Chen, Y. H., Patel, A., Yatskar, M., Callison-Burch, C., Head, A., Hendrix, R., Bastani, F., VanderBilt, E., Lambert, N., Kim, Y.-J., Choudhury, S., Chasins, S., & Farhadi, A. (2024). Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models. *arXiv preprint*.
<https://arxiv.org/abs/2409.17146>

[^14]: Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., & Parikh, D. (2017). Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. *Computer Vision and Pattern Recognition (CVPR)*.

[^15]: Anderson, P., Fernando, B., Johnson, M., & Gould, S. (2016). SPICE: Semantic Propositional Image Caption Evaluation. *European Conference on Computer Vision (ECCV)*.
<https://panderson.me/spice/>

[^20]: Lin, F. (2025). "Vision Language Models: A Survey of 26K Papers". *arXiv preprint*.
<https://arxiv.org/abs/2510.09586>

[^21]: Li, J., Li, D., Savarese, S., & Hoi, S. (2023). "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models". *International Conference on Machine Learning (ICML)*.
<https://arxiv.org/abs/2301.12597>

[^22]: Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2024). "Improved Baselines with Visual Instruction Tuning". *arXiv preprint*.
<https://arxiv.org/abs/2310.03744>

[^24]: Wang, Y., et al. (2022). "Multimodal Token Fusion for Vision Transformers". *Computer Vision and Pattern Recognition (CVPR)*.
<https://arxiv.org/abs/2204.08721>

[^25]: Jia, D., et al. (2024). "GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer". *Computer Vision and Pattern Recognition (CVPR)*.
<https://arxiv.org/abs/2406.01210>

[^26]: Chen, X., et al. (2024). "Heterogeneous Contrastive Learning for Foundation Models and Beyond". *arXiv preprint*.
<https://arxiv.org/abs/2404.00225>

[^27]: Mohsin, M. T., et al. (2024). "Multimodal Foundation Models for Early Disease Detection". *arXiv preprint*.
<https://arxiv.org/abs/2510.01899>

[^28]: Han, X., Chen, S., Fu, Z., Feng, Z., Fan, L., et al. (2025). "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision". *arXiv preprint*.
<https://arxiv.org/abs/2504.02477>

[^29]: Dufumier, B., et al. (2024). "What to align in multimodal contrastive learning?" *ICLR 2025*.
<https://arxiv.org/abs/2409.07402>

### æ•™ç§‘æ›¸

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [https://probml.github.io/pml-book/book2.html](https://probml.github.io/pml-book/book2.html)
- Prince, S. J. D. (2023). *Understanding Deep Learning*. MIT Press. [https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)
- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2023). *Dive into Deep Learning*. Cambridge University Press. [https://d2l.ai/](https://d2l.ai/)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
