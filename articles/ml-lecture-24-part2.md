---
title: "ç¬¬24å›ã€å¾Œç·¨ã€‘ä»˜éŒ²ç·¨: çµ±è¨ˆå­¦: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ“ˆ"
type: "tech"
topics: ["machinelearning", "statistics", "julia", "bayesian", "hypothesis"]
published: true
slug: "ml-lecture-24-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> **ç¬¬24å›ã€å‰ç·¨ã€‘**: [ç¬¬24å›ã€å‰ç·¨ã€‘](https://zenn.dev/fumishiki/ml-lecture-24-part1)

## Part 2


$$
\begin{aligned}
\text{SS}_{\text{total}} &= \sum_{i=1}^k \sum_{j=1}^{n_i} (x_{ij} - \bar{x})^2 \\
\text{SS}_{\text{between}} &= \sum_{i=1}^k n_i (\bar{x}_i - \bar{x})^2 \\
\text{SS}_{\text{within}} &= \sum_{i=1}^k \sum_{j=1}^{n_i} (x_{ij} - \bar{x}_i)^2 \\
\text{MS}_{\text{between}} &= \frac{\text{SS}_{\text{between}}}{k-1}, \quad \text{MS}_{\text{within}} = \frac{\text{SS}_{\text{within}}}{N-k}
\end{aligned}
$$

**æ•°å€¤æ¤œè¨¼**:

```julia
using HypothesisTests

group_a = [0.72, 0.71, 0.73, 0.70, 0.72]
group_b = [0.78, 0.77, 0.79, 0.76, 0.78]
group_c = [0.68, 0.67, 0.69, 0.66, 0.68]

# ä¸€å…ƒé…ç½®ANOVA
test = OneWayANOVATest(group_a, group_b, group_c)
println("F=$(round(test.F, digits=3)), p=$(round(pvalue(test), digits=6))")
println(pvalue(test) < 0.05 ? "âœ… å°‘ãªãã¨ã‚‚1çµ„ã®å¹³å‡ãŒç•°ãªã‚‹" : "âŒ å…¨ç¾¤ã®å¹³å‡ã«å·®ãªã—")
```

å‡ºåŠ›:
```
F=90.0, p=0.000000
âœ… å°‘ãªãã¨ã‚‚1çµ„ã®å¹³å‡ãŒç•°ãªã‚‹
```

#### 3.4.3 æ­£è¦æ€§æ¤œå®š

**å•é¡Œ**: tæ¤œå®šãƒ»ANOVAã¯æ­£è¦æ€§ã‚’ä»®å®šã€‚ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã†ã‹æ¤œè¨¼ã—ãŸã„ã€‚

| æ¤œå®š | ç‰¹å¾´ | å¸°ç„¡ä»®èª¬ |
|:-----|:-----|:--------|
| **Shapiro-Wilkæ¤œå®š** | æœ€ã‚‚å¼·åŠ›ï¼ˆå°~ä¸­ã‚µãƒ³ãƒ—ãƒ«ï¼‰ | ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã† |
| **Kolmogorov-Smirnovæ¤œå®š** | æ±ç”¨çš„ï¼ˆä»»æ„ã®åˆ†å¸ƒï¼‰ | ãƒ‡ãƒ¼ã‚¿ãŒæŒ‡å®šåˆ†å¸ƒã«å¾“ã† |
| **Anderson-Darlingæ¤œå®š** | è£¾ã®é©åˆåº¦ã‚’é‡è¦– | ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã† |

**æ•°å€¤æ¤œè¨¼**:

```julia
using HypothesisTests, Distributions

# æ­£è¦åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿
normal_data = rand(Normal(0, 1), 30)
test_normal = ExactOneSampleKSTest(normal_data, Normal(0, 1))
println("æ­£è¦ãƒ‡ãƒ¼ã‚¿: p=$(round(pvalue(test_normal), digits=4))")

# éæ­£è¦ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸€æ§˜åˆ†å¸ƒï¼‰
uniform_data = rand(Uniform(0, 1), 30)
test_uniform = ExactOneSampleKSTest(uniform_data, Normal(0.5, 1))
println("ä¸€æ§˜ãƒ‡ãƒ¼ã‚¿: p=$(round(pvalue(test_uniform), digits=4))")
```

### 3.5 ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š

**ç”¨é€”**: æ­£è¦æ€§ãŒæº€ãŸã•ã‚Œãªã„ã€ã¾ãŸã¯é †åºãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€‚

| æ¤œå®š | ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ç‰ˆ | ç”¨é€” |
|:-----|:----------------|:-----|
| **Mann-Whitney Uæ¤œå®š** | 2æ¨™æœ¬tæ¤œå®š | 2ç¾¤ã®ä¸­å¤®å€¤ã®å·® |
| **Wilcoxonç¬¦å·é †ä½æ¤œå®š** | å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š | å¯¾å¿œã®ã‚ã‚‹2ç¾¤ã®ä¸­å¤®å€¤å·® |
| **Kruskal-Wallisæ¤œå®š** | ä¸€å…ƒé…ç½®ANOVA | 3ç¾¤ä»¥ä¸Šã®ä¸­å¤®å€¤ã®å·® |

**Mann-Whitney Uæ¤œå®šã®åŸç†**:

1. 2ç¾¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦é †ä½ä»˜ã‘ã€‚
2. å„ç¾¤ã®é †ä½å’Œã‚’è¨ˆç®—ã€‚
3. Uçµ±è¨ˆé‡ã‚’è¨ˆç®—:

$$
U_1 = n_1 n_2 + \frac{n_1(n_1+1)}{2} - R_1
$$

ã“ã“ã§ $R_1$ ã¯ç¾¤1ã®é †ä½å’Œã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using HypothesisTests

group1 = [1, 2, 3, 4, 5]
group2 = [6, 7, 8, 9, 10]

# Mann-Whitney Uæ¤œå®š
test = MannWhitneyUTest(group1, group2)
println("U=$(test.U), p=$(round(pvalue(test), digits=4))")
```

> **Note:** **é€²æ—: 65% å®Œäº†** ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ»ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã®ç†è«–å®Œå…¨ç‰ˆã‚’åˆ¶è¦‡ã€‚å¤šé‡æ¯”è¼ƒè£œæ­£ã¸ã€‚

### 3.6 å¤šé‡æ¯”è¼ƒè£œæ­£ç†è«–

**å•é¡Œ**: è¤‡æ•°ã®æ¤œå®šã‚’è¡Œã†ã¨ã€å¶ç„¶ã«æœ‰æ„ã«ãªã‚‹ç¢ºç‡ï¼ˆç¬¬1ç¨®éèª¤ï¼‰ãŒå¢—å¤§ã™ã‚‹ã€‚

**ä¾‹**: $\alpha = 0.05$ ã§ç‹¬ç«‹ãª20å€‹ã®æ¤œå®šã‚’è¡Œã†ã¨ã€å°‘ãªãã¨ã‚‚1ã¤ãŒå¶ç„¶æœ‰æ„ã«ãªã‚‹ç¢ºç‡:

$$
1 - (1 - 0.05)^{20} \approx 0.64 \quad \text{(64%!)}
$$

**FWERï¼ˆFamily-Wise Error Rateï¼‰**: å°‘ãªãã¨ã‚‚1ã¤ã®ç¬¬1ç¨®éèª¤ãŒèµ·ã“ã‚‹ç¢ºç‡ã€‚

**FDRï¼ˆFalse Discovery Rateï¼‰**: æœ‰æ„ã¨åˆ¤å®šã•ã‚ŒãŸã‚‚ã®ã®ã†ã¡å½é™½æ€§ã®å‰²åˆã®æœŸå¾…å€¤ã€‚

#### 3.6.1 FWERåˆ¶å¾¡æ³•

| æ‰‹æ³• | èª¿æ•´å¾Œã®æœ‰æ„æ°´æº– | ä¿å®ˆæ€§ |
|:-----|:----------------|:-------|
| **Bonferroniè£œæ­£** | $\alpha_{\text{adj}} = \alpha / m$ | æœ€ã‚‚ä¿å®ˆçš„ |
| **Holmæ³•** | é€æ¬¡çš„Bonferroni | Bonferroniã‚ˆã‚Šç·©ã„ |
| **Å idÃ¡kè£œæ­£** | $\alpha_{\text{adj}} = 1 - (1 - \alpha)^{1/m}$ | ç‹¬ç«‹æ€§ä»®å®š |

**Holmæ³•ã®æ‰‹é †**:

1. på€¤ã‚’æ˜‡é †ã«ä¸¦ã¹ã‚‹: $p_{(1)} \leq p_{(2)} \leq \cdots \leq p_{(m)}$
2. $i = 1, 2, \ldots$ ã®é †ã«ä»¥ä¸‹ã‚’ãƒã‚§ãƒƒã‚¯:
   - $p_{(i)} \leq \alpha / (m - i + 1)$ ãªã‚‰æ£„å´ã€æ¬¡ã¸
   - åˆã‚ã¦ä¸ç­‰å¼ãŒæˆç«‹ã—ãªã‹ã£ãŸã‚‰åœæ­¢

#### 3.6.2 FDRåˆ¶å¾¡æ³•

**Benjamini-Hochbergæ³•** [^2]:

1. på€¤ã‚’æ˜‡é †ã«ä¸¦ã¹ã‚‹: $p_{(1)} \leq p_{(2)} \leq \cdots \leq p_{(m)}$
2. $i = m, m-1, \ldots, 1$ ã®é †ã«ä»¥ä¸‹ã‚’ãƒã‚§ãƒƒã‚¯:
   - $p_{(i)} \leq \frac{i}{m} \alpha$ ãªã‚‰ $i$ ç•ªç›®ã¾ã§å…¨ã¦æ£„å´ã€åœæ­¢
   - æˆç«‹ã—ãªã‘ã‚Œã°æ¬¡ã¸

**æ•°å¼å°å‡º**:

FDRã®å®šç¾©:

$$
\text{FDR} = \mathbb{E}\left[\frac{V}{R}\right]
$$

ã“ã“ã§ $V$ = å½é™½æ€§æ•°ã€$R$ = ç·ç™ºè¦‹æ•°ï¼ˆ$R = V + S$, $S$ = çœŸé™½æ€§æ•°ï¼‰ã€‚

Benjamini-Hochbergã¯ç‹¬ç«‹ãªæ¤œå®šã«ãŠã„ã¦ $\text{FDR} \leq \alpha$ ã‚’ä¿è¨¼ã™ã‚‹ [^2]ã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using MultipleTesting

# 100å€‹ã®æ¤œå®šï¼ˆ90å€‹ã¯å¸°ç„¡ä»®èª¬ãŒçœŸã€10å€‹ã¯å¯¾ç«‹ä»®èª¬ãŒçœŸï¼‰
p_values_null = rand(100)  # H0ãŒçœŸã®på€¤: ä¸€æ§˜åˆ†å¸ƒ
p_values_alt  = rand(Beta(0.1, 1), 10)  # H1ãŒçœŸã®på€¤: 0ã«åã‚‹
p_values = vcat(p_values_null, p_values_alt)

# è£œæ­£ãªã—
n_sig_uncorrected = sum(p_values .< 0.05)
println("è£œæ­£ãªã—: $(n_sig_uncorrected) / 110 ãŒæœ‰æ„")

# Bonferroniè£œæ­£
p_bonf = adjust(PValues(p_values), Bonferroni())
n_sig_bonf = sum(p_bonf .< 0.05)
println("Bonferroni: $(n_sig_bonf) / 110 ãŒæœ‰æ„")

# Benjamini-Hochberg (FDR)
p_bh = adjust(PValues(p_values), BenjaminiHochberg())
n_sig_bh = sum(p_bh .< 0.05)
println("Benjamini-Hochberg: $(n_sig_bh) / 110 ãŒæœ‰æ„")
```

å‡ºåŠ›ä¾‹:
```
è£œæ­£ãªã—: 15 / 110 ãŒæœ‰æ„
Bonferroni: 3 / 110 ãŒæœ‰æ„
Benjamini-Hochberg: 9 / 110 ãŒæœ‰æ„
```

> **Note:** **é€²æ—: 75% å®Œäº†** å¤šé‡æ¯”è¼ƒè£œæ­£ï¼ˆFWER/FDRï¼‰ã‚’å®Œå…¨ç†è§£ã€‚GLMç†è«–ã¸ã€‚

### 3.7 ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆGLMï¼‰

**å•é¡Œ**: ç·šå½¢å›å¸° $y = X\beta + \epsilon$ ã¯é€£ç¶šå€¤ãƒ»æ­£è¦åˆ†å¸ƒã‚’ä»®å®šã€‚ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ï¼ˆåˆ†é¡ï¼‰ã‚„ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯ä¸é©ã€‚

**GLMã®æ§‹æˆè¦ç´ **:

1. **æŒ‡æ•°å‹åˆ†å¸ƒæ—**: å¿œç­”å¤‰æ•° $y$ ã®åˆ†å¸ƒï¼ˆæ­£è¦ãƒ»äºŒé …ãƒ»ãƒã‚¢ã‚½ãƒ³ç­‰ï¼‰ã€‚
2. **ãƒªãƒ³ã‚¯é–¢æ•°** $g(\cdot)$: å¹³å‡ $\mu = \mathbb{E}[y]$ ã‚’ç·šå½¢äºˆæ¸¬å­ $\eta = X\beta$ ã«ç¹‹ãã€‚
3. **ç·šå½¢äºˆæ¸¬å­**: $\eta = X\beta$

$$
g(\mu) = X\beta \quad \Rightarrow \quad \mu = g^{-1}(X\beta)
$$

| åˆ†å¸ƒ | å…¸å‹çš„ç”¨é€” | æ¨™æº–çš„ãƒªãƒ³ã‚¯é–¢æ•° |
|:-----|:----------|:----------------|
| æ­£è¦åˆ†å¸ƒ | é€£ç¶šå€¤ | æ’ç­‰ $g(\mu) = \mu$ |
| äºŒé …åˆ†å¸ƒ | åˆ†é¡ | ãƒ­ã‚¸ãƒƒãƒˆ $g(\mu) = \log\frac{\mu}{1-\mu}$ |
| ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ | ã‚«ã‚¦ãƒ³ãƒˆ | å¯¾æ•° $g(\mu) = \log\mu$ |

#### 3.7.1 ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆLogistic Regressionï¼‰

**ç”¨é€”**: äºŒå€¤åˆ†é¡ï¼ˆ$y \in \{0, 1\}$ï¼‰ã€‚

**ãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
y_i &\sim \text{Bernoulli}(p_i) \\
\log\frac{p_i}{1 - p_i} &= \beta_0 + \beta_1 x_i \quad \text{(ãƒ­ã‚¸ãƒƒãƒˆå¤‰æ›)} \\
\Rightarrow \quad p_i &= \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}} \quad \text{(ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°)}
\end{aligned}
$$

**ã‚ªãƒƒã‚ºæ¯”ï¼ˆOdds Ratioï¼‰**: ä¿‚æ•° $\beta_1$ ã®è§£é‡ˆ

$$
\text{OR} = e^{\beta_1}
$$

$x$ ãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€ã‚ªãƒƒã‚ºï¼ˆ$p / (1-p)$ï¼‰ãŒ $e^{\beta_1}$ å€ã«ãªã‚‹ã€‚

**æœ€å°¤æ¨å®š**: å¯¾æ•°å°¤åº¦ã‚’æœ€å¤§åŒ–ã€‚

$$
\ell(\beta) = \sum_{i=1}^n \left[ y_i \log p_i + (1 - y_i) \log(1 - p_i) \right]
$$

å‹¾é…:

$$
\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n (y_i - p_i) x_{ij}
$$

**æ•°å€¤æ¤œè¨¼**:

```julia
using GLM, DataFrames

# ãƒ‡ãƒ¼ã‚¿: xï¼ˆé€£ç¶šå¤‰æ•°ï¼‰, yï¼ˆ0/1ã®ãƒ©ãƒ™ãƒ«ï¼‰
df = DataFrame(
    x = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],
    y = [0, 0, 0, 0, 1, 0, 1, 1, 1, 1]
)

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
model = glm(@formula(y ~ x), df, Binomial(), LogitLink())
println(model)

# ä¿‚æ•°ã®è§£é‡ˆ
Î²1 = coef(model)[2]
OR = exp(Î²1)
println("\nä¿‚æ•°Î²1=$(round(Î²1, digits=3)), ã‚ªãƒƒã‚ºæ¯”OR=$(round(OR, digits=3))")
println("xãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€ã‚ªãƒƒã‚ºãŒ$(round(OR, digits=3))å€ã«ãªã‚‹")

# äºˆæ¸¬
df.y_pred = predict(model, df)
println("\näºˆæ¸¬ç¢ºç‡:")
println(df)
```

#### 3.7.2 ãƒã‚¢ã‚½ãƒ³å›å¸°ï¼ˆPoisson Regressionï¼‰

**ç”¨é€”**: ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆ$y \in \{0, 1, 2, \ldots\}$ï¼‰ã€‚ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿå›æ•°ã®äºˆæ¸¬ã€‚

**ãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
y_i &\sim \text{Poisson}(\lambda_i) \\
\log \lambda_i &= \beta_0 + \beta_1 x_i \quad \text{(å¯¾æ•°ãƒªãƒ³ã‚¯é–¢æ•°)} \\
\Rightarrow \quad \lambda_i &= e^{\beta_0 + \beta_1 x_i}
\end{aligned}
$$

**ä¿‚æ•°ã®è§£é‡ˆ**: $x$ ãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€æœŸå¾…ã‚«ã‚¦ãƒ³ãƒˆ $\lambda$ ãŒ $e^{\beta_1}$ å€ã«ãªã‚‹ã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using GLM, DataFrames, Distributions

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: 1æ™‚é–“ã‚ãŸã‚Šã®ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿå›æ•°ï¼‰
df = DataFrame(
    workload = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # è² è·ãƒ¬ãƒ™ãƒ«
    errors = [2, 3, 3, 5, 6, 8, 9, 12, 14, 16]   # ã‚¨ãƒ©ãƒ¼å›æ•°
)

# ãƒã‚¢ã‚½ãƒ³å›å¸°
model = glm(@formula(errors ~ workload), df, Poisson(), LogLink())
println(model)

# ä¿‚æ•°ã®è§£é‡ˆ
Î²1 = coef(model)[2]
multiplier = exp(Î²1)
println("\nworkloadãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€æœŸå¾…ã‚¨ãƒ©ãƒ¼å›æ•°ãŒ$(round(multiplier, digits=3))å€ã«ãªã‚‹")

# äºˆæ¸¬
df.errors_pred = predict(model, df)
println("\näºˆæ¸¬ã‚¨ãƒ©ãƒ¼å›æ•°:")
println(df)
```

#### 3.7.3 æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®çµ±ä¸€ç†è«–

**GLMã®åŸºç›¤**: æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼ˆExponential Familyï¼‰

$$
p(y | \theta, \phi) = \exp\left(\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)\right)
$$

| é … | åç§° | å½¹å‰² |
|:---|:-----|:-----|
| $\theta$ | è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å¹³å‡ã‚’æ±ºå®š |
| $\phi$ | åˆ†æ•£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | åˆ†æ•£ã‚’æ±ºå®š |
| $b(\theta)$ | ç´¯ç©ç”Ÿæˆé–¢æ•° | å¹³å‡: $\mu = b'(\theta)$ |
| $a(\phi)$ | åˆ†æ•£é–¢æ•° | åˆ†æ•£: $\text{Var}(Y) = b''(\theta) a(\phi)$ |

**ä¸»è¦ãªåˆ†å¸ƒ**:

| åˆ†å¸ƒ | $\theta$ | $b(\theta)$ | $a(\phi)$ | $\mu = b'(\theta)$ |
|:-----|:---------|:-----------|:----------|:------------------|
| æ­£è¦åˆ†å¸ƒ | $\mu$ | $\theta^2 / 2$ | $\sigma^2$ | $\theta$ |
| äºŒé …åˆ†å¸ƒ | $\log \frac{p}{1-p}$ | $\log(1 + e^\theta)$ | $1$ | $\frac{e^\theta}{1 + e^\theta}$ |
| ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ | $\log \lambda$ | $e^\theta$ | $1$ | $e^\theta$ |

**GLMã®çµ±ä¸€æ§‹é€ **:

1. **ãƒ©ãƒ³ãƒ€ãƒ æˆåˆ†**: å¿œç­”å¤‰æ•° $y$ ãŒæŒ‡æ•°å‹åˆ†å¸ƒæ—ã«å¾“ã†ã€‚
2. **ç·šå½¢äºˆæ¸¬å­**: $\eta = X\beta$
3. **ãƒªãƒ³ã‚¯é–¢æ•°**: $g(\mu) = \eta$ï¼ˆæ¨™æº–çš„ãƒªãƒ³ã‚¯é–¢æ•°: $g(\mu) = \theta$ï¼‰

> **Note:** **é€²æ—: 80% å®Œäº†** GLMç†è«–ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒ»ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ»æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼‰ã‚’ç†è§£ã€‚ãƒ™ã‚¤ã‚ºçµ±è¨ˆã¸ã€‚

### 3.8 ãƒ™ã‚¤ã‚ºçµ±è¨ˆå…¥é–€

#### 3.8.1 ãƒ™ã‚¤ã‚ºã®å®šç†ã®å°å‡º

**ç¬¬4å›ã§å­¦ã‚“ã æ¡ä»¶ä»˜ãç¢ºç‡ã®å®šç¾©**:

$$
p(\theta | D) = \frac{p(\theta, D)}{p(D)}, \quad p(D | \theta) = \frac{p(\theta, D)}{p(\theta)}
$$

ä¸¡è¾ºã« $p(\theta)$ ã‚’æ›ã‘ã‚‹ã¨:

$$
p(\theta, D) = p(D | \theta) p(\theta) = p(\theta | D) p(D)
$$

ã‚ˆã£ã¦:

$$
p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)}
$$

ã“ã‚ŒãŒ**ãƒ™ã‚¤ã‚ºã®å®šç†**ã ã€‚

| é … | åç§° | æ„å‘³ |
|:---|:-----|:-----|
| $p(\theta \| D)$ | äº‹å¾Œåˆ†å¸ƒï¼ˆPosteriorï¼‰ | ãƒ‡ãƒ¼ã‚¿è¦³æ¸¬å¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒ |
| $p(D \| \theta)$ | å°¤åº¦ï¼ˆLikelihoodï¼‰ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸‹ã§ã®ãƒ‡ãƒ¼ã‚¿ã®ç¢ºç‡ |
| $p(\theta)$ | äº‹å‰åˆ†å¸ƒï¼ˆPriorï¼‰ | ãƒ‡ãƒ¼ã‚¿è¦³æ¸¬å‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¿¡å¿µ |
| $p(D)$ | å‘¨è¾ºå°¤åº¦ï¼ˆEvidenceï¼‰ | æ­£è¦åŒ–å®šæ•° $p(D) = \int p(D \| \theta) p(\theta) d\theta$ |

#### 3.8.2 é »åº¦è«–çµ±è¨ˆ vs ãƒ™ã‚¤ã‚ºçµ±è¨ˆ

**å“²å­¦çš„å¯¾ç«‹**:

| é …ç›® | é »åº¦è«– | ãƒ™ã‚¤ã‚º |
|:-----|:------|:-------|
| **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ€§è³ª** | å›ºå®šå€¤ï¼ˆæœªçŸ¥ï¼‰ | ç¢ºç‡å¤‰æ•° |
| **ç¢ºç‡ã®è§£é‡ˆ** | é•·æœŸçš„é »åº¦ | ä¿¡å¿µã®åº¦åˆã„ |
| **æ¨è«–ã®å¯¾è±¡** | ç‚¹æ¨å®šãƒ»ä¿¡é ¼åŒºé–“ | äº‹å¾Œåˆ†å¸ƒå…¨ä½“ |
| **ä¸ç¢ºå®Ÿæ€§ã®è¡¨ç¾** | æ¨™æº–èª¤å·® | äº‹å¾Œåˆ†å¸ƒã®å¹… |
| **äº‹å‰çŸ¥è­˜** | ä½¿ã‚ãªã„ï¼ˆå®¢è¦³æ€§ï¼‰ | ä½¿ã†ï¼ˆä¸»è¦³æ€§ï¼‰ |

**å…·ä½“ä¾‹**: ã‚³ã‚¤ãƒ³æŠ•ã’ï¼ˆ10å›ä¸­7å›è¡¨ï¼‰

**é »åº¦è«–çš„æ¨å®š**ï¼ˆç¬¬7å›ã®MLEï¼‰:

$$
\hat{\theta}_{\text{MLE}} = \frac{k}{n} = \frac{7}{10} = 0.7
$$

95%ä¿¡é ¼åŒºé–“ï¼ˆWaldæ³•ï¼‰:

$$
\text{CI} = \hat{\theta} \pm 1.96 \sqrt{\frac{\hat{\theta}(1-\hat{\theta})}{n}} = 0.7 \pm 1.96 \sqrt{\frac{0.7 \times 0.3}{10}} = [0.416, 0.984]
$$

**ãƒ™ã‚¤ã‚ºæ¨å®š**ï¼ˆäº‹å‰åˆ†å¸ƒBeta(2,2)ã€å…±å½¹æ€§ã‚ˆã‚Šäº‹å¾Œåˆ†å¸ƒBeta(9, 5)ï¼‰:

$$
p(\theta | k=7, n=10) = \text{Beta}(9, 5)
$$

äº‹å¾Œå¹³å‡ï¼ˆç‚¹æ¨å®šï¼‰:

$$
\mathbb{E}[\theta | D] = \frac{\alpha}{\alpha + \beta} = \frac{9}{9+5} = 0.643
$$

95%ä¿¡ç”¨åŒºé–“ï¼ˆCredible Intervalï¼‰:

$$
\text{CrI} = [\text{quantile}(0.025), \text{quantile}(0.975)] \approx [0.366, 0.882]
$$

**è§£é‡ˆã®é•ã„**:

- **é »åº¦è«–CI**: ã€ŒåŒã˜å®Ÿé¨“ã‚’100å›ç¹°ã‚Šè¿”ã›ã°ã€95å›ã¯ã“ã®åŒºé–“ãŒçœŸã® $\theta$ ã‚’å«ã‚€ã€
- **ãƒ™ã‚¤ã‚ºCrI**: ã€Œãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ãŸä»Šã€$\theta$ ãŒã“ã®åŒºé–“ã«ã‚ã‚‹ç¢ºç‡ãŒ95%ã€ï¼ˆã‚ˆã‚Šç›´æ„Ÿçš„ï¼‰

#### 3.8.1 å…±å½¹äº‹å‰åˆ†å¸ƒ

**å®šç¾©**: äº‹å‰åˆ†å¸ƒã¨äº‹å¾Œåˆ†å¸ƒãŒåŒã˜åˆ†å¸ƒæ—ã«å±ã™ã‚‹ã¨ãã€ãã®äº‹å‰åˆ†å¸ƒã‚’å…±å½¹ã¨ã„ã†ã€‚

| å°¤åº¦ | å…±å½¹äº‹å‰åˆ†å¸ƒ | äº‹å¾Œåˆ†å¸ƒ |
|:-----|:-----------|:--------|
| äºŒé …åˆ†å¸ƒ | ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ | ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ |
| æ­£è¦åˆ†å¸ƒï¼ˆæ—¢çŸ¥åˆ†æ•£ï¼‰ | æ­£è¦åˆ†å¸ƒ | æ­£è¦åˆ†å¸ƒ |
| ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ | ã‚¬ãƒ³ãƒåˆ†å¸ƒ | ã‚¬ãƒ³ãƒåˆ†å¸ƒ |

**ä¾‹**: ã‚³ã‚¤ãƒ³æŠ•ã’ï¼ˆäºŒé …åˆ†å¸ƒï¼‰+ ãƒ™ãƒ¼ã‚¿äº‹å‰åˆ†å¸ƒ

$$
\begin{aligned}
\text{å°¤åº¦:} \quad & p(k | n, \theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k} \\
\text{äº‹å‰åˆ†å¸ƒ:} \quad & p(\theta) = \text{Beta}(\alpha, \beta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1} \\
\text{äº‹å¾Œåˆ†å¸ƒ:} \quad & p(\theta | k, n) = \text{Beta}(\alpha + k, \beta + n - k)
\end{aligned}
$$

**æ•°å€¤æ¤œè¨¼**:

```julia
using Distributions, Plots

# äº‹å‰åˆ†å¸ƒ: Beta(2, 2) (å¼±ã„ä¿¡å¿µ: Î¸â‰ˆ0.5)
Î±, Î² = 2.0, 2.0
prior = Beta(Î±, Î²)

# ãƒ‡ãƒ¼ã‚¿: 10å›æŠ•ã’ã¦7å›è¡¨
n, k = 10, 7

# äº‹å¾Œåˆ†å¸ƒ: Beta(Î±+k, Î²+n-k) = Beta(9, 5)
posterior = Beta(Î± + k, Î² + n - k)

# å¯è¦–åŒ–
Î¸_range = 0:0.01:1
plot(Î¸_range, pdf.(prior, Î¸_range), label="äº‹å‰åˆ†å¸ƒ Beta(2,2)", linewidth=2)
plot!(Î¸_range, pdf.(posterior, Î¸_range), label="äº‹å¾Œåˆ†å¸ƒ Beta(9,5)", linewidth=2)
xlabel!("Î¸ (ã‚³ã‚¤ãƒ³ãŒè¡¨ã®ç¢ºç‡)")
ylabel!("å¯†åº¦")
title!("ãƒ™ã‚¤ã‚ºæ›´æ–°: ã‚³ã‚¤ãƒ³æŠ•ã’")
savefig("bayesian_update.png")
```

#### 3.8.2 MCMCï¼ˆMarkov Chain Monte Carloï¼‰

**å•é¡Œ**: äº‹å¾Œåˆ†å¸ƒ $p(\theta | D)$ ãŒè¤‡é›‘ã§è§£æçš„ã«è¨ˆç®—ã§ããªã„ã€‚

**MCMC**: ãƒãƒ«ã‚³ãƒ•é€£é–ã‚’ä½¿ã£ã¦äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã€‚

**Metropolis-Hastingsæ³•** [^3]:

1. åˆæœŸå€¤ $\theta^{(0)}$ ã‚’è¨­å®šã€‚
2. $t = 1, 2, \ldots$ ã«ã¤ã„ã¦:
   - ææ¡ˆåˆ†å¸ƒ $q(\theta' | \theta^{(t-1)})$ ã‹ã‚‰å€™è£œ $\theta'$ ã‚’ç”Ÿæˆã€‚
   - å—ç†ç¢ºç‡ã‚’è¨ˆç®—:
     $$
     \alpha = \min\left(1, \frac{p(\theta' | D) q(\theta^{(t-1)} | \theta')}{p(\theta^{(t-1)} | D) q(\theta' | \theta^{(t-1)})}\right)
     $$
   - ç¢ºç‡ $\alpha$ ã§ $\theta^{(t)} = \theta'$ã€ãã†ã§ãªã‘ã‚Œã° $\theta^{(t)} = \theta^{(t-1)}$ã€‚

**Turing.jlã§å®Ÿè£…**:

```julia
using Turing, Distributions, StatsPlots

# ãƒ¢ãƒ‡ãƒ«å®šç¾©: ã‚³ã‚¤ãƒ³æŠ•ã’ï¼ˆãƒ™ã‚¤ã‚ºæ¨å®šï¼‰
@model function coinflip(y)
    # äº‹å‰åˆ†å¸ƒ
    Î¸ ~ Beta(2, 2)

    # å°¤åº¦
    y ~ Binomial(length(y), Î¸)
end

# ãƒ‡ãƒ¼ã‚¿: 10å›ä¸­7å›è¡¨
data = 7

# MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆNUTS: No-U-Turn Sampler, Hamiltonian Monte Carloã®æ”¹è‰¯ç‰ˆï¼‰
chain = sample(coinflip([data]), NUTS(), 1000)

# äº‹å¾Œåˆ†å¸ƒã®å¯è¦–åŒ–
plot(chain)
```

> **Note:** **é€²æ—: 90% å®Œäº†** ãƒ™ã‚¤ã‚ºçµ±è¨ˆï¼ˆå…±å½¹äº‹å‰åˆ†å¸ƒãƒ»MCMCï¼‰ã‚’å®Œå…¨ç†è§£ã€‚å®Ÿé¨“è¨ˆç”»æ³•ã¸ã€‚

### 3.9 å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆExperimental Designï¼‰

**ç›®çš„**: é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã§æœ€å¤§ã®æƒ…å ±ã‚’å¾—ã‚‹å®Ÿé¨“ã‚’è¨­è¨ˆã™ã‚‹ã€‚

#### 3.9.1 å®Œå…¨ç„¡ä½œç‚ºåŒ–ãƒ‡ã‚¶ã‚¤ãƒ³ï¼ˆCompletely Randomized Design, CRDï¼‰

**ç‰¹å¾´**: å‡¦ç†ï¼ˆtreatmentï¼‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å‰²ã‚Šå½“ã¦ã‚‹ã€‚æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã€‚

**æ¬ ç‚¹**: ãƒ–ãƒ­ãƒƒã‚¯é–“ã®å¤‰å‹•ï¼ˆä¾‹: æ¸¬å®šæ—¥ã®é•ã„ï¼‰ã‚’åˆ¶å¾¡ã§ããªã„ã€‚

#### 3.9.2 ä¹±å¡Šæ³•ï¼ˆRandomized Block Design, RBDï¼‰

**ç‰¹å¾´**: è¢«é¨“è€…ã‚’ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆä¾‹: å¹´é½¢å±¤ã€æ¸¬å®šæ—¥ï¼‰ã«åˆ†ã‘ã€å„ãƒ–ãƒ­ãƒƒã‚¯å†…ã§å‡¦ç†ã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã€‚

**åˆ©ç‚¹**: ãƒ–ãƒ­ãƒƒã‚¯é–“å¤‰å‹•ã‚’é™¤å» â†’ æ®‹å·®ãŒå°ã•ããªã‚‹ â†’ æ¤œå‡ºåŠ›å‘ä¸Šã€‚

#### 3.9.3 ãƒ©ãƒ†ãƒ³æ–¹æ ¼ï¼ˆLatin Square Designï¼‰

**ç‰¹å¾´**: 2ã¤ã®è¦å› ï¼ˆä¾‹: è¡Œ=æ—¥ã€åˆ—=æ©Ÿæ¢°ï¼‰ã‚’åŒæ™‚ã«åˆ¶å¾¡ã€‚

**åˆ¶ç´„**: å‡¦ç†æ•° = è¡Œæ•° = åˆ—æ•°ã€‚

#### 3.9.4 ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨­è¨ˆï¼ˆPower Analysisï¼‰

**å•é¡Œ**: å®Ÿé¨“å‰ã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’æ±ºå®šã€‚

**æ‰‹é †**:

1. æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœé‡ $d$ ã‚’è¨­å®šï¼ˆéå»ã®ç ”ç©¶ã‚„äºˆå‚™å®Ÿé¨“ã‹ã‚‰ï¼‰ã€‚
2. æœ‰æ„æ°´æº– $\alpha$ ã‚’è¨­å®šï¼ˆé€šå¸¸0.05ï¼‰ã€‚
3. ç›®æ¨™æ¤œå‡ºåŠ› $1 - \beta$ ã‚’è¨­å®šï¼ˆé€šå¸¸0.8ï¼‰ã€‚
4. æ¤œå®šã®ç¨®é¡ã«å¿œã˜ãŸå…¬å¼ã¾ãŸã¯ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã§ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã€‚

**tæ¤œå®šã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºå…¬å¼**ï¼ˆå†æ²ï¼‰:

$$
n = \frac{2(z_{1-\alpha/2} + z_{1-\beta})^2}{d^2}
$$

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œp < 0.05ã§æœ‰æ„ã€ã¨è¨€ãˆã‚‹ã€‚ã ãŒã€ãã‚Œã¯æœ¬å½“ã«**ã‚ãªãŸã®ä¸»å¼µ**ã‚’æ”¯æŒã—ã¦ã„ã‚‹ã®ã‹ï¼Ÿ**

ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã‚’è€ƒãˆã‚ˆã†:

1. **ã‚·ãƒŠãƒªã‚ªA**: æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ã‚’10ç¨®é¡è©¦ã—ã€1ã¤ã ã‘p < 0.05ã§æœ‰æ„ãªæ”¹å–„ã€‚ä»–9ã¤ã¯æœ‰æ„å·®ãªã—ã€‚
2. **ã‚·ãƒŠãƒªã‚ªB**: åŒã˜å®Ÿé¨“ã‚’100å›è¡Œã„ã€æœ‰æ„ã ã£ãŸ5å›ã ã‘è«–æ–‡ã«å ±å‘Šã€‚
3. **ã‚·ãƒŠãƒªã‚ªC**: ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã‹ã‚‰ã€Œã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯åŠ¹æœãŒã‚ã‚‹ã€ã¨äº‹å¾Œçš„ã«ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æã€‚

**å…¨ã¦çµ±è¨ˆçš„ã«ã¯ã€Œp < 0.05ã€ã ãŒã€ç§‘å­¦çš„ã«ã¯ç„¡æ„å‘³ã ã€‚**

- **ã‚·ãƒŠãƒªã‚ªA**: å¤šé‡æ¯”è¼ƒã®ç½ ã€‚Bonferroniè£œæ­£ã™ã‚Œã°p = 0.05 Ã— 10 = 0.5ã§æœ‰æ„ã§ãªã„ã€‚
- **ã‚·ãƒŠãƒªã‚ªB**: å‡ºç‰ˆãƒã‚¤ã‚¢ã‚¹ã€‚å¤±æ•—ã—ãŸ95å›ã‚’éš è”½ã€‚
- **ã‚·ãƒŠãƒªã‚ªC**: p-hackingã€‚ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã‹ã‚‰ä»®èª¬ã‚’ç«‹ã¦ã‚‹ã€‚

**è­°è«–ã®ç¨®**:

1. **äº‹å‰ç™»éŒ²ï¼ˆPre-registrationï¼‰**ã¯è§£æ±ºç­–ã‹ï¼Ÿã€€å®Ÿé¨“å‰ã«ä»®èª¬ãƒ»æ‰‹æ³•ã‚’å…¬é–‹ç™»éŒ²ã™ã‚Œã°ã€p-hackingã‚’é˜²ã’ã‚‹ã€‚ã ãŒæŸ”è»Ÿæ€§ãŒå¤±ã‚ã‚Œã‚‹ã€‚
2. **på€¤ã®ä»£æ›¿æ¡ˆ**ã¯ï¼Ÿã€€ä¿¡é ¼åŒºé–“ãƒ»åŠ¹æœé‡ãƒ»ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¯ã€på€¤ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã‹ï¼Ÿ
3. **çµ±è¨ˆçš„æœ‰æ„æ€§ã®åŸºæº–ï¼ˆÎ±=0.05ï¼‰**ã¯æ£æ„çš„ã§ã¯ãªã„ã‹ï¼Ÿã€€ãªãœ0.05ãªã®ã‹ï¼Ÿã€€0.01ã‚„0.001ã§ã¯ãƒ€ãƒ¡ãªã®ã‹ï¼Ÿ

ã“ã®å•ã„ã«å®Œå…¨ãªç­”ãˆã¯ãªã„ã€‚ã ãŒ**çµ±è¨ˆå­¦ã¯é“å…·ã§ã‚ã‚Šã€é“å…·ã®ä½¿ã„æ–¹æ¬¡ç¬¬ã§ç§‘å­¦çš„èª å®Ÿã•ãŒå•ã‚ã‚Œã‚‹**ã“ã¨ã‚’å¿˜ã‚Œã¦ã¯ãªã‚‰ãªã„ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼

---


> Progress: [85%]
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. ANOVAã®Fçµ±è¨ˆé‡ãŒç¾¤é–“åˆ†æ•£ã¨ç¾¤å†…åˆ†æ•£ã®æ¯”ã§æ§‹æˆã•ã‚Œã‚‹æ•°å­¦çš„æ„å‘³ã‚’è¿°ã¹ã‚ˆã€‚
> 2. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒªãƒ³ã‚¯é–¢æ•°ãŒlogitã§ã‚ã‚‹ç†ç”±ã‚’ç¢ºç‡ã®ç¯„å›²ã®åˆ¶ç´„ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Neyman, J., & Pearson, E. S. (1928). *On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: Part I*. Biometrika.
<https://www.jstor.org/stable/2331945>

[^2]: Benjamini, Y., & Hochberg, Y. (1995). *Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing*. Journal of the Royal Statistical Society: Series B.
<https://doi.org/10.1111/j.2517-6161.1995.tb02031.x>

[^3]: Hastings, W. K. (1970). *Monte Carlo Sampling Methods Using Markov Chains and Their Applications*. Biometrika.
<https://doi.org/10.1093/biomet/57.1.97>


### æ•™ç§‘æ›¸

- **Statistical Inference** - Casella & Berger (2002): é »åº¦è«–çµ±è¨ˆã®æ±ºå®šç‰ˆã€‚å¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã€‚
- **Bayesian Data Analysis** - Gelman et al. (2013): ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®æ¨™æº–æ•™ç§‘æ›¸ã€‚
- **The Elements of Statistical Learning** - Hastie, Tibshirani, Friedman (2009): æ©Ÿæ¢°å­¦ç¿’Ã—çµ±è¨ˆã®èåˆã€‚[ç„¡æ–™PDF](https://web.stanford.edu/~hastie/ElemStatLearn/)
- **çµ±è¨ˆå­¦å…¥é–€** - æ±äº¬å¤§å­¦æ•™é¤Šå­¦éƒ¨çµ±è¨ˆå­¦æ•™å®¤ (1991): æ—¥æœ¬èªã®å®šç•ªå…¥é–€æ›¸ã€‚

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- [StatQuest (YouTube)](https://www.youtube.com/@statquest): çµ±è¨ˆå­¦ã®ç›´æ„Ÿçš„è§£èª¬å‹•ç”»ã€‚
- [StatsBase.jl Documentation](https://juliastats.org/StatsBase.jl/stable/)
- [HypothesisTests.jl Documentation](https://juliastats.org/HypothesisTests.jl/stable/)
- [GLM.jl Documentation](https://juliastats.org/GLM.jl/stable/)
- [Turing.jl Documentation](https://turinglang.org/stable/)

---

## ä»˜éŒ²A: çµ±è¨ˆå­¦ã®æ­´å²çš„ç™ºå±•

### A.1 é »åº¦è«–çµ±è¨ˆã®èª•ç”Ÿï¼ˆ1900-1950å¹´ä»£ï¼‰

| å¹´ | äººç‰© | è²¢çŒ® |
|:---|:-----|:-----|
| 1900 | Karl Pearson | ã‚«ã‚¤äºŒä¹—æ¤œå®šã€Pearsonç›¸é–¢ä¿‚æ•° |
| 1908 | William Gosset (Student) | tåˆ†å¸ƒã€tæ¤œå®šï¼ˆå°‘ã‚µãƒ³ãƒ—ãƒ«çµ±è¨ˆï¼‰ |
| 1920å¹´ä»£ | Ronald Fisher | æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã€åˆ†æ•£åˆ†æï¼ˆANOVAï¼‰ã€å®Ÿé¨“è¨ˆç”»æ³• |
| 1928 | Neyman & Pearson | Neyman-Pearsonä»®èª¬æ¤œå®šæ çµ„ã¿ [^1] |
| 1935 | Fisher | ãƒ©ãƒ³ãƒ€ãƒ åŒ–æ¯”è¼ƒè©¦é¨“ï¼ˆRCTï¼‰ã®åŸç† |

**é »åº¦è«–ã®å“²å­¦**: ç¢ºç‡ = é•·æœŸçš„é »åº¦ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å›ºå®šå€¤ï¼ˆæœªçŸ¥ï¼‰ã€‚å®¢è¦³æ€§ã‚’é‡è¦–ã€‚

### A.2 ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®å¾©èˆˆï¼ˆ1950-1990å¹´ä»£ï¼‰

| å¹´ | äººç‰©/å‡ºæ¥äº‹ | è²¢çŒ® |
|:---|:----------|:-----|
| 1763 | Thomas Bayesï¼ˆæ­»å¾Œå‡ºç‰ˆï¼‰ | ãƒ™ã‚¤ã‚ºã®å®šç†ã®åŸå‹ |
| 1950å¹´ä»£ | Dennis Lindley | ãƒ™ã‚¤ã‚ºæ±ºå®šç†è«– |
| 1953 | Metropolis et al. | Metropolisã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆMCMCï¼‰ [^3] |
| 1970 | Hastings | Metropolis-Hastingsã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
| 1990 | Gelfand & Smith | Gibbs Samplingã®å®Ÿç”¨åŒ– |

**ãƒ™ã‚¤ã‚ºå¾©èˆˆã®ç†ç”±**: ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®ç™ºå±•ã§MCMCãŒå®Ÿç”¨åŒ– â†’ è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã®äº‹å¾Œåˆ†å¸ƒã‚’è¨ˆç®—å¯èƒ½ã«ã€‚

### A.3 ç¾ä»£çµ±è¨ˆå­¦ï¼ˆ1990å¹´ä»£ã€œç¾åœ¨ï¼‰

| å¹´ | æ‰‹æ³• | è²¢çŒ® |
|:---|:-----|:-----|
| 1995 | Benjamini & Hochberg | FDRåˆ¶å¾¡æ³•ï¼ˆå¤šé‡æ¯”è¼ƒï¼‰ [^2] |
| 2000å¹´ä»£ | ãƒ™ã‚¤ã‚ºãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ç„¡é™æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«ï¼ˆDirichlet Processç­‰ï¼‰ |
| 2010å¹´ä»£ | Hamiltonian Monte Carlo (HMC) | é«˜æ¬¡å…ƒMCMCã®é«˜é€ŸåŒ–ï¼ˆNUTSï¼‰ |
| 2015å¹´ä»£ | å› æœæ¨è«–ã®æ™®åŠ | Pearl/Rubinæ çµ„ã¿ã®çµ±åˆã€æ©Ÿæ¢°å­¦ç¿’ã¨ã®èåˆ |
| 2020å¹´ä»£ | ç¢ºç‡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° | Turing.jl, PyMC, Stanç­‰ã®æˆç†Ÿ |

---

## ä»˜éŒ²B: Juliaã§ä½¿ãˆã‚‹çµ±è¨ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å®Œå…¨ãƒªã‚¹ãƒˆ

### B.1 åŸºç¤çµ±è¨ˆ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **Statistics** (stdlib) | åŸºæœ¬çµ±è¨ˆé‡ | `mean`, `std`, `var`, `median`, `quantile`, `cor`, `cov` |
| **StatsBase.jl** | è¨˜è¿°çµ±è¨ˆãƒ»é‡ã¿ä»˜ãçµ±è¨ˆ | `skewness`, `kurtosis`, `mad`, `mode`, `sem`, `zscore`, `sample`, `weights` |
| **Distributions.jl** | ç¢ºç‡åˆ†å¸ƒ | `Normal`, `Beta`, `Gamma`, `Binomial`, `Poisson`, `TDist`, `FDist`, `pdf`, `cdf`, `quantile`, `rand` |

### B.2 ä»®èª¬æ¤œå®š

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦æ¤œå®š |
|:----------|:-----|:---------|
| **HypothesisTests.jl** | ä»®èª¬æ¤œå®šå…¨èˆ¬ | `OneSampleTTest`, `EqualVarianceTTest`, `UnequalVarianceTTest`, `MannWhitneyUTest`, `WilcoxonSignedRankTest`, `KruskalWallisTest`, `OneWayANOVATest`, `ChisqTest`, `FisherExactTest`, `KSTest`, `AndersonDarlingTest` |
| **MultipleTesting.jl** | å¤šé‡æ¯”è¼ƒè£œæ­£ | `adjust`, `Bonferroni`, `Holm`, `BenjaminiHochberg`, `BenjaminiYekutieli` |

### B.3 å›å¸°ãƒ»GLM

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **GLM.jl** | ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ« | `glm`, `@formula`, `Binomial`, `Poisson`, `Gamma`, `LogitLink`, `LogLink`, `InverseLink`, `coef`, `confint`, `predict` |
| **MixedModels.jl** | æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ« | `LinearMixedModel`, `fit!`, `ranef`, `fixef` |

### B.4 ãƒ™ã‚¤ã‚ºçµ±è¨ˆ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•°/ãƒã‚¯ãƒ­ |
|:----------|:-----|:---------------|
| **Turing.jl** | ç¢ºç‡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° | `@model`, `~`, `sample`, `NUTS`, `HMC`, `Gibbs`, `plot`, `summarize` |
| **AdvancedMH.jl** | MCMCæ‹¡å¼µ | `MetropolisHastings`, `RWMH`, `StaticMH` |
| **MCMCChains.jl** | MCMCçµæœã®è§£æ | `Chains`, `describe`, `plot`, `ess`, `gelmandiag` |
| **AbstractMCMC.jl** | MCMCã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ | MCMCå®Ÿè£…ã®å…±é€šåŸºç›¤ |

### B.5 ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ãƒ»ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **Bootstrap.jl** | ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³• | `bootstrap`, `BasicSampling`, `confint`, `PercentileConfInt`, `BCaConfInt` |

### B.6 ç”Ÿå­˜æ™‚é–“è§£æ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **Survival.jl** | ç”Ÿå­˜æ™‚é–“è§£æ | `Surv`, `kaplan_meier`, `cox_ph`, `nelson_aalen` |

### B.7 æ™‚ç³»åˆ—è§£æ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **TimeSeries.jl** | æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ | `TimeArray`, `values`, `timestamp`, `lag`, `lead`, `diff` |
| **StateSpaceModels.jl** | çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ« | `StateSpaceModel`, `kalman_filter`, `smoother` |

### B.8 å®Ÿé¨“è¨ˆç”»æ³•

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **ExperimentalDesign.jl** | å®Ÿé¨“è¨ˆç”» | `factorial_design`, `latin_square`, `balanced_design` |

### B.9 å¯è¦–åŒ–

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **StatsPlots.jl** | çµ±è¨ˆçš„ãƒ—ãƒ­ãƒƒãƒˆ | `boxplot`, `violin`, `density`, `marginalscatter`, `corrplot`, `@df` |
| **Makie.jl** | é«˜å“è³ªå¯è¦–åŒ– | `scatter`, `lines`, `barplot`, `heatmap`, `density` |
| **AlgebraOfGraphics.jl** | Grammar of Graphics | `data`, `mapping`, `visual`, `draw` |

---

## ä»˜éŒ²C: çµ±è¨ˆå­¦ã®ä¸»è¦å®šç†ã¾ã¨ã‚

### C.1 ç¢ºç‡è«–ã®åŸºç¤å®šç†

**å¤§æ•°ã®æ³•å‰‡ï¼ˆLaw of Large Numbersï¼‰**:

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{p} \mu \quad \text{as } n \to \infty
$$

æ¨™æœ¬å¹³å‡ã¯æ¯å¹³å‡ã«ç¢ºç‡åæŸã™ã‚‹ã€‚

**ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCentral Limit Theoremï¼‰**:

$$
\sqrt{n} \frac{\bar{X}_n - \mu}{\sigma} \xrightarrow{d} \mathcal{N}(0, 1) \quad \text{as } n \to \infty
$$

æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒã¯æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãï¼ˆæ¯é›†å›£åˆ†å¸ƒã«é–¢ã‚ã‚‰ãšï¼‰ã€‚

### C.2 æ¨å®šã®ç†è«–

**CramÃ©r-Raoä¸‹ç•Œï¼ˆCramÃ©r-Rao Lower Boundï¼‰**:

ä¸åæ¨å®šé‡ $\hat{\theta}$ ã®åˆ†æ•£ã¯æ¬¡ã®ä¸‹ç•Œã‚’æŒã¤:

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{I(\theta)}
$$

ã“ã“ã§ $I(\theta)$ ã¯Fisheræƒ…å ±é‡ã€‚ç­‰å·æˆç«‹æ™‚ã¯**æœ‰åŠ¹æ¨å®šé‡**ã€‚

**æ¼¸è¿‘æ­£è¦æ€§ï¼ˆAsymptotic Normalityï¼‰**:

MLEã¯æ¼¸è¿‘çš„ã«æ­£è¦åˆ†å¸ƒã«å¾“ã†:

$$
\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta) \xrightarrow{d} \mathcal{N}(0, I(\theta)^{-1})
$$

### C.3 æ¤œå®šã®ç†è«–

**Neyman-Pearsonè£œé¡Œï¼ˆNeyman-Pearson Lemmaï¼‰**:

å°¤åº¦æ¯”æ¤œå®šã¯æ‰€å®šã®æœ‰æ„æ°´æº– $\alpha$ ã§æœ€ã‚‚æ¤œå‡ºåŠ›ãŒé«˜ã„ï¼ˆmost powerful testï¼‰ã€‚

$$
\frac{p(x | H_1)}{p(x | H_0)} > c \quad \Rightarrow \quad \text{reject } H_0
$$

### C.4 ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®å®šç†

**ãƒ™ã‚¤ã‚ºã®å®šç†ï¼ˆBayes' Theoremï¼‰**:

$$
p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)} = \frac{p(D | \theta) p(\theta)}{\int p(D | \theta') p(\theta') d\theta'}
$$

**ãƒãƒ«ã‚³ãƒ•é€£é–ã®åæŸ**:

é©åˆ‡ãªæ¡ä»¶ä¸‹ã§MCMCã‚µãƒ³ãƒ—ãƒ«ã¯äº‹å¾Œåˆ†å¸ƒã«åæŸ:

$$
\lim_{t \to \infty} \theta^{(t)} \sim p(\theta | D)
$$

---

## ä»˜éŒ²D: çµ±è¨ˆå­¦ã®å®Ÿè·µãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### D.1 å®Ÿé¨“å‰ï¼ˆäº‹å‰è¨ˆç”»ï¼‰

- [ ] ç ”ç©¶ä»®èª¬ã‚’æ˜ç¢ºã«å®šç¾©ï¼ˆ$H_0$, $H_1$ï¼‰
- [ ] æœ‰æ„æ°´æº– $\alpha$ ã‚’æ±ºå®šï¼ˆé€šå¸¸0.05ï¼‰
- [ ] ç›®æ¨™æ¤œå‡ºåŠ›ã‚’æ±ºå®šï¼ˆé€šå¸¸0.8ï¼‰
- [ ] æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœé‡ã‚’è¨­å®šï¼ˆéå»ç ”ç©¶ãƒ»äºˆå‚™å®Ÿé¨“ã‹ã‚‰ï¼‰
- [ ] ãƒ‘ãƒ¯ãƒ¼åˆ†æã§å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
- [ ] æ¤œå®šæ‰‹æ³•ã‚’äº‹å‰ã«æ±ºå®šï¼ˆtæ¤œå®šãƒ»ANOVAãƒ»ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ç­‰ï¼‰
- [ ] å¤šé‡æ¯”è¼ƒãŒã‚ã‚‹å ´åˆã¯è£œæ­£æ–¹æ³•ã‚’æ±ºå®šï¼ˆBonferroniãƒ»BHç­‰ï¼‰
- [ ] äº‹å‰ç™»éŒ²ï¼ˆPre-registrationï¼‰ã‚’æ¤œè¨ï¼ˆp-hackingã‚’é˜²ãï¼‰

### D.2 ãƒ‡ãƒ¼ã‚¿åé›†

- [ ] ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã‚’å¾¹åº•
- [ ] ãƒ–ãƒ­ãƒƒã‚¯è¦å› ãŒã‚ã‚Œã°ä¹±å¡Šæ³•ã‚’æ¤œè¨
- [ ] æ¸¬å®šèª¤å·®ã‚’æœ€å°åŒ–ï¼ˆæ©Ÿå™¨ã®æ ¡æ­£ãƒ»ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ¨™æº–åŒ–ï¼‰
- [ ] æ¬ æãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²ãƒ»ç†ç”±ã®è¨˜è¼‰
- [ ] å¤–ã‚Œå€¤ã®è¨˜éŒ²ï¼ˆå‰Šé™¤å‰ã«ç†ç”±ã‚’æ˜è¨˜ï¼‰

### D.3 è¨˜è¿°çµ±è¨ˆ

- [ ] å¹³å‡ãƒ»ä¸­å¤®å€¤ãƒ»æ¨™æº–åå·®ãƒ»IQRã‚’è¨ˆç®—
- [ ] æ­ªåº¦ãƒ»å°–åº¦ã‚’ç¢ºèªï¼ˆåˆ†å¸ƒã®å½¢çŠ¶ï¼‰
- [ ] å¤–ã‚Œå€¤ã®æ¤œå‡ºï¼ˆIQRæ³•ãƒ»Grubbsæ¤œå®šï¼‰
- [ ] ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ»ç®±ã²ã’å›³ã§å¯è¦–åŒ–

### D.4 æ¨æ¸¬çµ±è¨ˆ

- [ ] å‰ææ¡ä»¶ã®ç¢ºèªï¼ˆæ­£è¦æ€§ãƒ»ç­‰åˆ†æ•£æ€§ãƒ»ç‹¬ç«‹æ€§ï¼‰
- [ ] æ­£è¦æ€§æ¤œå®šï¼ˆShapiro-Wilkãƒ»Kolmogorov-Smirnovï¼‰
- [ ] ç­‰åˆ†æ•£æ€§æ¤œå®šï¼ˆLeveneãƒ»Bartlettï¼‰
- [ ] å‰æãŒæº€ãŸã•ã‚Œãªã„å ´åˆã¯ä»£æ›¿æ‰‹æ³•ï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ»å¤‰æ›ãƒ»é ‘å¥ãªæ‰‹æ³•ï¼‰

### D.5 ä»®èª¬æ¤œå®š

- [ ] æ¤œå®šçµ±è¨ˆé‡ï¼ˆt, F, Ï‡Â², Uç­‰ï¼‰ã‚’è¨ˆç®—
- [ ] è‡ªç”±åº¦ã‚’ç¢ºèª
- [ ] på€¤ã‚’è¨ˆç®—
- [ ] åŠ¹æœé‡ï¼ˆCohen's d, partial Î·Â², rÂ²ç­‰ï¼‰ã‚’è¨ˆç®—
- [ ] ä¿¡é ¼åŒºé–“ã‚’ä½µè¨˜
- [ ] å¤šé‡æ¯”è¼ƒè£œæ­£ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

### D.6 çµæœã®å ±å‘Š

- [ ] è¨˜è¿°çµ±è¨ˆï¼ˆM, SD, nï¼‰ã‚’å ±å‘Š
- [ ] æ¤œå®šçµ±è¨ˆé‡ãƒ»è‡ªç”±åº¦ãƒ»på€¤ã‚’å ±å‘Šï¼ˆä¾‹: $t(9) = 60.0, p < .001$ï¼‰
- [ ] åŠ¹æœé‡ã‚’å ±å‘Šï¼ˆä¾‹: $d = 6.0$ï¼‰
- [ ] 95%ä¿¡é ¼åŒºé–“ã‚’å ±å‘Šï¼ˆä¾‹: $95\% \text{CI} [0.768, 0.782]$ï¼‰
- [ ] å¤šé‡æ¯”è¼ƒè£œæ­£æ–¹æ³•ã‚’æ˜è¨˜
- [ ] å›³è¡¨ã§è¦–è¦šåŒ–ï¼ˆç®±ã²ã’å›³ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãæ£’ã‚°ãƒ©ãƒ•ç­‰ï¼‰
- [ ] çµ±è¨ˆçš„æœ‰æ„æ€§ã¨å®Ÿç”¨çš„æœ‰æ„æ€§ã‚’åŒºåˆ¥

### D.7 è§£é‡ˆãƒ»è­°è«–

- [ ] på€¤ã®æ­£ã—ã„è§£é‡ˆï¼ˆã€Œ$H_0$ãŒçœŸã§ã‚ã‚‹ç¢ºç‡ã€ã§ã¯ãªã„ï¼‰
- [ ] åŠ¹æœé‡ã®å®Ÿç”¨çš„æ„ç¾©ã‚’è­°è«–
- [ ] æ¤œå‡ºåŠ›ä¸è¶³ã®å¯èƒ½æ€§ã‚’æ¤œè¨ï¼ˆp > 0.05ã®å ´åˆï¼‰
- [ ] ä»£æ›¿èª¬æ˜ï¼ˆäº¤çµ¡å› å­ï¼‰ã®å¯èƒ½æ€§ã‚’è­°è«–
- [ ] é™ç•Œï¼ˆã‚µãƒ³ãƒ—ãƒ«é¸æŠãƒã‚¤ã‚¢ã‚¹ãƒ»æ¸¬å®šèª¤å·®ç­‰ï¼‰ã‚’æ˜è¨˜
- [ ] å› æœé–¢ä¿‚ã¨ç›¸é–¢ã®åŒºåˆ¥

---

## ä»˜éŒ²B: GLMç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã¨æœ€æ–°æ‰‹æ³•

### B.1 æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆMixed Effects Modelsï¼‰

**å•é¡Œ**: ãƒ‡ãƒ¼ã‚¿ã«éšå±¤æ§‹é€ ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹: ç”Ÿå¾’â†’ã‚¯ãƒ©ã‚¹â†’å­¦æ ¡ï¼‰ã€è¦³æ¸¬ãŒç‹¬ç«‹ã§ãªã„ã€‚

**ç·šå½¢æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆLMMï¼‰**:

$$
y_{ij} = \beta_0 + \beta_1 x_{ij} + u_i + \epsilon_{ij}
$$

ã“ã“ã§:
- $y_{ij}$: ã‚°ãƒ«ãƒ¼ãƒ—$i$ã®è¦³æ¸¬$j$ã®å¿œç­”å¤‰æ•°
- $u_i \sim \mathcal{N}(0, \sigma_u^2)$: ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¬ãƒ™ãƒ«ã®ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ
- $\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$: å€‹ä½“ãƒ¬ãƒ™ãƒ«ã®èª¤å·®

**å›ºå®šåŠ¹æœ vs ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ**:

| é …ç›® | å›ºå®šåŠ¹æœ | ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ |
|:-----|:--------|:-----------|
| è§£é‡ˆ | æ¯é›†å›£å…¨ä½“ã®å¹³å‡åŠ¹æœ | ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ã°ã‚‰ã¤ã |
| æ¨å®š | ä¿‚æ•°$\beta$ | åˆ†æ•£æˆåˆ†$\sigma_u^2$ |
| ç›®çš„ | åŠ¹æœã®å¤§ãã•ã‚’çŸ¥ã‚ŠãŸã„ | ã‚°ãƒ«ãƒ¼ãƒ—é–“å¤‰å‹•ã‚’åˆ¶å¾¡ã—ãŸã„ |

**Juliaå®Ÿè£…ä¾‹**ï¼ˆMixedModels.jlï¼‰:

```julia
using MixedModels, DataFrames, RDatasets

# ãƒ‡ãƒ¼ã‚¿: sleepstudyï¼ˆç¡çœ ä¸è¶³ãŒåå¿œæ™‚é–“ã«ä¸ãˆã‚‹å½±éŸ¿ï¼‰
sleepstudy = dataset("lme4", "sleepstudy")

# æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«: åå¿œæ™‚é–“ ~ æ—¥æ•° + (1 + æ—¥æ•° | è¢«é¨“è€…)
# å›ºå®šåŠ¹æœ: æ—¥æ•°ã®åŠ¹æœ
# ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ: è¢«é¨“è€…ã”ã¨ã®åˆ‡ç‰‡ã¨ã‚¹ãƒ­ãƒ¼ãƒ—
fm = fit(MixedModel, @formula(Reaction ~ Days + (1 + Days | Subject)), sleepstudy)

println(fm)

# ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœã®å¯è¦–åŒ–
ranef_df = DataFrame(ranef(fm)[:Subject])
```

å‡ºåŠ›ä¾‹:
```
Linear mixed model fit by maximum likelihood
 Reaction ~ 1 + Days + (1 + Days | Subject)
   logLik   -2 logLik     AIC       AICc        BIC
  -875.97    1751.94   1763.94   1764.47   1783.10

Variance components:
            Column    Variance   Std.Dev.   Corr.
Subject  (Intercept)  612.100    24.741
         Days          35.072     5.923    0.07
Residual              654.941    25.592
```

### B.2 ä¸€èˆ¬åŒ–åŠ æ³•ãƒ¢ãƒ‡ãƒ«ï¼ˆGAM: Generalized Additive Modelsï¼‰

**å•é¡Œ**: ç·šå½¢æ€§ã®ä»®å®šãŒå³ã—ã™ãã‚‹å ´åˆã€éç·šå½¢é–¢ä¿‚ã‚’æŸ”è»Ÿã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ãŸã„ã€‚

**GAMã®å®šå¼åŒ–**:

$$
g(\mu) = \beta_0 + f_1(x_1) + f_2(x_2) + \cdots + f_p(x_p)
$$

ã“ã“ã§$f_i$ã¯ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é–¢æ•°ï¼ˆã‚¹ãƒ—ãƒ©ã‚¤ãƒ³ç­‰ï¼‰ã€‚

**ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³**:

$$
\min_f \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \int (f''(x))^2 dx
$$

ç¬¬1é …: ãƒ•ã‚£ãƒƒãƒˆã€ç¬¬2é …: æ»‘ã‚‰ã‹ã•ã®ãƒšãƒŠãƒ«ãƒ†ã‚£

**Juliaã§ã®ç°¡æ˜“å®Ÿè£…**:

```julia
using GLM, DataFrames, Plots

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: éç·šå½¢é–¢ä¿‚
x = range(0, 10, length=100)
y_true = sin.(x) .+ 0.5 .* x
y = y_true .+ randn(100) .* 0.3

# å¤šé …å¼åŸºåº•å±•é–‹ã§GAMã‚’è¿‘ä¼¼
function polynomial_features(x, degree)
    hcat([x.^d for d in 0:degree]...)
end

# æ¬¡æ•°5ã®å¤šé …å¼GAM
X_poly = polynomial_features(x, 5)
df = DataFrame(X_poly, :auto)
df.y = y

model = lm(@formula(y ~ x1 + x2 + x3 + x4 + x5), df)

# äºˆæ¸¬ã¨å¯è¦–åŒ–
y_pred = predict(model)

plot(x, y, seriestype=:scatter, label="Data", alpha=0.5)
plot!(x, y_true, linewidth=2, label="True function")
plot!(x, y_pred, linewidth=2, label="GAM fit", linestyle=:dash)
xlabel!("x")
ylabel!("y")
```

### B.3 ã‚¼ãƒ­éå‰°ãƒ¢ãƒ‡ãƒ«ï¼ˆZero-Inflated Modelsï¼‰

**å•é¡Œ**: ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã‚¼ãƒ­ãŒéå‰°ã«å«ã¾ã‚Œã‚‹ï¼ˆä¾‹: ç—…é™¢å—è¨ºå›æ•°ã€äº‹æ•…ä»¶æ•°ï¼‰ã€‚

**ã‚¼ãƒ­éå‰°ãƒã‚¢ã‚½ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆZIPï¼‰**:

$$
P(Y = y) = \begin{cases}
\pi + (1 - \pi) e^{-\lambda} & \text{if } y = 0 \\
(1 - \pi) \frac{\lambda^y e^{-\lambda}}{y!} & \text{if } y > 0
\end{cases}
$$

ã“ã“ã§:
- $\pi$: æ§‹é€ çš„ã‚¼ãƒ­ã®ç¢ºç‡ï¼ˆã€Œæ±ºã—ã¦ã‚¤ãƒ™ãƒ³ãƒˆãŒèµ·ã“ã‚‰ãªã„ã€ï¼‰
- $1 - \pi$: ãƒã‚¢ã‚½ãƒ³éç¨‹ã«å¾“ã†ç¢ºç‡

**2æ®µéšãƒ¢ãƒ‡ãƒ«**:

1. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§$\pi$ã‚’æ¨å®š
2. ãƒã‚¢ã‚½ãƒ³å›å¸°ã§$\lambda$ã‚’æ¨å®š

**æ•°å€¤ä¾‹**:

```julia
using Distributions, Optim

# ZIPå°¤åº¦é–¢æ•°
function zip_loglik(params, y)
    Ï€, Î» = params[1], exp(params[2])  # Î» > 0ã‚’ä¿è¨¼
    ll_zero  = log(Ï€ + (1 - Ï€) * exp(-Î»))
    ll_pos(yi) = log(1 - Ï€) + logpdf(Poisson(Î»), yi)
    -sum(yi == 0 ? ll_zero : ll_pos(yi) for yi in y)  # è² ã®å¯¾æ•°å°¤åº¦ï¼ˆæœ€å°åŒ–ï¼‰
end

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: ã‚¼ãƒ­éå‰°
true_Ï€ = 0.3
true_Î» = 2.0
n = 1000

y = [rand() < true_Ï€ ? 0 : rand(Poisson(true_Î»)) for _ in 1:n]

println("ã‚¼ãƒ­ã®å‰²åˆ: $(sum(y .== 0) / n) (ç†è«–å€¤: $(true_Ï€ + (1-true_Ï€)*exp(-true_Î»)))")

# æœ€å°¤æ¨å®š
result = optimize(p -> zip_loglik(p, y), [0.2, log(2.0)], BFGS())
Ï€_hat, Î»_hat = result.minimizer[1], exp(result.minimizer[2])

println("æ¨å®šå€¤: Ï€=$(round(Ï€_hat, digits=3)), Î»=$(round(Î»_hat, digits=3))")
println("çœŸå€¤: Ï€=$true_Ï€, Î»=$true_Î»")
```

### B.4 æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ï¼ˆTime Series Modelsï¼‰

#### B.4.1 è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆARï¼‰

**AR(p)ãƒ¢ãƒ‡ãƒ«**:

$$
y_t = \phi_0 + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \epsilon_t
$$

ã“ã“ã§$\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ã¯ãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¤ã‚ºã€‚

**å®šå¸¸æ€§æ¡ä»¶**: ç‰¹æ€§æ–¹ç¨‹å¼ã®æ ¹ãŒå˜ä½å††ã®å¤–å´ã«ã‚ã‚‹ã€‚

**Juliaå®Ÿè£…ä¾‹**:

```julia
using LinearAlgebra, Statistics, Plots

# AR(1)ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé€æ¬¡çš„: @inbounds ã§é«˜é€ŸåŒ–ï¼‰
function ar1_simulate(Ï•, Ïƒ, n)
    y = zeros(n)
    y[1] = randn() * Ïƒ / sqrt(1 - Ï•^2)  # å®šå¸¸åˆ†å¸ƒã‹ã‚‰åˆæœŸå€¤
    @inbounds for t in 2:n
        y[t] = Ï• * y[t-1] + randn() * Ïƒ
    end
    return y
end

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
Ï• = 0.8  # è‡ªå·±ç›¸é–¢ä¿‚æ•°
Ïƒ = 1.0
n = 200

y = ar1_simulate(Ï•, Ïƒ, n)

# è‡ªå·±ç›¸é–¢é–¢æ•°ï¼ˆACFï¼‰: @views ã§ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã‚¹ãƒ©ã‚¤ã‚¹ã€dot ã§å†…ç©
function acf(x, max_lag)
    n  = length(x)
    x_c = x .- mean(x)
    c0  = dot(x_c, x_c) / n
    ck(k) = @views dot(x_c[1:n-k], x_c[k+1:n]) / (n * c0)
    [1.0; [ck(k) for k in 1:max_lag]]
end

acf_vals = acf(y, 20)

# å¯è¦–åŒ–
p1 = plot(y, label="AR(1) series", xlabel="Time", ylabel="Value")
p2 = bar(0:20, acf_vals, label="ACF", xlabel="Lag", ylabel="Correlation")

plot(p1, p2, layout=(2, 1), size=(800, 600))
```

#### B.4.2 çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ï¼ˆState Space Modelsï¼‰

**ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿**:

$$
\begin{aligned}
\text{çŠ¶æ…‹æ–¹ç¨‹å¼:} \quad & x_t = F x_{t-1} + w_t, \quad w_t \sim \mathcal{N}(0, Q) \\
\text{è¦³æ¸¬æ–¹ç¨‹å¼:} \quad & y_t = H x_t + v_t, \quad v_t \sim \mathcal{N}(0, R)
\end{aligned}
$$

**äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—**:

$$
\begin{aligned}
\hat{x}_{t|t-1} &= F \hat{x}_{t-1|t-1} \\
P_{t|t-1} &= F P_{t-1|t-1} F^\top + Q
\end{aligned}
$$

**æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—**:

$$
\begin{aligned}
K_t &= P_{t|t-1} H^\top (H P_{t|t-1} H^\top + R)^{-1} \quad \text{(ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³)} \\
\hat{x}_{t|t} &= \hat{x}_{t|t-1} + K_t (y_t - H \hat{x}_{t|t-1}) \\
P_{t|t} &= (I - K_t H) P_{t|t-1}
\end{aligned}
$$

**Juliaå®Ÿè£…ä¾‹**:

```julia
using LinearAlgebra

# ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿å®Ÿè£…ï¼ˆé€æ¬¡çš„: @views + @inbounds ã§æœ€é©åŒ–ï¼‰
function kalman_filter(y, F, H, Q, R, x0, P0)
    n = length(y)
    d = length(x0)

    x_pred = zeros(d, n)
    x_filt = zeros(d, n)
    P_pred = zeros(d, d, n)
    P_filt = zeros(d, d, n)

    @views x_filt[:, 1]    .= x0
    @views P_filt[:, :, 1] .= P0

    @inbounds for t in 2:n
        @views begin
            # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
            x_pred[:, t]    .= F * x_filt[:, t-1]
            P_pred[:, :, t] .= F * P_filt[:, :, t-1] * F' + Q

            # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
            innovation = y[t] - H * x_pred[:, t]
            S = H * P_pred[:, :, t] * H' + R
            K = P_pred[:, :, t] * H' / S  # ã‚¹ã‚«ãƒ©ãƒ¼ S ã®ã¨ã / ã§ OK

            x_filt[:, t]    .= x_pred[:, t] + K * innovation
            P_filt[:, :, t] .= (I - K * H) * P_pred[:, :, t]
        end
    end

    return x_filt, P_filt
end

# ãƒ†ã‚¹ãƒˆ: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¬ãƒ™ãƒ«ãƒ¢ãƒ‡ãƒ«
F = [1.0;;]
H = [1.0;;]
Q = [0.1;;]
R = [1.0;;]

# çœŸã®çŠ¶æ…‹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰
n = 100
x_true = cumsum(randn(n) .* sqrt(0.1))
y_obs = x_true .+ randn(n)

x_filt, P_filt = kalman_filter(y_obs, F, H, Q, R, [0.0], [1.0;;])

# å¯è¦–åŒ–
plot(1:n, x_true, label="True state", linewidth=2)
plot!(1:n, y_obs, seriestype=:scatter, label="Observations", alpha=0.5)
plot!(1:n, vec(x_filt[1, :]), label="Filtered estimate", linewidth=2, linestyle=:dash)
```

### B.5 ãƒ™ã‚¤ã‚ºéšå±¤ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè·µ

#### B.5.1 éƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆPartial Poolingï¼‰

**å•é¡Œ**: ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«æ¨å®šã—ãŸã„ãŒã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã„ã€‚

**3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:

| æ‰‹æ³• | èª¬æ˜ | å•é¡Œç‚¹ |
|:-----|:-----|:------|
| **å®Œå…¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°** | å…¨ã‚°ãƒ«ãƒ¼ãƒ—ã‚’1ã¤ã¨ã—ã¦æ‰±ã† | ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®é•ã„ã‚’ç„¡è¦– |
| **ãƒãƒ¼ãƒ—ãƒ¼ãƒªãƒ³ã‚°** | ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ç‹¬ç«‹æ¨å®š | å°ã‚µãƒ³ãƒ—ãƒ«ã§ä¸å®‰å®š |
| **éƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°** | éšå±¤ãƒ¢ãƒ‡ãƒ«ã§æƒ…å ±å…±æœ‰ | âœ… ä¸¡è€…ã®ãƒãƒ©ãƒ³ã‚¹ |

**éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
y_{ij} &\sim \mathcal{N}(\mu_i, \sigma^2) \\
\mu_i &\sim \mathcal{N}(\mu_{\text{global}}, \tau^2) \\
\mu_{\text{global}} &\sim \mathcal{N}(0, 10^2) \\
\sigma, \tau &\sim \text{Half-Cauchy}(0, 5)
\end{aligned}
$$

**Turing.jlå®Ÿè£…**:

```julia
using Turing, Distributions, DataFrames, StatsPlots

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: å­¦æ ¡ã”ã¨ã®ç”Ÿå¾’ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢
n_schools = 10
students_per_school = [5, 8, 12, 6, 15, 7, 20, 9, 11, 13]
true_school_means = randn(n_schools) .* 5 .+ 70

data = DataFrame(school_id=Int[], score=Float64[])
for i in 1:n_schools
    for j in 1:students_per_school[i]
        push!(data, (school_id=i, score=true_school_means[i] + randn() * 10))
    end
end

# éšå±¤ãƒ¢ãƒ‡ãƒ«
@model function hierarchical_model(school_id, score)
    n_schools = length(unique(school_id))

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    Î¼_global ~ Normal(70, 20)
    Ï„ ~ truncated(Cauchy(0, 5), 0, Inf)
    Ïƒ ~ truncated(Cauchy(0, 5), 0, Inf)

    # å­¦æ ¡ãƒ¬ãƒ™ãƒ«ã®å¹³å‡
    Î¼_school ~ filldist(Normal(Î¼_global, Ï„), n_schools)

    # å°¤åº¦
    for i in eachindex(score)
        score[i] ~ Normal(Î¼_school[school_id[i]], Ïƒ)
    end
end

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
model = hierarchical_model(data.school_id, data.score)
chain = sample(model, NUTS(), 2000)

# çµæœã®å¯è¦–åŒ–
plot(chain[[:Î¼_global, :Ï„, :Ïƒ]])
```

#### B.5.2 åæŸè¨ºæ–­ï¼ˆConvergence Diagnosticsï¼‰

**Gelman-Rubinçµ±è¨ˆé‡ï¼ˆ$\hat{R}$ï¼‰**:

è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã®åæŸã‚’è¨ºæ–­ã€‚$\hat{R} \approx 1$ãªã‚‰åæŸã€‚

$$
\hat{R} = \sqrt{\frac{\hat{V}}{W}}
$$

ã“ã“ã§:
- $W$: ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£ã®å¹³å‡
- $\hat{V}$: ãƒã‚§ãƒ¼ãƒ³é–“åˆ†æ•£ã¨ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£ã®é‡ã¿ä»˜ãå¹³å‡

**æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆESS: Effective Sample Sizeï¼‰**:

è‡ªå·±ç›¸é–¢ã‚’è€ƒæ…®ã—ãŸå®ŸåŠ¹çš„ãªã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚

$$
\text{ESS} = \frac{N}{1 + 2\sum_{k=1}^\infty \rho_k}
$$

ã“ã“ã§$\rho_k$ã¯é…ã‚Œ$k$ã§ã®è‡ªå·±ç›¸é–¢ã€‚

**Juliaå®Ÿè£…ä¾‹**:

```julia
using MCMCChains, StatsBase

# ãƒã‚§ãƒ¼ãƒ³è¨ºæ–­
println("=== åæŸè¨ºæ–­ ===")
println(gelmandiag(chain))  # Gelman-Rubinçµ±è¨ˆé‡

println("\n=== æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º ===")
println(ess(chain))

println("\n=== è‡ªå·±ç›¸é–¢ ===")
println(autocor(chain))

# ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
plot(chain[[:Î¼_global]])
```

### B.6 ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«é¸æŠ

#### B.6.1 WAICï¼ˆWidely Applicable Information Criterionï¼‰

**å®šç¾©**:

$$
\text{WAIC} = -2 (\text{lppd} - p_{\text{WAIC}})
$$

ã“ã“ã§:
- $\text{lppd}$: log pointwise predictive density
- $p_{\text{WAIC}}$: æœ‰åŠ¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°

**è¨ˆç®—**:

$$
\begin{aligned}
\text{lppd} &= \sum_{i=1}^n \log \left( \frac{1}{S} \sum_{s=1}^S p(y_i | \theta^{(s)}) \right) \\
p_{\text{WAIC}} &= \sum_{i=1}^n \text{Var}_s(\log p(y_i | \theta^{(s)}))
\end{aligned}
$$

**Juliaå®Ÿè£…ä¾‹**:

```julia
using Turing, StatsBase

# ãƒ¢ãƒ‡ãƒ«1: å˜ç´”ãƒ¢ãƒ‡ãƒ«
@model function model1(y)
    Î¼ ~ Normal(0, 10)
    Ïƒ ~ truncated(Normal(0, 5), 0, Inf)
    y ~ Normal(Î¼, Ïƒ)
end

# ãƒ¢ãƒ‡ãƒ«2: éšå±¤ãƒ¢ãƒ‡ãƒ«ï¼ˆå‰è¿°ï¼‰
# ... (hierarchical_model)

# WAICè¨ˆç®—
function waic(chain, model, data)
    n = length(data)
    S = size(chain, 1)

    log_lik = zeros(S, n)
    @inbounds for s in 1:S
        Î¸ = chain[s, :]
        @views log_lik[s, :] .= logpdf.(Normal(Î¸.Î¼, Î¸.Ïƒ), data)
    end

    lppd   = sum(log.(mean(exp.(log_lik), dims=1)))
    p_waic = sum(var(log_lik, dims=1))

    return (; waic = -2(lppd - p_waic), lppd, p_waic)
end

# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
waic1 = waic(chain1, model1, data)
waic2 = waic(chain2, model2, data)

println("Model 1 WAIC: $(waic1.waic)")
println("Model 2 WAIC: $(waic2.waic)")
println("Better model: $(waic1.waic < waic2.waic ? "Model 1" : "Model 2")")
```

#### B.6.2 ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆBayes Factorï¼‰

**å®šç¾©**:

$$
\text{BF}_{12} = \frac{p(D | M_1)}{p(D | M_2)}
$$

**è§£é‡ˆ**ï¼ˆKass & Raftery, 1995ï¼‰:

| BF | è¨¼æ‹ ã®å¼·ã• |
|:---|:----------|
| 1-3 | ã»ã¨ã‚“ã©ä¾¡å€¤ãªã— |
| 3-20 | è‚¯å®šçš„ |
| 20-150 | å¼·ã„ |
| >150 | éå¸¸ã«å¼·ã„ |

**å•é¡Œç‚¹**: å‘¨è¾ºå°¤åº¦$p(D | M)$ã®è¨ˆç®—ãŒå›°é›£ã€‚

### B.7 ãƒ™ã‚¤ã‚ºãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¥é–€

#### B.7.1 Dirichlet Processï¼ˆãƒ‡ã‚£ãƒªã‚¯ãƒ¬éç¨‹ï¼‰

**å•é¡Œ**: ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒäº‹å‰ã«åˆ†ã‹ã‚‰ãªã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€‚

**Dirichlet Process Mixture Model (DPMM)**:

$$
\begin{aligned}
G &\sim \text{DP}(\alpha, H) \quad \text{ï¼ˆãƒ‡ã‚£ãƒªã‚¯ãƒ¬éç¨‹ï¼‰} \\
\theta_i &\sim G \\
y_i &\sim F(\theta_i)
\end{aligned}
$$

ã“ã“ã§:
- $\alpha$: é›†ä¸­åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¤§ãã„ã»ã©å¤šãã®ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰
- $H$: ãƒ™ãƒ¼ã‚¹åˆ†å¸ƒ
- $F$: å°¤åº¦é–¢æ•°

**Chinese Restaurant Processï¼ˆCRPï¼‰**: DPã®ç›´æ„Ÿçš„ãªèª¬æ˜

æ–°ã—ã„å®¢ãŒå…¥åº—ã™ã‚‹ã¨ã:
- ç¢ºç‡$\frac{n_k}{\alpha + n - 1}$ã§æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«$k$ã«åº§ã‚‹ï¼ˆ$n_k$äººåº§ã£ã¦ã„ã‚‹ï¼‰
- ç¢ºç‡$\frac{\alpha}{\alpha + n - 1}$ã§æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œã‚‹

**Juliaå®Ÿè£…ä¾‹ï¼ˆç°¡ç•¥ç‰ˆï¼‰**:

```julia
using Distributions, StatsPlots

# Chinese Restaurant Process simulation
function crp_simulate(n, Î±)
    tables = Int[]  # å„å®¢ãŒã©ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«åº§ã£ã¦ã„ã‚‹ã‹
    table_counts = Int[]  # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®äººæ•°

    for i in 1:n
        if isempty(tables)
            # æœ€åˆã®å®¢
            push!(tables, 1)
            push!(table_counts, 1)
        else
            # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åº§ã‚‹ç¢ºç‡ vs æ–°ãƒ†ãƒ¼ãƒ–ãƒ«
            probs = vcat(table_counts, Î±) ./ (Î± + i - 1)
            k = sample(1:(length(table_counts)+1), Weights(probs))

            if k <= length(table_counts)
                # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«
                table_counts[k] += 1
            else
                # æ–°ãƒ†ãƒ¼ãƒ–ãƒ«
                push!(table_counts, 1)
            end
            push!(tables, k)
        end
    end

    return tables, table_counts
end

# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
n = 100
Î±_values = [0.1, 1.0, 10.0]

for Î± in Î±_values
    tables, counts = crp_simulate(n, Î±)
    n_clusters = length(counts)
    println("Î±=$Î±: $(n_clusters) clusters formed")
end
```

å‡ºåŠ›ä¾‹:
```
Î±=0.1: 3 clusters formed
Î±=1.0: 8 clusters formed
Î±=10.0: 24 clusters formed
```

#### B.7.2 Gaussian Processï¼ˆã‚¬ã‚¦ã‚¹éç¨‹ï¼‰

**å®šç¾©**: é–¢æ•°ã®äº‹å‰åˆ†å¸ƒã‚’å®šç¾©ã™ã‚‹ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ‰‹æ³•ã€‚

$$
f(x) \sim \mathcal{GP}(m(x), k(x, x'))
$$

ã“ã“ã§:
- $m(x)$: å¹³å‡é–¢æ•°ï¼ˆé€šå¸¸0ï¼‰
- $k(x, x')$: ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆå…±åˆ†æ•£ï¼‰

**RBFã‚«ãƒ¼ãƒãƒ«**:

$$
k(x, x') = \sigma^2 \exp\left(-\frac{(x - x')^2}{2\ell^2}\right)
$$

**äºˆæ¸¬åˆ†å¸ƒ**:

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿$(X, y)$ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€æ–°ã—ã„ç‚¹$x_*$ã§ã®äºˆæ¸¬:

$$
\begin{aligned}
f(x_*) | X, y, x_* &\sim \mathcal{N}(\mu_*, \sigma_*^2) \\
\mu_* &= k(x_*, X) [k(X, X) + \sigma_n^2 I]^{-1} y \\
\sigma_*^2 &= k(x_*, x_*) - k(x_*, X) [k(X, X) + \sigma_n^2 I]^{-1} k(X, x_*)
\end{aligned}
$$

**Juliaå®Ÿè£…ä¾‹**:

```julia
using LinearAlgebra, Plots

# RBFã‚«ãƒ¼ãƒãƒ«ï¼ˆçŸ­å½¢å¼ï¼‰
rbf_kernel(x1, x2; Ïƒ=1.0, â„“=1.0) = Ïƒ^2 * exp(-(x1-x2)^2 / (2â„“^2))

# ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°: A\b ã§ inv(A)*b ã‚ˆã‚Šæ•°å€¤å®‰å®š
function gp_predict(X_train, y_train, X_test; Ïƒ=1.0, â„“=1.0, Ïƒ_n=0.1)
    # ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ï¼ˆ2Då†…åŒ…è¡¨è¨˜ï¼‰
    K    = [rbf_kernel(xi, xj; Ïƒ, â„“) for xi in X_train, xj in X_train]
    K_s  = [rbf_kernel(xs, xj; Ïƒ, â„“) for xs in X_test,  xj in X_train]
    K_ss = [rbf_kernel(xs, xt; Ïƒ, â„“) for xs in X_test,  xt in X_test ]

    # äºˆæ¸¬: A \ b ã¯ inv(A)*b ã‚ˆã‚Šæ•°å€¤å®‰å®šï¼ˆCholesky / LU è‡ªå‹•é¸æŠï¼‰
    K_reg  = K + Ïƒ_n^2 * I
    Î±      = K_reg \ y_train
    Î¼_pred = K_s * Î±
    Î£_pred = K_ss - K_s * (K_reg \ K_s')

    return Î¼_pred, sqrt.(diag(Î£_pred))
end

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
X_train = [0.0, 1.0, 3.0, 5.0, 7.0]
y_train = sin.(X_train) .+ randn(5) .* 0.1

X_test = range(0, 8, length=100)
Î¼_pred, Ïƒ_pred = gp_predict(X_train, y_train, collect(X_test))

# å¯è¦–åŒ–
plot(X_test, Î¼_pred, ribbon=2*Ïƒ_pred, label="GP mean Â± 2Ïƒ", fillalpha=0.3)
scatter!(X_train, y_train, label="Training data", markersize=6, color=:red)
plot!(X_test, sin.(X_test), label="True function", linestyle=:dash, color=:black)
xlabel!("x")
ylabel!("f(x)")
```

### B.8 æœ€æ–°ã®MCMCæ‰‹æ³•ï¼ˆ2024-2025å¹´ï¼‰

#### B.8.1 Stochastic Gradient MCMC (SG-MCMC)

**å•é¡Œ**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®å¾“æ¥ã®MCMCã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’æ¯å›ä½¿ç”¨ï¼‰ã€‚

**SG-MCMCã®ã‚¢ã‚¤ãƒ‡ã‚¢**: ãƒŸãƒ‹ãƒãƒƒãƒã§MCMCã‚’å®Ÿè¡Œã€‚

**Stochastic Gradient Langevin Dynamics (SGLD)**:

$$
\theta_{t+1} = \theta_t + \frac{\epsilon_t}{2} \left[ \nabla \log p(\theta) + \frac{N}{n} \sum_{i \in \mathcal{B}_t} \nabla \log p(y_i | \theta) \right] + \eta_t
$$

ã“ã“ã§:
- $\mathcal{B}_t$: æ™‚åˆ»$t$ã®ãƒŸãƒ‹ãƒãƒƒãƒ
- $\eta_t \sim \mathcal{N}(0, \epsilon_t)$: ãƒ©ãƒ³ã‚¸ãƒ¥ãƒãƒ³ãƒã‚¤ã‚º
- $\epsilon_t$: ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼ˆæ¸›è¡°ï¼‰

**æ€§è³ª**: $\epsilon_t \to 0$ã¨ã™ã‚Œã°çœŸã®äº‹å¾Œåˆ†å¸ƒã«åæŸï¼ˆç†è«–ä¿è¨¼ï¼‰ã€‚

**é©ç”¨ä¾‹** (2024-2025å¹´è«–æ–‡):
- å¤§è¦æ¨¡ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ™ã‚¤ã‚ºæ¨è«–
- æ·±å±¤å­¦ç¿’ã®ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–

#### B.8.2 Sequential Monte Carlo (SMC)

**å•é¡Œ**: å¾“æ¥ã®MCMCã¯åˆæœŸå€¤ä¾å­˜æ€§ãŒå¼·ã„ã€‚è¤‡æ•°ã®ãƒã‚§ãƒ¼ãƒ³ã‚’èµ°ã‚‰ã›ã¦ã‚‚ç‹¬ç«‹æ€§ãŒä½ã„ã€‚

**SMCã®ã‚¢ã‚¤ãƒ‡ã‚¢**: ç²’å­ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ã¦ã€ç°¡å˜ãªåˆ†å¸ƒã‹ã‚‰å¾ã€…ã«ç›®æ¨™åˆ†å¸ƒã¸ç§»è¡Œã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. åˆæœŸåˆ†å¸ƒ$\pi_0$ï¼ˆç°¡å˜ãªåˆ†å¸ƒï¼‰ã‹ã‚‰ç²’å­ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
2. $t = 1, \ldots, T$ã«ã¤ã„ã¦:
   - é‡ã¿ä»˜ã‘: $w_i^{(t)} \propto \pi_t(\theta_i^{(t-1)}) / \pi_{t-1}(\theta_i^{(t-1)})$
   - ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: é‡ã¿ã«åŸºã¥ã„ã¦ç²’å­ã‚’é¸æŠ
   - ç§»å‹•: MCMC kernelã§ç²’å­ã‚’å°‘ã—å‹•ã‹ã™
3. æœ€çµ‚çš„ã«ç›®æ¨™åˆ†å¸ƒ$\pi_T = p(\theta | D)$

**åˆ©ç‚¹**:
- ä¸¦åˆ—åŒ–ãŒå®¹æ˜“
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªäº‹å¾Œåˆ†å¸ƒã«å¼·ã„

### B.9 å®Ÿè·µçš„ãªãƒ¢ãƒ‡ãƒ«æ¤œè¨¼

#### B.9.1 Posterior Predictive Checksï¼ˆäº‹å¾Œäºˆæ¸¬ãƒã‚§ãƒƒã‚¯ï¼‰

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã€å®Ÿãƒ‡ãƒ¼ã‚¿ã«ä¼¼ã¦ã„ã‚‹ã‹æ¤œè¨¼ã€‚

$$
y^{\text{rep}} \sim p(y^{\text{rep}} | D) = \int p(y^{\text{rep}} | \theta) p(\theta | D) d\theta
$$

**æ‰‹é †**:
1. äº‹å¾Œåˆ†å¸ƒã‹ã‚‰$\theta^{(s)}$ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
2. $y^{\text{rep},(s)} \sim p(y | \theta^{(s)})$ã‚’ç”Ÿæˆ
3. $y^{\text{rep}}$ã¨$y$ã‚’è¦–è¦šçš„ãƒ»çµ±è¨ˆçš„ã«æ¯”è¼ƒ

**Juliaå®Ÿè£…ä¾‹**:

```julia
using Turing, Distributions, StatsPlots

# ãƒ¢ãƒ‡ãƒ«: æ­£è¦åˆ†å¸ƒ
@model function normal_model(y)
    Î¼ ~ Normal(0, 10)
    Ïƒ ~ truncated(Normal(0, 5), 0, Inf)
    y ~ Normal(Î¼, Ïƒ)
end

# ãƒ‡ãƒ¼ã‚¿
y_obs = randn(100) .* 2 .+ 5

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
chain = sample(normal_model(y_obs), NUTS(), 1000)

# äº‹å¾Œäºˆæ¸¬ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
y_rep = zeros(1000, length(y_obs))
@inbounds for s in 1:1000
    Î¼_s, Ïƒ_s = chain[:Î¼][s], chain[:Ïƒ][s]
    @views y_rep[s, :] .= rand(Normal(Î¼_s, Ïƒ_s), length(y_obs))
end

# æ¤œè¨¼: å¹³å‡ã¨æ¨™æº–åå·®
test_stat_obs = (mean(y_obs), std(y_obs))
test_stat_rep = [@views (mean(y_rep[s, :]), std(y_rep[s, :])) for s in 1:1000]

# ãƒ—ãƒ­ãƒƒãƒˆ
scatter([t[1] for t in test_stat_rep], [t[2] for t in test_stat_rep],
        label="Replicated data", alpha=0.3)
scatter!([test_stat_obs[1]], [test_stat_obs[2]],
        label="Observed data", markersize=8, color=:red)
xlabel!("Mean")
ylabel!("SD")
title!("Posterior Predictive Check")
```

#### B.9.2 Cross-Validation for Bayesian Models

**Leave-One-Out Cross-Validation (LOO-CV)**:

$$
\text{elpd}_{\text{LOO}} = \sum_{i=1}^n \log p(y_i | y_{-i})
$$

ã“ã“ã§$y_{-i}$ã¯$i$ç•ªç›®ã‚’é™¤ã„ãŸãƒ‡ãƒ¼ã‚¿ã€‚

**Pareto-Smoothed Importance Sampling (PSIS)**:

å®Ÿéš›ã«$n$å›ãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´ã›ãšã€é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§è¿‘ä¼¼ï¼ˆVehtari et al., 2017ï¼‰ã€‚

**Juliaå®Ÿè£…ä¾‹** (LOO.jl):

```julia
# using LOO  # ï¼ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦ï¼‰

# LOO-CVè¨ˆç®—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
function loo_cv(chain, model, data)
    n = length(data)
    S = size(chain, 1)

    log_lik = zeros(S, n)
    @inbounds for s in 1:S
        Î¸ = chain[s, :]
        @views log_lik[s, :] .= logpdf.(Normal(Î¸.Î¼, Î¸.Ïƒ), data)
    end

    # Importance sampling: LOO-CVï¼ˆPareto smoothing ç°¡ç•¥ç‰ˆï¼‰
    elpd_loo = sum(@views log(mean(exp.(log_lik[:, i]))) for i in 1:n)
    return elpd_loo
end
```

---


> Progress: [95%]
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. MCMCã®åæŸè¨ºæ–­æŒ‡æ¨™ $\hat{R}$ ãŒ1.0ã«è¿‘ã„ã¨ãä½•ãŒä¿è¨¼ã•ã‚Œã‚‹ã‹ï¼Ÿ
> 2. çµ±è¨ˆçš„æœ‰æ„å·®ã¨å®Ÿç”¨çš„æœ‰æ„å·®ï¼ˆæœ€å°è‡¨åºŠçš„æ„ç¾©å·®ï¼‰ãŒä¹–é›¢ã™ã‚‹å…·ä½“ä¾‹ã‚’æŒ™ã’ã‚ˆã€‚

## è‘—è€…ãƒªãƒ³ã‚¯
- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
