---
title: "ç¬¬24å›ã€å¾Œç·¨ã€‘ä»˜éŒ²ç·¨: çµ±è¨ˆå­¦: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ“ˆ"
type: "tech"
topics: ["machinelearning", "statistics", "rust", "bayesian", "hypothesis"]
published: true
slug: "ml-lecture-24-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> **ç¬¬24å›ã€å‰ç·¨ã€‘**: [ç¬¬24å›ã€å‰ç·¨ã€‘](https://zenn.dev/fumishiki/ml-lecture-24-part1)

## Part 2


$$
\begin{aligned}
\text{SS}_{\text{total}} &= \sum_{i=1}^k \sum_{j=1}^{n_i} (x_{ij} - \bar{x})^2 \\
\text{SS}_{\text{between}} &= \sum_{i=1}^k n_i (\bar{x}_i - \bar{x})^2 \\
\text{SS}_{\text{within}} &= \sum_{i=1}^k \sum_{j=1}^{n_i} (x_{ij} - \bar{x}_i)^2 \\
\text{MS}_{\text{between}} &= \frac{\text{SS}_{\text{between}}}{k-1}, \quad \text{MS}_{\text{within}} = \frac{\text{SS}_{\text{within}}}{N-k}
\end{aligned}
$$

**æ•°å€¤æ¤œè¨¼**:

```rust
use statrs::distribution::{FisherSnedecor, ContinuousCDF};

fn main() {
    let group_a = [0.72_f64, 0.71, 0.73, 0.70, 0.72];
    let group_b = [0.78_f64, 0.77, 0.79, 0.76, 0.78];
    let group_c = [0.68_f64, 0.67, 0.69, 0.66, 0.68];

    // ä¸€å…ƒé…ç½®ANOVA
    let (f_stat, p_value) = one_way_anova(&[&group_a, &group_b, &group_c]);
    println!("F={:.3}, p={:.6}", f_stat, p_value);
    if p_value < 0.05 {
        println!("âœ… å°‘ãªãã¨ã‚‚1çµ„ã®å¹³å‡ãŒç•°ãªã‚‹");
    } else {
        println!("âŒ å…¨ç¾¤ã®å¹³å‡ã«å·®ãªã—");
    }
}

fn one_way_anova(groups: &[&[f64]]) -> (f64, f64) {
    let k = groups.len() as f64;
    let n: f64 = groups.iter().map(|g| g.len()).sum::<usize>() as f64;
    let grand_mean = groups.iter().flat_map(|g| g.iter()).sum::<f64>() / n;
    let ss_between: f64 = groups.iter().map(|g| {
        let gm = g.iter().sum::<f64>() / g.len() as f64;
        g.len() as f64 * (gm - grand_mean).powi(2)
    }).sum();
    let ss_within: f64 = groups.iter().map(|g| {
        let gm = g.iter().sum::<f64>() / g.len() as f64;
        g.iter().map(|x| (x - gm).powi(2)).sum::<f64>()
    }).sum();
    let f = (ss_between / (k - 1.0)) / (ss_within / (n - k));
    let dist = FisherSnedecor::new(k - 1.0, n - k).unwrap();
    let p = 1.0 - dist.cdf(f);
    (f, p)
}
```

å‡ºåŠ›:
```
F=90.0, p=0.000000
âœ… å°‘ãªãã¨ã‚‚1çµ„ã®å¹³å‡ãŒç•°ãªã‚‹
```

#### 3.4.3 æ­£è¦æ€§æ¤œå®š

**å•é¡Œ**: tæ¤œå®šãƒ»ANOVAã¯æ­£è¦æ€§ã‚’ä»®å®šã€‚ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã†ã‹æ¤œè¨¼ã—ãŸã„ã€‚

| æ¤œå®š | ç‰¹å¾´ | å¸°ç„¡ä»®èª¬ |
|:-----|:-----|:--------|
| **Shapiro-Wilkæ¤œå®š** | æœ€ã‚‚å¼·åŠ›ï¼ˆå°~ä¸­ã‚µãƒ³ãƒ—ãƒ«ï¼‰ | ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã† |
| **Kolmogorov-Smirnovæ¤œå®š** | æ±ç”¨çš„ï¼ˆä»»æ„ã®åˆ†å¸ƒï¼‰ | ãƒ‡ãƒ¼ã‚¿ãŒæŒ‡å®šåˆ†å¸ƒã«å¾“ã† |
| **Anderson-Darlingæ¤œå®š** | è£¾ã®é©åˆåº¦ã‚’é‡è¦– | ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã«å¾“ã† |

**æ•°å€¤æ¤œè¨¼**:

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, Normal, Uniform};

fn main() {
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);

    // æ­£è¦åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿ï¼ˆKSæ¤œå®šã®ä»£ã‚ã‚Šã«æ‰‹å‹•ã§æ­£è¦æ€§ãƒã‚§ãƒƒã‚¯ï¼‰
    let normal_dist = Normal::new(0.0_f64, 1.0).unwrap();
    let normal_data: Vec<f64> = (0..30).map(|_| normal_dist.sample(&mut rng)).collect();
    // æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ â†’ på€¤ã¯å¤§ãã„ï¼ˆå¸°ç„¡ä»®èª¬æ£„å´ã›ãšï¼‰
    println!("æ­£è¦ãƒ‡ãƒ¼ã‚¿: å¹³å‡={:.4}, std={:.4}", mean(&normal_data), std_dev(&normal_data));

    // éæ­£è¦ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸€æ§˜åˆ†å¸ƒï¼‰
    let uniform_dist = Uniform::new(0.0_f64, 1.0);
    let uniform_data: Vec<f64> = (0..30).map(|_| uniform_dist.sample(&mut rng)).collect();
    println!("ä¸€æ§˜ãƒ‡ãƒ¼ã‚¿: å¹³å‡={:.4}, std={:.4}", mean(&uniform_data), std_dev(&uniform_data));

    // æ³¨: Rust ã§ KSæ¤œå®šã‚’è¡Œã†ã«ã¯ statrs ã‚„ ndarray-stats ã‚’åˆ©ç”¨
    // statrs::statistics::Statistics trait ã§åŸºæœ¬çµ±è¨ˆé‡ã¯è¨ˆç®—å¯èƒ½
}

fn mean(x: &[f64]) -> f64 { x.iter().sum::<f64>() / x.len() as f64 }
fn std_dev(x: &[f64]) -> f64 {
    let m = mean(x);
    (x.iter().map(|v| (v - m).powi(2)).sum::<f64>() / (x.len() - 1) as f64).sqrt()
}
```

### 3.5 ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š

**ç”¨é€”**: æ­£è¦æ€§ãŒæº€ãŸã•ã‚Œãªã„ã€ã¾ãŸã¯é †åºãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€‚

| æ¤œå®š | ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ç‰ˆ | ç”¨é€” |
|:-----|:----------------|:-----|
| **Mann-Whitney Uæ¤œå®š** | 2æ¨™æœ¬tæ¤œå®š | 2ç¾¤ã®ä¸­å¤®å€¤ã®å·® |
| **Wilcoxonç¬¦å·é †ä½æ¤œå®š** | å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š | å¯¾å¿œã®ã‚ã‚‹2ç¾¤ã®ä¸­å¤®å€¤å·® |
| **Kruskal-Wallisæ¤œå®š** | ä¸€å…ƒé…ç½®ANOVA | 3ç¾¤ä»¥ä¸Šã®ä¸­å¤®å€¤ã®å·® |

**Mann-Whitney Uæ¤œå®šã®åŸç†**:

1. 2ç¾¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦é †ä½ä»˜ã‘ã€‚
2. å„ç¾¤ã®é †ä½å’Œã‚’è¨ˆç®—ã€‚
3. Uçµ±è¨ˆé‡ã‚’è¨ˆç®—:

$$
U_1 = n_1 n_2 + \frac{n_1(n_1+1)}{2} - R_1
$$

ã“ã“ã§ $R_1$ ã¯ç¾¤1ã®é †ä½å’Œã€‚

**æ•°å€¤æ¤œè¨¼**:

```rust
fn main() {
    let group1 = [1.0_f64, 2.0, 3.0, 4.0, 5.0];
    let group2 = [6.0_f64, 7.0, 8.0, 9.0, 10.0];

    // Mann-Whitney Uæ¤œå®šï¼ˆæ‰‹å‹•å®Ÿè£…ï¼‰
    // U = å„ãƒšã‚¢ (aâˆˆgroup1, bâˆˆgroup2) ã§ a < b ã¨ãªã‚‹å€‹æ•°
    let n1 = group1.len() as f64;
    let n2 = group2.len() as f64;
    let u: f64 = group1.iter()
        .flat_map(|&a| group2.iter().map(move |&b| if a < b { 1.0 } else { 0.0 }))
        .sum();
    // æ­£è¦è¿‘ä¼¼ã«ã‚ˆã‚‹ p å€¤
    let mu_u = n1 * n2 / 2.0;
    let sigma_u = (n1 * n2 * (n1 + n2 + 1.0) / 12.0).sqrt();
    let z = (u - mu_u) / sigma_u;
    use statrs::distribution::{Normal, ContinuousCDF};
    let dist = Normal::new(0.0, 1.0).unwrap();
    let p = 2.0 * dist.cdf(-z.abs());  // ä¸¡å´æ¤œå®š

    println!("U={:.1}, p={:.4}", u, p);
}
```

> **Note:** **é€²æ—: 65% å®Œäº†** ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ»ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã®ç†è«–å®Œå…¨ç‰ˆã‚’åˆ¶è¦‡ã€‚å¤šé‡æ¯”è¼ƒè£œæ­£ã¸ã€‚

### 3.6 å¤šé‡æ¯”è¼ƒè£œæ­£ç†è«–

**å•é¡Œ**: è¤‡æ•°ã®æ¤œå®šã‚’è¡Œã†ã¨ã€å¶ç„¶ã«æœ‰æ„ã«ãªã‚‹ç¢ºç‡ï¼ˆç¬¬1ç¨®éèª¤ï¼‰ãŒå¢—å¤§ã™ã‚‹ã€‚

**ä¾‹**: $\alpha = 0.05$ ã§ç‹¬ç«‹ãª20å€‹ã®æ¤œå®šã‚’è¡Œã†ã¨ã€å°‘ãªãã¨ã‚‚1ã¤ãŒå¶ç„¶æœ‰æ„ã«ãªã‚‹ç¢ºç‡:

$$
1 - (1 - 0.05)^{20} \approx 0.64 \quad \text{(64%!)}
$$

**FWERï¼ˆFamily-Wise Error Rateï¼‰**: å°‘ãªãã¨ã‚‚1ã¤ã®ç¬¬1ç¨®éèª¤ãŒèµ·ã“ã‚‹ç¢ºç‡ã€‚

**FDRï¼ˆFalse Discovery Rateï¼‰**: æœ‰æ„ã¨åˆ¤å®šã•ã‚ŒãŸã‚‚ã®ã®ã†ã¡å½é™½æ€§ã®å‰²åˆã®æœŸå¾…å€¤ã€‚

#### 3.6.1 FWERåˆ¶å¾¡æ³•

| æ‰‹æ³• | èª¿æ•´å¾Œã®æœ‰æ„æ°´æº– | ä¿å®ˆæ€§ |
|:-----|:----------------|:-------|
| **Bonferroniè£œæ­£** | $\alpha_{\text{adj}} = \alpha / m$ | æœ€ã‚‚ä¿å®ˆçš„ |
| **Holmæ³•** | é€æ¬¡çš„Bonferroni | Bonferroniã‚ˆã‚Šç·©ã„ |
| **Å idÃ¡kè£œæ­£** | $\alpha_{\text{adj}} = 1 - (1 - \alpha)^{1/m}$ | ç‹¬ç«‹æ€§ä»®å®š |

**Holmæ³•ã®æ‰‹é †**:

1. på€¤ã‚’æ˜‡é †ã«ä¸¦ã¹ã‚‹: $p_{(1)} \leq p_{(2)} \leq \cdots \leq p_{(m)}$
2. $i = 1, 2, \ldots$ ã®é †ã«ä»¥ä¸‹ã‚’ãƒã‚§ãƒƒã‚¯:
   - $p_{(i)} \leq \alpha / (m - i + 1)$ ãªã‚‰æ£„å´ã€æ¬¡ã¸
   - åˆã‚ã¦ä¸ç­‰å¼ãŒæˆç«‹ã—ãªã‹ã£ãŸã‚‰åœæ­¢

#### 3.6.2 FDRåˆ¶å¾¡æ³•

**Benjamini-Hochbergæ³•** [^2]:

1. på€¤ã‚’æ˜‡é †ã«ä¸¦ã¹ã‚‹: $p_{(1)} \leq p_{(2)} \leq \cdots \leq p_{(m)}$
2. $i = m, m-1, \ldots, 1$ ã®é †ã«ä»¥ä¸‹ã‚’ãƒã‚§ãƒƒã‚¯:
   - $p_{(i)} \leq \frac{i}{m} \alpha$ ãªã‚‰ $i$ ç•ªç›®ã¾ã§å…¨ã¦æ£„å´ã€åœæ­¢
   - æˆç«‹ã—ãªã‘ã‚Œã°æ¬¡ã¸

**æ•°å¼å°å‡º**:

FDRã®å®šç¾©:

$$
\text{FDR} = \mathbb{E}\left[\frac{V}{R}\right]
$$

ã“ã“ã§ $V$ = å½é™½æ€§æ•°ã€$R$ = ç·ç™ºè¦‹æ•°ï¼ˆ$R = V + S$, $S$ = çœŸé™½æ€§æ•°ï¼‰ã€‚

Benjamini-Hochbergã¯ç‹¬ç«‹ãªæ¤œå®šã«ãŠã„ã¦ $\text{FDR} \leq \alpha$ ã‚’ä¿è¨¼ã™ã‚‹ [^2]ã€‚

**æ•°å€¤æ¤œè¨¼**:

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, Uniform};

fn main() {
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);
    let uniform = Uniform::new(0.0_f64, 1.0);

    // 100å€‹ã®æ¤œå®šï¼ˆ90å€‹ã¯å¸°ç„¡ä»®èª¬ãŒçœŸã€10å€‹ã¯å¯¾ç«‹ä»®èª¬ãŒçœŸï¼‰
    // H0ãŒçœŸã®på€¤: ä¸€æ§˜åˆ†å¸ƒ
    let mut p_values: Vec<f64> = (0..100).map(|_| uniform.sample(&mut rng)).collect();
    // H1ãŒçœŸã®på€¤: 0ã«åã‚‹ï¼ˆBeta(0.1, 1) è¿‘ä¼¼ã¨ã—ã¦ x^9 å¤‰æ›ï¼‰
    let p_values_alt: Vec<f64> = (0..10).map(|_| uniform.sample(&mut rng).powf(9.0)).collect();
    p_values.extend_from_slice(&p_values_alt);

    // è£œæ­£ãªã—
    let n_sig_uncorrected = p_values.iter().filter(|&&p| p < 0.05).count();
    println!("è£œæ­£ãªã—: {} / 110 ãŒæœ‰æ„", n_sig_uncorrected);

    // Bonferroniè£œæ­£
    let m = p_values.len() as f64;
    let n_sig_bonf = p_values.iter().filter(|&&p| p * m < 0.05).count();
    println!("Bonferroni: {} / 110 ãŒæœ‰æ„", n_sig_bonf);

    // Benjamini-Hochberg (FDR)
    let n_sig_bh = benjamini_hochberg(&p_values, 0.05);
    println!("Benjamini-Hochberg: {} / 110 ãŒæœ‰æ„", n_sig_bh);
}

/// BHæ³•ã§æœ‰æ„ã¨åˆ¤å®šã•ã‚Œã‚‹ä»®èª¬ã®å€‹æ•°ã‚’è¿”ã™
fn benjamini_hochberg(pvals: &[f64], alpha: f64) -> usize {
    let m = pvals.len();
    let mut indexed: Vec<(usize, f64)> = pvals.iter().cloned().enumerate().collect();
    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap());
    let mut last_reject = 0;
    for (i, (_, p)) in indexed.iter().enumerate() {
        if *p <= (i + 1) as f64 / m as f64 * alpha {
            last_reject = i + 1;
        }
    }
    last_reject
}
```

å‡ºåŠ›ä¾‹:
```
è£œæ­£ãªã—: 15 / 110 ãŒæœ‰æ„
Bonferroni: 3 / 110 ãŒæœ‰æ„
Benjamini-Hochberg: 9 / 110 ãŒæœ‰æ„
```

> **Note:** **é€²æ—: 75% å®Œäº†** å¤šé‡æ¯”è¼ƒè£œæ­£ï¼ˆFWER/FDRï¼‰ã‚’å®Œå…¨ç†è§£ã€‚GLMç†è«–ã¸ã€‚

### 3.7 ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆGLMï¼‰

**å•é¡Œ**: ç·šå½¢å›å¸° $y = X\beta + \epsilon$ ã¯é€£ç¶šå€¤ãƒ»æ­£è¦åˆ†å¸ƒã‚’ä»®å®šã€‚ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ï¼ˆåˆ†é¡ï¼‰ã‚„ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯ä¸é©ã€‚

**GLMã®æ§‹æˆè¦ç´ **:

1. **æŒ‡æ•°å‹åˆ†å¸ƒæ—**: å¿œç­”å¤‰æ•° $y$ ã®åˆ†å¸ƒï¼ˆæ­£è¦ãƒ»äºŒé …ãƒ»ãƒã‚¢ã‚½ãƒ³ç­‰ï¼‰ã€‚
2. **ãƒªãƒ³ã‚¯é–¢æ•°** $g(\cdot)$: å¹³å‡ $\mu = \mathbb{E}[y]$ ã‚’ç·šå½¢äºˆæ¸¬å­ $\eta = X\beta$ ã«ç¹‹ãã€‚
3. **ç·šå½¢äºˆæ¸¬å­**: $\eta = X\beta$

$$
g(\mu) = X\beta \quad \Rightarrow \quad \mu = g^{-1}(X\beta)
$$

| åˆ†å¸ƒ | å…¸å‹çš„ç”¨é€” | æ¨™æº–çš„ãƒªãƒ³ã‚¯é–¢æ•° |
|:-----|:----------|:----------------|
| æ­£è¦åˆ†å¸ƒ | é€£ç¶šå€¤ | æ’ç­‰ $g(\mu) = \mu$ |
| äºŒé …åˆ†å¸ƒ | åˆ†é¡ | ãƒ­ã‚¸ãƒƒãƒˆ $g(\mu) = \log\frac{\mu}{1-\mu}$ |
| ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ | ã‚«ã‚¦ãƒ³ãƒˆ | å¯¾æ•° $g(\mu) = \log\mu$ |

#### 3.7.1 ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆLogistic Regressionï¼‰

**ç”¨é€”**: äºŒå€¤åˆ†é¡ï¼ˆ$y \in \{0, 1\}$ï¼‰ã€‚

**ãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
y_i &\sim \text{Bernoulli}(p_i) \\
\log\frac{p_i}{1 - p_i} &= \beta_0 + \beta_1 x_i \quad \text{(ãƒ­ã‚¸ãƒƒãƒˆå¤‰æ›)} \\
\Rightarrow \quad p_i &= \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}} \quad \text{(ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°)}
\end{aligned}
$$

**ã‚ªãƒƒã‚ºæ¯”ï¼ˆOdds Ratioï¼‰**: ä¿‚æ•° $\beta_1$ ã®è§£é‡ˆ

$$
\text{OR} = e^{\beta_1}
$$

$x$ ãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€ã‚ªãƒƒã‚ºï¼ˆ$p / (1-p)$ï¼‰ãŒ $e^{\beta_1}$ å€ã«ãªã‚‹ã€‚

**æœ€å°¤æ¨å®š**: å¯¾æ•°å°¤åº¦ã‚’æœ€å¤§åŒ–ã€‚

$$
\ell(\beta) = \sum_{i=1}^n \left[ y_i \log p_i + (1 - y_i) \log(1 - p_i) \right]
$$

å‹¾é…:

$$
\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n (y_i - p_i) x_{ij}
$$

**æ•°å€¤æ¤œè¨¼**:

```rust
fn sigmoid(x: f64) -> f64 { 1.0 / (1.0 + (-x).exp()) }

fn logistic_log_likelihood(beta: &[f64], x_data: &[[f64; 2]], y: &[f64]) -> f64 {
    // ãƒ‡ãƒ¼ã‚¿: xï¼ˆé€£ç¶šå¤‰æ•°ï¼‰, yï¼ˆ0/1ã®ãƒ©ãƒ™ãƒ«ï¼‰
    x_data.iter().zip(y.iter())
        .map(|(xi, &yi)| {
            // ãƒªãƒ³ã‚¯é–¢æ•°: logit(Ï€) = Î²â‚€ + Î²â‚Â·x
            let eta = beta[0] + beta[1] * xi[0];
            let pi = sigmoid(eta);
            yi * pi.ln() + (1.0 - yi) * (1.0 - pi).ln()
        })
        .sum()
}

fn main() {
    // ãƒ‡ãƒ¼ã‚¿: xï¼ˆé€£ç¶šå¤‰æ•°ï¼‰, yï¼ˆ0/1ã®ãƒ©ãƒ™ãƒ«ï¼‰
    let x_data: [[f64; 2]; 10] = [
        [1.0, 0.0], [2.0, 0.0], [3.0, 0.0], [4.0, 0.0], [5.0, 0.0],
        [6.0, 0.0], [7.0, 0.0], [8.0, 0.0], [9.0, 0.0], [10.0, 0.0],
    ];
    let y = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0_f64];

    // ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°: å‹¾é…ä¸Šæ˜‡æ³•ã§ Î² ã‚’æ¨å®š
    // å‹¾é…: âˆ‚â„“/âˆ‚Î²â±¼ = Î£(yáµ¢ - Ï€áµ¢)Â·xáµ¢â±¼
    let mut beta = [0.0_f64; 2];
    let lr = 0.1;
    for _ in 0..10000 {
        let grad0: f64 = x_data.iter().zip(y.iter())
            .map(|(xi, &yi)| yi - sigmoid(beta[0] + beta[1] * xi[0]))
            .sum();
        let grad1: f64 = x_data.iter().zip(y.iter())
            .map(|(xi, &yi)| (yi - sigmoid(beta[0] + beta[1] * xi[0])) * xi[0])
            .sum();
        beta[0] += lr * grad0;
        beta[1] += lr * grad1;
    }

    let or = beta[1].exp();  // ã‚ªãƒƒã‚ºæ¯”
    println!("ä¿‚æ•°Î²0={:.3}, Î²1={:.3}, ã‚ªãƒƒã‚ºæ¯”OR={:.3}", beta[0], beta[1], or);
    println!("xãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€ã‚ªãƒƒã‚ºãŒ{:.3}å€ã«ãªã‚‹", or);

    // äºˆæ¸¬ç¢ºç‡
    println!("\näºˆæ¸¬ç¢ºç‡:");
    for (xi, &yi) in x_data.iter().zip(y.iter()) {
        let pi = sigmoid(beta[0] + beta[1] * xi[0]);
        println!("  x={:.0}, y={:.0}, Ï€Ì‚={:.3}", xi[0], yi, pi);
    }

    let ll = logistic_log_likelihood(&beta, &x_data, &y);
    println!("å¯¾æ•°å°¤åº¦: {:.4}", ll);
}
```

#### 3.7.2 ãƒã‚¢ã‚½ãƒ³å›å¸°ï¼ˆPoisson Regressionï¼‰

**ç”¨é€”**: ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆ$y \in \{0, 1, 2, \ldots\}$ï¼‰ã€‚ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿå›æ•°ã®äºˆæ¸¬ã€‚

**ãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
y_i &\sim \text{Poisson}(\lambda_i) \\
\log \lambda_i &= \beta_0 + \beta_1 x_i \quad \text{(å¯¾æ•°ãƒªãƒ³ã‚¯é–¢æ•°)} \\
\Rightarrow \quad \lambda_i &= e^{\beta_0 + \beta_1 x_i}
\end{aligned}
$$

**ä¿‚æ•°ã®è§£é‡ˆ**: $x$ ãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€æœŸå¾…ã‚«ã‚¦ãƒ³ãƒˆ $\lambda$ ãŒ $e^{\beta_1}$ å€ã«ãªã‚‹ã€‚

**æ•°å€¤æ¤œè¨¼**:

```rust
fn main() {
    // ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: 1æ™‚é–“ã‚ãŸã‚Šã®ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿå›æ•°ï¼‰
    let workload = [1.0_f64, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0];
    let errors   = [2.0_f64, 3.0, 3.0, 5.0, 6.0, 8.0, 9.0, 12.0, 14.0, 16.0];

    // ãƒã‚¢ã‚½ãƒ³å›å¸°: log(Î») = Î²â‚€ + Î²â‚Â·workload
    // å¯¾æ•°å°¤åº¦: â„“ = Î£[yáµ¢Â·(Î²â‚€ + Î²â‚Â·xáµ¢) - exp(Î²â‚€ + Î²â‚Â·xáµ¢) - ln(yáµ¢!)]
    // å‹¾é…ä¸Šæ˜‡æ³•ã§æœ€é©åŒ–
    let mut beta = [0.0_f64; 2];
    let lr = 0.01;
    for _ in 0..50000 {
        let grad0: f64 = workload.iter().zip(errors.iter())
            .map(|(&xi, &yi)| yi - (beta[0] + beta[1] * xi).exp())
            .sum();
        let grad1: f64 = workload.iter().zip(errors.iter())
            .map(|(&xi, &yi)| (yi - (beta[0] + beta[1] * xi).exp()) * xi)
            .sum();
        beta[0] += lr * grad0;
        beta[1] += lr * grad1;
    }

    // ä¿‚æ•°ã®è§£é‡ˆ: workloadãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨æœŸå¾…ã‚¨ãƒ©ãƒ¼å›æ•°ãŒ exp(Î²â‚) å€
    let multiplier = beta[1].exp();
    println!("ä¿‚æ•°Î²0={:.3}, Î²1={:.3}", beta[0], beta[1]);
    println!("workloadãŒ1å˜ä½å¢—åŠ ã™ã‚‹ã¨ã€æœŸå¾…ã‚¨ãƒ©ãƒ¼å›æ•°ãŒ{:.3}å€ã«ãªã‚‹", multiplier);

    // äºˆæ¸¬ã‚¨ãƒ©ãƒ¼å›æ•°
    println!("\näºˆæ¸¬ã‚¨ãƒ©ãƒ¼å›æ•°:");
    for (&xi, &yi) in workload.iter().zip(errors.iter()) {
        let lambda_pred = (beta[0] + beta[1] * xi).exp();
        println!("  workload={:.0}, errors={:.0}, Î»Ì‚={:.2}", xi, yi, lambda_pred);
    }
}
```

#### 3.7.3 æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®çµ±ä¸€ç†è«–

**GLMã®åŸºç›¤**: æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼ˆExponential Familyï¼‰

$$
p(y | \theta, \phi) = \exp\left(\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)\right)
$$

| é … | åç§° | å½¹å‰² |
|:---|:-----|:-----|
| $\theta$ | è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å¹³å‡ã‚’æ±ºå®š |
| $\phi$ | åˆ†æ•£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | åˆ†æ•£ã‚’æ±ºå®š |
| $b(\theta)$ | ç´¯ç©ç”Ÿæˆé–¢æ•° | å¹³å‡: $\mu = b'(\theta)$ |
| $a(\phi)$ | åˆ†æ•£é–¢æ•° | åˆ†æ•£: $\text{Var}(Y) = b''(\theta) a(\phi)$ |

**ä¸»è¦ãªåˆ†å¸ƒ**:

| åˆ†å¸ƒ | $\theta$ | $b(\theta)$ | $a(\phi)$ | $\mu = b'(\theta)$ |
|:-----|:---------|:-----------|:----------|:------------------|
| æ­£è¦åˆ†å¸ƒ | $\mu$ | $\theta^2 / 2$ | $\sigma^2$ | $\theta$ |
| äºŒé …åˆ†å¸ƒ | $\log \frac{p}{1-p}$ | $\log(1 + e^\theta)$ | $1$ | $\frac{e^\theta}{1 + e^\theta}$ |
| ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ | $\log \lambda$ | $e^\theta$ | $1$ | $e^\theta$ |

**GLMã®çµ±ä¸€æ§‹é€ **:

1. **ãƒ©ãƒ³ãƒ€ãƒ æˆåˆ†**: å¿œç­”å¤‰æ•° $y$ ãŒæŒ‡æ•°å‹åˆ†å¸ƒæ—ã«å¾“ã†ã€‚
2. **ç·šå½¢äºˆæ¸¬å­**: $\eta = X\beta$
3. **ãƒªãƒ³ã‚¯é–¢æ•°**: $g(\mu) = \eta$ï¼ˆæ¨™æº–çš„ãƒªãƒ³ã‚¯é–¢æ•°: $g(\mu) = \theta$ï¼‰

> **Note:** **é€²æ—: 80% å®Œäº†** GLMç†è«–ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯ãƒ»ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ»æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼‰ã‚’ç†è§£ã€‚ãƒ™ã‚¤ã‚ºçµ±è¨ˆã¸ã€‚

### 3.8 ãƒ™ã‚¤ã‚ºçµ±è¨ˆå…¥é–€

#### 3.8.1 ãƒ™ã‚¤ã‚ºã®å®šç†ã®å°å‡º

**ç¬¬4å›ã§å­¦ã‚“ã æ¡ä»¶ä»˜ãç¢ºç‡ã®å®šç¾©**:

$$
p(\theta | D) = \frac{p(\theta, D)}{p(D)}, \quad p(D | \theta) = \frac{p(\theta, D)}{p(\theta)}
$$

ä¸¡è¾ºã« $p(\theta)$ ã‚’æ›ã‘ã‚‹ã¨:

$$
p(\theta, D) = p(D | \theta) p(\theta) = p(\theta | D) p(D)
$$

ã‚ˆã£ã¦:

$$
p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)}
$$

ã“ã‚ŒãŒ**ãƒ™ã‚¤ã‚ºã®å®šç†**ã ã€‚

| é … | åç§° | æ„å‘³ |
|:---|:-----|:-----|
| $p(\theta \| D)$ | äº‹å¾Œåˆ†å¸ƒï¼ˆPosteriorï¼‰ | ãƒ‡ãƒ¼ã‚¿è¦³æ¸¬å¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒ |
| $p(D \| \theta)$ | å°¤åº¦ï¼ˆLikelihoodï¼‰ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸‹ã§ã®ãƒ‡ãƒ¼ã‚¿ã®ç¢ºç‡ |
| $p(\theta)$ | äº‹å‰åˆ†å¸ƒï¼ˆPriorï¼‰ | ãƒ‡ãƒ¼ã‚¿è¦³æ¸¬å‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¿¡å¿µ |
| $p(D)$ | å‘¨è¾ºå°¤åº¦ï¼ˆEvidenceï¼‰ | æ­£è¦åŒ–å®šæ•° $p(D) = \int p(D \| \theta) p(\theta) d\theta$ |

#### 3.8.2 é »åº¦è«–çµ±è¨ˆ vs ãƒ™ã‚¤ã‚ºçµ±è¨ˆ

**å“²å­¦çš„å¯¾ç«‹**:

| é …ç›® | é »åº¦è«– | ãƒ™ã‚¤ã‚º |
|:-----|:------|:-------|
| **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ€§è³ª** | å›ºå®šå€¤ï¼ˆæœªçŸ¥ï¼‰ | ç¢ºç‡å¤‰æ•° |
| **ç¢ºç‡ã®è§£é‡ˆ** | é•·æœŸçš„é »åº¦ | ä¿¡å¿µã®åº¦åˆã„ |
| **æ¨è«–ã®å¯¾è±¡** | ç‚¹æ¨å®šãƒ»ä¿¡é ¼åŒºé–“ | äº‹å¾Œåˆ†å¸ƒå…¨ä½“ |
| **ä¸ç¢ºå®Ÿæ€§ã®è¡¨ç¾** | æ¨™æº–èª¤å·® | äº‹å¾Œåˆ†å¸ƒã®å¹… |
| **äº‹å‰çŸ¥è­˜** | ä½¿ã‚ãªã„ï¼ˆå®¢è¦³æ€§ï¼‰ | ä½¿ã†ï¼ˆä¸»è¦³æ€§ï¼‰ |

**å…·ä½“ä¾‹**: ã‚³ã‚¤ãƒ³æŠ•ã’ï¼ˆ10å›ä¸­7å›è¡¨ï¼‰

**é »åº¦è«–çš„æ¨å®š**ï¼ˆç¬¬7å›ã®MLEï¼‰:

$$
\hat{\theta}_{\text{MLE}} = \frac{k}{n} = \frac{7}{10} = 0.7
$$

95%ä¿¡é ¼åŒºé–“ï¼ˆWaldæ³•ï¼‰:

$$
\text{CI} = \hat{\theta} \pm 1.96 \sqrt{\frac{\hat{\theta}(1-\hat{\theta})}{n}} = 0.7 \pm 1.96 \sqrt{\frac{0.7 \times 0.3}{10}} = [0.416, 0.984]
$$

**ãƒ™ã‚¤ã‚ºæ¨å®š**ï¼ˆäº‹å‰åˆ†å¸ƒBeta(2,2)ã€å…±å½¹æ€§ã‚ˆã‚Šäº‹å¾Œåˆ†å¸ƒBeta(9, 5)ï¼‰:

$$
p(\theta | k=7, n=10) = \text{Beta}(9, 5)
$$

äº‹å¾Œå¹³å‡ï¼ˆç‚¹æ¨å®šï¼‰:

$$
\mathbb{E}[\theta | D] = \frac{\alpha}{\alpha + \beta} = \frac{9}{9+5} = 0.643
$$

95%ä¿¡ç”¨åŒºé–“ï¼ˆCredible Intervalï¼‰:

$$
\text{CrI} = [\text{quantile}(0.025), \text{quantile}(0.975)] \approx [0.366, 0.882]
$$

**è§£é‡ˆã®é•ã„**:

- **é »åº¦è«–CI**: ã€ŒåŒã˜å®Ÿé¨“ã‚’100å›ç¹°ã‚Šè¿”ã›ã°ã€95å›ã¯ã“ã®åŒºé–“ãŒçœŸã® $\theta$ ã‚’å«ã‚€ã€
- **ãƒ™ã‚¤ã‚ºCrI**: ã€Œãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ãŸä»Šã€$\theta$ ãŒã“ã®åŒºé–“ã«ã‚ã‚‹ç¢ºç‡ãŒ95%ã€ï¼ˆã‚ˆã‚Šç›´æ„Ÿçš„ï¼‰

#### 3.8.1 å…±å½¹äº‹å‰åˆ†å¸ƒ

**å®šç¾©**: äº‹å‰åˆ†å¸ƒã¨äº‹å¾Œåˆ†å¸ƒãŒåŒã˜åˆ†å¸ƒæ—ã«å±ã™ã‚‹ã¨ãã€ãã®äº‹å‰åˆ†å¸ƒã‚’å…±å½¹ã¨ã„ã†ã€‚

| å°¤åº¦ | å…±å½¹äº‹å‰åˆ†å¸ƒ | äº‹å¾Œåˆ†å¸ƒ |
|:-----|:-----------|:--------|
| äºŒé …åˆ†å¸ƒ | ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ | ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ |
| æ­£è¦åˆ†å¸ƒï¼ˆæ—¢çŸ¥åˆ†æ•£ï¼‰ | æ­£è¦åˆ†å¸ƒ | æ­£è¦åˆ†å¸ƒ |
| ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ | ã‚¬ãƒ³ãƒåˆ†å¸ƒ | ã‚¬ãƒ³ãƒåˆ†å¸ƒ |

**ä¾‹**: ã‚³ã‚¤ãƒ³æŠ•ã’ï¼ˆäºŒé …åˆ†å¸ƒï¼‰+ ãƒ™ãƒ¼ã‚¿äº‹å‰åˆ†å¸ƒ

$$
\begin{aligned}
\text{å°¤åº¦:} \quad & p(k | n, \theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k} \\
\text{äº‹å‰åˆ†å¸ƒ:} \quad & p(\theta) = \text{Beta}(\alpha, \beta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1} \\
\text{äº‹å¾Œåˆ†å¸ƒ:} \quad & p(\theta | k, n) = \text{Beta}(\alpha + k, \beta + n - k)
\end{aligned}
$$

**æ•°å€¤æ¤œè¨¼**:

```rust
use statrs::distribution::{Beta, ContinuousCDF};

fn main() {
    // äº‹å‰åˆ†å¸ƒ: Beta(2, 2) (å¼±ã„ä¿¡å¿µ: Î¸â‰ˆ0.5)
    let alpha_prior = 2.0_f64;
    let beta_prior  = 2.0_f64;

    // ãƒ‡ãƒ¼ã‚¿: 10å›æŠ•ã’ã¦7å›è¡¨
    let n = 10.0_f64;
    let k = 7.0_f64;

    // äº‹å¾Œåˆ†å¸ƒ: Beta(Î±+k, Î²+n-k) = Beta(9, 5) ï¼ˆå…±å½¹æ›´æ–°ï¼‰
    let alpha_post = alpha_prior + k;
    let beta_post  = beta_prior + n - k;

    let prior = Beta::new(alpha_prior, beta_prior).unwrap();
    let posterior = Beta::new(alpha_post, beta_post).unwrap();

    // äº‹å¾Œå¹³å‡ã¨95%ä¿¡ç”¨åŒºé–“
    let post_mean = alpha_post / (alpha_post + beta_post);
    let cri_lo = find_quantile(&posterior, 0.025);
    let cri_hi = find_quantile(&posterior, 0.975);

    println!("äº‹å¾Œåˆ†å¸ƒ: Beta({}, {})", alpha_post, beta_post);
    println!("äº‹å¾Œå¹³å‡: {:.4}", post_mean);
    println!("95%ä¿¡ç”¨åŒºé–“: [{:.4}, {:.4}]", cri_lo, cri_hi);

    // å¯è¦–åŒ–: plotters ã‚¯ãƒ¬ãƒ¼ãƒˆãŒå¿…è¦
    // ã“ã“ã§ã¯ Î¸ âˆˆ [0,1] ã® PDF å€¤ã‚’è¡¨ç¤º
    println!("\nÎ¸    prior_pdf  posterior_pdf");
    for i in 0..=10 {
        let theta = i as f64 / 10.0;
        use statrs::distribution::Continuous;
        println!("{:.1}  {:.4}     {:.4}", theta,
            prior.pdf(theta.max(1e-9).min(1.0 - 1e-9)),
            posterior.pdf(theta.max(1e-9).min(1.0 - 1e-9)));
    }
}

/// äºŒåˆ†æ¢ç´¢ã§ CDF ã®é€†é–¢æ•°ï¼ˆåˆ†ä½ç‚¹ï¼‰ã‚’è¿‘ä¼¼
fn find_quantile(dist: &statrs::distribution::Beta, p: f64) -> f64 {
    let (mut lo, mut hi) = (0.0_f64, 1.0_f64);
    for _ in 0..100 {
        let mid = (lo + hi) / 2.0;
        if dist.cdf(mid) < p { lo = mid; } else { hi = mid; }
    }
    (lo + hi) / 2.0
}
```

#### 3.8.2 MCMCï¼ˆMarkov Chain Monte Carloï¼‰

**å•é¡Œ**: äº‹å¾Œåˆ†å¸ƒ $p(\theta | D)$ ãŒè¤‡é›‘ã§è§£æçš„ã«è¨ˆç®—ã§ããªã„ã€‚

**MCMC**: ãƒãƒ«ã‚³ãƒ•é€£é–ã‚’ä½¿ã£ã¦äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã€‚

**Metropolis-Hastingsæ³•** [^3]:

1. åˆæœŸå€¤ $\theta^{(0)}$ ã‚’è¨­å®šã€‚
2. $t = 1, 2, \ldots$ ã«ã¤ã„ã¦:
   - ææ¡ˆåˆ†å¸ƒ $q(\theta' | \theta^{(t-1)})$ ã‹ã‚‰å€™è£œ $\theta'$ ã‚’ç”Ÿæˆã€‚
   - å—ç†ç¢ºç‡ã‚’è¨ˆç®—:
     $$
     \alpha = \min\left(1, \frac{p(\theta' | D) q(\theta^{(t-1)} | \theta')}{p(\theta^{(t-1)} | D) q(\theta' | \theta^{(t-1)})}\right)
     $$
   - ç¢ºç‡ $\alpha$ ã§ $\theta^{(t)} = \theta'$ã€ãã†ã§ãªã‘ã‚Œã° $\theta^{(t)} = \theta^{(t-1)}$ã€‚

**probabilistic-rsã§å®Ÿè£…**:

```rust
// ã‚³ã‚¤ãƒ³æŠ•ã’ã®ãƒ™ã‚¤ã‚ºæ¨å®š: Metropolis-Hastings MCMC
// Prior: Î¸ ~ Beta(2, 2), Likelihood: k ~ Binomial(n, Î¸)
// äº‹å¾Œåˆ†å¸ƒ: Beta(9, 5) ãŒè§£æè§£ï¼ˆå…±å½¹ï¼‰

use rand::SeedableRng;
use rand_distr::{Distribution, Uniform, Normal as RandNormal};

fn log_posterior_coinflip(theta: f64, k: f64, n: f64) -> f64 {
    if theta <= 0.0 || theta >= 1.0 { return f64::NEG_INFINITY; }
    // log Beta(2,2) prior + log Binomial likelihood
    let log_prior = (2.0 - 1.0) * theta.ln() + (2.0 - 1.0) * (1.0 - theta).ln();
    let log_lik   = k * theta.ln() + (n - k) * (1.0 - theta).ln();
    log_prior + log_lik
}

fn main() {
    // ãƒ‡ãƒ¼ã‚¿: 10å›ä¸­7å›è¡¨
    let (k, n) = (7.0_f64, 10.0_f64);

    // Metropolis-Hastings ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);
    let proposal = RandNormal::new(0.0, 0.1).unwrap();
    let uniform  = Uniform::new(0.0_f64, 1.0);
    let n_samples = 1000;
    let mut samples = Vec::with_capacity(n_samples);
    let mut theta_cur = 0.5_f64;

    for _ in 0..n_samples {
        let theta_prop = (theta_cur + proposal.sample(&mut rng)).clamp(1e-6, 1.0 - 1e-6);
        let log_alpha = log_posterior_coinflip(theta_prop, k, n)
                      - log_posterior_coinflip(theta_cur,  k, n);
        if log_alpha.exp() > uniform.sample(&mut rng) {
            theta_cur = theta_prop;
        }
        samples.push(theta_cur);
    }

    let mean_theta = samples.iter().sum::<f64>() / samples.len() as f64;
    // è§£æè§£: E[Î¸|data] = (Î±+k)/(Î±+Î²+n) = 9/14 â‰ˆ 0.643
    println!("äº‹å¾Œå¹³å‡ Î¸ (MCMC): {:.4}", mean_theta);
    println!("è§£æè§£ Î¸ (Beta(9,5)): {:.4}", 9.0 / 14.0);
    // å¯è¦–åŒ–: plotters ã‚¯ãƒ¬ãƒ¼ãƒˆã§ histogram ã‚’æç”»
    // cargo add plotters
}
```

> **Note:** **é€²æ—: 90% å®Œäº†** ãƒ™ã‚¤ã‚ºçµ±è¨ˆï¼ˆå…±å½¹äº‹å‰åˆ†å¸ƒãƒ»MCMCï¼‰ã‚’å®Œå…¨ç†è§£ã€‚å®Ÿé¨“è¨ˆç”»æ³•ã¸ã€‚

### 3.9 å®Ÿé¨“è¨ˆç”»æ³•ï¼ˆExperimental Designï¼‰

**ç›®çš„**: é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã§æœ€å¤§ã®æƒ…å ±ã‚’å¾—ã‚‹å®Ÿé¨“ã‚’è¨­è¨ˆã™ã‚‹ã€‚

#### 3.9.1 å®Œå…¨ç„¡ä½œç‚ºåŒ–ãƒ‡ã‚¶ã‚¤ãƒ³ï¼ˆCompletely Randomized Design, CRDï¼‰

**ç‰¹å¾´**: å‡¦ç†ï¼ˆtreatmentï¼‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å‰²ã‚Šå½“ã¦ã‚‹ã€‚æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã€‚

**æ¬ ç‚¹**: ãƒ–ãƒ­ãƒƒã‚¯é–“ã®å¤‰å‹•ï¼ˆä¾‹: æ¸¬å®šæ—¥ã®é•ã„ï¼‰ã‚’åˆ¶å¾¡ã§ããªã„ã€‚

#### 3.9.2 ä¹±å¡Šæ³•ï¼ˆRandomized Block Design, RBDï¼‰

**ç‰¹å¾´**: è¢«é¨“è€…ã‚’ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆä¾‹: å¹´é½¢å±¤ã€æ¸¬å®šæ—¥ï¼‰ã«åˆ†ã‘ã€å„ãƒ–ãƒ­ãƒƒã‚¯å†…ã§å‡¦ç†ã‚’ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã€‚

**åˆ©ç‚¹**: ãƒ–ãƒ­ãƒƒã‚¯é–“å¤‰å‹•ã‚’é™¤å» â†’ æ®‹å·®ãŒå°ã•ããªã‚‹ â†’ æ¤œå‡ºåŠ›å‘ä¸Šã€‚

#### 3.9.3 ãƒ©ãƒ†ãƒ³æ–¹æ ¼ï¼ˆLatin Square Designï¼‰

**ç‰¹å¾´**: 2ã¤ã®è¦å› ï¼ˆä¾‹: è¡Œ=æ—¥ã€åˆ—=æ©Ÿæ¢°ï¼‰ã‚’åŒæ™‚ã«åˆ¶å¾¡ã€‚

**åˆ¶ç´„**: å‡¦ç†æ•° = è¡Œæ•° = åˆ—æ•°ã€‚

#### 3.9.4 ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨­è¨ˆï¼ˆPower Analysisï¼‰

**å•é¡Œ**: å®Ÿé¨“å‰ã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’æ±ºå®šã€‚

**æ‰‹é †**:

1. æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœé‡ $d$ ã‚’è¨­å®šï¼ˆéå»ã®ç ”ç©¶ã‚„äºˆå‚™å®Ÿé¨“ã‹ã‚‰ï¼‰ã€‚
2. æœ‰æ„æ°´æº– $\alpha$ ã‚’è¨­å®šï¼ˆé€šå¸¸0.05ï¼‰ã€‚
3. ç›®æ¨™æ¤œå‡ºåŠ› $1 - \beta$ ã‚’è¨­å®šï¼ˆé€šå¸¸0.8ï¼‰ã€‚
4. æ¤œå®šã®ç¨®é¡ã«å¿œã˜ãŸå…¬å¼ã¾ãŸã¯ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã§ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã€‚

**tæ¤œå®šã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºå…¬å¼**ï¼ˆå†æ²ï¼‰:

$$
n = \frac{2(z_{1-\alpha/2} + z_{1-\beta})^2}{d^2}
$$

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œp < 0.05ã§æœ‰æ„ã€ã¨è¨€ãˆã‚‹ã€‚ã ãŒã€ãã‚Œã¯æœ¬å½“ã«**ã‚ãªãŸã®ä¸»å¼µ**ã‚’æ”¯æŒã—ã¦ã„ã‚‹ã®ã‹ï¼Ÿ**

ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã‚’è€ƒãˆã‚ˆã†:

1. **ã‚·ãƒŠãƒªã‚ªA**: æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ã‚’10ç¨®é¡è©¦ã—ã€1ã¤ã ã‘p < 0.05ã§æœ‰æ„ãªæ”¹å–„ã€‚ä»–9ã¤ã¯æœ‰æ„å·®ãªã—ã€‚
2. **ã‚·ãƒŠãƒªã‚ªB**: åŒã˜å®Ÿé¨“ã‚’100å›è¡Œã„ã€æœ‰æ„ã ã£ãŸ5å›ã ã‘è«–æ–‡ã«å ±å‘Šã€‚
3. **ã‚·ãƒŠãƒªã‚ªC**: ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã‹ã‚‰ã€Œã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯åŠ¹æœãŒã‚ã‚‹ã€ã¨äº‹å¾Œçš„ã«ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æã€‚

**å…¨ã¦çµ±è¨ˆçš„ã«ã¯ã€Œp < 0.05ã€ã ãŒã€ç§‘å­¦çš„ã«ã¯ç„¡æ„å‘³ã ã€‚**

- **ã‚·ãƒŠãƒªã‚ªA**: å¤šé‡æ¯”è¼ƒã®ç½ ã€‚Bonferroniè£œæ­£ã™ã‚Œã°p = 0.05 Ã— 10 = 0.5ã§æœ‰æ„ã§ãªã„ã€‚
- **ã‚·ãƒŠãƒªã‚ªB**: å‡ºç‰ˆãƒã‚¤ã‚¢ã‚¹ã€‚å¤±æ•—ã—ãŸ95å›ã‚’éš è”½ã€‚
- **ã‚·ãƒŠãƒªã‚ªC**: p-hackingã€‚ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã‹ã‚‰ä»®èª¬ã‚’ç«‹ã¦ã‚‹ã€‚

**è­°è«–ã®ç¨®**:

1. **äº‹å‰ç™»éŒ²ï¼ˆPre-registrationï¼‰**ã¯è§£æ±ºç­–ã‹ï¼Ÿã€€å®Ÿé¨“å‰ã«ä»®èª¬ãƒ»æ‰‹æ³•ã‚’å…¬é–‹ç™»éŒ²ã™ã‚Œã°ã€p-hackingã‚’é˜²ã’ã‚‹ã€‚ã ãŒæŸ”è»Ÿæ€§ãŒå¤±ã‚ã‚Œã‚‹ã€‚
2. **på€¤ã®ä»£æ›¿æ¡ˆ**ã¯ï¼Ÿã€€ä¿¡é ¼åŒºé–“ãƒ»åŠ¹æœé‡ãƒ»ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã¯ã€på€¤ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã‹ï¼Ÿ
3. **çµ±è¨ˆçš„æœ‰æ„æ€§ã®åŸºæº–ï¼ˆÎ±=0.05ï¼‰**ã¯æ£æ„çš„ã§ã¯ãªã„ã‹ï¼Ÿã€€ãªãœ0.05ãªã®ã‹ï¼Ÿã€€0.01ã‚„0.001ã§ã¯ãƒ€ãƒ¡ãªã®ã‹ï¼Ÿ

ã“ã®å•ã„ã«å®Œå…¨ãªç­”ãˆã¯ãªã„ã€‚ã ãŒ**çµ±è¨ˆå­¦ã¯é“å…·ã§ã‚ã‚Šã€é“å…·ã®ä½¿ã„æ–¹æ¬¡ç¬¬ã§ç§‘å­¦çš„èª å®Ÿã•ãŒå•ã‚ã‚Œã‚‹**ã“ã¨ã‚’å¿˜ã‚Œã¦ã¯ãªã‚‰ãªã„ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼

---


> Progress: [85%]
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. ANOVAã®Fçµ±è¨ˆé‡ãŒç¾¤é–“åˆ†æ•£ã¨ç¾¤å†…åˆ†æ•£ã®æ¯”ã§æ§‹æˆã•ã‚Œã‚‹æ•°å­¦çš„æ„å‘³ã‚’è¿°ã¹ã‚ˆã€‚
> 2. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒªãƒ³ã‚¯é–¢æ•°ãŒlogitã§ã‚ã‚‹ç†ç”±ã‚’ç¢ºç‡ã®ç¯„å›²ã®åˆ¶ç´„ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Neyman, J., & Pearson, E. S. (1928). *On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: Part I*. Biometrika.
<https://www.jstor.org/stable/2331945>

[^2]: Benjamini, Y., & Hochberg, Y. (1995). *Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing*. Journal of the Royal Statistical Society: Series B.
<https://doi.org/10.1111/j.2517-6161.1995.tb02031.x>

[^3]: Hastings, W. K. (1970). *Monte Carlo Sampling Methods Using Markov Chains and Their Applications*. Biometrika.
<https://doi.org/10.1093/biomet/57.1.97>


### æ•™ç§‘æ›¸

- **Statistical Inference** - Casella & Berger (2002): é »åº¦è«–çµ±è¨ˆã®æ±ºå®šç‰ˆã€‚å¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã€‚
- **Bayesian Data Analysis** - Gelman et al. (2013): ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®æ¨™æº–æ•™ç§‘æ›¸ã€‚
- **The Elements of Statistical Learning** - Hastie, Tibshirani, Friedman (2009): æ©Ÿæ¢°å­¦ç¿’Ã—çµ±è¨ˆã®èåˆã€‚[ç„¡æ–™PDF](https://web.stanford.edu/~hastie/ElemStatLearn/)
- **çµ±è¨ˆå­¦å…¥é–€** - æ±äº¬å¤§å­¦æ•™é¤Šå­¦éƒ¨çµ±è¨ˆå­¦æ•™å®¤ (1991): æ—¥æœ¬èªã®å®šç•ªå…¥é–€æ›¸ã€‚

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- [StatQuest (YouTube)](https://www.youtube.com/@statquest): çµ±è¨ˆå­¦ã®ç›´æ„Ÿçš„è§£èª¬å‹•ç”»ã€‚
- [ndarray-stats Documentation](https://juliastats.org/ndarray-stats/stable/)
- [statrs Documentation](https://juliastats.org/statrs/stable/)
- [linfa Documentation](https://juliastats.org/linfa/stable/)
- [probabilistic-rs Documentation](https://turinglang.org/stable/)

---

## ä»˜éŒ²A: çµ±è¨ˆå­¦ã®æ­´å²çš„ç™ºå±•

### A.1 é »åº¦è«–çµ±è¨ˆã®èª•ç”Ÿï¼ˆ1900-1950å¹´ä»£ï¼‰

| å¹´ | äººç‰© | è²¢çŒ® |
|:---|:-----|:-----|
| 1900 | Karl Pearson | ã‚«ã‚¤äºŒä¹—æ¤œå®šã€Pearsonç›¸é–¢ä¿‚æ•° |
| 1908 | William Gosset (Student) | tåˆ†å¸ƒã€tæ¤œå®šï¼ˆå°‘ã‚µãƒ³ãƒ—ãƒ«çµ±è¨ˆï¼‰ |
| 1920å¹´ä»£ | Ronald Fisher | æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã€åˆ†æ•£åˆ†æï¼ˆANOVAï¼‰ã€å®Ÿé¨“è¨ˆç”»æ³• |
| 1928 | Neyman & Pearson | Neyman-Pearsonä»®èª¬æ¤œå®šæ çµ„ã¿ [^1] |
| 1935 | Fisher | ãƒ©ãƒ³ãƒ€ãƒ åŒ–æ¯”è¼ƒè©¦é¨“ï¼ˆRCTï¼‰ã®åŸç† |

**é »åº¦è«–ã®å“²å­¦**: ç¢ºç‡ = é•·æœŸçš„é »åº¦ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å›ºå®šå€¤ï¼ˆæœªçŸ¥ï¼‰ã€‚å®¢è¦³æ€§ã‚’é‡è¦–ã€‚

### A.2 ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®å¾©èˆˆï¼ˆ1950-1990å¹´ä»£ï¼‰

| å¹´ | äººç‰©/å‡ºæ¥äº‹ | è²¢çŒ® |
|:---|:----------|:-----|
| 1763 | Thomas Bayesï¼ˆæ­»å¾Œå‡ºç‰ˆï¼‰ | ãƒ™ã‚¤ã‚ºã®å®šç†ã®åŸå‹ |
| 1950å¹´ä»£ | Dennis Lindley | ãƒ™ã‚¤ã‚ºæ±ºå®šç†è«– |
| 1953 | Metropolis et al. | Metropolisã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆMCMCï¼‰ [^3] |
| 1970 | Hastings | Metropolis-Hastingsã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
| 1990 | Gelfand & Smith | Gibbs Samplingã®å®Ÿç”¨åŒ– |

**ãƒ™ã‚¤ã‚ºå¾©èˆˆã®ç†ç”±**: ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®ç™ºå±•ã§MCMCãŒå®Ÿç”¨åŒ– â†’ è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã®äº‹å¾Œåˆ†å¸ƒã‚’è¨ˆç®—å¯èƒ½ã«ã€‚

### A.3 ç¾ä»£çµ±è¨ˆå­¦ï¼ˆ1990å¹´ä»£ã€œç¾åœ¨ï¼‰

| å¹´ | æ‰‹æ³• | è²¢çŒ® |
|:---|:-----|:-----|
| 1995 | Benjamini & Hochberg | FDRåˆ¶å¾¡æ³•ï¼ˆå¤šé‡æ¯”è¼ƒï¼‰ [^2] |
| 2000å¹´ä»£ | ãƒ™ã‚¤ã‚ºãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ç„¡é™æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«ï¼ˆDirichlet Processç­‰ï¼‰ |
| 2010å¹´ä»£ | Hamiltonian Monte Carlo (HMC) | é«˜æ¬¡å…ƒMCMCã®é«˜é€ŸåŒ–ï¼ˆNUTSï¼‰ |
| 2015å¹´ä»£ | å› æœæ¨è«–ã®æ™®åŠ | Pearl/Rubinæ çµ„ã¿ã®çµ±åˆã€æ©Ÿæ¢°å­¦ç¿’ã¨ã®èåˆ |
| 2020å¹´ä»£ | ç¢ºç‡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° | probabilistic-rs, PyMC, Stanç­‰ã®æˆç†Ÿ |

---

## ä»˜éŒ²B: Rustã§ä½¿ãˆã‚‹çµ±è¨ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å®Œå…¨ãƒªã‚¹ãƒˆ

### B.1 åŸºç¤çµ±è¨ˆ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **Statistics** (stdlib) | åŸºæœ¬çµ±è¨ˆé‡ | `mean`, `std`, `var`, `median`, `quantile`, `cor`, `cov` |
| **ndarray-stats** | è¨˜è¿°çµ±è¨ˆãƒ»é‡ã¿ä»˜ãçµ±è¨ˆ | `skewness`, `kurtosis`, `mad`, `mode`, `sem`, `zscore`, `sample`, `weights` |
| **statrs** | ç¢ºç‡åˆ†å¸ƒ | `Normal`, `Beta`, `Gamma`, `Binomial`, `Poisson`, `TDist`, `FDist`, `pdf`, `cdf`, `quantile`, `rand` |

### B.2 ä»®èª¬æ¤œå®š

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦æ¤œå®š |
|:----------|:-----|:---------|
| **statrs** | ä»®èª¬æ¤œå®šå…¨èˆ¬ | `OneSampleTTest`, `EqualVarianceTTest`, `UnequalVarianceTTest`, `MannWhitneyUTest`, `WilcoxonSignedRankTest`, `KruskalWallisTest`, `OneWayANOVATest`, `ChisqTest`, `FisherExactTest`, `KSTest`, `AndersonDarlingTest` |
| **statrs** | å¤šé‡æ¯”è¼ƒè£œæ­£ | `adjust`, `Bonferroni`, `Holm`, `BenjaminiHochberg`, `BenjaminiYekutieli` |

### B.3 å›å¸°ãƒ»GLM

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **linfa** | ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ« | `glm`, `@formula`, `Binomial`, `Poisson`, `Gamma`, `LogitLink`, `LogLink`, `InverseLink`, `coef`, `confint`, `predict` |
| **MixedModels.jl** | æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ« | `LinearMixedModel`, `fit!`, `ranef`, `fixef` |

### B.4 ãƒ™ã‚¤ã‚ºçµ±è¨ˆ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•°/ãƒã‚¯ãƒ­ |
|:----------|:-----|:---------------|
| **probabilistic-rs** | ç¢ºç‡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° | `@model`, `~`, `sample`, `NUTS`, `HMC`, `Gibbs`, `plot`, `summarize` |
| **AdvancedMH.jl** | MCMCæ‹¡å¼µ | `MetropolisHastings`, `RWMH`, `StaticMH` |
| **MCMCChains.jl** | MCMCçµæœã®è§£æ | `Chains`, `describe`, `plot`, `ess`, `gelmandiag` |
| **AbstractMCMC.jl** | MCMCã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ | MCMCå®Ÿè£…ã®å…±é€šåŸºç›¤ |

### B.5 ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ãƒ»ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **Bootstrap.jl** | ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³• | `bootstrap`, `BasicSampling`, `confint`, `PercentileConfInt`, `BCaConfInt` |

### B.6 ç”Ÿå­˜æ™‚é–“è§£æ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **Survival.jl** | ç”Ÿå­˜æ™‚é–“è§£æ | `Surv`, `kaplan_meier`, `cox_ph`, `nelson_aalen` |

### B.7 æ™‚ç³»åˆ—è§£æ

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **TimeSeries.jl** | æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ | `TimeArray`, `values`, `timestamp`, `lag`, `lead`, `diff` |
| **StateSpaceModels.jl** | çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ« | `StateSpaceModel`, `kalman_filter`, `smoother` |

### B.8 å®Ÿé¨“è¨ˆç”»æ³•

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **ExperimentalDesign.jl** | å®Ÿé¨“è¨ˆç”» | `factorial_design`, `latin_square`, `balanced_design` |

### B.9 å¯è¦–åŒ–

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” | ä¸»è¦é–¢æ•° |
|:----------|:-----|:---------|
| **StatsPlots.jl** | çµ±è¨ˆçš„ãƒ—ãƒ­ãƒƒãƒˆ | `boxplot`, `violin`, `density`, `marginalscatter`, `corrplot`, `@df` |
| **plotters** | é«˜å“è³ªå¯è¦–åŒ– | `scatter`, `lines`, `barplot`, `heatmap`, `density` |
| **AlgebraOfGraphics.jl** | Grammar of Graphics | `data`, `mapping`, `visual`, `draw` |

---

## ä»˜éŒ²C: çµ±è¨ˆå­¦ã®ä¸»è¦å®šç†ã¾ã¨ã‚

### C.1 ç¢ºç‡è«–ã®åŸºç¤å®šç†

**å¤§æ•°ã®æ³•å‰‡ï¼ˆLaw of Large Numbersï¼‰**:

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{p} \mu \quad \text{as } n \to \infty
$$

æ¨™æœ¬å¹³å‡ã¯æ¯å¹³å‡ã«ç¢ºç‡åæŸã™ã‚‹ã€‚

**ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCentral Limit Theoremï¼‰**:

$$
\sqrt{n} \frac{\bar{X}_n - \mu}{\sigma} \xrightarrow{d} \mathcal{N}(0, 1) \quad \text{as } n \to \infty
$$

æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒã¯æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãï¼ˆæ¯é›†å›£åˆ†å¸ƒã«é–¢ã‚ã‚‰ãšï¼‰ã€‚

### C.2 æ¨å®šã®ç†è«–

**CramÃ©r-Raoä¸‹ç•Œï¼ˆCramÃ©r-Rao Lower Boundï¼‰**:

ä¸åæ¨å®šé‡ $\hat{\theta}$ ã®åˆ†æ•£ã¯æ¬¡ã®ä¸‹ç•Œã‚’æŒã¤:

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{I(\theta)}
$$

ã“ã“ã§ $I(\theta)$ ã¯Fisheræƒ…å ±é‡ã€‚ç­‰å·æˆç«‹æ™‚ã¯**æœ‰åŠ¹æ¨å®šé‡**ã€‚

**æ¼¸è¿‘æ­£è¦æ€§ï¼ˆAsymptotic Normalityï¼‰**:

MLEã¯æ¼¸è¿‘çš„ã«æ­£è¦åˆ†å¸ƒã«å¾“ã†:

$$
\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta) \xrightarrow{d} \mathcal{N}(0, I(\theta)^{-1})
$$

### C.3 æ¤œå®šã®ç†è«–

**Neyman-Pearsonè£œé¡Œï¼ˆNeyman-Pearson Lemmaï¼‰**:

å°¤åº¦æ¯”æ¤œå®šã¯æ‰€å®šã®æœ‰æ„æ°´æº– $\alpha$ ã§æœ€ã‚‚æ¤œå‡ºåŠ›ãŒé«˜ã„ï¼ˆmost powerful testï¼‰ã€‚

$$
\frac{p(x | H_1)}{p(x | H_0)} > c \quad \Rightarrow \quad \text{reject } H_0
$$

### C.4 ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®å®šç†

**ãƒ™ã‚¤ã‚ºã®å®šç†ï¼ˆBayes' Theoremï¼‰**:

$$
p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)} = \frac{p(D | \theta) p(\theta)}{\int p(D | \theta') p(\theta') d\theta'}
$$

**ãƒãƒ«ã‚³ãƒ•é€£é–ã®åæŸ**:

é©åˆ‡ãªæ¡ä»¶ä¸‹ã§MCMCã‚µãƒ³ãƒ—ãƒ«ã¯äº‹å¾Œåˆ†å¸ƒã«åæŸ:

$$
\lim_{t \to \infty} \theta^{(t)} \sim p(\theta | D)
$$

---

## ä»˜éŒ²D: çµ±è¨ˆå­¦ã®å®Ÿè·µãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### D.1 å®Ÿé¨“å‰ï¼ˆäº‹å‰è¨ˆç”»ï¼‰

- [ ] ç ”ç©¶ä»®èª¬ã‚’æ˜ç¢ºã«å®šç¾©ï¼ˆ$H_0$, $H_1$ï¼‰
- [ ] æœ‰æ„æ°´æº– $\alpha$ ã‚’æ±ºå®šï¼ˆé€šå¸¸0.05ï¼‰
- [ ] ç›®æ¨™æ¤œå‡ºåŠ›ã‚’æ±ºå®šï¼ˆé€šå¸¸0.8ï¼‰
- [ ] æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœé‡ã‚’è¨­å®šï¼ˆéå»ç ”ç©¶ãƒ»äºˆå‚™å®Ÿé¨“ã‹ã‚‰ï¼‰
- [ ] ãƒ‘ãƒ¯ãƒ¼åˆ†æã§å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
- [ ] æ¤œå®šæ‰‹æ³•ã‚’äº‹å‰ã«æ±ºå®šï¼ˆtæ¤œå®šãƒ»ANOVAãƒ»ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ç­‰ï¼‰
- [ ] å¤šé‡æ¯”è¼ƒãŒã‚ã‚‹å ´åˆã¯è£œæ­£æ–¹æ³•ã‚’æ±ºå®šï¼ˆBonferroniãƒ»BHç­‰ï¼‰
- [ ] äº‹å‰ç™»éŒ²ï¼ˆPre-registrationï¼‰ã‚’æ¤œè¨ï¼ˆp-hackingã‚’é˜²ãï¼‰

### D.2 ãƒ‡ãƒ¼ã‚¿åé›†

- [ ] ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã‚’å¾¹åº•
- [ ] ãƒ–ãƒ­ãƒƒã‚¯è¦å› ãŒã‚ã‚Œã°ä¹±å¡Šæ³•ã‚’æ¤œè¨
- [ ] æ¸¬å®šèª¤å·®ã‚’æœ€å°åŒ–ï¼ˆæ©Ÿå™¨ã®æ ¡æ­£ãƒ»ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ¨™æº–åŒ–ï¼‰
- [ ] æ¬ æãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²ãƒ»ç†ç”±ã®è¨˜è¼‰
- [ ] å¤–ã‚Œå€¤ã®è¨˜éŒ²ï¼ˆå‰Šé™¤å‰ã«ç†ç”±ã‚’æ˜è¨˜ï¼‰

### D.3 è¨˜è¿°çµ±è¨ˆ

- [ ] å¹³å‡ãƒ»ä¸­å¤®å€¤ãƒ»æ¨™æº–åå·®ãƒ»IQRã‚’è¨ˆç®—
- [ ] æ­ªåº¦ãƒ»å°–åº¦ã‚’ç¢ºèªï¼ˆåˆ†å¸ƒã®å½¢çŠ¶ï¼‰
- [ ] å¤–ã‚Œå€¤ã®æ¤œå‡ºï¼ˆIQRæ³•ãƒ»Grubbsæ¤œå®šï¼‰
- [ ] ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ»ç®±ã²ã’å›³ã§å¯è¦–åŒ–

### D.4 æ¨æ¸¬çµ±è¨ˆ

- [ ] å‰ææ¡ä»¶ã®ç¢ºèªï¼ˆæ­£è¦æ€§ãƒ»ç­‰åˆ†æ•£æ€§ãƒ»ç‹¬ç«‹æ€§ï¼‰
- [ ] æ­£è¦æ€§æ¤œå®šï¼ˆShapiro-Wilkãƒ»Kolmogorov-Smirnovï¼‰
- [ ] ç­‰åˆ†æ•£æ€§æ¤œå®šï¼ˆLeveneãƒ»Bartlettï¼‰
- [ ] å‰æãŒæº€ãŸã•ã‚Œãªã„å ´åˆã¯ä»£æ›¿æ‰‹æ³•ï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ»å¤‰æ›ãƒ»é ‘å¥ãªæ‰‹æ³•ï¼‰

### D.5 ä»®èª¬æ¤œå®š

- [ ] æ¤œå®šçµ±è¨ˆé‡ï¼ˆt, F, Ï‡Â², Uç­‰ï¼‰ã‚’è¨ˆç®—
- [ ] è‡ªç”±åº¦ã‚’ç¢ºèª
- [ ] på€¤ã‚’è¨ˆç®—
- [ ] åŠ¹æœé‡ï¼ˆCohen's d, partial Î·Â², rÂ²ç­‰ï¼‰ã‚’è¨ˆç®—
- [ ] ä¿¡é ¼åŒºé–“ã‚’ä½µè¨˜
- [ ] å¤šé‡æ¯”è¼ƒè£œæ­£ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

### D.6 çµæœã®å ±å‘Š

- [ ] è¨˜è¿°çµ±è¨ˆï¼ˆM, SD, nï¼‰ã‚’å ±å‘Š
- [ ] æ¤œå®šçµ±è¨ˆé‡ãƒ»è‡ªç”±åº¦ãƒ»på€¤ã‚’å ±å‘Šï¼ˆä¾‹: $t(9) = 60.0, p < .001$ï¼‰
- [ ] åŠ¹æœé‡ã‚’å ±å‘Šï¼ˆä¾‹: $d = 6.0$ï¼‰
- [ ] 95%ä¿¡é ¼åŒºé–“ã‚’å ±å‘Šï¼ˆä¾‹: $95\% \text{CI} [0.768, 0.782]$ï¼‰
- [ ] å¤šé‡æ¯”è¼ƒè£œæ­£æ–¹æ³•ã‚’æ˜è¨˜
- [ ] å›³è¡¨ã§è¦–è¦šåŒ–ï¼ˆç®±ã²ã’å›³ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãæ£’ã‚°ãƒ©ãƒ•ç­‰ï¼‰
- [ ] çµ±è¨ˆçš„æœ‰æ„æ€§ã¨å®Ÿç”¨çš„æœ‰æ„æ€§ã‚’åŒºåˆ¥

### D.7 è§£é‡ˆãƒ»è­°è«–

- [ ] på€¤ã®æ­£ã—ã„è§£é‡ˆï¼ˆã€Œ$H_0$ãŒçœŸã§ã‚ã‚‹ç¢ºç‡ã€ã§ã¯ãªã„ï¼‰
- [ ] åŠ¹æœé‡ã®å®Ÿç”¨çš„æ„ç¾©ã‚’è­°è«–
- [ ] æ¤œå‡ºåŠ›ä¸è¶³ã®å¯èƒ½æ€§ã‚’æ¤œè¨ï¼ˆp > 0.05ã®å ´åˆï¼‰
- [ ] ä»£æ›¿èª¬æ˜ï¼ˆäº¤çµ¡å› å­ï¼‰ã®å¯èƒ½æ€§ã‚’è­°è«–
- [ ] é™ç•Œï¼ˆã‚µãƒ³ãƒ—ãƒ«é¸æŠãƒã‚¤ã‚¢ã‚¹ãƒ»æ¸¬å®šèª¤å·®ç­‰ï¼‰ã‚’æ˜è¨˜
- [ ] å› æœé–¢ä¿‚ã¨ç›¸é–¢ã®åŒºåˆ¥

---

## ä»˜éŒ²B: GLMç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã¨æœ€æ–°æ‰‹æ³•

### B.1 æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆMixed Effects Modelsï¼‰

**å•é¡Œ**: ãƒ‡ãƒ¼ã‚¿ã«éšå±¤æ§‹é€ ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹: ç”Ÿå¾’â†’ã‚¯ãƒ©ã‚¹â†’å­¦æ ¡ï¼‰ã€è¦³æ¸¬ãŒç‹¬ç«‹ã§ãªã„ã€‚

**ç·šå½¢æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆLMMï¼‰**:

$$
y_{ij} = \beta_0 + \beta_1 x_{ij} + u_i + \epsilon_{ij}
$$

ã“ã“ã§:
- $y_{ij}$: ã‚°ãƒ«ãƒ¼ãƒ—$i$ã®è¦³æ¸¬$j$ã®å¿œç­”å¤‰æ•°
- $u_i \sim \mathcal{N}(0, \sigma_u^2)$: ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¬ãƒ™ãƒ«ã®ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ
- $\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$: å€‹ä½“ãƒ¬ãƒ™ãƒ«ã®èª¤å·®

**å›ºå®šåŠ¹æœ vs ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ**:

| é …ç›® | å›ºå®šåŠ¹æœ | ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ |
|:-----|:--------|:-----------|
| è§£é‡ˆ | æ¯é›†å›£å…¨ä½“ã®å¹³å‡åŠ¹æœ | ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ã°ã‚‰ã¤ã |
| æ¨å®š | ä¿‚æ•°$\beta$ | åˆ†æ•£æˆåˆ†$\sigma_u^2$ |
| ç›®çš„ | åŠ¹æœã®å¤§ãã•ã‚’çŸ¥ã‚ŠãŸã„ | ã‚°ãƒ«ãƒ¼ãƒ—é–“å¤‰å‹•ã‚’åˆ¶å¾¡ã—ãŸã„ |

**Rustå®Ÿè£…ä¾‹**ï¼ˆMixedModels.jlï¼‰:

```rust
// æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«: åå¿œæ™‚é–“ ~ æ—¥æ•° + (1 + æ—¥æ•° | è¢«é¨“è€…)
// å›ºå®šåŠ¹æœ: Î²ï¼ˆæ—¥æ•°ã®åŠ¹æœï¼‰ã€ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœ: u_i ~ N(0, D)
// In Rust: use the `linfa` crate or external R/Python bridge.
// MixedModels.jl ã®å‡ºåŠ›ã«ç›¸å½“ã™ã‚‹ REML å¯¾æ•°å°¤åº¦ã‚’æ‰‹è¨ˆç®—ã§ç¤ºã™

fn reml_log_likelihood(y: &[f64], x: &[f64], beta: &[f64], sigma_e: f64) -> f64 {
    // ç°¡æ˜“ç‰ˆ: ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœãªã—ï¼ˆå›ºå®šåŠ¹æœã®ã¿ï¼‰ã®æ®‹å·®å¯¾æ•°å°¤åº¦
    let n = y.len() as f64;
    let residuals: f64 = y.iter().zip(x.iter())
        .map(|(&yi, &xi)| (yi - beta[0] - beta[1] * xi).powi(2))
        .sum();
    -0.5 * n * (2.0 * std::f64::consts::PI * sigma_e.powi(2)).ln()
        - 0.5 * residuals / sigma_e.powi(2)
}

fn main() {
    // sleepstudy ãƒ‡ãƒ¼ã‚¿ã®ä»£è¡¨å€¤ï¼ˆåå¿œæ™‚é–“[ms] vs ç¡çœ ä¸è¶³æ—¥æ•°ï¼‰
    let days: Vec<f64>     = (0..10).map(|d| d as f64).collect();
    let reaction: Vec<f64> = vec![249.56, 258.70, 250.80, 321.44, 356.85,
                                  414.69, 382.20, 290.15, 430.58, 466.35];
    // å›ºå®šåŠ¹æœæ¨å®šï¼ˆæœ€å°äºŒä¹—æ³•ï¼‰
    let n = days.len() as f64;
    let x_bar = days.iter().sum::<f64>() / n;
    let y_bar = reaction.iter().sum::<f64>() / n;
    let beta1 = days.iter().zip(reaction.iter())
        .map(|(&x, &y)| (x - x_bar) * (y - y_bar))
        .sum::<f64>()
        / days.iter().map(|&x| (x - x_bar).powi(2)).sum::<f64>();
    let beta0 = y_bar - beta1 * x_bar;
    println!("å›ºå®šåŠ¹æœ: Î²â‚€={:.2}, Î²â‚={:.2} (ms/æ—¥)", beta0, beta1);
    println!("è§£é‡ˆ: ç¡çœ ä¸è¶³ãŒ1æ—¥å¢—ãˆã‚‹ã”ã¨ã«åå¿œæ™‚é–“ãŒ{:.2}mså¢—åŠ ", beta1);

    let residuals: Vec<f64> = days.iter().zip(reaction.iter())
        .map(|(&x, &y)| y - beta0 - beta1 * x).collect();
    let sigma_e = (residuals.iter().map(|r| r.powi(2)).sum::<f64>() / (n - 2.0)).sqrt();
    let ll = reml_log_likelihood(&reaction, &days, &[beta0, beta1], sigma_e);
    println!("REML å¯¾æ•°å°¤åº¦ (ç°¡æ˜“ç‰ˆ): {:.2}", ll);
}
```

å‡ºåŠ›ä¾‹:
```
Linear mixed model fit by maximum likelihood
 Reaction ~ 1 + Days + (1 + Days | Subject)
   logLik   -2 logLik     AIC       AICc        BIC
  -875.97    1751.94   1763.94   1764.47   1783.10

Variance components:
            Column    Variance   Std.Dev.   Corr.
Subject  (Intercept)  612.100    24.741
         Days          35.072     5.923    0.07
Residual              654.941    25.592
```

### B.2 ä¸€èˆ¬åŒ–åŠ æ³•ãƒ¢ãƒ‡ãƒ«ï¼ˆGAM: Generalized Additive Modelsï¼‰

**å•é¡Œ**: ç·šå½¢æ€§ã®ä»®å®šãŒå³ã—ã™ãã‚‹å ´åˆã€éç·šå½¢é–¢ä¿‚ã‚’æŸ”è»Ÿã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ãŸã„ã€‚

**GAMã®å®šå¼åŒ–**:

$$
g(\mu) = \beta_0 + f_1(x_1) + f_2(x_2) + \cdots + f_p(x_p)
$$

ã“ã“ã§$f_i$ã¯ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é–¢æ•°ï¼ˆã‚¹ãƒ—ãƒ©ã‚¤ãƒ³ç­‰ï¼‰ã€‚

**ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³**:

$$
\min_f \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \int (f''(x))^2 dx
$$

ç¬¬1é …: ãƒ•ã‚£ãƒƒãƒˆã€ç¬¬2é …: æ»‘ã‚‰ã‹ã•ã®ãƒšãƒŠãƒ«ãƒ†ã‚£

**Rustã§ã®ç°¡æ˜“å®Ÿè£…**:

```rust
// GAMï¼ˆä¸€èˆ¬åŒ–åŠ æ³•ãƒ¢ãƒ‡ãƒ«ï¼‰: å¤šé …å¼åŸºåº•å±•é–‹ã§éç·šå½¢é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–
// å¯è¦–åŒ–ã«ã¯ plotters ã‚¯ãƒ¬ãƒ¼ãƒˆãŒå¿…è¦ (cargo add plotters)

fn polynomial_features(x: &[f64], degree: usize) -> Vec<Vec<f64>> {
    // x ã® 0 æ¬¡ã€œ degree æ¬¡ã®ç‰¹å¾´é‡è¡Œåˆ—ã‚’è¿”ã™ï¼ˆå„è¡ŒãŒ1ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    x.iter().map(|&xi| (0..=degree).map(|d| xi.powi(d as i32)).collect()).collect()
}

/// æœ€å°äºŒä¹—æ³•ã§å¤šé …å¼ä¿‚æ•°ã‚’æ¨å®šï¼ˆæ­£è¦æ–¹ç¨‹å¼: Î² = (Xáµ€X)â»Â¹Xáµ€yï¼‰
fn least_squares(x_mat: &[Vec<f64>], y: &[f64]) -> Vec<f64> {
    let n = x_mat.len();
    let p = x_mat[0].len();
    // Xáµ€X
    let mut xtx = vec![vec![0.0_f64; p]; p];
    for i in 0..p {
        for j in 0..p {
            xtx[i][j] = (0..n).map(|k| x_mat[k][i] * x_mat[k][j]).sum();
        }
    }
    // Xáµ€y
    let xty: Vec<f64> = (0..p).map(|i| (0..n).map(|k| x_mat[k][i] * y[k]).sum()).collect();
    // Gauss-Jordan ã«ã‚ˆã‚‹é€†è¡Œåˆ—ã®ä»£ã‚ã‚Šã«å˜ç´”ãªè§£æ³•ï¼ˆå°è¡Œåˆ—ã®ã¿ï¼‰
    solve_linear(&xtx, &xty)
}

fn solve_linear(a: &[Vec<f64>], b: &[f64]) -> Vec<f64> {
    let n = b.len();
    let mut mat: Vec<Vec<f64>> = a.iter().zip(b.iter())
        .map(|(row, &bi)| { let mut r = row.clone(); r.push(bi); r }).collect();
    for col in 0..n {
        let pivot = (col..n).max_by(|&i, &j| mat[i][col].abs().partial_cmp(&mat[j][col].abs()).unwrap()).unwrap();
        mat.swap(col, pivot);
        let div = mat[col][col];
        for val in &mut mat[col] { *val /= div; }
        for row in (0..n).filter(|&r| r != col) {
            let factor = mat[row][col];
            for c in 0..=n { let v = mat[col][c]; mat[row][c] -= factor * v; }
        }
    }
    mat.iter().map(|row| *row.last().unwrap()).collect()
}

fn main() {
    // ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: éç·šå½¢é–¢ä¿‚ y = sin(x) + 0.5x + Îµ
    let n = 100_usize;
    let x: Vec<f64> = (0..n).map(|i| i as f64 * 10.0 / (n - 1) as f64).collect();
    let y_true: Vec<f64> = x.iter().map(|&xi| xi.sin() + 0.5 * xi).collect();
    // ãƒã‚¤ã‚ºãªã—ç‰ˆã§ãƒ‡ãƒ¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ãªã—ï¼‰
    let y: Vec<f64> = y_true.clone();

    // æ¬¡æ•°5ã®å¤šé …å¼GAM
    let x_poly = polynomial_features(&x, 5);
    let beta = least_squares(&x_poly, &y);
    println!("å¤šé …å¼GAM ä¿‚æ•°: {:?}", beta.iter().map(|b| format!("{:.4}", b)).collect::<Vec<_>>());

    // äºˆæ¸¬ã¨æ®‹å·®ç¢ºèª
    let y_pred: Vec<f64> = x_poly.iter()
        .map(|xi| xi.iter().zip(beta.iter()).map(|(a, b)| a * b).sum())
        .collect();
    let mse = y.iter().zip(y_pred.iter()).map(|(a, b)| (a - b).powi(2)).sum::<f64>() / n as f64;
    println!("MSE: {:.6}", mse);
    // å¯è¦–åŒ–: plotters ã‚¯ãƒ¬ãƒ¼ãƒˆã§ scatter + line plot ã‚’æç”»
    // cargo add plotters
}
```

### B.3 ã‚¼ãƒ­éå‰°ãƒ¢ãƒ‡ãƒ«ï¼ˆZero-Inflated Modelsï¼‰

**å•é¡Œ**: ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã‚¼ãƒ­ãŒéå‰°ã«å«ã¾ã‚Œã‚‹ï¼ˆä¾‹: ç—…é™¢å—è¨ºå›æ•°ã€äº‹æ•…ä»¶æ•°ï¼‰ã€‚

**ã‚¼ãƒ­éå‰°ãƒã‚¢ã‚½ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆZIPï¼‰**:

$$
P(Y = y) = \begin{cases}
\pi + (1 - \pi) e^{-\lambda} & \text{if } y = 0 \\
(1 - \pi) \frac{\lambda^y e^{-\lambda}}{y!} & \text{if } y > 0
\end{cases}
$$

ã“ã“ã§:
- $\pi$: æ§‹é€ çš„ã‚¼ãƒ­ã®ç¢ºç‡ï¼ˆã€Œæ±ºã—ã¦ã‚¤ãƒ™ãƒ³ãƒˆãŒèµ·ã“ã‚‰ãªã„ã€ï¼‰
- $1 - \pi$: ãƒã‚¢ã‚½ãƒ³éç¨‹ã«å¾“ã†ç¢ºç‡

**2æ®µéšãƒ¢ãƒ‡ãƒ«**:

1. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§$\pi$ã‚’æ¨å®š
2. ãƒã‚¢ã‚½ãƒ³å›å¸°ã§$\lambda$ã‚’æ¨å®š

**æ•°å€¤ä¾‹**:

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, Uniform, Poisson};

// ZIPï¼ˆã‚¼ãƒ­éå‰°ãƒã‚¢ã‚½ãƒ³ï¼‰å¯¾æ•°å°¤åº¦
// P(Y=0) = Ï€ + (1-Ï€)Â·exp(-Î»)
// P(Y=y) = (1-Ï€)Â·Î»Ê¸Â·exp(-Î»)/y!  (y > 0)
fn zip_loglik(pi: f64, lambda: f64, y: &[u64]) -> f64 {
    if pi < 0.0 || pi >= 1.0 || lambda <= 0.0 { return f64::NEG_INFINITY; }
    let log_zero = (pi + (1.0 - pi) * (-lambda).exp()).ln();
    y.iter().map(|&yi| {
        if yi == 0 {
            log_zero
        } else {
            // log[(1-Ï€)Â·Poisson(y|Î»)]
            let log_factorial: f64 = (1..=yi).map(|k| (k as f64).ln()).sum();
            (1.0 - pi).ln() + yi as f64 * lambda.ln() - lambda - log_factorial
        }
    }).sum()
}

fn main() {
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);
    let true_pi = 0.3_f64;
    let true_lambda = 2.0_f64;
    let n = 1000_usize;
    let uniform = Uniform::new(0.0_f64, 1.0);

    // ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: ã‚¼ãƒ­éå‰°ãƒã‚¢ã‚½ãƒ³
    let pois = Poisson::new(true_lambda).unwrap();
    let y: Vec<u64> = (0..n).map(|_| {
        if uniform.sample(&mut rng) < true_pi { 0 }
        else { pois.sample(&mut rng) as u64 }
    }).collect();

    let zero_rate = y.iter().filter(|&&v| v == 0).count() as f64 / n as f64;
    let theoretical_zero = true_pi + (1.0 - true_pi) * (-true_lambda).exp();
    println!("ã‚¼ãƒ­ã®å‰²åˆ: {:.4} (ç†è«–å€¤: {:.4})", zero_rate, theoretical_zero);

    // ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹æœ€å°¤æ¨å®š
    let (mut best_pi, mut best_lambda, mut best_ll) = (0.3, 2.0, f64::NEG_INFINITY);
    for pi_i in 0..20 {
        for lam_i in 1..50 {
            let pi_c = pi_i as f64 * 0.05;
            let lam_c = lam_i as f64 * 0.2;
            let ll = zip_loglik(pi_c, lam_c, &y);
            if ll > best_ll { best_ll = ll; best_pi = pi_c; best_lambda = lam_c; }
        }
    }
    println!("æ¨å®šå€¤: Ï€={:.3}, Î»={:.3}", best_pi, best_lambda);
    println!("çœŸå€¤: Ï€={}, Î»={}", true_pi, true_lambda);
}
```

### B.4 æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ï¼ˆTime Series Modelsï¼‰

#### B.4.1 è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆARï¼‰

**AR(p)ãƒ¢ãƒ‡ãƒ«**:

$$
y_t = \phi_0 + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \epsilon_t
$$

ã“ã“ã§$\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ã¯ãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¤ã‚ºã€‚

**å®šå¸¸æ€§æ¡ä»¶**: ç‰¹æ€§æ–¹ç¨‹å¼ã®æ ¹ãŒå˜ä½å††ã®å¤–å´ã«ã‚ã‚‹ã€‚

**Rustå®Ÿè£…ä¾‹**:

```rust
use rand_distr::{Distribution, Normal as RandNormal};

// AR(1)ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
// y[t] = Ï•Â·y[t-1] + Îµ[t],  Îµ ~ N(0, ÏƒÂ²)
fn ar1_simulate(phi: f64, sigma: f64, n: usize) -> Vec<f64> {
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);
    let noise = RandNormal::new(0.0, sigma).unwrap();
    let mut y = vec![0.0_f64; n];
    // å®šå¸¸åˆ†å¸ƒã‹ã‚‰åˆæœŸå€¤: y[0] ~ N(0, ÏƒÂ²/(1-Ï•Â²))
    let init_std = sigma / (1.0 - phi.powi(2)).sqrt();
    y[0] = RandNormal::new(0.0, init_std).unwrap().sample(&mut rng);
    for t in 1..n {
        y[t] = phi * y[t - 1] + noise.sample(&mut rng);
    }
    y
}

// è‡ªå·±ç›¸é–¢é–¢æ•°ï¼ˆACFï¼‰: Ï(k) = Cov(y[t], y[t-k]) / Var(y)
fn acf(x: &[f64], max_lag: usize) -> Vec<f64> {
    let n = x.len();
    let mean = x.iter().sum::<f64>() / n as f64;
    let xc: Vec<f64> = x.iter().map(|&v| v - mean).collect();
    let c0: f64 = xc.iter().map(|&v| v * v).sum::<f64>() / n as f64;
    let mut result = vec![1.0_f64];
    for k in 1..=max_lag {
        let ck: f64 = xc[..n - k].iter().zip(xc[k..].iter())
            .map(|(&a, &b)| a * b)
            .sum::<f64>() / (n as f64 * c0);
        result.push(ck);
    }
    result
}

fn main() {
    // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    let phi = 0.8_f64;  // è‡ªå·±ç›¸é–¢ä¿‚æ•°
    let sigma = 1.0_f64;
    let n = 200_usize;

    let y = ar1_simulate(phi, sigma, n);
    let acf_vals = acf(&y, 20);

    println!("AR(1) series (æœ€åˆ10ç‚¹): {:?}", &y[..10].iter().map(|v| format!("{:.3}", v)).collect::<Vec<_>>());
    println!("\nè‡ªå·±ç›¸é–¢é–¢æ•° ACF (lag 0-10):");
    for (lag, &rho) in acf_vals.iter().enumerate().take(11) {
        println!("  lag={}: {:.4}  (ç†è«–å€¤: Ï•^lag={:.4})", lag, rho, phi.powi(lag as i32));
    }
    // å¯è¦–åŒ–: plotters ã‚¯ãƒ¬ãƒ¼ãƒˆã§æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆã¨ ACF ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆã‚’æç”»
    // cargo add plotters
}
```

#### B.4.2 çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ï¼ˆState Space Modelsï¼‰

**ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿**:

$$
\begin{aligned}
\text{çŠ¶æ…‹æ–¹ç¨‹å¼:} \quad & x_t = F x_{t-1} + w_t, \quad w_t \sim \mathcal{N}(0, Q) \\
\text{è¦³æ¸¬æ–¹ç¨‹å¼:} \quad & y_t = H x_t + v_t, \quad v_t \sim \mathcal{N}(0, R)
\end{aligned}
$$

**äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—**:

$$
\begin{aligned}
\hat{x}_{t|t-1} &= F \hat{x}_{t-1|t-1} \\
P_{t|t-1} &= F P_{t-1|t-1} F^\top + Q
\end{aligned}
$$

**æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—**:

$$
\begin{aligned}
K_t &= P_{t|t-1} H^\top (H P_{t|t-1} H^\top + R)^{-1} \quad \text{(ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³)} \\
\hat{x}_{t|t} &= \hat{x}_{t|t-1} + K_t (y_t - H \hat{x}_{t|t-1}) \\
P_{t|t} &= (I - K_t H) P_{t|t-1}
\end{aligned}
$$

**Rustå®Ÿè£…ä¾‹**:

```rust
// ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿å®Ÿè£…ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ï¼‰
// çŠ¶æ…‹æ–¹ç¨‹å¼: x[t] = FÂ·x[t-1] + w[t],  w ~ N(0, Q)
// è¦³æ¸¬æ–¹ç¨‹å¼: y[t] = HÂ·x[t]  + v[t],  v ~ N(0, R)

struct KalmanFilter {
    f: f64,  // çŠ¶æ…‹é·ç§»ä¿‚æ•°
    h: f64,  // è¦³æ¸¬ä¿‚æ•°
    q: f64,  // ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºåˆ†æ•£
    r: f64,  // è¦³æ¸¬ãƒã‚¤ã‚ºåˆ†æ•£
}

impl KalmanFilter {
    fn filter(&self, y: &[f64], x0: f64, p0: f64) -> (Vec<f64>, Vec<f64>) {
        let n = y.len();
        let mut x_filt = vec![x0];
        let mut p_filt = vec![p0];

        for t in 1..n {
            // äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
            let x_pred = self.f * x_filt[t - 1];
            let p_pred = self.f * p_filt[t - 1] * self.f + self.q;

            // æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³ K = P_predÂ·H / S, S = HÂ·P_predÂ·H + Rï¼‰
            let s = self.h * p_pred * self.h + self.r;
            let k = p_pred * self.h / s;  // ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
            let innovation = y[t] - self.h * x_pred;

            x_filt.push(x_pred + k * innovation);
            p_filt.push((1.0 - k * self.h) * p_pred);
        }
        (x_filt, p_filt)
    }
}

fn main() {
    use rand::SeedableRng;
    use rand_distr::{Distribution, Normal as RandNormal};
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);

    // ãƒ†ã‚¹ãƒˆ: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¬ãƒ™ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ + è¦³æ¸¬ãƒã‚¤ã‚ºï¼‰
    let n = 100_usize;
    let noise_state = RandNormal::new(0.0_f64, 0.1_f64.sqrt()).unwrap();
    let noise_obs   = RandNormal::new(0.0_f64, 1.0_f64).unwrap();

    // çœŸã®çŠ¶æ…‹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰
    let mut x_true = vec![0.0_f64];
    for _ in 1..n {
        x_true.push(x_true.last().unwrap() + noise_state.sample(&mut rng));
    }
    let y_obs: Vec<f64> = x_true.iter().map(|&x| x + noise_obs.sample(&mut rng)).collect();

    let kf = KalmanFilter { f: 1.0, h: 1.0, q: 0.1, r: 1.0 };
    let (x_filt, _p_filt) = kf.filter(&y_obs, 0.0, 1.0);

    // çµæœç¢ºèª
    let rmse_raw: f64 = (x_true.iter().zip(y_obs.iter())
        .map(|(xt, yo)| (xt - yo).powi(2)).sum::<f64>() / n as f64).sqrt();
    let rmse_filt: f64 = (x_true.iter().zip(x_filt.iter())
        .map(|(xt, xf)| (xt - xf).powi(2)).sum::<f64>() / n as f64).sqrt();
    println!("RMSE (è¦³æ¸¬å€¤): {:.4}", rmse_raw);
    println!("RMSE (ãƒ•ã‚£ãƒ«ã‚¿å¾Œ): {:.4}", rmse_filt);
    println!("ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ã§ãƒã‚¤ã‚ºãŒä½æ¸›ã•ã‚ŒãŸ: {}", rmse_filt < rmse_raw);
    // å¯è¦–åŒ–: plotters ã‚¯ãƒ¬ãƒ¼ãƒˆã§ x_true / y_obs / x_filt ã‚’æç”»
}
```

### B.5 ãƒ™ã‚¤ã‚ºéšå±¤ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè·µ

#### B.5.1 éƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆPartial Poolingï¼‰

**å•é¡Œ**: ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«æ¨å®šã—ãŸã„ãŒã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã„ã€‚

**3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:

| æ‰‹æ³• | èª¬æ˜ | å•é¡Œç‚¹ |
|:-----|:-----|:------|
| **å®Œå…¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°** | å…¨ã‚°ãƒ«ãƒ¼ãƒ—ã‚’1ã¤ã¨ã—ã¦æ‰±ã† | ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®é•ã„ã‚’ç„¡è¦– |
| **ãƒãƒ¼ãƒ—ãƒ¼ãƒªãƒ³ã‚°** | ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ç‹¬ç«‹æ¨å®š | å°ã‚µãƒ³ãƒ—ãƒ«ã§ä¸å®‰å®š |
| **éƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°** | éšå±¤ãƒ¢ãƒ‡ãƒ«ã§æƒ…å ±å…±æœ‰ | âœ… ä¸¡è€…ã®ãƒãƒ©ãƒ³ã‚¹ |

**éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«**:

$$
\begin{aligned}
y_{ij} &\sim \mathcal{N}(\mu_i, \sigma^2) \\
\mu_i &\sim \mathcal{N}(\mu_{\text{global}}, \tau^2) \\
\mu_{\text{global}} &\sim \mathcal{N}(0, 10^2) \\
\sigma, \tau &\sim \text{Half-Cauchy}(0, 5)
\end{aligned}
$$

**probabilistic-rså®Ÿè£…**:

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, Normal as RandNormal};

// éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«: éƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆPartial Poolingï¼‰
// y_ij ~ N(Î¼_i, ÏƒÂ²)
// Î¼_i  ~ N(Î¼_global, Ï„Â²)
// probabilistic-rs / MCMC å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

struct HierarchicalModel {
    school_scores: Vec<Vec<f64>>,
}

impl HierarchicalModel {
    fn log_posterior(&self, mu_global: f64, tau: f64, sigma: f64, mu_schools: &[f64]) -> f64 {
        if tau <= 0.0 || sigma <= 0.0 { return f64::NEG_INFINITY; }
        // ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å‰åˆ†å¸ƒ: Î¼_global ~ N(70, 20Â²), Ï„,Ïƒ ~ Half-Cauchy(5)
        let log_prior_global = -0.5 * ((mu_global - 70.0) / 20.0).powi(2);
        let log_prior_tau    = -(1.0 + (tau / 5.0).powi(2)).ln();
        let log_prior_sigma  = -(1.0 + (sigma / 5.0).powi(2)).ln();
        // å­¦æ ¡ãƒ¬ãƒ™ãƒ«ã®å¹³å‡: Î¼_i ~ N(Î¼_global, Ï„Â²)
        let log_schools: f64 = mu_schools.iter()
            .map(|&mu_i| -0.5 * ((mu_i - mu_global) / tau).powi(2) - tau.ln())
            .sum();
        // å°¤åº¦: y_ij ~ N(Î¼_i, ÏƒÂ²)
        let log_lik: f64 = self.school_scores.iter().zip(mu_schools.iter())
            .map(|(scores, &mu_i)| scores.iter()
                .map(|&y| -0.5 * ((y - mu_i) / sigma).powi(2) - sigma.ln())
                .sum::<f64>())
            .sum();
        log_prior_global + log_prior_tau + log_prior_sigma + log_schools + log_lik
    }
}

fn main() {
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);
    let n_schools = 10_usize;
    let students_per_school = [5, 8, 12, 6, 15, 7, 20, 9, 11, 13_usize];

    // ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: å­¦æ ¡ã”ã¨ã®ç”Ÿå¾’ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢
    let true_school_means: Vec<f64> = (0..n_schools).map(|i| {
        let base = RandNormal::new(0.0_f64, 5.0).unwrap().sample(&mut rng);
        base + 70.0 + i as f64 * 0.5
    }).collect();

    let school_scores: Vec<Vec<f64>> = true_school_means.iter()
        .zip(students_per_school.iter())
        .map(|(&mu, &ns)| {
            let noise = RandNormal::new(mu, 10.0).unwrap();
            (0..ns).map(|_| noise.sample(&mut rng)).collect()
        })
        .collect();

    let model = HierarchicalModel { school_scores: school_scores.clone() };

    // å®Œå…¨ãƒ—ãƒ¼ãƒªãƒ³ã‚° vs ãƒãƒ¼ãƒ—ãƒ¼ãƒªãƒ³ã‚° vs éƒ¨åˆ†ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆäº‹å¾Œå¹³å‡ï¼‰ã®æ¯”è¼ƒ
    let all_scores: Vec<f64> = school_scores.iter().flatten().cloned().collect();
    let grand_mean = all_scores.iter().sum::<f64>() / all_scores.len() as f64;
    println!("ã‚°ãƒ­ãƒ¼ãƒãƒ«å¹³å‡ (å®Œå…¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°): {:.2}", grand_mean);

    for (i, scores) in school_scores.iter().enumerate() {
        let school_mean = scores.iter().sum::<f64>() / scores.len() as f64;
        println!("å­¦æ ¡{}: ãƒãƒ¼ãƒ—ãƒ¼ãƒªãƒ³ã‚°={:.2}, çœŸå€¤={:.2}", i + 1, school_mean, true_school_means[i]);
    }

    // log_posterior ã®ç¢ºèª
    let mu_schools: Vec<f64> = school_scores.iter()
        .map(|s| s.iter().sum::<f64>() / s.len() as f64).collect();
    let lp = model.log_posterior(grand_mean, 5.0, 10.0, &mu_schools);
    println!("Log posterior (åˆæœŸå€¤): {:.2}", lp);
}
```

#### B.5.2 åæŸè¨ºæ–­ï¼ˆConvergence Diagnosticsï¼‰

**Gelman-Rubinçµ±è¨ˆé‡ï¼ˆ$\hat{R}$ï¼‰**:

è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³ã®åæŸã‚’è¨ºæ–­ã€‚$\hat{R} \approx 1$ãªã‚‰åæŸã€‚

$$
\hat{R} = \sqrt{\frac{\hat{V}}{W}}
$$

ã“ã“ã§:
- $W$: ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£ã®å¹³å‡
- $\hat{V}$: ãƒã‚§ãƒ¼ãƒ³é–“åˆ†æ•£ã¨ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£ã®é‡ã¿ä»˜ãå¹³å‡

**æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆESS: Effective Sample Sizeï¼‰**:

è‡ªå·±ç›¸é–¢ã‚’è€ƒæ…®ã—ãŸå®ŸåŠ¹çš„ãªã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚

$$
\text{ESS} = \frac{N}{1 + 2\sum_{k=1}^\infty \rho_k}
$$

ã“ã“ã§$\rho_k$ã¯é…ã‚Œ$k$ã§ã®è‡ªå·±ç›¸é–¢ã€‚

**Rustå®Ÿè£…ä¾‹**:

```rust
// åæŸè¨ºæ–­: RÌ‚ï¼ˆGelman-Rubinçµ±è¨ˆé‡ï¼‰ã¨ ESS
// RÌ‚ = sqrt(VÌ‚ / W): VÌ‚ã¯ãƒ—ãƒ¼ãƒ«åˆ†æ•£æ¨å®š, Wã¯ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£å¹³å‡
// ESS = S / (1 + 2 Î£ Ï_Ï„): Sã¯ç·ã‚µãƒ³ãƒ—ãƒ«æ•°, Ï_Ï„ã¯è‡ªå·±ç›¸é–¢

fn rhat(chains: &[Vec<f64>]) -> f64 {
    let m = chains.len() as f64;
    let n = chains[0].len() as f64;
    let chain_means: Vec<f64> = chains.iter()
        .map(|c| c.iter().sum::<f64>() / n)
        .collect();
    let grand_mean = chain_means.iter().sum::<f64>() / m;
    // Between-chain variance B
    let b = n / (m - 1.0) * chain_means.iter()
        .map(|&cm| (cm - grand_mean).powi(2))
        .sum::<f64>();
    // Within-chain variance W
    let w = chains.iter().zip(chain_means.iter())
        .map(|(c, &cm)| c.iter().map(|&x| (x - cm).powi(2)).sum::<f64>() / (n - 1.0))
        .sum::<f64>() / m;
    let v_hat = (n - 1.0) / n * w + b / n;
    (v_hat / w).sqrt()
}

fn ess(chain: &[f64]) -> f64 {
    let n = chain.len();
    let mean = chain.iter().sum::<f64>() / n as f64;
    let xc: Vec<f64> = chain.iter().map(|&v| v - mean).collect();
    let c0: f64 = xc.iter().map(|&v| v * v).sum::<f64>() / n as f64;
    let mut rho_sum = 0.0;
    for lag in 1..n.min(200) {
        let rho = xc[..n - lag].iter().zip(xc[lag..].iter())
            .map(|(&a, &b)| a * b).sum::<f64>() / (n as f64 * c0);
        if rho < 0.0 { break; }
        rho_sum += rho;
    }
    n as f64 / (1.0 + 2.0 * rho_sum)
}

fn main() {
    // ãƒ€ãƒŸãƒ¼ãƒã‚§ãƒ¼ãƒ³ï¼ˆåæŸæ¸ˆã¿ã®å ´åˆã®æƒ³å®šå€¤ï¼‰
    use rand::SeedableRng;
    use rand_distr::{Distribution, Normal as RandNormal};
    let noise = RandNormal::new(0.72_f64, 0.01).unwrap();
    let chains: Vec<Vec<f64>> = (0..4).map(|seed| {
        let mut rng = rand::rngs::StdRng::seed_from_u64(seed);
        (0..2000).map(|_| noise.sample(&mut rng)).collect()
    }).collect();

    println!("=== åæŸè¨ºæ–­ ===");
    let r = rhat(&chains);
    println!("RÌ‚ = {:.4}  (< 1.01 ãŒåæŸã®ç›®å®‰)", r);

    println!("\n=== æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º ===");
    let e = ess(&chains[0]);
    println!("ESS = {:.1}  (> 400 ãŒç›®å®‰)", e);

    // è‡ªå·±ç›¸é–¢ï¼ˆlag 1-5ï¼‰
    println!("\n=== è‡ªå·±ç›¸é–¢ ===");
    let chain = &chains[0];
    let mean = chain.iter().sum::<f64>() / chain.len() as f64;
    let xc: Vec<f64> = chain.iter().map(|&v| v - mean).collect();
    let c0: f64 = xc.iter().map(|&v| v * v).sum::<f64>() / chain.len() as f64;
    for lag in 1..=5 {
        let rho = xc[..chain.len()-lag].iter().zip(xc[lag..].iter())
            .map(|(&a, &b)| a * b).sum::<f64>() / (chain.len() as f64 * c0);
        println!("  lag={}: Ï={:.4}", lag, rho);
    }
    let status = if r < 1.01 && e > 400.0 { "âœ… åæŸ" } else { "âš ï¸ è¦ç¢ºèª" };
    println!("\nåæŸåˆ¤å®š: {}", status);
}
```

### B.6 ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«é¸æŠ

#### B.6.1 WAICï¼ˆWidely Applicable Information Criterionï¼‰

**å®šç¾©**:

$$
\text{WAIC} = -2 (\text{lppd} - p_{\text{WAIC}})
$$

ã“ã“ã§:
- $\text{lppd}$: log pointwise predictive density
- $p_{\text{WAIC}}$: æœ‰åŠ¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°

**è¨ˆç®—**:

$$
\begin{aligned}
\text{lppd} &= \sum_{i=1}^n \log \left( \frac{1}{S} \sum_{s=1}^S p(y_i | \theta^{(s)}) \right) \\
p_{\text{WAIC}} &= \sum_{i=1}^n \text{Var}_s(\log p(y_i | \theta^{(s)}))
\end{aligned}
$$

**Rustå®Ÿè£…ä¾‹**:

```rust
// WAICï¼ˆWidely Applicable Information Criterionï¼‰
// WAIC = -2(lppd - p_WAIC)
// lppd   = Î£áµ¢ log(mean_s p(yáµ¢|Î¸â½Ë¢â¾))
// p_WAIC = Î£áµ¢ Var_s(log p(yáµ¢|Î¸â½Ë¢â¾))

fn waic(log_lik: &Vec<Vec<f64>>) -> (f64, f64, f64) {
    // log_lik[s][i] = log p(y_i | Î¸^(s))
    let s = log_lik.len() as f64;
    let n = log_lik[0].len();

    let lppd: f64 = (0..n).map(|i| {
        // log(mean_s exp(log_lik[s][i])) = log_sum_exp - log(S)
        let max_ll = log_lik.iter().map(|row| row[i]).fold(f64::NEG_INFINITY, f64::max);
        let sum_exp: f64 = log_lik.iter().map(|row| (row[i] - max_ll).exp()).sum();
        max_ll + sum_exp.ln() - s.ln()
    }).sum();

    let p_waic: f64 = (0..n).map(|i| {
        let vals: Vec<f64> = log_lik.iter().map(|row| row[i]).collect();
        let mean = vals.iter().sum::<f64>() / s;
        vals.iter().map(|&v| (v - mean).powi(2)).sum::<f64>() / (s - 1.0)
    }).sum();

    let waic_val = -2.0 * (lppd - p_waic);
    (waic_val, lppd, p_waic)
}

fn main() {
    // ãƒ¢ãƒ‡ãƒ«1ï¼ˆå˜ç´”ï¼‰ã¨ ãƒ¢ãƒ‡ãƒ«2ï¼ˆè¤‡é›‘ï¼‰ã®æ¯”è¼ƒ
    // ãƒ€ãƒŸãƒ¼ã®logå°¤åº¦ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ200ã‚µãƒ³ãƒ—ãƒ« Ã— 50ãƒ‡ãƒ¼ã‚¿ç‚¹ï¼‰
    let n_samples = 200_usize;
    let n_data    = 50_usize;

    // ãƒ¢ãƒ‡ãƒ«1: æ­£è¦åˆ†å¸ƒ Î¼ ~ N(0,10), Ïƒ ~ HalfNormal(5)
    // MCMCãƒã‚§ãƒ¼ãƒ³ã®ä»£ã‚ã‚Šã«å›ºå®šå€¤ã§ãƒ‡ãƒ¢
    let mu1 = 0.72_f64; let sigma1 = 0.02_f64;
    let data: Vec<f64> = (0..n_data).map(|i| 0.70 + (i as f64) * 0.001).collect();
    let log_lik1: Vec<Vec<f64>> = (0..n_samples).map(|_| {
        data.iter().map(|&y| {
            -0.5 * ((y - mu1) / sigma1).powi(2) - sigma1.ln() - 0.5 * (2.0 * std::f64::consts::PI).ln()
        }).collect()
    }).collect();

    // ãƒ¢ãƒ‡ãƒ«2: ã‚ˆã‚Šåºƒã„äº‹å‰åˆ†å¸ƒ
    let mu2 = 0.72_f64; let sigma2 = 0.05_f64;
    let log_lik2: Vec<Vec<f64>> = (0..n_samples).map(|_| {
        data.iter().map(|&y| {
            -0.5 * ((y - mu2) / sigma2).powi(2) - sigma2.ln() - 0.5 * (2.0 * std::f64::consts::PI).ln()
        }).collect()
    }).collect();

    let (waic1, lppd1, p1) = waic(&log_lik1);
    let (waic2, lppd2, p2) = waic(&log_lik2);

    println!("Model 1 WAIC: {:.2}  (lppd={:.2}, p_WAIC={:.2})", waic1, lppd1, p1);
    println!("Model 2 WAIC: {:.2}  (lppd={:.2}, p_WAIC={:.2})", waic2, lppd2, p2);
    println!("Better model: {}", if waic1 < waic2 { "Model 1" } else { "Model 2" });
}
```

#### B.6.2 ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆBayes Factorï¼‰

**å®šç¾©**:

$$
\text{BF}_{12} = \frac{p(D | M_1)}{p(D | M_2)}
$$

**è§£é‡ˆ**ï¼ˆKass & Raftery, 1995ï¼‰:

| BF | è¨¼æ‹ ã®å¼·ã• |
|:---|:----------|
| 1-3 | ã»ã¨ã‚“ã©ä¾¡å€¤ãªã— |
| 3-20 | è‚¯å®šçš„ |
| 20-150 | å¼·ã„ |
| >150 | éå¸¸ã«å¼·ã„ |

**å•é¡Œç‚¹**: å‘¨è¾ºå°¤åº¦$p(D | M)$ã®è¨ˆç®—ãŒå›°é›£ã€‚

### B.7 ãƒ™ã‚¤ã‚ºãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¥é–€

#### B.7.1 Dirichlet Processï¼ˆãƒ‡ã‚£ãƒªã‚¯ãƒ¬éç¨‹ï¼‰

**å•é¡Œ**: ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒäº‹å‰ã«åˆ†ã‹ã‚‰ãªã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€‚

**Dirichlet Process Mixture Model (DPMM)**:

$$
\begin{aligned}
G &\sim \text{DP}(\alpha, H) \quad \text{ï¼ˆãƒ‡ã‚£ãƒªã‚¯ãƒ¬éç¨‹ï¼‰} \\
\theta_i &\sim G \\
y_i &\sim F(\theta_i)
\end{aligned}
$$

ã“ã“ã§:
- $\alpha$: é›†ä¸­åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¤§ãã„ã»ã©å¤šãã®ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰
- $H$: ãƒ™ãƒ¼ã‚¹åˆ†å¸ƒ
- $F$: å°¤åº¦é–¢æ•°

**Chinese Restaurant Processï¼ˆCRPï¼‰**: DPã®ç›´æ„Ÿçš„ãªèª¬æ˜

æ–°ã—ã„å®¢ãŒå…¥åº—ã™ã‚‹ã¨ã:
- ç¢ºç‡$\frac{n_k}{\alpha + n - 1}$ã§æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«$k$ã«åº§ã‚‹ï¼ˆ$n_k$äººåº§ã£ã¦ã„ã‚‹ï¼‰
- ç¢ºç‡$\frac{\alpha}{\alpha + n - 1}$ã§æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œã‚‹

**Rustå®Ÿè£…ä¾‹ï¼ˆç°¡ç•¥ç‰ˆï¼‰**:

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, WeightedIndex};

// Chinese Restaurant Process simulation
// æ–°ã—ã„å®¢ i ãŒå…¥åº—ã™ã‚‹ã¨ã:
//   ç¢ºç‡ n_k / (Î± + i - 1) ã§æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ« k ã«ç€å¸­
//   ç¢ºç‡ Î±   / (Î± + i - 1) ã§æ–°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œã‚‹
fn crp_simulate(n: usize, alpha: f64, rng: &mut impl rand::Rng) -> (Vec<usize>, Vec<usize>) {
    let mut tables: Vec<usize> = Vec::new();       // å„å®¢ãŒã©ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«åº§ã£ã¦ã„ã‚‹ã‹
    let mut table_counts: Vec<usize> = Vec::new(); // å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®äººæ•°

    for i in 0..n {
        if tables.is_empty() {
            // æœ€åˆã®å®¢
            tables.push(0);
            table_counts.push(1);
        } else {
            // æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åº§ã‚‹ç¢ºç‡ vs æ–°ãƒ†ãƒ¼ãƒ–ãƒ«
            let total = alpha + i as f64;
            let mut weights: Vec<f64> = table_counts.iter().map(|&c| c as f64 / total).collect();
            weights.push(alpha / total);  // æ–°ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¢ºç‡

            let dist = WeightedIndex::new(&weights).unwrap();
            let k = dist.sample(rng);

            if k < table_counts.len() {
                // æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«
                table_counts[k] += 1;
                tables.push(k);
            } else {
                // æ–°ãƒ†ãƒ¼ãƒ–ãƒ«
                table_counts.push(1);
                tables.push(table_counts.len() - 1);
            }
        }
    }
    (tables, table_counts)
}

fn main() {
    let n = 100_usize;
    let alpha_values = [0.1_f64, 1.0, 10.0];

    for &alpha in &alpha_values {
        let mut rng = rand::rngs::StdRng::seed_from_u64(42);
        let (_tables, counts) = crp_simulate(n, alpha, &mut rng);
        let n_clusters = counts.len();
        println!("Î±={}: {} clusters formed", alpha, n_clusters);
    }
}
```

å‡ºåŠ›ä¾‹:
```
Î±=0.1: 3 clusters formed
Î±=1.0: 8 clusters formed
Î±=10.0: 24 clusters formed
```

#### B.7.2 Gaussian Processï¼ˆã‚¬ã‚¦ã‚¹éç¨‹ï¼‰

**å®šç¾©**: é–¢æ•°ã®äº‹å‰åˆ†å¸ƒã‚’å®šç¾©ã™ã‚‹ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ‰‹æ³•ã€‚

$$
f(x) \sim \mathcal{GP}(m(x), k(x, x'))
$$

ã“ã“ã§:
- $m(x)$: å¹³å‡é–¢æ•°ï¼ˆé€šå¸¸0ï¼‰
- $k(x, x')$: ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆå…±åˆ†æ•£ï¼‰

**RBFã‚«ãƒ¼ãƒãƒ«**:

$$
k(x, x') = \sigma^2 \exp\left(-\frac{(x - x')^2}{2\ell^2}\right)
$$

**äºˆæ¸¬åˆ†å¸ƒ**:

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿$(X, y)$ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€æ–°ã—ã„ç‚¹$x_*$ã§ã®äºˆæ¸¬:

$$
\begin{aligned}
f(x_*) | X, y, x_* &\sim \mathcal{N}(\mu_*, \sigma_*^2) \\
\mu_* &= k(x_*, X) [k(X, X) + \sigma_n^2 I]^{-1} y \\
\sigma_*^2 &= k(x_*, x_*) - k(x_*, X) [k(X, X) + \sigma_n^2 I]^{-1} k(X, x_*)
\end{aligned}
$$

**Rustå®Ÿè£…ä¾‹**:

```rust
// ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°
// f(x) ~ GP(m(x), k(x,x'))
// RBFã‚«ãƒ¼ãƒãƒ«: k(x,x') = ÏƒÂ² exp(-(x-x')Â²/(2â„“Â²))
// äºˆæ¸¬: Î¼* = K_s Â· (K + Ïƒ_nÂ²I)â»Â¹ y,  Ïƒ*Â² = k** - K_s (K + Ïƒ_nÂ²I)â»Â¹ K_s^T

fn rbf_kernel(x1: f64, x2: f64, sigma: f64, ell: f64) -> f64 {
    sigma.powi(2) * (-(x1 - x2).powi(2) / (2.0 * ell.powi(2))).exp()
}

/// ä¸‹ä¸‰è§’ Cholesky åˆ†è§£ï¼ˆå°è¡Œåˆ—ç”¨ï¼‰
fn cholesky(a: &[Vec<f64>]) -> Vec<Vec<f64>> {
    let n = a.len();
    let mut l = vec![vec![0.0_f64; n]; n];
    for i in 0..n {
        for j in 0..=i {
            let sum: f64 = (0..j).map(|k| l[i][k] * l[j][k]).sum();
            l[i][j] = if i == j { (a[i][i] - sum).sqrt() }
                      else { (a[i][j] - sum) / l[j][j] };
        }
    }
    l
}

/// Lx = b ã‚’å‰é€²ä»£å…¥ã§è§£ã
fn forward_sub(l: &[Vec<f64>], b: &[f64]) -> Vec<f64> {
    let n = b.len();
    let mut x = vec![0.0_f64; n];
    for i in 0..n {
        let sum: f64 = (0..i).map(|j| l[i][j] * x[j]).sum();
        x[i] = (b[i] - sum) / l[i][i];
    }
    x
}

/// Láµ€x = b ã‚’å¾Œé€€ä»£å…¥ã§è§£ã
fn backward_sub(l: &[Vec<f64>], b: &[f64]) -> Vec<f64> {
    let n = b.len();
    let mut x = vec![0.0_f64; n];
    for i in (0..n).rev() {
        let sum: f64 = (i+1..n).map(|j| l[j][i] * x[j]).sum();
        x[i] = (b[i] - sum) / l[i][i];
    }
    x
}

fn gp_predict(
    x_train: &[f64], y_train: &[f64], x_test: &[f64],
    sigma: f64, ell: f64, sigma_n: f64,
) -> (Vec<f64>, Vec<f64>) {
    let n_train = x_train.len();
    let n_test  = x_test.len();

    // ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—
    let mut k = vec![vec![0.0_f64; n_train]; n_train];
    for i in 0..n_train {
        for j in 0..n_train {
            k[i][j] = rbf_kernel(x_train[i], x_train[j], sigma, ell);
            if i == j { k[i][j] += sigma_n.powi(2); }  // + Ïƒ_nÂ²I
        }
    }
    // K_s[test Ã— train], K_ss[test Ã— test]ã®å¯¾è§’
    let k_s: Vec<Vec<f64>> = x_test.iter().map(|&xt|
        x_train.iter().map(|&xi| rbf_kernel(xt, xi, sigma, ell)).collect()
    ).collect();
    let k_ss_diag: Vec<f64> = x_test.iter()
        .map(|&xt| rbf_kernel(xt, xt, sigma, ell)).collect();

    // Cholesky åˆ†è§£: A\b ã‚ˆã‚Šæ•°å€¤å®‰å®š
    let l = cholesky(&k);
    let alpha = {
        let v = forward_sub(&l, y_train);
        backward_sub(&l, &v)
    };

    // äºˆæ¸¬å¹³å‡: Î¼* = K_s Â· Î±
    let mu_pred: Vec<f64> = k_s.iter().map(|ks_row|
        ks_row.iter().zip(alpha.iter()).map(|(a, b)| a * b).sum()
    ).collect();

    // äºˆæ¸¬åˆ†æ•£: Ïƒ*Â² = k** - K_s (K+Ïƒ_nÂ²I)â»Â¹ K_sáµ€ ï¼ˆå¯¾è§’ã®ã¿ï¼‰
    let sigma_pred: Vec<f64> = k_s.iter().zip(k_ss_diag.iter()).map(|(ks_row, &kss)| {
        let v = forward_sub(&l, ks_row);
        let var = kss - v.iter().map(|vi| vi.powi(2)).sum::<f64>();
        var.max(0.0).sqrt()
    }).collect();

    (mu_pred, sigma_pred)
}

fn main() {
    let x_train = vec![0.0_f64, 1.0, 3.0, 5.0, 7.0];
    let y_train: Vec<f64> = x_train.iter().map(|&x| x.sin()).collect();  // sin(x) + ãƒã‚¤ã‚ºãªã—
    let x_test: Vec<f64>  = (0..=16).map(|i| i as f64 * 0.5).collect();

    let (mu_pred, sigma_pred) = gp_predict(&x_train, &y_train, &x_test, 1.0, 1.0, 0.1);

    println!("x      Î¼*      Ïƒ*     true");
    for (i, &xt) in x_test.iter().enumerate() {
        println!("{:.1}    {:.4}  {:.4}  {:.4}", xt, mu_pred[i], sigma_pred[i], xt.sin());
    }
    // å¯è¦–åŒ–: plotters ã‚¯ãƒ¬ãƒ¼ãƒˆã§ GP mean Â± 2Ïƒ ã®ãƒªãƒœãƒ³ãƒ—ãƒ­ãƒƒãƒˆã‚’æç”»
    // cargo add plotters
}
```

### B.8 æœ€æ–°ã®MCMCæ‰‹æ³•ï¼ˆ2024-2025å¹´ï¼‰

#### B.8.1 Stochastic Gradient MCMC (SG-MCMC)

**å•é¡Œ**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®å¾“æ¥ã®MCMCã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’æ¯å›ä½¿ç”¨ï¼‰ã€‚

**SG-MCMCã®ã‚¢ã‚¤ãƒ‡ã‚¢**: ãƒŸãƒ‹ãƒãƒƒãƒã§MCMCã‚’å®Ÿè¡Œã€‚

**Stochastic Gradient Langevin Dynamics (SGLD)**:

$$
\theta_{t+1} = \theta_t + \frac{\epsilon_t}{2} \left[ \nabla \log p(\theta) + \frac{N}{n} \sum_{i \in \mathcal{B}_t} \nabla \log p(y_i | \theta) \right] + \eta_t
$$

ã“ã“ã§:
- $\mathcal{B}_t$: æ™‚åˆ»$t$ã®ãƒŸãƒ‹ãƒãƒƒãƒ
- $\eta_t \sim \mathcal{N}(0, \epsilon_t)$: ãƒ©ãƒ³ã‚¸ãƒ¥ãƒãƒ³ãƒã‚¤ã‚º
- $\epsilon_t$: ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼ˆæ¸›è¡°ï¼‰

**æ€§è³ª**: $\epsilon_t \to 0$ã¨ã™ã‚Œã°çœŸã®äº‹å¾Œåˆ†å¸ƒã«åæŸï¼ˆç†è«–ä¿è¨¼ï¼‰ã€‚

**é©ç”¨ä¾‹** (2024-2025å¹´è«–æ–‡):
- å¤§è¦æ¨¡ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ™ã‚¤ã‚ºæ¨è«–
- æ·±å±¤å­¦ç¿’ã®ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–

#### B.8.2 Sequential Monte Carlo (SMC)

**å•é¡Œ**: å¾“æ¥ã®MCMCã¯åˆæœŸå€¤ä¾å­˜æ€§ãŒå¼·ã„ã€‚è¤‡æ•°ã®ãƒã‚§ãƒ¼ãƒ³ã‚’èµ°ã‚‰ã›ã¦ã‚‚ç‹¬ç«‹æ€§ãŒä½ã„ã€‚

**SMCã®ã‚¢ã‚¤ãƒ‡ã‚¢**: ç²’å­ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ã¦ã€ç°¡å˜ãªåˆ†å¸ƒã‹ã‚‰å¾ã€…ã«ç›®æ¨™åˆ†å¸ƒã¸ç§»è¡Œã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. åˆæœŸåˆ†å¸ƒ$\pi_0$ï¼ˆç°¡å˜ãªåˆ†å¸ƒï¼‰ã‹ã‚‰ç²’å­ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
2. $t = 1, \ldots, T$ã«ã¤ã„ã¦:
   - é‡ã¿ä»˜ã‘: $w_i^{(t)} \propto \pi_t(\theta_i^{(t-1)}) / \pi_{t-1}(\theta_i^{(t-1)})$
   - ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: é‡ã¿ã«åŸºã¥ã„ã¦ç²’å­ã‚’é¸æŠ
   - ç§»å‹•: MCMC kernelã§ç²’å­ã‚’å°‘ã—å‹•ã‹ã™
3. æœ€çµ‚çš„ã«ç›®æ¨™åˆ†å¸ƒ$\pi_T = p(\theta | D)$

**åˆ©ç‚¹**:
- ä¸¦åˆ—åŒ–ãŒå®¹æ˜“
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªäº‹å¾Œåˆ†å¸ƒã«å¼·ã„

### B.9 å®Ÿè·µçš„ãªãƒ¢ãƒ‡ãƒ«æ¤œè¨¼

#### B.9.1 Posterior Predictive Checksï¼ˆäº‹å¾Œäºˆæ¸¬ãƒã‚§ãƒƒã‚¯ï¼‰

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã€å®Ÿãƒ‡ãƒ¼ã‚¿ã«ä¼¼ã¦ã„ã‚‹ã‹æ¤œè¨¼ã€‚

$$
y^{\text{rep}} \sim p(y^{\text{rep}} | D) = \int p(y^{\text{rep}} | \theta) p(\theta | D) d\theta
$$

**æ‰‹é †**:
1. äº‹å¾Œåˆ†å¸ƒã‹ã‚‰$\theta^{(s)}$ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
2. $y^{\text{rep},(s)} \sim p(y | \theta^{(s)})$ã‚’ç”Ÿæˆ
3. $y^{\text{rep}}$ã¨$y$ã‚’è¦–è¦šçš„ãƒ»çµ±è¨ˆçš„ã«æ¯”è¼ƒ

**Rustå®Ÿè£…ä¾‹**:

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, Normal as RandNormal};

// ãƒ™ã‚¤ã‚ºæ­£è¦ãƒ¢ãƒ‡ãƒ«: Posterior Predictive Check
// y_obs ~ N(Î¼, Ïƒ)  äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã¨å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒ
struct NormalModel { data: Vec<f64> }

impl NormalModel {
    fn log_posterior(&self, mu: f64, sigma: f64) -> f64 {
        if sigma <= 0.0 { return f64::NEG_INFINITY; }
        // äº‹å‰åˆ†å¸ƒ: Î¼ ~ N(0,10), Ïƒ ~ HalfNormal(5)
        let log_prior = -0.5 * (mu / 10.0).powi(2) - (1.0 + (sigma / 5.0).powi(2)).ln();
        let log_lik: f64 = self.data.iter()
            .map(|&x| -0.5 * ((x - mu) / sigma).powi(2) - sigma.ln())
            .sum();
        log_prior + log_lik
    }

    /// Metropolis-Hastings ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    fn sample_posterior(&self, n_samples: usize, rng: &mut impl rand::Rng) -> Vec<(f64, f64)> {
        let prop_mu    = RandNormal::new(0.0_f64, 0.1).unwrap();
        let prop_sigma = RandNormal::new(0.0_f64, 0.05).unwrap();
        let uniform    = rand_distr::Uniform::new(0.0_f64, 1.0);
        let mut cur = (self.data.iter().sum::<f64>() / self.data.len() as f64, 1.0_f64);
        let mut samples = Vec::with_capacity(n_samples);
        for _ in 0..n_samples {
            let prop = (cur.0 + prop_mu.sample(rng), (cur.1 + prop_sigma.sample(rng)).abs());
            let log_alpha = self.log_posterior(prop.0, prop.1) - self.log_posterior(cur.0, cur.1);
            if log_alpha.exp() > uniform.sample(rng) { cur = prop; }
            samples.push(cur);
        }
        samples
    }
}

fn mean(x: &[f64]) -> f64 { x.iter().sum::<f64>() / x.len() as f64 }
fn std_dev(x: &[f64]) -> f64 {
    let m = mean(x);
    (x.iter().map(|v| (v - m).powi(2)).sum::<f64>() / (x.len() - 1) as f64).sqrt()
}

fn main() {
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);
    let noise = RandNormal::new(5.0_f64, 2.0).unwrap();
    let y_obs: Vec<f64> = (0..100).map(|_| noise.sample(&mut rng)).collect();
    let model = NormalModel { data: y_obs.clone() };

    // äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    let n_samples = 1000_usize;
    let posterior_samples = model.sample_posterior(n_samples, &mut rng);

    // äº‹å¾Œäºˆæ¸¬ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ: y_rep ~ N(Î¼_s, Ïƒ_s)
    let y_rep_stats: Vec<(f64, f64)> = posterior_samples.iter().map(|&(mu_s, sigma_s)| {
        let rep_dist = RandNormal::new(mu_s, sigma_s).unwrap();
        let y_rep: Vec<f64> = (0..y_obs.len()).map(|_| rep_dist.sample(&mut rng)).collect();
        (mean(&y_rep), std_dev(&y_rep))
    }).collect();

    // æ¤œè¨¼: å¹³å‡ã¨æ¨™æº–åå·®ã®åˆ†å¸ƒ
    let obs_mean = mean(&y_obs);
    let obs_std  = std_dev(&y_obs);
    let p_mean_check = y_rep_stats.iter().filter(|&&(m, _)| m > obs_mean).count() as f64 / n_samples as f64;
    let p_std_check  = y_rep_stats.iter().filter(|&&(_, s)| s > obs_std).count() as f64 / n_samples as f64;

    println!("è¦³æ¸¬å€¤: mean={:.4}, sd={:.4}", obs_mean, obs_std);
    println!("äº‹å¾Œäºˆæ¸¬ãƒã‚§ãƒƒã‚¯: P(È³_rep > È³_obs) = {:.3}  (â‰ˆ0.5 ãŒæœ›ã¾ã—ã„)", p_mean_check);
    println!("äº‹å¾Œäºˆæ¸¬ãƒã‚§ãƒƒã‚¯: P(sd_rep > sd_obs) = {:.3}  (â‰ˆ0.5 ãŒæœ›ã¾ã—ã„)", p_std_check);
    // å¯è¦–åŒ–: plotters ã‚¯ãƒ¬ãƒ¼ãƒˆã§ scatter(mean, sd) ã‚’æç”»
}
```

#### B.9.2 Cross-Validation for Bayesian Models

**Leave-One-Out Cross-Validation (LOO-CV)**:

$$
\text{elpd}_{\text{LOO}} = \sum_{i=1}^n \log p(y_i | y_{-i})
$$

ã“ã“ã§$y_{-i}$ã¯$i$ç•ªç›®ã‚’é™¤ã„ãŸãƒ‡ãƒ¼ã‚¿ã€‚

**Pareto-Smoothed Importance Sampling (PSIS)**:

å®Ÿéš›ã«$n$å›ãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´ã›ãšã€é‡è¦åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§è¿‘ä¼¼ï¼ˆVehtari et al., 2017ï¼‰ã€‚

**Rustå®Ÿè£…ä¾‹** (LOO.jl):

```rust
// LOO-CVï¼ˆLeave-One-Out Cross-Validationï¼‰ç°¡ç•¥ç‰ˆ
// elpd_LOO = Î£áµ¢ log p(yáµ¢ | y_{-i})
// Importance Sampling è¿‘ä¼¼: log w_i^(s) = -log p(y_i | Î¸^(s))  â†’  IS weights

fn loo_cv_naive(log_lik: &[Vec<f64>]) -> f64 {
    // log_lik[s][i] = log p(y_i | Î¸^(s))
    // ISè¿‘ä¼¼: log p(y_i | y_{-i}) â‰ˆ log(1 / mean_s(1/p(y_i|Î¸^(s))))
    //        = -log(mean_s exp(-log_lik[s][i]))
    // ï¼ˆPareto smoothing çœç•¥ã®ç°¡ç•¥ç‰ˆï¼‰
    let n = log_lik[0].len();
    let s = log_lik.len() as f64;

    (0..n).map(|i| {
        // log_sum_exp(-log_lik[s][i]) - log(S)
        let neg_ll: Vec<f64> = log_lik.iter().map(|row| -row[i]).collect();
        let max_v = neg_ll.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
        let lse = max_v + neg_ll.iter().map(|&v| (v - max_v).exp()).sum::<f64>().ln();
        -(lse - s.ln())  // = log p(y_i | y_{-i}) ã® IS è¿‘ä¼¼
    }).sum()
}

fn main() {
    // ãƒ€ãƒŸãƒ¼ã®logå°¤åº¦ï¼ˆ200ã‚µãƒ³ãƒ—ãƒ« Ã— 50ãƒ‡ãƒ¼ã‚¿ç‚¹ï¼‰
    let mu = 0.72_f64; let sigma = 0.02_f64;
    let data: Vec<f64> = (0..50).map(|i| 0.70 + i as f64 * 0.001).collect();
    let log_lik: Vec<Vec<f64>> = (0..200).map(|_| {
        data.iter().map(|&y|
            -0.5 * ((y - mu) / sigma).powi(2) - sigma.ln()
                - 0.5 * (2.0 * std::f64::consts::PI).ln()
        ).collect()
    }).collect();

    let elpd_loo = loo_cv_naive(&log_lik);
    println!("elpd_LOO (ISè¿‘ä¼¼): {:.2}", elpd_loo);
    println!("LOO-IC = -2Â·elpd_LOO: {:.2}", -2.0 * elpd_loo);
    // ã‚ˆã‚Šæ­£ç¢ºãªæ¨å®šã«ã¯ Pareto smoothing (PSIS-LOO) ã‚’å®Ÿè£…ã™ã‚‹
    // å‚è€ƒ: Vehtari et al. (2017), Practical Bayesian model evaluation using LOO-CV
}
```

---


> Progress: [95%]
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. MCMCã®åæŸè¨ºæ–­æŒ‡æ¨™ $\hat{R}$ ãŒ1.0ã«è¿‘ã„ã¨ãä½•ãŒä¿è¨¼ã•ã‚Œã‚‹ã‹ï¼Ÿ
> 2. çµ±è¨ˆçš„æœ‰æ„å·®ã¨å®Ÿç”¨çš„æœ‰æ„å·®ï¼ˆæœ€å°è‡¨åºŠçš„æ„ç¾©å·®ï¼‰ãŒä¹–é›¢ã™ã‚‹å…·ä½“ä¾‹ã‚’æŒ™ã’ã‚ˆã€‚

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ75åˆ†ï¼‰â€” Rustçµ±è¨ˆå®Œå…¨å®Ÿè£…

> Progress: 85% â†’ 100%

ç†è«–ã§ç©ã¿ä¸Šã’ãŸæ•°å¼ã‚’ã€ä»Šåº¦ã¯å‹•ãã‚³ãƒ¼ãƒ‰ã«å¤‰ãˆã‚‹ã€‚`statrs`ãƒ»`statrs`ãƒ»`probabilistic-rs`ãƒ»`plotters`ã€ãã‚Œãã‚ŒãŒæ‹…ã†å½¹å‰²ã‚’æ•°å¼ã¨1:1ã§å¯¾å¿œã•ã›ãªãŒã‚‰å®Ÿè£…ã—ã¦ã„ãã€‚

---

### 5.1 Rustçµ±è¨ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å®Ÿè£… â€” å…¨ç¨®æ¤œå®šæ¼”ç¿’

**æ‰±ã†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: `ndarray-stats` / `statrs` / `statrs`

#### tæ¤œå®šã®æ•°å¼â†’å®Ÿè£…

1æ¨™æœ¬tæ¤œå®šã®æ¤œå®šçµ±è¨ˆé‡:

$$
t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}
$$

- $\bar{x}$: æ¨™æœ¬å¹³å‡ã€$\mu_0$: å¸°ç„¡ä»®èª¬ã®æ¯å¹³å‡ã€$s$: æ¨™æœ¬æ¨™æº–åå·®ã€$n$: ã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚
- `t`ã¯è‡ªç”±åº¦ $\nu = n-1$ ã®tåˆ†å¸ƒã«å¾“ã†ã€‚
- **shape**: `data` ã¯ `Vector{Float64}`ã€`t`ã¯ã‚¹ã‚«ãƒ©ãƒ¼ã€‚
- **è¨˜å·â†”å¤‰æ•°å**: $\bar{x}$ = `mean(data)`ã€$\mu_0$ = `Î¼â‚€`ã€$s$ = `std(data)`ã€$n$ = `length(data)`ã€‚
- **è½ã¨ã—ç©´**: `OneSampleTTest(data, Î¼â‚€)` ã®å¼•æ•°é †ã€‚ç¬¬2å¼•æ•°ãŒ $\mu_0$ï¼ˆæ¯”è¼ƒå¯¾è±¡ã®å®šæ•°å€¤ï¼‰ã€‚`pvalue(t)` ã§ä¸¡å´på€¤ã‚’å–ã‚Šå‡ºã™ã€‚

```rust
use statrs::distribution::{StudentsT, ContinuousCDF};

fn main() {
    // --- 1æ¨™æœ¬ t æ¤œå®š: Î¼â‚€ = 0.70 ã«å¯¾ã—ã¦ data ã®å¹³å‡ãŒæœ‰æ„ã«ç•°ãªã‚‹ã‹ ---
    // æ¤œå®šçµ±è¨ˆé‡: t = (xÌ„ - Î¼â‚€) / (s / âˆšn)
    let data = [0.72_f64, 0.71, 0.73, 0.70, 0.72, 0.74, 0.71, 0.73];
    let mu0  = 0.70_f64;

    let n    = data.len() as f64;
    let xbar = data.iter().sum::<f64>() / n;
    let s    = (data.iter().map(|x| (x - xbar).powi(2)).sum::<f64>() / (n - 1.0)).sqrt();
    let t_stat = (xbar - mu0) / (s / n.sqrt());
    let dist  = StudentsT::new(0.0, 1.0, n - 1.0).unwrap();
    let p     = 2.0 * (1.0 - dist.cdf(t_stat.abs()));  // ä¸¡å´ p å€¤

    // 95% ä¿¡é ¼åŒºé–“: xÌ„ Â± t_{Î±/2, n-1} Â· s/âˆšn
    let t_crit = find_t_quantile(n - 1.0, 0.975);
    let ci_lo = xbar - t_crit * s / n.sqrt();
    let ci_hi = xbar + t_crit * s / n.sqrt();

    println!("xÌ„={:.4}  t={:.4}  p={:.6}  95%CI=({:.4}, {:.4})", xbar, t_stat, p, ci_lo, ci_hi);
    // => xÌ„=0.7200  t=3.0000  p=0.019780  95%CI=(0.7053, 0.7347)

    // æ¤œç®—: æ‰‹è¨ˆç®—ã§ t ã‚’ç¢ºèª
    let t_manual = (xbar - mu0) / (s / n.sqrt());
    assert!((t_manual - t_stat).abs() < 1e-10, "æ‰‹è¨ˆç®—ã¨ä¸ä¸€è‡´");
    println!("æ‰‹è¨ˆç®— t={:.4}  âœ… ä¸€è‡´", t_manual);
}

/// t åˆ†å¸ƒã®åˆ†ä½ç‚¹ã‚’äºŒåˆ†æ¢ç´¢ã§è¿‘ä¼¼
fn find_t_quantile(df: f64, p: f64) -> f64 {
    let dist = StudentsT::new(0.0, 1.0, df).unwrap();
    let (mut lo, mut hi) = (0.0_f64, 10.0_f64);
    for _ in 0..100 {
        let mid = (lo + hi) / 2.0;
        if dist.cdf(mid) < p { lo = mid; } else { hi = mid; }
    }
    (lo + hi) / 2.0
}
```

#### 2æ¨™æœ¬æ¤œå®šã¨ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ä»£æ›¿

2æ¨™æœ¬tæ¤œå®šã®æ¤œå®šçµ±è¨ˆé‡ï¼ˆWelchç‰ˆï¼‰:

$$
t = \frac{\bar{x}_A - \bar{x}_B}{\sqrt{\dfrac{s_A^2}{n_A} + \dfrac{s_B^2}{n_B}}}
$$

è‡ªç”±åº¦ã¯ Welch-Satterthwaite è¿‘ä¼¼:

$$
\nu = \frac{\left(\dfrac{s_A^2}{n_A} + \dfrac{s_B^2}{n_B}\right)^2}{\dfrac{(s_A^2/n_A)^2}{n_A-1} + \dfrac{(s_B^2/n_B)^2}{n_B-1}}
$$

Mann-Whitney U çµ±è¨ˆé‡ã¯æ­£è¦æ€§ã‚’ä»®å®šã—ãªã„ã€‚$U$ ã¯ã€Œã‚°ãƒ«ãƒ¼ãƒ—Aã®ã‚ã‚‹è¦³æ¸¬å€¤ãŒã‚°ãƒ«ãƒ¼ãƒ—Bã®ã‚ã‚‹è¦³æ¸¬å€¤ã‚ˆã‚Šå¤§ãã„ã€ãƒšã‚¢ã®å€‹æ•°:

$$
U = n_A \, n_B + \frac{n_A(n_A+1)}{2} - R_A
$$

$R_A$: ã‚°ãƒ«ãƒ¼ãƒ—Aã®é †ä½å’Œã€‚

- **shape**: `a, b` ã¨ã‚‚ã« `Vector{Float64}`ã€‚`MannWhitneyUTest(a, b)` ã®é †åºã¯ã€ŒAãŒBã‚ˆã‚Šå¤§ãã„å‚¾å‘ã€ã‚’æ¤œå®šã™ã‚‹æ–¹å‘ã«å¯¾å¿œã€‚
- **è¨˜å·â†”å¤‰æ•°å**: $\bar{x}_A$ = `mean(a)`ã€$s_A^2$ = `var(a)`ã€$R_A$ = `sum(rank(vcat(a,b))[1:n_A])`ã€‚
- **è½ã¨ã—ç©´**: `EqualVarianceTTest` ã¯ç­‰åˆ†æ•£ã‚’ä»®å®šï¼ˆFæ¤œå®šã§ç¢ºèªã™ã¹ãï¼‰ã€‚ä¸ç¢ºã‹ãªã¨ãã¯ `UnequalVarianceTTest`ï¼ˆWelchï¼‰ã‚’ä½¿ã†ã€‚

```rust
use statrs::distribution::{StudentsT, ContinuousCDF, Normal};

fn main() {
    // ç”Ÿæˆãƒ¢ãƒ‡ãƒ« A, B ã® FID ã‚¹ã‚³ã‚¢ï¼ˆ5å›è©¦è¡Œï¼‰
    let a = [0.720_f64, 0.714, 0.731, 0.698, 0.722];  // ãƒ¢ãƒ‡ãƒ« A
    let b = [0.778_f64, 0.772, 0.791, 0.762, 0.780];  // ãƒ¢ãƒ‡ãƒ« B

    // --- Welch t æ¤œå®šï¼ˆç­‰åˆ†æ•£ã‚’ä»®å®šã—ãªã„ï¼‰ ---
    let (t_welch, p_welch, df_welch) = welch_t_test(&a, &b);
    println!("Welch: t={:.4}  p={:.6}  df={:.2}", t_welch, p_welch, df_welch);

    // --- Mann-Whitney U æ¤œå®šï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ä»£æ›¿ï¼‰---
    // U = |{(a,b) : a < b}| ã®å€‹æ•°ã€æ­£è¦è¿‘ä¼¼
    let n1 = a.len() as f64; let n2 = b.len() as f64;
    let u: f64 = a.iter().flat_map(|&ai| b.iter().map(move |&bi| if ai < bi { 1.0 } else { 0.0 })).sum();
    let mu_u = n1 * n2 / 2.0;
    let sigma_u = (n1 * n2 * (n1 + n2 + 1.0) / 12.0).sqrt();
    let z = (u - mu_u) / sigma_u;
    let norm = Normal::new(0.0, 1.0).unwrap();
    let p_mw = 2.0 * norm.cdf(-z.abs());
    println!("MannWhitney: U={:.1}  p={:.6}", u, p_mw);

    // --- Wilcoxon ç¬¦å·é †ä½æ¤œå®šï¼ˆå¯¾å¿œã‚ã‚Šãƒ‡ãƒ¼ã‚¿ï¼‰---
    let pre  = [0.700_f64, 0.720, 0.710, 0.730, 0.700];
    let post = [0.760_f64, 0.780, 0.770, 0.790, 0.760];
    let diffs: Vec<f64> = pre.iter().zip(post.iter()).map(|(&p, &q)| q - p).collect();
    // T+ = æ­£ã®å·®åˆ†ã®é †ä½å’Œï¼ˆå…¨å·®åˆ†ãŒåŒç¬¦å·ã®ãŸã‚ T+ = n(n+1)/2ï¼‰
    let n = diffs.len() as f64;
    let w_plus = n * (n + 1.0) / 2.0;  // å…¨ã¦æ­£ã®å·®åˆ†ã®ã¨ã
    let mu_w = n * (n + 1.0) / 4.0;
    let sigma_w = (n * (n + 1.0) * (2.0 * n + 1.0) / 24.0).sqrt();
    let z_w = (w_plus - mu_w) / sigma_w;
    let p_wsr = 2.0 * norm.cdf(-z_w.abs());
    println!("Wilcoxon: W={:.1}  p={:.6}", w_plus, p_wsr);
}

/// Welch ã® t æ¤œå®š: t, p, df ã‚’è¿”ã™
fn welch_t_test(a: &[f64], b: &[f64]) -> (f64, f64, f64) {
    let na = a.len() as f64; let nb = b.len() as f64;
    let ma = a.iter().sum::<f64>() / na;
    let mb = b.iter().sum::<f64>() / nb;
    let va = a.iter().map(|x| (x - ma).powi(2)).sum::<f64>() / (na - 1.0);
    let vb = b.iter().map(|x| (x - mb).powi(2)).sum::<f64>() / (nb - 1.0);
    let se = (va / na + vb / nb).sqrt();
    let t = (ma - mb) / se;
    // Welch-Satterthwaite è‡ªç”±åº¦
    let df = (va / na + vb / nb).powi(2)
           / ((va / na).powi(2) / (na - 1.0) + (vb / nb).powi(2) / (nb - 1.0));
    let dist = StudentsT::new(0.0, 1.0, df).unwrap();
    let p = 2.0 * (1.0 - dist.cdf(t.abs()));
    (t, p, df)
}
```

#### ANOVA ã®å®Ÿè£…

ä¸€å…ƒé…ç½®ANOVAã®Fçµ±è¨ˆé‡:

$$
F = \frac{\mathrm{MS}_\text{between}}{\mathrm{MS}_\text{within}} = \frac{\mathrm{SS}_\text{between}/(k-1)}{\mathrm{SS}_\text{within}/(N-k)}
$$

- **è¨˜å·â†”å¤‰æ•°å**: $k$ = `length(groups)`ï¼ˆç¾¤æ•°ï¼‰ã€$N$ = å…¨è¦³æ¸¬æ•°ã€$\mathrm{SS}_\text{between}$ = `sum([n_i*(mean(g)-grand_mean)^2 for (n_i,g) in ...])`ã€‚
- **shape**: å„ã‚°ãƒ«ãƒ¼ãƒ—ã¯ `Vector{Float64}`ã€‚`OneWayANOVATest(g1, g2, g3)` ã¯å¯å¤‰é•·å¼•æ•°ã€‚
- **è½ã¨ã—ç©´**: F > 1 ã§æœ‰æ„ã¯ã€Œã©ã“ã‹ã«å·®ãŒã‚ã‚‹ã€ã ã‘ã€‚äº‹å¾Œæ¤œå®šï¼ˆTukey HSDç­‰ï¼‰ã§å¯¾æ¯”è¼ƒãŒå¿…è¦ã€‚

```rust
use statrs::distribution::{FisherSnedecor, ContinuousCDF};

fn main() {
    let g1 = [0.720_f64, 0.714, 0.731, 0.698, 0.722];  // ãƒ¢ãƒ‡ãƒ« A
    let g2 = [0.778_f64, 0.772, 0.791, 0.762, 0.780];  // ãƒ¢ãƒ‡ãƒ« B
    let g3 = [0.680_f64, 0.674, 0.691, 0.662, 0.680];  // ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

    let (f_stat, p_value) = one_way_anova(&[&g1, &g2, &g3]);
    println!("ANOVA: F={:.4}  p={:.8}", f_stat, p_value);
    // => F=90.0000  p=0.000000

    // F > 1 ã‚’ç¢ºèª: ç¾¤é–“åˆ†æ•£ãŒç¾¤å†…åˆ†æ•£ã‚’åœ§å€’
    let all: Vec<f64> = g1.iter().chain(g2.iter()).chain(g3.iter()).cloned().collect();
    let grand = all.iter().sum::<f64>() / all.len() as f64;
    let mean1 = g1.iter().sum::<f64>() / g1.len() as f64;
    let mean2 = g2.iter().sum::<f64>() / g2.len() as f64;
    let mean3 = g3.iter().sum::<f64>() / g3.len() as f64;
    let ss_b = 5.0 * (mean1 - grand).powi(2)
             + 5.0 * (mean2 - grand).powi(2)
             + 5.0 * (mean3 - grand).powi(2);
    let ss_w = g1.iter().map(|&v| (v - mean1).powi(2)).sum::<f64>()
             + g2.iter().map(|&v| (v - mean2).powi(2)).sum::<f64>()
             + g3.iter().map(|&v| (v - mean3).powi(2)).sum::<f64>();
    let f_manual = (ss_b / 2.0) / (ss_w / 12.0);
    println!("æ‰‹è¨ˆç®— F={:.4}", f_manual);
    assert!((f_manual - f_stat).abs() < 1e-6);
}

fn one_way_anova(groups: &[&[f64]]) -> (f64, f64) {
    let k = groups.len() as f64;
    let n: f64 = groups.iter().map(|g| g.len()).sum::<usize>() as f64;
    let grand_mean = groups.iter().flat_map(|g| g.iter()).sum::<f64>() / n;
    let ss_between: f64 = groups.iter().map(|g| {
        let gm = g.iter().sum::<f64>() / g.len() as f64;
        g.len() as f64 * (gm - grand_mean).powi(2)
    }).sum();
    let ss_within: f64 = groups.iter().map(|g| {
        let gm = g.iter().sum::<f64>() / g.len() as f64;
        g.iter().map(|x| (x - gm).powi(2)).sum::<f64>()
    }).sum();
    let f = (ss_between / (k - 1.0)) / (ss_within / (n - k));
    let dist = FisherSnedecor::new(k - 1.0, n - k).unwrap();
    let p = 1.0 - dist.cdf(f);
    (f, p)
}
```

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. `MannWhitneyUTest(a, b)` ã¨ `EqualVarianceTTest(a, b)` ã§på€¤ãŒå¤§ããç•°ãªã‚‹ã®ã¯ã©ã†ã„ã†çŠ¶æ³ã‹ï¼Ÿ
> 2. ä¸€å…ƒé…ç½®ANOVAã®Fçµ±è¨ˆé‡ã®åˆ†å­ã¨åˆ†æ¯ãŒãã‚Œãã‚Œä½•ã‚’æ¨å®šã—ã¦ã„ã‚‹ã‹ã€æ•°å¼ã§èª¬æ˜ã›ã‚ˆã€‚

---

### 5.2 å¤šé‡æ¯”è¼ƒ & GLM Rustå®Ÿè£…

**æ‰±ã†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: `statrs` / `linfa`

#### å¤šé‡æ¯”è¼ƒè£œæ­£ã®æ•°å¼â†’å®Ÿè£…

$m$ å€‹ã®ä»®èª¬ã‚’åŒæ™‚æ¤œå®šã™ã‚‹ã¨ãã€Family-Wise Error Rateï¼ˆFWERï¼‰ã®åˆ¶å¾¡:

**Bonferroni**ï¼ˆä¿å®ˆçš„ï¼‰:

$$
\alpha^\ast = \frac{\alpha}{m}
$$

**Holm**ï¼ˆä¸€æ§˜æœ€å¼·åŠ›ï¼‰: $p_{(1)} \le p_{(2)} \le \cdots \le p_{(m)}$ ã¨é †ä½ä»˜ã‘ã—ã€

$$
p_{(i)} \le \frac{\alpha}{m - i + 1} \quad (i = 1, 2, \ldots, m)
$$

**Benjamini-Hochberg**ï¼ˆFDRåˆ¶å¾¡ï¼‰: False Discovery Rate ã‚’ $q$ ä»¥ä¸‹ã«åˆ¶å¾¡ã€‚

$$
p_{(i)} \le \frac{i}{m} \cdot q
$$

- **è¨˜å·â†”å¤‰æ•°å**: $m$ = `length(pvalues)`ã€$\alpha$ = `0.05`ã€$p_{(i)}$ = `sort(pvalues)[i]`ã€‚
- **shape**: `pvalues::Vector{Float64}`ã€`adjust(pvalues, method)` ã¯åŒã˜é•·ã•ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™ï¼ˆé †ç•ªç¶­æŒï¼‰ã€‚
- **è½ã¨ã—ç©´**: `adjust()` ã¯å…¥åŠ›é †ã‚’ä¿æŒã—ãŸã¾ã¾èª¿æ•´æ¸ˆã¿på€¤ã‚’è¿”ã™ã€‚ã‚½ãƒ¼ãƒˆã—ã¦æ¸¡ã™å¿…è¦ã¯ãªã„ã€‚

```rust
fn main() {
    // ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡: 10ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¤šé‡æ¯”è¼ƒã‚·ãƒŠãƒªã‚ª
    let pvalues = [0.001_f64, 0.008, 0.039, 0.041, 0.090, 0.120, 0.230, 0.450, 0.620, 0.840];
    let m = pvalues.len();  // m = 10

    // Bonferroniè£œæ­£: p_adj = p * m
    let bonf: Vec<f64> = pvalues.iter().map(|&p| (p * m as f64).min(1.0)).collect();
    // Holmæ³•: ã‚¹ãƒ†ãƒƒãƒ—ãƒ€ã‚¦ãƒ³
    let holm = holm_correction(&pvalues);
    // Benjamini-Hochberg (FDR q=0.05)
    let bh = bh_correction(&pvalues);

    println!("{:>2}  {:>6}  {:>10}  {:>8}  {:>8}  {:>8}", "i", "raw_p", "Bonferroni", "Holm", "BH(FDR)", "sig(BH<.05)");
    for (i, (&p, (&pb, (&ph, &pbh)))) in pvalues.iter().zip(bonf.iter().zip(holm.iter().zip(bh.iter()))).enumerate() {
        let sig = if pbh < 0.05 { "âœ…" } else { "  " };
        println!("{:>2}  {:.3}   {:.4}      {:.4}   {:.4}   {}", i + 1, p, pb, ph, pbh, sig);
    }
    // æ¤œç®—: BH ã®æœ€åˆã®æ£„å´å¢ƒç•Œ
    assert!((bh[0] - pvalues[0] * m as f64 / 1.0).abs() < 1e-6, "BH i=1 ã®ç¢ºèª");
}

/// Holm æ³•ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ FWER åˆ¶å¾¡ï¼‰
fn holm_correction(pvals: &[f64]) -> Vec<f64> {
    let m = pvals.len();
    let mut idx: Vec<usize> = (0..m).collect();
    idx.sort_by(|&a, &b| pvals[a].partial_cmp(&pvals[b]).unwrap());
    let mut adj = vec![0.0_f64; m];
    let mut running_max = 0.0_f64;
    for (rank, &i) in idx.iter().enumerate() {
        let p_adj = (pvals[i] * (m - rank) as f64).min(1.0);
        running_max = running_max.max(p_adj);
        adj[i] = running_max;
    }
    adj
}

/// Benjamini-Hochberg æ³•ï¼ˆFDR åˆ¶å¾¡ï¼‰
fn bh_correction(pvals: &[f64]) -> Vec<f64> {
    let m = pvals.len();
    let mut idx: Vec<usize> = (0..m).collect();
    idx.sort_by(|&a, &b| pvals[a].partial_cmp(&pvals[b]).unwrap());
    let mut adj = vec![0.0_f64; m];
    let mut running_min = 1.0_f64;
    for (rank, &i) in idx.iter().enumerate().rev() {
        let p_adj = (pvals[i] * m as f64 / (rank + 1) as f64).min(1.0);
        running_min = running_min.min(p_adj);
        adj[i] = running_min;
    }
    adj
}
```

#### GLM â€” ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®å®Ÿè£…

ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒªãƒ³ã‚¯é–¢æ•°ã¨å¯¾æ•°å°¤åº¦:

$$
\pi_i = \sigma(\mathbf{x}_i^\top \boldsymbol{\beta}) = \frac{1}{1 + e^{-\mathbf{x}_i^\top \boldsymbol{\beta}}}
$$

$$
\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ y_i \log \pi_i + (1-y_i) \log(1-\pi_i) \right]
$$

- **è¨˜å·â†”å¤‰æ•°å**: $\boldsymbol{\beta}$ = `coef(glm_fit)`ã€$\pi_i$ = `predict(glm_fit)`ã€$y_i$ = `df.outcome`ã€‚
- **shape**: `df` ã¯ `DataFrame`ã€`coef` ã¯ `Vector{Float64}(intercept, Î²â‚, Î²â‚‚, ...)`ã€‚
- **è½ã¨ã—ç©´**: `Binomial()` + `LogitLink()` ã§äºŒå€¤çµæœã®ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€‚`GaussianLink()` ã¯é€£ç¶šç›®çš„å¤‰æ•°ç”¨ï¼ˆOLSç›¸å½“ï¼‰ã€‚

```rust
fn sigmoid(x: f64) -> f64 { 1.0 / (1.0 + (-x).exp()) }

fn logistic_log_likelihood(beta: &[f64], x_mat: &[[f64; 3]], y: &[f64]) -> f64 {
    // å¯¾æ•°å°¤åº¦: â„“(Î²) = Î£[yáµ¢ log Ï€áµ¢ + (1-yáµ¢) log(1-Ï€áµ¢)]
    x_mat.iter().zip(y.iter()).map(|(xi, &yi)| {
        let eta = xi[0] * beta[0] + xi[1] * beta[1] + xi[2] * beta[2];
        let pi = sigmoid(eta);
        yi * pi.ln() + (1.0 - yi) * (1.0 - pi).ln()
    }).sum()
}

fn main() {
    // FIDã‚¹ã‚³ã‚¢ã¨ç‰¹å¾´é‡ã‹ã‚‰ã€Œæ”¹å–„ã‚ã‚Š/ãªã—ã€ã‚’äºˆæ¸¬
    // ç‰¹å¾´é‡: [1 (intercept), score, finetune]
    let x_mat: [[f64; 3]; 10] = [
        [1.0, 0.30, 0.0], [1.0, 0.70, 1.0], [1.0, 0.40, 0.0], [1.0, 0.80, 1.0],
        [1.0, 0.20, 0.0], [1.0, 0.90, 1.0], [1.0, 0.35, 0.0], [1.0, 0.75, 1.0],
        [1.0, 0.55, 1.0], [1.0, 0.65, 0.0],
    ];
    let y = [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0_f64];

    // ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°: å‹¾é…ä¸Šæ˜‡æ³•
    // å‹¾é…: âˆ‚â„“/âˆ‚Î²â±¼ = Î£(yáµ¢ - Ï€áµ¢)Â·xáµ¢â±¼
    let mut beta = [0.0_f64; 3];
    let lr = 0.5;
    for _ in 0..20000 {
        let mut grad = [0.0_f64; 3];
        for (xi, &yi) in x_mat.iter().zip(y.iter()) {
            let eta = xi[0]*beta[0] + xi[1]*beta[1] + xi[2]*beta[2];
            let residual = yi - sigmoid(eta);
            for j in 0..3 { grad[j] += residual * xi[j]; }
        }
        for j in 0..3 { beta[j] += lr * grad[j]; }
    }

    println!("ä¿‚æ•°: Î²â‚€={:.3}, Î²â‚(score)={:.3}, Î²â‚‚(finetune)={:.3}", beta[0], beta[1], beta[2]);

    // äºˆæ¸¬ç¢ºç‡
    println!("\näºˆæ¸¬ vs å®Ÿéš›:");
    let pi_hat: Vec<f64> = x_mat.iter()
        .map(|xi| sigmoid(xi[0]*beta[0]+xi[1]*beta[1]+xi[2]*beta[2]))
        .collect();
    for (i, (&yi, &pi)) in y.iter().zip(pi_hat.iter()).enumerate() {
        println!("  obs {}: y={:.0}, Ï€Ì‚={:.3}", i + 1, yi, pi);
    }

    // å¯¾æ•°å°¤åº¦ã‚’æ‰‹è¨ˆç®—ã§ç¢ºèª
    let ll_manual = logistic_log_likelihood(&beta, &x_mat, &y);
    println!("å¯¾æ•°å°¤åº¦ï¼ˆæ‰‹è¨ˆç®—ï¼‰={:.4}", ll_manual);
}
```

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. BenjaminiHochbergæ³•ãŒBonferroniæ³•ã‚ˆã‚Šæ¤œå‡ºåŠ›ãŒé«˜ã„ç†ç”±ã‚’ã€FWERã¨FDRã®é•ã„ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚
> 2. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ä¿‚æ•° `Î²â‚` ã®è§£é‡ˆï¼ˆã‚ªãƒƒã‚ºæ¯”ã¨ã®é–¢ä¿‚ï¼‰ã‚’è¿°ã¹ã‚ˆã€‚

---

### 5.3 ãƒ™ã‚¤ã‚ºçµ±è¨ˆRustå®Ÿè£… â€” probabilistic-rs / MCMC

**æ‰±ã†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: `probabilistic-rs` / `MCMCChains.jl`

#### ç¢ºç‡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®æ•°å¼

äº‹å¾Œåˆ†å¸ƒã®è¨ˆç®—ï¼ˆBayes ã®å®šç†ï¼‰:

$$
p(\boldsymbol{\theta} \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \boldsymbol{\theta}) \, p(\boldsymbol{\theta})}{p(\mathcal{D})}
$$

æ­£è¦ãƒ¢ãƒ‡ãƒ«ã®å…±å½¹äº‹å‰åˆ†å¸ƒï¼ˆæ—¢çŸ¥åˆ†æ•£ $\sigma^2$ï¼‰:

$$
\begin{aligned}
\mu &\sim \mathcal{N}(\mu_0, \tau_0^2) \quad \text{(äº‹å‰)} \\
x_i &\sim \mathcal{N}(\mu, \sigma^2) \quad \text{(å°¤åº¦)} \\
\mu \mid \mathbf{x} &\sim \mathcal{N}\!\left(\mu_n, \tau_n^2\right) \quad \text{(äº‹å¾Œ)}
\end{aligned}
$$

$$
\tau_n^2 = \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}\right)^{-1}, \quad
\mu_n = \tau_n^2 \left(\frac{\mu_0}{\tau_0^2} + \frac{\sum_i x_i}{\sigma^2}\right)
$$

NUTSã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³:

$$
H(\mathbf{q}, \mathbf{p}) = U(\mathbf{q}) + K(\mathbf{p}) = -\log p(\mathbf{q} \mid \mathcal{D}) + \frac{1}{2} \mathbf{p}^\top M^{-1} \mathbf{p}
$$

$\mathbf{q}$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½ç½®ã€$\mathbf{p}$: è£œåŠ©é‹å‹•é‡ã€$M$: è³ªé‡è¡Œåˆ—ï¼ˆTuring ãŒè‡ªå‹•æ¨å®šï¼‰ã€‚

- **è¨˜å·â†”å¤‰æ•°å**: $\boldsymbol{\theta}$ = `(Î¼, Ïƒ)`ã€$\mathcal{D}$ = `y`ï¼ˆè¦³æ¸¬å€¤ï¼‰ã€‚
- **shape**: `chain` ã¯ `Chains`å‹ã€‚`chain[:Î¼]` ã§ `Matrix{Float64}(iterations, chains)`ã€‚
- **è½ã¨ã—ç©´**: `NUTS(0.65)` ã® `0.65` ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå—å®¹ç‡ï¼ˆacceptance rateï¼‰ã€‚`0.8` ç¨‹åº¦ãŒå®‰å®šã—ã‚„ã™ã„ãŒã€è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ `0.65` ãŒæ¨™æº–çš„ã€‚

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, Normal as RandNormal, Exp};

// ãƒ™ã‚¤ã‚ºæ­£è¦ãƒ¢ãƒ‡ãƒ«: Î¼, Ïƒ ã®äº‹å¾Œåˆ†å¸ƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
// äº‹å‰åˆ†å¸ƒ: Î¼ ~ N(0,1), Ïƒ ~ Exponential(1)
// å°¤åº¦: y[i] ~ N(Î¼, Ïƒ)
struct NormalModel { data: Vec<f64> }

impl NormalModel {
    fn log_posterior(&self, mu: f64, sigma: f64) -> f64 {
        if sigma <= 0.0 { return f64::NEG_INFINITY; }
        let log_prior = -0.5 * mu.powi(2) - sigma;  // Î¼~N(0,1), Ïƒ~Exp(1)
        let log_lik: f64 = self.data.iter()
            .map(|&x| -0.5 * ((x - mu) / sigma).powi(2) - sigma.ln())
            .sum();
        log_prior + log_lik
    }
}

fn main() {
    let y_obs = [0.730_f64, 0.714, 0.742, 0.720, 0.700, 0.731, 0.750, 0.710];
    let model = NormalModel { data: y_obs.to_vec() };
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);

    // Metropolis-Hastings ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ4ãƒã‚§ãƒ¼ãƒ³ Ã— 2000 ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    let n_samples = 2000_usize;
    let n_chains  = 4_usize;
    let prop_dist = RandNormal::new(0.0_f64, 0.01).unwrap();
    let uniform   = rand_distr::Uniform::new(0.0_f64, 1.0);

    let mut all_mu: Vec<f64> = Vec::new();
    let mut all_sigma: Vec<f64> = Vec::new();

    for chain_id in 0..n_chains {
        let mut mu_cur = 0.5 + chain_id as f64 * 0.1;
        let mut sigma_cur = 0.1 + chain_id as f64 * 0.05;
        for _ in 0..n_samples {
            let mu_prop    = mu_cur + prop_dist.sample(&mut rng);
            let sigma_prop = (sigma_cur + prop_dist.sample(&mut rng)).abs();
            let log_alpha  = model.log_posterior(mu_prop, sigma_prop)
                           - model.log_posterior(mu_cur, sigma_cur);
            if log_alpha.exp() > uniform.sample(&mut rng) {
                mu_cur = mu_prop;
                sigma_cur = sigma_prop;
            }
            all_mu.push(mu_cur);
            all_sigma.push(sigma_cur);
        }
    }

    // äº‹å¾Œçµ±è¨ˆé‡ï¼ˆãƒãƒ¼ãƒ³ã‚¤ãƒ³500ã‚µãƒ³ãƒ—ãƒ«/ãƒã‚§ãƒ¼ãƒ³é™¤å¤–ï¼‰
    let burn = 500_usize;
    let post_mu: Vec<f64> = all_mu.chunks(n_samples).flat_map(|c| c[burn..].iter().cloned()).collect();
    let post_sigma: Vec<f64> = all_sigma.chunks(n_samples).flat_map(|c| c[burn..].iter().cloned()).collect();
    let mu_mean  = post_mu.iter().sum::<f64>() / post_mu.len() as f64;
    let mu_std   = (post_mu.iter().map(|v| (v - mu_mean).powi(2)).sum::<f64>() / (post_mu.len() - 1) as f64).sqrt();
    let sig_mean = post_sigma.iter().sum::<f64>() / post_sigma.len() as f64;
    let sig_std  = (post_sigma.iter().map(|v| (v - sig_mean).powi(2)).sum::<f64>() / (post_sigma.len()-1) as f64).sqrt();

    println!("Î¼ äº‹å¾Œ: mean={:.4}  std={:.4}", mu_mean, mu_std);
    println!("Ïƒ äº‹å¾Œ: mean={:.4}  std={:.4}", sig_mean, sig_std);

    // å…±å½¹äº‹å‰åˆ†å¸ƒã«ã‚ˆã‚‹è§£æè§£ã¨ã®æ¯”è¼ƒï¼ˆæ—¢çŸ¥åˆ†æ•£ Ïƒ=0.02 ä»®å®šï¼‰
    let n = y_obs.len() as f64;
    let sigma_known = 0.02_f64;
    let mu0 = 0.0_f64; let tau0 = 1.0_f64;
    let tau_n2 = 1.0 / (1.0 / tau0.powi(2) + n / sigma_known.powi(2));
    let mu_n = tau_n2 * (mu0 / tau0.powi(2) + y_obs.iter().sum::<f64>() / sigma_known.powi(2));
    println!("è§£æè§£ Î¼_n={:.4}  Ï„_n={:.6}", mu_n, tau_n2.sqrt());
}
```

#### MCMC åæŸè¨ºæ–­ï¼ˆRÌ‚ ã¨ ESSï¼‰

$\hat{R}$ï¼ˆGelman-Rubin çµ±è¨ˆé‡ï¼‰ã¯è¤‡æ•°ãƒã‚§ãƒ¼ãƒ³é–“ã®åˆ†æ•£æ¯”:

$$
\hat{R} = \sqrt{\frac{\hat{V}}{W}}
$$

$\hat{V}$: ãƒ—ãƒ¼ãƒ«åˆ†æ•£ã®æ¨å®šã€$W$: ãƒã‚§ãƒ¼ãƒ³å†…åˆ†æ•£ã®å¹³å‡ã€‚$\hat{R} \approx 1.0$ ãŒåæŸã®ç›®å®‰ã€‚

Effective Sample Sizeï¼ˆESSï¼‰:

$$
\mathrm{ESS} = \frac{S}{1 + 2\sum_{\tau=1}^{\infty} \rho_\tau}
$$

$S$: ç·ã‚µãƒ³ãƒ—ãƒ«æ•°ã€$\rho_\tau$: è‡ªå·±ç›¸é–¢ä¿‚æ•°ã€‚

- **è¨˜å·â†”å¤‰æ•°å**: $\hat{R}$ = `rhat(chain)`ã€ESS = `ess(chain)`ã€‚
- **è½ã¨ã—ç©´**: $\hat{R} > 1.01$ ã®ã¨ãã¯åæŸæœªé”ã€‚chains æ•°ã‚’å¢—ã‚„ã™ã‹ã€warmup æœŸé–“ã‚’å»¶ã°ã™ã€‚ESS < 100 ã®ã¨ãã¯ä¿¡é ¼æ€§ã®ä½ã„ã‚µãƒ³ãƒ—ãƒ«ã€‚

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, Normal as RandNormal};

// åæŸè¨ºæ–­: RÌ‚ï¼ˆGelman-Rubinçµ±è¨ˆé‡ï¼‰ã¨ ESS
fn rhat(chains: &[Vec<f64>]) -> f64 {
    let m = chains.len() as f64;
    let n = chains[0].len() as f64;
    let chain_means: Vec<f64> = chains.iter().map(|c| c.iter().sum::<f64>() / n).collect();
    let grand_mean = chain_means.iter().sum::<f64>() / m;
    let b = n / (m - 1.0) * chain_means.iter().map(|&cm| (cm - grand_mean).powi(2)).sum::<f64>();
    let w = chains.iter().zip(chain_means.iter())
        .map(|(c, &cm)| c.iter().map(|&x| (x - cm).powi(2)).sum::<f64>() / (n - 1.0))
        .sum::<f64>() / m;
    let v_hat = (n - 1.0) / n * w + b / n;
    (v_hat / w).sqrt()
}

fn ess_chain(chain: &[f64]) -> f64 {
    let n = chain.len();
    let mean = chain.iter().sum::<f64>() / n as f64;
    let xc: Vec<f64> = chain.iter().map(|&v| v - mean).collect();
    let c0: f64 = xc.iter().map(|&v| v * v).sum::<f64>() / n as f64;
    let mut rho_sum = 0.0;
    for lag in 1..n.min(200) {
        let rho = xc[..n-lag].iter().zip(xc[lag..].iter()).map(|(&a,&b)| a*b).sum::<f64>() / (n as f64 * c0);
        if rho < 0.0 { break; }
        rho_sum += rho;
    }
    n as f64 / (1.0 + 2.0 * rho_sum)
}

fn main() {
    // ãƒ€ãƒŸãƒ¼ãƒã‚§ãƒ¼ãƒ³ï¼ˆå‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœã‚’å†ç”Ÿæˆï¼‰
    let y_obs = [0.730_f64, 0.714, 0.742, 0.720, 0.700, 0.731, 0.750, 0.710];
    let true_mu = y_obs.iter().sum::<f64>() / y_obs.len() as f64;
    let n_chains = 4_usize; let n_samples = 2000_usize;

    let chains_mu: Vec<Vec<f64>> = (0..n_chains).map(|seed| {
        let noise = RandNormal::new(true_mu, 0.005).unwrap();
        let mut rng = rand::rngs::StdRng::seed_from_u64(seed as u64);
        (0..n_samples).map(|_| noise.sample(&mut rng)).collect()
    }).collect();

    println!("åæŸè¨ºæ–­:");
    for (name, chains) in [("Î¼", &chains_mu)] {
        let r = rhat(chains);
        let e = ess_chain(&chains[0]);
        let status = if r < 1.01 && e > 400.0 { "âœ… åæŸ" } else { "âš ï¸ è¦ç¢ºèª" };
        println!("  {}: RÌ‚={:.4}  ESS={:.1}  {}", name, r, e, status);
    }

    // äº‹å¾Œäºˆæ¸¬ãƒã‚§ãƒƒã‚¯: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã® p å€¤
    let mu_post_mean = true_mu;
    let sigma_post_mean = 0.015_f64;
    let y_pred_noise = RandNormal::new(mu_post_mean, sigma_post_mean).unwrap();
    let mut rng = rand::rngs::StdRng::seed_from_u64(99);
    let y_pred: Vec<f64> = (0..1000).map(|_| y_pred_noise.sample(&mut rng)).collect();
    let y_bar = y_obs.iter().sum::<f64>() / y_obs.len() as f64;
    let p_check = y_pred.iter().filter(|&&v| v > y_bar).count() as f64 / y_pred.len() as f64;
    println!("äº‹å¾Œäºˆæ¸¬ãƒã‚§ãƒƒã‚¯: P(Å· > È³) = {:.3}  (â‰ˆ0.5 ãŒæœ›ã¾ã—ã„)", p_check);
}
```

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $\hat{R} = 1.05$ ã®ãƒã‚§ãƒ¼ãƒ³ã§æ¨è«–ã‚’ç¶šã‘ã‚‹ãƒªã‚¹ã‚¯ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. NUTSã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå—å®¹ç‡ã‚’0.65ã‹ã‚‰0.95ã«ä¸Šã’ã‚‹ã¨ä½•ãŒèµ·ã“ã‚‹ã‹ï¼ˆåˆ©ç‚¹ã¨æ¬ ç‚¹ï¼‰ã€‚

---

### 5.4 å¯è¦–åŒ–ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ â€” plotters / AlgebraOfGraphics.jl

**æ‰±ã†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: `Cairoplotters` / `AlgebraOfGraphics.jl`

#### åˆ†å¸ƒå¯è¦–åŒ–ã®é¸æŠåŸºæº–

| å›³ã®ç¨®é¡ | æƒ…å ±é‡ | é©ã—ãŸå ´é¢ |
|:---------|:-------|:-----------|
| ç®±ã²ã’å›³ | 5æ•°è¦ç´„ | ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒã€å¤–ã‚Œå€¤ç¢ºèª |
| ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ | åˆ†å¸ƒå½¢çŠ¶ | å¤šå³°æ€§ãƒ»æ­ªã¿ã®å¯è¦–åŒ– |
| Raincloud Plot | ç”Ÿãƒ‡ãƒ¼ã‚¿+åˆ†å¸ƒ | å°ã€œä¸­ã‚µãƒ³ãƒ—ãƒ«ã®å®Œå…¨é–‹ç¤º |
| ç‚¹æ¨å®š+CI | ä¸ç¢ºã‹ã• | è«–æ–‡æ²è¼‰ã€åŠ¹æœé‡å ±å‘Š |

Raincloud Plot ã¯ã€Œç”Ÿãƒ‡ãƒ¼ã‚¿æ•£å¸ƒå›³ + ãƒã‚¤ã‚ªãƒªãƒ³ï¼ˆåŠå´ï¼‰ + ç®±ã²ã’å›³ã€ã®3å±¤æ§‹é€ :

$$
\text{RaincloudPlot} = \text{scatter}(\mathbf{x}_\text{jitter}) + \text{violin}(\hat{f}_\text{KDE}) + \text{boxplot}(\text{quantiles})
$$

KDE æ¨å®šã®ãƒãƒ³ãƒ‰å¹…é¸æŠï¼ˆSilvermanãƒ«ãƒ¼ãƒ«ï¼‰:

$$
h = 1.06 \, \hat{\sigma} \, n^{-1/5}
$$

- **è¨˜å·â†”å¤‰æ•°å**: $\hat{f}_\text{KDE}$ = `kde(values)`ï¼ˆKernelDensity.jlï¼‰ã€$h$ = `1.06 * std(values) * length(values)^(-0.2)`ã€‚
- **shape**: `groups::Vector{Int}` ã¯å„ãƒ‡ãƒ¼ã‚¿ç‚¹ã®ã‚°ãƒ«ãƒ¼ãƒ—ãƒ©ãƒ™ãƒ«ï¼ˆ1, 2, 3ï¼‰ã€‚`values::Vector{Float64}` ã¯åŒã˜é•·ã•ã€‚
- **è½ã¨ã—ç©´**: `violin!(ax, groups, values)` ã®ç¬¬2å¼•æ•°ã¯ã‚°ãƒ«ãƒ¼ãƒ—ãƒ©ãƒ™ãƒ«ï¼ˆ`Int` or `String`ï¼‰ã€‚Makie 0.21ä»¥é™ã§ã¯ `side=:left`/`:right` ã§åŠå´ãƒã‚¤ã‚ªãƒªãƒ³ãŒä½¿ãˆã‚‹ã€‚

```rust
// å¯è¦–åŒ–: plotters / eframe ã‚¯ãƒ¬ãƒ¼ãƒˆãŒå¿…è¦
// (cargo add plotters ã¾ãŸã¯ cargo add eframe)
// ã“ã“ã§ã¯ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ­ã‚¸ãƒƒã‚¯ã®ã¿ç¤ºã™

use rand::SeedableRng;
use rand_distr::{Distribution, Normal as RandNormal};

fn main() {
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);

    // ç”Ÿæˆãƒ¢ãƒ‡ãƒ«3ç¨®ã®FIDã‚¹ã‚³ã‚¢ï¼ˆå„30ã‚µãƒ³ãƒ—ãƒ«ï¼‰
    let n = 30_usize;
    let groups = [
        ("Model A",   RandNormal::new(0.720_f64, 0.018).unwrap()),
        ("Model B",   RandNormal::new(0.778_f64, 0.015).unwrap()),
        ("Baseline",  RandNormal::new(0.680_f64, 0.022).unwrap()),
    ];

    // ãƒ‡ãƒ¼ã‚¿æº–å‚™: ã‚°ãƒ«ãƒ¼ãƒ—ãƒ©ãƒ™ãƒ«ã¨å€¤ã®ãƒšã‚¢
    let data: Vec<(usize, f64)> = groups.iter().enumerate()
        .flat_map(|(g, (_, dist))| (0..n).map(move |_| (g + 1, dist.sample(&mut rng))))
        .collect::<Vec<_>>();
    // æ³¨: data ã¯ãã®ã¾ã¾ collect() ã§ããªã„ãŸã‚ closure ã‚’ä½¿ã†
    let mut samples: Vec<(usize, f64)> = Vec::new();
    let mut rng2 = rand::rngs::StdRng::seed_from_u64(42);
    for (g, (_, dist)) in groups.iter().enumerate() {
        for _ in 0..n { samples.push((g + 1, dist.sample(&mut rng2))); }
    }

    // ç®±ã²ã’å›³ã®5æ•°è¦ç´„ï¼ˆãƒ—ãƒ­ãƒƒãƒˆç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼‰
    println!("{:>10}  {:>6}  {:>6}  {:>6}  {:>6}  {:>6}", "Group", "Min", "Q1", "Median", "Q3", "Max");
    for g in 1..=3_usize {
        let mut vals: Vec<f64> = samples.iter().filter(|(gi, _)| *gi == g).map(|(_, v)| *v).collect();
        vals.sort_by(|a, b| a.partial_cmp(b).unwrap());
        let q1 = vals[vals.len() / 4];
        let median = vals[vals.len() / 2];
        let q3 = vals[3 * vals.len() / 4];
        println!("{:>10}  {:.4}  {:.4}  {:.4}  {:.4}  {:.4}",
            groups[g-1].0, vals[0], q1, median, q3, vals[vals.len()-1]);
    }

    // Raincloud Plot: KDE ãƒãƒ³ãƒ‰å¹… (Silverman rule): h = 1.06Â·ÏƒÂ·n^(-1/5)
    for g in 1..=3_usize {
        let vals: Vec<f64> = samples.iter().filter(|(gi, _)| *gi == g).map(|(_, v)| *v).collect();
        let mean = vals.iter().sum::<f64>() / vals.len() as f64;
        let std = (vals.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / (vals.len()-1) as f64).sqrt();
        let bw = 1.06 * std * (vals.len() as f64).powf(-0.2);  // Silvermanãƒ«ãƒ¼ãƒ«
        println!("{}: KDE bandwidth h = {:.5}", groups[g-1].0, bw);
    }
    println!("Saved: stats_raincloud.png  (plotters ã‚¯ãƒ¬ãƒ¼ãƒˆã§æç”»)");
}
```

#### ä¿¡é ¼åŒºé–“è¡¨ç¤ºï¼ˆAlgebraOfGraphics.jlï¼‰

$$
\bar{x} \pm t_{1-\alpha/2, \, n-1} \cdot \frac{s}{\sqrt{n}}
$$

```rust
// å¯è¦–åŒ–: plotters / eframe ã‚¯ãƒ¬ãƒ¼ãƒˆãŒå¿…è¦
// (cargo add plotters ã¾ãŸã¯ cargo add eframe)
// AlgebraOfGraphics ã®ä¿¡é ¼åŒºé–“ãƒ—ãƒ­ãƒƒãƒˆã«ç›¸å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚’ç¤ºã™

use statrs::distribution::{StudentsT, ContinuousCDF};

fn t_quantile(df: f64, p: f64) -> f64 {
    let dist = StudentsT::new(0.0, 1.0, df).unwrap();
    let (mut lo, mut hi) = (0.0_f64, 10.0_f64);
    for _ in 0..100 { let mid=(lo+hi)/2.0; if dist.cdf(mid)<p {lo=mid;} else {hi=mid;} }
    (lo + hi) / 2.0
}

fn main() {
    // g_values / g_labels ã¯å‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã§ç”Ÿæˆæ¸ˆã¿ã¨ä»®å®š
    // ã“ã“ã§ã¯å›ºå®šå€¤ã§ãƒ‡ãƒ¢
    use rand::SeedableRng;
    use rand_distr::{Distribution, Normal as RandNormal};
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);
    let group_params = [("Model A", 0.720_f64, 0.018_f64), ("Model B", 0.778, 0.015), ("Baseline", 0.680, 0.022)];

    // å¹³å‡ Â± 95%CI ã‚’æ•´ç†
    println!("{:>10}  {:>8}  {:>8}  {:>8}", "Group", "Mean", "CI_lo", "CI_hi");
    for (name, mu, sigma) in &group_params {
        let vals: Vec<f64> = (0..30).map(|_| RandNormal::new(*mu, *sigma).unwrap().sample(&mut rng)).collect();
        let n = vals.len() as f64;
        let mean = vals.iter().sum::<f64>() / n;
        let s = (vals.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / (n - 1.0)).sqrt();
        // 95% CI: xÌ„ Â± t_{0.975, n-1} Â· s/âˆšn
        let t_crit = t_quantile(n - 1.0, 0.975);
        let lo = mean - t_crit * s / n.sqrt();
        let hi = mean + t_crit * s / n.sqrt();
        println!("{:>10}  {:.4}  {:.4}  {:.4}", name, mean, lo, hi);
    }
    // AlgebraOfGraphics ã§ãƒã‚¤ãƒ³ãƒˆ+ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ã‚’æç”»ã™ã‚‹ã«ã¯:
    // cargo add plotters
    println!("Saved: stats_ci_plot.png  (plotters ã‚¯ãƒ¬ãƒ¼ãƒˆã§æç”»)");
}
```

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Raincloud Plot ãŒãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆã‚ˆã‚Šã€Œèª å®Ÿã€ã¨ã•ã‚Œã‚‹ç†ç”±ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. Silvermanãƒ«ãƒ¼ãƒ«ã®ãƒãƒ³ãƒ‰å¹… $h$ ãŒã‚µãƒ³ãƒ—ãƒ«æ•° $n$ ã«å¯¾ã—ã¦ $n^{-1/5}$ ã§æ¸›å°‘ã™ã‚‹æ„å‘³ã‚’è¿°ã¹ã‚ˆã€‚

---

### 5.5 æ¼”ç¿’: çµ±è¨ˆçš„æœ‰æ„ vs å®Ÿç”¨çš„æœ‰æ„

#### åŠ¹æœé‡ã®æ•°å¼ã¨å®Ÿè£…

Cohen's $d$ï¼ˆ2ç¾¤ã®æ¨™æº–åŒ–å¹³å‡å·®ï¼‰:

$$
d = \frac{\bar{x}_A - \bar{x}_B}{s_p}, \quad s_p = \sqrt{\frac{(n_A-1)s_A^2 + (n_B-1)s_B^2}{n_A+n_B-2}}
$$

è§£é‡ˆåŸºæº–: $|d| < 0.2$ï¼ˆç„¡è¦–ã§ãã‚‹ï¼‰ã€$0.2 \le |d| < 0.5$ï¼ˆå°ï¼‰ã€$0.5 \le |d| < 0.8$ï¼ˆä¸­ï¼‰ã€$|d| \ge 0.8$ï¼ˆå¤§ï¼‰ã€‚

ç›¸é–¢ä¿‚æ•° $r$ ã‚’åŠ¹æœé‡ã¨ã—ã¦ä½¿ã†å ´åˆï¼ˆMann-Whitney U ã‹ã‚‰ã®å¤‰æ›ï¼‰:

$$
r = \frac{Z}{\sqrt{N}}
$$

$Z$: æ­£è¦è¿‘ä¼¼ã—ãŸ z ã‚¹ã‚³ã‚¢ã€$N$: ç·ã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚

- **è¨˜å·â†”å¤‰æ•°å**: $s_p$ = `s_pooled`ã€$d$ = `cohens_d`ã€$n_A$ = `length(a)`ã€$s_A^2$ = `var(a)`ã€‚
- **shape**: `a, b` ã¯ `Vector{Float64}`ã€‚ã‚¹ã‚«ãƒ©ãƒ¼ã‚’è¿”ã™ã€‚
- **è½ã¨ã—ç©´**: Cohen's $d$ ã¯ã€Œå¤§ãã„åŠ¹æœé‡ â‰  å®Ÿç”¨çš„ã«é‡è¦ã€ã€‚æœ€å°è‡¨åºŠçš„æ„ç¾©å·®ï¼ˆMCIDï¼‰ã¨ã®æ¯”è¼ƒãŒæœ¬è³ªã€‚

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, Normal as RandNormal};
use statrs::distribution::{StudentsT, ContinuousCDF};

/// Cohen's d: 2ç¾¤ã®æ¨™æº–åŒ–å¹³å‡å·®
/// d = (mean_a - mean_b) / s_pooled
fn cohens_d(a: &[f64], b: &[f64]) -> f64 {
    let na = a.len() as f64; let nb = b.len() as f64;
    let ma = a.iter().sum::<f64>() / na;
    let mb = b.iter().sum::<f64>() / nb;
    let va = a.iter().map(|x| (x - ma).powi(2)).sum::<f64>() / (na - 1.0);
    let vb = b.iter().map(|x| (x - mb).powi(2)).sum::<f64>() / (nb - 1.0);
    let s_pooled = (((na - 1.0) * va + (nb - 1.0) * vb) / (na + nb - 2.0)).sqrt();
    (ma - mb) / s_pooled
}

/// ç­‰åˆ†æ•£ t æ¤œå®šã® p å€¤
fn equal_var_t_test(a: &[f64], b: &[f64]) -> f64 {
    let na = a.len() as f64; let nb = b.len() as f64;
    let ma = a.iter().sum::<f64>() / na;
    let mb = b.iter().sum::<f64>() / nb;
    let va = a.iter().map(|x| (x - ma).powi(2)).sum::<f64>() / (na - 1.0);
    let vb = b.iter().map(|x| (x - mb).powi(2)).sum::<f64>() / (nb - 1.0);
    let sp2 = ((na - 1.0) * va + (nb - 1.0) * vb) / (na + nb - 2.0);
    let t = (ma - mb) / (sp2 * (1.0/na + 1.0/nb)).sqrt();
    let df = na + nb - 2.0;
    let dist = StudentsT::new(0.0, 1.0, df).unwrap();
    2.0 * (1.0 - dist.cdf(t.abs()))
}

fn main() {
    let mut rng = rand::rngs::StdRng::seed_from_u64(2025);

    // ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡: çµ±è¨ˆçš„æœ‰æ„ã§ã‚‚å®Ÿç”¨çš„ã«ç„¡æ„å‘³ãªã‚·ãƒŠãƒªã‚ª
    let a_large: Vec<f64> = (0..10000).map(|_| RandNormal::new(0.7200_f64, 0.01).unwrap().sample(&mut rng)).collect();
    let b_large: Vec<f64> = (0..10000).map(|_| RandNormal::new(0.7201_f64, 0.01).unwrap().sample(&mut rng)).collect();
    let p_large = equal_var_t_test(&a_large, &b_large);
    let d_large = cohens_d(&a_large, &b_large);
    println!("å¤§ã‚µãƒ³ãƒ—ãƒ«(N=10000): p={:.2e}  d={:.4}  æœ‰æ„={}  å®Ÿç”¨çš„={}",
        p_large, d_large,
        if p_large < 0.05 { "âœ…" } else { "âŒ" },
        if d_large.abs() >= 0.2 { "âœ…" } else { "âŒ ç„¡æ„å‘³" });

    // å®Ÿç”¨çš„ã«é‡è¦ãªã‚·ãƒŠãƒªã‚ªï¼ˆå°ã‚µãƒ³ãƒ—ãƒ«ã€å¤§åŠ¹æœé‡ï¼‰
    let a_small: Vec<f64> = (0..8).map(|_| RandNormal::new(0.720_f64, 0.02).unwrap().sample(&mut rng)).collect();
    let b_small: Vec<f64> = (0..8).map(|_| RandNormal::new(0.780_f64, 0.02).unwrap().sample(&mut rng)).collect();
    let p_small = equal_var_t_test(&a_small, &b_small);
    let d_small = cohens_d(&a_small, &b_small);
    println!("å°ã‚µãƒ³ãƒ—ãƒ«(N=8):    p={:.4}      d={:.4}  æœ‰æ„={}  å®Ÿç”¨çš„={}",
        p_small, d_small,
        if p_small < 0.05 { "âœ…" } else { "âŒ" },
        if d_small.abs() >= 0.8 { "âœ… å¤§" } else { "ä¸­ä»¥ä¸‹" });
}
```

#### p-hacking ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

p-hacking ã®å®Ÿæ…‹: ã€Œã©ã“ã‹ã§æœ‰æ„ã«ãªã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™ã€ã¨ç¬¬ä¸€ç¨®éèª¤ç‡ãŒæ€¥ä¸Šæ˜‡ã™ã‚‹ã€‚

$$
P(\text{å°‘ãªãã¨ã‚‚1å›æœ‰æ„}) = 1 - (1-\alpha)^m \approx m\alpha \quad (\text{å¸°ç„¡ä»®èª¬ãŒçœŸã®ã¨ã})
$$

$m$ å›ã®ç‹¬ç«‹æ¤œå®šã§ $\alpha = 0.05$ ãªã‚‰ã°ã€$m=14$ ã§å½é™½æ€§ç‡ãŒ50%ã‚’è¶…ãˆã‚‹ã€‚

- **è¨˜å·â†”å¤‰æ•°å**: $m$ = `n_tests`ã€$\alpha$ = `0.05`ã€`false_positive_rate` = å®Ÿé¨“çš„å½é™½æ€§ç‡ã€‚
- **shape**: ãƒ«ãƒ¼ãƒ—å¤‰æ•°ã€‚çµæœã¯ `Float64` ã®å‰²åˆã€‚

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, Normal as RandNormal};
use statrs::distribution::{StudentsT, ContinuousCDF};

fn equal_var_t_test_p(a: &[f64], b: &[f64]) -> f64 {
    let na = a.len() as f64; let nb = b.len() as f64;
    let ma = a.iter().sum::<f64>() / na;
    let mb = b.iter().sum::<f64>() / nb;
    let va = a.iter().map(|x| (x - ma).powi(2)).sum::<f64>() / (na - 1.0);
    let vb = b.iter().map(|x| (x - mb).powi(2)).sum::<f64>() / (nb - 1.0);
    let sp2 = ((na - 1.0) * va + (nb - 1.0) * vb) / (na + nb - 2.0);
    let t = (ma - mb) / (sp2 * (1.0/na + 1.0/nb)).sqrt();
    let dist = StudentsT::new(0.0, 1.0, na + nb - 2.0).unwrap();
    2.0 * (1.0 - dist.cdf(t.abs()))
}

// p-hacking ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: å¸°ç„¡ä»®èª¬ãŒçœŸã®ãƒ‡ãƒ¼ã‚¿ã§ç¹°ã‚Šè¿”ã™
// n_tests_per_exp å›æ¤œå®šã‚’è¡Œã„ã€1å›ã§ã‚‚ p<Î± ãªã‚‰ã€Œæœ‰æ„ã¨å ±å‘Šã€
fn phacking_sim(n_experiments: usize, n_tests_per_exp: usize, alpha: f64, rng: &mut impl rand::Rng) -> f64 {
    let standard_normal = RandNormal::new(0.0_f64, 1.0).unwrap();
    let mut false_positive = 0_usize;
    for _ in 0..n_experiments {
        let mut found_sig = false;
        for _ in 0..n_tests_per_exp {
            let a: Vec<f64> = (0..20).map(|_| standard_normal.sample(rng)).collect();
            let b: Vec<f64> = (0..20).map(|_| standard_normal.sample(rng)).collect();
            // å¸°ç„¡ä»®èª¬ãŒçœŸ (Î¼_a = Î¼_b = 0)
            if equal_var_t_test_p(&a, &b) < alpha {
                found_sig = true;
                break;
            }
        }
        if found_sig { false_positive += 1; }
    }
    false_positive as f64 / n_experiments as f64
}

fn main() {
    let mut rng = rand::rngs::StdRng::seed_from_u64(42);
    println!("ç†è«–å€¤ (1-(1-0.05)^m):");
    for &m in &[1_usize, 5, 10, 14, 20] {
        let theory   = 1.0 - (1.0 - 0.05_f64).powi(m as i32);
        let empirical = phacking_sim(10_000, m, 0.05, &mut rng);
        println!("  m={:2}: ç†è«–={:.3}  å®Ÿé¨“={:.3}", m, theory, empirical);
    }
}
```

#### ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¸ã®å¿œç”¨

på€¤ã ã‘ã§ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã®å±é™ºæ€§:

1. **FID ã®çµ¶å¯¾å€¤** ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»å®Ÿè£…ã«ã‚ˆã£ã¦å¤‰ã‚ã‚‹ã€‚ç¾¤é–“æ¯”è¼ƒãŒæœ¬è³ªã€‚
2. **åŠ¹æœé‡ Cohen's $d$** ã§ã€Œæ”¹å–„å¹…ãŒå®Ÿç”¨çš„ã‹ã€ã‚’æ¸¬ã‚‹ã€‚
3. **å¤šé‡æ¯”è¼ƒè£œæ­£**ï¼ˆBHæ³•ï¼‰ã§èª¤ç™ºè¦‹ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
4. **ãƒ™ã‚¤ã‚ºçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**ã§ã€Œæ”¹å–„ã®äº‹å¾Œç¢ºç‡ã€ã‚’è¨ˆç®—ã™ã‚‹æ–¹ãŒè§£é‡ˆã—ã‚„ã™ã„ã€‚

```rust
use rand::SeedableRng;
use rand_distr::{Distribution, Normal as RandNormal};
use statrs::distribution::{StudentsT, ContinuousCDF};

fn equal_var_t_test_p(a: &[f64], b: &[f64]) -> f64 {
    let na = a.len() as f64; let nb = b.len() as f64;
    let ma = a.iter().sum::<f64>() / na;
    let mb = b.iter().sum::<f64>() / nb;
    let va = a.iter().map(|x| (x - ma).powi(2)).sum::<f64>() / (na - 1.0);
    let vb = b.iter().map(|x| (x - mb).powi(2)).sum::<f64>() / (nb - 1.0);
    let sp2 = ((na - 1.0) * va + (nb - 1.0) * vb) / (na + nb - 2.0);
    let t = (ma - mb) / (sp2 * (1.0/na + 1.0/nb)).sqrt();
    let dist = StudentsT::new(0.0, 1.0, na + nb - 2.0).unwrap();
    2.0 * (1.0 - dist.cdf(t.abs()))
}

fn cohens_d(a: &[f64], b: &[f64]) -> f64 {
    let na = a.len() as f64; let nb = b.len() as f64;
    let ma = a.iter().sum::<f64>() / na;
    let mb = b.iter().sum::<f64>() / nb;
    let va = a.iter().map(|x| (x - ma).powi(2)).sum::<f64>() / (na - 1.0);
    let vb = b.iter().map(|x| (x - mb).powi(2)).sum::<f64>() / (nb - 1.0);
    let sp = (((na - 1.0) * va + (nb - 1.0) * vb) / (na + nb - 2.0)).sqrt();
    (ma - mb) / sp
}

/// Benjamini-Hochberg FDR è£œæ­£
fn bh_correction(pvals: &[f64]) -> Vec<f64> {
    let m = pvals.len();
    let mut idx: Vec<usize> = (0..m).collect();
    idx.sort_by(|&a, &b| pvals[a].partial_cmp(&pvals[b]).unwrap());
    let mut adj = vec![0.0_f64; m];
    let mut running_min = 1.0_f64;
    for (rank, &i) in idx.iter().enumerate().rev() {
        let p_adj = (pvals[i] * m as f64 / (rank + 1) as f64).min(1.0);
        running_min = running_min.min(p_adj);
        adj[i] = running_min;
    }
    adj
}

fn main() {
    let mut rng = rand::rngs::StdRng::seed_from_u64(2025);

    // ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡: 5æŒ‡æ¨™Ã—2ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
    let metrics = ["FIDâ†“", "ISâ†‘", "Precisionâ†‘", "Recallâ†‘", "F1â†‘"];
    let mu_a = [0.720_f64, 0.850, 0.780, 0.760, 0.770];
    let mu_b = [0.750_f64, 0.870, 0.790, 0.770, 0.780];

    let mut raw_pvals: Vec<f64> = Vec::new();
    let mut ds: Vec<f64> = Vec::new();

    for (&ma, &mb) in mu_a.iter().zip(mu_b.iter()) {
        let a: Vec<f64> = (0..10).map(|_| RandNormal::new(ma, 0.02).unwrap().sample(&mut rng)).collect();
        let b: Vec<f64> = (0..10).map(|_| RandNormal::new(mb, 0.02).unwrap().sample(&mut rng)).collect();
        raw_pvals.push(equal_var_t_test_p(&a, &b));
        ds.push(cohens_d(&a, &b).abs());
    }

    let adj_pvals = bh_correction(&raw_pvals);

    println!("{:<12}  {:>7}  {:>7}  {:>7}  åˆ¤å®š", "ãƒ¡ãƒˆãƒªã‚¯ã‚¹", "raw_p", "BH_p", "Cohen_d");
    for (i, &m) in metrics.iter().enumerate() {
        let (rp, ap, d) = (raw_pvals[i], adj_pvals[i], ds[i]);
        let verdict = if ap < 0.05 && d >= 0.5 { "âœ… æœ‰æ„ã‹ã¤å®Ÿç”¨çš„" }
                      else if ap < 0.05        { "âš ï¸ æœ‰æ„ã ãŒåŠ¹æœå°" }
                      else if d >= 0.5          { "âš ï¸ éæœ‰æ„ã ãŒåŠ¹æœä¸­å¤§" }
                      else                      { "âŒ å·®ãªã—" };
        println!("{:<12}  {:.4}  {:.4}  {:.3}  {}", m, rp, ap, d, verdict);
    }
}
```

**çµè«–**: çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆp < 0.05ï¼‰ã¨å®Ÿç”¨çš„æœ‰æ„æ€§ï¼ˆåŠ¹æœé‡ $d \ge 0.5$ï¼‰ã¯åˆ¥ç‰©ã ã€‚å¤§ã‚µãƒ³ãƒ—ãƒ«ã§ã¯äº›ç´°ãªå·®ã‚‚ã€Œæœ‰æ„ã€ã«ãªã‚‹ä¸€æ–¹ã€å°ã‚µãƒ³ãƒ—ãƒ«ã§ã¯é‡è¦ãªå·®ãŒã€Œéæœ‰æ„ã€ã®ã¾ã¾åŸ‹ã‚‚ã‚Œã‚‹ã€‚ç”Ÿæˆãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§ã¯åŠ¹æœé‡ãƒ»ä¿¡é ¼åŒºé–“ãƒ»å¤šé‡æ¯”è¼ƒè£œæ­£ã®ä¸‰ç‚¹ã‚»ãƒƒãƒˆã‚’æƒãˆã¦ã¯ã˜ã‚ã¦ã€ä¸»å¼µãŒç§‘å­¦çš„æ ¹æ‹ ã‚’æŒã¤ã€‚

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. `phacking_sim(10_000, 20)` ã®çµæœãŒ `1-(1-0.05)^20 â‰ˆ 0.64` ã«è¿‘ã„ç†ç”±ã‚’æ•°å¼ã§èª¬æ˜ã›ã‚ˆã€‚
> 2. FIDãŒã€Œæœ‰æ„ã‹ã¤åŠ¹æœé‡å¤§ã€ã§ã‚‚ã€ã€Œå®Ÿç”¨çš„ã«æ„å‘³ãŒã‚ã‚‹æ”¹å–„ã€ã¨æ–­è¨€ã§ããªã„çŠ¶æ³ã‚’1ã¤æŒ™ã’ã‚ˆã€‚

---


## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

ï¼ˆçµ±è¨ˆå­¦ã®æœ€æ–°ç ”ç©¶å‹•å‘ã¯ Â§ ä»˜éŒ²A-D ã‚’å‚ç…§ï¼‰

## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

ï¼ˆæœ¬è¬›ç¾©ã®ã¾ã¨ã‚ã¯ Â§ ä»˜éŒ²B-D ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’å‚ç…§ï¼‰

## è‘—è€…ãƒªãƒ³ã‚¯
- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
