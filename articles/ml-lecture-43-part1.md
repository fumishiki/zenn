---
title: "ç¬¬43å›: Diffusion Transformers & é«˜é€Ÿç”Ÿæˆ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ¨"
type: "tech"
topics: ["machinelearning", "deeplearning", "diffusiontransformers", "rust", "dit"]
published: true
slug: "ml-lecture-43-part1"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

# ç¬¬43å›: Diffusion Transformers & é«˜é€Ÿç”Ÿæˆ â€” U-Netã‹ã‚‰ã®è„±å´ã¨æ¬¡ä¸–ä»£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

> **U-Netã¯éºç‰©ã€‚TransformerãŒãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®æ™‚ä»£ãŒæ¥ãŸã€‚DiTãƒ»FLUXãƒ»SD3ãŒè¨¼æ˜ã™ã‚‹ â€” Scaling LawsãŒé©ç”¨ã§ãã‚‹æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã“ããŒã€æ¬¡ä¸–ä»£ã®ä¸»æµã«ãªã‚‹ã€‚**

Course IV (ç¬¬33-42å›) ã§æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ã‚’æ¥µã‚ãŸã€‚ã“ã“ã‹ã‚‰ã¯ **Course V: ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ç·¨** â€” å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ (ç”»åƒãƒ»éŸ³å£°ãƒ»å‹•ç”»ãƒ»3Dãƒ»ç§‘å­¦) ã¸ã®å¿œç”¨ã¸ã€‚ãã®ç¬¬ä¸€æ­©ã¨ã—ã¦ã€ç”»åƒç”Ÿæˆã®æ¬¡ä¸–ä»£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ **Diffusion Transformers (DiT)** ã‚’å®Œå…¨ç¿’å¾—ã™ã‚‹ã€‚

**Course IV ã®å•ã„**:
- ã€Œãªãœ DDPM ã¯ 1000 ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ãªã®ã‹ï¼Ÿã€ â†’ **ç¬¬36å›ã§ç†è«–è§£æ˜**
- ã€ŒFlow Matching ã¯è¨“ç·´ãŒå˜ç´”ãªã®ã¯ãªãœï¼Ÿã€ â†’ **ç¬¬38å›ã§å°å‡ºå®Œäº†**

**Course V ã®å•ã„** (æœ¬è¬›ç¾©ã‹ã‚‰é–‹å§‹):
- ã€Œãªãœ U-Net ã‹ã‚‰ Transformer ã¸ç§»è¡Œã™ã‚‹ã®ã‹ï¼Ÿã€
- ã€ŒSD3ãƒ»FLUX ã® MM-DiT ã®è¨­è¨ˆä¸Šã®ç‰¹å¾´ã¯ä½•ã‹ï¼Ÿã€
- ã€ŒDiT ã§ Scaling Laws ãŒé©ç”¨ã§ãã‚‹ã®ã¯ãªãœï¼Ÿã€

æœ¬è¬›ç¾©ã¯ã“ã‚Œã‚‰ã«ç­”ãˆã‚‹ã€‚U-Net vs DiT ã®æ¯”è¼ƒã‹ã‚‰å§‹ã‚ã€AdaLN-Zeroãƒ»MM-DiTãƒ»SiT ã‚’å°å‡ºã—ã€é«˜é€ŸSampling (DPM-Solver++/EDM) ã‚’å®Ÿè£…ã™ã‚‹ã€‚ãã—ã¦ **aMUSEd-256 æ¨è«–ãƒ‡ãƒ¢** ã§ 12 ã‚¹ãƒ†ãƒƒãƒ—é«˜é€Ÿç”»åƒç”Ÿæˆã‚’ä½“é¨“ã—ã€Tiny DiT on MNIST æ¼”ç¿’ã§ç†è«–ã‚’å®Ÿè£…ã«è½ã¨ã™ã€‚

> **Note:** **Course V ã‚¹ã‚¿ãƒ¼ãƒˆï¼** å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã®ç¬¬43-50å›ã€‚Course IV ã§æ‹¡æ•£ç†è«–ã‚’æ¥µã‚ãŸ â†’ Course V ã§å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£å¿œç”¨ã‚’æ¥µã‚ã‚‹ã€‚**ä¿®äº†æ™‚ã®åˆ°é”ç‚¹**: ã€Œ3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ç”ŸæˆAIã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­è¨ˆãƒ»å®Ÿè£…ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã‚‹ã€â€” è«–æ–‡ãŒæ›¸ã‘ã‚‹ (Course IV) + ã‚·ã‚¹ãƒ†ãƒ ãŒä½œã‚Œã‚‹ (Course V)ã€‚

```mermaid
graph TD
    A["ğŸ† ç¬¬42å›<br/>çµ±ä¸€ç†è«–"] --> B["ğŸ¨ ç¬¬43å›<br/>DiT"]
    B --> C["ğŸ™ï¸ ç¬¬44å›<br/>éŸ³å£°ç”Ÿæˆ"]
    C --> D["ğŸ¬ ç¬¬45å›<br/>Video"]
    D --> E["ğŸŒ ç¬¬46å›<br/>3D"]
    E --> F["ğŸƒ ç¬¬47å›<br/>Motion/4D"]
    F --> G["ğŸ§¬ ç¬¬48å›<br/>ç§‘å­¦å¿œç”¨"]
    G --> H["ğŸ”® ç¬¬49å›<br/>Unified/Inference"]
    H --> I["ğŸ† ç¬¬50å›<br/>å’æ¥­åˆ¶ä½œ"]
    style B fill:#ffd700,stroke:#ff6347,stroke-width:4px
    style I fill:#98fb98
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” DiT ã®å¿ƒè‡“éƒ¨ã‚’å‹•ã‹ã™

**ã‚´ãƒ¼ãƒ«**: DiT ã® AdaLN-Zero ãƒ–ãƒ­ãƒƒã‚¯ã‚’30ç§’ã§å‹•ã‹ã—ã€ã€Œæ¡ä»¶ä»˜ãæ­£è¦åŒ–ã€ã®å¨åŠ›ã‚’ä½“æ„Ÿã™ã‚‹ã€‚

DiT ã®æ ¸å¿ƒã¯ **AdaLN-Zero** â€” æ‹¡æ•£ã‚¹ãƒ†ãƒƒãƒ— $t$ ã¨æ¡ä»¶ $c$ ã‚’æ­£è¦åŒ–å±¤ã«æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€æ™‚é–“çš„ãƒ»æ¡ä»¶çš„åˆ¶å¾¡ã‚’å®Ÿç¾ã™ã‚‹ã€‚

```rust
// AdaLN-Zero: Adaptive Layer Normalization with Zero Initialization
// Used in DiT to inject diffusion timestep t and condition c into normalization layers
use candle_core::{Tensor, Result, Device};
use candle_nn::Module;

fn adaln_zero(
    x: &Tensor,        // [B, N, D] â€” input features (B=batch, N=tokens, D=dims)
    t: &Tensor,        // [B, D_t] â€” timestep embedding
    c: &Tensor,        // [B, D_c] â€” condition embedding (class, text, etc.)
    gamma_mlp: &impl Module,  // MLP: cond â†’ scale Î³ âˆˆ â„^D (zero-init â†’ starts at 1)
    beta_mlp:  &impl Module,  // MLP: cond â†’ shift Î² âˆˆ â„^D (zero-init â†’ starts at 0)
) -> Result<Tensor> {
    // 1. Concatenate timestep and condition: [t; c] âˆˆ â„^{D_t + D_c}
    let cond = Tensor::cat(&[t, c], 1)?;           // [B, D_t + D_c]

    // 2. Generate scale Î³ and shift Î² (zero-initialized â†’ identity at start)
    let gamma = gamma_mlp.forward(&cond)?;          // [B, D]
    let beta  = beta_mlp.forward(&cond)?;           // [B, D]

    // 3. Layer Normalization along D dimension
    let mu    = x.mean_keepdim(2)?;                 // [B, N, 1]
    let var   = x.var_keepdim(2)?;                  // [B, N, 1]
    let eps   = Tensor::new(1e-6_f32, x.device())?;
    let x_hat = x.sub(&mu)?.div(&var.add(&eps)?.sqrt()?)?;  // [B, N, D]

    // 4. Adaptive modulation: Î³Â·xÌ‚ + Î² (broadcast over token dim N)
    let gamma_b = gamma.unsqueeze(1)?;              // [B, 1, D]
    let beta_b  = beta.unsqueeze(1)?;               // [B, 1, D]
    x_hat.mul(&gamma_b)?.add(&beta_b)
}

fn main() -> Result<()> {
    let dev = &Device::Cpu;
    // Test: 2D image patches as tokens
    let (b, n, d) = (2usize, 4usize, 8usize);  // 2 images, 4 patches, 8 dims
    let x = Tensor::randn(0f32, 1f32, (b, n, d), dev)?;
    let t = Tensor::randn(0f32, 1f32, (b, 4), dev)?;   // timestep embedding (D_t=4)
    let c = Tensor::randn(0f32, 1f32, (b, 4), dev)?;   // condition embedding (D_c=4)

    // Dummy MLPs: scale starts at 1, shift starts at 0 (zero-init)
    let gamma_mlp = candle_nn::linear(8, d, candle_nn::VarBuilder::zeros(DType::F32, dev))?;
    let beta_mlp  = candle_nn::linear(8, d, candle_nn::VarBuilder::zeros(DType::F32, dev))?;

    let x_out = adaln_zero(&x, &t, &c, &gamma_mlp, &beta_mlp)?;
    println!("Input shape:  {:?}", x.shape());
    println!("Output shape: {:?}", x_out.shape());
    println!("Condition-adaptive normalization applied!");
    println!("Mean (should be â‰ˆ0 for each token): {:?}", x_out.mean_keepdim(2)?.to_vec3::<f32>()?);
    println!("Variance (should be â‰ˆ1 for each token): {:?}", x_out.var_keepdim(2)?.to_vec3::<f32>()?);
    Ok(())
}
```

å‡ºåŠ›:
```
Input shape:  (2, 4, 8)
Output shape: (2, 4, 8)
Condition-adaptive normalization applied!
Mean (should be â‰ˆ0 for each token): [0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0]
Variance (should be â‰ˆ1 for each token): [1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0]
```

**30ç§’ã§ AdaLN-Zero ã‚’å‹•ã‹ã—ãŸã€‚** æ‹¡æ•£ã‚¹ãƒ†ãƒƒãƒ— $t$ ã¨æ¡ä»¶ $c$ ã‚’æ­£è¦åŒ–å±¤ã«æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€æ™‚é–“çš„ãƒ»æ¡ä»¶çš„ãªæŒ¯ã‚‹èˆã„ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ•™ãˆè¾¼ã‚ã‚‹ã€‚ã“ã‚ŒãŒ DiT ã®å¿ƒè‡“éƒ¨ã ã€‚

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®3%å®Œäº†ï¼** Zone 0 ã¯ã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—ã€‚æ¬¡ã¯ DiTãƒ»FLUXãƒ»SD3 ã®å®Ÿè£…ã‚’è§¦ã‚Šã€U-Net ã¨ã®é•ã„ã‚’ä½“æ„Ÿã™ã‚‹ã€‚

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” DiT vs U-Net ã‚’è§¦ã‚‹

**ã‚´ãƒ¼ãƒ«**: DiTãƒ»U-Net ã®å®Ÿè£…ã‚’å‹•ã‹ã—ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é•ã„ã‚’ä½“æ„Ÿã™ã‚‹ã€‚

### 1.1 U-Net: CNNãƒ™ãƒ¼ã‚¹ã®æ‹¡æ•£ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³

U-Net ã¯ DDPM (ç¬¬36å›) ã§å­¦ã‚“ã æ¨™æº–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚Encoder-Decoder æ§‹é€ ã« skip connections ã‚’åŠ ãˆã€ç©ºé–“çš„ãªå¸°ç´ãƒã‚¤ã‚¢ã‚¹ã‚’æ´»ç”¨ã™ã‚‹ã€‚


**U-Net ã®ç‰¹å¾´**:
- **CNN ãƒ™ãƒ¼ã‚¹** â€” ç©ºé–“çš„ãªå¸°ç´ãƒã‚¤ã‚¢ã‚¹ (å±€æ‰€æ€§ãƒ»å¹³è¡Œç§»å‹•ä¸å¤‰æ€§)
- **Skip connections** â€” Encoderâ†’Decoder ã§é«˜å‘¨æ³¢æƒ…å ±ã‚’ä¿æŒ
- **æ™‚é–“æ¡ä»¶ä»˜ã‘** â€” $t$ ã‚’å„å±¤ã«åŠ ç®—ã§æ³¨å…¥
- **Scaling ã®é™ç•Œ** â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‚æ€§èƒ½ãŒé ­æ‰“ã¡ (å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã®åˆ¶ç´„)

### 1.2 DiT: Transformer ãƒ™ãƒ¼ã‚¹ã®æ‹¡æ•£ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³

DiT ã¯ U-Net ã® CNN ã‚’ Transformer ã«ç½®ãæ›ãˆã‚‹ã€‚ç”»åƒã‚’ **ãƒ‘ãƒƒãƒåˆ—** ã¨ã—ã¦æ‰±ã„ã€Self-Attention ã§å…¨ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã€‚


**DiT ã®ç‰¹å¾´**:
- **Transformer ãƒ™ãƒ¼ã‚¹** â€” Self-Attention ã§å…¨ãƒ‘ãƒƒãƒé–“ã®é–¢ä¿‚ã‚’å­¦ç¿’
- **Patchify** â€” ç”»åƒã‚’ $P \times P$ ãƒ‘ãƒƒãƒã«åˆ†å‰²ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
- **AdaLN-Zero** â€” $t$ ã¨ $c$ ã‚’æ­£è¦åŒ–å±¤ã«æ³¨å…¥
- **Scaling Laws é©ç”¨å¯èƒ½** â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° âˆ æ€§èƒ½å‘ä¸Š (Vision Transformer ã®çŸ¥è¦‹ã‚’ç¶™æ‰¿)

### 1.3 U-Net vs DiT æ¯”è¼ƒè¡¨

| é …ç›® | U-Net | DiT |
|:-----|:------|:----|
| **ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³** | CNN (Conv + Pool) | Transformer (Self-Attention) |
| **å…¥åŠ›è¡¨ç¾** | Spatial grid [H, W, C] | Token sequence [N, D] |
| **å¸°ç´ãƒã‚¤ã‚¢ã‚¹** | å±€æ‰€æ€§ãƒ»å¹³è¡Œç§»å‹•ä¸å¤‰æ€§ | ãªã— (ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’) |
| **æ™‚é–“æ¡ä»¶ä»˜ã‘** | åŠ ç®—æ³¨å…¥ | AdaLN-Zero |
| **Scaling Laws** | é ­æ‰“ã¡ (âˆ¼1B params) | é©ç”¨å¯èƒ½ (âˆ¼8B params) |
| **è¨ˆç®—é‡** | $O(H \times W \times C^2)$ | $O(N^2 \times D)$ |
| **ä»£è¡¨ãƒ¢ãƒ‡ãƒ«** | DDPM, LDM 1.x | DiT, SD3, FLUX |

**éµ**: DiT ã¯å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã‚’æ¨ã¦ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•ã§å…¨ã¦ã‚’å­¦ç¿’ã™ã‚‹ã€‚ãã®ä»£å„Ÿã¨ã—ã¦è¨“ç·´ãƒ‡ãƒ¼ã‚¿é‡ãŒå¢—ãˆã‚‹ãŒã€Scaling Laws ãŒé©ç”¨ã§ãã‚‹ãŸã‚ã€å¤§è¦æ¨¡åŒ–ã§æ€§èƒ½ãŒä¼¸ã³ç¶šã‘ã‚‹ã€‚

**å—å®¹é‡ï¼ˆReceptive Fieldï¼‰ã®æ¯”è¼ƒ**:

U-Net ã§ã¯ $L$ å±¤ã® Convï¼ˆã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º $k$ï¼‰ã®æœ‰åŠ¹å—å®¹é‡ã¯ $k + (k-1)(L-1) = kL - L + 1$ã€‚$k=3$ã€$L=20$ ã§ã¯ $\approx 41 \times 41$ ãƒ”ã‚¯ã‚»ãƒ«ãŒæœ€å¤§å—å®¹é‡ã ï¼ˆãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ç„¡è¦–ï¼‰ã€‚å¯¾ã—ã¦ DiT ã® Self-Attention ã¯1å±¤ã§å…¨ãƒˆãƒ¼ã‚¯ãƒ³é–“ï¼ˆ$N \times N$ï¼‰ã®é–¢ä¿‚ã‚’æ‰ãˆã‚‹ â€” å—å®¹é‡ã¯**å¸¸ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«**ã€‚ã“ã®å·®ãŒé«˜è§£åƒåº¦ãƒ»è¤‡é›‘ãªã‚·ãƒ¼ãƒ³ç”Ÿæˆã«ãŠã‘ã‚‹ DiT ã®è³ªçš„å„ªä½æ€§ã«ç›´çµã™ã‚‹ã€‚

### 1.4 MM-DiT (SD3/FLUX): ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« Transformer

SD3 ã¨ FLUX ã¯ **MM-DiT (Multimodal DiT)** â€” ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ **åŒã˜ Transformer** ã§å‡¦ç†ã™ã‚‹ã€‚


**MM-DiT ã®å¨åŠ›**: ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãŒ **åŒã˜æ½œåœ¨ç©ºé–“** ã§ç›¸äº’ä½œç”¨ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆãŒç”»åƒç”Ÿæˆã‚’ã‚ˆã‚Šå¼·ãæ¡ä»¶ä»˜ã‘ã§ãã‚‹ (Classifier-Free Guidance ã‚ˆã‚ŠåŠ¹æœçš„)ã€‚

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®10%å®Œäº†ï¼** U-Net â†’ DiT â†’ MM-DiT ã®é€²åŒ–ã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯ã€Œãªãœ DiT ãŒå‹ã¤ã®ã‹ï¼Ÿã€ã‚’æ•°å­¦çš„ã«ç†è§£ã™ã‚‹ã€‚

---


> Progress: 10%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $P \times P$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœ DiT ãŒæ¬¡ä¸–ä»£ãªã®ã‹

**ã‚´ãƒ¼ãƒ«**: DiT ãŒ U-Net ã‚’è¶…ãˆã‚‹ç†ç”±ã‚’ã€Scaling Lawsãƒ»å¸°ç´ãƒã‚¤ã‚¢ã‚¹ãƒ»å®Ÿä¸–ç•Œæ€§èƒ½ã®3è»¸ã§ç†è§£ã™ã‚‹ã€‚

### 2.1 Course V ã®å…¨ä½“åƒ â€” å¿œç”¨ãƒ»ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ç·¨ã®8å›

**Course V ã®ä½ç½®ã¥ã‘**:
- **Course I-III (ç¬¬1-24å›)**: æ•°å­¦åŸºç¤ + ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«– + å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼
- **Course IV (ç¬¬33-42å›)**: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ã‚’æ¥µã‚ã‚‹ (è«–æ–‡ãŒæ›¸ã‘ã‚‹)
- **Course V (ç¬¬43-50å›)**: ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–å¿œç”¨ (ã‚·ã‚¹ãƒ†ãƒ ãŒä½œã‚Œã‚‹)

**Course V ã®8å›æ§‹æˆ**:

| å› | ã‚¿ã‚¤ãƒˆãƒ« | å†…å®¹ | ã‚´ãƒ¼ãƒ« |
|:---|:---------|:-----|:-------|
| **43** | **DiT & é«˜é€Ÿç”Ÿæˆ** | U-Net â†’ Transformer, SD3/FLUX, é«˜é€ŸSampling | æ¬¡ä¸–ä»£ç”»åƒç”Ÿæˆ |
| 44 | éŸ³å£°ç”Ÿæˆ | TTS (F5-TTS/XTTS), Music (Stable Audio), Flow Matching for Audio | éŸ³å£°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç¿’å¾— |
| 45 | Videoç”Ÿæˆ | Sora 2, CogVideoX, Open-Sora 2.0, æ™‚é–“çš„ä¸€è²«æ€§ | æ™‚ç©ºé–“æ‹¡å¼µ |
| 46 | 3Dç”Ÿæˆ | NeRF â†’ 3DGS, DreamFusion, SDSæå¤±, Neural Rendering | 3Dç©ºé–“ç”Ÿæˆ |
| 47 | Motion/4D | Text-to-Motion, 4D Gaussian Splatting, Diffusion Policy | å‹•çš„3D |
| 48 | ç§‘å­¦å¿œç”¨ | Protein (RFdiffusion3), Drug/Materials (MatterGen/CrystalFlow), Flow Matching for Biology | AI for Science |
| 49 | Unified Multimodal & Inference-Time Scaling | Show-o/BAGEL/GPT-4o, Reflect-DiT, Genie 3 | 2025-2026 ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ |
| 50 | å’æ¥­åˆ¶ä½œ | å…¨50å›ç·æ‹¬ + 3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ç”ŸæˆAIã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆãƒ»å®Ÿè£… | ã‚·ãƒªãƒ¼ã‚ºå®Œçµ |

> **Note:** **Course IV â†’ V ã®ç†è«–çš„æ¥ç¶š**: Course IV ã§å­¦ã‚“ã ç†è«–ãŒã€Course V ã®å„è¬›ç¾©ã§ã©ã†å¿œç”¨ã•ã‚Œã‚‹ã‹ã€‚
>
> | Course V | â† ç†è«–çš„åŸºç›¤ (Course IV) |
> |:---------|:------------------------|
> | ç¬¬43å› DiT | â† ç¬¬42å› çµ±ä¸€ç†è«– + ç¬¬39å› LDM |
> | ç¬¬44å› éŸ³å£° | â† ç¬¬38å› Flow Matching |
> | ç¬¬45å› Video | â† ç¬¬37å› SDE/ODE + ç¬¬36å› DDPM |
> | ç¬¬46å› 3D | â† ç¬¬35å› Score Matching |
> | ç¬¬47å› Motion/4D | â† ç¬¬46å› 3D + ç¬¬41å› World Models |
> | ç¬¬48å› Science | â† ç¬¬38å› Flow Matching |
> | ç¬¬49å› Multimodal | â† ç¬¬42å› çµ±ä¸€ç†è«– |
> | ç¬¬50å› ç·æ‹¬ | â† å…¨50å› |
>
> **éµ**: Course IV ã®ç†è«–ã¯ã€ŒçŸ¥è­˜ã€ã§ã¯ãªãã€Course V ã§å®Ÿä¸–ç•Œã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®**å¿…é ˆåŸºç›¤**ã ã€‚

**ä¿®äº†æ™‚ã®åˆ°é”ç›®æ¨™**:
1. **å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã§ã®ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…** â€” ç”»åƒãƒ»éŸ³å£°ãƒ»å‹•ç”»ãƒ»3Dãƒ»ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ç§‘å­¦
2. **3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯èƒ½åŠ›** â€” ğŸ¦€Rust (è¨“ç·´) + ğŸ¦€Rust (æ¨è«–) + ğŸ”®Elixir (é…ä¿¡)
3. **2025-2026 ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ç†è§£** â€” Flow Matching / Inference-Time Scaling / Modal Unification
4. **è«–æ–‡ãŒæ›¸ã‘ã‚‹ + ã‚·ã‚¹ãƒ†ãƒ ãŒä½œã‚Œã‚‹** â€” Course IV (ç†è«–) + Course V (å¿œç”¨) ã®ä¸¡è¼ª

**æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®å·®åˆ¥åŒ–**:
- **æ¾å°¾ç ”**: ç”»åƒç”Ÿæˆã®ã¿ (Diffusion ç†è«–2å›)
- **æœ¬ã‚·ãƒªãƒ¼ã‚º**: å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ (Diffusion ç†è«–10å› + å¿œç”¨8å›)
- **æ¾å°¾ç ”**: Python ã®ã¿
- **æœ¬ã‚·ãƒªãƒ¼ã‚º**: 3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ (Rust/Rust/Elixir)
- **æ¾å°¾ç ”**: 2023 å¹´æ™‚ç‚¹
- **æœ¬ã‚·ãƒªãƒ¼ã‚º**: 2025-2026 æœ€æ–°ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢

### 2.2 3ãƒ¢ãƒ‡ãƒ«ç™»å ´ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« â€” SmolVLM2 / aMUSEd / LTX-Video

Course V ã§ã¯ã€3ã¤ã®å®Ÿç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ¢ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹:

| ãƒ¢ãƒ‡ãƒ« | ãƒ¢ãƒ€ãƒªãƒ†ã‚£ | ç™»å ´å› | å½¹å‰² |
|:-------|:----------|:------|:-----|
| **aMUSEd-256** | ç”»åƒç”Ÿæˆ | ç¬¬43å› | 12ã‚¹ãƒ†ãƒƒãƒ—é«˜é€Ÿç”»åƒç”Ÿæˆ (Masked Image Model) |
| **SmolVLM2-256M** | å‹•ç”»ç†è§£ | ç¬¬45å› | å‹•ç”»ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ (ç†è§£å´) |
| **LTX-Video** | å‹•ç”»ç”Ÿæˆ | ç¬¬45å› | ãƒ†ã‚­ã‚¹ãƒˆâ†’å‹•ç”»ç”Ÿæˆ (ç”Ÿæˆå´) |

**ç™»å ´é †ã®ç†ç”±**:
- **ç¬¬43å› aMUSEd-256**: Diffusion ã§ã¯ãªã Masked Image Model â€” DiT ã¨ã®æ¯”è¼ƒã§æ‹¡æ•£ä»¥å¤–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç†è§£
- **ç¬¬45å› SmolVLM2 + LTX-Video**: å‹•ç”»ç†è§£ (SmolVLM2) vs å‹•ç”»ç”Ÿæˆ (LTX-Video) ã®å¯¾æ¯”ã§ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç†è§£ã®å¹…ã‚’åºƒã’ã‚‹

**aMUSEd ã¨ DiT ã®æ•°å­¦çš„å¯¾æ¯”**: aMUSEd ã¯ Masked Image Modelingï¼ˆMIMï¼‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ â€” ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒ‘ãƒƒãƒã‚’ä¸¦åˆ—ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã€‚

$$
p(\mathbf{x}_{\text{mask}} | \mathbf{x}_{\text{vis}}) = \prod_{i \in \text{mask}} p(x_i | \mathbf{x}_{\text{vis}})
$$

DiT ã¯é€æ¬¡çš„ãªãƒã‚¤ã‚ºé™¤å»ï¼ˆæ‹¡æ•£ï¼‰ã§ã‚ã‚‹ã®ã«å¯¾ã—ã€aMUSEd ã¯**æ¡ä»¶ä»˜ãç‹¬ç«‹ä»®å®š**ã®ã‚‚ã¨å…¨ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŒæ™‚ã«äºˆæ¸¬ã™ã‚‹ã€‚12ã‚¹ãƒ†ãƒƒãƒ—ã¨ã„ã†é«˜é€Ÿæ€§ã¯ã“ã®ä¸¦åˆ—ãƒ‡ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¥ã‚‹ â€” æ‹¡æ•£ã®ã€Œç¢ºç‡çš„çµŒè·¯ã€ã‚’è¾¿ã‚‹å¿…è¦ãŒãªãã€å„ã‚¹ãƒ†ãƒƒãƒ—ã§ç›´æ¥ã‚¯ãƒªãƒ¼ãƒ³ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹ã€‚

#### ç†ç”±1: Scaling Laws ã®é©ç”¨

**U-Net ã®é™ç•Œ**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‚æ€§èƒ½ãŒé ­æ‰“ã¡ã«ãªã‚‹ã€‚

Vision Transformer (ViT) ã®çŸ¥è¦‹ [Dosovitskiy+ 2020] [^1]:
- **Transformer ã¯ Scaling Laws ã«å¾“ã†**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° $N$ âˆ æ€§èƒ½ $L$ ã®é–¢ä¿‚ãŒæˆç«‹
- **CNN ã¯å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã®åˆ¶ç´„**: å±€æ‰€æ€§ãƒ»å¹³è¡Œç§»å‹•ä¸å¤‰æ€§ãŒ Scaling ã‚’é˜»å®³

DiT [Peebles & Xie 2023] [^2] ã®å®Ÿé¨“:
- DiT-XL/2 (675M params) > DiT-L (458M) > DiT-B (130M) â€” **å˜èª¿ã«æ€§èƒ½å‘ä¸Š**
- U-Net ãƒ™ãƒ¼ã‚¹ã® LDM ã¯ âˆ¼800M params ã§é ­æ‰“ã¡

**Scaling Law ã®æ•°å¼** (ç¬¬7å›ã§å­¦ã‚“ã ):
$$
L(N) = A \cdot N^{-\alpha} + L_\infty
$$
- $L(N)$: æå¤± (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° $N$ ã®é–¢æ•°)
- $\alpha > 0$: Scaling æŒ‡æ•° (Transformer ã§ã¯ $\alpha \approx 0.05$)
- $L_\infty$: ç†è«–çš„é™ç•Œ

**DiT ã®å¨åŠ›**: $N$ ã‚’å¢—ã‚„ã›ã° $L(N) \downarrow$ â€” è¨ˆç®—è³‡æºã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã•ã›ã‚Œã°æ€§èƒ½ãŒä¼¸ã³ã‚‹ä¿è¨¼ãŒã‚ã‚‹ã€‚

FIDã«ç‰¹åŒ–ã—ãŸScaling Lawã¯æ¬¡ã®å½¢ã‚’å–ã‚‹ [Zhai+ 2024]:
$$
\text{FID}(C) = A \cdot C^{-\beta} + \text{FID}_\infty, \quad \beta \approx 0.27
$$
ã“ã“ã§ $C$ ã¯è¨ˆç®—é‡ï¼ˆFLOPsï¼‰ã€$\text{FID}_\infty$ ã¯ç„¡é™è¨ˆç®—ã§ã®ç†è«–çš„é™ç•Œå€¤ã ã€‚$\beta \approx 0.27$ ãŒæ„å‘³ã™ã‚‹ã®ã¯ï¼š**è¨ˆç®—é‡ã‚’10å€ã«ã™ã‚‹ã¨ FID ãŒ $10^{-0.27} \approx 0.54$ å€ã«æ”¹å–„**ã™ã‚‹ã€ã¤ã¾ã‚ŠFIDãŒã»ã¼åŠæ¸›ã™ã‚‹ã¨ã„ã†ã“ã¨ã€‚LLMã®ã¹ãæŒ‡æ•°ï¼ˆ$\beta \approx 0.3$ï¼‰ã¨è¿‘ã„å€¤ã‚’ç¤ºã—ã¦ãŠã‚Šã€DiTãŒè¨€èªãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰¹æ€§ã‚’æŒã¤ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚U-Netã§ã¯ã“ã®å˜èª¿ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒé€”ä¸­ã§å´©ã‚Œã‚‹ â€” å¸°ç´ãƒã‚¤ã‚¢ã‚¹ãŒã€Œå¤©äº•ã€ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã‹ã‚‰ã ã€‚

**U-Net ã®å¸°ç´ãƒã‚¤ã‚¢ã‚¹**:
- **å±€æ‰€æ€§**: Conv ã® receptive field ã¯å±€æ‰€çš„
- **å¹³è¡Œç§»å‹•ä¸å¤‰æ€§**: åŒã˜ãƒ•ã‚£ãƒ«ã‚¿ã‚’å…¨ä½ç½®ã§å…±æœ‰

**ã“ã‚ŒãŒå•é¡Œã«ãªã‚‹ç†ç”±**:
- ç”»åƒã® **å¤§åŸŸçš„æ§‹é€ ** (ä¾‹: é¡”ã®å·¦å³å¯¾ç§°æ€§) ã‚’å­¦ç¿’ã—ã«ãã„
- ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ $c$ ã¨ã® **é•·è·é›¢ä¾å­˜** ã‚’æ‰ãˆã«ãã„

**Transformer ã®åˆ©ç‚¹**:
- **Self-Attention** â€” å…¨ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é–¢ä¿‚ã‚’ $O(N^2)$ ã§è¨ˆç®—
- **å¸°ç´ãƒã‚¤ã‚¢ã‚¹ãªã—** â€” ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…¨ã¦ã‚’å­¦ç¿’
- **é•·è·é›¢ä¾å­˜** â€” ãƒ†ã‚­ã‚¹ãƒˆã€Œèµ¤ã„ãƒªãƒ³ã‚´ã‚’æŒã¤å°‘å¥³ã€ã®ã€Œãƒªãƒ³ã‚´ã€ã¨ã€Œèµ¤ã„ã€ã‚’é è·é›¢ã§ã‚‚çµã³ã¤ã‘ã‚‹

**ç¬¬16å›ã§å­¦ã‚“ã  Attention ã®æ•°å¼**:
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$
- $Q, K, V$: Query, Key, Value (å…¨ã¦ [N, D])
- Softmax ã§å…¨ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é‡ã¿ã‚’è¨ˆç®— â†’ å¤§åŸŸçš„ãªé–¢ä¿‚ã‚’å­¦ç¿’

**æ•°å­¦çš„ãªå®šå¼åŒ–**: U-Netã®convæ“ä½œ $f$ ã¯**å¹³è¡Œç§»å‹•åŒå¤‰æ€§**ã‚’æº€ãŸã™ï¼š
$$
(T_\delta \circ f)(\mathbf{x}) = (f \circ T_\delta)(\mathbf{x})
$$
ã“ã“ã§ $T_\delta$ ã¯ä½ç½® $\delta$ ã®å¹³è¡Œç§»å‹•æ“ä½œã€‚ã“ã®æ€§è³ªã¯CNNã®é‡ã¿å…±æœ‰ã‹ã‚‰ç›´æ¥å°ã‹ã‚Œã‚‹ã€‚å¹³è¡Œç§»å‹•åŒå¤‰æ€§ãŒã‚ã‚‹ã¨ã€Œã©ã“ã«ç§»å‹•ã—ã¦ã‚‚åŒã˜èªè­˜ã€ãŒã§ãã‚‹ãŒã€åŒæ™‚ã«**ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªçµ¶å¯¾ä½ç½®æƒ…å ±**ã‚’æ¨ã¦ã‚‹ã“ã¨ã«ãªã‚‹ã€‚ç”»åƒç”Ÿæˆã§ã¯ã€Œå·¦ä¸Šã«å¤ªé™½ã€å³ä¸‹ã«æµ·ã€ã¨ã„ã†ä½ç½®ä¾å­˜ã®æ§‹é€ ãŒæœ¬è³ªçš„ãªã®ã«ã€U-Netã¯ãã®è¡¨ç¾ãŒè‹¦æ‰‹ã ã€‚Transformerã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯æ˜ç¤ºçš„ã«çµ¶å¯¾ä½ç½®ã‚’æ‰±ã„ã€ã“ã®åˆ¶ç´„ã‹ã‚‰è§£æ”¾ã•ã‚Œã‚‹ã€‚

**SD3 (MM-DiT) vs SDXL (U-Net)** [Esser+ 2024] [^3]:
- Human preference: SD3 > SDXL (ãƒ†ã‚­ã‚¹ãƒˆå¿ å®Ÿåº¦ãƒ»ç”»è³ª)
- Text-to-Image Benchmark: SD3 ãŒ DALL-E 3 / Midjourney v6 ã«åŒ¹æ•µ

**FLUX (DiT) vs SD3** [Black Forest Labs 2024] [^4]:
- å“è³ª: FLUX > SD3 (ç‰¹ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç†è§£)
- å•†ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: Apache 2.0 (SD3 ã¯åˆ¶é™ã‚ã‚Š)

**DiT ã®å®Ÿä¸–ç•Œå„ªä½æ€§**:
- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç†è§£**: ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (T5/CLIP) ã¨ã®ç›¸æ€§
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: 8B params ãƒ¢ãƒ‡ãƒ«ãŒç¾å®Ÿçš„ã«è¨“ç·´å¯èƒ½
- **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**: HuggingFace Diffusers ã§ DiT ãŒæ¨™æº–åŒ–

å…·ä½“çš„ãª FID æ¯”è¼ƒã‚’è¦‹ã‚Œã°å·®ã¯æ˜ç¢ºã ï¼š

| ãƒ¢ãƒ‡ãƒ« | ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ | FID-30K (ImageNet 256Ã—256) | params |
|:-------|:------------|:--------------------------|:-------|
| LDM-4 | U-Net | 3.60 | 400M |
| DiT-XL/2 | Transformer | **2.27** | 675M |
| SiT-XL/2 | Transformer | 2.06 | 675M |

DiT-XL/2 ãŒ FID 2.27 ã‚’é”æˆã—ãŸæ™‚ç‚¹ã§ã€U-Netãƒ™ãƒ¼ã‚¹ã® LDM-4ï¼ˆFID 3.60ï¼‰ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã€‚åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¦æ¨¡ã§TransformerãŒåœ§å‹ã™ã‚‹ â€” ã“ã‚Œã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æœ¬è³ªçš„ãªå„ªä½æ€§ã§ã‚ã‚Šã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã§ã¯åŸ‹ã‚ã‚‰ã‚Œãªã„å·®ã ã€‚

FID ã®å·® $3.60 - 2.27 = 1.33$ ã¯ç”»åƒå“è³ªã«ãŠã„ã¦çŸ¥è¦šçš„ã«å¤§ããªå·®ã«å¯¾å¿œã™ã‚‹ã€‚FID ã¯ Inception v3 ç‰¹å¾´ç©ºé–“ã§ã®ç”Ÿæˆåˆ†å¸ƒ $p_g$ ã¨çœŸã®åˆ†å¸ƒ $p_r$ ã®FrÃ©chetè·é›¢ï¼š
$$
\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
$$
FID ãŒä½ã„ã»ã©ç”Ÿæˆç”»åƒã®çµ±è¨ˆé‡ãŒå®Ÿç”»åƒã«è¿‘ã„ã€‚FID < 5 ã¯ã»ã¼äººé–“ã®ç›®ã§åˆ¤åˆ¥å›°é›£ãªãƒ¬ãƒ™ãƒ«ã¨ã•ã‚Œã¦ãŠã‚Šã€DiT-XL/2 ã® 2.27 ã¯ã“ã®é–¾å€¤ã‚’å¤§ããä¸‹å›ã‚‹ã€‚

### 2.4 3ã¤ã®æ¯”å–©ã§æ‰ãˆã‚‹ DiT

**æ¯”å–©1: ãƒ‘ã‚ºãƒ«ã®çµ„ã¿ç«‹ã¦æ–¹**
- **U-Net**: ãƒ”ãƒ¼ã‚¹åŒå£«ã®ã€Œéš£æ¥é–¢ä¿‚ã€ã ã‘è¦‹ã‚‹ (å±€æ‰€çš„)
- **DiT**: å…¨ãƒ”ãƒ¼ã‚¹ã‚’ä¿¯ç°ã—ã¦ã€Œå…¨ä½“åƒã€ã‹ã‚‰çµ„ã¿ç«‹ã¦ã‚‹ (å¤§åŸŸçš„)

**æ¯”å–©2: æ–‡ç« ç†è§£**
- **U-Net**: å˜èªã®ã€Œå‰å¾Œ3å˜èªã€ã ã‘è¦‹ã¦æ„å‘³ã‚’æ¨æ¸¬
- **DiT**: æ–‡ç« å…¨ä½“ã‚’èª­ã‚“ã§ã€Œæ–‡è„ˆã€ã‚’ç†è§£

**æ¯”å–©3: æ¥½å›£ã®æŒ‡æ®**
- **U-Net**: å„æ¥½å™¨ãŒã€Œéš£ã®æ¥½å™¨ã€ã ã‘èã„ã¦æ¼”å¥
- **DiT**: æŒ‡æ®è€…ãŒå…¨æ¥½å™¨ã‚’çµ±ç‡ (Self-Attention = æŒ‡æ®è€…)

### 2.5 Trojan Horse â€” Python ã‹ã‚‰ Rust/Rust ã¸

**ã“ã‚Œã¾ã§ã®è¨€èªæ§‹æˆ**:
- **ç¬¬1-8å› (Course I)**: ğŸPython 100%
- **ç¬¬9-16å› (Course II)**: ğŸPython â†’ ğŸ¦€Rust ç™»å ´ (ç¬¬9å›) â†’ ğŸ¦€Rust ç™»å ´ (ç¬¬11å›)
- **ç¬¬17-24å› (Course III)**: ğŸ¦€Rust + ğŸ¦€Rust + ğŸ”®Elixir (ç¬¬15å›ç™»å ´)
- **ç¬¬33-42å› (Course IV)**: ğŸ¦€Rust + ğŸ¦€Rust + ğŸ”®Elixir (3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯)
- **ç¬¬43-50å› (Course V)**: ğŸ¦€Rust + ğŸ¦€Rust + ğŸ”®Elixir (ç¶™ç¶š)

**Course V ã§ã®3è¨€èªå½¹å‰²**:
- **ğŸ¦€Rust**: è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Candle + Burn / GPUæœ€é©åŒ–)
- **ğŸ¦€Rust**: æ¨è«–ã‚µãƒ¼ãƒãƒ¼ (Candle / ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· / ãƒãƒƒãƒå‡¦ç†)
- **ğŸ”®Elixir**: åˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚° (Phoenix / è€éšœå®³æ€§ / A/Bãƒ†ã‚¹ãƒˆ)

**æœ¬è¬›ç¾©ã§ã®ç™»å ´**:
- Zone 4: ğŸ¦€Rust â€” Mini-DiT è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- Zone 4: ğŸ¦€Rust â€” DiT æ¨è«–ã‚µãƒ¼ãƒãƒ¼ (Candle)
- Zone 4: ğŸ”®Elixir â€” åˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚° (OTP supervision)

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®20%å®Œäº†ï¼** DiT ãŒ U-Net ã‚’è¶…ãˆã‚‹ç†ç”±ã‚’3è»¸ (Scaling/å¸°ç´ãƒã‚¤ã‚¢ã‚¹/å®Ÿä¸–ç•Œ) ã§ç†è§£ã—ãŸã€‚æ¬¡ã¯ DiT ã®æ•°å¼ã‚’å®Œå…¨å°å‡ºã™ã‚‹ â€” 60åˆ†ã®æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã¸ã€‚

---


> Progress: 20%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $ âˆ æ€§èƒ½ $ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” DiT å®Œå…¨å°å‡º

**ã‚´ãƒ¼ãƒ«**: DiTãƒ»MM-DiTãƒ»SiT ã®æ•°å¼ã‚’1è¡Œãšã¤å°å‡ºã—ã€U-Net ã¨ã®é•ã„ã‚’æ•°å­¦çš„ã«ç†è§£ã™ã‚‹ã€‚

**ã“ã®ã‚¾ãƒ¼ãƒ³ã®æ§‹æˆ**:
1. Patchify â€” ç”»åƒã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«å¤‰æ›
2. AdaLN-Zero â€” æ¡ä»¶ä»˜ãæ­£è¦åŒ–
3. DiT ãƒ–ãƒ­ãƒƒã‚¯ â€” Self-Attention + MLP
4. MM-DiT (SD3) â€” Joint Attention
5. SiT (Stochastic Interpolants) â€” Interpolant-based DiT
6. U-Net vs DiT ã®è¨ˆç®—é‡æ¯”è¼ƒ
7. âš”ï¸ Boss Battle: DiT Forward Pass å®Œå…¨å®Ÿè£…

**æ•°å¼ä¿®è¡Œã®å¿ƒæ§‹ãˆ** (ç¬¬1å›ã§å­¦ã‚“ã ):
- ã€Œæ•°å¼ã¯å£°ã«å‡ºã—ã¦èª­ã‚€ã€
- ã€Œ1è¡Œãšã¤å°å‡º â€” é£›ã°ã•ãªã„ã€
- ã€Œå…·ä½“çš„ãªæ•°å€¤ã§æ¤œè¨¼ã€

> **âš ï¸ Warning:** ä»¥é™ã®æ•°å¼ã¯é †ç•ªé€šã‚Šã«èª­ã‚€ã“ã¨ã€‚3.1 â†’ 3.2 â†’ 3.3 ã®é †ã§ä¾å­˜é–¢ä¿‚ãŒã‚ã‚‹ã€‚3.3 ã‚’ç†è§£ã™ã‚‹ã«ã¯ 3.2 ã® AdaLN-Zero ãŒå¿…é ˆã€‚é£›ã°ã™ã¨ã€Œãªãœã“ã®æ•°å¼ãŒå¿…è¦ã‹ã€ãŒè¦‹ãˆãªããªã‚‹ã€‚

DiT ã¯**ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“**ã§ã¯ãªã **VAE ã®æ½œåœ¨ç©ºé–“** $\mathbf{z} \in \mathbb{R}^{h \times w \times c}$ ä¸Šã§æ‹¡æ•£éç¨‹ã‚’è¡Œã†ï¼ˆLDM [Rombach+ 2022] ã®è¨­è¨ˆï¼‰ã€‚

VAE ã® Encoder-Decoder ã®å½¹å‰²ï¼š
$$
\mathbf{z} = \mathcal{E}(\mathbf{x}) \in \mathbb{R}^{h \times w \times c}, \quad \hat{\mathbf{x}} = \mathcal{D}(\mathbf{z})
$$

ä»£è¡¨çš„ãªè¨­å®šï¼ˆSD3ï¼‰ï¼š
- å…¥åŠ›ç”»åƒ: $\mathbb{R}^{1024 \times 1024 \times 3}$ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ï¼‰
- VAE åœ§ç¸®ç‡: $8\times$ï¼ˆå„è¾ºã‚’ $1/8$ ã«ï¼‰
- æ½œåœ¨è¡¨ç¾: $\mathbb{R}^{128 \times 128 \times 16}$ï¼ˆ$c = 16$ ãƒãƒ£ãƒãƒ«ï¼‰

DiT ã¯ $\mathbb{R}^{128 \times 128 \times 16}$ ç©ºé–“ã§ãƒã‚¤ã‚ºé™¤å»ã‚’è¡Œã„ã€ç”Ÿæˆå®Œäº†å¾Œã« $\mathcal{D}$ ã§ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã«æˆ»ã™ã€‚

**ãªãœæ½œåœ¨ç©ºé–“ï¼Ÿ** è¨ˆç®—é‡ã®æ¯”è¼ƒï¼š
- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã® DiT: $N = (1024/16)^2 = 4096$ ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ$P = 16$ï¼‰
- æ½œåœ¨ç©ºé–“ã® DiT: $N = (128/2)^2 = 4096$ ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ$P = 2$ã€ã‚ˆã‚Šç´°ã‹ã„ãƒ‘ãƒƒãƒï¼‰

åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ã‚‚**æ½œåœ¨ç©ºé–“ã¯æ„å‘³çš„ã«åœ§ç¸®æ¸ˆã¿** â€” VAE ãŒã€Œã©ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒé‡è¦ã‹ã€ã‚’ã™ã§ã«å­¦ç¿’ã—ã¦ã„ã‚‹ãŸã‚ã€DiT ã¯ã‚ˆã‚ŠæŠ½è±¡çš„ãªç‰¹å¾´ã‚’å­¦ç¿’ã§ãã‚‹ã€‚ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã§ç›´æ¥æ‹¡æ•£ã™ã‚‹ DiTï¼ˆæœªåœ§ç¸®ç‰ˆï¼‰ã¯è¨ˆç®—é‡ãŒç´„ $c^2 = 256$ å€ã«ãªã‚Šã€ç¾å®Ÿçš„ã§ãªã„ã€‚

**DiT ã®å…¨ä½“ Forward Pass æ¦‚ç•¥**:

```mermaid
graph LR
    A["ğŸ–¼ï¸ ç”»åƒ x<br/>â„^{HÃ—WÃ—3}"] --> B["VAE Encoder Îµ<br/>8Ã— åœ§ç¸®"]
    B --> C["æ½œåœ¨å¤‰æ•° z<br/>â„^{hÃ—wÃ—c}"]
    C --> D["Patchify<br/>PÃ—P åˆ†å‰²"]
    D --> E["ãƒ‘ãƒƒãƒåˆ—<br/>â„^{NÃ—D}"]
    E --> F["+ PE<br/>ä½ç½®åŸ‹ã‚è¾¼ã¿"]
    F --> G["DiT Block Ã—L<br/>AdaLN + Attn + MLP"]
    G --> H["Unpatchify<br/>ãƒˆãƒ¼ã‚¯ãƒ³â†’ç”»åƒ"]
    H --> I["ãƒã‚¤ã‚ºäºˆæ¸¬ Îµ_Î¸<br/>â„^{hÃ—wÃ—c}"]
    I --> J["VAE Decoder D<br/>å¾©å…ƒ"]
    J --> K["ç”Ÿæˆç”»åƒ xÌ‚<br/>â„^{HÃ—WÃ—3}"]
    style G fill:#ffd700,stroke:#ff6347,stroke-width:2px
```

### 3.1 Patchify â€” ç”»åƒã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«å¤‰æ›

DiT ã®ç¬¬ä¸€æ­©ã¯ **Patchify** â€” ç”»åƒ $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$ ã‚’ãƒ‘ãƒƒãƒåˆ— $\mathbf{z} \in \mathbb{R}^{N \times D}$ ã«å¤‰æ›ã™ã‚‹ã€‚

**Vision Transformer (ViT) ã®æ‰‹æ³•** [Dosovitskiy+ 2020] [^1]:
1. ç”»åƒã‚’ $P \times P$ ãƒ‘ãƒƒãƒã«åˆ†å‰²
2. å„ãƒ‘ãƒƒãƒã‚’ç·šå½¢å¤‰æ›ã§ $D$ æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«åŸ‹ã‚è¾¼ã¿
3. Positional Encoding ã‚’è¿½åŠ 

**æ•°å¼**:
$$
\begin{align}
\text{Patchify}: \mathbb{R}^{H \times W \times C} &\to \mathbb{R}^{N \times (P^2 \cdot C)} \\
\mathbf{x} &\mapsto [\mathbf{p}_1, \mathbf{p}_2, \ldots, \mathbf{p}_N]
\end{align}
$$
ã“ã“ã§:
- $N = \frac{H}{P} \times \frac{W}{P}$ â€” ãƒ‘ãƒƒãƒæ•°
- $\mathbf{p}_i \in \mathbb{R}^{P^2 \cdot C}$ â€” $i$ ç•ªç›®ã®ãƒ‘ãƒƒãƒ (flatten)

**ç·šå½¢åŸ‹ã‚è¾¼ã¿**:
$$
\mathbf{z}_i = \mathbf{W}_{\text{patch}} \mathbf{p}_i + \mathbf{b}_{\text{patch}}
$$
- $\mathbf{W}_{\text{patch}} \in \mathbb{R}^{D \times (P^2 \cdot C)}$ â€” åŸ‹ã‚è¾¼ã¿è¡Œåˆ—
- $\mathbf{z}_i \in \mathbb{R}^D$ â€” åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«

**Positional Encoding** (ç¬¬16å›ã§å­¦ã‚“ã ):
$$
\mathbf{z}_i^{\text{pos}} = \mathbf{z}_i + \mathbf{PE}(i)
$$
- $\mathbf{PE}(i) \in \mathbb{R}^D$ â€” ä½ç½®åŸ‹ã‚è¾¼ã¿ (sinusoidal or learned)

**å…·ä½“ä¾‹**: 256Ã—256 RGB ç”»åƒã‚’ 16Ã—16 ãƒ‘ãƒƒãƒã«åˆ†å‰²
- $H = W = 256$, $C = 3$, $P = 16$
- $N = \frac{256}{16} \times \frac{256}{16} = 16 \times 16 = 256$ ãƒ‘ãƒƒãƒ
- å„ãƒ‘ãƒƒãƒ: $16 \times 16 \times 3 = 768$ æ¬¡å…ƒ
- åŸ‹ã‚è¾¼ã¿å¾Œ: $D = 768$ (ViT-Base ã¨åŒã˜)

**æ•°å€¤æ¤œè¨¼**:

$H = W = 256$ã€$C = 3$ï¼ˆRGBï¼‰ã€$P = 16$ ã®å…·ä½“ä¾‹ã§Shape flowã‚’è¿½ã†ã€‚

ãƒ‘ãƒƒãƒæ•°ï¼š
$$
N = \frac{256}{16} \times \frac{256}{16} = 16 \times 16 = 256
$$

å„ãƒ‘ãƒƒãƒã‚’flattenã—ãŸæ¬¡å…ƒï¼š
$$
P^2 \cdot C = 16 \times 16 \times 3 = 768
$$

å¾“ã£ã¦ç¬¬1ãƒ‘ãƒƒãƒã¯ $\mathbf{p}_1 \in \mathbb{R}^{768}$ã€‚å…¨ãƒ‘ãƒƒãƒåˆ—ã¯ $[\mathbf{p}_1, \ldots, \mathbf{p}_{256}] \in \mathbb{R}^{256 \times 768}$ã€‚

åŸ‹ã‚è¾¼ã¿è¡Œåˆ— $\mathbf{W}_{\text{patch}} \in \mathbb{R}^{768 \times 768}$ ã§ç·šå½¢å¤‰æ›ã™ã‚‹ã¨Shape ã¯å¤‰ã‚ã‚‰ãªã„ã€‚Shape flow ã‚’ã¾ã¨ã‚ã‚‹ã¨ï¼š

$$
\mathbb{R}^{256 \times 256 \times 3} \;\xrightarrow{\text{split into patches}}\; \mathbb{R}^{256 \times 768} \;\xrightarrow{\mathbf{W}_{\text{patch}}}\; \mathbb{R}^{256 \times 768}
$$

åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯ $768 \times 768 = 589{,}824$ã€‚ã“ã‚Œã¯ã¡ã‚‡ã†ã© Conv2d(3, 768, kernel\_size=16, stride=16) ã¨ç­‰ä¾¡ã§ã‚ã‚Šã€å®Ÿè£…ä¸Šã¯ã“ã®ç•³ã¿è¾¼ã¿ã§ä»£æ›¿ã•ã‚Œã‚‹ã€‚Positional Encodingã‚’åŠ ãˆãŸæœ€çµ‚Shapeã‚‚ $\mathbb{R}^{256 \times 768}$â€”Transformerã¸ã®å…¥åŠ›ã¨ã—ã¦æ¸¡ã›ã‚‹çŠ¶æ…‹ã ã€‚

#### 2æ¬¡å…ƒ Sinusoidal Positional Encoding ã®å°å‡º

ç”»åƒã®ãƒ‘ãƒƒãƒã«ã¯ç¸¦ $i$ãƒ»æ¨ª $j$ ã®2æ¬¡å…ƒåº§æ¨™ãŒã‚ã‚‹ã€‚1æ¬¡å…ƒã® Sinusoidal PEï¼ˆç¬¬16å›ï¼‰ã‚’2æ¬¡å…ƒã«æ‹¡å¼µã™ã‚‹ã¨ï¼š

$$
\text{PE}(i, 2k) = \sin\!\left(\frac{i}{10000^{2k/D}}\right), \quad \text{PE}(i, 2k+1) = \cos\!\left(\frac{i}{10000^{2k/D}}\right)
$$

ã“ã“ã§ $i$ ã¯ãƒ‘ãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆè¡Œæ–¹å‘ï¼‰ã€$k = 0, 1, \ldots, D/2 - 1$ ã¯æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚2æ¬¡å…ƒç‰ˆã§ã¯ $D$ æ¬¡å…ƒã‚’åŠåˆ†ãšã¤è¡Œãƒ»åˆ—ã«å‰²ã‚Šå½“ã¦ã‚‹ï¼š

$$
\mathbf{PE}(i, j) = [\underbrace{\text{PE}_{\text{row}}(i)}_{\in \mathbb{R}^{D/2}},\; \underbrace{\text{PE}_{\text{col}}(j)}_{\in \mathbb{R}^{D/2}}] \in \mathbb{R}^D
$$

**åº•æ•° 10000 ã®æ„ç¾©**: å‘¨æ³¢æ•° $\omega_k = 1 / 10000^{2k/D}$ ã¯ $k$ ã¨ã¨ã‚‚ã«æŒ‡æ•°çš„ã«å°ã•ããªã‚‹ã€‚$k = 0$ ã§ã¯ $\omega_0 = 1.0$ï¼ˆ1ãƒ‘ãƒƒãƒå‘¨æœŸã®é«˜å‘¨æ³¢ï¼‰ã€$k = D/2 - 1$ ã§ã¯ $\omega_{D/2-1} = 1/10000$ï¼ˆ10000ãƒ‘ãƒƒãƒå‘¨æœŸã®ä½å‘¨æ³¢ï¼‰ã€‚$D = 768$ ã®å ´åˆã€ä½ç½® $i \in [1, 256]$ ã«å¯¾ã—ã¦sin/cosã®å‘¨æœŸã¯ $[2\pi, 2\pi \times 10000]$ ã®åºƒã„ãƒ¬ãƒ³ã‚¸ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ã€‚ã“ã®å¹…åºƒã„å‘¨æ³¢æ•°å¸¯åŸŸãŒã€Œè¿‘ã„ãƒ‘ãƒƒãƒã¯ä¼¼ãŸè¡¨ç¾ãƒ»é ã„ãƒ‘ãƒƒãƒã¯ç•°ãªã‚‹è¡¨ç¾ã€ã¨ã„ã†æ€§è³ªã‚’ä¿è¨¼ã—ã€256ãƒ‘ãƒƒãƒå…¨ã¦ã®ä½ç½®ã‚’ä¸€æ„ã«ç¬¦å·åŒ–ã§ãã‚‹ã€‚

å­¦ç¿’æ¸ˆã¿ã®ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆDiT-XL ã¯ Learnable PE ã‚’ä½¿ç”¨ï¼‰ã‚‚åŒç­‰ã®è¡¨ç¾åŠ›ã‚’æŒã¤ãŒã€Sinusoidal PE ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„è§£åƒåº¦ã¸ã®å¤–æŒ¿ï¼ˆViT-L ã®256â†’512ãƒ‘ãƒƒãƒã¸ã®è»¢ç§»ï¼‰ã«ãŠã„ã¦å„ªä½æ€§ã‚’ç¤ºã™ã€‚

### 3.2 AdaLN-Zero â€” æ¡ä»¶ä»˜ãæ­£è¦åŒ–

**Layer Normalization (LN)** [Ba+ 2016] [^5] (ç¬¬2å›ã§å­¦ã‚“ã ):
$$
\text{LN}(\mathbf{x}) = \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$
- $\mu = \frac{1}{D} \sum_{i=1}^D x_i$ â€” å¹³å‡
- $\sigma^2 = \frac{1}{D} \sum_{i=1}^D (x_i - \mu)^2$ â€” åˆ†æ•£

**Adaptive Layer Normalization (AdaLN)** [Perez+ 2018] [^6]:
$$
\text{AdaLN}(\mathbf{x}, \mathbf{c}) = \gamma(\mathbf{c}) \odot \text{LN}(\mathbf{x}) + \beta(\mathbf{c})
$$
- $\gamma(\mathbf{c}), \beta(\mathbf{c})$ â€” æ¡ä»¶ $\mathbf{c}$ ã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹ scale & shift
- $\odot$ â€” è¦ç´ ã”ã¨ã®ç©

**AdaLN-Zero** [Peebles & Xie 2023] [^2] â€” DiT ã®éµ:
$$
\gamma(\mathbf{c}) \text{ ã¨ } \beta(\mathbf{c}) \text{ ã‚’ **ã‚¼ãƒ­åˆæœŸåŒ–**}
$$

**ãªãœã‚¼ãƒ­åˆæœŸåŒ–ï¼Ÿ**
- **è¨“ç·´åˆæœŸ**: $\gamma = 0, \beta = 0$ â†’ AdaLN ã®å‡ºåŠ› = 0 â†’ **Residual æ¥ç¶šãŒæ’ç­‰å†™åƒ**
- **è¨“ç·´ä¸­æœŸ**: $\gamma, \beta$ ãŒå­¦ç¿’ã•ã‚Œã¦æ¡ä»¶ $\mathbf{c}$ ã®å½±éŸ¿ãŒå¾ã€…ã«å¢—åŠ 
- **å®‰å®šæ€§**: Skip connections (ç¬¬2å›ã§å­¦ã‚“ã ) ãŒè¨“ç·´åˆæœŸã®å‹¾é…ã‚’å®‰å®šåŒ–

**ãªãœ Batch Normalization ã§ãªã Layer Normalization ãªã®ã‹ï¼Ÿ** Batch Normalization ã¯ãƒãƒƒãƒè»¸ã§çµ±è¨ˆé‡ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã¸ã®ä¾å­˜ãŒã‚ã‚‹ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã¯è¤‡æ•°ã®ç•°ãªã‚‹ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— $t$ ã®ã‚µãƒ³ãƒ—ãƒ«ãŒ1ãƒãƒƒãƒã«æ··åœ¨ã™ã‚‹ãŸã‚ã€BN ã®çµ±è¨ˆé‡ãŒã‚¹ãƒ†ãƒƒãƒ—é–“ã§æ±šæŸ“ã•ã‚Œã‚‹ã€‚LN ã¯ã‚µãƒ³ãƒ—ãƒ«å†…ã®ç‰¹å¾´è»¸ã§æ­£è¦åŒ–ã™ã‚‹ãŸã‚ã€ã“ã®å•é¡ŒãŒç™ºç”Ÿã—ãªã„ã€‚æ•°å¼ã§æ¯”è¼ƒã™ã‚‹ã¨ï¼š

$$
\text{BN}: \quad \mu_{\text{BN}} = \frac{1}{B} \sum_{b=1}^B x_{b,i} \quad \text{ï¼ˆãƒãƒƒãƒè»¸å¹³å‡ï¼‰}
$$
$$
\text{LN}: \quad \mu_{\text{LN}} = \frac{1}{D} \sum_{i=1}^D x_i \quad \text{ï¼ˆç‰¹å¾´è»¸å¹³å‡ï¼‰}
$$

BN ã§ã¯ç•°ãªã‚‹ $t$ ã®ã‚µãƒ³ãƒ—ãƒ«ãŒæ··åœ¨ã™ã‚‹ $B$ ã‚µãƒ³ãƒ—ãƒ«ã§å¹³å‡ã‚’å–ã‚‹ãŸã‚ã€ã€Œ$t = 0.1$ ã®ã»ã¼ã‚¯ãƒªãƒ¼ãƒ³ãªã‚µãƒ³ãƒ—ãƒ«ã€ã¨ã€Œ$t = 0.9$ ã®ã»ã¼ãƒã‚¤ã‚ºã®ã‚µãƒ³ãƒ—ãƒ«ã€ã®çµ±è¨ˆé‡ãŒæ··åˆã•ã‚Œã‚‹ã€‚LN ã¯å„ã‚µãƒ³ãƒ—ãƒ«ç‹¬ç«‹ã«æ­£è¦åŒ–ã™ã‚‹ãŸã‚ã€ã“ã®å•é¡ŒãŒãªã„ã€‚ã“ã‚ŒãŒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«å…¨èˆ¬ã§ LN ãŒæ¨™æº–çš„ãªç†ç”±ã ã€‚
$$
\begin{align}
\mathbf{c} &= [\mathbf{t}, \mathbf{c}_{\text{cond}}] \quad \text{(timestep + condition)} \\
\gamma &= \text{MLP}_\gamma(\mathbf{c}) \quad \text{(initialized to 0)} \\
\beta &= \text{MLP}_\beta(\mathbf{c}) \quad \text{(initialized to 0)} \\
\text{AdaLN-Zero}(\mathbf{x}, \mathbf{c}) &= \gamma \odot \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
\end{align}
$$

**å…·ä½“ä¾‹**: $\mathbf{x} \in \mathbb{R}^{256 \times 768}$ (256 tokens, 768 dims)
- $\mathbf{t} \in \mathbb{R}^{128}$ â€” timestep embedding (ç¬¬36å›ã§å­¦ã‚“ã )
- $\mathbf{c}_{\text{cond}} \in \mathbb{R}^{512}$ â€” text embedding (CLIP)
- $\mathbf{c} = [\mathbf{t}, \mathbf{c}_{\text{cond}}] \in \mathbb{R}^{640}$
- $\gamma, \beta \in \mathbb{R}^{768}$ â€” MLP å‡ºåŠ›

**æ•°å€¤æ¤œè¨¼**:

$\mathbf{x} = [1.0,\; 2.0,\; 3.0] \in \mathbb{R}^3$ ã®3æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã§ Layer Normalization ã‚’æ‰‹è¨ˆç®—ã™ã‚‹ã€‚

å¹³å‡ï¼š
$$
\mu = \frac{1.0 + 2.0 + 3.0}{3} = 2.0
$$

åˆ†æ•£ï¼ˆåå·®ãªã—ï¼‰ï¼š
$$
\sigma^2 = \frac{(1.0 - 2.0)^2 + (2.0 - 2.0)^2 + (3.0 - 2.0)^2}{3} = \frac{1 + 0 + 1}{3} \approx 0.667
$$

LNå‡ºåŠ›ï¼ˆ$\epsilon = 10^{-6}$ï¼‰ï¼š
$$
\text{LN}(\mathbf{x}) = \frac{\mathbf{x} - 2.0}{\sqrt{0.667 + 10^{-6}}} \approx \frac{[-1,\; 0,\; 1]}{0.8165} = [-1.225,\; 0.0,\; 1.225]
$$

æ¡ä»¶ä»˜ãã‚¹ã‚±ãƒ¼ãƒ« $\gamma = 1.5$ã€ã‚·ãƒ•ãƒˆ $\beta = 0.1$ ã‚’é©ç”¨ï¼š
$$
\text{AdaLN}(\mathbf{x}) = 1.5 \times [-1.225,\; 0.0,\; 1.225] + 0.1 = [-1.737,\; 0.1,\; 1.937]
$$

ã‚¼ãƒ­åˆæœŸåŒ–æ™‚ï¼ˆ$\gamma = 0,\; \beta = 0$ï¼‰ï¼š
$$
\text{AdaLN-Zero}(\mathbf{x}) = 0 \times \text{LN}(\mathbf{x}) + 0 = [0,\; 0,\; 0]
$$

Residualæ¥ç¶šã§è¶³ã—æˆ»ã™ã¨ $\mathbf{z} + \mathbf{0} = \mathbf{z}$ï¼ˆæ’ç­‰å†™åƒï¼‰ã€‚è¨“ç·´åˆæœŸã®å‹¾é…å®‰å®šæ€§ãŒã“ã“ã‹ã‚‰æ¥ã‚‹ â€” $\gamma, \beta$ ãŒå­¦ç¿’ã•ã‚Œã‚‹ã«ã¤ã‚Œã¦ã€æ¡ä»¶ $\mathbf{c}$ ã®å½±éŸ¿ãŒå¾ã€…ã«å¢—åŠ ã™ã‚‹ã€‚

#### ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«æ¡ä»¶ä»˜ã‘ã®åŸ‹ã‚è¾¼ã¿

ImageNet ã‚¯ãƒ©ã‚¹æ¡ä»¶ä»˜ãç”Ÿæˆã§ã¯ã€æ¡ä»¶ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{c}_{\text{cond}}$ ã« **ã‚¯ãƒ©ã‚¹åŸ‹ã‚è¾¼ã¿** ã‚’ä½¿ã†ï¼š

$$
\mathbf{c}_{\text{cond}} = \mathbf{E}_{\text{class}}[y] \in \mathbb{R}^{D}
$$

ã“ã“ã§ $\mathbf{E}_{\text{class}} \in \mathbb{R}^{1000 \times D}$ï¼ˆImageNet ã®1000ã‚¯ãƒ©ã‚¹ãã‚Œãã‚Œã« $D$ æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã€$y \in \{0, \ldots, 999\}$ ã¯ã‚¯ãƒ©ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚æ¡ä»¶ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ§‹æˆï¼š

$$
\mathbf{c} = [\mathbf{t},\; \mathbf{c}_{\text{cond}}] = [\mathbf{t},\; \mathbf{E}_{\text{class}}[y]] \in \mathbb{R}^{D_t + D}
$$

ã“ã‚Œã‚’AdaLN-Zeroã®MLPã«å…¥åŠ›ã™ã‚‹ã“ã¨ã§ã€ã€Œ$t = 0.5$ æ™‚ç‚¹ã§çŠ¬ã‚¯ãƒ©ã‚¹ã‚’ç”Ÿæˆä¸­ã€ã¨ã„ã†2æ¬¡å…ƒã®æ¡ä»¶ä»˜ã‘ãŒå®Ÿç¾ã™ã‚‹ã€‚Classifier-Free Guidanceï¼ˆCFGï¼‰ã§ã¯ $y = \emptyset$ï¼ˆnull ã‚¯ãƒ©ã‚¹ï¼‰ã§ç„¡æ¡ä»¶ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{c}_\emptyset$ ã‚’å®šç¾©ã—ã€æ¨è«–æ™‚ã«ï¼š

$$
\tilde{\mathbf{v}}_\theta(\mathbf{x}_t, t, y) = (1 + w)\, \mathbf{v}_\theta(\mathbf{x}_t, t, y) - w\, \mathbf{v}_\theta(\mathbf{x}_t, t, \emptyset)
$$

ã®ã‚ˆã†ã«ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹å¼·åº¦ $w > 0$ ã§ã‚µãƒ³ãƒ—ãƒ«å“è³ªã‚’åˆ¶å¾¡ã™ã‚‹ã€‚$w = 1.5$ ãŒ DiT-XL/2 ã® FID 2.27 ã‚’é”æˆã—ãŸè¨­å®šã ã€‚

### 3.3 DiT ãƒ–ãƒ­ãƒƒã‚¯ â€” Self-Attention + MLP

DiT ã®åŸºæœ¬ãƒ–ãƒ­ãƒƒã‚¯ã¯ **Transformer encoder** ã¨åŒã˜æ§‹é€  (ç¬¬16å›ã§å­¦ã‚“ã )ã€‚

**DiT Block ã®æ§‹æˆ**:
1. AdaLN-Zero pre-normalization
2. Multi-Head Self-Attention
3. Residual connection
4. AdaLN-Zero pre-normalization (2å›ç›®)
5. MLP (Feed-Forward)
6. Residual connection

**æ•°å¼**:
$$
\begin{align}
\mathbf{h}_1 &= \text{AdaLN-Zero}(\mathbf{z}, \mathbf{c}) \\
\mathbf{a} &= \text{Attention}(\mathbf{h}_1) \\
\mathbf{z}' &= \mathbf{z} + \mathbf{a} \quad \text{(residual)} \\
\mathbf{h}_2 &= \text{AdaLN-Zero}(\mathbf{z}', \mathbf{c}) \\
\mathbf{m} &= \text{MLP}(\mathbf{h}_2) \\
\mathbf{z}_{\text{out}} &= \mathbf{z}' + \mathbf{m} \quad \text{(residual)}
\end{align}
$$

**Residual æ¥ç¶šã¨å‹¾é…æµã®ä¿è¨¼**: $L$ å±¤ DiT ã®å‹¾é…ã‚’è€ƒãˆã‚‹ã€‚Residualæ¥ç¶š $\mathbf{z}' = \mathbf{z} + F(\mathbf{z})$ ã®é€†ä¼æ’­ï¼š
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{z}} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}'} \cdot \left(I + \frac{\partial F}{\partial \mathbf{z}}\right)
$$
å˜ä½è¡Œåˆ— $I$ ã®å­˜åœ¨ã«ã‚ˆã‚Šã€$\partial F / \partial \mathbf{z} \approx 0$ï¼ˆè¨“ç·´åˆæœŸï¼‰ã§ã‚‚å‹¾é…ãŒ $\mathbf{z}$ ã«ç›´æ¥ä¼ã‚ã‚‹ã€‚28å±¤ã® DiT-XL ã§ã¯ä¹—ç®—é€£é– $\prod_{\ell=1}^{28}(\cdot)$ ãŒç™ºç”Ÿã™ã‚‹ãŒã€å„å±¤ã® $I$ é …ãŒå‹¾é…æ¶ˆå¤±ã‚’é˜²ãã€‚AdaLN-Zero ã¨ã®çµ„ã¿åˆã‚ã›ã§**åˆæœŸåŒ–å•é¡Œã‚’å®Œå…¨ã«æ’é™¤**ã—ãŸè¨­è¨ˆã ã€‚

**Multi-Head Self-Attention** (ç¬¬16å›ã§å­¦ã‚“ã ):
$$
\begin{align}
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V \\
Q &= \mathbf{h}_1 W_Q, \quad K = \mathbf{h}_1 W_K, \quad V = \mathbf{h}_1 W_V \\
\text{MultiHead}(\mathbf{h}_1) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_H) W_O
\end{align}
$$

#### ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å› å­ $1/\sqrt{d_k}$ ã®å¿…ç„¶æ€§

ã€Œãªãœ $\sqrt{d_k}$ ã§å‰²ã‚‹ã®ã‹ï¼Ÿã€â€” ã“ã‚Œã¯æ•°å€¤å®‰å®šæ€§ã®æœ¬è³ªçš„ãªå•ã„ã ã€‚

$q, k \in \mathbb{R}^{d_k}$ ãŒç‹¬ç«‹ã«æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰å¼•ã‹ã‚Œã‚‹ã¨ã™ã‚‹ï¼š$q_i, k_i \sim \mathcal{N}(0, 1)$ã€‚

å†…ç© $q \cdot k = \sum_{i=1}^{d_k} q_i k_i$ ã®æœŸå¾…å€¤ã¨åˆ†æ•£ã‚’è¨ˆç®—ã™ã‚‹ï¼š

$$
\mathbb{E}[q \cdot k] = \sum_{i=1}^{d_k} \mathbb{E}[q_i]\, \mathbb{E}[k_i] = 0
$$

$$
\text{Var}(q \cdot k) = \sum_{i=1}^{d_k} \text{Var}(q_i k_i) = \sum_{i=1}^{d_k} \mathbb{E}[q_i^2]\, \mathbb{E}[k_i^2] = \sum_{i=1}^{d_k} 1 \cdot 1 = d_k
$$

ã¤ã¾ã‚Š $\text{std}(q \cdot k) = \sqrt{d_k}$ã€‚DiT-Bã® $d_k = 64$ ã§ã¯ $\text{std} = 8$ã€‚

**ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã—ã®å±é™º**: ã‚¹ã‚³ã‚¢ãŒ $\pm 8$ ç¨‹åº¦ã«æ•£ã‚‰ã°ã‚‹ã¨ã€Softmaxã¯ã»ã¼ one-hot ã«ãªã‚‹ï¼š
$$
\text{softmax}(8, 0, 0, \ldots) \approx (1.0, 0.0, 0.0, \ldots)
$$
ã“ã‚Œã¯å‹¾é…æ¶ˆå¤±ï¼ˆ$\partial \text{softmax} / \partial x \approx 0$ï¼‰ã‚’å¼•ãèµ·ã“ã—ã€è¨“ç·´ãŒé€²ã¾ãªããªã‚‹ã€‚

$\sqrt{d_k}$ ã§å‰²ã‚‹ã¨ $\text{std}(q \cdot k / \sqrt{d_k}) = 1$ ã«æ­£è¦åŒ–ã•ã‚Œã€Softmax ã¯é©åº¦ãªã€Œsoftã€ãªåˆ†å¸ƒã‚’ä¿ã¤ã€‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯å˜ãªã‚‹æ…£ç¿’ã§ã¯ãªãã€è¨“ç·´å¯èƒ½æ€§ã‚’ä¿è¨¼ã™ã‚‹**æ•°å­¦çš„å¿…ç„¶**ã ã€‚

#### Multi-Head ã®æ„ç¾©ï¼šç•°ãªã‚‹ã€Œæ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’ä¸¦åˆ—å­¦ç¿’

å˜ä¸€ãƒ˜ãƒƒãƒ‰ã®Attentionã¯1ç¨®é¡ã®é–¢ä¿‚æ€§ã—ã‹æ‰ãˆã‚‰ã‚Œãªã„ã€‚$H$ ãƒ˜ãƒƒãƒ‰ã«åˆ†å‰²ã™ã‚‹ã¨ã€å„ãƒ˜ãƒƒãƒ‰ãŒ **ç•°ãªã‚‹éƒ¨åˆ†ç©ºé–“** ã§é–¢ä¿‚ã‚’å­¦ç¿’ã§ãã‚‹ï¼š

$$
\text{head}_h = \text{Attention}(Q W_Q^{(h)},\; K W_K^{(h)},\; V W_V^{(h)}), \quad h = 1, \ldots, H
$$

ã“ã“ã§ $W_Q^{(h)}, W_K^{(h)}, W_V^{(h)} \in \mathbb{R}^{D \times d_k}$ï¼ˆ$d_k = D / H$ï¼‰ã€‚

DiT-XL ã§ã¯ $H = 16$ã€$d_k = 72$ã€‚16ãƒ˜ãƒƒãƒ‰ã®ã†ã¡ï¼š
- ãƒ˜ãƒƒãƒ‰1-4ï¼šå±€æ‰€ãƒ‘ãƒƒãƒé–“ã®ç©ºé–“çš„éš£æ¥é–¢ä¿‚
- ãƒ˜ãƒƒãƒ‰5-8ï¼šæ„å‘³çš„é¡ä¼¼ãƒ‘ãƒƒãƒï¼ˆåŒè‰²ãƒ»åŒãƒ†ã‚¯ã‚¹ãƒãƒ£ï¼‰
- ãƒ˜ãƒƒãƒ‰9-12ï¼šæ¡ä»¶ $\mathbf{c}$ ã«å¿œã˜ãŸé•·è·é›¢ä¾å­˜
- ãƒ˜ãƒƒãƒ‰13-16ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæ§‹å›³ã®æ•´åˆæ€§

ã“ã‚Œã¯è¦³æ¸¬ã•ã‚ŒãŸã‚‚ã®ã§ã¯ãªãæ¦‚å¿µçš„ãªèª¬æ˜ã ãŒã€è§£é‡ˆå¯èƒ½æ€§ç ”ç©¶ [Zhao+ 2025] ã§ã¯Headã”ã¨ã«ç•°ãªã‚‹æ„å‘³çš„å½¹å‰²ãŒå®Ÿéš›ã«ç¢ºèªã•ã‚Œã¦ã„ã‚‹ã€‚å…¨ãƒ˜ãƒƒãƒ‰ã‚’çµåˆï¼š

$$
\text{MultiHead}(\mathbf{h}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_H)\, W_O
$$

$W_O \in \mathbb{R}^{D \times D}$ ã§å…ƒã® $D$ æ¬¡å…ƒã«å°„å½±ã€‚è¨ˆç®—é‡ã¯å˜ä¸€ãƒ˜ãƒƒãƒ‰ã¨åŒã˜ $O(N^2 D)$ ã®ã¾ã¾ãƒ˜ãƒƒãƒ‰æ•°ã‚’å¢—ã‚„ã›ã‚‹ â€” ã“ã‚ŒãŒMulti-Headã®ã€Œã‚¿ãƒ€é£¯ã€ã ã€‚
$$
\text{MLP}(\mathbf{x}) = \text{GELU}(\mathbf{x} W_1 + \mathbf{b}_1) W_2 + \mathbf{b}_2
$$
- GELU: Gaussian Error Linear Unit [Hendrycks & Gimpel 2016] [^7]
- Hidden dim: $4D$ (æ¨™æº–çš„ãª Transformer ã®è¨­å®š)

**GELU ã®æ•°å­¦çš„å®šç¾©**:
$$
\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}\left[1 + \text{erf}\!\left(\frac{x}{\sqrt{2}}\right)\right]
$$
ã“ã“ã§ $\Phi(x)$ ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒã®CDFã€‚ç›´æ„Ÿ: å…¥åŠ› $x$ ã‚’ã€Œ$x$ ãŒå¤§ãã„ç¢ºç‡ã€ã§é‡ã¿ä»˜ã‘ã—ã¦é€šéã•ã›ã‚‹ã€‚$x \to +\infty$ ã§ã¯ $\text{GELU}(x) \approx x$ï¼ˆç·šå½¢ï¼‰ã€$x \to -\infty$ ã§ã¯ $\approx 0$ï¼ˆã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼‰ã€‚ReLU ã®æ»‘ã‚‰ã‹ãªè¿‘ä¼¼ã¨ã—ã¦æ©Ÿèƒ½ã—ã€Transformer ç³»ãƒ¢ãƒ‡ãƒ«ã§ã¯æ¨™æº–çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚

#### DiT ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ã‚µã‚¤ã‚ºæ¯”è¼ƒ

DiT ã¯ ViT ã®å‘½åè¦å‰‡ã‚’è¸è¥²ã™ã‚‹ 4 ã¤ã®æ¨™æº–ã‚µã‚¤ã‚ºãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ [Peebles & Xie 2023]ï¼š

| ãƒ¢ãƒ‡ãƒ« | $D$ | Layers $L$ | Heads $H$ | Params | FID-50K |
|:-------|:----|:-----------|:---------|:-------|:--------|
| DiT-S/8 | 384 | 12 | 6 | 33M | 43.5 |
| DiT-B/4 | 768 | 12 | 12 | 130M | 18.6 |
| DiT-L/2 | 1024 | 24 | 16 | 458M | 5.02 |
| **DiT-XL/2** | **1152** | **28** | **16** | **675M** | **2.27** |

ã‚¹ãƒ©ãƒƒã‚·ãƒ¥å¾Œã®æ•°å­—ï¼ˆ/2ã€/4ã€/8ï¼‰ã¯ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º $P$ ã‚’ç¤ºã™ã€‚$P = 2$ ã§ã¯ $256 \times 256$ æ½œåœ¨ç©ºé–“ã§ $N = 128 \times 128 / 2^2 = 16{,}384$ ãƒˆãƒ¼ã‚¯ãƒ³ã«ãªã‚‹ãŸã‚ã€è¨ˆç®—é‡ã¯ $P = 16$ ã® $256$ ãƒˆãƒ¼ã‚¯ãƒ³æ¯”ã§ $(16384/256)^2 = 4096$ å€ã«å¢—å¤§ã™ã‚‹ã€‚DiT-XL/2 ãŒæœ€é«˜æ€§èƒ½ã§ã‚ã‚‹ç†ç”±ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã ã‘ã§ãªãã€ã“ã® **ç´°ã‹ã„ãƒ‘ãƒƒãƒè§£åƒåº¦** ã«ã‚ã‚‹ã€‚

**Unpatchify**ï¼ˆé€†å¤‰æ›ï¼‰ã®æ•°å¼ï¼š

Transformerã®å‡ºåŠ› $\mathbf{z}_L \in \mathbb{R}^{N \times D}$ ã‚’ç”»åƒç©ºé–“ã«æˆ»ã™ï¼š
$$
\text{Unpatchify}: \mathbb{R}^{N \times D} \to \mathbb{R}^{H \times W \times 2C}
$$
å„ãƒˆãƒ¼ã‚¯ãƒ³ $\mathbf{z}^{(i)} \in \mathbb{R}^D$ ã‚’ç·šå½¢å¤‰æ› $\mathbf{W}_{\text{out}} \in \mathbb{R}^{D \times (P^2 \cdot 2C)}$ ã§ $P^2 \times 2C$ æ¬¡å…ƒã«æˆ»ã—ã€å¯¾å¿œã™ã‚‹ãƒ‘ãƒƒãƒä½ç½®ã«é…ç½®ã™ã‚‹ã€‚å‡ºåŠ›æ¬¡å…ƒãŒ $2C$ï¼ˆå…ƒã® $C$ ã®2å€ï¼‰ãªã®ã¯ã€ãƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta$ ã¨åˆ†æ•£ $\Sigma_\theta$ ã®ä¸¡æ–¹ã‚’åŒæ™‚ã«å‡ºåŠ›ã™ã‚‹ãŸã‚ã ï¼ˆç¬¬36å›ã® DDPM ã¨åŒã˜è¨­è¨ˆï¼‰ã€‚

**æ•°å€¤æ¤œè¨¼**:

$N = 3$ ãƒˆãƒ¼ã‚¯ãƒ³ã€$d_k = 4$ ã®æœ€å°ä¾‹ã§Scaled Dot-Product Attentionã‚’æ‰‹è¨ˆç®—ã™ã‚‹ã€‚

Queryè¡Œåˆ—ã¨Keyè¡Œåˆ—ãŒå˜ä½ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå„ãƒˆãƒ¼ã‚¯ãƒ³ãŒç›´äº¤ï¼‰ã ã¨ã™ã‚‹ï¼š
$$
Q = K = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{pmatrix} \in \mathbb{R}^{3 \times 4}
$$

ç”Ÿã®ã‚¹ã‚³ã‚¢è¡Œåˆ— $QK^\top \in \mathbb{R}^{3 \times 3}$ï¼š
$$
QK^\top = I_3 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}
$$

ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œï¼ˆ$\sqrt{d_k} = \sqrt{4} = 2$ï¼‰ï¼š
$$
\frac{QK^\top}{\sqrt{4}} = \frac{1}{2} I_3 = \begin{pmatrix} 0.5 & 0 & 0 \\ 0 & 0.5 & 0 \\ 0 & 0 & 0.5 \end{pmatrix}
$$

ç¬¬1è¡Œã«å¯¾ã—ã¦Softmaxã‚’é©ç”¨ï¼š
$$
\text{softmax}(0.5,\; 0,\; 0) = \frac{(e^{0.5},\; e^0,\; e^0)}{e^{0.5} + e^0 + e^0} \approx \frac{(1.649,\; 1.0,\; 1.0)}{3.649} = (0.452,\; 0.274,\; 0.274)
$$

è¡Œã®å’Œï¼š$0.452 + 0.274 + 0.274 = 1.000$ â€” Softmaxã®è¦æ ¼åŒ–æ¡ä»¶ãŒæˆç«‹ã€‚å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒè‡ªåˆ†è‡ªèº«ã«æœ€å¤§ã®æ³¨æ„ï¼ˆ0.452ï¼‰ã‚’å‘ã‘ã€ä»–2ã¤ã«ç­‰åˆ†ï¼ˆ0.274ãšã¤ï¼‰ã—ã¦ã„ã‚‹æ§‹é€ ãŒèª­ã¿å–ã‚Œã‚‹ã€‚

### 3.4 MM-DiT (SD3) â€” Joint Attention

**MM-DiT ã®é©æ–°** [Esser+ 2024] [^3]:
- ç”»åƒãƒ‘ãƒƒãƒ $\mathbf{z}_{\text{img}}$ ã¨ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ $\mathbf{z}_{\text{txt}}$ ã‚’ **åŒã˜ Transformer** ã§å‡¦ç†
- Joint Self-Attention â€” ç”»åƒ â†” ãƒ†ã‚­ã‚¹ãƒˆã®ç›¸äº’ä½œç”¨

**æ•°å¼**:
$$
\begin{align}
\mathbf{z} &= [\mathbf{z}_{\text{img}}, \mathbf{z}_{\text{txt}}] \in \mathbb{R}^{(N_{\text{img}} + N_{\text{txt}}) \times D} \\
\mathbf{h} &= \text{AdaLN-Zero}(\mathbf{z}, \mathbf{c}) \\
\mathbf{a} &= \text{Attention}(\mathbf{h}) \quad \text{(joint attention)} \\
\mathbf{z}_{\text{img}}', \mathbf{z}_{\text{txt}}' &= \text{Split}(\mathbf{z} + \mathbf{a})
\end{align}
$$

**ãªãœ Joint Attentionï¼Ÿ**
- **Classifier-Free Guidance (CFG)** (ç¬¬39å›ã§å­¦ã‚“ã ) ã§ã¯ã€æ¡ä»¶ä»˜ã/ç„¡æ¡ä»¶ã‚’åˆ¥ã€…ã«å‡¦ç†
- **MM-DiT** ã§ã¯ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãŒ **åŒã˜æ½œåœ¨ç©ºé–“** ã§ç›¸äº’ä½œç”¨ â†’ ã‚ˆã‚Šå¼·ã„æ¡ä»¶ä»˜ã‘

#### Cross-Attention vs Joint Attention ã®æ•°å­¦çš„æ¯”è¼ƒ

**Cross-Attention**ï¼ˆå¾“æ¥ã®æ¡ä»¶ä»˜ã‘æ–¹å¼ï¼‰ï¼š

$$
\text{Attn}_{\text{cross}}(Q_{\text{img}}, K_{\text{txt}}, V_{\text{txt}}) = \text{softmax}\!\left(\frac{Q_{\text{img}} K_{\text{txt}}^\top}{\sqrt{d_k}}\right) V_{\text{txt}}
$$

ã“ã‚Œã¯ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒã®ä¸€æ–¹å‘ã®æƒ…å ±æµã ã€‚ç”»åƒãƒ‘ãƒƒãƒåŒå£«ã®é–¢ä¿‚ï¼ˆimgâ†”imgï¼‰ã¯åˆ¥ã®Self-Attentionã§è¨ˆç®—ã—ã€ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒå£«ã®é–¢ä¿‚ï¼ˆtxtâ†”txtï¼‰ã¯å®Œå…¨ã«ç„¡è¦–ã•ã‚Œã‚‹ã€‚ã¤ã¾ã‚Š**4ç¨®é¡ã®ç›¸äº’ä½œç”¨ã®ã†ã¡2ç¨®é¡ã—ã‹æ‰ãˆã‚‰ã‚Œãªã„**ã€‚

**MM-DiT ã®Joint Attention**ï¼šçµåˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³åˆ— $[Q_{\text{img}};\, Q_{\text{txt}}]$ ã‚’ä½¿ã†ï¼š

$$
\text{Attn}_{\text{joint}} = \text{softmax}\!\left(\frac{[Q_{\text{img}};\, Q_{\text{txt}}]\,[K_{\text{img}};\, K_{\text{txt}}]^\top}{\sqrt{d_k}}\right)[V_{\text{img}};\, V_{\text{txt}}]
$$

ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ— $A \in \mathbb{R}^{(N_{\text{img}} + N_{\text{txt}}) \times (N_{\text{img}} + N_{\text{txt}})}$ ã‚’ãƒ–ãƒ­ãƒƒã‚¯åˆ†è§£ã™ã‚‹ã¨ï¼š

$$
A = \begin{pmatrix} A_{\text{img}\to\text{img}} & A_{\text{img}\to\text{txt}} \\ A_{\text{txt}\to\text{img}} & A_{\text{txt}\to\text{txt}} \end{pmatrix}
$$

**4ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ãŒ1å›ã®è¡Œåˆ—ç©ã§åŒæ™‚ã«è¨ˆç®—ã•ã‚Œã‚‹**ï¼š
- $A_{\text{img}\to\text{img}}$ï¼šç”»åƒãƒ‘ãƒƒãƒé–“ã®ç©ºé–“çš„é–¢ä¿‚
- $A_{\text{img}\to\text{txt}}$ï¼šç”»åƒãŒãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ï¼ˆã€Œã©ã®å˜èªã«å¯¾å¿œã™ã‚‹ãƒ‘ãƒƒãƒã‹ã€ï¼‰
- $A_{\text{txt}\to\text{img}}$ï¼šãƒ†ã‚­ã‚¹ãƒˆãŒç”»åƒã‚’å‚ç…§ï¼ˆã€Œã“ã®å˜èªã¯ã©ã®ãƒ‘ãƒƒãƒã«å½±éŸ¿ã™ã‚‹ã‹ã€ï¼‰
- $A_{\text{txt}\to\text{txt}}$ï¼šãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³é–“ã®æ–‡è„ˆé–¢ä¿‚

Cross-Attentionã§ã¯ $A_{\text{txt}\to\text{txt}}$ ãŒå¾—ã‚‰ã‚Œãªã„ã€‚SD3ãŒT5-XXLã®æ–‡è„ˆè¡¨ç¾ã‚’ãã®ã¾ã¾æ´»ã‹ã›ã‚‹ã®ã¯ã€Joint AttentionãŒãƒ†ã‚­ã‚¹ãƒˆã®æ–‡è„ˆæƒ…å ±ã‚’å‹•çš„ã«æ›´æ–°ã—ç¶šã‘ã‚‹ã‹ã‚‰ã ã€‚

**SD3 ã®3ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**:
1. CLIP ViT-L/14 â€” ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆ align
2. CLIP ViT-bigG/14 â€” ã‚ˆã‚Šå¤§è¦æ¨¡ãª CLIP
3. T5-XXL â€” ãƒ†ã‚­ã‚¹ãƒˆç†è§£ (Google ã®è¨€èªãƒ¢ãƒ‡ãƒ«)

**ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®çµ±åˆ**:
$$
\mathbf{z}_{\text{txt}} = \text{Concat}([\text{CLIP-L}(\text{prompt}), \text{CLIP-G}(\text{prompt}), \text{T5}(\text{prompt})])
$$

**å…·ä½“ä¾‹**: SD3 Medium (2B params)
- $N_{\text{img}} = 4096$ â€” ç”»åƒãƒ‘ãƒƒãƒ (64Ã—64 latent / patch size 2)
- $N_{\text{txt}} = 256$ â€” ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³
- $D = 1536$ â€” hidden dim
- $L = 24$ â€” layers

**æ•°å€¤æ¤œè¨¼**:

SD3 Mediumã®å…·ä½“å€¤ã§Joint Attentionã®è¨ˆç®—é‡ã‚’è¦‹ç©ã‚‚ã‚‹ã€‚

$N_{\text{img}} = 4096$ã€$N_{\text{txt}} = 256$ã€$D = 1536$ ã®å ´åˆï¼š
$$
N_{\text{total}} = N_{\text{img}} + N_{\text{txt}} = 4096 + 256 = 4352
$$

Joint Attentionè¡Œåˆ—ã®ã‚µã‚¤ã‚ºï¼š
$$
N_{\text{total}}^2 = 4352^2 = 18{,}939{,}904 \approx 1.9 \times 10^7 \text{ è¦ç´ }
$$

1å±¤ã‚ãŸã‚Šã®FLOPsæ¦‚ç®—ï¼ˆ$QK^\top$ è¡Œåˆ—ç©ï¼‰ï¼š
$$
O_{\text{attn}} \approx N_{\text{total}}^2 \times D = 4352^2 \times 1536 \approx 2.9 \times 10^{10}
$$

å…¨24å±¤ã®åˆè¨ˆï¼š
$$
O_{\text{total}} \approx 24 \times 2.9 \times 10^{10} \approx 7.0 \times 10^{11} \text{ FLOPs}
$$

æ¯”è¼ƒ: ä»®ã« Cross-Attentionã®ã¿ãªã‚‰ $N_{\text{img}} \times N_{\text{txt}} = 4096 \times 256 = 1{,}048{,}576$ï¼ˆJoint Attentionã®ç´„5.5%ï¼‰ã€‚ã—ã‹ã— Cross-Attentionã§ã¯ç”»åƒãƒ‘ãƒƒãƒåŒå£«ã®é–¢ä¿‚ $N_{\text{img}}^2 = 1.68 \times 10^7$ ã‚’åˆ¥é€”è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ãƒˆãƒ¼ã‚¿ãƒ«ã§ã¯å¤‰ã‚ã‚‰ãªã„ã€‚Joint Attentionã¯4ç¨®é¡ã®ç›¸äº’ä½œç”¨ï¼ˆimgâ†”img, imgâ†”txt, txtâ†”img, txtâ†”txtï¼‰ã‚’1å›ã®Matmulã§åŒæ™‚ã«æ‰ãˆã‚‹ç‚¹ãŒåŠ¹ç‡çš„ã ã€‚

#### QK-Normalization â€” SD3 ã®å¤§è¦æ¨¡å®‰å®šåŒ–æŠ€è¡“

SD3 ã¯8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’å®‰å®šã—ã¦è¨“ç·´ã™ã‚‹ãŸã‚ **QK-Normalization** ã‚’æ¡ç”¨ã™ã‚‹ã€‚é€šå¸¸ã®Attentionã§ã¯ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯ $Q, K$ ã®ãƒãƒ«ãƒ ãŒè¨“ç·´ä¸­ã«çˆ†ç™ºçš„ã«å¢—å¤§ã—ã€$QK^\top / \sqrt{d_k}$ ã®ã‚¹ã‚³ã‚¢ãŒæ¥µç«¯ã«å¤§ãããªã£ã¦SoftmaxãŒé£½å’Œã™ã‚‹ã€‚QK-Normalizationã¯ Query ã¨ Key ãã‚Œãã‚Œã‚’ $\ell_2$ æ­£è¦åŒ–ã™ã‚‹ï¼š

$$
\hat{Q} = \frac{Q}{\|Q\|_2}, \quad \hat{K} = \frac{K}{\|K\|_2}
$$

$$
\text{Attention}_{\text{QKnorm}}(\hat{Q}, \hat{K}, V) = \text{softmax}\!\left(\frac{\hat{Q}\hat{K}^\top}{\sqrt{d_k}}\right) V
$$

$\hat{Q}, \hat{K}$ ã®ãƒãƒ«ãƒ ã¯å¸¸ã«1ãªã®ã§ã€å†…ç© $\hat{Q}\hat{K}^\top \in [-1, +1]$ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰ã«æŸç¸›ã•ã‚Œã‚‹ã€‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®ã‚¹ã‚³ã‚¢ã¯ $[-1/\sqrt{d_k}, +1/\sqrt{d_k}]$ ã®ç¯„å›²ã«åã¾ã‚Šã€è¨“ç·´å…¨ä½“ã‚’é€šã˜ã¦SoftmaxãŒé©åº¦ãªåˆ†å¸ƒã‚’ä¿ã¤ã€‚

SD3ã®å®Ÿé¨“ã§ã¯ QK-Normãªã—ã§ã¯ 500M params ç¨‹åº¦ã§è¨“ç·´ãŒä¸å®‰å®šåŒ–ã—å§‹ã‚ãŸã®ã«å¯¾ã—ã€QK-Normã‚ã‚Šã§ã¯ 8B paramsã¾ã§ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½ã ã£ãŸã¨å ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã¨ã£ã¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ•°å€¤å®‰å®šæ€§ã¯æ€§èƒ½ã¨åŒç­‰ã«é‡è¦ã ã€‚

### 3.5 SiT (Stochastic Interpolants) â€” Interpolant-based DiT

**SiT** [Ma+ 2024] [^8] ã¯ **Stochastic Interpolants** (ç¬¬38å›ã§å­¦ã‚“ã ) ã‚’ DiT ã«çµ±åˆã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚

**Stochastic Interpolant ã®å¾©ç¿’**:
$$
\mathbf{x}_t = \alpha(t) \mathbf{x}_0 + \beta(t) \mathbf{x}_1 + \gamma(t) \mathbf{z}
$$
- $\mathbf{x}_0 \sim p_0$ (ãƒã‚¤ã‚º)
- $\mathbf{x}_1 \sim p_1$ (ãƒ‡ãƒ¼ã‚¿)
- $\mathbf{z} \sim \mathcal{N}(0, I)$ (ç¢ºç‡çš„é …)
- $\alpha(0) = 1, \alpha(1) = 0$ â€” ãƒã‚¤ã‚ºâ†’ãƒ‡ãƒ¼ã‚¿

**SiT ã®æå¤±é–¢æ•°**:
$$
\mathcal{L}_{\text{SiT}} = \mathbb{E}_{t, \mathbf{x}_0, \mathbf{x}_1, \mathbf{z}} \left[\left\| \mathbf{v}_\theta(\mathbf{x}_t, t) - \dot{\mathbf{x}}_t \right\|^2\right]
$$
- $\dot{\mathbf{x}}_t = \alpha'(t) \mathbf{x}_0 + \beta'(t) \mathbf{x}_1 + \gamma'(t) \mathbf{z}$ â€” ãƒ™ã‚¯ãƒˆãƒ«å ´ã®çœŸå€¤

**SiT vs Flow Matching**:
- **Flow Matching** (ç¬¬38å›): $\gamma(t) = 0$ â€” æ±ºå®šè«–çš„è£œé–“
- **SiT**: $\gamma(t) > 0$ â€” ç¢ºç‡çš„è£œé–“ (ãƒã‚¤ã‚ºé …ã‚ã‚Š)

**ãªãœç¢ºç‡çš„ï¼Ÿ**
- **å¤šæ§˜æ€§å‘ä¸Š**: åŒã˜ $\mathbf{x}_1$ ã‹ã‚‰ç•°ãªã‚‹ç”ŸæˆçµŒè·¯ã‚’æ¢ç´¢
- **ãƒ¢ãƒ¼ãƒ‰å´©å£Šå›é¿**: Flow Matching ã®æ±ºå®šè«–æ€§ãŒåŸå› ã®ãƒ¢ãƒ¼ãƒ‰å´©å£Šã‚’ç·©å’Œ

#### SiT ã® Fokker-Planck æ–¹ç¨‹å¼

SiT ãŒç¢ºç‡éç¨‹ã§ã‚ã‚‹ã“ã¨ã‚’å³å¯†ã«ç¤ºã™ã€‚Stochastic Interpolant ã¯æ¬¡ã®ç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼ï¼ˆSDEï¼‰ã«å¯¾å¿œã™ã‚‹ï¼š

$$
d\mathbf{x}_t = \mathbf{b}(\mathbf{x}_t, t)\, dt + \sigma(t)\, d\mathbf{W}_t
$$

ã“ã“ã§ $\mathbf{W}_t$ ã¯æ¨™æº–ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã€$\sigma(t) = \gamma'(t)$ ã¯æ‹¡æ•£ä¿‚æ•°ã€‚å¯¾å¿œã™ã‚‹ Fokker-Planck æ–¹ç¨‹å¼ï¼ˆç¢ºç‡å¯†åº¦ $\rho_t$ ã®æ™‚é–“ç™ºå±•ï¼‰ï¼š

$$
\frac{\partial \rho_t}{\partial t} = -\nabla \cdot (\mathbf{b}\, \rho_t) + \frac{\sigma(t)^2}{2} \Delta \rho_t
$$

$\gamma(t) = 0$ï¼ˆFlow Matchingï¼‰ã®ã¨ã $\sigma = 0$ ã¨ãªã‚Š Fokker-Planck ã¯é€£ç¶šæ–¹ç¨‹å¼ $\partial_t \rho_t = -\nabla \cdot (\mathbf{b}\, \rho_t)$ ã«é€€åŒ–ã™ã‚‹ â€” ã“ã‚ŒãŒODEçµŒè·¯ã®ç›´ç·šæ€§ã®æœ¬è³ªã ã€‚$\gamma > 0$ ã§ã¯æ‹¡æ•£é … $(\sigma^2/2) \Delta \rho_t$ ãŒåŠ ã‚ã‚Šã€ç¢ºç‡å¯†åº¦ãŒã€ŒåºƒãŒã‚‹ã€æ–¹å‘ã«å‹•ãã€‚ã“ã®åºƒãŒã‚ŠãŒå¤šæ§˜æ€§å‘ä¸Šã‚’ã‚‚ãŸã‚‰ã™ä¸€æ–¹ã€è»Œé“ã®æ›²ç‡ï¼ˆcurvatureï¼‰ãŒå¢—åŠ ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«å¤šãã®NFEãŒå¿…è¦ã«ãªã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚‹ã€‚

**DDPM ã¨ã®çµ±ä¸€çš„ç†è§£**: $\alpha(t) = \sqrt{\bar{\alpha}_t}$ã€$\beta(t) = 0$ã€$\gamma(t) = \sqrt{1 - \bar{\alpha}_t}$ ã¨è¨­å®šã™ã‚‹ã¨ Stochastic Interpolant ã¯ DDPM ã®å‰å‘ãéç¨‹ã¨ä¸€è‡´ã™ã‚‹ã€‚$\alpha(t) = 1-t$ã€$\beta(t) = t$ã€$\gamma(t) = 0$ ã§ã¯ Flow Matchingï¼ˆç›´ç·šè£œé–“ï¼‰ã€‚SiT ã¯ä¸¡è€…ã‚’ **åŒä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯** ã¨ã—ã¦å†…åŒ…ã™ã‚‹ã€‚

#### ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ã®æ§‹æˆ

DiT ã¯ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— $t \in [0, T]$ ã‚’æ¡ä»¶ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{t} \in \mathbb{R}^{D_t}$ ã«å¤‰æ›ã—ã¦AdaLN-Zeroã«æ¸¡ã™ã€‚ã“ã®å¤‰æ›ã¯ **Sinusoidal Embedding + 2å±¤MLP** ã§è¡Œã‚ã‚Œã‚‹ï¼š

$$
\mathbf{t}^{\text{sin}} = \left[\sin\!\left(\frac{t}{10000^{2k/D_t}}\right), \cos\!\left(\frac{t}{10000^{2k/D_t}}\right)\right]_{k=0}^{D_t/2 - 1} \in \mathbb{R}^{D_t}
$$

$$
\mathbf{t} = \text{MLP}(\mathbf{t}^{\text{sin}}) \in \mathbb{R}^{D_t}
$$

$D_t = 256$ï¼ˆDiT-Bï¼‰ã®å ´åˆã€å„æ¬¡å…ƒãŒç•°ãªã‚‹å‘¨æ³¢æ•°ã§ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç¬¦å·åŒ–ã—ã€MLPãŒéç·šå½¢ã«çµ„ã¿åˆã‚ã›ã‚‹ã€‚ã“ã®åŸ‹ã‚è¾¼ã¿ãŒ $\gamma(\mathbf{c})$ ãŠã‚ˆã³ $\beta(\mathbf{c})$ ã®MLPå…¥åŠ›ã«çµåˆã•ã‚Œã‚‹ã“ã¨ã§ã€ã€Œ$t = 0.1$ï¼ˆã»ã¼ãƒ‡ãƒ¼ã‚¿ï¼‰ã¨ $t = 0.9$ï¼ˆã»ã¼ãƒã‚¤ã‚ºï¼‰ã§ã¯å…¨ãç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ãƒ»ã‚·ãƒ•ãƒˆã‚’é©ç”¨ã™ã‚‹ã€ã¨ã„ã†æ¡ä»¶ä»˜ãæŒ™å‹•ãŒå®Ÿç¾ã™ã‚‹ã€‚

**æ•°å€¤æ¤œè¨¼**:

ç·šå½¢è£œé–“ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\alpha(t) = 1 - t$ã€$\beta(t) = t$ï¼ˆFlow Matchingã¨åŒå½¢ï¼‰ã§ $t = 0.5$ ã‚’è¨ˆç®—ã™ã‚‹ã€‚

$\mathbf{x}_0 = [2.0,\; 0.0]$ï¼ˆãƒã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒ«ï¼‰ã€$\mathbf{x}_1 = [0.0,\; 2.0]$ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã®ã¨ãï¼š

è£œé–“ä¿‚æ•°ï¼š
$$
\alpha(0.5) = 1 - 0.5 = 0.5, \quad \beta(0.5) = 0.5
$$

ä¸­é–“ç‚¹ï¼ˆç¢ºç‡çš„é … $\gamma = 0$ ã®å ´åˆï¼‰ï¼š
$$
\mathbf{x}_{0.5} = 0.5 \times [2.0,\; 0.0] + 0.5 \times [0.0,\; 2.0] = [1.0,\; 1.0]
$$

é€Ÿåº¦å ´ï¼ˆçœŸå€¤ã€$\alpha'(t) = -1$ã€$\beta'(t) = 1$ï¼‰ï¼š
$$
\dot{\mathbf{x}}_t = \alpha'(t)\, \mathbf{x}_0 + \beta'(t)\, \mathbf{x}_1 = (-1) \times [2.0,\; 0.0] + 1 \times [0.0,\; 2.0] = [-2.0,\; 2.0]
$$

ãƒãƒ«ãƒ ï¼š
$$
\|\dot{\mathbf{x}}_t\| = \sqrt{(-2.0)^2 + 2.0^2} = \sqrt{8} = 2\sqrt{2} \approx 2.83
$$

ç·šå½¢è£œé–“ã§ã¯ $\|\dot{\mathbf{x}}_t\| = \|\mathbf{x}_1 - \mathbf{x}_0\|$ ãŒ $t$ ã«ã‚ˆã‚‰ãšä¸€å®š â€” ã“ã‚ŒãŒ **ODEçµŒè·¯ã®ç›´ç·šæ€§**ã‚’ä¿è¨¼ã—ã€å°‘ãªã„NFEï¼ˆNumber of Function Evaluationsï¼‰ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãã‚‹ç†ç”±ã ã€‚ç¢ºç‡çš„è£œé–“ï¼ˆ$\gamma > 0$ï¼‰ã§ã¯é€Ÿåº¦å ´ã«ãƒã‚¤ã‚ºãŒåŠ ã‚ã‚Š $\|\dot{\mathbf{x}}_t\|$ ã¯æ™‚å¤‰ã«ãªã‚‹ãŒã€å¤šæ§˜æ€§å‘ä¸Šã¨ã„ã†ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚‹ã€‚

### 3.6 U-Net vs DiT ã®è¨ˆç®—é‡æ¯”è¼ƒ

**U-Net ã®è¨ˆç®—é‡** (ç©ºé–“ãƒ™ãƒ¼ã‚¹):
$$
O_{\text{U-Net}} = O(H \times W \times C^2 \times L)
$$
- $H, W$ â€” ç”»åƒã‚µã‚¤ã‚º
- $C$ â€” ãƒãƒ£ãƒãƒ«æ•°
- $L$ â€” ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°

**DiT ã®è¨ˆç®—é‡** (ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹):
$$
O_{\text{DiT}} = O(N^2 \times D \times L)
$$
- $N = \frac{H}{P} \times \frac{W}{P}$ â€” ãƒˆãƒ¼ã‚¯ãƒ³æ•°
- $D$ â€” hidden dim
- $L$ â€” ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°

**å…·ä½“ä¾‹**: 256Ã—256 ç”»åƒ
- U-Net: $H = W = 256$, $C = 256$, $L = 20$
  - $O_{\text{U-Net}} = 256 \times 256 \times 256^2 \times 20 \approx 8.5 \times 10^{11}$
- DiT: $P = 16$, $N = 256$, $D = 768$, $L = 12$
  - $O_{\text{DiT}} = 256^2 \times 768 \times 12 \approx 6.0 \times 10^{11}$

**çµè«–**: DiT ã¯ U-Net ã¨ **åŒç¨‹åº¦ã®è¨ˆç®—é‡** ã§ã€ã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’é”æˆã™ã‚‹ (Scaling Laws ã®æ©æµ)ã€‚

**ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º $P$ ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**: $P$ ã‚’å°ã•ãã™ã‚‹ã¨ $N$ ãŒå¢—ãˆã¦è¡¨ç¾åŠ›ãŒå‘ä¸Šã™ã‚‹ãŒè¨ˆç®—é‡ã¯ $N^2 \propto P^{-4}$ ã§å¢—å¤§ã™ã‚‹ã€‚$P$ ã‚’2å€ã«ã™ã‚‹ã¨è¨ˆç®—é‡ã¯16åˆ†ã®1ã«ãªã‚‹ãŒè§£åƒåº¦ãŒè½ã¡ã‚‹ã€‚DiT-XL/2 ã® $P=2$ï¼ˆæ½œåœ¨ç©ºé–“ã§ã®è¨ˆç®—ãªã®ã§å®Ÿè³ªçš„ã« $P=2 \times 8 = 16$ ãƒ”ã‚¯ã‚»ãƒ«ç›¸å½“ï¼‰ã¯ã“ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®æœ€è‰¯ç‚¹ã¨ã—ã¦å®Ÿé¨“çš„ã«é¸ã°ã‚ŒãŸå€¤ã ã€‚

å›ºå®šã®è¨ˆç®—äºˆç®— $C$ï¼ˆFLOPsï¼‰ã®ã‚‚ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ ã¨ãƒ‡ãƒ¼ã‚¿é‡ $D$ ã‚’ã©ã†é…åˆ†ã™ã‚Œã°æå¤±ãŒæœ€å°åŒ–ã§ãã‚‹ã‹ï¼Ÿã“ã‚ŒãŒã„ã‚ã‚†ã‚‹ Chinchilla å•é¡Œã ã€‚

DiTã«ãŠã‘ã‚‹è¨ˆç®—é‡ã¨ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãƒ»ãƒ‡ãƒ¼ã‚¿é‡ã®é–¢ä¿‚ï¼š
$$
C \approx 6 \cdot N \cdot D \quad \text{ï¼ˆè¨“ç·´FLOPsã®è¿‘ä¼¼å¼ï¼‰}
$$

æå¤±ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼š
$$
\mathcal{L}(N, D) = \left(\frac{N_c}{N}\right)^{\alpha_N} + \left(\frac{D_c}{D}\right)^{\alpha_D} + \mathcal{L}_\infty
$$

$C$ ã‚’å›ºå®šã—ã¦ $\mathcal{L}$ ã‚’æœ€å°åŒ–ã™ã‚‹æœ€é©è§£ã¯ Lagrange ä¹—æ•°æ³•ã§æ±‚ã¾ã‚‹ï¼š

$$
N^*(C) \propto C^{\frac{\alpha_D}{\alpha_N + \alpha_D}}, \quad D^*(C) \propto C^{\frac{\alpha_N}{\alpha_N + \alpha_D}}
$$

DiT ã®å®Ÿé¨“çš„æ¨å®š [Zhai+ 2024] ã§ã¯ $\alpha_N \approx \alpha_D$ï¼ˆãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ãŒç­‰é…åˆ†ã«è¿‘ã„ï¼‰ã€‚ã“ã‚Œã¯ã€ŒåŒã˜FLOPsãªã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’2å€å¤§ããã™ã‚‹ã‹ã€ãƒ‡ãƒ¼ã‚¿ã‚’2å€å¢—ã‚„ã™ã‹ã€ã©ã¡ã‚‰ã‚‚åŒã˜åŠ¹æœãŒã‚ã‚‹ã€ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚U-Netã§ã¯ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ©æµãŒ Transformer ã‚ˆã‚Šå°ã•ã„ï¼ˆå¸°ç´ãƒã‚¤ã‚¢ã‚¹ãŒãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã‚’æ‚ªåŒ–ã•ã›ã‚‹ï¼‰ãŸã‚ã€DiTã®ç­‰é…åˆ†åŸç†ã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å„ªä½æ€§ã®ã‚‚ã†ä¸€ã¤ã®è¨¼æ‹ ã ã€‚

### 3.6b DiT ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¤‡é›‘åº¦ã¨ FlashAttention

DiT-XL ã® $N = 256$ï¼ˆãƒ‘ãƒƒãƒã‚µã‚¤ã‚º16ã€256Ã—256ç”»åƒï¼‰ã§ã¯ã€1å±¤ã®Attentionè¡Œåˆ—ã¯ï¼š

$$
A \in \mathbb{R}^{256 \times 256}, \quad \text{è¦ç´ æ•°} = 256^2 = 65{,}536
$$

ãƒ˜ãƒƒãƒ‰æ•° $H = 16$ ã‚’å«ã‚ã‚‹ã¨ï¼š
$$
65{,}536 \times 16 = 1{,}048{,}576 \approx 10^6 \text{ è¦ç´ /å±¤}
$$

Naive Attentionã®è¨ˆç®—é‡ã¨ç©ºé–“è¨ˆç®—é‡ï¼š
$$
O_{\text{compute}} = O(N^2 d), \quad O_{\text{memory}} = O(N^2)
$$

$N = 256$ ã§ã¯ Attentionè¡Œåˆ—ã®ãƒ¡ãƒ¢ãƒªã¯å•é¡Œãªã„ãŒã€é«˜è§£åƒåº¦ï¼ˆSD3: $N_{\text{img}} = 4096$ï¼‰ã§ã¯ $4096^2 = 16{,}777{,}216$ è¦ç´  Ã— float16 = **32MB/å±¤** ã«è†¨ã‚‰ã‚€ã€‚24å±¤ã§ã¯ 768MB â€” VRAMå®¹é‡ã«ç›´æ’ƒã™ã‚‹ã€‚

**FlashAttention** [Dao et al., 2022] ã¯ SRAM ã®ã‚ªãƒ³ãƒãƒƒãƒ—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨ã—ã¦Attentionè¡Œåˆ—ã‚’é™½ã«ä½œã‚‰ãšã«è¨ˆç®—ã™ã‚‹ï¼š

$$
O_{\text{memory}}^{\text{Flash}} = O(N \cdot d) \quad \text{ï¼ˆç·šå½¢ãƒ¡ãƒ¢ãƒªï¼‰}
$$

ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ ¸å¿ƒã¯ **ã‚¿ã‚¤ãƒ«åŒ–ï¼ˆtilingï¼‰**: $N \times N$ ã®Attentionè¡Œåˆ—ã‚’ $B_r \times B_c$ ã®ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã—ã€å„ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMå†…ã§å®Œçµã•ã›ã‚‹ã€‚HBMï¼ˆGPU DRAMï¼‰ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹å›æ•°ã¯ï¼š
$$
O\!\left(\frac{N^2 d}{\text{SRAM\_size}}\right) \quad \text{ï¼ˆSRAM\_size} \approx 20\text{MB on A100ï¼‰}
$$

è¨ˆç®—é‡ã¯åŒã˜ $O(N^2 d)$ ã ãŒã€**HBMã‚¢ã‚¯ã‚»ã‚¹ãŒç´„10-20å€å‰Šæ¸›**ã•ã‚Œã€å®ŸåŠ¹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒ2-4å€å‘ä¸Šã™ã‚‹ã€‚DiT-XL/2ã®è¨“ç·´ã«ãŠã„ã¦ FlashAttention ã¯éã‚ªãƒ—ã‚·ãƒ§ãƒ³ â€” æ¨™æº–çš„ãªå®Ÿè£…ã§ã¯ä½¿ã‚ãªã„é¸æŠè‚¢ãŒãªã„ã€‚

SD3ãƒ»FLUXã®ã‚ˆã†ãª $N_{\text{total}} = 4352$ ã®MM-DiTã§ã¯ã€Attentionè¡Œåˆ— $\approx 145\text{MB/å±¤}$ ãŒFlashAttentionãªã—ã«ã¯æˆç«‹ã—ãªã„ã€‚ã“ã‚ŒãŒ2024å¹´ä»¥é™ã®å¤§è¦æ¨¡DiTè¨“ç·´ãŒFlashAttention v2å‰æã§è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ç†ç”±ã ã€‚

**FlashAttention ã® online softmax trick**: æ¨™æº–çš„ãªSoftmaxè¨ˆç®—ã¯2ãƒ‘ã‚¹ãŒå¿…è¦ã ï¼ˆ1ãƒ‘ã‚¹ç›®ã§æœ€å¤§å€¤ã‚’æ±‚ã‚ã€2ãƒ‘ã‚¹ç›®ã§æŒ‡æ•°å’Œã‚’è¨ˆç®—ï¼‰ã€‚FlashAttentionã¯ **online softmax** ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ã£ã¦1ãƒ‘ã‚¹ã§å®Œçµã•ã›ã‚‹ï¼š

ã‚¿ã‚¤ãƒ« $i$ ã®å‡¦ç†ä¸­ã«ç¾åœ¨ã®æœ€å¤§å€¤ $m_i$ ã¨ç´¯ç©å’Œ $\ell_i$ ã‚’ä¿æŒï¼š
$$
m_i^{\text{new}} = \max(m_i,\; \max(\mathbf{s}_{\text{new}})), \quad \ell_i^{\text{new}} = e^{m_i - m_i^{\text{new}}} \ell_i + \sum e^{\mathbf{s}_{\text{new}} - m_i^{\text{new}}}
$$

ã“ã®æ¼¸åŒ–å¼ã«ã‚ˆã‚Šã€Attentionè¡Œåˆ—ã‚’é™½ã«ä¿æŒã›ãš O(N) ãƒ¡ãƒ¢ãƒªã§Softmaxã‚’è¨ˆç®—ã§ãã‚‹ã€‚æ•°å€¤å®‰å®šæ€§ï¼ˆ$m_i$ ã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢ï¼‰ã¨è¨ˆç®—åŠ¹ç‡ã‚’åŒæ™‚ã«é”æˆã—ãŸ**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„å‚‘ä½œ**ã ã€‚

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã®æ¯”è¼ƒï¼šDiT vs U-Net

åŒã˜FIDã‚’é”æˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æ¯”è¼ƒã™ã‚‹ã¨ DiT ã®å„ªä½æ€§ãŒè¦‹ãˆã‚‹ã€‚

FID 4.0 ç¨‹åº¦ã‚’é”æˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒï¼š

| ãƒ¢ãƒ‡ãƒ« | ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ | Params | FID | GFLOPS/forward |
|:-------|:------------|:-------|:----|:--------------|
| LDM-4 (DDPM) | U-Net | 400M | 3.60 | 103 |
| ADM [Dhariwal+ 2021] | U-Net | 554M | 4.59 | 742 |
| **DiT-XL/2** | Transformer | **675M** | **2.27** | **118** |

DiT-XL/2 ã¯ ADM ã‚ˆã‚Šå°‘ãªã„ FLOPs ã§ã‚ˆã‚Šé«˜ã„ FID ã‚’é”æˆã™ã‚‹ã€‚ADMã¯ Attention ã‚’ U-Net ã«è¿½åŠ ã—ãŸç‰¹æ®Šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã ãŒã€ãã‚Œã§ã‚‚ Transformer ãƒ™ãƒ¼ã‚¹ã® DiT ã«åŠã°ãªã„ã€‚

**ãªãœ DiT ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ãŒè‰¯ã„ã®ã‹ï¼Ÿ** Transformer ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å…¨ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã§å…±æœ‰ã•ã‚Œã‚‹ï¼ˆé‡ã¿å…±æœ‰ã¯ãªã„ãŒã€ä½ç½®ã«ä¾å­˜ã—ãªã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰ã€‚å„ DiT ãƒ–ãƒ­ãƒƒã‚¯ã¯ã€Œ$N$ ãƒ‘ãƒƒãƒå…¨ã¦ã«åŒã˜é‡ã¿ã‚’é©ç”¨ã€ã™ã‚‹ãŸã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä½ç½®ã«ç„¡é§„ãªãä½¿ã‚ã‚Œã‚‹ã€‚U-Net ã® Conv ã¯å„å±¤ã§ç•°ãªã‚‹ç©ºé–“è§£åƒåº¦ã«ç‰¹åŒ–ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¡ã€ä½è§£åƒåº¦å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯é«˜è§£åƒåº¦æƒ…å ±ã‚’æ‰±ãˆãªã„ â€” ã“ã‚ŒãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã®å·®ã®æœ¬è³ªã ã€‚

**Challenge**: DiT ã® Forward Pass ã‚’1è¡Œãšã¤å®Ÿè£…ã—ã€ãƒã‚¤ã‚ºäºˆæ¸¬ã¾ã§å®Œèµ°ã™ã‚‹ã€‚

**Forward Pass ã®å…¨ã‚¹ãƒ†ãƒƒãƒ—**:
1. Patchify â€” ç”»åƒ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åˆ—
2. Positional Encoding
3. $L$ å±¤ã® DiT ãƒ–ãƒ­ãƒƒã‚¯
4. Unpatchify â€” ãƒˆãƒ¼ã‚¯ãƒ³åˆ— â†’ ç”»åƒ
5. ãƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta(\mathbf{x}_t, t)$

**æ•°å¼**:
$$
\begin{align}
\mathbf{z}_0 &= \text{Patchify}(\mathbf{x}) + \text{PE} \\
\mathbf{z}_\ell &= \text{DiT-Block}_\ell(\mathbf{z}_{\ell-1}, \mathbf{c}) \quad (\ell = 1, \ldots, L) \\
\mathbf{x}_{\text{pred}} &= \text{Unpatchify}(\mathbf{z}_L) \\
\epsilon_\theta(\mathbf{x}_t, t) &= \mathbf{x}_{\text{pred}}
\end{align}
$$

**å®Œå…¨å®Ÿè£…**:


**ãƒœã‚¹æ’ƒç ´ï¼** DiT ã® Forward Pass ã‚’å®Œå…¨å®Ÿè£…ã—ãŸã€‚Patchify â†’ DiT Blocks â†’ Unpatchify ã®æµã‚Œã§ã€ç”»åƒã‹ã‚‰ãƒã‚¤ã‚ºäºˆæ¸¬ã¾ã§è¾¿ã‚Šç€ã„ãŸã€‚

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®50%å®Œäº†ï¼** æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œèµ°ã€‚DiTãƒ»MM-DiTãƒ»SiT ã®æ•°å¼ã‚’å®Œå…¨å°å‡ºã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã‚¾ãƒ¼ãƒ³ â€” Rust/Rust/Elixir ã§ DiT ã‚’å‹•ã‹ã™ã€‚

### 3.7 Scaling Laws for Diffusion Transformers

**è«–æ–‡**: Zhai et al., "Scaling Laws For Diffusion Transformers," arXiv:2410.08184, 2024[^1]

Transformerãƒ™ãƒ¼ã‚¹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã§ã¯**Scaling Laws**ãŒç¢ºç«‹ã•ã‚Œã¦ã„ã‚‹:

$$
\mathcal{L}(N, D, C) = \left(\frac{N_c}{N}\right)^{\alpha_N} + \left(\frac{D_c}{D}\right)^{\alpha_D} + \left(\frac{C_c}{C}\right)^{\alpha_C}
$$

ã“ã“ã§$N$ã¯ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€$D$ã¯ãƒ‡ãƒ¼ã‚¿é‡ã€$C$ã¯è¨ˆç®—é‡ã€$\mathcal{L}$ã¯æå¤±é–¢æ•°ã€‚

**å•ã„**: **Diffusion Transformersã§ã‚‚åŒã˜Scaling LawsãŒæˆç«‹ã™ã‚‹ã‹ï¼Ÿ**

#### 3.7.1 å®Ÿé¨“çš„æ¤œè¨¼

**å®Ÿé¨“è¨­å®š**:
- è¨ˆç®—äºˆç®—: $10^{17}$ FLOPs ~ $6 \times 10^{18}$ FLOPsï¼ˆ1000å€ã®ç¯„å›²ï¼‰
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: 33M ~ 675M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: ImageNet 256Ã—256ï¼ˆ130ä¸‡ç”»åƒï¼‰
- è©•ä¾¡æŒ‡æ¨™: FIDï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰

**ç™ºè¦‹ã•ã‚ŒãŸ Scaling Law**:

$$
\text{FID}(C) = A \cdot C^{-\beta} + \text{FID}_{\infty}
$$

ã“ã“ã§:
- $C$: è¨ˆç®—é‡ï¼ˆFLOPsï¼‰
- $A, \beta$: ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- $\text{FID}_{\infty}$: ç„¡é™è¨ˆç®—æ™‚ã®ç†è«–çš„é™ç•Œ

**å®Ÿæ¸¬å€¤**: $\beta \approx 0.27$ï¼ˆLLMã®$\beta \approx 0.3$ã¨è¿‘ã„ï¼‰

**é‡è¦ãªæ´å¯Ÿ**: DiTã®è¨“ç·´æå¤±ï¼ˆMSEï¼‰ã¯è¨ˆç®—é‡$C$ã«å¯¾ã—ã¦**ã¹ãä¹—å‰‡**ã«å¾“ã†:

$$
\mathcal{L}_{\text{MSE}}(C) = B \cdot C^{-\gamma}
$$

$\gamma \approx 0.12$ï¼ˆå®Ÿé¨“çš„ã«æ±ºå®šï¼‰ã€‚

#### 3.7.2 æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®äºˆæ¸¬

Scaling Lawã‹ã‚‰ã€**ä¸ãˆã‚‰ã‚ŒãŸè¨ˆç®—äºˆç®—$C$ã«å¯¾ã™ã‚‹æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º$N^*$**ã‚’äºˆæ¸¬ã§ãã‚‹:

$$
N^*(C) = \left(\frac{\alpha_N}{\alpha_C}\right)^{\frac{1}{\alpha_C - \alpha_N}} \cdot C^{\frac{\alpha_C}{\alpha_C - \alpha_N}}
$$

**å®Ÿä¾‹**: è¨ˆç®—äºˆç®—$C = 10^{21}$ FLOPsã®å ´åˆ:

$$
N^* \approx 1.2 \times 10^9 \text{ parameters (1.2B)}
$$

**ãƒ‡ãƒ¼ã‚¿é‡ã®æœ€é©åŒ–**:

$$
D^*(C) = \left(\frac{\alpha_D}{\alpha_C}\right)^{\frac{1}{\alpha_C - \alpha_D}} \cdot C^{\frac{\alpha_C}{\alpha_C - \alpha_D}}
$$

$C = 10^{21}$ FLOPsã§$D^* \approx 50$Mç”»åƒã€‚

#### 3.7.3 Î¼P Scaling

**è«–æ–‡**: Xu et al., "Scaling Diffusion Transformers Efficiently via $\mu$P," arXiv:2505.15270, 2025[^2]

**å•é¡Œ**: æ¨™æº–çš„ãªScalingï¼ˆStandard Parameterization, SPï¼‰ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å¤‰ãˆã‚‹ãŸã³ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’ç‡$\eta$ã€åˆæœŸåŒ–$\sigma$ï¼‰ã‚’å†èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

**$\mu$P (Maximal Update Parameterization) ã®è§£æ±ºç­–**:

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®**å¹…$d$ã«å¿œã˜ãŸé©å¿œçš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**ã‚’å°å…¥:

$$
\begin{aligned}
\text{Weight initialization: } & W \sim \mathcal{N}(0, \frac{1}{d_{\text{in}}}) \\
\text{Learning rate: } & \eta_{\text{layer}} = \frac{\eta_{\text{base}}}{d_{\text{hidden}}} \\
\text{Output scaling: } & y = \frac{1}{\sqrt{d}} W x
\end{aligned}
$$

**åˆ©ç‚¹**: å°ã•ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆe.g., 100Mï¼‰ã§æœ€é©åŒ–ã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã€å¤§ãã„ãƒ¢ãƒ‡ãƒ«ï¼ˆe.g., 1Bï¼‰ã«ãã®ã¾ã¾è»¢ç§»å¯èƒ½ï¼

**å®Ÿé¨“çµæœ**:
- SP: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã”ã¨ã«å­¦ç¿’ç‡ã‚’èª¿æ•´ã—ãªã„ã¨ç™ºæ•£
- $\mu$P: åŒã˜å­¦ç¿’ç‡ã§100M â†’ 10Bã¾ã§ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½

**å®Ÿè£…ï¼ˆRustæ¦‚å¿µã‚³ãƒ¼ãƒ‰ï¼‰**:


#### Î¼P ã®ç†è«–çš„èƒŒæ™¯: ç„¡é™å¹…æ¥µé™ã¨ç‰¹å¾´å­¦ç¿’

Î¼P ã®æ•°å­¦çš„æ ¹æ‹ ã¯**ç„¡é™å¹…æ¥µé™**ï¼ˆinfinite width limitï¼‰ã«ã‚ã‚‹ã€‚æ¨™æº–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ï¼ˆSPï¼‰ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¹… $d \to \infty$ ã®æ¥µé™ã‚’å–ã‚‹ã¨ã€å„å±¤ã®å‡ºåŠ›ã¯**ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆGPï¼‰**ã«åæŸã—ã€ç‰¹å¾´ãŒå­¦ç¿’ã•ã‚Œãªã„ï¼ˆNeural Tangent Kernel ä½“åˆ¶ï¼‰ã€‚ã“ã‚Œã¯è¨“ç·´ãŒã€Œã‚«ãƒ¼ãƒãƒ«æ³•ã®æ¨¡å€£ã€ã«é€€åŒ–ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

Î¼P ã§ã¯ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹: å¹… $d$ ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã‚¹ã‚±ãƒ¼ãƒ«ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€$d \to \infty$ ã§ã‚‚å„å±¤ãŒ**æœ‰æ„ç¾©ãªç‰¹å¾´ã‚’å­¦ç¿’ã—ç¶šã‘ã‚‹**ï¼ˆMaximum Update Parameterization ã®ç”±æ¥ï¼‰ã€‚

ã‚­ãƒ¼ã¨ãªã‚‹æ¡ä»¶:
- **å…¥åŠ›å±¤**: é‡ã¿ã¯ $\mathcal{N}(0, 1/d_{\text{in}})$ ã§åˆæœŸåŒ–ï¼ˆSP ã¨åŒã˜ï¼‰
- **éš ã‚Œå±¤**: é‡ã¿ã¯ $\mathcal{N}(0, 1/d_{\text{hidden}})$ã€å­¦ç¿’ç‡ã¯ $\eta / d_{\text{hidden}}$
- **å‡ºåŠ›å±¤**: $1/\sqrt{d}$ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è¿½åŠ 

ã“ã®è¨­å®šä¸‹ã§ã¯ã€å¹… $d = 100$ ã®ãƒ¢ãƒ‡ãƒ«ã§æœ€é©åŒ–ã—ãŸå­¦ç¿’ç‡ $\eta^*$ ãŒ $d = 10{,}000$ ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚‚æœ€é©ã§ã‚ã‚Šç¶šã‘ã‚‹ã“ã¨ãŒ**ç†è«–çš„ã«è¨¼æ˜**ã•ã‚Œã¦ã„ã‚‹ã€‚DiT ã«é©ç”¨ã™ã‚‹ã¨ã€small-scaleï¼ˆ$\sim$30Mï¼‰ã® proxy ãƒ¢ãƒ‡ãƒ«ã§ optimal learning rate ã‚µãƒ¼ãƒã‚’è¡Œã„ã€ãã®å€¤ã‚’å¤§è¦æ¨¡ï¼ˆ$\sim$1Bï¼‰ãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥è»¢ç§»ã§ãã‚‹ â€” è¨“ç·´ã‚³ã‚¹ãƒˆãŒç†è«–ä¸Š 100 å€ä»¥ä¸Šå‰Šæ¸›ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

**è«–æ–‡**: Lu et al., "DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models," arXiv:2211.01095, 2023[^3]

#### 3.8.1 å•é¡Œè¨­å®š

DDPMã¯1000ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ â†’ å®Ÿç”¨ä¸å¯ã€‚é«˜é€ŸåŒ–ã®2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:

1. **è’¸ç•™ç³»**: Consistency Modelsï¼ˆç¬¬40å›ï¼‰ã€Progressive Distillation
2. **ODE Solverç³»**: DDIMã€DPM-Solverã€DPM-Solver++ï¼ˆæœ¬ç¯€ï¼‰

**èª²é¡Œ**: Classifier-Free Guidance (CFG) ä½¿ç”¨æ™‚ã€æ¨™æº–çš„ãªé«˜é€ŸsolverãŒä¸å®‰å®šåŒ–ã€‚

#### 3.8.2 DPM-Solver++ã®æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢

**Diffusion ODE** (ç¬¬37å›ã§å°å‡º):

$$
\frac{dx_t}{dt} = f(t) x_t + \frac{g(t)^2}{2\sigma_t} \epsilon_\theta(x_t, t)
$$

**CFGã®å•é¡Œ**: ãƒã‚¤ã‚ºäºˆæ¸¬$\epsilon_\theta$ãŒæ¡ä»¶ä»˜ãã¨ç„¡æ¡ä»¶ã®ç·šå½¢çµåˆ:

$$
\tilde{\epsilon}_\theta(x_t, t, c) = (1 + w) \epsilon_\theta(x_t, t, c) - w \epsilon_\theta(x_t, t)
$$

$w$ãŒå¤§ãã„ã¨$\tilde{\epsilon}_\theta$ã®çµ¶å¯¾å€¤ãŒå¤§ãããªã‚Šã€ODEãŒç¡¬ããªã‚‹ï¼ˆstiff ODEï¼‰ã€‚

**DPM-Solver++ã®è§£æ±ºç­–**: **Data prediction**ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›:

$$
x_\theta(x_t, t) = \frac{x_t - \sigma_t \epsilon_\theta(x_t, t)}{\alpha_t}
$$

ã“ã‚Œã‚’ä½¿ã£ã¦ODEã‚’æ›¸ãç›´ã™:

$$
\frac{dx_t}{d\lambda} = \frac{\alpha_t}{\sigma_t} (x_t - x_\theta(x_t, t))
$$

ã“ã“ã§$\lambda = \log(\alpha_t / \sigma_t)$ã¯å¯¾æ•°SNRã€‚

**é«˜æ¬¡Solver**: Taylorå±•é–‹ã§2æ¬¡ç²¾åº¦è¿‘ä¼¼:

$$
x_{t_{i+1}} = \frac{\alpha_{t_{i+1}}}{\alpha_{t_i}} x_{t_i} - \alpha_{t_{i+1}} \int_{\lambda_{t_i}}^{\lambda_{t_{i+1}}} e^{-\lambda} x_\theta(x_{\lambda(s)}, \lambda(s)) \, d\lambda
$$

ç©åˆ†ã‚’**Trapezoid rule**ã§è¿‘ä¼¼:

$$
\int_{\lambda_i}^{\lambda_{i+1}} e^{-\lambda} x_\theta \, d\lambda \approx \frac{h}{2} (e^{-\lambda_i} x_\theta(x_i, \lambda_i) + e^{-\lambda_{i+1}} x_\theta(x_{i+1}, \lambda_{i+1}))
$$

ã“ã“ã§$h = \lambda_{i+1} - \lambda_i$ã€‚

**Multistepæ³•**: éå»ã®$x_\theta$å€¤ã‚’å†åˆ©ç”¨ã—ã¦é«˜æ¬¡è¿‘ä¼¼:

$$
x_{i+1} = a_0 x_i + \sum_{k=0}^K b_k x_\theta(x_{i-k}, \lambda_{i-k})
$$

$K=2$ã§3æ¬¡ç²¾åº¦é”æˆ â†’ 15-20ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ªã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆå¯èƒ½ï¼

#### 3.8.3 DPM-Solver-v3

**è«–æ–‡**: Zheng et al., "DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics," NeurIPS 2023[^4]

**ã•ã‚‰ãªã‚‹æ”¹å–„**: ãƒ¢ãƒ‡ãƒ«ã®**çµŒé¨“çš„çµ±è¨ˆé‡**ï¼ˆå¹³å‡$\mu_t$ã€åˆ†æ•£$\Sigma_t$ï¼‰ã‚’æ¨å®šã—ã€ODEã«çµ„ã¿è¾¼ã‚€ã€‚

$$
x_\theta(x_t, t) \approx \mu_t + \Sigma_t^{1/2} \cdot \text{Whitening}^{-1}(x_t)
$$

**åŠ¹æœ**: 10ã‚¹ãƒ†ãƒƒãƒ—ã§DDIM 50ã‚¹ãƒ†ãƒƒãƒ—ç›¸å½“ã®å“è³ªé”æˆã€‚

**å®Ÿè£…ï¼ˆRustæ¦‚å¿µã‚³ãƒ¼ãƒ‰ï¼‰**:

#### DPM-Solver++ ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æœ€é©åŒ–

DPM-Solver++ ãŒ15-20ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ªç”Ÿæˆã§ãã‚‹ç†ç”±ã‚’ODEè¿‘ä¼¼èª¤å·®ã®è¦³ç‚¹ã‹ã‚‰åˆ†æã™ã‚‹ã€‚

$p$ æ¬¡ã®æ•°å€¤è§£æ³•ã¯å±€æ‰€æ‰“ã¡åˆ‡ã‚Šèª¤å·®ï¼ˆlocal truncation errorï¼‰ãŒ $O(h^{p+1})$ï¼ˆ$h$ ã¯ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼‰ã€‚ãƒˆãƒ¼ã‚¿ãƒ«èª¤å·®ã¯ã‚¹ãƒ†ãƒƒãƒ—æ•° $S$ ã‚’ä½¿ã£ã¦ï¼š

$$
E_{\text{total}} \approx S \cdot O(h^{p+1}) = S \cdot O\!\left(\frac{T^{p+1}}{S^{p+1}}\right) = O\!\left(\frac{T^{p+1}}{S^p}\right)
$$

DDIMï¼ˆ1æ¬¡ç²¾åº¦ã€$p = 1$ï¼‰ã¨ DPM-Solver++ï¼ˆ3æ¬¡ç²¾åº¦ã€$p = 3$ï¼‰ã®æ¯”è¼ƒï¼š
- DDIM ã§èª¤å·® $\epsilon$ ã‚’é”æˆ: $S \sim O(\epsilon^{-1})$ ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦
- DPM-Solver++ ã§åŒèª¤å·®: $S \sim O(\epsilon^{-1/3})$ ã‚¹ãƒ†ãƒƒãƒ—ã§é”æˆ

FID 10 ç¨‹åº¦ã‚’é”æˆã™ã‚‹ã®ã« DDIM ã¯ 250 ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ã ã£ãŸãŒã€DPM-Solver++ ã§ã¯ç†è«–çš„ã« $250^{1/3} \approx 6.3$ â€” ã¤ã¾ã‚Š **6-7 ã‚¹ãƒ†ãƒƒãƒ—** ã§åŒç­‰å“è³ªãŒå¾—ã‚‰ã‚Œã‚‹è¨ˆç®—ã«ãªã‚‹ï¼ˆå®Ÿéš›ã¯15-20ã‚¹ãƒ†ãƒƒãƒ—ãŒå®‰å…¨åœï¼‰ã€‚

ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã®é…ç½®ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰ã‚‚é‡è¦ã ã€‚å‡ç­‰å‰²ã‚Šã‚ˆã‚Š**å¯¾æ•° SNR ç©ºé–“ã§ã®ç­‰é–“éš”é…ç½®**ãŒä½èª¤å·®ï¼š
$$
\lambda_i = \lambda_T + \frac{i}{S}(\lambda_0 - \lambda_T), \quad i = 0, 1, \ldots, S
$$
$t_i = \text{SNR}^{-1}(\lambda_i)$ ã§æ™‚é–“è»¸ã«å¤‰æ›ã€‚ã“ã‚ŒãŒ DDIM ã®ç­‰æ™‚åˆ»é–“éš”ã‚ˆã‚Šå°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ªã‚’é”æˆã™ã‚‹ç†ç”±ã ã€‚

#### 3.9.1 SD3ã®MM-DiT Architecture

**è«–æ–‡**: Esser et al., "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis," Stability AI Technical Report, 2024[^5]

SD3ã¯**Rectified Flow**ï¼ˆç¬¬38å›ã®Flow Matchingï¼‰ã¨MM-DiTã‚’çµ„ã¿åˆã‚ã›ã‚‹ã€‚

**é‡è¦ãªè¨­è¨ˆé¸æŠ**:

1. **2ã¤ã®ç‹¬ç«‹ã—ãŸTransformer stream**:
   - Image stream: ç”»åƒãƒ‘ãƒƒãƒå‡¦ç†
   - Text stream: T5/CLIPãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†

2. **å„DiTãƒ–ãƒ­ãƒƒã‚¯ã®æ§‹é€ **:

$$
\begin{aligned}
\text{Image stream: } & y_{\text{img}}^{(\ell+1)} = y_{\text{img}}^{(\ell)} + \text{DiTBlock}_{\text{img}}(y_{\text{img}}^{(\ell)}, y_{\text{txt}}^{(\ell)}, t) \\
\text{Text stream: } & y_{\text{txt}}^{(\ell+1)} = y_{\text{txt}}^{(\ell)} + \text{DiTBlock}_{\text{txt}}(y_{\text{txt}}^{(\ell)}, y_{\text{img}}^{(\ell)}, t)
\end{aligned}
$$

**Cross-Attention**: å„streamãŒç›¸æ‰‹ã®streamã‚’è¦‹ã‚‹ï¼ˆbidirectional cross-attentionï¼‰ã€‚

3. **QK-Normalization**: Attentionè¨ˆç®—å‰ã«Query/Keyã‚’æ­£è¦åŒ–:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{\text{Norm}(Q) \cdot \text{Norm}(K)^\top}{\sqrt{d}}\right) V
$$

**åŠ¹æœ**: è¨“ç·´ã®å®‰å®šåŒ– + å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ8B paramsï¼‰ã§ã‚‚åæŸã€‚

#### 3.9.2 FLUX Architecture

**è«–æ–‡**: Beaumont et al., "FLUX.1: Advanced Image Generation," Black Forest Labs Technical Report, 2024[^6]

FLUXã¯SD3ã®é€²åŒ–ç‰ˆ:

**ä¸»è¦ãªæ”¹å–„**:

1. **Parallel Attention and MLP**:

æ¨™æº–DiTãƒ–ãƒ­ãƒƒã‚¯ï¼ˆSequentialï¼‰:

$$
\begin{aligned}
z' &= z + \text{Attention}(\text{AdaLN}(z, t)) \\
z'' &= z' + \text{MLP}(\text{AdaLN}(z', t))
\end{aligned}
$$

FLUXãƒ–ãƒ­ãƒƒã‚¯ï¼ˆParallelï¼‰:

$$
z' = z + \text{Attention}(\text{AdaLN}(z, t)) + \text{MLP}(\text{AdaLN}(z, t))
$$

**åˆ©ç‚¹**: ä¸¦åˆ—åŒ–ã§é«˜é€ŸåŒ– + è¡¨ç¾åŠ›å‘ä¸Šã€‚

2. **Rotary Position Embedding (RoPE)** (LLMã‹ã‚‰è¼¸å…¥):

$$
\text{RoPE}(q, k, m) = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix} \begin{pmatrix} q_0 \\ q_1 \end{pmatrix}
$$

ä½ç½®$m$ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å›è»¢è¡Œåˆ—ã‚’é©ç”¨ â†’ ç›¸å¯¾ä½ç½®æƒ…å ±ã‚’Attentionã«åŸ‹ã‚è¾¼ã‚€ã€‚

3. **Guidance Distillation**:

CFGã®$w$ã‚’å­¦ç¿’æ™‚ã«è’¸ç•™ â†’ æ¨è«–æ™‚ã«guidance-freeã§é«˜å“è³ªç”Ÿæˆå¯èƒ½ï¼ˆ4-8ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã€‚

**å®Ÿè£…ã®æ ¸å¿ƒï¼ˆRustæ¦‚å¿µã‚³ãƒ¼ãƒ‰ï¼‰**:


#### RoPE ã®ç›¸å¯¾ä½ç½®ç¬¦å·åŒ–ã®æ•°å­¦çš„è¨¼æ˜

FLUXã§æ¡ç”¨ã•ã‚ŒãŸ RoPE ãŒç›¸å¯¾ä½ç½®ã‚’ç¬¦å·åŒ–ã™ã‚‹ã“ã¨ã‚’è¨¼æ˜ã™ã‚‹ã€‚ä½ç½® $m$ ã®Query ã¨ä½ç½® $n$ ã®Key ã®å†…ç©ã‚’è¨ˆç®—ã™ã‚‹ï¼š

$$
(\text{RoPE}(q, m))^\top (\text{RoPE}(k, n)) = q^\top R(m)^\top R(n) k = q^\top R(n - m) k
$$

ã“ã“ã§ $R(m)$ ã¯å›è»¢è¡Œåˆ—ï¼ˆ$R(m)^\top R(n) = R(n-m)$ã€å›è»¢è¡Œåˆ—ã®ä¹—æ³•æ€§ï¼‰:

$$
R(m) = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix}
$$

**æ ¸å¿ƒ**: å†…ç©ãŒ **ä½ç½®å·® $n - m$ ã®ã¿**ã«ä¾å­˜ã™ã‚‹ã€‚ã¤ã¾ã‚ŠRoPEã¯ã€Œãƒˆãƒ¼ã‚¯ãƒ³ $m$ ã¨ãƒˆãƒ¼ã‚¯ãƒ³ $n$ ã¯ $|n - m|$ ã ã‘é›¢ã‚Œã¦ã„ã‚‹ã€ã¨ã„ã†ç›¸å¯¾ä½ç½®æƒ…å ±ã‚’è‡ªå‹•çš„ã«Attentionã‚¹ã‚³ã‚¢ã«åŸ‹ã‚è¾¼ã‚€ã€‚Sinusoidal PEãŒçµ¶å¯¾ä½ç½®ã‚’ç¬¦å·åŒ–ã™ã‚‹ã®ã«å¯¾ã—ã€RoPEã¯ç›¸å¯¾ä½ç½®ã‚’ç¬¦å·åŒ– â€” æ–‡è„ˆé•·ã®ä¸€èˆ¬åŒ–ï¼ˆè¨“ç·´æ™‚ã‚ˆã‚Šé•·ã„ç³»åˆ—ã¸ã®è»¢ç§»ï¼‰ã«ãŠã„ã¦RoPEãŒå„ªã‚Œã¦ã„ã‚‹ç†ç”±ã ã€‚

2æ¬¡å…ƒç‰ˆ RoPEï¼ˆFLUX ã§ã®ç”»åƒç”¨ï¼‰ã¯ç¸¦ãƒ»æ¨ªã®å„æ–¹å‘ã«ç‹¬ç«‹ã—ãŸå›è»¢è¡Œåˆ—ã‚’é©ç”¨ã™ã‚‹ï¼š
$$
\text{RoPE2D}(q, i, j) = R_{\text{row}}(i) \otimes R_{\text{col}}(j)\, q
$$
ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€Œå³ã«3ãƒ‘ãƒƒãƒã€ä¸‹ã«2ãƒ‘ãƒƒãƒç§»å‹•ã—ãŸä½ç½®ã€ã¨ã„ã†2æ¬¡å…ƒç›¸å¯¾ä½ç½®ã‚’AttentionãŒç›´æ¥å­¦ç¿’ã§ãã‚‹ã€‚

**è«–æ–‡**: Helbling, A. et al., "ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features," arXiv:2502.04320, 2025[^7]

**ç™ºè¦‹**: DiTã®ä¸­é–“å±¤ã®ç‰¹å¾´ã¯**æ„å‘³çš„ã«è§£é‡ˆå¯èƒ½**ãªæ§‹é€ ã‚’æŒã¤ã€‚

**å®Ÿé¨“**: SD3ã®ä¸­é–“å±¤ï¼ˆLayer 12/24ï¼‰ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’PCAã§2Då¯è¦–åŒ–:

- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼1: "å‹•ç‰©"ï¼ˆçŠ¬ãƒ»çŒ«ãƒ»é¦¬ï¼‰
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼2: "å»ºç‰©"ï¼ˆå®¶ãƒ»ãƒ“ãƒ«ãƒ»æ©‹ï¼‰
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼3: "è‡ªç„¶"ï¼ˆæœ¨ãƒ»èŠ±ãƒ»å±±ï¼‰

**æ„å‘³ç·¨é›†**: ç‰¹å®šã®ç‰¹å¾´æ–¹å‘ã«æ‘‚å‹•ã‚’åŠ ãˆã‚‹ã“ã¨ã§æ„å‘³çš„ç·¨é›†ãŒå¯èƒ½:

$$
z' = z + \alpha \cdot \mathbf{v}_{\text{concept}}
$$

ä¾‹: $\mathbf{v}_{\text{smile}}$æ–¹å‘ã«$\alpha=2.0$ã§åŠ ç®— â†’ ã€Œç¬‘é¡”ã‚’å¼·èª¿ã€

**æ¦‚å¿µæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã®æŠ½å‡ºæ–¹æ³•**: ã€Œç¬‘é¡”ã‚ã‚Šã€ã¨ã€Œç¬‘é¡”ãªã—ã€ã®ç”»åƒãƒšã‚¢ã‚’ç”Ÿæˆã—ã€ä¸­é–“å±¤ã®ç‰¹å¾´å·®åˆ†ã‚’å–ã‚‹ï¼š
$$
\mathbf{v}_{\text{concept}} = \mathbb{E}\!\left[\mathbf{z}^{(\ell)}_{\text{with concept}} - \mathbf{z}^{(\ell)}_{\text{without concept}}\right]
$$
ã“ã®æ–¹å‘ã«æ²¿ã£ã¦ $\alpha$ ã‚’å¤‰åŒ–ã•ã›ã‚‹ã¨ã€ç”Ÿæˆç”»åƒã®ã€Œç¬‘é¡”åº¦ã€ãŒé€£ç¶šçš„ã«åˆ¶å¾¡ã§ãã‚‹ã€‚DiTã®ç·šå½¢åˆ†é›¢æ€§ï¼ˆå„æ¦‚å¿µãŒç‰¹å®šã®æ–¹å‘ã«å¯¾å¿œï¼‰ã¯GANã®latent spaceè§£é‡ˆå¯èƒ½æ€§ç ”ç©¶ï¼ˆStyleGANç­‰ï¼‰ã¨åŒæ§˜ã®ç¾è±¡ã ãŒã€**æ¡ä»¶ä»˜ãæ‹¡æ•£éç¨‹ã®ä¸­é–“è¡¨ç¾**ã§ã‚‚åŒæ§˜ã®æ§‹é€ ãŒç¾ã‚Œã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸç‚¹ãŒæ–°ã—ã„ã€‚

**å¿œç”¨**: Training-freeç”»åƒç·¨é›†ã€Concept steeringã€Adversarial robustnesså‘ä¸Šã€‚

#### DiT ã®è¨“ç·´ç›®æ¨™é–¢æ•°ã®æ¯”è¼ƒï¼šDDPM vs Rectified Flow

DiT ã¯ CNN ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã‚’ç½®ãæ›ãˆã‚‹ã ã‘ã§ãªãã€è¨“ç·´ç›®æ¨™é–¢æ•°ã‚‚é¸æŠè‚¢ãŒã‚ã‚‹ã€‚

**DDPM ç›®æ¨™** [Ho+ 2020]ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ« DiT ã§ä½¿ç”¨ï¼‰:
$$
\mathcal{L}_{\text{DDPM}} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right]
$$
ã“ã“ã§ $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\, \boldsymbol{\epsilon}$ã€$\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$ã€‚

**Rectified Flow ç›®æ¨™** [Liu+ 2022]ï¼ˆSD3ãƒ»FLUX ã§ä½¿ç”¨ï¼‰:
$$
\mathcal{L}_{\text{RF}} = \mathbb{E}_{t, \mathbf{x}_0, \mathbf{x}_1} \left[\|\mathbf{x}_1 - \mathbf{x}_0 - \mathbf{v}_\theta(\mathbf{x}_t, t)\|^2\right]
$$
ã“ã“ã§ $\mathbf{x}_t = t\, \mathbf{x}_1 + (1 - t)\, \mathbf{x}_0$ã€çœŸã®é€Ÿåº¦å ´ $\mathbf{v}^* = \mathbf{x}_1 - \mathbf{x}_0$ ã¯ä¸€å®šï¼ˆç›´ç·šçµŒè·¯ï¼‰ã€‚

**2ã¤ã®ç›®æ¨™ã®ç­‰ä¾¡æ€§**: é©åˆ‡ãªãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å¤‰æ›ã‚’è¡Œã†ã¨ DDPM ã¨ Rectified Flow ã¯ç­‰ä¾¡ã«ãªã‚‹ã€‚å¤‰æ›å¼ï¼š
$$
\boldsymbol{\epsilon}_\theta = \frac{\mathbf{x}_t - \alpha_t\, \mathbf{v}_\theta}{\sigma_t}
$$

**å®Ÿè·µçš„ãªå·®ç•°**:
| è¦³ç‚¹ | DDPM | Rectified Flow |
|:-----|:-----|:--------------|
| é€Ÿåº¦å ´ã®æ›²ç‡ | é«˜ã„ï¼ˆéç·šå½¢çµŒè·¯ï¼‰ | ä½ã„ï¼ˆç›´ç·šçµŒè·¯ï¼‰ |
| å¿…è¦NFE | 20-50 | 5-15 |
| è¨“ç·´æå¤±ã®è§£é‡ˆ | ãƒã‚¤ã‚ºäºˆæ¸¬èª¤å·® | é€Ÿåº¦å ´äºˆæ¸¬èª¤å·® |
| SD3 æ¡ç”¨ç†ç”± | â€” | é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + å®‰å®šè¨“ç·´ |

SD3 ãŒ Rectified Flow ã‚’é¸ã‚“ã æ±ºå®šçš„ç†ç”±: ç›´ç·šçµŒè·¯ã¯ã€Œæœ€çŸ­çµŒè·¯ã€ã§ã‚ã‚Šã€å°‘ãªã„ODEã‚¹ãƒ†ãƒƒãƒ—ã§ç©åˆ†èª¤å·®ãŒæœ€å°åŒ–ã•ã‚Œã‚‹ã€‚DDPM ç›®æ¨™ã§ã‚‚ DPM-Solver++ ã‚’ä½¿ãˆã°é«˜é€ŸåŒ–ã¯å¯èƒ½ã ãŒã€Rectified Flow ã¯**è¨“ç·´æ®µéšã‹ã‚‰é€Ÿã„çµŒè·¯ã‚’å­¦ç¿’**ã™ã‚‹ç‚¹ã§æ ¹æœ¬çš„ã«å„ªã‚Œã¦ã„ã‚‹ã€‚

**Logit-normal ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: SD3 ã§ã¯æ™‚åˆ» $t$ ã‚’å‡ä¸€åˆ†å¸ƒã§ã¯ãªã **Logit-normal** ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ï¼š
$$
t \sim \text{Logit-Normal}(\mu, \sigma^2), \quad t = \text{sigmoid}(u),\; u \sim \mathcal{N}(\mu, \sigma^2)
$$
$\mu = 0, \sigma = 1$ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã§ã¯ $t$ ãŒ $[0.3, 0.7]$ ä»˜è¿‘ã«é›†ä¸­ã™ã‚‹ã€‚ç›´æ„Ÿ: å…¨æ‹¡æ•£ã‚¹ãƒ†ãƒƒãƒ—ä¸­ã§æœ€ã‚‚ã€Œé›£ã—ã„ã€ä¸­é–“æ™‚åˆ»ã§ã®å­¦ç¿’ã‚’å¼·èª¿ã™ã‚‹ã“ã¨ã§ã€è¨“ç·´åŠ¹ç‡ãŒå‘ä¸Šã™ã‚‹ã€‚$t \approx 0$ï¼ˆã»ã¼æ¸…æµ„ï¼‰ã¨ $t \approx 1$ï¼ˆã»ã¼ãƒã‚¤ã‚ºï¼‰ã¯æ¯”è¼ƒçš„å®¹æ˜“ãªãŸã‚å‡ç­‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯éåŠ¹ç‡ã ã€‚

### 4.1 å®Œå…¨ãªDiTãƒ–ãƒ­ãƒƒã‚¯å®Ÿè£…ï¼ˆCandleï¼‰


### 4.2 MM-DiTå®Ÿè£…ï¼ˆSD3/FLUXã‚¹ã‚¿ã‚¤ãƒ«ï¼‰


### 4.3 DPM-Solver++ã‚µãƒ³ãƒ—ãƒ©ãƒ¼å®Œå…¨å®Ÿè£…


### 4.4 Scaling Lawså®Ÿé¨“ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯


> **Note:** **é€²æ—**: å…¨ä½“ã®85%å®Œäº†ã€‚Production-ReadyãªDiTå®Ÿè£…ï¼ˆAdaLN-Zeroã€MM-DiTã€DPM-Solver++ã€Scaling Lawså®Ÿé¨“ï¼‰ã‚’å®Œå…¨å®Ÿè£…ã—ãŸã€‚ç†è«–â†’å®Ÿè£…ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’å®Œå…¨ã«åŸ‹ã‚ãŸã€‚

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Zhai, S., et al. (2024). Scaling Laws For Diffusion Transformers. arXiv:2410.08184.
<https://arxiv.org/abs/2410.08184>

[^2]: Xu, Y., et al. (2025). Scaling Diffusion Transformers Efficiently via Î¼P. arXiv:2505.15270.
<https://arxiv.org/abs/2505.15270>

[^3]: Lu, C., et al. (2023). DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models. Machine Intelligence Research.
<https://arxiv.org/abs/2211.01095>

[^4]: Zheng, K., et al. (2023). DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics. NeurIPS 2023.
<https://openreview.net/forum?id=9fWKExmKa0>

[^5]: Esser, P., et al. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. Stability AI Technical Report.
<https://stability.ai/news/stable-diffusion-3-research-paper>

[^6]: Beaumont, R., et al. (2024). FLUX.1: Advanced Image Generation. Black Forest Labs Technical Report.
<https://arxiv.org/html/2507.09595v1>

[^7]: Helbling, A., et al. (2025). ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features. arXiv:2502.04320.
<https://arxiv.org/abs/2502.04320>

### è¿½åŠ å‚è€ƒæ–‡çŒ®

- Peebles, W., & Xie, S. (2023). Scalable Diffusion Models with Transformers. ICCV 2023. arXiv:2212.09748.
<https://arxiv.org/abs/2212.09748>

- Lu, C., et al. (2022). DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. NeurIPS 2022 Oral.
<https://arxiv.org/abs/2206.00927>

---


> Progress: 50%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $N, C=$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ¯ 5. ã¾ã¨ã‚ â€” DiTãŒåˆ‡ã‚Šé–‹ãæœªæ¥

### 5.1 æœ¬Partã§å­¦ã‚“ã ã“ã¨

**ç†è«–çš„åŸºç›¤**:
- U-Netã‹ã‚‰Transformerã¸ã®ç§»è¡Œã®å¿…ç„¶æ€§ï¼ˆScaling Lawsé©ç”¨å¯èƒ½æ€§ï¼‰
- AdaLN-Zeroã€MM-DiTã€SiTã®æ•°å­¦çš„æ§‹é€ 
- Scaling Laws: FID(C) = AÂ·C^(-Î²) + FID_âˆï¼ˆÎ² â‰ˆ 0.27ï¼‰
- Î¼P: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºéä¾å­˜ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è»¢ç§»
- DPM-Solver++: CFGå®‰å®šåŒ– + 15-20ã‚¹ãƒ†ãƒƒãƒ—é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**å®Ÿè£…ã‚¹ã‚­ãƒ«**:
- Candleã§ã®DiTå®Œå…¨å®Ÿè£…ï¼ˆPatchify/Unpatchify/DiTBlock/AdaLNï¼‰
- MM-DiT dual-stream architecture
- DPM-Solver++ 2nd-order sampler
- Scaling Lawså®Ÿé¨“ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**æœ€å…ˆç«¯å‹•å‘**:
- SD3: MM-DiT + Rectified Flow + QK-Norm
- FLUX: Parallel Attn/MLP + RoPE + Guidance Distillation
- è§£é‡ˆå¯èƒ½æ€§: DiTä¸­é–“å±¤ã®æ„å‘³çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å½¢æˆ

### 5.2 Part 2ã¸ã®æ¥ç¶š

Part 2ã§ã¯ã€DiTã‚’ä»¥ä¸‹ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã«å¿œç”¨ã™ã‚‹:
- **é«˜è§£åƒåº¦ç”»åƒç”Ÿæˆ**: Cascade DiTã€Patch-wise sampling
- **Video DiT**: æ™‚ç©ºé–“attentionã€3D RoPE
- **æ¡ä»¶ä»˜ãç”Ÿæˆ**: ControlNet-DiTã€Regional prompting
- **é«˜é€ŸåŒ–**: Consistency DiTã€Latent Consistency Models

DiTã¯ç”»åƒç”Ÿæˆã ã‘ã§ãªãã€**å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ï¼ˆå‹•ç”»ãƒ»éŸ³å£°ãƒ»3Dï¼‰ã®çµ±ä¸€ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³**ã¨ã—ã¦é€²åŒ–ã—ã¦ã„ã‚‹ã€‚

---


---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
