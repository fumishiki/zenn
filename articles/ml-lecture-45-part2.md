---
title: "ç¬¬45å› (Part 2): Videoç”Ÿæˆ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ¬"
type: "tech"
topics: ["machinelearning","deeplearning","video","julia","rust","elixir"]
published: true
slug: "ml-lecture-45-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---
## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” 3è¨€èªã§å‹•ç”»ç”Ÿæˆã‚’å®Ÿè£…

### 4.1 âš¡ Julia: Video Diffusionè¨“ç·´å®Ÿè£…

#### 4.1.1 ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ â€” å‹•ç”»ã‚’ãƒãƒƒãƒå‡¦ç†

```julia
using VideoIO, Images, Random, Flux

struct VideoDataset
    video_paths::Vector{String}
    num_frames::Int
    height::Int
    width::Int
end

function load_video(path::String, num_frames::Int, height::Int, width::Int)
    # VideoIOã§å‹•ç”»èª­ã¿è¾¼ã¿
    reader = VideoIO.openvideo(path)
    frames = []

    for i in 1:num_frames
        if !eof(reader)
            img = read(reader)
            # Resize + æ­£è¦åŒ–
            img_resized = imresize(img, (height, width))
            img_normalized = Float32.(channelview(img_resized)) .* 2 .- 1  # [-1, 1]
            push!(frames, img_normalized)
        else
            break
        end
    end

    close(reader)

    # (C, H, W, T)å½¢å¼ã«ã‚¹ã‚¿ãƒƒã‚¯
    return cat(frames..., dims=4)
end

function Base.getindex(dataset::VideoDataset, idx::Int)
    path = dataset.video_paths[idx]
    video = load_video(path, dataset.num_frames, dataset.height, dataset.width)
    return video
end

function Base.length(dataset::VideoDataset)
    return length(dataset.video_paths)
end

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
video_paths = readdir("/path/to/videos", join=true)
dataset = VideoDataset(video_paths, num_frames=16, height=64, width=64)

function collate_fn(batch)
    # ãƒãƒƒãƒã‚’ã‚¹ã‚¿ãƒƒã‚¯
    return cat([dataset[i] for i in batch]..., dims=5)  # (C, H, W, T, B)
end

dataloader = Flux.DataLoader(1:length(dataset), batchsize=4, shuffle=true, collate=collate_fn)
```

#### 4.1.2 3D U-Netã®ç°¡æ˜“å®Ÿè£…

```julia
using Flux, Functors

# 3D Convolution Block
struct Conv3DBlock
    conv1::Conv
    conv2::Conv
    norm::BatchNorm
end

@functor Conv3DBlock

function Conv3DBlock(in_ch::Int, out_ch::Int; kernel=(3,3,3), pad=(1,1,1))
    conv1 = Conv(kernel, in_ch => out_ch, pad=pad, init=Flux.glorot_uniform)
    conv2 = Conv(kernel, out_ch => out_ch, pad=pad, init=Flux.glorot_uniform)
    norm = BatchNorm(out_ch)
    return Conv3DBlock(conv1, conv2, norm)
end

function (block::Conv3DBlock)(x)
    x = block.conv1(x)
    x = relu.(x)
    x = block.conv2(x)
    x = block.norm(x)
    return relu.(x)
end

# Simple 3D U-Net
struct Simple3DUNet
    down1::Conv3DBlock
    down2::Conv3DBlock
    bottleneck::Conv3DBlock
    up1::ConvTranspose
    up2::ConvTranspose
    final_conv::Conv
end

@functor Simple3DUNet

function Simple3DUNet(in_ch::Int, out_ch::Int)
    down1 = Conv3DBlock(in_ch, 64)
    down2 = Conv3DBlock(64, 128)
    bottleneck = Conv3DBlock(128, 256)
    up1 = ConvTranspose((3,3,3), 256 => 128, stride=(2,2,2), pad=(1,1,1))
    up2 = ConvTranspose((3,3,3), 128 => 64, stride=(2,2,2), pad=(1,1,1))
    final_conv = Conv((1,1,1), 64 => out_ch)
    return Simple3DUNet(down1, down2, bottleneck, up1, up2, final_conv)
end

function (model::Simple3DUNet)(x)
    # x: (C, H, W, T, B)
    d1 = model.down1(x)
    d1_pool = maxpool(d1, (2,2,2))

    d2 = model.down2(d1_pool)
    d2_pool = maxpool(d2, (2,2,2))

    bn = model.bottleneck(d2_pool)

    u1 = model.up1(bn)
    u1 = u1 .+ d2  # Skip connection

    u2 = model.up2(u1)
    u2 = u2 .+ d1

    out = model.final_conv(u2)
    return out
end
```

#### 4.1.3 Video Diffusionè¨“ç·´ãƒ«ãƒ¼ãƒ—

```julia
using Statistics, ProgressMeter

function add_noise(x0, t, Î²_schedule)
    # Forward process: x_t = âˆšÎ±_t x_0 + âˆš(1-Î±_t) Îµ
    Î²t = Î²_schedule[t]
    Î±t = cumprod(1 .- Î²_schedule)[t]

    Îµ = randn(Float32, size(x0))
    xt = sqrt(Î±t) .* x0 .+ sqrt(1 - Î±t) .* Îµ

    return xt, Îµ
end

function train_video_diffusion(model, dataloader, num_epochs, Î²_schedule)
    opt = Flux.setup(Adam(1e-4), model)

    for epoch in 1:num_epochs
        epoch_loss = 0.0f0

        @showprogress for batch in dataloader
            x0 = batch  # (C, H, W, T, B)
            B = size(x0, 5)

            # ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—
            t = rand(1:length(Î²_schedule), B)

            # ãƒã‚¤ã‚ºè¿½åŠ 
            xt, Îµ_true = add_noise(x0, t, Î²_schedule)

            # ãƒã‚¤ã‚ºäºˆæ¸¬
            loss, grads = Flux.withgradient(model) do m
                Îµ_pred = m(xt)
                mean((Îµ_pred .- Îµ_true).^2)
            end

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            Flux.update!(opt, model, grads[1])

            epoch_loss += loss
        end

        avg_loss = epoch_loss / length(dataloader)
        println("Epoch $epoch, Loss: $avg_loss")
    end

    return model
end

# è¨“ç·´å®Ÿè¡Œ
model = Simple3DUNet(3, 3)  # RGBå‹•ç”»
Î²_schedule = LinRange(1e-4, 0.02, 1000)

trained_model = train_video_diffusion(model, dataloader, num_epochs=10, Î²_schedule)
```

#### 4.1.4 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆDDIMï¼‰

```julia
function ddim_sample(model, num_frames, height, width, Î²_schedule; num_steps=50)
    T_max = length(Î²_schedule)
    step_size = div(T_max, num_steps)
    timesteps = T_max:-step_size:1

    # ãƒã‚¤ã‚ºã‹ã‚‰é–‹å§‹
    xt = randn(Float32, 3, height, width, num_frames, 1)

    @showprogress for i in 1:length(timesteps)
        t = timesteps[i]
        t_prev = i < length(timesteps) ? timesteps[i+1] : 0

        Î±t = cumprod(1 .- Î²_schedule)[t]
        Î±t_prev = t_prev > 0 ? cumprod(1 .- Î²_schedule)[t_prev] : 1.0f0

        # ãƒã‚¤ã‚ºäºˆæ¸¬
        Îµ_pred = model(xt)

        # x0äºˆæ¸¬
        x0_pred = (xt .- sqrt(1 - Î±t) .* Îµ_pred) ./ sqrt(Î±t)

        # DDIMã‚¹ãƒ†ãƒƒãƒ—
        dir_xt = sqrt(1 - Î±t_prev) .* Îµ_pred
        xt = sqrt(Î±t_prev) .* x0_pred .+ dir_xt
    end

    # [-1, 1] â†’ [0, 1]
    video = (xt .+ 1) ./ 2
    return video[:, :, :, :, 1]  # ãƒãƒƒãƒæ¬¡å…ƒå‰Šé™¤
end

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ
generated_video = ddim_sample(trained_model, 16, 64, 64, Î²_schedule, num_steps=50)
```

### 4.2 ğŸ¦€ Rust: LTX-Videoæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

#### 4.2.1 ONNX Runtimeçµ±åˆ

```rust
// Cargo.toml
// [dependencies]
// ort = "2.0"
// ndarray = "0.15"
// image = "0.24"

use ort::{Session, Value, GraphOptimizationLevel};
use ndarray::{Array4, Array5, s};
use std::path::Path;

pub struct LTXVideoInference {
    session: Session,
}

impl LTXVideoInference {
    pub fn new(model_path: &Path) -> Result<Self, Box<dyn std::error::Error>> {
        let session = Session::builder()?
            .with_optimization_level(GraphOptimizationLevel::Level3)?
            .commit_from_file(model_path)?;

        Ok(Self { session })
    }

    pub fn generate(&self, prompt_embedding: &Array4<f32>, num_frames: usize)
        -> Result<Array5<f32>, Box<dyn std::error::Error>>
    {
        // ãƒã‚¤ã‚ºLatentç”Ÿæˆ
        let latent_shape = (1, 4, num_frames / 4, 64, 64);  // åœ§ç¸®æ¸ˆã¿
        let mut noise: Array5<f32> = Array5::from_shape_fn(latent_shape, |_| {
            rand::random::<f32>() * 2.0 - 1.0
        });

        // Diffusionãƒ«ãƒ¼ãƒ—ï¼ˆ50ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        for step in (0..50).rev() {
            let t = Array4::from_elem((1, 1, 1, 1), step as f32 / 50.0);

            // ONNXæ¨è«–
            let inputs = vec![
                Value::from_array(noise.view())?,
                Value::from_array(t.view())?,
                Value::from_array(prompt_embedding.view())?,
            ];

            let outputs = self.session.run(inputs)?;
            let noise_pred = outputs[0].try_extract_tensor::<f32>()?;

            // DDIMæ›´æ–°
            let alpha_t = 1.0 - (step as f32 / 50.0) * 0.02;
            let alpha_prev = if step > 0 {
                1.0 - ((step - 1) as f32 / 50.0) * 0.02
            } else {
                1.0
            };

            let x0_pred = (noise.view() - (1.0 - alpha_t).sqrt() * noise_pred.view())
                / alpha_t.sqrt();

            noise = alpha_prev.sqrt() * &x0_pred + (1.0 - alpha_prev).sqrt() * noise_pred.view();
        }

        Ok(noise)
    }
}

// ä½¿ç”¨ä¾‹
fn main() -> Result<(), Box<dyn std::error::Error>> {
    let model_path = Path::new("ltx_video.onnx");
    let inference = LTXVideoInference::new(model_path)?;

    // ãƒ€ãƒŸãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆå®Ÿéš›ã¯CLIP/T5ã‹ã‚‰å–å¾—ï¼‰
    let prompt_emb = Array4::zeros((1, 512, 1, 1));

    let video = inference.generate(&prompt_emb, 16)?;

    println!("Generated video shape: {:?}", video.shape());

    Ok(())
}
```

#### 4.2.2 ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã®ä¸¦åˆ—å‡¦ç†

```rust
use rayon::prelude::*;

pub fn parallel_frame_processing(
    frames: &Array5<f32>,  // (B, C, T, H, W)
) -> Array5<f32> {
    let (b, c, t, h, w) = frames.dim();
    let mut processed = Array5::zeros((b, c, t, h, w));

    // ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«ä¸¦åˆ—å‡¦ç†
    (0..t).into_par_iter().for_each(|frame_idx| {
        let frame = frames.slice(s![.., .., frame_idx, .., ..]);

        // ä½•ã‚‰ã‹ã®å¾Œå‡¦ç†ï¼ˆä¾‹: Sharpeningï¼‰
        let mut processed_frame = frame.to_owned();
        // ... (å‡¦ç†)

        // çµæœã‚’æ›¸ãæˆ»ã—ï¼ˆè¦: Mutex or lock-freeæ§‹é€ ï¼‰
        processed.slice_mut(s![.., .., frame_idx, .., ..])
            .assign(&processed_frame);
    });

    processed
}
```

### 4.3 ğŸ”® Elixir: å‹•ç”»é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹

```elixir
# lib/video_service/generator.ex
defmodule VideoService.Generator do
  use GenServer

  # Rustler NIFçµ±åˆ
  use Rustler, otp_app: :video_service, crate: "video_generator"

  # NIF functions
  def generate_video_nif(_prompt, _num_frames), do: :erlang.nif_error(:nif_not_loaded)

  # GenServer callbacks
  def start_link(_) do
    GenServer.start_link(__MODULE__, %{}, name: __MODULE__)
  end

  def init(state) do
    {:ok, state}
  end

  def handle_call({:generate, prompt, num_frames}, _from, state) do
    # Dirty schedulerã§å®Ÿè¡Œï¼ˆGPUè¨ˆç®—ã®ãŸã‚ï¼‰
    Task.start(fn ->
      video_data = generate_video_nif(prompt, num_frames)
      # çµæœã‚’Phoenix Channelã§é…ä¿¡
      VideoService.Endpoint.broadcast("video:lobby", "generation_complete", %{
        video: Base.encode64(video_data),
        prompt: prompt
      })
    end)

    {:reply, :ok, state}
  end
end

# lib/video_service_web/channels/video_channel.ex
defmodule VideoServiceWeb.VideoChannel do
  use Phoenix.Channel

  def join("video:lobby", _message, socket) do
    {:ok, socket}
  end

  def handle_in("generate", %{"prompt" => prompt, "num_frames" => num_frames}, socket) do
    VideoService.Generator.generate(prompt, num_frames)
    {:noreply, socket}
  end
end

# Supervisor
defmodule VideoService.Application do
  use Application

  def start(_type, _args) do
    children = [
      VideoServiceWeb.Endpoint,
      {VideoService.Generator, []},
      {Task.Supervisor, name: VideoService.TaskSupervisor}
    ]

    opts = [strategy: :one_for_one, name: VideoService.Supervisor]
    Supervisor.start_link(children, opts)
  end
end
```

### 4.4 3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Elixiråˆ†æ•£ã‚µãƒ¼ãƒ“ã‚¹                      â”‚
â”‚  Phoenix WebSocket  â†’  å‹•ç”»ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä»˜              â”‚
â”‚       â†“                                                     â”‚
â”‚  GenServer  â†’  è² è·åˆ†æ•£ï¼ˆ10ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼‰             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â†“ Rustler NIF (C-ABI)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Rustæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³                        â”‚
â”‚  ONNX Runtime  â†’  LTX-Video DiTæ¨è«–                         â”‚
â”‚  ä¸¦åˆ—ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†  â†’  Rayonã§8ã‚³ã‚¢æ´»ç”¨                      â”‚
â”‚  é‡å­åŒ–(FP16)  â†’  ãƒ¡ãƒ¢ãƒªå‰Šæ¸›                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â†“ jlrs (Juliaâ†”Rust FFI)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Juliaè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³                   â”‚
â”‚  Lux.jl  â†’  Video Diffusionè¨“ç·´                             â”‚
â”‚  Reactant  â†’  GPU AOTã‚³ãƒ³ãƒ‘ã‚¤ãƒ«                             â”‚
â”‚  DataLoader  â†’  é«˜é€Ÿå‹•ç”»ãƒãƒƒãƒå‡¦ç†                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å½¹å‰²åˆ†æ‹…**:

| è¨€èª | å½¹å‰² | ä½¿ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª | å¼·ã¿ |
|:-----|:-----|:---------------|:-----|
| âš¡ Julia | è¨“ç·´ãƒ»å®Ÿé¨“ | Lux.jl, Reactant, VideoIO | æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1ã€GPUæœ€é©åŒ– |
| ğŸ¦€ Rust | æ¨è«–æœ€é©åŒ– | ort, ndarray, rayon | ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã€ä¸¦åˆ—å‡¦ç† |
| ğŸ”® Elixir | ã‚µãƒ¼ãƒ“ãƒ³ã‚°ãƒ»åˆ†æ•£ | Phoenix, Rustler, GenServer | è€éšœå®³æ€§ã€ä¸¦è¡Œæ€§ |

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” å‹•ç”»ç”Ÿæˆã®æœ€å‰ç·šãƒ‡ãƒ¢

### 5.1 ğŸ¯ SmolVLM2-256Må‹•ç”»ç†è§£ãƒ‡ãƒ¢

**ç›®æ¨™**: 256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§GPUãƒ¡ãƒ¢ãƒª1.38GBã®è¶…è»½é‡ãƒ¢ãƒ‡ãƒ«ã§å‹•ç”»ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã€‚

#### 5.1.1 SmolVLM2ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```julia
# requirements: PythonCall.jl + Pythonç’°å¢ƒ:
# pip install transformers>=4.40.0 torch>=2.0.0 pillow opencv-python

using PythonCall

transformers = pyimport("transformers")
torch        = pyimport("torch")
PIL_Image    = pyimport("PIL.Image")
np           = pyimport("numpy")

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆHugging Faceï¼‰
model_id  = "HuggingFaceTB/SmolVLM2-256M-Video-Instruct"
processor = transformers.AutoProcessor.from_pretrained(model_id)
model     = transformers.AutoModelForVision2Seq.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

# å‹•ç”»èª­ã¿è¾¼ã¿ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºï¼‰
function load_video_frames(video_path, num_frames=8)
    cv2 = pyimport("cv2")
    cap = cv2.VideoCapture(video_path)
    total_frames = pyconvert(Int, cap.get(cv2.CAP_PROP_FRAME_COUNT))
    step = total_frames Ã· num_frames

    frames = Py[]
    for i in 0:(num_frames - 1)
        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)
        ret_frame = cap.read()
        ret   = pyconvert(Bool, ret_frame[0])
        frame = ret_frame[1]
        if ret
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            push!(frames, PIL_Image.fromarray(frame_rgb))
        end
    end
    cap.release()
    return frames
end

# å‹•ç”»ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ
video_path = "sample_video.mp4"
frames = load_video_frames(video_path, 8)

prompt = "Describe what is happening in this video."

# ãƒ—ãƒ­ã‚»ãƒƒã‚µã§å…¥åŠ›æº–å‚™
inputs = processor(
    text=prompt,
    images=pylist(frames),
    return_tensors="pt"
).to(model.device, dtype=torch.float16)

# æ¨è«–ï¼ˆtorch.no_grad() ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰
no_grad_ctx = torch.no_grad()
no_grad_ctx.__enter__()
outputs = model.generate(
    inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_new_tokens=100,
    do_sample=false
)
no_grad_ctx.__exit__(nothing, nothing, nothing)

# ãƒ‡ã‚³ãƒ¼ãƒ‰
caption = pyconvert(String, processor.batch_decode(outputs, skip_special_tokens=true)[0])
println("Video Caption: $(caption)")

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
if pyconvert(Bool, torch.cuda.is_available())
    allocated = pyconvert(Float64, torch.cuda.memory_allocated()) / 1024^3
    println("GPU Memory Used: $(round(allocated, digits=2)) GB")
end
```

**å‡ºåŠ›ä¾‹**:
```
Video Caption: A person is walking down a street, carrying a bag. They pass by several shops and a car drives by.
GPU Memory Used: 1.42 GB
```

#### 5.1.2 å‹•ç”»QAãƒ‡ãƒ¢

```julia
# è¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã®Visual Question Answering
questions = [
    "What is the person wearing?",
    "How many cars are visible?",
    "What is the weather like?",
]

for question in questions
    inputs = processor(
        text=question,
        images=pylist(frames),
        return_tensors="pt"
    ).to(model.device, dtype=torch.float16)

    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=50
    )
    answer = pyconvert(String, processor.batch_decode(outputs, skip_special_tokens=true)[0])
    println("Q: $(question)")
    println("A: $(answer)\n")
end
```

**SmolVLM2ã®ç‰¹å¾´**:

| æŒ‡æ¨™ | SmolVLM2-256M | GPT-4V | LLaVA-1.5-7B |
|:-----|:--------------|:-------|:-------------|
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | 256M | ä¸æ˜ï¼ˆæ¨å®š1T+ï¼‰ | 7B |
| GPU ãƒ¡ãƒ¢ãƒª | 1.38 GB | 40+ GB | 14 GB |
| Video-MME Score | 41.5 | 59.9 | 38.2 |
| å‹•ç”»å¯¾å¿œ | âœ… | âœ… | âŒ |
| ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ | âœ… | âŒ | âœ… |

> **Note:** **Trojan Horseç™ºå‹•**: 256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å‹•ç”»ç†è§£ãŒå‹•ãã€‚Raspberry Pi 5ï¼ˆ8GB RAMï¼‰ã§ã‚‚æ¨è«–å¯èƒ½ â†’ **ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®ãƒ“ãƒ‡ã‚ªAI**ãŒç¾å®Ÿã«ã€‚

### 5.2 ğŸ¯ LTX-Videoå‹•ç”»ç”Ÿæˆãƒ‡ãƒ¢

**ç›®æ¨™**: DiT+VAEçµ±åˆå‹ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ã‚’ç”Ÿæˆã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ï¼ˆH100ã§2ç§’ï¼‰ã€‚

#### 5.2.1 LTX-Videoã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```julia
# LTX-Videoæ¨è«–ï¼ˆHugging Face Diffusersçµ±åˆï¼‰
using PythonCall

diffusers = pyimport("diffusers")
torch     = pyimport("torch")
imageio   = pyimport("imageio")

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ­ãƒ¼ãƒ‰
pipe = diffusers.LTXVideoPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    torch_dtype=torch.float16
).to("cuda")

# ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt = "A serene underwater scene with colorful coral and fish swimming"

# å‹•ç”»ç”Ÿæˆ
video_frames = pipe(
    prompt=prompt,
    num_frames=121,  # 5ç§’ @ 24fps
    height=512,
    width=768,
    num_inference_steps=50,
    guidance_scale=7.5
).frames[0]

# å‹•ç”»ä¿å­˜
imageio.mimsave("output_video.mp4", video_frames, fps=24)

println("Generated $(pyconvert(Int, pylen(video_frames))) frames")
```

#### 5.2.2 Image-to-Videoå¤‰æ›

```julia
PIL_Image = pyimport("PIL.Image")

# é–‹å§‹ãƒ•ãƒ¬ãƒ¼ãƒ æŒ‡å®š
start_image = PIL_Image.open("start_frame.png")

video_frames = pipe(
    prompt="A bird taking flight from a tree branch",
    image=start_image,
    num_frames=121,
    height=512,
    width=768,
    num_inference_steps=50
).frames[0]

imageio.mimsave("i2v_output.mp4", video_frames, fps=24)
```

#### 5.2.3 LTX-Videoã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

**3ã¤ã®é©æ–°**:

1. **çµ±åˆVAE-DiTè¨­è¨ˆ**: VAEã¨DiTã‚’åˆ¥ã€…ã«è¨“ç·´ã›ãšã€End-to-Endã§æœ€é©åŒ–ã€‚

   ```
   å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
       â†“ T5 Encoder
   Text Embedding
       â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  LTX-Videoçµ±åˆãƒ¢ãƒ‡ãƒ«       â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
   â”‚  â”‚ VAE Encoder        â”‚   â”‚
   â”‚  â”‚ (Spacetime Patch)  â”‚   â”‚
   â”‚  â”‚ 32Ã—32Ã—8 â†’ 1 token  â”‚   â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
   â”‚           â†“                â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
   â”‚  â”‚ DiT Blocks (L=28)  â”‚   â”‚
   â”‚  â”‚ Cross-Attention    â”‚   â”‚
   â”‚  â”‚ + Self-Attention   â”‚   â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
   â”‚           â†“                â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
   â”‚  â”‚ VAE Decoder        â”‚   â”‚
   â”‚  â”‚ + Final Denoise    â”‚   â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
   ç”Ÿæˆå‹•ç”» (768Ã—512Ã—24fpsÃ—5sec)
   ```

2. **é«˜åœ§ç¸®VAE**: 1:192åœ§ç¸®ï¼ˆCogVideoXã¨åŒç­‰ï¼‰ã‚’1:384ã«æ”¹å–„ã€‚

   - Spatial: 32Ã—32ãƒ”ã‚¯ã‚»ãƒ« â†’ 1 token
   - Temporal: 8ãƒ•ãƒ¬ãƒ¼ãƒ  â†’ 1 token
   - åˆè¨ˆ: $32 \times 32 \times 8 = 8192$ pixels/token

3. **æœ€çµ‚denoiseã‚’VAE Decoderã«çµ±åˆ**: Diffusionã®æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã§å®Ÿè¡Œ â†’ ç´°éƒ¨ã®é®®æ˜ã•å‘ä¸Šã€‚

**æ€§èƒ½**:

| ãƒ¢ãƒ‡ãƒ« | è§£åƒåº¦ | ãƒ•ãƒ¬ãƒ¼ãƒ æ•° | ç”Ÿæˆæ™‚é–“ (H100) | FVD â†“ |
|:-------|:-------|:----------|:----------------|:------|
| LTX-Video | 768Ã—512 | 121 (5ç§’) | 2.0ç§’ | 242 |
| CogVideoX | 720Ã—480 | 49 (2ç§’) | 10.0ç§’ | 255 |
| Open-Sora 2.0 | 720p | 240 (10ç§’) | 35.0ç§’ | 280 |

**FVDï¼ˆFrÃ©chet Video Distanceï¼‰**: ä½ã„ã»ã©é«˜å“è³ªï¼ˆç”»åƒã®FIDã«ç›¸å½“ï¼‰ã€‚

### 5.3 SmolVLM2ç†è§£ vs LTX-Videoç”Ÿæˆã®å¯¾æ¯”å®Ÿé¨“

**å®Ÿé¨“è¨­è¨ˆ**: åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§LTX-VideoãŒç”Ÿæˆã—ãŸå‹•ç”»ã‚’ã€SmolVLM2ã«ç†è§£ã•ã›ã‚‹ã€‚

```julia
# Step 1: LTX-Videoã§å‹•ç”»ç”Ÿæˆ
prompt_generation = "A cat jumping from a table to a chair"
generated_frames = pipe(
    prompt=prompt_generation,
    num_frames=121,
    height=512,
    width=768
).frames[0]

imageio.mimsave("generated_cat.mp4", generated_frames, fps=24)

# Step 2: SmolVLM2ã§å‹•ç”»ç†è§£
# generated_frames[::15] ã«ç›¸å½“ï¼ˆç­‰é–“éš”ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
n_total = pyconvert(Int, pylen(generated_frames))
frames_for_vlm = [generated_frames[i] for i in 0:15:(n_total - 1)]
frames_pil = [PIL_Image.fromarray(f) for f in frames_for_vlm]

inputs_vlm = processor(
    text="Describe what is happening in this video in detail.",
    images=pylist(frames_pil),
    return_tensors="pt"
).to("cuda", dtype=torch.float16)

outputs_vlm = model.generate(
    inputs_vlm["input_ids"],
    attention_mask=inputs_vlm["attention_mask"],
    max_new_tokens=150
)
description = pyconvert(String, processor.batch_decode(outputs_vlm, skip_special_tokens=true)[0])

println("Original Prompt: $(prompt_generation)")
println("SmolVLM2 Description: $(description)")

# Step 3: ä¸€è‡´åº¦è©•ä¾¡ï¼ˆBERTScoreï¼‰
bert_score_mod = pyimport("bert_score")

P, R, F1 = bert_score_mod.score([description], [prompt_generation], lang="en")
println("BERTScore F1: $(round(pyconvert(Float64, F1.item()), digits=3))")
```

**çµæœä¾‹**:
```
Original Prompt: A cat jumping from a table to a chair
SmolVLM2 Description: The video shows a cat on a wooden table. The cat then jumps off the table and lands on a nearby chair.
BERTScore F1: 0.782
```

**è€ƒå¯Ÿ**:

| è¦³ç‚¹ | åˆ†æ |
|:-----|:-----|
| å‹•ä½œã®æ­£ç¢ºæ€§ | "jumping"ã‚’æ­£ã—ãèªè­˜ã€æ–¹å‘ï¼ˆtableâ†’chairï¼‰ã‚‚ä¸€è‡´ |
| ç´°éƒ¨ã®è£œå®Œ | "wooden table"ãªã©ç”Ÿæˆå´ãŒæŒ‡å®šã—ãªã„è©³ç´°ã‚‚æ¨æ¸¬ |
| ä¸€è‡´åº¦ã‚¹ã‚³ã‚¢ | F1=0.782ã¯é«˜å“è³ªï¼ˆ>0.7ã§è‰¯å¥½ã¨ã•ã‚Œã‚‹ï¼‰ |

> **Note:** **çµ±åˆãƒ‡ãƒ¢ã®æ„ç¾©**: SmolVLM2ï¼ˆç†è§£ï¼‰ã¨LTX-Videoï¼ˆç”Ÿæˆï¼‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€**Video-to-Text-to-Video**ã®ãƒ«ãƒ¼ãƒ—ãŒå¯èƒ½ã«ã€‚æ—¢å­˜å‹•ç”»ã®ç·¨é›†æŒ‡ç¤ºã‚„ã€å‹•ç”»è¦ç´„â†’å†ç”Ÿæˆãªã©ã®å¿œç”¨ãŒé–‹ã‘ã‚‹ã€‚

### 5.4 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ â€” Videoç”Ÿæˆã®ç†è§£åº¦ç¢ºèª

#### ãƒ†ã‚¹ãƒˆâ‘  æ™‚ç©ºé–“Attentionã®è¨ˆç®—é‡

**å•é¡Œ**: å‹•ç”» $T=120$ãƒ•ãƒ¬ãƒ¼ãƒ ã€$H=W=64$ã€åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ$D=512$ã«ã¤ã„ã¦ã€Spatial Attentionã¨Temporal Attentionã®è¨ˆç®—é‡ã‚’æ±‚ã‚ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**Spatial Attention**ï¼ˆå„ãƒ•ãƒ¬ãƒ¼ãƒ å†…ï¼‰:
- 1ãƒ•ãƒ¬ãƒ¼ãƒ ã®Tokenæ•°: $N_s = \frac{H}{16} \times \frac{W}{16} = 4 \times 4 = 16$
- Attentionè¨ˆç®—é‡ï¼ˆ1ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰: $O(N_s^2 D) = O(16^2 \times 512) = O(131K)$
- å…¨ãƒ•ãƒ¬ãƒ¼ãƒ : $O(131K \times 120) = O(15.7M)$

**Temporal Attention**ï¼ˆå„ãƒ”ã‚¯ã‚»ãƒ«ä½ç½®ã®æ™‚é–“ç³»åˆ—ï¼‰:
- ãƒ”ã‚¯ã‚»ãƒ«ä½ç½®æ•°: $4 \times 4 = 16$
- 1ä½ç½®ã®Attentionè¨ˆç®—é‡: $O(T^2 D) = O(120^2 \times 512) = O(7.37M)$
- å…¨ä½ç½®: $O(7.37M \times 16) = O(118M)$

**çµè«–**: Temporal Attentionã®æ–¹ãŒè¨ˆç®—é‡ãŒå¤§ãã„ï¼ˆç´„7.5å€ï¼‰ã€‚

</details>

#### ãƒ†ã‚¹ãƒˆâ‘¡ 3D VAEåœ§ç¸®ç‡ã®è¨ˆç®—

**å•é¡Œ**: å…¥åŠ› $T=49$ãƒ•ãƒ¬ãƒ¼ãƒ ã€$H=W=768$ã€$C=3$ã€‚å‡ºåŠ› $T'=13$ã€$H'=W'=96$ã€$C'=16$ã€‚åœ§ç¸®ç‡ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

å…¥åŠ›ã‚µã‚¤ã‚º: $49 \times 768 \times 768 \times 3 = 86.7M$ pixels
å‡ºåŠ›ã‚µã‚¤ã‚º: $13 \times 96 \times 96 \times 16 = 1.93M$ elements

åœ§ç¸®ç‡: $r = \frac{86.7M}{1.93M} \approx 45$

ãŸã ã—ã€è«–æ–‡ã§ã¯æ™‚ç©ºé–“åˆã‚ã›ã¦**192å€**ã¨è¨˜è¼‰ â†’ EncoderãŒè¤‡æ•°æ®µéšã§åœ§ç¸®ã—ã¦ã„ã‚‹ã¨æ¨æ¸¬ã€‚

</details>

#### ãƒ†ã‚¹ãƒˆâ‘¢ Optical Flow Lossã®æ„å‘³

**å•é¡Œ**: Optical Flow LossãŒå°ã•ã„å‹•ç”»ã¯ã€ã©ã®ã‚ˆã†ãªæ€§è³ªã‚’æŒã¤ã‹ï¼Ÿ3ã¤ç­”ãˆã‚ˆã€‚

<details><summary>è§£ç­”</summary>

1. **ç‰©ç†çš„ã«ä¸€è²«ã—ãŸå‹•ã**: ãƒ”ã‚¯ã‚»ãƒ«ãŒæ»‘ã‚‰ã‹ã«ç§»å‹•ï¼ˆç¬é–“ç§»å‹•ã—ãªã„ï¼‰
2. **æ™‚é–“çš„é€£ç¶šæ€§**: ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã§å¤§ããªè·³èºãŒãªã„
3. **äºˆæ¸¬å¯èƒ½ãªè»Œè·¡**: æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½ç½®ãŒç¾åœ¨ã®ãƒ•ãƒ­ãƒ¼ã‹ã‚‰äºˆæ¸¬å¯èƒ½

é€†ã«ã€LossãŒå¤§ãã„ = ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè·³èºï¼ˆãƒãƒ©ã¤ãï¼‰ã€‚

</details>

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” Tiny Video Diffusion on Moving MNIST

**ç›®æ¨™**: Moving MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ60Ã—60ã€20ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã§ã€ç°¡æ˜“Video Diffusionã‚’è¨“ç·´ã€‚

#### ãƒãƒ£ãƒ¬ãƒ³ã‚¸â‘  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ

```julia
using Images, Random

function generate_moving_mnist(num_samples=100, num_frames=20, img_size=60)
    # MNISTæ•°å­—ã‚’1ã¤é¸ã‚“ã§ãƒ©ãƒ³ãƒ€ãƒ ã«ç§»å‹•ã•ã›ã‚‹
    dataset = [let sx = rand(1:40), sy = rand(1:40), dx = rand(-2:2), dy = rand(-2:2)
        frames = [begin
            frame = zeros(Float32, img_size, img_size)
            x = clamp(sx + t*dx, 1, img_size - 10)
            y = clamp(sy + t*dy, 1, img_size - 10)
            @views frame[Int(x):Int(x)+9, Int(y):Int(y)+9] .= 1.0f0
            frame
        end for t in 1:num_frames]
        cat(frames..., dims=3)
    end for _ in 1:num_samples]

    return dataset
end

dataset = generate_moving_mnist(100, 20, 60)
```

#### ãƒãƒ£ãƒ¬ãƒ³ã‚¸â‘¡ Simple Video Diffusionãƒ¢ãƒ‡ãƒ«

```julia
using Flux, Functors

# 2D+æ™‚é–“æ–¹å‘ã®ç°¡æ˜“ãƒ¢ãƒ‡ãƒ«ï¼ˆ3D Convã®ä»£ã‚ã‚Šï¼‰
struct SimpleVideoDiffusion
    spatial_conv::Chain
    temporal_conv::Conv
    out_conv::Conv
end

@functor SimpleVideoDiffusion

function SimpleVideoDiffusion()
    spatial_conv = Chain(
        Conv((3, 3), 1 => 16, relu, pad=1),
        Conv((3, 3), 16 => 32, relu, pad=1)
    )
    temporal_conv = Conv((3,), 32 => 32, relu, pad=1)
    out_conv = Conv((1, 1), 32 => 1)

    return SimpleVideoDiffusion(spatial_conv, temporal_conv, out_conv)
end

function (model::SimpleVideoDiffusion)(x)
    # x: (H, W, T, B)
    H, W, T, B = size(x)

    # ç©ºé–“æ–¹å‘ã®å‡¦ç†ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ï¼‰
    x_spatial = cat([model.spatial_conv(reshape(x[:, :, t, :], H, W, 1, B)) for t in 1:T]..., dims=4)  # (H, W, 32, T*B)

    # æ™‚é–“æ–¹å‘ã®å‡¦ç†ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ã”ã¨ï¼‰
    # ç°¡ç•¥åŒ–: å…¨ä½“ã«æ™‚é–“Convã‚’é©ç”¨
    x_temporal = reshape(x_spatial, H, W, 32*T, B)
    x_temporal = model.temporal_conv(x_temporal)

    # å‡ºåŠ›
    out = model.out_conv(x_temporal)
    return reshape(out, H, W, T, B)
end
```

#### ãƒãƒ£ãƒ¬ãƒ³ã‚¸â‘¢ è¨“ç·´ã¨ç”Ÿæˆ

```julia
# è¨“ç·´
model = SimpleVideoDiffusion()
opt = Flux.setup(Adam(1e-3), model)
Î²_schedule = LinRange(1e-4, 0.02, 50)

for epoch in 1:10
    for batch_idx in 1:10
        x0 = dataset[batch_idx]  # (H, W, T)
        x0 = reshape(x0, size(x0)..., 1)  # ãƒãƒƒãƒæ¬¡å…ƒè¿½åŠ 

        t = rand(1:50)
        xt, Îµ = add_noise(x0, t, Î²_schedule)

        loss, grads = Flux.withgradient(model) do m
            Îµ_pred = m(xt)
            mean((Îµ_pred .- Îµ).^2)
        end

        Flux.update!(opt, model, grads[1])
    end
end

# ç”Ÿæˆ
generated = ddim_sample(model, 20, 60, 60, Î²_schedule, num_steps=20)
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**: ç™½ã„æ­£æ–¹å½¢ãŒæ»‘ã‚‰ã‹ã«ç§»å‹•ã™ã‚‹20ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‹•ç”»ã€‚

> **Note:** **å­¦ç¿’ã®ãƒã‚¤ãƒ³ãƒˆ**:
> - Temporal Coherenceã®é‡è¦æ€§ã‚’ä½“æ„Ÿ
> - ç°¡æ˜“3D Convã§ã‚‚æ™‚é–“çš„ä¸€è²«æ€§ã¯å­¦ç¿’å¯èƒ½
> - å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆCogVideoXç­‰ï¼‰ã¯ã“ã‚Œã‚’å¤§è¦æ¨¡åŒ–+Attentionè¿½åŠ 

---


> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. LTX-VideoãŒ700Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§FLUXç­‰ã®å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã«è¿‘ã„å“è³ªã‚’é”æˆã§ãã‚‹ã€ŒFlexible Attentionã€ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. SmolVLM2ã®256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§GPUãƒ¡ãƒ¢ãƒª1.38GBã«åã¾ã‚‹è¨­è¨ˆä¸Šã®å·¥å¤«ã‚’ã€attentionè¨ˆç®—ã®è¦³ç‚¹ã‹ã‚‰è¿°ã¹ã‚ˆã€‚

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” 2024-2025æœ€å‰ç·šã®å‹•ç”»ç”Ÿæˆ + ã¾ã¨ã‚

### 6.1 HunyuanVideo â€” Tencentã®13Bå•†ç”¨ç´šãƒ¢ãƒ‡ãƒ«

#### 6.1.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

**è¦æ¨¡**: 13Bï¼ˆ130å„„ï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ â€” ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹æœ€å¤§ç´šã€‚

**ä¸»è¦æŠ€è¡“**:

1. **Causal 3D VAE**: æ™‚é–“æ–¹å‘ã«Causalï¼ˆéå»ã®ã¿å‚ç…§ï¼‰ãª3D VAE
   - æ¨è«–æ™‚ã«è‡ªå·±å›å¸°çš„ã«ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆå¯èƒ½
   - Latentåœ§ç¸®ç‡: ç©ºé–“8Ã—8ã€æ™‚é–“4 â†’ åˆè¨ˆ256å€

2. **Expert Transformer**:
   ```
   DiT Block
       â†“
   Expert Router (Gating)
       â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
   â”‚Expertâ”‚Expertâ”‚Expertâ”‚  â† 8å€‹ã®Expertï¼ˆå„2B paramsï¼‰
   â”‚  1   â”‚  2   â”‚  3   â”‚
   â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
       â†“ Top-2é¸æŠ
   Weighted Combination
   ```

   - MoEï¼ˆMixture of Expertsï¼‰ã‚’å‹•ç”»ç”Ÿæˆã«å¿œç”¨
   - å„ãƒ•ãƒ¬ãƒ¼ãƒ ã§æœ€é©ãªExpertã‚’å‹•çš„ã«é¸æŠ
   - è¨“ç·´æ™‚ã¯å…¨Expertã€æ¨è«–æ™‚ã¯Top-2ã®ã¿ â†’ è¨ˆç®—é‡å‰Šæ¸›

3. **Joint Image-Video Training**:
   - ç”»åƒï¼ˆT=1ï¼‰ã¨å‹•ç”»ï¼ˆT>1ï¼‰ã‚’åŒæ™‚è¨“ç·´
   - ç”»åƒã§é«˜å“è³ªãªç©ºé–“è¡¨ç¾ã‚’å­¦ç¿’ â†’ å‹•ç”»ã«è»¢ç§»

#### 6.1.2 æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

**VBenchæ¯”è¼ƒ**ï¼ˆ16æŒ‡æ¨™ã®ç·åˆã‚¹ã‚³ã‚¢ï¼‰:

| ãƒ¢ãƒ‡ãƒ« | Overall Score | ä¸»è¦³è©•ä¾¡ | Open-Source |
|:-------|:--------------|:---------|:------------|
| HunyuanVideo | **79.6** | **80.2** | âœ… |
| Runway Gen-3 Alpha | 77.8 | 78.5 | âŒ |
| Luma 1.6 | 76.3 | 75.9 | âŒ |
| CogVideoX-5B | 74.2 | 73.1 | âœ… |
| Open-Sora 2.0 | 71.8 | 70.5 | âœ… |

**çµè«–**: ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åˆã®ãƒˆãƒƒãƒ—ãƒ†ã‚£ã‚¢å“è³ªã€‚

### 6.2 Open-Sora 2.0 â€” $200kã§å•†ç”¨ãƒ¬ãƒ™ãƒ«

#### 6.2.1 åŠ¹ç‡åŒ–ã®4æœ¬æŸ±

**è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«**: "Training a Commercial-Level Video Generation Model in $200k"

1. **ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**:
   - 8.7Må‹•ç”» â†’ 4Må‹•ç”»ã«å³é¸ï¼ˆå“è³ªãƒ•ã‚£ãƒ«ã‚¿ï¼‰
   - Aesthetic Scoreï¼ˆCLIP-basedï¼‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
   - å‹•ç”»ã®å¤šæ§˜æ€§ã‚’ä¿ã¡ã¤ã¤ä½å“è³ªã‚’æ’é™¤

2. **ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æœ€é©åŒ–**:
   - **3D Full Attention â†’ Spatial+Temporalåˆ†é›¢**
     $$
     \text{Full Attn}: O\left(\left(\frac{THW}{p^3}\right)^2\right) \quad \to \quad \text{Sep Attn}: O\left(\frac{T^2HW}{p^3} + \frac{THW^2}{p^2}\right)
     $$
   - è¨ˆç®—é‡ã‚’ç´„1/10ã«å‰Šæ¸›

3. **è¨“ç·´æˆ¦ç•¥**:
   - **Curriculum Learning**: ä½è§£åƒåº¦â†’é«˜è§£åƒåº¦
   - **Progressive Frame**: 16ãƒ•ãƒ¬ãƒ¼ãƒ â†’48ãƒ•ãƒ¬ãƒ¼ãƒ â†’240ãƒ•ãƒ¬ãƒ¼ãƒ 
   - **Mixed Precision**: BF16è¨“ç·´

4. **ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–**:
   - **ZeRO-3**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
   - **FlashAttention-2**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
   - **Gradient Checkpointing**: ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ãƒ‰è¨ˆç®—é‡

**è¨“ç·´ã‚³ã‚¹ãƒˆ**:
- 1280 Ã— A100-80G GPU
- 3æ—¥é–“
- ç·ã‚³ã‚¹ãƒˆ: **$200,000**

**æ¯”è¼ƒ** (æ¨å®š):

| ãƒ¢ãƒ‡ãƒ« | æ¨å®šè¨“ç·´ã‚³ã‚¹ãƒˆ | GPUæ™‚é–“ | ãƒ‡ãƒ¼ã‚¿é‡ |
|:-------|:---------------|:--------|:---------|
| Sora (OpenAI) | $5M+ | ä¸æ˜ | ä¸æ˜ |
| Runway Gen-3 | $2M+ | ä¸æ˜ | ä¸æ˜ |
| Open-Sora 2.0 | **$200K** | 3,840 GPU-days | 4Må‹•ç”» |

**æ°‘ä¸»åŒ–ã®è¡æ’ƒ**: å•†ç”¨ç´šãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚³ã‚¹ãƒˆã‚’1/10ã«å‰Šæ¸› â†’ ç ”ç©¶æ©Ÿé–¢ãƒ»ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã§ã‚‚åˆ°é”å¯èƒ½ã€‚

### 6.3 Wan 2.1 â€” Alibabaã®å¤šæ©Ÿèƒ½å‹•ç”»ç”Ÿæˆ

#### 6.3.1 5ã¤ã®æ©Ÿèƒ½ã‚’1ãƒ¢ãƒ‡ãƒ«ã§çµ±åˆ

**Unified Model Architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Wan 2.1 Unified Model (14B)         â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Condition Encoder                   â”‚  â”‚
â”‚  â”‚  ãƒ»Text (T5)                        â”‚  â”‚
â”‚  â”‚  ãƒ»Image (CLIP ViT)                 â”‚  â”‚
â”‚  â”‚  ãƒ»Video Frames (Temporal Encoder)  â”‚  â”‚
â”‚  â”‚  ãƒ»Audio (Wav2Vec 2.0)              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  DiT Backbone (28 layers)            â”‚  â”‚
â”‚  â”‚  ãƒ»Cross-Attention (æ¡ä»¶ä»˜ã‘)        â”‚  â”‚
â”‚  â”‚  ãƒ»Self-Attention (æ™‚ç©ºé–“)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Task-Specific Heads                 â”‚  â”‚
â”‚  â”‚  ãƒ»T2V: ãƒ†ã‚­ã‚¹ãƒˆâ†’å‹•ç”»                â”‚  â”‚
â”‚  â”‚  ãƒ»I2V: ç”»åƒâ†’å‹•ç”»                    â”‚  â”‚
â”‚  â”‚  ãƒ»V2V: å‹•ç”»ç·¨é›†                     â”‚  â”‚
â”‚  â”‚  ãƒ»T2I: ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒ                â”‚  â”‚
â”‚  â”‚  ãƒ»V2A: å‹•ç”»â†’éŸ³å£°                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.3.2 æ€§èƒ½ã¨å®Ÿç”¨æ€§

**T2Væ€§èƒ½**:

| æŒ‡æ¨™ | Wan 2.1-14B | Sora | CogVideoX-5B |
|:-----|:------------|:-----|:-------------|
| Resolution | 720p | 1080p | 720p |
| Max Duration | 10ç§’ | 20ç§’ | 6ç§’ |
| FVD â†“ | 268 | ä¸æ˜ | 255 |
| æ¨è«–é€Ÿåº¦ (4090) | 3.5åˆ†/5ç§’ | N/A | 10åˆ†/2ç§’ |

**å•†ç”¨åˆ©ç”¨**: Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ â†’ å®Œå…¨ã«å•†ç”¨åˆ©ç”¨å¯èƒ½ã€‚

**ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°**: 220ä¸‡+ (2025å¹´5æœˆæ™‚ç‚¹) â†’ å®Ÿå‹™ã§ã®æ¡ç”¨ãŒé€²ã‚“ã§ã„ã‚‹è¨¼æ‹ ã€‚

### 6.4 å‹•ç”»ç”Ÿæˆè©•ä¾¡æŒ‡æ¨™ã®æ·±å €ã‚Š

#### 6.4.1 FVDï¼ˆFrÃ©chet Video Distanceï¼‰

**å®šç¾©**: ç”»åƒã®FIDã‚’å‹•ç”»ã«æ‹¡å¼µã€‚

$$
\text{FVD} = \|\boldsymbol{\mu}_r - \boldsymbol{\mu}_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
$$

- $\boldsymbol{\mu}_r, \Sigma_r$: å®Ÿå‹•ç”»ã®ç‰¹å¾´é‡ï¼ˆI3Dç‰¹å¾´ï¼‰ã®å¹³å‡ãƒ»å…±åˆ†æ•£
- $\boldsymbol{\mu}_g, \Sigma_g$: ç”Ÿæˆå‹•ç”»ã®ç‰¹å¾´é‡

**I3Dï¼ˆInflated 3D ConvNetï¼‰**: Kinetics-400ã§è¨“ç·´ã•ã‚ŒãŸå‹•ç”»åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã€‚æœ€çµ‚å±¤ã®ç‰¹å¾´é‡ï¼ˆ2048æ¬¡å…ƒï¼‰ã‚’ä½¿ç”¨ã€‚

**å•é¡Œç‚¹**:
- I3Dã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆKineticsï¼‰ã«åã‚Š â†’ å‹•ç”»ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ã‚ˆã£ã¦ã‚¹ã‚³ã‚¢ãŒä¸å®‰å®š
- æ™‚é–“çš„ä¸€è²«æ€§ã‚’ç›´æ¥è©•ä¾¡ã—ãªã„

#### 6.4.2 Temporal CoherenceæŒ‡æ¨™

**CLIP Temporal Consistency**:

```julia
using LinearAlgebra, Statistics

# CLIP temporal consistency: mean cosine similarity between consecutive frame embeddings
# embeddings: Matrix{Float32} of shape (T, D), each row is a frame embedding
function temporal_consistency(embeddings::Matrix{Float32})::Float64
    # Normalize each row
    norms = [norm(embeddings[i, :]) for i in axes(embeddings, 1)]
    E_n = embeddings ./ max.(norms, eps(Float32))'

    # Consecutive cosine similarities
    sims = [dot(E_n[i, :], E_n[i+1, :]) for i in 1:size(E_n, 1)-1]
    return mean(sims)
end

# Numerical check: identical frames â†’ similarity = 1.0
let e = rand(Float32, 5, 512)
    tc = temporal_consistency(vcat(e, e))  # duplicated frames
    @assert tc â‰ˆ 1.0f0 atol=1e-4 "identical frames should give TC=1"
    println("temporal_consistency check: $(round(tc, digits=4))")  # â†’ 1.0
end
```

**å¹³å‡ã‚¹ã‚³ã‚¢**: 0.9ä»¥ä¸ŠãŒé«˜å“è³ªï¼ˆæ»‘ã‚‰ã‹ãªå‹•ç”»ï¼‰ã€‚

#### 6.4.3 VBench â€” 16æ¬¡å…ƒè©•ä¾¡

**16æŒ‡æ¨™**ï¼ˆä¸€éƒ¨æŠœç²‹ï¼‰:

| ã‚«ãƒ†ã‚´ãƒª | æŒ‡æ¨™ | å†…å®¹ |
|:---------|:-----|:-----|
| Quality | Subject Consistency | ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¤–è¦³ä¸€è²«æ€§ |
|         | Background Consistency | èƒŒæ™¯ã®ä¸€è²«æ€§ |
|         | Aesthetic Quality | ç¾çš„å“è³ªï¼ˆCLIP-basedï¼‰ |
| Semantics | Object Class | ãƒ†ã‚­ã‚¹ãƒˆã§æŒ‡å®šã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ |
|           | Multiple Objects | è¤‡æ•°ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ­£ç¢ºæ€§ |
| Temporal | Motion Smoothness | å‹•ãã®æ»‘ã‚‰ã‹ã• |
|          | Dynamic Degree | å‹•ãã®åº¦åˆã„ |
| Physics | Human Action | äººé–“ã®å‹•ä½œã®è‡ªç„¶ã• |
|         | Physical Law | ç‰©ç†æ³•å‰‡éµå®ˆï¼ˆé‡åŠ›ç­‰ï¼‰ |

**ç·åˆã‚¹ã‚³ã‚¢**: 16æŒ‡æ¨™ã®å¹³å‡ã€‚80ç‚¹ä»¥ä¸Šã§å•†ç”¨ç´šã€‚

<details><summary>VBenchã‚¹ã‚³ã‚¢ã®è§£é‡ˆä¾‹</summary>

| ãƒ¢ãƒ‡ãƒ« | Subject Cons. | Motion Smooth | Physical Law | Overall |
|:-------|:--------------|:--------------|:-------------|:--------|
| Sora | 92.5 | 88.3 | **85.1** | 83.7 |
| HunyuanVideo | **93.2** | **89.7** | 82.4 | **79.6** |
| CogVideoX | 91.0 | 86.5 | 78.2 | 74.2 |

**åˆ†æ**:
- HunyuanVideoã¯ä¸€è²«æ€§ãƒ»æ»‘ã‚‰ã‹ã•ã§ãƒˆãƒƒãƒ—
- Soraã¯ç‰©ç†æ³•å‰‡ã®å­¦ç¿’ãŒæœ€ã‚‚é€²ã‚“ã§ã„ã‚‹
- å…¨ä½“ã‚¹ã‚³ã‚¢ã§HunyuanãŒSoraã«è¿«ã‚‹ï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ï¼‰

</details>

### 6.5 é•·æ™‚é–“å‹•ç”»ç”Ÿæˆã®3ã¤ã®æˆ¦ç•¥

#### æˆ¦ç•¥â‘  Autoregressive Extensionï¼ˆè‡ªå·±å›å¸°çš„æ‹¡å¼µï¼‰

**åŸç†**: ç”Ÿæˆã—ãŸæœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ¬¡ã®ç”Ÿæˆã®é–‹å§‹ãƒ•ãƒ¬ãƒ¼ãƒ ã«ä½¿ç”¨ã€‚

```
ç”Ÿæˆ1: ãƒã‚¤ã‚º â†’ ãƒ•ãƒ¬ãƒ¼ãƒ 1-16
ç”Ÿæˆ2: ãƒ•ãƒ¬ãƒ¼ãƒ 16 â†’ ãƒ•ãƒ¬ãƒ¼ãƒ 17-32
ç”Ÿæˆ3: ãƒ•ãƒ¬ãƒ¼ãƒ 32 â†’ ãƒ•ãƒ¬ãƒ¼ãƒ 33-48
...
```

**å•é¡Œç‚¹**: ã‚¨ãƒ©ãƒ¼ã®ç´¯ç©ï¼ˆDriftï¼‰ã€‚

**å¯¾ç­–**: Overlapï¼ˆé‡è¤‡ï¼‰æˆ¦ç•¥ã€‚
```
ç”Ÿæˆ1: ãƒ•ãƒ¬ãƒ¼ãƒ 1-20
ç”Ÿæˆ2: ãƒ•ãƒ¬ãƒ¼ãƒ 16-36 (4ãƒ•ãƒ¬ãƒ¼ãƒ é‡è¤‡)
ç”Ÿæˆ3: ãƒ•ãƒ¬ãƒ¼ãƒ 32-52
```

é‡è¤‡éƒ¨åˆ†ã§å¹³å‡ã‚’å–ã‚‹ â†’ æ»‘ã‚‰ã‹ãªæ¥ç¶šã€‚

**æ•°å¼çš„å®šå¼åŒ–**:

Overlapé ˜åŸŸã§ã®ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°é‡ã¿:
$$
w_{\text{blend}}(f) = \begin{cases}
\frac{f - f_{\text{start}}}{f_{\text{overlap}}} & f \in [f_{\text{start}}, f_{\text{start}} + f_{\text{overlap}}] \\
1 & \text{otherwise}
\end{cases}
$$

æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ :
$$
\mathbf{x}_f = w_{\text{blend}}(f) \cdot \mathbf{x}_f^{\text{new}} + (1 - w_{\text{blend}}(f)) \cdot \mathbf{x}_f^{\text{old}}
$$

**å®Ÿè£…ä¾‹ï¼ˆJuliaï¼‰**:

```julia
function autoregressive_video_generation(model, total_frames, chunk_size, overlap)
    all_frames = []
    current_frame = randn(Float32, H, W, C)  # åˆæœŸãƒã‚¤ã‚º

    for start_idx in 1:chunk_size-overlap:total_frames
        end_idx = min(start_idx + chunk_size - 1, total_frames)

        # ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
        chunk = generate_chunk(model, current_frame, chunk_size)

        if start_idx == 1
            # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã¯å…¨ã¦è¿½åŠ 
            push!(all_frames, chunk...)
        else
            # Overlapé ˜åŸŸã§ãƒ–ãƒ¬ãƒ³ãƒ‰
            for i in 1:overlap
                w = (i - 1) / overlap
                @. all_frames[end - overlap + i] = w * chunk[i] + (1 - w) * all_frames[end - overlap + i]
            end

            # æ®‹ã‚Šã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿½åŠ 
            append!(all_frames, chunk[overlap+1:end])
        end

        # æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ¬¡ã®é–‹å§‹ç‚¹ã«
        current_frame = chunk[end]
    end

    return all_frames
end
```

**Driftå•é¡Œã®ç†è«–çš„åˆ†æ**:

ç´¯ç©èª¤å·®ã®ä¸Šç•Œ:
$$
\mathbb{E}[\|\mathbf{x}_T - \mathbf{x}_T^{\text{gt}}\|^2] \leq T \cdot \epsilon^2
$$

ã“ã“ã§$\epsilon$ã¯1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šã®èª¤å·®ã€‚Overlapã§èª¤å·®ã‚’$\alpha < 1$å€ã«æŠ‘åˆ¶ã™ã‚‹ã¨:
$$
\mathbb{E}[\|\mathbf{x}_T - \mathbf{x}_T^{\text{gt}}\|^2] \leq T \cdot (\alpha\epsilon)^2
$$

#### æˆ¦ç•¥â‘¡ Key Frame + Interpolation

**åŸç†**: é‡è¦ãªã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿ã‚’ç”Ÿæˆ â†’ é–“ã‚’Interpolationã€‚

```
ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ : 1, 10, 20, 30, ...
Interpolation: ãƒ•ãƒ¬ãƒ¼ãƒ 1-10ã‚’Flowãƒ™ãƒ¼ã‚¹è£œé–“
```

**Interpolationãƒ¢ãƒ‡ãƒ«**: FILMï¼ˆFrame Interpolation for Large Motionï¼‰ç­‰ã€‚

**åˆ©ç‚¹**: è¨ˆç®—é‡å‰Šæ¸›ï¼ˆç”Ÿæˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒ1/10ï¼‰ã€‚

**æ¬ ç‚¹**: è¤‡é›‘ãªå‹•ãã§InterpolationãŒç ´ç¶»ã€‚

**FILMï¼ˆFrame Interpolation for Large Motionï¼‰ã®æ•°å¼**:

ä¸­é–“ãƒ•ãƒ¬ãƒ¼ãƒ $t \in (0, 1)$ã®ç”Ÿæˆ:
$$
\mathbf{x}_t = (1-t) \cdot \text{Warp}(\mathbf{x}_0, \mathbf{f}_{0 \to t}) + t \cdot \text{Warp}(\mathbf{x}_1, \mathbf{f}_{1 \to t})
$$

ã“ã“ã§$\mathbf{f}_{0 \to t}, \mathbf{f}_{1 \to t}$ã¯åŒæ–¹å‘Optical Flowï¼ˆFILMãŒäºˆæ¸¬ï¼‰ã€‚

**Multi-scale Pyramidæ§‹é€ **:

```
å…¥åŠ›ãƒ•ãƒ¬ãƒ¼ãƒ  (x0, x1)
    â†“ Downsample 4x
ä½è§£åƒåº¦Flowæ¨å®š â†’ f_low
    â†“ Upsample + Refine
ä¸­è§£åƒåº¦Flowæ¨å®š â†’ f_mid
    â†“ Upsample + Refine
é«˜è§£åƒåº¦Flowæ¨å®š â†’ f_high
    â†“ Warp + Blend
ä¸­é–“ãƒ•ãƒ¬ãƒ¼ãƒ  x_t
```

**Juliaå®Ÿè£…ä¾‹**:

```julia
using Images, Interpolations

function film_interpolation(frame0, frame1, t::Float32)
    # ç°¡æ˜“ç‰ˆ: ç·šå½¢è£œé–“ï¼ˆå®Ÿéš›ã®FILMã¯CNNã§Flowæ¨å®šï¼‰
    H, W, C = size(frame0)

    # Optical Flowæ¨å®šï¼ˆç°¡ç•¥åŒ–ï¼‰
    flow_0_to_t = estimate_flow(frame0, frame1) .* t
    flow_1_to_t = estimate_flow(frame1, frame0) .* (1 - t)

    # Warp
    warped_0 = warp_frame(frame0, flow_0_to_t)
    warped_1 = warp_frame(frame1, flow_1_to_t)

    # Blend
    intermediate = (1 - t) .* warped_0 .+ t .* warped_1

    return intermediate
end

function generate_with_interpolation(model, num_key_frames, key_frame_interval)
    # ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ
    key_frames = [generate_single_frame(model) for _ in 1:num_key_frames]

    # è£œé–“
    all_frames = [key_frames[1]]
    for i in 1:num_key_frames-1
        append!(all_frames, [film_interpolation(key_frames[i], key_frames[i+1], j / key_frame_interval)
                              for j in 1:key_frame_interval-1])
        push!(all_frames, key_frames[i+1])
    end

    return all_frames
end
```

**è¨ˆç®—é‡æ¯”è¼ƒ**:

| æ‰‹æ³• | ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ | Interpolation | ç·è¨ˆç®—é‡ |
|:-----|:----------------|:--------------|:---------|
| Full Generation | $T$ frames | - | $O(T \cdot C_{\text{gen}})$ |
| Key Frame + Interp | $T/k$ frames | $(T-T/k)$ frames | $O(T/k \cdot C_{\text{gen}} + T \cdot C_{\text{interp}})$ |

$C_{\text{interp}} \ll C_{\text{gen}}$ï¼ˆè£œé–“ã¯Diffusionã‚ˆã‚Šé¥ã‹ã«é«˜é€Ÿï¼‰ãªã®ã§ã€$k=10$ã§ç´„9å€é«˜é€ŸåŒ–ã€‚

#### æˆ¦ç•¥â‘¢ Hierarchical Generation

**åŸç†**: ä½è§£åƒåº¦ãƒ»ä½ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã§å…¨ä½“ã‚’ç”Ÿæˆ â†’ Super-resolution + Frame Interpolationã€‚

```
Stage 1: 256Ã—256ã€4 fpsã€10ç§’ â†’ 40ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ
Stage 2: å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’512Ã—512ã«Super-resolution
Stage 3: 4 fps â†’ 24 fps ã«Frame Interpolation (6å€)
```

**åˆ©ç‚¹**: Stage 1ã§å¤§åŸŸçš„ãªä¸€è²«æ€§ã‚’ç¢ºä¿ â†’ Stage 2-3ã§ç´°éƒ¨ã‚’è¿½åŠ ã€‚

**å®Ÿè£…ä¾‹**: Open-Sora 2.0ã€CogVideoXã€‚

**Multi-stage Pipelineã®æ•°å¼**:

**Stage 1ï¼ˆBase Generationï¼‰**:
$$
\mathbf{X}_{\text{base}} \sim p_\theta(\mathbf{X} \mid c), \quad \mathbf{X} \in \mathbb{R}^{T_{\text{low}} \times H_{\text{low}} \times W_{\text{low}} \times 3}
$$

**Stage 2ï¼ˆSuper-resolutionï¼‰**:
$$
\mathbf{X}_{\text{SR}} = f_{\text{SR}}(\mathbf{X}_{\text{base}}), \quad \mathbf{X}_{\text{SR}} \in \mathbb{R}^{T_{\text{low}} \times H_{\text{high}} \times W_{\text{high}} \times 3}
$$

**Stage 3ï¼ˆFrame Interpolationï¼‰**:
$$
\mathbf{X}_{\text{final}} = f_{\text{interp}}(\mathbf{X}_{\text{SR}}), \quad \mathbf{X}_{\text{final}} \in \mathbb{R}^{T_{\text{high}} \times H_{\text{high}} \times W_{\text{high}} \times 3}
$$

**CogVideoXå®Ÿè£…ï¼ˆæ¦‚è¦ï¼‰**:

```julia
function hierarchical_generation(base_model, sr_model, interp_model, prompt)
    # Stage 1: Base generation (256x256, 4fps, 49 frames = 12ç§’)
    base_video = generate_base(base_model, prompt, size=(256, 256), fps=4, duration=12)

    # Stage 2: Super-resolution (256x256 â†’ 720x480)
    sr_video = [super_resolve(sr_model, frame, target_size=(720, 480)) for frame in base_video]

    # Stage 3: Frame interpolation (4fps â†’ 24fps, 6å€)
    final_video = []
    for i in 1:length(sr_video)-1
        push!(final_video, sr_video[i])
        # 5ã¤ã®ä¸­é–“ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è£œé–“
        append!(final_video, [interpolate_frame(interp_model, sr_video[i], sr_video[i+1], j/6)
                               for j in 1:5])
    end
    push!(final_video, sr_video[end])

    return final_video
end
```

**ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®åˆ†æ**:

| Stage | è§£åƒåº¦ | ãƒ•ãƒ¬ãƒ¼ãƒ æ•° | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | å‡¦ç†æ™‚é–“ |
|:------|:-------|:----------|:------------|:---------|
| Stage 1 | 256Â² | 40 | 25 MB | 30ç§’ |
| Stage 2 | 720Â² | 40 | 200 MB | 10ç§’ |
| Stage 3 | 720Â² | 240 | 1.2 GB | 20ç§’ |

**Pipelineã®ä¸¦åˆ—åŒ–**:

```julia
using Distributed

function parallel_hierarchical_generation(prompts::Vector{String})
    # Stage 1ã‚’å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä¸¦åˆ—å®Ÿè¡Œ
    base_videos = pmap(prompt -> generate_base(base_model, prompt), prompts)

    # Stage 2-3ã‚‚ä¸¦åˆ—åŒ–
    final_videos = pmap(base_video -> begin
        sr_video = super_resolve_video(sr_model, base_video)
        interpolate_video(interp_model, sr_video)
    end, base_videos)

    return final_videos
end
```

### 6.6 Video Tokenizationã®æœ€å‰ç·š

#### 6.6.1 é›¢æ•£è¡¨ç¾ vs é€£ç¶šè¡¨ç¾

**2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | ä»£è¡¨ãƒ¢ãƒ‡ãƒ« | è¡¨ç¾ | ç”Ÿæˆæ–¹å¼ | åˆ©ç‚¹ | æ¬ ç‚¹ |
|:----------|:----------|:-----|:---------|:-----|:-----|
| é›¢æ•£è¡¨ç¾ | VQ-VAE / MAGVIT-v2 | Codebook index | Autoregressive | å³å¯†ãªå°¤åº¦ | Codebook collapse |
| é€£ç¶šè¡¨ç¾ | 3D VAE (CogVideoX) | é€£ç¶šLatent | Diffusion/Flow | è¡¨ç¾åŠ›é«˜ã„ | å°¤åº¦è¨ˆç®—ä¸å¯ |

#### 6.6.2 MAGVIT-v2 â€” çµ±ä¸€èªå½™ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

```
å…¥åŠ›å‹•ç”» (T, H, W, 3)
    â†“ 3D CNN Encoder
Latent (T/4, H/8, W/8, D)
    â†“ Vector Quantization
Discrete Tokens (T/4 Ã— H/8 Ã— W/8) âˆˆ {0, ..., 262143}
    â†“ Lookup Table
Quantized Latent
    â†“ 3D CNN Decoder
å†æ§‹æˆå‹•ç”» (T, H, W, 3)
```

**Lookup-Free Quantizationï¼ˆLFQï¼‰**:

å¾“æ¥ã®VQ-VAEå•é¡Œç‚¹: Codebook collapseã§ä¸€éƒ¨ã®ã‚³ãƒ¼ãƒ‰ã—ã‹ä½¿ã‚ã‚Œãªã„ã€‚

**LFQè§£æ±ºç­–**: Codebookã‚’æŒãŸãšã€Latentã‚’ç›´æ¥é‡å­åŒ–ã€‚

$$
\mathbf{z}_{\text{quant}} = \text{round}(\mathbf{z}) - \text{sg}(\mathbf{z} - \text{round}(\mathbf{z}))
$$

ã“ã“ã§$\text{sg}$ã¯stop-gradientï¼ˆStraight-Through Estimatorï¼‰ã€‚

**Commitment Lossä¸è¦**: é‡å­åŒ–ãŒè‡ªå‹•çš„ã«æ•´æ•°ã«åæŸã€‚

**Juliaå®Ÿè£…ä¾‹**:

```julia
function lookup_free_quantization(z::Array{Float32})
    # Latentã‚’[-1, 1]ã«Clip
    z_clipped = clamp.(z, -1.0f0, 1.0f0)

    # 8ãƒ“ãƒƒãƒˆé‡å­åŒ– (256ãƒ¬ãƒ™ãƒ«)
    z_scaled = (z_clipped .+ 1.0f0) .* 127.5f0
    z_quantized_int = round.(Int, z_scaled)

    # Float32ã«æˆ»ã™
    z_quantized = z_quantized_int ./ 127.5f0 .- 1.0f0

    # Straight-Through Estimator
    z_st = z_quantized .+ (z .- z_quantized)  # Forward: quantized, Backward: identity

    return z_st, z_quantized_int
end

# Codebook sizeã®è¨ˆç®—
function calculate_codebook_size(latent_shape, num_levels_per_dim)
    # latent_shape: (D,) â€” Latentæ¬¡å…ƒ
    # num_levels_per_dim: å„æ¬¡å…ƒã®é‡å­åŒ–ãƒ¬ãƒ™ãƒ«æ•°ï¼ˆä¾‹: 256ï¼‰

    D = length(latent_shape)
    codebook_size = num_levels_per_dim ^ D

    return codebook_size
end

# ä¾‹: D=8æ¬¡å…ƒã€å„æ¬¡å…ƒ256ãƒ¬ãƒ™ãƒ«
codebook_size = calculate_codebook_size((8,), 256)
println("Codebook size: $codebook_size")  # 256^8 = ç´„18å…†ï¼ˆå·¨å¤§ï¼‰
```

**å®Ÿç”¨çš„ã«ã¯**: ä½æ¬¡å…ƒï¼ˆD=4-8ï¼‰+å¤šæ®µéšé‡å­åŒ–ã§ç®¡ç†ã€‚

#### 6.6.3 Cosmos Tokenizer â€” ç”»åƒãƒ»å‹•ç”»çµ±åˆ

**NVIDIA Cosmos Tokenizer**ï¼ˆ2024ï¼‰:

- **ç”»åƒãƒ¢ãƒ¼ãƒ‰**: T=1ãƒ•ãƒ¬ãƒ¼ãƒ 
- **å‹•ç”»ãƒ¢ãƒ¼ãƒ‰**: T>1ãƒ•ãƒ¬ãƒ¼ãƒ 
- **åŒã˜ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€**: çµ±ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**Causal 3D Convã®é‡è¦æ€§**: å‹•ç”»ã®è‡ªå·±å›å¸°ç”Ÿæˆã‚’å¯èƒ½ã«ã€‚

```
é€šå¸¸3D Conv: Paddingä¸¡å´ â†’ æœªæ¥ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‚ç…§ï¼ˆæ¨è«–æ™‚ä¸å¯ï¼‰
Causal 3D Conv: Paddingéå»ã®ã¿ â†’ æ¨è«–æ™‚ã«é€æ¬¡ç”Ÿæˆå¯èƒ½
```

**Juliaå®Ÿè£…ä¾‹ï¼ˆCausal Paddingï¼‰**:

```julia
using Flux

struct CausalConv3D
    weight::Array{Float32, 5}  # (kernel_t, kernel_h, kernel_w, in_ch, out_ch)
    bias::Vector{Float32}
    stride::Tuple{Int, Int, Int}
end

function (conv::CausalConv3D)(x::Array{Float32, 5})  # (T, H, W, C, B)
    T, H, W, C_in, B = size(x)
    kt, kh, kw, _, C_out = size(conv.weight)

    # Causal Padding: æ™‚é–“æ–¹å‘ã¯éå»ã®ã¿
    pad_t = kt - 1
    pad_h = kh Ã· 2
    pad_w = kw Ã· 2

    # Paddingé©ç”¨
    x_padded = zeros(Float32, T + pad_t, H + 2pad_h, W + 2pad_w, C_in, B)
    x_padded[pad_t+1:end, pad_h+1:end-pad_h, pad_w+1:end-pad_w, :, :] = x

    # ç•³ã¿è¾¼ã¿ï¼ˆNNlibã‚’ä½¿ã†å®Ÿè£…ã¯çœç•¥ã€æ¦‚å¿µã®ã¿ï¼‰
    output = conv3d_manual(x_padded, conv.weight, conv.stride)

    return output
end
```

**çµ±ä¸€Tokenizerã®åˆ©ç‚¹**:

1. **Transfer Learning**: ç”»åƒã§è¨“ç·´ã—ãŸTokenizerã‚’å‹•ç”»ã«è»¢ç”¨
2. **Image-to-Video**: ç”»åƒTokenã‚’VAEã«é€šã—ã¦å‹•ç”»ç”Ÿæˆé–‹å§‹
3. **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡**: ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆè±Šå¯Œï¼‰ã§äº‹å‰è¨“ç·´ â†’ å‹•ç”»ï¼ˆå°‘ãªã„ï¼‰ã§Fine-tune

### 6.7 å‹•ç”»ç”Ÿæˆã®ç ”ç©¶ç³»è­œå›³

```mermaid
graph TD
    A[2015: DCGAN] -->|æ•µå¯¾çš„å­¦ç¿’| B[2017: Video GAN]
    B --> C[2020: DVD-GAN]

    D[2020: DDPM] -->|Diffusion| E[2022: Video Diffusion Models]
    E --> F[2023: Video LDM]
    F --> G[2024: Sora]
    G --> H[2025: Sora 2]

    E --> I[2023: Make-A-Video]
    I --> J[2024: CogVideoX]
    J --> K[2024: HunyuanVideo]
    K --> L[2025: Open-Sora 2.0]

    F --> M[2024: LTX-Video]
    M -->|Flow Matching| N[2025: æ¬¡ä¸–ä»£]

    O[2023: NeRF] -->|3Dè¡¨ç¾| P[2024: 4Dç”Ÿæˆ]
    P --> Q[2025: Dynamic Scene]

    style H fill:#f9f,stroke:#333,stroke-width:4px
    style L fill:#9f9,stroke:#333,stroke-width:4px
    style M fill:#99f,stroke:#333,stroke-width:4px
```

**ä¸»è¦ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**:

| å¹´ | ãƒ¢ãƒ‡ãƒ« | é©æ–° |
|:---|:-------|:-----|
| 2015 | DCGAN | æ•µå¯¾çš„å­¦ç¿’ã®æˆåŠŸ |
| 2020 | DDPM | Diffusionãƒ¢ãƒ‡ãƒ«ã®å¾©æ´» |
| 2022 | Video Diffusion Models | Diffusionã‚’å‹•ç”»ã«æ‹¡å¼µ |
| 2023 | Make-A-Video | ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãå‹•ç”»ç”Ÿæˆ |
| 2024 | **Sora** | ä¸–ç•Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¨ã—ã¦ã®å‹•ç”»ç”Ÿæˆ |
| 2024 | CogVideoX / HunyuanVideo | ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹é«˜å“è³ªåŒ– |
| 2025 | **Open-Sora 2.0** | $200kæ°‘ä¸»åŒ– |
| 2025 | **Sora 2** | 15-25ç§’ç”Ÿæˆã€ç‰©ç†æ³•å‰‡æ”¹å–„ |

---


### 6.8 ä»Šå›å­¦ã‚“ã ã“ã¨ï¼ˆ4ã¤ã®Key Takeawaysï¼‰

1. **æ™‚ç©ºé–“Diffusionã®æœ¬è³ª**: é™æ­¢ç”»Diffusionã«æ™‚é–“è»¸ã‚’è¿½åŠ  â†’ Temporal Attentionã§æ™‚é–“çš„ä¸€è²«æ€§ã‚’å­¦ç¿’ã€‚Optical Flow LossãŒç‰©ç†çš„å‹•ãã‚’ä¿è¨¼ã€‚

2. **3D U-Net â†’ DiTã®é©å‘½**: ç•³ã¿è¾¼ã¿ã®å±€æ‰€æ€§ â†’ Transformerã®å¤§åŸŸAttentionã€‚Sora 2ã®Spacetime DiTãŒæ™‚ç©ºé–“ã‚’çµ±ä¸€TokenåŒ– â†’ Scaling LawsãŒé©ç”¨å¯èƒ½ã«ã€‚

3. **3D VAEã®åœ§ç¸®è¡“**: CogVideoXã¯192å€åœ§ç¸®ï¼ˆæ™‚ç©ºé–“åˆè¨ˆï¼‰ã€‚Latentç©ºé–“ã§Diffusionã™ã‚‹ã“ã¨ã§è¨ˆç®—é‡ã‚’æ¿€æ¸›ã€‚

4. **2024-2025æœ€å‰ç·š**: HunyuanVideoï¼ˆ13Bå•†ç”¨ç´šï¼‰ã€Open-Sora 2.0ï¼ˆ$200kæ°‘ä¸»åŒ–ï¼‰ã€Wan 2.1ï¼ˆå¤šæ©Ÿèƒ½çµ±åˆï¼‰ã€LTX-Videoï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ï¼‰ã€‚SmolVLM2ã¨ã®çµ„ã¿åˆã‚ã›ã§Video-to-Text-to-Videoãƒ«ãƒ¼ãƒ—ãŒå®Ÿç¾ã€‚

### 6.9 FAQ â€” ã‚ˆãã‚ã‚‹5ã¤ã®è³ªå•

#### Q1: å‹•ç”»ç”Ÿæˆã®è¨ˆç®—ã‚³ã‚¹ãƒˆã¯ã©ã‚Œãã‚‰ã„ï¼Ÿ

**A**: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã«ä¾å­˜ã€‚

| ãƒ¢ãƒ‡ãƒ« | GPU | è§£åƒåº¦ | ãƒ•ãƒ¬ãƒ¼ãƒ æ•° | ç”Ÿæˆæ™‚é–“ |
|:-------|:----|:-------|:----------|:---------|
| LTX-Video | H100 | 768Ã—512 | 121 (5ç§’) | 2ç§’ |
| CogVideoX-5B | A100 | 720Ã—480 | 49 (2ç§’) | 10ç§’ |
| Open-Sora 2.0 | A100 | 720p | 240 (10ç§’) | 35ç§’ |
| Wan 2.1-14B | RTX 4090 | 480p | 120 (5ç§’) | 3.5åˆ† |

**ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ**: RTX 4090ï¼ˆ24GB VRAMï¼‰ã§å®Ÿç”¨çš„ãªæ¨è«–ãŒå¯èƒ½ã€‚

#### Q2: Soraã¯ãªãœç‰©ç†æ³•å‰‡ã‚’å­¦ç¿’ã§ãã‚‹ã®ã‹ï¼Ÿ

**A**: 3ã¤ã®è¦å› :
1. **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿**: æ¨å®š1Bå‹•ç”»ï¼ˆYouTubeã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ã§è¨“ç·´
2. **é•·æ™‚é–“å‹•ç”»**: 15-25ç§’ã®é•·ã„å‹•ç”»ã§å› æœé–¢ä¿‚ã‚’å­¦ç¿’
3. **Spacetime DiT**: æ™‚ç©ºé–“ã‚’çµ±ä¸€çš„ã«æ‰±ã† â†’ ç‰©ç†çš„åˆ¶ç´„ãŒå‰µç™º

ãŸã ã—ã€å®Œç’§ã§ã¯ãªã„ï¼ˆä¾‹: ãƒã‚¹ã‚±ãƒƒãƒˆãƒœãƒ¼ãƒ«ã®åå°„è§’åº¦ãŒä¸æ­£ç¢ºï¼‰ã€‚

#### Q3: å‹•ç”»ç”Ÿæˆã§ã¾ã è§£æ±ºã•ã‚Œã¦ã„ãªã„å•é¡Œã¯ï¼Ÿ

**A**: ä¸»ãªèª²é¡Œ:
1. **é•·æ™‚é–“ä¸€è²«æ€§**: 1åˆ†ä»¥ä¸Šã®å‹•ç”»ã§ç ´ç¶»
2. **ç‰©ç†æ³•å‰‡**: é‡åŠ›ãƒ»è¡çªã®å³å¯†ãªéµå®ˆ
3. **è¨ˆç®—ã‚³ã‚¹ãƒˆ**: é«˜è§£åƒåº¦é•·æ™‚é–“å‹•ç”»ã¯ä¾ç„¶ã¨ã—ã¦é«˜ã‚³ã‚¹ãƒˆ
4. **è©•ä¾¡æŒ‡æ¨™**: äººé–“ã®ä¸»è¦³ã¨ä¸€è‡´ã™ã‚‹è‡ªå‹•æŒ‡æ¨™ãŒãªã„
5. **åˆ¶å¾¡æ€§**: ç´°ã‹ã„å‹•ãã®æŒ‡å®šãŒå›°é›£

#### Q4: å‹•ç”»ç”Ÿæˆã¨å‹•ç”»ç·¨é›†ã®é•ã„ã¯ï¼Ÿ

**A**: 2ã¤ã®é•ã„:

| ã‚¿ã‚¹ã‚¯ | å…¥åŠ› | å‡ºåŠ› | æŠ€è¡“ |
|:-------|:-----|:-----|:-----|
| å‹•ç”»ç”Ÿæˆï¼ˆT2Vï¼‰ | ãƒ†ã‚­ã‚¹ãƒˆ | æ–°è¦å‹•ç”» | Diffusion/Flow Matching |
| å‹•ç”»ç·¨é›†ï¼ˆV2Vï¼‰ | æ—¢å­˜å‹•ç”»+æŒ‡ç¤º | ç·¨é›†æ¸ˆã¿å‹•ç”» | Inpainting/Style Transfer |

**çµ±åˆãƒ¢ãƒ‡ãƒ«**: Wan 2.1ã‚„Runway Gen-3ã¯ä¸¡æ–¹å¯¾å¿œã€‚

#### Q5: SmolVLM2ã¨LTX-Videoã‚’çµ„ã¿åˆã‚ã›ã‚‹æ„ç¾©ã¯ï¼Ÿ

**A**: **Video-to-Text-to-Video**ãƒ«ãƒ¼ãƒ—ã®å®Ÿç¾:
1. SmolVLM2ã§æ—¢å­˜å‹•ç”»ã‚’ç†è§£ï¼ˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼‰
2. ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç·¨é›†ï¼ˆä¾‹: "cat"ã‚’"dog"ã«å¤‰æ›´ï¼‰
3. LTX-Videoã§æ–°ã—ã„å‹•ç”»ç”Ÿæˆ

**å¿œç”¨ä¾‹**:
- å‹•ç”»è¦ç´„ â†’ å†ç”Ÿæˆï¼ˆé•·å‹•ç”»ã‚’çŸ­ãã¾ã¨ã‚ã‚‹ï¼‰
- ã‚¹ã‚¿ã‚¤ãƒ«å¤‰æ›ï¼ˆå®Ÿå†™â†’ã‚¢ãƒ‹ãƒ¡ï¼‰
- ã‚·ãƒ¼ãƒ³å·®ã—æ›¿ãˆï¼ˆèƒŒæ™¯ã ã‘å¤‰æ›´ï¼‰

### 6.10 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« â€” 1é€±é–“ãƒ—ãƒ©ãƒ³

| æ—¥ | å†…å®¹ | æ™‚é–“ | åˆ°é”ç›®æ¨™ |
|:---|:-----|:-----|:---------|
| Day 1 | Zone 0-2 | 2h | å‹•ç”»ç”Ÿæˆã®ç›´æ„Ÿç†è§£ |
| Day 2 | Zone 3å‰åŠï¼ˆ3.1-3.3ï¼‰ | 3h | Spacetime DiTå°å‡º |
| Day 3 | Zone 3å¾ŒåŠï¼ˆ3.4-3.6ï¼‰ | 3h | 3D VAEãƒ»Optical Flow |
| Day 4 | Zone 4ï¼ˆJuliaå®Ÿè£…ï¼‰ | 3h | Video Diffusionè¨“ç·´ |
| Day 5 | Zone 5ï¼ˆãƒ‡ãƒ¢å®Ÿè¡Œï¼‰ | 2h | SmolVLM2+LTX-Video |
| Day 6 | Zone 6ï¼ˆæœ€å‰ç·šèª¿æŸ»ï¼‰ | 2h | HunyuanVideoç­‰ã®è«–æ–‡ |
| Day 7 | å¾©ç¿’+å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ | 3h | Moving MNIST |

**åˆè¨ˆ**: 18æ™‚é–“ â†’ é€±æœ«2æ—¥ + å¹³æ—¥å¤œ3æ—¥ã§å®Œèµ°å¯èƒ½ã€‚

### 6.11 æ¬¡å›äºˆå‘Š â€” ç¬¬46å›: 3Dç”Ÿæˆ & Neural Rendering

**ã‚¿ã‚¤ãƒˆãƒ«**: ã€Œ2Då‹•ç”»ã‹ã‚‰3Dç©ºé–“ã®ç†è§£ã¨ç”Ÿæˆã¸ã€

**å­¦ã¶ã“ã¨**:
1. **3Dè¡¨ç¾ã®åˆ†é¡**: Mesh / Voxel / Implicit / Radiance Field / 3DGS
2. **NeRFï¼ˆNeural Radiance Fieldsï¼‰**: Volume Renderingæ–¹ç¨‹å¼ãƒ»ä½ç½®ç¬¦å·åŒ–
3. **3D Gaussian Splattingï¼ˆ3DGSï¼‰**: æ˜ç¤ºçš„è¡¨ç¾ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
4. **DreamFusion**: SDS Lossï¼ˆScore Distillation Samplingï¼‰ã§Text-to-3D
5. **3D VAE**: å‹•ç”»VAEã‚’3Dç©ºé–“ã«æ‹¡å¼µ

**æ¥ç¶š**: å‹•ç”»ï¼ˆ2D+æ™‚é–“ï¼‰ â†’ 3Dç©ºé–“ï¼ˆ3D+æ™‚é–“=4Dï¼‰ã¸æ‹¡å¼µã€‚

> **Note:** **é€²æ—**: å…¨ä½“ã®90%å®Œäº†ã€‚æ®‹ã‚Š5å›ã§å’æ¥­åˆ¶ä½œã¸ã€‚ç¬¬50å›ã§ã¯3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ç”ŸæˆAIã‚·ã‚¹ãƒ†ãƒ ã®å®ŒæˆãŒå¾…ã£ã¦ã„ã¾ã™ã€‚

---


> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. HunyuanVideoã®Causal 3D VAEãŒæ™‚é–“æ–¹å‘ã«Causalè¨­è¨ˆã«ã™ã‚‹ç†ç”±ã‚’ã€è‡ªå·±å›å¸°çš„ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆã¨ã®é–¢ä¿‚ã§èª¬æ˜ã›ã‚ˆã€‚
> 2. MoEã‚’å‹•ç”»ç”ŸæˆDiTã«é©ç”¨ã—ãŸéš›ã«æ¨è«–æ™‚Top-2 Expertã®ã¿ä½¿ã†è¨­è¨ˆã®è¨ˆç®—é‡ä¸Šã®åˆ©ç‚¹ã‚’ã€å…¨Expertä½¿ç”¨æ™‚ã¨ã®æ¯”è¼ƒã§ç¤ºã›ã€‚

## ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

### å•ã„: Soraã¯"å‹•ç”»ç”Ÿæˆ"ã§ã¯ãªã"ä¸–ç•Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿"ãªã®ã‹ï¼Ÿ

OpenAI Technical Report (2024)ã®è¨˜è¿°:
> "Sora is a **world simulator**. It learns not just to generate videos, but to model the physical world."

**è­°è«–ã®3ã¤ã®è¦–ç‚¹**:

#### è¦–ç‚¹â‘  è³›æˆæ´¾ â€” ç‰©ç†æ³•å‰‡ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹è¨¼æ‹ 

**æ ¹æ‹ **:
1. **å› æœé–¢ä¿‚**: ãƒœãƒ¼ãƒ«ã‚’æŠ•ã’ã‚‹ â†’ å¼§ã‚’æã„ã¦è½ä¸‹ï¼ˆé‡åŠ›ï¼‰
2. **è¡çªåå¿œ**: ç‰©ä½“åŒå£«ãŒã¶ã¤ã‹ã‚‹ã¨è·³ã­è¿”ã‚‹
3. **æŒç¶šæ€§**: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒç”»é¢å¤–ã«æ¶ˆãˆã¦ã‚‚ã€æˆ»ã£ã¦ãã‚‹ã¨åŒã˜çŠ¶æ…‹

**Soraã®å‡ºåŠ›ä¾‹**ï¼ˆTechnical Reportã‚ˆã‚Šï¼‰:
- ãƒã‚¹ã‚±ãƒƒãƒˆãƒœãƒ¼ãƒ«ãŒãƒªãƒ³ã‚°ã«å½“ãŸã£ã¦è·³ã­è¿”ã‚‹
- æ°´é¢ã«çŸ³ã‚’æŠ•ã’ã‚‹ã¨æ³¢ç´‹ãŒåºƒãŒã‚‹

**çµè«–**: å˜ãªã‚‹ç”»åƒç”Ÿæˆã§ã¯ãªãã€ã€Œç‰©ç†çš„ã«èµ·ã“ã‚Šã†ã‚‹ã“ã¨ã€ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã€‚

#### è¦–ç‚¹â‘¡ åå¯¾æ´¾ â€” è¦‹ãŸç›®ã®ãƒªã‚¢ãƒ«ã•ã«ã™ããªã„

**æ ¹æ‹ **:
1. **ç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³ä¸åœ¨**: Soraã¯å†…éƒ¨ã«æ˜ç¤ºçš„ãªç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’æŒãŸãªã„
2. **å¤±æ•—ä¾‹**: Technical Reportã§ã‚‚ç‰©ç†çš„çŸ›ç›¾ã‚’èªã‚ã¦ã„ã‚‹
   - ãƒã‚¹ã‚±ãƒƒãƒˆãƒœãƒ¼ãƒ«ã®åå°„è§’åº¦ãŒä¸æ­£ç¢º
   - æ¤…å­ãŒçªç„¶æ¶ˆãˆã‚‹
3. **è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¨¡å€£**: å¤§è¦æ¨¡å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã€Œã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’å­¦ç¿’ã—ãŸã ã‘

**çµè«–**: ç‰©ç†æ³•å‰‡ã‚’"ç†è§£"ã—ã¦ã„ã‚‹ã®ã§ã¯ãªãã€"æ¨¡å€£"ã—ã¦ã„ã‚‹ã ã‘ã€‚

#### è¦–ç‚¹â‘¢ ä¸­ç«‹æ´¾ â€” å‰µç™ºçš„ç‰©ç†ç†è§£

**ä¸»å¼µ**: æ˜ç¤ºçš„ãªç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³ã¯ãªã„ãŒã€**æš—é»™çš„ã«ç‰©ç†åˆ¶ç´„ã‚’å­¦ç¿’**ã—ã¦ã„ã‚‹ã€‚

**ç†è«–çš„æ ¹æ‹ **:
- Diffusionãƒ¢ãƒ‡ãƒ«ã¯ã€Œãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ã‚¹ã‚³ã‚¢ï¼ˆå‹¾é…ï¼‰ã€ã‚’å­¦ç¿’
- ç‰©ç†çš„ã«ä¸€è²«ã—ãŸå‹•ç”» = ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®é«˜å¯†åº¦é ˜åŸŸ
- Soraã¯ã‚¹ã‚³ã‚¢ã‚’å­¦ç¿’ â†’ çµæœçš„ã«ç‰©ç†æ³•å‰‡ã«å¾“ã†å‹•ç”»ã‚’ç”Ÿæˆ

**é¡æ¨**: è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-4ï¼‰ã‚‚æ–‡æ³•è¦å‰‡ã‚’æ˜ç¤ºçš„ã«æŒãŸãªã„ãŒã€å¤§è¦æ¨¡è¨“ç·´ã§æ–‡æ³•ã‚’ã€Œå‰µç™ºçš„ã«å­¦ç¿’ã€ã€‚

**çµè«–**: å®Œå…¨ãªç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§ã¯ãªã„ãŒã€å˜ãªã‚‹ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã‚‚ãªã„ã€‚**ä¸­é–“çŠ¶æ…‹**ã€‚

### ã‚ãªãŸã®è€ƒãˆã¯ï¼Ÿ

1. Soraã¯ç‰©ç†æ³•å‰‡ã‚’æœ¬å½“ã«ç†è§£ã—ã¦ã„ã‚‹ã‹ï¼Ÿ
2. ã€Œç†è§£ã€ã¨ã€Œæ¨¡å€£ã€ã®å¢ƒç•Œç·šã¯ã©ã“ã«ã‚ã‚‹ã‹ï¼Ÿ
3. å®Œå…¨ãªä¸–ç•Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã«ã¯ä½•ãŒè¶³ã‚Šãªã„ã‹ï¼Ÿ

<details><summary>æ­´å²çš„ã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆ â€” AIã®ã€Œç†è§£ã€è«–äº‰</summary>

**Searleã®ä¸­å›½èªã®éƒ¨å±‹ï¼ˆ1980ï¼‰**:
- è¨˜å·æ“ä½œã ã‘ã§ã¯ã€Œç†è§£ã€ã«ãªã‚‰ãªã„
- Soraã¯ã€Œå‹•ç”»ã®éƒ¨å±‹ã€ã«ã™ããªã„ã®ã‹ï¼Ÿ

**Moravecã®ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹ï¼ˆ1988ï¼‰**:
- è«–ç†æ¨è«–ã¯ç°¡å˜ã€ç‰©ç†ä¸–ç•Œã®èªè­˜ã¯é›£ã—ã„
- SoraãŒç‰©ç†ã‚’å­¦ç¿’ã—ãŸãªã‚‰ã€AIã®å¤§ããªä¸€æ­©

**ç¾ä»£ã®è¦–ç‚¹ï¼ˆLeCun, 2024ï¼‰**:
- ã€Œç†è§£ã€= ä¸–ç•Œã®å› æœãƒ¢ãƒ‡ãƒ«ã‚’æŒã¤ã“ã¨
- Soraã¯éƒ¨åˆ†çš„ãªå› æœãƒ¢ãƒ‡ãƒ«ã‚’ç²å¾—ã—ã¦ã„ã‚‹å¯èƒ½æ€§

</details>

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Vaswani, A., et al. (2017). "Attention Is All You Need". *NeurIPS 2017*.
<https://arxiv.org/abs/1706.03762>

### æ•™ç§‘æ›¸ãƒ»ã‚µãƒ¼ãƒ™ã‚¤

- Sohl-Dickstein, J., et al. (2015). "Deep Unsupervised Learning using Nonequilibrium Thermodynamics". *ICML 2015*.
- Song, Y., et al. (2020). "Score-Based Generative Modeling through Stochastic Differential Equations". *ICLR 2021*.
- Lipman, Y., et al. (2022). "Flow Matching for Generative Modeling". *arXiv:2210.02747*.
- Yun, C., et al. (2019). "Are Transformers Universal Approximators of Sequence-to-Sequence Functions?". *arXiv:1912.10077*.

---


## ğŸ”— å‰ç·¨ãƒ»å¾Œç·¨ãƒªãƒ³ã‚¯

- **å‰ç·¨ (Part 1 â€” ç†è«–ç·¨)**: [ç¬¬45å›: Videoç”Ÿæˆ (Part 1)](ml-lecture-45-part1)

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
