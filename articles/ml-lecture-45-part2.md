---
title: "ç¬¬45å› (Part 2): Videoç”Ÿæˆ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ¬"
type: "tech"
topics: ["machinelearning","deeplearning","video","rust","rust","elixir"]
published: true
slug: "ml-lecture-45-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---
## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” 3è¨€èªã§å‹•ç”»ç”Ÿæˆã‚’å®Ÿè£…

### 4.1 ğŸ¦€ Rust: Video Diffusionè¨“ç·´å®Ÿè£…

#### 4.1.1 ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ â€” å‹•ç”»ã‚’ãƒãƒƒãƒå‡¦ç†

```rust
// ä¾å­˜ã‚¯ãƒ¬ãƒ¼ãƒˆ: ndarray = "0.15", ndarray-rand = "0.14"
use std::fs;
use ndarray::{Array3, Array4, Array5, Axis};

// å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ä½“
pub struct VideoDataset {
    pub video_paths: Vec<String>,
    pub num_frames: usize,
    pub height: usize,
    pub width: usize,
}

impl VideoDataset {
    pub fn new(video_paths: Vec<String>, num_frames: usize, height: usize, width: usize) -> Self {
        Self { video_paths, num_frames, height, width }
    }

    pub fn len(&self) -> usize {
        self.video_paths.len()
    }

    pub fn get(&self, idx: usize) -> Array4<f32> {
        load_video(&self.video_paths[idx], self.num_frames, self.height, self.width)
    }
}

// å‹•ç”»èª­ã¿è¾¼ã¿: (C, H, W, T) å½¢å¼ã® Array4<f32> ã‚’è¿”ã™
pub fn load_video(path: &str, num_frames: usize, height: usize, width: usize) -> Array4<f32> {
    // VideoIOã§å‹•ç”»èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã¯ ffmpeg ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ï¼‰
    let mut frames: Vec<Array3<f32>> = Vec::new();

    for _i in 0..num_frames {
        // Resize + æ­£è¦åŒ–: [-1, 1]
        let img_normalized = Array3::<f32>::zeros((3, height, width))
            .mapv(|v| v * 2.0 - 1.0);
        frames.push(img_normalized);
    }

    // (C, H, W, T) å½¢å¼ã«ã‚¹ã‚¿ãƒƒã‚¯
    let views: Vec<_> = frames.iter().map(|f| f.view().insert_axis(Axis(3))).collect();
    ndarray::concatenate(Axis(3), &views[..]).expect("stack failed")
}

// ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
fn main() {
    // å‹•ç”»ãƒ‘ã‚¹ä¸€è¦§å–å¾—
    let video_paths: Vec<String> = fs::read_dir("/path/to/videos")
        .expect("read_dir failed")
        .filter_map(|e| e.ok())
        .map(|e| e.path().to_string_lossy().into_owned())
        .collect();

    let dataset = VideoDataset::new(video_paths, 16, 64, 64);

    // ãƒãƒƒãƒã‚’ã‚¹ã‚¿ãƒƒã‚¯: (C, H, W, T, B)
    let batch_size = 4;
    let batch_frames: Vec<Array4<f32>> = (0..batch_size)
        .map(|i| dataset.get(i))
        .collect();
    let batch_views: Vec<_> = batch_frames.iter().map(|f| f.view().insert_axis(Axis(4))).collect();
    let _batch: Array5<f32> = ndarray::concatenate(Axis(4), &batch_views[..])
        .expect("batch stack failed"); // (C, H, W, T, B)
}
```

#### 4.1.2 3D U-Netã®ç°¡æ˜“å®Ÿè£…

```rust
// ä¾å­˜ã‚¯ãƒ¬ãƒ¼ãƒˆ: ndarray = "0.15", ndarray-rand = "0.14"
use ndarray::Array5;

// 3D Convolution Block
#[derive(Debug)]
pub struct Conv3DBlock {
    conv1_weight: ndarray::Array5<f32>, // (out_ch, in_ch, 3, 3, 3)
    conv2_weight: ndarray::Array5<f32>,
    norm_scale: ndarray::Array1<f32>,
}

impl Conv3DBlock {
    pub fn new(in_ch: usize, out_ch: usize) -> Self {
        // kernel=(3,3,3), pad=(1,1,1), GlorotåˆæœŸåŒ–
        use ndarray_rand::RandomExt;
        use ndarray_rand::rand_distr::StandardNormal;
        Self {
            conv1_weight: ndarray::Array5::random((out_ch, in_ch, 3, 3, 3), StandardNormal),
            conv2_weight: ndarray::Array5::random((out_ch, out_ch, 3, 3, 3), StandardNormal),
            norm_scale: ndarray::Array1::ones(out_ch),
        }
    }

    pub fn forward(&self, x: &Array5<f32>) -> Array5<f32> {
        // conv1 â†’ relu â†’ conv2 â†’ batchnorm â†’ relu
        // ï¼ˆæ¦‚å¿µçš„ãªå®Ÿè£…: å®Ÿéš›ã¯ tch-rs ã‚„ burn ã® Conv3d ã‚’ä½¿ç”¨ï¼‰
        let x = x.mapv(|v| v.max(0.0)); // relu
        x.mapv(|v| v.max(0.0))          // relu
    }
}

// Simple 3D U-Net
#[derive(Debug)]
pub struct Simple3DUNet {
    down1: Conv3DBlock,
    down2: Conv3DBlock,
    bottleneck: Conv3DBlock,
    // up1, up2: ConvTranspose stride=(2,2,2)ï¼ˆæ¦‚å¿µçš„ï¼‰
    out_channels: usize,
}

impl Simple3DUNet {
    pub fn new(in_ch: usize, out_ch: usize) -> Self {
        Self {
            down1: Conv3DBlock::new(in_ch, 64),
            down2: Conv3DBlock::new(64, 128),
            bottleneck: Conv3DBlock::new(128, 256),
            out_channels: out_ch,
        }
    }

    pub fn forward(&self, x: &Array5<f32>) -> Array5<f32> {
        // x: (C, H, W, T, B)
        let d1 = self.down1.forward(x);
        let d1_pool = d1.clone(); // max pool op comment: maxpool (2,2,2)

        let d2 = self.down2.forward(&d1_pool);
        let d2_pool = d2.clone(); // max pool op comment: maxpool (2,2,2)

        let bn = self.bottleneck.forward(&d2_pool);

        // up1: ConvTranspose stride=(2,2,2)
        let u1 = &bn + &d2; // Skip connection

        // up2: ConvTranspose stride=(2,2,2)
        let u2 = &u1 + &d1;

        // final_conv (1,1,1)
        u2
    }
}
```

#### 4.1.3 Video Diffusionè¨“ç·´ãƒ«ãƒ¼ãƒ—

```rust
// ä¾å­˜ã‚¯ãƒ¬ãƒ¼ãƒˆ: ndarray = "0.15", ndarray-rand = "0.14", rand = "0.8"
use ndarray::{Array5, Axis};
use ndarray_rand::RandomExt;
use ndarray_rand::rand_distr::StandardNormal;

fn add_noise(
    x0: &Array5<f32>,
    t: usize,
    beta_schedule: &[f32],
) -> (Array5<f32>, Array5<f32>) {
    // Forward process: x_t = âˆšalpha_t x_0 + âˆš(1-alpha_t) Îµ
    let alpha_t: f32 = beta_schedule[..=t].iter().map(|b| 1.0 - b).product();

    let epsilon = Array5::<f32>::random(x0.raw_dim(), StandardNormal);
    let xt = alpha_t.sqrt() * x0 + (1.0 - alpha_t).sqrt() * &epsilon;

    (xt, epsilon)
}

fn train_video_diffusion(
    model: &mut Simple3DUNet,
    dataset: &VideoDataset,
    num_epochs: usize,
    beta_schedule: &[f32],
) {
    // opt: Adam(lr=1e-4)ï¼ˆå®Ÿéš›ã¯ burn ã‚„ tch-rs ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ä½¿ç”¨ï¼‰

    for epoch in 1..=num_epochs {
        let mut epoch_loss = 0.0f32;

        for batch_idx in 0..dataset.len() {
            let x0 = dataset.get(batch_idx); // (C, H, W, T)
            // ãƒãƒƒãƒæ¬¡å…ƒè¿½åŠ : (C, H, W, T, B=1)
            let x0_5d = x0.insert_axis(Axis(4));
            let b = x0_5d.shape()[4];

            // ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—
            let t = rand::random::<usize>() % beta_schedule.len();

            // ãƒã‚¤ã‚ºè¿½åŠ 
            let (xt, epsilon_true) = add_noise(&x0_5d, t, beta_schedule);

            // ãƒã‚¤ã‚ºäºˆæ¸¬
            let epsilon_pred = model.forward(&xt);

            // MSE Loss
            let loss = (&epsilon_pred - &epsilon_true)
                .mapv(|x| x * x)
                .mean()
                .unwrap();

            // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ï¼ˆå®Ÿéš›ã¯ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã§è¡Œã†ï¼‰
            epoch_loss += loss;
        }

        let avg_loss = epoch_loss / dataset.len() as f32;
        println!("Epoch {epoch}, Loss: {avg_loss}");
    }
}

// è¨“ç·´å®Ÿè¡Œ
fn main() {
    let mut model = Simple3DUNet::new(3, 3); // RGBå‹•ç”»
    let beta_schedule: Vec<f32> = (0..1000)
        .map(|i| 1e-4 + (0.02 - 1e-4) * i as f32 / 999.0)
        .collect();

    // train_video_diffusion(&mut model, &dataset, 10, &beta_schedule);
}
```

#### 4.1.4 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆDDIMï¼‰

```rust
// DDIM ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
use ndarray::{Array5, Axis};
use ndarray_rand::RandomExt;
use ndarray_rand::rand_distr::StandardNormal;

fn ddim_sample(
    model: &Simple3DUNet,
    num_frames: usize,
    height: usize,
    width: usize,
    beta_schedule: &[f32],
    num_steps: usize,
) -> ndarray::Array4<f32> {
    let t_max = beta_schedule.len();
    let step_size = t_max / num_steps;
    // T_max, T_max-step, ..., stepï¼ˆé™é †ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
    let timesteps: Vec<usize> = (0..num_steps)
        .map(|i| t_max.saturating_sub(1 + i * step_size))
        .collect();

    // ãƒã‚¤ã‚ºã‹ã‚‰é–‹å§‹
    let mut xt = Array5::<f32>::random((3, height, width, num_frames, 1), StandardNormal);

    for (i, &t) in timesteps.iter().enumerate() {
        let t_prev = timesteps.get(i + 1).copied();

        let alpha_t: f32 = beta_schedule[..=t].iter().map(|b| 1.0 - b).product();
        let alpha_t_prev: f32 = t_prev.map_or(1.0f32, |tp| {
            beta_schedule[..=tp].iter().map(|b| 1.0 - b).product()
        });

        // ãƒã‚¤ã‚ºäºˆæ¸¬
        let epsilon_pred = model.forward(&xt);

        // x0äºˆæ¸¬
        let x0_pred = (&xt - (1.0 - alpha_t).sqrt() * &epsilon_pred) / alpha_t.sqrt();

        // DDIMã‚¹ãƒ†ãƒƒãƒ—
        let dir_xt = (1.0 - alpha_t_prev).sqrt() * &epsilon_pred;
        xt = alpha_t_prev.sqrt() * &x0_pred + dir_xt;
    }

    // [-1, 1] â†’ [0, 1]
    let video = xt.mapv(|v| (v + 1.0) / 2.0);
    // ãƒãƒƒãƒæ¬¡å…ƒå‰Šé™¤: (3, H, W, T, 1) â†’ (3, H, W, T)
    video.index_axis_move(Axis(4), 0)
}

// ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ
fn main() {
    let beta_schedule: Vec<f32> = (0..1000)
        .map(|i| 1e-4 + (0.02 - 1e-4) * i as f32 / 999.0)
        .collect();
    let model = Simple3DUNet::new(3, 3);
    let generated_video = ddim_sample(&model, 16, 64, 64, &beta_schedule, 50);
}
```

### 4.2 ğŸ¦€ Rust: LTX-Videoæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

#### 4.2.1 ONNX Runtimeçµ±åˆ

```rust
// Cargo.toml
// [dependencies]
// ort = "2.0"
// ndarray = "0.15"
// image = "0.24"

use ort::{Session, Value, GraphOptimizationLevel};
use ndarray::{Array4, Array5, s};
use std::path::Path;

pub struct LTXVideoInference {
    session: Session,
}

impl LTXVideoInference {
    pub fn new(model_path: &Path) -> Result<Self, Box<dyn std::error::Error>> {
        let session = Session::builder()?
            .with_optimization_level(GraphOptimizationLevel::Level3)?
            .commit_from_file(model_path)?;

        Ok(Self { session })
    }

    pub fn generate(&self, prompt_embedding: &Array4<f32>, num_frames: usize)
        -> Result<Array5<f32>, Box<dyn std::error::Error>>
    {
        // ãƒã‚¤ã‚ºLatentç”Ÿæˆ
        let latent_shape = (1, 4, num_frames / 4, 64, 64);  // åœ§ç¸®æ¸ˆã¿
        let mut noise: Array5<f32> = Array5::from_shape_fn(latent_shape, |_| {
            rand::random::<f32>() * 2.0 - 1.0
        });

        // Diffusionãƒ«ãƒ¼ãƒ—ï¼ˆ50ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        for step in (0..50).rev() {
            let t = Array4::from_elem((1, 1, 1, 1), step as f32 / 50.0);

            // ONNXæ¨è«–
            let inputs = vec![
                Value::from_array(noise.view())?,
                Value::from_array(t.view())?,
                Value::from_array(prompt_embedding.view())?,
            ];

            let outputs = self.session.run(inputs)?;
            let noise_pred = outputs[0].try_extract_tensor::<f32>()?;

            // DDIMæ›´æ–°
            let alpha_t = 1.0 - (step as f32 / 50.0) * 0.02;
            let alpha_prev = if step > 0 {
                1.0 - ((step - 1) as f32 / 50.0) * 0.02
            } else {
                1.0
            };

            let x0_pred = (noise.view() - (1.0 - alpha_t).sqrt() * noise_pred.view())
                / alpha_t.sqrt();

            noise = alpha_prev.sqrt() * &x0_pred + (1.0 - alpha_prev).sqrt() * noise_pred.view();
        }

        Ok(noise)
    }
}

// ä½¿ç”¨ä¾‹
fn main() -> Result<(), Box<dyn std::error::Error>> {
    let model_path = Path::new("ltx_video.onnx");
    let inference = LTXVideoInference::new(model_path)?;

    // ãƒ€ãƒŸãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆå®Ÿéš›ã¯CLIP/T5ã‹ã‚‰å–å¾—ï¼‰
    let prompt_emb = Array4::zeros((1, 512, 1, 1));

    let video = inference.generate(&prompt_emb, 16)?;

    println!("Generated video shape: {:?}", video.shape());

    Ok(())
}
```

#### 4.2.2 ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã®ä¸¦åˆ—å‡¦ç†

```rust
use rayon::prelude::*;

pub fn parallel_frame_processing(
    frames: &Array5<f32>,  // (B, C, T, H, W)
) -> Array5<f32> {
    let (b, c, t, h, w) = frames.dim();
    let mut processed = Array5::zeros((b, c, t, h, w));

    // ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«ä¸¦åˆ—å‡¦ç†
    (0..t).into_par_iter().for_each(|frame_idx| {
        let frame = frames.slice(s![.., .., frame_idx, .., ..]);

        // ä½•ã‚‰ã‹ã®å¾Œå‡¦ç†ï¼ˆä¾‹: Sharpeningï¼‰
        let mut processed_frame = frame.to_owned();
        // ... (å‡¦ç†)

        // çµæœã‚’æ›¸ãæˆ»ã—ï¼ˆè¦: Mutex or lock-freeæ§‹é€ ï¼‰
        processed.slice_mut(s![.., .., frame_idx, .., ..])
            .assign(&processed_frame);
    });

    processed
}
```

### 4.3 ğŸ”® Elixir: å‹•ç”»é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹

```elixir
# lib/video_service/generator.ex
defmodule VideoService.Generator do
  use GenServer

  # Rustler NIFçµ±åˆ
  use Rustler, otp_app: :video_service, crate: "video_generator"

  # NIF functions
  def generate_video_nif(_prompt, _num_frames), do: :erlang.nif_error(:nif_not_loaded)

  # GenServer callbacks
  def start_link(_) do
    GenServer.start_link(__MODULE__, %{}, name: __MODULE__)
  end

  def init(state) do
    {:ok, state}
  end

  def handle_call({:generate, prompt, num_frames}, _from, state) do
    # Dirty schedulerã§å®Ÿè¡Œï¼ˆGPUè¨ˆç®—ã®ãŸã‚ï¼‰
    Task.start(fn ->
      video_data = generate_video_nif(prompt, num_frames)
      # çµæœã‚’Phoenix Channelã§é…ä¿¡
      VideoService.Endpoint.broadcast("video:lobby", "generation_complete", %{
        video: Base.encode64(video_data),
        prompt: prompt
      })
    end)

    {:reply, :ok, state}
  end
end

# lib/video_service_web/channels/video_channel.ex
defmodule VideoServiceWeb.VideoChannel do
  use Phoenix.Channel

  def join("video:lobby", _message, socket) do
    {:ok, socket}
  end

  def handle_in("generate", %{"prompt" => prompt, "num_frames" => num_frames}, socket) do
    VideoService.Generator.generate(prompt, num_frames)
    {:noreply, socket}
  end
end

# Supervisor
defmodule VideoService.Application do
  use Application

  def start(_type, _args) do
    children = [
      VideoServiceWeb.Endpoint,
      {VideoService.Generator, []},
      {Task.Supervisor, name: VideoService.TaskSupervisor}
    ]

    opts = [strategy: :one_for_one, name: VideoService.Supervisor]
    Supervisor.start_link(children, opts)
  end
end
```

### 4.4 3è¨€èªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Elixiråˆ†æ•£ã‚µãƒ¼ãƒ“ã‚¹                      â”‚
â”‚  Phoenix WebSocket  â†’  å‹•ç”»ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä»˜              â”‚
â”‚       â†“                                                     â”‚
â”‚  GenServer  â†’  è² è·åˆ†æ•£ï¼ˆ10ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼‰             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â†“ Rustler NIF (C-ABI)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Rustæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³                        â”‚
â”‚  ONNX Runtime  â†’  LTX-Video DiTæ¨è«–                         â”‚
â”‚  ä¸¦åˆ—ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†  â†’  Rayonã§8ã‚³ã‚¢æ´»ç”¨                      â”‚
â”‚  é‡å­åŒ–(FP16)  â†’  ãƒ¡ãƒ¢ãƒªå‰Šæ¸›                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â†“ rustler NIF (Rustâ†”Elixir)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Rustè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Candle/Burn)      â”‚
â”‚  Lux.jl  â†’  Video Diffusionè¨“ç·´                             â”‚
â”‚  Reactant  â†’  GPU AOTã‚³ãƒ³ãƒ‘ã‚¤ãƒ«                             â”‚
â”‚  DataLoader  â†’  é«˜é€Ÿå‹•ç”»ãƒãƒƒãƒå‡¦ç†                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å½¹å‰²åˆ†æ‹…**:

| è¨€èª | å½¹å‰² | ä½¿ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª | å¼·ã¿ |
|:-----|:-----|:---------------|:-----|
| ğŸ¦€ Rust | è¨“ç·´ãƒ»å®Ÿé¨“ | Candle, Burn, VideoIO | æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1ã€GPUæœ€é©åŒ– |
| ğŸ¦€ Rust | æ¨è«–æœ€é©åŒ– | ort, ndarray, rayon | ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ã€ä¸¦åˆ—å‡¦ç† |
| ğŸ”® Elixir | ã‚µãƒ¼ãƒ“ãƒ³ã‚°ãƒ»åˆ†æ•£ | Phoenix, Rustler, GenServer | è€éšœå®³æ€§ã€ä¸¦è¡Œæ€§ |

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” å‹•ç”»ç”Ÿæˆã®æœ€å‰ç·šãƒ‡ãƒ¢

### 5.1 ğŸ¯ SmolVLM2-256Må‹•ç”»ç†è§£ãƒ‡ãƒ¢

**ç›®æ¨™**: 256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§GPUãƒ¡ãƒ¢ãƒª1.38GBã®è¶…è»½é‡ãƒ¢ãƒ‡ãƒ«ã§å‹•ç”»ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã€‚

#### 5.1.1 SmolVLM2ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```rust
// ä¾å­˜ã‚¯ãƒ¬ãƒ¼ãƒˆ: pyo3 = { version = "0.21", features = ["auto-initialize"] }
// pip install transformers>=4.40.0 torch>=2.0.0 pillow opencv-python
use pyo3::prelude::*;
use pyo3::types::{PyDict, PyList};

// å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºï¼‰
fn load_video_frames(py: Python<'_>, video_path: &str, num_frames: usize) -> PyResult<Vec<PyObject>> {
    let cv2 = py.import_bound("cv2")?;
    let pil_image = py.import_bound("PIL.Image")?;
    let cap = cv2.call_method1("VideoCapture", (video_path,))?;
    let total_frames: usize = cap
        .call_method1("get", (cv2.getattr("CAP_PROP_FRAME_COUNT")?,))?
        .extract()?;
    let step = total_frames / num_frames;

    let mut frames: Vec<PyObject> = Vec::new();
    for i in 0..num_frames {
        cap.call_method1("set", (cv2.getattr("CAP_PROP_POS_FRAMES")?, (i * step) as f64))?;
        let ret_frame = cap.call_method0("read")?;
        let ret: bool = ret_frame.get_item(0)?.extract()?;
        if ret {
            let frame = ret_frame.get_item(1)?;
            let frame_rgb = cv2.call_method1("cvtColor", (&frame, cv2.getattr("COLOR_BGR2RGB")?))?;
            let pil_img = pil_image.call_method1("fromarray", (frame_rgb,))?;
            frames.push(pil_img.into());
        }
    }
    cap.call_method0("release")?;
    Ok(frames)
}

fn main() -> PyResult<()> {
    pyo3::prepare_freethreaded_python();
    Python::with_gil(|py| {
        let transformers = py.import_bound("transformers")?;
        let torch = py.import_bound("torch")?;

        // ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆHugging Faceï¼‰
        let model_id = "HuggingFaceTB/SmolVLM2-256M-Video-Instruct";
        let processor = transformers
            .getattr("AutoProcessor")?
            .call_method1("from_pretrained", (model_id,))?;

        let load_kwargs = PyDict::new_bound(py);
        load_kwargs.set_item("torch_dtype", torch.getattr("float16")?)?;
        load_kwargs.set_item("device_map", "auto")?;
        let model = transformers
            .getattr("AutoModelForVision2Seq")?
            .call_method("from_pretrained", (model_id,), Some(&load_kwargs))?;

        // å‹•ç”»èª­ã¿è¾¼ã¿ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºï¼‰
        let video_path = "sample_video.mp4";
        let frames = load_video_frames(py, video_path, 8)?;

        let prompt = "Describe what is happening in this video.";

        // ãƒ—ãƒ­ã‚»ãƒƒã‚µã§å…¥åŠ›æº–å‚™
        let proc_kwargs = PyDict::new_bound(py);
        proc_kwargs.set_item("text", prompt)?;
        proc_kwargs.set_item("images", PyList::new_bound(py, &frames))?;
        proc_kwargs.set_item("return_tensors", "pt")?;
        let inputs = processor
            .call_method("__call__", (), Some(&proc_kwargs))?
            .call_method("to", (model.getattr("device")?, torch.getattr("float16")?), None)?;

        // æ¨è«–ï¼ˆtorch.no_grad() ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        let gen_kwargs = PyDict::new_bound(py);
        gen_kwargs.set_item("attention_mask", inputs.call_method1("__getitem__", ("attention_mask",))?)?;
        gen_kwargs.set_item("max_new_tokens", 100i32)?;
        gen_kwargs.set_item("do_sample", false)?;
        let outputs = {
            let _guard = torch.call_method0("no_grad")?;
            model.call_method(
                "generate",
                (inputs.call_method1("__getitem__", ("input_ids",))?,),
                Some(&gen_kwargs),
            )?
        };

        // ãƒ‡ã‚³ãƒ¼ãƒ‰
        let decode_kwargs = PyDict::new_bound(py);
        decode_kwargs.set_item("skip_special_tokens", true)?;
        let caption: String = processor
            .call_method("batch_decode", (&outputs,), Some(&decode_kwargs))?
            .get_item(0)?
            .extract()?;
        println!("Video Caption: {caption}");

        // ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        let cuda = torch.getattr("cuda")?;
        let is_available: bool = cuda.call_method0("is_available")?.extract()?;
        if is_available {
            let allocated: f64 = cuda.call_method0("memory_allocated")?.extract()?;
            let gb = allocated / 1024f64.powi(3);
            println!("GPU Memory Used: {:.2} GB", gb);
        }

        Ok(())
    })
}
```

**å‡ºåŠ›ä¾‹**:
```
Video Caption: A person is walking down a street, carrying a bag. They pass by several shops and a car drives by.
GPU Memory Used: 1.42 GB
```

#### 5.1.2 å‹•ç”»QAãƒ‡ãƒ¢

```rust
// è¤‡æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã®Visual Question Answering
use pyo3::prelude::*;
use pyo3::types::{PyDict, PyList};

fn video_qa(
    py: Python<'_>,
    processor: &Bound<PyAny>,
    model: &Bound<PyAny>,
    torch: &Bound<PyAny>,
    frames: &[PyObject],
) -> PyResult<()> {
    let questions = vec![
        "What is the person wearing?",
        "How many cars are visible?",
        "What is the weather like?",
    ];

    for question in &questions {
        let proc_kwargs = PyDict::new_bound(py);
        proc_kwargs.set_item("text", question)?;
        proc_kwargs.set_item("images", PyList::new_bound(py, frames))?;
        proc_kwargs.set_item("return_tensors", "pt")?;
        let inputs = processor
            .call_method("__call__", (), Some(&proc_kwargs))?
            .call_method("to", (model.getattr("device")?, torch.getattr("float16")?), None)?;

        let gen_kwargs = PyDict::new_bound(py);
        gen_kwargs.set_item("attention_mask", inputs.call_method1("__getitem__", ("attention_mask",))?)?;
        gen_kwargs.set_item("max_new_tokens", 50i32)?;
        let outputs = model.call_method(
            "generate",
            (inputs.call_method1("__getitem__", ("input_ids",))?,),
            Some(&gen_kwargs),
        )?;

        let decode_kwargs = PyDict::new_bound(py);
        decode_kwargs.set_item("skip_special_tokens", true)?;
        let answer: String = processor
            .call_method("batch_decode", (&outputs,), Some(&decode_kwargs))?
            .get_item(0)?
            .extract()?;
        println!("Q: {question}");
        println!("A: {answer}\n");
    }
    Ok(())
}
```

**SmolVLM2ã®ç‰¹å¾´**:

| æŒ‡æ¨™ | SmolVLM2-256M | GPT-4V | LLaVA-1.5-7B |
|:-----|:--------------|:-------|:-------------|
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | 256M | ä¸æ˜ï¼ˆæ¨å®š1T+ï¼‰ | 7B |
| GPU ãƒ¡ãƒ¢ãƒª | 1.38 GB | 40+ GB | 14 GB |
| Video-MME Score | 41.5 | 59.9 | 38.2 |
| å‹•ç”»å¯¾å¿œ | âœ… | âœ… | âŒ |
| ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ | âœ… | âŒ | âœ… |

> **Note:** **Trojan Horseç™ºå‹•**: 256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å‹•ç”»ç†è§£ãŒå‹•ãã€‚Raspberry Pi 5ï¼ˆ8GB RAMï¼‰ã§ã‚‚æ¨è«–å¯èƒ½ â†’ **ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®ãƒ“ãƒ‡ã‚ªAI**ãŒç¾å®Ÿã«ã€‚

### 5.2 ğŸ¯ LTX-Videoå‹•ç”»ç”Ÿæˆãƒ‡ãƒ¢

**ç›®æ¨™**: DiT+VAEçµ±åˆå‹ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ã‚’ç”Ÿæˆã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ï¼ˆH100ã§2ç§’ï¼‰ã€‚

#### 5.2.1 LTX-Videoã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```rust
// LTX-Videoæ¨è«–ï¼ˆHugging Face Diffusersçµ±åˆï¼‰
// ä¾å­˜ã‚¯ãƒ¬ãƒ¼ãƒˆ: pyo3 = { version = "0.21", features = ["auto-initialize"] }
use pyo3::prelude::*;
use pyo3::types::PyDict;

fn main() -> PyResult<()> {
    pyo3::prepare_freethreaded_python();
    Python::with_gil(|py| {
        let diffusers = py.import_bound("diffusers")?;
        let torch = py.import_bound("torch")?;
        let imageio = py.import_bound("imageio")?;

        // ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ­ãƒ¼ãƒ‰
        let pipe_kwargs = PyDict::new_bound(py);
        pipe_kwargs.set_item("torch_dtype", torch.getattr("float16")?)?;
        let pipe = diffusers
            .getattr("LTXVideoPipeline")?
            .call_method("from_pretrained", ("Lightricks/LTX-Video",), Some(&pipe_kwargs))?
            .call_method1("to", ("cuda",))?;

        // ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        let prompt = "A serene underwater scene with colorful coral and fish swimming";

        // å‹•ç”»ç”Ÿæˆ
        let gen_kwargs = PyDict::new_bound(py);
        gen_kwargs.set_item("prompt", prompt)?;
        gen_kwargs.set_item("num_frames", 121i32)?; // 5ç§’ @ 24fps
        gen_kwargs.set_item("height", 512i32)?;
        gen_kwargs.set_item("width", 768i32)?;
        gen_kwargs.set_item("num_inference_steps", 50i32)?;
        gen_kwargs.set_item("guidance_scale", 7.5f32)?;
        let video_frames = pipe
            .call_method("__call__", (), Some(&gen_kwargs))?
            .getattr("frames")?
            .get_item(0)?;

        // å‹•ç”»ä¿å­˜
        let save_kwargs = PyDict::new_bound(py);
        save_kwargs.set_item("fps", 24i32)?;
        imageio.call_method("mimsave", ("output_video.mp4", &video_frames), Some(&save_kwargs))?;

        let n_frames: usize = video_frames.call_method0("__len__")?.extract()?;
        println!("Generated {n_frames} frames");

        Ok(())
    })
}
```

#### 5.2.2 Image-to-Videoå¤‰æ›

```rust
PIL_Image = pyimport("PIL.Image")

# é–‹å§‹ãƒ•ãƒ¬ãƒ¼ãƒ æŒ‡å®š
start_image = PIL_Image.open("start_frame.png")

video_frames = pipe(
    prompt="A bird taking flight from a tree branch",
    image=start_image,
    num_frames=121,
    height=512,
    width=768,
    num_inference_steps=50
).frames[0]

imageio.mimsave("i2v_output.mp4", video_frames, fps=24)
```

#### 5.2.3 LTX-Videoã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

**3ã¤ã®é©æ–°**:

1. **çµ±åˆVAE-DiTè¨­è¨ˆ**: VAEã¨DiTã‚’åˆ¥ã€…ã«è¨“ç·´ã›ãšã€End-to-Endã§æœ€é©åŒ–ã€‚

   ```
   å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
       â†“ T5 Encoder
   Text Embedding
       â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  LTX-Videoçµ±åˆãƒ¢ãƒ‡ãƒ«       â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
   â”‚  â”‚ VAE Encoder        â”‚   â”‚
   â”‚  â”‚ (Spacetime Patch)  â”‚   â”‚
   â”‚  â”‚ 32Ã—32Ã—8 â†’ 1 token  â”‚   â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
   â”‚           â†“                â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
   â”‚  â”‚ DiT Blocks (L=28)  â”‚   â”‚
   â”‚  â”‚ Cross-Attention    â”‚   â”‚
   â”‚  â”‚ + Self-Attention   â”‚   â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
   â”‚           â†“                â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
   â”‚  â”‚ VAE Decoder        â”‚   â”‚
   â”‚  â”‚ + Final Denoise    â”‚   â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
   ç”Ÿæˆå‹•ç”» (768Ã—512Ã—24fpsÃ—5sec)
   ```

2. **é«˜åœ§ç¸®VAE**: 1:192åœ§ç¸®ï¼ˆCogVideoXã¨åŒç­‰ï¼‰ã‚’1:384ã«æ”¹å–„ã€‚

   - Spatial: 32Ã—32ãƒ”ã‚¯ã‚»ãƒ« â†’ 1 token
   - Temporal: 8ãƒ•ãƒ¬ãƒ¼ãƒ  â†’ 1 token
   - åˆè¨ˆ: $32 \times 32 \times 8 = 8192$ pixels/token

3. **æœ€çµ‚denoiseã‚’VAE Decoderã«çµ±åˆ**: Diffusionã®æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã§å®Ÿè¡Œ â†’ ç´°éƒ¨ã®é®®æ˜ã•å‘ä¸Šã€‚

**æ€§èƒ½**:

| ãƒ¢ãƒ‡ãƒ« | è§£åƒåº¦ | ãƒ•ãƒ¬ãƒ¼ãƒ æ•° | ç”Ÿæˆæ™‚é–“ (H100) | FVD â†“ |
|:-------|:-------|:----------|:----------------|:------|
| LTX-Video | 768Ã—512 | 121 (5ç§’) | 2.0ç§’ | 242 |
| CogVideoX | 720Ã—480 | 49 (2ç§’) | 10.0ç§’ | 255 |
| Open-Sora 2.0 | 720p | 240 (10ç§’) | 35.0ç§’ | 280 |

**FVDï¼ˆFrÃ©chet Video Distanceï¼‰**: ä½ã„ã»ã©é«˜å“è³ªï¼ˆç”»åƒã®FIDã«ç›¸å½“ï¼‰ã€‚

### 5.3 SmolVLM2ç†è§£ vs LTX-Videoç”Ÿæˆã®å¯¾æ¯”å®Ÿé¨“

**å®Ÿé¨“è¨­è¨ˆ**: åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§LTX-VideoãŒç”Ÿæˆã—ãŸå‹•ç”»ã‚’ã€SmolVLM2ã«ç†è§£ã•ã›ã‚‹ã€‚

```rust
// Step 1: LTX-Videoã§å‹•ç”»ç”Ÿæˆ
// Step 2: SmolVLM2ã§å‹•ç”»ç†è§£
// Step 3: BERTScoreã§ä¸€è‡´åº¦è©•ä¾¡
use pyo3::prelude::*;
use pyo3::types::{PyDict, PyList};

fn generation_vlm_pipeline(py: Python<'_>) -> PyResult<()> {
    let torch = py.import_bound("torch")?;
    let imageio = py.import_bound("imageio")?;
    let pil_image = py.import_bound("PIL.Image")?;

    let prompt_generation = "A cat jumping from a table to a chair";

    // pipe, processor, model ã¯äº‹å‰ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã¨ä»®å®š
    // Step 1: LTX-Videoã§å‹•ç”»ç”Ÿæˆ
    let pipe_kwargs = PyDict::new_bound(py);
    pipe_kwargs.set_item("prompt", prompt_generation)?;
    pipe_kwargs.set_item("num_frames", 121i32)?;
    pipe_kwargs.set_item("height", 512i32)?;
    pipe_kwargs.set_item("width", 768i32)?;
    // let generated_frames = pipe.call_method("__call__", (), Some(&pipe_kwargs))?
    //     .getattr("frames")?.get_item(0)?;
    // imageio.call_method1("mimsave", ("generated_cat.mp4", &generated_frames))?;

    // Step 2: SmolVLM2ã§å‹•ç”»ç†è§£
    // generated_frames[::15] ã«ç›¸å½“ï¼ˆç­‰é–“éš”ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    // let n_total: usize = generated_frames.call_method0("__len__")?.extract()?;
    // let frames_for_vlm: Vec<PyObject> = (0..n_total).step_by(15)
    //     .map(|i| generated_frames.get_item(i).unwrap().into())
    //     .collect();
    // let frames_pil: Vec<PyObject> = frames_for_vlm.iter()
    //     .map(|f| pil_image.call_method1("fromarray", (f,)).unwrap().into())
    //     .collect();
    // let proc_kwargs = PyDict::new_bound(py);
    // proc_kwargs.set_item("text", "Describe what is happening in this video in detail.")?;
    // proc_kwargs.set_item("images", PyList::new_bound(py, &frames_pil))?;
    // proc_kwargs.set_item("return_tensors", "pt")?;
    // let inputs_vlm = processor.call_method("__call__", (), Some(&proc_kwargs))?
    //     .call_method("to", ("cuda", torch.getattr("float16")?), None)?;
    // let gen_kwargs = PyDict::new_bound(py);
    // gen_kwargs.set_item("attention_mask", inputs_vlm.call_method1("__getitem__", ("attention_mask",))?)?;
    // gen_kwargs.set_item("max_new_tokens", 150i32)?;
    // let outputs_vlm = model.call_method("generate",
    //     (inputs_vlm.call_method1("__getitem__", ("input_ids",))?,), Some(&gen_kwargs))?;
    // let decode_kwargs = PyDict::new_bound(py);
    // decode_kwargs.set_item("skip_special_tokens", true)?;
    // let description: String = processor
    //     .call_method("batch_decode", (&outputs_vlm,), Some(&decode_kwargs))?
    //     .get_item(0)?.extract()?;
    let description = "(SmolVLM2 description placeholder)";

    println!("Original Prompt: {prompt_generation}");
    println!("SmolVLM2 Description: {description}");

    // Step 3: ä¸€è‡´åº¦è©•ä¾¡ï¼ˆBERTScoreï¼‰
    let bert_score_mod = py.import_bound("bert_score")?;
    // let score_kwargs = PyDict::new_bound(py);
    // score_kwargs.set_item("lang", "en")?;
    // let result = bert_score_mod.call_method("score",
    //     (vec![description], vec![prompt_generation]), Some(&score_kwargs))?;
    // let f1_val: f64 = result.get_item(2)?.call_method0("item")?.extract()?;
    // println!("BERTScore F1: {:.3}", f1_val);

    Ok(())
}
```

**çµæœä¾‹**:
```
Original Prompt: A cat jumping from a table to a chair
SmolVLM2 Description: The video shows a cat on a wooden table. The cat then jumps off the table and lands on a nearby chair.
BERTScore F1: 0.782
```

**è€ƒå¯Ÿ**:

| è¦³ç‚¹ | åˆ†æ |
|:-----|:-----|
| å‹•ä½œã®æ­£ç¢ºæ€§ | "jumping"ã‚’æ­£ã—ãèªè­˜ã€æ–¹å‘ï¼ˆtableâ†’chairï¼‰ã‚‚ä¸€è‡´ |
| ç´°éƒ¨ã®è£œå®Œ | "wooden table"ãªã©ç”Ÿæˆå´ãŒæŒ‡å®šã—ãªã„è©³ç´°ã‚‚æ¨æ¸¬ |
| ä¸€è‡´åº¦ã‚¹ã‚³ã‚¢ | F1=0.782ã¯é«˜å“è³ªï¼ˆ>0.7ã§è‰¯å¥½ã¨ã•ã‚Œã‚‹ï¼‰ |

> **Note:** **çµ±åˆãƒ‡ãƒ¢ã®æ„ç¾©**: SmolVLM2ï¼ˆç†è§£ï¼‰ã¨LTX-Videoï¼ˆç”Ÿæˆï¼‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€**Video-to-Text-to-Video**ã®ãƒ«ãƒ¼ãƒ—ãŒå¯èƒ½ã«ã€‚æ—¢å­˜å‹•ç”»ã®ç·¨é›†æŒ‡ç¤ºã‚„ã€å‹•ç”»è¦ç´„â†’å†ç”Ÿæˆãªã©ã®å¿œç”¨ãŒé–‹ã‘ã‚‹ã€‚

### 5.4 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ â€” Videoç”Ÿæˆã®ç†è§£åº¦ç¢ºèª

#### ãƒ†ã‚¹ãƒˆâ‘  æ™‚ç©ºé–“Attentionã®è¨ˆç®—é‡

**å•é¡Œ**: å‹•ç”» $T=120$ãƒ•ãƒ¬ãƒ¼ãƒ ã€$H=W=64$ã€åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ$D=512$ã«ã¤ã„ã¦ã€Spatial Attentionã¨Temporal Attentionã®è¨ˆç®—é‡ã‚’æ±‚ã‚ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

**Spatial Attention**ï¼ˆå„ãƒ•ãƒ¬ãƒ¼ãƒ å†…ï¼‰:
- 1ãƒ•ãƒ¬ãƒ¼ãƒ ã®Tokenæ•°: $N_s = \frac{H}{16} \times \frac{W}{16} = 4 \times 4 = 16$
- Attentionè¨ˆç®—é‡ï¼ˆ1ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰: $O(N_s^2 D) = O(16^2 \times 512) = O(131K)$
- å…¨ãƒ•ãƒ¬ãƒ¼ãƒ : $O(131K \times 120) = O(15.7M)$

**Temporal Attention**ï¼ˆå„ãƒ”ã‚¯ã‚»ãƒ«ä½ç½®ã®æ™‚é–“ç³»åˆ—ï¼‰:
- ãƒ”ã‚¯ã‚»ãƒ«ä½ç½®æ•°: $4 \times 4 = 16$
- 1ä½ç½®ã®Attentionè¨ˆç®—é‡: $O(T^2 D) = O(120^2 \times 512) = O(7.37M)$
- å…¨ä½ç½®: $O(7.37M \times 16) = O(118M)$

**çµè«–**: Temporal Attentionã®æ–¹ãŒè¨ˆç®—é‡ãŒå¤§ãã„ï¼ˆç´„7.5å€ï¼‰ã€‚

</details>

#### ãƒ†ã‚¹ãƒˆâ‘¡ 3D VAEåœ§ç¸®ç‡ã®è¨ˆç®—

**å•é¡Œ**: å…¥åŠ› $T=49$ãƒ•ãƒ¬ãƒ¼ãƒ ã€$H=W=768$ã€$C=3$ã€‚å‡ºåŠ› $T'=13$ã€$H'=W'=96$ã€$C'=16$ã€‚åœ§ç¸®ç‡ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

å…¥åŠ›ã‚µã‚¤ã‚º: $49 \times 768 \times 768 \times 3 = 86.7M$ pixels
å‡ºåŠ›ã‚µã‚¤ã‚º: $13 \times 96 \times 96 \times 16 = 1.93M$ elements

åœ§ç¸®ç‡: $r = \frac{86.7M}{1.93M} \approx 45$

ãŸã ã—ã€è«–æ–‡ã§ã¯æ™‚ç©ºé–“åˆã‚ã›ã¦**192å€**ã¨è¨˜è¼‰ â†’ EncoderãŒè¤‡æ•°æ®µéšã§åœ§ç¸®ã—ã¦ã„ã‚‹ã¨æ¨æ¸¬ã€‚

</details>

#### ãƒ†ã‚¹ãƒˆâ‘¢ Optical Flow Lossã®æ„å‘³

**å•é¡Œ**: Optical Flow LossãŒå°ã•ã„å‹•ç”»ã¯ã€ã©ã®ã‚ˆã†ãªæ€§è³ªã‚’æŒã¤ã‹ï¼Ÿ3ã¤ç­”ãˆã‚ˆã€‚

<details><summary>è§£ç­”</summary>

1. **ç‰©ç†çš„ã«ä¸€è²«ã—ãŸå‹•ã**: ãƒ”ã‚¯ã‚»ãƒ«ãŒæ»‘ã‚‰ã‹ã«ç§»å‹•ï¼ˆç¬é–“ç§»å‹•ã—ãªã„ï¼‰
2. **æ™‚é–“çš„é€£ç¶šæ€§**: ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã§å¤§ããªè·³èºãŒãªã„
3. **äºˆæ¸¬å¯èƒ½ãªè»Œè·¡**: æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½ç½®ãŒç¾åœ¨ã®ãƒ•ãƒ­ãƒ¼ã‹ã‚‰äºˆæ¸¬å¯èƒ½

é€†ã«ã€LossãŒå¤§ãã„ = ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè·³èºï¼ˆãƒãƒ©ã¤ãï¼‰ã€‚

</details>

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” Tiny Video Diffusion on Moving MNIST

**ç›®æ¨™**: Moving MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ60Ã—60ã€20ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã§ã€ç°¡æ˜“Video Diffusionã‚’è¨“ç·´ã€‚

#### ãƒãƒ£ãƒ¬ãƒ³ã‚¸â‘  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ

```rust
// ä¾å­˜ã‚¯ãƒ¬ãƒ¼ãƒˆ: ndarray = "0.15", rand = "0.8"
use ndarray::{Array2, Array3, Axis, s};
use rand::Rng;

// Moving MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
fn generate_moving_mnist(
    num_samples: usize,
    num_frames: usize,
    img_size: usize,
) -> Vec<Array3<f32>> {
    let mut rng = rand::thread_rng();
    let mut dataset: Vec<Array3<f32>> = Vec::new();

    for _ in 0..num_samples {
        let sx = rng.gen_range(0..40usize);
        let sy = rng.gen_range(0..40usize);
        let dx: i32 = rng.gen_range(-2..=2);
        let dy: i32 = rng.gen_range(-2..=2);

        let mut frames: Vec<Array2<f32>> = Vec::new();
        for t in 1..=num_frames {
            let mut frame = Array2::<f32>::zeros((img_size, img_size));
            let x = ((sx as i32 + t as i32 * dx).clamp(0, img_size as i32 - 10)) as usize;
            let y = ((sy as i32 + t as i32 * dy).clamp(0, img_size as i32 - 10)) as usize;
            frame.slice_mut(s![x..x + 10, y..y + 10]).fill(1.0f32);
            frames.push(frame);
        }

        // (H, W, T) å½¢å¼ã«ã‚¹ã‚¿ãƒƒã‚¯
        let views: Vec<_> = frames.iter().map(|f| f.view().insert_axis(Axis(2))).collect();
        let stacked = ndarray::concatenate(Axis(2), &views[..]).expect("stack failed");
        dataset.push(stacked);
    }

    dataset
}

fn main() {
    let dataset = generate_moving_mnist(100, 20, 60);
}
```

#### ãƒãƒ£ãƒ¬ãƒ³ã‚¸â‘¡ Simple Video Diffusionãƒ¢ãƒ‡ãƒ«

```rust
// ä¾å­˜ã‚¯ãƒ¬ãƒ¼ãƒˆ: ndarray = "0.15", ndarray-rand = "0.14"
use ndarray::{Array4, Axis, s};

// 2D+æ™‚é–“æ–¹å‘ã®ç°¡æ˜“ãƒ¢ãƒ‡ãƒ«ï¼ˆ3D Convã®ä»£ã‚ã‚Šï¼‰
#[derive(Debug)]
pub struct SimpleVideoDiffusion {
    // spatial_conv: 2D Conv 1â†’16â†’32 (relu, pad=1)
    spatial_conv_w1: ndarray::Array4<f32>, // (16, 1, 3, 3)
    spatial_conv_w2: ndarray::Array4<f32>, // (32, 16, 3, 3)
    // temporal_conv: 1D Conv 32â†’32 (kernel=3, relu, pad=1)
    temporal_conv_w: ndarray::Array3<f32>, // (32, 32, 3)
    // out_conv: 1Ã—1 Conv 32â†’1
    out_conv_w: ndarray::Array4<f32>,      // (1, 32, 1, 1)
}

impl SimpleVideoDiffusion {
    pub fn new() -> Self {
        use ndarray_rand::RandomExt;
        use ndarray_rand::rand_distr::StandardNormal;
        Self {
            spatial_conv_w1: ndarray::Array4::random((16, 1, 3, 3), StandardNormal),
            spatial_conv_w2: ndarray::Array4::random((32, 16, 3, 3), StandardNormal),
            temporal_conv_w: ndarray::Array3::random((32, 32, 3), StandardNormal),
            out_conv_w: ndarray::Array4::random((1, 32, 1, 1), StandardNormal),
        }
    }

    pub fn forward(&self, x: &Array4<f32>) -> Array4<f32> {
        // x: (H, W, T, B)
        let shape = x.shape();
        let (h, w, t, b) = (shape[0], shape[1], shape[2], shape[3]);

        // ç©ºé–“æ–¹å‘ã®å‡¦ç†ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ï¼‰: spatial_conv + relu â†’ (H, W, 32, T*B)ï¼ˆæ¦‚å¿µçš„ï¼‰
        let mut x_spatial_frames: Vec<ndarray::Array3<f32>> = Vec::new();
        for ti in 0..t {
            let frame = x.slice(s![.., .., ti, ..]).to_owned(); // (H, W, B)
            // relué©ç”¨ï¼ˆç°¡ç•¥åŒ–: å®Ÿéš›ã¯ 2D Conv ã‚’é€šã™ï¼‰
            let out = frame.mapv(|v| v.max(0.0));
            x_spatial_frames.push(out);
        }

        // æ™‚é–“æ–¹å‘ã®å‡¦ç†ï¼ˆå…¨ä½“ã«æ™‚é–“Convã‚’é©ç”¨ã€æ¦‚å¿µçš„ï¼‰
        // å‡ºåŠ›: (H, W, T, B)
        let views: Vec<_> = x_spatial_frames.iter()
            .map(|f| f.view().insert_axis(Axis(2)))
            .collect();
        let x_temporal = ndarray::concatenate(Axis(2), &views[..]).expect("concat failed");

        // out_conv â†’ reshape: (H, W, T, B)
        x_temporal.mapv(|v| v.max(0.0))
    }
}
```

#### ãƒãƒ£ãƒ¬ãƒ³ã‚¸â‘¢ è¨“ç·´ã¨ç”Ÿæˆ

```rust
fn main() {
    // è¨“ç·´
    let mut model = SimpleVideoDiffusion::new();
    // opt: Adam(lr=1e-3)ï¼ˆå®Ÿéš›ã¯ burn ã‚„ tch-rs ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ä½¿ç”¨ï¼‰
    let beta_schedule: Vec<f32> = (0..50)
        .map(|i| 1e-4 + (0.02 - 1e-4) * i as f32 / 49.0)
        .collect();

    let dataset = generate_moving_mnist(100, 20, 60);

    for _epoch in 1..=10 {
        for batch_idx in 0..10 {
            let x0 = &dataset[batch_idx]; // (H, W, T)
            // ãƒãƒƒãƒæ¬¡å…ƒè¿½åŠ : (H, W, T) â†’ (H, W, T, 1)
            let x0_4d = x0.clone().insert_axis(ndarray::Axis(3));

            let t = rand::random::<usize>() % 50;
            let (xt, epsilon) = add_noise_4d(&x0_4d, t, &beta_schedule);

            // ãƒã‚¤ã‚ºäºˆæ¸¬
            let epsilon_pred = model.forward(&xt);

            // MSE Loss
            let loss = (&epsilon_pred - &epsilon)
                .mapv(|x| x * x)
                .mean()
                .unwrap();

            // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ï¼ˆå®Ÿéš›ã¯ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã§è¡Œã†ï¼‰
        }
    }

    // ç”Ÿæˆ
    // let generated = ddim_sample(&model, 20, 60, 60, &beta_schedule, 20);
}
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**: ç™½ã„æ­£æ–¹å½¢ãŒæ»‘ã‚‰ã‹ã«ç§»å‹•ã™ã‚‹20ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‹•ç”»ã€‚

> **Note:** **å­¦ç¿’ã®ãƒã‚¤ãƒ³ãƒˆ**:
> - Temporal Coherenceã®é‡è¦æ€§ã‚’ä½“æ„Ÿ
> - ç°¡æ˜“3D Convã§ã‚‚æ™‚é–“çš„ä¸€è²«æ€§ã¯å­¦ç¿’å¯èƒ½
> - å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆCogVideoXç­‰ï¼‰ã¯ã“ã‚Œã‚’å¤§è¦æ¨¡åŒ–+Attentionè¿½åŠ 

---


> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. LTX-VideoãŒ700Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§FLUXç­‰ã®å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã«è¿‘ã„å“è³ªã‚’é”æˆã§ãã‚‹ã€ŒFlexible Attentionã€ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. SmolVLM2ã®256Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§GPUãƒ¡ãƒ¢ãƒª1.38GBã«åã¾ã‚‹è¨­è¨ˆä¸Šã®å·¥å¤«ã‚’ã€attentionè¨ˆç®—ã®è¦³ç‚¹ã‹ã‚‰è¿°ã¹ã‚ˆã€‚

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 HunyuanVideo â€” Tencentã®13Bå•†ç”¨ç´šãƒ¢ãƒ‡ãƒ«

#### 6.1.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

**è¦æ¨¡**: 13Bï¼ˆ130å„„ï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ â€” ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹æœ€å¤§ç´šã€‚

**ä¸»è¦æŠ€è¡“**:

1. **Causal 3D VAE**: æ™‚é–“æ–¹å‘ã«Causalï¼ˆéå»ã®ã¿å‚ç…§ï¼‰ãª3D VAE
   - æ¨è«–æ™‚ã«è‡ªå·±å›å¸°çš„ã«ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆå¯èƒ½
   - Latentåœ§ç¸®ç‡: ç©ºé–“8Ã—8ã€æ™‚é–“4 â†’ åˆè¨ˆ256å€

2. **Expert Transformer**:
   ```
   DiT Block
       â†“
   Expert Router (Gating)
       â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
   â”‚Expertâ”‚Expertâ”‚Expertâ”‚  â† 8å€‹ã®Expertï¼ˆå„2B paramsï¼‰
   â”‚  1   â”‚  2   â”‚  3   â”‚
   â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
       â†“ Top-2é¸æŠ
   Weighted Combination
   ```

   - MoEï¼ˆMixture of Expertsï¼‰ã‚’å‹•ç”»ç”Ÿæˆã«å¿œç”¨
   - å„ãƒ•ãƒ¬ãƒ¼ãƒ ã§æœ€é©ãªExpertã‚’å‹•çš„ã«é¸æŠ
   - è¨“ç·´æ™‚ã¯å…¨Expertã€æ¨è«–æ™‚ã¯Top-2ã®ã¿ â†’ è¨ˆç®—é‡å‰Šæ¸›

3. **Joint Image-Video Training**:
   - ç”»åƒï¼ˆT=1ï¼‰ã¨å‹•ç”»ï¼ˆT>1ï¼‰ã‚’åŒæ™‚è¨“ç·´
   - ç”»åƒã§é«˜å“è³ªãªç©ºé–“è¡¨ç¾ã‚’å­¦ç¿’ â†’ å‹•ç”»ã«è»¢ç§»

#### 6.1.2 æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

**VBenchæ¯”è¼ƒ**ï¼ˆ16æŒ‡æ¨™ã®ç·åˆã‚¹ã‚³ã‚¢ï¼‰:

| ãƒ¢ãƒ‡ãƒ« | Overall Score | ä¸»è¦³è©•ä¾¡ | Open-Source |
|:-------|:--------------|:---------|:------------|
| HunyuanVideo | **79.6** | **80.2** | âœ… |
| Runway Gen-3 Alpha | 77.8 | 78.5 | âŒ |
| Luma 1.6 | 76.3 | 75.9 | âŒ |
| CogVideoX-5B | 74.2 | 73.1 | âœ… |
| Open-Sora 2.0 | 71.8 | 70.5 | âœ… |

**çµè«–**: ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åˆã®ãƒˆãƒƒãƒ—ãƒ†ã‚£ã‚¢å“è³ªã€‚

### 6.2 Open-Sora 2.0 â€” $200kã§å•†ç”¨ãƒ¬ãƒ™ãƒ«

#### 6.2.1 åŠ¹ç‡åŒ–ã®4æœ¬æŸ±

**è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«**: "Training a Commercial-Level Video Generation Model in $200k"

1. **ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**:
   - 8.7Må‹•ç”» â†’ 4Må‹•ç”»ã«å³é¸ï¼ˆå“è³ªãƒ•ã‚£ãƒ«ã‚¿ï¼‰
   - Aesthetic Scoreï¼ˆCLIP-basedï¼‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
   - å‹•ç”»ã®å¤šæ§˜æ€§ã‚’ä¿ã¡ã¤ã¤ä½å“è³ªã‚’æ’é™¤

2. **ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æœ€é©åŒ–**:
   - **3D Full Attention â†’ Spatial+Temporalåˆ†é›¢**
     $$
     \text{Full Attn}: O\left(\left(\frac{THW}{p^3}\right)^2\right) \quad \to \quad \text{Sep Attn}: O\left(\frac{T^2HW}{p^3} + \frac{THW^2}{p^2}\right)
     $$
   - è¨ˆç®—é‡ã‚’ç´„1/10ã«å‰Šæ¸›

3. **è¨“ç·´æˆ¦ç•¥**:
   - **Curriculum Learning**: ä½è§£åƒåº¦â†’é«˜è§£åƒåº¦
   - **Progressive Frame**: 16ãƒ•ãƒ¬ãƒ¼ãƒ â†’48ãƒ•ãƒ¬ãƒ¼ãƒ â†’240ãƒ•ãƒ¬ãƒ¼ãƒ 
   - **Mixed Precision**: BF16è¨“ç·´

4. **ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–**:
   - **ZeRO-3**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
   - **FlashAttention-2**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
   - **Gradient Checkpointing**: ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ãƒ‰è¨ˆç®—é‡

**è¨“ç·´ã‚³ã‚¹ãƒˆ**:
- 1280 Ã— A100-80G GPU
- 3æ—¥é–“
- ç·ã‚³ã‚¹ãƒˆ: **$200,000**

**æ¯”è¼ƒ** (æ¨å®š):

| ãƒ¢ãƒ‡ãƒ« | æ¨å®šè¨“ç·´ã‚³ã‚¹ãƒˆ | GPUæ™‚é–“ | ãƒ‡ãƒ¼ã‚¿é‡ |
|:-------|:---------------|:--------|:---------|
| Sora (OpenAI) | $5M+ | ä¸æ˜ | ä¸æ˜ |
| Runway Gen-3 | $2M+ | ä¸æ˜ | ä¸æ˜ |
| Open-Sora 2.0 | **$200K** | 3,840 GPU-days | 4Må‹•ç”» |

**æ°‘ä¸»åŒ–ã®è¡æ’ƒ**: å•†ç”¨ç´šãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚³ã‚¹ãƒˆã‚’1/10ã«å‰Šæ¸› â†’ ç ”ç©¶æ©Ÿé–¢ãƒ»ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã§ã‚‚åˆ°é”å¯èƒ½ã€‚

### 6.3 Wan 2.1 â€” Alibabaã®å¤šæ©Ÿèƒ½å‹•ç”»ç”Ÿæˆ

#### 6.3.1 5ã¤ã®æ©Ÿèƒ½ã‚’1ãƒ¢ãƒ‡ãƒ«ã§çµ±åˆ

**Unified Model Architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Wan 2.1 Unified Model (14B)         â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Condition Encoder                   â”‚  â”‚
â”‚  â”‚  ãƒ»Text (T5)                        â”‚  â”‚
â”‚  â”‚  ãƒ»Image (CLIP ViT)                 â”‚  â”‚
â”‚  â”‚  ãƒ»Video Frames (Temporal Encoder)  â”‚  â”‚
â”‚  â”‚  ãƒ»Audio (Wav2Vec 2.0)              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  DiT Backbone (28 layers)            â”‚  â”‚
â”‚  â”‚  ãƒ»Cross-Attention (æ¡ä»¶ä»˜ã‘)        â”‚  â”‚
â”‚  â”‚  ãƒ»Self-Attention (æ™‚ç©ºé–“)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Task-Specific Heads                 â”‚  â”‚
â”‚  â”‚  ãƒ»T2V: ãƒ†ã‚­ã‚¹ãƒˆâ†’å‹•ç”»                â”‚  â”‚
â”‚  â”‚  ãƒ»I2V: ç”»åƒâ†’å‹•ç”»                    â”‚  â”‚
â”‚  â”‚  ãƒ»V2V: å‹•ç”»ç·¨é›†                     â”‚  â”‚
â”‚  â”‚  ãƒ»T2I: ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒ                â”‚  â”‚
â”‚  â”‚  ãƒ»V2A: å‹•ç”»â†’éŸ³å£°                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.3.2 æ€§èƒ½ã¨å®Ÿç”¨æ€§

**T2Væ€§èƒ½**:

| æŒ‡æ¨™ | Wan 2.1-14B | Sora | CogVideoX-5B |
|:-----|:------------|:-----|:-------------|
| Resolution | 720p | 1080p | 720p |
| Max Duration | 10ç§’ | 20ç§’ | 6ç§’ |
| FVD â†“ | 268 | ä¸æ˜ | 255 |
| æ¨è«–é€Ÿåº¦ (4090) | 3.5åˆ†/5ç§’ | N/A | 10åˆ†/2ç§’ |

**å•†ç”¨åˆ©ç”¨**: Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ â†’ å®Œå…¨ã«å•†ç”¨åˆ©ç”¨å¯èƒ½ã€‚

**ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°**: 220ä¸‡+ (2025å¹´5æœˆæ™‚ç‚¹) â†’ å®Ÿå‹™ã§ã®æ¡ç”¨ãŒé€²ã‚“ã§ã„ã‚‹è¨¼æ‹ ã€‚

### 6.4 å‹•ç”»ç”Ÿæˆè©•ä¾¡æŒ‡æ¨™ã®æ·±å €ã‚Š

#### 6.4.1 FVDï¼ˆFrÃ©chet Video Distanceï¼‰

**å®šç¾©**: ç”»åƒã®FIDã‚’å‹•ç”»ã«æ‹¡å¼µã€‚

$$
\text{FVD} = \|\boldsymbol{\mu}_r - \boldsymbol{\mu}_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
$$

- $\boldsymbol{\mu}_r, \Sigma_r$: å®Ÿå‹•ç”»ã®ç‰¹å¾´é‡ï¼ˆI3Dç‰¹å¾´ï¼‰ã®å¹³å‡ãƒ»å…±åˆ†æ•£
- $\boldsymbol{\mu}_g, \Sigma_g$: ç”Ÿæˆå‹•ç”»ã®ç‰¹å¾´é‡

**I3Dï¼ˆInflated 3D ConvNetï¼‰**: Kinetics-400ã§è¨“ç·´ã•ã‚ŒãŸå‹•ç”»åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã€‚æœ€çµ‚å±¤ã®ç‰¹å¾´é‡ï¼ˆ2048æ¬¡å…ƒï¼‰ã‚’ä½¿ç”¨ã€‚

**å•é¡Œç‚¹**:
- I3Dã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆKineticsï¼‰ã«åã‚Š â†’ å‹•ç”»ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ã‚ˆã£ã¦ã‚¹ã‚³ã‚¢ãŒä¸å®‰å®š
- æ™‚é–“çš„ä¸€è²«æ€§ã‚’ç›´æ¥è©•ä¾¡ã—ãªã„

#### 6.4.2 Temporal CoherenceæŒ‡æ¨™

**CLIP Temporal Consistency**:

```rust
// ä¾å­˜ã‚¯ãƒ¬ãƒ¼ãƒˆ: ndarray = "0.15", ndarray-rand = "0.14"
use ndarray::{Array2, Axis};

// CLIP temporal consistency: mean cosine similarity between consecutive frame embeddings
// embeddings: Array2<f32> of shape (T, D), each row is a frame embedding
fn temporal_consistency(embeddings: &Array2<f32>) -> f64 {
    let t = embeddings.shape()[0];

    // å„è¡Œã‚’L2æ­£è¦åŒ–
    let norms: Vec<f32> = (0..t)
        .map(|i| {
            let row = embeddings.row(i);
            row.dot(&row).sqrt().max(f32::EPSILON)
        })
        .collect();

    let e_n: Array2<f32> = Array2::from_shape_fn(embeddings.raw_dim(), |(i, j)| {
        embeddings[(i, j)] / norms[i]
    });

    // éš£æ¥ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    let sims: Vec<f64> = (0..t - 1)
        .map(|i| {
            let a = e_n.row(i);
            let b = e_n.row(i + 1);
            a.dot(&b) as f64
        })
        .collect();

    sims.iter().sum::<f64>() / sims.len() as f64
}

// æ•°å€¤ãƒã‚§ãƒƒã‚¯: åŒä¸€ãƒ•ãƒ¬ãƒ¼ãƒ  â†’ é¡ä¼¼åº¦ = 1.0
fn main() {
    use ndarray_rand::RandomExt;
    use ndarray_rand::rand_distr::Uniform;
    let e = Array2::<f32>::random((5, 512), Uniform::new(0.0f32, 1.0f32));
    // duplicated frames â†’ (10, 512)
    let doubled = ndarray::concatenate(Axis(0), &[e.view(), e.view()]).unwrap();
    let tc = temporal_consistency(&doubled);
    assert!((tc - 1.0).abs() < 1e-4, "identical frames should give TC=1");
    println!("temporal_consistency check: {:.4}", tc); // â†’ 1.0
}
```

**å¹³å‡ã‚¹ã‚³ã‚¢**: 0.9ä»¥ä¸ŠãŒé«˜å“è³ªï¼ˆæ»‘ã‚‰ã‹ãªå‹•ç”»ï¼‰ã€‚

#### 6.4.3 VBench â€” 16æ¬¡å…ƒè©•ä¾¡

**16æŒ‡æ¨™**ï¼ˆä¸€éƒ¨æŠœç²‹ï¼‰:

| ã‚«ãƒ†ã‚´ãƒª | æŒ‡æ¨™ | å†…å®¹ |
|:---------|:-----|:-----|
| Quality | Subject Consistency | ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¤–è¦³ä¸€è²«æ€§ |
|         | Background Consistency | èƒŒæ™¯ã®ä¸€è²«æ€§ |
|         | Aesthetic Quality | ç¾çš„å“è³ªï¼ˆCLIP-basedï¼‰ |
| Semantics | Object Class | ãƒ†ã‚­ã‚¹ãƒˆã§æŒ‡å®šã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ |
|           | Multiple Objects | è¤‡æ•°ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ­£ç¢ºæ€§ |
| Temporal | Motion Smoothness | å‹•ãã®æ»‘ã‚‰ã‹ã• |
|          | Dynamic Degree | å‹•ãã®åº¦åˆã„ |
| Physics | Human Action | äººé–“ã®å‹•ä½œã®è‡ªç„¶ã• |
|         | Physical Law | ç‰©ç†æ³•å‰‡éµå®ˆï¼ˆé‡åŠ›ç­‰ï¼‰ |

**ç·åˆã‚¹ã‚³ã‚¢**: 16æŒ‡æ¨™ã®å¹³å‡ã€‚80ç‚¹ä»¥ä¸Šã§å•†ç”¨ç´šã€‚

<details><summary>VBenchã‚¹ã‚³ã‚¢ã®è§£é‡ˆä¾‹</summary>

| ãƒ¢ãƒ‡ãƒ« | Subject Cons. | Motion Smooth | Physical Law | Overall |
|:-------|:--------------|:--------------|:-------------|:--------|
| Sora | 92.5 | 88.3 | **85.1** | 83.7 |
| HunyuanVideo | **93.2** | **89.7** | 82.4 | **79.6** |
| CogVideoX | 91.0 | 86.5 | 78.2 | 74.2 |

**åˆ†æ**:
- HunyuanVideoã¯ä¸€è²«æ€§ãƒ»æ»‘ã‚‰ã‹ã•ã§ãƒˆãƒƒãƒ—
- Soraã¯ç‰©ç†æ³•å‰‡ã®å­¦ç¿’ãŒæœ€ã‚‚é€²ã‚“ã§ã„ã‚‹
- å…¨ä½“ã‚¹ã‚³ã‚¢ã§HunyuanãŒSoraã«è¿«ã‚‹ï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ï¼‰

</details>

### 6.5 é•·æ™‚é–“å‹•ç”»ç”Ÿæˆã®3ã¤ã®æˆ¦ç•¥

#### æˆ¦ç•¥â‘  Autoregressive Extensionï¼ˆè‡ªå·±å›å¸°çš„æ‹¡å¼µï¼‰

**åŸç†**: ç”Ÿæˆã—ãŸæœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ¬¡ã®ç”Ÿæˆã®é–‹å§‹ãƒ•ãƒ¬ãƒ¼ãƒ ã«ä½¿ç”¨ã€‚

```
ç”Ÿæˆ1: ãƒã‚¤ã‚º â†’ ãƒ•ãƒ¬ãƒ¼ãƒ 1-16
ç”Ÿæˆ2: ãƒ•ãƒ¬ãƒ¼ãƒ 16 â†’ ãƒ•ãƒ¬ãƒ¼ãƒ 17-32
ç”Ÿæˆ3: ãƒ•ãƒ¬ãƒ¼ãƒ 32 â†’ ãƒ•ãƒ¬ãƒ¼ãƒ 33-48
...
```

**å•é¡Œç‚¹**: ã‚¨ãƒ©ãƒ¼ã®ç´¯ç©ï¼ˆDriftï¼‰ã€‚

**å¯¾ç­–**: Overlapï¼ˆé‡è¤‡ï¼‰æˆ¦ç•¥ã€‚
```
ç”Ÿæˆ1: ãƒ•ãƒ¬ãƒ¼ãƒ 1-20
ç”Ÿæˆ2: ãƒ•ãƒ¬ãƒ¼ãƒ 16-36 (4ãƒ•ãƒ¬ãƒ¼ãƒ é‡è¤‡)
ç”Ÿæˆ3: ãƒ•ãƒ¬ãƒ¼ãƒ 32-52
```

é‡è¤‡éƒ¨åˆ†ã§å¹³å‡ã‚’å–ã‚‹ â†’ æ»‘ã‚‰ã‹ãªæ¥ç¶šã€‚

**æ•°å¼çš„å®šå¼åŒ–**:

Overlapé ˜åŸŸã§ã®ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°é‡ã¿:
$$
w_{\text{blend}}(f) = \begin{cases}
\frac{f - f_{\text{start}}}{f_{\text{overlap}}} & f \in [f_{\text{start}}, f_{\text{start}} + f_{\text{overlap}}] \\
1 & \text{otherwise}
\end{cases}
$$

æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ :
$$
\mathbf{x}_f = w_{\text{blend}}(f) \cdot \mathbf{x}_f^{\text{new}} + (1 - w_{\text{blend}}(f)) \cdot \mathbf{x}_f^{\text{old}}
$$

**å®Ÿè£…ä¾‹ï¼ˆRustï¼‰**:

```rust
use ndarray::{Array3, Axis};
use ndarray_rand::RandomExt;
use ndarray_rand::rand_distr::StandardNormal;

fn generate_chunk(
    _model: &SimpleVideoDiffusion,
    _current_frame: &Array3<f32>,
    chunk_size: usize,
) -> Vec<Array3<f32>> {
    // ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆï¼ˆæ¦‚å¿µçš„ï¼‰
    vec![Array3::<f32>::zeros((60, 60, 1)); chunk_size]
}

fn autoregressive_video_generation(
    model: &SimpleVideoDiffusion,
    total_frames: usize,
    chunk_size: usize,
    overlap: usize,
    h: usize,
    w: usize,
    c: usize,
) -> Vec<Array3<f32>> {
    let mut all_frames: Vec<Array3<f32>> = Vec::new();
    // åˆæœŸãƒã‚¤ã‚º
    let mut current_frame = Array3::<f32>::random((h, w, c), StandardNormal);

    let step = chunk_size - overlap;
    let mut start_idx = 0;

    while start_idx < total_frames {
        // ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
        let chunk = generate_chunk(model, &current_frame, chunk_size);

        if start_idx == 0 {
            // æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã¯å…¨ã¦è¿½åŠ 
            all_frames.extend(chunk.iter().cloned());
        } else {
            // Overlapé ˜åŸŸã§ãƒ–ãƒ¬ãƒ³ãƒ‰
            let n = all_frames.len();
            for i in 0..overlap {
                let w_blend = i as f32 / overlap as f32;
                let blended = w_blend * &chunk[i] + (1.0 - w_blend) * &all_frames[n - overlap + i];
                all_frames[n - overlap + i] = blended;
            }

            // æ®‹ã‚Šã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿½åŠ 
            all_frames.extend(chunk[overlap..].iter().cloned());
        }

        // æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ¬¡ã®é–‹å§‹ç‚¹ã«
        current_frame = chunk.last().unwrap().clone();
        start_idx += step;
    }

    all_frames
}
```

**Driftå•é¡Œã®ç†è«–çš„åˆ†æ**:

ç´¯ç©èª¤å·®ã®ä¸Šç•Œ:
$$
\mathbb{E}[\|\mathbf{x}_T - \mathbf{x}_T^{\text{gt}}\|^2] \leq T \cdot \epsilon^2
$$

ã“ã“ã§$\epsilon$ã¯1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šã®èª¤å·®ã€‚Overlapã§èª¤å·®ã‚’$\alpha < 1$å€ã«æŠ‘åˆ¶ã™ã‚‹ã¨:
$$
\mathbb{E}[\|\mathbf{x}_T - \mathbf{x}_T^{\text{gt}}\|^2] \leq T \cdot (\alpha\epsilon)^2
$$

#### æˆ¦ç•¥â‘¡ Key Frame + Interpolation

**åŸç†**: é‡è¦ãªã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿ã‚’ç”Ÿæˆ â†’ é–“ã‚’Interpolationã€‚

```
ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ : 1, 10, 20, 30, ...
Interpolation: ãƒ•ãƒ¬ãƒ¼ãƒ 1-10ã‚’Flowãƒ™ãƒ¼ã‚¹è£œé–“
```

**Interpolationãƒ¢ãƒ‡ãƒ«**: FILMï¼ˆFrame Interpolation for Large Motionï¼‰ç­‰ã€‚

**åˆ©ç‚¹**: è¨ˆç®—é‡å‰Šæ¸›ï¼ˆç”Ÿæˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒ1/10ï¼‰ã€‚

**æ¬ ç‚¹**: è¤‡é›‘ãªå‹•ãã§InterpolationãŒç ´ç¶»ã€‚

**FILMï¼ˆFrame Interpolation for Large Motionï¼‰ã®æ•°å¼**:

ä¸­é–“ãƒ•ãƒ¬ãƒ¼ãƒ $t \in (0, 1)$ã®ç”Ÿæˆ:
$$
\mathbf{x}_t = (1-t) \cdot \text{Warp}(\mathbf{x}_0, \mathbf{f}_{0 \to t}) + t \cdot \text{Warp}(\mathbf{x}_1, \mathbf{f}_{1 \to t})
$$

ã“ã“ã§$\mathbf{f}_{0 \to t}, \mathbf{f}_{1 \to t}$ã¯åŒæ–¹å‘Optical Flowï¼ˆFILMãŒäºˆæ¸¬ï¼‰ã€‚

**Multi-scale Pyramidæ§‹é€ **:

```
å…¥åŠ›ãƒ•ãƒ¬ãƒ¼ãƒ  (x0, x1)
    â†“ Downsample 4x
ä½è§£åƒåº¦Flowæ¨å®š â†’ f_low
    â†“ Upsample + Refine
ä¸­è§£åƒåº¦Flowæ¨å®š â†’ f_mid
    â†“ Upsample + Refine
é«˜è§£åƒåº¦Flowæ¨å®š â†’ f_high
    â†“ Warp + Blend
ä¸­é–“ãƒ•ãƒ¬ãƒ¼ãƒ  x_t
```

**Rustå®Ÿè£…ä¾‹**:

```rust
use ndarray::Array3;

// Optical Flowæ¨å®šï¼ˆç°¡ç•¥åŒ–ï¼‰
fn estimate_flow(frame0: &Array3<f32>, _frame1: &Array3<f32>) -> Array3<f32> {
    Array3::<f32>::zeros(frame0.raw_dim())
}

// ãƒ•ãƒ¬ãƒ¼ãƒ Warpï¼ˆç°¡ç•¥åŒ–: å®Ÿéš›ã®FILMã¯CNNã§åŒæ–¹å‘Flowã‚’æ¨å®šï¼‰
fn warp_frame(frame: &Array3<f32>, _flow: &Array3<f32>) -> Array3<f32> {
    frame.clone()
}

fn film_interpolation(frame0: &Array3<f32>, frame1: &Array3<f32>, t: f32) -> Array3<f32> {
    // ç°¡æ˜“ç‰ˆ: ç·šå½¢è£œé–“ï¼ˆå®Ÿéš›ã®FILMã¯CNNã§Flowæ¨å®šï¼‰

    // Optical Flowæ¨å®šï¼ˆç°¡ç•¥åŒ–ï¼‰
    let flow_0_to_t = estimate_flow(frame0, frame1).mapv(|v| v * t);
    let flow_1_to_t = estimate_flow(frame1, frame0).mapv(|v| v * (1.0 - t));

    // Warp
    let warped_0 = warp_frame(frame0, &flow_0_to_t);
    let warped_1 = warp_frame(frame1, &flow_1_to_t);

    // Blend
    (1.0 - t) * &warped_0 + t * &warped_1
}

fn generate_single_frame(_model: &Simple3DUNet) -> Array3<f32> {
    // ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆï¼ˆæ¦‚å¿µçš„ï¼‰
    Array3::<f32>::zeros((3, 512, 768))
}

fn generate_with_interpolation(
    model: &Simple3DUNet,
    num_key_frames: usize,
    key_frame_interval: usize,
) -> Vec<Array3<f32>> {
    // ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ
    let key_frames: Vec<Array3<f32>> = (0..num_key_frames)
        .map(|_| generate_single_frame(model))
        .collect();

    // è£œé–“
    let mut all_frames = vec![key_frames[0].clone()];
    for i in 0..num_key_frames - 1 {
        all_frames.extend((1..key_frame_interval).map(|j| {
            film_interpolation(
                &key_frames[i],
                &key_frames[i + 1],
                j as f32 / key_frame_interval as f32,
            )
        }));
        all_frames.push(key_frames[i + 1].clone());
    }

    all_frames
}
```

**è¨ˆç®—é‡æ¯”è¼ƒ**:

| æ‰‹æ³• | ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ | Interpolation | ç·è¨ˆç®—é‡ |
|:-----|:----------------|:--------------|:---------|
| Full Generation | $T$ frames | - | $O(T \cdot C_{\text{gen}})$ |
| Key Frame + Interp | $T/k$ frames | $(T-T/k)$ frames | $O(T/k \cdot C_{\text{gen}} + T \cdot C_{\text{interp}})$ |

$C_{\text{interp}} \ll C_{\text{gen}}$ï¼ˆè£œé–“ã¯Diffusionã‚ˆã‚Šé¥ã‹ã«é«˜é€Ÿï¼‰ãªã®ã§ã€$k=10$ã§ç´„9å€é«˜é€ŸåŒ–ã€‚

#### æˆ¦ç•¥â‘¢ Hierarchical Generation

**åŸç†**: ä½è§£åƒåº¦ãƒ»ä½ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã§å…¨ä½“ã‚’ç”Ÿæˆ â†’ Super-resolution + Frame Interpolationã€‚

```
Stage 1: 256Ã—256ã€4 fpsã€10ç§’ â†’ 40ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ
Stage 2: å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’512Ã—512ã«Super-resolution
Stage 3: 4 fps â†’ 24 fps ã«Frame Interpolation (6å€)
```

**åˆ©ç‚¹**: Stage 1ã§å¤§åŸŸçš„ãªä¸€è²«æ€§ã‚’ç¢ºä¿ â†’ Stage 2-3ã§ç´°éƒ¨ã‚’è¿½åŠ ã€‚

**å®Ÿè£…ä¾‹**: Open-Sora 2.0ã€CogVideoXã€‚

**Multi-stage Pipelineã®æ•°å¼**:

**Stage 1ï¼ˆBase Generationï¼‰**:
$$
\mathbf{X}_{\text{base}} \sim p_\theta(\mathbf{X} \mid c), \quad \mathbf{X} \in \mathbb{R}^{T_{\text{low}} \times H_{\text{low}} \times W_{\text{low}} \times 3}
$$

**Stage 2ï¼ˆSuper-resolutionï¼‰**:
$$
\mathbf{X}_{\text{SR}} = f_{\text{SR}}(\mathbf{X}_{\text{base}}), \quad \mathbf{X}_{\text{SR}} \in \mathbb{R}^{T_{\text{low}} \times H_{\text{high}} \times W_{\text{high}} \times 3}
$$

**Stage 3ï¼ˆFrame Interpolationï¼‰**:
$$
\mathbf{X}_{\text{final}} = f_{\text{interp}}(\mathbf{X}_{\text{SR}}), \quad \mathbf{X}_{\text{final}} \in \mathbb{R}^{T_{\text{high}} \times H_{\text{high}} \times W_{\text{high}} \times 3}
$$

**CogVideoXå®Ÿè£…ï¼ˆæ¦‚è¦ï¼‰**:

```rust
use ndarray::Array3;

fn generate_base(
    _base_model: &Simple3DUNet,
    _prompt: &str,
    size: (usize, usize),
    _fps: usize,
    _duration: usize,
) -> Vec<Array3<f32>> {
    // Stage 1: ä½è§£åƒåº¦ãƒ»ä½ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã§å…¨ä½“ç”Ÿæˆï¼ˆæ¦‚å¿µçš„ï¼‰
    vec![Array3::<f32>::zeros((3, size.0, size.1)); 40]
}

fn super_resolve(
    _sr_model: &Simple3DUNet,
    _frame: &Array3<f32>,
    target_size: (usize, usize),
) -> Array3<f32> {
    // Stage 2: Super-resolutionï¼ˆæ¦‚å¿µçš„ï¼‰
    Array3::<f32>::zeros((3, target_size.0, target_size.1))
}

fn interpolate_frame(
    _interp_model: &Simple3DUNet,
    frame_a: &Array3<f32>,
    frame_b: &Array3<f32>,
    t: f32,
) -> Array3<f32> {
    (1.0 - t) * frame_a + t * frame_b
}

fn hierarchical_generation(
    base_model: &Simple3DUNet,
    sr_model: &Simple3DUNet,
    interp_model: &Simple3DUNet,
    prompt: &str,
) -> Vec<Array3<f32>> {
    // Stage 1: Base generation (256Ã—256, 4fps, 49 frames = 12ç§’)
    let base_video = generate_base(base_model, prompt, (256, 256), 4, 12);

    // Stage 2: Super-resolution (256Ã—256 â†’ 720Ã—480)
    let sr_video: Vec<Array3<f32>> = base_video.iter()
        .map(|frame| super_resolve(sr_model, frame, (720, 480)))
        .collect();

    // Stage 3: Frame interpolation (4fps â†’ 24fps, 6å€)
    let mut final_video: Vec<Array3<f32>> = Vec::new();
    for i in 0..sr_video.len() - 1 {
        final_video.push(sr_video[i].clone());
        // 5ã¤ã®ä¸­é–“ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è£œé–“
        for j in 1..=5 {
            final_video.push(interpolate_frame(
                interp_model,
                &sr_video[i],
                &sr_video[i + 1],
                j as f32 / 6.0,
            ));
        }
    }
    final_video.push(sr_video.last().unwrap().clone());

    final_video
}
```

**ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®åˆ†æ**:

| Stage | è§£åƒåº¦ | ãƒ•ãƒ¬ãƒ¼ãƒ æ•° | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | å‡¦ç†æ™‚é–“ |
|:------|:-------|:----------|:------------|:---------|
| Stage 1 | 256Â² | 40 | 25 MB | 30ç§’ |
| Stage 2 | 720Â² | 40 | 200 MB | 10ç§’ |
| Stage 3 | 720Â² | 240 | 1.2 GB | 20ç§’ |

**Pipelineã®ä¸¦åˆ—åŒ–**:

```rust
use rayon::prelude::*;
use ndarray::Array3;

fn super_resolve_video(
    sr_model: &Simple3DUNet,
    base_video: &[Array3<f32>],
) -> Vec<Array3<f32>> {
    base_video.iter()
        .map(|frame| super_resolve(sr_model, frame, (720, 480)))
        .collect()
}

fn interpolate_video(
    interp_model: &Simple3DUNet,
    sr_video: &[Array3<f32>],
) -> Vec<Array3<f32>> {
    let mut final_video: Vec<Array3<f32>> = Vec::new();
    for i in 0..sr_video.len() - 1 {
        final_video.push(sr_video[i].clone());
        for j in 1..=5 {
            final_video.push(interpolate_frame(
                interp_model, &sr_video[i], &sr_video[i + 1], j as f32 / 6.0,
            ));
        }
    }
    final_video.push(sr_video.last().unwrap().clone());
    final_video
}

fn parallel_hierarchical_generation(
    base_model: &(impl Sync + Fn(&str) -> Vec<Array3<f32>>),
    sr_model: &Simple3DUNet,
    interp_model: &Simple3DUNet,
    prompts: &[String],
) -> Vec<Vec<Array3<f32>>> {
    // Stage 1ã‚’å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä¸¦åˆ—å®Ÿè¡Œ
    let base_videos: Vec<Vec<Array3<f32>>> = prompts.par_iter()
        .map(|prompt| base_model(prompt))
        .collect::<Vec<_>>();

    // Stage 2-3ã‚‚ä¸¦åˆ—åŒ–
    base_videos.par_iter()
        .map(|base_video| {
            let sr_video = super_resolve_video(sr_model, base_video);
            interpolate_video(interp_model, &sr_video)
        })
        .collect::<Vec<_>>()
}
```

### 6.6 Video Tokenizationã®æœ€å‰ç·š

#### 6.6.1 é›¢æ•£è¡¨ç¾ vs é€£ç¶šè¡¨ç¾

**2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | ä»£è¡¨ãƒ¢ãƒ‡ãƒ« | è¡¨ç¾ | ç”Ÿæˆæ–¹å¼ | åˆ©ç‚¹ | æ¬ ç‚¹ |
|:----------|:----------|:-----|:---------|:-----|:-----|
| é›¢æ•£è¡¨ç¾ | VQ-VAE / MAGVIT-v2 | Codebook index | Autoregressive | å³å¯†ãªå°¤åº¦ | Codebook collapse |
| é€£ç¶šè¡¨ç¾ | 3D VAE (CogVideoX) | é€£ç¶šLatent | Diffusion/Flow | è¡¨ç¾åŠ›é«˜ã„ | å°¤åº¦è¨ˆç®—ä¸å¯ |

#### 6.6.2 MAGVIT-v2 â€” çµ±ä¸€èªå½™ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

```
å…¥åŠ›å‹•ç”» (T, H, W, 3)
    â†“ 3D CNN Encoder
Latent (T/4, H/8, W/8, D)
    â†“ Vector Quantization
Discrete Tokens (T/4 Ã— H/8 Ã— W/8) âˆˆ {0, ..., 262143}
    â†“ Lookup Table
Quantized Latent
    â†“ 3D CNN Decoder
å†æ§‹æˆå‹•ç”» (T, H, W, 3)
```

**Lookup-Free Quantizationï¼ˆLFQï¼‰**:

å¾“æ¥ã®VQ-VAEå•é¡Œç‚¹: Codebook collapseã§ä¸€éƒ¨ã®ã‚³ãƒ¼ãƒ‰ã—ã‹ä½¿ã‚ã‚Œãªã„ã€‚

**LFQè§£æ±ºç­–**: Codebookã‚’æŒãŸãšã€Latentã‚’ç›´æ¥é‡å­åŒ–ã€‚

$$
\mathbf{z}_{\text{quant}} = \text{round}(\mathbf{z}) - \text{sg}(\mathbf{z} - \text{round}(\mathbf{z}))
$$

ã“ã“ã§$\text{sg}$ã¯stop-gradientï¼ˆStraight-Through Estimatorï¼‰ã€‚

**Commitment Lossä¸è¦**: é‡å­åŒ–ãŒè‡ªå‹•çš„ã«æ•´æ•°ã«åæŸã€‚

**Rustå®Ÿè£…ä¾‹**:

```rust
use ndarray::ArrayD;

// Lookup-Free Quantization (LFQ)
fn lookup_free_quantization(z: &ArrayD<f32>) -> (ArrayD<f32>, ArrayD<i32>) {
    // Latentã‚’[-1, 1]ã«Clip
    let z_clipped = z.mapv(|v| v.clamp(-1.0f32, 1.0f32));

    // 8ãƒ“ãƒƒãƒˆé‡å­åŒ– (256ãƒ¬ãƒ™ãƒ«)
    let z_scaled = z_clipped.mapv(|v| (v + 1.0f32) * 127.5f32);
    let z_quantized_int = z_scaled.mapv(|v| v.round() as i32);

    // Float32ã«æˆ»ã™
    let z_quantized = z_quantized_int.mapv(|v| v as f32 / 127.5f32 - 1.0f32);

    // Straight-Through Estimator
    // Forward: quantized, Backward: identityï¼ˆå‹¾é…ã¯zã«ç›´æ¥æµã‚Œã‚‹ï¼‰
    let z_st = &z_quantized + &(z - &z_quantized);

    (z_st, z_quantized_int)
}

// Codebook sizeã®è¨ˆç®—
fn calculate_codebook_size(latent_dims: u32, num_levels_per_dim: u64) -> u64 {
    // latent_dims: Latentæ¬¡å…ƒæ•°
    // num_levels_per_dim: å„æ¬¡å…ƒã®é‡å­åŒ–ãƒ¬ãƒ™ãƒ«æ•°ï¼ˆä¾‹: 256ï¼‰
    num_levels_per_dim.pow(latent_dims)
}

fn main() {
    // ä¾‹: D=8æ¬¡å…ƒã€å„æ¬¡å…ƒ256ãƒ¬ãƒ™ãƒ«
    let codebook_size = calculate_codebook_size(8, 256);
    println!("Codebook size: {codebook_size}"); // 256^8 = ç´„18å…†ï¼ˆå·¨å¤§ï¼‰
}
```

**å®Ÿç”¨çš„ã«ã¯**: ä½æ¬¡å…ƒï¼ˆD=4-8ï¼‰+å¤šæ®µéšé‡å­åŒ–ã§ç®¡ç†ã€‚

#### 6.6.3 Cosmos Tokenizer â€” ç”»åƒãƒ»å‹•ç”»çµ±åˆ

**NVIDIA Cosmos Tokenizer**ï¼ˆ2024ï¼‰:

- **ç”»åƒãƒ¢ãƒ¼ãƒ‰**: T=1ãƒ•ãƒ¬ãƒ¼ãƒ 
- **å‹•ç”»ãƒ¢ãƒ¼ãƒ‰**: T>1ãƒ•ãƒ¬ãƒ¼ãƒ 
- **åŒã˜ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€**: çµ±ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**Causal 3D Convã®é‡è¦æ€§**: å‹•ç”»ã®è‡ªå·±å›å¸°ç”Ÿæˆã‚’å¯èƒ½ã«ã€‚

```
é€šå¸¸3D Conv: Paddingä¸¡å´ â†’ æœªæ¥ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‚ç…§ï¼ˆæ¨è«–æ™‚ä¸å¯ï¼‰
Causal 3D Conv: Paddingéå»ã®ã¿ â†’ æ¨è«–æ™‚ã«é€æ¬¡ç”Ÿæˆå¯èƒ½
```

**Rustå®Ÿè£…ä¾‹ï¼ˆCausal Paddingï¼‰**:

```rust
// ä¾å­˜ã‚¯ãƒ¬ãƒ¼ãƒˆ: ndarray = "0.15", ndarray-rand = "0.14"
use ndarray::{Array5, s};

// Causal 3D Convolutionï¼ˆæ™‚é–“æ–¹å‘ã¯éå»ã®ã¿ã‚’å‚ç…§ï¼‰
pub struct CausalConv3D {
    weight: Array5<f32>,          // (out_ch, in_ch, kernel_t, kernel_h, kernel_w)
    bias: ndarray::Array1<f32>,
    stride: (usize, usize, usize),
}

impl CausalConv3D {
    pub fn new(
        in_ch: usize,
        out_ch: usize,
        kernel: (usize, usize, usize),
        stride: (usize, usize, usize),
    ) -> Self {
        use ndarray_rand::RandomExt;
        use ndarray_rand::rand_distr::StandardNormal;
        Self {
            weight: Array5::random((out_ch, in_ch, kernel.0, kernel.1, kernel.2), StandardNormal),
            bias: ndarray::Array1::zeros(out_ch),
            stride,
        }
    }

    pub fn forward(&self, x: &Array5<f32>) -> Array5<f32> {
        // x: (B, C_in, T, H, W)
        let shape = x.shape();
        let (b, c_in, t, h, w) = (shape[0], shape[1], shape[2], shape[3], shape[4]);
        let (_, _, kt, kh, kw) = self.weight.dim();

        // Causal Padding: æ™‚é–“æ–¹å‘ã¯éå»ã®ã¿
        let pad_t = kt - 1;
        let pad_h = kh / 2;
        let pad_w = kw / 2;

        // Paddingé©ç”¨
        let mut x_padded = Array5::<f32>::zeros((
            b, c_in,
            t + pad_t,
            h + 2 * pad_h,
            w + 2 * pad_w,
        ));
        x_padded
            .slice_mut(s![.., .., pad_t.., pad_h..h + pad_h, pad_w..w + pad_w])
            .assign(x);

        // ç•³ã¿è¾¼ã¿ï¼ˆæ¦‚å¿µã®ã¿: å®Ÿéš›ã¯ tch-rs / burn ã® Conv3d ã‚’ä½¿ç”¨ï¼‰
        // output = conv3d(x_padded, &self.weight, self.stride)
        Array5::<f32>::zeros((b, self.weight.shape()[0], t, h, w))
    }
}
```

**çµ±ä¸€Tokenizerã®åˆ©ç‚¹**:

1. **Transfer Learning**: ç”»åƒã§è¨“ç·´ã—ãŸTokenizerã‚’å‹•ç”»ã«è»¢ç”¨
2. **Image-to-Video**: ç”»åƒTokenã‚’VAEã«é€šã—ã¦å‹•ç”»ç”Ÿæˆé–‹å§‹
3. **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡**: ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆè±Šå¯Œï¼‰ã§äº‹å‰è¨“ç·´ â†’ å‹•ç”»ï¼ˆå°‘ãªã„ï¼‰ã§Fine-tune

### 6.7 å‹•ç”»ç”Ÿæˆã®ç ”ç©¶ç³»è­œå›³

```mermaid
graph TD
    A[2015: DCGAN] -->|æ•µå¯¾çš„å­¦ç¿’| B[2017: Video GAN]
    B --> C[2020: DVD-GAN]

    D[2020: DDPM] -->|Diffusion| E[2022: Video Diffusion Models]
    E --> F[2023: Video LDM]
    F --> G[2024: Sora]
    G --> H[2025: Sora 2]

    E --> I[2023: Make-A-Video]
    I --> J[2024: CogVideoX]
    J --> K[2024: HunyuanVideo]
    K --> L[2025: Open-Sora 2.0]

    F --> M[2024: LTX-Video]
    M -->|Flow Matching| N[2025: æ¬¡ä¸–ä»£]

    O[2023: NeRF] -->|3Dè¡¨ç¾| P[2024: 4Dç”Ÿæˆ]
    P --> Q[2025: Dynamic Scene]

    style H fill:#f9f,stroke:#333,stroke-width:4px
    style L fill:#9f9,stroke:#333,stroke-width:4px
    style M fill:#99f,stroke:#333,stroke-width:4px
```

**ä¸»è¦ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**:

| å¹´ | ãƒ¢ãƒ‡ãƒ« | é©æ–° |
|:---|:-------|:-----|
| 2015 | DCGAN | æ•µå¯¾çš„å­¦ç¿’ã®æˆåŠŸ |
| 2020 | DDPM | Diffusionãƒ¢ãƒ‡ãƒ«ã®å¾©æ´» |
| 2022 | Video Diffusion Models | Diffusionã‚’å‹•ç”»ã«æ‹¡å¼µ |
| 2023 | Make-A-Video | ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãå‹•ç”»ç”Ÿæˆ |
| 2024 | **Sora** | ä¸–ç•Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¨ã—ã¦ã®å‹•ç”»ç”Ÿæˆ |
| 2024 | CogVideoX / HunyuanVideo | ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹é«˜å“è³ªåŒ– |
| 2025 | **Open-Sora 2.0** | $200kæ°‘ä¸»åŒ– |
| 2025 | **Sora 2** | 15-25ç§’ç”Ÿæˆã€ç‰©ç†æ³•å‰‡æ”¹å–„ |

---



## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.8 ä»Šå›å­¦ã‚“ã ã“ã¨ï¼ˆ4ã¤ã®Key Takeawaysï¼‰

1. **æ™‚ç©ºé–“Diffusionã®æœ¬è³ª**: é™æ­¢ç”»Diffusionã«æ™‚é–“è»¸ã‚’è¿½åŠ  â†’ Temporal Attentionã§æ™‚é–“çš„ä¸€è²«æ€§ã‚’å­¦ç¿’ã€‚Optical Flow LossãŒç‰©ç†çš„å‹•ãã‚’ä¿è¨¼ã€‚

2. **3D U-Net â†’ DiTã®é©å‘½**: ç•³ã¿è¾¼ã¿ã®å±€æ‰€æ€§ â†’ Transformerã®å¤§åŸŸAttentionã€‚Sora 2ã®Spacetime DiTãŒæ™‚ç©ºé–“ã‚’çµ±ä¸€TokenåŒ– â†’ Scaling LawsãŒé©ç”¨å¯èƒ½ã«ã€‚

3. **3D VAEã®åœ§ç¸®è¡“**: CogVideoXã¯192å€åœ§ç¸®ï¼ˆæ™‚ç©ºé–“åˆè¨ˆï¼‰ã€‚Latentç©ºé–“ã§Diffusionã™ã‚‹ã“ã¨ã§è¨ˆç®—é‡ã‚’æ¿€æ¸›ã€‚

4. **2024-2025æœ€å‰ç·š**: HunyuanVideoï¼ˆ13Bå•†ç”¨ç´šï¼‰ã€Open-Sora 2.0ï¼ˆ$200kæ°‘ä¸»åŒ–ï¼‰ã€Wan 2.1ï¼ˆå¤šæ©Ÿèƒ½çµ±åˆï¼‰ã€LTX-Videoï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ï¼‰ã€‚SmolVLM2ã¨ã®çµ„ã¿åˆã‚ã›ã§Video-to-Text-to-Videoãƒ«ãƒ¼ãƒ—ãŒå®Ÿç¾ã€‚

### 6.9 FAQ â€” ã‚ˆãã‚ã‚‹5ã¤ã®è³ªå•

#### Q1: å‹•ç”»ç”Ÿæˆã®è¨ˆç®—ã‚³ã‚¹ãƒˆã¯ã©ã‚Œãã‚‰ã„ï¼Ÿ

**A**: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã«ä¾å­˜ã€‚

| ãƒ¢ãƒ‡ãƒ« | GPU | è§£åƒåº¦ | ãƒ•ãƒ¬ãƒ¼ãƒ æ•° | ç”Ÿæˆæ™‚é–“ |
|:-------|:----|:-------|:----------|:---------|
| LTX-Video | H100 | 768Ã—512 | 121 (5ç§’) | 2ç§’ |
| CogVideoX-5B | A100 | 720Ã—480 | 49 (2ç§’) | 10ç§’ |
| Open-Sora 2.0 | A100 | 720p | 240 (10ç§’) | 35ç§’ |
| Wan 2.1-14B | RTX 4090 | 480p | 120 (5ç§’) | 3.5åˆ† |

**ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ**: RTX 4090ï¼ˆ24GB VRAMï¼‰ã§å®Ÿç”¨çš„ãªæ¨è«–ãŒå¯èƒ½ã€‚

#### Q2: Soraã¯ãªãœç‰©ç†æ³•å‰‡ã‚’å­¦ç¿’ã§ãã‚‹ã®ã‹ï¼Ÿ

**A**: 3ã¤ã®è¦å› :
1. **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿**: æ¨å®š1Bå‹•ç”»ï¼ˆYouTubeã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ã§è¨“ç·´
2. **é•·æ™‚é–“å‹•ç”»**: 15-25ç§’ã®é•·ã„å‹•ç”»ã§å› æœé–¢ä¿‚ã‚’å­¦ç¿’
3. **Spacetime DiT**: æ™‚ç©ºé–“ã‚’çµ±ä¸€çš„ã«æ‰±ã† â†’ ç‰©ç†çš„åˆ¶ç´„ãŒå‰µç™º

ãŸã ã—ã€å®Œç’§ã§ã¯ãªã„ï¼ˆä¾‹: ãƒã‚¹ã‚±ãƒƒãƒˆãƒœãƒ¼ãƒ«ã®åå°„è§’åº¦ãŒä¸æ­£ç¢ºï¼‰ã€‚

#### Q3: å‹•ç”»ç”Ÿæˆã§ã¾ã è§£æ±ºã•ã‚Œã¦ã„ãªã„å•é¡Œã¯ï¼Ÿ

**A**: ä¸»ãªèª²é¡Œ:
1. **é•·æ™‚é–“ä¸€è²«æ€§**: 1åˆ†ä»¥ä¸Šã®å‹•ç”»ã§ç ´ç¶»
2. **ç‰©ç†æ³•å‰‡**: é‡åŠ›ãƒ»è¡çªã®å³å¯†ãªéµå®ˆ
3. **è¨ˆç®—ã‚³ã‚¹ãƒˆ**: é«˜è§£åƒåº¦é•·æ™‚é–“å‹•ç”»ã¯ä¾ç„¶ã¨ã—ã¦é«˜ã‚³ã‚¹ãƒˆ
4. **è©•ä¾¡æŒ‡æ¨™**: äººé–“ã®ä¸»è¦³ã¨ä¸€è‡´ã™ã‚‹è‡ªå‹•æŒ‡æ¨™ãŒãªã„
5. **åˆ¶å¾¡æ€§**: ç´°ã‹ã„å‹•ãã®æŒ‡å®šãŒå›°é›£

#### Q4: å‹•ç”»ç”Ÿæˆã¨å‹•ç”»ç·¨é›†ã®é•ã„ã¯ï¼Ÿ

**A**: 2ã¤ã®é•ã„:

| ã‚¿ã‚¹ã‚¯ | å…¥åŠ› | å‡ºåŠ› | æŠ€è¡“ |
|:-------|:-----|:-----|:-----|
| å‹•ç”»ç”Ÿæˆï¼ˆT2Vï¼‰ | ãƒ†ã‚­ã‚¹ãƒˆ | æ–°è¦å‹•ç”» | Diffusion/Flow Matching |
| å‹•ç”»ç·¨é›†ï¼ˆV2Vï¼‰ | æ—¢å­˜å‹•ç”»+æŒ‡ç¤º | ç·¨é›†æ¸ˆã¿å‹•ç”» | Inpainting/Style Transfer |

**çµ±åˆãƒ¢ãƒ‡ãƒ«**: Wan 2.1ã‚„Runway Gen-3ã¯ä¸¡æ–¹å¯¾å¿œã€‚

#### Q5: SmolVLM2ã¨LTX-Videoã‚’çµ„ã¿åˆã‚ã›ã‚‹æ„ç¾©ã¯ï¼Ÿ

**A**: **Video-to-Text-to-Video**ãƒ«ãƒ¼ãƒ—ã®å®Ÿç¾:
1. SmolVLM2ã§æ—¢å­˜å‹•ç”»ã‚’ç†è§£ï¼ˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼‰
2. ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç·¨é›†ï¼ˆä¾‹: "cat"ã‚’"dog"ã«å¤‰æ›´ï¼‰
3. LTX-Videoã§æ–°ã—ã„å‹•ç”»ç”Ÿæˆ

**å¿œç”¨ä¾‹**:
- å‹•ç”»è¦ç´„ â†’ å†ç”Ÿæˆï¼ˆé•·å‹•ç”»ã‚’çŸ­ãã¾ã¨ã‚ã‚‹ï¼‰
- ã‚¹ã‚¿ã‚¤ãƒ«å¤‰æ›ï¼ˆå®Ÿå†™â†’ã‚¢ãƒ‹ãƒ¡ï¼‰
- ã‚·ãƒ¼ãƒ³å·®ã—æ›¿ãˆï¼ˆèƒŒæ™¯ã ã‘å¤‰æ›´ï¼‰

### 6.10 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« â€” 1é€±é–“ãƒ—ãƒ©ãƒ³

| æ—¥ | å†…å®¹ | æ™‚é–“ | åˆ°é”ç›®æ¨™ |
|:---|:-----|:-----|:---------|
| Day 1 | Zone 0-2 | 2h | å‹•ç”»ç”Ÿæˆã®ç›´æ„Ÿç†è§£ |
| Day 2 | Zone 3å‰åŠï¼ˆ3.1-3.3ï¼‰ | 3h | Spacetime DiTå°å‡º |
| Day 3 | Zone 3å¾ŒåŠï¼ˆ3.4-3.6ï¼‰ | 3h | 3D VAEãƒ»Optical Flow |
| Day 4 | Zone 4ï¼ˆRustå®Ÿè£…ï¼‰ | 3h | Video Diffusionè¨“ç·´ |
| Day 5 | Zone 5ï¼ˆãƒ‡ãƒ¢å®Ÿè¡Œï¼‰ | 2h | SmolVLM2+LTX-Video |
| Day 6 | Zone 6ï¼ˆæœ€å‰ç·šèª¿æŸ»ï¼‰ | 2h | HunyuanVideoç­‰ã®è«–æ–‡ |
| Day 7 | å¾©ç¿’+å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ | 3h | Moving MNIST |

**åˆè¨ˆ**: 18æ™‚é–“ â†’ é€±æœ«2æ—¥ + å¹³æ—¥å¤œ3æ—¥ã§å®Œèµ°å¯èƒ½ã€‚

### 6.11 æ¬¡å›äºˆå‘Š â€” ç¬¬46å›: 3Dç”Ÿæˆ & Neural Rendering

**ã‚¿ã‚¤ãƒˆãƒ«**: ã€Œ2Då‹•ç”»ã‹ã‚‰3Dç©ºé–“ã®ç†è§£ã¨ç”Ÿæˆã¸ã€

**å­¦ã¶ã“ã¨**:
1. **3Dè¡¨ç¾ã®åˆ†é¡**: Mesh / Voxel / Implicit / Radiance Field / 3DGS
2. **NeRFï¼ˆNeural Radiance Fieldsï¼‰**: Volume Renderingæ–¹ç¨‹å¼ãƒ»ä½ç½®ç¬¦å·åŒ–
3. **3D Gaussian Splattingï¼ˆ3DGSï¼‰**: æ˜ç¤ºçš„è¡¨ç¾ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
4. **DreamFusion**: SDS Lossï¼ˆScore Distillation Samplingï¼‰ã§Text-to-3D
5. **3D VAE**: å‹•ç”»VAEã‚’3Dç©ºé–“ã«æ‹¡å¼µ

**æ¥ç¶š**: å‹•ç”»ï¼ˆ2D+æ™‚é–“ï¼‰ â†’ 3Dç©ºé–“ï¼ˆ3D+æ™‚é–“=4Dï¼‰ã¸æ‹¡å¼µã€‚

> **Note:** **é€²æ—**: å…¨ä½“ã®90%å®Œäº†ã€‚æ®‹ã‚Š5å›ã§å’æ¥­åˆ¶ä½œã¸ã€‚ç¬¬50å›ã§ã¯3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ç”ŸæˆAIã‚·ã‚¹ãƒ†ãƒ ã®å®ŒæˆãŒå¾…ã£ã¦ã„ã¾ã™ã€‚

---


> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. HunyuanVideoã®Causal 3D VAEãŒæ™‚é–“æ–¹å‘ã«Causalè¨­è¨ˆã«ã™ã‚‹ç†ç”±ã‚’ã€è‡ªå·±å›å¸°çš„ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆã¨ã®é–¢ä¿‚ã§èª¬æ˜ã›ã‚ˆã€‚
> 2. MoEã‚’å‹•ç”»ç”ŸæˆDiTã«é©ç”¨ã—ãŸéš›ã«æ¨è«–æ™‚Top-2 Expertã®ã¿ä½¿ã†è¨­è¨ˆã®è¨ˆç®—é‡ä¸Šã®åˆ©ç‚¹ã‚’ã€å…¨Expertä½¿ç”¨æ™‚ã¨ã®æ¯”è¼ƒã§ç¤ºã›ã€‚

## ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

### å•ã„: Soraã¯"å‹•ç”»ç”Ÿæˆ"ã§ã¯ãªã"ä¸–ç•Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿"ãªã®ã‹ï¼Ÿ

OpenAI Technical Report (2024)ã®è¨˜è¿°:
> "Sora is a **world simulator**. It learns not just to generate videos, but to model the physical world."

**è­°è«–ã®3ã¤ã®è¦–ç‚¹**:

#### è¦–ç‚¹â‘  è³›æˆæ´¾ â€” ç‰©ç†æ³•å‰‡ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹è¨¼æ‹ 

**æ ¹æ‹ **:
1. **å› æœé–¢ä¿‚**: ãƒœãƒ¼ãƒ«ã‚’æŠ•ã’ã‚‹ â†’ å¼§ã‚’æã„ã¦è½ä¸‹ï¼ˆé‡åŠ›ï¼‰
2. **è¡çªåå¿œ**: ç‰©ä½“åŒå£«ãŒã¶ã¤ã‹ã‚‹ã¨è·³ã­è¿”ã‚‹
3. **æŒç¶šæ€§**: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒç”»é¢å¤–ã«æ¶ˆãˆã¦ã‚‚ã€æˆ»ã£ã¦ãã‚‹ã¨åŒã˜çŠ¶æ…‹

**Soraã®å‡ºåŠ›ä¾‹**ï¼ˆTechnical Reportã‚ˆã‚Šï¼‰:
- ãƒã‚¹ã‚±ãƒƒãƒˆãƒœãƒ¼ãƒ«ãŒãƒªãƒ³ã‚°ã«å½“ãŸã£ã¦è·³ã­è¿”ã‚‹
- æ°´é¢ã«çŸ³ã‚’æŠ•ã’ã‚‹ã¨æ³¢ç´‹ãŒåºƒãŒã‚‹

**çµè«–**: å˜ãªã‚‹ç”»åƒç”Ÿæˆã§ã¯ãªãã€ã€Œç‰©ç†çš„ã«èµ·ã“ã‚Šã†ã‚‹ã“ã¨ã€ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã€‚

#### è¦–ç‚¹â‘¡ åå¯¾æ´¾ â€” è¦‹ãŸç›®ã®ãƒªã‚¢ãƒ«ã•ã«ã™ããªã„

**æ ¹æ‹ **:
1. **ç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³ä¸åœ¨**: Soraã¯å†…éƒ¨ã«æ˜ç¤ºçš„ãªç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’æŒãŸãªã„
2. **å¤±æ•—ä¾‹**: Technical Reportã§ã‚‚ç‰©ç†çš„çŸ›ç›¾ã‚’èªã‚ã¦ã„ã‚‹
   - ãƒã‚¹ã‚±ãƒƒãƒˆãƒœãƒ¼ãƒ«ã®åå°„è§’åº¦ãŒä¸æ­£ç¢º
   - æ¤…å­ãŒçªç„¶æ¶ˆãˆã‚‹
3. **è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¨¡å€£**: å¤§è¦æ¨¡å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã€Œã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’å­¦ç¿’ã—ãŸã ã‘

**çµè«–**: ç‰©ç†æ³•å‰‡ã‚’"ç†è§£"ã—ã¦ã„ã‚‹ã®ã§ã¯ãªãã€"æ¨¡å€£"ã—ã¦ã„ã‚‹ã ã‘ã€‚

#### è¦–ç‚¹â‘¢ ä¸­ç«‹æ´¾ â€” å‰µç™ºçš„ç‰©ç†ç†è§£

**ä¸»å¼µ**: æ˜ç¤ºçš„ãªç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³ã¯ãªã„ãŒã€**æš—é»™çš„ã«ç‰©ç†åˆ¶ç´„ã‚’å­¦ç¿’**ã—ã¦ã„ã‚‹ã€‚

**ç†è«–çš„æ ¹æ‹ **:
- Diffusionãƒ¢ãƒ‡ãƒ«ã¯ã€Œãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ã‚¹ã‚³ã‚¢ï¼ˆå‹¾é…ï¼‰ã€ã‚’å­¦ç¿’
- ç‰©ç†çš„ã«ä¸€è²«ã—ãŸå‹•ç”» = ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®é«˜å¯†åº¦é ˜åŸŸ
- Soraã¯ã‚¹ã‚³ã‚¢ã‚’å­¦ç¿’ â†’ çµæœçš„ã«ç‰©ç†æ³•å‰‡ã«å¾“ã†å‹•ç”»ã‚’ç”Ÿæˆ

**é¡æ¨**: è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-4ï¼‰ã‚‚æ–‡æ³•è¦å‰‡ã‚’æ˜ç¤ºçš„ã«æŒãŸãªã„ãŒã€å¤§è¦æ¨¡è¨“ç·´ã§æ–‡æ³•ã‚’ã€Œå‰µç™ºçš„ã«å­¦ç¿’ã€ã€‚

**çµè«–**: å®Œå…¨ãªç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§ã¯ãªã„ãŒã€å˜ãªã‚‹ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã‚‚ãªã„ã€‚**ä¸­é–“çŠ¶æ…‹**ã€‚

### ã‚ãªãŸã®è€ƒãˆã¯ï¼Ÿ

1. Soraã¯ç‰©ç†æ³•å‰‡ã‚’æœ¬å½“ã«ç†è§£ã—ã¦ã„ã‚‹ã‹ï¼Ÿ
2. ã€Œç†è§£ã€ã¨ã€Œæ¨¡å€£ã€ã®å¢ƒç•Œç·šã¯ã©ã“ã«ã‚ã‚‹ã‹ï¼Ÿ
3. å®Œå…¨ãªä¸–ç•Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã«ã¯ä½•ãŒè¶³ã‚Šãªã„ã‹ï¼Ÿ

<details><summary>æ­´å²çš„ã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆ â€” AIã®ã€Œç†è§£ã€è«–äº‰</summary>

**Searleã®ä¸­å›½èªã®éƒ¨å±‹ï¼ˆ1980ï¼‰**:
- è¨˜å·æ“ä½œã ã‘ã§ã¯ã€Œç†è§£ã€ã«ãªã‚‰ãªã„
- Soraã¯ã€Œå‹•ç”»ã®éƒ¨å±‹ã€ã«ã™ããªã„ã®ã‹ï¼Ÿ

**Moravecã®ãƒ‘ãƒ©ãƒ‰ãƒƒã‚¯ã‚¹ï¼ˆ1988ï¼‰**:
- è«–ç†æ¨è«–ã¯ç°¡å˜ã€ç‰©ç†ä¸–ç•Œã®èªè­˜ã¯é›£ã—ã„
- SoraãŒç‰©ç†ã‚’å­¦ç¿’ã—ãŸãªã‚‰ã€AIã®å¤§ããªä¸€æ­©

**ç¾ä»£ã®è¦–ç‚¹ï¼ˆLeCun, 2024ï¼‰**:
- ã€Œç†è§£ã€= ä¸–ç•Œã®å› æœãƒ¢ãƒ‡ãƒ«ã‚’æŒã¤ã“ã¨
- Soraã¯éƒ¨åˆ†çš„ãªå› æœãƒ¢ãƒ‡ãƒ«ã‚’ç²å¾—ã—ã¦ã„ã‚‹å¯èƒ½æ€§

</details>

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Vaswani, A., et al. (2017). "Attention Is All You Need". *NeurIPS 2017*.
<https://arxiv.org/abs/1706.03762>

### æ•™ç§‘æ›¸ãƒ»ã‚µãƒ¼ãƒ™ã‚¤

- Sohl-Dickstein, J., et al. (2015). "Deep Unsupervised Learning using Nonequilibrium Thermodynamics". *ICML 2015*.
- Song, Y., et al. (2020). "Score-Based Generative Modeling through Stochastic Differential Equations". *ICLR 2021*.
- Lipman, Y., et al. (2022). "Flow Matching for Generative Modeling". *arXiv:2210.02747*.
- Yun, C., et al. (2019). "Are Transformers Universal Approximators of Sequence-to-Sequence Functions?". *arXiv:1912.10077*.

---


## ğŸ”— å‰ç·¨ãƒ»å¾Œç·¨ãƒªãƒ³ã‚¯

- **å‰ç·¨ (Part 1 â€” ç†è«–ç·¨)**: [ç¬¬45å›: Videoç”Ÿæˆ (Part 1)](ml-lecture-45-part1)

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
