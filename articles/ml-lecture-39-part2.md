---
title: "ç¬¬39å›: Latent Diffusion Models: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ–¼ï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "ldm", "julia", "stablediffusion"]
published: true
slug: "ml-lecture-39-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Julia", "Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

**â†’ å‰ç·¨ï¼ˆç†è«–ç·¨ï¼‰**: [ml-lecture-39-part1](./ml-lecture-39-part1)

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” LDMè¨“ç·´â†’æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### ç’°å¢ƒæ§‹ç¯‰ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

**Juliaç’°å¢ƒ** (âš¡è¨“ç·´):
```julia
using Pkg
Pkg.add([
    "Lux",           # NN framework
    "Reactant",      # XLA compiler (GPUé«˜é€ŸåŒ–)
    "Optimisers",    # Adamç­‰
    "Zygote",        # è‡ªå‹•å¾®åˆ†
    "MLDatasets",    # MNISTç­‰
    "Images",        # ç”»åƒå‡¦ç†
    "CUDA",          # GPU
    "BenchmarkTools" # æ€§èƒ½æ¸¬å®š
])
```

**Rustç’°å¢ƒ** (ğŸ¦€æ¨è«–):
```toml
[dependencies]
candle-core = "0.7"
candle-nn = "0.7"
safetensors = "0.4"
```

### LaTeXè¨˜æ³•ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

| è¨˜å· | LaTeX | æ„å‘³ |
|:-----|:------|:-----|
| $\mathcal{E}$ | `\mathcal{E}` | Encoder |
| $\mathcal{D}$ | `\mathcal{D}` | Decoder |
| $\bar{\alpha}_t$ | `\bar{\alpha}_t` | Cumulative product |
| $\epsilon_\theta(z_t, t, c)$ | `\epsilon_\theta(z_t, t, c)` | Noise prediction |
| $\tilde{\epsilon}$ | `\tilde{\epsilon}` | Modified noise (CFG) |
| $\mathbb{E}_{q(z\|x)}[\cdot]$ | `\mathbb{E}_{q(z\|x)}[\cdot]` | Expectation |
| $\text{KL}[q \| p]$ | `\text{KL}[q \| p]` | KL divergence |

### Mathâ†’Codeç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ (LDMç·¨)

**ãƒ‘ã‚¿ãƒ¼ãƒ³1: VAE Encoder**
$$
z = \mathcal{E}(x), \quad x \in \mathbb{R}^{H \times W \times C} \to z \in \mathbb{R}^{h \times w \times c}
$$

```julia
z = encoder(x, ps_encoder, st_encoder)[1]
# x: [H, W, C, B] â†’ z: [h, w, c, B]
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³2: Forward Diffusion**
$$
z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
$$

```julia
Îµ = randn(rng, Float32, size(zâ‚€))
z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³3: CFG**
$$
\tilde{\epsilon}_\theta = \epsilon_\theta(z_t, t, \emptyset) + w \cdot (\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset))
$$

```julia
Îµ_uncond = unet((z_t, t, nothing), ps, st)[1]
Îµ_cond = unet((z_t, t, c), ps, st)[1]
Îµ_cfg = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³4: DDIM Sampling**
$$
z_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \left( \frac{z_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1-\bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_\theta + \sigma_t \epsilon
$$

```julia
pred_xâ‚€ = (z_t .- sqrt(1 - Î±_bar[t]) .* Îµ_Î¸) ./ sqrt(Î±_bar[t])
dir_z = sqrt(1 - Î±_bar[t-1] - ÏƒÂ²) .* Îµ_Î¸
noise = Ïƒ .* randn(rng, Float32, size(z_t))
z_prev = sqrt(Î±_bar[t-1]) .* pred_xâ‚€ .+ dir_z .+ noise
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³5: Cross-Attention**
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

```julia
scores = (Q * K') ./ sqrt(d_k)  # [N_q, N_k]
attn = softmax(scores, dims=2)   # [N_q, N_k]
out = attn * V                   # [N_q, d_v]
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³6: Min-SNR Weighting**
$$
w(t) = \min\left(\text{SNR}(t), \gamma\right), \quad \text{SNR}(t) = \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}
$$

```julia
snr = Î±_bar ./ (1 .- Î±_bar)
weight = min.(snr, Î³)
loss = weight[t] * mse(Îµ_pred, Îµ_true)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³7: Zero Terminal SNR Rescaling**
$$
\tilde{\alpha}_t = \frac{\alpha_t}{\alpha_T}
$$

```julia
Î±_cumprod = cumprod(alphas)
Î±_cumprod_rescaled = Î±_cumprod ./ Î±_cumprod[end]
```

### âš¡ Juliaå®Œå…¨å®Ÿè£…: Mini Latent Diffusion

**ã‚¹ãƒ†ãƒƒãƒ—1: VAEå®šç¾©**

```julia
using Lux, Random, Optimisers, Zygote

# Encoder
function create_encoder(; in_ch=3, latent_ch=4, base_ch=64)
    return Chain(
        Conv((3,3), in_ch => base_ch, pad=1, activation=relu),
        Conv((4,4), base_ch => base_ch*2, stride=2, pad=1, activation=relu),  # /2
        Conv((4,4), base_ch*2 => base_ch*4, stride=2, pad=1, activation=relu),  # /4
        Conv((4,4), base_ch*4 => base_ch*8, stride=2, pad=1, activation=relu),  # /8
        Conv((3,3), base_ch*8 => latent_ch, pad=1)  # Output z
    )
end

# Decoder (mirror)
function create_decoder(; latent_ch=4, out_ch=3, base_ch=64)
    return Chain(
        Conv((3,3), latent_ch => base_ch*8, pad=1, activation=relu),
        ConvTranspose((4,4), base_ch*8 => base_ch*4, stride=2, pad=1, activation=relu),  # *2
        ConvTranspose((4,4), base_ch*4 => base_ch*2, stride=2, pad=1, activation=relu),  # *4
        ConvTranspose((4,4), base_ch*2 => base_ch, stride=2, pad=1, activation=relu),    # *8
        Conv((3,3), base_ch => out_ch, pad=1, activation=tanh)
    )
end

# VAEè¨“ç·´
function train_vae!(encoder, decoder, dataloader; epochs=10, lr=1e-3, Î²=0.5)
    ps_enc, st_enc = Lux.setup(Random.default_rng(), encoder)
    ps_dec, st_dec = Lux.setup(Random.default_rng(), decoder)
    opt = Adam(lr)
    opt_state_enc = Optimisers.setup(opt, ps_enc)
    opt_state_dec = Optimisers.setup(opt, ps_dec)

    for epoch in 1:epochs
        total_loss = 0.0
        for (x,) in dataloader
            # Forward
            z, st_enc = encoder(x, ps_enc, st_enc)
            x_recon, st_dec = decoder(z, ps_dec, st_dec)

            # Loss: Reconstruction + KL (simplified)
            recon_loss = mean((x_recon .- x) .^ 2)
            kl_loss = 0.5f0 * mean(z .^ 2)  # Simplified KL to N(0,I)
            loss = recon_loss + Î² * kl_loss

            # Backprop
            gs_enc = gradient(p -> loss, ps_enc)[1]
            gs_dec = gradient(p -> loss, ps_dec)[1]
            opt_state_enc, ps_enc = Optimisers.update(opt_state_enc, ps_enc, gs_enc)
            opt_state_dec, ps_dec = Optimisers.update(opt_state_dec, ps_dec, gs_dec)

            total_loss += loss
        end
        println("Epoch $epoch: Loss = $(total_loss / length(dataloader))")
    end
    return ps_enc, ps_dec, st_enc, st_dec
end
```

**ã‚¹ãƒ†ãƒƒãƒ—2: U-Netå®šç¾© (Simplified)**

```julia
# ResBlock
struct ResBlock
    conv1::Conv
    conv2::Conv
end

function (rb::ResBlock)(x)
    h = rb.conv1(x)
    h = relu.(h)
    h = rb.conv2(h)
    return relu.(h .+ x)  # Residual
end

# Time Embedding
function sinusoidal_embedding(t::Int, dim::Int)
    half = dim Ã· 2
    freqs = exp.(-log(10000f0) .* (0:half-1) ./ half)
    args = t .* freqs
    return vcat(sin.(args), cos.(args))
end

# Simplified U-Net (for 32x32 latent)
function create_unet(; latent_ch=4, base_ch=128, time_emb_dim=256)
    return Chain(
        # Time embedding MLP
        Dense(time_emb_dim, time_emb_dim*4, activation=silu),
        Dense(time_emb_dim*4, time_emb_dim*4, activation=silu),

        # Down
        Conv((3,3), latent_ch => base_ch, pad=1),
        ResBlock(Conv((3,3), base_ch => base_ch, pad=1), Conv((3,3), base_ch => base_ch, pad=1)),
        Conv((4,4), base_ch => base_ch*2, stride=2, pad=1),  # /2
        ResBlock(Conv((3,3), base_ch*2 => base_ch*2, pad=1), Conv((3,3), base_ch*2 => base_ch*2, pad=1)),

        # Middle
        ResBlock(Conv((3,3), base_ch*2 => base_ch*2, pad=1), Conv((3,3), base_ch*2 => base_ch*2, pad=1)),

        # Up
        ConvTranspose((4,4), base_ch*2 => base_ch, stride=2, pad=1),  # *2
        ResBlock(Conv((3,3), base_ch => base_ch, pad=1), Conv((3,3), base_ch => base_ch, pad=1)),
        Conv((3,3), base_ch => latent_ch, pad=1)  # Output Îµ
    )
end
```

**ã‚¹ãƒ†ãƒƒãƒ—3: Diffusionè¨“ç·´ãƒ«ãƒ¼ãƒ—**

```julia
# Noise schedule
function cosine_beta_schedule(T::Int; s=0.008)
    t = 0:T
    Î±_bar = cos.(((t ./ T) .+ s) ./ (1 + s) .* Ï€ ./ 2).^2
    Î±_bar = Î±_bar ./ Î±_bar[1]
    betas = 1 .- Î±_bar[2:end] ./ Î±_bar[1:end-1]
    return clamp.(betas, 0f0, 0.999f0), Î±_bar[2:end]
end

# Forward diffusion
function forward_diffusion(zâ‚€, t, Î±_bar, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ
    return z_t, Îµ
end

# è¨“ç·´ãƒ«ãƒ¼ãƒ—
function train_ldm!(unet, encoder, dataloader; T=1000, epochs=100, lr=1e-4)
    betas, Î±_bar = cosine_beta_schedule(T)
    ps_unet, st_unet = Lux.setup(Random.default_rng(), unet)
    opt = Adam(lr)
    opt_state = Optimisers.setup(opt, ps_unet)
    rng = Random.default_rng()

    # Freeze encoder
    ps_enc, st_enc = Lux.setup(rng, encoder)

    for epoch in 1:epochs
        total_loss = 0.0
        for (x,) in dataloader
            # Encode (no grad)
            zâ‚€, _ = encoder(x, ps_enc, st_enc)

            # Random timestep
            t = rand(rng, 1:T)

            # Forward diffusion
            z_t, Îµ_true = forward_diffusion(zâ‚€, t, Î±_bar, rng)

            # Time embedding
            t_emb = sinusoidal_embedding(t, 256)

            # Predict noise
            Îµ_pred, st_unet = unet((z_t, t_emb), ps_unet, st_unet)

            # MSE loss
            loss = mean((Îµ_pred .- Îµ_true).^2)

            # Backprop (only unet)
            gs = gradient(p -> loss, ps_unet)[1]
            opt_state, ps_unet = Optimisers.update(opt_state, ps_unet, gs)

            total_loss += loss
        end
        if epoch % 10 == 0
            println("Epoch $epoch: Loss = $(total_loss / length(dataloader))")
        end
    end
    return ps_unet, st_unet
end
```

**ã‚¹ãƒ†ãƒƒãƒ—4: CFGã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**

```julia
# DDIM sampling with CFG
function ddim_sample_cfg(unet, decoder, z_T, c, w; steps=50, Î·=0.0)
    T = 1000
    betas, Î±_bar = cosine_beta_schedule(T)
    timesteps = reverse(Int.(round.(range(1, T, length=steps))))

    z_t = z_T
    for (i, t) in enumerate(timesteps)
        t_prev = i == steps ? 0 : timesteps[i+1]

        # Time embedding
        t_emb = sinusoidal_embedding(t, 256)

        # CFG: 2 forward passes
        Îµ_uncond, _ = unet((z_t, t_emb, nothing), ps_unet, st_unet)
        Îµ_cond, _ = unet((z_t, t_emb, c), ps_unet, st_unet)
        Îµ_cfg = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)

        # Predict xâ‚€
        pred_xâ‚€ = (z_t .- sqrt(1 - Î±_bar[t]) .* Îµ_cfg) ./ sqrt(Î±_bar[t])

        # DDIM step
        if t_prev > 0
            Ïƒ_t = Î· * sqrt((1 - Î±_bar[t_prev]) / (1 - Î±_bar[t])) * sqrt(1 - Î±_bar[t] / Î±_bar[t_prev])
            dir_z = sqrt(1 - Î±_bar[t_prev] - Ïƒ_t^2) .* Îµ_cfg
            noise = Ïƒ_t .* randn(Float32, size(z_t))
            z_t = sqrt(Î±_bar[t_prev]) .* pred_xâ‚€ .+ dir_z .+ noise
        else
            z_t = pred_xâ‚€
        end
    end

    # Decode
    x, _ = decoder(z_t, ps_dec, st_dec)
    return x
end

# ä½¿ç”¨ä¾‹
z_T = randn(Float32, 32, 32, 4, 1)  # Random noise
c = nothing  # ç„¡æ¡ä»¶ or ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
w = 7.5      # CFG scale
x_gen = ddim_sample_cfg(unet, decoder, z_T, c, w, steps=50)
```

<details><summary>å®Œå…¨ãªè¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ</summary>

```julia
using MLDatasets, Images

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
train_data = MNIST(split=:train)
X_train = Float32.(reshape(train_data.features, 28, 28, 1, :))
X_train = (X_train .* 2f0) .- 1f0  # [-1, 1]

# Dataloader
batchsize = 64
dataloader = [(X_train[:,:,:,i:min(i+batchsize-1,end)],)
              for i in 1:batchsize:size(X_train,4)]

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
encoder = create_encoder(in_ch=1, latent_ch=4, base_ch=32)
decoder = create_decoder(latent_ch=4, out_ch=1, base_ch=32)
unet = create_unet(latent_ch=4, base_ch=64)

# Stage 1: VAEè¨“ç·´
println("Training VAE...")
ps_enc, ps_dec, st_enc, st_dec = train_vae!(encoder, decoder, dataloader, epochs=20)

# Stage 2: Diffusionè¨“ç·´
println("Training Diffusion...")
ps_unet, st_unet = train_ldm!(unet, encoder, dataloader, T=1000, epochs=100)

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
println("Generating samples...")
z_T = randn(Float32, 7, 7, 4, 16)  # 28/4=7 (downsampled)
x_gen = ddim_sample_cfg(unet, decoder, z_T, nothing, 1.0, steps=50)

# ä¿å­˜
using FileIO
save("generated.png", colorview(Gray, x_gen[:,:,1,1]))
```

</details>

### ğŸ¦€ Rustæ¨è«–å®Ÿè£…

**safetensorsã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰**:

```rust
use candle_core::{Device, Tensor};
use candle_nn::{VarBuilder, Module};

// VAE Decoder
struct Decoder {
    conv1: candle_nn::Conv2d,
    conv2: candle_nn::Conv2d,
    // ... more layers
}

impl Decoder {
    fn new(vb: VarBuilder) -> Result<Self> {
        let conv1 = candle_nn::conv2d(4, 512, 3, Default::default(), vb.pp("conv1"))?;
        let conv2 = candle_nn::conv_transpose2d(512, 256, 4, Default::default(), vb.pp("conv2"))?;
        // ...
        Ok(Self { conv1, conv2 })
    }

    fn forward(&self, z: &Tensor) -> Result<Tensor> {
        let x = self.conv1.forward(z)?;
        let x = x.relu()?;
        let x = self.conv2.forward(&x)?;
        // ...
        Ok(x.tanh()?)
    }
}

// Load weights
fn load_ldm_model(path: &str) -> Result<(UNet, Decoder)> {
    let device = Device::cuda_if_available(0)?;
    let vb = unsafe { VarBuilder::from_mmaped_safetensors(&[path], candle_core::DType::F32, &device)? };

    let unet = UNet::new(vb.pp("unet"))?;
    let decoder = Decoder::new(vb.pp("decoder"))?;

    Ok((unet, decoder))
}
```

**CFGæ¨è«–**:

```rust
fn cfg_sample(
    unet: &UNet,
    decoder: &Decoder,
    z_t: Tensor,
    c: Option<&Tensor>,
    w: f32,
    steps: usize,
) -> Result<Tensor> {
    let mut z = z_t;
    let timesteps: Vec<usize> = (0..steps).rev().collect();

    for t in timesteps {
        let t_tensor = Tensor::new(&[t as f32], z.device())?;

        // Unconditional
        let eps_uncond = unet.forward(&z, &t_tensor, None)?;

        // Conditional
        let eps_cond = if let Some(cond) = c {
            unet.forward(&z, &t_tensor, Some(cond))?
        } else {
            eps_uncond.clone()
        };

        // CFG
        let eps_cfg = (eps_uncond + (eps_cond - &eps_uncond)? * w)?;

        // DDIM step
        z = ddim_step(&z, &eps_cfg, t)?;
    }

    // Decode
    decoder.forward(&z)
}
```

**ãƒãƒƒãƒæ¨è«–æœ€é©åŒ–**:

```rust
// ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š
fn batch_generate(
    unet: &UNet,
    decoder: &Decoder,
    batch_size: usize,
    c: &Tensor,
    w: f32,
) -> Result<Vec<Tensor>> {
    // ãƒã‚¤ã‚ºãƒãƒƒãƒç”Ÿæˆ
    let z_T = Tensor::randn(0f32, 1f32, (batch_size, 4, 64, 64), &Device::Cuda(0))?;

    // ãƒãƒƒãƒæ¨è«–
    let x_batch = cfg_sample(unet, decoder, z_T, Some(c), w, 50)?;

    // Split batch
    (0..batch_size)
        .map(|i| x_batch.narrow(0, i, 1))
        .collect::<Result<Vec<_>>>()
}
```

### æ•°å€¤å®‰å®šæ€§ã¨ãƒ‡ãƒãƒƒã‚°

**ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨ãƒ‡ãƒãƒƒã‚°æ–¹æ³•**:

| ã‚¨ãƒ©ãƒ¼ | åŸå›  | è§£æ±ºç­– |
|:-------|:-----|:-------|
| **NaN loss** | å‹¾é…çˆ†ç™º / æ•°å€¤ä¸å®‰å®š | Gradient clipping / learning rateå‰Šæ¸› / Mixed precision |
| **Mode collapse** | CFG scaleé«˜ã™ã | $w \in [1, 7.5]$ ã«åˆ¶é™ |
| **ã¼ã‚„ã‘ãŸç”»åƒ** | VAEéåœ§ç¸® / Î²é«˜ã™ã | $\beta < 1$ / åœ§ç¸®ç‡å‰Šæ¸› |
| **Posterior collapse** | VAEè¨“ç·´ä¸ååˆ† | KL annealing / Free bits |
| **çœŸã£é»’/çœŸã£ç™½ç”»åƒ** | Zero terminal SNRæœªå¯¾å¿œ | Noise schedule rescaling |

**æ•°å€¤å®‰å®šåŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯**:

```julia
# Gradient clipping
function clip_grad!(grads, max_norm=1.0)
    total_norm = sqrt(sum(x -> sum(x .^ 2), grads))
    clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1
        foreach(g -> g .*= clip_coef, grads)
    end
end

# Mixed precision (FP16è¨“ç·´)
using CUDA
x_fp16 = cu(Float16.(x))
# ... forward pass in FP16
loss_fp32 = Float32(loss)  # Lossè¨ˆç®—ã¯FP32
```

**ãƒ‡ãƒãƒƒã‚°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:

```julia
# 1. VAEå†æ§‹æˆç¢ºèª
z = encoder(x)
x_recon = decoder(z)
@assert mean(abs.(x - x_recon)) < 0.5  # å†æ§‹æˆèª¤å·®

# 2. Forward diffusionç¢ºèª
z_t, Îµ = forward_diffusion(z, T, Î±_bar)
@assert mean(abs.(z_t)) â‰ˆ 1.0 atol=0.5  # T=1000ã§ã»ã¼ã‚¬ã‚¦ã‚·ã‚¢ãƒ³

# 3. Noise predictionç¢ºèª
Îµ_pred = unet(z_t, t)
@assert size(Îµ_pred) == size(Îµ)  # å½¢çŠ¶ä¸€è‡´

# 4. CFGç¢ºèª
Îµ_cfg = cfg_forward(unet, z_t, t, c, w)
@assert !any(isnan.(Îµ_cfg))  # NaNãƒã‚§ãƒƒã‚¯
```

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®70%å®Œäº†ï¼** å®Ÿè£…ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚Juliaè¨“ç·´â†’Rustæ¨è«–ã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§CFGå®Ÿé¨“ã¸ã€‚

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” CFGå®Ÿé¨“ã¨å“è³ªåˆ†æ

### è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

**Q1: CFGã®guidance scale $w$ã®åŠ¹æœ**

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®å‡ºåŠ›ã‚’äºˆæ¸¬ã›ã‚ˆ:
```julia
w_values = [0.0, 1.0, 3.0, 7.5, 15.0]
for w in w_values
    x = ddim_sample_cfg(unet, decoder, z_T, c, w)
    println("w=$w: quality=?, diversity=?")
end
```

<details><summary>è§£ç­”</summary>

- $w=0.0$: quality=ä½, diversity=é«˜ (ç„¡æ¡ä»¶ç”Ÿæˆ)
- $w=1.0$: quality=ä¸­, diversity=ä¸­ (æ¨™æº–æ¡ä»¶ä»˜ã)
- $w=3.0$: quality=é«˜, diversity=ä¸­ (è»½ã„CFG)
- $w=7.5$: quality=æœ€é«˜, diversity=ä½ (SDæ¨å¥¨å€¤)
- $w=15.0$: quality=éé£½å’Œ, diversity=æœ€ä½ (over-guidance)

</details>

**Q2: Negative Promptã®æ•°å­¦**

Negative Promptå®Ÿè£…ã®ã“ã®è¡Œã‚’èª¬æ˜ã›ã‚ˆ:
```julia
Îµ_cfg = Îµ_neg .+ w .* (Îµ_pos .- Îµ_neg)
```

<details><summary>è§£ç­”</summary>

$\tilde{\epsilon} = \epsilon_\text{neg} + w(\epsilon_\text{pos} - \epsilon_\text{neg})$ ã¯ã€Œ$\epsilon_\text{neg}$ã‹ã‚‰$\epsilon_\text{pos}$ã¸$w$å€å¼·ãç§»å‹•ã€ã‚’æ„å‘³ã™ã‚‹ã€‚ã“ã‚Œã¯ãƒ™ã‚¯ãƒˆãƒ«ã®ç·šå½¢çµåˆã§ã€CFGã®ä¸€èˆ¬åŒ–ã€‚Negative Promptã¯ã€Œé¿ã‘ãŸã„æ¦‚å¿µã€ã‚’$\epsilon_\text{neg}$ã¨ã—ã¦æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ç„¡æ¡ä»¶$\emptyset$ã®ä»£ã‚ã‚Šã«ä½¿ã†ã€‚

</details>

**Q3: Zero Terminal SNRã®åŠ¹æœ**

ä»¥ä¸‹ã®rescalingå‰å¾Œã§ä½•ãŒå¤‰ã‚ã‚‹ã‹:
```julia
# Before
Î±_bar_before = [0.99, 0.98, ..., 0.01]  # Î±_T = 0.01 â‰  0

# After rescaling
Î±_bar_after = Î±_bar_before ./ Î±_bar_before[end]
# Î±_bar_after[end] = 1.0 â†’ sqrt(Î±_bar_after[end]) = 1.0
```

<details><summary>è§£ç­”</summary>

Zero Terminal SNRã¯ $\bar{\alpha}_T = 0$ ã‚’å¼·åˆ¶ã™ã‚‹ã€‚Rescalingå‰ã¯ $\bar{\alpha}_T = 0.01 \neq 0$ ãªã®ã§ã€$T$ã‚¹ãƒ†ãƒƒãƒ—ç›®ã§ã‚‚ã‚ãšã‹ã«ä¿¡å·ãŒæ®‹ã‚‹ã€‚Rescalingå¾Œã¯ $\bar{\alpha}_T = 0$ ã¨ãªã‚Šã€å®Œå…¨ãªã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã«åˆ°é”ã€‚ã“ã‚Œã«ã‚ˆã‚Šéå¸¸ã«æ˜ã‚‹ã„/æš—ã„ç”»åƒã®ç”Ÿæˆå“è³ªãŒå‘ä¸Šã™ã‚‹ï¼ˆLin et al. 2023 [^zero_snr]ï¼‰ã€‚

</details>

**Q4: Text Conditioningå®Ÿè£…**

CLIP text encodingã§ã¯ `hidden = transformer(tokens)` ã§å…¨77ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹ï¼ˆshape: `[77, 768]`ï¼‰ã‚’å–å¾—ã—ã€`c = hidden` ã¨ã—ã¦ãã®ã¾ã¾ä½¿ã†ã€‚

ãªãœ`hidden[0]` (CLSãƒˆãƒ¼ã‚¯ãƒ³)ã ã‘ã§ãªãå…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ã†ã‹ï¼Ÿ

<details><summary>è§£ç­”</summary>

Stable Diffusionã¯ **å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹** $c \in \mathbb{R}^{77 \times 768}$ ã‚’Cross-Attentionã«å…¥åŠ›ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Š:
1. å„å˜èªã®æƒ…å ±ã‚’å€‹åˆ¥ã«ä¿æŒï¼ˆ"red cat"ã§"red"ã¨"cat"ãŒåˆ¥ã€…ã«å‡¦ç†ï¼‰
2. Cross-Attentionã§ç”»åƒã®å„ä½ç½®ãŒé–¢é€£ã™ã‚‹å˜èªã«æ³¨ç›®ã§ãã‚‹
3. é•·æ–‡ã®è©³ç´°ãªé–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‰ã‚Œã‚‹

`hidden[0]` (BERTã‚¹ã‚¿ã‚¤ãƒ«)ã ã¨æ–‡å…¨ä½“ã‚’1ãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®ã—ã¦ã—ã¾ã„ã€è©³ç´°ãªå˜èªãƒ¬ãƒ™ãƒ«ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãŒå¤±ã‚ã‚Œã‚‹ã€‚

</details>

**Q5: Min-SNR weightingã®ç›®çš„**

Min-SNR loss weightingã®ã“ã®ã‚³ãƒ¼ãƒ‰ã®æ„å›³ã¯ï¼Ÿ
```julia
snr = Î±_bar ./ (1 .- Î±_bar)
weight = min.(snr, 5.0)  # Î³=5
loss = weight[t] * mse(Îµ_pred, Îµ_true)
```

<details><summary>è§£ç­”</summary>

SNRãŒé«˜ã„ï¼ˆãƒã‚¤ã‚ºå°‘ãªã„ï¼‰timestepã¯å­¦ç¿’ãŒç°¡å˜ã§ã€ä½ã„ï¼ˆãƒã‚¤ã‚ºå¤šã„ï¼‰timestepã¯é›£ã—ã„ã€‚å‡ç­‰ã«weightã™ã‚‹ã¨ç°¡å˜ãªtimestepã«éé©åˆã™ã‚‹ã€‚Min-SNR weightingã¯:
1. $\text{SNR}(t)$ã‚’è¨ˆç®—ï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰
2. $\gamma=5$ã§ã‚¯ãƒªãƒƒãƒ— â†’ SNRé«˜ã™ãã‚‹timestepã®weightã‚’å‰Šæ¸›
3. é›£ã—ã„timestepï¼ˆä½SNRï¼‰ã®å­¦ç¿’ã‚’ä¿ƒé€²

Hang et al. (2023) [^min_snr]ã¯3.4å€ã®è¨“ç·´é«˜é€ŸåŒ–ã‚’å ±å‘Šã€‚

</details>

### å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: CFG Scaleå®Ÿé¨“

**èª²é¡Œ**: Guidance scale $w \in \{1, 3, 5, 7.5, 10, 15\}$ ã§ç”Ÿæˆã—ã€å“è³ªã¨FID/IS/CLIP Scoreã‚’è¨ˆæ¸¬ã›ã‚ˆã€‚

```julia
using Flux, CUDA

# å®Ÿé¨“è¨­å®š
w_values = [1.0, 3.0, 5.0, 7.5, 10.0, 15.0]
n_samples = 100
results = Dict()

for w in w_values
    println("Testing w=$w...")
    images = [begin
        z_T = randn(Float32, 32, 32, 4, 1)
        c = text_encoder("a beautiful landscape")  # CLIP encoding
        ddim_sample_cfg(unet, decoder, z_T, c, w, steps=50)
    end for _ in 1:n_samples]

    # å“è³ªæŒ‡æ¨™è¨ˆç®—
    fid = compute_fid(images, real_images)
    is_score = compute_inception_score(images)
    clip_score = compute_clip_score(images, "a beautiful landscape")

    results[w] = (fid=fid, is=is_score, clip=clip_score)
    println("  FID: $fid, IS: $is_score, CLIP: $clip_score")
end

# çµæœå¯è¦–åŒ–
using Plots
plot(values(results) .|> r -> r.fid, label="FIDâ†“", xlabel="w", ylabel="Score")
plot!(values(results) .|> r -> r.clip, label="CLIP Scoreâ†‘")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

| $w$ | FIDâ†“ | ISâ†‘ | CLIP Scoreâ†‘ | å¤šæ§˜æ€§ |
|:----|:-----|:----|:------------|:-------|
| 1.0 | 25.3 | 2.1 | 0.75 | é«˜ |
| 3.0 | 18.7 | 2.8 | 0.82 | ä¸­ |
| 5.0 | 14.2 | 3.2 | 0.87 | ä¸­ |
| **7.5** | **12.1** | **3.5** | **0.91** | ä½ |
| 10.0 | 13.5 | 3.4 | 0.89 | ä½ |
| 15.0 | 18.9 | 3.1 | 0.85 | æœ€ä½(mode collapse) |

**çµè«–**: $w=7.5$ ãŒå“è³ªã¨å¤šæ§˜æ€§ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ï¼ˆStable Diffusionæ¨™æº–å€¤ï¼‰ã€‚

### Symbol Reading Test

**Q1**: æ¬¡ã®æ•°å¼ã‚’èª­ã¿ä¸Šã’ã‚ˆ:
$$
\mathcal{L}_\text{LDM} = \mathbb{E}_{z_0 \sim q(z_0), \epsilon \sim \mathcal{N}(0,I), t} \left[ \|\epsilon - \epsilon_\theta(z_t, t)\|^2 \right]
$$

<details><summary>è§£ç­”</summary>

ã€Œã‚¨ãƒ«LDM equals æœŸå¾…å€¤(z0 is distributed according to q of z0, epsilon is distributed according to standard Gaussian, over t) of L2 norm of epsilon minus epsilon-theta of z-t comma t squaredã€

ã¾ãŸã¯æ—¥æœ¬èªã§:ã€Œæ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æå¤±ã¯ã€æ½œåœ¨å¤‰æ•°z0ã€æ¨™æº–æ­£è¦ãƒã‚¤ã‚ºÎµã€timestep tã«ã¤ã„ã¦ã®æœŸå¾…å€¤ã§ã€çœŸã®ãƒã‚¤ã‚ºÎµã¨ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ÎµÎ¸(z_t, t)ã®L2ãƒãƒ«ãƒ ã®2ä¹—ã€

</details>

**Q2**: ã“ã®æ•°å¼ã®æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆ:
$$
\tilde{\epsilon}_\theta = (1-w) \epsilon_\theta(z_t, t, \emptyset) + w \cdot \epsilon_\theta(z_t, t, c)
$$

<details><summary>è§£ç­”</summary>

CFG (Classifier-Free Guidance)ã®ç·šå½¢çµåˆå½¢å¼ã€‚ç„¡æ¡ä»¶ãƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta(z_t, t, \emptyset)$ ã¨æ¡ä»¶ä»˜ããƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta(z_t, t, c)$ ã‚’ã€é‡ã¿ $(1-w)$ ã¨ $w$ ã§åŠ é‡å¹³å‡ã€‚$w=1$ ã§æ¨™æº–æ¡ä»¶ä»˜ãã€$w>1$ ã§mode-seekingã€‚

</details>

**Q3**: Zero Terminal SNRã®rescalingå¼ã‚’æ›¸ã‘ã€‚

<details><summary>è§£ç­”</summary>

$$
\tilde{\alpha}_t = \frac{\alpha_t}{\alpha_T}
$$

ã¾ãŸã¯ç´¯ç©ç©ã§:
$$
\tilde{\bar{\alpha}}_t = \frac{\bar{\alpha}_t}{\bar{\alpha}_T}
$$

ã“ã‚Œã«ã‚ˆã‚Š $\tilde{\bar{\alpha}}_T = 1 \to \sqrt{\tilde{\bar{\alpha}}_T} = 1 \to$ æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã§å®Œå…¨ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã€‚

</details>

### LaTeX Writing Test

**Q1**: "The encoder maps x to z" ã‚’æ•°å¼ã§æ›¸ã‘ã€‚

<details><summary>è§£ç­”</summary>

$$
z = \mathcal{E}(x), \quad x \in \mathbb{R}^{H \times W \times C}, \quad z \in \mathbb{R}^{h \times w \times c}
$$

ã¾ãŸã¯é–¢æ•°è¨˜æ³•:
$$
\mathcal{E}: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^{h \times w \times c}
$$

</details>

**Q2**: CFGã®ä¿®æ­£ãƒã‚¤ã‚ºäºˆæ¸¬å¼ã‚’LaTeXã§æ›¸ã‘ã€‚

<details><summary>è§£ç­”</summary>

```latex
\tilde{\epsilon}_\theta(z_t, t, c, w) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right)
```

ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°çµæœ:
$$
\tilde{\epsilon}_\theta(z_t, t, c, w) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right)
$$

</details>

**Q3**: Multi-Head Cross-Attentionå¼ã‚’LaTeXã§æ›¸ã‘ã€‚

<details><summary>è§£ç­”</summary>

```latex
\begin{aligned}
Q &= W_Q \mathbf{f} \\
K &= W_K \mathbf{c} \\
V &= W_V \mathbf{c} \\
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V \\
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
\end{aligned}
```

</details>

### Code Translation Test

**Q1**: Forward diffusionã®æ•°å¼ $z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$ ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

```julia
function forward_diffusion(zâ‚€, t, Î±_bar, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ
    return z_t, Îµ
end
```

ãƒã‚¤ãƒ³ãƒˆ:
- `.` broadcastæ¼”ç®—å­ã§è¦ç´ ã”ã¨ã®æ¼”ç®—
- `randn` ã§ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºç”Ÿæˆ
- `sqrt(Î±_bar[t])` ã§timestepã®ã‚¹ã‚±ãƒ¼ãƒ«å–å¾—

</details>

**Q2**: CFGã®æ•°å¼ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

```julia
function cfg_forward(unet, z_t, t, c, w, ps, st)
    # Unconditional
    Îµ_uncond, _ = unet((z_t, t, nothing), ps, st)

    # Conditional
    Îµ_cond, _ = unet((z_t, t, c), ps, st)

    # CFG combination
    Îµ_cfg = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)

    return Îµ_cfg
end
```

ã¾ãŸã¯ç­‰ä¾¡ãªå½¢:
```julia
Îµ_cfg = (1 - w) .* Îµ_uncond .+ w .* Îµ_cond
```

</details>

**Q3**: Cross-Attentionã®å¼ $\text{Attention}(Q, K, V) = \text{softmax}(QK^\top/\sqrt{d_k}) V$ ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

```julia
function scaled_dot_product_attention(Q, K, V; dropout_rate=0.0)
    d_k = size(K, 2)

    # Scores: Q * K^T / sqrt(d_k)
    scores = (Q * K') ./ sqrt(Float32(d_k))  # [N_q, N_k]

    # Softmax
    attn_weights = softmax(scores, dims=2)  # Over keys

    # Apply dropout
    if dropout_rate > 0
        attn_weights = dropout(attn_weights, dropout_rate)
    end

    # Weighted sum
    output = attn_weights * V  # [N_q, d_v]

    return output, attn_weights
end
```

ãƒã‚¤ãƒ³ãƒˆ:
- `K'` ã§è»¢ç½®
- `softmax(..., dims=2)` ã§keyæ¬¡å…ƒã«æ²¿ã£ã¦ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
- `dropout` ã§æ­£å‰‡åŒ–

</details>

### Paper Reading Test

**Pass 1å®Ÿè£…**: ä»¥ä¸‹ã®è«–æ–‡æ¦‚è¦ã‚’èª­ã¿ã€Pass 1æƒ…å ±ã‚’æŠ½å‡ºã›ã‚ˆã€‚

**Paper**: "High-Resolution Image Synthesis with Latent Diffusion Models" (Rombach et al., CVPR 2022)

**Abstract** (æŠœç²‹):
> By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis quality on image data. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders.

**Pass 1ã‚¿ã‚¹ã‚¯**: ä»¥ä¸‹ã‚’æŠ½å‡ºã›ã‚ˆ:
1. Category (ã‚«ãƒ†ã‚´ãƒª)
2. Context (æ–‡è„ˆ)
3. Correctness (æ­£å½“æ€§ã®ç›´æ„Ÿ)
4. Contributions (è²¢çŒ®)
5. Clarity (æ˜ç­æ€§)

<details><summary>è§£ç­”</summary>

```julia
pass1_info = Dict(
    "category" => "Image Synthesis / Generative Models",
    "context" => "Diffusion models achieve SOTA but are computationally expensive in pixel space",
    "correctness" => "Appears sound - addresses real bottleneck (computation cost)",
    "contributions" => [
        "Apply diffusion in latent space (not pixel space)",
        "Use pretrained VAE for compression",
        "Enable training on limited compute",
        "Retain quality and controllability"
    ],
    "clarity" => "Well-written. Clear problem statement and solution.",
    "decision" => "READ - Addresses important problem with novel approach"
)
```

**5Cåˆ¤å®š**:
- Category: âœ“ æ˜ç¢º
- Context: âœ“ ãƒ”ã‚¯ã‚»ãƒ«æ‹¡æ•£ã®å•é¡Œç‚¹ã‚’æ˜ç¤º
- Correctness: âœ“ ç†è«–çš„ã«å¥å…¨
- Contributions: âœ“ 4ã¤ã®ä¸»è¦è²¢çŒ®
- Clarity: âœ“ æ˜ç­

â†’ **Pass 2ã¸é€²ã‚€ä¾¡å€¤ã‚ã‚Š**

</details>

### Implementation Challenge: Mini LDM End-to-End

**èª²é¡Œ**: MNISTä¸Šã§Mini Latent Diffusion Modelã‚’è¨“ç·´ã—ã€ç”Ÿæˆå“è³ªã‚’è©•ä¾¡ã›ã‚ˆã€‚

**è¦ä»¶**:
- VAE: 28Ã—28Ã—1 â†’ 7Ã—7Ã—4 (åœ§ç¸®ç‡16x)
- U-Net: æ½œåœ¨ç©ºé–“ã§æ‹¡æ•£
- CFG: guidance scale 1.0, 3.0, 7.5ã§æ¯”è¼ƒ
- Sampling: DDIM 50 steps
- è©•ä¾¡: FID, ä¸»è¦³å“è³ª

```julia
using Lux, MLDatasets, Images, Optimisers, CUDA

# === Stage 1: VAEè¨“ç·´ ===
function create_mnist_vae()
    encoder = Chain(
        Conv((3,3), 1 => 32, pad=1, activation=relu),
        Conv((4,4), 32 => 64, stride=2, pad=1, activation=relu),  # 28 -> 14
        Conv((4,4), 64 => 64, stride=2, pad=1, activation=relu),  # 14 -> 7
        Conv((3,3), 64 => 4, pad=1)  # Latent
    )

    decoder = Chain(
        Conv((3,3), 4 => 64, pad=1, activation=relu),
        ConvTranspose((4,4), 64 => 64, stride=2, pad=1, activation=relu),  # 7 -> 14
        ConvTranspose((4,4), 64 => 32, stride=2, pad=1, activation=relu),  # 14 -> 28
        Conv((3,3), 32 => 1, pad=1, activation=tanh)
    )

    return encoder, decoder
end

# VAEè¨“ç·´
println("Stage 1: Training VAE...")
train_data = MNIST(split=:train)
X = Float32.(reshape(train_data.features, 28, 28, 1, :))
X = (X .* 2f0) .- 1f0  # Normalize to [-1, 1]

encoder, decoder = create_mnist_vae()
ps_enc, st_enc = Lux.setup(Random.default_rng(), encoder)
ps_dec, st_dec = Lux.setup(Random.default_rng(), decoder)

# ... è¨“ç·´ãƒ«ãƒ¼ãƒ— (20 epochs)

# === Stage 2: Diffusionè¨“ç·´ ===
println("Stage 2: Training Diffusion...")

function create_tiny_unet()
    # Simplified U-Net for 7Ã—7Ã—4 latent
    return Chain(
        Conv((3,3), 4 => 64, pad=1),
        ResBlock(64),
        Conv((4,4), 64 => 128, stride=2, pad=1),  # 7 -> 4 (round up)
        ResBlock(128),
        ConvTranspose((4,4), 128 => 64, stride=2, pad=1),  # 4 -> 7
        ResBlock(64),
        Conv((3,3), 64 => 4, pad=1)
    )
end

unet = create_tiny_unet()
ps_unet, st_unet = Lux.setup(Random.default_rng(), unet)

# ... Diffusionè¨“ç·´ (100 epochs)

# === Stage 3: CFG Sampling ===
println("Stage 3: Generating with different CFG scales...")

w_values = [1.0, 3.0, 7.5]
n_samples = 16

for w in w_values
    println("  Generating with w=$w...")
    samples = [begin
        z_T = randn(Float32, 7, 7, 4, 1)
        c = nothing  # Unconditioned for MNIST
        ddim_sample_cfg(unet, decoder, z_T, c, w, steps=50)
    end for _ in 1:n_samples]

    # ä¿å­˜
    grid = hcat(samples...)
    save("mnist_cfg_w$(w).png", colorview(Gray, grid[:,:,1,1]))

    # FIDè¨ˆç®— (ç°¡ç•¥ç‰ˆ)
    fid = compute_fid_mnist(samples, X)
    println("  w=$w: FID = $fid")
end

# çµæœ:
# w=1.0: FID = 45.2 (å¤šæ§˜æ€§é«˜ã„ã€å“è³ªä¸­)
# w=3.0: FID = 32.1 (ãƒãƒ©ãƒ³ã‚¹)
# w=7.5: FID = 38.5 (å“è³ªé«˜ã„ãŒå¤šæ§˜æ€§ä½ä¸‹)
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**: å„CFGã‚¹ã‚±ãƒ¼ãƒ«ã§16ã‚µãƒ³ãƒ—ãƒ«ã®ã‚°ãƒªãƒƒãƒ‰ç”»åƒã€‚

### å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸2: Noise Offsetå®Ÿé¨“

**èª²é¡Œ**: Noise offsetã‚’0.0, 0.05, 0.1ã§è¨“ç·´ã—ã€æ˜ã‚‹ã„/æš—ã„ç”»åƒã®ç”Ÿæˆå“è³ªã‚’æ¯”è¼ƒã›ã‚ˆã€‚

```julia
# Noise offsetä»˜ãforward diffusion
function forward_diffusion_with_offset(zâ‚€, t, Î±_bar, offset, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    Îµ_offset = Îµ .+ offset  # ãƒã‚¤ã‚¢ã‚¹è¿½åŠ 
    z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ_offset
    return z_t, Îµ
end

# è¨“ç·´
offset_values = [0.0, 0.05, 0.1]
for offset in offset_values
    println("Training with noise offset=$offset...")
    ps_unet, st_unet = train_ldm_with_offset!(unet, encoder, dataloader, offset)

    # ãƒ†ã‚¹ãƒˆ: æš—ã„ç”»åƒç”Ÿæˆ
    c_dark = text_encoder("a dark forest at night")
    x_dark = ddim_sample_cfg(unet, decoder, z_T, c_dark, 7.5)

    # ãƒ†ã‚¹ãƒˆ: æ˜ã‚‹ã„ç”»åƒç”Ÿæˆ
    c_bright = text_encoder("a bright sunny beach")
    x_bright = ddim_sample_cfg(unet, decoder, z_T, c_bright, 7.5)

    # å“è³ªè©•ä¾¡ï¼ˆä¸»è¦³ or LPIPSç­‰ï¼‰
    save("dark_offset_$(offset).png", x_dark)
    save("bright_offset_$(offset).png", x_bright)
end
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**: Noise offset > 0 ã§æš—ã„ç”»åƒã®ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ãŒå‘ä¸Šã€‚

### è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] VAEã®åœ§ç¸®ç‡ã¨å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£
- [ ] DDPMã¨LDMã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] CFGã®3è¦–ç‚¹ï¼ˆÎµ / score / ç¢ºç‡ï¼‰ã‚’å°å‡ºã§ãã‚‹
- [ ] Negative Promptã®æ•°å­¦çš„æ„å‘³ã‚’ç†è§£
- [ ] Cross-Attentionã®å®Ÿè£…ã‚’æ›¸ã‘ã‚‹
- [ ] FLUX architectureã®ç‰¹å¾´ã‚’åˆ—æŒ™ã§ãã‚‹
- [ ] Min-SNR weightingã®åŠ¹æœã‚’èª¬æ˜ã§ãã‚‹
- [ ] Zero Terminal SNRã®å¿…è¦æ€§ã‚’ç†è§£
- [ ] Juliaè¨“ç·´ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‘ã‚‹
- [ ] Rustæ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‘ã‚‹

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®85%å®Œäº†ï¼** å®Ÿé¨“ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚CFGå®Ÿé¨“ã§å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã¸ã€‚

> **Progress: 85%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. LDM ã®æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆ`z = encode(x)` â†’ æ‹¡æ•£ â†’ `x = decode(z)`ï¼‰ã§ã€VAE ãƒ‡ã‚³ãƒ¼ãƒ€ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’æ¸›ã‚‰ã™ãŸã‚ã®å®Ÿè£…ä¸Šã®å·¥å¤«ï¼ˆå¾Œå‡¦ç†ã€æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç­‰ï¼‰ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. CFG ã‚¹ã‚±ãƒ¼ãƒ« $w$ ã‚’ä¸Šã’ã™ãã‚‹ã¨å“è³ªãŒä¸‹ãŒã‚‹åŸå› ã‚’ã€éé£½å’Œï¼ˆè‰²é£½å’Œï¼‰ãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆç™ºç”Ÿã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

---

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ20åˆ†ï¼‰â€” ç ”ç©¶æœ€å‰ç·šã¨ãƒªã‚½ãƒ¼ã‚¹

### LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œ

```mermaid
graph TD
    DDPM[DDPM<br>Ho+ 2020] --> LDM[Latent Diffusion<br>Rombach+ 2021]
    LDM --> SD15[Stable Diffusion 1.5<br>2022]
    SD15 --> SD2[SD 2.0/2.1<br>2022]
    SD2 --> SDXL[SDXL<br>2023]
    SDXL --> SD3[SD 3.0/3.5<br>2024]

    LDM --> Imagen[Imagen<br>Google 2022]
    Imagen --> Imagen2[Imagen 2<br>2023]

    SD3 --> FLUX[FLUX.1<br>Black Forest Labs 2025]

    style LDM fill:#ffcccc
    style FLUX fill:#ccffff
```

### LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼æ¯”è¼ƒè¡¨

| ãƒ¢ãƒ‡ãƒ« | Year | Params | Text Encoder | Latent | Flow | FID (COCO) | é€Ÿåº¦ |
|:-------|:-----|:-------|:-------------|:-------|:-----|:-----------|:-----|
| **SD 1.5** | 2022 | 860M | CLIP ViT-L/14 | 8Ã—8Ã—4 | DDPM | 12.6 | 50 steps |
| **SD 2.0** | 2022 | 860M | OpenCLIP ViT-H/14 | 8Ã—8Ã—4 | DDPM | 11.2 | 50 steps |
| **SDXL** | 2023 | 2.6B | CLIP+OpenCLIP | 8Ã—8Ã—4 | DDPM | 9.5 | 50 steps |
| **SD 3.0** | 2024 | 2B/8B | CLIP+T5 | 8Ã—8Ã—16 | **RF** | 8.7 | 28 steps |
| **FLUX.1** | 2025 | 12B | CLIP+T5 | 8Ã—8Ã—16 | **RF** | **7.2** | **20 steps** |
| **Imagen** | 2022 | 3B | T5-XXL | - | Cascade | 7.3 | 250 steps |

### æ¨è–¦æ›¸ç±ã¨ãƒªã‚½ãƒ¼ã‚¹

**æ›¸ç±**:

| ã‚¿ã‚¤ãƒˆãƒ« | è‘—è€… | ç‰¹å¾´ |
|:---------|:-----|:-----|
| **Deep Learning** | Goodfellow et al. | VAE/GANåŸºç¤ |
| **Probabilistic Machine Learning** | Murphy | ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ç†è«– |
| **Understanding Deep Learning** | Prince | Diffusionè©³è§£ (2023) |

**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹**:

| ãƒªã‚½ãƒ¼ã‚¹ | URL | å†…å®¹ |
|:---------|:----|:-----|
| **Hugging Face Diffusers** | [huggingface.co/docs/diffusers](https://huggingface.co/docs/diffusers) | å®Ÿè£…ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
| **Lil'Log: Diffusion** | [lilianweng.github.io](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) | è©³ç´°è§£èª¬ |
| **MIT 6.S184** | [diffusion.csail.mit.edu](https://diffusion.csail.mit.edu/) | SDE/FMç†è«– (2026) |
| **Stable Diffusion Papers** | [stability.ai/research](https://stability.ai/research) | å…¬å¼è«–æ–‡ãƒªã‚¹ãƒˆ |

**æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)**:

1. **FLUX Architecture** (2025): Rectified Flowçµ±åˆã€Transformer-basedã€12Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
2. **SD 3.5** (2024): CLIP+T5 dual encoderã€é«˜è§£åƒåº¦ç”Ÿæˆ
3. **PixArt-Î£** (2024): Efficient training with T5
4. **WÃ¼rstchen** (2023): 3-stage compression (42x)
5. **Playground v2.5** (2024): Aesthetic quality optimization

### ç”¨èªé›†

<details><summary>å…¨ç”¨èªãƒªã‚¹ãƒˆ (ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †)</summary>

| ç”¨èª | å®šç¾© |
|:-----|:-----|
| **CFG** | Classifier-Free Guidance: ç„¡æ¡ä»¶ã¨æ¡ä»¶ä»˜ãã‚¹ã‚³ã‚¢ã®ç·šå½¢çµåˆ |
| **CLIP** | Contrastive Language-Image Pre-training: Vision-languageãƒ¢ãƒ‡ãƒ« |
| **Cross-Attention** | Query=ç”»åƒ, Key/Value=ãƒ†ã‚­ã‚¹ãƒˆã®Attention |
| **DDIM** | Denoising Diffusion Implicit Models: æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| **DDPM** | Denoising Diffusion Probabilistic Models: ç¢ºç‡çš„æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« |
| **Encoder $\mathcal{E}$** | ãƒ”ã‚¯ã‚»ãƒ«â†’æ½œåœ¨ç©ºé–“ã®åœ§ç¸® |
| **Decoder $\mathcal{D}$** | æ½œåœ¨ç©ºé–“â†’ãƒ”ã‚¯ã‚»ãƒ«ã®å¾©å…ƒ |
| **FID** | FrÃ©chet Inception Distance: ç”Ÿæˆå“è³ªæŒ‡æ¨™ |
| **FLUX** | Flow Matching + Transformer LDM (2025) |
| **Guidance Scale $w$** | CFGã®å¼·åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| **KL-reg** | KL divergenceæ­£å‰‡åŒ–ï¼ˆVAEï¼‰ |
| **Latent Space** | ä½æ¬¡å…ƒåœ§ç¸®è¡¨ç¾ç©ºé–“ |
| **LDM** | Latent Diffusion Model |
| **LPIPS** | Learned Perceptual Image Patch Similarity |
| **Min-SNR** | Signal-to-Noise Ratioæœ€å°åŒ–é‡ã¿ä»˜ã‘ |
| **Negative Prompt** | é¿ã‘ãŸã„æ¦‚å¿µã®æŒ‡å®š |
| **Noise Offset** | Forward processã¸ã®ãƒã‚¤ã‚¢ã‚¹è¿½åŠ  |
| **Rectified Flow** | ç›´ç·šçš„ODEè¼¸é€ï¼ˆç¬¬38å›ï¼‰ |
| **SNR** | Signal-to-Noise Ratio: $\bar{\alpha}_t / (1-\bar{\alpha}_t)$ |
| **SpatialTransformer** | Self-Attn + Cross-Attn + FFN |
| **T5** | Text-To-Text Transfer Transformer |
| **v-prediction** | Velocity prediction: $v = \sqrt{\bar{\alpha}} \epsilon - \sqrt{1-\bar{\alpha}} z_0$ |
| **VQ-VAE** | Vector Quantized VAE: é›¢æ•£æ½œåœ¨è¡¨ç¾ |
| **Zero Terminal SNR** | $\bar{\alpha}_T = 0$ å¼·åˆ¶ |

</details>

### LDMã®çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
graph TD
    LDM[Latent Diffusion Model]

    LDM --> Theory[ç†è«–]
    Theory --> VAE[VAEåœ§ç¸®]
    Theory --> Diffusion[æ½œåœ¨ç©ºé–“æ‹¡æ•£]
    Theory --> CFG[Classifier-Free Guidance]

    LDM --> Arch[ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
    Arch --> UNet[U-Net + SpatialTransformer]
    Arch --> TextEnc[Text Encoder: CLIP/T5]
    Arch --> FLUX_A[FLUX: Transformer]

    LDM --> Train[è¨“ç·´]
    Train --> Stage1[Stage 1: VAE]
    Train --> Stage2[Stage 2: Diffusion]
    Train --> Tricks[Min-SNR / Noise Offset / v-pred]

    LDM --> Sample[ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    Sample --> DDIM_S[DDIM]
    Sample --> CFG_S[CFG]
    Sample --> NegPrompt[Negative Prompt]

    LDM --> Apps[å¿œç”¨]
    Apps --> SD[Stable Diffusion]
    Apps --> Imagen_A[Imagen]
    Apps --> FLUX_B[FLUX.1]
```

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®95%å®Œäº†ï¼** ç™ºå±•ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œã¨æœ€æ–°ç ”ç©¶ã‚’ä¿¯ç°ã—ãŸã€‚æœ€å¾Œã¯æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ã¸ã€‚

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### æ ¸å¿ƒçš„è¦ç‚¹

**1. ãªãœæ½œåœ¨ç©ºé–“ã‹**
- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®æ¬¡å…ƒçˆ†ç™º â†’ è¨ˆç®—é‡ $\mathcal{O}(d^2)$ ã§ç ´ç¶»
- VAEåœ§ç¸® (48x) â†’ 2300å€ã®é«˜é€ŸåŒ–
- Inductive bias: æ„å‘³ç©ºé–“ã§ã®æ‹¡æ•£ â†’ å“è³ªå‘ä¸Š

**2. CFGã®æœ¬è³ª**
$$
\tilde{\epsilon} = \epsilon_\text{uncond} + w(\epsilon_\text{cond} - \epsilon_\text{uncond})
$$
- ç„¡æ¡ä»¶ã¨æ¡ä»¶ä»˜ãã® **å·®åˆ†ãƒ™ã‚¯ãƒˆãƒ«** ãŒæ¡ä»¶ã®æ–¹å‘
- $w > 1$ ã§mode-seeking â†’ å“è³ªâ†‘ å¤šæ§˜æ€§â†“
- 3è¦–ç‚¹ï¼ˆÎµ / score / ç¢ºç‡ï¼‰ã§åŒã˜å¼ã‚’å°å‡ºå¯èƒ½

**3. Text Conditioningã®ä»•çµ„ã¿**
- CLIP: Vision-languageã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
- T5: æ·±ã„æ–‡ç†è§£ï¼ˆè©³ç´°ãªé–¢ä¿‚æ€§ï¼‰
- Cross-Attention: ç”»åƒã®å„ä½ç½®ãŒå˜èªã«æ³¨ç›®

**4. FLUXã®é©æ–°**
- Rectified Flow: ç›´ç·šçš„è¼¸é€ â†’ 20 steps
- Transformer: é•·è·é›¢ä¾å­˜ã‚’åŠ¹ç‡çš„ã«
- CLIP+T5 dual: ä¸¡æ–¹ã®å¼·ã¿ã‚’çµ±åˆ

### FAQ

<details><summary>Q1: VAEã®å“è³ªåŠ£åŒ–ã¯ãªã„ã®ã‹ï¼Ÿ</summary>

**A**: KL-reg VAEã¯å†æ§‹æˆå“è³ªã‚’é‡è¦–ã™ã‚‹ãŸã‚ã€çŸ¥è¦šçš„ã«ã¯åŠ£åŒ–ãŒå°‘ãªã„ã€‚$\beta < 1$ ã®Î²-VAEã‚„VQ-VAEã§ã•ã‚‰ã«æ”¹å–„å¯èƒ½ã€‚LPIPSæå¤±ã§äººé–“ã®çŸ¥è¦šã«åˆã‚ã›ã‚‹ã€‚

</details>

<details><summary>Q2: CFG scale $w$ ã®æœ€é©å€¤ã¯ï¼Ÿ</summary>

**A**: ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚Stable Diffusion 1.xã¯7.5ãŒæ¨™æº–ã ãŒã€å†™å®Ÿçš„ãªå†™çœŸã¯3-5ã€ã‚¢ãƒ¼ãƒˆã¯10-15ã‚‚ä½¿ã‚ã‚Œã‚‹ã€‚FID/IS/CLIP Scoreã§å®Ÿé¨“çš„ã«æ±ºå®šã€‚

</details>

<details><summary>Q3: Negative Promptã¯å¿…é ˆï¼Ÿ</summary>

**A**: å¿…é ˆã§ã¯ãªã„ãŒã€å“è³ªå‘ä¸Šã«æœ‰åŠ¹ã€‚"blurry, low quality"ç­‰ã§ä½å“è³ªã‚µãƒ³ãƒ—ãƒ«ã‚’å›é¿ã€‚ç„¡æ¡ä»¶ $\emptyset$ ã¨ã®ä¸­é–“çš„ãªå½¹å‰²ã€‚

</details>

<details><summary>Q4: FLUXã¯SD 3ã‚ˆã‚Šä½•ãŒå„ªã‚Œã‚‹ï¼Ÿ</summary>

**A**: (1) Transformer backbone â†’ U-Netã‚ˆã‚Šè¡¨ç¾åŠ›ã€(2) Rectified Flow â†’ 20 stepsã§åæŸã€(3) 12B params â†’ ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ã€‚FID 7.2 (SD3: 8.7)ã€‚

</details>

<details><summary>Q5: LDMã®è¨“ç·´ã‚³ã‚¹ãƒˆã¯ï¼Ÿ</summary>

**A**: SD 1.5è¦æ¨¡ï¼ˆ860M paramsï¼‰ã§ã€V100Ã—8ã§ç´„2é€±é–“ã€‚FLUX.1 (12B)ã¯A100Ã—256ã§æ•°é€±é–“ã¨æ¨å®šã€‚VAEäº‹å‰è¨“ç·´å«ã‚ã‚‹ã¨+1é€±é–“ã€‚

</details>

### å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (1é€±é–“ãƒ—ãƒ©ãƒ³)

| Day | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ | ç¢ºèª |
|:----|:-------|:-----|:-----|
| **1** | Zone 0-2 èª­äº† + VAEå¾©ç¿’ | 2h | ãªãœæ½œåœ¨ç©ºé–“ã‹èª¬æ˜ã§ãã‚‹ |
| **2** | Zone 3.1-3.5 CFGå°å‡º | 3h | CFGå¼ã‚’3è¦–ç‚¹ã‹ã‚‰å°å‡º |
| **3** | Zone 3.6-3.13 ç†è«–å®Œèµ° | 3h | FLUX architectureã‚’èª¬æ˜ |
| **4** | Zone 4 Juliaå®Ÿè£… | 4h | Mini LDMè¨“ç·´æˆåŠŸ |
| **5** | Zone 5 CFGå®Ÿé¨“ | 3h | Guidance scaleæƒå¼•å®Œäº† |
| **6** | Rustæ¨è«–å®Ÿè£… | 3h | ONNXæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ |
| **7** | ç·å¾©ç¿’ + Bosså†æŒ‘æˆ¦ | 2h | CFGå®Œå…¨åˆ†è§£ã‚’å†å°å‡º |

### é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```julia
# è‡ªå·±è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
progress = Dict(
    "VAEåœ§ç¸®ç†è«–" => false,
    "CFGå®Œå…¨å°å‡º" => false,
    "Cross-Attentionå®Ÿè£…" => false,
    "FLUXç†è§£" => false,
    "Juliaè¨“ç·´æˆåŠŸ" => false,
    "Rustæ¨è«–æˆåŠŸ" => false,
    "CFGå®Ÿé¨“å®Œäº†" => false
)

# å„é …ç›®ã‚’trueã«ã—ã¦é€²æ—ç¢ºèª
completed = count(values(progress))
total = length(progress)
println("Progress: $completed / $total ($(round(100*completed/total, digits=1))%)")

if completed == total
    println("ğŸ‰ ç¬¬39å›å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ï¼")
end
```

### æ¬¡å›äºˆå‘Š: ç¬¬40å› Consistency Models & é«˜é€Ÿç”Ÿæˆç†è«–

**ãƒ†ãƒ¼ãƒ**: 1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆç†è«–ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«é«˜é€ŸåŒ–æœ€å‰ç·šã€‚

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**:
- Consistency Models (Song et al. 2023)
- Self-consistencyæ¡ä»¶
- Consistency Training (CT) / Distillation (CD)
- Pseudo-Huber loss (ICLR 2025)
- Progressive Distillation
- Latent Consistency Models (LCM)
- DPM-Solver++ / UniPC / EDM
- Rectified Flowè’¸ç•™
- Adversarial Post-Training (DMD2)

**äºˆç¿’èª²é¡Œ**:
- ç¬¬36å›ã®DDIMã‚’å¾©ç¿’ï¼ˆæ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- ç¬¬38å›ã®Rectified Flowã‚’å¾©ç¿’ï¼ˆç›´ç·šçš„è¼¸é€ï¼‰

**åˆ°é”ç›®æ¨™**: ã€Œ1000ã‚¹ãƒ†ãƒƒãƒ—â†’1ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®ç†è«–çš„æ©‹æ¸¡ã—ã‚’å®Œå…¨ç†è§£ã€

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®100%å®Œäº†ï¼ ğŸ‰**
>
> ç¬¬39å›èª­äº†ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼
>
> **ç²å¾—ã‚¹ã‚­ãƒ«**:
> - âœ… Latent Diffusionã®ç†è«–çš„åŸºç›¤
> - âœ… Classifier-Free Guidanceã®å®Œå…¨å°å‡º
> - âœ… Text Conditioningã®å®Ÿè£…ãƒ¬ãƒ™ãƒ«ç†è§£
> - âœ… FLUX Architectureè§£æ
> - âœ… Juliaè¨“ç·´ + Rustæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
> - âœ… CFGå®Ÿé¨“ã«ã‚ˆã‚‹å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç†è§£
>
> Course IVã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã¯æ®‹ã‚Š3å›ï¼ˆL40-42ï¼‰ã€‚ç†è«–ã®å®Œæˆã¾ã§ã‚ã¨å°‘ã—ï¼

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**å•ã„**: ã€ŒStable Diffusionã®çŸ¥è­˜ã¯2026å¹´æ™‚ç‚¹ã§æ—¢ã«é™³è…åŒ–ã—ã¦ã„ã‚‹ã®ã§ã¯ï¼Ÿã€

**è€ƒå¯Ÿãƒã‚¤ãƒ³ãƒˆ**:

1. **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é€²åŒ–**: U-Net (SD 1.x/2.x/XL) â†’ Transformer (FLUX/Lumina) â†’ ??? (2027)
2. **Flowçµ±åˆ**: DDPM (SD 1.x-2.x) â†’ Rectified Flow (SD 3/FLUX) â†’ ã•ã‚‰ãªã‚‹æœ€é©åŒ–
3. **Text Encoder**: CLIPå˜ä½“ â†’ CLIP+T5 â†’ T5å˜ä½“ â†’ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMçµ±åˆï¼Ÿ
4. **è¨“ç·´ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ **: 2æ®µéšï¼ˆVAEâ†’Diffusionï¼‰â†’ End-to-Endï¼Ÿ
5. **1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆ**: Consistency Models / Distillation â†’ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆã®æœªæ¥

**æ­´å²çš„è¦–ç‚¹**:

- 2020: DDPMç™»å ´ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ï¼‰
- 2021: Latent Diffusionï¼ˆæ½œåœ¨ç©ºé–“é©å‘½ï¼‰
- 2022: Stable Diffusion 1.xï¼ˆæ°‘ä¸»åŒ–ï¼‰
- 2023: SDXLï¼ˆå“è³ªå‘ä¸Šï¼‰
- 2024: SD 3ï¼ˆRectified Flowçµ±åˆï¼‰
- 2025: FLUX.1ï¼ˆTransformerå®Œå…¨ç§»è¡Œï¼‰
- 2026: ??? ï¼ˆæ¬¡ã®é©æ–°ã¯ï¼Ÿï¼‰

**å•ã„ç›´ã—**: SD 1.5ã®ã€Œç†è«–ã€ã¯é™³è…åŒ–ã—ãŸã‹ã€ãã‚Œã¨ã‚‚ã€Œå®Ÿè£…ã€ã ã‘ãŒé™³è…åŒ–ã—ãŸã‹ï¼Ÿ

<details><summary>è­°è«–ã®ç¨®</summary>

**ç«‹å ´1: ç†è«–ã¯ä¸å¤‰**
- VAEåœ§ç¸®ã®åŸç†ã¯å¤‰ã‚ã‚‰ãªã„
- CFGã®æ•°å­¦ã¯æ™®éçš„
- DDPMã®ç†è«–ã¯Rectified Flowã®åŸºç›¤

**ç«‹å ´2: å®Ÿè£…ã¯é™³è…åŒ–**
- U-Netã¯éå»ã®éºç‰©
- CLIPå˜ä½“ã¯ä¸ååˆ†
- 50 stepsã¯é…ã™ãã‚‹

**çµ±åˆè¦–ç‚¹**: ç†è«–ã‚’ç†è§£ã—ãŸä¸Šã§æœ€æ–°å®Ÿè£…ã«é©å¿œã™ã‚‹ã“ã¨ãŒã€ŒçœŸã®ç†è§£ã€ã§ã¯ï¼Ÿ

**å•ã„**: FLUXã‚’å­¦ã¹ã°ååˆ†ã‹ã€ãã‚Œã¨ã‚‚SD 1.5ã‹ã‚‰ç©ã¿ä¸Šã’ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ï¼Ÿ

</details>

> **Progress: 95%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Latent Consistency Models (LCM) ãŒ LDM + Consistency Distillation ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ç†ç”±ï¼šæ½œåœ¨ç©ºé–“ã§ã® Self-consistency æ¡ä»¶ $f_\theta(z_t,t,c) = f_\theta(z_{t'},t',c)$ ã‚’ã©ã†è¨“ç·´ã§å¼·åˆ¶ã™ã‚‹ã‹èª¬æ˜ã›ã‚ˆã€‚
> 2. SD 3.x / FLUX ç³»ãŒæ¡ç”¨ã™ã‚‹ v-prediction ãƒ‘ãƒ©ãƒ¡ã‚¿ãƒ©ã‚¤ã‚º $v_t = \alpha_t \epsilon - \sigma_t x_0$ ã®ãƒ¡ãƒªãƒƒãƒˆã‚’ã€$\epsilon$-prediction ã¨ã®å­¦ç¿’å®‰å®šæ€§ã®é•ã„ã§èª¬æ˜ã›ã‚ˆã€‚

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^ldm]: Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. *CVPR 2022*.
<https://arxiv.org/abs/2112.10752>

[^cfg]: Ho, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance. *arXiv:2207.12598*.
<https://arxiv.org/abs/2207.12598>

[^classifier_guidance]: Dhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. *NeurIPS 2021*.
<https://arxiv.org/abs/2105.05233>

[^flux]: Greenberg, O. (2025). Demystifying Flux Architecture. *arXiv:2507.09595*.
<https://arxiv.org/abs/2507.09595>

[^min_snr]: Hang, T., Gu, S., Li, C., et al. (2023). Efficient Diffusion Training via Min-SNR Weighting Strategy. *ICCV 2023*.
<https://arxiv.org/abs/2303.09556>

[^zero_snr]: Lin, S., et al. (2023). Common Diffusion Noise Schedules and Sample Steps are Flawed. *arXiv:2305.08891*.
<https://arxiv.org/abs/2305.08891>

[^clip]: Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. *ICML 2021*.
<https://arxiv.org/abs/2103.00020>

[^t5]: Raffel, C., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *JMLR*.
<https://arxiv.org/abs/1910.10683>

[^imagen]: Saharia, C., et al. (2022). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. *NeurIPS 2022*.
<https://arxiv.org/abs/2205.11487>

[^sdxl]: Podell, D., et al. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. *arXiv:2307.01952*.
<https://arxiv.org/abs/2307.01952>

[^sd3]: Esser, P., et al. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. *arXiv:2403.03206*.
<https://arxiv.org/abs/2403.03206>

[^lcm]: Luo, S., et al. (2023). Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference. ICLR 2024. arXiv:2310.04378.
<https://arxiv.org/abs/2310.04378>

[^efficient_survey]: Shen, H., et al. (2025). "Efficient Diffusion Models: A Survey". *Transactions on Machine Learning Research (TMLR)*. arXiv:2502.06805.
   https://arxiv.org/abs/2502.06805

---

## 7. æœ€æ–°ç ”ç©¶å‹•å‘ï¼ˆ2024-2025ï¼‰

### 7.1 SDXL: Stable Diffusion XL ã®æ”¹å–„ç‚¹

Podell et al. (2023) [^sdxl] ã¯ã€Stable Diffusion v1.5 ã‚’å¤§å¹…ã«å¼·åŒ–ã—ãŸ **SDXL** ã‚’ç™ºè¡¨ã€‚ä¸»ãªæ”¹å–„:

#### 7.1.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ‹¡å¼µ

**U-Net ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—**:

| Component | SD v1.5 | SDXL | å€ç‡ |
|:----------|:--------|:-----|:-----|
| U-Net params | 860M | **2.6B** | 3.0Ã— |
| Cross-attention layers | 16 | **70** | 4.4Ã— |
| Attention heads | 8 | **20** | 2.5Ã— |

**2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**:

1. **CLIP ViT-L/14**: SD v1.5 ã¨åŒã˜ï¼ˆ768æ¬¡å…ƒï¼‰
2. **OpenCLIP ViT-bigG**: æ–°è¦è¿½åŠ ï¼ˆ1280æ¬¡å…ƒï¼‰

çµåˆæ–¹æ³•:

$$
c_\text{text} = \text{Concat}(\text{CLIP}(T), \text{OpenCLIP}(T)) \in \mathbb{R}^{2048}
$$

**Micro-conditioning**:

ç”»åƒã®å…ƒã‚µã‚¤ã‚º $H_\text{orig} \times W_\text{orig}$ ã¨ã‚¯ãƒ­ãƒƒãƒ—åº§æ¨™ $(y_\text{crop}, x_\text{crop})$ ã‚’è¿½åŠ æ¡ä»¶ã¨ã—ã¦åŸ‹ã‚è¾¼ã‚€:

$$
c_\text{size} = \text{MLP}([H_\text{orig}, W_\text{orig}, y_\text{crop}, x_\text{crop}])
$$

Cross-attention ã®éš›ã« $c_\text{text} + c_\text{size}$ ã‚’ä½¿ç”¨ã€‚

**åŠ¹æœ**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®è§£åƒåº¦ãƒ»ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸› â†’ ä½è§£åƒåº¦ç”»åƒã®ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«æ™‚ã® artifacts å‰Šæ¸›ã€‚

#### 7.1.2 è¨“ç·´æˆ¦ç•¥

**Multi-aspect ratio training**:

ãƒã‚±ãƒƒãƒˆåŒ–: $[(512, 512), (768, 512), (512, 768), (1024, 1024), \ldots]$

å„ãƒãƒƒãƒã§ç•°ãªã‚‹è§£åƒåº¦ã‚’æ··åœ¨ â†’ VAE ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ã‚µã‚¤ã‚ºã‚‚å¯å¤‰ã€‚

**Two-stage training**:

1. **Base Model** (2.6B params): 256Ã—256 â†’ 512Ã—512 â†’ 1024Ã—1024 ã‚’æ®µéšçš„ã«ã€‚
2. **Refiner Model** (2.6B params): Base ã®å‡ºåŠ›ã‚’å…¥åŠ›ã¨ã—ã¦ã€é«˜å‘¨æ³¢ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ã‚’è¿½åŠ ã€‚

Refiner ã¯ **$t \in [0, 200]$** ï¼ˆä½ãƒã‚¤ã‚ºé ˜åŸŸï¼‰ã®ã¿è¨“ç·´ â†’ Base ãŒç”Ÿæˆã—ãŸæ§‹é€ ã‚’å£Šã•ãšã€è³ªæ„Ÿãƒ»ã‚¨ãƒƒã‚¸ã‚’æ´—ç·´ã€‚

**Pipeline**:

```
Text â†’ Base Model (t=1000 â†’ t=200) â†’ Refiner (t=200 â†’ t=0) â†’ Image
```

#### 7.1.3 å®Ÿé¨“çµæœ

| Model | Resolution | FID â†“ | CLIP Score â†‘ | User Preference |
|:------|:-----------|:------|:-------------|:----------------|
| SD v1.5 | 512Ã—512 | 18.3 | 0.304 | 28% |
| SD v2.1 | 768Ã—768 | 15.7 | 0.312 | 35% |
| **SDXL** | 1024Ã—1024 | **9.55** | **0.329** | **68%** |
| Midjourney v5 | 1024Ã—1024 | - | - | 32% |

**äººé–“è©•ä¾¡**: SDXL ã¯ Midjourney v5 ã«å¯¾ã—ã¦ 68% ã®å‹ç‡ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¿ å®Ÿåº¦ + ç¾çš„å“è³ªï¼‰ã€‚

**æ¨è«–æ™‚é–“** (A100):

- Base: ~3.5s (50 DDIM steps)
- Refiner: ~1.5s (20 steps)
- **åˆè¨ˆ**: ~5sï¼ˆSD v1.5 ã® 2å€ã ãŒã€å“è³ªã¯åœ§å€’çš„å‘ä¸Šï¼‰

### 7.2 Latent Consistency Models (LCM)

Luo et al. (2023) [^lcm] ã¯ã€**Consistency Distillation ã‚’æ½œåœ¨ç©ºé–“ã«é©ç”¨**ã—ã€**2-4ã‚¹ãƒ†ãƒƒãƒ—**ã§é«˜å“è³ªç”Ÿæˆã‚’å®Ÿç¾ã€‚

#### 7.2.1 Consistency Models ã®å¾©ç¿’

Song et al. (2023) ã® Consistency Models ã¯ã€ODE ã®ä»»æ„æ™‚åˆ»ã‹ã‚‰ã®è»Œé“ãŒåŒã˜çµ‚ç‚¹ã«åæŸã™ã‚‹æ€§è³ªã‚’åˆ©ç”¨:

$$
f_\theta(x_t, t) = x_0 \quad \forall t \in [0, T]
$$

è¨“ç·´: **Self-consistency**

$$
\mathcal{L}_\text{CM} = \mathbb{E}_{t, x_0} \left[ \| f_\theta(x_t, t) - \text{sg}[f_\theta(x_{t+\Delta t}, t+\Delta t)] \|^2 \right]
$$

$\text{sg}[\cdot]$ ã¯ stop-gradientã€‚

#### 7.2.2 LCM ã®æ‹¡å¼µ

**å•é¡Œ**: Pixel-space Consistency Models ã¯é«˜è§£åƒåº¦ã§ä¸å®‰å®šï¼ˆ1024Ã—1024 ã§ç™ºæ•£ï¼‰ã€‚

**è§£æ±º**: **Latent space** ã§ consistency ã‚’å­¦ç¿’:

$$
f_\theta(z_t, t, c) = z_0
$$

ã“ã“ã§ $z_t = \mathcal{E}(x_t)$ ï¼ˆVAE æ½œåœ¨è¡¨ç¾ï¼‰ã€‚

**Distillation from pre-trained LDM**:

æ•™å¸«ãƒ¢ãƒ‡ãƒ«: SDXL, Stable Diffusion v1.5 ç­‰

$$
\mathcal{L}_\text{LCM} = \mathbb{E}_{t, z_0, c} \left[ w(t) \| f_\theta(z_t, t, c) - \hat{z}_0(z_t, t, c) \|^2 \right]
$$

ã“ã“ã§ $\hat{z}_0$ ã¯æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã® DDIM 1-step äºˆæ¸¬:

$$
\hat{z}_0 = \frac{z_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(z_t, t, c)}{\sqrt{\bar{\alpha}_t}}
$$

é‡ã¿:

$$
w(t) = \frac{1}{\sqrt{\bar{\alpha}_t (1 - \bar{\alpha}_t)}}
$$

#### 7.2.3 å®Ÿè£…ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ

**Classifier-Free Guidance in Distillation**:

æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã® CFG å‡ºåŠ›ã‚’è’¸ç•™:

$$
\tilde{\epsilon}_\theta = \epsilon_\theta(z_t, t, \emptyset) + w \cdot (\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset))
$$

LCM ã¯ **å˜ä¸€ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹**ã§ $w$ ã‚’åŸ‹ã‚è¾¼ã¿:

$$
f_\theta(z_t, t, c, w)
$$

$w$ ã‚’å…¥åŠ›ã«è¿½åŠ  â†’ æ¨è«–æ™‚ã« guidance ã‚’å¤‰æ›´å¯èƒ½ï¼ˆå†è¨“ç·´ä¸è¦ï¼‰ã€‚

**Sampling**:

```julia
function lcm_sample(z_T, c, w_cfg, steps=4)
    z = z_T
    Î”t = 1.0 / steps

    for i in steps:-1:1
        t = i * Î”t
        # Single function evaluation
        zâ‚€_pred = f_Î¸(z, t, c, w_cfg)

        if i > 1
            # Add noise for next step
            t_prev = (i-1) * Î”t
            z = sqrt(á¾±[t_prev]) * zâ‚€_pred + sqrt(1 - á¾±[t_prev]) * randn(size(z))
        else
            z = zâ‚€_pred
        end
    end

    return z
end
```

#### 7.2.4 çµæœ

**COCO-2014 256Ã—256** (SD v1.5 ãƒ™ãƒ¼ã‚¹):

| Model | Steps | FID â†“ | CLIP Score â†‘ | Time (A100) |
|:------|:------|:------|:-------------|:------------|
| SD v1.5 DDIM | 50 | 12.8 | 0.304 | 2.5s |
| SD v1.5 DDIM | 10 | 18.3 | 0.289 | 0.5s |
| **LCM** | **4** | **13.9** | **0.301** | **0.2s** |
| **LCM** | **2** | **16.2** | **0.295** | **0.1s** |

**4ã‚¹ãƒ†ãƒƒãƒ—ã§ 50ã‚¹ãƒ†ãƒƒãƒ— DDIM ã«åŒ¹æ•µã€12.5å€é«˜é€ŸåŒ–**ã€‚

**è¨“ç·´ã‚³ã‚¹ãƒˆ**: A100 8å°ã§ **32æ™‚é–“** â€” SD v1.5 ã®å®Œå…¨è¨“ç·´ï¼ˆæ•°åƒGPUæ—¥ï¼‰ã«æ¯”ã¹æ¥µã‚ã¦åŠ¹ç‡çš„ã€‚

### 7.3 Efficient Diffusion Models Survey (TMLR 2025)

Li et al. (2025) [^efficient_survey] ã®ã‚µãƒ¼ãƒ™ã‚¤ã‹ã‚‰é‡è¦ãªæŠ€è¡“ã‚’æŠœç²‹ã€‚

#### 7.3.1 Sampling é«˜é€ŸåŒ–ã®åˆ†é¡

**1. Truncated Sampling**:

æ—©æœŸåœæ­¢: $t \in [T_\text{stop}, T]$ ã®ã¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€$t < T_\text{stop}$ ã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚

ä¾‹: $T=1000$, $T_\text{stop}=200$ â†’ 80% å‰Šæ¸›ã€‚

å“è³ªåŠ£åŒ–ã‚’æœ€å°åŒ–ã™ã‚‹ $T_\text{stop}$ ã®é¸æŠåŸºæº–:

$$
T_\text{stop} = \arg\min_t \text{SNR}(t) > \tau
$$

$\tau \approx 0.1$ ãŒçµŒé¨“çš„ã«è‰¯ã„ï¼ˆImageNet ã§ã®å®Ÿé¨“ï¼‰ã€‚

**2. Knowledge Distillation**:

- **Progressive Distillation** (Salimans & Ho, 2022): 1000ã‚¹ãƒ†ãƒƒãƒ— â†’ 500 â†’ 250 â†’ ... â†’ 4ã‚¹ãƒ†ãƒƒãƒ—ã€‚å„æ®µéšã§å‰æ®µéšã‚’æ•™å¸«ã«ã€‚
- **Consistency Distillation**: å‰è¿°ã® LCMã€‚
- **Guided Distillation**: CFG ã®é‡ã¿ $w$ ã‚‚è’¸ç•™ã€‚

**3. Fast ODE Solvers**:

- **DPM-Solver** (Lu et al., 2022): æŒ‡æ•°ç©åˆ†ã«åŸºã¥ãé«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ â†’ 10-20ã‚¹ãƒ†ãƒƒãƒ—ã§ DDIM 50ã‚¹ãƒ†ãƒƒãƒ—ã«åŒ¹æ•µã€‚
- **UniPC** (Zhao et al., 2023): Predictor-Corrector æ³• â†’ ã•ã‚‰ã« 5-10ã‚¹ãƒ†ãƒƒãƒ—ã€‚

#### 7.3.2 ãƒ¢ãƒ‡ãƒ«åœ§ç¸®

**é‡å­åŒ–**:

| Method | Precision | FID Degradation | Speedup |
|:-------|:----------|:----------------|:--------|
| FP32 (baseline) | 32-bit | 0.0 | 1.0Ã— |
| FP16 | 16-bit | +0.2 | 1.3Ã— |
| INT8 (PTQ) | 8-bit | +0.8 | 2.1Ã— |
| **Q-Diffusion** | 4-bit | +1.2 | **3.5Ã—** |

Q-Diffusion (Li et al., 2023): ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥é‡å­åŒ– â€” $t$ å¤§ãã„ï¼ˆé«˜ãƒã‚¤ã‚ºï¼‰â†’ ä½ç²¾åº¦OKã€$t$ å°ã•ã„ â†’ é«˜ç²¾åº¦å¿…è¦ã€‚

**ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°**:

Structured pruning: Attention head ã‚’å‰Šé™¤ã€‚

SD v1.5: 70 Attention layers â†’ 50 layersï¼ˆ28%å‰Šæ¸›ï¼‰ã€FID +1.5ã€‚

#### 7.3.3 ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–

**Flash Attention** (Dao et al., 2022):

Self-attention ã®è¨ˆç®—é‡ $O(N^2)$ ã‚’ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã« â†’ GPU ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ 2-3å€ã€‚

SD v1.5 + Flash Attention: 2.5s â†’ **1.7s** (A100)ã€‚

**Tensor Core æœ€é©åŒ–**:

FP16 mixed precision + Tensor Core â†’ è¿½åŠ ã® 1.5Ã— é«˜é€ŸåŒ–ã€‚

**Result**: SD v1.5 ã‚’ **1ç§’ä»¥ä¸‹**ï¼ˆ16ã‚¹ãƒ†ãƒƒãƒ— DDIM + Flash Attention + FP16 + DPM-Solverï¼‰ã§æ¨è«–å¯èƒ½ï¼ˆA100ï¼‰ã€‚

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
