---
title: "ç¬¬39å›: Latent Diffusion Models: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ–¼ï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "ldm", "rust", "stablediffusion"]
published: true
slug: "ml-lecture-39-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

**â†’ å‰ç·¨ï¼ˆç†è«–ç·¨ï¼‰**: [ml-lecture-39-part1](./ml-lecture-39-part1)

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” LDMè¨“ç·´â†’æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### ç’°å¢ƒæ§‹ç¯‰ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

**Pythonç’°å¢ƒ** (ğŸè¨“ç·´):
```bash
pip install torch torchvision safetensors
```

**Rustç’°å¢ƒ** (ğŸ¦€æ¨è«–):
```toml
[dependencies]
tch = "0.17"
safetensors = "0.4"
```

### LaTeXè¨˜æ³•ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

| è¨˜å· | LaTeX | æ„å‘³ |
|:-----|:------|:-----|
| $\mathcal{E}$ | `\mathcal{E}` | Encoder |
| $\mathcal{D}$ | `\mathcal{D}` | Decoder |
| $\bar{\alpha}_t$ | `\bar{\alpha}_t` | Cumulative product |
| $\epsilon_\theta(z_t, t, c)$ | `\epsilon_\theta(z_t, t, c)` | Noise prediction |
| $\tilde{\epsilon}$ | `\tilde{\epsilon}` | Modified noise (CFG) |
| $\mathbb{E}_{q(z\|x)}[\cdot]$ | `\mathbb{E}_{q(z\|x)}[\cdot]` | Expectation |
| $\text{KL}[q \| p]$ | `\text{KL}[q \| p]` | KL divergence |

### Mathâ†’Codeç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ (LDMç·¨)

**ãƒ‘ã‚¿ãƒ¼ãƒ³1: VAE Encoder**
$$
z = \mathcal{E}(x), \quad x \in \mathbb{R}^{H \times W \times C} \to z \in \mathbb{R}^{h \times w \times c}
$$

```rust
// x: [B, C, H, W] â†’ z: [B, c, h, w]
let z = encoder.forward(&x)?;
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³2: Forward Diffusion**
$$
z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
$$

```rust
let eps = Tensor::randn(0f32, 1.0, z0.shape(), z0.device())?;
let z_t = (z0 * alpha_bar[t].sqrt())?.add(&(eps * (1.0 - alpha_bar[t]).sqrt())?)?;
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³3: CFG**
$$
\tilde{\epsilon}_\theta = \epsilon_\theta(z_t, t, \emptyset) + w \cdot (\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset))
$$

```rust
let eps_uncond = unet.forward(&z_t, t, None)?;
let eps_cond   = unet.forward(&z_t, t, Some(&c))?;
// ÎµÌƒ = Îµ_uncond + w * (Îµ_cond âˆ’ Îµ_uncond)
let eps_cfg = eps_uncond.add(&(eps_cond.sub(&eps_uncond)?.mul(w)?)?)?;
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³4: DDIM Sampling**
$$
z_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \left( \frac{z_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1-\bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_\theta + \sigma_t \epsilon
$$

```rust
// DDIM step: predict xâ‚€, then interpolate toward z_{t-1}
let pred_x0 = (z_t.sub(&(eps_theta.mul((1.0 - alpha_bar[t]).sqrt())?)?)?.div((alpha_bar[t].sqrt()))?;
let dir_z    = eps_theta.mul((1.0 - alpha_bar[t - 1] - sigma * sigma).sqrt())?;
let noise    = Tensor::randn(0f32, 1.0, z_t.shape(), z_t.device())?.mul(sigma)?;
let z_prev   = (pred_x0.mul(alpha_bar[t - 1].sqrt())?.add(&dir_z)?.add(&noise)?;
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³5: Cross-Attention**
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

```rust
// scores: [N_q, N_k]
let scale = (d_k as f64).sqrt() as f32;
let scores = q.matmul(&k.t()?)?.mul(1.0 / scale)?;
// attn:   [N_q, N_k]
let attn = scores.softmax(-1, tch::Kind::Float);
// out:    [N_q, d_v]
let out  = attn.matmul(&v)?;
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³6: Min-SNR Weighting**
$$
w(t) = \min\left(\text{SNR}(t), \gamma\right), \quad \text{SNR}(t) = \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}
$$

```rust
// snr[t] = Î±Ì„_t / (1 âˆ’ Î±Ì„_t),  clipped at Î³
let snr: Vec<f32> = alpha_bar.iter().map(|&a| a / (1.0 - a)).collect();
let weight: Vec<f32> = snr.iter().map(|&s| s.min(gamma)).collect();
let loss = mse(&eps_pred, &eps_true)? * weight[t];
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³7: Zero Terminal SNR Rescaling**
$$
\tilde{\alpha}_t = \frac{\alpha_t}{\alpha_T}
$$

```rust
// Cumulative product: á¾±_t = âˆ_{s=1}^{t} Î±_s
let alpha_cumprod: Vec<f32> = alphas.iter()
    .scan(1.0f32, |acc, &a| { *acc *= a; Some(*acc) })
    .collect();
let last = *alpha_cumprod.last().unwrap();
let alpha_cumprod_rescaled: Vec<f32> = alpha_cumprod.iter().map(|&a| a / last).collect();
```

### ğŸ¦€ Rustå®Œå…¨å®Ÿè£…: Mini Latent Diffusion

**ã‚¹ãƒ†ãƒƒãƒ—1: VAEå®šç¾©**

```python
import torch
import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self, in_ch: int, latent_ch: int, base_ch: int):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch,       base_ch,     3, padding=1)
        self.conv2 = nn.Conv2d(base_ch,     base_ch*2,   4, stride=2, padding=1)
        self.conv3 = nn.Conv2d(base_ch*2,   base_ch*4,   4, stride=2, padding=1)
        self.conv4 = nn.Conv2d(base_ch*4,   base_ch*8,   4, stride=2, padding=1)
        self.conv5 = nn.Conv2d(base_ch*8,   latent_ch,   3, padding=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv1(x).relu()
        x = self.conv2(x).relu()
        x = self.conv3(x).relu()
        x = self.conv4(x).relu()
        return self.conv5(x)   # z, no activation

class Decoder(nn.Module):
    def __init__(self, latent_ch: int, out_ch: int, base_ch: int):
        super().__init__()
        self.conv1   = nn.Conv2d(latent_ch,   base_ch*8,   3, padding=1)
        self.deconv1 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 4, stride=2, padding=1)
        self.deconv2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 4, stride=2, padding=1)
        self.deconv3 = nn.ConvTranspose2d(base_ch*2, base_ch,   4, stride=2, padding=1)
        self.conv2   = nn.Conv2d(base_ch,     out_ch,      3, padding=1)

    def forward(self, z: torch.Tensor) -> torch.Tensor:
        x = self.conv1(z).relu()
        x = self.deconv1(x).relu()
        x = self.deconv2(x).relu()
        x = self.deconv3(x).relu()
        return self.conv2(x).tanh()

def train_vae(
    encoder: Encoder,
    decoder: Decoder,
    dataloader: torch.utils.data.DataLoader,
    epochs: int,
    beta: float,
    opt: torch.optim.Optimizer,
) -> None:
    for epoch in range(epochs):
        total_loss = 0.0
        for x in dataloader:
            z       = encoder(x)
            x_recon = decoder(z)
            recon_loss = (x_recon - x).pow(2).mean()
            kl_loss    = z.pow(2).mean() * 0.5
            loss       = recon_loss + beta * kl_loss
            opt.zero_grad(set_to_none=True)
            loss.backward()
            opt.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}: Loss = {total_loss / len(dataloader):.4f}")
```

**ã‚¹ãƒ†ãƒƒãƒ—2: U-Netå®šç¾© (Simplified)**

```python
class ResBlock(nn.Module):
    def __init__(self, ch: int):
        super().__init__()
        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1)
        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        h = self.conv1(x).relu()
        h = self.conv2(h)
        return (h + x).relu()    # residual

class UNet(nn.Module):
    def __init__(self, latent_ch: int, base_ch: int, time_emb_dim: int):
        super().__init__()
        self.time_mlp = nn.Sequential(
            nn.Linear(time_emb_dim, time_emb_dim * 4),
            nn.SiLU(),
            nn.Linear(time_emb_dim * 4, time_emb_dim * 4),
        )
        self.in_proj   = nn.Conv2d(latent_ch, base_ch,    3, padding=1)
        self.res_down1 = ResBlock(base_ch)
        self.down_conv = nn.Conv2d(base_ch,   base_ch*2,  4, stride=2, padding=1)
        self.res_down2 = ResBlock(base_ch * 2)
        self.res_mid   = ResBlock(base_ch * 2)
        self.up_conv   = nn.ConvTranspose2d(base_ch*2, base_ch, 4, stride=2, padding=1)
        self.res_up    = ResBlock(base_ch)
        self.out_proj  = nn.Conv2d(base_ch, latent_ch, 3, padding=1)

    def forward(self, z: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:
        x = self.in_proj(z)
        x = self.res_down1(x)
        x = self.down_conv(x)
        x = self.res_down2(x)
        x = self.res_mid(x)
        x = self.up_conv(x)
        x = self.res_up(x)
        return self.out_proj(x)   # predicted Îµ
```

**ã‚¹ãƒ†ãƒƒãƒ—3: Diffusionè¨“ç·´ãƒ«ãƒ¼ãƒ—**

```python
import torch.nn.functional as F

def train_ldm(
    unet: UNet,
    encoder: Encoder,
    dataloader: torch.utils.data.DataLoader,
    device: torch.device,
    epochs: int,
    big_t: int,
    opt: torch.optim.Optimizer,
) -> None:
    _, alpha_bar = cosine_beta_schedule(big_t, 0.008)
    for epoch in range(epochs):
        total_loss = 0.0
        for x in dataloader:
            x = x.to(device)
            with torch.inference_mode():
                z0 = encoder(x)
            t = torch.randint(0, big_t, (1,)).item()
            z_t, eps_true = forward_diffusion(z0, t, alpha_bar, device)
            t_emb_vec = sinusoidal_embedding(t, 256)
            t_emb = torch.tensor(t_emb_vec, device=device).unsqueeze(0)
            eps_pred = unet(z_t, t_emb)
            loss = F.mse_loss(eps_pred, eps_true)
            opt.zero_grad(set_to_none=True)
            loss.backward()
            opt.step()
            total_loss += loss.item()
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}: Loss = {total_loss / len(dataloader):.4f}")
```

**ã‚¹ãƒ†ãƒƒãƒ—4: CFGã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**

```rust
use anyhow::Result;
use tch::{Tensor, Device, nn};

// DDIM sampling with Classifier-Free Guidance
fn ddim_sample_cfg(
    unet: &UNet,
    decoder: &Decoder,
    z_t_init: Tensor,
    c: Option<&Tensor>,    // text conditioning; None = unconditional
    w: f32,                // CFG guidance scale
    steps: usize,
    eta: f32,              // Î·=0 â†’ deterministic DDIM
    device: &Device,
) -> Result<Tensor> {
    let big_t = 1000usize;
    let (_, alpha_bar) = cosine_beta_schedule(big_t, 0.008);

    // Evenly-spaced timesteps from T-1 down to 0
    let timesteps: Vec<usize> = (0..steps)
        .map(|i| big_t - 1 - i * big_t / steps)
        .collect();

    let mut z = z_t_init;
    for (i, &t) in timesteps.iter().enumerate() {
        let t_emb_vec = sinusoidal_embedding(t, 256);
        let t_emb = Tensor::from_vec(t_emb_vec, &[1, 256], device)?;

        // Unconditional forward pass
        let eps_uncond = unet.forward(&z, &t_emb)?;

        // Conditional forward pass (or reuse uncond if no conditioning)
        let eps_cond = match c {
            Some(_cond) => unet.forward(&z, &t_emb)?, // use cond-aware UNet in real impl
            None        => eps_uncond.clone(),
        };

        // CFG combination: ÎµÌƒ = Îµ_uncond + wÂ·(Îµ_cond âˆ’ Îµ_uncond)
        let eps_cfg = eps_uncond.add(&(eps_cond.sub(&eps_uncond)?.affine(w as f64, 0.0)?)?)?;

        // Predict xâ‚€
        let ab_t = alpha_bar[t];
        let pred_x0 = z.sub(&eps_cfg.affine((1.0 - ab_t).sqrt() as f64, 0.0)?)?
                       .affine(1.0 / ab_t.sqrt() as f64, 0.0)?;

        // DDIM step toward z_{t_prev}
        let t_prev_opt = timesteps.get(i + 1).copied();
        z = if let Some(t_prev) = t_prev_opt {
            let ab_prev = alpha_bar[t_prev];
            let sigma = eta * ((1.0 - ab_prev) / (1.0 - ab_t) * (1.0 - ab_t / ab_prev)).sqrt();
            let dir_z  = eps_cfg.affine((1.0 - ab_prev - sigma * sigma).sqrt() as f64, 0.0)?;
            let noise  = Tensor::randn(0f32, 1.0, z.shape(), device)?.affine(sigma as f64, 0.0)?;
            pred_x0.affine(ab_prev.sqrt() as f64, 0.0)?.add(&dir_z)?.add(&noise)?
        } else {
            pred_x0
        };
    }

    // Decode latent â†’ pixel space
    decoder.forward(&z)
}

// Usage example
fn generate(unet: &UNet, decoder: &Decoder, device: &Device) -> Result<Tensor> {
    let z_t = Tensor::randn(0f32, 1.0, (1, 4, 32, 32), device)?;
    let w = 7.5f32;  // CFG scale
    ddim_sample_cfg(unet, decoder, z_t, None, w, 50, 0.0, device)
}
```

<details><summary>å®Œå…¨ãªè¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ</summary>

```python
# ãƒ‡ãƒ¼ã‚¿æº–å‚™: MNIST 28Ã—28Ã—1, normalized to [-1, 1]
import torchvision
from torchvision import transforms
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
dataset   = torchvision.datasets.MNIST(root="./data", train=True, transform=transform, download=True)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)

device  = torch.device("cuda" if torch.cuda.is_available() else "cpu")
encoder = Encoder(1, 4, 32).to(device)
decoder = Decoder(4, 1, 32).to(device)
unet    = UNet(4, 64, 256).to(device)

opt = torch.optim.AdamW(
    list(encoder.parameters()) + list(decoder.parameters()) + list(unet.parameters())
)

# Stage 1: VAEè¨“ç·´
print("Training VAE...")
train_vae(encoder, decoder, dataloader, 20, 0.5, opt)

# Stage 2: Diffusionè¨“ç·´
print("Training Diffusion...")
train_ldm(unet, encoder, dataloader, device, 100, 1000, opt)

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: 28/4 = 7 (latent spatial size)
print("Generating samples...")
z_t = torch.randn(16, 4, 7, 7, device=device)
x_gen = ddim_sample_cfg(unet, decoder, z_t, None, 1.0, 50, 0.0, device)
```

</details>

### ğŸ¦€ Rustæ¨è«–å®Ÿè£…

**safetensorsã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰**:

```rust
use anyhow::Result;
use tch::{Tensor, Device, nn, Kind};

// VAE Decoder
struct Decoder {
    conv1: nn::Conv2d,
    conv2: nn::ConvTranspose2d,
    // ... more layers
}

impl Decoder {
    fn new(vs: &nn::Path) -> Self {
        let conv1 = nn::conv2d(vs / "conv1", 4, 512, 3, Default::default());
        let conv2 = nn::conv_transpose2d(vs / "conv2", 512, 256, 4, Default::default());
        // ...
        Self { conv1, conv2 }
    }

    fn forward(&self, z: &Tensor) -> Tensor {
        let x = self.conv1.forward(z).relu();
        let x = self.conv2.forward(&x);
        // ...
        x.tanh()
    }
}

// Load weights
fn load_ldm_model(path: &str) -> Result<(UNet, Decoder)> {
    let device = Device::cuda_if_available();
    let mut vs = nn::VarStore::new(device);
    vs.load(path)?;
    let root = vs.root();

    let unet    = UNet::new(&root.sub("unet"));
    let decoder = Decoder::new(&root.sub("decoder"));

    Ok((unet, decoder))
}
```

**CFGæ¨è«–**:

```rust
fn cfg_sample(
    unet: &UNet,
    decoder: &Decoder,
    z_t: Tensor,
    c: Option<&Tensor>,
    w: f32,
    steps: usize,
) -> Result<Tensor> {
    let mut z = z_t;
    let timesteps: Vec<usize> = (0..steps).rev().collect();

    for t in timesteps {
        let t_tensor = Tensor::new(&[t as f32], z.device())?;

        // Unconditional
        let eps_uncond = unet.forward(&z, &t_tensor, None)?;

        // Conditional
        let eps_cond = if let Some(cond) = c {
            unet.forward(&z, &t_tensor, Some(cond))?
        } else {
            eps_uncond.clone()
        };

        // CFG
        let eps_cfg = (eps_uncond + (eps_cond - &eps_uncond)? * w)?;

        // DDIM step
        z = ddim_step(&z, &eps_cfg, t)?;
    }

    // Decode
    decoder.forward(&z)
}
```

**ãƒãƒƒãƒæ¨è«–æœ€é©åŒ–**:

```rust
// ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š
fn batch_generate(
    unet: &UNet,
    decoder: &Decoder,
    batch_size: usize,
    c: &Tensor,
    w: f32,
) -> Result<Vec<Tensor>> {
    // ãƒã‚¤ã‚ºãƒãƒƒãƒç”Ÿæˆ
    let z_T = Tensor::randn(0f32, 1f32, (batch_size, 4, 64, 64), &Device::Cuda(0))?;

    // ãƒãƒƒãƒæ¨è«–
    let x_batch = cfg_sample(unet, decoder, z_T, Some(c), w, 50)?;

    // Split batch
    (0..batch_size)
        .map(|i| x_batch.narrow(0, i, 1))
        .collect::<Result<Vec<_>>>()
}
```

### æ•°å€¤å®‰å®šæ€§ã¨ãƒ‡ãƒãƒƒã‚°

**ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨ãƒ‡ãƒãƒƒã‚°æ–¹æ³•**:

| ã‚¨ãƒ©ãƒ¼ | åŸå›  | è§£æ±ºç­– |
|:-------|:-----|:-------|
| **NaN loss** | å‹¾é…çˆ†ç™º / æ•°å€¤ä¸å®‰å®š | Gradient clipping / learning rateå‰Šæ¸› / Mixed precision |
| **Mode collapse** | CFG scaleé«˜ã™ã | $w \in [1, 7.5]$ ã«åˆ¶é™ |
| **ã¼ã‚„ã‘ãŸç”»åƒ** | VAEéåœ§ç¸® / Î²é«˜ã™ã | $\beta < 1$ / åœ§ç¸®ç‡å‰Šæ¸› |
| **Posterior collapse** | VAEè¨“ç·´ä¸ååˆ† | KL annealing / Free bits |
| **çœŸã£é»’/çœŸã£ç™½ç”»åƒ** | Zero terminal SNRæœªå¯¾å¿œ | Noise schedule rescaling |

**æ•°å€¤å®‰å®šåŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯**:

```rust
// Gradient clipping (tch-rs: opt.clip_grad_norm(max_norm))
fn clip_grad_norm(grads: &mut [Tensor], max_norm: f32) -> Result<()> {
    let total_norm_sq: f32 = grads.iter()
        .map(|g| g.sqr()?.sum_all()?.to_scalar::<f32>())
        .collect::<Result<Vec<_>>>()?
        .iter().sum();
    let total_norm = total_norm_sq.sqrt();
    let clip_coef = (max_norm / (total_norm + 1e-6)).min(1.0);
    for g in grads.iter_mut() {
        *g = g.affine(clip_coef as f64, 0.0)?;
    }
    Ok(())
}

// Mixed precision: use tch::autocast for BF16
// let _guard = tch::autocast(true);  // BF16 autocast
// loss accumulation in F32: loss.to_kind(tch::Kind::Float)
```

**ãƒ‡ãƒãƒƒã‚°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:

```rust
// 1. VAEå†æ§‹æˆç¢ºèª
let z      = encoder.forward(&x)?;
let x_recon = decoder.forward(&z)?;
let recon_err = x.sub(&x_recon)?.abs()?.mean_all()?.to_scalar::<f32>()?;
assert!(recon_err < 0.5, "å†æ§‹æˆèª¤å·®ãŒå¤§ãã™ãã¾ã™: {}", recon_err);

// 2. Forward diffusionç¢ºèª â€” T=1000ã§ã¯ã»ã¼ã‚¬ã‚¦ã‚·ã‚¢ãƒ³
let (z_t, eps) = forward_diffusion(&z, big_t - 1, &alpha_bar, &device)?;
let z_t_mean = z_t.abs()?.mean_all()?.to_scalar::<f32>()?;
debug_assert!((z_t_mean - 1.0).abs() < 0.5, "z_t ã®çµ±è¨ˆãŒç•°å¸¸: {}", z_t_mean);

// 3. Noise predictionç¢ºèª â€” shapeä¸€è‡´
let eps_pred = unet.forward(&z_t, &t_emb)?;
assert_eq!(eps_pred.shape(), eps.shape(), "Îµ_pred shape mismatch");

// 4. CFGç¢ºèª â€” NaNãƒã‚§ãƒƒã‚¯
let eps_cfg = cfg_forward(&unet, &z_t, &t_emb, None, 7.5, &device)?;
let has_nan = eps_cfg.to_vec1::<f32>()?.iter().any(|v| v.is_nan());
assert!(!has_nan, "CFGå‡ºåŠ›ã«NaNãŒå«ã¾ã‚Œã¦ã„ã¾ã™");
```

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®70%å®Œäº†ï¼** å®Ÿè£…ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚Rustè¨“ç·´â†’Rustæ¨è«–ã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§CFGå®Ÿé¨“ã¸ã€‚

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” CFGå®Ÿé¨“ã¨å“è³ªåˆ†æ

### è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

**Q1: CFGã®guidance scale $w$ã®åŠ¹æœ**

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®å‡ºåŠ›ã‚’äºˆæ¸¬ã›ã‚ˆ:
```rust
let w_values = [0.0f32, 1.0, 3.0, 7.5, 15.0];
for w in w_values {
    let x = ddim_sample_cfg(&unet, &decoder, z_t.clone(), None, w, 50, 0.0, &device)?;
    println!("w={}: quality=?, diversity=?", w);
}
```

<details><summary>è§£ç­”</summary>

- $w=0.0$: quality=ä½, diversity=é«˜ (ç„¡æ¡ä»¶ç”Ÿæˆ)
- $w=1.0$: quality=ä¸­, diversity=ä¸­ (æ¨™æº–æ¡ä»¶ä»˜ã)
- $w=3.0$: quality=é«˜, diversity=ä¸­ (è»½ã„CFG)
- $w=7.5$: quality=æœ€é«˜, diversity=ä½ (SDæ¨å¥¨å€¤)
- $w=15.0$: quality=éé£½å’Œ, diversity=æœ€ä½ (over-guidance)

</details>

**Q2: Negative Promptã®æ•°å­¦**

Negative Promptå®Ÿè£…ã®ã“ã®è¡Œã‚’èª¬æ˜ã›ã‚ˆ:
```rust
// ÎµÌƒ = Îµ_neg + wÂ·(Îµ_pos âˆ’ Îµ_neg)
let eps_cfg = eps_neg.add(&(eps_pos.sub(&eps_neg)?.affine(w as f64, 0.0)?)?)?;
```

<details><summary>è§£ç­”</summary>

$\tilde{\epsilon} = \epsilon_\text{neg} + w(\epsilon_\text{pos} - \epsilon_\text{neg})$ ã¯ã€Œ$\epsilon_\text{neg}$ã‹ã‚‰$\epsilon_\text{pos}$ã¸$w$å€å¼·ãç§»å‹•ã€ã‚’æ„å‘³ã™ã‚‹ã€‚ã“ã‚Œã¯ãƒ™ã‚¯ãƒˆãƒ«ã®ç·šå½¢çµåˆã§ã€CFGã®ä¸€èˆ¬åŒ–ã€‚Negative Promptã¯ã€Œé¿ã‘ãŸã„æ¦‚å¿µã€ã‚’$\epsilon_\text{neg}$ã¨ã—ã¦æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ç„¡æ¡ä»¶$\emptyset$ã®ä»£ã‚ã‚Šã«ä½¿ã†ã€‚

</details>

**Q3: Zero Terminal SNRã®åŠ¹æœ**

ä»¥ä¸‹ã®rescalingå‰å¾Œã§ä½•ãŒå¤‰ã‚ã‚‹ã‹:
```rust
// Before: Î±_bar_before[T-1] = 0.01 â‰  0  (signal leaks at final step)
let alpha_bar_before: Vec<f32> = vec![0.99, 0.98, /* â€¦ */ 0.01];

// After rescaling: divide by last element â†’ Î±_bar_after[T-1] = 1.0 â†’ sqrt = 1.0
let last = *alpha_bar_before.last().unwrap();
let alpha_bar_after: Vec<f32> = alpha_bar_before.iter().map(|&a| a / last).collect();
// alpha_bar_after.last() == Some(&1.0)  â†’ pure Gaussian at T
```

<details><summary>è§£ç­”</summary>

Zero Terminal SNRã¯ $\bar{\alpha}_T = 0$ ã‚’å¼·åˆ¶ã™ã‚‹ã€‚Rescalingå‰ã¯ $\bar{\alpha}_T = 0.01 \neq 0$ ãªã®ã§ã€$T$ã‚¹ãƒ†ãƒƒãƒ—ç›®ã§ã‚‚ã‚ãšã‹ã«ä¿¡å·ãŒæ®‹ã‚‹ã€‚Rescalingå¾Œã¯ $\bar{\alpha}_T = 0$ ã¨ãªã‚Šã€å®Œå…¨ãªã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã«åˆ°é”ã€‚ã“ã‚Œã«ã‚ˆã‚Šéå¸¸ã«æ˜ã‚‹ã„/æš—ã„ç”»åƒã®ç”Ÿæˆå“è³ªãŒå‘ä¸Šã™ã‚‹ï¼ˆLin et al. 2023 [^zero_snr]ï¼‰ã€‚

</details>

**Q4: Text Conditioningå®Ÿè£…**

CLIP text encodingã§ã¯ `hidden = transformer(tokens)` ã§å…¨77ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹ï¼ˆshape: `[77, 768]`ï¼‰ã‚’å–å¾—ã—ã€`c = hidden` ã¨ã—ã¦ãã®ã¾ã¾ä½¿ã†ã€‚

ãªãœ`hidden[0]` (CLSãƒˆãƒ¼ã‚¯ãƒ³)ã ã‘ã§ãªãå…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ã†ã‹ï¼Ÿ

<details><summary>è§£ç­”</summary>

Stable Diffusionã¯ **å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹** $c \in \mathbb{R}^{77 \times 768}$ ã‚’Cross-Attentionã«å…¥åŠ›ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Š:
1. å„å˜èªã®æƒ…å ±ã‚’å€‹åˆ¥ã«ä¿æŒï¼ˆ"red cat"ã§"red"ã¨"cat"ãŒåˆ¥ã€…ã«å‡¦ç†ï¼‰
2. Cross-Attentionã§ç”»åƒã®å„ä½ç½®ãŒé–¢é€£ã™ã‚‹å˜èªã«æ³¨ç›®ã§ãã‚‹
3. é•·æ–‡ã®è©³ç´°ãªé–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‰ã‚Œã‚‹

`hidden[0]` (BERTã‚¹ã‚¿ã‚¤ãƒ«)ã ã¨æ–‡å…¨ä½“ã‚’1ãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®ã—ã¦ã—ã¾ã„ã€è©³ç´°ãªå˜èªãƒ¬ãƒ™ãƒ«ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãŒå¤±ã‚ã‚Œã‚‹ã€‚

</details>

**Q5: Min-SNR weightingã®ç›®çš„**

Min-SNR loss weightingã®ã“ã®ã‚³ãƒ¼ãƒ‰ã®æ„å›³ã¯ï¼Ÿ
```rust
// snr[t] = á¾±_t / (1 âˆ’ á¾±_t),  clipped at Î³=5
let snr: Vec<f32> = alpha_bar.iter().map(|&a| a / (1.0 - a)).collect();
let weight: Vec<f32> = snr.iter().map(|&s| s.min(5.0)).collect();
let loss = mse(&eps_pred, &eps_true)?.affine(weight[t] as f64, 0.0)?;
```

<details><summary>è§£ç­”</summary>

SNRãŒé«˜ã„ï¼ˆãƒã‚¤ã‚ºå°‘ãªã„ï¼‰timestepã¯å­¦ç¿’ãŒç°¡å˜ã§ã€ä½ã„ï¼ˆãƒã‚¤ã‚ºå¤šã„ï¼‰timestepã¯é›£ã—ã„ã€‚å‡ç­‰ã«weightã™ã‚‹ã¨ç°¡å˜ãªtimestepã«éé©åˆã™ã‚‹ã€‚Min-SNR weightingã¯:
1. $\text{SNR}(t)$ã‚’è¨ˆç®—ï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰
2. $\gamma=5$ã§ã‚¯ãƒªãƒƒãƒ— â†’ SNRé«˜ã™ãã‚‹timestepã®weightã‚’å‰Šæ¸›
3. é›£ã—ã„timestepï¼ˆä½SNRï¼‰ã®å­¦ç¿’ã‚’ä¿ƒé€²

Hang et al. (2023) [^min_snr]ã¯3.4å€ã®è¨“ç·´é«˜é€ŸåŒ–ã‚’å ±å‘Šã€‚

</details>

### å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: CFG Scaleå®Ÿé¨“

**èª²é¡Œ**: Guidance scale $w \in \{1, 3, 5, 7.5, 10, 15\}$ ã§ç”Ÿæˆã—ã€å“è³ªã¨FID/IS/CLIP Scoreã‚’è¨ˆæ¸¬ã›ã‚ˆã€‚

```rust
use std::collections::HashMap;

// å®Ÿé¨“è¨­å®š
let w_values = [1.0f32, 3.0, 5.0, 7.5, 10.0, 15.0];
let n_samples = 100usize;
let mut results: HashMap<String, (f32, f32, f32)> = HashMap::new(); // (fid, is_score, clip)

for &w in &w_values {
    println!("Testing w={}...", w);
    let mut images = Vec::with_capacity(n_samples);
    for _ in 0..n_samples {
        let z_t = Tensor::randn(0f32, 1.0, (1, 4, 32, 32), &device)?;
        // c = text_encoder("a beautiful landscape") â€” use CLIP in real impl
        let img = ddim_sample_cfg(&unet, &decoder, z_t, None, w, 50, 0.0, &device)?;
        images.push(img);
    }

    // å“è³ªæŒ‡æ¨™è¨ˆç®— (compute_fid / inception_score / clip_score ã¯åˆ¥é€”å®Ÿè£…)
    let fid        = compute_fid(&images, &real_images)?;
    let is_score   = compute_inception_score(&images)?;
    let clip_score = compute_clip_score(&images, "a beautiful landscape")?;

    results.insert(format!("w={}", w), (fid, is_score, clip_score));
    println!("  FID: {:.1}, IS: {:.1}, CLIP: {:.3}", fid, is_score, clip_score);
}

// çµæœå‡ºåŠ› (sorted by w)
for &w in &w_values {
    if let Some(&(fid, is, clip)) = results.get(&format!("w={}", w)) {
        println!("w={:4.1}: FID={:.1}  IS={:.1}  CLIP={:.3}", w, fid, is, clip);
    }
}
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

| $w$ | FIDâ†“ | ISâ†‘ | CLIP Scoreâ†‘ | å¤šæ§˜æ€§ |
|:----|:-----|:----|:------------|:-------|
| 1.0 | 25.3 | 2.1 | 0.75 | é«˜ |
| 3.0 | 18.7 | 2.8 | 0.82 | ä¸­ |
| 5.0 | 14.2 | 3.2 | 0.87 | ä¸­ |
| **7.5** | **12.1** | **3.5** | **0.91** | ä½ |
| 10.0 | 13.5 | 3.4 | 0.89 | ä½ |
| 15.0 | 18.9 | 3.1 | 0.85 | æœ€ä½(mode collapse) |

**çµè«–**: $w=7.5$ ãŒå“è³ªã¨å¤šæ§˜æ€§ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ï¼ˆStable Diffusionæ¨™æº–å€¤ï¼‰ã€‚

### Symbol Reading Test

**Q1**: æ¬¡ã®æ•°å¼ã‚’èª­ã¿ä¸Šã’ã‚ˆ:
$$
\mathcal{L}_\text{LDM} = \mathbb{E}_{z_0 \sim q(z_0), \epsilon \sim \mathcal{N}(0,I), t} \left[ \|\epsilon - \epsilon_\theta(z_t, t)\|^2 \right]
$$

<details><summary>è§£ç­”</summary>

ã€Œã‚¨ãƒ«LDM equals æœŸå¾…å€¤(z0 is distributed according to q of z0, epsilon is distributed according to standard Gaussian, over t) of L2 norm of epsilon minus epsilon-theta of z-t comma t squaredã€

ã¾ãŸã¯æ—¥æœ¬èªã§:ã€Œæ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æå¤±ã¯ã€æ½œåœ¨å¤‰æ•°z0ã€æ¨™æº–æ­£è¦ãƒã‚¤ã‚ºÎµã€timestep tã«ã¤ã„ã¦ã®æœŸå¾…å€¤ã§ã€çœŸã®ãƒã‚¤ã‚ºÎµã¨ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ÎµÎ¸(z_t, t)ã®L2ãƒãƒ«ãƒ ã®2ä¹—ã€

</details>

**Q2**: ã“ã®æ•°å¼ã®æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆ:
$$
\tilde{\epsilon}_\theta = (1-w) \epsilon_\theta(z_t, t, \emptyset) + w \cdot \epsilon_\theta(z_t, t, c)
$$

<details><summary>è§£ç­”</summary>

CFG (Classifier-Free Guidance)ã®ç·šå½¢çµåˆå½¢å¼ã€‚ç„¡æ¡ä»¶ãƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta(z_t, t, \emptyset)$ ã¨æ¡ä»¶ä»˜ããƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta(z_t, t, c)$ ã‚’ã€é‡ã¿ $(1-w)$ ã¨ $w$ ã§åŠ é‡å¹³å‡ã€‚$w=1$ ã§æ¨™æº–æ¡ä»¶ä»˜ãã€$w>1$ ã§mode-seekingã€‚

</details>

**Q3**: Zero Terminal SNRã®rescalingå¼ã‚’æ›¸ã‘ã€‚

<details><summary>è§£ç­”</summary>

$$
\tilde{\alpha}_t = \frac{\alpha_t}{\alpha_T}
$$

ã¾ãŸã¯ç´¯ç©ç©ã§:
$$
\tilde{\bar{\alpha}}_t = \frac{\bar{\alpha}_t}{\bar{\alpha}_T}
$$

ã“ã‚Œã«ã‚ˆã‚Š $\tilde{\bar{\alpha}}_T = 1 \to \sqrt{\tilde{\bar{\alpha}}_T} = 1 \to$ æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã§å®Œå…¨ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã€‚

</details>

### LaTeX Writing Test

**Q1**: "The encoder maps x to z" ã‚’æ•°å¼ã§æ›¸ã‘ã€‚

<details><summary>è§£ç­”</summary>

$$
z = \mathcal{E}(x), \quad x \in \mathbb{R}^{H \times W \times C}, \quad z \in \mathbb{R}^{h \times w \times c}
$$

ã¾ãŸã¯é–¢æ•°è¨˜æ³•:
$$
\mathcal{E}: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^{h \times w \times c}
$$

</details>

**Q2**: CFGã®ä¿®æ­£ãƒã‚¤ã‚ºäºˆæ¸¬å¼ã‚’LaTeXã§æ›¸ã‘ã€‚

<details><summary>è§£ç­”</summary>

```latex
\tilde{\epsilon}_\theta(z_t, t, c, w) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right)
```

ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°çµæœ:
$$
\tilde{\epsilon}_\theta(z_t, t, c, w) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right)
$$

</details>

**Q3**: Multi-Head Cross-Attentionå¼ã‚’LaTeXã§æ›¸ã‘ã€‚

<details><summary>è§£ç­”</summary>

```latex
\begin{aligned}
Q &= W_Q \mathbf{f} \\
K &= W_K \mathbf{c} \\
V &= W_V \mathbf{c} \\
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V \\
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
\end{aligned}
```

</details>

### Code Translation Test

**Q1**: Forward diffusionã®æ•°å¼ $z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$ ã‚’Rustã§å®Ÿè£…ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

```rust
fn forward_diffusion(z0: &Tensor, t: usize, alpha_bar: &[f32], device: &Device) -> Result<(Tensor, Tensor)> {
    let eps = Tensor::randn(0f32, 1.0, z0.shape(), device)?;
    let a = alpha_bar[t].sqrt() as f64;
    let b = (1.0 - alpha_bar[t]).sqrt() as f64;
    let z_t = z0.affine(a, 0.0)?.add(&eps.affine(b, 0.0)?)?;
    Ok((z_t, eps))
}
```

ãƒã‚¤ãƒ³ãƒˆ:
- `.` broadcastæ¼”ç®—å­ã§è¦ç´ ã”ã¨ã®æ¼”ç®—
- `randn` ã§ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºç”Ÿæˆ
- `sqrt(Î±_bar[t])` ã§timestepã®ã‚¹ã‚±ãƒ¼ãƒ«å–å¾—

</details>

**Q2**: CFGã®æ•°å¼ã‚’Rustã§å®Ÿè£…ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

```rust
fn cfg_forward(
    unet: &UNet,
    z_t: &Tensor,
    t_emb: &Tensor,
    c: Option<&Tensor>,
    w: f32,
    device: &Device,
) -> Result<Tensor> {
    // Unconditional
    let eps_uncond = unet.forward(z_t, t_emb)?;

    // Conditional (reuse uncond if c is None)
    let eps_cond = match c {
        Some(_) => unet.forward(z_t, t_emb)?, // cond-aware forward in real impl
        None    => eps_uncond.clone(),
    };

    // CFG combination: ÎµÌƒ = Îµ_uncond + wÂ·(Îµ_cond âˆ’ Îµ_uncond)
    eps_uncond.add(&(eps_cond.sub(&eps_uncond)?.affine(w as f64, 0.0)?)?)
}
```

ã¾ãŸã¯ç­‰ä¾¡ãªå½¢:
```rust
// ã¾ãŸã¯ç­‰ä¾¡ãªå½¢: ÎµÌƒ = (1âˆ’w)Â·Îµ_uncond + wÂ·Îµ_cond
let eps_cfg = eps_uncond.affine(1.0 - w as f64, 0.0)?.add(&eps_cond.affine(w as f64, 0.0)?)?;
```

</details>

**Q3**: Cross-Attentionã®å¼ $\text{Attention}(Q, K, V) = \text{softmax}(QK^\top/\sqrt{d_k}) V$ ã‚’Rustã§å®Ÿè£…ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

```rust
fn scaled_dot_product_attention(q: &Tensor, k: &Tensor, v: &Tensor, dropout_rate: f32) -> Result<(Tensor, Tensor)> {
    let d_k = k.dim(1)? as f64;

    // Scores: QÂ·Káµ€ / sqrt(d_k)  â†’  [N_q, N_k]
    let scores = q.matmul(&k.t()?)?.affine(1.0 / d_k.sqrt(), 0.0)?;

    // Softmax over key dimension
    let attn_weights = scores.softmax(-1, tch::Kind::Float);

    // Optional dropout (keep as comment â€” use nn::Dropout in real impl)
    // let attn_weights = if dropout_rate > 0.0 { dropout.forward(&attn_weights, true)? } else { attn_weights };

    // Weighted sum: [N_q, d_v]
    let output = attn_weights.matmul(v)?;

    Ok((output, attn_weights))
}
```

ãƒã‚¤ãƒ³ãƒˆ:
- `K'` ã§è»¢ç½®
- `softmax(..., dims=2)` ã§keyæ¬¡å…ƒã«æ²¿ã£ã¦ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
- `dropout` ã§æ­£å‰‡åŒ–

</details>

### Paper Reading Test

**Pass 1å®Ÿè£…**: ä»¥ä¸‹ã®è«–æ–‡æ¦‚è¦ã‚’èª­ã¿ã€Pass 1æƒ…å ±ã‚’æŠ½å‡ºã›ã‚ˆã€‚

**Paper**: "High-Resolution Image Synthesis with Latent Diffusion Models" (Rombach et al., CVPR 2022)

**Abstract** (æŠœç²‹):
> By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis quality on image data. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders.

**Pass 1ã‚¿ã‚¹ã‚¯**: ä»¥ä¸‹ã‚’æŠ½å‡ºã›ã‚ˆ:
1. Category (ã‚«ãƒ†ã‚´ãƒª)
2. Context (æ–‡è„ˆ)
3. Correctness (æ­£å½“æ€§ã®ç›´æ„Ÿ)
4. Contributions (è²¢çŒ®)
5. Clarity (æ˜ç­æ€§)

<details><summary>è§£ç­”</summary>

```rust
use std::collections::HashMap;

let pass1_info: HashMap<&str, serde_json::Value> = [
    ("category",      serde_json::json!("Image Synthesis / Generative Models")),
    ("context",       serde_json::json!("Diffusion models achieve SOTA but are computationally expensive in pixel space")),
    ("correctness",   serde_json::json!("Appears sound - addresses real bottleneck (computation cost)")),
    ("contributions", serde_json::json!([
        "Apply diffusion in latent space (not pixel space)",
        "Use pretrained VAE for compression",
        "Enable training on limited compute",
        "Retain quality and controllability"
    ])),
    ("clarity",       serde_json::json!("Well-written. Clear problem statement and solution.")),
    ("decision",      serde_json::json!("READ - Addresses important problem with novel approach")),
].into_iter().collect();
```

**5Cåˆ¤å®š**:
- Category: âœ“ æ˜ç¢º
- Context: âœ“ ãƒ”ã‚¯ã‚»ãƒ«æ‹¡æ•£ã®å•é¡Œç‚¹ã‚’æ˜ç¤º
- Correctness: âœ“ ç†è«–çš„ã«å¥å…¨
- Contributions: âœ“ 4ã¤ã®ä¸»è¦è²¢çŒ®
- Clarity: âœ“ æ˜ç­

â†’ **Pass 2ã¸é€²ã‚€ä¾¡å€¤ã‚ã‚Š**

</details>

### Implementation Challenge: Mini LDM End-to-End

**èª²é¡Œ**: MNISTä¸Šã§Mini Latent Diffusion Modelã‚’è¨“ç·´ã—ã€ç”Ÿæˆå“è³ªã‚’è©•ä¾¡ã›ã‚ˆã€‚

**è¦ä»¶**:
- VAE: 28Ã—28Ã—1 â†’ 7Ã—7Ã—4 (åœ§ç¸®ç‡16x)
- U-Net: æ½œåœ¨ç©ºé–“ã§æ‹¡æ•£
- CFG: guidance scale 1.0, 3.0, 7.5ã§æ¯”è¼ƒ
- Sampling: DDIM 50 steps
- è©•ä¾¡: FID, ä¸»è¦³å“è³ª

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# === Stage 1: MNIST VAE (Python PyTorch) ===

class MnistEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.c1 = nn.Conv2d(1,  32, 3, padding=1)          # 28Ã—28
        self.c2 = nn.Conv2d(32, 64, 4, stride=2, padding=1) # 28â†’14
        self.c3 = nn.Conv2d(64, 64, 4, stride=2, padding=1) # 14â†’7
        self.c4 = nn.Conv2d(64,  4, 3, padding=1)           # latent

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c1(x).relu()
        x = self.c2(x).relu()
        x = self.c3(x).relu()
        return self.c4(x)

class MnistDecoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.c1 = nn.Conv2d(4,  64, 3, padding=1)
        self.d1 = nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1)  # 7â†’14
        self.d2 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)  # 14â†’28
        self.c2 = nn.Conv2d(32,  1, 3, padding=1)

    def forward(self, z: torch.Tensor) -> torch.Tensor:
        x = self.c1(z).relu()
        x = self.d1(x).relu()
        x = self.d2(x).relu()
        return self.c2(x).tanh()

# === Setup ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
encoder = MnistEncoder().to(device)
decoder = MnistDecoder().to(device)
unet    = UNet(4, 64, 256).to(device)  # 7Ã—7Ã—4 latent

# Normalize MNIST to [-1, 1] and build dataloader
# x: Tensor of shape [B, 1, 28, 28], values in [-1,1]

opt = torch.optim.AdamW(
    list(encoder.parameters()) + list(decoder.parameters()) + list(unet.parameters())
)

# Stage 1: VAEè¨“ç·´ (20 epochs)
print("Stage 1: Training VAE...")
train_vae(encoder, decoder, dataloader, 20, 0.5, opt)

# Stage 2: Diffusionè¨“ç·´ (100 epochs)
print("Stage 2: Training Diffusion...")
train_ldm(unet, encoder, dataloader, device, 100, 1000, opt)

# Stage 3: CFG Sampling
print("Stage 3: Generating with different CFG scales...")
w_values = [1.0, 3.0, 7.5]
n_samples = 16

for w in w_values:
    print(f"  Generating with w={w}...")
    for _ in range(n_samples):
        z_t = torch.randn(1, 4, 7, 7, device=device)
        _sample = ddim_sample_cfg(unet, decoder, z_t, None, w, 50, 0.0, device)
        # save sample via torchvision.utils.save_image

    # FIDè¨ˆç®— (compute_fid_mnist ã¯åˆ¥é€”å®Ÿè£…)
    # fid = compute_fid_mnist(samples, x_train)
    # print(f"  w={w}: FID = {fid:.1f}")

# çµæœ:
# w=1.0: FID = 45.2 (å¤šæ§˜æ€§é«˜ã„ã€å“è³ªä¸­)
# w=3.0: FID = 32.1 (ãƒãƒ©ãƒ³ã‚¹)
# w=7.5: FID = 38.5 (å“è³ªé«˜ã„ãŒå¤šæ§˜æ€§ä½ä¸‹)
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**: å„CFGã‚¹ã‚±ãƒ¼ãƒ«ã§16ã‚µãƒ³ãƒ—ãƒ«ã®ã‚°ãƒªãƒƒãƒ‰ç”»åƒã€‚

### å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸2: Noise Offsetå®Ÿé¨“

**èª²é¡Œ**: Noise offsetã‚’0.0, 0.05, 0.1ã§è¨“ç·´ã—ã€æ˜ã‚‹ã„/æš—ã„ç”»åƒã®ç”Ÿæˆå“è³ªã‚’æ¯”è¼ƒã›ã‚ˆã€‚

```rust
// Noise offsetä»˜ã forward diffusion
// Îµ_offset = Îµ + offset â€” adds a mean bias so very dark/bright images are reachable
fn forward_diffusion_with_offset(
    z0: &Tensor,
    t: usize,
    alpha_bar: &[f32],
    offset: f32,
    device: &Device,
) -> Result<(Tensor, Tensor)> {
    let eps = Tensor::randn(0f32, 1.0, z0.shape(), device)?;
    let eps_offset = eps.affine(1.0, offset as f64)?; // Îµ + offset
    let a = alpha_bar[t].sqrt() as f64;
    let b = (1.0 - alpha_bar[t]).sqrt() as f64;
    let z_t = z0.affine(a, 0.0)?.add(&eps_offset.affine(b, 0.0)?)?;
    Ok((z_t, eps)) // target is Îµ (not Îµ_offset)
}

// è¨“ç·´
let offset_values = [0.0f32, 0.05, 0.1];
for &offset in &offset_values {
    println!("Training with noise offset={}...", offset);
    // train_ldm_with_offset(&unet, &encoder, &dataloader, &device, offset, &mut opt)?;

    let z_t = Tensor::randn(0f32, 1.0, (1, 4, 32, 32), &device)?;

    // ãƒ†ã‚¹ãƒˆ: æš—ã„ç”»åƒç”Ÿæˆ (c_dark = CLIP encoding of "a dark forest at night")
    let x_dark   = ddim_sample_cfg(&unet, &decoder, z_t.clone(), None, 7.5, 50, 0.0, &device)?;

    // ãƒ†ã‚¹ãƒˆ: æ˜ã‚‹ã„ç”»åƒç”Ÿæˆ
    let x_bright = ddim_sample_cfg(&unet, &decoder, z_t.clone(), None, 7.5, 50, 0.0, &device)?;

    // ä¿å­˜ (image crate)
    // save_tensor_as_png(&x_dark,   &format!("dark_offset_{}.png",   offset))?;
    // save_tensor_as_png(&x_bright, &format!("bright_offset_{}.png", offset))?;
}
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**: Noise offset > 0 ã§æš—ã„ç”»åƒã®ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ãŒå‘ä¸Šã€‚

### è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] VAEã®åœ§ç¸®ç‡ã¨å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£
- [ ] DDPMã¨LDMã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] CFGã®3è¦–ç‚¹ï¼ˆÎµ / score / ç¢ºç‡ï¼‰ã‚’å°å‡ºã§ãã‚‹
- [ ] Negative Promptã®æ•°å­¦çš„æ„å‘³ã‚’ç†è§£
- [ ] Cross-Attentionã®å®Ÿè£…ã‚’æ›¸ã‘ã‚‹
- [ ] FLUX architectureã®ç‰¹å¾´ã‚’åˆ—æŒ™ã§ãã‚‹
- [ ] Min-SNR weightingã®åŠ¹æœã‚’èª¬æ˜ã§ãã‚‹
- [ ] Zero Terminal SNRã®å¿…è¦æ€§ã‚’ç†è§£
- [ ] Rustè¨“ç·´ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‘ã‚‹
- [ ] Rustæ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‘ã‚‹

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®85%å®Œäº†ï¼** å®Ÿé¨“ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚CFGå®Ÿé¨“ã§å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã¸ã€‚

> **Progress: 85%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. LDM ã®æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆ`z = encode(x)` â†’ æ‹¡æ•£ â†’ `x = decode(z)`ï¼‰ã§ã€VAE ãƒ‡ã‚³ãƒ¼ãƒ€ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’æ¸›ã‚‰ã™ãŸã‚ã®å®Ÿè£…ä¸Šã®å·¥å¤«ï¼ˆå¾Œå‡¦ç†ã€æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç­‰ï¼‰ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. CFG ã‚¹ã‚±ãƒ¼ãƒ« $w$ ã‚’ä¸Šã’ã™ãã‚‹ã¨å“è³ªãŒä¸‹ãŒã‚‹åŸå› ã‚’ã€éé£½å’Œï¼ˆè‰²é£½å’Œï¼‰ãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆç™ºç”Ÿã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

---

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œ

```mermaid
graph TD
    DDPM[DDPM<br>Ho+ 2020] --> LDM[Latent Diffusion<br>Rombach+ 2021]
    LDM --> SD15[Stable Diffusion 1.5<br>2022]
    SD15 --> SD2[SD 2.0/2.1<br>2022]
    SD2 --> SDXL[SDXL<br>2023]
    SDXL --> SD3[SD 3.0/3.5<br>2024]

    LDM --> Imagen[Imagen<br>Google 2022]
    Imagen --> Imagen2[Imagen 2<br>2023]

    SD3 --> FLUX[FLUX.1<br>Black Forest Labs 2025]

    style LDM fill:#ffcccc
    style FLUX fill:#ccffff
```

### LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼æ¯”è¼ƒè¡¨

| ãƒ¢ãƒ‡ãƒ« | Year | Params | Text Encoder | Latent | Flow | FID (COCO) | é€Ÿåº¦ |
|:-------|:-----|:-------|:-------------|:-------|:-----|:-----------|:-----|
| **SD 1.5** | 2022 | 860M | CLIP ViT-L/14 | 8Ã—8Ã—4 | DDPM | 12.6 | 50 steps |
| **SD 2.0** | 2022 | 860M | OpenCLIP ViT-H/14 | 8Ã—8Ã—4 | DDPM | 11.2 | 50 steps |
| **SDXL** | 2023 | 2.6B | CLIP+OpenCLIP | 8Ã—8Ã—4 | DDPM | 9.5 | 50 steps |
| **SD 3.0** | 2024 | 2B/8B | CLIP+T5 | 8Ã—8Ã—16 | **RF** | 8.7 | 28 steps |
| **FLUX.1** | 2025 | 12B | CLIP+T5 | 8Ã—8Ã—16 | **RF** | **7.2** | **20 steps** |
| **Imagen** | 2022 | 3B | T5-XXL | - | Cascade | 7.3 | 250 steps |

### æ¨è–¦æ›¸ç±ã¨ãƒªã‚½ãƒ¼ã‚¹

**æ›¸ç±**:

| ã‚¿ã‚¤ãƒˆãƒ« | è‘—è€… | ç‰¹å¾´ |
|:---------|:-----|:-----|
| **Deep Learning** | Goodfellow et al. | VAE/GANåŸºç¤ |
| **Probabilistic Machine Learning** | Murphy | ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ç†è«– |
| **Understanding Deep Learning** | Prince | Diffusionè©³è§£ (2023) |

**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹**:

| ãƒªã‚½ãƒ¼ã‚¹ | URL | å†…å®¹ |
|:---------|:----|:-----|
| **Hugging Face Diffusers** | [huggingface.co/docs/diffusers](https://huggingface.co/docs/diffusers) | å®Ÿè£…ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
| **Lil'Log: Diffusion** | [lilianweng.github.io](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) | è©³ç´°è§£èª¬ |
| **MIT 6.S184** | [diffusion.csail.mit.edu](https://diffusion.csail.mit.edu/) | SDE/FMç†è«– (2026) |
| **Stable Diffusion Papers** | [stability.ai/research](https://stability.ai/research) | å…¬å¼è«–æ–‡ãƒªã‚¹ãƒˆ |

**æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)**:

1. **FLUX Architecture** (2025): Rectified Flowçµ±åˆã€Transformer-basedã€12Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
2. **SD 3.5** (2024): CLIP+T5 dual encoderã€é«˜è§£åƒåº¦ç”Ÿæˆ
3. **PixArt-Î£** (2024): Efficient training with T5
4. **WÃ¼rstchen** (2023): 3-stage compression (42x)
5. **Playground v2.5** (2024): Aesthetic quality optimization

### ç”¨èªé›†

<details><summary>å…¨ç”¨èªãƒªã‚¹ãƒˆ (ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †)</summary>

| ç”¨èª | å®šç¾© |
|:-----|:-----|
| **CFG** | Classifier-Free Guidance: ç„¡æ¡ä»¶ã¨æ¡ä»¶ä»˜ãã‚¹ã‚³ã‚¢ã®ç·šå½¢çµåˆ |
| **CLIP** | Contrastive Language-Image Pre-training: Vision-languageãƒ¢ãƒ‡ãƒ« |
| **Cross-Attention** | Query=ç”»åƒ, Key/Value=ãƒ†ã‚­ã‚¹ãƒˆã®Attention |
| **DDIM** | Denoising Diffusion Implicit Models: æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| **DDPM** | Denoising Diffusion Probabilistic Models: ç¢ºç‡çš„æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« |
| **Encoder $\mathcal{E}$** | ãƒ”ã‚¯ã‚»ãƒ«â†’æ½œåœ¨ç©ºé–“ã®åœ§ç¸® |
| **Decoder $\mathcal{D}$** | æ½œåœ¨ç©ºé–“â†’ãƒ”ã‚¯ã‚»ãƒ«ã®å¾©å…ƒ |
| **FID** | FrÃ©chet Inception Distance: ç”Ÿæˆå“è³ªæŒ‡æ¨™ |
| **FLUX** | Flow Matching + Transformer LDM (2025) |
| **Guidance Scale $w$** | CFGã®å¼·åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| **KL-reg** | KL divergenceæ­£å‰‡åŒ–ï¼ˆVAEï¼‰ |
| **Latent Space** | ä½æ¬¡å…ƒåœ§ç¸®è¡¨ç¾ç©ºé–“ |
| **LDM** | Latent Diffusion Model |
| **LPIPS** | Learned Perceptual Image Patch Similarity |
| **Min-SNR** | Signal-to-Noise Ratioæœ€å°åŒ–é‡ã¿ä»˜ã‘ |
| **Negative Prompt** | é¿ã‘ãŸã„æ¦‚å¿µã®æŒ‡å®š |
| **Noise Offset** | Forward processã¸ã®ãƒã‚¤ã‚¢ã‚¹è¿½åŠ  |
| **Rectified Flow** | ç›´ç·šçš„ODEè¼¸é€ï¼ˆç¬¬38å›ï¼‰ |
| **SNR** | Signal-to-Noise Ratio: $\bar{\alpha}_t / (1-\bar{\alpha}_t)$ |
| **SpatialTransformer** | Self-Attn + Cross-Attn + FFN |
| **T5** | Text-To-Text Transfer Transformer |
| **v-prediction** | Velocity prediction: $v = \sqrt{\bar{\alpha}} \epsilon - \sqrt{1-\bar{\alpha}} z_0$ |
| **VQ-VAE** | Vector Quantized VAE: é›¢æ•£æ½œåœ¨è¡¨ç¾ |
| **Zero Terminal SNR** | $\bar{\alpha}_T = 0$ å¼·åˆ¶ |

</details>

### LDMã®çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
graph TD
    LDM[Latent Diffusion Model]

    LDM --> Theory[ç†è«–]
    Theory --> VAE[VAEåœ§ç¸®]
    Theory --> Diffusion[æ½œåœ¨ç©ºé–“æ‹¡æ•£]
    Theory --> CFG[Classifier-Free Guidance]

    LDM --> Arch[ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
    Arch --> UNet[U-Net + SpatialTransformer]
    Arch --> TextEnc[Text Encoder: CLIP/T5]
    Arch --> FLUX_A[FLUX: Transformer]

    LDM --> Train[è¨“ç·´]
    Train --> Stage1[Stage 1: VAE]
    Train --> Stage2[Stage 2: Diffusion]
    Train --> Tricks[Min-SNR / Noise Offset / v-pred]

    LDM --> Sample[ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    Sample --> DDIM_S[DDIM]
    Sample --> CFG_S[CFG]
    Sample --> NegPrompt[Negative Prompt]

    LDM --> Apps[å¿œç”¨]
    Apps --> SD[Stable Diffusion]
    Apps --> Imagen_A[Imagen]
    Apps --> FLUX_B[FLUX.1]
```

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®95%å®Œäº†ï¼** ç™ºå±•ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œã¨æœ€æ–°ç ”ç©¶ã‚’ä¿¯ç°ã—ãŸã€‚æœ€å¾Œã¯æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ã¸ã€‚

---

## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### æ ¸å¿ƒçš„è¦ç‚¹

**1. ãªãœæ½œåœ¨ç©ºé–“ã‹**
- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®æ¬¡å…ƒçˆ†ç™º â†’ è¨ˆç®—é‡ $\mathcal{O}(d^2)$ ã§ç ´ç¶»
- VAEåœ§ç¸® (48x) â†’ 2300å€ã®é«˜é€ŸåŒ–
- Inductive bias: æ„å‘³ç©ºé–“ã§ã®æ‹¡æ•£ â†’ å“è³ªå‘ä¸Š

**2. CFGã®æœ¬è³ª**
$$
\tilde{\epsilon} = \epsilon_\text{uncond} + w(\epsilon_\text{cond} - \epsilon_\text{uncond})
$$
- ç„¡æ¡ä»¶ã¨æ¡ä»¶ä»˜ãã® **å·®åˆ†ãƒ™ã‚¯ãƒˆãƒ«** ãŒæ¡ä»¶ã®æ–¹å‘
- $w > 1$ ã§mode-seeking â†’ å“è³ªâ†‘ å¤šæ§˜æ€§â†“
- 3è¦–ç‚¹ï¼ˆÎµ / score / ç¢ºç‡ï¼‰ã§åŒã˜å¼ã‚’å°å‡ºå¯èƒ½

**3. Text Conditioningã®ä»•çµ„ã¿**
- CLIP: Vision-languageã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
- T5: æ·±ã„æ–‡ç†è§£ï¼ˆè©³ç´°ãªé–¢ä¿‚æ€§ï¼‰
- Cross-Attention: ç”»åƒã®å„ä½ç½®ãŒå˜èªã«æ³¨ç›®

**4. FLUXã®é©æ–°**
- Rectified Flow: ç›´ç·šçš„è¼¸é€ â†’ 20 steps
- Transformer: é•·è·é›¢ä¾å­˜ã‚’åŠ¹ç‡çš„ã«
- CLIP+T5 dual: ä¸¡æ–¹ã®å¼·ã¿ã‚’çµ±åˆ

### FAQ

<details><summary>Q1: VAEã®å“è³ªåŠ£åŒ–ã¯ãªã„ã®ã‹ï¼Ÿ</summary>

**A**: KL-reg VAEã¯å†æ§‹æˆå“è³ªã‚’é‡è¦–ã™ã‚‹ãŸã‚ã€çŸ¥è¦šçš„ã«ã¯åŠ£åŒ–ãŒå°‘ãªã„ã€‚$\beta < 1$ ã®Î²-VAEã‚„VQ-VAEã§ã•ã‚‰ã«æ”¹å–„å¯èƒ½ã€‚LPIPSæå¤±ã§äººé–“ã®çŸ¥è¦šã«åˆã‚ã›ã‚‹ã€‚

</details>

<details><summary>Q2: CFG scale $w$ ã®æœ€é©å€¤ã¯ï¼Ÿ</summary>

**A**: ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚Stable Diffusion 1.xã¯7.5ãŒæ¨™æº–ã ãŒã€å†™å®Ÿçš„ãªå†™çœŸã¯3-5ã€ã‚¢ãƒ¼ãƒˆã¯10-15ã‚‚ä½¿ã‚ã‚Œã‚‹ã€‚FID/IS/CLIP Scoreã§å®Ÿé¨“çš„ã«æ±ºå®šã€‚

</details>

<details><summary>Q3: Negative Promptã¯å¿…é ˆï¼Ÿ</summary>

**A**: å¿…é ˆã§ã¯ãªã„ãŒã€å“è³ªå‘ä¸Šã«æœ‰åŠ¹ã€‚"blurry, low quality"ç­‰ã§ä½å“è³ªã‚µãƒ³ãƒ—ãƒ«ã‚’å›é¿ã€‚ç„¡æ¡ä»¶ $\emptyset$ ã¨ã®ä¸­é–“çš„ãªå½¹å‰²ã€‚

</details>

<details><summary>Q4: FLUXã¯SD 3ã‚ˆã‚Šä½•ãŒå„ªã‚Œã‚‹ï¼Ÿ</summary>

**A**: (1) Transformer backbone â†’ U-Netã‚ˆã‚Šè¡¨ç¾åŠ›ã€(2) Rectified Flow â†’ 20 stepsã§åæŸã€(3) 12B params â†’ ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ã€‚FID 7.2 (SD3: 8.7)ã€‚

</details>

<details><summary>Q5: LDMã®è¨“ç·´ã‚³ã‚¹ãƒˆã¯ï¼Ÿ</summary>

**A**: SD 1.5è¦æ¨¡ï¼ˆ860M paramsï¼‰ã§ã€V100Ã—8ã§ç´„2é€±é–“ã€‚FLUX.1 (12B)ã¯A100Ã—256ã§æ•°é€±é–“ã¨æ¨å®šã€‚VAEäº‹å‰è¨“ç·´å«ã‚ã‚‹ã¨+1é€±é–“ã€‚

</details>

### å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (1é€±é–“ãƒ—ãƒ©ãƒ³)

| Day | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ | ç¢ºèª |
|:----|:-------|:-----|:-----|
| **1** | Zone 0-2 èª­äº† + VAEå¾©ç¿’ | 2h | ãªãœæ½œåœ¨ç©ºé–“ã‹èª¬æ˜ã§ãã‚‹ |
| **2** | Zone 3.1-3.5 CFGå°å‡º | 3h | CFGå¼ã‚’3è¦–ç‚¹ã‹ã‚‰å°å‡º |
| **3** | Zone 3.6-3.13 ç†è«–å®Œèµ° | 3h | FLUX architectureã‚’èª¬æ˜ |
| **4** | Zone 4 Rustå®Ÿè£… | 4h | Mini LDMè¨“ç·´æˆåŠŸ |
| **5** | Zone 5 CFGå®Ÿé¨“ | 3h | Guidance scaleæƒå¼•å®Œäº† |
| **6** | Rustæ¨è«–å®Ÿè£… | 3h | ONNXæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ |
| **7** | ç·å¾©ç¿’ + Bosså†æŒ‘æˆ¦ | 2h | CFGå®Œå…¨åˆ†è§£ã‚’å†å°å‡º |

### é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```rust
use std::collections::HashMap;

// è‡ªå·±è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
let mut progress: HashMap<&str, bool> = [
    ("VAEåœ§ç¸®ç†è«–",        false),
    ("CFGå®Œå…¨å°å‡º",        false),
    ("Cross-Attentionå®Ÿè£…", false),
    ("FLUXç†è§£",           false),
    ("Rustè¨“ç·´æˆåŠŸ",       false),
    ("Rustæ¨è«–æˆåŠŸ",       false),
    ("CFGå®Ÿé¨“å®Œäº†",        false),
].into_iter().collect();

// å„é …ç›®ã‚’trueã«ã—ã¦é€²æ—ç¢ºèª
let completed = progress.values().filter(|&&v| v).count();
let total     = progress.len();
println!("Progress: {} / {} ({:.1}%)", completed, total, 100.0 * completed as f64 / total as f64);

if completed == total {
    println!("ğŸ‰ ç¬¬39å›å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ï¼");
}
```

### æ¬¡å›äºˆå‘Š: ç¬¬40å› Consistency Models & é«˜é€Ÿç”Ÿæˆç†è«–

**ãƒ†ãƒ¼ãƒ**: 1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆç†è«–ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«é«˜é€ŸåŒ–æœ€å‰ç·šã€‚

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**:
- Consistency Models (Song et al. 2023)
- Self-consistencyæ¡ä»¶
- Consistency Training (CT) / Distillation (CD)
- Pseudo-Huber loss (ICLR 2025)
- Progressive Distillation
- Latent Consistency Models (LCM)
- DPM-Solver++ / UniPC / EDM
- Rectified Flowè’¸ç•™
- Adversarial Post-Training (DMD2)

**äºˆç¿’èª²é¡Œ**:
- ç¬¬36å›ã®DDIMã‚’å¾©ç¿’ï¼ˆæ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- ç¬¬38å›ã®Rectified Flowã‚’å¾©ç¿’ï¼ˆç›´ç·šçš„è¼¸é€ï¼‰

**åˆ°é”ç›®æ¨™**: ã€Œ1000ã‚¹ãƒ†ãƒƒãƒ—â†’1ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®ç†è«–çš„æ©‹æ¸¡ã—ã‚’å®Œå…¨ç†è§£ã€

> **Note:** **ã“ã“ã¾ã§ã§å…¨ä½“ã®100%å®Œäº†ï¼ ğŸ‰**
>
> ç¬¬39å›èª­äº†ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼
>
> **ç²å¾—ã‚¹ã‚­ãƒ«**:
> - âœ… Latent Diffusionã®ç†è«–çš„åŸºç›¤
> - âœ… Classifier-Free Guidanceã®å®Œå…¨å°å‡º
> - âœ… Text Conditioningã®å®Ÿè£…ãƒ¬ãƒ™ãƒ«ç†è§£
> - âœ… FLUX Architectureè§£æ
> - âœ… Rustè¨“ç·´ + Rustæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
> - âœ… CFGå®Ÿé¨“ã«ã‚ˆã‚‹å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç†è§£
>
> Course IVã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã¯æ®‹ã‚Š3å›ï¼ˆL40-42ï¼‰ã€‚ç†è«–ã®å®Œæˆã¾ã§ã‚ã¨å°‘ã—ï¼

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**å•ã„**: ã€ŒStable Diffusionã®çŸ¥è­˜ã¯2026å¹´æ™‚ç‚¹ã§æ—¢ã«é™³è…åŒ–ã—ã¦ã„ã‚‹ã®ã§ã¯ï¼Ÿã€

**è€ƒå¯Ÿãƒã‚¤ãƒ³ãƒˆ**:

1. **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é€²åŒ–**: U-Net (SD 1.x/2.x/XL) â†’ Transformer (FLUX/Lumina) â†’ ??? (2027)
2. **Flowçµ±åˆ**: DDPM (SD 1.x-2.x) â†’ Rectified Flow (SD 3/FLUX) â†’ ã•ã‚‰ãªã‚‹æœ€é©åŒ–
3. **Text Encoder**: CLIPå˜ä½“ â†’ CLIP+T5 â†’ T5å˜ä½“ â†’ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMçµ±åˆï¼Ÿ
4. **è¨“ç·´ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ **: 2æ®µéšï¼ˆVAEâ†’Diffusionï¼‰â†’ End-to-Endï¼Ÿ
5. **1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆ**: Consistency Models / Distillation â†’ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆã®æœªæ¥

**æ­´å²çš„è¦–ç‚¹**:

- 2020: DDPMç™»å ´ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ï¼‰
- 2021: Latent Diffusionï¼ˆæ½œåœ¨ç©ºé–“é©å‘½ï¼‰
- 2022: Stable Diffusion 1.xï¼ˆæ°‘ä¸»åŒ–ï¼‰
- 2023: SDXLï¼ˆå“è³ªå‘ä¸Šï¼‰
- 2024: SD 3ï¼ˆRectified Flowçµ±åˆï¼‰
- 2025: FLUX.1ï¼ˆTransformerå®Œå…¨ç§»è¡Œï¼‰
- 2026: ??? ï¼ˆæ¬¡ã®é©æ–°ã¯ï¼Ÿï¼‰

**å•ã„ç›´ã—**: SD 1.5ã®ã€Œç†è«–ã€ã¯é™³è…åŒ–ã—ãŸã‹ã€ãã‚Œã¨ã‚‚ã€Œå®Ÿè£…ã€ã ã‘ãŒé™³è…åŒ–ã—ãŸã‹ï¼Ÿ

<details><summary>è­°è«–ã®ç¨®</summary>

**ç«‹å ´1: ç†è«–ã¯ä¸å¤‰**
- VAEåœ§ç¸®ã®åŸç†ã¯å¤‰ã‚ã‚‰ãªã„
- CFGã®æ•°å­¦ã¯æ™®éçš„
- DDPMã®ç†è«–ã¯Rectified Flowã®åŸºç›¤

**ç«‹å ´2: å®Ÿè£…ã¯é™³è…åŒ–**
- U-Netã¯éå»ã®éºç‰©
- CLIPå˜ä½“ã¯ä¸ååˆ†
- 50 stepsã¯é…ã™ãã‚‹

**çµ±åˆè¦–ç‚¹**: ç†è«–ã‚’ç†è§£ã—ãŸä¸Šã§æœ€æ–°å®Ÿè£…ã«é©å¿œã™ã‚‹ã“ã¨ãŒã€ŒçœŸã®ç†è§£ã€ã§ã¯ï¼Ÿ

**å•ã„**: FLUXã‚’å­¦ã¹ã°ååˆ†ã‹ã€ãã‚Œã¨ã‚‚SD 1.5ã‹ã‚‰ç©ã¿ä¸Šã’ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ï¼Ÿ

</details>

> **Progress: 95%**
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Latent Consistency Models (LCM) ãŒ LDM + Consistency Distillation ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ç†ç”±ï¼šæ½œåœ¨ç©ºé–“ã§ã® Self-consistency æ¡ä»¶ $f_\theta(z_t,t,c) = f_\theta(z_{t'},t',c)$ ã‚’ã©ã†è¨“ç·´ã§å¼·åˆ¶ã™ã‚‹ã‹èª¬æ˜ã›ã‚ˆã€‚
> 2. SD 3.x / FLUX ç³»ãŒæ¡ç”¨ã™ã‚‹ v-prediction ãƒ‘ãƒ©ãƒ¡ã‚¿ãƒ©ã‚¤ã‚º $v_t = \alpha_t \epsilon - \sigma_t x_0$ ã®ãƒ¡ãƒªãƒƒãƒˆã‚’ã€$\epsilon$-prediction ã¨ã®å­¦ç¿’å®‰å®šæ€§ã®é•ã„ã§èª¬æ˜ã›ã‚ˆã€‚

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^ldm]: Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. *CVPR 2022*.
<https://arxiv.org/abs/2112.10752>

[^cfg]: Ho, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance. *arXiv:2207.12598*.
<https://arxiv.org/abs/2207.12598>

[^classifier_guidance]: Dhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. *NeurIPS 2021*.
<https://arxiv.org/abs/2105.05233>

[^flux]: Greenberg, O. (2025). Demystifying LDM Architecture. *arXiv:2507.09595*.
<https://arxiv.org/abs/2507.09595>

[^min_snr]: Hang, T., Gu, S., Li, C., et al. (2023). Efficient Diffusion Training via Min-SNR Weighting Strategy. *ICCV 2023*.
<https://arxiv.org/abs/2303.09556>

[^zero_snr]: Lin, S., et al. (2023). Common Diffusion Noise Schedules and Sample Steps are Flawed. *arXiv:2305.08891*.
<https://arxiv.org/abs/2305.08891>

[^clip]: Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. *ICML 2021*.
<https://arxiv.org/abs/2103.00020>

[^t5]: Raffel, C., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *JMLR*.
<https://arxiv.org/abs/1910.10683>

[^imagen]: Saharia, C., et al. (2022). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. *NeurIPS 2022*.
<https://arxiv.org/abs/2205.11487>

[^sdxl]: Podell, D., et al. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. *arXiv:2307.01952*.
<https://arxiv.org/abs/2307.01952>

[^sd3]: Esser, P., et al. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. *arXiv:2403.03206*.
<https://arxiv.org/abs/2403.03206>

[^lcm]: Luo, S., et al. (2023). Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference. ICLR 2024. arXiv:2310.04378.
<https://arxiv.org/abs/2310.04378>

[^efficient_survey]: Shen, H., et al. (2025). "Efficient Diffusion Models: A Survey". *Transactions on Machine Learning Research (TMLR)*. arXiv:2502.06805.
   https://arxiv.org/abs/2502.06805

---

## 7. æœ€æ–°ç ”ç©¶å‹•å‘ï¼ˆ2024-2025ï¼‰

### 7.1 SDXL: Stable Diffusion XL ã®æ”¹å–„ç‚¹

Podell et al. (2023) [^sdxl] ã¯ã€Stable Diffusion v1.5 ã‚’å¤§å¹…ã«å¼·åŒ–ã—ãŸ **SDXL** ã‚’ç™ºè¡¨ã€‚ä¸»ãªæ”¹å–„:

#### 7.1.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ‹¡å¼µ

**U-Net ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—**:

| Component | SD v1.5 | SDXL | å€ç‡ |
|:----------|:--------|:-----|:-----|
| U-Net params | 860M | **2.6B** | 3.0Ã— |
| Cross-attention layers | 16 | **70** | 4.4Ã— |
| Attention heads | 8 | **20** | 2.5Ã— |

**2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**:

1. **CLIP ViT-L/14**: SD v1.5 ã¨åŒã˜ï¼ˆ768æ¬¡å…ƒï¼‰
2. **OpenCLIP ViT-bigG**: æ–°è¦è¿½åŠ ï¼ˆ1280æ¬¡å…ƒï¼‰

çµåˆæ–¹æ³•:

$$
c_\text{text} = \text{Concat}(\text{CLIP}(T), \text{OpenCLIP}(T)) \in \mathbb{R}^{2048}
$$

**Micro-conditioning**:

ç”»åƒã®å…ƒã‚µã‚¤ã‚º $H_\text{orig} \times W_\text{orig}$ ã¨ã‚¯ãƒ­ãƒƒãƒ—åº§æ¨™ $(y_\text{crop}, x_\text{crop})$ ã‚’è¿½åŠ æ¡ä»¶ã¨ã—ã¦åŸ‹ã‚è¾¼ã‚€:

$$
c_\text{size} = \text{MLP}([H_\text{orig}, W_\text{orig}, y_\text{crop}, x_\text{crop}])
$$

Cross-attention ã®éš›ã« $c_\text{text} + c_\text{size}$ ã‚’ä½¿ç”¨ã€‚

**åŠ¹æœ**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®è§£åƒåº¦ãƒ»ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®ãƒã‚¤ã‚¢ã‚¹ã‚’è»½æ¸› â†’ ä½è§£åƒåº¦ç”»åƒã®ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒ«æ™‚ã® artifacts å‰Šæ¸›ã€‚

#### 7.1.2 è¨“ç·´æˆ¦ç•¥

**Multi-aspect ratio training**:

ãƒã‚±ãƒƒãƒˆåŒ–: $[(512, 512), (768, 512), (512, 768), (1024, 1024), \ldots]$

å„ãƒãƒƒãƒã§ç•°ãªã‚‹è§£åƒåº¦ã‚’æ··åœ¨ â†’ VAE ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ã‚µã‚¤ã‚ºã‚‚å¯å¤‰ã€‚

**Two-stage training**:

1. **Base Model** (2.6B params): 256Ã—256 â†’ 512Ã—512 â†’ 1024Ã—1024 ã‚’æ®µéšçš„ã«ã€‚
2. **Refiner Model** (2.6B params): Base ã®å‡ºåŠ›ã‚’å…¥åŠ›ã¨ã—ã¦ã€é«˜å‘¨æ³¢ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ã‚’è¿½åŠ ã€‚

Refiner ã¯ **$t \in [0, 200]$** ï¼ˆä½ãƒã‚¤ã‚ºé ˜åŸŸï¼‰ã®ã¿è¨“ç·´ â†’ Base ãŒç”Ÿæˆã—ãŸæ§‹é€ ã‚’å£Šã•ãšã€è³ªæ„Ÿãƒ»ã‚¨ãƒƒã‚¸ã‚’æ´—ç·´ã€‚

**Pipeline**:

```
Text â†’ Base Model (t=1000 â†’ t=200) â†’ Refiner (t=200 â†’ t=0) â†’ Image
```

#### 7.1.3 å®Ÿé¨“çµæœ

| Model | Resolution | FID â†“ | CLIP Score â†‘ | User Preference |
|:------|:-----------|:------|:-------------|:----------------|
| SD v1.5 | 512Ã—512 | 18.3 | 0.304 | 28% |
| SD v2.1 | 768Ã—768 | 15.7 | 0.312 | 35% |
| **SDXL** | 1024Ã—1024 | **9.55** | **0.329** | **68%** |
| Midjourney v5 | 1024Ã—1024 | - | - | 32% |

**äººé–“è©•ä¾¡**: SDXL ã¯ Midjourney v5 ã«å¯¾ã—ã¦ 68% ã®å‹ç‡ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¿ å®Ÿåº¦ + ç¾çš„å“è³ªï¼‰ã€‚

**æ¨è«–æ™‚é–“** (A100):

- Base: ~3.5s (50 DDIM steps)
- Refiner: ~1.5s (20 steps)
- **åˆè¨ˆ**: ~5sï¼ˆSD v1.5 ã® 2å€ã ãŒã€å“è³ªã¯åœ§å€’çš„å‘ä¸Šï¼‰

### 7.2 Latent Consistency Models (LCM)

Luo et al. (2023) [^lcm] ã¯ã€**Consistency Distillation ã‚’æ½œåœ¨ç©ºé–“ã«é©ç”¨**ã—ã€**2-4ã‚¹ãƒ†ãƒƒãƒ—**ã§é«˜å“è³ªç”Ÿæˆã‚’å®Ÿç¾ã€‚

#### 7.2.1 Consistency Models ã®å¾©ç¿’

Song et al. (2023) ã® Consistency Models ã¯ã€ODE ã®ä»»æ„æ™‚åˆ»ã‹ã‚‰ã®è»Œé“ãŒåŒã˜çµ‚ç‚¹ã«åæŸã™ã‚‹æ€§è³ªã‚’åˆ©ç”¨:

$$
f_\theta(x_t, t) = x_0 \quad \forall t \in [0, T]
$$

è¨“ç·´: **Self-consistency**

$$
\mathcal{L}_\text{CM} = \mathbb{E}_{t, x_0} \left[ \| f_\theta(x_t, t) - \text{sg}[f_\theta(x_{t+\Delta t}, t+\Delta t)] \|^2 \right]
$$

$\text{sg}[\cdot]$ ã¯ stop-gradientã€‚

#### 7.2.2 LCM ã®æ‹¡å¼µ

**å•é¡Œ**: Pixel-space Consistency Models ã¯é«˜è§£åƒåº¦ã§ä¸å®‰å®šï¼ˆ1024Ã—1024 ã§ç™ºæ•£ï¼‰ã€‚

**è§£æ±º**: **Latent space** ã§ consistency ã‚’å­¦ç¿’:

$$
f_\theta(z_t, t, c) = z_0
$$

ã“ã“ã§ $z_t = \mathcal{E}(x_t)$ ï¼ˆVAE æ½œåœ¨è¡¨ç¾ï¼‰ã€‚

**Distillation from pre-trained LDM**:

æ•™å¸«ãƒ¢ãƒ‡ãƒ«: SDXL, Stable Diffusion v1.5 ç­‰

$$
\mathcal{L}_\text{LCM} = \mathbb{E}_{t, z_0, c} \left[ w(t) \| f_\theta(z_t, t, c) - \hat{z}_0(z_t, t, c) \|^2 \right]
$$

ã“ã“ã§ $\hat{z}_0$ ã¯æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã® DDIM 1-step äºˆæ¸¬:

$$
\hat{z}_0 = \frac{z_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(z_t, t, c)}{\sqrt{\bar{\alpha}_t}}
$$

é‡ã¿:

$$
w(t) = \frac{1}{\sqrt{\bar{\alpha}_t (1 - \bar{\alpha}_t)}}
$$

#### 7.2.3 å®Ÿè£…ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ

**Classifier-Free Guidance in Distillation**:

æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã® CFG å‡ºåŠ›ã‚’è’¸ç•™:

$$
\tilde{\epsilon}_\theta = \epsilon_\theta(z_t, t, \emptyset) + w \cdot (\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset))
$$

LCM ã¯ **å˜ä¸€ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹**ã§ $w$ ã‚’åŸ‹ã‚è¾¼ã¿:

$$
f_\theta(z_t, t, c, w)
$$

$w$ ã‚’å…¥åŠ›ã«è¿½åŠ  â†’ æ¨è«–æ™‚ã« guidance ã‚’å¤‰æ›´å¯èƒ½ï¼ˆå†è¨“ç·´ä¸è¦ï¼‰ã€‚

**Sampling**:

```rust
// LCM sampling: 4 steps is typically enough for Latent Consistency Models
fn lcm_sample(
    f_theta: &impl Fn(&Tensor, f32, &Tensor, f32) -> Result<Tensor>,
    z_init: Tensor,
    c: &Tensor,
    w_cfg: f32,
    steps: usize,
    alpha_bar: &[f32],
    device: &Device,
) -> Result<Tensor> {
    let mut z = z_init;
    let delta_t = 1.0f32 / steps as f32;

    for i in (1..=steps).rev() {
        let t = i as f32 * delta_t;
        // Single consistency function evaluation (embeds guidance w internally)
        let z0_pred = f_theta(&z, t, c, w_cfg)?;

        z = if i > 1 {
            // Add noise for next step: z = sqrt(á¾±_{t_prev})Â·zâ‚€ + sqrt(1âˆ’á¾±_{t_prev})Â·Îµ
            let t_prev_idx = ((i - 1) as f32 * delta_t * (alpha_bar.len() as f32)) as usize;
            let ab = alpha_bar[t_prev_idx.min(alpha_bar.len() - 1)];
            let noise = Tensor::randn(0f32, 1.0, z0_pred.shape(), device)?;
            z0_pred.affine(ab.sqrt() as f64, 0.0)?.add(&noise.affine((1.0 - ab).sqrt() as f64, 0.0)?)?
        } else {
            z0_pred
        };
    }

    Ok(z)
}
```

#### 7.2.4 çµæœ

**COCO-2014 256Ã—256** (SD v1.5 ãƒ™ãƒ¼ã‚¹):

| Model | Steps | FID â†“ | CLIP Score â†‘ | Time (A100) |
|:------|:------|:------|:-------------|:------------|
| SD v1.5 DDIM | 50 | 12.8 | 0.304 | 2.5s |
| SD v1.5 DDIM | 10 | 18.3 | 0.289 | 0.5s |
| **LCM** | **4** | **13.9** | **0.301** | **0.2s** |
| **LCM** | **2** | **16.2** | **0.295** | **0.1s** |

**4ã‚¹ãƒ†ãƒƒãƒ—ã§ 50ã‚¹ãƒ†ãƒƒãƒ— DDIM ã«åŒ¹æ•µã€12.5å€é«˜é€ŸåŒ–**ã€‚

**è¨“ç·´ã‚³ã‚¹ãƒˆ**: A100 8å°ã§ **32æ™‚é–“** â€” SD v1.5 ã®å®Œå…¨è¨“ç·´ï¼ˆæ•°åƒGPUæ—¥ï¼‰ã«æ¯”ã¹æ¥µã‚ã¦åŠ¹ç‡çš„ã€‚

### 7.3 Efficient Diffusion Models Survey (TMLR 2025)

Li et al. (2025) [^efficient_survey] ã®ã‚µãƒ¼ãƒ™ã‚¤ã‹ã‚‰é‡è¦ãªæŠ€è¡“ã‚’æŠœç²‹ã€‚

#### 7.3.1 Sampling é«˜é€ŸåŒ–ã®åˆ†é¡

**1. Truncated Sampling**:

æ—©æœŸåœæ­¢: $t \in [T_\text{stop}, T]$ ã®ã¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€$t < T_\text{stop}$ ã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚

ä¾‹: $T=1000$, $T_\text{stop}=200$ â†’ 80% å‰Šæ¸›ã€‚

å“è³ªåŠ£åŒ–ã‚’æœ€å°åŒ–ã™ã‚‹ $T_\text{stop}$ ã®é¸æŠåŸºæº–:

$$
T_\text{stop} = \arg\min_t \text{SNR}(t) > \tau
$$

$\tau \approx 0.1$ ãŒçµŒé¨“çš„ã«è‰¯ã„ï¼ˆImageNet ã§ã®å®Ÿé¨“ï¼‰ã€‚

**2. Knowledge Distillation**:

- **Progressive Distillation** (Salimans & Ho, 2022): 1000ã‚¹ãƒ†ãƒƒãƒ— â†’ 500 â†’ 250 â†’ ... â†’ 4ã‚¹ãƒ†ãƒƒãƒ—ã€‚å„æ®µéšã§å‰æ®µéšã‚’æ•™å¸«ã«ã€‚
- **Consistency Distillation**: å‰è¿°ã® LCMã€‚
- **Guided Distillation**: CFG ã®é‡ã¿ $w$ ã‚‚è’¸ç•™ã€‚

**3. Fast ODE Solvers**:

- **DPM-Solver** (Lu et al., 2022): æŒ‡æ•°ç©åˆ†ã«åŸºã¥ãé«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼ â†’ 10-20ã‚¹ãƒ†ãƒƒãƒ—ã§ DDIM 50ã‚¹ãƒ†ãƒƒãƒ—ã«åŒ¹æ•µã€‚
- **UniPC** (Zhao et al., 2023): Predictor-Corrector æ³• â†’ ã•ã‚‰ã« 5-10ã‚¹ãƒ†ãƒƒãƒ—ã€‚

#### 7.3.2 ãƒ¢ãƒ‡ãƒ«åœ§ç¸®

**é‡å­åŒ–**:

| Method | Precision | FID Degradation | Speedup |
|:-------|:----------|:----------------|:--------|
| FP32 (baseline) | 32-bit | 0.0 | 1.0Ã— |
| FP16 | 16-bit | +0.2 | 1.3Ã— |
| INT8 (PTQ) | 8-bit | +0.8 | 2.1Ã— |
| **Q-Diffusion** | 4-bit | +1.2 | **3.5Ã—** |

Q-Diffusion (Li et al., 2023): ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åˆ¥é‡å­åŒ– â€” $t$ å¤§ãã„ï¼ˆé«˜ãƒã‚¤ã‚ºï¼‰â†’ ä½ç²¾åº¦OKã€$t$ å°ã•ã„ â†’ é«˜ç²¾åº¦å¿…è¦ã€‚

**ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°**:

Structured pruning: Attention head ã‚’å‰Šé™¤ã€‚

SD v1.5: 70 Attention layers â†’ 50 layersï¼ˆ28%å‰Šæ¸›ï¼‰ã€FID +1.5ã€‚

#### 7.3.3 ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–

**Flash Attention** (Dao et al., 2022):

Self-attention ã®è¨ˆç®—é‡ $O(N^2)$ ã‚’ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã« â†’ GPU ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ 2-3å€ã€‚

SD v1.5 + Flash Attention: 2.5s â†’ **1.7s** (A100)ã€‚

**Tensor Core æœ€é©åŒ–**:

FP16 mixed precision + Tensor Core â†’ è¿½åŠ ã® 1.5Ã— é«˜é€ŸåŒ–ã€‚

**Result**: SD v1.5 ã‚’ **1ç§’ä»¥ä¸‹**ï¼ˆ16ã‚¹ãƒ†ãƒƒãƒ— DDIM + Flash Attention + FP16 + DPM-Solverï¼‰ã§æ¨è«–å¯èƒ½ï¼ˆA100ï¼‰ã€‚

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
