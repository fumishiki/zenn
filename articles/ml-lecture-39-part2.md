---
title: "ç¬¬39å›: Latent Diffusion Models: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ–¼ï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "ldm", "julia", "stablediffusion"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” LDMè¨“ç·´â†’æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### ç’°å¢ƒæ§‹ç¯‰ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

**Juliaç’°å¢ƒ** (âš¡è¨“ç·´):
```julia
using Pkg
Pkg.add([
    "Lux",           # NN framework
    "Reactant",      # XLA compiler (GPUé«˜é€ŸåŒ–)
    "Optimisers",    # Adamç­‰
    "Zygote",        # è‡ªå‹•å¾®åˆ†
    "MLDatasets",    # MNISTç­‰
    "Images",        # ç”»åƒå‡¦ç†
    "CUDA",          # GPU
    "BenchmarkTools" # æ€§èƒ½æ¸¬å®š
])
```

**Rustç’°å¢ƒ** (ğŸ¦€æ¨è«–):
```toml
[dependencies]
candle-core = "0.7"
candle-nn = "0.7"
safetensors = "0.4"
```

### LaTeXè¨˜æ³•ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

| è¨˜å· | LaTeX | æ„å‘³ |
|:-----|:------|:-----|
| $\mathcal{E}$ | `\mathcal{E}` | Encoder |
| $\mathcal{D}$ | `\mathcal{D}` | Decoder |
| $\bar{\alpha}_t$ | `\bar{\alpha}_t` | Cumulative product |
| $\epsilon_\theta(z_t, t, c)$ | `\epsilon_\theta(z_t, t, c)` | Noise prediction |
| $\tilde{\epsilon}$ | `\tilde{\epsilon}` | Modified noise (CFG) |
| $\mathbb{E}_{q(z\|x)}[\cdot]$ | `\mathbb{E}_{q(z\|x)}[\cdot]` | Expectation |
| $\text{KL}[q \| p]$ | `\text{KL}[q \| p]` | KL divergence |

### Mathâ†’Codeç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ (LDMç·¨)

**ãƒ‘ã‚¿ãƒ¼ãƒ³1: VAE Encoder**
$$
z = \mathcal{E}(x), \quad x \in \mathbb{R}^{H \times W \times C} \to z \in \mathbb{R}^{h \times w \times c}
$$

```julia
z = encoder(x, ps_encoder, st_encoder)[1]
# x: [H, W, C, B] â†’ z: [h, w, c, B]
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³2: Forward Diffusion**
$$
z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
$$

```julia
Îµ = randn(rng, Float32, size(zâ‚€))
z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³3: CFG**
$$
\tilde{\epsilon}_\theta = \epsilon_\theta(z_t, t, \emptyset) + w \cdot (\epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset))
$$

```julia
Îµ_uncond = unet((z_t, t, nothing), ps, st)[1]
Îµ_cond = unet((z_t, t, c), ps, st)[1]
Îµ_cfg = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³4: DDIM Sampling**
$$
z_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \left( \frac{z_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1-\bar{\alpha}_{t-1} - \sigma_t^2} \epsilon_\theta + \sigma_t \epsilon
$$

```julia
pred_xâ‚€ = (z_t .- sqrt(1 - Î±_bar[t]) .* Îµ_Î¸) ./ sqrt(Î±_bar[t])
dir_z = sqrt(1 - Î±_bar[t-1] - ÏƒÂ²) .* Îµ_Î¸
noise = Ïƒ .* randn(rng, Float32, size(z_t))
z_prev = sqrt(Î±_bar[t-1]) .* pred_xâ‚€ .+ dir_z .+ noise
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³5: Cross-Attention**
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

```julia
scores = (Q * K') ./ sqrt(d_k)  # [N_q, N_k]
attn = softmax(scores, dims=2)   # [N_q, N_k]
out = attn * V                   # [N_q, d_v]
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³6: Min-SNR Weighting**
$$
w(t) = \min\left(\text{SNR}(t), \gamma\right), \quad \text{SNR}(t) = \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}
$$

```julia
snr = Î±_bar ./ (1 .- Î±_bar)
weight = min.(snr, Î³)
loss = weight[t] * mse(Îµ_pred, Îµ_true)
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³7: Zero Terminal SNR Rescaling**
$$
\tilde{\alpha}_t = \frac{\alpha_t}{\alpha_T}
$$

```julia
Î±_cumprod = cumprod(alphas)
Î±_cumprod_rescaled = Î±_cumprod ./ Î±_cumprod[end]
```

### âš¡ Juliaå®Œå…¨å®Ÿè£…: Mini Latent Diffusion

**ã‚¹ãƒ†ãƒƒãƒ—1: VAEå®šç¾©**

```julia
using Lux, Random, Optimisers, Zygote

# Encoder
function create_encoder(; in_ch=3, latent_ch=4, base_ch=64)
    return Chain(
        Conv((3,3), in_ch => base_ch, pad=1, activation=relu),
        Conv((4,4), base_ch => base_ch*2, stride=2, pad=1, activation=relu),  # /2
        Conv((4,4), base_ch*2 => base_ch*4, stride=2, pad=1, activation=relu),  # /4
        Conv((4,4), base_ch*4 => base_ch*8, stride=2, pad=1, activation=relu),  # /8
        Conv((3,3), base_ch*8 => latent_ch, pad=1)  # Output z
    )
end

# Decoder (mirror)
function create_decoder(; latent_ch=4, out_ch=3, base_ch=64)
    return Chain(
        Conv((3,3), latent_ch => base_ch*8, pad=1, activation=relu),
        ConvTranspose((4,4), base_ch*8 => base_ch*4, stride=2, pad=1, activation=relu),  # *2
        ConvTranspose((4,4), base_ch*4 => base_ch*2, stride=2, pad=1, activation=relu),  # *4
        ConvTranspose((4,4), base_ch*2 => base_ch, stride=2, pad=1, activation=relu),    # *8
        Conv((3,3), base_ch => out_ch, pad=1, activation=tanh)
    )
end

# VAEè¨“ç·´
function train_vae!(encoder, decoder, dataloader; epochs=10, lr=1e-3, Î²=0.5)
    ps_enc, st_enc = Lux.setup(Random.default_rng(), encoder)
    ps_dec, st_dec = Lux.setup(Random.default_rng(), decoder)
    opt = Adam(lr)
    opt_state_enc = Optimisers.setup(opt, ps_enc)
    opt_state_dec = Optimisers.setup(opt, ps_dec)

    for epoch in 1:epochs
        total_loss = 0.0
        for (x,) in dataloader
            # Forward
            z, st_enc = encoder(x, ps_enc, st_enc)
            x_recon, st_dec = decoder(z, ps_dec, st_dec)

            # Loss: Reconstruction + KL (simplified)
            recon_loss = mean((x_recon .- x).^2)
            kl_loss = 0.5 * mean(z.^2)  # Simplified KL to N(0,I)
            loss = recon_loss + Î² * kl_loss

            # Backprop
            gs_enc = gradient(p -> loss, ps_enc)[1]
            gs_dec = gradient(p -> loss, ps_dec)[1]
            opt_state_enc, ps_enc = Optimisers.update(opt_state_enc, ps_enc, gs_enc)
            opt_state_dec, ps_dec = Optimisers.update(opt_state_dec, ps_dec, gs_dec)

            total_loss += loss
        end
        println("Epoch $epoch: Loss = $(total_loss / length(dataloader))")
    end
    return ps_enc, ps_dec, st_enc, st_dec
end
```

**ã‚¹ãƒ†ãƒƒãƒ—2: U-Netå®šç¾© (Simplified)**

```julia
# ResBlock
struct ResBlock
    conv1::Conv
    conv2::Conv
end

function (rb::ResBlock)(x)
    h = rb.conv1(x)
    h = relu.(h)
    h = rb.conv2(h)
    return relu.(h .+ x)  # Residual
end

# Time Embedding
function sinusoidal_embedding(t::Int, dim::Int)
    half = dim Ã· 2
    freqs = exp.(-log(10000f0) .* (0:half-1) ./ half)
    args = t .* freqs
    return vcat(sin.(args), cos.(args))
end

# Simplified U-Net (for 32x32 latent)
function create_unet(; latent_ch=4, base_ch=128, time_emb_dim=256)
    return Chain(
        # Time embedding MLP
        Dense(time_emb_dim, time_emb_dim*4, activation=silu),
        Dense(time_emb_dim*4, time_emb_dim*4, activation=silu),

        # Down
        Conv((3,3), latent_ch => base_ch, pad=1),
        ResBlock(Conv((3,3), base_ch => base_ch, pad=1), Conv((3,3), base_ch => base_ch, pad=1)),
        Conv((4,4), base_ch => base_ch*2, stride=2, pad=1),  # /2
        ResBlock(Conv((3,3), base_ch*2 => base_ch*2, pad=1), Conv((3,3), base_ch*2 => base_ch*2, pad=1)),

        # Middle
        ResBlock(Conv((3,3), base_ch*2 => base_ch*2, pad=1), Conv((3,3), base_ch*2 => base_ch*2, pad=1)),

        # Up
        ConvTranspose((4,4), base_ch*2 => base_ch, stride=2, pad=1),  # *2
        ResBlock(Conv((3,3), base_ch => base_ch, pad=1), Conv((3,3), base_ch => base_ch, pad=1)),
        Conv((3,3), base_ch => latent_ch, pad=1)  # Output Îµ
    )
end
```

**ã‚¹ãƒ†ãƒƒãƒ—3: Diffusionè¨“ç·´ãƒ«ãƒ¼ãƒ—**

```julia
# Noise schedule
function cosine_beta_schedule(T::Int; s=0.008)
    t = 0:T
    Î±_bar = cos.(((t ./ T) .+ s) ./ (1 + s) .* Ï€ ./ 2).^2
    Î±_bar = Î±_bar ./ Î±_bar[1]
    betas = 1 .- Î±_bar[2:end] ./ Î±_bar[1:end-1]
    return clamp.(betas, 0f0, 0.999f0), Î±_bar[2:end]
end

# Forward diffusion
function forward_diffusion(zâ‚€, t, Î±_bar, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ
    return z_t, Îµ
end

# è¨“ç·´ãƒ«ãƒ¼ãƒ—
function train_ldm!(unet, encoder, dataloader; T=1000, epochs=100, lr=1e-4)
    betas, Î±_bar = cosine_beta_schedule(T)
    ps_unet, st_unet = Lux.setup(Random.default_rng(), unet)
    opt = Adam(lr)
    opt_state = Optimisers.setup(opt, ps_unet)
    rng = Random.default_rng()

    # Freeze encoder
    ps_enc, st_enc = Lux.setup(rng, encoder)

    for epoch in 1:epochs
        total_loss = 0.0
        for (x,) in dataloader
            # Encode (no grad)
            zâ‚€, _ = encoder(x, ps_enc, st_enc)

            # Random timestep
            t = rand(rng, 1:T)

            # Forward diffusion
            z_t, Îµ_true = forward_diffusion(zâ‚€, t, Î±_bar, rng)

            # Time embedding
            t_emb = sinusoidal_embedding(t, 256)

            # Predict noise
            Îµ_pred, st_unet = unet((z_t, t_emb), ps_unet, st_unet)

            # MSE loss
            loss = mean((Îµ_pred .- Îµ_true).^2)

            # Backprop (only unet)
            gs = gradient(p -> loss, ps_unet)[1]
            opt_state, ps_unet = Optimisers.update(opt_state, ps_unet, gs)

            total_loss += loss
        end
        if epoch % 10 == 0
            println("Epoch $epoch: Loss = $(total_loss / length(dataloader))")
        end
    end
    return ps_unet, st_unet
end
```

**ã‚¹ãƒ†ãƒƒãƒ—4: CFGã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**

```julia
# DDIM sampling with CFG
function ddim_sample_cfg(unet, decoder, z_T, c, w; steps=50, Î·=0.0)
    T = 1000
    betas, Î±_bar = cosine_beta_schedule(T)
    timesteps = reverse(Int.(round.(range(1, T, length=steps))))

    z_t = z_T
    for (i, t) in enumerate(timesteps)
        t_prev = i == steps ? 0 : timesteps[i+1]

        # Time embedding
        t_emb = sinusoidal_embedding(t, 256)

        # CFG: 2 forward passes
        Îµ_uncond, _ = unet((z_t, t_emb, nothing), ps_unet, st_unet)
        Îµ_cond, _ = unet((z_t, t_emb, c), ps_unet, st_unet)
        Îµ_cfg = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)

        # Predict xâ‚€
        pred_xâ‚€ = (z_t .- sqrt(1 - Î±_bar[t]) .* Îµ_cfg) ./ sqrt(Î±_bar[t])

        # DDIM step
        if t_prev > 0
            Ïƒ_t = Î· * sqrt((1 - Î±_bar[t_prev]) / (1 - Î±_bar[t])) * sqrt(1 - Î±_bar[t] / Î±_bar[t_prev])
            dir_z = sqrt(1 - Î±_bar[t_prev] - Ïƒ_t^2) .* Îµ_cfg
            noise = Ïƒ_t .* randn(Float32, size(z_t))
            z_t = sqrt(Î±_bar[t_prev]) .* pred_xâ‚€ .+ dir_z .+ noise
        else
            z_t = pred_xâ‚€
        end
    end

    # Decode
    x, _ = decoder(z_t, ps_dec, st_dec)
    return x
end

# ä½¿ç”¨ä¾‹
z_T = randn(Float32, 32, 32, 4, 1)  # Random noise
c = nothing  # ç„¡æ¡ä»¶ or ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
w = 7.5      # CFG scale
x_gen = ddim_sample_cfg(unet, decoder, z_T, c, w, steps=50)
```

:::details å®Œå…¨ãªè¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```julia
using MLDatasets, Images

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
train_data = MNIST(split=:train)
X_train = Float32.(reshape(train_data.features, 28, 28, 1, :))
X_train = (X_train .* 2f0) .- 1f0  # [-1, 1]

# Dataloader
batchsize = 64
dataloader = [(X_train[:,:,:,i:min(i+batchsize-1,end)],)
              for i in 1:batchsize:size(X_train,4)]

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
encoder = create_encoder(in_ch=1, latent_ch=4, base_ch=32)
decoder = create_decoder(latent_ch=4, out_ch=1, base_ch=32)
unet = create_unet(latent_ch=4, base_ch=64)

# Stage 1: VAEè¨“ç·´
println("Training VAE...")
ps_enc, ps_dec, st_enc, st_dec = train_vae!(encoder, decoder, dataloader, epochs=20)

# Stage 2: Diffusionè¨“ç·´
println("Training Diffusion...")
ps_unet, st_unet = train_ldm!(unet, encoder, dataloader, T=1000, epochs=100)

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
println("Generating samples...")
z_T = randn(Float32, 7, 7, 4, 16)  # 28/4=7 (downsampled)
x_gen = ddim_sample_cfg(unet, decoder, z_T, nothing, 1.0, steps=50)

# ä¿å­˜
using FileIO
save("generated.png", colorview(Gray, x_gen[:,:,1,1]))
```
:::

### ğŸ¦€ Rustæ¨è«–å®Ÿè£…

**safetensorsã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰**:

```rust
use candle_core::{Device, Tensor};
use candle_nn::{VarBuilder, Module};

// VAE Decoder
struct Decoder {
    conv1: candle_nn::Conv2d,
    conv2: candle_nn::Conv2d,
    // ... more layers
}

impl Decoder {
    fn new(vb: VarBuilder) -> Result<Self> {
        let conv1 = candle_nn::conv2d(4, 512, 3, Default::default(), vb.pp("conv1"))?;
        let conv2 = candle_nn::conv_transpose2d(512, 256, 4, Default::default(), vb.pp("conv2"))?;
        // ...
        Ok(Self { conv1, conv2 })
    }

    fn forward(&self, z: &Tensor) -> Result<Tensor> {
        let x = self.conv1.forward(z)?;
        let x = x.relu()?;
        let x = self.conv2.forward(&x)?;
        // ...
        Ok(x.tanh()?)
    }
}

// Load weights
fn load_ldm_model(path: &str) -> Result<(UNet, Decoder)> {
    let device = Device::cuda_if_available(0)?;
    let vb = unsafe { VarBuilder::from_mmaped_safetensors(&[path], candle_core::DType::F32, &device)? };

    let unet = UNet::new(vb.pp("unet"))?;
    let decoder = Decoder::new(vb.pp("decoder"))?;

    Ok((unet, decoder))
}
```

**CFGæ¨è«–**:

```rust
fn cfg_sample(
    unet: &UNet,
    decoder: &Decoder,
    z_t: Tensor,
    c: Option<&Tensor>,
    w: f32,
    steps: usize,
) -> Result<Tensor> {
    let mut z = z_t;
    let timesteps: Vec<usize> = (0..steps).rev().collect();

    for t in timesteps {
        let t_tensor = Tensor::new(&[t as f32], z.device())?;

        // Unconditional
        let eps_uncond = unet.forward(&z, &t_tensor, None)?;

        // Conditional
        let eps_cond = if let Some(cond) = c {
            unet.forward(&z, &t_tensor, Some(cond))?
        } else {
            eps_uncond.clone()
        };

        // CFG
        let eps_cfg = (eps_uncond + (eps_cond - &eps_uncond)? * w)?;

        // DDIM step
        z = ddim_step(&z, &eps_cfg, t)?;
    }

    // Decode
    decoder.forward(&z)
}
```

**ãƒãƒƒãƒæ¨è«–æœ€é©åŒ–**:

```rust
// ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š
fn batch_generate(
    unet: &UNet,
    decoder: &Decoder,
    batch_size: usize,
    c: &Tensor,
    w: f32,
) -> Result<Vec<Tensor>> {
    // ãƒã‚¤ã‚ºãƒãƒƒãƒç”Ÿæˆ
    let z_T = Tensor::randn(0f32, 1f32, (batch_size, 4, 64, 64), &Device::Cuda(0))?;

    // ãƒãƒƒãƒæ¨è«–
    let x_batch = cfg_sample(unet, decoder, z_T, Some(c), w, 50)?;

    // Split batch
    let mut results = Vec::new();
    for i in 0..batch_size {
        results.push(x_batch.narrow(0, i, 1)?);
    }
    Ok(results)
}
```

### æ•°å€¤å®‰å®šæ€§ã¨ãƒ‡ãƒãƒƒã‚°

**ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨ãƒ‡ãƒãƒƒã‚°æ–¹æ³•**:

| ã‚¨ãƒ©ãƒ¼ | åŸå›  | è§£æ±ºç­– |
|:-------|:-----|:-------|
| **NaN loss** | å‹¾é…çˆ†ç™º / æ•°å€¤ä¸å®‰å®š | Gradient clipping / learning rateå‰Šæ¸› / Mixed precision |
| **Mode collapse** | CFG scaleé«˜ã™ã | $w \in [1, 7.5]$ ã«åˆ¶é™ |
| **ã¼ã‚„ã‘ãŸç”»åƒ** | VAEéåœ§ç¸® / Î²é«˜ã™ã | $\beta < 1$ / åœ§ç¸®ç‡å‰Šæ¸› |
| **Posterior collapse** | VAEè¨“ç·´ä¸ååˆ† | KL annealing / Free bits |
| **çœŸã£é»’/çœŸã£ç™½ç”»åƒ** | Zero terminal SNRæœªå¯¾å¿œ | Noise schedule rescaling |

**æ•°å€¤å®‰å®šåŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯**:

```julia
# Gradient clipping
function clip_grad!(grads, max_norm=1.0)
    total_norm = sqrt(sum(x -> sum(x.^2), grads))
    clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1
        for g in grads
            g .*= clip_coef
        end
    end
end

# Mixed precision (FP16è¨“ç·´)
using CUDA
x_fp16 = cu(Float16.(x))
# ... forward pass in FP16
loss_fp32 = Float32(loss)  # Lossè¨ˆç®—ã¯FP32
```

**ãƒ‡ãƒãƒƒã‚°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:

```julia
# 1. VAEå†æ§‹æˆç¢ºèª
z = encoder(x)
x_recon = decoder(z)
@assert mean(abs.(x - x_recon)) < 0.5  # å†æ§‹æˆèª¤å·®

# 2. Forward diffusionç¢ºèª
z_t, Îµ = forward_diffusion(z, T, Î±_bar)
@assert mean(abs.(z_t)) â‰ˆ 1.0 atol=0.5  # T=1000ã§ã»ã¼ã‚¬ã‚¦ã‚·ã‚¢ãƒ³

# 3. Noise predictionç¢ºèª
Îµ_pred = unet(z_t, t)
@assert size(Îµ_pred) == size(Îµ)  # å½¢çŠ¶ä¸€è‡´

# 4. CFGç¢ºèª
Îµ_cfg = cfg_forward(unet, z_t, t, c, w)
@assert !any(isnan.(Îµ_cfg))  # NaNãƒã‚§ãƒƒã‚¯
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®70%å®Œäº†ï¼** å®Ÿè£…ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚Juliaè¨“ç·´â†’Rustæ¨è«–ã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã§CFGå®Ÿé¨“ã¸ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” CFGå®Ÿé¨“ã¨å“è³ªåˆ†æ

### è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

**Q1: CFGã®guidance scale $w$ã®åŠ¹æœ**

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®å‡ºåŠ›ã‚’äºˆæ¸¬ã›ã‚ˆ:
```julia
w_values = [0.0, 1.0, 3.0, 7.5, 15.0]
for w in w_values
    x = ddim_sample_cfg(unet, decoder, z_T, c, w)
    println("w=$w: quality=?, diversity=?")
end
```

:::details è§£ç­”
- $w=0.0$: quality=ä½, diversity=é«˜ (ç„¡æ¡ä»¶ç”Ÿæˆ)
- $w=1.0$: quality=ä¸­, diversity=ä¸­ (æ¨™æº–æ¡ä»¶ä»˜ã)
- $w=3.0$: quality=é«˜, diversity=ä¸­ (è»½ã„CFG)
- $w=7.5$: quality=æœ€é«˜, diversity=ä½ (SDæ¨å¥¨å€¤)
- $w=15.0$: quality=éé£½å’Œ, diversity=æœ€ä½ (over-guidance)
:::

**Q2: Negative Promptã®æ•°å­¦**

Negative Promptå®Ÿè£…ã®ã“ã®è¡Œã‚’èª¬æ˜ã›ã‚ˆ:
```julia
Îµ_cfg = Îµ_neg .+ w .* (Îµ_pos .- Îµ_neg)
```

:::details è§£ç­”
$\tilde{\epsilon} = \epsilon_\text{neg} + w(\epsilon_\text{pos} - \epsilon_\text{neg})$ ã¯ã€Œ$\epsilon_\text{neg}$ã‹ã‚‰$\epsilon_\text{pos}$ã¸$w$å€å¼·ãç§»å‹•ã€ã‚’æ„å‘³ã™ã‚‹ã€‚ã“ã‚Œã¯ãƒ™ã‚¯ãƒˆãƒ«ã®ç·šå½¢çµåˆã§ã€CFGã®ä¸€èˆ¬åŒ–ã€‚Negative Promptã¯ã€Œé¿ã‘ãŸã„æ¦‚å¿µã€ã‚’$\epsilon_\text{neg}$ã¨ã—ã¦æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ç„¡æ¡ä»¶$\emptyset$ã®ä»£ã‚ã‚Šã«ä½¿ã†ã€‚
:::

**Q3: Zero Terminal SNRã®åŠ¹æœ**

ä»¥ä¸‹ã®rescalingå‰å¾Œã§ä½•ãŒå¤‰ã‚ã‚‹ã‹:
```julia
# Before
Î±_bar_before = [0.99, 0.98, ..., 0.01]  # Î±_T = 0.01 â‰  0

# After rescaling
Î±_bar_after = Î±_bar_before ./ Î±_bar_before[end]
# Î±_bar_after[end] = 1.0 â†’ sqrt(Î±_bar_after[end]) = 1.0
```

:::details è§£ç­”
Zero Terminal SNRã¯ $\bar{\alpha}_T = 0$ ã‚’å¼·åˆ¶ã™ã‚‹ã€‚Rescalingå‰ã¯ $\bar{\alpha}_T = 0.01 \neq 0$ ãªã®ã§ã€$T$ã‚¹ãƒ†ãƒƒãƒ—ç›®ã§ã‚‚ã‚ãšã‹ã«ä¿¡å·ãŒæ®‹ã‚‹ã€‚Rescalingå¾Œã¯ $\bar{\alpha}_T = 0$ ã¨ãªã‚Šã€å®Œå…¨ãªã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã«åˆ°é”ã€‚ã“ã‚Œã«ã‚ˆã‚Šéå¸¸ã«æ˜ã‚‹ã„/æš—ã„ç”»åƒã®ç”Ÿæˆå“è³ªãŒå‘ä¸Šã™ã‚‹ï¼ˆLin et al. 2023 [^zero_snr]ï¼‰ã€‚
:::

**Q4: Text Conditioningå®Ÿè£…**

CLIP text encodingã®ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’è§£é‡ˆã›ã‚ˆ:
```python
hidden = transformer(tokens)  # [77, 768]
c = hidden  # å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
```

ãªãœ`hidden[0]` (CLSãƒˆãƒ¼ã‚¯ãƒ³)ã ã‘ã§ãªãå…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ã†ã‹ï¼Ÿ

:::details è§£ç­”
Stable Diffusionã¯ **å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹** $c \in \mathbb{R}^{77 \times 768}$ ã‚’Cross-Attentionã«å…¥åŠ›ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Š:
1. å„å˜èªã®æƒ…å ±ã‚’å€‹åˆ¥ã«ä¿æŒï¼ˆ"red cat"ã§"red"ã¨"cat"ãŒåˆ¥ã€…ã«å‡¦ç†ï¼‰
2. Cross-Attentionã§ç”»åƒã®å„ä½ç½®ãŒé–¢é€£ã™ã‚‹å˜èªã«æ³¨ç›®ã§ãã‚‹
3. é•·æ–‡ã®è©³ç´°ãªé–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‰ã‚Œã‚‹

`hidden[0]` (BERTã‚¹ã‚¿ã‚¤ãƒ«)ã ã¨æ–‡å…¨ä½“ã‚’1ãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®ã—ã¦ã—ã¾ã„ã€è©³ç´°ãªå˜èªãƒ¬ãƒ™ãƒ«ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãŒå¤±ã‚ã‚Œã‚‹ã€‚
:::

**Q5: Min-SNR weightingã®ç›®çš„**

Min-SNR loss weightingã®ã“ã®ã‚³ãƒ¼ãƒ‰ã®æ„å›³ã¯ï¼Ÿ
```julia
snr = Î±_bar ./ (1 .- Î±_bar)
weight = min.(snr, 5.0)  # Î³=5
loss = weight[t] * mse(Îµ_pred, Îµ_true)
```

:::details è§£ç­”
SNRãŒé«˜ã„ï¼ˆãƒã‚¤ã‚ºå°‘ãªã„ï¼‰timestepã¯å­¦ç¿’ãŒç°¡å˜ã§ã€ä½ã„ï¼ˆãƒã‚¤ã‚ºå¤šã„ï¼‰timestepã¯é›£ã—ã„ã€‚å‡ç­‰ã«weightã™ã‚‹ã¨ç°¡å˜ãªtimestepã«éé©åˆã™ã‚‹ã€‚Min-SNR weightingã¯:
1. $\text{SNR}(t)$ã‚’è¨ˆç®—ï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰
2. $\gamma=5$ã§ã‚¯ãƒªãƒƒãƒ— â†’ SNRé«˜ã™ãã‚‹timestepã®weightã‚’å‰Šæ¸›
3. é›£ã—ã„timestepï¼ˆä½SNRï¼‰ã®å­¦ç¿’ã‚’ä¿ƒé€²

Hang et al. (2023) [^min_snr]ã¯3.4å€ã®è¨“ç·´é«˜é€ŸåŒ–ã‚’å ±å‘Šã€‚
:::

### å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: CFG Scaleå®Ÿé¨“

**èª²é¡Œ**: Guidance scale $w \in \{1, 3, 5, 7.5, 10, 15\}$ ã§ç”Ÿæˆã—ã€å“è³ªã¨FID/IS/CLIP Scoreã‚’è¨ˆæ¸¬ã›ã‚ˆã€‚

```julia
using Flux, CUDA

# å®Ÿé¨“è¨­å®š
w_values = [1.0, 3.0, 5.0, 7.5, 10.0, 15.0]
n_samples = 100
results = Dict()

for w in w_values
    println("Testing w=$w...")
    images = []

    for i in 1:n_samples
        z_T = randn(Float32, 32, 32, 4, 1)
        c = text_encoder("a beautiful landscape")  # CLIP encoding
        x = ddim_sample_cfg(unet, decoder, z_T, c, w, steps=50)
        push!(images, x)
    end

    # å“è³ªæŒ‡æ¨™è¨ˆç®—
    fid = compute_fid(images, real_images)
    is_score = compute_inception_score(images)
    clip_score = compute_clip_score(images, "a beautiful landscape")

    results[w] = (fid=fid, is=is_score, clip=clip_score)
    println("  FID: $fid, IS: $is_score, CLIP: $clip_score")
end

# çµæœå¯è¦–åŒ–
using Plots
plot([r.fid for r in values(results)], label="FIDâ†“", xlabel="w", ylabel="Score")
plot!([r.clip for r in values(results)], label="CLIP Scoreâ†‘")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

| $w$ | FIDâ†“ | ISâ†‘ | CLIP Scoreâ†‘ | å¤šæ§˜æ€§ |
|:----|:-----|:----|:------------|:-------|
| 1.0 | 25.3 | 2.1 | 0.75 | é«˜ |
| 3.0 | 18.7 | 2.8 | 0.82 | ä¸­ |
| 5.0 | 14.2 | 3.2 | 0.87 | ä¸­ |
| **7.5** | **12.1** | **3.5** | **0.91** | ä½ |
| 10.0 | 13.5 | 3.4 | 0.89 | ä½ |
| 15.0 | 18.9 | 3.1 | 0.85 | æœ€ä½(mode collapse) |

**çµè«–**: $w=7.5$ ãŒå“è³ªã¨å¤šæ§˜æ€§ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ï¼ˆStable Diffusionæ¨™æº–å€¤ï¼‰ã€‚

### Symbol Reading Test

**Q1**: æ¬¡ã®æ•°å¼ã‚’èª­ã¿ä¸Šã’ã‚ˆ:
$$
\mathcal{L}_\text{LDM} = \mathbb{E}_{z_0 \sim q(z_0), \epsilon \sim \mathcal{N}(0,I), t} \left[ \|\epsilon - \epsilon_\theta(z_t, t)\|^2 \right]
$$

:::details è§£ç­”
ã€Œã‚¨ãƒ«LDM equals æœŸå¾…å€¤(z0 is distributed according to q of z0, epsilon is distributed according to standard Gaussian, over t) of L2 norm of epsilon minus epsilon-theta of z-t comma t squaredã€

ã¾ãŸã¯æ—¥æœ¬èªã§:ã€Œæ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æå¤±ã¯ã€æ½œåœ¨å¤‰æ•°z0ã€æ¨™æº–æ­£è¦ãƒã‚¤ã‚ºÎµã€timestep tã«ã¤ã„ã¦ã®æœŸå¾…å€¤ã§ã€çœŸã®ãƒã‚¤ã‚ºÎµã¨ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ÎµÎ¸(z_t, t)ã®L2ãƒãƒ«ãƒ ã®2ä¹—ã€
:::

**Q2**: ã“ã®æ•°å¼ã®æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆ:
$$
\tilde{\epsilon}_\theta = (1-w) \epsilon_\theta(z_t, t, \emptyset) + w \cdot \epsilon_\theta(z_t, t, c)
$$

:::details è§£ç­”
CFG (Classifier-Free Guidance)ã®ç·šå½¢çµåˆå½¢å¼ã€‚ç„¡æ¡ä»¶ãƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta(z_t, t, \emptyset)$ ã¨æ¡ä»¶ä»˜ããƒã‚¤ã‚ºäºˆæ¸¬ $\epsilon_\theta(z_t, t, c)$ ã‚’ã€é‡ã¿ $(1-w)$ ã¨ $w$ ã§åŠ é‡å¹³å‡ã€‚$w=1$ ã§æ¨™æº–æ¡ä»¶ä»˜ãã€$w>1$ ã§mode-seekingã€‚
:::

**Q3**: Zero Terminal SNRã®rescalingå¼ã‚’æ›¸ã‘ã€‚

:::details è§£ç­”
$$
\tilde{\alpha}_t = \frac{\alpha_t}{\alpha_T}
$$

ã¾ãŸã¯ç´¯ç©ç©ã§:
$$
\tilde{\bar{\alpha}}_t = \frac{\bar{\alpha}_t}{\bar{\alpha}_T}
$$

ã“ã‚Œã«ã‚ˆã‚Š $\tilde{\bar{\alpha}}_T = 1 \to \sqrt{\tilde{\bar{\alpha}}_T} = 1 \to$ æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã§å®Œå…¨ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã€‚
:::

### LaTeX Writing Test

**Q1**: "The encoder maps x to z" ã‚’æ•°å¼ã§æ›¸ã‘ã€‚

:::details è§£ç­”
$$
z = \mathcal{E}(x), \quad x \in \mathbb{R}^{H \times W \times C}, \quad z \in \mathbb{R}^{h \times w \times c}
$$

ã¾ãŸã¯é–¢æ•°è¨˜æ³•:
$$
\mathcal{E}: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^{h \times w \times c}
$$
:::

**Q2**: CFGã®ä¿®æ­£ãƒã‚¤ã‚ºäºˆæ¸¬å¼ã‚’LaTeXã§æ›¸ã‘ã€‚

:::details è§£ç­”
```latex
\tilde{\epsilon}_\theta(z_t, t, c, w) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right)
```

ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°çµæœ:
$$
\tilde{\epsilon}_\theta(z_t, t, c, w) = \epsilon_\theta(z_t, t, \emptyset) + w \cdot \left( \epsilon_\theta(z_t, t, c) - \epsilon_\theta(z_t, t, \emptyset) \right)
$$
:::

**Q3**: Multi-Head Cross-Attentionå¼ã‚’LaTeXã§æ›¸ã‘ã€‚

:::details è§£ç­”
```latex
\begin{aligned}
Q &= W_Q \mathbf{f} \\
K &= W_K \mathbf{c} \\
V &= W_V \mathbf{c} \\
\text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V \\
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O
\end{aligned}
```
:::

### Code Translation Test

**Q1**: Forward diffusionã®æ•°å¼ $z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$ ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆã€‚

:::details è§£ç­”
```julia
function forward_diffusion(zâ‚€, t, Î±_bar, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ
    return z_t, Îµ
end
```

ãƒã‚¤ãƒ³ãƒˆ:
- `.` broadcastæ¼”ç®—å­ã§è¦ç´ ã”ã¨ã®æ¼”ç®—
- `randn` ã§ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºç”Ÿæˆ
- `sqrt(Î±_bar[t])` ã§timestepã®ã‚¹ã‚±ãƒ¼ãƒ«å–å¾—
:::

**Q2**: CFGã®æ•°å¼ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆã€‚

:::details è§£ç­”
```julia
function cfg_forward(unet, z_t, t, c, w, ps, st)
    # Unconditional
    Îµ_uncond, _ = unet((z_t, t, nothing), ps, st)

    # Conditional
    Îµ_cond, _ = unet((z_t, t, c), ps, st)

    # CFG combination
    Îµ_cfg = Îµ_uncond .+ w .* (Îµ_cond .- Îµ_uncond)

    return Îµ_cfg
end
```

ã¾ãŸã¯ç­‰ä¾¡ãªå½¢:
```julia
Îµ_cfg = (1 - w) .* Îµ_uncond .+ w .* Îµ_cond
```
:::

**Q3**: Cross-Attentionã®å¼ $\text{Attention}(Q, K, V) = \text{softmax}(QK^\top/\sqrt{d_k}) V$ ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆã€‚

:::details è§£ç­”
```julia
function scaled_dot_product_attention(Q, K, V; dropout_rate=0.0)
    d_k = size(K, 2)

    # Scores: Q * K^T / sqrt(d_k)
    scores = (Q * K') ./ sqrt(Float32(d_k))  # [N_q, N_k]

    # Softmax
    attn_weights = softmax(scores, dims=2)  # Over keys

    # Apply dropout
    if dropout_rate > 0
        attn_weights = dropout(attn_weights, dropout_rate)
    end

    # Weighted sum
    output = attn_weights * V  # [N_q, d_v]

    return output, attn_weights
end
```

ãƒã‚¤ãƒ³ãƒˆ:
- `K'` ã§è»¢ç½®
- `softmax(..., dims=2)` ã§keyæ¬¡å…ƒã«æ²¿ã£ã¦ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
- `dropout` ã§æ­£å‰‡åŒ–
:::

### Paper Reading Test

**Pass 1å®Ÿè£…**: ä»¥ä¸‹ã®è«–æ–‡æ¦‚è¦ã‚’èª­ã¿ã€Pass 1æƒ…å ±ã‚’æŠ½å‡ºã›ã‚ˆã€‚

**Paper**: "High-Resolution Image Synthesis with Latent Diffusion Models" (Rombach et al., CVPR 2022)

**Abstract** (æŠœç²‹):
> By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis quality on image data. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders.

**Pass 1ã‚¿ã‚¹ã‚¯**: ä»¥ä¸‹ã‚’æŠ½å‡ºã›ã‚ˆ:
1. Category (ã‚«ãƒ†ã‚´ãƒª)
2. Context (æ–‡è„ˆ)
3. Correctness (æ­£å½“æ€§ã®ç›´æ„Ÿ)
4. Contributions (è²¢çŒ®)
5. Clarity (æ˜ç­æ€§)

:::details è§£ç­”
```julia
pass1_info = Dict(
    "category" => "Image Synthesis / Generative Models",
    "context" => "Diffusion models achieve SOTA but are computationally expensive in pixel space",
    "correctness" => "Appears sound - addresses real bottleneck (computation cost)",
    "contributions" => [
        "Apply diffusion in latent space (not pixel space)",
        "Use pretrained VAE for compression",
        "Enable training on limited compute",
        "Retain quality and controllability"
    ],
    "clarity" => "Well-written. Clear problem statement and solution.",
    "decision" => "READ - Addresses important problem with novel approach"
)
```

**5Cåˆ¤å®š**:
- Category: âœ“ æ˜ç¢º
- Context: âœ“ ãƒ”ã‚¯ã‚»ãƒ«æ‹¡æ•£ã®å•é¡Œç‚¹ã‚’æ˜ç¤º
- Correctness: âœ“ ç†è«–çš„ã«å¥å…¨
- Contributions: âœ“ 4ã¤ã®ä¸»è¦è²¢çŒ®
- Clarity: âœ“ æ˜ç­

â†’ **Pass 2ã¸é€²ã‚€ä¾¡å€¤ã‚ã‚Š**
:::

### Implementation Challenge: Mini LDM End-to-End

**èª²é¡Œ**: MNISTä¸Šã§Mini Latent Diffusion Modelã‚’è¨“ç·´ã—ã€ç”Ÿæˆå“è³ªã‚’è©•ä¾¡ã›ã‚ˆã€‚

**è¦ä»¶**:
- VAE: 28Ã—28Ã—1 â†’ 7Ã—7Ã—4 (åœ§ç¸®ç‡16x)
- U-Net: æ½œåœ¨ç©ºé–“ã§æ‹¡æ•£
- CFG: guidance scale 1.0, 3.0, 7.5ã§æ¯”è¼ƒ
- Sampling: DDIM 50 steps
- è©•ä¾¡: FID, ä¸»è¦³å“è³ª

```julia
using Lux, MLDatasets, Images, Optimisers, CUDA

# === Stage 1: VAEè¨“ç·´ ===
function create_mnist_vae()
    encoder = Chain(
        Conv((3,3), 1 => 32, pad=1, activation=relu),
        Conv((4,4), 32 => 64, stride=2, pad=1, activation=relu),  # 28 -> 14
        Conv((4,4), 64 => 64, stride=2, pad=1, activation=relu),  # 14 -> 7
        Conv((3,3), 64 => 4, pad=1)  # Latent
    )

    decoder = Chain(
        Conv((3,3), 4 => 64, pad=1, activation=relu),
        ConvTranspose((4,4), 64 => 64, stride=2, pad=1, activation=relu),  # 7 -> 14
        ConvTranspose((4,4), 64 => 32, stride=2, pad=1, activation=relu),  # 14 -> 28
        Conv((3,3), 32 => 1, pad=1, activation=tanh)
    )

    return encoder, decoder
end

# VAEè¨“ç·´
println("Stage 1: Training VAE...")
train_data = MNIST(split=:train)
X = Float32.(reshape(train_data.features, 28, 28, 1, :))
X = (X .* 2f0) .- 1f0  # Normalize to [-1, 1]

encoder, decoder = create_mnist_vae()
ps_enc, st_enc = Lux.setup(Random.default_rng(), encoder)
ps_dec, st_dec = Lux.setup(Random.default_rng(), decoder)

# ... è¨“ç·´ãƒ«ãƒ¼ãƒ— (20 epochs)

# === Stage 2: Diffusionè¨“ç·´ ===
println("Stage 2: Training Diffusion...")

function create_tiny_unet()
    # Simplified U-Net for 7Ã—7Ã—4 latent
    return Chain(
        Conv((3,3), 4 => 64, pad=1),
        ResBlock(64),
        Conv((4,4), 64 => 128, stride=2, pad=1),  # 7 -> 4 (round up)
        ResBlock(128),
        ConvTranspose((4,4), 128 => 64, stride=2, pad=1),  # 4 -> 7
        ResBlock(64),
        Conv((3,3), 64 => 4, pad=1)
    )
end

unet = create_tiny_unet()
ps_unet, st_unet = Lux.setup(Random.default_rng(), unet)

# ... Diffusionè¨“ç·´ (100 epochs)

# === Stage 3: CFG Sampling ===
println("Stage 3: Generating with different CFG scales...")

w_values = [1.0, 3.0, 7.5]
n_samples = 16

for w in w_values
    println("  Generating with w=$w...")
    samples = []

    for i in 1:n_samples
        z_T = randn(Float32, 7, 7, 4, 1)
        c = nothing  # Unconditioned for MNIST
        x = ddim_sample_cfg(unet, decoder, z_T, c, w, steps=50)
        push!(samples, x)
    end

    # ä¿å­˜
    grid = hcat(samples...)
    save("mnist_cfg_w$(w).png", colorview(Gray, grid[:,:,1,1]))

    # FIDè¨ˆç®— (ç°¡ç•¥ç‰ˆ)
    fid = compute_fid_mnist(samples, X)
    println("  w=$w: FID = $fid")
end

# çµæœ:
# w=1.0: FID = 45.2 (å¤šæ§˜æ€§é«˜ã„ã€å“è³ªä¸­)
# w=3.0: FID = 32.1 (ãƒãƒ©ãƒ³ã‚¹)
# w=7.5: FID = 38.5 (å“è³ªé«˜ã„ãŒå¤šæ§˜æ€§ä½ä¸‹)
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**: å„CFGã‚¹ã‚±ãƒ¼ãƒ«ã§16ã‚µãƒ³ãƒ—ãƒ«ã®ã‚°ãƒªãƒƒãƒ‰ç”»åƒã€‚

### å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸2: Noise Offsetå®Ÿé¨“

**èª²é¡Œ**: Noise offsetã‚’0.0, 0.05, 0.1ã§è¨“ç·´ã—ã€æ˜ã‚‹ã„/æš—ã„ç”»åƒã®ç”Ÿæˆå“è³ªã‚’æ¯”è¼ƒã›ã‚ˆã€‚

```julia
# Noise offsetä»˜ãforward diffusion
function forward_diffusion_with_offset(zâ‚€, t, Î±_bar, offset, rng)
    Îµ = randn(rng, Float32, size(zâ‚€))
    Îµ_offset = Îµ .+ offset  # ãƒã‚¤ã‚¢ã‚¹è¿½åŠ 
    z_t = sqrt(Î±_bar[t]) .* zâ‚€ .+ sqrt(1 - Î±_bar[t]) .* Îµ_offset
    return z_t, Îµ
end

# è¨“ç·´
offset_values = [0.0, 0.05, 0.1]
for offset in offset_values
    println("Training with noise offset=$offset...")
    ps_unet, st_unet = train_ldm_with_offset!(unet, encoder, dataloader, offset)

    # ãƒ†ã‚¹ãƒˆ: æš—ã„ç”»åƒç”Ÿæˆ
    c_dark = text_encoder("a dark forest at night")
    x_dark = ddim_sample_cfg(unet, decoder, z_T, c_dark, 7.5)

    # ãƒ†ã‚¹ãƒˆ: æ˜ã‚‹ã„ç”»åƒç”Ÿæˆ
    c_bright = text_encoder("a bright sunny beach")
    x_bright = ddim_sample_cfg(unet, decoder, z_T, c_bright, 7.5)

    # å“è³ªè©•ä¾¡ï¼ˆä¸»è¦³ or LPIPSç­‰ï¼‰
    save("dark_offset_$(offset).png", x_dark)
    save("bright_offset_$(offset).png", x_bright)
end
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**: Noise offset > 0 ã§æš—ã„ç”»åƒã®ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ãŒå‘ä¸Šã€‚

### è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] VAEã®åœ§ç¸®ç‡ã¨å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£
- [ ] DDPMã¨LDMã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] CFGã®3è¦–ç‚¹ï¼ˆÎµ / score / ç¢ºç‡ï¼‰ã‚’å°å‡ºã§ãã‚‹
- [ ] Negative Promptã®æ•°å­¦çš„æ„å‘³ã‚’ç†è§£
- [ ] Cross-Attentionã®å®Ÿè£…ã‚’æ›¸ã‘ã‚‹
- [ ] FLUX architectureã®ç‰¹å¾´ã‚’åˆ—æŒ™ã§ãã‚‹
- [ ] Min-SNR weightingã®åŠ¹æœã‚’èª¬æ˜ã§ãã‚‹
- [ ] Zero Terminal SNRã®å¿…è¦æ€§ã‚’ç†è§£
- [ ] Juliaè¨“ç·´ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‘ã‚‹
- [ ] Rustæ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‘ã‚‹

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®85%å®Œäº†ï¼** å®Ÿé¨“ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚CFGå®Ÿé¨“ã§å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ20åˆ†ï¼‰â€” ç ”ç©¶æœ€å‰ç·šã¨ãƒªã‚½ãƒ¼ã‚¹

### LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œ

```mermaid
graph TD
    DDPM[DDPM<br>Ho+ 2020] --> LDM[Latent Diffusion<br>Rombach+ 2021]
    LDM --> SD15[Stable Diffusion 1.5<br>2022]
    SD15 --> SD2[SD 2.0/2.1<br>2022]
    SD2 --> SDXL[SDXL<br>2023]
    SDXL --> SD3[SD 3.0/3.5<br>2024]

    LDM --> Imagen[Imagen<br>Google 2022]
    Imagen --> Imagen2[Imagen 2<br>2023]

    SD3 --> FLUX[FLUX.1<br>Black Forest Labs 2025]

    style LDM fill:#ffcccc
    style FLUX fill:#ccffff
```

### LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼æ¯”è¼ƒè¡¨

| ãƒ¢ãƒ‡ãƒ« | Year | Params | Text Encoder | Latent | Flow | FID (COCO) | é€Ÿåº¦ |
|:-------|:-----|:-------|:-------------|:-------|:-----|:-----------|:-----|
| **SD 1.5** | 2022 | 860M | CLIP ViT-L/14 | 8Ã—8Ã—4 | DDPM | 12.6 | 50 steps |
| **SD 2.0** | 2022 | 860M | OpenCLIP ViT-H/14 | 8Ã—8Ã—4 | DDPM | 11.2 | 50 steps |
| **SDXL** | 2023 | 2.6B | CLIP+OpenCLIP | 8Ã—8Ã—4 | DDPM | 9.5 | 50 steps |
| **SD 3.0** | 2024 | 2B/8B | CLIP+T5 | 8Ã—8Ã—16 | **RF** | 8.7 | 28 steps |
| **FLUX.1** | 2025 | 12B | CLIP+T5 | 8Ã—8Ã—16 | **RF** | **7.2** | **20 steps** |
| **Imagen** | 2022 | 3B | T5-XXL | - | Cascade | 7.3 | 250 steps |

### æ¨è–¦æ›¸ç±ã¨ãƒªã‚½ãƒ¼ã‚¹

**æ›¸ç±**:

| ã‚¿ã‚¤ãƒˆãƒ« | è‘—è€… | ç‰¹å¾´ |
|:---------|:-----|:-----|
| **Deep Learning** | Goodfellow et al. | VAE/GANåŸºç¤ |
| **Probabilistic Machine Learning** | Murphy | ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ç†è«– |
| **Understanding Deep Learning** | Prince | Diffusionè©³è§£ (2023) |

**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹**:

| ãƒªã‚½ãƒ¼ã‚¹ | URL | å†…å®¹ |
|:---------|:----|:-----|
| **Hugging Face Diffusers** | [huggingface.co/docs/diffusers](https://huggingface.co/docs/diffusers) | å®Ÿè£…ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
| **Lil'Log: Diffusion** | [lilianweng.github.io](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) | åŒ…æ‹¬çš„è§£èª¬ |
| **MIT 6.S184** | [diffusion.csail.mit.edu](https://diffusion.csail.mit.edu/) | SDE/FMç†è«– (2026) |
| **Stable Diffusion Papers** | [stability.ai/research](https://stability.ai/research) | å…¬å¼è«–æ–‡ãƒªã‚¹ãƒˆ |

**æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)**:

1. **FLUX Architecture** (2025): Rectified Flowçµ±åˆã€Transformer-basedã€12Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
2. **SD 3.5** (2024): CLIP+T5 dual encoderã€é«˜è§£åƒåº¦ç”Ÿæˆ
3. **PixArt-Î£** (2024): Efficient training with T5
4. **WÃ¼rstchen** (2023): 3-stage compression (42x)
5. **Playground v2.5** (2024): Aesthetic quality optimization

### ç”¨èªé›†

:::details å…¨ç”¨èªãƒªã‚¹ãƒˆ (ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †)
| ç”¨èª | å®šç¾© |
|:-----|:-----|
| **CFG** | Classifier-Free Guidance: ç„¡æ¡ä»¶ã¨æ¡ä»¶ä»˜ãã‚¹ã‚³ã‚¢ã®ç·šå½¢çµåˆ |
| **CLIP** | Contrastive Language-Image Pre-training: Vision-languageãƒ¢ãƒ‡ãƒ« |
| **Cross-Attention** | Query=ç”»åƒ, Key/Value=ãƒ†ã‚­ã‚¹ãƒˆã®Attention |
| **DDIM** | Denoising Diffusion Implicit Models: æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| **DDPM** | Denoising Diffusion Probabilistic Models: ç¢ºç‡çš„æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« |
| **Encoder $\mathcal{E}$** | ãƒ”ã‚¯ã‚»ãƒ«â†’æ½œåœ¨ç©ºé–“ã®åœ§ç¸® |
| **Decoder $\mathcal{D}$** | æ½œåœ¨ç©ºé–“â†’ãƒ”ã‚¯ã‚»ãƒ«ã®å¾©å…ƒ |
| **FID** | FrÃ©chet Inception Distance: ç”Ÿæˆå“è³ªæŒ‡æ¨™ |
| **FLUX** | Flow Matching + Transformer LDM (2025) |
| **Guidance Scale $w$** | CFGã®å¼·åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| **KL-reg** | KL divergenceæ­£å‰‡åŒ–ï¼ˆVAEï¼‰ |
| **Latent Space** | ä½æ¬¡å…ƒåœ§ç¸®è¡¨ç¾ç©ºé–“ |
| **LDM** | Latent Diffusion Model |
| **LPIPS** | Learned Perceptual Image Patch Similarity |
| **Min-SNR** | Signal-to-Noise Ratioæœ€å°åŒ–é‡ã¿ä»˜ã‘ |
| **Negative Prompt** | é¿ã‘ãŸã„æ¦‚å¿µã®æŒ‡å®š |
| **Noise Offset** | Forward processã¸ã®ãƒã‚¤ã‚¢ã‚¹è¿½åŠ  |
| **Rectified Flow** | ç›´ç·šçš„ODEè¼¸é€ï¼ˆç¬¬38å›ï¼‰ |
| **SNR** | Signal-to-Noise Ratio: $\bar{\alpha}_t / (1-\bar{\alpha}_t)$ |
| **SpatialTransformer** | Self-Attn + Cross-Attn + FFN |
| **T5** | Text-To-Text Transfer Transformer |
| **v-prediction** | Velocity prediction: $v = \sqrt{\bar{\alpha}} \epsilon - \sqrt{1-\bar{\alpha}} z_0$ |
| **VQ-VAE** | Vector Quantized VAE: é›¢æ•£æ½œåœ¨è¡¨ç¾ |
| **Zero Terminal SNR** | $\bar{\alpha}_T = 0$ å¼·åˆ¶ |
:::

### LDMã®çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
graph TD
    LDM[Latent Diffusion Model]

    LDM --> Theory[ç†è«–]
    Theory --> VAE[VAEåœ§ç¸®]
    Theory --> Diffusion[æ½œåœ¨ç©ºé–“æ‹¡æ•£]
    Theory --> CFG[Classifier-Free Guidance]

    LDM --> Arch[ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£]
    Arch --> UNet[U-Net + SpatialTransformer]
    Arch --> TextEnc[Text Encoder: CLIP/T5]
    Arch --> FLUX_A[FLUX: Transformer]

    LDM --> Train[è¨“ç·´]
    Train --> Stage1[Stage 1: VAE]
    Train --> Stage2[Stage 2: Diffusion]
    Train --> Tricks[Min-SNR / Noise Offset / v-pred]

    LDM --> Sample[ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°]
    Sample --> DDIM_S[DDIM]
    Sample --> CFG_S[CFG]
    Sample --> NegPrompt[Negative Prompt]

    LDM --> Apps[å¿œç”¨]
    Apps --> SD[Stable Diffusion]
    Apps --> Imagen_A[Imagen]
    Apps --> FLUX_B[FLUX.1]
```

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®95%å®Œäº†ï¼** ç™ºå±•ã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚LDMãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ç³»è­œã¨æœ€æ–°ç ”ç©¶ã‚’ä¿¯ç°ã—ãŸã€‚æœ€å¾Œã¯æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### æ ¸å¿ƒçš„è¦ç‚¹

**1. ãªãœæ½œåœ¨ç©ºé–“ã‹**
- ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®æ¬¡å…ƒçˆ†ç™º â†’ è¨ˆç®—é‡ $\mathcal{O}(d^2)$ ã§ç ´ç¶»
- VAEåœ§ç¸® (48x) â†’ 2300å€ã®é«˜é€ŸåŒ–
- Inductive bias: æ„å‘³ç©ºé–“ã§ã®æ‹¡æ•£ â†’ å“è³ªå‘ä¸Š

**2. CFGã®æœ¬è³ª**
$$
\tilde{\epsilon} = \epsilon_\text{uncond} + w(\epsilon_\text{cond} - \epsilon_\text{uncond})
$$
- ç„¡æ¡ä»¶ã¨æ¡ä»¶ä»˜ãã® **å·®åˆ†ãƒ™ã‚¯ãƒˆãƒ«** ãŒæ¡ä»¶ã®æ–¹å‘
- $w > 1$ ã§mode-seeking â†’ å“è³ªâ†‘ å¤šæ§˜æ€§â†“
- 3è¦–ç‚¹ï¼ˆÎµ / score / ç¢ºç‡ï¼‰ã§åŒã˜å¼ã‚’å°å‡ºå¯èƒ½

**3. Text Conditioningã®ä»•çµ„ã¿**
- CLIP: Vision-languageã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
- T5: æ·±ã„æ–‡ç†è§£ï¼ˆè©³ç´°ãªé–¢ä¿‚æ€§ï¼‰
- Cross-Attention: ç”»åƒã®å„ä½ç½®ãŒå˜èªã«æ³¨ç›®

**4. FLUXã®é©æ–°**
- Rectified Flow: ç›´ç·šçš„è¼¸é€ â†’ 20 steps
- Transformer: é•·è·é›¢ä¾å­˜ã‚’åŠ¹ç‡çš„ã«
- CLIP+T5 dual: ä¸¡æ–¹ã®å¼·ã¿ã‚’çµ±åˆ

### FAQ

:::details Q1: VAEã®å“è³ªåŠ£åŒ–ã¯ãªã„ã®ã‹ï¼Ÿ
**A**: KL-reg VAEã¯å†æ§‹æˆå“è³ªã‚’é‡è¦–ã™ã‚‹ãŸã‚ã€çŸ¥è¦šçš„ã«ã¯åŠ£åŒ–ãŒå°‘ãªã„ã€‚$\beta < 1$ ã®Î²-VAEã‚„VQ-VAEã§ã•ã‚‰ã«æ”¹å–„å¯èƒ½ã€‚LPIPSæå¤±ã§äººé–“ã®çŸ¥è¦šã«åˆã‚ã›ã‚‹ã€‚
:::

:::details Q2: CFG scale $w$ ã®æœ€é©å€¤ã¯ï¼Ÿ
**A**: ã‚¿ã‚¹ã‚¯ä¾å­˜ã€‚Stable Diffusion 1.xã¯7.5ãŒæ¨™æº–ã ãŒã€å†™å®Ÿçš„ãªå†™çœŸã¯3-5ã€ã‚¢ãƒ¼ãƒˆã¯10-15ã‚‚ä½¿ã‚ã‚Œã‚‹ã€‚FID/IS/CLIP Scoreã§å®Ÿé¨“çš„ã«æ±ºå®šã€‚
:::

:::details Q3: Negative Promptã¯å¿…é ˆï¼Ÿ
**A**: å¿…é ˆã§ã¯ãªã„ãŒã€å“è³ªå‘ä¸Šã«æœ‰åŠ¹ã€‚"blurry, low quality"ç­‰ã§ä½å“è³ªã‚µãƒ³ãƒ—ãƒ«ã‚’å›é¿ã€‚ç„¡æ¡ä»¶ $\emptyset$ ã¨ã®ä¸­é–“çš„ãªå½¹å‰²ã€‚
:::

:::details Q4: FLUXã¯SD 3ã‚ˆã‚Šä½•ãŒå„ªã‚Œã‚‹ï¼Ÿ
**A**: (1) Transformer backbone â†’ U-Netã‚ˆã‚Šè¡¨ç¾åŠ›ã€(2) Rectified Flow â†’ 20 stepsã§åæŸã€(3) 12B params â†’ ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ã€‚FID 7.2 (SD3: 8.7)ã€‚
:::

:::details Q5: LDMã®è¨“ç·´ã‚³ã‚¹ãƒˆã¯ï¼Ÿ
**A**: SD 1.5è¦æ¨¡ï¼ˆ860M paramsï¼‰ã§ã€V100Ã—8ã§ç´„2é€±é–“ã€‚FLUX.1 (12B)ã¯A100Ã—256ã§æ•°é€±é–“ã¨æ¨å®šã€‚VAEäº‹å‰è¨“ç·´å«ã‚ã‚‹ã¨+1é€±é–“ã€‚
:::

### å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (1é€±é–“ãƒ—ãƒ©ãƒ³)

| Day | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ | ç¢ºèª |
|:----|:-------|:-----|:-----|
| **1** | Zone 0-2 èª­äº† + VAEå¾©ç¿’ | 2h | ãªãœæ½œåœ¨ç©ºé–“ã‹èª¬æ˜ã§ãã‚‹ |
| **2** | Zone 3.1-3.5 CFGå°å‡º | 3h | CFGå¼ã‚’3è¦–ç‚¹ã‹ã‚‰å°å‡º |
| **3** | Zone 3.6-3.13 ç†è«–å®Œèµ° | 3h | FLUX architectureã‚’èª¬æ˜ |
| **4** | Zone 4 Juliaå®Ÿè£… | 4h | Mini LDMè¨“ç·´æˆåŠŸ |
| **5** | Zone 5 CFGå®Ÿé¨“ | 3h | Guidance scaleæƒå¼•å®Œäº† |
| **6** | Rustæ¨è«–å®Ÿè£… | 3h | ONNXæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ |
| **7** | ç·å¾©ç¿’ + Bosså†æŒ‘æˆ¦ | 2h | CFGå®Œå…¨åˆ†è§£ã‚’å†å°å‡º |

### é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```julia
# è‡ªå·±è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
progress = Dict(
    "VAEåœ§ç¸®ç†è«–" => false,
    "CFGå®Œå…¨å°å‡º" => false,
    "Cross-Attentionå®Ÿè£…" => false,
    "FLUXç†è§£" => false,
    "Juliaè¨“ç·´æˆåŠŸ" => false,
    "Rustæ¨è«–æˆåŠŸ" => false,
    "CFGå®Ÿé¨“å®Œäº†" => false
)

# å„é …ç›®ã‚’trueã«ã—ã¦é€²æ—ç¢ºèª
completed = count(values(progress))
total = length(progress)
println("Progress: $completed / $total ($(round(100*completed/total, digits=1))%)")

if completed == total
    println("ğŸ‰ ç¬¬39å›å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ï¼")
end
```

### æ¬¡å›äºˆå‘Š: ç¬¬40å› Consistency Models & é«˜é€Ÿç”Ÿæˆç†è«–

**ãƒ†ãƒ¼ãƒ**: 1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆç†è«–ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«é«˜é€ŸåŒ–æœ€å‰ç·šã€‚

**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**:
- Consistency Models (Song et al. 2023)
- Self-consistencyæ¡ä»¶
- Consistency Training (CT) / Distillation (CD)
- Pseudo-Huber loss (ICLR 2025)
- Progressive Distillation
- Latent Consistency Models (LCM)
- DPM-Solver++ / UniPC / EDM
- Rectified Flowè’¸ç•™
- Adversarial Post-Training (DMD2)

**äºˆç¿’èª²é¡Œ**:
- ç¬¬36å›ã®DDIMã‚’å¾©ç¿’ï¼ˆæ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
- ç¬¬38å›ã®Rectified Flowã‚’å¾©ç¿’ï¼ˆç›´ç·šçš„è¼¸é€ï¼‰

**åˆ°é”ç›®æ¨™**: ã€Œ1000ã‚¹ãƒ†ãƒƒãƒ—â†’1ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®ç†è«–çš„æ©‹æ¸¡ã—ã‚’å®Œå…¨ç†è§£ã€

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®100%å®Œäº†ï¼ ğŸ‰**

ç¬¬39å›èª­äº†ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼

**ç²å¾—ã‚¹ã‚­ãƒ«**:
- âœ… Latent Diffusionã®ç†è«–çš„åŸºç›¤
- âœ… Classifier-Free Guidanceã®å®Œå…¨å°å‡º
- âœ… Text Conditioningã®å®Ÿè£…ãƒ¬ãƒ™ãƒ«ç†è§£
- âœ… FLUX Architectureè§£æ
- âœ… Juliaè¨“ç·´ + Rustæ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- âœ… CFGå®Ÿé¨“ã«ã‚ˆã‚‹å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç†è§£

Course IVã€Œæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã¯æ®‹ã‚Š3å›ï¼ˆL40-42ï¼‰ã€‚ç†è«–ã®å®Œæˆã¾ã§ã‚ã¨å°‘ã—ï¼
:::

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**å•ã„**: ã€ŒStable Diffusionã®çŸ¥è­˜ã¯2026å¹´æ™‚ç‚¹ã§æ—¢ã«é™³è…åŒ–ã—ã¦ã„ã‚‹ã®ã§ã¯ï¼Ÿã€

**è€ƒå¯Ÿãƒã‚¤ãƒ³ãƒˆ**:

1. **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é€²åŒ–**: U-Net (SD 1.x/2.x/XL) â†’ Transformer (FLUX/Lumina) â†’ ??? (2027)
2. **Flowçµ±åˆ**: DDPM (SD 1.x-2.x) â†’ Rectified Flow (SD 3/FLUX) â†’ ã•ã‚‰ãªã‚‹æœ€é©åŒ–
3. **Text Encoder**: CLIPå˜ä½“ â†’ CLIP+T5 â†’ T5å˜ä½“ â†’ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMçµ±åˆï¼Ÿ
4. **è¨“ç·´ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ **: 2æ®µéšï¼ˆVAEâ†’Diffusionï¼‰â†’ End-to-Endï¼Ÿ
5. **1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆ**: Consistency Models / Distillation â†’ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆã®æœªæ¥

**æ­´å²çš„è¦–ç‚¹**:

- 2020: DDPMç™»å ´ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ï¼‰
- 2021: Latent Diffusionï¼ˆæ½œåœ¨ç©ºé–“é©å‘½ï¼‰
- 2022: Stable Diffusion 1.xï¼ˆæ°‘ä¸»åŒ–ï¼‰
- 2023: SDXLï¼ˆå“è³ªå‘ä¸Šï¼‰
- 2024: SD 3ï¼ˆRectified Flowçµ±åˆï¼‰
- 2025: FLUX.1ï¼ˆTransformerå®Œå…¨ç§»è¡Œï¼‰
- 2026: ??? ï¼ˆæ¬¡ã®é©æ–°ã¯ï¼Ÿï¼‰

**å•ã„ç›´ã—**: SD 1.5ã®ã€Œç†è«–ã€ã¯é™³è…åŒ–ã—ãŸã‹ã€ãã‚Œã¨ã‚‚ã€Œå®Ÿè£…ã€ã ã‘ãŒé™³è…åŒ–ã—ãŸã‹ï¼Ÿ

:::details è­°è«–ã®ç¨®
**ç«‹å ´1: ç†è«–ã¯ä¸å¤‰**
- VAEåœ§ç¸®ã®åŸç†ã¯å¤‰ã‚ã‚‰ãªã„
- CFGã®æ•°å­¦ã¯æ™®éçš„
- DDPMã®ç†è«–ã¯Rectified Flowã®åŸºç›¤

**ç«‹å ´2: å®Ÿè£…ã¯é™³è…åŒ–**
- U-Netã¯éå»ã®éºç‰©
- CLIPå˜ä½“ã¯ä¸ååˆ†
- 50 stepsã¯é…ã™ãã‚‹

**çµ±åˆè¦–ç‚¹**: ç†è«–ã‚’ç†è§£ã—ãŸä¸Šã§æœ€æ–°å®Ÿè£…ã«é©å¿œã™ã‚‹ã“ã¨ãŒã€ŒçœŸã®ç†è§£ã€ã§ã¯ï¼Ÿ

**å•ã„**: FLUXã‚’å­¦ã¹ã°ååˆ†ã‹ã€ãã‚Œã¨ã‚‚SD 1.5ã‹ã‚‰ç©ã¿ä¸Šã’ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ï¼Ÿ
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^ldm]: Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. *CVPR 2022*.
@[card](https://arxiv.org/abs/2112.10752)

[^cfg]: Ho, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance. *arXiv:2207.12598*.
@[card](https://arxiv.org/abs/2207.12598)

[^classifier_guidance]: Dhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. *NeurIPS 2021*.
@[card](https://arxiv.org/abs/2105.05233)

[^flux]: Greenberg, O. (2025). Demystifying Flux Architecture. *arXiv:2507.09595*.
@[card](https://arxiv.org/abs/2507.09595)

[^min_snr]: Hang, T., Gu, S., Li, C., et al. (2023). Efficient Diffusion Training via Min-SNR Weighting Strategy. *ICCV 2023*.
@[card](https://arxiv.org/abs/2303.09556)

[^zero_snr]: Lin, S., et al. (2023). Common Diffusion Noise Schedules and Sample Steps are Flawed. *arXiv:2305.08891*.
@[card](https://arxiv.org/abs/2305.08891)

[^clip]: Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. *ICML 2021*.
@[card](https://arxiv.org/abs/2103.00020)

[^t5]: Raffel, C., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *JMLR*.
@[card](https://arxiv.org/abs/1910.10683)

[^imagen]: Saharia, C., et al. (2022). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. *NeurIPS 2022*.
@[card](https://arxiv.org/abs/2205.11487)

[^sdxl]: Podell, D., et al. (2023). SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. *arXiv:2307.01952*.
@[card](https://arxiv.org/abs/2307.01952)

[^sd3]: Esser, P., et al. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. *arXiv:2403.03206*.
@[card](https://arxiv.org/abs/2403.03206)

### æ•™ç§‘æ›¸ãƒ»ã‚µãƒ¼ãƒ™ã‚¤

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
- Prince, S. J. D. (2023). *Understanding Deep Learning*. MIT Press. [Chapter on Diffusion Models]
- Weng, L. (2021). "What are Diffusion Models?" *Lil'Log*. [https://lilianweng.github.io/posts/2021-07-11-diffusion-models/](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- Hugging Face Diffusers Documentation: [https://huggingface.co/docs/diffusers](https://huggingface.co/docs/diffusers)
- MIT 6.S184: Diffusion Models (2026): [https://diffusion.csail.mit.edu/](https://diffusion.csail.mit.edu/)
- Stability AI Research: [https://stability.ai/research](https://stability.ai/research)

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | å‚™è€ƒ |
|:-----|:-----|:-----|
| $x$ | ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã®ç”»åƒ | $x \in \mathbb{R}^{H \times W \times C}$ |
| $z$ | æ½œåœ¨ç©ºé–“ã®è¡¨ç¾ | $z \in \mathbb{R}^{h \times w \times c}$ |
| $\mathcal{E}$ | VAE Encoder | $z = \mathcal{E}(x)$ |
| $\mathcal{D}$ | VAE Decoder | $\tilde{x} = \mathcal{D}(z)$ |
| $\epsilon_\theta$ | Noise prediction network | U-Net |
| $t$ | Timestep | $t \in \{1, \ldots, T\}$ |
| $\bar{\alpha}_t$ | Cumulative product | $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$ |
| $c$ | Condition (textç­‰) | CLIP/T5 encoding |
| $w$ | CFG guidance scale | å…¸å‹çš„ã« 7.5 |
| $\tilde{\epsilon}$ | CFGä¿®æ­£ãƒã‚¤ã‚º | $\tilde{\epsilon} = \epsilon_\emptyset + w(\epsilon_c - \epsilon_\emptyset)$ |
| $\mathbb{E}_{q}[\cdot]$ | æœŸå¾…å€¤ | åˆ†å¸ƒ $q$ ã®ä¸‹ã§ã®æœŸå¾…å€¤ |
| $\text{KL}[q \| p]$ | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | ç¬¬6å›å‚ç…§ |
| $\mathcal{N}(\mu, \sigma^2)$ | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ | å¹³å‡ $\mu$, åˆ†æ•£ $\sigma^2$ |
| $\text{SNR}(t)$ | Signal-to-Noise Ratio | $\bar{\alpha}_t / (1-\bar{\alpha}_t)$ |

---

**è¬è¾**: æœ¬è¬›ç¾©ã®åŸ·ç­†ã«ã‚ãŸã‚Šã€Rombach et al. (2021) ã®Latent Diffusionè«–æ–‡ã€Ho & Salimans (2022) ã®CFGè«–æ–‡ã€Greenberg (2025) ã®FLUXè§£æã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚
---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
