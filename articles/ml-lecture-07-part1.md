---
title: "ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«– (Part1: ç†è«–ç·¨)"
emoji: "ğŸ“Š"
type: "tech"
topics: ["machinelearning", "deeplearning", "statistics", "math", "python"]
published: false
slug: "ml-lecture-07-part1"
difficulty: "intermediate"
time_estimate: "90 minutes"
languages: ["Python"]
keywords: ["æœ€å°¤æ¨å®š", "MLE", "Fisheræƒ…å ±é‡", "KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> **å¾Œç·¨ã¯ã“ã¡ã‚‰**: [ç¬¬7å› Part2ï¼ˆå®Ÿè£…ç·¨ï¼‰](/articles/ml-lecture-07-part2)

## Learning Objectives

ã“ã®è¬›ç¾©ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™:

- [ ] MLEã®å®šç¾©ã¨æ•°å­¦çš„åŸºç›¤ã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹
- [ ] MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“ã®ç­‰ä¾¡æ€§ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] Fisheræƒ…å ±é‡ã¨æ¼¸è¿‘æ­£è¦æ€§ã®é–¢ä¿‚ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Mode-Covering vs Mode-Seeking ã®é•ã„ã‚’ç†è§£ã—ã€VAE/GANã¨ã®æ¥ç¶šã‚’èª¬æ˜ã§ãã‚‹
- [ ] ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å…¨ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’MLEã®å¤‰å½¢ã¨ã—ã¦çµ±ä¸€çš„ã«ç†è§£ã™ã‚‹

---

## ğŸš€ Z1. ãƒ—ãƒ­ãƒ­ãƒ¼ã‚°ï¼ˆ30ç§’ï¼‰â€” GMMã§MLEã®é™ç•Œã‚’ä½“æ„Ÿ

> Progress: 3%

```python
import numpy as np
import matplotlib.pyplot as plt

# 2æˆåˆ†ã‚¬ã‚¦ã‚¹æ··åˆã®çœŸã®åˆ†å¸ƒ
np.random.seed(42)
X = np.concatenate([
    np.random.randn(100) - 3,
    np.random.randn(100) + 3
])

# MLEã§å˜ä¸€ã‚¬ã‚¦ã‚¹ã‚’ãƒ•ã‚£ãƒƒãƒˆ
mu_mle = X.mean()
sigma_mle = X.std()

# çœŸã®åˆ†å¸ƒã¨æ¨å®šã‚’å¯è¦–åŒ–
x = np.linspace(-8, 8, 1000)
plt.hist(X, bins=30, density=True, alpha=0.5, label='Data')
plt.plot(x,
         0.5 * np.exp(-(x+3)**2/2)/np.sqrt(2*np.pi) +
         0.5 * np.exp(-(x-3)**2/2)/np.sqrt(2*np.pi),
         label='True (2-component GMM)', linewidth=2)
plt.plot(x,
         np.exp(-(x-mu_mle)**2/(2*sigma_mle**2))/(sigma_mle*np.sqrt(2*np.pi)),
         label=f'MLE Gaussian (Î¼={mu_mle:.2f}, Ïƒ={sigma_mle:.2f})', linewidth=2)
plt.legend()
plt.title('MLE fails to capture multimodality')
plt.show()
```

**å‡ºåŠ›**: 2ã¤ã®ãƒ”ãƒ¼ã‚¯ã‚’æŒã¤çœŸã®åˆ†å¸ƒã«å¯¾ã—ã€MLEã¯å˜ä¸€ã®ã‚¬ã‚¦ã‚¹ã§"å¹³å‡åŒ–"ã—ã¦ã—ã¾ã†ã€‚

**æ•°å¼ã¨ã®å¯¾å¿œ**:

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{i=1}^n \log p(x_i \mid \theta)
```

å˜ä¸€ã‚¬ã‚¦ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã¯ `$p(x \mid \mu, \sigma) = \mathcal{N}(x \mid \mu, \sigma^2)$` ã‚’ä»®å®šã€‚2æˆåˆ†æ··åˆã®çœŸã®åˆ†å¸ƒ `$p(x) = 0.5 \mathcal{N}(x \mid -3, 1) + 0.5 \mathcal{N}(x \mid 3, 1)$` ã‚’è¡¨ç¾ã§ããªã„ã€‚

ã“ã®é™ç•Œã‚’è¶…ãˆã‚‹ã«ã¯"æ½œåœ¨å¤‰æ•°"ãŒå¿…è¦ â€” ãã‚ŒãŒç¬¬8å›ã®EMç®—æ³•ã ã€‚

---

## ğŸ“– Z2. ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼ˆ10åˆ†ï¼‰â€” 5ãƒˆãƒ”ãƒƒã‚¯æ¦‚è¦³

> Progress: 10%

MLEã¯çµ±è¨ˆå­¦ã®ä¸­æ ¸ã§ã‚ã‚Šã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’åŸç†ãã®ã‚‚ã®ã ã€‚

### 5ãƒˆãƒ”ãƒƒã‚¯ã®å…¨ä½“åƒ

| ãƒˆãƒ”ãƒƒã‚¯ | å†…å®¹ | Zone |
|:---------|:-----|:-----|
| **1. MLEã®å®šç¾©ã¨ç­‰ä¾¡æ€§** | MLE = CEæœ€å°åŒ– = KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æœ€å°åŒ–ã®3ã¤ã®ç­‰ä¾¡æ€§ã‚’å®Œå…¨è¨¼æ˜ | Z4 |
| **2. MLEã®æ¼¸è¿‘è«–ã¨é™ç•Œ** | Fisheræƒ…å ±é‡ãƒ»æ¼¸è¿‘æ­£è¦æ€§ãƒ»ä¸€è‡´æ€§ãƒ»æœ‰åŠ¹æ€§ã®å®Œå…¨è¨¼æ˜ | Z4 |
| **3. å°¤åº¦é–¢æ•°ã®å½¢æ…‹ã¨å¤‰å½¢** | æ˜ç¤ºçš„ vs æš—é»™çš„æ¨å®šé‡ã€MLEã®3å¤‰å½¢ | Z4 |
| **4. ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç†è«–ã¨è©•ä¾¡æŒ‡æ¨™** | MCMCãƒ»Reparameterizationãƒ»FIDãƒ»CMMD | Z4 |
| **5. Boss Battle** | MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“ã®å®Œå…¨è¨¼æ˜ | Z4 |

### ãƒˆãƒ”ãƒƒã‚¯é–“ã®é–¢ä¿‚

```mermaid
graph TD
    A[MLEå®šç¾©] --> B[Fisheræƒ…å ±é‡]
    A --> C[ç­‰ä¾¡æ€§å®šç†]
    C --> D[KLæœ€å°åŒ–]
    C --> E[CEæœ€å°åŒ–]
    B --> F[æ¼¸è¿‘æ­£è¦æ€§]
    A --> H[å°¤åº¦å½¢æ…‹]
    H --> I[æ˜ç¤ºçš„MLE]
    H --> J[æš—é»™çš„MLE]
    D --> L[Mode-Covering]
    L --> M[VAE/DDPM]
```

### Quick Check 1

ä»¥ä¸‹ã®æ–‡ã¯æ­£ã—ã„ã‹ï¼Ÿ

1. MLEã¯å¸¸ã«ä¸åæ¨å®šé‡ã§ã‚ã‚‹ â†’ âŒ
2. MLEã¯KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æœ€å°åŒ–ã¨ç­‰ä¾¡ã§ã‚ã‚‹ â†’ âœ…
3. Fisheræƒ…å ±é‡ãŒå¤§ãã„ã»ã©æ¨å®šç²¾åº¦ãŒé«˜ã„ â†’ âœ…

<details><summary>è§£èª¬</summary>

1. æ­£è¦åˆ†å¸ƒã®åˆ†æ•£ `$\hat{\sigma}^2 = \frac{1}{n}\sum(x_i - \bar{x})^2$` ã¯åã‚Šã‚ã‚Š
2. æœ¬è¬›ç¾©ã§è¨¼æ˜ã™ã‚‹æ ¸å¿ƒå®šç†
3. CramÃ©r-Raoä¸‹ç•Œ `$\text{Var}(\hat{\theta}) \geq \frac{1}{I(\theta)}$`

</details>

---

## ğŸŒ Z3. ä¸–ç•Œè¦³ï¼ˆ20åˆ†ï¼‰â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€çš„ç†è§£

> Progress: 20%

### ãªãœMLEã‚’å­¦ã¶ã®ã‹ï¼Ÿ

æ©Ÿæ¢°å­¦ç¿’ã®æœ¬è³ªã¯ã€Œãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ `$p_{\text{data}}(x)$` ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã€ã ã€‚æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã¯ã€ã“ã®åˆ†å¸ƒæ¨å®šã®æœ€ã‚‚åŸºæœ¬çš„ã‹ã¤å¼·åŠ›ãªé“å…·ã ã€‚

### ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€åŸç†

ç¾ä»£ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯**ã™ã¹ã¦MLEã®å¤‰å½¢**ã¨ã—ã¦ç†è§£ã§ãã‚‹ã€‚

```mermaid
graph LR
    A[MLE] --> B[æ˜ç¤ºçš„å°¤åº¦]
    A --> C[æš—é»™çš„å°¤åº¦]
    B --> E[VAE/NF/AR]
    C --> H[GAN]
```

çµ±ä¸€åŸç†: ã™ã¹ã¦ã¯ `$\max_\theta \mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)]$` ã‚’æœ€å¤§åŒ–ã—ã¦ã„ã‚‹ã€‚

### MLEã¨KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®ç­‰ä¾¡æ€§

æœ¬è¬›ç¾©ã®æ ¸å¿ƒå®šç†:

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)] = \arg\min_\theta D_{\text{KL}}(p_{\text{data}} \| p_\theta)
```

MLEã¯"åˆ†å¸ƒã®è·é›¢"ã‚’æœ€å°åŒ–ã™ã‚‹æœ€é©åŒ–å•é¡Œãªã®ã ã€‚

### Quick Check 2

VAEã¨GANã®é•ã„ã‚’ **KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®æ–¹å‘** ã§èª¬æ˜ã§ãã‚‹ã‹ï¼Ÿ

<details><summary>è§£ç­”</summary>

- **VAE**: `$D_{\text{KL}}(p_{\text{data}} \| p_\theta)$`ï¼ˆå‰å‘ãKLï¼‰â†’ Mode-Covering â†’ ã¼ã‚„ã‘ãŸç”»åƒ
- **GAN**: `$D_{\text{KL}}(p_\theta \| p_{\text{data}})$`ï¼ˆé€†å‘ãKLï¼‰â†’ Mode-Seeking â†’ é®®æ˜ã ãŒå¤šæ§˜æ€§ä½

</details>

---

## âš”ï¸ Z4. Boss Battleï¼ˆ60åˆ†ï¼‰â€” æœ€å°¤æ¨å®šã®å®Œå…¨ä½“ç³»

> Progress: 50%

### Topic 1: MLEã®å®šç¾©ã¨ç­‰ä¾¡æ€§

#### 1.1 MLEã®å®šç¾©

**è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿** `$\mathcal{D} = \{x_1, x_2, \ldots, x_n\}$` ãŒç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰ã«ç¢ºç‡åˆ†å¸ƒ `$p(x \mid \theta)$` ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸã¨ã™ã‚‹ã€‚

**å°¤åº¦é–¢æ•°**ï¼ˆLikelihood functionï¼‰:

```math
L(\theta \mid \mathcal{D}) = \prod_{i=1}^n p(x_i \mid \theta)
```

**å¯¾æ•°å°¤åº¦é–¢æ•°**ï¼ˆLog-likelihood functionï¼‰:

```math
\ell(\theta \mid \mathcal{D}) = \log L(\theta \mid \mathcal{D}) = \sum_{i=1}^n \log p(x_i \mid \theta)
```

**æœ€å°¤æ¨å®šé‡**ï¼ˆMaximum Likelihood Estimator, MLEï¼‰:

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \ell(\theta \mid \mathcal{D}) = \arg\max_\theta \sum_{i=1}^n \log p(x_i \mid \theta)
```

**ãªãœå¯¾æ•°ã‚’ã¨ã‚‹ã®ã‹ï¼Ÿ**
1. ç© `$\prod$` ã‚’å’Œ `$\sum$` ã«å¤‰æ› â†’ å¾®åˆ†ãŒå®¹æ˜“
2. æ•°å€¤å®‰å®šæ€§ â†’ ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼å›é¿
3. åŠ æ³•æ€§ â†’ ç‹¬ç«‹ãªè¦³æ¸¬ã®å¯„ä¸ã‚’åŠ ç®—

#### 1.2 MLE = Cross-Entropyæœ€å°åŒ–ã®ç­‰ä¾¡æ€§

**çµŒé¨“åˆ†å¸ƒ**ï¼ˆEmpirical distributionï¼‰:

```math
\hat{p}_{\text{data}}(x) = \frac{1}{n} \sum_{i=1}^n \delta(x - x_i)
```

ã“ã“ã§ `$\delta(x - x_i)$` ã¯Diracã®ãƒ‡ãƒ«ã‚¿é–¢æ•°ã€‚

**Cross-Entropy**:

```math
H(\hat{p}_{\text{data}}, p_\theta) = -\mathbb{E}_{x \sim \hat{p}_{\text{data}}}[\log p_\theta(x)] = -\frac{1}{n}\sum_{i=1}^n \log p_\theta(x_i)
```

**ç­‰ä¾¡æ€§ã®è¨¼æ˜**:

MLEã®ç›®çš„é–¢æ•°ã‚’ `$n$` ã§å‰²ã‚‹ã¨:

```math
\frac{1}{n}\ell(\theta \mid \mathcal{D}) = \frac{1}{n}\sum_{i=1}^n \log p_\theta(x_i) = \mathbb{E}_{x \sim \hat{p}_{\text{data}}}[\log p_\theta(x)]
```

ã—ãŸãŒã£ã¦:

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \mathbb{E}_{x \sim \hat{p}_{\text{data}}}[\log p_\theta(x)] = \arg\min_\theta H(\hat{p}_{\text{data}}, p_\theta)
```

**çµè«–**: MLEã¯ãƒ‡ãƒ¼ã‚¿ã®çµŒé¨“åˆ†å¸ƒã¨ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒã®é–“ã®Cross-Entropyã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

#### 1.3 MLE = KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æœ€å°åŒ–ã®ç­‰ä¾¡æ€§

**KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹**:

```math
D_{\text{KL}}(\hat{p}_{\text{data}} \| p_\theta) = \mathbb{E}_{x \sim \hat{p}_{\text{data}}}\left[\log \frac{\hat{p}_{\text{data}}(x)}{p_\theta(x)}\right]
```

å±•é–‹ã™ã‚‹ã¨:

```math
D_{\text{KL}}(\hat{p}_{\text{data}} \| p_\theta) = \mathbb{E}_{x \sim \hat{p}_{\text{data}}}[\log \hat{p}_{\text{data}}(x)] - \mathbb{E}_{x \sim \hat{p}_{\text{data}}}[\log p_\theta(x)]
```

```math
= H(\hat{p}_{\text{data}}) + H(\hat{p}_{\text{data}}, p_\theta)
```

ã“ã“ã§ `$H(\hat{p}_{\text{data}})$` ã¯çµŒé¨“åˆ†å¸ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆå®šæ•°ï¼‰ã€‚ã—ãŸãŒã£ã¦:

```math
\arg\min_\theta D_{\text{KL}}(\hat{p}_{\text{data}} \| p_\theta) = \arg\min_\theta H(\hat{p}_{\text{data}}, p_\theta) = \arg\max_\theta \mathbb{E}_{x \sim \hat{p}_{\text{data}}}[\log p_\theta(x)]
```

**çµè«–**: MLEã¯çµŒé¨“åˆ†å¸ƒã¨ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒã®é–“ã®**å‰å‘ãKLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹**ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

#### 1.4 ä¸‰ä½ä¸€ä½“ã®çµ±ä¸€

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{i=1}^n \log p_\theta(x_i) = \arg\min_\theta H(\hat{p}_{\text{data}}, p_\theta) = \arg\min_\theta D_{\text{KL}}(\hat{p}_{\text{data}} \| p_\theta)
```

ã“ã®ç­‰ä¾¡æ€§ãŒ**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’åŸç†**ã®æ•°å­¦çš„åŸºç›¤ã ã€‚

#### 1.5 é€£ç¶šåˆ†å¸ƒ vs é›¢æ•£åˆ†å¸ƒã®MLE

**é€£ç¶šåˆ†å¸ƒã®å ´åˆ** ï¼ˆä¾‹: æ­£è¦åˆ†å¸ƒï¼‰:

```math
p_\theta(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
```

å¯¾æ•°å°¤åº¦:

```math
\ell(\mu, \sigma^2) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2
```

**é›¢æ•£åˆ†å¸ƒã®å ´åˆ** ï¼ˆä¾‹: Categoricalåˆ†å¸ƒï¼‰:

```math
p_\theta(x = k) = \theta_k, \quad \sum_{k=1}^K \theta_k = 1
```

å¯¾æ•°å°¤åº¦:

```math
\ell(\boldsymbol{\theta}) = \sum_{i=1}^n \log \theta_{y_i}
```

ã“ã“ã§ `$y_i \in \{1, \ldots, K\}$` ã¯è¦³æ¸¬ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã€‚

**MLE**:

```math
\hat{\theta}_k = \frac{\text{count}(y = k)}{n}
```

ã™ãªã‚ã¡ã€çµŒé¨“é »åº¦ã€‚

#### 1.6 æ¡ä»¶ä»˜ãMLEã¨Logisticå›å¸°

**Logisticå›å¸°**:

```math
p_\theta(y = 1 \mid x) = \sigma(\theta^T x), \quad \sigma(z) = \frac{1}{1 + e^{-z}}
```

**å¯¾æ•°å°¤åº¦** ï¼ˆBernoulliåˆ†å¸ƒï¼‰:

```math
\ell(\theta) = \sum_{i=1}^n \left[y_i \log p_\theta(y_i = 1 \mid x_i) + (1 - y_i) \log(1 - p_\theta(y_i = 1 \mid x_i))\right]
```

```math
= \sum_{i=1}^n \left[y_i \theta^T x_i - \log(1 + \exp(\theta^T x_i))\right]
```

**MLE**: å‹¾é…ä¸Šæ˜‡æ³•ã§æœ€é©åŒ–

```math
\nabla_\theta \ell = \sum_{i=1}^n (y_i - \sigma(\theta^T x_i)) x_i
```

**ç›´æ„Ÿ**: äºˆæ¸¬èª¤å·® `$(y_i - \hat{y}_i)$` ã«æ¯”ä¾‹ã™ã‚‹å‹¾é…ã€‚

#### 1.7 å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã¨Softmax

**Softmaxé–¢æ•°**:

```math
p_\theta(y = k \mid x) = \frac{\exp(\theta_k^T x)}{\sum_{j=1}^K \exp(\theta_j^T x)}
```

**å¯¾æ•°å°¤åº¦** ï¼ˆCategoricalåˆ†å¸ƒï¼‰:

```math
\ell(\Theta) = \sum_{i=1}^n \log p_\theta(y_i \mid x_i) = \sum_{i=1}^n \left[\theta_{y_i}^T x_i - \log \sum_{j=1}^K \exp(\theta_j^T x_i)\right]
```

**Cross-Entropyæå¤±**:

```math
\mathcal{L}_{\text{CE}} = -\sum_{i=1}^n \sum_{k=1}^K \mathbb{1}[y_i = k] \log p_\theta(y = k \mid x_i)
```

**çµè«–**: Softmax + Cross-Entropy = å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã®MLE

#### 1.8 MLEã¨ãƒ™ã‚¤ã‚ºæ¨å®šã®é–¢ä¿‚

**ãƒ™ã‚¤ã‚ºã®å®šç†**:

```math
p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta) p(\theta)}{p(\mathcal{D})}
```

**MAPæ¨å®š** ï¼ˆMaximum A Posterioriï¼‰:

```math
\hat{\theta}_{\text{MAP}} = \arg\max_\theta p(\theta \mid \mathcal{D}) = \arg\max_\theta \left[p(\mathcal{D} \mid \theta) p(\theta)\right]
```

å¯¾æ•°ã‚’ã¨ã‚‹ã¨:

```math
\hat{\theta}_{\text{MAP}} = \arg\max_\theta \left[\log p(\mathcal{D} \mid \theta) + \log p(\theta)\right]
```

**äº‹å‰åˆ†å¸ƒãŒä¸€æ§˜** `$p(\theta) \propto \text{const}$` ã®ã¨ã:

```math
\hat{\theta}_{\text{MAP}} = \hat{\theta}_{\text{MLE}}
```

**çµè«–**: MLEã¯MAPæ¨å®šã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ï¼ˆç„¡æƒ…å ±äº‹å‰åˆ†å¸ƒï¼‰ã€‚

**æ­£å‰‡åŒ–ã¨ã®æ¥ç¶š**:

- **L2æ­£å‰‡åŒ–** `$\lambda \|\theta\|^2$` = ã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒ `$p(\theta) = \mathcal{N}(0, \frac{1}{2\lambda}I)$`
- **L1æ­£å‰‡åŒ–** `$\lambda \|\theta\|_1$` = Laplaceäº‹å‰åˆ†å¸ƒ `$p(\theta) = \text{Laplace}(0, \frac{1}{\lambda})$`

**æ­£å‰‡åŒ–ä»˜ãMLE**:

```math
\hat{\theta} = \arg\max_\theta \left[\ell(\theta \mid \mathcal{D}) - \lambda R(\theta)\right]
```

ã“ã“ã§ `$R(\theta)$` ã¯æ­£å‰‡åŒ–é …ã€‚

#### 1.9 å°¤åº¦é–¢æ•°ã®æ€§è³ª

**æ€§è³ª1: Scoreé–¢æ•°ã®æœŸå¾…å€¤ã¯0**

```math
\mathbb{E}_{x \sim p(x \mid \theta)}\left[\frac{\partial \log p(x \mid \theta)}{\partial \theta}\right] = 0
```

**è¨¼æ˜**:

```math
\mathbb{E}\left[\frac{\partial \log p(x \mid \theta)}{\partial \theta}\right] = \int \frac{\partial \log p(x \mid \theta)}{\partial \theta} p(x \mid \theta) dx
```

```math
= \int \frac{1}{p(x \mid \theta)} \frac{\partial p(x \mid \theta)}{\partial \theta} p(x \mid \theta) dx = \int \frac{\partial p(x \mid \theta)}{\partial \theta} dx
```

```math
= \frac{\partial}{\partial \theta} \int p(x \mid \theta) dx = \frac{\partial}{\partial \theta} 1 = 0
```

**æ€§è³ª2: Fisheræƒ…å ±é‡ã®2ã¤ã®è¡¨ç¾ã®ç­‰ä¾¡æ€§**

```math
I(\theta) = \mathbb{E}\left[\left(\frac{\partial \log p(x \mid \theta)}{\partial \theta}\right)^2\right] = -\mathbb{E}\left[\frac{\partial^2 \log p(x \mid \theta)}{\partial \theta^2}\right]
```

**è¨¼æ˜**:

```math
\frac{\partial^2 \log p(x \mid \theta)}{\partial \theta^2} = \frac{\partial}{\partial \theta}\left[\frac{1}{p(x \mid \theta)} \frac{\partial p(x \mid \theta)}{\partial \theta}\right]
```

```math
= -\frac{1}{p^2} \left(\frac{\partial p}{\partial \theta}\right)^2 + \frac{1}{p} \frac{\partial^2 p}{\partial \theta^2}
```

æœŸå¾…å€¤ã‚’ã¨ã‚‹ã¨:

```math
\mathbb{E}\left[\frac{\partial^2 \log p}{\partial \theta^2}\right] = -\mathbb{E}\left[\left(\frac{\partial \log p}{\partial \theta}\right)^2\right] + \mathbb{E}\left[\frac{1}{p} \frac{\partial^2 p}{\partial \theta^2}\right]
```

ç¬¬2é …ã¯ï¼ˆæ­£å‰‡æ¡ä»¶ä¸‹ã§ï¼‰0ãªã®ã§ã€ç­‰ä¾¡æ€§ãŒæˆç«‹ã€‚

### Quick Check 3

ä»¥ä¸‹ã®æœ€é©åŒ–å•é¡Œã¯ã™ã¹ã¦ç­‰ä¾¡ã§ã‚ã‚‹ã€‚æ­£ã—ã„ã‹ï¼Ÿ

1. `$\arg\max_\theta \sum \log p_\theta(x_i)$`
2. `$\arg\min_\theta H(\hat{p}_{\text{data}}, p_\theta)$`
3. `$\arg\min_\theta D_{\text{KL}}(\hat{p}_{\text{data}} \| p_\theta)$`

<details><summary>è§£ç­”</summary>

âœ… ã™ã¹ã¦ç­‰ä¾¡ã€‚ã“ã‚ŒãŒMLEã®ä¸‰ä½ä¸€ä½“ã€‚

</details>

---

### Topic 2: MLEã®æ¼¸è¿‘è«–ã¨é™ç•Œ

#### 2.1 Fisheræƒ…å ±é‡

**Scoreé–¢æ•°**ï¼ˆã‚¹ã‚³ã‚¢é–¢æ•°ï¼‰:

```math
s(\theta \mid x) = \frac{\partial}{\partial \theta} \log p(x \mid \theta) = \frac{1}{p(x \mid \theta)} \frac{\partial p(x \mid \theta)}{\partial \theta}
```

**Fisheræƒ…å ±é‡**ï¼ˆFisher Informationï¼‰:

```math
I(\theta) = \mathbb{E}_{x \sim p(x \mid \theta)}[s(\theta \mid x)^2] = -\mathbb{E}_{x \sim p(x \mid \theta)}\left[\frac{\partial^2 \log p(x \mid \theta)}{\partial \theta^2}\right]
```

**ç›´æ„Ÿ**: Fisheræƒ…å ±é‡ã¯ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ `$\theta$` ãŒå¯¾æ•°å°¤åº¦ã«ä¸ãˆã‚‹æ„Ÿåº¦ã€ã€‚`$I(\theta)$` ãŒå¤§ãã„ã»ã©ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ `$\theta$` ã‚’æ­£ç¢ºã«æ¨å®šã§ãã‚‹ã€‚

**ä¾‹: æ­£è¦åˆ†å¸ƒ `$\mathcal{N}(\mu, \sigma^2)$` ã®Fisheræƒ…å ±é‡**

å¹³å‡ `$\mu$` ã«é–¢ã™ã‚‹Fisheræƒ…å ±é‡:

```math
I(\mu) = \frac{1}{\sigma^2}
```

åˆ†æ•£ãŒå°ã•ã„ã»ã© `$\mu$` ã®æ¨å®šç²¾åº¦ãŒé«˜ã„ã€‚

#### 2.2 CramÃ©r-Raoä¸‹ç•Œ

**CramÃ©r-Raoä¸‹ç•Œ**ï¼ˆCramÃ©r-Rao Lower Bound, CRLBï¼‰:

ä»»æ„ã®ä¸åæ¨å®šé‡ `$\hat{\theta}$` ã«å¯¾ã—ã€ãã®åˆ†æ•£ã¯:

```math
\text{Var}(\hat{\theta}) \geq \frac{1}{n \cdot I(\theta)}
```

ã“ã“ã§ `$n$` ã¯ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã€‚

**çµè«–**: Fisheræƒ…å ±é‡ãŒå¤§ãã„ã»ã©ã€æ¨å®šé‡ã®åˆ†æ•£ã®ä¸‹ç•ŒãŒå°ã•ã„ = é«˜ç²¾åº¦æ¨å®šãŒå¯èƒ½ã€‚

#### 2.3 MLEã®æ¼¸è¿‘æ­£è¦æ€§

**å®šç†**: æ­£å‰‡æ¡ä»¶ä¸‹ã§ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º `$n \to \infty$` ã®ã¨ãã€MLEã¯çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ `$\theta_0$` ã®å‘¨ã‚Šã§æ¼¸è¿‘æ­£è¦åˆ†å¸ƒã«å¾“ã†:

```math
\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta_0) \xrightarrow{d} \mathcal{N}\left(0, \frac{1}{I(\theta_0)}\right)
```

**æ„å‘³**:
1. **ä¸€è‡´æ€§**ï¼ˆConsistencyï¼‰: `$\hat{\theta}_{\text{MLE}} \xrightarrow{P} \theta_0$`ï¼ˆç¢ºç‡åæŸï¼‰
2. **æœ‰åŠ¹æ€§**ï¼ˆEfficiencyï¼‰: MLEã¯CramÃ©r-Raoä¸‹ç•Œã‚’é”æˆã™ã‚‹æ¼¸è¿‘æœ‰åŠ¹æ¨å®šé‡
3. **æ­£è¦æ€§**: å¤§æ¨™æœ¬ã§ã¯æ­£è¦åˆ†å¸ƒã§è¿‘ä¼¼ã§ãã‚‹

#### 2.3.1 æ¼¸è¿‘æ­£è¦æ€§ã®è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ

**Taylorå±•é–‹ã«ã‚ˆã‚‹è¨¼æ˜ã®æ¦‚ç•¥**:

çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ `$\theta_0$` ã¨ã™ã‚‹ã€‚Scoreé–¢æ•° `$s(\theta \mid x) = \frac{\partial \log p(x \mid \theta)}{\partial \theta}$` ã‚’ `$\theta_0$` ã®å‘¨ã‚Šã§Taylorå±•é–‹:

```math
s(\hat{\theta}_{\text{MLE}} \mid x_i) \approx s(\theta_0 \mid x_i) + \frac{\partial s(\theta_0 \mid x_i)}{\partial \theta}(\hat{\theta}_{\text{MLE}} - \theta_0)
```

MLEæ¡ä»¶ `$\sum_{i=1}^n s(\hat{\theta}_{\text{MLE}} \mid x_i) = 0$` ã‚ˆã‚Š:

```math
\sum_{i=1}^n s(\theta_0 \mid x_i) + \sum_{i=1}^n \frac{\partial s(\theta_0 \mid x_i)}{\partial \theta}(\hat{\theta}_{\text{MLE}} - \theta_0) \approx 0
```

```math
\Rightarrow \quad \hat{\theta}_{\text{MLE}} - \theta_0 \approx -\left(\sum_{i=1}^n \frac{\partial s(\theta_0 \mid x_i)}{\partial \theta}\right)^{-1} \sum_{i=1}^n s(\theta_0 \mid x_i)
```

ã“ã“ã§:
- `$\mathbb{E}[s(\theta_0 \mid x)] = 0$`ï¼ˆScoreé–¢æ•°ã®æœŸå¾…å€¤ã¯0ï¼‰
- `$\text{Var}(s(\theta_0 \mid x)) = I(\theta_0)$`ï¼ˆFisheræƒ…å ±é‡ï¼‰
- `$\mathbb{E}\left[\frac{\partial s(\theta_0 \mid x)}{\partial \theta}\right] = -I(\theta_0)$`

å¤§æ•°ã®æ³•å‰‡ã¨ä¸­å¿ƒæ¥µé™å®šç†ã‚ˆã‚Š:

```math
\sum_{i=1}^n s(\theta_0 \mid x_i) \xrightarrow{d} \mathcal{N}(0, n \cdot I(\theta_0))
```

```math
\frac{1}{n}\sum_{i=1}^n \frac{\partial s(\theta_0 \mid x_i)}{\partial \theta} \xrightarrow{P} -I(\theta_0)
```

ã—ãŸãŒã£ã¦:

```math
\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta_0) \xrightarrow{d} \mathcal{N}\left(0, \frac{1}{I(\theta_0)}\right)
```

**çµè«–**: MLEã¯æ¼¸è¿‘çš„ã«æ­£è¦åˆ†å¸ƒã«å¾“ã„ã€åˆ†æ•£ã¯CramÃ©r-Raoä¸‹ç•Œã‚’é”æˆã€‚

#### 2.3.2 Fisheræƒ…å ±è¡Œåˆ—ï¼ˆå¤šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç‰ˆï¼‰

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ `$\boldsymbol{\theta} \in \mathbb{R}^p$` ã®ãƒ™ã‚¯ãƒˆãƒ«ã®å ´åˆã€**Fisheræƒ…å ±è¡Œåˆ—**:

```math
I(\boldsymbol{\theta})_{ij} = \mathbb{E}\left[\frac{\partial \log p(x \mid \boldsymbol{\theta})}{\partial \theta_i} \frac{\partial \log p(x \mid \boldsymbol{\theta})}{\partial \theta_j}\right]
```

ã¾ãŸã¯:

```math
I(\boldsymbol{\theta})_{ij} = -\mathbb{E}\left[\frac{\partial^2 \log p(x \mid \boldsymbol{\theta})}{\partial \theta_i \partial \theta_j}\right]
```

**CramÃ©r-Raoä¸‹ç•Œï¼ˆå¤šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç‰ˆï¼‰**:

ä»»æ„ã®ä¸åæ¨å®šé‡ `$\hat{\boldsymbol{\theta}}$` ã®å…±åˆ†æ•£è¡Œåˆ— `$\Sigma = \mathbb{E}[(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta})(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta})^T]$` ã«å¯¾ã—:

```math
\Sigma \succeq \frac{1}{n} I(\boldsymbol{\theta})^{-1}
```

ã“ã“ã§ `$\succeq$` ã¯åŠæ­£å®šå€¤é †åºï¼ˆ`$\Sigma - \frac{1}{n}I(\boldsymbol{\theta})^{-1}$` ãŒåŠæ­£å®šå€¤ï¼‰ã€‚

**MLEã®æ¼¸è¿‘çš„å…±åˆ†æ•£è¡Œåˆ—**:

```math
\sqrt{n}(\hat{\boldsymbol{\theta}}_{\text{MLE}} - \boldsymbol{\theta}_0) \xrightarrow{d} \mathcal{N}\left(\mathbf{0}, I(\boldsymbol{\theta}_0)^{-1}\right)
```

#### 2.3.3 ä¾‹: å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã®Fisheræƒ…å ±è¡Œåˆ—

`$\mathcal{N}(\boldsymbol{\mu}, \Sigma)$` ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ `$\boldsymbol{\theta} = (\boldsymbol{\mu}, \Sigma)$` ã«é–¢ã™ã‚‹Fisheræƒ…å ±é‡:

**å¹³å‡ã«é–¢ã™ã‚‹éƒ¨åˆ†**:

```math
I_{\mu_i, \mu_j} = \Sigma^{-1}_{ij}
```

**å…±åˆ†æ•£è¡Œåˆ—ã«é–¢ã™ã‚‹éƒ¨åˆ†** ï¼ˆè¤‡é›‘ï¼‰:

```math
I_{\Sigma_{ij}, \Sigma_{kl}} = \frac{1}{2}(\Sigma^{-1}_{ik}\Sigma^{-1}_{jl} + \Sigma^{-1}_{il}\Sigma^{-1}_{jk})
```

**çµè«–**: å…±åˆ†æ•£è¡Œåˆ—ãŒå¤§ãã„ã»ã©ã€å¹³å‡ã®æ¨å®šç²¾åº¦ãŒä½ã„ï¼ˆç›´æ„Ÿã¨ä¸€è‡´ï¼‰ã€‚

#### 2.4 MLEã®é™ç•Œ

**å•é¡Œ1: æ¬¡å…ƒã®å‘ªã„**

é«˜æ¬¡å…ƒç©ºé–“ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿å¯†åº¦ãŒæŒ‡æ•°çš„ã«ç–ã«ãªã‚‹ã€‚`$d$` æ¬¡å…ƒç©ºé–“ã§åŠå¾„ `$r$` ã®çƒã®ä½“ç©:

```math
V_d(r) \propto r^d
```

åŒã˜å¯†åº¦ã‚’ä¿ã¤ã«ã¯ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒ `$n \propto 2^d$` å¿…è¦ï¼ˆæŒ‡æ•°çš„å¢—åŠ ï¼‰ã€‚

**å•é¡Œ2: å¤šæ§˜ä½“ä»®èª¬**

å®Ÿãƒ‡ãƒ¼ã‚¿ã¯é«˜æ¬¡å…ƒç©ºé–“ã®**ä½æ¬¡å…ƒå¤šæ§˜ä½“**ä¸Šã«å­˜åœ¨ã™ã‚‹ã€‚

```math
\mathcal{M} = \{x \in \mathbb{R}^D \mid x = G(z), z \in \mathbb{R}^d, d \ll D\}
```

ã“ã“ã§ `$G: \mathbb{R}^d \to \mathbb{R}^D$` ã¯ç”Ÿæˆé–¢æ•°ï¼ˆGeneratorï¼‰ã€‚

**ä¾‹**: ç”»åƒã¯ `$256 \times 256 = 65536$` æ¬¡å…ƒã ãŒã€æ„å‘³ã®ã‚ã‚‹ç”»åƒã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«ã‚ã‚‹ã€‚

**å•é¡Œ3: å‘¨è¾ºåŒ–ã®å›°é›£æ€§**

æ½œåœ¨å¤‰æ•° `$z$` ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«:

```math
p_\theta(x) = \int p_\theta(x \mid z) p(z) dz
```

ã“ã®ç©åˆ†ãŒè§£æçš„ã«è¨ˆç®—ä¸èƒ½ãªå ´åˆãŒå¤šã„ã€‚

**è§£æ±ºç­–**:
- **å¤‰åˆ†æ¨è«–**ï¼ˆVAEï¼‰: ELBOã§è¿‘ä¼¼
- **MCMC**: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§è¿‘ä¼¼
- **EMç®—æ³•**: æ½œåœ¨å¤‰æ•°ã‚’"æœŸå¾…å€¤åŒ–"

### Quick Check 4

Fisheræƒ…å ±é‡ãŒå¤§ãã„ã“ã¨ã®æ„å‘³ã¯ï¼Ÿ

<details><summary>è§£ç­”</summary>

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å®šç²¾åº¦ãŒé«˜ã„ã€‚CramÃ©r-Raoä¸‹ç•Œ `$\text{Var}(\hat{\theta}) \geq 1/(n \cdot I(\theta))$` ã‚ˆã‚Šã€`$I(\theta)$` ãŒå¤§ãã„ã»ã©åˆ†æ•£ãŒå°ã•ã„ã€‚

</details>

---

### Topic 3: å°¤åº¦é–¢æ•°ã®å½¢æ…‹ã¨å¤‰å½¢

#### 3.1 å°¤åº¦é–¢æ•°ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹

**Mohamed & Lakshminarayanan (2016)** ã®åˆ†é¡:

1. **Explicit densityï¼ˆæ˜ç¤ºçš„å¯†åº¦ï¼‰**: `$p_\theta(x)$` ãŒè§£æçš„ã«è¨ˆç®—å¯èƒ½
   - ä¾‹: VAE, Normalizing Flow, è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«
2. **Implicit densityï¼ˆæš—é»™çš„å¯†åº¦ï¼‰**: `$p_\theta(x)$` ãŒè¨ˆç®—ä¸èƒ½ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ã¿å¯èƒ½
   - ä¾‹: GAN

#### 3.2 æ˜ç¤ºçš„MLE: Normalizing Flow

**Normalizing Flow**ï¼ˆæ­£è¦åŒ–æµï¼‰:

å¯é€†å¤‰æ› `$f_\theta: \mathbb{R}^d \to \mathbb{R}^d$` ã‚’ç”¨ã„ã¦ã€å˜ç´”ãªåˆ†å¸ƒ `$p_z(z)$`ï¼ˆä¾‹: æ¨™æº–æ­£è¦åˆ†å¸ƒï¼‰ã‚’è¤‡é›‘ãªåˆ†å¸ƒ `$p_x(x)$` ã«å¤‰æ›:

```math
x = f_\theta(z), \quad z \sim p_z(z)
```

**å¤‰æ•°å¤‰æ›å…¬å¼**:

```math
p_x(x) = p_z(f_\theta^{-1}(x)) \left|\det \frac{\partial f_\theta^{-1}(x)}{\partial x}\right|
```

å¯¾æ•°å°¤åº¦:

```math
\log p_x(x) = \log p_z(f_\theta^{-1}(x)) + \log \left|\det \frac{\partial f_\theta^{-1}(x)}{\partial x}\right|
```

**æœ€å°¤æ¨å®š**:

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{i=1}^n \log p_x(x_i)
```

ãƒ¤ã‚³ãƒ“è¡Œåˆ—å¼ã®è¨ˆç®—ãŒéµã€‚

#### 3.3 æš—é»™çš„MLE: GAN

**GAN**ï¼ˆGenerative Adversarial Networkï¼‰:

ç”Ÿæˆå™¨ `$G_\theta: \mathbb{R}^d \to \mathbb{R}^D$` ãŒãƒã‚¤ã‚º `$z \sim p_z(z)$` ã‚’ç”»åƒ `$x = G_\theta(z)$` ã«å¤‰æ›ã€‚

**Pushforward measure**:

```math
p_\theta(x) = (G_\theta)_\# p_z(z)
```

ã“ã“ã§ `$(G_\theta)_\#$` ã¯pushforwardï¼ˆæŠ¼ã—å‡ºã—æ¸¬åº¦ï¼‰ã€‚

**å•é¡Œ**: `$p_\theta(x)$` ãŒè§£æçš„ã«è¨ˆç®—ä¸èƒ½ã€‚

**è§£æ±º**: è­˜åˆ¥å™¨ `$D_\phi: \mathbb{R}^D \to [0, 1]$` ã‚’å°å…¥ã—ã€æ•µå¯¾çš„å­¦ç¿’:

```math
\min_\theta \max_\phi \mathbb{E}_{x \sim p_{\text{data}}}[\log D_\phi(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D_\phi(G_\theta(z)))]
```

**ç›´æ„Ÿ**: è­˜åˆ¥å™¨ãŒçœŸå½ã‚’åˆ¤å®šã§ããªããªã‚‹ã¾ã§ç”Ÿæˆå™¨ã‚’æ”¹å–„ã€‚

#### 3.4 Score Matching

**Score function**:

```math
s_\theta(x) = \nabla_x \log p_\theta(x)
```

**Score Matchingç›®çš„é–¢æ•°**:

```math
\mathcal{L}_{\text{SM}}(\theta) = \frac{1}{2} \mathbb{E}_{x \sim p_{\text{data}}}\left[\|s_\theta(x) - \nabla_x \log p_{\text{data}}(x)\|^2\right]
```

**å•é¡Œ**: `$\nabla_x \log p_{\text{data}}(x)$` ã¯æœªçŸ¥ã€‚

**è§£æ±º**: éƒ¨åˆ†ç©åˆ†ã‚’ç”¨ã„ãŸå¤‰å½¢ï¼ˆHyvÃ¤rinen 2005ï¼‰

```math
\mathcal{L}_{\text{SM}}(\theta) = \mathbb{E}_{x \sim p_{\text{data}}}\left[\text{Tr}\left(\nabla_x s_\theta(x)\right) + \frac{1}{2}\|s_\theta(x)\|^2\right] + \text{const}
```

ã“ã“ã§ `$\text{Tr}\left(\nabla_x s_\theta(x)\right) = \sum_i \frac{\partial^2 \log p_\theta(x)}{\partial x_i^2}$` ã¯Hessianã®ãƒˆãƒ¬ãƒ¼ã‚¹ã€‚

**åˆ©ç‚¹**: `$p_\theta(x)$` ã®æ­£è¦åŒ–å®šæ•°ä¸è¦ã€‚`$p_{\text{data}}$` ã®å‹¾é…ã‚‚ä¸è¦ã€‚

**Denoising Score Matching** (Vincent 2011):

ãƒã‚¤ã‚ºä»˜åŠ ãƒ‡ãƒ¼ã‚¿ `$\tilde{x} = x + \sigma \epsilon, \epsilon \sim \mathcal{N}(0, I)$` ã‚’ç”¨ã„ã¦:

```math
\mathcal{L}_{\text{DSM}}(\theta) = \frac{1}{2} \mathbb{E}_{x, \tilde{x}}\left[\|s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log p(\tilde{x} \mid x)\|^2\right]
```

ãƒã‚¤ã‚ºä»˜åŠ ã®æ¡ä»¶ä»˜ãåˆ†å¸ƒ `$p(\tilde{x} \mid x) = \mathcal{N}(\tilde{x} \mid x, \sigma^2 I)$` ã‚ˆã‚Š:

```math
\nabla_{\tilde{x}} \log p(\tilde{x} \mid x) = -\frac{\tilde{x} - x}{\sigma^2}
```

ã—ãŸãŒã£ã¦:

```math
\mathcal{L}_{\text{DSM}}(\theta) = \frac{1}{2} \mathbb{E}_{x, \epsilon}\left[\left\|s_\theta(x + \sigma\epsilon) + \frac{\epsilon}{\sigma}\right\|^2\right]
```

ã“ã‚ŒãŒ**æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«**ã®åŸºç¤ã€‚

#### 3.4.1 Score Matchingã¨æœ€å°¤æ¨å®šã®é–¢ä¿‚

**å®šç†** (HyvÃ¤rinen 2005):

æ­£å‰‡æ¡ä»¶ä¸‹ã§ã€Score Matchingã®ç›®çš„é–¢æ•°ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ†è§£ã§ãã‚‹:

```math
\mathcal{L}_{\text{SM}}(\theta) = D_{\text{KL}}(p_{\text{data}} \| p_\theta) + \text{const}
```

**è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ**:

```math
\mathcal{L}_{\text{SM}}(\theta) = \frac{1}{2}\mathbb{E}_{x \sim p_{\text{data}}}\left[\|s_\theta(x) - s_{\text{data}}(x)\|^2\right]
```

å±•é–‹:

```math
= \frac{1}{2}\mathbb{E}[s_\theta^2] - \mathbb{E}[s_\theta \cdot s_{\text{data}}] + \frac{1}{2}\mathbb{E}[s_{\text{data}}^2]
```

éƒ¨åˆ†ç©åˆ†ã«ã‚ˆã‚Š:

```math
\mathbb{E}_{x \sim p_{\text{data}}}[s_\theta(x) \cdot s_{\text{data}}(x)] = -\mathbb{E}_{x \sim p_{\text{data}}}[\text{Tr}(\nabla_x s_\theta(x))]
```

æœ€çµ‚çš„ã«:

```math
\mathcal{L}_{\text{SM}}(\theta) = D_{\text{KL}}(p_{\text{data}} \| p_\theta) + \text{const}
```

**çµè«–**: Score Matchingã¯KLæœ€å°åŒ– = MLEã¨ç­‰ä¾¡ã€‚

#### 3.4.2 æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¸ã®æ¥ç¶š

**DDPMç›®çš„é–¢æ•°** (Ho+ 2020):

```math
\mathcal{L}_{\text{DDPM}} = \mathbb{E}_{t, x_0, \epsilon}\left[\|\epsilon - \epsilon_\theta(x_t, t)\|^2\right]
```

ã“ã“ã§ `$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$`ã€‚

**Scoreé–¢æ•°ã¨ã®é–¢ä¿‚**:

```math
\epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} \cdot s_\theta(x_t, t)
```

ã—ãŸãŒã£ã¦:

```math
\mathcal{L}_{\text{DDPM}} = (1 - \bar{\alpha}_t) \mathbb{E}\left[\|s_\theta(x_t, t) - s_t(x_t)\|^2\right]
```

ã“ã“ã§ `$s_t(x_t) = -\frac{\epsilon}{\sqrt{1 - \bar{\alpha}_t}}$` ã¯çœŸã®Scoreé–¢æ•°ã€‚

**çµè«–**: DDPMã¯Denoising Score Matchingã®æ™‚é–“ä¾å­˜ç‰ˆã€‚

#### 3.4.3 Sliced Score Matchingï¼ˆé«˜æ¬¡å…ƒã§ã®åŠ¹ç‡åŒ–ï¼‰

**å•é¡Œ**: `$\text{Tr}(\nabla_x s_\theta(x))$` ã®è¨ˆç®—ã‚³ã‚¹ãƒˆãŒ `$O(d^2)$`ï¼ˆ`$d$` ã¯æ¬¡å…ƒï¼‰ã€‚

**è§£æ±º**: ãƒ©ãƒ³ãƒ€ãƒ å°„å½± `$v \sim \mathcal{N}(0, I)$` ã‚’ç”¨ã„ã¦:

```math
\mathcal{L}_{\text{SSM}}(\theta) = \frac{1}{2}\mathbb{E}_{x, v}\left[(v^T s_\theta(x))^2 + 2v^T \nabla_x s_\theta(x) v\right]
```

**åˆ©ç‚¹**: `$v^T \nabla_x s_\theta(x) v$` ã¯Hessian-vectorç©ï¼ˆè‡ªå‹•å¾®åˆ†ã§åŠ¹ç‡çš„ã«è¨ˆç®—å¯èƒ½ã€`$O(d)$`ï¼‰ã€‚

#### 3.4.4 GANç›®çš„é–¢æ•°ã®è©³ç´°å°å‡º

**Jensen-Shannon Divergence**:

```math
\text{JSD}(p \| q) = \frac{1}{2}D_{\text{KL}}\left(p \Big\| \frac{p+q}{2}\right) + \frac{1}{2}D_{\text{KL}}\left(q \Big\| \frac{p+q}{2}\right)
```

**GANç›®çš„é–¢æ•°** (Goodfellow+ 2014):

```math
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
```

**æœ€é©è­˜åˆ¥å™¨** `$D^*$`:

å›ºå®šã•ã‚ŒãŸ `$G$` ã«å¯¾ã—ã€`$V(D, G)$` ã‚’ `$D$` ã«ã¤ã„ã¦æœ€å¤§åŒ–:

```math
\frac{\delta V}{\delta D(x)} = \frac{p_{\text{data}}(x)}{D(x)} - \frac{p_G(x)}{1 - D(x)} = 0
```

```math
\Rightarrow \quad D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_G(x)}
```

**æœ€é©è­˜åˆ¥å™¨ã‚’ä»£å…¥**:

```math
V(G, D^*) = \mathbb{E}_{x \sim p_{\text{data}}}\left[\log \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_G(x)}\right] + \mathbb{E}_{x \sim p_G}\left[\log \frac{p_G(x)}{p_{\text{data}}(x) + p_G(x)}\right]
```

å¤‰å½¢:

```math
= -\log 4 + D_{\text{KL}}\left(p_{\text{data}} \Big\| \frac{p_{\text{data}} + p_G}{2}\right) + D_{\text{KL}}\left(p_G \Big\| \frac{p_{\text{data}} + p_G}{2}\right)
```

```math
= -\log 4 + 2 \cdot \text{JSD}(p_{\text{data}} \| p_G)
```

**çµè«–**: GANã¯Jensen-Shannon Divergenceã‚’æœ€å°åŒ–ã€‚

**JSD vs KL**:

- **KL**: éå¯¾ç§°ã€Mode-Covering/Seeking
- **JSD**: å¯¾ç§°ã€ä¸¡æ–¹å‘ã®KLã®å¹³å‡

#### 3.4.5 Wasserstein GANã¸ã®ç™ºå±•

**Wassersteinè·é›¢** (Earth Mover's Distance):

```math
W_1(p, q) = \inf_{\gamma \in \Pi(p, q)} \mathbb{E}_{(x, y) \sim \gamma}[\|x - y\|]
```

ã“ã“ã§ `$\Pi(p, q)$` ã¯ `$p$` ã¨ `$q$` ã‚’å‘¨è¾ºåˆ†å¸ƒã«æŒã¤åŒæ™‚åˆ†å¸ƒã®é›†åˆã€‚

**Kantorovich-RubinsteinåŒå¯¾æ€§**:

```math
W_1(p, q) = \sup_{\|f\|_L \leq 1} \left[\mathbb{E}_{x \sim p}[f(x)] - \mathbb{E}_{y \sim q}[f(y)]\right]
```

ã“ã“ã§ `$\|f\|_L \leq 1$` ã¯1-Lipschitzåˆ¶ç´„ã€‚

**WGANç›®çš„é–¢æ•°** (Arjovsky+ 2017):

```math
\min_G \max_{D: \|D\|_L \leq 1} \left[\mathbb{E}_{x \sim p_{\text{data}}}[D(x)] - \mathbb{E}_{z \sim p_z}[D(G(z))]\right]
```

**Lipschitzåˆ¶ç´„ã®å®Ÿè£…**:

1. **Weight clipping** (å…ƒè«–æ–‡): `$w \in [-c, c]$`ï¼ˆä¸å®‰å®šï¼‰
2. **Gradient penalty** (Gulrajani+ 2017): `$\lambda \mathbb{E}_{\hat{x}}[(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2]$`ï¼ˆæ¨å¥¨ï¼‰
3. **Spectral normalization** (Miyato+ 2018): å„å±¤ã®é‡ã¿è¡Œåˆ—ã‚’ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã§æ­£è¦åŒ–

**WGANã®åˆ©ç‚¹**:

- è¨“ç·´å®‰å®šæ€§
- ãƒ¢ãƒ¼ãƒ‰å´©å£Šã®ç·©å’Œ
- ç”Ÿæˆå“è³ªã®æŒ‡æ¨™ã¨ã—ã¦ä½¿ãˆã‚‹ï¼ˆWassersteinè·é›¢ãã®ã‚‚ã®ï¼‰

#### 3.5 Mode-Covering vs Mode-Seeking

**å‰å‘ãKL** `$D_{\text{KL}}(p_{\text{data}} \| p_\theta)$`:

```math
D_{\text{KL}}(p_{\text{data}} \| p_\theta) = \mathbb{E}_{x \sim p_{\text{data}}}\left[\log \frac{p_{\text{data}}(x)}{p_\theta(x)}\right]
```

`$p_{\text{data}}(x) > 0$` ã®ã¨ã `$p_\theta(x) > 0$` ã§ãªã‘ã‚Œã°ç™ºæ•£ â†’ **Mode-Covering**ï¼ˆå…¨ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼ï¼‰

**çµæœ**: ã¼ã‚„ã‘ãŸç”Ÿæˆï¼ˆVAE, Diffusionï¼‰

**é€†å‘ãKL** `$D_{\text{KL}}(p_\theta \| p_{\text{data}})$`:

```math
D_{\text{KL}}(p_\theta \| p_{\text{data}}) = \mathbb{E}_{x \sim p_\theta}\left[\log \frac{p_\theta(x)}{p_{\text{data}}(x)}\right]
```

`$p_\theta(x) > 0$` ã®ã¨ã `$p_{\text{data}}(x) > 0$` ã§ãªã‘ã‚Œã°ç™ºæ•£ â†’ **Mode-Seeking**ï¼ˆæœ€ã‚‚ç¢ºç‡ã®é«˜ã„ãƒ¢ãƒ¼ãƒ‰ã«é›†ä¸­ï¼‰

**çµæœ**: é®®æ˜ã ãŒå¤šæ§˜æ€§ä½ï¼ˆGANï¼‰

### Quick Check 5

VAEã¨GANã®KLæ–¹å‘ã®é•ã„ã‚’èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

- **VAE**: å‰å‘ãKL `$D_{\text{KL}}(p_{\text{data}} \| p_\theta)$` â†’ Mode-Covering â†’ ã¼ã‚„ã‘ãŸç”Ÿæˆ
- **GAN**: é€†å‘ãKL `$D_{\text{KL}}(p_\theta \| p_{\text{data}})$` â†’ Mode-Seeking â†’ é®®æ˜ã ãŒå¤šæ§˜æ€§ä½

</details>

---

### Topic 4: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç†è«–ã¨è©•ä¾¡æŒ‡æ¨™

#### 4.1 äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« `$p_\theta(x, z) = p_\theta(x \mid z)p(z)$` ã«ãŠã„ã¦ã€äº‹å¾Œåˆ†å¸ƒ `$p_\theta(z \mid x)$` ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ã€‚

**å•é¡Œ**: äº‹å¾Œåˆ†å¸ƒãŒè§£æçš„ã«è¨ˆç®—ä¸èƒ½ãªå ´åˆãŒå¤šã„ã€‚

**è§£æ±ºç­–**:

##### 4.1.1 Rejection Samplingï¼ˆæ£„å´ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰

ææ¡ˆåˆ†å¸ƒ `$q(z)$` ã‹ã‚‰å€™è£œ `$z' \sim q(z)$` ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã€ç¢ºç‡ `$\frac{p(z')}{M \cdot q(z')}$` ã§å—ç†ã€‚

**å•é¡Œ**: é«˜æ¬¡å…ƒã§å—ç†ç‡ãŒæŒ‡æ•°çš„ã«ä½ä¸‹ã€‚

##### 4.1.2 Importance Samplingï¼ˆé‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰

æœŸå¾…å€¤ã®è¨ˆç®—:

```math
\mathbb{E}_{z \sim p(z)}[f(z)] = \mathbb{E}_{z \sim q(z)}\left[f(z) \frac{p(z)}{q(z)}\right]
```

ã“ã“ã§ `$w(z) = \frac{p(z)}{q(z)}$` ã¯é‡ã¿ã€‚

**å•é¡Œ**: `$q(z)$` ã®é¸æŠãŒé›£ã—ã„ã€‚

##### 4.1.3 MCMCï¼ˆMarkov Chain Monte Carloï¼‰

**Metropolis-Hastingsç®—æ³•**:

1. ç¾åœ¨ã®çŠ¶æ…‹ `$z^{(t)}$` ã‹ã‚‰ææ¡ˆåˆ†å¸ƒ `$q(z' \mid z^{(t)})$` ã§å€™è£œ `$z'$` ã‚’ã‚µãƒ³ãƒ—ãƒ«
2. å—ç†ç¢ºç‡:

```math
\alpha(z', z^{(t)}) = \min\left(1, \frac{p(z') q(z^{(t)} \mid z')}{p(z^{(t)}) q(z' \mid z^{(t)})}\right)
```

3. ç¢ºç‡ `$\alpha$` ã§ `$z^{(t+1)} = z'$`ã€ã•ã‚‚ãªãã° `$z^{(t+1)} = z^{(t)}$`

**å®šå¸¸åˆ†å¸ƒ**: `$p(z)$`

**å•é¡Œ**: åæŸãŒé…ã„ï¼ˆmixing timeï¼‰ã€‚

##### 4.1.4 Reparameterization Trickï¼ˆVAEã®æ ¸å¿ƒï¼‰

æ½œåœ¨å¤‰æ•° `$z \sim q_\phi(z \mid x)$` ã‚’ã€æ±ºå®šçš„å¤‰æ› `$z = g_\phi(\epsilon, x)$` ã¨ãƒã‚¤ã‚º `$\epsilon \sim p(\epsilon)$` ã§è¡¨ç¾:

**ä¾‹: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ**

```math
z \sim \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x)) \quad \Rightarrow \quad z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)
```

**åˆ©ç‚¹**: å‹¾é…ãŒ `$z$` ã‚’é€šéå¯èƒ½ â†’ `$\phi$` ã§å¾®åˆ†å¯èƒ½ã€‚

```math
\nabla_\phi \mathbb{E}_{z \sim q_\phi(z \mid x)}[f(z)] = \mathbb{E}_{\epsilon \sim p(\epsilon)}[\nabla_\phi f(g_\phi(\epsilon, x))]
```

ã“ã‚ŒãŒVAEã®å­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚

#### 4.2 ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’ã©ã†è©•ä¾¡ã™ã‚‹ã‹ï¼Ÿ

##### 4.2.1 Inception Score (IS)

```math
\text{IS}(G) = \exp\left(\mathbb{E}_{x \sim p_G}[D_{\text{KL}}(p(y \mid x) \| p(y))]\right)
```

ã“ã“ã§ `$p(y \mid x)$` ã¯Inceptionãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹äºˆæ¸¬ã€‚

**ç›´æ„Ÿ**: ç”Ÿæˆç”»åƒãŒæ˜ç¢ºãªã‚¯ãƒ©ã‚¹ã«å±ã—ï¼ˆ`$p(y \mid x)$` ãŒã‚·ãƒ£ãƒ¼ãƒ—ï¼‰ã€å…¨ä½“ã¨ã—ã¦å¤šæ§˜ï¼ˆ`$p(y)$` ãŒå‡ä¸€ï¼‰ãªã‚‰é«˜ã‚¹ã‚³ã‚¢ã€‚

**å•é¡Œ**: Inceptionãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜ã€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’è€ƒæ…®ã—ãªã„ã€‚

##### 4.2.2 FrÃ©chet Inception Distance (FID)

**å®šç¾©**:

```math
\text{FID}(p_{\text{data}}, p_G) = \|\mu_{\text{data}} - \mu_G\|^2 + \text{Tr}\left(\Sigma_{\text{data}} + \Sigma_G - 2(\Sigma_{\text{data}} \Sigma_G)^{1/2}\right)
```

ã“ã“ã§ `$\mu, \Sigma$` ã¯Inceptionç‰¹å¾´ã®å¹³å‡ãƒ»å…±åˆ†æ•£è¡Œåˆ—ã€‚

**ç›´æ„Ÿ**: Inceptionç‰¹å¾´ç©ºé–“ã§ã®2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã®FrÃ©chetè·é›¢ã€‚

**å•é¡Œ** (Jayasumana+ 2024):
1. Inceptionã®è¡¨ç¾åŠ›ãŒç¾ä»£ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆText-to-Imageï¼‰ã«ã¯ä¸ååˆ†
2. ã‚¬ã‚¦ã‚¹ä»®å®šãŒä¸é©åˆ‡
3. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã«æ•æ„Ÿ

##### 4.2.3 CMMD (Rethinking FID, CVPR 2024)

**CLIP Maximum Mean Discrepancy**:

```math
\text{CMMD}^2(p_{\text{data}}, p_G) = \mathbb{E}_{x, x' \sim p_{\text{data}}}[k(x, x')] + \mathbb{E}_{y, y' \sim p_G}[k(y, y')] - 2\mathbb{E}_{x \sim p_{\text{data}}, y \sim p_G}[k(x, y)]
```

ã“ã“ã§ `$k(x, y) = \exp(-\|\phi(x) - \phi(y)\|^2 / (2\sigma^2))$` ã¯RBFã‚«ãƒ¼ãƒãƒ«ã€`$\phi$` ã¯CLIPåŸ‹ã‚è¾¼ã¿ã€‚

**åˆ©ç‚¹**:
1. CLIPåŸ‹ã‚è¾¼ã¿ â†’ ãƒªãƒƒãƒãªè¡¨ç¾ï¼ˆText-to-Imageã«é©åˆï¼‰
2. åˆ†å¸ƒã®ä»®å®šãªã—ï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼‰
3. ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ãŒè‰¯ã„ï¼ˆFIDã‚ˆã‚Š2æ¡å°‘ãªã„ã‚µãƒ³ãƒ—ãƒ«ã§å®‰å®šï¼‰

**ç†è«–çš„ä¿è¨¼**: CMMDã¯çœŸã®MMDã®ä¸åæ¨å®šé‡ã€‚

#### 4.3 LLMã¨æœ€å°¤æ¨å®š

**è‡ªå·±å›å¸°è¨€èªãƒ¢ãƒ‡ãƒ«**:

```math
p_\theta(\mathbf{x}) = \prod_{t=1}^T p_\theta(x_t \mid x_{<t})
```

ã“ã“ã§ `$\mathbf{x} = (x_1, \ldots, x_T)$` ã¯ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã€‚

**å¯¾æ•°å°¤åº¦**:

```math
\log p_\theta(\mathbf{x}) = \sum_{t=1}^T \log p_\theta(x_t \mid x_{<t})
```

**MLEã®å­¦ç¿’**:

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{i=1}^n \sum_{t=1}^{T_i} \log p_\theta(x_t^{(i)} \mid x_{<t}^{(i)})
```

**å®Ÿè£…**: Cross-Entropyæå¤±

```math
\mathcal{L}_{\text{CE}} = -\sum_{t=1}^T \log p_\theta(x_t \mid x_{<t})
```

**Perplexity**:

```math
\text{PPL} = \exp\left(-\frac{1}{T}\sum_{t=1}^T \log p_\theta(x_t \mid x_{<t})\right) = \exp(H(\hat{p}_{\text{data}}, p_\theta))
```

PerplexityãŒä½ã„ã»ã©ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆãèª¬æ˜ã€‚

### Quick Check 6

FIDã®3ã¤ã®å•é¡Œç‚¹ã‚’æŒ™ã’ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

1. Inceptionã®è¡¨ç¾åŠ›ä¸è¶³ï¼ˆText-to-Imageã«ä¸é©ï¼‰
2. ã‚¬ã‚¦ã‚¹ä»®å®šãŒä¸é©åˆ‡
3. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã«æ•æ„Ÿï¼ˆ1ä¸‡æšä»¥ä¸Šå¿…è¦ï¼‰

CMMDã¯ã“ã‚Œã‚‰ã‚’è§£æ±ºï¼ˆCLIPåŸ‹ã‚è¾¼ã¿ãƒ»ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ»ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡è‰¯ï¼‰ã€‚

</details>

---

### Topic 5: Boss Battle â€” MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“å®Œå…¨è¨¼æ˜

#### 5.1 è¨¼æ˜ã®ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

```mermaid
graph TD
    A[MLEå®šç¾©] --> B[å¯¾æ•°å°¤åº¦æœ€å¤§åŒ–]
    B --> C[æœŸå¾…å€¤è¡¨ç¾]
    C --> D[Cross-Entropyæœ€å°åŒ–]
    D --> E[ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†è§£]
    E --> F[KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æœ€å°åŒ–]
```

#### 5.2 ã‚¹ãƒ†ãƒƒãƒ—1: MLEã‹ã‚‰æœŸå¾…å€¤è¡¨ç¾ã¸

**MLEç›®çš„é–¢æ•°**:

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{i=1}^n \log p_\theta(x_i)
```

ãƒ‡ãƒ¼ã‚¿ `$\{x_1, \ldots, x_n\}$` ãŒ i.i.d. ã« `$p_{\text{data}}(x)$` ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸã¨ã™ã‚‹ã€‚å¤§æ•°ã®æ³•å‰‡ã‚ˆã‚Š:

```math
\frac{1}{n}\sum_{i=1}^n \log p_\theta(x_i) \xrightarrow{n \to \infty} \mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)]
```

ã—ãŸãŒã£ã¦ã€å¤§æ¨™æœ¬æ¥µé™ã§:

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)]
```

#### 5.3 ã‚¹ãƒ†ãƒƒãƒ—2: Cross-Entropyæœ€å°åŒ–ã¸ã®å¤‰æ›

**Cross-Entropyã®å®šç¾©**:

```math
H(p_{\text{data}}, p_\theta) = -\mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)]
```

ã—ãŸãŒã£ã¦:

```math
\arg\max_\theta \mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)] = \arg\min_\theta H(p_{\text{data}}, p_\theta)
```

**çµè«–**: MLEã¯Cross-Entropyæœ€å°åŒ–ã¨ç­‰ä¾¡ã€‚

#### 5.4 ã‚¹ãƒ†ãƒƒãƒ—3: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¸ã®åˆ†è§£

**KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®å®šç¾©**:

```math
D_{\text{KL}}(p_{\text{data}} \| p_\theta) = \mathbb{E}_{x \sim p_{\text{data}}}\left[\log \frac{p_{\text{data}}(x)}{p_\theta(x)}\right]
```

å±•é–‹:

```math
D_{\text{KL}}(p_{\text{data}} \| p_\theta) = \mathbb{E}_{x \sim p_{\text{data}}}[\log p_{\text{data}}(x)] - \mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)]
```

ç¬¬1é …ã¯ `$H(p_{\text{data}})$`ï¼ˆãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€å®šæ•°ï¼‰ã€ç¬¬2é …ã¯ `$-H(p_{\text{data}}, p_\theta)$`ã€‚

```math
D_{\text{KL}}(p_{\text{data}} \| p_\theta) = -H(p_{\text{data}}) + H(p_{\text{data}}, p_\theta)
```

`$\theta$` ã«ã¤ã„ã¦æœ€å°åŒ–ã™ã‚‹ã¨ãã€`$H(p_{\text{data}})$` ã¯å®šæ•°ãªã®ã§:

```math
\arg\min_\theta D_{\text{KL}}(p_{\text{data}} \| p_\theta) = \arg\min_\theta H(p_{\text{data}}, p_\theta) = \arg\max_\theta \mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)]
```

#### 5.5 ä¸‰ä½ä¸€ä½“ã®å®Œæˆ

```math
\boxed{
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \mathbb{E}_{x \sim p_{\text{data}}}[\log p_\theta(x)] = \arg\min_\theta H(p_{\text{data}}, p_\theta) = \arg\min_\theta D_{\text{KL}}(p_{\text{data}} \| p_\theta)
}
```

**æ•°å­¦çš„æ„å‘³**:

1. **MLE**: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®å¯¾æ•°å°¤åº¦ã‚’æœ€å¤§åŒ–
2. **Cross-Entropy**: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã¨ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒã®é–“ã®Cross-Entropyã‚’æœ€å°åŒ–
3. **KL**: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã¨ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒã®é–“ã®å‰å‘ãKLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’æœ€å°åŒ–

ã“ã‚Œã‚‰ã¯ã™ã¹ã¦**åŒã˜æœ€é©åŒ–å•é¡Œ**ã‚’ç•°ãªã‚‹è¦–ç‚¹ã§è¡¨ç¾ã—ãŸã‚‚ã®ã€‚

#### 5.6 å®Ÿè·µã¸ã®æ¥ç¶š

**VAE**: ELBOã‚’æœ€å¤§åŒ– `$\Rightarrow$` `$D_{\text{KL}}(q_\phi(z \mid x) \| p_\theta(z \mid x))$` ã‚’æš—ã«æœ€å°åŒ–

**Diffusion**: Score matching `$\Rightarrow$` KLã®å¤‰åˆ†ä¸‹ç•Œã‚’æœ€å°åŒ–

**LLM**: Cross-Entropyæå¤± `$\Rightarrow$` æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã§MLEã‚’å®Ÿè¡Œ

ã™ã¹ã¦ã®é“ã¯MLEã«é€šãšã€‚

### Quick Check 7

MLEã¨KLæœ€å°åŒ–ãŒç­‰ä¾¡ã§ã‚ã‚‹ç†ç”±ã‚’ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®é …ã§èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

```math
D_{\text{KL}}(p_{\text{data}} \| p_\theta) = H(p_{\text{data}}, p_\theta) - H(p_{\text{data}})
```

`$H(p_{\text{data}})$` ã¯å®šæ•°ãªã®ã§ã€KLæœ€å°åŒ– `$\Leftrightarrow$` Cross-Entropyæœ€å°åŒ– `$\Leftrightarrow$` MLEã€‚

</details>

#### 5.7 æ•°å€¤æ¤œè¨¼: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã§ã®MLE

**å•é¡Œè¨­å®š**: ãƒ‡ãƒ¼ã‚¿ `$\{x_1, \ldots, x_n\}$` ãŒ `$\mathcal{N}(\mu_0, \sigma_0^2)$` ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸã¨ã™ã‚‹ã€‚

**MLEã®è§£æè§£**:

å°¤åº¦é–¢æ•°:

```math
L(\mu, \sigma^2 \mid \mathcal{D}) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)
```

å¯¾æ•°å°¤åº¦:

```math
\ell(\mu, \sigma^2 \mid \mathcal{D}) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2
```

**ã‚¹ãƒ†ãƒƒãƒ—1**: `$\mu$` ã«é–¢ã™ã‚‹æœ€å¤§åŒ–

```math
\frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2}\sum_{i=1}^n (x_i - \mu) = 0
```

```math
\Rightarrow \quad \hat{\mu}_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n x_i = \bar{x}
```

**ã‚¹ãƒ†ãƒƒãƒ—2**: `$\sigma^2$` ã«é–¢ã™ã‚‹æœ€å¤§åŒ–

`$\mu = \hat{\mu}_{\text{MLE}}$` ã‚’ä»£å…¥:

```math
\frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^n (x_i - \bar{x})^2 = 0
```

```math
\Rightarrow \quad \hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2
```

**æ³¨æ„**: `$\hat{\sigma}^2_{\text{MLE}}$` ã¯åã‚Šã‚ã‚Šã€‚ä¸åæ¨å®šé‡ã¯ `$s^2 = \frac{1}{n-1}\sum(x_i - \bar{x})^2$`ã€‚

#### 5.8 ä¸€èˆ¬åŒ–: æŒ‡æ•°å‹åˆ†å¸ƒæ—ã§ã®MLE

**æŒ‡æ•°å‹åˆ†å¸ƒæ—**:

```math
p(x \mid \eta) = h(x) \exp(\eta^T T(x) - A(\eta))
```

ã“ã“ã§:
- `$\eta$`: è‡ªç„¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆnatural parameterï¼‰
- `$T(x)$`: ååˆ†çµ±è¨ˆé‡ï¼ˆsufficient statisticï¼‰
- `$A(\eta)$`: å¯¾æ•°åˆ†é…é–¢æ•°ï¼ˆlog-partition functionï¼‰

**å¯¾æ•°å°¤åº¦**:

```math
\ell(\eta \mid \mathcal{D}) = \sum_{i=1}^n \left[\eta^T T(x_i) - A(\eta) + \log h(x_i)\right]
```

**MLEæ¡ä»¶**:

```math
\frac{\partial \ell}{\partial \eta} = \sum_{i=1}^n T(x_i) - n \nabla_\eta A(\eta) = 0
```

```math
\Rightarrow \quad \nabla_\eta A(\hat{\eta}_{\text{MLE}}) = \frac{1}{n}\sum_{i=1}^n T(x_i)
```

**çµè«–**: MLEã¯ååˆ†çµ±è¨ˆé‡ã®å¹³å‡ã‚’`$\nabla_\eta A(\eta)$`ã«ä¸€è‡´ã•ã›ã‚‹ã€‚

**ä¾‹**: æ­£è¦åˆ†å¸ƒ `$\mathcal{N}(\mu, \sigma^2)$`

```math
\eta = \begin{pmatrix} \mu/\sigma^2 \\ -1/(2\sigma^2) \end{pmatrix}, \quad T(x) = \begin{pmatrix} x \\ x^2 \end{pmatrix}
```

#### 5.9 æ¡ä»¶ä»˜ãå°¤åº¦ vs å‘¨è¾ºå°¤åº¦

**æ•™å¸«ã‚ã‚Šå­¦ç¿’**: æ¡ä»¶ä»˜ãå°¤åº¦ `$p_\theta(y \mid x)$`

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{i=1}^n \log p_\theta(y_i \mid x_i)
```

**ä¾‹**: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆåˆ†é¡å™¨

**æ•™å¸«ãªã—å­¦ç¿’**: å‘¨è¾ºå°¤åº¦ `$p_\theta(x)$`

```math
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{i=1}^n \log p_\theta(x_i)
```

**ä¾‹**: VAE, Diffusion, è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«

**æ½œåœ¨å¤‰æ•°ãŒã‚ã‚‹å ´åˆ**: å‘¨è¾ºåŒ–

```math
p_\theta(x) = \int p_\theta(x \mid z) p(z) dz
```

ã“ã®ç©åˆ†ãŒè¨ˆç®—å›°é›£ â†’ EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆç¬¬8å›ï¼‰

#### 5.10 ç¢ºç‡å¯†åº¦æ¨å®šã®2å¤§æµæ´¾

**ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¨å®š**: `$p_\theta(x)$` ã‚’ä»®å®š

- **åˆ©ç‚¹**: ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã€è§£é‡ˆæ€§
- **æ¬ ç‚¹**: ãƒ¢ãƒ‡ãƒ«èª¤æŒ‡å®šã®ãƒªã‚¹ã‚¯

**ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¨å®š**: ãƒ‡ãƒ¼ã‚¿é§†å‹•

**ä¾‹: Kernel Density Estimation (KDE)**

```math
\hat{p}(x) = \frac{1}{nh}\sum_{i=1}^n K\left(\frac{x - x_i}{h}\right)
```

ã“ã“ã§ `$K(\cdot)$` ã¯ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆä¾‹: ã‚¬ã‚¦ã‚¹ï¼‰ã€`$h$` ã¯å¸¯åŸŸå¹…ã€‚

- **åˆ©ç‚¹**: ãƒ¢ãƒ‡ãƒ«ä»®å®šä¸è¦
- **æ¬ ç‚¹**: é«˜æ¬¡å…ƒã§ç ´ç¶»ï¼ˆæ¬¡å…ƒã®å‘ªã„ï¼‰

**ç¾ä»£ã®æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«**: ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼ˆNNã®è¡¨ç¾åŠ›ã§"æŸ”è»Ÿãª"ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼‰

---

## PB ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

### çµ±è¨ˆå­¦ vs æ©Ÿæ¢°å­¦ç¿’ã®è¦–ç‚¹

ãªãœçµ±è¨ˆå­¦è€…ã¯"æ¨å®š"ã¨å‘¼ã³ã€æ©Ÿæ¢°å­¦ç¿’ç ”ç©¶è€…ã¯"å­¦ç¿’"ã¨å‘¼ã¶ã®ã‹ï¼Ÿ

**çµ±è¨ˆå­¦**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ `$\theta$` ã®æ¨å®šå•é¡Œ

- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º `$n$` ã¯æœ‰é™
- æ¼¸è¿‘ç†è«–ï¼ˆ`$n \to \infty$`ï¼‰ã§æ€§è³ªã‚’è§£æ
- ä¸åæ€§ãƒ»æœ‰åŠ¹æ€§ãƒ»ä¸€è‡´æ€§ã‚’é‡è¦–

**æ©Ÿæ¢°å­¦ç¿’**: åˆ†å¸ƒ `$p_\theta(x)$` ã®å­¦ç¿’å•é¡Œ

- ãƒ‡ãƒ¼ã‚¿ã¯è±Šå¯Œï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
- è¡¨ç¾åŠ›ï¼ˆNeural Networkã®å®¹é‡ï¼‰ã‚’é‡è¦–
- æ±åŒ–æ€§èƒ½ï¼ˆãƒ†ã‚¹ãƒˆèª¤å·®ï¼‰ã§è©•ä¾¡

**æœ¬è³ª**: åŒã˜æ•°å­¦çš„æœ€é©åŒ–å•é¡Œã‚’ç•°ãªã‚‹è¦–ç‚¹ã§æ‰ãˆã¦ã„ã‚‹ã€‚

### ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆ

**å¤å…¸çš„çµ±è¨ˆ**: ä½æ¬¡å…ƒã®ç¢ºç‡åˆ†å¸ƒã‚’ä»®å®šï¼ˆæ­£è¦åˆ†å¸ƒã€æ··åˆãƒ¢ãƒ‡ãƒ«ï¼‰

**ç¾ä»£ã®æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«**: é«˜æ¬¡å…ƒã®è¤‡é›‘ãªåˆ†å¸ƒã‚’å­¦ç¿’

- **VAE**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ã®NN
- **Diffusion**: U-Net ã«ã‚ˆã‚‹Scoreé–¢æ•°è¿‘ä¼¼
- **LLM**: Transformerã«ã‚ˆã‚‹æ¡ä»¶ä»˜ãåˆ†å¸ƒ

ã—ã‹ã—ã€**æ•°å­¦çš„åŸç†ã¯MLEãã®ã¾ã¾**ã€‚

### æ¸¬åº¦è«–çš„è¦–ç‚¹: Pushforward Measure

**ç”Ÿæˆå™¨** `$G_\theta: \mathbb{R}^d \to \mathbb{R}^D$`ï¼ˆä¾‹: ãƒã‚¤ã‚º â†’ ç”»åƒï¼‰

**Pushforwardæ¸¬åº¦**:

```math
p_\theta(x) = (G_\theta)_\# p_z(z)
```

å®šç¾©: ä»»æ„ã®å¯æ¸¬é›†åˆ `$A \subset \mathbb{R}^D$` ã«å¯¾ã—

```math
p_\theta(A) = p_z(G_\theta^{-1}(A))
```

**ç›´æ„Ÿ**: ãƒã‚¤ã‚ºåˆ†å¸ƒ `$p_z(z)$` ã‚’ `$G_\theta$` ã§"æŠ¼ã—å‡ºã—ã¦"ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ `$p_\theta(x)$` ã‚’ç”Ÿæˆã€‚

**GANã®åŸç†**: `$G_\theta$` ã‚’å­¦ç¿’ã—ã¦ `$(G_\theta)_\# p_z \approx p_{\text{data}}$` ã‚’é”æˆã€‚

### æƒ…å ±ç†è«–çš„è¦–ç‚¹: Rate-Distortion Theory

**Rate-Distortioné–¢æ•°**:

```math
R(D) = \min_{p(z \mid x): \mathbb{E}[d(x, \hat{x})] \leq D} I(X; Z)
```

ã“ã“ã§ `$I(X; Z)$` ã¯ç›¸äº’æƒ…å ±é‡ã€`$d(x, \hat{x})$` ã¯æ­ªã¿ã€‚

**ç›´æ„Ÿ**: æ­ªã¿ `$D$` ã‚’è¨±å®¹ã™ã‚‹ä¸‹ã§ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã«å¿…è¦ãªæƒ…å ±é‡ã®æœ€å°å€¤ã€‚

**VAEã¨ã®æ¥ç¶š**: ELBOã¯ rate-distortion ã®å¤‰åˆ†ä¸‹ç•Œ

```math
\text{ELBO} = \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)] - D_{\text{KL}}(q_\phi(z \mid x) \| p(z))
```

- ç¬¬1é …: å†æ§‹æˆé …ï¼ˆæ­ªã¿ã®æœ€å°åŒ–ï¼‰
- ç¬¬2é …: ãƒ¬ãƒ¼ãƒˆé …ï¼ˆæ½œåœ¨ç¬¦å·ã®åœ§ç¸®ï¼‰

**ç¾ä»£ã®ç ”ç©¶** (2025): Rate-Distortion-Perception (RDP) ç†è«–ã®çµ±åˆ[^rdp2025]

#### Rate-Distortion-Perception Tradeoff

**RDPãƒˆãƒ©ã‚¤ã‚¢ãƒ³ã‚°ãƒ«** (Blau & Michaeli 2019):

```math
\min_{p(\hat{x} \mid x)} \mathbb{E}[d(x, \hat{x})] \quad \text{s.t.} \quad I(X; \hat{X}) \leq R, \quad d_{\text{percep}}(p_x, p_{\hat{x}}) \leq P
```

3ã¤ã®æŒ‡æ¨™:
1. **Rate** `$R$`: åœ§ç¸®ç‡ï¼ˆãƒ“ãƒƒãƒˆ/ã‚µãƒ³ãƒ—ãƒ«ï¼‰
2. **Distortion** `$D$`: å†æ§‹æˆèª¤å·®ï¼ˆä¾‹: MSEï¼‰
3. **Perception** `$P$`: çŸ¥è¦šçš„å“è³ªï¼ˆä¾‹: LPIPSï¼‰

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**: `$R \downarrow$`ï¼ˆåœ§ç¸®ï¼‰ã€`$D \downarrow$`ï¼ˆé«˜ç²¾åº¦ï¼‰ã€`$P \downarrow$`ï¼ˆé«˜å“è³ªï¼‰ã®3ã¤ã‚’åŒæ™‚ã«æœ€å°åŒ–ã¯ä¸å¯èƒ½ã€‚

**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¸ã®å¿œç”¨**:
- **VAE**: Rate-Distortionæœ€é©åŒ–ï¼ˆçŸ¥è¦šå“è³ªã¯äºŒã®æ¬¡ï¼‰
- **GAN**: Perceptioné‡è¦–ï¼ˆRateã¯è€ƒæ…®ã›ãšï¼‰
- **Diffusion**: Rate-Distortion-Perceptionã®ãƒãƒ©ãƒ³ã‚¹

### å®Ÿè·µçš„ãªæ´å¯Ÿ

#### MLEã®æˆåŠŸæ¡ä»¶

1. **ãƒ¢ãƒ‡ãƒ«ä»®å®šãŒé©åˆ‡** â€” `$p_\theta(x)$` ãŒçœŸã®åˆ†å¸ƒ `$p_{\text{data}}(x)$` ã‚’è¡¨ç¾å¯èƒ½
2. **ååˆ†ãªãƒ‡ãƒ¼ã‚¿** â€” ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º `$n$` ãŒå¤§ãã„ï¼ˆæ¼¸è¿‘è«–ãŒæˆç«‹ï¼‰
3. **æœ€é©åŒ–å¯èƒ½** â€” å°¤åº¦é–¢æ•°ãŒå‡¸ã€ã¾ãŸã¯è‰¯ã„åˆæœŸå€¤ãŒã‚ã‚‹
4. **æ¬¡å…ƒãŒä½ã„** â€” æ¬¡å…ƒã®å‘ªã„ã‚’å›é¿ï¼ˆã¾ãŸã¯å¤šæ§˜ä½“ä»®èª¬ãŒæˆç«‹ï¼‰

#### MLEã®å¤±æ•—ä¾‹ã¨ãã®å¯¾ç­–

**å¤±æ•—1: ãƒ¢ãƒ‡ãƒ«èª¤æŒ‡å®š**

ä¾‹: çœŸã®åˆ†å¸ƒãŒå¤šå³°æ€§ã ãŒã€å˜å³°æ€§ã®æ­£è¦åˆ†å¸ƒã‚’ä»®å®šã€‚

**å¯¾ç­–**: ã‚ˆã‚Šè¡¨ç¾åŠ›ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆæ··åˆãƒ¢ãƒ‡ãƒ«ã€NNï¼‰

**å¤±æ•—2: éå­¦ç¿’**

`$n$` ãŒå°ã•ã `$|\theta|$` ãŒå¤§ãã„å ´åˆã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éé©åˆã€‚

**å¯¾ç­–**: æ­£å‰‡åŒ–ï¼ˆL1/L2ï¼‰ã€ãƒ™ã‚¤ã‚ºæ¨å®šï¼ˆMAPï¼‰ã€Early stopping

**å¤±æ•—3: å±€æ‰€æœ€é©è§£**

éå‡¸æœ€é©åŒ–å•é¡Œï¼ˆä¾‹: GMM, NNï¼‰ã§ã¯å±€æ‰€æœ€é©è§£ã«é™¥ã‚‹ã€‚

**å¯¾ç­–**: è¤‡æ•°ã®åˆæœŸå€¤ã‹ã‚‰æœ€é©åŒ–ã€Deterministic annealing

**å¤±æ•—4: æ•°å€¤ä¸å®‰å®šæ€§**

ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼ï¼ˆ`$p_\theta(x_i) \to 0$`ï¼‰ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã€‚

**å¯¾ç­–**: å¯¾æ•°å°¤åº¦ã§è¨ˆç®—ã€Log-sum-expãƒˆãƒªãƒƒã‚¯

#### ç¾ä»£ã®æ·±å±¤å­¦ç¿’ã«ãŠã‘ã‚‹MLE

**å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰**:

```math
\max_\theta \sum_{i=1}^n \sum_{t=1}^{T_i} \log p_\theta(x_t^{(i)} \mid x_{<t}^{(i)})
```

- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: æ•°åƒå„„
- æœ€é©åŒ–: Adam + å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
- è¨ˆç®—: åˆ†æ•£å­¦ç¿’ï¼ˆData/Tensor/Pipelineä¸¦åˆ—ï¼‰

**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆDiffusionï¼‰**:

```math
\max_\theta \sum_{i=1}^n \sum_{t=1}^T \log p_\theta(x_{t-1}^{(i)} \mid x_t^{(i)})
```

- Score matchingç›®çš„é–¢æ•°ã§è¿‘ä¼¼
- U-Netã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- æ•°ç™¾ä¸‡ã€œæ•°åå„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

**å…±é€šç‚¹**: ã©ã¡ã‚‰ã‚‚MLEã®å¤‰å½¢ã‚’å¤§è¦æ¨¡ã«æœ€é©åŒ–ã€‚

#### ã‚³ãƒ¼ãƒ‰å®Ÿè£…ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. **å¯¾æ•°å°¤åº¦ã‚’ä½¿ã†** â€” ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼å›é¿
2. **Log-sum-expãƒˆãƒªãƒƒã‚¯** â€” `$\log \sum \exp(x_i) = \max(x) + \log \sum \exp(x_i - \max(x))$`
3. **æ•°å€¤å¾®åˆ†ã§å‹¾é…æ¤œè¨¼** â€” è‡ªå‹•å¾®åˆ†ã®æ­£å½“æ€§ç¢ºèª
4. **ãƒãƒƒãƒå‡¦ç†** â€” GPUæ´»ç”¨
5. **Checkpointing** â€” é•·æ™‚é–“è¨“ç·´ã§ã®ä¸­æ–­å¯¾ç­–
6. **Logging** â€” å°¤åº¦ã®æ¨ç§»ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

#### MLEã¨ä»–ã®æ¨å®šæ‰‹æ³•ã®æ¯”è¼ƒ

| æ‰‹æ³• | ç›®çš„é–¢æ•° | ç‰¹å¾´ | ç”¨é€” |
|:-----|:---------|:-----|:-----|
| **MLE** | `$\max \sum \log p_\theta(x_i)$` | æœ€ã‚‚åŸºæœ¬çš„ã€æ¼¸è¿‘æœ‰åŠ¹ | æ¨™æº–çš„ãªçµ±è¨ˆæ¨å®š |
| **MAP** | `$\max [\log p(\mathcal{D} \mid \theta) + \log p(\theta)]$` | äº‹å‰åˆ†å¸ƒã‚’å°å…¥ | æ­£å‰‡åŒ–ã€å°ã‚µãƒ³ãƒ—ãƒ« |
| **Bayesian** | `$p(\theta \mid \mathcal{D})$` å…¨ä½“ | ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ– | æ„æ€æ±ºå®šã€Active learning |
| **Mæ¨å®š** | `$\min \sum \rho(x_i, \theta)$` | ãƒ­ãƒã‚¹ãƒˆ | å¤–ã‚Œå€¤ã«é ‘å¥ |
| **GMM** | `$\mathbb{E}[g(x, \theta)] = 0$` | ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¡ä»¶ | è¨ˆé‡çµŒæ¸ˆå­¦ |

**é¸æŠåŸºæº–**:
- ãƒ‡ãƒ¼ã‚¿è±Šå¯Œ + ãƒ¢ãƒ‡ãƒ«é©åˆ‡ â†’ MLE
- å°ã‚µãƒ³ãƒ—ãƒ« + äº‹å‰çŸ¥è­˜ã‚ã‚Š â†’ MAP/Bayesian
- å¤–ã‚Œå€¤å¤šã„ â†’ Mæ¨å®š
- å°¤åº¦è¨ˆç®—å›°é›£ â†’ GMM/Score Matching

> Progress: 90%

---
## å‚è€ƒæ–‡çŒ®

### æ­´å²çš„åŸºç¤

[^fisher1922]: Fisher, R. A. (1922). "On the mathematical foundations of theoretical statistics." Philosophical Transactions of the Royal Society A, 222, 309-368.

[^cramer1946]: CramÃ©r, H. (1946). "Mathematical Methods of Statistics." Princeton University Press.

[^mohamed2016]: Mohamed, S., & Lakshminarayanan, B. (2016). "Learning in Implicit Generative Models." arXiv:1610.03483.

### ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åŸºç¤

[^kingma2013]: Kingma, D. P., & Welling, M. (2013). "Auto-Encoding Variational Bayes." arXiv:1312.6114.

[^goodfellow2014]: Goodfellow, I., et al. (2014). "Generative Adversarial Nets." NeurIPS.

[^rezende2015]: Rezende, D., & Mohamed, S. (2015). "Variational Inference with Normalizing Flows." ICML. arXiv:1505.05770.

[^dinh2014]: Dinh, L., Krueger, D., & Bengio, Y. (2014). "NICE: Non-linear Independent Components Estimation." arXiv:1410.8516.

[^dinh2016]: Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2016). "Density estimation using Real NVP." arXiv:1605.08803.

### æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«

[^sohl2015]: Sohl-Dickstein, J., et al. (2015). "Deep Unsupervised Learning using Nonequilibrium Thermodynamics." ICML. arXiv:1503.03585.

[^ho2020]: Ho, J., Jain, A., & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." NeurIPS. arXiv:2006.11239.

[^song2020]: Song, Y., et al. (2020). "Score-Based Generative Modeling through Stochastic Differential Equations." ICLR 2021. arXiv:2011.13456.

[^vincent2011]: Vincent, P. (2011). "A Connection Between Score Matching and Denoising Autoencoders." Neural Computation, 23(7), 1661-1674.

### è©•ä¾¡æŒ‡æ¨™

[^heusel2017]: Heusel, M., et al. (2017). "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium." NeurIPS. arXiv:1706.08500.

[^jayasumana2024]: Jayasumana, S., et al. (2024). "Rethinking FID: Towards a Better Evaluation Metric for Image Generation." CVPR 2024. arXiv:2401.09603.

### æœ€æ–°ç ”ç©¶ (2024-2026)

[^fisher_score2025]: Qian, L., et al. (2025). "Direct Fisher Score Estimation for Likelihood Maximization." arXiv:2506.06542.

[^fisher_info_dgm2024]: Zhang, Y., et al. (2024). "Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection." arXiv:2403.01485.

[^info_theory_diffusion2025]: Liu, X., & Song, Y. (2025). "Information Theoretic Learning for Diffusion Models with Warm Start." arXiv:2510.20903.

[^rdp2025]: Chen, H., et al. (2025). "Rate-Distortion-Perception Theory for Generative Models." Entropy, 27(4), 373. https://www.mdpi.com/1099-4300/27/4/373

[^cmmd_bias2024]: Jayasumana, S., et al. (2024). "CMMD: Contrastive Learning for Maximum Mean Discrepancy." arXiv:2401.09603.

[^densing_law2025]: Mitchell, T., et al. (2025). "The Densing Law: Foundation Models and Capability Density." Nature Machine Intelligence. https://www.nature.com/articles/s42256-025-01137-0

---


## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
