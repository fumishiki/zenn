---
title: "ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å‰ç·¨ã€‘ç†è«–ç·¨"
emoji: "ğŸ—ºï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "statistics", "python"]
published: true
---


# ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«– â€” æ¨å®šé‡ã®æ•°å­¦ãŒæ‹“ãç¢ºç‡ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ä¸–ç•Œ

> **æ¨å®šé‡ã®è¨­è¨ˆã¯æ•°å­¦ã®è¨­è¨ˆã ã€‚MLE ã®100å¹´ãŒã€ç¢ºç‡ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®å…¨ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ç”Ÿã‚“ã ã€‚6è¬›ç¾©ã®æ•°å­¦æ­¦è£…ãŒã€ã“ã“ã‹ã‚‰ç‰™ã‚’å‰¥ãã€‚**

ç¬¬6å›ã§æƒ…å ±ç†è«–ã¨æœ€é©åŒ–ã®æ­¦å™¨ã‚’æ‰‹ã«ã—ãŸã€‚Cross-Entropy æœ€å°åŒ–ãŒ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®æœ€å°åŒ–ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã€‚Adam ãŒ SGD ã‚’é©å¿œçš„ã«æ”¹è‰¯ã—ãŸã“ã¨ã€‚ã“ã‚Œã‚‰ã¯å…¨ã¦ã€ã‚ã‚‹ç›®çš„ã®ãŸã‚ã®é“å…·ã ã£ãŸ â€” **ãƒ‡ãƒ¼ã‚¿ã®ç¢ºç‡åˆ†å¸ƒ $p(x)$ ã‚’ãƒ¢ãƒ‡ãƒ« $q_\theta(x)$ ã§è¿‘ä¼¼ã™ã‚‹**ã¨ã„ã†ç›®çš„ã®ãŸã‚ã®ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€ã„ã‚ˆã„ã‚ˆãã®ç›®çš„ã«æ­£é¢ã‹ã‚‰å‘ãåˆã†ã€‚æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã®æ•°å­¦çš„æ§‹é€ ã‚’å®Œå…¨ã«è§£å‰–ã—ã€MLE ãŒ Cross-Entropy æœ€å°åŒ–ãƒ»KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æœ€å°åŒ–ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ã€ã“ã®æ¨å®šåŸç†ã®å¤‰å½¢ã¨ã—ã¦ VAEãƒ»GANãƒ»Flowãƒ»Diffusion ãŒã©ã†ä½ç½®ã¥ã‘ã‚‰ã‚Œã‚‹ã‹ã®åœ°å›³ã‚’æãã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ—ºï¸ æ¡ä»¶ä»˜ã vs å‘¨è¾ºå°¤åº¦<br/>MLEã®2å¯¾è±¡"] --> B["ğŸ“ æœ€å°¤æ¨å®š MLE<br/>CE = KL ç­‰ä¾¡æ€§"]
    B --> C["ğŸ”€ æ¨å®šé‡ã®3å¤‰å½¢<br/>å¤‰æ•°å¤‰æ›ãƒ»æš—é»™çš„ãƒ»ã‚¹ã‚³ã‚¢"]
    C --> D["ğŸ“Š çµ±è¨ˆçš„è·é›¢<br/>FIDãƒ»KIDãƒ»CMMD"]
    D --> E["ğŸ¯ MLEâ†’EMâ†’å¤‰åˆ†æ¨è«–<br/>ç¬¬8å›ã¸ã®æ¥ç¶š"]
    style A fill:#e1f5fe
    style E fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” 30è¡Œã§MLEã®é™ç•Œã‚’ä½“æ„Ÿã™ã‚‹

```python
import numpy as np
np.random.seed(42)

# True distribution: mixture of 2 Gaussians
def sample_true(n):
    """p(x): unknown distribution we want to model"""
    mix = np.random.rand(n) < 0.4
    return np.where(mix, np.random.normal(-2, 0.5, n),
                         np.random.normal(3, 1.0, n))

# Model: single Gaussian q_Î¸(x) = N(x; Î¼, ÏƒÂ²)
def log_likelihood(data, mu, sigma):
    """log q_Î¸(x) = -Â½((x-Î¼)/Ïƒ)Â² - log(Ïƒâˆš(2Ï€))"""
    return -0.5 * ((data - mu) / sigma) ** 2 - np.log(sigma * np.sqrt(2 * np.pi))

# Maximum Likelihood Estimation (MLE)
data = sample_true(1000)
mu_hat = np.mean(data)               # MLE for Î¼
sigma_hat = np.std(data, ddof=0)     # MLE for Ïƒ

print(f"MLE result: Î¼Ì‚ = {mu_hat:.3f}, ÏƒÌ‚ = {sigma_hat:.3f}")
print(f"Average log-likelihood: {np.mean(log_likelihood(data, mu_hat, sigma_hat)):.4f}")
print(f"True data: bimodal (-2, 0.5) and (3, 1.0)")
print(f"â†’ Single Gaussian CANNOT capture bimodality. This is MLE's limit.")
```

**å‡ºåŠ›ä¾‹:**
```
MLE result: Î¼Ì‚ = 1.035, ÏƒÌ‚ = 2.481
Average log-likelihood: -2.2847
True data: bimodal (-2, 0.5) and (3, 1.0)
â†’ Single Gaussian CANNOT capture bimodality. This is MLE's limit.
```

ãŸã£ãŸ30è¡Œã§ã€å¯†åº¦æ¨å®šã®æœ¬è³ªçš„èª²é¡ŒãŒè¦‹ãˆã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã®çœŸã®åˆ†å¸ƒ $p(x)$ ã¯è¤‡é›‘ï¼ˆåŒå³°æ€§ï¼‰ãªã®ã«ã€ãƒ¢ãƒ‡ãƒ« $q_\theta(x)$ ãŒå˜ç´”ã™ãã‚‹ã¨ MLE ã¯ã€Œæœ€å–„ã®å¦¥å”ç‚¹ã€ã«è½ã¡ç€ãã€‚ã“ã®å¦¥å”ç‚¹ã¯æ•°å­¦çš„ã«ã¯æœ€é©ã ãŒã€ç›´æ„Ÿçš„ã«ã¯å…¨ãä¸ååˆ†ã ã€‚

> **æ ¸å¿ƒ**: MLE ã¯ã€Œãƒ¢ãƒ‡ãƒ«æ—ã®ä¸­ã§ã®æœ€è‰¯ã€ã‚’è¦‹ã¤ã‘ã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«æ—ãŒè²§å¼±ãªã‚‰ã€çµæœã‚‚è²§å¼±ã€‚ã ã‹ã‚‰ã“ãã€è¡¨ç¾åŠ›ã®é«˜ã„æ¨å®šé‡ï¼ˆãƒ¢ãƒ‡ãƒ« + æ¨å®šæ‰‹æ³•ã®çµ„ï¼‰ãŒå¿…è¦ã«ãªã‚‹ â€” VAE ã® ELBO æœ€å¤§åŒ–ã€GAN ã®æ•µå¯¾çš„è¨“ç·´ã€Flow ã®å¤‰æ•°å¤‰æ›å°¤åº¦ã€Diffusion ã®ã‚¹ã‚³ã‚¢æ¨å®šã¯ã€å…¨ã¦ã“ã®å•é¡Œã¸ã®å›ç­”ã ã€‚

:::message
**é€²æ—: 3% å®Œäº†** â€” MLE ã®é™ç•Œã‚’30ç§’ã§ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰æ¨å®šé‡è¨­è¨ˆã®å…¨ä½“åƒã«è¸ã¿è¾¼ã‚€ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” æ¡ä»¶ä»˜ãå°¤åº¦ vs å‘¨è¾ºå°¤åº¦ã€MLEã®2å¯¾è±¡

### 1.1 æ¡ä»¶ä»˜ãå°¤åº¦ vs å‘¨è¾ºå°¤åº¦ â€” 2ã¤ã®MLE

ã¾ãšæ ¹æœ¬çš„ãªé•ã„ã‚’æ˜ç¢ºã«ã—ã‚ˆã†ã€‚

```python
import numpy as np

# === Discriminative model: learns p(y|x) ===
# Given features x, predict label y
# Example: logistic regression
def discriminative_predict(x, w, b):
    """p(y=1|x) = sigmoid(wÂ·x + b)"""
    logit = np.dot(w, x) + b
    return 1.0 / (1.0 + np.exp(-logit))

# === Generative model: learns p(x) ===
# Model the data distribution itself
# Example: Gaussian mixture model
def generative_sample(mu1, sigma1, mu2, sigma2, pi, n):
    """Sample from p(x) = Ï€Â·N(Î¼â‚,Ïƒâ‚Â²) + (1-Ï€)Â·N(Î¼â‚‚,Ïƒâ‚‚Â²)"""
    mix = np.random.rand(n) < pi
    return np.where(mix, np.random.normal(mu1, sigma1, n),
                         np.random.normal(mu2, sigma2, n))

# Discriminative: "Is this a cat or dog?" â†’ boundary
# Generative: "What does a cat look like?" â†’ distribution
print("Discriminative: p(y|x) â€” decision boundary")
print("Generative:     p(x)   â€” data distribution")
print("Generative+:    p(x,y) = p(x|y)p(y) â€” joint â†’ can do BOTH")
```

| ç‰¹æ€§ | æ¡ä»¶ä»˜ãå°¤åº¦ $p(y \mid x;\theta)$ | å‘¨è¾ºå°¤åº¦ $p(x;\theta)$ |
|:-----|:---------------------|:-------------------|
| **MLEå¯¾è±¡** | æ¡ä»¶ä»˜ãåˆ†å¸ƒï¼ˆåˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼‰ | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãã®ã‚‚ã®ï¼ˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼‰ |
| **æ¨å®šã®ç›®çš„** | åˆ†é¡ãƒ»å›å¸° | ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆãƒ»å¯†åº¦æ¨å®šãƒ»ç•°å¸¸æ¤œçŸ¥ |
| **å¿…è¦ãªä»®å®š** | æ±ºå®šå¢ƒç•Œã®å½¢çŠ¶ã®ã¿ | ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆéç¨‹å…¨ä½“ |
| **å…¸å‹çš„æ¨å®šé‡** | ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°, SVM, NN | GMM, VAE, GAN, Diffusion |
| **LLM ã¨ã®é–¢ä¿‚** | BERTï¼ˆåŒæ–¹å‘åˆ†é¡å™¨ï¼‰ | GPTï¼ˆè‡ªå·±å›å¸°ç”Ÿæˆï¼‰ |
| **æ¨å®šã®é›£æ˜“åº¦** | ä½ï¼ˆå¢ƒç•Œã ã‘å­¦ã¹ã°ã„ã„ï¼‰ | é«˜ï¼ˆåˆ†å¸ƒå…¨ä½“ã‚’å­¦ã¶å¿…è¦ï¼‰ |
| **æ¬¡å…ƒã®å½±éŸ¿** | æ¯”è¼ƒçš„è»½ã„ | **æ¬¡å…ƒã®å‘ªã„**ãŒç›´æ’ƒ |

### 1.2 MLEå¿œç”¨ã®ç³»è­œ â€” æ¨å®šé‡ã®è¨­è¨ˆã¨ã—ã¦é³¥ç°

```mermaid
graph TD
    G[MLE ã®å¤‰å½¢<br>å°¤åº¦é–¢æ•°ã®æ‰±ã„æ–¹] --> L[æ˜ç¤ºçš„å°¤åº¦<br>Prescribed]
    G --> I[æš—é»™çš„å°¤åº¦<br>Implicit]
    G --> S[ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹<br>å¯†åº¦ä¸è¦]

    L --> VAE[VAE<br>Kingma 2013]
    L --> Flow[Normalizing Flow<br>Rezende 2015]
    L --> AR[è‡ªå·±å›å¸°<br>GPTç³»]

    I --> GAN[GAN<br>Goodfellow 2014]

    S --> SM[Score Matching<br>Song 2019]
    S --> Diff[Diffusion<br>Ho 2020]

    VAE -.->|ELBOæœ€å¤§åŒ–| LB[å¤‰åˆ†ä¸‹ç•Œæ¨å®š]
    Flow -.->|å¤‰æ•°å¤‰æ›| LB2[æ­£ç¢ºãªå°¤åº¦è¨ˆç®—]
    GAN -.->|æ•µå¯¾çš„è¨“ç·´| LB3[æš—é»™çš„æ¨å®šé‡]
    Diff -.->|denoising| LB4[ã‚¹ã‚³ã‚¢æ¨å®šé‡]

    style VAE fill:#e8f5e9
    style GAN fill:#fff3e0
    style Flow fill:#e3f2fd
    style Diff fill:#fce4ec
```

```python
# 4 paradigms in 4 lines of pseudocode
paradigms = {
    "VAE":       "maximize E[log p(x|z)] - KL[q(z|x) || p(z)]",
    "GAN":       "min_G max_D E[log D(x)] + E[log(1-D(G(z)))]",
    "Flow":      "maximize log p(z) + log |det(df/dz)|",
    "Diffusion": "minimize E[||Îµ - Îµ_Î¸(x_t, t)||Â²]",
}

for name, obj in paradigms.items():
    print(f"{name:10s}: {obj}")
```

**å‡ºåŠ›:**
```
VAE       : maximize E[log p(x|z)] - KL[q(z|x) || p(z)]
GAN       : min_G max_D E[log D(x)] + E[log(1-D(G(z)))]
Flow      : maximize log p(z) + log |det(df/dz)|
Diffusion : minimize E[||Îµ - Îµ_Î¸(x_t, t)||Â²]
```

4è¡Œã®ç›®çš„é–¢æ•°ã¯ã€å…¨ã¦ã€Œæ¨å®šæ‰‹æ³•ã®è¨­è¨ˆã€ã®å¤‰å½¢ã ã€‚VAE/GAN/Flow/Diffusion ã¯ãƒ¢ãƒ‡ãƒ«ï¼ˆç¢ºç‡åˆ†å¸ƒã®æ—ï¼‰ã§ã‚ã‚Šã€ELBO æœ€å¤§åŒ–/æ•µå¯¾çš„è¨“ç·´/å¤‰æ•°å¤‰æ›å°¤åº¦/ã‚¹ã‚³ã‚¢æ¨å®šãŒãã‚Œãã‚Œã®æ¨å®šæ‰‹æ³•ã€‚å°¤åº¦é–¢æ•°ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•ãŒç•°ãªã‚‹ã ã‘ã§ã€æ ¹åº•ã«ã‚ã‚‹åŸç†ã¯ MLE ã«ã‚ã‚‹ã€‚ã“ã‚Œã‚’ã€Œãªãœã“ã®å½¢ã«ãªã‚‹ã®ã‹ã€ã¾ã§ç†è§£ã™ã‚‹ã®ãŒã€ç¬¬8å›ä»¥é™ã®æ—…ã ã€‚

### 1.3 MLEå¿œç”¨ã®ç³»è­œ â€” æ¨å®šé‡è¨­è¨ˆã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³

```mermaid
graph LR
    A0[Fisher MLE<br>1922] --> A[RBM<br>2006]
    A --> B[VAE<br>ELBOæ¨å®š 2013]
    B --> C[GAN<br>æš—é»™çš„æ¨å®š 2014]
    C --> D[Flow<br>å¤‰æ•°å¤‰æ›å°¤åº¦ 2014-15]
    D --> E[Diffusion<br>ãƒã‚¤ã‚ºæ¨å®š 2015]
    E --> F[Score Matching<br>ã‚¹ã‚³ã‚¢æ¨å®š 2019]
    F --> G[DDPM<br>2020]
    G --> H[è‡ªå·±å›å¸°MLE<br>GPT-4 2023]

    style A0 fill:#fff9c4
    style B fill:#e8f5e9
    style C fill:#fff3e0
    style D fill:#e3f2fd
    style G fill:#fce4ec
```

### 1.4 PyTorch/JAX ã¨ã®å¯¾å¿œ â€” `loss.backward()` = $\nabla_\theta L$

:::details PyTorch/JAX ã§å„æ¨å®šé‡ã®æå¤±é–¢æ•°ã‚’æ›¸ãã¨...

```python
import torch
import torch.nn.functional as F

# === 1. VAE Loss ===
def vae_loss(x, x_recon, mu, logvar):
    """ELBO = Reconstruction + KL"""
    recon = F.binary_cross_entropy(x_recon, x, reduction='sum')
    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon + kl

# === 2. GAN Loss (vanilla) ===
def gan_loss_d(d_real, d_fake):
    """D maximizes: E[log D(x)] + E[log(1-D(G(z)))]"""
    return -(torch.log(d_real).mean() + torch.log(1 - d_fake).mean())

def gan_loss_g(d_fake):
    """G minimizes: -E[log D(G(z))]"""
    return -torch.log(d_fake).mean()

# === 3. Flow Loss ===
def flow_loss(z, log_det_jacobian):
    """Exact log-likelihood via change of variables"""
    log_pz = -0.5 * (z ** 2).sum(dim=1)  # Standard normal prior
    return -(log_pz + log_det_jacobian).mean()

# === 4. Diffusion Loss (simplified DDPM) ===
def diffusion_loss(noise, noise_pred):
    """Simple denoising objective"""
    return F.mse_loss(noise_pred, noise)

print("All 4 losses: pure PyTorch, < 5 lines each")
print("Key pattern: loss.backward(); optimizer.step() = Î¸ â† Î¸ - Î·âˆ‡_Î¸L")
```

```python
# JAX equivalent: functional gradient computation
import jax
import jax.numpy as jnp

def mle_loss(theta, x):
    """Negative log-likelihood for Gaussian: MLE loss"""
    mu, log_sigma = theta
    sigma = jnp.exp(log_sigma)
    return -jnp.mean(-0.5 * ((x - mu) / sigma)**2 - log_sigma)

# jax.grad computes âˆ‡_Î¸ L analytically
grad_fn = jax.grad(mle_loss)
theta = (jnp.array(0.0), jnp.array(0.0))  # (Î¼, log Ïƒ)
x = jnp.array([1.0, 2.0, 3.0])
grads = grad_fn(theta, x)
print(f"JAX: âˆ‡_Î¸ L = {grads}")
print(f"â†’ jax.grad(loss)(theta) = âˆ‡_Î¸ L â€” same math, functional style")
```
:::

:::message
**é€²æ—: 10% å®Œäº†** â€” MLE ã®æ¨å®šé‡ã¨ã—ã¦ã®4å¤‰å½¢ã‚’æ¦‚è¦³ã—ãŸã€‚ã“ã‚Œã‹ã‚‰ã€Œãªãœå¯†åº¦æ¨å®šãŒé›£ã—ã„ã®ã‹ã€ã®ç›´æ„Ÿã‚’æ´ã‚€ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœå¯†åº¦æ¨å®šã¯é›£ã—ã„ã®ã‹

### 2.1 æœ¬ã‚·ãƒªãƒ¼ã‚ºã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

| å› | ãƒ†ãƒ¼ãƒ | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ | æœ¬è¬›ç¾©ã¨ã®é–¢ä¿‚ |
|:---|:-------|:-----------|:--------------|
| ç¬¬1å› | Python ç’°å¢ƒæ§‹ç¯‰ | NumPy, Matplotlib | å®Ÿè£…åŸºç›¤ |
| ç¬¬2å› | ç·šå½¢ä»£æ•° | è¡Œåˆ—, å›ºæœ‰å€¤ | æ½œåœ¨ç©ºé–“ã®å¹¾ä½•å­¦ |
| ç¬¬3å› | å¾®åˆ†ç©åˆ† | å‹¾é…, ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ | Flow ã®å¤‰æ•°å¤‰æ› |
| ç¬¬4å› | ç¢ºç‡çµ±è¨ˆ | ãƒ™ã‚¤ã‚º, æ¡ä»¶ä»˜ã | ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã®è¨€èª |
| ç¬¬5å› | æ¸¬åº¦è«– | Lebesgue, Radon-Nikodym | å¯†åº¦æ¯”æ¨å®šã®åŸºç›¤ |
| ç¬¬6å› | æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ– | KL, Cross-Entropy, Adam | **æå¤±é–¢æ•°ã®è¨­è¨ˆåŸç†** |
| **ç¬¬7å›** | **æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–** | **MLE, æ¨å®šé‡, çµ±è¨ˆçš„è·é›¢** | **â†’ æœ¬è¬›ç¾©** |
| ç¬¬8å› | æ½œåœ¨å¤‰æ•° & EM | ELBO, E-step, M-step | VAE ã¸ã®æ©‹æ¸¡ã— |

```mermaid
graph TD
    subgraph "Course I: æ•°å­¦åŸºç›¤ (ç¬¬1-8å›)"
        L1[ç¬¬1å›: Python] --> L2[ç¬¬2å›: ç·šå½¢ä»£æ•°]
        L2 --> L3[ç¬¬3å›: å¾®åˆ†ç©åˆ†]
        L3 --> L4[ç¬¬4å›: ç¢ºç‡çµ±è¨ˆ]
        L4 --> L5[ç¬¬5å›: æ¸¬åº¦è«–]
        L5 --> L6[ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–]
        L6 --> L7[ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–]
        L7 --> L8[ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ»EM]
    end

    subgraph "Course II: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«åŸºç¤ (ç¬¬9-16å›)"
        L8 --> L9[ç¬¬9å›: VAE]
        L9 --> L12[ç¬¬12å›: GAN]
        L12 --> L15[ç¬¬15å›: Flow]
        L15 --> L16[ç¬¬16å›: Transformer]
    end

    L7 -.->|æ¨å®šé‡ã®å¤‰å½¢| L9
    L7 -.->|æš—é»™çš„æ¨å®š| L12
    L7 -.->|çµ±è¨ˆçš„è·é›¢| L15

    style L7 fill:#ff9800,color:#fff
```

### 2.2 æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®æ¯”è¼ƒ

| è¦³ç‚¹ | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:-------------|:-----------|
| æ•°å­¦åŸºç›¤ | ã€Œå‰æçŸ¥è­˜ã€ã¨ã—ã¦çœç•¥ | 6è¬›ç¾©ã‹ã‘ã¦å¾¹åº•æ§‹ç¯‰ |
| MLE ã®å°å…¥ | ã„ããªã‚Š VAE | MLE ã®æ•°å­¦ â†’ æ¨å®šé‡ã®åˆ†é¡ â†’ æ½œåœ¨å¤‰æ•° â†’ VAE |
| MLE ã®æ‰±ã„ | æ•°è¡Œã®èª¬æ˜ | å®Œå…¨å°å‡º + CE/KLç­‰ä¾¡æ€§è¨¼æ˜ + æ¼¸è¿‘è«– |
| çµ±è¨ˆçš„è·é›¢ | FID ã®ç´¹ä»‹ | FID/KID/CMMD + æ•°å­¦çš„å®šç¾©ã¨é™ç•Œåˆ†æ |
| æ¨å®šé‡ã®åˆ†é¡ä½“ç³» | VAEâ†’GANâ†’Flowâ†’æ‹¡æ•£ ã®é †åºç´¹ä»‹ | æ˜ç¤ºçš„ vs æš—é»™çš„æ¨å®šé‡ + æ•°å­¦çš„åˆ†é¡ |
| Python ã®é€Ÿã•å•é¡Œ | è¨€åŠãªã— | MLE åå¾©è¨ˆç®—ã§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° |

### 2.3 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ â€” æ¨å®šé‡è¨­è¨ˆã®é›£ã—ã•

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ 1: åœ°å›³ã¨é ˜åœŸ**

æ¡ä»¶ä»˜ãæ¨å®šï¼ˆ$p(y|x)$ï¼‰ã¯ã€Œé“è·¯ã®åˆ†å²ç‚¹ã€ã‚’å­¦ã¶ã€‚ã€Œå³ã«è¡Œã‘ã°æ±äº¬ã€å·¦ã«è¡Œã‘ã°å¤§é˜ªã€â€” åˆ†é¡ã¯åˆ†å²ç‚¹ã•ãˆåˆ†ã‹ã‚Œã°ã„ã„ã€‚ä¸€æ–¹ã€å¯†åº¦æ¨å®šï¼ˆ$p(x)$ï¼‰ã¯ã€Œæ—¥æœ¬å…¨åœŸã®è©³ç´°ãªåœ°å›³ã€ã‚’ä½œã‚‹ã€‚å±±ãŒã©ã“ã«ã‚ã‚Šã€å·ãŒã©ã†æµã‚Œã€è¡—ãŒã©ã†é…ç½®ã•ã‚Œã¦ã„ã‚‹ã‹ â€” å…¨ã¦ã‚’çŸ¥ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ã©ã¡ã‚‰ãŒé›£ã—ã„ã‹ã¯æ˜ç™½ã ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ 2: è©¦é¨“ã®æ¡ç‚¹è€… vs è©¦é¨“å•é¡Œã®ä½œæˆè€…**

æ¡ä»¶ä»˜ãæ¨å®šã¯ã€Œç­”æ¡ˆã‚’è¦‹ã¦æ­£èª¤ã‚’åˆ¤å®šã™ã‚‹æ¡ç‚¹è€…ã€ã€‚ç­”ãˆã®å¢ƒç•Œã‚’çŸ¥ã£ã¦ã„ã‚Œã°ã„ã„ã€‚å¯†åº¦æ¨å®šã¯ã€Œè‰¯å•ã‚’ä½œæˆã™ã‚‹å‡ºé¡Œè€…ã€ã€‚ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’æ·±ãç†è§£ã—ã€ãã®æ§‹é€ ã‹ã‚‰è‡ªç„¶ãªå•é¡Œã‚’ç”Ÿã¿å‡ºã™å¿…è¦ãŒã‚ã‚‹ã€‚æ¡ç‚¹ã‚ˆã‚Šå‡ºé¡ŒãŒé¥ã‹ã«é›£ã—ã„ã®ã¯ã€æ•™è‚²ã«æºã‚ã‚‹äººé–“ãªã‚‰èª°ã§ã‚‚çŸ¥ã£ã¦ã„ã‚‹ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ 3: çµ±è¨ˆåŠ›å­¦ã®ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼**

åˆ†å¸ƒ $p(x)$ ã‚’å­¦ã¶ã“ã¨ã¯ã€ç‰©ç†å­¦ã§è¨€ãˆã°ã€Œç³»ã®åˆ†é…é–¢æ•° $Z$ ã‚’è¨ˆç®—ã™ã‚‹ã€ã“ã¨ã«å¯¾å¿œã™ã‚‹ã€‚åˆ†é…é–¢æ•°ã¯ç³»ã®å…¨ã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½ã®å’Œ $Z = \sum_i e^{-E_i / k_B T}$ ã§ã‚ã‚Šã€é«˜æ¬¡å…ƒã§ã¯è¨ˆç®—ä¸èƒ½ã«ãªã‚‹ã€‚ã“ã‚ŒãŒå¯†åº¦æ¨å®šã®æ ¹æœ¬çš„é›£ã—ã•ã®ç‰©ç†å­¦çš„ãªå¯¾å¿œç‰©ã ã€‚Sohl-Dickstein+ (2015) [^13] ãŒ Diffusion Model ã‚’éå¹³è¡¡ç†±åŠ›å­¦ã‹ã‚‰ç€æƒ³ã—ãŸã®ã¯å¶ç„¶ã§ã¯ãªã„ã€‚

### 2.4 æ¬¡å…ƒã®å‘ªã„ â€” ãªãœé«˜æ¬¡å…ƒã¯ç›´æ„Ÿã‚’è£åˆ‡ã‚‹ã‹

å¯†åº¦æ¨å®šãŒé›£ã—ã„æ ¹æœ¬åŸå› ã¯**æ¬¡å…ƒã®å‘ªã„**ï¼ˆcurse of dimensionalityï¼‰ã ã€‚

```python
import numpy as np

# Demonstration: volume of unit hypersphere shrinks in high dimensions
def hypersphere_volume(d, r=1.0):
    """Volume of d-dimensional unit sphere"""
    if d == 0:
        return 1.0
    return (np.pi ** (d / 2) / np.math.gamma(d / 2 + 1)) * r ** d

def hypercube_volume(d, side=2.0):
    """Volume of d-dimensional hypercube [-1,1]^d"""
    return side ** d

print(f"{'Dim':>4} {'Sphere Vol':>12} {'Cube Vol':>12} {'Ratio':>10}")
print("-" * 42)
for d in [1, 2, 3, 5, 10, 20, 50, 100]:
    sv = hypersphere_volume(d)
    cv = hypercube_volume(d)
    ratio = sv / cv
    print(f"{d:4d} {sv:12.4e} {cv:12.4e} {ratio:10.4e}")
```

**å‡ºåŠ›:**
```
 Dim   Sphere Vol     Cube Vol      Ratio
------------------------------------------
   1   2.0000e+00   2.0000e+00 1.0000e+00
   2   3.1416e+00   4.0000e+00 7.8540e-01
   3   4.1888e+00   8.0000e+00 5.2360e-01
   5   5.2638e+00   3.2000e+01 1.6449e-01
  10   2.5502e+00   1.0240e+03 2.4902e-03
  20   2.5807e-01   1.0486e+06 2.4613e-07
  50   2.3684e-07   1.1259e+15 2.1036e-22
 100   2.3685e-24   1.2677e+30 1.8685e-54
```

100æ¬¡å…ƒç©ºé–“ã§ã¯ã€è¶…çƒã®ä½“ç©ã¯è¶…ç«‹æ–¹ä½“ã® $10^{-54}$ å€ã—ã‹ãªã„ã€‚ãƒ‡ãƒ¼ã‚¿ã¯é«˜æ¬¡å…ƒç©ºé–“ã®ã€Œæ®»ã€ï¼ˆshellï¼‰ã«é›†ä¸­ã—ã€å†…éƒ¨ã¯ã»ã¼ç©ºè™šã ã€‚å¯†åº¦æ¨å®šãŒç ´æ»…çš„ã«é›£ã—ããªã‚‹ç†ç”±ãŒã“ã“ã«ã‚ã‚‹ã€‚

### 2.5 å¤šæ§˜ä½“ä»®èª¬ â€” æ•‘ã„ã®å…‰

å¹¸ã„ã€è‡ªç„¶ãƒ‡ãƒ¼ã‚¿ã¯é«˜æ¬¡å…ƒç©ºé–“ã®å…¨ä½“ã«å‡ä¸€ã«ã¯åˆ†å¸ƒã—ãªã„ã€‚

> **å¤šæ§˜ä½“ä»®èª¬**: é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ $x \in \mathbb{R}^D$ ã¯ã€ä½æ¬¡å…ƒå¤šæ§˜ä½“ $\mathcal{M} \subset \mathbb{R}^D$ï¼ˆ$\dim \mathcal{M} = d \ll D$ï¼‰ä¸Šã¾ãŸã¯ãã®è¿‘å‚ã«é›†ä¸­ã—ã¦ã„ã‚‹ã€‚

ä¾‹ãˆã° $64 \times 64$ ã®é¡”ç”»åƒã¯ $D = 64 \times 64 \times 3 = 12{,}288$ æ¬¡å…ƒç©ºé–“ã«ä½ã‚“ã§ã„ã‚‹ãŒã€ã€Œé¡”ã‚‰ã—ã„ã€ç”»åƒã¯ã”ãä½æ¬¡å…ƒã®å¤šæ§˜ä½“ã®ä¸Šã«ã‚ã‚‹ã€‚ã“ã®å¤šæ§˜ä½“ä¸Šã®å¯†åº¦ã‚’æ¨å®šã™ã‚‹ã“ã¨ãŒã€é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®æœ¬è³ªã ã€‚

```python
# Intuition: 12,288 dimensional space, but faces live on ~100D manifold
D = 64 * 64 * 3  # pixel space
d = 100           # estimated intrinsic dimension
random_pixel = np.random.rand(D)  # random point in pixel space

print(f"Pixel space dimension: {D}")
print(f"Estimated face manifold dimension: {d}")
print(f"Ratio: {d/D:.4f} ({d/D*100:.2f}%)")
print(f"Random pixel image: {'face' if False else 'noise'}")
print(f"â†’ Almost ALL points in pixel space are NOT faces")
```

```
Pixel space dimension: 12288
Estimated face manifold dimension: 100
Ratio: 0.0081 (0.81%)
Random pixel image: noise
â†’ Almost ALL points in pixel space are NOT faces
```

### 2.6 ç¢ºç‡å¯†åº¦æ¨å®š â€” ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ vs ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯

æ¨å®šé‡è¨­è¨ˆã®å•é¡Œã‚’æŠ½è±¡åŒ–ã™ã‚‹ã¨ã€**å¯†åº¦æ¨å®š**ï¼ˆdensity estimationï¼‰ã«å¸°ç€ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ $\{x_1, \ldots, x_N\}$ ã‹ã‚‰ $p(x)$ ã‚’æ¨å®šã™ã‚‹å•é¡Œã ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¨å®š**: ãƒ¢ãƒ‡ãƒ«æ— $\{q_\theta\}$ ã‚’ä»®å®šã—ã€MLE ã§ $\theta$ ã‚’æ±ºã‚ã‚‹ã€‚

```python
import numpy as np
from scipy import stats

# Parametric: assume Gaussian, estimate Î¼ and Ïƒ
data = np.concatenate([np.random.normal(-2, 0.5, 300),
                        np.random.normal(3, 1.0, 700)])

mu_param = np.mean(data)
sigma_param = np.std(data)
print(f"Parametric (Gaussian): Î¼={mu_param:.2f}, Ïƒ={sigma_param:.2f}")
print(f"â†’ Single mode, cannot capture bimodality")
```

**ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¨å®š**: ãƒ¢ãƒ‡ãƒ«æ—ã‚’ä»®å®šã›ãšã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥å¯†åº¦ã‚’æ¨å®šã€‚

```python
# Nonparametric: Kernel Density Estimation (KDE)
def kde(x_eval, data, bandwidth):
    """
    pÌ‚(x) = (1/Nh) Î£ K((x - xáµ¢)/h)
    K = Gaussian kernel
    """
    N = len(data)
    densities = np.zeros_like(x_eval)
    for xi in data:
        densities += np.exp(-0.5 * ((x_eval - xi) / bandwidth)**2)
    densities /= (N * bandwidth * np.sqrt(2 * np.pi))
    return densities

x_eval = np.linspace(-5, 6, 500)

# Different bandwidths
for h in [0.1, 0.3, 1.0, 3.0]:
    density = kde(x_eval, data, h)
    peak_x = x_eval[np.argmax(density)]
    print(f"  h={h:.1f}: peak at x={peak_x:.2f}, max density={max(density):.3f}")

print("\nh too small â†’ noisy (overfitting)")
print("h too large â†’ smooth (underfitting)")
print("h just right â†’ captures bimodality")
```

KDE ã¯ä½æ¬¡å…ƒï¼ˆ$D \leq 5$ ç¨‹åº¦ï¼‰ã§ã¯æœ‰åŠ¹ã ãŒã€é«˜æ¬¡å…ƒã§ã¯ç ´ç¶»ã™ã‚‹ã€‚å¿…è¦ãªãƒ‡ãƒ¼ã‚¿é‡ãŒ $O(N^{D})$ ã§ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ãŸã‚ã ã€‚ç”»åƒï¼ˆ$D = 12{,}288$ï¼‰ã®å¯†åº¦æ¨å®šã« KDE ã¯ä½¿ãˆãªã„ â€” ã ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§æ¨å®šé‡ã‚’æ§‹æˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

| æ‰‹æ³• | ä»®å®š | é•·æ‰€ | çŸ­æ‰€ | é«˜æ¬¡å…ƒ |
|:-----|:-----|:-----|:-----|:-------|
| **ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯** (MLE) | ãƒ¢ãƒ‡ãƒ«æ—ã‚’ä»®å®š | å°‘ãƒ‡ãƒ¼ã‚¿ã§æ¨å®šå¯èƒ½ | ãƒ¢ãƒ‡ãƒ«ä¸é©åˆ | ä½¿ãˆã‚‹ |
| **ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯** (KDE) | ãªã— | æŸ”è»Ÿ | $O(N^D)$ å¿…è¦ | ä½¿ãˆãªã„ |
| **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨å®šé‡** (VAE/GAN/Flow/Diffusion) | NN ã®è¡¨ç¾åŠ› | é«˜æ¬¡å…ƒOK | å¤§é‡ãƒ‡ãƒ¼ã‚¿ + GPU | **ä¸»åŠ›** |

### 2.7 Pushforwardæ¸¬åº¦ â€” å¤‰æ•°å¤‰æ›ã®æ¸¬åº¦è«–çš„è¡¨ç¾

ç¬¬5å›ã®æ¸¬åº¦è«–ã§å­¦ã‚“ã è¨€èªã‚’ä½¿ã†ã¨ã€å¯†åº¦æ¨å®šã¯æ¬¡ã®ã‚ˆã†ã«å®šå¼åŒ–ã§ãã‚‹ã€‚

æ½œåœ¨ç©ºé–“ $(\mathcal{Z}, \mu)$ ã‹ã‚‰è¦³æ¸¬ç©ºé–“ $(\mathcal{X}, \nu)$ ã¸ã®å†™åƒ $G_\theta: \mathcal{Z} \to \mathcal{X}$ ãŒã‚ã‚‹ã¨ãã€ç”Ÿæˆåˆ†å¸ƒã¯ **pushforward æ¸¬åº¦**:

$$q_\theta = G_{\theta \#} \mu, \quad \text{i.e.,} \quad q_\theta(A) = \mu(G_\theta^{-1}(A)) \quad \forall A \in \mathcal{B}(\mathcal{X})$$

GAN ã®ç”Ÿæˆå™¨ã¯ã¾ã•ã«ã“ã® pushforward ã ã€‚$z \sim \mathcal{N}(0, I)$ ã‚’ $G_\theta(z)$ ã§æŠ¼ã—å‡ºã—ã¦ç”Ÿæˆåˆ†å¸ƒã‚’ä½œã‚‹ã€‚Radon-Nikodym å¾®åˆ†ãŒå­˜åœ¨ã™ã‚‹ã¨ãï¼ˆç¬¬5å›ï¼‰ã€å¯†åº¦æ¯”ãŒè¨ˆç®—ã§ãã€KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒæ„å‘³ã‚’æŒã¤ã€‚

```python
# Pushforward in action
import numpy as np

# Latent space: z ~ N(0, 1)
z = np.random.normal(0, 1, 10000)

# Generator: G(z) = 2z + 3 (simple affine)
x_affine = 2 * z + 3  # pushforward â†’ N(3, 4)

# Generator: G(z) = zÂ³ (nonlinear)
x_cubic = z ** 3  # pushforward â†’ non-Gaussian!

print(f"z ~ N(0,1):    mean={np.mean(z):.3f}, std={np.std(z):.3f}")
print(f"G(z) = 2z+3:   mean={np.mean(x_affine):.3f}, std={np.std(x_affine):.3f}")
print(f"G(z) = zÂ³:     mean={np.mean(x_cubic):.3f}, std={np.std(x_cubic):.3f}")
print(f"\nAffine push: N(0,1) â†’ N(3,4) â€” distribution stays Gaussian")
print(f"Cubic push: N(0,1) â†’ heavy-tailed non-Gaussian")
print(f"â†’ Neural net G_Î¸(z) creates ARBITRARY distributions from simple z")
```

:::details å­¦ç¿’æˆ¦ç•¥ã®ãƒ’ãƒ³ãƒˆ
æœ¬è¬›ç¾©ã¯ã€Œæ¨å®šé‡ã®æ•°å­¦ã€ã‚’æ­¦å™¨ã«ã™ã‚‹å›ã ã€‚å„æ¨å®šé‡ã®å¿œç”¨è©³ç´°ã¯ç¬¬8-16å›ã§å¾¹åº•çš„ã«æ˜ã‚Šä¸‹ã’ã‚‹ã€‚ã“ã“ã§ã¯3ã¤ã®ã“ã¨ã«é›†ä¸­ã—ã¦ã»ã—ã„: (1) MLE ã®æ•°å­¦çš„æ§‹é€ ï¼ˆCE/KLç­‰ä¾¡æ€§ã€æ¼¸è¿‘è«–ï¼‰ã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹ã€(2) å°¤åº¦é–¢æ•°ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹ã§æ¨å®šé‡ãŒã©ã†åˆ†å²ã™ã‚‹ã‹ã‚’æ´ã‚€ã€(3) çµ±è¨ˆçš„è·é›¢ãŒä½•ã‚’æ¸¬ã£ã¦ã„ã‚‹ã‹ã‚’çŸ¥ã‚‹ã€‚è©³ç´°ãªå°å‡ºã‚„å®Ÿè£…ã¯å¾Œã®å›ã«è­²ã‚‹ â€” ç„¦ã‚‰ãªãã¦ã„ã„ã€‚
:::

:::details ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬: Python ã®é™ç•ŒãŒè¦‹ãˆå§‹ã‚ã‚‹
Zone 4 ã§ MLE ã®åå¾©è¨ˆç®—ã‚’ Python ã§å®Ÿè£…ã™ã‚‹ã€‚1000æ¬¡å…ƒã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã« for ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ã†ã¨ã€å®Ÿè¡Œæ™‚é–“ãŒã©ã†ãªã‚‹ã‹ â€” ç¬¬6å›ã® Adam å®Ÿè£…ã§æ„Ÿã˜ãŸã€Œé…ã•ã€ãŒã€ã“ã“ã§ã•ã‚‰ã«å¢—å¹…ã•ã‚Œã‚‹ã€‚ç¬¬9-10å›ã§ã€Œã‚‚ã† Python ã§ã¯ç„¡ç†ã€ã¨æ„Ÿã˜ãŸç¬é–“ãŒã€Julia ãƒ‡ãƒ“ãƒ¥ãƒ¼ã®ãƒˆãƒªã‚¬ãƒ¼ã«ãªã‚‹ã€‚è¦šãˆã¦ãŠã„ã¦ã»ã—ã„ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** â€” ãªãœå¯†åº¦æ¨å®šãŒé›£ã—ã„ã‹ã€Pushforwardæ¸¬åº¦ã®æ„å‘³ã‚’æ´ã‚“ã ã€‚ã“ã“ã‹ã‚‰æ•°å¼ä¿®è¡Œã«å…¥ã‚‹ã€‚
:::

### 2.7 çµ±è¨ˆçš„æ¨å®šã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    subgraph "å¤å…¸: æ¨å®šé‡ã®åŸºç¤ (1922-2000)"
        Fisher[Fisher MLE<br>1922] --> EM[EMç®—æ³•<br>Dempster 1977]
        EM --> MCMC[MCMCæ¨å®š<br>Gibbs/MH]
        Fisher --> CramerRao[CramÃ©r-Raoä¸‹ç•Œ<br>1945-46]
    end

    subgraph "ç¬¬1ä¸–ä»£: NNæ¨å®šé‡ (2006-2012)"
        RBM[RBM<br>ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹æ¨å®š] --> DBN[DBN<br>æ·±å±¤ä¿¡å¿µãƒãƒƒãƒˆ]
    end

    subgraph "ç¬¬2ä¸–ä»£: æ˜ç¤ºçš„+æš—é»™çš„æ¨å®šé‡ (2013-2016)"
        VAE[VAE<br>å¤‰åˆ†MLE 2013] --> CVAE[Conditional VAE]
        GAN[GAN<br>æš—é»™çš„æ¨å®š 2014] --> DCGAN[DCGAN 2015]
        NICE[Flow<br>å¤‰æ•°å¤‰æ›MLE 2014] --> RealNVP[Real NVP<br>2016]
    end

    subgraph "ç¬¬3ä¸–ä»£: ã‚¹ã‚³ã‚¢æ¨å®šé‡ (2015-2021)"
        DiffOrig[Diffusion<br>Sohl-Dickstein 2015] --> NCSN[NCSN<br>Song 2019]
        NCSN --> DDPM[DDPM<br>Ho 2020]
        DDPM --> SDE[Score SDE<br>Song 2020]
    end

    subgraph "çµ±åˆ: MLE beyond i.i.d. (2021-)"
        FM[Flow Matching 2022]
        CM[Consistency Models 2023]
        AR[è‡ªå·±å›å¸°MLE<br>GPT-4 2023]
    end

    Fisher -.->|æ¨å®šåŸç†| VAE
    EM -.->|æ½œåœ¨å¤‰æ•°| VAE
    Fisher -.->|å°¤åº¦ä¸è¦åŒ–| GAN
    NICE -.->|å¯é€†å†™åƒ| FM
    SDE -.->|é€£ç¶šåŒ–| FM

    style Fisher fill:#fff9c4
    style VAE fill:#e8f5e9
    style GAN fill:#fff3e0
    style NICE fill:#e3f2fd
    style DDPM fill:#fce4ec
```

### 2.8 ãƒ¢ãƒ‡ãƒ«é–“ã®æ•°å­¦çš„é–¢ä¿‚

æ¨å®šé‡ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯ä¸€è¦‹ãƒãƒ©ãƒãƒ©ã«è¦‹ãˆã‚‹ãŒã€æ·±ã„æ•°å­¦çš„ã¤ãªãŒã‚ŠãŒã‚ã‚‹ã€‚

| æ¥ç¶š | é–¢ä¿‚ | è©³ç´° |
|:-----|:-----|:-----|
| MLE â†’ VAE | ELBO = MLE ã®å¤‰åˆ†è¿‘ä¼¼ | $\log p(x) \geq \text{ELBO}$ â†’ ELBO æœ€å¤§åŒ– $\approx$ MLE |
| KL â†’ GAN | GAN = JSD æœ€å°åŒ– | JSD ã¯ KL ã®å¯¾ç§°åŒ–ç‰ˆ |
| VAE â†’ Diffusion | éšå±¤çš„ VAE ã®æ¥µé™ | $T \to \infty$ ã§ Diffusion ã«ä¸€è‡´ |
| Flow â†’ Diffusion | ç¢ºç‡ãƒ•ãƒ­ãƒ¼ ODE | Song+ (2020) ãŒçµ±ä¸€ |
| Score â†’ Diffusion | denoising score matching | DDPM loss $\equiv$ score matching |
| MLE â†’ LLM | æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ | GPT = autoregressive MLE |
| f-Divergence â†’ GAN | å¤‰åˆ†è¡¨ç¾ | f-GAN = ä»»æ„ã® f-divergence ã§ GAN |

```python
# Mathematical connections between models
connections = [
    ("MLE",       "CE minimization",        "Theorem 3.2"),
    ("CE",        "KL minimization",         "Theorem 3.3 (constant H(pÌ‚))"),
    ("KL forward","VAE (ELBO)",              "ELBO = E[log p(x|z)] - KL[q(z|x)||p(z)]"),
    ("KL reverse","GAN (approximately)",     "Mode-seeking â†’ sharp samples"),
    ("JSD",       "Vanilla GAN",             "min_G JSD(p_data, p_g) - log4"),
    ("Score fn",  "Diffusion (DDPM)",        "Îµ-prediction â‰¡ score matching"),
    ("Change var","Normalizing Flow",        "log q(x) = log p(z) + log|det J|"),
    ("MLE auto",  "LLM (GPT)",              "CE loss = autoregressive MLE"),
]

print(f"{'From':>15} {'â†’':>3} {'To':>25}  {'Via':>45}")
print("-" * 95)
for src, dst, via in connections:
    print(f"{src:>15} {'â†’':>3} {dst:>25}  {via:>45}")
```



---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” MLE ã®æ•°å­¦æ§‹é€ ã¨æ¨å®šé‡ã®åˆ†é¡

æœ¬è¬›ç¾©ã®æ•°å­¦ã‚¾ãƒ¼ãƒ³ã¯3ã¤ã®å±±ã‚’æ”»ç•¥ã™ã‚‹:

1. **æœ€å°¤æ¨å®šï¼ˆMLEï¼‰** â€” æ¨å®šé‡ã®æ•°å­¦çš„åŸºç›¤ã¨æ¼¸è¿‘è«–
2. **å°¤åº¦é–¢æ•°ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹** â€” æ˜ç¤ºçš„ vs æš—é»™çš„æ¨å®šé‡
3. **çµ±è¨ˆçš„è·é›¢ã®å¿œç”¨** â€” FID, KID, CMMD ã®å®šç¾©ã¨é™ç•Œ

```mermaid
graph TD
    A[MLE<br>å®šç¾© 3.1] --> B[MLE = CEæœ€å°åŒ–<br>å®šç† 3.2]
    B --> C[MLE = KLæœ€å°åŒ–<br>å®šç† 3.3]
    C --> D[MLE ã®æ¼¸è¿‘è«–<br>Fisher 1922]
    D --> E[MLE ã®é™ç•Œ<br>æ½œåœ¨å¤‰æ•°ã¸ã®å‹•æ©Ÿ]

    F[æ˜ç¤ºçš„æ¨å®šé‡<br>Prescribed å®šç¾© 3.5] --> H[å°¤åº¦è¨ˆç®—å¯èƒ½]
    G[æš—é»™çš„æ¨å®šé‡<br>Implicit å®šç¾© 3.6] --> I[å°¤åº¦è¨ˆç®—ä¸èƒ½]

    H --> J[VAE / Flow]
    I --> K[GAN]

    E --> L[æ½œåœ¨å¤‰æ•°ã®å°å…¥<br>ç¬¬8å›ã¸]

    M[FID<br>Wâ‚‚è·é›¢] --> N[KID<br>MMD]
    N --> O[CMMD<br>CLIP-MMD]

    style A fill:#e8f5e9
    style B fill:#e8f5e9
    style C fill:#e8f5e9
    style M fill:#e3f2fd
```

### 3.1 æœ€å°¤æ¨å®šï¼ˆMLEï¼‰ã®å®šç¾©

:::message
ã“ã“ã‹ã‚‰æœ¬è¬›ç¾©ã®æ ¸å¿ƒã«å…¥ã‚‹ã€‚ç¬¬6å›ã® Cross-Entropy ã¨ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒã€ã“ã“ã§ã€Œåˆæµã€ã™ã‚‹ã€‚ãƒšãƒ³ã¨ç´™ã‚’ç”¨æ„ã—ã¦ã€ä¸€è¡Œãšã¤è¿½ã£ã¦ã»ã—ã„ã€‚
:::

**å®šç¾© 3.1ï¼ˆæœ€å°¤æ¨å®šé‡ï¼‰**

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\mathcal{D} = \{x_1, x_2, \ldots, x_N\}$ ãŒçœŸã®åˆ†å¸ƒ $p_\text{data}(x)$ ã‹ã‚‰ i.i.d. ã«ç”Ÿæˆã•ã‚ŒãŸã¨ã™ã‚‹ã€‚ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ¢ãƒ‡ãƒ« $q_\theta(x)$ ã«å¯¾ã—ã¦ã€**æœ€å°¤æ¨å®šé‡**ï¼ˆMaximum Likelihood Estimator, MLEï¼‰ã¯:

$$\hat{\theta}_\text{MLE} = \arg\max_\theta \prod_{i=1}^{N} q_\theta(x_i)$$

å¯¾æ•°ã‚’å–ã‚‹ã¨ï¼ˆ$\log$ ã¯å˜èª¿å¢—åŠ ãªã®ã§ $\arg\max$ ã¯å¤‰ã‚ã‚‰ãªã„ï¼‰:

$$\hat{\theta}_\text{MLE} = \arg\max_\theta \sum_{i=1}^{N} \log q_\theta(x_i) = \arg\max_\theta \frac{1}{N} \sum_{i=1}^{N} \log q_\theta(x_i)$$

Fisher (1922) [^1] ãŒã€ŒOn the mathematical foundations of theoretical statisticsã€ã§ä½“ç³»åŒ–ã—ãŸæ‰‹æ³•ã§ã‚ã‚Šã€çµ±è¨ˆå­¦ã§100å¹´ä»¥ä¸Šã®æ­´å²ã‚’æŒã¤ã€‚

```python
import numpy as np

# MLE for Gaussian: analytical solution
data = np.array([1.2, 2.3, 1.8, 2.1, 1.5, 2.7, 1.9, 2.4])

# MLE estimates
mu_mle = np.mean(data)          # Î¼Ì‚ = (1/N) Î£ xáµ¢
sigma_mle = np.std(data, ddof=0)  # ÏƒÌ‚ = âˆš((1/N) Î£(xáµ¢ - Î¼Ì‚)Â²)

# Average log-likelihood
log_lik = -0.5 * np.log(2 * np.pi * sigma_mle**2) - 0.5 * ((data - mu_mle) / sigma_mle)**2
avg_log_lik = np.mean(log_lik)

print(f"Data: {data}")
print(f"MLE: Î¼Ì‚ = {mu_mle:.4f}, ÏƒÌ‚ = {sigma_mle:.4f}")
print(f"Average log-likelihood: {avg_log_lik:.4f}")

# Verify: this is the maximum
for mu_test in [1.5, 1.99, mu_mle, 2.1, 2.5]:
    ll = np.mean(-0.5 * np.log(2 * np.pi * sigma_mle**2)
                  - 0.5 * ((data - mu_test) / sigma_mle)**2)
    marker = " â† MLE (maximum)" if abs(mu_test - mu_mle) < 1e-10 else ""
    print(f"  Î¼ = {mu_test:.4f}: avg log-lik = {ll:.4f}{marker}")
```

### 3.2 MLE ã¨ Cross-Entropy ã®ç­‰ä¾¡æ€§

**å®šç† 3.2ï¼ˆMLE = Cross-Entropy æœ€å°åŒ–ï¼‰**

ä»»æ„ã®æœ‰é™ $N$ ã«å¯¾ã—ã¦:

$$\hat{\theta}_\text{MLE} = \arg\min_\theta H(\hat{p}, q_\theta)$$

ã“ã“ã§ $\hat{p}(x) = \frac{1}{N}\sum_{i=1}^N \delta(x - x_i)$ ã¯çµŒé¨“åˆ†å¸ƒã€$H(\hat{p}, q_\theta)$ ã¯ Cross-Entropyã€‚ã“ã®ç­‰å¼ã¯ $N \to \infty$ ã‚’å¿…è¦ã¨ã—ãªã„ â€” çµŒé¨“åˆ†å¸ƒ $\hat{p}$ ã«å¯¾ã™ã‚‹ç­‰ä¾¡æ€§ã¯æœ‰é™ $N$ ã§å³å¯†ã«æˆç«‹ã™ã‚‹ã€‚$N \to \infty$ ãŒå¿…è¦ãªã®ã¯ $\hat{p} \to p_\text{data}$ ã®æ„å‘³ã§ã®ä¸€è‡´æ€§ï¼ˆæ€§è³ª 3.4aï¼‰ã€‚

**å°å‡º:**

Step 1: çµŒé¨“åˆ†å¸ƒ $\hat{p}(x) = \frac{1}{N}\sum_{i=1}^{N} \delta(x - x_i)$ ã‚’å°å…¥ã™ã‚‹ã€‚

Step 2: MLE ã®ç›®çš„é–¢æ•°ã‚’å¤‰å½¢ã™ã‚‹:

$$\frac{1}{N} \sum_{i=1}^{N} \log q_\theta(x_i) = \mathbb{E}_{\hat{p}}[\log q_\theta(x)]$$

Step 3: ã“ã‚Œã¯ Cross-Entropy ã®ç¬¦å·åè»¢ã«ç­‰ã—ã„:

$$\mathbb{E}_{\hat{p}}[\log q_\theta(x)] = -H(\hat{p}, q_\theta)$$

Step 4: ã‚ˆã£ã¦:

$$\arg\max_\theta \mathbb{E}_{\hat{p}}[\log q_\theta(x)] = \arg\min_\theta H(\hat{p}, q_\theta) \quad \blacksquare$$

ã“ã®ç­‰ä¾¡æ€§ã¯å¼·åŠ›ã ã€‚ç¬¬6å›ã§å­¦ã‚“ã  Cross-Entropy ã®ã‚ã‚‰ã‚†ã‚‹æ€§è³ªãŒã€MLE ã«ãã®ã¾ã¾é©ç”¨ã§ãã‚‹ã€‚

### 3.3 MLE ã¨ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®ç­‰ä¾¡æ€§

**å®šç† 3.3ï¼ˆMLE = KL æœ€å°åŒ–ï¼‰**

$$\hat{\theta}_\text{MLE} = \arg\min_\theta D_\text{KL}(\hat{p} \| q_\theta)$$

**å°å‡º:**

Step 1: Cross-Entropy ã®åˆ†è§£ï¼ˆç¬¬6å› å®šç† 3.4ï¼‰ã‚’æ€ã„å‡ºã™:

$$H(\hat{p}, q_\theta) = H(\hat{p}) + D_\text{KL}(\hat{p} \| q_\theta)$$

Step 2: $H(\hat{p})$ ã¯ $\theta$ ã«ä¾å­˜ã—ãªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯å®šæ•°ï¼‰ã€‚

Step 3: ã‚ˆã£ã¦:

$$\arg\min_\theta H(\hat{p}, q_\theta) = \arg\min_\theta D_\text{KL}(\hat{p} \| q_\theta) \quad \blacksquare$$

:::message
ã“ã“ã§å…¨ã¦ãŒç¹‹ãŒã£ãŸã€‚**MLE = CE æœ€å°åŒ– = KL æœ€å°åŒ–**ã€‚ç¬¬6å›ã§å­¦ã‚“ã  KL ã®æ€§è³ªãŒå…¨ã¦ MLE ã«é©ç”¨ã§ãã‚‹:
- $D_\text{KL} \geq 0$ï¼ˆGibbs ã®ä¸ç­‰å¼ï¼‰â†’ MLE ã¯æœ€é©ã§éè² ã®èª¤å·®
- $D_\text{KL} = 0 \Leftrightarrow \hat{p} = q_\theta$ â†’ MLE ã¯çœŸã®åˆ†å¸ƒã§æå¤±ã‚¼ãƒ­
- KL ã¯éå¯¾ç§° â†’ MLE ã¯ **mode-covering**ï¼ˆå…¨ã¦ã®ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼ã—ã‚ˆã†ã¨ã™ã‚‹ï¼‰
:::

```python
import numpy as np

# Numerical verification: MLE = CE minimization = KL minimization
np.random.seed(42)
data = np.random.normal(2.0, 1.0, 10000)  # true: N(2, 1)

# Scan over Î¼ values, fix Ïƒ=1
mus = np.linspace(0, 4, 100)
avg_log_liks = []
cross_entropies = []
kl_divs = []

# Empirical entropy H(pÌ‚) (constant)
H_p = 0.5 * np.log(2 * np.pi * np.e * np.var(data))

for mu in mus:
    sigma = 1.0
    # Average log-likelihood
    ll = np.mean(-0.5 * np.log(2 * np.pi * sigma**2) - 0.5 * ((data - mu) / sigma)**2)
    avg_log_liks.append(ll)
    # Cross-entropy H(pÌ‚, q_Î¸) = -E[log q_Î¸(x)]
    ce = -ll
    cross_entropies.append(ce)
    # KL = CE - H(pÌ‚)
    kl = ce - H_p
    kl_divs.append(kl)

# Find optima
best_mle = mus[np.argmax(avg_log_liks)]
best_ce = mus[np.argmin(cross_entropies)]
best_kl = mus[np.argmin(kl_divs)]

print(f"argmax log-likelihood: Î¼ = {best_mle:.4f}")
print(f"argmin Cross-Entropy:  Î¼ = {best_ce:.4f}")
print(f"argmin KL divergence:  Î¼ = {best_kl:.4f}")
print(f"All three agree: {np.allclose(best_mle, best_ce) and np.allclose(best_ce, best_kl)}")
print(f"(True Î¼ = 2.0, sample mean = {np.mean(data):.4f})")
```

### 3.4 MLE ã®æ¼¸è¿‘è«– â€” Fisher ã®éºç”£

Fisher (1922) [^1] ã¯ MLE ã®3ã¤ã®æ¼¸è¿‘çš„æ€§è³ªã‚’ï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ï¼‰ç¤ºã—ãŸ:

**æ€§è³ª 3.4aï¼ˆä¸€è‡´æ€§, Consistencyï¼‰**

$$\hat{\theta}_\text{MLE} \xrightarrow{p} \theta^* \quad (N \to \infty)$$

MLE ã¯ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ç¢ºç‡åæŸã™ã‚‹ã€‚

**æ€§è³ª 3.4bï¼ˆæ¼¸è¿‘æ­£è¦æ€§, Asymptotic Normalityï¼‰**

$$\sqrt{N}(\hat{\theta}_\text{MLE} - \theta^*) \xrightarrow{d} \mathcal{N}(0, \mathcal{I}(\theta^*)^{-1})$$

ã“ã“ã§ $\mathcal{I}(\theta)$ ã¯ **Fisher æƒ…å ±è¡Œåˆ—**ï¼ˆç¬¬6å› Zone 6 ã§å°å…¥ï¼‰:

$$\mathcal{I}(\theta)_{ij} = -\mathbb{E}_{p_\theta}\left[\frac{\partial^2}{\partial \theta_i \partial \theta_j} \log p_\theta(x)\right]$$

**æ€§è³ª 3.4cï¼ˆæ¼¸è¿‘æœ‰åŠ¹æ€§, Asymptotic Efficiencyï¼‰**

**Cramer-Rao ä¸ç­‰å¼** (CramÃ©r 1946 [^14] / Rao 1945 [^15]): ä»»æ„ã®ä¸åæ¨å®šé‡ $\hat{\theta}$ ã«å¯¾ã—ã¦:

$$\text{Var}(\hat{\theta}) \geq [\mathcal{I}(\theta)]^{-1}$$

ã“ã®ä¸‹ç•Œã‚’**Cramer-Rao ä¸‹ç•Œ**ã¨å‘¼ã¶ã€‚MLE ã¯ã“ã®ä¸‹ç•Œã‚’æ¼¸è¿‘çš„ã«é”æˆã™ã‚‹ã€‚ã¤ã¾ã‚Šã€æ¼¸è¿‘çš„ã«æœ€å°åˆ†æ•£ã®ä¸åæ¨å®šé‡ã«ç­‰ã—ã„ã€‚

```python
import numpy as np

# Demonstration: MLE convergence and asymptotic normality
np.random.seed(42)
true_mu, true_sigma = 3.0, 2.0
sample_sizes = [10, 50, 100, 500, 1000, 5000]
n_trials = 1000

print(f"True parameters: Î¼ = {true_mu}, Ïƒ = {true_sigma}")
print(f"Fisher info for Î¼: I(Î¼) = 1/ÏƒÂ² = {1/true_sigma**2:.4f}")
print(f"Asymptotic variance of Î¼Ì‚: 1/(NÂ·I(Î¼)) = ÏƒÂ²/N")
print()
print(f"{'N':>6} {'Mean(Î¼Ì‚)':>10} {'Std(Î¼Ì‚)':>10} {'Theory':>10} {'Ratio':>8}")
print("-" * 50)

for N in sample_sizes:
    mu_hats = []
    for _ in range(n_trials):
        data = np.random.normal(true_mu, true_sigma, N)
        mu_hats.append(np.mean(data))

    empirical_std = np.std(mu_hats)
    theoretical_std = true_sigma / np.sqrt(N)

    print(f"{N:6d} {np.mean(mu_hats):10.4f} {empirical_std:10.4f} "
          f"{theoretical_std:10.4f} {empirical_std/theoretical_std:8.4f}")
```

**å‡ºåŠ›ä¾‹:**
```
True parameters: Î¼ = 3.0, Ïƒ = 2.0
Fisher info for Î¼: I(Î¼) = 1/ÏƒÂ² = 0.2500
Asymptotic variance of Î¼Ì‚: 1/(NÂ·I(Î¼)) = ÏƒÂ²/N

     N    Mean(Î¼Ì‚)    Std(Î¼Ì‚)     Theory    Ratio
--------------------------------------------------
    10     3.0012     0.6367     0.6325    1.0067
    50     2.9992     0.2826     0.2828    0.9994
   100     3.0037     0.1988     0.2000    0.9940
   500     3.0003     0.0897     0.0894    1.0030
  1000     2.9999     0.0628     0.0632    0.9934
  5000     3.0001     0.0283     0.0283    1.0005
```

Ratio ãŒã»ã¼ 1.0 â€” MLE ã®åˆ†æ•£ãŒ Fisher æƒ…å ±è¡Œåˆ—ã‹ã‚‰äºˆæ¸¬ã•ã‚Œã‚‹ç†è«–å€¤ã«ä¸€è‡´ã—ã¦ã„ã‚‹ã€‚

### 3.5 MLE ã®é™ç•Œã¨æ½œåœ¨å¤‰æ•°ã¸ã®å‹•æ©Ÿ

MLE ã«ã¯æ ¹æœ¬çš„ãªé™ç•ŒãŒã‚ã‚‹ã€‚

**é™ç•Œ 1: ãƒ¢ãƒ‡ãƒ«æ—ã®è¡¨ç¾åŠ›ã«ä¾å­˜**

Zone 0 ã§è¦‹ãŸé€šã‚Šã€å˜å³°ã‚¬ã‚¦ã‚¹ã§åŒå³°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã¨ã€ã€Œæœ€è‰¯ã®å¦¥å”ã€ã«ã—ã‹ãªã‚‰ãªã„ã€‚

**é™ç•Œ 2: é«˜æ¬¡å…ƒã§ã®è¨ˆç®—å›°é›£æ€§**

$p_\theta(x)$ ã®æ­£è¦åŒ–å®šæ•°ã®è¨ˆç®—:

$$Z(\theta) = \int p_\theta(x) \, dx$$

ãŒé«˜æ¬¡å…ƒã§ã¯ tractable ã§ãªããªã‚‹ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ã« $\text{softmax}$ ã‚’ä½¿ãˆã°é›¢æ•£çš„ãªæ­£è¦åŒ–ã¯ã§ãã‚‹ãŒã€é€£ç¶šç©ºé–“ã§ã®æ­£è¦åŒ–ã¯ä¸€èˆ¬ã«ä¸å¯èƒ½ã€‚

**é™ç•Œ 3: å‘¨è¾ºåŒ–ã®å›°é›£æ€§**

æ½œåœ¨å¤‰æ•° $z$ ã‚’å°å…¥ã™ã‚‹ã¨:

$$p_\theta(x) = \int p_\theta(x, z) \, dz = \int p_\theta(x | z) \, p(z) \, dz$$

ã“ã®ç©åˆ†ã¯ã€$p_\theta(x|z)$ ãŒãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å ´åˆã€è§£æçš„ã«è¨ˆç®—ã§ããªã„ã€‚

```python
import numpy as np
from scipy import stats

# Limitation 1: model misspecification
np.random.seed(42)

# True distribution: mixture of 3 Gaussians
def true_pdf(x):
    return (0.3 * stats.norm.pdf(x, -3, 0.5) +
            0.4 * stats.norm.pdf(x, 0, 1.0) +
            0.3 * stats.norm.pdf(x, 4, 0.7))

# Sample from true distribution
def sample_true(n):
    components = np.random.choice(3, size=n, p=[0.3, 0.4, 0.3])
    mus = [-3, 0, 4]
    sigmas = [0.5, 1.0, 0.7]
    return np.array([np.random.normal(mus[c], sigmas[c]) for c in components])

data = sample_true(5000)

# MLE with single Gaussian â†’ bad fit
mu_single = np.mean(data)
sigma_single = np.std(data)

# KL divergence (approximate via Monte Carlo)
x_grid = np.linspace(-6, 7, 10000)
p_true = true_pdf(x_grid)
q_model = stats.norm.pdf(x_grid, mu_single, sigma_single)

# Avoid log(0)
mask = (p_true > 1e-10) & (q_model > 1e-10)
kl_approx = np.trapz(p_true[mask] * np.log(p_true[mask] / q_model[mask]), x_grid[mask])

print(f"True distribution: 3-component Gaussian mixture")
print(f"MLE (single Gaussian): Î¼ = {mu_single:.3f}, Ïƒ = {sigma_single:.3f}")
print(f"KL(p_true || q_model) â‰ˆ {kl_approx:.4f} nats")
print(f"â†’ Large KL because single Gaussian cannot capture 3 modes")
print(f"\nSolution: introduce LATENT VARIABLES (Lecture 8)")
print(f"  p(x) = Î£_k Ï€_k Â· N(x; Î¼_k, Ïƒ_kÂ²)  â† mixture model")
print(f"  p(x) = âˆ« p(x|z) p(z) dz             â† continuous latent (VAE)")
```

:::message
ã“ã“ãŒç¬¬8å›ï¼ˆæ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³•ï¼‰ã¸ã®æ¥ç¶šç‚¹ã ã€‚MLE ã®é™ç•Œã‚’æ‰“ç ´ã™ã‚‹ãŸã‚ã«ã€æ½œåœ¨å¤‰æ•° $z$ ã‚’å°å…¥ã—ã¦ $p(x) = \int p(x|z)p(z)dz$ ã¨åˆ†è§£ã™ã‚‹ã€‚ã ãŒã€ã“ã®ç©åˆ†ã¯è§£æçš„ã«è¨ˆç®—ã§ããªã„ã€‚EMç®—æ³•ãŒãã‚Œã‚’è¿‘ä¼¼çš„ã«è§£ãã€ã•ã‚‰ã« VAE ãŒ neural network ã§å¼·åŠ›ã«ã™ã‚‹ã€‚ã“ã®æµã‚Œã‚’é ­ã«å…¥ã‚Œã¦ãŠã„ã¦ã»ã—ã„ã€‚
:::

### 3.6 å°¤åº¦é–¢æ•°ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹ â€” æ˜ç¤ºçš„ vs æš—é»™çš„æ¨å®šé‡

Mohamed & Lakshminarayanan (2016) [^6] ã¯ã€ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã®æ¨å®šæ‰‹æ³•ã‚’å°¤åº¦é–¢æ•°ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹ã§2ã¤ã«å¤§åˆ¥ã—ãŸã€‚

**å®šç¾© 3.5ï¼ˆPrescribed Model / è¦å®šãƒ¢ãƒ‡ãƒ«ï¼‰**

ç¢ºç‡å¯†åº¦é–¢æ•° $q_\theta(x)$ ãŒé™½ã«å®šç¾©ã§ãã€$x$ ã‚’ä»£å…¥ã—ã¦ $q_\theta(x)$ ã®å€¤ãŒè¨ˆç®—å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã€‚

$$\text{Prescribed}: \quad q_\theta(x) \text{ is explicitly defined and evaluable}$$

ä¾‹: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã€GMMã€VAEï¼ˆELBO çµŒç”±ï¼‰ã€Normalizing Flow

**å®šç¾© 3.6ï¼ˆImplicit Model / æš—é»™çš„ãƒ¢ãƒ‡ãƒ«ï¼‰**

ç¢ºç‡å¯†åº¦é–¢æ•°ã‚’é™½ã«å®šç¾©ã›ãšã€ç”Ÿæˆéç¨‹ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹ç¶šãï¼‰ã®ã¿ã‚’å®šç¾©ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚

$$\text{Implicit}: \quad x = G_\theta(z), \quad z \sim p(z)$$

å¯†åº¦ $q_\theta(x)$ ã¯å®šç¾©ã¯ã•ã‚Œã‚‹ãŒã€è¨ˆç®—ä¸èƒ½ï¼ˆintractableï¼‰ã€‚

ä¾‹: GAN

```python
# Prescribed model: can compute q_Î¸(x)
def prescribed_density(x, mu, sigma):
    """Gaussian: density is COMPUTABLE"""
    return np.exp(-0.5 * ((x - mu) / sigma)**2) / (sigma * np.sqrt(2 * np.pi))

# Implicit model: can only SAMPLE
def implicit_sample(z, generator_weights):
    """GAN generator: density is NOT computable, but sampling is easy"""
    # x = G_Î¸(z) â€” a neural network transform
    # We can get x, but CANNOT compute p(x)
    return z  # placeholder for neural net

x_test = 1.5

# Prescribed: "the probability of x = 1.5 is 0.242"
print(f"Prescribed: q(x={x_test}) = {prescribed_density(x_test, 2.0, 1.0):.4f}")

# Implicit: "I can generate samples, but can't tell you p(x = 1.5)"
print(f"Implicit: q(x={x_test}) = ??? (not computable)")
print(f"Implicit: samples = {np.random.normal(2.0, 1.0, 5).round(3)}")
```

ã“ã®åˆ†é¡ãŒæ·±ã„æ„å‘³ã‚’æŒã¤ã®ã¯ã€**è¨“ç·´æ–¹æ³•ãŒæ ¹æœ¬çš„ã«ç•°ãªã‚‹**ã‹ã‚‰ã ã€‚

| ãƒ¢ãƒ‡ãƒ«+æ¨å®šæ‰‹æ³•ã®åˆ†é¡ | å°¤åº¦ $q_\theta(x)$ | æ¨å®šæ‰‹æ³• | ä»£è¡¨ãƒ¢ãƒ‡ãƒ« |
|:-----|:-------------------|:---------|:-------|
| **æ˜ç¤ºçš„æ¨å®šé‡** (Prescribed) | è¨ˆç®—å¯èƒ½ | ç›´æ¥MLE / å¤‰åˆ†æ¨è«– | Flow, è‡ªå·±å›å¸° |
| **æš—é»™çš„æ¨å®šé‡** (Implicit) | è¨ˆç®—ä¸èƒ½ | æ•µå¯¾çš„è¨“ç·´ / ã‚«ãƒ¼ãƒãƒ«æ³• | GAN |
| **æ˜ç¤ºçš„ + æ½œåœ¨å¤‰æ•°** | å‘¨è¾ºåŒ–ãŒå›°é›£ | ELBO æœ€å¤§åŒ–ï¼ˆå¤‰åˆ†MLEï¼‰ | VAE |
| **ã‚¹ã‚³ã‚¢æ¨å®šé‡** | ä¸è¦ï¼ˆ$\nabla_x \log p$ ã®ã¿ï¼‰ | Score Matching | NCSN, DDPM |

### 3.7 MLEå¤‰å½¢1: å¤‰æ•°å¤‰æ›ã«ã‚ˆã‚‹å°¤åº¦è¨ˆç®—ï¼ˆæ¦‚è¦ã€è©³ç´°ã¯Course IIï¼‰

Normalizing Flow [^7] [^11] [^12] ã¯å¤‰æ•°å¤‰æ›å…¬å¼ã‚’ä½¿ã£ã¦å³å¯†ãªå°¤åº¦è¨ˆç®—ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚

**å®šç† 3.7ï¼ˆå¤‰æ•°å¤‰æ›å…¬å¼ï¼‰**

$z \sim p(z)$ã€$x = f(z)$ ã§ $f$ ãŒå¾®åˆ†åŒç›¸å†™åƒï¼ˆbijection + differentiableï¼‰ã®ã¨ã:

$$q_\theta(x) = p(z) \left|\det \frac{\partial f^{-1}}{\partial x}\right| = p(z) \left|\det \frac{\partial f}{\partial z}\right|^{-1}$$

å¯¾æ•°ã‚’å–ã‚‹ã¨:

$$\log q_\theta(x) = \log p(f^{-1}(x)) + \log \left|\det \frac{\partial f^{-1}}{\partial x}\right|$$

```python
import numpy as np

# Simple 1D flow example: f(z) = z + Î±Â·tanh(z)
alpha = 0.8

def flow_forward(z):
    """x = f(z) = z + Î±Â·tanh(z)"""
    return z + alpha * np.tanh(z)

def flow_log_det_jacobian(z):
    """log |df/dz| = log |1 + Î±Â·(1 - tanhÂ²(z))|"""
    return np.log(np.abs(1 + alpha * (1 - np.tanh(z)**2)))

# Compute log-likelihood
z_samples = np.random.normal(0, 1, 10000)
x_samples = flow_forward(z_samples)

# log p(z) for standard normal
log_pz = -0.5 * z_samples**2 - 0.5 * np.log(2 * np.pi)

# log q(x) = log p(z) - log |df/dz|   (inverse function theorem)
log_qx = log_pz - flow_log_det_jacobian(z_samples)

print(f"Prior: z ~ N(0, 1)")
print(f"Flow: x = z + {alpha}Â·tanh(z)")
print(f"z statistics: mean = {z_samples.mean():.3f}, std = {z_samples.std():.3f}")
print(f"x statistics: mean = {x_samples.mean():.3f}, std = {x_samples.std():.3f}")
print(f"Average log q(x): {log_qx.mean():.4f}")
print(f"â†’ Flow transforms simple distribution into complex one with EXACT likelihood")
```

NICE [^11] ã¨ Real NVP [^12] ã¯ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒä¸‰è§’è¡Œåˆ—ã«ãªã‚‹ã‚ˆã†ã« $f$ ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã§ã€è¡Œåˆ—å¼ã®è¨ˆç®—ã‚’ $O(D)$ ã«å‰Šæ¸›ã—ãŸã€‚

### 3.8 MLEå¤‰å½¢2: æš—é»™çš„æ¨å®šé‡ â€” GAN ã®ç›®çš„é–¢æ•°ï¼ˆæ¦‚è¦ã€è©³ç´°ã¯Course IIï¼‰

Goodfellow+ (2014) [^2] ã¯ã€å¯†åº¦ã‚’é™½ã«å®šç¾©ã—ãªã„å…¨ãæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ãŸã€‚

**å®šç¾© 3.8ï¼ˆGAN ã®ç›®çš„é–¢æ•°ï¼‰**

$$\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_\text{data}}[\log D(x)] + \mathbb{E}_{z \sim p(z)}[\log(1 - D(G(z)))]$$

ã“ã“ã§ $G: z \to x$ ã¯ç”Ÿæˆå™¨ã€$D: x \to [0, 1]$ ã¯åˆ¤åˆ¥å™¨ã€‚

**å®šç† 3.8aï¼ˆæœ€é©åˆ¤åˆ¥å™¨ï¼‰**

å›ºå®šã•ã‚ŒãŸ $G$ ã«å¯¾ã—ã¦ã€æœ€é©ãªåˆ¤åˆ¥å™¨ã¯:

$$D^*_G(x) = \frac{p_\text{data}(x)}{p_\text{data}(x) + p_g(x)}$$

**å°å‡º:**

$V(D, G)$ ã‚’ $D(x)$ ã«ã¤ã„ã¦æœ€å¤§åŒ–ã™ã‚‹ã€‚$y = D(x)$ ã¨æ›¸ãã¨:

$$f(y) = a \log y + b \log(1 - y)$$

$$f'(y) = \frac{a}{y} - \frac{b}{1-y} = 0 \implies y = \frac{a}{a+b}$$

ã“ã“ã§ $a = p_\text{data}(x)$, $b = p_g(x)$ ãªã®ã§ $D^*(x) = \frac{p_\text{data}(x)}{p_\text{data}(x) + p_g(x)}$ã€‚$\blacksquare$

**å®šç† 3.8bï¼ˆGAN ã¨ JSDï¼‰**

æœ€é©åˆ¤åˆ¥å™¨ $D^*$ ã®ä¸‹ã§:

$$V(D^*, G) = -\log 4 + 2 \cdot D_\text{JS}(p_\text{data} \| p_g)$$

ã“ã“ã§ $D_\text{JS}$ ã¯ Jensen-Shannon ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆç¬¬6å› 3.11bï¼‰ã€‚

ã‚ˆã£ã¦ **GAN ã®è¨“ç·´ã¯ JSD ã®æœ€å°åŒ–**ã«ç­‰ã—ã„ã€‚

```python
import numpy as np

# GAN objective demonstration
def optimal_discriminator(p_data, p_gen):
    """D*(x) = p_data(x) / (p_data(x) + p_gen(x))"""
    return p_data / (p_data + p_gen + 1e-10)

def jsd(p, q, x_grid):
    """Jensen-Shannon divergence"""
    m = 0.5 * (p + q)
    kl_pm = np.trapz(p * np.log(p / (m + 1e-10) + 1e-10) * (p > 1e-10), x_grid)
    kl_qm = np.trapz(q * np.log(q / (m + 1e-10) + 1e-10) * (q > 1e-10), x_grid)
    return 0.5 * (kl_pm + kl_qm)

from scipy import stats
x = np.linspace(-5, 8, 1000)

# True distribution
p = 0.5 * stats.norm.pdf(x, 0, 1) + 0.5 * stats.norm.pdf(x, 4, 1)

# Generator distribution (progressively improving)
stages = [
    ("Random",     stats.norm.pdf(x, 5, 3)),
    ("Learning",   stats.norm.pdf(x, 2, 2)),
    ("Good",       0.5 * stats.norm.pdf(x, 0.2, 1.1) + 0.5 * stats.norm.pdf(x, 3.8, 1.1)),
    ("Converged",  0.5 * stats.norm.pdf(x, 0, 1) + 0.5 * stats.norm.pdf(x, 4, 1)),
]

print(f"{'Stage':>12} {'JSD':>10} {'V(D*,G)':>12} {'D* at x=2':>12}")
print("-" * 50)
for name, q in stages:
    js = jsd(p, q, x)
    v = -np.log(4) + 2 * js
    d_star = optimal_discriminator(p[500], q[500])  # at x â‰ˆ 2
    print(f"{name:>12} {js:10.4f} {v:12.4f} {d_star:12.4f}")
```

### 3.9 MLEå¤‰å½¢3: ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°æ¨å®šé‡ï¼ˆæ¦‚è¦ã€è©³ç´°ã¯Course IIï¼‰

Song & Ermon (2019) [^10] ã¯ã€å¯†åº¦ $p(x)$ ã®ä»£ã‚ã‚Šã«**ã‚¹ã‚³ã‚¢é–¢æ•°**ã‚’å­¦ã¶ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ãŸã€‚

**å®šç¾© 3.9ï¼ˆã‚¹ã‚³ã‚¢é–¢æ•°ï¼‰**

$$s_\theta(x) \approx \nabla_x \log p_\text{data}(x)$$

ã‚¹ã‚³ã‚¢é–¢æ•°ã¯ç¢ºç‡å¯†åº¦ã®å‹¾é…ã§ã‚ã‚Šã€æ­£è¦åŒ–å®šæ•° $Z$ ã«ä¾å­˜ã—ãªã„:

$$\nabla_x \log p(x) = \nabla_x \log \frac{\tilde{p}(x)}{Z} = \nabla_x \log \tilde{p}(x)$$

ã“ã‚ŒãŒç”»æœŸçš„ãªç†ç”±ã¯ã€æ­£è¦åŒ–å®šæ•°ã®è¨ˆç®—ã‚’å®Œå…¨ã«å›é¿ã§ãã‚‹ã“ã¨ã ã€‚

Ho+ (2020) [^5] ã¯ã€ã“ã®ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã¨æ‹¡æ•£éç¨‹ã‚’çµ„ã¿åˆã‚ã›ãŸ DDPM ã‚’ææ¡ˆã—ã€ç”»åƒç”Ÿæˆã®å“è³ªã‚’åŠ‡çš„ã«å‘ä¸Šã•ã›ãŸã€‚DDPM ã®æå¤±é–¢æ•°:

$$\mathcal{L}_\text{simple} = \mathbb{E}_{t, x_0, \epsilon}\left[\|\epsilon - \epsilon_\theta(x_t, t)\|^2\right]$$

ã¯ã€denoising score matching ã®é‡ã¿ä»˜ãå¤‰å½¢ã¨ã—ã¦è§£é‡ˆã§ãã‚‹ã€‚

```python
import numpy as np

# Score function demonstration
def gaussian_score(x, mu, sigma):
    """âˆ‡_x log N(x; Î¼, ÏƒÂ²) = -(x - Î¼)/ÏƒÂ²"""
    return -(x - mu) / sigma**2

# Score for mixture is weighted sum
def mixture_score(x, mus, sigmas, weights):
    """Score of Gaussian mixture (not simple weighted average of scores!)"""
    # p(x) = Î£ w_k N(x; Î¼_k, Ïƒ_kÂ²)
    # âˆ‡ log p(x) = (Î£ w_k N(x;Î¼_k,Ïƒ_kÂ²) Â· score_k) / p(x)
    densities = np.array([w * np.exp(-0.5*((x-m)/s)**2) / (s*np.sqrt(2*np.pi))
                          for w, m, s in zip(weights, mus, sigmas)])
    scores = np.array([-(x - m) / s**2 for m, s in zip(mus, sigmas)])
    p_x = densities.sum(axis=0)
    return (densities * scores).sum(axis=0) / (p_x + 1e-10)

x_grid = np.linspace(-5, 8, 200)
mus = [0, 4]
sigmas = [1, 1]
weights = [0.5, 0.5]

scores = mixture_score(x_grid, mus, sigmas, weights)

print("Score function tells you: 'which direction increases density'")
print(f"At x = -3: score = {mixture_score(np.array([-3.0]), mus, sigmas, weights)[0]:.3f} (â†’ positive, go right)")
print(f"At x =  0: score = {mixture_score(np.array([0.0]), mus, sigmas, weights)[0]:.3f} (â†’ near zero, at mode)")
print(f"At x =  2: score = {mixture_score(np.array([2.0]), mus, sigmas, weights)[0]:.3f} (â†’ valley between modes)")
print(f"At x =  4: score = {mixture_score(np.array([4.0]), mus, sigmas, weights)[0]:.3f} (â†’ near zero, at mode)")
print(f"At x =  7: score = {mixture_score(np.array([7.0]), mus, sigmas, weights)[0]:.3f} (â†’ negative, go left)")
```

### 3.10 Mode-Covering vs Mode-Seeking

ç¬¬6å›ã§ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®éå¯¾ç§°æ€§ã‚’å­¦ã‚“ã ã€‚ã“ã“ã§ã¯ãã®çµæœãŒæ¨å®šé‡ã®æŒ™å‹•ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’æ˜ã‚Šä¸‹ã’ã‚‹ã€‚

**å‰å‘ã KLï¼ˆMode-Coveringï¼‰** â€” MLE / VAE

$$D_\text{KL}(p_\text{data} \| q_\theta) = \mathbb{E}_{p_\text{data}}\left[\log \frac{p_\text{data}(x)}{q_\theta(x)}\right]$$

$p_\text{data}(x) > 0$ ã®å ´æ‰€ã§ $q_\theta(x) \approx 0$ ã ã¨ $\log \frac{p}{q} \to \infty$ â€” **ãƒšãƒŠãƒ«ãƒ†ã‚£å¤§**ã€‚
â†’ $q_\theta$ ã¯ $p_\text{data}$ ã®å…¨ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼ã—ã‚ˆã†ã¨ã™ã‚‹ï¼ˆmode-coveringï¼‰ã€‚
â†’ çµæœ: ã¼ã‚„ã‘ã‚‹ãŒã€å…¨ãƒ¢ãƒ¼ãƒ‰ã‚’å«ã‚€ã€‚

**é€†å‘ã KLï¼ˆMode-Seekingï¼‰** â€” GANï¼ˆå®Ÿè³ªçš„ã«ï¼‰

$$D_\text{KL}(q_\theta \| p_\text{data}) = \mathbb{E}_{q_\theta}\left[\log \frac{q_\theta(x)}{p_\text{data}(x)}\right]$$

$q_\theta(x) > 0$ ã®å ´æ‰€ã§ $p_\text{data}(x) \approx 0$ ã ã¨ $\log \frac{q}{p} \to \infty$ â€” **ãƒšãƒŠãƒ«ãƒ†ã‚£å¤§**ã€‚
â†’ $q_\theta$ ã¯ $p_\text{data}$ ã®ãƒ¢ãƒ¼ãƒ‰ã®ä¸Šã ã‘ã«é›†ä¸­ã™ã‚‹ï¼ˆmode-seekingï¼‰ã€‚
â†’ çµæœ: é®®æ˜ã ãŒã€ä¸€éƒ¨ã®ãƒ¢ãƒ¼ãƒ‰ã‚’ç„¡è¦–ã™ã‚‹ï¼ˆmode collapseï¼‰ã€‚

```python
import numpy as np
from scipy import stats

# Demonstration: mode-covering vs mode-seeking
np.random.seed(42)
x = np.linspace(-6, 10, 1000)

# True distribution: bimodal
p_true = 0.5 * stats.norm.pdf(x, 0, 1) + 0.5 * stats.norm.pdf(x, 6, 1)

# Mode-covering (forward KL / MLE): tries to cover both modes
# â†’ single Gaussian spreads wide
q_covering = stats.norm.pdf(x, 3, 3.5)

# Mode-seeking (reverse KL): locks onto one mode
q_seeking = stats.norm.pdf(x, 0, 1.0)

# Compute KLs
def kl_numerical(p, q, x_grid):
    mask = (p > 1e-10) & (q > 1e-10)
    return np.trapz(p[mask] * np.log(p[mask] / q[mask]), x_grid[mask])

kl_forward_covering = kl_numerical(p_true, q_covering, x)
kl_forward_seeking = kl_numerical(p_true, q_seeking, x)
kl_reverse_covering = kl_numerical(q_covering, p_true, x)
kl_reverse_seeking = kl_numerical(q_seeking, p_true, x)

print("Mode-Covering (wide Gaussian, Î¼=3, Ïƒ=3.5):")
print(f"  Forward KL  D(p||q): {kl_forward_covering:.4f}")
print(f"  Reverse KL  D(q||p): {kl_reverse_covering:.4f}")
print()
print("Mode-Seeking (narrow Gaussian, Î¼=0, Ïƒ=1.0):")
print(f"  Forward KL  D(p||q): {kl_forward_seeking:.4f}")
print(f"  Reverse KL  D(q||p): {kl_reverse_seeking:.4f}")
print()
print("â†’ Mode-covering has LOWER forward KL (MLE prefers it)")
print("â†’ Mode-seeking has LOWER reverse KL (GAN-style prefers it)")
```

:::message
**å¼•ã£ã‹ã‹ã‚Šãƒã‚¤ãƒ³ãƒˆ**: GAN ãŒã€Œé€†å‘ã KL ã‚’æœ€å°åŒ–ã™ã‚‹ã€ã¨æ›¸ã„ãŸãŒã€å³å¯†ã«ã¯ GAN ã¯ JSD ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚JSD ã¯ KL ã®å¯¾ç§°åŒ–ç‰ˆã§ã€forward ã¨ reverse ã®ä¸­é–“çš„ãªæŒ¯ã‚‹èˆã„ã‚’ã™ã‚‹ã€‚ãã‚Œã§ã‚‚ GAN ãŒ mode-seeking ã«ãªã‚Šã‚„ã™ã„ã®ã¯ã€åˆ¤åˆ¥å™¨ã®å‹•æ…‹ãŒé€†å‘ã KL çš„ãªåœ§åŠ›ã‚’ç”Ÿã‚€ãŸã‚ã ã€‚ã“ã®å¾®å¦™ãªé•ã„ã¯ç¬¬12å›ï¼ˆGAN ã®ç†è«–ï¼‰ã§è©³ã—ãæ‰±ã†ã€‚
:::

### 3.11 äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç†è«–

æ¨å®šé‡ã§å­¦ç¿’ã—ãŸåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã«ã¯ã€äº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç†è«–ãŒå¿…è¦ã ã€‚ä¸»è¦ãªæ‰‹æ³•ã‚’æ•´ç†ã™ã‚‹ã€‚

| ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³• | åŸç† | åˆ©ç”¨ãƒ¢ãƒ‡ãƒ« | è¨ˆç®—ã‚³ã‚¹ãƒˆ |
|:----------------|:-----|:-----------|:-----------|
| **ç¥–å…ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°** | åŒæ™‚åˆ†å¸ƒã‚’æ¡ä»¶ä»˜ãåˆ†è§£ | è‡ªå·±å›å¸°ï¼ˆGPTï¼‰ | $O(T)$ é€æ¬¡ |
| **Rejection Sampling** | ææ¡ˆåˆ†å¸ƒã‹ã‚‰å€™è£œç”Ÿæˆ â†’ æ£„å´ | ç†è«–çš„ | é«˜æ¬¡å…ƒã§æŒ‡æ•°çš„ |
| **Importance Sampling** | é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒ« | VAE ã® IWAE | $O(K \cdot N)$ |
| **MCMC** | Markov Chain ã§å®šå¸¸åˆ†å¸ƒã«åæŸ | ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ¢ãƒ‡ãƒ« | åæŸä¿è¨¼ãªã— |
| **Reparameterization** | $z = \mu + \sigma \cdot \epsilon$ | VAE | $O(1)$ |
| **Langevin Dynamics** | $x_{t+1} = x_t + \eta \nabla_x \log p + \sqrt{2\eta}\epsilon$ | Score Model | $O(T)$ åå¾© |
| **é€†æ‹¡æ•£éç¨‹** | $x_{t-1} \sim p_\theta(x_{t-1}|x_t)$ | Diffusion | $O(T)$ åå¾© |

```python
import numpy as np

# Ancestral sampling from autoregressive model (simplified)
def ancestral_sampling_demo():
    """p(x1, x2, x3) = p(x1) Â· p(x2|x1) Â· p(x3|x1,x2)"""
    x1 = np.random.choice(['A', 'B'], p=[0.7, 0.3])

    # p(x2|x1)
    if x1 == 'A':
        x2 = np.random.choice(['C', 'D'], p=[0.6, 0.4])
    else:
        x2 = np.random.choice(['C', 'D'], p=[0.2, 0.8])

    # p(x3|x1,x2)
    x3 = np.random.choice(['E', 'F'], p=[0.5, 0.5])

    return x1 + x2 + x3

# Reparameterization trick
def reparameterization_demo(mu, sigma, n_samples=5):
    """z = Î¼ + Ïƒ Â· Îµ, Îµ ~ N(0,1) â€” gradient flows through Î¼ and Ïƒ"""
    epsilon = np.random.normal(0, 1, n_samples)
    z = mu + sigma * epsilon
    return z

# Langevin dynamics
def langevin_sampling(score_fn, x_init, step_size=0.01, n_steps=100):
    """x_{t+1} = x_t + Î· Â· âˆ‡_x log p(x_t) + âˆš(2Î·) Â· Îµ"""
    x = x_init.copy()
    trajectory = [x.copy()]
    for _ in range(n_steps):
        noise = np.random.normal(0, 1, x.shape)
        x = x + step_size * score_fn(x) + np.sqrt(2 * step_size) * noise
        trajectory.append(x.copy())
    return np.array(trajectory)

# Demo: Langevin sampling from N(2, 1)
score_fn = lambda x: -(x - 2.0)  # score of N(2, 1)
x_init = np.array([10.0])        # start far away
traj = langevin_sampling(score_fn, x_init, step_size=0.05, n_steps=200)

print(f"Langevin dynamics: start at x = {x_init[0]:.1f}")
print(f"  After 50 steps:  x = {traj[50, 0]:.3f}")
print(f"  After 100 steps: x = {traj[100, 0]:.3f}")
print(f"  After 200 steps: x = {traj[200, 0]:.3f}")
print(f"  Target: N(2, 1)")
```

### 3.12 çµ±è¨ˆçš„è·é›¢ã®å¿œç”¨ â€” æ¨å®šé‡ã®è©•ä¾¡æŒ‡æ¨™

æ¨å®šé‡ã®å“è³ªã‚’æ•°å­¦çš„ã«ã©ã†æ¸¬ã‚‹ã‹ã€‚ã“ã‚Œã¯çµ±è¨ˆçš„è·é›¢ã®å¿œç”¨å•é¡Œã ã€‚ä¸»è¦ãªæŒ‡æ¨™ã‚’æ•°å­¦çš„ã«å®šç¾©ã™ã‚‹ã€‚

**å®šç¾© 3.12aï¼ˆFrechet Inception Distance, FIDï¼‰** [^4]

$$\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)$$

ã“ã“ã§ $(\mu_r, \Sigma_r)$ ã¨ $(\mu_g, \Sigma_g)$ ã¯ãã‚Œãã‚Œå®Ÿç”»åƒã¨ç”Ÿæˆç”»åƒã® Inception-v3 ç‰¹å¾´ç©ºé–“ã§ã®å¹³å‡ã¨å…±åˆ†æ•£ã€‚

FID ã¯2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã® **Frechet è·é›¢**ï¼ˆWasserstein-2 è·é›¢ï¼‰:

$$W_2^2(\mathcal{N}(\mu_1, \Sigma_1), \mathcal{N}(\mu_2, \Sigma_2)) = \|\mu_1 - \mu_2\|^2 + \text{Tr}(\Sigma_1 + \Sigma_2 - 2(\Sigma_1\Sigma_2)^{1/2})$$

```python
import numpy as np

def compute_fid(mu1, sigma1, mu2, sigma2):
    """Frechet Inception Distance between two Gaussian distributions"""
    diff = mu1 - mu2

    # Matrix square root via eigendecomposition
    # (Î£â‚Î£â‚‚)^{1/2}
    product = sigma1 @ sigma2
    eigvals, eigvecs = np.linalg.eigh(product)
    eigvals = np.maximum(eigvals, 0)  # numerical stability
    sqrt_product = eigvecs @ np.diag(np.sqrt(eigvals)) @ eigvecs.T

    fid = np.dot(diff, diff) + np.trace(sigma1 + sigma2 - 2 * sqrt_product)
    return fid
    # NOTE: This computes (Î£â‚Î£â‚‚)^{1/2} via eigh, which assumes the product is
    # symmetric. The exact FrÃ©chet distance uses (Î£â‚^{1/2} Î£â‚‚ Î£â‚^{1/2})^{1/2},
    # which is always symmetric positive semi-definite. When Î£â‚ and Î£â‚‚ commute
    # (or are close), the two coincide. For production use, prefer scipy.linalg.sqrtm.

# Example: 2D feature space
np.random.seed(42)
d = 2

# Real data statistics
mu_r = np.array([1.0, 2.0])
sigma_r = np.array([[1.0, 0.3], [0.3, 0.8]])

# Generated data statistics (progressively improving)
models = {
    "Random":    (np.array([5.0, 5.0]), np.eye(2) * 3),
    "Epoch 10":  (np.array([2.0, 3.0]), np.array([[1.5, 0.2], [0.2, 1.2]])),
    "Epoch 100": (np.array([1.1, 2.1]), np.array([[1.1, 0.35], [0.35, 0.85]])),
    "Converged": (np.array([1.0, 2.0]), np.array([[1.0, 0.3], [0.3, 0.8]])),
}

print(f"{'Model':>12} {'FID':>10}")
print("-" * 25)
for name, (mu_g, sigma_g) in models.items():
    fid = compute_fid(mu_r, sigma_r, mu_g, sigma_g)
    print(f"{name:>12} {fid:10.4f}")
```

**å®šç¾© 3.12bï¼ˆKID: Kernel Inception Distanceï¼‰**

FID ã®ã‚¬ã‚¦ã‚¹ä»®å®šã‚’ç·©å’Œã—ãŸã€ã‚«ãƒ¼ãƒãƒ«ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆçš„è·é›¢ã€‚MMDï¼ˆMaximum Mean Discrepancyï¼‰ã‚’ Inception ç‰¹å¾´ç©ºé–“ã§è¨ˆç®—ã™ã‚‹:

$$\text{KID} = \text{MMD}^2_k(\{r_i\}, \{g_j\}) = \frac{1}{\binom{n}{2}}\sum_{i \neq j}k(r_i, r_j) + \frac{1}{\binom{m}{2}}\sum_{i \neq j}k(g_i, g_j) - \frac{2}{nm}\sum_{i,j}k(r_i, g_j)$$

FID ã¨ç•°ãªã‚Šä¸åæ¨å®šé‡ã§ã‚ã‚Šã€ã‚µãƒ³ãƒ—ãƒ«æ•°ã¸ã®ä¾å­˜ãŒå°ã•ã„ã€‚

**å®šç¾© 3.12cï¼ˆCMMDï¼‰** [^9]

Jayasumana+ (2024) ã¯ FID ã®å•é¡Œç‚¹ï¼ˆã‚¬ã‚¦ã‚¹ä»®å®šã€Inception-v3 ã®æ—§ã•ï¼‰ã‚’æŒ‡æ‘˜ã—ã€CLIP ç‰¹å¾´ç©ºé–“ã§ã® **Maximum Mean Discrepancy (MMD)** ã‚’ææ¡ˆã—ãŸ:

$$\text{CMMD}^2 = \frac{1}{n^2}\sum_{i,j}k(r_i, r_j) + \frac{1}{m^2}\sum_{i,j}k(g_i, g_j) - \frac{2}{nm}\sum_{i,j}k(r_i, g_j)$$

ã“ã“ã§ $k$ ã¯ã‚¬ã‚¦ã‚¹ RBF ã‚«ãƒ¼ãƒãƒ«ã€$r_i, g_j$ ã¯ CLIP ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã€‚

çµ±è¨ˆçš„è·é›¢ã®æ¯”è¼ƒ:

| ç‰¹æ€§ | FID [^4] | KID | CMMD [^9] |
|:-----|:---------|:--------|:----------|
| æ•°å­¦çš„åŸºç›¤ | $W_2$ è·é›¢ï¼ˆã‚¬ã‚¦ã‚¹è¿‘ä¼¼ï¼‰ | $\text{MMD}^2$ï¼ˆInceptionç©ºé–“ï¼‰ | $\text{MMD}^2$ï¼ˆCLIPç©ºé–“ï¼‰ |
| åˆ†å¸ƒä»®å®š | ã‚¬ã‚¦ã‚¹ | ãªã—ï¼ˆã‚«ãƒ¼ãƒãƒ«ï¼‰ | ãªã—ï¼ˆã‚«ãƒ¼ãƒãƒ«ï¼‰ |
| ãƒã‚¤ã‚¢ã‚¹ | ã‚ã‚Šï¼ˆ$N$ ã«ä¾å­˜ï¼‰ | **ä¸åæ¨å®šé‡** | **ä¸åæ¨å®šé‡** |
| äººé–“ã®åˆ¤æ–­ã¨ã®ç›¸é–¢ | ä¸­ç¨‹åº¦ | ä¸­ã€œé«˜ | **é«˜ã„** |
| è¨ˆç®—ã‚³ã‚¹ãƒˆ | $O(d^3)$ï¼ˆå…±åˆ†æ•£ã®å›ºæœ‰å€¤ï¼‰ | $O(N^2 d)$ | $O(N^2 d)$ |

### 3.13 LLM ã¨æœ€å°¤æ¨å®š â€” æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬

æœ¬è¬›ç¾©ã® LLM æ¥ç¶šã‚’æ˜ç¢ºã«ã—ã¦ãŠã“ã†ã€‚GPT ç³»ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¯**è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«**ã§ã‚ã‚Šã€MLE ã§è¨“ç·´ã•ã‚Œã‚‹ï¼ˆæ˜ç¤ºçš„æ¨å®šé‡ã®ä»£è¡¨ä¾‹ï¼‰ã€‚

$$p_\theta(x_1, x_2, \ldots, x_T) = \prod_{t=1}^{T} p_\theta(x_t | x_1, \ldots, x_{t-1})$$

è¨“ç·´ã®æå¤±é–¢æ•°:

$$\mathcal{L}(\theta) = -\frac{1}{T}\sum_{t=1}^{T} \log p_\theta(x_t | x_{<t})$$

ã“ã‚Œã¯**Cross-Entropy æå¤±**ãã®ã‚‚ã®ã§ã‚ã‚Šã€å®šç† 3.2 ã‹ã‚‰ MLE ã¨ç­‰ä¾¡ã€‚

```python
import numpy as np

# Simplified next-token prediction
vocab_size = 50000
sequence = [42, 1337, 7, 256, 99]  # token IDs

# Model output: logits â†’ softmax â†’ p(x_t | x_{<t})
def softmax(logits):
    exp_logits = np.exp(logits - np.max(logits))
    return exp_logits / exp_logits.sum()

def cross_entropy_loss(predictions, targets):
    """CE loss = -mean(log p(x_t | x_{<t}))"""
    total_loss = 0
    for pred_logits, target in zip(predictions, targets):
        probs = softmax(pred_logits)
        total_loss += -np.log(probs[target] + 1e-10)
    return total_loss / len(targets)

# Simulate model predictions (random logits)
np.random.seed(42)
predictions = [np.random.randn(vocab_size) for _ in range(len(sequence) - 1)]
targets = sequence[1:]  # next token at each position

loss = cross_entropy_loss(predictions, targets)
perplexity = np.exp(loss)

print(f"Sequence: {sequence}")
print(f"Cross-Entropy Loss: {loss:.4f}")
print(f"Perplexity: {perplexity:.2f}")
print(f"â†’ PPL = exp(CE) = 2^(CE/log2) = {2**(loss/np.log(2)):.2f}")
print(f"â†’ Random baseline PPL â‰ˆ vocab_size = {vocab_size}")
print(f"\nThis is EXACTLY what GPT training does:")
print(f"  minimize CE = maximize log-likelihood = minimize KL(p_data || q_Î¸)")
```

:::message
**é€²æ—: 50% å®Œäº†** â€” MLE ã®ç†è«–ã€æ¨å®šé‡ã®åˆ†é¡ä½“ç³»ã€è©•ä¾¡æŒ‡æ¨™ã®æ•°å­¦ã‚’æ”»ç•¥ã—ãŸã€‚ã“ã“ã‹ã‚‰å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã«å…¥ã‚‹ã€‚
:::

### 3.14 ãƒœã‚¹æˆ¦ â€” MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“

å…¨ã¦ã‚’çµ±åˆã™ã‚‹ã€‚

$$\underbrace{\hat{\theta}_\text{MLE}}_\text{MLE} = \arg\max_\theta \underbrace{\frac{1}{N}\sum_{i=1}^{N} \log q_\theta(x_i)}_\text{å¹³å‡å¯¾æ•°å°¤åº¦} = \arg\min_\theta \underbrace{H(\hat{p}, q_\theta)}_\text{Cross-Entropy} = \arg\min_\theta \underbrace{D_\text{KL}(\hat{p} \| q_\theta)}_\text{KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹}$$

å„é …ã®æ„å‘³:

| è¡¨ç¾ | è¦–ç‚¹ | ç›´æ„Ÿ |
|:-----|:-----|:-----|
| $\arg\max_\theta \frac{1}{N}\sum \log q_\theta(x_i)$ | **çµ±è¨ˆå­¦** | ãƒ‡ãƒ¼ã‚¿ã‚’æœ€ã‚‚ã€Œã‚‚ã£ã¨ã‚‚ã‚‰ã—ãã€èª¬æ˜ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| $\arg\min_\theta H(\hat{p}, q_\theta)$ | **æƒ…å ±ç†è«–** | ãƒ¢ãƒ‡ãƒ«ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç¬¦å·åŒ–ã™ã‚‹ã‚³ã‚¹ãƒˆã®æœ€å°åŒ– |
| $\arg\min_\theta D_\text{KL}(\hat{p} \| q_\theta)$ | **ç¢ºç‡è«–** | åˆ†å¸ƒé–“ã®æƒ…å ±æå¤±ã®æœ€å°åŒ– |

$$\boxed{\text{LLM è¨“ç·´} = \text{æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã® CE æœ€å°åŒ–} = \text{è¨€èªã® MLE} = \text{KL æœ€å°åŒ–}}$$

```python
import numpy as np

# Boss battle: verify the trinity numerically
np.random.seed(42)

# True distribution: N(3, 2Â²)
true_mu, true_sigma = 3.0, 2.0
N = 100000
data = np.random.normal(true_mu, true_sigma, N)

# Empirical entropy H(pÌ‚)
H_p = 0.5 * np.log(2 * np.pi * np.e * np.var(data))

# Scan Î¸ = (Î¼, Ïƒ=2 fixed)
mus = np.linspace(0, 6, 200)
results = {"mu": [], "avg_ll": [], "CE": [], "KL": []}

for mu in mus:
    sigma = 2.0
    # Average log-likelihood
    ll = np.mean(-0.5 * np.log(2 * np.pi * sigma**2) -
                  0.5 * ((data - mu) / sigma)**2)
    ce = -ll
    kl = ce - H_p

    results["mu"].append(mu)
    results["avg_ll"].append(ll)
    results["CE"].append(ce)
    results["KL"].append(kl)

# Find optima
i_max_ll = np.argmax(results["avg_ll"])
i_min_ce = np.argmin(results["CE"])
i_min_kl = np.argmin(results["KL"])

print("=== The Trinity ===")
print(f"argmax avg-log-likelihood: Î¼ = {results['mu'][i_max_ll]:.4f}")
print(f"argmin Cross-Entropy:      Î¼ = {results['mu'][i_min_ce]:.4f}")
print(f"argmin KL divergence:      Î¼ = {results['mu'][i_min_kl]:.4f}")
print(f"Sample mean (analytical):  Î¼ = {np.mean(data):.4f}")
print(f"\nAll identical: {i_max_ll == i_min_ce == i_min_kl}")
print(f"\nAt optimum:")
print(f"  Max avg log-lik:  {results['avg_ll'][i_max_ll]:.6f}")
print(f"  Min CE:           {results['CE'][i_min_ce]:.6f}")
print(f"  Min KL:           {results['KL'][i_min_kl]:.6f}")
print(f"  H(pÌ‚):            {H_p:.6f}")
print(f"  CE - H(pÌ‚) = KL:  {results['CE'][i_min_ce] - H_p:.6f} = {results['KL'][i_min_kl]:.6f}")
```

:::message
ãƒœã‚¹æ’ƒç ´ã€‚MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“ã‚’æ•°å€¤çš„ã«ç¢ºèªã—ãŸã€‚ã“ã®ç­‰ä¾¡æ€§ã¯ç¢ºç‡ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®å…¨ã¦ã«é€šåº•ã™ã‚‹åŸç†ã ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Fisher, R. A. (1922). "On the mathematical foundations of theoretical statistics." *Philosophical Transactions of the Royal Society of London, Series A*, 222, 309-368.
@[card](https://doi.org/10.1098/rsta.1922.0009)

[^2]: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. (2014). "Generative Adversarial Nets." *NeurIPS 2014*.
@[card](https://arxiv.org/abs/1406.2661)

[^3]: Kingma, D. P. & Welling, M. (2013). "Auto-Encoding Variational Bayes." *ICLR 2014*.
@[card](https://arxiv.org/abs/1312.6114)

[^4]: Heusel, M., Ramsauer, H., Unterthiner, T., et al. (2017). "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium." *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1706.08500)

[^5]: Ho, J., Jain, A. & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS 2020*.
@[card](https://arxiv.org/abs/2006.11239)

[^6]: Mohamed, S. & Lakshminarayanan, B. (2016). "Learning in Implicit Generative Models." *arXiv:1610.03483*.
@[card](https://arxiv.org/abs/1610.03483)

[^7]: Rezende, D. J. & Mohamed, S. (2015). "Variational Inference with Normalizing Flows." *ICML 2015*.
@[card](https://arxiv.org/abs/1505.05770)

[^8]: Salimans, T., Goodfellow, I., Zaremba, W., et al. (2016). "Improved Techniques for Training GANs." *NeurIPS 2016*.
@[card](https://arxiv.org/abs/1606.03498)

[^9]: Jayasumana, S., Ramalingam, S., Veit, A., et al. (2024). "Rethinking FID: Towards a Better Evaluation Metric for Image Generation." *CVPR 2024*.
@[card](https://arxiv.org/abs/2401.09603)

[^10]: Song, Y. & Ermon, S. (2019). "Generative Modeling by Estimating Gradients of the Data Distribution." *NeurIPS 2019*.
@[card](https://arxiv.org/abs/1907.05600)

[^11]: Dinh, L., Krueger, D. & Bengio, Y. (2014). "NICE: Non-linear Independent Components Estimation." *ICLR 2015 Workshop*.
@[card](https://arxiv.org/abs/1410.8516)

[^12]: Dinh, L., Sohl-Dickstein, J. & Bengio, S. (2016). "Density estimation using Real NVP." *ICLR 2017*.
@[card](https://arxiv.org/abs/1605.08803)

[^13]: Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N. & Ganguli, S. (2015). "Deep Unsupervised Learning using Nonequilibrium Thermodynamics." *ICML 2015*.
@[card](https://arxiv.org/abs/1503.03585)

[^14]: CramÃ©r, H. (1946). *Mathematical Methods of Statistics*. Princeton University Press.

[^15]: Rao, C. R. (1945). "Information and the accuracy attainable in the estimation of statistical parameters." *Bulletin of the Calcutta Mathematical Society*, 37, 81-91.

### æ•™ç§‘æ›¸

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Goodfellow, I., Bengio, Y. & Courville, A. (2016). *Deep Learning*. MIT Press. [Free: deeplearningbook.org]
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Free: probml.github.io]
- Cover, T. M. & Thomas, J. A. (2006). *Elements of Information Theory*. 2nd ed. Wiley.

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | èª­ã¿æ–¹ | æ„å‘³ | åˆå‡º |
|:-----|:-------|:-----|:-----|
| $\hat{\theta}_\text{MLE}$ | ã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆ ã‚¨ãƒ ã‚¨ãƒ«ã‚¤ãƒ¼ | æœ€å°¤æ¨å®šé‡ | å®šç¾© 3.1 |
| $q_\theta(x)$ | ã‚­ãƒ¥ãƒ¼ ã‚·ãƒ¼ã‚¿ ã‚¨ãƒƒã‚¯ã‚¹ | ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®å¯†åº¦ | å®šç¾© 3.1 |
| $p_\text{data}(x)$ | ãƒ”ãƒ¼ ãƒ‡ãƒ¼ã‚¿ | ãƒ‡ãƒ¼ã‚¿ã®çœŸã®åˆ†å¸ƒ | Zone 0 |
| $\hat{p}(x)$ | ãƒ”ãƒ¼ãƒãƒƒãƒˆ | çµŒé¨“åˆ†å¸ƒ | å®šç† 3.2 |
| $H(\hat{p}, q_\theta)$ | ã‚¨ã‚¤ãƒ | Cross-Entropy | å®šç† 3.2 |
| $D_\text{KL}(\hat{p} \| q_\theta)$ | ã‚±ãƒ¼ã‚¨ãƒ« | KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç† 3.3 |
| $\mathcal{I}(\theta)$ | ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼ ã‚¢ã‚¤ | Fisher æƒ…å ±è¡Œåˆ— | æ€§è³ª 3.4b |
| $G_\theta(z)$ | ã‚¸ãƒ¼ ã‚·ãƒ¼ã‚¿ | GAN ã®ç”Ÿæˆå™¨ | å®šç¾© 3.8 |
| $D_\phi(x)$ | ãƒ‡ã‚£ãƒ¼ ãƒ•ã‚¡ã‚¤ | GAN ã®åˆ¤åˆ¥å™¨ | å®šç¾© 3.8 |
| $D_\text{JS}$ | ã‚¸ã‚§ãƒ¼ã‚¨ã‚¹ | Jensen-Shannon ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç† 3.8b |
| $s_\theta(x)$ | ã‚¨ã‚¹ ã‚·ãƒ¼ã‚¿ | ã‚¹ã‚³ã‚¢é–¢æ•°ã®æ¨å®š | å®šç¾© 3.9 |
| $\nabla_x \log p(x)$ | ãƒŠãƒ–ãƒ© ã‚¨ãƒƒã‚¯ã‚¹ | ã‚¹ã‚³ã‚¢é–¢æ•°ï¼ˆçœŸï¼‰ | å®šç¾© 3.9 |
| $\epsilon_\theta(x_t, t)$ | ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ ã‚·ãƒ¼ã‚¿ | DDPM ã®ãƒã‚¤ã‚ºäºˆæ¸¬å™¨ | 3.9 |
| $\text{FID}$ | ã‚¨ãƒ•ã‚¢ã‚¤ãƒ‡ã‚£ãƒ¼ | Frechet Inception Distance | å®šç¾© 3.12a |
| $\text{IS}$ | ã‚¢ã‚¤ã‚¨ã‚¹ | Inception Score | å®šç¾© 3.12b |
| $\text{CMMD}$ | ã‚·ãƒ¼ã‚¨ãƒ ã‚¨ãƒ ãƒ‡ã‚£ãƒ¼ | CLIP MMD | å®šç¾© 3.12c |
| $f^{-1}$ | ã‚¨ãƒ• ã‚¤ãƒ³ãƒãƒ¼ã‚¹ | Flow ã®é€†å¤‰æ› | å®šç† 3.7 |
| $\det J$ | ãƒ‡ãƒƒãƒˆ ã‚¸ã‚§ãƒ¼ | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ | å®šç† 3.7 |
| $p(z)$ | ãƒ”ãƒ¼ ã‚¼ãƒƒãƒˆ | æ½œåœ¨ç©ºé–“ã®äº‹å‰åˆ†å¸ƒ | 3.6 |
| $x_t$ | ã‚¨ãƒƒã‚¯ã‚¹ ãƒ†ã‚£ãƒ¼ | æ‹¡æ•£éç¨‹ã®æ™‚åˆ» $t$ ã®çŠ¶æ…‹ | 3.9 |
| $\text{ELBO}$ | ã‚¨ãƒ«ãƒœ | å¤‰åˆ†ä¸‹ç•Œï¼ˆç¬¬8å›ã§å°å‡ºï¼‰ | 3.5 |
| $\pi_k$ | ãƒ‘ã‚¤ ã‚±ãƒ¼ | æ··åˆä¿‚æ•°ï¼ˆGMMï¼‰ | 4.1 |
| $\gamma_{nk}$ | ã‚¬ãƒ³ãƒ | è²¬ä»»åº¦ï¼ˆEM ã® E-stepï¼‰ | 4.1 |
| $G_{\theta\#}\mu$ | ãƒ—ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ | Pushforward æ¸¬åº¦ | 2.7 |
| $\mathcal{M}$ | ã‚¨ãƒ  | ãƒ‡ãƒ¼ã‚¿å¤šæ§˜ä½“ | 2.5 |
| $D^*_G(x)$ | ãƒ‡ã‚£ãƒ¼ã‚¹ã‚¿ãƒ¼ | GAN ã®æœ€é©åˆ¤åˆ¥å™¨ | å®šç† 3.8a |

---

## å®Ÿè·µãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

:::details æ¨å®šé‡é¸æŠãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆï¼ˆå°åˆ·ç”¨ï¼‰

**å•é¡Œåˆ¥æ¨å®šé‡é¸æŠã‚¬ã‚¤ãƒ‰**

| æ¨å®šã®ç›®çš„ | ç¬¬ä¸€é¸æŠ | ç¬¬äºŒé¸æŠ | ç†ç”± |
|:-----|:---------|:---------|:-----|
| é«˜å“è³ªå¯†åº¦æ¨å®š | ã‚¹ã‚³ã‚¢æ¨å®šé‡ï¼ˆDiffusionï¼‰ | æš—é»™çš„æ¨å®šï¼ˆGANï¼‰ | æ¨å®šç²¾åº¦ + å®‰å®šæ€§ |
| é›¢æ•£ç³»åˆ—æ¨å®š | è‡ªå·±å›å¸°MLEï¼ˆGPTï¼‰ | - | é›¢æ•£ãƒ‡ãƒ¼ã‚¿ã«æœ€é© |
| æ½œåœ¨è¡¨ç¾å­¦ç¿’ | å¤‰åˆ†MLEï¼ˆVAEï¼‰ | å¤‰æ•°å¤‰æ›MLEï¼ˆFlowï¼‰ | æ»‘ã‚‰ã‹ãªæ½œåœ¨ç©ºé–“ |
| ç•°å¸¸æ¤œçŸ¥ | Flow / VAE | - | å°¤åº¦è¨ˆç®—ãŒå¿…è¦ |
| æ­£ç¢ºãªå¯†åº¦æ¨å®š | å¤‰æ•°å¤‰æ›MLEï¼ˆFlowï¼‰ | è‡ªå·±å›å¸°MLE | æ­£ç¢ºãªå°¤åº¦ |
| é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | æš—é»™çš„æ¨å®š / å¤‰åˆ†MLE | Consistency Model | 1-shotç”Ÿæˆ |
| æ¡ä»¶ä»˜ãæ¨å®š | ã‚¹ã‚³ã‚¢æ¨å®šé‡ | æš—é»™çš„æ¨å®š | Classifier-free guidance |
| æ™‚ç³»åˆ—æ¨å®š | ã‚¹ã‚³ã‚¢æ¨å®šé‡ | - | æ™‚é–“æ•´åˆæ€§ |

**MLE ã®å…¬å¼é›†**

$$\hat{\theta}_\text{MLE} = \arg\max_\theta \frac{1}{N}\sum_{i=1}^{N} \log q_\theta(x_i) = \arg\min_\theta H(\hat{p}, q_\theta) = \arg\min_\theta D_\text{KL}(\hat{p} \| q_\theta)$$

**ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã® MLEï¼ˆè¦šãˆã‚‹ã¹ãï¼‰:**

$$\hat{\mu} = \frac{1}{N}\sum_{i=1}^{N} x_i, \quad \hat{\sigma}^2 = \frac{1}{N}\sum_{i=1}^{N}(x_i - \hat{\mu})^2$$

**MLEã®4å¤‰å½¢ã®æå¤±é–¢æ•°**

```
VAE:       L = -E_q[log p(x|z)] + KL[q(z|x) || p(z)]
GAN:       L_D = -E[log D(x)] - E[log(1-D(G(z)))]
           L_G = -E[log D(G(z))]
Flow:      L = -E[log p(fâ»Â¹(x)) + log|det(âˆ‚fâ»Â¹/âˆ‚x)|]
Diffusion: L = E[||Îµ - Îµ_Î¸(âˆšá¾±â‚œxâ‚€ + âˆš(1-á¾±â‚œ)Îµ, t)||Â²]
```

**çµ±è¨ˆçš„è·é›¢ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼**

```python
# FID: Wâ‚‚ distance with Gaussian approximation
FID = np.dot(mu_r-mu_g, mu_r-mu_g) + np.trace(sigma_r + sigma_g - 2*sqrtm(sigma_r@sigma_g))

# CMMD: MMD in CLIP space
CMMD2 = mean_k(r,r) + mean_k(g,g) - 2*mean_k(r,g)  # k = RBF kernel

# Perplexity: exponentiated cross-entropy
PPL = np.exp(cross_entropy_loss)
```

**é‡è¦ãªç­‰ä¾¡é–¢ä¿‚**

```
MLE â‰¡ Cross-Entropyæœ€å°åŒ– â‰¡ KLæœ€å°åŒ– â‰¡ å‰å‘ãKLæœ€å°åŒ–
GAN â‰¡ JSDæœ€å°åŒ– â‰¡ å¯†åº¦æ¯”æ¨å®š
LLMè¨“ç·´ â‰¡ è‡ªå·±å›å¸°MLE â‰¡ æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³CEæœ€å°åŒ–
Score Matching â‰¡ Denoising â‰¡ Diffusion (ç°¡æ˜“ç‰ˆ)
MAP â‰¡ MLE + L2æ­£å‰‡åŒ– (ã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒã®å ´åˆ)
```

**Mode-Covering vs Mode-Seeking è¦šãˆæ–¹**

```
Forward KL: D(p_data || q_model)
  â†’ q must cover where p > 0
  â†’ "Cover all modes" â†’ blurry but complete
  â†’ Used by: MLE, VAE

Reverse KL: D(q_model || p_data)
  â†’ q must stay where p > 0
  â†’ "Seek one mode" â†’ sharp but incomplete
  â†’ Used by: GAN (approximately via JSD)
```
:::

:::details çµ±è¨ˆçš„æ¨å®šã®å¹´ä»£è¨˜ï¼ˆè¦šãˆã‚‹ã¹ãè«–æ–‡ Top 13ï¼‰

| å¹´ | è«–æ–‡ | è²¢çŒ® | arXiv |
|:---|:-----|:-----|:------|
| 1922 | Fisher | MLE ã®ä½“ç³»åŒ– | - |
| 2013 | Kingma & Welling | VAE | 1312.6114 |
| 2014 | Goodfellow+ | GAN | 1406.2661 |
| 2014 | Dinh+ | NICE (Flow ã®å§‹ç¥–) | 1410.8516 |
| 2015 | Sohl-Dickstein+ | Diffusion ã®ç€æƒ³ | 1503.03585 |
| 2015 | Rezende & Mohamed | Normalizing Flows | 1505.05770 |
| 2016 | Salimans+ | Inception Score | 1606.03498 |
| 2016 | Dinh+ | Real NVP | 1605.08803 |
| 2016 | Mohamed+ | Prescribed vs Implicit | 1610.03483 |
| 2017 | Heusel+ | FID | 1706.08500 |
| 2019 | Song & Ermon | Score Matching ç”Ÿæˆ | 1907.05600 |
| 2020 | Ho+ | DDPM | 2006.11239 |
| 2024 | Jayasumana+ | CMMD (FID æ”¹å–„) | 2401.09603 |
:::

:::details æ¨å®šé‡ã®æ•°å­¦çš„å‰ææ¡ä»¶ãƒãƒƒãƒ—

```
ç¬¬2å› ç·šå½¢ä»£æ•°
  â”œâ”€â”€ å›ºæœ‰å€¤åˆ†è§£ â†’ FID ã®è¡Œåˆ—å¹³æ–¹æ ¹
  â”œâ”€â”€ è¡Œåˆ—å¼ â†’ Flow ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³
  â””â”€â”€ å†…ç©ç©ºé–“ â†’ Fisher æƒ…å ±è¡Œåˆ—

ç¬¬3å› å¾®åˆ†ç©åˆ†
  â”œâ”€â”€ åå¾®åˆ† â†’ MLE ã®å‹¾é…
  â”œâ”€â”€ ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ â†’ å¤‰æ•°å¤‰æ›å…¬å¼
  â””â”€â”€ é€£é–å¾‹ â†’ Backpropagation

ç¬¬4å› ç¢ºç‡çµ±è¨ˆ
  â”œâ”€â”€ ç¢ºç‡åˆ†å¸ƒ â†’ å¯†åº¦æ¨å®šã®å®šç¾©
  â”œâ”€â”€ ãƒ™ã‚¤ã‚ºã®å®šç† â†’ äº‹å¾Œæ¨è«–
  â””â”€â”€ æ¡ä»¶ä»˜ãç¢ºç‡ â†’ è‡ªå·±å›å¸°åˆ†è§£

ç¬¬5å› æ¸¬åº¦è«–
  â”œâ”€â”€ Lebesgue ç©åˆ† â†’ æœŸå¾…å€¤ã®å³å¯†å®šç¾©
  â”œâ”€â”€ Radon-Nikodym â†’ å¯†åº¦æ¯”æ¨å®š
  â””â”€â”€ Pushforward æ¸¬åº¦ â†’ GAN ã®ç”Ÿæˆå™¨

ç¬¬6å› æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–
  â”œâ”€â”€ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ â†’ MLE ç­‰ä¾¡æ€§
  â”œâ”€â”€ Cross-Entropy â†’ æå¤±é–¢æ•°
  â”œâ”€â”€ Adam â†’ è¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
  â””â”€â”€ Jensen ä¸ç­‰å¼ â†’ ELBO (ç¬¬8å›)

ç¬¬7å› æœ¬è¬›ç¾©
  â”œâ”€â”€ MLE â†’ å…¨æ¨å®šé‡ã®åŸºç›¤
  â”œâ”€â”€ åˆ†é¡ä½“ç³» â†’ ãƒ¢ãƒ‡ãƒ«é¸æŠã®æŒ‡é‡
  â””â”€â”€ è©•ä¾¡æŒ‡æ¨™ â†’ å“è³ªæ¸¬å®š
```
:::

:::details æ•°å€¤ã®ç›´æ„Ÿï¼ˆè¦šãˆã¦ãŠãã¨ä¾¿åˆ©ï¼‰

| é‡ | å…¸å‹å€¤ | æ„å‘³ |
|:---|:-------|:-----|
| CIFAR-10 FID (DDPM) | 3.17 | ç”»åƒç”Ÿæˆã® SOTA ãƒ¬ãƒ™ãƒ« |
| ImageNet FID (Diffusion) | ~2-5 | å¤§è¦æ¨¡ç”»åƒç”Ÿæˆ |
| GPT-4 Perplexity | ~10-20 (æ¨å®š) | éå¸¸ã«è‰¯ã„è¨€èªãƒ¢ãƒ‡ãƒ« |
| Random baseline PPL | vocab_size (~50K) | å­¦ç¿’å‰ã®çŠ¶æ…‹ |
| é¡”ç”»åƒã®å†…åœ¨æ¬¡å…ƒ | ~100 | 12,288æ¬¡å…ƒä¸­ |
| MNIST ã®å†…åœ¨æ¬¡å…ƒ | ~10-15 | 784æ¬¡å…ƒä¸­ |
| IS (CIFAR-10, æœ€è‰¯) | ~9.5 | æœ€å¤§å€¤ã¯10ï¼ˆã‚¯ãƒ©ã‚¹æ•°ï¼‰ |
| ã‚¬ã‚¦ã‚¹ MLE ã®åæŸ | $O(1/\sqrt{N})$ | Fisher æƒ…å ±ã‹ã‚‰ |
:::

---

## è£œéº â€” Fisher æƒ…å ±ã¨ CramÃ©r-Rao é™ç•Œã®ç¾ä»£çš„å¿œç”¨

:::message
**çµ±è¨ˆçš„æ¨è«–ã®åŸºç¤å®šç†**: CramÃ©r-Rao é™ç•Œã¯ã€ã‚ã‚‰ã‚†ã‚‹ä¸åæ¨å®šé‡ã®åˆ†æ•£ã®ä¸‹é™ã‚’ä¸ãˆã‚‹ã€‚Fisher æƒ…å ±è¡Œåˆ—ã¯ã“ã®é™ç•Œã‚’å®šé‡åŒ–ã—ã€æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹æœ€é©åŒ–ãƒ»ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ãƒ»ãƒ¢ãƒ‡ãƒ«é¸æŠã®ç†è«–çš„åŸºç›¤ã¨ãªã‚‹ã€‚
:::

### Fisher æƒ…å ±è¡Œåˆ—ã®å®šç¾©ã¨è§£é‡ˆ

**å®šç¾©**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta \in \mathbb{R}^d$ ã‚’æŒã¤çµ±è¨ˆãƒ¢ãƒ‡ãƒ« $p(x;\theta)$ ã«å¯¾ã—ã€**Fisher æƒ…å ±è¡Œåˆ—** $\mathcal{I}(\theta)$ ã¯:

$$
\mathcal{I}(\theta)_{ij} = \mathbb{E}_{p(x;\theta)} \left[ \left( \frac{\partial \log p(x;\theta)}{\partial \theta_i} \right) \left( \frac{\partial \log p(x;\theta)}{\partial \theta_j} \right) \right]
$$

ã¾ãŸã¯åŒå€¤çš„ã«ï¼ˆæ­£å‰‡æ¡ä»¶ä¸‹ï¼‰:

$$
\mathcal{I}(\theta)_{ij} = -\mathbb{E}_{p(x;\theta)} \left[ \frac{\partial^2 \log p(x;\theta)}{\partial \theta_i \partial \theta_j} \right]
$$

**å¹¾ä½•å­¦çš„è§£é‡ˆ**: $\mathcal{I}(\theta)$ ã¯çµ±è¨ˆå¤šæ§˜ä½“ä¸Šã® Riemann è¨ˆé‡ã‚’å®šç¾©ã—ã€KL divergence ã®å±€æ‰€çš„ãª2æ¬¡è¿‘ä¼¼ã‚’ä¸ãˆã‚‹:

$$
D_{\text{KL}}(p(\cdot;\theta) \| p(\cdot;\theta + d\theta)) \approx \frac{1}{2} d\theta^\top \mathcal{I}(\theta) d\theta
$$

### CramÃ©r-Rao ä¸‹é™ã®å®šç†

**å®šç†** (CramÃ©r-Rao): $\hat{\theta}(x)$ ã‚’ $\theta$ ã®ä»»æ„ã®ä¸åæ¨å®šé‡ã¨ã™ã‚‹ã€‚ã“ã®ã¨ã:

$$
\text{Var}(\hat{\theta}) \geq \mathcal{I}(\theta)^{-1}
$$

ã“ã“ã§ä¸ç­‰å¼ã¯åŠæ­£å®šå€¤ã®æ„å‘³ã§æˆç«‹ï¼ˆ$A \geq B \Leftrightarrow A - B$ ãŒåŠæ­£å®šå€¤ï¼‰ã€‚

**ç³»**: ã‚¹ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å ´åˆ:

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{\mathcal{I}(\theta)}
$$

**è¨¼æ˜ã‚¹ã‚±ãƒƒãƒ**: Cauchy-Schwarz ä¸ç­‰å¼ã‚’ $\mathbb{E}[\hat{\theta} \cdot s]$ï¼ˆã“ã“ã§ $s = \partial \log p / \partial \theta$ ã¯ score functionï¼‰ã«é©ç”¨ã€‚

### æœ€å°¤æ¨å®šé‡ã®æ¼¸è¿‘çš„æ€§è³ª

**å®šç†** (MLE ã®æ¼¸è¿‘æ­£è¦æ€§): æ­£å‰‡æ¡ä»¶ä¸‹ã§ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º $N$ ã«å¯¾ã™ã‚‹MLE $\hat{\theta}_{\text{MLE}}$ ã¯:

$$
\sqrt{N}(\hat{\theta}_{\text{MLE}} - \theta_0) \xrightarrow{d} \mathcal{N}(0, \mathcal{I}(\theta_0)^{-1})
$$

**å«æ„**:
1. MLE ã¯æ¼¸è¿‘çš„ã«ä¸å
2. MLE ã¯ CramÃ©r-Rao ä¸‹é™ã‚’æ¼¸è¿‘çš„ã«é”æˆï¼ˆ**æ¼¸è¿‘æœ‰åŠ¹æ€§**ï¼‰
3. åæŸé€Ÿåº¦ã¯ $O(1/\sqrt{N})$

### å®Ÿç”¨çš„å¿œç”¨1: ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–

MLE $\hat{\theta}$ ã®ä¿¡é ¼åŒºé–“ã‚’ Fisher æƒ…å ±ã‹ã‚‰æ§‹ç¯‰:

$$
\hat{\theta} \pm z_{\alpha/2} \sqrt{\frac{1}{N \mathcal{I}(\hat{\theta})}}
$$

ã“ã“ã§ $z_{\alpha/2}$ ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒã® $(1-\alpha/2)$ åˆ†ä½ç‚¹ã€‚

**ä¾‹**: æ·±å±¤å­¦ç¿’ã®é‡ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸ç¢ºå®Ÿæ€§æ¨å®š:

```python
def fisher_information_uncertainty(model, data_loader, param_idx):
    """Fisheræƒ…å ±ã«åŸºã¥ãä¸ç¢ºå®Ÿæ€§æ¨å®š"""
    fisher_diag = torch.zeros_like(model.parameters()[param_idx])

    for x, y in data_loader:
        log_prob = model.log_likelihood(x, y)
        grad = torch.autograd.grad(log_prob, model.parameters()[param_idx],
                                    create_graph=True)[0]
        fisher_diag += grad ** 2

    fisher_diag /= len(data_loader.dataset)
    std_error = 1.0 / torch.sqrt(fisher_diag)
    return std_error
```

### å®Ÿç”¨çš„å¿œç”¨2: è‡ªç„¶å‹¾é…æ³•

**è‡ªç„¶å‹¾é…**: Fisher æƒ…å ±ã‚’ç”¨ã„ãŸå‹¾é…ã®å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°:

$$
\theta_{t+1} = \theta_t + \alpha \mathcal{I}(\theta_t)^{-1} \nabla_\theta \mathcal{L}(\theta_t)
$$

**å‹•æ©Ÿ**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã§ã® Euclid è·é›¢ã§ã¯ãªãã€çµ±è¨ˆçš„è·é›¢ï¼ˆKL divergenceï¼‰ã§æœ€æ€¥é™ä¸‹ã€‚

**å®Ÿè£…**ï¼ˆK-FAC è¿‘ä¼¼ï¼‰:

$$
\mathcal{I}(\theta) \approx \text{BlockDiag}(A_1 \otimes G_1, \ldots, A_L \otimes G_L)
$$

ã“ã“ã§ $A_\ell$ ã¯å±¤ $\ell$ ã®å…¥åŠ›ã®2æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã€$G_\ell$ ã¯å‹¾é…ã®2æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã€‚

### å®Ÿç”¨çš„å¿œç”¨3: ãƒ¢ãƒ‡ãƒ«é¸æŠã¨ FIM

**FIM ãƒˆãƒ¬ãƒ¼ã‚¹**: ãƒ¢ãƒ‡ãƒ«ã®ã€Œè¤‡é›‘ã•ã€ã®å°ºåº¦:

$$
\text{Complexity} = \text{tr}(\mathcal{I}(\theta))
$$

**BIC (Bayesian Information Criterion)** ã®å°å‡ºã§ Fisher æƒ…å ±ãŒç¾ã‚Œã‚‹:

$$
\text{BIC} = -2 \log \mathcal{L}(\hat{\theta}) + k \log N
$$

ã“ã“ã§ $k = \text{rank}(\mathcal{I}(\hat{\theta}))$ ã¯æœ‰åŠ¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€‚

### ä¾‹: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã® Fisher æƒ…å ±

$$
p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right)
$$

Fisher æƒ…å ±è¡Œåˆ—:

$$
\mathcal{I}(\mu, \sigma^2) = \begin{bmatrix}
\frac{1}{\sigma^2} & 0 \\
0 & \frac{1}{2\sigma^4}
\end{bmatrix}
$$

**CramÃ©r-Rao é™ç•Œ**:

$$
\text{Var}(\hat{\mu}) \geq \frac{\sigma^2}{N}, \quad \text{Var}(\hat{\sigma}^2) \geq \frac{2\sigma^4}{N}
$$

ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã¨ã‚µãƒ³ãƒ—ãƒ«åˆ†æ•£ã¯ã€ã“ã‚Œã‚‰ã®é™ç•Œã‚’ï¼ˆæ¼¸è¿‘çš„ã«ï¼‰é”æˆã€‚

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
