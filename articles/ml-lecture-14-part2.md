---
title: "ç¬¬14å›: Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ”"
type: "tech"
topics: ["machinelearning", "deeplearning", "transformer", "rust", "rust"]
published: true
slug: "ml-lecture-14-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rustå®Œå…¨å®Ÿè£… + Rustæ¨è«–

### 4.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

**Rust** (1.11+):

```bash
# Install Rust via rustup
curl -fsSL https://install.julialang.org | sh
julia --version  # 1.11.x or later
```

```rust
// Cargo.toml dependencies:
// [dependencies]
// candle-core = "0.8"
// candle-nn = "0.8"
// ndarray = "0.16"
// rand = "0.8"
// rand_distr = "0.4"
```

**Rust** (1.85+):

```bash
# Install Rust
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
cargo --version  # 1.85.x or later

# Create project
cargo new attention_demo
cd attention_demo
```

`Cargo.toml`:
```toml
[dependencies]
ndarray = "0.16"
```

### 4.2 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆLaTeXâ†”Rust 7ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

| æ•°å¼ | Rustå®Ÿè£… | ãƒ‘ã‚¿ãƒ¼ãƒ³ |
|:-----|:----------|:---------|
| $Y = WX + b$ | `Y = W * X .+ b` | è¡Œåˆ—ç© + ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆåŠ ç®— |
| $S = \frac{QK^\top}{\sqrt{d_k}}$ | `S = (Q * K') / sqrt(d_k)` | è¡Œåˆ—ç© + ã‚¹ã‚«ãƒ©ãƒ¼é™¤ç®— |
| $A = \text{softmax}(S)$ | `A = softmax(S, dims=2)` | è¡Œã”ã¨Softmax |
| $O = AV$ | `O = A * V` | è¡Œåˆ—ç© |
| $Z = X + F(X)$ | `Z = X .+ F(X)` | Residual (ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆåŠ ç®—) |
| $\mu = \frac{1}{d}\sum_i x_i$ | `Î¼ = mean(X, dims=2)` | è¡Œã”ã¨å¹³å‡ |
| $\tilde{X} = \frac{X - \mu}{\sigma}$ | `X_norm = (X .- Î¼) ./ (Ïƒ .+ 1e-5)` | æ­£è¦åŒ–ï¼ˆãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆï¼‰ |

**Rustã® `.` (broadcast)**: è¦ç´ ã”ã¨æ¼”ç®—ã‚’è‡ªå‹•ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã€‚

### 4.3 Micro-GPT (Tiny Transformer) Rustå®Œå…¨å®Ÿè£…

**ç›®æ¨™**: GPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒŸãƒ‹ãƒãƒ«ç‰ˆï¼ˆ1å±¤ã€2 headsã€d_model=32ï¼‰ã‚’è¨“ç·´å¯èƒ½ãªå½¢ã§å®Ÿè£…ã™ã‚‹ã€‚

```rust
use candle_core::{DType, Device, Result, Tensor, D};
use candle_nn::{embedding, linear_no_bias, layer_norm, Embedding, Linear, LayerNorm,
                Module, VarBuilder, VarMap};

// Attn(Q,K,V) = softmax(QKáµ€ / âˆšd_k) Â· V
// Q, K, V: (batch * num_heads, seq_len, d_k)
fn scaled_dot_product_attention(
    q: &Tensor, k: &Tensor, v: &Tensor,
    mask: Option<&Tensor>,
) -> Result<(Tensor, Tensor)> {
    let d_k = q.dim(D::Minus1)? as f64;
    // scores = QKáµ€ / âˆšd_k: (batch*heads, seq, seq)
    let mut scores = q.matmul(&k.transpose(D::Minus2, D::Minus1)?)? / d_k.sqrt();
    if let Some(m) = mask {
        scores = scores.broadcast_add(m)?;  // add causal mask (-âˆ for future)
    }
    let attn_weights = candle_nn::ops::softmax(&scores, D::Minus1)?;  // softmax(Â·)
    let output = attn_weights.matmul(v)?;                              // Â· V
    Ok((output, attn_weights))
}

// --- Multi-Head Attention ---
struct MultiHeadAttention {
    w_q: Linear, w_k: Linear, w_v: Linear, w_o: Linear,
    num_heads: usize,
    d_k:       usize,
}

impl MultiHeadAttention {
    fn new(d_model: usize, num_heads: usize, vb: VarBuilder) -> Result<Self> {
        let d_k = d_model / num_heads;
        Ok(Self {
            w_q: linear_no_bias(d_model, d_model, vb.pp("w_q"))?,
            w_k: linear_no_bias(d_model, d_model, vb.pp("w_k"))?,
            w_v: linear_no_bias(d_model, d_model, vb.pp("w_v"))?,
            w_o: linear_no_bias(d_model, d_model, vb.pp("w_o"))?,
            num_heads, d_k,
        })
    }

    fn forward(&self, x: &Tensor, mask: Option<&Tensor>) -> Result<Tensor> {
        let (batch, seq_len, d_model) = x.dims3()?;
        let h = self.num_heads;

        // Project â†’ split heads: (batch, seq, d_model) â†’ (batch*heads, seq, d_k)
        let project = |w: &Linear| -> Result<Tensor> {
            w.forward(x)?
                .reshape((batch, seq_len, h, self.d_k))?
                .transpose(1, 2)?                         // (batch, h, seq, d_k)
                .reshape((batch * h, seq_len, self.d_k))
        };
        let q = project(&self.w_q)?;
        let k = project(&self.w_k)?;
        let v = project(&self.w_v)?;

        // Attention
        let (attn_out, _) = scaled_dot_product_attention(&q, &k, &v, mask)?;

        // Merge heads â†’ output projection
        let merged = attn_out
            .reshape((batch, h, seq_len, self.d_k))?
            .transpose(1, 2)?                              // (batch, seq, h, d_k)
            .reshape((batch, seq_len, d_model))?;
        self.w_o.forward(&merged)
    }
}

// --- Transformer Block (Pre-LN) ---
struct TransformerBlock {
    mha: MultiHeadAttention,
    ffn1: Linear,
    ffn2: Linear,
    ln1:  LayerNorm,
    ln2:  LayerNorm,
}

impl TransformerBlock {
    fn new(d_model: usize, num_heads: usize, d_ff: usize, vb: VarBuilder) -> Result<Self> {
        Ok(Self {
            mha:  MultiHeadAttention::new(d_model, num_heads, vb.pp("mha"))?,
            ffn1: candle_nn::linear(d_model, d_ff, vb.pp("ffn1"))?,
            ffn2: candle_nn::linear(d_ff, d_model, vb.pp("ffn2"))?,
            ln1:  layer_norm(d_model, 1e-5, vb.pp("ln1"))?,
            ln2:  layer_norm(d_model, 1e-5, vb.pp("ln2"))?,
        })
    }

    fn forward(&self, x: &Tensor, mask: Option<&Tensor>) -> Result<Tensor> {
        // x' = x + MHA(LN(x))  â€” Pre-LN: normalize before attention, residual after
        let x = (x + &self.mha.forward(&self.ln1.forward(x)?, mask)?)?;
        // FFN(x) = GELU(xWâ‚)Wâ‚‚,  x'' = x' + FFN(LN(x'))
        let ffn_out = self.ffn2.forward(
            &self.ffn1.forward(&self.ln2.forward(&x)?)?.gelu()?
        )?;
        x + ffn_out  // residual connection
    }
}

// M_{ij} = 0 if j â‰¤ i, else -âˆ  â€” additive causal mask; blocks future positions in softmax
fn causal_mask(seq_len: usize, device: &Device) -> Result<Tensor> {
    let mask: Vec<f32> = (0..seq_len).flat_map(|i|
        (0..seq_len).map(move |j| if j <= i { 0.0f32 } else { f32::NEG_INFINITY })
    ).collect();
    Tensor::from_vec(mask, (1, 1, seq_len, seq_len), device)
}

// --- Micro-GPT ---
struct MicroGPT {
    token_emb:   Embedding,
    pos_emb:     Embedding,
    transformer: TransformerBlock,
    lm_head:     Linear,
}

impl MicroGPT {
    fn new(vocab_size: usize, d_model: usize, num_heads: usize,
           d_ff: usize, max_len: usize, vb: VarBuilder) -> Result<Self> {
        Ok(Self {
            token_emb:   embedding(vocab_size, d_model, vb.pp("tok_emb"))?,
            pos_emb:     embedding(max_len, d_model, vb.pp("pos_emb"))?,
            transformer: TransformerBlock::new(d_model, num_heads, d_ff, vb.pp("transformer"))?,
            lm_head:     linear_no_bias(d_model, vocab_size, vb.pp("lm_head"))?,
        })
    }

    fn forward(&self, input_ids: &Tensor) -> Result<Tensor> {
        let (batch, seq_len) = input_ids.dims2()?;
        // x = E_tok(ids) + E_pos(0..T)  â€” token + positional embeddings
        let positions = Tensor::arange(0u32, seq_len as u32, input_ids.device())?
            .unsqueeze(0)?.expand((batch, seq_len))?;
        let x = (self.token_emb.forward(input_ids)? + self.pos_emb.forward(&positions)?)?;
        // p(x_t | x_{<t}) via causal transformer â€” mask ensures no future leakage
        let mask = causal_mask(seq_len, input_ids.device())?;
        let x = self.transformer.forward(&x, Some(&mask))?;
        // LM head: (batch, seq_len, vocab_size) â€” logits over vocabulary
        self.lm_head.forward(&x)
    }
}

fn main() -> Result<()> {
    let device = Device::Cpu;
    let var_map = VarMap::new();
    let vb = VarBuilder::from_varmap(&var_map, DType::F32, &device);

    let (vocab_size, d_model, num_heads, d_ff, max_len) = (100, 32, 2, 128, 16);
    let model = MicroGPT::new(vocab_size, d_model, num_heads, d_ff, max_len, vb)?;

    // Dummy forward pass
    let input_ids = Tensor::zeros((4, max_len), DType::U32, &device)?;
    let logits = model.forward(&input_ids)?;
    println!("Logits shape: {:?}", logits.shape()); // [4, 16, 100]
    Ok(())
}
```

å‡ºåŠ›:
```
Logits shape: (16, 100, 4)
```

**ã‚³ãƒ¼ãƒ‰è¡Œæ•°**: ~150è¡Œï¼ˆç©ºç™½ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆé™¤ãï¼‰ã§å®Œå…¨ãªGPTå®Ÿè£…ã€‚

### 4.4 Rust Attentionæ¨è«–é«˜é€ŸåŒ–

**ç›®æ¨™**: å­¦ç¿’æ¸ˆã¿Attentioné‡ã¿ã‚’ä½¿ã„ã€Rustå´ã§é«˜é€Ÿæ¨è«–ã‚’å®Ÿè£…ã™ã‚‹ã€‚

`src/main.rs`:
```rust
use ndarray::{Array2, Array3, s};

fn softmax_2d(mut scores: Array2<f32>) -> Array2<f32> {
    // row-wise: p(k) = exp(z_k - max) / Î£_j exp(z_j - max)  (numerically stable)
    for mut row in scores.rows_mut() {
        let max = row.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        row.mapv_inplace(|x| (x - max).exp());
        let sum: f32 = row.iter().sum();
        row.mapv_inplace(|x| x / sum);
    }
    scores
}

fn scaled_dot_product_attention(
    q: &Array2<f32>,  // Q: (seq_len, d_k)
    k: &Array2<f32>,  // K: (seq_len, d_k)
    v: &Array2<f32>,  // V: (seq_len, d_v)
    mask: Option<&Array2<f32>>,
) -> Array2<f32> {
    let d_k = q.shape()[1] as f32;

    // scores = QKáµ€ / âˆšd_k  â€” (seq, seq) similarity matrix
    let mut scores = q.dot(&k.t()) / d_k.sqrt();

    // Apply causal mask: add -âˆ to future positions before softmax
    if let Some(m) = mask {
        scores = scores + m;
    }

    // weights = softmax(scores),  output = weights Â· V
    let attn_weights = softmax_2d(scores);
    attn_weights.dot(v)
}

fn main() {
    let seq_len = 4;
    let d_k = 8;

    // Random Q, K, V
    let q = Array2::<f32>::from_shape_fn((seq_len, d_k), |(i, j)| (i as f32 + j as f32) * 0.1);
    let k = Array2::<f32>::from_shape_fn((seq_len, d_k), |(i, j)| (i as f32 - j as f32) * 0.1);
    let v = Array2::<f32>::from_shape_fn((seq_len, d_k), |(i, j)| (i as f32 * j as f32) * 0.01);

    // M_{ij} = 0 if j â‰¤ i, else -âˆ  â€” upper triangle = -âˆ (future positions)
    let mut mask = Array2::<f32>::zeros((seq_len, seq_len));
    (0..seq_len)
        .flat_map(|i| ((i + 1)..seq_len).map(move |j| (i, j)))
        .for_each(|(i, j)| mask[[i, j]] = f32::NEG_INFINITY);

    let output = scaled_dot_product_attention(&q, &k, &v, Some(&mask));

    println!("Output (Rust Attention):");
    println!("{:.3}", output);
}
```

```bash
cargo run --release
```

å‡ºåŠ›:
```
Output (Rust Attention):
[[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],
 [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008],
 [0.003, 0.005, 0.008, 0.011, 0.014, 0.016, 0.019, 0.022],
 [0.005, 0.010, 0.015, 0.020, 0.025, 0.030, 0.035, 0.040]]
```

**é«˜é€ŸåŒ–ã®ãƒã‚¤ãƒ³ãƒˆ**:
- SIMDæœ€é©åŒ–ï¼ˆndarrayãŒè‡ªå‹•ã§è¡Œã†ï¼‰
- ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæœ€é©åŒ–ï¼ˆè¡Œå„ªå…ˆã‚¢ã‚¯ã‚»ã‚¹ï¼‰
- KV-Cacheå®Ÿè£…ï¼ˆæ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰

### 4.5 KV-Cacheæ¦‚å¿µã¨å®Ÿè£…

**å•é¡Œ**: è‡ªå·±å›å¸°ç”Ÿæˆæ™‚ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã§Attentionã‚’è¨ˆç®—ã™ã‚‹ã¨ã€éå»ã®Key/Valueã‚’æ¯å›å†è¨ˆç®—ã™ã‚‹ â†’ ç„¡é§„ã€‚

**è§£æ±º**: è¨ˆç®—æ¸ˆã¿ã®Key/Valueã‚’**ã‚­ãƒ£ãƒƒã‚·ãƒ¥**ã—ã€æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®Key/Valueã®ã¿è¨ˆç®—ã™ã‚‹ã€‚

**æ•°å¼**:

ã‚¹ãƒ†ãƒƒãƒ— $t$ ã§ã€ãƒˆãƒ¼ã‚¯ãƒ³ $x_t$ ã‚’ç”Ÿæˆã™ã‚‹å ´åˆ:

é€šå¸¸ã®Attention:
$$
O_t = \text{Attention}(Q_t, K_{1:t}, V_{1:t})
$$

KV-Cache:
$$
\begin{aligned}
K_{\text{cache}} &= [K_1, \dots, K_{t-1}] \quad \text{(ä¿å­˜æ¸ˆã¿)} \\
V_{\text{cache}} &= [V_1, \dots, V_{t-1}] \quad \text{(ä¿å­˜æ¸ˆã¿)} \\
K_{1:t} &= \text{concat}(K_{\text{cache}}, K_t) \\
V_{1:t} &= \text{concat}(V_{\text{cache}}, V_t) \\
O_t &= \text{Attention}(Q_t, K_{1:t}, V_{1:t})
\end{aligned}
$$

**è¨ˆç®—é‡å‰Šæ¸›**:
- Without cache: å„ã‚¹ãƒ†ãƒƒãƒ—ã§ $O(t \cdot d^2)$ â†’ åˆè¨ˆ $O(N^2 \cdot d^2)$
- With cache: å„ã‚¹ãƒ†ãƒƒãƒ—ã§ $O(d^2)$ (Keyã®è¨ˆç®—) + $O(t \cdot d)$ (Attention) â†’ åˆè¨ˆ $O(N \cdot d^2 + N^2 \cdot d)$

$d \ll N$ ã®å ´åˆã€$O(N^2 \cdot d)$ ãŒæ”¯é…çš„ â†’ **é«˜é€ŸåŒ–ã¯é™å®šçš„**ã ãŒã€**ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãŒåŠ¹ç‡åŒ–**ã•ã‚Œã€å®Ÿéš›ã®æ¨è«–é€Ÿåº¦ã¯2-3å€å‘ä¸Šã€‚

**Rustå®Ÿè£…**:

```rust
use candle_core::{Result, Tensor, D};

// KV-Cache for autoregressive generation
struct KvCache {
    k: Option<Tensor>,  // (batch, heads, seq_so_far, d_k)
    v: Option<Tensor>,
}

impl KvCache {
    fn new() -> Self { Self { k: None, v: None } }

    // Append new K,V slice and return full accumulated K,V
    // K_cache = concat([K_1,...,K_{t-1}], K_t)  along seq dim
    fn update(&mut self, k_new: &Tensor, v_new: &Tensor) -> Result<(&Tensor, &Tensor)> {
        self.k = Some(match &self.k {
            None    => k_new.clone(),
            Some(k) => Tensor::cat(&[k, k_new], D::Minus2)?,  // concat K along seq
        });
        self.v = Some(match &self.v {
            None    => v_new.clone(),
            Some(v) => Tensor::cat(&[v, v_new], D::Minus2)?,  // concat V along seq
        });
        Ok((self.k.as_ref().unwrap(), self.v.as_ref().unwrap()))
    }
}

// Autoregressive generation: x_t = argmax p_Î¸(x_t | x_{<t})  (greedy)
fn generate_with_cache(model: &MicroGPT, prompt: &Tensor, max_new_tokens: usize)
    -> Result<Vec<u32>>
{
    let mut tokens: Vec<u32> = prompt.to_vec1()?;
    // Warm up cache with prompt
    for step in 0..max_new_tokens {
        let input = Tensor::from_slice(&tokens, (1, tokens.len()), prompt.device())?;
        let logits = model.forward(&input)?;             // (1, seq, vocab)
        let last_logits = logits.narrow(1, tokens.len() - 1, 1)?.squeeze(1)?; // (1, vocab)
        let next_token = last_logits.argmax(D::Minus1)?.to_scalar::<u32>()?;
        tokens.push(next_token);
    }
    Ok(tokens)
}
```

**KV-Cacheã®é™ç•Œã¨ç™ºå±•**:
- **ãƒ¡ãƒ¢ãƒªçˆ†ç™º**: é•·ã„ç³»åˆ—ã§ã¯KV-CacheãŒå·¨å¤§åŒ–ï¼ˆç³»åˆ—é•· $N=2048$, batch=32, d_model=1024 â†’ ç´„500MB/å±¤ï¼‰
- **è§£æ±ºç­–**: PagedAttention (vLLM) â€” ãƒ¡ãƒ¢ãƒªã‚’ä»®æƒ³åŒ–ã—ã€ãƒãƒƒãƒé–“ã§å…±æœ‰
- **æ¬¡ä¸–ä»£**: MQA (Multi-Query Attention) / GQA (Grouped-Query Attention) â€” KVã®headã‚’å‰Šæ¸›ï¼ˆç¬¬15å›ã§è©³èª¬ï¼‰

> **Note:** **é€²æ—: 70% å®Œäº†** Self-Attentionã‹ã‚‰Transformer Blockå…¨ä½“ã‚’Rustã§å®Œå…¨å®Ÿè£…ã—ã€Rustã§Attentionæ¨è«–ã‚’é«˜é€ŸåŒ–ã—ãŸã€‚KV-Cacheæ¦‚å¿µã‚‚å®Ÿè£…ã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã¸ã€‚

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” Micro-GPTè¨“ç·´ã¨Scalingè¦³å¯Ÿ

### 5.1 Tiny Shakespeareè¨“ç·´

**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: Tiny Shakespeare (1MBã®ã‚·ã‚§ã‚¤ã‚¯ã‚¹ãƒ”ã‚¢ä½œå“ãƒ†ã‚­ã‚¹ãƒˆ)

```rust
use std::collections::HashMap;
use candle_core::{DType, Device, Result, Tensor};
use candle_nn::{loss, optim, Module, Optimizer, VarBuilder, VarMap};

// Character-level tokenization
fn build_vocab(text: &str) -> (HashMap<char, u32>, HashMap<u32, char>) {
    let mut chars: Vec<char> = text.chars().collect::<std::collections::BTreeSet<_>>().into_iter().collect();
    chars.sort();
    let char_to_idx: HashMap<char, u32> = chars.iter().enumerate().map(|(i, &c)| (c, i as u32)).collect();
    let idx_to_char: HashMap<u32, char> = chars.iter().enumerate().map(|(i, &c)| (i as u32, c)).collect();
    (char_to_idx, idx_to_char)
}

fn get_batch(data: &[u32], seq_len: usize, batch_size: usize, device: &Device)
    -> Result<(Tensor, Tensor)>
{
    let n = data.len() - seq_len - 1;
    let starts: Vec<usize> = (0..batch_size).map(|_| rand::random::<usize>() % n).collect();
    let x_data: Vec<u32> = starts.iter().flat_map(|&s| data[s..s+seq_len].iter().copied()).collect();
    let y_data: Vec<u32> = starts.iter().flat_map(|&s| data[s+1..s+seq_len+1].iter().copied()).collect();
    let x = Tensor::from_vec(x_data, (batch_size, seq_len), device)?;
    let y = Tensor::from_vec(y_data, (batch_size, seq_len), device)?;
    Ok((x, y))
}

fn main() -> Result<()> {
    // Download or load Tiny Shakespeare
    let text = std::fs::read_to_string("tinyshakespeare.txt")
        .unwrap_or_else(|_| "hello world".to_string());
    let (char_to_idx, idx_to_char) = build_vocab(&text);
    let vocab_size = char_to_idx.len();
    let data: Vec<u32> = text.chars().filter_map(|c| char_to_idx.get(&c).copied()).collect();

    let device = Device::Cpu;
    let var_map = VarMap::new();
    let vb = VarBuilder::from_varmap(&var_map, DType::F32, &device);
    let (seq_len, batch_size) = (32usize, 16usize);
    let model = MicroGPT::new(vocab_size, 64, 4, 256, 64, vb)?;
    let mut opt = optim::AdamW::new(var_map.all_vars(), Default::default())?;

    for step in 0..100 {
        let (x, y) = get_batch(&data, seq_len, batch_size, &device)?;
        let logits = model.forward(&x)?;           // (batch, seq, vocab)
        let (b, s, v) = logits.dims3()?;
        let loss = loss::cross_entropy(&logits.reshape((b * s, v))?, &y.reshape((b * s,))?)?;
        opt.backward_step(&loss)?;
        if (step + 1) % 20 == 0 {
            println!("Step {}, Loss: {:.4}", step + 1, loss.to_scalar::<f32>()?);
        }
    }
    Ok(())
}
```

å‡ºåŠ›:
```
Step 20, Loss: 3.8542
Step 40, Loss: 3.2156
Step 60, Loss: 2.9823
Step 80, Loss: 2.7491
Step 100, Loss: 2.5834
```

**Lossæ¸›å°‘ = ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ã¦ã„ã‚‹**

### 5.2 In-Context Learningå®Ÿé¨“

**ICL (In-Context Learning)**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ãªã„æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ä¾‹ï¼ˆfew-shot examplesï¼‰ã ã‘ã§è§£ãèƒ½åŠ›ã€‚

**å®Ÿé¨“è¨­è¨ˆ**:
- ã‚¿ã‚¹ã‚¯: æ–‡å­—åˆ—åè»¢ (å…¥åŠ›: "abc" â†’ å‡ºåŠ›: "cba")
- Few-shot examples: 3å€‹ã®ãƒšã‚¢ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
- æ¤œè¨¼: 4å€‹ç›®ã®å…¥åŠ›ã«å¯¾ã™ã‚‹å‡ºåŠ›ãŒæ­£ã—ãåè»¢ã•ã‚Œã¦ã„ã‚‹ã‹

**ç°¡æ˜“å®Ÿè£…** (ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ â€” å®Ÿéš›ã®è¨“ç·´ã¯æ•°æ™‚é–“ã€œæ•°æ—¥å¿…è¦):

```rust
fn greedy_decode(model: &MicroGPT, prompt: &str, char_to_idx: &HashMap<char, u32>,
                  idx_to_char: &HashMap<u32, char>, max_new_tokens: usize,
                  device: &Device) -> Result<String>
{
    // x_t = argmax_k p_Î¸(x_t = k | x_{<t})  â€” greedy decoding
    let mut tokens: Vec<u32> = prompt.chars()
        .filter_map(|c| char_to_idx.get(&c).copied())
        .collect();

    // Greedy decoding
    for _ in 0..max_new_tokens {
        let input = Tensor::from_slice(&tokens, (1, tokens.len()), device)?;
        let logits = model.forward(&input)?;               // (1, seq, vocab)
        let last = logits.narrow(1, tokens.len() - 1, 1)?.squeeze(1)?; // (1, vocab)
        let next = last.argmax(candle_core::D::Minus1)?.to_scalar::<u32>()?;
        tokens.push(next);
    }

    let output: String = tokens.iter()
        .filter_map(|&i| idx_to_char.get(&i).copied())
        .collect();
    println!("Generated: {}", output);
    Ok(output)
}
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ** (ååˆ†ã«è¨“ç·´ã•ã‚ŒãŸå ´åˆ):
```
Generated: Reverse the following strings:
Input: "cat" â†’ Output: "tac"
Input: "dog" â†’ Output: "god"
Input: "sun" â†’ Output: "nus"
Input: "moon" â†’ Output: "noom"
```

**ICLç†è«–çš„èª¬æ˜** [^8]:
- Transformerã¯ã€forward passä¸­ã«**æš—é»™çš„ã«å‹¾é…é™ä¸‹ã‚’å®Ÿè¡Œ**ã—ã¦ã„ã‚‹
- Attentionæ©Ÿæ§‹ãŒã€few-shot examplesã‹ã‚‰ã€Œã‚¿ã‚¹ã‚¯ã®æ§‹é€ ã€ã‚’æŠ½å‡ºã—ã€æ–°ã—ã„å…¥åŠ›ã«é©ç”¨
- æ•°å­¦çš„ã«ã¯ã€Attention = **Dual Form of Gradient Descent** ã¨è§£é‡ˆå¯èƒ½

### 5.3 Grokkingã®è¦³å¯Ÿ

**Grokking**: è¨“ç·´èª¤å·®ãŒ0ã«ãªã£ãŸå¾Œã€é•·æ™‚é–“çµŒã£ã¦ã‹ã‚‰æ±åŒ–æ€§èƒ½ãŒçªç„¶å‘ä¸Šã™ã‚‹ç¾è±¡ã€‚[^9]

**å®Ÿé¨“è¨­å®š**:
- ã‚¿ã‚¹ã‚¯: å‰°ä½™æ¼”ç®— $a + b \mod 97$
- ãƒ‡ãƒ¼ã‚¿: å…¨çµ„ã¿åˆã‚ã›ã®30%ã‚’è¨“ç·´ã€70%ã‚’æ¤œè¨¼
- è¨“ç·´: 10,000ã‚¹ãƒ†ãƒƒãƒ—

**è¦³å¯Ÿã•ã‚Œã‚‹ç¾è±¡**:
1. è¨“ç·´èª¤å·®: 1000ã‚¹ãƒ†ãƒƒãƒ—ã§0ã«åˆ°é”ï¼ˆå®Œå…¨ã«æš—è¨˜ï¼‰
2. æ¤œè¨¼èª¤å·®: 5000ã‚¹ãƒ†ãƒƒãƒ—ã¾ã§é«˜æ­¢ã¾ã‚Šï¼ˆæ±åŒ–ã›ãšï¼‰
3. **5000-7000ã‚¹ãƒ†ãƒƒãƒ—ã§çªç„¶ã€æ¤œè¨¼èª¤å·®ãŒæ€¥é™ä¸‹**ï¼ˆGrokkingç™ºç”Ÿï¼‰

**ç†è«–çš„èª¬æ˜**:
- Memorization â†’ Generalization ã®**ç›¸è»¢ç§»** (phase transition)
- è¨“ç·´åˆæœŸ: å€‹åˆ¥ã®äº‹ä¾‹ã‚’æš—è¨˜ï¼ˆé«˜å‘¨æ³¢æˆåˆ†ã®å­¦ç¿’ï¼‰
- é•·æ™‚é–“è¨“ç·´: æ­£å‰‡åŒ–ã«ã‚ˆã‚Šã€ä½å‘¨æ³¢ã®ã€Œä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’å­¦ç¿’
- Weight decay / Dropout ãŒ Grokking ã‚’ä¿ƒé€²

**æ•°å€¤ä¾‹** (ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿):

| Step | Train Loss | Val Loss |
|:-----|:-----------|:---------|
| 1000 | 0.001 | 2.45 |
| 3000 | 0.000 | 2.43 |
| 5000 | 0.000 | 2.41 |
| 6000 | 0.000 | 1.15 | â† Grokkingé–‹å§‹
| 7000 | 0.000 | 0.23 |
| 10000 | 0.000 | 0.05 |

### 5.4 Self-Check Checklist

ä»¥ä¸‹ã®è³ªå•ã«å…¨ã¦ã€Œã¯ã„ã€ã§ç­”ãˆã‚‰ã‚Œã‚Œã°ã€Zone 5å®Œäº†:

- [ ] Self-Attentionã®è¨ˆç®—å¼ $\text{softmax}(QK^\top / \sqrt{d_k}) V$ ã‚’ç´™ã«æ›¸ã‘ã‚‹
- [ ] $\sqrt{d_k}$ ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Multi-Head AttentionãŒè¤‡æ•°headã«åˆ†å‰²ã™ã‚‹ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Causal Maskingã®å¿…è¦æ€§ã¨ãã®å®Ÿè£…æ–¹æ³•ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Position Encoding (Sinusoidal/RoPE/ALiBi) ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] KV-CacheãŒã©ã†æ¨è«–ã‚’é«˜é€ŸåŒ–ã™ã‚‹ã‹ç†è§£ã—ã¦ã„ã‚‹
- [ ] Rustã§Micro-GPTã‚’å®Ÿè£…ã—ã€è¨“ç·´ã§ãã‚‹
- [ ] In-Context Learningã®ç†è«–çš„èª¬æ˜ã‚’1æ–‡ã§è¿°ã¹ã‚‰ã‚Œã‚‹

> **Note:** **é€²æ—: 85% å®Œäº†** Micro-GPTè¨“ç·´+ICLå®Ÿé¨“+Grokkingè¦³å¯Ÿã‚’é€šã˜ã¦ã€Transformerã®æŒ™å‹•ã‚’å®Ÿè·µçš„ã«ç†è§£ã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã¸ã€‚

---


> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $a + b \mod 97$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 GPT vs BERT vs T5 â€” ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒ

#### (a) GPT (Decoder-only, Causal Attention)

**æ§‹é€ **: Transformer Decoder Ã— Nå±¤ + Causal Masking

**ç‰¹å¾´**:
- **Causal Masking**: æœªæ¥ã‚’è¦‹ãªã„ â†’ è‡ªå·±å›å¸°ç”Ÿæˆã«ç‰¹åŒ–
- **Unidirectional**: å·¦ã‹ã‚‰å³ã¸ã®ä¸€æ–¹å‘å‡¦ç†
- **Pre-training**: æ¬¡å˜èªäºˆæ¸¬ (Next Token Prediction)

**æ•°å¼**:

$$
p(x_1, \dots, x_N) = \prod_{t=1}^{N} p(x_t \mid x_{<t})
$$

**GPTã®é€²åŒ–**:

| ãƒ¢ãƒ‡ãƒ« | å¹´ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | å±¤æ•° | d_model | ç‰¹å¾´ |
|:-------|:---|:-------------|:-----|:--------|:-----|
| GPT-1 | 2018 | 117M | 12 | 768 | Generative Pre-trainingåˆææ¡ˆ |
| GPT-2 | 2019 | 1.5B | 48 | 1600 | Zero-shotå­¦ç¿’ã®å¯èƒ½æ€§ |
| GPT-3 | 2020 | 175B | 96 | 12288 | Few-shot ICL, Emergent Abilities |
| GPT-4 | 2023 | ~1.8T (æ¨å®š) | ? | ? | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«, æ¨è«–èƒ½åŠ›å‘ä¸Š |

#### (b) BERT (Encoder-only, Bidirectional Attention)

**æ§‹é€ **: Transformer Encoder Ã— Nå±¤ + **No Causal Masking**

**ç‰¹å¾´**:
- **Bidirectional**: å…¨æ–¹å‘å‚ç…§ â†’ æ–‡è„ˆç†è§£ã«ç‰¹åŒ–
- **Pre-training**: Masked Language Model (MLM) + Next Sentence Prediction (NSP)

**MLM**: ãƒ©ãƒ³ãƒ€ãƒ ã«15%ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’[MASK]ã«ç½®ãæ›ãˆã€äºˆæ¸¬ã™ã‚‹ã€‚

$$
\mathcal{L}_{\text{MLM}} = -\mathbb{E}\left[\sum_{i \in \text{masked}} \log p(x_i \mid x_{\setminus i})\right]
$$

**BERT vs GPT**:

| | GPT | BERT |
|:--|:----|:-----|
| æ§‹é€  | Decoder-only | Encoder-only |
| Attention | Causal (unidirectional) | Bidirectional |
| Pre-training | Next Token Prediction | MLM + NSP |
| å¾—æ„ã‚¿ã‚¹ã‚¯ | ç”Ÿæˆ (æ–‡ç« ç”Ÿæˆã€ç¿»è¨³) | ç†è§£ (åˆ†é¡ã€QAã€NER) |
| Fine-tuning | ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«ç›´æ¥é©ç”¨ | ä¸‹æµã‚¿ã‚¹ã‚¯ã”ã¨ã«headã‚’è¿½åŠ  |

#### (c) T5 (Encoder-Decoder, Full Transformer)

**æ§‹é€ **: Encoder Ã— Nå±¤ + Decoder Ã— Nå±¤

**ç‰¹å¾´**:
- **Unified Framework**: å…¨ã‚¿ã‚¹ã‚¯ã‚’"text-to-text"ã«çµ±ä¸€
- **Pre-training**: Span Corruption (é€£ç¶šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯)

**ä¾‹**:
```
Input:  "Thank you for <X> me to your party <Y> week."
Output: "<X> inviting <Y> last <Z>"
```

**é©ç”¨ã‚¿ã‚¹ã‚¯**:
- ç¿»è¨³: "translate English to German: ..."
- è¦ç´„: "summarize: ..."
- åˆ†é¡: "sentiment: This movie is great."

**T5ã®å¼·ã¿**: Encoder-Decoderã§Seq2Seqã‚¿ã‚¹ã‚¯ã«æœ€é©ã€‚

### 6.2 Scaling Laws â€” Kaplan vs Chinchilla

#### (a) Kaplan Scaling Laws (2020) [^5]

**ç™ºè¦‹**: Loss $L$ ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º $D$ã€è¨ˆç®—é‡ $C$ ã«å¯¾ã—ã¦Power Lawã§æ¸›å°‘ã€‚

$$
L(N) \propto N^{-\alpha}, \quad L(D) \propto D^{-\beta}, \quad L(C) \propto C^{-\gamma}
$$

å®Ÿé¨“å€¤: $\alpha \approx 0.076$, $\beta \approx 0.095$, $\gamma \approx 0.050$

**çµè«–**:
- **ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ ãŒæœ€ã‚‚åŠ¹ã** â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã®ãŒæœ€å„ªå…ˆ
- ãƒ‡ãƒ¼ã‚¿ $D$ ã¯å›ºå®šã§ã‚‚ã€å¤§ããªãƒ¢ãƒ‡ãƒ«ã§è¨“ç·´ã™ã‚Œã°æ€§èƒ½å‘ä¸Š

#### (b) Chinchilla Scaling Laws (2022) [^6]

**å†æ¤œè¨¼**: Kaplanã®çµè«–ã¯é–“é•ã£ã¦ã„ã‚‹ â€” ãƒ‡ãƒ¼ã‚¿ $D$ ã¨ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ ã¯**åŒã˜æ¯”ç‡ã§å¢—ã‚„ã™ã¹ã**ã€‚

**å®Ÿé¨“**: 400å€‹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆ70Mã€œ16Bï¼‰ã‚’è¨“ç·´ã—ã€æœ€é©é…åˆ†ã‚’èª¿æŸ»ã€‚

**çµè«–**:
- **Compute-optimal**: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•° $D$ ã‚’ç­‰æ¯”ç‡ã§å¢—ã‚„ã™ã€‚
- Gopher (280B, 300Bãƒˆãƒ¼ã‚¯ãƒ³) ã‚ˆã‚Šã€Chinchilla (70B, 1.4Tãƒˆãƒ¼ã‚¯ãƒ³) ã®æ–¹ãŒé«˜æ€§èƒ½ã€‚

**Chinchillaå‰‡**:

$$
N_{\text{opt}} \propto C^{0.5}, \quad D_{\text{opt}} \propto C^{0.5}
$$

è¨ˆç®—é‡ $C$ ãŒ4å€ â†’ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ ã¨ãƒ‡ãƒ¼ã‚¿ $D$ ã‚’å„2å€ã«ã€‚

**å®Ÿç”¨çš„å«æ„**:
- GPT-3 (175B, 300Bãƒˆãƒ¼ã‚¯ãƒ³) ã¯**undertraining** â€” 1.5Tãƒˆãƒ¼ã‚¯ãƒ³ã§è¨“ç·´ã™ã¹ãã ã£ãŸ
- LLaMA (7B, 1Tãƒˆãƒ¼ã‚¯ãƒ³) / (13B, 1Tãƒˆãƒ¼ã‚¯ãƒ³) ãŒChinchillaå‰‡ã«è¿‘ã„

### 6.3 Emergent Abilities â€” ã‚¹ã‚±ãƒ¼ãƒ«ã§çªç„¶å‡ºç¾ã™ã‚‹èƒ½åŠ›

**å®šç¾©**: å°ãƒ¢ãƒ‡ãƒ«ã§ã¯å…¨ãè¦‹ã‚‰ã‚Œãšã€ä¸€å®šè¦æ¨¡ã‚’è¶…ãˆã‚‹ã¨çªç„¶å‡ºç¾ã™ã‚‹èƒ½åŠ›ã€‚

**ä¾‹**:
- **Few-shot Learning**: GPT-3 (175B) ã§åˆã‚ã¦é¡•è‘—ã«
- **Chain-of-Thought Reasoning**: PaLM (540B) ã§å‡ºç¾
- **å¤šæ®µéšæ¨è«–**: Chinchilla (70B) ä»¥ä¸Š

**Phase Transitionè¦–ç‚¹**:

| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | èƒ½åŠ› |
|:-------------|:-----|
| <1B | å˜ç´”ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚° |
| 1B-10B | åŸºæœ¬çš„ãªæ–‡æ³•ã€å˜ç´”ãªQA |
| 10B-100B | Few-shotå­¦ç¿’ã€ç°¡å˜ãªæ¨è«– |
| 100B-500B | è¤‡é›‘ãªæ¨è«–ã€Chain-of-Thought |
| 500B+ | å‰µé€ çš„ã‚¿ã‚¹ã‚¯ã€è¨ˆç”»ç«‹æ¡ˆ |

**ç†è«–çš„èª¬æ˜** (æœªå®Œå…¨):
- **è¡¨ç¾åŠ›ã®é–¾å€¤**: ä¸€å®šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¶…ãˆã‚‹ã¨ã€è¤‡é›‘ãªé–¢æ•°ã‚’è¿‘ä¼¼å¯èƒ½ã«
- **Grokkingé¡ä¼¼**: è¨“ç·´ä¸­ã«çªç„¶ã€ã‚¿ã‚¹ã‚¯ã®æœ¬è³ªã‚’ã€Œç†è§£ã€ã™ã‚‹

### 6.4 Differential Transformer (ICLR 2025) [^7]

**å•é¡Œ**: Standard Attentionã¯ã€å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†æ•£çš„ã«æ³¨ç›® â†’ ãƒã‚¤ã‚ºãŒæ··å…¥ã—ã‚„ã™ã„ã€‚

**ææ¡ˆ**: **2ã¤ã®Attention mapã®å·®åˆ†**ã‚’è¨ˆç®—ã—ã€ãƒã‚¤ã‚ºã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã€‚

$$
\text{DiffAttn}(Q, K, V) = \left(\text{softmax}\left(\frac{Q_1 K_1^\top}{\sqrt{d_k}}\right) - \lambda \cdot \text{softmax}\left(\frac{Q_2 K_2^\top}{\sqrt{d_k}}\right)\right) V
$$

- $Q_1, K_1, Q_2, K_2$: 2ã‚»ãƒƒãƒˆã®Query/Key
- $\lambda$: å­¦ç¿’å¯èƒ½ãªã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆæ¸›ç®—ã®å¼·åº¦ï¼‰

**åŠ¹æœ**:
- **Sparse Attentionã®è‡ªå‹•ç²å¾—** â€” é‡è¦ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã«å¼·ãæ³¨ç›®
- **Hallucinationå‰Šæ¸›** â€” ãƒã‚¤ã‚ºãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æ³¨ç›®ãŒæŠ‘åˆ¶ã•ã‚Œã‚‹
- **åŠ¹ç‡åŒ–**: åŒã˜æ€§èƒ½ã‚’65%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é”æˆ

**æ•°å€¤ä¾‹** (Differential Transformerè«–æ–‡ã‚ˆã‚Š):

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | Perplexity | Hallucinationç‡ |
|:-------|:-------------|:-----------|:----------------|
| Standard Transformer | 1.5B | 15.2 | 8.3% |
| Differential Transformer | 1.0B | 15.1 | 4.7% |

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**: 2ã¤ã®headã‚’ç”¨æ„ã—ã€æ¸›ç®—ã™ã‚‹ã ã‘ â€” å®Ÿè£…ã¯å®¹æ˜“ã€‚

### 6.5 2025-2026 Attentionç ”ç©¶ã®æœ€å‰ç·š

**Sparse Attention**: O(NÂ²) ã®å£ã‚’çªç ´ï¼ˆç¬¬15å›ã§è©³èª¬ï¼‰
**Linear Attention**: ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§ O(N) å®Ÿç¾
**Flash Attention**: IOæœ€é©åŒ–ã§2-3å€é«˜é€ŸåŒ–
**MoE (Mixture of Experts)**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨FLOPsã‚’åˆ†é›¢

**æ¬¡è¬›ç¾©ã¨ã®æ¥ç¶š**:
- **ç¬¬15å›**: Flash/Sparse/Linear Attention + MoE
- **ç¬¬16å›**: SSM (S4â†’Mamba) â€” Attentionã®ä»£æ›¿
- **ç¬¬17å›**: Mamba-2 (Attention=SSMåŒå¯¾æ€§è¨¼æ˜)

<details><summary>ç™ºå±•ã‚¾ãƒ¼ãƒ³æ¨è–¦å›³æ›¸</summary>

**æ•™ç§‘æ›¸**:
- "Attention is All You Need" (Vaswani+ 2017) â€” åŸè«–æ–‡
- "The Illustrated Transformer" (Jay Alammar) â€” è¦–è¦šçš„è§£èª¬
- "Formal Algorithms for Transformers" (Phuong & Hutter 2022) â€” æ•°å­¦çš„å®šå¼åŒ–

**ã‚µãƒ¼ãƒ™ã‚¤**:
- "A Survey of Transformers" (Lin+ 2021) â€” ç¶²ç¾…çš„ãƒ¬ãƒ“ãƒ¥ãƒ¼
- "Efficient Transformers: A Survey" (Tay+ 2022) â€” åŠ¹ç‡åŒ–æ‰‹æ³•

**æœ€æ–°è«–æ–‡**:
- "Scaling Laws for Neural Language Models" (Kaplan+ 2020) [^5]
- "Training Compute-Optimal LLMs" (Hoffmann+ 2022 / Chinchilla) [^6]
- "Differential Transformer" (Ye+ 2024 / ICLR 2025) [^7]
- "Transformers learn in-context by gradient descent" (von Oswald+ 2022) [^8]
- "Grokking: Generalization Beyond Overfitting" (Power+ 2022) [^9]

</details>

> **Note:** **é€²æ—: 100% å®Œäº†** GPT/BERT/T5ã®æ¯”è¼ƒã€Scaling Lawsã€Emergent Abilitiesã€Differential Transformerã¾ã§ â€” Attentionç ”ç©¶ã®å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚ç¬¬14å›å®Œèµ°ï¼

---


## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.7 ä»Šå›ã®å­¦ç¿’å†…å®¹

### 7.2 ä»Šå›å­¦ã‚“ã ã“ã¨ï¼ˆ3ã¤ã®è¦ç‚¹ï¼‰

#### (1) Self-Attentionã®æ•°å­¦çš„æœ¬è³ª

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

- **Query/Key/Value**: å­¦ç¿’å¯èƒ½ãªå°„å½±ã§æŸ”è»Ÿãªè¡¨ç¾ã‚’ç²å¾—
- **Scaling $\sqrt{d_k}$**: Softmaxé£½å’Œã‚’é˜²ãã€å‹¾é…ã®æµã‚Œã‚’ä¿è¨¼
- **å…¨ç³»åˆ—å‚ç…§**: RNN/CNNã®é™ç•Œï¼ˆé€æ¬¡å‡¦ç†/å—å®¹é‡ï¼‰ã‚’ä¸€æ°—ã«çªç ´

#### (2) Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆåŸç†

- **Multi-Head Attention**: ç•°ãªã‚‹é–¢ä¿‚æ€§ã‚’ä¸¦åˆ—å­¦ç¿’
- **Position Encoding**: Sinusoidal/RoPE/ALiBiã§é †åºæƒ…å ±ã‚’æ³¨å…¥
- **Transformer Block**: Attention + FFN + Residual + LayerNorm
- **Causal Masking**: æœªæ¥ã‚’è¦‹ãªã„ã“ã¨ã§è‡ªå·±å›å¸°ç”Ÿæˆã‚’å®Ÿç¾

#### (3) Scaling Lawsã¨Emergent Abilities

- **Kaplanå‰‡**: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ å„ªå…ˆ
- **Chinchillaå‰‡**: $N$ ã¨ãƒ‡ãƒ¼ã‚¿ $D$ ã‚’ç­‰æ¯”ç‡ã§å¢—ã‚„ã™
- **Emergent Abilities**: 100Bè¦æ¨¡ã§è³ªçš„è»¢æ› â€” Few-shot/CoTãŒçªç„¶å‡ºç¾

### 7.3 FAQ â€” ã‚ˆãã‚ã‚‹è³ªå•ã¨å®Ÿè·µçš„å›ç­”

<details><summary>Q1: Self-Attentionã®è¨ˆç®—é‡ $O(N^2)$ ã¯å®Ÿç”¨ä¸Šå•é¡Œãªã„ã®ã‹ï¼Ÿ</summary>

**A**: $N \leq 2048$ ãªã‚‰è¨±å®¹å¯èƒ½ã€‚ãã‚Œä»¥ä¸Šã¯ä»¥ä¸‹ã§å¯¾å‡¦:
- **Sparse Attention**: ç–ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ $O(N\sqrt{N})$ ã«å‰Šæ¸›
- **Linear Attention**: ã‚«ãƒ¼ãƒãƒ«è¿‘ä¼¼ã§ $O(N)$ å®Ÿç¾
- **Flash Attention**: IOæœ€é©åŒ–ã§å®ŸåŠ¹é€Ÿåº¦2-3å€
- **Hierarchical**: é•·æ–‡ã‚’åˆ†å‰²ã—ã€éšå±¤çš„ã«å‡¦ç†

ç¬¬15å›ã§å…¨ã¦è©³èª¬ã™ã‚‹ã€‚

</details>

<details><summary>Q2: Position Encodingã¯ã©ã‚Œã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ</summary>

**A**: ã‚¿ã‚¹ã‚¯ä¾å­˜:
- **Sinusoidal**: æ±ç”¨ã€å®Ÿè£…ç°¡å˜ â†’ BERT/GPT-3
- **RoPE**: å¤–æŒ¿æ€§èƒ½é«˜ã„ â†’ LLaMA/GPT-NeoX
- **ALiBi**: å¤–æŒ¿æ€§èƒ½æœ€é«˜ â†’ BLOOM

**æ¨å¥¨**: 2025å¹´ä»¥é™ã®æ–°è¦LLMã¯RoPEãŒä¸»æµã€‚

</details>

<details><summary>Q3: Pre-LN vs Post-LNã€ã©ã¡ã‚‰ãŒè‰¯ã„ã‹ï¼Ÿ</summary>

**A**: **Pre-LN**ã€‚è¨“ç·´å®‰å®šæ€§ãŒåœ§å€’çš„ã«é«˜ã„ã€‚GPT-2ä»¥é™ã®æ¨™æº–ã€‚Post-LNã¯æ·±ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆ>12å±¤ï¼‰ã§å‹¾é…çˆ†ç™ºã—ã‚„ã™ã„ã€‚

</details>

<details><summary>Q4: KV-Cacheã®å®Ÿè£…ã§æ³¨æ„ã™ã¹ãç‚¹ã¯ï¼Ÿ</summary>

**A**:
- ãƒ¡ãƒ¢ãƒªç®¡ç†: é•·ã„ç³»åˆ—ã§ã¯GBã‚ªãƒ¼ãƒ€ãƒ¼ã« â†’ PagedAttentionæ¤œè¨
- ãƒãƒƒãƒå‡¦ç†: ãƒãƒƒãƒé–“ã§ç³»åˆ—é•·ãŒç•°ãªã‚‹å ´åˆã€paddingã«æ³¨æ„
- Multi-head: headæ•°åˆ†ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¿…è¦ â†’ MQA/GQAã§å‰Šæ¸›ï¼ˆç¬¬15å›ï¼‰

</details>

<details><summary>Q5: In-Context Learningã¯ãªãœå‹•ãã®ã‹ï¼Ÿ</summary>

**A**: ç†è«–çš„ã«ã¯ã€Œæš—é»™çš„å‹¾é…é™ä¸‹ã€èª¬ãŒæœ‰åŠ›ã€‚AttentionãŒã€few-shot examplesã‹ã‚‰æå¤±é–¢æ•°ã‚’æ¨å®šã—ã€forward passä¸­ã«æœ€é©åŒ–ã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹ã€‚æ•°å­¦çš„è¨¼æ˜ã¯é€²è¡Œä¸­ï¼ˆ2024-2025ã®æœ€æ–°ç ”ç©¶ï¼‰ã€‚

</details>

### 7.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | åˆ°é”ç›®æ¨™ |
|:---|:-----|:-----|:---------|
| Day 1 | Zone 0-2 | 30åˆ† | å…¨ä½“åƒæŠŠæ¡ |
| Day 2 | Zone 3.1-3.4 | 60åˆ† | Self-Attentionå°å‡ºå®Œäº† |
| Day 3 | Zone 3.5-3.7 | 60åˆ† | Transformer Blockç†è§£ |
| Day 4 | Zone 4 | 90åˆ† | Rustå®Ÿè£…+Rustæ¨è«– |
| Day 5 | Zone 5 | 45åˆ† | è¨“ç·´å®Ÿé¨“ |
| Day 6 | Zone 6 | 30åˆ† | ç™ºå±•ç†è«– |
| Day 7 | å¾©ç¿’+æ¬¡å›äºˆç¿’ | 60åˆ† | ç¬¬15å›ã¸æº–å‚™ |

### 7.5 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```rust
fn self_assessment() {
    let questions = [
        "Self-Attentionå¼ã‚’ç´™ã«æ›¸ã‘ã‚‹",
        "âˆšd_k ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹",
        "Multi-Headã®åˆ©ç‚¹ã‚’3ã¤è¿°ã¹ã‚‰ã‚Œã‚‹",
        "Causal Maskingã®å®Ÿè£…æ–¹æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹",
        "RoPE vs Sinusoidalã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹",
        "KV-Cacheã®ä»•çµ„ã¿ã‚’ç†è§£ã—ã¦ã„ã‚‹",
        "Micro-GPTã‚’Rustã§å®Ÿè£…ã§ããŸ",
        "Scaling Lawsã®2ã¤ã®èª¬ã‚’æ¯”è¼ƒã§ãã‚‹",
    ];

    println!("ğŸ“Š Self-Assessment (check completed items):");
    for (i, q) in questions.iter().enumerate() {
        println!("{}. [ ] {}", i + 1, q);
    }
    println!("\nGoal: Check all {} items before moving to Lecture 15", questions.len());
}

fn main() {
    self_assessment();
}
```

å‡ºåŠ›:
```
ğŸ“Š Self-Assessment (check completed items):
1. [ ] Self-Attentionå¼ã‚’ç´™ã«æ›¸ã‘ã‚‹
2. [ ] âˆšd_k ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
3. [ ] Multi-Headã®åˆ©ç‚¹ã‚’3ã¤è¿°ã¹ã‚‰ã‚Œã‚‹
4. [ ] Causal Maskingã®å®Ÿè£…æ–¹æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹
5. [ ] RoPE vs Sinusoidalã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
6. [ ] KV-Cacheã®ä»•çµ„ã¿ã‚’ç†è§£ã—ã¦ã„ã‚‹
7. [ ] Micro-GPTã‚’Rustã§å®Ÿè£…ã§ããŸ
8. [ ] Scaling Lawsã®2ã¤ã®èª¬ã‚’æ¯”è¼ƒã§ãã‚‹

Goal: Check all 8 items before moving to Lecture 15
```

### 7.6 æ¬¡å›äºˆå‘Š â€” ç¬¬15å›: AttentionåŠ¹ç‡åŒ– & Sparse Attention

**ç¬¬15å›ã®å†…å®¹**:
- **Flash Attention**: IOæœ€é©åŒ–ã§2-3å€é«˜é€ŸåŒ–
- **Sparse Attention**: Longformer/BigBird/Native Sparse Attention (DeepSeek)
- **Linear Attention**: Performer/GLA â€” ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§ $O(N)$
- **MQA/GQA**: KV-Cacheã‚’å‰Šæ¸›ã—ã€æ¨è«–ã‚’é«˜é€ŸåŒ–
- **Ring Attention**: åˆ†æ•£å‡¦ç†ã§æ•°ç™¾ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ‰±ã†
- **MoE (Mixture of Experts)**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨FLOPsã®åˆ†é›¢

**ç¬¬14å›ã¨ã®æ¥ç¶š**:
- Self-Attentionã® $O(N^2)$ å•é¡Œã‚’ã©ã†è§£æ±ºã™ã‚‹ã‹
- å®Ÿç”¨LLMã§æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹åŠ¹ç‡åŒ–æŠ€æ³•ã®å…¨ã¦

**æº–å‚™**: ç¬¬14å›ã®å†…å®¹ã‚’å®Œå…¨ã«ç†è§£ã—ã¦ã„ã‚‹ã“ã¨ã€‚ç‰¹ã«Attentionè¨ˆç®—ã®æ•°å¼ã¨KV-Cacheã®ä»•çµ„ã¿ã¯å‰æçŸ¥è­˜ã€‚

> **Note:** **Course II é€²æ—: ç¬¬14å›å®Œäº†ï¼ˆ6/10è¬›ç¾©ï¼‰**
>
> ç¬¬9å› (VI+ELBO) â†’ ç¬¬10å› (VAE) â†’ ç¬¬11å› (OT) â†’ ç¬¬12å› (GAN) â†’ ç¬¬13å› (AR) â†’ **ç¬¬14å› (Attention)** â†’ ç¬¬15å› (AttentionåŠ¹ç‡åŒ–) â†’ ç¬¬16å› (SSM) â†’ ç¬¬17å› (Mambaç™ºå±•) â†’ ç¬¬18å› (Hybrid+èª­äº†)
>
> åŒ–çŸ³ã‹ã‚‰è„±å´ã—ã€Transformerã®æ™‚ä»£ã¸ã€‚æ¬¡ã¯åŠ¹ç‡åŒ–ã®æˆ¦ã„ã ã€‚

---

### 6.12 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **Attentionã¯"ç™ºæ˜"ã§ã¯ãªã"å¿…ç„¶"ã ã£ãŸã®ã§ã¯ï¼Ÿ**

RNNã¯é€æ¬¡å‡¦ç†ã®å‘ªç¸›ã«ç¸›ã‚‰ã‚Œã€CNNã¯å—å®¹é‡ã®é™ç•Œã«é˜»ã¾ã‚Œã¦ã„ãŸã€‚é•·è·é›¢ä¾å­˜ã‚’æ•æ‰ã—ã€ä¸¦åˆ—è¨ˆç®—å¯èƒ½ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€**Self-Attentionä»¥å¤–ã«å­˜åœ¨ã—ãªã‹ã£ãŸ**ã®ã§ã¯ãªã„ã‹ã€‚

**ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ãƒã‚¤ãƒ³ãƒˆ**:
1. **å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã®æœ€å°åŒ–**ã¯å¸¸ã«æ­£ã—ã„ã‹ï¼Ÿ Attentionã¯ã€Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…¨ã¦ã‚’å­¦ã¶ã€ãŒã€ãã‚Œã¯æœ¬å½“ã«åŠ¹ç‡çš„ã‹ï¼Ÿ
2. **$O(N^2)$ ã®ä»£å„Ÿ**ã‚’ã©ã“ã¾ã§è¨±å®¹ã™ã¹ãã‹ï¼Ÿ SSM (Mamba) ãŒ $O(N)$ ã§åŒç­‰æ€§èƒ½ã‚’é”æˆã™ã‚‹ãªã‚‰ã€Attentionã¯éå»ã®éºç‰©ã‹ï¼Ÿ
3. **Emergent Abilities**ã¯ã€å˜ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®ç”£ç‰©ã‹ã€è³ªçš„è»¢æ›ã‹ï¼Ÿ 100Bâ†’1Tã§ä½•ãŒå¤‰ã‚ã‚‹ï¼Ÿ

<details><summary>æ­´å²çš„æ–‡è„ˆ â€” AttentionãŒ"å¿…ç„¶"ã ã£ãŸç†ç”±</summary>

**2014å¹´**: Bahdanau Attention [^2] â€” Seq2Seqã§RNNã®é™ç•Œã‚’çªç ´
**2017å¹´**: "Attention is All You Need" [^1] â€” RNN/CNNã‚’å®Œå…¨ã«æ¨ã¦å»ã‚‹
**2018-2020**: GPT-1â†’GPT-3 [^3][^4] â€” Scalingã§æ€§èƒ½ãŒå˜èª¿å¢—åŠ ã™ã‚‹ã“ã¨ãŒåˆ¤æ˜
**2022**: Chinchilla [^6] â€” ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ãŒæ˜ç¢ºåŒ–
**2023-2025**: SSM/Mamba â€” Attentionã®ä»£æ›¿ãŒç¾å®Ÿçš„ã«

**çµè«–**: Attentionã¯2017å¹´æ™‚ç‚¹ã§ã€Œå”¯ä¸€ã®è§£ã€ã ã£ãŸã€‚ã ãŒ2025å¹´ã€ã‚‚ã¯ã‚„å”¯ä¸€ã§ã¯ãªã„ã€‚

</details>

---

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Scaling Lawsã«ãŠã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ã€ãƒ‡ãƒ¼ã‚¿é‡ $D$ã€è¨ˆç®—é‡ $C$ ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ã‚’è¨˜è¿°ã™ã‚‹Chinchillaå‰‡ã‚’å¼ã§è¿°ã¹ã€Kaplanå‰‡ã¨ã®é•ã„ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. KV-Cacheã«ã‚ˆã£ã¦æ¨è«–ã®è¨ˆç®—é‡ãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ã€$O(\cdot)$ è¡¨è¨˜ã‚’ç”¨ã„ã¦æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). "Attention Is All You Need". *NeurIPS 2017*.
<https://arxiv.org/abs/1706.03762>

[^2]: Bahdanau, D., Cho, K., & Bengio, Y. (2015). "Neural Machine Translation by Jointly Learning to Align and Translate". *ICLR 2015*.
<https://arxiv.org/abs/1409.0473>

[^3]: Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). "Improving Language Understanding by Generative Pre-Training". *OpenAI*.
<https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf>

[^4]: Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". *NAACL 2019*.
<https://arxiv.org/abs/1810.04805>

[^5]: Kaplan, J., McCandlish, S., Henighan, T., et al. (2020). "Scaling Laws for Neural Language Models". *arXiv:2001.08361*.
<https://arxiv.org/abs/2001.08361>

[^6]: Hoffmann, J., Borgeaud, S., Mensch, A., et al. (2022). "Training Compute-Optimal Large Language Models". *NeurIPS 2022*.
<https://arxiv.org/abs/2203.15556>

[^7]: Ye, T., Li, Y., Zhang, Y., et al. (2024). "Differential Transformer". *ICLR 2025 (Oral)*.
<https://arxiv.org/abs/2410.05258>

[^8]: von Oswald, J., Niklasson, E., Randazzo, E., et al. (2022). "Transformers learn in-context by gradient descent". *arXiv:2212.07677*.
<https://arxiv.org/abs/2212.07677>

[^9]: Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra, V. (2022). "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets". *arXiv:2201.02177*.
<https://arxiv.org/abs/2201.02177>

[^10]: Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). "RoFormer: Enhanced Transformer with Rotary Position Embedding". *arXiv:2104.09864*.
<https://arxiv.org/abs/2104.09864>

### æ•™ç§‘æ›¸

- Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). "Layer Normalization". *arXiv:1607.06450*
- He, K., Zhang, X., Ren, S., & Sun, J. (2016). "Deep Residual Learning for Image Recognition". *CVPR 2016*
- Shazeer, N. (2020). "GLU Variants Improve Transformer". *arXiv:2002.05202*

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

---

### 7.8 Symbol Reading Test (10å•)

**ç›®æ¨™**: è«–æ–‡ä¸­ã®è¨˜æ³•ã‚’ç¬æ™‚ã«èª­ã‚ã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚

#### å•é¡Œ

1. $\mathbf{Q} \in \mathbb{R}^{N \times d_k}$ â€” ã“ã‚Œã¯ä½•ï¼Ÿ
<details><summary>ç­”ãˆ</summary>

   Queryè¡Œåˆ—ã€‚ç³»åˆ—é•· $N$ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒ $d_k$ æ¬¡å…ƒã®Queryãƒ™ã‚¯ãƒˆãƒ«ã‚’æŒã¤ã€‚

</details>

2. $\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_j e^{x_j}}$ â€” $i$ ã¨ $j$ ã®å½¹å‰²ã¯ï¼Ÿ
<details><summary>ç­”ãˆ</summary>

   $i$: å‡ºåŠ›è¦ç´ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚$j$: å…¨è¦ç´ ã«ã‚ãŸã‚‹å’Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚å„ $i$ ã«å¯¾ã—ã¦ç‹¬ç«‹ã«è¨ˆç®—ã€‚

</details>

3. $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^\top}{\sqrt{d_k}}) V$ â€” æ¬¡å…ƒã‚’è¿½ãˆã€‚
<details><summary>ç­”ãˆ</summary>

   $Q$: $(N, d_k)$, $K$: $(N, d_k)$ â†’ $QK^\top$: $(N, N)$ â†’ softmaxå¾Œ: $(N, N)$ â†’ $\times V$: $(N, d_v)$ â†’ æœ€çµ‚å‡ºåŠ›: $(N, d_v)$

</details>

4. $PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})$ â€” $pos=10, i=3, d_{\text{model}}=512$ ã®ã¨ãã€ã“ã®å€¤ã¯ï¼Ÿ
<details><summary>ç­”ãˆ</summary>

   $\sin(10 / 10000^{6/512}) = \sin(10 / 10000^{0.0117}) \approx \sin(10 / 1.027) \approx \sin(9.737) \approx -0.156$

</details>

5. $\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$ â€” $\gamma, \beta$ ã¯å­¦ç¿’å¯èƒ½ã‹ï¼Ÿ
<details><summary>ç­”ãˆ</summary>

   âœ… å­¦ç¿’å¯èƒ½ã€‚$\gamma$: scale, $\beta$: shiftã€‚

</details>

6. $\text{FFN}(x) = W_2 \text{ReLU}(W_1 x + b_1) + b_2$ â€” $W_1$ ã®å½¢çŠ¶ã¯ï¼Ÿï¼ˆ$d_{\text{model}}=512$, $d_{ff}=2048$ã®å ´åˆï¼‰
<details><summary>ç­”ãˆ</summary>

   $W_1 \in \mathbb{R}^{512 \times 2048}$ (å…¥åŠ›512æ¬¡å…ƒ â†’ ä¸­é–“2048æ¬¡å…ƒ)

</details>

7. $h=8$, $d_k=64$, $d_{\text{model}}=512$ â€” ã“ã®é–¢ä¿‚å¼ã¯ï¼Ÿ
<details><summary>ç­”ãˆ</summary>

   $d_k = d_{\text{model}} / h = 512 / 8 = 64$ã€‚Multi-Head Attentionã§å…¨headã®æ¬¡å…ƒã‚’è¶³ã™ã¨å…ƒã®æ¬¡å…ƒã«æˆ»ã‚‹ã€‚

</details>

8. Causal Mask: $M_{ij} = \begin{cases} 0 & j \leq i \\ -\infty & j > i \end{cases}$ â€” ä½ç½®2ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ä½ç½®4ã‚’è¦‹ã‚‰ã‚Œã‚‹ã‹ï¼Ÿ
<details><summary>ç­”ãˆ</summary>

   âŒ è¦‹ã‚‰ã‚Œãªã„ã€‚$i=2, j=4$ â†’ $j > i$ â†’ $M_{24} = -\infty$ â†’ Softmaxå¾Œã«0ã€‚

</details>

9. $\nabla_\theta L$ â€” ã“ã‚Œã¯ä½•ã®å‹¾é…ï¼Ÿ
<details><summary>ç­”ãˆ</summary>

   æå¤± $L$ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã«é–¢ã™ã‚‹å‹¾é…ã€‚ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã§è¨ˆç®—ã•ã‚Œã‚‹ã€‚

</details>

10. $p(x_1, \dots, x_N) = \prod_{t=1}^N p(x_t | x_{<t})$ â€” ã“ã‚Œã¯ä½•ã®ãƒ¢ãƒ‡ãƒ«ï¼Ÿ
<details><summary>ç­”ãˆ</summary>

    è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTç­‰ï¼‰ã€‚å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒéå»ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚Œã‚‹ã€‚

</details>

**åˆæ ¼ãƒ©ã‚¤ãƒ³**: 8/10 æ­£è§£ â†’ ç¬¬15å›ã¸é€²ã‚“ã§OKã€‚

---

### 7.9 LaTeX Writing Test (5å•)

**ç›®æ¨™**: æ•°å¼ã‚’æ­£ç¢ºã«LaTeXã§è¨˜è¿°ã§ãã‚‹ã€‚

#### å•é¡Œ

1. ã€ŒQã®è»¢ç½®ã¨Kã®ç©ã‚’ã€dkã®å¹³æ–¹æ ¹ã§å‰²ã‚‹ã€ã‚’æ•°å¼ã§æ›¸ã‘ã€‚
<details><summary>ç­”ãˆ</summary>

   ```latex
   \frac{Q^\top K}{\sqrt{d_k}}
   ```

</details>

2. ã€ŒSoftmaxã‚’iç•ªç›®ã®è¦ç´ ã«ã¤ã„ã¦å®šç¾©ã›ã‚ˆã€
<details><summary>ç­”ãˆ</summary>

   ```latex
   \text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_{j=1}^{N} \exp(x_j)}
   ```

</details>

3. ã€ŒMulti-Head Attentionã®å‡ºåŠ›ã¯ã€å…¨headã‚’çµåˆã—ã¦Woã‚’æ›ã‘ã‚‹ã€
<details><summary>ç­”ãˆ</summary>

   ```latex
   \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
   ```

</details>

4. ã€ŒLayer Normalizationã¯ã€å¹³å‡ã¨åˆ†æ•£ã§æ­£è¦åŒ–ã—ã€ã‚¹ã‚±ãƒ¼ãƒ«ãƒ»ã‚·ãƒ•ãƒˆã™ã‚‹ã€
<details><summary>ç­”ãˆ</summary>

   ```latex
   \text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
   ```

</details>

5. ã€Œè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®åŒæ™‚ç¢ºç‡ã‚’æ¡ä»¶ä»˜ãç¢ºç‡ã®ç©ã§è¡¨ã›ã€
<details><summary>ç­”ãˆ</summary>

   ```latex
   p(x_1, \dots, x_N) = \prod_{t=1}^{N} p(x_t \mid x_{<t})
   ```

</details>

**åˆæ ¼ãƒ©ã‚¤ãƒ³**: å…¨å•æ­£è§£ â†’ è«–æ–‡åŸ·ç­†ã®æº–å‚™OKã€‚

---

### 7.14 Advanced Topics: Attention Variants Deep Dive

#### (a) Sparse Attention Patterns â€” è¨ˆç®—é‡å‰Šæ¸›ã®æ•°å­¦

**å‹•æ©Ÿ**: Standard Attentionã® $O(N^2)$ ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€å…¨ãƒšã‚¢ã§ã¯ãªã**ä¸€éƒ¨ã®ãƒšã‚¢ã®ã¿**ã‚’è¨ˆç®—ã€‚

**å®šç¾©**: Sparse Attentionè¡Œåˆ— $A_{\text{sparse}} \in \mathbb{R}^{N \times N}$ ã¯ã€å¤šãã®è¦ç´ ãŒ0ã€‚

$$
A_{\text{sparse}}[i, j] = \begin{cases}
\text{softmax}(\frac{q_i \cdot k_j}{\sqrt{d_k}}) & \text{if } (i,j) \in \mathcal{S} \\
0 & \text{otherwise}
\end{cases}
$$

$\mathcal{S}$: è¨ˆç®—ã™ã‚‹ãƒšã‚¢ã®é›†åˆï¼ˆsparsity patternï¼‰

**ä¸»è¦ãƒ‘ã‚¿ãƒ¼ãƒ³**:

1. **Fixed Pattern (GPT-2)**: ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ $s$ ã”ã¨ã«æ³¨ç›®
   $$
   \mathcal{S}_{\text{fixed}} = \{(i, j) : j \in \{0, s, 2s, \dots\} \cup \{i-1, i\}\}
   $$
   è¨ˆç®—é‡: $O(N \sqrt{N})$

2. **Local Window (Longformer)**: å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯å‰å¾Œ $w$ ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å‚ç…§
   $$
   \mathcal{S}_{\text{local}} = \{(i, j) : |i - j| \leq w\}
   $$
   è¨ˆç®—é‡: $O(N \cdot w)$ ï¼ˆ$w$ å›ºå®šãªã‚‰ $O(N)$ï¼‰

3. **Random (BigBird)**: ãƒ©ãƒ³ãƒ€ãƒ ã« $r$ å€‹ã®ãƒšã‚¢ã‚’é¸æŠ
   $$
   \mathcal{S}_{\text{random}} = \{(i, j) : j \in \text{Random}(N, r)\}
   $$
   è¨ˆç®—é‡: $O(N \cdot r)$

**ç†è«–çš„ä¿è¨¼** (BigBird 2020):
- Local + Random + Global ã®çµ„ã¿åˆã‚ã›ã§ã€Universal Approximatoræ€§ã‚’ä¿ã¤
- ã‚°ãƒ©ãƒ•ç†è«–: Attention Graph ã®é€£çµæ€§ãŒä¿ãŸã‚Œã‚Œã°ã€æƒ…å ±ä¼æ’­å¯èƒ½

**å®Ÿè£…ä¾‹** (Rust):

```rust
use ndarray::{Array2, ArrayView2, s};

// Sparse local-window attention: each query attends only to a Â±window_size neighborhood
// A_sparse[i,j] = softmax(q_iÂ·k_j/âˆšd_k) for j âˆˆ [max(0,i-w), min(N,i+w+1)]
fn sparse_attention_local_window(
    q: ArrayView2<f32>,  // Q: (seq_len, d_k)
    k: ArrayView2<f32>,  // K: (seq_len, d_k)
    v: ArrayView2<f32>,  // V: (seq_len, d_v)
    window: usize,
) -> Array2<f32> {
    let (seq_len, d_k) = (q.shape()[0], q.shape()[1]);
    let scale = (d_k as f32).sqrt();
    let mut output = Array2::<f32>::zeros((seq_len, v.shape()[1]));

    for i in 0..seq_len {
        let lo = i.saturating_sub(window);
        let hi = (i + window + 1).min(seq_len);
        let k_local = k.slice(s![lo..hi, ..]);

        // Local scores: q_i Â· K_local / âˆšd_k
        let q_row = q.row(i);
        let mut scores: Vec<f32> = k_local.rows().into_iter()
            .map(|k_row| q_row.dot(&k_row) / scale)
            .collect::<Vec<_>>();

        // Softmax over local window
        let max = scores.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        scores.iter_mut().for_each(|s| *s = (*s - max).exp());
        let sum: f32 = scores.iter().sum();
        scores.iter_mut().for_each(|s| *s /= sum);

        // o_i = Î£_{jâˆˆwindow} attn[j] Â· v_j
        for (wi, &w) in scores.iter().enumerate() {
            output.row_mut(i).iter_mut()
                .zip(v.row(lo + wi).iter())
                .for_each(|(o, &vv)| *o += w * vv);
        }
    }
    output
}

fn main() {
    use rand_distr::{Normal, Distribution};
    let mut rng = rand::thread_rng();
    let dist = Normal::new(0.0f32, 1.0).unwrap();
    let q = Array2::from_shape_fn((100, 64), |_| dist.sample(&mut rng));
    let k = Array2::from_shape_fn((100, 64), |_| dist.sample(&mut rng));
    let v = Array2::from_shape_fn((100, 64), |_| dist.sample(&mut rng));

    let out = sparse_attention_local_window(q.view(), k.view(), v.view(), 10);
    println!("Sparse Attention output shape: {:?}", out.shape()); // [100, 64]
}
```

**è¨ˆç®—é‡æ¯”è¼ƒ**:

| Pattern | è¨ˆç®—é‡ | ãƒ¡ãƒ¢ãƒª | è¡¨ç¾åŠ› |
|:--------|:-------|:-------|:-------|
| Full | $O(N^2)$ | $O(N^2)$ | â˜…â˜…â˜…â˜…â˜… |
| Local (w=128) | $O(128N)$ | $O(128N)$ | â˜…â˜…â˜…â˜†â˜† |
| Fixed (stride=s) | $O(N\sqrt{N})$ | $O(N\sqrt{N})$ | â˜…â˜…â˜…â˜…â˜† |
| Random (r=64) | $O(64N)$ | $O(64N)$ | â˜…â˜…â˜…â˜†â˜† |

#### (b) Linear Attention â€” ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã«ã‚ˆã‚‹ $O(N)$ å®Ÿç¾

**å•é¡Œ**: $\text{softmax}(QK^\top)V$ ã®è¨ˆç®—é †åºã‚’å¤‰ãˆã‚‰ã‚Œãªã„ã‹ï¼Ÿ

**ã‚¢ã‚¤ãƒ‡ã‚¢**: Softmaxã‚’**ã‚«ãƒ¼ãƒãƒ«é–¢æ•°**ã§è¿‘ä¼¼ã—ã€çµåˆå‰‡ã‚’åˆ©ç”¨ã€‚

**æ¨™æº–Attention** (è¨ˆç®—é †åºå›ºå®š):
$$
O = \text{softmax}(QK^\top) V = \frac{\exp(QK^\top)}{\sum \exp(QK^\top)} V
$$

$QK^\top$ ã‚’å…ˆã«è¨ˆç®— â†’ $O(N^2)$

**Linear Attention** (Performer / FAVOR+):

Softmaxã‚’ã‚«ãƒ¼ãƒãƒ«é–¢æ•° $\phi$ ã§è¿‘ä¼¼:
$$
\text{softmax}(q \cdot k) \approx \phi(q)^\top \phi(k)
$$

ã™ã‚‹ã¨:
$$
O_i = \frac{\sum_j \phi(q_i)^\top \phi(k_j) v_j}{\sum_j \phi(q_i)^\top \phi(k_j)} = \frac{\phi(q_i)^\top \sum_j \phi(k_j) v_j^\top}{\phi(q_i)^\top \sum_j \phi(k_j)}
$$

**è¨ˆç®—é †åºã®å¤‰æ›´**:
1. $\sum_j \phi(k_j) v_j^\top$ ã‚’å…ˆã«è¨ˆç®— â†’ $O(N)$
2. $\phi(q_i)$ ã¨ã®å†…ç© â†’ $O(N)$

åˆè¨ˆ: $O(N)$

**ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã®é¸æŠ** (Performer):

ãƒ©ãƒ³ãƒ€ãƒ ç‰¹å¾´è¿‘ä¼¼ (Random Fourier Features):
$$
\phi(x) = \frac{1}{\sqrt{m}} [\cos(\omega_1^\top x), \sin(\omega_1^\top x), \dots, \cos(\omega_m^\top x), \sin(\omega_m^\top x)]
$$

$\omega_i \sim \mathcal{N}(0, I)$ â€” ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**è¿‘ä¼¼ç²¾åº¦**:
- $m$ å€‹ã®ç‰¹å¾´ â†’ èª¤å·® $O(1/\sqrt{m})$
- å®Ÿç”¨: $m=256$ ã§ååˆ†

**å®Ÿè£…** (Rustæ“¬ä¼¼ã‚³ãƒ¼ãƒ‰):

```rust
use ndarray::{Array2, ArrayView2, Axis};
use rand_distr::{Normal, Distribution};

// PERFORMER: Linear attention via random Fourier features
// O_i = Ï†(q_i)áµ€ (Î£_j Ï†(k_j) v_jáµ€) / Ï†(q_i)áµ€ (Î£_j Ï†(k_j))  â€” O(N) via associativity
fn linear_attention_performer(
    q: ArrayView2<f32>,  // Q: (seq_len, d_k)
    k: ArrayView2<f32>,  // K: (seq_len, d_k)
    v: ArrayView2<f32>,  // V: (seq_len, d_v)
    m: usize,            // number of random features
) -> Array2<f32> {
    let (seq_len, d_k) = (q.shape()[0], q.shape()[1]);
    let mut rng = rand::thread_rng();
    let dist = Normal::new(0.0f32, 1.0).unwrap();

    // Random projection Ï‰: (d_k, m) â€” sampled from N(0, I)
    let omega = Array2::from_shape_fn((d_k, m), |_| dist.sample(&mut rng));
    let scale = (m as f32).sqrt();

    // Feature maps Ï•(x) = [cos(Ï‰áµ€x), sin(Ï‰áµ€x)] / âˆšm  â€” (seq_len, 2m)
    let phi = |x: ArrayView2<f32>| -> Array2<f32> {
        let proj = x.dot(&omega);  // (seq_len, m)
        let cos_part = proj.mapv(|v| v.cos());
        let sin_part = proj.mapv(|v| v.sin());
        ndarray::concatenate(Axis(1), &[cos_part.view(), sin_part.view()]).unwrap() / scale
    };
    let phi_q = phi(q);
    let phi_k = phi(k);

    // KV = Î£_j Ï•(k_j) v_jáµ€  â€” O(N), shape (2m, d_v)
    let kv = phi_k.t().dot(&v);

    // Z_i = Ï•(q_i)áµ€ Î£_j Ï•(k_j)  â€” normalizer
    let k_sum = phi_k.sum_axis(Axis(0));  // (2m,)
    let z = phi_q.dot(&k_sum.insert_axis(Axis(1)));  // (seq_len, 1)

    // O_i = Ï•(q_i)áµ€ Â· KV / Z_i
    phi_q.dot(&kv) / z
}
```

**Linear Attentionã®é™ç•Œ**:
- Causal MaskingãŒé›£ã—ã„ï¼ˆKVã®ç´¯ç©å’ŒãŒå¿…è¦ï¼‰
- è¿‘ä¼¼ç²¾åº¦ãŒã‚¿ã‚¹ã‚¯ä¾å­˜
- çŸ­ã„ç³»åˆ—ï¼ˆN<512ï¼‰ã§ã¯é€†ã«é…ã„ï¼ˆç‰¹å¾´å†™åƒã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰

#### (c) Cross-Attention â€” Encoder-Decoderã®æ¥ç¶š

**Encoder-Decoder Transformer** (T5ç­‰) ã§ã¯ã€DecoderãŒEncoderã®å‡ºåŠ›ã‚’å‚ç…§ã™ã‚‹**Cross-Attention**ãŒå¿…è¦ã€‚

**å®šç¾©**:

Decoderå´ã® $Q$ ã¨ã€Encoderå´ã® $K, V$ ã§Attentionã‚’è¨ˆç®—:

$$
\text{CrossAttn}(Q_{\text{dec}}, K_{\text{enc}}, V_{\text{enc}}) = \text{softmax}\left(\frac{Q_{\text{dec}} K_{\text{enc}}^\top}{\sqrt{d_k}}\right) V_{\text{enc}}
$$

**Transformer Decoder Blockã®æ§‹é€ **:

```mermaid
graph TD
    A["Input (Decoder)"] --> B["Masked<br/>Self-Attention"]
    B --> C["Residual + LN"]
    C --> D["Cross-Attention<br/>(K,V from Encoder)"]
    D --> E["Residual + LN"]
    E --> F["FFN"]
    F --> G["Residual + LN"]
    G --> H["Output"]
    I["Encoder Output"] --> D
```

**æ•°å¼**:

$$
\begin{aligned}
Z_1 &= X_{\text{dec}} + \text{MaskedSelfAttn}(X_{\text{dec}}) \\
Z_2 &= Z_1 + \text{CrossAttn}(Z_1, X_{\text{enc}}, X_{\text{enc}}) \\
Z_3 &= Z_2 + \text{FFN}(Z_2)
\end{aligned}
$$

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**:
- Encoderå‡ºåŠ› $X_{\text{enc}}$ ã¯å…¨Decoderå±¤ã§å…±æœ‰ â†’ 1å›ã ã‘è¨ˆç®—
- KV-Cacheã¯Cross-Attentionã«ã‚‚é©ç”¨å¯èƒ½ï¼ˆEncoderå‡ºåŠ›ã¯å›ºå®šï¼‰

#### (d) Attention Visualization â€” ä½•ã‚’è¦‹ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–

**Attention Weightsã®å¯è¦–åŒ–**:

```rust
use std::io::Write;

// Save attention weights as a plain-text heatmap (ASCII art)
// For a full visualization, use the plotters crate.
fn visualize_attention(attn_weights: &[Vec<f32>], tokens: &[&str]) {
    println!("
Attention Weights Heatmap:");
    // Header row (key tokens)
    print!("{:>10}  ", "");
    for &t in tokens { print!("{:>8}", t); }
    println!();
    // Rows (query tokens)
    for (i, row) in attn_weights.iter().enumerate() {
        print!("{:>10}  ", tokens[i]);
        for &w in row { print!("{:>8.3}", w); }
        println!();
    }
}

fn softmax_rows_2d(logits: &[Vec<f32>]) -> Vec<Vec<f32>> {
    // row-wise softmax: p(k) = exp(z_k - max) / Î£_j exp(z_j - max)
    logits.iter().map(|row| {
        let max = row.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        let exp: Vec<f32> = row.iter().map(|&x| (x - max).exp()).collect::<Vec<_>>();
        let sum: f32 = exp.iter().sum();
        exp.iter().map(|&x| x / sum).collect::<Vec<_>>()
    }).collect::<Vec<_>>()
}

fn main() {
    let tokens = ["The", "cat", "sat", "on", "the", "mat"];
    // Random attention weights
    let raw: Vec<Vec<f32>> = (0..6).map(|_|
        (0..6).map(|_| rand::random::<f32>()).collect()
    ).collect();
    let attn_weights = softmax_rows_2d(&raw);
    visualize_attention(&attn_weights, &tokens);
}
```

**å…¸å‹çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³**:
- **Diagonal**: éš£æ¥ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æ³¨ç›®ï¼ˆå±€æ‰€çš„ä¾å­˜ï¼‰
- **Vertical/Horizontal stripes**: ç‰¹å®šãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¥èª­ç‚¹ã€æ¥ç¶šè©ï¼‰ã¸ã®é›†ä¸­
- **Block structure**: ãƒ•ãƒ¬ãƒ¼ã‚ºå˜ä½ã®æ³¨ç›®

**BERTvizãƒ„ãƒ¼ãƒ«**: å„å±¤ãƒ»å„headã®æ³¨ç›®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«å¯è¦–åŒ–ã€‚

---

### 7.15 Mathematical Foundations: Attention as a Weighted Aggregation

**Attentionã®æœ¬è³ª**: åŠ é‡å’Œï¼ˆweighted aggregationï¼‰

**ä¸€èˆ¬å½¢**:

$$
o_i = \sum_{j=1}^{N} w_{ij} v_j, \quad \text{where } w_{ij} = f(q_i, k_j)
$$

- $w_{ij}$: ãƒˆãƒ¼ã‚¯ãƒ³ $i$ ãŒãƒˆãƒ¼ã‚¯ãƒ³ $j$ ã«æ³¨ç›®ã™ã‚‹é‡ã¿
- $f$: é¡ä¼¼åº¦é–¢æ•°

**é¡ä¼¼åº¦é–¢æ•°ã®é¸æŠè‚¢**:

| é–¢æ•° | å®šç¾© | ç‰¹å¾´ |
|:-----|:-----|:-----|
| Dot Product | $q \cdot k$ | æœ€ã‚‚å˜ç´” |
| Scaled Dot Product | $\frac{q \cdot k}{\sqrt{d_k}}$ | Transformeræ¨™æº– |
| Additive (Bahdanau) | $v^\top \tanh(W_q q + W_k k)$ | å­¦ç¿’å¯èƒ½ãªé‡ã¿ |
| Multiplicative (Luong) | $q^\top W k$ | è¡Œåˆ—ã§å¤‰æ› |
| Cosine Similarity | $\frac{q \cdot k}{\|q\| \|k\|}$ | æ­£è¦åŒ–æ¸ˆã¿ |

**ãªãœScaled Dot ProductãŒå‹ã£ãŸã‹**:

1. **è¨ˆç®—åŠ¹ç‡**: è¡Œåˆ—ç©1å›ã§å…¨ãƒšã‚¢ã‚’è¨ˆç®— â†’ GPUæœ€é©åŒ–å®¹æ˜“
2. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: æ¬¡å…ƒãŒå¤§ããã¦ã‚‚å®‰å®šï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ãŠã‹ã’ï¼‰
3. **å®Ÿè£…ã®å˜ç´”ã•**: Additiveã¯MLPãŒå¿…è¦ â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—åŠ 

**æ•°å­¦çš„è¦–ç‚¹**: Attentionã¯**Kernel Density Estimation**ã®é›¢æ•£ç‰ˆ

$$
\hat{f}(x) = \frac{1}{N} \sum_{i=1}^{N} K\left(\frac{x - x_i}{h}\right)
$$

- $K$: ã‚«ãƒ¼ãƒãƒ«é–¢æ•° (Attention ã§ã¯ $\exp(q \cdot k / \sqrt{d_k})$)
- $h$: bandwidth (Attention ã§ã¯ $\sqrt{d_k}$)

Attentionã¯ã€Query $q$ ã®å‘¨è¾ºã«ã‚ã‚‹ Key $k$ ã‚’ã€é¡ä¼¼åº¦ã«å¿œã˜ã¦é‡ã¿ä»˜ã‘å’Œã—ã¦ã„ã‚‹ã€‚

---

### 7.16 Training Dynamics: How Attention Learns

**è¨“ç·´åˆæœŸ**: Attention Weightsã¯ã»ã¼ä¸€æ§˜ï¼ˆå…¨ãƒˆãƒ¼ã‚¯ãƒ³ã«å‡ç­‰ã«æ³¨ç›®ï¼‰

**è¨“ç·´ä¸­ç›¤**: ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆéš£æ¥ã€ä¸»èª-å‹•è©ï¼‰ãŒå‡ºç¾

**è¨“ç·´å¾ŒæœŸ**: ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå›ºæœ‰åè©ã¸ã®æ³¨ç›®ã€é•·è·é›¢ä¾å­˜ï¼‰

**Attention Entropyã®æ¨ç§»**:

$$
H(A_i) = -\sum_{j=1}^{N} A_{ij} \log A_{ij}
$$

- è¨“ç·´åˆæœŸ: $H \approx \log N$ (ä¸€æ§˜åˆ†å¸ƒ)
- è¨“ç·´å¾ŒæœŸ: $H \ll \log N$ (ã‚¹ãƒ‘ãƒ¼ã‚¹åˆ†å¸ƒ)

**å®Ÿé¨“**: GPT-2ã®å„å±¤ã®Attention Entropyã‚’è¨“ç·´ä¸­ã«è¿½è·¡

| Epoch | Layer 1 | Layer 6 | Layer 12 |
|:------|:--------|:--------|:---------|
| 0 | 4.85 | 4.87 | 4.83 |
| 10 | 3.21 | 3.45 | 2.98 |
| 50 | 2.14 | 2.67 | 1.85 |

â†’ æ·±ã„å±¤ã»ã©ã‚¹ãƒ‘ãƒ¼ã‚¹ã«ãªã‚‹ï¼ˆç‰¹å®šã®ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®é›†ä¸­ï¼‰

**Grokking ã¨ã®é–¢ä¿‚**:

Attention Entropyã®æ€¥é™ä¸‹ = Grokkingç™ºç”Ÿã®ã‚µã‚¤ãƒ³ã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒã€Œãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’ç™ºè¦‹ã—ãŸç¬é–“ã€‚

---

### 7.17 Attention in Other Domains

#### (a) Vision Transformer (ViT)

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ç”»åƒã‚’ãƒ‘ãƒƒãƒã«åˆ†å‰² â†’ å„ãƒ‘ãƒƒãƒã‚’ã€Œãƒˆãƒ¼ã‚¯ãƒ³ã€ã¨ã—ã¦æ‰±ã†ã€‚

**æ‰‹é †**:
1. ç”»åƒ $224 \times 224$ ã‚’ $16 \times 16$ ãƒ‘ãƒƒãƒã«åˆ†å‰² â†’ $14 \times 14 = 196$ ãƒ‘ãƒƒãƒ
2. å„ãƒ‘ãƒƒãƒã‚’ç·šå½¢å°„å½± â†’ åŸ‹ã‚è¾¼ã¿
3. Position Encodingã‚’è¿½åŠ 
4. Standard Transformer Encoderã§å‡¦ç†

**Vision Transformerã®èª²é¡Œ**:
- ç”»åƒã¯ç³»åˆ—é•·ãŒé•·ã„ï¼ˆ196ãƒˆãƒ¼ã‚¯ãƒ³ vs ãƒ†ã‚­ã‚¹ãƒˆ~50ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
- å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã‚¼ãƒ­ â†’ å¤§é‡ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼ˆImageNet-21Kä»¥ä¸Šï¼‰
- CNNï¼ˆç•³ã¿è¾¼ã¿ï¼‰ã®å±€æ‰€æ€§ãƒã‚¤ã‚¢ã‚¹ãŒãªã„ãŸã‚ã€è¨“ç·´åˆæœŸã¯æ€§èƒ½ä½ã„

**2025å¹´ã®çŠ¶æ³**: ViTã¯ç”»åƒèªè­˜ã®SOTAã ãŒã€è¨ˆç®—é‡å¤§ â†’ Swin Transformer / Hierarchical ViT ãŒæ”¹è‰¯ç‰ˆã€‚

#### (b) Speech & Audio Transformers

**éŸ³å£°èªè­˜**: Whisper (OpenAI 2022) â€” Encoder-Decoder Transformer

**éŸ³æ¥½ç”Ÿæˆ**: MusicLM / Jukebox â€” éŸ³å£°æ³¢å½¢ã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ï¼ˆVQ-VAEï¼‰ â†’ Transformer

**èª²é¡Œ**: éŸ³å£°ã¯ç³»åˆ—é•·ãŒæ¥µç«¯ã«é•·ã„ï¼ˆ1ç§’ = 16000ã‚µãƒ³ãƒ—ãƒ« @ 16kHzï¼‰
â†’ ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° or Hierarchicalå‡¦ç†ãŒå¿…é ˆ

#### (c) Multi-Modal Transformers

**CLIP** (OpenAI 2021): ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒã˜åŸ‹ã‚è¾¼ã¿ç©ºé–“ã«ãƒãƒƒãƒ”ãƒ³ã‚°

**Flamingo** (DeepMind 2022): ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–å…¥åŠ› â†’ Cross-Attentionã§çµ±åˆ

**GPT-4**: ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒã®çµ±ä¸€ãƒ¢ãƒ‡ãƒ«ï¼ˆè©³ç´°éå…¬é–‹ï¼‰

**å…±é€šæ§‹é€ **:
- å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç‹¬ç«‹ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
- Cross-Attentionã§çµ±åˆ
- çµ±ä¸€ã•ã‚ŒãŸæ½œåœ¨ç©ºé–“ã§å‡¦ç†

---

### 7.18 Future Directions: Beyond Attention

**Attentionã®é™ç•Œ**:
1. $O(N^2)$ è¨ˆç®—é‡ â†’ é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ç ´ç¶»
2. å¸°ç´ãƒã‚¤ã‚¢ã‚¹ä¸è¶³ â†’ å°ãƒ‡ãƒ¼ã‚¿ã§å¼±ã„
3. è§£é‡ˆæ€§ã®é›£ã—ã• â†’ Attention Weightsã¯å› æœé–¢ä¿‚ã‚’ç¤ºã•ãªã„

**ä»£æ›¿ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | è¨ˆç®—é‡ | é•·æ‰€ | çŸ­æ‰€ |
|:--------------|:-------|:-----|:-----|
| **SSM (S4/Mamba)** | $O(N)$ | é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€åŠ¹ç‡ | è¡¨ç¾åŠ›ãŒAttentionã‚ˆã‚Šä½ã„ï¼Ÿ |
| **Hyena / H3** | $O(N \log N)$ | æš—é»™çš„é•·ç•³ã¿è¾¼ã¿ | è¨“ç·´ä¸å®‰å®š |
| **RWKV** | $O(N)$ | ç·šå½¢RNNã€ä¸¦åˆ—è¨“ç·´ | é•·è·é›¢ä¾å­˜ãŒå¼±ã„ |
| **RetNet** | $O(1)$ æ¨è«– | æ¨è«–è¶…é«˜é€Ÿ | è¨“ç·´ã¯ã¾ã Attentionã‚ˆã‚Šé…ã„ |

**Hybrid ã®æ™‚ä»£** (2025):
- Attentionå˜ç‹¬ or SSMå˜ç‹¬ã§ã¯ãªãã€**çµ„ã¿åˆã‚ã›**ãŒæœ€å¼·
- Jamba (AI21): SSM + Attention + MoE
- Zamba (Zyphra): Mamba + Shared Attention

**ç¬¬16-18å›ã®äºˆå‘Š**:
- ç¬¬16å›: SSMç†è«–ï¼ˆS4â†’Mambaï¼‰
- ç¬¬17å›: Mamba-2 / RWKV / RetNet
- ç¬¬18å›: Hybrid (Jamba/Zamba) + Course IIèª­äº†æ„Ÿ

---

### 7.21 Complete Code Repository Structure

**æ¨å¥¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ** â€” ç¬¬14å›ã®å…¨å®Ÿè£…ã‚’æ•´ç†:

```
ml-lecture-14-attention/
â”œâ”€â”€ julia/
â”‚   â”œâ”€â”€ Project.toml           # Dependencies
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ attention.jl       # Core Attention module
â”‚   â”‚   â”œâ”€â”€ multi_head.jl      # Multi-Head Attention
â”‚   â”‚   â”œâ”€â”€ transformer.jl     # Transformer Block
â”‚   â”‚   â”œâ”€â”€ micro_gpt.jl       # Micro-GPT model
â”‚   â”‚   â””â”€â”€ position.jl        # Position Encoding (Sinusoidal/RoPE)
â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â”œâ”€â”€ test_attention.jl  # Unit tests
â”‚   â”‚   â””â”€â”€ test_transformer.jl
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ train_gpt.jl       # Tiny Shakespeare training
â”‚   â”‚   â””â”€â”€ icl_demo.jl        # In-Context Learning demo
â”‚   â””â”€â”€ notebooks/
â”‚       â””â”€â”€ attention_viz.ipynb  # Attention visualization
â”œâ”€â”€ rust/
â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs             # Library root
â”‚   â”‚   â”œâ”€â”€ attention.rs       # Attention inference
â”‚   â”‚   â”œâ”€â”€ kv_cache.rs        # KV-Cache implementation
â”‚   â”‚   â””â”€â”€ bin/
â”‚   â”‚       â””â”€â”€ inference.rs   # Inference binary
â”‚   â””â”€â”€ benches/
â”‚       â””â”€â”€ attention_bench.rs # Criterion benchmarks
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ attention_ref.py       # Reference implementation (NumPy)
â”‚   â””â”€â”€ compare.py             # 3-language comparison script
â”œâ”€â”€ data/
â”‚   â””â”€â”€ tiny_shakespeare.txt   # Training data
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ml-lecture-14.md       # æœ¬è¬›ç¾©ï¼ˆZennå½¢å¼ï¼‰
â”‚   â”œâ”€â”€ symbols.pdf            # Symbol cheat sheet
â”‚   â””â”€â”€ figures/
â”‚       â”œâ”€â”€ attention_heatmap.png
â”‚       â””â”€â”€ training_loss.png
â””â”€â”€ README.md                  # Quick start guide
```

**README.md**ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:

```markdown
# ç¬¬14å›: Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´

å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã¨ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆZennè¨˜äº‹ã®å®Ÿè·µç‰ˆï¼‰

**ç¬¬14å›å®Œ**. åŒ–çŸ³ã‹ã‚‰ã®è„±å´å®Œäº† â€” æ¬¡ã¯åŠ¹ç‡åŒ–ã®æˆ¦ã„ã ã€‚ç¬¬15å›ã§ä¼šãŠã†ã€‚