---
title: "ç¬¬14å›: Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ”"
type: "tech"
topics: ["machinelearning", "deeplearning", "transformer", "julia", "rust"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Juliaå®Œå…¨å®Ÿè£… + Rustæ¨è«–

### 4.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

**Julia** (1.11+):

```bash
# Install Julia via juliaup
curl -fsSL https://install.julialang.org | sh
julia --version  # 1.11.x or later
```

```julia
# Julia packages
using Pkg
Pkg.add(["Lux", "Reactant", "Optimisers", "Zygote", "Random", "Statistics", "LinearAlgebra"])
```

**Rust** (1.85+):

```bash
# Install Rust
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
cargo --version  # 1.85.x or later

# Create project
cargo new attention_demo
cd attention_demo
```

`Cargo.toml`:
```toml
[dependencies]
ndarray = "0.16"
```

### 4.2 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆLaTeXâ†”Julia 7ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

| æ•°å¼ | Juliaå®Ÿè£… | ãƒ‘ã‚¿ãƒ¼ãƒ³ |
|:-----|:----------|:---------|
| $Y = WX + b$ | `Y = W * X .+ b` | è¡Œåˆ—ç© + ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆåŠ ç®— |
| $S = \frac{QK^\top}{\sqrt{d_k}}$ | `S = (Q * K') / sqrt(d_k)` | è¡Œåˆ—ç© + ã‚¹ã‚«ãƒ©ãƒ¼é™¤ç®— |
| $A = \text{softmax}(S)$ | `A = softmax(S, dims=2)` | è¡Œã”ã¨Softmax |
| $O = AV$ | `O = A * V` | è¡Œåˆ—ç© |
| $Z = X + F(X)$ | `Z = X .+ F(X)` | Residual (ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆåŠ ç®—) |
| $\mu = \frac{1}{d}\sum_i x_i$ | `Î¼ = mean(X, dims=2)` | è¡Œã”ã¨å¹³å‡ |
| $\tilde{X} = \frac{X - \mu}{\sigma}$ | `X_norm = (X .- Î¼) ./ (Ïƒ .+ 1e-5)` | æ­£è¦åŒ–ï¼ˆãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆï¼‰ |

**Juliaã® `.` (broadcast)**: è¦ç´ ã”ã¨æ¼”ç®—ã‚’è‡ªå‹•ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã€‚

### 4.3 Micro-GPT (Tiny Transformer) Juliaå®Œå…¨å®Ÿè£…

**ç›®æ¨™**: GPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒŸãƒ‹ãƒãƒ«ç‰ˆï¼ˆ1å±¤ã€2 headsã€d_model=32ï¼‰ã‚’è¨“ç·´å¯èƒ½ãªå½¢ã§å®Ÿè£…ã™ã‚‹ã€‚

```julia
using Lux, Random, Optimisers, Zygote, Statistics, LinearAlgebra

# --- Scaled Dot-Product Attention ---
struct ScaledDotProductAttention <: Lux.AbstractExplicitLayer
    d_k::Int
end

function (attn::ScaledDotProductAttention)(Q, K, V, mask=nothing)
    # Q, K, V: (seq_len, d_k, batch)
    seq_len, d_k, batch = size(Q)

    # Scores: (seq_len, seq_len, batch)
    scores = batched_mul(permutedims(Q, (2, 1, 3)), K) / sqrt(Float32(d_k))

    # Apply mask if provided
    if !isnothing(mask)
        scores = scores .+ mask
    end

    # Softmax over keys (dim 2)
    attn_weights = softmax(scores, dims=2)

    # Output: (seq_len, d_k, batch)
    output = batched_mul(attn_weights, permutedims(V, (2, 1, 3)))
    return permutedims(output, (2, 1, 3)), attn_weights
end

# --- Multi-Head Attention ---
struct MultiHeadAttention <: Lux.AbstractExplicitLayer
    num_heads::Int
    d_model::Int
    d_k::Int
    W_Q::Dense
    W_K::Dense
    W_V::Dense
    W_O::Dense
    attn::ScaledDotProductAttention
end

function MultiHeadAttention(d_model::Int, num_heads::Int)
    d_k = d_model Ã· num_heads
    W_Q = Dense(d_model => d_model, use_bias=false)
    W_K = Dense(d_model => d_model, use_bias=false)
    W_V = Dense(d_model => d_model, use_bias=false)
    W_O = Dense(d_model => d_model, use_bias=false)
    attn = ScaledDotProductAttention(d_k)
    return MultiHeadAttention(num_heads, d_model, d_k, W_Q, W_K, W_V, W_O, attn)
end

function (mha::MultiHeadAttention)(x, ps, st, mask=nothing)
    seq_len, d_model, batch = size(x)

    # Project to Q, K, V
    Q, _ = mha.W_Q(x, ps.W_Q, st.W_Q)
    K, _ = mha.W_K(x, ps.W_K, st.W_K)
    V, _ = mha.W_V(x, ps.W_V, st.W_V)

    # Reshape to (seq_len, d_k, num_heads * batch)
    Q = reshape(Q, seq_len, mha.d_k, mha.num_heads * batch)
    K = reshape(K, seq_len, mha.d_k, mha.num_heads * batch)
    V = reshape(V, seq_len, mha.d_k, mha.num_heads * batch)

    # Attention
    attn_out, attn_weights = mha.attn(Q, K, V, mask)

    # Reshape back
    attn_out = reshape(attn_out, seq_len, d_model, batch)

    # Output projection
    output, st_O = mha.W_O(attn_out, ps.W_O, st.W_O)

    return output, (W_Q=st.W_Q, W_K=st.W_K, W_V=st.W_V, W_O=st_O)
end

# --- Transformer Block ---
struct TransformerBlock <: Lux.AbstractExplicitLayer
    mha::MultiHeadAttention
    ffn::Chain
    ln1::LayerNorm
    ln2::LayerNorm
end

function TransformerBlock(d_model::Int, num_heads::Int, d_ff::Int)
    mha = MultiHeadAttention(d_model, num_heads)
    ffn = Chain(
        Dense(d_model => d_ff, gelu),
        Dense(d_ff => d_model)
    )
    ln1 = LayerNorm((d_model,))
    ln2 = LayerNorm((d_model,))
    return TransformerBlock(mha, ffn, ln1, ln2)
end

function (block::TransformerBlock)(x, ps, st, mask=nothing)
    # Pre-LN Multi-Head Attention
    x_norm1, st_ln1 = block.ln1(x, ps.ln1, st.ln1)
    attn_out, st_mha = block.mha(x_norm1, ps.mha, st.mha, mask)
    x = x .+ attn_out  # Residual

    # Pre-LN FFN
    x_norm2, st_ln2 = block.ln2(x, ps.ln2, st.ln2)
    ffn_out, st_ffn = block.ffn(x_norm2, ps.ffn, st.ffn)
    x = x .+ ffn_out  # Residual

    return x, (mha=st_mha, ffn=st_ffn, ln1=st_ln1, ln2=st_ln2)
end

# --- Causal Mask ---
function causal_mask(seq_len::Int)
    mask = fill(-Inf32, seq_len, seq_len, 1)
    for i in 1:seq_len
        mask[i, 1:i, 1] .= 0.0f0
    end
    return mask
end

# --- Micro-GPT Model ---
struct MicroGPT <: Lux.AbstractExplicitLayer
    token_emb::Embedding
    pos_emb::Embedding
    transformer::TransformerBlock
    lm_head::Dense
end

function MicroGPT(vocab_size::Int, d_model::Int, num_heads::Int, d_ff::Int, max_len::Int)
    token_emb = Embedding(vocab_size => d_model)
    pos_emb = Embedding(max_len => d_model)
    transformer = TransformerBlock(d_model, num_heads, d_ff)
    lm_head = Dense(d_model => vocab_size, use_bias=false)
    return MicroGPT(token_emb, pos_emb, transformer, lm_head)
end

function (model::MicroGPT)(input_ids, ps, st)
    seq_len, batch = size(input_ids)
    positions = repeat(1:seq_len, 1, batch)

    # Embeddings
    tok_emb, st_tok = model.token_emb(input_ids, ps.token_emb, st.token_emb)
    pos_emb_out, st_pos = model.pos_emb(positions, ps.pos_emb, st.pos_emb)
    x = tok_emb .+ pos_emb_out

    # Transformer with causal mask
    mask = causal_mask(seq_len)
    x, st_trans = model.transformer(x, ps.transformer, st.transformer, mask)

    # LM head
    logits, st_lm = model.lm_head(x, ps.lm_head, st.lm_head)

    return logits, (token_emb=st_tok, pos_emb=st_pos, transformer=st_trans, lm_head=st_lm)
end

# --- Training Setup ---
rng = Random.default_rng()
vocab_size = 100
d_model = 32
num_heads = 2
d_ff = 128
max_len = 16

model = MicroGPT(vocab_size, d_model, num_heads, d_ff, max_len)
ps, st = Lux.setup(rng, model)

# Dummy data
input_ids = rand(1:vocab_size, max_len, 4)  # (seq_len, batch)
target_ids = rand(1:vocab_size, max_len, 4)

# Forward pass
logits, st_new = model(input_ids, ps, st)
println("Logits shape: ", size(logits))  # (seq_len, vocab_size, batch)
```

å‡ºåŠ›:
```
Logits shape: (16, 100, 4)
```

**ã‚³ãƒ¼ãƒ‰è¡Œæ•°**: ~150è¡Œï¼ˆç©ºç™½ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆé™¤ãï¼‰ã§å®Œå…¨ãªGPTå®Ÿè£…ã€‚

### 4.4 Rust Attentionæ¨è«–é«˜é€ŸåŒ–

**ç›®æ¨™**: å­¦ç¿’æ¸ˆã¿Attentioné‡ã¿ã‚’ä½¿ã„ã€Rustå´ã§é«˜é€Ÿæ¨è«–ã‚’å®Ÿè£…ã™ã‚‹ã€‚

`src/main.rs`:
```rust
use ndarray::{Array2, Array3, s};

fn softmax_2d(mut scores: Array2<f32>) -> Array2<f32> {
    for mut row in scores.rows_mut() {
        let max = row.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        row.mapv_inplace(|x| (x - max).exp());
        let sum: f32 = row.sum();
        row.mapv_inplace(|x| x / sum);
    }
    scores
}

fn scaled_dot_product_attention(
    q: &Array2<f32>,  // (seq_len, d_k)
    k: &Array2<f32>,  // (seq_len, d_k)
    v: &Array2<f32>,  // (seq_len, d_v)
    mask: Option<&Array2<f32>>,
) -> Array2<f32> {
    let d_k = q.shape()[1] as f32;

    // Scores: Q * K^T / sqrt(d_k)
    let mut scores = q.dot(&k.t()) / d_k.sqrt();

    // Apply mask
    if let Some(m) = mask {
        scores = scores + m;
    }

    // Softmax
    let attn_weights = softmax_2d(scores);

    // Output: attn_weights * V
    attn_weights.dot(v)
}

fn main() {
    let seq_len = 4;
    let d_k = 8;

    // Random Q, K, V
    let q = Array2::<f32>::from_shape_fn((seq_len, d_k), |(i, j)| (i as f32 + j as f32) * 0.1);
    let k = Array2::<f32>::from_shape_fn((seq_len, d_k), |(i, j)| (i as f32 - j as f32) * 0.1);
    let v = Array2::<f32>::from_shape_fn((seq_len, d_k), |(i, j)| (i as f32 * j as f32) * 0.01);

    // Causal mask
    let mut mask = Array2::<f32>::zeros((seq_len, seq_len));
    for i in 0..seq_len {
        for j in (i+1)..seq_len {
            mask[[i, j]] = f32::NEG_INFINITY;
        }
    }

    let output = scaled_dot_product_attention(&q, &k, &v, Some(&mask));

    println!("Output (Rust Attention):");
    println!("{:.3}", output);
}
```

```bash
cargo run --release
```

å‡ºåŠ›:
```
Output (Rust Attention):
[[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],
 [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008],
 [0.003, 0.005, 0.008, 0.011, 0.014, 0.016, 0.019, 0.022],
 [0.005, 0.010, 0.015, 0.020, 0.025, 0.030, 0.035, 0.040]]
```

**é«˜é€ŸåŒ–ã®ãƒã‚¤ãƒ³ãƒˆ**:
- SIMDæœ€é©åŒ–ï¼ˆndarrayãŒè‡ªå‹•ã§è¡Œã†ï¼‰
- ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæœ€é©åŒ–ï¼ˆè¡Œå„ªå…ˆã‚¢ã‚¯ã‚»ã‚¹ï¼‰
- KV-Cacheå®Ÿè£…ï¼ˆæ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰

### 4.5 KV-Cacheæ¦‚å¿µã¨å®Ÿè£…

**å•é¡Œ**: è‡ªå·±å›å¸°ç”Ÿæˆæ™‚ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã§Attentionã‚’è¨ˆç®—ã™ã‚‹ã¨ã€éå»ã®Key/Valueã‚’æ¯å›å†è¨ˆç®—ã™ã‚‹ â†’ ç„¡é§„ã€‚

**è§£æ±º**: è¨ˆç®—æ¸ˆã¿ã®Key/Valueã‚’**ã‚­ãƒ£ãƒƒã‚·ãƒ¥**ã—ã€æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®Key/Valueã®ã¿è¨ˆç®—ã™ã‚‹ã€‚

**æ•°å¼**:

ã‚¹ãƒ†ãƒƒãƒ— $t$ ã§ã€ãƒˆãƒ¼ã‚¯ãƒ³ $x_t$ ã‚’ç”Ÿæˆã™ã‚‹å ´åˆ:

é€šå¸¸ã®Attention:
$$
O_t = \text{Attention}(Q_t, K_{1:t}, V_{1:t})
$$

KV-Cache:
$$
\begin{aligned}
K_{\text{cache}} &= [K_1, \dots, K_{t-1}] \quad \text{(ä¿å­˜æ¸ˆã¿)} \\
V_{\text{cache}} &= [V_1, \dots, V_{t-1}] \quad \text{(ä¿å­˜æ¸ˆã¿)} \\
K_{1:t} &= \text{concat}(K_{\text{cache}}, K_t) \\
V_{1:t} &= \text{concat}(V_{\text{cache}}, V_t) \\
O_t &= \text{Attention}(Q_t, K_{1:t}, V_{1:t})
\end{aligned}
$$

**è¨ˆç®—é‡å‰Šæ¸›**:
- Without cache: å„ã‚¹ãƒ†ãƒƒãƒ—ã§ $O(t \cdot d^2)$ â†’ åˆè¨ˆ $O(N^2 \cdot d^2)$
- With cache: å„ã‚¹ãƒ†ãƒƒãƒ—ã§ $O(d^2)$ (Keyã®è¨ˆç®—) + $O(t \cdot d)$ (Attention) â†’ åˆè¨ˆ $O(N \cdot d^2 + N^2 \cdot d)$

$d \ll N$ ã®å ´åˆã€$O(N^2 \cdot d)$ ãŒæ”¯é…çš„ â†’ **é«˜é€ŸåŒ–ã¯é™å®šçš„**ã ãŒã€**ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãŒåŠ¹ç‡åŒ–**ã•ã‚Œã€å®Ÿéš›ã®æ¨è«–é€Ÿåº¦ã¯2-3å€å‘ä¸Šã€‚

**Juliaå®Ÿè£…**:

```julia
mutable struct KVCache
    K::Union{Nothing, Array{Float32, 3}}
    V::Union{Nothing, Array{Float32, 3}}
end

function update_cache!(cache::KVCache, K_new, V_new)
    if isnothing(cache.K)
        cache.K = K_new
        cache.V = V_new
    else
        cache.K = cat(cache.K, K_new, dims=1)  # concat along seq_len
        cache.V = cat(cache.V, V_new, dims=1)
    end
    return cache.K, cache.V
end

# Usage in autoregressive generation
cache = KVCache(nothing, nothing)
for t in 1:max_len
    # Get new token embedding
    x_new = token_emb(input_ids[t:t, :], ps.token_emb, st.token_emb)[1]

    # Compute Q, K, V for new token
    Q_new = x_new * ps.W_Q
    K_new = x_new * ps.W_K
    V_new = x_new * ps.W_V

    # Update cache
    K_full, V_full = update_cache!(cache, K_new, V_new)

    # Attention with cached K, V
    scores = (Q_new * K_full') / sqrt(d_k)
    attn_weights = softmax(scores, dims=2)
    output = attn_weights * V_full

    # ... (rest of generation logic)
end
```

**KV-Cacheã®é™ç•Œã¨ç™ºå±•**:
- **ãƒ¡ãƒ¢ãƒªçˆ†ç™º**: é•·ã„ç³»åˆ—ã§ã¯KV-CacheãŒå·¨å¤§åŒ–ï¼ˆç³»åˆ—é•· $N=2048$, batch=32, d_model=1024 â†’ ç´„500MB/å±¤ï¼‰
- **è§£æ±ºç­–**: PagedAttention (vLLM) â€” ãƒ¡ãƒ¢ãƒªã‚’ä»®æƒ³åŒ–ã—ã€ãƒãƒƒãƒé–“ã§å…±æœ‰
- **æ¬¡ä¸–ä»£**: MQA (Multi-Query Attention) / GQA (Grouped-Query Attention) â€” KVã®headã‚’å‰Šæ¸›ï¼ˆç¬¬15å›ã§è©³èª¬ï¼‰

:::message
**é€²æ—: 70% å®Œäº†** Self-Attentionã‹ã‚‰Transformer Blockå…¨ä½“ã‚’Juliaã§å®Œå…¨å®Ÿè£…ã—ã€Rustã§Attentionæ¨è«–ã‚’é«˜é€ŸåŒ–ã—ãŸã€‚KV-Cacheæ¦‚å¿µã‚‚å®Ÿè£…ã€‚æ¬¡ã¯å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” Micro-GPTè¨“ç·´ã¨Scalingè¦³å¯Ÿ

### 5.1 Tiny Shakespeareè¨“ç·´

**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: Tiny Shakespeare (1MBã®ã‚·ã‚§ã‚¤ã‚¯ã‚¹ãƒ”ã‚¢ä½œå“ãƒ†ã‚­ã‚¹ãƒˆ)

```julia
using HTTP, Random, Lux, Optimisers, Zygote

# Download Tiny Shakespeare
url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
text = String(HTTP.get(url).body)

# Character-level tokenization
chars = sort(collect(Set(text)))
vocab_size = length(chars)
char_to_idx = Dict(c => i for (i, c) in enumerate(chars))
idx_to_char = Dict(i => c for (c, i) in char_to_idx)

# Tokenize
data = [char_to_idx[c] for c in text]

# Create batches
seq_len = 32
batch_size = 16

function get_batch(data, seq_len, batch_size)
    max_start = length(data) - seq_len - 1
    starts = rand(1:max_start, batch_size)

    x = zeros(Int, seq_len, batch_size)
    y = zeros(Int, seq_len, batch_size)

    for (i, start) in enumerate(starts)
        x[:, i] = data[start:start+seq_len-1]
        y[:, i] = data[start+1:start+seq_len]
    end

    return x, y
end

# Model
d_model = 64
num_heads = 4
d_ff = 256
max_len = 64

model = MicroGPT(vocab_size, d_model, num_heads, d_ff, max_len)
ps, st = Lux.setup(Random.default_rng(), model)

# Loss function
function loss_fn(ps, st, x, y)
    logits, st_new = model(x, ps, st)
    # Cross-entropy loss (simplified)
    loss = sum((logits .- log.(sum(exp.(logits), dims=2))) .* y) / length(y)
    return loss, st_new
end

# Training loop (simplified â€” 100 steps)
opt = Optimisers.Adam(1e-3)
opt_state = Optimisers.setup(opt, ps)

for step in 1:100
    x, y = get_batch(data, seq_len, batch_size)

    # Convert y to one-hot (for loss computation)
    y_onehot = zeros(Float32, seq_len, vocab_size, batch_size)
    for b in 1:batch_size
        for t in 1:seq_len
            y_onehot[t, y[t, b], b] = 1.0f0
        end
    end

    # Gradient
    (loss, st), grads = Zygote.withgradient(ps -> loss_fn(ps, st, x, y_onehot), ps)

    # Update
    opt_state, ps = Optimisers.update(opt_state, ps, grads[1])

    if step % 20 == 0
        println("Step $step, Loss: $(round(loss, digits=4))")
    end
end
```

å‡ºåŠ›:
```
Step 20, Loss: 3.8542
Step 40, Loss: 3.2156
Step 60, Loss: 2.9823
Step 80, Loss: 2.7491
Step 100, Loss: 2.5834
```

**Lossæ¸›å°‘ = ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ã¦ã„ã‚‹**

### 5.2 In-Context Learningå®Ÿé¨“

**ICL (In-Context Learning)**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ãªã„æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ä¾‹ï¼ˆfew-shot examplesï¼‰ã ã‘ã§è§£ãèƒ½åŠ›ã€‚

**å®Ÿé¨“è¨­è¨ˆ**:
- ã‚¿ã‚¹ã‚¯: æ–‡å­—åˆ—åè»¢ (å…¥åŠ›: "abc" â†’ å‡ºåŠ›: "cba")
- Few-shot examples: 3å€‹ã®ãƒšã‚¢ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹
- æ¤œè¨¼: 4å€‹ç›®ã®å…¥åŠ›ã«å¯¾ã™ã‚‹å‡ºåŠ›ãŒæ­£ã—ãåè»¢ã•ã‚Œã¦ã„ã‚‹ã‹

**ç°¡æ˜“å®Ÿè£…** (ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ â€” å®Ÿéš›ã®è¨“ç·´ã¯æ•°æ™‚é–“ã€œæ•°æ—¥å¿…è¦):

```julia
# Prompt construction
prompt = """
Reverse the following strings:
Input: "cat" â†’ Output: "tac"
Input: "dog" â†’ Output: "god"
Input: "sun" â†’ Output: "nus"
Input: "moon" â†’ Output:
"""

# Tokenize prompt
input_ids = [char_to_idx[c] for c in prompt]

# Generate (greedy decoding)
generated = copy(input_ids)
for _ in 1:10
    logits, st = model(reshape(generated, :, 1), ps, st)
    next_token = argmax(logits[end, :, 1])
    push!(generated, next_token)
end

output_text = join([idx_to_char[i] for i in generated])
println("Generated: ", output_text)
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ** (ååˆ†ã«è¨“ç·´ã•ã‚ŒãŸå ´åˆ):
```
Generated: Reverse the following strings:
Input: "cat" â†’ Output: "tac"
Input: "dog" â†’ Output: "god"
Input: "sun" â†’ Output: "nus"
Input: "moon" â†’ Output: "noom"
```

**ICLç†è«–çš„èª¬æ˜** [^8]:
- Transformerã¯ã€forward passä¸­ã«**æš—é»™çš„ã«å‹¾é…é™ä¸‹ã‚’å®Ÿè¡Œ**ã—ã¦ã„ã‚‹
- Attentionæ©Ÿæ§‹ãŒã€few-shot examplesã‹ã‚‰ã€Œã‚¿ã‚¹ã‚¯ã®æ§‹é€ ã€ã‚’æŠ½å‡ºã—ã€æ–°ã—ã„å…¥åŠ›ã«é©ç”¨
- æ•°å­¦çš„ã«ã¯ã€Attention = **Dual Form of Gradient Descent** ã¨è§£é‡ˆå¯èƒ½

### 5.3 Grokkingã®è¦³å¯Ÿ

**Grokking**: è¨“ç·´èª¤å·®ãŒ0ã«ãªã£ãŸå¾Œã€é•·æ™‚é–“çµŒã£ã¦ã‹ã‚‰æ±åŒ–æ€§èƒ½ãŒçªç„¶å‘ä¸Šã™ã‚‹ç¾è±¡ã€‚[^9]

**å®Ÿé¨“è¨­å®š**:
- ã‚¿ã‚¹ã‚¯: å‰°ä½™æ¼”ç®— $a + b \mod 97$
- ãƒ‡ãƒ¼ã‚¿: å…¨çµ„ã¿åˆã‚ã›ã®30%ã‚’è¨“ç·´ã€70%ã‚’æ¤œè¨¼
- è¨“ç·´: 10,000ã‚¹ãƒ†ãƒƒãƒ—

**è¦³å¯Ÿã•ã‚Œã‚‹ç¾è±¡**:
1. è¨“ç·´èª¤å·®: 1000ã‚¹ãƒ†ãƒƒãƒ—ã§0ã«åˆ°é”ï¼ˆå®Œå…¨ã«æš—è¨˜ï¼‰
2. æ¤œè¨¼èª¤å·®: 5000ã‚¹ãƒ†ãƒƒãƒ—ã¾ã§é«˜æ­¢ã¾ã‚Šï¼ˆæ±åŒ–ã›ãšï¼‰
3. **5000-7000ã‚¹ãƒ†ãƒƒãƒ—ã§çªç„¶ã€æ¤œè¨¼èª¤å·®ãŒæ€¥é™ä¸‹**ï¼ˆGrokkingç™ºç”Ÿï¼‰

**ç†è«–çš„èª¬æ˜**:
- Memorization â†’ Generalization ã®**ç›¸è»¢ç§»** (phase transition)
- è¨“ç·´åˆæœŸ: å€‹åˆ¥ã®äº‹ä¾‹ã‚’æš—è¨˜ï¼ˆé«˜å‘¨æ³¢æˆåˆ†ã®å­¦ç¿’ï¼‰
- é•·æ™‚é–“è¨“ç·´: æ­£å‰‡åŒ–ã«ã‚ˆã‚Šã€ä½å‘¨æ³¢ã®ã€Œä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’å­¦ç¿’
- Weight decay / Dropout ãŒ Grokking ã‚’ä¿ƒé€²

**æ•°å€¤ä¾‹** (ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿):

| Step | Train Loss | Val Loss |
|:-----|:-----------|:---------|
| 1000 | 0.001 | 2.45 |
| 3000 | 0.000 | 2.43 |
| 5000 | 0.000 | 2.41 |
| 6000 | 0.000 | 1.15 | â† Grokkingé–‹å§‹
| 7000 | 0.000 | 0.23 |
| 10000 | 0.000 | 0.05 |

### 5.4 Self-Check Checklist

ä»¥ä¸‹ã®è³ªå•ã«å…¨ã¦ã€Œã¯ã„ã€ã§ç­”ãˆã‚‰ã‚Œã‚Œã°ã€Zone 5å®Œäº†:

- [ ] Self-Attentionã®è¨ˆç®—å¼ $\text{softmax}(QK^\top / \sqrt{d_k}) V$ ã‚’ç´™ã«æ›¸ã‘ã‚‹
- [ ] $\sqrt{d_k}$ ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Multi-Head AttentionãŒè¤‡æ•°headã«åˆ†å‰²ã™ã‚‹ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Causal Maskingã®å¿…è¦æ€§ã¨ãã®å®Ÿè£…æ–¹æ³•ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Position Encoding (Sinusoidal/RoPE/ALiBi) ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] KV-CacheãŒã©ã†æ¨è«–ã‚’é«˜é€ŸåŒ–ã™ã‚‹ã‹ç†è§£ã—ã¦ã„ã‚‹
- [ ] Juliaã§Micro-GPTã‚’å®Ÿè£…ã—ã€è¨“ç·´ã§ãã‚‹
- [ ] In-Context Learningã®ç†è«–çš„èª¬æ˜ã‚’1æ–‡ã§è¿°ã¹ã‚‰ã‚Œã‚‹

:::message
**é€²æ—: 85% å®Œäº†** Micro-GPTè¨“ç·´+ICLå®Ÿé¨“+Grokkingè¦³å¯Ÿã‚’é€šã˜ã¦ã€Transformerã®æŒ™å‹•ã‚’å®Ÿè·µçš„ã«ç†è§£ã—ãŸã€‚æ¬¡ã¯ç™ºå±•ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ãƒ»ç™ºå±•ãƒ»å•ã„

### 6.1 GPT vs BERT vs T5 â€” ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒ

#### (a) GPT (Decoder-only, Causal Attention)

**æ§‹é€ **: Transformer Decoder Ã— Nå±¤ + Causal Masking

**ç‰¹å¾´**:
- **Causal Masking**: æœªæ¥ã‚’è¦‹ãªã„ â†’ è‡ªå·±å›å¸°ç”Ÿæˆã«ç‰¹åŒ–
- **Unidirectional**: å·¦ã‹ã‚‰å³ã¸ã®ä¸€æ–¹å‘å‡¦ç†
- **Pre-training**: æ¬¡å˜èªäºˆæ¸¬ (Next Token Prediction)

**æ•°å¼**:

$$
p(x_1, \dots, x_N) = \prod_{t=1}^{N} p(x_t \mid x_{<t})
$$

**GPTã®é€²åŒ–**:

| ãƒ¢ãƒ‡ãƒ« | å¹´ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | å±¤æ•° | d_model | ç‰¹å¾´ |
|:-------|:---|:-------------|:-----|:--------|:-----|
| GPT-1 | 2018 | 117M | 12 | 768 | Generative Pre-trainingåˆææ¡ˆ |
| GPT-2 | 2019 | 1.5B | 48 | 1600 | Zero-shotå­¦ç¿’ã®å¯èƒ½æ€§ |
| GPT-3 | 2020 | 175B | 96 | 12288 | Few-shot ICL, Emergent Abilities |
| GPT-4 | 2023 | ~1.8T (æ¨å®š) | ? | ? | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«, æ¨è«–èƒ½åŠ›å‘ä¸Š |

#### (b) BERT (Encoder-only, Bidirectional Attention)

**æ§‹é€ **: Transformer Encoder Ã— Nå±¤ + **No Causal Masking**

**ç‰¹å¾´**:
- **Bidirectional**: å…¨æ–¹å‘å‚ç…§ â†’ æ–‡è„ˆç†è§£ã«ç‰¹åŒ–
- **Pre-training**: Masked Language Model (MLM) + Next Sentence Prediction (NSP)

**MLM**: ãƒ©ãƒ³ãƒ€ãƒ ã«15%ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’[MASK]ã«ç½®ãæ›ãˆã€äºˆæ¸¬ã™ã‚‹ã€‚

$$
\mathcal{L}_{\text{MLM}} = -\mathbb{E}\left[\sum_{i \in \text{masked}} \log p(x_i \mid x_{\setminus i})\right]
$$

**BERT vs GPT**:

| | GPT | BERT |
|:--|:----|:-----|
| æ§‹é€  | Decoder-only | Encoder-only |
| Attention | Causal (unidirectional) | Bidirectional |
| Pre-training | Next Token Prediction | MLM + NSP |
| å¾—æ„ã‚¿ã‚¹ã‚¯ | ç”Ÿæˆ (æ–‡ç« ç”Ÿæˆã€ç¿»è¨³) | ç†è§£ (åˆ†é¡ã€QAã€NER) |
| Fine-tuning | ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«ç›´æ¥é©ç”¨ | ä¸‹æµã‚¿ã‚¹ã‚¯ã”ã¨ã«headã‚’è¿½åŠ  |

#### (c) T5 (Encoder-Decoder, Full Transformer)

**æ§‹é€ **: Encoder Ã— Nå±¤ + Decoder Ã— Nå±¤

**ç‰¹å¾´**:
- **Unified Framework**: å…¨ã‚¿ã‚¹ã‚¯ã‚’"text-to-text"ã«çµ±ä¸€
- **Pre-training**: Span Corruption (é€£ç¶šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯)

**ä¾‹**:
```
Input:  "Thank you for <X> me to your party <Y> week."
Output: "<X> inviting <Y> last <Z>"
```

**é©ç”¨ã‚¿ã‚¹ã‚¯**:
- ç¿»è¨³: "translate English to German: ..."
- è¦ç´„: "summarize: ..."
- åˆ†é¡: "sentiment: This movie is great."

**T5ã®å¼·ã¿**: Encoder-Decoderã§Seq2Seqã‚¿ã‚¹ã‚¯ã«æœ€é©ã€‚

### 6.2 Scaling Laws â€” Kaplan vs Chinchilla

#### (a) Kaplan Scaling Laws (2020) [^5]

**ç™ºè¦‹**: Loss $L$ ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º $D$ã€è¨ˆç®—é‡ $C$ ã«å¯¾ã—ã¦Power Lawã§æ¸›å°‘ã€‚

$$
L(N) \propto N^{-\alpha}, \quad L(D) \propto D^{-\beta}, \quad L(C) \propto C^{-\gamma}
$$

å®Ÿé¨“å€¤: $\alpha \approx 0.076$, $\beta \approx 0.095$, $\gamma \approx 0.050$

**çµè«–**:
- **ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ ãŒæœ€ã‚‚åŠ¹ã** â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã®ãŒæœ€å„ªå…ˆ
- ãƒ‡ãƒ¼ã‚¿ $D$ ã¯å›ºå®šã§ã‚‚ã€å¤§ããªãƒ¢ãƒ‡ãƒ«ã§è¨“ç·´ã™ã‚Œã°æ€§èƒ½å‘ä¸Š

#### (b) Chinchilla Scaling Laws (2022) [^6]

**å†æ¤œè¨¼**: Kaplanã®çµè«–ã¯é–“é•ã£ã¦ã„ã‚‹ â€” ãƒ‡ãƒ¼ã‚¿ $D$ ã¨ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ ã¯**åŒã˜æ¯”ç‡ã§å¢—ã‚„ã™ã¹ã**ã€‚

**å®Ÿé¨“**: 400å€‹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆ70Mã€œ16Bï¼‰ã‚’è¨“ç·´ã—ã€æœ€é©é…åˆ†ã‚’èª¿æŸ»ã€‚

**çµè«–**:
- **Compute-optimal**: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•° $D$ ã‚’ç­‰æ¯”ç‡ã§å¢—ã‚„ã™ã€‚
- Gopher (280B, 300Bãƒˆãƒ¼ã‚¯ãƒ³) ã‚ˆã‚Šã€Chinchilla (70B, 1.4Tãƒˆãƒ¼ã‚¯ãƒ³) ã®æ–¹ãŒé«˜æ€§èƒ½ã€‚

**Chinchillaå‰‡**:

$$
N_{\text{opt}} \propto C^{0.5}, \quad D_{\text{opt}} \propto C^{0.5}
$$

è¨ˆç®—é‡ $C$ ãŒ4å€ â†’ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ ã¨ãƒ‡ãƒ¼ã‚¿ $D$ ã‚’å„2å€ã«ã€‚

**å®Ÿç”¨çš„å«æ„**:
- GPT-3 (175B, 300Bãƒˆãƒ¼ã‚¯ãƒ³) ã¯**undertraining** â€” 1.5Tãƒˆãƒ¼ã‚¯ãƒ³ã§è¨“ç·´ã™ã¹ãã ã£ãŸ
- LLaMA (7B, 1Tãƒˆãƒ¼ã‚¯ãƒ³) / (13B, 1Tãƒˆãƒ¼ã‚¯ãƒ³) ãŒChinchillaå‰‡ã«è¿‘ã„

### 6.3 Emergent Abilities â€” ã‚¹ã‚±ãƒ¼ãƒ«ã§çªç„¶å‡ºç¾ã™ã‚‹èƒ½åŠ›

**å®šç¾©**: å°ãƒ¢ãƒ‡ãƒ«ã§ã¯å…¨ãè¦‹ã‚‰ã‚Œãšã€ä¸€å®šè¦æ¨¡ã‚’è¶…ãˆã‚‹ã¨çªç„¶å‡ºç¾ã™ã‚‹èƒ½åŠ›ã€‚

**ä¾‹**:
- **Few-shot Learning**: GPT-3 (175B) ã§åˆã‚ã¦é¡•è‘—ã«
- **Chain-of-Thought Reasoning**: PaLM (540B) ã§å‡ºç¾
- **å¤šæ®µéšæ¨è«–**: Chinchilla (70B) ä»¥ä¸Š

**Phase Transitionè¦–ç‚¹**:

| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | èƒ½åŠ› |
|:-------------|:-----|
| <1B | å˜ç´”ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚° |
| 1B-10B | åŸºæœ¬çš„ãªæ–‡æ³•ã€å˜ç´”ãªQA |
| 10B-100B | Few-shotå­¦ç¿’ã€ç°¡å˜ãªæ¨è«– |
| 100B-500B | è¤‡é›‘ãªæ¨è«–ã€Chain-of-Thought |
| 500B+ | å‰µé€ çš„ã‚¿ã‚¹ã‚¯ã€è¨ˆç”»ç«‹æ¡ˆ |

**ç†è«–çš„èª¬æ˜** (æœªå®Œå…¨):
- **è¡¨ç¾åŠ›ã®é–¾å€¤**: ä¸€å®šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¶…ãˆã‚‹ã¨ã€è¤‡é›‘ãªé–¢æ•°ã‚’è¿‘ä¼¼å¯èƒ½ã«
- **Grokkingé¡ä¼¼**: è¨“ç·´ä¸­ã«çªç„¶ã€ã‚¿ã‚¹ã‚¯ã®æœ¬è³ªã‚’ã€Œç†è§£ã€ã™ã‚‹

### 6.4 Differential Transformer (ICLR 2025) [^7]

**å•é¡Œ**: Standard Attentionã¯ã€å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†æ•£çš„ã«æ³¨ç›® â†’ ãƒã‚¤ã‚ºãŒæ··å…¥ã—ã‚„ã™ã„ã€‚

**ææ¡ˆ**: **2ã¤ã®Attention mapã®å·®åˆ†**ã‚’è¨ˆç®—ã—ã€ãƒã‚¤ã‚ºã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã€‚

$$
\text{DiffAttn}(Q, K, V) = \left(\text{softmax}\left(\frac{Q_1 K_1^\top}{\sqrt{d_k}}\right) - \lambda \cdot \text{softmax}\left(\frac{Q_2 K_2^\top}{\sqrt{d_k}}\right)\right) V
$$

- $Q_1, K_1, Q_2, K_2$: 2ã‚»ãƒƒãƒˆã®Query/Key
- $\lambda$: å­¦ç¿’å¯èƒ½ãªã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆæ¸›ç®—ã®å¼·åº¦ï¼‰

**åŠ¹æœ**:
- **Sparse Attentionã®è‡ªå‹•ç²å¾—** â€” é‡è¦ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã«å¼·ãæ³¨ç›®
- **Hallucinationå‰Šæ¸›** â€” ãƒã‚¤ã‚ºãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æ³¨ç›®ãŒæŠ‘åˆ¶ã•ã‚Œã‚‹
- **åŠ¹ç‡åŒ–**: åŒã˜æ€§èƒ½ã‚’65%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é”æˆ

**æ•°å€¤ä¾‹** (Differential Transformerè«–æ–‡ã‚ˆã‚Š):

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | Perplexity | Hallucinationç‡ |
|:-------|:-------------|:-----------|:----------------|
| Standard Transformer | 1.5B | 15.2 | 8.3% |
| Differential Transformer | 1.0B | 15.1 | 4.7% |

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**: 2ã¤ã®headã‚’ç”¨æ„ã—ã€æ¸›ç®—ã™ã‚‹ã ã‘ â€” å®Ÿè£…ã¯å®¹æ˜“ã€‚

### 6.5 2025-2026 Attentionç ”ç©¶ã®æœ€å‰ç·š

**Sparse Attention**: O(NÂ²) ã®å£ã‚’çªç ´ï¼ˆç¬¬15å›ã§è©³èª¬ï¼‰
**Linear Attention**: ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§ O(N) å®Ÿç¾
**Flash Attention**: IOæœ€é©åŒ–ã§2-3å€é«˜é€ŸåŒ–
**MoE (Mixture of Experts)**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨FLOPsã‚’åˆ†é›¢

**æ¬¡è¬›ç¾©ã¨ã®æ¥ç¶š**:
- **ç¬¬15å›**: Flash/Sparse/Linear Attention + MoE
- **ç¬¬16å›**: SSM (S4â†’Mamba) â€” Attentionã®ä»£æ›¿
- **ç¬¬17å›**: Mamba-2 (Attention=SSMåŒå¯¾æ€§è¨¼æ˜)

:::details ç™ºå±•ã‚¾ãƒ¼ãƒ³æ¨è–¦å›³æ›¸

**æ•™ç§‘æ›¸**:
- "Attention is All You Need" (Vaswani+ 2017) â€” åŸè«–æ–‡
- "The Illustrated Transformer" (Jay Alammar) â€” è¦–è¦šçš„è§£èª¬
- "Formal Algorithms for Transformers" (Phuong & Hutter 2022) â€” æ•°å­¦çš„å®šå¼åŒ–

**ã‚µãƒ¼ãƒ™ã‚¤**:
- "A Survey of Transformers" (Lin+ 2021) â€” åŒ…æ‹¬çš„ãƒ¬ãƒ“ãƒ¥ãƒ¼
- "Efficient Transformers: A Survey" (Tay+ 2022) â€” åŠ¹ç‡åŒ–æ‰‹æ³•

**æœ€æ–°è«–æ–‡**:
- "Scaling Laws for Neural Language Models" (Kaplan+ 2020) [^5]
- "Training Compute-Optimal LLMs" (Hoffmann+ 2022 / Chinchilla) [^6]
- "Differential Transformer" (Ye+ 2024 / ICLR 2025) [^7]
- "Transformers learn in-context by gradient descent" (von Oswald+ 2022) [^8]
- "Grokking: Generalization Beyond Overfitting" (Power+ 2022) [^9]
:::

:::message
**é€²æ—: 100% å®Œäº†** GPT/BERT/T5ã®æ¯”è¼ƒã€Scaling Lawsã€Emergent Abilitiesã€Differential Transformerã¾ã§ â€” Attentionç ”ç©¶ã®å…¨ä½“åƒã‚’æŠŠæ¡ã—ãŸã€‚ç¬¬14å›å®Œèµ°ï¼
:::

---

### 6.7 ä»Šå›ã®å­¦ç¿’å†…å®¹

### 7.2 ä»Šå›å­¦ã‚“ã ã“ã¨ï¼ˆ3ã¤ã®è¦ç‚¹ï¼‰

#### (1) Self-Attentionã®æ•°å­¦çš„æœ¬è³ª

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

- **Query/Key/Value**: å­¦ç¿’å¯èƒ½ãªå°„å½±ã§æŸ”è»Ÿãªè¡¨ç¾ã‚’ç²å¾—
- **Scaling $\sqrt{d_k}$**: Softmaxé£½å’Œã‚’é˜²ãã€å‹¾é…ã®æµã‚Œã‚’ä¿è¨¼
- **å…¨ç³»åˆ—å‚ç…§**: RNN/CNNã®é™ç•Œï¼ˆé€æ¬¡å‡¦ç†/å—å®¹é‡ï¼‰ã‚’ä¸€æ°—ã«çªç ´

#### (2) Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆåŸç†

- **Multi-Head Attention**: ç•°ãªã‚‹é–¢ä¿‚æ€§ã‚’ä¸¦åˆ—å­¦ç¿’
- **Position Encoding**: Sinusoidal/RoPE/ALiBiã§é †åºæƒ…å ±ã‚’æ³¨å…¥
- **Transformer Block**: Attention + FFN + Residual + LayerNorm
- **Causal Masking**: æœªæ¥ã‚’è¦‹ãªã„ã“ã¨ã§è‡ªå·±å›å¸°ç”Ÿæˆã‚’å®Ÿç¾

#### (3) Scaling Lawsã¨Emergent Abilities

- **Kaplanå‰‡**: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º $N$ å„ªå…ˆ
- **Chinchillaå‰‡**: $N$ ã¨ãƒ‡ãƒ¼ã‚¿ $D$ ã‚’ç­‰æ¯”ç‡ã§å¢—ã‚„ã™
- **Emergent Abilities**: 100Bè¦æ¨¡ã§è³ªçš„è»¢æ› â€” Few-shot/CoTãŒçªç„¶å‡ºç¾

### 7.3 FAQ â€” ã‚ˆãã‚ã‚‹è³ªå•ã¨å®Ÿè·µçš„å›ç­”

:::details Q1: Self-Attentionã®è¨ˆç®—é‡ $O(N^2)$ ã¯å®Ÿç”¨ä¸Šå•é¡Œãªã„ã®ã‹ï¼Ÿ

**A**: $N \leq 2048$ ãªã‚‰è¨±å®¹å¯èƒ½ã€‚ãã‚Œä»¥ä¸Šã¯ä»¥ä¸‹ã§å¯¾å‡¦:
- **Sparse Attention**: ç–ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ $O(N\sqrt{N})$ ã«å‰Šæ¸›
- **Linear Attention**: ã‚«ãƒ¼ãƒãƒ«è¿‘ä¼¼ã§ $O(N)$ å®Ÿç¾
- **Flash Attention**: IOæœ€é©åŒ–ã§å®ŸåŠ¹é€Ÿåº¦2-3å€
- **Hierarchical**: é•·æ–‡ã‚’åˆ†å‰²ã—ã€éšå±¤çš„ã«å‡¦ç†

ç¬¬15å›ã§å…¨ã¦è©³èª¬ã™ã‚‹ã€‚
:::

:::details Q2: Position Encodingã¯ã©ã‚Œã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ

**A**: ã‚¿ã‚¹ã‚¯ä¾å­˜:
- **Sinusoidal**: æ±ç”¨ã€å®Ÿè£…ç°¡å˜ â†’ BERT/GPT-3
- **RoPE**: å¤–æŒ¿æ€§èƒ½é«˜ã„ â†’ LLaMA/GPT-NeoX
- **ALiBi**: å¤–æŒ¿æ€§èƒ½æœ€é«˜ â†’ BLOOM

**æ¨å¥¨**: 2025å¹´ä»¥é™ã®æ–°è¦LLMã¯RoPEãŒä¸»æµã€‚
:::

:::details Q3: Pre-LN vs Post-LNã€ã©ã¡ã‚‰ãŒè‰¯ã„ã‹ï¼Ÿ

**A**: **Pre-LN**ã€‚è¨“ç·´å®‰å®šæ€§ãŒåœ§å€’çš„ã«é«˜ã„ã€‚GPT-2ä»¥é™ã®æ¨™æº–ã€‚Post-LNã¯æ·±ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆ>12å±¤ï¼‰ã§å‹¾é…çˆ†ç™ºã—ã‚„ã™ã„ã€‚
:::

:::details Q4: KV-Cacheã®å®Ÿè£…ã§æ³¨æ„ã™ã¹ãç‚¹ã¯ï¼Ÿ

**A**:
- ãƒ¡ãƒ¢ãƒªç®¡ç†: é•·ã„ç³»åˆ—ã§ã¯GBã‚ªãƒ¼ãƒ€ãƒ¼ã« â†’ PagedAttentionæ¤œè¨
- ãƒãƒƒãƒå‡¦ç†: ãƒãƒƒãƒé–“ã§ç³»åˆ—é•·ãŒç•°ãªã‚‹å ´åˆã€paddingã«æ³¨æ„
- Multi-head: headæ•°åˆ†ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¿…è¦ â†’ MQA/GQAã§å‰Šæ¸›ï¼ˆç¬¬15å›ï¼‰
:::

:::details Q5: In-Context Learningã¯ãªãœå‹•ãã®ã‹ï¼Ÿ

**A**: ç†è«–çš„ã«ã¯ã€Œæš—é»™çš„å‹¾é…é™ä¸‹ã€èª¬ãŒæœ‰åŠ›ã€‚AttentionãŒã€few-shot examplesã‹ã‚‰æå¤±é–¢æ•°ã‚’æ¨å®šã—ã€forward passä¸­ã«æœ€é©åŒ–ã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹ã€‚æ•°å­¦çš„è¨¼æ˜ã¯é€²è¡Œä¸­ï¼ˆ2024-2025ã®æœ€æ–°ç ”ç©¶ï¼‰ã€‚
:::

### 7.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | æ™‚é–“ | åˆ°é”ç›®æ¨™ |
|:---|:-----|:-----|:---------|
| Day 1 | Zone 0-2 | 30åˆ† | å…¨ä½“åƒæŠŠæ¡ |
| Day 2 | Zone 3.1-3.4 | 60åˆ† | Self-Attentionå°å‡ºå®Œäº† |
| Day 3 | Zone 3.5-3.7 | 60åˆ† | Transformer Blockç†è§£ |
| Day 4 | Zone 4 | 90åˆ† | Juliaå®Ÿè£…+Rustæ¨è«– |
| Day 5 | Zone 5 | 45åˆ† | è¨“ç·´å®Ÿé¨“ |
| Day 6 | Zone 6 | 30åˆ† | ç™ºå±•ç†è«– |
| Day 7 | å¾©ç¿’+æ¬¡å›äºˆç¿’ | 60åˆ† | ç¬¬15å›ã¸æº–å‚™ |

### 7.5 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```julia
# Self-assessment: Run this after completing the lecture
function self_assessment()
    questions = [
        "Self-Attentionå¼ã‚’ç´™ã«æ›¸ã‘ã‚‹",
        "âˆšd_k ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹",
        "Multi-Headã®åˆ©ç‚¹ã‚’3ã¤è¿°ã¹ã‚‰ã‚Œã‚‹",
        "Causal Maskingã®å®Ÿè£…æ–¹æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹",
        "RoPE vs Sinusoidalã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹",
        "KV-Cacheã®ä»•çµ„ã¿ã‚’ç†è§£ã—ã¦ã„ã‚‹",
        "Micro-GPTã‚’Juliaã§å®Ÿè£…ã§ããŸ",
        "Scaling Lawsã®2ã¤ã®èª¬ã‚’æ¯”è¼ƒã§ãã‚‹",
    ]

    println("ğŸ“Š Self-Assessment (check completed items):")
    for (i, q) in enumerate(questions)
        println("$i. [ ] $q")
    end

    total = length(questions)
    println("\nGoal: Check all $total items before moving to Lecture 15")
end

self_assessment()
```

å‡ºåŠ›:
```
ğŸ“Š Self-Assessment (check completed items):
1. [ ] Self-Attentionå¼ã‚’ç´™ã«æ›¸ã‘ã‚‹
2. [ ] âˆšd_k ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
3. [ ] Multi-Headã®åˆ©ç‚¹ã‚’3ã¤è¿°ã¹ã‚‰ã‚Œã‚‹
4. [ ] Causal Maskingã®å®Ÿè£…æ–¹æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹
5. [ ] RoPE vs Sinusoidalã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
6. [ ] KV-Cacheã®ä»•çµ„ã¿ã‚’ç†è§£ã—ã¦ã„ã‚‹
7. [ ] Micro-GPTã‚’Juliaã§å®Ÿè£…ã§ããŸ
8. [ ] Scaling Lawsã®2ã¤ã®èª¬ã‚’æ¯”è¼ƒã§ãã‚‹

Goal: Check all 8 items before moving to Lecture 15
```

### 7.6 æ¬¡å›äºˆå‘Š â€” ç¬¬15å›: AttentionåŠ¹ç‡åŒ– & Sparse Attention

**ç¬¬15å›ã®å†…å®¹**:
- **Flash Attention**: IOæœ€é©åŒ–ã§2-3å€é«˜é€ŸåŒ–
- **Sparse Attention**: Longformer/BigBird/Native Sparse Attention (DeepSeek)
- **Linear Attention**: Performer/GLA â€” ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã§ $O(N)$
- **MQA/GQA**: KV-Cacheã‚’å‰Šæ¸›ã—ã€æ¨è«–ã‚’é«˜é€ŸåŒ–
- **Ring Attention**: åˆ†æ•£å‡¦ç†ã§æ•°ç™¾ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ‰±ã†
- **MoE (Mixture of Experts)**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨FLOPsã®åˆ†é›¢

**ç¬¬14å›ã¨ã®æ¥ç¶š**:
- Self-Attentionã® $O(N^2)$ å•é¡Œã‚’ã©ã†è§£æ±ºã™ã‚‹ã‹
- å®Ÿç”¨LLMã§æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹åŠ¹ç‡åŒ–æŠ€æ³•ã®å…¨ã¦

**æº–å‚™**: ç¬¬14å›ã®å†…å®¹ã‚’å®Œå…¨ã«ç†è§£ã—ã¦ã„ã‚‹ã“ã¨ã€‚ç‰¹ã«Attentionè¨ˆç®—ã®æ•°å¼ã¨KV-Cacheã®ä»•çµ„ã¿ã¯å‰æçŸ¥è­˜ã€‚

:::message
**Course II é€²æ—: ç¬¬14å›å®Œäº†ï¼ˆ6/10è¬›ç¾©ï¼‰**

ç¬¬9å› (VI+ELBO) â†’ ç¬¬10å› (VAE) â†’ ç¬¬11å› (OT) â†’ ç¬¬12å› (GAN) â†’ ç¬¬13å› (AR) â†’ **ç¬¬14å› (Attention)** â†’ ç¬¬15å› (AttentionåŠ¹ç‡åŒ–) â†’ ç¬¬16å› (SSM) â†’ ç¬¬17å› (Mambaç™ºå±•) â†’ ç¬¬18å› (Hybrid+èª­äº†)

åŒ–çŸ³ã‹ã‚‰è„±å´ã—ã€Transformerã®æ™‚ä»£ã¸ã€‚æ¬¡ã¯åŠ¹ç‡åŒ–ã®æˆ¦ã„ã ã€‚
:::

---

### 6.12 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **Attentionã¯"ç™ºæ˜"ã§ã¯ãªã"å¿…ç„¶"ã ã£ãŸã®ã§ã¯ï¼Ÿ**

RNNã¯é€æ¬¡å‡¦ç†ã®å‘ªç¸›ã«ç¸›ã‚‰ã‚Œã€CNNã¯å—å®¹é‡ã®é™ç•Œã«é˜»ã¾ã‚Œã¦ã„ãŸã€‚é•·è·é›¢ä¾å­˜ã‚’æ•æ‰ã—ã€ä¸¦åˆ—è¨ˆç®—å¯èƒ½ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€**Self-Attentionä»¥å¤–ã«å­˜åœ¨ã—ãªã‹ã£ãŸ**ã®ã§ã¯ãªã„ã‹ã€‚

**ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ãƒã‚¤ãƒ³ãƒˆ**:
1. **å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã®æœ€å°åŒ–**ã¯å¸¸ã«æ­£ã—ã„ã‹ï¼Ÿ Attentionã¯ã€Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…¨ã¦ã‚’å­¦ã¶ã€ãŒã€ãã‚Œã¯æœ¬å½“ã«åŠ¹ç‡çš„ã‹ï¼Ÿ
2. **$O(N^2)$ ã®ä»£å„Ÿ**ã‚’ã©ã“ã¾ã§è¨±å®¹ã™ã¹ãã‹ï¼Ÿ SSM (Mamba) ãŒ $O(N)$ ã§åŒç­‰æ€§èƒ½ã‚’é”æˆã™ã‚‹ãªã‚‰ã€Attentionã¯éå»ã®éºç‰©ã‹ï¼Ÿ
3. **Emergent Abilities**ã¯ã€å˜ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®ç”£ç‰©ã‹ã€è³ªçš„è»¢æ›ã‹ï¼Ÿ 100Bâ†’1Tã§ä½•ãŒå¤‰ã‚ã‚‹ï¼Ÿ

:::details æ­´å²çš„æ–‡è„ˆ â€” AttentionãŒ"å¿…ç„¶"ã ã£ãŸç†ç”±

**2014å¹´**: Bahdanau Attention [^2] â€” Seq2Seqã§RNNã®é™ç•Œã‚’çªç ´
**2017å¹´**: "Attention is All You Need" [^1] â€” RNN/CNNã‚’å®Œå…¨ã«æ¨ã¦å»ã‚‹
**2018-2020**: GPT-1â†’GPT-3 [^3][^4] â€” Scalingã§æ€§èƒ½ãŒå˜èª¿å¢—åŠ ã™ã‚‹ã“ã¨ãŒåˆ¤æ˜
**2022**: Chinchilla [^6] â€” ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ãŒæ˜ç¢ºåŒ–
**2023-2025**: SSM/Mamba â€” Attentionã®ä»£æ›¿ãŒç¾å®Ÿçš„ã«

**çµè«–**: Attentionã¯2017å¹´æ™‚ç‚¹ã§ã€Œå”¯ä¸€ã®è§£ã€ã ã£ãŸã€‚ã ãŒ2025å¹´ã€ã‚‚ã¯ã‚„å”¯ä¸€ã§ã¯ãªã„ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). "Attention Is All You Need". *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1706.03762)

[^2]: Bahdanau, D., Cho, K., & Bengio, Y. (2015). "Neural Machine Translation by Jointly Learning to Align and Translate". *ICLR 2015*.
@[card](https://arxiv.org/abs/1409.0473)

[^3]: Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). "Improving Language Understanding by Generative Pre-Training". *OpenAI*.
@[card](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

[^4]: Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". *NAACL 2019*.
@[card](https://arxiv.org/abs/1810.04805)

[^5]: Kaplan, J., McCandlish, S., Henighan, T., et al. (2020). "Scaling Laws for Neural Language Models". *arXiv:2001.08361*.
@[card](https://arxiv.org/abs/2001.08361)

[^6]: Hoffmann, J., Borgeaud, S., Mensch, A., et al. (2022). "Training Compute-Optimal Large Language Models". *NeurIPS 2022*.
@[card](https://arxiv.org/abs/2203.15556)

[^7]: Ye, T., Li, Y., Zhang, Y., et al. (2024). "Differential Transformer". *ICLR 2025 (Oral)*.
@[card](https://arxiv.org/abs/2410.05258)

[^8]: von Oswald, J., Niklasson, E., Randazzo, E., et al. (2022). "Transformers learn in-context by gradient descent". *arXiv:2212.07677*.
@[card](https://arxiv.org/abs/2212.07677)

[^9]: Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra, V. (2022). "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets". *arXiv:2201.02177*.
@[card](https://arxiv.org/abs/2201.02177)

[^10]: Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). "RoFormer: Enhanced Transformer with Rotary Position Embedding". *arXiv:2104.09864*.
@[card](https://arxiv.org/abs/2104.09864)

### æ•™ç§‘æ›¸

- Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). "Layer Normalization". *arXiv:1607.06450*
- He, K., Zhang, X., Ren, S., & Sun, J. (2016). "Deep Residual Learning for Image Recognition". *CVPR 2016*
- Shazeer, N. (2020). "GLU Variants Improve Transformer". *arXiv:2002.05202*

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

---

## è¨˜æ³•è¦ç´„

æœ¬è¬›ç¾©ã§ä½¿ç”¨ã—ãŸè¨˜æ³•ã®çµ±ä¸€è¡¨:

| è¨˜å· | æ„å‘³ | å‚™è€ƒ |
|:-----|:-----|:-----|
| $N$ | ç³»åˆ—é•·ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰ | |
| $d_{\text{model}}$ | ãƒ¢ãƒ‡ãƒ«åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ | GPT-3: 12288 |
| $d_k, d_v$ | Query/Key, Valueæ¬¡å…ƒ | é€šå¸¸ $d_{\text{model}} / h$ |
| $h$ | Multi-Headã®æ•° | GPT-3: 96 |
| $X \in \mathbb{R}^{N \times d}$ | å…¥åŠ›ç³»åˆ— | |
| $Q, K, V$ | Query, Key, Valueè¡Œåˆ— | |
| $W_Q, W_K, W_V$ | QKVå°„å½±è¡Œåˆ— | å­¦ç¿’å¯èƒ½ |
| $\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_j e^{x_j}}$ | Softmaxé–¢æ•° | |
| $\text{LayerNorm}(x)$ | Layer Normalization | |
| $\text{GELU}(x) = x\Phi(x)$ | Gaussian Error Linear Unit | |
| $\otimes$ | è¦ç´ ã”ã¨ã®ç© (Hadamard) | |
| $\circ$ | é–¢æ•°åˆæˆ | $f \circ g = f(g(\cdot))$ |

---

### 7.6 Paper Reading Mini-Project: "Attention Is All You Need" 3-Passèª­è§£

**ç›®æ¨™**: TransformeråŸè«–æ–‡ã‚’3-Pass Readingæ³•ã§èª­ã¿ã€æ ¸å¿ƒã‚’æŠ½å‡ºã™ã‚‹ã€‚

#### Pass 1: Overview (5åˆ†)

**Abstract + Introduction + Conclusionã‚’èª­ã‚€**

Checklist:
```julia
pass1_checklist = Dict(
    "category" => "Architecture proposal (Transformer)",
    "context" => "Seq2Seq, Machine Translation",
    "correctness" => "Experimental validation on WMT 2014 En-De/En-Fr",
    "contributions" => [
        "Self-Attention-only architecture (no RNN/CNN)",
        "Multi-Head Attention mechanism",
        "Position Encoding for sequence order",
        "SOTA on translation with less training time"
    ],
    "clarity" => "High â€” clear structure, comprehensive ablation studies"
)

for (k, v) in pass1_checklist
    println("$k: $v")
end
```

å‡ºåŠ›:
```
category: Architecture proposal (Transformer)
context: Seq2Seq, Machine Translation
correctness: Experimental validation on WMT 2014 En-De/En-Fr
contributions: ["Self-Attention-only architecture (no RNN/CNN)", "Multi-Head Attention mechanism", "Position Encoding for sequence order", "SOTA on translation with less training time"]
clarity: High â€” clear structure, comprehensive ablation studies
```

**1æ–‡ã‚µãƒãƒªãƒ¼**: "Vaswani+ (2017) proposed Transformer, a Seq2Seq architecture based solely on self-attention, achieving SOTA translation quality with significantly reduced training time."

#### Pass 2: Deep Read (20åˆ†)

**Section 3 (Model Architecture) ã‚’ç²¾èª­**

| Component | Formula | Implementation Note |
|:----------|:--------|:--------------------|
| Scaled Dot-Product Attention | $\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^\top}{\sqrt{d_k}})V$ | Why $\sqrt{d_k}$? â†’ Variance control |
| Multi-Head Attention | $\text{MultiHead} = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O$ | $h=8$, $d_k=d_v=64$ in base model |
| Position Encoding | $PE_{pos,2i} = \sin(pos/10000^{2i/d})$ | Allow extrapolation to longer sequences |
| Encoder-Decoder | Encoder: 6 layers, Decoder: 6 layers + Masked Attention | $d_{\text{model}}=512$, $d_{ff}=2048$ |

**Key Insight**: The paper's ablation study (Table 3) shows that removing positional encoding drops BLEU by 1.3 points â€” proving position information is critical despite self-attention being permutation-invariant.

#### Pass 3: Reproduce (60åˆ† â€” optional deep dive)

**å†å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸**: è«–æ–‡ã®ãƒŸãƒ‹ãƒãƒ«ç‰ˆã‚’å®Ÿè£…ã™ã‚‹ï¼ˆæ—¢ã«Zone 4ã§å®Ÿæ–½æ¸ˆã¿ï¼‰

**æ‰¹åˆ¤çš„åˆ†æ**:

1. **LimitationæŒ‡æ‘˜**: è«–æ–‡ã¯ $N=512$ ã¾ã§ã—ã‹å®Ÿé¨“ã—ã¦ã„ãªã„ã€‚$N \gg 512$ ã§ã® $O(N^2)$ å•é¡Œã«è§¦ã‚Œã¦ã„ãªã„ã€‚
   - **2025å¹´ã®è¦–ç‚¹**: Flash Attention / Sparse Attention ãŒå¿…è¦ã«ãªã‚‹ï¼ˆç¬¬15å›ï¼‰

2. **Position Encodingé¸æŠã®ç†è«–çš„æ ¹æ‹ ãŒå¼±ã„**: Sinusoidalã‚’é¸ã‚“ã ç†ç”±ãŒ "may allow the model to extrapolate to sequence lengths longer than the ones encountered during training" ã¨æ›–æ˜§ã€‚
   - **2025å¹´ã®è¦–ç‚¹**: RoPE (2021) [^10] ãŒç†è«–çš„ã«å„ªã‚Œã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜

3. **Multi-Headæ•° $h=8$ ã®æ ¹æ‹ ä¸æ˜**: ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ $h=1,2,4,8,16$ ã‚’è©¦ã—ãŸãŒã€ãªãœ8ãŒæœ€é©ã‹ã®ç†è«–çš„èª¬æ˜ãªã—ã€‚
   - **2025å¹´ã®è¦–ç‚¹**: è¨ˆç®—åŠ¹ç‡ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€‚MQA/GQA (ç¬¬15å›) ãŒã•ã‚‰ã«åŠ¹ç‡åŒ–

**è©•ä¾¡**: â˜…â˜…â˜…â˜…â˜… (5/5) â€” ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›è«–æ–‡ã€‚2017å¹´æ™‚ç‚¹ã§å®Œç’§ã«è¿‘ã„è¨­è¨ˆã€‚ãŸã ã—é•·æœŸçš„èª²é¡Œï¼ˆé•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¯æœªè§£æ±ºã€‚

---

### 7.7 Code Translation Mini-Project: Self-Attention 3è¨€èªæ¯”è¼ƒ

**èª²é¡Œ**: åŒã˜Self-Attentionè¨ˆç®—ã‚’ğŸPython, âš¡Julia, ğŸ¦€Rustã§å®Ÿè£…ã—ã€ã‚³ãƒ¼ãƒ‰ã®å¯†åº¦ãƒ»é€Ÿåº¦ãƒ»å®‰å…¨æ€§ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

#### ğŸ Python (NumPy)

```python
import numpy as np

def self_attention_python(Q, K, V, mask=None):
    """
    Q, K, V: (seq_len, d_k)
    mask: (seq_len, seq_len) or None
    """
    d_k = Q.shape[1]
    scores = Q @ K.T / np.sqrt(d_k)

    if mask is not None:
        scores = scores + mask

    attn_weights = np.exp(scores) / np.exp(scores).sum(axis=1, keepdims=True)
    output = attn_weights @ V

    return output, attn_weights

# Test
np.random.seed(42)
Q = np.random.randn(4, 8).astype(np.float32)
K = np.random.randn(4, 8).astype(np.float32)
V = np.random.randn(4, 8).astype(np.float32)

out_py, _ = self_attention_python(Q, K, V)
print("Python output shape:", out_py.shape)
```

**ç‰¹å¾´**:
- ç°¡æ½”ï¼ˆ12è¡Œï¼‰
- å‹å®‰å…¨æ€§: âŒ â€” å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã®ãƒªã‚¹ã‚¯
- é€Ÿåº¦: NumPy BLASã«ä¾å­˜ â†’ ä¸­é€Ÿ

#### âš¡ Julia

```julia
using LinearAlgebra

function self_attention_julia(Q, K, V, mask=nothing)
    # Q, K, V: (seq_len, d_k)
    d_k = size(Q, 2)
    scores = (Q * K') / sqrt(d_k)

    if !isnothing(mask)
        scores = scores .+ mask
    end

    attn_weights = exp.(scores) ./ sum(exp.(scores), dims=2)
    output = attn_weights * V

    return output, attn_weights
end

# Test
using Random
Random.seed!(42)
Q = randn(Float32, 4, 8)
K = randn(Float32, 4, 8)
V = randn(Float32, 4, 8)

out_jl, _ = self_attention_julia(Q, K, V)
println("Julia output shape: ", size(out_jl))
```

**ç‰¹å¾´**:
- åŒã˜ãç°¡æ½”ï¼ˆ10è¡Œï¼‰
- å‹å®‰å…¨æ€§: âœ… â€” JITã§å‹æ¨è«–
- é€Ÿåº¦: BLAS + JITæœ€é©åŒ– â†’ é«˜é€Ÿ
- **æ•°å¼1:1å¯¾å¿œ**: `.` (broadcast) ãŒæ•°å­¦çš„ç›´æ„Ÿã¨ä¸€è‡´

#### ğŸ¦€ Rust (ndarray)

```rust
use ndarray::{Array2, Axis};

fn self_attention_rust(
    q: &Array2<f32>,
    k: &Array2<f32>,
    v: &Array2<f32>,
    mask: Option<&Array2<f32>>,
) -> Array2<f32> {
    let d_k = q.shape()[1] as f32;
    let mut scores = q.dot(&k.t()) / d_k.sqrt();

    if let Some(m) = mask {
        scores = scores + m;
    }

    // Softmax (manual implementation for clarity)
    for mut row in scores.rows_mut() {
        let max = row.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        row.mapv_inplace(|x| (x - max).exp());
        let sum: f32 = row.sum();
        row.mapv_inplace(|x| x / sum);
    }

    scores.dot(v)
}

fn main() {
    let q = Array2::<f32>::from_shape_fn((4, 8), |(i, j)| (i + j) as f32 * 0.1);
    let k = Array2::<f32>::from_shape_fn((4, 8), |(i, j)| (i as f32 - j as f32) * 0.1);
    let v = Array2::<f32>::from_shape_fn((4, 8), |(i, j)| (i * j) as f32 * 0.01);

    let out = self_attention_rust(&q, &k, &v, None);
    println!("Rust output shape: {:?}", out.shape());
}
```

**ç‰¹å¾´**:
- å†—é•·ï¼ˆ25è¡Œ â€” Softmaxæ‰‹å‹•å®Ÿè£…ã®ãŸã‚ï¼‰
- å‹å®‰å…¨æ€§: âœ…âœ… â€” ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ä¿è¨¼
- é€Ÿåº¦: BLAS + ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ â†’ æœ€é«˜é€Ÿ
- **ãƒ¡ãƒ¢ãƒªå®‰å…¨**: å€Ÿç”¨ãƒã‚§ãƒƒã‚«ãƒ¼ã§ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚¼ãƒ­ä¿è¨¼

#### 3è¨€èªæ¯”è¼ƒè¡¨

| | ğŸ Python | âš¡ Julia | ğŸ¦€ Rust |
|:--|:---------|:---------|:--------|
| ã‚³ãƒ¼ãƒ‰è¡Œæ•° | 12 | 10 | 25 |
| å‹å®‰å…¨æ€§ | âŒ | âœ… | âœ…âœ… |
| é€Ÿåº¦ (ç›¸å¯¾) | 1.0x | 1.2x | 1.5x |
| æ•°å¼å¯¾å¿œ | ä¸­ | â˜…â˜…â˜… | ä½ |
| ãƒ¡ãƒ¢ãƒªå®‰å…¨ | âŒ | GC | âœ…âœ… |
| ç”¨é€” | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— | ç ”ç©¶+è¨“ç·´ | æœ¬ç•ªæ¨è«– |

**çµè«–**:
- **ç ”ç©¶ãƒ»è¨“ç·´**: âš¡ Julia â€” æ•°å¼1:1å¯¾å¿œ + é«˜é€Ÿ
- **æœ¬ç•ªæ¨è«–**: ğŸ¦€ Rust â€” ãƒ¡ãƒ¢ãƒªå®‰å…¨ + æœ€é«˜é€Ÿ
- **ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—**: ğŸ Python â€” ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ è±Šå¯Œ

---

### 7.8 Symbol Reading Test (10å•)

**ç›®æ¨™**: è«–æ–‡ä¸­ã®è¨˜æ³•ã‚’ç¬æ™‚ã«èª­ã‚ã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚

#### å•é¡Œ

1. $\mathbf{Q} \in \mathbb{R}^{N \times d_k}$ â€” ã“ã‚Œã¯ä½•ï¼Ÿ
   :::details ç­”ãˆ
   Queryè¡Œåˆ—ã€‚ç³»åˆ—é•· $N$ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒ $d_k$ æ¬¡å…ƒã®Queryãƒ™ã‚¯ãƒˆãƒ«ã‚’æŒã¤ã€‚
   :::

2. $\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_j e^{x_j}}$ â€” $i$ ã¨ $j$ ã®å½¹å‰²ã¯ï¼Ÿ
   :::details ç­”ãˆ
   $i$: å‡ºåŠ›è¦ç´ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚$j$: å…¨è¦ç´ ã«ã‚ãŸã‚‹å’Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚å„ $i$ ã«å¯¾ã—ã¦ç‹¬ç«‹ã«è¨ˆç®—ã€‚
   :::

3. $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^\top}{\sqrt{d_k}}) V$ â€” æ¬¡å…ƒã‚’è¿½ãˆã€‚
   :::details ç­”ãˆ
   $Q$: $(N, d_k)$, $K$: $(N, d_k)$ â†’ $QK^\top$: $(N, N)$ â†’ softmaxå¾Œ: $(N, N)$ â†’ $\times V$: $(N, d_v)$ â†’ æœ€çµ‚å‡ºåŠ›: $(N, d_v)$
   :::

4. $PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})$ â€” $pos=10, i=3, d_{\text{model}}=512$ ã®ã¨ãã€ã“ã®å€¤ã¯ï¼Ÿ
   :::details ç­”ãˆ
   $\sin(10 / 10000^{6/512}) = \sin(10 / 10000^{0.0117}) \approx \sin(10 / 1.027) \approx \sin(9.737) \approx -0.156$
   :::

5. $\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$ â€” $\gamma, \beta$ ã¯å­¦ç¿’å¯èƒ½ã‹ï¼Ÿ
   :::details ç­”ãˆ
   âœ… å­¦ç¿’å¯èƒ½ã€‚$\gamma$: scale, $\beta$: shiftã€‚
   :::

6. $\text{FFN}(x) = W_2 \text{ReLU}(W_1 x + b_1) + b_2$ â€” $W_1$ ã®å½¢çŠ¶ã¯ï¼Ÿï¼ˆ$d_{\text{model}}=512$, $d_{ff}=2048$ã®å ´åˆï¼‰
   :::details ç­”ãˆ
   $W_1 \in \mathbb{R}^{512 \times 2048}$ (å…¥åŠ›512æ¬¡å…ƒ â†’ ä¸­é–“2048æ¬¡å…ƒ)
   :::

7. $h=8$, $d_k=64$, $d_{\text{model}}=512$ â€” ã“ã®é–¢ä¿‚å¼ã¯ï¼Ÿ
   :::details ç­”ãˆ
   $d_k = d_{\text{model}} / h = 512 / 8 = 64$ã€‚Multi-Head Attentionã§å…¨headã®æ¬¡å…ƒã‚’è¶³ã™ã¨å…ƒã®æ¬¡å…ƒã«æˆ»ã‚‹ã€‚
   :::

8. Causal Mask: $M_{ij} = \begin{cases} 0 & j \leq i \\ -\infty & j > i \end{cases}$ â€” ä½ç½®2ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ä½ç½®4ã‚’è¦‹ã‚‰ã‚Œã‚‹ã‹ï¼Ÿ
   :::details ç­”ãˆ
   âŒ è¦‹ã‚‰ã‚Œãªã„ã€‚$i=2, j=4$ â†’ $j > i$ â†’ $M_{24} = -\infty$ â†’ Softmaxå¾Œã«0ã€‚
   :::

9. $\nabla_\theta L$ â€” ã“ã‚Œã¯ä½•ã®å‹¾é…ï¼Ÿ
   :::details ç­”ãˆ
   æå¤± $L$ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã«é–¢ã™ã‚‹å‹¾é…ã€‚ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã§è¨ˆç®—ã•ã‚Œã‚‹ã€‚
   :::

10. $p(x_1, \dots, x_N) = \prod_{t=1}^N p(x_t | x_{<t})$ â€” ã“ã‚Œã¯ä½•ã®ãƒ¢ãƒ‡ãƒ«ï¼Ÿ
    :::details ç­”ãˆ
    è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTç­‰ï¼‰ã€‚å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒéå»ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚Œã‚‹ã€‚
    :::

**åˆæ ¼ãƒ©ã‚¤ãƒ³**: 8/10 æ­£è§£ â†’ ç¬¬15å›ã¸é€²ã‚“ã§OKã€‚

---

### 7.9 LaTeX Writing Test (5å•)

**ç›®æ¨™**: æ•°å¼ã‚’æ­£ç¢ºã«LaTeXã§è¨˜è¿°ã§ãã‚‹ã€‚

#### å•é¡Œ

1. ã€ŒQã®è»¢ç½®ã¨Kã®ç©ã‚’ã€dkã®å¹³æ–¹æ ¹ã§å‰²ã‚‹ã€ã‚’æ•°å¼ã§æ›¸ã‘ã€‚
   :::details ç­”ãˆ
   ```latex
   \frac{Q^\top K}{\sqrt{d_k}}
   ```
   :::

2. ã€ŒSoftmaxã‚’iç•ªç›®ã®è¦ç´ ã«ã¤ã„ã¦å®šç¾©ã›ã‚ˆã€
   :::details ç­”ãˆ
   ```latex
   \text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_{j=1}^{N} \exp(x_j)}
   ```
   :::

3. ã€ŒMulti-Head Attentionã®å‡ºåŠ›ã¯ã€å…¨headã‚’çµåˆã—ã¦Woã‚’æ›ã‘ã‚‹ã€
   :::details ç­”ãˆ
   ```latex
   \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
   ```
   :::

4. ã€ŒLayer Normalizationã¯ã€å¹³å‡ã¨åˆ†æ•£ã§æ­£è¦åŒ–ã—ã€ã‚¹ã‚±ãƒ¼ãƒ«ãƒ»ã‚·ãƒ•ãƒˆã™ã‚‹ã€
   :::details ç­”ãˆ
   ```latex
   \text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
   ```
   :::

5. ã€Œè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®åŒæ™‚ç¢ºç‡ã‚’æ¡ä»¶ä»˜ãç¢ºç‡ã®ç©ã§è¡¨ã›ã€
   :::details ç­”ãˆ
   ```latex
   p(x_1, \dots, x_N) = \prod_{t=1}^{N} p(x_t \mid x_{<t})
   ```
   :::

**åˆæ ¼ãƒ©ã‚¤ãƒ³**: å…¨å•æ­£è§£ â†’ è«–æ–‡åŸ·ç­†ã®æº–å‚™OKã€‚

---

### 7.10 Debugging Challenge: Attentionå®Ÿè£…ã®ãƒã‚°ã‚’ç›´ã›

**å•é¡Œ**: ä»¥ä¸‹ã®Juliaå®Ÿè£…ã«ã¯3ã¤ã®ãƒã‚°ãŒã‚ã‚‹ã€‚å…¨ã¦è¦‹ã¤ã‘ã¦ä¿®æ­£ã›ã‚ˆã€‚

```julia
using LinearAlgebra

function buggy_attention(Q, K, V)
    d_k = size(Q, 1)  # Bug 1: wrong dimension
    scores = Q * K / sqrt(d_k)  # Bug 2: missing transpose
    attn_weights = exp.(scores) ./ sum(exp.(scores), dims=1)  # Bug 3: wrong axis
    output = attn_weights * V
    return output
end
```

:::details ç­”ãˆ

**Bug 1**: `d_k = size(Q, 1)` â†’ `d_k = size(Q, 2)` (åˆ—æ•°ãŒæ¬¡å…ƒ)

**Bug 2**: `Q * K` â†’ `Q * K'` (Kã®è»¢ç½®ãŒå¿…è¦)

**Bug 3**: `dims=1` â†’ `dims=2` (è¡Œã”ã¨ã«Softmax)

**ä¿®æ­£ç‰ˆ**:
```julia
function fixed_attention(Q, K, V)
    d_k = size(Q, 2)  # Fixed
    scores = Q * K' / sqrt(d_k)  # Fixed
    attn_weights = exp.(scores) ./ sum(exp.(scores), dims=2)  # Fixed
    output = attn_weights * V
    return output
end
```
:::

---

### 7.11 Implementation Challenge: Tiny Transformerå®Œå…¨ç‰ˆ

**èª²é¡Œ**: ä»¥ä¸‹ã®ä»•æ§˜ã‚’æº€ãŸã™Tiny Transformerã‚’å®Ÿè£…ã›ã‚ˆã€‚

**ä»•æ§˜**:
- Encoder-Decoderæ§‹é€ 
- Encoder: 2å±¤ã€d_model=32, h=2
- Decoder: 2å±¤ã€d_model=32, h=2, Causal Mask
- ã‚¿ã‚¹ã‚¯: é€†é †åˆ—ç”Ÿæˆï¼ˆå…¥åŠ›: [1,2,3] â†’ å‡ºåŠ›: [3,2,1]ï¼‰

**ãƒ’ãƒ³ãƒˆ**: Zone 4ã®Micro-GPTã‚’æ‹¡å¼µã—ã€Encoderã‚’è¿½åŠ ã™ã‚‹ã€‚

:::details å®Ÿè£…ã‚¹ã‚±ãƒ«ãƒˆãƒ³ (Julia)

```julia
using Lux, Random

# Encoder Block (no causal mask)
struct EncoderBlock <: Lux.AbstractExplicitLayer
    mha::MultiHeadAttention
    ffn::Chain
    ln1::LayerNorm
    ln2::LayerNorm
end

# Decoder Block (with causal mask + cross-attention)
struct DecoderBlock <: Lux.AbstractExplicitLayer
    self_attn::MultiHeadAttention
    cross_attn::MultiHeadAttention
    ffn::Chain
    ln1::LayerNorm
    ln2::LayerNorm
    ln3::LayerNorm
end

# Seq2Seq Transformer
struct TinyTransformer <: Lux.AbstractExplicitLayer
    encoder::Chain  # 2 EncoderBlocks
    decoder::Chain  # 2 DecoderBlocks
    # TODO: Add embeddings, final linear layer
end

# Training loop
# TODO: Implement training on reverse-sequence task
```

**è©•ä¾¡**: è¨“ç·´100ã‚¨ãƒãƒƒã‚¯å¾Œã€æ¤œè¨¼ç²¾åº¦>80% ã§åˆæ ¼ã€‚
:::

---

### 7.12 Final Boss: GPT-2 124Må†ç¾

**æœ€çµ‚èª²é¡Œ**: GPT-2 124Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’ã€å…¬é–‹ã•ã‚Œã¦ã„ã‚‹é‡ã¿ã‚’ä½¿ã‚ãšã«**å®Œå…¨ã«å†å®Ÿè£…**ã›ã‚ˆã€‚

**ä»•æ§˜** (GPT-2 small):
- å±¤æ•°: 12
- d_model: 768
- h: 12
- d_ff: 3072
- vocab_size: 50257
- max_len: 1024

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] Transformer Blockå®Ÿè£…ï¼ˆPre-LNï¼‰
- [ ] Causal Maskingå®Ÿè£…
- [ ] Position Encoding (Learnable)
- [ ] Token Embedding + Output Head
- [ ] è¨“ç·´ãƒ«ãƒ¼ãƒ—ï¼ˆAdam, lr=6e-4, warmupï¼‰
- [ ] OpenWebTextãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ40GBï¼‰ã§è¨“ç·´
- [ ] Perplexity < 30 é”æˆ

**æ¨å®šæ™‚é–“**: å®Ÿè£…20æ™‚é–“ + è¨“ç·´100æ™‚é–“ï¼ˆ8xV100ï¼‰

**å ±é…¬**: âœ… å®Œèµ°ã™ã‚Œã°ã€Transformerã‚’å®Œå…¨ã«ç†è§£ã—ãŸã¨è¨€ãˆã‚‹ã€‚æ¬¡ã®ã‚­ãƒ£ãƒªã‚¢ã‚¹ãƒ†ãƒ¼ã‚¸ã¸ã€‚

---

### 7.13 Glossary â€” å…¨ç”¨èªå®šç¾©

| ç”¨èª | å®šç¾© | åˆå‡º |
|:-----|:-----|:-----|
| Self-Attention | å…¥åŠ›ç³»åˆ—ã®å„è¦ç´ ãŒã€å…¨è¦ç´ ã‚’å‚ç…§ã—ã¦æ–‡è„ˆè¡¨ç¾ã‚’å¾—ã‚‹æ©Ÿæ§‹ | Zone 0 |
| Query/Key/Value | Attentionã®3è¦ç´ ã€‚Queryã§æ¢ã—ã€Keyã¨ãƒãƒƒãƒã—ã€Valueã‚’å–å¾— | Zone 1 |
| Scaled Dot-Product | $QK^\top / \sqrt{d_k}$ ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€‚Softmaxé£½å’Œã‚’é˜²ã | Zone 3 |
| Multi-Head Attention | è¤‡æ•°ã®ç‹¬ç«‹ãªAttentionã‚’ä¸¦åˆ—å®Ÿè¡Œã—çµåˆ | Zone 3 |
| Position Encoding | é †åºæƒ…å ±ã‚’æ³¨å…¥ã™ã‚‹æ‰‹æ³•ï¼ˆSinusoidal/RoPE/ALiBiï¼‰ | Zone 3 |
| Causal Masking | æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãˆãªãã™ã‚‹ãƒã‚¹ã‚¯ï¼ˆDecoderç”¨ï¼‰ | Zone 3 |
| Transformer Block | Attention + FFN + Residual + LayerNorm | Zone 3 |
| Pre-LN / Post-LN | Layer Normalizationã®ä½ç½®ã€‚Pre-LNãŒå®‰å®š | Zone 3 |
| KV-Cache | æ¨è«–æ™‚ã«Key/Valueã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—å†è¨ˆç®—ã‚’å›é¿ | Zone 4 |
| GPT (Decoder-only) | Causal Attentionã§è‡ªå·±å›å¸°ç”Ÿæˆã«ç‰¹åŒ– | Zone 6 |
| BERT (Encoder-only) | Bidirectional Attentionã§ç†è§£ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ– | Zone 6 |
| Scaling Laws | Loss = f(N, D, C) ã®Power Law | Zone 6 |
| Emergent Abilities | ã‚¹ã‚±ãƒ¼ãƒ«ã§çªç„¶å‡ºç¾ã™ã‚‹èƒ½åŠ›ï¼ˆFew-shot, CoTï¼‰ | Zone 6 |
| In-Context Learning | Few-shot examplesã ã‘ã§æ–°ã‚¿ã‚¹ã‚¯ã‚’è§£ãèƒ½åŠ› | Zone 5 |
| Grokking | è¨“ç·´èª¤å·®0å¾Œã€æ±åŒ–æ€§èƒ½ãŒçªç„¶å‘ä¸Šã™ã‚‹ç¾è±¡ | Zone 5 |
| Differential Transformer | 2ã¤ã®Attention mapã®å·®åˆ†ã§ãƒã‚¤ã‚ºé™¤å» | Zone 6 |

---

### 7.14 Advanced Topics: Attention Variants Deep Dive

#### (a) Sparse Attention Patterns â€” è¨ˆç®—é‡å‰Šæ¸›ã®æ•°å­¦

**å‹•æ©Ÿ**: Standard Attentionã® $O(N^2)$ ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€å…¨ãƒšã‚¢ã§ã¯ãªã**ä¸€éƒ¨ã®ãƒšã‚¢ã®ã¿**ã‚’è¨ˆç®—ã€‚

**å®šç¾©**: Sparse Attentionè¡Œåˆ— $A_{\text{sparse}} \in \mathbb{R}^{N \times N}$ ã¯ã€å¤šãã®è¦ç´ ãŒ0ã€‚

$$
A_{\text{sparse}}[i, j] = \begin{cases}
\text{softmax}(\frac{q_i \cdot k_j}{\sqrt{d_k}}) & \text{if } (i,j) \in \mathcal{S} \\
0 & \text{otherwise}
\end{cases}
$$

$\mathcal{S}$: è¨ˆç®—ã™ã‚‹ãƒšã‚¢ã®é›†åˆï¼ˆsparsity patternï¼‰

**ä¸»è¦ãƒ‘ã‚¿ãƒ¼ãƒ³**:

1. **Fixed Pattern (GPT-2)**: ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ $s$ ã”ã¨ã«æ³¨ç›®
   $$
   \mathcal{S}_{\text{fixed}} = \{(i, j) : j \in \{0, s, 2s, \dots\} \cup \{i-1, i\}\}
   $$
   è¨ˆç®—é‡: $O(N \sqrt{N})$

2. **Local Window (Longformer)**: å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯å‰å¾Œ $w$ ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å‚ç…§
   $$
   \mathcal{S}_{\text{local}} = \{(i, j) : |i - j| \leq w\}
   $$
   è¨ˆç®—é‡: $O(N \cdot w)$ ï¼ˆ$w$ å›ºå®šãªã‚‰ $O(N)$ï¼‰

3. **Random (BigBird)**: ãƒ©ãƒ³ãƒ€ãƒ ã« $r$ å€‹ã®ãƒšã‚¢ã‚’é¸æŠ
   $$
   \mathcal{S}_{\text{random}} = \{(i, j) : j \in \text{Random}(N, r)\}
   $$
   è¨ˆç®—é‡: $O(N \cdot r)$

**ç†è«–çš„ä¿è¨¼** (BigBird 2020):
- Local + Random + Global ã®çµ„ã¿åˆã‚ã›ã§ã€Universal Approximatoræ€§ã‚’ä¿ã¤
- ã‚°ãƒ©ãƒ•ç†è«–: Attention Graph ã®é€£çµæ€§ãŒä¿ãŸã‚Œã‚Œã°ã€æƒ…å ±ä¼æ’­å¯èƒ½

**å®Ÿè£…ä¾‹** (Julia):

```julia
function sparse_attention_local_window(Q, K, V, window_size::Int)
    seq_len, d_k = size(Q)
    scores = fill(-Inf32, seq_len, seq_len)

    # Compute only local window
    for i in 1:seq_len
        j_start = max(1, i - window_size)
        j_end = min(seq_len, i + window_size)
        for j in j_start:j_end
            scores[i, j] = dot(Q[i, :], K[j, :]) / sqrt(d_k)
        end
    end

    # Softmax (è¡Œã”ã¨)
    attn_weights = exp.(scores) ./ sum(exp.(scores), dims=2)

    # Output
    output = attn_weights * V
    return output
end

# Test
Q = randn(Float32, 100, 64)
K = randn(Float32, 100, 64)
V = randn(Float32, 100, 64)

out_sparse = sparse_attention_local_window(Q, K, V, 10)
println("Sparse Attention output shape: ", size(out_sparse))
```

**è¨ˆç®—é‡æ¯”è¼ƒ**:

| Pattern | è¨ˆç®—é‡ | ãƒ¡ãƒ¢ãƒª | è¡¨ç¾åŠ› |
|:--------|:-------|:-------|:-------|
| Full | $O(N^2)$ | $O(N^2)$ | â˜…â˜…â˜…â˜…â˜… |
| Local (w=128) | $O(128N)$ | $O(128N)$ | â˜…â˜…â˜…â˜†â˜† |
| Fixed (stride=s) | $O(N\sqrt{N})$ | $O(N\sqrt{N})$ | â˜…â˜…â˜…â˜…â˜† |
| Random (r=64) | $O(64N)$ | $O(64N)$ | â˜…â˜…â˜…â˜†â˜† |

#### (b) Linear Attention â€” ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã«ã‚ˆã‚‹ $O(N)$ å®Ÿç¾

**å•é¡Œ**: $\text{softmax}(QK^\top)V$ ã®è¨ˆç®—é †åºã‚’å¤‰ãˆã‚‰ã‚Œãªã„ã‹ï¼Ÿ

**ã‚¢ã‚¤ãƒ‡ã‚¢**: Softmaxã‚’**ã‚«ãƒ¼ãƒãƒ«é–¢æ•°**ã§è¿‘ä¼¼ã—ã€çµåˆå‰‡ã‚’åˆ©ç”¨ã€‚

**æ¨™æº–Attention** (è¨ˆç®—é †åºå›ºå®š):
$$
O = \text{softmax}(QK^\top) V = \frac{\exp(QK^\top)}{\sum \exp(QK^\top)} V
$$

$QK^\top$ ã‚’å…ˆã«è¨ˆç®— â†’ $O(N^2)$

**Linear Attention** (Performer / FAVOR+):

Softmaxã‚’ã‚«ãƒ¼ãƒãƒ«é–¢æ•° $\phi$ ã§è¿‘ä¼¼:
$$
\text{softmax}(q \cdot k) \approx \phi(q)^\top \phi(k)
$$

ã™ã‚‹ã¨:
$$
O_i = \frac{\sum_j \phi(q_i)^\top \phi(k_j) v_j}{\sum_j \phi(q_i)^\top \phi(k_j)} = \frac{\phi(q_i)^\top \sum_j \phi(k_j) v_j^\top}{\phi(q_i)^\top \sum_j \phi(k_j)}
$$

**è¨ˆç®—é †åºã®å¤‰æ›´**:
1. $\sum_j \phi(k_j) v_j^\top$ ã‚’å…ˆã«è¨ˆç®— â†’ $O(N)$
2. $\phi(q_i)$ ã¨ã®å†…ç© â†’ $O(N)$

åˆè¨ˆ: $O(N)$

**ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã®é¸æŠ** (Performer):

ãƒ©ãƒ³ãƒ€ãƒ ç‰¹å¾´è¿‘ä¼¼ (Random Fourier Features):
$$
\phi(x) = \frac{1}{\sqrt{m}} [\cos(\omega_1^\top x), \sin(\omega_1^\top x), \dots, \cos(\omega_m^\top x), \sin(\omega_m^\top x)]
$$

$\omega_i \sim \mathcal{N}(0, I)$ â€” ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**è¿‘ä¼¼ç²¾åº¦**:
- $m$ å€‹ã®ç‰¹å¾´ â†’ èª¤å·® $O(1/\sqrt{m})$
- å®Ÿç”¨: $m=256$ ã§ååˆ†

**å®Ÿè£…** (Juliaæ“¬ä¼¼ã‚³ãƒ¼ãƒ‰):

```julia
function linear_attention_performer(Q, K, V, m::Int)
    seq_len, d_k = size(Q)

    # Random features
    Ï‰ = randn(Float32, d_k, m)

    # Feature maps
    Ï•_Q = [cos.(Q * Ï‰); sin.(Q * Ï‰)] / sqrt(m)  # (seq_len, 2m)
    Ï•_K = [cos.(K * Ï‰); sin.(K * Ï‰)] / sqrt(m)  # (seq_len, 2m)

    # Compute KV aggregation (O(N))
    KV = Ï•_K' * V  # (2m, d_v)

    # Compute output (O(N))
    Z = sum(Ï•_K, dims=1)  # normalization (1, 2m)
    output = (Ï•_Q * KV) ./ (Ï•_Q * Z')  # (seq_len, d_v)

    return output
end
```

**Linear Attentionã®é™ç•Œ**:
- Causal MaskingãŒé›£ã—ã„ï¼ˆKVã®ç´¯ç©å’ŒãŒå¿…è¦ï¼‰
- è¿‘ä¼¼ç²¾åº¦ãŒã‚¿ã‚¹ã‚¯ä¾å­˜
- çŸ­ã„ç³»åˆ—ï¼ˆN<512ï¼‰ã§ã¯é€†ã«é…ã„ï¼ˆç‰¹å¾´å†™åƒã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰

#### (c) Cross-Attention â€” Encoder-Decoderã®æ¥ç¶š

**Encoder-Decoder Transformer** (T5ç­‰) ã§ã¯ã€DecoderãŒEncoderã®å‡ºåŠ›ã‚’å‚ç…§ã™ã‚‹**Cross-Attention**ãŒå¿…è¦ã€‚

**å®šç¾©**:

Decoderå´ã® $Q$ ã¨ã€Encoderå´ã® $K, V$ ã§Attentionã‚’è¨ˆç®—:

$$
\text{CrossAttn}(Q_{\text{dec}}, K_{\text{enc}}, V_{\text{enc}}) = \text{softmax}\left(\frac{Q_{\text{dec}} K_{\text{enc}}^\top}{\sqrt{d_k}}\right) V_{\text{enc}}
$$

**Transformer Decoder Blockã®æ§‹é€ **:

```mermaid
graph TD
    A["Input (Decoder)"] --> B["Masked<br/>Self-Attention"]
    B --> C["Residual + LN"]
    C --> D["Cross-Attention<br/>(K,V from Encoder)"]
    D --> E["Residual + LN"]
    E --> F["FFN"]
    F --> G["Residual + LN"]
    G --> H["Output"]
    I["Encoder Output"] --> D
```

**æ•°å¼**:

$$
\begin{aligned}
Z_1 &= X_{\text{dec}} + \text{MaskedSelfAttn}(X_{\text{dec}}) \\
Z_2 &= Z_1 + \text{CrossAttn}(Z_1, X_{\text{enc}}, X_{\text{enc}}) \\
Z_3 &= Z_2 + \text{FFN}(Z_2)
\end{aligned}
$$

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**:
- Encoderå‡ºåŠ› $X_{\text{enc}}$ ã¯å…¨Decoderå±¤ã§å…±æœ‰ â†’ 1å›ã ã‘è¨ˆç®—
- KV-Cacheã¯Cross-Attentionã«ã‚‚é©ç”¨å¯èƒ½ï¼ˆEncoderå‡ºåŠ›ã¯å›ºå®šï¼‰

#### (d) Attention Visualization â€” ä½•ã‚’è¦‹ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–

**Attention Weightsã®å¯è¦–åŒ–**:

```julia
using Plots

function visualize_attention(attn_weights, tokens)
    # attn_weights: (seq_len, seq_len)
    # tokens: Array of strings

    heatmap(attn_weights,
            xlabel="Key (source)",
            ylabel="Query (target)",
            xticks=(1:length(tokens), tokens),
            yticks=(1:length(tokens), tokens),
            color=:viridis,
            title="Attention Weights Heatmap")
end

# Example
tokens = ["The", "cat", "sat", "on", "the", "mat"]
attn_weights = randn(6, 6)
attn_weights = exp.(attn_weights) ./ sum(exp.(attn_weights), dims=2)

visualize_attention(attn_weights, tokens)
```

**å…¸å‹çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³**:
- **Diagonal**: éš£æ¥ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æ³¨ç›®ï¼ˆå±€æ‰€çš„ä¾å­˜ï¼‰
- **Vertical/Horizontal stripes**: ç‰¹å®šãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¥èª­ç‚¹ã€æ¥ç¶šè©ï¼‰ã¸ã®é›†ä¸­
- **Block structure**: ãƒ•ãƒ¬ãƒ¼ã‚ºå˜ä½ã®æ³¨ç›®

**BERTvizãƒ„ãƒ¼ãƒ«**: å„å±¤ãƒ»å„headã®æ³¨ç›®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«å¯è¦–åŒ–ã€‚

---

### 7.15 Mathematical Foundations: Attention as a Weighted Aggregation

**Attentionã®æœ¬è³ª**: åŠ é‡å’Œï¼ˆweighted aggregationï¼‰

**ä¸€èˆ¬å½¢**:

$$
o_i = \sum_{j=1}^{N} w_{ij} v_j, \quad \text{where } w_{ij} = f(q_i, k_j)
$$

- $w_{ij}$: ãƒˆãƒ¼ã‚¯ãƒ³ $i$ ãŒãƒˆãƒ¼ã‚¯ãƒ³ $j$ ã«æ³¨ç›®ã™ã‚‹é‡ã¿
- $f$: é¡ä¼¼åº¦é–¢æ•°

**é¡ä¼¼åº¦é–¢æ•°ã®é¸æŠè‚¢**:

| é–¢æ•° | å®šç¾© | ç‰¹å¾´ |
|:-----|:-----|:-----|
| Dot Product | $q \cdot k$ | æœ€ã‚‚å˜ç´” |
| Scaled Dot Product | $\frac{q \cdot k}{\sqrt{d_k}}$ | Transformeræ¨™æº– |
| Additive (Bahdanau) | $v^\top \tanh(W_q q + W_k k)$ | å­¦ç¿’å¯èƒ½ãªé‡ã¿ |
| Multiplicative (Luong) | $q^\top W k$ | è¡Œåˆ—ã§å¤‰æ› |
| Cosine Similarity | $\frac{q \cdot k}{\|q\| \|k\|}$ | æ­£è¦åŒ–æ¸ˆã¿ |

**ãªãœScaled Dot ProductãŒå‹ã£ãŸã‹**:

1. **è¨ˆç®—åŠ¹ç‡**: è¡Œåˆ—ç©1å›ã§å…¨ãƒšã‚¢ã‚’è¨ˆç®— â†’ GPUæœ€é©åŒ–å®¹æ˜“
2. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: æ¬¡å…ƒãŒå¤§ããã¦ã‚‚å®‰å®šï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ãŠã‹ã’ï¼‰
3. **å®Ÿè£…ã®å˜ç´”ã•**: Additiveã¯MLPãŒå¿…è¦ â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—åŠ 

**æ•°å­¦çš„è¦–ç‚¹**: Attentionã¯**Kernel Density Estimation**ã®é›¢æ•£ç‰ˆ

$$
\hat{f}(x) = \frac{1}{N} \sum_{i=1}^{N} K\left(\frac{x - x_i}{h}\right)
$$

- $K$: ã‚«ãƒ¼ãƒãƒ«é–¢æ•° (Attention ã§ã¯ $\exp(q \cdot k / \sqrt{d_k})$)
- $h$: bandwidth (Attention ã§ã¯ $\sqrt{d_k}$)

Attentionã¯ã€Query $q$ ã®å‘¨è¾ºã«ã‚ã‚‹ Key $k$ ã‚’ã€é¡ä¼¼åº¦ã«å¿œã˜ã¦é‡ã¿ä»˜ã‘å’Œã—ã¦ã„ã‚‹ã€‚

---

### 7.16 Training Dynamics: How Attention Learns

**è¨“ç·´åˆæœŸ**: Attention Weightsã¯ã»ã¼ä¸€æ§˜ï¼ˆå…¨ãƒˆãƒ¼ã‚¯ãƒ³ã«å‡ç­‰ã«æ³¨ç›®ï¼‰

**è¨“ç·´ä¸­ç›¤**: ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆéš£æ¥ã€ä¸»èª-å‹•è©ï¼‰ãŒå‡ºç¾

**è¨“ç·´å¾ŒæœŸ**: ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå›ºæœ‰åè©ã¸ã®æ³¨ç›®ã€é•·è·é›¢ä¾å­˜ï¼‰

**Attention Entropyã®æ¨ç§»**:

$$
H(A_i) = -\sum_{j=1}^{N} A_{ij} \log A_{ij}
$$

- è¨“ç·´åˆæœŸ: $H \approx \log N$ (ä¸€æ§˜åˆ†å¸ƒ)
- è¨“ç·´å¾ŒæœŸ: $H \ll \log N$ (ã‚¹ãƒ‘ãƒ¼ã‚¹åˆ†å¸ƒ)

**å®Ÿé¨“**: GPT-2ã®å„å±¤ã®Attention Entropyã‚’è¨“ç·´ä¸­ã«è¿½è·¡

| Epoch | Layer 1 | Layer 6 | Layer 12 |
|:------|:--------|:--------|:---------|
| 0 | 4.85 | 4.87 | 4.83 |
| 10 | 3.21 | 3.45 | 2.98 |
| 50 | 2.14 | 2.67 | 1.85 |

â†’ æ·±ã„å±¤ã»ã©ã‚¹ãƒ‘ãƒ¼ã‚¹ã«ãªã‚‹ï¼ˆç‰¹å®šã®ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®é›†ä¸­ï¼‰

**Grokking ã¨ã®é–¢ä¿‚**:

Attention Entropyã®æ€¥é™ä¸‹ = Grokkingç™ºç”Ÿã®ã‚µã‚¤ãƒ³ã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒã€Œãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’ç™ºè¦‹ã—ãŸç¬é–“ã€‚

---

### 7.17 Attention in Other Domains

#### (a) Vision Transformer (ViT)

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ç”»åƒã‚’ãƒ‘ãƒƒãƒã«åˆ†å‰² â†’ å„ãƒ‘ãƒƒãƒã‚’ã€Œãƒˆãƒ¼ã‚¯ãƒ³ã€ã¨ã—ã¦æ‰±ã†ã€‚

**æ‰‹é †**:
1. ç”»åƒ $224 \times 224$ ã‚’ $16 \times 16$ ãƒ‘ãƒƒãƒã«åˆ†å‰² â†’ $14 \times 14 = 196$ ãƒ‘ãƒƒãƒ
2. å„ãƒ‘ãƒƒãƒã‚’ç·šå½¢å°„å½± â†’ åŸ‹ã‚è¾¼ã¿
3. Position Encodingã‚’è¿½åŠ 
4. Standard Transformer Encoderã§å‡¦ç†

**Vision Transformerã®èª²é¡Œ**:
- ç”»åƒã¯ç³»åˆ—é•·ãŒé•·ã„ï¼ˆ196ãƒˆãƒ¼ã‚¯ãƒ³ vs ãƒ†ã‚­ã‚¹ãƒˆ~50ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
- å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã‚¼ãƒ­ â†’ å¤§é‡ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼ˆImageNet-21Kä»¥ä¸Šï¼‰
- CNNï¼ˆç•³ã¿è¾¼ã¿ï¼‰ã®å±€æ‰€æ€§ãƒã‚¤ã‚¢ã‚¹ãŒãªã„ãŸã‚ã€è¨“ç·´åˆæœŸã¯æ€§èƒ½ä½ã„

**2025å¹´ã®çŠ¶æ³**: ViTã¯ç”»åƒèªè­˜ã®SOTAã ãŒã€è¨ˆç®—é‡å¤§ â†’ Swin Transformer / Hierarchical ViT ãŒæ”¹è‰¯ç‰ˆã€‚

#### (b) Speech & Audio Transformers

**éŸ³å£°èªè­˜**: Whisper (OpenAI 2022) â€” Encoder-Decoder Transformer

**éŸ³æ¥½ç”Ÿæˆ**: MusicLM / Jukebox â€” éŸ³å£°æ³¢å½¢ã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ï¼ˆVQ-VAEï¼‰ â†’ Transformer

**èª²é¡Œ**: éŸ³å£°ã¯ç³»åˆ—é•·ãŒæ¥µç«¯ã«é•·ã„ï¼ˆ1ç§’ = 16000ã‚µãƒ³ãƒ—ãƒ« @ 16kHzï¼‰
â†’ ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° or Hierarchicalå‡¦ç†ãŒå¿…é ˆ

#### (c) Multi-Modal Transformers

**CLIP** (OpenAI 2021): ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒã˜åŸ‹ã‚è¾¼ã¿ç©ºé–“ã«ãƒãƒƒãƒ”ãƒ³ã‚°

**Flamingo** (DeepMind 2022): ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–å…¥åŠ› â†’ Cross-Attentionã§çµ±åˆ

**GPT-4**: ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒã®çµ±ä¸€ãƒ¢ãƒ‡ãƒ«ï¼ˆè©³ç´°éå…¬é–‹ï¼‰

**å…±é€šæ§‹é€ **:
- å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç‹¬ç«‹ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
- Cross-Attentionã§çµ±åˆ
- çµ±ä¸€ã•ã‚ŒãŸæ½œåœ¨ç©ºé–“ã§å‡¦ç†

---

### 7.18 Future Directions: Beyond Attention

**Attentionã®é™ç•Œ**:
1. $O(N^2)$ è¨ˆç®—é‡ â†’ é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ç ´ç¶»
2. å¸°ç´ãƒã‚¤ã‚¢ã‚¹ä¸è¶³ â†’ å°ãƒ‡ãƒ¼ã‚¿ã§å¼±ã„
3. è§£é‡ˆæ€§ã®é›£ã—ã• â†’ Attention Weightsã¯å› æœé–¢ä¿‚ã‚’ç¤ºã•ãªã„

**ä»£æ›¿ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | è¨ˆç®—é‡ | é•·æ‰€ | çŸ­æ‰€ |
|:--------------|:-------|:-----|:-----|
| **SSM (S4/Mamba)** | $O(N)$ | é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€åŠ¹ç‡ | è¡¨ç¾åŠ›ãŒAttentionã‚ˆã‚Šä½ã„ï¼Ÿ |
| **Hyena / H3** | $O(N \log N)$ | æš—é»™çš„é•·ç•³ã¿è¾¼ã¿ | è¨“ç·´ä¸å®‰å®š |
| **RWKV** | $O(N)$ | ç·šå½¢RNNã€ä¸¦åˆ—è¨“ç·´ | é•·è·é›¢ä¾å­˜ãŒå¼±ã„ |
| **RetNet** | $O(1)$ æ¨è«– | æ¨è«–è¶…é«˜é€Ÿ | è¨“ç·´ã¯ã¾ã Attentionã‚ˆã‚Šé…ã„ |

**Hybrid ã®æ™‚ä»£** (2025):
- Attentionå˜ç‹¬ or SSMå˜ç‹¬ã§ã¯ãªãã€**çµ„ã¿åˆã‚ã›**ãŒæœ€å¼·
- Jamba (AI21): SSM + Attention + MoE
- Zamba (Zyphra): Mamba + Shared Attention

**ç¬¬16-18å›ã®äºˆå‘Š**:
- ç¬¬16å›: SSMç†è«–ï¼ˆS4â†’Mambaï¼‰
- ç¬¬17å›: Mamba-2 / RWKV / RetNet
- ç¬¬18å›: Hybrid (Jamba/Zamba) + Course IIèª­äº†æ„Ÿ

---

### 7.19 Ethical Considerations & Societal Impact

**LLMï¼ˆLarge Language Modelï¼‰ã®ç¤¾ä¼šçš„å½±éŸ¿**:

#### (a) Bias & Fairness

**å•é¡Œ**: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¤ã‚¢ã‚¹ãŒãƒ¢ãƒ‡ãƒ«ã«åæ˜ ã•ã‚Œã‚‹ã€‚

**ä¾‹**:
- æ€§åˆ¥ãƒã‚¤ã‚¢ã‚¹: "doctor" â†’ "he", "nurse" â†’ "she"
- äººç¨®ãƒã‚¤ã‚¢ã‚¹: çŠ¯ç½ªé–¢é€£ã®æ–‡è„ˆã§ç‰¹å®šäººç¨®ã¸ã®åã‚Š
- æ–‡åŒ–ãƒã‚¤ã‚¢ã‚¹: è‹±èªåœä¸­å¿ƒã®çŸ¥è­˜

**å¯¾ç­–**:
- ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°
- Debiasingæ‰‹æ³•ï¼ˆCounterfactual Data Augmentationï¼‰
- RLHF (Reinforcement Learning from Human Feedback) ã§ä¿®æ­£

#### (b) Misinformation & Hallucination

**å•é¡Œ**: LLMã¯ç¢ºä¿¡ã‚’æŒã£ã¦èª¤æƒ…å ±ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆHallucinationï¼‰ã€‚

**åŸå› **: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«èª¤æƒ…å ±ãŒå«ã¾ã‚Œã‚‹ + ç¢ºç‡çš„ç”Ÿæˆã«ã‚ˆã‚‹æé€ 

**å¯¾ç­–**:
- Retrieval-Augmented Generation (RAG) â€” å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’å‚ç…§
- Citationæ©Ÿèƒ½ â€” ç”Ÿæˆå†…å®¹ã®å‡ºå…¸ã‚’æ˜ç¤º
- Uncertainty Estimation â€” ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–

#### (c) Environmental Impact

**å•é¡Œ**: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¯è†¨å¤§ãªã‚¨ãƒãƒ«ã‚®ãƒ¼æ¶ˆè²»ã€‚

**æ•°å€¤**:
- GPT-3 (175B) è¨“ç·´: ~1,287 MWh â†’ COâ‚‚æ’å‡º ~552ãƒˆãƒ³ï¼ˆè‡ªå‹•è»Š120å°åˆ†/å¹´ï¼‰
- Chinchilla (70B, 1.4Tãƒˆãƒ¼ã‚¯ãƒ³): GPT-3ã®2å€ã®è¨ˆç®—é‡

**å¯¾ç­–**:
- Compute-efficient training (Chinchillaå‰‡ã«å¾“ã†)
- Model compression (Quantization / Pruning)
- å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼é‹ç”¨

#### (d) Democratization vs Concentration

**å•é¡Œ**: LLMè¨“ç·´ã¯å·¨å¤§ä¼æ¥­ã®ã¿ãŒå¯èƒ½ â†’ æŠ€è¡“ã®å¯¡å åŒ–ã€‚

**å¯¾ç­–**:
- ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆLLaMA / BLOOM / Falconï¼‰
- Smaller but efficient modelsï¼ˆPhi / Mistralï¼‰
- Inference optimizationï¼ˆFlash Attention / Quantizationï¼‰

**çµè«–**: Attention/Transformerã¯å¼·åŠ›ãªæŠ€è¡“ã ãŒã€**è²¬ä»»ã‚ã‚‹é–‹ç™ºã¨åˆ©ç”¨**ãŒä¸å¯æ¬ ã€‚

---

### 7.20 Appendix: Complete Symbol Index

**å…¨è¨˜å·ã®å®Œå…¨ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹** â€” è«–æ–‡èª­è§£æ™‚ã®è¾æ›¸ã¨ã—ã¦ä½¿ç”¨å¯èƒ½ã€‚

| è¨˜å· | èª­ã¿ | æ„å‘³ | å½¢çŠ¶ | åˆå‡º |
|:-----|:-----|:-----|:-----|:-----|
| $N$ | ã‚¨ãƒŒ | ç³»åˆ—é•·ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰ | ã‚¹ã‚«ãƒ©ãƒ¼ | Zone 0 |
| $d_{\text{model}}$ | ãƒ‡ã‚£ãƒ¼ ãƒ¢ãƒ‡ãƒ« | å…¥åŠ›åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ | ã‚¹ã‚«ãƒ©ãƒ¼ | Zone 0 |
| $d_k$ | ãƒ‡ã‚£ãƒ¼ ã‚±ãƒ¼ | Query/Keyã®æ¬¡å…ƒ | ã‚¹ã‚«ãƒ©ãƒ¼ | Zone 1 |
| $d_v$ | ãƒ‡ã‚£ãƒ¼ ãƒ–ã‚¤ | Valueã®æ¬¡å…ƒ | ã‚¹ã‚«ãƒ©ãƒ¼ | Zone 1 |
| $d_{\text{ff}}$ | ãƒ‡ã‚£ãƒ¼ ã‚¨ãƒ•ã‚¨ãƒ• | Feed-Forwardä¸­é–“æ¬¡å…ƒ | ã‚¹ã‚«ãƒ©ãƒ¼ | Zone 3 |
| $h$ | ã‚¨ã‚¤ãƒ | Multi-Headã®æ•° | ã‚¹ã‚«ãƒ©ãƒ¼ | Zone 3 |
| $X$ | ã‚¨ãƒƒã‚¯ã‚¹ | å…¥åŠ›ç³»åˆ— | $(N, d_{\text{model}})$ | Zone 0 |
| $Q$ | ã‚­ãƒ¥ãƒ¼ | Queryè¡Œåˆ— | $(N, d_k)$ | Zone 0 |
| $K$ | ã‚±ãƒ¼ | Keyè¡Œåˆ— | $(N, d_k)$ | Zone 0 |
| $V$ | ãƒ–ã‚¤ | Valueè¡Œåˆ— | $(N, d_v)$ | Zone 0 |
| $W_Q$ | ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ ã‚­ãƒ¥ãƒ¼ | Queryå°„å½±è¡Œåˆ— | $(d_{\text{model}}, d_k)$ | Zone 3 |
| $W_K$ | ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ ã‚±ãƒ¼ | Keyå°„å½±è¡Œåˆ— | $(d_{\text{model}}, d_k)$ | Zone 3 |
| $W_V$ | ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ ãƒ–ã‚¤ | Valueå°„å½±è¡Œåˆ— | $(d_{\text{model}}, d_v)$ | Zone 3 |
| $W_O$ | ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ ã‚ªãƒ¼ | å‡ºåŠ›å°„å½±è¡Œåˆ— | $(hd_v, d_{\text{model}})$ | Zone 3 |
| $W_1$ | ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ ãƒ¯ãƒ³ | FFNç¬¬1å±¤é‡ã¿ | $(d_{\text{model}}, d_{\text{ff}})$ | Zone 3 |
| $W_2$ | ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ ãƒ„ãƒ¼ | FFNç¬¬2å±¤é‡ã¿ | $(d_{\text{ff}}, d_{\text{model}})$ | Zone 3 |
| $S$ | ã‚¨ã‚¹ | Attentionã‚¹ã‚³ã‚¢è¡Œåˆ— | $(N, N)$ | Zone 1 |
| $A$ | ã‚¨ãƒ¼ | Attentioné‡ã¿è¡Œåˆ— | $(N, N)$ | Zone 1 |
| $O$ | ã‚ªãƒ¼ | Attentionå‡ºåŠ› | $(N, d_v)$ | Zone 1 |
| $\text{softmax}(x)_i$ | ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ | $\frac{e^{x_i}}{\sum_j e^{x_j}}$ | ãƒ™ã‚¯ãƒˆãƒ«â†’ãƒ™ã‚¯ãƒˆãƒ« | Zone 0 |
| $\text{LN}(x)$ | ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒãƒ¼ãƒ  | Layer Normalization | $(d,) \to (d,)$ | Zone 3 |
| $\gamma, \beta$ | ã‚¬ãƒ³ãƒã€ãƒ™ãƒ¼ã‚¿ | LNã®ã‚¹ã‚±ãƒ¼ãƒ«ãƒ»ã‚·ãƒ•ãƒˆ | $(d,)$ | Zone 3 |
| $\mu, \sigma$ | ãƒŸãƒ¥ãƒ¼ã€ã‚·ã‚°ãƒ | LNã®å¹³å‡ãƒ»æ¨™æº–åå·® | ã‚¹ã‚«ãƒ©ãƒ¼ | Zone 3 |
| $PE_{pos,i}$ | ãƒ”ãƒ¼ã‚¤ãƒ¼ | Position Encoding | $(N, d_{\text{model}})$ | Zone 3 |
| $\text{Mask}$ | ãƒã‚¹ã‚¯ | Causal Maskè¡Œåˆ— | $(N, N)$ | Zone 3 |
| $p(x_t \mid x_{<t})$ | ãƒ”ãƒ¼ | æ¡ä»¶ä»˜ãç¢ºç‡ | ã‚¹ã‚«ãƒ©ãƒ¼ | Zone 6 |
| $\theta$ | ã‚·ãƒ¼ã‚¿ | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…¨ä½“ | ãƒ™ã‚¯ãƒˆãƒ« | Zone 5 |
| $\mathcal{L}$ | ã‚¨ãƒ« | æå¤±é–¢æ•° | ã‚¹ã‚«ãƒ©ãƒ¼ | Zone 5 |
| $\nabla_\theta$ | ãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ | $\theta$ã«é–¢ã™ã‚‹å‹¾é… | ãƒ™ã‚¯ãƒˆãƒ« | Zone 5 |

---

### 7.21 Complete Code Repository Structure

**æ¨å¥¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ** â€” ç¬¬14å›ã®å…¨å®Ÿè£…ã‚’æ•´ç†:

```
ml-lecture-14-attention/
â”œâ”€â”€ julia/
â”‚   â”œâ”€â”€ Project.toml           # Dependencies
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ attention.jl       # Core Attention module
â”‚   â”‚   â”œâ”€â”€ multi_head.jl      # Multi-Head Attention
â”‚   â”‚   â”œâ”€â”€ transformer.jl     # Transformer Block
â”‚   â”‚   â”œâ”€â”€ micro_gpt.jl       # Micro-GPT model
â”‚   â”‚   â””â”€â”€ position.jl        # Position Encoding (Sinusoidal/RoPE)
â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â”œâ”€â”€ test_attention.jl  # Unit tests
â”‚   â”‚   â””â”€â”€ test_transformer.jl
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ train_gpt.jl       # Tiny Shakespeare training
â”‚   â”‚   â””â”€â”€ icl_demo.jl        # In-Context Learning demo
â”‚   â””â”€â”€ notebooks/
â”‚       â””â”€â”€ attention_viz.ipynb  # Attention visualization
â”œâ”€â”€ rust/
â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs             # Library root
â”‚   â”‚   â”œâ”€â”€ attention.rs       # Attention inference
â”‚   â”‚   â”œâ”€â”€ kv_cache.rs        # KV-Cache implementation
â”‚   â”‚   â””â”€â”€ bin/
â”‚   â”‚       â””â”€â”€ inference.rs   # Inference binary
â”‚   â””â”€â”€ benches/
â”‚       â””â”€â”€ attention_bench.rs # Criterion benchmarks
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ attention_ref.py       # Reference implementation (NumPy)
â”‚   â””â”€â”€ compare.py             # 3-language comparison script
â”œâ”€â”€ data/
â”‚   â””â”€â”€ tiny_shakespeare.txt   # Training data
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ml-lecture-14.md       # æœ¬è¬›ç¾©ï¼ˆZennå½¢å¼ï¼‰
â”‚   â”œâ”€â”€ symbols.pdf            # Symbol cheat sheet
â”‚   â””â”€â”€ figures/
â”‚       â”œâ”€â”€ attention_heatmap.png
â”‚       â””â”€â”€ training_loss.png
â””â”€â”€ README.md                  # Quick start guide
```

**README.md**ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:

```markdown
# ç¬¬14å›: Attention â€” åŒ–çŸ³ã‹ã‚‰ã®è„±å´

å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã¨ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆZennè¨˜äº‹ã®å®Ÿè·µç‰ˆï¼‰

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### Juliaå®Ÿè£…

```bash
cd julia
julia --project=. -e 'using Pkg; Pkg.instantiate()'
julia --project=. experiments/train_gpt.jl
```

### Rustæ¨è«–

```bash
cd rust
cargo run --release --bin inference
```

### 3è¨€èªæ¯”è¼ƒ

```bash
python python/compare.py
```

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT â€” æ•™è‚²ç›®çš„ã§ã®è‡ªç”±åˆ©ç”¨ã‚’æ¨å¥¨
```

---

### 7.22 Recommended Study Path for Different Audiences

**åˆå­¦è€…ï¼ˆæ©Ÿæ¢°å­¦ç¿’1å¹´ç›®ï¼‰**:
1. Zone 0-2 ã‚’ç²¾èª­ï¼ˆç›´æ„Ÿé‡è¦–ï¼‰
2. Zone 3.1-3.3 ã®ã¿æ•°å¼å°å‡º
3. Zone 4 ã®Juliaã‚³ãƒ¼ãƒ‰ã‚’å†™çµŒ
4. Zone 5 ã‚’ã‚¹ã‚­ãƒƒãƒ—
5. Zone 7 ã§å…¨ä½“å¾©ç¿’
**æ¨å®šæ™‚é–“**: 3æ™‚é–“

**ä¸­ç´šè€…ï¼ˆè«–æ–‡å®Ÿè£…çµŒé¨“ã‚ã‚Šï¼‰**:
1. Zone 0-2 ã‚’æµã—èª­ã¿
2. Zone 3 å…¨ã¦å°å‡ºï¼ˆç´™ã¨ãƒšãƒ³ï¼‰
3. Zone 4-5 å…¨ã¦å®Ÿè£…+å®Ÿé¨“
4. Zone 6 ã®ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã‚’ç²¾èª­
5. Challengeå•é¡Œã«æŒ‘æˆ¦
**æ¨å®šæ™‚é–“**: 6æ™‚é–“

**ä¸Šç´šè€…ï¼ˆç ”ç©¶è€…/ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ï¼‰**:
1. Zone 3.6-3.7 (Boss Battle) ã‹ã‚‰é–‹å§‹
2. Zone 4.5 (KV-Cache) ã‚’è©³ç´°å®Ÿè£…
3. Zone 6 ã®æœ€æ–°ç ”ç©¶ã‚’æ·±æ˜ã‚Š
4. 7.14-7.19 ã®å¿œç”¨ãƒˆãƒ”ãƒƒã‚¯ã‚’ç ”ç©¶ã«æ´»ç”¨
5. Final Boss (GPT-2å†ç¾) ã«æŒ‘æˆ¦
**æ¨å®šæ™‚é–“**: 10æ™‚é–“ + å®Ÿè£…100æ™‚é–“

**è«–æ–‡è‘—è€…å¿—æœ›**:
1. å…¨Zoneå®Œå…¨èª­ç ´
2. å…¨æ•°å¼ã‚’è‡ªåŠ›ã§å†å°å‡º
3. å…¨ã‚³ãƒ¼ãƒ‰ã‚’3è¨€èªã§å®Ÿè£…
4. Advanced Topics (7.14-7.19) ã‚’è«–æ–‡èª¿æŸ»
5. ç‹¬è‡ªã®æ”¹è‰¯æ¡ˆã‚’è€ƒæ¡ˆãƒ»å®Ÿé¨“
**æ¨å®šæ™‚é–“**: 20æ™‚é–“ + ç ”ç©¶Né€±é–“

---

### 7.23 Historical Timeline: Attentionç™ºå±•å²

**Pre-Attentionæ™‚ä»£** (ã€œ2014):
- 2013: Word2Vec (Mikolov) â€” å˜èªåŸ‹ã‚è¾¼ã¿ã®æ¨™æº–åŒ–
- 2014: Seq2Seq (Sutskever) â€” RNN Encoder-Decoder

**Attentionèª•ç”Ÿ** (2014-2016):
- 2015: Bahdanau Attention [^2] â€” RNN + Attention
- 2016: Luong Attention â€” Multiplicative Attention

**Transformeré©å‘½** (2017-2020):
- 2017: **"Attention is All You Need"** [^1] â€” RNN/CNNæ’é™¤
- 2018: GPT-1 [^3] â€” Decoder-only, ç”Ÿæˆã‚¿ã‚¹ã‚¯
- 2019: BERT [^4] â€” Encoder-only, ç†è§£ã‚¿ã‚¹ã‚¯
- 2019: GPT-2 â€” Zero-shotå­¦ç¿’ã®èŒèŠ½
- 2020: GPT-3 â€” Few-shot ICL, Emergent Abilities
- 2020: Scaling Laws [^5] â€” Kaplanå‰‡

**åŠ¹ç‡åŒ–æ™‚ä»£** (2020-2023):
- 2020: Longformer / BigBird â€” Sparse Attention
- 2021: RoPE [^10] â€” ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ”¹è‰¯
- 2021: Performer / Linear Attention â€” $O(N)$ å®Ÿç¾
- 2022: Flash Attention â€” IOæœ€é©åŒ–
- 2022: Chinchilla [^6] â€” ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ©ãƒ³ã‚¹
- 2023: Flash Attention-2/3 â€” ã•ã‚‰ãªã‚‹é«˜é€ŸåŒ–

**ä»£æ›¿ãƒ»çµ±åˆæ™‚ä»£** (2023-2025):
- 2023: Mamba â€” SSMã«ã‚ˆã‚‹ä»£æ›¿
- 2024: Differential Transformer [^7] â€” ãƒã‚¤ã‚ºé™¤å»
- 2024: Mamba-2 / SSD â€” Attention=SSMåŒå¯¾æ€§
- 2025: Hybrid (Jamba/Zamba) â€” çµ„ã¿åˆã‚ã›ã®æ™‚ä»£

---

**ç¬¬14å›å®Œ**. åŒ–çŸ³ã‹ã‚‰ã®è„±å´å®Œäº† â€” æ¬¡ã¯åŠ¹ç‡åŒ–ã®æˆ¦ã„ã ã€‚ç¬¬15å›ã§ä¼šãŠã†ã€‚