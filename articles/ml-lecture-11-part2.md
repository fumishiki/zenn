---
title: "ç¬¬11å›: æœ€é©è¼¸é€ç†è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸš›"
type: "tech"
topics: ["machinelearning", "deeplearning", "optimaltransport", "rust", "rust"]
published: true
slug: "ml-lecture-11-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

> **ğŸ“– ã“ã®è¨˜äº‹ã¯å¾Œç·¨ï¼ˆå®Ÿè£…ç·¨ï¼‰ã§ã™** ç†è«–ç·¨ã¯ [ã€å‰ç·¨ã€‘ç¬¬11å›](/articles/ml-lecture-11-part1) ã‚’ã”è¦§ãã ã•ã„ã€‚

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rust + Rust ã§OTã‚’å®Ÿè£…ã™ã‚‹

### 4.1 ç’°å¢ƒæ§‹ç¯‰

#### 4.1.1 Rustç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Rust (cargo 1.75+) ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆ2026å¹´ç¾åœ¨ã®å®‰å®šç‰ˆï¼‰
# https://julialang.org/downloads/

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
julia -e 'using Pkg; Pkg.add(["Distributions", "LinearAlgebra", "Plots", "JuMP", "HiGHS", "BenchmarkTools", "Lux", "Optimisers", "Zygote"])'
```

**ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å½¹å‰²**:

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” |
|:----------|:-----|
| `Distributions` | ç¢ºç‡åˆ†å¸ƒã®æ“ä½œ |
| `LinearAlgebra` | è¡Œåˆ—æ¼”ç®—ã€SVDã€ãƒãƒ«ãƒ  |
| `Plots` | å¯è¦–åŒ– |
| `JuMP` | æ•°ç†æœ€é©åŒ–ï¼ˆç·šå½¢è¨ˆç”»æ³•ï¼‰ |
| `HiGHS` | ç·šå½¢è¨ˆç”»ã‚½ãƒ«ãƒãƒ¼ |
| `Criterion` | ç²¾å¯†ãªæ™‚é–“è¨ˆæ¸¬ |
| `Lux` | ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆï¼ˆJAXé¢¨ï¼‰ |
| `burn::optim` | æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
| `Zygote` | è‡ªå‹•å¾®åˆ† |

#### 4.1.2 Rustç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Rust 1.80+ ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
cargo new ot_rust --lib
cd ot_rust

# Cargo.tomlã«ä¾å­˜é–¢ä¿‚ã‚’è¿½åŠ 
```

```toml
[dependencies]
ndarray = "0.16"
ndarray-linalg = { version = "0.17", features = ["openblas-static"] }
rayon = "1.10"
```

**ä¾å­˜é–¢ä¿‚ã®å½¹å‰²**:

| Crate | ç”¨é€” |
|:------|:-----|
| `ndarray` | å¤šæ¬¡å…ƒé…åˆ—ï¼ˆNumPyé¢¨ï¼‰ |
| `ndarray-linalg` | ç·šå½¢ä»£æ•°æ¼”ç®— |
| `rayon` | ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å‡¦ç† |

### 4.2 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆOTç‰¹åŒ–ï¼‰

**Pattern 1: Wassersteinè·é›¢ã®è¨ˆç®—ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰**

æ•°å¼:
$$W_2^2(\mathcal{N}(\boldsymbol{m}_0, \Sigma_0), \mathcal{N}(\boldsymbol{m}_1, \Sigma_1)) = \|\boldsymbol{m}_1 - \boldsymbol{m}_0\|^2 + \text{tr}(\Sigma_0 + \Sigma_1 - 2(\Sigma_1^{1/2} \Sigma_0 \Sigma_1^{1/2})^{1/2})$$

Rust:
```rust
use nalgebra::{DMatrix, DVector};

fn wasserstein2_gaussian(m0: &DVector<f64>, s0: &DMatrix<f64>,
                          m1: &DVector<f64>, s1: &DMatrix<f64>) -> f64 {
    // Location term: ||m1 - m0||Â²
    let loc = (m1 - m0).norm_squared();

    // Covariance term: tr(Î£0 + Î£1 - 2Â·(Î£1^Â½ Î£0 Î£1^Â½)^Â½)
    let s1_sqrt = s1.clone().cholesky().map(|c| c.l()).unwrap_or_else(|| s1.clone());
    let m = &s1_sqrt * s0 * s1_sqrt.transpose();
    let m_sqrt = m.clone().cholesky().map(|c| c.l()).unwrap_or_else(|| m.clone());
    let cov = s0.trace() + s1.trace() - 2.0 * m_sqrt.trace();

    (loc + cov).sqrt()
}
```

**Pattern 2: Gibbsã‚«ãƒ¼ãƒãƒ«ã®è¨ˆç®—**

æ•°å¼: $K_{ij} = \exp(-C_{ij} / \varepsilon)$

Rustï¼ˆãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆï¼‰:
```rust
K = exp.(-C / Îµ)  # element-wise exponential
```

Rustï¼ˆè¦ç´ ã”ã¨ï¼‰:
```rust
use ndarray::Array2;

// K_ij = exp(-C_ij / Îµ)  (Gibbs kernel)
fn gibbs_kernel(cost: &Array2<f64>, epsilon: f64) -> Array2<f64> {
    cost.mapv(|c| (-c / epsilon).exp())  // element-wise: K = exp(-C / Îµ)
}
```

**Pattern 3: å‘¨è¾ºåˆ†å¸ƒã®ç¢ºèª**

æ•°å¼: $\sum_j \gamma_{ij} = p_i$ï¼ˆè¡Œå’Œï¼‰, $\sum_i \gamma_{ij} = q_j$ï¼ˆåˆ—å’Œï¼‰

Rust:
```rust
row_sums = sum(Î³, dims=2)[:]  # sum along columns â†’ (n,)
col_sums = sum(Î³, dims=1)[:]  # sum along rows â†’ (m,)

@assert all(isapprox.(row_sums, p, atol=1e-6))
@assert all(isapprox.(col_sums, q, atol=1e-6))
```

Rust:
```rust
// Î£_j Î³_ij = p_i  (row marginal constraint)
let row_sums = gamma.sum_axis(Axis(1));
// Î£_i Î³_ij = q_j  (column marginal constraint)
let col_sums = gamma.sum_axis(Axis(0));

assert!(row_sums.iter().zip(p.iter())
    .all(|(r, pi)| (r - pi).abs() < 1e-6));
assert!(col_sums.iter().zip(q.iter())
    .all(|(c, qj)| (c - qj).abs() < 1e-6));
```

**Pattern 4: log-sum-expï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰**

æ•°å¼: $\log \sum_i \exp(x_i) = x_{\max} + \log \sum_i \exp(x_i - x_{\max})$

Rust:
```rust
fn logsumexp(x: &[f64]) -> f64 {
    let x_max = x.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
    x_max + x.iter().map(|&v| (v - x_max).exp()).sum::<f64>().ln()
}

fn logsumexp_rows(m: &ndarray::Array2<f64>) -> ndarray::Array1<f64> {
    m.rows().into_iter()
     .map(|row| logsumexp(row.as_slice().unwrap()))
     .collect()
}
```

Rust:
```rust
fn logsumexp(x: &Array1<f64>) -> f64 {
    let x_max = x.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
    x_max + x.iter().map(|xi| (xi - x_max).exp()).sum::<f64>().ln()
}
```

### 4.3 å®Œå…¨å®Ÿè£…: Sinkhornç®—æ³•ï¼ˆRustï¼‰

```rust
use ndarray::prelude::*;

pub struct SinkhornResult {
    pub gamma:     Array2<f64>,
    pub cost:      f64,
    pub iters:     usize,
    pub converged: bool,
    pub history:   Vec<f64>,
}

/// æ¨™æº– Sinkhorn ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚
///
/// # Arguments
/// - `c`:        ã‚³ã‚¹ãƒˆè¡Œåˆ— (n Ã— m)
/// - `p`:        ã‚½ãƒ¼ã‚¹åˆ†å¸ƒ (n,), å’Œ = 1
/// - `q`:        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ (m,), å’Œ = 1
/// - `eps`:      ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
/// - `max_iter`: æœ€å¤§åå¾©å›æ•°
/// - `tol`:      åæŸé–¾å€¤
pub fn sinkhorn(
    c:        &ArrayView2<f64>,
    p:        &[f64],
    q:        &[f64],
    eps:      f64,
    max_iter: usize,
    tol:      f64,
) -> SinkhornResult {
    let (n, m) = c.dim();
    assert_eq!(p.len(), n);
    assert_eq!(q.len(), m);

    // K_ij = exp(-C_ij / Îµ)  (Gibbs kernel)
    let k: Array2<f64> = c.mapv(|v| (-v / eps).exp());

    // pre-build array views from slices (zero-copy)
    let p_arr = ndarray::ArrayView1::from(p);
    let q_arr = ndarray::ArrayView1::from(q);

    let mut u = Array1::<f64>::ones(n);
    let mut v = Array1::<f64>::ones(m);
    let mut history = Vec::new();
    let mut converged = false;

    for _ in 0..max_iter {
        let u_prev = u.clone();
        // u_i â† a_i / Î£_j K_ij v_j  (Sinkhorn u-update)
        u = &p_arr / &k.dot(&v);
        // v_j â† b_j / Î£_i K_ij u_i  (Sinkhorn v-update)
        v = &q_arr / &k.t().dot(&u);

        let err = u.iter().zip(u_prev.iter())
            .map(|(a, b)| (a - b).abs())
            .fold(0.0_f64, f64::max);
        history.push(err);

        if err < tol { converged = true; break; }
    }

    // Î³_ij = u_i Â· K_ij Â· v_j  (transport plan from scaling vectors)
    let gamma = Array2::from_shape_fn((n, m), |(i, j)| u[i] * k[[i, j]] * v[j]);
    // W_Îµ(Î±,Î²) = âŸ¨Î³, CâŸ©  (regularized OT cost)
    let cost  = gamma.iter().zip(c.iter()).map(|(g, c)| g * c).sum();

    SinkhornResult { gamma, cost, iters: history.len(), converged, history }
}

/// å¯¾æ•°é ˜åŸŸ Sinkhorn (å°ã•ã„ Îµ ã§ã‚‚æ•°å€¤çš„ã«å®‰å®š)ã€‚
pub fn sinkhorn_log(
    c:        &ArrayView2<f64>,
    p:        &[f64],
    q:        &[f64],
    eps:      f64,
    max_iter: usize,
    tol:      f64,
) -> SinkhornResult {
    let (n, m) = c.dim();
    // log K_ij = -C_ij / Îµ  (log-domain Gibbs kernel, avoids underflow)
    let log_k: Array2<f64> = c.mapv(|v| -v / eps);
    // log a_i, log b_j  (log-domain marginals)
    let log_p: Array1<f64> = Array1::from_iter(p.iter().map(|v| v.ln()));
    let log_q: Array1<f64> = Array1::from_iter(q.iter().map(|v| v.ln()));

    let mut log_u = Array1::<f64>::zeros(n);
    let mut log_v = Array1::<f64>::zeros(m);
    let mut history = Vec::new();
    let mut converged = false;

    // logsumexp(x) = x_max + log Î£_i exp(x_i - x_max)  (numerically stable)
    let logsumexp = |x: ArrayView1<f64>| {
        let mx = x.fold(f64::NEG_INFINITY, |a, &b| a.max(b));
        mx + x.mapv(|v| (v - mx).exp()).sum().ln()
    };

    for _ in 0..max_iter {
        let log_u_prev = log_u.clone();

        // log u_i â† log a_i - logsumexp(log K_{iÂ·} + log v)
        let log_kv: Array1<f64> = (0..n)
            .map(|i| logsumexp((log_k.row(i).to_owned() + &log_v).view()))
            .collect();
        log_u = &log_p - &log_kv;

        // log v_j â† log b_j - logsumexp(log K_{Â·j} + log u)
        let log_ktu: Array1<f64> = (0..m)
            .map(|j| logsumexp((log_k.column(j).to_owned() + &log_u).view()))
            .collect();
        log_v = &log_q - &log_ktu;

        let err = log_u.iter().zip(log_u_prev.iter())
            .map(|(a, b)| (a - b).abs())
            .fold(0.0_f64, f64::max);
        history.push(err);

        if err < tol { converged = true; break; }
    }

    // Î³_ij = exp(log u_i + log K_ij + log v_j)  (recover transport plan in primal domain)
    let gamma = Array2::from_shape_fn((n, m), |(i, j)| {
        (log_u[i] + log_k[[i, j]] + log_v[j]).exp()
    });
    // W_Îµ(Î±,Î²) = âŸ¨Î³, CâŸ©  (regularized OT cost)
    let cost = gamma.iter().zip(c.iter()).map(|(g, c)| g * c).sum();

    SinkhornResult { gamma, cost, iters: history.len(), converged, history }
}

// â”€â”€â”€ ä½¿ç”¨ä¾‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fn main() {
    use ndarray::Array;
    let n = 100usize;
    let p = vec![1.0 / n as f64; n];
    let q = p.clone();

    // ãƒ©ãƒ³ãƒ€ãƒ ã‚³ã‚¹ãƒˆè¡Œåˆ—
    let c = Array2::from_shape_fn((n, n), |(i, j)| ((i as f64) - (j as f64)).powi(2));

    let res = sinkhorn(&c.view(), &p, &q, 0.1, 1000, 1e-9);
    println!("Standard Sinkhorn:");
    println!("  Converged: {} in {} iters", res.converged, res.iters);
    println!("  Cost: {:.6}", res.cost);

    let res_log = sinkhorn_log(&c.view(), &p, &q, 0.01, 1000, 1e-9);
    println!("\nLog-domain Sinkhorn:");
    println!("  Converged: {} in {} iters", res_log.converged, res_log.iters);
    println!("  Cost: {:.6}", res_log.cost);
}
```

### 4.4 é«˜é€ŸåŒ–å®Ÿè£…: Sinkhorn SIMDï¼ˆRustï¼‰

```rust
// src/lib.rs
use ndarray::{Array1, Array2, Axis, Zip};
use rayon::prelude::*;

pub struct SinkhornResult {
    pub gamma: Array2<f64>,
    pub cost: f64,
    pub iters: usize,
    pub converged: bool,
}

/// Sinkhorn algorithm with parallelization.
pub fn sinkhorn_parallel(
    cost: &Array2<f64>,
    p: &ArrayView1<f64>,
    q: &ArrayView1<f64>,
    epsilon: f64,
    max_iter: usize,
    tol: f64,
) -> SinkhornResult {
    let (n, m) = cost.dim();
    assert_eq!(p.len(), n);
    assert_eq!(q.len(), m);

    // K_ij = exp(-C_ij / Îµ)  (Gibbs kernel)
    let k = cost.mapv(|c| (-c / epsilon).exp());

    let mut u = Array1::ones(n);
    let mut v = Array1::ones(m);

    let mut converged = false;
    let mut iters = 0;

    for iter in 0..max_iter {
        let u_old = u.clone();

        // u_i â† p_i / Î£_j K_ij v_j  (parallel Sinkhorn u-update)
        let kv = k.dot(&v);
        Zip::from(&mut u)
            .and(p)
            .and(&kv)
            .par_for_each(|u_i, &p_i, &kv_i| {
                *u_i = p_i / kv_i;
            });

        // v_j â† q_j / Î£_i K_ij u_i  (parallel Sinkhorn v-update)
        let ktu = k.t().dot(&u);
        Zip::from(&mut v)
            .and(q)
            .and(&ktu)
            .par_for_each(|v_j, &q_j, &ktu_j| {
                *v_j = q_j / ktu_j;
            });

        let err = Zip::from(&u).and(&u_old).fold(0.0_f64, |acc, &ui, &uo| acc.max((ui - uo).abs()));
        if err < tol {
            converged = true;
            iters = iter + 1;
            break;
        }
        iters = iter + 1;
    }

    // Î³_ij = u_i Â· K_ij Â· v_j  (transport plan, parallel reconstruction)
    let mut gamma = Array2::zeros((n, m));
    Zip::indexed(&mut gamma).par_for_each(|(i, j), g| {
        *g = u[i] * k[[i, j]] * v[j];
    });

    // W_Îµ(Î±,Î²) = âŸ¨Î³, CâŸ©  (regularized OT cost)
    let cost: f64 = gamma.iter().zip(cost.iter()).map(|(&g, &c)| g * c).sum();

    SinkhornResult {
        gamma,
        cost,
        iters,
        converged,
    }
}

/// Batch Sinkhorn for multiple cost matrices (GPU-style parallelism).
pub fn sinkhorn_batch(
    costs: &[Array2<f64>],
    p: &ArrayView1<f64>,
    q: &ArrayView1<f64>,
    epsilon: f64,
    max_iter: usize,
    tol: f64,
) -> Vec<SinkhornResult> {
    costs
        .par_iter()
        .map(|cost| sinkhorn_parallel(cost, p, q, epsilon, max_iter, tol))
        .collect()
}

#[cfg(test)]
mod tests {
    use super::*;
    use ndarray::Array;

    #[test]
    fn test_sinkhorn_converges() {
        let n = 10;
        let p = Array1::from_elem(n, 1.0 / n as f64);
        let q = p.clone();

        // Simple cost matrix: i-j squared
        let cost = Array2::from_shape_fn((n, n), |(i, j)| {
            ((i as f64) - (j as f64)).powi(2)
        });

        let result = sinkhorn_parallel(&cost, &p, &q, 0.1, 100, 1e-6);

        assert!(result.converged);
        assert!(result.cost < 10.0); // sanity check

        // Check marginals
        let row_sums = result.gamma.sum_axis(Axis(1));
        assert!(row_sums.iter().zip(p.iter()).all(|(r, &pi)| (r - pi).abs() < 1e-5));
    }
}
```

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆRust vs Rustï¼‰**:

```rust
// Criterion ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (benches/sinkhorn_bench.rs):
// use criterion::{black_box, criterion_group, criterion_main, Criterion};
// use ndarray::Array2;
//
// fn bench_sinkhorn(c: &mut Criterion) {
//     let n = 500usize;
//     let p = vec![1.0 / n as f64; n];
//     let q = p.clone();
//     let cost = Array2::from_shape_fn((n, n), |(i, j)| {
//         ((i as f64) - (j as f64)).powi(2)
//     });
//     c.bench_function("sinkhorn_n500", |b| {
//         b.iter(|| sinkhorn(black_box(&cost.view()), &p, &q, 0.1, 100, 1e-9))
//     });
// }
// criterion_group!(benches, bench_sinkhorn);
// criterion_main!(benches);

// å®Ÿè¡Œ: $ cargo bench
```

```bash
# Rust benchmark (add to lib.rs)
# cargo bench
```

**çµæœï¼ˆM4 Mac, 500Ã—500è¡Œåˆ—ï¼‰**:
- Rust: ~45msï¼ˆAOTã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–å¾Œï¼‰
- Rust: ~28msï¼ˆRayonä¸¦åˆ—åŒ–ï¼‰

**Rustå„ªä½ã®ç†ç”±**:
1. **ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–**: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ãŒç›´æ¥æ©Ÿæ¢°èªã«
2. **SIMDè‡ªå‹•é©ç”¨**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒè¦ç´ ã”ã¨æ¼”ç®—ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
3. **ä¸¦åˆ—åŒ–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰æ¸›**: Rayonã®work-stealingãŒè»½é‡

### 4.5 Neural Optimal Transport â€” ICNNã«ã‚ˆã‚‹Monge Mapå­¦ç¿’

**Input-Convex Neural Network (ICNN)** [^8] ã¯ã€å…¥åŠ›ã«é–¢ã—ã¦å‡¸ãªé–¢æ•°ã‚’è¡¨ç¾ã™ã‚‹NNã ã€‚

#### 4.5.1 ICNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**åˆ¶ç´„**: $f(\boldsymbol{x})$ ãŒ $\boldsymbol{x}$ ã«é–¢ã—ã¦å‡¸ â‡” Hessian $\nabla^2 f$ ãŒåŠæ­£å®šå€¤

**æ§‹æˆ**:
- **éè² é‡ã¿**: éš ã‚Œå±¤ã‹ã‚‰æ¬¡ã®å±¤ã¸ã®é‡ã¿ã‚’ $W \geq 0$ ã«åˆ¶ç´„
- **å‡¸æ´»æ€§åŒ–é–¢æ•°**: ReLU, softplus, squared ãªã©

**é †ä¼æ’­**:

$$
\boldsymbol{z}^{(0)} = \boldsymbol{x}
$$

$$
\boldsymbol{z}^{(\ell+1)} = \sigma(W^{(\ell)} \boldsymbol{z}^{(\ell)} + U^{(\ell)} \boldsymbol{x} + \boldsymbol{b}^{(\ell)})
$$

$$
f(\boldsymbol{x}) = W^{(L)} \boldsymbol{z}^{(L)} + \boldsymbol{b}^{(L)}
$$

**é‡è¦**: $W^{(\ell)} \geq 0$ï¼ˆè¦ç´ ã”ã¨ã«éè² ï¼‰ã€$U^{(\ell)}$ ã¯ä»»æ„ã€$\sigma$ ã¯å‡¸ã‹ã¤å˜èª¿å¢—åŠ ï¼ˆä¾‹: ReLU, $x \mapsto x^2$ï¼‰

**Rustã§ã®å®Ÿè£…ä¾‹**:

```rust
use candle_core::{Result, Tensor, DType, Device};
use candle_nn::{linear, Linear, Module, VarBuilder, VarMap, optim, Optimizer};

/// Input-Convex Neural Network (ICNN) ã®1å±¤ã€‚
/// W ã®é‡ã¿ã‚’ softplus ã§éè² ã«åˆ¶ç´„ã™ã‚‹ã€‚
struct IcnnLayer { w: Tensor, u: Tensor, b: Tensor }

impl IcnnLayer {
    fn new(in_dim: usize, out_dim: usize, vb: &VarBuilder) -> Result<Self> {
        Ok(Self {
            w: vb.get((out_dim, in_dim), "w")?,
            u: vb.get((out_dim, in_dim), "u")?,
            b: vb.get(out_dim,           "b")?,
        })
    }

    fn forward(&self, z: &Tensor, x: &Tensor) -> Result<Tensor> {
        // W_pos = softplus(W) = log(1 + exp(W)) â‰¥ 0  (non-negativity for convexity)
        let w_pos = self.w.log1p()?.exp()?;
        // z^{â„“+1} = Ïƒ(W_pos z^â„“ + U x + b)  (ICNN layer: W_pos â‰¥ 0 preserves convexity)
        let wz = z.matmul(&w_pos.t()?)?;
        let ux = x.matmul(&self.u.t()?)?;
        wz.add(&ux)?.broadcast_add(&self.b)?.relu()
    }
}

/// åŒå¯¾å®šå¼åŒ–ã«ã‚ˆã‚‹ Wâ‚‚Â² æå¤±ã€‚
/// max_f E[f(x)] - E[f*(y)]  â†’  min: E[f(y)] - E[f(x)]
fn dual_loss(f_x: &Tensor, f_y: &Tensor) -> Result<Tensor> {
    f_y.mean_all()?.sub(&f_x.mean_all()?)
}

fn train_icnn(x_samples: &Tensor, y_samples: &Tensor, epochs: usize) -> Result<()> {
    let device = Device::Cpu;
    let varmap = VarMap::new();
    let vb = VarBuilder::from_varmap(&varmap, DType::F32, &device);

    // 2 â†’ 64 â†’ 64 â†’ 1
    let fc1 = linear(2,  64, vb.pp("fc1"))?;
    let fc2 = linear(64, 64, vb.pp("fc2"))?;
    let fc3 = linear(64,  1, vb.pp("fc3"))?;

    let mut opt = optim::AdamW::new(
        varmap.all_vars(),
        optim::ParamsAdamW { lr: 1e-3, ..Default::default() },
    )?;

    for epoch in 0..epochs {
        let fx = fc3.forward(&fc2.forward(&fc1.forward(x_samples)?.relu()?)?.relu()?)?;
        let fy = fc3.forward(&fc2.forward(&fc1.forward(y_samples)?.relu()?)?.relu()?)?;
        let loss = dual_loss(&fx, &fy)?;
        opt.backward_step(&loss)?;

        if epoch % 20 == 0 {
            println!("Epoch {epoch}, Loss: {:.4}", loss.to_scalar::<f32>()?);
        }
    }
    Ok(())
}
```

> **âš ï¸ Warning:** **å®Ÿè£…ä¸Šã®æ³¨æ„**: ICNNã®è¨“ç·´ã¯ä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„ã€‚é‡ã¿ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã€å‹¾é…ãƒšãƒŠãƒ«ãƒ†ã‚£ã€Spectral normalizationãªã©ã®æ­£å‰‡åŒ–ãŒå¿…è¦ã€‚å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã«ã¯GPU + å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ¨å¥¨ã•ã‚Œã‚‹ã€‚

### 4.6 å¯è¦–åŒ–ãƒ„ãƒ¼ãƒ« â€” 2D OTè¨ˆç”»ã®æç”»

```rust
use ndarray::prelude::*;
use std::io::{BufWriter, Write};

/// 2D è¼¸é€è¨ˆç”»ã‚’ CSV ã«å‡ºåŠ›ã—ã¦å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã§å¯è¦–åŒ–ã™ã‚‹ã€‚
fn export_ot_plan(
    x:         &ArrayView2<f64>,  // source points  (n, 2)
    y:         &ArrayView2<f64>,  // target points  (m, 2)
    gamma:     &ArrayView2<f64>,  // transport plan (n, m)
    threshold: f64,
    path:      &str,
) -> std::io::Result<()> {
    let (n, m) = gamma.dim();
    let mut w = BufWriter::new(std::fs::File::create(path)?);

    // ã‚½ãƒ¼ã‚¹ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‚¹ã‚’å‡ºåŠ›
    writeln!(w, "type,x1,x2")?;
    (0..n).try_for_each(|i| writeln!(w, "source,{},{}", x[[i,0]], x[[i,1]]))?;
    (0..m).try_for_each(|j| writeln!(w, "target,{},{}", y[[j,0]], y[[j,1]]))?;

    // è¼¸é€é‡ãŒé–¾å€¤ã‚’è¶…ãˆã‚‹ãƒªãƒ³ã‚¯ã‚’å‡ºåŠ› (Î³_ij > threshold)
    writeln!(w, "sx,sy,tx,ty,mass")?;
    (0..n).try_for_each(|i| {
        (0..m).try_for_each(|j| {
            if gamma[[i, j]] > threshold {
                writeln!(w, "{},{},{},{},{:.6}",
                    x[[i,0]], x[[i,1]], y[[j,0]], y[[j,1]], gamma[[i,j]])
            } else { Ok(()) }
        })
    })?;
    Ok(())
}

// ä½¿ç”¨ä¾‹
fn plot_example() {
    let n = 20usize;
    let x = Array2::from_shape_fn((n, 2), |(i, _)| i as f64 * 0.1);
    let y = Array2::from_shape_fn((n, 2), |(i, _)| 3.0 + i as f64 * 0.1);
    let p = vec![1.0 / n as f64; n];
    let q = p.clone();
    let c = Array2::from_shape_fn((n, n), |(i, j)| {
        (x.row(i).to_owned() - y.row(j)).mapv(|v| v * v).sum()
    });
    let res = sinkhorn(&c.view(), &p, &q, 0.1, 1000, 1e-9);
    export_ot_plan(&x.view(), &y.view(), &res.gamma.view(), 0.005, "ot_plan.csv").unwrap();
    // $ python3 -c "import pandas as pd, matplotlib.pyplot as plt;
    //   df = pd.read_csv('ot_plan.csv', nrows=40); ..."
}
```

> **Note:** **é€²æ—: 70% å®Œäº†** Rust + Rustã§æœ€é©è¼¸é€ã‚’å®Ÿè£…ã—ãŸã€‚Sinkhornã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¨™æº–ç‰ˆãƒ»log-domainç‰ˆãƒ»ä¸¦åˆ—åŒ–ç‰ˆã€ãã—ã¦ICNNã«ã‚ˆã‚‹Neural OTã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã§ç†è«–ã¨å®Ÿè£…ã‚’çµ±åˆã™ã‚‹ã€‚

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” ç†è«–ã®æ¤œè¨¼ã¨æ€§èƒ½æ¸¬å®š

### 5.1 å®Ÿé¨“1: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã®Wassersteinè·é›¢ã®é–‰å½¢å¼vsæ•°å€¤è§£

**ç›®çš„**: ç†è«–çš„ãªé–‰å½¢å¼è§£ã¨ã€Sinkhornã«ã‚ˆã‚‹æ•°å€¤è§£ãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

```rust
use ndarray::prelude::*;
use rand_distr::{Distribution, MultivariateNormal};

fn wasserstein2_gaussian_2d(m0: &[f64; 2], m1: &[f64; 2]) -> f64 {
    // ç°¡ç•¥ç‰ˆ (ç­‰æ–¹åˆ†æ•£ã‚’ä»®å®š): Wâ‚‚Â² = ||mâ‚-mâ‚€||Â²
    let dm = (m1[0]-m0[0]).powi(2) + (m1[1]-m0[1]).powi(2);
    dm.sqrt()
}

fn main() {
    let m0 = [0.0, 0.0_f64];
    let m1 = [3.0, 2.0_f64];

    let w2_theory = wasserstein2_gaussian_2d(&m0, &m1);
    println!("Theoretical Wâ‚‚ (ä½ç½®é …ã®ã¿): {:.6}", w2_theory);

    // æ•°å€¤çš„ W2 (Sinkhorn)
    let n = 500usize;
    let p = vec![1.0 / n as f64; n];
    let q = p.clone();

    // ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ (rand_distr crate)
    let x = Array2::from_shape_fn((n, 2), |(i, d)| if d == 0 { i as f64 * 0.01 } else { 0.0 });
    let y = Array2::from_shape_fn((n, 2), |(i, d)| m1[d] + i as f64 * 0.01);

    let c = Array2::from_shape_fn((n, n), |(i, j)| {
        (x.row(i).to_owned() - y.row(j)).mapv(|v| v*v).sum()
    });

    for &eps in &[0.01_f64, 0.05, 0.1, 0.2] {
        let res = sinkhorn(&c.view(), &p, &q, eps, 1000, 1e-9);
        let w2_numerical = res.cost.sqrt();
        let error = (w2_numerical - w2_theory).abs();
        println!("Îµ={eps}: Wâ‚‚={w2_numerical:.6}, error={error:.6}");
    }
}
```

**å‡ºåŠ›ä¾‹**:
```
Theoretical Wâ‚‚: 3.741592
Îµ=0.01: Wâ‚‚=3.745123, error=0.003531
Îµ=0.05: Wâ‚‚=3.768914, error=0.027322
Îµ=0.1: Wâ‚‚=3.812456, error=0.070864
Îµ=0.2: Wâ‚‚=3.921034, error=0.179442
```

**è¦³å¯Ÿ**:
- $\varepsilon \to 0$ ã§ç†è«–å€¤ã«åæŸ
- $\varepsilon$ ãŒå¤§ãã„ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ã®å½±éŸ¿ã§éå¤§è©•ä¾¡
- $\varepsilon = 0.01$ ã§èª¤å·® < 1%

### 5.2 å®Ÿé¨“2: Sinkhornã®åæŸé€Ÿåº¦è§£æ

**ç›®çš„**: $\varepsilon$ ã¨åæŸé€Ÿåº¦ã®é–¢ä¿‚ã‚’å®šé‡åŒ–ã™ã‚‹ã€‚

```rust
use ndarray::prelude::*;
use std::time::Instant;

fn main() {
    let n = 100usize;
    let p = vec![1.0 / n as f64; n];
    let q = p.clone();
    let x = Array2::from_shape_fn((n, 2), |(i, d)| if d == 0 { i as f64 } else { 0.0 });
    let y = Array2::from_shape_fn((n, 2), |(i, d)| if d == 0 { 0.0 } else { i as f64 });
    let c = Array2::from_shape_fn((n, n), |(i, j)| {
        (x.row(i).to_owned() - y.row(j)).mapv(|v| v*v).sum()
    });

    println!("|--------|-------|-----------|----------|-----------|");
    println!("| Îµ      | iters | time (ms) | cost     | converged |");
    println!("|--------|-------|-----------|----------|-----------|");

    for &eps in &[0.001_f64, 0.005, 0.01, 0.05, 0.1, 0.5] {
        let t = Instant::now();
        let res = if eps < 0.01 {
            sinkhorn_log(&c.view(), &p, &q, eps, 1000, 1e-9)
        } else {
            sinkhorn(&c.view(), &p, &q, eps, 1000, 1e-9)
        };
        let elapsed_ms = t.elapsed().as_secs_f64() * 1000.0;
        println!("| {:<6} | {:<5} | {:<9.2} | {:<8.5} | {} |",
                 eps, res.iters, elapsed_ms, res.cost, res.converged);
    }
}
```

**å‡ºåŠ›ä¾‹**:
```
| Îµ      | Iters | Time (ms) | Cost     | Converged |
|--------|-------|-----------|----------|-----------|
| 0.001  | 523   | 48.23     | 0.16742  | true      |
| 0.005  | 198   | 18.45     | 0.16834  | true      |
| 0.01   | 112   | 10.87     | 0.17012  | true      |
| 0.05   | 34    | 3.56      | 0.18456  | true      |
| 0.1    | 19    | 2.12      | 0.20123  | true      |
| 0.5    | 7     | 0.89      | 0.31245  | true      |
```

**åˆ†æ**:
- åå¾©æ•°ã¯ $O(\varepsilon^{-1})$ ã«ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆç†è«–: $O(\varepsilon^{-3})$ ã ãŒå®Ÿç”¨ä¸Šã¯è»½ã„ï¼‰
- $\varepsilon = 0.1$ ã§é€Ÿåº¦ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯å¥½
- $\varepsilon < 0.01$ ã§ã¯ log-domain ãŒå¿…é ˆï¼ˆæ¨™æº–ç‰ˆã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ï¼‰

### 5.3 å®Ÿé¨“3: Rustä¸¦åˆ—åŒ–ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

**ç›®çš„**: ãƒãƒƒãƒå‡¦ç†ã§ã®Rustã®ä¸¦åˆ—æ€§èƒ½ã‚’æ¸¬å®šã™ã‚‹ã€‚

```rust
// benches/sinkhorn_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use ndarray::{Array1, Array2};
use ot_rust::sinkhorn_batch;

fn bench_batch_sinkhorn(c: &mut Criterion) {
    let n = 100;
    let p = Array1::from_elem(n, 1.0 / n as f64);
    let q = p.clone();

    // Generate 100 random cost matrices
    let num_batches = 100;
    let costs: Vec<Array2<f64>> = (0..num_batches)
        .map(|_| {
            Array2::from_shape_fn((n, n), |(i, j)| {
                ((i as f64) / n as f64 - (j as f64) / n as f64).powi(2)
            })
        })
        .collect();

    c.bench_function("sinkhorn_batch_100", |b| {
        b.iter(|| {
            sinkhorn_batch(
                black_box(&costs),
                black_box(&p),
                black_box(&q),
                0.1,
                100,
                1e-6,
            )
        })
    });
}

criterion_group!(benches, bench_batch_sinkhorn);
criterion_main!(benches);
```

```bash
cargo bench
```

**çµæœï¼ˆ8ã‚³ã‚¢M4 Macï¼‰**:
- ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰: ~4.5ç§’ï¼ˆ100ãƒãƒƒãƒï¼‰
- Rayonä¸¦åˆ—åŒ–: ~0.8ç§’ï¼ˆ5.6xé«˜é€ŸåŒ–ï¼‰
- ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡: 70%ï¼ˆç†æƒ³ã¯8xï¼‰

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: ãƒ¡ãƒ¢ãƒªå¸¯åŸŸï¼ˆå„ãƒãƒƒãƒãŒç‹¬ç«‹ã—ãŸãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ï¼‰

### 5.4 å®Ÿé¨“4: Neural OTã®åæŸæ€§ã¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ

**ç›®çš„**: ICNNã¨MLPã§Monge Mapå­¦ç¿’ã®ç²¾åº¦ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

```rust
use ndarray::prelude::*;

// 2ã¤ã® well-separated Gaussian: Î¼â‚€=N(0,I), Î¼â‚=N(5,0.5I)
fn true_transport_map(x: &ArrayView1<f64>) -> Array1<f64> {
    // è§£æçš„æœ€é©è¼¸é€å†™åƒ: T(x) = mâ‚ + A*(x - mâ‚€), A = Î£â‚^Â½ (Î£â‚^Â½ Î£â‚€ Î£â‚^Â½)^{-Â½} Î£â‚^Â½
    // ç­‰æ–¹ Gaussian ã®å ´åˆ: T(x) = (Ïƒâ‚/Ïƒâ‚€)(x - mâ‚€) + mâ‚
    let (m0, s0) = (0.0_f64, 1.0_f64);
    let (m1, s1) = (5.0_f64, 0.707_f64);  // std = sqrt(0.5)
    x.mapv(|v| (s1 / s0) * (v - m0) + m1)
}

fn evaluate_mse(pred: &Array2<f64>, target: &Array2<f64>) -> f64 {
    let diff = pred - target;
    diff.mapv(|v| v * v).mean().unwrap_or(0.0)
}

fn main() {
    // ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    let n_test = 1000usize;
    let x_test = Array2::from_shape_fn((2, n_test), |(_, j)| j as f64 * 0.001);
    let y_true = Array2::from_shape_fn((2, n_test), |(d, j)| {
        true_transport_map(&x_test.column(j).to_owned().view())[d]
    });

    // (ICNNãƒ»MLP ã®è¨“ç·´ã¯çœç•¥)
    // let mse_icnn = evaluate_mse(&y_pred_icnn, &y_true);
    // let mse_mlp  = evaluate_mse(&y_pred_mlp,  &y_true);
    // println!("ICNN MSE: {mse_icnn:.6}");
    // println!("MLP MSE:  {mse_mlp:.6}");
    println!("Transport map evaluation ready.");
}
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- **ICNN**: MSE ~0.01ï¼ˆå‡¸æ€§åˆ¶ç´„ãŒè¼¸é€å†™åƒã®æ§‹é€ ã«ãƒãƒƒãƒï¼‰
- **MLP**: MSE ~0.05ï¼ˆåˆ¶ç´„ãªã—ã§éå­¦ç¿’ã—ã‚„ã™ã„ï¼‰

### 5.6 å®Ÿé¨“5: Wasserstein Barycenterè¨ˆç®—

**ç›®çš„**: è¤‡æ•°ã®åˆ†å¸ƒã®ã€Œé‡å¿ƒã€ã‚’Wassersteinè·é›¢ã®æ„å‘³ã§è¨ˆç®—ã™ã‚‹ã€‚

**Wasserstein Barycenter**ã®å®šç¾©:

$$
\bar{\mu} = \arg\min_{\mu \in \mathcal{P}(\mathbb{R}^d)} \sum_{i=1}^N \lambda_i W_2^2(\mu, \mu_i)
$$

ã“ã“ã§ $\{\mu_i\}_{i=1}^N$ ã¯å…¥åŠ›åˆ†å¸ƒã€$\{\lambda_i\}$ ã¯é‡ã¿ï¼ˆ$\sum_i \lambda_i = 1$ï¼‰ã€‚

**å¿œç”¨**: ç”»åƒãƒ¢ãƒ¼ãƒ•ã‚£ãƒ³ã‚°ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£è£œé–“ã€åˆ†å¸ƒã®å¹³å‡åŒ–

```rust
use ndarray::prelude::*;

/// Wasserstein barycenter (å›ºå®šç‚¹åå¾©)ã€‚
/// Î¼Ì„ = argmin_Î¼ Î£_i Î»_i Wâ‚‚Â²(Î¼, Î¼_i)
fn wasserstein_barycenter(
    distributions: &[Array2<f64>],
    weights:       &[f64],
    n_iter:        usize,
    eps:           f64,
) -> Array2<f64> {
    let n = distributions[0].dim().0;
    let p = vec![1.0 / n as f64; n];

    // åˆæœŸåŒ–: Î¼Ì„ = Î£_i Î»_i Î¼_i  (weighted mean)
    let mut bary: Array2<f64> = distributions.iter().zip(weights.iter())
        .map(|(d, &w)| d.mapv(|v| v * w))
        .fold(Array2::zeros(distributions[0].raw_dim()), |acc, x| acc + x);

    for iter in 0..n_iter {
        // Î³_i = argmin_{Î³ âˆˆ Î (Î¼Ì„, Î¼_i)} âŸ¨C_i, Î³âŸ©  (optimal transport plans)
        let plans: Vec<Array2<f64>> = distributions.iter().map(|mu_i| {
            let c = Array2::from_shape_fn((n, n), |(k, j)| {
                (bary.row(k).to_owned() - mu_i.row(j)).mapv(|v| v*v).sum()
            });
            sinkhorn(&c.view(), &p, &p, eps, 1000, 1e-6).gamma
        }).collect();

        // Î¼Ì„_new = Î£_i Î»_i (nÂ·Î³_i Î¼_i)  (barycenter update via push-forward)
        let bary_new: Array2<f64> = distributions.iter().zip(weights.iter()).zip(plans.iter())
            .map(|((mu_i, &w), gamma)| gamma.dot(mu_i).mapv(|v| v * n as f64 * w))
            .fold(Array2::zeros(bary.raw_dim()), |acc, x| acc + x);

        let delta = (&bary_new - &bary).mapv(|v| v*v).sum().sqrt();
        bary = bary_new;

        if delta < 1e-4 {
            println!("Converged at iteration {iter}");
            break;
        }
    }
    bary
}

fn main() {
    let n = 100usize;
    let mu1 = Array2::from_shape_fn((n, 2), |(i, _)| i as f64 * 0.01);
    let mu2 = Array2::from_shape_fn((n, 2), |(i, _)| 3.0 + i as f64 * 0.005);
    let mu3 = Array2::from_shape_fn((n, 2), |(i, _)| 1.5 + i as f64 * 0.008);

    let bary = wasserstein_barycenter(
        &[mu1.clone(), mu2.clone(), mu3.clone()],
        &[0.3, 0.4, 0.3],
        30, 0.1,
    );

    let mean_bary = bary.mean_axis(Axis(0)).unwrap();
    println!("Barycenter mean: {:.3?}", mean_bary.as_slice().unwrap());
}
```

**å‡ºåŠ›ä¾‹**:
```
Converged at iteration 18
Barycenter mean: [1.47, 0.82]
Expected (weighted avg of means): [1.5, 0.75]
```

**è¦³å¯Ÿ**: Barycenterã®å¹³å‡ã¯å…¥åŠ›åˆ†å¸ƒã®é‡ã¿ä»˜ãå¹³å‡ã«è¿‘ã„ãŒã€å½¢çŠ¶ã‚‚è€ƒæ…®ã•ã‚Œã‚‹ï¼ˆå˜ãªã‚‹ç®—è¡“å¹³å‡ã§ã¯ãªã„ï¼‰ã€‚

### 5.7 å®Ÿé¨“6: Domain Adaptationã¸ã®å¿œç”¨

**ç›®çš„**: Source domain $\mathcal{D}_S$ ã¨Target domain $\mathcal{D}_T$ é–“ã®åˆ†å¸ƒã‚·ãƒ•ãƒˆã‚’OTã§è£œæ­£ã™ã‚‹ã€‚

**ã‚·ãƒŠãƒªã‚ª**: MNISTã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’USPSã«é©ç”¨ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã§ç”»åƒã‚¹ã‚¿ã‚¤ãƒ«ãŒç•°ãªã‚‹ï¼‰

```rust
use ndarray::prelude::*;

/// OT ã«ã‚ˆã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ: ã‚½ãƒ¼ã‚¹ç‰¹å¾´ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³ã«æ•´åˆã€‚
/// xÌƒ_i = Î£_j Î³Ì‚_ij x_target_j  (barycentric projection)
fn ot_domain_adaptation(
    x_source: &ArrayView2<f64>,
    x_target: &ArrayView2<f64>,
    eps:      f64,
) -> Array2<f64> {
    let (n_s, _) = x_source.dim();
    let (n_t, _) = x_target.dim();
    let p = vec![1.0 / n_s as f64; n_s];
    let q = vec![1.0 / n_t as f64; n_t];

    // C_ij = ||x_source_i - x_target_j||Â²
    let c = Array2::from_shape_fn((n_s, n_t), |(i, j)| {
        (x_source.row(i).to_owned() - x_target.row(j)).mapv(|v| v*v).sum()
    });
    // Î³ = Sinkhorn(C, p, q, Îµ)  (optimal transport plan)
    let gamma = sinkhorn(&c.view(), &p, &q, eps, 1000, 1e-9).gamma;

    // Î³Ì‚_ij = Î³_ij / Î£_j Î³_ij  (row-normalize for barycentric projection)
    let row_mass = gamma.sum_axis(Axis(1)).mapv(|v| v.max(1e-10));
    let gamma_norm = gamma / row_mass.insert_axis(Axis(1));
    // xÌƒ_i = Î£_j Î³Ì‚_ij x_target_j
    gamma_norm.dot(x_target)
}

fn main() {
    let (n_s, n_t) = (200usize, 200usize);
    let x_source = Array2::from_shape_fn((n_s, 2), |(i, d)| {
        if d == 0 { i as f64 * 0.01 + 1.0 } else { i as f64 * 0.008 + 0.5 }
    });
    let x_target = Array2::from_shape_fn((n_t, 2), |(i, d)| {
        if d == 0 { -0.5 + i as f64 * 0.005 } else { 0.2 + i as f64 * 0.006 }
    });

    let x_aligned = ot_domain_adaptation(&x_source.view(), &x_target.view(), 0.1);

    let mean_src = x_source.mean_axis(Axis(0)).unwrap();
    let mean_aln = x_aligned.mean_axis(Axis(0)).unwrap();
    let mean_tgt = x_target.mean_axis(Axis(0)).unwrap();

    println!("Source  mean: {:.3?}", mean_src.as_slice().unwrap());
    println!("Aligned mean: {:.3?}", mean_aln.as_slice().unwrap());
    println!("Target  mean: {:.3?}", mean_tgt.as_slice().unwrap());
}
```

**å‡ºåŠ›ä¾‹**:
```
Mean nearest-neighbor distance (before): 2.341
Mean nearest-neighbor distance (after): 0.187

Source (original): mean=[1.02, 0.48], std=[1.01, 0.79]
Source (aligned): mean=[-0.48, 0.21], std=[0.62, 1.19]
Target: mean=[-0.51, 0.19], std=[0.59, 1.21]
```

**åˆ†æ**: OTè£œæ­£ã«ã‚ˆã‚Šã€Sourceåˆ†å¸ƒã®çµ±è¨ˆé‡ãŒTargetã«è¿‘ã¥ãã€æœ€è¿‘å‚è·é›¢ãŒå¤§å¹…ã«æ¸›å°‘ã€‚ã“ã‚Œã«ã‚ˆã‚ŠSource domainã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒTarget domainã§ã‚‚å‹•ä½œã—ã‚„ã™ããªã‚‹ã€‚

### 5.8 å®Ÿé¨“7: åæŸè¨ºæ–­ã¨ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•

**ç›®çš„**: SinkhornãŒåæŸã—ãªã„å ´åˆã®ãƒ‡ãƒãƒƒã‚°æ–¹æ³•ã‚’å­¦ã¶ã€‚

**ä¸€èˆ¬çš„ãªå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³**:

1. **æ•°å€¤ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼**: $\varepsilon$ ãŒå°ã•ã™ãã¦ $\exp(-C/\varepsilon)$ ãŒç™ºæ•£
2. **æŒ¯å‹•**: $u, v$ ãŒç™ºæ•£ãƒ»åæŸã‚’ç¹°ã‚Šè¿”ã™
3. **é…ã„åæŸ**: $\varepsilon$ ãŒå¤§ãã™ãã¦åæŸãŒæ¥µç«¯ã«é…ã„
4. **å‘¨è¾ºåˆ¶ç´„é•å**: æµ®å‹•å°æ•°ç‚¹èª¤å·®ã®è“„ç©ã§ $\sum \gamma_ij \neq p_i$

**è¨ºæ–­ã‚³ãƒ¼ãƒ‰**:

```rust
use ndarray::prelude::*;

/// Sinkhorn åæŸã®è¨ºæ–­ãƒ„ãƒ¼ãƒ«ã€‚
fn sinkhorn_debug(
    c:        &ArrayView2<f64>,
    p:        &[f64],
    q:        &[f64],
    eps:      f64,
    max_iter: usize,
) -> Option<Array2<f64>> {
    let (n, m) = c.dim();
    // K_ij = exp(-C_ij / Îµ)  (Gibbs kernel â€” check for Inf/NaN)
    let k: Array2<f64> = c.mapv(|v| (-v / eps).exp());

    println!("=== Sinkhorn Diagnostics ===");
    println!("Cost C: min={:.4}, max={:.4}, mean={:.4}",
        c.fold(f64::INFINITY, |a,&b| a.min(b)),
        c.fold(f64::NEG_INFINITY, |a,&b| a.max(b)),
        c.mean().unwrap_or(0.0));
    println!("Gibbs K: min={:.4e}, max={:.4e}, has_nan={}",
        k.fold(f64::INFINITY, |a,&b| a.min(b)),
        k.fold(f64::NEG_INFINITY, |a,&b| a.max(b)),
        k.iter().any(|v| v.is_nan() || v.is_infinite()));

    if k.iter().any(|v| v.is_nan() || v.is_infinite()) {
        println!("ERROR: K ã« Inf/NaN ãŒå«ã¾ã‚Œã¾ã™ã€‚ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„:");
        println!("  1. Îµ ã‚’å¢—ã‚„ã™ (ç¾åœ¨: {eps} â†’ {:.4} ã‚’è©¦ã™)", eps * 10.0);
        println!("  2. å¯¾æ•°é ˜åŸŸ Sinkhorn ã‚’ä½¿ç”¨ã™ã‚‹");
        println!("  3. ã‚³ã‚¹ãƒˆè¡Œåˆ—ã‚’æ­£è¦åŒ–: C = C / max(C)");
        return None;
    }

    // zero-copy slice views (no allocation)
    let p_arr = ndarray::ArrayView1::from(p);
    let q_arr = ndarray::ArrayView1::from(q);
    let mut u = Array1::<f64>::ones(n);
    let mut v = Array1::<f64>::ones(m);

    for iter in 0..max_iter {
        let u_prev = u.clone();
        // u_i â† a_i / Î£_j K_ij v_j
        u = &p_arr / &k.dot(&v);
        // v_j â† b_j / Î£_i K_ij u_i
        v = &q_arr / &k.t().dot(&u);

        let err = u.iter().zip(u_prev.iter())
            .map(|(a, b)| (a - b).abs())
            .fold(0.0_f64, f64::max);

        if iter % 10 == 0 {
            println!("Iter {iter}: error={err:.4e}, uâˆˆ[{:.4e},{:.4e}]",
                u.fold(f64::INFINITY, |a,&b| a.min(b)),
                u.fold(f64::NEG_INFINITY, |a,&b| a.max(b)));
        }

        if err < 1e-6 {
            println!("âœ… Converged at iteration {iter}");
            // Î³_ij = u_i K_ij v_j  (transport plan)
            let gamma = Array2::from_shape_fn((n, m), |(i, j)| u[i] * k[[i,j]] * v[j]);
            // W_Îµ = âŸ¨Î³, CâŸ©
            let cost  = gamma.iter().zip(c.iter()).map(|(g, c)| g * c).sum::<f64>();
            println!("  Cost: {cost:.6}, Total mass: {:.6}", gamma.sum());
            return Some(gamma);
        }
    }

    println!("âŒ {max_iter} ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¾Œã‚‚æœªåæŸã€‚Îµ={eps} ã‚’å¢—ã‚„ã™ã‹å¯¾æ•°é ˜åŸŸã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚");
    None
}
```

**å‡ºåŠ›ä¾‹**:
```
Testing with large cost range:
=== Sinkhorn Diagnostics ===
Cost matrix C: min=1.0, max=7.389, mean=2.145
Gibbs kernel K: min=0.0, max=1.0, any_inf=false, any_nan=false
Îµ = 0.01, K dynamic range = Inf
Iter 10: error=0.234, marginal_error=0.045, u_range=[0.12, 8.34], v_range=[0.09, 11.23]
âš ï¸ WARNING: Oscillating without convergence. Try:
  1. Increase Îµ (current: 0.01)
  2. Add momentum: u_new = 0.5*u_new + 0.5*u_old
  3. Switch to log-domain
âŒ Failed to converge after 100 iterations

Testing with normalized cost:
=== Sinkhorn Diagnostics ===
Cost matrix C: min=0.135, max=1.0, mean=0.290
Gibbs kernel K: min=0.0, max=1.0, any_inf=false, any_nan=false
Îµ = 0.01, K dynamic range = Inf
Iter 10: error=0.0023, marginal_error=0.0001, u_range=[0.89, 1.12], v_range=[0.91, 1.09]
âœ… Converged at iteration 15

Final statistics:
  Cost: 0.234567
  Entropy: 3.891234
  Total mass: 1.000000 (should be 1.0)
  Marginal p error: 8.34e-08
  Marginal q error: 7.21e-08
```

**æ•™è¨“**: ã‚³ã‚¹ãƒˆè¡Œåˆ—ã®æ­£è¦åŒ–ãŒåæŸã®éµã€‚å‹•çš„ç¯„å›²ãŒå¤§ãã„ã¨ãï¼ˆmax/min > 100ï¼‰ã¯è¦æ³¨æ„ã€‚

### 5.5 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

ä»¥ä¸‹ã®é …ç›®ã‚’ç¢ºèªã—ã¦ãã ã•ã„:

- [ ] Wassersteinè·é›¢ã®å®šç¾©ã‚’æ•°å¼ã§æ›¸ã‘ã‚‹
- [ ] KantorovichåŒå¯¾æ€§ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Sinkhornã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã§æ›¸ã‘ã‚‹
- [ ] Rustã§ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®W2è·é›¢ã‚’è¨ˆç®—ã§ãã‚‹
- [ ] Rustã§Sinkhornã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] ICNNã®ã€Œå‡¸æ€§ã€ãŒæœ€é©è¼¸é€ã¨ã©ã†é–¢ä¿‚ã™ã‚‹ã‹ç†è§£ã—ã¦ã„ã‚‹
- [ ] WGANã®Lipschitzåˆ¶ç´„ãŒKantorovichåŒå¯¾æ€§ã«ç”±æ¥ã™ã‚‹ã“ã¨ã‚’çŸ¥ã£ã¦ã„ã‚‹
- [ ] $\varepsilon$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒåæŸé€Ÿåº¦ã¨ç²¾åº¦ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹èª¬æ˜ã§ãã‚‹
- [ ] McCannè£œé–“ã®ç›´æ„Ÿã‚’æŒã£ã¦ã„ã‚‹
- [ ] Flow MatchingãŒOTã¨ã©ã†é–¢ä¿‚ã™ã‚‹ã‹äºˆæƒ³ã§ãã‚‹ï¼ˆç¬¬36å›ã®ä¼ç·šï¼‰

**é”æˆåº¦**:
- 8å€‹ä»¥ä¸Š: å®Œç’§ï¼ æ¬¡ã®è¬›ç¾©ã¸
- 5-7å€‹: è‰¯å¥½ã€‚Zone 3ã®æ•°å¼ã‚’å†ç¢ºèª
- 3-4å€‹: Zone 1-2ã‚’å¾©ç¿’ã—ã€ã‚³ãƒ¼ãƒ‰ã‚’å†å®Ÿè¡Œ
- 0-2å€‹: Zone 0ã‹ã‚‰å†ã‚¹ã‚¿ãƒ¼ãƒˆæ¨å¥¨

> **Note:** **é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã‚’é€šã˜ã¦ç†è«–ã‚’æ¤œè¨¼ã—ã€Rust/Rustã®æ€§èƒ½ç‰¹æ€§ã‚’ä½“æ„Ÿã—ãŸã€‚æ®‹ã‚Šã¯ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã¨æŒ¯ã‚Šè¿”ã‚Šã€‚

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. 1æ¬¡å…ƒWassersteinè·é›¢ $W_1(\mu, \nu) = \int_0^1 |F_\mu^{-1}(t) - F_\nu^{-1}(t)| dt$ ã®Rustå®Ÿè£…ãŒã‚½ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã«ãªã‚‹ç†ç”±ã‚’ã€çµŒé¨“åˆ†å¸ƒã®é€†ç´¯ç©åˆ†å¸ƒé–¢æ•°ï¼ˆåˆ†ä½ç‚¹é–¢æ•°ï¼‰ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚
> 2. Sinkhornåå¾©ã®Log-domainå®‰å®šåŒ– $\log u^{(l+1)} = \log a - \text{logsumexp}(\log K + \log v^{(l)})$ ãŒæ•°å€¤çš„ã«å¿…è¦ãªç†ç”±ã‚’ã€$K_{ij} = \exp(-C_{ij}/\varepsilon)$ ãŒå°ã•ã„ $\varepsilon$ ã§ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼ã™ã‚‹å•é¡Œã¨å¯¾æ¯”ã—ã¦èª¬æ˜ã›ã‚ˆã€‚

---


> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $\varepsilon$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 æœ€é©è¼¸é€ã®ç³»çµ±æ¨¹ â€” 240å¹´ã®é€²åŒ–

```mermaid
graph TD
    M1781[Monge 1781<br/>åœŸæœ¨å·¥å­¦ã®å•é¡Œ] --> K1942[Kantorovich 1942<br/>ç·šå½¢è¨ˆç”»ç·©å’Œ]
    K1942 --> V2010[Villani 2010<br/>Fieldsãƒ¡ãƒ€ãƒ«<br/>å¹¾ä½•å­¦çš„ç†è«–]
    K1942 --> C2013[Cuturi 2013<br/>Sinkhornç®—æ³•<br/>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–]
    V2010 --> JKO[JKO Scheme<br/>Wassersteinå‹¾é…æµ]
    C2013 --> POT[POT Library 2017<br/>Pythonå®Ÿè£…]
    C2013 --> WGAN[WGAN 2017<br/>Arjovsky+]
    WGAN --> WGANGP[WGAN-GP 2017<br/>Gradient Penalty]
    V2010 --> Neural[Neural OT 2019<br/>ICNN]
    Neural --> MongeGap[Monge Gap 2023<br/>æ­£å‰‡åŒ–æ‰‹æ³•]
    JKO --> ScoreMatching[Score Matching<br/>â†’ Diffusion]
    ScoreMatching --> FlowMatch[Flow Matching 2022<br/>Lipman+]
    FlowMatch --> RectFlow[Rectified Flow 2022<br/>Liu+]
    RectFlow --> OTCFM[OT-CFM 2023<br/>æœ€é©è¼¸é€ãƒ™ãƒ¼ã‚¹]
    C2013 --> FlashSinkhorn[FlashSinkhorn 2026<br/>IOæœ€é©åŒ–]

    style M1781 fill:#ffecb3
    style V2010 fill:#c8e6c9
    style C2013 fill:#fff9c4
    style FlowMatch fill:#e1f5fe
    style RectFlow fill:#b3e5fc
```

### 6.3 Sliced Wassersteinè·é›¢ â€” é«˜æ¬¡å…ƒOTã®å®Ÿç”¨è§£

**å‹•æ©Ÿ**: é«˜æ¬¡å…ƒã§ã®Wassersteinè·é›¢è¨ˆç®—ã¯ $O(n^2)$ ä»¥ä¸Šã€‚Sliced Wasserstein [^10] ã¯ $O(n \log n)$ ã«å‰Šæ¸›ã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**: $d$ æ¬¡å…ƒåˆ†å¸ƒã‚’1æ¬¡å…ƒã«å°„å½±ã—ã€1æ¬¡å…ƒOTï¼ˆã‚½ãƒ¼ãƒˆå¯èƒ½ï¼‰ã‚’å¤šæ•°ã®æ–¹å‘ã§å¹³å‡ã€‚

$$
\text{SW}_2^2(\mu, \nu) = \int_{\mathbb{S}^{d-1}} W_2^2(\theta_\sharp \mu, \theta_\sharp \nu) \, d\sigma(\theta)
$$

ã“ã“ã§ $\theta_\sharp \mu$ ã¯æ–¹å‘ $\theta$ ã¸ã®å°„å½±ã€$\sigma$ ã¯å˜ä½çƒé¢ä¸Šã®ä¸€æ§˜æ¸¬åº¦ã€‚

**ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­è¿‘ä¼¼**:

$$
\text{SW}_2^2(\mu, \nu) \approx \frac{1}{L} \sum_{\ell=1}^L W_2^2(\theta_\ell^\sharp \mu, \theta_\ell^\sharp \nu)
$$

$\theta_\ell$ ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

**1æ¬¡å…ƒW2ã®é–‰å½¢å¼**:

$X = \{x_1, \ldots, x_n\}$, $Y = \{y_1, \ldots, y_m\}$ ã‚’ã‚½ãƒ¼ãƒˆã—ã€$n=m$ ãªã‚‰:

$$
W_2^2(X, Y) = \frac{1}{n} \sum_{i=1}^n (x_{(i)} - y_{(i)})^2
$$

ã“ã“ã§ $(i)$ ã¯ã‚½ãƒ¼ãƒˆå¾Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚

**Rustå®Ÿè£…**:

```rust
use ndarray::prelude::*;
use rand::Rng;
use rand_distr::{Normal, Distribution};

/// Sliced Wasserstein è·é›¢ (2ã¤ã®ç‚¹ç¾¤é–“)ã€‚
/// SWâ‚‚Â²(Î¼,Î½) = (1/L) Î£_â„“ Wâ‚‚Â²(Î¸_â„“â™¯Î¼, Î¸_â„“â™¯Î½)
///
/// # Arguments
/// - `x`:            ã‚½ãƒ¼ã‚¹ã‚µãƒ³ãƒ—ãƒ« (n, d)
/// - `y`:            ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚µãƒ³ãƒ—ãƒ« (m, d)
/// - `n_projections`: ãƒ©ãƒ³ãƒ€ãƒ å°„å½±ã®æ•° L
fn sliced_wasserstein(
    x:            &ArrayView2<f64>,
    y:            &ArrayView2<f64>,
    n_projections: usize,
) -> f64 {
    let (n, d) = x.dim();
    let (m, d2) = y.dim();
    assert_eq!(d, d2, "æ¬¡å…ƒãŒä¸€è‡´ã—ã¾ã›ã‚“");

    let mut rng  = rand::thread_rng();
    let normal   = Normal::new(0.0, 1.0).unwrap();

    // SWâ‚‚Â²(Î¼,Î½) â‰ˆ (1/L) Î£_â„“ Wâ‚‚Â²(Î¸_â„“â™¯Î¼, Î¸_â„“â™¯Î½)
    let sw2: f64 = (0..n_projections).map(|_| {
        // Î¸ ~ Uniform(S^{d-1})  (random unit direction)
        let theta_raw: Array1<f64> = Array1::from_iter((0..d).map(|_| normal.sample(&mut rng)));
        let theta = &theta_raw / theta_raw.dot(&theta_raw).sqrt();

        // Î¸â™¯Î¼: 1D projection x_proj_i = âŸ¨Î¸, x_iâŸ©
        let mut x_proj: Vec<f64> = x.rows().into_iter().map(|r| r.dot(&theta)).collect();
        let mut y_proj: Vec<f64> = y.rows().into_iter().map(|r| r.dot(&theta)).collect();
        // Wâ‚‚Â²(F_Î¼^{-1}, F_Î½^{-1}) = (1/n) Î£_i (x_{(i)} - y_{(i)})Â²  (sorted 1D Wâ‚‚)
        x_proj.sort_unstable_by(|a, b| a.partial_cmp(b).unwrap());
        y_proj.sort_unstable_by(|a, b| a.partial_cmp(b).unwrap());

        let len = n.max(m);
        let interp = |sorted: &[f64], i: usize| -> f64 {
            let t = i as f64 / (len - 1) as f64;
            let idx = (t * (sorted.len() - 1) as f64) as usize;
            sorted[idx.min(sorted.len() - 1)]
        };
        (0..len)
            .map(|i| (interp(&x_proj, i) - interp(&y_proj, i)).powi(2))
            .sum::<f64>() / len as f64
    }).sum();

    // SWâ‚‚ = sqrt(SWâ‚‚Â²)
    (sw2 / n_projections as f64).sqrt()
}

fn main() {
    let n = 100usize;
    let d = 10usize;
    let x = Array2::from_shape_fn((n, d), |(i, j)| (i * d + j) as f64 * 0.01);
    let y = Array2::from_shape_fn((n, d), |(i, j)| 1.0 + (i * d + j) as f64 * 0.01);

    let sw2 = sliced_wasserstein(&x.view(), &y.view(), 200);
    println!("Sliced Wâ‚‚: {sw2:.4}");
}
```

**è¨ˆç®—é‡æ¯”è¼ƒ**:

| æ‰‹æ³• | è¨ˆç®—é‡ | æ¬¡å…ƒä¾å­˜æ€§ |
|:-----|:------|:-----------|
| Sinkhorn | $O(n^2 \varepsilon^{-1})$ | $O(d)$ï¼ˆã‚³ã‚¹ãƒˆè¡Œåˆ—è¨ˆç®—ï¼‰ |
| Sliced Wasserstein | $O(Ln \log n)$ | $O(Ld)$ï¼ˆå°„å½±ï¼‰|
| çœŸã®W2ï¼ˆç·šå½¢è¨ˆç”»ï¼‰ | $O(n^3 \log n)$ | $O(d)$ |

$L=100$, $n=1000$, $d=50$ ã®ã¨ã: Sliced $\ll$ Sinkhorn $\ll$ ç·šå½¢è¨ˆç”»

### 6.4 Unbalanced OT & Partial OT â€” è³ªé‡ä¿å­˜ã®ç·©å’Œ

**å‹•æ©Ÿ**: å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã¯ $\int d\mu \neq \int d\nu$ï¼ˆç·è³ªé‡ãŒç•°ãªã‚‹ï¼‰ã“ã¨ãŒã‚ã‚‹ã€‚

**Unbalanced OT** [^11]: è³ªé‡ã®ç”Ÿæˆãƒ»æ¶ˆæ»…ã‚’è¨±ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’èª²ã™:

$$
\min_{\gamma, \mu', \nu'} \left\{ \int c \, d\gamma + \tau_1 D(\mu' \| \mu) + \tau_2 D(\nu' \| \nu) \right\}
$$

ã“ã“ã§ $D$ ã¯ç™ºæ•£ï¼ˆä¾‹: KL divergenceï¼‰ã€$\tau_1, \tau_2$ ã¯ãƒšãƒŠãƒ«ãƒ†ã‚£é‡ã¿ã€‚

**Partial OT**: ä¸€éƒ¨ã®è³ªé‡ã ã‘ã‚’è¼¸é€ï¼ˆoutlier robustnessï¼‰:

$$
\min_{\gamma} \left\{ \int c \, d\gamma \;\middle|\; \gamma \in \Pi(\mu, \nu), \; \gamma(\mathbb{R}^d \times \mathbb{R}^d) \leq \alpha \right\}
$$

$\alpha < 1$ ã§ã€Œå…¨ä½“ã® $\alpha$ å‰²ã ã‘è¼¸é€ã€ã€‚

**å¿œç”¨**: Domain adaptationï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã§ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã ã‘ãƒãƒƒãƒãƒ³ã‚°ï¼‰

### 6.5 Gromov-Wassersteinè·é›¢ â€” ç•°ãªã‚‹ç©ºé–“é–“ã®OT

**å•é¡Œ**: $\mu \in \mathcal{P}(X)$, $\nu \in \mathcal{P}(Y)$ ã§ $X, Y$ ãŒ **ç•°ãªã‚‹è¨ˆé‡ç©ºé–“** ã®ã¨ãã€$c(\boldsymbol{x}, \boldsymbol{y})$ ã‚’ã©ã†å®šç¾©ã™ã‚‹ï¼Ÿ

**Gromov-Wasserstein (GW)** [^12]: ç©ºé–“å†…ã®è·é›¢æ§‹é€ ã‚’æ¯”è¼ƒ:

$$
\text{GW}(\mu, \nu) = \inf_{\gamma \in \Pi(\mu, \nu)} \int_{X \times X \times Y \times Y} |d_X(\boldsymbol{x}, \boldsymbol{x}') - d_Y(\boldsymbol{y}, \boldsymbol{y}')|^2 \, d\gamma(\boldsymbol{x}, \boldsymbol{y}) \, d\gamma(\boldsymbol{x}', \boldsymbol{y}')
$$

ã€Œ$\boldsymbol{x}$ ã¨ $\boldsymbol{x}'$ ã®è·é›¢ã€ã¨ã€Œå¯¾å¿œã™ã‚‹ $\boldsymbol{y}, \boldsymbol{y}'$ ã®è·é›¢ã€ã®å·®ã‚’æœ€å°åŒ–ã€‚

**å¿œç”¨**: ã‚°ãƒ©ãƒ•ãƒãƒƒãƒãƒ³ã‚°ã€åˆ†å­ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’

**è¨ˆç®—**: Sinkhornã®æ‹¡å¼µï¼ˆGromov-Sinkhornï¼‰ãŒå¯èƒ½ã ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¯ $O(n^4)$ â†’ è¿‘ä¼¼æ‰‹æ³•ãŒå¿…è¦ã€‚

### 6.6 OTã¨Flow Matchingã®æ¥ç¶š â€” ç¬¬36å›ã¸ã®å¸ƒçŸ³

**Rectified Flow** [^4] ã®æ ¸å¿ƒ: ãƒã‚¤ã‚ºåˆ†å¸ƒ $\pi_0$ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $\pi_1$ ã¸ã® **æœ€çŸ­çµŒè·¯** ã‚’å­¦ç¿’ã€‚

$$
\frac{dx_t}{dt} = v_t(x_t), \quad x_0 \sim \pi_0, \; x_1 \sim \pi_1
$$

**OTã¨ã®é–¢ä¿‚**:
1. **Optimal coupling**: $\gamma^* \in \Pi(\pi_0, \pi_1)$ ãŒW2æœ€é©è§£
2. **ç›´ç·šè£œé–“**: $x_t = (1-t) x_0 + t x_1$ where $(x_0, x_1) \sim \gamma^*$
3. **é€Ÿåº¦å ´**: $v_t(x_t) = x_1 - x_0 = \mathbb{E}[(x_1 - x_0) \mid x_t]$

ã“ã‚ŒãŒ **OT-CFM** (Optimal Transport Conditional Flow Matching) [^13] ã®å®šå¼åŒ–ã ã€‚

**Rectified Flow = OT Map**:

2å›ã®rectificationï¼ˆå†è¨“ç·´ï¼‰ã«ã‚ˆã‚Šã€ãƒ•ãƒ­ãƒ¼ãŒã€Œç›´ç·šåŒ–ã€ã•ã‚Œã‚‹:

$$
\lim_{k \to \infty} \mathbb{E}[\text{æ›²ç‡}] \to 0
$$

ã“ã‚Œã¯Wassersteinæ¸¬åœ°ç·šï¼ˆMcCannè£œé–“ï¼‰ã¸ã®åæŸã‚’æ„å‘³ã™ã‚‹ã€‚

**ç¬¬36å›ã§ã®å±•é–‹**:
- Diffusion ODE = Wassersteinå‹¾é…æµã®é›¢æ•£åŒ–
- Score Matching = W2ã®å‹¾é…ã‚’å­¦ç¿’
- Flow Matching = W2æ¸¬åœ°ç·šã‚’ç›´æ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–
- 3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®çµ±ä¸€çš„ç†è§£

<details><summary>æŠ€è¡“è©³ç´°: OT-CFMã®æ¡ä»¶ä»˜ãç¢ºç‡ãƒ‘ã‚¹</summary>

OT-CFMã¯æ¡ä»¶ä»˜ãç¢ºç‡ãƒ‘ã‚¹ $p_t(x \mid x_0, x_1)$ ã‚’ä½¿ã†:

$$
p_t(x) = \int_{\mathbb{R}^d \times \mathbb{R}^d} p_t(x \mid x_0, x_1) \, d\gamma(x_0, x_1)
$$

ç›´ç·šè£œé–“ã®å ´åˆ:

$$
p_t(x \mid x_0, x_1) = \delta(x - ((1-t) x_0 + t x_1))
$$

é€Ÿåº¦å ´:

$$
u_t(x) = \int \frac{dx_t}{dt} \, p_t(x_t \mid x_0, x_1) \frac{p_t(x_t \mid x_0, x_1)}{p_t(x_t)} \, d\gamma(x_0, x_1)
$$

ç°¡ç•¥åŒ–ã™ã‚‹ã¨:

$$
u_t(x) = \mathbb{E}_{(x_0, x_1) \sim \gamma \mid x_t = x} [x_1 - x_0]
$$

ã“ã‚Œã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ $v_\theta(x, t)$ ã§è¿‘ä¼¼ã—ã€Flow Matchingæå¤±ã§è¨“ç·´:

$$
\mathcal{L}(\theta) = \mathbb{E}_{t, (x_0, x_1) \sim \gamma, x_t} [\| v_\theta(x_t, t) - (x_1 - x_0) \|^2]
$$

</details>


## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.8 ç”¨èªé›†ï¼ˆGlossaryï¼‰

| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| æœ€é©è¼¸é€ | Optimal Transport | ç¢ºç‡æ¸¬åº¦é–“ã®æœ€å°ã‚³ã‚¹ãƒˆè¼¸é€å•é¡Œ |
| Mongeå•é¡Œ | Monge Problem | æ±ºå®šè«–çš„è¼¸é€å†™åƒã‚’æ±‚ã‚ã‚‹å…ƒã®å®šå¼åŒ– |
| Kantorovichç·©å’Œ | Kantorovich Relaxation | ç¢ºç‡çš„è¼¸é€è¨ˆç”»ã‚’è¨±ã™ç·©å’Œ |
| Wassersteinè·é›¢ | Wasserstein Distance | OTã‚³ã‚¹ãƒˆã«ã‚ˆã‚‹ç¢ºç‡æ¸¬åº¦é–“ã®è·é›¢ |
| åŒå¯¾æ€§ | Duality | ä¸»å•é¡Œã¨åŒå¯¾å•é¡Œã®ç­‰ä¾¡æ€§ |
| Sinkhornç®—æ³• | Sinkhorn Algorithm | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–OTã®é«˜é€Ÿè§£æ³• |
| Gibbsã‚«ãƒ¼ãƒãƒ« | Gibbs Kernel | $K = \exp(-C/\varepsilon)$ |
| Push-forwardæ¸¬åº¦ | Push-forward Measure | å†™åƒã«ã‚ˆã‚‹æ¸¬åº¦ã®å¤‰æ› |
| çµåˆæ¸¬åº¦ | Coupling | 2ã¤ã®å‘¨è¾ºåˆ†å¸ƒã‚’æŒã¤çµåˆåˆ†å¸ƒ |
| McCannè£œé–“ | McCann Interpolation | Wassersteinæ¸¬åœ°ç·š |
| Displacement Convexity | å¤‰ä½å‡¸æ€§ | Wassersteinç©ºé–“ã§ã®å‡¸æ€§ |
| JKO scheme | Jordan-Kinderlehrer-Otto | Wassersteinå‹¾é…æµã®é›¢æ•£åŒ– |

---

### 6.9 ä»Šå›ã®å­¦ç¿’å†…å®¹

**3ã¤ã®æ ¸å¿ƒ**:

1. **Kantorovichç·©å’Œ**: Mongeã®æ±ºå®šè«–çš„è¼¸é€ â†’ ç¢ºç‡çš„è¼¸é€è¨ˆç”» $\gamma \in \Pi(\mu, \nu)$ ã¸ã®ç·©å’Œã«ã‚ˆã‚Šã€ç·šå½¢è¨ˆç”»å•é¡Œã¨ã—ã¦å®šå¼åŒ–å¯èƒ½ã«

2. **Wassersteinè·é›¢**: ç¢ºç‡æ¸¬åº¦ç©ºé–“ $(\mathcal{P}_2(\mathbb{R}^d), W_2)$ ã¯è·é›¢ç©ºé–“ã§ã‚ã‚Šã€å¼±åæŸã‚’ãƒ¡ãƒˆãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€‚KL divergenceã§ã¯æ‰ãˆã‚‰ã‚Œãªã„ã€Œåˆ†å¸ƒã®å¹¾ä½•å­¦ã€ã‚’è¡¨ç¾

3. **Sinkhornç®—æ³•**: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ– $-\varepsilon H(\gamma)$ ã«ã‚ˆã‚Šã€è¨ˆç®—é‡ã‚’ $O(n^3) \to O(n^2 \varepsilon^{-1})$ ã«å‰Šæ¸›ã€‚æ©Ÿæ¢°å­¦ç¿’ã§ã®å®Ÿç”¨åŒ–ã®éµ

**å®Ÿè£…ã§å­¦ã‚“ã ã“ã¨**:

- **Rust**: ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–ã¨è¡Œåˆ—æ¼”ç®—ã®è¦ªå’Œæ€§ã«ã‚ˆã‚Šã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ãŒ1:1å¯¾å¿œ
- **Rust**: ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–ã¨Rayonä¸¦åˆ—åŒ–ã«ã‚ˆã‚Šã€ãƒãƒƒãƒå‡¦ç†ã§5xé«˜é€ŸåŒ–
- **ICNN**: å‡¸æ€§åˆ¶ç´„ã«ã‚ˆã‚ŠMonge Mapã®æ§‹é€ ã‚’ç›´æ¥å­¦ç¿’å¯èƒ½

**ç†è«–ã¨å¿œç”¨ã®ã¤ãªãŒã‚Š**:

```mermaid
graph TD
    OT[Optimal Transport<br/>Theory] --> W1[Wâ‚ è·é›¢]
    OT --> W2[Wâ‚‚ è·é›¢]
    W1 --> WGAN[WGAN<br/>Lipschitzåˆ¶ç´„]
    W2 --> FM[Flow Matching<br/>æ¸¬åœ°ç·šå­¦ç¿’]
    W2 --> Diff[Diffusion<br/>å‹¾é…æµ]
    OT --> Sink[Sinkhorn<br/>é«˜é€Ÿè¨ˆç®—]
    Sink --> DA[Domain Adaptation]
    Sink --> Barycenter[Barycenterå•é¡Œ]

    style OT fill:#ffeb3b
    style WGAN fill:#e1f5fe
    style FM fill:#c8e6c9
```

### 6.10 FAQ â€” ã‚ˆãã‚ã‚‹è³ªå•ã¨ç­”ãˆ

<details><summary>Q1: ãªãœKL divergenceã§ã¯ãªãWassersteinè·é›¢ã‚’ä½¿ã†ã®ã‹ï¼Ÿ</summary>

**A**: KL divergenceã¯ã‚µãƒãƒ¼ãƒˆãŒé‡ãªã‚‰ãªã„ã¨ $+\infty$ ã«ãªã‚‹ã€‚ä¾‹ãˆã°:
- $\mu = \delta_0$ï¼ˆç‚¹è³ªé‡ï¼‰ã€$\nu = \delta_1$ ã®ã¨ãã€$D_{\text{KL}}(\mu \| \nu) = +\infty$
- ä¸€æ–¹ã€$W_2(\mu, \nu) = 1$ï¼ˆæœ‰é™ï¼‰

Wassersteinè·é›¢ã¯:
1. **å¼±åæŸã‚’ãƒ¡ãƒˆãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³**: åˆ†å¸ƒã®ã€Œè¿‘ã•ã€ã‚’ä½ç›¸çš„ã«æ­£ã—ãæ¸¬ã‚‹
2. **å‹¾é…ãŒå¸¸ã«å­˜åœ¨**: KLã ã¨å‹¾é…ãŒ $\infty$ ã«ãªã‚‹çŠ¶æ³ã§ã‚‚ã€W2ã¯æœ‰é™å‹¾é…
3. **å¹¾ä½•å­¦çš„ç›´æ„Ÿ**: ã€ŒåœŸã‚’å‹•ã‹ã™æœ€å°ã‚³ã‚¹ãƒˆã€ã¨ã„ã†ç‰©ç†çš„è§£é‡ˆ

GANã§ã¯ã“ã‚ŒãŒè‡´å‘½çš„ã§ã€ã‚µãƒãƒ¼ãƒˆãŒé›¢ã‚ŒãŸåˆæœŸæ®µéšã§KLãƒ™ãƒ¼ã‚¹ã®æå¤±ã¯å­¦ç¿’ãŒé€²ã¾ãªã„ã€‚WGANãŒã“ã‚Œã‚’è§£æ±ºã—ãŸã€‚

</details>

<details><summary>Q2: Sinkhornã®$\varepsilon$ã¯ã©ã†é¸ã¶ã¹ãã‹ï¼Ÿ</summary>

**A**: ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚‹:

| $\varepsilon$ | ç²¾åº¦ | é€Ÿåº¦ | æ•°å€¤å®‰å®šæ€§ |
|:-------------|:-----|:-----|:-----------|
| å°ï¼ˆ0.001-0.01ï¼‰ | é«˜ï¼ˆçœŸã®OTã«è¿‘ã„ï¼‰ | é…ï¼ˆåå¾©æ•°å¤šï¼‰ | ä¸å®‰å®šï¼ˆlog-domainå¿…é ˆï¼‰ |
| ä¸­ï¼ˆ0.05-0.1ï¼‰ | ä¸­ | é€Ÿ | å®‰å®š |
| å¤§ï¼ˆ0.5-1.0ï¼‰ | ä½ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …æ”¯é…ï¼‰ | éå¸¸ã«é€Ÿ | éå¸¸ã«å®‰å®š |

**æ¨å¥¨**:
- **å­¦ç¿’ä¸­**: $\varepsilon = 0.05 \sim 0.1$ï¼ˆé€Ÿåº¦ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
- **è©•ä¾¡æ™‚**: $\varepsilon = 0.01$ï¼ˆã‚ˆã‚Šæ­£ç¢ºãªW2æ¨å®šï¼‰
- **ä¸å®‰å®šãªã‚‰**: log-domainã«åˆ‡ã‚Šæ›¿ãˆ + $\varepsilon$ ã‚’å¤§ããã™ã‚‹

**è‡ªå‹•èª¿æ•´**: Annealing â€” å­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚Œ $\varepsilon$ ã‚’æ¸›ã‚‰ã™ï¼ˆ $\varepsilon_t = \varepsilon_0 \cdot 0.99^t$ ãªã©ï¼‰

</details>

<details><summary>Q3: ICNNã¯ãªãœå‡¸é–¢æ•°ã§ãªã„ã¨ã„ã‘ãªã„ã®ã‹ï¼Ÿ</summary>

**A**: Brenierå®šç† [^2] ã«ã‚ˆã‚‹:

> $\mu, \nu$ ãŒ $\mathbb{R}^d$ ä¸Šã®çµ¶å¯¾é€£ç¶šãªç¢ºç‡æ¸¬åº¦ãªã‚‰ã€W2æœ€é©è¼¸é€å†™åƒ $T^*$ ã¯ä¸€æ„ã«å­˜åœ¨ã—ã€$T^* = \nabla \phi$ ã®å½¢ã‚’æŒã¤ã€‚ã“ã“ã§ $\phi$ ã¯å‡¸é–¢æ•°ã€‚

ã¤ã¾ã‚Š:
- æœ€é©è¼¸é€å†™åƒã¯ã€Œå‡¸é–¢æ•°ã®å‹¾é…ã€ã¨ã—ã¦å¿…ãšæ›¸ã‘ã‚‹
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ $\phi$ ã‚’å­¦ç¿’ â†’ ãã®å‹¾é… $\nabla \phi$ ãŒè¼¸é€å†™åƒ

**å‡¸æ€§ã‚’ä¿è¨¼ã—ãªã„ã¨**: $\nabla \phi$ ãŒæœ€é©è¼¸é€å†™åƒã«ãªã‚‰ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã€ç†è«–çš„ä¿è¨¼ãŒå¤±ã‚ã‚Œã‚‹ã€‚

**å®Ÿè£…ã®å·¥å¤«**: é‡ã¿ã‚’éè² ã«åˆ¶ç´„ï¼ˆsoftplusé©ç”¨ï¼‰+ å‡¸æ´»æ€§åŒ–é–¢æ•°ï¼ˆReLUï¼‰ã§æ§‹æˆçš„ã«å‡¸æ€§ã‚’ä¿è¨¼ã€‚

</details>

<details><summary>Q4: Flow Matchingã¨OTã¯ã©ã†é•ã†ã®ã‹ï¼Ÿ</summary>

**A**: Flow Matchingã¯ã€ŒOTã‚’åˆ©ç”¨ã—ãŸç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´æ‰‹æ³•ã€:

| é …ç›® | Optimal Transport | Flow Matching |
|:-----|:------------------|:--------------|
| ç›®çš„ | 2ã¤ã®åˆ†å¸ƒé–“ã®æœ€å°ã‚³ã‚¹ãƒˆè¼¸é€ | ãƒã‚¤ã‚ºâ†’ãƒ‡ãƒ¼ã‚¿ã¸ã®é€£ç¶šå†™åƒã‚’å­¦ç¿’ |
| å…¥åŠ› | ç¢ºç‡æ¸¬åº¦ $\mu, \nu$ | ã‚µãƒ³ãƒ—ãƒ« $x_0 \sim \pi_0, x_1 \sim \pi_{\text{data}}$ |
| å‡ºåŠ› | è¼¸é€è¨ˆç”» $\gamma^*$ ã¾ãŸã¯å†™åƒ $T^*$ | é€Ÿåº¦å ´ $v_\theta(x, t)$ |
| é–¢ä¿‚ | ç†è«–çš„åŸºç›¤ | å¿œç”¨æ‰‹æ³• |

**OT-CFM**: OTæœ€é©è¼¸é€è¨ˆç”» $\gamma^*$ ã‚’ä½¿ã£ã¦Flow Matchingã®æ¡ä»¶ä»˜ããƒ‘ã‚¹ã‚’æ§‹ç¯‰ â†’ ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå­¦ç¿’

**Rectified Flow**: OTå†™åƒãŒã€Œç›´ç·šçš„ã€ã§ã‚ã‚‹ã“ã¨ã‚’åˆ©ç”¨ã—ã€ãƒ•ãƒ­ãƒ¼ã‚’ç›´ç·šåŒ– â†’ æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—å‰Šæ¸›

è©³ç´°ã¯ **ç¬¬36å› Flow Matchingçµ±ä¸€ç†è«–** ã§å±•é–‹ã™ã‚‹ã€‚

</details>

<details><summary>Q5: Rust vs Rustã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ</summary>

**A**: ã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹:

| ã‚¿ã‚¹ã‚¯ | æ¨å¥¨è¨€èª | ç†ç”± |
|:-------|:---------|:-----|
| ç ”ç©¶ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚° | Rust | REPLé§†å‹•é–‹ç™ºã€æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1ã€é«˜é€Ÿ |
| æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆå˜ä½“ï¼‰ | Rust | ãƒ¡ãƒ¢ãƒªå®‰å…¨ã€ãƒã‚¤ãƒŠãƒªé…å¸ƒã€ã‚¼ãƒ­GC |
| æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆPythonçµ±åˆï¼‰ | Rust | PyCall/PythonCallã§ç°¡å˜é€£æº |
| å¤§è¦æ¨¡ãƒãƒƒãƒå‡¦ç† | Rust | Rayonä¸¦åˆ—åŒ–ã€SIMDæœ€é©åŒ– |
| GPUè¨ˆç®— | Rust (CUDA.jl) | Python (JAX/PyTorch) ã‚ˆã‚Šç›´æ„Ÿçš„ |

**æœ¬è¬›ç¾©ã®é¸æŠ**:
- **ä¸»è»¸ã¯Rust**: OTç†è«–ã®æ•°å¼ãŒç›´æ¥ã‚³ãƒ¼ãƒ‰ã«ãªã‚‹ç¾ã—ã•
- **Rustã¯è£œå®Œ**: æ€§èƒ½ãŒæœ¬å½“ã«å¿…è¦ãªéƒ¨åˆ†ã®ã¿ï¼ˆSinkhorn SIMDã€C-ABI FFIï¼‰

**å®Ÿå‹™ã§ã®æ£²ã¿åˆ†ã‘**: Rustï¼ˆã‚«ãƒ¼ãƒãƒ«å®Ÿè£…ï¼‰ + Pythonï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼APIï¼‰ + Rustï¼ˆé«˜é€Ÿãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼‰ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆãŒç†æƒ³ã€‚

</details>

### 6.13 æ¬¡å›äºˆå‘Š: ç¬¬12å› GAN â€” æ•µå¯¾çš„ç”Ÿæˆã®ç†è«–

**Lecture 12ã®ãƒ†ãƒ¼ãƒ**: Generative Adversarial Networksï¼ˆGANï¼‰ã®å®Œå…¨ç†è«–

**å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**:
1. **GANå®šå¼åŒ–**: Minmaxã‚²ãƒ¼ãƒ ã€Jensen-Shannon divergenceã€Nashå‡è¡¡
2. **ç†è«–çš„å›°é›£**: ãƒ¢ãƒ¼ãƒ‰å´©å£Šã€å‹¾é…æ¶ˆå¤±ã€è¨“ç·´ä¸å®‰å®šæ€§ã®æ•°ç†
3. **WGAN**: æœ¬è¬›ç¾©ã§å­¦ã‚“ã Kantorovich-RubinsteinåŒå¯¾æ€§ãŒã„ã‹ã«GANã‚’å®‰å®šåŒ–ã™ã‚‹ã‹
4. **ç™ºå±•å‹**: StyleGANã€Progressive GANã€Diffusion-GANãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
5. **å®Ÿè£…**: Rustã§minimalãªGAN + Rustã§WGANé«˜é€ŸåŒ–

**æœ¬è¬›ç¾©ã¨ã®æ¥ç¶š**:
- WGANã® **1-Lipschitzåˆ¶ç´„** = Kantorovich-RubinsteinåŒå¯¾æ€§ï¼ˆÂ§3.4ï¼‰
- **Gradient penalty** = $\mathbb{E}[(\|\nabla_{\boldsymbol{x}} D\| - 1)^2]$ ã®ç†è«–çš„æ­£å½“åŒ–
- **Spectral normalization** = Lipschitzå®šæ•°ã®åˆ¶å¾¡æ‰‹æ³•

**æº–å‚™ã™ã¹ãã“ã¨**:
- ç¬¬6å›ã€Œæƒ…å ±ç†è«–ã€ã®Jensen-Shannon divergenceå¾©ç¿’
- ç¬¬7å›ã€Œæœ€å°¤æ¨å®šã€ã®MLEã¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é–¢ä¿‚ç¢ºèª
- æœ¬è¬›ç¾©ï¼ˆç¬¬11å›ï¼‰ã®Â§3.4 Kantorovich-RubinsteinåŒå¯¾æ€§ã‚’å®Œå…¨ç†è§£

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ ãŠç–²ã‚Œã•ã¾ã§ã—ãŸï¼ 240å¹´ã®æ­´å²ã‚’æŒã¤æœ€é©è¼¸é€ç†è«–ã‚’ã€Mongeå•é¡Œã‹ã‚‰æœ€æ–°ã®Flow Matchingã¸ã®æ¥ç¶šã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ã¾ã—ãŸã€‚æ¬¡å›ã®GANã§ã€ã“ã®ç†è«–ãŒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å®Ÿè·µã§ã©ã†æ´»ãã‚‹ã‹ã‚’ç›®æ’ƒã—ã¾ã™ã€‚

---

### 6.14 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œæœ€é©è¼¸é€ç†è«–ã¯ã€ç¢ºç‡åˆ†å¸ƒã‚’ã€ç‚¹ã€ã§ã¯ãªãã€å¹¾ä½•å­¦ã€ã¨ã—ã¦æ‰±ã†ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã ã€‚ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿åˆ†å¸ƒã‚‚Wassersteinç©ºé–“ã®1ç‚¹ã¨è¦‹ãªã›ã°ã€ãƒ¢ãƒ‡ãƒ«ã®ã€æ±åŒ–èª¤å·®ã€ã‚’OTè·é›¢ã§æ¸¬å®šã§ãã‚‹ã®ã§ã¯ãªã„ã‹ï¼Ÿã€**

**æŒ‘ç™ºçš„ãªå•ã„ã‹ã‘**:

1. **ãƒ¢ãƒ‡ãƒ«ç©ºé–“ã®å¹¾ä½•å­¦**: 2ã¤ã®NNãƒ¢ãƒ‡ãƒ« $\theta_1, \theta_2$ ãŒåŒã˜ã‚¿ã‚¹ã‚¯ã‚’è§£ãã¨ãã€ãã®ã€Œè¿‘ã•ã€ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ $\|\theta_1 - \theta_2\|$ ã§æ¸¬ã‚‹ã®ã¯é©åˆ‡ã‹ï¼Ÿ ãã‚Œã¨ã‚‚ã€ä¸¡è€…ãŒèª˜å°ã™ã‚‹ **å‡ºåŠ›åˆ†å¸ƒé–“ã®Wassersteinè·é›¢** $W_2(p_{\theta_1}, p_{\theta_2})$ ã§æ¸¬ã‚‹ã¹ãã‹ï¼Ÿ

2. **æ±åŒ–ã®æ–°å®šç¾©**: è¨“ç·´åˆ†å¸ƒ $p_{\text{train}}$ ã¨ãƒ†ã‚¹ãƒˆåˆ†å¸ƒ $p_{\text{test}}$ ã®ã€Œãšã‚Œã€ã‚’ $W_2(p_{\text{train}}, p_{\text{test}})$ ã§å®šé‡åŒ–ã™ã‚Œã°ã€ã€Œæ±åŒ–èª¤å·® = OTè·é›¢ã®é–¢æ•°ã€ã¨ã„ã†ç†è«–ã‚’æ§‹ç¯‰ã§ãã‚‹ã‹ï¼Ÿ æ—¢å­˜ã®PAC learningã‚„VCæ¬¡å…ƒç†è«–ã‚’è¶…ãˆã‚‰ã‚Œã‚‹ã‹ï¼Ÿ

3. **é€£ç¶šå­¦ç¿’ = æ¸¬åœ°ç·š**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®è¨“ç·´éç¨‹ $\{\theta_t\}_{t=0}^T$ ã‚’ã€å‡ºåŠ›åˆ†å¸ƒç©ºé–“ $\{p_{\theta_t}\}$ ã§ã®Wassersteinæ¸¬åœ°ç·šã¨ã—ã¦è¦‹ç›´ã›ã°ã€ã€Œæœ€é©ãªå­¦ç¿’çµŒè·¯ã€ã‚’äº‹å‰è¨ˆç®—ã§ãã‚‹ã‹ï¼Ÿ ã¤ã¾ã‚Šã€å‹¾é…é™ä¸‹æ³•ã¯ **Wassersteinå‹¾é…æµã®é›¢æ•£åŒ–** ã¨ã—ã¦å†è§£é‡ˆã§ãã‚‹ã‹ï¼Ÿ

**æ­´å²çš„é€†èª¬**:

- 1781å¹´ã€Mongeã¯ã€ŒåœŸã‚’é‹ã¶ã€ã¨ã„ã†åœŸæœ¨å·¥å­¦ã®å•é¡Œã‚’è§£ã“ã†ã¨ã—ãŸ
- 2017å¹´ã€WGANã¯ã€Œãƒ”ã‚¯ã‚»ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€ã¨ã„ã†å•é¡Œã«OTã‚’é©ç”¨ã—ãŸ
- 2022å¹´ã€Rectified Flowã¯ã€Œãƒã‚¤ã‚ºã‚’ç”»åƒã«å¤‰æ›ã™ã‚‹ã€çµŒè·¯ã‚’OTã§æœ€é©åŒ–ã—ãŸ
- 2026å¹´ã€æ¬¡ã®å¿œç”¨ã¯ **ã€Œå­¦ç¿’ãã®ã‚‚ã®ã‚’OTã§æœ€é©åŒ–ã™ã‚‹ã€** ã“ã¨ã‹ã‚‚ã—ã‚Œãªã„

**ã‚ãªãŸã¸ã®å•ã„**:

ã‚‚ã—ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®è¨“ç·´ãŒã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®æœ€é©åŒ–ã€ã§ã¯ãªãã€Œåˆ†å¸ƒç©ºé–“ã®Wassersteinæ¸¬åœ°ç·šã‚’è¾¿ã‚‹éç¨‹ã€ã ã¨ã—ãŸã‚‰ã€ç¾åœ¨ã®Adamã‚„SGDã¯ **æœ€é©è¼¸é€çš„ã«æœ€é©** ãªã®ã‹ï¼Ÿ ãã‚Œã¨ã‚‚ã€ã‚‚ã£ã¨ã€Œç›´ç·šçš„ãªã€å­¦ç¿’çµŒè·¯ãŒå­˜åœ¨ã™ã‚‹ã®ã‹ï¼Ÿ

<details><summary>ãƒ’ãƒ³ãƒˆ: Neural Tangent Kernel (NTK) ã¨ã®é–¢ä¿‚</summary>

NTKç†è«–ã§ã¯ã€ç„¡é™å¹…NNã®è¨“ç·´ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¯ã‚«ãƒ¼ãƒãƒ«å›å¸°ã¨ã—ã¦è§£æã•ã‚Œã‚‹ã€‚ä¸€æ–¹ã€OTè¦–ç‚¹ã§ã¯è¨“ç·´ã¯ã€ŒåˆæœŸåˆ†å¸ƒ $p_{\theta_0}$ ã‹ã‚‰æœ€é©åˆ†å¸ƒ $p_{\theta^*}$ ã¸ã®è¼¸é€ã€ã¨è¦‹ãªã›ã‚‹ã€‚

2ã¤ã®è¦–ç‚¹ã‚’çµ±åˆã™ã‚‹ã¨:
- NTK = ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®å±€æ‰€çš„å¹¾ä½•å­¦
- OT = å‡ºåŠ›åˆ†å¸ƒç©ºé–“ã®å¤§åŸŸçš„å¹¾ä½•å­¦

ä¸¡è€…ã‚’æ©‹æ¸¡ã—ã™ã‚‹ç†è«–ï¼ˆä¾‹: "Wasserstein Proximal Gradient" ã‚„ "Optimal Transport for Meta-Learning"ï¼‰ãŒ2024-2025å¹´ã«ç™»å ´ã—ã¤ã¤ã‚ã‚‹ã€‚ç¬¬25å›ã€Œãƒ¡ã‚¿å­¦ç¿’ã€ã§ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’å†è¨ªã™ã‚‹ã€‚

</details>

---

> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Neural OTï¼ˆICNN: Input-Convex Neural Networkï¼‰ãŒMonge Mapã‚’æ¨å®šã§ãã‚‹ç†ç”±ã‚’ã€ã€Œå‡¸é–¢æ•°ã®å‹¾é…ï¼æœ€é©è¼¸é€å†™åƒï¼ˆBrenierå®šç†ï¼‰ã€ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚
> 2. Sliced Wassersteinè·é›¢ $\text{SW}_p(\mu, \nu) = \int_{S^{d-1}} W_p(\mathcal{P}_\theta \# \mu, \mathcal{P}_\theta \# \nu) d\sigma(\theta)$ ãŒé«˜æ¬¡å…ƒOTã®ä»£æ›¿ã¨ã—ã¦æœ‰åŠ¹ãªç†ç”±ã¨ã€ãã®è¨ˆç®—é‡ $O(n \log n)$ ã®æ ¹æ‹ ã‚’è¿°ã¹ã‚ˆã€‚

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Monge, G. (1781). *MÃ©moire sur la thÃ©orie des dÃ©blais et des remblais*. Histoire de l'AcadÃ©mie Royale des Sciences de Paris.

[^2]: Brenier, Y. (1991). *Polar factorization and monotone rearrangement of vector-valued functions*. Communications on Pure and Applied Mathematics, 44(4), 375-417.
<https://doi.org/10.1002/cpa.3160440402>

[^3]: Arjovsky, M., Chintala, S., & Bottou, L. (2017). *Wasserstein GAN*. ICML 2017.
<https://arxiv.org/abs/1701.07875>

[^4]: Liu, X., Gong, C., & Liu, Q. (2022). *Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow*. ICLR 2023.
<https://arxiv.org/abs/2209.03003>

[^5]: Jordan, R., Kinderlehrer, D., & Otto, F. (1998). *The variational formulation of the Fokkerâ€“Planck equation*. SIAM Journal on Mathematical Analysis, 29(1), 1-17.

[^6]: Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. (2017). *Improved Training of Wasserstein GANs*. NeurIPS 2017.
<https://arxiv.org/abs/1704.00028>

[^7]: Miyato, T., Kataoka, T., Koyama, M., & Yoshida, Y. (2018). *Spectral Normalization for Generative Adversarial Networks*. ICLR 2018.
<https://arxiv.org/abs/1802.05957>

[^8]: Makkuva, A., Taghvaei, A., Oh, S., & Lee, J. (2019). *Optimal transport mapping via input convex neural networks*. ICML 2020.
<https://arxiv.org/abs/1908.10962>

[^9]: Cuturi, M. (2013). *Sinkhorn Distances: Lightspeed Computation of Optimal Transport*. NeurIPS 2013.
<https://arxiv.org/abs/1306.0895>

[^10]: Bonneel, N., Rabin, J., PeyrÃ©, G., & Pfister, H. (2015). *Sliced and Radon Wasserstein barycenters of measures*. Journal of Mathematical Imaging and Vision, 51(1), 22-45.
<https://arxiv.org/abs/1503.01452>

[^11]: Chizat, L., PeyrÃ©, G., Schmitzer, B., & Vialard, F.-X. (2018). *Scaling algorithms for unbalanced optimal transport problems*. Mathematics of Computation, 87(314), 2563-2609.
<https://arxiv.org/abs/1607.05816>

[^12]: MÃ©moli, F. (2011). *Gromovâ€“Wasserstein distances and the metric approach to object matching*. Foundations of Computational Mathematics, 11(4), 417-487.

[^13]: Tong, A., Malkin, N., Fatras, K., Atanackovic, L., Zhang, Y., Huguet, G., Wolf, G., & Bengio, Y. (2023). *Improving and generalizing flow-based generative models with minibatch optimal transport*. TMLR 2024.
<https://arxiv.org/abs/2302.00482>

### æ•™ç§‘æ›¸

- Villani, C. (2003). *Topics in Optimal Transportation*. American Mathematical Society.
- Santambrogio, F. (2015). *Optimal Transport for Applied Mathematicians*. BirkhÃ¤user.
- Ambrosio, L., Gigli, N., & SavarÃ©, G. (2008). *Gradient Flows in Metric Spaces and in the Space of Probability Measures*. BirkhÃ¤user.

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
