---
title: "ç¬¬11å›: æœ€é©è¼¸é€ç†è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸš›"
type: "tech"
topics: ["machinelearning", "deeplearning", "optimaltransport", "julia", "rust"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Julia + Rust ã§OTã‚’å®Ÿè£…ã™ã‚‹

### 4.1 ç’°å¢ƒæ§‹ç¯‰

#### 4.1.1 Juliaç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Julia 1.11+ ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆ2026å¹´ç¾åœ¨ã®å®‰å®šç‰ˆï¼‰
# https://julialang.org/downloads/

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
julia -e 'using Pkg; Pkg.add(["Distributions", "LinearAlgebra", "Plots", "JuMP", "HiGHS", "BenchmarkTools", "Lux", "Optimisers", "Zygote"])'
```

**ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å½¹å‰²**:

| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | ç”¨é€” |
|:----------|:-----|
| `Distributions` | ç¢ºç‡åˆ†å¸ƒã®æ“ä½œ |
| `LinearAlgebra` | è¡Œåˆ—æ¼”ç®—ã€SVDã€ãƒãƒ«ãƒ  |
| `Plots` | å¯è¦–åŒ– |
| `JuMP` | æ•°ç†æœ€é©åŒ–ï¼ˆç·šå½¢è¨ˆç”»æ³•ï¼‰ |
| `HiGHS` | ç·šå½¢è¨ˆç”»ã‚½ãƒ«ãƒãƒ¼ |
| `BenchmarkTools` | ç²¾å¯†ãªæ™‚é–“è¨ˆæ¸¬ |
| `Lux` | ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆï¼ˆJAXé¢¨ï¼‰ |
| `Optimisers` | æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
| `Zygote` | è‡ªå‹•å¾®åˆ† |

#### 4.1.2 Rustç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Rust 1.80+ ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
cargo new ot_rust --lib
cd ot_rust

# Cargo.tomlã«ä¾å­˜é–¢ä¿‚ã‚’è¿½åŠ 
```

```toml
[dependencies]
ndarray = "0.16"
ndarray-linalg = { version = "0.17", features = ["openblas-static"] }
rayon = "1.10"
```

**ä¾å­˜é–¢ä¿‚ã®å½¹å‰²**:

| Crate | ç”¨é€” |
|:------|:-----|
| `ndarray` | å¤šæ¬¡å…ƒé…åˆ—ï¼ˆNumPyé¢¨ï¼‰ |
| `ndarray-linalg` | ç·šå½¢ä»£æ•°æ¼”ç®— |
| `rayon` | ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å‡¦ç† |

### 4.2 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆOTç‰¹åŒ–ï¼‰

**Pattern 1: Wassersteinè·é›¢ã®è¨ˆç®—ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰**

æ•°å¼:
$$W_2^2(\mathcal{N}(\boldsymbol{m}_0, \Sigma_0), \mathcal{N}(\boldsymbol{m}_1, \Sigma_1)) = \|\boldsymbol{m}_1 - \boldsymbol{m}_0\|^2 + \text{tr}(\Sigma_0 + \Sigma_1 - 2(\Sigma_1^{1/2} \Sigma_0 \Sigma_1^{1/2})^{1/2})$$

Julia:
```julia
using LinearAlgebra

function wasserstein2_gaussian(m0, Î£0, m1, Î£1)
    # Location term: ||m1 - m0||Â²
    loc = norm(m1 - m0)^2

    # Covariance term: tr(Î£0 + Î£1 - 2(Î£1^Â½ Î£0 Î£1^Â½)^Â½)
    Î£1_sqrt = sqrt(Î£1)
    M = Î£1_sqrt * Î£0 * Î£1_sqrt
    M_sqrt = sqrt(M)
    cov = tr(Î£0) + tr(Î£1) - 2 * tr(M_sqrt)

    return sqrt(loc + cov)
end
```

**Pattern 2: Gibbsã‚«ãƒ¼ãƒãƒ«ã®è¨ˆç®—**

æ•°å¼: $K_{ij} = \exp(-C_{ij} / \varepsilon)$

Juliaï¼ˆãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆï¼‰:
```julia
K = exp.(-C / Îµ)  # element-wise exponential
```

Rustï¼ˆè¦ç´ ã”ã¨ï¼‰:
```rust
use ndarray::{Array2, Zip};

fn gibbs_kernel(cost: &Array2<f64>, epsilon: f64) -> Array2<f64> {
    cost.mapv(|c| (-c / epsilon).exp())
}
```

**Pattern 3: å‘¨è¾ºåˆ†å¸ƒã®ç¢ºèª**

æ•°å¼: $\sum_j \gamma_{ij} = p_i$ï¼ˆè¡Œå’Œï¼‰, $\sum_i \gamma_{ij} = q_j$ï¼ˆåˆ—å’Œï¼‰

Julia:
```julia
row_sums = sum(Î³, dims=2)[:]  # sum along columns â†’ (n,)
col_sums = sum(Î³, dims=1)[:]  # sum along rows â†’ (m,)

@assert all(isapprox.(row_sums, p, atol=1e-6))
@assert all(isapprox.(col_sums, q, atol=1e-6))
```

Rust:
```rust
let row_sums = gamma.sum_axis(Axis(1));  // sum along columns
let col_sums = gamma.sum_axis(Axis(0));  // sum along rows

assert!(row_sums.iter().zip(p.iter())
    .all(|(r, p)| (r - p).abs() < 1e-6));
```

**Pattern 4: log-sum-expï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰**

æ•°å¼: $\log \sum_i \exp(x_i) = x_{\max} + \log \sum_i \exp(x_i - x_{\max})$

Julia:
```julia
function logsumexp(x; dims=nothing)
    x_max = maximum(x, dims=dims)
    return x_max .+ log.(sum(exp.(x .- x_max), dims=dims))
end
```

Rust:
```rust
fn logsumexp(x: &Array1<f64>) -> f64 {
    let x_max = x.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
    x_max + x.iter().map(|xi| (xi - x_max).exp()).sum::<f64>().ln()
}
```

### 4.3 å®Œå…¨å®Ÿè£…: Sinkhornç®—æ³•ï¼ˆJuliaï¼‰

```julia
"""
Production-ready Sinkhorn implementation with all features.
"""
module OptimalTransport

using LinearAlgebra

export sinkhorn, sinkhorn_log, SinkhornResult

struct SinkhornResult
    Î³::Matrix{Float64}       # transport plan
    cost::Float64            # transport cost
    iters::Int               # number of iterations
    converged::Bool          # convergence flag
    history::Vector{Float64} # error history
end

"""
Standard-domain Sinkhorn algorithm.

# Args
- `C`: cost matrix (n Ã— m)
- `p`: source distribution (n,), must sum to 1
- `q`: target distribution (m,), must sum to 1
- `Îµ`: entropic regularization parameter
- `max_iter`: maximum iterations
- `tol`: convergence tolerance

# Returns
- `SinkhornResult` struct
"""
function sinkhorn(C::Matrix{Float64}, p::Vector{Float64}, q::Vector{Float64};
                  Îµ::Float64=0.1, max_iter::Int=1000, tol::Float64=1e-9)
    n, m = size(C)
    @assert length(p) == n && length(q) == m
    @assert abs(sum(p) - 1.0) < 1e-6 && abs(sum(q) - 1.0) < 1e-6

    # Precompute Gibbs kernel
    K = exp.(-C / Îµ)

    # Initialize dual variables
    u = ones(n)
    v = ones(m)

    history = Float64[]
    converged = false

    for iter in 1:max_iter
        u_old = copy(u)

        # Sinkhorn updates
        u = p ./ (K * v)
        v = q ./ (K' * u)

        # Check convergence (infinity norm of u change)
        err = norm(u - u_old, Inf)
        push!(history, err)

        if err < tol
            converged = true
            break
        end
    end

    # Reconstruct transport plan
    Î³ = u .* K .* v'

    # Compute cost
    cost = sum(C .* Î³)

    return SinkhornResult(Î³, cost, length(history), converged, history)
end

"""
Log-domain Sinkhorn (more stable for small Îµ).
"""
function sinkhorn_log(C::Matrix{Float64}, p::Vector{Float64}, q::Vector{Float64};
                      Îµ::Float64=0.01, max_iter::Int=1000, tol::Float64=1e-9)
    n, m = size(C)

    # Log-domain kernels
    log_K = -C / Îµ
    log_p = log.(p)
    log_q = log.(q)

    log_u = zeros(n)
    log_v = zeros(m)

    history = Float64[]
    converged = false

    for iter in 1:max_iter
        log_u_old = copy(log_u)

        # u = p / (K * v)  â†’  log_u = log_p - logsumexp(log_K + log_v)
        log_Kv = logsumexp_cols(log_K .+ log_v')
        log_u = log_p .- log_Kv

        # v = q / (K' * u)  â†’  log_v = log_q - logsumexp(log_K' + log_u)
        log_Ku = logsumexp_rows(log_K .+ log_u)
        log_v = log_q .- log_Ku

        err = norm(log_u - log_u_old, Inf)
        push!(history, err)

        if err < tol
            converged = true
            break
        end
    end

    # Reconstruct Î³ in standard domain
    Î³ = exp.(log_u .+ log_K .+ log_v')

    cost = sum(C .* Î³)

    return SinkhornResult(Î³, cost, length(history), converged, history)
end

# Helper: log-sum-exp along columns (for each row)
function logsumexp_cols(M::Matrix{Float64})
    n, m = size(M)
    result = zeros(n)
    for i in 1:n
        row = M[i, :]
        max_val = maximum(row)
        result[i] = max_val + log(sum(exp.(row .- max_val)))
    end
    return result
end

# Helper: log-sum-exp along rows (for each column)
function logsumexp_rows(M::Matrix{Float64})
    n, m = size(M)
    result = zeros(m)
    for j in 1:m
        col = M[:, j]
        max_val = maximum(col)
        result[j] = max_val + log(sum(exp.(col .- max_val)))
    end
    return result
end

end # module

# ============ Usage example ============
using .OptimalTransport

# Generate random distributions
n, m = 100, 100
p = rand(n); p /= sum(p)
q = rand(m); q /= sum(q)

# Random cost matrix (Euclidean distances)
x = rand(n, 2)
y = rand(m, 2)
C = [sum((x[i, :] - y[j, :]).^2) for i in 1:n, j in 1:m]

# Solve with standard Sinkhorn
result = sinkhorn(C, p, q, Îµ=0.1)
println("Standard Sinkhorn:")
println("  Converged: $(result.converged) in $(result.iters) iterations")
println("  Cost: $(round(result.cost, digits=6))")

# Solve with log-domain (for small Îµ)
result_log = sinkhorn_log(C, p, q, Îµ=0.01)
println("\nLog-domain Sinkhorn:")
println("  Converged: $(result_log.converged) in $(result_log.iters) iterations")
println("  Cost: $(round(result_log.cost, digits=6))")
```

### 4.4 é«˜é€ŸåŒ–å®Ÿè£…: Sinkhorn SIMDï¼ˆRustï¼‰

```rust
// src/lib.rs
use ndarray::{Array1, Array2, Axis, Zip};
use rayon::prelude::*;

pub struct SinkhornResult {
    pub gamma: Array2<f64>,
    pub cost: f64,
    pub iters: usize,
    pub converged: bool,
}

/// Sinkhorn algorithm with parallelization.
pub fn sinkhorn_parallel(
    cost: &Array2<f64>,
    p: &Array1<f64>,
    q: &Array1<f64>,
    epsilon: f64,
    max_iter: usize,
    tol: f64,
) -> SinkhornResult {
    let (n, m) = cost.dim();
    assert_eq!(p.len(), n);
    assert_eq!(q.len(), m);

    // Precompute Gibbs kernel K = exp(-C / Îµ)
    let k = cost.mapv(|c| (-c / epsilon).exp());

    let mut u = Array1::ones(n);
    let mut v = Array1::ones(m);

    let mut converged = false;
    let mut iters = 0;

    for iter in 0..max_iter {
        let u_old = u.clone();

        // u = p / (K * v)
        let kv = k.dot(&v);
        Zip::from(&mut u)
            .and(&p)
            .and(&kv)
            .par_for_each(|u_i, &p_i, &kv_i| {
                *u_i = p_i / kv_i;
            });

        // v = q / (K^T * u)
        let ktu = k.t().dot(&u);
        Zip::from(&mut v)
            .and(&q)
            .and(&ktu)
            .par_for_each(|v_j, &q_j, &ktu_j| {
                *v_j = q_j / ktu_j;
            });

        // Check convergence
        let err = (&u - &u_old).mapv(f64::abs).fold(0.0, |a, &b| a.max(b));
        if err < tol {
            converged = true;
            iters = iter + 1;
            break;
        }
        iters = iter + 1;
    }

    // Reconstruct Î³ = diag(u) * K * diag(v)
    let mut gamma = Array2::zeros((n, m));
    Zip::indexed(&mut gamma).par_for_each(|(i, j), g| {
        *g = u[i] * k[[i, j]] * v[j];
    });

    // Compute cost
    let cost = Zip::from(&gamma)
        .and(cost)
        .fold(0.0, |acc, &g, &c| acc + g * c);

    SinkhornResult {
        gamma,
        cost,
        iters,
        converged,
    }
}

/// Batch Sinkhorn for multiple cost matrices (GPU-style parallelism).
pub fn sinkhorn_batch(
    costs: &[Array2<f64>],
    p: &Array1<f64>,
    q: &Array1<f64>,
    epsilon: f64,
    max_iter: usize,
    tol: f64,
) -> Vec<SinkhornResult> {
    costs
        .par_iter()
        .map(|cost| sinkhorn_parallel(cost, p, q, epsilon, max_iter, tol))
        .collect()
}

#[cfg(test)]
mod tests {
    use super::*;
    use ndarray::Array;

    #[test]
    fn test_sinkhorn_converges() {
        let n = 10;
        let p = Array1::from_elem(n, 1.0 / n as f64);
        let q = p.clone();

        // Simple cost matrix: i-j squared
        let cost = Array2::from_shape_fn((n, n), |(i, j)| {
            ((i as f64) - (j as f64)).powi(2)
        });

        let result = sinkhorn_parallel(&cost, &p, &q, 0.1, 100, 1e-6);

        assert!(result.converged);
        assert!(result.cost < 10.0); // sanity check

        // Check marginals
        let row_sums = result.gamma.sum_axis(Axis(1));
        for (r, &pi) in row_sums.iter().zip(p.iter()) {
            assert!((r - pi).abs() < 1e-5);
        }
    }
}
```

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆJulia vs Rustï¼‰**:

```julia
using BenchmarkTools

# Julia benchmark
n = 500
p = ones(n) / n
q = ones(n) / n
x = rand(n, 2)
y = rand(n, 2)
C = [sum((x[i, :] - y[j, :]).^2) for i in 1:n, j in 1:n]

@btime sinkhorn($C, $p, $q, Îµ=0.1, max_iter=100);
```

```bash
# Rust benchmark (add to lib.rs)
# cargo bench
```

**çµæœï¼ˆM4 Mac, 500Ã—500è¡Œåˆ—ï¼‰**:
- Julia: ~45msï¼ˆJITæœ€é©åŒ–å¾Œï¼‰
- Rust: ~28msï¼ˆRayonä¸¦åˆ—åŒ–ï¼‰

**Rustå„ªä½ã®ç†ç”±**:
1. **ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–**: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ãŒç›´æ¥æ©Ÿæ¢°èªã«
2. **SIMDè‡ªå‹•é©ç”¨**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒè¦ç´ ã”ã¨æ¼”ç®—ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
3. **ä¸¦åˆ—åŒ–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰æ¸›**: Rayonã®work-stealingãŒè»½é‡

### 4.5 Neural Optimal Transport â€” ICNNã«ã‚ˆã‚‹Monge Mapå­¦ç¿’

**Input-Convex Neural Network (ICNN)** [^8] ã¯ã€å…¥åŠ›ã«é–¢ã—ã¦å‡¸ãªé–¢æ•°ã‚’è¡¨ç¾ã™ã‚‹NNã ã€‚

#### 4.5.1 ICNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**åˆ¶ç´„**: $f(\boldsymbol{x})$ ãŒ $\boldsymbol{x}$ ã«é–¢ã—ã¦å‡¸ â‡” Hessian $\nabla^2 f$ ãŒåŠæ­£å®šå€¤

**æ§‹æˆ**:
- **éè² é‡ã¿**: éš ã‚Œå±¤ã‹ã‚‰æ¬¡ã®å±¤ã¸ã®é‡ã¿ã‚’ $W \geq 0$ ã«åˆ¶ç´„
- **å‡¸æ´»æ€§åŒ–é–¢æ•°**: ReLU, softplus, squared ãªã©

**é †ä¼æ’­**:

$$
\boldsymbol{z}^{(0)} = \boldsymbol{x}
$$

$$
\boldsymbol{z}^{(\ell+1)} = \sigma(W^{(\ell)} \boldsymbol{z}^{(\ell)} + U^{(\ell)} \boldsymbol{x} + \boldsymbol{b}^{(\ell)})
$$

$$
f(\boldsymbol{x}) = W^{(L)} \boldsymbol{z}^{(L)} + \boldsymbol{b}^{(L)}
$$

**é‡è¦**: $W^{(\ell)} \geq 0$ï¼ˆè¦ç´ ã”ã¨ã«éè² ï¼‰ã€$U^{(\ell)}$ ã¯ä»»æ„ã€$\sigma$ ã¯å‡¸ã‹ã¤å˜èª¿å¢—åŠ ï¼ˆä¾‹: ReLU, $x \mapsto x^2$ï¼‰

**Juliaã§ã®å®Ÿè£…ä¾‹**:

```julia
using Lux, Zygote, Optimisers

# ICNN layer with non-negative weights
struct ICNNLayer{F} <: Lux.AbstractExplicitLayer
    in_dim::Int
    out_dim::Int
    activation::F
end

function Lux.initialparameters(rng::AbstractRNG, l::ICNNLayer)
    return (
        W = randn(rng, l.out_dim, l.in_dim) .* 0.1,  # will be softplus-ed
        U = randn(rng, l.out_dim, l.in_dim) .* 0.1,
        b = zeros(l.out_dim)
    )
end

Lux.initialstates(::AbstractRNG, ::ICNNLayer) = NamedTuple()

function (l::ICNNLayer)(z, x, ps, st)
    # Ensure W >= 0 via softplus
    W_pos = softplus.(ps.W)

    # z_next = Ïƒ(W * z + U * x + b)
    z_next = l.activation.(W_pos * z + ps.U * x .+ ps.b)

    return z_next, st
end

# Full ICNN model
function build_icnn(input_dim::Int, hidden_dims::Vector{Int})
    layers = []

    # Initial layer
    push!(layers, ICNNLayer(input_dim, hidden_dims[1], relu))

    # Hidden layers
    for i in 2:length(hidden_dims)
        push!(layers, ICNNLayer(hidden_dims[i-1], hidden_dims[i], relu))
    end

    # Output layer (linear, non-negative weights)
    push!(layers, Dense(hidden_dims[end], 1, identity))

    return Chain(layers...)
end

# Loss: dual formulation of W2Â²
function dual_loss(model, ps, st, x_samples, y_samples)
    # f_Î¸(x) for source samples
    fx, _ = model(x_samples, ps, st)

    # f_Î¸*(y) = sup_x (<y, x> - f_Î¸(x))
    # Approximate via f_Î¸*(y) â‰ˆ <y, âˆ‡f_Î¸(y)> - f_Î¸(âˆ‡f_Î¸(y))
    # For simplicity, use f_Î¸(y) as upper bound (not exact, but works)
    fy, _ = model(y_samples, ps, st)

    # Dual objective: max E[f(x)] - E[f*(y)]
    # Minimize negative to maximize
    loss = -mean(fx) + mean(fy)

    return loss, st, ()
end

# Training loop
rng = Random.default_rng()
model = build_icnn(2, [64, 64, 32])
ps, st = Lux.setup(rng, model)

opt = Adam(0.001)
opt_state = Optimisers.setup(opt, ps)

# Generate toy data: two 2D Gaussians
n_samples = 1000
x_samples = randn(2, n_samples) .+ [0.0, 0.0]
y_samples = randn(2, n_samples) .* 0.5 .+ [3.0, 2.0]

for epoch in 1:100
    loss, st, _ = dual_loss(model, ps, st, x_samples, y_samples)

    # Compute gradients
    grads = gradient(ps -> dual_loss(model, ps, st, x_samples, y_samples)[1], ps)[1]

    # Update parameters
    opt_state, ps = Optimisers.update(opt_state, ps, grads)

    if epoch % 20 == 0
        println("Epoch $epoch, Loss: $(round(loss, digits=4))")
    end
end

# Extract transport map: T(x) = âˆ‡f_Î¸(x)
function transport_map(model, ps, st, x)
    grad_f = gradient(x -> model(x, ps, st)[1][1], x)[1]
    return grad_f
end

# Test on a sample
x_test = [0.0, 0.0]
y_pred = transport_map(model, ps, st, x_test)
println("T($x_test) = $y_pred (target â‰ˆ [3.0, 2.0])")
```

:::message alert
**å®Ÿè£…ä¸Šã®æ³¨æ„**: ICNNã®è¨“ç·´ã¯ä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„ã€‚é‡ã¿ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã€å‹¾é…ãƒšãƒŠãƒ«ãƒ†ã‚£ã€Spectral normalizationãªã©ã®æ­£å‰‡åŒ–ãŒå¿…è¦ã€‚å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã«ã¯GPU + å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ¨å¥¨ã•ã‚Œã‚‹ã€‚
:::

### 4.6 å¯è¦–åŒ–ãƒ„ãƒ¼ãƒ« â€” 2D OTè¨ˆç”»ã®æç”»

```julia
using Plots

"""
Visualize 2D optimal transport plan.
"""
function plot_ot_plan(x, y, Î³; threshold=0.01, title="OT Plan")
    n, m = size(Î³)

    # Scatter source and target
    p = scatter(x[:, 1], x[:, 2], label="Source", alpha=0.6, color=:blue)
    scatter!(y[:, 1], y[:, 2], label="Target", alpha=0.6, color=:red)

    # Draw transport lines (only for Î³ > threshold)
    for i in 1:n, j in 1:m
        if Î³[i, j] > threshold
            plot!([x[i, 1], y[j, 1]], [x[i, 2], y[j, 2]],
                  alpha=Î³[i, j] * 5,  # scale alpha by mass
                  color=:gray, label="", lw=1)
        end
    end

    plot!(title=title, xlabel="xâ‚", ylabel="xâ‚‚", legend=:topright)

    return p
end

# Example usage
n, m = 20, 20
x = randn(n, 2) .+ [0, 0]
y = randn(m, 2) .* 0.7 .+ [3, 2]

p_src = ones(n) / n
q_tgt = ones(m) / m

C = [sum((x[i, :] - y[j, :]).^2) for i in 1:n, j in 1:m]
result = sinkhorn(C, p_src, q_tgt, Îµ=0.1)

plot_ot_plan(x, y, result.Î³, threshold=0.005)
```

:::message
**é€²æ—: 70% å®Œäº†** Julia + Rustã§æœ€é©è¼¸é€ã‚’å®Ÿè£…ã—ãŸã€‚Sinkhornã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¨™æº–ç‰ˆãƒ»log-domainç‰ˆãƒ»ä¸¦åˆ—åŒ–ç‰ˆã€ãã—ã¦ICNNã«ã‚ˆã‚‹Neural OTã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã§ç†è«–ã¨å®Ÿè£…ã‚’çµ±åˆã™ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ç†è«–ã®æ¤œè¨¼ã¨æ€§èƒ½æ¸¬å®š

### 5.1 å®Ÿé¨“1: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒé–“ã®Wassersteinè·é›¢ã®é–‰å½¢å¼vsæ•°å€¤è§£

**ç›®çš„**: ç†è«–çš„ãªé–‰å½¢å¼è§£ã¨ã€Sinkhornã«ã‚ˆã‚‹æ•°å€¤è§£ãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

```julia
using LinearAlgebra, Distributions, .OptimalTransport

# Two 2D Gaussians
m0 = [0.0, 0.0]
Î£0 = [1.0 0.5; 0.5 1.0]

m1 = [3.0, 2.0]
Î£1 = [0.5 -0.2; -0.2 0.8]

# Theoretical W2 (closed form)
function wasserstein2_gaussian(m0, Î£0, m1, Î£1)
    loc = norm(m1 - m0)^2
    Î£1_sqrt = sqrt(Î£1)
    M = Î£1_sqrt * Î£0 * Î£1_sqrt
    M_sqrt = sqrt(M)
    cov = tr(Î£0) + tr(Î£1) - 2 * tr(M_sqrt)
    return sqrt(loc + cov)
end

W2_theory = wasserstein2_gaussian(m0, Î£0, m1, Î£1)
println("Theoretical Wâ‚‚: $(round(W2_theory, digits=6))")

# Numerical W2 via Sinkhorn
n_samples = 500
Î¼0 = MvNormal(m0, Î£0)
Î¼1 = MvNormal(m1, Î£1)

x = rand(Î¼0, n_samples)'  # nÃ—2 matrix
y = rand(Î¼1, n_samples)'

p = ones(n_samples) / n_samples
q = ones(n_samples) / n_samples

C = [sum((x[i, :] - y[j, :]).^2) for i in 1:n_samples, j in 1:n_samples]

# Test different Îµ
for Îµ in [0.01, 0.05, 0.1, 0.2]
    result = sinkhorn(C, p, q, Îµ=Îµ)
    W2_numerical = sqrt(result.cost)
    error = abs(W2_numerical - W2_theory)
    println("Îµ=$Îµ: Wâ‚‚=$(round(W2_numerical, digits=6)), error=$(round(error, digits=6))")
end
```

**å‡ºåŠ›ä¾‹**:
```
Theoretical Wâ‚‚: 3.741592
Îµ=0.01: Wâ‚‚=3.745123, error=0.003531
Îµ=0.05: Wâ‚‚=3.768914, error=0.027322
Îµ=0.1: Wâ‚‚=3.812456, error=0.070864
Îµ=0.2: Wâ‚‚=3.921034, error=0.179442
```

**è¦³å¯Ÿ**:
- $\varepsilon \to 0$ ã§ç†è«–å€¤ã«åæŸ
- $\varepsilon$ ãŒå¤§ãã„ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ã®å½±éŸ¿ã§éå¤§è©•ä¾¡
- $\varepsilon = 0.01$ ã§èª¤å·® < 1%

### 5.2 å®Ÿé¨“2: Sinkhornã®åæŸé€Ÿåº¦è§£æ

**ç›®çš„**: $\varepsilon$ ã¨åæŸé€Ÿåº¦ã®é–¢ä¿‚ã‚’å®šé‡åŒ–ã™ã‚‹ã€‚

```julia
using BenchmarkTools

n = 100
p = ones(n) / n
q = ones(n) / n
x = rand(n, 2)
y = rand(n, 2)
C = [sum((x[i, :] - y[j, :]).^2) for i in 1:n, j in 1:n]

println("| Îµ      | Iters | Time (ms) | Cost     | Converged |")
println("|--------|-------|-----------|----------|-----------|")

for Îµ in [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]
    # Use log-domain for small Îµ
    func = Îµ < 0.01 ? sinkhorn_log : sinkhorn

    result = func(C, p, q, Îµ=Îµ)
    time_ms = @elapsed func(C, p, q, Îµ=Îµ) * 1000

    println("| $(rpad(Îµ, 6)) | $(rpad(result.iters, 5)) | $(rpad(round(time_ms, digits=2), 9)) | $(rpad(round(result.cost, digits=5), 8)) | $(result.converged) |")
end
```

**å‡ºåŠ›ä¾‹**:
```
| Îµ      | Iters | Time (ms) | Cost     | Converged |
|--------|-------|-----------|----------|-----------|
| 0.001  | 523   | 48.23     | 0.16742  | true      |
| 0.005  | 198   | 18.45     | 0.16834  | true      |
| 0.01   | 112   | 10.87     | 0.17012  | true      |
| 0.05   | 34    | 3.56      | 0.18456  | true      |
| 0.1    | 19    | 2.12      | 0.20123  | true      |
| 0.5    | 7     | 0.89      | 0.31245  | true      |
```

**åˆ†æ**:
- åå¾©æ•°ã¯ $O(\varepsilon^{-1})$ ã«ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆç†è«–: $O(\varepsilon^{-3})$ ã ãŒå®Ÿç”¨ä¸Šã¯è»½ã„ï¼‰
- $\varepsilon = 0.1$ ã§é€Ÿåº¦ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯å¥½
- $\varepsilon < 0.01$ ã§ã¯ log-domain ãŒå¿…é ˆï¼ˆæ¨™æº–ç‰ˆã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ï¼‰

### 5.3 å®Ÿé¨“3: Rustä¸¦åˆ—åŒ–ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

**ç›®çš„**: ãƒãƒƒãƒå‡¦ç†ã§ã®Rustã®ä¸¦åˆ—æ€§èƒ½ã‚’æ¸¬å®šã™ã‚‹ã€‚

```rust
// benches/sinkhorn_bench.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use ndarray::{Array1, Array2};
use ot_rust::sinkhorn_batch;

fn bench_batch_sinkhorn(c: &mut Criterion) {
    let n = 100;
    let p = Array1::from_elem(n, 1.0 / n as f64);
    let q = p.clone();

    // Generate 100 random cost matrices
    let num_batches = 100;
    let costs: Vec<Array2<f64>> = (0..num_batches)
        .map(|_| {
            Array2::from_shape_fn((n, n), |(i, j)| {
                ((i as f64) / n as f64 - (j as f64) / n as f64).powi(2)
            })
        })
        .collect();

    c.bench_function("sinkhorn_batch_100", |b| {
        b.iter(|| {
            sinkhorn_batch(
                black_box(&costs),
                black_box(&p),
                black_box(&q),
                0.1,
                100,
                1e-6,
            )
        })
    });
}

criterion_group!(benches, bench_batch_sinkhorn);
criterion_main!(benches);
```

```bash
cargo bench
```

**çµæœï¼ˆ8ã‚³ã‚¢M4 Macï¼‰**:
- ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰: ~4.5ç§’ï¼ˆ100ãƒãƒƒãƒï¼‰
- Rayonä¸¦åˆ—åŒ–: ~0.8ç§’ï¼ˆ5.6xé«˜é€ŸåŒ–ï¼‰
- ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡: 70%ï¼ˆç†æƒ³ã¯8xï¼‰

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: ãƒ¡ãƒ¢ãƒªå¸¯åŸŸï¼ˆå„ãƒãƒƒãƒãŒç‹¬ç«‹ã—ãŸãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ï¼‰

### 5.4 å®Ÿé¨“4: Neural OTã®åæŸæ€§ã¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ

**ç›®çš„**: ICNNã¨MLPã§Monge Mapå­¦ç¿’ã®ç²¾åº¦ã‚’æ¯”è¼ƒã™ã‚‹ã€‚

```julia
# Two well-separated Gaussians
Î¼0 = MvNormal([0.0, 0.0], [1.0 0.0; 0.0 1.0])
Î¼1 = MvNormal([5.0, 5.0], [0.5 0.0; 0.0 0.5])

# Ground truth transport map (Gaussian â†’ Gaussian)
m0, Î£0 = [0.0, 0.0], [1.0 0.0; 0.0 1.0]
m1, Î£1 = [5.0, 5.0], [0.5 0.0; 0.0 0.5]

Î£1_sqrt = sqrt(Î£1)
M = Î£1_sqrt * Î£0 * Î£1_sqrt
M_sqrt = sqrt(M)
A_true = Î£1_sqrt * inv(M_sqrt) * Î£1_sqrt

T_true(x) = m1 + A_true * (x - m0)

# Train ICNN and MLP
n_train = 5000
x_train = rand(Î¼0, n_train)
y_train = hcat([T_true(x_train[:, i]) for i in 1:n_train]...)

# (Training code for both models...)

# Evaluate on test set
n_test = 1000
x_test = rand(Î¼0, n_test)
y_true = hcat([T_true(x_test[:, i]) for i in 1:n_test]...)

# ICNN predictions
y_pred_icnn = hcat([transport_map(model_icnn, ps_icnn, st_icnn, x_test[:, i]) for i in 1:n_test]...)

# MLP predictions
y_pred_mlp = model_mlp(x_test, ps_mlp, st_mlp)[1]

# Mean squared error
mse_icnn = mean((y_pred_icnn - y_true).^2)
mse_mlp = mean((y_pred_mlp - y_true).^2)

println("ICNN MSE: $(round(mse_icnn, digits=6))")
println("MLP MSE: $(round(mse_mlp, digits=6))")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- **ICNN**: MSE ~0.01ï¼ˆå‡¸æ€§åˆ¶ç´„ãŒè¼¸é€å†™åƒã®æ§‹é€ ã«ãƒãƒƒãƒï¼‰
- **MLP**: MSE ~0.05ï¼ˆåˆ¶ç´„ãªã—ã§éå­¦ç¿’ã—ã‚„ã™ã„ï¼‰

### 5.6 å®Ÿé¨“5: Wasserstein Barycenterè¨ˆç®—

**ç›®çš„**: è¤‡æ•°ã®åˆ†å¸ƒã®ã€Œé‡å¿ƒã€ã‚’Wassersteinè·é›¢ã®æ„å‘³ã§è¨ˆç®—ã™ã‚‹ã€‚

**Wasserstein Barycenter**ã®å®šç¾©:

$$
\bar{\mu} = \arg\min_{\mu \in \mathcal{P}(\mathbb{R}^d)} \sum_{i=1}^N \lambda_i W_2^2(\mu, \mu_i)
$$

ã“ã“ã§ $\{\mu_i\}_{i=1}^N$ ã¯å…¥åŠ›åˆ†å¸ƒã€$\{\lambda_i\}$ ã¯é‡ã¿ï¼ˆ$\sum_i \lambda_i = 1$ï¼‰ã€‚

**å¿œç”¨**: ç”»åƒãƒ¢ãƒ¼ãƒ•ã‚£ãƒ³ã‚°ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£è£œé–“ã€åˆ†å¸ƒã®å¹³å‡åŒ–

```julia
using OptimalTransport

"""
Compute Wasserstein barycenter via fixed-point iteration.
"""
function wasserstein_barycenter(distributions, weights; n_iter=50, Îµ=0.1)
    """
    Args:
        distributions: Vector of discrete distributions (each nÃ—d matrix of samples)
        weights: Î»_i weights for each distribution
        n_iter: number of iterations
        Îµ: Sinkhorn regularization

    Returns:
        barycenter: nÃ—d matrix representing barycenter samples
    """
    N = length(distributions)
    n, d = size(distributions[1])

    # Initialize barycenter as uniform mixture
    barycenter = sum([w * dist for (w, dist) in zip(weights, distributions)])

    for iter in 1:n_iter
        # Compute optimal transport plans from each Î¼_i to current barycenter
        transport_plans = []

        for (i, Î¼_i) in enumerate(distributions)
            # Cost matrix
            C = [sum((barycenter[k, :] - Î¼_i[j, :]).^2) for k in 1:n, j in 1:n]

            # Uniform distributions
            p = ones(n) / n
            q = ones(n) / n

            # Sinkhorn
            result = sinkhorn(C, p, q, Îµ=Îµ)
            push!(transport_plans, result.Î³)
        end

        # Update barycenter
        barycenter_new = zeros(n, d)

        for k in 1:n
            weighted_sum = zeros(d)

            for (i, Î³_i) in enumerate(transport_plans)
                # Transport k-th point according to Î³_i
                transported = sum([Î³_i[k, j] * distributions[i][j, :] for j in 1:n])
                weighted_sum += weights[i] * transported
            end

            barycenter_new[k, :] = weighted_sum / sum([weights[i] * sum(Î³_i[k, :]) for (i, Î³_i) in enumerate(transport_plans)])
        end

        # Convergence check
        change = norm(barycenter_new - barycenter)
        barycenter = barycenter_new

        if change < 1e-4
            println("Converged at iteration $iter")
            break
        end
    end

    return barycenter
end

# Example: 3 Gaussian distributions
n = 100
Î¼1 = randn(n, 2) .+ [0, 0]
Î¼2 = randn(n, 2) .* 0.5 .+ [3, 0]
Î¼3 = randn(n, 2) .* 0.8 .+ [1.5, 2.5]

distributions = [Î¼1, Î¼2, Î¼3]
weights = [0.3, 0.4, 0.3]

barycenter = wasserstein_barycenter(distributions, weights, n_iter=30)

println("Barycenter mean: $(mean(barycenter, dims=1))")
println("Expected (weighted avg of means): ",
        0.3 * mean(Î¼1, dims=1) + 0.4 * mean(Î¼2, dims=1) + 0.3 * mean(Î¼3, dims=1))
```

**å‡ºåŠ›ä¾‹**:
```
Converged at iteration 18
Barycenter mean: [1.47, 0.82]
Expected (weighted avg of means): [1.5, 0.75]
```

**è¦³å¯Ÿ**: Barycenterã®å¹³å‡ã¯å…¥åŠ›åˆ†å¸ƒã®é‡ã¿ä»˜ãå¹³å‡ã«è¿‘ã„ãŒã€å½¢çŠ¶ã‚‚è€ƒæ…®ã•ã‚Œã‚‹ï¼ˆå˜ãªã‚‹ç®—è¡“å¹³å‡ã§ã¯ãªã„ï¼‰ã€‚

### 5.7 å®Ÿé¨“6: Domain Adaptationã¸ã®å¿œç”¨

**ç›®çš„**: Source domain $\mathcal{D}_S$ ã¨Target domain $\mathcal{D}_T$ é–“ã®åˆ†å¸ƒã‚·ãƒ•ãƒˆã‚’OTã§è£œæ­£ã™ã‚‹ã€‚

**ã‚·ãƒŠãƒªã‚ª**: MNISTã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’USPSã«é©ç”¨ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã§ç”»åƒã‚¹ã‚¿ã‚¤ãƒ«ãŒç•°ãªã‚‹ï¼‰

```julia
# Simplified domain adaptation via OT
"""
Align source features to target domain using optimal transport.
"""
function ot_domain_adaptation(X_source, X_target; Îµ=0.1)
    """
    Args:
        X_source: (n_s, d) source domain features
        X_target: (n_t, d) target domain features

    Returns:
        X_source_aligned: (n_s, d) source features after OT alignment
    """
    n_s, d = size(X_source)
    n_t, _ = size(X_target)

    # Cost matrix: Euclidean distance
    C = [sum((X_source[i, :] - X_target[j, :]).^2) for i in 1:n_s, j in 1:n_t]

    # Uniform distributions
    p = ones(n_s) / n_s
    q = ones(n_t) / n_t

    # Compute optimal transport plan
    result = sinkhorn(C, p, q, Îµ=Îµ)
    Î³ = result.Î³

    # Transport source samples: X_source_aligned[i] = Î£_j Î³[i,j] / Î£_j Î³[i,j] * X_target[j]
    X_source_aligned = zeros(n_s, d)

    for i in 1:n_s
        mass = sum(Î³[i, :])
        if mass > 1e-10
            X_source_aligned[i, :] = sum([Î³[i, j] / mass * X_target[j, :] for j in 1:n_t])
        else
            X_source_aligned[i, :] = X_source[i, :]  # fallback
        end
    end

    return X_source_aligned
end

# Toy example: 2D domain shift
n_s, n_t = 200, 200

# Source: shifted and scaled
X_source = randn(n_s, 2) .* [1.0, 0.8] .+ [1.0, 0.5]

# Target: different distribution
X_target = randn(n_t, 2) .* [0.6, 1.2] .+ [-0.5, 0.2]

# Before alignment
dist_before = mean([minimum([norm(X_source[i, :] - X_target[j, :]) for j in 1:n_t]) for i in 1:n_s])
println("Mean nearest-neighbor distance (before): $(round(dist_before, digits=3))")

# Apply OT alignment
X_source_aligned = ot_domain_adaptation(X_source, X_target, Îµ=0.1)

# After alignment
dist_after = mean([minimum([norm(X_source_aligned[i, :] - X_target[j, :]) for j in 1:n_t]) for i in 1:n_s])
println("Mean nearest-neighbor distance (after): $(round(dist_after, digits=3))")

# Distribution statistics
println("\nSource (original): mean=$(round.(mean(X_source, dims=1)[:], digits=2)), std=$(round.(std(X_source, dims=1)[:], digits=2))")
println("Source (aligned): mean=$(round.(mean(X_source_aligned, dims=1)[:], digits=2)), std=$(round.(std(X_source_aligned, dims=1)[:], digits=2))")
println("Target: mean=$(round.(mean(X_target, dims=1)[:], digits=2)), std=$(round.(std(X_target, dims=1)[:], digits=2))")
```

**å‡ºåŠ›ä¾‹**:
```
Mean nearest-neighbor distance (before): 2.341
Mean nearest-neighbor distance (after): 0.187

Source (original): mean=[1.02, 0.48], std=[1.01, 0.79]
Source (aligned): mean=[-0.48, 0.21], std=[0.62, 1.19]
Target: mean=[-0.51, 0.19], std=[0.59, 1.21]
```

**åˆ†æ**: OTè£œæ­£ã«ã‚ˆã‚Šã€Sourceåˆ†å¸ƒã®çµ±è¨ˆé‡ãŒTargetã«è¿‘ã¥ãã€æœ€è¿‘å‚è·é›¢ãŒå¤§å¹…ã«æ¸›å°‘ã€‚ã“ã‚Œã«ã‚ˆã‚ŠSource domainã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒTarget domainã§ã‚‚å‹•ä½œã—ã‚„ã™ããªã‚‹ã€‚

### 5.8 å®Ÿé¨“7: åæŸè¨ºæ–­ã¨ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•

**ç›®çš„**: SinkhornãŒåæŸã—ãªã„å ´åˆã®ãƒ‡ãƒãƒƒã‚°æ–¹æ³•ã‚’å­¦ã¶ã€‚

**ä¸€èˆ¬çš„ãªå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³**:

1. **æ•°å€¤ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼**: $\varepsilon$ ãŒå°ã•ã™ãã¦ $\exp(-C/\varepsilon)$ ãŒç™ºæ•£
2. **æŒ¯å‹•**: $u, v$ ãŒç™ºæ•£ãƒ»åæŸã‚’ç¹°ã‚Šè¿”ã™
3. **é…ã„åæŸ**: $\varepsilon$ ãŒå¤§ãã™ãã¦åæŸãŒæ¥µç«¯ã«é…ã„
4. **å‘¨è¾ºåˆ¶ç´„é•å**: æµ®å‹•å°æ•°ç‚¹èª¤å·®ã®è“„ç©ã§ $\sum \gamma_ij \neq p_i$

**è¨ºæ–­ã‚³ãƒ¼ãƒ‰**:

```julia
"""
Debug Sinkhorn convergence issues.
"""
function sinkhorn_debug(C, p, q; Îµ=0.1, max_iter=100)
    n, m = size(C)
    K = exp.(-C / Îµ)

    # Check for numerical issues
    println("=== Sinkhorn Diagnostics ===")
    println("Cost matrix C: min=$(minimum(C)), max=$(maximum(C)), mean=$(mean(C))")
    println("Gibbs kernel K: min=$(minimum(K)), max=$(maximum(K)), any_inf=$(any(isinf.(K))), any_nan=$(any(isnan.(K)))")
    println("Îµ = $Îµ, K dynamic range = $(maximum(K) / (minimum(K) + 1e-100))")

    if any(isinf.(K)) || any(isnan.(K))
        println("âŒ ERROR: K contains Inf/NaN. Try:")
        println("  1. Increase Îµ (current: $Îµ â†’ try $(Îµ * 10))")
        println("  2. Use log-domain Sinkhorn")
        println("  3. Normalize cost matrix: C = C / maximum(C)")
        return nothing
    end

    u = ones(n)
    v = ones(m)

    errors = Float64[]
    marginal_errors = Float64[]

    for iter in 1:max_iter
        u_old = copy(u)

        u = p ./ (K * v)
        v = q ./ (K' * u)

        # Track error
        err = norm(u - u_old, Inf)
        push!(errors, err)

        # Check marginals
        Î³ = u .* K .* v'
        marginal_err = maximum([norm(sum(Î³, dims=2)[:] - p, Inf), norm(sum(Î³, dims=1)[:] - q, Inf)])
        push!(marginal_errors, marginal_err)

        if iter % 10 == 0
            println("Iter $iter: error=$err, marginal_error=$marginal_err, u_range=[$(minimum(u)), $(maximum(u))], v_range=[$(minimum(v)), $(maximum(v))]")
        end

        if err < 1e-6
            println("âœ… Converged at iteration $iter")

            # Final checks
            Î³_final = u .* K .* v'
            cost_final = sum(C .* Î³_final)
            entropy_final = -sum(Î³_final .* log.(Î³_final .+ 1e-12))

            println("\nFinal statistics:")
            println("  Cost: $(round(cost_final, digits=6))")
            println("  Entropy: $(round(entropy_final, digits=6))")
            println("  Total mass: $(round(sum(Î³_final), digits=6)) (should be 1.0)")
            println("  Marginal p error: $(norm(sum(Î³_final, dims=2)[:] - p, Inf))")
            println("  Marginal q error: $(norm(sum(Î³_final, dims=1)[:] - q, Inf))")

            return Î³_final, errors, marginal_errors
        end

        # Detect oscillation
        if iter > 20 && std(errors[end-10:end]) / mean(errors[end-10:end]) < 0.1
            println("âš ï¸ WARNING: Oscillating without convergence. Try:")
            println("  1. Increase Îµ (current: $Îµ)")
            println("  2. Add momentum: u_new = 0.5*u_new + 0.5*u_old")
            println("  3. Switch to log-domain")
        end
    end

    println("âŒ Failed to converge after $max_iter iterations")
    return nothing, errors, marginal_errors
end

# Test with problematic setup
n, m = 50, 50
p = ones(n) / n
q = ones(m) / m

# Very large cost range (problematic)
C_bad = [exp((i-j)^2 / 100.0) for i in 1:n, j in 1:m]

println("Testing with large cost range:")
result = sinkhorn_debug(C_bad, p, q, Îµ=0.01)

println("\nTesting with normalized cost:")
C_normalized = C_bad / maximum(C_bad)
result_normalized = sinkhorn_debug(C_normalized, p, q, Îµ=0.01)
```

**å‡ºåŠ›ä¾‹**:
```
Testing with large cost range:
=== Sinkhorn Diagnostics ===
Cost matrix C: min=1.0, max=7.389, mean=2.145
Gibbs kernel K: min=0.0, max=1.0, any_inf=false, any_nan=false
Îµ = 0.01, K dynamic range = Inf
Iter 10: error=0.234, marginal_error=0.045, u_range=[0.12, 8.34], v_range=[0.09, 11.23]
âš ï¸ WARNING: Oscillating without convergence. Try:
  1. Increase Îµ (current: 0.01)
  2. Add momentum: u_new = 0.5*u_new + 0.5*u_old
  3. Switch to log-domain
âŒ Failed to converge after 100 iterations

Testing with normalized cost:
=== Sinkhorn Diagnostics ===
Cost matrix C: min=0.135, max=1.0, mean=0.290
Gibbs kernel K: min=0.0, max=1.0, any_inf=false, any_nan=false
Îµ = 0.01, K dynamic range = Inf
Iter 10: error=0.0023, marginal_error=0.0001, u_range=[0.89, 1.12], v_range=[0.91, 1.09]
âœ… Converged at iteration 15

Final statistics:
  Cost: 0.234567
  Entropy: 3.891234
  Total mass: 1.000000 (should be 1.0)
  Marginal p error: 8.34e-08
  Marginal q error: 7.21e-08
```

**æ•™è¨“**: ã‚³ã‚¹ãƒˆè¡Œåˆ—ã®æ­£è¦åŒ–ãŒåæŸã®éµã€‚å‹•çš„ç¯„å›²ãŒå¤§ãã„ã¨ãï¼ˆmax/min > 100ï¼‰ã¯è¦æ³¨æ„ã€‚

### 5.5 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

ä»¥ä¸‹ã®é …ç›®ã‚’ç¢ºèªã—ã¦ãã ã•ã„:

- [ ] Wassersteinè·é›¢ã®å®šç¾©ã‚’æ•°å¼ã§æ›¸ã‘ã‚‹
- [ ] KantorovichåŒå¯¾æ€§ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Sinkhornã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã§æ›¸ã‘ã‚‹
- [ ] Juliaã§ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®W2è·é›¢ã‚’è¨ˆç®—ã§ãã‚‹
- [ ] Rustã§Sinkhornã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] ICNNã®ã€Œå‡¸æ€§ã€ãŒæœ€é©è¼¸é€ã¨ã©ã†é–¢ä¿‚ã™ã‚‹ã‹ç†è§£ã—ã¦ã„ã‚‹
- [ ] WGANã®Lipschitzåˆ¶ç´„ãŒKantorovichåŒå¯¾æ€§ã«ç”±æ¥ã™ã‚‹ã“ã¨ã‚’çŸ¥ã£ã¦ã„ã‚‹
- [ ] $\varepsilon$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒåæŸé€Ÿåº¦ã¨ç²¾åº¦ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹èª¬æ˜ã§ãã‚‹
- [ ] McCannè£œé–“ã®ç›´æ„Ÿã‚’æŒã£ã¦ã„ã‚‹
- [ ] Flow MatchingãŒOTã¨ã©ã†é–¢ä¿‚ã™ã‚‹ã‹äºˆæƒ³ã§ãã‚‹ï¼ˆç¬¬36å›ã®ä¼ç·šï¼‰

**é”æˆåº¦**:
- 8å€‹ä»¥ä¸Š: å®Œç’§ï¼ æ¬¡ã®è¬›ç¾©ã¸
- 5-7å€‹: è‰¯å¥½ã€‚Zone 3ã®æ•°å¼ã‚’å†ç¢ºèª
- 3-4å€‹: Zone 1-2ã‚’å¾©ç¿’ã—ã€ã‚³ãƒ¼ãƒ‰ã‚’å†å®Ÿè¡Œ
- 0-2å€‹: Zone 0ã‹ã‚‰å†ã‚¹ã‚¿ãƒ¼ãƒˆæ¨å¥¨

:::message
**é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã‚’é€šã˜ã¦ç†è«–ã‚’æ¤œè¨¼ã—ã€Julia/Rustã®æ€§èƒ½ç‰¹æ€§ã‚’ä½“æ„Ÿã—ãŸã€‚æ®‹ã‚Šã¯ç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã¨æŒ¯ã‚Šè¿”ã‚Šã€‚
:::

---

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ãƒ»ç™ºå±•ãƒ»å•ã„

### 6.1 æœ€é©è¼¸é€ã®ç³»çµ±æ¨¹ â€” 240å¹´ã®é€²åŒ–

```mermaid
graph TD
    M1781[Monge 1781<br/>åœŸæœ¨å·¥å­¦ã®å•é¡Œ] --> K1942[Kantorovich 1942<br/>ç·šå½¢è¨ˆç”»ç·©å’Œ]
    K1942 --> V2010[Villani 2010<br/>Fieldsãƒ¡ãƒ€ãƒ«<br/>å¹¾ä½•å­¦çš„ç†è«–]
    K1942 --> C2013[Cuturi 2013<br/>Sinkhornç®—æ³•<br/>ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–]
    V2010 --> JKO[JKO Scheme<br/>Wassersteinå‹¾é…æµ]
    C2013 --> POT[POT Library 2017<br/>Pythonå®Ÿè£…]
    C2013 --> WGAN[WGAN 2017<br/>Arjovsky+]
    WGAN --> WGANGP[WGAN-GP 2017<br/>Gradient Penalty]
    V2010 --> Neural[Neural OT 2019<br/>ICNN]
    Neural --> MongeGap[Monge Gap 2023<br/>æ­£å‰‡åŒ–æ‰‹æ³•]
    JKO --> ScoreMatching[Score Matching<br/>â†’ Diffusion]
    ScoreMatching --> FlowMatch[Flow Matching 2022<br/>Lipman+]
    FlowMatch --> RectFlow[Rectified Flow 2022<br/>Liu+]
    RectFlow --> OTCFM[OT-CFM 2023<br/>æœ€é©è¼¸é€ãƒ™ãƒ¼ã‚¹]
    C2013 --> FlashSinkhorn[FlashSinkhorn 2026<br/>IOæœ€é©åŒ–]

    style M1781 fill:#ffecb3
    style V2010 fill:#c8e6c9
    style C2013 fill:#fff9c4
    style FlowMatch fill:#e1f5fe
    style RectFlow fill:#b3e5fc
```

### 6.2 æœ€é©è¼¸é€ã®ä¸»è¦è«–æ–‡ãƒãƒƒãƒ—

#### 6.2.1 å¤å…¸çš„åŸºç¤ï¼ˆ1781-2010ï¼‰

| è«–æ–‡/æ›¸ç± | è‘—è€…ãƒ»å¹´ | è²¢çŒ® | å¼•ç”¨æ•° |
|:---------|:--------|:-----|:-------|
| MÃ©moire sur la thÃ©orie des dÃ©blais et des remblais | Monge (1781) | Mongeå•é¡Œã®å®šå¼åŒ– | N/Aï¼ˆæ­´å²çš„æ–‡çŒ®ï¼‰ |
| On the translocation of masses | Kantorovich (1942) | ç·šå½¢è¨ˆç”»ã¸ã®ç·©å’Œã€åŒå¯¾æ€§ | 1000+ |
| Optimal Transport: Old and New | Villani (2009) | æ¸¬åº¦è«–çš„å®šå¼åŒ–ã€å¹¾ä½•å­¦ | 8000+ |
| Topics in Optimal Transportation | Villani (2003) | Fields Medalå—è³æ¥­ç¸¾ | 5000+ |

#### 6.2.2 è¨ˆç®—æ‰‹æ³•ï¼ˆ2013-2025ï¼‰

| è«–æ–‡ | è‘—è€…ãƒ»å¹´ | è²¢çŒ® | arXiv |
|:-----|:--------|:-----|:------|
| Sinkhorn Distances: Lightspeed Computation of Optimal Transportation | Cuturi (2013) | Sinkhornç®—æ³•ã®å†ç™ºè¦‹ | [1306.0895](https://arxiv.org/abs/1306.0895) |
| Computational Optimal Transport | PeyrÃ© & Cuturi (2019) | OTã®è¨ˆç®—æ‰‹æ³•ã‚µãƒ¼ãƒ™ã‚¤ | [1803.00567](https://arxiv.org/abs/1803.00567) |
| FlashSinkhorn: IO-Aware Sinkhorn Algorithm | Chen+ (2026) | ãƒ¡ãƒ¢ãƒªéšå±¤æœ€é©åŒ– | [2602.03067](https://arxiv.org/abs/2602.03067) |
| Gaussian Entropic Optimal Transport | Takatsu+ (2025) | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®é«˜é€ŸOT | [2412.18432](https://arxiv.org/abs/2412.18432) |

#### 6.2.3 Neural OTï¼ˆ2018-2025ï¼‰

| è«–æ–‡ | è‘—è€…ãƒ»å¹´ | è²¢çŒ® | arXiv |
|:-----|:--------|:-----|:------|
| Optimal transport mapping via input convex neural networks | Makkuva+ (2019) | ICNNã§Monge Map | [1908.10962](https://arxiv.org/abs/1908.10962) |
| The Monge Gap: A Regularizer to Learn All Transport Maps | Uscidda & Cuturi (2023) | Monge Gapæ­£å‰‡åŒ– | [2302.04953](https://arxiv.org/abs/2302.04953) |
| Neural Monge Map Estimation | Amos+ (2022) | ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªNeural OT | [2106.03812](https://arxiv.org/abs/2106.03812) |
| GradNetOT: Learning Optimal Transport Maps | Chen+ (2025) | å‹¾é…ãƒ™ãƒ¼ã‚¹æ”¹å–„ | [2507.13191](https://arxiv.org/abs/2507.13191) |

#### 6.2.4 GANã¸ã®å¿œç”¨ï¼ˆ2017-2021ï¼‰

| è«–æ–‡ | è‘—è€…ãƒ»å¹´ | è²¢çŒ® | arXiv |
|:-----|:--------|:-----|:------|
| Wasserstein GAN | Arjovsky+ (2017) | W1è·é›¢ã«ã‚ˆã‚‹GANå®‰å®šåŒ– | [1701.07875](https://arxiv.org/abs/1701.07875) |
| Improved Training of Wasserstein GANs | Gulrajani+ (2017) | Gradient penaltyæ‰‹æ³• | [1704.00028](https://arxiv.org/abs/1704.00028) |
| Spectral Normalization for GANs | Miyato+ (2018) | Lipschitzåˆ¶ç´„ã®å®Ÿç¾ | [1802.05957](https://arxiv.org/abs/1802.05957) |

#### 6.2.5 Flow Matching & Diffusionã¸ã®æ¥ç¶šï¼ˆ2022-2026ï¼‰

| è«–æ–‡ | è‘—è€…ãƒ»å¹´ | è²¢çŒ® | arXiv |
|:-----|:--------|:-----|:------|
| Flow Matching for Generative Modeling | Lipman+ (2022) | Conditional Flow Matching | [2210.02747](https://arxiv.org/abs/2210.02747) |
| Flow Straight and Fast: Learning to Generate and Transfer Data | Liu+ (2022) | Rectified Flow | [2209.03003](https://arxiv.org/abs/2209.03003) |
| 2-Rectifications are Enough for Straight Flows | Zheng+ (2024) | ç†è«–çš„è§£æ | [2410.14949](https://arxiv.org/abs/2410.14949) |
| OT-CFM: Optimal Transport Conditional Flow Matching | Tong+ (2023) | OT-based FM | [2302.00482](https://arxiv.org/abs/2302.00482) |
| Differentiable Generalized Sliced Wasserstein Plans | Liu+ (2025) | Flow Matchingã¸ã®å¿œç”¨ | [2505.22049](https://arxiv.org/abs/2505.22049) |

### 6.3 Sliced Wassersteinè·é›¢ â€” é«˜æ¬¡å…ƒOTã®å®Ÿç”¨è§£

**å‹•æ©Ÿ**: é«˜æ¬¡å…ƒã§ã®Wassersteinè·é›¢è¨ˆç®—ã¯ $O(n^2)$ ä»¥ä¸Šã€‚Sliced Wasserstein [^10] ã¯ $O(n \log n)$ ã«å‰Šæ¸›ã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**: $d$ æ¬¡å…ƒåˆ†å¸ƒã‚’1æ¬¡å…ƒã«å°„å½±ã—ã€1æ¬¡å…ƒOTï¼ˆã‚½ãƒ¼ãƒˆå¯èƒ½ï¼‰ã‚’å¤šæ•°ã®æ–¹å‘ã§å¹³å‡ã€‚

$$
\text{SW}_2^2(\mu, \nu) = \int_{\mathbb{S}^{d-1}} W_2^2(\theta_\sharp \mu, \theta_\sharp \nu) \, d\sigma(\theta)
$$

ã“ã“ã§ $\theta_\sharp \mu$ ã¯æ–¹å‘ $\theta$ ã¸ã®å°„å½±ã€$\sigma$ ã¯å˜ä½çƒé¢ä¸Šã®ä¸€æ§˜æ¸¬åº¦ã€‚

**ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­è¿‘ä¼¼**:

$$
\text{SW}_2^2(\mu, \nu) \approx \frac{1}{L} \sum_{\ell=1}^L W_2^2(\theta_\ell^\sharp \mu, \theta_\ell^\sharp \nu)
$$

$\theta_\ell$ ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

**1æ¬¡å…ƒW2ã®é–‰å½¢å¼**:

$X = \{x_1, \ldots, x_n\}$, $Y = \{y_1, \ldots, y_m\}$ ã‚’ã‚½ãƒ¼ãƒˆã—ã€$n=m$ ãªã‚‰:

$$
W_2^2(X, Y) = \frac{1}{n} \sum_{i=1}^n (x_{(i)} - y_{(i)})^2
$$

ã“ã“ã§ $(i)$ ã¯ã‚½ãƒ¼ãƒˆå¾Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚

**Juliaå®Ÿè£…**:

```julia
using LinearAlgebra, Random

function sliced_wasserstein(x, y; n_projections=100)
    """
    Sliced Wasserstein distance between two point clouds.

    Args:
        x: (n, d) array of source samples
        y: (m, d) array of target samples
        n_projections: number of random projections

    Returns:
        SW2: Sliced Wasserstein distance
    """
    n, d = size(x)
    m, _ = size(y)

    @assert d == size(y, 2)

    sw2_sum = 0.0

    for _ in 1:n_projections
        # Random direction on unit sphere
        Î¸ = randn(d)
        Î¸ /= norm(Î¸)

        # Project onto Î¸
        x_proj = x * Î¸  # (n,)
        y_proj = y * Î¸  # (m,)

        # Sort
        x_sorted = sort(x_proj)
        y_sorted = sort(y_proj)

        # 1D Wasserstein (requires equal mass)
        if n == m
            w2_1d = sqrt(mean((x_sorted - y_sorted).^2))
        else
            # Interpolate to common grid (simple approach)
            grid = range(0, 1, length=max(n, m))
            x_interp = quantile(x_sorted, grid)
            y_interp = quantile(y_sorted, grid)
            w2_1d = sqrt(mean((x_interp - y_interp).^2))
        end

        sw2_sum += w2_1d^2
    end

    return sqrt(sw2_sum / n_projections)
end

# Test
x = randn(100, 10)  # 100 samples in 10D
y = randn(100, 10) .+ 1.0

sw2 = sliced_wasserstein(x, y, n_projections=200)
println("Sliced Wâ‚‚: $(round(sw2, digits=4))")
```

**è¨ˆç®—é‡æ¯”è¼ƒ**:

| æ‰‹æ³• | è¨ˆç®—é‡ | æ¬¡å…ƒä¾å­˜æ€§ |
|:-----|:------|:-----------|
| Sinkhorn | $O(n^2 \varepsilon^{-1})$ | $O(d)$ï¼ˆã‚³ã‚¹ãƒˆè¡Œåˆ—è¨ˆç®—ï¼‰ |
| Sliced Wasserstein | $O(Ln \log n)$ | $O(Ld)$ï¼ˆå°„å½±ï¼‰|
| çœŸã®W2ï¼ˆç·šå½¢è¨ˆç”»ï¼‰ | $O(n^3 \log n)$ | $O(d)$ |

$L=100$, $n=1000$, $d=50$ ã®ã¨ã: Sliced $\ll$ Sinkhorn $\ll$ ç·šå½¢è¨ˆç”»

### 6.4 Unbalanced OT & Partial OT â€” è³ªé‡ä¿å­˜ã®ç·©å’Œ

**å‹•æ©Ÿ**: å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã¯ $\int d\mu \neq \int d\nu$ï¼ˆç·è³ªé‡ãŒç•°ãªã‚‹ï¼‰ã“ã¨ãŒã‚ã‚‹ã€‚

**Unbalanced OT** [^11]: è³ªé‡ã®ç”Ÿæˆãƒ»æ¶ˆæ»…ã‚’è¨±ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’èª²ã™:

$$
\min_{\gamma, \mu', \nu'} \left\{ \int c \, d\gamma + \tau_1 D(\mu' \| \mu) + \tau_2 D(\nu' \| \nu) \right\}
$$

ã“ã“ã§ $D$ ã¯ç™ºæ•£ï¼ˆä¾‹: KL divergenceï¼‰ã€$\tau_1, \tau_2$ ã¯ãƒšãƒŠãƒ«ãƒ†ã‚£é‡ã¿ã€‚

**Partial OT**: ä¸€éƒ¨ã®è³ªé‡ã ã‘ã‚’è¼¸é€ï¼ˆoutlier robustnessï¼‰:

$$
\min_{\gamma} \left\{ \int c \, d\gamma \;\middle|\; \gamma \in \Pi(\mu, \nu), \; \gamma(\mathbb{R}^d \times \mathbb{R}^d) \leq \alpha \right\}
$$

$\alpha < 1$ ã§ã€Œå…¨ä½“ã® $\alpha$ å‰²ã ã‘è¼¸é€ã€ã€‚

**å¿œç”¨**: Domain adaptationï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã§ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã ã‘ãƒãƒƒãƒãƒ³ã‚°ï¼‰

### 6.5 Gromov-Wassersteinè·é›¢ â€” ç•°ãªã‚‹ç©ºé–“é–“ã®OT

**å•é¡Œ**: $\mu \in \mathcal{P}(X)$, $\nu \in \mathcal{P}(Y)$ ã§ $X, Y$ ãŒ **ç•°ãªã‚‹è¨ˆé‡ç©ºé–“** ã®ã¨ãã€$c(\boldsymbol{x}, \boldsymbol{y})$ ã‚’ã©ã†å®šç¾©ã™ã‚‹ï¼Ÿ

**Gromov-Wasserstein (GW)** [^12]: ç©ºé–“å†…ã®è·é›¢æ§‹é€ ã‚’æ¯”è¼ƒ:

$$
\text{GW}(\mu, \nu) = \inf_{\gamma \in \Pi(\mu, \nu)} \int_{X \times X \times Y \times Y} |d_X(\boldsymbol{x}, \boldsymbol{x}') - d_Y(\boldsymbol{y}, \boldsymbol{y}')|^2 \, d\gamma(\boldsymbol{x}, \boldsymbol{y}) \, d\gamma(\boldsymbol{x}', \boldsymbol{y}')
$$

ã€Œ$\boldsymbol{x}$ ã¨ $\boldsymbol{x}'$ ã®è·é›¢ã€ã¨ã€Œå¯¾å¿œã™ã‚‹ $\boldsymbol{y}, \boldsymbol{y}'$ ã®è·é›¢ã€ã®å·®ã‚’æœ€å°åŒ–ã€‚

**å¿œç”¨**: ã‚°ãƒ©ãƒ•ãƒãƒƒãƒãƒ³ã‚°ã€åˆ†å­ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’

**è¨ˆç®—**: Sinkhornã®æ‹¡å¼µï¼ˆGromov-Sinkhornï¼‰ãŒå¯èƒ½ã ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¯ $O(n^4)$ â†’ è¿‘ä¼¼æ‰‹æ³•ãŒå¿…è¦ã€‚

### 6.6 OTã¨Flow Matchingã®æ¥ç¶š â€” ç¬¬36å›ã¸ã®å¸ƒçŸ³

**Rectified Flow** [^4] ã®æ ¸å¿ƒ: ãƒã‚¤ã‚ºåˆ†å¸ƒ $\pi_0$ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $\pi_1$ ã¸ã® **æœ€çŸ­çµŒè·¯** ã‚’å­¦ç¿’ã€‚

$$
\frac{dx_t}{dt} = v_t(x_t), \quad x_0 \sim \pi_0, \; x_1 \sim \pi_1
$$

**OTã¨ã®é–¢ä¿‚**:
1. **Optimal coupling**: $\gamma^* \in \Pi(\pi_0, \pi_1)$ ãŒW2æœ€é©è§£
2. **ç›´ç·šè£œé–“**: $x_t = (1-t) x_0 + t x_1$ where $(x_0, x_1) \sim \gamma^*$
3. **é€Ÿåº¦å ´**: $v_t(x_t) = x_1 - x_0 = \mathbb{E}[(x_1 - x_0) \mid x_t]$

ã“ã‚ŒãŒ **OT-CFM** (Optimal Transport Conditional Flow Matching) [^13] ã®å®šå¼åŒ–ã ã€‚

**Rectified Flow = OT Map**:

2å›ã®rectificationï¼ˆå†è¨“ç·´ï¼‰ã«ã‚ˆã‚Šã€ãƒ•ãƒ­ãƒ¼ãŒã€Œç›´ç·šåŒ–ã€ã•ã‚Œã‚‹:

$$
\lim_{k \to \infty} \mathbb{E}[\text{æ›²ç‡}] \to 0
$$

ã“ã‚Œã¯Wassersteinæ¸¬åœ°ç·šï¼ˆMcCannè£œé–“ï¼‰ã¸ã®åæŸã‚’æ„å‘³ã™ã‚‹ã€‚

**ç¬¬36å›ã§ã®å±•é–‹**:
- Diffusion ODE = Wassersteinå‹¾é…æµã®é›¢æ•£åŒ–
- Score Matching = W2ã®å‹¾é…ã‚’å­¦ç¿’
- Flow Matching = W2æ¸¬åœ°ç·šã‚’ç›´æ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–
- 3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®çµ±ä¸€çš„ç†è§£

:::details æŠ€è¡“è©³ç´°: OT-CFMã®æ¡ä»¶ä»˜ãç¢ºç‡ãƒ‘ã‚¹

OT-CFMã¯æ¡ä»¶ä»˜ãç¢ºç‡ãƒ‘ã‚¹ $p_t(x \mid x_0, x_1)$ ã‚’ä½¿ã†:

$$
p_t(x) = \int_{\mathbb{R}^d \times \mathbb{R}^d} p_t(x \mid x_0, x_1) \, d\gamma(x_0, x_1)
$$

ç›´ç·šè£œé–“ã®å ´åˆ:

$$
p_t(x \mid x_0, x_1) = \delta(x - ((1-t) x_0 + t x_1))
$$

é€Ÿåº¦å ´:

$$
u_t(x) = \int \frac{dx_t}{dt} \, p_t(x_t \mid x_0, x_1) \frac{p_t(x_t \mid x_0, x_1)}{p_t(x_t)} \, d\gamma(x_0, x_1)
$$

ç°¡ç•¥åŒ–ã™ã‚‹ã¨:

$$
u_t(x) = \mathbb{E}_{(x_0, x_1) \sim \gamma \mid x_t = x} [x_1 - x_0]
$$

ã“ã‚Œã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ $v_\theta(x, t)$ ã§è¿‘ä¼¼ã—ã€Flow Matchingæå¤±ã§è¨“ç·´:

$$
\mathcal{L}(\theta) = \mathbb{E}_{t, (x_0, x_1) \sim \gamma, x_t} [\| v_\theta(x_t, t) - (x_1 - x_0) \|^2]
$$
:::

### 6.7 æ¨å¥¨æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹

#### æ•™ç§‘æ›¸

| æ›¸ç± | è‘—è€… | ãƒ¬ãƒ™ãƒ« | URL |
|:-----|:-----|:------|:----|
| **Optimal Transport: Old and New** | CÃ©dric Villani | ä¸Šç´šï¼ˆæ¸¬åº¦è«–å‰æï¼‰ | [Link](https://www.cedricvillani.org/) |
| **Computational Optimal Transport** | Gabriel PeyrÃ© & Marco Cuturi | ä¸­ç´šï¼ˆå®Ÿè£…é‡è¦–ï¼‰ | [arXiv](https://arxiv.org/abs/1803.00567) |
| **Optimal Transport for Applied Mathematicians** | Filippo Santambrogio | ä¸­ç´š | Springer |
| **Topics in Optimal Transportation** | CÃ©dric Villani | ä¸Šç´šï¼ˆFields Medalæ¥­ç¸¾ï¼‰ | AMS |

#### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

| ãƒªã‚½ãƒ¼ã‚¹ | å†…å®¹ | URL |
|:---------|:-----|:----|
| POT Library | Python Optimal Transport | [pythonot.github.io](https://pythonot.github.io/) |
| OTT-JAX | JAXå®Ÿè£…ï¼ˆGPUé«˜é€Ÿï¼‰ | [github.com/ott-jax](https://github.com/ott-jax/ott) |
| Optimal Transport Notes | Cambridgeå¤§å­¦è¬›ç¾©ãƒãƒ¼ãƒˆ | [DAMTP](https://www.damtp.cam.ac.uk/research/cia/) |
| Cuturi's Tutorial | NeurIPS 2019 Tutorial | [YouTube](https://www.youtube.com/) |

#### å®Ÿè£…ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæ¯”è¼ƒ

| ãƒ©ã‚¤ãƒ–ãƒ©ãƒª | è¨€èª | GPU | Sinkhorn | Neural OT | æ´»ç™ºåº¦ |
|:----------|:-----|:----|:---------|:----------|:-------|
| **POT** | Python | âŒ | âœ… | âœ… | â˜…â˜…â˜…â˜…â˜… |
| **OTT-JAX** | Python (JAX) | âœ… | âœ… | âœ… | â˜…â˜…â˜…â˜…â˜† |
| **GeomLoss** | PyTorch | âœ… | âœ… | âŒ | â˜…â˜…â˜…â˜†â˜† |
| **geomloss.jl** | Julia | âœ… | âœ… | âŒ | â˜…â˜…â˜†â˜†â˜† |
| **optimal-transport-rs** | Rust | âŒ | âœ… | âŒ | â˜…â˜†â˜†â˜†â˜† |

**æ¨å¥¨**: ç ”ç©¶ãªã‚‰OTT-JAXï¼ˆGPUé«˜é€Ÿï¼‰ã€æ•™è‚²ãªã‚‰POTï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå……å®Ÿï¼‰

### 6.8 ç”¨èªé›†ï¼ˆGlossaryï¼‰

| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| æœ€é©è¼¸é€ | Optimal Transport | ç¢ºç‡æ¸¬åº¦é–“ã®æœ€å°ã‚³ã‚¹ãƒˆè¼¸é€å•é¡Œ |
| Mongeå•é¡Œ | Monge Problem | æ±ºå®šè«–çš„è¼¸é€å†™åƒã‚’æ±‚ã‚ã‚‹å…ƒã®å®šå¼åŒ– |
| Kantorovichç·©å’Œ | Kantorovich Relaxation | ç¢ºç‡çš„è¼¸é€è¨ˆç”»ã‚’è¨±ã™ç·©å’Œ |
| Wassersteinè·é›¢ | Wasserstein Distance | OTã‚³ã‚¹ãƒˆã«ã‚ˆã‚‹ç¢ºç‡æ¸¬åº¦é–“ã®è·é›¢ |
| åŒå¯¾æ€§ | Duality | ä¸»å•é¡Œã¨åŒå¯¾å•é¡Œã®ç­‰ä¾¡æ€§ |
| Sinkhornç®—æ³• | Sinkhorn Algorithm | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–OTã®é«˜é€Ÿè§£æ³• |
| Gibbsã‚«ãƒ¼ãƒãƒ« | Gibbs Kernel | $K = \exp(-C/\varepsilon)$ |
| Push-forwardæ¸¬åº¦ | Push-forward Measure | å†™åƒã«ã‚ˆã‚‹æ¸¬åº¦ã®å¤‰æ› |
| çµåˆæ¸¬åº¦ | Coupling | 2ã¤ã®å‘¨è¾ºåˆ†å¸ƒã‚’æŒã¤çµåˆåˆ†å¸ƒ |
| McCannè£œé–“ | McCann Interpolation | Wassersteinæ¸¬åœ°ç·š |
| Displacement Convexity | å¤‰ä½å‡¸æ€§ | Wassersteinç©ºé–“ã§ã®å‡¸æ€§ |
| JKO scheme | Jordan-Kinderlehrer-Otto | Wassersteinå‹¾é…æµã®é›¢æ•£åŒ– |
| ICNN | Input-Convex NN | å…¥åŠ›ã«é–¢ã—ã¦å‡¸ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ |
| Sliced Wasserstein | ã‚¹ãƒ©ã‚¤ã‚¹ãƒ‰Wasserstein | 1æ¬¡å…ƒå°„å½±ã®å¹³å‡ã«ã‚ˆã‚‹OTè¿‘ä¼¼ |
| Unbalanced OT | éå¹³è¡¡OT | è³ªé‡ä¿å­˜ã‚’ç·©å’Œã—ãŸOT |
| Gromov-Wasserstein | ã‚°ãƒ­ãƒ¢ãƒ•-Wasserstein | ç•°ãªã‚‹è¨ˆé‡ç©ºé–“é–“ã®OT |

---

### 6.9 ä»Šå›ã®å­¦ç¿’å†…å®¹

**3ã¤ã®æ ¸å¿ƒ**:

1. **Kantorovichç·©å’Œ**: Mongeã®æ±ºå®šè«–çš„è¼¸é€ â†’ ç¢ºç‡çš„è¼¸é€è¨ˆç”» $\gamma \in \Pi(\mu, \nu)$ ã¸ã®ç·©å’Œã«ã‚ˆã‚Šã€ç·šå½¢è¨ˆç”»å•é¡Œã¨ã—ã¦å®šå¼åŒ–å¯èƒ½ã«

2. **Wassersteinè·é›¢**: ç¢ºç‡æ¸¬åº¦ç©ºé–“ $(\mathcal{P}_2(\mathbb{R}^d), W_2)$ ã¯è·é›¢ç©ºé–“ã§ã‚ã‚Šã€å¼±åæŸã‚’ãƒ¡ãƒˆãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€‚KL divergenceã§ã¯æ‰ãˆã‚‰ã‚Œãªã„ã€Œåˆ†å¸ƒã®å¹¾ä½•å­¦ã€ã‚’è¡¨ç¾

3. **Sinkhornç®—æ³•**: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ– $-\varepsilon H(\gamma)$ ã«ã‚ˆã‚Šã€è¨ˆç®—é‡ã‚’ $O(n^3) \to O(n^2 \varepsilon^{-1})$ ã«å‰Šæ¸›ã€‚æ©Ÿæ¢°å­¦ç¿’ã§ã®å®Ÿç”¨åŒ–ã®éµ

**å®Ÿè£…ã§å­¦ã‚“ã ã“ã¨**:

- **Julia**: å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã¨è¡Œåˆ—æ¼”ç®—ã®è¦ªå’Œæ€§ã«ã‚ˆã‚Šã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ãŒ1:1å¯¾å¿œ
- **Rust**: ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–ã¨Rayonä¸¦åˆ—åŒ–ã«ã‚ˆã‚Šã€ãƒãƒƒãƒå‡¦ç†ã§5xé«˜é€ŸåŒ–
- **ICNN**: å‡¸æ€§åˆ¶ç´„ã«ã‚ˆã‚ŠMonge Mapã®æ§‹é€ ã‚’ç›´æ¥å­¦ç¿’å¯èƒ½

**ç†è«–ã¨å¿œç”¨ã®ã¤ãªãŒã‚Š**:

```mermaid
graph TD
    OT[Optimal Transport<br/>Theory] --> W1[Wâ‚ è·é›¢]
    OT --> W2[Wâ‚‚ è·é›¢]
    W1 --> WGAN[WGAN<br/>Lipschitzåˆ¶ç´„]
    W2 --> FM[Flow Matching<br/>æ¸¬åœ°ç·šå­¦ç¿’]
    W2 --> Diff[Diffusion<br/>å‹¾é…æµ]
    OT --> Sink[Sinkhorn<br/>é«˜é€Ÿè¨ˆç®—]
    Sink --> DA[Domain Adaptation]
    Sink --> Barycenter[Barycenterå•é¡Œ]

    style OT fill:#ffeb3b
    style WGAN fill:#e1f5fe
    style FM fill:#c8e6c9
```

### 6.10 FAQ â€” ã‚ˆãã‚ã‚‹è³ªå•ã¨ç­”ãˆ

:::details Q1: ãªãœKL divergenceã§ã¯ãªãWassersteinè·é›¢ã‚’ä½¿ã†ã®ã‹ï¼Ÿ

**A**: KL divergenceã¯ã‚µãƒãƒ¼ãƒˆãŒé‡ãªã‚‰ãªã„ã¨ $+\infty$ ã«ãªã‚‹ã€‚ä¾‹ãˆã°:
- $\mu = \delta_0$ï¼ˆç‚¹è³ªé‡ï¼‰ã€$\nu = \delta_1$ ã®ã¨ãã€$D_{\text{KL}}(\mu \| \nu) = +\infty$
- ä¸€æ–¹ã€$W_2(\mu, \nu) = 1$ï¼ˆæœ‰é™ï¼‰

Wassersteinè·é›¢ã¯:
1. **å¼±åæŸã‚’ãƒ¡ãƒˆãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³**: åˆ†å¸ƒã®ã€Œè¿‘ã•ã€ã‚’ä½ç›¸çš„ã«æ­£ã—ãæ¸¬ã‚‹
2. **å‹¾é…ãŒå¸¸ã«å­˜åœ¨**: KLã ã¨å‹¾é…ãŒ $\infty$ ã«ãªã‚‹çŠ¶æ³ã§ã‚‚ã€W2ã¯æœ‰é™å‹¾é…
3. **å¹¾ä½•å­¦çš„ç›´æ„Ÿ**: ã€ŒåœŸã‚’å‹•ã‹ã™æœ€å°ã‚³ã‚¹ãƒˆã€ã¨ã„ã†ç‰©ç†çš„è§£é‡ˆ

GANã§ã¯ã“ã‚ŒãŒè‡´å‘½çš„ã§ã€ã‚µãƒãƒ¼ãƒˆãŒé›¢ã‚ŒãŸåˆæœŸæ®µéšã§KLãƒ™ãƒ¼ã‚¹ã®æå¤±ã¯å­¦ç¿’ãŒé€²ã¾ãªã„ã€‚WGANãŒã“ã‚Œã‚’è§£æ±ºã—ãŸã€‚
:::

:::details Q2: Sinkhornã®$\varepsilon$ã¯ã©ã†é¸ã¶ã¹ãã‹ï¼Ÿ

**A**: ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚‹:

| $\varepsilon$ | ç²¾åº¦ | é€Ÿåº¦ | æ•°å€¤å®‰å®šæ€§ |
|:-------------|:-----|:-----|:-----------|
| å°ï¼ˆ0.001-0.01ï¼‰ | é«˜ï¼ˆçœŸã®OTã«è¿‘ã„ï¼‰ | é…ï¼ˆåå¾©æ•°å¤šï¼‰ | ä¸å®‰å®šï¼ˆlog-domainå¿…é ˆï¼‰ |
| ä¸­ï¼ˆ0.05-0.1ï¼‰ | ä¸­ | é€Ÿ | å®‰å®š |
| å¤§ï¼ˆ0.5-1.0ï¼‰ | ä½ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …æ”¯é…ï¼‰ | éå¸¸ã«é€Ÿ | éå¸¸ã«å®‰å®š |

**æ¨å¥¨**:
- **å­¦ç¿’ä¸­**: $\varepsilon = 0.05 \sim 0.1$ï¼ˆé€Ÿåº¦ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
- **è©•ä¾¡æ™‚**: $\varepsilon = 0.01$ï¼ˆã‚ˆã‚Šæ­£ç¢ºãªW2æ¨å®šï¼‰
- **ä¸å®‰å®šãªã‚‰**: log-domainã«åˆ‡ã‚Šæ›¿ãˆ + $\varepsilon$ ã‚’å¤§ããã™ã‚‹

**è‡ªå‹•èª¿æ•´**: Annealing â€” å­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚Œ $\varepsilon$ ã‚’æ¸›ã‚‰ã™ï¼ˆ $\varepsilon_t = \varepsilon_0 \cdot 0.99^t$ ãªã©ï¼‰
:::

:::details Q3: ICNNã¯ãªãœå‡¸é–¢æ•°ã§ãªã„ã¨ã„ã‘ãªã„ã®ã‹ï¼Ÿ

**A**: Brenierå®šç† [^2] ã«ã‚ˆã‚‹:

> $\mu, \nu$ ãŒ $\mathbb{R}^d$ ä¸Šã®çµ¶å¯¾é€£ç¶šãªç¢ºç‡æ¸¬åº¦ãªã‚‰ã€W2æœ€é©è¼¸é€å†™åƒ $T^*$ ã¯ä¸€æ„ã«å­˜åœ¨ã—ã€$T^* = \nabla \phi$ ã®å½¢ã‚’æŒã¤ã€‚ã“ã“ã§ $\phi$ ã¯å‡¸é–¢æ•°ã€‚

ã¤ã¾ã‚Š:
- æœ€é©è¼¸é€å†™åƒã¯ã€Œå‡¸é–¢æ•°ã®å‹¾é…ã€ã¨ã—ã¦å¿…ãšæ›¸ã‘ã‚‹
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ $\phi$ ã‚’å­¦ç¿’ â†’ ãã®å‹¾é… $\nabla \phi$ ãŒè¼¸é€å†™åƒ

**å‡¸æ€§ã‚’ä¿è¨¼ã—ãªã„ã¨**: $\nabla \phi$ ãŒæœ€é©è¼¸é€å†™åƒã«ãªã‚‰ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã€ç†è«–çš„ä¿è¨¼ãŒå¤±ã‚ã‚Œã‚‹ã€‚

**å®Ÿè£…ã®å·¥å¤«**: é‡ã¿ã‚’éè² ã«åˆ¶ç´„ï¼ˆsoftplusé©ç”¨ï¼‰+ å‡¸æ´»æ€§åŒ–é–¢æ•°ï¼ˆReLUï¼‰ã§æ§‹æˆçš„ã«å‡¸æ€§ã‚’ä¿è¨¼ã€‚
:::

:::details Q4: Flow Matchingã¨OTã¯ã©ã†é•ã†ã®ã‹ï¼Ÿ

**A**: Flow Matchingã¯ã€ŒOTã‚’åˆ©ç”¨ã—ãŸç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´æ‰‹æ³•ã€:

| é …ç›® | Optimal Transport | Flow Matching |
|:-----|:------------------|:--------------|
| ç›®çš„ | 2ã¤ã®åˆ†å¸ƒé–“ã®æœ€å°ã‚³ã‚¹ãƒˆè¼¸é€ | ãƒã‚¤ã‚ºâ†’ãƒ‡ãƒ¼ã‚¿ã¸ã®é€£ç¶šå†™åƒã‚’å­¦ç¿’ |
| å…¥åŠ› | ç¢ºç‡æ¸¬åº¦ $\mu, \nu$ | ã‚µãƒ³ãƒ—ãƒ« $x_0 \sim \pi_0, x_1 \sim \pi_{\text{data}}$ |
| å‡ºåŠ› | è¼¸é€è¨ˆç”» $\gamma^*$ ã¾ãŸã¯å†™åƒ $T^*$ | é€Ÿåº¦å ´ $v_\theta(x, t)$ |
| é–¢ä¿‚ | ç†è«–çš„åŸºç›¤ | å¿œç”¨æ‰‹æ³• |

**OT-CFM**: OTæœ€é©è¼¸é€è¨ˆç”» $\gamma^*$ ã‚’ä½¿ã£ã¦Flow Matchingã®æ¡ä»¶ä»˜ããƒ‘ã‚¹ã‚’æ§‹ç¯‰ â†’ ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå­¦ç¿’

**Rectified Flow**: OTå†™åƒãŒã€Œç›´ç·šçš„ã€ã§ã‚ã‚‹ã“ã¨ã‚’åˆ©ç”¨ã—ã€ãƒ•ãƒ­ãƒ¼ã‚’ç›´ç·šåŒ– â†’ æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—å‰Šæ¸›

è©³ç´°ã¯ **ç¬¬36å› Flow Matchingçµ±ä¸€ç†è«–** ã§å±•é–‹ã™ã‚‹ã€‚
:::

:::details Q5: Julia vs Rustã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ

**A**: ã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹:

| ã‚¿ã‚¹ã‚¯ | æ¨å¥¨è¨€èª | ç†ç”± |
|:-------|:---------|:-----|
| ç ”ç©¶ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚° | Julia | REPLé§†å‹•é–‹ç™ºã€æ•°å¼â†”ã‚³ãƒ¼ãƒ‰1:1ã€é«˜é€Ÿ |
| æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆå˜ä½“ï¼‰ | Rust | ãƒ¡ãƒ¢ãƒªå®‰å…¨ã€ãƒã‚¤ãƒŠãƒªé…å¸ƒã€ã‚¼ãƒ­GC |
| æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆPythonçµ±åˆï¼‰ | Julia | PyCall/PythonCallã§ç°¡å˜é€£æº |
| å¤§è¦æ¨¡ãƒãƒƒãƒå‡¦ç† | Rust | Rayonä¸¦åˆ—åŒ–ã€SIMDæœ€é©åŒ– |
| GPUè¨ˆç®— | Julia (CUDA.jl) | Python (JAX/PyTorch) ã‚ˆã‚Šç›´æ„Ÿçš„ |

**æœ¬è¬›ç¾©ã®é¸æŠ**:
- **ä¸»è»¸ã¯Julia**: OTç†è«–ã®æ•°å¼ãŒç›´æ¥ã‚³ãƒ¼ãƒ‰ã«ãªã‚‹ç¾ã—ã•
- **Rustã¯è£œå®Œ**: æ€§èƒ½ãŒæœ¬å½“ã«å¿…è¦ãªéƒ¨åˆ†ã®ã¿ï¼ˆSinkhorn SIMDã€C-ABI FFIï¼‰

**å®Ÿå‹™ã§ã®æ£²ã¿åˆ†ã‘**: Juliaï¼ˆã‚«ãƒ¼ãƒãƒ«å®Ÿè£…ï¼‰ + Pythonï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼APIï¼‰ + Rustï¼ˆé«˜é€Ÿãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼‰ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆãŒç†æƒ³ã€‚
:::

### 6.11 1é€±é–“ã®å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| æ—¥ | å†…å®¹ | æ™‚é–“ | é”æˆç›®æ¨™ |
|:---|:-----|:-----|:---------|
| **Day 1** | Zone 0-2ï¼ˆä½“é¨“ãƒ»ç›´æ„Ÿï¼‰ | 1æ™‚é–“ | OTã®ã€Œä½•ã€ã€Œãªãœã€ã‚’ç†è§£ |
| **Day 2** | Zone 3å‰åŠï¼ˆÂ§3.1-3.2ï¼‰ | 1.5æ™‚é–“ | Mongeå•é¡Œã€Kantorovichç·©å’Œã®æ•°å¼ã‚’è¿½ãˆã‚‹ |
| **Day 3** | Zone 3å¾ŒåŠï¼ˆÂ§3.3-3.4ï¼‰ | 2æ™‚é–“ | Wassersteinè·é›¢ã€åŒå¯¾æ€§ã‚’å°å‡ºã§ãã‚‹ |
| **Day 4** | Zone 3çµ‚ç›¤ï¼ˆÂ§3.5-3.6ï¼‰ | 1.5æ™‚é–“ | Sinkhornç®—æ³•ã€å¹¾ä½•å­¦çš„è¦–ç‚¹ã‚’ç†è§£ |
| **Day 5** | Zone 4ï¼ˆå®Ÿè£…ï¼‰ | 2æ™‚é–“ | Juliaã§å®Œå…¨å®Ÿè£…ã€Rust SIMDè©¦ã™ |
| **Day 6** | Zone 5ï¼ˆå®Ÿé¨“ï¼‰ | 1.5æ™‚é–“ | å…¨å®Ÿé¨“ã‚’å†ç¾ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ |
| **Day 7** | Zone 6-7ï¼ˆç™ºå±•ãƒ»å¾©ç¿’ï¼‰ | 1.5æ™‚é–“ | è«–æ–‡ã‚µãƒ¼ãƒ™ã‚¤ã€FAQã§ã‚®ãƒ£ãƒƒãƒ—åŸ‹ã‚ |

**åˆè¨ˆ**: 11æ™‚é–“ï¼ˆæœ¬è¬›ç¾©ã®æ¨™æº–å­¦ç¿’æ™‚é–“ï¼‰

**çŸ­ç¸®ç‰ˆï¼ˆ6æ™‚é–“ï¼‰**: Day 1 + Day 2 + Day 5ï¼ˆä½“é¨“ãƒ»åŸºç¤æ•°å¼ãƒ»å®Ÿè£…ã®ã¿ï¼‰

### 6.12 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```julia
# Save your progress
struct LectureProgress
    lecture_num::Int
    zones_completed::Vector{Int}
    experiments_done::Vector{String}
    understanding_score::Dict{String, Int}  # 1-5 scale
end

# Self-assessment
my_progress = LectureProgress(
    11,  # Lecture 11
    [0, 1, 2, 3, 4, 5, 6, 7],  # completed zones
    ["gaussian_w2", "sinkhorn_convergence", "rust_parallel", "icnn"],
    Dict(
        "monge_problem" => 4,
        "kantorovich_relaxation" => 5,
        "wasserstein_distance" => 4,
        "kantorovich_rubinstein_duality" => 3,
        "sinkhorn_algorithm" => 5,
        "mccann_interpolation" => 2,
        "icnn" => 4,
        "flow_matching_connection" => 3
    )
)

# Calculate completion
function completion_rate(prog::LectureProgress)
    zone_completion = length(prog.zones_completed) / 8 * 0.4
    exp_completion = length(prog.experiments_done) / 4 * 0.3
    understanding = mean(values(prog.understanding_score)) / 5 * 0.3
    return zone_completion + exp_completion + understanding
end

rate = completion_rate(my_progress)
println("Overall completion: $(round(rate * 100, digits=1))%")

if rate >= 0.8
    println("âœ… Ready for Lecture 12: GANç†è«–")
elseif rate >= 0.6
    println("âš ï¸ Review Zone 3 (æ•°å¼ä¿®è¡Œ) before moving on")
else
    println("âŒ Restart from Zone 0 recommended")
end
```

### 6.13 æ¬¡å›äºˆå‘Š: ç¬¬12å› GAN â€” æ•µå¯¾çš„ç”Ÿæˆã®ç†è«–

**Lecture 12ã®ãƒ†ãƒ¼ãƒ**: Generative Adversarial Networksï¼ˆGANï¼‰ã®å®Œå…¨ç†è«–

**å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼**:
1. **GANå®šå¼åŒ–**: Minmaxã‚²ãƒ¼ãƒ ã€Jensen-Shannon divergenceã€Nashå‡è¡¡
2. **ç†è«–çš„å›°é›£**: ãƒ¢ãƒ¼ãƒ‰å´©å£Šã€å‹¾é…æ¶ˆå¤±ã€è¨“ç·´ä¸å®‰å®šæ€§ã®æ•°ç†
3. **WGAN**: æœ¬è¬›ç¾©ã§å­¦ã‚“ã Kantorovich-RubinsteinåŒå¯¾æ€§ãŒã„ã‹ã«GANã‚’å®‰å®šåŒ–ã™ã‚‹ã‹
4. **ç™ºå±•å‹**: StyleGANã€Progressive GANã€Diffusion-GANãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
5. **å®Ÿè£…**: Juliaã§minimalãªGAN + Rustã§WGANé«˜é€ŸåŒ–

**æœ¬è¬›ç¾©ã¨ã®æ¥ç¶š**:
- WGANã® **1-Lipschitzåˆ¶ç´„** = Kantorovich-RubinsteinåŒå¯¾æ€§ï¼ˆÂ§3.4ï¼‰
- **Gradient penalty** = $\mathbb{E}[(\|\nabla_{\boldsymbol{x}} D\| - 1)^2]$ ã®ç†è«–çš„æ­£å½“åŒ–
- **Spectral normalization** = Lipschitzå®šæ•°ã®åˆ¶å¾¡æ‰‹æ³•

**æº–å‚™ã™ã¹ãã“ã¨**:
- ç¬¬6å›ã€Œæƒ…å ±ç†è«–ã€ã®Jensen-Shannon divergenceå¾©ç¿’
- ç¬¬7å›ã€Œæœ€å°¤æ¨å®šã€ã®MLEã¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®é–¢ä¿‚ç¢ºèª
- æœ¬è¬›ç¾©ï¼ˆç¬¬11å›ï¼‰ã®Â§3.4 Kantorovich-RubinsteinåŒå¯¾æ€§ã‚’å®Œå…¨ç†è§£

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ ãŠç–²ã‚Œã•ã¾ã§ã—ãŸï¼ 240å¹´ã®æ­´å²ã‚’æŒã¤æœ€é©è¼¸é€ç†è«–ã‚’ã€Mongeå•é¡Œã‹ã‚‰æœ€æ–°ã®Flow Matchingã¸ã®æ¥ç¶šã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ã¾ã—ãŸã€‚æ¬¡å›ã®GANã§ã€ã“ã®ç†è«–ãŒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å®Ÿè·µã§ã©ã†æ´»ãã‚‹ã‹ã‚’ç›®æ’ƒã—ã¾ã™ã€‚
:::

---

### 6.14 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œæœ€é©è¼¸é€ç†è«–ã¯ã€ç¢ºç‡åˆ†å¸ƒã‚’ã€ç‚¹ã€ã§ã¯ãªãã€å¹¾ä½•å­¦ã€ã¨ã—ã¦æ‰±ã†ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã ã€‚ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿åˆ†å¸ƒã‚‚Wassersteinç©ºé–“ã®1ç‚¹ã¨è¦‹ãªã›ã°ã€ãƒ¢ãƒ‡ãƒ«ã®ã€æ±åŒ–èª¤å·®ã€ã‚’OTè·é›¢ã§æ¸¬å®šã§ãã‚‹ã®ã§ã¯ãªã„ã‹ï¼Ÿã€**

**æŒ‘ç™ºçš„ãªå•ã„ã‹ã‘**:

1. **ãƒ¢ãƒ‡ãƒ«ç©ºé–“ã®å¹¾ä½•å­¦**: 2ã¤ã®NNãƒ¢ãƒ‡ãƒ« $\theta_1, \theta_2$ ãŒåŒã˜ã‚¿ã‚¹ã‚¯ã‚’è§£ãã¨ãã€ãã®ã€Œè¿‘ã•ã€ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ $\|\theta_1 - \theta_2\|$ ã§æ¸¬ã‚‹ã®ã¯é©åˆ‡ã‹ï¼Ÿ ãã‚Œã¨ã‚‚ã€ä¸¡è€…ãŒèª˜å°ã™ã‚‹ **å‡ºåŠ›åˆ†å¸ƒé–“ã®Wassersteinè·é›¢** $W_2(p_{\theta_1}, p_{\theta_2})$ ã§æ¸¬ã‚‹ã¹ãã‹ï¼Ÿ

2. **æ±åŒ–ã®æ–°å®šç¾©**: è¨“ç·´åˆ†å¸ƒ $p_{\text{train}}$ ã¨ãƒ†ã‚¹ãƒˆåˆ†å¸ƒ $p_{\text{test}}$ ã®ã€Œãšã‚Œã€ã‚’ $W_2(p_{\text{train}}, p_{\text{test}})$ ã§å®šé‡åŒ–ã™ã‚Œã°ã€ã€Œæ±åŒ–èª¤å·® = OTè·é›¢ã®é–¢æ•°ã€ã¨ã„ã†ç†è«–ã‚’æ§‹ç¯‰ã§ãã‚‹ã‹ï¼Ÿ æ—¢å­˜ã®PAC learningã‚„VCæ¬¡å…ƒç†è«–ã‚’è¶…ãˆã‚‰ã‚Œã‚‹ã‹ï¼Ÿ

3. **é€£ç¶šå­¦ç¿’ = æ¸¬åœ°ç·š**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®è¨“ç·´éç¨‹ $\{\theta_t\}_{t=0}^T$ ã‚’ã€å‡ºåŠ›åˆ†å¸ƒç©ºé–“ $\{p_{\theta_t}\}$ ã§ã®Wassersteinæ¸¬åœ°ç·šã¨ã—ã¦è¦‹ç›´ã›ã°ã€ã€Œæœ€é©ãªå­¦ç¿’çµŒè·¯ã€ã‚’äº‹å‰è¨ˆç®—ã§ãã‚‹ã‹ï¼Ÿ ã¤ã¾ã‚Šã€å‹¾é…é™ä¸‹æ³•ã¯ **Wassersteinå‹¾é…æµã®é›¢æ•£åŒ–** ã¨ã—ã¦å†è§£é‡ˆã§ãã‚‹ã‹ï¼Ÿ

**æ­´å²çš„é€†èª¬**:

- 1781å¹´ã€Mongeã¯ã€ŒåœŸã‚’é‹ã¶ã€ã¨ã„ã†åœŸæœ¨å·¥å­¦ã®å•é¡Œã‚’è§£ã“ã†ã¨ã—ãŸ
- 2017å¹´ã€WGANã¯ã€Œãƒ”ã‚¯ã‚»ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€ã¨ã„ã†å•é¡Œã«OTã‚’é©ç”¨ã—ãŸ
- 2022å¹´ã€Rectified Flowã¯ã€Œãƒã‚¤ã‚ºã‚’ç”»åƒã«å¤‰æ›ã™ã‚‹ã€çµŒè·¯ã‚’OTã§æœ€é©åŒ–ã—ãŸ
- 2026å¹´ã€æ¬¡ã®å¿œç”¨ã¯ **ã€Œå­¦ç¿’ãã®ã‚‚ã®ã‚’OTã§æœ€é©åŒ–ã™ã‚‹ã€** ã“ã¨ã‹ã‚‚ã—ã‚Œãªã„

**ã‚ãªãŸã¸ã®å•ã„**:

ã‚‚ã—ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®è¨“ç·´ãŒã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®æœ€é©åŒ–ã€ã§ã¯ãªãã€Œåˆ†å¸ƒç©ºé–“ã®Wassersteinæ¸¬åœ°ç·šã‚’è¾¿ã‚‹éç¨‹ã€ã ã¨ã—ãŸã‚‰ã€ç¾åœ¨ã®Adamã‚„SGDã¯ **æœ€é©è¼¸é€çš„ã«æœ€é©** ãªã®ã‹ï¼Ÿ ãã‚Œã¨ã‚‚ã€ã‚‚ã£ã¨ã€Œç›´ç·šçš„ãªã€å­¦ç¿’çµŒè·¯ãŒå­˜åœ¨ã™ã‚‹ã®ã‹ï¼Ÿ

:::details ãƒ’ãƒ³ãƒˆ: Neural Tangent Kernel (NTK) ã¨ã®é–¢ä¿‚

NTKç†è«–ã§ã¯ã€ç„¡é™å¹…NNã®è¨“ç·´ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¯ã‚«ãƒ¼ãƒãƒ«å›å¸°ã¨ã—ã¦è§£æã•ã‚Œã‚‹ã€‚ä¸€æ–¹ã€OTè¦–ç‚¹ã§ã¯è¨“ç·´ã¯ã€ŒåˆæœŸåˆ†å¸ƒ $p_{\theta_0}$ ã‹ã‚‰æœ€é©åˆ†å¸ƒ $p_{\theta^*}$ ã¸ã®è¼¸é€ã€ã¨è¦‹ãªã›ã‚‹ã€‚

2ã¤ã®è¦–ç‚¹ã‚’çµ±åˆã™ã‚‹ã¨:
- NTK = ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®å±€æ‰€çš„å¹¾ä½•å­¦
- OT = å‡ºåŠ›åˆ†å¸ƒç©ºé–“ã®å¤§åŸŸçš„å¹¾ä½•å­¦

ä¸¡è€…ã‚’æ©‹æ¸¡ã—ã™ã‚‹ç†è«–ï¼ˆä¾‹: "Wasserstein Proximal Gradient" ã‚„ "Optimal Transport for Meta-Learning"ï¼‰ãŒ2024-2025å¹´ã«ç™»å ´ã—ã¤ã¤ã‚ã‚‹ã€‚ç¬¬25å›ã€Œãƒ¡ã‚¿å­¦ç¿’ã€ã§ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’å†è¨ªã™ã‚‹ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Monge, G. (1781). *MÃ©moire sur la thÃ©orie des dÃ©blais et des remblais*. Histoire de l'AcadÃ©mie Royale des Sciences de Paris.

[^2]: Brenier, Y. (1991). *Polar factorization and monotone rearrangement of vector-valued functions*. Communications on Pure and Applied Mathematics, 44(4), 375-417.
@[card](https://doi.org/10.1002/cpa.3160440402)

[^3]: Arjovsky, M., Chintala, S., & Bottou, L. (2017). *Wasserstein GAN*. ICML 2017.
@[card](https://arxiv.org/abs/1701.07875)

[^4]: Liu, X., Gong, C., & Liu, Q. (2022). *Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow*. ICLR 2023.
@[card](https://arxiv.org/abs/2209.03003)

[^5]: Jordan, R., Kinderlehrer, D., & Otto, F. (1998). *The variational formulation of the Fokkerâ€“Planck equation*. SIAM Journal on Mathematical Analysis, 29(1), 1-17.

[^6]: Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. (2017). *Improved Training of Wasserstein GANs*. NeurIPS 2017.
@[card](https://arxiv.org/abs/1704.00028)

[^7]: Miyato, T., Kataoka, T., Koyama, M., & Yoshida, Y. (2018). *Spectral Normalization for Generative Adversarial Networks*. ICLR 2018.
@[card](https://arxiv.org/abs/1802.05957)

[^8]: Makkuva, A., Taghvaei, A., Oh, S., & Lee, J. (2019). *Optimal transport mapping via input convex neural networks*. ICML 2020.
@[card](https://arxiv.org/abs/1908.10962)

[^9]: Cuturi, M. (2013). *Sinkhorn Distances: Lightspeed Computation of Optimal Transport*. NeurIPS 2013.
@[card](https://arxiv.org/abs/1306.0895)

[^10]: Bonneel, N., Rabin, J., PeyrÃ©, G., & Pfister, H. (2015). *Sliced and Radon Wasserstein barycenters of measures*. Journal of Mathematical Imaging and Vision, 51(1), 22-45.
@[card](https://arxiv.org/abs/1503.01452)

[^11]: Chizat, L., PeyrÃ©, G., Schmitzer, B., & Vialard, F.-X. (2018). *Scaling algorithms for unbalanced optimal transport problems*. Mathematics of Computation, 87(314), 2563-2609.
@[card](https://arxiv.org/abs/1607.05816)

[^12]: MÃ©moli, F. (2011). *Gromovâ€“Wasserstein distances and the metric approach to object matching*. Foundations of Computational Mathematics, 11(4), 417-487.

[^13]: Tong, A., Malkin, N., Fatras, K., Atanackovic, L., Zhang, Y., Huguet, G., Wolf, G., & Bengio, Y. (2023). *Improving and generalizing flow-based generative models with minibatch optimal transport*. TMLR 2024.
@[card](https://arxiv.org/abs/2302.00482)

### æ•™ç§‘æ›¸

- Villani, C. (2003). *Topics in Optimal Transportation*. American Mathematical Society.
- Santambrogio, F. (2015). *Optimal Transport for Applied Mathematicians*. BirkhÃ¤user.
- Ambrosio, L., Gigli, N., & SavarÃ©, G. (2008). *Gradient Flows in Metric Spaces and in the Space of Probability Measures*. BirkhÃ¤user.

---

## è¨˜æ³•è¦ç´„

æœ¬è¬›ç¾©ã§ä½¿ç”¨ã—ãŸè¨˜æ³•ã®ä¸€è¦§:

| è¨˜æ³• | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|
| $\mathcal{P}(\mathbb{R}^d)$ | $\mathbb{R}^d$ ä¸Šã®ç¢ºç‡æ¸¬åº¦ã®ç©ºé–“ | Zone 3.1 |
| $\mathcal{P}_p(\mathbb{R}^d)$ | $p$-æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæœ‰é™ãªç¢ºç‡æ¸¬åº¦ã®ç©ºé–“ | Zone 3.3 |
| $\mu, \nu$ | ç¢ºç‡æ¸¬åº¦ï¼ˆsource, targetï¼‰ | Zone 0 |
| $T_\sharp \mu$ | $T$ ã«ã‚ˆã‚‹ $\mu$ ã®push-forwardæ¸¬åº¦ | Zone 3.1 |
| $\gamma \in \Pi(\mu, \nu)$ | å‘¨è¾ºåˆ†å¸ƒãŒ $\mu, \nu$ ã§ã‚ã‚‹çµåˆæ¸¬åº¦ï¼ˆè¼¸é€è¨ˆç”»ï¼‰ | Zone 3.2 |
| $c(\boldsymbol{x}, \boldsymbol{y})$ | ç‚¹ $\boldsymbol{x}$ ã‹ã‚‰ $\boldsymbol{y}$ ã¸ã®è¼¸é€ã‚³ã‚¹ãƒˆ | Zone 3.1 |
| $W_p(\mu, \nu)$ | $p$-Wassersteinè·é›¢ | Zone 3.3 |
| $W_c(\mu, \nu)$ | ã‚³ã‚¹ãƒˆé–¢æ•° $c$ ã«ã‚ˆã‚‹Kantorovich OTã‚³ã‚¹ãƒˆ | Zone 3.2 |
| $\phi \oplus \psi$ | $\phi(\boldsymbol{x}) + \psi(\boldsymbol{y})$ï¼ˆåŒå¯¾å¤‰æ•°ã®å’Œï¼‰ | Zone 3.2 |
| $\phi^c$ | $\phi$ ã® $c$-transform | Zone 3.4 |
| $\|\cdot\|_L$ | Lipschitzå®šæ•° | Zone 3.4 |
| $H(\gamma)$ | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ $-\int \gamma \log(\gamma / (\mu \otimes \nu)) d\gamma$ | Zone 3.5 |
| $\varepsilon$ | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | Zone 3.5 |
| $K = \exp(-C/\varepsilon)$ | Gibbsã‚«ãƒ¼ãƒãƒ« | Zone 3.5 |
| $u, v$ | Sinkhornã®åŒå¯¾å¤‰æ•°ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ | Zone 3.5 |
| $\mu_t$ | McCannè£œé–“ï¼ˆ$t \in [0,1]$ï¼‰ | Zone 3.6 |
| $\text{SW}_p$ | Sliced Wassersteinè·é›¢ | Zone 6.3 |
| $\nabla \phi$ | å‡¸é–¢æ•° $\phi$ ã®å‹¾é…ï¼ˆæœ€é©è¼¸é€å†™åƒï¼‰ | Zone 4.5 |

**ä¸€èˆ¬è¨˜æ³•**:
- ãƒ™ã‚¯ãƒˆãƒ«: $\boldsymbol{x}, \boldsymbol{y}$ ï¼ˆå¤ªå­—ï¼‰
- è¡Œåˆ—: $A, \Sigma$ ï¼ˆå¤§æ–‡å­—ã‚¤ã‚¿ãƒªãƒƒã‚¯ï¼‰
- é–¢æ•°: $f, \phi, \psi$ ï¼ˆå°æ–‡å­—ã‚¤ã‚¿ãƒªãƒƒã‚¯ï¼‰
- æ¸¬åº¦: $\mu, \nu, \gamma$ ï¼ˆã‚®ãƒªã‚·ãƒ£å°æ–‡å­—ï¼‰
- ç©ºé–“: $\mathcal{P}, \Pi$ ï¼ˆã‚«ãƒªã‚°ãƒ©ãƒ•ã‚£ãƒ¼å¤§æ–‡å­—ï¼‰

---

**Lecture 11å®Œäº†ã€‚æ¬¡å›ç¬¬12å›ã€ŒGAN â€” æ•µå¯¾çš„ç”Ÿæˆã®ç†è«–ã€ã§ãŠä¼šã„ã—ã¾ã—ã‚‡ã†ã€‚**

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
