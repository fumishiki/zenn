---
title: "ç¬¬29å›: RAG (æ¤œç´¢å¢—å¼·ç”Ÿæˆ): 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
slug: "ml-lecture-29-part2"
emoji: "ğŸ”"
type: "tech"
topics: ["machinelearning", "rag", "vectordatabase", "rust", "rust"]
published: true
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---
> **ğŸ“– å‰ç·¨ï¼ˆç†è«–ç·¨ï¼‰**: [ç¬¬29å›å‰ç·¨: RAGç†è«–ç·¨](./ml-lecture-29-part1) | **â† ç†è«–ãƒ»æ•°å¼ã‚¾ãƒ¼ãƒ³ã¸**


## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rust/Rust/Elixirã§RAGã‚’å®Œå…¨å®Ÿè£…

### 4.1 ğŸ¦€ Rust: HNSW Vector Databaseå®Ÿè£…

#### 4.1.1 HNSWã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŸç†

**HNSW (Hierarchical Navigable Small World)** [^6] ã¯ã€è¿‘ä¼¼æœ€è¿‘å‚æ¢ç´¢ï¼ˆANNï¼‰ã®æœ€é«˜å³°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚

**Key Idea**: éšå±¤çš„ãªã‚°ãƒ©ãƒ•æ§‹é€ ã§ã€ç²—ã„å±¤ã‹ã‚‰ç´°ã‹ã„å±¤ã¸ã¨æ¢ç´¢ã‚’çµã‚Šè¾¼ã‚€ã€‚

```mermaid
graph TD
    L2["Layer 2<br/>(æœ€ç²—)"] --> L1["Layer 1"]
    L1 --> L0["Layer 0<br/>(å…¨ãƒ‡ãƒ¼ã‚¿)"]

    L2 -.Entry Point.-> N1["Node 1"]
    N1 -.Navigate.-> N2["Node 2"]
    N2 -.Descend.-> L1

    style L0 fill:#c8e6c9
```

**éšå±¤æ§‹é€ **:

$$
\begin{aligned}
&\text{Layer } L: \text{ å°‘æ•°ã®ãƒãƒ¼ãƒ‰ï¼ˆé è·é›¢ã‚¸ãƒ£ãƒ³ãƒ—ï¼‰} \\
&\text{Layer } L-1: \text{ ã‚ˆã‚Šå¤šãã®ãƒãƒ¼ãƒ‰} \\
&\vdots \\
&\text{Layer } 0: \text{ å…¨ãƒãƒ¼ãƒ‰ï¼ˆé«˜ç²¾åº¦æ¢ç´¢ï¼‰}
\end{aligned}
$$

**æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
1. Entry point: æœ€ä¸Šå±¤Lã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ
2. Greedy search: ç¾åœ¨å±¤ã§æœ€è¿‘å‚ã‚’æ¢ç´¢
3. Descend: ã‚ˆã‚Šä¸‹ã®å±¤ã¸ç§»å‹•
4. Repeat 2-3 until Layer 0
5. Return: Layer 0ã§ã®æœ€è¿‘å‚kå€‹
```

**è¨ˆç®—é‡**:

| Phase | Complexity | èª¬æ˜ |
|:------|:-----------|:-----|
| **Indexæ§‹ç¯‰** | $O(N \log N)$ | Nå€‹ã®ãƒ™ã‚¯ãƒˆãƒ«æŒ¿å…¥ |
| **æ¢ç´¢** | $O(\log N)$ | éšå±¤çš„æ¢ç´¢ |
| **ç²¾åº¦** | 95-99% | Recall@k |

#### 4.1.2 Rustã«ã‚ˆã‚‹åŸºæœ¬å®Ÿè£…

```rust
// HNSW Implementation in Rust
use std::collections::{BinaryHeap, HashMap, HashSet};
use std::cmp::Ordering;

// Vector type (f32 for efficiency)
type Vector = Vec<f32>;

// Distance metric: Euclidean L2
fn l2_distance(a: &Vector, b: &Vector) -> f32 {
    a.iter()
        .zip(b.iter())
        .map(|(x, y)| (x - y).powi(2))
        .sum::<f32>()
        .sqrt()
}

// Cosine similarity (for normalized vectors)
fn cosine_similarity(a: &Vector, b: &Vector) -> f32 {
    let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let norm_a: f32 = a.iter().map(|x| x.powi(2)).sum::<f32>().sqrt();
    let norm_b: f32 = b.iter().map(|x| x.powi(2)).sum::<f32>().sqrt();

    dot / (norm_a * norm_b)
}

// Node in HNSW graph
#[derive(Clone)]
struct Node {
    id: usize,
    vector: Vector,
    // Neighbors at each layer: layer -> neighbor_ids
    neighbors: HashMap<usize, Vec<usize>>,
}

impl Node {
    fn new(id: usize, vector: Vector) -> Self {
        Self {
            id,
            vector,
            neighbors: HashMap::new(),
        }
    }
}

// Priority queue element for search
#[derive(Clone, Copy)]
struct SearchCandidate {
    id: usize,
    distance: f32,
}

impl Eq for SearchCandidate {}

impl PartialEq for SearchCandidate {
    fn eq(&self, other: &Self) -> bool {
        self.distance == other.distance
    }
}

impl Ord for SearchCandidate {
    fn cmp(&self, other: &Self) -> Ordering {
        // Min-heap (reverse order)
        other.distance.partial_cmp(&self.distance).unwrap()
    }
}

impl PartialOrd for SearchCandidate {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

// HNSW Index
struct HNSWIndex {
    nodes: Vec<Node>,
    entry_point: Option<usize>,
    max_layers: usize,
    m: usize,          // Max connections per layer
    ef_construction: usize, // Size of dynamic candidate list during construction
    ml: f32,           // Normalization factor for layer assignment
}

impl HNSWIndex {
    fn new(m: usize, ef_construction: usize, max_layers: usize) -> Self {
        Self {
            nodes: Vec::new(),
            entry_point: None,
            max_layers,
            m,
            ef_construction,
            ml: 1.0 / (m as f32).ln(),
        }
    }

    // Assign random layer for new node
    fn random_layer(&self) -> usize {
        let uniform = rand::random::<f32>();
        let layer = (-uniform.ln() * self.ml).floor() as usize;
        layer.min(self.max_layers - 1)
    }

    // Insert vector into index
    fn insert(&mut self, vector: Vector) {
        let id = self.nodes.len();
        let layer = self.random_layer();

        let mut node = Node::new(id, vector.clone());

        // Initialize neighbors for each layer
        for l in 0..=layer {
            node.neighbors.insert(l, Vec::new());
        }

        if self.entry_point.is_none() {
            // First node
            self.entry_point = Some(id);
            self.nodes.push(node);
            return;
        }

        // Search for nearest neighbors at each layer
        let entry = self.entry_point.unwrap();
        let mut current = entry;

        // Traverse from top layer to insertion layer
        for l in (layer + 1..self.max_layers).rev() {
            current = self.search_layer(&vector, current, 1, l)[0].id;
        }

        // Insert and connect at each layer from insertion layer to 0
        for l in (0..=layer).rev() {
            let candidates = self.search_layer(&vector, current, self.ef_construction, l);

            // Select M nearest neighbors
            let m = if l == 0 { self.m * 2 } else { self.m };
            let neighbors: Vec<usize> = candidates
                .iter()
                .take(m)
                .map(|c| c.id)
                .collect();

            node.neighbors.insert(l, neighbors.clone());

            // Bidirectional links
            for &neighbor_id in &neighbors {
                if let Some(neighbor) = self.nodes.get_mut(neighbor_id) {
                    if let Some(neighbor_list) = neighbor.neighbors.get_mut(&l) {
                        neighbor_list.push(id);

                        // Prune if exceeds max connections
                        if neighbor_list.len() > m {
                            neighbor_list.truncate(m);
                        }
                    }
                }
            }

            current = candidates[0].id;
        }

        // Update entry point if new node has higher layer
        if layer > self.max_layer() {
            self.entry_point = Some(id);
        }

        self.nodes.push(node);
    }

    // Get maximum layer of current index
    fn max_layer(&self) -> usize {
        self.nodes
            .iter()
            .flat_map(|n| n.neighbors.keys())
            .max()
            .copied()
            .unwrap_or(0)
    }

    // Search at a specific layer
    fn search_layer(
        &self,
        query: &Vector,
        entry_point: usize,
        ef: usize,
        layer: usize,
    ) -> Vec<SearchCandidate> {
        let mut visited = HashSet::new();
        let mut candidates = BinaryHeap::new();
        let mut w = BinaryHeap::new(); // Working set

        let entry_dist = l2_distance(query, &self.nodes[entry_point].vector);
        candidates.push(SearchCandidate {
            id: entry_point,
            distance: entry_dist,
        });
        w.push(SearchCandidate {
            id: entry_point,
            distance: entry_dist,
        });
        visited.insert(entry_point);

        while let Some(c) = candidates.pop() {
            if c.distance > w.peek().unwrap().distance {
                break;
            }

            // Explore neighbors
            if let Some(neighbors) = self.nodes[c.id].neighbors.get(&layer) {
                for &neighbor_id in neighbors {
                    if visited.insert(neighbor_id) {
                        let dist = l2_distance(query, &self.nodes[neighbor_id].vector);

                        if dist < w.peek().unwrap().distance || w.len() < ef {
                            candidates.push(SearchCandidate {
                                id: neighbor_id,
                                distance: dist,
                            });
                            w.push(SearchCandidate {
                                id: neighbor_id,
                                distance: dist,
                            });

                            if w.len() > ef {
                                w.pop();
                            }
                        }
                    }
                }
            }
        }

        w.into_sorted_vec()
    }

    // Search for k nearest neighbors
    fn search(&self, query: &Vector, k: usize, ef: usize) -> Vec<(usize, f32)> {
        if self.entry_point.is_none() {
            return Vec::new();
        }

        let entry = self.entry_point.unwrap();
        let mut current = entry;

        // Traverse from top to layer 1
        for l in (1..=self.max_layer()).rev() {
            current = self.search_layer(query, current, 1, l)[0].id;
        }

        // Search at layer 0 with larger ef
        let candidates = self.search_layer(query, current, ef.max(k), 0);

        candidates
            .into_iter()
            .take(k)
            .map(|c| (c.id, c.distance))
            .collect()
    }
}
```

#### 4.1.3 qdrantçµ±åˆ â€” Production-ready Vector DB

**qdrant** [^7] ã¯Rustè£½ã®é«˜æ€§èƒ½ãƒ™ã‚¯ãƒˆãƒ«DBã§ã€Productionç’°å¢ƒã§åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚

```rust
// qdrant integration example
use qdrant_client::{client::QdrantClient, qdrant::{
    CreateCollection, Distance, VectorParams, SearchPoints, PointStruct,
}};

async fn qdrant_example() -> Result<(), Box<dyn std::error::Error>> {
    // Connect to qdrant server
    let client = QdrantClient::from_url("http://localhost:6334").build()?;

    // Create collection
    client
        .create_collection(&CreateCollection {
            collection_name: "documents".to_string(),
            vectors_config: Some(VectorParams {
                size: 384, // Embedding dimension
                distance: Distance::Cosine as i32,
                ..Default::default()
            }.into()),
            ..Default::default()
        })
        .await?;

    // Insert vectors
    let points = vec![
        PointStruct::new(
            1,
            vec![0.1, 0.2, 0.3, /* ... 384 dims */],
            serde_json::json!({
                "text": "Paris is the capital of France.",
                "category": "geography"
            }),
        ),
    ];

    client
        .upsert_points("documents", points, None)
        .await?;

    // Search
    let search_result = client
        .search_points(&SearchPoints {
            collection_name: "documents".to_string(),
            vector: vec![0.15, 0.25, 0.35, /* query vector */],
            limit: 10,
            with_payload: Some(true.into()),
            ..Default::default()
        })
        .await?;

    for point in search_result.result {
        println!("ID: {}, Score: {}", point.id.unwrap(), point.score);
    }

    Ok(())
}
```

**qdrant ã®å¼·ã¿**:

| Feature | Description |
|:--------|:------------|
| **HNSW Index** | 95-99% recall, $O(\log N)$ æ¢ç´¢ |
| **Filtering** | Payloadï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼‰ã§ã®äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° |
| **Horizontal Scaling** | Sharding + Replication |
| **Persistence** | WAL + Snapshot for durability |
| **Multi-tenancy** | Collectionåˆ†é›¢ |

#### 4.1.4 Chunkingæˆ¦ç•¥ã®å®Ÿè£…

**Chunking**: é•·æ–‡æ›¸ã‚’æ¤œç´¢å¯èƒ½ãªãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã€‚

##### Fixed-Size Chunking

```rust
fn fixed_size_chunking(text: &str, chunk_size: usize, overlap: usize) -> Vec<String> {
    let words: Vec<&str> = text.split_whitespace().collect();
    (0..words.len())
        .step_by(chunk_size - overlap)
        .map(|i| words[i..(i + chunk_size).min(words.len())].join(" "))
        .collect()
}

// Example
let text = "Paris is the capital of France. It is known for the Eiffel Tower. \
            Tokyo is the capital of Japan.";
let chunks = fixed_size_chunking(text, 10, 2);
for (i, chunk) in chunks.iter().enumerate() {
    println!("Chunk {}: {}", i, chunk);
}
```

##### Semantic Chunking

æ„å‘³çš„å¢ƒç•Œï¼ˆæ–‡ãƒ»æ®µè½ï¼‰ã§ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã€‚

```rust
fn semantic_chunking(text: &str, max_chunk_size: usize) -> Vec<String> {
    let sentences: Vec<&str> = text
        .split('.')
        .filter(|s| !s.trim().is_empty())
        .collect();

    let mut chunks = Vec::new();
    let mut current_chunk = String::new();

    for sentence in sentences {
        let sentence = sentence.trim();
        if current_chunk.len() + sentence.len() > max_chunk_size && !current_chunk.is_empty() {
            chunks.push(current_chunk.clone());
            current_chunk.clear();
        }
        current_chunk.push_str(sentence);
        current_chunk.push_str(". ");
    }

    if !current_chunk.is_empty() {
        chunks.push(current_chunk);
    }

    chunks
}
```

##### Sliding Window Chunking

ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’ä¿è¨¼ã—ã¤ã¤ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã€‚

```rust
fn sliding_window_chunking(tokens: &[String], window_size: usize, stride: usize) -> Vec<Vec<String>> {
    (0..tokens.len())
        .step_by(stride)
        .map(|i| &tokens[i..(i + window_size).min(tokens.len())])
        .filter(|chunk| chunk.len() >= window_size / 2)
        .map(|chunk| chunk.to_vec())
        .collect()
}
```

**Chunkingæˆ¦ç•¥ã®æ¯”è¼ƒ**:

| æˆ¦ç•¥ | é•·æ‰€ | çŸ­æ‰€ | é©ç”¨å ´é¢ |
|:-----|:-----|:-----|:---------|
| **Fixed-Size** | ã‚·ãƒ³ãƒ—ãƒ«ãƒ»é«˜é€Ÿ | æ„å‘³å¢ƒç•Œç„¡è¦– | å‡è³ªãªãƒ†ã‚­ã‚¹ãƒˆ |
| **Semantic** | æ„å‘³ä¿æŒ | å¯å¤‰é•· | æ–‡æ›¸ãƒ»è¨˜äº‹ |
| **Sliding Window** | æ–‡è„ˆä¿æŒ | å†—é•·æ€§é«˜ | ã‚³ãƒ¼ãƒ‰ãƒ»å¯¾è©± |

### 4.2 ğŸ¦€ Rust: BM25æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…

#### 4.2.1 ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã¨IDFè¨ˆç®—

```rust
use std::collections::{HashMap, HashSet};

// Tokenizer: å°æ–‡å­—åŒ– + ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
const STOPWORDS: &[&str] = &[
    "the", "is", "at", "which", "on", "a", "an", "and", "or", "of", "to", "in",
];

fn tokenize(text: &str) -> Vec<String> {
    text.to_lowercase()
        .chars()
        .map(|c| if c.is_alphanumeric() || c.is_whitespace() { c } else { ' ' })
        .collect::<String>()
        .split_whitespace()
        .filter(|w| !STOPWORDS.contains(w))
        .map(str::to_owned)
        .collect()
}

// Document corpus
struct Document {
    id: usize,
    text: String,
    tokens: Vec<String>,
}

fn build_corpus(texts: &[&str]) -> Vec<Document> {
    texts.iter().enumerate()
        .map(|(i, &text)| Document { id: i + 1, text: text.to_owned(), tokens: tokenize(text) })
        .collect()
}

// IDF: log((N - df + 0.5) / (df + 0.5))
fn compute_idf(corpus: &[Document]) -> HashMap<String, f64> {
    let n_docs = corpus.len() as f64;
    let mut doc_freq: HashMap<String, usize> = HashMap::new();
    for doc in corpus {
        let unique: HashSet<&str> = doc.tokens.iter().map(String::as_str).collect();
        for token in &unique {
            *doc_freq.entry(token.to_string()).or_default() += 1;
        }
    }
    doc_freq.into_iter()
        .map(|(term, df)| {
            let idf = ((n_docs - df as f64 + 0.5) / (df as f64 + 0.5)).ln();
            (term, idf)
        })
        .collect()
}
```

#### 4.2.2 BM25ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Ÿè£…

```rust
// BM25 parameters
struct BM25Params { k1: f64, b: f64 }
const DEFAULT_BM25: BM25Params = BM25Params { k1: 1.2, b: 0.75 };

fn bm25_score(
    query_tokens: &[String],
    doc: &Document,
    idf: &HashMap<String, f64>,
    avg_doc_len: f64,
    params: &BM25Params,
) -> f64 {
    let doc_len = doc.tokens.len() as f64;
    query_tokens.iter().map(|term| {
        let tf      = doc.tokens.iter().filter(|t| *t == term).count() as f64;
        let idf_val = idf.get(term).copied().unwrap_or(0.0);
        idf_val * (tf * (params.k1 + 1.0))
            / (tf + params.k1 * (1.0 - params.b + params.b * (doc_len / avg_doc_len)))
    }).sum()
}

// BM25 ranking
fn bm25_search(
    query: &str,
    corpus: &[Document],
    idf: &HashMap<String, f64>,
    top_k: usize,
    params: &BM25Params,
) -> Vec<(usize, f64)> {
    let query_tokens = tokenize(query);
    let avg_doc_len = corpus.iter().map(|d| d.tokens.len() as f64).sum::<f64>()
        / corpus.len() as f64;
    let mut scores: Vec<(usize, f64)> = corpus.iter()
        .map(|doc| (doc.id, bm25_score(&query_tokens, doc, idf, avg_doc_len, params)))
        .collect();
    scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
    scores.truncate(top_k);
    scores
}
```

#### 4.2.3 Dense Retrieval with Embeddings

```rust
// Simplified embedding (å®Ÿéš›ã¯Sentence-BERT via Python/ONNX)
fn simple_embedding(text: &str, dim: usize) -> Vec<f32> {
    let tokens = tokenize(text);
    let mut embedding = vec![0.0f32; dim];
    // TF-IDF based embedding (simplified)
    for token in &tokens {
        let idx = token.bytes().fold(0usize, |acc, b| acc.wrapping_mul(31).wrapping_add(b as usize)) % dim;
        embedding[idx] += 1.0;
    }
    // L2 normalize
    let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > 1e-8 { embedding.iter_mut().for_each(|x| *x /= norm); }
    embedding
}

// Cosine similarity
fn cosine_sim(a: &[f32], b: &[f32]) -> f32 {
    let dot: f32 = a.iter().zip(b).map(|(x, y)| x * y).sum();
    let na: f32  = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let nb: f32  = b.iter().map(|x| x * x).sum::<f32>().sqrt();
    dot / (na * nb + 1e-8)
}

// Dense retrieval
fn dense_search(
    query: &str,
    corpus: &[Document],
    embeddings: &[Vec<f32>],
    top_k: usize,
) -> Vec<(usize, f32)> {
    let query_emb = simple_embedding(query, 384);
    let mut scores: Vec<(usize, f32)> = corpus.iter()
        .zip(embeddings.iter())
        .map(|(doc, emb)| (doc.id, cosine_sim(&query_emb, emb)))
        .collect();
    scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
    scores.truncate(top_k);
    scores
}
```

#### 4.2.4 Hybrid Retrieval: BM25 + Dense with RRF

```rust
// Reciprocal Rank Fusion
fn reciprocal_rank_fusion(rankings: &[Vec<(usize, f64)>], k: usize) -> Vec<(usize, f64)> {
    let mut rrf_scores: HashMap<usize, f64> = HashMap::new();
    for ranking in rankings {
        for (rank, &(doc_id, _)) in ranking.iter().enumerate() {
            *rrf_scores.entry(doc_id).or_default() += 1.0 / (k + rank + 1) as f64;
        }
    }
    let mut result: Vec<(usize, f64)> = rrf_scores.into_iter().collect();
    result.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
    result
}

// Hybrid search pipeline
fn hybrid_search(
    query: &str,
    corpus: &[Document],
    idf: &HashMap<String, f64>,
    embeddings: &[Vec<f32>],
    top_k: usize,
) -> Vec<(usize, f64)> {
    // BM25 retrieval
    let bm25_results = bm25_search(query, corpus, idf, top_k * 2, &DEFAULT_BM25);
    // Dense retrieval
    let dense_results: Vec<(usize, f64)> = dense_search(query, corpus, embeddings, top_k * 2)
        .into_iter().map(|(id, s)| (id, s as f64)).collect();
    // RRF fusion
    let mut fused = reciprocal_rank_fusion(&[bm25_results, dense_results], 60);
    fused.truncate(top_k);
    fused
}
```

#### 4.2.5 Reranking with Cross-Encoder

```rust
// Simplified cross-encoder scoring (å®Ÿéš›ã¯BERTãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨)
fn cross_encoder_score(query: &str, doc_text: &str) -> f64 {
    let query_tokens: HashSet<String> = tokenize(query).into_iter().collect();
    tokenize(doc_text).iter().enumerate()
        .filter(|(_, token)| query_tokens.contains(*token))
        .map(|(i, _)| 1.0 / (1.0 + 0.1 * i as f64))
        .sum()
}

// Rerank top results
fn rerank(
    query: &str,
    corpus: &[Document],
    initial_ranking: &[(usize, f64)],
    top_k: usize,
) -> Vec<(usize, f64)> {
    // Score each candidate with cross-encoder
    let mut reranked: Vec<(usize, f64)> = initial_ranking.iter()
        .filter_map(|&(doc_id, _)| {
            corpus.iter().find(|d| d.id == doc_id)
                .map(|doc| (doc_id, cross_encoder_score(query, &doc.text)))
        })
        .collect();
    reranked.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
    reranked.truncate(top_k);
    reranked
}
```

#### 4.2.6 Complete RAG Pipeline in Rust

```rust
// End-to-end RAG pipeline
struct RAGPipeline {
    corpus: Vec<Document>,
    idf: HashMap<String, f64>,
    embeddings: Vec<Vec<f32>>,
}

impl RAGPipeline {
    fn new(texts: &[&str]) -> Self {
        let corpus     = build_corpus(texts);
        let idf        = compute_idf(&corpus);
        let embeddings = corpus.iter().map(|d| simple_embedding(&d.text, 384)).collect();
        RAGPipeline { corpus, idf, embeddings }
    }

    fn search(&self, query: &str, top_k: usize, use_rerank: bool) -> Vec<(usize, f64)> {
        // Stage 1: Hybrid retrieval (BM25 + Dense)
        let candidates = hybrid_search(query, &self.corpus, &self.idf, &self.embeddings, top_k * 3);
        // Stage 2: Reranking (optional)
        if use_rerank { rerank(query, &self.corpus, &candidates, top_k) }
        else          { candidates.into_iter().take(top_k).collect() }
    }
}

fn main() {
    let texts = [
        "Paris is the capital of France. It is known for the Eiffel Tower.",
        "Tokyo is the capital of Japan. It has a population of 14 million.",
        "Berlin is the capital of Germany. The Berlin Wall fell in 1989.",
        "London is the capital of England. Big Ben is a famous landmark.",
    ];
    let pipeline = RAGPipeline::new(&texts);
    let results  = pipeline.search("What is the capital of France?", 3, true);

    println!("Search Results:");
    for (i, (doc_id, score)) in results.iter().enumerate() {
        let doc = pipeline.corpus.iter().find(|d| d.id == *doc_id).unwrap();
        println!("{}. [Score: {:.3}] {}", i + 1, score, doc.text);
    }
}
```

### 4.3 ğŸ”® Elixir: åˆ†æ•£RAGã‚µãƒ¼ãƒ“ãƒ³ã‚°å®Ÿè£…

#### 4.3.1 GenServer ã«ã‚ˆã‚‹çŠ¶æ…‹ç®¡ç†

```elixir
# RAG Server with GenServer
defmodule RAG.Server do
  use GenServer
  require Logger

  # Client API

  def start_link(opts \\ []) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  def search(query, opts \\ []) do
    GenServer.call(__MODULE__, {:search, query, opts}, :infinity)
  end

  def index_documents(documents) do
    GenServer.cast(__MODULE__, {:index, documents})
  end

  # Server Callbacks

  @impl true
  def init(_opts) do
    state = %{
      documents: [],
      embeddings: %{},
      cache: %{},
      stats: %{searches: 0, cache_hits: 0}
    }

    {:ok, state}
  end

  @impl true
  def handle_call({:search, query, opts}, _from, state) do
    # Check cache first
    case Map.get(state.cache, query) do
      nil ->
        # Cache miss - perform search
        results = perform_search(query, state.documents, state.embeddings, opts)

        # Update cache
        new_cache = Map.put(state.cache, query, results)
        |> limit_cache_size(1000)  # LRU eviction

        new_state = state
        |> Map.update!(:stats, &Map.update!(&1, :searches, fn x -> x + 1 end))
        |> Map.put(:cache, new_cache)

        {:reply, {:ok, results}, new_state}

      cached_results ->
        # Cache hit
        new_state = Map.update!(state, :stats, &Map.update!(&1, :cache_hits, fn x -> x + 1 end))
        Logger.debug("Cache hit for query: #{query}")

        {:reply, {:ok, cached_results}, new_state}
    end
  end

  @impl true
  def handle_cast({:index, documents}, state) do
    # Index documents (compute embeddings, build index)
    embeddings = documents
    |> Enum.map(&{&1.id, compute_embedding(&1.text)})
    |> Map.new()

    new_state = state
    |> Map.put(:documents, documents)
    |> Map.put(:embeddings, embeddings)
    |> Map.put(:cache, %{})  # Clear cache on reindex

    Logger.info("Indexed #{length(documents)} documents")

    {:noreply, new_state}
  end

  # Helper functions

  defp perform_search(query, documents, embeddings, opts) do
    top_k = Keyword.get(opts, :top_k, 10)

    query_emb = compute_embedding(query)

    # Compute similarities
    results = Enum.map(documents, fn doc ->
      similarity = cosine_similarity(query_emb, embeddings[doc.id])
      %{doc_id: doc.id, text: doc.text, score: similarity}
    end)
    |> Enum.sort_by(& &1.score, :desc)
    |> Enum.take(top_k)

    results
  end

  defp compute_embedding(text) do
    # Call Python embedding service or use ONNX
    # Simplified: random embedding
    for _ <- 1..384, do: :rand.uniform()
  end

  defp cosine_similarity(a, b) do
    dot_product = Enum.zip(a, b) |> Enum.map(fn {x, y} -> x * y end) |> Enum.sum()
    norm_a = a |> Enum.map(&(&1 * &1)) |> Enum.sum() |> :math.sqrt()
    norm_b = b |> Enum.map(&(&1 * &1)) |> Enum.sum() |> :math.sqrt()
    dot_product / (norm_a * norm_b + 1.0e-8)
  end

  defp limit_cache_size(cache, max_size) do
    if map_size(cache) > max_size do
      # Simple LRU: remove oldest (first inserted)
      cache
      |> Enum.take(max_size)
      |> Map.new()
    else
      cache
    end
  end
end
```

#### 4.3.2 åˆ†æ•£æ¤œç´¢ with Task.async

```elixir
defmodule RAG.DistributedSearch do
  @moduledoc """
  Distributed RAG search across multiple nodes
  """

  def parallel_search(query, shards, opts \\ []) do
    timeout = Keyword.get(opts, :timeout, 5000)
    results =
      shards
      |> Task.async_stream(&search_shard(query, &1, opts),
           max_concurrency: length(shards), timeout: timeout)
      |> Enum.map(fn {:ok, r} -> r end)
    merge_results(results, opts)
  end

  defp search_shard(query, shard, opts) do
    # Call RAG.Server on specific node/shard
    case :rpc.call(shard.node, RAG.Server, :search, [query, opts]) do
      {:ok, results} -> results
      {:badrpc, reason} ->
        Logger.error("RPC error for shard #{shard.id}: #{inspect(reason)}")
        []
    end
  end

  defp merge_results(results_list, opts) do
    top_k = Keyword.get(opts, :top_k, 10)

    # Flatten and sort by score
    results_list
    |> List.flatten()
    |> Enum.sort_by(& &1.score, :desc)
    |> Enum.take(top_k)
  end
end
```

#### 4.3.3 ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡

```elixir
defmodule RAG.RateLimiter do
  use GenServer

  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  def check_rate(user_id) do
    GenServer.call(__MODULE__, {:check_rate, user_id})
  end

  @impl true
  def init(opts) do
    max_requests = Keyword.get(opts, :max_requests, 100)
    window_ms = Keyword.get(opts, :window_ms, 60_000)

    state = %{
      max_requests: max_requests,
      window_ms: window_ms,
      requests: %{}
    }

    # Periodic cleanup
    :timer.send_interval(window_ms, :cleanup)

    {:ok, state}
  end

  @impl true
  def handle_call({:check_rate, user_id}, _from, state) do
    now = System.monotonic_time(:millisecond)
    window_start = now - state.window_ms

    # Get user requests in current window
    user_requests = Map.get(state.requests, user_id, [])
    |> Enum.filter(fn timestamp -> timestamp >= window_start end)

    if length(user_requests) < state.max_requests do
      # Allow request
      new_requests = [now | user_requests]
      new_state = put_in(state.requests[user_id], new_requests)

      {:reply, :ok, new_state}
    else
      # Rate limit exceeded
      {:reply, {:error, :rate_limit_exceeded}, state}
    end
  end

  @impl true
  def handle_info(:cleanup, state) do
    now = System.monotonic_time(:millisecond)
    window_start = now - state.window_ms

    # Remove expired requests
    new_requests = state.requests
    |> Enum.map(fn {user_id, timestamps} ->
      {user_id, Enum.filter(timestamps, &(&1 >= window_start))}
    end)
    |> Enum.reject(fn {_user_id, timestamps} -> Enum.empty?(timestamps) end)
    |> Map.new()

    {:noreply, %{state | requests: new_requests}}
  end
end
```

#### 4.3.4 Production RAG Service

```elixir
defmodule RAG.Application do
  use Application

  def start(_type, _args) do
    children = [
      # RAG Server
      {RAG.Server, []},

      # Rate Limiter
      {RAG.RateLimiter, [max_requests: 100, window_ms: 60_000]},

      # HTTP API (Phoenix endpoint)
      RAG.Web.Endpoint,

      # Background indexer
      RAG.BackgroundIndexer
    ]

    opts = [strategy: :one_for_one, name: RAG.Supervisor]
    Supervisor.start_link(children, opts)
  end
end

# HTTP Endpoint (Phoenix controller)
defmodule RAG.Web.SearchController do
  use Phoenix.Controller

  def search(conn, %{"query" => query} = params) do
    user_id = get_session(conn, :user_id)
    top_k   = Map.get(params, "top_k", 10)

    with :ok <- RAG.RateLimiter.check_rate(user_id),
         {:ok, results} <- RAG.Server.search(query, top_k: top_k) do
      json(conn, %{query: query, results: results})
    else
      {:error, :rate_limit_exceeded} ->
        conn |> put_status(:too_many_requests) |> json(%{error: "Rate limit exceeded"})
      {:error, reason} ->
        conn |> put_status(:internal_server_error) |> json(%{error: reason})
    end
  end
end
```


---

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Rustã®HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Ÿè£…ã«ãŠã„ã¦ã€éšå±¤ã‚°ãƒ©ãƒ•æ§‹é€ ãŒANNï¼ˆè¿‘ä¼¼æœ€è¿‘å‚æ¢ç´¢ï¼‰ã®è¨ˆç®—é‡ã‚’O(log N)ã«æŠ‘ãˆã‚‹ä»•çµ„ã¿ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. Elixirã®GenStage + Broadwayã«ã‚ˆã‚‹åˆ†æ•£RAGã‚µãƒ¼ãƒ“ãƒ³ã‚°ã§ã€ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡ãŒãªãœã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå®‰å®šåŒ–ã«ä¸å¯æ¬ ã‹ã€‚

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” RAGè©•ä¾¡ã¨SmolVLM2çµ±åˆ

### 5.1 RAGè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### 5.1.1 Retrieval Metrics

**Precision@k**: Top-kä»¶ä¸­ã®é–¢é€£æ–‡æ›¸ã®å‰²åˆ

$$
\text{Precision@}k = \frac{\text{\# of relevant docs in top-}k}{k}
$$

**Recall@k**: å…¨é–¢é€£æ–‡æ›¸ä¸­ã€Top-kä»¶ã«å«ã¾ã‚Œã‚‹å‰²åˆ

$$
\text{Recall@}k = \frac{\text{\# of relevant docs in top-}k}{\text{\# of all relevant docs}}
$$

**Mean Reciprocal Rank (MRR)**: æœ€åˆã®é–¢é€£æ–‡æ›¸ã®ãƒ©ãƒ³ã‚¯ã®é€†æ•°ã®å¹³å‡

$$
\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
$$

**Normalized Discounted Cumulative Gain (NDCG@k)**:

$$
\begin{aligned}
\text{DCG@}k &= \sum_{i=1}^k \frac{2^{\text{rel}_i} - 1}{\log_2(i + 1)} \\
\text{NDCG@}k &= \frac{\text{DCG@}k}{\text{IDCG@}k}
\end{aligned}
$$

ã“ã“ã§ $\text{IDCG@}k$ ã¯ç†æƒ³çš„ãªé †ä½ã§ã®DCGã€‚

#### 5.1.2 Generation Metrics

**Context Relevance**: æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¯ã‚¨ãƒªã«é–¢é€£ã—ã¦ã„ã‚‹ã‹

```rust
// Context Relevance Score
fn context_relevance(query: &str, contexts: &[&str]) -> f64 {
    let qt: HashSet<String> = tokenize(query).into_iter().collect();
    let sum: f64 = contexts.iter()
        .map(|ctx| {
            let ct: HashSet<String> = tokenize(ctx).into_iter().collect();
            ct.intersection(&qt).count() as f64 / (qt.len() as f64 + 1e-8)
        })
        .sum();
    sum / contexts.len().max(1) as f64
}
```

**Answer Faithfulness**: ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿ å®Ÿã‹

$$
\text{Faithfulness} = \frac{\text{\# of claims supported by context}}{\text{\# of total claims}}
$$

**Answer Relevance**: ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã‚¯ã‚¨ãƒªã«é–¢é€£ã—ã¦ã„ã‚‹ã‹

```rust
// Answer Relevancy: cosine similarity between query and answer embeddings
fn answer_relevance(query_emb: &[f32], answer_emb: &[f32]) -> f32 {
    cosine_sim(query_emb, answer_emb)
}
```

#### 5.1.3 RAGAS Framework

**RAGAS** [^8] (RAG Assessment): RAGè©•ä¾¡ã®çµ±åˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**4ã¤ã®ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:

| Metric | èª¬æ˜ | å¼ |
|:-------|:-----|:---|
| **Context Precision** | é–¢é€£æ–‡æ›¸ãŒä¸Šä½ã«ãƒ©ãƒ³ã‚¯ã•ã‚Œã¦ã„ã‚‹ã‹ | $\frac{\sum_{k=1}^K v_k \cdot \text{Precision@}k}{K}$ |
| **Context Recall** | å…¨é–¢é€£æ–‡æ›¸ãŒæ¤œç´¢ã•ã‚ŒãŸã‹ | $\frac{\text{# retrieved relevant}}{\text{# total relevant}}$ |
| **Faithfulness** | å›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ”¯æŒã•ã‚Œã¦ã„ã‚‹ã‹ | $\frac{\text{# supported claims}}{\text{# total claims}}$ |
| **Answer Relevancy** | å›ç­”ãŒã‚¯ã‚¨ãƒªã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ | $\text{cos}(\text{emb}_q, \text{emb}_a)$ |

**çµ±åˆã‚¹ã‚³ã‚¢**:

$$
\text{RAGAS Score} = \left( \text{Precision} \times \text{Recall} \times \text{Faithfulness} \times \text{Relevancy} \right)^{1/4}
$$

å¹¾ä½•å¹³å‡ã§å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒãƒ©ãƒ³ã‚¹ã€‚

#### 5.1.4 Rustå®Ÿè£…: RAGASè©•ä¾¡

```rust
struct RAGASEvaluator {
    pipeline: RAGPipeline,
}

impl RAGASEvaluator {
    /// Evaluate single query â†’ (context_precision, context_recall, faithfulness, answer_relevancy, ragas_score, answer)
    fn evaluate_query(
        &self,
        query: &str,
        ground_truth_docs: &HashSet<usize>,
    ) -> (f64, f64, f64, f64, f64, String) {
        let retrieved = self.pipeline.search(query, 10, true);
        let retrieved_ids: HashSet<usize> = retrieved.iter().map(|&(id, _)| id).collect();

        // Context Precision
        let precision_scores: Vec<f64> = (1..=retrieved.len()).map(|k| {
            let top_k_ids: HashSet<usize> = retrieved[..k].iter().map(|&(id, _)| id).collect();
            if ground_truth_docs.contains(&retrieved[k - 1].0) {
                top_k_ids.intersection(ground_truth_docs).count() as f64 / k as f64
            } else { 0.0 }
        }).collect();
        let context_precision = precision_scores.iter().sum::<f64>() / precision_scores.len().max(1) as f64;

        // Context Recall
        let context_recall = retrieved_ids.intersection(ground_truth_docs).count() as f64
            / (ground_truth_docs.len() as f64 + 1e-8);

        // Faithfulness (simplified)
        let retrieved_texts: Vec<&str> = retrieved.iter()
            .filter_map(|&(id, _)| self.pipeline.corpus.iter().find(|d| d.id == id).map(|d| d.text.as_str()))
            .collect();
        let answer = generate_answer(query, &retrieved_texts);
        let faithfulness = compute_faithfulness(&answer, &retrieved_texts);

        // Answer Relevancy (cosine similarity)
        let query_emb  = simple_embedding(query, 384);
        let answer_emb = simple_embedding(&answer, 384);
        let answer_relevancy = cosine_sim(&query_emb, &answer_emb) as f64;

        // RAGAS Score (geometric mean)
        let ragas_score = (context_precision * context_recall * faithfulness * answer_relevancy).powf(0.25);

        (context_precision, context_recall, faithfulness, answer_relevancy, ragas_score, answer)
    }
}

fn compute_faithfulness(answer: &str, contexts: &[&str]) -> f64 {
    let claims: Vec<&str> = answer.split(". ").collect();
    let supported = claims.iter().filter(|&&claim| {
        contexts.iter().any(|ctx| {
            ctx.to_lowercase().contains(&claim.to_lowercase()) || token_overlap(claim, ctx) > 0.5
        })
    }).count();
    supported as f64 / (claims.len() as f64 + 1e-8)
}

fn token_overlap(text1: &str, text2: &str) -> f64 {
    let t1: HashSet<String> = tokenize(text1).into_iter().collect();
    let t2: HashSet<String> = tokenize(text2).into_iter().collect();
    let overlap = t1.intersection(&t2).count();
    overlap as f64 / (t1.union(&t2).count() as f64 + 1e-8)
}

fn generate_answer(query: &str, contexts: &[&str]) -> String {
    // Simulated LLM generation (å®Ÿéš›ã¯LLMå‘¼ã³å‡ºã—)
    let combined = contexts[..contexts.len().min(3)].join(" ");
    format!("Based on the context, {}, the answer to '{}' is found in the documents.", combined, query)
}
```

### 5.2 SmolVLM2-256M + RAGçµ±åˆæ¼”ç¿’

#### 5.2.1 ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«RAGã®è¨­è¨ˆ

**ã‚·ãƒŠãƒªã‚ª**: ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢

```mermaid
graph LR
    Q["Query<br/>(Text/Image)"] --> E["Encoder<br/>(SmolVLM2)"]
    E --> QE["Query Embedding"]
    QE --> VDB["Vector DB<br/>(Image+Text)"]
    VDB --> R["Retrieved<br/>Multimodal Docs"]
    R --> G["Generator<br/>(SmolVLM2)"]
    Q --> G
    G --> A["Answer"]
```

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

1. **Indexing**: ç”»åƒ + ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’SmolVLM2ã§Embedding â†’ Vector DBã«ä¿å­˜
2. **Retrieval**: ã‚¯ã‚¨ãƒªã‚’Embedding â†’ Top-kç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢
3. **Generation**: æ¤œç´¢çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦SmolVLM2ã§ç”Ÿæˆ

#### 5.2.2 Rust + Rustçµ±åˆå®Ÿè£…

```rust
// Multimodal RAG Pipeline

// SmolVLM2 embedding serviceï¼ˆRustãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰çµŒç”±ï¼‰
fn smolvlm2_embed(text: &str, endpoint: &str) -> Result<Vec<f32>, Box<dyn std::error::Error>> {
    let client = reqwest::blocking::Client::new();
    let result: serde_json::Value = client
        .post(endpoint)
        .json(&serde_json::json!({ "text": text }))
        .send()?
        .json()?;
    let embedding: Vec<f32> = result["embedding"]
        .as_array().unwrap_or(&vec![])
        .iter().map(|v| v.as_f64().unwrap_or(0.0) as f32)
        .collect();
    Ok(embedding)
}

// Multimodal document
struct MultimodalDocument {
    id: usize,
    text: String,
    image_path: Option<String>,
    embedding: Vec<f32>,
}

// Build multimodal index
fn build_multimodal_index(docs: &[(&str, Option<&str>)], endpoint: &str) -> Vec<MultimodalDocument> {
    docs.iter().enumerate().map(|(i, &(text, image_path))| {
        let embed_input = match image_path {
            Some(img) => format!("{} [IMG: {}]", text, img),
            None      => text.to_owned(),
        };
        let embedding = smolvlm2_embed(&embed_input, endpoint).unwrap_or_default();
        MultimodalDocument { id: i + 1, text: text.to_owned(), image_path: image_path.map(str::to_owned), embedding }
    }).collect()
}

// Multimodal search
fn multimodal_search<'a>(
    query: &str,
    index: &'a [MultimodalDocument],
    top_k: usize,
    endpoint: &str,
) -> Vec<(usize, f32, &'a MultimodalDocument)> {
    let query_emb = smolvlm2_embed(query, endpoint).unwrap_or_default();
    let mut scores: Vec<(usize, f32, &MultimodalDocument)> = index.iter()
        .map(|doc| (doc.id, cosine_sim(&query_emb, &doc.embedding), doc))
        .collect();
    scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
    scores.truncate(top_k);
    scores
}

fn main() {
    let multimodal_docs = [
        ("The Eiffel Tower in Paris at sunset.",        Some("images/eiffel_tower.jpg")),
        ("Tokyo Tower with cherry blossoms in spring.", Some("images/tokyo_tower.jpg")),
        ("Berlin Wall memorial with historical graffiti.", None),
        ("Big Ben clock tower in London.",               Some("images/big_ben.jpg")),
    ];
    let endpoint = "http://localhost:8080/embed";
    let index    = build_multimodal_index(&multimodal_docs, endpoint);
    let results  = multimodal_search("Show me towers in European cities", &index, 3, endpoint);

    for (i, (doc_id, score, doc)) in results.iter().enumerate() {
        println!("{}. [Score: {:.3}] {}", i + 1, score, doc.text);
        if let Some(img) = &doc.image_path { println!("   Image: {}", img); }
    }
}
```

#### 5.2.3 Rust Embedding Service (ONNX Runtime)

```rust
// SmolVLM2 embedding service with ONNX Runtime
use actix_web::{post, web, App, HttpResponse, HttpServer, Responder};
use ndarray::{Array1, Array2};
use ort::{Environment, SessionBuilder, Value};
use serde::{Deserialize, Serialize};

#[derive(Deserialize)]
struct EmbedRequest {
    text: String,
}

#[derive(Serialize)]
struct EmbedResponse {
    embedding: Vec<f32>,
}

#[post("/embed")]
async fn embed_endpoint(req: web::Json<EmbedRequest>) -> impl Responder {
    // Tokenize text (simplified)
    let tokens = tokenize(&req.text);

    // Run inference
    match run_embedding_model(&tokens) {
        Ok(embedding) => HttpResponse::Ok().json(EmbedResponse {
            embedding: embedding.to_vec(),
        }),
        Err(e) => HttpResponse::InternalServerError().body(format!("Error: {}", e)),
    }
}

fn tokenize(text: &str) -> Vec<i64> {
    // Simplified tokenizer (in practice, use HuggingFace tokenizers)
    text.chars()
        .filter(|c| c.is_alphanumeric() || c.is_whitespace())
        .map(|c| c as i64)
        .collect()
}

fn run_embedding_model(tokens: &[i64]) -> Result<Array1<f32>, Box<dyn std::error::Error>> {
    // Load ONNX model
    let environment = Environment::builder().with_name("smolvlm2").build()?;

    let session = SessionBuilder::new(&environment)?
        .with_model_from_file("models/smolvlm2_encoder.onnx")?;

    // Prepare input
    let input_ids = Array2::from_shape_vec((1, tokens.len()), tokens.to_vec())?;

    let input_tensor = Value::from_array(session.allocator(), &input_ids)?;

    // Run inference
    let outputs = session.run(vec![input_tensor])?;

    // Extract embedding (CLS token)
    let embedding_tensor = outputs[0].try_extract::<f32>()?;
    let embedding = embedding_tensor.view().to_owned();

    // Mean pooling (simplified)
    let mean_embedding = embedding.mean_axis(ndarray::Axis(1)).unwrap();

    Ok(mean_embedding)
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    HttpServer::new(|| App::new().service(embed_endpoint))
        .bind(("127.0.0.1", 8080))?
        .run()
        .await
}
```

### 5.3 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

<details><summary>è¨˜å·èª­è§£10å•</summary>

**å•1**: BM25ã®å¼ã§ $k_1$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¹å‰²ã¯ï¼Ÿ

a) æ–‡æ›¸é•·æ­£è¦åŒ–
b) TFé£½å’Œåº¦åˆ¶å¾¡
c) IDFé‡ã¿ä»˜ã‘
d) ã‚¯ã‚¨ãƒªæ‹¡å¼µ

<details><summary>è§£ç­”</summary>

**b) TFé£½å’Œåº¦åˆ¶å¾¡**

$$
\frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (\cdots)}
$$

$k_1 \to \infty$ ã§é£½å’Œãªã—ã€$k_1 = 0$ ã§TFç„¡è¦–ã€‚
</details>

**å•2**: HNSW ã®æ¢ç´¢è¨ˆç®—é‡ã¯ï¼Ÿ

a) $O(N)$
b) $O(N \log N)$
c) $O(\log N)$
d) $O(1)$

<details><summary>è§£ç­”</summary>

**c) $O(\log N)$**

éšå±¤çš„æ¢ç´¢ã«ã‚ˆã‚Šå¯¾æ•°æ™‚é–“ã§è¿‘ä¼¼æœ€è¿‘å‚ã‚’ç™ºè¦‹ã€‚
</details>

**å•3**: Self-RAG ã®åçœãƒˆãƒ¼ã‚¯ãƒ³ **[IsSup]** ã®æ„å‘³ã¯ï¼Ÿ

a) æ¤œç´¢ãŒå¿…è¦ã‹
b) æ¤œç´¢çµæœãŒé–¢é€£ã—ã¦ã„ã‚‹ã‹
c) ç”ŸæˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ”¯æŒã•ã‚Œã¦ã„ã‚‹ã‹
d) ç”ŸæˆãŒã‚¯ã‚¨ãƒªã«æœ‰ç”¨ã‹

<details><summary>è§£ç­”</summary>

**c) ç”ŸæˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ”¯æŒã•ã‚Œã¦ã„ã‚‹ã‹**

[IsSup] = Fully/Partially/No
</details>

**å•4**: RRF (Reciprocal Rank Fusion) ã®å¼ã¯ï¼Ÿ

a) $\sum_r \frac{1}{k + \text{rank}_r(d)}$
b) $\sum_r \text{rank}_r(d)$
c) $\prod_r \frac{1}{\text{rank}_r(d)}$
d) $\max_r \text{rank}_r(d)$

<details><summary>è§£ç­”</summary>

**a) $\sum_r \frac{1}{k + \text{rank}_r(d)}$**

è¤‡æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’çµ±åˆã€$k=60$ ãŒæ¨™æº–ã€‚
</details>

**å•5**: ColBERT ã® MaxSim å¼ã¯ï¼Ÿ

a) $\sum_{i} \max_j \mathbf{E}_Q[i] \cdot \mathbf{E}_D[j]$
b) $\max_{i,j} \mathbf{E}_Q[i] \cdot \mathbf{E}_D[j]$
c) $\sum_{i,j} \mathbf{E}_Q[i] \cdot \mathbf{E}_D[j]$
d) $\mathbf{E}_Q \cdot \mathbf{E}_D^\top$



> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. GraphRAGãŒNaive RAGã‚ˆã‚Šè¤‡é›‘ãªå¤šãƒ›ãƒƒãƒ—è³ªå•ï¼ˆã€Œã‚¨ãƒƒãƒ•ã‚§ãƒ«å¡”ãŒã‚ã‚‹å›½ã®GDPã€ï¼‰ã‚’è§£æ±ºã§ãã‚‹ç†ç”±ã‚’ã€çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«ã¨ã„ã†è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚
> 2. Long-context LLMï¼ˆ128k tokenè¶…ï¼‰ã®ç™»å ´ã«ã‚ˆã‚Šã€ŒRAGã¯ä¸è¦ã«ãªã‚‹ã®ã‹ã€ã¨ã„ã†å•ã„ã«å¯¾ã—ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ»ã‚³ã‚¹ãƒˆãƒ»é®®åº¦ã®3è»¸ã§è«–ã˜ã‚ˆã€‚

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 RAGç ”ç©¶ç³»è­œ

```mermaid
graph TD
    R1["2020<br/>RAG (Lewis+)<br/>NIPS"] --> R2["2021<br/>REALM (Guu+)<br/>ICML"]
    R2 --> R3["2022<br/>Atlas (Izacard+)<br/>JMLR"]
    R3 --> R4["2023<br/>Self-RAG (Asai+)<br/>Preprint"]
    R4 --> R5["2024<br/>CRAG (Yan+)<br/>Preprint"]
    R4 --> R6["2024<br/>Adaptive-RAG (Jeong+)<br/>Preprint"]

    R1 -.å›ºå®šæ¤œç´¢.-> R1D["Retrieve â†’ Generate"]
    R2 -.å­¦ç¿’å¯èƒ½æ¤œç´¢.-> R2D["End-to-endå­¦ç¿’"]
    R3 -.Few-shotå¼·åŒ–.-> R3D["Multi-documentèåˆ"]
    R4 -.åçœãƒˆãƒ¼ã‚¯ãƒ³.-> R4D["è‡ªå·±åˆ¶å¾¡æ¤œç´¢"]
    R5 -.çŸ¥è­˜è£œæ­£.-> R5D["æ¤œç´¢çµæœè©•ä¾¡+è£œæ­£"]
    R6 -.é©å¿œæˆ¦ç•¥.-> R6D["ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦èªè­˜"]

    style R4 fill:#c8e6c9
    style R5 fill:#c8e6c9
    style R6 fill:#c8e6c9
```

### 6.2 GraphRAG â€” ã‚°ãƒ©ãƒ•çŸ¥è­˜ãƒ™ãƒ¼ã‚¹

**GraphRAG**: çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ã‚°ãƒ©ãƒ•æ§‹é€ ã§ç®¡ç†

```mermaid
graph LR
    E1["Paris"] -->|capital_of| E2["France"]
    E1 -->|has_landmark| E3["Eiffel Tower"]
    E2 -->|continent| E4["Europe"]
    E3 -->|built_in| E5["1889"]
```

**åˆ©ç‚¹**:
- ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é–¢ä¿‚ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–
- Multi-hop reasoning ãŒå®¹æ˜“
- çŸ¥è­˜ã®ä¸€è²«æ€§ä¿è¨¼

**ã‚¯ã‚¨ãƒªä¾‹**:

```
Query: "What landmarks are in European capitals?"

Graph Traversal:
1. capitals in Europe â†’ [Paris, Berlin, London, ...]
2. landmarks in Paris â†’ [Eiffel Tower, ...]
3. Return: [Eiffel Tower, Brandenburg Gate, Big Ben, ...]
```

**å®Ÿè£…æŠ€è¡“**: Neo4j, NetworkX, DGL

### 6.3 Multi-modal RAG

**ãƒ†ã‚­ã‚¹ãƒˆ + ç”»åƒ + éŸ³å£°** ã‚’çµ±åˆã—ãŸRAG

```mermaid
graph LR
    T["Text"] --> E["Unified<br/>Encoder"]
    I["Image"] --> E
    A["Audio"] --> E
    E --> V["Vector DB"]
    V --> R["Retrieved<br/>Multimodal"]
    R --> G["Generator"]
```

**ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹**:
- åŒ»ç™‚ç”»åƒè¨ºæ–­ï¼ˆç”»åƒ + ç—…æ­´ãƒ†ã‚­ã‚¹ãƒˆï¼‰
- å‹•ç”»æ¤œç´¢ï¼ˆæ˜ åƒ + å­—å¹• + éŸ³å£°ï¼‰
- Eã‚³ãƒãƒ¼ã‚¹ï¼ˆå•†å“ç”»åƒ + ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰

**SOTA Models**: CLIP, BLIP-2, CoCa, SmolVLM2

### 6.4 Long-context vs RAGè«–äº‰

| | Long-context LLM | RAG |
|:--|:----------------|:----|
| **Contexté•·** | 100K-1M tokens | æ•°åƒtokens |
| **ç²¾åº¦** | ä¸­ï¼ˆMiddle-lostå•é¡Œï¼‰ | é«˜ï¼ˆé–¢é€£éƒ¨åˆ†ã®ã¿ï¼‰ |
| **ã‚³ã‚¹ãƒˆ** | é«˜ï¼ˆå…¨æ–‡å‡¦ç†ï¼‰ | ä½ï¼ˆæ¤œç´¢å¾Œã®ã¿ï¼‰ |
| **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·** | é«˜ | ä¸­ï¼ˆæ¤œç´¢ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰ |
| **çŸ¥è­˜æ›´æ–°** | å†å­¦ç¿’å¿…è¦ | æ–‡æ›¸è¿½åŠ ã®ã¿ |

**Middle-lostå•é¡Œ**: Long-contextã§ã¯ä¸­é–“éƒ¨åˆ†ã®æƒ…å ±ãŒå¤±ã‚ã‚Œã‚„ã™ã„

**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥**: RAGã§çµã‚Šè¾¼ã¿ â†’ Long-contextã§ç²¾å¯†å‡¦ç†


## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.6 æœ¬è¬›ç¾©ã§å­¦ã‚“ã 3ã¤ã®æ ¸å¿ƒ

#### æ ¸å¿ƒ1: RAGã¯çŸ¥è­˜ã®å‹•çš„æ‹¡å¼µ

**Without RAG**: LLMã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®çŸ¥è­˜ã®ã¿ï¼ˆå›ºå®šãƒ»å¤ã„ãƒ»ä¸å®Œå…¨ï¼‰

**With RAG**: å¤–éƒ¨çŸ¥è­˜ã‚’æ¤œç´¢â†’çµ±åˆï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ»æœ€æ–°ãƒ»æ–‡è„ˆç‰¹åŒ–ï¼‰

$$
P(a \mid q) = \sum_{d \in \text{Retrieved}(q)} P(a \mid q, d) \cdot \text{Score}(d, q)
$$

#### æ ¸å¿ƒ2: æ¤œç´¢ç²¾åº¦ãŒRAGã®æˆå¦ã‚’æ±ºã‚ã‚‹

**æ¤œç´¢æˆ¦ç•¥ã®é€²åŒ–**:

```
Naive (BM25ã®ã¿) â†’ Dense (Embedding) â†’ Hybrid (BM25+Dense) â†’ Agentic (Self-RAG/CRAG)
```

**ç²¾åº¦å‘ä¸Šã®éµ**:
1. **Hybrid Retrieval**: Sparse + Dense ã®ç›¸è£œæ€§
2. **Reranking**: Cross-Encoder ã§ç²¾å¯†åŒ–
3. **Agentic Control**: æ¤œç´¢ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãƒ»æˆ¦ç•¥ã®è‡ªå¾‹åˆ¤æ–­

#### æ ¸å¿ƒ3: å®Ÿè£…ã¯3è¨€èªãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯

- **ğŸ¦€ Rust**: Vector DB (HNSW, qdrant) â€” é«˜é€Ÿãƒ»å®‰å…¨
- **ğŸ¦€ Rust**: æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (BM25, Embedding, RRF) â€” è¡¨ç¾åŠ›ãƒ»é€Ÿåº¦
- **ğŸ”® Elixir**: åˆ†æ•£ã‚µãƒ¼ãƒ“ãƒ³ã‚° (GenServer, Rate Limiting) â€” ä¸¦è¡Œæ€§ãƒ»è€éšœå®³æ€§

### 6.7 FAQ 5å•

**Q1: RAGã¨Fine-tuningã‚’ä½µç”¨ã™ã¹ãã‹ï¼Ÿ**

**A**: ç”¨é€”ã«ã‚ˆã‚‹ã€‚

- **Fine-tuning**: ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®è¨€èªã‚¹ã‚¿ã‚¤ãƒ«ãƒ»ã‚¿ã‚¹ã‚¯ç‰¹åŒ–
- **RAG**: æœ€æ–°çŸ¥è­˜ãƒ»å‹•çš„çŸ¥è­˜

ä½µç”¨ä¾‹: Fine-tunedãƒ¢ãƒ‡ãƒ« + RAG = ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ– + æœ€æ–°çŸ¥è­˜

**Q2: ãƒ™ã‚¯ãƒˆãƒ«DBã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥ã¯ï¼Ÿ**

**A**: Sharding + Replication

- **Sharding**: ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°ãƒãƒ¼ãƒ‰ã«åˆ†å‰²ï¼ˆæ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰
- **Replication**: å„Shardã‚’è¤‡è£½ï¼ˆå¯ç”¨æ€§å‘ä¸Šï¼‰
- qdrant/Milvusã¯æ¨™æº–å¯¾å¿œ

**Q3: BM25ã¨Denseã§ã©ã¡ã‚‰ã‚’å„ªå…ˆï¼Ÿ**

**A**: ã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹

- **BM25**: å›ºæœ‰åè©ãƒ»å®Œå…¨ä¸€è‡´ãƒ»ãƒ¬ã‚¢å˜èª
- **Dense**: æ„å‘³çš„é¡ä¼¼æ€§ãƒ»è¨€ã„æ›ãˆãƒ»å¤šè¨€èª
- **æ¨å¥¨**: Hybrid (RRFèåˆ)

**Q4: Chunkã‚µã‚¤ã‚ºã®æœ€é©å€¤ã¯ï¼Ÿ**

**A**: ã‚¿ã‚¹ã‚¯ãƒ»ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹

- **ä¸€èˆ¬**: 256-512 tokens
- **çŸ­æ–‡ã‚¿ã‚¹ã‚¯**: 128 tokens
- **é•·æ–‡ç†è§£**: 1024 tokens
- **å®Ÿé¨“**: Recall/Latency ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã§èª¿æ•´

**Q5: Agentic RAGã®å­¦ç¿’ã‚³ã‚¹ãƒˆã¯ï¼Ÿ**

**A**: é«˜ã„ãŒåŠ¹æœå¤§

- Self-RAG: åçœãƒˆãƒ¼ã‚¯ãƒ³ã®æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”ŸæˆãŒå¿…è¦
- CRAG: Evaluatorå­¦ç¿’ï¼ˆè»½é‡LMï¼‰
- **ROIé«˜**: æ¤œç´¢ç²¾åº¦ãŒåŠ‡çš„å‘ä¸Šï¼ˆGPT-4è¶…ãˆï¼‰

### 6.10 æ¬¡å›äºˆå‘Š: ç¬¬30å› ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Œå…¨ç‰ˆ

**ç¬¬30å›ã§å­¦ã¶ã“ã¨**:

- **ReAct**: Reasoning + Acting ã®çµ±åˆ
- **Tool Use**: å¤–éƒ¨ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ï¼ˆæ¤œç´¢ãƒ»è¨ˆç®—ãƒ»APIï¼‰
- **Multi-Agent Systems**: å”èª¿ãƒ»ç«¶äº‰ãƒ»äº¤æ¸‰
- **AutoGPT/BabyAGI**: è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…
- **Planning**: PDDL/HTN ã«ã‚ˆã‚‹é•·æœŸè¨ˆç”»
- **Memory**: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ãƒ»æ„å‘³è¨˜æ†¶ãƒ»ä½œæ¥­è¨˜æ†¶



---


## ğŸ“š å‚è€ƒæ–‡çŒ®

[^1]: Lewis, P., et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." *NeurIPS 2020*. [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)

[^2]: Asai, A., et al. (2024). "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection." *ICLR 2024 (Oral)*. [arXiv:2310.11511](https://arxiv.org/abs/2310.11511)

[^3]: Yan, S., et al. (2024). "Corrective Retrieval Augmented Generation." *arXiv preprint*. [arXiv:2401.15884](https://arxiv.org/abs/2401.15884)

[^6]: Malkov, Y. A., & Yashunin, D. A. (2018). "Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs." *IEEE TPAMI*. [arXiv:1603.09320](https://arxiv.org/abs/1603.09320)

[^7]: qdrant. "Qdrant - Vector Database." [GitHub](https://github.com/qdrant/qdrant) | [Docs](https://qdrant.tech/)

[^8]: RAGAS. "RAG Assessment Framework." [GitHub](https://github.com/explodinggradients/ragas)

[^9]: Johnson, J., Douze, M., & JÃ©gou, H. (2019). "Billion-scale similarity search with GPUs." *IEEE Transactions on Big Data*. FAISS [GitHub](https://github.com/facebookresearch/faiss)

> **ğŸ“– å‰ç·¨ï¼ˆç†è«–ç·¨ï¼‰**: [ç¬¬29å›å‰ç·¨: RAGç†è«–ç·¨](./ml-lecture-29-part1) | **â† ç†è«–ãƒ»æ•°å¼ã‚¾ãƒ¼ãƒ³ã¸**

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

---