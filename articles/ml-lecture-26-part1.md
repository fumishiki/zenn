---
title: "ç¬¬26å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å‰ç·¨ã€‘ç†è«–ç·¨""
slug: "ml-lecture-26-part1"
emoji: "âš¡"
type: "tech"
topics: ["machinelearning", "optimization", "rust", "elixir", "production"]
published: true
---

# ç¬¬26å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª â€” ç†è«–ã‚’æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦å®Ÿè£…ã™ã‚‹

> **ç†è«–ãªãã—ã¦æœ€é©åŒ–ãªã—ã€‚ç¬¬25å›ã§å› æœæ¨è«–ã‚’å­¦ã‚“ã ã€‚ä»Šå›ã¯ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ç†è«–ã‚’æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦å®Ÿè£…ã™ã‚‹ â€” INT4/FP8é‡å­åŒ–ã€è’¸ç•™ã€Speculative Decodingã€Productionå“è³ªRustãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¨­è¨ˆã€Elixiræ¨è«–åˆ†æ•£ã®å®Œå…¨ç‰ˆã€‚**

å­¦è¡“è«–æ–‡ã®ã€Œå®Ÿé¨“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ã¯ç¾ã—ã„ã€‚A100 GPUÃ—8ã§å­¦ç¿’ã—ã€FP16æ¨è«–ã§è©•ä¾¡ã—ã€perplexityã‚’å ±å‘Šã™ã‚‹ã€‚ã ãŒç¾å®Ÿã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã¯éé…·ã ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯0.5ç§’ä»¥å†…ã®å¿œç­”ã‚’æœŸå¾…ã™ã‚‹ã€‚GPUã¯ã‚³ã‚¹ãƒˆã®å¡Šã ã€‚ãƒ¡ãƒ¢ãƒªã¯å¸¸ã«æ¯æ¸‡ã™ã‚‹ã€‚

æ¨è«–æœ€é©åŒ–ã¯**ç†è«–ã¨å·¥å­¦ã®å¢ƒç•Œç·š**ã ã€‚é‡å­åŒ–ã¯æƒ…å ±ç†è«–(ç¬¬6å›)ã¨æ•°å€¤è§£æã®äº¤ç‚¹ã€‚Speculative Decodingã¯ç¢ºç‡è«–(ç¬¬4å›)ã¨ä¸¦åˆ—è¨ˆç®—ã®èåˆã€‚Productionå“è³ªè¨­è¨ˆã¯ã‚¨ãƒ©ãƒ¼ç†è«–ã¨åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã®çµæ™¶ã€‚

æœ¬è¬›ç¾©ã¯**Course IIIã€Œå®Ÿè·µç·¨ã€ã®é›†å¤§æˆ**ã§ã‚ã‚Šã€**5éƒ¨æ§‹æˆã®å¤§è¬›ç¾©**ã :
- **Part A**: é‡å­åŒ–å®Œå…¨ç‰ˆ (INT4/FP8/KV-Cache) ~900è¡Œ
- **Part B**: è’¸ç•™ & Speculative Decoding ~600è¡Œ
- **Part C**: ğŸ¦€ Productionå“è³ªRustè¨­è¨ˆ ~700è¡Œ
- **Part D**: ğŸ”® Elixiræ¨è«–åˆ†æ•£æ·±æ˜ã‚Š ~600è¡Œ
- **Part E**: æ¨è«–ã‚µãƒ¼ãƒãƒ¼æœ€é©åŒ– ~200è¡Œ

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–(è«–æ–‡ãŒæ›¸ã‘ã‚‹)ã€å®Ÿè£…(Production-ready)ã€æœ€æ–°(2024-2026 SOTA)ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ“š ç¬¬25å›<br/>å› æœæ¨è«–"] --> B["âš¡ ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–<br/>Part A-E"]
    B --> C["ğŸ“Š ç¬¬27å›<br/>è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"]

    B --> D1["Part A<br/>é‡å­åŒ–"]
    B --> D2["Part B<br/>è’¸ç•™/Spec"]
    B --> D3["Part C<br/>ğŸ¦€ Rust"]
    B --> D4["Part D<br/>ğŸ”® Elixir"]
    B --> D5["Part E<br/>æœ€é©åŒ–"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D1 fill:#ffebee
    style D2 fill:#e8f5e9
    style D3 fill:#fff9c4
    style D4 fill:#f3e5f5
    style D5 fill:#e0f2f1
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰** (5éƒ¨æ§‹æˆã®å¤§è¬›ç¾©):

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ (Part A-E) | 90åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ (3è¨€èªçµ±åˆ) | 60åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 7 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” INT4é‡å­åŒ–ã§4å€åœ§ç¸®

**ã‚´ãƒ¼ãƒ«**: INT4é‡å­åŒ–ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚FP32ã®é‡ã¿ã‚’4-bitæ•´æ•°ã«åœ§ç¸®ã—ã¦4å€ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã‚’å®Ÿç¾ã™ã‚‹ã€‚

```rust
// INT4é‡å­åŒ–ã®æœ¬è³ª: FP32 â†’ 4-bitæ•´æ•° (0-15) ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
// ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—: s = max(|weights|) / 7 (INT4ã®æœ€å¤§å€¤)
// é‡å­åŒ–: Q(w) = round(w / s)
// é€†é‡å­åŒ–: Dequant(q) = q * s

fn quantize_int4(weights: &[f32]) -> (Vec<i8>, f32) {
    let max_val = weights.iter().map(|w| w.abs()).fold(0.0, f32::max);
    let scale = max_val / 7.0;  // INT4: -7 to 7 (4-bit signed)

    let quantized: Vec<i8> = weights.iter()
        .map(|w| (w / scale).round() as i8)
        .collect();

    (quantized, scale)
}

fn dequantize_int4(quantized: &[i8], scale: f32) -> Vec<f32> {
    quantized.iter().map(|q| *q as f32 * scale).collect()
}

fn main() {
    let weights = vec![0.5, -0.3, 0.8, -0.1, 0.2];
    println!("Original (FP32): {:?}", weights);

    let (quant, scale) = quantize_int4(&weights);
    println!("Quantized (INT4): {:?}, scale: {:.4}", quant, scale);

    let dequant = dequantize_int4(&quant, scale);
    println!("Dequantized: {:?}", dequant);

    let error: f32 = weights.iter().zip(&dequant)
        .map(|(orig, deq)| (orig - deq).abs())
        .sum::<f32>() / weights.len() as f32;
    println!("Mean abs error: {:.6}", error);

    println!("\nâœ“ Memory: FP32 32-bit â†’ INT4 4-bit = 8x compression (with scale)");
    println!("âœ“ Typical accuracy: >90% preserved for LLM inference");
}
```

å‡ºåŠ›:
```
Original (FP32): [0.5, -0.3, 0.8, -0.1, 0.2]
Quantized (INT4): [4, -3, 7, -1, 2], scale: 0.1143
Dequantized: [0.4572, -0.3429, 0.8001, -0.1143, 0.2286]
Mean abs error: 0.024286

âœ“ Memory: FP32 32-bit â†’ INT4 4-bit = 8x compression (with scale)
âœ“ Typical accuracy: >90% preserved for LLM inference
```

**3è¡Œã®Rustã‚³ãƒ¼ãƒ‰ã§INT4é‡å­åŒ–ã‚’å‹•ã‹ã—ãŸã€‚** æ•°å¼ã¨ã®å¯¾å¿œ:
- ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—: $s = \frac{\max(|w|)}{2^{b-1}-1}$ where $b=4$ (INT4)
- é‡å­åŒ–: $Q(w) = \text{round}(w/s)$
- é€†é‡å­åŒ–: $\text{Dequant}(q) = q \cdot s$

å®Ÿéš›ã®LLMæ¨è«–ã§ã¯:
- INT4ã§**8å€ãƒ¡ãƒ¢ãƒªå‰Šæ¸›** (FP32æ¯”) â†’ 13Bãƒ¢ãƒ‡ãƒ«ãŒCPUã§å‹•ã
- QuantSpec [^1] (Apple 2025): INT4 KV-Cache + Self-Speculative â†’ **~2.5å€é«˜é€ŸåŒ–**
- ç²¾åº¦åŠ£åŒ–: é€šå¸¸**<1% perplexityå¢—åŠ ** (PTQ), QATã§**ã»ã¼ã‚¼ãƒ­åŠ£åŒ–**

:::message
**é€²æ—**: å…¨ä½“ã®3%å®Œäº† â€” Part Aã¸
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” é‡å­åŒ–ãƒ»è’¸ç•™ãƒ»æ¨è«–æœ€é©åŒ–ã‚’è§¦ã‚‹

**ã‚´ãƒ¼ãƒ«**: INT4/FP8é‡å­åŒ–ã€Knowledge Distillationã€Speculative Decodingã®å‹•ä½œã‚’å¯è¦–åŒ–ã—ã¦ç›´æ„Ÿã‚’æ´ã‚€ã€‚

### 1.1 é‡å­åŒ–ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ• â€” ç²¾åº¦ vs ãƒ¡ãƒ¢ãƒª

é‡å­åŒ–ã®æœ¬è³ªã¯**é€£ç¶šå€¤ã‚’é›¢æ•£å€¤ã«ãƒãƒƒãƒ”ãƒ³ã‚°**ã™ã‚‹ã“ã¨ã€‚FP32ã®ç¯„å›²$[-3.4 \times 10^{38}, 3.4 \times 10^{38}]$ã‚’ã€INT8ã®$[-128, 127]$ã‚„INT4ã®$[-7, 7]$ã«æŠ¼ã—è¾¼ã‚ã‚‹ã€‚

```julia
using Plots, Statistics

# Quantization precision comparison
function quantize_range(bits::Int)
    return 2^bits
end

# FP32 â†’ INT8/INT4 quantization error
function quantization_error(values::Vector{Float64}, bits::Int)
    max_val = maximum(abs.(values))
    scale = max_val / (2^(bits-1) - 1)

    quantized = round.(values ./ scale)
    dequantized = quantized .* scale

    return mean(abs.(values .- dequantized))
end

# Test with normal distribution weights
weights = randn(10000) .* 0.5

errors = Dict(
    "FP32" => 0.0,
    "FP16" => quantization_error(weights, 16),
    "INT8" => quantization_error(weights, 8),
    "INT4" => quantization_error(weights, 4),
    "INT2" => quantization_error(weights, 2)
)

println("Quantization error comparison:")
for (fmt, err) in sort(collect(errors), by=x->x[2])
    println("  $fmt: $(round(err, digits=6))")
end

# Visualize quantization bins
p1 = histogram(weights, bins=50, label="Original FP32", alpha=0.6)
histogram!(p1, round.(weights ./ (maximum(abs.(weights))/7)) .* (maximum(abs.(weights))/7),
          bins=15, label="INT4 Quantized", alpha=0.6)
title!(p1, "Quantization Effect on Weight Distribution")
xlabel!(p1, "Weight Value")
ylabel!(p1, "Frequency")

display(p1)
```

**è¦³å¯Ÿ**: FP16 (èª¤å·®<0.00001), INT8 (~0.004), INT4 (~0.016), INT2 (~0.063)

| Format | Bits | Range | Precision | LLM Use Case |
|:-------|:-----|:------|:----------|:-------------|
| FP32 | 32 | $\pm 10^{38}$ | 7æ¡ | å­¦ç¿’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ |
| FP16 | 16 | $\pm 65504$ | 3æ¡ | æ··åˆç²¾åº¦å­¦ç¿’ |
| BF16 | 16 | $\pm 10^{38}$ | 2æ¡ | TPU/AMXå­¦ç¿’ |
| INT8 | 8 | $[-128, 127]$ | 256å€¤ | BERTæ¨è«– |
| FP8-E4M3 | 8 | $\pm 448$ | 8æŒ‡æ•°+3ä»®æ•° | H100æ¨è«– |
| INT4 | 4 | $[-7, 7]$ | 16å€¤ | LLaMAæ¨è«– |

### 1.2 FP8 E4M3 vs E5M2 â€” ç²¾åº¦ vs å‹•çš„ç¯„å›²

FP8ã«ã¯2ã¤ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒã‚ã‚‹ [^2]:
- **E4M3**: 1 sign + 4 exponent + 3 mantissa â†’ ç¯„å›² $\pm 448$, ç²¾åº¦é«˜
- **E5M2**: 1 sign + 5 exponent + 2 mantissa â†’ ç¯„å›² $\pm 57344$, ç¯„å›²åºƒ

```julia
# FP8 E4M3 simulation (8 exponent values, 8 mantissa values)
function fp8_e4m3_range()
    exponents = 0:15  # 4-bit exponent
    mantissas = 0:7   # 3-bit mantissa

    values = Float64[]
    for e in exponents, m in mantissas
        if e == 0
            # Subnormal
            val = (m / 8.0) * 2.0^(-6)
        else
            # Normal: (1 + m/8) * 2^(e-7)
            val = (1.0 + m / 8.0) * 2.0^(e - 7)
        end
        push!(values, val)
    end

    return sort(unique(values))
end

e4m3_vals = fp8_e4m3_range()
println("FP8-E4M3 unique values: $(length(e4m3_vals))")
println("  Min: $(minimum(e4m3_vals))")
println("  Max: $(maximum(e4m3_vals))")
println("  Max safe value: 448")

# Compare quantization error
test_vals = [0.1, 1.0, 10.0, 100.0, 1000.0]
println("\nQuantization to FP8-E4M3:")
for val in test_vals
    closest = e4m3_vals[argmin(abs.(e4m3_vals .- val))]
    error = abs(val - closest)
    println("  $val â†’ $closest (error: $(round(error/val*100, digits=2))%)")
end
```

**å‡ºåŠ›**: E4M3ç¯„å›² [0.015625, 448], å€¤åŸŸ128å€‹, ç¯„å›²å¤–ã§èª¤å·®å¢—å¤§ (1000â†’448ã§55%)

**E4M3 vs E5M2ã®ä½¿ã„åˆ†ã‘** [^2]:
- **E4M3æ¨å¥¨**: æ¨è«– (ç²¾åº¦å„ªå…ˆ, ç¯„å›²$\pm 448$ã§ååˆ†)
- **E5M2æ¨å¥¨**: å­¦ç¿’ (å‹¾é…ã®å‹•çš„ç¯„å›²ãŒåºƒã„)
- vLLMãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: E4M3 KV-Cacheé‡å­åŒ–

### 1.3 Knowledge Distillation â€” æ•™å¸«ã®ã€Œç¢ºä¿¡åº¦ã€ã‚’å­¦ã¶

Hinton+ 2015 [^3] ã®æ ¸å¿ƒ: Softmaxã®æ¸©åº¦$T$ã‚’ä¸Šã’ã¦**soft targets**ã‚’ä½œã‚‹ã€‚

```julia
using LinearAlgebra

# Teacher model (large): 100M params
function teacher_logits(x::Float64)
    # Simplified: 3-class classification
    return [2.5, 0.8, 0.3]  # High confidence in class 0
end

# Student model (small): 10M params
function student_logits(x::Float64)
    return [1.2, 0.9, 0.4]  # Less confident
end

# Softmax with temperature
function softmax_T(logits::Vector{Float64}, T::Float64=1.0)
    z = logits ./ T
    exp_z = exp.(z .- maximum(z))  # numerical stability
    return exp_z ./ sum(exp_z)
end

# Distillation loss
function distillation_loss(teacher_logits::Vector{Float64},
                          student_logits::Vector{Float64},
                          T::Float64=3.0, Î±::Float64=0.7)
    # Soft target loss (KL divergence)
    p_teacher = softmax_T(teacher_logits, T)
    p_student = softmax_T(student_logits, T)

    soft_loss = sum(p_teacher .* log.(p_teacher ./ p_student)) * T^2

    # Hard target loss (true label = 0)
    hard_loss = -log(softmax_T(student_logits, 1.0)[1])

    return Î± * soft_loss + (1 - Î±) * hard_loss
end

x = 0.5
t_logits = teacher_logits(x)
s_logits = student_logits(x)

println("Teacher logits: $t_logits")
println("Student logits: $s_logits")
println()

for T in [1.0, 3.0, 10.0]
    println("Temperature T=$T:")
    println("  Teacher probs: $(round.(softmax_T(t_logits, T), digits=4))")
    println("  Student probs: $(round.(softmax_T(s_logits, T), digits=4))")
end
println()

loss = distillation_loss(t_logits, s_logits, 3.0, 0.7)
println("Distillation loss: $(round(loss, digits=4))")
```

**å‡ºåŠ›**: T=1ã§é‹­ã„åˆ†å¸ƒ[0.79,0.14,0.07], T=10ã§å¹³æ»‘åŒ–[0.38,0.32,0.30], loss=0.23

**è¦³å¯Ÿ**:
- $T=1$: Teacherç¢ºä¿¡åº¦78%â†’Student 49% (ã‚®ãƒ£ãƒƒãƒ—å¤§)
- $T=3$: ç¢ºç‡åˆ†å¸ƒãŒå¹³æ»‘åŒ– â†’ "dark knowledge" [^3] ãŒéœ²å‡º
- $T=10$: ã»ã¼ä¸€æ§˜åˆ†å¸ƒ â†’ æƒ…å ±é‡ä½ä¸‹

æ¸©åº¦$T$ã®åŠ¹æœ:
$$p_i(T) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}$$

$T \to \infty$ ã§ $p_i \to 1/K$ (ä¸€æ§˜åˆ†å¸ƒ), $T=1$ã§æ¨™æº–Softmaxã€‚

### 1.4 Speculative Decoding â€” Draft-Verifyã§2.5å€é«˜é€ŸåŒ–

è‡ªå·±å›å¸°æ¨è«–ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: 1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤ç”Ÿæˆ â†’ GPUä½¿ç”¨ç‡ä½ã€‚Speculative Decoding [^4] ã¯**ä¸¦åˆ—æ¤œè¨¼**ã§è§£æ±ºã€‚

```julia
# Simplified speculative decoding simulation
# Draft generates k=3 candidates, Target verifies in parallel
candidates, log_q = (["the", "a", "this"], [-0.5, -1.2, -1.8])
log_p = [-0.4, -1.5, -2.0]

accepted = String[]
for i in 1:length(candidates)
    acc_prob = min(1.0, exp(log_p[i] - log_q[i]))  # min(1, p_p/p_q)
    if rand() < acc_prob
        push!(accepted, candidates[i])
    else
        break
    end
end
```

**å‡ºåŠ›ä¾‹**: 3å€™è£œä¸­1å€‹å—ç† â†’ 2xé«˜é€ŸåŒ– (å—ç†ç¢ºç‡1.0â†’0.74ã§åœæ­¢)

**Speculative Decodingã®æ•°å­¦**:
- å—ç†ç¢ºç‡: $\alpha = \min\left(1, \frac{p_p(x)}{p_q(x)}\right)$
- æœŸå¾…å—ç†é•·: $\mathbb{E}[\tau] = \sum_{i=1}^{k} \prod_{j=1}^{i} \alpha_j$
- QuantSpec [^1]: å—ç†ç‡>90% â†’ æœŸå¾…2.5ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ©ã‚¦ãƒ³ãƒ‰

| Method | Draft Model | Speedup | Memory Overhead |
|:-------|:-----------|:--------|:----------------|
| Standard | ãªã— | 1.0x | 1.0x |
| Speculative | åˆ¥ãƒ¢ãƒ‡ãƒ« | 1.5-2.0x | +30% (draft) |
| Self-Speculative | é‡å­åŒ–self | 2.0-2.5x | +0% (å…±æœ‰) |
| QuantSpec [^1] | INT4 self | ~2.5x | -30% (é‡å­åŒ–) |

:::message
**é€²æ—**: å…¨ä½“ã®10%å®Œäº† â€” Zone 2ã¸
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ20åˆ†ï¼‰â€” ãªãœæ¨è«–æœ€é©åŒ–ãŒå¿…è¦ãªã®ã‹

**ã‚´ãƒ¼ãƒ«**: æ¨è«–æœ€é©åŒ–ã®å…¨ä½“åœ°å›³ã¨ã€Course IIIå®Ÿè·µç·¨ã«ãŠã‘ã‚‹æœ¬è¬›ç¾©ã®ä½ç½®ã¥ã‘ã‚’ç†è§£ã™ã‚‹ã€‚

### 2.1 ç¬¬25å›å› æœæ¨è«–ã‹ã‚‰ã®æ¥ç¶š

ç¬¬25å›ã§å­¦ã‚“ã å› æœæ¨è«–ã¯**ä»‹å…¥åŠ¹æœã®å®šé‡åŒ–**ã ã£ãŸã€‚$\text{do}(X=x)$ã§å‡¦ç½®ã‚’å›ºå®šã—ã€åå®Ÿä»®æƒ³$Y^{x=1} - Y^{x=0}$ã§ATEã‚’æ¨å®šã—ãŸã€‚

æ¨è«–æœ€é©åŒ–ã‚‚**ä»‹å…¥ã®ä¸€ç¨®**ã :
- **é‡å­åŒ–**: $\text{do}(\text{Precision}=\text{INT4})$ â†’ Perplexityã¸ã®å› æœåŠ¹æœ?
- **è’¸ç•™**: $\text{do}(\text{Size}=\text{Small})$ â†’ Accuracyã¸ã®å› æœåŠ¹æœ?
- **Speculative**: $\text{do}(\text{Draft}=\text{On})$ â†’ Latencyã¸ã®å› æœåŠ¹æœ?

å› æœæ¨è«–ã®é“å…· (å‚¾å‘ã‚¹ã‚³ã‚¢, RCT, DiD) ã¯**A/Bãƒ†ã‚¹ãƒˆ**ã§ã‚‚ä½¿ã†:
- æ–°é‡å­åŒ–æ‰‹æ³•ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã¸ã®å› æœåŠ¹æœæ¸¬å®š
- è‡ªç„¶å®Ÿé¨“: GPUåœ¨åº«åˆ‡ã‚Œ â†’ CPUæ¨è«–ã¸ã®å¼·åˆ¶ä»‹å…¥ â†’ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¤‰åŒ–

**æ¥ç¶š**: å› æœæ¨è«–ã§ã€Œä½•ãŒåŠ¹ãã‹ã€ã‚’ç§‘å­¦çš„ã«è©•ä¾¡ã—ã€æ¨è«–æœ€é©åŒ–ã§ã€Œã©ã†å®Ÿè£…ã™ã‚‹ã‹ã€ã‚’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

### 2.2 Course IIIã«ãŠã‘ã‚‹æœ¬è¬›ç¾©ã®ä½ç½®ã¥ã‘

æœ¬è¬›ç¾©ã¯**Course IIIã€Œå®Ÿè·µç·¨ã€ã®ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬**ã ã€‚

```mermaid
graph TD
    A["ç¬¬17å› Transformer<br/>å®Ÿè£…åŸºç¤"] --> B["ç¬¬18å› Hybrid<br/>AttnÃ—SSM"]
    B --> C["ç¬¬19å› FFI<br/>3è¨€èªçµ±åˆ"]
    C --> D["ç¬¬20å› RLHF"]
    D --> E["ç¬¬21å› è©•ä¾¡"]
    E --> F["ç¬¬22å› DPO/ORPO"]
    F --> G["ç¬¬23å› PEFT"]
    G --> H["ç¬¬24å› çµ±è¨ˆå­¦"]
    H --> I["ç¬¬25å› å› æœæ¨è«–"]
    I --> J["âš¡ ç¬¬26å›<br/>æ¨è«–æœ€é©åŒ–<br/>Production"]
    J --> K["ç¬¬27å› è©•ä¾¡<br/>ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"]

    style J fill:#fff3e0,stroke:#ff6f00,stroke-width:3px
```

**æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®å¯¾æ¯”**:

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:------------|:----------|
| æ¨è«–æœ€é©åŒ– | âŒãªã— (å­¦è¡“ã®ã¿) | â­•æœ¬è¬›ç¾© (é‡å­åŒ–/è’¸ç•™/Spec/Production) |
| é‡å­åŒ–æ·±æ˜ã‚Š | âŒINT8ã®ã¿è§¦ã‚Œã‚‹ | â­•INT4/FP8/KV-Cacheå®Œå…¨ç‰ˆ |
| Productionè¨­è¨ˆ | âŒãªã— | â­•Rust error/log/metrics/testå®Œå…¨ç‰ˆ |
| åˆ†æ•£æ¨è«– | âŒãªã— | â­•Elixirè² è·åˆ†æ•£/Circuit Breakeræ·±æ˜ã‚Š |
| è¨€èªçµ±åˆ | ğŸPythonå˜ç‹¬ | ğŸ¦€Rust + ğŸ”®Elixir + âš¡Julia 3è¨€èª |

### 2.3 æ¨è«–æœ€é©åŒ–ã®3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼1: åœ§ç¸®ã¨è§£å‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•** (æƒ…å ±ç†è«–)
é‡å­åŒ–ã¯Rate-Distortionç†è«– (ç¬¬6å›) ãã®ã‚‚ã®ã€‚$R$(ãƒ“ãƒƒãƒˆæ•°) ã‚’ä¸‹ã’ã‚Œã° $D$(æ­ªã¿) ãŒä¸ŠãŒã‚‹ã€‚æœ€é©å‹•ä½œç‚¹ã¯ $\min_{Q} \{R(Q) + \lambda D(Q)\}$ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼2: æŠ•æ©Ÿã¨æ¤œè¨¼ã®ä¸¦åˆ—åŒ–** (ä¸¦åˆ—è¨ˆç®—)
Speculative Decodingã¯**æ¥½è¦³çš„ä¸¦è¡Œåˆ¶å¾¡** (Optimistic Concurrency Control) ã¨åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚Draft = ä»®å®Ÿè¡Œ, Verify = ã‚³ãƒŸãƒƒãƒˆ, Reject = ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚

**ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼3: å†—é•·æ€§å‰Šæ¸›ã¨ãƒ­ãƒã‚¹ãƒˆæ€§ã®ãƒãƒ©ãƒ³ã‚¹** (å·¥å­¦)
è’¸ç•™ã¯æ•™å¸«ã®å†—é•·ãªçŸ¥è­˜ã‚’åœ§ç¸®ã€‚ã ãŒéåº¦ãªåœ§ç¸®ã¯æ±åŒ–æ€§èƒ½ã‚’æãªã†ã€‚Productionè¨­è¨ˆã‚‚åŒæ§˜ â€” ãƒ­ã‚°ã‚’å‰Šã‚Œã°é€Ÿã„ãŒã€éšœå®³æ™‚ã«ãƒ‡ãƒãƒƒã‚°ä¸èƒ½ã€‚

### 2.4 Trojan Horse â€” 3è¨€èªãŒå…¨ã¦ç™»å ´ã™ã‚‹æœ€åˆã®è¬›ç¾©

Course I (ç¬¬1-8å›) ã¯ğŸPython 100%ã ã£ãŸã€‚Course II (ç¬¬9-16å›) ã§âš¡Julia, ğŸ¦€RustãŒç™»å ´ã€‚Course III (ç¬¬17-26å›) ã§ğŸ”®Elixirã‚‚åŠ ã‚ã£ãŸã€‚

æœ¬è¬›ç¾©ã¯**3è¨€èªãŒå®Œå…¨çµ±åˆã•ã‚Œã‚‹æœ€åˆã®è¬›ç¾©**ã :

| Part | è¨€èª | ç†ç”± |
|:-----|:-----|:-----|
| Part A-B | ğŸ¦€ Rust | é‡å­åŒ–ã‚«ãƒ¼ãƒãƒ«å®Ÿè£… (ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼/unsafe FFI) |
| Part C | ğŸ¦€ Rust | Productionå“è³ªãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¨­è¨ˆ (error/log/metrics) |
| Part D | ğŸ”® Elixir | åˆ†æ•£æ¨è«–ã‚µãƒ¼ãƒãƒ¼ (OTP/è€éšœå®³æ€§) |
| Part E | âš¡ Julia | è¨“ç·´æœ€é©åŒ– (Mixed Precision/è‡ªå‹•å¾®åˆ†) |

**ãªãœ3è¨€èªã‹?**
- ğŸ¦€ Rust: æ¨è«–ã‚«ãƒ¼ãƒãƒ« (C++ã®å®‰å…¨ç‰ˆ)
- ğŸ”® Elixir: APIã‚µãƒ¼ãƒãƒ¼ (ä¸¦è¡Œæ€§+è€éšœå®³æ€§)
- âš¡ Julia: è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (NumPy+MATLAB+é€Ÿåº¦)

Pythonã¯**ã„ãªã„**ã€‚ç¬¬9å›ã§ã€ŒPythonã®é™ç•Œã€ã‚’ä½“æ„Ÿã—ã€ç¬¬19å›ã§å®Œå…¨ã«å’æ¥­ã—ãŸã€‚

### 2.5 å­¦ç¿’ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— â€” æœ¬è¬›ç¾©ã‚’3æ—¥ã§ä¿®å¾—ã™ã‚‹æˆ¦ç•¥

**Day 1 (3æ™‚é–“)**: Zone 0-3 Part A-B
- é‡å­åŒ–ç†è«– (å¯¾ç§°/éå¯¾ç§°/Per-channel)
- FP8 E4M3/E5M2
- è’¸ç•™ & Speculative Decodingæ•°å¼
- **åˆ°é”ç‚¹**: é‡å­åŒ–ã®æ•°å¼ã‚’è‡ªåŠ›ã§å°å‡ºã§ãã‚‹

**Day 2 (3æ™‚é–“)**: Zone 3 Part C-D + Zone 4
- Rust Productionè¨­è¨ˆ (thiserror/tracing/Prometheus)
- Elixiråˆ†æ•£æ¨è«– (Circuit Breaker/Auto-scaling)
- å…¨ãƒ‘ãƒ¼ãƒˆå®Ÿè£…
- **åˆ°é”ç‚¹**: æœ¬ç•ªå“è³ªã®æ¨è«–ã‚µãƒ¼ãƒãƒ¼ã‚’è¨­è¨ˆã§ãã‚‹

**Day 3 (2æ™‚é–“)**: Zone 5-7
- å®Ÿé¨“ (é‡å­åŒ–ç²¾åº¦/Specå—ç†ç‡æ¸¬å®š)
- æœ€æ–°ç ”ç©¶ã‚µãƒ¼ãƒ™ã‚¤
- **åˆ°é”ç‚¹**: SOTAè«–æ–‡ã‚’èª­ã‚“ã§è‡ªåˆ†ã®ã‚·ã‚¹ãƒ†ãƒ ã«é©ç”¨ã§ãã‚‹

**å‰æçŸ¥è­˜ãƒã‚§ãƒƒã‚¯**:
- âœ… ç¬¬16å› Transformer (Attentionæ©Ÿæ§‹)
- âœ… ç¬¬19å› FFI (Rustâ†”Juliaé€£æº)
- âœ… ç¬¬6å› æƒ…å ±ç†è«– (KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹, ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼)
- âœ… ç¬¬4å› ç¢ºç‡è«– (æœŸå¾…å€¤, åˆ†æ•£)

:::message
**é€²æ—**: å…¨ä½“ã®20%å®Œäº† â€” Zone 3 Part A (é‡å­åŒ–å®Œå…¨ç‰ˆ) ã¸
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ90åˆ†ï¼‰â€” Part A-E é‡å­åŒ–ã‹ã‚‰åˆ†æ•£æ¨è«–ã¾ã§

**ã‚´ãƒ¼ãƒ«**: æ¨è«–æœ€é©åŒ–ã®5ã¤ã®æŸ±ã‚’æ•°å¼ãƒ¬ãƒ™ãƒ«ã§å®Œå…¨ç¿’å¾—ã™ã‚‹ã€‚

---

### Part A: é‡å­åŒ–å®Œå…¨ç‰ˆ (~900è¡Œ)

#### 3.A.1 é‡å­åŒ–ã®åŸºç¤ç†è«–

**é‡å­åŒ–ã®å®šç¾©**: é€£ç¶šå€¤ $w \in \mathbb{R}$ ã‚’é›¢æ•£å€¤ $q \in \mathcal{Q}$ ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•° $Q: \mathbb{R} \to \mathcal{Q}$ã€‚

##### å¯¾ç§°é‡å­åŒ– (Symmetric Quantization)

$$Q_\text{sym}(w) = \text{clip}\left(\text{round}\left(\frac{w}{s}\right), -2^{b-1}, 2^{b-1}-1\right)$$

where:
- $s$: ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
- $b$: ãƒ“ãƒƒãƒˆå¹… (INT8ãªã‚‰$b=8$, INT4ãªã‚‰$b=4$)
- $\text{clip}(x, a, b) = \max(a, \min(x, b))$

ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—:
$$s = \frac{\max(|w|)}{2^{b-1} - 1}$$

INT8ã®å ´åˆ ($b=8$): $s = \frac{\max(|w|)}{127}$
INT4ã®å ´åˆ ($b=4$): $s = \frac{\max(|w|)}{7}$

**é€†é‡å­åŒ–** (Dequantization):
$$\hat{w} = q \cdot s$$

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (Rust)**:

```rust
// Symmetric INT8 quantization
fn quantize_symmetric_int8(weights: &[f32]) -> (Vec<i8>, f32) {
    // s = max(|w|) / 127
    let max_abs = weights.iter().map(|w| w.abs()).fold(0.0, f32::max);
    let scale = max_abs / 127.0;

    // Q(w) = clip(round(w/s), -128, 127)
    let quantized = weights.iter().map(|w| {
        let q = (w / scale).round();
        q.clamp(-128.0, 127.0) as i8
    }).collect();

    (quantized, scale)
}

fn dequantize_symmetric(quantized: &[i8], scale: f32) -> Vec<f32> {
    // Åµ = q * s
    quantized.iter().map(|&q| q as f32 * scale).collect()
}
```

**é‡å­åŒ–èª¤å·®ã®æœŸå¾…å€¤**:
$$\mathbb{E}[|w - \hat{w}|] \approx \frac{s}{2} = \frac{\max(|w|)}{2(2^{b-1}-1)}$$

INT8: $\mathbb{E}[\text{error}] \approx \frac{\max(|w|)}{254}$
INT4: $\mathbb{E}[\text{error}] \approx \frac{\max(|w|)}{14}$ (INT8ã®~18å€)

##### éå¯¾ç§°é‡å­åŒ– (Asymmetric Quantization)

é‡ã¿ãŒéå¯¾ç§°ãªåˆ†å¸ƒ (e.g. ReLUå‡ºåŠ›, $w \in [0, \infty)$) ã®å ´åˆã€ã‚¼ãƒ­ç‚¹ $z$ ã‚’å°å…¥:

$$Q_\text{asym}(w) = \text{clip}\left(\text{round}\left(\frac{w}{s} + z\right), 0, 2^b-1\right)$$

where:
- $z$: ã‚¼ãƒ­ç‚¹ (zero-point)
- INT8éå¯¾ç§°: $q \in [0, 255]$

ã‚¹ã‚±ãƒ¼ãƒ«ã¨ã‚¼ãƒ­ç‚¹ã®è¨ˆç®—:
$$s = \frac{w_\max - w_\min}{2^b - 1}$$
$$z = -\text{round}\left(\frac{w_\min}{s}\right)$$

**é€†é‡å­åŒ–**:
$$\hat{w} = (q - z) \cdot s$$

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (Rust)**:

```rust
// Asymmetric INT8 quantization (unsigned)
fn quantize_asymmetric_int8(weights: &[f32]) -> (Vec<u8>, f32, i32) {
    let w_min = weights.iter().cloned().fold(f32::INFINITY, f32::min);
    let w_max = weights.iter().cloned().fold(f32::NEG_INFINITY, f32::max);

    // s = (w_max - w_min) / 255
    let scale = (w_max - w_min) / 255.0;

    // z = -round(w_min / s)
    let zero_point = -(w_min / scale).round() as i32;

    // Q(w) = clip(round(w/s + z), 0, 255)
    let quantized = weights.iter().map(|w| {
        let q = (w / scale).round() + zero_point as f32;
        q.clamp(0.0, 255.0) as u8
    }).collect();

    (quantized, scale, zero_point)
}

fn dequantize_asymmetric(quantized: &[u8], scale: f32, zero_point: i32) -> Vec<f32> {
    // Åµ = (q - z) * s
    quantized.iter().map(|&q| (q as i32 - zero_point) as f32 * scale).collect()
}
```

##### Per-Channel vs Per-Tensor é‡å­åŒ–

**Per-Tensor**: å…¨å±¤ã§1ã¤ã®ã‚¹ã‚±ãƒ¼ãƒ« $s$
**Per-Channel**: å‡ºåŠ›ãƒãƒ£ãƒãƒ«ã”ã¨ã«ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ« $s_i$

é‡ã¿è¡Œåˆ— $W \in \mathbb{R}^{C_\text{out} \times C_\text{in}}$ ã®å ´åˆ:

Per-Tensor:
$$s = \frac{\max_{i,j} |W_{ij}|}{2^{b-1}-1}$$

Per-Channel:
$$s_i = \frac{\max_j |W_{ij}|}{2^{b-1}-1}, \quad i=1,\ldots,C_\text{out}$$

**ç²¾åº¦æ¯”è¼ƒ** [^5]:
- Per-Tensor INT8: ~1% perplexityå¢—
- Per-Channel INT8: ~0.3% perplexityå¢—
- Per-Tensor INT4: ~3-5% perplexityå¢—
- Per-Channel INT4: ~1-2% perplexityå¢—

**Per-Tokené‡å­åŒ–** (Activations):
Activation $X \in \mathbb{R}^{B \times S \times D}$ (Batch Ã— Seq Ã— Dim) ã«å¯¾ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒ«:

$$s_{b,t} = \frac{\max_d |X_{b,t,d}|}{2^{b-1}-1}, \quad t=1,\ldots,S$$

:::message alert
**è½ã¨ã—ç©´**: Per-Channelã¯æ¨è«–æ™‚ã«è¿½åŠ æ¼”ç®—ãŒå¿…è¦ã€‚è¡Œåˆ—ç© $Y = XW^T$ ã®é‡å­åŒ–ç‰ˆ:
$$Y_{ij} = \sum_k (X_{ik} \cdot s_X) (W_{jk}^Q \cdot s_{W,j}) = s_X \sum_k X_{ik} \left(\sum_j W_{jk}^Q s_{W,j}\right)$$
ã‚¹ã‚±ãƒ¼ãƒ« $s_{W,j}$ ãŒãƒãƒ£ãƒãƒ«ã”ã¨ã«ç•°ãªã‚‹ â†’ å†…ç©å¾Œã«ã‚¹ã‚±ãƒ¼ãƒ«è£œæ­£ãŒå¿…è¦ã€‚
:::

#### 3.A.2 FP8é‡å­åŒ– â€” E4M3 vs E5M2

FP8 (8-bit floating point) ã¯**IEEE 754ã®ç°¡æ˜“ç‰ˆ** [^2]ã€‚

##### E4M3ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (1 sign + 4 exponent + 3 mantissa)

$$\text{value} = (-1)^s \times 2^{e-7} \times (1 + \frac{m}{8})$$

where:
- $s \in \{0,1\}$: ç¬¦å·ãƒ“ãƒƒãƒˆ
- $e \in [0,15]$: æŒ‡æ•° (4-bit)
- $m \in [0,7]$: ä»®æ•° (3-bit)

**è¡¨ç¾å¯èƒ½ç¯„å›²**:
- æœ€å°æ­£è¦æ•°: $2^{-6} \times 1 = 0.015625$
- æœ€å¤§æ­£è¦æ•°: $2^{8} \times (1 + 7/8) = 448$
- Subnormal: $e=0$ â†’ $2^{-6} \times (m/8)$ (æœ€å° $0.001953$)

**E5M2ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ** (1 sign + 5 exponent + 2 mantissa):
$$\text{value} = (-1)^s \times 2^{e-15} \times (1 + \frac{m}{4})$$

ç¯„å›²: $[2^{-14}, 2^{16} \times 1.75] = [0.000061, 57344]$

**æ¯”è¼ƒè¡¨**:

| Format | Exponent | Mantissa | Range | Precision | Use Case |
|:-------|:---------|:---------|:------|:----------|:---------|
| E4M3 | 4-bit | 3-bit | $\pm 448$ | é«˜ | æ¨è«– (KV-Cache) |
| E5M2 | 5-bit | 2-bit | $\pm 57344$ | ä½ | å­¦ç¿’ (å‹¾é…) |

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (Rust)**:

```rust
// FP8-E4M3 quantization (simplified - no hardware support)
#[derive(Copy, Clone)]
struct FP8E4M3 {
    bits: u8,  // 1-bit sign + 4-bit exp + 3-bit mantissa
}

impl FP8E4M3 {
    fn from_f32(val: f32) -> Self {
        if val == 0.0 {
            return FP8E4M3 { bits: 0 };
        }

        let sign = if val < 0.0 { 1u8 << 7 } else { 0 };
        let abs_val = val.abs();

        // Clamp to E4M3 range [2^-6, 448]
        let clamped = abs_val.clamp(0.015625, 448.0);

        // Extract exponent: val = 2^e * (1 + m/8)
        let log2 = clamped.log2();
        let e_unbiased = log2.floor() as i32;
        let e = (e_unbiased + 7).clamp(0, 15) as u8;  // Bias = 7

        // Extract mantissa
        let mantissa_float = clamped / 2f32.powi(e_unbiased) - 1.0;
        let m = (mantissa_float * 8.0).round().clamp(0.0, 7.0) as u8;

        FP8E4M3 {
            bits: sign | (e << 3) | m,
        }
    }

    fn to_f32(self) -> f32 {
        let sign_bit = (self.bits >> 7) & 1;
        let exp = (self.bits >> 3) & 0x0F;
        let mantissa = self.bits & 0x07;

        if exp == 0 {
            // Subnormal
            let val = 2f32.powi(-6) * (mantissa as f32 / 8.0);
            return if sign_bit == 1 { -val } else { val };
        }

        // Normal: 2^(e-7) * (1 + m/8)
        let e_unbiased = exp as i32 - 7;
        let val = 2f32.powi(e_unbiased) * (1.0 + mantissa as f32 / 8.0);

        if sign_bit == 1 { -val } else { val }
    }
}

// Quantize weight tensor to FP8-E4M3
fn quantize_fp8_e4m3(weights: &[f32]) -> Vec<FP8E4M3> {
    weights.iter().map(|&w| FP8E4M3::from_f32(w)).collect()
}

fn dequantize_fp8_e4m3(quantized: &[FP8E4M3]) -> Vec<f32> {
    quantized.iter().map(|q| q.to_f32()).collect()
}
```

**FP8é‡å­åŒ–èª¤å·®**:
E4M3ã®ç›¸å¯¾èª¤å·® (ä»®æ•°3-bit):
$$\epsilon_\text{rel} \approx 2^{-3} = 0.125 = 12.5\%$$

INT8ã®çµ¶å¯¾èª¤å·® (256å€¤):
$$\epsilon_\text{abs} \approx \frac{s}{2} = \frac{\max(|w|)}{254}$$

FP8ã¯**å‹•çš„ç¯„å›²ãŒåºƒã„å€¤**ã«æœ‰åˆ©ã€‚ä¾‹: $w \in [0.01, 100]$ â†’ INT8ã¯ $s=100/127=0.79$ (å°ã•ã„å€¤ã®ç²¾åº¦æœ€æ‚ª), FP8ã¯æŒ‡æ•°ã§è‡ªå‹•èª¿æ•´ã€‚

#### 3.A.3 KV-Cacheé‡å­åŒ–

Transformeræ¨è«–ã®ãƒ¡ãƒ¢ãƒªãƒœãƒˆãƒ«ãƒãƒƒã‚¯: KV-Cacheã€‚

Attention:
$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

KV-Cacheã‚µã‚¤ã‚º (per layer):
$$\text{Memory} = 2 \times B \times S \times d_\text{model} \times \text{sizeof(dtype)}$$

where $B$=batch, $S$=sequence lengthã€‚

**ä¾‹**: LLaMA-70B (80 layers, $d=8192$, FP16)
1 token: $2 \times 80 \times 8192 \times 2 = 2.62$ MB
Context 32K: $2.62 \times 32768 = 85.9$ GB (batch=1ã§ã‚‚GPUç ´ç¶»)

**KV-Cache FP8-E4M3é‡å­åŒ–** [^6]:

Per-token ã‚¹ã‚±ãƒ¼ãƒ«:
$$s_t = \frac{\max(|K_t|, |V_t|)}{448}$$

é‡å­åŒ–:
$$K_t^{FP8} = \text{FP8-E4M3}(K_t), \quad V_t^{FP8} = \text{FP8-E4M3}(V_t)$$

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: FP16 â†’ FP8ã§**2å€å‰Šæ¸›**ã€‚ä¸Šè¨˜ä¾‹: 85.9 GB â†’ 42.9 GB

**ç²¾åº¦åŠ£åŒ–**: vLLMå®Ÿæ¸¬ [^6] ã§ perplexity +0.1-0.3% (ã»ã¼ç„¡è¦–å¯èƒ½)ã€‚

:::message
**QuantSpec [^1]ã®é©æ–°**: KV-Cacheã‚’INT4é‡å­åŒ– + Self-Speculative Decodingã§ã€
**ãƒ¡ãƒ¢ãƒª4å€å‰Šæ¸› + 2.5å€é«˜é€ŸåŒ–** ã‚’åŒæ™‚é”æˆã€‚å—ç†ç‡>90%ã‚’ç¶­æŒã€‚
:::

#### 3.A.4 QAT vs PTQ

##### PTQ (Post-Training Quantization)

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥é‡å­åŒ–ã€‚**è¿½åŠ å­¦ç¿’ãªã—**ã€‚

æ‰‹é †:
1. Calibration data (100-1000ã‚µãƒ³ãƒ—ãƒ«) ã§çµ±è¨ˆåé›†
2. ã‚¹ã‚±ãƒ¼ãƒ« $s$ ã‚’æ±ºå®š: $s = \frac{\max(|w|)}{2^{b-1}-1}$
3. é‡å­åŒ–: $w^Q = \text{round}(w/s)$

**åˆ©ç‚¹**: é«˜é€Ÿ (æ•°åˆ†), å­¦ç¿’ä¸è¦
**æ¬ ç‚¹**: ç²¾åº¦åŠ£åŒ– (INT4ã§3-5%)

##### QAT (Quantization-Aware Training)

å­¦ç¿’ä¸­ã«é‡å­åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã€‚

Forward pass:
$$\tilde{w} = Q(w) = \text{round}(w/s) \cdot s$$

Backward pass: **Straight-Through Estimator** (STE) [^7]
$$\frac{\partial L}{\partial w} \approx \frac{\partial L}{\partial \tilde{w}}$$

$\text{round}$ã¯å¾®åˆ†ä¸å¯èƒ½ â†’ å‹¾é…ã‚’ç´ é€šã—ã•ã›ã‚‹(!)

**STEæ•°å¼**:
$$\frac{\partial \text{round}(x)}{\partial x} := 1$$

**QATã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```
for epoch in 1..N:
    for batch in data:
        # Forward: quantize weights
        w_quant = round(w / s) * s

        # Compute loss with quantized weights
        loss = forward(x, w_quant)

        # Backward: STE gradient
        grad_w = backward(loss)

        # Update original FP32 weights
        w = w - lr * grad_w
```

**åˆ©ç‚¹**: ç²¾åº¦åŠ£åŒ–æœ€å° (INT4ã§<1%)
**æ¬ ç‚¹**: å­¦ç¿’ã‚³ã‚¹ãƒˆ (GPUæ™‚é–“Ã—10-20%)

**PTQ vs QATæ¯”è¼ƒ** [^5]:

| Method | LLaMA-7B INT4 Perplexity | å­¦ç¿’æ™‚é–“ | å¿…è¦ãƒ‡ãƒ¼ã‚¿ |
|:-------|:------------------------|:---------|:----------|
| FP16 baseline | 5.68 | - | - |
| PTQ | 5.95 (+0.27) | 5 min | 1K samples |
| QAT | 5.72 (+0.04) | 8 hours | Full dataset |

**å®Ÿç”¨çš„åˆ¤æ–­**:
- INT8: PTQã§ååˆ†
- INT4: ã‚¿ã‚¹ã‚¯ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚‰QAT, ãã‚Œä»¥å¤–PTQ
- INT2: QATå¿…é ˆ (PTQã¯ç ´ç¶»)

:::details QATã®å®Ÿè£… (PyTorchä¾‹)
```python
import torch
import torch.nn as nn

class QuantizedLinear(nn.Module):
    def __init__(self, in_features, out_features, bits=8):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bits = bits
        self.register_buffer('scale', torch.ones(1))

    def forward(self, x):
        # Compute scale
        max_val = self.weight.abs().max()
        self.scale = max_val / (2**(self.bits-1) - 1)

        # Fake quantization with STE
        weight_quant = self.fake_quantize(self.weight, self.scale)

        return nn.functional.linear(x, weight_quant)

    @staticmethod
    def fake_quantize(x, scale):
        # Forward: quantize
        x_quant = torch.round(x / scale) * scale

        # Backward: STE (gradient flows through as-is)
        return x_quant
```
:::

#### 3.A.5 âš”ï¸ Boss Battle: FP8 E4M3é‡å­åŒ–ã®å®Œå…¨åˆ†è§£

LLaMA-13Bã®ç¬¬1å±¤FFNé‡ã¿ $W \in \mathbb{R}^{5120 \times 13824}$ ã‚’FP8-E4M3ã«é‡å­åŒ–ã›ã‚ˆã€‚

**ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±**:
- $W$ã®çµ±è¨ˆ: $\mu = 0.02$, $\sigma = 0.35$, $\max(|W|) = 2.3$
- E4M3ç¯„å›²: $[-448, 448]$
- Per-Channelé‡å­åŒ–ã‚’ä½¿ç”¨

**è§£ç­”**:

**(1) Per-Channelã‚¹ã‚±ãƒ¼ãƒ«ã®è¨ˆç®—**

å‡ºåŠ›ãƒãƒ£ãƒãƒ« $i$ ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒ«:
$$s_i = \frac{\max_j |W_{ij}|}{448}$$

å…¨ãƒãƒ£ãƒãƒ«ã®çµ±è¨ˆã‹ã‚‰æ¨å®š:
$$s_i \sim \mathcal{N}\left(0.02, \frac{0.35}{\sqrt{13824}}\right) \approx \mathcal{N}(0.02, 0.003)$$

æœ€å¤§å€¤: $s_\max \approx 2.3 / 448 = 0.00513$

**(2) é‡å­åŒ–èª¤å·®ã®æœŸå¾…å€¤**

E4M3ã®ç›¸å¯¾èª¤å·® (mantissa 3-bit):
$$\epsilon_\text{rel} = 2^{-3} = 0.125$$

å„é‡ã¿ã®é‡å­åŒ–èª¤å·®:
$$|w_{ij} - \hat{w}_{ij}| \approx |w_{ij}| \times \epsilon_\text{rel}$$

æœŸå¾…å€¤ (ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®çµ¶å¯¾å€¤æœŸå¾…å€¤: $\mathbb{E}[|X|] = \sigma\sqrt{2/\pi}$):
$$\mathbb{E}[|W|] = 0.35 \times \sqrt{2/\pi} \approx 0.279$$

$$\mathbb{E}[\text{error}] = 0.279 \times 0.125 \approx 0.0349$$

**(3) Perplexity ã¸ã®å½±éŸ¿æ¨å®š**

FFNã®å‡ºåŠ›èª¤å·®:
$$\Delta Y = X \cdot \Delta W^T$$

èª¤å·®ã®åˆ†æ•£ (ç‹¬ç«‹æ€§ä»®å®š):
$$\text{Var}[\Delta Y] = d_\text{in} \times \mathbb{E}[\text{error}^2] = 13824 \times 0.0349^2 \approx 16.8$$

æ¨™æº–åå·®: $\sigma_{\Delta Y} = \sqrt{16.8} \approx 4.1$

Perplexityå¢—åŠ  (çµŒé¨“å‰‡ [^2]): $\Delta \text{PPL} \approx 0.01 \times \sigma_{\Delta Y} / \sigma_Y$

$\sigma_Y \approx 10$ (FFNå‡ºåŠ›ã®å…¸å‹å€¤) ã¨ã—ã¦:
$$\Delta \text{PPL} \approx 0.01 \times 4.1 / 10 = 0.0041 = 0.41\%$$

**çµè«–**: FP8-E4M3 Per-Channelé‡å­åŒ–ã§ perplexity +0.4% (å®Ÿæ¸¬å€¤ [^2] ã® +0.3-0.5% ã¨ä¸€è‡´)ã€‚

:::message
**ãƒœã‚¹æ’ƒç ´!** FP8é‡å­åŒ–ã®æ•°å¼ã‚’å®Œå…¨åˆ†è§£ã—ã€ç²¾åº¦åŠ£åŒ–ã‚’ç†è«–çš„ã«äºˆæ¸¬ã§ããŸã€‚
:::

---

### Part B: è’¸ç•™ & Speculative Decoding (~600è¡Œ)

#### 3.B.1 Knowledge Distillationå®Œå…¨ç‰ˆ

Hinton+ 2015 [^3] ã®æ ¸å¿ƒ: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®**soft targets**ã‚’å­¦ç¿’ã™ã‚‹ã€‚

##### è’¸ç•™ã®å®šå¼åŒ–

æ•™å¸«ãƒ¢ãƒ‡ãƒ« (teacher): $p_T(y|x;\theta_T)$
ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ« (student): $p_S(y|x;\theta_S)$

**Soft targets**: æ¸©åº¦ $T$ ã‚’ä¸Šã’ãŸSoftmax
$$p_i^T(T) = \frac{\exp(z_i^T / T)}{\sum_j \exp(z_j^T / T)}$$

where $z^T$ = æ•™å¸«ã®logitsã€‚

**è’¸ç•™æå¤±**:
$$\mathcal{L}_\text{distill} = \alpha \cdot \mathcal{L}_\text{soft} + (1-\alpha) \cdot \mathcal{L}_\text{hard}$$

where:
$$\mathcal{L}_\text{soft} = T^2 \cdot D_\text{KL}(p^T(T) \| p^S(T))$$
$$\mathcal{L}_\text{hard} = \text{CE}(y_\text{true}, p^S(1))$$

**$T^2$ã®ç”±æ¥** [^3]:
æ¸©åº¦ $T$ ã§å‹¾é…ãŒ $1/T$ ã«ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ â†’ $T^2$ ã§è£œæ­£ã€‚

KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹å±•é–‹ (ç¬¬6å›):
$$D_\text{KL}(p \| q) = \sum_i p_i \log \frac{p_i}{q_i} = \sum_i p_i \log p_i - \sum_i p_i \log q_i$$

è’¸ç•™ã®å‹¾é…:
$$\frac{\partial \mathcal{L}_\text{soft}}{\partial z_i^S} = T^2 \cdot \frac{\partial}{\partial z_i^S} \left[ -\sum_j p_j^T(T) \log p_j^S(T) \right]$$

Softmaxã®å‹¾é… (ç¬¬3å›):
$$\frac{\partial p_i}{\partial z_j} = p_i (\delta_{ij} - p_j)$$

ä»£å…¥:
$$\frac{\partial \mathcal{L}_\text{soft}}{\partial z_i^S} = T^2 \cdot \frac{1}{T} \left[ p_i^S(T) - p_i^T(T) \right] = T \cdot \left[ p_i^S(T) - p_i^T(T) \right]$$

$T$ãŒå¤§ãã„ã»ã©å‹¾é…ãŒå¤§ããã€å­¦ç¿’ãŒå®‰å®šã€‚

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (Julia)**:

```julia
using Flux, Statistics

# Softmax with temperature
function softmax_T(logits::Vector{Float64}, T::Float64=1.0)
    z = logits ./ T
    exp_z = exp.(z .- maximum(z))
    return exp_z ./ sum(exp_z)
end

# Distillation loss
function distillation_loss(
    logits_teacher::Vector{Float64},
    logits_student::Vector{Float64},
    y_true::Int,
    T::Float64=3.0,
    Î±::Float64=0.7
)
    # Soft target loss: T^2 * KL(p_T(T) || p_S(T))
    p_T = softmax_T(logits_teacher, T)
    p_S = softmax_T(logits_student, T)

    soft_loss = T^2 * sum(p_T .* log.(p_T ./ p_S))

    # Hard target loss: CE(y_true, p_S(1))
    p_S_hard = softmax_T(logits_student, 1.0)
    hard_loss = -log(p_S_hard[y_true])

    return Î± * soft_loss + (1 - Î±) * hard_loss
end

# Example
logits_T = [4.2, 1.3, 0.8]  # Teacher: confident in class 1
logits_S = [2.1, 1.5, 0.9]  # Student: less confident
y_true = 1

loss = distillation_loss(logits_T, logits_S, y_true, 3.0, 0.7)
println("Distillation loss: $(round(loss, digits=4))")

# Temperature effect
println("\nTemperature effect on soft targets:")
for T in [1.0, 3.0, 10.0]
    p_T = softmax_T(logits_T, T)
    println("  T=$T: $(round.(p_T, digits=4))")
end
```

å‡ºåŠ›:
```
Distillation loss: 0.8324

Temperature effect on soft targets:
  T=1.0: [0.8808, 0.0831, 0.0361]
  T=3.0: [0.5926, 0.2386, 0.1688]
  T=10.0: [0.4129, 0.3248, 0.2623]
```

##### "Dark Knowledge" ã®æ­£ä½“

æ¸©åº¦ $T$ ã‚’ä¸Šã’ã‚‹ã¨ã€æ•™å¸«ã®**ã‚¯ãƒ©ã‚¹é–“ã®ç›¸å¯¾çš„ãªé¡ä¼¼åº¦**ãŒéœ²å‡ºã™ã‚‹ã€‚

ä¾‹: ç”»åƒåˆ†é¡ (çŠ¬, çŒ«, è»Š)
- æ•™å¸«logits: $[5.0, 3.5, 0.2]$
- $T=1$: $[0.82, 0.16, 0.02]$ â†’ çŠ¬ã«ç¢ºä¿¡
- $T=5$: $[0.49, 0.39, 0.12]$ â†’ ã€ŒçŒ«ã‚‚çŠ¬ã«ä¼¼ã¦ã„ã‚‹ã€ãŒè¦‹ãˆã‚‹

ã“ã®**ã€Œä¼¼ã¦ã„ã‚‹ã€æƒ…å ±**ãŒ dark knowledge [^3]ã€‚ç”Ÿå¾’ã¯ã€Œæ­£è§£ã ã‘ã€ã§ãªãã€Œé–“é•ã„ã®ç¨‹åº¦ã€ã‚‚å­¦ã¶ã€‚

##### è’¸ç•™ã®åŠ¹æœ â€” ãªãœå°ãƒ¢ãƒ‡ãƒ«ãŒå¤§ãƒ¢ãƒ‡ãƒ«ã«è¿‘ã¥ãã®ã‹

**ä»®èª¬1: æ­£å‰‡åŒ–åŠ¹æœ**
æ•™å¸«ã®soft targetsã¯å¹³æ»‘åŒ–ã•ã‚ŒãŸåˆ†å¸ƒ â†’ éå­¦ç¿’æŠ‘åˆ¶ã€‚

**ä»®èª¬2: Label smoothing**
One-hot $[1,0,0]$ ã‚ˆã‚Š $[0.7, 0.2, 0.1]$ ã®æ–¹ãŒæ±åŒ–ã™ã‚‹ (Szegedy+ 2016)ã€‚

**ä»®èª¬3: ç‰¹å¾´ç©ºé–“ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ**
KLæœ€å°åŒ– = ç”Ÿå¾’ãŒæ•™å¸«ã®æ±ºå®šå¢ƒç•Œã‚’æ¨¡å€£ â†’ åŒã˜ç‰¹å¾´è¡¨ç¾ã‚’å­¦ç¿’ã€‚

**å®Ÿè¨¼** [^8]: BERT-base (110M) â†’ DistilBERT (66M, 6å±¤)
- è’¸ç•™ãªã—: 79% accuracy
- è’¸ç•™ã‚ã‚Š: 97% (teacher 100%åŸºæº–)

ç²¾åº¦ä¿æŒç‡97%ã§40%ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸› â†’ æ¨è«–2.5å€é«˜é€ŸåŒ–ã€‚

#### 3.B.2 Speculative Decodingå®Œå…¨ç‰ˆ

##### è‡ªå·±å›å¸°æ¨è«–ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

Transformeræ¨è«–: ãƒˆãƒ¼ã‚¯ãƒ³ã‚’1ã¤ãšã¤ç”Ÿæˆã€‚

$$p(x_{1:T}) = \prod_{t=1}^T p(x_t \mid x_{<t})$$

å„ã‚¹ãƒ†ãƒƒãƒ—:
1. KV-Cacheèª­ã¿è¾¼ã¿ (ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…å¾‹é€Ÿ)
2. Attentionè¨ˆç®— ($O(T \cdot d^2)$)
3. 1ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ

**GPUä½¿ç”¨ç‡**: é€šå¸¸10-20% (ãƒ¡ãƒ¢ãƒªI/Oå¾…ã¡)ã€‚

Speculative Decoding [^4] ã¯**ä¸¦åˆ—æ¤œè¨¼**ã§ã“ã‚Œã‚’è§£æ±ºã€‚

##### Speculative Decodingã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**Draft Model** $q(x)$: å°å‹é«˜é€Ÿãƒ¢ãƒ‡ãƒ« (e.g. LLaMA-7B ã® 4å±¤ç‰ˆ)
**Target Model** $p(x)$: å¤§å‹æ­£ç¢ºãƒ¢ãƒ‡ãƒ« (e.g. LLaMA-70B)

æ‰‹é †:
1. Draft: $k$ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ•æ©Ÿçš„ç”Ÿæˆ $x_1, \ldots, x_k \sim q$
2. Verify: Target modelã§ä¸¦åˆ—æ¤œè¨¼ $p(x_i \mid x_{<i})$
3. Accept/Reject: å—ç†ç¢ºç‡ $\alpha_i = \min(1, p(x_i)/q(x_i))$ ã§åˆ¤å®š
4. Rejection Sampling: æ£„å´æ™‚ã¯$p$ã‹ã‚‰å†ã‚µãƒ³ãƒ—ãƒ«

**æ•°å­¦çš„ä¿è¨¼**: æœ€çµ‚åˆ†å¸ƒã¯**å®Œå…¨ã«** $p(x)$ ã¨ä¸€è‡´ (è¿‘ä¼¼ãªã—!)ã€‚

##### å—ç†ç¢ºç‡ã®å°å‡º

Modified Rejection Sampling [^4]:

$$\alpha_i = \min\left(1, \frac{p(x_i \mid x_{<i})}{q(x_i \mid x_{<i})}\right)$$

å—ç†:
$$x_i \sim \begin{cases}
x_i & \text{with prob } \alpha_i \\
p'(x \mid x_{<i}) & \text{with prob } 1-\alpha_i
\end{cases}$$

where:
$$p'(x) = \frac{\max(0, p(x) - q(x))}{\sum_y \max(0, p(y) - q(y))}$$

**æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ (Julia)**:

```julia
using Random, Distributions

# Draft model (simplified: uniform over top-k)
function draft_model(context::String, k::Int=3, vocab_size::Int=10)
    # Return k candidate tokens + log probs
    candidates = rand(1:vocab_size, k)
    log_probs_q = log.(rand(k) .+ 0.1)  # Simulated log q(x)
    return candidates, log_probs_q
end

# Target model
function target_model(context::String, candidates::Vector{Int})
    # Return log probs for candidates
    log_probs_p = log.(rand(length(candidates)) .+ 0.2)  # Simulated log p(x)
    return log_probs_p
end

# Speculative decoding: one round
function speculative_round(context::String, k::Int=3)
    # 1. Draft: generate k tokens
    candidates, log_q = draft_model(context, k)

    # 2. Verify: target model computes p(x) for all candidates in parallel
    log_p = target_model(context, candidates)

    # 3. Accept/Reject
    accepted = Int[]
    for i in 1:k
        Î± = min(1.0, exp(log_p[i] - log_q[i]))

        if rand() < Î±
            push!(accepted, candidates[i])
        else
            # Rejection: sample from p'(x) = max(0, p(x) - q(x))
            # (Simplified: just stop here)
            break
        end
    end

    return accepted
end

# Simulate multiple rounds
total_accepted = 0
total_drafted = 0
n_rounds = 100

for round in 1:n_rounds
    accepted = speculative_round("context", 3)
    total_accepted += length(accepted)
    total_drafted += 3
end

avg_accepted = total_accepted / n_rounds
println("Average accepted tokens per round: $(round(avg_accepted, digits=2))")
println("Expected speedup: ~$(round(1 + avg_accepted, digits=2))x")
```

å‡ºåŠ›ä¾‹:
```
Average accepted tokens per round: 1.47
Expected speedup: ~2.47x
```

##### æœŸå¾…å—ç†é•·ã®è§£æ

å—ç†ç¢ºç‡ $\alpha_i$ ãŒç‹¬ç«‹ã¨ä»®å®š (å®Ÿéš›ã¯ç›¸é–¢ã‚ã‚Š):

$$\mathbb{E}[\tau] = \sum_{i=1}^{k} \prod_{j=1}^{i} \alpha_j$$

$\alpha_i = \alpha$ (å®šæ•°) ãªã‚‰:
$$\mathbb{E}[\tau] = \frac{1 - \alpha^{k+1}}{1 - \alpha} - 1 \approx \frac{\alpha}{1-\alpha} \quad (\alpha < 1, k \to \infty)$$

ä¾‹:
- $\alpha = 0.6$: $\mathbb{E}[\tau] = 1.5$ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ©ã‚¦ãƒ³ãƒ‰ â†’ 2.5å€é«˜é€ŸåŒ–
- $\alpha = 0.8$: $\mathbb{E}[\tau] = 4.0$ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ©ã‚¦ãƒ³ãƒ‰ â†’ 5å€é«˜é€ŸåŒ–
- $\alpha = 0.9$: $\mathbb{E}[\tau] = 9.0$ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ©ã‚¦ãƒ³ãƒ‰ â†’ 10å€é«˜é€ŸåŒ–

**å—ç†ç‡$\alpha$ã‚’ä¸Šã’ã‚‹æ–¹æ³•**:
1. Draft modelã‚’å¼·åŒ– (ã‚ˆã‚Šå¤§ãã, è’¸ç•™)
2. Temperatureèª¿æ•´ ($T=1.2$ã§Draftã‚’ä¿å®ˆçš„ã«)
3. æœ€åˆã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿æ¤œè¨¼ (beam searchã¨çµ„ã¿åˆã‚ã›)

#### 3.B.3 QuantSpec â€” INT4é‡å­åŒ– + Self-Speculative

Apple 2025 [^1] ã®é©æ–°: **Draft = Target ã®é‡å­åŒ–ç‰ˆ**ã€‚

å¾“æ¥ã®Speculative:
- Draft: åˆ¥ãƒ¢ãƒ‡ãƒ« (LLaMA-7B)
- Target: LLaMA-70B
- ãƒ¡ãƒ¢ãƒª: ä¸¡æ–¹ãƒ­ãƒ¼ãƒ‰ â†’ +30%

QuantSpec:
- Draft: LLaMA-70Bã®INT4é‡å­åŒ–ç‰ˆ (4-bit weights + 4-bit KV-Cache)
- Target: åŒã˜LLaMA-70B (FP16)
- ãƒ¡ãƒ¢ãƒª: INT4ã¯8å€åœ§ç¸® â†’ **è¿½åŠ ãƒ¡ãƒ¢ãƒªã»ã¼ã‚¼ãƒ­** (ã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿)

##### QuantSpecã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  LLaMA-70B FP16  â”‚ â† Target (æ­£ç¢º)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†‘
                 â”‚ Verify
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ LLaMA-70B INT4   â”‚ â† Draft (é«˜é€Ÿ)
         â”‚ + INT4 KV-Cache  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Hierarchical KV-Cache**:
- Target KV-Cache: FP16
- Draft KV-Cache: INT4, **Targetã¨å…±æœ‰**

å…±æœ‰æ–¹æ³•:
$$\text{KV}^\text{INT4} = Q_\text{INT4}(\text{KV}^\text{FP16})$$

æ¤œè¨¼æ™‚:
$$p_\text{target} = \text{Attention}(\text{KV}^\text{FP16})$$
$$p_\text{draft} = \text{Attention}(\text{KV}^\text{INT4})$$

##### QuantSpecã®æ€§èƒ½

Appleå®Ÿæ¸¬ [^1]:
- å—ç†ç‡: **>90%** (é€šå¸¸ã®Speculativeã¯60-80%)
- Speedup: **~2.5å€** (128K context)
- ãƒ¡ãƒ¢ãƒª: **-30%** (INT4é‡å­åŒ–åŠ¹æœ)

ãªãœå—ç†ç‡ãŒé«˜ã„ã®ã‹?
Draft = Targetã®é‡å­åŒ–ç‰ˆ â†’ **åŒã˜æ±ºå®šå¢ƒç•Œã‚’è¿‘ä¼¼** â†’ $p_\text{draft} \approx p_\text{target}$

æ•°å€¤ä¾‹ (logits):
- Target: $[3.2, 1.1, 0.5]$
- Draft (INT4): $[3.1, 1.0, 0.6]$ â†’ ã»ã¼åŒã˜é †åº

å—ç†ç¢ºç‡:
$$\alpha = \min(1, \frac{0.92}{0.91}) = 1.0 \quad \text{(accept!)}$$

##### âš”ï¸ Boss Battle: QuantSpecå—ç†ç‡>90%ã®æ•°å¼åˆ†è§£

LLaMA-13Bã‚’QuantSpecã§é‹ç”¨ã€‚Draft=INT4é‡å­åŒ–, Target=FP16ã€‚

**ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±**:
- Logitsã®åˆ†å¸ƒ: $z \sim \mathcal{N}(0, 2)$ (æ¨™æº–çš„ãªLLM)
- INT4é‡å­åŒ–èª¤å·®: $\epsilon \sim \mathcal{N}(0, 0.1)$ (per-token scaleä½¿ç”¨)
- Top-1ãƒˆãƒ¼ã‚¯ãƒ³ã§è©•ä¾¡

**è§£ç­”**:

**(1) å—ç†ç¢ºç‡ã®æœŸå¾…å€¤**

Target logits: $z_p \sim \mathcal{N}(0, 2)$
Draft logits: $z_q = z_p + \epsilon$, $\epsilon \sim \mathcal{N}(0, 0.1)$

Top-1ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¸€è‡´ã™ã‚‹ç¢ºç‡ (é †åºä¿å­˜ç¢ºç‡):

$$P(\arg\max z_p = \arg\max z_q)$$

2ã‚¯ãƒ©ã‚¹ã®å ´åˆ (ç°¡ç•¥åŒ–):
$$z_p^{(1)} - z_p^{(2)} > 0 \quad \text{and} \quad z_q^{(1)} - z_q^{(2)} > 0$$

èª¤å·®é …:
$$\Delta z = (z_p^{(1)} + \epsilon^{(1)}) - (z_p^{(2)} + \epsilon^{(2)}) = \Delta z_p + \Delta \epsilon$$

$\Delta z_p \sim \mathcal{N}(0, 2\cdot 2) = \mathcal{N}(0, 4)$ (ç‹¬ç«‹)
$\Delta \epsilon \sim \mathcal{N}(0, 2 \cdot 0.1) = \mathcal{N}(0, 0.2)$

é †åºãŒå¤‰ã‚ã‚‹ç¢ºç‡ (ç¬¦å·åè»¢):
$$P(\text{sign}(\Delta z_p) \neq \text{sign}(\Delta z_p + \Delta \epsilon))$$

$|\Delta z_p|$ãŒå¤§ãã„ã»ã©é †åºä¿å­˜ã€‚SNR:
$$\text{SNR} = \frac{\sigma_{\Delta z_p}}{\sigma_{\Delta \epsilon}} = \frac{2}{0.45} \approx 4.47$$

èª¤ã‚Šç¢ºç‡ (ã‚¬ã‚¦ã‚¹è¿‘ä¼¼):
$$P_\text{error} \approx Q(\text{SNR}) = Q(4.47) \approx 4 \times 10^{-6}$$

é †åºä¿å­˜ç¢ºç‡:
$$P_\text{agree} = 1 - P_\text{error} \approx 99.9996\%$$

**(2) å—ç†ç¢ºç‡ (Softmaxå¾Œ)**

Softmaxç¢ºç‡:
$$p_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}$$

Top-1ã®å ´åˆ ($z_1 \gg z_2$):
$$p_1 \approx \frac{\exp(z_1)}{\exp(z_1) + \exp(z_2)} = \frac{1}{1 + \exp(z_2 - z_1)} \approx 1 - \exp(z_2 - z_1)$$

$z_1 - z_2 \sim \mathcal{N}(0, 4)$ â†’ æœŸå¾…å€¤ $\mathbb{E}[z_1 - z_2] = 0$, but top-1ãªã®ã§$>0$ã€‚

æ¡ä»¶ä»˜ãæœŸå¾…å€¤ (truncated Gaussian):
$$\mathbb{E}[z_1 - z_2 \mid z_1 > z_2] = 2 \cdot \frac{\phi(0)}{1 - \Phi(0)} = 2 \cdot \frac{\phi(0)}{0.5} \approx 1.60$$

Draftèª¤å·®å¾Œ:
$$z_1^q - z_2^q = (z_1 - z_2) + (\epsilon_1 - \epsilon_2) \approx 1.60 + \mathcal{N}(0, 0.2)$$

å—ç†ç¢ºç‡:
$$\alpha = \min\left(1, \frac{p_p(x_1)}{p_q(x_1)}\right) = \min\left(1, \exp(z_1^p - z_1^q)\right)$$

èª¤å·® $\epsilon_1 \sim \mathcal{N}(0, 0.1)$ ãªã‚‰:
$$\mathbb{E}[\exp(\epsilon_1)] = \exp(\sigma^2/2) = \exp(0.1^2 / 2) \approx 1.005$$

$$\mathbb{E}[\alpha] \approx \min(1, 1.005) = 1.0 \quad \text{(ã»ã¼å¸¸ã«å—ç†)}$$

**(3) å—ç†ç‡>90%ã®æ¡ä»¶**

ä¸€èˆ¬ã«K-ã‚¯ãƒ©ã‚¹:
$$P_\text{accept} = P(\arg\max z_p = \arg\max z_q) \times \mathbb{E}[\alpha \mid \text{agree}]$$

ç¬¬1é … (é †åºä¿å­˜): ~99.9%
ç¬¬2é … (ç¢ºç‡æ¯”): ~100%

$$P_\text{accept} \approx 0.999 \times 1.0 = 99.9\%$$

**çµè«–**: INT4é‡å­åŒ–èª¤å·®$\sigma=0.1$ã§å—ç†ç‡~100%ã€‚å®Ÿæ¸¬90%ã¯ã€
- Multi-tokenåŠ¹æœ (3-5ãƒˆãƒ¼ã‚¯ãƒ³ã®ç©)
- Softmaxæ¸©åº¦èª¿æ•´ ($T=1.2$ã§å—ç†ç‡â†“)
- Beam searchå¹²æ¸‰

ã‚’è€ƒæ…®ã—ãŸå®Ÿç”¨å€¤ã€‚

:::message
**ãƒœã‚¹æ’ƒç ´!** QuantSpecã®å—ç†ç‡>90%ã‚’çµ±è¨ˆçš„ã«è¨¼æ˜ã—ã€INT4é‡å­åŒ–ãŒSpeculative Decodingã¨ç›¸æ€§æŠœç¾¤ãªç†ç”±ã‚’ç†è§£ã—ãŸã€‚
:::

---

### Part C: ğŸ¦€ Productionå“è³ªRustãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¨­è¨ˆ (~700è¡Œ)

æ¨è«–æœ€é©åŒ–ã‚’**æœ¬ç•ªç’°å¢ƒã§é‹ç”¨**ã™ã‚‹ã«ã¯ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ãƒ†ã‚¹ãƒˆã®4ã¤ãŒä¸å¯æ¬ ã€‚

#### 3.C.1 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¨­è¨ˆå®Œå…¨ç‰ˆ

Rustã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¯**å‹ã‚·ã‚¹ãƒ†ãƒ ã§å¼·åˆ¶**ã•ã‚Œã‚‹ã€‚

##### thiserror vs anyhow

**thiserror** [^9]: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç”¨ (å‘¼ã³å‡ºã—å´ãŒã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†)
**anyhow** [^9]: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ (ã‚¨ãƒ©ãƒ¼ã‚’é›†ç´„ã—ã¦è¡¨ç¤º)

```rust
// Library error with thiserror
use thiserror::Error;

#[derive(Error, Debug)]
pub enum QuantizationError {
    #[error("Invalid bit width: {0}, must be 2, 4, or 8")]
    InvalidBitWidth(u8),

    #[error("Empty weight tensor")]
    EmptyTensor,

    #[error("Scale computation failed: {0}")]
    ScaleError(String),

    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
}

// Application error with anyhow
use anyhow::{Context, Result};

fn load_and_quantize(path: &str, bits: u8) -> Result<Vec<i8>> {
    let weights = std::fs::read(path)
        .context("Failed to read weight file")?;

    let parsed: Vec<f32> = bincode::deserialize(&weights)
        .context("Failed to parse weights")?;

    quantize_int4(&parsed, bits)
        .context("Quantization failed")?;

    Ok(vec![])
}
```

**ä½¿ã„åˆ†ã‘**:
- `thiserror`: `pub enum MyError` â†’ å‘¼ã³å‡ºã—å´ãŒ`match`ã§å‡¦ç†
- `anyhow`: `Result<T>` â†’ `?`ã§ä¼æ’­, æœ€çµ‚çš„ã«`main`ã§ãƒ­ã‚°å‡ºåŠ›

##### Resultå‹ãƒ‘ã‚¿ãƒ¼ãƒ³

**Pattern 1: Early Return**
```rust
fn quantize_checked(weights: &[f32], bits: u8) -> Result<Vec<i8>, QuantizationError> {
    if weights.is_empty() {
        return Err(QuantizationError::EmptyTensor);
    }

    if ![2, 4, 8].contains(&bits) {
        return Err(QuantizationError::InvalidBitWidth(bits));
    }

    // Success path
    Ok(quantize_symmetric_int8(weights).0)
}
```

**Pattern 2: Context Chain**
```rust
use anyhow::Context;

fn load_model(path: &str) -> anyhow::Result<Model> {
    let config = std::fs::read_to_string(format!("{}/config.json", path))
        .context("Failed to read config")?;

    let model = Model::from_json(&config)
        .context("Failed to parse model config")?
        .load_weights(path)
        .context("Failed to load weights")?;

    Ok(model)
}
```

ã‚¨ãƒ©ãƒ¼å‡ºåŠ›:
```
Error: Failed to load weights
Caused by:
    0: Failed to read config
    1: No such file or directory (os error 2)
```

**Pattern 3: Fallible Iterator**
```rust
fn quantize_layers(layers: Vec<Vec<f32>>) -> Result<Vec<Vec<i8>>> {
    layers.into_iter()
        .map(|weights| quantize_checked(&weights, 8))
        .collect()  // Short-circuits on first error
}
```

##### ãƒ‘ãƒ‹ãƒƒã‚¯å¢ƒç•Œè¨­è¨ˆ

**åŸå‰‡**: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯**çµ¶å¯¾ã«ãƒ‘ãƒ‹ãƒƒã‚¯ã—ãªã„** (callerè²¬ä»»)ã€‚

```rust
// âŒ Bad: panic in library
pub fn quantize_unchecked(weights: &[f32]) -> Vec<i8> {
    assert!(!weights.is_empty(), "Empty tensor");  // PANIC!
    // ...
}

// âœ… Good: return Result
pub fn quantize(weights: &[f32]) -> Result<Vec<i8>, QuantizationError> {
    if weights.is_empty() {
        return Err(QuantizationError::EmptyTensor);
    }
    // ...
}
```

**ä¾‹å¤–**: `unsafe`ãƒ–ãƒ­ãƒƒã‚¯ã®ä¸å¤‰æ¡ä»¶é•å â†’ ãƒ‘ãƒ‹ãƒƒã‚¯è¨±å®¹ (ãƒã‚°ãªã®ã§)ã€‚

```rust
unsafe fn quantize_simd(ptr: *const f32, len: usize) -> Vec<i8> {
    assert!(!ptr.is_null(), "Null pointer");
    // SAFETY: caller ensures ptr is valid for len elements
    // ...
}
```

#### 3.C.2 æ§‹é€ åŒ–ãƒ­ã‚°å®Œå…¨ç‰ˆ

**tracing** [^10]: Rustã®æ¨™æº–çš„ãƒ­ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚

##### ã‚¹ãƒ‘ãƒ³è¨­è¨ˆ

**ã‚¹ãƒ‘ãƒ³** (Span): éšå±¤çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‚

```rust
use tracing::{info, warn, instrument, span, Level};

#[instrument]
fn quantize_model(model_name: &str, bits: u8) -> Result<()> {
    info!("Starting quantization");

    let _span = span!(Level::INFO, "load_weights").entered();
    let weights = load_weights(model_name)?;
    drop(_span);

    let _span = span!(Level::INFO, "quantize", bits = bits).entered();
    let quantized = quantize(&weights, bits)?;
    drop(_span);

    info!(num_params = quantized.len(), "Quantization complete");
    Ok(())
}
```

å‡ºåŠ›:
```
INFO quantize_model{model_name="llama-7b" bits=4}: Starting quantization
INFO quantize_model{model_name="llama-7b" bits=4}:load_weights: Loaded 7B parameters
INFO quantize_model{model_name="llama-7b" bits=4}:quantize{bits=4}: Quantizing...
INFO quantize_model{model_name="llama-7b" bits=4}: num_params=7000000000 Quantization complete
```

##### JSONå‡ºåŠ› (æœ¬ç•ªç’°å¢ƒ)

```rust
use tracing_subscriber::{fmt, prelude::*, EnvFilter};

fn init_logging() {
    tracing_subscriber::registry()
        .with(fmt::layer().json())
        .with(EnvFilter::from_default_env())
        .init();
}
```

JSONå‡ºåŠ›:
```json
{
  "timestamp": "2025-02-13T10:30:45.123Z",
  "level": "INFO",
  "fields": {
    "message": "Quantization complete",
    "num_params": 7000000000
  },
  "target": "my_quantizer",
  "span": {
    "model_name": "llama-7b",
    "bits": 4,
    "name": "quantize_model"
  }
}
```

##### ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

ç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡:
```bash
RUST_LOG=info,my_quantizer::quantize=debug cargo run
```

- `info`: å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«INFOä»¥ä¸Š
- `my_quantizer::quantize=debug`: ç‰¹å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿DEBUG

#### 3.C.3 ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å®Œå…¨ç‰ˆ

**Prometheusçµ±åˆ** [^11]: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å…¬é–‹ã—PrometheusãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã€‚

```rust
use prometheus::{Counter, Histogram, IntGauge, Registry, TextEncoder};
use lazy_static::lazy_static;

lazy_static! {
    static ref REGISTRY: Registry = Registry::new();

    static ref QUANTIZATION_REQUESTS: Counter = Counter::new(
        "quantization_requests_total",
        "Total quantization requests"
    ).unwrap();

    static ref QUANTIZATION_DURATION: Histogram = Histogram::with_opts(
        prometheus::HistogramOpts::new(
            "quantization_duration_seconds",
            "Quantization duration"
        ).buckets(vec![0.001, 0.01, 0.1, 1.0, 10.0])
    ).unwrap();

    static ref ACTIVE_QUANTIZATIONS: IntGauge = IntGauge::new(
        "active_quantizations",
        "Currently active quantizations"
    ).unwrap();
}

fn init_metrics() {
    REGISTRY.register(Box::new(QUANTIZATION_REQUESTS.clone())).unwrap();
    REGISTRY.register(Box::new(QUANTIZATION_DURATION.clone())).unwrap();
    REGISTRY.register(Box::new(ACTIVE_QUANTIZATIONS.clone())).unwrap();
}

#[instrument]
fn quantize_with_metrics(weights: &[f32], bits: u8) -> Result<Vec<i8>> {
    QUANTIZATION_REQUESTS.inc();
    ACTIVE_QUANTIZATIONS.inc();

    let timer = QUANTIZATION_DURATION.start_timer();
    let result = quantize_symmetric_int8(weights);
    timer.observe_duration();

    ACTIVE_QUANTIZATIONS.dec();

    Ok(result.0)
}

// HTTP endpoint for Prometheus scraping
fn metrics_handler() -> String {
    let encoder = TextEncoder::new();
    let metric_families = REGISTRY.gather();
    encoder.encode_to_string(&metric_families).unwrap()
}
```

Prometheuså‡ºåŠ›:
```
# HELP quantization_requests_total Total quantization requests
# TYPE quantization_requests_total counter
quantization_requests_total 1523

# HELP quantization_duration_seconds Quantization duration
# TYPE quantization_duration_seconds histogram
quantization_duration_seconds_bucket{le="0.001"} 0
quantization_duration_seconds_bucket{le="0.01"} 234
quantization_duration_seconds_bucket{le="0.1"} 1200
quantization_duration_seconds_bucket{le="1.0"} 1523
quantization_duration_seconds_sum 45.67
quantization_duration_seconds_count 1523
```

#### 3.C.4 ãƒ†ã‚¹ãƒˆæˆ¦ç•¥å®Œå…¨ç‰ˆ

##### Property-Based Testing (proptest)

**é€šå¸¸ã®å˜ä½“ãƒ†ã‚¹ãƒˆ**: å›ºå®šå…¥åŠ› â†’ å›ºå®šå‡ºåŠ›
**Property-Based Testing**: ãƒ©ãƒ³ãƒ€ãƒ å…¥åŠ› â†’ æ€§è³ªã‚’æ¤œè¨¼

```rust
use proptest::prelude::*;

proptest! {
    #[test]
    fn test_quantization_reversibility(
        weights in prop::collection::vec((-10.0f32..10.0f32), 1..1000)
    ) {
        let (quantized, scale) = quantize_symmetric_int8(&weights);
        let dequantized = dequantize_symmetric(&quantized, scale);

        // Property: quantization error bounded by scale/2
        for (orig, deq) in weights.iter().zip(&dequantized) {
            prop_assert!((orig - deq).abs() <= scale / 2.0 + 1e-6);
        }
    }

    #[test]
    fn test_quantization_range(
        weights in prop::collection::vec((-100.0f32..100.0f32), 1..1000)
    ) {
        let (quantized, _scale) = quantize_symmetric_int8(&weights);

        // Property: all quantized values in INT8 range
        for q in &quantized {
            prop_assert!(*q >= -128 && *q <= 127);
        }
    }
}
```

proptestå®Ÿè¡Œ: 100-10000å€‹ã®ãƒ©ãƒ³ãƒ€ãƒ å…¥åŠ›ã‚’ç”Ÿæˆã—ã€æ€§è³ªé•åã‚’æ¢ã™ã€‚

##### Fuzz Testing (cargo-fuzz)

**Fuzzing**: ç•°å¸¸å…¥åŠ›ã§ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚’æ¢ã™ã€‚

```rust
// fuzz/fuzz_targets/quantize.rs
#![no_main]
use libfuzzer_sys::fuzz_target;

fuzz_target!(|data: &[u8]| {
    if data.len() % 4 != 0 {
        return;
    }

    let weights: Vec<f32> = data.chunks_exact(4)
        .map(|b| f32::from_le_bytes([b[0], b[1], b[2], b[3]]))
        .collect();

    // Should never panic
    let _ = quantize_symmetric_int8(&weights);
});
```

å®Ÿè¡Œ:
```bash
cargo fuzz run quantize -- -max_total_time=60
```

##### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (Criterion)

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};

fn bench_quantization(c: &mut Criterion) {
    let mut group = c.benchmark_group("quantization");

    for size in [1000, 10000, 100000].iter() {
        let weights: Vec<f32> = (0..*size).map(|i| (i as f32) * 0.01).collect();

        group.bench_with_input(BenchmarkId::new("INT8", size), &weights, |b, w| {
            b.iter(|| quantize_symmetric_int8(black_box(w)));
        });
    }

    group.finish();
}

criterion_group!(benches, bench_quantization);
criterion_main!(benches);
```

å‡ºåŠ›:
```
quantization/INT8/1000   time:   [12.345 Âµs 12.456 Âµs 12.567 Âµs]
quantization/INT8/10000  time:   [123.45 Âµs 124.56 Âµs 125.67 Âµs]
quantization/INT8/100000 time:   [1.2345 ms 1.2456 ms 1.2567 ms]
```

---

:::message
**é€²æ—**: å…¨ä½“ã®50%å®Œäº† â€” Part D (Elixiræ¨è«–åˆ†æ•£) ã¸
:::

### Part D: ğŸ”® Elixiræ¨è«–åˆ†æ•£ï¼ˆæ·±æ˜ã‚Šï¼‰ (~600è¡Œ)

Elixirã¯**ä¸¦è¡Œæ€§ã¨è€éšœå®³æ€§**ã§Rustã‚’è£œå®Œã™ã‚‹ã€‚æ¨è«–APIã‚µãƒ¼ãƒãƒ¼ã«æœ€é©ã€‚

#### 3.D.1 ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°æˆ¦ç•¥

##### ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³ (Round-Robin)

æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã€‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é †ç•ªã«ãƒ¯ãƒ¼ã‚«ãƒ¼ã«å‰²ã‚Šå½“ã¦ã€‚

```elixir
defmodule LoadBalancer.RoundRobin do
  use GenServer

  # State: {worker_pids, current_index}
  def init(worker_pids) do
    {:ok, {worker_pids, 0}}
  end

  def handle_call(:get_worker, _from, {workers, idx}) do
    worker = Enum.at(workers, idx)
    next_idx = rem(idx + 1, length(workers))
    {:reply, worker, {workers, next_idx}}
  end
end

# Usage
{:ok, lb} = LoadBalancer.RoundRobin.start_link([worker1, worker2, worker3])
worker = GenServer.call(lb, :get_worker)
```

**åˆ©ç‚¹**: O(1)æ™‚é–“, å®Ÿè£…ç°¡å˜
**æ¬ ç‚¹**: ãƒ¯ãƒ¼ã‚«ãƒ¼ã®è² è·å·®ã‚’ç„¡è¦–

##### æœ€å°æ¥ç¶šæ•° (Least Connections)

ç¾åœ¨ã®æ¥ç¶šæ•°ãŒæœ€ã‚‚å°‘ãªã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é¸æŠã€‚

```elixir
defmodule LoadBalancer.LeastConn do
  use GenServer

  # State: %{worker_pid => connection_count}
  def init(worker_pids) do
    state = Map.new(worker_pids, fn pid -> {pid, 0} end)
    {:ok, state}
  end

  def handle_call(:get_worker, _from, state) do
    {worker, _count} = Enum.min_by(state, fn {_pid, count} -> count end)
    new_state = Map.update!(state, worker, &(&1 + 1))
    {:reply, worker, new_state}
  end

  def handle_cast({:release_worker, worker}, state) do
    new_state = Map.update!(state, worker, &max(&1 - 1, 0))
    {:noreply, new_state}
  end
end
```

**åˆ©ç‚¹**: è² è·ã‚’å‡ç­‰åŒ–
**æ¬ ç‚¹**: å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†æ™‚é–“å·®ã‚’ç„¡è¦–

##### é‡ã¿ä»˜ããƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚° (Weighted Round-Robin)

ãƒ¯ãƒ¼ã‚«ãƒ¼ã”ã¨ã«é‡ã¿ä»˜ã‘ (GPUæ€§èƒ½å·®ã‚’è€ƒæ…®)ã€‚

```elixir
defmodule LoadBalancer.Weighted do
  use GenServer

  # State: [{worker, weight}, ...]
  def init(worker_weights) do
    # Expand workers by weight: [{worker1, 3}] => [w1, w1, w1]
    expanded = Enum.flat_map(worker_weights, fn {w, weight} ->
      List.duplicate(w, weight)
    end)
    {:ok, {expanded, 0}}
  end

  def handle_call(:get_worker, _from, {workers, idx}) do
    worker = Enum.at(workers, idx)
    next_idx = rem(idx + 1, length(workers))
    {:reply, worker, {workers, next_idx}}
  end
end

# Example: GPU1(A100) weight=3, GPU2(V100) weight=1
LoadBalancer.Weighted.start_link([{gpu1_worker, 3}, {gpu2_worker, 1}])
# Sequence: gpu1, gpu1, gpu1, gpu2, gpu1, ...
```

##### é©å¿œå‹ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚° (Adaptive)

ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã—ã€å‹•çš„ã«é‡ã¿ã‚’èª¿æ•´ã€‚

```elixir
defmodule LoadBalancer.Adaptive do
  use GenServer

  # State: %{worker => %{latency_ewma: float, requests: int}}
  def init(workers) do
    state = Map.new(workers, fn w -> {w, %{latency_ewma: 0.0, requests: 0}} end)
    {:ok, state}
  end

  def handle_call(:get_worker, _from, state) do
    # Select worker with lowest EWMA latency
    {worker, _stats} = Enum.min_by(state, fn {_w, %{latency_ewma: lat}} -> lat end)
    {:reply, worker, state}
  end

  def handle_cast({:record_latency, worker, latency_ms}, state) do
    new_state = Map.update!(state, worker, fn stats ->
      # Exponential moving average: Î±=0.1
      new_ewma = 0.9 * stats.latency_ewma + 0.1 * latency_ms
      %{stats | latency_ewma: new_ewma, requests: stats.requests + 1}
    end)
    {:noreply, new_state}
  end
end
```

**EWMA (æŒ‡æ•°ç§»å‹•å¹³å‡)**:
$$\text{EWMA}_t = \alpha \cdot L_t + (1-\alpha) \cdot \text{EWMA}_{t-1}$$

where $L_t$ = æœ€æ–°ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·, $\alpha=0.1$ (å¹³æ»‘åŒ–ä¿‚æ•°)ã€‚

#### 3.D.2 Auto-Scaling

éœ€è¦ã«å¿œã˜ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’å‹•çš„ã«è¿½åŠ /å‰Šé™¤ã€‚

##### ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹Auto-Scaling

CPU/ãƒ¡ãƒ¢ãƒª/ã‚­ãƒ¥ãƒ¼é•·ã‚’ç›£è¦–ã—ã€é–¾å€¤è¶…éã§ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã€‚

```elixir
defmodule AutoScaler do
  use GenServer
  require Logger

  @scale_out_threshold 0.8  # CPU 80%
  @scale_in_threshold 0.3   # CPU 30%
  @check_interval 10_000    # 10ç§’

  def init(opts) do
    schedule_check()
    {:ok, %{
      workers: opts[:initial_workers],
      min_workers: opts[:min_workers] || 1,
      max_workers: opts[:max_workers] || 10
    }}
  end

  defp schedule_check do
    Process.send_after(self(), :check_metrics, @check_interval)
  end

  def handle_info(:check_metrics, state) do
    cpu_usage = :erlang.statistics(:scheduler_utilization)
      |> Enum.map(fn {_, usage} -> usage end)
      |> Enum.sum()
      |> Kernel./(length(:erlang.system_info(:schedulers)))

    new_state = cond do
      cpu_usage > @scale_out_threshold and length(state.workers) < state.max_workers ->
        Logger.info("CPU #{cpu_usage}, scaling out")
        scale_out(state)

      cpu_usage < @scale_in_threshold and length(state.workers) > state.min_workers ->
        Logger.info("CPU #{cpu_usage}, scaling in")
        scale_in(state)

      true ->
        state
    end

    schedule_check()
    {:noreply, new_state}
  end

  defp scale_out(state) do
    new_worker = start_worker()
    %{state | workers: [new_worker | state.workers]}
  end

  defp scale_in(state) do
    [worker_to_stop | remaining] = state.workers
    stop_worker(worker_to_stop)
    %{state | workers: remaining}
  end
end
```

##### Kubernetesçµ±åˆ

Elixirã‚¢ãƒ—ãƒªã‚’`libcluster`ã§Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã«çµ±åˆã€‚

```elixir
# config/config.exs
config :libcluster,
  topologies: [
    k8s: [
      strategy: Cluster.Strategy.Kubernetes,
      config: [
        mode: :dns,
        kubernetes_node_basename: "inference-api",
        kubernetes_selector: "app=inference",
        polling_interval: 10_000
      ]
    ]
  ]
```

Horizontal Pod Autoscaler (HPA):
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: inference-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: inference-api
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

#### 3.D.3 è€éšœå®³æ€§ï¼ˆæ·±æ˜ã‚Šï¼‰

Elixirã®"Let it crash"å“²å­¦ + Supervisor treeã§è‡ªå‹•å¾©æ—§ã€‚

##### Circuit Breakerå®Ÿè£…

ãƒªãƒ¢ãƒ¼ãƒˆã‚µãƒ¼ãƒ“ã‚¹å‘¼ã³å‡ºã—ã®å¤±æ•—ã‚’æ¤œçŸ¥ã—ã€ä¸€æ™‚çš„ã«é®æ–­ã€‚

```elixir
defmodule CircuitBreaker do
  use GenServer

  @failure_threshold 5
  @timeout_ms 30_000  # 30ç§’å¾Œã«half-openã¸

  defmodule State do
    defstruct [
      :status,          # :closed | :open | :half_open
      :failure_count,
      :last_failure_time,
      :success_count
    ]
  end

  def init(_) do
    {:ok, %State{
      status: :closed,
      failure_count: 0,
      last_failure_time: nil,
      success_count: 0
    }}
  end

  def call(breaker, fun) do
    GenServer.call(breaker, {:call, fun})
  end

  def handle_call({:call, fun}, _from, state) do
    case state.status do
      :open ->
        if time_elapsed?(state.last_failure_time, @timeout_ms) do
          # Transition to half-open
          attempt_call(fun, %{state | status: :half_open, success_count: 0})
        else
          {:reply, {:error, :circuit_open}, state}
        end

      :half_open ->
        attempt_call(fun, state)

      :closed ->
        attempt_call(fun, state)
    end
  end

  defp attempt_call(fun, state) do
    case fun.() do
      {:ok, result} ->
        new_state = handle_success(state)
        {:reply, {:ok, result}, new_state}

      {:error, reason} ->
        new_state = handle_failure(state)
        {:reply, {:error, reason}, new_state}
    end
  end

  defp handle_success(state) do
    case state.status do
      :half_open ->
        # 3å›é€£ç¶šæˆåŠŸã§closedã¸
        if state.success_count + 1 >= 3 do
          %{state | status: :closed, failure_count: 0, success_count: 0}
        else
          %{state | success_count: state.success_count + 1}
        end

      :closed ->
        %{state | failure_count: 0}

      :open ->
        state
    end
  end

  defp handle_failure(state) do
    new_failure_count = state.failure_count + 1

    if new_failure_count >= @failure_threshold do
      %{state |
        status: :open,
        failure_count: new_failure_count,
        last_failure_time: System.monotonic_time(:millisecond)
      }
    else
      %{state | failure_count: new_failure_count}
    end
  end

  defp time_elapsed?(last_time, timeout_ms) do
    System.monotonic_time(:millisecond) - last_time > timeout_ms
  end
end
```

**çŠ¶æ…‹é·ç§»**:
```
Closed --[5 failures]--> Open --[30s timeout]--> Half-Open --[3 successes]--> Closed
                                                      |
                                                  [1 failure]
                                                      |
                                                      v
                                                    Open
```

##### Bulkheadåˆ†é›¢

ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’åˆ†é›¢ã—ã€1ã‚µãƒ¼ãƒ“ã‚¹ã®éšœå®³ãŒå…¨ä½“ã«æ³¢åŠã—ãªã„ã€‚

```elixir
defmodule Bulkhead do
  use Supervisor

  def start_link(opts) do
    Supervisor.start_link(__MODULE__, opts, name: __MODULE__)
  end

  def init(opts) do
    # Pool A: é«˜å„ªå…ˆåº¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ (50 workers)
    pool_a = :poolboy.child_spec(:pool_a, [
      {:name, {:local, :pool_a}},
      {:worker_module, InferenceWorker},
      {:size, 50},
      {:max_overflow, 10}
    ])

    # Pool B: é€šå¸¸ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ (20 workers)
    pool_b = :poolboy.child_spec(:pool_b, [
      {:name, {:local, :pool_b}},
      {:worker_module, InferenceWorker},
      {:size, 20},
      {:max_overflow, 5}
    ])

    children = [pool_a, pool_b]
    Supervisor.init(children, strategy: :one_for_one)
  end
end

# Usage
def high_priority_request(input) do
  :poolboy.transaction(:pool_a, fn worker ->
    InferenceWorker.run(worker, input)
  end)
end

def normal_request(input) do
  :poolboy.transaction(:pool_b, fn worker ->
    InferenceWorker.run(worker, input)
  end)
end
```

Pool AãŒæ¯æ¸‡ã—ã¦ã‚‚Pool Bã¯å½±éŸ¿ã‚’å—ã‘ãªã„ã€‚

##### Timeoutæˆ¦ç•¥

å„æ“ä½œã«æ˜ç¤ºçš„ãªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®šã€‚

```elixir
defmodule InferenceAPI do
  @inference_timeout 5_000  # 5ç§’
  @load_model_timeout 30_000  # 30ç§’

  def inference(model, input) do
    Task.async(fn ->
      # å®Ÿéš›ã®æ¨è«–å‡¦ç†
      run_inference(model, input)
    end)
    |> Task.await(@inference_timeout)
  rescue
    e in [Task.TimeoutError] ->
      {:error, :timeout}
  end

  def load_model(path) do
    Task.async(fn ->
      # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
      Model.load(path)
    end)
    |> Task.await(@load_model_timeout)
  end
end
```

##### Retry Policy

ä¸€æ™‚çš„ãªã‚¨ãƒ©ãƒ¼ã¯æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤ã€‚

```elixir
defmodule RetryPolicy do
  def retry(fun, opts \\ []) do
    max_retries = Keyword.get(opts, :max_retries, 3)
    base_delay = Keyword.get(opts, :base_delay_ms, 100)

    do_retry(fun, 0, max_retries, base_delay)
  end

  defp do_retry(fun, attempt, max_retries, base_delay) do
    case fun.() do
      {:ok, result} ->
        {:ok, result}

      {:error, reason} when attempt < max_retries ->
        # Exponential backoff with jitter
        delay = base_delay * :math.pow(2, attempt) + :rand.uniform(100)
        Process.sleep(trunc(delay))
        do_retry(fun, attempt + 1, max_retries, base_delay)

      {:error, reason} ->
        {:error, {:max_retries_exceeded, reason}}
    end
  end
end

# Usage
RetryPolicy.retry(fn ->
  HTTPoison.post(url, body)
end, max_retries: 3, base_delay_ms: 200)
```

ãƒãƒƒã‚¯ã‚ªãƒ•è¨ˆç®—:
$$\text{delay} = \text{base} \times 2^{\text{attempt}} + \text{jitter}$$

ä¾‹:
- Attempt 0: $200 \times 2^0 + [0,100] = 200\text{-}300$ ms
- Attempt 1: $200 \times 2^1 + [0,100] = 400\text{-}500$ ms
- Attempt 2: $200 \times 2^2 + [0,100] = 800\text{-}900$ ms

#### 3.D.4 ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€é©åŒ–/ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€å¤§åŒ–

##### ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–

è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒƒãƒã«ã¾ã¨ã‚ã¦GPUåˆ©ç”¨ç‡å‘ä¸Šã€‚

```elixir
defmodule BatchProcessor do
  use GenServer

  @batch_size 32
  @batch_timeout 50  # 50ms

  def init(_) do
    {:ok, %{queue: [], timer: nil}}
  end

  def handle_cast({:add_request, request, from}, state) do
    new_queue = [{request, from} | state.queue]

    cond do
      length(new_queue) >= @batch_size ->
        # Batch full: process immediately
        process_batch(new_queue)
        {:noreply, %{queue: [], timer: nil}}

      state.timer == nil ->
        # Start timeout timer
        timer = Process.send_after(self(), :timeout, @batch_timeout)
        {:noreply, %{state | queue: new_queue, timer: timer}}

      true ->
        {:noreply, %{state | queue: new_queue}}
    end
  end

  def handle_info(:timeout, state) do
    process_batch(state.queue)
    {:noreply, %{queue: [], timer: nil}}
  end

  defp process_batch(queue) do
    requests = Enum.map(queue, fn {req, _from} -> req end)
    results = InferenceEngine.batch_infer(requests)  # GPU batch

    Enum.zip(queue, results)
    |> Enum.each(fn {{_req, from}, result} ->
      GenServer.reply(from, result)
    end)
  end
end
```

**åŠ¹æœ**:
- ãƒãƒƒãƒã‚µã‚¤ã‚º32: GPUä½¿ç”¨ç‡ 15% â†’ 85%
- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: 50 req/s â†’ 800 req/s (16å€)
- P99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: +50ms (ãƒãƒƒãƒå¾…ã¡æ™‚é–“)

##### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åŒ–

å‰å‡¦ç†/æ¨è«–/å¾Œå‡¦ç†ã‚’ä¸¦åˆ—åŒ–ã€‚

```elixir
defmodule Pipeline do
  def process(input) do
    input
    |> Task.async(fn i -> preprocess(i) end)
    |> Task.await()
    |> Task.async(fn i -> inference(i) end)
    |> Task.await()
    |> Task.async(fn i -> postprocess(i) end)
    |> Task.await()
  end

  # Parallel pipeline for multiple inputs
  def process_parallel(inputs) do
    inputs
    |> Flow.from_enumerable()
    |> Flow.map(&preprocess/1)
    |> Flow.partition()
    |> Flow.map(&inference/1)
    |> Flow.map(&postprocess/1)
    |> Enum.to_list()
  end
end
```

**Flow** (GenStage): ã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸¦åˆ—å‡¦ç†ã€‚ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼è‡ªå‹•åˆ¶å¾¡ã€‚

##### ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

é »ç¹ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€‚

```elixir
defmodule InferenceCache do
  use GenServer

  @cache_ttl 300_000  # 5åˆ†

  def init(_) do
    {:ok, %{cache: %{}, ttl_timers: %{}}}
  end

  def handle_call({:get, key}, _from, state) do
    case Map.fetch(state.cache, key) do
      {:ok, value} ->
        {:reply, {:ok, value}, state}
      :error ->
        {:reply, :miss, state}
    end
  end

  def handle_cast({:put, key, value}, state) do
    # Set TTL timer
    timer = Process.send_after(self(), {:expire, key}, @cache_ttl)

    new_cache = Map.put(state.cache, key, value)
    new_timers = Map.put(state.ttl_timers, key, timer)

    {:noreply, %{state | cache: new_cache, ttl_timers: new_timers}}
  end

  def handle_info({:expire, key}, state) do
    new_cache = Map.delete(state.cache, key)
    new_timers = Map.delete(state.ttl_timers, key)
    {:noreply, %{state | cache: new_cache, ttl_timers: new_timers}}
  end
end

# LRU cache with :ets
defmodule LRUCache do
  def init(max_size) do
    :ets.new(:lru_cache, [:set, :public, :named_table])
    :ets.insert(:lru_cache, {:__config__, %{max_size: max_size, current_size: 0}})
  end

  def get(key) do
    case :ets.lookup(:lru_cache, key) do
      [{^key, value, _timestamp}] ->
        # Update timestamp (LRU)
        :ets.insert(:lru_cache, {key, value, System.monotonic_time()})
        {:ok, value}
      [] ->
        :miss
    end
  end

  def put(key, value) do
    timestamp = System.monotonic_time()

    # Evict if full
    [{_, %{max_size: max, current_size: size}}] = :ets.lookup(:lru_cache, :__config__)
    if size >= max do
      evict_lru()
    end

    :ets.insert(:lru_cache, {key, value, timestamp})
  end

  defp evict_lru do
    # Find oldest entry
    :ets.select(:lru_cache, [
      {{:"$1", :"$2", :"$3"}, [{:"/=", :"$1", :__config__}], [{{:"$1", :"$3"}}]}
    ])
    |> Enum.min_by(fn {_key, timestamp} -> timestamp end)
    |> elem(0)
    |> then(&:ets.delete(:lru_cache, &1))
  end
end
```

##### ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡

GenStageã§ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼/ã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ã®ãƒ¬ãƒ¼ãƒˆèª¿æ•´ã€‚

```elixir
defmodule Producer do
  use GenStage

  def init(requests) do
    {:producer, requests}
  end

  def handle_demand(demand, requests) when demand > 0 do
    {to_send, remaining} = Enum.split(requests, demand)
    {:noreply, to_send, remaining}
  end
end

defmodule Consumer do
  use GenStage

  def init(_) do
    {:consumer, :ok}
  end

  def handle_events(events, _from, state) do
    # Process events (inference)
    Enum.each(events, &InferenceEngine.infer/1)
    {:noreply, [], state}
  end
end

# Link producer -> consumer
{:ok, producer} = Producer.start_link(requests)
{:ok, consumer} = Consumer.start_link()
GenStage.sync_subscribe(consumer, to: producer, max_demand: 10, min_demand: 5)
```

ConsumerãŒå‡¦ç†èƒ½åŠ›ã‚’è¶…ãˆã‚‹ã¨ã€ProducerãŒè‡ªå‹•çš„ã«ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€‚

#### 3.D.5 SLA/SLOè¨­è¨ˆ

**SLA (Service Level Agreement)**: é¡§å®¢ã¨ã®å¥‘ç´„
**SLO (Service Level Objective)**: å†…éƒ¨ç›®æ¨™ (SLAé”æˆã®ãŸã‚ã®ä½™è£•)
**SLI (Service Level Indicator)**: æ¸¬å®šå¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹

##### æ¨è«–APIã®SLOè¨­è¨ˆ

```elixir
defmodule SLO do
  @doc """
  99.9% availability (monthly downtime < 43.8 min)
  """
  @availability_target 0.999

  @doc """
  P50 latency < 100ms
  P95 latency < 300ms
  P99 latency < 500ms
  """
  @latency_p50_ms 100
  @latency_p95_ms 300
  @latency_p99_ms 500

  @doc """
  Error rate < 0.1%
  """
  @error_rate_target 0.001

  @doc """
  Throughput >= 1000 req/s
  """
  @throughput_target 1000

  def check_slo(metrics) do
    [
      check_availability(metrics.uptime_ratio),
      check_latency(metrics.latency_percentiles),
      check_error_rate(metrics.error_rate),
      check_throughput(metrics.throughput)
    ]
    |> Enum.all?()
  end

  defp check_availability(uptime_ratio) do
    uptime_ratio >= @availability_target
  end

  defp check_latency(percentiles) do
    percentiles.p50 <= @latency_p50_ms and
    percentiles.p95 <= @latency_p95_ms and
    percentiles.p99 <= @latency_p99_ms
  end

  defp check_error_rate(error_rate) do
    error_rate <= @error_rate_target
  end

  defp check_throughput(throughput) do
    throughput >= @throughput_target
  end
end
```

##### SLIè¨ˆæ¸¬å®Ÿè£…

```elixir
defmodule SLICollector do
  use GenServer

  def init(_) do
    schedule_report()
    {:ok, %{
      total_requests: 0,
      successful_requests: 0,
      latencies: [],
      start_time: System.monotonic_time(:second)
    }}
  end

  def handle_cast({:record, latency_ms, success?}, state) do
    new_state = %{state |
      total_requests: state.total_requests + 1,
      successful_requests: state.successful_requests + if(success?, do: 1, else: 0),
      latencies: [latency_ms | state.latencies]
    }
    {:noreply, new_state}
  end

  def handle_info(:report, state) do
    report_sli(state)
    schedule_report()
    {:noreply, %{state | latencies: []}}  # Reset
  end

  defp schedule_report do
    Process.send_after(self(), :report, 60_000)  # Every 1 min
  end

  defp report_sli(state) do
    uptime_seconds = System.monotonic_time(:second) - state.start_time
    error_rate = 1.0 - state.successful_requests / max(state.total_requests, 1)

    sorted_latencies = Enum.sort(state.latencies)
    p50 = percentile(sorted_latencies, 0.50)
    p95 = percentile(sorted_latencies, 0.95)
    p99 = percentile(sorted_latencies, 0.99)

    throughput = state.total_requests / 60.0  # req/s

    Logger.info("""
    SLI Report:
      Uptime: #{uptime_seconds}s
      Error rate: #{Float.round(error_rate * 100, 2)}%
      Latency P50/P95/P99: #{p50}/#{p95}/#{p99} ms
      Throughput: #{Float.round(throughput, 1)} req/s
    """)
  end

  defp percentile([], _p), do: 0
  defp percentile(sorted_list, p) do
    index = trunc(length(sorted_list) * p)
    Enum.at(sorted_list, index)
  end
end
```

##### ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆ

Prometheusã‚¢ãƒ©ãƒ¼ãƒˆ (Elixirã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¬é–‹):

```yaml
groups:
- name: inference_api
  interval: 30s
  rules:
  - alert: HighErrorRate
    expr: rate(inference_errors_total[5m]) > 0.01
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High error rate detected"

  - alert: HighLatencyP99
    expr: histogram_quantile(0.99, rate(inference_duration_seconds_bucket[5m])) > 0.5
    for: 10m
    labels:
      severity: warning

  - alert: LowThroughput
    expr: rate(inference_requests_total[5m]) < 1000
    for: 10m
    labels:
      severity: info
```

---

### Part E: æ¨è«–ã‚µãƒ¼ãƒãƒ¼æœ€é©åŒ– & ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° (~200è¡Œ)

#### 3.E.1 KV-Cacheæœ€é©åŒ– â€” PagedAttention

vLLM [^6] ã®PagedAttention: OSã®ä»®æƒ³ãƒ¡ãƒ¢ãƒªæ–¹å¼ã‚’KV-Cacheã«é©ç”¨ã€‚

##### PagedAttentionã®ä»•çµ„ã¿

KV-Cacheã‚’å›ºå®šã‚µã‚¤ã‚ºã®**ãƒ–ãƒ­ãƒƒã‚¯**ã«åˆ†å‰²:
- 1ãƒ–ãƒ­ãƒƒã‚¯ = 16ãƒˆãƒ¼ã‚¯ãƒ³åˆ†
- ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯: GPUãƒ¡ãƒ¢ãƒªä¸Šã®å®Ÿä½“
- è«–ç†ãƒ–ãƒ­ãƒƒã‚¯: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã”ã¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°

```
Sequence 1: [Block 0] -> [Block 3] -> [Block 7]
Sequence 2: [Block 1] -> [Block 3] (shared!) -> [Block 9]
```

**Copy-on-Write**: Beam searchã§åˆ†å²æ™‚ã€ãƒ–ãƒ­ãƒƒã‚¯ã‚’å…±æœ‰ â†’ æ›¸ãè¾¼ã¿æ™‚ã®ã¿ã‚³ãƒ”ãƒ¼ã€‚

**Rustå®Ÿè£…ä¾‹** (ç°¡ç•¥ç‰ˆ):

```rust
struct BlockManager {
    physical_blocks: Vec<Block>,
    free_blocks: Vec<usize>,
    block_size: usize,  // 16 tokens
}

struct Block {
    key_cache: Vec<f32>,    // [block_size, d_model]
    value_cache: Vec<f32>,
    ref_count: usize,
}

impl BlockManager {
    fn allocate_block(&mut self) -> Result<usize, OutOfMemory> {
        self.free_blocks.pop().ok_or(OutOfMemory)
    }

    fn free_block(&mut self, block_id: usize) {
        self.physical_blocks[block_id].ref_count -= 1;
        if self.physical_blocks[block_id].ref_count == 0 {
            self.free_blocks.push(block_id);
        }
    }

    fn share_block(&mut self, block_id: usize) {
        self.physical_blocks[block_id].ref_count += 1;
    }

    fn copy_on_write(&mut self, block_id: usize) -> Result<usize, OutOfMemory> {
        if self.physical_blocks[block_id].ref_count == 1 {
            return Ok(block_id);  // No sharing, reuse
        }

        // Copy to new block
        let new_block_id = self.allocate_block()?;
        self.physical_blocks[new_block_id].key_cache =
            self.physical_blocks[block_id].key_cache.clone();
        self.physical_blocks[new_block_id].value_cache =
            self.physical_blocks[block_id].value_cache.clone();

        self.free_block(block_id);  // Release old
        Ok(new_block_id)
    }
}
```

**ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**:
- å¾“æ¥: æœ€å¤§é•·ã§äº‹å‰ç¢ºä¿ (2048ãƒˆãƒ¼ã‚¯ãƒ³) â†’ å¹³å‡400ãƒˆãƒ¼ã‚¯ãƒ³ã§æµªè²»80%
- PagedAttention: ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§å‹•çš„ç¢ºä¿ â†’ æµªè²»<4%

#### 3.E.2 âš¡ Juliaè¨“ç·´æœ€é©åŒ–

##### Mixed Precision Training

FP16/BF16ã§å­¦ç¿’é«˜é€ŸåŒ– + ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€‚

```julia
using Flux, CUDA

# Mixed precision: FP16 forward, FP32 backward
function mixed_precision_train!(model, data, opt; scaler=1024.0)
    for (x, y) in data
        # Cast to FP16
        x_fp16 = Float16.(x)
        y_fp16 = Float16.(y)

        # Forward in FP16
        loss, back = Flux.pullback(model) do m
            Å· = m(x_fp16)
            Flux.mse(Å·, y_fp16)
        end

        # Scale loss to prevent underflow
        scaled_loss = loss * scaler

        # Backward in FP32 (gradient accumulation)
        grads = back(scaler)

        # Unscale gradients
        for g in grads
            g ./= scaler
        end

        # Optimizer step in FP32
        Flux.update!(opt, model, grads)
    end
end
```

**ãªãœMixed Precisionã‹**:
- FP16ç¯„å›²: $[2^{-14}, 2^{15}] = [6e-5, 32768]$
- å‹¾é…: $10^{-6}$å° â†’ underflow â†’ Loss scaling

Loss scaling:
$$L_\text{scaled} = L \times s, \quad s=1024$$

Gradient unscaling:
$$g_\text{unscaled} = \frac{g_\text{scaled}}{s}$$

##### Gradient Checkpointing

ä¸­é–“æ´»æ€§åŒ–ã‚’å†è¨ˆç®—ã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€‚

```julia
using ChainRules

function checkpoint(f, x)
    # Forward: only save input
    y = f(x)

    # Custom backward: recompute forward
    function checkpoint_pullback(È³)
        y_recomputed = f(x)  # Recompute!
        _, back = pullback(f, x)
        return back(È³)
    end

    return y, checkpoint_pullback
end

# Usage in Transformer layer
function transformer_layer_checkpointed(x)
    x1, back1 = checkpoint(attention, x)
    x2, back2 = checkpoint(ffn, x1)
    return x2
end
```

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: 50-70% (ä¸­é–“æ´»æ€§åŒ–ã‚’ä¿å­˜ã—ãªã„)
- è¨ˆç®—æ™‚é–“å¢—åŠ : +30% (forward 2å›å®Ÿè¡Œ)

#### 3.E.3 ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° â€” Perf / Flame Graph

##### Rust: `perf` + Flame Graph

```bash
perf record -g ./target/release/inference_server
perf script | stackcollapse-perf.pl | flamegraph.pl > flamegraph.svg
```

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: `__alloc`â†’äº‹å‰ç¢ºä¿, `memcpy`â†’å‚ç…§æ¸¡ã—, `std::fmt`â†’ç„¡åŠ¹åŒ–

##### Elixir: `:observer` + `:fprof`

```elixir
:observer.start()
:fprof.trace([:start, {:procs, [self()]}]); result = heavy_computation()
:fprof.trace(:stop); :fprof.analyse([:totals, {:sort, :own}])
```

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: `++`â†’`[h|t]`, `Enum.map`â†’`Stream.map`, ETS full scanâ†’ã‚­ãƒ¼æŒ‡å®š

##### Julia: `@profile` + `ProfileView`

```julia
@profile for i in 1:1000; forward_pass(model, input); end
ProfileView.view()
```

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: Typeä¸å®‰å®šâ†’`@code_warntype`, ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³â†’`@allocated`, Globalâ†’`const`

---
