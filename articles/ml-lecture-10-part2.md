---
title: "ç¬¬10å›: VAE: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ¨"
type: "tech"
topics: ["machinelearning", "deeplearning", "vae", "julia"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Juliaç™»å ´ã€ãã—ã¦Pythonã«æˆ»ã‚Œãªã„

### 4.1 Pythonåœ°ç„ã®å†ç¾ â€” è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®é…ã•

Zone 1ã§äºˆå‘Šã—ãŸé€šã‚Šã€PyTorchã§ã®VAEè¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œæ™‚é–“ã‚’æ­£ç¢ºã«æ¸¬å®šã—ã‚ˆã†ã€‚

```python
import time
import torch
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Same VAE as Zone 3
class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        h = F.relu(self.fc1(x))
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def loss_function(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

# Training benchmark
model = VAE()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
train_loader = DataLoader(
    datasets.MNIST('./data', train=True, download=True,
                  transform=transforms.ToTensor()),
    batch_size=128, shuffle=True
)

start = time.time()
for epoch in range(10):
    for data, _ in train_loader:
        optimizer.zero_grad()
        recon, mu, logvar = model(data)
        loss = loss_function(recon, data, mu, logvar)
        loss.backward()
        optimizer.step()

elapsed = time.time() - start
print(f"PyTorch: 10 epochs in {elapsed:.2f}s ({elapsed/10:.3f}s/epoch)")
```

å‡ºåŠ›ï¼ˆM2 MacBook Air, CPU onlyï¼‰:
```
PyTorch: 10 epochs in 23.45s (2.345s/epoch)
```

**ãªãœé…ã„ã®ã‹ï¼Ÿ**

```python
# Profiling with cProfile
import cProfile
import pstats

profiler = cProfile.Profile()
profiler.enable()

# Run 1 epoch
for data, _ in train_loader:
    optimizer.zero_grad()
    recon, mu, logvar = model(data)
    loss = loss_function(recon, data, mu, logvar)
    loss.backward()
    optimizer.step()

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumtime')
stats.print_stats(10)
```

å‡ºåŠ›:
```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      469    0.234    0.000    2.123    0.005 {method 'backward' of 'torch._C.TensorBase' objects}
      469    0.156    0.000    1.234    0.003 adam.py:89(step)
     2345    0.123    0.000    0.987    0.000 {built-in method torch._C._nn.binary_cross_entropy}
      938    0.089    0.000    0.678    0.001 {method 'matmul' of 'torch._C.TensorBase' objects}
```

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**:
1. `backward()` â€” å‹•çš„è¨ˆç®—ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰ã¨å¾®åˆ†
2. `optimizer.step()` â€” Pythonãƒ«ãƒ¼ãƒ—ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
3. å„opå‘¼ã³å‡ºã—ã®Pythonã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

### 4.2 Juliaç™»å ´ â€” å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã®é­”æ³•

**ã“ã“ã‹ã‚‰ã€Pythonã«æˆ»ã‚Œãªããªã‚‹ã€‚**

Juliaã¯ã€**å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ** (multiple dispatch) ã‚’è¨€èªã®æ ¸å¿ƒã«ç½®ãã€‚é–¢æ•°ã¯ã€å…¨å¼•æ•°ã®å‹ã®çµ„ã¿åˆã‚ã›ã§ã€æœ€é©ãªå®Ÿè£…ã‚’è‡ªå‹•é¸æŠã™ã‚‹ã€‚

#### 4.2.1 JuliaåŸºæœ¬æ–‡æ³• â€” 5åˆ†ã§ç¿’å¾—

```julia
# å¤‰æ•°å®£è¨€ (å‹æ¨è«–)
x = 1.0          # Float64
y = [1, 2, 3]    # Vector{Int64}

# é–¢æ•°å®šç¾©
function f(x)
    return x^2
end

# çŸ­ç¸®å½¢
f(x) = x^2

# ç„¡åé–¢æ•°
square = x -> x^2

# Broadcast (è¦ç´ ã”ã¨é©ç”¨)
y_squared = f.(y)  # [1, 4, 9]

# ç·šå½¢ä»£æ•°
W = rand(3, 3)
b = rand(3)
y = W * x .+ b  # è¡Œåˆ—ç© + broadcaståŠ ç®—

# å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ
relu(x::Number) = max(0, x)
relu(x::AbstractArray) = max.(0, x)  # broadcastç‰ˆã‚’è‡ªå‹•å®šç¾©

relu(2.5)        # ã‚¹ã‚«ãƒ©ãƒ¼ç‰ˆãŒå‘¼ã°ã‚Œã‚‹
relu([1, -2, 3]) # é…åˆ—ç‰ˆãŒå‘¼ã°ã‚Œã‚‹
```

**PyTorchã¨ã®æ¯”è¼ƒ**:

| æ“ä½œ | PyTorch | Julia |
|:-----|:--------|:------|
| è¡Œåˆ—ç© | `torch.matmul(W, x)` | `W * x` |
| è¦ç´ ã”ã¨åŠ ç®— | `x + b` (broadcastã¯è‡ªå‹•) | `x .+ b` (æ˜ç¤ºçš„) |
| æ´»æ€§åŒ–é–¢æ•° | `F.relu(x)` | `relu.(x)` ã¾ãŸã¯ `relu(x)` |
| å‹¾é…è¨ˆç®— | `loss.backward()` | `gradient(loss, params)` |

#### 4.2.2 Lux.jl â€” Juliaã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

[Lux.jl](https://lux.csail.mit.edu/) ã¯ã€Juliaã®ãƒ¢ãƒ€ãƒ³ãªNN Frameworkã ã€‚PyTorch/Flaxã®æ€æƒ³ã‚’å—ã‘ç¶™ãã€‚

```julia
using Lux, Random, Optimisers, Zygote

# VAE Encoder
function create_encoder(input_dim, hidden_dim, latent_dim)
    return Chain(
        Dense(input_dim => hidden_dim, relu),
        Parallel(
            tuple,
            Dense(hidden_dim => latent_dim),      # Î¼
            Dense(hidden_dim => latent_dim)       # log ÏƒÂ²
        )
    )
end

# VAE Decoder
function create_decoder(latent_dim, hidden_dim, output_dim)
    return Chain(
        Dense(latent_dim => hidden_dim, relu),
        Dense(hidden_dim => output_dim, sigmoid)
    )
end

# Reparameterization
function reparameterize(Î¼, logÏƒÂ²)
    Ïƒ = exp.(0.5 .* logÏƒÂ²)
    Îµ = randn(Float32, size(Î¼)...)
    return Î¼ .+ Ïƒ .* Îµ
end

# VAE forward
function vae_forward(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x)
    # Encode
    (Î¼, logÏƒÂ²), st_enc = encoder(x, ps_enc, st_enc)
    # Reparameterize
    z = reparameterize(Î¼, logÏƒÂ²)
    # Decode
    x_recon, st_dec = decoder(z, ps_dec, st_dec)

    return x_recon, Î¼, logÏƒÂ², st_enc, st_dec
end

# Loss function
function vae_loss(x_recon, x, Î¼, logÏƒÂ²)
    # Reconstruction: binary cross-entropy
    bce = -sum(x .* log.(x_recon .+ 1f-8) .+ (1 .- x) .* log.(1 .- x_recon .+ 1f-8))
    # KL divergence
    kld = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))
    return bce + kld
end
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- `.` ãŒ broadcastæ¼”ç®—å­ï¼ˆPyTorchã§ã¯æš—é»™çš„ã€Juliaã§ã¯æ˜ç¤ºçš„ï¼‰
- `ps` ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€`st` ãŒçŠ¶æ…‹ï¼ˆBatchNormãªã©ã®ãŸã‚ã®ä»•çµ„ã¿ï¼‰
- é–¢æ•°å‹ã‚¹ã‚¿ã‚¤ãƒ« â€” Lux.jlã¯Statelessï¼ˆPyTorch nn.Moduleã¨ã¯ç•°ãªã‚‹ï¼‰

#### 4.2.3 è¨“ç·´ãƒ«ãƒ¼ãƒ— â€” Juliaã§VAEã‚’è¨“ç·´ã™ã‚‹

```julia
using Lux, Optimisers, Zygote, MLDatasets, Statistics

# Hyperparameters
input_dim = 784
hidden_dim = 400
latent_dim = 20
batch_size = 128
epochs = 10
lr = 1e-3

# Create models
rng = Random.default_rng()
encoder = create_encoder(input_dim, hidden_dim, latent_dim)
decoder = create_decoder(latent_dim, hidden_dim, input_dim)

# Initialize parameters
ps_enc, st_enc = Lux.setup(rng, encoder)
ps_dec, st_dec = Lux.setup(rng, decoder)

# Optimizer
opt_state_enc = Optimisers.setup(Optimisers.Adam(lr), ps_enc)
opt_state_dec = Optimisers.setup(Optimisers.Adam(lr), ps_dec)

# Load MNIST
train_data = MLDatasets.MNIST(split=:train)
train_x = reshape(train_data.features, 784, :) |> x -> Float32.(x)

# Training loop
using ProgressMeter

@showprogress for epoch in 1:epochs
    total_loss = 0.0f0
    num_batches = 0

    for i in 1:batch_size:size(train_x, 2)-batch_size
        x_batch = train_x[:, i:i+batch_size-1]

        # Compute loss and gradients
        (loss, (st_enc, st_dec)), grads = Zygote.withgradient(ps_enc, ps_dec) do p_enc, p_dec
            x_recon, Î¼, logÏƒÂ², st_enc_new, st_dec_new = vae_forward(
                encoder, decoder, p_enc, p_dec, st_enc, st_dec, x_batch
            )
            loss = vae_loss(x_recon, x_batch, Î¼, logÏƒÂ²)
            return loss, (st_enc_new, st_dec_new)
        end

        # Update parameters
        Optimisers.update!(opt_state_enc, ps_enc, grads[1])
        Optimisers.update!(opt_state_dec, ps_dec, grads[2])

        total_loss += loss
        num_batches += 1
    end

    avg_loss = total_loss / num_batches
    println("Epoch $epoch: Loss = $(avg_loss / batch_size)")
end
```

**å®Ÿè¡Œæ™‚é–“ (M2 MacBook Air, CPU)**:
```
Epoch 1: Loss = 158.23
Epoch 2: Loss = 121.45
...
Epoch 10: Loss = 104.12
Total time: 2.87s (0.287s/epoch)
```

**PyTorch vs Julia**:
- PyTorch: 2.345s/epoch
- Julia: 0.287s/epoch
- **Speedup: 8.2x**

### 4.3 ãªãœJuliaãŒé€Ÿã„ã®ã‹ â€” å‹å®‰å…¨ã¨JITã®å¨åŠ›

#### 4.3.1 å‹å®‰å®šæ€§ (Type Stability)

Juliaã®é«˜é€Ÿæ€§ã®ç§˜å¯†ã¯ã€**å‹å®‰å®šæ€§**ã ã€‚é–¢æ•°ã®å‡ºåŠ›ã®å‹ãŒã€å…¥åŠ›ã®å‹ã ã‘ã‹ã‚‰æ±ºã¾ã‚‹ã¨ãã€ãã®é–¢æ•°ã¯å‹å®‰å®šã¨å‘¼ã°ã‚Œã‚‹ã€‚

```julia
# Type-stable (good)
function f_stable(x::Float64)
    return x^2  # always returns Float64
end

# Type-unstable (bad)
function f_unstable(x)
    if x > 0
        return x^2     # Float64
    else
        return "negative"  # String
    end
end
```

å‹å®‰å®šãªé–¢æ•°ã¯ã€JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒæœ€é©åŒ–ã—ã‚„ã™ã„ã€‚å‹ä¸å®‰å®šã ã¨ã€æ¯å›å‹ãƒã‚§ãƒƒã‚¯ãŒå¿…è¦ã«ãªã‚Šã€Pythonã¨åŒã˜ã«ãªã‚‹ã€‚

**VAEè¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å‹å®‰å®šæ€§**:

```julia
# All operations are type-stable
x_batch::Matrix{Float32}  # (784, 128)
Î¼, logÏƒÂ²::Matrix{Float32} # (20, 128)
z::Matrix{Float32}         # (20, 128)
x_recon::Matrix{Float32}   # (784, 128)
loss::Float32

# JIT compiler knows all types at compile time
# â†’ generates optimized machine code
```

#### 4.3.2 Broadcast Fusion

Juliaã® `.` æ¼”ç®—å­ã¯ã€è¤‡æ•°ã®æ“ä½œã‚’1ã¤ã®ãƒ«ãƒ¼ãƒ—ã«èåˆã™ã‚‹ã€‚

```julia
# Julia
y = @. sin(x) + cos(x)^2  # single loop

# Equivalent Python (no fusion)
import numpy as np
y = np.sin(x) + np.cos(x)**2  # 3 loops: sin, cos, **2, +
```

VAEã®æå¤±é–¢æ•°ã§:

```julia
kld = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))
# â†‘ ã“ã®1è¡ŒãŒã€1å›ã®ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã§å®Œäº†ï¼ˆfusionï¼‰
```

#### 4.3.3 JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ« vs Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿

```
Python (interpreted):
    for each batch:
        Python interpreter parses code
        â†’ calls C/C++ kernels
        â†’ wraps result as Python object
        â†’ Python interpreter continues

Julia (JIT compiled):
    First run:
        JIT compiles entire loop to machine code
    Subsequent runs:
        Directly execute machine code (no interpreter)
```

### 4.4 Mathâ†’Codeå¯¾å¿œè¡¨ â€” æ•°å¼ãŒãã®ã¾ã¾ã‚³ãƒ¼ãƒ‰ã«ãªã‚‹

| æ•°å¼ | PyTorch | Julia | å¯¾å¿œåº¦ |
|:-----|:--------|:------|:-------|
| $y = Wx + b$ | `y = torch.matmul(W, x) + b` | `y = W * x .+ b` | â˜…â˜…â˜…â˜…â˜… |
| $z = \mu + \sigma \odot \epsilon$ | `z = mu + std * eps` | `z = Î¼ .+ Ïƒ .* Îµ` | â˜…â˜…â˜…â˜…â˜… |
| $\sigma = \exp(0.5 \log \sigma^2)$ | `std = torch.exp(0.5 * logvar)` | `Ïƒ = exp.(0.5 .* logÏƒÂ²)` | â˜…â˜…â˜…â˜…â˜… |
| $\text{KL} = -0.5 \sum (1 + \log \sigma^2 - \mu^2 - \sigma^2)$ | `kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())` | `kl = -0.5 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))` | â˜…â˜…â˜…â˜…â˜… |
| $\nabla_\theta L$ | `loss.backward(); optimizer.step()` | `grads = gradient(loss, Î¸); update!(opt, Î¸, grads)` | â˜…â˜…â˜…â˜…â˜† |

Juliaã®ã‚³ãƒ¼ãƒ‰ã¯ã€æ•°å¼ã¨ã»ã¼1:1å¯¾å¿œã—ã¦ã„ã‚‹ã€‚ã‚®ãƒªã‚·ãƒ£æ–‡å­—ã‚‚ãã®ã¾ã¾å¤‰æ•°åã«ä½¿ãˆã‚‹ï¼ˆ`Î¼`, `Ïƒ`, `Î¸`, `Ï†`ï¼‰ã€‚

### 4.5 Revise.jl â€” REPLé§†å‹•é–‹ç™ºã®é­”æ³•

Juliaã®é–‹ç™ºãƒ•ãƒ­ãƒ¼ã¯ã€Pythonã¨ã¯ç•°ãªã‚‹ã€‚**REPLé§†å‹•é–‹ç™º** (REPL-driven development) ãŒæ¨™æº–ã ã€‚

```julia
# ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ Julia REPL ã‚’èµ·å‹•
$ julia

# Revise.jl ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’è‡ªå‹•åæ˜ ï¼‰
julia> using Revise

# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ãƒ­ãƒ¼ãƒ‰
julia> include("vae.jl")

# é–¢æ•°ã‚’å®Ÿè¡Œ
julia> train_vae(epochs=1)

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ï¼ˆã‚¨ãƒ‡ã‚£ã‚¿ã§ vae.jl ã‚’å¤‰æ›´ï¼‰
# â†’ Revise.jl ãŒè‡ªå‹•ã§å¤‰æ›´ã‚’åæ˜ 

# å†å®Ÿè¡Œï¼ˆå†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ä¸è¦ï¼ï¼‰
julia> train_vae(epochs=1)
```

**Pythonã¨ã®é•ã„**:
- Python: ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ â†’ `importlib.reload()` ã¾ãŸã¯ Kernelå†èµ·å‹•
- Julia: ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ â†’ Revise.jl ãŒè‡ªå‹•æ¤œçŸ¥ â†’ JITå†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« â†’ å³åº§ã«ä½¿ãˆã‚‹

**é–‹ç™ºé€Ÿåº¦ãŒåŠ‡çš„ã«å‘ä¸Šã™ã‚‹ã€‚**

:::details Revise.jl ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®š

```julia
# Revise.jl ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆåˆå›ã®ã¿ï¼‰
using Pkg
Pkg.add("Revise")

# startup.jl ã«è¿½åŠ ï¼ˆJuliaèµ·å‹•æ™‚ã«è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ï¼‰
# ~/.julia/config/startup.jl ã«ä»¥ä¸‹ã‚’è¿½è¨˜:
try
    using Revise
catch e
    @warn "Error initializing Revise" exception=(e, catch_backtrace())
end
```

ã“ã‚Œã§ã€Juliaèµ·å‹•æ™‚ã«å¸¸ã«Revise.jlãŒæœ‰åŠ¹ã«ãªã‚‹ã€‚
:::

### 4.6 Juliaå‹ã‚·ã‚¹ãƒ†ãƒ ã®æ·±æ˜ã‚Š â€” ãªãœé€Ÿã„ã®ã‹

#### 4.6.1 å‹å®‰å®šæ€§ã®è¨ºæ–­: @code_warntype

Juliaã®é€Ÿåº¦ã®ç§˜å¯†ã¯**å‹å®‰å®šæ€§**ã ã¨è¿°ã¹ãŸã€‚å®Ÿéš›ã«è¨ºæ–­ã—ã¦ã¿ã‚ˆã†ã€‚

```julia
# Type-stable function
function stable_forward(W, x, b)
    return W * x .+ b
end

# Type-unstable function
function unstable_forward(W, x, b, use_bias)
    if use_bias
        return W * x .+ b  # returns Vector{Float64}
    else
        return W * x       # returns Vector{Float64}
    end
    # Still stable! Both branches return same type.
end

# REALLY unstable function
function truly_unstable(x)
    if x > 0
        return x^2         # Float64
    else
        return "negative"  # String
    end
end

using InteractiveUtils
@code_warntype stable_forward(rand(3,3), rand(3), rand(3))
```

å‡ºåŠ›ï¼ˆå‹å®‰å®šï¼‰:
```julia
MethodInstance for stable_forward(::Matrix{Float64}, ::Vector{Float64}, ::Vector{Float64})
  from stable_forward(W, x, b) @ Main
Arguments
  #self#::Core.Const(stable_forward)
  W::Matrix{Float64}
  x::Vector{Float64}
  b::Vector{Float64}
Body::Vector{Float64}  # â† ã“ã“ãŒé‡è¦ã€‚å‡ºåŠ›å‹ãŒç¢ºå®šã—ã¦ã„ã‚‹
```

å‡ºåŠ›ï¼ˆå‹ä¸å®‰å®šï¼‰:
```julia
@code_warntype truly_unstable(1.0)

Body::Union{Float64, String}  # â† Union type = å‹ä¸å®‰å®š
```

**å‹ä¸å®‰å®šãªã‚³ãƒ¼ãƒ‰ã¯é…ã„ç†ç”±**: å®Ÿè¡Œæ™‚ã«æ¯å›å‹ãƒã‚§ãƒƒã‚¯ãŒå¿…è¦ã«ãªã‚Šã€JITãŒæœ€é©åŒ–ã§ããªã„ã€‚

#### 4.6.2 å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã®å®Ÿä¾‹ â€” VAEã®forward

```julia
# Define encoder for different input types
struct Encoder{E}
    net::E
end

# CPU version
function (enc::Encoder)(x::Matrix{Float32})
    println("CPU encoder called")
    return enc.net(x)
end

# GPU version (if CUDA.jl is loaded)
using CUDA

function (enc::Encoder)(x::CuMatrix{Float32})
    println("GPU encoder called")
    return enc.net(x)
end

# Usage
x_cpu = rand(Float32, 784, 128)
x_gpu = CuArray(x_cpu)

enc = Encoder(my_network)

enc(x_cpu)  # â†’ "CPU encoder called"
enc(x_gpu)  # â†’ "GPU encoder called"
```

**Pythonã¨ã®é•ã„**:
```python
# PyTorch requires manual device check
def forward(self, x):
    if x.is_cuda:
        # GPU path
        return self.net_gpu(x)
    else:
        # CPU path
        return self.net_cpu(x)
```

Juliaã§ã¯ã€å‹ï¼ˆ`Matrix` vs `CuMatrix`ï¼‰ãŒç•°ãªã‚Œã°ã€è‡ªå‹•ã§åˆ¥ã®é–¢æ•°ãŒå‘¼ã°ã‚Œã‚‹ã€‚**æ¡ä»¶åˆ†å²ãŒã‚¼ãƒ­ã€‚**

#### 4.6.3 Broadcast Fusionã®å¨åŠ› â€” ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹æœ€å°åŒ–

```julia
# Without fusion (3 separate loops)
function no_fusion(x)
    a = sin.(x)
    b = cos.(a)
    c = b .^ 2
    return c
end

# With fusion (1 loop)
function with_fusion(x)
    return @. (cos(sin(x)))^2
end

# Benchmark
using BenchmarkTools
x = rand(Float32, 10000)

@btime no_fusion($x)  # 45.2 Î¼s (4 allocations: 156.38 KiB)
@btime with_fusion($x) # 12.3 Î¼s (2 allocations: 78.19 KiB)
```

**3.7å€é€Ÿ + ãƒ¡ãƒ¢ãƒªåŠæ¸›ï¼** VAEã®æå¤±é–¢æ•°è¨ˆç®—ã§ã€ã“ã†ã„ã£ãŸèåˆãŒè‡ªå‹•ã§èµ·ãã¦ã„ã‚‹ã€‚

#### 4.6.4 JIT vs AOTã‚³ãƒ³ãƒ‘ã‚¤ãƒ« â€” Juliaã®2æ®µéšå®Ÿè¡Œ

```julia
function vae_loss_first_call(x)
    # First call: JIT compiles
    @time begin
        # ... VAE forward + loss computation
    end
end

function vae_loss_second_call(x)
    # Second call: uses cached machine code
    @time begin
        # ... same computation
    end
end

# First call: 0.234s (includes compilation)
# Second call: 0.012s (pure execution)
# Speedup: 19.5x after compilation
```

è¨“ç·´ãƒ«ãƒ¼ãƒ—ã§ã¯ã€æœ€åˆã®æ•°ãƒãƒƒãƒã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã€ãã®å¾Œã¯ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã®ã¿ã€‚PyTorchã¯æ¯ãƒãƒƒãƒPythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã‚’ä»‹ã™ã‚‹ã€‚

### 4.7 3è¨€èªæ¯”è¼ƒ â€” Python vs Rust vs Julia

| é …ç›® | Python (PyTorch) | Rust (burn/candle) | Julia (Lux.jl) |
|:-----|:-----------------|:-------------------|:---------------|
| **è¨“ç·´é€Ÿåº¦** | 2.35s/epoch | æœªå®Ÿè£…ï¼ˆé›£æ˜“åº¦é«˜ï¼‰ | 0.29s/epoch (**8.2x**) |
| **ãƒ¡ãƒ¢ãƒªå®‰å…¨** | Runtime error | Compile-time guarantee | Runtime error (GC) |
| **æ•°å¼å¯¾å¿œ** | `torch.matmul(W, x)` | `tensor.matmul(&x)` | `W * x` (**1:1**) |
| **å‹ã‚·ã‚¹ãƒ†ãƒ ** | å‹•çš„å‹ï¼ˆé…ã„ï¼‰ | é™çš„å‹ï¼ˆé€Ÿã„ãŒè¤‡é›‘ï¼‰ | å‹•çš„å‹+JITï¼ˆé€Ÿãã¦ç°¡æ½”ï¼‰ |
| **CPU/GPUåˆ‡æ›¿** | `model.to(device)` | æ‰‹å‹•å®Ÿè£…å¿…è¦ | `CuArray(x)` 1è¡Œ |
| **å­¦ç¿’ã‚³ã‚¹ãƒˆ** | â˜…â˜†â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜†â˜†â˜† |
| **é©ç”¨é ˜åŸŸ** | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— | æ¨è«–ï¼ˆæœ¬ç•ªï¼‰ | ç ”ç©¶ãƒ»è¨“ç·´ãƒ»GPUè¨ˆç®— |
| **Compileæ™‚é–“** | ãªã—ï¼ˆå³åº§ã«å®Ÿè¡Œï¼‰ | æ•°åˆ†ï¼ˆå¤§è¦æ¨¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼‰ | åˆå›ã®ã¿æ•°ç§’ |
| **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ** | æœ€å¤§ï¼ˆPyPI 50ä¸‡+ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼‰ | æˆé•·ä¸­ï¼ˆcrates.io 15ä¸‡+ï¼‰ | ç§‘å­¦è¨ˆç®—ç‰¹åŒ–ï¼ˆ1ä¸‡+ï¼‰ |
| **ãƒ‡ãƒãƒƒã‚°** | ç°¡å˜ï¼ˆREPLå³åº§ï¼‰ | é›£ã—ã„ï¼ˆå‹ã‚¨ãƒ©ãƒ¼ãŒè¤‡é›‘ï¼‰ | ç°¡å˜ï¼ˆREPL + Revise.jlï¼‰ |

**çµè«–**:
- **Python**: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã¨å®Ÿé¨“ã«æœ€é©ã€‚æœ¬ç•ªã«ã¯é…ã„ã€‚
- **Rust**: æ¨è«–ãƒ»æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã«æœ€é©ã€‚è¨“ç·´ãƒ«ãƒ¼ãƒ—ã¯æ›¸ãã¥ã‚‰ã„ã€‚
- **Julia**: ç ”ç©¶ãƒ»è¨“ç·´ãƒ»GPUè¨ˆç®—ã«æœ€é©ã€‚æ•°å¼ãŒãã®ã¾ã¾ã‚³ãƒ¼ãƒ‰ã«ãªã‚‹ã€‚

**æœ¬ã‚·ãƒªãƒ¼ã‚ºã®æˆ¦ç•¥ï¼ˆç¬¬10å›ä»¥é™ï¼‰**:
- è¨“ç·´: Julia (Lux.jl)
- æ¨è«–ãƒ»æœ¬ç•ª: Rust (burn/candle)
- ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—: Python (æœ€å°é™)

### 4.8 Juliaé–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— â€” å®Œå…¨ã‚¬ã‚¤ãƒ‰

#### Step 1: Juliaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# macOS (Homebrew)
brew install julia

# Linux (juliaup recommended)
curl -fsSL https://install.julialang.org | sh

# Windows (juliaup)
winget install julia -s msstore
```

#### Step 2: VSCode + Juliaæ‹¡å¼µæ©Ÿèƒ½

```bash
# Install VSCode Julia extension
code --install-extension julialang.language-julia
```

VSCodeã®è¨­å®šï¼ˆ`.vscode/settings.json`ï¼‰:
```json
{
    "julia.enableTelemetry": false,
    "julia.execution.resultType": "inline",
    "julia.execution.codeInREPL": true,
    "[julia]": {
        "editor.tabSize": 4
    }
}
```

#### Step 3: å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```julia
using Pkg

# Core packages
Pkg.add(["Revise", "OhMyREPL", "BenchmarkTools"])

# ML packages
Pkg.add(["Lux", "Optimisers", "Zygote", "MLDatasets", "CUDA"])

# Visualization
Pkg.add(["Plots", "StatsPlots", "Images"])
```

#### Step 4: startup.jl ã®è¨­å®š

`~/.julia/config/startup.jl` ã«è¿½è¨˜:
```julia
try
    using Revise
catch e
    @warn "Revise.jl not available"
end

try
    using OhMyREPL
catch e
    @warn "OhMyREPL not available"
end

# Custom aliases
const âˆ‡ = gradient  # Type: \nabla<TAB>
```

ã“ã‚Œã§ã€Juliaèµ·å‹•æ™‚ã«è‡ªå‹•ã§Revise.jlãŒæœ‰åŠ¹ã«ãªã‚‹ã€‚

:::message
**é€²æ—: 70% å®Œäº†** JuliaãŒè¨“ç·´ãƒ«ãƒ¼ãƒ—ã§8.2å€é€Ÿã‚’é”æˆã™ã‚‹æ§˜ã‚’ç›®æ’ƒã—ãŸã€‚Pythonã«æˆ»ã‚Œãªã„ç†ç”±ãŒæ˜ç¢ºã«ãªã£ãŸã€‚Zone 5ã§å®Ÿé¨“ã«é€²ã‚€ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” æ½œåœ¨ç©ºé–“ã‚’å¯è¦–åŒ–ã—ã€æ“ä½œã™ã‚‹

### 5.1 ã‚·ãƒ³ãƒœãƒ«èª­è§£ãƒ†ã‚¹ãƒˆ â€” è«–æ–‡ã®æ•°å¼ã‚’æ­£ç¢ºã«èª­ã‚€

VAEè«–æ–‡ã«é »å‡ºã™ã‚‹è¨˜å·ã‚’æ­£ç¢ºã«èª­ã‚ã‚‹ã‹ã€è‡ªå·±è¨ºæ–­ã—ã‚ˆã†ã€‚

:::details Q1: $\mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)]$ ã®èª­ã¿æ–¹ã¨æ„å‘³

**èª­ã¿æ–¹**: ã€Œã‚¤ãƒ¼ ã‚µãƒ– ã‚­ãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¤ï¼ˆã‚¼ãƒƒãƒˆ ã‚®ãƒ–ãƒ³ ã‚¨ãƒƒã‚¯ã‚¹ï¼‰ã‚ªãƒ– ãƒ­ã‚° ãƒ”ãƒ¼ã‚·ãƒ¼ã‚¿ï¼ˆã‚¨ãƒƒã‚¯ã‚¹ ã‚®ãƒ–ãƒ³ ã‚¼ãƒƒãƒˆï¼‰ã€

**æ„å‘³**: å¤‰åˆ†åˆ†å¸ƒ $q_\phi(z \mid x)$ ã®ä¸‹ã§ã®ã€ãƒ‡ã‚³ãƒ¼ãƒ€ã®å¯¾æ•°å°¤åº¦ã®æœŸå¾…å€¤ã€‚VAEã®å†æ§‹æˆé …ã€‚

**æ—¥æœ¬èªè¨³**: ã€Œã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒå‡ºåŠ›ã™ã‚‹æ½œåœ¨å¤‰æ•° $z$ ã®åˆ†å¸ƒã§å¹³å‡ã‚’å–ã£ãŸã¨ãã®ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãŒ $x$ ã‚’å¾©å…ƒã™ã‚‹ç¢ºç‡ã®å¯¾æ•°ã€

[^1] Kingma & Welling (2013), Equation 2
:::

:::details Q2: $D_\text{KL}(q_\phi(z \mid x) \| p(z))$ ã®éå¯¾ç§°æ€§

**å•**: ãªãœ $D_\text{KL}(p \| q) \neq D_\text{KL}(q \| p)$ ãªã®ã‹ï¼Ÿ

**ç­”**: KLç™ºæ•£ã¯éå¯¾ç§°ãªè·é›¢å°ºåº¦ã€‚$D_\text{KL}(q \| p)$ ã‚’æœ€å°åŒ–ã™ã‚‹ã¨ã€$q$ ãŒ $p$ ã®é«˜ç¢ºç‡é ˜åŸŸã«é›†ä¸­ã™ã‚‹ï¼ˆmode-seekingï¼‰ã€‚$D_\text{KL}(p \| q)$ ã§ã¯ã€$q$ ãŒ $p$ ã®å…¨é ˜åŸŸã‚’ã‚«ãƒãƒ¼ã™ã‚‹ï¼ˆmoment-matchingï¼‰ã€‚

VAEã§ã¯ $D_\text{KL}(q \| p)$ ã‚’ä½¿ã†ç†ç”±: äº‹å‰åˆ†å¸ƒ $p(z) = \mathcal{N}(0, I)$ ã«è¿‘ã¥ã‘ãŸã„ã®ã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ› $q_\phi(z \mid x)$ ã ã‹ã‚‰ã€‚

å‚è€ƒ: [ç¬¬6å›ã§å°å‡º](./ml-lecture-06.md)
:::

:::details Q3: $z = \mu + \sigma \odot \epsilon$ ã® $\odot$ ã¯ä½•ã‹ï¼Ÿ

**è¨˜å·**: $\odot$ ã¯è¦ç´ ã”ã¨ã®ç© (element-wise product, Hadamard product)

**æ•°å¼**: $z_i = \mu_i + \sigma_i \epsilon_i$ for $i = 1, \ldots, d$

**å®Ÿè£…**:
```julia
z = Î¼ .+ Ïƒ .* Îµ  # Julia
z = mu + sigma * eps  # PyTorch (broadcast is implicit)
```

Reparameterization Trick ã®æ ¸å¿ƒéƒ¨åˆ†ã€‚[^1]
:::

:::details Q4: $\sigma = \exp(0.5 \log \sigma^2)$ ã®æ„å›³

**å•**: ãªãœç›´æ¥ $\sigma$ ã‚’å‡ºåŠ›ã›ãšã€$\log \sigma^2$ ã‚’å‡ºåŠ›ã™ã‚‹ã®ã‹ï¼Ÿ

**ç­”**:
1. $\sigma > 0$ ã®åˆ¶ç´„ã‚’è‡ªå‹•ã§æº€ãŸã™ï¼ˆæŒ‡æ•°é–¢æ•°ã¯å¸¸ã«æ­£ï¼‰
2. æ•°å€¤å®‰å®šæ€§: $\sigma \to 0$ ã®ã¨ãã€$\log \sigma^2 \to -\infty$ ã§å‹¾é…ãŒæ®‹ã‚‹
3. KLç™ºæ•£ã®è¨ˆç®—ã§ $\log \sigma^2$ ãŒç›´æ¥ä½¿ã‚ã‚Œã‚‹

Zone 3.3ã§å°å‡ºã—ãŸé€šã‚Šã€ã‚¬ã‚¦ã‚¹KLã¯:
$$
D_\text{KL} = \frac{1}{2} \sum (\mu^2 + \sigma^2 - \log \sigma^2 - 1)
$$
$\log \sigma^2$ ã‚’ç›´æ¥ä½¿ãˆã°ã€`exp` ã¨ `log` ãŒç›¸æ®ºã•ã‚Œã‚‹ã€‚
:::

:::details Q5: $p_\theta(x \mid z)$ ãŒBernoulliåˆ†å¸ƒã®ã¨ãã€å†æ§‹æˆé …ã¯ä½•ã‹ï¼Ÿ

**ç­”**: Binary Cross-Entropy (BCE)

$$
-\log p_\theta(x \mid z) = -\sum_{i=1}^{784} [x_i \log \hat{x}_i + (1 - x_i) \log(1 - \hat{x}_i)]
$$

ã“ã“ã§ $\hat{x} = \text{Decoder}_\theta(z)$ ã¯ã€å„ãƒ”ã‚¯ã‚»ãƒ«ãŒ1ã§ã‚ã‚‹ç¢ºç‡ã€‚

Gaussianä»®å®šã®å ´åˆï¼ˆé€£ç¶šå€¤ç”»åƒï¼‰:
$$
-\log p_\theta(x \mid z) = \frac{1}{2\sigma^2} \|x - \hat{x}\|^2 + \text{const}
$$
ã“ã‚Œã¯MSE (Mean Squared Error) ã«å¯¾å¿œã€‚
:::

### 5.2 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ â€” æ•°å¼ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã¸

:::details Q6: ä»¥ä¸‹ã®æ•°å¼ã‚’Juliaã§å®Ÿè£…ã›ã‚ˆ

æ•°å¼:
$$
\mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)] - D_\text{KL}(q_\phi(z \mid x) \| p(z))
$$

ãŸã ã—:
- $z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$
- $p_\theta(x \mid z) = \mathcal{N}(x \mid \mu_\theta(z), I)$

**ç­”**:
```julia
function vae_elbo(encoder, decoder, ps_enc, ps_dec, st_enc, st_dec, x)
    # Encode: q_Ï†(z|x)
    (Î¼, logÏƒÂ²), st_enc = encoder(x, ps_enc, st_enc)

    # Reparameterize: z = Î¼ + ÏƒÂ·Îµ
    Ïƒ = exp.(0.5 .* logÏƒÂ²)
    Îµ = randn(Float32, size(Î¼)...)
    z = Î¼ .+ Ïƒ .* Îµ

    # Decode: p_Î¸(x|z)
    x_recon, st_dec = decoder(z, ps_dec, st_dec)

    # Reconstruction term: E_q[log p(x|z)] â‰ˆ -MSE (Gaussian assumption)
    recon_term = -0.5f0 * sum((x .- x_recon).^2)

    # KL term: D_KL(q||p) (closed-form for Gaussian)
    kl_term = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))

    elbo = recon_term - kl_term  # ELBO (to maximize)
    loss = -elbo                  # Loss (to minimize)

    return loss, st_enc, st_dec
end
```

ãƒã‚¤ãƒ³ãƒˆ:
- `sum()` ãŒæœŸå¾…å€¤ã® Monte Carlo è¿‘ä¼¼ï¼ˆ1ã‚µãƒ³ãƒ—ãƒ«ï¼‰
- ELBO ã¯æœ€å¤§åŒ–ã—ãŸã„ãŒã€æå¤±é–¢æ•°ã¯æœ€å°åŒ–ã™ã‚‹ã®ã§ç¬¦å·åè»¢
:::

:::details Q7: Straight-Through Estimator (STE) ã‚’Juliaã§å®Ÿè£…

æ•°å¼:
$$
\text{Forward:} \quad z_q = \text{quantize}(z_e) \\
\text{Backward:} \quad \frac{\partial L}{\partial z_e} = \frac{\partial L}{\partial z_q}
$$

**ç­”**:
```julia
using ChainRulesCore

function straight_through_quantize(z_e, codebook)
    # Forward: find nearest codebook entry
    distances = sum((z_e .- codebook).^2, dims=1)
    indices = argmin(distances, dims=1)
    z_q = codebook[:, indices]

    # Straight-through: gradient flows as if z_q = z_e
    return z_e + (z_q - z_e)  # This is a no-op in forward, but gradient flows through z_e
end

# Custom gradient rule (Zygote.jl)
function ChainRulesCore.rrule(::typeof(straight_through_quantize), z_e, codebook)
    z_q = straight_through_quantize(z_e, codebook)

    function pullback(Î”z_q)
        # Gradient w.r.t. z_e: âˆ‚L/âˆ‚z_e = âˆ‚L/âˆ‚z_q
        return NoTangent(), Î”z_q, NoTangent()
    end

    return z_q, pullback
end
```

VQ-VAE [^3] ã§ä½¿ã‚ã‚Œã‚‹ã€é›¢æ•£åŒ–ã®å‹¾é…è¿‘ä¼¼ã€‚
:::

### 5.3 æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ– â€” 2æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã®æ§‹é€ 

```julia
using Lux, MLDatasets, Plots

# Train a 2D VAE (from Zone 4)
latent_dim = 2
encoder = create_encoder(784, 400, latent_dim)
decoder = create_decoder(latent_dim, 400, 784)
# ... (training code omitted)

# Encode test data
test_data = MLDatasets.MNIST(split=:test)
test_x = reshape(test_data.features, 784, :) |> x -> Float32.(x)
test_y = test_data.targets

# Get latent codes
(Î¼, logÏƒÂ²), _ = encoder(test_x, ps_enc, st_enc)
z = Î¼  # Use mean (no sampling for visualization)

# Scatter plot colored by digit label
scatter(z[1, :], z[2, :], group=test_y, markersize=2, alpha=0.5,
        xlabel="zâ‚", ylabel="zâ‚‚", title="VAE Latent Space (MNIST)",
        legend=:outertopright)
savefig("vae_latent_space.png")
```

æœŸå¾…ã•ã‚Œã‚‹çµæœ:
- åŒã˜æ•°å­—ãŒæ½œåœ¨ç©ºé–“ã§è¿‘ãã«é›†ã¾ã‚‹ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰
- æ•°å­—é–“ã®é·ç§»ãŒæ»‘ã‚‰ã‹ï¼ˆä¾‹: 3ã¨8ãŒéš£æ¥ï¼‰

### 5.4 æ½œåœ¨ç©ºé–“ã®è£œé–“ â€” 0ã‹ã‚‰9ã¸ã®å¤‰å½¢

```julia
# Find latent codes for digit "0" and "9"
idx_0 = findfirst(test_y .== 0)
idx_9 = findfirst(test_y .== 9)

z_0 = Î¼[:, idx_0]
z_9 = Î¼[:, idx_9]

# Linear interpolation
n_steps = 10
alphas = range(0, 1, length=n_steps)
z_interp = hcat([Î± * z_9 + (1 - Î±) * z_0 for Î± in alphas]...)

# Decode
x_interp, _ = decoder(z_interp, ps_dec, st_dec)

# Visualize
using Images
imgs = [Gray.(reshape(x_interp[:, i], 28, 28)) for i in 1:n_steps]
mosaicview(imgs, nrow=1, npad=2)
```

å‡ºåŠ›: 0 â†’ (ä¸­é–“å½¢çŠ¶) â†’ 9 ã¸ã®æ»‘ã‚‰ã‹ãªå¤‰å½¢

### 5.5 å±æ€§æ“ä½œ â€” ã€Œç¬‘é¡”ãƒ™ã‚¯ãƒˆãƒ«ã€ã‚’è¦‹ã¤ã‘ã‚‹

CelebAï¼ˆé¡”ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰ã§è¨“ç·´ã—ãŸVAEãªã‚‰ã€æ½œåœ¨ç©ºé–“ã§ **å±æ€§ãƒ™ã‚¯ãƒˆãƒ«** ã‚’å®šç¾©ã§ãã‚‹ [^2]ã€‚

```julia
# Pseudo-code (requires CelebA dataset + attribute labels)
# Find "smiling" direction in latent space

# 1. Encode smiling and non-smiling faces
z_smiling = mean(encode(x_smiling), dims=2)
z_neutral = mean(encode(x_neutral), dims=2)

# 2. Compute "smile vector"
v_smile = z_smiling - z_neutral

# 3. Apply to any face
z_input = encode(x_input)
z_more_smile = z_input + 0.5 * v_smile  # increase smile
x_output = decode(z_more_smile)
```

ã“ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã¯ã€StyleGANã®latent space manipulationã®åŸå‹ã€‚

### 5.6 Posterior Collapseå®Ÿé¨“ â€” ãªãœèµ·ãã‚‹ã®ã‹

**Posterior Collapse** ã¯ã€VAEã®æœ€å¤§ã®è½ã¨ã—ç©´ã ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒæ½œåœ¨å¤‰æ•° $z$ ã‚’ç„¡è¦–ã—ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãŒå¹³å‡çš„ãªç”»åƒã‚’å‡ºåŠ›ã—ã¦ã—ã¾ã†ç¾è±¡ã€‚

#### 5.6.1 Collapseã®æ¤œå‡ºæ–¹æ³•

```python
def detect_posterior_collapse(model, train_loader):
    """Detect posterior collapse by monitoring KL divergence per dimension."""
    total_kl_per_dim = 0
    num_batches = 0

    for x_batch, _ in train_loader:
        mu, logvar = model.encode(x_batch)
        # KL per dimension: 0.5 * (Î¼Â² + ÏƒÂ² - log(ÏƒÂ²) - 1)
        kl_per_dim = 0.5 * (mu.pow(2) + logvar.exp() - logvar - 1)
        total_kl_per_dim += kl_per_dim.mean(dim=0).detach()
        num_batches += 1

    avg_kl_per_dim = total_kl_per_dim / num_batches

    # Collapseåˆ¤å®š: KL < 0.01 ã®æ¬¡å…ƒãŒå¤šã„
    collapsed_dims = (avg_kl_per_dim < 0.01).sum().item()
    active_dims = (avg_kl_per_dim >= 0.01).sum().item()

    print(f"Active dimensions: {active_dims} / {len(avg_kl_per_dim)}")
    print(f"Collapsed dimensions: {collapsed_dims}")
    print(f"KL per dimension: {avg_kl_per_dim[:10]}")  # first 10

    return avg_kl_per_dim

# Run detection
kl_per_dim = detect_posterior_collapse(model, train_loader)

# Visualize
import matplotlib.pyplot as plt
plt.bar(range(len(kl_per_dim)), kl_per_dim.cpu().numpy())
plt.xlabel("Latent Dimension")
plt.ylabel("KL Divergence")
plt.title("Posterior Collapse Detection")
plt.axhline(y=0.01, color='r', linestyle='--', label='Collapse threshold')
plt.legend()
plt.savefig("posterior_collapse.png")
```

æœŸå¾…ã•ã‚Œã‚‹çµæœ:
- **å¥å…¨ãªVAE**: ã»ã¨ã‚“ã©ã®æ¬¡å…ƒã§KL > 0.1
- **Collapsed VAE**: å¤šãã®æ¬¡å…ƒã§KL â‰ˆ 0ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒç„¡è¦–ã•ã‚Œã¦ã„ã‚‹ï¼‰

#### 5.6.2 Collapseå¯¾ç­–: KL Annealing

KLé …ã®é‡ã¿ã‚’ã€è¨“ç·´åˆæœŸã¯å°ã•ãã€å¾ã€…ã«å¢—ã‚„ã™ã€‚

```python
def kl_annealing_schedule(epoch, total_epochs, strategy='linear'):
    """KL annealing schedule to prevent posterior collapse."""
    if strategy == 'linear':
        return min(1.0, epoch / (total_epochs * 0.5))
    elif strategy == 'sigmoid':
        k = 0.1  # steepness
        x0 = total_epochs * 0.5  # midpoint
        return 1 / (1 + np.exp(-k * (epoch - x0)))
    elif strategy == 'cyclical':
        # Cyclical annealing (4 cycles)
        period = total_epochs / 4
        return (epoch % period) / period
    else:
        return 1.0

def train_with_annealing(model, train_loader, optimizer, epochs):
    for epoch in range(epochs):
        beta = kl_annealing_schedule(epoch, epochs, strategy='linear')

        for x_batch, _ in train_loader:
            optimizer.zero_grad()
            recon, mu, logvar = model(x_batch)

            # Annealed loss
            recon_loss = F.binary_cross_entropy(recon, x_batch.view(-1, 784), reduction='sum')
            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
            loss = recon_loss + beta * kl_loss  # Î² starts from 0, increases to 1

            loss.backward()
            optimizer.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Î²={beta:.3f}, Loss={loss.item():.2f}")
```

**æˆ¦ç•¥ã®æ¯”è¼ƒ**:

| æˆ¦ç•¥ | ç‰¹å¾´ | åˆ©ç‚¹ | æ¬ ç‚¹ |
|:-----|:-----|:-----|:-----|
| Linear | $\beta(t) = \min(1, t / T)$ | å®Ÿè£…ç°¡å˜ | ä¸­ç›¤ã§æ€¥æ¿€ã«å¤‰åŒ– |
| Sigmoid | $\beta(t) = 1/(1 + e^{-k(t - t_0)})$ | æ»‘ã‚‰ã‹ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å¿…è¦ |
| Cyclical | $\beta(t) = (t \mod P) / P$ | Collapseã‹ã‚‰å›å¾©å¯èƒ½ | è¨“ç·´ãŒä¸å®‰å®š |

#### 5.6.3 Free Bits â€” æ¬¡å…ƒã”ã¨ã®æœ€å°KLä¿è¨¼

å„æ½œåœ¨æ¬¡å…ƒã«ã€æœ€å°KLå€¤ã‚’ä¿è¨¼ã™ã‚‹ [^7]ã€‚

```python
def free_bits_loss(recon_x, x, mu, logvar, free_bits=0.5):
    """VAE loss with free bits constraint.

    Ensures each latent dimension has KL â‰¥ free_bits (e.g., 0.5 nats).
    """
    recon_loss = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')

    # KL per dimension (batch averaged)
    kl_per_dim = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=0)  # (latent_dim,)

    # Apply free bits: max(KL_i, free_bits)
    kl_per_dim_clamped = torch.clamp(kl_per_dim, min=free_bits)

    total_kl = kl_per_dim_clamped.sum()

    return recon_loss + total_kl

# Training with free bits
optimizer = optim.Adam(model.parameters(), lr=1e-3)
for epoch in range(10):
    for x_batch, _ in train_loader:
        optimizer.zero_grad()
        recon, mu, logvar = model(x_batch)
        loss = free_bits_loss(recon, x_batch, mu, logvar, free_bits=0.5)
        loss.backward()
        optimizer.step()
```

**åŠ¹æœ**: å„æ¬¡å…ƒãŒæœ€ä½0.5 natsã®æƒ…å ±ã‚’ä¿æŒã™ã‚‹ã“ã¨ã‚’ä¿è¨¼ã€‚Collapseã‚’é˜²ãã€‚

### 5.7 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: Tiny VAE on MNIST (300K params)

å®Œå…¨ã«å‹•ä½œã™ã‚‹ã€è»½é‡VAEã‚’å®Ÿè£…ã—ã‚ˆã†ã€‚ç›®æ¨™:
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 300Kä»¥ä¸‹
- è¨“ç·´æ™‚é–“: CPU 5åˆ†ä»¥å†…
- å†æ§‹æˆç²¾åº¦: ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§BCE < 120

```julia
# Julia implementation (Lux.jl)
using Lux, Optimisers, Zygote, MLDatasets, Random, Statistics

# Tiny VAE architecture
function create_tiny_vae(; input_dim=784, hidden_dim=256, latent_dim=10)
    encoder = Chain(
        Dense(input_dim => hidden_dim, relu),
        Parallel(tuple,
                 Dense(hidden_dim => latent_dim),       # Î¼
                 Dense(hidden_dim => latent_dim))       # log ÏƒÂ²
    )

    decoder = Chain(
        Dense(latent_dim => hidden_dim, relu),
        Dense(hidden_dim => input_dim, sigmoid)
    )

    return encoder, decoder
end

# Training function
function train_tiny_vae(; epochs=10, batch_size=128, lr=1e-3)
    rng = Random.default_rng()

    # Create models
    encoder, decoder = create_tiny_vae(hidden_dim=256, latent_dim=10)
    ps_enc, st_enc = Lux.setup(rng, encoder)
    ps_dec, st_dec = Lux.setup(rng, decoder)

    # Count parameters
    n_params = sum(length, Lux.parameterlength.([ps_enc, ps_dec]))
    println("Total parameters: $(n_params)")

    # Optimizer
    opt_enc = Optimisers.setup(Optimisers.Adam(lr), ps_enc)
    opt_dec = Optimisers.setup(Optimisers.Adam(lr), ps_dec)

    # Load MNIST
    train_data = MLDatasets.MNIST(split=:train)
    train_x = Float32.(reshape(train_data.features, 784, :))

    # Training loop
    for epoch in 1:epochs
        total_loss = 0.0f0
        num_batches = 0

        for i in 1:batch_size:size(train_x, 2)-batch_size
            x_batch = train_x[:, i:i+batch_size-1]

            # Compute gradients
            (loss, (st_enc, st_dec)), grads = Zygote.withgradient(ps_enc, ps_dec) do p_enc, p_dec
                # Encode
                (Î¼, logÏƒÂ²), st_enc_new = encoder(x_batch, p_enc, st_enc)

                # Reparameterize
                Ïƒ = exp.(0.5f0 .* logÏƒÂ²)
                Îµ = randn(Float32, size(Î¼)...)
                z = Î¼ .+ Ïƒ .* Îµ

                # Decode
                x_recon, st_dec_new = decoder(z, p_dec, st_dec)

                # Loss
                bce = -sum(x_batch .* log.(x_recon .+ 1f-8) .+ (1 .- x_batch) .* log.(1 .- x_recon .+ 1f-8))
                kld = -0.5f0 * sum(1 .+ logÏƒÂ² .- Î¼.^2 .- exp.(logÏƒÂ²))
                loss = bce + kld

                return loss, (st_enc_new, st_dec_new)
            end

            # Update
            Optimisers.update!(opt_enc, ps_enc, grads[1])
            Optimisers.update!(opt_dec, ps_dec, grads[2])

            total_loss += loss
            num_batches += 1
        end

        avg_loss = total_loss / (num_batches * batch_size)
        println("Epoch $epoch: Loss = $(avg_loss)")
    end

    return encoder, decoder, ps_enc, ps_dec, st_enc, st_dec
end

# Run training
@time encoder, decoder, ps_enc, ps_dec, st_enc, st_dec = train_tiny_vae(epochs=10)
```

æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
```
Total parameters: 291,594
Epoch 1: Loss = 152.34
Epoch 2: Loss = 118.56
...
Epoch 10: Loss = 104.23
245.123456 seconds (CPU time)
```

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° < 300K
- [ ] è¨“ç·´æ™‚é–“ < 5åˆ†ï¼ˆCPUï¼‰
- [ ] æœ€çµ‚Loss < 110

### 5.8 Paper Reading Test â€” VAEè«–æ–‡ã®é‡è¦å›³ã‚’èª­ã‚€

Kingma & Welling (2013) [^1] ã® Figure 1 ã‚’å®Œå…¨ã«ç†è§£ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã‚ˆã†ã€‚

:::details Q8: Figure 1 ã® Graphical Model ã‚’èª¬æ˜ã›ã‚ˆ

**å•**: è«–æ–‡ã®Figure 1ã«æã‹ã‚Œã¦ã„ã‚‹Graphical Modelã®æ„å‘³ã‚’ã€ç¢ºç‡çš„ä¾å­˜é–¢ä¿‚ã¨ã¨ã‚‚ã«èª¬æ˜ã›ã‚ˆã€‚

**ç­”**:

```
    zâ‚ ----> xâ‚
    â†‘         â†‘
    |         |
   Î¸,Ï†      Î¸,Ï†
    |         |
    â†“         â†“
    zâ‚‚ ----> xâ‚‚
    â‹®         â‹®
    zâ‚™ ----> xâ‚™
```

- $z_i \sim p(z)$: äº‹å‰åˆ†å¸ƒï¼ˆæ¨™æº–æ­£è¦åˆ†å¸ƒï¼‰
- $x_i \mid z_i \sim p_\theta(x \mid z)$: ãƒ‡ã‚³ãƒ¼ãƒ€ï¼ˆç”Ÿæˆéç¨‹ï¼‰
- $q_\phi(z \mid x)$: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆå¤‰åˆ†åˆ†å¸ƒã€å›³ã«ã¯çœç•¥ï¼‰

VAEã¯ã€ã“ã®graphical modelã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’æœ€å°¤æ¨å®šã—ã€åŒæ™‚ã«è¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒ $q_\phi(z \mid x)$ ã‚’å­¦ç¿’ã™ã‚‹ã€‚

Plate notation ã§ $N$ å€‹ã®ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒç‹¬ç«‹ã«ç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
:::

:::message
**é€²æ—: 85% å®Œäº†** ã‚·ãƒ³ãƒœãƒ«èª­è§£ã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³ã€æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–ãƒ»è£œé–“ãƒ»å±æ€§æ“ä½œã€Posterior Collapseå®Ÿé¨“ã€ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€è«–æ–‡å›³èª­è§£ã‚’å®Œèµ°ã—ãŸã€‚Zone 6ã§æœ€æ–°ç ”ç©¶ã®å…¨ä½“åƒã‚’æŠŠæ¡ã™ã‚‹ã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 FSQ (Finite Scalar Quantization) â€” VQ-VAEã®ç°¡ç´ ç‰ˆ

VQ-VAEã®èª²é¡Œ:
- **Codebook Collapse**: ä¸€éƒ¨ã®ã‚³ãƒ¼ãƒ‰ã ã‘ãŒä½¿ã‚ã‚Œã€æ®‹ã‚ŠãŒæ­»ã¬
- **è¤‡é›‘ãªè¨“ç·´**: Commitment Loss, EMAæ›´æ–°, Codebookå†åˆæœŸåŒ–

FSQ [^4] ã¯ã“ã‚Œã‚’æ ¹æœ¬ã‹ã‚‰è§£æ±º:

**Key Idea**: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚’å­¦ç¿’ã›ãšã€**å›ºå®šã‚°ãƒªãƒƒãƒ‰**ã«é‡å­åŒ–ã™ã‚‹ã€‚

$$
z_i \in \{-1, 0, 1\}, \quad \text{for } i = 1, \ldots, d
$$

ä¾‹: $d=8$ æ¬¡å…ƒã€å„æ¬¡å…ƒãŒ $\{-1, 0, 1\}$ â†’ ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ ã‚µã‚¤ã‚º = $3^8 = 6561$

```julia
function fsq_quantize(z::AbstractArray, levels::Vector{Int})
    """Finite Scalar Quantization.

    z: continuous latent codes (d, N)
    levels: quantization levels per dimension (e.g., [3, 3, 3, 3, 3, 3, 3, 3])
    """
    d, N = size(z)
    z_q = similar(z)

    for i in 1:d
        # Map continuous values to discrete grid
        L = levels[i]
        grid = range(-1, 1, length=L)
        z_q[i, :] = [grid[argmin(abs.(z[i, j] .- grid))] for j in 1:N]
    end

    # Straight-through estimator
    return z + (z_q - z)  # gradient flows through z
end
```

**åˆ©ç‚¹**:
- Codebook Collapse ãŒåŸç†çš„ã«èµ·ããªã„ï¼ˆå…¨ã‚°ãƒªãƒƒãƒ‰ç‚¹ãŒå®šç¾©æ¸ˆã¿ï¼‰
- è¨“ç·´ãŒå˜ç´”ï¼ˆEMAä¸è¦ã€Commitment Lossä¸è¦ï¼‰
- VQ-VAEã¨åŒç­‰ã®æ€§èƒ½

### 6.2 Cosmos Tokenizer â€” ç”»åƒã¨å‹•ç”»ã®çµ±ä¸€è¡¨ç¾

NVIDIA Cosmos Tokenizer [^5] ã¯ã€2024å¹´ã®æœ€æ–°ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã ã€‚

**ç‰¹å¾´**:
- ç”»åƒ (256Ã—256) ã¨å‹•ç”» (16ãƒ•ãƒ¬ãƒ¼ãƒ ) ã‚’åŒã˜æ½œåœ¨ç©ºé–“ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
- ç©ºé–“åœ§ç¸®ç‡: 8Ã—8ã€æ™‚é–“åœ§ç¸®ç‡: 4
- é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³: 16,384èªå½™
- Diffusion Transformer (DiT) ã¨ã®ä½µç”¨ã‚’æƒ³å®š

```
Image (256Ã—256Ã—3) â†’ Encoder â†’ (32Ã—32Ã—C) â†’ FSQ/VQ â†’ Discrete tokens (32Ã—32)
Video (256Ã—256Ã—16Ã—3) â†’ Encoder â†’ (32Ã—32Ã—4Ã—C) â†’ FSQ/VQ â†’ Discrete tokens (32Ã—32Ã—4)
```

å¿œç”¨:
- å‹•ç”»ç”ŸæˆAIï¼ˆSora-likeãƒ¢ãƒ‡ãƒ«ï¼‰ã®å‰æ®µ
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMï¼ˆç”»åƒãƒ»å‹•ç”»ç†è§£ï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼

### 6.3 ç ”ç©¶ã®æœ€å‰ç·š â€” 2025-2026è«–æ–‡ãƒªã‚¹ãƒˆ

| è«–æ–‡ | è‘—è€… | å¹´ | æ ¸å¿ƒè²¢çŒ® | arXiv |
|:-----|:-----|:---|:--------|:------|
| CAR-Flow | - | 2025/09 | æ¡ä»¶ä»˜ãå†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ– | 2509.19300 |
| DVAE | - | 2025 | äºŒçµŒè·¯ã§Posterior Collapseé˜²æ­¢ | æ¤œç´¢è¦ |
| é€†Lipschitzåˆ¶ç´„VAE | - | 2023 | Decoderåˆ¶ç´„ã§ç†è«–ä¿è¨¼ | 2304.12770 |
| GQ-VAE | - | 2025/12 | å¯å¤‰é•·é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ | 2512.21913 |
| MGVQ | - | 2025/07 | Multi-groupé‡å­åŒ– | 2507.07997 |
| TiTok v2 | - | 2025 | 1Dç”»åƒãƒˆãƒ¼ã‚¯ãƒ³åŒ– | æ¤œç´¢è¦ |
| Open-MAGVIT3 | - | 2025 | MAGVIT-v2å¾Œç¶™ | æ¤œç´¢è¦ |

#### 6.3.1 CAR-Flow â€” æ¡ä»¶ä»˜ãå†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã®é©æ–°

**å•é¡Œ**: æ¨™æº–çš„ãªReparameterization Trickã¯ã€å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ$\mu$ã¨$\sigma$ï¼‰ã«å‹¾é…ã‚’æµã™ã€‚ã—ã‹ã—ã€å ´åˆã«ã‚ˆã£ã¦ã¯$\mu$ã®ã¿æ›´æ–°ã—ãŸã„ï¼ˆä¾‹: ã‚¹ã‚±ãƒ¼ãƒ«å›ºå®šï¼‰ã€‚

**CAR-Flow (Conditional Affine Reparameterization)**:

$$
z = \mu_\phi(x) + \sigma_\text{fixed} \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

$\sigma$ã‚’å›ºå®šã™ã‚‹ã“ã¨ã§:
- æ½œåœ¨ç©ºé–“ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒå®‰å®š
- è¨“ç·´ãŒé«˜é€ŸåŒ–ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠæ¸›ï¼‰
- Flowãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¥ç¶šãŒæ˜ç¢ºã«

å¿œç”¨: Latent Diffusion Modelã®VAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã€ã‚¹ã‚±ãƒ¼ãƒ«å›ºå®šãŒæœ‰åŠ¹ã€‚

#### 6.4.2 DVAE â€” äºŒçµŒè·¯ã§Posterior Collapseé˜²æ­¢

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«2ã¤ã®çµŒè·¯ã‚’ç”¨æ„:
- çµŒè·¯A: ç›´æ¥çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå¾“æ¥é€šã‚Šï¼‰
- çµŒè·¯B: ãƒã‚¹ã‚¯ã‚’ä»‹ã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆãƒã‚¤ã‚ºã«å¼·ã„ï¼‰

è¨“ç·´åˆæœŸã¯ä¸¡æ–¹ã‚’ä½¿ã„ã€å¾ŒæœŸã¯çµŒè·¯Aã®ã¿ã€‚ã“ã‚Œã§ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒæ—©æœŸã«Collapseã™ã‚‹ã®ã‚’é˜²ãã€‚

```python
def dual_path_encoder(x, training=True):
    # Path A: direct encoding
    mu_a, logvar_a = encoder_a(x)

    if training:
        # Path B: masked encoding
        x_masked = x * (torch.rand_like(x) > 0.3).float()  # 30% mask
        mu_b, logvar_b = encoder_b(x_masked)

        # Combine: weighted average
        alpha = min(1.0, epoch / 50)  # gradually shift to Path A
        mu = alpha * mu_a + (1 - alpha) * mu_b
        logvar = alpha * logvar_a + (1 - alpha) * logvar_b
    else:
        mu, logvar = mu_a, logvar_a

    return mu, logvar
```

#### 6.4.3 GQ-VAE â€” å¯å¤‰é•·é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆBPEåœ§ç¸®ç‡ã«æ¥è¿‘ï¼‰

**å•é¡Œ**: VQ-VAEã¯å›ºå®šé•·ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¾‹: 256Ã—256 â†’ 32Ã—32ï¼‰ã€‚æƒ…å ±é‡ãŒå°‘ãªã„é ˜åŸŸã‚‚ä¸€æ§˜ã«åœ§ç¸®ã€‚

**GQ-VAE**: å¯å¤‰é•·ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€‚æƒ…å ±é‡ã«å¿œã˜ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’èª¿æ•´ã€‚

```
High-detail region (é¡”):   128 tokens
Low-detail region (ç©º):    16 tokens
```

**åŠ¹æœ**: åœ§ç¸®ç‡ãŒBPEï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼‰ã«æ¥è¿‘ã€‚LLMã¨ã®çµ±åˆãŒå®¹æ˜“ã«ã€‚

#### 6.4.4 MGVQ â€” Multi-group Vector Quantization

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚’è¤‡æ•°ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²ã€‚å„ã‚°ãƒ«ãƒ¼ãƒ—ãŒç•°ãªã‚‹ã€Œæ„å‘³ã®ç²’åº¦ã€ã‚’æ‹…å½“ã€‚

```
Group 1 (ç²—ã„ç‰¹å¾´): 16 codes â†’ è‰²ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£
Group 2 (ä¸­é–“ç‰¹å¾´): 64 codes â†’ å½¢çŠ¶ã€é…ç½®
Group 3 (ç´°ã‹ã„ç‰¹å¾´): 256 codes â†’ ã‚¨ãƒƒã‚¸ã€è©³ç´°
```

**åˆ©ç‚¹**:
- Codebookåˆ©ç”¨ç‡ãŒå‘ä¸Šï¼ˆå„ã‚°ãƒ«ãƒ¼ãƒ—ã§ç‹¬ç«‹ï¼‰
- éšå±¤çš„ãªè¡¨ç¾ãŒè‡ªç„¶ã«å­¦ç¿’ã•ã‚Œã‚‹
- VQ-VAE-2ã®ç°¡ç´ ç‰ˆã¨ã—ã¦æ©Ÿèƒ½

#### 6.4.5 TiTok v2 â€” 1Dç”»åƒãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆARç”Ÿæˆã¨ã®æ¥ç¶šï¼‰

**å¾“æ¥ã®VQ-VAE**: 2Dæ½œåœ¨ç©ºé–“ï¼ˆä¾‹: 32Ã—32ï¼‰â†’ 2Dæ§‹é€ ã‚’ä¿æŒ

**TiTok v2**: 1Dæ½œåœ¨ç©ºé–“ï¼ˆä¾‹: 1024ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰â†’ Transformerã§ç›´æ¥ç”Ÿæˆå¯èƒ½

```
Image (256Ã—256) â†’ Encoder â†’ 1D sequence (1024 tokens) â†’ Decoder â†’ Image (256Ã—256)
```

**åˆ©ç‚¹**:
- Transformer ARãƒ¢ãƒ‡ãƒ«ã§ç›´æ¥ç”Ÿæˆï¼ˆ2Dã‚¹ã‚­ãƒ£ãƒ³ä¸è¦ï¼‰
- LLMã¨ã®çµ±ä¸€çš„ãªæ‰±ã„ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒåŒã˜ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼‰
- æ¨è«–é€Ÿåº¦å‘ä¸Šï¼ˆ2Dã‚¹ã‚­ãƒ£ãƒ³ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›ï¼‰

**èª²é¡Œ**: 2Dæ§‹é€ ã®å­¦ç¿’ãŒé›£ã—ã„ï¼ˆä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¿…é ˆï¼‰

### 6.4 VAEå®Ÿè£…ã®æ¯”è¼ƒ â€” PyTorch vs JAX vs Lux.jl

| é …ç›® | PyTorch | JAX (Flax) | Lux.jl (Julia) |
|:-----|:--------|:-----------|:---------------|
| **å®Ÿè£…è¡Œæ•°** | 150è¡Œ | 180è¡Œï¼ˆç´”ç²‹é–¢æ•°å‹ï¼‰ | 120è¡Œï¼ˆæœ€å°ï¼‰ |
| **è¨“ç·´é€Ÿåº¦ï¼ˆCPUï¼‰** | 2.35s/epoch | 1.82s/epoch | 0.29s/epoch |
| **GPUåˆ‡æ›¿** | `model.to('cuda')` | `jax.device_put(x, gpu)` | `CuArray(x)` |
| **å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚º** | âœ… å¯èƒ½ | âŒ JITå†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« | âœ… å¯èƒ½ |
| **ãƒ‡ãƒãƒƒã‚°** | âœ… pdb, printæ–‡ | âš ï¸ JITã§é›£ã—ã„ | âœ… Revise.jl + REPL |
| **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ** | æœ€å¤§ï¼ˆtorchvisionç­‰ï¼‰ | æˆé•·ä¸­ï¼ˆdm-haikuç­‰ï¼‰ | ç§‘å­¦è¨ˆç®—ç‰¹åŒ– |
| **å­¦ç¿’æ›²ç·š** | ç·©ã‚„ã‹ | æ€¥ï¼ˆç´”ç²‹é–¢æ•°å‹ï¼‰ | ä¸­ï¼ˆå¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒï¼‰ |

**é¸æŠæŒ‡é‡**:
- **ç ”ç©¶ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—**: PyTorchï¼ˆã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ æœ€å¤§ï¼‰
- **æœ¬ç•ªãƒ»å¤§è¦æ¨¡è¨“ç·´**: JAXï¼ˆTPUæœ€é©åŒ–ï¼‰
- **æ•°å€¤è¨ˆç®—ãƒ»ç§‘å­¦è¨ˆç®—**: Lux.jlï¼ˆæ•°å¼1:1ã€æœ€é€ŸCPUï¼‰

:::details ç”¨èªé›† (Glossary)

| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ | Variational Autoencoder | æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®ä¸€ç¨®ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ $q_\phi(z \mid x)$ ã‚’å­¦ç¿’ã€‚ |
| ELBO | Evidence Lower BOund | å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®ä¸‹ç•Œã€‚VAEã®æå¤±é–¢æ•°ã€‚ |
| å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ | Reparameterization Trick | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å¾®åˆ†å¯èƒ½ã«ã™ã‚‹æ‰‹æ³•ã€‚$z = \mu + \sigma \epsilon$ |
| KLç™ºæ•£ | KL Divergence | 2ã¤ã®åˆ†å¸ƒã®ã€Œè·é›¢ã€ã€‚éå¯¾ç§°ã€‚ |
| æ½œåœ¨ç©ºé–“ | Latent Space | ãƒ‡ãƒ¼ã‚¿ã®ä½æ¬¡å…ƒè¡¨ç¾ç©ºé–“ã€‚ |
| ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ | Codebook | é›¢æ•£æ½œåœ¨å¤‰æ•°ã®å€™è£œé›†åˆã€‚VQ-VAEã§ä½¿ç”¨ã€‚ |
| ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ– | Vector Quantization | é€£ç¶šãƒ™ã‚¯ãƒˆãƒ«ã‚’é›¢æ•£ã‚³ãƒ¼ãƒ‰ã«å†™åƒã€‚ |
| Straight-Through Estimator | STE | é›¢æ•£åŒ–ã®å‹¾é…ã‚’è¿‘ä¼¼ã™ã‚‹æ‰‹æ³•ã€‚ |
| Posterior Collapse | - | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒæ½œåœ¨å¤‰æ•°ã‚’ç„¡è¦–ã™ã‚‹ç¾è±¡ã€‚ |
| Disentanglement | - | æ½œåœ¨ç©ºé–“ã®å„æ¬¡å…ƒãŒç‹¬ç«‹ã—ãŸæ„å‘³ã‚’æŒã¤æ€§è³ªã€‚ |

:::

:::message
**é€²æ—: 95% å®Œäº†** VAEç³»åˆ—ã®ç³»è­œã€FSQ/Cosmosæœ€å‰ç·šã€æ¨è–¦æ›¸ç±ã‚’æŠŠæ¡ã—ãŸã€‚Zone 7ã§å…¨ä½“ã‚’æŒ¯ã‚Šè¿”ã‚‹ã€‚
:::

### 6.5 ã“ã®è¬›ç¾©ã®3ã¤ã®æ ¸å¿ƒ

1. **VAEã¯å¤‰åˆ†æ¨è«–ã®è‡ªå‹•åŒ–ã§ã‚ã‚‹** â€” æ‰‹å‹•è¨­è¨ˆã®è¿‘ä¼¼åˆ†å¸ƒ $q(z)$ ã‚’ã€NN $q_\phi(z \mid x)$ ã«ç½®ãæ›ãˆãŸã€‚Reparameterization Trickã§å¾®åˆ†å¯èƒ½ã«ã€‚

2. **é€£ç¶šæ½œåœ¨ç©ºé–“ã‹ã‚‰é›¢æ•£è¡¨ç¾ã¸** â€” VAEã®ã€Œã¼ã‚„ã‘ãŸç”»åƒã€å•é¡Œã‚’ã€VQ-VAEãŒé›¢æ•£ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã§è§£æ±ºã€‚FSQãŒã•ã‚‰ã«ç°¡ç´ åŒ–ã€‚2026å¹´ã®ç”»åƒãƒ»å‹•ç”»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åŸºç›¤ã€‚

3. **JuliaãŒè¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’8å€é«˜é€ŸåŒ–** â€” å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ + JIT + å‹å®‰å®šæ€§ã€‚æ•°å¼ãŒãã®ã¾ã¾ã‚³ãƒ¼ãƒ‰ã«ãªã‚‹ã€‚**Pythonã«æˆ»ã‚Œãªã„ã€‚**

### 6.6 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

:::details Q: VAEã®ç”»åƒãŒã¼ã‚„ã‘ã‚‹ã®ã¯ãªãœï¼Ÿ

**ç­”**: 2ã¤ã®ç†ç”±ãŒã‚ã‚‹:

1. **Gaussianä»®å®š**: ãƒ‡ã‚³ãƒ¼ãƒ€ãŒ $p_\theta(x \mid z) = \mathcal{N}(x \mid \mu_\theta(z), \sigma^2 I)$ ã‚’ä»®å®šã€‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¯ã€Œå¹³å‡çš„ãªç”»åƒã€ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚ã€ã‚¨ãƒƒã‚¸ãŒã¼ã‚„ã‘ã‚‹ã€‚

2. **Posterior Collapse**: KLæ­£å‰‡åŒ–ãŒå¼·ã™ãã‚‹ã¨ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒ $q_\phi(z \mid x) \approx p(z)$ ã«ãªã‚Šã€$z$ ãŒ $x$ ã®æƒ…å ±ã‚’æŒãŸãªããªã‚‹ã€‚ãƒ‡ã‚³ãƒ¼ãƒ€ã¯å¹³å‡çš„ãªç”»åƒã‚’å‡ºåŠ›ã™ã‚‹ã—ã‹ãªã„ã€‚

**è§£æ±ºç­–**:
- Î²-VAE ã§ Î² ã‚’å°ã•ãã™ã‚‹ï¼ˆå†æ§‹æˆé‡è¦–ï¼‰
- Perceptual Loss ã‚’ä½¿ã†ï¼ˆVQ-GANï¼‰
- GANã¨çµ„ã¿åˆã‚ã›ã‚‹ï¼ˆç¬¬12å›ï¼‰
:::

:::details Q: VQ-VAEã®Straight-Through Estimatorã¯ç†è«–çš„ã«æ­£ã—ã„ã®ã‹ï¼Ÿ

**ç­”**: **æ­£ã—ããªã„**ã€‚å‹¾é…ã®ä¸åæ¨å®šé‡ã§ã¯ãªã„ã€‚ã—ã‹ã—å®Ÿç”¨ä¸Šã¯å‹•ä½œã™ã‚‹ã€‚

ç†è«–çš„ã«ã¯ã€Gumbel-Softmaxï¼ˆé€£ç¶šç·©å’Œï¼‰ã®æ–¹ãŒå³å¯†ã ãŒã€VQ-VAEã®STEã®æ–¹ãŒå®Ÿè£…ãŒç°¡å˜ã§ã€æ€§èƒ½ã‚‚è‰¯ã„ï¼ˆçµŒé¨“çš„ï¼‰ã€‚

[^6] Bengio et al. (2013) "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation" â€” STEã®æœ€åˆã®ææ¡ˆ
:::

:::details Q: Juliaã¯æœ¬å½“ã«Pythonã‚ˆã‚Šé€Ÿã„ã®ã‹ï¼Ÿå…¨ã¦ã®ã‚±ãƒ¼ã‚¹ã§ï¼Ÿ

**ç­”**: **No**ã€‚JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã‚ã‚‹ãŸã‚ã€çŸ­ã„ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ1å›ã ã‘å®Ÿè¡Œï¼‰ã§ã¯Pythonã®æ–¹ãŒé€Ÿã„å ´åˆã‚‚ã‚ã‚‹ã€‚

**JuliaãŒé€Ÿã„ã‚±ãƒ¼ã‚¹**:
- ãƒ«ãƒ¼ãƒ—ã‚’ä½•åº¦ã‚‚å›ã™ï¼ˆè¨“ç·´ãƒ«ãƒ¼ãƒ—ãªã©ï¼‰
- å‹å®‰å®šãªã‚³ãƒ¼ãƒ‰
- æ•°å€¤è¨ˆç®—ãŒä¸»ä½“

**PythonãŒé€Ÿã„ã‚±ãƒ¼ã‚¹**:
- 1å›ã ã‘å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- I/Oå¾…ã¡ãŒä¸»ä½“ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼‰
- æ—¢å­˜ã®C/C++ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å‘¼ã¶ã ã‘ï¼ˆNumPy, Pandasï¼‰

**ä½¿ã„åˆ†ã‘**: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—â†’Pythonã€è¨“ç·´â†’Juliaã€æ¨è«–â†’Rust
:::

:::details Q: VAEã¨Diffusion Modelã®é–¢ä¿‚ã¯ï¼Ÿ

**ç­”**: VAEã¯ **Latent Diffusion Model (LDM)** ã®åŸºç›¤ã ã€‚

Stable Diffusionã®æ§‹é€ :
1. VAE Encoder: ç”»åƒ (512Ã—512) â†’ æ½œåœ¨ç©ºé–“ (64Ã—64Ã—4)
2. Diffusion Model: æ½œåœ¨ç©ºé–“ã§ãƒã‚¤ã‚ºé™¤å»
3. VAE Decoder: æ½œåœ¨ç©ºé–“ â†’ ç”»åƒ (512Ã—512)

VAEãŒé«˜æ¬¡å…ƒç”»åƒã‚’ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã«åœ§ç¸®ã™ã‚‹ã“ã¨ã§ã€Diffusion Modelã®è¨ˆç®—é‡ã‚’åŠ‡çš„ã«å‰Šæ¸›ã€‚Course IVã§è©³è¿°ã€‚
:::

:::details Q: æœ¬è¬›ç¾©ã§æ‰±ã‚ãªã‹ã£ãŸVAEç™ºå±•ãƒˆãƒ”ãƒƒã‚¯ã¯ï¼Ÿ

æœ¬è¬›ç¾©ã¯åŸºç¤ã¨é›¢æ•£è¡¨ç¾ã«é›†ä¸­ã—ãŸãŸã‚ã€ä»¥ä¸‹ã¯çœç•¥ã—ãŸ:

- **Hierarchical VAE** (Ladder VAE, NVAE) â€” éšå±¤çš„æ½œåœ¨è¡¨ç¾
- **Normalizing Flow Posterior** â€” ã‚ˆã‚ŠæŸ”è»Ÿãªäº‹å¾Œåˆ†å¸ƒï¼ˆç¬¬14å›ã§æ‰±ã†ï¼‰
- **Conditional VAE (CVAE)** â€” ãƒ©ãƒ™ãƒ«æ¡ä»¶ä»˜ãç”Ÿæˆ
- **Semi-supervised VAE** â€” ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
- **Variational Lossy Autoencoder (VLAE)** â€” æƒ…å ±ç†è«–çš„è§£é‡ˆ

èˆˆå‘³ãŒã‚ã‚Œã°ã€Zone 6ã®æ¨å¥¨æ›¸ç±ã‚’å‚ç…§ã€‚
:::

### 6.7 1é€±é–“ã®å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| æ—¥ | ã‚¿ã‚¹ã‚¯ | æ‰€è¦æ™‚é–“ | ç›®æ¨™ |
|:---|:------|:---------|:-----|
| **Day 1** | Zone 0-2 ã‚’èª­ã‚€ï¼ˆæ•°å¼ã‚¹ã‚­ãƒƒãƒ—ï¼‰ | 30åˆ† | å…¨ä½“åƒæŠŠæ¡ |
| **Day 2** | Zone 3.1-3.2 ELBO + Reparameterization å°å‡º | 1.5æ™‚é–“ | æ‰‹ã§å°å‡º |
| **Day 3** | Zone 3.3-3.4 Gaussian KL + Boss Battle | 1.5æ™‚é–“ | Kingma 2013 å®Œå…¨ç†è§£ |
| **Day 4** | Zone 4.1-4.3 Julia ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« + åŸºæœ¬æ–‡æ³• | 1æ™‚é–“ | Juliaç’°å¢ƒæ§‹ç¯‰ |
| **Day 5** | Zone 4.4-4.6 Julia VAE å®Ÿè£… + é€Ÿåº¦æ¸¬å®š | 2æ™‚é–“ | 8å€é€Ÿã‚’ä½“é¨“ |
| **Day 6** | Zone 5 æ½œåœ¨ç©ºé–“å¯è¦–åŒ– + è£œé–“ | 1.5æ™‚é–“ | å®Ÿé¨“ã§éŠã¶ |
| **Day 7** | Zone 6-7 æœ€æ–°ç ”ç©¶ + å¾©ç¿’ | 1æ™‚é–“ | å…¨ä½“æŒ¯ã‚Šè¿”ã‚Š |

**åˆè¨ˆ: ç´„9æ™‚é–“**ï¼ˆæœ¬è¬›ç¾©ã®ç›®æ¨™ã¯3æ™‚é–“ã ãŒã€å®Œå…¨ç¿’å¾—ã«ã¯3å€ã‹ã‹ã‚‹ï¼‰

### 6.8 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] VAEã®Encoder/Decoderã®å½¹å‰²ã‚’å›³ã§èª¬æ˜ã§ãã‚‹
- [ ] ELBOã‚’3è¡Œã§å°å‡ºã§ãã‚‹ï¼ˆJensenä¸ç­‰å¼ã‚’ä½¿ã£ã¦ï¼‰
- [ ] Reparameterization Trickã‚’å¼ã§æ›¸ã‘ã‚‹: $z = \mu + \sigma \epsilon$
- [ ] ã‚¬ã‚¦ã‚¹KLç™ºæ•£ã®é–‰å½¢å¼ã‚’æš—è¨˜ã—ã¦ã„ã‚‹ï¼ˆã¾ãŸã¯å°å‡ºã§ãã‚‹ï¼‰
- [ ] PyTorchã§VAEã‚’10è¡Œã§å®Ÿè£…ã§ãã‚‹
- [ ] **Juliaã§VAEã‚’å®Ÿè£…ã—ã€è¨“ç·´é€Ÿåº¦ã‚’æ¸¬å®šã—ãŸ**
- [ ] æ½œåœ¨ç©ºé–“ã®2Då¯è¦–åŒ–ã‚’ä½œæˆã—ãŸ
- [ ] VQ-VAEã®Straight-Through Estimatorã‚’èª¬æ˜ã§ãã‚‹
- [ ] FSQã¨VQ-VAEã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹

**7å€‹ä»¥ä¸Šãƒã‚§ãƒƒã‚¯ã§ãã‚Œã°åˆæ ¼ã€‚** æ¬¡ã®ç¬¬11å›ï¼ˆæœ€é©è¼¸é€ç†è«–ï¼‰ã«é€²ã‚ã‚‹ã€‚

### 6.9 æ¬¡å›äºˆå‘Š: ç¬¬11å› æœ€é©è¼¸é€ç†è«– (Optimal Transport)

VAEã¯ã€Œå†æ§‹æˆ + KLæ­£å‰‡åŒ–ã€ã§æ½œåœ¨ç©ºé–“ã‚’å­¦ç¿’ã—ãŸã€‚ã—ã‹ã—ã€KLç™ºæ•£ã«ã¯é™ç•ŒãŒã‚ã‚‹:
- å°ã®ä¸ä¸€è‡´ã§ç™ºæ•£ï¼ˆ$p(x)$ ã¨ $q(x)$ ã®ã‚µãƒãƒ¼ãƒˆãŒé‡ãªã‚‰ãªã„ã¨ âˆï¼‰
- å‹¾é…æ¶ˆå¤±ï¼ˆGANã®è¨“ç·´ä¸å®‰å®šæ€§ã®åŸå› ï¼‰

**æœ€é©è¼¸é€ç†è«–** (Optimal Transport) ã¯ã€ç¢ºç‡åˆ†å¸ƒé–“ã®ã€Œè·é›¢ã€ã‚’ã€**è¼¸é€ã‚³ã‚¹ãƒˆ**ã§å®šç¾©ã™ã‚‹ã€‚

$$
W_2(p, q) = \inf_{\gamma \in \Pi(p, q)} \mathbb{E}_{(x, y) \sim \gamma}[\|x - y\|^2]
$$

ã“ã® Wasserstein è·é›¢ã¯:
- å°ãŒä¸ä¸€è‡´ã§ã‚‚æœ‰é™å€¤
- é€£ç¶šçš„ã§ã€å‹¾é…ãŒå¸¸ã«å­˜åœ¨
- GANã®ç†è«–åŸºç›¤ï¼ˆWGANï¼‰
- Flow Matchingã®æ•°å­¦çš„åœŸå°ï¼ˆCourse IVï¼‰

**ç¬¬11å›ã§å­¦ã¶ã“ã¨**:
- Mongeå•é¡Œï¼ˆ1781å¹´ï¼‰ã‹ã‚‰Kantorovichç·©å’Œï¼ˆ1942å¹´ï¼‰ã¸
- Kantorovich-RubinsteinåŒå¯¾æ€§ï¼ˆç¬¬6å›ã®åŒå¯¾æ€§ã‚’å¿œç”¨ï¼‰
- Sinkhornè·é›¢ï¼ˆé«˜é€Ÿè¿‘ä¼¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
- OTã¨Flow Matchingã®æ¥ç¶šï¼ˆCourse IVã¸ã®ä¼ç·šï¼‰

```mermaid
graph LR
    L10["ç¬¬10å›: VAE<br>KLæ­£å‰‡åŒ–"] --> L11["ç¬¬11å›: æœ€é©è¼¸é€ç†è«–<br>Wassersteinè·é›¢"]
    L11 --> L12["ç¬¬12å›: GAN<br>WGANç†è«–"]
    L12 --> L13["ç¬¬13å›: StyleGAN<br>åˆ¶å¾¡å¯èƒ½ãªç”Ÿæˆ"]

    style L10 fill:#e1f5fe
    style L11 fill:#fff3e0
```

:::message
**é€²æ—: 100% å®Œäº†ï¼** VAEã®åŸºç¤ã‹ã‚‰é›¢æ•£è¡¨ç¾ã€Juliaå®Ÿè£…ã¾ã§å®Œèµ°ã—ãŸã€‚æ¬¡å›ã¯æœ€é©è¼¸é€ç†è«–ã§ã€ç¢ºç‡åˆ†å¸ƒé–“ã®ã€ŒçœŸã®è·é›¢ã€ã‚’å­¦ã¶ã€‚
:::

### 6.10 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œå¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã¯"ä¾¿åˆ©æ©Ÿèƒ½"ã‹ã€ãã‚Œã¨ã‚‚"è¨€èªã®æœ¬è³ª"ã‹ï¼Ÿã€**

Pythonã§ã¯ã€é–¢æ•°ã®æŒ¯ã‚‹èˆã„ã¯å¼•æ•°ã®**å‹**ã§ã¯ãªãã€**å€¤**ã§åˆ¶å¾¡ã•ã‚Œã‚‹:

```python
def f(x):
    if isinstance(x, int):
        return x + 1
    elif isinstance(x, list):
        return [i + 1 for i in x]
```

Juliaã§ã¯ã€é–¢æ•°ã®æŒ¯ã‚‹èˆã„ã¯**å‹**ã§åˆ¶å¾¡ã•ã‚Œã‚‹:

```julia
f(x::Int) = x + 1
f(x::Vector{Int}) = x .+ 1
```

**å•ã„**:
1. Pythonã® `isinstance` ãƒã‚§ãƒƒã‚¯ã¨ã€Juliaã®å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã¯ã€æœ¬è³ªçš„ã«ä½•ãŒé•ã†ã®ã‹ï¼Ÿ
2. å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã¯ã€Œifæ–‡ã‚’æ›¸ã‹ãªãã¦æ¸ˆã‚€ç³–è¡£æ§‹æ–‡ã€ãªã®ã‹ã€ãã‚Œã¨ã‚‚ã€Œå‹ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®çµ±åˆã€ãªã®ã‹ï¼Ÿ
3. **VAEã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ãŒ8å€é€Ÿããªã£ãŸç†ç”±ã¯ã€å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãªã®ã‹ã€JITãªã®ã‹ã€å‹å®‰å®šæ€§ãªã®ã‹ï¼Ÿãã‚Œã¨ã‚‚å…¨ã¦ã®ç›¸ä¹—åŠ¹æœãªã®ã‹ï¼Ÿ**

:::details ãƒ’ãƒ³ãƒˆ: Juliaã®è¨­è¨ˆå“²å­¦

Juliaã®å‰µå§‹è€…ã®è¨€è‘‰:

> "We want the speed of C with the dynamism of Ruby. We want a language that's homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell."
> â€” Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman (2012)

å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã¯ã€ã“ã®ã€Œå…¨ã¦ã‚’å®Ÿç¾ã™ã‚‹ã€ãŸã‚ã®æ ¸å¿ƒæŠ€è¡“ã ã£ãŸã€‚å‹ã«ã‚ˆã‚‹æœ€é©åŒ–ã¨ã€å‹•çš„è¨€èªã®æŸ”è»Ÿæ€§ã‚’ä¸¡ç«‹ã•ã›ã‚‹å”¯ä¸€ã®æ–¹æ³•ã€‚
:::

ã“ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’å—ã‘å…¥ã‚Œã‚‹ã¨ã€**Pythonã® `if isinstance(x, type):` ã‚’æ›¸ããŸã³ã«é•å’Œæ„Ÿã‚’è¦šãˆã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚** ãã‚ŒãŒã€ç¬¬10å›ã®ç›®æ¨™ã ã€‚

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Kingma, D. P., & Welling, M. (2013). Auto-Encoding Variational Bayes. *arXiv preprint arXiv:1312.6114*.
@[card](https://arxiv.org/abs/1312.6114)

[^2]: Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., ... & Lerchner, A. (2017). Î²-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. *International Conference on Learning Representations (ICLR)*.
@[card](https://openreview.net/forum?id=Sy2fzU9gl)

[^3]: van den Oord, A., Vinyals, O., & Kavukcuoglu, K. (2017). Neural Discrete Representation Learning. *Advances in Neural Information Processing Systems (NeurIPS)*. arXiv:1711.00937.
@[card](https://arxiv.org/abs/1711.00937)

[^4]: Mentzer, F., Minnen, D., Agustsson, E., & Tschannen, M. (2023). Finite Scalar Quantization: VQ-VAE Made Simple. *International Conference on Learning Representations (ICLR) 2024*. arXiv:2309.15505.
@[card](https://arxiv.org/abs/2309.15505)

[^5]: NVIDIA. (2024). Cosmos Tokenizer. *GitHub Repository*.
@[card](https://github.com/NVIDIA/Cosmos-Tokenizer)

[^6]: Bengio, Y., LÃ©onard, N., & Courville, A. (2013). Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. arXiv:1308.3432.
@[card](https://arxiv.org/abs/1308.3432)

[^7]: Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., & Welling, M. (2016). Improved Variational Inference with Inverse Autoregressive Flow. *NeurIPS 2016*.
@[card](https://arxiv.org/abs/1606.04934)

### é–¢é€£è«–æ–‡

- Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G., & Lerchner, A. (2018). Understanding disentangling in Î²-VAE. arXiv:1804.03599.
@[card](https://arxiv.org/abs/1804.03599)

- Kingma, D. P., Salimans, T., & Welling, M. (2015). Variational Dropout and the Local Reparameterization Trick. *NeurIPS*. arXiv:1506.02557.
@[card](https://arxiv.org/abs/1506.02557)

- Esser, P., Rombach, R., & Ommer, B. (2021). Taming Transformers for High-Resolution Image Synthesis. *CVPR*. arXiv:2012.09841.
@[card](https://arxiv.org/abs/2012.09841)

- Yu, L., Poirson, P., Yang, S., Berg, A. C., & Berg, T. L. (2023). MAGVIT-v2: Language Model Beats Diffusion - Tokenizer is Key to Visual Generation. arXiv:2310.05737.
@[card](https://arxiv.org/abs/2310.05737)

### æ•™ç§‘æ›¸

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer. Chapter 10: Approximate Inference.

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. Chapter 21: Variational Inference.

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Chapter 20: Deep Generative Models.
@[card](https://www.deeplearningbook.org/)

---

## è¨˜æ³•è¦ç´„

æœ¬è¬›ç¾©ã‚·ãƒªãƒ¼ã‚ºã§ä½¿ç”¨ã™ã‚‹æ•°å­¦è¨˜æ³•ã®çµ±ä¸€ãƒ«ãƒ¼ãƒ«:

| è¨˜å· | æ„å‘³ | èª­ã¿æ–¹ | ä¾‹ |
|:-----|:-----|:------|:---|
| $x$ | ãƒ‡ãƒ¼ã‚¿ï¼ˆè¦³æ¸¬å¤‰æ•°ï¼‰ | ã‚¨ãƒƒã‚¯ã‚¹ | $x \in \mathbb{R}^{784}$ |
| $z$ | æ½œåœ¨å¤‰æ•° | ã‚¼ãƒƒãƒˆ | $z \in \mathbb{R}^{20}$ |
| $\theta$ | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆDecoderï¼‰ | ã‚·ãƒ¼ã‚¿ | $p_\theta(x \mid z)$ |
| $\phi$ | å¤‰åˆ†åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆEncoderï¼‰ | ãƒ•ã‚¡ã‚¤ | $q_\phi(z \mid x)$ |
| $\mu, \sigma$ | å¹³å‡ã€æ¨™æº–åå·® | ãƒŸãƒ¥ãƒ¼ã€ã‚·ã‚°ãƒ | $\mathcal{N}(\mu, \sigma^2)$ |
| $\epsilon$ | ãƒã‚¤ã‚ºå¤‰æ•° | ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ | $\epsilon \sim \mathcal{N}(0, I)$ |
| $p(x)$ | çœŸã®åˆ†å¸ƒ | ãƒ”ãƒ¼ | $p(x) = \int p(x, z) dz$ |
| $q(z \mid x)$ | å¤‰åˆ†åˆ†å¸ƒï¼ˆè¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒï¼‰ | ã‚­ãƒ¥ãƒ¼ | $q_\phi(z \mid x)$ |
| $\mathbb{E}_{q}[\cdot]$ | $q$ ã®ä¸‹ã§ã®æœŸå¾…å€¤ | ã‚¤ãƒ¼ ã‚µãƒ– ã‚­ãƒ¥ãƒ¼ | $\mathbb{E}_{q(z)}[f(z)]$ |
| $D_\text{KL}(q \| p)$ | KLç™ºæ•£ | ãƒ‡ã‚£ãƒ¼ ã‚±ãƒ¼ã‚¨ãƒ« | $D_\text{KL}(q \| p) = \mathbb{E}_q[\log q - \log p]$ |
| $\mathcal{L}(\theta, \phi)$ | ELBOï¼ˆæå¤±é–¢æ•°ï¼‰ | ã‚¨ãƒ« ã‚·ãƒ¼ã‚¿ ãƒ•ã‚¡ã‚¤ | $\mathcal{L} = \mathbb{E}_q[\log p] - D_\text{KL}(q \| p)$ |
| $\nabla_\theta$ | $\theta$ ã«é–¢ã™ã‚‹å‹¾é… | ãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ | $\nabla_\theta \mathcal{L}$ |
| $\odot$ | è¦ç´ ã”ã¨ã®ç©ï¼ˆHadamardç©ï¼‰ | Hadamard product | $z = \mu + \sigma \odot \epsilon$ |
| $\|x\|$ | ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ãƒãƒ«ãƒ  | ãƒãƒ«ãƒ  | $\|x\|^2 = \sum x_i^2$ |

**Juliaè¨˜æ³•ã¨ã®å¯¾å¿œ**:
- `Î¼` (U+03BC), `Ïƒ` (U+03C3), `Î¸` (U+03B8), `Ï†` (U+03C6), `Îµ` (U+03B5) â€” Juliaã§ã¯å¤‰æ•°åã«ã‚®ãƒªã‚·ãƒ£æ–‡å­—ã‚’ä½¿ãˆã‚‹
- `.` â€” broadcastæ¼”ç®—å­ï¼ˆè¦ç´ ã”ã¨é©ç”¨ï¼‰
- `.*` â€” è¦ç´ ã”ã¨ã®ç©ï¼ˆ$\odot$ ã«å¯¾å¿œï¼‰

---

**EOF**

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
