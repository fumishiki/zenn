---
title: "ç¬¬36å›: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«åŸºç¤ / DDPM & ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning", "deeplearning", "ddpm", "rust", "diffusion"]
published: true
slug: "ml-lecture-36-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rustè¨“ç·´ + Rustæ¨è«–

### 4.1 ç’°å¢ƒæ§‹ç¯‰ & ãƒ©ã‚¤ãƒ–ãƒ©ãƒªé¸å®š

**Rustç’°å¢ƒ**:

```rust
// Cargo.toml [dependencies] â€” Rust è¨“ç·´ç’°å¢ƒ:
// candle-core = { version = "0.8", features = ["cuda"] }
// candle-nn   = "0.8"
// ndarray     = "0.15"
// ndarray-rand = "0.14"
// image       = "0.25"
```

**Rustç’°å¢ƒ** (æ¨è«–):

```toml
# Cargo.toml
[dependencies]
ndarray = "0.15"
ort = "2.0"  # ONNX Runtime
image = "0.25"
```

### 4.2 Tiny DDPM Rustå®Ÿè£… (è¨“ç·´ãƒ«ãƒ¼ãƒ—å®Œå…¨ç‰ˆ)

**ç›®æ¨™**: MNIST ã§ 500K paramsã€CPU 5åˆ†ã§è¨“ç·´ã€‚

#### 4.2.1 Noise Schedule

```rust
use std::f32::consts::PI;

/// Cosine noise schedule (Nichol & Dhariwal 2021).
/// f(t) = cosÂ²(Ï€/2 Â· (t/T + s)/(1+s)),  á¾±â‚œ = f(t)/f(0),  Î±â‚œ = á¾±â‚œ/á¾±â‚œâ‚‹â‚,  Î²â‚œ = 1-Î±â‚œ
/// Returns (beta, alpha, alpha_bar) each of length `t_steps`.
fn cosine_schedule(t_steps: usize, s: f32) -> (Vec<f32>, Vec<f32>, Vec<f32>) {
    // f(t) = cosÂ²(Ï€/2 Â· (t/T + s)/(1+s))
    let f: Vec<f32> = (0..=t_steps)
        .map(|t| {
            let ratio = t as f32 / t_steps as f32;
            ((ratio + s) / (1.0 + s) * PI / 2.0).cos().powi(2)
        })
        .collect();
    let alpha_bar: Vec<f32> = f[1..].iter().map(|&ft| ft / f[0]).collect(); // á¾±â‚œ = f(t)/f(0)
    let mut alpha = vec![alpha_bar[0]; t_steps];
    for i in 1..t_steps {
        alpha[i] = alpha_bar[i] / alpha_bar[i - 1]; // Î±â‚œ = á¾±â‚œ / á¾±â‚œâ‚‹â‚
    }
    let beta: Vec<f32> = alpha.iter().map(|&a| 1.0 - a).collect(); // Î²â‚œ = 1 - Î±â‚œ
    (beta, alpha, alpha_bar)
}

fn main() {
    let (beta, _alpha, alpha_bar) = cosine_schedule(1000, 0.008);
    let beta_min = beta.iter().cloned().fold(f32::INFINITY, f32::min);
    let beta_max = beta.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
    println!("Î² range: [{}, {}]", beta_min, beta_max);
    println!("á¾±_T = {}", alpha_bar.last().unwrap()); // Should be â‰ˆ 0
}
```

#### 4.2.2 Simplified U-Net (Tinyç‰ˆ)

```rust
use candle_core::{Tensor, Device, DType};
use candle_nn::{Conv2d, ConvTranspose2d, Linear, Module, VarBuilder};

/// Sinusoidal time embedding: returns Vec of length `d`.
fn time_embedding(t: usize, d: usize) -> Vec<f32> {
    let half = d / 2;
    let log_scale = (10000.0f32).ln() / (half - 1) as f32;
    (0..half)
        .flat_map(|i| {
            let freq = (-(log_scale * i as f32)).exp();
            let val  = t as f32 * freq;
            [val.sin(), val.cos()]
        })
        .collect()
}

/// Tiny U-Net for MNIST 28Ã—28 (~500K params).
struct TinyUNet {
    time_fc1:   Linear,
    time_fc2:   Linear,
    enc1_conv:  Conv2d,
    enc2_conv:  Conv2d,
    bottleneck: Conv2d,
    dec1_conv:  ConvTranspose2d,
    out_conv:   Conv2d,
}

impl TinyUNet {
    fn new(vb: VarBuilder, d_model: usize) -> candle_core::Result<Self> {
        let t_dim = 128usize;
        let cfg2  = candle_nn::Conv2dConfig       { padding: 1, ..Default::default() };
        let cfg2s = candle_nn::Conv2dConfig       { padding: 1, stride: 2, ..Default::default() };
        let cfgT  = candle_nn::ConvTranspose2dConfig { padding: 1, stride: 2, ..Default::default() };
        Ok(Self {
            time_fc1:   candle_nn::linear(t_dim,        d_model * 4, vb.pp("time_fc1"))?,
            time_fc2:   candle_nn::linear(d_model * 4,  d_model * 4, vb.pp("time_fc2"))?,
            enc1_conv:  candle_nn::conv2d(1,             d_model,     3, cfg2,  vb.pp("enc1"))?,
            enc2_conv:  candle_nn::conv2d(d_model,       d_model * 2, 3, cfg2s, vb.pp("enc2"))?,
            bottleneck: candle_nn::conv2d(d_model * 2,   d_model * 2, 3, cfg2,  vb.pp("btn"))?,
            dec1_conv:  candle_nn::conv_transpose2d(d_model * 4, d_model, 4, cfgT, vb.pp("dec1"))?,
            out_conv:   candle_nn::conv2d(d_model, 1, 3, cfg2, vb.pp("out"))?,
        })
    }

    fn forward(&self, x: &Tensor, t: usize) -> candle_core::Result<Tensor> {
        let dev = x.device();
        // Time embedding â†’ (1, d*4)
        let t_vec = time_embedding(t, 128);
        let t_emb = Tensor::from_vec(t_vec, (1, 128), dev)?;
        let t_emb = candle_nn::ops::silu(&self.time_fc1.forward(&t_emb)?)?;
        let _t_emb = self.time_fc2.forward(&t_emb)?;

        // Encoder
        let h1 = candle_nn::ops::silu(&self.enc1_conv.forward(x)?)?;   // (B, d, 28, 28)
        let h2 = candle_nn::ops::silu(&self.enc2_conv.forward(&h1)?)?;  // (B, d*2, 14, 14)

        // Bottleneck
        let h  = candle_nn::ops::silu(&self.bottleneck.forward(&h2)?)?; // (B, d*2, 14, 14)

        // Decoder with skip connection
        let h_cat = Tensor::cat(&[&h, &h2], 1)?;                         // (B, d*4, 14, 14)
        let h = candle_nn::ops::silu(&self.dec1_conv.forward(&h_cat)?)?; // (B, d,   28, 28)

        self.out_conv.forward(&h) // (B, 1, 28, 28)
    }
}
```

<details><summary>å®Œå…¨ãªU-Netå®Ÿè£… (Self-Attentionä»˜ã)</summary>

æœ¬æ ¼çš„ãªU-Netã«ã¯16Ã—16è§£åƒåº¦ã§Self-Attentionã‚’è¿½åŠ ã™ã‚‹ã€‚ä»¥ä¸‹ã¯å®Œå…¨ç‰ˆ (MNIST ã§ã¯éå‰°):

```rust
use candle_core::Tensor;

/// Multi-Head Self-Attention layer.
/// x: Tensor of shape (B, C, H, W) â€” applied at low-resolution feature maps.
struct SelfAttention {
    heads:   usize,
    d_model: usize,
}

impl SelfAttention {
    fn forward(&self, x: &Tensor) -> candle_core::Result<Tensor> {
        let (b, c, h, w) = x.dims4()?;
        assert_eq!(c % self.heads, 0, "C must be divisible by heads");
        let n = h * w;

        // Reshape to (B, N, C) for attention
        let x_flat = x.reshape((b, c, n))?.transpose(1, 2)?; // (B, N, C)

        // Simplified: Q = K = V = x_flat (identity projection for demo)
        let scale = (c / self.heads) as f64;
        let scores = x_flat.matmul(&x_flat.transpose(1, 2)?)? / scale; // (B, N, N)
        let attn   = candle_nn::ops::softmax(&scores, candle_core::D::Minus1)?;
        let out    = attn.matmul(&x_flat)?;                             // (B, N, C)

        // Reshape back to (B, C, H, W) and add residual
        let out = out.transpose(1, 2)?.reshape((b, c, h, w))?;
        out + x // Residual connection
    }
}
```

</details>

#### 4.2.3 è¨“ç·´ãƒ«ãƒ¼ãƒ—

```rust
use candle_core::Tensor;
use candle_nn::{AdamW, Optimizer, ParamsAdamW};

/// Single training step; returns MSE loss scalar.
fn train_step(
    model:      &TinyUNet,
    opt:        &mut AdamW,
    x0:         &Tensor,
    alpha_bar:  &[f32],
    t_steps:    usize,
    rng:        &mut impl rand::Rng,
) -> candle_core::Result<f32> {
    let t    = rng.gen_range(0..t_steps);
    let ab_t = alpha_bar[t];

    // q(xâ‚œ|xâ‚€) = N(âˆšá¾±â‚œÂ·xâ‚€, (1-á¾±â‚œ)Â·I)  â†’  xâ‚œ = âˆšá¾±â‚œÂ·xâ‚€ + âˆš(1-á¾±â‚œ)Â·Îµ
    let eps = Tensor::randn(0f32, 1f32, x0.shape(), x0.device())?;
    let x_t = (x0 * ab_t.sqrt() as f64 + &eps * (1.0 - ab_t).sqrt() as f64)?;

    // L_simple = E[||Îµ - Îµ_Î¸(xâ‚œ, t)||Â²]  (Ho et al. 2020)
    let eps_pred = model.forward(&x_t, t)?;
    let loss     = (&eps - &eps_pred)?.sqr()?.mean_all()?;
    opt.backward_step(&loss)?;
    loss.to_scalar::<f32>()
}

/// Full training loop.
fn train_ddpm(
    model:      &TinyUNet,
    train_data: &[Tensor],
    alpha_bar:  &[f32],
    beta:       &[f32],
    t_steps:    usize,
    epochs:     usize,
    lr:         f64,
) -> candle_core::Result<()> {
    let params  = ParamsAdamW { lr, ..Default::default() };
    let mut opt = AdamW::new(vec![], params)?;
    let mut rng = rand::thread_rng();

    for epoch in 0..epochs {
        let mut total_loss = 0.0f32;
        for (batch_idx, x0) in train_data.iter().enumerate() {
            let loss = train_step(model, &mut opt, x0, alpha_bar, t_steps, &mut rng)?;
            total_loss += loss;
            if batch_idx % 100 == 0 {
                println!("Epoch {}, Batch {}, Loss: {:.4}", epoch + 1, batch_idx, loss);
            }
        }
        let avg = total_loss / train_data.len() as f32;
        println!("Epoch {} completed. Avg Loss: {:.4}", epoch + 1, avg);
    }
    Ok(())
}
```

#### 4.2.4 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (DDPM & DDIM)

```rust
use candle_core::{Tensor, Device};

/// DDPM sampling: stochastic reverse process from x_T â†’ x_0.
fn ddpm_sample(
    model:      &TinyUNet,
    x_t_init:  Tensor,
    beta:       &[f32],
    alpha:      &[f32],
    alpha_bar:  &[f32],
    t_steps:    usize,
) -> candle_core::Result<Tensor> {
    let mut x_t = x_t_init;
    let dev = x_t.device().clone();

    for t in (0..t_steps).rev() {
        // p_Î¸(x_{t-1}|xâ‚œ) = N(Î¼_Î¸(xâ‚œ,t), Ïƒâ‚œÂ²Â·I)
        let eps_pred = model.forward(&x_t, t)?;
        // Î¼_Î¸ = (xâ‚œ âˆ’ Î²â‚œ/âˆš(1-á¾±â‚œ)Â·Îµ_Î¸) / âˆšÎ±â‚œ
        let coeff = beta[t] / (1.0 - alpha_bar[t]).sqrt();
        let mu    = ((&x_t - &eps_pred * coeff as f64)? / alpha[t].sqrt() as f64)?;

        x_t = if t > 0 {
            let sigma = beta[t].sqrt(); // Ïƒâ‚œ = âˆšÎ²â‚œ
            let z = Tensor::randn(0f32, 1f32, x_t.shape(), &dev)?;
            (mu + z * sigma as f64)? // x_{t-1} = Î¼_Î¸ + Ïƒâ‚œÂ·z
        } else {
            mu
        };
    }
    Ok(x_t)
}

/// DDIM sampling: accelerated deterministic (Î·=0) or stochastic (Î·=1) reverse.
fn ddim_sample(
    model:     &TinyUNet,
    x_t_init:  Tensor,
    alpha_bar: &[f32],
    steps:     usize,
    eta:       f32,
) -> candle_core::Result<Tensor> {
    let total = alpha_bar.len();
    // Sub-sequence Ï„: `steps` indices spread across [0, total)
    let tau: Vec<usize> = (0..steps)
        .map(|i| (i * total / steps).min(total - 1))
        .collect();
    let mut x_t = x_t_init;
    let dev = x_t.device().clone();

    for i in (1..tau.len()).rev() {
        let (t, t_prev) = (tau[i], tau[i - 1]);
        let (ab_t, ab_prev) = (alpha_bar[t], alpha_bar[t_prev]);

        let eps_pred = model.forward(&x_t, t)?;

        // xÌ‚â‚€ = (xâ‚œ - âˆš(1-á¾±â‚œ)Â·Îµ_Î¸) / âˆšá¾±â‚œ
        let x0_pred = ((&x_t - &eps_pred * (1.0 - ab_t).sqrt() as f64)? / ab_t.sqrt() as f64)?;

        // Ïƒâ‚œ(Î·) = Î·Â·âˆš((1-á¾±_{t-1})/(1-á¾±â‚œ))Â·âˆš(1 - á¾±â‚œ/á¾±_{t-1})  (Î·=0 â†’ deterministic)
        let sigma_t   = eta * ((1.0 - ab_prev) / (1.0 - ab_t)).sqrt()
                            * (1.0 - ab_t / ab_prev).sqrt();
        let dir_coeff = (1.0 - ab_prev - sigma_t * sigma_t).sqrt();
        let dir_xt    = (&eps_pred * dir_coeff as f64)?; // âˆš(1-á¾±_{t-1}-Ïƒâ‚œÂ²)Â·Îµ_Î¸

        x_t = if eta > 0.0 {
            let noise = Tensor::randn(0f32, 1f32, x_t.shape(), &dev)?;
            (x0_pred * ab_prev.sqrt() as f64 + dir_xt + noise * sigma_t as f64)?
        } else {
            (x0_pred * ab_prev.sqrt() as f64 + dir_xt)?
        };
    }

    // Final step: t = Ï„[0] â†’ x_0
    let t0   = tau[0];
    let ab0  = alpha_bar[t0];
    let eps_pred = model.forward(&x_t, t0)?;
    let x0 = ((&x_t - &eps_pred * (1.0 - ab0).sqrt() as f64)? / ab0.sqrt() as f64)?;
    Ok(x0)
}
```

### 4.3 ğŸ¦€ Rustæ¨è«–å®Ÿè£… (DDIMé«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)

**Rustå®Ÿè£…** ã¯è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« (ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ) ã‚’èª­ã¿è¾¼ã¿ã€DDIM ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’é«˜é€Ÿå®Ÿè¡Œã€‚

#### 4.3.1 Rustå´ã‚³ãƒ¼ãƒ‰

```rust
// src/ddim.rs
use ndarray::{Array4, s};
use ort::{Session, Value};

pub struct DDIMSampler {
    session: Session,
    alpha_bar: Vec<f32>,
    steps: usize,
}

type Result<T> = std::result::Result<T, Box<dyn std::error::Error>>;

impl DDIMSampler {
    pub fn new(model_path: &str, alpha_bar: Vec<f32>, steps: usize) -> Result<Self> {
        let session = Session::builder()?
            .with_model_from_file(model_path)?;
        Ok(Self { session, alpha_bar, steps })
    }

    pub fn sample(&self, x_t: Array4<f32>, eta: f32) -> Result<Array4<f32>> {
        let n = self.alpha_bar.len();
        let tau: Vec<usize> = (0..self.steps)
            .map(|i| (i * n / self.steps).min(n - 1))
            .collect();

        let mut x = x_t;

        for i in (1..tau.len()).rev() {
            let (t, t_prev) = (tau[i], tau[i - 1]);

            // Predict noise via ONNX model
            let Îµ_pred = self.predict_noise(&x, t)?;

            // DDIM step
            x = self.ddim_step(x, Îµ_pred, t, t_prev, eta);
        }

        // Final step
        let Îµ_pred = self.predict_noise(&x, tau[0])?;
        let á¾±_t = self.alpha_bar[tau[0]];
        let x_0 = (&x - (1.0 - á¾±_t).sqrt() * &Îµ_pred) / á¾±_t.sqrt();

        Ok(x_0)
    }

    fn predict_noise(&self, x_t: &Array4<f32>, t: usize) -> Result<Array4<f32>> {
        // Convert to ONNX input
        let x_input = Value::from_array(x_t.view())?;
        let t_input = Value::from_array(ndarray::arr0(t as f32).view())?;

        // Run inference
        let outputs = self.session.run(vec![x_input, t_input])?;
        let Îµ = outputs[0].try_extract_tensor::<f32>()?;

        Ok(Îµ.to_owned().into_dimensionality()?)
    }

    fn ddim_step(&self, x_t: Array4<f32>, Îµ: Array4<f32>, t: usize, t_prev: usize, Î·: f32) -> Array4<f32> {
        let (á¾±_t, á¾±_prev) = (self.alpha_bar[t], self.alpha_bar[t_prev]);

        // Predicted x_0
        let x_0_pred = (&x_t - (1.0 - á¾±_t).sqrt() * &Îµ) / á¾±_t.sqrt();

        // Variance
        let Ïƒ_t = Î· * ((1.0 - á¾±_prev) / (1.0 - á¾±_t)).sqrt()
            * (1.0 - á¾±_t / á¾±_prev).sqrt();

        // Direction + DDIM step
        let dir_xt = (1.0 - á¾±_prev - Ïƒ_t.powi(2)).sqrt() * &Îµ;
        á¾±_prev.sqrt() * x_0_pred + dir_xt
    }
}
```

#### 4.3.2 ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (Rust â†’ ONNX)

```rust
// Export via tract-onnx or burn's ONNX export.
// For candle, serialize weights with safetensors then convert with a Python script:
//   candle_core::safetensors::save(&weights_map, "tiny_ddpm.safetensors")
//   burn::export::to_onnx(&model_record, filepath)

fn export_to_onnx(var_map: &candle_nn::VarMap, filepath: &str) -> candle_core::Result<()> {
    let path = std::path::Path::new(filepath).with_extension("safetensors");
    var_map.save(&path)?;
    println!("Model exported to {}", path.display());
    Ok(())
}

fn main() -> candle_core::Result<()> {
    let var_map = candle_nn::VarMap::new();
    // ... (train model) ...
    export_to_onnx(&var_map, "tiny_ddpm.onnx")
}
```

#### 4.3.3 Rustå®Ÿè¡Œ

```rust
// src/main.rs
use ndarray::Array4;
use ndarray_rand::RandomExt;
use ndarray_rand::rand_distr::StandardNormal;

mod ddim;

fn main() {
    // Load alpha_bar schedule
    let alpha_bar: Vec<f32> = load_alpha_bar_from_file("alpha_bar.json");

    // Create sampler
    let sampler = ddim::DDIMSampler::new("tiny_ddpm.onnx", alpha_bar, 50).unwrap();

    // Sample from noise
    let x_T = Array4::random((1, 1, 28, 28), StandardNormal);
    let x_0 = sampler.sample(x_T, 0.0).unwrap();  // Deterministic (Î·=0)

    println!("Generated image shape: {:?}", x_0.shape());
    save_image(&x_0, "generated.png");
}

fn load_alpha_bar_from_file(path: &str) -> Vec<f32> {
    // Load from JSON (implementation omitted for brevity)
    vec![0.999, 0.998, /* ... */, 0.001]
}

fn save_image(x: &Array4<f32>, path: &str) {
    // Convert to image and save (implementation omitted)
}
```

### 4.4 Math â†’ Code 1:1å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³

| æ•°å¼ | Rust | Rust |
|:-----|:------|:-----|
| $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}$ | `x_t = sqrt(á¾±[t]) .* xâ‚€ .+ sqrt(1 - á¾±[t]) .* Îµ` | `x_t = alpha_bar_t.sqrt() * x_0 + (1.0 - alpha_bar_t).sqrt() * epsilon` |
| $\boldsymbol{\mu}_\theta = \frac{1}{\sqrt{\alpha_t}} (\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta)$ | `Î¼ = (1 / sqrt(Î±[t])) .* (x_t .- (Î²[t] / sqrt(1 - á¾±[t])) .* Îµ_pred)` | `mu = (x_t - (beta_t / (1.0 - alpha_bar_t).sqrt()) * epsilon_pred) / alpha_t.sqrt()` |
| $\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}} \boldsymbol{\epsilon}_\theta$ | `x_prev = sqrt(á¾±[t_prev]) .* xâ‚€_pred .+ sqrt(1 - á¾±[t_prev]) .* Îµ_pred` | `x_prev = alpha_bar_prev.sqrt() * x_0_pred + (1.0 - alpha_bar_prev).sqrt() * epsilon_pred` |

> **Note:** **é€²æ—: 70% å®Œäº†** Rustè¨“ç·´ + Rustæ¨è«–ã®å®Ÿè£…å®Œäº†ã€‚Zone 5ã§å®Ÿé¨“ã¸ã€‚

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” Tiny DDPM on MNIST

### 5.1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ (MNIST)

```rust
use candle_core::{Tensor, Device};

/// Load MNIST, normalize to [-1, 1], and return batched Tensors of shape (B, 1, 28, 28).
fn load_mnist_batched(batch_size: usize, device: &Device) -> candle_core::Result<Vec<Tensor>> {
    // Uses the `mnist` crate: https://crates.io/crates/mnist
    let mnist::Mnist { trn_img, .. } =
        mnist::MnistBuilder::new().label_format_digit().finalize();

    // Normalize pixel values [0, 255] â†’ [-1.0, 1.0]
    let data: Vec<f32> = trn_img.iter()
        .map(|&p| p as f32 / 127.5 - 1.0)
        .collect();

    let n = data.len() / (28 * 28); // 60 000 samples
    let all = Tensor::from_vec(data, (n, 1, 28, 28), device)?;
    println!("Training samples: {}", n);

    // Split into mini-batches
    (0..n / batch_size)
        .map(|i| all.narrow(0, i * batch_size, batch_size))
        .collect()
}
```

### 5.2 è¨“ç·´å®Ÿè¡Œ (CPU 5åˆ†)

```rust
use candle_core::Device;
use candle_nn::VarMap;

fn main() -> candle_core::Result<()> {
    let dev     = Device::Cpu;
    let var_map = VarMap::new();
    let vb      = candle_nn::VarBuilder::from_varmap(&var_map, candle_core::DType::F32, &dev);

    // Initialize model
    let model = TinyUNet::new(vb, 64)?;

    // Noise schedule
    let t_steps = 1000usize;
    let (beta, alpha, alpha_bar) = cosine_schedule(t_steps, 0.008);

    // Load data and train
    let train_batches = load_mnist_batched(128, &dev)?;
    train_ddpm(&model, &train_batches, &alpha_bar, &beta, t_steps, 10, 1e-3)?;

    println!("Training completed!");
    Ok(())
}
```

**Expected output**:
```
Epoch 1, Batch 100, Loss: 0.523
...
Epoch 10 completed. Avg Loss: 0.089
Training completed!
```

### 5.3 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° & å¯è¦–åŒ–

```rust
use candle_core::{Tensor, Device};

/// Print pixel statistics for a batch of samples (shape: NÃ—1Ã—HÃ—W, range [-1, 1]).
fn print_sample_stats(samples: &Tensor, label: &str) -> candle_core::Result<()> {
    let data = samples.flatten_all()?.to_vec1::<f32>()?;
    let min  = data.iter().cloned().fold(f32::INFINITY, f32::min);
    let max  = data.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
    let mean = data.iter().sum::<f32>() / data.len() as f32;
    println!("{}: min={:.3}, max={:.3}, mean={:.3}", label, min, max, mean);
    Ok(())
}

fn main() -> candle_core::Result<()> {
    let dev = Device::Cpu;

    // Sample 16 images (DDPM 1000 steps)
    let x_t = Tensor::randn(0f32, 1f32, (16, 1, 28, 28), &dev)?;
    let samples_ddpm = ddpm_sample(&model, x_t.clone(), &beta, &alpha, &alpha_bar, 1000)?;

    // Sample 16 images (DDIM 50 steps, deterministic Î·=0)
    let samples_ddim = ddim_sample(&model, x_t, &alpha_bar, 50, 0.0)?;

    // Print statistics; use `image` crate to save PNGs for visual inspection
    print_sample_stats(&samples_ddpm, "DDPM (1000 steps)")?;
    print_sample_stats(&samples_ddim, "DDIM (50 steps, deterministic)")?;
    Ok(())
}
```

### 5.4 å®šé‡è©•ä¾¡ & æ¯”è¼ƒ

**FID (FrÃ©chet Inception Distance)** ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ã„ãŸã‚ã€ç°¡æ˜“çš„ãª **å†æ§‹æˆèª¤å·®** ã¨ **å¤šæ§˜æ€§** ã‚’æ¸¬å®š:

```rust
use candle_core::Tensor;

/// Encode x_0 to x_t (t=500), denoise back with DDIM, and return MSE.
fn test_reconstruction(
    model:     &TinyUNet,
    x0:        &Tensor,
    alpha_bar: &[f32],
    t:         usize,
) -> candle_core::Result<f32> {
    let ab_t = alpha_bar[t];

    // q(xâ‚œ|xâ‚€) = N(âˆšá¾±â‚œÂ·xâ‚€, (1-á¾±â‚œ)Â·I)  â†’  xâ‚œ = âˆšá¾±â‚œÂ·xâ‚€ + âˆš(1-á¾±â‚œ)Â·Îµ
    let eps = Tensor::randn(0f32, 1f32, x0.shape(), x0.device())?;
    let x_t = (x0 * ab_t.sqrt() as f64 + &eps * (1.0 - ab_t).sqrt() as f64)?;

    // Denoise with DDIM (sub-schedule up to t)
    let x_recon = ddim_sample(model, x_t, &alpha_bar[..=t], 50, 0.0)?;

    // MSE
    (x0 - &x_recon)?.sqr()?.mean_all()?.to_scalar::<f32>()
}

fn main() -> candle_core::Result<()> {
    // Test on 100 samples
    let mse_sum: f32 = (0..100)
        .map(|i| {
            let x0 = test_data.narrow(0, i, 1).unwrap();
            test_reconstruction(&model, &x0, &alpha_bar, 500).unwrap()
        })
        .sum();
    println!("Average reconstruction MSE: {:.4}", mse_sum / 100.0);
    Ok(())
}
```

**aMUSEd-256 æ¨è«–ãƒ‡ãƒ¢ã¨ã®å“è³ªæ¯”è¼ƒ**:

aMUSEd-256 [Hugging Face](https://huggingface.co/amused/amused-256) ã¯éæ‹¡æ•£ãƒ¢ãƒ‡ãƒ« (Masked Image Modeling) ã§256Ã—256ç”»åƒã‚’ç”Ÿæˆã€‚

| ãƒ¢ãƒ‡ãƒ« | è§£åƒåº¦ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | è¨“ç·´æ™‚é–“ (CPU) | å“è³ª (ä¸»è¦³) |
|:-------|:-------|:----------|:--------------|:-----------|
| **Tiny DDPM (æœ¬å®Ÿè£…)** | 28Ã—28 | ~500K | 5åˆ† | MNISTæ•°å­—ã€ã‚·ãƒ£ãƒ¼ãƒ— |
| **aMUSEd-256** | 256Ã—256 | ~800M | N/A (äº‹å‰è¨“ç·´æ¸ˆã¿) | é«˜å“è³ªã€å¤šæ§˜ |

**çµè«–**: Tiny DDPMã¯ç†è«–å­¦ç¿’ç”¨ã€‚Productionå“è³ªã¯aMUSEd-256ã‚„Stable Diffusion (ç¬¬39å›) ã§å®Ÿç¾ã€‚

### 5.5 è¨“ç·´æ›²ç·šåˆ†æ & ãƒ‡ãƒãƒƒã‚°

**Lossæ›²ç·šã®å…¸å‹çš„ãƒ‘ã‚¿ãƒ¼ãƒ³**:

```rust
use std::f32::consts::PI;

/// Print training curves (epoch, loss, lr).
/// Use the `plotters` crate for a full chart.
fn print_training_curves(loss_history: &[f32], lr_schedule: &[f32]) {
    println!("{:<8} {:>10} {:>12}", "Epoch", "Loss", "LR");
    for (epoch, (&loss, &lr)) in loss_history.iter().zip(lr_schedule).enumerate() {
        println!("{:<8} {:>10.4} {:>12.6}", epoch + 1, loss, lr);
    }
    if let Some(&final_loss) = loss_history.last() {
        println!("Final loss target: 0.089 | actual: {:.4}", final_loss);
    }
}

fn main() {
    let loss_history = vec![0.523f32, 0.420, 0.350, 0.280, 0.220, 0.175, 0.140, 0.115, 0.099, 0.089];
    // Cosine LR decay: lr * cos(Ï€ * epoch / (2 * epochs))
    let lr_schedule: Vec<f32> = (0..=10)
        .map(|e| 1e-3 * (PI * e as f32 / 20.0).cos())
        .collect();
    print_training_curves(&loss_history, &lr_schedule);
}
```

**å…¸å‹çš„ãªå•é¡Œã¨å¯¾å‡¦**:

| ç—‡çŠ¶ | åŸå›  | å¯¾å‡¦ |
|:-----|:-----|:-----|
| Loss ãŒç™ºæ•£ (NaN) | Learning rate é«˜ã™ã | LR ã‚’ 1/10 ã«æ¸›ã‚‰ã™ |
| Loss ãŒä¸‹ãŒã‚‰ãªã„ | ãƒ¢ãƒ‡ãƒ«ãŒå°ã•ã™ã | d_model ã‚’ 64 â†’ 128 |
| ç”Ÿæˆç”»åƒãŒãƒã‚¤ã‚ºã®ã¿ | è¨“ç·´ä¸è¶³ | epochs ã‚’ 10 â†’ 50 |
| ç”Ÿæˆç”»åƒãŒå˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³ | Mode collapse | Batch size ã‚’ 128 â†’ 256 |

**è¨“ç·´å®‰å®šåŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯**:

```rust
use candle_core::Tensor;
use candle_nn::{AdamW, Optimizer};

/// Training step with gradient-norm clipping.
/// Returns (loss, grad_norm).
fn train_step_with_clip(
    model:     &TinyUNet,
    opt:       &mut AdamW,
    x0:        &Tensor,
    alpha_bar: &[f32],
    t_steps:   usize,
    clip_norm: f32,
    rng:       &mut impl rand::Rng,
) -> candle_core::Result<(f32, f32)> {
    let t    = rng.gen_range(0..t_steps);
    let ab_t = alpha_bar[t];

    // q(xâ‚œ|xâ‚€) = N(âˆšá¾±â‚œÂ·xâ‚€, (1-á¾±â‚œ)Â·I)  â†’  xâ‚œ = âˆšá¾±â‚œÂ·xâ‚€ + âˆš(1-á¾±â‚œ)Â·Îµ
    let eps  = Tensor::randn(0f32, 1f32, x0.shape(), x0.device())?;
    let x_t  = (x0 * ab_t.sqrt() as f64 + &eps * (1.0 - ab_t).sqrt() as f64)?;

    let eps_pred = model.forward(&x_t, t)?;
    let loss     = (&eps - &eps_pred)?.sqr()?.sum_all()?;

    // Compute per-parameter gradient norms for logging
    let grads = loss.backward()?;
    let grad_norm: f32 = grads.values()
        .filter_map(|g| g.sqr().ok()?.sum_all().ok()?.to_scalar::<f32>().ok())
        .sum::<f32>()
        .sqrt();

    // Clip by scaling gradients if norm exceeds threshold
    if grad_norm > clip_norm {
        let _scale = clip_norm / grad_norm; // apply scale to grads in production
    }
    opt.step(&grads)?;

    Ok((loss.to_scalar::<f32>()?, grad_norm))
}
```

**EMA (Exponential Moving Average) for Stable Inference**:

```rust
use std::collections::HashMap;
use candle_core::Tensor;

/// Exponential Moving Average of model weights for stable inference.
struct Ema {
    shadow: HashMap<String, Tensor>,
    decay:  f64,
}

impl Ema {
    fn new(var_map: &candle_nn::VarMap, decay: f64) -> candle_core::Result<Self> {
        let shadow = var_map
            .data()
            .lock()
            .unwrap()
            .iter()
            .map(|(k, v)| (k.clone(), v.as_tensor().clone()))
            .collect();
        Ok(Self { shadow, decay })
    }

    /// shadow = decay * shadow + (1 âˆ’ decay) * current
    fn update(&mut self, var_map: &candle_nn::VarMap) -> candle_core::Result<()> {
        for (name, current) in var_map.data().lock().unwrap().iter() {
            if let Some(shadow) = self.shadow.get_mut(name) {
                *shadow = (shadow.affine(self.decay, 0.0)?
                    + current.affine(1.0 - self.decay, 0.0)?)?;
            }
        }
        Ok(())
    }
}

fn main() -> candle_core::Result<()> {
    let var_map = candle_nn::VarMap::new();
    let mut ema = Ema::new(&var_map, 0.9999)?;

    for _epoch in 0..epochs {
        // ... train_step(...) ...
        ema.update(&var_map)?; // Update EMA after each batch
    }

    // Load EMA weights back into model for sampling
    println!("Using EMA weights for sampling");
    Ok(())
}
```

### 5.6 ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å“è³ªã®å®šé‡è©•ä¾¡

**FID (FrÃ©chet Inception Distance)** ã®å®Œå…¨å®Ÿè£…:

```rust
use candle_core::{Tensor, Device};
use candle_nn::{Conv2d, Linear, Module};

/// Lightweight CNN feature extractor for MNIST-scale FID computation.
struct SimpleFeatureExtractor {
    conv1: Conv2d,
    conv2: Conv2d,
    fc:    Linear,
}

impl SimpleFeatureExtractor {
    fn new(vb: candle_nn::VarBuilder) -> candle_core::Result<Self> {
        let cfg = candle_nn::Conv2dConfig { padding: 1, ..Default::default() };
        Ok(Self {
            conv1: candle_nn::conv2d(1,  32, 3, cfg, vb.pp("conv1"))?,
            conv2: candle_nn::conv2d(32, 64, 3, cfg, vb.pp("conv2"))?,
            fc:    candle_nn::linear(7 * 7 * 64, 256, vb.pp("fc"))?,
        })
    }

    fn forward(&self, x: &Tensor) -> candle_core::Result<Tensor> {
        let h = self.conv1.forward(x)?.relu()?.avg_pool2d(2)?;  // (B, 32, 14, 14)
        let h = self.conv2.forward(&h)?.relu()?.avg_pool2d(2)?; // (B, 64,  7,  7)
        let h = h.flatten_from(1)?;                              // (B, 3136)
        self.fc.forward(&h)                                      // (B, 256)
    }
}

/// Simplified FID: squared distance between feature means (full cov requires nalgebra).
fn compute_fid(real_feats: &[Vec<f32>], fake_feats: &[Vec<f32>]) -> f32 {
    let dim = real_feats[0].len();
    let mu_real: Vec<f32> = (0..dim)
        .map(|d| real_feats.iter().map(|f| f[d]).sum::<f32>() / real_feats.len() as f32)
        .collect();
    let mu_fake: Vec<f32> = (0..dim)
        .map(|d| fake_feats.iter().map(|f| f[d]).sum::<f32>() / fake_feats.len() as f32)
        .collect();
    mu_real.iter().zip(&mu_fake).map(|(a, b)| (a - b).powi(2)).sum()
}

fn main() -> candle_core::Result<()> {
    let dev     = Device::Cpu;
    let var_map = candle_nn::VarMap::new();
    let vb      = candle_nn::VarBuilder::from_varmap(&var_map, candle_core::DType::F32, &dev);
    let extractor = SimpleFeatureExtractor::new(vb)?;

    // Extract features from 1000 real and fake samples
    let extract = |batch: &[Tensor]| -> candle_core::Result<Vec<Vec<f32>>> {
        batch.iter()
            .map(|x| extractor.forward(x)?.to_vec2::<f32>().map(|v| v.into_iter().flatten().collect()))
            .collect()
    };
    let real_feats = extract(&real_batch)?;
    let fake_feats = extract(&fake_batch)?;

    println!("FID Score: {:.2}", compute_fid(&real_feats, &fake_feats));
    Ok(())
}
```

**Inception Score (IS)** ã®å®Ÿè£…:

```rust
/// Compute Inception Score from per-sample softmax predictions.
/// `pyx`: shape (N, num_classes), each row is p(y | x) for one sample.
fn compute_inception_score(pyx: &[Vec<f32>]) -> f32 {
    let n = pyx.len();
    let k = pyx[0].len();

    // Marginal distribution p(y) = mean over samples
    let py: Vec<f32> = (0..k)
        .map(|c| pyx.iter().map(|p| p[c]).sum::<f32>() / n as f32)
        .collect();

    // KL(p(y|x) || p(y)) per sample, then average
    let mean_kl: f32 = pyx.iter()
        .map(|p_given_x| {
            p_given_x.iter().zip(&py)
                .filter(|(&px, &py_c)| px > 0.0 && py_c > 0.0)
                .map(|(&px, &py_c)| px * (px.ln() - py_c.ln()))
                .sum::<f32>()
        })
        .sum::<f32>() / n as f32;

    mean_kl.exp() // IS = exp(E[KL])
}

fn main() -> candle_core::Result<()> {
    // mnist_classifier returns (N, 10) softmax probabilities
    let pyx: Vec<Vec<f32>> = fake_batch.iter()
        .map(|x| {
            mnist_classifier.forward(x)?
                .to_vec2::<f32>()
                .map(|v| v.into_iter().flatten().collect())
        })
        .collect::<candle_core::Result<_>>()?;

    println!("Inception Score: {:.2}", compute_inception_score(&pyx));
    Ok(())
}
```

**Expected results** (Tiny DDPM on MNIST after 50 epochs):

| Metric | Value | å‚™è€ƒ |
|:-------|:------|:-----|
| **FID** | 15-25 | Lower is better (Real = 0) |
| **IS** | 8-9 | Higher is better (Max = 10 for MNIST) |
| **Reconstruction MSE** | 0.01-0.03 | Lower is better |

### 5.7 ã‚¹ãƒ†ãƒƒãƒ—æ•° vs å“è³ªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

**å®Ÿé¨“**: DDPM ã¨ DDIM ã§ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§ã®ç”Ÿæˆå“è³ªã‚’æ¯”è¼ƒã€‚

```rust
use candle_core::{Tensor, Device};

fn main() -> candle_core::Result<()> {
    let dev         = Device::Cpu;
    let step_counts = [10usize, 20, 50, 100, 200, 500, 1000];
    let mut fid_ddpm = Vec::with_capacity(step_counts.len());
    let mut fid_ddim = Vec::with_capacity(step_counts.len());

    let x_t_base = Tensor::randn(0f32, 1f32, (16, 1, 28, 28), &dev)?;

    for &steps in &step_counts {
        // DDPM with `steps` uniform timesteps
        let samples_d = ddpm_sample(&model, x_t_base.clone(), &beta, &alpha, &alpha_bar, steps)?;
        let fid_d = compute_fid_from_tensors(&real_batch, &samples_d, &extractor)?;
        fid_ddpm.push(fid_d);

        // DDIM deterministic (Î· = 0)
        let samples_i = ddim_sample(&model, x_t_base.clone(), &alpha_bar, steps, 0.0)?;
        let fid_i = compute_fid_from_tensors(&real_batch, &samples_i, &extractor)?;
        fid_ddim.push(fid_i);

        println!("Steps: {:4},  FID (DDPM): {:.2},  FID (DDIM): {:.2}", steps, fid_d, fid_i);
    }

    // Summary table (use plotters crate for log-scale chart)
    println!("\n{:<8} {:>12} {:>12}", "Steps", "FID DDPM", "FID DDIM");
    for (&s, (&fd, &fi)) in step_counts.iter().zip(fid_ddpm.iter().zip(&fid_ddim)) {
        println!("{:<8} {:>12.2} {:>12.2}", s, fd, fi);
    }
    Ok(())
}
```

**Expected curve**:

```mermaid
graph LR
    A[10 steps: FID ~50] --> B[50 steps: FID ~20]
    B --> C[200 steps: FID ~15]
    C --> D[1000 steps: FID ~12]

    style A fill:#ff9999
    style B fill:#ffcc99
    style C fill:#99ccff
    style D fill:#99ff99
```

**çµè«–**:
- **DDPM**: 1000ã‚¹ãƒ†ãƒƒãƒ—ã§æœ€é«˜å“è³ª
- **DDIM**: 50ã‚¹ãƒ†ãƒƒãƒ—ã§ DDPM 200ã‚¹ãƒ†ãƒƒãƒ—ã¨åŒç­‰
- **é«˜é€Ÿç”Ÿæˆ**: DDIM Î·=0 (deterministic) ãŒæ¨è«–ã‚³ã‚¹ãƒˆæœ€å°

### 5.8 ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å½±éŸ¿å®Ÿé¨“

**å®Ÿé¨“**: Linear vs Cosine vs Zero Terminal SNR ã®3ã¤ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§è¨“ç·´ãƒ»æ¯”è¼ƒã€‚

```rust
/// Linear schedule: Î²â‚œ = Î²_start + (Î²_end - Î²_start)Â·t/(T-1)
/// Î±â‚œ = 1 - Î²â‚œ,  á¾±â‚œ = Î _{s=1}^t Î±â‚›
fn linear_schedule(t_steps: usize, beta_start: f32, beta_end: f32) -> (Vec<f32>, Vec<f32>, Vec<f32>) {
    // Î²â‚œ linearly from Î²_start to Î²_end
    let beta: Vec<f32> = (0..t_steps)
        .map(|t| beta_start + (beta_end - beta_start) * t as f32 / (t_steps - 1) as f32)
        .collect();
    let alpha: Vec<f32> = beta.iter().map(|&b| 1.0 - b).collect(); // Î±â‚œ = 1 - Î²â‚œ
    // á¾±â‚œ = Î _{s=1}^t Î±â‚›  (scan = cumulative product)
    let alpha_bar: Vec<f32> = alpha.iter()
        .scan(1.0f32, |acc, &a| { *acc *= a; Some(*acc) })
        .collect();
    (beta, alpha, alpha_bar)
}

/// Zero Terminal SNR (Lin+ 2023): rescale á¾±â‚œ so á¾±_T = 0 exactly.
/// á¾±â‚œ â† (á¾±â‚œ - á¾±_T) / (1 - á¾±_T)  fixes train/inference mismatch.
fn zero_terminal_snr_schedule(t_steps: usize) -> (Vec<f32>, Vec<f32>, Vec<f32>) {
    let (mut beta, mut alpha, mut alpha_bar) = linear_schedule(t_steps, 1e-4, 0.02);
    let ab_last = *alpha_bar.last().unwrap();
    // á¾±â‚œ â† (á¾±â‚œ âˆ’ á¾±_T) / (1 âˆ’ á¾±_T)  â†’ á¾±_T = 0
    alpha_bar.iter_mut().for_each(|ab| *ab = (*ab - ab_last) / (1.0 - ab_last));
    alpha[0] = alpha_bar[0];
    for i in 1..t_steps { alpha[i] = alpha_bar[i] / alpha_bar[i - 1]; } // Î±â‚œ = á¾±â‚œ/á¾±â‚œâ‚‹â‚
    beta.iter_mut().zip(&alpha).for_each(|(b, &a)| *b = 1.0 - a); // Î²â‚œ = 1 - Î±â‚œ
    (beta, alpha, alpha_bar)
}

fn main() -> candle_core::Result<()> {
    let t_steps = 1000usize;
    let (beta_linear, _, ab_linear) = linear_schedule(t_steps, 1e-4, 0.02);
    let (beta_cosine, _, ab_cosine) = cosine_schedule(t_steps, 0.008);
    let (beta_zt,     _, ab_zt)     = zero_terminal_snr_schedule(t_steps);

    // Train and evaluate each schedule (abbreviated)
    let fid_linear = evaluate_schedule(&model, &train_batches, &beta_linear, &ab_linear, 50)?;
    let fid_cosine = evaluate_schedule(&model, &train_batches, &beta_cosine, &ab_cosine, 50)?;
    let fid_zt     = evaluate_schedule(&model, &train_batches, &beta_zt,     &ab_zt,     50)?;

    println!("FID â€” Linear: {:.2}, Cosine: {:.2}, Zero-Terminal: {:.2}",
             fid_linear, fid_cosine, fid_zt);
    Ok(())
}
```

**Expected results**:

| Schedule | FID | è¨“ç·´å®‰å®šæ€§ | å‚™è€ƒ |
|:---------|:----|:----------|:-----|
| **Linear** | 25-30 | â­â­â­ | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã€‚$\bar{\alpha}_T > 0$ å•é¡Œ |
| **Cosine** | 15-20 | â­â­â­â­ | Improved DDPM [^3] ã§ææ¡ˆã€‚å®‰å®š |
| **Zero Terminal SNR** | 12-18 | â­â­â­â­â­ | è¨“ç·´/æ¨è«–ä¸ä¸€è‡´ã‚’è§£æ¶ˆ [^5] |

**å¯è¦–åŒ–**: SNRæ›²ç·šã‚’æ¯”è¼ƒ:

```rust
// log SNR(t) = log(á¾±â‚œ / (1-á¾±â‚œ)) â€” quantifies signal-to-noise at each timestep
fn log_snr(alpha_bar: &[f32]) -> Vec<f32> {
    alpha_bar.iter()
        .map(|&ab| (ab / (1.0 - ab)).ln()) // log(á¾±â‚œ/(1-á¾±â‚œ))
        .collect()
}

fn main() {
    let t_steps = 1000usize;
    let (_, _, ab_linear) = linear_schedule(t_steps, 1e-4, 0.02);
    let (_, _, ab_cosine) = cosine_schedule(t_steps, 0.008);
    let (_, _, ab_zt)     = zero_terminal_snr_schedule(t_steps);

    let snr_linear = log_snr(&ab_linear);
    let snr_cosine = log_snr(&ab_cosine);
    let snr_zt     = log_snr(&ab_zt);

    // Print sample; use plotters crate for a full chart
    println!("{:<12} {:>10} {:>10} {:>12}", "Timestep t", "Linear", "Cosine", "Zero-SNR");
    for t in (0..t_steps).step_by(100) {
        println!("{:<12} {:>10.3} {:>10.3} {:>12.3}",
                 t + 1, snr_linear[t], snr_cosine[t], snr_zt[t]);
    }
}
```

**é‡è¦ãªè¦³å¯Ÿ**:
- **Linear**: log(SNR) ãŒç·šå½¢æ¸›è¡°ã€‚çµ‚ç«¯ã§ SNR > 0 (å•é¡Œ)
- **Cosine**: log(SNR) ãŒç·©ã‚„ã‹ã«æ¸›è¡°ã€‚ä¸­é–“æ™‚åˆ»ã®SNRãŒé«˜ã„
- **Zero Terminal SNR**: log(SNR(T)) = -âˆ (å®Œå…¨ãƒã‚¤ã‚º)

### 5.5 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

<details><summary>Q1: Forward Process ã®é–‰å½¢å¼è§£ã‚’å°å‡ºã›ã‚ˆ</summary>

**å•é¡Œ**: $q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} \mathbf{x}_{t-1}, (1-\alpha_t) \mathbf{I})$ ã‹ã‚‰ã€$q(\mathbf{x}_t \mid \mathbf{x}_0)$ ã‚’å°å‡ºã›ã‚ˆã€‚

**è§£ç­”**: Section 3.1å‚ç…§ã€‚æ•°å­¦çš„å¸°ç´æ³•ã§è¨¼æ˜ã€‚çµæœ:

$$
q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I})
$$

</details>

<details><summary>Q2: Îµ-prediction ã¨ xâ‚€-prediction ã®å¤‰æ›å¼ã‚’ç¤ºã›</summary>

**å•é¡Œ**: $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}$ ã‹ã‚‰ã€$\mathbf{x}_0$ ã‚’ $\boldsymbol{\epsilon}$ ã§è¡¨ã›ã€‚

**è§£ç­”**:

$$
\mathbf{x}_0 = \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}}{\sqrt{\bar{\alpha}_t}}
$$

é€†ã« $\boldsymbol{\epsilon}$ ã‚’ $\mathbf{x}_0$ ã§è¡¨ã™ã¨:

$$
\boldsymbol{\epsilon} = \frac{\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0}{\sqrt{1-\bar{\alpha}_t}}
$$

</details>

<details><summary>Q3: DDIM ã®æ±ºå®šè«–æ€§ã‚’èª¬æ˜ã›ã‚ˆ</summary>

**å•é¡Œ**: DDIMãŒæ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã™ã‚‹ç†ç”±ã¯ï¼Ÿ

**è§£ç­”**: DDIM ã® $\eta = 0$ è¨­å®š:

$$
\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}} \boldsymbol{\epsilon}_\theta
$$

ãƒã‚¤ã‚ºé … $\sigma_t \boldsymbol{\epsilon}_t = 0$ â†’ åŒã˜ $\mathbf{x}_T$ ã‹ã‚‰å¸¸ã«åŒã˜ $\mathbf{x}_0$ ãŒç”Ÿæˆã•ã‚Œã‚‹ã€‚

</details>

<details><summary>Q4: VLB ã®3é …ã‚’èª¬æ˜ã›ã‚ˆ</summary>

**å•é¡Œ**: $L_\text{VLB} = L_T + \sum_{t=2}^T L_{t-1} + L_0$ ã®å„é …ã®æ„å‘³ã¯ï¼Ÿ

**è§£ç­”**:

- $L_T = D_\text{KL}(q(\mathbf{x}_T \mid \mathbf{x}_0) \| p(\mathbf{x}_T))$: æœ€çµ‚ãƒã‚¤ã‚ºãŒæ¨™æº–æ­£è¦åˆ†å¸ƒã«è¿‘ã„ã‹
- $L_{t-1} = D_\text{KL}(q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \| p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t))$: Reverse Processã®ç²¾åº¦
- $L_0 = -\log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1)$: å†æ§‹æˆé …

</details>

<details><summary>Q5: SNRè¦–ç‚¹ã§Noise Scheduleã‚’è©•ä¾¡ã›ã‚ˆ</summary>

**å•é¡Œ**: Linear schedule $\beta_t = 10^{-4} + (t-1)/(T-1) \cdot (0.02 - 10^{-4})$ ã®å•é¡Œç‚¹ã¯ï¼Ÿ

**è§£ç­”**: $\bar{\alpha}_T > 0$ â†’ SNR$(T) > 0$ (Zero Terminal SNR ã‚’æº€ãŸã•ãªã„ [^5])ã€‚è¨“ç·´ã¨æ¨è«–ã®ä¸ä¸€è‡´ãŒç”Ÿã˜ã‚‹ã€‚**è§£æ±ºç­–**: Cosine scheduleã¾ãŸã¯Rescalingã€‚

</details>

> **Note:** **é€²æ—: 85% å®Œäº†** å®Ÿé¨“å®Œäº†ã€‚è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆã§ç†è§£ã‚’ç¢ºèªã€‚Zone 6ã§ç™ºå±•ã¸ã€‚

---

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. DDPM Rustå®Ÿè£…ã§ç´¯ç©ç© $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$ ã‚’æ•°å€¤å®‰å®šã«è¨ˆç®—ã™ã‚‹ãŸã‚logç©ºé–“ã‚’ä½¿ã†ç†ç”±ã¨ã€ãã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿°ã¹ã‚ˆã€‚
> 2. DDIMæ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ$\eta=0$ï¼‰ã¨ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ$\eta=1$ï¼‰ã®é•ã„ã‚’ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œå¤‰æ•°ã¨æ•°å¼ã§ç¤ºã›ã€‚

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼æ¦‚è¦ (DPM-Solver++ / UniPC / EDM)

**DDIM** ã¯ Euleræ³• (1æ¬¡)ã€‚**é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼** ã¯ 2-3æ¬¡ã®ç²¾åº¦ã§ã€ã•ã‚‰ã«é«˜é€ŸåŒ–ã€‚

#### 6.1.1 DPM-Solver++ (Lu+ 2022 [^4])

**å‹•æ©Ÿ**: Diffusion ODE $\frac{d\mathbf{x}}{dt} = f(\mathbf{x}, t)$ ã‚’ **é«˜æ¬¡æ•°å€¤è§£æ³•** ã§è§£ãã€‚

**ã‚¢ã‚¤ãƒ‡ã‚¢**: $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ ã‚’å¤šé …å¼è¿‘ä¼¼ â†’ 2-3æ¬¡ã®ç²¾åº¦ã€‚

$$
\mathbf{x}_{t-\Delta t} = \mathbf{x}_t + \int_t^{t-\Delta t} \left( -\frac{1}{2\sigma_s} \sigma_s' \boldsymbol{\epsilon}_\theta(\mathbf{x}_s, s) \right) ds
$$

**Taylorå±•é–‹** ã§ $\boldsymbol{\epsilon}_\theta$ ã‚’è¿‘ä¼¼:

$$
\boldsymbol{\epsilon}_\theta(\mathbf{x}_s, s) \approx \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) + (s-t) \boldsymbol{\epsilon}_\theta'(\mathbf{x}_t, t)
$$

**çµæœ**: 10-20ã‚¹ãƒ†ãƒƒãƒ—ã§é«˜å“è³ªç”Ÿæˆã€‚

<details><summary>DPM-Solver++ å®Ÿè£… (Rust)</summary>

```rust
use candle_core::Tensor;

/// DPM-Solver++ 2nd-order step: t â†’ t_prev via Heun predictor-corrector.
/// Solves Diffusion ODE: dx/dt = -Â½Ïƒ'(t)/Ïƒ(t)Â·Îµ_Î¸(x, t)
fn dpm_solver_step(
    model:     &TinyUNet,
    x_t:       &Tensor,
    t:         usize,
    t_prev:    usize,
    alpha_bar: &[f32],
) -> candle_core::Result<Tensor> {
    let (ab_t, ab_prev) = (alpha_bar[t], alpha_bar[t_prev]);

    // xÌ‚â‚€ = (xâ‚œ - âˆš(1-á¾±â‚œ)Â·Îµ_Î¸) / âˆšá¾±â‚œ
    let eps_t  = model.forward(x_t, t)?;
    let x0_t   = ((x_t - &eps_t * (1.0 - ab_t).sqrt() as f64)? / ab_t.sqrt() as f64)?;

    // Predictor (Heun): half step to t_mid
    let t_mid  = (t + t_prev) / 2;
    let ab_mid = alpha_bar[t_mid];
    let x_mid  = (x0_t.affine(ab_mid.sqrt() as f64, 0.0)?
               +  eps_t.affine((1.0 - ab_mid).sqrt() as f64, 0.0)?)?;

    // Corrector: 2nd model call at t_mid
    let eps_mid = model.forward(&x_mid, t_mid)?;
    let x0_mid  = ((&x_mid - &eps_mid * (1.0 - ab_mid).sqrt() as f64)? / ab_mid.sqrt() as f64)?;

    // x_{t_prev} = âˆšá¾±_{t_prev}Â·xÌ‚â‚€_mid + âˆš(1-á¾±_{t_prev})Â·Îµ_mid
    let x_prev = (x0_mid.affine(ab_prev.sqrt() as f64, 0.0)?
               +  eps_mid.affine((1.0 - ab_prev).sqrt() as f64, 0.0)?)?;
    Ok(x_prev)
}
```

</details>

#### 6.1.2 UniPC (Zhao+ 2023)

**çµ±ä¸€äºˆæ¸¬å™¨-ä¿®æ­£å™¨ (Predictor-Corrector)** ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚

- **Predictor**: é«˜æ¬¡ã§ $\mathbf{x}_{t-1}$ ã‚’äºˆæ¸¬
- **Corrector**: äºˆæ¸¬å€¤ã‚’1å›æ”¹å–„

**æ€§èƒ½**: 5-10ã‚¹ãƒ†ãƒƒãƒ—ã§ DDIM 50ã‚¹ãƒ†ãƒƒãƒ—ã¨åŒç­‰ã€‚

#### 6.1.3 EDM (Karras+ 2022)

**Elucidating the Design Space of Diffusion Models** â€” ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä½“ç³»çš„æœ€é©åŒ–ã€‚

- **Noise Schedule**: Log-SNR ç©ºé–“ã§ uniform sampling
- **Loss Weighting**: $\lambda(t) = \text{SNR}(t) / (1 + \text{SNR}(t))$
- **Preconditioning**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‡ºåŠ›ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æœ€é©åŒ–

### 6.2 Improved DDPM (Nichol & Dhariwal 2021 [^3])

**æ”¹å–„ç‚¹**:

1. **å­¦ç¿’åˆ†æ•£ $\sigma_t^2$**: å›ºå®š â†’ å­¦ç¿’å¯èƒ½
2. **Cosine Schedule**: Linear â†’ Cosine
3. **Hybrid Loss**: $L_\text{VLB}$ ã¨ $L_\text{simple}$ ã®çµ„ã¿åˆã‚ã›

$$
L_\text{hybrid} = L_\text{simple} + \lambda L_\text{VLB}
$$

**çµæœ**: ImageNet 256Ã—256 ã§ FIDå¤§å¹…æ”¹å–„ (25.0 â†’ 4.59)ã€‚

### 6.3 Classifier Guidance æ¦‚å¿µ (â†’ å®Œå…¨ç‰ˆã¯ç¬¬39å› LDM)

**å‹•æ©Ÿ**: æ¡ä»¶ä»˜ãç”Ÿæˆ $p(\mathbf{x} \mid y)$ ã‚’å®Ÿç¾ã€‚

**Classifier Guidance** (Dhariwal & Nichol 2021):

$$
\tilde{\boldsymbol{\epsilon}}_\theta = \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) - \sqrt{1-\bar{\alpha}_t} \nabla_{\mathbf{x}_t} \log p_\phi(y \mid \mathbf{x}_t)
$$

ã“ã“ã§ $p_\phi(y \mid \mathbf{x}_t)$ ã¯åˆ¥é€”è¨“ç·´ã—ãŸåˆ†é¡å™¨ã€‚

**å•é¡Œ**: åˆ†é¡å™¨ $p_\phi$ ãŒå¿…è¦ â†’ **Classifier-Free Guidance (CFG)** (ç¬¬39å›) ãŒè§£æ±ºã€‚

**Classifier-Free Guidance** ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢:

$$
\tilde{\boldsymbol{\epsilon}}_\theta = (1 + w) \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y) - w \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \emptyset)
$$

ã“ã“ã§ $w$ ã¯ guidance scaleã€$\emptyset$ ã¯æ¡ä»¶ãªã— (unconditional)ã€‚

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ**:
- è¨“ç·´æ™‚ã« $p = 0.1$ ã®ç¢ºç‡ã§æ¡ä»¶ $y$ ã‚’ãƒ‰ãƒ­ãƒƒãƒ— (ç„¡æ¡ä»¶è¨“ç·´)
- æ¨è«–æ™‚ã«æ¡ä»¶ä»˜ããƒ»ç„¡æ¡ä»¶ã®2å›æ¨è«–ã—ã¦ç·šå½¢çµåˆ

```rust
use candle_core::Tensor;

/// Classifier-Free Guidance DDIM step.
/// Pass `None` for `t_prev` at the last step to treat á¾±_prev = 1.
fn ddim_step_cfg(
    model_cond:   &TinyUNet,   // conditioned model (or unified model with class token)
    model_uncond: &TinyUNet,   // unconditional model (null-class token)
    x_t:          &Tensor,
    t:             usize,
    t_prev:        Option<usize>,
    alpha_bar:    &[f32],
    w:             f32,        // guidance scale
    eta:           f32,        // 0 = deterministic DDIM, 1 = DDPM-like
) -> candle_core::Result<Tensor> {
    // Conditional and unconditional noise predictions
    let eps_cond   = model_cond.forward(x_t, t)?;
    let eps_uncond = model_uncond.forward(x_t, t)?;

    // CFG: ÎµÌƒ_Î¸ = Îµ_uncond + wÂ·(Îµ_cond âˆ’ Îµ_uncond)  (Dhariwal & Nichol 2021)
    let eps_guided = (&eps_uncond + (&eps_cond - &eps_uncond)? * w as f64)?;

    let ab_t    = alpha_bar[t];
    let ab_prev = t_prev.map(|tp| alpha_bar[tp]).unwrap_or(1.0);

    // Predicted x_0
    // xÌ‚â‚€ = (xâ‚œ - âˆš(1-á¾±â‚œ)Â·ÎµÌƒ_Î¸) / âˆšá¾±â‚œ
    let x0_pred = ((x_t - &eps_guided * (1.0 - ab_t).sqrt() as f64)? / ab_t.sqrt() as f64)?;

    // Ïƒâ‚œ(Î·) = Î·Â·âˆš((1-á¾±_{t-1})/(1-á¾±â‚œ))Â·âˆš(1 - á¾±â‚œ/á¾±_{t-1})  (Î·=0 â†’ deterministic)
    let sigma_t   = eta * ((1.0 - ab_prev) / (1.0 - ab_t)).sqrt()
                       * (1.0 - ab_t / ab_prev).sqrt();
    let dir_coeff = (1.0 - ab_prev - sigma_t * sigma_t).sqrt(); // âˆš(1-á¾±_{t-1}-Ïƒâ‚œÂ²)
    let dir_xt    = (&eps_guided * dir_coeff as f64)?;

    Ok((x0_pred * ab_prev.sqrt() as f64 + dir_xt)?)
}
```

**åŠ¹æœ**: $w = 7.5$ ã§ FID æ”¹å–„ & æ¡ä»¶ä¸€è‡´åº¦å‘ä¸Š (CLIP score â†‘)ã€‚

### 6.4 ç¢ºç‡ãƒ•ãƒ­ãƒ¼ODE & ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã®å†è§£é‡ˆ

**DDPMã¨ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ã®ç­‰ä¾¡æ€§** (Song+ 2020):

DDPM ã® Reverse Process ã¯ **Score-based Generative Model** ã¨åŒå€¤:

$$
\frac{d\mathbf{x}}{dt} = -\frac{1}{2} \beta(t) \left( \mathbf{x} + 2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) \right)
$$

ã“ã“ã§ $\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ ã¯ **ã‚¹ã‚³ã‚¢é–¢æ•°**ã€‚

**DDPMã¨ã®å¯¾å¿œ**:

$$
\nabla_{\mathbf{x}} \log p_t(\mathbf{x}_t) \approx -\frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1-\bar{\alpha}_t}}
$$

ã“ã‚Œã«ã‚ˆã‚Š:
- **DDPM**: é›¢æ•£æ™‚é–“ã®ç¢ºç‡éç¨‹
- **Score-based**: é€£ç¶šæ™‚é–“ã®ODE/SDE

ã¯æ•°å­¦çš„ã«åŒã˜å¯¾è±¡ã‚’ç•°ãªã‚‹è¦–ç‚¹ã§è¨˜è¿°ã—ã¦ã„ã‚‹ã€‚

**Probability Flow ODE** (Song+ 2020):

DDIMã®æ¥µé™ ($\eta = 0$) ã¯æ¬¡ã®ODEã¨ç­‰ä¾¡:

$$
\frac{d\mathbf{x}}{dt} = -\frac{1}{2} \sigma(t)^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x})
$$

ã“ã®ODEã‚’æ•°å€¤çš„ã«è§£ã = DDIMã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

**Stochastic Differential Equation (SDE)** ç‰ˆ (ç¬¬37å›ã§è©³èª¬):

DDPMã®ç¢ºç‡çš„ç‰ˆã¯æ¬¡ã®SDEã§è¨˜è¿°:

$$
d\mathbf{x} = -\frac{1}{2} \beta(t) \mathbf{x} \, dt + \sqrt{\beta(t)} \, d\mathbf{w}
$$

ã“ã“ã§ $d\mathbf{w}$ ã¯ãƒ–ãƒ©ã‚¦ãƒ³é‹å‹•ã€‚

**å®Ÿè£… â€” ODE Solver with ode_solvers**:

```rust
use candle_core::Tensor;

/// Probability Flow ODE (Song+ 2020):
///   dx/dt = -0.5 Â· ÏƒÂ²(t) Â· score(x, t),   score â‰ˆ -Îµ_Î¸ / sqrt(1 âˆ’ á¾±_t)
///
/// Solved here with a simple Euler discretization.
/// For higher accuracy use the `ode_solvers` crate (RK45 / Adams).
fn probability_flow_ode(
    model:     &TinyUNet,
    x_t_init:  Tensor,
    alpha_bar: &[f32],
    steps:     usize,
) -> candle_core::Result<Tensor> {
    let t_max = alpha_bar.len() - 1;
    // Timestep sub-sequence T â†’ 0
    let ts: Vec<usize> = (0..=t_max).rev()
        .step_by((t_max / steps).max(1))
        .collect();

    let mut x = x_t_init;
    for window in ts.windows(2) {
        let (t, t_next) = (window[0], window[1]);
        let ab_t = alpha_bar[t];

        // score â‰ˆ -Îµ_Î¸ / âˆš(1-á¾±â‚œ)  (Tweedie: âˆ‡_x log pâ‚œ(x) = -Îµ_Î¸/âˆš(1-á¾±â‚œ))
        let eps   = model.forward(&x, t)?;
        let score = (&eps * -(1.0 / (1.0 - ab_t).sqrt()) as f64)?;

        // ODE step: dx = -Â½Â·ÏƒÂ²(t)Â·scoreÂ·dt  (dt < 0 â†’ going backward in time)
        let sigma_sq = (1.0 - ab_t) / ab_t; // ÏƒÂ²(t) = (1-á¾±â‚œ)/á¾±â‚œ
        let dt       = t_next as f32 - t as f32; // negative
        let dx       = (&score * (-0.5 * sigma_sq * dt) as f64)?;
        x = (x + dx)?;
    }
    Ok(x) // final x_0
}

fn main() -> candle_core::Result<()> {
    let dev = candle_core::Device::Cpu;
    let x_t = Tensor::randn(0f32, 1f32, (1, 1, 28, 28), &dev)?;
    let (_, _, alpha_bar) = cosine_schedule(1000, 0.008);
    let x_0 = probability_flow_ode(&model, x_t, &alpha_bar, 50)?;
    println!("PF-ODE sample shape: {:?}", x_0.shape());
    Ok(())
}
```

**åˆ©ç‚¹**:
- é«˜ç²¾åº¦ãªæ•°å€¤è§£æ³• (Runge-Kutta, Adams, BDF) ãŒä½¿ãˆã‚‹
- Adaptive step size ã§åŠ¹ç‡çš„
- ç†è«–çš„ä¿è¨¼ (ODEã‚½ãƒ«ãƒãƒ¼ã®åæŸæ€§)

### 6.5 æ¡ä»¶ä»˜ãç”Ÿæˆã®ç™ºå±•å½¢æ…‹

**Inpainting (é ˜åŸŸä¿®å¾©)**:

DDPMã§ç”»åƒã®ä¸€éƒ¨ã‚’ä¿®å¾©:

```rust
use candle_core::{Tensor, Device};

/// DDPM inpainting: reverse-diffuse while preserving known pixels.
/// `mask`: 1.0 = generate freely, 0.0 = preserve from known_region.
fn ddpm_inpaint(
    model:        &TinyUNet,
    x_t_init:     Tensor,
    mask:         &Tensor,
    known_region: &Tensor,
    beta:         &[f32],
    alpha:        &[f32],
    alpha_bar:    &[f32],
    t_steps:      usize,
) -> candle_core::Result<Tensor> {
    let mut x_t = x_t_init;
    let dev = x_t.device().clone();

    for t in (0..t_steps).rev() {
        let eps_pred = model.forward(&x_t, t)?;

        // Î¼_Î¸ = (xâ‚œ âˆ’ Î²â‚œ/âˆš(1-á¾±â‚œ)Â·Îµ_Î¸) / âˆšÎ±â‚œ  (DDPM reverse mean)
        let coeff = (1.0 - alpha[t]) / (1.0 - alpha_bar[t]).sqrt();
        let mu    = ((&x_t - &eps_pred * coeff as f64)? / alpha[t].sqrt() as f64)?;

        let x_generated = if t > 0 {
            // ÏƒÌƒâ‚œÂ² = (1-á¾±_{t-1})/(1-á¾±â‚œ)Â·Î²â‚œ  (posterior variance)
            let sigma = ((1.0 - alpha_bar[t - 1]) / (1.0 - alpha_bar[t]) * (1.0 - alpha[t])).sqrt();
            let z = Tensor::randn(0f32, 1f32, x_t.shape(), &dev)?;
            (mu + z * sigma as f64)?
        } else {
            mu
        };

        // Blend: keep known_region where mask = 0
        let one = Tensor::ones_like(mask)?;
        x_t = (mask * &x_generated + (&one - mask)? * known_region)?;
    }
    Ok(x_t)
}

fn main() -> candle_core::Result<()> {
    let dev = Device::Cpu;

    // Mask: 1.0 everywhere except center 14Ã—14 patch
    let mut mask_data = vec![1.0f32; 28 * 28];
    for row in 7..21 { for col in 7..21 { mask_data[row * 28 + col] = 0.0; } }
    let mask         = Tensor::from_vec(mask_data, (1, 1, 28, 28), &dev)?;
    let known_region = test_data.narrow(0, 0, 1)?;
    let x_t          = Tensor::randn(0f32, 1f32, (1, 1, 28, 28), &dev)?;

    let inpainted = ddpm_inpaint(&model, x_t, &mask, &known_region,
                                 &beta, &alpha, &alpha_bar, 1000)?;
    println!("Inpainted shape: {:?}", inpainted.shape());
    Ok(())
}
```

**Super-resolution (è¶…è§£åƒ)**:

ä½è§£åƒåº¦ç”»åƒã‹ã‚‰é«˜è§£åƒåº¦ã‚’ç”Ÿæˆ:

```rust
use candle_core::{Tensor, Device};

/// SR-DDPM: generate high-res image conditioned on upsampled low-res.
/// `model` must accept 2-channel input (x_t âˆ¥ x_low_res concatenated on channel dim).
fn ddpm_super_resolution(
    model:     &TinyUNet,
    x_t_init:  Tensor,
    x_low_res: &Tensor,   // already upsampled to the same (H, W) as x_t
    beta:      &[f32],
    alpha:     &[f32],
    alpha_bar: &[f32],
    t_steps:   usize,
) -> candle_core::Result<Tensor> {
    let mut x_t = x_t_init;
    let dev = x_t.device().clone();

    for t in (0..t_steps).rev() {
        // Concat low-res condition along channel dim: (B, 2, H, W)
        let x_input  = Tensor::cat(&[&x_t, x_low_res], 1)?;
        let eps_pred = model.forward(&x_input, t)?;

        // Î¼_Î¸ = (xâ‚œ âˆ’ Î²â‚œ/âˆš(1-á¾±â‚œ)Â·Îµ_Î¸) / âˆšÎ±â‚œ
        let coeff = (1.0 - alpha[t]) / (1.0 - alpha_bar[t]).sqrt();
        let mu    = ((&x_t - &eps_pred * coeff as f64)? / alpha[t].sqrt() as f64)?;

        x_t = if t > 0 {
            let sigma = ((1.0 - alpha_bar[t - 1]) / (1.0 - alpha_bar[t]) * (1.0 - alpha[t])).sqrt();
            let z = Tensor::randn(0f32, 1f32, x_t.shape(), &dev)?;
            (mu + z * sigma as f64)?
        } else {
            mu
        };
    }
    Ok(x_t)
}

fn main() -> candle_core::Result<()> {
    let dev = Device::Cpu;
    // Nearest-neighbor upsample 14Ã—14 â†’ 28Ã—28 (use image crate for bilinear)
    let x_low  = test_data.narrow(0, 0, 1)?.upsample_nearest2d(28, 28)?;
    let x_t    = Tensor::randn(0f32, 1f32, (1, 1, 28, 28), &dev)?;
    let x_high = ddpm_super_resolution(&model, x_t, &x_low, &beta, &alpha, &alpha_bar, 1000)?;
    println!("Super-resolved shape: {:?}", x_high.shape());
    Ok(())
}
```

**Text-to-Image (æ¦‚å¿µ, å®Œå…¨ç‰ˆã¯ç¬¬39å›)**:

ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (CLIP/T5) â†’ åŸ‹ã‚è¾¼ã¿ â†’ U-Netã«æ³¨å…¥:

```rust
use candle_core::Tensor;
use candle_nn::{Linear, Module};

/// Text-to-Image U-Net: injects text embedding via cross-attention (conceptual).
struct TextConditionedUNet {
    text_encoder:  Linear,   // text embedding dim â†’ spatial embedding dim
    cross_attn_q:  Linear,   // query projection (from spatial features)
    cross_attn_kv: Linear,   // key/value projection (from text features)
    base_unet:     TinyUNet,
}

impl TextConditionedUNet {
    /// x_t: (B, C, H, W),  text_emb: (1, text_dim)
    fn forward(&self, x_t: &Tensor, t: usize, text_emb: &Tensor) -> candle_core::Result<Tensor> {
        // Encode text: (1, text_dim) â†’ (1, 1, emb_dim)
        let text_feat = self.text_encoder.forward(text_emb)?.unsqueeze(1)?;

        // Flatten spatial: (B, C, H, W) â†’ (B, N, C)
        let (b, c, h, w) = x_t.dims4()?;
        let x_flat = x_t.reshape((b, c, h * w))?.transpose(1, 2)?; // (B, N, C)

        // Cross-attention: x_flat queries attend to text_feat keys/values
        let q  = self.cross_attn_q.forward(&x_flat)?;
        let kv = self.cross_attn_kv.forward(&text_feat)?;
        let scale       = (q.dim(candle_core::D::Minus1)? as f64).sqrt();
        let attn        = candle_nn::ops::softmax(&q.matmul(&kv.transpose(1, 2)?)? / scale,
                                                  candle_core::D::Minus1)?;
        let x_attended  = attn.matmul(&kv)?.transpose(1, 2)?.reshape((b, c, h, w))?;

        // Base U-Net with text-conditioned features
        self.base_unet.forward(&x_attended, t)
    }
}
```

### 6.6 Production-Ready å®Ÿè£…ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³

**ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆ** â€” Model / Scheduler / Sampler ã®åˆ†é›¢:

```rust
use candle_core::Tensor;

/// Modular design â€” Scheduler / Sampler separation via traits.

// â”€â”€ Scheduler trait â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

trait NoiseScheduler {
    fn beta(&self)      -> &[f32];
    fn alpha(&self)     -> &[f32];
    fn alpha_bar(&self) -> &[f32];
    fn t_steps(&self)   -> usize { self.beta().len() }
}

struct CosineScheduler       { beta: Vec<f32>, alpha: Vec<f32>, alpha_bar: Vec<f32> }
struct ZeroTerminalSNRScheduler { beta: Vec<f32>, alpha: Vec<f32>, alpha_bar: Vec<f32> }

impl CosineScheduler {
    fn new(t_steps: usize) -> Self {
        let (b, a, ab) = cosine_schedule(t_steps, 0.008);
        Self { beta: b, alpha: a, alpha_bar: ab }
    }
}
impl NoiseScheduler for CosineScheduler {
    fn beta(&self)      -> &[f32] { &self.beta }
    fn alpha(&self)     -> &[f32] { &self.alpha }
    fn alpha_bar(&self) -> &[f32] { &self.alpha_bar }
}

impl ZeroTerminalSNRScheduler {
    fn new(t_steps: usize) -> Self {
        let (b, a, ab) = zero_terminal_snr_schedule(t_steps);
        Self { beta: b, alpha: a, alpha_bar: ab }
    }
}
impl NoiseScheduler for ZeroTerminalSNRScheduler {
    fn beta(&self)      -> &[f32] { &self.beta }
    fn alpha(&self)     -> &[f32] { &self.alpha }
    fn alpha_bar(&self) -> &[f32] { &self.alpha_bar }
}

// â”€â”€ Sampler trait â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

trait Sampler {
    fn sample(&self, model: &TinyUNet, x_t: Tensor, steps: usize) -> candle_core::Result<Tensor>;
}

struct DdpmSampler<S: NoiseScheduler>       { scheduler: S }
struct DdimSampler<S: NoiseScheduler>       { scheduler: S, eta: f32 }
struct DpmSolverPPSampler<S: NoiseScheduler> { scheduler: S, order: usize }

impl<S: NoiseScheduler> Sampler for DdpmSampler<S> {
    fn sample(&self, model: &TinyUNet, x_t: Tensor, _steps: usize) -> candle_core::Result<Tensor> {
        let s = &self.scheduler;
        ddpm_sample(model, x_t, s.beta(), s.alpha(), s.alpha_bar(), s.t_steps())
    }
}
impl<S: NoiseScheduler> Sampler for DdimSampler<S> {
    fn sample(&self, model: &TinyUNet, x_t: Tensor, steps: usize) -> candle_core::Result<Tensor> {
        ddim_sample(model, x_t, self.scheduler.alpha_bar(), steps, self.eta)
    }
}
impl<S: NoiseScheduler> Sampler for DpmSolverPPSampler<S> {
    fn sample(&self, model: &TinyUNet, x_t: Tensor, steps: usize) -> candle_core::Result<Tensor> {
        dpm_solver_pp_sample(model, x_t, self.scheduler.alpha_bar(), steps, self.order)
    }
}
```

**ä½¿ç”¨ä¾‹**:

```rust
use candle_core::{Tensor, Device};

fn main() -> candle_core::Result<()> {
    let dev = Device::Cpu;

    // Create samplers (scheduler is cheap to clone)
    let sampler_ddim = DdimSampler        { scheduler: CosineScheduler::new(1000), eta: 0.0 };
    let sampler_dpm  = DpmSolverPPSampler { scheduler: CosineScheduler::new(1000), order: 2 };

    // Sample from N(0, I) noise
    let x_t = Tensor::randn(0f32, 1f32, (16, 1, 28, 28), &dev)?;

    let samples_ddim = sampler_ddim.sample(&model, x_t.clone(), 50)?;
    let samples_dpm  = sampler_dpm .sample(&model, x_t,         20)?;

    println!("DDIM samples shape: {:?}", samples_ddim.shape());
    println!("DPM++ samples shape: {:?}", samples_dpm.shape());
    Ok(())
}
```

**åˆ©ç‚¹**:
- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¨ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’è‡ªç”±ã«çµ„ã¿åˆã‚ã›
- æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’è¿½åŠ ã—ã¦ã‚‚æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã«å½±éŸ¿ãªã—
- ãƒ†ã‚¹ãƒˆãƒ»æ¯”è¼ƒãŒå®¹æ˜“

### 6.7 Rust Production Inference ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**ONNX Export from Rust**:

```rust
// Export via tract-onnx or burn's ONNX export.
// For candle: save weights as safetensors, then convert with an ONNX conversion script.
//
//   use candle_core::safetensors;
//   safetensors::save(&tensor_map, "tiny_ddpm.safetensors")?;
//   burn::export::to_onnx(&model_record, filepath);

fn export_to_onnx(var_map: &candle_nn::VarMap, filepath: &str) -> candle_core::Result<()> {
    let path = std::path::Path::new(filepath).with_extension("safetensors");
    var_map.save(&path)?;
    println!("Model exported to {}", path.display());
    Ok(())
}

fn main() -> candle_core::Result<()> {
    let var_map = candle_nn::VarMap::new();
    // ... (train model) ...
    export_to_onnx(&var_map, "tiny_ddpm.onnx")
}
```

**Rust Inference with ort (ONNX Runtime)**:

```rust
use ort::{Environment, SessionBuilder, Value};
use ndarray::{Array4, s};

pub struct DDPMInference {
    session: ort::Session,
    alpha_bar: Vec<f32>,
}

impl DDPMInference {
    pub fn new(model_path: &str, alpha_bar: Vec<f32>) -> Result<Self, Box<dyn std::error::Error>> {
        let environment = Environment::builder().build()?;
        let session = SessionBuilder::new(&environment)?
            .with_model_from_file(model_path)?;

        Ok(Self { session, alpha_bar })
    }

    pub fn predict_noise(&self, x_t: &Array4<f32>, t: usize) -> Result<Array4<f32>, Box<dyn std::error::Error>> {
        // Prepare input
        let x_input = Value::from_array(self.session.allocator(), x_t)?;
        let t_input = Value::from_array(self.session.allocator(), &ndarray::arr1(&[t as f32]))?;

        // Run inference
        let outputs = self.session.run(vec![x_input, t_input])?;
        let epsilon = outputs[0].try_extract::<f32>()?.view().to_owned();

        Ok(epsilon.into_dimensionality::<ndarray::Ix4>()?)
    }

    pub fn ddim_sample(&self, x_t: Array4<f32>, steps: usize, Î·: f32) -> Result<Array4<f32>, Box<dyn std::error::Error>> {
        let mut x = x_t;
        let T = self.alpha_bar.len();
        let step_indices: Vec<usize> = (0..steps).map(|i| T * i / steps).collect();

        for i in (1..step_indices.len()).rev() {
            let (t, t_prev) = (step_indices[i], step_indices[i - 1]);

            // Predict noise
            let Îµ = self.predict_noise(&x, t)?;

            // DDIM step
            let á¾±_t    = self.alpha_bar[t];
            let á¾±_prev = self.alpha_bar[t_prev];

            let x_0_pred = (&x - (1.0 - á¾±_t).sqrt() * &Îµ) / á¾±_t.sqrt();
            let Ïƒ_t = Î· * ((1.0 - á¾±_prev) / (1.0 - á¾±_t)).sqrt()
                         * (1.0 - á¾±_t / á¾±_prev).sqrt();
            let dir_xt = (1.0 - á¾±_prev - Ïƒ_t.powi(2)).sqrt() * &Îµ;

            x = á¾±_prev.sqrt() * x_0_pred + dir_xt;
        }

        Ok(x)
    }
}

// Usage
fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Load alpha_bar from file
    let alpha_bar: Vec<f32> = load_alpha_bar_from_file("alpha_bar.bin")?;

    // Create inference engine
    let ddpm = DDPMInference::new("tiny_ddpm.onnx", alpha_bar)?;

    // Sample
    let x_t = Array4::<f32>::from_shape_fn((1, 1, 28, 28), |_| rand::random::<f32>());
    let x_0 = ddpm.ddim_sample(x_t, 50, 0.0)?;
    println!("Generated sample shape: {:?}", x_0.shape());
    Ok(())
}
```

**Benchmark** (M1 Mac, MNIST 28Ã—28, 50 steps):

| Implementation | Latency | Throughput (samples/sec) |
|:---------------|:--------|:-------------------------|
| Rust Candle (CPU) | 2.3s | 0.43 |
| Rust ONNX (CPU) | 0.8s | 1.25 |
| Rust ONNX (CoreML) | 0.3s | 3.33 |

**Production deployment architecture**:

```mermaid
graph LR
    A[Rust Training<br/>Candle] --> B[ONNX Export]
    B --> C[Rust Inference Server]
    C --> D[gRPC API]
    D --> E[Client Apps]

    C --> F[ONNX Runtime]
    F --> G[CPU/GPU/CoreML]

    style A fill:#99ccff
    style C fill:#ff9999
    style G fill:#99ff99
```

### 6.8 æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)

| ç ”ç©¶ | ä¸»å¼µ | è«–æ–‡ |
|:-----|:-----|:-----|
| **DDPMæœ€é©åæŸãƒ¬ãƒ¼ãƒˆ** | TVè·é›¢ $O(d/T)$ åæŸè¨¼æ˜ã€ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®ä¸‹ç•Œ | [arXiv:2510.27562](https://arxiv.org/abs/2510.27562) (2025H2) |
| **DDPM Score Matchingæ¼¸è¿‘åŠ¹ç‡æ€§** | DDPMã‚¹ã‚³ã‚¢æ¨å®šãŒçµ±è¨ˆçš„ã«æœ€é©ã€ç†è«–çš„æ­£å½“åŒ– | [arXiv:2504.05161](https://arxiv.org/abs/2504.05161) (ICLR 2025) |
| **Zero Terminal SNR** | è¨“ç·´/æ¨è«–ã®ä¸ä¸€è‡´ã‚’è§£æ¶ˆ | [arXiv:2305.08891](https://arxiv.org/abs/2305.08891) (Lin+ 2023) |
| **Consistency Models** | 1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆã§å“è³ªç¶­æŒ | [arXiv:2303.01469](https://arxiv.org/abs/2303.01469) (Song+ 2023) |
| **Flow Matching** | ODEãƒ™ãƒ¼ã‚¹ã®æ–°ã—ã„å®šå¼åŒ– | [arXiv:2210.02747](https://arxiv.org/abs/2210.02747) (Lipman+ 2022) |

> **Note:** **é€²æ—: 95% å®Œäº†** é«˜æ¬¡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»Improved DDPMãƒ»Guidanceã‚’æ¦‚è¦³ã€‚Zone 7ã§ç·æ‹¬ã¸ã€‚

---

> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. DDPMã®æœ€é©åæŸãƒ¬ãƒ¼ãƒˆ $O(d/T)$ï¼ˆarXiv:2510.27562ï¼‰ã«ãŠã„ã¦ $d$ ãŒæŒ‡ã™é‡ã¨ã€åæŸã‚’ä¿è¨¼ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•° $T$ ã®é¸ã³æ–¹ã®æŒ‡é‡ã‚’è¿°ã¹ã‚ˆã€‚
> 2. Cosineã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒ Linearã‚ˆã‚Šé«˜SNRé ˜åŸŸã§æœ‰åˆ©ã§ã€ä½SNRé ˜åŸŸã§å·®ãŒç¸®ã¾ã‚‹ç†ç”±ã‚’ $\bar{\alpha}_t$ ã®ã‚°ãƒ©ãƒ•ã®å½¢çŠ¶ã¨å¯¾å¿œã•ã›ã¦èª¬æ˜ã›ã‚ˆã€‚

## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 7.1 æœ¬è¬›ç¾©ã®åˆ°é”ç‚¹

**3ã¤ã®æ ¸å¿ƒ**:

1. **Forward/Reverse Process ã®å®Œå…¨å°å‡º**: é–‰å½¢å¼è§£ (æ•°å­¦çš„å¸°ç´æ³•)ã€ãƒ™ã‚¤ã‚ºåè»¢ (æ¡ä»¶ä»˜ãã‚¬ã‚¦ã‚¹)
2. **VLB ã¨ç°¡ç´ åŒ–æå¤±**: $L_T + \sum L_t + L_0$ ã®å®Œå…¨å±•é–‹ã€$L_\text{simple}$ ãŒå„ªã‚Œã‚‹ç†ç”±
3. **DDIMæ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: Non-Markovian forwardã€Probability Flow ODE ã¨ã®æ¥ç¶š

**é‡è¦ãªå¼**:

$$
\begin{aligned}
\text{Forward:} \quad & q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t) \mathbf{I}) \\
\text{Reverse:} \quad & p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \sigma_t^2 \mathbf{I}) \\
\text{Loss:} \quad & L_\text{simple} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} [\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2] \\
\text{DDIM:} \quad & \mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}} \boldsymbol{\epsilon}_\theta
\end{aligned}
$$

**å®Ÿè£…**: ğŸ¦€ Rustè¨“ç·´ (Candle + Zygote) + ğŸ¦€ Rustæ¨è«– (ONNX Runtime) ã§ Production-readyã€‚

### 7.2 FAQ

<details><summary>Q1: DDPMã¨Score Matchingã®é•ã„ã¯ï¼Ÿ</summary>

**A**: **æœ¬è³ªçš„ã«åŒã˜** (Section 3.10)ã€‚DDPMã¯é›¢æ•£æ™‚åˆ»ã€Score Matchingã¯é€£ç¶šæ™‚åˆ»ã€‚æ•°å¼:

$$
\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \mid \mathbf{x}_0) = - \frac{\boldsymbol{\epsilon}}{\sqrt{1-\bar{\alpha}_t}}
$$

ãƒã‚¤ã‚ºäºˆæ¸¬ = ã‚¹ã‚³ã‚¢äºˆæ¸¬ (rescaled)ã€‚

</details>

<details><summary>Q2: ãªãœ $L_\text{simple}$ ãŒ $L_\text{VLB}$ ã‚ˆã‚Šå„ªã‚Œã‚‹ï¼Ÿ</summary>

**A**: $L_\text{VLB}$ ã®é‡ã¿ $\frac{\beta_t^2}{2\sigma_t^2 \alpha_t (1-\bar{\alpha}_t)}$ ã¯ã€ä½ãƒã‚¤ã‚ºé ˜åŸŸã‚’éé‡è¦– â†’ çŸ¥è¦šå“è³ªä½ä¸‹ã€‚$L_\text{simple}$ ã¯å…¨æ™‚åˆ»ã‚’å‡ç­‰ã«å­¦ç¿’ â†’ FIDæ”¹å–„ã€‚

</details>

<details><summary>Q3: DDIM ã® $\eta$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ„å‘³ã¯ï¼Ÿ</summary>

**A**: $\eta = 0$: æ±ºå®šè«–çš„ (åŒã˜ $\mathbf{x}_T$ â†’ åŒã˜ $\mathbf{x}_0$)ã€‚$\eta = 1$: DDPMé¢¨ (ç¢ºç‡çš„)ã€‚ä¸­é–“å€¤ã§åˆ¶å¾¡å¯èƒ½ã€‚

</details>

<details><summary>Q4: Cosine schedule vs Linear schedule ã®é•ã„ã¯ï¼Ÿ</summary>

**A**: **Cosine** (æ¨å¥¨): SNRç·©ã‚„ã‹ã«æ¸›å°‘ã€è¨“ç·´å®‰å®šã€Zero Terminal SNRã«è¿‘ã„ã€‚**Linear**: å¤ã„ã€$\bar{\alpha}_T > 0$ ã§è¨“ç·´/æ¨è«–ä¸ä¸€è‡´ã€‚

</details>

<details><summary>Q5: U-Net ã® Self-Attention ã¯ã©ã“ã«é…ç½®ï¼Ÿ</summary>

**A**: **16Ã—16ä»¥ä¸‹ã®ä½è§£åƒåº¦** ã§ã®ã¿ã€‚è¨ˆç®—é‡ $O(N^2)$ ã®ãŸã‚ã€é«˜è§£åƒåº¦ã§ã¯çœç•¥ã€‚MNISTã§ã¯28Ã—28ãªã®ã§ã€1å±¤ã®ã¿ã§ååˆ†ã€‚

</details>

### 7.3 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (1é€±é–“ãƒ—ãƒ©ãƒ³)

| æ—¥ | å†…å®¹ | æ™‚é–“ |
|:---|:-----|:-----|
| 1 | Zone 0-2 (ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ â†’ ç›´æ„Ÿ) | 30åˆ† |
| 2 | Zone 3.1-3.4 (Forward â†’ VLB) | 60åˆ† |
| 3 | Zone 3.5-3.7 (3å½¢æ…‹ â†’ SNR) | 45åˆ† |
| 4 | Zone 3.8-3.10 (U-Net â†’ Score) | 60åˆ† |
| 5 | Zone 4 (Rustå®Ÿè£…) | 90åˆ† |
| 6 | Zone 5 (å®Ÿé¨“ + è‡ªå·±è¨ºæ–­) | 60åˆ† |
| 7 | Zone 6-7 (ç™ºå±• + æŒ¯ã‚Šè¿”ã‚Š) | 45åˆ† |

### 7.4 æ¬¡å›äºˆå‘Š: ç¬¬37å› SDE/ODE & ç¢ºç‡éç¨‹è«–

**DDPM (é›¢æ•£)** ã‚’ **SDE (é€£ç¶šæ™‚é–“)** ã«æ‹¡å¼µã™ã‚‹ã€‚

**äºˆå‘Šå†…å®¹**:

- **VP-SDE / VE-SDE / Sub-VP SDE**: DDPMã¨NCSNã®SDEçµ±ä¸€
- **Reverse-time SDE** (Anderson 1982): é€†æ™‚é–“æ‹¡æ•£ã®å­˜åœ¨å®šç†
- **Probability Flow ODE**: åŒä¸€å‘¨è¾ºåˆ†å¸ƒã‚’æŒã¤æ±ºå®šè«–çš„éç¨‹
- **Score SDEçµ±ä¸€ç†è«–** (Song+ 2021): Forwardâ†’Reverseâ†’Scoreâ†’ODE
- **åæŸæ€§è§£æ**: TVè·é›¢ $O(d/T)$ åæŸã€Manifoldä»®èª¬ä¸‹ã®ç·šå½¢åæŸ

**Course I ç¬¬5å›ã¨ã®é–¢ä¿‚**: ç¬¬5å›ã§ä¼Šè—¤ç©åˆ†ãƒ»ä¼Šè—¤ã®è£œé¡Œãƒ»SDEåŸºç¤ã‚’å°å…¥æ¸ˆã¿ã€‚ç¬¬37å›ã¯ã“ã‚Œã‚’**Diffusionå›ºæœ‰ã®SDE (VP/VE/Reverse/PF-ODE)** ã«ç‰¹åŒ–ã€‚

**æ¥ç¶š**: DDPMé›¢æ•£ â†’ SDEé€£ç¶š â†’ Flow Matchingçµ±ä¸€ (ç¬¬38å›)ã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ† ç¬¬36å›å®Œäº†ï¼DDPMç†è«–ãƒ»å®Ÿè£…ãƒ»å®Ÿé¨“ã‚’å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ã€‚ç¬¬37å›ã§SDEé€£ç¶šæ™‚é–“ã¸ã€‚

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**"1000ã‚¹ãƒ†ãƒƒãƒ—"ã¯ç†è«–ç†è§£ã®ä¸è¶³ã§ã¯ï¼Ÿ**

DDPM [^1] (2020) ã¯1000ã‚¹ãƒ†ãƒƒãƒ—ã€‚ã ãŒ2021å¹´ã®DDIM [^2] ã§50ã‚¹ãƒ†ãƒƒãƒ—ã€2022å¹´ã®DPM-Solver++ [^4] ã§10-20ã‚¹ãƒ†ãƒƒãƒ—ã€2023å¹´ã®Consistency Models (ç¬¬40å›) ã§1ã‚¹ãƒ†ãƒƒãƒ—ã€‚

**å•ã„**:

1. ãªãœDDPMã¯1000ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ã ã£ãŸã®ã‹ï¼Ÿ â†’ **ãƒãƒ«ã‚³ãƒ•ä»®å®š** + **å›ºå®šã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«** ã®åˆ¶ç´„
2. DDIMã®æœ¬è³ªã¯ä½•ã‹ï¼Ÿ â†’ **Non-Markovian** ã§è‡ªç”±åº¦ç²å¾— + **Probability Flow ODE** è¿‘ä¼¼
3. 1ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆã®ç†è«–çš„é™ç•Œã¯ï¼Ÿ â†’ **è’¸ç•™** vs **Flow Matching** vs **Consistency** (ç¬¬40å›ã§è¨¼æ˜)

**æŒ‘ç™ºçš„ä»®èª¬**: "1000ã‚¹ãƒ†ãƒƒãƒ—" ã¯å®Ÿè£…ã®ä¾¿å®œã€‚**ç†è«–çš„ã«ã¯10-50ã‚¹ãƒ†ãƒƒãƒ—ã§ååˆ†** (DDIM/DPM-Solver++)ã€**æœ€çµ‚çš„ã«ã¯1ã‚¹ãƒ†ãƒƒãƒ—ãŒå¯èƒ½** (Consistency Models)ã€‚ã‚¹ãƒ†ãƒƒãƒ—æ•°å‰Šæ¸›ã®æ­´å²ã¯ã€**ç†è«–ã®æ´—ç·´ã®æ­´å²** ã§ã‚ã‚‹ã€‚

**ã‚ãªãŸã¯ã©ã†è€ƒãˆã‚‹ã‹ï¼Ÿ**

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. *NeurIPS 2020*. [arXiv:2006.11239](https://arxiv.org/abs/2006.11239)

<https://arxiv.org/abs/2006.11239>

[^2]: Song, J., Meng, C., & Ermon, S. (2020). Denoising Diffusion Implicit Models. *ICLR 2021*. [arXiv:2010.02502](https://arxiv.org/abs/2010.02502)

<https://arxiv.org/abs/2010.02502>

[^3]: Nichol, A., & Dhariwal, P. (2021). Improved Denoising Diffusion Probabilistic Models. *ICML 2021*. [arXiv:2102.09672](https://arxiv.org/abs/2102.09672)

<https://arxiv.org/abs/2102.09672>

[^4]: Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., & Zhu, J. (2022). DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models. *NeurIPS 2022*. [arXiv:2211.01095](https://arxiv.org/abs/2211.01095)

<https://arxiv.org/abs/2211.01095>

[^5]: Lin, S., Liu, B., Li, J., & Yang, X. (2023). Common Diffusion Noise Schedules and Sample Steps are Flawed. *WACV 2024*. [arXiv:2305.08891](https://arxiv.org/abs/2305.08891)

<https://arxiv.org/abs/2305.08891>

[^6]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2020). Score-Based Generative Modeling through Stochastic Differential Equations. *ICLR 2021*. [arXiv:2011.13456](https://arxiv.org/abs/2011.13456)

<https://arxiv.org/abs/2011.13456>

### æ•™ç§‘æ›¸

- Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the Design Space of Diffusion-Based Generative Models. *NeurIPS 2022*. [arXiv:2206.00364](https://arxiv.org/abs/2206.00364)
- Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., ... & Cui, B. (2023). Diffusion Models: A Comprehensive Survey of Methods and Applications. *ACM Computing Surveys*. [arXiv:2209.00796](https://arxiv.org/abs/2209.00796)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
