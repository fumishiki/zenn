---
title: "ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³•: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å‰ç·¨ã€‘ç†è«–ç·¨"
emoji: "ğŸ”"
type: "tech"
topics: ["machinelearning", "deeplearning", "statistics", "python"]
published: true
---


# ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³• â€” è¦‹ãˆãªã„ã‚‚ã®ã‚’æ¨å®šã™ã‚‹æŠ€è¡“

> **è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®è£ã«ã¯ã€å¸¸ã«ã€Œè¦‹ãˆãªã„æ§‹é€ ã€ãŒéš ã‚Œã¦ã„ã‚‹ã€‚ãã‚Œã‚’æ•°å­¦çš„ã«æ‰±ã†æ–¹æ³•ãŒEMç®—æ³•ã ã€‚**

ç›®ã®å‰ã«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒå…¨ã¦ã ã¨æ€ã†ã ã‚ã†ã‹ã€‚å®Ÿã¯ãã†ã§ã¯ãªã„ã€‚æ‰‹æ›¸ãæ•°å­—ç”»åƒã®èƒŒå¾Œã«ã¯ã€Œã©ã®æ•°å­—ã‚’æ›¸ã“ã†ã¨ã—ãŸã‹ã€ã¨ã„ã†æ„å›³ãŒéš ã‚Œã¦ã„ã‚‹ã€‚éŸ³å£°ä¿¡å·ã®è£ã«ã¯ã€Œã©ã®éŸ³ç´ ã‚’ç™ºè©±ã—ã¦ã„ã‚‹ã‹ã€ã¨ã„ã†çŠ¶æ…‹ãŒã‚ã‚‹ã€‚é¡§å®¢ã®è³¼è²·ãƒ‡ãƒ¼ã‚¿ã®å‘ã“ã†ã«ã¯ã€Œã©ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«å±ã™ã‚‹ã‹ã€ã¨ã„ã†æ§‹é€ ãŒæ½œã‚“ã§ã„ã‚‹ã€‚

ã“ã®ã€Œè¦‹ãˆãªã„æ§‹é€ ã€ã‚’ **æ½œåœ¨å¤‰æ•°** (latent variable) ã¨å‘¼ã¶ã€‚ãã—ã¦æ½œåœ¨å¤‰æ•°ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã™ã‚‹æœ€ã‚‚åŸºæœ¬çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒ **EMç®—æ³•** (Expectation-Maximization algorithm) ã ã€‚1977å¹´ã«Dempster, Laird, RubinãŒå®šå¼åŒ–ã—ãŸã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  [^1] ã¯ã€åŠä¸–ç´€è¿‘ãçµŒã£ãŸä»Šã‚‚æ©Ÿæ¢°å­¦ç¿’ã®æ ¹å¹¹ã‚’æ”¯ãˆã¦ã„ã‚‹ã€‚

æœ¬è¬›ç¾©ã¯Course Iã€Œæ•°å­¦åŸºç¤ç·¨ã€ã®æœ€çµ‚å› â€” 8å›ã«ã‚ãŸã‚‹æ•°å­¦ã®æ—…ã®ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬ã ã€‚ç¬¬7å›ã§å­¦ã‚“ã æœ€å°¤æ¨å®šã®é™ç•Œã‚’çªç ´ã—ã€Course IIã®å¤‰åˆ†æ¨è«–ãƒ»VAEã¸æ©‹ã‚’æ¶ã‘ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ”¢ è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ x"] --> B["â“ æ½œåœ¨å¤‰æ•° z ã¯ï¼Ÿ"]
    B --> C["ğŸ“ EMç®—æ³•<br/>E-step + M-step"]
    C --> D["ğŸ¯ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸* æ¨å®š"]
    D --> E["ğŸŒ‰ VAE/Diffusionã¸"]
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style E fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” è¦‹ãˆãªã„å¤‰æ•°ã‚’å½“ã¦ã‚‹

**ã‚´ãƒ¼ãƒ«**: æ½œåœ¨å¤‰æ•°ã¨EMç®—æ³•ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãŒæ··ã–ã£ãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã€‚ã©ã¡ã‚‰ã®åˆ†å¸ƒã‹ã‚‰æ¥ãŸã‹ã¯è¦‹ãˆãªã„ã€‚ãã‚Œã‚’å½“ã¦ã‚‹ã®ãŒEMç®—æ³•ã ã€‚

```python
import numpy as np

# 2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆã©ã¡ã‚‰ã‹ã‚‰æ¥ãŸã‹ã¯ã€Œéš ã‚Œã¦ã„ã‚‹ã€ï¼‰
np.random.seed(42)
z_true = np.random.choice([0, 1], size=200, p=[0.4, 0.6])  # latent variable
x = np.where(z_true == 0,
             np.random.normal(-2, 0.8, 200),   # cluster 0
             np.random.normal(3, 1.2, 200))     # cluster 1

# EM algorithm: 10 iterations
mu = np.array([-1.0, 1.0])  # initial guess
sigma = np.array([1.0, 1.0])
pi = np.array([0.5, 0.5])

for step in range(10):
    # E-step: compute responsibilities Î³(z_nk)
    pdf0 = pi[0] * np.exp(-0.5*((x - mu[0])/sigma[0])**2) / (sigma[0] * np.sqrt(2*np.pi))
    pdf1 = pi[1] * np.exp(-0.5*((x - mu[1])/sigma[1])**2) / (sigma[1] * np.sqrt(2*np.pi))
    gamma = pdf1 / (pdf0 + pdf1)
    # M-step: update parameters
    N0, N1 = (1 - gamma).sum(), gamma.sum()
    mu[0] = ((1 - gamma) * x).sum() / N0
    mu[1] = (gamma * x).sum() / N1
    sigma[0] = np.sqrt(((1 - gamma) * (x - mu[0])**2).sum() / N0)
    sigma[1] = np.sqrt((gamma * (x - mu[1])**2).sum() / N1)
    pi[0], pi[1] = N0 / len(x), N1 / len(x)

print(f"Estimated: mu=({mu[0]:.2f}, {mu[1]:.2f}), sigma=({sigma[0]:.2f}, {sigma[1]:.2f})")
print(f"True:      mu=(-2.00, 3.00), sigma=(0.80, 1.20)")
print(f"Mix weights: ({pi[0]:.2f}, {pi[1]:.2f}) vs true (0.40, 0.60)")
```

å‡ºåŠ›:
```
Estimated: mu=(-1.99, 3.06), sigma=(0.78, 1.18)
True:      mu=(-2.00, 3.00), sigma=(0.80, 1.20)
Mix weights: (0.39, 0.61) vs true (0.40, 0.60)
```

**ãŸã£ãŸ10å›ã®åå¾©ã§ã€ã€Œè¦‹ãˆãªã„ã€æ½œåœ¨å¤‰æ•° $z$ ã®æ§‹é€ ã‚’æ­£ç¢ºã«å¾©å…ƒã§ãã¦ã„ã‚‹ã€‚** ã“ã‚ŒãŒEMç®—æ³•ã®å¨åŠ›ã ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å¼:

$$
p(x \mid \theta) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x \mid \mu_k, \sigma_k^2)
$$

ã€Œæ··åˆã€(mixture) ã¨ã„ã†è¨€è‘‰ã®é€šã‚Šã€è¤‡æ•°ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‚’é‡ã¿ $\pi_k$ ã§æ··ãœåˆã‚ã›ã¦ã„ã‚‹ã€‚ã©ã®æˆåˆ†ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸã‹ã‚’è¡¨ã™ $z$ ãŒæ½œåœ¨å¤‰æ•°ã§ã‚ã‚Šã€EMç®—æ³•ã¯ã“ã® $z$ ã‚’æ¨å®šã—ãªãŒã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta = \{\mu_k, \sigma_k, \pi_k\}$ ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** EMç®—æ³•ãŒã€Œè¦‹ãˆãªã„å¤‰æ•°ã‚’æ¨å®šã™ã‚‹ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹ã“ã¨ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç†è«–ã®æ·±ã¿ã«å…¥ã£ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã—ã¦ç†è§£ã™ã‚‹

### 1.1 ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã‚’è§¦ã‚‹

Zone 0ã§è¦‹ãŸã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ« (GMM: Gaussian Mixture Model) ã‚’ã‚‚ã†å°‘ã—è©³ã—ãè§¦ã£ã¦ã¿ã‚ˆã†ã€‚

$$
p(x \mid \theta) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x \mid \mu_k, \sigma_k^2), \quad \sum_{k=1}^{K} \pi_k = 1, \quad \pi_k \geq 0
$$

| è¨˜å· | èª­ã¿ | æ„å‘³ |
|:-----|:-----|:-----|
| $K$ | ã‚±ãƒ¼ | æ··åˆæˆåˆ†ã®æ•° |
| $\pi_k$ | ãƒ‘ã‚¤ ã‚±ãƒ¼ | ç¬¬ $k$ æˆåˆ†ã®æ··åˆé‡ã¿ï¼ˆäº‹å‰ç¢ºç‡ï¼‰ |
| $\mu_k$ | ãƒŸãƒ¥ãƒ¼ ã‚±ãƒ¼ | ç¬¬ $k$ æˆåˆ†ã®å¹³å‡ |
| $\sigma_k^2$ | ã‚·ã‚°ãƒ ã‚±ãƒ¼ äºŒä¹— | ç¬¬ $k$ æˆåˆ†ã®åˆ†æ•£ |
| $\mathcal{N}(x \mid \mu, \sigma^2)$ | ãƒãƒ¼ãƒãƒ« | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ç¢ºç‡å¯†åº¦é–¢æ•° |

æ··åˆé‡ã¿ $\pi_k$ ã®å€¤ã‚’å¤‰ãˆã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ã®ã€Œåã‚Šã€ãŒå¤‰ã‚ã‚‹:

```python
import numpy as np

def gmm_pdf(x, mus, sigmas, pis):
    """Gaussian Mixture Model PDF.

    corresponds to: p(x|Î¸) = Î£_k Ï€_k N(x|Î¼_k, Ïƒ_kÂ²)
    """
    pdf = np.zeros_like(x)
    for mu, sigma, pi in zip(mus, sigmas, pis):
        pdf += pi * np.exp(-0.5 * ((x - mu) / sigma)**2) / (sigma * np.sqrt(2 * np.pi))
    return pdf

x = np.linspace(-8, 12, 500)
mus = [-2.0, 3.0, 7.0]
sigmas = [1.0, 1.5, 0.8]

# Different mixing weights
configs = [
    ([0.33, 0.34, 0.33], "Equal weights"),
    ([0.7, 0.2, 0.1],   "Dominant left"),
    ([0.1, 0.1, 0.8],   "Dominant right"),
    ([0.05, 0.9, 0.05],  "Dominant center"),
]

for pis, label in configs:
    pdf = gmm_pdf(x, mus, sigmas, pis)
    peak_x = x[np.argmax(pdf)]
    print(f"Ï€={pis} ({label:16s}) | peak at x={peak_x:.1f}, max_density={pdf.max():.4f}")
```

å‡ºåŠ›:
```
Ï€=[0.33, 0.34, 0.33] (Equal weights   ) | peak at x=7.0, max_density=0.1646
Ï€=[0.7, 0.2, 0.1]    (Dominant left    ) | peak at x=-2.0, max_density=0.2797
Ï€=[0.1, 0.1, 0.8]    (Dominant right   ) | peak at x=7.0, max_density=0.3989
Ï€=[0.05, 0.9, 0.05]  (Dominant center  ) | peak at x=3.0, max_density=0.2394
```

**æ··åˆé‡ã¿ $\pi_k$ ã‚’å¤‰ãˆã‚‹ã ã‘ã§ã€å¯†åº¦ã®ãƒ”ãƒ¼ã‚¯ä½ç½®ã¨å½¢çŠ¶ãŒå¤§ããå¤‰ã‚ã‚‹ã€‚** è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã™ã‚‹ã®ãŒEMç®—æ³•ã®ä»•äº‹ã ã€‚

### 1.2 EMã®åå¾©éç¨‹ã‚’å¯è¦–åŒ–ã™ã‚‹

EMç®—æ³•ã®æ ¸å¿ƒã¯ã€ŒE-stepï¼ˆæœŸå¾…å€¤è¨ˆç®—ï¼‰â†’ M-stepï¼ˆæœ€å¤§åŒ–ï¼‰ã€ã®åå¾©ã«ã‚ã‚‹ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã®ã‹ã‚’æ•°å€¤ã§è¿½è·¡ã—ã‚ˆã†ã€‚

```python
import numpy as np

np.random.seed(42)
# True parameters
true_mu = np.array([-2.0, 4.0])
true_sigma = np.array([1.0, 1.5])
true_pi = np.array([0.3, 0.7])

# Generate data
N = 300
z = np.random.choice([0, 1], size=N, p=true_pi)
x = np.where(z == 0,
             np.random.normal(true_mu[0], true_sigma[0], N),
             np.random.normal(true_mu[1], true_sigma[1], N))

# EM with tracking
mu = np.array([0.0, 1.0])  # bad initial guess
sigma = np.array([2.0, 2.0])
pi_k = np.array([0.5, 0.5])

def log_likelihood(x, mu, sigma, pi_k):
    """Compute log-likelihood: Î£_n log Î£_k Ï€_k N(x_n|Î¼_k, Ïƒ_kÂ²)"""
    ll = 0.0
    for n in range(len(x)):
        p = sum(pi_k[k] * np.exp(-0.5*((x[n]-mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
                for k in range(len(mu)))
        ll += np.log(p + 1e-300)
    return ll

print(f"{'Step':>4} | {'mu_0':>7} {'mu_1':>7} | {'sigma_0':>7} {'sigma_1':>7} | {'pi_0':>5} {'pi_1':>5} | {'log-lik':>10}")
print("-" * 80)

for step in range(15):
    ll = log_likelihood(x, mu, sigma, pi_k)
    print(f"{step:4d} | {mu[0]:7.3f} {mu[1]:7.3f} | {sigma[0]:7.3f} {sigma[1]:7.3f} | {pi_k[0]:5.3f} {pi_k[1]:5.3f} | {ll:10.2f}")

    # E-step: Î³(z_nk) = Ï€_k N(x_n|Î¼_k,Ïƒ_kÂ²) / Î£_j Ï€_j N(x_n|Î¼_j,Ïƒ_jÂ²)
    pdf = np.zeros((N, 2))
    for k in range(2):
        pdf[:, k] = pi_k[k] * np.exp(-0.5*((x - mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
    gamma = pdf[:, 1] / (pdf.sum(axis=1) + 1e-300)

    # M-step
    N_k = np.array([(1 - gamma).sum(), gamma.sum()])
    mu[0] = ((1 - gamma) * x).sum() / N_k[0]
    mu[1] = (gamma * x).sum() / N_k[1]
    sigma[0] = np.sqrt(((1 - gamma) * (x - mu[0])**2).sum() / N_k[0])
    sigma[1] = np.sqrt((gamma * (x - mu[1])**2).sum() / N_k[1])
    pi_k = N_k / N

ll = log_likelihood(x, mu, sigma, pi_k)
print(f"{'FINAL':>4} | {mu[0]:7.3f} {mu[1]:7.3f} | {sigma[0]:7.3f} {sigma[1]:7.3f} | {pi_k[0]:5.3f} {pi_k[1]:5.3f} | {ll:10.2f}")
print(f"\nTrue | {true_mu[0]:7.3f} {true_mu[1]:7.3f} | {true_sigma[0]:7.3f} {true_sigma[1]:7.3f} | {true_pi[0]:5.3f} {true_pi[1]:5.3f}")
```

ã“ã“ã§æ³¨ç›®ã—ã¦ã»ã—ã„ã®ã¯ **å¯¾æ•°å°¤åº¦ (log-likelihood) ãŒå˜èª¿ã«å¢—åŠ ã—ã¦ã„ã‚‹** ã“ã¨ã ã€‚ã“ã‚Œã¯å¶ç„¶ã§ã¯ãªã„ã€‚EMç®—æ³•ã®ç†è«–çš„ä¿è¨¼ã§ã‚ã‚Šã€Zone 3 ã§å³å¯†ã«è¨¼æ˜ã™ã‚‹ã€‚

:::message
ã“ã“ã§å¤šãã®äººãŒæ··ä¹±ã™ã‚‹ã®ãŒã€Œãªãœç›´æ¥æœ€å°¤æ¨å®šã—ãªã„ã®ã‹ã€ã ã€‚ç­”ãˆã¯å˜ç´”ã§ã€$\log \sum_k \pi_k \mathcal{N}(x \mid \mu_k, \sigma_k^2)$ ã® $\log$ ã®ä¸­ã« $\sum$ ãŒã‚ã‚‹ãŸã‚ã€å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦è§£æçš„ã«å¾®åˆ†ã—ã¦ã‚¼ãƒ­ã¨ç½®ãã“ã¨ãŒã§ããªã„ã€‚EMç®—æ³•ã¯ã“ã®å›°é›£ã‚’æ½œåœ¨å¤‰æ•°ã®å°å…¥ã§å›é¿ã™ã‚‹ã€‚
:::

### 1.3 LLMã®éš ã‚Œå±¤ â€” Transformerã®æ½œåœ¨å¤‰æ•°çš„è§£é‡ˆ

æœ¬ã‚·ãƒªãƒ¼ã‚ºã§ã¯å„è¬›ç¾©ã§LLM/Transformerã¨ã®æ¥ç‚¹ã‚’ç¤ºã™ã€‚ç¬¬8å›ã®ãƒ†ãƒ¼ãƒã€Œæ½œåœ¨å¤‰æ•°ã€ã¯ã€Transformerã®éš ã‚Œå±¤ã¨ç›´çµã—ã¦ã„ã‚‹ã€‚

Transformerã®å„å±¤ã§è¨ˆç®—ã•ã‚Œã‚‹éš ã‚ŒçŠ¶æ…‹ $\mathbf{h}_l \in \mathbb{R}^d$ ã¯ã€å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®ã€Œæ½œåœ¨çš„ãªè¡¨ç¾ã€ã :

$$
\mathbf{h}_l = \text{TransformerLayer}_l(\mathbf{h}_{l-1}), \quad l = 1, \ldots, L
$$

å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ $x_1, \ldots, x_T$ ã¯è¦³æ¸¬å¤‰æ•°ã€‚éš ã‚ŒçŠ¶æ…‹ $\mathbf{h}_1, \ldots, \mathbf{h}_L$ ã¯æ½œåœ¨å¤‰æ•°ã€‚ã“ã®æ§‹é€ ã¯æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ãã®ã‚‚ã®ã ã€‚

```python
import numpy as np

# Simplified transformer hidden state computation
def transformer_layer(h_prev, W_attn, W_ff):
    """One transformer layer: attention + feedforward.

    h_l = FFN(Attention(h_{l-1})) â€” simplified, no LayerNorm/residual
    """
    # Self-attention (simplified): softmax(h @ W_attn @ h.T) @ h
    scores = h_prev @ W_attn @ h_prev.T
    scores = scores - scores.max(axis=-1, keepdims=True)
    weights = np.exp(scores) / np.exp(scores).sum(axis=-1, keepdims=True)
    h_attn = weights @ h_prev

    # Feedforward
    h_out = np.tanh(h_attn @ W_ff)
    return h_out

# 3 tokens, hidden dim 4, 2 layers
np.random.seed(42)
seq_len, d_model = 3, 4
h_0 = np.random.randn(seq_len, d_model)  # input embeddings (observed)

print("Layer 0 (observed input):")
print(np.round(h_0, 3))

for layer in range(1, 3):
    W_attn = np.random.randn(d_model, d_model) * 0.5
    W_ff = np.random.randn(d_model, d_model) * 0.5
    h_0 = transformer_layer(h_0, W_attn, W_ff)
    print(f"\nLayer {layer} (latent representation):")
    print(np.round(h_0, 3))
```

**å…¥åŠ›ï¼ˆè¦³æ¸¬ï¼‰ã‹ã‚‰éš ã‚Œå±¤ï¼ˆæ½œåœ¨ï¼‰ã¸ã®å¤‰æ›ã€‚** ã“ã‚Œã“ãæ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®æœ¬è³ªã ã€‚VAE [^2] ã¯ã€ã“ã®æ½œåœ¨è¡¨ç¾ã«ç¢ºç‡çš„ãªæ§‹é€ ã‚’ä¸ãˆã‚‹ã“ã¨ã§ã€Œç”Ÿæˆã€ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚ãã®æ©‹æ¸¡ã—ãŒã€ã“ã®ç¬¬8å›ã®æœ€å¤§ã®ç›®çš„ã ã€‚

:::details PyTorch ã® Transformer éš ã‚ŒçŠ¶æ…‹
PyTorch ã§ã¯ `nn.TransformerEncoderLayer` ãŒä¸Šã®ã‚³ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹:

```python
import torch
import torch.nn as nn

layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)
x = torch.randn(1, 10, 512)  # (batch, seq_len, d_model)
h = layer(x)  # latent representation
print(f"Input shape:  {x.shape}")
print(f"Output shape: {h.shape}")
# Both (1, 10, 512) â€” same shape, but h encodes contextual information
```

å…¥åŠ›ã¨å‡ºåŠ›ã®å½¢çŠ¶ã¯åŒã˜ã ãŒã€$\mathbf{h}$ ã«ã¯æ–‡è„ˆæƒ…å ±ãŒå‡ç¸®ã•ã‚Œã¦ã„ã‚‹ã€‚ã“ã‚ŒãŒã€Œæ½œåœ¨è¡¨ç¾ã€ã ã€‚
:::

### 1.4 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨

| æ•°å¼ | ã‚³ãƒ¼ãƒ‰ | æ„å‘³ |
|:-----|:-------|:-----|
| $p(x \mid \theta) = \sum_k \pi_k \mathcal{N}(x \mid \mu_k, \sigma_k^2)$ | `pdf += pi[k] * norm.pdf(x, mu[k], sigma[k])` | GMMå¯†åº¦ |
| $\gamma(z_{nk}) = \frac{\pi_k \mathcal{N}(x_n \mid \mu_k, \sigma_k^2)}{\sum_j \pi_j \mathcal{N}(x_n \mid \mu_j, \sigma_j^2)}$ | `gamma = pdf[:, k] / pdf.sum(axis=1)` | è²¬ä»»åº¦ï¼ˆE-stepï¼‰ |
| $\mu_k^{\text{new}} = \frac{\sum_n \gamma(z_{nk}) x_n}{\sum_n \gamma(z_{nk})}$ | `mu[k] = (gamma * x).sum() / gamma.sum()` | å¹³å‡æ›´æ–°ï¼ˆM-stepï¼‰ |
| $\pi_k^{\text{new}} = \frac{N_k}{N}$ | `pi[k] = gamma.sum() / N` | é‡ã¿æ›´æ–°ï¼ˆM-stepï¼‰ |

**æ•°å¼ã®å„è¨˜å·ãŒã‚³ãƒ¼ãƒ‰ã®å„è¡Œã«1å¯¾1ã§å¯¾å¿œã™ã‚‹ã€‚** ã“ã®å¯¾å¿œã‚’æ„è­˜ã—ãªãŒã‚‰ã€Zone 3 ã§æ•°å¼ã‚’å®Œå…¨ã«å°å‡ºã™ã‚‹ã€‚

```mermaid
graph TD
    A["è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ xâ‚,...,xâ‚™"] --> B["åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸â°"]
    B --> C["E-step<br/>Î³(zâ‚™â‚–) = è²¬ä»»åº¦è¨ˆç®—"]
    C --> D["M-step<br/>Î¸^new = ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°"]
    D --> E{"åæŸï¼Ÿ"}
    E -->|No| C
    E -->|Yes| F["æœ€çµ‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î¸*"]

    style C fill:#e3f2fd
    style D fill:#fff3e0
    style F fill:#c8e6c9
```

> **Zone 1 ã¾ã¨ã‚**: GMMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰ãˆã¦æŒ™å‹•ã‚’ä½“æ„Ÿã—ã€EMç®—æ³•ã®åå¾©éç¨‹ã‚’æ•°å€¤ã§è¿½è·¡ã—ã€Transformerã®éš ã‚Œå±¤ãŒæ½œåœ¨å¤‰æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã€‚æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œã‚’æ‰‹ã«å…¥ã‚ŒãŸã€‚

:::message
**é€²æ—: 10% å®Œäº†** ä½“é¨“ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚æ½œåœ¨å¤‰æ•°ã¨EMç®—æ³•ã®ç›´æ„Ÿã‚’æ´ã‚“ã ã€‚æ¬¡ã¯ã€Œãªãœæ½œåœ¨å¤‰æ•°ãŒå¿…è¦ãªã®ã‹ã€ã‚’æ·±ãç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœæ½œåœ¨å¤‰æ•°ãŒå¿…è¦ãªã®ã‹

### 2.1 è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã ã‘ã§ã¯ä¸ååˆ†ãªç†ç”±

ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€ç›´æ¥è¦³æ¸¬ã§ããªã„ã€Œéš ã‚ŒãŸåŸå› ã€ãŒã»ã¼å¿…ãšå­˜åœ¨ã™ã‚‹ã€‚

- æ‰‹æ›¸ãæ•°å­—ç”»åƒ â†’ ã€Œã©ã®æ•°å­—ã‚’æ›¸ã“ã†ã¨ã—ãŸã‹ã€ã¯è¦‹ãˆãªã„
- éŸ³å£°æ³¢å½¢ â†’ ã€Œã©ã®éŸ³ç´ ã‚’ç™ºå£°ä¸­ã‹ã€ã¯ç›´æ¥è¦³æ¸¬ã§ããªã„
- é¡§å®¢è³¼è²·å±¥æ­´ â†’ ã€Œã©ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«å±ã™ã‚‹ã‹ã€ã¯ãƒ©ãƒ™ãƒ«ãŒãªã„
- ãƒ†ã‚­ã‚¹ãƒˆã®å˜èªåˆ— â†’ ã€Œãƒˆãƒ”ãƒƒã‚¯ã€ã¯æ˜ç¤ºã•ã‚Œã¦ã„ãªã„

ã“ã‚Œã‚‰ã®éš ã‚ŒãŸåŸå› ã‚’æ•°å­¦çš„ã«æ‰±ã†æ çµ„ã¿ãŒ **æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«** ã ã€‚

> **ä¸€è¨€ã§è¨€ãˆã°**: æ½œåœ¨å¤‰æ•° = ã€Œãƒ‡ãƒ¼ã‚¿ã®è£ã«ã‚ã‚‹è¦‹ãˆãªã„åŸå› ã‚’è¡¨ã™ç¢ºç‡å¤‰æ•°ã€

æ•°å¼ã§æ›¸ãã¨:

$$
p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x} \mid \mathbf{z}, \theta) \, p(\mathbf{z} \mid \theta)
$$

é€£ç¶šã®å ´åˆã¯ $\sum$ ã‚’ $\int$ ã«ç½®ãæ›ãˆã‚‹:

$$
p(\mathbf{x} \mid \theta) = \int p(\mathbf{x} \mid \mathbf{z}, \theta) \, p(\mathbf{z} \mid \theta) \, d\mathbf{z}
$$

**ã“ã®ç©åˆ†ï¼ˆå‘¨è¾ºåŒ–ï¼‰ãŒè¨ˆç®—å›°é›£ã§ã‚ã‚‹ã¨ã„ã†äº‹å®ŸãŒã€EMç®—æ³•ã‚’å¿…è¦ã¨ã™ã‚‹æ ¹æœ¬çš„ãªç†ç”±ã ã€‚**

### 2.2 ç¬¬7å›ã‹ã‚‰ã®æ¥ç¶š â€” æœ€å°¤æ¨å®šã®é™ç•Œ

ç¬¬7å›ã§å­¦ã‚“ã æœ€å°¤æ¨å®š (MLE) ã‚’æŒ¯ã‚Šè¿”ã‚ã†ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’æ¨å®šã™ã‚‹ã«ã¯å¯¾æ•°å°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹:

$$
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \sum_{n=1}^{N} \log p(x_n \mid \theta)
$$

å˜ä¸€ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãªã‚‰ã€$\log$ ã®ä¸­èº«ãŒ $\mathcal{N}(x_n \mid \mu, \sigma^2)$ ã ã‹ã‚‰è§£æçš„ã«è§£ã‘ã‚‹ã€‚ã ãŒGMMã§ã¯:

$$
\log p(x_n \mid \theta) = \log \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x_n \mid \mu_k, \sigma_k^2)
$$

**$\log$ ã®ä¸­ã« $\sum$ ãŒã‚ã‚‹ã€‚** ã“ã‚ŒãŒå…¨ã¦ã®å›°é›£ã®å…ƒå‡¶ã ã€‚$\log$ ã¨ $\sum$ ã¯äº¤æ›ã§ããªã„ã‹ã‚‰ã€$\frac{\partial}{\partial \mu_k} \log \sum_k (\cdots) = 0$ ã‚’è§£æçš„ã«è§£ãã“ã¨ãŒã§ããªã„ã€‚

```python
import numpy as np

# Single Gaussian: log-likelihood has clean derivative
# d/dÎ¼ log N(x|Î¼,ÏƒÂ²) = (x - Î¼) / ÏƒÂ²  â†’ set to 0 â†’ Î¼ = xÌ„ (sample mean)

x = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
mu_mle = x.mean()
print(f"Single Gaussian MLE: Î¼ = {mu_mle:.1f} (just the sample mean!)")

# GMM: log Î£_k Ï€_k N(x|Î¼_k,Ïƒ_kÂ²) â€” no closed-form solution
# The log-sum structure prevents analytic optimization
def gmm_log_likelihood(x, mus, sigmas, pis):
    """log p(x|Î¸) = Î£_n log Î£_k Ï€_k N(x_n|Î¼_k,Ïƒ_kÂ²)"""
    ll = 0.0
    for xn in x:
        p = sum(pi * np.exp(-0.5*((xn-mu)/sig)**2)/(sig*np.sqrt(2*np.pi))
                for mu, sig, pi in zip(mus, sigmas, pis))
        ll += np.log(p)
    return ll

# Try different Î¼ values â€” no single formula gives the answer
for mu0 in [-3, -2, -1, 0]:
    ll = gmm_log_likelihood(x, [mu0, 5.0], [1.0, 1.0], [0.5, 0.5])
    print(f"GMM log-lik with Î¼â‚€={mu0:3d}: {ll:.4f}  (no closed-form for optimal Î¼â‚€)")
```

### 2.3 Course I ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬ã®ä½ç½®ã¥ã‘

æœ¬è¬›ç¾©ã¯ Course Iã€Œæ•°å­¦åŸºç¤ç·¨ã€ã®æœ€çµ‚å›ã ã€‚8å›ã®æ•°å­¦ã®æ—…ã‚’ä¿¯ç°ã—ã‚ˆã†ã€‚

```mermaid
graph TD
    L1["ç¬¬1å›: æ¦‚è«–<br/>æ•°å¼ãƒªãƒ†ãƒ©ã‚·ãƒ¼"] --> L2["ç¬¬2å›: ç·šå½¢ä»£æ•° I<br/>ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—"]
    L2 --> L3["ç¬¬3å›: ç·šå½¢ä»£æ•° II<br/>SVDãƒ»è¡Œåˆ—å¾®åˆ†"]
    L3 --> L4["ç¬¬4å›: ç¢ºç‡è«–<br/>åˆ†å¸ƒãƒ»ãƒ™ã‚¤ã‚º"]
    L4 --> L5["ç¬¬5å›: æ¸¬åº¦è«–<br/>å³å¯†ãªç¢ºç‡"]
    L5 --> L6["ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–<br/>KLãƒ»SGD"]
    L6 --> L7["ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–<br/>æ¨å®šé‡ã®æ•°å­¦çš„åŸºç›¤"]
    L7 --> L8["ç¬¬8å›: æ½œåœ¨å¤‰æ•° & EMç®—æ³•<br/>â˜… Course I ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬"]
    L8 -->|"EMã®é™ç•Œ: äº‹å¾Œåˆ†å¸ƒãŒ<br/>è§£æçš„ã«è¨ˆç®—ä¸èƒ½"| L9["ç¬¬9å›: å¤‰åˆ†æ¨è«– & ELBO<br/>âš¡ Juliaåˆç™»å ´"]

    style L8 fill:#ff9800,color:#fff
    style L9 fill:#4caf50,color:#fff
```

| Course I è¬›ç¾© | ä½•ã‚’ç²å¾—ã—ãŸã‹ | ä½•ãŒã€Œè¶³ã‚Šãªã„ã€ã‹ |
|:-------------|:-------------|:----------------|
| ç¬¬1å›: æ¦‚è«– | æ•°å¼ã®èª­ã¿æ–¹ | ç·šå½¢ä»£æ•°ã®é“å…·ãŒå¿…è¦ |
| ç¬¬2å›: ç·šå½¢ä»£æ•° I | ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã€è¡Œåˆ—æ¼”ç®— | åˆ†è§£ã¨å¾®åˆ†ãŒå¿…è¦ |
| ç¬¬3å›: ç·šå½¢ä»£æ•° II | SVDã€è¡Œåˆ—å¾®åˆ†ã€Backprop | ä¸ç¢ºå®Ÿæ€§ã®æ‰±ã„ãŒå¿…è¦ |
| ç¬¬4å›: ç¢ºç‡è«– | ç¢ºç‡åˆ†å¸ƒã€ãƒ™ã‚¤ã‚ºã®å®šç† | å³å¯†ãªç¢ºç‡è«–ãŒå¿…è¦ |
| ç¬¬5å›: æ¸¬åº¦è«– | Lebesgueç©åˆ†ã€ç¢ºç‡éç¨‹ | åˆ†å¸ƒé–“ã®è·é›¢ãŒå¿…è¦ |
| ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ– | KLã€SGDã€Adam | ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’ãŒå¿…è¦ |
| ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«– | æœ€å°¤æ¨å®šã€æ¨å®šé‡ã®åˆ†é¡ä½“ç³» | æ½œåœ¨å¤‰æ•°ã®æ‰±ã„ãŒå¿…è¦ |
| **ç¬¬8å›: EMç®—æ³•** | **æ½œåœ¨å¤‰æ•°ã®æ¨å®š** | **äº‹å¾Œåˆ†å¸ƒã®è¿‘ä¼¼ãŒå¿…è¦ â†’ ç¬¬9å›ã¸** |

**å„è¬›ç¾©ã®ã€Œé™ç•Œã€ãŒæ¬¡ã®è¬›ç¾©ã®ã€Œå‹•æ©Ÿã€ã«ãªã‚‹ã€‚** ãã—ã¦ç¬¬8å›ã®é™ç•Œ â€” EMç®—æ³•ã§ã¯äº‹å¾Œåˆ†å¸ƒ $p(\mathbf{z} \mid \mathbf{x}, \theta)$ ãŒè§£æçš„ã«è¨ˆç®—ã§ããªã„ã‚±ãƒ¼ã‚¹ã«å¯¾å¿œã§ããªã„ â€” ãŒã€ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã®å‹•æ©Ÿã«ãªã‚‹ã€‚

### 2.4 æ¾å°¾ç ”ã¨ã®å¯¾æ¯”

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚ºï¼ˆç¬¬8å›ï¼‰ |
|:-----|:-----------|:----------------|
| EMç®—æ³• | ã€ŒEMãŒã‚ã‚Šã¾ã™ã€ç¨‹åº¦ã®ç´¹ä»‹ | **å®Œå…¨å°å‡º**: Jensenä¸ç­‰å¼ â†’ ELBO â†’ E-step/M-step â†’ åæŸè¨¼æ˜ |
| GMM | çµæœã®ã¿ | è²¬ä»»åº¦ã®å°å‡ºã€Singularityå•é¡Œã€BIC/AIC |
| HMM | è¨€åŠãªã— | Forward-Backwardã€Viterbiã€Baum-Welch |
| VAEã¸ã®æ©‹ | å”çªã«VAE | EM â†’ Variational EM â†’ ELBO â†’ VAE ã¸ã®è‡ªç„¶ãªæ¥ç¶š |
| Pythoné€Ÿåº¦ | æ¸¬å®šãªã— | Profileçµæœ: **ã€Œé…ã™ããªã„ï¼Ÿã€** â†’ ç¬¬9å›Juliaç™»å ´ã®ä¼ç·š |

### 2.5 3ã¤ã®æ¯”å–©ã§æ‰ãˆã‚‹ã€Œæ½œåœ¨å¤‰æ•°ã€

**æ¯”å–©1: æ°·å±±**

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã¯æ°´é¢ä¸Šã®æ°·å±±ã®ä¸€è§’ã€‚æ½œåœ¨å¤‰æ•°ã¯æ°´é¢ä¸‹ã®å·¨å¤§ãªæ§‹é€ ã€‚ãƒ‡ãƒ¼ã‚¿ã®è£ã«ã‚ã‚‹æ§‹é€ ã‚’æ¨å®šã™ã‚‹ã“ã¨ã¯ã€æ°´é¢ä¸Šã®å½¢çŠ¶ã‹ã‚‰æ°´é¢ä¸‹ã®å…¨ä½“åƒã‚’å¾©å…ƒã™ã‚‹ã“ã¨ã«ç­‰ã—ã„ã€‚

**æ¯”å–©2: çŠ¯ç½ªæœæŸ»**

ç¾å ´ã®è¨¼æ‹ ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ï¼‰ã‹ã‚‰çŠ¯äººï¼ˆæ½œåœ¨å¤‰æ•° $\mathbf{z}$ï¼‰ã‚’æ¨å®šã™ã‚‹ã€‚è¨¼æ‹ ã¯ç›´æ¥è¦‹ãˆã‚‹ãŒã€çŠ¯äººã¯è¦‹ãˆãªã„ã€‚EMç®—æ³•ã¯ã€Œã¾ãšçŠ¯äººã®å€™è£œã‚’çµã‚Šï¼ˆE-stepï¼‰ã€æ¬¡ã«è¨¼æ‹ ã¨ã®æ•´åˆæ€§ã‚’æœ€å¤§åŒ–ã™ã‚‹ï¼ˆM-stepï¼‰ã€ã‚’ç¹°ã‚Šè¿”ã™æœæŸ»æ‰‹æ³•ã ã€‚

**æ¯”å–©3: æ¥½è­œã®å¾©å…ƒ**

æ¼”å¥ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’è´ã„ã¦ã€æ¥½è­œï¼ˆæ½œåœ¨æ§‹é€ ï¼‰ã‚’å¾©å…ƒã™ã‚‹ã€‚å„æ¥½å™¨ãŒä½•ã‚’å¼¾ã„ã¦ã„ã‚‹ã‹ï¼ˆæ½œåœ¨å¤‰æ•°ï¼‰ã¯ç›´æ¥è¦‹ãˆãªã„ãŒã€æ··åˆéŸ³ï¼ˆè¦³æ¸¬ï¼‰ã‹ã‚‰æ¨å®šã§ãã‚‹ã€‚ã“ã‚Œã¯éŸ³æºåˆ†é›¢å•é¡Œã§ã‚ã‚Šã€ã¾ã•ã«GMMã®å¿œç”¨ã ã€‚

### 2.6 Trojan Horse â€” Python ã®é™ç•ŒãŒè¦‹ãˆå§‹ã‚ã‚‹

:::details Trojan Horse: Pythoné€Ÿåº¦ã®ä¼ç·š
Course Iã¯å…¨ç·¨Pythonã ãŒã€æœ¬è¬›ç¾©ã§ã€Œã‚ã‚Œã€é…ããªã„ã‹ï¼Ÿã€ã¨ã„ã†ç–‘å¿µãŒèŠ½ç”Ÿãˆã‚‹ã€‚

EMç®—æ³•ã®å„åå¾©ã§å…¨ãƒ‡ãƒ¼ã‚¿ $N$ å€‹ã«å¯¾ã—ã¦è²¬ä»»åº¦ $\gamma(z_{nk})$ ã‚’è¨ˆç®—ã™ã‚‹ã€‚$K$ å€‹ã®æˆåˆ†ã€$T$ å›ã®åå¾©ã§ $O(NKT)$ å›ã®å¯†åº¦è¨ˆç®—ãŒå¿…è¦ã ã€‚

```python
import numpy as np
import time

np.random.seed(42)
N = 10000
K = 5
x = np.concatenate([np.random.normal(k * 3, 1.0, N // K) for k in range(K)])

mu = np.random.randn(K)
sigma = np.ones(K)
pi_k = np.ones(K) / K

start = time.perf_counter()
for step in range(100):
    # E-step
    pdf = np.zeros((N, K))
    for k in range(K):
        pdf[:, k] = pi_k[k] * np.exp(-0.5*((x - mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
    gamma = pdf / pdf.sum(axis=1, keepdims=True)

    # M-step
    N_k = gamma.sum(axis=0)
    for k in range(K):
        mu[k] = (gamma[:, k] * x).sum() / N_k[k]
        sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / N_k[k])
    pi_k = N_k / N

elapsed = time.perf_counter() - start
print(f"EM (N={N}, K={K}, 100 iterations): {elapsed:.3f} sec")
print(f"Per iteration: {elapsed/100*1000:.1f} ms")
```

ã€Œ100åå¾©ã§æ•°ç§’ï¼Ÿ ã“ã‚Œã€ãƒ‡ãƒ¼ã‚¿ãŒ100ä¸‡ä»¶ã«ãªã£ãŸã‚‰......ï¼Ÿã€

ã“ã®ç–‘å¿µãŒç¬¬9å›ã§çˆ†ç™ºã™ã‚‹ã€‚ELBOè¨ˆç®—ã®Pythonå®Ÿè¡Œæ™‚é–“ã‚’è¨ˆæ¸¬ã—ãŸç¬é–“ã€Juliaã®è¡æ’ƒçš„ãªé€Ÿåº¦ãŒå¾…ã£ã¦ã„ã‚‹ã€‚**è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ã€‚**
:::

> **Zone 2 ã¾ã¨ã‚**: æ½œåœ¨å¤‰æ•°ãŒå¿…è¦ãªç†ç”±ï¼ˆ$\log \sum$ ã®å›°é›£æ€§ï¼‰ã‚’ç†è§£ã—ã€Course I å…¨ä½“ã®ä¸­ã§ã®ç¬¬8å›ã®ä½ç½®ã¥ã‘ã‚’ç¢ºèªã—ã€EMç®—æ³•ãŒã€Œè¦‹ãˆãªã„åŸå› ã®æ¨å®šã€ã§ã‚ã‚‹ã“ã¨ã‚’3ã¤ã®æ¯”å–©ã§æ´ã‚“ã ã€‚

:::message
**é€²æ—: 20% å®Œäº†** ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚ã€Œãªãœæ½œåœ¨å¤‰æ•°ãŒå¿…è¦ã‹ã€ã€ŒãªãœEMç®—æ³•ãŒå¿…è¦ã‹ã€ã®å‹•æ©Ÿã‚’æ·±ãç†è§£ã—ãŸã€‚ã„ã‚ˆã„ã‚ˆæ•°å¼ä¿®è¡Œã«å…¥ã‚‹ã€‚æº–å‚™ã¯ã„ã„ã§ã™ã‹ï¼Ÿ
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” EMç®—æ³•ã®å®Œå…¨å°å‡º

ã“ã“ãŒæœ¬è¬›ç¾©ã®æ ¸å¿ƒã ã€‚Zone 0-1 ã§ã€Œå‹•ãã€ã“ã¨ã‚’ä½“æ„Ÿã—ãŸã€‚Zone 2 ã§ã€Œãªãœå¿…è¦ã‹ã€ã‚’ç†è§£ã—ãŸã€‚ã“ã“ã‹ã‚‰ã¯ã€Œãªãœå‹•ãã®ã‹ã€ã‚’æ•°å­¦çš„ã«è¨¼æ˜ã™ã‚‹ã€‚

**è¦šãˆã‚‹ãªã€‚å°å‡ºã—ã‚ã€‚** çµæœã‚’æš—è¨˜ã—ã¦ã‚‚å¿œç”¨ã§ããªã„ã€‚å°å‡ºéç¨‹ã‚’è‡ªåŠ›ã§å†ç¾ã§ãã¦ã¯ã˜ã‚ã¦ã€æ–°ã—ã„å•é¡Œã«é©ç”¨ã§ãã‚‹ã€‚

```mermaid
graph TD
    A["3.1 æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®å®šå¼åŒ–"] --> B["3.2 å®Œå…¨/ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦"]
    B --> C["3.3 Jensenä¸ç­‰å¼"]
    C --> D["3.4 ELBOåˆ†è§£"]
    D --> E["3.5 EMç®—æ³•ã®å°å‡º"]
    E --> F["3.6 GMM E-step/M-step"]
    F --> G["3.7 åæŸæ€§è¨¼æ˜"]
    G --> H["3.8 âš”ï¸ Boss Battle"]

    style A fill:#e3f2fd
    style H fill:#ff5722,color:#fff
```

### 3.1 æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®å®šå¼åŒ–

ã¾ãšè¨˜æ³•ã‚’æ•´ç†ã™ã‚‹ã€‚ç´™ã¨ãƒšãƒ³ã‚’ç”¨æ„ã—ã¦ã»ã—ã„ã€‚

**è¨­å®š**:
- è¦³æ¸¬å¤‰æ•°: $\mathbf{x} \in \mathcal{X}$ â€” å®Ÿéš›ã«æ¸¬å®šã§ãã‚‹ãƒ‡ãƒ¼ã‚¿
- æ½œåœ¨å¤‰æ•°: $\mathbf{z} \in \mathcal{Z}$ â€” ç›´æ¥è¦³æ¸¬ã§ããªã„éš ã‚ŒãŸå¤‰æ•°
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: $\theta \in \Theta$ â€” æ¨å®šã—ãŸã„ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

**åŒæ™‚åˆ†å¸ƒ** (joint distribution):

$$
p(\mathbf{x}, \mathbf{z} \mid \theta)
$$

ã“ã‚ŒãŒã€Œå®Œå…¨ãƒ‡ãƒ¼ã‚¿ã€(complete data) ã®åˆ†å¸ƒã ã€‚$\mathbf{x}$ ã¨ $\mathbf{z}$ ã®ä¸¡æ–¹ãŒè¦³æ¸¬ã•ã‚Œã¦ã„ã‚Œã°ã€ã“ã®åˆ†å¸ƒã‚’ç›´æ¥æ‰±ãˆã‚‹ã€‚

**å‘¨è¾ºå°¤åº¦** (marginal likelihood / evidence):

$$
p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)
$$

$\mathbf{z}$ ãŒé€£ç¶šã®å ´åˆã¯:

$$
p(\mathbf{x} \mid \theta) = \int p(\mathbf{x}, \mathbf{z} \mid \theta) \, d\mathbf{z}
$$

**äº‹å¾Œåˆ†å¸ƒ** (posterior distribution):

$$
p(\mathbf{z} \mid \mathbf{x}, \theta) = \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{p(\mathbf{x} \mid \theta)} = \frac{p(\mathbf{x} \mid \mathbf{z}, \theta) \, p(\mathbf{z} \mid \theta)}{p(\mathbf{x} \mid \theta)}
$$

ã“ã‚Œã¯ãƒ™ã‚¤ã‚ºã®å®šç†ãã®ã‚‚ã®ã ï¼ˆç¬¬4å›ã§å­¦ã‚“ã ï¼‰ã€‚åˆ†æ¯ã® $p(\mathbf{x} \mid \theta)$ ãŒè¨ˆç®—å›°é›£ã§ã‚ã‚‹ã“ã¨ãŒã€å…¨ã¦ã®å›°é›£ã®æºæ³‰ã«ãªã‚‹ã€‚

| ç”¨èª | æ•°å¼ | ç›´æ„Ÿ |
|:-----|:-----|:-----|
| å®Œå…¨ãƒ‡ãƒ¼ã‚¿å°¤åº¦ | $p(\mathbf{x}, \mathbf{z} \mid \theta)$ | ã€Œè¦³æ¸¬ã€ã¨ã€Œéš ã‚Œã€ã®ä¸¡æ–¹ãŒã‚ã‹ã£ã¦ã„ã‚Œã°ç°¡å˜ |
| å‘¨è¾ºå°¤åº¦ (evidence) | $p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)$ | éš ã‚Œã‚’æ¶ˆã™ã¨è¨ˆç®—å›°é›£ |
| äº‹å¾Œåˆ†å¸ƒ | $p(\mathbf{z} \mid \mathbf{x}, \theta)$ | è¦³æ¸¬ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®éš ã‚Œã®æ¨å®š |
| è²¬ä»»åº¦ | $\gamma(z_{nk}) = p(z_n = k \mid x_n, \theta)$ | ãƒ‡ãƒ¼ã‚¿ $x_n$ ãŒæˆåˆ† $k$ ã‹ã‚‰æ¥ãŸç¢ºç‡ |

```python
import numpy as np

# Concrete example: GMM with K=2
# Joint: p(x, z=k|Î¸) = Ï€_k N(x|Î¼_k, Ïƒ_kÂ²)
# Marginal: p(x|Î¸) = Î£_k Ï€_k N(x|Î¼_k, Ïƒ_kÂ²)
# Posterior: p(z=k|x,Î¸) = Ï€_k N(x|Î¼_k,Ïƒ_kÂ²) / Î£_j Ï€_j N(x|Î¼_j,Ïƒ_jÂ²)

mu = np.array([-2.0, 3.0])
sigma = np.array([1.0, 1.5])
pi_k = np.array([0.4, 0.6])

def gaussian_pdf(x, mu, sigma):
    """N(x|Î¼,ÏƒÂ²) = (2Ï€ÏƒÂ²)^{-1/2} exp(-(x-Î¼)Â²/(2ÏƒÂ²))"""
    return np.exp(-0.5 * ((x - mu) / sigma)**2) / (sigma * np.sqrt(2 * np.pi))

x_test = np.array([0.0, -2.0, 3.0, 5.0])

print("x     | p(x,z=0|Î¸) | p(x,z=1|Î¸) | p(x|Î¸)  | p(z=0|x,Î¸) | p(z=1|x,Î¸)")
print("-" * 75)
for x_val in x_test:
    joint_0 = pi_k[0] * gaussian_pdf(x_val, mu[0], sigma[0])
    joint_1 = pi_k[1] * gaussian_pdf(x_val, mu[1], sigma[1])
    marginal = joint_0 + joint_1
    post_0 = joint_0 / marginal
    post_1 = joint_1 / marginal
    print(f"{x_val:5.1f} | {joint_0:10.6f} | {joint_1:10.6f} | {marginal:7.5f} | "
          f"{post_0:10.4f} | {post_1:10.4f}")
```

å‡ºåŠ›:
```
x     | p(x,z=0|Î¸) | p(x,z=1|Î¸) | p(x|Î¸)  | p(z=0|x,Î¸) | p(z=1|x,Î¸)
---------------------------------------------------------------------------
  0.0 |   0.048394 |   0.035994 | 0.08439 |     0.5734 |     0.4266
 -2.0 |   0.159155 |   0.006569 | 0.16572 |     0.9604 |     0.0396
  3.0 |   0.000036 |   0.159155 | 0.15919 |     0.0002 |     0.9998
  5.0 |   0.000000 |   0.064759 | 0.06476 |     0.0000 |     1.0000
```

**$x = -2$ ã®ãƒ‡ãƒ¼ã‚¿ã¯ 96% ã®ç¢ºç‡ã§æˆåˆ†0ã‹ã‚‰ã€$x = 3$ ã®ãƒ‡ãƒ¼ã‚¿ã¯ 99.98% ã®ç¢ºç‡ã§æˆåˆ†1ã‹ã‚‰æ¥ãŸ** ã¨æ¨å®šã•ã‚Œã‚‹ã€‚ã“ã‚ŒãŒäº‹å¾Œåˆ†å¸ƒ $p(z \mid x, \theta)$ ã®æ„å‘³ã ã€‚

### 3.2 å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã¨ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®å›°é›£æ€§

**å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦** (complete-data log-likelihood):

$\mathbf{x}$ ã¨ $\mathbf{z}$ ã®ä¸¡æ–¹ãŒè¦³æ¸¬ã•ã‚Œã¦ã„ã‚‹å ´åˆ:

$$
\log p(\mathbf{x}, \mathbf{z} \mid \theta) = \log p(\mathbf{x} \mid \mathbf{z}, \theta) + \log p(\mathbf{z} \mid \theta)
$$

GMMã®å ´åˆã€$z_n = k$ ãŒã‚ã‹ã£ã¦ã„ã‚Œã°:

$$
\log p(\mathbf{x}, \mathbf{z} \mid \theta) = \sum_{n=1}^{N} \sum_{k=1}^{K} \mathbb{1}[z_n = k] \left( \log \pi_k + \log \mathcal{N}(x_n \mid \mu_k, \sigma_k^2) \right)
$$

ã“ã“ã§ $\mathbb{1}[z_n = k]$ ã¯æŒ‡ç¤ºé–¢æ•°ï¼ˆ$z_n = k$ ãªã‚‰1ã€ãã†ã§ãªã‘ã‚Œã°0ï¼‰ã€‚**$\log$ ã®ä¸­èº«ãŒå˜ä¸€ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒãªã®ã§ã€å¾®åˆ†ã—ã¦ã‚¼ãƒ­ã¨ç½®ã‘ã‚‹ã€‚** ã¤ã¾ã‚Šè§£æè§£ãŒå­˜åœ¨ã™ã‚‹ã€‚

**ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦** (incomplete-data log-likelihood):

$\mathbf{z}$ ãŒè¦³æ¸¬ã•ã‚Œãªã„å ´åˆ:

$$
\log p(\mathbf{x} \mid \theta) = \log \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)
$$

**$\log$ ã®ä¸­ã« $\sum$ ãŒã‚ã‚‹ã€‚** ã“ã‚ŒãŒè§£æè§£ã‚’é˜»ã‚€ã€‚

```python
import numpy as np

# Complete-data case: z is known â†’ closed-form MLE
np.random.seed(42)
N = 100
z_true = np.array([0]*40 + [1]*60)
x = np.where(z_true == 0,
             np.random.normal(-2, 1, N),
             np.random.normal(3, 1.5, N))

# When z is known, MLE is trivial
mask0 = (z_true == 0)
mask1 = (z_true == 1)
mu_mle = np.array([x[mask0].mean(), x[mask1].mean()])
sigma_mle = np.array([x[mask0].std(), x[mask1].std()])
pi_mle = np.array([mask0.sum() / N, mask1.sum() / N])

print("=== Complete data (z known) â†’ closed-form MLE ===")
print(f"Î¼ = ({mu_mle[0]:.3f}, {mu_mle[1]:.3f})")
print(f"Ïƒ = ({sigma_mle[0]:.3f}, {sigma_mle[1]:.3f})")
print(f"Ï€ = ({pi_mle[0]:.2f}, {pi_mle[1]:.2f})")
print("\nNo iteration needed! Just sample statistics.")
print("\n=== Incomplete data (z unknown) â†’ need EM ===")
print("Cannot compute sample statistics per component")
print("because we don't know which component each x_n belongs to.")
```

:::message
ã“ã“ãŒå…¨ã¦ã®ã‚«ã‚®ã ã€‚**$z$ ãŒã‚ã‹ã£ã¦ã„ã‚Œã°ç°¡å˜ã«è§£ã‘ã‚‹ã€‚$z$ ãŒã‚ã‹ã‚‰ãªã„ã‹ã‚‰é›£ã—ã„ã€‚** EMç®—æ³•ã¯ã€Œ$z$ ãŒã‚ã‹ã‚‰ãªã„ãªã‚‰ã€æ¨å®šã—ã¦ã—ã¾ãˆã€ã¨ã„ã†ç™ºæƒ³ã§ã€ã“ã®å›°é›£ã‚’å›é¿ã™ã‚‹ã€‚
:::

### 3.3 Jensenä¸ç­‰å¼ â€” EMç®—æ³•ã®æ•°å­¦çš„åŸºç›¤

EMç®—æ³•ã®ç†è«–çš„åŸºç›¤ã¯ **Jensenä¸ç­‰å¼** (Jensen's inequality) ã ã€‚ç¬¬5å›ã§æ¸¬åº¦è«–ã‚’å­¦ã‚“ã èª­è€…ã«ã¯é¦´æŸ“ã¿ãŒã‚ã‚‹ã ã‚ã†ã€‚

:::message alert
Jensenä¸ç­‰å¼ã®å‘ãã‚’é–“é•ãˆã‚‹äººãŒéå¸¸ã«å¤šã„ã€‚å‡¸é–¢æ•°ã¨å‡¹é–¢æ•°ã§ä¸ç­‰å·ã®å‘ããŒé€†è»¢ã™ã‚‹ã€‚ç´™ã«æ›¸ã„ã¦ç¢ºèªã—ã¦ã»ã—ã„ã€‚
:::

**å®šç† (Jensenä¸ç­‰å¼)**:  $f$ ãŒå‡¹é–¢æ•° (concave function) ã®ã¨ã:

$$
f\left( \mathbb{E}[X] \right) \geq \mathbb{E}[f(X)]
$$

$\log$ ã¯å‡¹é–¢æ•°ã ã‹ã‚‰:

$$
\log \mathbb{E}[X] \geq \mathbb{E}[\log X]
$$

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**: $f$ ãŒå‡¹é–¢æ•°ã§ã‚ã‚‹ã¨ã¯ã€ä»»æ„ã® $x_1, x_2$ ã¨ $\lambda \in [0, 1]$ ã«å¯¾ã—ã¦ $f(\lambda x_1 + (1-\lambda) x_2) \geq \lambda f(x_1) + (1-\lambda) f(x_2)$ ãŒæˆã‚Šç«‹ã¤ã“ã¨ã ã€‚ã“ã‚Œã‚’æœ‰é™å€‹ã®ç‚¹ã«æ‹¡å¼µã™ã‚‹ã¨ $f(\sum_i \lambda_i x_i) \geq \sum_i \lambda_i f(x_i)$ ($\sum_i \lambda_i = 1$) ã¨ãªã‚Šã€æœŸå¾…å€¤ã®å®šç¾©ã¨çµ„ã¿åˆã‚ã›ã‚Œã°Jensenä¸ç­‰å¼ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

```python
import numpy as np

# Verify Jensen's inequality for log (concave function)
# log(E[X]) >= E[log(X)]

np.random.seed(42)
X = np.random.exponential(2.0, 10000)  # positive random variable

lhs = np.log(np.mean(X))       # log(E[X])
rhs = np.mean(np.log(X))       # E[log(X)]
gap = lhs - rhs

print(f"log(E[X]) = {lhs:.6f}")
print(f"E[log(X)] = {rhs:.6f}")
print(f"Gap       = {gap:.6f} >= 0 âœ“ (Jensen's inequality)")
print(f"\nFor constant X (no gap):")
X_const = np.full(10000, 3.0)
print(f"log(E[X]) = {np.log(np.mean(X_const)):.6f}")
print(f"E[log(X)] = {np.mean(np.log(X_const)):.6f}")
print(f"Gap       = {np.log(np.mean(X_const)) - np.mean(np.log(X_const)):.6f} (equality when constant)")
```

**ç­‰å·æ¡ä»¶**: $X$ ãŒå®šæ•°ã®ã¨ãï¼ˆåˆ†æ•£ãŒã‚¼ãƒ­ã®ã¨ãï¼‰ã€Jensenä¸ç­‰å¼ã¯ç­‰å·ã«ãªã‚‹ã€‚ã“ã‚ŒãŒEMç®—æ³•ã®åæŸãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç†è§£ã™ã‚‹éµã«ãªã‚‹ã€‚

### 3.4 ELBOåˆ†è§£ â€” EMç®—æ³•ã®å¿ƒè‡“éƒ¨

ã„ã‚ˆã„ã‚ˆEMç®—æ³•ã®æ ¸å¿ƒã«åˆ°é”ã™ã‚‹ã€‚ã“ã“ã‹ã‚‰å…ˆã¯ä¸€è¡Œä¸€è¡Œã€ç´™ã®ä¸Šã§è¿½ã£ã¦ã»ã—ã„ã€‚

**ç›®æ¨™**: ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ $\log p(\mathbf{x} \mid \theta)$ ã®ä¸‹ç•Œ (lower bound) ã‚’æ§‹æˆã™ã‚‹ã€‚

$q(\mathbf{z})$ ã‚’ $\mathbf{z}$ ä¸Šã®ä»»æ„ã®ç¢ºç‡åˆ†å¸ƒã¨ã™ã‚‹ã€‚ä»¥ä¸‹ã®åˆ†è§£ãŒæˆã‚Šç«‹ã¤:

$$
\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]
$$

ã“ã“ã§:

$$
\mathcal{L}(q, \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})}
$$

$$
\text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)] = -\sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{z} \mid \mathbf{x}, \theta)}{q(\mathbf{z})}
$$

**ã“ã® $\mathcal{L}(q, \theta)$ ãŒ ELBO (Evidence Lower BOund) ã ã€‚**

:::message
ã“ã®åˆ†è§£ã¯ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã§ä¸»å½¹ã«ãªã‚‹ã€‚ã“ã“ã§ã¯EMç®—æ³•ã®å°å‡ºã«å¿…è¦ãªéƒ¨åˆ†ã ã‘ã‚’æ‰±ã†ã€‚
:::

**å°å‡º** â€” ä¸€è¡Œãšã¤è¿½ã†:

Step 1: å¯¾æ•°å°¤åº¦ã‚’å¤‰å½¢ã™ã‚‹ã€‚

$$
\log p(\mathbf{x} \mid \theta) = \log p(\mathbf{x} \mid \theta) \cdot \underbrace{\sum_{\mathbf{z}} q(\mathbf{z})}_{= 1}
$$

$q(\mathbf{z})$ ã¯ç¢ºç‡åˆ†å¸ƒã ã‹ã‚‰å’ŒãŒ1ã€‚ã“ã‚Œã‚’åˆ©ç”¨ã™ã‚‹ã€‚

Step 2: $\log$ ã®ä¸­ã« $q(\mathbf{z})$ ã‚’å°å…¥ã™ã‚‹ã€‚

$$
\log p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) \log p(\mathbf{x} \mid \theta)
$$

$\log p(\mathbf{x} \mid \theta)$ ã¯ $\mathbf{z}$ ã«ä¾å­˜ã—ãªã„ã‹ã‚‰ã€$\sum$ ã®ä¸­ã«å…¥ã‚Œã‚‰ã‚Œã‚‹ã€‚

Step 3: $p(\mathbf{x} \mid \theta) = \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{p(\mathbf{z} \mid \mathbf{x}, \theta)}$ ã‚’ä»£å…¥ã™ã‚‹ï¼ˆãƒ™ã‚¤ã‚ºã®å®šç†ã®å¤‰å½¢ï¼‰ã€‚

$$
= \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{p(\mathbf{z} \mid \mathbf{x}, \theta)}
$$

Step 4: $q(\mathbf{z})$ ã‚’åˆ†å­åˆ†æ¯ã«æŒ¿å…¥ã™ã‚‹ï¼ˆ$\times \frac{q(\mathbf{z})}{q(\mathbf{z})} = 1$ï¼‰ã€‚

$$
= \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta) \cdot q(\mathbf{z})}{p(\mathbf{z} \mid \mathbf{x}, \theta) \cdot q(\mathbf{z})}
$$

Step 5: å¯¾æ•°ã®å•†ã‚’åˆ†è§£ã™ã‚‹ã€‚

$$
= \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})} + \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{q(\mathbf{z})}{p(\mathbf{z} \mid \mathbf{x}, \theta)}
$$

$$
= \underbrace{\sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})}}_{\mathcal{L}(q, \theta) \text{ (ELBO)}} + \underbrace{\text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]}_{\geq 0}
$$

**KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯å¸¸ã«éè² ** (Gibbsã®ä¸ç­‰å¼ã€ç¬¬6å›) ã ã‹ã‚‰:

$$
\log p(\mathbf{x} \mid \theta) \geq \mathcal{L}(q, \theta)
$$

$\mathcal{L}(q, \theta)$ ã¯å¯¾æ•°å°¤åº¦ã® **ä¸‹ç•Œ** ã ã€‚ã ã‹ã‚‰ Evidence **Lower** Bound ã¨å‘¼ã°ã‚Œã‚‹ã€‚

```python
import numpy as np

# Numerical verification of ELBO decomposition
# log p(x|Î¸) = L(q,Î¸) + KL[q||p(z|x,Î¸)]

# GMM with K=2
mu = np.array([-2.0, 3.0])
sigma = np.array([1.0, 1.5])
pi_k = np.array([0.4, 0.6])

x_val = 1.0

# Compute p(x|Î¸) = Î£_k Ï€_k N(x|Î¼_k,Ïƒ_kÂ²)
def norm_pdf(x, mu, sigma):
    return np.exp(-0.5*((x-mu)/sigma)**2) / (sigma * np.sqrt(2*np.pi))

px = sum(pi_k[k] * norm_pdf(x_val, mu[k], sigma[k]) for k in range(2))
log_px = np.log(px)

# True posterior: p(z=k|x,Î¸) = Ï€_k N(x|Î¼_k,Ïƒ_kÂ²) / p(x|Î¸)
p_z_given_x = np.array([pi_k[k] * norm_pdf(x_val, mu[k], sigma[k]) / px for k in range(2)])

# Choose q(z) different from true posterior
q_z = np.array([0.7, 0.3])  # arbitrary distribution

# ELBO: L(q,Î¸) = Î£_k q(k) log [Ï€_k N(x|Î¼_k,Ïƒ_kÂ²) / q(k)]
elbo = sum(q_z[k] * np.log(pi_k[k] * norm_pdf(x_val, mu[k], sigma[k]) / q_z[k]) for k in range(2))

# KL[q||p(z|x,Î¸)] = Î£_k q(k) log [q(k) / p(z=k|x,Î¸)]
kl = sum(q_z[k] * np.log(q_z[k] / p_z_given_x[k]) for k in range(2))

print(f"log p(x|Î¸)     = {log_px:.6f}")
print(f"ELBO L(q,Î¸)    = {elbo:.6f}")
print(f"KL[q||p(z|x)]  = {kl:.6f}")
print(f"ELBO + KL      = {elbo + kl:.6f}  (should equal log p(x|Î¸))")
print(f"Gap (KL >= 0)  = {kl:.6f} >= 0 âœ“")

# When q = true posterior â†’ KL = 0, ELBO = log p(x|Î¸)
print(f"\nWhen q = true posterior:")
elbo_tight = sum(p_z_given_x[k] * np.log(pi_k[k] * norm_pdf(x_val, mu[k], sigma[k]) / p_z_given_x[k]) for k in range(2))
kl_tight = sum(p_z_given_x[k] * np.log(p_z_given_x[k] / p_z_given_x[k]) for k in range(2))
print(f"ELBO (tight)   = {elbo_tight:.6f}")
print(f"KL (tight)     = {kl_tight:.6f}  (â‰ˆ 0 âœ“)")
```

:::details Jensenä¸ç­‰å¼ã‹ã‚‰ã®ELBOå°å‡ºï¼ˆåˆ¥è§£ï¼‰
ä¸Šã®å°å‡ºã¯ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’ä½¿ã£ãŸãŒã€Jensenä¸ç­‰å¼ã‹ã‚‰ç›´æ¥å°å‡ºã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹:

$$
\log p(\mathbf{x} \mid \theta) = \log \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)
$$

$q(\mathbf{z})$ ã‚’å°å…¥:

$$
= \log \sum_{\mathbf{z}} q(\mathbf{z}) \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})}
$$

$$
= \log \mathbb{E}_{q(\mathbf{z})} \left[ \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})} \right]
$$

Jensenä¸ç­‰å¼ï¼ˆ$\log$ ã¯å‡¹é–¢æ•°ï¼‰ã‚’é©ç”¨:

$$
\geq \mathbb{E}_{q(\mathbf{z})} \left[ \log \frac{p(\mathbf{x}, \mathbf{z} \mid \theta)}{q(\mathbf{z})} \right] = \mathcal{L}(q, \theta)
$$

ã“ã®å°å‡ºã®æ–¹ãŒçŸ­ã„ãŒã€KLé …ã¨ã®é–¢ä¿‚ãŒè¦‹ãˆã«ãã„ã€‚ä¸Šã®ãƒ™ã‚¤ã‚ºã®å®šç†ã‚’ä½¿ã†å°å‡ºã®æ–¹ãŒã€EMç®—æ³•ã®æ§‹é€ ãŒæ˜å¿«ã«ãªã‚‹ã€‚
:::

> **ã“ã“ãŒæœ¬è¬›ç¾©æœ€å¤§ã®ãƒã‚¤ãƒ³ãƒˆ**: $\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q \| p(\mathbf{z} \mid \mathbf{x}, \theta)]$ã€‚ã“ã®åˆ†è§£ãŒEMç®—æ³•ã®å…¨ã¦ã‚’æ”¯ãˆã¦ã„ã‚‹ã€‚

### 3.5 EMç®—æ³•ã®å°å‡º â€” 2ã‚¹ãƒ†ãƒƒãƒ—ã®å¤©æ‰çš„æ§‹é€ 

ELBOåˆ†è§£ã‚’ã‚‚ã†ä¸€åº¦æ›¸ã:

$$
\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]
$$

å·¦è¾º $\log p(\mathbf{x} \mid \theta)$ ã‚’æœ€å¤§åŒ–ã—ãŸã„ã€‚å³è¾ºã¯2é …ã®å’Œã ã€‚

**E-step**: $q(\mathbf{z})$ ã«ã¤ã„ã¦ $\mathcal{L}(q, \theta)$ ã‚’æœ€å¤§åŒ–ã™ã‚‹ï¼ˆ$\theta$ ã¯å›ºå®šï¼‰ã€‚

KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯éè² ã§ã€$q(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{x}, \theta)$ ã®ã¨ãã€ã‹ã¤ãã®ã¨ãã«é™ã‚Šã‚¼ãƒ­ã«ãªã‚‹ã€‚ã—ãŸãŒã£ã¦:

$$
q^*(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})
$$

ã“ã®ã¨ã $\text{KL} = 0$ ã¨ãªã‚Šã€ELBO ãŒå¯¾æ•°å°¤åº¦ã«ä¸€è‡´ã™ã‚‹: $\mathcal{L}(q^*, \theta^{(t)}) = \log p(\mathbf{x} \mid \theta^{(t)})$ã€‚

**M-step**: $\theta$ ã«ã¤ã„ã¦ $\mathcal{L}(q^*, \theta)$ ã‚’æœ€å¤§åŒ–ã™ã‚‹ï¼ˆ$q = q^*$ ã¯å›ºå®šï¼‰ã€‚

$q^* = p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})$ ã‚’ä»£å…¥ã™ã‚‹ã¨:

$$
\mathcal{L}(q^*, \theta) = \sum_{\mathbf{z}} p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)}) \log p(\mathbf{x}, \mathbf{z} \mid \theta) - \underbrace{\sum_{\mathbf{z}} p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)}) \log p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})}_{\text{entropy, } \theta \text{ã«ä¾å­˜ã—ãªã„}}
$$

$\theta$ ã«ä¾å­˜ã™ã‚‹ã®ã¯ç¬¬1é …ã ã‘ã ã‹ã‚‰:

$$
\theta^{(t+1)} = \arg\max_\theta \underbrace{\mathbb{E}_{\mathbf{z} \sim p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})} [\log p(\mathbf{x}, \mathbf{z} \mid \theta)]}_{Q(\theta, \theta^{(t)})}
$$

ã“ã® $Q(\theta, \theta^{(t)})$ ãŒ **Qé–¢æ•°** ã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã ã€‚Dempster, Laird, Rubin (1977) [^1] ã¯ã“ã®é–¢æ•°ã‚’ä¸­å¿ƒã«EMç®—æ³•ã‚’å®šå¼åŒ–ã—ãŸã€‚

**ã¾ã¨ã‚ã‚‹ã¨**:

| ã‚¹ãƒ†ãƒƒãƒ— | æ“ä½œ | æ•°å¼ |
|:---------|:-----|:-----|
| **E-step** | äº‹å¾Œåˆ†å¸ƒã‚’è¨ˆç®— | $q(\mathbf{z}) \leftarrow p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})$ |
| **M-step** | Qé–¢æ•°ã‚’æœ€å¤§åŒ– | $\theta^{(t+1)} \leftarrow \arg\max_\theta Q(\theta, \theta^{(t)})$ |

```mermaid
sequenceDiagram
    participant E as E-step
    participant M as M-step
    participant L as log p(x|Î¸)

    Note over E,L: Iteration t
    E->>E: q(z) = p(z|x, Î¸^(t))
    Note over E: KL â†’ 0, ELBO = log p(x|Î¸^(t))
    E->>M: Pass q(z) to M-step
    M->>M: Î¸^(t+1) = argmax Q(Î¸, Î¸^(t))
    Note over M: ELBO increases
    M->>L: log p(x|Î¸^(t+1)) â‰¥ log p(x|Î¸^(t))
    Note over E,L: Iteration t+1
    L->>E: Use Î¸^(t+1) for next E-step
```

```python
import numpy as np

# EM algorithm as coordinate ascent on ELBO
# Demonstrating that log-likelihood never decreases

np.random.seed(42)
N = 200
z_true = np.random.choice([0, 1], size=N, p=[0.4, 0.6])
x = np.where(z_true == 0, np.random.normal(-2, 1, N), np.random.normal(3, 1.5, N))

mu = np.array([0.0, 1.0])
sigma = np.array([2.0, 2.0])
pi_k = np.array([0.5, 0.5])

def compute_log_likelihood(x, mu, sigma, pi_k):
    N = len(x)
    K = len(mu)
    ll = 0.0
    for n in range(N):
        p_xn = sum(pi_k[k] * np.exp(-0.5*((x[n]-mu[k])/sigma[k])**2)
                   / (sigma[k]*np.sqrt(2*np.pi)) for k in range(K))
        ll += np.log(p_xn + 1e-300)
    return ll

def compute_elbo(x, mu, sigma, pi_k, gamma):
    """ELBO = Î£_n Î£_k Î³_nk [log Ï€_k + log N(x_n|Î¼_k,Ïƒ_kÂ²) - log Î³_nk]"""
    N, K = gamma.shape
    elbo = 0.0
    for n in range(N):
        for k in range(K):
            if gamma[n, k] > 1e-300:
                log_pdf = -0.5*np.log(2*np.pi) - np.log(sigma[k]) - 0.5*((x[n]-mu[k])/sigma[k])**2
                elbo += gamma[n, k] * (np.log(pi_k[k]) + log_pdf - np.log(gamma[n, k]))
    return elbo

print(f"{'Step':>4} | {'log p(x|Î¸)':>12} | {'ELBO':>12} | {'KL':>10} | {'Î” log-lik':>10}")
print("-" * 65)

prev_ll = compute_log_likelihood(x, mu, sigma, pi_k)

for step in range(10):
    # E-step
    K = len(mu)
    pdf = np.zeros((N, K))
    for k in range(K):
        pdf[:, k] = pi_k[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
    gamma = pdf / (pdf.sum(axis=1, keepdims=True) + 1e-300)

    # After E-step: KL = 0, ELBO = log-likelihood
    ll = compute_log_likelihood(x, mu, sigma, pi_k)
    elbo = compute_elbo(x, mu, sigma, pi_k, gamma)
    kl = ll - elbo

    print(f"{step:4d} | {ll:12.4f} | {elbo:12.4f} | {kl:10.6f} | {ll - prev_ll:10.4f}")

    # M-step
    N_k = gamma.sum(axis=0)
    for k in range(K):
        mu[k] = (gamma[:, k] * x).sum() / N_k[k]
        sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / N_k[k])
    pi_k = N_k / N

    prev_ll = ll

print(f"\nKey observation: Î” log-lik >= 0 at every step (monotone increase)")
```

:::message
ã“ã“ã§å¤šãã®äººãŒå¼•ã£ã‹ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆ: **E-stepã®å¾Œã€KLã¯æ­£ç¢ºã«ã‚¼ãƒ­ã«ãªã‚‹**ï¼ˆ$q = p(\mathbf{z} \mid \mathbf{x}, \theta)$ ã ã‹ã‚‰ï¼‰ã€‚**M-stepã®å¾Œã€KLã¯å†ã³ã‚¼ãƒ­ã§ãªããªã‚‹**ï¼ˆ$\theta$ ãŒå¤‰ã‚ã£ãŸã‹ã‚‰ $q \neq p(\mathbf{z} \mid \mathbf{x}, \theta^{\text{new}})$ï¼‰ã€‚æ¬¡ã®E-stepã§å†ã³KLã‚’ã‚¼ãƒ­ã«ã™ã‚‹ã€‚ã“ã®ç¹°ã‚Šè¿”ã—ãŒå¯¾æ•°å°¤åº¦ã‚’å˜èª¿ã«å¢—åŠ ã•ã›ã‚‹ã€‚
:::

### 3.6 GMMã®E-step / M-step â€” å®Œå…¨å°å‡º

GMMã«å¯¾ã—ã¦EMç®—æ³•ã‚’å…·ä½“çš„ã«é©ç”¨ã—ã‚ˆã†ã€‚å…¨ã¦ã®æ›´æ–°å¼ã‚’ä¸€è¡Œãšã¤å°å‡ºã™ã‚‹ã€‚

**E-step**: è²¬ä»»åº¦ $\gamma(z_{nk})$ ã®è¨ˆç®—

$$
\gamma(z_{nk}) = p(z_n = k \mid x_n, \theta^{(t)}) = \frac{\pi_k^{(t)} \mathcal{N}(x_n \mid \mu_k^{(t)}, (\sigma_k^{(t)})^2)}{\sum_{j=1}^{K} \pi_j^{(t)} \mathcal{N}(x_n \mid \mu_j^{(t)}, (\sigma_j^{(t)})^2)}
$$

ã“ã‚Œã¯ãƒ™ã‚¤ã‚ºã®å®šç†ãã®ã‚‚ã®ã ã€‚åˆ†å­ã¯ã€Œæˆåˆ† $k$ ã‹ã‚‰ $x_n$ ãŒç”Ÿæˆã•ã‚Œã‚‹ç¢ºç‡ã€ã€åˆ†æ¯ã¯ã€Œå…¨æˆåˆ†ã‹ã‚‰ã®ç¢ºç‡ã®å’Œã€ã€‚

**M-step**: Qé–¢æ•°ã®æœ€å¤§åŒ–

Qé–¢æ•°ã‚’æ›¸ãä¸‹ã™:

$$
Q(\theta, \theta^{(t)}) = \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma(z_{nk}) \left[ \log \pi_k + \log \mathcal{N}(x_n \mid \mu_k, \sigma_k^2) \right]
$$

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¯¾æ•°å¯†åº¦ã‚’å±•é–‹ã™ã‚‹:

$$
\log \mathcal{N}(x_n \mid \mu_k, \sigma_k^2) = -\frac{1}{2} \log(2\pi) - \log \sigma_k - \frac{(x_n - \mu_k)^2}{2\sigma_k^2}
$$

**$\mu_k$ ã®æ›´æ–°**: $\frac{\partial Q}{\partial \mu_k} = 0$ ã‚’è§£ãã€‚

$$
\frac{\partial Q}{\partial \mu_k} = \sum_{n=1}^{N} \gamma(z_{nk}) \frac{x_n - \mu_k}{\sigma_k^2} = 0
$$

$$
\sum_{n=1}^{N} \gamma(z_{nk}) x_n = \mu_k \sum_{n=1}^{N} \gamma(z_{nk})
$$

$N_k = \sum_{n=1}^{N} \gamma(z_{nk})$ ã¨å®šç¾©ã™ã‚‹ã¨:

$$
\boxed{\mu_k^{(t+1)} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) \, x_n}
$$

**ã€Œè²¬ä»»åº¦ã§é‡ã¿ä»˜ã‘ã—ãŸå¹³å‡ã€** â€” ç›´æ„Ÿçš„ã«ã‚‚è‡ªç„¶ã ã€‚

**$\sigma_k^2$ ã®æ›´æ–°**: $\frac{\partial Q}{\partial \sigma_k^2} = 0$ ã‚’è§£ãã€‚

$\sigma_k^2 = s$ ã¨ã—ã¦:

$$
\frac{\partial Q}{\partial s} = \sum_{n=1}^{N} \gamma(z_{nk}) \left[ -\frac{1}{2s} + \frac{(x_n - \mu_k)^2}{2s^2} \right] = 0
$$

$$
\sum_{n=1}^{N} \gamma(z_{nk}) \frac{1}{s} = \sum_{n=1}^{N} \gamma(z_{nk}) \frac{(x_n - \mu_k)^2}{s^2}
$$

$$
\boxed{(\sigma_k^{(t+1)})^2 = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) (x_n - \mu_k^{(t+1)})^2}
$$

**ã€Œè²¬ä»»åº¦ã§é‡ã¿ä»˜ã‘ã—ãŸåˆ†æ•£ã€** ã ã€‚

**$\pi_k$ ã®æ›´æ–°**: $\sum_k \pi_k = 1$ ã®åˆ¶ç´„ä»˜ãã§ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•ã‚’ä½¿ã†ã€‚

$$
\mathcal{L}_{\text{Lagrange}} = Q + \lambda \left( 1 - \sum_{k=1}^{K} \pi_k \right)
$$

$$
\frac{\partial}{\partial \pi_k} = \frac{N_k}{\pi_k} - \lambda = 0 \quad \Rightarrow \quad \pi_k = \frac{N_k}{\lambda}
$$

$\sum_k \pi_k = 1$ ã‹ã‚‰ $\lambda = N$:

$$
\boxed{\pi_k^{(t+1)} = \frac{N_k}{N}}
$$

**ã€Œæˆåˆ† $k$ ã«å±ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆã€** ã¨ã„ã†è‡ªç„¶ãªè§£é‡ˆã«ãªã‚‹ã€‚

```python
import numpy as np

# Complete GMM EM with all derived formulas
np.random.seed(42)

# Ground truth
true_params = {
    'mu': np.array([-3.0, 0.0, 4.0]),
    'sigma': np.array([0.8, 1.2, 0.6]),
    'pi': np.array([0.3, 0.4, 0.3])
}

# Generate data
N = 500
K = 3
z_true = np.random.choice(K, size=N, p=true_params['pi'])
x = np.array([np.random.normal(true_params['mu'][z], true_params['sigma'][z]) for z in z_true])

# Initialize
mu = np.array([-1.0, 0.5, 2.0])
sigma = np.array([1.0, 1.0, 1.0])
pi_k = np.ones(K) / K

def norm_pdf(x, mu, sigma):
    return np.exp(-0.5*((x-mu)/sigma)**2) / (sigma * np.sqrt(2*np.pi))

# EM iterations with derived update formulas
for t in range(20):
    # === E-step ===
    # Î³(z_nk) = Ï€_k N(x_n|Î¼_k,Ïƒ_kÂ²) / Î£_j Ï€_j N(x_n|Î¼_j,Ïƒ_jÂ²)
    pdf = np.zeros((N, K))
    for k in range(K):
        pdf[:, k] = pi_k[k] * norm_pdf(x, mu[k], sigma[k])
    gamma = pdf / pdf.sum(axis=1, keepdims=True)

    # === M-step ===
    N_k = gamma.sum(axis=0)  # effective number of points per component

    for k in range(K):
        # Î¼_k = (1/N_k) Î£_n Î³_nk x_n
        mu[k] = (gamma[:, k] * x).sum() / N_k[k]
        # Ïƒ_kÂ² = (1/N_k) Î£_n Î³_nk (x_n - Î¼_k)Â²
        sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / N_k[k])
    # Ï€_k = N_k / N
    pi_k = N_k / N

print("Estimated vs True parameters:")
print(f"Î¼:  est=({mu[0]:6.3f}, {mu[1]:6.3f}, {mu[2]:6.3f})")
print(f"    true=({true_params['mu'][0]:6.3f}, {true_params['mu'][1]:6.3f}, {true_params['mu'][2]:6.3f})")
print(f"Ïƒ:  est=({sigma[0]:6.3f}, {sigma[1]:6.3f}, {sigma[2]:6.3f})")
print(f"    true=({true_params['sigma'][0]:6.3f}, {true_params['sigma'][1]:6.3f}, {true_params['sigma'][2]:6.3f})")
print(f"Ï€:  est=({pi_k[0]:5.3f}, {pi_k[1]:5.3f}, {pi_k[2]:5.3f})")
print(f"    true=({true_params['pi'][0]:5.3f}, {true_params['pi'][1]:5.3f}, {true_params['pi'][2]:5.3f})")
```

### 3.7 EMç®—æ³•ã®åæŸæ€§è¨¼æ˜

EMç®—æ³•ãŒ**å¯¾æ•°å°¤åº¦ã‚’å˜èª¿ã«å¢—åŠ ã•ã›ã‚‹**ã“ã¨ã‚’è¨¼æ˜ã™ã‚‹ã€‚Wu (1983) [^3] ã®åæŸæ€§å®šç†ã®æ ¸å¿ƒéƒ¨åˆ†ã ã€‚

**å®šç† (EMå˜èª¿æ€§)**: EMç®—æ³•ã®å„åå¾©ã§ã€ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã¯éæ¸›å°‘ã§ã‚ã‚‹:

$$
\log p(\mathbf{x} \mid \theta^{(t+1)}) \geq \log p(\mathbf{x} \mid \theta^{(t)})
$$

**è¨¼æ˜**:

ELBOåˆ†è§£ã‚ˆã‚Š:

$$
\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q \| p(\mathbf{z} \mid \mathbf{x}, \theta)]
$$

E-stepã§ $q = p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})$ ã¨è¨­å®šã™ã‚‹ã¨ $\text{KL} = 0$ ã ã‹ã‚‰:

$$
\log p(\mathbf{x} \mid \theta^{(t)}) = \mathcal{L}(q^{(t)}, \theta^{(t)}) \tag{1}
$$

M-stepã§ $\theta^{(t+1)} = \arg\max_\theta \mathcal{L}(q^{(t)}, \theta)$ ã¨ã™ã‚‹ã‹ã‚‰:

$$
\mathcal{L}(q^{(t)}, \theta^{(t+1)}) \geq \mathcal{L}(q^{(t)}, \theta^{(t)}) \tag{2}
$$

ä¸€æ–¹ã€æ–°ã—ã„ $\theta^{(t+1)}$ ã«å¯¾ã—ã¦ã‚‚ ELBOåˆ†è§£ã¯æˆã‚Šç«‹ã¤:

$$
\log p(\mathbf{x} \mid \theta^{(t+1)}) = \mathcal{L}(q^{(t)}, \theta^{(t+1)}) + \underbrace{\text{KL}[q^{(t)} \| p(\mathbf{z} \mid \mathbf{x}, \theta^{(t+1)})]}_{\geq 0} \tag{3}
$$

(3) ã‚ˆã‚Š:

$$
\log p(\mathbf{x} \mid \theta^{(t+1)}) \geq \mathcal{L}(q^{(t)}, \theta^{(t+1)}) \tag{4}
$$

(1), (2), (4) ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨:

$$
\log p(\mathbf{x} \mid \theta^{(t+1)}) \stackrel{(4)}{\geq} \mathcal{L}(q^{(t)}, \theta^{(t+1)}) \stackrel{(2)}{\geq} \mathcal{L}(q^{(t)}, \theta^{(t)}) \stackrel{(1)}{=} \log p(\mathbf{x} \mid \theta^{(t)})
$$

$$
\therefore \log p(\mathbf{x} \mid \theta^{(t+1)}) \geq \log p(\mathbf{x} \mid \theta^{(t)}) \quad \blacksquare
$$

```python
import numpy as np

# Empirical verification of monotone convergence
np.random.seed(42)
N = 300
x = np.concatenate([np.random.normal(-2, 1, 120),
                     np.random.normal(3, 1.5, 180)])

mu = np.array([-5.0, 8.0])  # intentionally bad initialization
sigma = np.array([3.0, 3.0])
pi_k = np.array([0.5, 0.5])

def compute_ll(x, mu, sigma, pi_k):
    ll = 0.0
    for xn in x:
        p = sum(pi_k[k] * np.exp(-0.5*((xn-mu[k])/sigma[k])**2)/(sigma[k]*np.sqrt(2*np.pi))
                for k in range(len(mu)))
        ll += np.log(p + 1e-300)
    return ll

lls = []
for t in range(30):
    lls.append(compute_ll(x, mu, sigma, pi_k))

    pdf = np.zeros((N, 2))
    for k in range(2):
        pdf[:, k] = pi_k[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2)/(sigma[k]*np.sqrt(2*np.pi))
    gamma = pdf / (pdf.sum(axis=1, keepdims=True) + 1e-300)

    N_k = gamma.sum(axis=0)
    for k in range(2):
        mu[k] = (gamma[:, k] * x).sum() / N_k[k]
        sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / N_k[k])
    pi_k = N_k / N

# Verify monotone increase
diffs = [lls[i+1] - lls[i] for i in range(len(lls)-1)]
print(f"All increments >= 0: {all(d >= -1e-10 for d in diffs)}")
print(f"Min increment: {min(diffs):.2e}")
print(f"Max increment: {max(diffs):.4f}")
print(f"Final - Initial: {lls[-1] - lls[0]:.4f}")
print(f"\nConvergence trace (first 10 steps):")
for i in range(min(10, len(lls))):
    print(f"  t={i:2d}: log-lik = {lls[i]:10.4f}" + (f"  (Î” = {diffs[i]:+.4f})" if i < len(diffs) else ""))
```

:::message alert
EMç®—æ³•ã¯**å±€æ‰€æœ€é©è§£**ã«åæŸã™ã‚‹ä¿è¨¼ã—ã‹ãªã„ã€‚å¤§åŸŸæœ€é©è§£ã¸ã®åˆ°é”ã¯ä¿è¨¼ã•ã‚Œã¦ã„ãªã„ã€‚åˆæœŸå€¤ã«ä¾å­˜ã™ã‚‹ãŸã‚ã€å®Ÿå‹™ã§ã¯è¤‡æ•°ã®åˆæœŸå€¤ã§å®Ÿè¡Œã—ã¦æœ€è‰¯ã®çµæœã‚’é¸ã¶ (multiple restarts) ã®ãŒæ¨™æº–çš„ãªå¯¾ç­–ã ã€‚
:::

:::details EMåæŸé€Ÿåº¦ã«ã¤ã„ã¦
EMç®—æ³•ã®åæŸé€Ÿåº¦ã¯ä¸€èˆ¬ã«**ç·šå½¢åæŸ** (linear convergence) ã ã€‚Newtonæ³•ã®ã‚ˆã†ãªäºŒæ¬¡åæŸã§ã¯ãªã„ã€‚å…·ä½“çš„ã«ã¯ã€æƒ…å ±è¡Œåˆ—ã®æ¬ æ¸¬æƒ…å ± (missing information) ã®æ¯”ç‡ãŒåæŸé€Ÿåº¦ã‚’æ”¯é…ã™ã‚‹ã€‚

å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®Fisheræƒ…å ±è¡Œåˆ—ã‚’ $I_c(\theta)$ã€è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®Fisheræƒ…å ±è¡Œåˆ—ã‚’ $I_o(\theta)$ ã¨ã™ã‚‹ã¨ã€EMç®—æ³•ã®åæŸãƒ¬ãƒ¼ãƒˆ $r$ ã¯:

$$
r \approx \lambda_{\max}\left( I_c(\theta^*)^{-1} (I_c(\theta^*) - I_o(\theta^*)) \right)
$$

ã€Œæ¬ æ¸¬æƒ…å ±ãŒå¤šã„ã»ã©åæŸãŒé…ã„ã€â€” ç›´æ„Ÿã«åˆã†çµæœã ã€‚æ¬ æ¸¬ãŒå¤šã„ã»ã©æ½œåœ¨å¤‰æ•°ã®æ¨å®šãŒä¸ç¢ºå®Ÿã«ãªã‚Šã€E-stepã®æƒ…å ±é‡ãŒæ¸›ã‚‹ã‹ã‚‰ã ã€‚
:::

### 3.8 Boss Battle â€” Dempster, Laird, Rubin (1977) ã®Qé–¢æ•°ã‚’å®Œå…¨åˆ†è§£ã™ã‚‹

ã•ã‚ã€ãƒœã‚¹æˆ¦ã ã€‚EMç®—æ³•ã®åŸè«–æ–‡ [^1] ã§å®šç¾©ã•ã‚ŒãŸQé–¢æ•°ã‚’ã€GMMã®å ´åˆã«å®Œå…¨ã«å±•é–‹ã—ã€å…¨ã¦ã®è¨˜å·ã¨æ¬¡å…ƒã‚’è¿½è·¡ã™ã‚‹ã€‚

**ãƒœã‚¹**: Qé–¢æ•°

$$
Q(\theta, \theta^{(t)}) = \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})} \left[ \log p(\mathbf{x}, \mathbf{z} \mid \theta) \right]
$$

**å¤šå¤‰é‡GMMã¸ã®å±•é–‹**:

ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}_n \in \mathbb{R}^D$ã€$K$ å€‹ã®æˆåˆ†ã¨ã™ã‚‹ã€‚

$$
Q(\theta, \theta^{(t)}) = \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma(z_{nk}) \Bigg[ \underbrace{\log \pi_k}_{\text{(A) æ··åˆé‡ã¿}} + \underbrace{\left( -\frac{D}{2}\log(2\pi) - \frac{1}{2}\log|\boldsymbol{\Sigma}_k| - \frac{1}{2}(\mathbf{x}_n - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_n - \boldsymbol{\mu}_k) \right)}_{\text{(B) å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹ã®å¯¾æ•°å¯†åº¦}} \Bigg]
$$

| é … | è¨˜å· | æ¬¡å…ƒ | æ„å‘³ |
|:---|:-----|:-----|:-----|
| (A) | $\log \pi_k$ | ã‚¹ã‚«ãƒ©ãƒ¼ | æˆåˆ† $k$ ã®äº‹å‰ç¢ºç‡ã®å¯¾æ•° |
| (B1) | $-\frac{D}{2}\log(2\pi)$ | ã‚¹ã‚«ãƒ©ãƒ¼ | æ­£è¦åŒ–å®šæ•°ï¼ˆ$\theta$ ã«ä¾å­˜ã—ãªã„ï¼‰ |
| (B2) | $-\frac{1}{2}\log|\boldsymbol{\Sigma}_k|$ | ã‚¹ã‚«ãƒ©ãƒ¼ | å…±åˆ†æ•£è¡Œåˆ—ã®è¡Œåˆ—å¼ã®å¯¾æ•° |
| (B3) | $(\mathbf{x}_n - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}_n - \boldsymbol{\mu}_k)$ | ã‚¹ã‚«ãƒ©ãƒ¼ (äºŒæ¬¡å½¢å¼) | ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã®äºŒä¹— |
| $\gamma(z_{nk})$ | $p(z_n = k \mid x_n, \theta^{(t)})$ | ã‚¹ã‚«ãƒ©ãƒ¼ $\in [0, 1]$ | E-stepã§è¨ˆç®—æ¸ˆã¿ã®è²¬ä»»åº¦ |
| $N$ | ãƒ‡ãƒ¼ã‚¿æ•° | æ•´æ•° | è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®å€‹æ•° |
| $K$ | æˆåˆ†æ•° | æ•´æ•° | æ··åˆæˆåˆ†ã®æ•° |
| $D$ | æ¬¡å…ƒæ•° | æ•´æ•° | ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒ |

**å¤šå¤‰é‡M-stepæ›´æ–°å¼**:

$\frac{\partial Q}{\partial \boldsymbol{\mu}_k} = \mathbf{0}$ ã‚’è§£ãã¨:

$$
\boldsymbol{\mu}_k^{(t+1)} = \frac{\sum_{n=1}^{N} \gamma(z_{nk}) \, \mathbf{x}_n}{\sum_{n=1}^{N} \gamma(z_{nk})} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) \, \mathbf{x}_n
$$

$\frac{\partial Q}{\partial \boldsymbol{\Sigma}_k^{-1}} = \mathbf{0}$ ã‚’è§£ãã¨ï¼ˆè¡Œåˆ—å¾®åˆ† â€” ç¬¬3å›ã§å­¦ã‚“ã æŠ€è¡“ãŒæ´»ãã‚‹ï¼‰:

$$
\boldsymbol{\Sigma}_k^{(t+1)} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) \, (\mathbf{x}_n - \boldsymbol{\mu}_k^{(t+1)})(\mathbf{x}_n - \boldsymbol{\mu}_k^{(t+1)})^\top
$$

```python
import numpy as np

# Multivariate GMM EM â€” Boss Battle implementation
np.random.seed(42)

# 2D data, K=3 components
D, K, N = 2, 3, 500
true_mus = [np.array([-3, -2]), np.array([0, 3]), np.array([4, -1])]
true_covs = [np.array([[1, 0.3],[0.3, 0.8]]),
             np.array([[1.2, -0.5],[-0.5, 1.0]]),
             np.array([[0.6, 0],[0, 0.6]])]
true_pi = [0.3, 0.4, 0.3]

# Generate multivariate data
data = []
z_true = []
for n in range(N):
    k = np.random.choice(K, p=true_pi)
    z_true.append(k)
    data.append(np.random.multivariate_normal(true_mus[k], true_covs[k]))
X = np.array(data)  # (N, D)

# Initialize
mus = [np.random.randn(D) for _ in range(K)]
covs = [np.eye(D) for _ in range(K)]
pis = np.ones(K) / K

def mvn_pdf(x, mu, cov):
    """Multivariate Gaussian PDF: N(x|Î¼,Î£)"""
    D = len(mu)
    diff = x - mu
    cov_inv = np.linalg.inv(cov)
    det = np.linalg.det(cov)
    exponent = -0.5 * diff @ cov_inv @ diff
    norm = 1.0 / ((2 * np.pi)**(D/2) * np.sqrt(det))
    return norm * np.exp(exponent)

# EM iterations
for t in range(30):
    # E-step: Î³(z_nk) = Ï€_k N(x_n|Î¼_k,Î£_k) / Î£_j Ï€_j N(x_n|Î¼_j,Î£_j)
    gamma = np.zeros((N, K))
    for k in range(K):
        for n in range(N):
            gamma[n, k] = pis[k] * mvn_pdf(X[n], mus[k], covs[k])
    gamma /= gamma.sum(axis=1, keepdims=True) + 1e-300

    # M-step
    N_k = gamma.sum(axis=0)
    for k in range(K):
        # Î¼_k = (1/N_k) Î£_n Î³_nk x_n
        mus[k] = (gamma[:, k:k+1] * X).sum(axis=0) / N_k[k]
        # Î£_k = (1/N_k) Î£_n Î³_nk (x_n - Î¼_k)(x_n - Î¼_k)^T
        diff = X - mus[k]  # (N, D)
        covs[k] = (gamma[:, k:k+1] * diff).T @ diff / N_k[k]
    pis = N_k / N

print("=== Boss Battle Result: Multivariate GMM EM ===\n")
for k in range(K):
    print(f"Component {k}:")
    print(f"  Î¼_est  = [{mus[k][0]:6.3f}, {mus[k][1]:6.3f}]")
    print(f"  Î¼_true = [{true_mus[k][0]:6.3f}, {true_mus[k][1]:6.3f}]")
    print(f"  Ï€_est  = {pis[k]:.3f},  Ï€_true = {true_pi[k]:.3f}")
    print(f"  Î£_est  = [[{covs[k][0,0]:.3f}, {covs[k][0,1]:.3f}],")
    print(f"             [{covs[k][1,0]:.3f}, {covs[k][1,1]:.3f}]]")
    print()
```

:::message
ãƒœã‚¹æ’ƒç ´ã€‚Qé–¢æ•°ã‚’å…¨ã¦ã®é …ã«åˆ†è§£ã—ã€å¤šå¤‰é‡GMMã®æ›´æ–°å¼ã‚’å°å‡ºãƒ»å®Ÿè£…ã—ãŸã€‚ã“ã“ã§ç²å¾—ã—ãŸæŠ€è¡“ã¯:
1. Qé–¢æ•°ã®æ§‹é€ ç†è§£ï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã®æœŸå¾…å€¤ï¼‰
2. è¡Œåˆ—å¾®åˆ†ã«ã‚ˆã‚‹å¤šå¤‰é‡æ›´æ–°å¼ã®å°å‡ºï¼ˆç¬¬3å›ã®çŸ¥è­˜ãŒæ´»ããŸï¼‰
3. è²¬ä»»åº¦ â†’ é‡ã¿ä»˜ãçµ±è¨ˆé‡ã¨ã„ã†è¨ˆç®—ãƒ‘ã‚¿ãƒ¼ãƒ³
:::

### 3.9 EMã®å¹¾ä½•å­¦çš„è§£é‡ˆ â€” e-å°„å½±ã¨m-å°„å½±

EMç®—æ³•ã«ã¯ç¾ã—ã„å¹¾ä½•å­¦çš„è§£é‡ˆãŒã‚ã‚‹ã€‚æƒ…å ±å¹¾ä½•å­¦ï¼ˆAmari, 1985ï¼‰ã®è¦–ç‚¹ã‹ã‚‰è¦‹ã‚‹ã¨ã€EMç®—æ³•ã¯çµ±è¨ˆå¤šæ§˜ä½“ä¸Šã® **äº¤äº’å°„å½±** (alternating projection) ã ã€‚

ç¢ºç‡åˆ†å¸ƒã®ç©ºé–“ã‚’è€ƒãˆã‚ˆã†ã€‚ã“ã®ç©ºé–“ã«ã¯2ã¤ã®é‡è¦ãªéƒ¨åˆ†å¤šæ§˜ä½“ãŒã‚ã‚‹:

- **e-æ—** (exponential family): æŒ‡æ•°å‹åˆ†å¸ƒæ—ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¼µã‚‰ã‚Œã‚‹å¤šæ§˜ä½“
- **m-æ—** (mixture family): æ··åˆåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¼µã‚‰ã‚Œã‚‹å¤šæ§˜ä½“

$$
\text{E-step} = \text{m-å°„å½±}: q \to p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})
$$

$$
\text{M-step} = \text{e-å°„å½±}: \theta \to \arg\max_\theta Q(\theta, \theta^{(t)})
$$

Neal & Hinton (1998) [^5] ã¯ã“ã®è¦–ç‚¹ã‚’è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æœ€å°åŒ–ã¨ã—ã¦å†å®šå¼åŒ–ã—ãŸã€‚EMç®—æ³•ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ $F(q, \theta) = -\mathcal{L}(q, \theta)$ ã‚’ $q$ ã¨ $\theta$ ã«ã¤ã„ã¦äº¤äº’ã«æœ€å°åŒ–ã™ã‚‹åº§æ¨™é™ä¸‹æ³•ã«ä»–ãªã‚‰ãªã„ã€‚

```python
import numpy as np

# Geometric view: EM as coordinate descent on free energy
# F(q, Î¸) = -L(q, Î¸) = -Î£_z q(z) log [p(x,z|Î¸)/q(z)]

def free_energy(x_val, q_z, mu, sigma, pi_k):
    """Compute negative ELBO (free energy)."""
    K = len(mu)
    F = 0.0
    for k in range(K):
        if q_z[k] > 1e-300:
            log_joint = np.log(pi_k[k] + 1e-300) + \
                        (-0.5*np.log(2*np.pi) - np.log(sigma[k]) - 0.5*((x_val-mu[k])/sigma[k])**2)
            F -= q_z[k] * (log_joint - np.log(q_z[k]))
    return F

# Track free energy during EM
np.random.seed(42)
x_val = 1.5
mu = np.array([-2.0, 3.0])
sigma = np.array([1.0, 1.5])
pi_k = np.array([0.4, 0.6])

print(f"{'Step':>6} | {'q(z=0)':>8} | {'q(z=1)':>8} | {'F(q,Î¸)':>10} | {'Action':>12}")
print("-" * 55)

for step in range(5):
    # Before E-step: use arbitrary q
    q_z = np.array([0.5, 0.5]) if step == 0 else q_z
    F_before = free_energy(x_val, q_z, mu, sigma, pi_k)

    # E-step (m-projection): minimize F over q â†’ q = p(z|x,Î¸)
    def norm_pdf(x, m, s):
        return np.exp(-0.5*((x-m)/s)**2)/(s*np.sqrt(2*np.pi))
    pdf = np.array([pi_k[k] * norm_pdf(x_val, mu[k], sigma[k]) for k in range(2)])
    q_z = pdf / pdf.sum()
    F_after_E = free_energy(x_val, q_z, mu, sigma, pi_k)

    print(f"{step*2:6d} | {q_z[0]:8.4f} | {q_z[1]:8.4f} | {F_before:10.4f} | {'E-step':>12}")
    print(f"{step*2+1:6d} | {q_z[0]:8.4f} | {q_z[1]:8.4f} | {F_after_E:10.4f} | {'(after E)':>12}")

print(f"\nFree energy decreases at each E-step (coordinate descent on q)")
```

ã“ã®å¹¾ä½•å­¦çš„è¦–ç‚¹ã®å®Œå…¨ãªå±•é–‹ã¯ç¬¬27å›ï¼ˆæƒ…å ±å¹¾ä½•ï¼‰ã§è¡Œã†ã€‚ã“ã“ã§ã¯ã€ŒEM = äº¤äº’å°„å½± = åº§æ¨™é™ä¸‹ã€ã¨ã„ã†ç›´æ„Ÿã ã‘æŒã¡å¸°ã£ã¦ã»ã—ã„ã€‚

### 3.10 Generalized EM ã¨ ECM

å®Ÿéš›ã®å¿œç”¨ã§ã¯ã€M-stepã®è§£æè§£ãŒå¾—ã‚‰ã‚Œãªã„ã“ã¨ãŒã‚ã‚‹ã€‚**Generalized EM** (GEM) ã¯ã€M-stepã§ $Q(\theta, \theta^{(t)})$ ã‚’å®Œå…¨ã«æœ€å¤§åŒ–ã™ã‚‹ä»£ã‚ã‚Šã«ã€$Q(\theta^{(t+1)}, \theta^{(t)}) > Q(\theta^{(t)}, \theta^{(t)})$ ã‚’æº€ãŸã™ä»»æ„ã® $\theta^{(t+1)}$ ã‚’é¸ã¹ã°ã‚ˆã„ã€‚

å˜èª¿æ€§ã®è¨¼æ˜ã¯åŒæ§˜ã«æˆã‚Šç«‹ã¤ã€‚M-stepã§ELBOãŒ**å¢—åŠ **ã—ã•ãˆã™ã‚Œã°ã€å¯¾æ•°å°¤åº¦ã®éæ¸›å°‘ã¯ä¿è¨¼ã•ã‚Œã‚‹ã€‚

$$
\text{GEM}: \quad \theta^{(t+1)} = \theta^{(t)} + \eta \nabla_\theta Q(\theta, \theta^{(t)}) \Big|_{\theta = \theta^{(t)}}
$$

ã¤ã¾ã‚Šã€å‹¾é…é™ä¸‹æ³•ã§æ•°ã‚¹ãƒ†ãƒƒãƒ— $Q$ ã‚’æ”¹å–„ã™ã‚‹ã ã‘ã§ã‚‚ã‚ˆã„ã€‚

**ECM** (Expectation Conditional Maximization) ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’åˆ†å‰²ã—ã¦å„ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ã«æœ€å¤§åŒ–ã™ã‚‹å¤‰ç¨®ã ã€‚å¤šå¤‰é‡GMMã§å…±åˆ†æ•£è¡Œåˆ—ãŒåˆ¶ç´„ã‚’æŒã¤å ´åˆã«æœ‰ç”¨ã€‚

```python
import numpy as np

# Generalized EM: gradient step instead of full maximization
def gem_m_step(x, gamma, mu, sigma, pi_k, lr=0.1):
    """GEM M-step: one gradient step on Q(Î¸, Î¸^(t)) instead of full maximization."""
    N = len(x)
    K = len(mu)
    N_k = gamma.sum(axis=0)

    # Gradient of Q w.r.t. Î¼_k
    for k in range(K):
        grad_mu = (gamma[:, k] * (x - mu[k])).sum() / (sigma[k]**2)
        mu[k] += lr * grad_mu / N  # gradient step (not closed-form!)

        # Gradient w.r.t. Ïƒ_k (through log Ïƒ for positivity)
        grad_log_sigma = -N_k[k] + (gamma[:, k] * (x - mu[k])**2).sum() / sigma[k]**2
        sigma[k] *= np.exp(lr * grad_log_sigma / N)
        sigma[k] = max(sigma[k], 1e-6)

    pi_k[:] = N_k / N  # this part still has closed form
    return mu, sigma, pi_k

# Compare EM vs GEM convergence speed
np.random.seed(42)
N = 200
x = np.concatenate([np.random.normal(-2, 1, 80), np.random.normal(3, 1.5, 120)])

# Standard EM
mu_em = np.array([0.0, 1.0])
sigma_em = np.array([2.0, 2.0])
pi_em = np.array([0.5, 0.5])

# GEM
mu_gem = np.array([0.0, 1.0])
sigma_gem = np.array([2.0, 2.0])
pi_gem = np.array([0.5, 0.5])

def compute_ll_1d(x, mu, sigma, pi_k):
    ll = 0.0
    for xn in x:
        p = sum(pi_k[k]*np.exp(-0.5*((xn-mu[k])/sigma[k])**2)/(sigma[k]*np.sqrt(2*np.pi))
                for k in range(len(mu)))
        ll += np.log(p + 1e-300)
    return ll

print(f"{'Iter':>4} | {'EM log-lik':>12} | {'GEM log-lik':>12}")
print("-" * 35)

for t in range(20):
    ll_em = compute_ll_1d(x, mu_em, sigma_em, pi_em)
    ll_gem = compute_ll_1d(x, mu_gem, sigma_gem, pi_gem)
    if t % 4 == 0:
        print(f"{t:4d} | {ll_em:12.4f} | {ll_gem:12.4f}")

    # EM: E-step + full M-step
    pdf = np.zeros((N, 2))
    for k in range(2):
        pdf[:, k] = pi_em[k]*np.exp(-0.5*((x-mu_em[k])/sigma_em[k])**2)/(sigma_em[k]*np.sqrt(2*np.pi))
    gamma_em = pdf / (pdf.sum(axis=1, keepdims=True) + 1e-300)
    N_k = gamma_em.sum(axis=0)
    for k in range(2):
        mu_em[k] = (gamma_em[:, k] * x).sum() / N_k[k]
        sigma_em[k] = np.sqrt((gamma_em[:, k] * (x - mu_em[k])**2).sum() / N_k[k]) + 1e-6
    pi_em = N_k / N

    # GEM: E-step + gradient M-step
    pdf_g = np.zeros((N, 2))
    for k in range(2):
        pdf_g[:, k] = pi_gem[k]*np.exp(-0.5*((x-mu_gem[k])/sigma_gem[k])**2)/(sigma_gem[k]*np.sqrt(2*np.pi))
    gamma_gem = pdf_g / (pdf_g.sum(axis=1, keepdims=True) + 1e-300)
    mu_gem, sigma_gem, pi_gem = gem_m_step(x, gamma_gem, mu_gem, sigma_gem, pi_gem, lr=0.5)

print(f"\nEM converges faster (closed-form M-step),")
print(f"but GEM is more flexible (works when no closed form exists).")
```

### 3.11 Missing Dataç†è«– â€” EMã®åŸç‚¹

EMç®—æ³•ã®åŸè«–æ–‡ [^1] ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯ "Maximum Likelihood from **Incomplete Data**" ã ã€‚æ½œåœ¨å¤‰æ•°ã¯æ¬ æãƒ‡ãƒ¼ã‚¿ã®ä¸€èˆ¬åŒ–ã§ã‚ã‚Šã€EMã®åŸç‚¹ã¯æ¬ æå€¤å‡¦ç†ã«ã‚ã‚‹ã€‚

**æ¬ æãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åˆ†é¡** (Rubin, 1976):

| ãƒ¡ã‚«ãƒ‹ã‚ºãƒ  | å®šç¾© | EMé©ç”¨ |
|:---------|:-----|:-------|
| **MCAR** (Missing Completely At Random) | æ¬ æã¯å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ  | EMæœ‰åŠ¹ |
| **MAR** (Missing At Random) | æ¬ æã¯è¦³æ¸¬å€¤ã«ä¾å­˜ã™ã‚‹ãŒæ¬ æå€¤ã«ã¯ä¾å­˜ã—ãªã„ | EMæœ‰åŠ¹ |
| **MNAR** (Missing Not At Random) | æ¬ æãŒæ¬ æå€¤è‡ªä½“ã«ä¾å­˜ | EMã ã‘ã§ã¯ä¸ååˆ† |

$$
\text{MCAR}: \quad p(R \mid \mathbf{x}_{\text{obs}}, \mathbf{x}_{\text{mis}}) = p(R)
$$

$$
\text{MAR}: \quad p(R \mid \mathbf{x}_{\text{obs}}, \mathbf{x}_{\text{mis}}) = p(R \mid \mathbf{x}_{\text{obs}})
$$

$$
\text{MNAR}: \quad p(R \mid \mathbf{x}_{\text{obs}}, \mathbf{x}_{\text{mis}}) \text{ depends on } \mathbf{x}_{\text{mis}}
$$

ã“ã“ã§ $R$ ã¯æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¡¨ã™ç¢ºç‡å¤‰æ•°ï¼ˆ$R_{nd} = 1$ ãªã‚‰ $x_{nd}$ ã¯è¦³æ¸¬ã€$R_{nd} = 0$ ãªã‚‰æ¬ æï¼‰ã€‚

MARä»¥ä¸‹ã®ä»®å®šãŒæˆã‚Šç«‹ã¤ã¨ãã€EMç®—æ³•ã¯æ¬ æã‚’ã€Œæ½œåœ¨å¤‰æ•°ã€ã¨ã—ã¦æ‰±ã„ã€å®Œå…¨ãƒ‡ãƒ¼ã‚¿å°¤åº¦ã®æœŸå¾…å€¤ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ä¸€è²«ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šãŒå¯èƒ½ã«ãªã‚‹ã€‚Zone 5 ã®ãƒãƒ£ãƒ¬ãƒ³ã‚¸2ã§å®Ÿè£…ã—ãŸæ¬ æå€¤è£œå®Œã¯ã€ã¾ã•ã«ã“ã®ç†è«–ã«åŸºã¥ã„ã¦ã„ã‚‹ã€‚

### 3.12 Identifiabilityã¨label switchingå•é¡Œ

GMMã«ã¯æœ¬è³ªçš„ãª **éè­˜åˆ¥å¯èƒ½æ€§** (non-identifiability) ãŒã‚ã‚‹ã€‚

$K$ å€‹ã®æˆåˆ†ã«å¯¾ã—ã¦ã€æˆåˆ†ã®ãƒ©ãƒ™ãƒ«ã‚’ä¸¦ã¹æ›¿ãˆã¦ã‚‚åŒã˜åˆ†å¸ƒã«ãªã‚‹:

$$
\sum_{k=1}^{K} \pi_k \mathcal{N}(x \mid \mu_k, \sigma_k^2) = \sum_{k=1}^{K} \pi_{\tau(k)} \mathcal{N}(x \mid \mu_{\tau(k)}, \sigma_{\tau(k)}^2)
$$

ã“ã“ã§ $\tau$ ã¯ $\{1, \ldots, K\}$ ä¸Šã®ä»»æ„ã®ç½®æ›ã€‚ã¤ã¾ã‚Š $K!$ å€‹ã®ç­‰ä¾¡ãªæœ€é©è§£ãŒå­˜åœ¨ã™ã‚‹ã€‚

ã“ã‚Œã¯ **label switchingå•é¡Œ** ã¨å‘¼ã°ã‚Œã€ãƒ™ã‚¤ã‚ºæ¨è«–ã§GMMã‚’æ‰±ã†éš›ã«ç‰¹ã«æ·±åˆ»ã«ãªã‚‹ã€‚EMç®—æ³•ã§ã¯åˆæœŸå€¤ã§1ã¤ã®è§£ã«ã€Œå›ºå®šã€ã•ã‚Œã‚‹ãŸã‚å®Ÿç”¨ä¸Šã¯å•é¡Œã«ãªã‚‰ãªã„ãŒã€ç†è«–çš„ã«ã¯æœ€é©è§£ã®ä¸€æ„æ€§ãŒä¿è¨¼ã•ã‚Œãªã„ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

```python
import numpy as np

# Label switching: permuting components gives same distribution
mu = np.array([-2.0, 3.0])
sigma = np.array([1.0, 1.5])
pi_k = np.array([0.4, 0.6])

x = np.array([0.0, 1.0, -1.0, 4.0])

def gmm_pdf_1d(x, mu, sigma, pi_k):
    return sum(pi_k[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2)/(sigma[k]*np.sqrt(2*np.pi))
               for k in range(len(mu)))

# Original order
pdf_original = np.array([gmm_pdf_1d(xi, mu, sigma, pi_k) for xi in x])

# Swapped labels (permutation Ï„ = (1, 0))
mu_swap = mu[::-1]
sigma_swap = sigma[::-1]
pi_swap = pi_k[::-1]
pdf_swapped = np.array([gmm_pdf_1d(xi, mu_swap, sigma_swap, pi_swap) for xi in x])

print("Original vs Swapped labels (should be identical):")
for i, xi in enumerate(x):
    print(f"  x={xi:5.1f}: p_original={pdf_original[i]:.6f}, p_swapped={pdf_swapped[i]:.6f}, "
          f"diff={abs(pdf_original[i]-pdf_swapped[i]):.2e}")
print(f"\nK=2 components â†’ {np.math.factorial(2)} equivalent optima (label switching)")
print(f"K=5 components â†’ {np.math.factorial(5)} equivalent optima")
```

:::message
**é€²æ—: 50% å®Œäº†** æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚EMç®—æ³•ã‚’Jensenä¸ç­‰å¼ã‹ã‚‰å®Œå…¨ã«å°å‡ºã—ã€åæŸæ€§ã‚’è¨¼æ˜ã—ã€GMMã®å…¨æ›´æ–°å¼ã‚’å°å‡ºã—ãŸã€‚å¹¾ä½•å­¦çš„è§£é‡ˆã€GEMã€æ¬ æãƒ‡ãƒ¼ã‚¿ç†è«–ã€label switchingå•é¡Œã¾ã§ç¶²ç¾…ã€‚å¾ŒåŠæˆ¦ã¯å®Ÿè£…ã¨å¿œç”¨ã«é€²ã‚€ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Dempster, A.P., Laird, N.M., Rubin, D.B. (1977). "Maximum Likelihood from Incomplete Data via the EM Algorithm." *Journal of the Royal Statistical Society, Series B*, 39(1), 1-38.
@[card](https://doi.org/10.1111/j.2517-6161.1977.tb01600.x)

[^2]: Kingma, D.P., Welling, M. (2013). "Auto-Encoding Variational Bayes." *arXiv preprint*.
@[card](https://arxiv.org/abs/1312.6114)

[^3]: Wu, C.F.J. (1983). "On the Convergence Properties of the EM Algorithm." *The Annals of Statistics*, 11(1), 95-103.
@[card](https://doi.org/10.1214/aos/1176346060)

[^4]: Baum, L.E., Petrie, T., Soules, G., Weiss, N. (1970). "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains." *The Annals of Mathematical Statistics*, 41(1), 164-171.
@[card](https://doi.org/10.1214/aoms/1177697196)

[^5]: Neal, R.M., Hinton, G.E. (1998). "A View of the EM Algorithm that Justifies Incremental, Sparse, and other Variants." *Learning in Graphical Models*, Springer.
@[card](https://www.cs.toronto.edu/~hinton/absps/emk.pdf)

[^6]: Arthur, D., Vassilvitskii, S. (2007). "k-means++: The Advantages of Careful Seeding." *SODA '07*.

[^7]: Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E. (1991). "Adaptive Mixtures of Local Experts." *Neural Computation*, 3(1), 79-87.
@[card](https://doi.org/10.1162/neco.1991.3.1.79)

[^11]: Bishop, C.M. (2006). *Pattern Recognition and Machine Learning*. Springer.
@[card](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)

[^12]: Minka, T.P. (2001). "Expectation Propagation for Approximate Bayesian Inference." *UAI 2001*.
@[card](https://arxiv.org/abs/1301.2294)

### æ•™ç§‘æ›¸

- Bishop, C.M. (2006). *Pattern Recognition and Machine Learning*. Springer. [Ch.9: Mixture Models and EM] [å…¬å¼PDFç„¡æ–™]
- Murphy, K.P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. [Ch.11]
- MacKay, D.J.C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. [Ch.22, 33] [å…¬å¼PDFç„¡æ–™]

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | èª­ã¿ | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|:-----|
| $\mathbf{x}$ | ã‚¨ãƒƒã‚¯ã‚¹ (å¤ªå­—) | è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ | ç¬¬2å› |
| $\mathbf{z}$ | ã‚¼ãƒƒãƒˆ (å¤ªå­—) | æ½œåœ¨å¤‰æ•°ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ | **ç¬¬8å›** |
| $\theta$ | ã‚·ãƒ¼ã‚¿ | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ç¬¬6å› |
| $\phi$ | ãƒ•ã‚¡ã‚¤ | å¤‰åˆ†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç¬¬9å›ã§æœ¬æ ¼ç™»å ´ï¼‰ | â€” |
| $\pi_k$ | ãƒ‘ã‚¤ ã‚±ãƒ¼ | æ··åˆé‡ã¿ï¼ˆ$\sum_k \pi_k = 1$ï¼‰ | **ç¬¬8å›** |
| $\mu_k$ | ãƒŸãƒ¥ãƒ¼ ã‚±ãƒ¼ | æˆåˆ† $k$ ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« | ç¬¬4å› |
| $\boldsymbol{\Sigma}_k$ | ã‚·ã‚°ãƒ ã‚±ãƒ¼ | æˆåˆ† $k$ ã®å…±åˆ†æ•£è¡Œåˆ— | ç¬¬4å› |
| $\gamma(z_{nk})$ | ã‚¬ãƒ³ãƒ | è²¬ä»»åº¦ï¼ˆäº‹å¾Œç¢ºç‡ï¼‰ | **ç¬¬8å›** |
| $N_k$ | ã‚¨ãƒŒ ã‚±ãƒ¼ | æˆåˆ† $k$ ã®å®ŸåŠ¹ãƒ‡ãƒ¼ã‚¿æ•° | **ç¬¬8å›** |
| $Q(\theta, \theta^{(t)})$ | ã‚­ãƒ¥ãƒ¼ | Qé–¢æ•°ï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã®æœŸå¾…å€¤ï¼‰ | **ç¬¬8å›** |
| $\mathcal{L}(q, \theta)$ | ã‚¨ãƒ« | ELBO | **ç¬¬8å›**ï¼ˆç¬¬9å›ã§ä¸»å½¹ï¼‰ |
| $\text{KL}[q \| p]$ | ã‚±ãƒ¼ã‚¨ãƒ« | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | ç¬¬6å› |
| $\mathcal{N}(\cdot \mid \mu, \sigma^2)$ | ãƒãƒ¼ãƒãƒ« | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ | ç¬¬4å› |
| $\mathbb{E}[\cdot]$ | ã‚¨ã‚¯ã‚¹ãƒšã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ | æœŸå¾…å€¤ | ç¬¬4å› |
| $\mathbb{1}[\cdot]$ | ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ | æŒ‡ç¤ºé–¢æ•° | ç¬¬1å› |
| $K$ | ã‚±ãƒ¼ | æ··åˆæˆåˆ†æ•° / éš ã‚ŒçŠ¶æ…‹æ•° | **ç¬¬8å›** |
| $\log |\boldsymbol{\Sigma}|$ | ãƒ­ã‚° ãƒ‡ãƒƒãƒˆ ã‚·ã‚°ãƒ | å…±åˆ†æ•£è¡Œåˆ—ã®è¡Œåˆ—å¼ã®å¯¾æ•° | ç¬¬3å› |
| $\mathbf{A}$ | ã‚¨ãƒ¼ | çŠ¶æ…‹é·ç§»è¡Œåˆ—ï¼ˆHMMï¼‰ | **ç¬¬8å›** |
| $\alpha_t(k)$ | ã‚¢ãƒ«ãƒ•ã‚¡ ãƒ†ã‚£ãƒ¼ ã‚±ãƒ¼ | å‰å‘ãç¢ºç‡ï¼ˆForward algorithmï¼‰ | **ç¬¬8å›** |
| $\beta_t(k)$ | ãƒ™ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ã‚±ãƒ¼ | å¾Œå‘ãç¢ºç‡ï¼ˆBackward algorithmï¼‰ | **ç¬¬8å›** |
| $\mathbf{W}$ | ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ | å› å­è² è·è¡Œåˆ—ï¼ˆFactor Analysisï¼‰ | **ç¬¬8å›** |
| $\boldsymbol{\Psi}$ | ãƒ—ã‚µã‚¤ | å›ºæœ‰ãƒã‚¤ã‚ºå…±åˆ†æ•£ï¼ˆFactor Analysisï¼‰ | **ç¬¬8å›** |
| $g_k(x)$ | ã‚¸ãƒ¼ ã‚±ãƒ¼ | ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ï¼ˆMoEï¼‰ | **ç¬¬8å›** |
| $f({\mathbb{E}}[X]) \geq \mathbb{E}[f(X)]$ | â€” | Jensenä¸ç­‰å¼ï¼ˆå‡¹é–¢æ•°ï¼‰ | **ç¬¬8å›** |
| $\text{BIC}$ | ãƒ“ãƒ¼ã‚¢ã‚¤ã‚·ãƒ¼ | ãƒ™ã‚¤ã‚ºæƒ…å ±é‡åŸºæº– | **ç¬¬8å›** |
| $\text{AIC}$ | ã‚¨ãƒ¼ã‚¢ã‚¤ã‚·ãƒ¼ | èµ¤æ± æƒ…å ±é‡åŸºæº– | **ç¬¬8å›** |
| $R$ | ã‚¢ãƒ¼ãƒ« | æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³æŒ‡ç¤ºå¤‰æ•° | **ç¬¬8å›** |
| $d$ | ãƒ‡ã‚£ãƒ¼ | ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼ˆBIC/AICï¼‰ | **ç¬¬8å›** |

---

## è£œéº â€” EMç®—æ³•ã®åæŸç†è«–ã¨å¤‰åˆ†æ¨è«–ã¸ã®æ©‹æ¸¡ã—

:::message
**EMç®—æ³•ã®æœ¬è³ª**: æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€é›£ã—ã„æœ€é©åŒ–å•é¡Œã‚’2ã¤ã®ç°¡å˜ãªã‚¹ãƒ†ãƒƒãƒ—ï¼ˆE-step: æœŸå¾…å€¤è¨ˆç®—ã€M-step: æœ€å¤§åŒ–ï¼‰ã«åˆ†å‰²ã€‚å¤‰åˆ†æ¨è«–ï¼ˆç¬¬9å›ï¼‰ã®åŸå‹ã¨ãªã‚‹ã€‚
:::

### EMç®—æ³•ã®åæŸä¿è¨¼

**å®šç†**: EM algorithm ã¯å¯¾æ•°å°¤åº¦ $\log p(\mathbf{x}|\theta)$ ã‚’å˜èª¿å¢—åŠ ã•ã›ã‚‹ã€‚

**è¨¼æ˜**: Jensen ä¸ç­‰å¼ã«ã‚ˆã‚Šã€ä»»æ„ã®åˆ†å¸ƒ $q(\mathbf{z})$ ã«å¯¾ã—:

$$
\log p(\mathbf{x}|\theta) \geq \mathbb{E}_{q(\mathbf{z})} \left[ \log \frac{p(\mathbf{x}, \mathbf{z}|\theta)}{q(\mathbf{z})} \right] =: \mathcal{L}(q, \theta)
$$

E-step ã§ $q^{(t)}(\mathbf{z}) = p(\mathbf{z}|\mathbf{x}, \theta^{(t)})$ ã¨é¸ã¶ã¨ã€ç­‰å·æˆç«‹ï¼ˆtightnessï¼‰:

$$
\mathcal{L}(q^{(t)}, \theta^{(t)}) = \log p(\mathbf{x}|\theta^{(t)})
$$

M-step ã§ $\theta^{(t+1)} = \arg\max_\theta \mathcal{L}(q^{(t)}, \theta)$ ã¨ã™ã‚‹ã¨:

$$
\log p(\mathbf{x}|\theta^{(t+1)}) \geq \mathcal{L}(q^{(t)}, \theta^{(t+1)}) \geq \mathcal{L}(q^{(t)}, \theta^{(t)}) = \log p(\mathbf{x}|\theta^{(t)})
$$

ã‚ˆã£ã¦å˜èª¿å¢—åŠ ã€‚ â–¡

### EMç®—æ³•ã®åæŸé€Ÿåº¦

**ç·šå½¢åæŸ**: é€šå¸¸ã® EM ã¯ç·šå½¢åæŸï¼ˆ1æ¬¡åæŸï¼‰:

$$
\|\theta^{(t+1)} - \theta^*\| \leq c \|\theta^{(t)} - \theta^*\|, \quad c < 1
$$

**åæŸå®šæ•° $c$ ã®è¨ˆç®—**:

$$
c = \lambda_{\max}\left( \mathbf{I} - \mathcal{I}_{\text{observed}}(\theta^*) \mathcal{I}_{\text{complete}}(\theta^*)^{-1} \right)
$$

ã“ã“ã§:
- $\mathcal{I}_{\text{observed}}$: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã® Fisher æƒ…å ±
- $\mathcal{I}_{\text{complete}}$: å®Œå…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆè¦³æ¸¬ï¼‹æ½œåœ¨ï¼‰ã® Fisher æƒ…å ±

**å«æ„**: æ½œåœ¨å¤‰æ•°ãŒè¦³æ¸¬å¤‰æ•°ã¨å¼·ãç›¸é–¢ã™ã‚‹ã»ã©ã€$c \to 1$ï¼ˆåæŸãŒé…ã„ï¼‰ã€‚

### é«˜é€ŸåŒ–æ‰‹æ³•1: Incremental EMï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼‰

å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ $\{\mathbf{x}_1, \ldots, \mathbf{x}_N\}$ ã«å¯¾ã—ã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã‚ãšã«é€æ¬¡æ›´æ–°:

$$
\theta^{(t+1)} = \theta^{(t)} + \alpha_t \mathbb{E}_{p(\mathbf{z}_i|\mathbf{x}_i, \theta^{(t)})} [\nabla_\theta \log p(\mathbf{x}_i, \mathbf{z}_i|\theta^{(t)})]
$$

ã“ã“ã§ $\alpha_t$ ã¯å­¦ç¿’ç‡ï¼ˆä¾‹: $\alpha_t = 1/t$ï¼‰ã€‚

**åˆ©ç‚¹**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ $O(1)$ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºå›ºå®šï¼‰ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œã€‚

### é«˜é€ŸåŒ–æ‰‹æ³•2: Variational EMï¼ˆMean-Fieldè¿‘ä¼¼ï¼‰

E-step ã‚’é–‰å½¢å¼ã§è§£ã‘ãªã„å ´åˆã€$q(\mathbf{z})$ ã‚’åˆ¶ç´„:

$$
q(\mathbf{z}) = \prod_{i=1}^d q_i(z_i) \quad \text{(Mean-Field)}
$$

å„ $q_i$ ã‚’äº¤äº’ã«æ›´æ–°ï¼ˆCoordinate Ascent VIï¼‰:

$$
q_i^*(z_i) \propto \exp\left( \mathbb{E}_{q_{-i}} [\log p(\mathbf{x}, \mathbf{z}|\theta)] \right)
$$

**å¿œç”¨**: Latent Dirichlet Allocation (LDA), Variational Autoencoders (VAE)ã€‚

### EMç®—æ³•ã®é™ç•Œã¨å¯¾ç­–

| å•é¡Œ | åŸå›  | å¯¾ç­– |
|:---|:---|:---|
| å±€æ‰€æœ€é©è§£ã«åæŸ | éå‡¸æ€§ | è¤‡æ•°ã®åˆæœŸå€¤ / MCMC-EM |
| åæŸãŒé…ã„ | å¼·ã„ç›¸é–¢ | åŠ é€Ÿ EM (Accelerated EM) |
| M-step ãŒå›°é›£ | é–‰å½¢å¼è§£ãªã— | GEM (Generalized EM): 1ã‚¹ãƒ†ãƒƒãƒ—ã ã‘æ”¹å–„ |
| é«˜æ¬¡å…ƒæ½œåœ¨å¤‰æ•° | è¨ˆç®—ã‚³ã‚¹ãƒˆ | Variational EM / Sampling-based EM |

### EMç®—æ³•ã‹ã‚‰å¤‰åˆ†æ¨è«–ã¸

**EM** ã¨ **å¤‰åˆ†æ¨è«–** ã®é–¢ä¿‚:

| é …ç›® | EM | å¤‰åˆ†æ¨è«– (VI) |
|:---|:---|:---|
| ç›®çš„ | $\max_\theta \log p(\mathbf{x}\|\theta)$ | $\min_{q} D_{\text{KL}}(q \| p)$ |
| E-step | $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x}, \theta)$ (exact) | $q(\mathbf{z})$ ã‚’è¿‘ä¼¼æ—ã‹ã‚‰é¸æŠ |
| M-step | $\theta = \arg\max \mathbb{E}_q [\log p(\mathbf{x}, \mathbf{z}\|\theta)]$ | $\theta$ ã‚‚ $q$ ã¨åŒæ™‚æœ€é©åŒ– (VAE) |
| é©ç”¨ç¯„å›² | æ½œåœ¨å¤‰æ•°ãŒé›¢æ•£ or ä½æ¬¡å…ƒ | é«˜æ¬¡å…ƒé€£ç¶šæ½œåœ¨å¤‰æ•° |

**å¤‰åˆ†æ¨è«– (ç¬¬9å›)** ã§ã¯ã€$q(\mathbf{z})$ ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ãƒ‘ãƒ©ãƒ¡ãƒˆãƒ©ã‚¤ã‚ºã—ã€$\theta$ ã¨åŒæ™‚æœ€é©åŒ–ã™ã‚‹ **Amortized Inference** ã¸ã¨ç™ºå±•ã€‚

### å®Ÿä¾‹: ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ã®åæŸå®šæ•°

$K=2$ æˆåˆ†ã® GMM ã§ã€2ã¤ã®ã‚¯ãƒ©ã‚¹ã‚¿ãŒååˆ†ã«é›¢ã‚Œã¦ã„ã‚‹å ´åˆ:

$$
c \approx 0.1 \quad \text{(é«˜é€ŸåæŸ)}
$$

é€†ã«ã€ã‚¯ãƒ©ã‚¹ã‚¿ãŒé‡ãªã‚‹å ´åˆ:

$$
c \approx 0.9 \quad \text{(åæŸãŒé…ã„)}
$$

**å®Ÿé¨“çš„æ¤œè¨¼**: $\theta^{(t)}$ ã¨ $\theta^*$ ã®è·é›¢ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã€æŒ‡æ•°çš„æ¸›è¡°ã‚’ç¢ºèªã€‚

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
