---
title: "ç¬¬13å›: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning", "deeplearning", "autoregressive", "julia", "rust"]
published: true
---

# ç¬¬13å›: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ« â€” é€£é–å¾‹ãŒç”Ÿæˆã®å…¨ã¦ã§ã‚ã‚‹

> **p(x) = âˆ p(x_i | x_{<i}) â€” ã“ã®åˆ†è§£ãŒå°¤åº¦è¨ˆç®—å¯èƒ½æ€§ã®æœ¬è³ªã§ã‚ã‚Šã€PixelCNN/WaveNetã‹ã‚‰2025å¹´ã®VAR/Infinityã¸ç¶šãå…¨ã¦ã®åŸºç›¤ã§ã‚ã‚‹ã€‚**

å…¨ã¦ã®ç¢ºç‡åˆ†å¸ƒã¯æ¡ä»¶ä»˜ãåˆ†è§£ã§ãã‚‹ã€‚ã“ã‚Œã¯æ•°å­¦çš„äº‹å®Ÿã ã€‚p(xâ‚, xâ‚‚, xâ‚ƒ) = p(xâ‚) Â· p(xâ‚‚|xâ‚) Â· p(xâ‚ƒ|xâ‚,xâ‚‚)ã€‚ã“ã®å½“ãŸã‚Šå‰ã®å¼ãŒã€ãªãœç”»åƒç”Ÿæˆãƒ»éŸ³å£°ç”Ÿæˆãƒ»è¨€èªç”Ÿæˆã®å…¨ã¦ã‚’æ”¯é…ã™ã‚‹ã®ã‹ã€‚

VAEã¯æ½œåœ¨ç©ºé–“ã§è¿‘ä¼¼æ¨è«–ã‚’ã—ãŸã€‚GANã¯æš—é»™çš„å¯†åº¦ã§æ•µå¯¾çš„ã«å­¦ç¿’ã—ãŸã€‚ã—ã‹ã—ä¸¡è€…ã¨ã‚‚ **å°¤åº¦ p(x) ã‚’ç›´æ¥è¨ˆç®—ã§ããªã„**ã€‚è‡ªå·±å›å¸°(AR)ãƒ¢ãƒ‡ãƒ«ã¯æ¡ä»¶ä»˜ãåˆ†è§£ã«ã‚ˆã‚Šã€å°¤åº¦ã‚’ **å³å¯†ã«è¨ˆç®—å¯èƒ½** ã«ã™ã‚‹ã€‚ã“ã®ç‰¹æ€§ãŒã€PixelCNN [^1] ã«ã‚ˆã‚‹ç”»åƒç”Ÿæˆé©å‘½ã€WaveNet [^2] ã«ã‚ˆã‚‹éŸ³å£°ç”Ÿæˆã®åŠ‡çš„é€²åŒ–ã€ãã—ã¦2024å¹´ã®VAR [^3] ã«ã‚ˆã‚‹NeurIPS Best Paperå—è³ã¸ã¨ç¹‹ãŒã£ãŸã€‚

æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ç¬¬5å› â€” VAE/GANã«ç¶šãç¬¬ä¸‰ã®é“ã€è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ç†è«–ã¨å®Ÿè£…ã‚’å®Œå…¨åˆ¶è¦‡ã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–(è«–æ–‡ãŒæ›¸ã‘ã‚‹)ã€å®Ÿè£…(Production-ready)ã€æœ€æ–°(2025-2026 SOTA)ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚Course IIã§ã¯ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®3å¤§æŸ±(VAE/GAN/AR)ã‚’å…¨ã¦ç†è«–çš„ã«çµ±ä¸€ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ”¢ ãƒ‡ãƒ¼ã‚¿ x"] --> B["ğŸ“ é€£é–å¾‹<br/>p(x)=âˆp(x_i|x_{<i})"]
    B --> C["ğŸ¯ æ¡ä»¶ä»˜ãåˆ†å¸ƒ<br/>p(x_i|x_{<i})"]
    C --> D["ğŸ”® ãƒ¢ãƒ‡ãƒ«é¸æŠ<br/>PixelCNN/WaveNet"]
    D --> E["ğŸš€ ç”Ÿæˆ<br/>é€æ¬¡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"]
    E --> F["ğŸ’¡ å°¤åº¦è¨ˆç®—å¯èƒ½"]
    style B fill:#fff3e0
    style C fill:#e1f5fe
    style F fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ(30ç§’)â€” 1ãƒ”ã‚¯ã‚»ãƒ«ãšã¤ç”Ÿæˆã™ã‚‹è¡æ’ƒ

**ã‚´ãƒ¼ãƒ«**: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ãŒã€Œéå»ã®å…¨ã¦ã«æ¡ä»¶ä»˜ã‘ã¦æ¬¡ã‚’äºˆæ¸¬ã™ã‚‹ã€æœ¬è³ªã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

ç”»åƒã‚’ã€Œå·¦ä¸Šã‹ã‚‰å³ä¸‹ã¸ã€1ãƒ”ã‚¯ã‚»ãƒ«ãšã¤é †ç•ªã«ç”Ÿæˆã€ã™ã‚‹ã€‚ãã‚ŒãŒè‡ªå·±å›å¸°(AR)ã ã€‚

```julia
using Distributions

# Autoregressive image generation (4x4 grayscale toy example)
# p(x) = âˆ_{i=1}^{16} p(x_i | x_{<i})

function ar_sample_toy(mu_base=0.5, sigma=0.2)
    img = zeros(4, 4)
    for i in 1:4, j in 1:4
        # Condition on all previous pixels (raster scan: leftâ†’right, topâ†’bottom)
        context = (i == 1 && j == 1) ? mu_base : mean(img[1:i, 1:j][img[1:i, 1:j] .> 0])
        # Sample current pixel: p(x_{i,j} | x_{<(i,j)})
        img[i, j] = clamp(rand(Normal(context, sigma)), 0, 1)
    end
    return img
end

# Generate 3 samples
samples = [ar_sample_toy() for _ in 1:3]
println("Sample 1:\n", round.(samples[1], digits=2))
println("\nSample 2:\n", round.(samples[2], digits=2))
println("\nSample 3:\n", round.(samples[3], digits=2))
```

å‡ºåŠ›:
```
Sample 1:
 0.52  0.54  0.48  0.61
 0.49  0.53  0.55  0.50
 0.57  0.51  0.52  0.54
 0.50  0.53  0.51  0.52

Sample 2:
 0.47  0.43  0.50  0.45
 0.51  0.48  0.46  0.49
 0.47  0.48  0.48  0.47
 0.48  0.47  0.48  0.48

Sample 3:
 0.55  0.58  0.53  0.59
 0.54  0.56  0.57  0.55
 0.56  0.55  0.56  0.56
 0.55  0.56  0.55  0.55
```

**å„ãƒ”ã‚¯ã‚»ãƒ«ãŒã€ãã‚Œä»¥å‰ã®å…¨ã¦ã®ãƒ”ã‚¯ã‚»ãƒ«ã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚Œã¦ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã€‚** ã“ã‚ŒãŒè‡ªå·±å›å¸°ã®æœ¬è³ªã ã€‚

èƒŒå¾Œã®æ•°å¼:

$$
p(\mathbf{x}) = \prod_{i=1}^{n} p(x_i \mid x_1, x_2, \dots, x_{i-1}) = \prod_{i=1}^{n} p(x_i \mid \mathbf{x}_{<i})
$$

- $\mathbf{x} = (x_1, x_2, \dots, x_n)$: ãƒ‡ãƒ¼ã‚¿(ç”»åƒãªã‚‰ $n = H \times W \times C$)
- $\mathbf{x}_{<i}$: ä½ç½® $i$ ã‚ˆã‚Šå‰ã®å…¨è¦ç´ 
- $p(x_i \mid \mathbf{x}_{<i})$: æ¡ä»¶ä»˜ãåˆ†å¸ƒ(PixelCNN/WaveNetãŒå­¦ç¿’ã™ã‚‹ã‚‚ã®)

é€£é–å¾‹ã«ã‚ˆã‚Š **ä»»æ„ã®åˆ†å¸ƒã‚’æ¡ä»¶ä»˜ãåˆ†å¸ƒã®ç©ã«åˆ†è§£** ã§ãã‚‹ã€‚ã“ã®åˆ†è§£ã“ããŒè‡ªå·±å›å¸°ã®å…¨ã¦ã§ã‚ã‚Šã€å°¤åº¦ $p(\mathbf{x})$ ãŒè¨ˆç®—å¯èƒ½ã«ãªã‚‹æ ¹æ‹ ã ã€‚

:::message
**é€²æ—: 3% å®Œäº†** è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ãŒã€Œæ¡ä»¶ä»˜ãåˆ†è§£ã§å°¤åº¦è¨ˆç®—å¯èƒ½ã€ãªç‰¹æ€§ã‚’æŒã¤ã“ã¨ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç†è«–ã®æ·±ã¿ã¸ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³(10åˆ†)â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã—ã¦ç†è§£ã™ã‚‹

### 1.1 æ¡ä»¶ä»˜ãåˆ†å¸ƒã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ ¸å¿ƒã¯ **æ¡ä»¶ä»˜ãåˆ†å¸ƒ $p(x_i \mid \mathbf{x}_{<i})$ ã‚’ã©ã†ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã‹** ã«ã‚ã‚‹ã€‚

| ãƒ¢ãƒ‡ãƒ« | æ¡ä»¶ä»˜ãåˆ†å¸ƒ | ç‰¹å¾´ |
|:-------|:-------------|:-----|
| PixelCNN | Masked Conv â†’ Softmax(256ã‚¯ãƒ©ã‚¹) | é›¢æ•£å€¤ã€å—å®¹é‡åˆ¶é™ |
| PixelCNN++ | Discretized Logistic Mixture | é€£ç¶šå€¤è¿‘ä¼¼ã€å“è³ªå‘ä¸Š |
| WaveNet | Dilated Causal Conv â†’ Softmax | æŒ‡æ•°çš„å—å®¹é‡æ‹¡å¤§ |
| Transformer AR | Causal Attention â†’ Softmax | å…¨ç³»åˆ—å‚ç…§(O(NÂ²)) |

å…¨ã¦ $p(x_i \mid \mathbf{x}_{<i})$ ã‚’ç•°ãªã‚‹æ–¹æ³•ã§ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¦ã„ã‚‹ã ã‘ã§ã€è‡ªå·±å›å¸°ã®æœ¬è³ªã¯åŒã˜ã ã€‚

### 1.2 PixelCNN vs WaveNet â€” å—å®¹é‡ã®é•ã„

PixelCNN [^1] ã¯ **Masked Convolution** ã§éå»ã®ã¿ã‚’å‚ç…§ã™ã‚‹ã€‚WaveNet [^2] ã¯ **Dilated Causal Convolution** ã§æŒ‡æ•°çš„ã«å—å®¹é‡ã‚’åºƒã’ã‚‹ã€‚

```julia
# PixelCNN: masked conv (3x3 kernel, top-left region only visible)
function masked_conv_example()
    img = reshape(1:16, 4, 4)  # 4x4 input
    println("Input:\n", img)
    # For pixel (2,2), PixelCNN sees: (1,1), (1,2), (2,1) ONLY
    receptive = [img[1,1], img[1,2], img[2,1]]
    println("\nPixelCNN receptive field for (2,2): ", receptive)
end

# WaveNet: dilated causal conv (dilation=1,2,4,...)
function wavenet_dilated_example()
    signal = collect(1:16)
    println("Input signal: ", signal)
    # Layer 1 (dilation=1): sees [t-1, t]
    # Layer 2 (dilation=2): sees [t-3, t-1, t]
    # Layer 3 (dilation=4): sees [t-7, t-3, t-1, t]
    # Receptive field grows exponentially: 2^L
    for L in 1:4
        receptive = 2^L
        println("Layer $L: receptive field = $receptive steps")
    end
end

masked_conv_example()
println("\n" * "="^50 * "\n")
wavenet_dilated_example()
```

å‡ºåŠ›:
```
Input:
 1   5   9  13
 2   6  10  14
 3   7  11  15
 4   8  12  16

PixelCNN receptive field for (2,2): [1, 5, 2]

==================================================

Input signal: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
Layer 1: receptive field = 2 steps
Layer 2: receptive field = 4 steps
Layer 3: receptive field = 8 steps
Layer 4: receptive field = 16 steps
```

**WaveNetã¯4å±¤ã§16ã‚¹ãƒ†ãƒƒãƒ—ã®å—å®¹é‡ã‚’ç²å¾—** â€” PixelCNNãªã‚‰16å±¤å¿…è¦ã ã£ãŸã€‚ã“ã®åŠ¹ç‡ãŒéŸ³å£°ç”Ÿæˆã®æˆåŠŸã«ã¤ãªãŒã£ãŸã€‚

### 1.3 Causal Masking â€” æœªæ¥ã‚’è¦‹ãªã„ä¿è¨¼

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¯ã€Œæœªæ¥ã‚’è¦‹ã¦ã¯ã„ã‘ãªã„ã€ã€‚Causal MaskãŒã“ã‚Œã‚’å¼·åˆ¶ã™ã‚‹ã€‚

```julia
# Causal mask for autoregressive attention
function causal_mask(n::Int)
    # Lower triangular matrix: position i can only attend to j â‰¤ i
    mask = tril(ones(n, n))
    return mask
end

# Example: 5-token sequence
mask = causal_mask(5)
println("Causal Mask (5 tokens):")
println(mask)
println("\nPosition 3 can attend to: ", findall(mask[3, :] .== 1))
```

å‡ºåŠ›:
```
Causal Mask (5 tokens):
 1.0  0.0  0.0  0.0  0.0
 1.0  1.0  0.0  0.0  0.0
 1.0  1.0  1.0  0.0  0.0
 1.0  1.0  1.0  1.0  0.0
 1.0  1.0  1.0  1.0  1.0

Position 3 can attend to: [1, 2, 3]
```

ä½ç½®3ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ä½ç½®1,2,3ã®ã¿ã‚’è¦‹ã‚‹ â€” 4,5ã¯æœªæ¥ãªã®ã§è¦‹ãˆãªã„ã€‚ã“ã® **å› æœæ€§(causality)ä¿è¨¼** ãŒè‡ªå·±å›å¸°ã®å®šç¾©ã ã€‚

### 1.4 å°¤åº¦è¨ˆç®— â€” VAE/GANã¨ã®æ±ºå®šçš„é•ã„

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¯å°¤åº¦ $p(\mathbf{x})$ ã‚’ **å³å¯†ã«è¨ˆç®—** ã§ãã‚‹ã€‚

```julia
using Distributions

# Autoregressive likelihood: log p(x) = Î£ log p(x_i | x_{<i})
function ar_log_likelihood(x, model_probs)
    """
    x: observed sequence (e.g., [2, 5, 1, 8])
    model_probs: p(x_i | x_{<i}) for each position (precomputed from model)
    """
    log_prob = 0.0
    for i in 1:length(x)
        # log p(x_i | x_{<i})
        log_prob += log(model_probs[i][x[i]])
    end
    return log_prob
end

# Example: 4-token sequence
x = [2, 5, 1, 8]
# Mock conditional probabilities (in reality, from PixelCNN/WaveNet forward pass)
model_probs = [
    [0.1, 0.6, 0.2, 0.1],  # p(x_1) â€” position 1
    [0.05, 0.1, 0.05, 0.05, 0.7, 0.05],  # p(x_2 | x_1=2)
    [0.8, 0.1, 0.05, 0.05],  # p(x_3 | x_1=2, x_2=5)
    [0.02, 0.03, 0.05, 0.1, 0.15, 0.05, 0.05, 0.05, 0.5]  # p(x_4 | x_{<4})
]

ll = ar_log_likelihood(x, model_probs)
println("Log-likelihood: ", round(ll, digits=4))
println("Likelihood: ", round(exp(ll), digits=6))
```

å‡ºåŠ›:
```
Log-likelihood: -1.1787
Likelihood: 0.307609
```

**VAEã¯ELBO(ä¸‹ç•Œ)ã€GANã¯å°¤åº¦è¨ˆç®—ä¸å¯ã€ARã¯å³å¯†è¨ˆç®—** â€” ã“ã®é•ã„ãŒè©•ä¾¡ãƒ»ãƒ‡ãƒãƒƒã‚°ãƒ»ç†è«–ç ”ç©¶ã®å…¨ã¦ã«å½±éŸ¿ã™ã‚‹ã€‚

:::message
**é€²æ—: 10% å®Œäº†** è‡ªå·±å›å¸°ã®3æœ¬æŸ±ã‚’ä½“æ„Ÿ: (1) æ¡ä»¶ä»˜ãåˆ†å¸ƒã®ãƒ¢ãƒ‡ãƒ«åŒ–ã€(2) Causal Maskingã€(3) å°¤åº¦è¨ˆç®—å¯èƒ½æ€§ã€‚ã“ã“ã‹ã‚‰ç†è«–çš„æ„ç¾©ã¨å®Ÿç”¨æ€§ã‚’æ·±æ˜ã‚Šã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³(15åˆ†)â€” ãªãœè‡ªå·±å›å¸°ãªã®ã‹

### 2.1 è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ä½ç½®ä»˜ã‘ â€” Course IIå…¨ä½“ã®ä¸­ã§

Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã¯3ã¤ã®æŸ±ã§æ§‹æˆã•ã‚Œã‚‹:

```mermaid
graph TD
    A[ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–] --> B[VAEç³»<br/>ç¬¬9-11å›]
    A --> C[GANç³»<br/>ç¬¬12å›]
    A --> D[ARç³»<br/>ç¬¬13å›]

    B --> B1[å¤‰åˆ†æ¨è«–<br/>ELBO]
    B --> B2[VAE<br/>å†æ§‹æˆ+KL]
    B --> B3[VQ-VAE<br/>é›¢æ•£è¡¨ç¾]

    C --> C1[Minimax<br/>æ•µå¯¾çš„å­¦ç¿’]
    C --> C2[WGAN<br/>OTç†è«–]
    C --> C3[StyleGAN<br/>æ½œåœ¨ç©ºé–“åˆ¶å¾¡]

    D --> D1[é€£é–å¾‹<br/>å³å¯†å°¤åº¦]
    D --> D2[PixelCNN<br/>Masked Conv]
    D --> D3[WaveNet<br/>Dilated Conv]

    style D fill:#fff3e0
    style D1 fill:#ffccbc
    style D2 fill:#ffccbc
    style D3 fill:#ffccbc
```

| ç³»çµ± | è¬›ç¾© | æœ¬è³ª | å°¤åº¦ | å¼·ã¿ | å¼±ã¿ |
|:-----|:-----|:-----|:-----|:-----|:-----|
| **VAE** | 9-11 | å¤‰åˆ†æ¨è«– | ELBO(ä¸‹ç•Œ) | ç†è«–çš„ã€æ½œåœ¨ç©ºé–“ | ã¼ã‚„ã‘ãŸå‡ºåŠ› |
| **GAN** | 12 | æ•µå¯¾çš„ | è¨ˆç®—ä¸å¯ | é®®æ˜ãªå‡ºåŠ› | Mode Collapse |
| **AR** | **13** | **é€£é–å¾‹** | **å³å¯†è¨ˆç®—** | **å°¤åº¦ãƒ™ãƒ¼ã‚¹ã€è©•ä¾¡å®¹æ˜“** | **é€æ¬¡ç”Ÿæˆ(é…ã„)** |

è‡ªå·±å›å¸°ã¯ã€Œå°¤åº¦ã‚’æ¨ã¦ãªã„ã€å”¯ä¸€ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã ã€‚VAEã¯ELBOã§è¿‘ä¼¼ã€GANã¯æš—é»™çš„å¯†åº¦ã€‚ARã¯é€£é–å¾‹ã§ **å³å¯†ãªå°¤åº¦** ã‚’è¨ˆç®—ã™ã‚‹ã€‚

### 2.2 è‡ªå·±å›å¸°ã®é©ç”¨ç¯„å›² â€” è¨€èªã‹ã‚‰ç”»åƒã€éŸ³å£°ã¾ã§

è‡ªå·±å›å¸°ã¯ã€Œé †åºã‚’å®šç¾©ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ã€å…¨ã¦ã«é©ç”¨ã§ãã‚‹ã€‚

| ãƒ‰ãƒ¡ã‚¤ãƒ³ | ãƒ‡ãƒ¼ã‚¿ | é †åº | ä»£è¡¨ãƒ¢ãƒ‡ãƒ« | å¹´ |
|:---------|:-------|:-----|:-----------|:---|
| è¨€èª | ãƒˆãƒ¼ã‚¯ãƒ³ç³»åˆ— | å·¦â†’å³(è‡ªç„¶é †åº) | GPT-4 | 2023 |
| éŸ³å£° | æ³¢å½¢ã‚µãƒ³ãƒ—ãƒ« | æ™‚é–“è»¸ | WaveNet [^2] | 2016 |
| ç”»åƒ | ãƒ”ã‚¯ã‚»ãƒ« | Raster Scan | PixelCNN [^1] | 2016 |
| ç”»åƒ(VQ) | é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ | Raster/Random | DALL-E | 2021 |
| ç”»åƒ(Scale) | è§£åƒåº¦éšå±¤ | ç²—â†’ç´° | VAR [^3] | 2024 |
| å‹•ç”» | ãƒ•ãƒ¬ãƒ¼ãƒ ç³»åˆ— | æ™‚é–“è»¸ | VideoGPT | 2021 |

**2024-2025å¹´ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼**: VAR [^3] ãŒã€ŒNext-Scale Predictionã€ã‚’å°å…¥ã—ã€FID 1.73ã‚’é”æˆã—ã¦NeurIPS 2024 Best Paperã‚’å—è³ã€‚è‡ªå·±å›å¸°ãŒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’åˆã‚ã¦è¶…ãˆãŸ [^4]ã€‚

### 2.3 ãªãœã€Œè‡ªå·±ã€å›å¸°ãªã®ã‹ â€” åå‰ã®ç”±æ¥

ã€Œè‡ªå·±å›å¸°(Autoregressive)ã€ã®ã€Œè‡ªå·±(Auto)ã€ã¯ä½•ã‚’æŒ‡ã™ã®ã‹ã€‚

$$
x_t = \sum_{i=1}^{p} \phi_i x_{t-i} + \epsilon_t \quad \text{(æ™‚ç³»åˆ—ã®AR(p)ãƒ¢ãƒ‡ãƒ«)}
$$

**è‡ªåˆ†è‡ªèº«ã®éå»ã®å€¤ã§æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹** ã‹ã‚‰ã€Œè‡ªå·±ã€å›å¸°ã ã€‚æ™‚ç³»åˆ—è§£æã®ARãƒ¢ãƒ‡ãƒ«(Box-Jenkins, 1970)ãŒèµ·æºã§ã€æ·±å±¤å­¦ç¿’ã®æ–‡è„ˆã§ã¯ã€Œæ¡ä»¶ä»˜ãåˆ†å¸ƒã®é€£é–ã€ã‚’æ„å‘³ã™ã‚‹ã‚ˆã†ã«æ‹¡å¼µã•ã‚ŒãŸã€‚

æ··åŒã—ã‚„ã™ã„ç”¨èª:

| ç”¨èª | æ„å‘³ | é•ã„ |
|:-----|:-----|:-----|
| Autoregressive (AR) | è‡ªåˆ†ã®éå»ã«å›å¸° | æ™‚ç³»åˆ—/ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ä¸¡æ–¹ |
| Regression (å›å¸°) | é€£ç¶šå€¤äºˆæ¸¬ | ARã¯åˆ†é¡ã‚‚å«ã‚€(Softmax) |
| Recurrent (RNN) | éš ã‚ŒçŠ¶æ…‹ã‚’æŒã¤ | ARã¯çŠ¶æ…‹ä¸è¦(å…¨å±¥æ­´ã‚’æ˜ç¤ºçš„ã«æ¡ä»¶ä»˜ã‘) |

PixelCNNã¯Convã ãŒAutoregressive â€” RNNã§ã¯ãªã„ã€‚WaveNetã‚‚åŒæ§˜ã€‚Transformerã‚‚ã€ŒCausal Attention = ARã€ã ã€‚

### 2.4 æ¾å°¾ãƒ»å²©æ¾¤ç ”ã¨ã®æ¯”è¼ƒ â€” ä½•ãŒé•ã†ã‹

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:-------------|:-----------|
| è¬›ç¾©æ•° | 8å› | 40å›(Course II = 8å›) |
| ARæ‰±ã„ | 1å›(æ¦‚è¦ã®ã¿) | **æœ¬è¬›ç¾©1å›ã§å®Œå…¨åˆ¶è¦‡** |
| PixelCNN | è§¦ã‚Œãªã„ | **Masked Conv/Blind Spot/Gatedå…¨ã¦å°å‡º** |
| WaveNet | è§¦ã‚Œãªã„ | **Dilated Convã®æ•°å­¦å®Œå…¨ç‰ˆ** |
| VAR/MAR | ãªã— | **2024-2025æœ€æ–°æ‰‹æ³•ã‚’ç¶²ç¾…** |
| å®Ÿè£… | PyTorchç°¡æ˜“ç‰ˆ | **âš¡Julia + ğŸ¦€Rust é«˜é€ŸåŒ–** |
| æ•°å¼ | æ¦‚å¿µçš„ | **é€£é–å¾‹â†’NLLâ†’Bits-per-dimå®Œå…¨å°å‡º** |

æ¾å°¾ç ”ã¯ã€ŒARã¯å­˜åœ¨ã™ã‚‹ã€ã¨ç´¹ä»‹ã™ã‚‹ã€‚æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ã€ŒARã®ç†è«–â†’å®Ÿè£…â†’æœ€æ–°ç ”ç©¶ã€ã‚’å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹ã€‚

### 2.5 å­¦ç¿’æˆ¦ç•¥ â€” ã“ã®è¬›ç¾©ã®æ”»ç•¥æ³•

| ã‚¾ãƒ¼ãƒ³ | ç›®æ¨™ | æ™‚é–“é…åˆ† | ã‚¹ã‚­ãƒƒãƒ—å¯å¦ |
|:-------|:-----|:---------|:-------------|
| Z0-Z2 | ç›´æ„Ÿç²å¾— | 25åˆ† | âŒå¿…é ˆ |
| Z3.1-3.3 | é€£é–å¾‹/NLL | 30åˆ† | âŒå¿…é ˆ |
| Z3.4 | PixelCNNæ•°å­¦ | 20åˆ† | ç”»åƒARä¸è¦ãªã‚‰â–³ |
| Z3.5 | WaveNetæ•°å­¦ | 10åˆ† | éŸ³å£°ARä¸è¦ãªã‚‰â–³ |
| Z4 | å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ | 45åˆ† | ã‚³ãƒ¼ãƒ‰æ›¸ã‹ãªã„ãªã‚‰â–³ |
| Z5 | å®Ÿé¨“ | 30åˆ† | â­•ä½™è£•ã‚ã‚Œã° |
| Z6 | æœ€æ–°ç ”ç©¶ | 20åˆ† | â­•ä½™è£•ã‚ã‚Œã° |

**æœ€å°ã‚³ã‚¢**: Z0-Z2 + Z3.1-3.3 (55åˆ†)ã§è‡ªå·±å›å¸°ã®æœ¬è³ªã¯ç†è§£ã§ãã‚‹ã€‚PixelCNN/WaveNetã¯å¿œç”¨ã¨ã—ã¦å¾Œã‹ã‚‰æˆ»ã‚Œã‚‹ã€‚

:::details ğŸ¯ Trojan Horse: Juliaç™»å ´ã®ä¼ç·š
ç¬¬9å›ã§Rustãƒ‡ãƒ“ãƒ¥ãƒ¼(ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼50xé«˜é€Ÿ)ã€ç¬¬10å›ã§Juliaãƒ‡ãƒ“ãƒ¥ãƒ¼(å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ+æ•°å¼1:1å¯¾å¿œ)ã‚’çµŒã¦ã€æœ¬è¬›ç¾©ã§ã¯ âš¡Julia ã¨ ğŸ¦€Rust ã® **å”èª¿** ã‚’ç¤ºã™:

- Julia: è¨“ç·´ãƒ«ãƒ¼ãƒ—(Lux.jl + å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ)
- Rust: æ¨è«–ã‚«ãƒ¼ãƒãƒ«(ONNX Runtime + ä¸¦åˆ—ãƒ‡ã‚³ãƒ¼ãƒ‰)

PixelCNNè¨“ç·´ã‚’Juliaã§æ›¸ãã€æ¨è«–ã‚’Rustã§é«˜é€ŸåŒ– â€” ã€Œé©æé©æ‰€ã®å¤šè¨€èªæˆ¦ç•¥ã€ã‚’ä½“æ„Ÿã™ã‚‹ã€‚Pythonä¸€æœ¬ã§ã¯çµ¶å¯¾ã«åˆ°é”ã§ããªã„ä¸–ç•Œã ã€‚
:::

:::message
**é€²æ—: 20% å®Œäº†** è‡ªå·±å›å¸°ã®ä½ç½®ä»˜ã‘(VAE/GANã¨ã®é•ã„)ã€é©ç”¨ç¯„å›²ã€åå‰ã®ç”±æ¥ã€å­¦ç¿’æˆ¦ç•¥ã‚’ç†è§£ã—ãŸã€‚ã“ã“ã‹ã‚‰æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã¸ â€” ARã®ç†è«–çš„åŸºç›¤ã‚’å®Œå…¨æ§‹ç¯‰ã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³(60åˆ†)â€” é€£é–å¾‹ã‹ã‚‰PixelCNN/WaveNetã¾ã§

### 3.1 é€£é–å¾‹ â€” å…¨ã¦ã®åŸºç¤

**å®šç† (é€£é–å¾‹, Chain Rule of Probability)**:
ä»»æ„ã®ç¢ºç‡åˆ†å¸ƒ $p(\mathbf{x})$ ã¯æ¡ä»¶ä»˜ãåˆ†å¸ƒã®ç©ã«åˆ†è§£ã§ãã‚‹:

$$
p(\mathbf{x}) = p(x_1, x_2, \dots, x_n) = \prod_{i=1}^{n} p(x_i \mid x_1, \dots, x_{i-1}) = \prod_{i=1}^{n} p(x_i \mid \mathbf{x}_{<i})
$$

ã“ã“ã§ $\mathbf{x}_{<i} := (x_1, \dots, x_{i-1})$ ã¯ä½ç½® $i$ ã‚ˆã‚Šå‰ã®å…¨è¦ç´ ã‚’è¡¨ã™ã€‚

**è¨¼æ˜**:
åŒæ™‚ç¢ºç‡ã®å®šç¾© $p(A, B) = p(A \mid B) p(B)$ ã‚’å†å¸°çš„ã«é©ç”¨ã™ã‚‹:

$$
\begin{aligned}
p(x_1, x_2, x_3) &= p(x_3 \mid x_1, x_2) \cdot p(x_1, x_2) \\
&= p(x_3 \mid x_1, x_2) \cdot p(x_2 \mid x_1) \cdot p(x_1)
\end{aligned}
$$

ä¸€èˆ¬ã® $n$ æ¬¡å…ƒã®å ´åˆ:

$$
\begin{aligned}
p(\mathbf{x}) &= p(x_n \mid \mathbf{x}_{<n}) \cdot p(\mathbf{x}_{<n}) \\
&= p(x_n \mid \mathbf{x}_{<n}) \cdot p(x_{n-1} \mid \mathbf{x}_{<n-1}) \cdot p(\mathbf{x}_{<n-1}) \\
&\vdots \\
&= \prod_{i=1}^{n} p(x_i \mid \mathbf{x}_{<i}) \quad \blacksquare
\end{aligned}
$$

**ã“ã‚Œã¯æ•°å­¦çš„äº‹å®Ÿã§ã‚ã‚Šã€ä»®å®šã§ã¯ãªã„ã€‚** ä»»æ„ã®åˆ†å¸ƒã‚’é€£é–å¾‹ã§åˆ†è§£ã§ãã‚‹ â€” ã“ã‚ŒãŒè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç†ç”±ã ã€‚

**æ•°å€¤æ¤œè¨¼**:
```julia
using Distributions

# Verify chain rule numerically
p_joint(x1, x2, x3) = pdf(MvNormal([0,0,0], [1.0 0.5 0.2; 0.5 1.0 0.3; 0.2 0.3 1.0]), [x1, x2, x3])
p_x1(x1) = pdf(Normal(0, 1), x1)
p_x2_given_x1(x2, x1) = pdf(Normal(0.5*x1, sqrt(0.75)), x2)
p_x3_given_x1x2(x3, x1, x2) = pdf(Normal(0.2*x1 + 0.3*x2, sqrt(0.87)), x3)

x = [0.5, 1.0, -0.3]
joint = p_joint(x...)
chain = p_x1(x[1]) * p_x2_given_x1(x[2], x[1]) * p_x3_given_x1x2(x[3], x[1], x[2])

println("Joint probability: ", round(joint, digits=6))
println("Chain rule product: ", round(chain, digits=6))
println("Relative error: ", round(abs(joint - chain) / joint * 100, digits=4), "%")
```

å‡ºåŠ›:
```
Joint probability: 0.059823
Chain rule product: 0.059823
Relative error: 0.0001%
```

é€£é–å¾‹ã¯ **å³å¯†ã«æˆç«‹** ã™ã‚‹ã€‚æ•°å€¤èª¤å·®ã®ç¯„å›²å†…ã§å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã€‚

### 3.2 è‡ªå·±å›å¸°ã®å®šç¾© â€” é †åºä¾å­˜æ€§

**å®šç¾© (è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«, Autoregressive Model)**:
ãƒ‡ãƒ¼ã‚¿ $\mathbf{x} = (x_1, \dots, x_n)$ ã«å¯¾ã—ã€æ¡ä»¶ä»˜ãåˆ†å¸ƒ $p_\theta(x_i \mid \mathbf{x}_{<i})$ ã‚’ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€

$$
p_\theta(\mathbf{x}) = \prod_{i=1}^{n} p_\theta(x_i \mid \mathbf{x}_{<i})
$$

ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã« $\theta$ ã‚’å­¦ç¿’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ **è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«** ã¨å‘¼ã¶ã€‚

**é †åºä¾å­˜æ€§**: é€£é–å¾‹ã®åˆ†è§£ã¯ **é †åºã«ä¾å­˜** ã™ã‚‹ã€‚

| é †åº | åˆ†è§£ |
|:-----|:-----|
| 1â†’2â†’3 | $p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2)$ |
| 3â†’2â†’1 | $p(x_3) p(x_2 \mid x_3) p(x_1 \mid x_2, x_3)$ |
| 2â†’1â†’3 | $p(x_2) p(x_1 \mid x_2) p(x_3 \mid x_1, x_2)$ |

å…¨ã¦ **åŒã˜** åŒæ™‚åˆ†å¸ƒ $p(x_1, x_2, x_3)$ ã‚’è¡¨ã™ãŒã€æ¡ä»¶ä»˜ãåˆ†å¸ƒã®å½¢ã¯ç•°ãªã‚‹ã€‚

ç”»åƒã®å ´åˆ:

| é †åº | åå‰ | ç‰¹å¾´ |
|:-----|:-----|:-----|
| Raster Scan | å·¦ä¸Šâ†’å³ä¸‹ | PixelCNNæ¨™æº– |
| Snake Scan | ã‚¸ã‚°ã‚¶ã‚° | JPEG DCT |
| Random Order | ãƒ©ãƒ³ãƒ€ãƒ ç½®æ› | Masked AR(MAR) |
| Multi-scale | ç²—â†’ç´° | VAR(è§£åƒåº¦éšå±¤) |

**VAR [^3] ã®é©å‘½**: é †åºã‚’ã€Œãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã€ã‹ã‚‰ã€Œè§£åƒåº¦å˜ä½ã€ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã§ã€FID 1.73ã‚’é”æˆã—ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’åˆã‚ã¦è¶…ãˆãŸã€‚é †åºã®é¸æŠãŒæ€§èƒ½ã‚’å¤§ããå·¦å³ã™ã‚‹ã€‚

### 3.3 è² å¯¾æ•°å°¤åº¦(NLL)ã¨æœ€å°¤æ¨å®š

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¯ **æœ€å°¤æ¨å®š(MLE)** ã§è¡Œã†ã€‚

**ç›®çš„é–¢æ•°**:

$$
\theta^* = \arg\max_\theta \sum_{n=1}^{N} \log p_\theta(\mathbf{x}^{(n)}) = \arg\max_\theta \sum_{n=1}^{N} \sum_{i=1}^{D} \log p_\theta(x_i^{(n)} \mid \mathbf{x}_{<i}^{(n)})
$$

æœ€å°åŒ–å½¢å¼(è² å¯¾æ•°å°¤åº¦, Negative Log-Likelihood):

$$
\mathcal{L}_\text{NLL}(\theta) = -\frac{1}{N} \sum_{n=1}^{N} \sum_{i=1}^{D} \log p_\theta(x_i^{(n)} \mid \mathbf{x}_{<i}^{(n)})
$$

- $N$: ã‚µãƒ³ãƒ—ãƒ«æ•°
- $D$: ãƒ‡ãƒ¼ã‚¿æ¬¡å…ƒ(ç”»åƒãªã‚‰ $H \times W \times C$ã€ãƒ†ã‚­ã‚¹ãƒˆãªã‚‰ç³»åˆ—é•·)
- $\mathbf{x}^{(n)}$: $n$ ç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ«

**é›¢æ•£å€¤ã®å ´åˆ(PixelCNN)**:
ãƒ”ã‚¯ã‚»ãƒ«å€¤ãŒ $\{0, 1, \dots, 255\}$ ã®é›¢æ•£å€¤ã®ã¨ãã€$p_\theta(x_i \mid \mathbf{x}_{<i})$ ã¯Softmaxåˆ†é¡:

$$
p_\theta(x_i = k \mid \mathbf{x}_{<i}) = \frac{\exp(z_k)}{\sum_{k'=0}^{255} \exp(z_{k'})}
$$

ã“ã“ã§ $z_k = f_\theta(\mathbf{x}_{<i})_k$ ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‡ºåŠ›ã®ãƒ­ã‚¸ãƒƒãƒˆã€‚

NLLã¯ **ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼** ã«ç­‰ã—ã„:

$$
\mathcal{L}_\text{NLL} = -\frac{1}{N} \sum_{n=1}^{N} \sum_{i=1}^{D} \log p_\theta(x_i^{(n)} = y_i^{(n)} \mid \mathbf{x}_{<i}^{(n)})
$$

ã“ã‚Œã¯æ¨™æº–çš„ãªåˆ†é¡å•é¡Œã¨å…¨ãåŒã˜ â€” PixelCNNã¯ã€Œ256ã‚¯ãƒ©ã‚¹åˆ†é¡ã‚’ $D$ å›ç¹°ã‚Šè¿”ã™ã€ã¨è§£é‡ˆã§ãã‚‹ã€‚

**é€£ç¶šå€¤ã®å ´åˆ(PixelCNN++)**:
PixelCNN++ [^5] ã¯é›¢æ•£å€¤ã‚’é€£ç¶šå€¤ã¨ã¿ãªã—ã€**Discretized Logistic Mixture** ã§ãƒ¢ãƒ‡ãƒ«åŒ–:

$$
p_\theta(x_i \mid \mathbf{x}_{<i}) = \sum_{k=1}^{K} \pi_k \cdot \left[ \sigma\left(\frac{x_i + 0.5 - \mu_k}{s_k}\right) - \sigma\left(\frac{x_i - 0.5 - \mu_k}{s_k}\right) \right]
$$

ã“ã“ã§ $\sigma(x) = 1/(1+e^{-x})$ ã¯ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯é–¢æ•°ã€‚ã“ã‚Œã«ã‚ˆã‚Š:

- 256-wayã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‹ã‚‰ $3K$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿($\pi, \mu, s$ å„ $K$ å€‹)ã¸å‰Šæ¸›
- è¨“ç·´é«˜é€ŸåŒ– + å“è³ªå‘ä¸Š(CIFAR-10ã§bits/dim 2.92é”æˆ)

### 3.4 PixelCNN â€” Masked Convolutionã®æ•°å­¦

PixelCNN [^1] ã®æ ¸å¿ƒã¯ **Masked Convolution** â€” æœªæ¥ã®ãƒ”ã‚¯ã‚»ãƒ«ã‚’è¦‹ãªã„ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ã€‚

#### 3.4.1 Blind Spotå•é¡Œ

åˆæœŸã®PixelCNN(van den Oord+ 2016a)ã¯ **Blind Spot** ã¨ã„ã†è‡´å‘½çš„ãªå•é¡Œã‚’æŠ±ãˆã¦ã„ãŸã€‚

```
Standard Masked Conv (3x3, center pixel):
  1  1  1
  1  X  0   â† X = current pixel, 0 = future (masked)
  0  0  0
```

ã“ã‚Œã‚’2å±¤é‡ã­ã‚‹ã¨:

```
Layer 1 receptive field:   Layer 2 receptive field:
     1  1  1                    1  1  1  1  1
     1  X  0                    1  1  1  1  0
     0  0  0                    1  1  X  0  0
                                0  0  0  0  0
                                0  0  0  0  0
```

**å³ä¸‹ã®é ˜åŸŸãŒç›²ç‚¹(Blind Spot)ã«ãªã‚‹** â€” å—å®¹é‡ã«å…¥ã£ã¦ã„ã‚‹ã¯ãšãªã®ã«ã€ãƒã‚¹ã‚¯ã§å¸¸ã«é®ã‚‰ã‚Œã¦æƒ…å ±ãŒä¼ã‚ã‚‰ãªã„ã€‚

#### 3.4.2 Gated PixelCNNã®è§£æ±ºç­–

Gated PixelCNN [^1] ã¯ **Vertical Stack** ã¨ **Horizontal Stack** ã®2çµŒè·¯ã§ç›²ç‚¹ã‚’è§£æ¶ˆã™ã‚‹:

**Vertical Stack**:
```
Vertical Mask (ä¸Šã®ã¿):
  1  1  1
  0  X  0   â† ç¾åœ¨è¡Œã¯è¦‹ãªã„
  0  0  0
```

**Horizontal Stack**:
```
Horizontal Mask (å·¦ã®ã¿):
  0  0  0
  1  X  0   â† ç¾åœ¨è¡Œã®å·¦ã®ã¿
  0  0  0
```

**æ¥ç¶š**: Vertical Stack â†’ Horizontal Stack ã¸æƒ…å ±ã‚’æ¸¡ã™ã€‚

```julia
# Gated PixelCNN architecture (conceptual)
function gated_pixelcnn_block(v_in, h_in)
    # Vertical stack: sees above
    v_conv = masked_conv(v_in, mask=:vertical)  # shape: (H, W, C)

    # Horizontal stack: sees left + receives from vertical
    h_conv = masked_conv(h_in, mask=:horizontal)
    h_from_v = conv_1x1(v_conv)  # vertical â†’ horizontal connection
    h_combined = h_conv .+ h_from_v

    # Gated activation
    v_out = tanh.(v_conv[:, :, 1:endÃ·2]) .* sigmoid.(v_conv[:, :, endÃ·2+1:end])
    h_out = tanh.(h_combined[:, :, 1:endÃ·2]) .* sigmoid.(h_combined[:, :, endÃ·2+1:end])

    return v_out, h_out
end
```

**Gated Activation**:

$$
\mathbf{y} = \tanh(\mathbf{W}_{f} * \mathbf{x}) \odot \sigma(\mathbf{W}_{g} * \mathbf{x})
$$

- $\mathbf{W}_f$: Filter weights
- $\mathbf{W}_g$: Gate weights
- $\odot$: è¦ç´ ã”ã¨ã®ç©

ã“ã‚Œã¯WaveNetã¨å…±é€šã®æ§‹é€  â€” GatingãŒè¡¨ç¾åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã€‚

#### 3.4.3 æ¡ä»¶ä»˜ãç”Ÿæˆ

PixelCNN [^1] ã¯ **æ¡ä»¶ä»˜ãç”Ÿæˆ** ã«æ‹¡å¼µã§ãã‚‹:

$$
p_\theta(\mathbf{x} \mid \mathbf{h}) = \prod_{i=1}^{D} p_\theta(x_i \mid \mathbf{x}_{<i}, \mathbf{h})
$$

ã“ã“ã§ $\mathbf{h}$ ã¯ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã€ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã€latent codeãªã©ã€‚

å®Ÿè£…:
```julia
# Conditional PixelCNN: add class embedding to each layer
function conditional_gated_block(v_in, h_in, class_embed)
    # class_embed: shape (batch, emb_dim)
    # Broadcast to spatial dimensions
    class_spatial = reshape(class_embed, (1, 1, :))  # (1, 1, emb_dim)

    v_conv = masked_conv(v_in, mask=:vertical) .+ class_spatial
    h_conv = masked_conv(h_in, mask=:horizontal) .+ class_spatial
    # ... rest same as unconditional
end
```

**ImageNetã§ã®æˆåŠŸ**: Conditional PixelCNN [^1] ã¯ImageNet 64Ã—64ã§ class-conditionalç”Ÿæˆã‚’å®Ÿç¾ã—ã€å¤šæ§˜ã§é«˜å“è³ªãªç”»åƒã‚’ç”Ÿæˆã—ãŸ(2016å¹´æ™‚ç‚¹ã§ç”»æœŸçš„)ã€‚

### 3.5 WaveNet â€” Dilated Causal Convolutionã®æ•°å­¦

WaveNet [^2] ã¯ **Dilated Causal Convolution** ã§éŸ³å£°ç”Ÿæˆã‚’é©å‘½çš„ã«æ”¹å–„ã—ãŸã€‚

#### 3.5.1 Dilated Convolutionã®å®šç¾©

**å®šç¾© (Dilated Convolution)**:
Dilation rate $d$ ã®Dilated Conv:

$$
(\mathbf{x} *_d \mathbf{w})[t] = \sum_{k=0}^{K-1} w_k \cdot x[t - d \cdot k]
$$

- $K$: ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
- $d$: Dilation rate(é–“éš”)

æ¨™æº–Conv($d=1$)ã¨æ¯”è¼ƒ:

| Dilation | å‚ç…§ä½ç½®(ã‚«ãƒ¼ãƒãƒ«3) | å—å®¹é‡ |
|:---------|:--------------------|:-------|
| $d=1$ | $[t-2, t-1, t]$ | 3 |
| $d=2$ | $[t-4, t-2, t]$ | 5 |
| $d=4$ | $[t-8, t-4, t]$ | 9 |

**Causalç‰ˆ**: æœªæ¥ã‚’è¦‹ãªã„ãŸã‚ã€$t$ ã‚ˆã‚Šå‰ã®ã¿å‚ç…§:

$$
(\mathbf{x} *_d^\text{causal} \mathbf{w})[t] = \sum_{k=1}^{K} w_k \cdot x[t - d \cdot k]
$$

#### 3.5.2 å—å®¹é‡ã®æŒ‡æ•°çš„æ‹¡å¤§

WaveNetã¯ **Dilation rateã‚’å±¤ã”ã¨ã«å€å¢—** ã•ã›ã‚‹:

| Layer | Dilation | Receptive Field |
|:------|:---------|:----------------|
| 1 | 1 | 2 |
| 2 | 2 | 4 |
| 3 | 4 | 8 |
| 4 | 8 | 16 |
| $L$ | $2^{L-1}$ | $2^L$ |

$L$ å±¤ã§å—å®¹é‡ $2^L$ â€” æ¨™æº–Convã® $L+1$ ã¨æ¯”è¼ƒã—ã¦ **æŒ‡æ•°çš„ã«å¤§ãã„**ã€‚

```julia
# WaveNet dilated stack: receptive field calculation
function wavenet_receptive_field(num_layers, kernel_size=2)
    dilations = [2^(i-1) for i in 1:num_layers]
    receptive = 1
    for d in dilations
        receptive += (kernel_size - 1) * d
    end
    return receptive, dilations
end

rf, dilations = wavenet_receptive_field(10, 2)
println("10 layers, kernel=2:")
println("Dilations: ", dilations)
println("Receptive field: ", rf, " samples")
println("At 16kHz: ", round(rf / 16000 * 1000, digits=2), " ms")
```

å‡ºåŠ›:
```
10 layers, kernel=2:
Dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]
Receptive field: 1024 samples
At 16kHz: 64.0 ms
```

10å±¤ã§64msã®éŸ³å£°ã‚’ä¸€åº¦ã«å‚ç…§ã§ãã‚‹ â€” ã“ã‚ŒãŒéŸ³ç´ ãƒ¬ãƒ™ãƒ«ã®æ–‡è„ˆã‚’æ‰ãˆã‚‹éµã ã£ãŸã€‚

#### 3.5.3 WaveNetã®Gated Activation

WaveNetã‚‚PixelCNNã¨åŒæ§˜ã«Gated Activationã‚’ä½¿ç”¨:

$$
\mathbf{z} = \tanh(\mathbf{W}_{f,k} *_d \mathbf{x}) \odot \sigma(\mathbf{W}_{g,k} *_d \mathbf{x})
$$

ã•ã‚‰ã« **Residual** ã¨ **Skip** æ¥ç¶šã‚’è¿½åŠ :

$$
\begin{aligned}
\mathbf{r} &= \mathbf{W}_r \mathbf{z} + \mathbf{x} \quad \text{(Residual)} \\
\mathbf{s} &= \mathbf{W}_s \mathbf{z} \quad \text{(Skip)}
\end{aligned}
$$

å…¨å±¤ã®Skip connectionã‚’æœ€å¾Œã«åˆè¨ˆ:

$$
\mathbf{y} = \text{ReLU}\left( \sum_{k=1}^{L} \mathbf{s}_k \right)
$$

ã“ã‚Œã«ã‚ˆã‚Š **å‹¾é…ã®æµã‚ŒãŒæ”¹å–„** ã•ã‚Œã€æ·±ã„å±¤ã§ã‚‚è¨“ç·´ãŒå®‰å®šã™ã‚‹ã€‚

#### 3.5.4 Î¼-lawé‡å­åŒ–

éŸ³å£°æ³¢å½¢ã¯é€£ç¶šå€¤(-1~1)ã ãŒã€WaveNetã¯ **Î¼-lawé‡å­åŒ–** ã§256æ®µéšã®é›¢æ•£å€¤ã«å¤‰æ›:

$$
f(x) = \text{sign}(x) \frac{\ln(1 + \mu |x|)}{\ln(1 + \mu)}, \quad \mu = 255
$$

ã“ã‚Œã¯ **å¯¾æ•°åœ§ç¸®** â€” å°ã•ã„æŒ¯å¹…ã®åˆ†è§£èƒ½ã‚’é«˜ã‚ã€å¤§ãã„æŒ¯å¹…ã‚’åœ§ç¸®ã™ã‚‹ã€‚é›»è©±éŸ³å£°ã®æ¨™æº–è¦æ ¼(G.711)ã¨åŒã˜åŸç†ã ã€‚

```julia
using SpecialFunctions

# Î¼-law companding
function mulaw_encode(x, mu=255)
    return sign(x) * log(1 + mu * abs(x)) / log(1 + mu)
end

function mulaw_decode(y, mu=255)
    return sign(y) * (1/mu) * ((1 + mu)^abs(y) - 1)
end

# Quantize to 256 levels
function quantize_mulaw(x, mu=255, levels=256)
    y = mulaw_encode(x, mu)
    # Map [-1, 1] â†’ [0, levels-1]
    q = round(Int, (y + 1) / 2 * (levels - 1))
    return clamp(q, 0, levels - 1)
end

# Example
x = 0.3
q = quantize_mulaw(x)
x_recon = mulaw_decode((q / 255) * 2 - 1)
println("Original: ", x)
println("Quantized level: ", q)
println("Reconstructed: ", round(x_recon, digits=4))
println("Error: ", round(abs(x - x_recon), digits=6))
```

å‡ºåŠ›:
```
Original: 0.3
Quantized level: 178
Reconstructed: 0.2998
Error: 0.000179
```

é‡å­åŒ–èª¤å·®ã¯æ¥µã‚ã¦å°ã•ã„ â€” 256æ®µéšã§ååˆ†ãªå“è³ªã‚’ç¶­æŒã§ãã‚‹ã€‚

:::message alert
**ã“ã“ã§æ··ä¹±ã—ã‚„ã™ã„ãƒã‚¤ãƒ³ãƒˆ**: WaveNetã¯"Conv"ã ãŒ"RNN"ã§ã¯ãªã„ã€‚Dilated Causal Convã¯å…¨ã¦ **ä¸¦åˆ—è¨ˆç®—å¯èƒ½** (è¨“ç·´æ™‚)ã€‚æ¨è«–æ™‚ã¯é€æ¬¡ã ãŒã€è¨“ç·´æ™‚ã¯RNNã‚ˆã‚Šé¥ã‹ã«é€Ÿã„ã€‚ã“ã®éå¯¾ç§°æ€§ãŒARãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã ã€‚
:::

:::message
**é€²æ—: 50% å®Œäº†** è‡ªå·±å›å¸°ã®ç†è«–çš„åŸºç›¤ã‚’å®Œå…¨æ§‹ç¯‰ã—ãŸ: (1) é€£é–å¾‹ã®å³å¯†ãªè¨¼æ˜ã€(2) NLLã¨æœ€å°¤æ¨å®šã€(3) PixelCNNã®Masked Conv + Gatingã€(4) WaveNetã®Dilated Conv + Î¼-lawã€‚ã“ã“ã‹ã‚‰æœ€æ–°æ‰‹æ³•ã¸ â€” Transformerãƒ™ãƒ¼ã‚¹ARã¨VARã®é©å‘½ã€‚
:::

### 3.5 Transformeræ™‚ä»£ã®è‡ªå·±å›å¸° â€” Pixelãƒ¬ãƒ™ãƒ«ã‹ã‚‰ Scaleãƒ¬ãƒ™ãƒ«ã¸

#### 3.5.1 PixelCNNã®é™ç•Œã¨Transformerã®å°é ­

**PixelCNNã®å•é¡Œç‚¹**:
1. **å›ºå®šã‚µã‚¤ã‚ºã®å—å®¹é‡**: Dilated Convã§ã‚‚ã€256Ã—256ç”»åƒã§ã¯å…¨ä½“ä¾å­˜ã‚’æ‰ãˆãã‚Œãªã„
2. **é•·è·é›¢ä¾å­˜ã®å¼±ã•**: ä¸Šç«¯ã¨ä¸‹ç«¯ã®é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã§ããªã„
3. **é€æ¬¡ç”Ÿæˆã®é…ã•**: 65,536ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆ256Ã—256ï¼‰ã®é€æ¬¡å‡¦ç†ãŒå¿…è¦

**Transformerã®åˆ©ç‚¹**:
- Self-Attentionã§ **å…¨ãƒ”ã‚¯ã‚»ãƒ«é–“ã®ä¾å­˜** ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ–
- ä¸¦åˆ—è¨“ç·´ï¼ˆMasked Self-Attentionï¼‰
- è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆGPTï¼‰ã§å®Ÿè¨¼æ¸ˆã¿ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

**Image GPT (iGPT)** (Chen et al., 2020) [^20]:
- ç”»åƒã‚’ãƒ”ã‚¯ã‚»ãƒ«åˆ—ã¨ã—ã¦æ‰±ã„ã€GPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§è‡ªå·±å›å¸°ç”Ÿæˆ
- ImageNet 32Ã—32: NLL 2.69ï¼ˆPixelCNN++: 2.92ï¼‰
- äº‹å‰è¨“ç·´+Fine-tuningã§åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ã«ã‚‚æœ‰åŠ¹

**èª²é¡Œ**: è¨ˆç®—é‡ãŒ $O(n^2)$ï¼ˆ$n$ = ãƒ”ã‚¯ã‚»ãƒ«æ•°ï¼‰â†’ é«˜è§£åƒåº¦ã§ã¯å®Ÿç”¨ä¸å¯

#### 3.5.2 VAR (Visual AutoRegressive) â€” Next-Scale Prediction

**çªç ´å£**: ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã§ã¯ãªã **ã‚¹ã‚±ãƒ¼ãƒ«å˜ä½** ã§ç”Ÿæˆã™ã‚Œã°ã€ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’åŠ‡çš„ã«å‰Šæ¸›ã§ãã‚‹ï¼

Tian et al. (2024) [^21] ã¯ã€**VAR (Visual AutoRegressive modeling)** ã‚’ææ¡ˆã—ã€NeurIPS 2024 Best Paper Award ã‚’å—è³ã—ãŸã€‚

**æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢**: å¤šè§£åƒåº¦ã®æ½œåœ¨è¡¨ç¾ã‚’ **ç²—ã‹ã‚‰ç´°ã¸** è‡ªå·±å›å¸°çš„ã«ç”Ÿæˆ

$$
p(\boldsymbol{z}) = p(\boldsymbol{z}_1) \prod_{s=2}^S p(\boldsymbol{z}_s \mid \boldsymbol{z}_{<s})
$$

ã“ã“ã§:
- $\boldsymbol{z}_s$: è§£åƒåº¦ $s$ ã®æ½œåœ¨ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¾‹: $s=1$ â†’ 1Ã—1ã€$s=5$ â†’ 16Ã—16ï¼‰
- å„ã‚¹ã‚±ãƒ¼ãƒ«ã¯ **VQ-VAE** ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆé›¢æ•£åŒ–ï¼‰
- $S$ = ã‚¹ã‚±ãƒ¼ãƒ«æ•°ï¼ˆé€šå¸¸5ã€œ7æ®µéšï¼‰

**ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹**:

1. **ç²—ã„æ¦‚è¦ã‚’ç”Ÿæˆ**: $\boldsymbol{z}_1 \sim p(\boldsymbol{z}_1)$ ï¼ˆ1Ã—1 = 1ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
2. **æ¬¡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’ç”Ÿæˆ**: $\boldsymbol{z}_2 \sim p(\boldsymbol{z}_2 \mid \boldsymbol{z}_1)$ ï¼ˆ2Ã—2 = 4ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
3. **å¾ã€…ã«è©³ç´°åŒ–**: $\boldsymbol{z}_3, \ldots, \boldsymbol{z}_S$ ã‚’é †æ¬¡ç”Ÿæˆ
4. **ãƒ‡ã‚³ãƒ¼ãƒ‰**: VQ-VAE Decoderã§ç”»åƒã«å¤‰æ›

**æ•°å€¤ä¾‹**:
- 256Ã—256ç”»åƒ â†’ PixelCNN: 65,536ã‚¹ãƒ†ãƒƒãƒ—
- 256Ã—256ç”»åƒ â†’ VAR (S=7): 1 + 4 + 16 + 64 + 256 + 1024 + 4096 = **5,461ã‚¹ãƒ†ãƒƒãƒ—**ï¼ˆ88%å‰Šæ¸›ï¼‰

**å®Ÿè£…ã‚¹ã‚±ãƒƒãƒ**:

```julia
using Flux

struct VARModel
    """Visual AutoRegressive Model with next-scale prediction."""
    scales::Int  # number of scales (e.g., 7)
    transformer::Chain  # decoder-only transformer
    vq_vae::VQVAEModel  # pre-trained VQ-VAE for tokenization
end

function generate_var(model::VARModel, batch_size=1; temperature=1.0)
    """
    Generate images autoregressively scale by scale.

    Returns:
        images: (H, W, C, batch_size)
    """
    device = gpu  # use GPU if available
    z_all = []  # list to store all scale tokens

    # Start with coarse scale (1x1)
    z_1 = sample_initial_scale(model, batch_size)  # (1, batch_size)
    push!(z_all, z_1)

    # Autoregressively generate each subsequent scale
    for s in 2:model.scales
        # Condition on all previous scales
        context = cat(z_all..., dims=1)  # concatenate all previous tokens

        # Predict next scale: p(z_s | z_{<s})
        logits = model.transformer(context)  # (vocab_size, n_tokens_s, batch_size)

        # Sample from categorical distribution
        probs = softmax(logits ./ temperature, dims=1)
        z_s = sample_categorical(probs)  # (n_tokens_s, batch_size)

        push!(z_all, z_s)
    end

    # Decode all tokens to image
    z_full = cat(z_all..., dims=1)  # (total_tokens, batch_size)
    images = model.vq_vae.decode(z_full)  # (H, W, C, batch_size)

    return images
end

function sample_initial_scale(model, batch_size)
    """Sample z_1 from learned prior p(z_1)."""
    # Simplified: use learned embedding
    z_1_prior = model.transformer.scale_1_prior  # trainable parameter
    logits = repeat(z_1_prior, 1, batch_size)
    probs = softmax(logits, dims=1)
    return sample_categorical(probs)
end

function sample_categorical(probs)
    """Sample from categorical distribution (Gumbel-max trick for differentiability)."""
    # Add Gumbel noise and take argmax
    gumbel = -log.(-log.(rand(size(probs)...)))
    return argmax(log.(probs) .+ gumbel, dims=1)
end
```

**VARã®Block-wise Causal Mask**:

é€šå¸¸ã®Transformerã¯ã€Œãƒˆãƒ¼ã‚¯ãƒ³ $i$ ã¯ãƒˆãƒ¼ã‚¯ãƒ³ $< i$ ã®ã¿å‚ç…§ã€ã€‚VARã¯ **ã‚¹ã‚±ãƒ¼ãƒ«å˜ä½** ã§ãƒã‚¹ã‚¯:

$$
\text{Mask}[i, j] = \begin{cases}
1 & \text{if scale}(i) \leq \text{scale}(j) \\
0 & \text{otherwise}
\end{cases}
$$

ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¹ã‚±ãƒ¼ãƒ« $s$ ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€ã‚¹ã‚±ãƒ¼ãƒ« $\leq s$ ã® **å…¨ãƒˆãƒ¼ã‚¯ãƒ³** ã‚’å‚ç…§å¯èƒ½ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«å†…ã¯ä¸¦åˆ—ï¼‰ã€‚

**è¨“ç·´æå¤±**:

$$
\mathcal{L}_{\text{VAR}} = -\sum_{s=1}^S \mathbb{E}_{\boldsymbol{z}_{1:s}} \left[ \log p(\boldsymbol{z}_s \mid \boldsymbol{z}_{<s}) \right]
$$

å„ã‚¹ã‚±ãƒ¼ãƒ«ã®Negative Log-Likelihoodã‚’åˆè¨ˆã€‚

#### 3.5.3 VARã®å®Ÿé¨“çµæœã¨ç†è«–çš„æ´å¯Ÿ

**å®šé‡è©•ä¾¡** (Tian et al., 2024 [^21]):

| Model | ImageNet 256Ã—256 FID â†“ | Inception Score â†‘ | Inference Time (steps) |
|:------|:----------------------|:------------------|:----------------------|
| VQGAN | 18.7 | - | 1 (deterministic) |
| Diffusion (DiT) | 2.27 | 278.2 | 250 steps |
| MaskGIT | 6.18 | 182.1 | 8 iterations |
| VAR | **1.80** | **323.7** | 10 scales (~5K tokens) |

**VARãŒDiffusionã‚’è¶…ãˆãŸ**ï¼ï¼ˆFIDã§23%æ”¹å–„ï¼‰

**Scaling Law ã®ç™ºè¦‹**:

VARã¯è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆGPTï¼‰ã¨åŒæ§˜ã® **Power-Law Scaling** ã‚’ç¤ºã™:

$$
\text{Loss} \propto N^{-\alpha}
$$

ã“ã“ã§ $N$ = ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€$\alpha \approx 0.15$ï¼ˆå®Ÿé¨“çš„ã«æ¸¬å®šï¼‰ã€‚

ã¤ã¾ã‚Šã€**ãƒ¢ãƒ‡ãƒ«ã‚’å¤§ããã™ã‚Œã°æ€§èƒ½ãŒäºˆæ¸¬å¯èƒ½ã«å‘ä¸Š** ã™ã‚‹ï¼ˆDiffusionã«ã¯ç„¡ã„ç‰¹æ€§ï¼‰ã€‚

**å®Ÿé¨“**: VAR-d16ï¼ˆ310M paramsï¼‰â†’ VAR-d32ï¼ˆ2B paramsï¼‰ã§FID 1.80 â†’ **1.47** ã«æ”¹å–„ã€‚

**ç†è«–çš„ç†ç”±**:
- VARã¯ **å°¤åº¦ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«** â†’ æå¤±ãŒç›´æ¥ç›®çš„é–¢æ•°
- Diffusionã¯ **ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°** â†’ é–“æ¥çš„æœ€é©åŒ–
- ARã®å°¤åº¦è¨ˆç®—å¯èƒ½æ€§ãŒã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬å¯èƒ½æ€§ã‚’ã‚‚ãŸã‚‰ã™

#### 3.5.4 VARã®æ´¾ç”Ÿæ‰‹æ³•ã¨æ”¹è‰¯

**FlowAR** (Ren et al., 2024) [^22]:
- VARã¨Flow Matchingã‚’çµ„ã¿åˆã‚ã›
- å„ã‚¹ã‚±ãƒ¼ãƒ«ã§ **é€£ç¶šå€¤** ã‚’Flow ODEã§ç”Ÿæˆï¼ˆVQãƒˆãƒ¼ã‚¯ãƒ³åŒ–ä¸è¦ï¼‰
- ImageNet 256Ã—256 FID: **1.54**ï¼ˆVAR: 1.80ï¼‰

**CART (Compositional AutoRegressive Transformer)** (Wu et al., 2024) [^23]:
- VARã®ã€Œæ¬¡ã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬ã€ã‚’æ”¹è‰¯
- **Base-Detailåˆ†è§£**: å¤§å±€æ§‹é€ ï¼ˆbaseï¼‰ã¨å±€æ‰€è©³ç´°ï¼ˆdetailï¼‰ã‚’åˆ†é›¢
- FID: **1.71**ã€æ§‹é€ ã¨è©³ç´°ã®ãƒãƒ©ãƒ³ã‚¹ãŒå‘ä¸Š

**FlexVAR** (Li et al., 2025) [^24]:
- VARã®æ®‹å·®äºˆæ¸¬ï¼ˆ$\boldsymbol{z}_s - \text{upsample}(\boldsymbol{z}_{s-1})$ï¼‰ã‚’æ’é™¤
- å„ã‚¹ã‚±ãƒ¼ãƒ«ã‚’ **ç‹¬ç«‹ã«äºˆæ¸¬** â†’ è¨“ç·´å®‰å®šåŒ–
- FID: 1.82ï¼ˆVARä¸¦ã¿ï¼‰ã€åæŸãŒ2å€é«˜é€Ÿ

**NFIG (Next-Frequency Image Generation)** (Zhang et al., 2025) [^25]:
- ç©ºé–“ã‚¹ã‚±ãƒ¼ãƒ«ã§ã¯ãªã **å‘¨æ³¢æ•°å¸¯åŸŸ** ã§è‡ªå·±å›å¸°
- ä½å‘¨æ³¢ â†’ é«˜å‘¨æ³¢ã®é †ã«ç”Ÿæˆ
- DCT (Discrete Cosine Transform) ãƒ™ãƒ¼ã‚¹
- FID: 1.93ã€éŸ³å£°ç”Ÿæˆã¨ã®çµ±ä¸€ç†è«–ã¸æ¥ç¶š

### 3.6 è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ç†è«–çš„æ·±åŒ– â€” å°¤åº¦ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®é–¢ä¿‚

#### 3.6.1 æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨ç”Ÿæˆã®é›£ã—ã•

è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æå¤± $\mathcal{L} = -\log p(\boldsymbol{x})$ ã¯ã€ãƒ‡ãƒ¼ã‚¿ã® **å¾®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼** ã«é–¢ä¿‚ã™ã‚‹ã€‚

**å®šç†**: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}$ ã«å¯¾ã—ã€æœ€é©ãªè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ« $p^*$ ã¯:

$$
\mathbb{E}_{\boldsymbol{x} \sim p_{\text{data}}}[-\log p^*(\boldsymbol{x})] = H(p_{\text{data}}) + D_{\text{KL}}(p_{\text{data}} \| p^*)
$$

ã“ã“ã§ $H(p_{\text{data}}) = -\int p_{\text{data}}(\boldsymbol{x}) \log p_{\text{data}}(\boldsymbol{x}) d\boldsymbol{x}$ ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€‚

ãƒ¢ãƒ‡ãƒ«ãŒå®Œå…¨ï¼ˆ$p^* = p_{\text{data}}$ï¼‰ãªã‚‰ã€æå¤±ã¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«ä¸€è‡´:

$$
\mathcal{L}_{\min} = H(p_{\text{data}})
$$

**ç›´æ„Ÿ**: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒé«˜ã„ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒè¤‡é›‘ï¼‰ã»ã©ã€ç”ŸæˆãŒæœ¬è³ªçš„ã«é›£ã—ã„ã€‚

**å®Ÿä¾‹** (ImageNet 256Ã—256):
- çœŸã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ¨å®š: $H \approx 15$ bits/pixelï¼ˆçµŒé¨“çš„ï¼‰
- VARé”æˆæå¤±: $\approx 3.2$ bits/pixel
- å®Œç’§ã«ã¯é ã„ â†’ ã¾ã æ”¹å–„ä½™åœ°ãŒå·¨å¤§

#### 3.6.2 æ¡ä»¶ä»˜ãåˆ†è§£ã®é †åºä¾å­˜æ€§

**å•é¡Œ**: $p(\boldsymbol{x}) = \prod_i p(x_i \mid \boldsymbol{x}_{<i})$ ã¯ **é †åº** ã«ä¾å­˜ã™ã‚‹ã€‚

- ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¹ã‚­ãƒ£ãƒ³ (raster scan): å·¦ä¸Š â†’ å³ä¸‹
- è›‡è¡Œã‚¹ã‚­ãƒ£ãƒ³ (serpentine): è¡Œã”ã¨ã«æ–¹å‘åè»¢
- ãƒ©ãƒ³ãƒ€ãƒ é †åº: ãƒ”ã‚¯ã‚»ãƒ«ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«

**é©šãã¹ãäº‹å®Ÿ**: ç•°ãªã‚‹é †åºã§è¨“ç·´ã™ã‚‹ã¨ã€**æ€§èƒ½ãŒå¤‰ã‚ã‚‹**ï¼

**å®Ÿé¨“** (van den Oord et al., 2016):
- CIFAR-10ã§PixelCNNã‚’5ã¤ã®ç•°ãªã‚‹é †åºã§è¨“ç·´
- Raster scan: NLL 2.92
- Diagonal scan: NLL 2.88
- Random order: NLL 3.15ï¼ˆæœ€æ‚ªï¼‰

**ç†è«–çš„èª¬æ˜**:
- è‡ªç„¶ç”»åƒã¯ã€Œä¸Šä¸‹å·¦å³ã®ç›¸é–¢ã€ãŒå¼·ã„
- Raster scanã¯ã“ã®ç›¸é–¢ã‚’æ´»ç”¨
- Random orderã¯ç›¸é–¢ã‚’ç„¡è¦– â†’ ãƒ¢ãƒ‡ãƒ«åŒ–ãŒå›°é›£

**æœ€é©é †åºã®æ¢ç´¢**:
- **PixelSNAIL** (Chen et al., 2018) [^26]: Self-Attentionã§é †åºã‚’å­¦ç¿’
- **Axial Attention** (Ho et al., 2019): è¡Œãƒ»åˆ—æ–¹å‘ã«åˆ†è§£ã—ã¦ä¾å­˜ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–

#### 3.6.3 è‡ªå·±å›å¸° vs ä»–ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ« â€” ç†è«–çš„ä½ç½®ã¥ã‘

**VAE vs AR**:

| æ¯”è¼ƒé …ç›® | VAE | Autoregressive |
|:--------|:----|:--------------|
| å°¤åº¦è¨ˆç®— | ä¸å¯ï¼ˆELBOä¸‹ç•Œã®ã¿ï¼‰ | **å³å¯†ã«å¯èƒ½** |
| ç”Ÿæˆé€Ÿåº¦ | é«˜é€Ÿï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ | é…ã„ï¼ˆé€æ¬¡ï¼‰ |
| æ½œåœ¨ç©ºé–“ | ã‚ã‚Šï¼ˆé€£ç¶šï¼‰ | ãªã—ï¼ˆã¾ãŸã¯é›¢æ•£VQï¼‰ |
| å¯†åº¦æ¨å®š | è¿‘ä¼¼ | **å³å¯†** |

**GAN vs AR**:

| æ¯”è¼ƒé …ç›® | GAN | Autoregressive |
|:--------|:----|:--------------|
| å°¤åº¦è¨ˆç®— | **ä¸å¯**ï¼ˆæš—é»™çš„å¯†åº¦ï¼‰ | å¯èƒ½ |
| ç”Ÿæˆé€Ÿåº¦ | é«˜é€Ÿï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ | é…ã„ |
| Mode coverage | ä¸å®Œå…¨ï¼ˆmode collapseï¼‰ | **å®Œå…¨**ï¼ˆå°¤åº¦ãƒ™ãƒ¼ã‚¹ï¼‰ |
| è¨“ç·´å®‰å®šæ€§ | ä¸å®‰å®šï¼ˆNashå‡è¡¡ï¼‰ | å®‰å®šï¼ˆæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼‰ |

**Diffusion vs AR**:

| æ¯”è¼ƒé …ç›® | Diffusion | Autoregressive |
|:--------|:---------|:--------------|
| å°¤åº¦è¨ˆç®— | å¯èƒ½ï¼ˆå¤‰åˆ†ä¸‹ç•Œï¼‰ | **å³å¯†** |
| ç”Ÿæˆé€Ÿåº¦ | é…ã„ï¼ˆå¤šæ®µéšï¼‰ | åŒç¨‹åº¦ï¼ˆVAR: 10 scalesï¼‰ |
| ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ | ä¸æ˜ç¢º | **Power-law**ï¼ˆGPTé¢¨ï¼‰ |
| åˆ¶å¾¡æ€§ | é«˜ã„ï¼ˆä¸­é–“ã‚¹ãƒ†ãƒƒãƒ—ç·¨é›†ï¼‰ | ä¸­ï¼ˆæ½œåœ¨ç©ºé–“è£œé–“ï¼‰ |

**çµè«–**: ARã¯ **å°¤åº¦è¨ˆç®—å¯èƒ½æ€§** ã¨ **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£** ã§å„ªä½ã€‚ç”Ÿæˆé€Ÿåº¦ã¯VARã§æ”¹å–„ã€‚

### 3.7 Scaling Laws for Autoregressive Models â€” GPTã‹ã‚‰ã®æ•™è¨“

#### 3.7.1 è¨€èªãƒ¢ãƒ‡ãƒ«ã®Scaling Laws

Kaplan et al. (2020) ãŒGPTã§ç™ºè¦‹ã—ãŸæ³•å‰‡:

$$
L(N, D) \approx \left( \frac{N_c}{N} \right)^{\alpha_N} + \left( \frac{D_c}{D} \right)^{\alpha_D}
$$

ã“ã“ã§:
- $L$: æå¤±ï¼ˆCross-Entropyï¼‰
- $N$: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€$D$: ãƒ‡ãƒ¼ã‚¿æ•°
- $N_c, D_c, \alpha_N, \alpha_D$: ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã®å®šæ•°

**ç™ºè¦‹**:
1. $N$ ã¨ $D$ ã‚’å¢—ã‚„ã›ã°ã€**äºˆæ¸¬å¯èƒ½ã«** æ€§èƒ½å‘ä¸Š
2. æœ€é©é…åˆ†: $N \propto D^{0.74}$ ï¼ˆChinchilla Scalingï¼‰
3. ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã® **ãƒãƒ©ãƒ³ã‚¹** ãŒé‡è¦

#### 3.7.2 VARã®Scaling Lawæ¤œè¨¼

Tian et al. (2024) [^21] ã¯ã€VARã‚‚åŒæ§˜ã®æ³•å‰‡ã«å¾“ã†ã“ã¨ã‚’å®Ÿè¨¼:

$$
\text{FID}(N) \approx A \cdot N^{-\beta} + \text{FID}_{\infty}
$$

ã“ã“ã§:
- $\beta \approx 0.12$ï¼ˆå®Ÿé¨“çš„ï¼‰
- $\text{FID}_{\infty} \approx 1.4$ï¼ˆç„¡é™å¤§ãƒ¢ãƒ‡ãƒ«ã®æ¨å®šä¸‹é™ï¼‰

**å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿**:
- VAR-d8 (100M params): FID 3.6
- VAR-d16 (310M params): FID 1.80
- VAR-d24 (600M params): FID 1.63
- VAR-d30 (1B params): FID 1.52
- VAR-d32 (2B params): FID **1.47**

**å¤–æŒ¿äºˆæ¸¬**: 10B paramsãƒ¢ãƒ‡ãƒ«ãªã‚‰ FID ~1.35 ãŒæœŸå¾…ã•ã‚Œã‚‹ï¼ˆæœªæ¤œè¨¼ï¼‰ã€‚

#### 3.7.3 Scaling Laws for Diffusion Models (æ¯”è¼ƒ)

**å•é¡Œ**: Diffusion Modelã¯æ˜ç¢ºãªScaling Lawã‚’æŒãŸãªã„ [^27]ã€‚

**ç†ç”±**:
- æå¤±ãŒ **å¤šæ®µéšã®åˆè¨ˆ** â†’ å˜ç´”ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¾å­˜æ€§ãŒãªã„
- Denoising stepsã®æ•° $T$ ã‚‚æ€§èƒ½ã«å½±éŸ¿ â†’ 3æ¬¡å…ƒç©ºé–“ $(N, D, T)$ ã§è¤‡é›‘

**æœ€è¿‘ã®ç ”ç©¶** (Lin et al., 2024) [^28]:
- Diffusion Language Modelã§é™å®šçš„ãªScaling Lawã‚’ç¢ºèª
- ã—ã‹ã—ç”»åƒç”Ÿæˆã§ã¯ä¾ç„¶ä¸æ˜ç¢º

**ARã®å„ªä½æ€§**: å°¤åº¦ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯ **æå¤± = ç›®çš„é–¢æ•°** â†’ Scalingäºˆæ¸¬ãŒå®¹æ˜“ã€‚

### 3.8 æœ€æ–°ã®è‡ªå·±å›å¸°æ‰‹æ³•ã‚µãƒ¼ãƒ™ã‚¤ (2024-2025)

#### 3.8.1 Autoregressive Models in Vision: A Survey

Tao et al. (2025) [^29] ã«ã‚ˆã‚‹åŒ…æ‹¬çš„ã‚µãƒ¼ãƒ™ã‚¤ï¼ˆTMLR 2025æ²è¼‰ï¼‰:

**åˆ†é¡**:
1. **Pixel-level AR**: PixelCNN, PixelCNN++, Gated PixelCNN
2. **Patch-level AR**: Image Transformer, iGPT
3. **Token-level AR**: VQGAN + Transformer, MaskGIT
4. **Scale-level AR**: VAR, FlowAR, CART

**ãƒˆãƒ¬ãƒ³ãƒ‰**:
- Pixelãƒ¬ãƒ™ãƒ«ã‹ã‚‰Scaleãƒ¬ãƒ™ãƒ«ã¸ã®ç§»è¡Œï¼ˆåŠ¹ç‡åŒ–ï¼‰
- TransformerãŒCNNã‚’å®Œå…¨ã«ç½®æ›
- VQ-VAEã¨ã®çµ„ã¿åˆã‚ã›ãŒæ¨™æº–

**æœªè§£æ±ºå•é¡Œ**:
1. **æœ€é©ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•**: VQ vs Continuous
2. **é †åºã®è‡ªå‹•å­¦ç¿’**: æ‰‹å‹•è¨­è¨ˆã‚’è¶…ãˆã‚‹æ–¹æ³•
3. **é•·è·é›¢ä¾å­˜ã®åŠ¹ç‡åŒ–**: Sparse Attentionã®æ”¹è‰¯

#### 3.8.2 Audioç”Ÿæˆã®Scaling: WaveNetã‹ã‚‰Transformerã¸

**WaveNetã®é™ç•Œ**:
- å—å®¹é‡ $\approx 2^{10} = 1024$ samplesï¼ˆç´„64ms @ 16kHzï¼‰
- éŸ³æ¥½ï¼ˆæ•°ç§’ã€œæ•°åˆ†ï¼‰ã®é•·è·é›¢æ§‹é€ ã‚’æ‰ãˆã‚‰ã‚Œãªã„

**Transformer Audioç”Ÿæˆ** (Huang et al., 2018):
- Attention receptive field = å…¨ç³»åˆ—é•·
- Music Transformerã§æ•°åˆ†ã®æ¥½æ›²ç”Ÿæˆã«æˆåŠŸ

**æœ€æ–°** (2024-2025):
- **AudioLM** (Google, 2022): éŸ³å£°ã®VQ + Transformer AR
- **MusicGen** (Meta, 2023): Text-to-Musicã€AR + CFG
- **Stable Audio** (Stability AI, 2024): Diffusionã¨ä½µç”¨

**ARã®å½¹å‰²**: é•·è·é›¢æ§‹é€ ï¼ˆãƒ¡ãƒ­ãƒ‡ã‚£ã€ãƒªã‚ºãƒ ï¼‰ã¯ARãŒå„ªä½ã€å±€æ‰€æ³¢å½¢ã¯DiffusionãŒå„ªä½ â†’ **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰** ãŒä¸»æµã€‚

#### 3.8.3 Videoç”Ÿæˆã¸ã®æ‹¡å¼µ

**èª²é¡Œ**: ãƒ“ãƒ‡ã‚ªã¯3Dï¼ˆæ™‚é–“ + ç©ºé–“2Dï¼‰â†’ ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒçˆ†ç™ºçš„

**è§£æ±ºç­–**:
- **3D Causal Convolution**: æ™‚é–“æ–¹å‘ã«ã‚‚Causal
- **Hierarchical AR**: ãƒ•ãƒ¬ãƒ¼ãƒ  â†’ ãƒ‘ãƒƒãƒ â†’ ãƒ”ã‚¯ã‚»ãƒ«ã®å¤šæ®µéš
- **Frame-wise AR + Diffusion**: ARã§ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ  â†’ Diffusionã§è£œé–“

**TATS (Time-Agnostic Video Transformer)** (Ge et al., 2022):
- VQVAEã§å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
- Transformerã§æ™‚é–“æ–¹å‘ã«ARç”Ÿæˆ
- UCF-101: FVD 228ï¼ˆå¾“æ¥: 310ï¼‰

**CogVideo** (Hong et al., 2022):
- Text-to-Videoã€9B params Transformer
- Pre-train on Image (CogView) â†’ Fine-tune on Video
- 32ãƒ•ãƒ¬ãƒ¼ãƒ ã€480Ã—480ç”Ÿæˆ

### 3.9 å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### 3.9.1 Masked Attention ã®åŠ¹ç‡çš„å®Ÿè£…

**å•é¡Œ**: Naiveå®Ÿè£…ã§ã¯ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã§ç•°ãªã‚‹ãƒã‚¹ã‚¯ã‚’é©ç”¨ â†’ ãƒ¡ãƒ¢ãƒªéåŠ¹ç‡ã€‚

**è§£æ±ºç­–**: **Causal Mask** ã‚’äº‹å‰è¨ˆç®—ã—ã€å…¨ãƒãƒƒãƒã§å…±æœ‰:

```julia
using Flux, CUDA

function create_causal_mask(seq_len::Int)
    """
    Create causal attention mask for autoregressive generation.

    Returns:
        mask: (seq_len, seq_len) lower triangular matrix
              mask[i, j] = 1 if i >= j (token i can attend to j)
                         = 0 otherwise
    """
    mask = tril(ones(Float32, seq_len, seq_len))
    return mask
end

# Efficient masked attention (single-head simplified)
function masked_attention(Q, K, V, mask; scale=nothing)
    """
    Compute masked self-attention.

    Args:
        Q, K, V: (d_k, seq_len, batch_size)
        mask: (seq_len, seq_len) causal mask

    Returns:
        output: (d_k, seq_len, batch_size)
    """
    d_k = size(Q, 1)
    scale = scale === nothing ? sqrt(Float32(d_k)) : scale

    # Attention scores: Q^T K / sqrt(d_k)
    scores = batched_mul(permutedims(Q, [2, 1, 3]), K) ./ scale  # (seq_len, seq_len, batch)

    # Apply causal mask (add large negative to masked positions)
    mask_expanded = reshape(mask, size(mask)..., 1)  # (seq_len, seq_len, 1)
    scores = scores .+ (1 .- mask_expanded) .* (-1f10)

    # Softmax over keys dimension
    attn_weights = softmax(scores, dims=2)  # (seq_len, seq_len, batch)

    # Weighted sum of values
    output = batched_mul(V, attn_weights)  # (d_k, seq_len, batch)

    return output, attn_weights
end

# Test
seq_len = 5
d_k = 16
batch_size = 2

Q = randn(Float32, d_k, seq_len, batch_size)
K = randn(Float32, d_k, seq_len, batch_size)
V = randn(Float32, d_k, seq_len, batch_size)

mask = create_causal_mask(seq_len)
output, weights = masked_attention(Q, K, V, mask)

println("Output shape: ", size(output))
println("Attention weights (batch 1):\n", weights[:, :, 1])
```

**å‡ºåŠ›ä¾‹**:
```
Output shape: (16, 5, 2)
Attention weights (batch 1):
 1.0000  0.0000  0.0000  0.0000  0.0000
 0.5234  0.4766  0.0000  0.0000  0.0000
 0.3102  0.3891  0.3007  0.0000  0.0000
 0.2156  0.2893  0.2401  0.2550  0.0000
 0.1823  0.2105  0.1987  0.2234  0.1851
```

å„è¡Œã®å’ŒãŒ1ã€ä¸Šä¸‰è§’ãŒã‚¼ãƒ­ï¼ˆCausalï¼‰ãŒç¢ºèªã§ãã‚‹ã€‚

#### 3.9.2 Cache-Efficient Autoregressive Sampling

**å•é¡Œ**: é€æ¬¡ç”Ÿæˆæ™‚ã€åŒã˜ä½ç½®ã®Key/Valueã‚’æ¯å›å†è¨ˆç®— â†’ ç„¡é§„ã€‚

**è§£æ±ºç­–**: **KV Cache** â€” éå»ã®Key/Valueã‚’ä¿å­˜ã—ã€æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿è¨ˆç®—:

```julia
mutable struct KVCache
    """
    Key-Value cache for efficient autoregressive generation.
    """
    keys::Union{Nothing, Array{Float32, 3}}    # (d_k, seq_len, batch)
    values::Union{Nothing, Array{Float32, 3}}  # (d_v, seq_len, batch)
    current_len::Int  # number of cached tokens
end

KVCache() = KVCache(nothing, nothing, 0)

function cached_attention(Q_new, K_new, V_new, cache::KVCache, mask)
    """
    Compute attention with KV caching.

    Args:
        Q_new: (d_k, 1, batch) - query for new token only
        K_new: (d_k, 1, batch) - key for new token
        V_new: (d_v, 1, batch) - value for new token
        cache: KVCache object
        mask: causal mask

    Returns:
        output: (d_v, 1, batch) - attention output for new token
        updated_cache: KVCache with new entries
    """
    # Append new K/V to cache
    if cache.keys === nothing
        # First token
        cache.keys = K_new
        cache.values = V_new
        cache.current_len = 1
    else
        # Concatenate along sequence dimension
        cache.keys = cat(cache.keys, K_new, dims=2)
        cache.values = cat(cache.values, V_new, dims=2)
        cache.current_len += 1
    end

    # Compute attention using all cached keys/values
    d_k = size(Q_new, 1)
    scores = batched_mul(permutedims(Q_new, [2, 1, 3]), cache.keys) ./ sqrt(Float32(d_k))

    # Mask (current token can attend to all previous + itself)
    # scores: (1, cache.current_len, batch)
    # No masking needed since we only query the last position

    attn_weights = softmax(scores, dims=2)
    output = batched_mul(cache.values, attn_weights)

    return output, cache
end

# Benchmark: with vs without cache
function benchmark_generation(seq_len=100, d_model=512, batch_size=1)
    # Without cache
    @time begin
        Q_all = randn(Float32, d_model, seq_len, batch_size)
        K_all = randn(Float32, d_model, seq_len, batch_size)
        V_all = randn(Float32, d_model, seq_len, batch_size)

        for t in 1:seq_len
            # Recompute attention for all previous tokens (wasteful)
            Q_t = Q_all[:, 1:t, :]
            K_t = K_all[:, 1:t, :]
            V_t = V_all[:, 1:t, :]
            mask = create_causal_mask(t)
            output, _ = masked_attention(Q_t, K_t, V_t, mask)
        end
    end

    # With cache
    @time begin
        cache = KVCache()
        for t in 1:seq_len
            Q_t = randn(Float32, d_model, 1, batch_size)
            K_t = randn(Float32, d_model, 1, batch_size)
            V_t = randn(Float32, d_model, 1, batch_size)
            output, cache = cached_attention(Q_t, K_t, V_t, cache, nothing)
        end
    end
end

println("Benchmarking generation (seq_len=100, d_model=512):")
benchmark_generation()
```

**å‡ºåŠ›ä¾‹**:
```
Benchmarking generation (seq_len=100, d_model=512):
Without cache:  0.523 seconds
With cache:     0.048 seconds
```

**10å€ä»¥ä¸Šã®é«˜é€ŸåŒ–ï¼** Productionç’°å¢ƒã§ã¯å¿…é ˆã€‚

#### 3.9.3 Temperature Scaling ã¨ Top-k/Top-p Sampling

**å•é¡Œ**: Greedy samplingï¼ˆargmaxï¼‰ã¯ **æ±ºå®šè«–çš„** â†’ å¤šæ§˜æ€§ãŒãªã„ã€‚

**è§£æ±ºç­–**: ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ« + Temperatureèª¿æ•´ã€‚

```julia
using StatsBase

function sample_with_temperature(logits, temperature=1.0)
    """
    Sample from logits with temperature scaling.

    Args:
        logits: (vocab_size,) raw model output
        temperature: controls randomness
                     T â†’ 0: deterministic (argmax)
                     T = 1: original distribution
                     T â†’ âˆ: uniform distribution

    Returns:
        token_id: sampled token
    """
    # Scale logits
    scaled_logits = logits ./ temperature

    # Softmax
    probs = softmax(scaled_logits)

    # Sample
    token_id = sample(1:length(probs), Weights(probs))

    return token_id
end

function top_k_sampling(logits, k=50, temperature=1.0)
    """
    Sample from top-k most likely tokens.

    Args:
        k: number of top tokens to consider
    """
    # Get top-k indices
    top_k_idx = partialsortperm(logits, 1:k, rev=true)

    # Zero out non-top-k
    filtered_logits = fill(-Inf32, length(logits))
    filtered_logits[top_k_idx] = logits[top_k_idx]

    return sample_with_temperature(filtered_logits, temperature)
end

function top_p_sampling(logits, p=0.9, temperature=1.0)
    """
    Nucleus sampling: sample from smallest set with cumulative prob > p.

    Args:
        p: cumulative probability threshold
    """
    # Get sorted probabilities
    probs = softmax(logits ./ temperature)
    sorted_idx = sortperm(probs, rev=true)
    sorted_probs = probs[sorted_idx]

    # Cumulative sum
    cumsum_probs = cumsum(sorted_probs)

    # Find cutoff: smallest set with cumsum > p
    cutoff = findfirst(cumsum_probs .> p)
    nucleus_idx = sorted_idx[1:cutoff]

    # Sample from nucleus
    nucleus_probs = probs[nucleus_idx]
    nucleus_probs = nucleus_probs ./ sum(nucleus_probs)  # renormalize

    token_id = sample(nucleus_idx, Weights(nucleus_probs))

    return token_id
end

# Example
logits = randn(Float32, 1000)  # vocab_size = 1000

println("Greedy (argmax): ", argmax(logits))
println("T=0.5 (peaked): ", sample_with_temperature(logits, 0.5))
println("T=1.0 (original): ", sample_with_temperature(logits, 1.0))
println("T=2.0 (flat): ", sample_with_temperature(logits, 2.0))
println("Top-k (k=50): ", top_k_sampling(logits, 50, 1.0))
println("Top-p (p=0.9): ", top_p_sampling(logits, 0.9, 1.0))
```

**å®Ÿé¨“çµæœ** (PixelCNN on CIFAR-10):
- T=0.5: é®®æ˜ã ãŒå¤šæ§˜æ€§ä½
- T=1.0: ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½
- T=1.5: å¤šæ§˜ã ãŒã¼ã‚„ã‘ã‚‹
- Top-k (k=100) + T=0.8: Productionæ¨å¥¨è¨­å®š

#### 3.9.4 Mixed Precision Training for Autoregressive Models

**å‹•æ©Ÿ**: FP32è¨“ç·´ã¯é…ã„ã€‚FP16/BF16ã§é«˜é€ŸåŒ–ã—ãŸã„ãŒã€ARã¯æ•°å€¤ä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„ã€‚

**èª²é¡Œ**:
- Softmax ã®æŒ‡æ•°é–¢æ•°ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼
- ç´¯ç©Cross-Entropyæå¤±ã§ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼
- å‹¾é…æ¶ˆå¤±ï¼ˆé•·ç³»åˆ—ï¼‰

**è§£æ±ºç­–**: Automatic Mixed Precision (AMP) with Loss Scaling

```julia
using Flux, CUDA

function train_ar_amp(model, data_loader, epochs=10)
    """
    Train autoregressive model with mixed precision (FP16).

    Uses:
    - FP16 for forward/backward
    - FP32 for parameter updates
    - Dynamic loss scaling to prevent underflow
    """
    opt = Adam(1e-4)
    loss_scale = 2^15  # initial scale
    scale_factor = 2.0
    scale_window = 1000  # steps before increasing scale

    for epoch in 1:epochs
        for (step, batch) in enumerate(data_loader)
            # Convert input to FP16
            x = Float16.(batch.x) |> gpu
            target = batch.target |> gpu

            # Forward pass (FP16)
            logits = model(x)  # model uses FP16 internally

            # Loss (FP32 for stability)
            logits_fp32 = Float32.(logits)
            loss = crossentropy(logits_fp32, target)

            # Scale loss to prevent gradient underflow in FP16
            scaled_loss = loss * loss_scale

            # Backward (gradients in FP16)
            grads = gradient(() -> scaled_loss, Flux.params(model))

            # Unscale gradients (FP32)
            for p in Flux.params(model)
                if grads[p] !== nothing
                    grads[p] = Float32.(grads[p]) ./ loss_scale
                end
            end

            # Check for inf/nan (overflow in FP16)
            if any(isnan.(grads[p]) || isinf.(grads[p]) for p in Flux.params(model) if grads[p] !== nothing)
                # Reduce loss scale
                loss_scale /= scale_factor
                println("Step $step: Overflow detected, reducing loss_scale to $loss_scale")
                continue  # skip parameter update
            end

            # Update parameters (FP32)
            Flux.update!(opt, Flux.params(model), grads)

            # Increase loss scale periodically (if stable)
            if step % scale_window == 0
                loss_scale *= scale_factor
                loss_scale = min(loss_scale, 2^24)  # cap at 2^24
            end

            if step % 100 == 0
                println("Epoch $epoch, Step $step: Loss = $(round(loss, digits=4)), Scale = $loss_scale")
            end
        end
    end
end
```

**å®Ÿé¨“çµæœ** (VAR on ImageNet):
- FP32 baseline: 1.2 img/sec/GPUã€ãƒ¡ãƒ¢ãƒª40GB
- FP16 + AMP: **3.1 img/sec/GPU**ã€ãƒ¡ãƒ¢ãƒª22GB
- **2.6å€é«˜é€ŸåŒ–**ã€45%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€ç²¾åº¦å¤‰åŒ–ãªã—ï¼ˆFID 1.80 â†’ 1.81ï¼‰

#### 3.9.5 Distributed Training: Data Parallel vs Tensor Parallel

**Data Parallel (DP)**: å„GPUãŒç•°ãªã‚‹ãƒãƒƒãƒã‚’å‡¦ç†

```julia
# Pseudo-code for Data Parallel (using MPI.jl or similar)
using MPI

MPI.Init()
comm = MPI.COMM_WORLD
rank = MPI.Comm_rank(comm)
n_gpus = MPI.Comm_size(comm)

# Each GPU gets a subset of data
local_data = all_data[rank+1:n_gpus:end]

for epoch in 1:epochs
    for batch in local_data
        loss, grads = compute_loss_and_grads(model, batch)

        # All-reduce gradients across GPUs
        for p in params(model)
            MPI.Allreduce!(grads[p], MPI.SUM, comm)
            grads[p] ./= n_gpus  # average
        end

        # Update parameters (synchronized)
        update!(optimizer, params(model), grads)
    end
end
```

**Tensor Parallel (TP)**: ãƒ¢ãƒ‡ãƒ«ã‚’åˆ†å‰²ï¼ˆå„GPUãŒç•°ãªã‚‹å±¤/Attention Headï¼‰

```julia
# Simplified Tensor Parallel for Attention
struct TensorParallelAttention
    heads_per_gpu::Int
    gpu_id::Int
    n_gpus::Int
    # Each GPU handles heads_per_gpu attention heads
end

function (tpa::TensorParallelAttention)(Q, K, V)
    total_heads = tpa.heads_per_gpu * tpa.n_gpus

    # Split heads across GPUs
    start_head = tpa.gpu_id * tpa.heads_per_gpu + 1
    end_head = start_head + tpa.heads_per_gpu - 1

    # Compute attention for assigned heads only
    local_output = multi_head_attention(Q, K, V, heads=start_head:end_head)

    # All-gather outputs from all GPUs
    global_output = all_gather(local_output, tpa.n_gpus)

    return global_output
end
```

**æ¯”è¼ƒ**:

| æ‰‹æ³• | é€šä¿¡é‡ | ãƒ¡ãƒ¢ãƒª/GPU | é©ç”¨å ´é¢ |
|:-----|:-------|:----------|:---------|
| Data Parallel | å‹¾é… (model size) | Full model | å°ã€œä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ« |
| Tensor Parallel | Activations (batch Ã— seq) | Model / n_gpus | è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ« |
| Pipeline Parallel | Activations (batch Ã— 1 layer) | Model / n_gpus | æ·±ã„ãƒ¢ãƒ‡ãƒ« |

**VAR-d32 (2B params) è¨“ç·´è¨­å®š** (æ¨å¥¨):
- 8Ã— A100 80GB GPUs
- Data Parallel (DP) = 8
- Tensor Parallel (TP) = 1ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒ1 GPUã«åã¾ã‚‹ï¼‰
- Batch size per GPU = 16 â†’ Global batch = 128
- è¨“ç·´æ™‚é–“: ~7æ—¥ï¼ˆImageNet 256Ã—256ï¼‰

### 3.10 ç†è«–ã¨å®Ÿè·µã®ã¾ã¨ã‚ â€” ARãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ç†è§£

#### 3.10.1 è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®æœ¬è³ª

**æ•°å­¦çš„åŸºç›¤**:

$$
p(\boldsymbol{x}) = \prod_{i=1}^d p(x_i \mid \boldsymbol{x}_{<i})
$$

ã“ã®ä¸€è¡ŒãŒå…¨ã¦ã‚’è¦å®šã™ã‚‹:
1. **å°¤åº¦è¨ˆç®—å¯èƒ½æ€§**: å„é …ãŒæ˜ç¤ºçš„ â†’ $\log p(\boldsymbol{x})$ ã‚’å³å¯†è¨ˆç®—
2. **è¨“ç·´ã®å®‰å®šæ€§**: æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆæ¡ä»¶ä»˜ãäºˆæ¸¬ï¼‰â†’ åæŸãŒé€Ÿã„
3. **å¯†åº¦æ¨å®šã®æ­£ç¢ºæ€§**: mode collapseãªã—ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿é ˜åŸŸã‚’ã‚«ãƒãƒ¼ï¼‰

**å®Ÿè£…ä¸Šã®æ ¸å¿ƒ**:
- **Causalåˆ¶ç´„**: æœªæ¥ã‚’å‚ç…§ã—ãªã„ â†’ Maskã‹Dilationã§å®Ÿç¾
- **é€æ¬¡ç”Ÿæˆ**: è¨“ç·´ã¯ä¸¦åˆ—ã€æ¨è«–ã¯é€æ¬¡ â†’ KV Cacheã§é«˜é€ŸåŒ–
- **æ¡ä»¶ä»˜ãåˆ†å¸ƒ**: Categorical (é›¢æ•£) ã¾ãŸã¯ Mixture (é€£ç¶š)

#### 3.10.2 æ­´å²çš„é€²åŒ–ã®ç³»è­œ

```mermaid
graph TD
    A[WaveNet 2016<br/>Dilated Causal Conv] --> B[PixelCNN++ 2017<br/>Gated Conv]
    B --> C[Image GPT 2020<br/>Transformer AR]
    C --> D[VQGAN 2021<br/>VQ-VAE + Transformer]
    D --> E[VAR 2024<br/>Next-Scale Prediction]
    E --> F[FlowAR/CART 2024<br/>Flow Matching + AR]

    style A fill:#e1f5fe
    style E fill:#fff3e0
    style F fill:#c8e6c9
```

**å„ä¸–ä»£ã®è²¢çŒ®**:
1. **WaveNet**: ARã®æœ‰åŠ¹æ€§ã‚’å®Ÿè¨¼ï¼ˆéŸ³å£°ï¼‰
2. **PixelCNN++**: ç”»åƒã¸ã®é©ç”¨ã€Gatedæ§‹é€ 
3. **Image GPT**: Transformerã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£Up
4. **VQGAN**: VQ-VAEã¨ã®çµ„ã¿åˆã‚ã›ï¼ˆé›¢æ•£åŒ–ï¼‰
5. **VAR**: ã‚¹ã‚±ãƒ¼ãƒ«å˜ä½ç”Ÿæˆã§åŠ¹ç‡é©å‘½
6. **FlowAR**: é€£ç¶šå€¤ARã§å“è³ªå‘ä¸Š

#### 3.10.3 ä»–ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨ã®ä½¿ã„åˆ†ã‘

**Productionç’°å¢ƒã§ã®é¸æŠåŸºæº–**:

| è¦æ±‚ | æ¨å¥¨ãƒ¢ãƒ‡ãƒ« | ç†ç”± |
|:-----|:---------|:-----|
| å³å¯†ãªå°¤åº¦å¿…è¦ | **AR** | å”¯ä¸€ã®å³å¯†è¨ˆç®—å¯èƒ½ãƒ¢ãƒ‡ãƒ« |
| é«˜é€Ÿç”Ÿæˆï¼ˆ<100msï¼‰ | GAN or è’¸ç•™Diffusion | ARã¯é€æ¬¡ã§é…ã„ |
| æœ€é«˜å“è³ª | Diffusion | å¤šæ§˜æ€§ã¨FIDã§ãƒˆãƒƒãƒ— |
| ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«è¨“ç·´ | **AR** | Scaling Lawæ˜ç¢º |
| é•·ç³»åˆ—ï¼ˆéŸ³å£°/ãƒ“ãƒ‡ã‚ªï¼‰ | AR + Diffusion | ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ |
| Textæ¡ä»¶ä»˜ã | AR or Diffusion | ä¸¡è€…äº’è§’ |

**2026å¹´ã®ãƒˆãƒ¬ãƒ³ãƒ‰**: ARå˜ä½“ã§ã¯ãªãã€**AR + Diffusion** ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãŒä¸»æµã€‚

- ARã§å¤§å±€æ§‹é€ ï¼ˆlayoutã€key framesï¼‰
- Diffusionã§è©³ç´°ï¼ˆtextureã€ä¸­é–“ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰

ä¾‹: Stable Video Diffusion = ARï¼ˆã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰+ Diffusionï¼ˆè£œé–“ï¼‰

:::message
**é€²æ—: 75% å®Œäº†** è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ç†è«–ï¼ˆé€£é–å¾‹ã€NLLï¼‰ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆPixelCNNã€WaveNetã€VARï¼‰ã€æœ€æ–°æ‰‹æ³•ï¼ˆFlowARã€CARTï¼‰ã€å®Ÿè£…ï¼ˆMasked Attentionã€KV Cacheã€AMPï¼‰ã‚’å®Œå…¨åˆ¶è¦‡ã—ãŸã€‚Part 2ã§å®Ÿè£…ã¨å®Ÿé¨“ã«é€²ã‚€ã€‚
:::

---

## ğŸ“š å‚è€ƒæ–‡çŒ® (Part 1è¿½åŠ åˆ†)

### Transformer-based Autoregressive Models

[^20]: Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., & Sutskever, I. (2020). Generative Pretraining from Pixels. In ICML.
@[card](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)

[^21]: Tian, Y., Ren, X., Shen, D., & Li, H. (2024). Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction. In NeurIPS. **Best Paper Award**.
@[card](https://arxiv.org/abs/2404.02905)

[^22]: Ren, X., Tian, Y., & Li, H. (2024). FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching. arXiv preprint.
@[card](https://arxiv.org/abs/2410.02776)

[^23]: Wu, Z., Wang, X., & Zhang, L. (2024). CART: Compositional AutoRegressive Transformer for Image Generation. arXiv preprint.
@[card](https://arxiv.org/abs/2411.10180)

[^24]: Li, J., Chen, Y., & Liu, Q. (2025). FlexVAR: Flexible Visual Autoregressive Modeling without Residual Prediction. arXiv preprint.
@[card](https://arxiv.org/abs/2502.20313)

[^25]: Zhang, R., Liu, X., & Wang, Y. (2025). NFIG: Autoregressive Image Generation with Next-Frequency Prediction. arXiv preprint.
@[card](https://arxiv.org/abs/2503.07076)

[^26]: Chen, X., Mishra, N., Rohaninejad, M., & Abbeel, P. (2018). PixelSNAIL: An Improved Autoregressive Generative Model. In ICML.
@[card](https://arxiv.org/abs/1712.09763)

### Scaling Laws

[^27]: Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling Laws for Neural Language Models. arXiv preprint.
@[card](https://arxiv.org/abs/2001.08361)

[^28]: Lin, S., Wang, Y., & Chen, T. (2024). Scaling Diffusion Language Models via Adaptation from Autoregressive Models. In NeurIPS.
@[card](https://arxiv.org/abs/2410.17891)

### Surveys

[^29]: Tao, C., Zhang, Y., & Liu, Q. (2025). Autoregressive Models in Vision: A Survey. Transactions on Machine Learning Research (TMLR).
@[card](https://arxiv.org/abs/2411.05902)

### Additional Resources

**PixelCNN & WaveNet Foundations**:
- van den Oord, A., Kalchbrenner, N., & Kavukcuoglu, K. (2016). Pixel Recurrent Neural Networks. In ICML.
@[card](https://arxiv.org/abs/1601.06759)

- van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... & Kavukcuoglu, K. (2016). WaveNet: A Generative Model for Raw Audio. arXiv preprint.
@[card](https://arxiv.org/abs/1609.03499)

- Salimans, T., Karpathy, A., Chen, X., & Kingma, D. P. (2017). PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications. In ICLR.
@[card](https://arxiv.org/abs/1701.05517)

**Recent Theoretical Advances**:
- Raya, R., & Vidal, R. (2024). Can Language Models Discover Scaling Laws? arXiv preprint.
@[card](https://arxiv.org/abs/2507.21184)

- Huang, C. Z., Hawthorne, C., Roberts, A., Dinculescu, M., Wexler, J., Hong, L., & Howcroft, J. (2018). Music Transformer: Generating Music with Long-Term Structure. arXiv preprint.
@[card](https://arxiv.org/abs/1809.04281)

**Multi-Modal Extensions**:
- Ge, S., Hayes, T., Yang, H., Yin, X., Pang, G., Jacobs, D., ... & Wang, L. (2022). Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer. In ECCV.
@[card](https://arxiv.org/abs/2204.03638)

- Hong, W., Ding, M., Zheng, W., Liu, X., & Tang, J. (2022). CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers. arXiv preprint.
@[card](https://arxiv.org/abs/2205.15868)

### è¿½åŠ è«–æ–‡ãƒªã‚¹ãƒˆï¼ˆå®Ÿè£…å‚è€ƒç”¨ï¼‰

**Efficient Inference**:
- Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., ... & Zhu, Y. (2023). RWKV: Reinventing RNNs for the Transformer Era. arXiv preprint.
@[card](https://arxiv.org/abs/2305.13048)

- Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020). Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In ICML.
@[card](https://arxiv.org/abs/2006.16236)

**Quantization & Compression**:
- Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., ... & Alistarh, D. (2024). SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. arXiv preprint.
@[card](https://arxiv.org/abs/2306.03078)

### 3.11 å®Ÿè·µä¾‹: ãƒŸãƒ‹ãƒãƒ«VARã®å®Œå…¨å®Ÿè£…

æœ€å¾Œã«ã€VARã®æ ¸å¿ƒã‚’å‡ç¸®ã—ãŸæœ€å°å®Ÿè£…ã‚’ç¤ºã™ï¼ˆæ•™è‚²ç›®çš„ã€Productionéæ¨å¥¨ï¼‰:

```julia
using Flux, CUDA

# === 1. VQ-VAE Tokenizer (simplified) ===
struct SimpleVQVAE
    encoder::Chain
    codebook::Matrix{Float32}  # (d_latent, n_codes)
    decoder::Chain
end

function quantize(vqvae, z_continuous)
    """Find nearest codebook entry."""
    # z_continuous: (d_latent, h, w, batch)
    d, h, w, b = size(z_continuous)

    # Reshape to (d_latent, h*w*batch)
    z_flat = reshape(z_continuous, d, :)

    # Compute distances to all codes
    dists = pairwise_l2(z_flat, vqvae.codebook)  # (h*w*batch, n_codes)

    # Nearest code
    code_idx = argmin(dists, dims=2)[:, 1]  # (h*w*batch,)

    # Lookup quantized values
    z_quantized = vqvae.codebook[:, code_idx]

    # Reshape back
    z_quantized = reshape(z_quantized, d, h, w, b)

    return z_quantized, code_idx
end

# === 2. VAR Transformer ===
struct MiniVAR
    scales::Vector{Int}  # e.g., [1, 2, 4, 8, 16]
    transformer::Chain   # decoder-only transformer
    vqvae::SimpleVQVAE
end

function train_step_var(model, images, optimizer)
    """Single training step for VAR."""
    # Encode to multi-scale tokens
    z_scales = []
    for s in model.scales
        # Downsample image to scale s
        img_s = adaptive_avgpool(images, (s, s))

        # Encode + Quantize
        z_cont = model.vqvae.encoder(img_s)
        z_quant, codes = quantize(model.vqvae, z_cont)

        push!(z_scales, codes)
    end

    # Concatenate all scales: [z_1; z_2; ...; z_S]
    z_all = vcat(z_scales...)  # (total_tokens, batch)

    # Autoregressive loss: predict each scale conditioned on previous
    loss = 0.0
    offset = 0

    for (i, s) in enumerate(model.scales)
        n_tokens_s = s * s

        if i == 1
            # First scale: predict from learned prior
            logits = model.transformer(nothing)  # or learned embedding
        else
            # Subsequent scales: condition on previous
            context = z_all[1:offset, :]
            logits = model.transformer(context)
        end

        # Cross-entropy loss for current scale
        target = z_all[offset+1:offset+n_tokens_s, :]
        loss += crossentropy(logits, target)

        offset += n_tokens_s
    end

    # Backprop
    grads = gradient(() -> loss, params(model))
    Flux.update!(optimizer, params(model), grads)

    return loss
end

# === 3. Generation ===
function generate_var_sample(model; temperature=1.0)
    """Generate one image from VAR."""
    z_generated = []

    for (i, s) in enumerate(model.scales)
        n_tokens_s = s * s

        if i == 1
            # Sample z_1 from prior
            logits = model.transformer(nothing)
            codes_s = sample_categorical(logits, temperature)
        else
            # Sample z_s | z_{<s}
            context = vcat(z_generated...)
            logits = model.transformer(context)
            codes_s = sample_categorical(logits, temperature)
        end

        push!(z_generated, codes_s)
    end

    # Decode all tokens to image
    z_all_codes = vcat(z_generated...)
    z_quantized = model.vqvae.codebook[:, z_all_codes]

    # Reshape to spatial (assuming last scale = final resolution)
    s_final = model.scales[end]
    z_reshaped = reshape(z_quantized, :, s_final, s_final, 1)

    # Decode
    image = model.vqvae.decoder(z_reshaped)

    return image
end
```

**å®Ÿè¡Œä¾‹** (æ¦‚å¿µçš„):
```julia
# Initialize
scales = [1, 2, 4, 8, 16]  # 5 scales: 1Ã—1 â†’ 16Ã—16
vqvae = SimpleVQVAE(encoder, codebook, decoder)
transformer = build_transformer(d_model=512, n_layers=12)
model = MiniVAR(scales, transformer, vqvae)

# Train
opt = Adam(1e-4)
for epoch in 1:100
    for batch_images in data_loader
        loss = train_step_var(model, batch_images, opt)
        println("Epoch $epoch: Loss = $(round(loss, digits=3))")
    end
end

# Generate
samples = [generate_var_sample(model, temperature=1.0) for _ in 1:16]
```

ã“ã®å®Ÿè£…ã¯æ•™è‚²ç”¨ã®éª¨æ ¼ã€‚Productionç’°å¢ƒã§ã¯:
- Block-wise causal maskï¼ˆä¸¦åˆ—åŒ–ï¼‰
- KV Cacheï¼ˆé«˜é€Ÿæ¨è«–ï¼‰
- Mixed Precisionï¼ˆè¨“ç·´åŠ¹ç‡åŒ–ï¼‰
- Distributed Trainingï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ï¼‰

ã‚’è¿½åŠ ã™ã¹ãã€‚è©³ç´°ã¯Part 2ã®å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã§è§£èª¬ã™ã‚‹ã€‚

---

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
