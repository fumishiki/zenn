---
title: "ç¬¬49å›: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆ & æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼""
emoji: "ğŸŒ"
type: "tech"
topics: ["machinelearning", "deeplearning", "multimodal", "julia", "inference"]
published: true
---

# ç¬¬49å›: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆ & æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° â€” å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆã¨æ¨è«–æ™‚è¨ˆç®—ã®é©å‘½

> **ç”»åƒãƒ»éŸ³å£°ãƒ»å‹•ç”»ãƒ»3Dãƒ»ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ç§‘å­¦ â€” å…¨ã¦ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§ã€‚è¨“ç·´æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‹ã‚‰æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¸ã€‚2025-2026å¹´ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆãŒã€ã“ã“ã«å®Œæˆã™ã‚‹ã€‚**

ç¬¬43-48å›ã§å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£(ç”»åƒãƒ»éŸ³å£°ãƒ»å‹•ç”»ãƒ»3Dãƒ»ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ç§‘å­¦)ã‚’å€‹åˆ¥ã«ç¿’å¾—ã—ãŸã€‚DiT/FLUXã€F5-TTS/Flow Matchingã€Sora 2/CogVideoXã€NeRF/3DGSã€MotionGPT-3/4DGSã€RFdiffusion3/MatterGen â€” ãã‚Œãã‚Œã®åˆ†é‡ã§æœ€å…ˆç«¯ã‚’å­¦ã‚“ã ã€‚

ã—ã‹ã—ã€å€‹åˆ¥ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã«ã¯é™ç•ŒãŒã‚ã‚‹ã€‚**ã€Œç”»åƒã‚’ç†è§£ã—ã¦éŸ³å£°ã§èª¬æ˜ã€ã€Œãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ã‚’ç”Ÿæˆã—ã€3Dã‚·ãƒ¼ãƒ³ã¨ã—ã¦å±•é–‹ã€** â€” ã“ã®ã‚ˆã†ãªè¤‡é›‘ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ã‚¹ã‚¯ã«ã¯ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±åˆã—ãŸçµ±ä¸€ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€2025-2026å¹´ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’2ã¤ã®è»¸ã§æ•´ç†ã™ã‚‹:

**è»¸1: Unified Multimodal Models** â€” ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ– â†’ çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã¸ã€‚Show-o/Show-o2ã€BAGELã€GPT-4oã€NExT-GPTãŒåˆ‡ã‚Šé–‹ãã€å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆã®ä¸–ç•Œã€‚ãã—ã¦ã€çµ±åˆã®ä»£å„Ÿã¨ã—ã¦ã®**Modal Aphasiaå•é¡Œ**ã€‚

**è»¸2: Inference-Time Scaling** â€” Training scaling laws â†’ Test-time scaling lawsã¸ã€‚Reflect-DiTã€Test-time TrainingãŒç¤ºã™ã€æ¨è«–æ™‚è¨ˆç®—ã®é©å‘½ã€‚è¨“ç·´å¾Œã§ã‚‚ã€æ¨è«–æ™‚ã«è¨ˆç®—ã‚’æŠ•å…¥ã™ã‚Œã°å“è³ªãŒå‘ä¸Šã™ã‚‹ â€” ã“ã‚ŒãŒæ¬¡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã ã€‚

ã•ã‚‰ã«ã€**Generative World Models**(Genie 3ã€Runway GWM-1ã€LingBot-World)ãŒã€çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’çµ„ã¿åˆã‚ã›ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªä¸–ç•Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¸ã¨é€²åŒ–ã™ã‚‹ã€‚

Course V æœ€çµ‚è¬›ç¾©ã®å‰ã«ã€2025-2026å¹´ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’å®Œå…¨ç†è§£ã™ã‚‹ã€‚æ¬¡ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’äºˆæ¸¬ã™ã‚‹åŠ›ã‚’ã€ã“ã“ã§æ‰‹ã«å…¥ã‚Œã‚ˆã†ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚æœ¬è¬›ç¾©ã¯ **Course V ç¬¬7å›** â€” å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç¿’å¾—å®Œäº†å¾Œã®çµ±åˆç·¨ã ã€‚
:::

```mermaid
graph TD
    A["ç¬¬43å› DiT"] --> I["ç¬¬49å›<br/>çµ±åˆ+æ¨è«–æ™‚<br/>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"]
    B["ç¬¬44å› éŸ³å£°"] --> I
    C["ç¬¬45å› å‹•ç”»"] --> I
    D["ç¬¬46å› 3D"] --> I
    E["ç¬¬47å› ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³"] --> I
    F["ç¬¬48å› ç§‘å­¦"] --> I
    I --> J["ç¬¬50å›<br/>ç·æ‹¬+å’æ¥­åˆ¶ä½œ"]
    style I fill:#ffd700,stroke:#ff6347,stroke-width:4px
    style J fill:#98fb98
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’1ãƒ¢ãƒ‡ãƒ«ã§

**ã‚´ãƒ¼ãƒ«**: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒã€ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒãƒ»éŸ³å£°ã‚’åŒæ™‚ã«æ‰±ã†æ§˜å­ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

å¾“æ¥ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«(CLIP=ç”»åƒç†è§£ã€DALL-E=ç”»åƒç”Ÿæˆã€Whisper=éŸ³å£°èªè­˜)ã¯ã€ãã‚Œãã‚Œç‹¬ç«‹ã—ã¦ã„ãŸã€‚**çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«**ã¯ã€1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç†è§£ãƒ»ç”Ÿæˆã™ã‚‹ã€‚

```julia
using Random, Statistics

# Unified Multimodal Model ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
# å…¥åŠ›: text/image/audio ã®ã„ãšã‚Œã‹ â†’ å‡ºåŠ›: text/image/audio ã®ã„ãšã‚Œã‹

# å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å˜ç´”ãªãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾
struct MultimodalInput
    modality::Symbol  # :text, :image, :audio
    data::Vector{Float64}
end

# çµ±åˆãƒ¢ãƒ‡ãƒ«: å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å…±é€šæ½œåœ¨ç©ºé–“ã¸ãƒãƒƒãƒ”ãƒ³ã‚°
function unified_encoder(input::MultimodalInput, shared_dim=128)
    # ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ â†’ å…±é€šæ½œåœ¨ç©ºé–“
    if input.modality == :text
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (å˜èªåŸ‹ã‚è¾¼ã¿ â†’ Transformer)
        return randn(shared_dim) .+ mean(input.data)
    elseif input.modality == :image
        # ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (ViT â†’ æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«)
        return randn(shared_dim) .+ std(input.data)
    elseif input.modality == :audio
        # éŸ³å£°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (Spectrogram â†’ Audio Transformer)
        return randn(shared_dim) .+ sum(input.data) / length(input.data)
    else
        error("Unknown modality: $(input.modality)")
    end
end

# å…±é€šæ½œåœ¨ç©ºé–“ã‹ã‚‰å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¸ãƒ‡ã‚³ãƒ¼ãƒ‰
function unified_decoder(latent::Vector{Float64}, target_modality::Symbol)
    if target_modality == :text
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ€ (æ½œåœ¨ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åˆ—)
        return "Generated text: " * string(round(mean(latent), digits=3))
    elseif target_modality == :image
        # ç”»åƒãƒ‡ã‚³ãƒ¼ãƒ€ (æ½œåœ¨ â†’ ç”»åƒãƒ‘ãƒƒãƒ)
        return "Generated image with mean: " * string(round(mean(latent), digits=3))
    elseif target_modality == :audio
        # éŸ³å£°ãƒ‡ã‚³ãƒ¼ãƒ€ (æ½œåœ¨ â†’ Waveform)
        return "Generated audio with RMS: " * string(round(std(latent), digits=3))
    else
        error("Unknown modality: $(target_modality)")
    end
end

# Any-to-Any å¤‰æ›ã®å®Ÿæ¼”
input_text = MultimodalInput(:text, randn(512))
input_image = MultimodalInput(:image, randn(256, 256) |> vec)
input_audio = MultimodalInput(:audio, randn(16000))

println("=== Unified Multimodal Model: Any-to-Any ===")
println()

# Text â†’ Image
latent_text = unified_encoder(input_text)
output_image = unified_decoder(latent_text, :image)
println("Text â†’ Image: ", output_image)

# Image â†’ Audio
latent_image = unified_encoder(input_image)
output_audio = unified_decoder(latent_image, :audio)
println("Image â†’ Audio: ", output_audio)

# Audio â†’ Text
latent_audio = unified_encoder(input_audio)
output_text = unified_decoder(latent_audio, :text)
println("Audio â†’ Text: ", output_text)

println()
println("å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãŒå…±é€šæ½œåœ¨ç©ºé–“ã§çµ±åˆã•ã‚Œã‚‹ â€” ã“ã‚ŒãŒ Unified Multimodal Models")
```

å‡ºåŠ›:
```
=== Unified Multimodal Model: Any-to-Any ===

Text â†’ Image: Generated image with mean: 0.234
Image â†’ Audio: Generated audio with RMS: 1.012
Audio â†’ Text: Generated text: -0.156

å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãŒå…±é€šæ½œåœ¨ç©ºé–“ã§çµ±åˆã•ã‚Œã‚‹ â€” ã“ã‚ŒãŒ Unified Multimodal Models
```

**30ç§’ã§ Any-to-Any ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤‰æ›ã‚’ä½“é¨“ã—ãŸã€‚** ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒã€ç”»åƒâ†’éŸ³å£°ã€éŸ³å£°â†’ãƒ†ã‚­ã‚¹ãƒˆ â€” å…¨ã¦ã®çµ„ã¿åˆã‚ã›ãŒ1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§å®Ÿè¡Œã•ã‚Œã‚‹ã€‚ã“ã‚ŒãŒ**çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«**ã®æœ¬è³ªã ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®3%å®Œäº†ï¼** Zone 0 ã¯ã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—ã€‚æ¬¡ã¯æœ€æ–°ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿéš›ã«è§¦ã£ã¦ã€çµ±åˆã®ãƒ¡ãƒªãƒƒãƒˆã¨èª²é¡Œã‚’ä½“æ„Ÿã™ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Unified Multimodal Models ã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**ã‚´ãƒ¼ãƒ«**: Show-oã€BAGELã€NExT-GPTã®3ã¤ã®çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å®Ÿè£…ã—ã€è¨­è¨ˆæ€æƒ³ã®é•ã„ã‚’ä½“æ„Ÿã™ã‚‹ã€‚

### 1.1 Show-o: Autoregressive + Diffusion ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰çµ±åˆ

Show-o[^1]ã¯**ICLR 2025**ã§ç™ºè¡¨ã•ã‚ŒãŸçµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã ã€‚ç‰¹å¾´ã¯ã€**ãƒ†ã‚­ã‚¹ãƒˆã¯è‡ªå·±å›å¸°(Causal Attention)ã€ç”»åƒã¯æ‹¡æ•£(Full Attention)**ã¨ã„ã†ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚

[^1]: Wu et al. (2023). "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation". ICLR 2025. arXiv:2408.12528

```julia
# Show-o ã®ã‚³ã‚¢è¨­è¨ˆ: ãƒ†ã‚­ã‚¹ãƒˆ=ARã€ç”»åƒ=Diffusion ã®çµ±åˆ

struct ShowOModel
    text_vocab::Int      # ãƒ†ã‚­ã‚¹ãƒˆèªå½™ã‚µã‚¤ã‚º
    image_codebook::Int  # ç”»åƒã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚µã‚¤ã‚º (VQ-VAE)
    hidden_dim::Int
    n_heads::Int
end

# ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®è‡ªå·±å›å¸°ç”Ÿæˆ (Causal Attention)
function text_autoregressive_forward(model::ShowOModel, text_tokens, past_kv=nothing)
    # Causal mask: æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„
    seq_len = length(text_tokens)
    causal_mask = tril(ones(seq_len, seq_len))  # ä¸‹ä¸‰è§’è¡Œåˆ—

    # Transformer with causal attention
    # Q, K, V = Linear(text_embed)
    # Attention = softmax(QK^T / âˆšd_k) * V (with causal_mask)
    logits = randn(seq_len, model.text_vocab)  # ç°¡ç•¥åŒ–

    return logits, nothing  # logits ã¨æ›´æ–°ã•ã‚ŒãŸKVã‚­ãƒ£ãƒƒã‚·ãƒ¥
end

# ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒªãƒ³ã‚° (Full Attention)
function image_diffusion_forward(model::ShowOModel, image_tokens, t)
    # Full attention: å…¨ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ç›¸äº’ä½œç”¨ã‚’è¨±å¯
    num_tokens = length(image_tokens)
    full_mask = ones(num_tokens, num_tokens)  # å…¨çµåˆ

    # Diffusion denoising step
    # xt = âˆšá¾±tÂ·x0 + âˆš(1-á¾±t)Â·Îµ
    # äºˆæ¸¬: Îµ_Î¸(xt, t)
    alpha_bar_t = 1 - t / 1000  # ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    predicted_noise = randn(size(image_tokens))  # ç°¡ç•¥åŒ–

    return predicted_noise
end

# çµ±åˆæ¨è«–: Text prompt â†’ Image generation
function show_o_generate(model::ShowOModel, text_prompt, num_diffusion_steps=20)
    # 1. ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªå·±å›å¸°ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    text_tokens = [rand(1:model.text_vocab) for _ in 1:10]  # ãƒ€ãƒŸãƒ¼ãƒˆãƒ¼ã‚¯ãƒ³
    text_logits, _ = text_autoregressive_forward(model, text_tokens)

    # 2. ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’æ¡ä»¶ã¨ã—ã¦ç”»åƒã‚’æ‹¡æ•£ç”Ÿæˆ
    image_tokens = randn(256)  # 16Ã—16 ãƒ‘ãƒƒãƒ

    for step in num_diffusion_steps:-1:1
        t = step / num_diffusion_steps
        noise_pred = image_diffusion_forward(model, image_tokens, t)
        # Denoising update (DDPMå¼)
        image_tokens = image_tokens - 0.1 * noise_pred  # ç°¡ç•¥åŒ–
    end

    return image_tokens
end

# å®Ÿè¡Œ
model = ShowOModel(50000, 8192, 768, 12)
generated_image = show_o_generate(model, "A cat on a mat")
println("Show-o: Text â†’ Image generation completed")
println("  Generated image tokens: ", size(generated_image))
println("  Key insight: ãƒ†ã‚­ã‚¹ãƒˆ=ARã€ç”»åƒ=Diffusion ã®çµ±åˆ")
```

**Show-oã®è¨­è¨ˆå“²å­¦**: ãƒ†ã‚­ã‚¹ãƒˆã¯**å› æœçš„**(éå»â†’æœªæ¥ã®é †åº)ã ãŒã€ç”»åƒã¯**åŒæ–¹å‘çš„**(å…¨ãƒ‘ãƒƒãƒé–“ã®ç›¸äº’ä½œç”¨)ã€‚ç•°ãªã‚‹æ€§è³ªã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ã€ç•°ãªã‚‹Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’é©ç”¨ã™ã‚‹ã€‚

### 1.2 BAGEL: äº‹å‰å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®çµ±åˆ

BAGEL[^2]ã¯**ByteDance**ãŒ2025å¹´ã«ç™ºè¡¨ã—ãŸã€**æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³ã®äº‹å‰å­¦ç¿’**ã§çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èƒ½åŠ›ã‚’ç²å¾—ã—ãŸãƒ¢ãƒ‡ãƒ«ã ã€‚

[^2]: ByteDance (2025). "Emerging Properties in Unified Multimodal Pretraining". arXiv:2505.14683

```julia
# BAGEL: Large-scale pretraining ã«ã‚ˆã‚‹çµ±åˆ

struct BAGELModel
    decoder_only::Bool  # True: decoder-only Transformer
    active_params::Int  # 7B active (14B total with MoE)
    pretraining_tokens::Int  # æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³
end

# çµ±åˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–: Text/Image/Video/Audio ã‚’å…¨ã¦é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«
function unified_tokenization(data, modality::Symbol)
    if modality == :text
        # BPE/SentencePiece tokenizer
        return [rand(1:50000) for _ in 1:100]
    elseif modality == :image
        # VQ-VAE tokenizer (256Ã—256 â†’ 16Ã—16 = 256 tokens)
        return [rand(1:8192) for _ in 1:256]
    elseif modality == :video
        # Video tokenizer (16 frames Ã— 16Ã—16 = 4096 tokens)
        return [rand(1:8192) for _ in 1:4096]
    elseif modality == :audio
        # Audio codec (EnCodec/WavTokenizer)
        return [rand(1:2048) for _ in 1:512]
    end
end

# Decoder-only Transformerã§å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±ä¸€å‡¦ç†
function bagel_forward(model::BAGELModel, tokens, modality_ids)
    # modality_ids: å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚¿ã‚¤ãƒ— (1=text, 2=image, 3=video, 4=audio)
    seq_len = length(tokens)

    # Modality-aware positional encoding
    pos_embed = randn(seq_len, 768)  # ä½ç½®åŸ‹ã‚è¾¼ã¿
    modality_embed = randn(seq_len, 768)  # ãƒ¢ãƒ€ãƒªãƒ†ã‚£åŸ‹ã‚è¾¼ã¿

    # Transformer layers (decoder-only, causal)
    hidden = pos_embed .+ modality_embed

    # æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ (å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±ä¸€èªå½™)
    logits = randn(seq_len, 65536)  # çµ±åˆèªå½™: text + image + video + audio

    return logits
end

# In-context learning: ç”»åƒæ“ä½œã‚¿ã‚¹ã‚¯ã‚’å°‘æ•°ä¾‹ã§å­¦ç¿’
function bagel_few_shot_image_editing(model::BAGELModel)
    # Example 1: "Rotate image 90Â°" â†’ rotated_image_tokens
    example1_text = unified_tokenization("Rotate 90 degrees", :text)
    example1_image_in = unified_tokenization(randn(256, 256), :image)
    example1_image_out = unified_tokenization(randn(256, 256), :image)  # å›è»¢å¾Œ

    # Example 2: "Make it grayscale" â†’ grayscale_image_tokens
    example2_text = unified_tokenization("Grayscale", :text)
    example2_image_in = unified_tokenization(randn(256, 256), :image)
    example2_image_out = unified_tokenization(randn(256, 256), :image)  # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«

    # Query: "Increase brightness" â†’ ?
    query_text = unified_tokenization("Increase brightness", :text)
    query_image_in = unified_tokenization(randn(256, 256), :image)

    # å…¨ã¦ã‚’é€£çµã—ã¦1ã¤ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ã—ã¦å‡¦ç†
    all_tokens = vcat(example1_text, example1_image_in, example1_image_out,
                     example2_text, example2_image_in, example2_image_out,
                     query_text, query_image_in)
    modality_ids = vcat(repeat([1], length(example1_text)),
                       repeat([2], length(example1_image_in)),
                       repeat([2], length(example1_image_out)),
                       repeat([1], length(example2_text)),
                       repeat([2], length(example2_image_in)),
                       repeat([2], length(example2_image_out)),
                       repeat([1], length(query_text)),
                       repeat([2], length(query_image_in)))

    # Forward pass: æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ = æ˜ã‚‹ãã—ãŸç”»åƒã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—
    logits = bagel_forward(model, all_tokens, modality_ids)

    # æœ€å¾Œã®256ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡º (ç”Ÿæˆã•ã‚ŒãŸç”»åƒ)
    generated_image_tokens = argmax.(eachrow(logits[end-255:end, :]))

    return generated_image_tokens
end

# å®Ÿè¡Œ
bagel_model = BAGELModel(true, 7_000_000_000, 3_000_000_000_000)
edited_image = bagel_few_shot_image_editing(bagel_model)
println("BAGEL: Few-shot image editing via in-context learning")
println("  Model: 7B active params, 3T pretraining tokens")
println("  Generated image tokens: ", length(edited_image))
println("  Key insight: äº‹å‰å­¦ç¿’ã§ emergent multimodal reasoning ç²å¾—")
```

**BAGELã®è¨­è¨ˆå“²å­¦**: å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’**é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³**ã«çµ±ä¸€ â†’ Decoder-only Transformerã§ä¸€æ‹¬å‡¦ç†ã€‚å¤§è¦æ¨¡äº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šã€**Few-shot multimodal reasoning**ãŒå‰µç™ºã™ã‚‹ã€‚

### 1.3 NExT-GPT: Any-to-Any ã®å…ˆé§†è€…

NExT-GPT[^3]ã¯2023å¹´ã«ç™ºè¡¨ã•ã‚ŒãŸ**Any-to-Any**ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å…ˆé§†çš„ç ”ç©¶ã ã€‚LLMã‚’ä¸­æ ¸ã«ã€å…¥åŠ›ãƒ»å‡ºåŠ›ç”¨ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€/ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’æ¥ç¶šã™ã‚‹ã€‚

[^3]: Wu et al. (2023). "NExT-GPT: Any-to-Any Multimodal LLM". arXiv:2309.05519

```julia
# NExT-GPT: LLMä¸­å¿ƒã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆ

struct NExTGPTModel
    llm_backbone::String  # "Vicuna-7B" ãªã©ã®LLM
    image_encoder::String  # "CLIP ViT-L/14"
    audio_encoder::String  # "ImageBind Audio"
    video_encoder::String  # "ImageBind Video"
    image_decoder::String  # "Stable Diffusion"
    audio_decoder::String  # "AudioLDM"
    video_decoder::String  # "Zeroscope"
end

# Input projection: ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ â†’ LLMåŸ‹ã‚è¾¼ã¿ç©ºé–“
function input_projection(encoder_output, target_dim=4096)
    # Linear projection: encoder_dim â†’ LLM hidden_dim
    # ä¾‹: CLIP 768-dim â†’ LLM 4096-dim
    projection_matrix = randn(target_dim, 768)
    return projection_matrix * encoder_output
end

# Output projection: LLMåŸ‹ã‚è¾¼ã¿ â†’ ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ãƒ‡ã‚³ãƒ¼ãƒ€
function output_projection(llm_hidden, decoder_input_dim=768)
    # Linear projection: LLM 4096-dim â†’ decoder 768-dim
    projection_matrix = randn(decoder_input_dim, 4096)
    return projection_matrix * llm_hidden
end

# Any-to-Any pipeline
function next_gpt_any_to_any(model::NExTGPTModel, input_modality::Symbol,
                             output_modality::Symbol, input_data)
    # Step 1: Input encoding
    if input_modality == :image
        encoder_output = randn(768)  # CLIP encoding
    elseif input_modality == :audio
        encoder_output = randn(768)  # ImageBind Audio encoding
    elseif input_modality == :text
        encoder_output = randn(768)  # Text embedding
    else
        error("Unsupported input modality")
    end

    # Step 2: Project to LLM space
    llm_input = input_projection(encoder_output)

    # Step 3: LLM reasoning (simplified)
    # å®Ÿéš›ã«ã¯: "Describe this image in audio form" ãªã©ã®æŒ‡ç¤ºã¨å…±ã«å‡¦ç†
    llm_output = llm_input .+ randn(4096) .* 0.1  # LLM forward pass

    # Step 4: Project to decoder space
    decoder_input = output_projection(llm_output)

    # Step 5: Decode to target modality
    if output_modality == :image
        output = "Generated image (via Stable Diffusion)"
    elseif output_modality == :audio
        output = "Generated audio (via AudioLDM)"
    elseif output_modality == :text
        output = "Generated text: '" * string(round(mean(decoder_input), digits=3)) * "'"
    else
        error("Unsupported output modality")
    end

    return output
end

# å®Ÿè¡Œ: ç”»åƒ â†’ éŸ³å£°
next_gpt_model = NExTGPTModel("Vicuna-7B", "CLIP", "ImageBind", "ImageBind",
                              "SD", "AudioLDM", "Zeroscope")
result = next_gpt_any_to_any(next_gpt_model, :image, :audio, randn(224, 224, 3))
println("NExT-GPT: Image â†’ Audio")
println("  Result: ", result)
println("  Key insight: LLMã‚’ä¸­æ ¸ã«ã€å…¥å‡ºåŠ›ã‚’ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã§å¤‰æ›")

# éŸ³å£° â†’ ãƒ†ã‚­ã‚¹ãƒˆ
result2 = next_gpt_any_to_any(next_gpt_model, :audio, :text, randn(16000))
println("\nNExT-GPT: Audio â†’ Text")
println("  Result: ", result2)
```

**NExT-GPTã®è¨­è¨ˆå“²å­¦**: LLMã®å¼·åŠ›ãªæ¨è«–èƒ½åŠ›ã‚’æ´»ç”¨ã€‚ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€/ãƒ‡ã‚³ãƒ¼ãƒ€ã¯æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’å†åˆ©ç”¨ â†’ ä½ã‚³ã‚¹ãƒˆçµ±åˆã€‚**1%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿è¨“ç·´**(projectionå±¤ã®ã¿)ã€‚

### 1.4 3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¯”è¼ƒ

| ãƒ¢ãƒ‡ãƒ« | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | è¨“ç·´ã‚³ã‚¹ãƒˆ | ç‰¹å¾´ |
|:-------|:-------------|:----------|:-----|
| **Show-o** | Hybrid (AR + Diffusion) | ä¸­ | ãƒ†ã‚­ã‚¹ãƒˆ=Causalã€ç”»åƒ=Full attention |
| **BAGEL** | Decoder-only unified | é«˜(æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³) | äº‹å‰å­¦ç¿’ã§ emergent reasoning |
| **NExT-GPT** | LLM + modality adapters | ä½(1%è¨“ç·´) | æ—¢å­˜ãƒ¢ãƒ‡ãƒ«å†åˆ©ç”¨ |

**3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½“é¨“ã—ãŸã€‚** çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã«ã¯è¤‡æ•°ã®è¨­è¨ˆæ€æƒ³ãŒã‚ã‚Šã€ãã‚Œãã‚Œã«ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚‹ã€‚æ¬¡ã¯ã€ãªãœçµ±åˆãŒå¿…è¦ãªã®ã‹ã€ãã—ã¦çµ±åˆã®ä»£å„Ÿã¯ä½•ã‹ã‚’ç†è§£ã™ã‚‹ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®10%å®Œäº†ï¼** Zone 1 ã§çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®3ã¤ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½“é¨“ã—ãŸã€‚æ¬¡ã¯ã€çµ±åˆã®æ„ç¾©ã¨Modal Aphasiaå•é¡Œã‚’ç›´æ„Ÿçš„ã«ç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœçµ±åˆã‹ï¼Ÿãã—ã¦ Modal Aphasia ã®ç½ 

**ã‚´ãƒ¼ãƒ«**: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ„ç¾©ã¨ã€çµ±åˆã«ä¼´ã†èª²é¡Œ(Modal Aphasia)ã‚’ç†è§£ã™ã‚‹ã€‚

### 2.1 ãªãœãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±åˆã™ã‚‹ã®ã‹ï¼Ÿ

**å¾“æ¥ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:
```
ãƒ†ã‚­ã‚¹ãƒˆ â†’ [CLIP] â†’ ç”»åƒåŸ‹ã‚è¾¼ã¿ â†’ [Stable Diffusion] â†’ ç”»åƒ
éŸ³å£° â†’ [Whisper] â†’ ãƒ†ã‚­ã‚¹ãƒˆ â†’ [ChatGPT] â†’ ãƒ†ã‚­ã‚¹ãƒˆ â†’ [TTS] â†’ éŸ³å£°
```

å•é¡Œç‚¹:
1. **ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®æƒ…å ±æå¤±**: ä¸­é–“è¡¨ç¾(ãƒ†ã‚­ã‚¹ãƒˆ)ã«å¤‰æ›ã™ã‚‹éš›ã€å…ƒã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹
2. **æ¨è«–ã‚³ã‚¹ãƒˆã®å¢—å¤§**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®é †æ¬¡å®Ÿè¡Œ â†’ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¢—åŠ 
3. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–ã®æ¬ å¦‚**: å„ãƒ¢ãƒ‡ãƒ«ã¯å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã¿ â†’ çµ±åˆçš„ãªæ¨è«–ãŒã§ããªã„

**çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒªãƒƒãƒˆ**:
1. **End-to-Endå­¦ç¿’**: å…¥åŠ›â†’å‡ºåŠ›ã‚’ç›´æ¥å­¦ç¿’ â†’ æƒ…å ±æå¤±ãªã—
2. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–**: ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»éŸ³å£°ã‚’åŒæ™‚ã«è€ƒæ…®ã—ãŸæ¨è«–
3. **åŠ¹ç‡æ€§**: 1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§å®Œçµ â†’ ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·

### 2.2 Course V å…¨ä½“ã®ä½ç½®ã¥ã‘

```mermaid
graph TD
    A["ç¬¬43å› DiT<br/>ç”»åƒç”Ÿæˆã®é©æ–°"] --> B["ç¬¬44å› éŸ³å£°<br/>Flow Matching TTS"]
    B --> C["ç¬¬45å› å‹•ç”»<br/>æ™‚é–“è»¸ã®è¿½åŠ "]
    C --> D["ç¬¬46å› 3D<br/>ç©ºé–“è¡¨ç¾"]
    D --> E["ç¬¬47å› ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³<br/>å‹•çš„3D"]
    E --> F["ç¬¬48å› ç§‘å­¦<br/>åˆ¶ç´„ä»˜ãç”Ÿæˆ"]
    F --> G["ç¬¬49å› çµ±åˆ<br/>å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆ"]
    G --> H["ç¬¬50å› ç·æ‹¬<br/>å’æ¥­åˆ¶ä½œ"]

    style G fill:#ffd700,stroke:#ff6347,stroke-width:4px
    style H fill:#98fb98
```

ç¬¬43-48å›ã§å€‹åˆ¥ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç¿’å¾— â†’ ç¬¬49å›ã§çµ±åˆ â†’ ç¬¬50å›ã§å’æ¥­åˆ¶ä½œã€‚**çµ±åˆã¯å¿œç”¨ç·¨ã®é›†å¤§æˆ**ã ã€‚

### 2.3 æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ–

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:-----------|:----------|
| **å¯¾è±¡ãƒ¢ãƒ€ãƒªãƒ†ã‚£** | ç”»åƒã®ã¿ | ç”»åƒãƒ»éŸ³å£°ãƒ»å‹•ç”»ãƒ»3Dãƒ»ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ç§‘å­¦ |
| **çµ±åˆãƒ¢ãƒ‡ãƒ«** | ãªã— | Show-o/BAGEL/NExT-GPTè©³è§£ |
| **æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°** | ãªã— | Reflect-DiT/Test-time Training |
| **World Models** | ç†è«–ã®ã¿ | Genie 3/Runway GWM-1å®Ÿè£… |
| **å®Ÿè£…è¨€èª** | Python | Julia + Rust + Elixir |

### 2.4 Modal Aphasia: çµ±åˆã®ä»£å„Ÿ

**Modal Aphasia**[^4]ã¯ã€çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒç¤ºã™é©šãã¹ãç¾è±¡ã : **è¦–è¦šçš„ã«ã¯å®Œç’§ã«è¨˜æ†¶ã—ã¦ã„ã‚‹ãŒã€è¨€èªçš„ã«ã¯èª¬æ˜ã§ããªã„**ã€‚

[^4]: Aerni et al. (2025). "Modal Aphasia: Can Unified Multimodal Models Describe Images From Memory?". arXiv:2510.21842

å®Ÿé¨“:
1. ãƒ¢ãƒ‡ãƒ«ã«æ˜ ç”»ãƒã‚¹ã‚¿ãƒ¼ã‚’è¦‹ã›ã‚‹
2. **ç”»åƒç”Ÿæˆã‚¿ã‚¹ã‚¯**: ãƒã‚¹ã‚¿ãƒ¼ã‚’å†ç¾ â†’ **ã»ã¼å®Œç’§**
3. **ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‚¿ã‚¹ã‚¯**: ãƒã‚¹ã‚¿ãƒ¼ã‚’èª¬æ˜ â†’ **é‡è¦ãªè©³ç´°ã‚’æ··åŒ**

ä¾‹: "The Godfather"ã®ãƒã‚¹ã‚¿ãƒ¼
- ç”»åƒç”Ÿæˆ: äººç‰©é…ç½®ã€è‰²èª¿ã€ãƒ•ã‚©ãƒ³ãƒˆ â€” å…¨ã¦æ­£ç¢º
- ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°: ã€Œä¸»äººå…¬ã¯éŠƒã‚’æŒã£ã¦ã„ã‚‹ã€(å®Ÿéš›ã¯æŒã£ã¦ã„ãªã„)

**ãªãœã“ã‚ŒãŒèµ·ã“ã‚‹ã®ã‹ï¼Ÿ**

ä»®èª¬1: **ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®è¡¨ç¾æ ¼å·®**
- ç”»åƒç”Ÿæˆ: é«˜æ¬¡å…ƒæ½œåœ¨ç©ºé–“(8192æ¬¡å…ƒã®VQ-VAE)ã§è©³ç´°ä¿æŒ
- ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: ä½æ¬¡å…ƒé›¢æ•£ç©ºé–“(50kèªå½™)ã§æŠ½è±¡åŒ– â†’ è©³ç´°ãŒå¤±ã‚ã‚Œã‚‹

ä»®èª¬2: **è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åã‚Š**
- ç”»åƒ-ç”»åƒãƒšã‚¢: é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ãŒè±Šå¯Œ â†’ æ­£ç¢ºãªè¦–è¦šè¨˜æ†¶
- ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¯æŠ½è±¡çš„ â†’ è©³ç´°ãªè¨€èªè¨˜æ†¶ãŒè‚²ãŸãªã„

ä»®èª¬3: **Attentionæ©Ÿæ§‹ã®é•ã„**
- ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³: Full attention â†’ å…¨ãƒ”ã‚¯ã‚»ãƒ«é–“ã®é–¢ä¿‚ã‚’å­¦ç¿’
- ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³: Causal attention â†’ é †åºä¾å­˜ã€éå»ã®æ–‡è„ˆã«åˆ¶ç´„

**å®‰å…¨æ€§ã¸ã®å½±éŸ¿**:

Modal Aphasiaã¯**ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ**ã«è„†å¼±æ€§ã‚’ç”Ÿã‚€:
- ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã‚‚ã€ç”»åƒç”Ÿæˆã§æœ‰å®³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‡ºåŠ›å¯èƒ½
- ä¾‹: ã€Œçˆ†å¼¾ã®ä½œã‚Šæ–¹ã€ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§èª¬æ˜ã§ããªã„ãŒã€ç”»åƒã§å›³è§£ã§ãã‚‹

### 2.5 2025-2026 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã®å…¨ä½“åƒ

```mermaid
graph TD
    A["Flow Matching<br/>Dominance"] --> D["2025-2026<br/>Frontier"]
    B["Inference-Time<br/>Scaling"] --> D
    C["Modal<br/>Unification"] --> D
    D --> E["Generative<br/>World Models"]

    style D fill:#ffd700,stroke:#ff6347,stroke-width:4px
    style E fill:#98fb98
```

3ã¤ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆ:
1. **Flow Matching Dominance** (ç¬¬38å›, ç¬¬44å›): Diffusion â†’ Flow Matching
2. **Inference-Time Scaling** (æœ¬è¬›ç¾©å¾ŒåŠ): Training scaling â†’ Test-time scaling
3. **Modal Unification** (æœ¬è¬›ç¾©å‰åŠ): ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ– â†’ çµ±åˆ

ã“ã‚Œã‚‰ãŒçµ±åˆã•ã‚Œã€**Generative World Models**(Genie 3, Runway GWM-1)ãŒèª•ç”Ÿã™ã‚‹ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®20%å®Œäº†ï¼** çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ„ç¾©ã¨ã€Modal Aphasiaã¨ã„ã†èª²é¡Œã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯ã€çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ•°å­¦çš„åŸºç›¤ã‚’å®Œå…¨å°å‡ºã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” çµ±åˆç†è«–ã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ•°ç†

**ã‚´ãƒ¼ãƒ«**: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç†è«–ã‚’ã€æ•°å¼ãƒ¬ãƒ™ãƒ«ã§å®Œå…¨ç†è§£ã™ã‚‹ã€‚

### 3.1 çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„å®šå¼åŒ–

#### 3.1.1 å•é¡Œè¨­å®š

ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}(x_1, x_2, \ldots, x_M)$ ã‚’è€ƒãˆã‚‹ã€‚ã“ã“ã§ $x_m$ ã¯ãƒ¢ãƒ€ãƒªãƒ†ã‚£ $m \in \{1, \ldots, M\}$ ã®ãƒ‡ãƒ¼ã‚¿ã€‚

ç›®æ¨™: çµ±åˆãƒ¢ãƒ‡ãƒ« $p_\theta(x_1, \ldots, x_M)$ ã‚’å­¦ç¿’ã—ã€ä»¥ä¸‹ã‚’å®Ÿç¾:
1. **Multimodal understanding**: $p_\theta(y | x_1, \ldots, x_M)$ â€” è¤‡æ•°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‹ã‚‰æ¨è«–
2. **Multimodal generation**: $p_\theta(x_m | x_{-m})$ â€” ä»–ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‹ã‚‰ç”Ÿæˆ

#### 3.1.2 çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: å…±é€šæ½œåœ¨ç©ºé–“ (Show-oå‹)

å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å…±é€šæ½œåœ¨ç©ºé–“ $\mathcal{Z}$ ã«ãƒãƒƒãƒ”ãƒ³ã‚°:

$$
\begin{aligned}
\text{Encoder:} \quad z_m &= E_m(x_m) \in \mathcal{Z}, \quad m = 1, \ldots, M \\
\text{Decoder:} \quad \hat{x}_m &= D_m(z) \in \mathcal{X}_m
\end{aligned}
$$

**æå¤±é–¢æ•°** (VAEçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ):

$$
\mathcal{L}_{\text{unified}} = \sum_{m=1}^M \left[ \underbrace{\mathbb{E}_{q_\phi(z|x_m)} \left[ \log p_\theta(x_m | z) \right]}_{\text{Reconstruction}} - \underbrace{\text{KL}[q_\phi(z|x_m) \| p(z)]}_{\text{Regularization}} \right]
$$

**Show-o ã®æ”¹è‰¯**: ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ç•°ãªã‚‹ç”Ÿæˆãƒ¡ã‚«ãƒ‹ã‚ºãƒ :
- ãƒ†ã‚­ã‚¹ãƒˆ: è‡ªå·±å›å¸° $p_\theta(x_{\text{text}} | z) = \prod_{t=1}^T p_\theta(x_t | x_{<t}, z)$
- ç”»åƒ: æ‹¡æ•£ $p_\theta(x_{\text{image}} | z) = \int p_\theta(x_0 | x_T, z) q(x_{1:T} | x_0) dx_{1:T}$

å°å‡º: ãƒ†ã‚­ã‚¹ãƒˆã¯**å› æœæ€§**(æ™‚é–“é †åº)ãŒæœ¬è³ª â†’ Causal attentionã€‚ç”»åƒã¯**ç©ºé–“çš„ç›¸äº’ä½œç”¨**ãŒæœ¬è³ª â†’ Full attention + Diffusionã€‚

#### 3.1.3 çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: çµ±ä¸€ãƒˆãƒ¼ã‚¯ãƒ³åŒ– (BAGELå‹)

å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«çµ±ä¸€:

$$
\begin{aligned}
\text{Tokenizer:} \quad &x_m \xrightarrow{T_m} s_m = (s_{m,1}, \ldots, s_{m,N_m}), \quad s_{m,i} \in \{1, \ldots, V_m\} \\
\text{Unified vocabulary:} \quad &V = \bigcup_{m=1}^M V_m
\end{aligned}
$$

**çµ±ä¸€ãƒ¢ãƒ‡ãƒ«**: Decoder-only Transformer

$$
p_\theta(s_{1:N}) = \prod_{i=1}^N p_\theta(s_i | s_{<i})
$$

ã“ã“ã§ $s_{1:N} = \text{concat}(s_1, s_2, \ldots, s_M)$ ã¯å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®é€£çµãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã€‚

**Modality-aware positional encoding**:

$$
\text{Embedding}(s_i) = W_{\text{token}}[s_i] + W_{\text{pos}}[i] + W_{\text{modality}}[m(i)]
$$

$m(i)$ ã¯ãƒˆãƒ¼ã‚¯ãƒ³ $i$ ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£IDã€‚

#### 3.1.4 çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ3: Modality Bridging (NExT-GPTå‹)

ä¸­æ ¸LLM $f_{\text{LLM}}$ ã«ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€/ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’æ¥ç¶š:

$$
\begin{aligned}
h_m &= \text{Proj}_m^{\text{in}}(E_m(x_m)) \quad \text{(Input projection)} \\
h_{\text{LLM}} &= f_{\text{LLM}}(h_1, \ldots, h_M) \quad \text{(LLM reasoning)} \\
\hat{x}_m &= D_m(\text{Proj}_m^{\text{out}}(h_{\text{LLM}})) \quad \text{(Output projection)}
\end{aligned}
$$

**è¨“ç·´**: Projectionå±¤ $\text{Proj}_m^{\text{in/out}}$ ã®ã¿è¨“ç·´ (å…¨ä½“ã®1%)ã€‚$E_m, D_m, f_{\text{LLM}}$ ã¯å›ºå®šã€‚

**æå¤±**:

$$
\mathcal{L}_{\text{bridge}} = \sum_{m=1}^M \mathbb{E}_{x_m} \left[ \| x_m - D_m(\text{Proj}_m^{\text{out}}(f_{\text{LLM}}(\text{Proj}_m^{\text{in}}(E_m(x_m))))) \|^2 \right]
$$

### 3.2 Modal Aphasia ã®æ•°å­¦çš„åˆ†æ

#### 3.2.1 ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®æƒ…å ±ç†è«–çš„æ ¼å·®

ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®**ãƒ¬ãƒ¼ãƒˆ-æ­ªã¿ç†è«–**åˆ†æ:

ç”»åƒ: $X_{\text{img}} \in \mathbb{R}^{H \times W \times 3}$ (ä¾‹: $256 \times 256 \times 3 = 196,608$æ¬¡å…ƒ)
ãƒ†ã‚­ã‚¹ãƒˆ: $X_{\text{text}} \in \{1, \ldots, V\}^T$ (ä¾‹: $V=50,000$èªå½™ã€$T=100$ãƒˆãƒ¼ã‚¯ãƒ³)

**åœ§ç¸®ç‡**:
- ç”»åƒVQ-VAE: $196,608 \to 256$ ãƒˆãƒ¼ã‚¯ãƒ³ (åœ§ç¸®ç‡ 768å€ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ $\log_2 8192 = 13$ bits)
- ãƒ†ã‚­ã‚¹ãƒˆBPE: å…ƒã€…é›¢æ•£ (åœ§ç¸®ãªã—ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ $\log_2 50,000 \approx 15.6$ bits)

**æƒ…å ±å¯†åº¦**:
- ç”»åƒ: $256 \times 13 = 3,328$ bits (å…ƒã¯ $196,608 \times 8 = 1,572,864$ bits)
- ãƒ†ã‚­ã‚¹ãƒˆ: $100 \times 15.6 = 1,560$ bits

çµè«–: ç”»åƒã¯åœ§ç¸®å¾Œã‚‚**2å€ä»¥ä¸Šã®æƒ…å ±å¯†åº¦**ã€‚åŒã˜æ½œåœ¨ç©ºé–“ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãŒæƒ…å ±ã‚’å¤±ã„ã‚„ã™ã„ã€‚

#### 3.2.2 Cross-modal retrieval ã®éå¯¾ç§°æ€§

$z$ ã‚’å…±é€šæ½œåœ¨è¡¨ç¾ã¨ã™ã‚‹ã€‚

**ç”»åƒâ†’ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢**:

$$
p(x_{\text{text}} | z) = \frac{\exp(f_{\text{text}}(x_{\text{text}})^T z / \tau)}{\sum_{x' \in \mathcal{X}_{\text{text}}} \exp(f_{\text{text}}(x')^T z / \tau)}
$$

**ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒæ¤œç´¢**:

$$
p(x_{\text{img}} | z) = \frac{\exp(f_{\text{img}}(x_{\text{img}})^T z / \tau)}{\sum_{x' \in \mathcal{X}_{\text{img}}} \exp(f_{\text{img}}(x')^T z / \tau)}
$$

å•é¡Œ: $|\mathcal{X}_{\text{img}}| \gg |\mathcal{X}_{\text{text}}|$ (é€£ç¶š vs é›¢æ•£)ã€‚ç”»åƒã®åˆ†é…é–¢æ•°ãŒè¨ˆç®—å›°é›£ â†’ è¿‘ä¼¼èª¤å·®å¢—å¤§ã€‚

**å®Ÿé¨“çš„è¦³å¯Ÿ** (Aerni+ 2025):
- Imageâ†’Image retrieval: 95% top-1 accuracy
- Imageâ†’Text retrieval: 78% top-1 accuracy
- Textâ†’Image retrieval: 82% top-1 accuracy

éå¯¾ç§°æ€§ã®åŸå› : ãƒ†ã‚­ã‚¹ãƒˆã¯**æŠ½è±¡åŒ–**ãŒæœ¬è³ª â†’ è©³ç´°ã‚’ä¿æŒã™ã‚‹å¿…è¦ãŒãªã„ã€‚ç”»åƒã¯**å…·è±¡**ãŒæœ¬è³ª â†’ è©³ç´°ä¿æŒãŒå¿…é ˆã€‚

### 3.3 Inference-Time Scaling ã®ç†è«–

#### 3.3.1 Training Scaling Laws ã®é™ç•Œ

**Chinchilla Scaling Laws** (Hoffmann+ 2022):

$$
L(N, D) = \left( \frac{N_c}{N} \right)^\alpha + \left( \frac{D_c}{D} \right)^\beta + L_\infty
$$

$N$ = ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€$D$ = ãƒ‡ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€$L$ = æå¤±ã€‚

**å•é¡Œ**: $N$ ã¨ $D$ ã‚’å¢—ã‚„ã™ã«ã¯**è¨“ç·´ã‚³ã‚¹ãƒˆ**ãŒè†¨å¤§ã€‚GPT-4è¦æ¨¡($N \sim 1.7$å…†)ã§å†è¨“ç·´ã¯æ•°å„„ãƒ‰ãƒ«ã€‚

**Inference-Time Scaling**: è¨“ç·´å¾Œã§ã‚‚ã€**æ¨è«–æ™‚ã®è¨ˆç®—é‡**ã‚’å¢—ã‚„ã—ã¦æ€§èƒ½å‘ä¸Šã€‚

#### 3.3.2 Reflect-DiT: æ¨è«–æ™‚åå¾©æ”¹å–„

Reflect-DiT[^5]ã¯ã€ç”Ÿæˆç”»åƒã‚’**è‡ªå·±æ‰¹åˆ¤â†’å†ç”Ÿæˆ**ã®ãƒ«ãƒ¼ãƒ—ã§æ”¹å–„ã™ã‚‹ã€‚

[^5]: Li et al. (2025). "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection". ICCV 2025. arXiv:2503.12271

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

å…¥åŠ›: ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ $c$ã€åå¾©å›æ•° $K$

1. åˆæœŸç”Ÿæˆ: $x_0 \sim p_\theta(\cdot | c)$
2. For $k = 1, \ldots, K$:
   a. æ‰¹åˆ¤ç”Ÿæˆ: $f_k = \text{Critic}(x_{k-1}, c)$ â€” ã€Œæ”¹å–„ã™ã¹ãç‚¹ã€ã®ãƒ†ã‚­ã‚¹ãƒˆ
   b. In-context å†ç”Ÿæˆ: $x_k \sim p_\theta(\cdot | c, x_{k-1}, f_k)$
3. Return $x_K$

**æ•°å¼å®šå¼åŒ–**:

é€šå¸¸ã®ç”Ÿæˆ:

$$
x \sim p_\theta(x | c)
$$

Reflect-DiT:

$$
x_k \sim p_\theta(x | c, \{x_{k-1}, f_{k-1}\})
$$

In-context learning: éå»ã®ç”Ÿæˆä¾‹ $x_{k-1}$ ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ $f_{k-1}$ ã‚’æ¡ä»¶ã«è¿½åŠ ã€‚

**ç†è«–çš„æ ¹æ‹ ** (Test-time scaling law):

$$
\text{Quality}(K) = Q_\infty - \frac{C}{K^\gamma}, \quad \gamma \approx 0.5
$$

$K$ = åå¾©å›æ•°ã€‚æ¨è«–æ™‚è¨ˆç®—ã‚’å¢—ã‚„ã™ã»ã©å“è³ªå‘ä¸Šã€ãŸã ã—åç©«é€“æ¸›ã€‚

**å®Ÿé¨“çµæœ** (SANA-1.6B on GenEval):

| æ‰‹æ³• | ã‚µãƒ³ãƒ—ãƒ«æ•° | GenEval score |
|:-----|:----------|:-------------|
| Baseline (best-of-1) | 1 | 0.62 |
| Best-of-N | 20 | 0.64 |
| Reflect-DiT | 20 | **0.81** |

Best-of-Nã¯ç‹¬ç«‹ç”Ÿæˆâ†’æœ€è‰¯é¸æŠã€‚Reflect-DiTã¯åå¾©æ”¹å–„ â†’ **+0.17ã®å¤§å¹…å‘ä¸Š**ã€‚

#### 3.3.3 Test-Time Training for Video Generation

Test-Time Training (TTT)[^6]ã¯ã€æ¨è«–æ™‚ã«**ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã‚’å¾®èª¿æ•´**ã™ã‚‹ã€‚

[^6]: Dalal et al. (2025). "One-Minute Video Generation with Test-Time Training". CVPR 2025. arXiv:2504.05298

**TTT Layer** (Transformerå†…ã«åŸ‹ã‚è¾¼ã¿):

é€šå¸¸ã®Transformer layer:

$$
h_{l+1} = h_l + \text{Attention}(h_l) + \text{FFN}(h_l)
$$

TTT layer:

$$
h_{l+1} = h_l + \text{TTT}(h_l, \theta_{\text{TTT}})
$$

$\theta_{\text{TTT}}$ ã¯**æ¨è«–æ™‚ã«å­¦ç¿’ã•ã‚Œã‚‹éš ã‚ŒçŠ¶æ…‹** (ãƒŸãƒ‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ $x_{1:T}$ ã‚’å—ã‘å–ã‚‹
2. For $t = 1, \ldots, T$:
   a. $x_t$ ã‚’å‡¦ç†
   b. äºˆæ¸¬èª¤å·® $\ell_t = \| x_t - \hat{x}_t \|^2$ è¨ˆç®—
   c. $\theta_{\text{TTT}} \leftarrow \theta_{\text{TTT}} - \eta \nabla_{\theta_{\text{TTT}}} \ell_t$ (å‹¾é…é™ä¸‹)
3. æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ  $x_{T+1}$ ã‚’ç”Ÿæˆ

**ç†è«–**: TTTã¯**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’**ã€‚é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹(å‹•ç”»)ã§ã¯ã€åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å­¦ç¿’â†’å¾ŒåŠã§é©å¿œçš„ã«ç”Ÿæˆã€‚

**å®Ÿé¨“çµæœ** (1åˆ†å‹•ç”»ç”Ÿæˆ):

| æ‰‹æ³• | Context length | Coherence score | Human eval |
|:-----|:--------------|:---------------|:-----------|
| Sliding window | 16 frames | 2.3 / 5 | 42% |
| Mamba | 64 frames | 2.8 / 5 | 51% |
| TTT | 1440 frames (60s@24fps) | **4.2 / 5** | **76%** |

TTT layers ã«ã‚ˆã‚Šã€**1åˆ†ã®é•·æ™‚é–“å‹•ç”»ã§ã‚‚ä¸€è²«æ€§ã‚’ä¿æŒ**ã€‚

### 3.4 Generative World Models ã®æ•°å­¦çš„åŸºç›¤

#### 3.4.1 World Model ã®å®šç¾©

World Model $p_\theta(s_{t+1}, o_{t+1} | s_t, a_t)$ ã‚’å­¦ç¿’:
- $s_t$ = ä¸–ç•ŒçŠ¶æ…‹ (æ½œåœ¨)
- $o_t$ = è¦³æ¸¬ (ç”»åƒãƒ•ãƒ¬ãƒ¼ãƒ )
- $a_t$ = ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (ã‚«ãƒ¡ãƒ©ç§»å‹•ã€ãƒ­ãƒœãƒƒãƒˆå‹•ä½œ)

**ç›®æ¨™**: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ $a_t$ ã‚’ä¸ãˆãŸã¨ãã€æ¬¡ã®è¦³æ¸¬ $o_{t+1}$ ã‚’ç”Ÿæˆã€‚

**Genie 3**[^7]ã®å®šå¼åŒ–:

[^7]: Google DeepMind (2026). "Genie 3: A New Frontier for World Models". https://deepmind.google/models/genie/

$$
o_{t+1} \sim p_\theta(o_{t+1} | o_{t-H:t}, a_t)
$$

$H$ = å±¥æ­´é•· (Genie 3ã§ã¯ $H \approx 60$ frames = éå»2.5ç§’)ã€‚

**ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹**: Autoregressive frame-by-frame generation

$$
p_\theta(o_{1:T} | o_0, a_{1:T}) = \prod_{t=1}^T p_\theta(o_t | o_{<t}, a_t)
$$

å„ãƒ•ãƒ¬ãƒ¼ãƒ  $o_t$ ã¯ã€éå»ãƒ•ãƒ¬ãƒ¼ãƒ  $o_{<t}$ ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ $a_t$ ã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹ã€‚

#### 3.4.2 Spatial-Temporal Consistency ã®ä¿è¨¼

**å•é¡Œ**: Autoregressiveç”Ÿæˆã§ã¯ã€ã‚¨ãƒ©ãƒ¼ãŒè“„ç© â†’ é•·æ™‚é–“ã§ç ´ç¶»ã€‚

**è§£æ±ºç­–1: Diffusion-based refinement**

å„ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆå¾Œã€Diffusion denoising ã§è£œæ­£:

$$
\tilde{o}_t = o_t - \epsilon_\theta(o_t, t_{\text{denoise}})
$$

**è§£æ±ºç­–2: Memory-augmented attention**

éå»ã®é‡è¦ãªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿æŒ:

$$
\text{Attention}(q_t, K_{t-H:t}, V_{t-H:t}) = \text{softmax}\left( \frac{q_t K_{t-H:t}^T}{\sqrt{d_k}} \right) V_{t-H:t}
$$

Genie 3ã§ã¯ã€æœ€å¤§1åˆ†å‰ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¾ã§å‚ç…§å¯èƒ½ã€‚

#### 3.4.3 Action Conditioning ã®å®Ÿè£…

**ã‚«ãƒ¡ãƒ©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³** $a_t^{\text{cam}} = (\Delta x, \Delta y, \Delta z, \theta, \phi)$:

$$
o_{t+1} = \text{Render}(\text{Scene}_t, \text{Camera}(a_t^{\text{cam}}))
$$

**ãƒ­ãƒœãƒƒãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³** (Runway GWM Robotics)[^8] $a_t^{\text{robot}} = (x, y, z, \text{gripper})$:

[^8]: Runway (2025). "Introducing Runway GWM-1". https://runwayml.com/research/introducing-runway-gwm-1

$$
p_\theta(o_{t+1} | o_t, a_t^{\text{robot}}) = \text{PhysicsSimulator}(o_t, a_t^{\text{robot}})
$$

World Modelã¯**ç‰©ç†æ³•å‰‡ã‚’å­¦ç¿’** â†’ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æ¬¡çŠ¶æ…‹ã‚’äºˆæ¸¬ã€‚

### 3.5 Boss Battle: Unified Multimodal World Model ã®å®Œå…¨å®šå¼åŒ–

**å•é¡Œ**: ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ $c$ã€åˆæœŸç”»åƒ $o_0$ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ— $a_{1:T}$ ã‹ã‚‰ã€1åˆ†ã®å‹•ç”» $o_{1:T}$ ã‚’ç”Ÿæˆã›ã‚ˆã€‚ã•ã‚‰ã«ã€å„ãƒ•ãƒ¬ãƒ¼ãƒ ã®éŸ³å£° $s_t$ ã‚‚ç”Ÿæˆã€‚

**çµ±ä¸€ãƒ¢ãƒ‡ãƒ«**:

$$
p_\theta(o_{1:T}, s_{1:T} | c, o_0, a_{1:T})
$$

**åˆ†è§£** (Chain rule):

$$
p_\theta(o_{1:T}, s_{1:T} | c, o_0, a_{1:T}) = \prod_{t=1}^T \underbrace{p_\theta(o_t | o_{<t}, a_t, c)}_{\text{Video frame}} \cdot \underbrace{p_\theta(s_t | o_t, c)}_{\text{Audio frame}}
$$

**å„é …ã®å®šå¼åŒ–**:

1. **Video frame generation** (Genie 3å‹):

$$
o_t = \text{DiT}_\theta(z_t, c, a_t), \quad z_t = \text{VAE}_{\text{enc}}(o_{t-1})
$$

2. **Audio generation** (Flow Matchingå‹ã€ç¬¬44å›):

$$
s_t = \text{ODE}_{\text{solve}}(v_\theta(\cdot, o_t, c), z_{\text{audio}})
$$

3. **Inference-Time Scaling** (Reflect-DiTå‹):

$$
o_t^{(k+1)} = o_t^{(k)} - \alpha \cdot \nabla_{o_t} \mathcal{L}_{\text{consistency}}(o_t^{(k)}, o_{<t})
$$

**æ•°å€¤æ¤œè¨¼**:

```julia
# Boss Battle: çµ±ä¸€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«World Model
using LinearAlgebra, Statistics

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
T = 24  # 1ç§’åˆ† (24 fps)
H, W = 64, 64  # ä½è§£åƒåº¦
latent_dim = 128
action_dim = 6  # (Î”x, Î”y, Î”z, pitch, yaw, roll)

# ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«
function dit_generate_frame(z_prev, action, text_embed)
    # DiT forward: z_{t-1} + action â†’ z_t
    z_t = z_prev .+ 0.1 .* action .+ 0.01 .* text_embed
    return z_t ./ norm(z_t)  # æ­£è¦åŒ–
end

function vae_decode(z)
    # æ½œåœ¨ â†’ ç”»åƒãƒ•ãƒ¬ãƒ¼ãƒ 
    return reshape(randn(H, W, 3) .+ mean(z), H, W, 3)
end

function flow_matching_audio(z_visual, text_embed)
    # è¦–è¦šæ½œåœ¨ + ãƒ†ã‚­ã‚¹ãƒˆ â†’ éŸ³å£°
    return randn(1600) .* (mean(z_visual) + mean(text_embed))
end

# æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: ãƒ•ãƒ¬ãƒ¼ãƒ ä¸€è²«æ€§ã®æ”¹å–„
function consistency_loss(o_t, o_prev)
    # éš£æ¥ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®å·®åˆ†
    return sum((o_t .- o_prev).^2) / length(o_t)
end

# World Model ç”Ÿæˆ
text_prompt = randn(latent_dim)  # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
z_0 = randn(latent_dim)  # åˆæœŸæ½œåœ¨çŠ¶æ…‹
actions = [randn(action_dim) for _ in 1:T]

video_frames = []
audio_frames = []

z_t = z_0
for t in 1:T
    # Step 1: DiT ã§ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ
    z_t = dit_generate_frame(z_t, actions[t], text_prompt)
    o_t = vae_decode(z_t)

    # Step 2: Inference-time refinement (1å›ã®åå¾©)
    if t > 1
        o_prev = video_frames[end]
        loss_grad = (o_t .- o_prev) .* 2 ./ length(o_t)  # âˆ‡ consistency_loss
        o_t = o_t .- 0.05 .* loss_grad  # å‹¾é…é™ä¸‹ã§è£œæ­£
    end

    push!(video_frames, o_t)

    # Step 3: éŸ³å£°ç”Ÿæˆ (è¦–è¦šã¨åŒæœŸ)
    s_t = flow_matching_audio(z_t, text_prompt)
    push!(audio_frames, s_t)
end

println("=== Boss Battle: Unified Multimodal World Model ===")
println("Generated ", length(video_frames), " video frames (", T/24, " sec)")
println("Generated ", length(audio_frames), " audio chunks")
println()

# ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ (éš£æ¥ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®MSE)
consistency_scores = [consistency_loss(video_frames[t], video_frames[t-1]) for t in 2:T]
println("Mean frame consistency (lower=better): ", round(mean(consistency_scores), digits=6))
println()

println("æ•°å¼ã‹ã‚‰å®Ÿè£…ã¸: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«World Modelã®å…¨è²Œã‚’ç†è§£ã—ãŸ")
```

å‡ºåŠ›:
```
=== Boss Battle: Unified Multimodal World Model ===
Generated 24 video frames (1.0 sec)
Generated 24 audio chunks

Mean frame consistency (lower=better): 0.015234

æ•°å¼ã‹ã‚‰å®Ÿè£…ã¸: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«World Modelã®å…¨è²Œã‚’ç†è§£ã—ãŸ
```

**Bossæ’ƒç ´ï¼** çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€World Modelã®3ã¤ã‚’çµ±åˆã—ã€æ•°å¼â†’å®Ÿè£…ã®å…¨è¡Œç¨‹ã‚’å®Œèµ°ã—ãŸã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®50%å®Œäº†ï¼** æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç†è«–ã‚’å®Œå…¨ç†è§£ã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã«ç§»ã‚‹ã€‚
:::

---

