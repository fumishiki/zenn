---
title: "ç¬¬49å›: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆ & æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸŒ"
type: "tech"
topics: ["machinelearning", "deeplearning", "multimodal", "julia", "inference"]
published: true
---

# ç¬¬49å›: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆ & æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° â€” å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆã¨æ¨è«–æ™‚è¨ˆç®—ã®é©å‘½

> **ç”»åƒãƒ»éŸ³å£°ãƒ»å‹•ç”»ãƒ»3Dãƒ»ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ç§‘å­¦ â€” å…¨ã¦ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§ã€‚è¨“ç·´æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‹ã‚‰æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¸ã€‚2025-2026å¹´ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆãŒã€ã“ã“ã«å®Œæˆã™ã‚‹ã€‚**

ç¬¬43-48å›ã§å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£(ç”»åƒãƒ»éŸ³å£°ãƒ»å‹•ç”»ãƒ»3Dãƒ»ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ç§‘å­¦)ã‚’å€‹åˆ¥ã«ç¿’å¾—ã—ãŸã€‚DiT/FLUXã€F5-TTS/Flow Matchingã€Sora 2/CogVideoXã€NeRF/3DGSã€MotionGPT-3/4DGSã€RFdiffusion3/MatterGen â€” ãã‚Œãã‚Œã®åˆ†é‡ã§æœ€å…ˆç«¯ã‚’å­¦ã‚“ã ã€‚

ã—ã‹ã—ã€å€‹åˆ¥ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã«ã¯é™ç•ŒãŒã‚ã‚‹ã€‚**ã€Œç”»åƒã‚’ç†è§£ã—ã¦éŸ³å£°ã§èª¬æ˜ã€ã€Œãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ã‚’ç”Ÿæˆã—ã€3Dã‚·ãƒ¼ãƒ³ã¨ã—ã¦å±•é–‹ã€** â€” ã“ã®ã‚ˆã†ãªè¤‡é›‘ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ã‚¹ã‚¯ã«ã¯ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±åˆã—ãŸçµ±ä¸€ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€2025-2026å¹´ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’2ã¤ã®è»¸ã§æ•´ç†ã™ã‚‹:

**è»¸1: Unified Multimodal Models** â€” ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ– â†’ çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã¸ã€‚Show-o/Show-o2ã€BAGELã€GPT-4oã€NExT-GPTãŒåˆ‡ã‚Šé–‹ãã€å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆã®ä¸–ç•Œã€‚ãã—ã¦ã€çµ±åˆã®ä»£å„Ÿã¨ã—ã¦ã®**Modal Aphasiaå•é¡Œ**ã€‚

**è»¸2: Inference-Time Scaling** â€” Training scaling laws â†’ Test-time scaling lawsã¸ã€‚Reflect-DiTã€Test-time TrainingãŒç¤ºã™ã€æ¨è«–æ™‚è¨ˆç®—ã®é©å‘½ã€‚è¨“ç·´å¾Œã§ã‚‚ã€æ¨è«–æ™‚ã«è¨ˆç®—ã‚’æŠ•å…¥ã™ã‚Œã°å“è³ªãŒå‘ä¸Šã™ã‚‹ â€” ã“ã‚ŒãŒæ¬¡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã ã€‚

ã•ã‚‰ã«ã€**Generative World Models**(Genie 3ã€Runway GWM-1ã€LingBot-World)ãŒã€çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’çµ„ã¿åˆã‚ã›ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªä¸–ç•Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¸ã¨é€²åŒ–ã™ã‚‹ã€‚

Course V æœ€çµ‚è¬›ç¾©ã®å‰ã«ã€2025-2026å¹´ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’å®Œå…¨ç†è§£ã™ã‚‹ã€‚æ¬¡ã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’äºˆæ¸¬ã™ã‚‹åŠ›ã‚’ã€ã“ã“ã§æ‰‹ã«å…¥ã‚Œã‚ˆã†ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚æœ¬è¬›ç¾©ã¯ **Course V ç¬¬7å›** â€” å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç¿’å¾—å®Œäº†å¾Œã®çµ±åˆç·¨ã ã€‚
:::

```mermaid
graph TD
    A["ç¬¬43å› DiT"] --> I["ç¬¬49å›<br/>çµ±åˆ+æ¨è«–æ™‚<br/>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"]
    B["ç¬¬44å› éŸ³å£°"] --> I
    C["ç¬¬45å› å‹•ç”»"] --> I
    D["ç¬¬46å› 3D"] --> I
    E["ç¬¬47å› ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³"] --> I
    F["ç¬¬48å› ç§‘å­¦"] --> I
    I --> J["ç¬¬50å›<br/>ç·æ‹¬+å’æ¥­åˆ¶ä½œ"]
    style I fill:#ffd700,stroke:#ff6347,stroke-width:4px
    style J fill:#98fb98
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’1ãƒ¢ãƒ‡ãƒ«ã§

**ã‚´ãƒ¼ãƒ«**: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒã€ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒãƒ»éŸ³å£°ã‚’åŒæ™‚ã«æ‰±ã†æ§˜å­ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

å¾“æ¥ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«(CLIP=ç”»åƒç†è§£ã€DALL-E=ç”»åƒç”Ÿæˆã€Whisper=éŸ³å£°èªè­˜)ã¯ã€ãã‚Œãã‚Œç‹¬ç«‹ã—ã¦ã„ãŸã€‚**çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«**ã¯ã€1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç†è§£ãƒ»ç”Ÿæˆã™ã‚‹ã€‚

```julia
using Random, Statistics

# Unified Multimodal Model ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
# å…¥åŠ›: text/image/audio ã®ã„ãšã‚Œã‹ â†’ å‡ºåŠ›: text/image/audio ã®ã„ãšã‚Œã‹

# å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å˜ç´”ãªãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾
struct MultimodalInput
    modality::Symbol  # :text, :image, :audio
    data::Vector{Float64}
end

# çµ±åˆãƒ¢ãƒ‡ãƒ«: å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å…±é€šæ½œåœ¨ç©ºé–“ã¸ãƒãƒƒãƒ”ãƒ³ã‚°
function unified_encoder(input::MultimodalInput, shared_dim=128)
    # ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ â†’ å…±é€šæ½œåœ¨ç©ºé–“
    if input.modality == :text
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (å˜èªåŸ‹ã‚è¾¼ã¿ â†’ Transformer)
        return randn(shared_dim) .+ mean(input.data)
    elseif input.modality == :image
        # ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (ViT â†’ æ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«)
        return randn(shared_dim) .+ std(input.data)
    elseif input.modality == :audio
        # éŸ³å£°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ (Spectrogram â†’ Audio Transformer)
        return randn(shared_dim) .+ sum(input.data) / length(input.data)
    else
        error("Unknown modality: $(input.modality)")
    end
end

# å…±é€šæ½œåœ¨ç©ºé–“ã‹ã‚‰å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¸ãƒ‡ã‚³ãƒ¼ãƒ‰
function unified_decoder(latent::Vector{Float64}, target_modality::Symbol)
    if target_modality == :text
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ€ (æ½œåœ¨ â†’ ãƒˆãƒ¼ã‚¯ãƒ³åˆ—)
        return "Generated text: " * string(round(mean(latent), digits=3))
    elseif target_modality == :image
        # ç”»åƒãƒ‡ã‚³ãƒ¼ãƒ€ (æ½œåœ¨ â†’ ç”»åƒãƒ‘ãƒƒãƒ)
        return "Generated image with mean: " * string(round(mean(latent), digits=3))
    elseif target_modality == :audio
        # éŸ³å£°ãƒ‡ã‚³ãƒ¼ãƒ€ (æ½œåœ¨ â†’ Waveform)
        return "Generated audio with RMS: " * string(round(std(latent), digits=3))
    else
        error("Unknown modality: $(target_modality)")
    end
end

# Any-to-Any å¤‰æ›ã®å®Ÿæ¼”
input_text = MultimodalInput(:text, randn(512))
input_image = MultimodalInput(:image, randn(256, 256) |> vec)
input_audio = MultimodalInput(:audio, randn(16000))

println("=== Unified Multimodal Model: Any-to-Any ===")
println()

# Text â†’ Image
latent_text = unified_encoder(input_text)
output_image = unified_decoder(latent_text, :image)
println("Text â†’ Image: ", output_image)

# Image â†’ Audio
latent_image = unified_encoder(input_image)
output_audio = unified_decoder(latent_image, :audio)
println("Image â†’ Audio: ", output_audio)

# Audio â†’ Text
latent_audio = unified_encoder(input_audio)
output_text = unified_decoder(latent_audio, :text)
println("Audio â†’ Text: ", output_text)

println()
println("å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãŒå…±é€šæ½œåœ¨ç©ºé–“ã§çµ±åˆã•ã‚Œã‚‹ â€” ã“ã‚ŒãŒ Unified Multimodal Models")
```

å‡ºåŠ›:
```
=== Unified Multimodal Model: Any-to-Any ===

Text â†’ Image: Generated image with mean: 0.234
Image â†’ Audio: Generated audio with RMS: 1.012
Audio â†’ Text: Generated text: -0.156

å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãŒå…±é€šæ½œåœ¨ç©ºé–“ã§çµ±åˆã•ã‚Œã‚‹ â€” ã“ã‚ŒãŒ Unified Multimodal Models
```

**30ç§’ã§ Any-to-Any ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤‰æ›ã‚’ä½“é¨“ã—ãŸã€‚** ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒã€ç”»åƒâ†’éŸ³å£°ã€éŸ³å£°â†’ãƒ†ã‚­ã‚¹ãƒˆ â€” å…¨ã¦ã®çµ„ã¿åˆã‚ã›ãŒ1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§å®Ÿè¡Œã•ã‚Œã‚‹ã€‚ã“ã‚ŒãŒ**çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«**ã®æœ¬è³ªã ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®3%å®Œäº†ï¼** Zone 0 ã¯ã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚¢ãƒƒãƒ—ã€‚æ¬¡ã¯æœ€æ–°ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿéš›ã«è§¦ã£ã¦ã€çµ±åˆã®ãƒ¡ãƒªãƒƒãƒˆã¨èª²é¡Œã‚’ä½“æ„Ÿã™ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” Unified Multimodal Models ã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**ã‚´ãƒ¼ãƒ«**: Show-oã€BAGELã€NExT-GPTã®3ã¤ã®çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å®Ÿè£…ã—ã€è¨­è¨ˆæ€æƒ³ã®é•ã„ã‚’ä½“æ„Ÿã™ã‚‹ã€‚

### 1.1 Show-o: Autoregressive + Diffusion ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰çµ±åˆ

Show-o[^1]ã¯**ICLR 2025**ã§ç™ºè¡¨ã•ã‚ŒãŸçµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã ã€‚ç‰¹å¾´ã¯ã€**ãƒ†ã‚­ã‚¹ãƒˆã¯è‡ªå·±å›å¸°(Causal Attention)ã€ç”»åƒã¯æ‹¡æ•£(Full Attention)**ã¨ã„ã†ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚

[^1]: Wu et al. (2023). "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation". ICLR 2025. arXiv:2408.12528

```julia
# Show-o ã®ã‚³ã‚¢è¨­è¨ˆ: ãƒ†ã‚­ã‚¹ãƒˆ=ARã€ç”»åƒ=Diffusion ã®çµ±åˆ

struct ShowOModel
    text_vocab::Int      # ãƒ†ã‚­ã‚¹ãƒˆèªå½™ã‚µã‚¤ã‚º
    image_codebook::Int  # ç”»åƒã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚µã‚¤ã‚º (VQ-VAE)
    hidden_dim::Int
    n_heads::Int
end

# ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®è‡ªå·±å›å¸°ç”Ÿæˆ (Causal Attention)
function text_autoregressive_forward(model::ShowOModel, text_tokens, past_kv=nothing)
    # Causal mask: æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ãªã„
    seq_len = length(text_tokens)
    causal_mask = tril(ones(seq_len, seq_len))  # ä¸‹ä¸‰è§’è¡Œåˆ—

    # Transformer with causal attention
    # Q, K, V = Linear(text_embed)
    # Attention = softmax(QK^T / âˆšd_k) * V (with causal_mask)
    logits = randn(seq_len, model.text_vocab)  # ç°¡ç•¥åŒ–

    return logits, nothing  # logits ã¨æ›´æ–°ã•ã‚ŒãŸKVã‚­ãƒ£ãƒƒã‚·ãƒ¥
end

# ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒªãƒ³ã‚° (Full Attention)
function image_diffusion_forward(model::ShowOModel, image_tokens, t)
    # Full attention: å…¨ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ç›¸äº’ä½œç”¨ã‚’è¨±å¯
    num_tokens = length(image_tokens)
    full_mask = ones(num_tokens, num_tokens)  # å…¨çµåˆ

    # Diffusion denoising step
    # xt = âˆšá¾±tÂ·x0 + âˆš(1-á¾±t)Â·Îµ
    # äºˆæ¸¬: Îµ_Î¸(xt, t)
    alpha_bar_t = 1 - t / 1000  # ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    predicted_noise = randn(size(image_tokens))  # ç°¡ç•¥åŒ–

    return predicted_noise
end

# çµ±åˆæ¨è«–: Text prompt â†’ Image generation
function show_o_generate(model::ShowOModel, text_prompt, num_diffusion_steps=20)
    # 1. ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªå·±å›å¸°ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    text_tokens = [rand(1:model.text_vocab) for _ in 1:10]  # ãƒ€ãƒŸãƒ¼ãƒˆãƒ¼ã‚¯ãƒ³
    text_logits, _ = text_autoregressive_forward(model, text_tokens)

    # 2. ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’æ¡ä»¶ã¨ã—ã¦ç”»åƒã‚’æ‹¡æ•£ç”Ÿæˆ
    image_tokens = randn(256)  # 16Ã—16 ãƒ‘ãƒƒãƒ

    for step in num_diffusion_steps:-1:1
        t = step / num_diffusion_steps
        noise_pred = image_diffusion_forward(model, image_tokens, t)
        # Denoising update (DDPMå¼)
        image_tokens = image_tokens - 0.1 * noise_pred  # ç°¡ç•¥åŒ–
    end

    return image_tokens
end

# å®Ÿè¡Œ
model = ShowOModel(50000, 8192, 768, 12)
generated_image = show_o_generate(model, "A cat on a mat")
println("Show-o: Text â†’ Image generation completed")
println("  Generated image tokens: ", size(generated_image))
println("  Key insight: ãƒ†ã‚­ã‚¹ãƒˆ=ARã€ç”»åƒ=Diffusion ã®çµ±åˆ")
```

**Show-oã®è¨­è¨ˆå“²å­¦**: ãƒ†ã‚­ã‚¹ãƒˆã¯**å› æœçš„**(éå»â†’æœªæ¥ã®é †åº)ã ãŒã€ç”»åƒã¯**åŒæ–¹å‘çš„**(å…¨ãƒ‘ãƒƒãƒé–“ã®ç›¸äº’ä½œç”¨)ã€‚ç•°ãªã‚‹æ€§è³ªã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ã€ç•°ãªã‚‹Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’é©ç”¨ã™ã‚‹ã€‚

### 1.2 BAGEL: äº‹å‰å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®çµ±åˆ

BAGEL[^2]ã¯**ByteDance**ãŒ2025å¹´ã«ç™ºè¡¨ã—ãŸã€**æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³ã®äº‹å‰å­¦ç¿’**ã§çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èƒ½åŠ›ã‚’ç²å¾—ã—ãŸãƒ¢ãƒ‡ãƒ«ã ã€‚

[^2]: ByteDance (2025). "Emerging Properties in Unified Multimodal Pretraining". arXiv:2505.14683

```julia
# BAGEL: Large-scale pretraining ã«ã‚ˆã‚‹çµ±åˆ

struct BAGELModel
    decoder_only::Bool  # True: decoder-only Transformer
    active_params::Int  # 7B active (14B total with MoE)
    pretraining_tokens::Int  # æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³
end

# çµ±åˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–: Text/Image/Video/Audio ã‚’å…¨ã¦é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«
function unified_tokenization(data, modality::Symbol)
    if modality == :text
        # BPE/SentencePiece tokenizer
        return [rand(1:50000) for _ in 1:100]
    elseif modality == :image
        # VQ-VAE tokenizer (256Ã—256 â†’ 16Ã—16 = 256 tokens)
        return [rand(1:8192) for _ in 1:256]
    elseif modality == :video
        # Video tokenizer (16 frames Ã— 16Ã—16 = 4096 tokens)
        return [rand(1:8192) for _ in 1:4096]
    elseif modality == :audio
        # Audio codec (EnCodec/WavTokenizer)
        return [rand(1:2048) for _ in 1:512]
    end
end

# Decoder-only Transformerã§å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±ä¸€å‡¦ç†
function bagel_forward(model::BAGELModel, tokens, modality_ids)
    # modality_ids: å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚¿ã‚¤ãƒ— (1=text, 2=image, 3=video, 4=audio)
    seq_len = length(tokens)

    # Modality-aware positional encoding
    pos_embed = randn(seq_len, 768)  # ä½ç½®åŸ‹ã‚è¾¼ã¿
    modality_embed = randn(seq_len, 768)  # ãƒ¢ãƒ€ãƒªãƒ†ã‚£åŸ‹ã‚è¾¼ã¿

    # Transformer layers (decoder-only, causal)
    hidden = pos_embed .+ modality_embed

    # æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ (å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±ä¸€èªå½™)
    logits = randn(seq_len, 65536)  # çµ±åˆèªå½™: text + image + video + audio

    return logits
end

# In-context learning: ç”»åƒæ“ä½œã‚¿ã‚¹ã‚¯ã‚’å°‘æ•°ä¾‹ã§å­¦ç¿’
function bagel_few_shot_image_editing(model::BAGELModel)
    # Example 1: "Rotate image 90Â°" â†’ rotated_image_tokens
    example1_text = unified_tokenization("Rotate 90 degrees", :text)
    example1_image_in = unified_tokenization(randn(256, 256), :image)
    example1_image_out = unified_tokenization(randn(256, 256), :image)  # å›è»¢å¾Œ

    # Example 2: "Make it grayscale" â†’ grayscale_image_tokens
    example2_text = unified_tokenization("Grayscale", :text)
    example2_image_in = unified_tokenization(randn(256, 256), :image)
    example2_image_out = unified_tokenization(randn(256, 256), :image)  # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«

    # Query: "Increase brightness" â†’ ?
    query_text = unified_tokenization("Increase brightness", :text)
    query_image_in = unified_tokenization(randn(256, 256), :image)

    # å…¨ã¦ã‚’é€£çµã—ã¦1ã¤ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ã—ã¦å‡¦ç†
    all_tokens = vcat(example1_text, example1_image_in, example1_image_out,
                     example2_text, example2_image_in, example2_image_out,
                     query_text, query_image_in)
    modality_ids = vcat(repeat([1], length(example1_text)),
                       repeat([2], length(example1_image_in)),
                       repeat([2], length(example1_image_out)),
                       repeat([1], length(example2_text)),
                       repeat([2], length(example2_image_in)),
                       repeat([2], length(example2_image_out)),
                       repeat([1], length(query_text)),
                       repeat([2], length(query_image_in)))

    # Forward pass: æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ = æ˜ã‚‹ãã—ãŸç”»åƒã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—
    logits = bagel_forward(model, all_tokens, modality_ids)

    # æœ€å¾Œã®256ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡º (ç”Ÿæˆã•ã‚ŒãŸç”»åƒ)
    generated_image_tokens = argmax.(eachrow(logits[end-255:end, :]))

    return generated_image_tokens
end

# å®Ÿè¡Œ
bagel_model = BAGELModel(true, 7_000_000_000, 3_000_000_000_000)
edited_image = bagel_few_shot_image_editing(bagel_model)
println("BAGEL: Few-shot image editing via in-context learning")
println("  Model: 7B active params, 3T pretraining tokens")
println("  Generated image tokens: ", length(edited_image))
println("  Key insight: äº‹å‰å­¦ç¿’ã§ emergent multimodal reasoning ç²å¾—")
```

**BAGELã®è¨­è¨ˆå“²å­¦**: å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’**é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³**ã«çµ±ä¸€ â†’ Decoder-only Transformerã§ä¸€æ‹¬å‡¦ç†ã€‚å¤§è¦æ¨¡äº‹å‰å­¦ç¿’ã«ã‚ˆã‚Šã€**Few-shot multimodal reasoning**ãŒå‰µç™ºã™ã‚‹ã€‚

### 1.3 NExT-GPT: Any-to-Any ã®å…ˆé§†è€…

NExT-GPT[^3]ã¯2023å¹´ã«ç™ºè¡¨ã•ã‚ŒãŸ**Any-to-Any**ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å…ˆé§†çš„ç ”ç©¶ã ã€‚LLMã‚’ä¸­æ ¸ã«ã€å…¥åŠ›ãƒ»å‡ºåŠ›ç”¨ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€/ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’æ¥ç¶šã™ã‚‹ã€‚

[^3]: Wu et al. (2023). "NExT-GPT: Any-to-Any Multimodal LLM". arXiv:2309.05519

```julia
# NExT-GPT: LLMä¸­å¿ƒã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆ

struct NExTGPTModel
    llm_backbone::String  # "Vicuna-7B" ãªã©ã®LLM
    image_encoder::String  # "CLIP ViT-L/14"
    audio_encoder::String  # "ImageBind Audio"
    video_encoder::String  # "ImageBind Video"
    image_decoder::String  # "Stable Diffusion"
    audio_decoder::String  # "AudioLDM"
    video_decoder::String  # "Zeroscope"
end

# Input projection: ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ â†’ LLMåŸ‹ã‚è¾¼ã¿ç©ºé–“
function input_projection(encoder_output, target_dim=4096)
    # Linear projection: encoder_dim â†’ LLM hidden_dim
    # ä¾‹: CLIP 768-dim â†’ LLM 4096-dim
    projection_matrix = randn(target_dim, 768)
    return projection_matrix * encoder_output
end

# Output projection: LLMåŸ‹ã‚è¾¼ã¿ â†’ ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ãƒ‡ã‚³ãƒ¼ãƒ€
function output_projection(llm_hidden, decoder_input_dim=768)
    # Linear projection: LLM 4096-dim â†’ decoder 768-dim
    projection_matrix = randn(decoder_input_dim, 4096)
    return projection_matrix * llm_hidden
end

# Any-to-Any pipeline
function next_gpt_any_to_any(model::NExTGPTModel, input_modality::Symbol,
                             output_modality::Symbol, input_data)
    # Step 1: Input encoding
    if input_modality == :image
        encoder_output = randn(768)  # CLIP encoding
    elseif input_modality == :audio
        encoder_output = randn(768)  # ImageBind Audio encoding
    elseif input_modality == :text
        encoder_output = randn(768)  # Text embedding
    else
        error("Unsupported input modality")
    end

    # Step 2: Project to LLM space
    llm_input = input_projection(encoder_output)

    # Step 3: LLM reasoning (simplified)
    # å®Ÿéš›ã«ã¯: "Describe this image in audio form" ãªã©ã®æŒ‡ç¤ºã¨å…±ã«å‡¦ç†
    llm_output = llm_input .+ randn(4096) .* 0.1  # LLM forward pass

    # Step 4: Project to decoder space
    decoder_input = output_projection(llm_output)

    # Step 5: Decode to target modality
    if output_modality == :image
        output = "Generated image (via Stable Diffusion)"
    elseif output_modality == :audio
        output = "Generated audio (via AudioLDM)"
    elseif output_modality == :text
        output = "Generated text: '" * string(round(mean(decoder_input), digits=3)) * "'"
    else
        error("Unsupported output modality")
    end

    return output
end

# å®Ÿè¡Œ: ç”»åƒ â†’ éŸ³å£°
next_gpt_model = NExTGPTModel("Vicuna-7B", "CLIP", "ImageBind", "ImageBind",
                              "SD", "AudioLDM", "Zeroscope")
result = next_gpt_any_to_any(next_gpt_model, :image, :audio, randn(224, 224, 3))
println("NExT-GPT: Image â†’ Audio")
println("  Result: ", result)
println("  Key insight: LLMã‚’ä¸­æ ¸ã«ã€å…¥å‡ºåŠ›ã‚’ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã§å¤‰æ›")

# éŸ³å£° â†’ ãƒ†ã‚­ã‚¹ãƒˆ
result2 = next_gpt_any_to_any(next_gpt_model, :audio, :text, randn(16000))
println("\nNExT-GPT: Audio â†’ Text")
println("  Result: ", result2)
```

**NExT-GPTã®è¨­è¨ˆå“²å­¦**: LLMã®å¼·åŠ›ãªæ¨è«–èƒ½åŠ›ã‚’æ´»ç”¨ã€‚ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€/ãƒ‡ã‚³ãƒ¼ãƒ€ã¯æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’å†åˆ©ç”¨ â†’ ä½ã‚³ã‚¹ãƒˆçµ±åˆã€‚**1%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿è¨“ç·´**(projectionå±¤ã®ã¿)ã€‚

### 1.4 3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¯”è¼ƒ

| ãƒ¢ãƒ‡ãƒ« | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | è¨“ç·´ã‚³ã‚¹ãƒˆ | ç‰¹å¾´ |
|:-------|:-------------|:----------|:-----|
| **Show-o** | Hybrid (AR + Diffusion) | ä¸­ | ãƒ†ã‚­ã‚¹ãƒˆ=Causalã€ç”»åƒ=Full attention |
| **BAGEL** | Decoder-only unified | é«˜(æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³) | äº‹å‰å­¦ç¿’ã§ emergent reasoning |
| **NExT-GPT** | LLM + modality adapters | ä½(1%è¨“ç·´) | æ—¢å­˜ãƒ¢ãƒ‡ãƒ«å†åˆ©ç”¨ |

**3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½“é¨“ã—ãŸã€‚** çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã«ã¯è¤‡æ•°ã®è¨­è¨ˆæ€æƒ³ãŒã‚ã‚Šã€ãã‚Œãã‚Œã«ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚‹ã€‚æ¬¡ã¯ã€ãªãœçµ±åˆãŒå¿…è¦ãªã®ã‹ã€ãã—ã¦çµ±åˆã®ä»£å„Ÿã¯ä½•ã‹ã‚’ç†è§£ã™ã‚‹ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®10%å®Œäº†ï¼** Zone 1 ã§çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®3ã¤ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½“é¨“ã—ãŸã€‚æ¬¡ã¯ã€çµ±åˆã®æ„ç¾©ã¨Modal Aphasiaå•é¡Œã‚’ç›´æ„Ÿçš„ã«ç†è§£ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœçµ±åˆã‹ï¼Ÿãã—ã¦ Modal Aphasia ã®ç½ 

**ã‚´ãƒ¼ãƒ«**: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ„ç¾©ã¨ã€çµ±åˆã«ä¼´ã†èª²é¡Œ(Modal Aphasia)ã‚’ç†è§£ã™ã‚‹ã€‚

### 2.1 ãªãœãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±åˆã™ã‚‹ã®ã‹ï¼Ÿ

**å¾“æ¥ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:
```
ãƒ†ã‚­ã‚¹ãƒˆ â†’ [CLIP] â†’ ç”»åƒåŸ‹ã‚è¾¼ã¿ â†’ [Stable Diffusion] â†’ ç”»åƒ
éŸ³å£° â†’ [Whisper] â†’ ãƒ†ã‚­ã‚¹ãƒˆ â†’ [ChatGPT] â†’ ãƒ†ã‚­ã‚¹ãƒˆ â†’ [TTS] â†’ éŸ³å£°
```

å•é¡Œç‚¹:
1. **ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®æƒ…å ±æå¤±**: ä¸­é–“è¡¨ç¾(ãƒ†ã‚­ã‚¹ãƒˆ)ã«å¤‰æ›ã™ã‚‹éš›ã€å…ƒã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹
2. **æ¨è«–ã‚³ã‚¹ãƒˆã®å¢—å¤§**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®é †æ¬¡å®Ÿè¡Œ â†’ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¢—åŠ 
3. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–ã®æ¬ å¦‚**: å„ãƒ¢ãƒ‡ãƒ«ã¯å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã¿ â†’ çµ±åˆçš„ãªæ¨è«–ãŒã§ããªã„

**çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒªãƒƒãƒˆ**:
1. **End-to-Endå­¦ç¿’**: å…¥åŠ›â†’å‡ºåŠ›ã‚’ç›´æ¥å­¦ç¿’ â†’ æƒ…å ±æå¤±ãªã—
2. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–**: ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»éŸ³å£°ã‚’åŒæ™‚ã«è€ƒæ…®ã—ãŸæ¨è«–
3. **åŠ¹ç‡æ€§**: 1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§å®Œçµ â†’ ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·

### 2.2 Course V å…¨ä½“ã®ä½ç½®ã¥ã‘

```mermaid
graph TD
    A["ç¬¬43å› DiT<br/>ç”»åƒç”Ÿæˆã®é©æ–°"] --> B["ç¬¬44å› éŸ³å£°<br/>Flow Matching TTS"]
    B --> C["ç¬¬45å› å‹•ç”»<br/>æ™‚é–“è»¸ã®è¿½åŠ "]
    C --> D["ç¬¬46å› 3D<br/>ç©ºé–“è¡¨ç¾"]
    D --> E["ç¬¬47å› ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³<br/>å‹•çš„3D"]
    E --> F["ç¬¬48å› ç§‘å­¦<br/>åˆ¶ç´„ä»˜ãç”Ÿæˆ"]
    F --> G["ç¬¬49å› çµ±åˆ<br/>å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆ"]
    G --> H["ç¬¬50å› ç·æ‹¬<br/>å’æ¥­åˆ¶ä½œ"]

    style G fill:#ffd700,stroke:#ff6347,stroke-width:4px
    style H fill:#98fb98
```

ç¬¬43-48å›ã§å€‹åˆ¥ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç¿’å¾— â†’ ç¬¬49å›ã§çµ±åˆ â†’ ç¬¬50å›ã§å’æ¥­åˆ¶ä½œã€‚**çµ±åˆã¯å¿œç”¨ç·¨ã®é›†å¤§æˆ**ã ã€‚

### 2.3 æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ–

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º |
|:-----|:-----------|:----------|
| **å¯¾è±¡ãƒ¢ãƒ€ãƒªãƒ†ã‚£** | ç”»åƒã®ã¿ | ç”»åƒãƒ»éŸ³å£°ãƒ»å‹•ç”»ãƒ»3Dãƒ»ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ç§‘å­¦ |
| **çµ±åˆãƒ¢ãƒ‡ãƒ«** | ãªã— | Show-o/BAGEL/NExT-GPTè©³è§£ |
| **æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°** | ãªã— | Reflect-DiT/Test-time Training |
| **World Models** | ç†è«–ã®ã¿ | Genie 3/Runway GWM-1å®Ÿè£… |
| **å®Ÿè£…è¨€èª** | Python | Julia + Rust + Elixir |

### 2.4 Modal Aphasia: çµ±åˆã®ä»£å„Ÿ

**Modal Aphasia**[^4]ã¯ã€çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒç¤ºã™é©šãã¹ãç¾è±¡ã : **è¦–è¦šçš„ã«ã¯å®Œç’§ã«è¨˜æ†¶ã—ã¦ã„ã‚‹ãŒã€è¨€èªçš„ã«ã¯èª¬æ˜ã§ããªã„**ã€‚

[^4]: Aerni et al. (2025). "Modal Aphasia: Can Unified Multimodal Models Describe Images From Memory?". arXiv:2510.21842

å®Ÿé¨“:
1. ãƒ¢ãƒ‡ãƒ«ã«æ˜ ç”»ãƒã‚¹ã‚¿ãƒ¼ã‚’è¦‹ã›ã‚‹
2. **ç”»åƒç”Ÿæˆã‚¿ã‚¹ã‚¯**: ãƒã‚¹ã‚¿ãƒ¼ã‚’å†ç¾ â†’ **ã»ã¼å®Œç’§**
3. **ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‚¿ã‚¹ã‚¯**: ãƒã‚¹ã‚¿ãƒ¼ã‚’èª¬æ˜ â†’ **é‡è¦ãªè©³ç´°ã‚’æ··åŒ**

ä¾‹: "The Godfather"ã®ãƒã‚¹ã‚¿ãƒ¼
- ç”»åƒç”Ÿæˆ: äººç‰©é…ç½®ã€è‰²èª¿ã€ãƒ•ã‚©ãƒ³ãƒˆ â€” å…¨ã¦æ­£ç¢º
- ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°: ã€Œä¸»äººå…¬ã¯éŠƒã‚’æŒã£ã¦ã„ã‚‹ã€(å®Ÿéš›ã¯æŒã£ã¦ã„ãªã„)

**ãªãœã“ã‚ŒãŒèµ·ã“ã‚‹ã®ã‹ï¼Ÿ**

ä»®èª¬1: **ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®è¡¨ç¾æ ¼å·®**
- ç”»åƒç”Ÿæˆ: é«˜æ¬¡å…ƒæ½œåœ¨ç©ºé–“(8192æ¬¡å…ƒã®VQ-VAE)ã§è©³ç´°ä¿æŒ
- ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: ä½æ¬¡å…ƒé›¢æ•£ç©ºé–“(50kèªå½™)ã§æŠ½è±¡åŒ– â†’ è©³ç´°ãŒå¤±ã‚ã‚Œã‚‹

ä»®èª¬2: **è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åã‚Š**
- ç”»åƒ-ç”»åƒãƒšã‚¢: é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ãŒè±Šå¯Œ â†’ æ­£ç¢ºãªè¦–è¦šè¨˜æ†¶
- ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢: ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¯æŠ½è±¡çš„ â†’ è©³ç´°ãªè¨€èªè¨˜æ†¶ãŒè‚²ãŸãªã„

ä»®èª¬3: **Attentionæ©Ÿæ§‹ã®é•ã„**
- ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³: Full attention â†’ å…¨ãƒ”ã‚¯ã‚»ãƒ«é–“ã®é–¢ä¿‚ã‚’å­¦ç¿’
- ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³: Causal attention â†’ é †åºä¾å­˜ã€éå»ã®æ–‡è„ˆã«åˆ¶ç´„

**å®‰å…¨æ€§ã¸ã®å½±éŸ¿**:

Modal Aphasiaã¯**ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ**ã«è„†å¼±æ€§ã‚’ç”Ÿã‚€:
- ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã‚‚ã€ç”»åƒç”Ÿæˆã§æœ‰å®³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‡ºåŠ›å¯èƒ½
- ä¾‹: ã€Œçˆ†å¼¾ã®ä½œã‚Šæ–¹ã€ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§èª¬æ˜ã§ããªã„ãŒã€ç”»åƒã§å›³è§£ã§ãã‚‹

### 2.5 2025-2026 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã®å…¨ä½“åƒ

```mermaid
graph TD
    A["Flow Matching<br/>Dominance"] --> D["2025-2026<br/>Frontier"]
    B["Inference-Time<br/>Scaling"] --> D
    C["Modal<br/>Unification"] --> D
    D --> E["Generative<br/>World Models"]

    style D fill:#ffd700,stroke:#ff6347,stroke-width:4px
    style E fill:#98fb98
```

3ã¤ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆ:
1. **Flow Matching Dominance** (ç¬¬38å›, ç¬¬44å›): Diffusion â†’ Flow Matching
2. **Inference-Time Scaling** (æœ¬è¬›ç¾©å¾ŒåŠ): Training scaling â†’ Test-time scaling
3. **Modal Unification** (æœ¬è¬›ç¾©å‰åŠ): ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ– â†’ çµ±åˆ

ã“ã‚Œã‚‰ãŒçµ±åˆã•ã‚Œã€**Generative World Models**(Genie 3, Runway GWM-1)ãŒèª•ç”Ÿã™ã‚‹ã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®20%å®Œäº†ï¼** çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ„ç¾©ã¨ã€Modal Aphasiaã¨ã„ã†èª²é¡Œã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯ã€çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ•°å­¦çš„åŸºç›¤ã‚’å®Œå…¨å°å‡ºã™ã‚‹ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” çµ±åˆç†è«–ã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ•°ç†

**ã‚´ãƒ¼ãƒ«**: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç†è«–ã‚’ã€æ•°å¼ãƒ¬ãƒ™ãƒ«ã§å®Œå…¨ç†è§£ã™ã‚‹ã€‚

### 3.1 çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„å®šå¼åŒ–

#### 3.1.1 å•é¡Œè¨­å®š

ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}(x_1, x_2, \ldots, x_M)$ ã‚’è€ƒãˆã‚‹ã€‚ã“ã“ã§ $x_m$ ã¯ãƒ¢ãƒ€ãƒªãƒ†ã‚£ $m \in \{1, \ldots, M\}$ ã®ãƒ‡ãƒ¼ã‚¿ã€‚

ç›®æ¨™: çµ±åˆãƒ¢ãƒ‡ãƒ« $p_\theta(x_1, \ldots, x_M)$ ã‚’å­¦ç¿’ã—ã€ä»¥ä¸‹ã‚’å®Ÿç¾:
1. **Multimodal understanding**: $p_\theta(y | x_1, \ldots, x_M)$ â€” è¤‡æ•°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‹ã‚‰æ¨è«–
2. **Multimodal generation**: $p_\theta(x_m | x_{-m})$ â€” ä»–ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‹ã‚‰ç”Ÿæˆ

#### 3.1.2 çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: å…±é€šæ½œåœ¨ç©ºé–“ (Show-oå‹)

å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å…±é€šæ½œåœ¨ç©ºé–“ $\mathcal{Z}$ ã«ãƒãƒƒãƒ”ãƒ³ã‚°:

$$
\begin{aligned}
\text{Encoder:} \quad z_m &= E_m(x_m) \in \mathcal{Z}, \quad m = 1, \ldots, M \\
\text{Decoder:} \quad \hat{x}_m &= D_m(z) \in \mathcal{X}_m
\end{aligned}
$$

**æå¤±é–¢æ•°** (VAEçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ):

$$
\mathcal{L}_{\text{unified}} = \sum_{m=1}^M \left[ \underbrace{\mathbb{E}_{q_\phi(z|x_m)} \left[ \log p_\theta(x_m | z) \right]}_{\text{Reconstruction}} - \underbrace{\text{KL}[q_\phi(z|x_m) \| p(z)]}_{\text{Regularization}} \right]
$$

**Show-o ã®æ”¹è‰¯**: ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ç•°ãªã‚‹ç”Ÿæˆãƒ¡ã‚«ãƒ‹ã‚ºãƒ :
- ãƒ†ã‚­ã‚¹ãƒˆ: è‡ªå·±å›å¸° $p_\theta(x_{\text{text}} | z) = \prod_{t=1}^T p_\theta(x_t | x_{<t}, z)$
- ç”»åƒ: æ‹¡æ•£ $p_\theta(x_{\text{image}} | z) = \int p_\theta(x_0 | x_T, z) q(x_{1:T} | x_0) dx_{1:T}$

å°å‡º: ãƒ†ã‚­ã‚¹ãƒˆã¯**å› æœæ€§**(æ™‚é–“é †åº)ãŒæœ¬è³ª â†’ Causal attentionã€‚ç”»åƒã¯**ç©ºé–“çš„ç›¸äº’ä½œç”¨**ãŒæœ¬è³ª â†’ Full attention + Diffusionã€‚

#### 3.1.3 çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: çµ±ä¸€ãƒˆãƒ¼ã‚¯ãƒ³åŒ– (BAGELå‹)

å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«çµ±ä¸€:

$$
\begin{aligned}
\text{Tokenizer:} \quad &x_m \xrightarrow{T_m} s_m = (s_{m,1}, \ldots, s_{m,N_m}), \quad s_{m,i} \in \{1, \ldots, V_m\} \\
\text{Unified vocabulary:} \quad &V = \bigcup_{m=1}^M V_m
\end{aligned}
$$

**çµ±ä¸€ãƒ¢ãƒ‡ãƒ«**: Decoder-only Transformer

$$
p_\theta(s_{1:N}) = \prod_{i=1}^N p_\theta(s_i | s_{<i})
$$

ã“ã“ã§ $s_{1:N} = \text{concat}(s_1, s_2, \ldots, s_M)$ ã¯å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®é€£çµãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã€‚

**Modality-aware positional encoding**:

$$
\text{Embedding}(s_i) = W_{\text{token}}[s_i] + W_{\text{pos}}[i] + W_{\text{modality}}[m(i)]
$$

$m(i)$ ã¯ãƒˆãƒ¼ã‚¯ãƒ³ $i$ ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£IDã€‚

#### 3.1.4 çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ3: Modality Bridging (NExT-GPTå‹)

ä¸­æ ¸LLM $f_{\text{LLM}}$ ã«ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€/ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’æ¥ç¶š:

$$
\begin{aligned}
h_m &= \text{Proj}_m^{\text{in}}(E_m(x_m)) \quad \text{(Input projection)} \\
h_{\text{LLM}} &= f_{\text{LLM}}(h_1, \ldots, h_M) \quad \text{(LLM reasoning)} \\
\hat{x}_m &= D_m(\text{Proj}_m^{\text{out}}(h_{\text{LLM}})) \quad \text{(Output projection)}
\end{aligned}
$$

**è¨“ç·´**: Projectionå±¤ $\text{Proj}_m^{\text{in/out}}$ ã®ã¿è¨“ç·´ (å…¨ä½“ã®1%)ã€‚$E_m, D_m, f_{\text{LLM}}$ ã¯å›ºå®šã€‚

**æå¤±**:

$$
\mathcal{L}_{\text{bridge}} = \sum_{m=1}^M \mathbb{E}_{x_m} \left[ \| x_m - D_m(\text{Proj}_m^{\text{out}}(f_{\text{LLM}}(\text{Proj}_m^{\text{in}}(E_m(x_m))))) \|^2 \right]
$$

### 3.2 Modal Aphasia ã®æ•°å­¦çš„åˆ†æ

#### 3.2.1 ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®æƒ…å ±ç†è«–çš„æ ¼å·®

ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®**ãƒ¬ãƒ¼ãƒˆ-æ­ªã¿ç†è«–**åˆ†æ:

ç”»åƒ: $X_{\text{img}} \in \mathbb{R}^{H \times W \times 3}$ (ä¾‹: $256 \times 256 \times 3 = 196,608$æ¬¡å…ƒ)
ãƒ†ã‚­ã‚¹ãƒˆ: $X_{\text{text}} \in \{1, \ldots, V\}^T$ (ä¾‹: $V=50,000$èªå½™ã€$T=100$ãƒˆãƒ¼ã‚¯ãƒ³)

**åœ§ç¸®ç‡**:
- ç”»åƒVQ-VAE: $196,608 \to 256$ ãƒˆãƒ¼ã‚¯ãƒ³ (åœ§ç¸®ç‡ 768å€ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ $\log_2 8192 = 13$ bits)
- ãƒ†ã‚­ã‚¹ãƒˆBPE: å…ƒã€…é›¢æ•£ (åœ§ç¸®ãªã—ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ $\log_2 50,000 \approx 15.6$ bits)

**æƒ…å ±å¯†åº¦**:
- ç”»åƒ: $256 \times 13 = 3,328$ bits (å…ƒã¯ $196,608 \times 8 = 1,572,864$ bits)
- ãƒ†ã‚­ã‚¹ãƒˆ: $100 \times 15.6 = 1,560$ bits

çµè«–: ç”»åƒã¯åœ§ç¸®å¾Œã‚‚**2å€ä»¥ä¸Šã®æƒ…å ±å¯†åº¦**ã€‚åŒã˜æ½œåœ¨ç©ºé–“ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãŒæƒ…å ±ã‚’å¤±ã„ã‚„ã™ã„ã€‚

#### 3.2.2 Cross-modal retrieval ã®éå¯¾ç§°æ€§

$z$ ã‚’å…±é€šæ½œåœ¨è¡¨ç¾ã¨ã™ã‚‹ã€‚

**ç”»åƒâ†’ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢**:

$$
p(x_{\text{text}} | z) = \frac{\exp(f_{\text{text}}(x_{\text{text}})^T z / \tau)}{\sum_{x' \in \mathcal{X}_{\text{text}}} \exp(f_{\text{text}}(x')^T z / \tau)}
$$

**ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒæ¤œç´¢**:

$$
p(x_{\text{img}} | z) = \frac{\exp(f_{\text{img}}(x_{\text{img}})^T z / \tau)}{\sum_{x' \in \mathcal{X}_{\text{img}}} \exp(f_{\text{img}}(x')^T z / \tau)}
$$

å•é¡Œ: $|\mathcal{X}_{\text{img}}| \gg |\mathcal{X}_{\text{text}}|$ (é€£ç¶š vs é›¢æ•£)ã€‚ç”»åƒã®åˆ†é…é–¢æ•°ãŒè¨ˆç®—å›°é›£ â†’ è¿‘ä¼¼èª¤å·®å¢—å¤§ã€‚

**å®Ÿé¨“çš„è¦³å¯Ÿ** (Aerni+ 2025):
- Imageâ†’Image retrieval: 95% top-1 accuracy
- Imageâ†’Text retrieval: 78% top-1 accuracy
- Textâ†’Image retrieval: 82% top-1 accuracy

éå¯¾ç§°æ€§ã®åŸå› : ãƒ†ã‚­ã‚¹ãƒˆã¯**æŠ½è±¡åŒ–**ãŒæœ¬è³ª â†’ è©³ç´°ã‚’ä¿æŒã™ã‚‹å¿…è¦ãŒãªã„ã€‚ç”»åƒã¯**å…·è±¡**ãŒæœ¬è³ª â†’ è©³ç´°ä¿æŒãŒå¿…é ˆã€‚

### 3.3 Inference-Time Scaling ã®ç†è«–

#### 3.3.1 Training Scaling Laws ã®é™ç•Œ

**Chinchilla Scaling Laws** (Hoffmann+ 2022):

$$
L(N, D) = \left( \frac{N_c}{N} \right)^\alpha + \left( \frac{D_c}{D} \right)^\beta + L_\infty
$$

$N$ = ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€$D$ = ãƒ‡ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€$L$ = æå¤±ã€‚

**å•é¡Œ**: $N$ ã¨ $D$ ã‚’å¢—ã‚„ã™ã«ã¯**è¨“ç·´ã‚³ã‚¹ãƒˆ**ãŒè†¨å¤§ã€‚GPT-4è¦æ¨¡($N \sim 1.7$å…†)ã§å†è¨“ç·´ã¯æ•°å„„ãƒ‰ãƒ«ã€‚

**Inference-Time Scaling**: è¨“ç·´å¾Œã§ã‚‚ã€**æ¨è«–æ™‚ã®è¨ˆç®—é‡**ã‚’å¢—ã‚„ã—ã¦æ€§èƒ½å‘ä¸Šã€‚

#### 3.3.2 Reflect-DiT: æ¨è«–æ™‚åå¾©æ”¹å–„

Reflect-DiT[^5]ã¯ã€ç”Ÿæˆç”»åƒã‚’**è‡ªå·±æ‰¹åˆ¤â†’å†ç”Ÿæˆ**ã®ãƒ«ãƒ¼ãƒ—ã§æ”¹å–„ã™ã‚‹ã€‚

[^5]: Li et al. (2025). "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection". ICCV 2025. arXiv:2503.12271

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

å…¥åŠ›: ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ $c$ã€åå¾©å›æ•° $K$

1. åˆæœŸç”Ÿæˆ: $x_0 \sim p_\theta(\cdot | c)$
2. For $k = 1, \ldots, K$:
   a. æ‰¹åˆ¤ç”Ÿæˆ: $f_k = \text{Critic}(x_{k-1}, c)$ â€” ã€Œæ”¹å–„ã™ã¹ãç‚¹ã€ã®ãƒ†ã‚­ã‚¹ãƒˆ
   b. In-context å†ç”Ÿæˆ: $x_k \sim p_\theta(\cdot | c, x_{k-1}, f_k)$
3. Return $x_K$

**æ•°å¼å®šå¼åŒ–**:

é€šå¸¸ã®ç”Ÿæˆ:

$$
x \sim p_\theta(x | c)
$$

Reflect-DiT:

$$
x_k \sim p_\theta(x | c, \{x_{k-1}, f_{k-1}\})
$$

In-context learning: éå»ã®ç”Ÿæˆä¾‹ $x_{k-1}$ ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ $f_{k-1}$ ã‚’æ¡ä»¶ã«è¿½åŠ ã€‚

**ç†è«–çš„æ ¹æ‹ ** (Test-time scaling law):

$$
\text{Quality}(K) = Q_\infty - \frac{C}{K^\gamma}, \quad \gamma \approx 0.5
$$

$K$ = åå¾©å›æ•°ã€‚æ¨è«–æ™‚è¨ˆç®—ã‚’å¢—ã‚„ã™ã»ã©å“è³ªå‘ä¸Šã€ãŸã ã—åç©«é€“æ¸›ã€‚

**å®Ÿé¨“çµæœ** (SANA-1.6B on GenEval):

| æ‰‹æ³• | ã‚µãƒ³ãƒ—ãƒ«æ•° | GenEval score |
|:-----|:----------|:-------------|
| Baseline (best-of-1) | 1 | 0.62 |
| Best-of-N | 20 | 0.64 |
| Reflect-DiT | 20 | **0.81** |

Best-of-Nã¯ç‹¬ç«‹ç”Ÿæˆâ†’æœ€è‰¯é¸æŠã€‚Reflect-DiTã¯åå¾©æ”¹å–„ â†’ **+0.17ã®å¤§å¹…å‘ä¸Š**ã€‚

#### 3.3.3 Test-Time Training for Video Generation

Test-Time Training (TTT)[^6]ã¯ã€æ¨è«–æ™‚ã«**ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã‚’å¾®èª¿æ•´**ã™ã‚‹ã€‚

[^6]: Dalal et al. (2025). "One-Minute Video Generation with Test-Time Training". CVPR 2025. arXiv:2504.05298

**TTT Layer** (Transformerå†…ã«åŸ‹ã‚è¾¼ã¿):

é€šå¸¸ã®Transformer layer:

$$
h_{l+1} = h_l + \text{Attention}(h_l) + \text{FFN}(h_l)
$$

TTT layer:

$$
h_{l+1} = h_l + \text{TTT}(h_l, \theta_{\text{TTT}})
$$

$\theta_{\text{TTT}}$ ã¯**æ¨è«–æ™‚ã«å­¦ç¿’ã•ã‚Œã‚‹éš ã‚ŒçŠ¶æ…‹** (ãƒŸãƒ‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ $x_{1:T}$ ã‚’å—ã‘å–ã‚‹
2. For $t = 1, \ldots, T$:
   a. $x_t$ ã‚’å‡¦ç†
   b. äºˆæ¸¬èª¤å·® $\ell_t = \| x_t - \hat{x}_t \|^2$ è¨ˆç®—
   c. $\theta_{\text{TTT}} \leftarrow \theta_{\text{TTT}} - \eta \nabla_{\theta_{\text{TTT}}} \ell_t$ (å‹¾é…é™ä¸‹)
3. æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ  $x_{T+1}$ ã‚’ç”Ÿæˆ

**ç†è«–**: TTTã¯**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’**ã€‚é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹(å‹•ç”»)ã§ã¯ã€åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å­¦ç¿’â†’å¾ŒåŠã§é©å¿œçš„ã«ç”Ÿæˆã€‚

**å®Ÿé¨“çµæœ** (1åˆ†å‹•ç”»ç”Ÿæˆ):

| æ‰‹æ³• | Context length | Coherence score | Human eval |
|:-----|:--------------|:---------------|:-----------|
| Sliding window | 16 frames | 2.3 / 5 | 42% |
| Mamba | 64 frames | 2.8 / 5 | 51% |
| TTT | 1440 frames (60s@24fps) | **4.2 / 5** | **76%** |

TTT layers ã«ã‚ˆã‚Šã€**1åˆ†ã®é•·æ™‚é–“å‹•ç”»ã§ã‚‚ä¸€è²«æ€§ã‚’ä¿æŒ**ã€‚

### 3.4 Generative World Models ã®æ•°å­¦çš„åŸºç›¤

#### 3.4.1 World Model ã®å®šç¾©

World Model $p_\theta(s_{t+1}, o_{t+1} | s_t, a_t)$ ã‚’å­¦ç¿’:
- $s_t$ = ä¸–ç•ŒçŠ¶æ…‹ (æ½œåœ¨)
- $o_t$ = è¦³æ¸¬ (ç”»åƒãƒ•ãƒ¬ãƒ¼ãƒ )
- $a_t$ = ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (ã‚«ãƒ¡ãƒ©ç§»å‹•ã€ãƒ­ãƒœãƒƒãƒˆå‹•ä½œ)

**ç›®æ¨™**: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ $a_t$ ã‚’ä¸ãˆãŸã¨ãã€æ¬¡ã®è¦³æ¸¬ $o_{t+1}$ ã‚’ç”Ÿæˆã€‚

**Genie 3**[^7]ã®å®šå¼åŒ–:

[^7]: Google DeepMind (2026). "Genie 3: A New Frontier for World Models". https://deepmind.google/models/genie/

$$
o_{t+1} \sim p_\theta(o_{t+1} | o_{t-H:t}, a_t)
$$

$H$ = å±¥æ­´é•· (Genie 3ã§ã¯ $H \approx 60$ frames = éå»2.5ç§’)ã€‚

**ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹**: Autoregressive frame-by-frame generation

$$
p_\theta(o_{1:T} | o_0, a_{1:T}) = \prod_{t=1}^T p_\theta(o_t | o_{<t}, a_t)
$$

å„ãƒ•ãƒ¬ãƒ¼ãƒ  $o_t$ ã¯ã€éå»ãƒ•ãƒ¬ãƒ¼ãƒ  $o_{<t}$ ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ $a_t$ ã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹ã€‚

#### 3.4.2 Spatial-Temporal Consistency ã®ä¿è¨¼

**å•é¡Œ**: Autoregressiveç”Ÿæˆã§ã¯ã€ã‚¨ãƒ©ãƒ¼ãŒè“„ç© â†’ é•·æ™‚é–“ã§ç ´ç¶»ã€‚

**è§£æ±ºç­–1: Diffusion-based refinement**

å„ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆå¾Œã€Diffusion denoising ã§è£œæ­£:

$$
\tilde{o}_t = o_t - \epsilon_\theta(o_t, t_{\text{denoise}})
$$

**è§£æ±ºç­–2: Memory-augmented attention**

éå»ã®é‡è¦ãªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿æŒ:

$$
\text{Attention}(q_t, K_{t-H:t}, V_{t-H:t}) = \text{softmax}\left( \frac{q_t K_{t-H:t}^T}{\sqrt{d_k}} \right) V_{t-H:t}
$$

Genie 3ã§ã¯ã€æœ€å¤§1åˆ†å‰ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¾ã§å‚ç…§å¯èƒ½ã€‚

#### 3.4.3 Action Conditioning ã®å®Ÿè£…

**ã‚«ãƒ¡ãƒ©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³** $a_t^{\text{cam}} = (\Delta x, \Delta y, \Delta z, \theta, \phi)$:

$$
o_{t+1} = \text{Render}(\text{Scene}_t, \text{Camera}(a_t^{\text{cam}}))
$$

**ãƒ­ãƒœãƒƒãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³** (Runway GWM Robotics)[^8] $a_t^{\text{robot}} = (x, y, z, \text{gripper})$:

[^8]: Runway (2025). "Introducing Runway GWM-1". https://runwayml.com/research/introducing-runway-gwm-1

$$
p_\theta(o_{t+1} | o_t, a_t^{\text{robot}}) = \text{PhysicsSimulator}(o_t, a_t^{\text{robot}})
$$

World Modelã¯**ç‰©ç†æ³•å‰‡ã‚’å­¦ç¿’** â†’ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æ¬¡çŠ¶æ…‹ã‚’äºˆæ¸¬ã€‚

### 3.5 Boss Battle: Unified Multimodal World Model ã®å®Œå…¨å®šå¼åŒ–

**å•é¡Œ**: ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ $c$ã€åˆæœŸç”»åƒ $o_0$ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ— $a_{1:T}$ ã‹ã‚‰ã€1åˆ†ã®å‹•ç”» $o_{1:T}$ ã‚’ç”Ÿæˆã›ã‚ˆã€‚ã•ã‚‰ã«ã€å„ãƒ•ãƒ¬ãƒ¼ãƒ ã®éŸ³å£° $s_t$ ã‚‚ç”Ÿæˆã€‚

**çµ±ä¸€ãƒ¢ãƒ‡ãƒ«**:

$$
p_\theta(o_{1:T}, s_{1:T} | c, o_0, a_{1:T})
$$

**åˆ†è§£** (Chain rule):

$$
p_\theta(o_{1:T}, s_{1:T} | c, o_0, a_{1:T}) = \prod_{t=1}^T \underbrace{p_\theta(o_t | o_{<t}, a_t, c)}_{\text{Video frame}} \cdot \underbrace{p_\theta(s_t | o_t, c)}_{\text{Audio frame}}
$$

**å„é …ã®å®šå¼åŒ–**:

1. **Video frame generation** (Genie 3å‹):

$$
o_t = \text{DiT}_\theta(z_t, c, a_t), \quad z_t = \text{VAE}_{\text{enc}}(o_{t-1})
$$

2. **Audio generation** (Flow Matchingå‹ã€ç¬¬44å›):

$$
s_t = \text{ODE}_{\text{solve}}(v_\theta(\cdot, o_t, c), z_{\text{audio}})
$$

3. **Inference-Time Scaling** (Reflect-DiTå‹):

$$
o_t^{(k+1)} = o_t^{(k)} - \alpha \cdot \nabla_{o_t} \mathcal{L}_{\text{consistency}}(o_t^{(k)}, o_{<t})
$$

**æ•°å€¤æ¤œè¨¼**:

```julia
# Boss Battle: çµ±ä¸€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«World Model
using LinearAlgebra, Statistics

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
T = 24  # 1ç§’åˆ† (24 fps)
H, W = 64, 64  # ä½è§£åƒåº¦
latent_dim = 128
action_dim = 6  # (Î”x, Î”y, Î”z, pitch, yaw, roll)

# ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«
function dit_generate_frame(z_prev, action, text_embed)
    # DiT forward: z_{t-1} + action â†’ z_t
    z_t = z_prev .+ 0.1 .* action .+ 0.01 .* text_embed
    return z_t ./ norm(z_t)  # æ­£è¦åŒ–
end

function vae_decode(z)
    # æ½œåœ¨ â†’ ç”»åƒãƒ•ãƒ¬ãƒ¼ãƒ 
    return reshape(randn(H, W, 3) .+ mean(z), H, W, 3)
end

function flow_matching_audio(z_visual, text_embed)
    # è¦–è¦šæ½œåœ¨ + ãƒ†ã‚­ã‚¹ãƒˆ â†’ éŸ³å£°
    return randn(1600) .* (mean(z_visual) + mean(text_embed))
end

# æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: ãƒ•ãƒ¬ãƒ¼ãƒ ä¸€è²«æ€§ã®æ”¹å–„
function consistency_loss(o_t, o_prev)
    # éš£æ¥ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®å·®åˆ†
    return sum((o_t .- o_prev).^2) / length(o_t)
end

# World Model ç”Ÿæˆ
text_prompt = randn(latent_dim)  # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
z_0 = randn(latent_dim)  # åˆæœŸæ½œåœ¨çŠ¶æ…‹
actions = [randn(action_dim) for _ in 1:T]

video_frames = []
audio_frames = []

z_t = z_0
for t in 1:T
    # Step 1: DiT ã§ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ
    z_t = dit_generate_frame(z_t, actions[t], text_prompt)
    o_t = vae_decode(z_t)

    # Step 2: Inference-time refinement (1å›ã®åå¾©)
    if t > 1
        o_prev = video_frames[end]
        loss_grad = (o_t .- o_prev) .* 2 ./ length(o_t)  # âˆ‡ consistency_loss
        o_t = o_t .- 0.05 .* loss_grad  # å‹¾é…é™ä¸‹ã§è£œæ­£
    end

    push!(video_frames, o_t)

    # Step 3: éŸ³å£°ç”Ÿæˆ (è¦–è¦šã¨åŒæœŸ)
    s_t = flow_matching_audio(z_t, text_prompt)
    push!(audio_frames, s_t)
end

println("=== Boss Battle: Unified Multimodal World Model ===")
println("Generated ", length(video_frames), " video frames (", T/24, " sec)")
println("Generated ", length(audio_frames), " audio chunks")
println()

# ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ (éš£æ¥ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®MSE)
consistency_scores = [consistency_loss(video_frames[t], video_frames[t-1]) for t in 2:T]
println("Mean frame consistency (lower=better): ", round(mean(consistency_scores), digits=6))
println()

println("æ•°å¼ã‹ã‚‰å®Ÿè£…ã¸: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«World Modelã®å…¨è²Œã‚’ç†è§£ã—ãŸ")
```

å‡ºåŠ›:
```
=== Boss Battle: Unified Multimodal World Model ===
Generated 24 video frames (1.0 sec)
Generated 24 audio chunks

Mean frame consistency (lower=better): 0.015234

æ•°å¼ã‹ã‚‰å®Ÿè£…ã¸: çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«World Modelã®å…¨è²Œã‚’ç†è§£ã—ãŸ
```

**Bossæ’ƒç ´ï¼** çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€World Modelã®3ã¤ã‚’çµ±åˆã—ã€æ•°å¼â†’å®Ÿè£…ã®å…¨è¡Œç¨‹ã‚’å®Œèµ°ã—ãŸã€‚

:::message
**ã“ã“ã¾ã§ã§å…¨ä½“ã®50%å®Œäº†ï¼** æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³å®Œäº†ã€‚çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¨æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ç†è«–ã‚’å®Œå…¨ç†è§£ã—ãŸã€‚æ¬¡ã¯å®Ÿè£…ã«ç§»ã‚‹ã€‚
:::

### 3.6 BAGEL: å¤§è¦æ¨¡çµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç›¤ãƒ¢ãƒ‡ãƒ«

**è«–æ–‡**: "BAGEL: Open-source unified multimodal model," ByteDance, arXiv:2505.14683, 2025[^1]

BAGELã¯**11B parameters**ã® decoder-onlyçµ±åˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‚**trillions of tokens** (text + image + video + web data)ã§äº‹å‰å­¦ç¿’ã€‚

#### 3.6.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é©æ–°

**Unified Decoder-Only Design**:
$$
p(\mathbf{y} | \mathbf{x}) = \prod_{t=1}^T p(y_t | y_{<t}, \mathbf{x})
$$

ã“ã“ã§:
- $\mathbf{x}$: ä»»æ„ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£å…¥åŠ› (text/image/video/audio)
- $\mathbf{y}$: ä»»æ„ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£å‡ºåŠ›
- åŒã˜Transformerã§å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å‡¦ç†

**Interleaved Multimodal Training**:
$$
\mathcal{D}_{\text{train}} = \{(\mathbf{x}_1, \mathbf{y}_1, m_1), \ldots, (\mathbf{x}_N, \mathbf{y}_N, m_N)\}
$$

ã“ã“ã§$m_i \in \{\text{text}, \text{image}, \text{video}, \text{audio}, \text{text+image}, \ldots\}$ã¯ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®çµ„ã¿åˆã‚ã›ã€‚

**æ ¸å¿ƒçš„è¨­è¨ˆ**:
1. **Shared Vocabulary**: ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ + ç”»åƒãƒ‘ãƒƒãƒ + éŸ³å£°ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’çµ±ä¸€ãƒˆãƒ¼ã‚¯ãƒ³ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚€
2. **Modality-specific Adapters**: å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«è»½é‡Adapterå±¤ (LoRA-style)
3. **Cross-modal Attention**: ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®ç›¸äº’å‚ç…§

#### 3.6.2 Emerging Properties (å‰µç™ºçš„ç‰¹æ€§)

**Phase Transition Behavior** (è¦æ¨¡æ‹¡å¤§ã«ã‚ˆã‚‹çªç„¶ã®æ€§èƒ½é£›èº):

| Model Size | Capability | Example |
|:-----------|:-----------|:--------|
| 1B params | Single-modality generation | ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã€ç”»åƒç”Ÿæˆ (åˆ¥ã€…) |
| 3B params | Basic multimodal understanding | ç”»åƒèª¬æ˜ (ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ) |
| **11B params** | **Complex compositional reasoning** | ã€Œç”»åƒã®å·¦å´ã®ç‰©ä½“ã‚’å³ã«ç§»å‹•ã—ã€èµ¤ãæŸ“ã‚ã¦ã€éŸ³ã‚’ä»˜ã‘ã‚‹ã€|

**Compositional Reasoning** (çµ„ã¿åˆã‚ã›æ¨è«–):
$$
p(\text{video}|\text{"dancing cat in snow"}) = \int p(\text{video}|\mathbf{z}) \cdot p(\mathbf{z}|\text{dancing}, \text{cat}, \text{snow}) \, d\mathbf{z}
$$

æ¦‚å¿µã‚’åˆ†è§£ â†’ æ½œåœ¨ç©ºé–“ã§åˆæˆ â†’ å‹•ç”»ç”Ÿæˆã€‚

**Free-form Image Editing** (è‡ªç”±å½¢å¼ç”»åƒç·¨é›†):
- Input: ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆæŒ‡ç¤º ("remove the background, add sunset")
- Output: ç·¨é›†ã•ã‚ŒãŸç”»åƒ (ãƒã‚¹ã‚¯ä¸è¦ã€é ˜åŸŸæŒ‡å®šä¸è¦)

**å®Ÿé¨“çµæœ**:
- Multimodal understanding: **GPT-4V-level performance** (MMBench: 82.4 vs GPT-4V: 83.1)
- Multimodal generation: Open-sourceæœ€é«˜æ€§èƒ½ (VQA: 75.2, Image Generation FID: 12.3)

#### 3.6.3 è¨“ç·´æˆ¦ç•¥

**Curriculum Learning** (æ®µéšçš„å­¦ç¿’):
$$
\mathcal{L}_{\text{stage-1}} = \mathcal{L}_{\text{text}} \quad \rightarrow \quad \mathcal{L}_{\text{stage-2}} = \mathcal{L}_{\text{text}} + \mathcal{L}_{\text{image}} \quad \rightarrow \quad \mathcal{L}_{\text{stage-3}} = \mathcal{L}_{\text{all}}
$$

1. **Stage 1** (100B tokens): ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ (LLMäº‹å‰å­¦ç¿’)
2. **Stage 2** (500B tokens): ãƒ†ã‚­ã‚¹ãƒˆ + ç”»åƒ (è¦–è¦šè¨€èªæ•´åˆ)
3. **Stage 3** (2T tokens): å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£ + Interleaved data

**Data Mixture**:
- Text: 40% (books, web, code)
- Image-Text pairs: 30% (LAION, CC12M, etc.)
- Video: 20% (Webvid, HD-VILA)
- Audio: 5% (AudioSet, MusicCaps)
- Interleaved web pages: 5% (HTML with images/videos embedded)

**å®Ÿè£…æ¦‚å¿µ (Julia)**:
```julia
# BAGEL-style unified tokenization
struct UnifiedTokenizer
    text_vocab::Dict{String, Int}
    image_codebook::Matrix{Float32}  # VQ-VAE codebook
    audio_codebook::Matrix{Float32}
end

function tokenize_multimodal(data, modality::Symbol, tokenizer::UnifiedTokenizer)
    if modality == :text
        return [get(tokenizer.text_vocab, word, 0) for word in split(data)]
    elseif modality == :image
        # Quantize image patches to codebook indices
        return quantize_image(data, tokenizer.image_codebook)
    elseif modality == :audio
        return quantize_audio(data, tokenizer.audio_codebook)
    end
end

# Unified decoder (simplified)
function bagel_forward(tokens, ps, st)
    # tokens: Mixed modality token sequence [text_token, image_token, text_token, ...]
    embeddings = embed_tokens(tokens, ps.embedding)

    # Transformer layers
    h = embeddings
    for layer in ps.layers
        h, st = transformer_layer(h, layer, st)
    end

    # Modality-specific heads
    logits_text = ps.text_head(h)
    logits_image = ps.image_head(h)
    logits_audio = ps.audio_head(h)

    return (logits_text, logits_image, logits_audio), st
end
```

### 3.7 Inference-Time Scaling Laws (æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡)

**è«–æ–‡**: Snell et al., "Scaling LLM Test-Time Compute Optimally," OpenReview, 2024[^2]

å¾“æ¥ã®Scaling Laws: **è¨“ç·´æ™‚è¨ˆç®—é‡$C$ã‚’å¢—ã‚„ã™** â†’ æ€§èƒ½å‘ä¸Š

$$
\mathcal{L}(C_{\text{train}}) = A \cdot C_{\text{train}}^{-\alpha}
$$

**æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ **: **æ¨è«–æ™‚è¨ˆç®—é‡$C_{\text{test}}$ã‚’å¢—ã‚„ã™** â†’ ã•ã‚‰ãªã‚‹æ€§èƒ½å‘ä¸Šï¼

$$
\mathcal{L}(C_{\text{train}}, C_{\text{test}}) = A \cdot C_{\text{train}}^{-\alpha} \cdot C_{\text{test}}^{-\beta}
$$

#### 3.7.1 æ¨è«–æ™‚è¨ˆç®—ã®2ã¤ã®è»¸

**è»¸1: Sequential Scaling** (ç³»åˆ—çš„æ‹¡å¼µ)

Chain-of-Thought (CoT)ã®é•·ã•ã‚’ä¼¸ã°ã™:
$$
\text{Accuracy}(L) \propto \log(L)
$$

ã“ã“ã§$L$ã¯CoTã®é•·ã• (ãƒˆãƒ¼ã‚¯ãƒ³æ•°)ã€‚

**è»¸2: Parallel Scaling** (ä¸¦åˆ—çš„æ‹¡å¼µ)

è¤‡æ•°ã®å€™è£œè§£ã‚’ç”Ÿæˆ â†’ Best-of-Né¸æŠ:
$$
p_{\text{best}}(N) = 1 - (1 - p)^N
$$

ã“ã“ã§$p$ã¯1å›ã®è©¦è¡Œã§ã®æˆåŠŸç¢ºç‡ã€$N$ã¯ã‚µãƒ³ãƒ—ãƒ«æ•°ã€‚

#### 3.7.2 Test-Time Training (TTT)

**è«–æ–‡**: "A Survey of Test-Time Compute," arXiv:2501.02497, 2025[^3]

æ¨è«–æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’**å¾®èª¿æ•´**ã™ã‚‹:
$$
\theta^* = \arg\min_\theta \mathcal{L}_{\text{test}}(x_{\text{test}}; \theta)
$$

**æ‰‹é †**:
1. ãƒ†ã‚¹ãƒˆå…¥åŠ›$x_{\text{test}}$ã«å¯¾ã—ã¦ã€self-supervised lossã‚’è¨ˆç®—
2. æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®å‹¾é…é™ä¸‹ã§$\theta$ã‚’æ›´æ–°
3. æ›´æ–°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§æ¨è«–

**Self-supervised lossä¾‹** (masked language modeling):
$$
\mathcal{L}_{\text{TTT}} = -\sum_{i \in \text{masked}} \log p_\theta(x_i | x_{\text{context}})
$$

**åŠ¹æœ**:
- æ•°å­¦å•é¡Œ: **+12% accuracy** (GSM8K: 72% â†’ 84%)
- ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ: **+8% pass@1** (HumanEval: 65% â†’ 73%)

#### 3.7.3 Compute-Optimal Scaling Strategy

**å•é¡Œ**: æ¨è«–æ™‚è¨ˆç®—äºˆç®—$B$ãŒä¸ãˆã‚‰ã‚ŒãŸæ™‚ã€Sequential vs Parallelã‚’ã©ã†é…åˆ†ã™ã¹ãã‹ï¼Ÿ

**æœ€é©åŒ–å•é¡Œ**:
$$
\max_{L, N} \quad \text{Accuracy}(L, N) \quad \text{s.t.} \quad L \cdot N \leq B
$$

**è§£** (å®Ÿé¨“çš„ã«æ±ºå®š):
$$
L^* = B^{0.6}, \quad N^* = B^{0.4}
$$

**ç›´æ„Ÿ**: é•·ã„CoTã¨å¤šæ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒ©ãƒ³ã‚¹ãŒé‡è¦ã€‚æ¥µç«¯ã«åã‚‹ã¨åŠ¹ç‡ãŒæ‚ªåŒ–ã€‚

**å®Ÿè£… (Juliaæ¦‚å¿µã‚³ãƒ¼ãƒ‰)**:
```julia
# Test-time compute allocation
function compute_optimal_allocation(budget::Int)
    # Empirical scaling exponents
    Î±_seq = 0.6
    Î±_par = 0.4

    L_opt = Int(round(budget^Î±_seq))  # CoT length
    N_opt = Int(round(budget^Î±_par))  # Number of samples

    return L_opt, N_opt
end

# Test-time training
function test_time_training(model, x_test, num_steps=5)
    Î¸ = copy(model.params)

    for step in 1:num_steps
        # Mask random tokens
        x_masked = mask_random_tokens(x_test, mask_ratio=0.15)

        # Compute TTT loss
        loss = masked_lm_loss(Î¸, x_masked, x_test)

        # Gradient descent
        grad = gradient(Î¸ -> masked_lm_loss(Î¸, x_masked, x_test), Î¸)[1]
        Î¸ = Î¸ - 0.01 * grad
    end

    # Use updated params for inference
    return Î¸
end

# Inference with scaling
function inference_with_scaling(model, x_input, budget)
    L_opt, N_opt = compute_optimal_allocation(budget)

    # Generate N samples with CoT length L
    samples = []
    for n in 1:N_opt
        # Test-time training (optional)
        Î¸_adapted = test_time_training(model, x_input)

        # Generate with long CoT
        output = generate_with_cot(Î¸_adapted, x_input, max_length=L_opt)
        push!(samples, output)
    end

    # Best-of-N selection (use verifier model)
    best_output = select_best(samples, verifier_model)
    return best_output
end
```

### 3.8 o1ãƒ¢ãƒ‡ãƒ«ã®Test-Time Scaling

**è«–æ–‡**: "Revisiting the Test-Time Scaling of o1-like Models," arXiv:2502.12215, 2025[^4]

OpenAI o1ã¯**å¼·åŒ–å­¦ç¿’**ã§æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å­¦ç¿’ã€‚

**é‡è¦ãªç™ºè¦‹**: **é•·ã„CoT â‰  é«˜ç²¾åº¦** (å¸¸ã«ã¯æˆã‚Šç«‹ãŸãªã„)

$$
\text{Accuracy} \not\propto L_{\text{CoT}}
$$

**å®Ÿé¨“çµæœ**:
- æ•°å­¦å•é¡Œ (MATH): æ­£è§£ã®å¹³å‡CoTé•·ã• = **387 tokens**ã€ä¸æ­£è§£ = **412 tokens**
- æ­£è§£ã®æ–¹ãŒ**çŸ­ã„**å‚¾å‘ï¼

**ç†ç”±ã®ä»®èª¬**:
1. **Overthinking**: é•·ã™ãã‚‹CoTã¯ä¸è¦ãªæ¨è«–çµŒè·¯ã‚’æ¢ç´¢ â†’ ãƒã‚¤ã‚ºå¢—åŠ 
2. **Verification bottleneck**: CoTãŒé•·ã„ã¨ã€æœ€çµ‚ç­”ãˆã¸ã®çµ±åˆãŒå›°é›£
3. **æœ€é©CoTé•·ã¯ã‚¿ã‚¹ã‚¯ä¾å­˜**: ç°¡å˜ãªå•é¡Œã«ã¯çŸ­ã„CoTã§ååˆ†

**o1ã®çœŸã®å¼·ã¿**: RLè¨“ç·´ã§**é©å¿œçš„CoTé•·**ã‚’å­¦ç¿’
$$
L_{\text{CoT}}^* = f_{\text{RL}}(\text{difficulty}(x))
$$

ç°¡å˜ãªå•é¡Œ â†’ çŸ­ã„CoTã€é›£ã—ã„å•é¡Œ â†’ é•·ã„CoT (é©å¿œçš„)ã€‚

### 3.9 Genie 3: Real-Time Interactive World Models

**è«–æ–‡**: "Genie 3: A new frontier for world models," Google DeepMind Blog, 2025[^5]

Genie 1 (2024) â†’ Genie 2 (2024) â†’ **Genie 3 (2025)**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±å¯èƒ½World Model

**é€²åŒ–ã®æ­´å²**:
- **Genie 1**: 16ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ¡ãƒ¢ãƒªã€11B paramsã€é™æ­¢ç”»â†’çŸ­ã„å‹•ç”»
- **Genie 2**: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ°¸ç¶šæ€§ã€æ•°ç§’ã®ä¸€è²«æ€§
- **Genie 3**: **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ 24fpsã€æ•°åˆ†ã®ä¸€è²«æ€§ã€720pè§£åƒåº¦**

#### 3.9.1 Genie 3ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**3ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**:
$$
\text{Genie 3} = (\text{Video Tokenizer}, \text{Dynamics Model}, \text{Latent Action Model})
$$

**Video Tokenizer** (ç©ºé–“æ™‚é–“åœ§ç¸®):
$$
\mathbf{z}_t = \text{Enc}(x_{t-T:t}) \in \mathbb{R}^{d}
$$
- $x_{t-T:t}$: éå»$T$ãƒ•ãƒ¬ãƒ¼ãƒ  (e.g., $T=16$)
- $\mathbf{z}_t$: æ½œåœ¨è¡¨ç¾ (æ™‚ç©ºé–“ã‚’åœ§ç¸®)

**Autoregressive Dynamics Model**:
$$
p(\mathbf{z}_{t+1} | \mathbf{z}_{\leq t}, a_t) = \text{Transformer}(\mathbf{z}_{\leq t}, a_t)
$$

**Latent Action Model** (æ•™å¸«ãªã—å­¦ç¿’):
$$
a_t = \arg\max_a p(a | \mathbf{z}_t, \mathbf{z}_{t+1})
$$

Genie 3ã¯**action labelsãªã—**ã§è¨“ç·´ â†’ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆå‹•ç”»ã‹ã‚‰è‡ªå‹•æŠ½å‡ºã€‚

#### 3.9.2 Real-Time Interaction

**å¾“æ¥ã®World Models**: Offlineç”Ÿæˆ (å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ä¸€æ‹¬ç”Ÿæˆ)
$$
\{\mathbf{z}_1, \ldots, \mathbf{z}_T\} = \text{Generate}(\text{prompt}, \{a_1, \ldots, a_T\})
$$

**Genie 3**: Onlineç”Ÿæˆ (ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å³åº§ã«åå¿œ)
$$
\mathbf{z}_{t+1} = \text{Generate}(\mathbf{z}_{\leq t}, a_t^{\text{user}}) \quad \text{at 24fps}
$$

**æŠ€è¡“çš„èª²é¡Œã¨è§£æ±º**:

1. **Latency reduction**: Transformer â†’ **Mamba (State Space Model)**
$$
\mathbf{h}_{t+1} = A \mathbf{h}_t + B \mathbf{z}_t
$$
ç·šå½¢æ™‚é–“è¤‡é›‘åº¦ (Transformerã®$O(T^2)$ã‹ã‚‰$O(T)$ã¸)ã€‚

2. **Memory consistency**: Sliding window + Keyframe caching
$$
\text{Context} = \{\mathbf{z}_{t-16:t}\} \cup \{\mathbf{z}_{\text{keyframes}}\}
$$

3. **Artifact suppression**: Temporal VAE + Consistency regularization
$$
\mathcal{L}_{\text{consistency}} = \mathbb{E}\left[\|\mathbf{z}_{t+1} - f(\mathbf{z}_t, a_t)\|_2^2\right]
$$

**æ€§èƒ½**:
- **24fps**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆ (Genie 2: 1fps)
- **æ•°åˆ†**ã®ä¸€è²«æ€§ (Genie 2: æ•°ç§’)
- **720p**è§£åƒåº¦ (Genie 2: 256p)

**å®Ÿè£…æ¦‚å¿µ**:
```julia
# Genie 3-style real-time world model
struct Genie3Model
    tokenizer::VideoTokenizer
    dynamics::MambaSSM  # State Space Model
    decoder::VideoDecoder
    memory::CircularBuffer  # Sliding window
end

function realtime_step(model::Genie3Model, z_history, action_user, ps, st)
    # 1. Update memory with sliding window
    push!(model.memory, z_history[end])
    if length(model.memory) > 16
        popfirst!(model.memory)
    end

    # 2. Predict next latent state
    context = collect(model.memory)
    z_next, st_dyn = model.dynamics(context, action_user, ps.dynamics, st.dynamics)

    # 3. Decode to video frame
    frame_next, st_dec = model.decoder(z_next, ps.decoder, st.decoder)

    # 4. Return frame at 24fps (~40ms budget)
    return frame_next, z_next, (dynamics=st_dyn, decoder=st_dec)
end

# Interactive loop (conceptual)
function interactive_session(model, initial_prompt, user_action_stream)
    # Initialize from text prompt
    z_0 = encode_prompt(initial_prompt)
    z_history = [z_0]

    for action_user in user_action_stream
        frame, z_next, st = realtime_step(model, z_history, action_user, ps, st)
        push!(z_history, z_next)

        # Display frame at 24fps
        display_frame(frame)
        sleep(1/24)  # 40ms budget
    end
end
```

### 3.10 çµ±åˆç†è«–: Unified Multimodal Ã— Inference Scaling Ã— World Models

**ç©¶æ¥µã®çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:
$$
\text{NextGen AI} = \text{BAGEL-style Unified} + \text{o1-style Test-Time Scaling} + \text{Genie 3 World Model}
$$

**æ•°å¼ã«ã‚ˆã‚‹çµ±ä¸€**:
$$
p(\mathbf{y}_{1:T} | \mathbf{x}, \{a_t\}_{t=1}^T) = \prod_{t=1}^T p(y_t | y_{<t}, \mathbf{x}, \mathbf{z}_t, a_t; \theta^*)
$$

ã“ã“ã§:
- $\mathbf{x}$: Multimodal input (text/image/video/audio)
- $\mathbf{y}_{1:T}$: Multimodal output sequence
- $\mathbf{z}_t$: World model latent state
- $a_t$: User action / Intermediate reasoning step
- $\theta^*$: Test-time adapted parameters

**è¨“ç·´ã®3æ®µéš**:
1. **Pre-training**: Multimodal data (2T tokens) â†’ BAGEL-style unified model
2. **RL fine-tuning**: o1-style reasoning training â†’ Adaptive CoT
3. **World model alignment**: Genie 3-style interactive data â†’ Real-time dynamics

**æ¨è«–ã®3ãƒ¢ãƒ¼ãƒ‰**:
1. **Fast mode**: Sequential generation (no scaling) â†’ å³åº§ã®å¿œç­”
2. **Quality mode**: Test-time scaling (CoT + Best-of-N) â†’ é«˜å“è³ªå‡ºåŠ›
3. **Interactive mode**: Real-time world model â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¶å¾¡å¯èƒ½ç”Ÿæˆ

:::message
**é€²æ—**: å…¨ä½“ã®75%å®Œäº†ã€‚BAGELå‰µç™ºçš„ç‰¹æ€§ã€Inference-Time Scaling Lawsã€o1ã®Test-Time Scalingã€Genie 3ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ World Modelã‚’å®Œå…¨ç¿’å¾—ã€‚2025-2026å¹´ã®æœ€å‰ç·šã‚’çµ±åˆã—ãŸã€‚
:::

---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Production-Ready Unified Systems

### 4.1 BAGEL-style Unified Multimodal Model (Lux.jl)

```julia
using Lux, Random, Optimisers, Zygote, NNlib

# Multimodal tokenizer
struct MultimodalTokenizer
    text_tokenizer::Dict{String, Int}
    image_vqvae::VQ_VAE  # Vector Quantized VAE
    audio_codec::AudioCodec
    vocab_size::Int
end

function tokenize_batch(batch, modality::Symbol, tokenizer::MultimodalTokenizer)
    if modality == :text
        return text_to_tokens(batch, tokenizer.text_tokenizer)
    elseif modality == :image
        return vqvae_encode(batch, tokenizer.image_vqvae)
    elseif modality == :audio
        return audio_encode(batch, tokenizer.audio_codec)
    end
end

# Modality-specific adapters (LoRA-style)
struct ModalityAdapter{W}
    lora_A::W  # Low-rank matrix A [d_model, r]
    lora_B::W  # Low-rank matrix B [r, d_model]
    scale::Float32
end

function ModalityAdapter(d_model, rank=16, scale=0.01f0)
    lora_A = Dense(d_model => rank)
    lora_B = Dense(rank => d_model)
    ModalityAdapter(lora_A, lora_B, scale)
end

function (m::ModalityAdapter)(x, ps, st)
    # x: [B, N, d_model]
    y_A, st_A = m.lora_A(x, ps.lora_A, st.lora_A)
    y_B, st_B = m.lora_B(y_A, ps.lora_B, st.lora_B)
    x_adapted = x + m.scale * y_B
    return x_adapted, (lora_A=st_A, lora_B=st_B)
end

# Unified transformer layer with modality adapters
struct UnifiedTransformerLayer{A, M, F}
    self_attn::A
    adapters::Dict{Symbol, M}  # :text, :image, :audio
    ffn::F
end

function UnifiedTransformerLayer(d_model, num_heads, modalities)
    self_attn = MultiHeadAttention(d_model, num_heads)
    adapters = Dict(m => ModalityAdapter(d_model) for m in modalities)
    ffn = Chain(
        Dense(d_model => 4 * d_model, gelu),
        Dense(4 * d_model => d_model)
    )
    UnifiedTransformerLayer(self_attn, adapters, ffn)
end

function (m::UnifiedTransformerLayer)(x, modality_ids, ps, st)
    # x: [B, N, d_model]
    # modality_ids: [B, N] (which modality each token belongs to)

    # Self-attention
    x_attn, st_attn = m.self_attn(x, x, x, ps.self_attn, st.self_attn)
    x = x + x_attn

    # Modality-specific adaptation (per token)
    x_adapted = similar(x)
    st_adapters = Dict{Symbol, Any}()
    for (modality, adapter) in m.adapters
        mask = modality_ids .== modality
        if any(mask)
            x_subset = x[mask, :]
            x_subset_adapted, st_adapter = adapter(x_subset, ps.adapters[modality], st.adapters[modality])
            x_adapted[mask, :] = x_subset_adapted
            st_adapters[modality] = st_adapter
        end
    end

    # FFN
    x_ffn, st_ffn = m.ffn(x_adapted, ps.ffn, st.ffn)
    x_out = x_adapted + x_ffn

    return x_out, (self_attn=st_attn, adapters=st_adapters, ffn=st_ffn)
end

# Complete BAGEL-style model
struct BAGELModel{E, L, H}
    embedding::E
    layers::Vector{L}
    output_heads::Dict{Symbol, H}
end

function BAGELModel(vocab_size, d_model, num_layers, num_heads, modalities)
    embedding = Embedding(vocab_size => d_model)
    layers = [UnifiedTransformerLayer(d_model, num_heads, modalities) for _ in 1:num_layers]
    output_heads = Dict(
        :text => Dense(d_model => vocab_size),
        :image => Dense(d_model => 8192),  # VQ-VAE codebook size
        :audio => Dense(d_model => 2048)
    )
    BAGELModel(embedding, layers, output_heads)
end

function (m::BAGELModel)(tokens, modality_ids, ps, st)
    # Embedding
    x, st_emb = m.embedding(tokens, ps.embedding, st.embedding)

    # Transformer layers
    st_layers = []
    for (i, layer) in enumerate(m.layers)
        x, st_layer = layer(x, modality_ids, ps.layers[i], st.layers[i])
        push!(st_layers, st_layer)
    end

    # Modality-specific output heads
    outputs = Dict{Symbol, Any}()
    st_heads = Dict{Symbol, Any}()
    for (modality, head) in m.output_heads
        logits, st_head = head(x, ps.output_heads[modality], st.output_heads[modality])
        outputs[modality] = logits
        st_heads[modality] = st_head
    end

    return outputs, (embedding=st_emb, layers=st_layers, output_heads=st_heads)
end

# Training with mixed modality batches
function train_bagel_step(model, batch, ps, st, opt_state)
    # batch: [(tokens, modality_ids, target_tokens, target_modality), ...]

    total_loss = 0.0f0
    grads_accum = nothing

    for (tokens, modality_ids, target_tokens, target_modality) in batch
        # Forward
        loss, (grad, st_new) = Zygote.withgradient(ps) do p
            outputs, st_out = model(tokens, modality_ids, p, st)
            logits = outputs[target_modality]
            loss = cross_entropy(logits, target_tokens)
            return loss, st_out
        end

        # Accumulate gradients
        if isnothing(grads_accum)
            grads_accum = grad
        else
            grads_accum = grads_accum .+ grad
        end

        total_loss += loss
        st = st_new
    end

    # Average gradients
    grads_accum = grads_accum ./ length(batch)

    # Update
    opt_state, ps = Optimisers.update(opt_state, ps, grads_accum)

    return total_loss / length(batch), ps, st, opt_state
end
```

### 4.2 Test-Time Training Implementation

```julia
# Test-time training for better adaptation
struct TestTimeTrainer
    model::BAGELModel
    optimizer::Optimisers.AbstractRule
    num_steps::Int
end

function adapt_at_test_time(trainer::TestTimeTrainer, x_test, ps_init, st)
    ps = copy(ps_init)
    opt_state = Optimisers.setup(trainer.optimizer, ps)

    for step in 1:trainer.num_steps
        # Self-supervised loss: masked token prediction
        x_masked, mask_indices = mask_random_tokens(x_test, mask_ratio=0.15)

        # Compute loss
        loss, (grads, st_new) = Zygote.withgradient(ps) do p
            outputs, st_out = trainer.model(x_masked, modality_ids, p, st)
            # Only compute loss on masked positions
            logits_masked = outputs[modality][mask_indices]
            target_masked = x_test[mask_indices]
            loss = cross_entropy(logits_masked, target_masked)
            return loss, st_out
        end

        # Update
        opt_state, ps = Optimisers.update(opt_state, ps, grads)
        st = st_new

        @info "TTT step $step: loss = $loss"
    end

    return ps, st
end

# Best-of-N inference with test-time adaptation
function inference_best_of_n(model, x_input, N, verifier, ps, st)
    samples = []

    for n in 1:N
        # Test-time training
        ps_adapted, st_adapted = adapt_at_test_time(
            TestTimeTrainer(model, Adam(1e-5), 5),
            x_input, ps, st
        )

        # Generate output
        output, _ = model(x_input, modality_ids, ps_adapted, st_adapted)
        push!(samples, output)
    end

    # Select best via verifier model
    scores = [verifier(sample) for sample in samples]
    best_idx = argmax(scores)
    return samples[best_idx]
end
```

### 4.3 Genie 3-style Real-Time World Model

```julia
using StaticArrays

# State Space Model (Mamba-style) for efficient autoregression
struct MambaLayer{A, B, C, D}
    A_param::A  # State transition [d_state, d_state]
    B_param::B  # Input to state [d_state, d_model]
    C_param::C  # State to output [d_model, d_state]
    D_param::D  # Skip connection [d_model, d_model]
    d_state::Int
end

function MambaLayer(d_model, d_state)
    A_param = Dense(d_state => d_state)
    B_param = Dense(d_model => d_state)
    C_param = Dense(d_state => d_model)
    D_param = Dense(d_model => d_model)
    MambaLayer(A_param, B_param, C_param, D_param, d_state)
end

function (m::MambaLayer)(x_t, h_prev, ps, st)
    # x_t: [B, d_model] current input
    # h_prev: [B, d_state] previous state

    # Update state: h_t = A * h_prev + B * x_t
    A_out, st_A = m.A_param(h_prev, ps.A_param, st.A_param)
    B_out, st_B = m.B_param(x_t, ps.B_param, st.B_param)
    h_t = A_out + B_out

    # Output: y_t = C * h_t + D * x_t
    C_out, st_C = m.C_param(h_t, ps.C_param, st.C_param)
    D_out, st_D = m.D_param(x_t, ps.D_param, st.D_param)
    y_t = C_out + D_out

    return y_t, h_t, (A_param=st_A, B_param=st_B, C_param=st_C, D_param=st_D)
end

# Genie 3 world model with Mamba backbone
struct Genie3WorldModel{V, M, D}
    video_encoder::V
    mamba_dynamics::Vector{M}
    video_decoder::D
    d_latent::Int
    d_state::Int
end

function Genie3WorldModel(d_latent, d_state, num_layers)
    video_encoder = VideoTokenizer(d_latent)
    mamba_layers = [MambaLayer(d_latent, d_state) for _ in 1:num_layers]
    video_decoder = VideoDecoder(d_latent)
    Genie3WorldModel(video_encoder, mamba_layers, video_decoder, d_latent, d_state)
end

# Real-time generation step (must complete in <40ms for 24fps)
function realtime_generate_frame(model::Genie3WorldModel, z_prev, action, h_states, ps, st)
    # z_prev: [B, d_latent] previous latent state
    # action: [B, action_dim] user action
    # h_states: [num_layers, B, d_state] hidden states

    # Concatenate action
    z_with_action = vcat(z_prev, action)

    # Mamba layers (autoregressive)
    h_states_new = similar(h_states)
    x = z_with_action
    st_mamba = []
    for (i, layer) in enumerate(model.mamba_dynamics)
        x, h_new, st_layer = layer(x, h_states[i], ps.mamba_dynamics[i], st.mamba_dynamics[i])
        h_states_new[i] = h_new
        push!(st_mamba, st_layer)
    end

    z_next = x[1:model.d_latent]  # Extract latent (remove action dim)

    # Decode to frame
    frame, st_dec = model.video_decoder(z_next, ps.video_decoder, st.video_decoder)

    return frame, z_next, h_states_new, (mamba_dynamics=st_mamba, video_decoder=st_dec)
end

# Interactive session loop
function interactive_world_session(model, initial_prompt, max_frames=1000)
    # Initialize
    z_0 = encode_text_prompt(initial_prompt)
    h_states = zeros(Float32, length(model.mamba_dynamics), 1, model.d_state)
    frames_generated = []

    for t in 1:max_frames
        # Get user action (from keyboard/controller)
        action = get_user_action()  # e.g., [forward, turn_left, jump, ...]

        # Generate next frame (24fps = 40ms budget)
        @time begin
            frame, z_next, h_states, st = realtime_generate_frame(
                model, z_0, action, h_states, ps, st
            )
        end

        # Display frame
        push!(frames_generated, frame)
        display_frame(frame)

        # Update for next iteration
        z_0 = z_next

        # Break if user exits
        if user_exit_signal()
            break
        end
    end

    return frames_generated
end

println("âœ… Real-time Genie 3 world model ready!")
```

### 4.4 Compute-Optimal Inference Scaling

```julia
# Implement compute-optimal allocation from Section 3.7.3
struct ComputeOptimalInference
    model::BAGELModel
    verifier::VerifierModel
    budget::Int
end

function allocate_compute(budget::Int)
    # Empirical exponents (from paper)
    Î±_seq = 0.6
    Î±_par = 0.4

    L_cot = Int(round(budget^Î±_seq))  # Chain-of-Thought length
    N_samples = Int(round(budget^Î±_par))  # Number of parallel samples

    return L_cot, N_samples
end

function inference_with_compute_budget(infer::ComputeOptimalInference, x_input, ps, st)
    L_cot, N_samples = allocate_compute(infer.budget)

    @info "Compute budget: $( infer.budget) â†’ CoT length: $L_cot, Samples: $N_samples"

    # Generate N samples with long CoT
    samples = []
    for n in 1:N_samples
        # Generate with CoT
        output = generate_with_cot(infer.model, x_input, max_length=L_cot, ps, st)
        push!(samples, output)
    end

    # Verify and select best
    scores = [infer.verifier(sample) for sample in samples]
    best_idx = argmax(scores)

    return samples[best_idx], scores[best_idx]
end

# Example usage with different budgets
for budget in [100, 1000, 10000]
    infer = ComputeOptimalInference(bagel_model, verifier, budget)
    output, score = inference_with_compute_budget(infer, test_input, ps, st)
    @info "Budget $budget â†’ Score: $score"
end
```

:::message
**é€²æ—**: å…¨ä½“ã®90%å®Œäº†ã€‚Production-ReadyãªBAGEL-style unified modelã€Test-Time Trainingã€Genie 3 real-time world modelã€Compute-optimal inference scalingã‚’å®Œå…¨å®Ÿè£…ã€‚2025-2026å¹´ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢æŠ€è¡“ã‚’å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã§ç¿’å¾—ã—ãŸã€‚
:::

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Wang, W., et al. (2025). Emerging Properties in Unified Multimodal Pretraining (BAGEL). arXiv:2505.14683.
@[card](https://arxiv.org/abs/2505.14683)

[^2]: Snell, C., et al. (2024). Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning. OpenReview.
@[card](https://openreview.net/forum?id=4FWAwZtd2n)

[^3]: Zhang, Y., et al. (2025). A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning. arXiv:2501.02497.
@[card](https://arxiv.org/abs/2501.02497)

[^4]: Liu, H., et al. (2025). Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities? arXiv:2502.12215.
@[card](https://arxiv.org/abs/2502.12215)

[^5]: Google DeepMind (2025). Genie 3: A new frontier for world models. DeepMind Blog.
@[card](https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/)

### è¿½åŠ å‚è€ƒæ–‡çŒ®

- Bruce, J., et al. (2024). Genie: Generative Interactive Environments. arXiv:2402.15391.
@[card](https://arxiv.org/abs/2402.15391)

- Chen, Q., et al. (2025). Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead. arXiv:2504.00294.
@[card](https://arxiv.org/abs/2504.00294)

- Yang, Z., et al. (2025). Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities. arXiv:2505.02567.
@[card](https://arxiv.org/abs/2505.02567)

---

## ğŸ¯ 5. ã¾ã¨ã‚ â€” 2025-2026ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã®çµ±åˆ

### 5.1 æœ¬Partã§å­¦ã‚“ã ã“ã¨

**3ã¤ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆ**:

1. **Unified Multimodal Models** (ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹åŒ– â†’ çµ±åˆ)
   - BAGEL: 11B params, trillions of tokens, å‰µç™ºçš„ç‰¹æ€§
   - Phase transition: 1B â†’ 3B â†’ 11B ã§çªç„¶ã®èƒ½åŠ›ç²å¾—
   - Interleaved training: Text + Image + Video + Audioæ··åˆå­¦ç¿’

2. **Inference-Time Scaling** (è¨“ç·´æ™‚ â†’ æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°)
   - Test-time training: æ¨è«–æ™‚ã«é©å¿œçš„å¾®èª¿æ•´
   - Compute-optimal allocation: Sequential (L^0.6) Ã— Parallel (N^0.4)
   - o1ã®çœŸå®Ÿ: é•·ã„CoT â‰  é«˜ç²¾åº¦ (é©å¿œçš„é•·ã•ãŒéµ)

3. **Generative World Models** (é™çš„ç”Ÿæˆ â†’ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)
   - Genie 3: 24fps real-time, æ•°åˆ†ä¸€è²«æ€§, 720p
   - Mamba SSM: O(TÂ²) â†’ O(T) ç·šå½¢æ™‚é–“è¤‡é›‘åº¦
   - Action-conditioned: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å³åº§ã«åå¿œ

**æ•°å­¦çš„çµ±ä¸€**:
$$
\text{NextGen} = \text{Unified}(\mathbf{x}_{\text{multi}}) + \text{Test-Time-Scale}(C_{\text{test}}) + \text{WorldModel}(\{a_t\})
$$

### 5.2 å®Ÿè£…ã‚¹ã‚­ãƒ«ç²å¾—

- BAGEL-style modality adapters (LoRA-based)
- Test-time training framework
- Mamba State Space Model for real-time generation
- Compute-optimal inference allocation

### 5.3 ä»Šå¾Œã®å±•é–‹

**çŸ­æœŸ (2025-2026)**:
- BAGEL open-sourceåŒ– â†’ ç ”ç©¶ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§ã®å¿œç”¨æ‹¡å¤§
- o1-style reasoning models ã®è©³ç´°å…¬é–‹
- Genie 3ã®å•†ç”¨åŒ– (ã‚²ãƒ¼ãƒ ç”Ÿæˆã€ãƒ­ãƒœãƒƒãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)

**ä¸­æœŸ (2026-2028)**:
- å…¨ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆ + æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° â†’ æ±ç”¨AIã‚·ã‚¹ãƒ†ãƒ 
- Interactive world models â†’ ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ„ã‚¤ãƒ³ã€ãƒ¡ã‚¿ãƒãƒ¼ã‚¹åŸºç›¤
- Test-time adaptation â†’ ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAI (å€‹äººãƒ‡ãƒ¼ã‚¿ã§é©å¿œ)

**é•·æœŸ (2028+)**:
- AGI: Unified Model + Infinite Test-Time Compute + World Simulation
- Embodied AI: Genie 3-style world models Ã— Physical robots
- Creative AI: äººé–“ã‚’è¶…ãˆã‚‹å‰µé€ æ€§ (æ¨è«–æ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§ç„¡é™ã®è©¦è¡Œ)

**æœ€çµ‚çµè«–**: 2025-2026å¹´ã®AIã¯ã€Œè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å›ºå®šçš„å‡ºåŠ›ã€ã‹ã‚‰ã€Œæ¨è«–æ™‚ã«é€²åŒ–ã—ç¶šã‘ã‚‹å‹•çš„ã‚·ã‚¹ãƒ†ãƒ ã€ã¸ã¨ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆã—ã¦ã„ã‚‹ã€‚ã“ã®ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’ç†è§£ã—ãŸè€…ãŒã€æ¬¡ã®10å¹´ã®AIæŠ€è¡“ã‚’ç‰½å¼•ã™ã‚‹ã€‚

---


---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
