---
title: "ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«– (Part2: å®Ÿè£…ç·¨)"
emoji: "ğŸ“Š"
type: "tech"
topics: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "çµ±è¨ˆå­¦", "æ•°å­¦", "Python"]
published: false
slug: "ml-lecture-07-part2"
difficulty: "intermediate"
time_estimate: "90 minutes"
languages: ["Python"]
keywords: ["æœ€å°¤æ¨å®š", "MLE", "Cross-Entropy", "forward KL", "reverse KL"]
---

> **ã“ã®è¬›ç¾©ã«ã¤ã„ã¦**
> Part1 ã®çµè«–ï¼ˆMLE = cross-entropy æœ€å°åŒ– = `D_KL(p||q)` æœ€å°åŒ–ï¼‰ã‚’ã€æ•°å€¤ã§å´©ã‚Œãªã„å½¢ã«è½ã¨ã™ã€‚
>
> ç†è«–ç·¨ã¯ [ã€å‰ç·¨ã€‘ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–](/articles/ml-lecture-07-part1) ã‚’ã”è¦§ãã ã•ã„ã€‚

## Learning Objectives

- [ ] MLE ã® `argmax` ã‚’ã€Œæå¤±æœ€å°åŒ–ã€ã¨ã—ã¦å®Ÿè£…ã§ãã‚‹
- [ ] `H(p,q)=H(p)+D_KL(p||q)` ã‚’æ•°å€¤ã§æ¤œç®—ã§ãã‚‹
- [ ] forward KL / reverse KL ã®é•ã„ã‚’ã€æœŸå¾…å€¤ã®å–ã‚Šæ–¹ã¨ã—ã¦èª¬æ˜ã§ãã‚‹
- [ ] FID ã®æ•°å¼ã¨ shape ã‚’èª¬æ˜ã—ã€æ•°å€¤å®‰å®šæ€§ã‚’å®ˆã£ã¦å®Ÿè£…ã§ãã‚‹

---

## ğŸ› ï¸ Z5. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” MLE ã¨ KL ã‚’å‹•ã‹ã—ã¦ç¢ºèªã™ã‚‹

æœ¬ã‚¾ãƒ¼ãƒ³ã§ã¯Part1 ã®ç†è«–ï¼ˆZ4 ã®5ãƒˆãƒ”ãƒƒã‚¯ï¼‰ã‚’ Python ã§å®Ÿè£…ã™ã‚‹ã€‚å„å®Ÿè£…ã¯ã€Œæ•°å¼â†’è¨˜å·å¯¾å¿œâ†’ã‚³ãƒ¼ãƒ‰â†’æ¤œç®—ã€ã®é †ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã€‚

### Z5.1 MLE = Cross-Entropy æœ€å°åŒ–ï¼ˆé›¢æ•£ã®æœ€å°ä¾‹ï¼‰

ã“ã“ã§å£Šã‚Œã‚‹ã®ã¯ã„ã¤ã‚‚ `softmax` ã¨ `log(0)`ã€‚å…ˆã«é˜²å¾¡ã™ã‚‹ã€‚

è¨˜å·â†”å¤‰æ•°å:

- $\hat p$ â†” `p_hat`
- $q_\theta$ â†” `softmax(theta)`
- `\(H(\hat p,q_\theta)\)` â†” `cross_entropy(p_hat,q)`

æ¤œç®—ï¼ˆã“ã®ã‚³ãƒ¼ãƒ‰ã®åˆå¦åŸºæº–ï¼‰:

- `KL(p||q) â‰¥ 0`
- `H(p,q)=H(p)+KL(p||q)`

$$
\hat\theta_{\mathrm{MLE}}
=\arg\max_\theta \sum_{i=1}^N \log q_\theta(x^{(i)})
=\arg\min_\theta \Bigl(-\sum_x \hat p(x)\log q_\theta(x)\Bigr)

H(p,q)=-\sum_x p(x)\log q(x),\quad
D_{\mathrm{KL}}(p\|q)=\sum_x p(x)\log\frac{p(x)}{q(x)}=H(p,q)-H(p)\ge 0
$$
```python
import torch
import torch.nn.functional as F
torch.set_float32_matmul_precision("high")

# Symbolâ†”variable: Î¸=theta (shape: (K,)), pÌ‚=p_hat (shape: (K,)), q_Î¸=q (shape: (K,))
def softmax(theta: torch.Tensor) -> torch.Tensor:
    # numerically stable: F.softmax shifts by max(theta) internally
    return F.softmax(theta, dim=-1)


def cross_entropy(p: torch.Tensor, q: torch.Tensor, eps: float = 1e-12) -> float:
    # H(p,q) = -Î£_x p(x) log q(x);  p,q shape: (K,)
    return float(-(p * torch.log(q + eps)).sum())


def kl(p: torch.Tensor, q: torch.Tensor, eps: float = 1e-12) -> float:
    # D_KL(pâ€–q) = Î£_x p(x)[log p(x) - log q(x)] â‰¥ 0;  shape: (K,)
    return float((p * (torch.log(p + eps) - torch.log(q + eps))).sum())


# counts shape: (K,) â†’ p_hat shape: (K,)  [K=3 vocabulary size]
counts = torch.tensor([50.0, 30.0, 20.0])
p_hat  = counts / counts.sum()

# theta shape: (K,) â†’ q shape: (K,)
theta = torch.tensor([0.2, -0.1, 0.0])
q     = softmax(theta)

H_pq  = cross_entropy(p_hat, q)
H_p   = cross_entropy(p_hat, p_hat)
KL_pq = kl(p_hat, q)

print('p_hat=', p_hat)
print('q    =', q)
print('H(p,q)=', H_pq)
print('H(p)  =', H_p)
print('KL    =', KL_pq)

assert KL_pq >= -1e-12
assert abs(H_pq - (H_p + KL_pq)) < 1e-10
```

```python
import triton
import triton.language as tl
import torch

# logsumexp is the mathematical core of GMM, softmax, and KL divergence in this lecture.
# logsumexp(a) = log Î£_k exp(a_k)  â€” computed with max-shift for numerical stability.


@triton.jit
def _logsumexp_kernel(
    x_ptr,              # input pointer: x shape (N,)
    out_ptr,            # output pointer: scalar result
    N: tl.constexpr,
    BLOCK: tl.constexpr,
):
    """Parallel reduction for logsumexp over N elements in one Triton program."""
    # load with out-of-bounds masking (BLOCK may exceed N)
    offs = tl.arange(0, BLOCK)                            # (BLOCK,)
    mask = offs < N
    x    = tl.load(x_ptr + offs, mask=mask, other=-float("inf"))

    # Step 1: global max for numerical stability (log-sum-exp shift trick)
    x_max = tl.max(x, axis=0)                            # scalar

    # Step 2: Î£ exp(x_i - x_max), with -inf guard on out-of-bounds lanes
    shifted = tl.where(mask, x - x_max, -float("inf"))
    exp_sum = tl.sum(tl.exp(shifted), axis=0)            # scalar

    # logsumexp(x) = x_max + log Î£ exp(x_i - x_max)
    tl.store(out_ptr, x_max + tl.log(exp_sum))


def logsumexp_triton(x: torch.Tensor) -> torch.Tensor:
    """Launcher: x shape (N,) on CUDA â†’ scalar tensor."""
    N     = x.numel()
    BLOCK = triton.next_power_of_2(N)                    # BLOCK â‰¥ N, passed as constexpr
    out   = torch.empty(1, device=x.device, dtype=x.dtype)
    _logsumexp_kernel[(1,)](x, out, N=N, BLOCK=BLOCK)
    return out[0]


# Numerical check: logsumexp([0.2, -0.1, 0.0])
logits = torch.tensor([0.2, -0.1, 0.0])
ref    = torch.logsumexp(logits, dim=0)
print(f"torch ref  logsumexp = {ref.item():.6f}")
if torch.cuda.is_available():
    tri = logsumexp_triton(logits.to("cuda"))  # .to(device) preferred over .cuda()
    print(f"triton GPU logsumexp = {tri.item():.6f}")
    assert abs(tri.item() - ref.item()) < 1e-5
# log softmax = logit_i - logsumexp â†’ softmax sums to 1 âœ…
```

ã“ã®æ¤œç®—ãŒé€šã‚‹ã¨ã€Part1 ã®ã€Œä¸‰ä½ä¸€ä½“ã€ãŒã‚³ãƒ¼ãƒ‰ä¸Šã§å›ºå®šã•ã‚Œã‚‹ã€‚

*mermaid: MLE ã¨ KL ã®é–¢ä¿‚*

```mermaid
flowchart LR
  A[max loglik] --> B[min -E_p log q]
  B --> C[min cross-entropy H(p,q)]
  C --> D[min KL(p||q) (up to constant H(p))]
```

### Z5.2 forward / reverse KLï¼ˆmode covering / seekingï¼‰

è¨€è‘‰ã§è¦šãˆã‚‹ã¨æ··ä¹±ã™ã‚‹ã€‚é•ã„ã¯æœŸå¾…å€¤ã®å–ã‚Šæ–¹ã€‚

$$
D_{\mathrm{KL}}(p\|q)=\mathbb{E}_p[\log p - \log q],\qquad
D_{\mathrm{KL}}(q\|p)=\mathbb{E}_q[\log q - \log p]
$$

- `E_p[-log q]` ã¯ã€Œ`p` ãŒã„ã‚‹å ´æ‰€ã§ `q` ãŒå°ã•ã„ã€ã“ã¨ã‚’å¼·ãç½°ã™ã‚‹ â†’ å–ã‚Šã“ã¼ã—ã«å¼±ã„ï¼ˆmode coveringï¼‰
- `E_q[-log p]` ã¯ã€Œ`q` ãŒç½®ã„ãŸå ´æ‰€ã§ `p` ãŒå°ã•ã„ã€ã“ã¨ã‚’å¼·ãç½°ã™ã‚‹ â†’ ç½®ãå ´ã‚’çµã‚‹ï¼ˆmode seekingï¼‰

**æ•°å€¤ä¾‹ï¼ˆ2å³°åˆ†å¸ƒï¼‰**: $p(x) = 0.5\mathcal{N}(-3,1) + 0.5\mathcal{N}(3,1)$ã€$q_\theta(x) = \mathcal{N}(\mu,\sigma^2)$ ã§æœ€é©åŒ–ã€‚

- Forward KL æœ€å°åŒ–: $q^*$ ã¯2å³°ã®é–“ï¼ˆ$\mu^* \approx 0$ï¼‰ã«åºƒãŒã‚Šã€ä¸¡æ–¹ã‚’ã‚«ãƒãƒ¼ã—ã‚ˆã†ã¨ã™ã‚‹ã€‚çµæœ: $\sigma^* \approx \sqrt{9+1} \approx 3.2$ï¼ˆ2å³°ã‚’åŒ…ã‚€ï¼‰ã€‚
- Reverse KL æœ€å°åŒ–: $q^*$ ã¯ã©ã¡ã‚‰ã‹ã®å³°ã«é›†ä¸­ï¼ˆ$\mu^* \approx \pm3$ã€$\sigma^* \approx 1$ï¼‰ã€‚$p(x) \approx 0$ ã®é ˜åŸŸã«ç¢ºç‡è³ªé‡ã‚’ç½®ãã¨ $\mathbb{E}_q[-\log p]$ ãŒçˆ†ç™ºã™ã‚‹ãŸã‚ã€‚

**è§£æçš„ç¢ºèª**: $p$ ãŒä¸Šè¨˜ã®æ··åˆã‚¬ã‚¦ã‚¹ã§ $q = \mathcal{N}(\mu, \sigma^2)$ ã®ã¨ã:

$$
D_{\mathrm{KL}}(p\|q) = \int p(x)\log p(x)\,dx - \int p(x)\log q(x)\,dx
$$

ç¬¬2é … $\mathbb{E}_p[\log q] = -\frac{1}{2}\left[\log(2\pi\sigma^2) + \frac{\mathbb{E}_p[(x-\mu)^2]}{\sigma^2}\right]$ ã‚’æœ€å°åŒ–ã™ã‚‹ã¨:

$$
\mu^* = \mathbb{E}_p[x] = 0, \quad (\sigma^*)^2 = \mathbb{E}_p[(x-\mu^*)^2] = \mathbb{E}_p[x^2] = 10
$$

$\mathbb{E}_p[x^2] = 0.5 \cdot (1 + 9) + 0.5 \cdot (1 + 9) = 10$ï¼ˆå„ã‚¬ã‚¦ã‚¹æˆåˆ†ã® $\sigma^2 + \mu^2$ ã®åŠ é‡å¹³å‡ï¼‰ã€‚Forward KL ã®è§£ã¯ $\mu^*=0, \sigma^* = \sqrt{10} \approx 3.16$ã€‚

*mermaid: mode covering / seekingï¼ˆç›´æ„Ÿï¼‰*

```mermaid
flowchart TD
  F[forward KL: E_p[-log q]] --> C[punish missing mass where p is]
  C --> MC[mode covering]
  R[reverse KL: E_q[-log p]] --> S[punish placing q where p is small]
  S --> MS[mode seeking]
```

**ãƒ¢ãƒ‡ãƒ«åˆ¥ã® KL ã®æ–¹å‘æ€§**:

| ãƒ¢ãƒ‡ãƒ« | æœ€å°åŒ–ã™ã‚‹ KL | å‚¾å‘ |
|:---|:---|:---|
| MLE / VAE encoder | $D_{KL}(p\|q_\theta)$ | mode covering |
| GAN discriminator | $D_{KL}(q\|p)$ (è¿‘ä¼¼) | mode seeking |
| VAE decoder (ELBO) | $D_{KL}(q_\phi\|p)$ | mode seekingå´ |
| Flow (exact NLL) | $D_{KL}(p\|p_\theta)$ | mode covering |


### Z5.3 FID ã‚’ã€Œå¼ã©ãŠã‚Šã€ã«å®Ÿè£…ã™ã‚‹ï¼ˆæ•°å€¤å®‰å®šæ€§ãŒæœ¬ä½“ï¼‰

FID ã¯ã€ç‰¹å¾´ç©ºé–“ã§å®Ÿåˆ†å¸ƒã¨ç”Ÿæˆåˆ†å¸ƒã‚’ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ã—ã€ãã®è·é›¢ã‚’æ¸¬ã‚‹ã€‚å®Ÿè£…ã®æ•µã¯è¡Œåˆ—å¹³æ–¹æ ¹ã€‚

shape:

- `Î¼_r, Î¼_g âˆˆ R^d`
- `Î£_r, Î£_g âˆˆ R^{dÃ—d}`

è½ã¨ã—ç©´:

- `Î£` ãŒéå¯¾ç§°ã«ãªã‚‹ â†’ å¯¾ç§°åŒ–
- å°ã•ã„è² ã®å›ºæœ‰å€¤ãŒå‡ºã‚‹ â†’ ä¸‹ã‹ã‚‰ã‚¯ãƒªãƒƒãƒ—ï¼ˆ`max(w,eps)`ï¼‰

$$
\mathrm{FID}(r,g)
= \|\mu_r-\mu_g\|_2^2
+ \mathrm{Tr}\Bigl(\Sigma_r + \Sigma_g - 2(\Sigma_r\Sigma_g)^{1/2}\Bigr)
$$
```python
import torch
torch.set_float32_matmul_precision("high")
torch.manual_seed(0)

# Symbolâ†”variable: Î¼_r=mu_r (shape: (d,)), Î£_r=Sigma_r (shape: (d,d)), d=feature dim
def cov(X: torch.Tensor) -> torch.Tensor:
    # X shape: (N, d) â†’ unbiased covariance (d, d)
    Xc = X - X.mean(dim=0, keepdim=True)
    return (Xc.T @ Xc) / (X.shape[0] - 1)


def sqrtm_psd(A: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:
    # Numerically stable matrix sqrt via eigendecomposition; A shape: (d, d)
    A    = 0.5 * (A + A.T)                      # enforce symmetry
    w, V = torch.linalg.eigh(A)                 # w shape: (d,), V shape: (d,d)
    w    = w.clamp(min=eps)                      # clip negative eigenvalues
    return (V * w.sqrt()) @ V.T


def fid_gaussian(
    mu_r: torch.Tensor, Sigma_r: torch.Tensor,
    mu_g: torch.Tensor, Sigma_g: torch.Tensor,
) -> float:
    # FID = â€–Î¼_r-Î¼_gâ€–Â² + Tr(Î£_r+Î£_g-2(Î£_r Î£_g)^Â½)
    d       = mu_r.shape[0]
    Sigma_r = 0.5 * (Sigma_r + Sigma_r.T) + 1e-6 * torch.eye(d)
    Sigma_g = 0.5 * (Sigma_g + Sigma_g.T) + 1e-6 * torch.eye(d)

    diff        = mu_r - mu_g                   # shape: (d,)
    Sr12        = sqrtm_psd(Sigma_r)
    middle      = Sr12 @ Sigma_g @ Sr12
    middle_sqrt = sqrtm_psd(middle)

    tr = torch.trace(Sigma_r + Sigma_g - 2.0 * middle_sqrt)
    return float(diff @ diff + tr)


# synthetic features (stand-in for Inception features)
# Xr, Xg shape: (N, d)
N, d             = 800, 16
Xr               = torch.randn(N, d)
Xg               = torch.randn(N, d) * 1.1 + 0.2
mu_r, mu_g       = Xr.mean(dim=0), Xg.mean(dim=0)
Sigma_r, Sigma_g = cov(Xr), cov(Xg)

fid  = fid_gaussian(mu_r, Sigma_r, mu_g, Sigma_g)
fid0 = fid_gaussian(mu_r, Sigma_r, mu_r, Sigma_r)
print('FID=', fid)
print('FID (same)=', fid0)
assert fid  >= -1e-6
assert abs(fid0) < 1e-6
```

*mermaid: FID ã®è¨ˆç®—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³*

```mermaid
flowchart LR
  R[real features] --> Mr[Î¼_r, Î£_r]
  G[gen features] --> Mg[Î¼_g, Î£_g]
  Mr --> FID[FID]
  Mg --> FID
```


### Z5.4 GMM ã® MLE â€” å®Œå…¨å®Ÿè£…

2æˆåˆ†ã‚¬ã‚¦ã‚¹æ··åˆ $p(x) = \pi_1 \mathcal{N}(x|\mu_1,\sigma_1^2) + \pi_2 \mathcal{N}(x|\mu_2,\sigma_2^2)$ ã® MLE ã‚’
å‹¾é…é™ä¸‹ã§æ±‚ã‚ã‚‹ã€‚EM ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ç¬¬8å›ã«å–ã£ã¦ãŠãã€‚

$$
\log p_\theta(\mathcal{D}) = \sum_{i=1}^N \log \left[\pi_1 \mathcal{N}(x_i|\mu_1,\sigma_1^2) + \pi_2 \mathcal{N}(x_i|\mu_2,\sigma_2^2)\right]
$$

è¨˜å·â†”å¤‰æ•°: $\pi_1$ = `pi1`, $\mu_k$ = `mu[k]`, $\sigma_k$ = `sigma[k]`, $N$ = `len(x)`ã€‚

**shape**: `x` ã¯ `(N,)`, `mu` ã¯ `(2,)`, `sigma` ã¯ `(2,)`, `pi1` ã¯ã‚¹ã‚«ãƒ©ãƒ¼ã€‚

```python
import torch
import torch.nn.functional as F
from torch import Tensor
import math

torch.manual_seed(42)
torch.set_float32_matmul_precision("high")

# Symbolâ†”variable: Ï€â‚=pi1, Î¼_k=mu1/mu2, Ïƒ_k=s1/s2 (via log_s1/log_s2), x shape: (N,)
_LOG_2PI = math.log(2 * math.pi)


def log_likelihood_gmm(params: Tensor, x: Tensor) -> Tensor:
    """2-component GMM negative log-likelihood (logsumexp-stable)."""
    # params shape: (5,)
    pi1_logit, mu1, log_s1, mu2, log_s2 = params.unbind()
    pi1 = torch.sigmoid(pi1_logit)           # Ï€â‚ âˆˆ (0,1) via sigmoid
    pi2 = 1.0 - pi1
    s1, s2 = log_s1.exp(), log_s2.exp()      # Ïƒ_k > 0 via exp reparameterisation

    # log N(x|Î¼_k, Ïƒ_kÂ²) = -Â½(x-Î¼_k)Â²/Ïƒ_kÂ² - log Ïƒ_k - Â½log(2Ï€); shape: (N,)
    log_n1 = -0.5 * ((x - mu1) / s1).pow(2) - log_s1 - 0.5 * _LOG_2PI
    log_n2 = -0.5 * ((x - mu2) / s2).pow(2) - log_s2 - 0.5 * _LOG_2PI

    # log[Ï€_k N(x|Î¼_k,Ïƒ_kÂ²)]; shape: (2, N)
    log_comp = torch.stack([pi1.log() + log_n1, pi2.log() + log_n2])

    # NLL = -Î£_i logsumexp_k [log Ï€_k + log N(x_i|Î¼_k, Ïƒ_kÂ²)]
    return -torch.logsumexp(log_comp, dim=0).sum()


# x_data shape: (N=500,)
x_data = torch.cat([
    torch.randn(300) - 3.0,   # N(-3, 1)
    torch.randn(200) + 3.0,   # N( 3, 1)
])

# params: [pi1_logit, mu1, log_s1, mu2, log_s2]
# init: pi1â‰ˆ0.5, mu1â‰ˆ-2, s1â‰ˆ1, mu2â‰ˆ2, s2â‰ˆ1
params = torch.tensor([0.0, -2.0, 0.0, 2.0, 0.0], requires_grad=True)
opt    = torch.optim.LBFGS([params], max_iter=500, line_search_fn="strong_wolfe")


def closure() -> Tensor:
    opt.zero_grad()
    loss = log_likelihood_gmm(params, x_data)
    loss.backward()
    return loss


opt.step(closure)

with torch.inference_mode():
    pi1_logit, mu1, log_s1, mu2, log_s2 = params.unbind()
    pi1 = torch.sigmoid(pi1_logit)
    print(f"pi1={pi1:.3f}, mu1={mu1:.3f}, s1={log_s1.exp():.3f}")
    print(f"pi2={1-pi1:.3f}, mu2={mu2:.3f}, s2={log_s2.exp():.3f}")
    # æœŸå¾…å€¤: pi1â‰ˆ0.6, mu1â‰ˆ-3, s1â‰ˆ1, mu2â‰ˆ3, s2â‰ˆ1
```

è½ã¨ã—ç©´: `log(mixture)` ã§ `mixture = 0` ãŒèµ·ãã‚‹ã¨ `-inf`ã€‚`+ 1e-12` ã§é˜²ãã€‚`sigma` ã‚’ç›´æ¥æœ€é©åŒ–ã™ã‚‹ã¨è² ã«ãªã‚‹ãŸã‚ `log(sigma)` ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã—ã¦ `exp` ã§æˆ»ã™ã€‚

### Z5.5 Score Matching ã®æ•°å€¤å®Ÿè£…

Score Matching ã¯å°¤åº¦ã‚’è©•ä¾¡ã§ããªãã¦ã‚‚å­¦ç¿’ã§ãã‚‹æ‰‹æ³• [^6]ã€‚ã‚¹ã‚³ã‚¢é–¢æ•° $s_\theta(x) = \nabla_x \log p_\theta(x)$ ã‚’ä¸€è‡´ã•ã›ã‚‹ã€‚

$$
J(\theta) = \mathbb{E}_{p_{\text{data}}}\left[\|s_\theta(x) - \nabla_x \log p_{\text{data}}(x)\|^2\right]
$$

$\nabla_x \log p_{\text{data}}$ ã¯æœªçŸ¥ã ãŒã€ç©åˆ†ã«ã‚ˆã‚‹éƒ¨åˆ†ç©åˆ†ã§:

$$
J(\theta) = \mathbb{E}_{p_{\text{data}}}\left[\frac{1}{2}\|s_\theta(x)\|^2 + \text{tr}(\nabla_x s_\theta(x))\right] + \text{const}
$$

**éƒ¨åˆ†ç©åˆ†ã®å±•é–‹ï¼ˆ1æ¬¡å…ƒç‰ˆï¼‰**:

$$
\mathbb{E}_p\left[(s_\theta(x) - \nabla_x \log p)^2\right]
= \mathbb{E}_p[s_\theta^2] - 2\mathbb{E}_p[s_\theta \cdot \nabla_x \log p] + \text{const}
$$

å•é¡Œã¯ $\mathbb{E}_p[s_\theta \cdot \nabla_x \log p]$ ã ãŒã€éƒ¨åˆ†ç©åˆ†ã§:

$$
\mathbb{E}_p\left[s_\theta \cdot \frac{p'}{p}\right] = \int s_\theta(x) p'(x)\,dx = \left[s_\theta(x)p(x)\right]_{-\infty}^{\infty} - \int s_\theta'(x) p(x)\,dx
$$

å¢ƒç•Œæ¡ä»¶ $p(\pm\infty) = 0$ ã‚ˆã‚Šå¢ƒç•Œé …ãŒã‚¼ãƒ­ã¨ãªã‚Š:

$$
= -\mathbb{E}_p\left[\nabla_x s_\theta(x)\right]
$$

ã—ãŸãŒã£ã¦:

$$
J(\theta) = \mathbb{E}_p\left[\frac{1}{2}s_\theta(x)^2 + \nabla_x s_\theta(x)\right] + \text{const}
$$

ã“ã‚ŒãŒã€Œãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢ã‚’çŸ¥ã‚‰ãªãã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã§ãã‚‹ã€ç†ç”±ã®å…¨è²Œã ã€‚

è¨˜å·â†”å¤‰æ•°: $s_\theta(x)$ = `score(x, theta)`, $\nabla_x s_\theta$ = Jacobian `dscore_dx`ã€‚

**shape**: $x \in \mathbb{R}^d$, $s_\theta(x) \in \mathbb{R}^d$, Jacobian ã¯ $(d, d)$ã€‚

**å®Ÿè£…ä¸Šã®éµ**: 1æ¬¡å…ƒã®å ´åˆ `tr(Jacobian) = ds/dx` ã¯æ•°å€¤å¾®åˆ†ã§è¨ˆç®—ã§ãã‚‹ï¼ˆä¸­å¿ƒå·®åˆ† `(s(x+Îµ) - s(x-Îµ)) / 2Îµ`ï¼‰ã€‚é«˜æ¬¡å…ƒã§ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒ $O(d^2)$ ã¨ãªã‚‹ãŸã‚ã€Hutchinson ãƒˆãƒ¬ãƒ¼ã‚¹æ¨å®šå™¨ $z^T J z$ï¼ˆ$z \sim \mathcal{N}(0,I)$ï¼‰ã‚’ä½¿ã† â€” ã“ã‚ŒãŒ Sliced Score Matching [^7] ã®å‹•æ©Ÿã€‚

**æ•°å€¤ã§ç†è§£**: ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}(\mu, \sigma^2)$ ã®ã‚¹ã‚³ã‚¢é–¢æ•°ã¯ $s_\theta(x) = -(x-\mu)/\sigma^2$ ã§è§£æçš„ã«æ›¸ã‘ã‚‹ã€‚Score Matching æå¤±ã‚’çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è©•ä¾¡ã™ã‚‹ã¨æœ€å°ã«ãªã‚Šã€èª¤ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã¯å¤§ãããªã‚‹ â€” ã“ã‚ŒãŒã€Œå°¤åº¦ãªã—ã§æ¨å®šã§ãã‚‹ã€ã“ã¨ã®è¨¼æ‹ ã ã€‚

**Denoising Score Matching ã¨ã®æ¥ç¶š** [^7]: Song & Ermon ã¯ $s_\theta(x)$ ã®ä»£ã‚ã‚Šã«ã€ãƒã‚¤ã‚ºåŠ å·¥ãƒ‡ãƒ¼ã‚¿ $\tilde{x} = x + \epsilon$ï¼ˆ$\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ï¼‰ã®ã‚¹ã‚³ã‚¢ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§:

$$
J_{DSM}(\theta) = \mathbb{E}_{x, \tilde{x}}\left[\left\|s_\theta(\tilde{x}) - \frac{\tilde{x} - x}{\sigma^2}\right\|^2\right]
$$

ã“ã‚Œã«ã‚ˆã‚Š Jacobian ã®è¨ˆç®—ãŒä¸è¦ã«ãªã‚Šã€é«˜æ¬¡å…ƒã§ã‚‚ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã«ãªã‚‹ã€‚ã“ã‚ŒãŒ Diffusion ãƒ¢ãƒ‡ãƒ«ã®æ ¹æœ¬åŸç†ï¼ˆç¬¬14å›ï¼‰ã€‚

è½ã¨ã—ç©´: `tr(âˆ‡_x s_Î¸)` ã¯å¯¾æ•°å°¤åº¦ã® Laplacian $\sum_i \partial^2 \log p_\theta / \partial x_i^2$ ã«ç­‰ã—ã„ã€‚Score Matching ãŒ MLE ã¨ç­‰ä¾¡ãªã®ã¯ã€éƒ¨åˆ†ç©åˆ†ã§ $\mathbb{E}_p[\nabla_x \log p_\theta \cdot \nabla_x \log p_\text{data}]$ ãŒè¨ˆç®—ãªã—ã«æ¶ˆãˆã‚‹ã‹ã‚‰ã ï¼ˆä¸Šè¨˜ã®å°å‡ºã‚’å‚ç…§ï¼‰ã€‚

### Z5.6 Rejection Sampling ã¨ Importance Sampling ã®å®Ÿè£…

**Rejection Sampling**:

$$
x \sim p(x) \propto \tilde{p}(x), \quad \text{proposal } q(x), \quad \text{accept if } u \leq \frac{\tilde{p}(x)}{M q(x)}
$$

è¨˜å·â†”å¤‰æ•°: $\tilde{p}(x)$ = `ptilde(x)` (unnormalized), $M$ = `M` (envelope constant), $u \sim U[0,1]$ = `u`.

**å—ã‘å…¥ã‚Œç‡ã®æ­£ç¢ºãªå¼**:

å—ã‘å…¥ã‚Œç‡ã¯:

$$
\text{acceptance rate} = \frac{1}{M} \cdot \frac{\int \tilde{p}(x)\,dx}{\int q(x)\,dx} = \frac{Z_p}{M}
$$

ã“ã“ã§ $Z_p = \int \tilde{p}(x)\,dx$ï¼ˆè¦æ ¼åŒ–å®šæ•°ï¼‰ã€‚$M$ ã‚’ $\max_x \tilde{p}(x)/q(x)$ ã«è¨­å®šã™ã‚‹ã¨ã€å—ã‘å…¥ã‚Œç‡ãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹ã€‚

**æ•°å€¤ã§ç†è§£**: Beta(2, 5) åˆ†å¸ƒã‚’ Uniform(0,1) ææ¡ˆåˆ†å¸ƒã‹ã‚‰æ£„å´ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹å ´åˆ:
- Beta(2, 5) ã®æœ€å¤§å€¤: $f^* = B(2,5)^{-1} \cdot (1/6)^1 \cdot (5/6)^4 \approx 0.082$ï¼ˆãƒ¢ãƒ¼ãƒ‰ $x = 1/6$ï¼‰
- $M = 0.1$ ã¨ã—ã¦å—ã‘å…¥ã‚Œç‡ã¯ $Z_p/M = 1/M = 10$ â€” å¹³å‡10å›ã«1å›ã®ã¿å—ã‘å…¥ã‚Œã‚‰ã‚Œã‚‹
- æ¤œç®—: 1000ã‚µãƒ³ãƒ—ãƒ«å–å¾—ã«å¹³å‡10000å›ã®ææ¡ˆãŒå¿…è¦

è½ã¨ã—ç©´: $M$ ãŒå°ã•ã™ãã‚‹ã¨ä¸€éƒ¨ã® $x$ ã§ $\tilde{p}(x) > M q(x)$ ã¨ãªã‚Šã€ã‚µãƒ³ãƒ—ãƒ«ãŒåã‚‹ã€‚æ¤œè¨¼ã™ã‚‹æ–¹æ³•ã¯ã€ã‚µãƒ³ãƒ—ãƒ«ã®æ¨™æœ¬å¹³å‡ã¨åˆ†æ•£ãŒè§£æå€¤ï¼ˆBeta(Î±,Î²): $E[x] = \alpha/(\alpha+\beta) = 2/7 \approx 0.286$ï¼‰ã«ä¸€è‡´ã™ã‚‹ã‹ç¢ºèªã™ã‚‹ã“ã¨ã€‚

**Importance Sampling** â€” æœŸå¾…å€¤ã®æ¨å®š:

$$
\mathbb{E}_{p}[f(x)] = \mathbb{E}_{q}\left[f(x) \frac{p(x)}{q(x)}\right] \approx \frac{1}{N}\sum_{i=1}^N f(x_i) w_i, \quad w_i = \frac{p(x_i)}{q(x_i)}
$$

**IS ã¨ RS ã®ä½¿ã„åˆ†ã‘**:

| æ‰‹æ³• | ç›®çš„ | è¦ä»¶ | ã‚³ã‚¹ãƒˆ |
|:-----|:-----|:-----|:-----|
| Rejection Sampling | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | $M q(x) \geq \tilde{p}(x)$ å…¨åŸŸ | $O(1/\text{rate})$ ã‚µãƒ³ãƒ—ãƒ«/ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ |
| Importance Sampling | æœŸå¾…å€¤æ¨å®š | $q$ ãŒ $p$ ã®å°ã‚’ã‚«ãƒãƒ¼ | $O(1/\text{ESS})$ åˆ†æ•£å¢—å¤§ |
| MCMC | é«˜æ¬¡å…ƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | å±€æ‰€ææ¡ˆ OK | æ··åˆæ™‚é–“ãŒå¿…è¦ |

RS ã¯æ­£ç¢ºãªã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãŒé«˜æ¬¡å…ƒã§ã¯ $M$ ãŒæŒ‡æ•°çš„ã«å¤§ãããªã‚‹ã€‚IS ã¯ã€Œè¿‘ä¼¼ã€ã§ã‚ˆã‘ã‚Œã°åˆ¶ç´„ãŒç·©ã„ãŒã€ESS ã®åŠ£åŒ–ã‚’ç›£è¦–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ˆZ5.16ï¼‰ã€‚

**æ•°å€¤ã§ç†è§£**: $\mathcal{N}(0,1)$ ææ¡ˆã‹ã‚‰ $\mathcal{N}(2,1)$ ã®æœŸå¾…å€¤ $E[x]=2$ ã‚’æ¨å®šã™ã‚‹å ´åˆã€é‡ã¿ãŒå¤§ãã„ $x \approx 2$ ã®è¿‘å‚ã‚µãƒ³ãƒ—ãƒ«ãŒæ”¯é…çš„ã«ãªã‚‹ã€‚é‡ã¿ã®åˆ†æ•£ãŒå°ã•ã‘ã‚Œã°æ¨å®šã¯åŠ¹ç‡çš„ã€‚

è½ã¨ã—ç©´: log-sum-exp shift `log_w -= log_w.max()` ãŒãªã„ã¨ `exp` ãŒæ¡ã‚ãµã‚Œã‚‹ã€‚`w /= w.sum()` ã§è‡ªå·±æ­£è¦åŒ–ã™ã‚‹ã“ã¨ã§æœªçŸ¥ã®è¦æ ¼åŒ–å®šæ•°ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã§ãã‚‹ã€‚ææ¡ˆåˆ†å¸ƒ $q$ ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆ $p$ ã®è£¾é‡ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ãªã„å ´åˆã€å°‘æ•°ã®è¶…å¤§é‡ã¿ãŒåˆ†æ•£ã‚’çˆ†ç™ºã•ã›ã‚‹ï¼ˆZ5.16 ã® Effective Sample Size ã§è¨ºæ–­: $\text{ESS} = 1/\sum w_i^2$ï¼‰ã€‚

è¨˜å·â†”å¤‰æ•°: $f(x)$ = `f`, $w_i = p(x_i)/q(x_i)$ = `w[i]`, $x_i \sim q$ = `x_samples`ã€‚

### Z5.7 CramÃ©r-Rao ä¸‹ç•Œã®æ•°å€¤æ¤œè¨¼

Fisher æƒ…å ±é‡ $I(\theta)$ ã‚’æ•°å€¤ã§è¨ˆç®—ã—ã€æ¨å®šé‡ã®åˆ†æ•£ãŒ CramÃ©r-Rao ä¸‹ç•Œ $I(\theta)^{-1}$ ã‚’ä¸‹å›ã‚‰ãªã„ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

$$
I(\theta) = \mathbb{E}_{x \sim p_\theta}\left[\left(\frac{\partial \log p_\theta(x)}{\partial \theta}\right)^2\right] = -\mathbb{E}_{x \sim p_\theta}\left[\frac{\partial^2 \log p_\theta(x)}{\partial \theta^2}\right]
$$

è¨˜å·â†”å¤‰æ•°: $I(\theta)$ = Fisheræƒ…å ±é‡, $\hat{\theta}_{MLE}$ = æ¨™æœ¬å¹³å‡, åˆ†æ•£ $\text{Var}(\hat{\theta})$ = ä¸ååˆ†æ•£ã€‚

**æ•°å€¤ã§ç†è§£ (è§£æçš„)**: $\mathcal{N}(\mu, \sigma^2)$ ã‹ã‚‰ Nå€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã§ $\mu$ ã‚’æ¨å®šã™ã‚‹å ´åˆ:
- Fisheræƒ…å ±é‡: $I(\mu) = N/\sigma^2$ï¼ˆ1ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Š $1/\sigma^2$ï¼‰
- CRB: $\text{Var}(\hat{\mu}) \geq 1/I(\mu) = \sigma^2/N$
- MLE $\hat{\mu} = \bar{x}$ ã®åˆ†æ•£: $\text{Var}(\bar{x}) = \sigma^2/N = 1/I(\mu)$

$N=50, \sigma=1$ ãªã‚‰ $\text{CRB} = 1/50 = 0.02$ã€‚æ¨™æœ¬å¹³å‡ã®åˆ†æ•£ã‚‚ã»ã¼ $0.02$ ã«ãªã‚‹ â€” MLE ã¯æœ‰åŠ¹æ¨å®šé‡ï¼ˆCRBã‚’é”æˆï¼‰ã ã€‚æ¼¸è¿‘æ­£è¦æ€§ã®æ•°å€¤çš„è¨¼æ‹ ã§ã‚‚ã‚ã‚‹ã€‚

è½ã¨ã—ç©´: æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼ˆæ­£è¦, ãƒã‚¢ã‚½ãƒ³, ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤ï¼‰ã§ã¯ MLE ãŒå¸¸ã«æœ‰åŠ¹ã€‚éæŒ‡æ•°å‹ã§ã¯ CRB ãŒé”æˆã§ããªã„å ´åˆãŒã‚ã‚‹ â€” Fisher æƒ…å ±é‡ã¯å±€æ‰€çš„ãªæ›²ç‡ã§ã‚ã‚Šã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæœ€é©æ€§ã‚’ä¿è¨¼ã—ãªã„ã€‚

**æŒ‡æ•°å‹åˆ†å¸ƒæ—ã«ãŠã‘ã‚‹ Fisher æƒ…å ±é‡ã®ä¸€è¦§**:

| åˆ†å¸ƒ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ | $I(\theta)$ï¼ˆN=1ï¼‰ |
|:-----|:-----------------|:-------------------|
| $\mathcal{N}(\mu, \sigma^2)$ï¼ˆ$\sigma$ æ—¢çŸ¥ï¼‰ | $\mu$ | $1/\sigma^2$ |
| $\mathcal{N}(\mu, \sigma^2)$ï¼ˆ$\mu$ æ—¢çŸ¥ï¼‰ | $\sigma^2$ | $1/(2\sigma^4)$ |
| $\text{Bernoulli}(p)$ | $p$ | $1/[p(1-p)]$ |
| $\text{Poisson}(\lambda)$ | $\lambda$ | $1/\lambda$ |
| $\text{Exp}(\lambda)$ | $\lambda$ | $1/\lambda^2$ |

ã“ã‚Œã‚‰ã¯ã™ã¹ã¦ MLE ãŒ CRB ã‚’é”æˆã™ã‚‹ã€‚æŒ‡æ•°å‹åˆ†å¸ƒæ—ã§ã¯ååˆ†çµ±è¨ˆé‡ $T(x)$ ãŒå­˜åœ¨ã—ã€$I(\theta) = \text{Var}(T(x))^{-1}$ ãŒæˆç«‹ã™ã‚‹ã‹ã‚‰ï¼ˆRao-Blackwell ã®å®šç†ï¼‰ã€‚

### Z5.8 Mode-Seeking vs Mode-Covering ã®å¯è¦–åŒ–å®Ÿé¨“

2å³°åˆ†å¸ƒã«å¯¾ã—ã¦ forward KL ($D_{KL}(p \| q)$) ã¨ reverse KL ($D_{KL}(q \| p)$) ã‚’æœ€å°åŒ–ã™ã‚‹ã¨ä½•ãŒèµ·ãã‚‹ã‹ã‚’æ•°å€¤ã§ç¢ºèªã€‚

$$
\text{forward: } D_{KL}(p\|q_\theta) = \mathbb{E}_p[\log p - \log q_\theta] \quad \text{(mode covering)}
$$

$$
\text{reverse: } D_{KL}(q_\theta\|p) = \mathbb{E}_{q_\theta}[\log q_\theta - \log p] \quad \text{(mode seeking)}
$$

è¨˜å·â†”å¤‰æ•°: $p$ = `p_true` (bimodal), $q_\theta = \mathcal{N}(\mu, \sigma^2)$ = æœ€é©åŒ–ã™ã‚‹ã‚¬ã‚¦ã‚¹, $\theta = (\mu, \log \sigma)$ = paramsã€‚

**Forward KL æœ€å°åŒ–ã®è§£æè§£**:

$p = 0.5\,\mathcal{N}(-3,1) + 0.5\,\mathcal{N}(3,1)$ã€$q_\theta = \mathcal{N}(\mu, \sigma^2)$ ã®å ´åˆã€$\partial D_{KL}(p\|q)/\partial\mu = 0$ ã‚ˆã‚Š:

$$
\mathbb{E}_p[x] = \int x\, p(x)\,dx = 0.5 \times (-3) + 0.5 \times 3 = 0
$$

ã—ãŸãŒã£ã¦ $\mu^* = 0$ã€‚åŒæ§˜ã« $\partial D_{KL}(p\|q)/\partial\sigma = 0$ ã‚ˆã‚Š:

$$
(\sigma^*)^2 = \mathbb{E}_p[x^2] - (\mathbb{E}_p[x])^2 = 0.5(9+1) + 0.5(9+1) - 0 = 10
$$

ã¤ã¾ã‚Š $\sigma^* = \sqrt{10} \approx 3.16$ã€‚ã“ã‚ŒãŒã€Œä¸¡å³°ã‚’ã¾ãŸãåºƒã„ã‚¬ã‚¦ã‚¹ã€ã®æ•°å€¤çš„æ ¹æ‹ ã ã€‚

**Reverse KL æœ€å°åŒ–ã®æŒ™å‹•**:

$\partial D_{KL}(q_\theta\|p)/\partial\mu = 0$ ã‚’è§£æçš„ã«æ±‚ã‚ã‚‹ã“ã¨ã¯é›£ã—ã„ãŒã€ç›´æ„Ÿçš„ã«ç†è§£ã§ãã‚‹ã€‚$q$ ãŒã‚¼ãƒ­ã«ãªã‚‹å ´æ‰€ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒ $q \log(q/p) = 0$ ãªã®ã§ã€$q$ ã¯ $p \approx 0$ ã®é ˜åŸŸï¼ˆ2å³°ã®é–“ãªã©ï¼‰ã‚’ã€Œé¿ã‘ã‚‹ã€ã€‚æ•°å€¤çš„ã«ã¯ $\mu \approx \pm 3$ï¼ˆã©ã¡ã‚‰ã‹ã®å³°ï¼‰ã«åæŸã™ã‚‹ â€” ã©ã¡ã‚‰ã®å³°ã«åæŸã™ã‚‹ã‹ã¯åˆæœŸå€¤ä¾å­˜ã€‚

```mermaid
graph LR
    A["Forward KL<br/>E_p[f]ã‚’æœ€å°åŒ–"] --> B["qãŒpã®ã‚ã‚‹æ‰€ã‚’<br/>å…¨ã¦ã‚«ãƒãƒ¼å¿…è¦"]
    B --> C["mode covering<br/>Î¼â‰ˆ0, Ïƒâ‰ˆ3.16"]
    D["Reverse KL<br/>E_q[g]ã‚’æœ€å°åŒ–"] --> E["qãŒã‚¼ãƒ­ã®æ‰€ã¯<br/>ãƒšãƒŠãƒ«ãƒ†ã‚£â‰ 0"]
    E --> F["mode seeking<br/>Î¼â‰ˆÂ±3, Ïƒâ‰ˆ1"]
    C --> G["VAEçš„å‹•ä½œ<br/>å¤šæ§˜ãªã‚µãƒ³ãƒ—ãƒ«"]
    F --> H["GANçš„å‹•ä½œ<br/>é®®æ˜ã ãŒå¤šæ§˜æ€§æ¬ å¦‚"]
```

ã“ã‚ŒãŒ VAEï¼ˆforward KL â†’ å¤šæ§˜ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰ã¨ GANï¼ˆreverse KL â†’ é®®æ˜ã ãŒå¤šæ§˜æ€§æ¬ å¦‚ï¼‰ã®æŒ™å‹•ã®é•ã„ã®æ•°å­¦çš„æ ¹æ‹ ã ã€‚ç”Ÿæˆãƒ¢ãƒ‡ãƒ«é¸æŠã®æœ¬è³ªã¯ã“ã“ã«ã‚ã‚‹ [^2][^3]ã€‚

### Z5.9 LLM ã®æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ MLE

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯è‡ªå·±å›å¸°çš„ãª MLE:

$$
\hat{\theta}_{MLE} = \arg\max_\theta \sum_{\text{seq}} \sum_{t=1}^T \log p_\theta(x_t | x_{<t})
$$

**æœ€å°å®Ÿè£…**: æ–‡å­—ãƒ¬ãƒ™ãƒ« uni-gram ãƒ¢ãƒ‡ãƒ«ã® MLEï¼ˆã‚«ã‚¦ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰ã€‚

è¨˜å·â†”å¤‰æ•°: $p_\theta(x_t | x_{<t})$ = æ¡ä»¶ä»˜ãç¢ºç‡, $\hat{\theta}$ = ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰è¨ˆç®—ã—ãŸãƒã‚¤ã‚°ãƒ©ãƒ é·ç§»ç¢ºç‡ã€‚

**æ•°å€¤ã§ç†è§£**: æ–‡å­—åˆ— `"abracadabra"` ã‚’ç¹°ã‚Šè¿”ã—ãŸè¨“ç·´ãƒ†ã‚­ã‚¹ãƒˆã§ãƒã‚¤ã‚°ãƒ©ãƒ MLEè¨“ç·´ã™ã‚‹ã¨ã€NLLã¯0ã«è¿‘ã¥ãï¼ˆå®Œå…¨æš—è¨˜ï¼‰ã€‚åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«ã™ã‚‹ã¨é«˜æ€§èƒ½ã«è¦‹ãˆã‚‹ãŒã€æœªè¦‹ã®æ–‡å­—åˆ—ã§ã¯æ€¥è½ã™ã‚‹ â€” ã“ã‚ŒãŒMLEéå­¦ç¿’ã®æœ¬è³ªã ã€‚

**ãªãœ Softmax + Cross-Entropy = MLE ãªã®ã‹**:

ç¾ä»£ã®LLMã¯ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹å‡ºåŠ›ã‚’æŒã¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¨ã—ã¦å®Ÿè£…ã•ã‚Œã‚‹ã€‚å‡ºåŠ› $\text{logit}_c = f_\theta(x_{<t})_c$ ã«å¯¾ã—:

$$
p_\theta(x_t = c | x_{<t}) = \text{softmax}(\text{logit})_c = \frac{\exp(\text{logit}_c)}{\sum_{c'}\exp(\text{logit}_{c'})}
$$

Cross-Entropy æå¤±ã¯:

$$
\mathcal{L}_{CE} = -\sum_{t}\log p_\theta(x_t|x_{<t}) = -\sum_t \text{logit}_{x_t} + \sum_t \log\sum_{c'}\exp(\text{logit}_{c'})
$$

ã“ã‚ŒãŒ MLE ã®è² ã®å¯¾æ•°å°¤åº¦ãã®ã‚‚ã®ã ï¼ˆZ5.1 ã®æ•°å€¤è¨¼æ˜ã‚’å‚ç…§ï¼‰ã€‚transformer ã® `nn.CrossEntropyLoss` ã¯ã“ã‚Œã‚’ç›´æ¥è¨ˆç®—ã—ã¦ã„ã‚‹ã€‚

**Dirichlet-Multinomial MLEï¼ˆãƒã‚¤ã‚°ãƒ©ãƒ  MAPï¼‰**:

èªå½™ $V$ ä¸Šã®ãƒã‚¤ã‚°ãƒ©ãƒ é·ç§»ç¢ºç‡ $\theta_c = P(x_t=c|x_{t-1})$ ã« Dirichlet äº‹å‰åˆ†å¸ƒ $\text{Dir}(\alpha \mathbf{1})$ ã‚’ã‹ã‘ã‚‹ã¨ã€MAP æ¨å®šï¼ˆLaplace smoothingï¼‰:

$$
\hat{\theta}_c^{MAP} = \frac{n_c + \alpha - 1}{N + V(\alpha-1)}
$$

$\alpha = 1$: MLEï¼ˆ= $n_c/N$ï¼‰ã€$\alpha = 2$: Laplace smoothingï¼ˆã‚¼ãƒ­ã‚«ã‚¦ãƒ³ãƒˆãŒ $1/(N+V)$ï¼‰ã€‚å®Ÿéš›ã®LLMã¯$\sim10^{10}$ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã§è¨“ç·´ã—ã€ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ãªã—ã§ã‚‚çµ±è¨ˆçš„ã«ååˆ†ãªã‚«ã‚¦ãƒ³ãƒˆã‚’ç¢ºä¿ã™ã‚‹ã€‚

### Z5.10 Python é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ â€” MLE ã®åå¾©è¨ˆç®—ã®å£

MLE ã® `L-BFGS-B` æœ€é©åŒ–ã§ `n` ã‚’å¤‰ãˆãŸã¨ãã®è¨ˆç®—æ™‚é–“ã¯æ¬¡ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹:

| n | æ™‚é–“ï¼ˆç›®å®‰ï¼‰ | è¨ˆç®—é‡ |
|---|---|---|
| 1,000 | `~0.001s` | $O(n)$ ã® NLL è¨ˆç®— |
| 10,000 | `~0.002s` | ã»ã¼ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ« |
| 100,000 | `~0.01s` | NumPy ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§åŠ¹ç‡çš„ |
| 1,000,000 | `~0.1s` | ãƒ¡ãƒ¢ãƒªè»¢é€ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã« |

`n=1M` ã§ã‚‚1ç§’æœªæº€ã«è¦‹ãˆã‚‹ãŒã€å®Ÿéš›ã®LLMã¯æ¬¡å…ƒãŒæ¡é•ã„ã ã€‚

**ãªãœ LLM ã®è¨“ç·´ã¯ã“ã‚“ãªã«é…ã„ã®ã‹** â€” ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã®æ•°å­¦:

Kaplan ã‚‰ [^NEEDS_VERIFY] ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã«ã‚ˆã‚‹ã¨ã€LLM ã®æå¤± $L$ ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° $N$ã€ãƒ‡ãƒ¼ã‚¿æ•° $D$ã€è¨ˆç®—é‡ $C = 6ND$ ã«å¯¾ã—ã¦:

$$
L(N) \approx \left(\frac{N_c}{N}\right)^{\alpha_N}, \quad L(D) \approx \left(\frac{D_c}{D}\right)^{\alpha_D}
$$

å…¸å‹å€¤: $\alpha_N \approx 0.076$, $\alpha_D \approx 0.095$ï¼ˆã»ã¼ $N^{-0.1}$ï¼‰ã€‚æå¤±ã‚’åŠæ¸›ã•ã›ã‚‹ãŸã‚ã«ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ $2^{1/0.076} \approx 8000$ å€ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

**è¨ˆç®—é‡ã‹ã‚‰è¦‹ãŸ MLE**:

GPT-3ï¼ˆ175B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿, 300B ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®è¨“ç·´ã¯:
- 1ã‚¹ãƒ†ãƒƒãƒ—ã®FLOPs: $\approx 2 \times 175 \times 10^9 \times 2 = 7 \times 10^{11}$ï¼ˆforward+backwardï¼‰
- å…¨è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°: $\approx 300B / \text{batch\_size}$
- æ¨å®šç·FLOP: $\approx 3 \times 10^{23}$

ã“ã‚Œã‚’ A100 GPUï¼ˆ312 TFLOPSï¼‰ã§è¨ˆç®—ã™ã‚‹ã¨ç†è«–ä¸Š $\approx 10^9$ ç§’ = **ç´„30å¹´** â€” ç¾å®Ÿã«ã¯1000å°ã®GPUã‚’ä½¿ã£ã¦æ•°ç™¾æ—¥ã«çŸ­ç¸®ã™ã‚‹ã€‚Python ãƒ«ãƒ¼ãƒ—ã§ã¯**ç‰©ç†çš„ã«ä¸å¯èƒ½**ã€‚GPU + CUDA + mixed precision ãŒä¸å¯æ¬ ãªç†ç”±ãŒã“ã“ã«ã‚ã‚‹ã€‚ç¬¬8å› EM ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã€Œé…ã™ãã‚‹ã€ã¨ã„ã†å®Ÿæ„ŸãŒé ‚ç‚¹ã«é”ã—ã€ç¬¬9å›ã§ Rust/Rust ãŒç™»å ´ã™ã‚‹ã€‚

### Z5.11 MAP æ¨å®š â€” MLE + äº‹å‰åˆ†å¸ƒ

MAP (Maximum A Posteriori) ã¯ MLE ã«äº‹å‰åˆ†å¸ƒã‚’åŠ ãˆãŸæ¨å®šé‡:

$$
\hat{\theta}_{MAP} = \arg\max_\theta \log p(\theta|\mathcal{D}) = \arg\max_\theta [\log p(\mathcal{D}|\theta) + \log p(\theta)]
$$

è¨˜å·â†”å¤‰æ•°: $\log p(\mathcal{D}|\theta)$ = log_likelihood, $\log p(\theta)$ = log_prior, $\lambda$ = lambda_regã€‚

**æ¥ç¶š**: ã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒ $p(\theta) = \mathcal{N}(0, 1/\lambda \cdot I)$ â†’ $\log p(\theta) = -\lambda \|\theta\|^2/2 + \text{const}$ â†’ MAP = MLE + L2æ­£å‰‡åŒ–ã€‚

**æ•°å€¤ã§ç†è§£ï¼ˆè§£æçš„ï¼‰**: ç›´ç·šå›å¸° $y = \theta x + \epsilon$ã€$\epsilon \sim \mathcal{N}(0, \sigma^2)$ ã« $\mathcal{N}(0, 1/\lambda)$ äº‹å‰åˆ†å¸ƒã‚’ã‹ã‘ã‚‹ã¨:

$$
\hat{\theta}_{MAP} = \frac{\sum x_i y_i}{\sum x_i^2 + \lambda \sigma^2}
$$

$\lambda \to 0$ ãªã‚‰ MLEï¼ˆ$\hat{\theta} = \sum x_i y_i / \sum x_i^2$ï¼‰ã€$\lambda \to \infty$ ãªã‚‰ $\hat{\theta} \to 0$ï¼ˆã‚¼ãƒ­ã¸ã®ç¸®å°ï¼‰ã€‚ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§ã® MAP ã¯é–‰å½¢å¼ãŒãªã„ãŒã€`L-BFGS-B` ã§æ•°å€¤çš„ã«è§£ã‘ã‚‹ã€‚æ•°å€¤å®‰å®šåŒ–ã«ã¯ `np.logaddexp(0, -logits)` ($\log(1+e^z)$ ã®å®‰å®šè¨ˆç®—) ãŒå¿…é ˆ â€” ç›´æ¥ `np.log(1 + np.exp(logits))` ã¯ `logits > 100` ã§ `inf`ã€‚

**ãƒ©ãƒ—ãƒ©ã‚¹äº‹å¾Œè¿‘ä¼¼ã¨ã®æ¥ç¶š**: MAP æ¨å®šé‡ $\hat{\theta}_{MAP}$ ã®è¿‘å‚ã§ãƒã‚¹ã‚¿ãƒªã‚¢ã‚’2æ¬¡è¿‘ä¼¼ã™ã‚‹ã¨:

$$
\log p(\theta|\mathcal{D}) \approx \log p(\hat{\theta}_{MAP}|\mathcal{D}) - \frac{1}{2}(\theta - \hat{\theta}_{MAP})^T H (\theta - \hat{\theta}_{MAP})
$$

ã“ã“ã§ $H = -\nabla^2_\theta \log p(\theta|\mathcal{D})|_{\hat{\theta}_{MAP}}$ ã¯ãƒ˜ãƒƒã‚»è¡Œåˆ—ã€‚ã“ã‚ŒãŒ Laplace è¿‘ä¼¼ã§ã€äº‹å¾Œåˆ†å¸ƒã‚’ $\mathcal{N}(\hat{\theta}_{MAP}, H^{-1})$ ã§è¿‘ä¼¼ã™ã‚‹ã€‚MLE ã®æ¼¸è¿‘æ­£è¦æ€§ï¼ˆPart1 Z4 T2ï¼‰ã¯ $\lambda \to 0$ ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã¨ã—ã¦ç†è§£ã§ãã‚‹:

$$
\hat{\theta}_{MLE} \xrightarrow{d} \mathcal{N}\left(\theta_0, \frac{1}{N} I(\theta_0)^{-1}\right) \quad \text{(æ¼¸è¿‘æ­£è¦æ€§)}
$$

**å®Ÿå‹™ä¸Šã® MAP ã¨ MLE ã®é•ã„ã®ã¾ã¨ã‚**:

| ç‰¹æ€§ | MLE | MAP (Gaussian prior) |
|:-----|:----|:--------------------|
| ç›®çš„é–¢æ•° | $\ell(\theta) = \sum \log p(x_i|\theta)$ | $\ell(\theta) - \lambda\|\theta\|^2/2$ |
| å°ãƒ‡ãƒ¼ã‚¿æŒ™å‹• | éå­¦ç¿’ | æ­£å‰‡åŒ–ã§å®‰å®š |
| å¤§ãƒ‡ãƒ¼ã‚¿æ¼¸è¿‘ | ä¸€è‡´æ¨å®šé‡ | $\lambda$ ã®å½±éŸ¿ãŒ $1/N$ ã«ç¸®å° |
| è§£é‡ˆ | ç‚¹æ¨å®š | äº‹å¾Œæœ€é »å€¤ï¼ˆãƒ¢ãƒ¼ãƒ‰ï¼‰ |
| ä¸ç¢ºå®Ÿæ€§ | ãªã— | Laplace è¿‘ä¼¼ã§è©•ä¾¡å¯ |

### Z5.12 Reparameterization Trick

VAE ã®å­¦ç¿’ã®æ ¸å¿ƒã€‚$z \sim q_\phi(z|x) = \mathcal{N}(\mu_\phi, \sigma_\phi^2)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å¾®åˆ†å¯èƒ½ã«ã™ã‚‹:

$$
z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

è¨˜å·â†”å¤‰æ•°: $\mu_\phi$ = `mu`, $\sigma_\phi$ = `sigma`, $\epsilon$ = `eps`, $z$ = `z`ã€‚

**shape**: `mu`, `sigma`, `z` ã¯ã™ã¹ã¦ `(batch, latent_dim)`ã€‚`eps` ã‚‚åŒã˜ shape ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

ãªãœå¿…è¦ã‹: $z \sim \mathcal{N}(\mu, \sigma^2)$ ã¯ã€Œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¼”ç®—ã€ãªã®ã§é€šå¸¸ã¯å‹¾é…ãŒæµã‚Œãªã„ã€‚Reparameterization ã«ã‚ˆã‚Šã€ç¢ºç‡å¤‰æ•° $z$ ã‚’ã€Œæ±ºå®šè«–çš„å¤‰æ› + ç‹¬ç«‹ãƒã‚¤ã‚º $\epsilon$ã€ã«åˆ†è§£ã—ã€$\partial z / \partial \mu = 1$ã€$\partial z / \partial \sigma = \epsilon$ ã¨ã—ã¦å‹¾é…ã‚’è¨ˆç®—å¯èƒ½ã«ã™ã‚‹ã€‚

**ELBO ã¨ã®æ¥ç¶š**: VAE ã®ç›®çš„é–¢æ•° ELBO ã¯:

$$
\mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p(z))
$$

å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ– $z = \mu_\phi + \sigma_\phi \odot \epsilon$ ã«ã‚ˆã‚Šã€ç¬¬1é …ã®æœŸå¾…å€¤ãŒ:

$$
\mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)}[\log p_\theta(x|\mu_\phi + \sigma_\phi \odot \epsilon)]
$$

ã¨ãªã‚Šã€$\phi$ ã«å¯¾ã™ã‚‹å‹¾é… $\nabla_\phi$ ãŒé€šã‚‹ã€‚ã“ã‚ŒãŒã€ŒELBO ã‚’ç¢ºç‡çš„å‹¾é…é™ä¸‹ã§æœ€é©åŒ–ã§ãã‚‹ã€ç†ç”±ã®å…¨è²Œã ã€‚

ç¬¬2é …ã® KL ã¯ $q_\phi = \mathcal{N}(\mu_\phi, \sigma_\phi^2)$ã€$p(z) = \mathcal{N}(0,I)$ ã®ã¨ãè§£æçš„ã«è¨ˆç®—ã§ãã‚‹:

$$
D_{KL}(\mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(0,I)) = \frac{1}{2}\sum_j\left(\mu_j^2 + \sigma_j^2 - 1 - \log \sigma_j^2\right)
$$

$\mu_j = 0, \sigma_j = 1$ ã§ã‚¼ãƒ­ï¼ˆäº‹å¾ŒãŒäº‹å‰ã¨ä¸€è‡´ï¼‰ã€$\sigma_j \to 0$ ã§ $+\infty$ï¼ˆå®Œå…¨ãªç‚¹é›†ä¸­ï¼‰ã€‚

**æ•°å€¤æ¤œç®—**: `mu = [2.0, -1.0]`, `sigma = [1.65, 0.61]` ã®ã¨ãã€5000ã‚µãƒ³ãƒ—ãƒ«ã®æ¨™æœ¬å¹³å‡ã¯ `[2.0Â±0.02, -1.0Â±0.01]`ã€æ¨™æœ¬æ¨™æº–åå·®ã¯ `[1.65Â±0.02, 0.61Â±0.01]` ã«åæŸã™ã‚‹ã€‚ã“ã‚ŒãŒ VAE ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€è¨“ç·´ã®æœ¬è³ªï¼ˆç¬¬10å›ã§å…¨å®Ÿè£…ï¼‰ã€‚

### Z5.13 Normalizing Flow â€” 1æ¬¡å…ƒå¤‰æ•°å¤‰æ›ï¼ˆæœ€å°å®Ÿè£…ï¼‰

Normalizing Flow ã®æœ¬è³ª: å˜ç´”ãªåˆ†å¸ƒï¼ˆä¾‹: $\mathcal{N}(0,1)$ï¼‰ã‚’å¯é€†å¤‰æ› $f_\theta$ ã§è¤‡é›‘ãªåˆ†å¸ƒã«å¤‰å½¢ã™ã‚‹ã€‚

$$
\log p_\theta(x) = \log p_z(f_\theta^{-1}(x)) + \log \left|\det \frac{\partial f_\theta^{-1}}{\partial x}\right|
$$

è¨˜å·â†”å¤‰æ•°: $f_\theta^{-1}$ = `inv_transform`, $\log|\det J|$ = `log_abs_det_jac`, $p_z$ = æ¨™æº–æ­£è¦åˆ†å¸ƒã®å¯†åº¦ã€‚

shape: $x \in \mathbb{R}^d$ ã«å¯¾ã—ã€Jacobian ã¯ $d \times d$ è¡Œåˆ—ï¼ˆ1æ¬¡å…ƒã§ã¯å˜ã«ã‚¹ã‚«ãƒ©ãƒ¼ã®å¾®åˆ†ï¼‰ã€‚

1æ¬¡å…ƒã‚¢ãƒ•ã‚£ãƒ³å¤‰æ› $x = \mu + \sigma \cdot z$ï¼ˆ$z \sim \mathcal{N}(0,1)$ï¼‰ã‚’ä¾‹ã«ã¨ã‚‹:

$$
f_\theta^{-1}(x) = \frac{x - \mu}{\sigma}, \quad \log\left|\frac{\partial f^{-1}}{\partial x}\right| = -\log \sigma
$$

**å¤‰æ•°å¤‰æ›å®šç†ã®å¤šå¤‰é‡ã¸ã®æ‹¡å¼µ**:

$d$ æ¬¡å…ƒã®å ´åˆã€å¯é€†å¤‰æ› $f_\theta: \mathbb{R}^d \to \mathbb{R}^d$ ã«å¯¾ã—:

$$
\log p_\theta(x) = \log p_z(f_\theta^{-1}(x)) + \log \left|\det \left(\frac{\partial f_\theta^{-1}}{\partial x}\right)\right|
$$

Jacobian è¡Œåˆ—å¼ $\det(\partial f^{-1}/\partial x)$ ã®è¨ˆç®—ãŒ NF ã®è¨­è¨ˆä¸Šã®èª²é¡Œã€‚ä¸€èˆ¬ã® $d \times d$ è¡Œåˆ—ã®è¡Œåˆ—å¼ã¯ $O(d^3)$ â€” ã“ã‚ŒãŒ NF ã®è¨­è¨ˆä¸Šã®å·¥å¤«ãŒå…¨ã¦é›†ä¸­ã™ã‚‹ç‚¹:

| NF ã®ç¨®é¡ | Jacobian è¨ˆç®— | ã‚³ã‚¹ãƒˆ |
|:---------|:------------|:------|
| Affineï¼ˆæœ¬ç¯€ï¼‰ | $\det = \prod \sigma_i$ | $O(d)$ |
| RealNVP [^5] | Coupling å±¤ã§ä¸‰è§’è¡Œåˆ— | $O(d)$ |
| Glow | $1 \times 1$ ç•³ã¿è¾¼ã¿ | $O(d^3)$ |
| FFJORD | Neural ODE + Hutchinson | $O(d)$ |

è¨˜å·â†”å¤‰æ•°: $\hat{\mu}$ = `mu_hat` = æ¨™æœ¬å¹³å‡, $\hat{\sigma}$ = `sigma_hat` = æ¨™æœ¬æ¨™æº–åå·®ã€‚

**æ•°å€¤ã§ç†è§£ï¼ˆè§£æçš„ï¼‰**: ã‚¢ãƒ•ã‚£ãƒ³ãƒ•ãƒ­ãƒ¼ $x = \mu + \sigma z$ ã® MLE ã¯é–‰å½¢å¼ã§è§£ã‘ã‚‹ã€‚$N$ ã‚µãƒ³ãƒ—ãƒ«ã®å¯¾æ•°å°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ã¨ $\hat{\mu} = \bar{x}$, $\hat{\sigma}^2 = \frac{1}{N}\sum(x_i - \bar{x})^2$ï¼ˆæ¨™æœ¬åˆ†æ•£ï¼‰ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚$x_i \sim \mathcal{N}(2.0, 0.25)$ ã‹ã‚‰500ã‚µãƒ³ãƒ—ãƒ«ãªã‚‰ $\hat{\mu} \approx 2.00$, $\hat{\sigma} \approx 0.50$ â€” ã“ã‚Œã¯è§£æè§£ãã®ã‚‚ã®ã ã€‚æ•°å€¤æœ€é©åŒ–ï¼ˆBFGSï¼‰ã§ã‚‚åŒã˜å€¤ã«åæŸã™ã‚‹ã“ã¨ãŒ NF ã®å®Ÿè£…æ­£å½“æ€§ã®è¨¼æ‹ ã€‚

**NF ã¨ GMM ã®é•ã„**: GMM ã¯ãƒ¢ãƒ¼ãƒ‰æ•° $K$ ã‚’äº‹å‰ã«æ±ºã‚ã‚‹ãŒã€NF ã¯å¤‰æ›ã®è¤‡é›‘ã•ï¼ˆå±¤æ•°ï¼‰ã§ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚ã‚¢ãƒ•ã‚£ãƒ³ãƒ•ãƒ­ãƒ¼ï¼ˆ1å±¤ï¼‰ã¯å˜ä¸€ã‚¬ã‚¦ã‚¹ç›¸å½“ â€” å¤šå³°ãƒ‡ãƒ¼ã‚¿ã«ã¯ RealNVP / Glow ãªã©ã®æ·±å±¤ NF ãŒå¿…è¦ï¼ˆç¬¬11å›ï¼‰ã€‚

**RealNVP ã‚«ãƒƒãƒ—ãƒªãƒ³ã‚°å±¤ã®ä»•çµ„ã¿ï¼ˆO(d) Jacobian ã®ç†ç”±ï¼‰**:

RealNVP [^5] ã¯ $d$ æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« $x$ ã‚’2åˆ†å‰² $(x_{1:k}, x_{k+1:d})$ ã—ã¦ã€æ¬¡ã®ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã‚’é©ç”¨ã™ã‚‹:

$$
y_{1:k} = x_{1:k}, \quad y_{k+1:d} = x_{k+1:d} \odot \exp(s_\theta(x_{1:k})) + t_\theta(x_{1:k})
$$

ã“ã“ã§ $s_\theta, t_\theta: \mathbb{R}^k \to \mathbb{R}^{d-k}$ ã¯ä»»æ„ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆï¼ˆå¯é€†æ€§ã¯ä¸è¦ï¼‰ã€‚Jacobian ã¯:

$$
J_f = \begin{pmatrix} I_k & 0 \\ \frac{\partial y_{k+1:d}}{\partial x_{1:k}} & \text{diag}(\exp(s_\theta(x_{1:k}))) \end{pmatrix}
$$

ä¸‰è§’è¡Œåˆ—ãªã®ã§ $\det(J_f) = \prod_{i=k+1}^d \exp(s_{\theta,i}(x_{1:k})) = \exp\bigl(\sum_{i} s_{\theta,i}(x_{1:k})\bigr)$ ãŒ $O(d)$ ã§è¨ˆç®—ã§ãã‚‹ã€‚é€†å¤‰æ›ã‚‚:

$$
x_{k+1:d} = (y_{k+1:d} - t_\theta(y_{1:k})) \odot \exp(-s_\theta(y_{1:k}))
$$

ã¨ã—ã¦ $O(d)$ ã§è¨ˆç®—ã§ãã‚‹ â€” ã“ã‚ŒãŒ RealNVP ã®è¨­è¨ˆä¸Šã®éµã ã€‚$s_\theta, t_\theta$ ã¯ä»»æ„è¤‡é›‘ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’ä½¿ãˆã‚‹ã®ã§è¡¨ç¾åŠ›ã¯é«˜ã„ã€‚å±¤ã‚’äº¤äº’ã«ç©ã¿é‡ã­ã‚‹ã“ã¨ã§ $x_{1:k}$ ã¨ $x_{k+1:d}$ ãŒäº’ã„ã«å¤‰æ›ã—åˆã„ã€å…¨æ¬¡å…ƒãŒç›¸äº’ä½œç”¨ã™ã‚‹ã€‚

### Z5.14 1æ¬¡å…ƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒå®Ÿé¨“

3ç¨®é¡ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆGMM-MLE, KDE, NF-ã‚¢ãƒ•ã‚£ãƒ³ï¼‰ã‚’åŒã˜ãƒ‡ãƒ¼ã‚¿ã§æ¯”è¼ƒã™ã‚‹æœ€å°å®Ÿé¨“:

$$
\text{NLL} = -\frac{1}{N_{test}} \sum_{i=1}^{N_{test}} \log p_\theta(x_i^{test})
$$

è¨˜å·â†”å¤‰æ•°: $N_{test}$ = ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°, $p_\theta(x)$ = å„ãƒ¢ãƒ‡ãƒ«ã®ç¢ºç‡å¯†åº¦ã€‚

**æ•°å€¤ã§ç†è§£**: 2å³°åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿ $0.4\,\mathcal{N}(-2,1) + 0.6\,\mathcal{N}(3,0.7)$ ã«å¯¾ã—ã¦å„ãƒ¢ãƒ‡ãƒ«ã‚’å½“ã¦ã¯ã‚ãŸå ´åˆã®ãƒ†ã‚¹ãƒˆ NLLï¼ˆç›®å®‰ï¼‰:

| ãƒ¢ãƒ‡ãƒ« | ãƒ†ã‚¹ãƒˆ NLL | å‚™è€ƒ |
|--------|-----------|------|
| KDE (bw=0.3) | `~1.65` | 2å³°ã‚’ç›´æ¥ã‚«ãƒãƒ¼ |
| NF-affine (1æˆåˆ†) | `~2.10` | å˜ä¸€ã‚¬ã‚¦ã‚¹ â†’ å¤šå³°ã«å¤±æ•— |
| GMM (K=2, Z5.4å‚ç…§) | `~1.60` | å¤šå³°ã‚’æ­£ç¢ºã«ãƒ¢ãƒ‡ãƒ« |

NF ã®å˜ç´”ãªã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã¯1æˆåˆ†ã‚¬ã‚¦ã‚¹ã¨ç­‰ä¾¡ â€” äºŒå³°ãƒ‡ãƒ¼ã‚¿ã«ã¯ GMMï¼ˆKâ‰¥2ï¼‰ã‹å¤šå±¤ NFï¼ˆRealNVP, Glowï¼‰ãŒå¿…è¦ã€‚NLL ã®æ•°å€¤å·®ã¯å°ã•ãè¦‹ãˆã‚‹ãŒã€$\Delta\text{NLL} = 0.5$ ã¯ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£æ¯”ã§ $e^{0.5} \approx 1.6$ å€ã«ç›¸å½“ã™ã‚‹ã€‚

### Z5.15 æ•°å€¤å®‰å®šæ€§ â€” Log-Likelihood ã®å®Ÿè£…ä¸Šã®æ³¨æ„

MLE ã‚’å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè£…ã™ã‚‹ã¨ãã€ã‚ˆãã‚ã‚‹æ•°å€¤çš„è½ã¨ã—ç©´ã¯ underflow ã ã€‚

$$
\prod_{i=1}^N p_\theta(x_i) \approx 0 \quad \text{(floating point underflow: } N > 300 \text{ ã§ã‚¼ãƒ­ã«ãªã‚‹)}
$$

$N = 300$, $p_\theta(x_i) = 0.1$ ã®ã¨ã $\prod p = 10^{-300}$ï¼ˆfloat64 æœ€å°å€¤ $\approx 10^{-308}$ï¼‰ã€‚å¯¾æ•°ã‚’å–ã‚Œã°è¨ˆç®—å¯èƒ½:

$$
\sum_{i=1}^N \log p_\theta(x_i) = 300 \times \log(0.1) \approx -691
$$

**å®Ÿè£…åŸå‰‡**: `np.log(norm.pdf(x))` ã¯ âŒã€`norm.logpdf(x)` ã¯ âœ…ã€‚`scipy.stats` ã® `logpdf` ã¯å¯¾æ•°ç¢ºç‡ã‚’è§£æçš„ã«è¨ˆç®—ã—ã€underflow ã‚’å›é¿ã™ã‚‹ã€‚

æ¤œç®—: $\log \mathcal{N}(5; 0, 1) = -25/2 - \frac{1}{2}\log(2\pi) \approx -13.419$ã€N=300 ã‚µãƒ³ãƒ—ãƒ«ã§ $\approx -4025.7$ã€‚

**GMM ã«å¯¾ã™ã‚‹æ··åˆå¯¾æ•°ã®å®‰å®šè¨ˆç®—**:

GMM ã®å¯¾æ•°å°¤åº¦ã¯æ··åˆã®log:

$$
\log p_\theta(x) = \log \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \sigma_k^2)
$$

ç›´æ¥ `np.log(sum(pi * normal.pdf(x) for ...))` ã¯ `pdf` ãŒ underflow ã—ã¦ã‚‚ã‚¼ãƒ­ã«ãªã‚‹ã€‚å®‰å…¨ãªè¨ˆç®—ã¯ log-sum-exp trick:

$$
\log \sum_k \pi_k \mathcal{N}(x|\mu_k, \sigma_k) = \text{logsumexp}_k\left[\log\pi_k + \log\mathcal{N}(x|\mu_k, \sigma_k)\right]
$$

ã“ã“ã§ `logsumexp` ã®å®šç¾©:

$$
\text{logsumexp}(a_1, \ldots, a_K) = a^* + \log\sum_k \exp(a_k - a^*), \quad a^* = \max_k a_k
$$

$a^*$ ã§ shift ã™ã‚‹ã“ã¨ã§ $\exp(a_k - a^*)$ ãŒå…¨ã¦ `[0,1]` ã«åã¾ã‚Šã€overflow ã‚‚ underflow ã‚‚é˜²ãã€‚ã“ã‚ŒãŒ Z5.4 ã® `log_likelihood_gmm` é–¢æ•°å†…ã§ `logsumexp` ã‚’ä½¿ã†ç†ç”±ã ã€‚

**æ•°å€¤ç²¾åº¦ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:

| æ“ä½œ | âŒ å±é™º | âœ… å®‰å…¨ |
|:-----|:--------|:--------|
| ç¢ºç‡ã®ç© | `prod(probs)` | `sum(log_probs)` |
| æ··åˆå¯†åº¦ã®log | `log(sum(pi*pdf))` | `logsumexp(log_pi + logpdf)` |
| ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ | `exp(x)/sum(exp(x))` | `exp(x-max(x))/sum(exp(x-max(x)))` |
| KL divergence | `sum(p*log(p/q))` | `sum(p*(logp - logq))` with `log(0)=-inf` å¯¾ç­– |
| æ­£è¦åˆ†å¸ƒpdf | `log(norm.pdf(x))` | `norm.logpdf(x)` |

### Z5.16 æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆESSï¼‰ã¨é‡è¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å“è³ª

Z5.6 ã®åŸºæœ¬ IS ã®å»¶é•·ã¨ã—ã¦ã€ææ¡ˆåˆ†å¸ƒ $q$ ã®ã€Œè‰¯ã•ã€ã‚’å®šé‡åŒ–ã™ã‚‹ ESS:

$$
\text{ESS} = \frac{\left(\sum_{i=1}^N w_i\right)^2}{\sum_{i=1}^N w_i^2}, \quad w_i = \frac{p(x_i)}{q(x_i)}
$$

è¨˜å·â†”å¤‰æ•°: $w_i$ = `w[i]`, $N$ = ã‚µãƒ³ãƒ—ãƒ«æ•°, ESS âˆˆ [1, N]ï¼ˆ$q=p$ ã®ã¨ãæœ€å¤§ $N$ï¼‰ã€‚

**ESS ã®å°å‡º**: IS æ¨å®šé‡ $\hat{\mu}_{IS} = \sum_i w_i f(x_i) / \sum_i w_i$ ã®åˆ†æ•£ã¯ã€é‡ã¿ã®å¤‰å‹•ä¿‚æ•° $\text{CV}^2(w) = \text{Var}(w)/\mathbb{E}[w]^2$ ã«æ¯”ä¾‹ã™ã‚‹ã€‚iid Monte Carlo ã ã¨åˆ†æ•£ãŒ $\text{Var}(f)/N$ ãªã®ã§ã€IS ã®ç­‰ä¾¡ iid ã‚µãƒ³ãƒ—ãƒ«æ•°ã¯:

$$
\text{ESS} = \frac{N}{1 + \text{CV}^2(w)} = \frac{N \left(\mathbb{E}[w]\right)^2}{\mathbb{E}[w^2]}
$$

ã‚µãƒ³ãƒ—ãƒ«è¿‘ä¼¼ã§ã¯ $\mathbb{E}[w] \approx \bar{w}$ã€$\mathbb{E}[w^2] \approx \overline{w^2}$ ã¨ã—ã¦ä¸Šè¨˜ã®å¼ã«ä¸€è‡´ã™ã‚‹ã€‚

**æ•°å€¤ã§ç†è§£**: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ $p = \mathcal{N}(5,1)$ã€ææ¡ˆ $q = \mathcal{N}(0, \sigma_q)$ ã§ $\sigma_q$ ã‚’å¤‰ãˆã‚‹ã¨ ESS ãŒåŠ‡çš„ã«å¤‰ã‚ã‚‹:

| $\sigma_q$ | ESS/N | å‚™è€ƒ |
|---|---|---|
| 1.0 | `~0.3%` | `q` ãŒ `p` ã«é‡ãªã‚‰ãªã„ â€” IS å´©å£Š |
| 3.0 | `~7%` | éƒ¨åˆ†çš„ã«ã‚«ãƒãƒ¼ |
| 6.0 | `~38%` | `q` ãŒ `p` ã®å°¾éƒ¨ã‚’ã‚«ãƒãƒ¼ |

$\text{ESS} = 0.3\%$ ã¯1000ã‚µãƒ³ãƒ—ãƒ«ã§å®Ÿè³ª3ã‚µãƒ³ãƒ—ãƒ«ç›¸å½“ â€” ã»ã¨ã‚“ã©æƒ…å ±ãŒãªã„ã€‚

**ç†è«–çš„ãªESSä¸Šé™**: $q = p$ ã®ã¨ãå…¨ã¦ã®é‡ã¿ãŒå‡ä¸€ï¼ˆ$w_i = c = \text{const}$ï¼‰ãªã®ã§ $\text{ESS} = N$ï¼ˆæœ€è‰¯ï¼‰ã€‚$q$ ã¨ $p$ ã® KL ä¹–é›¢ãŒå¤§ãã„ã»ã©é‡ã¿ã®åˆ†æ•£ãŒå¤§ãããªã‚Š ESS ãŒä½ä¸‹ã™ã‚‹ã€‚é–¢ä¿‚å¼:

$$
\frac{N}{\text{ESS}} - 1 \approx \text{CV}^2(w) \approx e^{D_{KL}(p\|q)} - 1
$$

ï¼ˆä¸€æ¬¡è¿‘ä¼¼ã€‚$D_{KL}$ ãŒå¤§ãã„é ˜åŸŸã§ã¯éå°è©•ä¾¡ã ãŒå‚¾å‘ã¯æ­£ã—ã„ï¼‰

**ç›´æ„Ÿ**: $q$ ãŒ $p$ ã‚’ã‚«ãƒãƒ¼ã—ãªã„ã¨ ESS/N â†’ 0 ã¨ãªã‚Šã€1-2å€‹ã®è¶…å¤§é‡ã¿ãŒæ¨å®šå€¤ã‚’æ”¯é…ã™ã‚‹ã€‚è¦å‰‡ã®ç›®å®‰ã¨ã—ã¦ `ESS/N < 10%` ãªã‚‰ææ¡ˆåˆ†å¸ƒã‚’å¤‰æ›´ã™ã¹ãã€‚

è½ã¨ã—ç©´: ESS ã¯ $q$ ã®åˆ†æ•£ãŒ $p$ ã®åˆ†æ•£ã‚ˆã‚Šå¤§ãã„ã¨ãï¼ˆè£¾ãŒåºƒã„ã¨ãï¼‰è‰¯ããªã‚‹ â€” é€†å‘ãï¼ˆ$q$ ãŒç´°ã„ã¨ãï¼‰ã¯æ‚²æƒ¨ã€‚ã“ã‚ŒãŒ Rejection Samplingï¼ˆä¸Šé™ $M$ ã‚’è¨­å®šã—ã¦ã‚«ãƒãƒ¼ä¿è¨¼ï¼‰ã®å‹•æ©Ÿã§ã‚‚ã‚ã‚‹ã€‚Sequential Monte Carloï¼ˆç²’å­ãƒ•ã‚£ãƒ«ã‚¿ï¼‰ã§ã¯ ESS ã‚’å¸¸æ™‚ç›£è¦–ã—ã€ESS < N/2 ã«ãªã£ãŸã‚‰ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

### Z5.17 Langevin Dynamics â€” ã‚¹ã‚³ã‚¢é–¢æ•°ã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

Z5.5 ã§å­¦ã‚“ã ã‚¹ã‚³ã‚¢é–¢æ•° $s_\theta(x) = \nabla_x \log p_\theta(x)$ ã¯ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚‚ç›´æ¥ä½¿ãˆã‚‹ã€‚Langevin ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ï¼ˆULAï¼‰:

$$
x_{t+1} = x_t + \frac{\epsilon}{2} \nabla_x \log p(x_t) + \sqrt{\epsilon}\, z_t, \quad z_t \sim \mathcal{N}(0, I)
$$

è¨˜å·â†”å¤‰æ•°: $\epsilon$ = ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º, $\nabla_x \log p$ = ã‚¹ã‚³ã‚¢é–¢æ•° `s_theta(x)`, $z_t$ = iid ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã€‚

**ç›´æ„Ÿ**: ç¬¬1é …ã¯ã€Œå¯¾æ•°å°¤åº¦ãŒé«˜ã„æ–¹å‘ã¸ã®å‹¾é…ä¸Šæ˜‡ã€ï¼ˆæ±ºå®šè«–çš„ãƒ‰ãƒªãƒ•ãƒˆï¼‰ã€ç¬¬2é …ã¯ã€Œãƒ©ãƒ³ãƒ€ãƒ ãªæ‹¡æ•£ã€ï¼ˆæ¢ç´¢ï¼‰ã€‚$\epsilon \to 0$ ã®é€£ç¶šæ¥µé™ $dx = \frac{1}{2}\nabla_x \log p(x)\,dt + dW_t$ ã¯ Langevin æ–¹ç¨‹å¼ãã®ã‚‚ã®ã ã€‚

**ãªãœ $p(x)$ ã®ä¸åã‚µãƒ³ãƒ—ãƒ«ãŒå¾—ã‚‰ã‚Œã‚‹ã‹**: å®šå¸¸åˆ†å¸ƒã‚’ $\pi(x)$ ã¨ãŠãã¨ã€è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ï¼ˆFokker-Planck æ–¹ç¨‹å¼ï¼‰ã‚ˆã‚Š $\pi(x) = p(x)$ ãŒæˆç«‹ã™ã‚‹ã€‚ã¤ã¾ã‚Šã€Œã‚¹ã‚³ã‚¢é–¢æ•°ã«å¾“ã£ã¦æ­©ãã¨ã€æœ€çµ‚çš„ã«ç›®æ¨™åˆ†å¸ƒ $p$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¾—ã‚‰ã‚Œã‚‹ã€ã€‚

**Diffusion ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¥ç¶š**: Score Matching (Z5.5) ã§ã‚¹ã‚³ã‚¢é–¢æ•° $s_\theta(x_t) \approx \nabla_{x_t} \log p_t(x_t)$ ã‚’å­¦ç¿’ã—ã€Langevin ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ â€” ã“ã‚ŒãŒ Song & Ermon (2019) [^7] ã®æ ¸å¿ƒã ã€‚DDPM [^12] ã®é€†æ‹¡æ•£ã‚¹ãƒ†ãƒƒãƒ—ã‚‚ Langevin ã®é›¢æ•£åŒ–ã¨ã—ã¦è§£é‡ˆã§ãã‚‹ã€‚

### Z5 Quick Check

**ãƒã‚§ãƒƒã‚¯ 1**: 2å³°åˆ†å¸ƒ $p(x) = 0.5\mathcal{N}(-3,1) + 0.5\mathcal{N}(3,1)$ ã«å¯¾ã—ã¦ forward KL ã‚’æœ€å°åŒ–ã—ãŸã‚¬ã‚¦ã‚¹ $q^*$ ã®å¹³å‡ $\mu^*$ ã¯ã„ãã‚‰ã‹ï¼Ÿ

<details><summary>ç­”ãˆ</summary>

$$
\mathbb{E}_p[x] = 0.5 \cdot (-3) + 0.5 \cdot 3 = 0
$$

Forward KL ã®æœ€å°åŒ–è§£ $\mu^* = \mathbb{E}_p[x] = 0$ã€‚ã“ã‚Œã¯ä¸¡å³°ã®ä¸­é–“ç‚¹ã€‚ç›´æ„Ÿ: $p$ ãŒã„ã‚‹å ´æ‰€ï¼ˆä¸¡å³°ï¼‰ã‚’å…¨ã¦ã‚«ãƒãƒ¼ã—ã‚ˆã†ã¨ã—ãŸçµæœã€ã©ã¡ã‚‰ã®å³°ã«ã‚‚å±ã•ãªã„ä¸­ç‚¹ã«è½ã¡ã‚‹ã€‚Z5.8 ã®æ•°å€¤å®Ÿé¨“ã§ç¢ºèªã§ãã‚‹ã€‚
</details>

**ãƒã‚§ãƒƒã‚¯ 2**: MLE ã¯ $\sum_i \log p_\theta(x_i)$ ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚ã“ã‚Œã‚’ $-D_{KL}(\hat{p}_{\text{data}} \| p_\theta)$ ã®æœ€å¤§åŒ–ã¨ã—ã¦æ›¸ã‘ã‚‹ã“ã¨ã‚’ç¤ºã›ã€‚

<details><summary>ç­”ãˆ</summary>

çµŒé¨“åˆ†å¸ƒ $\hat{p}(x) = \frac{1}{N}\sum_i \delta(x - x_i)$ ã‚’ä½¿ã†ã¨:

$$
D_{KL}(\hat{p} \| p_\theta) = \sum_x \hat{p}(x) \log \frac{\hat{p}(x)}{p_\theta(x)} = \underbrace{H(\hat{p})}_{\text{å®šæ•°}} - \mathbb{E}_{\hat{p}}[\log p_\theta(x)]
$$

ã—ãŸãŒã£ã¦ $\max_\theta \frac{1}{N}\sum_i \log p_\theta(x_i) \iff \min_\theta D_{KL}(\hat{p} \| p_\theta)$ã€‚
</details>

**ãƒã‚§ãƒƒã‚¯ 3**: Fisher æƒ…å ±é‡ $I(\theta) = -\mathbb{E}[\partial^2 \log p_\theta / \partial \theta^2]$ ã«ã¤ã„ã¦ã€$\mathcal{N}(\mu, \sigma^2)$ ã§ã® $I(\mu)$ ã‚’æ±‚ã‚ã‚ˆï¼ˆ$\sigma^2$ ã¯æ—¢çŸ¥ï¼‰ã€‚

<details><summary>ç­”ãˆ</summary>

$$
\log p(x|\mu) = -\frac{(x-\mu)^2}{2\sigma^2} + \text{const}
$$

$$
\frac{\partial \log p}{\partial \mu} = \frac{x-\mu}{\sigma^2}, \quad \frac{\partial^2 \log p}{\partial \mu^2} = -\frac{1}{\sigma^2}
$$

$$
I(\mu) = -\mathbb{E}\left[-\frac{1}{\sigma^2}\right] = \frac{1}{\sigma^2}
$$

CramÃ©r-Rao ä¸‹ç•Œ: $\text{Var}(\hat{\mu}) \geq \sigma^2/N$ï¼ˆ$N$ ã‚µãƒ³ãƒ—ãƒ«ã®å ´åˆï¼‰ã€‚æ¨™æœ¬å¹³å‡ãŒã“ã‚Œã‚’é”æˆã™ã‚‹ã“ã¨ãŒ Z5.7 ã®æ•°å€¤å®Ÿé¨“ã§ç¢ºèªã§ãã‚‹ã€‚
</details>

---

> Progress: 85%

## ğŸ”¬ Z5b. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

Z5b ã®ç›®æ¨™: (a) æ•°å¼ã‚’ã€Œèª­ã‚ã‚‹ã€ï¼ˆè¨˜å·ã®æ„å‘³ã¨æ§‹é€ ã‚’æ—¥æœ¬èªã§èª¬æ˜ã§ãã‚‹ï¼‰ã€(b) æ•°å¼ã‚’ã€Œæ›¸ã‘ã‚‹ã€ï¼ˆLaTeX ã§æ­£ç¢ºã«å†ç¾ã§ãã‚‹ï¼‰ã€(c) æ•°å¼ã‚’ã€Œå®Ÿè£…ã§ãã‚‹ã€ï¼ˆã‚³ãƒ¼ãƒ‰ã¨1:1å¯¾å¿œã§ãã‚‹ï¼‰ã€‚ã“ã®3å±¤ãŒæƒã£ã¦ã¯ã˜ã‚ã¦ã€Œç†è§£ã—ãŸã€ã¨è¨€ãˆã‚‹ã€‚

### Z5b.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

<details><summary>Q1: $\hat{\theta}_{MLE} = \arg\max_\theta \prod_{i=1}^N p_\theta(x_i)$</summary>

**èª­ã¿æ–¹**: ã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆ ã‚µãƒ– ã‚¨ãƒ ã‚¨ãƒ«ã‚¤ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¢ãƒ¼ã‚°ãƒãƒƒã‚¯ã‚¹ ã‚·ãƒ¼ã‚¿ ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ ã‚¢ã‚¤ ã‚¤ã‚³ãƒ¼ãƒ«1 ãƒˆã‚¥ãƒ¼ N ãƒ”ãƒ¼ã‚µãƒ–ã‚·ãƒ¼ã‚¿ ã‚¨ãƒƒã‚¯ã‚¹ã‚¢ã‚¤

**æ„å‘³**: æœ€å°¤æ¨å®šé‡ã®å®šç¾©ã€‚ãƒ‡ãƒ¼ã‚¿ $\mathcal{D}$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€ãã®ç”Ÿèµ·ç¢ºç‡ï¼ˆå°¤åº¦ï¼‰ã‚’æœ€å¤§ã«ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ã€‚ç©ã¯è¨ˆç®—ä¸Šå¯¾æ•°å’Œã«å¤‰æ›ï¼ˆlog-likelihoodï¼‰ã€‚
</details>

<details><summary>Q2: $I(\theta) = \mathbb{E}_{p_\theta}\left[\left(\frac{\partial \log p_\theta(x)}{\partial \theta}\right)^2\right]$</summary>

**èª­ã¿æ–¹**: ã‚¢ã‚¤ ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ã‚¯ã‚¹ãƒšã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ”ãƒ¼ã‚µãƒ–ã‚·ãƒ¼ã‚¿ ãƒ–ãƒ©ã‚±ãƒƒãƒˆ ãƒ‘ãƒ¼ã‚·ãƒ£ãƒ« ãƒ­ã‚° ãƒ”ãƒ¼ã‚µãƒ–ã‚·ãƒ¼ã‚¿ ã‚¨ãƒƒã‚¯ã‚¹ ãƒ‘ãƒ¼ã‚·ãƒ£ãƒ« ã‚·ãƒ¼ã‚¿ ã‚¹ã‚¯ã‚¨ã‚¢ãƒ¼ãƒ‰

**æ„å‘³**: Fisher æƒ…å ±é‡ã€‚ã‚¹ã‚³ã‚¢é–¢æ•°ï¼ˆå¯¾æ•°å°¤åº¦ã®å‹¾é…ï¼‰ã®åˆ†æ•£ã€‚$I(\theta)$ ãŒå¤§ãã„ã»ã© $\theta$ ä»˜è¿‘ã®å°¤åº¦ã®ã€Œé‹­ã•ã€ãŒé«˜ãã€æ¨å®šç²¾åº¦ã®ä¸Šé™ãŒé«˜ã„ã€‚CramÃ©r-Rao ä¸‹ç•Œ $\text{Var}(\hat{\theta}) \geq 1/I(\theta)$ ã‚’ä¸ãˆã‚‹ã€‚
</details>

<details><summary>Q3: $\sqrt{N}(\hat{\theta}_{MLE} - \theta_0) \xrightarrow{d} \mathcal{N}(0, I(\theta_0)^{-1})$</summary>

**æ„å‘³**: MLE ã®æ¼¸è¿‘æ­£è¦æ€§ã€‚çœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta_0$ å‘¨ã‚Šã§ã€$\sqrt{N}$ ã§ã‚¹ã‚±ãƒ¼ãƒ«ã—ãŸ MLE ã¯æ¼¸è¿‘çš„ã«ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«åæŸã€‚å…±åˆ†æ•£ã¯ $I(\theta_0)^{-1}$ = CramÃ©r-Rao ä¸‹ç•Œã‚’é”æˆï¼ˆæ¼¸è¿‘æœ‰åŠ¹æ€§ï¼‰ã€‚
</details>

<details><summary>Q4: $D_{KL}(p_{\text{data}} \| p_\theta) = H(p_{\text{data}}, p_\theta) - H(p_{\text{data}})$</summary>

**æ„å‘³**: KL = Cross-Entropy - ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€‚$H(p_{\text{data}})$ ã¯å®šæ•°ãªã®ã§ã€KL æœ€å°åŒ– âŸº Cross-Entropy æœ€å°åŒ– âŸº MLEã€‚ä¸‰ä½ä¸€ä½“ã®æ ¸å¿ƒã€‚
</details>

<details><summary>Q5: $s_\theta(x) = \nabla_x \log p_\theta(x)$</summary>

**èª­ã¿æ–¹**: ã‚¹ã‚³ã‚¢ ã‚µãƒ–ã‚·ãƒ¼ã‚¿ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¤ã‚³ãƒ¼ãƒ« ã‚°ãƒ©ã‚¸ã‚§ãƒ³ãƒˆ ã‚µãƒ–ã‚¨ãƒƒã‚¯ã‚¹ ãƒ­ã‚° ãƒ”ãƒ¼ã‚µãƒ–ã‚·ãƒ¼ã‚¿ ã‚¨ãƒƒã‚¯ã‚¹

**æ„å‘³**: ã‚¹ã‚³ã‚¢é–¢æ•°ï¼ˆscore functionï¼‰ã€‚å¯¾æ•°å°¤åº¦ã®å…¥åŠ› $x$ ã«é–¢ã™ã‚‹å‹¾é…ã€‚$p_\theta(x)$ ã®æ­£è¦åŒ–å®šæ•°ãŒä¸è¦ãªãŸã‚ã€è¨ˆç®—ã§ããªã„åˆ†å¸ƒã§ã‚‚ã‚¹ã‚³ã‚¢é–¢æ•°ã¯æ¨å®šã§ãã‚‹ã€‚Score Matching ã®æ ¸å¿ƒçš„ã‚¢ã‚¤ãƒ‡ã‚¢ã€‚
</details>

<details><summary>Q6: $z = f_\theta^{-1}(x),\; \log p_\theta(x) = \log p_z(z) + \log|\det J_{f^{-1}}(x)|$</summary>

**æ„å‘³**: Normalizing Flow ã®å¤‰æ•°å¤‰æ›å…¬å¼ã€‚$f_\theta$ ãŒå¯é€†å¤‰æ›ï¼ˆflowï¼‰ã®ã¨ãã€$x$ ã§ã®å¯†åº¦ã¯åŸºåº•åˆ†å¸ƒã§ã®å¯†åº¦ï¼‹Jacobian ã®å¯¾æ•°è¡Œåˆ—å¼ã§è¨ˆç®—ã§ãã‚‹ã€‚Jacobian ãŒã€Œä½“ç©å¤‰åŒ–ç‡ã€ã‚’è£œæ­£ã™ã‚‹ã€‚
</details>

<details><summary>Q7: $\hat{p}_{data}(x) = \frac{1}{N}\sum_{i=1}^N \delta(x - x_i)$</summary>

**æ„å‘³**: çµŒé¨“åˆ†å¸ƒï¼ˆempirical distributionï¼‰ã€‚è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $N$ ç‚¹ã‚’ç­‰é‡ã¿ã®ãƒ‡ãƒ«ã‚¿é–¢æ•°ã§è¡¨ã—ãŸåˆ†å¸ƒã€‚$N \to \infty$ ã§çœŸã® $p_{data}(x)$ ã«å¼±åæŸã™ã‚‹ã€‚MLE ã¯ $KL(\hat{p}_{data} \| p_\theta)$ ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã¨ç­‰ä¾¡ã€‚
</details>

### Z5b.2 LaTeX ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ

<details><summary>Q1: MLE ã®ä¸‰ä½ä¸€ä½“ï¼ˆ3ã¤ã®ç­‰ä¾¡ãªç›®çš„é–¢æ•°ï¼‰</summary>

$$
\hat{\theta}_{MLE} = \arg\max_\theta \sum_{i=1}^N \log p_\theta(x_i)
= \arg\min_\theta H(p_{\text{data}}, p_\theta)
= \arg\min_\theta D_{KL}(p_{\text{data}} \| p_\theta)
$$
</details>

<details><summary>Q2: CramÃ©r-Rao ä¸‹ç•Œï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ç‰ˆï¼‰</summary>

$$
\text{Var}(\hat{\theta}) \geq \frac{1}{I(\theta)} = \left(\mathbb{E}\left[\left(\frac{\partial \log p_\theta}{\partial \theta}\right)^2\right]\right)^{-1}
$$
</details>

<details><summary>Q3: Score Matching ç›®çš„é–¢æ•°ï¼ˆç©åˆ†by partså¾Œï¼‰</summary>

$$
J(\theta) = \mathbb{E}_{p_{\text{data}}}\left[\frac{1}{2}\|s_\theta(x)\|^2 + \text{tr}(\nabla_x s_\theta(x))\right]
$$
</details>

<details><summary>Q4: FID ã®å®šç¾©ï¼ˆã‚¬ã‚¦ã‚¹è¿‘ä¼¼ï¼‰</summary>

$$
\text{FID}(p_r, p_g) = \|\mu_r - \mu_g\|_2^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)
$$
</details>

<details><summary>Q5: MAP æ¨å®šã®ç›®çš„é–¢æ•°ï¼ˆã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒï¼‰</summary>

$$
\hat{\theta}_{MAP} = \arg\max_\theta \left[\sum_{i=1}^N \log p_\theta(x_i) - \frac{\|\theta\|^2}{2\tau^2}\right]
= \arg\min_\theta \left[-\sum_{i=1}^N \log p_\theta(x_i) + \frac{\lambda}{2}\|\theta\|^2\right]
$$

ã“ã“ã§ $\lambda = 1/\tau^2$ ã¯ L2 æ­£å‰‡åŒ–ä¿‚æ•°ã€‚
</details>

<details><summary>Q6: Rejection Sampling ã®å—ç†ç¢ºç‡</summary>

$$
\Pr(\text{accept}) = \frac{p(x)}{M q(x)}, \quad \mathbb{E}[\text{accept}] = \frac{1}{M}
$$

ã‚ˆã‚Šæ­£ç¢ºã«: $\int \frac{p(x)}{Mq(x)} q(x)dx = \frac{1}{M}\int p(x)dx = \frac{1}{M}$
</details>

<details><summary>Q7: Importance Sampling æ¨å®šé‡</summary>

$$
\mathbb{E}_{p(x)}[f(x)] = \int f(x) p(x) dx = \int f(x) \frac{p(x)}{q(x)} q(x) dx \approx \frac{1}{N} \sum_{i=1}^N f(x_i) w_i
$$

$w_i = p(x_i) / q(x_i)$ ã¯é‡ã¿ï¼ˆimportance weightï¼‰ã€‚
</details>

<details><summary>Q8: Normalizing Flow ã®å¤‰æ•°å¤‰æ›å…¬å¼</summary>

$$
\log p_\theta(x) = \log p_z(f_\theta^{-1}(x)) + \log \left|\det \frac{\partial f_\theta^{-1}}{\partial x}\right|
$$

1æ¬¡å…ƒã‚¢ãƒ•ã‚£ãƒ³å¤‰æ› $x = \mu + \sigma z$ ã®å ´åˆ: $\log p_\theta(x) = \log \mathcal{N}\left(\frac{x-\mu}{\sigma}; 0, 1\right) - \log \sigma$
</details>

### Z5b.3 æ•°å¼ç¿»è¨³ãƒ†ã‚¹ãƒˆ

<details><summary>Q1: MLE ã®æœ€é©åŒ–å•é¡Œã‚’æ•°å¼ã§æ›¸ã‘</summary>

$$
\hat{\theta}_{MLE} = \arg\max_{\theta} \sum_{i=1}^N \log p_\theta(x_i)
                   = \arg\min_{\theta} \underbrace{-\frac{1}{N}\sum_{i=1}^N \log p_\theta(x_i)}_{\text{NLL}(\theta)}
$$

$\log$ ã®å˜èª¿æ€§ã«ã‚ˆã‚Š $\arg\max$ ã¯ $\arg\min \mathrm{NLL}$ ã¨ç­‰ä¾¡ã€‚$\mathrm{NLL} \geq 0$ï¼ˆç¢ºç‡å¯†åº¦ãªã®ã§ $p_\theta(x) \leq 1$ ã¨ã¯é™ã‚‰ãªã„ãŒã€é€£ç¶šåˆ†å¸ƒã®å ´åˆã¯è‡ªç„¶ã«NLLãŒæ­£ã«ãªã‚‹ã“ã¨ãŒå¤šã„ï¼‰ã€‚

</details>

<details><summary>Q2: Fisher æƒ…å ±é‡ã®å®šç¾©ã¨è§£é‡ˆ</summary>

$$
I(\theta) = \mathbb{E}_{p_\theta(x)}\!\left[\left(\frac{\partial \log p_\theta(x)}{\partial \theta}\right)^2\right]
           = -\mathbb{E}_{p_\theta(x)}\!\left[\frac{\partial^2 \log p_\theta(x)}{\partial \theta^2}\right]
$$

ã‚¹ã‚³ã‚¢ $s(\theta; x) = \partial_\theta \log p_\theta(x)$ ã®åˆ†æ•£ = $I(\theta)$ã€‚ã‚¹ã‚³ã‚¢ã®æœŸå¾…å€¤ã¯ã‚¼ãƒ­ï¼ˆ$\mathbb{E}[s] = 0$ï¼‰ãªã®ã§ã€Fisher æƒ…å ±é‡ã¯ã€Œã‚¹ã‚³ã‚¢ã®ãƒãƒ©ã¤ãã€ãã®ã‚‚ã®ã€‚CRB: $\mathrm{Var}[\hat{\theta}] \geq 1/I(\theta)$ã€‚

</details>

<details><summary>Q3: Forward KL ã‚’ç©åˆ†å½¢å¼ã§æ›¸ã‘</summary>

$$
D_{KL}(p \| q_\theta) = \int p(x) \log\frac{p(x)}{q_\theta(x)}\,dx
                       = \underbrace{\mathbb{E}_{p}[\log p(x)]}_{\text{å®šæ•°ï¼ˆ}\theta\text{ã«ã‚ˆã‚‰ãªã„ï¼‰}} - \mathbb{E}_{p}[\log q_\theta(x)]
$$

$\theta$ ã«é–¢ã™ã‚‹æœ€å°åŒ–ã§ã¯å®šæ•°é …ã‚’ç„¡è¦–ã§ãã‚‹ã®ã§ã€$\min_\theta D_{KL}(p\|q_\theta) \Leftrightarrow \max_\theta \mathbb{E}_p[\log q_\theta(x)]$ = MLEï¼ˆ$p$ ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã¨è¦‹ãªã›ã°ï¼‰ã€‚

</details>

<details><summary>Q4: MAP æ¨å®šã®ç›®çš„é–¢æ•°ï¼ˆã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒã®å ´åˆï¼‰</summary>

$$
\hat{\theta}_{MAP} = \arg\max_\theta \underbrace{\sum_{i=1}^N \log p_\theta(x_i)}_{\text{å¯¾æ•°å°¤åº¦}} + \underbrace{\log p(\theta)}_{\text{å¯¾æ•°äº‹å‰åˆ†å¸ƒ}}
$$

ã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒ $p(\theta) = \mathcal{N}(0, \tau^2 I)$ ã®ã¨ã:

$$
\log p(\theta) = -\frac{1}{2\tau^2}\|\theta\|^2 + \mathrm{const}
$$

ã‚ˆã£ã¦ MAP = NLL + L2 æ­£å‰‡åŒ–($\lambda = 1/\tau^2$)ã€‚$\tau^2 \to \infty$ï¼ˆäº‹å‰åˆ†å¸ƒãŒç„¡æƒ…å ±ï¼‰ãªã‚‰ MAP â†’ MLEã€‚

</details>

<details><summary>Q5: Reparameterization Trick ã®æ•°å¼</summary>

$$
z \sim q_\phi(z|x) = \mathcal{N}(\mu_\phi(x),\, \sigma_\phi^2(x) I)
\quad\Rightarrow\quad
z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$$

è¨˜å·â†”å¤‰æ•°: $\mu_\phi$ = `mu`, $\sigma_\phi = \exp(\texttt{log\_sigma})$, $\epsilon$ = `eps`ã€‚Shape: `(batch, latent_dim)`ã€‚å‹¾é…ã¯ $\mu_\phi, \sigma_\phi$ ã‚’é€šã˜ã¦æµã‚Œã‚‹; $\epsilon$ ã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒ¼ãƒ‰ãªã®ã§åˆ‡æ–­ã€‚

</details>

<details><summary>Q6: Normalizing Flow ã®å¤‰æ•°å¤‰æ›å…¬å¼</summary>

$$
\log p_\theta(x) = \log p_z(f_\theta^{-1}(x)) + \log\left|\det \frac{\partial f_\theta^{-1}}{\partial x}\right|
$$

1æ¬¡å…ƒã‚¢ãƒ•ã‚£ãƒ³ $f: z \mapsto \mu + \sigma z$ ãªã‚‰ $f^{-1}(x) = (x-\mu)/\sigma$ã€Jacobian = $1/\sigma$ã€ã‚ˆã£ã¦:

$$
\log p_\theta(x) = \log \mathcal{N}\!\left(\frac{x-\mu}{\sigma}; 0, 1\right) - \log \sigma
$$

dæ¬¡å…ƒã¸ã®ä¸€èˆ¬åŒ–ã¯ $\log|\det J|$ ã®åŠ¹ç‡çš„è¨ˆç®—ï¼ˆRealNVP ãªã‚‰ $O(d)$ã€ä¸€èˆ¬è¡Œåˆ—ãªã‚‰ $O(d^3)$ï¼‰ãŒéµã€‚

</details>

### Z5b.4 è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**ç†è«–ãƒã‚§ãƒƒã‚¯ï¼ˆPart1 å†…å®¹ï¼‰**
- [ ] MLE ã®å®šç¾©ï¼ˆæœ€å°¤æ¨å®šé‡ã®å¼ï¼‰ã‚’æ›¸ã‘ã‚‹
- [ ] $\log \prod p_\theta(x_i) = \sum \log p_\theta(x_i)$ ã®å¤‰æ›ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] MLE = Cross-Entropy æœ€å°åŒ–ã®ç­‰ä¾¡æ€§è¨¼æ˜ã‚’å†ç¾ã§ãã‚‹
- [ ] MLE = KL æœ€å°åŒ–ã®ç­‰ä¾¡æ€§è¨¼æ˜ã‚’å†ç¾ã§ãã‚‹
- [ ] Fisher æƒ…å ±é‡ã®å®šç¾©ã‚’æ›¸ã‘ã‚‹
- [ ] CramÃ©r-Rao ä¸‹ç•Œã‚’ Fisher æƒ…å ±é‡ã§è¡¨ç¾ã§ãã‚‹
- [ ] æ¼¸è¿‘æ­£è¦æ€§ï¼ˆ$\sqrt{N}(\hat{\theta} - \theta_0) \to \mathcal{N}(0, I^{-1})$ï¼‰ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Forward KL ã¨ Reverse KL ã®é•ã„ã‚’æœŸå¾…å€¤ã®è¦³ç‚¹ã‹ã‚‰èª¬æ˜ã§ãã‚‹
- [ ] Mode-Covering (VAE) ã¨ Mode-Seeking (GAN) ã®æ•°å€¤çš„æŒ™å‹•ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Score Matching ãŒå°¤åº¦ä¸è¦ãªç†ç”±ï¼ˆç©åˆ† by partsï¼‰ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Rejection Sampling ã®å—ç†ç‡ãŒ $1/M$ ã«æ¯”ä¾‹ã™ã‚‹ã“ã¨ã‚’èª¬æ˜ã§ãã‚‹
- [ ] FID ã®ã€Œè¡Œåˆ—å¹³æ–¹æ ¹ã€ãŒå¿…è¦ãªç†ç”±ã‚’èª¬æ˜ã§ãã‚‹

**å®Ÿè£…ãƒã‚§ãƒƒã‚¯ï¼ˆPart2 å†…å®¹ï¼‰**
- [ ] `log_prob` é–¢æ•°ãŒ `sum(log p_theta(x_i))` ã‚’æ­£ã—ãè¿”ã™ã“ã¨ã‚’æ¤œç®—ã§ãã‚‹
- [ ] Forward KL ã¨ Reverse KL ã‚’åŒã˜ãƒ‡ãƒ¼ã‚¿ã§æ•°å€¤æœ€é©åŒ–ã—ã¦å·®ã‚’ç¢ºèªã§ãã‚‹
- [ ] Z5.7 ã® CramÃ©r-Rao æ¤œè¨¼ã§ Fisher æƒ…å ±é‡ã®é€†æ•° â‰¤ æ¨™æœ¬åˆ†æ•£ã‚’ç¢ºèªã§ãã‚‹
- [ ] Z5.11 ã® MAP æ¨å®šã§ `tau2 â†’ âˆ` ã®ã¨ã MLE ã«åæŸã™ã‚‹ã“ã¨ã‚’ç¢ºèªã§ãã‚‹
- [ ] Z5.12 ã® Reparameterization ã§ shape `(batch, latent_dim)` ã‚’æ­£ã—ããƒˆãƒ¬ãƒ¼ã‚¹ã§ãã‚‹
- [ ] Z5.13 ã®ã‚¢ãƒ•ã‚£ãƒ³ NF ã§ MLE ã®è§£æè§£ã¨æ•°å€¤è§£ãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã§ãã‚‹
- [ ] Z5.14 ã®æ¯”è¼ƒå®Ÿé¨“ã§ NF-affine ãŒäºŒå³°ãƒ‡ãƒ¼ã‚¿ã§å¤±æ•—ã™ã‚‹ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹

**é«˜åº¦ãªãƒã‚§ãƒƒã‚¯ï¼ˆä½™è£•ãŒã‚ã‚‹äººå‘ã‘ï¼‰**
- [ ] $p(x) = \int p(x|z)p(z)dz$ ãŒè§£æä¸å¯èƒ½ãªä¾‹ã‚’3ã¤æŒ™ã’ã‚‰ã‚Œã‚‹
- [ ] MLE ãŒã€Œã‚¬ã‚¦ã‚¹æ—ã€ã§ã¯ãªã„åˆ†å¸ƒï¼ˆä¾‹: Cauchyï¼‰ã«å¯¾ã—ã¦ä½•ãŒèµ·ãã‚‹ã‹ã‚’èª¬æ˜ã§ãã‚‹
- [ ] FID ã® sampling bias $O(1/n_g)$ ã¨ ãã®è£œæ­£æ–¹æ³•ã‚’èª¬æ˜ã§ãã‚‹

**æ¡ç‚¹åŸºæº–**: 17å•ä»¥ä¸Š âœ… å®Œå…¨ç¿’å¾— | 12-16å• ğŸ¦€ è‹¦æ‰‹åˆ†é‡ã‚’å†å®Ÿè£… | 11å•æœªæº€ ğŸ“š Z5 å…¨ä½“ã‚’å†å®Ÿæ–½

### Z5b.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” GMM MLE vs EM ã®åæŸæ¯”è¼ƒ

Z5.4 ã® gradient-based MLE ã¨ç¬¬8å›ã®EM ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¯”è¼ƒã™ã‚‹ãŸã‚ã®æº–å‚™å®Ÿé¨“ã€‚

**ã‚¿ã‚¹ã‚¯**: `n=500` ã®2å³°ãƒ‡ãƒ¼ã‚¿ï¼ˆ`N(-3,1)` ã¨ `N(3,1)` ã®ç­‰é‡ã¿æ··åˆï¼‰ã§ GMM MLE ã‚’å®Ÿè¡Œã—:
1. åˆæœŸå€¤ `p0 = [0, 0, 0, 0, 0]` ã‹ã‚‰ã®åæŸã‚’ç¢ºèª
2. `optimize.minimize` ã®åå¾©å›æ•° (`res.nit`) ã‚’è¨˜éŒ²
3. ã€Œæ½œåœ¨å¤‰æ•°ãªã—ã§ gradient descent ã¯ãªãœé›£ã—ã„ã‹ã€ã‚’è€ƒå¯Ÿ

**è€ƒå¯Ÿã®æŒ‡é‡**: GMM ã®å¯¾æ•°å°¤åº¦é–¢æ•°:

$$
\ell(\theta) = \sum_{i=1}^N \log\left[\pi_1 \mathcal{N}(x_i|\mu_1, \sigma_1^2) + \pi_2 \mathcal{N}(x_i|\mu_2, \sigma_2^2)\right]
$$

ã¯ $(\mu_1, \mu_2)$ ã®å…¥ã‚Œæ›¿ãˆã«å¯¾ã—ã¦å¯¾ç§°ãªãŸã‚ã€$\hat{\mu}_1 = -3, \hat{\mu}_2 = 3$ ã¨ $\hat{\mu}_1 = 3, \hat{\mu}_2 = -3$ ã®2ã¤ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«æœ€å¤§å€¤ãŒã‚ã‚‹ã€‚$\sigma_k \to 0$ ã®ã¨ãã€Œ1ç‚¹ã«é›†ä¸­ã—ãŸæˆåˆ†ã€ãŒå°¤åº¦ã‚’ $+\infty$ ã«ã§ãã‚‹ç¸®é€€è§£ï¼ˆdegenerate solutionï¼‰ãŒå­˜åœ¨ã™ã‚‹ã€‚

**åˆæœŸå€¤ä¾å­˜æ€§ã®æ•°å€¤è¨¼æ‹ **: åˆæœŸå€¤ã‚’å¤‰ãˆã‚‹ã¨ç•°ãªã‚‹è§£ã«åæŸã™ã‚‹:

| åˆæœŸ $(\mu_1, \mu_2)$ | åæŸå…ˆ $(\hat{\mu}_1, \hat{\mu}_2)$ |
|:---|:---|
| `(0, 0)` | `(-3, 3)` or `(3, -3)` ã©ã¡ã‚‰ã‹ï¼ˆä¸å®šï¼‰ |
| `(-1, 1)` | `(-3, 3)` ã«ãªã‚Šã‚„ã™ã„ |
| `(1, -1)` | `(3, -3)` ã«ãªã‚Šã‚„ã™ã„ |
| `(-4, 4)` | `(-3, 3)` å®‰å®š |

**EM ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã®å·®ç•°**: EM ã® E-step ã¯ã€Œå„ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒå„æˆåˆ†ã«å±ã™ã‚‹ç¢ºç‡ï¼ˆè²¬ä»»åº¦ï¼‰ã€ã‚’è¨ˆç®—ã—ã€M-step ã¯ãã®ç¢ºç‡ã«é‡ã¿ä»˜ã‘ã—ã¦å„æˆåˆ†ã‚’ç‹¬ç«‹ã«æœ€é©åŒ–ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å¯¾ç§°æ€§ã®ç½ ã‚’é¿ã‘ã‚„ã™ããªã‚‹ã€‚EM ã® M-step ã§ã¯ $\sigma_k \to 0$ ã®ç¸®é€€ãŒèµ·ããªã„ã‚ˆã†ã€å„æˆåˆ†ãŒå°‘ãªãã¨ã‚‚1ç‚¹ã‚’æ‹…å½“ã™ã‚‹ã“ã¨ã‚’ä¿è¨¼ã§ãã‚‹ã€‚

**æœŸå¾…ã•ã‚Œã‚‹çµè«–**: GMM ã®å°¤åº¦ã¯å¤šå³°çš„ãªãŸã‚ã€å‹¾é…æ³•ã¯åˆæœŸå€¤ä¾å­˜ã€‚EM ã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã§å˜èª¿å¢—åŠ ãŒä¿è¨¼ã•ã‚Œã‚‹ï¼ˆJensen ä¸ç­‰å¼ï¼‰â€” ã“ã‚ŒãŒç¬¬8å›ã®å‹•æ©Ÿã€‚

### Z5b.6 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ â€” KDE (Kernel Density Estimation) ã¨ã®æ¯”è¼ƒ

MLE ã®ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ä»£æ›¿ã§ã‚ã‚‹ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ KDE ã®æ•°å­¦çš„æœ¬è³ªã‚’ç†è§£ã™ã‚‹ã€‚

$$
\hat{p}_{KDE}(x) = \frac{1}{Nh} \sum_{i=1}^N K\left(\frac{x - x_i}{h}\right), \quad K(u) = \frac{1}{\sqrt{2\pi}} e^{-u^2/2}
$$

è¨˜å·â†”å¤‰æ•°: $h$ = bandwidth, $K$ = Gaussian kernel, $N$ = ãƒ‡ãƒ¼ã‚¿æ•°, $\hat{p}_{KDE}$ = æ¨å®šå¯†åº¦ã€‚

**bandwidth ã®å½±éŸ¿**:
- $h \to 0$: å„ãƒ‡ãƒ¼ã‚¿ç‚¹ã«ãƒ‡ãƒ«ã‚¿é–¢æ•°ï¼ˆéå­¦ç¿’ â€” è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã¯å®Œç’§ã€æœªè¦‹ãƒ‡ãƒ¼ã‚¿ã§ã¯å¤±æ•—ï¼‰
- $h \to \infty$: å‡ä¸€åˆ†å¸ƒï¼ˆéå¹³æ»‘åŒ–ï¼‰
- æœ€é© $h$ï¼ˆSilvermanå‰‡ï¼‰: $h^* = \left(\frac{4\hat{\sigma}^5}{3N}\right)^{1/5} \approx 1.06\hat{\sigma}N^{-1/5}$

$N=400$ ãƒ‡ãƒ¼ã‚¿ã€$\hat{\sigma} = 3$ ãªã‚‰ $h^* \approx 1.06 \times 3 \times 400^{-0.2} \approx 0.97$ã€‚

**æ•°å€¤ç¢ºèª**: ä»»æ„ã® $h > 0$ ã«å¯¾ã—ã¦ $\int \hat{p}_{KDE}(x)\,dx = 1$ ãŒæˆç«‹ã™ã‚‹ï¼ˆå„ã‚«ãƒ¼ãƒãƒ«ã®ç©åˆ†ãŒ1ãªã®ã§ç·å’Œã‚‚1ï¼‰ã€‚h=0.2, 0.5, 2.0 ã®ã„ãšã‚Œã§ã‚‚ç©åˆ†ã¯ `â‰ˆ 1.000`ã€‚

KDE ã®æ ¹æœ¬çš„ãªé™ç•Œ: è©•ä¾¡æ™‚ã®ã‚³ã‚¹ãƒˆãŒ $O(N)$ï¼ˆNç‚¹å…¨ã¦ã¨ã®è·é›¢è¨ˆç®—ï¼‰ã€å¯†åº¦æ¨å®šãŒ $O(N^2)$ ãƒ¡ãƒ¢ãƒªï¼ˆå·®åˆ†è¡Œåˆ—ï¼‰ã€‚é«˜æ¬¡å…ƒã§ã¯ã€Œæ¬¡å…ƒã®å‘ªã„ã€ã§æœ‰åŠ¹ãªãƒãƒ³ãƒ‰å¹…ãŒæŒ‡æ•°çš„ã«ç¸®å° â€” $d=100$ ãªã‚‰ $h \propto N^{-1/104}$ ã¨åæŸãŒæ¥µã‚ã¦é…ã„ã€‚ã“ã‚ŒãŒæš—é»™çš„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆGANï¼‰ã®å‹•æ©Ÿã ã€‚

### Z5b.7 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

| é …ç›® | å®Œäº† | ãƒ¡ãƒ¢ |
|:-----|:----:|:-----|
| Part1 Z1: GMM MLEé™ç•Œã®ä½“æ„Ÿ | â˜ | |
| Part1 Z2: 5ãƒˆãƒ”ãƒƒã‚¯æ¦‚è¦³ï¼ˆæ¯”è¼ƒè¡¨ï¼‰ | â˜ | |
| Part1 Z3: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€åŸç† | â˜ | |
| Part1 Z4 T1: MLE ä¸‰ä½ä¸€ä½“è¨¼æ˜ | â˜ | |
| Part1 Z4 T2: Fisheræƒ…å ±é‡ãƒ»CRB | â˜ | |
| Part1 Z4 T3: æ˜ç¤ºçš„/æš—é»™çš„MLE | â˜ | |
| Part1 Z4 T4: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç†è«– | â˜ | |
| Part1 Z4 T5: Score Matchingç†è«– | â˜ | |
| Part2 Z5.1: CEæœ€å°åŒ–=MLE ã®å®Ÿè£… | â˜ | |
| Part2 Z5.2: Forward/Reverse KLæ¯”è¼ƒ | â˜ | |
| Part2 Z5.3: FID å®Ÿè£…ï¼ˆè¡Œåˆ—å¹³æ–¹æ ¹ï¼‰ | â˜ | |
| Part2 Z5.4: GMM MLE å®Ÿè£… | â˜ | |
| Part2 Z5.5: Score Matching å®Ÿè£… | â˜ | |
| Part2 Z5.6: Rejection/IS ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | â˜ | |
| Part2 Z5.7: CramÃ©r-Rao æ•°å€¤æ¤œè¨¼ | â˜ | |
| Part2 Z5.8: Mode-Seeking/Coveringå®Ÿé¨“ | â˜ | |
| Part2 Z5.11: MAP vs MLE æ¯”è¼ƒ | â˜ | |
| Part2 Z5.12: Reparameterization Trick | â˜ | |
| Part2 Z5 Quick Check 3å•å…¨æ­£è§£ | â˜ | |
| Part2 Z5b è‡ªå·±ãƒã‚§ãƒƒã‚¯ 11å•ä»¥ä¸Š | â˜ | |
| Part2 Z6 arXivè«–æ–‡ 3æœ¬èª­ã‚“ã  | â˜ | |
| Part2 Z7 FAQ å…¨èª­ã¿ | â˜ | |
| PB: ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„ã«è‡ªåˆ†ã®ç­”ãˆ | â˜ | |

**å®Œäº†ç‡**: `__/23 é …ç›®`

---

> Progress: 85%

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆ20åˆ†ï¼‰â€” çµ±è¨ˆæ¨è«–ã®ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢

æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å…¨å¼•ç”¨ã¯ arXiv è«–æ–‡ã®ã¿ã€‚

```mermaid
graph LR
    A["Fisher 1922<br/>MLEèª•ç”Ÿ"] --> B["CramÃ©r-Rao 1945<br/>æ¨å®šç†è«–å®Œæˆ"]
    B --> C["Kullback-Leibler 1951<br/>KL=MLEæ¥ç¶š"]
    C --> D["HyvÃ¤rinen 2005<br/>Score Matching"]
    D --> E["Ho+ 2020<br/>DDPM"]
    C --> F["Goodfellow+ 2014<br/>GAN"]
    C --> G["Kingma+ 2013<br/>VAE"]
    E --> H["ç¾åœ¨: Flow Matching<br/>Diffusionçµ±ä¸€"]
```

### Z6.1 MLE ã‚’è¶…ãˆã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

**CMMD (Clean Maximum Mean Discrepancy)** [^10]: FID ã®å•é¡Œç‚¹ï¼ˆInception-V3ã®åã‚Šãƒ»ãƒãƒƒãƒã‚µã‚¤ã‚ºä¾å­˜ãƒ»ãƒ©ãƒ³ãƒ€ãƒ å¤‰å‹•å¤§ï¼‰ã‚’æ”¹å–„ã™ã‚‹è©•ä¾¡æŒ‡æ¨™ã€‚

$$
\text{CMMD}(p_r, p_g) = \text{MMD}^2(\phi(p_r), \phi(p_g))
$$

ã“ã“ã§ $\phi$ ã¯ CLIP åŸ‹ã‚è¾¼ã¿ï¼ˆViT-L/14ï¼‰ã€‚Kernel: $k(x,y) = \exp(-\|x-y\|^2/(2\sigma^2))$ï¼ˆRBFï¼‰ã€‚

Heusel ã‚‰ (2017) ã® FID [^8] ãŒ Diffusion ãƒ¢ãƒ‡ãƒ«ã‚’ä¸å½“ã«é«˜ãè©•ä¾¡ã™ã‚‹ã¨ã„ã†å•é¡Œï¼ˆ2023å¹´: NeurIPS ã§éœ²å‘ˆï¼‰ã«å¯¾ã—ã¦ã€Jayasumana ã‚‰ (2024) [^10] ãŒææ¡ˆã€‚å·®åˆ†:
- **FID**: Gaussian è¿‘ä¼¼ â†’ éã‚¬ã‚¦ã‚¹åˆ†å¸ƒã§èª¤å·®å¤§
- **CMMD**: ã‚«ãƒ¼ãƒãƒ«æ³• â†’ åˆ†å¸ƒå½¢çŠ¶ã«ä¾å­˜ã—ãªã„ã€ãƒã‚¤ã‚¢ã‚¹è£œæ­£ä»˜ã

### Z6.2 Simulation-Based Inference (SBI)

ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¯å‹•ã‹ã›ã‚‹ãŒå°¤åº¦ $p(x|\theta)$ ãŒæ›¸ãä¸‹ã›ãªã„ã‚±ãƒ¼ã‚¹ï¼ˆåˆ†å­å‹•åŠ›å­¦ãƒ»æ°—å€™ãƒ¢ãƒ‡ãƒ«ãƒ»ç–«å­¦ï¼‰ã§ã® MLE ä»£æ›¿ã€‚

Cranmer, Brehmer, Louppe (2020) [^11]:

$$
r(x|\theta_0, \theta_1) = \frac{p(x|\theta_0)}{p(x|\theta_1)} \approx \frac{D(x)}{1 - D(x)} \quad \text{(å°¤åº¦æ¯”æ¨å®š)}
$$

ã“ã“ã§ $D(x)$ ã¯ $\theta_0$ vs $\theta_1$ ã®åˆ†é¡å™¨ã®å‡ºåŠ›ã€‚ã“ã‚Œã«ã‚ˆã‚Šæš—é»™çš„å°¤åº¦ãƒ¢ãƒ‡ãƒ«ã§ã‚‚æ¨è«–ãŒå¯èƒ½ã€‚

**æ¥ç¶š**: GAN ã®è­˜åˆ¥å™¨ $D(x)$ ã‚‚å°¤åº¦æ¯”æ¨å®šå™¨ã¨ã—ã¦è§£é‡ˆã§ãã‚‹ï¼ˆç¬¬13å›ï¼‰ã€‚

SBI ãŒç‰¹ã«é‡è¦ãªç†ç”±: ç¾ä»£ã®ç§‘å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç´ ç²’å­ç‰©ç†ãƒ»æ°—å€™ãƒ»å‰µè–¬ï¼‰ã¯ã€Œå‰å‘ãã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯å¯èƒ½ã ãŒå°¤åº¦ã¯è¨ˆç®—ä¸å¯èƒ½ã€ã¨ã„ã†ã‚±ãƒ¼ã‚¹ãŒã»ã¨ã‚“ã©ã ã€‚MLE ã®ä»£ã‚ã‚Šã«å°¤åº¦æ¯”ï¼ˆclassifierï¼‰ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€äº‹å¾Œåˆ†å¸ƒæ¨å®šãŒå¯èƒ½ã«ãªã‚‹ã€‚

### Z6.3 ã‚¹ã‚³ã‚¢é–¢æ•°ã¨ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°

Ho ã‚‰ (2020) ã® DDPM [^12] ã®æ ¸å¿ƒ:

$$
\mathcal{L}_{\text{DDPM}} = \mathbb{E}_{t, x_0, \epsilon}\left[\|\epsilon - \epsilon_\theta(x_t, t)\|^2\right]
$$

ã“ã‚Œã¯ Denoising Score Matching [^7] ã®ä¸€å½¢æ…‹ã€‚$\epsilon_\theta$ ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã¯ $\nabla_{x_t} \log p_t(x_t)$ ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã¨ç­‰ä¾¡ã€‚

**æ¥ç¶šå…ˆ**: Score Matching (Z5.5) â†’ DDPM ã®æå¤±é–¢æ•° â†’ Flow Matching (ç¬¬5å›) â†’ ç¾åœ¨ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®å…¨ã¦ã€‚MLE ã¨ Score Matching ãŒ Diffusion ãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åŸºç›¤ã€‚

### Z6.4 Identifiability ã¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«

Khemakhem ã‚‰ (2020) [^13]:

$$
p_\theta(x) = \int p_\theta(x|z) p(z) dz \quad \text{ã¯ä¸€èˆ¬ã«éè­˜åˆ¥}
$$

éè­˜åˆ¥æ€§ï¼ˆIdentifiabilityï¼‰: ç•°ãªã‚‹ $\theta_1 \neq \theta_2$ ãŒåŒã˜ $p_\theta(x)$ ã‚’ç”Ÿæˆã§ãã‚‹ã€‚ã“ã‚Œã¯ MLE ã®å¿œç”¨ã§ VAE ãŒã€Œæ„å‘³ã®ã‚ã‚‹ã€æ½œåœ¨ç©ºé–“ã‚’å­¦ã¶ã“ã¨ã‚’å¦¨ã’ã‚‹ã€‚

è§£æ±ºç­–: **iVAE** (Identifiable VAE) ã¯è£œåŠ©å¤‰æ•° $u$ï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ãªã©ï¼‰ã‚’ä½¿ã„ã€$p(z|u)$ ã‚’æ¡ä»¶ä»˜ã‘ã‚‹ã“ã¨ã§è­˜åˆ¥å¯èƒ½æ€§ã‚’ä¿è¨¼ã€‚Î²-VAE ã®ç†è«–çš„æ ¹æ‹ ã®ä¸€ã¤ã€‚

### Z6.5 Rate-Distortion Perception ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

Blau & Michaeli (2019) ã®ã€ŒPerception-Distortion Tradeoffã€ [^14]:

$$
\text{ç”Ÿæˆå“è³ªã®é™ç•Œ: } \text{Distortion}(d) + \text{Perception}(p) \leq C(\text{data complexity})
$$

**ç›´æ„Ÿ**: ç”»åƒå¾©å…ƒãƒ¢ãƒ‡ãƒ«ã§ã€Œæ­ªã¿ãŒå°ã•ã„ã€ï¼ˆPSNR é«˜ã„ï¼‰ã¨ã€ŒçŸ¥è¦šå“è³ªãŒé«˜ã„ã€ï¼ˆäººé–“ã®è©•ä¾¡ï¼‰ã¯ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«ã‚ã‚‹ã€‚ã“ã‚Œã¯ Rate-Distortion ç†è«–ï¼ˆç¬¬6å›ï¼‰ã®å¿œç”¨ã€‚

```mermaid
graph TD
    A["é«˜PSNR (ä½æ­ªã¿)<br/>ã¼ã‚„ã‘ãŸç”»åƒ"] --> B["çŸ¥è¦šå“è³ªä½"]
    C["ä½PSNR (é«˜æ­ªã¿)<br/>é®®æ˜ãª hallucination"] --> D["çŸ¥è¦šå“è³ªé«˜"]
    E["ç†æƒ³<br/>ä¸¡æ–¹é«˜ã„"] --> F["ç†è«–çš„ã«ä¸å¯èƒ½<br/>(ãƒ‡ãƒ¼ã‚¿è¤‡é›‘åº¦ã®ä¸Šé™)"]
```

MLE ã¯ã€Œå¹³å‡çš„ãªã€ç”»åƒï¼ˆdistortion æœ€å°åŒ–ï¼‰ã‚’å­¦ã¶å‚¾å‘ãŒã‚ã‚‹ã€‚GAN ã¯ reverse KL ã§ã€Œã‚‰ã—ã„ã€ç”»åƒï¼ˆperception æœ€å¤§åŒ–ï¼‰ã‚’å­¦ã¶ã€‚ã“ã®é•ã„ãŒã¾ã•ã« Forward/Reverse KL ã®é•ã„ã«å¯¾å¿œã™ã‚‹ï¼ˆZ5.8ï¼‰ã€‚

### Z6.6 æœ€æ–°ç ”ç©¶: FD-DINOv2 ã¨è©•ä¾¡æŒ‡æ¨™ã®é€²åŒ–

Inception-V3 ã«åŸºã¥ã FID ã®ä»£æ›¿ã¨ã—ã¦ã€DINOv2 (ViT-L/14) ã‚’ç‰¹å¾´æŠ½å‡ºå™¨ã«ä½¿ã£ãŸè©•ä¾¡æŒ‡æ¨™ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹:

$$
\text{FD-DINOv2}(p_r, p_g) = \|\mu_r^{DINOv2} - \mu_g^{DINOv2}\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2\sqrt{\Sigma_r \Sigma_g})
$$

FID ã¨ã®å·®åˆ†:
- Inception-V3: ImageNet 1k åˆ†é¡ã«ç‰¹åŒ–ã€ç”Ÿæˆå¤šæ§˜æ€§ã‚’ä½è©•ä¾¡ã™ã‚‹å‚¾å‘
- DINOv2: self-supervisedã€å¹¾ä½•å­¦çš„/æ„å‘³çš„ç‰¹å¾´ã‚’ã‚ˆã‚Šè±Šå¯Œã«ã‚­ãƒ£ãƒ—ãƒãƒ£
- CMMD [^10]: ã‚«ãƒ¼ãƒãƒ«æ³•ã§ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ã‚’å›é¿ã€ã‚ˆã‚Šæ±ç”¨çš„

ç¾åœ¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹: è¤‡æ•°æŒ‡æ¨™ï¼ˆFID + IS + CMMD + Human Evaluationï¼‰ã®ç·åˆè©•ä¾¡ã€‚

ãªãŠã€è©•ä¾¡æŒ‡æ¨™ã®é¸æŠã‚‚ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã®ä¸€éƒ¨ã ã€‚FID ãŒä½ã„ã ã‘ã§ã¯ã€Œè‰¯ã„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€ã¨ã¯è¨€ãˆãªã„ â€” ãã‚Œã¯ Inception-V3 ã®åŸ‹ã‚è¾¼ã¿ç©ºé–“ã§ã®é¡ä¼¼æ€§ã‚’æ„å‘³ã™ã‚‹ã«éããªã„ã€‚æœ€çµ‚çš„ã«ã¯ã€ç”Ÿæˆã•ã‚ŒãŸç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆ/éŸ³å£°ãŒã€Œäººé–“ã®ç›®çš„ã«åˆã£ã¦ã„ã‚‹ã‹ã€ãŒå•é¡Œã§ã‚ã‚Šã€ã“ã‚Œã¯ã‚¿ã‚¹ã‚¯ä¾å­˜ã®è©•ä¾¡ï¼ˆä¾‹: ç”Ÿæˆç”»åƒã‚’ç”¨ã„ãŸ downstream åˆ†é¡ç²¾åº¦ï¼‰ã§æ¸¬ã‚‹ã“ã¨ãŒå¤šã„ã€‚

> Progress: 95%

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. æœ€å°¤æ¨å®šé‡ $\hat{\theta}_{\text{MLE}} = \arg\max_\theta \log p(\mathcal{D}|\theta)$ ãŒãƒã‚¤ã‚¢ã‚¹ã‚’æŒã¤å ´åˆã®å…·ä½“ä¾‹ã‚’æŒ™ã’ã€ãªãœãƒã‚¤ã‚¢ã‚¹ãŒç”Ÿã˜ã‚‹ã‹èª¬æ˜ã›ã‚ˆã€‚
> 2. ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼æƒ…å ±é‡ $\mathcal{I}(\theta) = \mathbb{E}\left[\left(\frac{\partial \log p(x|\theta)}{\partial \theta}\right)^2\right]$ ãŒæ¨å®šã®ç²¾åº¦é™ç•Œï¼ˆã‚¯ãƒ©ãƒ¡ãƒ¼ãƒ«ãƒ»ãƒ©ã‚ªä¸‹ç•Œï¼‰ã«ã©ã†é–¢ä¿‚ã™ã‚‹ã‹è¿°ã¹ã‚ˆã€‚

## ğŸ¯ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆ10åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### Z7.0 çŸ¥è­˜ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—

```mermaid
mindmap
  root((ç¬¬7å› MLE))
    å®šç¾©ã¨ç­‰ä¾¡æ€§
      MLEå®šç¾© Fisher 1922
      CEæœ€å°åŒ–ç­‰ä¾¡
      KLæœ€å°åŒ–ç­‰ä¾¡
      ä¸‰ä½ä¸€ä½“è¨¼æ˜
    æ¼¸è¿‘è«–
      Fisheræƒ…å ±é‡
      CramÃ©r-Raoä¸‹ç•Œ
      æ¼¸è¿‘æ­£è¦æ€§
      ä¸€è‡´æ€§ æœ‰åŠ¹æ€§
    å°¤åº¦ã®å½¢æ…‹
      æ˜ç¤ºçš„ NF/Flow
      æš—é»™çš„ GAN
      ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°
      Diffusionæ¥ç¶š
    ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç†è«–
      Rejection Sampling
      Importance Sampling
      MCMC
      Reparameterization
    è©•ä¾¡æŒ‡æ¨™
      FIDè¡Œåˆ—å¹³æ–¹æ ¹
      CMMD CLIP
      NLL Perplexity
    ç”Ÿæˆãƒ¢ãƒ‡ãƒ«çµ±ä¸€
      MLEå¤‰å½¢ã¨ã—ã¦
      Forward KL VAE
      Reverse KL GAN
      Score SM Diffusion
    æ•°å€¤å®Ÿè£…
      Log-probå®‰å®šåŒ–
      score matching 1D
      Fisher æ•°å€¤å¾®åˆ†
      ESS å“è³ªæ¤œè¨¼
      Normalizing Flow
```

**å„ãƒãƒ¼ãƒ‰ã®ç¬¬7å›ã¨ã®å¯¾å¿œ**: ã€Œå®šç¾©ã¨ç­‰ä¾¡æ€§ã€â†’ Z5.1-Z5.2 | ã€Œæ¼¸è¿‘è«–ã€â†’ Z5.7 | ã€Œå°¤åº¦ã®å½¢æ…‹ã€â†’ Z5.5, Z5.13 | ã€Œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç†è«–ã€â†’ Z5.6, Z5.16 | ã€Œè©•ä¾¡æŒ‡æ¨™ã€â†’ Z5.3 | ã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«çµ±ä¸€ã€â†’ Z5.8 | ã€Œæ•°å€¤å®Ÿè£…ã€â†’ Z5.15, Z5.16

### Z7.1 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾ç…§è¡¨

| æ•°å¼ | Python | æ³¨æ„ç‚¹ |
|:-----|:-------|:-------|
| $\hat{\theta}_{MLE} = \arg\max_\theta \sum \log p_\theta(x_i)$ | `minimize(nll, theta0)` | `-sum(log_p(theta, x_data))` |
| $D_{KL}(p \| q) \geq 0$ | `(p * (p.log() - q.log())).sum()` | `eps` ã§ log(0) å›é¿ |
| $H(p,q) = H(p) + D_{KL}(p\|q)$ | `cross_entropy(p,q) = entropy(p) + kl(p,q)` | æ•°å€¤æ¤œç®—å¿…é ˆ |
| $I(\theta) = \mathbb{E}[(\partial_\theta \log p)^2]$ | `scores.var()` | `scores` ã¯ scoreé–¢æ•°ã®é…åˆ— |
| $\text{FID} = \|\mu_r-\mu_g\|^2 + \text{Tr}(\cdot)$ | `fid_gaussian(mu_r, Sigma_r, ...)` | `sqrtm` ã®å¯¾ç§°åŒ–å¿…é ˆ |
| $J_{SM}(\theta) = \mathbb{E}[\frac{1}{2}\|s_\theta\|^2 + \text{tr}(\nabla s_\theta)]$ | `score_matching_loss(score_fn, x_data)` | 1Dã¯æ•°å€¤å¾®åˆ†, é«˜æ¬¡å…ƒã¯Hutchinson |
| $w_i = p(x_i)/q(x_i)$ (IS) | `log_w = log_p(x) - log_q(x)` | log space ã§è¨ˆç®—ã— `exp` |
| $\hat{p}_{KDE}(x) = \frac{1}{Nh}\sum K(\frac{x-x_i}{h})$ | `gaussian_kde(x_data)` | bw_method ã§å¸¯åŸŸå¹…åˆ¶å¾¡ |
| $\text{ESS} = (\sum w_i)^2 / \sum w_i^2$ | `w.sum()**2 / (w**2).sum()` | w ã¯ unnormalized ã§ã‚ˆã„ |
| $z = \mu + \sigma \epsilon, \epsilon \sim \mathcal{N}(0,I)$ | `z = mu + log_sigma.exp() * eps` | `eps = torch.randn(shape)` |
| $\log p_\theta(x) = \log p_z(f^{-1}(x)) + \log|\det J|$ | `log_pz + log_abs_det_jac` | 1D: `log_abs_det_jac = -log_sigma` |
| $\text{Var}(\hat{\theta}) \geq 1/I(\theta)$ | `1 / fisher_info(theta, x)` | `fisher_info` = score ã®åˆ†æ•£ |

### Z7.2 FAQ

<details><summary>Q1: MLE ã¯å¸¸ã«æ­£ã—ã„æ¨å®šé‡ã‹ï¼Ÿ</summary>

ã„ã„ãˆã€‚MLE ã®å•é¡Œç‚¹:
1. **å°‘ã‚µãƒ³ãƒ—ãƒ«ã§ã®éå­¦ç¿’**: `n` ãŒå°ã•ã„ã¨ã MLE ã¯åˆ†æ•£ãŒå¤§ããã€ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œæš—è¨˜ã€ã™ã‚‹å‚¾å‘ï¼ˆZ5.9 ã® bigram ä¾‹ï¼‰
2. **å±€æ‰€æœ€é©è§£**: å¤šå³°çš„å°¤åº¦é–¢æ•°ï¼ˆGMM ãªã©ï¼‰ã§ã¯å±€æ‰€æœ€é©ã«åæŸ
3. **éæ­£å‰‡ãƒ¢ãƒ‡ãƒ«**: $p_\theta$ ãŒæ­£å‰‡ã§ãªã„ã¨æ¼¸è¿‘æ­£è¦æ€§ãŒæˆç«‹ã—ãªã„
4. **è¨ˆç®—ä¸å¯èƒ½æ€§**: $p(x) = \int p(x|z)p(z)dz$ ãŒè§£æçš„ã«è§£ã‘ãªã„ã¨ãç›´æ¥ MLE ã¯å›°é›£ï¼ˆâ†’ EM / ELBO ãŒå¿…è¦ï¼‰

ã©ã‚“ãªã¨ãã« MAP/Bayesian ã‚’é¸ã¶ã‹: å°ã‚µãƒ³ãƒ—ãƒ«ã€äº‹å‰æƒ…å ±ãŒã‚ã‚‹ã€ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ãŒå¿…è¦ãªã¨ãã€‚
</details>

<details><summary>Q2: Forward KL ã¨ Reverse KL ã¯ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ</summary>

ç”¨é€”ã«ã‚ˆã£ã¦æ±ºã¾ã‚‹:

| çŠ¶æ³ | æ¨å¥¨ | ç†ç”± |
|:-----|:-----|:-----|
| ç”Ÿæˆå“è³ªï¼ˆå¤šæ§˜æ€§é‡è¦–ï¼‰ | Forward KL | Mode covering â†’ å…¨ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼ |
| ç”Ÿæˆå“è³ªï¼ˆé®®æ˜ã•é‡è¦–ï¼‰ | Reverse KL | Mode seeking â†’ é®®æ˜ã ãŒå¤šæ§˜æ€§ä½ |
| å¤‰åˆ†æ¨è«– ($q(z|x)$) | Reverse KL | $q$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ã§ ELBO ãŒè¨ˆç®—å¯èƒ½ |
| ãƒã‚¤ã‚ºè€æ€§ | Forward KL | $p$ ãŒæ˜ç¢ºã§ãªã„é ˜åŸŸã¸ã® $q$ ã®é…ç½®ã‚’é˜²ã |
| Normalizing Flow | Forward KL | ãƒ•ãƒ­ãƒ¼ã¯ $p_\theta$ ã‚’æ˜ç¤ºçš„ã«è¨ˆç®—ã§ãã‚‹ |
</details>

<details><summary>Q3: FID ã¯ãªãœè¡Œåˆ—å¹³æ–¹æ ¹ãŒå¿…è¦ã‹ï¼Ÿ</summary>

2ã¤ã®ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}(\mu_r, \Sigma_r)$ ã¨ $\mathcal{N}(\mu_g, \Sigma_g)$ ã® FrÃ©chet è·é›¢:

$$
d^2 = \|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2\sqrt{\Sigma_r \Sigma_g})
$$

$\sqrt{\Sigma_r \Sigma_g}$ ã¯è¡Œåˆ— $(\Sigma_r^{1/2} \Sigma_g \Sigma_r^{1/2})^{1/2}$ ã¨ã—ã¦è¨ˆç®—ã™ã‚‹ã€‚å˜ç´”ã« $\sqrt{\Sigma_r} \cdot \sqrt{\Sigma_g}$ ã§ã¯è¡Œåˆ—ç©ã®éå¯æ›æ€§ã§èª¤å·®ãŒå‡ºã‚‹ã€‚`sqrtm_psd` ã§å¯¾ç§°æ­£å®šå€¤è¡Œåˆ—ã®è¡Œåˆ—å¹³æ–¹æ ¹ã‚’å›ºæœ‰åˆ†è§£ã§æ±‚ã‚ã‚‹ã“ã¨ãŒæ•°å€¤å®‰å®šæ€§ã®éµã€‚
</details>

<details><summary>Q4: Score Matching ã¨ Diffusion ãƒ¢ãƒ‡ãƒ«ã¯ã©ã†ç¹‹ãŒã‚‹ã‹ï¼Ÿ</summary>

æ¥ç¶šã®éµ: **Tweedie ã®å…¬å¼**ï¼ˆEfron 2011ï¼‰ã¨ **Denoising Score Matching**ï¼ˆVincent 2011ï¼‰[^7]ã€‚

$x_t = x_0 + \sigma_t \epsilon$ï¼ˆå‰å‘ãæ‹¡æ•£ï¼‰ã®ã¨ã:

$$
\nabla_{x_t} \log p_t(x_t) = -\frac{\epsilon}{\sigma_t} \quad \text{(Tweedie)}
$$

DDPM ã®æå¤± $\|\epsilon - \hat{\epsilon}_\theta(x_t, t)\|^2$ ã¯ã€Œãƒã‚¤ã‚ºã‚’äºˆæ¸¬ã™ã‚‹ã€ãŒã€ã“ã‚Œã¯ã€Œã‚¹ã‚³ã‚¢é–¢æ•° $\nabla \log p_t$ ã‚’å­¦ç¿’ã™ã‚‹ã€ã¨ç­‰ä¾¡ã€‚**Score Matching = Denoising = Diffusion ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´**ã€ã“ã‚ŒãŒç¬¬5å›ã¨ã®æ¥ç¶šã€‚
</details>

<details><summary>Q5: MLE ã¨ MAP ã¯ã©ã†ä½¿ã„åˆ†ã‘ã‚‹ã‹ï¼Ÿ</summary>

åˆ¤æ–­åŸºæº–ã¯ **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º $n$** ã¨ **äº‹å‰æƒ…å ±ã®ä¿¡é ¼åº¦**:

| æ¡ä»¶ | æ¨å¥¨ | ç†ç”± |
|:-----|:-----|:-----|
| $n \gg 1$ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿ï¼‰ | MLE | æ¼¸è¿‘è«–: MLE ã¯ä¸€è‡´æ€§ãƒ»æœ‰åŠ¹æ€§ã‚’ã‚‚ã¡ã€äº‹å‰åˆ†å¸ƒã®å½±éŸ¿ãŒæ¶ˆãˆã‚‹ |
| $n$ ãŒå°ã•ã„ï¼ˆ< 100ï¼‰| MAP | äº‹å‰åˆ†å¸ƒãŒæ­£å‰‡åŒ–ã®å½¹å‰²ï¼ˆL2æ­£å‰‡åŒ– = ã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒï¼‰ |
| äº‹å‰æƒ…å ±ãŒå¼·ã„ | MAP or Bayesian | ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’æ´»ç”¨ |
| è¨ˆç®—ã‚³ã‚¹ãƒˆæœ€å„ªå…ˆ | MLE | MAP ã¯äº‹å‰åˆ†å¸ƒã‚’è¨­è¨ˆãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã‚³ã‚¹ãƒˆãŒå¿…è¦ |
| ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã—ãŸã„ | Bayesian | äº‹å¾Œåˆ†å¸ƒ $p(\theta|x)$ ã‚’ä½¿ã†ï¼ˆMAP ã¯ç‚¹æ¨å®šã«ã¨ã©ã¾ã‚‹ï¼‰ |

**è½ã¨ã—ç©´**: MAP ã®äº‹å‰åˆ†å¸ƒé¸æŠãŒæ£æ„çš„ã«ãªã‚ŠãŒã¡ã€‚L2 æ­£å‰‡åŒ–ã®ä¿‚æ•° $\lambda$ = ç²¾åº¦ $\tau^2/\sigma^2$ ã¨å¯¾å¿œã™ã‚‹ãŒã€ã“ã‚Œã‚’æ­£å½“åŒ–ã§ãã‚‹æ ¹æ‹ ãŒå¿…è¦ã€‚
</details>

<details><summary>Q6: FID ã®é™ç•Œã¯ä½•ã‹ï¼Ÿ</summary>

FID ã«ã¯ä»¥ä¸‹ã®æ§‹é€ çš„ãªå•é¡ŒãŒã‚ã‚‹:

1. **Inception-V3 ãƒã‚¤ã‚¢ã‚¹**: ImageNet åˆ†é¡ã§è¨“ç·´ã•ã‚ŒãŸç‰¹å¾´æŠ½å‡ºå™¨ã€‚ãƒ†ã‚­ã‚¹ãƒˆãƒ»åŒ»ç™‚ç”»åƒãƒ»ç§‘å­¦ãƒ‡ãƒ¼ã‚¿ã«ã¯ä¸é©åˆ‡ãªå ´åˆãŒã‚ã‚‹
2. **ã‚¬ã‚¦ã‚¹è¿‘ä¼¼**: FrÃ©chet è·é›¢ã¯åˆ†å¸ƒãŒã‚¬ã‚¦ã‚¹ã§ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã™ã‚‹ã€‚å®Ÿéš›ã®ç”»åƒç‰¹å¾´åˆ†å¸ƒã¯éã‚¬ã‚¦ã‚¹çš„
3. **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¾å­˜æ€§**: ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ã¨ FID ãŒé«˜ãå‡ºã‚‹ï¼ˆæ¨å¥¨ â‰¥ 50k ã‚µãƒ³ãƒ—ãƒ«ï¼‰
4. **ãƒ¢ãƒ¼ãƒ‰å´©å£Šã®è¦‹é€ƒã—**: $\mu, \Sigma$ ã¯ãƒ¢ãƒ¼ãƒ‰ã®æ•°ã§ã¯ãªãå¹³å‡ãƒ»åˆ†æ•£ã‚’æ‰ãˆã‚‹ã€‚ã€Œ1ã¤ã®ãƒªã‚¢ãƒ«ãªãƒ¢ãƒ¼ãƒ‰ã€ã¨ã€Œ50ã®ãƒ€ãƒ¡ãªãƒ¢ãƒ¼ãƒ‰ã€ã‚’åŒºåˆ¥ã§ããªã„å ´åˆãŒã‚ã‚‹

$$
\text{FID bias} \propto \frac{1}{n_g}: \quad \mathbb{E}[\text{FID}] = \text{True FID} + \frac{C}{n_g}
$$

å¯¾ç­–: Precision/Recall (KynkÃ¤Ã¤nniemi et al. 2019) ã‚„ CMMD [^10] ã¨ã®çµ„ã¿åˆã‚ã›ãŒç¾åœ¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€‚
</details>

<details><summary>Q7: æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ MLE ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã¨ã¯ï¼Ÿ</summary>

Kaplan et al. (2020) ã® Scaling Laws: ãƒ†ã‚¹ãƒˆãƒ­ã‚¹ $L$ ã¯ $N$ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼‰ã¨ $D$ï¼ˆãƒ‡ãƒ¼ã‚¿é‡ï¼‰ã®**ã¹ãä¹—å‰‡**ã«å¾“ã†:

$$
L(N, D) \approx \left(\frac{N_c}{N}\right)^{\alpha_N} + \left(\frac{D_c}{D}\right)^{\alpha_D} + L_{\infty}
$$

$\alpha_N \approx 0.076$, $\alpha_D \approx 0.095$ï¼ˆå…ƒè«–æ–‡ã®æ¨å®šå€¤ï¼‰ã€‚

**ç›´æ„Ÿ**: GPT ã®è¨“ç·´ã¯å˜ç´”ãª MLEï¼ˆ$\max_\theta \sum \log p_\theta(x_t|x_{<t})$ï¼‰ãªã®ã«ã€ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã ã‘ã§å‰µç™ºçš„ãªèƒ½åŠ›ãŒç”Ÿã¾ã‚Œã‚‹ã€‚ã“ã‚Œã¯ã€Œåˆ†å¸ƒ $p(\text{text})$ ã®åœ§ç¸®ã€ãŒçŸ¥è­˜ã®ç²å¾—ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã™ã‚‹ã€‚

ãŸã ã— Hoffmann et al. (2022) ã® Chinchilla è«–æ–‡ã§æœ€é©ãª $N:D$ æ¯”ãŒä¿®æ­£ã•ã‚ŒãŸï¼ˆ$D \approx 20N$ï¼‰ã€‚
</details>

<details><summary>Q8: MLE ã¯ãªãœã€Œåˆ†å¸ƒã‚’å­¦ã¶ã€ã¨è¨€ãˆã‚‹ã®ã‹ï¼Ÿ</summary>

MLE ã®ç›®çš„é–¢æ•°ã‚’æƒ…å ±ç†è«–ã®è¦–ç‚¹ã§æ›¸ãç›´ã™ã¨:

$$
\hat{\theta}_{MLE} = \arg\max_\theta \sum_{i=1}^N \log p_\theta(x_i) = \arg\min_\theta KL(\hat{p}_{data} \| p_\theta)
$$

ã“ã“ã§ $\hat{p}_{data} = \frac{1}{N} \sum_{i=1}^N \delta(x - x_i)$ ã¯çµŒé¨“åˆ†å¸ƒï¼ˆempirical distributionï¼‰ã€‚

ã“ã‚ŒãŒç¤ºã™ã‚‚ã®: **MLE ã¯ã€ŒçµŒé¨“åˆ†å¸ƒ $\hat{p}_{data}$ ã¨ $p_\theta$ ã® KL ã‚’æœ€å°åŒ–ã™ã‚‹ã€å•é¡Œ**ã€‚è¨€ã„æ›ãˆã‚‹ã¨ã€MLE ã¯çœŸã®åˆ†å¸ƒ $p_{data}(x)$ ã« $p_\theta(x)$ ã‚’è¿‘ã¥ã‘ã‚‹æ“ä½œã«ä»–ãªã‚‰ãªã„ã€‚

ãŸã ã— $\hat{p}_{data}$ ã¯æœ‰é™ã‚µãƒ³ãƒ—ãƒ«ã®è¿‘ä¼¼ã€‚$n \to \infty$ ã§ $\hat{p}_{data} \to p_{data}$ï¼ˆå¼±åæŸï¼‰ã¨ãªã‚‹ã¨ãã€æ¼¸è¿‘ä¸€è‡´æ€§ãŒæˆç«‹ã™ã‚‹ã€‚
</details>

### Z7.3 æ¬¡å›äºˆå‘Š â€” ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã¨ EM ç®—æ³•

æœ¬è¬›ç¾©ã§ MLE ã®å¨åŠ›ã¨é™ç•ŒãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚æ¬¡ã®éšœå£:

$$
\log p_\theta(x) = \log \int p_\theta(x|z) p(z) dz \quad \text{â€” ã“ã®ç©åˆ†ãŒè§£æä¸èƒ½}
$$

| æœ¬è¬›ç¾© (ç¬¬7å›) | ç¬¬8å›ã¸ã®æ©‹æ¸¡ã— |
|:-------------|:-------------|
| MLE ã®å®šç¾©ã¨ä¸‰ä½ä¸€ä½“ | å‘¨è¾ºå°¤åº¦ $p(x) = \int p(x|z)p(z)dz$ ãŒå›°é›£ãªç†ç”± |
| GMM ã® gradient-based MLE | GMM-EM: è§£æçš„ãª E-step / M-step æ›´æ–° |
| æ¼¸è¿‘è«–ï¼ˆFisher æƒ…å ±é‡ï¼‰| EM ã®åæŸæ€§ï¼ˆJensen ä¸ç­‰å¼ã‹ã‚‰ã®è¨¼æ˜ï¼‰ |
| ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚° | Variational EM â†’ VAE ã¸ã®æ©‹æ¸¡ã— |
| Python ã®é…ã•ä½“æ„Ÿ | EM ã®åå¾©è¨ˆç®—ã§ã€Œé…ã™ãã‚‹ã€ã‚’å†å®Ÿæ„Ÿ â†’ Rust/Rust äºˆå‘Š |

**æ ¸å¿ƒã®ã‚®ãƒ£ãƒƒãƒ—**: å‘¨è¾ºå°¤åº¦ $\int p(x|z)p(z)dz$ ã‚’ç›´æ¥æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã¯ï¼ˆé€£ç¶šæ½œåœ¨å¤‰æ•°ã§ã¯ï¼‰è¨ˆç®—ä¸èƒ½ã€‚EM ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã“ã‚Œã‚’ **ELBOï¼ˆä¸‹ç•Œï¼‰ã®æœ€å¤§åŒ–**ã«ç½®ãæ›ãˆã‚‹é©å‘½çš„ãªæ‰‹æ³•ã€‚Jensen ä¸ç­‰å¼ â†’ ELBO â†’ E-step / M-step ã¨ã„ã†æµã‚ŒãŒç¬¬8å›ã®ãƒœã‚¹æˆ¦ã€‚

**Jensen ä¸ç­‰å¼ã¨ ELBO ã®é–¢ä¿‚ï¼ˆå…ˆå–ã‚Šï¼‰**:

$$
\log p_\theta(x) = \log \int p_\theta(x|z) p(z)\,dz = \log \mathbb{E}_{p(z)}\left[\frac{p_\theta(x|z) p(z)}{q(z)}\right]
$$

$\log$ ã¯å‡¹é–¢æ•°ãªã®ã§ Jensen ä¸ç­‰å¼ $\log \mathbb{E}[f] \geq \mathbb{E}[\log f]$ ã‚ˆã‚Š:

$$
\log p_\theta(x) \geq \mathbb{E}_{q(z)}\left[\log \frac{p_\theta(x|z) p(z)}{q(z)}\right] = \underbrace{\mathbb{E}_{q}[\log p_\theta(x|z)] - D_{KL}(q \| p)}_{\text{ELBO}}
$$

ç­‰å·æ¡ä»¶ã¯ $q(z) = p(z|x)$ï¼ˆçœŸã®äº‹å¾Œåˆ†å¸ƒï¼‰ã€‚E-step ã§ $q \leftarrow p(z|x)$ï¼ˆç­‰å·ã«è¿‘ã¥ã‘ã‚‹ï¼‰ã€M-step ã§ $\theta \leftarrow \arg\max \mathbb{E}_q[\log p_\theta(x,z)]$ ï¼ˆELBO ã‚’æœ€å¤§åŒ–ï¼‰â€” ã“ã‚ŒãŒ EM ã®æœ¬è³ªã ã€‚æœ¬è¬›ç¾©ã®MLEä¸‰ä½ä¸€ä½“ï¼ˆZ5.1ï¼‰ã®è‡ªç„¶ãªå»¶é•·ã¨ã—ã¦ç†è§£ã§ãã‚‹ã€‚

**ã“ã®è¬›ç¾©ã§å­¦ã‚“ã ã“ã¨**: MLE ã¯ã€Œä¸–ç•ŒãŒã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ãŸç¢ºç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ã™ã€æ“ä½œã ã€‚Cross-Entropy ã¨ KL divergence ãŒãã®ç­‰ä¾¡ãªè¡¨ç¾ã§ã‚ã‚Šã€Fisher æƒ…å ±é‡ãŒæ¨å®šç²¾åº¦ã®ç†è«–çš„é™ç•Œã‚’ä¸ãˆã‚‹ã€‚Score Matching ã¨ Normalizing Flow ã¯ã€Œå°¤åº¦ãŒè¨ˆç®—ã§ããªã„/æ›¸ã‘ãªã„ã€å•é¡Œã¸ã®2ã¤ã®æ–¹å‘æ€§ã®å›ç­”ã€‚ãã—ã¦ã“ã‚Œã‚‰å…¨ã¦ãŒã€Diffusion ãƒ¢ãƒ‡ãƒ«ãƒ»GPTãƒ»VAE ã¨ã„ã†ç¾ä»£ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åŸºç›¤ã‚’å½¢æˆã—ã¦ã„ã‚‹ã€‚

---

### Z7.4 å®Ÿè£…ã¾ã¨ã‚

æœ¬è¬›ç¾©ã§å®Ÿè£…ã—ãŸä¸»è¦ãªé–¢æ•°ä¸€è¦§ï¼ˆã‚³ãƒ¼ãƒ‰ä»˜ã = 3æœ¬ã€ä»–ã¯æ•°å­¦çš„è€ƒå¯Ÿã¨ã—ã¦å±•é–‹ï¼‰:

| é–¢æ•°/ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« | å®Ÿè£…å ´æ‰€ | å†…å®¹ |
|:--------------|:---------|:-----|
| `mle_cross_entropy` | Z5.1ï¼ˆã‚³ãƒ¼ãƒ‰ï¼‰ | MLE = CE = KL ã®æ•°å€¤è¨¼æ˜ âœ… |
| `fid_gaussian` | Z5.3ï¼ˆã‚³ãƒ¼ãƒ‰ï¼‰ | FID å®Ÿè£…ã€`sqrtm` ã®å¯¾ç§°åŒ– âœ… |
| `gmm_mle` | Z5.4ï¼ˆã‚³ãƒ¼ãƒ‰ï¼‰ | GMM L-BFGS-B + log_sigma å®‰å®šåŒ– âœ… |
| Score Matching | Z5.5 | Hutchinson æ¨å®šå™¨ã®å°å‡º |
| Rejection / IS | Z5.6 | å—ç†ç‡ $1/M$ã€ESS ã¨ã®æ¥ç¶š |
| CRB æ¤œè¨¼ | Z5.7 | $\text{Var}(\bar{x}) = \sigma^2/N = 1/I(\mu)$ è§£æ |
| Mode-seeking/covering | Z5.8 | Forward KL â†’ $\mu \approx 0$ã€Reverse KL â†’ $\mu \approx \pm 3$ |
| Bigram LLM MLE | Z5.9 | å°ãƒ‡ãƒ¼ã‚¿éå­¦ç¿’ã€Laplace smoothing |
| MAP æ¨å®š | Z5.11 | $\lambda \to 0$ â†’ MLEã€$\lambda \to \infty$ â†’ ã‚¼ãƒ­åç¸® |
| Reparameterization | Z5.12 | shape `(batch, latent_dim)` è¿½è·¡ |
| Affine NF | Z5.13 | MLE è§£æè§£ $\hat{\mu}=\bar{x}$, $\hat{\sigma}=s$ ã¨ä¸€è‡´ |
| NLL æ¯”è¼ƒ | Z5.14 | KDE `~1.65`, NF-affine `~2.10` |
| æ•°å€¤å®‰å®šæ€§ | Z5.15 | `logpdf` vs `log(pdf)`, `logsumexp` |
| ESS è¨ˆç®— | Z5.16 | $\sigma_q$ ã¨ ESS/N ã®æ„Ÿåº¦ |

> Progress: 100%

> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãŠã‘ã‚‹Eã‚¹ãƒ†ãƒƒãƒ—ã¨Mã‚¹ãƒ†ãƒƒãƒ—ã‚’ãã‚Œãã‚Œä¸€æ–‡ã§è¿°ã¹ã€åæŸã‚’ä¿è¨¼ã™ã‚‹æ•°å­¦çš„æ ¹æ‹ ï¼ˆELBO ã®å˜èª¿å¢—åŠ æ€§ï¼‰ã‚’èª¬æ˜ã›ã‚ˆã€‚
> 2. GMMã§EMã‚’ä½¿ã†éš›ã€æˆåˆ†æ•° $K$ ã‚’éå¤§ã«è¨­å®šã™ã‚‹ã¨ä½•ãŒèµ·ãã‚‹ã‹ã€‚BIC/AICã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«é¸æŠãŒã©ã†è§£æ±ºã™ã‚‹ã‹èª¬æ˜ã›ã‚ˆã€‚

---

## PB ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’å™¨ã€‚ç”»åƒç”Ÿæˆã¯"å¿œç”¨ä¾‹ã®ä¸€ã¤"ã«éããªã„ã®ã§ã¯ï¼Ÿ**

ã“ã®å•ã„ã‚’3ã¤ã®è¦–ç‚¹ã‹ã‚‰è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚

1. **MLE ã®æ±ç”¨æ€§**: $\max_\theta \mathbb{E}[\log p_\theta(x)]$ ã¨ã„ã†ç›®çš„é–¢æ•°ã¯ã€ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»éŸ³å£°ãƒ»åˆ†å­æ§‹é€ ãƒ»ã‚¿ãƒ³ãƒ‘ã‚¯è³ªé…åˆ—ã«ç­‰ã—ãé©ç”¨ã§ãã‚‹ã€‚ã€Œç”»åƒç”ŸæˆãŒç›®çš„ã€ã§ã¯ãªãã€ã€Œç¢ºç‡åˆ†å¸ƒã®è¿‘ä¼¼ãŒç›®çš„ã€ã§ã‚ã‚Šã€ç”»åƒã¯ç¢ºç‡åˆ†å¸ƒãŒå®šç¾©ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿å‹ã®ä¸€ã¤ã«éããªã„ã€‚

2. **è¨€èªãƒ¢ãƒ‡ãƒ«ã®å†è§£é‡ˆ**: GPT-4 ã¯ã€Œãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå™¨ã€ã§ã¯ãªãã€Œãƒ†ã‚­ã‚¹ãƒˆã®ç¢ºç‡åˆ†å¸ƒ $p(x_{t}|x_{<t})$ ã®æ¨å®šå™¨ã€ã ã€‚ç”Ÿæˆã¯ãã“ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚æ¨è«–ï¼ˆIn-context Learningï¼‰ã¯äº‹å¾Œåˆ†å¸ƒã®æ›´æ–° $p(\theta|\text{context})$ ã¨ã—ã¦ç†è§£ã§ãã‚‹ï¼ˆç¬¬7å›æœ«å°¾ã§è¨¼æ˜ï¼‰ã€‚

3. **World Models**: æœ€æ–°ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ sensorimotor contingencyï¼ˆæ„Ÿè¦šé‹å‹•éšä¼´æ€§ï¼‰ã‚’å­¦ã¶ World Model ã¨ã—ã¦å†è§£é‡ˆã•ã‚Œã¤ã¤ã‚ã‚‹ã€‚$p(x_{t+1}|x_{\leq t}, a_t)$ ã¨ã„ã†æ¡ä»¶ä»˜ãåˆ†å¸ƒã®å­¦ç¿’ãŒã€ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ãƒ»å¼·åŒ–å­¦ç¿’ãƒ»ç§‘å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ±ä¸€ã™ã‚‹ã€‚**MLE ã¯ã€Œä¸–ç•Œã®ç¢ºç‡çš„ãƒ¢ãƒ‡ãƒ«ã€ã‚’å­¦ã¶æ±ç”¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ã‹ã‚‚ã—ã‚Œãªã„ã€‚

<details><summary>æ­´å²çš„æ–‡è„ˆ: Fisher ã®ã€Œæœ€å°¤æ³•ã€ã¨ Neyman-Pearson ã¨ã®ç¢ºåŸ·</summary>

Fisher ãŒ1922å¹´ã« MLE ã‚’å®šå¼åŒ–ã—ãŸå½“æ™‚ã€çµ±è¨ˆå­¦ã®ä¸»æµã¯æœ€å°äºŒä¹—æ³•ï¼ˆGauss 1809ï¼‰ã ã£ãŸã€‚Fisher ã®é©æ–°ã¯ã€Œå°¤åº¦é–¢æ•°ã€ã¨ã„ã†æ¦‚å¿µã‚’å°å…¥ã—ã€ã€Œãƒ‡ãƒ¼ã‚¿ãŒä¸ãˆã‚‰ã‚ŒãŸã‚‚ã¨ã§ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã€ã‚’ç›´æ¥å®šé‡åŒ–ã—ãŸã“ã¨ã ã€‚

**é€†è»¢ã®ç™ºæƒ³**: ç¢ºç‡ã¨ã¯ã€ŒæœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ä¿¡å¿µã€ã§ã¯ãªãã€ã€Œè¦³æ¸¬ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã€ã¨ã—ã¦èª­ã¿ç›´ã›ã‚‹ã€‚ã“ã‚ŒãŒå°¤åº¦ï¼ˆlikelihoodï¼‰ã®æ„å‘³ã€‚

Fisher ã¨ Neyman-Pearsonï¼ˆä¿¡é ¼åŒºé–“ãƒ»ä»®èª¬æ¤œå®šã®å‰µå§‹è€…ï¼‰ã¯æ¿€ã—ãå¯¾ç«‹ã—ãŸã€‚Fisher ã¯ã€Œæ¤œå®šã®æœ‰æ„æ°´æº– 0.05 ã¯æ£æ„çš„ã ã€ã¨ä¸»å¼µã—ã€Neyman ã¯ã€ŒMLE ã®æ¼¸è¿‘è«–ã¯å®Ÿç”¨æ€§ã‚’æ¬ ãã€ã¨åè«–ã—ãŸã€‚ç¾åœ¨ã®çµ±è¨ˆå­¦ã¯ã“ã®äºŒã¤ã®æµæ´¾ã‚’éƒ½åˆã‚ˆãæ··ãœã¦ä½¿ã£ã¦ã„ã‚‹ãŒã€å“²å­¦çš„ã«ã¯ä»Šã‚‚ç›¸å®¹ã‚Œãªã„ã€‚

100å¹´å¾Œã€ã“ã® Fisher ã®ç›´æ„ŸãŒæ·±å±¤å­¦ç¿’ã®æå¤±é–¢æ•°ï¼ˆCross-Entropyï¼‰ã«ç›´çµã—ã€ChatGPT ã®è¨“ç·´ã«è‡³ã‚‹ã€‚**MLE ã¯ã€Œä¸–ç•ŒãŒã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ãŸç¢ºç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ã™ã€ã¨ã„ã†è¡Œç‚º** â€” æ©Ÿæ¢°ãŒä¸–ç•Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹è©¦ã¿ã€ã¨ã‚‚èª­ã‚ã‚‹ã€‚
</details>

<details><summary>æ€è€ƒå®Ÿé¨“: MLE ã§ã€Œäººé–“ã®çŸ¥è­˜ã€ã¯å­¦ã¹ã‚‹ã‹ï¼Ÿ</summary>

Large Language Models ã®è¨“ç·´ã¯ $\max_\theta \sum_{t} \log p_\theta(x_t|x_{<t})$ ã¨ã„ã†å˜ç´”ãª MLE ã ã€‚ã—ã‹ã—ã€Œãƒ†ã‚­ã‚¹ãƒˆã®ç¢ºç‡åˆ†å¸ƒã‚’å®Œç’§ã«å­¦ã¶ã€ã“ã¨ã¯ã€ãã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ãŸèªçŸ¥ãƒ»æ–‡åŒ–ãƒ»çŸ¥è­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ã¶ã“ã¨ã¨ç­‰ä¾¡ã‹ï¼Ÿ

è«–ç‚¹:
- **è‚¯å®šå´**: ãƒ†ã‚­ã‚¹ãƒˆã¯ã€Œäººé–“ã®èªçŸ¥ã®ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã€ã€‚åˆ†å¸ƒã‚’å­¦ã¹ã°èªçŸ¥ã‚’å­¦ã¹ã‚‹ï¼ˆSapir-Whorf ä»®èª¬ã®å¼±å½¢ï¼‰
- **å¦å®šå´**: ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸–ç•Œã®è¦³æ¸¬ã§ã¯ãªãã€è¦³æ¸¬ã®è¨˜å·çš„å°„å½±ã€‚$p(\text{text})$ ã®å®Œç’§ãªæ¨å®šå™¨ã§ã‚‚ã€ç‰©ç†ä¸–ç•Œ $p(\text{world})$ ã¯æ¨è«–ã§ããªã„ï¼ˆSymbol Grounding å•é¡Œï¼‰
- **ç¾å®Ÿ**: LLM ã¯ã€Œè¨˜å·æ“ä½œãŒå¾—æ„ã€ã€Œå¸¸è­˜æ¨è«–ã¯ã§ãã‚‹ãŒèº«ä½“çš„ç›´æ„Ÿã¯ã§ããªã„ã€ã¨ã„ã† asymmetry â€” ã“ã‚Œã¯ MLE ã®ä½•ã‚’åæ˜ ã—ã¦ã„ã‚‹ã‹ï¼Ÿ
</details>

---

> **ğŸ“– å‰ç·¨ã‚‚ã‚ã‚ã›ã¦ã”è¦§ãã ã•ã„**
> [ã€å‰ç·¨ã€‘ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–](/articles/ml-lecture-07-part1) ã§ã¯ã€æœ€å°¤æ¨å®šãƒ»Fisheræƒ…å ±é‡ãƒ»çµ±è¨ˆçš„æ¨è«–ã®ç†è«–ã‚’å­¦ã³ã¾ã—ãŸã€‚

## å‚è€ƒæ–‡çŒ®

[^1]: Fisher, R. A. (1922). "On the Mathematical Foundations of Theoretical Statistics." *Philosophical Transactions of the Royal Society of London. Series A*, 222, 309-368.
[^2]: Goodfellow, I., et al. (2014). "Generative Adversarial Nets." *NeurIPS 2014*. [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)
[^3]: Kingma, D. P., & Welling, M. (2013). "Auto-Encoding Variational Bayes." *ICLR 2014*. [arXiv:1312.6114](https://arxiv.org/abs/1312.6114)
[^4]: Mohamed, S., & Lakshminarayanan, B. (2016). "Learning in Implicit Generative Models." *arXiv preprint*. [arXiv:1610.03483](https://arxiv.org/abs/1610.03483)
[^5]: Rezende, D. J., & Mohamed, S. (2015). "Variational Inference with Normalizing Flows." *ICML 2015*. [arXiv:1505.05770](https://arxiv.org/abs/1505.05770)
[^6]: HyvÃ¤rinen, A. (2005). "Estimation of Non-Normalized Statistical Models by Score Matching." *JMLR*, 6, 695-709.
[^7]: Song, Y., & Ermon, S. (2019). "Generative Modeling by Estimating Gradients of the Data Distribution." *NeurIPS 2019*. [arXiv:1907.05600](https://arxiv.org/abs/1907.05600)
[^8]: Heusel, M., et al. (2017). "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium." *NeurIPS 2017*. [arXiv:1706.08500](https://arxiv.org/abs/1706.08500)
[^9]: Salimans, T., et al. (2016). "Improved Techniques for Training GANs." *NeurIPS 2016*. [arXiv:1606.03498](https://arxiv.org/abs/1606.03498)
[^10]: Jayasumana, S., et al. (2024). "Rethinking FID: Towards a Better Evaluation Metric for Image Generation." *CVPR 2024*. [arXiv:2401.09603](https://arxiv.org/abs/2401.09603)
[^11]: Cranmer, K., Brehmer, J., & Louppe, G. (2020). "The frontier of simulation-based inference." *PNAS*, 117(48), 30055-30062. [arXiv:1911.01429](https://arxiv.org/abs/1911.01429)
[^12]: Ho, J., Jain, A., & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS 2020*. [arXiv:2006.11239](https://arxiv.org/abs/2006.11239)
[^13]: Khemakhem, I., et al. (2020). "Variational Autoencoders and Nonlinear ICA: A Unifying Framework." *AISTATS 2020*. [arXiv:1907.04809](https://arxiv.org/abs/1907.04809)

## è‘—è€…ãƒªãƒ³ã‚¯
- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
