---
title: "ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ—ºï¸"
type: "tech"
topics: ["machinelearning", "deeplearning", "statistics", "python"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” MLE å®Ÿè£…ã¨æ¨å®šé‡ã®å®Ÿè·µ

### 4.1 MLE ã®å®Œå…¨å®Ÿè£… â€” ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«

Zone 0 ã§å˜å³°ã‚¬ã‚¦ã‚¹ã®é™ç•Œã‚’è¦‹ãŸã€‚ã“ã“ã§ã¯æ··åˆãƒ¢ãƒ‡ãƒ«ã® MLE ã‚’å®Ÿè£…ã™ã‚‹ã€‚

```python
import numpy as np

class GaussianMixtureMLE:
    """
    Gaussian Mixture Model with EM algorithm for MLE.
    p(x) = Î£_k Ï€_k Â· N(x; Î¼_k, Ïƒ_kÂ²)
    """
    def __init__(self, n_components):
        self.K = n_components
        self.mus = None
        self.sigmas = None
        self.pis = None

    def initialize(self, data):
        """K-means++ style initialization"""
        N = len(data)
        # Random initialization
        indices = np.random.choice(N, self.K, replace=False)
        self.mus = data[indices].copy()
        self.sigmas = np.full(self.K, np.std(data))
        self.pis = np.full(self.K, 1.0 / self.K)

    def e_step(self, data):
        """E-step: compute responsibilities Î³(z_nk)"""
        N = len(data)
        gamma = np.zeros((N, self.K))
        for k in range(self.K):
            gamma[:, k] = self.pis[k] * self._gaussian(data, self.mus[k], self.sigmas[k])
        # Normalize
        gamma_sum = gamma.sum(axis=1, keepdims=True)
        gamma /= (gamma_sum + 1e-300)
        return gamma

    def m_step(self, data, gamma):
        """M-step: update parameters using responsibilities"""
        N = len(data)
        N_k = gamma.sum(axis=0)  # effective number per component

        for k in range(self.K):
            # Update means
            self.mus[k] = np.sum(gamma[:, k] * data) / (N_k[k] + 1e-10)
            # Update variances
            diff = data - self.mus[k]
            self.sigmas[k] = np.sqrt(np.sum(gamma[:, k] * diff**2) / (N_k[k] + 1e-10))
            self.sigmas[k] = max(self.sigmas[k], 1e-6)  # prevent singularity
            # Update mixing coefficients
            self.pis[k] = N_k[k] / N

    def log_likelihood(self, data):
        """Compute log p(D|Î¸) = Î£_n log Î£_k Ï€_k N(x_n; Î¼_k, Ïƒ_kÂ²)"""
        N = len(data)
        ll = 0
        for n in range(N):
            p_n = sum(self.pis[k] * self._gaussian(data[n:n+1], self.mus[k], self.sigmas[k])[0]
                      for k in range(self.K))
            ll += np.log(p_n + 1e-300)
        return ll

    def fit(self, data, max_iter=100, tol=1e-6):
        """EM algorithm for MLE"""
        self.initialize(data)
        prev_ll = -np.inf
        history = []

        for iteration in range(max_iter):
            # E-step
            gamma = self.e_step(data)
            # M-step
            self.m_step(data, gamma)
            # Log-likelihood
            ll = self.log_likelihood(data)
            history.append(ll)

            if abs(ll - prev_ll) < tol:
                print(f"Converged at iteration {iteration + 1}")
                break
            prev_ll = ll

        return history

    @staticmethod
    def _gaussian(x, mu, sigma):
        return np.exp(-0.5 * ((x - mu) / sigma)**2) / (sigma * np.sqrt(2 * np.pi))


# Demonstration
np.random.seed(42)

# True distribution: 3-component mixture
true_params = {
    'mus': [-3, 0, 4],
    'sigmas': [0.5, 1.0, 0.7],
    'pis': [0.3, 0.4, 0.3]
}

# Sample data
N = 2000
components = np.random.choice(3, size=N, p=true_params['pis'])
data = np.array([np.random.normal(true_params['mus'][c], true_params['sigmas'][c])
                 for c in components])

# Fit GMM
gmm = GaussianMixtureMLE(n_components=3)
history = gmm.fit(data)

print(f"\nTrue parameters:")
for k in range(3):
    print(f"  Component {k}: Ï€={true_params['pis'][k]:.2f}, "
          f"Î¼={true_params['mus'][k]:.2f}, Ïƒ={true_params['sigmas'][k]:.2f}")

print(f"\nEstimated parameters:")
order = np.argsort(gmm.mus)  # sort by mean
for i, k in enumerate(order):
    print(f"  Component {i}: Ï€={gmm.pis[k]:.2f}, "
          f"Î¼={gmm.mus[k]:.2f}, Ïƒ={gmm.sigmas[k]:.2f}")

print(f"\nFinal log-likelihood: {history[-1]:.2f}")
print(f"Iterations: {len(history)}")
```

### 4.2 Mathâ†’Code ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³

| æ•°å¼ | Python | æ„å‘³ |
|:-----|:-------|:-----|
| $\prod_{i=1}^{N} q_\theta(x_i)$ | `np.prod(q_theta(data))` | å°¤åº¦ï¼ˆæ•°å€¤çš„ã«ä¸å®‰å®šï¼‰ |
| $\sum_{i=1}^{N} \log q_\theta(x_i)$ | `np.sum(np.log(q_theta(data)))` | å¯¾æ•°å°¤åº¦ï¼ˆã“ã¡ã‚‰ã‚’ä½¿ã†ï¼‰ |
| $\hat{\theta} = \arg\max_\theta$ | `theta[np.argmax(ll)]` or gradient | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š |
| $\frac{1}{N}\sum \log q_\theta(x_i)$ | `np.mean(np.log(q_theta(data)))` | å¹³å‡å¯¾æ•°å°¤åº¦ |
| $\mathcal{N}(x; \mu, \sigma^2)$ | `np.exp(-0.5*((x-mu)/sigma)**2) / (sigma*np.sqrt(2*np.pi))` | ã‚¬ã‚¦ã‚¹å¯†åº¦ |
| $\gamma_{nk} = \frac{\pi_k q_k(x_n)}{\sum_j \pi_j q_j(x_n)}$ | `gamma[:, k] / gamma.sum(axis=1)` | è²¬ä»»åº¦ |
| $D_\text{KL}(p \| q)$ | `np.sum(p * np.log(p / q))` | KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ |
| $H(p, q) = -\mathbb{E}_p[\log q]$ | `-np.mean(np.log(q_theta(data)))` | Cross-Entropy |
| $\text{FID}$ | `||mu1-mu2||Â² + Tr(Î£1+Î£2-2âˆš(Î£1Î£2))` | ç”Ÿæˆå“è³ª |
| $\text{PPL} = \exp(\mathcal{L})$ | `np.exp(loss)` | Perplexity |

### 4.3 PyTorch å®Ÿè£…ã¨ã®å¯¾å¿œ

:::details PyTorch ã§ã® MLE å®Ÿè£…

```python
import torch
import torch.nn as nn
import torch.optim as optim

class SimpleGenerativeModel(nn.Module):
    """Simple parametric generative model: mixture of Gaussians"""
    def __init__(self, n_components):
        super().__init__()
        self.K = n_components
        self.mus = nn.Parameter(torch.randn(n_components))
        self.log_sigmas = nn.Parameter(torch.zeros(n_components))
        self.logits = nn.Parameter(torch.zeros(n_components))

    def log_prob(self, x):
        """log q_Î¸(x) = log Î£_k Ï€_k N(x; Î¼_k, Ïƒ_kÂ²)"""
        sigmas = torch.exp(self.log_sigmas)
        pis = torch.softmax(self.logits, dim=0)

        # (N, K) matrix of log-probabilities
        x = x.unsqueeze(1)  # (N, 1)
        log_probs = (-0.5 * ((x - self.mus) / sigmas)**2
                     - torch.log(sigmas)
                     - 0.5 * torch.log(torch.tensor(2 * torch.pi)))
        log_pis = torch.log(pis)

        # Log-sum-exp trick for numerical stability
        return torch.logsumexp(log_probs + log_pis, dim=1)

    def sample(self, n):
        """Sample from q_Î¸(x)"""
        with torch.no_grad():
            sigmas = torch.exp(self.log_sigmas)
            pis = torch.softmax(self.logits, dim=0)
            components = torch.multinomial(pis, n, replacement=True)
            samples = torch.randn(n) * sigmas[components] + self.mus[components]
        return samples

# Training loop: MLE via gradient descent
# model = SimpleGenerativeModel(3)
# optimizer = optim.Adam(model.parameters(), lr=0.01)
# for epoch in range(1000):
#     nll = -model.log_prob(data).mean()  # negative log-likelihood
#     optimizer.zero_grad()
#     nll.backward()
#     optimizer.step()
print("PyTorch MLE = minimize negative log-likelihood via Adam")
print("This is EXACTLY how LLM training works (with Cross-Entropy loss)")
```
:::

### 4.4 MLE ã®é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ â€” Python ã®é™ç•Œ

:::message alert
ã“ã“ã‹ã‚‰ Python ã®é…ã•ãŒæœ¬æ ¼çš„ã«è¦‹ãˆå§‹ã‚ã‚‹ã€‚ç¬¬9-10å›ã§ã€Œã‚‚ã†é™ç•Œã€ã¨æ„Ÿã˜ã‚‹ä¼ç·šã ã€‚
:::

```python
import numpy as np
import time

def benchmark_mle_python(N, D, K, n_iter=50):
    """
    Benchmark: GMM MLE (EM algorithm) in pure Python/NumPy
    N: number of data points
    D: dimensionality
    K: number of components
    """
    np.random.seed(42)

    # Generate D-dimensional data
    data = np.random.randn(N, D)
    mus = np.random.randn(K, D)
    sigmas = np.ones((K, D))
    pis = np.ones(K) / K

    start = time.perf_counter()

    for iteration in range(n_iter):
        # E-step: compute responsibilities
        gamma = np.zeros((N, K))
        for k in range(K):
            diff = data - mus[k]  # (N, D)
            exponent = -0.5 * np.sum(diff**2 / sigmas[k]**2, axis=1)
            norm_const = np.prod(sigmas[k]) * (2 * np.pi) ** (D / 2)
            gamma[:, k] = pis[k] * np.exp(exponent) / norm_const

        gamma_sum = gamma.sum(axis=1, keepdims=True)
        gamma /= (gamma_sum + 1e-300)

        # M-step
        N_k = gamma.sum(axis=0)
        for k in range(K):
            w = gamma[:, k:k+1]  # (N, 1)
            mus[k] = (w * data).sum(axis=0) / (N_k[k] + 1e-10)
            diff = data - mus[k]
            sigmas[k] = np.sqrt((w * diff**2).sum(axis=0) / (N_k[k] + 1e-10))
            sigmas[k] = np.maximum(sigmas[k], 1e-6)
            pis[k] = N_k[k] / N

    elapsed = time.perf_counter() - start
    return elapsed

# Benchmark across scales
print(f"{'N':>8} {'D':>4} {'K':>4} {'Time (s)':>10} {'iter/s':>10}")
print("-" * 42)

configs = [
    (1000,   10,  3),
    (5000,   10,  3),
    (10000,  10,  5),
    (10000,  50,  5),
    (50000,  10,  5),
    (10000, 100, 10),
]

for N, D, K in configs:
    t = benchmark_mle_python(N, D, K, n_iter=50)
    print(f"{N:8d} {D:4d} {K:4d} {t:10.4f} {50/t:10.1f}")
```

**å‡ºåŠ›ä¾‹:**
```
       N    D    K   Time (s)    iter/s
------------------------------------------
    1000   10    3     0.0321    1557.6
    5000   10    3     0.1205     415.0
   10000   10    5     0.3812     131.2
   10000   50    5     0.7834      63.8
   50000   10    5     1.8921      26.4
   10000  100   10     2.4567      20.4
```

```python
# The Python problem: scaling
print("\n=== Python's Scaling Problem ===")
print("10K points, 100D, 10 components: ~2.5 seconds for 50 iterations")
print("Real-world: 100K+ images, 512D embeddings, 100+ components")
print("Estimated time: ~250 seconds = 4+ minutes per EM run")
print("\nFor neural network-based models (VAE, GAN, Diffusion):")
print("  Training = 1000s of gradient steps Ã— forward + backward")
print("  Python overhead becomes DOMINANT bottleneck")
print("\nâ†’ Lecture 9-10: Julia debut for compute-heavy tasks")
print("â†’ Lecture 11-14: Rust for performance-critical kernels")
```

### 4.5 FIDï¼ˆçµ±è¨ˆçš„è·é›¢ï¼‰è¨ˆç®—ã®å®Ÿè£…

```python
import numpy as np

def compute_fid_full(real_features, gen_features):
    """
    Compute FID between two sets of features.

    Math: FID = ||Î¼_r - Î¼_g||Â² + Tr(Î£_r + Î£_g - 2(Î£_rÂ·Î£_g)^{1/2})

    In practice, features come from Inception-v3's pool3 layer (2048-dim).
    Here we work with arbitrary features for demonstration.
    """
    # Statistics
    mu_r = real_features.mean(axis=0)
    mu_g = gen_features.mean(axis=0)
    sigma_r = np.cov(real_features, rowvar=False)
    sigma_g = np.cov(gen_features, rowvar=False)

    # Mean difference term
    diff = mu_r - mu_g
    mean_term = np.dot(diff, diff)

    # Matrix square root via eigendecomposition
    product = sigma_r @ sigma_g
    eigvals, eigvecs = np.linalg.eigh(product)
    eigvals = np.maximum(eigvals, 0)  # clip negative eigenvalues
    sqrt_product = eigvecs @ np.diag(np.sqrt(eigvals)) @ eigvecs.T

    # Trace term
    trace_term = np.trace(sigma_r + sigma_g - 2 * sqrt_product)

    return mean_term + trace_term

# Demo: simulated features (64-dim instead of 2048-dim for speed)
np.random.seed(42)
D = 64
N = 5000

# Real features
real_features = np.random.multivariate_normal(
    mean=np.zeros(D),
    cov=np.eye(D) + 0.1 * np.random.randn(D, D) @ np.random.randn(D, D).T / D,
    size=N
)

# Generated features at different quality levels
quality_levels = {
    "Random noise": np.random.randn(N, D) * 3 + 2,
    "Poor model":   real_features + np.random.randn(N, D) * 2,
    "Good model":   real_features + np.random.randn(N, D) * 0.5,
    "Great model":  real_features + np.random.randn(N, D) * 0.1,
    "Perfect":      real_features + np.random.randn(N, D) * 0.01,
}

print(f"{'Quality':>15} {'FID':>10}")
print("-" * 28)
for name, gen_features in quality_levels.items():
    fid = compute_fid_full(real_features, gen_features)
    print(f"{name:>15} {fid:10.2f}")
```

### 4.6 è«–æ–‡èª­è§£ãƒ•ãƒ­ãƒ¼ï¼ˆ3-Pass Readingï¼‰

```mermaid
graph TD
    P1[Pass 1: é³¥ç°<br>5-10åˆ†] --> P2[Pass 2: æ§‹é€ <br>30-60åˆ†]
    P2 --> P3[Pass 3: å†ç¾<br>æ•°æ™‚é–“]

    P1 -.-> Q1[ã‚¿ã‚¤ãƒˆãƒ«ãƒ»è¦æ—¨ãƒ»å›³è¡¨]
    P2 -.-> Q2[å°å…¥ãƒ»æ‰‹æ³•ãƒ»å®Ÿé¨“ã‚’ç²¾èª­]
    P3 -.-> Q3[å…¨å°å‡ºã‚’è¿½ã„ã€ã‚³ãƒ¼ãƒ‰ã§å†ç¾]

    style P1 fill:#e8f5e9
    style P2 fill:#fff3e0
    style P3 fill:#fce4ec
```

:::details æœ¬è¬›ç¾©ã®è«–æ–‡: Goodfellow+ (2014) "Generative Adversarial Nets" â€” Pass 1 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```python
paper_pass1 = {
    "title": "Generative Adversarial Nets",
    "authors": "Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, Bengio",
    "year": 2014,
    "venue": "NeurIPS 2014",
    "arxiv": "1406.2661",

    "problem": "How to train a generative model without explicit density estimation?",
    "approach": "Adversarial training: Generator G vs Discriminator D in minimax game",
    "key_equation": "min_G max_D E[log D(x)] + E[log(1-D(G(z)))]",
    "key_result": "At Nash equilibrium, p_g = p_data (Theorem 1)",
    "connection_to_this_lecture": {
        "MLE": "GAN avoids MLE entirely â€” no likelihood computation needed",
        "KL": "Optimal GAN minimizes JSD, which is symmetric KL variant",
        "Implicit_model": "GAN = canonical implicit model (Mohamed 2016)",
        "Evaluation": "Early GAN evaluation relied on visual inspection â†’ FID came later",
    },

    "5_minute_summary": (
        "Instead of maximizing likelihood, pit two networks against each other. "
        "The generator tries to fool the discriminator, the discriminator tries to "
        "distinguish real from fake. At convergence, the generator perfectly mimics "
        "the data distribution. Brilliant in simplicity, unstable in practice."
    ),

    "questions_for_pass2": [
        "How is the Nash equilibrium proven? (Theorem 1)",
        "What happens when discriminator is too strong?",
        "Why does mode collapse occur in practice?",
        "How does this relate to f-divergence variational bounds?",
    ]
}

for key, val in paper_pass1.items():
    if isinstance(val, dict):
        print(f"\n{key}:")
        for k, v in val.items():
            print(f"  {k}: {v}")
    elif isinstance(val, list):
        print(f"\n{key}:")
        for item in val:
            print(f"  - {item}")
    else:
        print(f"{key}: {val}")
```
:::

### 4.7 æ¨å®šé‡ã®åˆ†é¡ãƒãƒ£ãƒ¼ãƒˆ â€” å®Ÿè£…ã§ã®åˆ¤æ–­ãƒ•ãƒ­ãƒ¼

```mermaid
flowchart TD
    Start[ç¢ºç‡ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãŒå¿…è¦] --> Q1{å°¤åº¦ q_Î¸ x<br>ã®è¨ˆç®—ãŒå¿…è¦?}

    Q1 -->|Yes| Q2{æ½œåœ¨å¤‰æ•°<br>ã‚’ä½¿ã†?}
    Q1 -->|No| Q3{ã‚µãƒ³ãƒ—ãƒ«å“è³ª<br>ã‚’é‡è¦–?}

    Q2 -->|Yes| VAE[å¤‰åˆ†MLE<br>ELBOæœ€å¤§åŒ–]
    Q2 -->|No| Q4{å¯é€†å¤‰æ›<br>ãŒå¯èƒ½?}

    Q4 -->|Yes| Flow[å¤‰æ•°å¤‰æ›MLE<br>æ­£ç¢ºãªå°¤åº¦]
    Q4 -->|No| AR[è‡ªå·±å›å¸°MLE<br>GPT, PixelCNN]

    Q3 -->|Yes| Q5{è¨“ç·´ã®å®‰å®šæ€§<br>ã‚‚é‡è¦?}
    Q3 -->|No| GAN[æš—é»™çš„æ¨å®šé‡<br>æ•µå¯¾çš„è¨“ç·´]

    Q5 -->|Yes| Diff[ã‚¹ã‚³ã‚¢æ¨å®šé‡<br>DDPM]
    Q5 -->|No| GAN

    VAE -.->|ã¼ã‚„ã‘ã‚‹| ImproveVAE[VQ-VAE, Hierarchical VAE]
    GAN -.->|ä¸å®‰å®š| ImproveGAN[StyleGAN, WGAN-GP]
    Diff -.->|é…ã„| ImproveDiff[DDIM, Consistency Model]
    Flow -.->|è¡¨ç¾åŠ›| ImproveFlow[Glow, Neural ODE]

    style VAE fill:#e8f5e9
    style GAN fill:#fff3e0
    style Flow fill:#e3f2fd
    style Diff fill:#fce4ec
```

:::message
**é€²æ—: 70% å®Œäº†** â€” MLE ã®å®Œå…¨å®Ÿè£…ã€é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€FID è¨ˆç®—ã€è«–æ–‡èª­è§£ãƒ•ãƒ­ãƒ¼ã‚’ç¿’å¾—ã—ãŸã€‚ã“ã“ã‹ã‚‰è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆã«å…¥ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ã¨å®Ÿé¨“

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

:::details Q1: $\hat{\theta}_\text{MLE} = \arg\max_\theta \sum_{i=1}^{N} \log q_\theta(x_i)$ ã‚’æ—¥æœ¬èªã§èª­ã¿ä¸Šã’ã¦ãã ã•ã„
ã€Œã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆ MLE ã¯ã€ã‚·ãƒ¼ã‚¿ã«ã¤ã„ã¦ã€$i = 1$ ã‹ã‚‰ $N$ ã¾ã§ã® $\log q_\theta(x_i)$ ã®ç·å’Œã‚’æœ€å¤§åŒ–ã™ã‚‹å¼•æ•°ã€‚ã€
æ„å‘³: ãƒ‡ãƒ¼ã‚¿ã®å¯¾æ•°å°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ãŒ MLEã€‚Fisher (1922) [^1] ãŒä½“ç³»åŒ–ã—ãŸæ¨å®šæ³•ã€‚
:::

:::details Q2: $p_\theta(x_1, \ldots, x_T) = \prod_{t=1}^{T} p_\theta(x_t | x_{<t})$ ã¯ä½•ã‚’è¡¨ã™ï¼Ÿ
è‡ªå·±å›å¸°åˆ†è§£ã€‚åŒæ™‚åˆ†å¸ƒã‚’ã€å„æ™‚åˆ»ã®æ¡ä»¶ä»˜ãåˆ†å¸ƒã®ç©ã«åˆ†è§£ã™ã‚‹ã€‚GPT ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã“ã®å½¢å¼ã§å®šç¾©ã•ã‚Œã‚‹ã€‚$x_t$ ã¯ $t$ ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã€$x_{<t}$ ã¯ãã‚Œä»¥å‰ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã€‚
:::

:::details Q3: $D^*_G(x) = \frac{p_\text{data}(x)}{p_\text{data}(x) + p_g(x)}$ ã¯ã©ã†ã„ã†æ„å‘³ï¼Ÿ
GAN ã®æœ€é©åˆ¤åˆ¥å™¨ã€‚$p_\text{data}(x)$ ã¨ $p_g(x)$ ã®æ¯”ç‡ã«åŸºã¥ã„ã¦ã€å…¥åŠ›ãŒæœ¬ç‰©ã‹å½ç‰©ã‹ã‚’åˆ¤å®šã™ã‚‹ã€‚$p_g = p_\text{data}$ ã®ã¨ã $D^* = 0.5$ï¼ˆåŒºåˆ¥ä¸èƒ½ï¼‰ã€‚Goodfellow+ (2014) [^2] ã®å®šç†1ã€‚
:::

:::details Q4: $\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r\Sigma_g)^{1/2})$ ã®å„é …ã¯ï¼Ÿ
ç¬¬1é … $\|\mu_r - \mu_g\|^2$: å¹³å‡ã®å·®ï¼ˆç‰¹å¾´ç©ºé–“ã§ã®ã€Œä½ç½®ãšã‚Œã€ï¼‰ã€‚ç¬¬2é …: å…±åˆ†æ•£ã®å·®ï¼ˆã€Œå½¢çŠ¶ã®é•ã„ã€ï¼‰ã€‚$\text{Tr}$ ã¯ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆå¯¾è§’è¦ç´ ã®å’Œï¼‰ã€‚Heusel+ (2017) [^4] ãŒææ¡ˆã€‚ä½ã„ã»ã©è‰¯ã„ã€‚
:::

:::details Q5: $\nabla_x \log p(x)$ ã¯ãªãœæ­£è¦åŒ–å®šæ•°ã«ä¾å­˜ã—ãªã„ï¼Ÿ
$\log p(x) = \log \tilde{p}(x) - \log Z$ã€‚$\nabla_x$ ã§å¾®åˆ†ã™ã‚‹ã¨ $\log Z$ ã¯å®šæ•°ãªã®ã§æ¶ˆãˆã‚‹: $\nabla_x \log p(x) = \nabla_x \log \tilde{p}(x)$ã€‚ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« [^10] ã®æ ¸å¿ƒã€‚
:::

:::details Q6: $\mathcal{L}_\text{simple} = \mathbb{E}_{t, x_0, \epsilon}[\|\epsilon - \epsilon_\theta(x_t, t)\|^2]$ ã¯ã©ã‚“ãªæå¤±ï¼Ÿ
DDPM [^5] ã® simple lossã€‚æ™‚åˆ» $t$ ã§ãƒã‚¤ã‚º $\epsilon$ ã‚’åŠ ãˆãŸ $x_t$ ã‹ã‚‰ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $\epsilon_\theta$ ãŒãƒã‚¤ã‚ºã‚’äºˆæ¸¬ã™ã‚‹ã€‚äºˆæ¸¬ã¨çœŸã®ãƒã‚¤ã‚ºã® MSE ã‚’æœ€å°åŒ–ã€‚denoising score matching ã¨ç­‰ä¾¡ã€‚
:::

:::details Q7: $\text{IS} = \exp(\mathbb{E}_{x \sim p_g}[D_\text{KL}(p(y|x) \| p(y))])$ ã®ç›´æ„Ÿã¯ï¼Ÿ
å„ç”Ÿæˆç”»åƒã®åˆ†é¡ç¢ºç‡ $p(y|x)$ ãŒé‹­ãï¼ˆå“è³ªãŒé«˜ã„ï¼‰ã€ã‹ã¤å…¨ä½“ã®å‘¨è¾ºåˆ†å¸ƒ $p(y)$ ãŒä¸€æ§˜ã«è¿‘ã„ï¼ˆå¤šæ§˜æ€§ãŒé«˜ã„ï¼‰ã¨ãã€KL ãŒå¤§ãããªã‚Š IS ãŒé«˜ããªã‚‹ã€‚Salimans+ (2016) [^8]ã€‚æœ€å¤§å€¤ã¯ã‚¯ãƒ©ã‚¹æ•°ã€‚
:::

:::details Q8: æ˜ç¤ºçš„æ¨å®šé‡ã¨æš—é»™çš„æ¨å®šé‡ã®é•ã„ã‚’ä¸€è¨€ã§
æ˜ç¤ºçš„æ¨å®šé‡ï¼ˆPrescribedï¼‰: å°¤åº¦ $q_\theta(x)$ ã®å€¤ãŒè¨ˆç®—å¯èƒ½ã€‚æš—é»™çš„æ¨å®šé‡ï¼ˆImplicitï¼‰: å°¤åº¦ã¯è¨ˆç®—ä¸èƒ½ã ãŒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯å¯èƒ½ã€‚Mohamed & Lakshminarayanan (2016) [^6] ã®åˆ†é¡ã€‚
:::

:::details Q9: $\log q_\theta(x) = \log p(f^{-1}(x)) + \log |\det \frac{\partial f^{-1}}{\partial x}|$ ã¯ä½•ã®å¼ï¼Ÿ
Normalizing Flow [^7] ã®å¯¾æ•°å°¤åº¦ã€‚å¤‰æ•°å¤‰æ›å…¬å¼ã€‚$f$ ã¯å¯é€†å¤‰æ›ã€$p(z)$ ã¯åŸºåº•åˆ†å¸ƒã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®è¡Œåˆ—å¼ãŒä½“ç©å¤‰åŒ–ã‚’è£œæ­£ã™ã‚‹ã€‚
:::

:::details Q10: $H(\hat{p}, q_\theta) = H(\hat{p}) + D_\text{KL}(\hat{p} \| q_\theta)$ ãŒMLE ã«é‡è¦ãªç†ç”±ã¯ï¼Ÿ
CE æœ€å°åŒ– = KL æœ€å°åŒ–ã®è¨¼æ˜ã®æ ¸å¿ƒã€‚$H(\hat{p})$ ã¯ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã§ $\theta$ ã«ä¾å­˜ã—ãªã„ã‹ã‚‰ã€CE ã‚’æœ€å°åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ KL ã‚’æœ€å°åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ä¸€è‡´ã™ã‚‹ã€‚ç¬¬6å›ã®å®šç† 3.4 ã¨æœ¬è¬›ç¾©ã®å®šç† 3.2-3.3 ã‚’æ¥ç¶šã™ã‚‹å¼ã€‚
:::

### 5.2 LaTeX è¨˜è¿°ãƒ†ã‚¹ãƒˆ

:::details L1: MLE ã®å®šç¾©ã‚’ LaTeX ã§æ›¸ã„ã¦ãã ã•ã„
```latex
\hat{\theta}_{\text{MLE}} = \arg\max_{\theta} \frac{1}{N} \sum_{i=1}^{N} \log q_{\theta}(x_i)
```
:::

:::details L2: GAN ã®ç›®çš„é–¢æ•°ã‚’ LaTeX ã§æ›¸ã„ã¦ãã ã•ã„
```latex
\min_G \max_D \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p(z)}[\log(1 - D(G(z)))]
```
:::

:::details L3: FID ã®å®šç¾©ã‚’ LaTeX ã§æ›¸ã„ã¦ãã ã•ã„
```latex
\text{FID} = \|\mu_r - \mu_g\|^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)
```
:::

:::details L4: å¤‰æ•°å¤‰æ›å…¬å¼ï¼ˆFlowï¼‰ã‚’ LaTeX ã§æ›¸ã„ã¦ãã ã•ã„
```latex
\log q_{\theta}(x) = \log p(f^{-1}(x)) + \log \left|\det \frac{\partial f^{-1}}{\partial x}\right|
```
:::

:::details L5: DDPM ã®æå¤±é–¢æ•°ã‚’ LaTeX ã§æ›¸ã„ã¦ãã ã•ã„
```latex
\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, x_0, \epsilon}\left[\|\epsilon - \epsilon_{\theta}(x_t, t)\|^2\right]
```
:::

### 5.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

:::details C1: $\hat{\mu}_\text{MLE} = \frac{1}{N}\sum_{i=1}^{N} x_i$ ã‚’ Python ã§
```python
mu_mle = np.mean(data)
# or explicitly: mu_mle = np.sum(data) / len(data)
```
:::

:::details C2: $D_\text{KL}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)}$ ã‚’ Python ã§
```python
kl = np.sum(p * np.log(p / (q + 1e-10)))
# with numerical stability: kl = np.sum(p * (np.log(p + 1e-10) - np.log(q + 1e-10)))
```
:::

:::details C3: Softmax $p_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$ ã‚’æ•°å€¤å®‰å®šã« Python ã§
```python
def softmax(z):
    z_shifted = z - np.max(z)  # numerical stability
    exp_z = np.exp(z_shifted)
    return exp_z / exp_z.sum()
```
:::

:::details C4: Cross-Entropy Loss $\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \log q_\theta(x_i)$ ã‚’ Python ã§
```python
ce_loss = -np.mean(np.log(q_theta(data) + 1e-10))
```
:::

:::details C5: Reparameterization trick $z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$ ã‚’ Python ã§
```python
epsilon = np.random.normal(0, 1, size=mu.shape)
z = mu + sigma * epsilon  # gradient flows through mu and sigma
```
:::

### 5.4 MLE å®Ÿé¨“: åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°æ¯”è¼ƒ

```python
import numpy as np
from scipy import stats

np.random.seed(42)

# True distributions to fit
distributions = {
    "Normal(3, 2)": np.random.normal(3, 2, 5000),
    "Exponential(2)": np.random.exponential(2, 5000),
    "Bimodal": np.concatenate([np.random.normal(-2, 0.5, 2500),
                                np.random.normal(3, 1, 2500)]),
    "Uniform(0,5)": np.random.uniform(0, 5, 5000),
    "Heavy-tailed (t, df=3)": np.random.standard_t(3, 5000),
}

# Fit single Gaussian via MLE to each
print(f"{'Distribution':>25} {'Î¼Ì‚':>8} {'ÏƒÌ‚':>8} {'KL approx':>12}")
print("-" * 56)

for name, data in distributions.items():
    mu_hat = np.mean(data)
    sigma_hat = np.std(data)

    # Approximate KL via histogram
    bins = np.linspace(data.min() - 1, data.max() + 1, 200)
    hist, bin_edges = np.histogram(data, bins=bins, density=True)
    centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    q_model = stats.norm.pdf(centers, mu_hat, sigma_hat)

    mask = (hist > 1e-10) & (q_model > 1e-10)
    dx = centers[1] - centers[0]
    kl = np.sum(hist[mask] * np.log(hist[mask] / q_model[mask]) * dx)

    print(f"{name:>25} {mu_hat:8.3f} {sigma_hat:8.3f} {kl:12.4f}")

print("\nâ†’ Gaussian MLE works well for Normal data, poorly for Bimodal/Heavy-tailed")
print("â†’ Model family MATTERS. MLE finds the best within the family, not the best overall.")
```

### 5.5 æ¨å®šé‡ã®åˆ†é¡ãƒãƒ£ãƒ¼ãƒˆä½œæˆ

```python
# Create comprehensive estimator taxonomy (by likelihood access)
taxonomy = {
    "Explicit Estimators (Prescribed)": {
        "Autoregressive": {
            "examples": ["GPT", "PixelCNN", "WaveNet"],
            "density": "exact (product of conditionals)",
            "sampling": "sequential (slow)",
            "papers": ["van den Oord+ 2016"],
        },
        "VAE": {
            "examples": ["VAE", "Î²-VAE", "VQ-VAE", "Hierarchical VAE"],
            "density": "lower bound (ELBO)",
            "sampling": "one-shot (fast)",
            "papers": ["Kingma & Welling 2013"],
        },
        "Normalizing Flow": {
            "examples": ["NICE", "Real NVP", "Glow", "Neural ODE"],
            "density": "exact (change of variables)",
            "sampling": "one-shot (fast)",
            "papers": ["Dinh+ 2014", "Rezende & Mohamed 2015"],
        },
    },
    "Implicit Estimators": {
        "GAN": {
            "examples": ["GAN", "DCGAN", "StyleGAN", "BigGAN"],
            "density": "not available",
            "sampling": "one-shot (fast)",
            "papers": ["Goodfellow+ 2014"],
        },
    },
    "Score Estimators": {
        "Score Matching": {
            "examples": ["NCSN", "Sliced Score Matching"],
            "density": "not directly (score only)",
            "sampling": "Langevin dynamics (slow)",
            "papers": ["Song & Ermon 2019"],
        },
        "Diffusion": {
            "examples": ["DDPM", "DDIM", "Stable Diffusion", "DALL-E 2"],
            "density": "lower bound (variational)",
            "sampling": "iterative denoising (slow, improving)",
            "papers": ["Sohl-Dickstein+ 2015", "Ho+ 2020"],
        },
    },
}

for category, subcategories in taxonomy.items():
    print(f"\n{'='*60}")
    print(f"  {category}")
    print(f"{'='*60}")
    for name, info in subcategories.items():
        print(f"\n  {name}")
        for key, val in info.items():
            if isinstance(val, list):
                print(f"    {key}: {', '.join(val)}")
            else:
                print(f"    {key}: {val}")
```

### 5.6 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: 1D æ¨å®šé‡æ¯”è¼ƒ

```python
import numpy as np
from scipy import stats

np.random.seed(42)

# ========================================
# Mini-project: Compare generative approaches on 1D data
# ========================================

# True distribution: mixture of 3 Gaussians
def sample_true(n):
    components = np.random.choice(3, size=n, p=[0.3, 0.4, 0.3])
    mus = [-3, 1, 5]
    sigmas = [0.6, 0.8, 0.5]
    return np.array([np.random.normal(mus[c], sigmas[c]) for c in components])

def true_density(x):
    return (0.3 * stats.norm.pdf(x, -3, 0.6) +
            0.4 * stats.norm.pdf(x, 1, 0.8) +
            0.3 * stats.norm.pdf(x, 5, 0.5))

data = sample_true(5000)

# === Approach 1: MLE with single Gaussian ===
mu1 = np.mean(data)
sig1 = np.std(data)
model1_density = lambda x: stats.norm.pdf(x, mu1, sig1)

# === Approach 2: MLE with Gaussian Mixture (3 components, simple EM) ===
# Initialize
mus = np.array([-2.0, 0.0, 4.0])
sigs = np.array([1.0, 1.0, 1.0])
pis = np.array([1/3, 1/3, 1/3])

for _ in range(100):  # EM iterations
    # E-step
    resp = np.zeros((len(data), 3))
    for k in range(3):
        resp[:, k] = pis[k] * stats.norm.pdf(data, mus[k], sigs[k])
    resp /= resp.sum(axis=1, keepdims=True) + 1e-300

    # M-step
    Nk = resp.sum(axis=0)
    for k in range(3):
        mus[k] = np.sum(resp[:, k] * data) / (Nk[k] + 1e-10)
        sigs[k] = np.sqrt(np.sum(resp[:, k] * (data - mus[k])**2) / (Nk[k] + 1e-10))
        sigs[k] = max(sigs[k], 0.01)
        pis[k] = Nk[k] / len(data)

order = np.argsort(mus)
model2_density = lambda x: sum(pis[k] * stats.norm.pdf(x, mus[k], sigs[k]) for k in range(3))

# === Approach 3: KDE (Nonparametric) ===
bandwidth = 0.3
model3_density = lambda x: sum(stats.norm.pdf(x, xi, bandwidth) for xi in data) / len(data)

# === Evaluate: KL divergence approximation ===
x_eval = np.linspace(-6, 8, 2000)
p_true = true_density(x_eval)
dx = x_eval[1] - x_eval[0]

def approx_kl(p, q_fn, x_grid, dx):
    q = np.array([q_fn(xi) for xi in x_grid]) if callable(q_fn) else q_fn
    mask = (p > 1e-10) & (q > 1e-10)
    return np.sum(p[mask] * np.log(p[mask] / q[mask]) * dx)

# Model 1 evaluation
q1 = model1_density(x_eval)
kl1 = approx_kl(p_true, q1, x_eval, dx)

# Model 2 evaluation
q2 = np.array([model2_density(xi) for xi in x_eval])
kl2 = approx_kl(p_true, q2, x_eval, dx)

# Model 3 evaluation (vectorized for speed)
q3 = np.zeros_like(x_eval)
for xi in data[:500]:  # subsample for speed
    q3 += stats.norm.pdf(x_eval, xi, bandwidth)
q3 /= 500
kl3 = approx_kl(p_true, q3, x_eval, dx)

print("=== 1D Generative Model Comparison ===")
print(f"{'Model':>20} {'KL(p||q)':>12} {'Verdict':>20}")
print("-" * 55)
print(f"{'Single Gaussian':>20} {kl1:12.4f} {'Underfitting':>20}")
print(f"{'GMM (K=3)':>20} {kl2:12.4f} {'Good fit':>20}")
print(f"{'KDE (h=0.3)':>20} {kl3:12.4f} {'Nonparametric fit':>20}")

print(f"\nGMM recovered parameters (sorted by Î¼):")
for i, k in enumerate(order):
    print(f"  Component {i}: Ï€={pis[k]:.3f}, Î¼={mus[k]:.3f}, Ïƒ={sigs[k]:.3f}")
print(f"True:          Ï€=[0.30, 0.40, 0.30], Î¼=[-3, 1, 5], Ïƒ=[0.6, 0.8, 0.5]")
```

### 5.7 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: Langevin Dynamics ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```python
import numpy as np

def langevin_sampling_2d(score_fn, n_samples=500, step_size=0.01, n_steps=1000):
    """
    Langevin dynamics in 2D:
    x_{t+1} = x_t + Î· Â· âˆ‡ log p(x_t) + âˆš(2Î·) Â· noise
    """
    # Initialize from broad distribution
    x = np.random.randn(n_samples, 2) * 5
    trajectory = [x.copy()]

    for t in range(n_steps):
        score = score_fn(x)
        noise = np.random.randn(*x.shape)
        x = x + step_size * score + np.sqrt(2 * step_size) * noise
        if t % 100 == 0:
            trajectory.append(x.copy())

    return x, trajectory

# Target: mixture of 4 Gaussians in 2D
means = np.array([[-3, -3], [-3, 3], [3, -3], [3, 3]])
sigma = 0.7

def score_gmm(x):
    """Score function âˆ‡_x log p(x) for 2D GMM"""
    # p(x) = (1/4) Î£ N(x; Î¼_k, ÏƒÂ²I)
    # âˆ‡ log p(x) = Î£ w_k(x) Â· (-(x - Î¼_k)/ÏƒÂ²)
    # where w_k(x) = N(x;Î¼_k,ÏƒÂ²I) / Î£_j N(x;Î¼_j,ÏƒÂ²I)
    densities = np.zeros((x.shape[0], 4))
    for k in range(4):
        diff = x - means[k]
        densities[:, k] = np.exp(-0.5 * np.sum(diff**2, axis=1) / sigma**2)

    weights = densities / (densities.sum(axis=1, keepdims=True) + 1e-300)

    score = np.zeros_like(x)
    for k in range(4):
        score += weights[:, k:k+1] * (-(x - means[k]) / sigma**2)

    return score

# Run Langevin dynamics
np.random.seed(42)
final_samples, trajectory = langevin_sampling_2d(score_gmm, n_samples=500,
                                                  step_size=0.005, n_steps=2000)

# Analyze results
print("=== Langevin Dynamics Sampling (2D GMM) ===")
print(f"Target: 4 Gaussians at {means.tolist()}, Ïƒ={sigma}")
print(f"\nFinal sample statistics:")
print(f"  Mean: [{final_samples[:, 0].mean():.2f}, {final_samples[:, 1].mean():.2f}]")
print(f"  Std:  [{final_samples[:, 0].std():.2f}, {final_samples[:, 1].std():.2f}]")

# Check if samples are near the modes
for k, mu in enumerate(means):
    near_mode = np.sum(np.linalg.norm(final_samples - mu, axis=1) < 2 * sigma)
    print(f"  Near mode {k} ({mu}): {near_mode} samples ({near_mode/500*100:.1f}%)")

print(f"\nTrajectory: {len(trajectory)} snapshots over 2000 steps")
print(f"â†’ Score-based sampling works! Samples converge to modes.")
print(f"â†’ This is how NCSN [Song & Ermon 2019] generates images.")
```

### 5.8 ãƒŸãƒ‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: MLE vs MAP æ¨å®šã®æ¯”è¼ƒ

```python
import numpy as np
from scipy import stats

np.random.seed(42)

# Small sample MLE vs MAP comparison
# True: Î¼ = 5.0, Ïƒ = 1.0
true_mu = 5.0
true_sigma = 1.0

# Prior for MAP: Î¼ ~ N(0, Ï„Â²) with Ï„ = 3
prior_mu = 0.0
prior_tau = 3.0

print(f"True Î¼ = {true_mu}, True Ïƒ = {true_sigma}")
print(f"Prior: Î¼ ~ N({prior_mu}, {prior_tau}Â²)")
print()
print(f"{'N':>5} {'MLE Î¼Ì‚':>10} {'MAP Î¼Ì‚':>10} {'MLE err':>10} {'MAP err':>10} {'Better':>8}")
print("-" * 58)

for N in [2, 5, 10, 20, 50, 100, 1000]:
    mle_errors = []
    map_errors = []
    n_trials = 2000

    for _ in range(n_trials):
        data = np.random.normal(true_mu, true_sigma, N)

        # MLE
        mu_mle = np.mean(data)

        # MAP with Gaussian prior
        # Posterior: N(Î¼_MAP, Ïƒ_MAPÂ²)
        # Î¼_MAP = (N/ÏƒÂ² Â· xÌ„ + 1/Ï„Â² Â· Î¼_0) / (N/ÏƒÂ² + 1/Ï„Â²)
        precision_lik = N / true_sigma**2
        precision_prior = 1.0 / prior_tau**2
        mu_map = (precision_lik * mu_mle + precision_prior * prior_mu) / \
                 (precision_lik + precision_prior)

        mle_errors.append((mu_mle - true_mu)**2)
        map_errors.append((mu_map - true_mu)**2)

    mle_mse = np.mean(mle_errors)
    map_mse = np.mean(map_errors)
    better = "MAP" if map_mse < mle_mse else "MLE"

    print(f"{N:5d} {np.mean([np.random.normal(true_mu, true_sigma, N).mean() for _ in range(100)]):10.3f} "
          f"{'â€”':>10} {mle_mse:10.4f} {map_mse:10.4f} {better:>8}")

print("\nâ†’ MAP wins with small N (prior helps), MLE wins with large N (data dominates)")
print("â†’ MAP = MLE + regularization. This is why weight decay works in deep learning.")
```

### 5.9 è‡ªå·±ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```
- [ ] MLE ã®å®šç¾©ã‚’å¼ã¨è¨€è‘‰ã®ä¸¡æ–¹ã§èª¬æ˜ã§ãã‚‹
- [ ] MLE = CE æœ€å°åŒ– = KL æœ€å°åŒ–ã®ç­‰ä¾¡æ€§ã‚’å°å‡ºã§ãã‚‹
- [ ] Fisher ã®æ¼¸è¿‘3æ€§è³ªï¼ˆä¸€è‡´æ€§ãƒ»æ¼¸è¿‘æ­£è¦æ€§ãƒ»æœ‰åŠ¹æ€§ï¼‰ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Prescribed model ã¨ Implicit model ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] MLE ã®4å¤‰å½¢ï¼ˆå¤‰åˆ†/æš—é»™çš„/å¤‰æ•°å¤‰æ›/ã‚¹ã‚³ã‚¢ï¼‰ã®æå¤±é–¢æ•°ã‚’æ›¸ã‘ã‚‹
- [ ] FID ã®è¨ˆç®—å¼ã¨ç›´æ„Ÿçš„æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹
- [ ] IS ã¨ CMMD ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Mode-covering ã¨ mode-seeking ã®é•ã„ã‚’å›³ã§èª¬æ˜ã§ãã‚‹
- [ ] GAN ã®æœ€é©åˆ¤åˆ¥å™¨ã‚’å°å‡ºã§ãã‚‹
- [ ] ã‚¹ã‚³ã‚¢é–¢æ•°ãŒæ­£è¦åŒ–å®šæ•°ã«ä¾å­˜ã—ãªã„ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] LLM è¨“ç·´ãŒ MLE ã§ã‚ã‚‹ã“ã¨ã‚’å¼ã§ç¤ºã›ã‚‹
- [ ] æ¬¡å…ƒã®å‘ªã„ãŒå¯†åº¦æ¨å®šã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¬æ˜ã§ãã‚‹
```

:::message
**é€²æ—: 85% å®Œäº†** â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆå®Œäº†ã€‚ã“ã“ã‹ã‚‰ç™ºå±•ã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.3 çµ±è¨ˆçš„è·é›¢ã®å•é¡Œç‚¹ã¨æœ€æ–°å‹•å‘ â€” MLE beyond i.i.d.

FID [^4] ã¯äº‹å®Ÿä¸Šã®æ¨™æº–çš„çµ±è¨ˆçš„è·é›¢ã ãŒã€æ·±åˆ»ãªå•é¡ŒãŒã‚ã‚‹ã€‚

```python
# FID's problems
problems = {
    "Inception-v3 ãŒå¤ã„": {
        "issue": "2015å¹´ã®ãƒ¢ãƒ‡ãƒ«ã€‚CLIP/DINO ãŒé¥ã‹ã«è‰¯ã„ç‰¹å¾´é‡ã‚’æŠ½å‡º",
        "impact": "ãƒ†ã‚¯ã‚¹ãƒãƒ£åé‡ã€ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹è»½è¦–",
        "alternative": "FD-DINOv2, CMMD (CLIP-based)"
    },
    "ã‚¬ã‚¦ã‚¹ä»®å®š": {
        "issue": "ç‰¹å¾´é‡ãŒã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«å¾“ã†ä»®å®šã¯ä¸€èˆ¬ã«ä¸æ­£ç¢º",
        "impact": "å¤šå³°çš„ãªç‰¹å¾´åˆ†å¸ƒã§ä¸æ­£ç¢º",
        "alternative": "CMMD (ã‚«ãƒ¼ãƒãƒ«æ³•ã€åˆ†å¸ƒä»®å®šãªã—)"
    },
    "ã‚µãƒ³ãƒ—ãƒ«ãƒã‚¤ã‚¢ã‚¹": {
        "issue": "FID ã¯ N ã«ä¾å­˜ã™ã‚‹ãƒã‚¤ã‚¢ã‚¹ã‚’æŒã¤",
        "impact": "ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ã¨ä¸å½“ã«é«˜ã„ FID",
        "alternative": "CMMD (ä¸åæ¨å®šé‡)"
    },
    "äººé–“åˆ¤æ–­ã¨ã®ä¸ä¸€è‡´": {
        "issue": "FID ãŒä½ã„ã®ã«äººé–“ã«ã¯ä½å“è³ªã«è¦‹ãˆã‚‹å ´åˆãŒã‚ã‚‹",
        "impact": "è©•ä¾¡æŒ‡æ¨™ã®ä¿¡é ¼æ€§ä½ä¸‹",
        "alternative": "CMMD + äººé–“è©•ä¾¡ã®çµ„ã¿åˆã‚ã›"
    },
}

for name, info in problems.items():
    print(f"\nå•é¡Œ: {name}")
    for key, val in info.items():
        print(f"  {key}: {val}")
```

Jayasumana+ (2024) [^9] ã¯ CMMD ã‚’ææ¡ˆã—ã€ã“ã‚Œã‚‰ã®å•é¡Œã®å¤šãã‚’è§£æ±ºã—ãŸã€‚CMMD ã¯ CLIP ç‰¹å¾´é‡ + ã‚¬ã‚¦ã‚¹ RBF ã‚«ãƒ¼ãƒãƒ«ã® MMD ã§ã€ä¸åæ¨å®šé‡ã‹ã¤åˆ†å¸ƒä»®å®šä¸è¦ã€‚

### 6.4 æ¨å®šé‡ã®æ¼¸è¿‘æ¯”è¼ƒ

| æ¨å®šé‡ã®ç‰¹æ€§ | å¤‰åˆ†MLE [^3] | æš—é»™çš„æ¨å®š [^2] | å¤‰æ•°å¤‰æ›MLE [^7][^11][^12] | ã‚¹ã‚³ã‚¢æ¨å®š [^5][^13] | è‡ªå·±å›å¸°MLE |
|:-----|:---------|:---------|:---------------------|:---------------------|:---------|
| **å°¤åº¦ã‚¢ã‚¯ã‚»ã‚¹** | ä¸‹ç•Œ (ELBO) | è¨ˆç®—ä¸èƒ½ | æ­£ç¢º | ä¸è¦ï¼ˆã‚¹ã‚³ã‚¢ã®ã¿ï¼‰ | æ­£ç¢º |
| **æ¨å®šç²¾åº¦** | ä¸­ï¼ˆmode-coveringï¼‰ | é«˜ï¼ˆmode-seekingï¼‰ | ä¸­ã€œé«˜ | **æœ€é«˜** | é«˜ |
| **æ¨å®šã®å®‰å®šæ€§** | **é«˜** | ä½ï¼ˆä¸å®‰å®šï¼‰ | é«˜ | **é«˜** | **é«˜** |
| **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é€Ÿåº¦** | **é€Ÿã„** (1-shot) | **é€Ÿã„** (1-shot) | **é€Ÿã„** (1-shot) | é…ã„ (T steps) | é…ã„ (T steps) |
| **æ½œåœ¨å¤‰æ•°** | ã‚ã‚Šï¼ˆæ»‘ã‚‰ã‹ï¼‰ | ãªã— (ç›´æ¥) | ã‚ã‚Šï¼ˆå¯é€†ï¼‰ | ã‚ã‚Šï¼ˆãƒã‚¤ã‚ºï¼‰ | ãªã— |
| **ãƒ¢ãƒ¼ãƒ‰å´©å£Š** | ãªã— | **ã‚ã‚Š** | ãªã— | ãªã— | ãªã— |
| **æ•°å­¦çš„åŸºç›¤** | å¤‰åˆ†æ¨è«– | ã‚²ãƒ¼ãƒ ç†è«– | å¤‰æ•°å¤‰æ› | ç¢ºç‡SDE / Score | ç¢ºç‡ã®é€£é–å¾‹ |
| **æå¤±ã®æœ€å°åŒ–å¯¾è±¡** | -ELBO | JSD | -log p(x) | $\|\epsilon - \hat\epsilon\|^2$ | CE |
| **ä»£è¡¨çš„æˆåŠŸä¾‹** | Î²-VAE, VQ-VAE | StyleGAN3 | Glow | Stable Diffusion | GPT-4 |
| **æœ¬ã‚·ãƒªãƒ¼ã‚º** | ç¬¬9-10å› | ç¬¬12-14å› | ç¬¬11å› | ç¬¬15,25-32å› | ç¬¬16å› |

### 6.5 Densing Law ã¨èƒ½åŠ›å¯†åº¦

æœ€æ–°ã®ç ”ç©¶ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ã€Œèƒ½åŠ›å¯†åº¦ã€ï¼ˆcapability densityï¼‰ã«æ³¨ç›®ã™ã‚‹å‹•ããŒã‚ã‚‹ã€‚åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§ã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’é”æˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯èƒ½åŠ›å¯†åº¦ãŒé«˜ã„ã€‚

```python
# Capability density concept
models = {
    "GPT-3 (2020)":    {"params_B": 175,  "benchmark": 70, "density": 70/175},
    "LLaMA-2 (2023)":  {"params_B": 70,   "benchmark": 75, "density": 75/70},
    "Mistral (2023)":   {"params_B": 7,    "benchmark": 68, "density": 68/7},
    "Phi-3 (2024)":     {"params_B": 3.8,  "benchmark": 69, "density": 69/3.8},
}

print(f"{'Model':>20} {'Params(B)':>10} {'Score':>8} {'Density':>10}")
print("-" * 52)
for name, info in models.items():
    print(f"{name:>20} {info['params_B']:10.1f} {info['benchmark']:8.0f} "
          f"{info['density']:10.2f}")

print("\nâ†’ Densing Law: capability density increases over time")
print("â†’ Smaller models achieve higher scores per parameter")
print("â†’ Implication: efficiency matters as much as scale")
```

ã“ã®å‚¾å‘ã¯å¯†åº¦æ¨å®šãƒ¢ãƒ‡ãƒ«ã«ã‚‚å½“ã¦ã¯ã¾ã‚‹ã€‚Stable Diffusion 3 ã¯å‰ä¸–ä»£ã‚ˆã‚Šå°ã•ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§ã‚ˆã‚Šé«˜å“è³ªãªç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã€‚åŠ¹ç‡ã®è¿½æ±‚ãŒã€æ¬¡ã®ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã ã€‚

### 6.6 Simulation-Based Inference â€” æš—é»™çš„æ¨å®šé‡ã®ç§‘å­¦å¿œç”¨

å¯†åº¦æ¨å®šãƒ»æ¨å®šé‡è¨­è¨ˆã¯ç”»åƒç”Ÿæˆã ã‘ã®ã‚‚ã®ã§ã¯ãªã„ã€‚ç§‘å­¦ã®ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã§ã€Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®é€†å•é¡Œã€ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚

| åˆ†é‡ | å¿œç”¨ | æ¨å®šé‡ã®å½¹å‰² |
|:-----|:-----|:----------------|
| ç²’å­ç‰©ç†å­¦ | LHC ã®è¡çªãƒ‡ãƒ¼ã‚¿ | ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿â†’ãƒ‡ãƒ¼ã‚¿ã®é€†æ¨å®š |
| å®‡å®™è«– | CMB ãƒ‡ãƒ¼ã‚¿ | å®‡å®™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å¾Œæ¨å®š |
| æ°—å€™ç§‘å­¦ | æ°—å€™ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ– |
| å‰µè–¬ | åˆ†å­ç”Ÿæˆ | ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã§ã®æ¢ç´¢ |
| ææ–™ç§‘å­¦ | çµæ™¶æ§‹é€ äºˆæ¸¬ | æ¡ä»¶ä»˜ãç”Ÿæˆ |
| è›‹ç™½è³ªè¨­è¨ˆ | ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€  | æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç”Ÿæˆ |

:::details World Models â€” å¯†åº¦æ¨å®šã®æ–°ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ 
å¯†åº¦æ¨å®šã‚’ã€Œä¸–ç•Œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã€ã¨ã—ã¦æ‰ãˆã‚‹æ½®æµãŒã‚ã‚‹ã€‚Sora (2024) ãŒãƒ“ãƒ‡ã‚ªç”Ÿæˆã§è¦‹ã›ãŸã®ã¯ã€ç‰©ç†æ³•å‰‡ã‚’æš—é»™çš„ã«å­¦ç¿’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å¯èƒ½æ€§ã ã€‚$p(x_{t+1} | x_{\leq t}, a)$ï¼ˆè¡Œå‹• $a$ ã«å¯¾ã™ã‚‹æ¬¡ã®ä¸–ç•ŒçŠ¶æ…‹ã®äºˆæ¸¬ï¼‰ã¯ã€å¼·åŒ–å­¦ç¿’ã®ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ãã®ã‚‚ã®ã§ã‚ã‚Šã€å¯†åº¦æ¨å®šã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®èåˆç‚¹ã ã€‚
:::

### 6.7 Identifiability å•é¡Œ

å¯†åº¦æ¨å®šã«ã¯æ ¹æœ¬çš„ãªç†è«–çš„å•é¡ŒãŒã‚ã‚‹ â€” åŒã˜ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p(x)$ ã‚’å®Ÿç¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ä¸€èˆ¬ã«ä¸€æ„ã§ã¯ãªã„ã€‚

$$q_{\theta_1}(x) = q_{\theta_2}(x) \quad \forall x, \quad \text{but} \quad \theta_1 \neq \theta_2$$

ä¾‹ãˆã° GMM ã®æˆåˆ†ã‚’ãƒ©ãƒ™ãƒ«å…¥ã‚Œæ›¿ãˆã—ã¦ã‚‚å°¤åº¦ã¯ä¸å¤‰ï¼ˆlabel switching problemï¼‰ã€‚VAE ã®æ½œåœ¨ç©ºé–“ã‚‚å›è»¢ä¸å¤‰æ€§ã‚’æŒã¤ã€‚ã“ã‚Œã¯ MLE ã®ç†è«–çš„å¸°çµã§ã‚ã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£é‡ˆã«æ³¨æ„ãŒå¿…è¦ãªã“ã¨ã‚’ç¤ºã™ã€‚

```python
import numpy as np

# Label switching: permuting components doesn't change likelihood
# GMM with K=2: (Ï€â‚, Î¼â‚, Ïƒâ‚, Ï€â‚‚, Î¼â‚‚, Ïƒâ‚‚)
theta1 = {"pi": [0.3, 0.7], "mu": [-2, 3], "sigma": [0.5, 1.0]}
theta2 = {"pi": [0.7, 0.3], "mu": [3, -2], "sigma": [1.0, 0.5]}  # swapped!

def gmm_likelihood(x, params):
    ll = 0
    for k in range(2):
        ll += params["pi"][k] * np.exp(-0.5 * ((x - params["mu"][k]) / params["sigma"][k])**2) \
              / (params["sigma"][k] * np.sqrt(2 * np.pi))
    return ll

x_test = np.array([0.0, 1.0, -1.5, 2.5])
ll1 = [gmm_likelihood(xi, theta1) for xi in x_test]
ll2 = [gmm_likelihood(xi, theta2) for xi in x_test]

print("Identifiability problem: label switching")
print(f"Î¸â‚: Ï€={theta1['pi']}, Î¼={theta1['mu']}, Ïƒ={theta1['sigma']}")
print(f"Î¸â‚‚: Ï€={theta2['pi']}, Î¼={theta2['mu']}, Ïƒ={theta2['sigma']}")
print(f"\nLikelihoods at test points:")
for xi, l1, l2 in zip(x_test, ll1, ll2):
    print(f"  x={xi:5.1f}: L(Î¸â‚)={l1:.6f}, L(Î¸â‚‚)={l2:.6f}, equal={np.isclose(l1, l2)}")
print(f"\nâ†’ Different parameters, SAME likelihood â†’ MLE is NOT unique")
print(f"â†’ For K components, there are K! equivalent solutions")
print(f"â†’ K=10: 10! = {np.math.factorial(10):,} equivalent solutions!")
```

### 6.8 MLEâ†’EMâ†’å¤‰åˆ†æ¨è«– â€” æ¨è«–ã®å›°é›£åº¦ãƒãƒƒãƒ—

```mermaid
graph TD
    subgraph "Course I: æ•°å­¦åŸºç›¤ (å®Œäº†)"
        L6[ç¬¬6å›: KL, CE, Adam]
        L7[ç¬¬7å›: MLE, æ¨å®šé‡ã®åˆ†é¡<br>â† æœ¬è¬›ç¾©]
    end

    subgraph "Course II: ç¢ºç‡ãƒ¢ãƒ‡ãƒ«åŸºç¤ (ç¬¬8-16å›)"
        L8[ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ»EM]
        L9[ç¬¬9å›: VAE]
        L10[ç¬¬10å›: VAE ç™ºå±•]
        L11[ç¬¬11å›: Flow]
        L12[ç¬¬12å›: GAN]
        L13[ç¬¬13å›: GAN ç™ºå±•]
        L14[ç¬¬14å›: è©•ä¾¡æŒ‡æ¨™ æ·±å €ã‚Š]
        L15[ç¬¬15å›: Diffusion åŸºç¤]
        L16[ç¬¬16å›: Transformer]
    end

    L6 --> L7
    L7 -->|MLE ã®é™ç•Œâ†’æ½œåœ¨å¤‰æ•°| L8
    L8 -->|ELBOâ†’å¤‰åˆ†MLE| L9
    L9 -->|å¤‰åˆ†æ¨å®šé‡ã®æ‹¡å¼µ| L10
    L7 -->|å¤‰æ•°å¤‰æ›æ¨å®šé‡| L11
    L7 -->|æš—é»™çš„æ¨å®šé‡| L12
    L12 --> L13
    L7 -->|çµ±è¨ˆçš„è·é›¢| L14
    L7 -->|ã‚¹ã‚³ã‚¢æ¨å®šé‡| L15
    L7 -->|è‡ªå·±å›å¸° MLE| L16

    style L7 fill:#ff9800,color:#fff
    style L8 fill:#e8f5e9
    style L9 fill:#e8f5e9
    style L12 fill:#fff3e0
    style L15 fill:#fce4ec
```

ã“ã®å›³ã®é€šã‚Šã€æœ¬è¬›ç¾©ã§ç¯‰ã„ãŸæ¨å®šé‡ã®æ•°å­¦çš„åŸºç›¤ã¯ç¬¬8-16å›ã®å…¨ã¦ã«æ¥ç¶šã—ã¦ã„ã‚‹ã€‚å„è¬›ç¾©ã§æˆ»ã£ã¦ãã‚‹ãŸã³ã«ã€æ¨å®šåŸç†ã®ç†è§£ãŒæ·±ã¾ã‚‹ã€‚

:::details ç”¨èªé›†ï¼ˆæœ¬è¬›ç¾©ã§å°å…¥ã—ãŸå…¨ç”¨èªï¼‰

| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| æœ€å°¤æ¨å®š | Maximum Likelihood Estimation (MLE) | å°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šæ³• |
| å¯¾æ•°å°¤åº¦ | Log-Likelihood | $\sum \log q_\theta(x_i)$ã€‚å°¤åº¦ã®å¯¾æ•° |
| çµŒé¨“åˆ†å¸ƒ | Empirical Distribution | $\hat{p}(x) = \frac{1}{N}\sum \delta(x-x_i)$ |
| åˆ¤åˆ¥ãƒ¢ãƒ‡ãƒ« | Discriminative Model | $p(y|x)$ ã‚’å­¦ç¿’ã™ã‚‹ãƒ¢ãƒ‡ãƒ« |
| ç”Ÿæˆãƒ¢ãƒ‡ãƒ« | Generative Model | $p(x)$ ã‚’æ¨å®šã™ã‚‹ç¢ºç‡ãƒ¢ãƒ‡ãƒ« |
| æ˜ç¤ºçš„æ¨å®šé‡ | Prescribed Estimator | å°¤åº¦ãŒé™½ã«è¨ˆç®—å¯èƒ½ãªæ¨å®šé‡ |
| æš—é»™çš„æ¨å®šé‡ | Implicit Estimator | ã‚µãƒ³ãƒ—ãƒ«ã®ã¿å¯èƒ½ã€å°¤åº¦è¨ˆç®—ä¸èƒ½ |
| å¤šæ§˜ä½“ä»®èª¬ | Manifold Hypothesis | ãƒ‡ãƒ¼ã‚¿ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«é›†ä¸­ |
| æ¬¡å…ƒã®å‘ªã„ | Curse of Dimensionality | é«˜æ¬¡å…ƒã§å¯†åº¦æ¨å®šãŒæŒ‡æ•°çš„ã«å›°é›£ |
| ã‚¹ã‚³ã‚¢é–¢æ•° | Score Function | $\nabla_x \log p(x)$ã€‚å¯†åº¦ã®å‹¾é… |
| Mode-Covering | Mode-Covering | å…¨ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼ï¼ˆå‰å‘ã KLï¼‰ |
| Mode-Seeking | Mode-Seeking | ç‰¹å®šãƒ¢ãƒ¼ãƒ‰ã«é›†ä¸­ï¼ˆé€†å‘ã KLï¼‰ |
| FID | Frechet Inception Distance | ç”Ÿæˆç”»åƒã¨å®Ÿç”»åƒã® Frechet è·é›¢ |
| IS | Inception Score | ç”Ÿæˆå“è³ªã¨å¤šæ§˜æ€§ã®æŒ‡æ¨™ |
| CMMD | CLIP Maximum Mean Discrepancy | FID ã®æ”¹è‰¯æŒ‡æ¨™ |
| å¤‰æ•°å¤‰æ›å…¬å¼ | Change of Variables | Flow ãƒ¢ãƒ‡ãƒ«ã®å°¤åº¦è¨ˆç®—ã®åŸºç¤ |
| è‡ªå·±å›å¸°åˆ†è§£ | Autoregressive Decomposition | $p(x) = \prod p(x_t | x_{<t})$ |
| Reparameterization | Reparameterization Trick | $z = \mu + \sigma\epsilon$ ã§å‹¾é…ä¼æ’­ |
| Langevin å‹•åŠ›å­¦ | Langevin Dynamics | ã‚¹ã‚³ã‚¢ã«åŸºã¥ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| Fisher æƒ…å ±è¡Œåˆ— | Fisher Information Matrix | $\mathcal{I}(\theta) = -\mathbb{E}[\nabla^2 \log p]$ |
| ä¸€è‡´æ€§ | Consistency | MLE ãŒçœŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åæŸã™ã‚‹æ€§è³ª |
| æ¼¸è¿‘æ­£è¦æ€§ | Asymptotic Normality | MLE ã®åˆ†å¸ƒãŒæ­£è¦ã«è¿‘ã¥ãæ€§è³ª |
| æ¼¸è¿‘æœ‰åŠ¹æ€§ | Asymptotic Efficiency | MLE ãŒæœ€å°åˆ†æ•£ã‚’é”æˆã™ã‚‹æ€§è³ª |
| ELBO | Evidence Lower Bound | $\log p(x)$ ã®å¤‰åˆ†ä¸‹ç•Œ |
| ç¥–å…ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | Ancestral Sampling | æ¡ä»¶ä»˜ãåˆ†å¸ƒã®é€£é–ã§ã‚µãƒ³ãƒ—ãƒ« |
| é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | Importance Sampling | ææ¡ˆåˆ†å¸ƒã‹ã‚‰ã®é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒ« |
| éå¹³è¡¡ç†±åŠ›å­¦ | Nonequilibrium Thermodynamics | Diffusion ãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†çš„ç€æƒ³ |
:::

:::details ä¸ç­‰å¼ãƒ»ç­‰å¼ã¾ã¨ã‚

| ç­‰å¼/ä¸ç­‰å¼ | æ•°å¼ | æ„å‘³ |
|:-----------|:-----|:-----|
| MLE = CE æœ€å°åŒ– | $\arg\max \sum \log q_\theta(x_i) = \arg\min H(\hat{p}, q_\theta)$ | å®šç† 3.2 |
| MLE = KL æœ€å°åŒ– | $\arg\min H(\hat{p}, q_\theta) = \arg\min D_\text{KL}(\hat{p} \| q_\theta)$ | å®šç† 3.3 |
| CE åˆ†è§£ | $H(\hat{p}, q_\theta) = H(\hat{p}) + D_\text{KL}(\hat{p} \| q_\theta)$ | ç¬¬6å› å®šç† 3.4 |
| GAN æœ€é©åˆ¤åˆ¥å™¨ | $D^*(x) = \frac{p_\text{data}}{p_\text{data} + p_g}$ | å®šç† 3.8a |
| GAN = JSD | $V(D^*, G) = -\log 4 + 2 \cdot \text{JSD}$ | å®šç† 3.8b |
| Fisher æ¼¸è¿‘ | $\sqrt{N}(\hat\theta - \theta^*) \to \mathcal{N}(0, \mathcal{I}^{-1})$ | æ€§è³ª 3.4b |
| Flow å°¤åº¦ | $\log q(x) = \log p(f^{-1}(x)) + \log |\det J|$ | å®šç† 3.7 |
| Score æ­£è¦åŒ–ä¸å¤‰ | $\nabla_x \log p(x) = \nabla_x \log \tilde{p}(x)$ | å®šç¾© 3.9 |
| ELBO | $\log p(x) \geq \text{ELBO}$ | ç¬¬8å› å…ˆå–ã‚Š |
:::

### 6.9 çŸ¥è­˜ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—

```mermaid
mindmap
  root((ç¬¬7å›))
    æœ€å°¤æ¨å®š
      Fisher 1922
      MLE = CEæœ€å°åŒ–
      MLE = KLæœ€å°åŒ–
      æ¼¸è¿‘3æ€§è³ª
        ä¸€è‡´æ€§
        æ¼¸è¿‘æ­£è¦æ€§
        æ¼¸è¿‘æœ‰åŠ¹æ€§
      é™ç•Œ
        ãƒ¢ãƒ‡ãƒ«æ—ä¾å­˜
        é«˜æ¬¡å…ƒå›°é›£
        å‘¨è¾ºåŒ–ä¸èƒ½
    æ¨å®šé‡ã®åˆ†é¡
      æ˜ç¤ºçš„æ¨å®šé‡
        VAE å¤‰åˆ†MLE
        Flow å¤‰æ•°å¤‰æ›MLE
        è‡ªå·±å›å¸°MLE
      æš—é»™çš„æ¨å®šé‡
        GAN
      ã‚¹ã‚³ã‚¢æ¨å®šé‡
        NCSN
        DDPM
    çµ±è¨ˆçš„è·é›¢
      FID W2è·é›¢
      KID MMD
      CMMD CLIP-MMD
    LLMæ¥ç¶š
      æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬
      è‡ªå·±å›å¸°MLE
      Perplexity
    æ¨å®šåŸç†ã®å¤‰å½¢
      KLâ†’æå¤±è¨­è¨ˆ
      JSDâ†’æš—é»™çš„æ¨å®š
      å¤‰æ•°å¤‰æ›â†’Flow
      Scoreâ†’Diffusion
```

### 6.10 æœ¬è¬›ç¾©ã®ã‚­ãƒ¼ãƒ†ã‚¤ã‚¯ã‚¢ã‚¦ã‚§ã‚¤

1. **MLE = CE æœ€å°åŒ– = KL æœ€å°åŒ–** â€” ã“ã®ä¸‰ä½ä¸€ä½“ãŒçµ±è¨ˆçš„æ¨å®šã®æ ¹å¹¹ã€‚ç¬¬6å›ã®æƒ…å ±ç†è«–ã¨æœ¬è¬›ç¾©ã® MLE ãŒåˆæµã—ãŸã€‚
2. **æ¨å®šé‡ã¯å°¤åº¦é–¢æ•°ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹ã§åˆ†é¡**ã§ãã‚‹: æ˜ç¤ºçš„ï¼ˆå¤‰åˆ†MLE, å¤‰æ•°å¤‰æ›MLE, è‡ªå·±å›å¸°MLEï¼‰ã€æš—é»™çš„ï¼ˆGANï¼‰ã€ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ï¼ˆDDPMï¼‰ã€‚å„ã€…ãŒç•°ãªã‚‹æ•°å­¦çš„åŸºç›¤ã‚’æŒã¤ã€‚
3. **æ˜ç¤ºçš„ vs æš—é»™çš„æ¨å®šé‡** â€” å°¤åº¦ãŒè¨ˆç®—å¯èƒ½ã‹å¦ã‹ã§æ¨å®šæ–¹æ³•ãŒæ ¹æœ¬çš„ã«ç•°ãªã‚‹ã€‚ã“ã®åˆ†é¡ãŒç¬¬8-16å›ã®å…¨ã¦ã®å‡ºç™ºç‚¹ã€‚
4. **çµ±è¨ˆçš„è·é›¢ã¯æ¨å®šé‡ã®è©•ä¾¡åŸç†** â€” FIDï¼ˆ$W_2$ è·é›¢ï¼‰ã¯æ¨™æº–ã ãŒé™ç•ŒãŒã‚ã‚‹ã€‚KID, CMMD ãŒæ”¹å–„ã‚’ææ¡ˆã€‚ã€Œä½•ã‚’ã‚‚ã£ã¦è‰¯ã„æ¨å®šã¨ã™ã‚‹ã‹ã€ã¯æ·±ã„å•ã„ã€‚

### 6.11 FAQ

:::details Q1: MLE ã¯ç”»åƒç”Ÿæˆä»¥å¤–ã«ã©ã†ä½¿ã‚ã‚Œã‚‹ï¼Ÿ
ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆGPT = è‡ªå·±å›å¸°MLEï¼‰ã€éŸ³å£°åˆæˆï¼ˆWaveNetï¼‰ã€åˆ†å­è¨­è¨ˆï¼ˆå‰µè–¬ï¼‰ã€ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€ äºˆæ¸¬ã€æ°—å€™ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ç•°å¸¸æ¤œçŸ¥ã€å…¨ã¦ãŒã€Œç¢ºç‡åˆ†å¸ƒã®æ¨å®šã€å•é¡Œã ã€‚MLE ã¨ãã®å¤‰å½¢ã¯ã€ç¢ºç‡åˆ†å¸ƒã§è¡¨ç¾ã§ãã‚‹ã‚ã‚‰ã‚†ã‚‹ãƒ‡ãƒ¼ã‚¿ã«é©ç”¨å¯èƒ½ã€‚
:::

:::details Q2: å¤‰åˆ†MLEï¼ˆVAEï¼‰ã¨æš—é»™çš„æ¨å®šï¼ˆGANï¼‰ã€ã©ã¡ã‚‰ãŒè‰¯ã„ï¼Ÿ
æ¨å®šã®ç›®çš„ã«ã‚ˆã‚‹ã€‚å¤‰åˆ†MLE: å°¤åº¦ãŒè¨ˆç®—å¯èƒ½ã€æ¨å®šãŒå®‰å®šã€æ½œåœ¨ç©ºé–“ãŒæ»‘ã‚‰ã‹ â†’ è¡¨ç¾å­¦ç¿’ã€åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ã€‚æš—é»™çš„æ¨å®š: ã‚µãƒ³ãƒ—ãƒ«å“è³ªãŒé«˜ã„ã€é®®æ˜ãªå‡ºåŠ› â†’ é«˜å“è³ªç”Ÿæˆã€è¶…è§£åƒã€‚2024å¹´ç¾åœ¨ã€å¤šãã®ã‚¿ã‚¹ã‚¯ã§ã‚¹ã‚³ã‚¢æ¨å®šé‡ï¼ˆDiffusion Modelï¼‰ãŒä¸¡æ–¹ã‚’ä¸Šå›ã‚‹ã€‚ã€Œã©ã¡ã‚‰ãŒè‰¯ã„ã€ã‚ˆã‚Šã€Œä½•ã‚’æ¨å®šã™ã‚‹ã‹ã€ã§é¸ã¶ã¹ãã€‚
:::

:::details Q3: Diffusion Model ã¯ãªãœã“ã‚Œã»ã©æˆåŠŸã—ãŸï¼Ÿ
3ã¤ã®ç†ç”±: (1) è¨“ç·´ãŒå®‰å®šï¼ˆå˜ç´”ãª MSE æå¤±ï¼‰ã€(2) ã‚µãƒ³ãƒ—ãƒ«å“è³ªãŒé«˜ã„ï¼ˆæ®µéšçš„ãªãƒã‚¤ã‚ºé™¤å»ï¼‰ã€(3) ç†è«–çš„åŸºç›¤ãŒå …å›ºï¼ˆã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚° + ç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼ï¼‰ã€‚DDPM [^5] ãŒå“è³ªã§ GAN ã«åŒ¹æ•µã—ã€ãƒ¢ãƒ¼ãƒ‰å´©å£Šãªã—ã®è¨“ç·´ã‚’å®Ÿç¾ã—ãŸã“ã¨ãŒè»¢æ›ç‚¹ã ã£ãŸã€‚
:::

:::details Q4: FIDï¼ˆçµ±è¨ˆçš„è·é›¢ï¼‰ã®çµ¶å¯¾å€¤ã¯ã©ã†è§£é‡ˆã™ã‚‹ï¼Ÿ
å¤§ã¾ã‹ã«: FID < 10 = æ¨å®šãŒéå¸¸ã«è‰¯ã„ã€10-50 = è‰¯ã„ã€50-100 = ã¾ã‚ã¾ã‚ã€> 100 = æ‚ªã„ã€‚ãŸã ã—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤§ããä¾å­˜ã™ã‚‹ã€‚CelebAï¼ˆé¡”ï¼‰ã¯ FID ãŒä½ããªã‚Šã‚„ã™ãã€ImageNetï¼ˆä¸€èˆ¬ç”»åƒï¼‰ã¯é«˜ããªã‚Šã‚„ã™ã„ã€‚åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã§ã®ç›¸å¯¾æ¯”è¼ƒãŒæœ‰åŠ¹ã€‚æ•°å­¦çš„ã«ã¯ã‚¬ã‚¦ã‚¹è¿‘ä¼¼ $W_2$ è·é›¢ã§ã‚ã‚‹ã“ã¨ã‚’å¸¸ã«æ„è­˜ã™ã¹ãã€‚
:::

:::details Q5: MLE ä»¥å¤–ã®æ¨å®šæ³•ã¯ãªã„ã®ã‹ï¼Ÿ
MAPï¼ˆMaximum A Posterioriï¼‰æ¨å®š: MLE + äº‹å‰åˆ†å¸ƒã€‚ãƒ™ã‚¤ã‚ºæ¨å®š: äº‹å¾Œåˆ†å¸ƒå…¨ä½“ã‚’æ¨å®šã€‚æ–¹æ³•ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼ˆMethod of Momentsï¼‰ã€‚æœ€å°è·é›¢æ¨å®šã€‚MLE ãŒæœ€ã‚‚åºƒãä½¿ã‚ã‚Œã‚‹ç†ç”±ã¯ã€æ¼¸è¿‘çš„ãªæœ€é©æ€§ï¼ˆFisher ã®å®šç†ï¼‰ã¨è¨ˆç®—ã®å®¹æ˜“ã•ã€‚
:::

:::details Q6: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTï¼‰ã¯æ˜ç¤ºçš„æ¨å®šé‡ï¼Ÿ
ãã†ã ã€‚$p(x_t | x_{<t})$ ãŒé™½ã«è¨ˆç®—å¯èƒ½ï¼ˆsoftmax å‡ºåŠ›ï¼‰ãªã®ã§ã€æ˜ç¤ºçš„æ¨å®šé‡ï¼ˆPrescribedï¼‰ã€‚å¯¾æ•°å°¤åº¦ã‚‚æ­£ç¢ºã«è¨ˆç®—ã§ãã‚‹ã€‚ã“ã‚ŒãŒ LLM ã® Perplexity = $2^{H}$ ã‚’è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ä½¿ãˆã‚‹ç†ç”±ã€‚
:::

:::details Q7: æ¬¡å…ƒã®å‘ªã„ã¯å›é¿ã§ããªã„ã®ã‹ï¼Ÿ
å®Œå…¨ã«ã¯å›é¿ã§ããªã„ãŒã€ç·©å’Œç­–ãŒã‚ã‚‹: (1) å¤šæ§˜ä½“ä»®èª¬ã‚’åˆ©ç”¨ï¼ˆä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ï¼‰ã€(2) åˆ†å‰²çµ±æ²»ï¼ˆè‡ªå·±å›å¸°ã¯1æ¬¡å…ƒãšã¤ï¼‰ã€(3) éšå±¤çš„æ§‹é€ ï¼ˆHierarchical VAEï¼‰ã€(4) ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆDiffusion ã¯æ®µéšçš„ï¼‰ã€‚å…¨ã¦ã®æˆåŠŸã—ãŸæ¨å®šé‡ã¯ã€ä½•ã‚‰ã‹ã®å½¢ã§æ¬¡å…ƒã®å‘ªã„ã‚’å›é¿ã—ã¦ã„ã‚‹ã€‚
:::

:::details Q8: KL æœ€å°åŒ–ã¨ Wasserstein è·é›¢æœ€å°åŒ–ã®é•ã„ã¯ï¼Ÿ
KL: å¯†åº¦æ¯” $p/q$ ã«åŸºã¥ãã€‚$q = 0$ ã®å ´æ‰€ã§ $p > 0$ ãªã‚‰ $\infty$ã€‚æ”¯æŒé›†åˆãŒç•°ãªã‚‹ã¨ä½¿ãˆãªã„ã€‚Wasserstein: ã€Œè³ªé‡ã‚’ç§»å‹•ã™ã‚‹ã‚³ã‚¹ãƒˆã€ã«åŸºã¥ãã€‚æ”¯æŒé›†åˆãŒç•°ãªã£ã¦ã‚‚å®šç¾©ã§ãã‚‹ã€‚WGAN [Arjovsky+ 2017] ãŒ Wasserstein è·é›¢ã§ GAN ã‚’å®‰å®šåŒ–ã•ã›ãŸã€‚ç¬¬13å›ã§è©³ã—ãæ‰±ã†ã€‚
:::

:::details Q9: ã€Œæ¨å®šé‡ã®è¨­è¨ˆãŒå…¨ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®æ ¹åº•ã«ã‚ã‚‹ã€ã¨ã¯ã©ã†ã„ã†æ„å‘³ï¼Ÿ
ç”»åƒç”Ÿæˆã¯ã€Œç”»åƒã®ç¢ºç‡åˆ†å¸ƒ $p(\text{image})$ ã®æ¨å®šã€ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¯ã€Œæ–‡ã®ç¢ºç‡åˆ†å¸ƒ $p(\text{text})$ ã®æ¨å®šã€ã€‚å¿œç”¨ã¯é•ã†ãŒã€æ•°å­¦ã¯åŒã˜ â€” å°¤åº¦é–¢æ•°ã‚’æœ€å¤§åŒ–ã™ã‚‹æ¨å®šé‡ã®è¨­è¨ˆã ã€‚ã ã‹ã‚‰ã“ãã€MLE ã‚„ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã„ã†å…±é€šã®æ¨å®šåŸç†ãŒå…¨ã¦ã«é€šç”¨ã™ã‚‹ã€‚
:::

:::details Q10: ã“ã®è¬›ç¾©ã®å†…å®¹ã¯ã€å®Ÿå‹™ã§ã©ã®ç¨‹åº¦å¿…è¦ï¼Ÿ
MLE = CE = KL ã®ç­‰ä¾¡æ€§ã¯ LLM ã‚’ä½¿ã†å…¨ã¦ã®äººã«å¿…é ˆã€‚æ¨å®šé‡ã®åˆ†é¡ä½“ç³»ã®ç†è§£ã¯ã€é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«é¸æŠã«ä¸å¯æ¬ ã€‚FID/KID/CMMD ã®æ•°å­¦çš„ç†è§£ã¯è«–æ–‡ã‚’èª­ã‚€éš›ã«å¿…è¦ã€‚ã€Œã¨ã‚Šã‚ãˆãš Diffusionã€ã§ã¯ãªãã€Œãªãœã‚¹ã‚³ã‚¢æ¨å®šé‡ãŒé©åˆ‡ã‹ã€ã‚’ç†è§£ã™ã‚‹åŠ›ã‚’é¤Šã†å›ã€‚
:::

### 6.12 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | å†…å®¹ | ç›®å®‰æ™‚é–“ |
|:---|:-----|:---------|
| Day 1 | Zone 0-2 ã‚’é€šèª­ + æ¬¡å…ƒã®å‘ªã„ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ | 45åˆ† |
| Day 2 | Zone 3 ã® 3.1-3.5ï¼ˆMLE ç†è«–ãƒ‘ãƒ¼ãƒˆï¼‰ã‚’ç´™ã§å°å‡º | 90åˆ† |
| Day 3 | Zone 3 ã® 3.6-3.10ï¼ˆæ¨å®šé‡åˆ†é¡ãƒ»æš—é»™çš„æ¨å®šãƒ»Scoreï¼‰ã‚’ç²¾èª­ | 60åˆ† |
| Day 4 | Zone 3 ã® 3.12-3.14ï¼ˆçµ±è¨ˆçš„è·é›¢ + ãƒœã‚¹æˆ¦ï¼‰+ Zone 4 ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ | 90åˆ† |
| Day 5 | Zone 5 ã®è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ + Goodfellow (2014) è«–æ–‡ Pass 1 | 60åˆ† |
| Day 6 | ãƒœã‚¹æˆ¦ã®ä¸‰ä½ä¸€ä½“ã‚’ç´™ã§å†ç¾ + åˆ†é¡ãƒãƒ£ãƒ¼ãƒˆä½œæˆ | 45åˆ† |
| Day 7 | ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆæœ€çµ‚ç¢ºèª + Zone 6 ã®æ¥ç¶šãƒãƒƒãƒ—ã§å…¨ä½“ã‚’ä¿¯ç° | 30åˆ† |

### 6.13 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```python
lecture7_progress = {
    "zone0_quickstart": True,
    "zone1_experience": True,
    "zone2_intuition": True,
    "zone3_math": {
        "mle_definition": False,       # Can you define MLE?
        "mle_ce_equivalence": False,    # Can you prove MLE = CE?
        "mle_kl_equivalence": False,    # Can you prove MLE = KL?
        "fisher_asymptotics": False,    # Can you state 3 properties?
        "mle_limitations": False,       # Can you list 3 limitations?
        "estimator_classification": False, # Can you classify estimators?
        "flow_change_of_var": False,    # Can you write the formula?
        "gan_objective": False,         # Can you write min-max?
        "optimal_discriminator": False, # Can you derive D*?
        "score_function": False,        # Can you explain score?
        "mode_cover_seek": False,       # Can you explain both?
        "fid_formula": False,           # Can you write FID?
        "llm_mle": False,              # Can you show LLM = MLE?
        "boss_trinity": False,          # Can you show MLE=CE=KL?
    },
    "zone4_implementation": False,
    "zone5_experiment": False,
}

completed = sum(1 for v in lecture7_progress["zone3_math"].values() if v)
total = len(lecture7_progress["zone3_math"])
print(f"Zone 3 progress: {completed}/{total} ({completed/total:.0%})")
print(f"Mark each as True when you can do it WITHOUT looking at notes.")
```

### 6.14 æ¬¡å›äºˆå‘Š â€” ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³•

ç¬¬7å›ã§ã€ŒMLE ã®é™ç•Œã€ã‚’æ˜ç¢ºã«ã—ãŸã€‚å˜ç´”ãªãƒ¢ãƒ‡ãƒ«æ—ã§ã¯è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’æ‰ãˆã‚‰ã‚Œãªã„ã€‚

ç¬¬8å›ã¯ã“ã®é™ç•Œã‚’æ‰“ç ´ã™ã‚‹ã€‚

- **æ½œåœ¨å¤‰æ•°ã®å°å…¥**: $p(x) = \int p(x|z) p(z) dz$ â€” è¦³æ¸¬ã®èƒŒå¾Œã«ã€Œéš ã‚ŒãŸå¤‰æ•°ã€ã‚’ä»®å®šã™ã‚‹
- **EMç®—æ³•**: å‘¨è¾ºå°¤åº¦ãŒè¨ˆç®—ä¸èƒ½ã§ã‚‚ã€E-step ã¨ M-step ã®äº¤äº’æœ€é©åŒ–ã§ MLE ã‚’è¿‘ä¼¼ã™ã‚‹
- **ELBO ã®å°å‡º**: Jensen ã®ä¸ç­‰å¼ã‹ã‚‰ $\log p(x) \geq \text{ELBO}$ ã‚’å°å‡º â€” ã“ã‚ŒãŒ VAE ã®æ•°å­¦çš„åŸºç›¤
- **GMM ã®å®Œå…¨å®Ÿè£…**: æœ¬è¬›ç¾©ã® GMM ã‚’ EM ã§è¨“ç·´ã—ã€å¤šå³°åˆ†å¸ƒã‚’æ­£ã—ãæ‰ãˆã‚‹
- **Python ã®é€Ÿåº¦å•é¡Œ**: EM ã®åå¾©è¨ˆç®—ãŒ Python ã®é™ç•Œã‚’éœ²å‘ˆã™ã‚‹

ç¬¬6å›ã® KL + ç¬¬7å›ã® MLE + ç¬¬8å›ã® ELBOã€‚ã“ã®3ã¤ãŒåˆæµã™ã‚‹ã¨ãã€ç¬¬9å›ã§ VAE ãŒè‡ªç„¶ã«èª•ç”Ÿã™ã‚‹ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ç¬¬7å›ã€Œæœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–ã€å®Œäº†ã€‚Course I ã®æ•°å­¦çš„æ­¦è£…ã¯ 7/8ã€‚æ¬¡å›ã¯æ½œåœ¨å¤‰æ•°ã§ MLE ã®é™ç•Œã‚’æ‰“ç ´ã™ã‚‹ã€‚
:::

### 6.15 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **æ¨å®šé‡ã®è¨­è¨ˆãŒå…¨ã¦ã‚’æ±ºã‚ã‚‹ã€‚VAE/GAN/Flow/Diffusion ã¯ã€MLE ã®100å¹´ã®æ•°å­¦ãŒç”Ÿã‚“ã å¤‰å½¢ã«éããªã„ã®ã§ã¯ï¼Ÿ**

ã“ã®å•ã„ã‚’3ã¤ã®è§’åº¦ã‹ã‚‰è€ƒãˆã¦ã¿ã¦ã»ã—ã„ã€‚

1. **æ•°å­¦çš„ç­‰ä¾¡æ€§**: ç”»åƒç”Ÿæˆã‚‚ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚‚ã€åˆ†å­ç”Ÿæˆã‚‚ã€æ•°å­¦çš„ã«ã¯å…¨ã¦åŒã˜ â€” é«˜æ¬¡å…ƒç¢ºç‡åˆ†å¸ƒ $p(x)$ ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚VAE ã® ELBO ã¯ç”»åƒã«ã‚‚ãƒ†ã‚­ã‚¹ãƒˆã«ã‚‚é©ç”¨ã§ãã‚‹ã€‚GAN ã®æ•µå¯¾çš„è¨“ç·´ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ã‚’å•ã‚ãªã„ã€‚æ¨å®šé‡è¨­è¨ˆã®ã€ŒçœŸã®å§¿ã€ã¯ã€ç‰¹å®šã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ç¸›ã‚‰ã‚Œãªã„**æ±ç”¨çš„ãªç¢ºç‡åˆ†å¸ƒå­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ã ã€‚

2. **ç§‘å­¦çš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ**: AlphaFold 2 ã¯ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæ§‹é€ ã‚’ã€Œç”Ÿæˆã€ã—ã€æ°—å€™ç§‘å­¦è€…ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®å‡ºåŠ›ã‹ã‚‰ã€Œäº‹å¾Œåˆ†å¸ƒã‚’æ¨å®šã€ã™ã‚‹ã€‚ã“ã‚Œã‚‰ã¯ç”»åƒç”Ÿæˆã¨ã¯ç„¡é–¢ä¿‚ã ãŒã€åŒã˜æ•°å­¦çš„é“å…·ï¼ˆMLEã€å¤‰åˆ†æ¨è«–ã€ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ï¼‰ã‚’ä½¿ã£ã¦ã„ã‚‹ã€‚çµ±è¨ˆçš„æ¨è«–ã®æœ€å¤§ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã¯ã€ç”»åƒç”Ÿæˆã§ã¯ãªã**ç§‘å­¦çš„ç™ºè¦‹**ã«ã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚

3. **èªçŸ¥ã®åã‚Š**: ç”»åƒç”ŸæˆãŒæ³¨ç›®ã•ã‚Œã‚‹ã®ã¯ã€äººé–“ã«ã¨ã£ã¦ã€Œè¦–è¦šçš„ã«åˆ†ã‹ã‚Šã‚„ã™ã„ã€ã‹ã‚‰ã«éããªã„ã€‚ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆGPTï¼‰ã¯è¨€èªã®ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’ã§ã‚ã‚Šã€éŸ³å£°åˆæˆã¯æ³¢å½¢ã®ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’ã§ã‚ã‚Šã€åˆ†å­è¨­è¨ˆã¯åŒ–å­¦ç©ºé–“ã®ç¢ºç‡åˆ†å¸ƒã®å­¦ç¿’ã ã€‚ã€Œç”»åƒç”Ÿæˆ AIã€ã¨å‘¼ã¶ã®ã¯ã€æœ¨ã‚’è¦‹ã¦æ£®ã‚’è¦‹ãªã„ã“ã¨ã ã€‚

:::details æ­´å²çš„æ–‡è„ˆ: Fisher ã®ã€Œæœ€å°¤æ³•ã€ã¨ç”Ÿæˆ AI ã®100å¹´
Fisher ãŒ 1922å¹´ã«æœ€å°¤æ¨å®šã‚’ä½“ç³»åŒ–ã—ãŸã¨ã [^1]ã€å½¼ã¯ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šã®ä¸€èˆ¬ç†è«–ã€ã‚’æ§‹ç¯‰ã—ã‚ˆã†ã¨ã—ã¦ã„ãŸã€‚100å¹´å¾Œã€ãã®ã€Œä¸€èˆ¬ç†è«–ã€ãŒ DALL-E ã‚„ Stable Diffusion ã®æ•°å­¦çš„åŸºç›¤ã«ãªã£ã¦ã„ã‚‹ã€‚Fisher ãŒ MLE ã‚’ã€ŒOn the mathematical foundations of theoretical statisticsã€ã¨é¡Œã—ãŸã®ã¯ã€ã€ŒåŸºç›¤ã€ï¼ˆfoundationsï¼‰ã‚’ä½œã‚ã†ã¨ã—ãŸã‹ã‚‰ã ã€‚å®Ÿéš›ã«ãã†ãªã£ãŸ â€” MLE ã¯çµ±è¨ˆå­¦ã®åŸºç›¤ã§ã‚ã‚‹ã ã‘ã§ãªãã€ç”Ÿæˆ AI ã®åŸºç›¤ã§ã‚‚ã‚ã‚‹ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Fisher, R. A. (1922). "On the mathematical foundations of theoretical statistics." *Philosophical Transactions of the Royal Society of London, Series A*, 222, 309-368.
@[card](https://doi.org/10.1098/rsta.1922.0009)

[^2]: Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. (2014). "Generative Adversarial Nets." *NeurIPS 2014*.
@[card](https://arxiv.org/abs/1406.2661)

[^3]: Kingma, D. P. & Welling, M. (2013). "Auto-Encoding Variational Bayes." *ICLR 2014*.
@[card](https://arxiv.org/abs/1312.6114)

[^4]: Heusel, M., Ramsauer, H., Unterthiner, T., et al. (2017). "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium." *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1706.08500)

[^5]: Ho, J., Jain, A. & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS 2020*.
@[card](https://arxiv.org/abs/2006.11239)

[^6]: Mohamed, S. & Lakshminarayanan, B. (2016). "Learning in Implicit Generative Models." *arXiv:1610.03483*.
@[card](https://arxiv.org/abs/1610.03483)

[^7]: Rezende, D. J. & Mohamed, S. (2015). "Variational Inference with Normalizing Flows." *ICML 2015*.
@[card](https://arxiv.org/abs/1505.05770)

[^8]: Salimans, T., Goodfellow, I., Zaremba, W., et al. (2016). "Improved Techniques for Training GANs." *NeurIPS 2016*.
@[card](https://arxiv.org/abs/1606.03498)

[^9]: Jayasumana, S., Ramalingam, S., Veit, A., et al. (2024). "Rethinking FID: Towards a Better Evaluation Metric for Image Generation." *CVPR 2024*.
@[card](https://arxiv.org/abs/2401.09603)

[^10]: Song, Y. & Ermon, S. (2019). "Generative Modeling by Estimating Gradients of the Data Distribution." *NeurIPS 2019*.
@[card](https://arxiv.org/abs/1907.05600)

[^11]: Dinh, L., Krueger, D. & Bengio, Y. (2014). "NICE: Non-linear Independent Components Estimation." *ICLR 2015 Workshop*.
@[card](https://arxiv.org/abs/1410.8516)

[^12]: Dinh, L., Sohl-Dickstein, J. & Bengio, S. (2016). "Density estimation using Real NVP." *ICLR 2017*.
@[card](https://arxiv.org/abs/1605.08803)

[^13]: Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N. & Ganguli, S. (2015). "Deep Unsupervised Learning using Nonequilibrium Thermodynamics." *ICML 2015*.
@[card](https://arxiv.org/abs/1503.03585)

[^14]: CramÃ©r, H. (1946). *Mathematical Methods of Statistics*. Princeton University Press.

[^15]: Rao, C. R. (1945). "Information and the accuracy attainable in the estimation of statistical parameters." *Bulletin of the Calcutta Mathematical Society*, 37, 81-91.

### æ•™ç§‘æ›¸

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Goodfellow, I., Bengio, Y. & Courville, A. (2016). *Deep Learning*. MIT Press. [Free: deeplearningbook.org]
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Free: probml.github.io]
- Cover, T. M. & Thomas, J. A. (2006). *Elements of Information Theory*. 2nd ed. Wiley.

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | èª­ã¿æ–¹ | æ„å‘³ | åˆå‡º |
|:-----|:-------|:-----|:-----|
| $\hat{\theta}_\text{MLE}$ | ã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆ ã‚¨ãƒ ã‚¨ãƒ«ã‚¤ãƒ¼ | æœ€å°¤æ¨å®šé‡ | å®šç¾© 3.1 |
| $q_\theta(x)$ | ã‚­ãƒ¥ãƒ¼ ã‚·ãƒ¼ã‚¿ ã‚¨ãƒƒã‚¯ã‚¹ | ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®å¯†åº¦ | å®šç¾© 3.1 |
| $p_\text{data}(x)$ | ãƒ”ãƒ¼ ãƒ‡ãƒ¼ã‚¿ | ãƒ‡ãƒ¼ã‚¿ã®çœŸã®åˆ†å¸ƒ | Zone 0 |
| $\hat{p}(x)$ | ãƒ”ãƒ¼ãƒãƒƒãƒˆ | çµŒé¨“åˆ†å¸ƒ | å®šç† 3.2 |
| $H(\hat{p}, q_\theta)$ | ã‚¨ã‚¤ãƒ | Cross-Entropy | å®šç† 3.2 |
| $D_\text{KL}(\hat{p} \| q_\theta)$ | ã‚±ãƒ¼ã‚¨ãƒ« | KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç† 3.3 |
| $\mathcal{I}(\theta)$ | ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼ ã‚¢ã‚¤ | Fisher æƒ…å ±è¡Œåˆ— | æ€§è³ª 3.4b |
| $G_\theta(z)$ | ã‚¸ãƒ¼ ã‚·ãƒ¼ã‚¿ | GAN ã®ç”Ÿæˆå™¨ | å®šç¾© 3.8 |
| $D_\phi(x)$ | ãƒ‡ã‚£ãƒ¼ ãƒ•ã‚¡ã‚¤ | GAN ã®åˆ¤åˆ¥å™¨ | å®šç¾© 3.8 |
| $D_\text{JS}$ | ã‚¸ã‚§ãƒ¼ã‚¨ã‚¹ | Jensen-Shannon ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | å®šç† 3.8b |
| $s_\theta(x)$ | ã‚¨ã‚¹ ã‚·ãƒ¼ã‚¿ | ã‚¹ã‚³ã‚¢é–¢æ•°ã®æ¨å®š | å®šç¾© 3.9 |
| $\nabla_x \log p(x)$ | ãƒŠãƒ–ãƒ© ã‚¨ãƒƒã‚¯ã‚¹ | ã‚¹ã‚³ã‚¢é–¢æ•°ï¼ˆçœŸï¼‰ | å®šç¾© 3.9 |
| $\epsilon_\theta(x_t, t)$ | ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ ã‚·ãƒ¼ã‚¿ | DDPM ã®ãƒã‚¤ã‚ºäºˆæ¸¬å™¨ | 3.9 |
| $\text{FID}$ | ã‚¨ãƒ•ã‚¢ã‚¤ãƒ‡ã‚£ãƒ¼ | Frechet Inception Distance | å®šç¾© 3.12a |
| $\text{IS}$ | ã‚¢ã‚¤ã‚¨ã‚¹ | Inception Score | å®šç¾© 3.12b |
| $\text{CMMD}$ | ã‚·ãƒ¼ã‚¨ãƒ ã‚¨ãƒ ãƒ‡ã‚£ãƒ¼ | CLIP MMD | å®šç¾© 3.12c |
| $f^{-1}$ | ã‚¨ãƒ• ã‚¤ãƒ³ãƒãƒ¼ã‚¹ | Flow ã®é€†å¤‰æ› | å®šç† 3.7 |
| $\det J$ | ãƒ‡ãƒƒãƒˆ ã‚¸ã‚§ãƒ¼ | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ | å®šç† 3.7 |
| $p(z)$ | ãƒ”ãƒ¼ ã‚¼ãƒƒãƒˆ | æ½œåœ¨ç©ºé–“ã®äº‹å‰åˆ†å¸ƒ | 3.6 |
| $x_t$ | ã‚¨ãƒƒã‚¯ã‚¹ ãƒ†ã‚£ãƒ¼ | æ‹¡æ•£éç¨‹ã®æ™‚åˆ» $t$ ã®çŠ¶æ…‹ | 3.9 |
| $\text{ELBO}$ | ã‚¨ãƒ«ãƒœ | å¤‰åˆ†ä¸‹ç•Œï¼ˆç¬¬8å›ã§å°å‡ºï¼‰ | 3.5 |
| $\pi_k$ | ãƒ‘ã‚¤ ã‚±ãƒ¼ | æ··åˆä¿‚æ•°ï¼ˆGMMï¼‰ | 4.1 |
| $\gamma_{nk}$ | ã‚¬ãƒ³ãƒ | è²¬ä»»åº¦ï¼ˆEM ã® E-stepï¼‰ | 4.1 |
| $G_{\theta\#}\mu$ | ãƒ—ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ | Pushforward æ¸¬åº¦ | 2.7 |
| $\mathcal{M}$ | ã‚¨ãƒ  | ãƒ‡ãƒ¼ã‚¿å¤šæ§˜ä½“ | 2.5 |
| $D^*_G(x)$ | ãƒ‡ã‚£ãƒ¼ã‚¹ã‚¿ãƒ¼ | GAN ã®æœ€é©åˆ¤åˆ¥å™¨ | å®šç† 3.8a |

---

## å®Ÿè·µãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

:::details æ¨å®šé‡é¸æŠãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆï¼ˆå°åˆ·ç”¨ï¼‰

**å•é¡Œåˆ¥æ¨å®šé‡é¸æŠã‚¬ã‚¤ãƒ‰**

| æ¨å®šã®ç›®çš„ | ç¬¬ä¸€é¸æŠ | ç¬¬äºŒé¸æŠ | ç†ç”± |
|:-----|:---------|:---------|:-----|
| é«˜å“è³ªå¯†åº¦æ¨å®š | ã‚¹ã‚³ã‚¢æ¨å®šé‡ï¼ˆDiffusionï¼‰ | æš—é»™çš„æ¨å®šï¼ˆGANï¼‰ | æ¨å®šç²¾åº¦ + å®‰å®šæ€§ |
| é›¢æ•£ç³»åˆ—æ¨å®š | è‡ªå·±å›å¸°MLEï¼ˆGPTï¼‰ | - | é›¢æ•£ãƒ‡ãƒ¼ã‚¿ã«æœ€é© |
| æ½œåœ¨è¡¨ç¾å­¦ç¿’ | å¤‰åˆ†MLEï¼ˆVAEï¼‰ | å¤‰æ•°å¤‰æ›MLEï¼ˆFlowï¼‰ | æ»‘ã‚‰ã‹ãªæ½œåœ¨ç©ºé–“ |
| ç•°å¸¸æ¤œçŸ¥ | Flow / VAE | - | å°¤åº¦è¨ˆç®—ãŒå¿…è¦ |
| æ­£ç¢ºãªå¯†åº¦æ¨å®š | å¤‰æ•°å¤‰æ›MLEï¼ˆFlowï¼‰ | è‡ªå·±å›å¸°MLE | æ­£ç¢ºãªå°¤åº¦ |
| é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | æš—é»™çš„æ¨å®š / å¤‰åˆ†MLE | Consistency Model | 1-shotç”Ÿæˆ |
| æ¡ä»¶ä»˜ãæ¨å®š | ã‚¹ã‚³ã‚¢æ¨å®šé‡ | æš—é»™çš„æ¨å®š | Classifier-free guidance |
| æ™‚ç³»åˆ—æ¨å®š | ã‚¹ã‚³ã‚¢æ¨å®šé‡ | - | æ™‚é–“æ•´åˆæ€§ |

**MLE ã®å…¬å¼é›†**

$$\hat{\theta}_\text{MLE} = \arg\max_\theta \frac{1}{N}\sum_{i=1}^{N} \log q_\theta(x_i) = \arg\min_\theta H(\hat{p}, q_\theta) = \arg\min_\theta D_\text{KL}(\hat{p} \| q_\theta)$$

**ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã® MLEï¼ˆè¦šãˆã‚‹ã¹ãï¼‰:**

$$\hat{\mu} = \frac{1}{N}\sum_{i=1}^{N} x_i, \quad \hat{\sigma}^2 = \frac{1}{N}\sum_{i=1}^{N}(x_i - \hat{\mu})^2$$

**MLEã®4å¤‰å½¢ã®æå¤±é–¢æ•°**

```
VAE:       L = -E_q[log p(x|z)] + KL[q(z|x) || p(z)]
GAN:       L_D = -E[log D(x)] - E[log(1-D(G(z)))]
           L_G = -E[log D(G(z))]
Flow:      L = -E[log p(fâ»Â¹(x)) + log|det(âˆ‚fâ»Â¹/âˆ‚x)|]
Diffusion: L = E[||Îµ - Îµ_Î¸(âˆšá¾±â‚œxâ‚€ + âˆš(1-á¾±â‚œ)Îµ, t)||Â²]
```

**çµ±è¨ˆçš„è·é›¢ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼**

```python
# FID: Wâ‚‚ distance with Gaussian approximation
FID = np.dot(mu_r-mu_g, mu_r-mu_g) + np.trace(sigma_r + sigma_g - 2*sqrtm(sigma_r@sigma_g))

# CMMD: MMD in CLIP space
CMMD2 = mean_k(r,r) + mean_k(g,g) - 2*mean_k(r,g)  # k = RBF kernel

# Perplexity: exponentiated cross-entropy
PPL = np.exp(cross_entropy_loss)
```

**é‡è¦ãªç­‰ä¾¡é–¢ä¿‚**

```
MLE â‰¡ Cross-Entropyæœ€å°åŒ– â‰¡ KLæœ€å°åŒ– â‰¡ å‰å‘ãKLæœ€å°åŒ–
GAN â‰¡ JSDæœ€å°åŒ– â‰¡ å¯†åº¦æ¯”æ¨å®š
LLMè¨“ç·´ â‰¡ è‡ªå·±å›å¸°MLE â‰¡ æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³CEæœ€å°åŒ–
Score Matching â‰¡ Denoising â‰¡ Diffusion (ç°¡æ˜“ç‰ˆ)
MAP â‰¡ MLE + L2æ­£å‰‡åŒ– (ã‚¬ã‚¦ã‚¹äº‹å‰åˆ†å¸ƒã®å ´åˆ)
```

**Mode-Covering vs Mode-Seeking è¦šãˆæ–¹**

```
Forward KL: D(p_data || q_model)
  â†’ q must cover where p > 0
  â†’ "Cover all modes" â†’ blurry but complete
  â†’ Used by: MLE, VAE

Reverse KL: D(q_model || p_data)
  â†’ q must stay where p > 0
  â†’ "Seek one mode" â†’ sharp but incomplete
  â†’ Used by: GAN (approximately via JSD)
```
:::

:::details çµ±è¨ˆçš„æ¨å®šã®å¹´ä»£è¨˜ï¼ˆè¦šãˆã‚‹ã¹ãè«–æ–‡ Top 13ï¼‰

| å¹´ | è«–æ–‡ | è²¢çŒ® | arXiv |
|:---|:-----|:-----|:------|
| 1922 | Fisher | MLE ã®ä½“ç³»åŒ– | - |
| 2013 | Kingma & Welling | VAE | 1312.6114 |
| 2014 | Goodfellow+ | GAN | 1406.2661 |
| 2014 | Dinh+ | NICE (Flow ã®å§‹ç¥–) | 1410.8516 |
| 2015 | Sohl-Dickstein+ | Diffusion ã®ç€æƒ³ | 1503.03585 |
| 2015 | Rezende & Mohamed | Normalizing Flows | 1505.05770 |
| 2016 | Salimans+ | Inception Score | 1606.03498 |
| 2016 | Dinh+ | Real NVP | 1605.08803 |
| 2016 | Mohamed+ | Prescribed vs Implicit | 1610.03483 |
| 2017 | Heusel+ | FID | 1706.08500 |
| 2019 | Song & Ermon | Score Matching ç”Ÿæˆ | 1907.05600 |
| 2020 | Ho+ | DDPM | 2006.11239 |
| 2024 | Jayasumana+ | CMMD (FID æ”¹å–„) | 2401.09603 |
:::

:::details æ¨å®šé‡ã®æ•°å­¦çš„å‰ææ¡ä»¶ãƒãƒƒãƒ—

```
ç¬¬2å› ç·šå½¢ä»£æ•°
  â”œâ”€â”€ å›ºæœ‰å€¤åˆ†è§£ â†’ FID ã®è¡Œåˆ—å¹³æ–¹æ ¹
  â”œâ”€â”€ è¡Œåˆ—å¼ â†’ Flow ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³
  â””â”€â”€ å†…ç©ç©ºé–“ â†’ Fisher æƒ…å ±è¡Œåˆ—

ç¬¬3å› å¾®åˆ†ç©åˆ†
  â”œâ”€â”€ åå¾®åˆ† â†’ MLE ã®å‹¾é…
  â”œâ”€â”€ ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ â†’ å¤‰æ•°å¤‰æ›å…¬å¼
  â””â”€â”€ é€£é–å¾‹ â†’ Backpropagation

ç¬¬4å› ç¢ºç‡çµ±è¨ˆ
  â”œâ”€â”€ ç¢ºç‡åˆ†å¸ƒ â†’ å¯†åº¦æ¨å®šã®å®šç¾©
  â”œâ”€â”€ ãƒ™ã‚¤ã‚ºã®å®šç† â†’ äº‹å¾Œæ¨è«–
  â””â”€â”€ æ¡ä»¶ä»˜ãç¢ºç‡ â†’ è‡ªå·±å›å¸°åˆ†è§£

ç¬¬5å› æ¸¬åº¦è«–
  â”œâ”€â”€ Lebesgue ç©åˆ† â†’ æœŸå¾…å€¤ã®å³å¯†å®šç¾©
  â”œâ”€â”€ Radon-Nikodym â†’ å¯†åº¦æ¯”æ¨å®š
  â””â”€â”€ Pushforward æ¸¬åº¦ â†’ GAN ã®ç”Ÿæˆå™¨

ç¬¬6å› æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–
  â”œâ”€â”€ KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ â†’ MLE ç­‰ä¾¡æ€§
  â”œâ”€â”€ Cross-Entropy â†’ æå¤±é–¢æ•°
  â”œâ”€â”€ Adam â†’ è¨“ç·´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
  â””â”€â”€ Jensen ä¸ç­‰å¼ â†’ ELBO (ç¬¬8å›)

ç¬¬7å› æœ¬è¬›ç¾©
  â”œâ”€â”€ MLE â†’ å…¨æ¨å®šé‡ã®åŸºç›¤
  â”œâ”€â”€ åˆ†é¡ä½“ç³» â†’ ãƒ¢ãƒ‡ãƒ«é¸æŠã®æŒ‡é‡
  â””â”€â”€ è©•ä¾¡æŒ‡æ¨™ â†’ å“è³ªæ¸¬å®š
```
:::

:::details æ•°å€¤ã®ç›´æ„Ÿï¼ˆè¦šãˆã¦ãŠãã¨ä¾¿åˆ©ï¼‰

| é‡ | å…¸å‹å€¤ | æ„å‘³ |
|:---|:-------|:-----|
| CIFAR-10 FID (DDPM) | 3.17 | ç”»åƒç”Ÿæˆã® SOTA ãƒ¬ãƒ™ãƒ« |
| ImageNet FID (Diffusion) | ~2-5 | å¤§è¦æ¨¡ç”»åƒç”Ÿæˆ |
| GPT-4 Perplexity | ~10-20 (æ¨å®š) | éå¸¸ã«è‰¯ã„è¨€èªãƒ¢ãƒ‡ãƒ« |
| Random baseline PPL | vocab_size (~50K) | å­¦ç¿’å‰ã®çŠ¶æ…‹ |
| é¡”ç”»åƒã®å†…åœ¨æ¬¡å…ƒ | ~100 | 12,288æ¬¡å…ƒä¸­ |
| MNIST ã®å†…åœ¨æ¬¡å…ƒ | ~10-15 | 784æ¬¡å…ƒä¸­ |
| IS (CIFAR-10, æœ€è‰¯) | ~9.5 | æœ€å¤§å€¤ã¯10ï¼ˆã‚¯ãƒ©ã‚¹æ•°ï¼‰ |
| ã‚¬ã‚¦ã‚¹ MLE ã®åæŸ | $O(1/\sqrt{N})$ | Fisher æƒ…å ±ã‹ã‚‰ |
:::

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
