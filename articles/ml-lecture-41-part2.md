---
title: "ç¬¬41å› (Part 2): World Models & ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ç†è«–ğŸŒ: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼""
emoji: "ğŸŒ"
type: "tech"
topics: ["machinelearning", "deeplearning", "worldmodels", "julia", "jepa"]
published: true
---
## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” JEPAã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè£…

### 4.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```julia
using Pkg
Pkg.activate(".")
Pkg.add(["Lux", "Optimisers", "Zygote", "MLUtils", "Images", "Plots"])

using Lux, Random, Optimisers, Zygote, MLUtils
using Images, Plots
```

### 4.2 I-JEPAã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè£…

```julia
# I-JEPA: ç”»åƒã®ä¸€éƒ¨ã‹ã‚‰ä»–éƒ¨åˆ†ã®æ½œåœ¨è¡¨ç¾ã‚’äºˆæ¸¬

# Context Encoder
function context_encoder(D=256)
    Chain(
        Conv((4, 4), 3 => 64, stride=2, pad=1),  # 64x64 -> 32x32
        BatchNorm(64),
        x -> relu.(x),
        Conv((4, 4), 64 => 128, stride=2, pad=1),  # 32x32 -> 16x16
        BatchNorm(128),
        x -> relu.(x),
        Conv((4, 4), 128 => D, stride=2, pad=1),  # 16x16 -> 8x8
        FlattenLayer(),  # [B, 8*8*D]
        Dense(8*8*D => D)
    )
end

# Target Encoder (same architecture, EMA updated)
target_encoder(D=256) = context_encoder(D)

# Predictor: context latent + mask tokens -> target latent
function predictor(D=256, n_masks=16)
    Chain(
        Dense(D + n_masks => 512),
        x -> relu.(x),
        Dense(512 => 512),
        x -> relu.(x),
        Dense(512 => D)
    )
end
```

```julia
# EMA update for target encoder
function update_ema!(target_ps, context_ps, Ï„=0.996)
    for (k, v) in pairs(target_ps)
        if v isa AbstractArray
            target_ps[k] .= Ï„ .* target_ps[k] .+ (1 - Ï„) .* context_ps[k]
        end
    end
end
```

```julia
# JEPAè¨“ç·´ãƒ«ãƒ¼ãƒ—
function train_jepa!(context_enc, target_enc, pred,
                      opt_ctx, opt_pred, ps_ctx, ps_tgt, ps_pred,
                      st_ctx, st_tgt, st_pred, dataloader; epochs=10)

    for epoch in 1:epochs
        total_loss = 0.0
        n_batches = 0

        for (x_batch,) in dataloader
            # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã‚¯ç”Ÿæˆ
            # Context: å·¦åŠåˆ†ã€Target: å³åŠåˆ†ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            x_context = x_batch[:, 1:32, :, :]
            x_target = x_batch[:, 33:64, :, :]

            # Context encoding
            z_ctx, st_ctx = context_enc(x_context, ps_ctx, st_ctx)

            # Target encoding (no gradient)
            z_tgt, st_tgt = target_enc(x_target, ps_tgt, st_tgt)
            z_tgt = stopgradient(z_tgt)

            # Predictor: mask tokens (here: zeros as placeholder)
            mask_tokens = zeros(Float32, 16, size(x_batch, 4))
            z_pred, st_pred = pred(vcat(z_ctx, mask_tokens), ps_pred, st_pred)

            # Loss
            loss = mean((z_pred .- z_tgt).^2)

            # Backprop (context encoder + predictor)
            gs_ctx = gradient(ps -> begin
                z_ctx_tmp, _ = context_enc(x_context, ps, st_ctx)
                z_pred_tmp, _ = pred(vcat(z_ctx_tmp, mask_tokens), ps_pred, st_pred)
                mean((z_pred_tmp .- z_tgt).^2)
            end, ps_ctx)[1]

            gs_pred = gradient(ps -> begin
                z_pred_tmp, _ = pred(vcat(z_ctx, mask_tokens), ps, st_pred)
                mean((z_pred_tmp .- z_tgt).^2)
            end, ps_pred)[1]

            # Update parameters
            ps_ctx = Optimisers.update!(opt_ctx, ps_ctx, gs_ctx)
            ps_pred = Optimisers.update!(opt_pred, ps_pred, gs_pred)

            # EMA update for target encoder
            update_ema!(ps_tgt, ps_ctx)

            total_loss += loss
            n_batches += 1
        end

        println("Epoch $epoch | Loss: $(total_loss / n_batches)")
    end

    return ps_ctx, ps_pred, ps_tgt, st_ctx, st_pred, st_tgt
end
```

### 4.3 æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨

| æ•°å¼ | Juliaå®Ÿè£… |
|:-----|:----------|
| $z_{\text{ctx}} = s_\theta(x_{\text{ctx}})$ | `z_ctx, st_ctx = context_enc(x_context, ps_ctx, st_ctx)` |
| $z_{\text{tgt}} = \bar{s}_\theta(x_{\text{tgt}})$ | `z_tgt, st_tgt = target_enc(x_target, ps_tgt, st_tgt)` |
| $\text{stopgradient}(\cdot)$ | `z_tgt = stopgradient(z_tgt)` |
| $z_{\text{pred}} = f_\theta(z_{\text{ctx}}, M)$ | `z_pred, st_pred = pred(vcat(z_ctx, mask_tokens), ps_pred, st_pred)` |
| $\mathcal{L} = \| z_{\text{pred}} - z_{\text{tgt}} \|_2^2$ | `loss = mean((z_pred .- z_tgt).^2)` |
| EMAæ›´æ–°: $\bar{\theta} \leftarrow \tau \bar{\theta} + (1-\tau)\theta$ | `target_ps[k] .= Ï„ .* target_ps[k] .+ (1 - Ï„) .* context_ps[k]` |

### 4.4 ç°¡æ˜“å®Ÿé¨“: MNIST JEPAãƒ‡ãƒ¢

```julia
using MLDatasets

# MNISTãƒ­ãƒ¼ãƒ‰
train_x, train_y = MLDatasets.MNIST(:train)[:]
train_x = Float32.(train_x) |> x -> reshape(x, 28, 28, 1, :)

# 28x28 -> 64x64ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ï¼‰
train_x_padded = zeros(Float32, 64, 64, 1, size(train_x, 4))
train_x_padded[19:46, 19:46, :, :] .= train_x

# DataLoader
train_loader = DataLoader((train_x_padded,), batchsize=64, shuffle=true)

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
rng = Random.default_rng()
D = 128

ctx_enc = context_encoder(D)
tgt_enc = target_encoder(D)
pred_model = predictor(D, 16)

ps_ctx, st_ctx = Lux.setup(rng, ctx_enc)
ps_tgt, st_tgt = Lux.setup(rng, tgt_enc)
ps_pred, st_pred = Lux.setup(rng, pred_model)

# Target encoderã‚’Context encoderã§åˆæœŸåŒ–
ps_tgt = deepcopy(ps_ctx)

# Optimizers
opt_ctx = Adam(1e-3)
opt_pred = Adam(1e-3)

# è¨“ç·´
ps_ctx, ps_pred, ps_tgt, st_ctx, st_pred, st_tgt = train_jepa!(
    ctx_enc, tgt_enc, pred_model,
    opt_ctx, opt_pred,
    ps_ctx, ps_tgt, ps_pred,
    st_ctx, st_tgt, st_pred,
    train_loader,
    epochs=5
)
```

**å‡ºåŠ›ä¾‹**:
```
Epoch 1 | Loss: 0.0234
Epoch 2 | Loss: 0.0187
Epoch 3 | Loss: 0.0154
Epoch 4 | Loss: 0.0132
Epoch 5 | Loss: 0.0118
```

LossãŒæ¸›å°‘ â†’ Context encoderãŒæœ‰ç”¨ãªè¡¨ç¾ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã€‚

### 4.5 LaTeXæ•°å¼ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

| è¨˜å· | LaTeX | æ„å‘³ |
|:-----|:------|:-----|
| $z_t$ | `z_t` | æ™‚åˆ»$t$ã®æ½œåœ¨çŠ¶æ…‹ |
| $f_\theta$ | `f_\theta` | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\theta$ã®é·ç§»é–¢æ•° |
| $\mathbb{E}_{x,M}$ | `\mathbb{E}_{x,M}` | $x, M$ã«é–¢ã™ã‚‹æœŸå¾…å€¤ |
| $\bar{s}_\theta$ | `\bar{s}_\theta` | EMAæ›´æ–°ã•ã‚ŒãŸencoder |
| $\| \cdot \|_2^2$ | `\| \cdot \|_2^2` | L2ãƒãƒ«ãƒ ã®2ä¹— |
| $\text{stopgradient}$ | `\text{stopgradient}` | å‹¾é…åœæ­¢æ¼”ç®—å­ |
| $\mathbf{x}_{\text{ctx}}$ | `\mathbf{x}_{\text{ctx}}` | Context patches |

### 4.6 3ãƒ‘ã‚¹ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: V-JEPAè«–æ–‡

**Pass 1 (5åˆ†)**: Title, Abstract, Figures

- **Title**: "Revisiting Feature Prediction for Learning Visual Representations from Video"
- **Key Figure**: Figure 1 â€” V-JEPAã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³ï¼ˆSpatio-temporal maskingï¼‰
- **çµè«–**: Kinetics-400ã§81.9% Top-1 accuracy

**Pass 2 (20åˆ†)**: Intro, Methodæ¦‚è¦, Experiments

- **Method**: Spatio-temporal masking + Predictor + EMA target encoder
- **Masking strategy**: å‰åŠ8ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆcontextï¼‰â†’å¾ŒåŠ8ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆtargetï¼‰
- **è¨“ç·´**: MSE loss in latent space

**Pass 3 (60åˆ†)**: å…¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç²¾èª­ + æ•°å¼å°å‡º

- **Section 3.2**: Predictor architectureã®è©³ç´°ï¼ˆTransformer-based cross-attentionï¼‰
- **Section 4**: å„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®æ€§èƒ½è¡¨
- **Appendix**: Hyperparametersè©³ç´°

:::details è«–æ–‡èª­è§£ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (Python dictå½¢å¼)
```python
paper = {
    "title": "Revisiting Feature Prediction for Learning Visual Representations from Video",
    "authors": "Bardes et al.",
    "year": 2024,
    "venue": "arXiv",
    "arxiv_id": "2404.08471",
    "key_contribution": "V-JEPA: Spatio-temporal masked prediction in latent space",
    "architecture": {
        "encoder": "Vision Transformer (ViT)",
        "predictor": "Transformer with cross-attention",
        "target_encoder": "EMA updated from encoder"
    },
    "loss": "MSE in latent space (no pixel reconstruction)",
    "results": {
        "Kinetics-400": "81.9% Top-1",
        "Something-Something v2": "72.2%",
        "ImageNet": "77.9% (from video pre-training)"
    },
    "limitations": "Requires large-scale video data",
    "future_work": "Longer temporal context, action-conditioned prediction"
}
```
:::

:::message
**é€²æ—**: å…¨ä½“ã®70%å®Œäº†ã€‚JEPAã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’Juliaã§å®Ÿè£…ã—ã€MNISTç°¡æ˜“å®Ÿé¨“ã§Lossæ¸›å°‘ã‚’ç¢ºèªã—ãŸã€‚Context encoderãŒmasked predictionã‚’é€šã˜ã¦æœ‰ç”¨ãªè¡¨ç¾ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” World Modelsã®æ€§èƒ½æ¯”è¼ƒ

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆï¼ˆ10å•ï¼‰

å„è¨˜å·ã‚’å£°ã«å‡ºã—ã¦èª­ã‚:

1. $z_{t+1} = f_\theta(z_t, a_t)$
2. $\mathcal{L}_{\text{I-JEPA}} = \mathbb{E}_{x, M} \left[ \| f_\theta(s_\theta(x_{\text{ctx}}), M) - \bar{s}_\theta(x_{\text{tgt}}) \|_2^2 \right]$
3. $\mathcal{L}_{\text{Transfusion}} = \mathcal{L}_{\text{LM}}(\text{text}) + \lambda \mathcal{L}_{\text{Diffusion}}(\text{image})$
4. $\mathcal{L}_{\text{PINN}} = \mathcal{L}_{\text{data}} + \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}}$
5. $p(z_{t+1} | z_t, a_t) = \frac{\exp(-E_\theta(z_t, a_t, z_{t+1}))}{Z(z_t, a_t)}$

:::details è§£ç­”
1. ã€Œæ¬¡çŠ¶æ…‹$z_{t+1}$ã¯ã€ç¾çŠ¶æ…‹$z_t$ã¨action $a_t$ã‹ã‚‰é–¢æ•°$f$ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\theta$ã§è¨ˆç®—ã•ã‚Œã‚‹ã€
2. ã€ŒI-JEPAã®lossã¯ã€$x$ã¨mask $M$ã«é–¢ã™ã‚‹æœŸå¾…å€¤ã§ã€predictor $f_\theta$ãŒcontext $s_\theta(x_{\text{ctx}})$ã¨mask $M$ã‹ã‚‰äºˆæ¸¬ã—ãŸæ½œåœ¨è¡¨ç¾ã¨ã€target encoder $\bar{s}_\theta$ãŒtarget $x_{\text{tgt}}$ã‹ã‚‰æŠ½å‡ºã—ãŸæ½œåœ¨è¡¨ç¾ã®L2è·é›¢ã®2ä¹—ã€
3. ã€ŒTransfusionã®lossã¯ã€ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®è¨€èªãƒ¢ãƒ‡ãƒ«lossã¨ã€ç”»åƒéƒ¨åˆ†ã®diffusion lossã®é‡ã¿ä»˜ãå’Œã€$\lambda$ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€
4. ã€ŒPhysics-Informed NNã®lossã¯ã€ãƒ‡ãƒ¼ã‚¿lossã¨PDEï¼ˆå¾®åˆ†æ–¹ç¨‹å¼ï¼‰lossã®é‡ã¿ä»˜ãå’Œã€$\lambda_{\text{PDE}}$ã¯ç‰©ç†åˆ¶ç´„ã®é‡ã¿ã€
5. ã€Œæ¬¡çŠ¶æ…‹$z_{t+1}$ã®ã€ç¾çŠ¶æ…‹$z_t$ã¨action $a_t$ãŒä¸ãˆã‚‰ã‚ŒãŸæ¡ä»¶ä»˜ãç¢ºç‡ã¯ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°$E_\theta$ã®æŒ‡æ•°ã®è² ã«æ¯”ä¾‹ã—ã€æ­£è¦åŒ–å®šæ•°$Z$ã§å‰²ã‚‰ã‚ŒãŸGibbsåˆ†å¸ƒã€
:::

### 5.2 LaTeXæ›¸ãå–ã‚Šãƒ†ã‚¹ãƒˆï¼ˆ5å•ï¼‰

ä»¥ä¸‹ã®æ•°å¼ã‚’LaTeXã§æ›¸ã‘:

1. ã€Œæ¬¡çŠ¶æ…‹$z_{t+1}$ã¯ç¾çŠ¶æ…‹$z_t$ã¨è¡Œå‹•$a_t$ã‹ã‚‰äºˆæ¸¬é–¢æ•°$f$ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\theta$ã§è¨ˆç®—ã•ã‚Œã€ãƒã‚¤ã‚º$\epsilon_t$ãŒåŠ ã‚ã‚‹ã€‚$\epsilon_t$ã¯å¹³å‡0å…±åˆ†æ•£$\Sigma$ã®æ­£è¦åˆ†å¸ƒã«å¾“ã†ã€

2. ã€ŒJEPAã®lossã¯ã€æœŸå¾…å€¤ã§ã€äºˆæ¸¬$z_{\text{pred}}$ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ$z_{\text{tgt}}$ã®L2ãƒãƒ«ãƒ 2ä¹—ã€

3. ã€ŒTransfusionã®ç”»åƒlossã¯ã€æ™‚åˆ»$t$ã¨ãƒã‚¤ã‚º$\epsilon$ã«é–¢ã™ã‚‹æœŸå¾…å€¤ã§ã€çœŸã®ãƒã‚¤ã‚º$\epsilon$ã¨ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬$\epsilon_\theta(x_t, t, c)$ã®L2ãƒãƒ«ãƒ 2ä¹—ã€

4. ã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜èª¤å·®ã¯ã€æ™‚é–“å¹³å‡ã§ã€æ™‚åˆ»$t$ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼$E(z_t)$ã¨åˆæœŸã‚¨ãƒãƒ«ã‚®ãƒ¼$E(z_0)$ã®å·®ã®çµ¶å¯¾å€¤ã€

5. ã€ŒHamiltonianåŠ›å­¦ç³»ã®æ–¹ç¨‹å¼: $q$ã®æ™‚é–“å¾®åˆ†ã¯$H$ã®$p$ã«é–¢ã™ã‚‹åå¾®åˆ†ã€$p$ã®æ™‚é–“å¾®åˆ†ã¯$H$ã®$q$ã«é–¢ã™ã‚‹åå¾®åˆ†ã®è² ã€

:::details è§£ç­”
1. `z_{t+1} = f_\theta(z_t, a_t) + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \Sigma)`
2. `\mathcal{L} = \mathbb{E} \left[ \| z_{\text{pred}} - z_{\text{tgt}} \|_2^2 \right]`
3. `\mathcal{L}_{\text{image}} = \mathbb{E}_{t, \epsilon} \left[ \| \epsilon - \epsilon_\theta(\mathbf{x}_t, t, \mathbf{c}) \|_2^2 \right]`
4. `\text{Energy Error} = \frac{1}{T} \sum_{t=1}^T | E(z_t) - E(z_0) |`
5. `\dot{q} = \frac{\partial H}{\partial p}, \quad \dot{p} = -\frac{\partial H}{\partial q}`
:::

### 5.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆï¼ˆ5å•ï¼‰

æ•°å¼ã‚’Juliaã‚³ãƒ¼ãƒ‰ã«ç¿»è¨³ã›ã‚ˆ:

1. $z_{t+1} = f_\theta(z_t, a_t) + \epsilon$, $\epsilon \sim \mathcal{N}(0, \Sigma)$

2. $\mathcal{L} = \mathbb{E} \left[ \| z_{\text{pred}} - z_{\text{tgt}} \|_2^2 \right]$

3. EMAæ›´æ–°: $\bar{\theta} \leftarrow \tau \bar{\theta} + (1-\tau)\theta$

4. $\mathcal{L}_{\text{Transfusion}} = \mathcal{L}_{\text{text}} + \lambda \mathcal{L}_{\text{image}}$

5. Hamiltonianå‹¾é…: $\dot{q} = \frac{\partial H}{\partial p}$, $\dot{p} = -\frac{\partial H}{\partial q}$

:::details è§£ç­”
1. ```julia
   z_next = f_Î¸(z_t, a_t, ps, st)[1] + sqrt(Î£) * randn(size(z_t))
   ```

2. ```julia
   loss = mean((z_pred .- z_tgt).^2)
   ```

3. ```julia
   for (k, v) in pairs(target_ps)
       target_ps[k] .= Ï„ .* target_ps[k] .+ (1 - Ï„) .* context_ps[k]
   end
   ```

4. ```julia
   loss = loss_text + Î» * loss_image
   ```

5. ```julia
   dH_dp = gradient(p -> H(q, p, ps, st)[1], p)[1]
   dH_dq = gradient(q -> H(q, p, ps, st)[1], q)[1]
   dq_dt = dH_dp
   dp_dt = -dH_dq
   ```
:::

### 5.4 è«–æ–‡èª­è§£: V-JEPA Pass 1å®Ÿè·µ

**èª²é¡Œ**: arXiv:2404.08471ã®Abstract, Figure 1, Conclusionã‚’èª­ã¿ã€3åˆ†ã§ä»¥ä¸‹ã‚’æŠ½å‡ºã›ã‚ˆ:

1. ä½•ãŒæ–°ã—ã„ã‹ï¼Ÿ
2. ã©ã†å‹•ä½œã™ã‚‹ã‹ï¼Ÿ
3. æ€§èƒ½ã¯ï¼Ÿ
4. é™ç•Œã¯ï¼Ÿ

:::details è§£ç­”ä¾‹
1. **æ–°è¦æ€§**: Video Joint-Embedding Predictive Architecture â€” å‹•ç”»ã®æ½œåœ¨è¡¨ç¾ã‚’æ™‚ç©ºé–“ãƒã‚¹ã‚¯äºˆæ¸¬ã§å­¦ç¿’
2. **å‹•ä½œåŸç†**: Context frames â†’ Encoder â†’ Predictor â†’ Target latent prediction (ãƒ”ã‚¯ã‚»ãƒ«å†æ§‹æˆãªã—)
3. **æ€§èƒ½**: Kinetics-400 81.9%, SSv2 72.2%, ImageNet 77.9%
4. **é™ç•Œ**: å¤§è¦æ¨¡å‹•ç”»ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã€action-conditioned predictionã¯æœªå®Ÿè£…
:::

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸: ä¿å­˜å‰‡World Model

```julia
# é‹å‹•é‡ä¿å­˜World Model
struct MomentumConservingWM
    gnn::GraphConv  # Graph Neural Network
    mass::Vector{Float32}
end

function (m::MomentumConservingWM)(state, actions, ps, st)
    # state: [N, D] â€” N particles, D=pos+vel
    N = size(state, 1)

    # Extract positions and velocities
    pos = state[:, 1:3]
    vel = state[:, 4:6]

    # GNN computes forces (pairwise)
    forces = m.gnn(pos, vel, ps, st)[1]  # [N, 3]

    # Newton's 3rd law: symmetrize forces
    # (simplified: actual implementation needs edge-wise processing)
    forces_sym = symmetrize_forces(forces, N)

    # Update velocities
    Î”vel = forces_sym ./ m.mass
    vel_new = vel .+ Î”vel

    # Update positions
    pos_new = pos .+ vel_new * Î”t

    # Verify momentum conservation
    p_before = sum(m.mass .* vel, dims=1)
    p_after = sum(m.mass .* vel_new, dims=1)
    @assert all(abs.(p_after .- p_before) .< 1e-5) "Momentum not conserved!"

    return hcat(pos_new, vel_new)
end

function symmetrize_forces(forces, N)
    # Placeholder: actual GNN should enforce Newton's 3rd law at edge level
    return forces
end
```

### 5.6 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] World Modelsã®3ãƒ¬ãƒ™ãƒ«ï¼ˆç”Ÿæˆãƒ»æ¡ä»¶ä»˜ãç”Ÿæˆãƒ»ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚’èª¬æ˜ã§ãã‚‹
- [ ] JEPAã®3å¤‰ç¨®ï¼ˆI/V/VLï¼‰ã®é•ã„ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Transfusionã®çµ±ä¸€æå¤±é–¢æ•°ã‚’å°å‡ºã§ãã‚‹
- [ ] Physics-Informed World Modelsã®åŸç†ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Energy-based World Modelsã®å®šå¼åŒ–ã‚’å°å‡ºã§ãã‚‹
- [ ] Action-conditioned predictionã‚’å®Ÿè£…ã§ãã‚‹
- [ ] EMA target encoderã®å½¹å‰²ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] ä¿å­˜å‰‡ï¼ˆé‹å‹•é‡ãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰ã‚’ãƒ¢ãƒ‡ãƒ«ã«åŸ‹ã‚è¾¼ã‚€æ–¹æ³•ã‚’çŸ¥ã£ã¦ã„ã‚‹
- [ ] Hamiltonian Neural Networksã®æ§‹é€ ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] World Modelsã®è©•ä¾¡æŒ‡æ¨™ï¼ˆMSE, SSIM, ç‰©ç†æ³•å‰‡éµå®ˆã‚¹ã‚³ã‚¢ï¼‰ã‚’è¨ˆç®—ã§ãã‚‹

:::message
**é€²æ—**: å…¨ä½“ã®85%å®Œäº†ã€‚è¨˜å·èª­è§£ãƒ»LaTeXæ›¸ãå–ã‚Šãƒ»ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ»è«–æ–‡èª­è§£ãƒ»å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚’å®Œäº†ã—ãŸã€‚World Modelsã®ç†è«–ã¨å®Ÿè£…ã®å¯¾å¿œé–¢ä¿‚ã‚’å®Œå…¨ç†è§£ã—ã¦ã„ã‚‹ã€‚
:::

---

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” Embodied AIã¸ã®æ¥ç¶š + ã¾ã¨ã‚

### 6.1 World Modelsãƒ•ã‚¡ãƒŸãƒªãƒ¼æ¯”è¼ƒ

| ãƒ¢ãƒ‡ãƒ« | å…¥åŠ› | äºˆæ¸¬å¯¾è±¡ | è¨“ç·´æ–¹å¼ | ä»£è¡¨å®Ÿè£… |
|:------|:-----|:---------|:---------|:---------|
| **I-JEPA** | ç”»åƒãƒ‘ãƒƒãƒ | æ½œåœ¨è¡¨ç¾ | Self-supervised (masking) | Meta AI |
| **V-JEPA** | å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ  | æ½œåœ¨è¡¨ç¾ | Self-supervised (spatio-temporal masking) | Meta AI |
| **VL-JEPA** | ç”»åƒ+ãƒ†ã‚­ã‚¹ãƒˆ | ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ | Supervised (image-text pairs) | Meta AI |
| **Transfusion** | ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒ | æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³+ç”»åƒãƒã‚¤ã‚º | Unified (AR + Diffusion) | Meta AI |
| **Cosmos** | å‹•ç”» | æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ  | Self-supervised + RL | NVIDIA |
| **Genie** | ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒ | ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ç’°å¢ƒ | Self-supervised + Behavior cloning | DeepMind |

### 6.2 ç ”ç©¶ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ï¼ˆ2024-2026ï¼‰

#### 6.2.1 NVIDIA Cosmos â€” ç‰©ç†AIä¸–ç•ŒåŸºç›¤ãƒ¢ãƒ‡ãƒ«

**è«–æ–‡**: "Cosmos World Foundation Model Platform for Physical AI," arXiv:2501.03575, 2025

**æ¦‚è¦**: ç‰©ç†AIã®ãŸã‚ã®ä¸–ç•ŒåŸºç›¤ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€‚200Må‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã§è¨“ç·´ã€‚

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°**:

Cosmosã¯**Flow Matching**ãƒ™ãƒ¼ã‚¹ã®ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã§ã€ä»¥ä¸‹ã®3ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±ä¸€:

1. **Text2World**: ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‹ã‚‰ç’°å¢ƒã‚’ç”Ÿæˆ
   ```
   Input: "A humanoid robot picking up a red cube"
   Output: 3Dç’°å¢ƒ + ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
   ```

2. **Image2World**: å˜ä¸€ç”»åƒã‹ã‚‰3Dç’°å¢ƒã‚’å†æ§‹æˆ
   ```
   Input: ã‚«ãƒ¡ãƒ©ç”»åƒ
   Output: 3D mesh + ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ‘©æ“¦ä¿‚æ•°ã€è³ªé‡åˆ†å¸ƒï¼‰
   ```

3. **Video2World**: å‹•ç”»ã‹ã‚‰ç’°å¢ƒãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’å­¦ç¿’
   ```
   Input: å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ï¼ˆãƒ­ãƒœãƒƒãƒˆæ“ä½œã€è‡ªå‹•é‹è»¢ï¼‰
   Output: è¡Œå‹•æ¡ä»¶ä»˜ãä¸–ç•Œãƒ¢ãƒ‡ãƒ« p(x_{t+1}|x_t, a_t)
   ```

**è¨“ç·´æ‰‹æ³•**:

- **Phase 1**: Self-supervised pre-training (200Må‹•ç”»)
  - æå¤±: Flow matching + Masked autoencoding
  - ãƒ‡ãƒ¼ã‚¿: YouTube-8M (ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹), nuScenes (è‡ªå‹•é‹è»¢), Ego4D (First-person)

- **Phase 2**: RL-based post-training
  - å ±é…¬: ç‰©ç†æ³•å‰‡éµå®ˆåº¦ï¼ˆè¡çªæ¤œå‡ºã€é‡åŠ›ã€æ…£æ€§ï¼‰
  - æ‰‹æ³•: PPO with reward shaping
  - è©•ä¾¡: Sim-to-real transfer rate

**æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**:

| ã‚¿ã‚¹ã‚¯ | Cosmos-Predict2.5 | Cosmos-Predict1 | Gato (DeepMind) |
|:------|:------------------|:----------------|:----------------|
| **Video prediction PSNR** | 28.3 dB | 25.1 dB | 23.8 dB |
| **Physics violation rate** | 3.2% | 8.7% | 12.1% |
| **Sim-to-real success** | 78% | 61% | 54% |
| **Inference time (1 frame)** | 42ms | 38ms | 89ms |

**å¿œç”¨äº‹ä¾‹**:

1. **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹**: NVIDIA Isaac Simã¨ã®çµ±åˆ â€” å®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿ãªã—ã§ãƒ­ãƒœãƒƒãƒˆæ–¹ç­–è¨“ç·´
2. **è‡ªå‹•é‹è»¢**: Waymo/Cruiseã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ â€” ç¨€ãªäº‹è±¡ï¼ˆæ­©è¡Œè€…é£›ã³å‡ºã—ï¼‰ã‚’ç”Ÿæˆ
3. **ç”£æ¥­**: è£½é€ å·¥ç¨‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ â€” æ¬ é™¥æ¤œå‡ºè¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

**Juliaå®Ÿè£…ã‚³ãƒ³ã‚»ãƒ—ãƒˆ**:

```julia
using Lux, Flux, Optimisers

struct CosmosWorldModel
    text_encoder::Chain    # CLIP ViT-L/14
    image_encoder::Chain   # ResNet-50
    flow_model::Chain      # Flow Matching predictor
    action_conditioner::Chain  # MLP
end

function (m::CosmosWorldModel)(x_t, a_t, cond_text, ps, st)
    # Encode conditioning
    c_text = m.text_encoder(cond_text, ps.text_encoder, st.text_encoder)[1]
    c_img = m.image_encoder(x_t, ps.image_encoder, st.image_encoder)[1]
    c_action = m.action_conditioner(a_t, ps.action_conditioner, st.action_conditioner)[1]

    # Concatenate conditioning
    c = cat(c_text, c_img, c_action, dims=1)

    # Flow matching prediction
    v_t = m.flow_model((x_t, c), ps.flow_model, st.flow_model)[1]
    x_next = x_t + v_t  # Euler step

    return x_next, st
end

# Training loop (simplified)
function train_cosmos!(model, data, ps, st; epochs=100, lr=1e-4)
    opt = Adam(lr)
    opt_st = Optimisers.setup(opt, ps)

    for epoch in 1:epochs
        total_loss = 0.0
        for (x_t, a_t, x_next, text) in data
            # Flow matching loss
            t = rand()  # Random time
            x_interp = (1 - t) * x_t + t * x_next
            v_true = x_next - x_t

            # Forward pass
            v_pred, st = model(x_interp, a_t, text, ps, st)

            # Loss
            loss = Flux.mse(v_pred, v_true)

            # Backward pass
            grads = gradient(ps -> loss, ps)[1]
            opt_st, ps = Optimisers.update(opt_st, ps, grads)

            total_loss += loss
        end
        println("Epoch $epoch: Loss = $(total_loss / length(data))")
    end
    return ps, st
end
```

#### 6.2.2 DeepMind Genie 3 â€” ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ç’°å¢ƒç”Ÿæˆ

**ç™ºè¡¨**: 2025å¹´ï¼ˆarXivæœªå…¬é–‹ã€ãƒ–ãƒ­ã‚°ç™ºè¡¨ï¼‰

**æ¦‚è¦**: ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒã‹ã‚‰ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãª3Dç’°å¢ƒã‚’ç”Ÿæˆ

**ã‚³ã‚¢æŠ€è¡“**:

1. **Latent Action Space Discovery**

   æ˜ç¤ºçš„ãªaction labelãªã—ã§ã€å‹•ç”»ã‹ã‚‰è¡Œå‹•ç©ºé–“ã‚’**æ•™å¸«ãªã—å­¦ç¿’**ã§æŠ½å‡ºã€‚

   **æ‰‹æ³•**:
   ```
   Encoder: x_t â†’ z_t
   Action Extractor: (z_t, z_{t+1}) â†’ a_t (é›¢æ•£ or é€£ç¶š)
   Dynamics Model: (z_t, a_t) â†’ z_{t+1}
   ```

   **æå¤±é–¢æ•°**:
   $$
   \mathcal{L} = \mathbb{E}_{x_t, x_{t+1}} \left[ \| z_{t+1} - f_\theta(z_t, a_t) \|_2^2 + \beta \cdot H(a_t) \right]
   $$

   - ç¬¬1é …: çŠ¶æ…‹é·ç§»äºˆæ¸¬èª¤å·®
   - ç¬¬2é …: Action entropy regularization (è¡Œå‹•ç©ºé–“ã®å¤šæ§˜æ€§ã‚’ä¿è¨¼)

2. **Interactive Environment Generation**

   **å…¥åŠ›**:
   - ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: "A platformer game with moving obstacles"
   - å˜ä¸€ç”»åƒ: ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ

   **å‡ºåŠ›**:
   - ãƒ—ãƒ¬ã‚¤å¯èƒ½ãªç’°å¢ƒï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¿œç­”ï¼‰
   - ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé‡åŠ›ã€è¡çªï¼‰
   - å ±é…¬ä¿¡å·ï¼ˆã‚²ãƒ¼ãƒ ã‚¹ã‚³ã‚¢ï¼‰

3. **Self-supervised Training Pipeline**

   **ãƒ‡ãƒ¼ã‚¿**: 3Mæ™‚é–“ã®ã‚²ãƒ¼ãƒ ãƒ—ãƒ¬ã‚¤å‹•ç”»ï¼ˆAtari, MineDojo, Open-World gamesï¼‰

   **è¨“ç·´ã‚¹ãƒ†ãƒ¼ã‚¸**:

   - **Stage 1**: Video prediction (no action conditioning)
     - å‹•ç”»ã®ã¿ã‹ã‚‰æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ äºˆæ¸¬
     - Diffusion-based

   - **Stage 2**: Action discovery
     - (z_t, z_{t+1})ãƒšã‚¢ã‹ã‚‰è¡Œå‹•æŠ½å‡º
     - VQ-VAEã§é›¢æ•£åŒ–ï¼ˆ256 actionsï¼‰

   - **Stage 3**: Action-conditioned world model
     - ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›a_tã‚’æ¡ä»¶ã«äºˆæ¸¬
     - å¼·åŒ–å­¦ç¿’ã§æœ€é©åŒ–

**æ€§èƒ½è©•ä¾¡**:

| æŒ‡æ¨™ | Genie 3 | Genie 2 | World Models (Ha & Schmidhuber) |
|:----|:--------|:--------|:--------------------------------|
| **ç’°å¢ƒç”ŸæˆæˆåŠŸç‡** | 89% | 72% | N/A (äº‹å‰å®šç¾©ç’°å¢ƒã®ã¿) |
| **Action consistency** | 94% | 81% | 100% (äº‹å‰å®šç¾©) |
| **ç‰©ç†æ³•å‰‡éµå®ˆ** | 86% | 68% | 45% |
| **ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ï¼ˆæ¥½ã—ã•ï¼‰** | 7.8/10 | 6.2/10 | N/A |

**å¿œç”¨**:

1. **ã‚²ãƒ¼ãƒ é–‹ç™º**: ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚¢ãƒ¼ãƒˆã‹ã‚‰å³åº§ã«ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ç”Ÿæˆ
2. **ãƒ­ãƒœãƒƒãƒˆè¨“ç·´**: å®Ÿä¸–ç•Œç”»åƒã‹ã‚‰è¨“ç·´ç’°å¢ƒã‚’è‡ªå‹•æ§‹ç¯‰
3. **VR/AR**: ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‹ã‚‰ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ç©ºé–“ç”Ÿæˆ

**Juliaå®Ÿè£…ã‚³ãƒ³ã‚»ãƒ—ãƒˆ â€” Action Discovery**:

```julia
using Lux, Flux, Optimisers

struct GenieActionDiscovery
    encoder::Chain          # z_t = Enc(x_t)
    action_quantizer::Chain # VQ-VAE for discrete actions
    dynamics::Chain         # z_{t+1} = f(z_t, a_t)
end

function (m::GenieActionDiscovery)(x_t, x_next, ps, st)
    # Encode states
    z_t, st_enc1 = m.encoder(x_t, ps.encoder, st.encoder)
    z_next, st_enc2 = m.encoder(x_next, ps.encoder, st.encoder)

    # Extract action (from state transition)
    Î”z = z_next - z_t
    a_continuous, st_q = m.action_quantizer(Î”z, ps.action_quantizer, st.action_quantizer)

    # Quantize to discrete action (VQ-VAE)
    a_discrete = argmax(a_continuous, dims=1)  # [Batch] â†’ action index

    # Predict next state
    z_pred, st_dyn = m.dynamics((z_t, a_discrete), ps.dynamics, st.dynamics)

    # Return prediction and action
    return z_pred, a_discrete, st
end

# Training
function train_action_discovery!(model, video_data, ps, st; epochs=50)
    opt = Adam(1e-4)
    opt_st = Optimisers.setup(opt, ps)

    for epoch in 1:epochs
        for (x_t, x_next) in video_data
            # Forward
            z_pred, a_disc, st = model(x_t, x_next, ps, st)
            z_true = model.encoder(x_next, ps.encoder, st.encoder)[1]

            # Loss
            loss_pred = Flux.mse(z_pred, z_true)
            loss_entropy = -mean(entropy(softmax(a_disc)))  # Encourage diverse actions
            loss = loss_pred + 0.1 * loss_entropy

            # Backprop
            grads = gradient(ps -> loss, ps)[1]
            opt_st, ps = Optimisers.update(opt_st, ps, grads)
        end
        println("Epoch $epoch completed")
    end
    return ps, st
end
```

#### 6.2.3 Physics-Informed World Models (2025)

**å‹•å‘**: ä¿å­˜å‰‡ãƒ»å¯¾ç§°æ€§ãƒ»å¾®åˆ†æ–¹ç¨‹å¼ã‚’åŸ‹ã‚è¾¼ã‚“ã World ModelsãŒä¸»æµã«

**èƒŒæ™¯**:

å¾“æ¥ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã¯**ç‰©ç†æ³•å‰‡ã‚’çŸ¥ã‚‰ãªã„**:

- ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒå‹æ‰‹ã«å¢—æ¸›
- é‹å‹•é‡ãŒä¿å­˜ã•ã‚Œãªã„
- éç‰©ç†çš„ãªè»Œé“ï¼ˆå£ã‚’ã™ã‚ŠæŠœã‘ã‚‹ç­‰ï¼‰

**è§£æ±ºç­–**: ç‰©ç†æ³•å‰‡ã‚’**æå¤±é–¢æ•°**ã¾ãŸã¯**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ã«åŸ‹ã‚è¾¼ã‚€

**æ‰‹æ³•1: Graph Neural Networks (GNNs) â€” é‹å‹•é‡ãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜**

**è«–æ–‡**: Nature Communications 2025 "Physics-informed GNN conserving linear and angular momentum"

**åŸç†**:

ç²’å­ç³»ã®é‹å‹•ã‚’å­¦ç¿’ã™ã‚‹éš›ã€ä»¥ä¸‹ã‚’ä¿è¨¼:

1. **é‹å‹•é‡ä¿å­˜**: $\sum_i m_i \mathbf{v}_i = \text{const}$
2. **è§’é‹å‹•é‡ä¿å­˜**: $\sum_i \mathbf{r}_i \times m_i \mathbf{v}_i = \text{const}$
3. **ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜**: $\sum_i \frac{1}{2}m_i \|\mathbf{v}_i\|^2 + U(\mathbf{r}) = \text{const}$

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

```
Input: Particle positions r_i, velocities v_i
GNN Edge Model: F_{ij} = MLP(r_i, r_j, v_i, v_j)
Symmetrization: F_{ij} = -F_{ji}  (Newton's 3rd law)
Update: v_i^{new} = v_i + Î£_j F_{ij} / m_i
```

**Juliaå®Œå…¨å®Ÿè£…**:

```julia
using Lux, Flux, LinearAlgebra

struct PhysicsInformedGNN
    edge_mlp::Chain       # Computes pairwise forces
    mass::Vector{Float32} # Particle masses
end

function (m::PhysicsInformedGNN)(positions, velocities, ps, st; Î”t=0.01)
    N = size(positions, 1)  # Number of particles
    forces = zeros(Float32, N, 3)

    # Compute pairwise forces (message passing)
    for i in 1:N
        for j in (i+1):N
            # Edge features
            r_ij = positions[j, :] - positions[i, :]
            v_ij = velocities[j, :] - velocities[i, :]
            edge_feat = vcat(r_ij, v_ij)

            # Compute force (symmetric)
            F_ij, _ = m.edge_mlp(edge_feat, ps.edge_mlp, st.edge_mlp)

            # Newton's 3rd law: F_ij = -F_ji
            forces[i, :] += F_ij
            forces[j, :] -= F_ij
        end
    end

    # Verify momentum conservation (should be ~0)
    total_force = sum(forces, dims=1)
    @assert all(abs.(total_force) .< 1e-5) "Newton's 3rd law violated!"

    # Update velocities and positions
    accelerations = forces ./ m.mass'  # Broadcasting over masses
    v_new = velocities + accelerations * Î”t
    r_new = positions + v_new * Î”t

    return r_new, v_new, st
end

# Energy conservation verification
function verify_conservation(r, v, masses, potential_fn)
    # Kinetic energy
    KE = sum(0.5 * masses .* sum(v.^2, dims=2))

    # Potential energy
    PE = potential_fn(r)

    # Total energy
    E_total = KE + PE

    return E_total
end
```

**æ‰‹æ³•2: Hamiltonian Neural Networks (HNNs) â€” ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ã®ä¿è¨¼**

**è«–æ–‡**: Greydanus et al., NeurIPS 2019

**åŸç†**:

HamiltonianåŠ›å­¦ã§ã¯ã€ç³»ã®æ™‚é–“ç™ºå±•ã¯ä»¥ä¸‹ã§è¨˜è¿°ã•ã‚Œã‚‹:

$$
\frac{dq}{dt} = \frac{\partial H}{\partial p}, \quad \frac{dp}{dt} = -\frac{\partial H}{\partial q}
$$

ã“ã“ã§$H(q, p)$ã¯Hamiltonianï¼ˆç·ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰ã€‚

**ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å­¦ç¿’**:

$$
H_\theta(q, p) = \text{MLP}_\theta([q; p])
$$

æ™‚é–“ç™ºå±•:

$$
\dot{q} = \nabla_p H_\theta, \quad \dot{p} = -\nabla_q H_\theta
$$

**ä¿è¨¼**: Hamiltonianã¯æ™‚é–“ä¸å¤‰ $\frac{dH}{dt} = 0$ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ï¼‰

**Juliaå®Ÿè£…**:

```julia
using Lux, Zygote, OrdinaryDiffEq

struct HamiltonianNN
    mlp::Chain  # Learns H(q, p)
end

function (m::HamiltonianNN)(qp, ps, st)
    # qp = [q; p] (generalized coordinates + momenta)
    H, st = m.mlp(qp, ps.mlp, st.mlp)
    return H[1], st  # Scalar energy
end

# Hamiltonian dynamics (ODE right-hand side)
function hamiltonian_dynamics(qp, model, ps, st, t)
    # Compute H(q, p)
    H, st = model(qp, ps, st)

    # Compute gradients
    âˆ‡H = gradient(qp -> model(qp, ps, st)[1], qp)[1]

    D = length(qp) Ã· 2
    dq = âˆ‡H[D+1:end]   # âˆ‚H/âˆ‚p
    dp = -âˆ‡H[1:D]       # -âˆ‚H/âˆ‚q

    return vcat(dq, dp)
end

# Solve dynamics
using OrdinaryDiffEq

function simulate_hamiltonian(model, qp0, ps, st, tspan)
    prob = ODEProblem((qp, p, t) -> hamiltonian_dynamics(qp, model, ps, st, t), qp0, tspan)
    sol = solve(prob, Tsit5())
    return sol
end

# Training
function train_hnn!(model, data, ps, st; epochs=100)
    # data: [(qp_0, qp_1, Î”t), ...]
    opt = Adam(1e-3)
    opt_st = Optimisers.setup(opt, ps)

    for epoch in 1:epochs
        total_loss = 0.0
        for (qp0, qp1, Î”t) in data
            # Simulate one step
            tspan = (0.0, Î”t)
            sol = simulate_hamiltonian(model, qp0, ps, st, tspan)
            qp_pred = sol[end]

            # Loss
            loss = Flux.mse(qp_pred, qp1)

            # Backprop
            grads = gradient(ps -> loss, ps)[1]
            opt_st, ps = Optimisers.update(opt_st, ps, grads)

            total_loss += loss
        end
        println("Epoch $epoch: Loss = $(total_loss / length(data))")
    end
    return ps, st
end
```

**æ‰‹æ³•3: PINNs (Physics-Informed Neural Networks) â€” å¾®åˆ†æ–¹ç¨‹å¼åˆ¶ç´„**

**åŸç†**:

åå¾®åˆ†æ–¹ç¨‹å¼ï¼ˆä¾‹: Navier-Stokesæµä½“æ–¹ç¨‹å¼ï¼‰ã‚’**æå¤±é–¢æ•°ã«ç›´æ¥åŸ‹ã‚è¾¼ã‚€**ã€‚

**ä¾‹: 1Dç†±æ–¹ç¨‹å¼**:

$$
\frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x^2}
$$

**ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ**: $u_\theta(x, t) = \text{MLP}([x, t])$

**æå¤±**:

$$
\mathcal{L} = \mathcal{L}_{\text{data}} + \lambda \mathcal{L}_{\text{PDE}}
$$

$$
\mathcal{L}_{\text{data}} = \sum_{i} (u_\theta(x_i, t_i) - u_i)^2
$$

$$
\mathcal{L}_{\text{PDE}} = \sum_{j} \left( \frac{\partial u_\theta}{\partial t} - \alpha \frac{\partial^2 u_\theta}{\partial x^2} \right)^2_{(x_j, t_j)}
$$

**Juliaå®Ÿè£…**:

```julia
using Lux, Zygote

struct PINN
    net::Chain  # u(x, t) approximator
    Î±::Float32  # Diffusion coefficient
end

function (m::PINN)(x, t, ps, st)
    input = vcat(x, t)
    u, st = m.net(input, ps.net, st.net)
    return u[1], st
end

# PDE residual
function pde_residual(m::PINN, x, t, ps, st)
    # Compute u(x, t) and its derivatives
    u, st = m(x, t, ps, st)

    # âˆ‚u/âˆ‚t
    âˆ‚u_âˆ‚t = gradient(t -> m(x, t, ps, st)[1], t)[1]

    # âˆ‚Â²u/âˆ‚xÂ²
    âˆ‚u_âˆ‚x = gradient(x -> m(x, t, ps, st)[1], x)[1]
    âˆ‚Â²u_âˆ‚xÂ² = gradient(x -> gradient(x -> m(x, t, ps, st)[1], x)[1], x)[1]

    # PDE residual: âˆ‚u/âˆ‚t - Î± âˆ‚Â²u/âˆ‚xÂ²
    residual = âˆ‚u_âˆ‚t - m.Î± * âˆ‚Â²u_âˆ‚xÂ²

    return residual^2
end

# Training
function train_pinn!(model, data_points, collocation_points, ps, st; epochs=1000, Î»=1.0)
    opt = Adam(1e-3)
    opt_st = Optimisers.setup(opt, ps)

    for epoch in 1:epochs
        # Data loss
        loss_data = 0.0
        for (x, t, u_true) in data_points
            u_pred, st = model(x, t, ps, st)
            loss_data += (u_pred - u_true)^2
        end

        # PDE loss
        loss_pde = 0.0
        for (x, t) in collocation_points
            loss_pde += pde_residual(model, x, t, ps, st)
        end

        # Total loss
        loss = loss_data + Î» * loss_pde

        # Backprop
        grads = gradient(ps -> loss, ps)[1]
        opt_st, ps = Optimisers.update(opt_st, ps, grads)

        if epoch % 100 == 0
            println("Epoch $epoch: Data Loss = $loss_data, PDE Loss = $loss_pde")
        end
    end
    return ps, st
end
```

**å¿œç”¨åˆ†é‡**:

| åˆ†é‡ | å•é¡Œ | æ‰‹æ³• | æˆæœ |
|:----|:-----|:-----|:-----|
| **æ°—å€™ç§‘å­¦** | å¤§æ°—å¾ªç’°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | GNN + ä¿å­˜å‰‡ | è¨ˆç®—é€Ÿåº¦100x, ç²¾åº¦åŒç­‰ |
| **æµä½“åŠ›å­¦** | Navier-Stokesæ–¹ç¨‹å¼ | PINNs | ãƒ‡ãƒ¼ã‚¿é‡1/10ã§å­¦ç¿’å¯èƒ½ |
| **åˆ†å­å‹•åŠ›å­¦** | ã‚¿ãƒ³ãƒ‘ã‚¯è³ªæŠ˜ã‚Šç•³ã¿ | HNN | ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜, é•·æ™‚é–“å®‰å®š |
| **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹** | Multi-body dynamics | GNN | Sim-to-realè»¢ç§»æˆåŠŸç‡+25% |
| **ææ–™ç§‘å­¦** | çµæ™¶æ§‹é€ äºˆæ¸¬ | PINNs + å¯¾ç§°æ€§ | æ–°ææ–™ç™ºè¦‹åŠ é€Ÿ |

### 6.3 å®Œå…¨å®Ÿè£…ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« â€” V-JEPA

ã“ã“ã¾ã§I-JEPAã‚’å®Ÿè£…ã—ãŸã€‚æ¬¡ã¯**V-JEPAï¼ˆå‹•ç”»ç‰ˆï¼‰**ã‚’å®Œå…¨å®Ÿè£…ã™ã‚‹ã€‚

#### 6.3.1 V-JEPAã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³è§£

**ã‚³ã‚¢è¦ç´ **:

1. **Spatio-temporal Encoder**: å‹•ç”»ãƒ‘ãƒƒãƒã‚’æ½œåœ¨è¡¨ç¾ã«å¤‰æ›
2. **Predictor**: ãƒã‚¹ã‚¯ã•ã‚ŒãŸæ™‚ç©ºé–“é ˜åŸŸã®è¡¨ç¾ã‚’äºˆæ¸¬
3. **Target Encoder**: EMAã§æ›´æ–°ã•ã‚Œã‚‹ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ

**ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥**:

```
Frame 0: [â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ]  â† Context
Frame 1: [â–‘â–‘â–‘â–‘ â–ˆâ–ˆâ–ˆâ–ˆ â–‘â–‘â–‘â–‘]  â† Masked
Frame 2: [â–ˆâ–ˆâ–ˆâ–ˆ â–‘â–‘â–‘â–‘ â–ˆâ–ˆâ–ˆâ–ˆ]  â† Masked
Frame 3: [â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘ â–‘â–‘â–‘â–‘]  â† Fully masked
```

**Juliaå®Œå…¨å®Ÿè£…**:

```julia
using Lux, Flux, Random, Statistics

# 3D Vision Transformer for video
struct VideoEncoder
    patch_embed::Chain      # 3D convolution for spatio-temporal patches
    pos_embed::Array{Float32, 3}  # Positional embedding [T, H*W, D]
    transformer::Chain      # Transformer blocks
    norm::LayerNorm
end

function VideoEncoder(; frames=4, patch_size=16, img_size=224, embed_dim=768, depth=12)
    n_patches_per_frame = (img_size Ã· patch_size)^2

    # 3D patch embedding: (C, H, W, T) â†’ (D, n_patches, T)
    patch_embed = Chain(
        Conv((3, patch_size, patch_size), 3 => embed_dim, stride=(1, patch_size, patch_size)),
        flatten
    )

    # Learnable positional embedding
    pos_embed = randn(Float32, frames, n_patches_per_frame, embed_dim) * 0.02

    # Transformer
    transformer = Chain([
        TransformerBlock(embed_dim, n_heads=12, mlp_ratio=4.0) for _ in 1:depth
    ]...)

    norm = LayerNorm(embed_dim)

    return VideoEncoder(patch_embed, pos_embed, transformer, norm)
end

function (m::VideoEncoder)(video, mask, ps, st)
    # video: [C, H, W, T, B] â€” (channels, height, width, time, batch)
    # mask: [T, n_patches, B] â€” 1=keep, 0=remove

    # Patch embedding
    patches, st_emb = m.patch_embed(video, ps.patch_embed, st.patch_embed)
    # patches: [D, n_patches, T, B]

    # Add positional embedding
    patches = patches .+ m.pos_embed  # Broadcasting

    # Apply mask (remove masked patches)
    T, n_patches, D = size(m.pos_embed)
    B = size(video, 5)
    patches_flat = reshape(patches, D, T * n_patches, B)
    mask_flat = reshape(mask, T * n_patches, B)

    # Keep only visible patches
    visible_patches = [patches_flat[:, mask_flat[:, b] .== 1, b] for b in 1:B]

    # Transformer (process each batch element separately due to variable length)
    encoded = []
    for b in 1:B
        x = visible_patches[b]
        x, st_trans = m.transformer(x, ps.transformer, st.transformer)
        x = m.norm(x, ps.norm, st.norm)[1]
        push!(encoded, x)
    end

    return encoded, st
end

# Predictor: predicts masked regions
struct VideoPredictor
    mask_token::Array{Float32, 1}  # Learnable mask token [D]
    transformer::Chain
    head::Dense
end

function VideoPredictor(; embed_dim=768, depth=6, n_masks=16)
    mask_token = randn(Float32, embed_dim) * 0.02

    transformer = Chain([
        TransformerBlock(embed_dim, n_heads=12, mlp_ratio=4.0) for _ in 1:depth
    ]...)

    head = Dense(embed_dim, embed_dim)  # Project to target dim

    return VideoPredictor(mask_token, transformer, head)
end

function (m::VideoPredictor)(context_tokens, mask_positions, ps, st)
    # context_tokens: [D, N_visible, B]
    # mask_positions: [N_masked, 3] â€” (t, patch_idx, batch) for each masked position

    D = size(context_tokens, 1)
    N_masked = size(mask_positions, 1)
    B = size(context_tokens, 3)

    # Create mask tokens
    mask_tokens = repeat(m.mask_token, 1, N_masked, B)  # [D, N_masked, B]

    # Concatenate context + mask tokens
    all_tokens = cat(context_tokens, mask_tokens, dims=2)  # [D, N_visible+N_masked, B]

    # Transformer
    x, st_trans = m.transformer(all_tokens, ps.transformer, st.transformer)

    # Extract predictions for masked positions
    predictions = x[:, (size(context_tokens, 2)+1):end, :]  # [D, N_masked, B]

    # Project
    predictions, st_head = m.head(predictions, ps.head, st.head)

    return predictions, st
end

# Full V-JEPA model
struct VJEPA
    context_encoder::VideoEncoder
    target_encoder::VideoEncoder
    predictor::VideoPredictor
end

function VJEPA(; frames=4, patch_size=16, img_size=224, embed_dim=768, enc_depth=12, pred_depth=6)
    context_encoder = VideoEncoder(frames=frames, patch_size=patch_size, img_size=img_size,
                                     embed_dim=embed_dim, depth=enc_depth)
    target_encoder = VideoEncoder(frames=frames, patch_size=patch_size, img_size=img_size,
                                    embed_dim=embed_dim, depth=enc_depth)
    predictor = VideoPredictor(embed_dim=embed_dim, depth=pred_depth)

    return VJEPA(context_encoder, target_encoder, predictor)
end

# Training step
function train_vjepa_step!(model, video_batch, ps, st; Ï„=0.996)
    # Generate random spatio-temporal mask
    T, H, W, C, B = size(video_batch)
    n_patches = (H Ã· 16) * (W Ã· 16)
    mask = generate_spatiotemporal_mask(T, n_patches, B, mask_ratio=0.6)

    # Context encoding (with mask)
    context_tokens, st_ctx = model.context_encoder(video_batch, mask, ps.context_encoder, st.context_encoder)

    # Target encoding (full video, stopgradient)
    target_tokens, st_tgt = model.target_encoder(video_batch, ones(size(mask)), ps.target_encoder, st.target_encoder)
    # Stopgradient: do not backprop through target encoder

    # Predictor
    mask_positions = findall(mask .== 0)
    predictions, st_pred = model.predictor(context_tokens, mask_positions, ps.predictor, st.predictor)

    # Loss: MSE in latent space
    targets = [target_tokens[b][:, mask_positions[b]] for b in 1:B]
    loss = sum([Flux.mse(predictions[:, :, b], targets[b]) for b in 1:B]) / B

    # EMA update for target encoder
    ps.target_encoder = update_ema(ps.target_encoder, ps.context_encoder, Ï„)

    return loss, st
end

function generate_spatiotemporal_mask(T, n_patches, B; mask_ratio=0.6)
    # Simple random masking (can use more sophisticated block masking)
    mask = rand(Float32, T, n_patches, B) .> mask_ratio
    return mask
end

function update_ema(target_ps, context_ps, Ï„)
    # Exponential moving average
    return Ï„ .* target_ps .+ (1 - Ï„) .* context_ps
end
```

#### 6.3.2 V-JEPAã®è¨“ç·´ â€” Kinetics-400ã§ã®å®Ÿé¨“

**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: Kinetics-400 (400 action classes, 240K training videos)

**è¨“ç·´è¨­å®š**:

```julia
# Hyperparameters
config = Dict(
    "frames" => 16,             # Input video length
    "patch_size" => 16,
    "img_size" => 224,
    "embed_dim" => 1024,        # ViT-Large
    "enc_depth" => 24,
    "pred_depth" => 12,
    "batch_size" => 16,         # Per GPU
    "epochs" => 300,
    "lr" => 1.5e-4,
    "weight_decay" => 0.05,
    "Ï„_initial" => 0.996,
    "Ï„_final" => 1.0,           # EMA momentum schedule
    "mask_ratio" => 0.6
)

# Learning rate schedule: cosine decay
function cosine_schedule(epoch, total_epochs, lr_max, lr_min=0.0)
    return lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(Ï€ * epoch / total_epochs))
end

# Ï„ schedule: gradually increase to 1.0
function tau_schedule(epoch, total_epochs, Ï„_init, Ï„_final)
    return Ï„_final - (Ï„_final - Ï„_init) * cos(Ï€ * epoch / total_epochs) / 2
end

# Training loop
function train_vjepa!(model, train_loader, ps, st, config)
    opt = AdamW(config["lr"], (0.9, 0.95), config["weight_decay"])
    opt_st = Optimisers.setup(opt, ps)

    for epoch in 1:config["epochs"]
        # Update learning rate
        lr = cosine_schedule(epoch, config["epochs"], config["lr"])
        opt.eta = lr

        # Update EMA momentum
        Ï„ = tau_schedule(epoch, config["epochs"], config["Ï„_initial"], config["Ï„_final"])

        total_loss = 0.0
        for video_batch in train_loader
            # Forward + backward
            loss, st = train_vjepa_step!(model, video_batch, ps, st, Ï„=Ï„)

            # Gradient descent
            grads = gradient(ps -> loss, ps)[1]
            opt_st, ps = Optimisers.update(opt_st, ps, grads)

            total_loss += loss
        end

        avg_loss = total_loss / length(train_loader)
        println("Epoch $epoch/$( config["epochs"]): Loss = $(round(avg_loss, digits=4)), LR = $(round(lr, sigdigits=3)), Ï„ = $(round(Ï„, digits=4))")

        # Save checkpoint every 50 epochs
        if epoch % 50 == 0
            save_checkpoint(ps, st, "vjepa_epoch$(epoch).jld2")
        end
    end

    return ps, st
end
```

**æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½** (è«–æ–‡å€¤ã¨æ¯”è¼ƒ):

| æŒ‡æ¨™ | V-JEPA (è«–æ–‡) | ä»Šå›ã®å®Ÿè£… (äºˆæƒ³) |
|:----|:--------------|:------------------|
| **Kinetics-400 Top-1** | 81.9% | 78-80% (fewer resources) |
| **è¨“ç·´æ™‚é–“** | 288 GPU hours (A100 x32) | ~600 GPU hours (single A100) |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** | 40GB/GPU | 38GB/GPU |

#### 6.3.3 Transfusionå®Œå…¨å®Ÿè£… â€” ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒçµ±ä¸€ãƒ¢ãƒ‡ãƒ«

**Challenge**: 1ã¤ã®Transformerã§ãƒ†ã‚­ã‚¹ãƒˆï¼ˆAR lossï¼‰ã¨ç”»åƒï¼ˆDiffusion lossï¼‰ã‚’åŒæ™‚è¨“ç·´

**Juliaå®Ÿè£…**:

```julia
using Lux, Flux, Transformers

struct TransfusionModel
    text_tokenizer::BPETokenizer     # For text
    image_patchify::Chain            # For image patches
    shared_transformer::Chain        # Unified Transformer
    text_head::Dense                 # Next token prediction
    image_head::Dense                # Noise prediction
end

function TransfusionModel(; vocab_size=50257, img_patch_size=16, d_model=2048, n_layers=24)
    text_tokenizer = load_tokenizer("gpt2")

    # Image patchify
    image_patchify = Chain(
        Conv((img_patch_size, img_patch_size), 3 => d_model, stride=img_patch_size),
        flatten
    )

    # Shared Transformer
    shared_transformer = Chain([
        TransformerBlock(d_model, n_heads=32, mlp_ratio=4.0) for _ in 1:n_layers
    ]...)

    # Task-specific heads
    text_head = Dense(d_model, vocab_size)  # Logits over vocab
    image_head = Dense(d_model, 3 * img_patch_size^2)  # Predict RGB patch

    return TransfusionModel(text_tokenizer, image_patchify, shared_transformer, text_head, image_head)
end

# Forward pass for text (autoregressive)
function forward_text(model, text_tokens, ps, st)
    # text_tokens: [seq_len, batch]
    # Embed
    embeddings = ps.text_embedding[text_tokens, :]  # [seq_len, d_model, batch]

    # Transformer (causal mask)
    h, st_trans = model.shared_transformer(embeddings, ps.shared_transformer, st.shared_transformer)

    # Predict next token
    logits, st_head = model.text_head(h, ps.text_head, st.text_head)

    return logits, st
end

# Forward pass for image (diffusion)
function forward_image(model, image, t, ps, st)
    # image: [C, H, W, batch] noisy image at timestep t
    # Patchify
    patches, st_patch = model.image_patchify(image, ps.image_patchify, st.image_patchify)
    # patches: [d_model, n_patches, batch]

    # Add time embedding
    t_emb = sinusoidal_embedding(t, d_model)
    patches = patches .+ t_emb

    # Transformer (bidirectional, no causal mask)
    h, st_trans = model.shared_transformer(patches, ps.shared_transformer, st.shared_transformer)

    # Predict noise
    noise_pred, st_head = model.image_head(h, ps.image_head, st.image_head)

    return noise_pred, st
end

# Unified training step
function train_transfusion_step!(model, text_batch, image_batch, ps, st; Î»=1.0)
    # Text loss (cross-entropy)
    text_logits, st = forward_text(model, text_batch, ps, st)
    text_target = text_batch[2:end, :]  # Shifted by 1 (next token)
    loss_text = Flux.crossentropy(softmax(text_logits[1:end-1, :, :]), text_target)

    # Image loss (diffusion)
    t = rand(1:1000)  # Random timestep
    noise = randn(size(image_batch))
    noisy_image = sqrt(alpha(t)) * image_batch + sqrt(1 - alpha(t)) * noise
    noise_pred, st = forward_image(model, noisy_image, t, ps, st)
    loss_image = Flux.mse(noise_pred, noise)

    # Combined loss
    loss = loss_text + Î» * loss_image

    return loss, st
end

# Helper: sinusoidal time embedding
function sinusoidal_embedding(t, d_model)
    half_d = d_model Ã· 2
    freqs = exp.(-log(10000) * (0:half_d-1) / half_d)
    args = t * freqs'
    emb = vcat(sin.(args), cos.(args))
    return emb
end

# Helper: alpha schedule (DDPM)
function alpha(t, T=1000)
    Î² = 0.0001 + (0.02 - 0.0001) * t / T
    return prod([1 - Î²_i for Î²_i in Î²])
end
```

**è¨“ç·´**:

```julia
# Transfusion training
config_transfusion = Dict(
    "batch_size" => 128,
    "epochs" => 100,
    "lr" => 1e-4,
    "Î»" => 0.1,  # Balance text vs image loss
    "d_model" => 2048,
    "n_layers" => 24
)

function train_transfusion!(model, text_data, image_data, ps, st, config)
    opt = Adam(config["lr"])
    opt_st = Optimisers.setup(opt, ps)

    for epoch in 1:config["epochs"]
        total_loss = 0.0
        for (text_batch, image_batch) in zip(text_data, image_data)
            loss, st = train_transfusion_step!(model, text_batch, image_batch, ps, st, Î»=config["Î»"])

            grads = gradient(ps -> loss, ps)[1]
            opt_st, ps = Optimisers.update(opt_st, ps, grads)

            total_loss += loss
        end

        println("Epoch $epoch: Loss = $(total_loss / min(length(text_data), length(image_data)))")
    end

    return ps, st
end
```

### 6.4 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç·åˆæ¯”è¼ƒ

å„World Modelsãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ€§èƒ½ã‚’çµ±åˆæ¯”è¼ƒã™ã‚‹ã€‚

#### 6.4.1 ç”»åƒç†è§£ã‚¿ã‚¹ã‚¯ (ImageNet-1K)

| ãƒ¢ãƒ‡ãƒ« | Top-1 Acc | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | è¨“ç·´æ™‚é–“ |
|:------|:----------|:----------|:----------|:---------|
| **I-JEPA (ViT-H/14)** | 85.0% | ImageNet-1K | 632M | 72h (8xA100) |
| **MAE (ViT-H/14)** | 87.8% | ImageNet-1K | 632M | 96h (8xA100) |
| **CLIP (ViT-L/14)** | 88.3% | 400M pairs | 428M | 2048h (256xV100) |
| **DINOv2 (ViT-g/14)** | 90.1% | LVD-142M | 1.1B | 10000h (?) |

**è€ƒå¯Ÿ**: I-JEPAã¯è¨“ç·´åŠ¹ç‡ã¯é«˜ã„ãŒã€ãƒ”ã‚¯ã‚»ãƒ«å†æ§‹æˆã™ã‚‹MAEã‚ˆã‚Šç²¾åº¦ã¯è‹¥å¹²åŠ£ã‚‹ã€‚

#### 6.4.2 å‹•ç”»ç†è§£ã‚¿ã‚¹ã‚¯ (Kinetics-400)

| ãƒ¢ãƒ‡ãƒ« | Top-1 Acc | è¨“ç·´æ–¹å¼ | äº‹å‰è¨“ç·´ãƒ‡ãƒ¼ã‚¿ | Fine-tuning |
|:------|:----------|:---------|:---------------|:------------|
| **V-JEPA** | 81.9% | Self-supervised | Kinetics-400 | Linear probe |
| **VideoMAE** | 83.5% | Self-supervised | Kinetics-400 | Fine-tune |
| **TimeSformer** | 80.7% | Supervised | ImageNet-21K | Fine-tune |
| **VideoSwin-B** | 82.7% | Supervised | Kinetics-400 | Full |

**è€ƒå¯Ÿ**: V-JEPAã¯Linear probeã§81.9%ã‚’é”æˆï¼ˆFine-tuneãªã—ï¼‰ã€‚åŠ¹ç‡çš„ãªè¡¨ç¾å­¦ç¿’ã€‚

#### 6.4.3 ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ã‚¹ã‚¯ (MS-COCO Caption)

| ãƒ¢ãƒ‡ãƒ« | CIDEr | BLEU-4 | è¨“ç·´æ–¹å¼ | ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º |
|:------|:------|:-------|:---------|:-------------|
| **VL-JEPA** | 128.3 | 38.2 | Self-supervised | 1.2B |
| **BLIP-2** | 144.5 | 42.1 | Supervised | 2.7B |
| **Flamingo** | 138.1 | 40.3 | Few-shot | 80B |
| **CoCa** | 143.6 | 41.7 | Contrastive + Captioning | 2.1B |

**è€ƒå¯Ÿ**: VL-JEPAã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°50%å‰Šæ¸›ã§BLIP-2ã®89%æ€§èƒ½ã‚’é”æˆã€‚

#### 6.4.4 ä¸–ç•Œãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ç²¾åº¦ (Push task - ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹)

| ãƒ¢ãƒ‡ãƒ« | MSE (pixel) | SSIM | ç‰©ç†æ³•å‰‡éµå®ˆç‡ | è¨“ç·´ãƒ‡ãƒ¼ã‚¿é‡ |
|:------|:------------|:-----|:---------------|:-------------|
| **Cosmos-Predict2.5** | 0.021 | 0.94 | 96.8% | 200M clips |
| **World Models (Ha)** | 0.089 | 0.78 | 54.3% | 10K episodes |
| **DreamerV3** | 0.034 | 0.89 | 72.1% | 1M steps |
| **RSSM (PlaNet)** | 0.056 | 0.84 | 68.9% | 500K steps |

**è€ƒå¯Ÿ**: Cosmosã¯å¤§è¦æ¨¡è¨“ç·´ã«ã‚ˆã‚Šç‰©ç†çš„ä¸€è²«æ€§ãŒå¤§å¹…å‘ä¸Šã€‚

#### 6.4.5 è¨“ç·´åŠ¹ç‡æ¯”è¼ƒ (GPUæ™‚é–“ã‚ãŸã‚Šã®æ€§èƒ½å‘ä¸Š)

| ãƒ¢ãƒ‡ãƒ« | 1000 GPUæ™‚é–“ã§ã®åˆ°é”ç²¾åº¦ | ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ |
|:------|:-------------------------|:----------|:----------|
| **I-JEPA** | ImageNet Top-1 82% | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜… |
| **MAE** | ImageNet Top-1 84% | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜† |
| **V-JEPA** | Kinetics Top-1 79% | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† |
| **Transfusion** | Mixed metrics | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜†â˜† |

### 6.5 æ¨è–¦æ›¸ç±ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

| ã‚¿ã‚¤ãƒˆãƒ« | è‘—è€… | URL | ãƒ¬ãƒ™ãƒ« |
|:--------|:-----|:----|:------|
| **JEPAå…¬å¼ãƒ–ãƒ­ã‚°** | Yann LeCun, Meta AI | [Meta AI Blog](https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/) | åˆç´š |
| **V-JEPAå…¬å¼ãƒšãƒ¼ã‚¸** | Meta AI | [V-JEPA](https://ai.meta.com/vjepa/) | åˆç´š |
| **Transfusionè«–æ–‡** | Zhou et al. | [arXiv:2408.11039](https://arxiv.org/abs/2408.11039) | ä¸­ç´š |
| **Cosmoså…¬å¼** | NVIDIA | [NVIDIA Cosmos](https://www.nvidia.com/en-us/ai/cosmos/) | ä¸­ç´š |
| **Physics-Informed ML** | Karniadakis et al. | [Nature Rev Physics](https://www.nature.com/articles/s42254-021-00314-5) | ä¸Šç´š |
| **Hamiltonian NNè«–æ–‡** | Greydanus et al. | [arXiv:1906.01563](https://arxiv.org/abs/1906.01563) | ä¸Šç´š |
| **World Models (Ha)** | Ha & Schmidhuber | [arXiv:1803.10122](https://arxiv.org/abs/1803.10122) | ä¸­ç´š |
| **DreamerV3** | Hafner et al. | [arXiv:2301.04104](https://arxiv.org/abs/2301.04104) | ä¸Šç´š |

### 6.6 å®Ÿè£…Tips & ãƒ‡ãƒãƒƒã‚°ã‚¬ã‚¤ãƒ‰

#### 6.6.1 JEPAè¨“ç·´ã®å…¸å‹çš„å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨å¯¾ç­–

**å¤±æ•—1: EMAã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ç™ºæ•£**

**ç—‡çŠ¶**: æå¤±ãŒè¨“ç·´é–‹å§‹ç›´å¾Œã« `NaN` or `Inf`

**åŸå› **: EMA momentum Ï„ ãŒå°ã•ã™ãã‚‹ï¼ˆä¾‹: Ï„=0.9ï¼‰â†’ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒæ€¥å¤‰

**å¯¾ç­–**:
```julia
# âŒ Bad: å›ºå®šÏ„=0.9
Ï„ = 0.9

# âœ… Good: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (0.996 â†’ 1.0)
function tau_schedule(epoch, total_epochs; Ï„_init=0.996, Ï„_final=1.0)
    return Ï„_final - (Ï„_final - Ï„_init) * cos(Ï€ * epoch / total_epochs) / 2
end
```

**å¤±æ•—2: ãƒã‚¹ã‚¯æ¯”ç‡ãŒæ¥µç«¯**

**ç—‡çŠ¶**: æå¤±ã¯æ¸›ã‚‹ãŒã€ä¸‹æµã‚¿ã‚¹ã‚¯ã§æ€§èƒ½ãŒå‡ºãªã„

**åŸå› **:
- ãƒã‚¹ã‚¯æ¯”ç‡90%ä»¥ä¸Š â†’ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸è¶³ã§äºˆæ¸¬ä¸å¯èƒ½
- ãƒã‚¹ã‚¯æ¯”ç‡10%ä»¥ä¸‹ â†’ ç°¡å˜ã™ãã¦è¡¨ç¾åŠ›ãŒè‚²ãŸãªã„

**å¯¾ç­–**:
```julia
# âœ… Optimal: I-JEPA=60-75%, V-JEPA=50-70%
mask_ratio = 0.6  # Start here
```

**å¤±æ•—3: Predictor ãŒ Context Encoder ã‚ˆã‚Šæ·±ã„**

**ç—‡çŠ¶**: éå­¦ç¿’ã€è¨“ç·´loss<æ¤œè¨¼loss ã®å·®ãŒå¤§ãã„

**åŸå› **: PredictorãŒå¼·ã™ãã¦ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆå­¦ç¿’ï¼ˆãƒã‚¹ã‚¯ä½ç½®ã ã‘ã‹ã‚‰äºˆæ¸¬ï¼‰

**å¯¾ç­–**:
```julia
# âœ… Rule: Predictor depth = 1/2 * Encoder depth
config = Dict(
    "enc_depth" => 12,
    "pred_depth" => 6  # Half
)
```

#### 6.6.2 Physics-Informed NN ã®ãƒ‡ãƒãƒƒã‚°

**å¤±æ•—1: PDE residual ãŒæ¸›ã‚‰ãªã„**

**ç—‡çŠ¶**: Data loss ã¯æ¸›ã‚‹ãŒ PDE loss ã¯é«˜æ­¢ã¾ã‚Š

**åŸå› **: Î»ï¼ˆPDE weightï¼‰ãŒå°ã•ã™ãã‚‹ã€ã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒç‰©ç†æ³•å‰‡ã‚’è¡¨ç¾ã§ããªã„

**å¯¾ç­–**:
```julia
# Adaptive Î»: PDE lossã¨Data lossã®ãƒãƒ©ãƒ³ã‚¹ã‚’è‡ªå‹•èª¿æ•´
function adaptive_lambda(loss_data, loss_pde; target_ratio=1.0)
    return target_ratio * loss_data / (loss_pde + 1e-8)
end

# è¨“ç·´ãƒ«ãƒ¼ãƒ—å†…
Î» = adaptive_lambda(loss_data, loss_pde)
loss = loss_data + Î» * loss_pde
```

**å¤±æ•—2: ä¿å­˜å‰‡é•åï¼ˆHNN/GNNï¼‰**

**ç—‡çŠ¶**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒæ™‚é–“ã¨ã¨ã‚‚ã«ãƒ‰ãƒªãƒ•ãƒˆ

**åŸå› **: æ•°å€¤ç©åˆ†èª¤å·®ã€ã¾ãŸã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒå¯¾ç§°æ€§ã‚’å®ˆã£ã¦ã„ãªã„

**å¯¾ç­–**:
```julia
# âœ… Symplectic integrator (Verletæ³•) ã‚’ä½¿ç”¨
function verlet_step(q, p, H_theta, ps, st, Î”t)
    # Half step momentum
    âˆ‡H_q = gradient(q -> H_theta(vcat(q, p), ps, st)[1], q)[1]
    p_half = p - 0.5 * Î”t * âˆ‡H_q

    # Full step position
    âˆ‡H_p = gradient(p -> H_theta(vcat(q, p), ps, st)[1], p)[1]
    q_new = q + Î”t * âˆ‡H_p

    # Half step momentum (final)
    âˆ‡H_q_new = gradient(q -> H_theta(vcat(q_new, p_half), ps, st)[1], q_new)[1]
    p_new = p_half - 0.5 * Î”t * âˆ‡H_q_new

    return q_new, p_new
end
```

**å¤±æ•—3: GNN ã® Newton's 3rd law é•å**

**ç—‡çŠ¶**: ç·é‹å‹•é‡ãŒä¿å­˜ã•ã‚Œãªã„

**å¯¾ç­–**:
```julia
# âœ… å¿…ãš F_ij = -F_ji ã‚’æ˜ç¤ºçš„ã«å¼·åˆ¶
function enforce_newtons_third_law(forces_matrix)
    # forces_matrix: [N, N, 3] â€” F[i,j,:] = force on i from j
    N = size(forces_matrix, 1)
    for i in 1:N
        for j in (i+1):N
            # Average and symmetrize
            F_ij = (forces_matrix[i, j, :] - forces_matrix[j, i, :]) / 2
            forces_matrix[i, j, :] = F_ij
            forces_matrix[j, i, :] = -F_ij
        end
    end
    return forces_matrix
end
```

#### 6.6.3 Transfusion è¨“ç·´ã®ã‚³ãƒ„

**å¤±æ•—1: Text loss ã¨ Image loss ã®ä¸å‡è¡¡**

**ç—‡çŠ¶**: Text loss â†’ 0, Image loss é«˜æ­¢ã¾ã‚Šï¼ˆã¾ãŸã¯é€†ï¼‰

**åŸå› **: Î»ï¼ˆãƒãƒ©ãƒ³ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ãŒä¸é©åˆ‡

**å¯¾ç­–**:
```julia
# âœ… Dynamic Î»: ä¸¡æ–¹ã®lossã‚’åŒã˜ã‚¹ã‚±ãƒ¼ãƒ«ã«
function balance_losses(loss_text, loss_image)
    scale_text = stop_gradient(loss_text)  # å‹¾é…åœæ­¢
    scale_image = stop_gradient(loss_image)
    Î»_dynamic = scale_text / (scale_image + 1e-8)
    return loss_text + Î»_dynamic * loss_image
end
```

**å¤±æ•—2: Image patches ã¨ Text tokens ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡çª**

**ç—‡çŠ¶**: ãƒ¢ãƒ‡ãƒ«ãŒãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’æ··åŒï¼ˆãƒ†ã‚­ã‚¹ãƒˆä½ç½®ã«ç”»åƒã‚’ç”Ÿæˆï¼‰

**å¯¾ç­–**:
```julia
# âœ… Modality-specific positional encoding
struct TransfusionWithModalityPE
    text_pos_embed::Array{Float32, 2}  # [max_seq_len, d_model]
    image_pos_embed::Array{Float32, 2} # [n_patches, d_model]
    modality_token::Array{Float32, 1}  # [d_model] â€” text vs image identifier
end

function add_modality_pe(embeddings, modality::Symbol, model::TransfusionWithModalityPE)
    if modality == :text
        return embeddings .+ model.text_pos_embed .+ model.modality_token
    elseif modality == :image
        return embeddings .+ model.image_pos_embed .- model.modality_token
    end
end
```

#### 6.6.4 ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

**å•é¡Œ**: V-JEPA (16 frames, 224x224) ã¯1ã‚µãƒ³ãƒ—ãƒ«=100MB â†’ ãƒãƒƒãƒã‚µã‚¤ã‚º16ã§OOM

**è§£æ±ºç­–**:

1. **Gradient checkpointing**: ä¸­é–“å±¤ã®æ´»æ€§åŒ–ã‚’å†è¨ˆç®—

```julia
using Flux: @checkpoint

function forward_with_checkpointing(model, x, ps, st)
    # Checkpointã§ä¸­é–“å±¤ã®æ´»æ€§åŒ–ã‚’ä¿å­˜ã—ãªã„
    h = @checkpoint model.encoder(x, ps.encoder, st.encoder)
    return model.predictor(h, ps.predictor, st.predictor)
end
```

2. **Mixed precision (FP16)**:

```julia
using CUDA

# ãƒ¢ãƒ‡ãƒ«ã‚’FP16ã«å¤‰æ›
ps_fp16 = Float16.(ps)

# è¨“ç·´æ™‚ã¯æå¤±ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¿…é ˆ
loss_scale = 1024.0
loss_scaled = loss * loss_scale
grads = gradient(ps -> loss_scaled, ps_fp16)[1]
grads = grads ./ loss_scale  # Unscale
```

3. **Patch-wise processing** (V-JEPA):

```julia
# âœ… å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¸€åº¦ã«å‡¦ç†ã›ãšã€æ™‚é–“æ–¹å‘ã«åˆ†å‰²
function chunked_video_encoding(encoder, video, ps, st; chunk_size=4)
    T = size(video, 4)
    chunks = []
    for t in 1:chunk_size:T
        t_end = min(t + chunk_size - 1, T)
        chunk = video[:, :, :, t:t_end, :]
        encoded, st = encoder(chunk, ps, st)
        push!(chunks, encoded)
    end
    return cat(chunks..., dims=2), st  # Concatenate along time
end
```

### 6.7 Research Roadmap â€” æ¬¡ã®5å¹´ï¼ˆ2025-2030ï¼‰

#### 6.7.1 çŸ­æœŸï¼ˆ2025-2026ï¼‰: åŠ¹ç‡åŒ– & ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

**äºˆæ¸¬ã•ã‚Œã‚‹é€²å±•**:

1. **V-JEPA â†’ Long-context Video JEPA**
   - ç¾çŠ¶: 16ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ0.5ç§’ï¼‰
   - 2026å¹´: 256ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ10ç§’ï¼‰
   - æŠ€è¡“: Sparse attention + Hierarchical encoding

2. **Transfusion â†’ 3Dãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±ä¸€**
   - Text + Image + Video + 3D mesh ã‚’å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§
   - å¿œç”¨: 3Dç”Ÿæˆã€NeRFçµ±åˆ

3. **Physics-Informed WM â†’ å¾®åˆ†å¯èƒ½ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿çµ±åˆ**
   - MuJoCo/Isaac Gym ã¨ World Model ã®èåˆ
   - End-to-end ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡å­¦ç¿’

#### 6.7.2 ä¸­æœŸï¼ˆ2027-2028ï¼‰: AGIæ¥ç¶š & Embodied AI

**äºˆæ¸¬**:

1. **Causal World Models**
   - è¦³æ¸¬ã ã‘ã§ãªã**å› æœé–¢ä¿‚**ã‚’å­¦ç¿’
   - Doæ¼”ç®—å­ $P(Y|do(X))$ ã‚’æ¨å®š
   - Pearl's Causal Hierarchy ã‚’å®Ÿè£…

2. **Self-improving World Models**
   - ç’°å¢ƒã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã§è‡ªå¾‹æ”¹å–„
   - AlphaGoå¼ã®è‡ªå·±å¯¾æˆ¦ â†’ ç’°å¢ƒç†è§£æ·±åŒ–

3. **Embodied Agents with World Models**
   - Genie 3 â†’ å®Ÿãƒ­ãƒœãƒƒãƒˆã«å±•é–‹
   - Sim-to-real gap å®Œå…¨è§£æ¶ˆ

#### 6.7.3 é•·æœŸï¼ˆ2029-2030ï¼‰: æ±ç”¨ç’°å¢ƒç†è§£

**ç©¶æ¥µç›®æ¨™**:

1. **Universal World Model**
   - ä»»æ„ã®ç’°å¢ƒï¼ˆç‰©ç†/ãƒ‡ã‚¸ã‚¿ãƒ«/ç¤¾ä¼šï¼‰ã‚’ç†è§£
   - Few-shot adaptation: 3ãƒ•ãƒ¬ãƒ¼ãƒ è¦³æ¸¬ã§æ–°ç’°å¢ƒã‚’ç†è§£

2. **Counterfactual Reasoning**
   - "ã‚‚ã—ã€‡ã€‡ã—ã¦ã„ãŸã‚‰ï¼Ÿ" ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
   - æ”¿ç­–æ±ºå®šæ”¯æ´ã€ãƒªã‚¹ã‚¯è©•ä¾¡

3. **World Model â†’ World Simulator**
   - å®Œå…¨ãªãƒ‡ã‚¸ã‚¿ãƒ«ãƒ„ã‚¤ãƒ³
   - å¿œç”¨: éƒ½å¸‚è¨ˆç”»ã€æ°—å€™å¤‰å‹•å¯¾ç­–ã€ãƒ‘ãƒ³ãƒ‡ãƒŸãƒƒã‚¯å¯¾å¿œ

**å¿…è¦ãªãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼**:

| èª²é¡Œ | ç¾çŠ¶ | å¿…è¦æŠ€è¡“ |
|:----|:-----|:---------|
| **é•·æœŸäºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§** | 10ã‚¹ãƒ†ãƒƒãƒ—ã§ç™ºæ•£ | Hierarchical planning, Uncertainty quantification |
| **Sample efficiency** | 100ä¸‡ãƒ•ãƒ¬ãƒ¼ãƒ å¿…è¦ | Meta-learning, Prior knowledge injection |
| **Generalization** | è¨“ç·´ç’°å¢ƒã®ã¿ | Causal reasoning, Abstract representations |
| **Interpretability** | ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ | Attention visualization, Concept probing |

### 6.8 ç”¨èªé›†

:::details World Modelsã®ç”¨èªï¼ˆ50éŸ³é †ï¼‰
- **Action-conditioned prediction**: è¡Œå‹•$a_t$ã‚’æ¡ä»¶ã¨ã—ã¦æ¬¡çŠ¶æ…‹ã‚’äºˆæ¸¬
- **Causal World Model**: å› æœé–¢ä¿‚ã‚’æ˜ç¤ºçš„ã«å­¦ç¿’ã™ã‚‹ä¸–ç•Œãƒ¢ãƒ‡ãƒ«
- **Cosmos**: NVIDIAã®ç‰©ç†AIå‘ã‘ä¸–ç•ŒåŸºç›¤ãƒ¢ãƒ‡ãƒ«
- **Counterfactual reasoning**: åäº‹å®Ÿæ¨è«– â€” "ã‚‚ã—ã€‡ã€‡ãªã‚‰"ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- **EMA (Exponential Moving Average)**: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®é‡ã¿ã‚’æ»‘ã‚‰ã‹ã«æ›´æ–°ã™ã‚‹æ‰‹æ³•
- **Energy-based World Model**: ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°$E_\theta$ã§ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’å®šç¾©
- **Genie**: DeepMindã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ç’°å¢ƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«
- **Gradient checkpointing**: ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã®ãŸã‚ä¸­é–“å±¤ã®æ´»æ€§åŒ–ã‚’å†è¨ˆç®—
- **Hamiltonian Neural Network (HNN)**: Hamiltonian$H(q,p)$ã‚’å­¦ç¿’ã—ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ã‚’ä¿è¨¼
- **I-JEPA**: Image-based Joint-Embedding Predictive Architecture
- **JEPA**: Joint-Embedding Predictive Architecture â€” ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬
- **Latent action space**: æ˜ç¤ºçš„ãªãƒ©ãƒ™ãƒ«ãªã—ã§è¡Œå‹•ç©ºé–“ã‚’è‡ªå‹•ç™ºè¦‹
- **Latent space prediction**: ãƒ”ã‚¯ã‚»ãƒ«å†æ§‹æˆã›ãšã€æ½œåœ¨è¡¨ç¾ã‚’äºˆæ¸¬
- **Model-based RL**: World Modelã§ç’°å¢ƒã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã€æ–¹ç­–ã‚’æœ€é©åŒ–
- **Physics-Informed Neural Networks (PINNs)**: å¾®åˆ†æ–¹ç¨‹å¼åˆ¶ç´„ã‚’æå¤±ã«åŸ‹ã‚è¾¼ã‚€
- **Reward prediction**: World Modelã§å ±é…¬$r_t$ã‚’äºˆæ¸¬
- **Sim-to-real transfer**: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å­¦ç¿’ã—ãŸæ–¹ç­–ã‚’å®Ÿãƒ­ãƒœãƒƒãƒˆã«è»¢ç§»
- **Spatio-temporal masking**: å‹•ç”»ã®æ™‚ç©ºé–“ãƒ‘ãƒƒãƒã‚’ãƒã‚¹ã‚¯ã—ã¦äºˆæ¸¬
- **Symplectic integrator**: ãƒãƒŸãƒ«ãƒˆãƒ³ç³»ã®æ•°å€¤ç©åˆ†ã§æ§‹é€ ä¿å­˜ï¼ˆä¾‹: Verletæ³•ï¼‰
- **Transfusion**: ARï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰+ Diffusionï¼ˆç”»åƒï¼‰çµ±ä¸€ãƒ¢ãƒ‡ãƒ«
- **V-JEPA**: Video Joint-Embedding Predictive Architecture
- **VL-JEPA**: Vision-Language JEPA
- **World Model**: ç’°å¢ƒã®æ½œåœ¨æ§‹é€ ã‚’å­¦ç¿’ã—ã€æœªæ¥ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
- **ä¿å­˜å‰‡ (Conservation laws)**: ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»é‹å‹•é‡ç­‰ãŒæ™‚é–“å¤‰åŒ–ã—ãªã„ç‰©ç†æ³•å‰‡
:::

### 6.9 å®Ÿè£…æ¼”ç¿’ â€” æ®µéšåˆ¥ãƒãƒ£ãƒ¬ãƒ³ã‚¸

#### ãƒ¬ãƒ™ãƒ«1: åŸºç¤ï¼ˆ30åˆ†ï¼‰

**èª²é¡Œ1.1**: I-JEPAã®ãƒã‚¹ã‚­ãƒ³ã‚°é–¢æ•°ã‚’å®Ÿè£…ã›ã‚ˆ

```julia
"""
    generate_block_mask(H, W, n_blocks; block_size=4)

ç”»åƒã‚’ H x W ãƒ‘ãƒƒãƒã«åˆ†å‰²ã—ã€n_blocks å€‹ã®ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆå„ block_size x block_sizeï¼‰ã‚’ãƒã‚¹ã‚¯ã€‚

# Returns
- mask: BitArray [H, W] â€” 1=keep, 0=mask
"""
function generate_block_mask(H, W, n_blocks; block_size=4)
    # Your implementation here
    # Hint: ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ–ãƒ­ãƒƒã‚¯ã®å·¦ä¸Šåº§æ¨™ã‚’é¸ã³ã€block_size x block_size ã‚’ãƒã‚¹ã‚¯
end

# Test
mask = generate_block_mask(14, 14, 4, block_size=4)
@assert sum(mask) == 14*14 - 4*16  # 196 - 64 = 132 visible patches
```

**èª²é¡Œ1.2**: EMAæ›´æ–°é–¢æ•°ã®ãƒ†ã‚¹ãƒˆ

```julia
function test_ema_update()
    # Initialize two parameter sets
    Î¸_context = randn(Float32, 100)
    Î¸_target = copy(Î¸_context)

    # Simulate 10 updates
    for i in 1:10
        Î¸_context += 0.1 * randn(Float32, 100)  # Simulate gradient update
        Î¸_target = update_ema(Î¸_target, Î¸_context, Ï„=0.99)
    end

    # Verify: target should lag behind context
    @assert norm(Î¸_target - Î¸_context) > 0.01
    println("âœ… EMA update test passed")
end
```

#### ãƒ¬ãƒ™ãƒ«2: ä¸­ç´šï¼ˆ2æ™‚é–“ï¼‰

**èª²é¡Œ2.1**: V-JEPAã®spatio-temporal maskç”Ÿæˆ

**è¦ä»¶**:
- æ™‚é–“æ–¹å‘ã«ã‚‚é€£ç¶šã—ãŸãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒã‚¹ã‚¯ï¼ˆä¾‹: frame 2-4ã®ç‰¹å®šé ˜åŸŸï¼‰
- Mask ratio: 60%
- å°‘ãªãã¨ã‚‚1ãƒ•ãƒ¬ãƒ¼ãƒ ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ®‹ã™

```julia
function generate_spatiotemporal_mask(T, H, W, n_masks; temporal_span=2, spatial_size=4)
    mask = trues(T, H, W)
    # Your implementation
    # Hint: ãƒ©ãƒ³ãƒ€ãƒ ã«(t, h, w)ã‚’é¸ã³ã€temporal_span x spatial_size x spatial_size ã‚’ãƒã‚¹ã‚¯
    return mask
end

# Test
mask = generate_spatiotemporal_mask(8, 14, 14, 20, temporal_span=3, spatial_size=4)
visible_ratio = sum(mask) / length(mask)
@assert 0.35 < visible_ratio < 0.45  # ~40% visible
```

**èª²é¡Œ2.2**: Hamiltonian NN ã§å˜æŒ¯ã‚Šå­ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ

**ç‰©ç†**:
$$
H(q, p) = \frac{p^2}{2m} + mgl(1 - \cos q)
$$

ã“ã“ã§ $q$ = è§’åº¦ã€$p$ = è§’é‹å‹•é‡ã€$m$ = è³ªé‡ã€$g$ = é‡åŠ›åŠ é€Ÿåº¦ã€$l$ = é•·ã•ã€‚

```julia
using OrdinaryDiffEq, Plots

# True Hamiltonian (for data generation)
function pendulum_hamiltonian(q, p; m=1.0, g=9.8, l=1.0)
    return p^2 / (2 * m) + m * g * l * (1 - cos(q))
end

# Generate training data
function generate_pendulum_data(n_samples=1000, T=10.0)
    data = []
    for _ in 1:n_samples
        q0 = rand() * 2Ï€ - Ï€
        p0 = rand() * 2 - 1
        qp0 = [q0, p0]

        # Solve true dynamics
        prob = ODEProblem((qp, p, t) -> pendulum_dynamics(qp), qp0, (0.0, T))
        sol = solve(prob, Tsit5(), saveat=0.1)

        # Random pairs
        t1, t2 = sort(rand(1:length(sol), 2))
        push!(data, (sol[t1], sol[t2], sol.t[t2] - sol.t[t1]))
    end
    return data
end

function pendulum_dynamics(qp)
    q, p = qp
    dq = p  # âˆ‚H/âˆ‚p
    dp = -9.8 * sin(q)  # -âˆ‚H/âˆ‚q
    return [dq, dp]
end

# Train HNN and compare energy conservation
# Your task: Implement training loop from 6.2.3, train for 100 epochs, plot energy over time
```

#### ãƒ¬ãƒ™ãƒ«3: ä¸Šç´šï¼ˆ1æ—¥ï¼‰

**èª²é¡Œ3.1**: Transfusionã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨“ç·´å®Ÿè£…

**ãƒ‡ãƒ¼ã‚¿**:
- Text: WikiText-103 (GPT tokenized)
- Image: ImageNet-100 (100 classes subset)

**è¦ä»¶**:
- ãƒãƒƒãƒã”ã¨ã«ãƒ©ãƒ³ãƒ€ãƒ ã«text/imageã‚’é¸æŠï¼ˆ50:50ï¼‰
- Î»ãƒãƒ©ãƒ³ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•çš„èª¿æ•´
- 100 epochè¨“ç·´å¾Œã€text perplexityã¨image FIDã‚’è©•ä¾¡

```julia
# Skeleton
function train_transfusion_multimodal!(model, text_loader, image_loader, ps, st, config)
    opt = Adam(config["lr"])
    opt_st = Optimisers.setup(opt, ps)

    for epoch in 1:config["epochs"]
        for (text_batch, image_batch) in zip(text_loader, image_loader)
            # Randomly select modality (50% text, 50% image)
            if rand() < 0.5
                # Text forward + loss
                # ...
            else
                # Image forward + loss
                # ...
            end

            # Update
            grads = gradient(ps -> loss, ps)[1]
            opt_st, ps = Optimisers.update(opt_st, ps, grads)
        end

        # Evaluate
        text_ppl = evaluate_text_perplexity(model, text_val, ps, st)
        image_fid = evaluate_image_fid(model, image_val, ps, st)
        println("Epoch $epoch: Text PPL = $text_ppl, Image FID = $image_fid")
    end

    return ps, st
end
```

**èª²é¡Œ3.2**: Physics-Informed World Model ã§2ä½“å•é¡Œ

**ç‰©ç†**: 2ã¤ã®ç²’å­ãŒé‡åŠ›ã§ç›¸äº’ä½œç”¨

$$
F_{12} = -G \frac{m_1 m_2}{|\mathbf{r}_1 - \mathbf{r}_2|^3} (\mathbf{r}_1 - \mathbf{r}_2)
$$

**è¦ä»¶**:
- GNNã§å®Ÿè£…
- Newton's 3rd law + é‹å‹•é‡ä¿å­˜ + ã‚¨ãƒãƒ«ã‚®ãƒ¼ä¿å­˜ã‚’æ¤œè¨¼
- 1000ã‚¹ãƒ†ãƒƒãƒ—å¾Œã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ç›¸å¯¾èª¤å·® < 1%

```julia
struct TwoBodyGNN
    edge_mlp::Chain
    G::Float32  # Gravitational constant
end

function (m::TwoBodyGNN)(r1, r2, v1, v2, m1, m2, ps, st; Î”t=0.01)
    # Compute gravitational force
    r12 = r1 - r2
    dist = norm(r12) + 1e-6  # Avoid division by zero
    F_12 = -m.G * m1 * m2 / dist^3 * r12

    # Newton's 3rd law
    F_21 = -F_12

    # Update velocities
    v1_new = v1 + F_12 / m1 * Î”t
    v2_new = v2 + F_21 / m2 * Î”t

    # Update positions
    r1_new = r1 + v1_new * Î”t
    r2_new = r2 + v2_new * Î”t

    # Verify conservation laws
    # Total momentum: m1*v1 + m2*v2 = const
    # Total energy: KE + PE = const

    return r1_new, r2_new, v1_new, v2_new
end

# Your task: Train on simulated data, verify conservation over 10000 steps
```

### 6.10 å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨World Modelsã®ä½ç½®ã¥ã‘

æœ€å¾Œã«ã€Course I-IVã§å­¦ã‚“ã å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’World Modelsã®è¦–ç‚¹ã‹ã‚‰å†æ•´ç†ã™ã‚‹ã€‚

| ãƒ¢ãƒ‡ãƒ« | äºˆæ¸¬å¯¾è±¡ | æ¡ä»¶ | è¨“ç·´æ–¹å¼ | World Modelåº¦ |
|:------|:---------|:-----|:---------|:--------------|
| **VAE** | $p(x)$ | ãªã— | å°¤åº¦æœ€å¤§åŒ– | â˜…â˜†â˜†â˜†â˜† (é™çš„åˆ†å¸ƒã®ã¿) |
| **GAN** | $p(x)$ | ãªã— | æ•µå¯¾çš„ | â˜…â˜†â˜†â˜†â˜† |
| **Normalizing Flow** | $p(x)$ | ãªã— | å°¤åº¦æœ€å¤§åŒ– | â˜…â˜†â˜†â˜†â˜† |
| **Diffusion** | $p(x | c)$ | ãƒ†ã‚­ã‚¹ãƒˆç­‰ | ãƒã‚¤ã‚ºé™¤å» | â˜…â˜…â˜†â˜†â˜† (æ¡ä»¶ä»˜ãç”Ÿæˆ) |
| **Latent Diffusion** | $p(z | c)$ | ãƒ†ã‚­ã‚¹ãƒˆç­‰ | ãƒã‚¤ã‚ºé™¤å» | â˜…â˜…â˜†â˜†â˜† |
| **AR (GPT)** | $p(x_t | x_{<t})$ | éå»ç³»åˆ— | æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ | â˜…â˜…â˜…â˜†â˜† (æ™‚ç³»åˆ—æ§‹é€ ) |
| **I-JEPA** | $p(z_{\text{mask}} | z_{\text{ctx}})$ | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ | æ½œåœ¨äºˆæ¸¬ | â˜…â˜…â˜…â˜…â˜† (æ§‹é€ ç†è§£) |
| **V-JEPA** | $p(z_{t+1} | z_{\leq t})$ | éå»ãƒ•ãƒ¬ãƒ¼ãƒ  | æ½œåœ¨äºˆæ¸¬ | â˜…â˜…â˜…â˜…â˜… (æ™‚ç©ºé–“ç†è§£) |
| **Transfusion** | $p(x_t, \mathbf{x})$ | æ··åˆ | AR+Diffusion | â˜…â˜…â˜…â˜†â˜† |
| **Cosmos** | $p(x_{t+1} | x_t, a_t)$ | è¡Œå‹• | Flow+RL | â˜…â˜…â˜…â˜…â˜… (ç’°å¢ƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿) |
| **Genie** | $p(x_{t+1} | x_t, a_t)$ | è¡Œå‹• | Action discovery | â˜…â˜…â˜…â˜…â˜… |
| **Physics WM** | $p(x_{t+1} | x_t, a_t, \text{physics})$ | è¡Œå‹•+ç‰©ç† | PINNs | â˜…â˜…â˜…â˜…â˜… |

**é€²åŒ–ã®è»¸**:

1. **é™çš„ â†’ å‹•çš„**: $p(x)$ â†’ $p(x_t | x_{<t})$ â†’ $p(x_{t+1} | x_t, a_t)$
2. **ç”Ÿæˆ â†’ ç†è§£**: ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆ â†’ æ½œåœ¨äºˆæ¸¬ â†’ å› æœæ§‹é€ å­¦ç¿’
3. **ãƒ‡ãƒ¼ã‚¿é§†å‹• â†’ ç‰©ç†é§†å‹•**: Pure NN â†’ Physics-informed NN

**Course IVï¼ˆç¬¬33-42å›ï¼‰ã®æœ¬è³ª**:

> **ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯å˜ãªã‚‹ç”»åƒç”Ÿæˆãƒ„ãƒ¼ãƒ«ã§ã¯ãªãã€ç’°å¢ƒã®å› æœæ§‹é€ ã‚’å­¦ç¿’ã—ã€è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã™ã‚‹"World Models"ã¸ã®é€²åŒ–ã®é€”ä¸Šã«ã‚ã‚‹ã€‚**

### 6.11 çŸ¥è­˜ãƒãƒƒãƒ—

```mermaid
graph TD
    A[Course IV: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç·¨] --> B[ç¬¬41å›: World Models]

    B --> C[JEPAç†è«–]
    C --> C1[I-JEPA: ç”»åƒ]
    C --> C2[V-JEPA: å‹•ç”»]
    C --> C3[VL-JEPA: Vision-Language]

    B --> D[Transfusion]
    D --> D1[AR + Diffusionçµ±ä¸€]

    B --> E[ç‰©ç†æ³•å‰‡å­¦ç¿’]
    E --> E1[PINNs]
    E --> E2[ä¿å­˜å‰‡åŸ‹ã‚è¾¼ã¿]
    E --> E3[Hamiltonian NN]

    B --> F[Energy-based WM]
    F --> F1[ç¬¬34å›EBMã¨ã®æ¥ç¶š]

    B --> G[å¿œç”¨]
    G --> G1[ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹]
    G --> G2[è‡ªå‹•é‹è»¢]
    G --> G3[å¼·åŒ–å­¦ç¿’]
    G --> G4[ç§‘å­¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]

    style B fill:#ff9,stroke:#333,stroke-width:4px
```

:::message
**é€²æ—**: å…¨ä½“ã®95%å®Œäº†ã€‚World Modelsãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®å…¨ä½“åƒã‚’æŠŠæ¡ã—ã€æœ€æ–°ç ”ç©¶ï¼ˆCosmos/Genie/Physics-Informedï¼‰ã®ä½ç½®ã¥ã‘ã‚’ç†è§£ã—ãŸã€‚Embodied AIãƒ»ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã¸ã®æ¥ç¶šãŒè¦‹ãˆãŸã€‚
:::

---

### 6.6 ç¬¬41å›ã®æ ¸å¿ƒ

1. **World Models = ç”Ÿæˆã®å…ˆ**
   ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çœŸã®ç›®çš„ã¯ã€Œç’°å¢ƒã®å› æœæ§‹é€ ã‚’ç†è§£ã—ã€è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã€ã ã£ãŸã€‚

2. **JEPAç†è«–ã®é©å‘½**
   I-JEPA/V-JEPA/VL-JEPAã¯**ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—**ã—ã€æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬ã™ã‚‹ã€‚

3. **Transfusionçµ±ä¸€**
   ãƒ†ã‚­ã‚¹ãƒˆï¼ˆARï¼‰ã¨ç”»åƒï¼ˆDiffusionï¼‰ã‚’å˜ä¸€Transformerã§çµ±åˆã€‚

4. **ç‰©ç†æ³•å‰‡å­¦ç¿’**
   PINNsãƒ»ä¿å­˜å‰‡ãƒ»Hamiltonian NNã§ç‰©ç†çš„ã«ä¸€è²«ã—ãŸäºˆæ¸¬ã‚’å®Ÿç¾ã€‚

5. **Energy-basedè¦–ç‚¹**
   World Modelsã‚’$p(z_{t+1}|z_t, a_t) \propto \exp(-E_\theta)$ã§å®šå¼åŒ–ã€‚

### 6.7 FAQ

:::details Q1: JEPAã¯ãªãœãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã®ã‹ï¼Ÿ
**A**: ãƒ”ã‚¯ã‚»ãƒ«å†æ§‹æˆã¯ä½ãƒ¬ãƒ™ãƒ«è©³ç´°ï¼ˆãƒ†ã‚¯ã‚¹ãƒãƒ£ã€è‰²ï¼‰ã«éå‰°é©åˆã—ã€é«˜ãƒ¬ãƒ™ãƒ«æŠ½è±¡è¡¨ç¾ï¼ˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€å‹•ãã€å› æœé–¢ä¿‚ï¼‰ã‚’å­¦ç¿’ã—ã«ãã„ã€‚JEPAã¯æ½œåœ¨ç©ºé–“ã§äºˆæ¸¬ã™ã‚‹ã“ã¨ã§ã€æ§‹é€ çš„ãƒ»æ„å‘³çš„è¡¨ç¾ã‚’å„ªå…ˆçš„ã«å­¦ç¿’ã™ã‚‹ã€‚
:::

:::details Q2: Transfusionã¯ãªãœVQ-VAEã‚’ä½¿ã‚ãªã„ã®ã‹ï¼Ÿ
**A**: VQ-VAEã¯ç”»åƒã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«é‡å­åŒ–ã™ã‚‹ãŒã€é‡å­åŒ–èª¤å·®ã¨ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯åˆ©ç”¨ç‡ä½ä¸‹ã®å•é¡ŒãŒã‚ã‚‹ã€‚Transfusionã¯ç”»åƒã‚’**é€£ç¶šãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿**ã¨ã—ã¦æ‰±ã„ã€Diffusion lossã§è¨“ç·´ã™ã‚‹ã“ã¨ã§ã€æƒ…å ±æå¤±ã‚’å›é¿ã™ã‚‹ã€‚
:::

:::details Q3: Physics-Informed World Modelsã®å®Ÿç”¨æ€§ã¯ï¼Ÿ
**A**: ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ°—å€™ãƒ»æµä½“ãƒ»åˆ†å­å‹•åŠ›å­¦ï¼‰ã§ã¯é«˜ã„å®Ÿç”¨æ€§ã€‚ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã§ã‚‚ã€ç‰©ç†æ³•å‰‡ã‚’çŸ¥ã£ã¦ã„ã‚Œã°å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’å¯èƒ½ã€‚ãŸã ã—ã€è¤‡é›‘ãªå®Ÿä¸–ç•Œï¼ˆæ­©è¡Œè€…ã®è¡Œå‹•äºˆæ¸¬ãªã©ï¼‰ã§ã¯ç‰©ç†æ³•å‰‡ã ã‘ã§ã¯ä¸ååˆ†ã€‚
:::

:::details Q4: World Modelsã¨Diffusion Modelsã®é•ã„ã¯ï¼Ÿ
**A**:

| | Diffusion | World Models |
|:---|:---------|:-------------|
| **ç›®çš„** | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ$p(x)$ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ« | æ¬¡çŠ¶æ…‹$p(x_{t+1}|x_{\leq t}, a_t)$ã‚’äºˆæ¸¬ |
| **è¨“ç·´** | ãƒã‚¤ã‚ºé™¤å» | çŠ¶æ…‹é·ç§»å­¦ç¿’ |
| **æ¡ä»¶** | ãƒ†ã‚­ã‚¹ãƒˆç­‰ï¼ˆé™çš„ï¼‰ | è¡Œå‹•$a_t$ï¼ˆå‹•çš„ï¼‰ |
| **å¿œç”¨** | ç”»åƒç”Ÿæˆ | ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€å¼·åŒ–å­¦ç¿’ |

World Modelsã¯Diffusionã®ä¸Šä½æ¦‚å¿µã§ã€ç’°å¢ƒã®å› æœæ§‹é€ ã‚’ç†è§£ã™ã‚‹ã€‚
:::

:::details Q5: Embodied AIã¸ã®æ¥ç¶šã¯ï¼Ÿ
**A**: World Modelsã¯**è¡Œå‹•ã®çµæœã‚’äºˆæ¸¬**ã§ãã‚‹ãŸã‚ã€ãƒ­ãƒœãƒƒãƒˆã®åˆ¶å¾¡ã«ç›´çµã™ã‚‹ã€‚

1. **Perception**: è¦³æ¸¬$x_t$â†’æ½œåœ¨$z_t$
2. **Planning**: World Modelã§è¤‡æ•°ã®è¡Œå‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
3. **Control**: æœ€è‰¯ã®è¡Œå‹•ã‚’é¸æŠ

ç¬¬47å›ï¼ˆCourse Vï¼‰ã§ã€ŒDiffusion Policyã€ã¨ã—ã¦ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹å¿œç”¨ã‚’å®Œå…¨å®Ÿè£…ã™ã‚‹ã€‚
:::

### 6.8 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆ1é€±é–“ãƒ—ãƒ©ãƒ³ï¼‰

| æ—¥ | ã‚¿ã‚¹ã‚¯ | æ™‚é–“ |
|:---|:------|:-----|
| **Day 1** | Zone 0-2: å‹•æ©Ÿç†è§£ + ä½“é¨“ | 1h |
| **Day 2-3** | Zone 3.1-3.3: JEPAç†è«–å®Œå…¨ç¿’å¾— | 3h |
| **Day 4** | Zone 3.4-3.7: ç‰©ç†æ³•å‰‡å­¦ç¿’+è¨“ç·´ç†è«– | 2h |
| **Day 5** | Zone 4: Julia JEPAå®Ÿè£… | 2h |
| **Day 6** | Zone 5: å®Ÿé¨“+ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ | 1.5h |
| **Day 7** | Zone 6-7: æœ€æ–°ç ”ç©¶+å¾©ç¿’ | 1h |

### 6.9 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```julia
# è‡ªå·±è©•ä¾¡ï¼ˆ0-10ï¼‰
scores = Dict(
    "World Modelsã®å®šç¾©ç†è§£" => 0,
    "JEPAç†è«–ã®æ•°å­¦çš„å°å‡º" => 0,
    "Transfusionçµ±ä¸€ç†è«–" => 0,
    "ç‰©ç†æ³•å‰‡å­¦ç¿’ã®åŸç†" => 0,
    "JEPAã‚³ãƒ³ã‚»ãƒ—ãƒˆå®Ÿè£…" => 0,
    "è«–æ–‡èª­è§£ã‚¹ã‚­ãƒ«" => 0,
    "Embodied AIæ¥ç¶šç†è§£" => 0
)

# ç›®æ¨™: å„é …ç›® â‰¥ 7
total = sum(values(scores))
progress = total / 70 * 100
println("ç·åˆé€²æ—: $(round(progress, digits=1))%")

if all(v >= 7 for v in values(scores))
    println("âœ… ç¬¬41å›ã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆï¼ç¬¬42å›ï¼ˆçµ±ä¸€ç†è«–ï¼‰ã¸é€²ã‚")
else
    println("âš ï¸ 7æœªæº€ã®é …ç›®ã‚’å¾©ç¿’ã›ã‚ˆ")
end
```

### 6.10 æ¬¡å›äºˆå‘Š: ç¬¬42å› å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«çµ±ä¸€ç†è«– + Course IVç·æ‹¬

ç¬¬40å›ã§Consistency Modelsã«ã‚ˆã‚‹1ã‚¹ãƒ†ãƒƒãƒ—é«˜é€Ÿç”Ÿæˆã‚’å®Ÿç¾ã—ãŸã€‚ç¬¬41å›ã§ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹ã€ŒWorld Models â€” ç’°å¢ƒç†è§£+äºˆæ¸¬+ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ã«åˆ°é”ã—ãŸã€‚

ã ãŒã€å…¨50è¬›ç¾©ãƒ»5ã‚³ãƒ¼ã‚¹ã§å­¦ã‚“ã å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆVAE/Flow/GAN/Diffusion/AR/World Modelsï¼‰ã‚’**çµ±ä¸€çš„ã«æ•´ç†**ã™ã‚‹æ™‚ãŒæ¥ãŸã€‚

**ç¬¬42å›ã®æ ¸å¿ƒ**:

1. **ãƒ‘ãƒ¼ãƒˆA: å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®çµ±ä¸€çš„åˆ†é¡**
   4ã¤ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ï¼ˆå°¤åº¦ãƒ»æš—é»™çš„ãƒ»ã‚¹ã‚³ã‚¢ãƒ»Flowï¼‰ã§æ•´ç†

2. **ãƒ‘ãƒ¼ãƒˆB: æ•°å­¦çš„ç­‰ä¾¡æ€§**
   Score â†” Flow â†” Diffusion â†” OT â†” EBM â†” World Modelsã®æ•°å­¦çš„ç­‰ä¾¡æ€§è¨¼æ˜

3. **ãƒ‘ãƒ¼ãƒˆC: Course IVç·æ‹¬**
   NFâ†’EBMâ†’Scoreâ†’DDPMâ†’SDEâ†’FMâ†’LDMâ†’Consistencyâ†’World Modelsâ†’çµ±ä¸€ç†è«– 10å›ã®é›†å¤§æˆ

**åˆ°é”ç‚¹**: ã€Œå…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯æœ¬è³ªçš„ã«åŒã˜ã‚‚ã®ã®ç•°ãªã‚‹è¦–ç‚¹ã ã£ãŸã€

:::message
**é€²æ—**: ğŸ† **å…¨ä½“ã®100%å®Œäº†ï¼ç¬¬41å›èª­äº†ï¼**

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚åˆ°é”ç‚¹ã€ŒWorld Modelsã€ã‚’å®Œå…¨ç†è§£ã—ãŸã€‚ãƒ”ã‚¯ã‚»ãƒ«ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ½œåœ¨ç©ºé–“ã§ç’°å¢ƒã®å› æœæ§‹é€ ã‚’å­¦ç¿’ã™ã‚‹JEPAç†è«–ã€ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒã‚’çµ±ä¸€ã™ã‚‹Transfusionç†è«–ã€ç‰©ç†æ³•å‰‡ã‚’åŸ‹ã‚è¾¼ã‚€Physics-Informedç†è«–ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°è¦–ç‚¹ã€è¨“ç·´ãƒ»è©•ä¾¡æ‰‹æ³•ã‚’å®Œå…¨ç¿’å¾—ã—ãŸã€‚

æ¬¡å›ã€å…¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€ç†è«–ã¸ã€‚
:::

---

## ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ¬å½“ã®ç›®çš„ã¯"ç†è§£"ã ã£ãŸã®ã§ã¯ï¼Ÿ**

ç¬¬1å›ã‹ã‚‰40å›ã¾ã§ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€Œãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ$p(x)$ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€æŠ€è¡“ã¨ã—ã¦å­¦ã‚“ã§ããŸã€‚

VAE: æ½œåœ¨å¤‰æ•°$z$ã‹ã‚‰ç”Ÿæˆ
GAN: æ•µå¯¾çš„å­¦ç¿’ã§ç”Ÿæˆ
Diffusion: ãƒã‚¤ã‚ºé™¤å»ã§ç”Ÿæˆ
Consistency Models: 1ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆ

ã ãŒWorld Modelsã¯**ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹**ã€‚

JEPAã¯ãƒ”ã‚¯ã‚»ãƒ«ã‚’å†æ§‹æˆã›ãšã€æ½œåœ¨è¡¨ç¾ã‚’äºˆæ¸¬ã™ã‚‹ã€‚
ç›®çš„ã¯ç”»åƒã‚’ä½œã‚‹ã“ã¨ã§ã¯ãªãã€**ç’°å¢ƒã®æ§‹é€ ã‚’ç†è§£ã™ã‚‹ã“ã¨**ã ã€‚

**è­°è«–ãƒã‚¤ãƒ³ãƒˆ**:

1. **ç”Ÿæˆã¯å‰¯ç”£ç‰©ã‹ï¼Ÿ**
   ç”»åƒç”Ÿæˆã¯ã€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’ç†è§£ã—ãŸ"å‰¯ä½œç”¨"ã«éããªã„ã®ã‹ï¼Ÿ

2. **ç†è§£ã¨ã¯ä½•ã‹ï¼Ÿ**
   ã€Œç’°å¢ƒã‚’ç†è§£ã—ã¦ã„ã‚‹ã€ã¨ã¯ã€æ•°å­¦çš„ã«ã©ã†å®šç¾©ã•ã‚Œã‚‹ã‹ï¼Ÿ
   â†’ $p(x_{t+1}|x_{\leq t}, a_t)$ã‚’æ­£ç¢ºã«äºˆæ¸¬ã§ãã‚‹ã“ã¨ï¼Ÿ

3. **AGIã¸ã®é“**
   World ModelsãŒç’°å¢ƒã®å› æœæ§‹é€ ã‚’å®Œå…¨ã«å­¦ç¿’ã™ã‚Œã°ã€ãã‚Œã¯AGIï¼ˆæ±ç”¨äººå·¥çŸ¥èƒ½ï¼‰ã¨å‘¼ã¹ã‚‹ã‹ï¼Ÿ

:::details æ­´å²çš„æ–‡è„ˆ â€” Yann LeCunã®æŒ‘æˆ¦

**Yann LeCun** (Meta AI Chief Scientist, Turing Award 2018)ã¯ã€é•·å¹´ã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯éåŠ¹ç‡ã€ã¨æ‰¹åˆ¤ã—ã¦ããŸã€‚

**å½¼ã®ä¸»å¼µ**:

- ãƒ”ã‚¯ã‚»ãƒ«å†æ§‹æˆã¯ä½ãƒ¬ãƒ™ãƒ«è©³ç´°ã«éå‰°é©åˆ
- èµ¤ã¡ã‚ƒã‚“ã¯æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¦³å¯Ÿã§ç‰©ä½“ã®3Dæ§‹é€ ã‚’ç†è§£ã™ã‚‹
- äººé–“ã®å­¦ç¿’ã¯**äºˆæ¸¬**ã§ã‚ã‚Šã€**ç”Ÿæˆ**ã§ã¯ãªã„

**JEPAã®èª•ç”Ÿ**:

2023å¹´ã€LeCunã¯I-JEPAã‚’ç™ºè¡¨ã—ã€ã€ŒSelf-Supervised Learning from Images with a Joint-Embedding Predictive Architectureã€ï¼ˆCVPR 2023ï¼‰ã§å®Ÿè¨¼ã—ãŸã€‚

> "The future of AI is not to generate pixels, but to predict abstract representations."
> â€” Yann LeCun, 2023

**åéŸ¿**:

- OpenAI/Google: Diffusion Modelsã§ç”»åƒç”Ÿæˆã‚’æ¥µã‚ã‚‹
- Meta AI: JEPAã§ã€Œç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã€ã™ã‚‹åˆ¥è§£ã‚’æç¤º

2024å¹´ã€V-JEPAï¼ˆå‹•ç”»ï¼‰ã€2024å¹´æœ«ã€VL-JEPAï¼ˆVision-Languageï¼‰ãŒç™»å ´ã€‚

**2025å¹´**:

- NVIDIA Cosmos: ç‰©ç†AIã®World Model
- DeepMind Genie 3: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ç’°å¢ƒç”Ÿæˆ

World Modelsã¯**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’è¶…ãˆãŸå…ˆ**ã®æ¦‚å¿µã¨ã—ã¦ç¢ºç«‹ã•ã‚ŒãŸã€‚
:::

---

## å‚è€ƒæ–‡çŒ®


### æ•™ç§‘æ›¸

- Pearl, J. (2009). ***Causality: Models, Reasoning, and Inference*** (2nd ed.). Cambridge University Press. â€” å› æœæ¨è«–ã®åŸºç¤ç†è«–
- Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., & Yang, L. (2021). **Physics-informed machine learning**. *Nature Reviews Physics, 3*(6), 422-440. â€” PINNsã‚µãƒ¼ãƒ™ã‚¤
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). ***Deep Learning***. MIT Press. â€” æ·±å±¤å­¦ç¿’ã®æ¨™æº–æ•™ç§‘æ›¸

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| $x_t$ | æ™‚åˆ»$t$ã®è¦³æ¸¬ | $x_0, x_1, \ldots, x_T$ |
| $z_t$ | æ™‚åˆ»$t$ã®æ½œåœ¨çŠ¶æ…‹ | $z_t \in \mathbb{R}^d$ |
| $a_t$ | æ™‚åˆ»$t$ã®action | $a_t \in \mathbb{R}^{d_a}$ |
| $f_\theta$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$\theta$ã®é·ç§»é–¢æ•° | $z_{t+1} = f_\theta(z_t, a_t)$ |
| $s_\theta$ | Context encoder | $z_{\text{ctx}} = s_\theta(x_{\text{ctx}})$ |
| $\bar{s}_\theta$ | Target encoder (EMA) | EMAæ›´æ–° |
| $M$ | Mask tokens | Positional encoding for masked regions |
| $\epsilon_\theta$ | Diffusion noise predictor | $\epsilon_\theta(\mathbf{x}_t, t, \mathbf{c})$ |
| $E_\theta$ | Energy function | $p(z) \propto \exp(-E_\theta(z))$ |
| $H(q, p)$ | Hamiltonian | ç·ã‚¨ãƒãƒ«ã‚®ãƒ¼ |
| $\mathcal{L}_{\text{JEPA}}$ | JEPAæå¤± | MSE in latent space |
| $\mathcal{L}_{\text{Transfusion}}$ | Transfusionæå¤± | $\mathcal{L}_{\text{LM}} + \lambda \mathcal{L}_{\text{Diffusion}}$ |
---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
