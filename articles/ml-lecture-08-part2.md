---
title: "ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³•: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ”"
type: "tech"
topics: ["machinelearning", "deeplearning", "statistics", "python"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” EMã®å®Ÿè·µçš„ã‚¹ã‚­ãƒ«

### 4.1 å®Ÿè£…ã®å…¨ä½“è¨­è¨ˆ

Zone 3ã§å°å‡ºã—ãŸæ•°å¼ã‚’ã€å®Ÿè·µçš„ãªã‚³ãƒ¼ãƒ‰ã«è½ã¨ã—è¾¼ã‚€ã€‚ã¾ãšå…¨ä½“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç¢ºèªã—ã‚ˆã†ã€‚

```mermaid
graph LR
    A["ãƒ‡ãƒ¼ã‚¿ X<br/>(N, D)"] --> B["åˆæœŸåŒ–<br/>K-means++"]
    B --> C["E-step<br/>è²¬ä»»åº¦Î³è¨ˆç®—"]
    C --> D["M-step<br/>Î¼, Î£, Ï€æ›´æ–°"]
    D --> E{"åæŸåˆ¤å®š<br/>|Î”log-lik| < Îµ?"}
    E -->|No| C
    E -->|Yes| F["çµæœ<br/>Î¸*, Î³*"]
    F --> G["ãƒ¢ãƒ‡ãƒ«é¸æŠ<br/>BIC/AIC"]

    style B fill:#e3f2fd
    style E fill:#fff3e0
    style G fill:#c8e6c9
```

### 4.2 æ•°å€¤å®‰å®šæ€§ â€” log-sum-exp ãƒˆãƒªãƒƒã‚¯

GMMã®å®Ÿè£…ã§æœ€ã‚‚å±é™ºãªã®ã¯ **æ•°å€¤ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼** ã ã€‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æŒ‡æ•°é–¢æ•° $\exp(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu}))$ ã¯ã€ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ãŒå¤§ãã„ã¨å®¹æ˜“ã« $10^{-300}$ ä»¥ä¸‹ã«ãªã‚‹ã€‚

**è§£æ±ºç­–: log-sum-exp ãƒˆãƒªãƒƒã‚¯**

$$
\log \sum_k \exp(a_k) = \max_k a_k + \log \sum_k \exp(a_k - \max_k a_k)
$$

```python
import numpy as np

def log_sum_exp(log_vals):
    """Numerically stable log-sum-exp.

    log Î£_k exp(a_k) = max(a) + log Î£_k exp(a_k - max(a))
    """
    max_val = np.max(log_vals, axis=-1, keepdims=True)
    return max_val.squeeze(-1) + np.log(np.sum(np.exp(log_vals - max_val), axis=-1))

# Without log-sum-exp: underflow
large_negative = np.array([-800, -810, -820])
print(f"Naive sum of exp: {np.sum(np.exp(large_negative))}")  # 0.0 (underflow!)

# With log-sum-exp: correct
result = log_sum_exp(large_negative)
print(f"Log-sum-exp:      {result:.4f}")  # correct value
print(f"Verification:     {np.log(np.exp(-800)*(1 + np.exp(-10) + np.exp(-20))):.4f}")  # same

# Application to GMM responsibilities
def e_step_stable(X, mus, covs, pis):
    """Numerically stable E-step using log-sum-exp.

    Î³(z_nk) = exp(log Ï€_k + log N(x_n|Î¼_k,Î£_k) - log Î£_j exp(log Ï€_j + log N(x_n|Î¼_j,Î£_j)))
    """
    N, D = X.shape
    K = len(mus)
    log_resp = np.zeros((N, K))

    for k in range(K):
        diff = X - mus[k]  # (N, D)
        cov_inv = np.linalg.inv(covs[k])
        log_det = np.log(np.linalg.det(covs[k]) + 1e-300)

        # log N(x_n|Î¼_k,Î£_k) = -D/2 log(2Ï€) - 1/2 log|Î£_k| - 1/2 (x-Î¼)^T Î£^{-1} (x-Î¼)
        mahal = np.sum(diff @ cov_inv * diff, axis=1)  # (N,)
        log_resp[:, k] = np.log(pis[k] + 1e-300) - 0.5 * D * np.log(2*np.pi) - 0.5 * log_det - 0.5 * mahal

    # Stable softmax over components
    log_sum = log_sum_exp(log_resp)  # (N,)
    log_gamma = log_resp - log_sum[:, np.newaxis]
    gamma = np.exp(log_gamma)

    return gamma, log_sum.sum()  # responsibilities and log-likelihood

# Test
np.random.seed(42)
X = np.random.randn(100, 2) * 3
mus = [np.array([0, 0]), np.array([5, 5])]
covs = [np.eye(2), np.eye(2)*2]
pis = np.array([0.5, 0.5])

gamma, ll = e_step_stable(X, mus, covs, pis)
print(f"\nStable E-step: log-lik = {ll:.4f}")
print(f"Î³ sum per row (should be 1): {gamma.sum(axis=1)[:5].round(6)}")
```

### 4.3 K-means++ åˆæœŸåŒ–

EMç®—æ³•ã¯åˆæœŸå€¤ã«ä¾å­˜ã™ã‚‹ã€‚æ‚ªã„åˆæœŸå€¤ã¯åæŸã®é…å»¶ã‚„å±€æ‰€æœ€é©è§£ã¸ã®åæŸã‚’å¼•ãèµ·ã“ã™ã€‚K-means++ [^6] ã¯åˆæœŸå€¤é¸æŠã®æ¨™æº–æ‰‹æ³•ã ã€‚

```python
import numpy as np

def kmeans_plus_plus_init(X, K, seed=42):
    """K-means++ initialization for GMM.

    1. Choose first center uniformly at random
    2. For each subsequent center:
       - Compute D(x) = distance to nearest existing center
       - Choose next center with probability proportional to D(x)Â²
    """
    rng = np.random.RandomState(seed)
    N, D = X.shape
    centers = []

    # First center: uniform random
    idx = rng.randint(N)
    centers.append(X[idx].copy())

    for _ in range(1, K):
        # Distance to nearest center
        dists = np.array([np.min([np.sum((x - c)**2) for c in centers]) for x in X])
        # Probability proportional to D(x)Â²
        probs = dists / dists.sum()
        idx = rng.choice(N, p=probs)
        centers.append(X[idx].copy())

    return np.array(centers)

# Demonstrate K-means++ vs random init
np.random.seed(42)
N = 300
X = np.vstack([np.random.randn(100, 2) + [-5, -5],
               np.random.randn(100, 2) + [0, 5],
               np.random.randn(100, 2) + [5, -3]])

centers_kpp = kmeans_plus_plus_init(X, 3)
centers_random = X[np.random.choice(N, 3, replace=False)]

print("K-means++ centers:")
for i, c in enumerate(centers_kpp):
    print(f"  Center {i}: ({c[0]:6.2f}, {c[1]:6.2f})")

print("\nRandom centers:")
for i, c in enumerate(centers_random):
    print(f"  Center {i}: ({c[0]:6.2f}, {c[1]:6.2f})")

print("\nTrue centers: (-5,-5), (0,5), (5,-3)")
print("K-means++ typically provides much better coverage.")
```

### 4.4 ãƒ¢ãƒ‡ãƒ«é¸æŠ â€” BIC ã¨ AIC

æˆåˆ†æ•° $K$ ã‚’ã©ã†æ±ºã‚ã‚‹ã‹ï¼Ÿãƒ‡ãƒ¼ã‚¿ã‚’æœ€ã‚‚ã‚ˆãèª¬æ˜ã™ã‚‹ $K$ ã‚’é¸ã³ãŸã„ãŒã€$K$ ã‚’å¢—ã‚„ã›ã°å°¤åº¦ã¯å¸¸ã«ä¸ŠãŒã‚‹ï¼ˆéå­¦ç¿’ï¼‰ã€‚**BIC** (Bayesian Information Criterion) ã¨ **AIC** (Akaike Information Criterion) ãŒã“ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ã€‚

$$
\text{BIC} = -2 \log p(\mathbf{x} \mid \hat{\theta}) + d \log N
$$

$$
\text{AIC} = -2 \log p(\mathbf{x} \mid \hat{\theta}) + 2d
$$

ã“ã“ã§ $d$ ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€‚GMMã®å ´åˆ $d = K(D + D(D+1)/2 + 1) - 1$ï¼ˆå¹³å‡ + å…±åˆ†æ•£ + æ··åˆé‡ã¿ã§ã€åˆ¶ç´„ã‚’å¼•ãï¼‰ã€‚1æ¬¡å…ƒãªã‚‰ $d = 3K - 1$ã€‚

| åŸºæº– | ãƒšãƒŠãƒ«ãƒ†ã‚£ | å‚¾å‘ |
|:-----|:---------|:-----|
| BIC | $d \log N$ (ãƒ‡ãƒ¼ã‚¿æ•°ã«ä¾å­˜) | ã‚ˆã‚Šå°‘ãªã„ $K$ ã‚’é¸ã³ã‚„ã™ã„ï¼ˆä¿å®ˆçš„ï¼‰ |
| AIC | $2d$ (ãƒ‡ãƒ¼ã‚¿æ•°ã«ä¾å­˜ã—ãªã„) | BICã‚ˆã‚Šå¤§ãã„ $K$ ã‚’é¸ã³ã‚„ã™ã„ |

```python
import numpy as np

def run_em_gmm_1d(x, K, max_iter=100, tol=1e-6, seed=42):
    """Run EM for 1D GMM with K components. Return log-likelihood and params."""
    rng = np.random.RandomState(seed)
    N = len(x)

    # K-means++ init
    mu = np.sort(rng.choice(x, K, replace=False))
    sigma = np.ones(K) * x.std()
    pi_k = np.ones(K) / K

    prev_ll = -np.inf
    for _ in range(max_iter):
        # E-step
        pdf = np.zeros((N, K))
        for k in range(K):
            pdf[:, k] = pi_k[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
        total = pdf.sum(axis=1, keepdims=True)
        gamma = pdf / (total + 1e-300)

        ll = np.sum(np.log(total.squeeze() + 1e-300))
        if abs(ll - prev_ll) < tol:
            break
        prev_ll = ll

        # M-step
        N_k = gamma.sum(axis=0)
        for k in range(K):
            mu[k] = (gamma[:, k] * x).sum() / (N_k[k] + 1e-300)
            sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / (N_k[k] + 1e-300)) + 1e-6
        pi_k = N_k / N

    return ll, mu, sigma, pi_k

# Generate data from K=3 components
np.random.seed(42)
x = np.concatenate([np.random.normal(-3, 0.8, 100),
                     np.random.normal(0, 1.0, 150),
                     np.random.normal(4, 0.6, 100)])
N = len(x)

print(f"{'K':>3} | {'log-lik':>10} | {'d (params)':>10} | {'BIC':>10} | {'AIC':>10}")
print("-" * 55)

bic_values = []
for K in range(1, 8):
    ll, mu, sigma, pi_k = run_em_gmm_1d(x, K)
    d = 3 * K - 1  # parameters: K means + K variances + (K-1) weights
    bic = -2 * ll + d * np.log(N)
    aic = -2 * ll + 2 * d
    bic_values.append(bic)
    marker = " â† best" if K == np.argmin(bic_values) + 1 and K > 1 else ""
    print(f"{K:3d} | {ll:10.2f} | {d:10d} | {bic:10.2f} | {aic:10.2f}{marker}")

best_K = np.argmin(bic_values) + 1
print(f"\nBIC selects K = {best_K} (true K = 3)")
```

### 4.5 Singularityå•é¡Œã¨å¯¾ç­–

GMMã®é‡å¤§ãªè½ã¨ã—ç©´: ã‚ã‚‹æˆåˆ†ãŒãƒ‡ãƒ¼ã‚¿1ç‚¹ã«ã€Œå´©å£Šã€ã™ã‚‹ã¨ $\sigma_k \to 0$ã€å°¤åº¦ãŒ $\to \infty$ ã«ç™ºæ•£ã™ã‚‹ã€‚

```python
import numpy as np

# Singularity demonstration
# If Î¼_k = x_n for some n, and Ïƒ_k â†’ 0:
# N(x_n|Î¼_k,Ïƒ_kÂ²) = 1/(Ïƒ_kâˆš2Ï€) â†’ âˆ

print("Singularity problem: when Ïƒ â†’ 0 for one component")
for sigma in [1.0, 0.1, 0.01, 0.001, 1e-6, 1e-10]:
    pdf = 1.0 / (sigma * np.sqrt(2 * np.pi))
    print(f"  Ïƒ = {sigma:.1e}  â†’  N(0|0,ÏƒÂ²) = {pdf:.6e}")
print("\nAs Ïƒ â†’ 0, the density â†’ âˆ (singularity!)")

# Standard fixes
print("\n=== Countermeasures ===")
print("1. Floor on variance: Ïƒ_kÂ² â‰¥ Îµ (e.g., Îµ = 1e-6)")
print("2. Regularization: Î£_k â†’ Î£_k + Î»I")
print("3. MAP estimation: Wishart prior on Î£_k")
print("4. Drop degenerate components: if N_k < threshold, remove component k")

# Implementation of variance floor
def m_step_with_floor(x, gamma, eps=1e-6):
    """M-step with variance floor to prevent singularity."""
    N_k = gamma.sum(axis=0)
    K = gamma.shape[1]
    mu = np.zeros(K)
    sigma = np.zeros(K)

    for k in range(K):
        mu[k] = (gamma[:, k] * x).sum() / (N_k[k] + 1e-300)
        var_k = (gamma[:, k] * (x - mu[k])**2).sum() / (N_k[k] + 1e-300)
        sigma[k] = np.sqrt(max(var_k, eps))  # Floor!

    pi_k = N_k / len(x)
    return mu, sigma, pi_k

print("\nVariance floor prevents Ïƒ â†’ 0 and keeps log-likelihood finite.")
```

### 4.6 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | æ•°å¼ | Python | èª¬æ˜ |
|:---------|:-----|:-------|:-----|
| è²¬ä»»åº¦ | $\gamma_{nk} = \frac{\pi_k f_k}{\sum_j \pi_j f_j}$ | `gamma = pdf / pdf.sum(axis=1, keepdims=True)` | è¡Œã”ã¨ã®æ­£è¦åŒ– |
| é‡ã¿ä»˜ãå¹³å‡ | $\frac{\sum_n w_n x_n}{\sum_n w_n}$ | `(w * x).sum() / w.sum()` | ãƒ™ã‚¯ãƒˆãƒ«åŒ– |
| å¯¾æ•°ã‚¬ã‚¦ã‚¹ | $-\frac{D}{2}\log 2\pi - \frac{1}{2}\log|\Sigma|$ | `-0.5*D*np.log(2*np.pi) - 0.5*np.linalg.slogdet(cov)[1]` | slogdet ã§å®‰å®š |
| ãƒãƒãƒ©ãƒãƒ“ã‚¹ | $(\mathbf{x}-\boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{x}-\boldsymbol{\mu})$ | `diff @ np.linalg.solve(cov, diff)` | solve > inv |
| log-sum-exp | $\log \sum_k e^{a_k}$ | `max(a) + np.log(np.sum(np.exp(a - max(a))))` | ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢ |
| è¡Œåˆ—å¼å¯¾æ•° | $\log |\Sigma|$ | `np.linalg.slogdet(cov)[1]` | ç›´æ¥è¨ˆç®—ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å›é¿ |
| å¯¾è§’å…±åˆ†æ•£ | $\text{diag}(\sigma_1^2, \ldots, \sigma_D^2)$ | `np.diag(sigma**2)` | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å‰Šæ¸› |

:::details è«–æ–‡èª­è§£ã‚¬ã‚¤ãƒ‰ â€” Dempster, Laird, Rubin (1977) ã‚’èª­ã‚€
EMç®—æ³•ã®åŸè«–æ–‡ [^1] ã¯50ãƒšãƒ¼ã‚¸è¿‘ã„å¤§ä½œã ãŒã€æ§‹é€ ã‚’çŸ¥ã£ã¦ã„ã‚Œã°èª­ã‚ã‚‹ã€‚

**3ãƒ‘ã‚¹ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**:

**Pass 1** (10åˆ†): Abstract â†’ Section 1 (Introduction) â†’ Section 2 ã®å®šç†æ–‡ â†’ Section 8 (Examples) ã® GMM éƒ¨åˆ†
```python
pass1_notes = {
    "title": "Maximum Likelihood from Incomplete Data via the EM Algorithm",
    "year": 1977,
    "venue": "JRSS-B",
    "key_contribution": "General framework for MLE with missing/latent data",
    "method": "E-step (compute expected sufficient statistics) + M-step (maximize)",
    "theoretical_guarantee": "Log-likelihood monotonically non-decreasing",
    "examples_covered": "GMM, Factor Analysis, Missing data, Variance components",
}
```

**Pass 2** (30åˆ†): Theorem 1 (convergence) ã®è¨¼æ˜ã‚’è¿½ã†ã€‚Section 3 ã® Qé–¢æ•°å®šç¾©ãŒæ ¸å¿ƒã€‚

**Pass 3** (60åˆ†): Section 4 ã®åæŸé€Ÿåº¦ã€Section 5 ã®æŒ‡æ•°å‹åˆ†å¸ƒæ—ã§ã®ç°¡ç•¥åŒ–ã€‚
:::

### 4.7 Pythonã®é™ç•Œ â€” Profileçµæœ

ã“ã“ã§Course I ã®ä¼ç·šã‚’å›åã™ã‚‹ã€‚EMç®—æ³•ã®Pythonå®Ÿè£…ã‚’æœ¬æ ¼çš„ã«Profile ã—ã¦ã¿ã‚ˆã†ã€‚

```python
import numpy as np
import time

def em_gmm_full(X, K, max_iter=100, tol=1e-6, seed=42):
    """Full GMM EM with profiling."""
    rng = np.random.RandomState(seed)
    N, D = X.shape

    # Init
    idx = rng.choice(N, K, replace=False)
    mus = X[idx].copy()
    covs = [np.eye(D) * X.var() for _ in range(K)]
    pis = np.ones(K) / K

    times = {'e_step': 0.0, 'm_step': 0.0}

    for iteration in range(max_iter):
        # E-step
        t0 = time.perf_counter()
        log_resp = np.zeros((N, K))
        for k in range(K):
            diff = X - mus[k]
            cov_inv = np.linalg.inv(covs[k])
            log_det = np.log(np.linalg.det(covs[k]) + 1e-300)
            mahal = np.sum(diff @ cov_inv * diff, axis=1)
            log_resp[:, k] = np.log(pis[k]+1e-300) - 0.5*D*np.log(2*np.pi) - 0.5*log_det - 0.5*mahal

        log_max = log_resp.max(axis=1, keepdims=True)
        gamma = np.exp(log_resp - log_max)
        gamma /= gamma.sum(axis=1, keepdims=True)
        times['e_step'] += time.perf_counter() - t0

        # M-step
        t0 = time.perf_counter()
        N_k = gamma.sum(axis=0)
        for k in range(K):
            mus[k] = (gamma[:, k:k+1] * X).sum(axis=0) / N_k[k]
            diff = X - mus[k]
            covs[k] = (gamma[:, k:k+1] * diff).T @ diff / N_k[k] + 1e-6 * np.eye(D)
        pis = N_k / N
        times['m_step'] += time.perf_counter() - t0

    return times, iteration + 1

# Benchmark with increasing data size
print(f"{'N':>8} | {'K':>3} | {'D':>3} | {'E-step (ms)':>12} | {'M-step (ms)':>12} | {'Total (ms)':>12} | {'Per iter':>10}")
print("-" * 80)

for N in [1000, 5000, 10000, 50000]:
    D, K = 10, 5
    np.random.seed(42)
    X = np.random.randn(N, D)

    times, n_iter = em_gmm_full(X, K, max_iter=50)
    total = (times['e_step'] + times['m_step']) * 1000
    print(f"{N:8d} | {K:3d} | {D:3d} | {times['e_step']*1000:12.1f} | {times['m_step']*1000:12.1f} | "
          f"{total:12.1f} | {total/n_iter:10.1f} ms")

print(f"\n{'='*60}")
print("N=50000 ã§æ—¢ã«æ•°ç§’ã‹ã‹ã‚‹ã€‚")
print("N=1000000 ã«ãªã£ãŸã‚‰ï¼Ÿ D=100 ã«ãªã£ãŸã‚‰ï¼Ÿ")
print("......ã€Œé…ã™ããªã„ï¼Ÿã€")
print(f"{'='*60}")
print("\nã“ã®ç–‘å•ã¸ã®å›ç­”ã¯ç¬¬9å›ã§ã€‚")
print("Julia ã® ELBO è¨ˆç®—ã¯ Python ã® 50å€é€Ÿã„ã€‚è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ã€‚")
```

:::message alert
**é…ã™ããªã„ï¼Ÿ** â€” N=50,000, K=5, D=10 ã§æ—¢ã«æ•°ç§’ã€‚ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ï¼ˆN=100ä¸‡ã€D=100ï¼‰ã§ã¯åˆ†å˜ä½ã«ãªã‚‹ã€‚ã“ã® Python ã®é™ç•ŒãŒã€ç¬¬9å›ã® Julia å°å…¥ã®ä¼ç·šã ã€‚
:::

> **Zone 4 ã¾ã¨ã‚**: æ•°å€¤å®‰å®šãªEMå®Ÿè£…ï¼ˆlog-sum-expï¼‰ã€K-means++ åˆæœŸåŒ–ã€BIC/AICã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«é¸æŠã€Singularityå¯¾ç­–ã€æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¿’å¾—ã—ãŸã€‚ãã—ã¦Pythonã®é€Ÿåº¦é™ç•Œã‚’ä½“æ„Ÿã—ãŸã€‚

:::message
**é€²æ—: 70% å®Œäº†** å®Ÿè£…ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚ç†è«–ã‚’å®Ÿè£…ã«è½ã¨ã—è¾¼ã‚€æŠ€è¡“ã‚’ç²å¾—ã—ãŸã€‚æ¬¡ã¯å®Ÿé¨“ã§ç†è§£ã‚’ç¢ºèªã™ã‚‹ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### 5.1 è¨˜å·èª­è§£ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®æ•°å¼ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹ã‹ç¢ºèªã—ã‚ˆã†ã€‚

:::details Q1: $p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)$
**èª­ã¿**: ã€Œãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ã‚·ã‚°ãƒ ã‚¼ãƒƒãƒˆ ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ã€

**æ„å‘³**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã®ä¸‹ã§ã®è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ $\mathbf{x}$ ã®å‘¨è¾ºå°¤åº¦ã€‚æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’å…¨ã¦è¶³ã—åˆã‚ã›ã¦ï¼ˆå‘¨è¾ºåŒ–ã—ã¦ï¼‰å¾—ã‚‰ã‚Œã‚‹ã€‚ã“ã‚ŒãŒã€Œevidenceã€ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ã€‚[^1]
:::

:::details Q2: $\gamma(z_{nk}) = \frac{\pi_k \mathcal{N}(x_n \mid \mu_k, \sigma_k^2)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n \mid \mu_j, \sigma_j^2)}$
**èª­ã¿**: ã€Œã‚¬ãƒ³ãƒ ã‚¼ãƒƒãƒˆ ã‚¨ãƒŒ ã‚±ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ‘ã‚¤ã‚±ãƒ¼ ã‚¨ãƒŒ ã‚¨ãƒƒã‚¯ã‚¹ã‚¨ãƒŒ ãƒŸãƒ¥ãƒ¼ã‚±ãƒ¼ ã‚·ã‚°ãƒã‚±ãƒ¼äºŒä¹— ã¶ã‚“ã®...ã€

**æ„å‘³**: ãƒ‡ãƒ¼ã‚¿ç‚¹ $x_n$ ãŒæ··åˆæˆåˆ† $k$ ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸäº‹å¾Œç¢ºç‡ï¼ˆè²¬ä»»åº¦ï¼‰ã€‚E-stepã§è¨ˆç®—ã™ã‚‹ã€‚ãƒ™ã‚¤ã‚ºã®å®šç†ãã®ã‚‚ã®ã ã€‚
:::

:::details Q3: $Q(\theta, \theta^{(t)}) = \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})} [\log p(\mathbf{x}, \mathbf{z} \mid \theta)]$
**èª­ã¿**: ã€Œã‚­ãƒ¥ãƒ¼ ã‚·ãƒ¼ã‚¿ ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ã‚¯ã‚¹ãƒšã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ã‚¼ãƒƒãƒˆ ãƒ†ã‚£ãƒ«ãƒ‡ ãƒ”ãƒ¼ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚¨ãƒƒã‚¯ã‚¹ ã‚·ãƒ¼ã‚¿ãƒ†ã‚£ãƒ¼ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ã€

**æ„å‘³**: Qé–¢æ•°ã€‚ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta^{(t)}$ ã§ã®äº‹å¾Œåˆ†å¸ƒã®ä¸‹ã§ã€å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã®æœŸå¾…å€¤ã‚’å–ã£ãŸã‚‚ã®ã€‚M-stepã§ã¯ã“ã‚Œã‚’ $\theta$ ã«ã¤ã„ã¦æœ€å¤§åŒ–ã™ã‚‹ã€‚[^1]
:::

:::details Q4: $\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q \| p(\mathbf{z} \mid \mathbf{x}, \theta)]$
**èª­ã¿**: ã€Œãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒ« ã‚­ãƒ¥ãƒ¼ ã‚·ãƒ¼ã‚¿ ãƒ—ãƒ©ã‚¹ ã‚±ãƒ¼ã‚¨ãƒ« ã‚­ãƒ¥ãƒ¼ ãƒ‘ãƒ©ãƒ¬ãƒ« ãƒ”ãƒ¼ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚¨ãƒƒã‚¯ã‚¹ ã‚·ãƒ¼ã‚¿ã€

**æ„å‘³**: ELBOåˆ†è§£ã€‚å¯¾æ•°å°¤åº¦ã¯ELBOï¼ˆä¸‹ç•Œï¼‰ã¨KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®å’Œã«åˆ†è§£ã•ã‚Œã‚‹ã€‚KL $\geq 0$ ã ã‹ã‚‰ELBOã¯å¯¾æ•°å°¤åº¦ã®ä¸‹ç•Œã€‚E-stepã§ KL = 0 ã«ã—ã€M-stepã§ELBOã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚
:::

:::details Q5: $\boldsymbol{\mu}_k^{(t+1)} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) \, \mathbf{x}_n$
**èª­ã¿**: ã€ŒãƒŸãƒ¥ãƒ¼ ã‚±ãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ—ãƒ©ã‚¹ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒŒã‚±ãƒ¼ ã¶ã‚“ã®ã‚¤ãƒ ã‚·ã‚°ãƒ ã‚¨ãƒŒ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¤ãƒ ã‚«ãƒ© ã‚¨ãƒŒ ã‚¬ãƒ³ãƒ ã‚¼ãƒƒãƒˆã‚¨ãƒŒã‚±ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ã‚¨ãƒŒã€

**æ„å‘³**: GMM M-stepã®å¹³å‡æ›´æ–°å¼ã€‚è²¬ä»»åº¦ $\gamma(z_{nk})$ ã§é‡ã¿ä»˜ã‘ã—ãŸãƒ‡ãƒ¼ã‚¿ã®åŠ é‡å¹³å‡ã€‚$N_k = \sum_n \gamma(z_{nk})$ ã¯æˆåˆ† $k$ ã®ã€Œå®ŸåŠ¹ãƒ‡ãƒ¼ã‚¿æ•°ã€ã€‚
:::

:::details Q6: $\text{BIC} = -2 \log p(\mathbf{x} \mid \hat{\theta}) + d \log N$
**èª­ã¿**: ã€Œãƒ“ãƒ¼ã‚¢ã‚¤ã‚·ãƒ¼ ã‚¤ã‚³ãƒ¼ãƒ« ãƒã‚¤ãƒŠã‚¹ãƒ‹ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ãƒãƒƒãƒˆ ãƒ—ãƒ©ã‚¹ ãƒ‡ã‚£ãƒ¼ ãƒ­ã‚° ã‚¨ãƒŒã€

**æ„å‘³**: ãƒ™ã‚¤ã‚ºæƒ…å ±é‡åŸºæº–ã€‚ç¬¬1é …ã¯å°¤åº¦ï¼ˆãƒ•ã‚£ãƒƒãƒˆã®è‰¯ã•ï¼‰ã€ç¬¬2é …ã¯ãƒ¢ãƒ‡ãƒ«è¤‡é›‘åº¦ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° $d$ ãŒå¤šã„ã»ã©ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒå¤§ããã€éå­¦ç¿’ã‚’é˜²ãã€‚
:::

:::details Q7: $\boldsymbol{\Sigma}_k^{(t+1)} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) (\mathbf{x}_n - \boldsymbol{\mu}_k)(\mathbf{x}_n - \boldsymbol{\mu}_k)^\top$
**èª­ã¿**: ã€Œã‚·ã‚°ãƒ ã‚±ãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ—ãƒ©ã‚¹ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒŒã‚±ãƒ¼ ã¶ã‚“ã® ã‚·ã‚°ãƒ ã‚¬ãƒ³ãƒ ã‚¼ãƒƒãƒˆã‚¨ãƒŒã‚±ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ã‚¨ãƒŒ ãƒã‚¤ãƒŠã‚¹ ãƒŸãƒ¥ãƒ¼ã‚±ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ã‚¨ãƒŒ ãƒã‚¤ãƒŠã‚¹ ãƒŸãƒ¥ãƒ¼ã‚±ãƒ¼ ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ¼ã‚ºã€

**æ„å‘³**: GMM M-stepã®å…±åˆ†æ•£è¡Œåˆ—æ›´æ–°å¼ã€‚è²¬ä»»åº¦ã§é‡ã¿ä»˜ã‘ã—ãŸå¤–ç©ã®å¹³å‡ã€‚$D \times D$ è¡Œåˆ—ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚
:::

:::details Q8: $p(\mathbf{x}, \mathbf{z} \mid \theta) = p(\mathbf{x} \mid \mathbf{z}, \theta) \, p(\mathbf{z} \mid \theta)$
**èª­ã¿**: ã€Œãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚¼ãƒƒãƒˆ ã‚·ãƒ¼ã‚¿ ãƒ”ãƒ¼ ã‚¼ãƒƒãƒˆ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ã€

**æ„å‘³**: åŒæ™‚åˆ†å¸ƒã®åˆ†è§£ã€‚$p(\mathbf{z} \mid \theta)$ ã¯æ½œåœ¨å¤‰æ•°ã®äº‹å‰åˆ†å¸ƒï¼ˆGMMã§ã¯æ··åˆé‡ã¿ $\pi_k$ï¼‰ã€$p(\mathbf{x} \mid \mathbf{z}, \theta)$ ã¯æ¡ä»¶ä»˜ãå°¤åº¦ï¼ˆGMMã§ã¯å„ã‚¬ã‚¦ã‚¹æˆåˆ†ï¼‰ã€‚
:::

:::details Q9: $\log p(\mathbf{x} \mid \theta^{(t+1)}) \geq \log p(\mathbf{x} \mid \theta^{(t)})$
**èª­ã¿**: ã€Œãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ãƒ—ãƒ©ã‚¹ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ«ã‚ªã‚¢ã‚°ãƒ¬ãƒ¼ã‚¿ãƒ¼ ãƒ­ã‚° ãƒ”ãƒ¼ ã‚¨ãƒƒã‚¯ã‚¹ ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ« ã‚·ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ã€

**æ„å‘³**: EMç®—æ³•ã®å˜èª¿æ€§ã€‚å„åå¾©ã§å¯¾æ•°å°¤åº¦ã¯æ¸›å°‘ã—ãªã„ã€‚ã“ã‚Œã¯EMç®—æ³•ã®ç†è«–çš„ä¿è¨¼ã§ã‚ã‚Šã€Wu (1983) [^3] ã§å³å¯†ã«è¨¼æ˜ã•ã‚ŒãŸã€‚
:::

:::details Q10: $\pi_k^{(t+1)} = \frac{N_k}{N}$, where $N_k = \sum_{n=1}^{N} \gamma(z_{nk})$
**èª­ã¿**: ã€Œãƒ‘ã‚¤ ã‚±ãƒ¼ ãƒ†ã‚£ãƒ¼ãƒ—ãƒ©ã‚¹ãƒ¯ãƒ³ ã‚¤ã‚³ãƒ¼ãƒ« ã‚¨ãƒŒã‚±ãƒ¼ ã¶ã‚“ã®ã‚¨ãƒŒã€

**æ„å‘³**: æ··åˆé‡ã¿ã®æ›´æ–°å¼ã€‚$N_k$ ã¯æˆåˆ† $k$ ã«å¸°å±ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã€Œå®ŸåŠ¹çš„ãªæ•°ã€ã§ã‚ã‚Šã€å…¨ãƒ‡ãƒ¼ã‚¿æ•° $N$ ã§å‰²ã‚‹ã“ã¨ã§ç¢ºç‡ï¼ˆæ¯”ç‡ï¼‰ã«ãªã‚‹ã€‚ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•ã‹ã‚‰å°å‡ºã•ã‚Œã‚‹ã€‚
:::

### 5.2 LaTeXè¨˜è¿°ãƒ†ã‚¹ãƒˆ

:::details LQ1: GMMã®å‘¨è¾ºå°¤åº¦ã‚’æ›¸ã‘
```latex
p(\mathbf{x} \mid \theta) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
```
$$p(\mathbf{x} \mid \theta) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$$
:::

:::details LQ2: ELBOåˆ†è§£ã‚’æ›¸ã‘
```latex
\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]
```
$$\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]$$
:::

:::details LQ3: Jensenä¸ç­‰å¼ï¼ˆå‡¹é–¢æ•°ç‰ˆï¼‰ã‚’æ›¸ã‘
```latex
f\left( \mathbb{E}[X] \right) \geq \mathbb{E}[f(X)] \quad (\text{for concave } f)
```
$$f\left( \mathbb{E}[X] \right) \geq \mathbb{E}[f(X)] \quad (\text{for concave } f)$$
:::

:::details LQ4: Qé–¢æ•°ã‚’æ›¸ã‘
```latex
Q(\theta, \theta^{(t)}) = \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})} \left[ \log p(\mathbf{x}, \mathbf{z} \mid \theta) \right]
```
$$Q(\theta, \theta^{(t)}) = \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z} \mid \mathbf{x}, \theta^{(t)})} \left[ \log p(\mathbf{x}, \mathbf{z} \mid \theta) \right]$$
:::

:::details LQ5: å¤šå¤‰é‡ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®KL divergenceã‚’æ›¸ã‘
```latex
\text{KL}[\mathcal{N}_0 \| \mathcal{N}_1] = \frac{1}{2} \left[ \text{tr}(\boldsymbol{\Sigma}_1^{-1} \boldsymbol{\Sigma}_0) + (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0)^\top \boldsymbol{\Sigma}_1^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0) - D + \log \frac{|\boldsymbol{\Sigma}_1|}{|\boldsymbol{\Sigma}_0|} \right]
```
$$\text{KL}[\mathcal{N}_0 \| \mathcal{N}_1] = \frac{1}{2} \left[ \text{tr}(\boldsymbol{\Sigma}_1^{-1} \boldsymbol{\Sigma}_0) + (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0)^\top \boldsymbol{\Sigma}_1^{-1} (\boldsymbol{\mu}_1 - \boldsymbol{\mu}_0) - D + \log \frac{|\boldsymbol{\Sigma}_1|}{|\boldsymbol{\Sigma}_0|} \right]$$
ã“ã®å…¬å¼ã¯ç¬¬9å›ï¼ˆå¤‰åˆ†æ¨è«–ï¼‰ã¨ç¬¬10å›ï¼ˆVAEï¼‰ã§é »å‡ºã™ã‚‹ã€‚ä»Šã®ã†ã¡ã«æ›¸ã‘ã‚‹ã‚ˆã†ã«ã—ã¦ãŠã“ã†ã€‚
:::

### 5.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

:::details CQ1: è²¬ä»»åº¦ã®è¨ˆç®—ã‚’NumPyã§æ›¸ã‘
```python
# Î³(z_nk) = Ï€_k N(x_n|Î¼_k,Ïƒ_kÂ²) / Î£_j Ï€_j N(x_n|Î¼_j,Ïƒ_jÂ²)
def compute_responsibilities(X, mus, sigmas, pis):
    N = len(X)
    K = len(mus)
    pdf = np.zeros((N, K))
    for k in range(K):
        pdf[:, k] = pis[k] * np.exp(-0.5*((X - mus[k])/sigmas[k])**2) / (sigmas[k]*np.sqrt(2*np.pi))
    gamma = pdf / pdf.sum(axis=1, keepdims=True)
    return gamma
```
:::

:::details CQ2: M-stepæ›´æ–°ï¼ˆå¤šå¤‰é‡ï¼‰ã‚’NumPyã§æ›¸ã‘
```python
# Î¼_k = (1/N_k) Î£_n Î³_nk x_n
# Î£_k = (1/N_k) Î£_n Î³_nk (x_n - Î¼_k)(x_n - Î¼_k)^T
def m_step_multivariate(X, gamma):
    N, D = X.shape
    K = gamma.shape[1]
    N_k = gamma.sum(axis=0)
    mus = np.zeros((K, D))
    covs = [np.zeros((D, D)) for _ in range(K)]
    for k in range(K):
        mus[k] = (gamma[:, k:k+1] * X).sum(axis=0) / N_k[k]
        diff = X - mus[k]
        covs[k] = (gamma[:, k:k+1] * diff).T @ diff / N_k[k]
    pis = N_k / N
    return mus, covs, pis
```
:::

:::details CQ3: BICè¨ˆç®—ã‚’NumPyã§æ›¸ã‘
```python
# BIC = -2 log p(x|Î¸Ì‚) + d log N
def compute_bic(log_likelihood, n_params, n_data):
    return -2 * log_likelihood + n_params * np.log(n_data)

# For 1D GMM with K components: d = 3K - 1
# (K means + K variances + K-1 free mixing weights)
```
:::

:::details CQ4: log-sum-exp ã‚’å®Ÿè£…ã›ã‚ˆ
```python
def log_sum_exp(a):
    """log Î£_k exp(a_k) = max(a) + log Î£_k exp(a_k - max(a))"""
    a_max = np.max(a, axis=-1, keepdims=True)
    return a_max.squeeze(-1) + np.log(np.sum(np.exp(a - a_max), axis=-1))
```
:::

:::details CQ5: å®Œå…¨ãª1D GMM EMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’50è¡Œä»¥å†…ã§æ›¸ã‘
```python
import numpy as np

def gmm_em_1d(x, K, n_iter=50, seed=42):
    rng = np.random.RandomState(seed)
    N = len(x)
    # Init
    mu = np.sort(rng.choice(x, K, replace=False).astype(float))
    sigma = np.full(K, x.std())
    pi = np.full(K, 1.0 / K)

    for _ in range(n_iter):
        # E-step: Î³_nk = Ï€_k N(x_n|Î¼_k,Ïƒ_k) / Î£_j Ï€_j N(x_n|Î¼_j,Ïƒ_j)
        pdf = np.column_stack([
            pi[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2) / (sigma[k]*np.sqrt(2*np.pi))
            for k in range(K)])
        gamma = pdf / (pdf.sum(axis=1, keepdims=True) + 1e-300)

        # M-step
        Nk = gamma.sum(axis=0)
        for k in range(K):
            mu[k] = (gamma[:,k] * x).sum() / Nk[k]
            sigma[k] = np.sqrt((gamma[:,k] * (x-mu[k])**2).sum() / Nk[k]) + 1e-6
        pi = Nk / N

    ll = np.sum(np.log(pdf.sum(axis=1) + 1e-300))
    return mu, sigma, pi, ll
```
:::

### 5.4 è«–æ–‡èª­è§£ãƒ†ã‚¹ãƒˆ â€” Dempster, Laird, Rubin (1977) Pass 1

:::details è«–æ–‡ Pass 1 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’åŸ‹ã‚ã‚ˆ
```python
paper_pass1 = {
    "title": "Maximum Likelihood from Incomplete Data via the EM Algorithm",
    "authors": "A.P. Dempster, N.M. Laird, D.B. Rubin",
    "year": 1977,
    "venue": "Journal of the Royal Statistical Society, Series B",
    "category": "Theory / Algorithm",

    # What problem does it solve?
    "problem": "MLE when data has missing/latent components (incomplete data)",

    # What is the key idea?
    "key_idea": "Alternate between E-step (compute expected sufficient statistics "
                "using current parameters) and M-step (maximize expected "
                "complete-data log-likelihood)",

    # What is the main result?
    "main_result": "Log-likelihood is monotonically non-decreasing under EM iterations. "
                   "Convergence to stationary point guaranteed under mild conditions.",

    # What experiments/examples?
    "examples": "GMM, Factor Analysis, variance components, missing data, "
                "grouped/censored data",

    # What are the limitations?
    "limitations": "Only local convergence guaranteed. Linear convergence rate. "
                   "Convergence speed depends on fraction of missing information.",

    # Relevance to this lecture?
    "relevance": "Foundational paper. All EM-based methods (VAE, HMM, Factor Analysis) "
                 "trace back to this formulation.",
}
```
:::

### 5.5 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**ãƒãƒ£ãƒ¬ãƒ³ã‚¸ 1: EMã®åæŸå¯è¦–åŒ–**

```python
import numpy as np

def em_convergence_study(n_restarts=5, K=3, N=300, max_iter=50):
    """Run EM from multiple random initializations and track convergence."""
    np.random.seed(42)
    x = np.concatenate([np.random.normal(-3, 0.8, 100),
                         np.random.normal(1, 1.0, 100),
                         np.random.normal(5, 0.6, 100)])

    results = []
    for restart in range(n_restarts):
        mu = np.random.uniform(x.min(), x.max(), K)
        sigma = np.ones(K) * x.std()
        pi_k = np.ones(K) / K
        lls = []

        for t in range(max_iter):
            pdf = np.zeros((N, K))
            for k in range(K):
                pdf[:, k] = pi_k[k] * np.exp(-0.5*((x-mu[k])/sigma[k])**2)/(sigma[k]*np.sqrt(2*np.pi))
            total = pdf.sum(axis=1)
            lls.append(np.sum(np.log(total + 1e-300)))
            gamma = pdf / (total[:, np.newaxis] + 1e-300)

            N_k = gamma.sum(axis=0)
            for k in range(K):
                mu[k] = (gamma[:, k] * x).sum() / (N_k[k] + 1e-300)
                sigma[k] = np.sqrt((gamma[:, k] * (x - mu[k])**2).sum() / (N_k[k] + 1e-300)) + 1e-6
            pi_k = N_k / N

        results.append({'final_ll': lls[-1], 'lls': lls, 'mu': mu.copy()})

    print("=== EM Convergence Study ===")
    print(f"{'Restart':>7} | {'Final log-lik':>14} | {'Converged Î¼':>30}")
    print("-" * 60)
    for i, r in enumerate(results):
        mu_str = ", ".join(f"{m:.2f}" for m in sorted(r['mu']))
        best = " â† best" if r['final_ll'] == max(rr['final_ll'] for rr in results) else ""
        print(f"{i:7d} | {r['final_ll']:14.4f} | ({mu_str}){best}")

    best_idx = np.argmax([r['final_ll'] for r in results])
    print(f"\nBest restart: {best_idx} (log-lik = {results[best_idx]['final_ll']:.4f})")
    return results

results = em_convergence_study()
```

**ãƒãƒ£ãƒ¬ãƒ³ã‚¸ 2: Missing Data Imputation via EM**

```python
import numpy as np

def em_missing_data(X_obs, mask, max_iter=30):
    """EM for missing data imputation (single Gaussian model).

    X_obs: (N, D) data with missing values set to 0
    mask: (N, D) boolean, True = observed, False = missing
    """
    N, D = X_obs.shape

    # Init: mean and covariance from observed entries
    mu = np.zeros(D)
    for d in range(D):
        obs_d = X_obs[mask[:, d], d]
        mu[d] = obs_d.mean() if len(obs_d) > 0 else 0.0
    cov = np.eye(D)

    for t in range(max_iter):
        # E-step: impute missing values using conditional distribution
        X_filled = X_obs.copy()
        for n in range(N):
            obs_idx = np.where(mask[n])[0]
            mis_idx = np.where(~mask[n])[0]
            if len(mis_idx) == 0:
                continue
            if len(obs_idx) == 0:
                X_filled[n, mis_idx] = mu[mis_idx]
                continue

            # Conditional: p(x_mis | x_obs) = N(Î¼_cond, Î£_cond)
            cov_oo = cov[np.ix_(obs_idx, obs_idx)]
            cov_mo = cov[np.ix_(mis_idx, obs_idx)]
            cov_oo_inv = np.linalg.inv(cov_oo + 1e-6 * np.eye(len(obs_idx)))
            mu_cond = mu[mis_idx] + cov_mo @ cov_oo_inv @ (X_obs[n, obs_idx] - mu[obs_idx])
            X_filled[n, mis_idx] = mu_cond

        # M-step: update Î¼ and Î£ from filled data
        mu = X_filled.mean(axis=0)
        diff = X_filled - mu
        cov = diff.T @ diff / N

    return X_filled, mu, cov

# Test with 20% missing data
np.random.seed(42)
N, D = 200, 3
true_mu = np.array([1.0, -2.0, 3.0])
true_cov = np.array([[1.0, 0.5, 0.2], [0.5, 2.0, -0.3], [0.2, -0.3, 1.5]])
X_true = np.random.multivariate_normal(true_mu, true_cov, N)

# Create missing data (MCAR - Missing Completely At Random)
mask = np.random.random((N, D)) > 0.2  # 20% missing
X_obs = X_true * mask  # zero out missing entries

X_filled, est_mu, est_cov = em_missing_data(X_obs, mask)

print("=== Missing Data Imputation via EM ===")
print(f"Missing rate: {(~mask).mean():.1%}")
print(f"\nTrue Î¼:      {true_mu}")
print(f"Estimated Î¼: {est_mu.round(3)}")
print(f"\nMSE (imputed vs true): {np.mean((X_filled[~mask] - X_true[~mask])**2):.4f}")
print(f"MSE (naive zero fill):  {np.mean((0 - X_true[~mask])**2):.4f}")
```

### 5.6 ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®åŒæ™‚åˆ†å¸ƒ $p(\mathbf{x}, \mathbf{z} \mid \theta)$ ã‚’æ›¸ãä¸‹ã›ã‚‹
- [ ] å‘¨è¾ºåŒ– $p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)$ ã®æ„å‘³ãŒã‚ã‹ã‚‹
- [ ] ã€Œ$\log \sum$ ãŒè§£æè§£ã‚’é˜»ã‚€ã€ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Jensenä¸ç­‰å¼ã‚’å‡¹é–¢æ•°/å‡¸é–¢æ•°ä¸¡æ–¹ã§æ›¸ã‘ã‚‹
- [ ] ELBOåˆ†è§£ã‚’å°å‡ºã§ãã‚‹ï¼ˆ2é€šã‚Š: ãƒ™ã‚¤ã‚ºåˆ†è§£ / Jensenä¸ç­‰å¼ï¼‰
- [ ] E-stepãŒã€ŒKL = 0ã«ã™ã‚‹ã€æ“ä½œã§ã‚ã‚‹ã“ã¨ã‚’èª¬æ˜ã§ãã‚‹
- [ ] M-stepãŒã€ŒELBOã‚’æœ€å¤§åŒ–ã™ã‚‹ã€æ“ä½œã§ã‚ã‚‹ã“ã¨ã‚’èª¬æ˜ã§ãã‚‹
- [ ] GMMã®è²¬ä»»åº¦ $\gamma(z_{nk})$ ã‚’å°å‡ºã§ãã‚‹
- [ ] GMMã® $\mu_k$, $\sigma_k^2$, $\pi_k$ ã®æ›´æ–°å¼ã‚’å°å‡ºã§ãã‚‹
- [ ] EMç®—æ³•ã®å˜èª¿æ€§ã‚’è¨¼æ˜ã§ãã‚‹
- [ ] log-sum-expãƒˆãƒªãƒƒã‚¯ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] BIC/AICã®ä½¿ã„æ–¹ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Singularityå•é¡Œã¨å¯¾ç­–ã‚’èª¬æ˜ã§ãã‚‹

:::message
**é€²æ—: 85% å®Œäº†** å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ã‚¯ãƒªã‚¢ã€‚è¨˜å·èª­è§£ã€LaTeXè¨˜è¿°ã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³ã€è«–æ–‡èª­è§£ã®å…¨æ–¹ä½ãƒ†ã‚¹ãƒˆã‚’å®Œäº†ã—ãŸã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 Mixture of Experts (MoE) â€” Transformeræ™‚ä»£ã®å¾©æ´»

Jacobs, Jordan, Nowlan, Hinton (1991) [^7] ãŒææ¡ˆã—ãŸMixture of Experts (MoE) ã¯ã€EMçš„ãªæ§‹é€ ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã ã€‚

$$
p(y \mid x) = \sum_{k=1}^{K} \underbrace{g_k(x)}_{\text{ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°}} \cdot \underbrace{f_k(x; \theta_k)}_{\text{å°‚é–€å®¶}}
$$

ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•° $g_k(x)$ ã¯ Softmax ã§å®Ÿè£…ã•ã‚Œã‚‹ã€‚å„å°‚é–€å®¶ $f_k$ ã¯å…¥åŠ›ç©ºé–“ã®ä¸€éƒ¨ã‚’æ‹…å½“ã™ã‚‹ã€‚

ã“ã®æ§‹é€ ã¯Transformerã®MoEå±¤ã¨ã—ã¦å¾©æ´»ã—ã¦ã„ã‚‹ã€‚GPT-4ã‚„Mixtral 8x7Bã¯ã€ã“ã®MoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ã£ã¦è¨ˆç®—åŠ¹ç‡ã‚’åŠ‡çš„ã«æ”¹å–„ã—ãŸã€‚è©³ç´°ã¯ç¬¬16å›ï¼ˆTransformerå®Œå…¨ç‰ˆï¼‰ã§æ‰±ã†ã€‚

### 6.2 Expectation Propagation â€” EMã®ä»£æ›¿

Minka (2001) [^12] ãŒææ¡ˆã—ãŸ **Expectation Propagation** (EP) ã¯ã€EMç®—æ³•ã®ä»£æ›¿ã¨ãªã‚‹è¿‘ä¼¼æ¨è«–æ‰‹æ³•ã ã€‚

EMãŒäº‹å¾Œåˆ†å¸ƒå…¨ä½“ã‚’è¨ˆç®—ã™ã‚‹ã®ã«å¯¾ã—ã€EPã¯äº‹å¾Œåˆ†å¸ƒã® **ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ**ï¼ˆå¹³å‡ã¨åˆ†æ•£ï¼‰ã ã‘ã‚’ä¿æŒã—ã€åå¾©çš„ã«æ›´æ–°ã™ã‚‹ã€‚

| æ‰‹æ³• | è¿‘ä¼¼åˆ†å¸ƒ | KLæ–¹å‘ | ç‰¹å¾´ |
|:-----|:---------|:-------|:-----|
| Variational EM | $q(\mathbf{z})$ ãŒ $p$ ã‚’è¿‘ä¼¼ | $\min \text{KL}[q \| p]$ | mode-seekingï¼ˆãƒ¢ãƒ¼ãƒ‰è¿½è·¡ï¼‰ |
| EP | å„å› å­ã‚’å€‹åˆ¥ã«è¿‘ä¼¼ | $\min \text{KL}[p \| q]$ | moment-matchingï¼ˆãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆä¸€è‡´ï¼‰ |

KLã®æ–¹å‘ãŒé€†ã§ã‚ã‚‹ã“ã¨ã«æ³¨ç›®ã—ã¦ã»ã—ã„ã€‚$\text{KL}[q \| p]$ ã®æœ€å°åŒ–ã¯ $q$ ãŒ $p$ ã®ãƒ¢ãƒ¼ãƒ‰ã®1ã¤ã«é›†ä¸­ã™ã‚‹å‚¾å‘ãŒã‚ã‚‹ãŒã€$\text{KL}[p \| q]$ ã®æœ€å°åŒ–ã¯ $q$ ãŒ $p$ ã®å…¨ã¦ã®ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚«ãƒãƒ¼ã—ã‚ˆã†ã¨ã™ã‚‹ã€‚

```python
import numpy as np

# Conceptual comparison: EM vs EP
# EM minimizes KL[q||p] â†’ mode-seeking
# EP minimizes KL[p||q] â†’ moment-matching

# Bimodal target distribution (mixture of 2 Gaussians)
def bimodal_pdf(x, mu1=-2, mu2=3, sigma=0.8):
    return 0.5 * np.exp(-0.5*((x-mu1)/sigma)**2)/(sigma*np.sqrt(2*np.pi)) + \
           0.5 * np.exp(-0.5*((x-mu2)/sigma)**2)/(sigma*np.sqrt(2*np.pi))

# KL[q||p] minimization â†’ q picks ONE mode
# Best Gaussian approximation (mode-seeking):
x_grid = np.linspace(-6, 8, 10000)
p = bimodal_pdf(x_grid)

# Mode-seeking: q centers on higher mode
q_mode_seeking_mu = -2.0  # or 3.0 â€” picks one mode
q_mode_seeking_sigma = 0.8

# KL[p||q] minimization â†’ q covers BOTH modes
# Moment-matching: mean and variance of p
p_normalized = p / (p.sum() * (x_grid[1] - x_grid[0]))
ep_mu = np.sum(x_grid * p_normalized * (x_grid[1] - x_grid[0]))
ep_var = np.sum((x_grid - ep_mu)**2 * p_normalized * (x_grid[1] - x_grid[0]))

print("=== EM vs EP approximation of bimodal distribution ===")
print(f"Target: mixture of N(-2, 0.8Â²) and N(3, 0.8Â²)")
print(f"\nEM (mode-seeking):     Î¼ = {q_mode_seeking_mu:.1f}, Ïƒ = {q_mode_seeking_sigma:.1f}")
print(f"                       â†’ concentrates on ONE mode")
print(f"\nEP (moment-matching):  Î¼ = {ep_mu:.2f}, Ïƒ = {np.sqrt(ep_var):.2f}")
print(f"                       â†’ covers BOTH modes (broader Gaussian)")
print(f"\nNeither is 'correct' â€” they make different tradeoffs.")
print(f"EM/VI is standard for VAE. EP is useful for Bayesian inference.")
```

EPã®è©³ç´°ã¯æœ¬ã‚·ãƒªãƒ¼ã‚ºã®ã‚¹ã‚³ãƒ¼ãƒ—å¤–ã ãŒã€EMçš„ãªåå¾©æ¨è«–ã®ã€Œåˆ¥ã®å‘³ã€ã¨ã—ã¦çŸ¥ã£ã¦ãŠãã¨ã€è¿‘ä¼¼æ¨è«–ã®å…¨ä½“åƒãŒè¦‹ãˆã‚„ã™ããªã‚‹ã€‚

### 6.3 æœ€æ–°ç ”ç©¶å‹•å‘ (2024-2026)

EMç®—æ³•ã¨æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã¯ã€ç¾åœ¨ã‚‚æ´»ç™ºã«ç ”ç©¶ã•ã‚Œã¦ã„ã‚‹ã€‚

| ç ”ç©¶ãƒ†ãƒ¼ãƒ | æ¦‚è¦ | EMã¨ã®é–¢ä¿‚ |
|:---------|:-----|:---------|
| Latent Thoughts EM | LLMã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ½œåœ¨å¤‰æ•°ã¨ã—ã¦æ‰±ã„ã€EMçš„ã«è¨“ç·´ | EMåŸç†ã®LLMè¨“ç·´ã¸ã®é©ç”¨ |
| MoLAE (Mixture of Latent Experts) | ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã¸ã®å…±æœ‰å°„å½±ã§MoEåŠ¹ç‡åŒ– | MoE + æ½œåœ¨å¤‰æ•°ã®çµ±åˆ |
| Amortized EM | å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®EMé«˜é€ŸåŒ–ï¼ˆæ¨è«–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ©ç”¨ï¼‰ | EMã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ”¹å–„ |
| Neural EM | E-stepã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ç½®æ› | EMæ§‹é€ ã®æ·±å±¤å­¦ç¿’åŒ– |

**EMç®—æ³•ã¯ã€Œå¤ã„ã€æ‰‹æ³•ã§ã¯ãªãã€å½¢ã‚’å¤‰ãˆã¦æœ€å…ˆç«¯ã«ç”Ÿãç¶šã‘ã¦ã„ã‚‹ã€‚** ç¬¬9å›ä»¥é™ã§ãã®ç¾ä»£çš„ãªå§¿ã‚’è©³ã—ãè¦‹ã¦ã„ãã€‚

### 6.4 æ¨è–¦æ›¸ç±ãƒ»ãƒªã‚½ãƒ¼ã‚¹

| æ›¸ç± | è‘—è€… | EMé–¢é€£ç«  | ãƒ¬ãƒ™ãƒ« |
|:-----|:-----|:--------|:------|
| *Pattern Recognition and Machine Learning* | Bishop (2006) [^11] | Ch. 9 Mixture Models and EM | â˜…â˜…â˜…â˜…â˜† |
| *Machine Learning: A Probabilistic Perspective* | Murphy (2012) | Ch. 11 Mixture Models and EM | â˜…â˜…â˜…â˜…â˜† |
| *Probabilistic Graphical Models* | Koller & Friedman (2009) | Ch. 19 Learning with Incomplete Data | â˜…â˜…â˜…â˜…â˜… |
| *Information Theory, Inference, and Learning* | MacKay (2003) | Ch. 22 EM Algorithm | â˜…â˜…â˜…â˜†â˜† |

| ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹ | URL | ç‰¹å¾´ |
|:-----------------|:----|:-----|
| Bishop PRML Ch.9 | å…¬å¼PDFç„¡æ–™å…¬é–‹ | GMMã¨EMã®æ•™ç§‘æ›¸çš„è§£èª¬ |
| Stanford CS229 EM | YouTube | Andrew Ng ã®ç›´æ„Ÿçš„ãªè¬›ç¾© |
| Lil'Log EM Algorithm | lilianweng.github.io | ç†è«–ã¨å®Ÿè£…ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ |

:::details ç”¨èªé›†
| ç”¨èª | è‹±èª | å®šç¾© |
|:-----|:-----|:-----|
| æ½œåœ¨å¤‰æ•° | latent variable | ç›´æ¥è¦³æ¸¬ã§ããªã„ç¢ºç‡å¤‰æ•° |
| å‘¨è¾ºå°¤åº¦ | marginal likelihood / evidence | æ½œåœ¨å¤‰æ•°ã‚’å‘¨è¾ºåŒ–ã—ãŸå°¤åº¦ |
| è²¬ä»»åº¦ | responsibility | ãƒ‡ãƒ¼ã‚¿ç‚¹ãŒå„æˆåˆ†ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸäº‹å¾Œç¢ºç‡ |
| Qé–¢æ•° | Q-function | å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã®äº‹å¾ŒæœŸå¾…å€¤ |
| ELBO | Evidence Lower Bound | å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®ä¸‹ç•Œ |
| ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ« | Gaussian Mixture Model (GMM) | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®é‡ã¿ä»˜ãå’Œã§è¡¨ã™å¯†åº¦ãƒ¢ãƒ‡ãƒ« |
| æŒ‡ç¤ºé–¢æ•° | indicator function | æ¡ä»¶ãŒçœŸãªã‚‰1ã€å½ãªã‚‰0ã‚’è¿”ã™é–¢æ•° |
| ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ | Mahalanobis distance | å…±åˆ†æ•£ã‚’è€ƒæ…®ã—ãŸè·é›¢ |
| Singularityå•é¡Œ | singularity problem | åˆ†æ•£â†’0ã§å°¤åº¦â†’âˆã«ç™ºæ•£ã™ã‚‹å•é¡Œ |
| å±€æ‰€æœ€é©è§£ | local optimum | è¿‘å‚ã§ã¯æœ€é©ã ãŒå¤§åŸŸçš„ã«ã¯æœ€é©ã§ãªã„è§£ |
| å˜èª¿åæŸ | monotone convergence | å„åå¾©ã§ç›®çš„é–¢æ•°ãŒéæ¸›å°‘ã§ã‚ã‚‹ã“ã¨ |
| Forward-Backward | forward-backward algorithm | HMMã®åŠ¹ç‡çš„ãªäº‹å¾Œç¢ºç‡è¨ˆç®—æ³• |
| Baum-Welch | Baum-Welch algorithm | HMMã«å¯¾ã™ã‚‹EMç®—æ³• |
| å¤‰åˆ†æ¨è«– | variational inference | äº‹å¾Œåˆ†å¸ƒã‚’æœ€é©åŒ–å•é¡Œã¨ã—ã¦è¿‘ä¼¼ã™ã‚‹æ‰‹æ³• |
| Amortizedæ¨è«– | amortized inference | æ¨è«–ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§æ±åŒ–ã™ã‚‹æ‰‹æ³• |
:::

```mermaid
mindmap
  root((ç¬¬8å›<br/>æ½œåœ¨å¤‰æ•° & EM))
    æ½œåœ¨å¤‰æ•°
      è¦³æ¸¬å¤‰æ•° x
      éš ã‚Œå¤‰æ•° z
      å‘¨è¾ºåŒ– p(x) = Î£_z p(x,z)
      äº‹å¾Œåˆ†å¸ƒ p(z|x)
    EMç®—æ³•
      Jensenä¸ç­‰å¼
      ELBOåˆ†è§£
      E-step: q = p(z|x,Î¸)
      M-step: max Q(Î¸,Î¸_t)
      å˜èª¿åæŸ
    GMM
      è²¬ä»»åº¦ Î³
      Î¼æ›´æ–°
      Ïƒæ›´æ–°
      Ï€æ›´æ–°
      BIC/AIC
    æ‹¡å¼µ
      HMM / Baum-Welch
      Factor Analysis
      PPCA
      Variational EM
      MoE
    â†’ Course II
      å¤‰åˆ†æ¨è«–(ç¬¬9å›)
      VAE(ç¬¬10å›)
      Juliaç™»å ´(ç¬¬9å›)
```

> **Zone 6 ã¾ã¨ã‚**: EMç®—æ³•ã®ç ”ç©¶ç³»è­œï¼ˆHMMã€FAã€PPCAã€MoEï¼‰ã‚’ä¿¯ç°ã—ã€Variational EM â†’ VAE ã¸ã®æ©‹æ¸¡ã—ã‚’ç†è§£ã—ãŸã€‚ç¬¬8å›ã®çŸ¥è­˜ãŒç¬¬9å›ä»¥é™ã§ã©ã†æ´»ãã‚‹ã‹ãŒæ˜ç¢ºã«ãªã£ãŸã€‚

---

### 6.5 æœ¬è¬›ç¾©ã®3ã¤ã®æ ¸å¿ƒ

**1. æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«** â€” ãƒ‡ãƒ¼ã‚¿ã®è£ã«ã€Œè¦‹ãˆãªã„åŸå› ã€ã‚’ä»®å®šã—ã€$p(\mathbf{x} \mid \theta) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \theta)$ ã¨åˆ†è§£ã™ã‚‹ã€‚ã“ã®å‘¨è¾ºåŒ–ãŒè¨ˆç®—å›°é›£ã§ã‚ã‚‹ã“ã¨ãŒã€EMç®—æ³•ã‚’å¿…è¦ã¨ã™ã‚‹æ ¹æœ¬çš„ç†ç”±ã€‚

**2. EMç®—æ³•ã®æ§‹é€ ** â€” ELBOåˆ†è§£ $\log p(\mathbf{x} \mid \theta) = \mathcal{L}(q, \theta) + \text{KL}[q \| p(\mathbf{z} \mid \mathbf{x}, \theta)]$ ã«åŸºã¥ãã€‚E-stepã§KL=0ã«ã—ï¼ˆELBOã‚’å¼•ãä¸Šã’ï¼‰ã€M-stepã§ELBOã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚å¯¾æ•°å°¤åº¦ã®å˜èª¿å¢—åŠ ãŒä¿è¨¼ã•ã‚Œã‚‹ã€‚

**3. VAEã¸ã®æ©‹** â€” EMç®—æ³•ã®é™ç•Œï¼ˆè§£æçš„äº‹å¾Œåˆ†å¸ƒãŒå¿…è¦ï¼‰ã‚’ Variational EM ãŒç·©å’Œã—ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã«ã‚ˆã‚‹æ¨è«–ï¼ˆAmortized Inferenceï¼‰ãŒVAEã«ç¹‹ãŒã‚‹ã€‚

### 6.6 FAQ

:::details Q1: EMç®—æ³•ã¯å¿…ãšå¤§åŸŸæœ€é©è§£ã«åæŸã—ã¾ã™ã‹ï¼Ÿ
ã„ã„ãˆã€‚EMç®—æ³•ã¯**å±€æ‰€æœ€é©è§£**ï¼ˆæ­£ç¢ºã«ã¯ä¸å‹•ç‚¹ï¼‰ã¸ã®åæŸã—ã‹ä¿è¨¼ã—ã¾ã›ã‚“ã€‚å¤§åŸŸæœ€é©è§£ã¸ã®åˆ°é”ã¯ä¿è¨¼ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å®Ÿå‹™ã§ã¯è¤‡æ•°ã®ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸå€¤ã‹ã‚‰å®Ÿè¡Œã—ï¼ˆmultiple restartsï¼‰ã€æœ€ã‚‚é«˜ã„å¯¾æ•°å°¤åº¦ã‚’é”æˆã—ãŸçµæœã‚’æ¡ç”¨ã™ã‚‹ã®ãŒæ¨™æº–çš„ã§ã™ã€‚
:::

:::details Q2: K-meansã¨GMM-EMã®é–¢ä¿‚ã¯ï¼Ÿ
K-meansã¯GMM-EMã®**ç‰¹æ®Šã‚±ãƒ¼ã‚¹**ã§ã™ã€‚å…¨æˆåˆ†ã®åˆ†æ•£ãŒç­‰ã—ãï¼ˆ$\sigma_k^2 = \sigma^2$ï¼‰ã€$\sigma^2 \to 0$ ã®æ¥µé™ã‚’å–ã‚‹ã¨ã€soft assignmentï¼ˆè²¬ä»»åº¦ $\gamma_{nk} \in [0, 1]$ï¼‰ãŒhard assignmentï¼ˆ$\gamma_{nk} \in \{0, 1\}$ï¼‰ã«ãªã‚Šã€K-meansã®æ›´æ–°å¼ã¨ä¸€è‡´ã—ã¾ã™ã€‚
:::

:::details Q3: EMç®—æ³•ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’ã«ã‚‚ä½¿ãˆã¾ã™ã‹ï¼Ÿ
ç›´æ¥çš„ã«ã¯ä½¿ã„ã«ãã„ã§ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹M-stepã®é–‰å½¢å¼è§£ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã§ã™ã€‚ä»£ã‚ã‚Šã«Variational EMã®æ çµ„ã¿ã§ã€å‹¾é…é™ä¸‹æ³•ã«ã‚ˆã‚‹M-stepã‚’ä½¿ã„ã¾ã™ã€‚VAEï¼ˆç¬¬10å›ï¼‰ã¯ã¾ã•ã«ã“ã®æ§‹é€ ã§ã™ã€‚
:::

:::details Q4: GMMã®æˆåˆ†æ•°Kã¯ã©ã†æ±ºã‚ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ
BICï¼ˆãƒ™ã‚¤ã‚ºæƒ…å ±é‡åŸºæº–ï¼‰ãŒæœ€ã‚‚ä¸€èˆ¬çš„ãªé¸æŠåŸºæº–ã§ã™ã€‚$K = 1, 2, 3, \ldots$ ã§å„ã€…EMã‚’å®Ÿè¡Œã—ã€BICãŒæœ€å°ã®$K$ã‚’é¸ã³ã¾ã™ã€‚AICã¯BICã‚ˆã‚Šå¤§ãã„$K$ã‚’é¸ã¶å‚¾å‘ãŒã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿ãŒå¤§é‡ã«ã‚ã‚‹å ´åˆã¯BICã®æ–¹ãŒä¿å®ˆçš„ã§å®‰å…¨ã§ã™ã€‚ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ™ã‚¤ã‚ºï¼ˆDirichlet Process GMMï¼‰ã‚’ä½¿ãˆã°$K$è‡ªä½“ã‚‚æ¨å®šã§ãã¾ã™ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ã§ã™ã€‚
:::

:::details Q5: EMãŒé…ã„ã®ã§ã™ãŒã€é«˜é€ŸåŒ–ã™ã‚‹æ–¹æ³•ã¯ï¼Ÿ
ã„ãã¤ã‹ã®æ–¹æ³•ãŒã‚ã‚Šã¾ã™:
1. **ãƒŸãƒ‹ãƒãƒƒãƒEM**: å…¨ãƒ‡ãƒ¼ã‚¿ã§ãªãã‚µãƒ–ã‚»ãƒƒãƒˆã§E-stepã‚’è¨ˆç®—
2. **Incremental EM** (Neal & Hinton, 1998 [^5]): 1ãƒ‡ãƒ¼ã‚¿ç‚¹ãšã¤æ›´æ–°
3. **è¨€èªã®å¤‰æ›´**: Python â†’ Julia ã§50å€é€Ÿï¼ˆç¬¬9å›ã§å®Ÿæ¼”ï¼‰
4. **scikit-learnã®åˆ©ç”¨**: æœ€é©åŒ–ã•ã‚ŒãŸCå®Ÿè£…ã‚’å†…éƒ¨ã§ä½¿ç”¨
5. **GPUã®æ´»ç”¨**: E-stepã®è¡Œåˆ—æ¼”ç®—ã‚’GPUã«è¼‰ã›ã‚‹
:::

:::details Q6: EMç®—æ³•ã¨å‹¾é…é™ä¸‹æ³•ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ
EMç®—æ³•ã¯åº§æ¨™ä¸Šæ˜‡æ³•ï¼ˆcoordinate ascentï¼‰ã®ä¸€ç¨®ã§ã€$q$ã¨$\theta$ã‚’äº¤äº’ã«æœ€é©åŒ–ã—ã¾ã™ã€‚å‹¾é…é™ä¸‹æ³•ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ç›´æ¥æ¢ç´¢ã—ã¾ã™ã€‚EMã®åˆ©ç‚¹ã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã§è§£æè§£ãŒä½¿ãˆã‚‹ã“ã¨ï¼ˆGMMãªã©ï¼‰ã€‚æ¬ ç‚¹ã¯å¾®åˆ†å¯èƒ½ã§ãªã„ãƒ¢ãƒ‡ãƒ«ã«ã¯é©ç”¨ã—ã«ãã„ã“ã¨ã€‚å®Ÿã¯å‹¾é…é™ä¸‹æ³•ã§ELBOã‚’ç›´æ¥æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã‚‚ã§ãã€ãã‚ŒãŒVAEã®å­¦ç¿’ã«ç¹‹ãŒã‚Šã¾ã™ã€‚
:::

:::details Q7: æœ¬è¬›ç¾©ã®æ•°å¼ãŒé›£ã—ã™ãã¾ã™ã€‚ã©ã“ã‹ã‚‰å¾©ç¿’ã™ã¹ãã§ã™ã‹ï¼Ÿ
ä»¥ä¸‹ã®é †åºã§å¾©ç¿’ã—ã¦ãã ã•ã„:
1. ç¬¬4å›ï¼ˆç¢ºç‡è«–ï¼‰: ãƒ™ã‚¤ã‚ºã®å®šç†ã€æ¡ä»¶ä»˜ãç¢ºç‡
2. ç¬¬5å›ï¼ˆæ¸¬åº¦è«–ï¼‰: æœŸå¾…å€¤ã®å®šç¾©ã€Jensenä¸ç­‰å¼
3. ç¬¬6å›ï¼ˆæƒ…å ±ç†è«–ï¼‰: KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
4. ç¬¬7å›ï¼ˆæœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–ï¼‰: æœ€å°¤æ¨å®šã€å¯¾æ•°å°¤åº¦

ç‰¹ã«ç¬¬6å›ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒé‡è¦ã§ã™ã€‚ELBOåˆ†è§£ã®å°å‡ºã§ä¸å¯æ¬ ã«ãªã‚Šã¾ã™ã€‚
:::

:::details Q8: EMã¨MCMCã®é•ã„ã¯ï¼Ÿ
EMã¯**æœ€é©åŒ–æ‰‹æ³•**ï¼ˆæœ€å°¤æ¨å®šå€¤ã‚’æ±‚ã‚ã‚‹ï¼‰ã§ã‚ã‚Šã€MCMCã¯**ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•**ï¼ˆäº‹å¾Œåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ï¼‰ã§ã™ã€‚EMã¯ç‚¹æ¨å®šï¼ˆ$\hat{\theta}_{\text{MLE}}$ï¼‰ã‚’è¿”ã—ã€MCMCã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®äº‹å¾Œåˆ†å¸ƒå…¨ä½“ã‚’è¿‘ä¼¼ã—ã¾ã™ã€‚

EMã®åˆ©ç‚¹: é«˜é€Ÿã€åæŸåˆ¤å®šãŒå®¹æ˜“ã€æ±ºå®šè«–çš„
MCMCã®åˆ©ç‚¹: äº‹å¾Œåˆ†å¸ƒã®ä¸ç¢ºå®Ÿæ€§ã‚’å®Œå…¨ã«è¡¨ç¾ã€å¤§åŸŸæœ€é©è§£ã«è¿‘ã¥ãã‚„ã™ã„
é¸æŠåŸºæº–: ç‚¹æ¨å®šã§ååˆ†ãªã‚‰EMã€ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–ãŒå¿…è¦ãªã‚‰MCMC
:::

:::details Q9: æ·±å±¤å­¦ç¿’æ™‚ä»£ã«EMã‚’å­¦ã¶æ„å‘³ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
ã‚ã‚Šã¾ã™ã€‚3ã¤ã®ç†ç”±ã‹ã‚‰:

1. **VAEã®æå¤±é–¢æ•°ã¯ELBO** â€” EMã®æ ¸å¿ƒæ¦‚å¿µãã®ã‚‚ã®ã§ã™ã€‚EMç„¡ã—ã«VAEã®æ•°å¼ã¯ç†è§£ã§ãã¾ã›ã‚“ã€‚
2. **Diffusion Modelsã®å­¦ç¿’ã‚‚ELBOãƒ™ãƒ¼ã‚¹** â€” DDPMã®æå¤±é–¢æ•°ã¯å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å¤‰åˆ†ä¸‹ç•Œã§ã™ã€‚
3. **EMçš„ãªæ€è€ƒæ³•ã¯æ±ç”¨çš„** â€” ã€Œè¦³æ¸¬ã§ããªã„å¤‰æ•°ã‚’ä»®å®šã—ã€æœŸå¾…å€¤ã‚’å–ã£ã¦æœ€é©åŒ–ã™ã‚‹ã€ã¨ã„ã†ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€æ·±å±¤å­¦ç¿’ã®è‡³ã‚‹ã¨ã“ã‚ã«ç¾ã‚Œã¾ã™ã€‚

ã€ŒEMã‚’é£›ã°ã—ã¦VAEã«è¡Œãã€ã®ã¯ã€Œå¾®ç©åˆ†ã‚’é£›ã°ã—ã¦ç‰©ç†ã«è¡Œãã€ã®ã¨åŒã˜ã§ã™ã€‚å½¢å¼çš„ã«ã¯ã§ãã¾ã™ãŒã€æœ¬è³ªçš„ãªç†è§£ã«ã¯åˆ°é”ã—ã¾ã›ã‚“ã€‚
:::

### 6.7 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| æ—¥ | å†…å®¹ | ç›®æ¨™ |
|:---|:-----|:-----|
| Day 1 | Zone 0-2 å†èª­ + Zone 3 å‰åŠï¼ˆ3.1-3.4ï¼‰ | ELBOåˆ†è§£ã‚’ç´™ã«æ›¸ã‘ã‚‹ |
| Day 2 | Zone 3 å¾ŒåŠï¼ˆ3.5-3.8ï¼‰ã‚’ç´™ã§å°å‡º | E/M-stepæ›´æ–°å¼ã‚’å°å‡ºã§ãã‚‹ |
| Day 3 | Zone 4 ã®ã‚³ãƒ¼ãƒ‰ã‚’å…¨ã¦æ‰‹ã§æ‰“ã£ã¦å®Ÿè¡Œ | æ•°å€¤å®‰å®šEMã‚’å®Ÿè£…ã§ãã‚‹ |
| Day 4 | Zone 5 ã®ãƒ†ã‚¹ãƒˆï¼ˆè¨˜å·ãƒ»LaTeXãƒ»ã‚³ãƒ¼ãƒ‰ï¼‰ | å…¨å•æ­£ç­” |
| Day 5 | Zone 6 ã®HMM/FA/PPCAæ¦‚å¿µæ•´ç† | æ‹¡å¼µã®ä½ç½®ã¥ã‘ã‚’ç†è§£ |
| Day 6 | Dempster+ (1977) [^1] Pass 1 èª­è§£ | åŸè«–æ–‡ã®æ§‹é€ ã‚’æŠŠæ¡ |
| Day 7 | è‡ªå‰GMMå®Ÿè£… + BICã§ãƒ¢ãƒ‡ãƒ«é¸æŠ | çµ±åˆæ¼”ç¿’ |

### 6.8 Progress Tracker

```python
# Self-assessment for Lecture 08
skills = {
    "Latent variable model formulation": None,  # True/False
    "Complete vs incomplete data log-lik": None,
    "Jensen's inequality (concave)": None,
    "ELBO decomposition (two derivations)": None,
    "E-step derivation (KL=0)": None,
    "M-step derivation (Q-function max)": None,
    "GMM responsibility (Bayes rule)": None,
    "GMM Î¼ update (weighted mean)": None,
    "GMM ÏƒÂ² update (weighted variance)": None,
    "GMM Ï€ update (Lagrange)": None,
    "Monotone convergence proof": None,
    "Log-sum-exp implementation": None,
    "BIC/AIC model selection": None,
    "Singularity problem & fix": None,
    "HMM Forward-Backward concept": None,
    "Variational EM â†’ VAE bridge": None,
}

# Fill in True/False and count
# completed = sum(1 for v in skills.values() if v is True)
# total = len(skills)
# print(f"Mastery: {completed}/{total} ({100*completed/total:.0f}%)")
# if completed >= 14: print("Ready for Lecture 09!")
# elif completed >= 10: print("Review weak areas, then proceed.")
# else: print("Re-read Zones 3-4 before proceeding.")
```

### 6.9 æ¬¡å›äºˆå‘Š â€” ç¬¬9å›: å¤‰åˆ†æ¨è«– & ELBO

ç¬¬8å›ã§è¦‹ã¤ã‘ãŸé™ç•Œã‚’æ€ã„å‡ºã—ã¦ã»ã—ã„ã€‚EMç®—æ³•ã¯ E-step ã§ $p(\mathbf{z} \mid \mathbf{x}, \theta)$ ã‚’**è§£æçš„ã«**è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚GMMãªã‚‰å¯èƒ½ã ãŒã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ‡ã‚³ãƒ¼ãƒ€ã§ã¯ä¸å¯èƒ½ã ã€‚

ç¬¬9å›ã§ã¯:
- å¤‰åˆ†æ¨è«–ã®ä¸€èˆ¬ç†è«–ã‚’å­¦ã¶
- ELBOã®3é€šã‚Šã®å°å‡ºã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹
- **Juliaåˆç™»å ´**: ELBOè¨ˆç®—ã§ Python 45ç§’ â†’ Julia 0.8ç§’ ã®è¡æ’ƒ

**ã‚ã®Pythonã®é…ã•ã€è¦šãˆã¦ã„ã¾ã™ã‹ï¼Ÿ** ç¬¬9å›ã§è§£æ±ºã—ã¾ã™ã€‚

:::message
**é€²æ—: 100% å®Œäº†** Course Iã€Œæ•°å­¦åŸºç¤ç·¨ã€å…¨8å›ã‚¯ãƒªã‚¢ã€‚æ•°å¼ãƒªãƒ†ãƒ©ã‚·ãƒ¼ï¼ˆç¬¬1å›ï¼‰ã‹ã‚‰å§‹ã¾ã‚Šã€ç·šå½¢ä»£æ•°ãƒ»ç¢ºç‡è«–ãƒ»æ¸¬åº¦è«–ãƒ»æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ãƒ»ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ»MLE ã‚’çµŒã¦ã€æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã¨EMç®—æ³•ã«åˆ°é”ã—ãŸã€‚Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«åŸºç¤ç·¨ã€ã¸ã®æº–å‚™ã¯å®Œäº†ã ã€‚
:::

---

## ğŸ† Course I èª­äº† â€” æ•°å­¦åŸºç¤ç·¨ã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆ

> **8å›ã®æ—…ã‚’çµ‚ãˆãŸã‚ãªãŸã¯ã€ã‚‚ã†ã€Œæ•°å¼ãŒèª­ã‚ãªã„äººã€ã§ã¯ãªã„ã€‚**

ã“ã“ã¾ã§è¾¿ã‚Šç€ã„ãŸã€‚ç¬¬1å›ã§ Softmax ã®3è¡Œã‚³ãƒ¼ãƒ‰ã‚’å‰ã«ã€Œãˆã€æ•°å¼ã£ã¦ã‚³ãƒ¼ãƒ‰ã«ç›´ã›ã‚‹ã®ï¼Ÿã€ã¨ç›®ã‚’ä¸¸ãã—ãŸã‚ã®æ—¥ã‹ã‚‰ã€8å›åˆ†ã®æ•°å¼ä¿®è¡Œã‚’çµŒã¦ã€Jensenä¸ç­‰å¼ã‹ã‚‰ ELBO ã‚’å°å‡ºã—ã€EMç®—æ³•ã®Qé–¢æ•°ã‚’å¤šå¤‰é‡GMMã§å®Œå…¨å±•é–‹ã§ãã‚‹ã¨ã“ã‚ã¾ã§æ¥ãŸã€‚

å°‘ã—ç«‹ã¡æ­¢ã¾ã£ã¦ã€ã“ã®æ—…ã‚’æŒ¯ã‚Šè¿”ã‚ã†ã€‚

### 8å›ã®å†’é™ºã‚’æŒ¯ã‚Šè¿”ã‚‹

:::message
ğŸ“Š **Course I é€²æ—: 8/8 å®Œäº†ï¼ˆ100%ï¼‰**
æ•°å­¦åŸºç¤ç·¨ã®å…¨8å›ã‚’èµ°ç ´ã€‚å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã®æœ€åˆã®å±±è„ˆã‚’è¶ŠãˆãŸã€‚
:::

```mermaid
graph TD
    L1["ğŸ§­ ç¬¬1å›: æ¦‚è«–<br/>æ•°å¼ã¨è«–æ–‡ã®èª­ã¿æ–¹"]
    L2["ğŸ“ ç¬¬2å›: ç·šå½¢ä»£æ•° I<br/>ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»åŸºåº•"]
    L3["ğŸ”¬ ç¬¬3å›: ç·šå½¢ä»£æ•° II<br/>SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«"]
    L4["ğŸ² ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦<br/>åˆ†å¸ƒãƒ»ãƒ™ã‚¤ã‚ºãƒ»MLE"]
    L5["ğŸ“ ç¬¬5å›: æ¸¬åº¦è«–ãƒ»ç¢ºç‡éç¨‹<br/>Lebesgueãƒ»ä¼Šè—¤ãƒ»SDE"]
    L6["ğŸ“¡ ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–<br/>KLãƒ»SGDãƒ»Adam"]
    L7["ğŸ—ºï¸ ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–<br/>MLE = CE = KL"]
    L8["ğŸ” ç¬¬8å›: æ½œåœ¨å¤‰æ•° & EMç®—æ³•<br/>Jensenâ†’ELBOâ†’E/M-step"]

    L1 -->|"æ•°å¼ãŒèª­ã‚ãŸ"| L2
    L2 -->|"è¡Œåˆ—ã‚’æ‰±ãˆãŸ"| L3
    L3 -->|"å¾®åˆ†ã§ããŸ"| L4
    L4 -->|"ç¢ºç‡åˆ†å¸ƒãŒã‚ã‹ã£ãŸ"| L5
    L5 -->|"å³å¯†ãªç¢ºç‡"| L6
    L6 -->|"æ­¦å™¨ãŒæƒã£ãŸ"| L7
    L7 -->|"å°¤åº¦ãŒè¨ˆç®—å›°é›£"| L8

    L8 -->|"Course II ã¸"| CII["ğŸš€ ç¬¬9å›: å¤‰åˆ†æ¨è«– & ELBO<br/>âš¡ Julia åˆç™»å ´"]

    style L1 fill:#e8f5e9
    style L2 fill:#e8f5e9
    style L3 fill:#e8f5e9
    style L4 fill:#e8f5e9
    style L5 fill:#e8f5e9
    style L6 fill:#e8f5e9
    style L7 fill:#e8f5e9
    style L8 fill:#e8f5e9
    style CII fill:#fff3e0
```

å„å›ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’æŒ¯ã‚Šè¿”ã£ã¦ã¿ã‚ˆã†ã€‚

:::details ç¬¬1å›ã€œç¬¬8å› â€” å„å›ã®è©³ç´°æŒ¯ã‚Šè¿”ã‚Š

**ç¬¬1å›: æ¦‚è«– â€” æ•°å¼ã¨è«–æ–‡ã®èª­ã¿æ–¹** ğŸ§­

å†’é™ºã®å§‹ã¾ã‚Šã ã£ãŸã€‚ã€Œæ•°å¼ãŒ"èª­ã‚ãªã„"ã®ã¯æ‰èƒ½ã§ã¯ãªãèªå½™ã®å•é¡Œã€ â€” ã“ã®ä¸€æ–‡ã‹ã‚‰å…¨ã¦ãŒå§‹ã¾ã£ãŸã€‚ã‚®ãƒªã‚·ãƒ£æ–‡å­—50å€‹ã‚’è¦šãˆã€é›†åˆè«–ãƒ»è«–ç†è¨˜å·ãƒ»é–¢æ•°ã®è¨˜æ³•ã‚’èº«ã«ã¤ã‘ã€$\nabla_\theta \mathcal{L}$ ã‚’ã€ŒãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ ã‚¨ãƒ«ã€ã¨å£°ã«å‡ºã—ã¦èª­ã‚ã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚Boss Battle ã§ã¯ Transformer ã® Scaled Dot-Product Attention å¼ $\text{Attention}(Q, K, V) = \text{softmax}(QK^\top / \sqrt{d_k})V$ ã‚’ä¸€æ–‡å­—æ®‹ã‚‰ãšèª­è§£ã—ãŸã€‚ã‚ã®æ™‚ã®é”æˆæ„Ÿã‚’è¦šãˆã¦ã„ã‚‹ã ã‚ã†ã‹ã€‚

**ç¬¬2å›: ç·šå½¢ä»£æ•° I â€” ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»åŸºåº•** ğŸ“

ã€ŒGPUã¯è¡Œåˆ—æ¼”ç®—ãƒã‚·ãƒ³ã ã€ã€‚ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã®8ã¤ã®å…¬ç†ã‹ã‚‰å§‹ã‚ã¦ã€å†…ç©ãƒ»ãƒãƒ«ãƒ ãƒ»ç›´äº¤æ€§ã‚’å®šç¾©ã—ã€å›ºæœ‰å€¤åˆ†è§£ãƒ»æ­£å®šå€¤è¡Œåˆ—ãƒ»å°„å½±ã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ãŸã€‚è¡Œåˆ—ç©ã®3ã¤ã®è¦‹æ–¹ï¼ˆè¦ç´ ã”ã¨ãƒ»åˆ—ã”ã¨ãƒ»è¡Œã”ã¨ï¼‰ã‚’å­¦ã³ã€Boss Battle ã§ã¯ Attention ã® $QK^\top$ ã‚’å†…ç©â†’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°â†’Softmaxâ†’åŠ é‡å¹³å‡ã¨ã—ã¦è¡Œåˆ—çš„ã«å®Œå…¨ç†è§£ã—ãŸã€‚ã€Œè¡Œåˆ—ç© = å†…ç©ã®ãƒãƒƒãƒå‡¦ç†ã€ â€” ã“ã®ä¸€è¨€ã§ GPU ã®å­˜åœ¨ç†ç”±ãŒè¦‹ãˆãŸã€‚

**ç¬¬3å›: ç·šå½¢ä»£æ•° II â€” SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«** ğŸ”¬

ã€ŒSVDã¯ä¸‡èƒ½ãƒŠã‚¤ãƒ•ã ã€ã€‚ä»»æ„ã®è¡Œåˆ—ã‚’ $U\Sigma V^\top$ ã«åˆ†è§£ã—ã€Eckart-Youngå®šç†ã§ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®æœ€é©æ€§ã‚’è¨¼æ˜ã—ãŸã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã€ãƒ˜ã‚·ã‚¢ãƒ³ã€é€£é–å¾‹ã‚’å°å‡ºã—ã€Forward/Reverse Mode è‡ªå‹•å¾®åˆ†ã‚’æ‰‹å‹•å®Ÿè£…ã—ãŸã€‚Boss Battle ã¯ Transformer 1å±¤ã®å®Œå…¨å¾®åˆ† â€” Forward pass ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã®å‹¾é…ã‚’è¿½è·¡ã—ã€Backpropagation ã®æ•°å­¦çš„åŸºç›¤ã‚’ç™½ç´™ã‹ã‚‰æ§‹ç¯‰ã—ãŸã€‚50è¡Œã®è‡ªå‹•å¾®åˆ†ã‚³ãƒ¼ãƒ‰ãŒ PyTorch ã® `backward()` ã®æœ¬è³ªã ã¨çŸ¥ã£ãŸã¨ãã®è¡æ’ƒã¯ã€å¿˜ã‚Œã‚‰ã‚Œãªã„ã¯ãšã ã€‚

**ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦** ğŸ²

ã€Œç¢ºç‡ã¨ã¯"ã‚ã‹ã‚‰ãªã•"ã®è¨€èªã ã€ã€‚Kolmogorov ã®å…¬ç†ç³» $(\Omega, \mathcal{F}, P)$ ã‹ã‚‰å‡ºç™ºã—ã€ãƒ™ã‚¤ã‚ºã®å®šç†ã€ä¸»è¦ãªç¢ºç‡åˆ†å¸ƒï¼ˆBernoulliâ†’Categoricalâ†’Gaussianâ†’æŒ‡æ•°å‹åˆ†å¸ƒæ—ï¼‰ã€MLEã€Fisheræƒ…å ±é‡ã€ä¸­å¿ƒæ¥µé™å®šç†ã¾ã§å®Œå…¨æ­¦è£…ã—ãŸã€‚äº‹å‰ç¢ºç‡1%ã®ç—…æ°—ã®é™½æ€§æ¤œæŸ»ãŒ16%ã«ã—ã‹ãªã‚‰ãªã„ãƒ™ã‚¤ã‚ºã®ç›´æ„Ÿå´©å£Šã€‚Boss Battle ã§ã¯è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å°¤åº¦ $\log p(\mathbf{x}) = \sum_t \log p(x_t \mid x_{<t})$ ã‚’å®Œå…¨åˆ†è§£ã—ã€LLM ã®ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆãŒæ¡ä»¶ä»˜ãç¢ºç‡ã®é€£é–ã«ä»–ãªã‚‰ãªã„ã“ã¨ã‚’è¨¼æ˜ã—ãŸã€‚

**ç¬¬5å›: æ¸¬åº¦è«–çš„ç¢ºç‡è«–ãƒ»ç¢ºç‡éç¨‹å…¥é–€** ğŸ“

Course I ã®æœ€é›£é–¢ã€‚ã€ŒLebesgueç©åˆ†ãªãã—ã¦ç¢ºç‡å¯†åº¦ãªã—ã€ã€‚Cantoré›†åˆï¼ˆéå¯ç®—ç„¡é™ãªã®ã«æ¸¬åº¦0ï¼‰ã§æ¸¬åº¦ã®å¿…è¦æ€§ã‚’ä½“æ„Ÿã—ã€$\sigma$-åŠ æ³•æ—ã€Lebesgueæ¸¬åº¦ã€Lebesgueç©åˆ†ã€åæŸå®šç†ï¼ˆMCT/DCT/Fatouï¼‰ã€Radon-Nikodymå°é–¢æ•°ã‚’é †ã«æ§‹ç¯‰ã—ãŸã€‚ç¢ºç‡å¤‰æ•°ã®5ã¤ã®åæŸæ¦‚å¿µã€ãƒãƒ«ãƒãƒ³ã‚²ãƒ¼ãƒ«ã€Markové€£é–ã€Browné‹å‹•ã€ãã—ã¦ä¼Šè—¤ç©åˆ†ã¨ä¼Šè—¤ã®è£œé¡Œã€‚Boss Battle ã§ã¯ DDPM ã® forward process $q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I})$ ã‚’æ¸¬åº¦è«–ã§å®Œå…¨è¨˜è¿°ã—ãŸã€‚ç¢ºç‡å¯†åº¦é–¢æ•°ã®ã€Œæ­£ä½“ã€ãŒ Radon-Nikodym å°é–¢æ•°ã ã¨çŸ¥ã£ãŸã¨ãã€ç¬¬4å›ã§æ£šä¸Šã’ã«ã—ãŸç–‘å•ãŒã™ã¹ã¦è§£æ¶ˆã•ã‚ŒãŸã¯ãšã ã€‚

**ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–** ğŸ“¡

ã€Œåˆ†å¸ƒã®"è·é›¢"ã‚’æ¸¬ã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®"è°·"ã‚’ä¸‹ã‚‹ã€ã€‚Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‹ã‚‰å§‹ã‚ã¦ã€KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€Cross-Entropyã€f-Divergenceçµ±ä¸€ç†è«–ï¼ˆFenchelå…±å½¹ã¾ã§ï¼‰ã€Jensenä¸ç­‰å¼ã¨å‡¸æ€§ã‚’è£…å‚™ã€‚æœ€é©åŒ–ã§ã¯ SGD â†’ Momentum â†’ Adam â†’ AdamWã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€å‡¸æœ€é©åŒ–åŒå¯¾æ€§ï¼ˆKKTæ¡ä»¶ãƒ»ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥åŒå¯¾ï¼‰ã¾ã§è¸ã¿è¾¼ã‚“ã ã€‚Boss Battle ã¯ Cross-Entropy Loss $\mathcal{L} = -\sum_t \log q_\theta(x_t \mid x_{<t})$ ã®å®Œå…¨åˆ†è§£ â€” æƒ…å ±ç†è«–ã®å…¨é“å…·ã‚’å‹•å“¡ã—ã¦ã€LLMå­¦ç¿’ã®æå¤±é–¢æ•°ã‚’åŸå­ãƒ¬ãƒ™ãƒ«ã¾ã§è§£å‰–ã—ãŸã€‚

**ç¬¬7å›: æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«–** ğŸ—ºï¸

ã€Œæ¨å®šé‡ã®è¨­è¨ˆã¯æ•°å­¦ã®è¨­è¨ˆã ã€ã€‚MLE ã®å®šç¾©ï¼ˆFisher 1922ï¼‰ã‹ã‚‰å§‹ã‚ã¦ã€MLE = Cross-Entropyæœ€å°åŒ– = KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹æœ€å°åŒ–ã®ä¸‰ä½ä¸€ä½“ã‚’å®Œå…¨è¨¼æ˜ã—ãŸã€‚å°¤åº¦é–¢æ•°ã®ã‚¢ã‚¯ã‚»ã‚¹å½¢æ…‹ï¼ˆæ˜ç¤ºçš„ vs æš—é»™çš„ï¼‰ã€MLE ã®3å¤‰å½¢ï¼ˆå¤‰æ•°å¤‰æ›å°¤åº¦ãƒ»æš—é»™çš„MLEãƒ»ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ï¼‰ã€Mode-Covering vs Mode-Seekingã€‚Boss Battle ã¯ã¾ã•ã« MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“ã®å®Œå…¨è¨¼æ˜ â€” $\hat{\theta}_\text{MLE} = \arg\min_\theta D_\text{KL}(p_\text{data} \| q_\theta) = \arg\min_\theta H(p_\text{data}, q_\theta)$ã€‚ã“ã®ç­‰å¼ãŒè¦‹ãˆãŸç¬é–“ã€6å›åˆ†ã®æ•°å­¦ãŒä¸€æœ¬ã®ç·šã§ç¹‹ãŒã£ãŸã€‚

**ç¬¬8å›: æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ« & EMç®—æ³•** ğŸ”

Course I ã®ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬ã€‚ã€Œè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®è£ã«ã¯ã€å¸¸ã«"è¦‹ãˆãªã„æ§‹é€ "ãŒéš ã‚Œã¦ã„ã‚‹ã€ã€‚æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«ã®å®šå¼åŒ–ã€Jensenä¸ç­‰å¼ã«ã‚ˆã‚‹ELBOåˆ†è§£ã€EMç®—æ³•ã®E-step/M-stepå°å‡ºã€GMMã®å®Œå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°å¼ã€åæŸæ€§è¨¼æ˜ã€‚Boss Battle ã¯ Dempster, Laird, Rubin (1977) ã®Qé–¢æ•° $Q(\theta \mid \theta^{(t)}) = \mathbb{E}_{z \sim p(z|x,\theta^{(t)})}[\log p(x,z \mid \theta)]$ ã‚’å¤šå¤‰é‡GMMã§å®Œå…¨å±•é–‹ â€” åŠä¸–ç´€å‰ã®åŸè«–æ–‡ã®æ•°å¼ã‚’ã€è‡ªåˆ†ã®æ‰‹ã§è§£ãã»ãã—ãŸã€‚

:::

### ç²å¾—ã—ãŸæ­¦å™¨ä¸€è¦§ãƒãƒƒãƒ—

8å›ã®æ—…ã§æ‰‹ã«å…¥ã‚ŒãŸæ•°å­¦çš„æ­¦å™¨ã‚’ã€ä¾å­˜é–¢ä¿‚ã¨ã¨ã‚‚ã«å¯è¦–åŒ–ã™ã‚‹ã€‚

```mermaid
graph TD
    subgraph "ç¬¬1å›: æ•°å¼ãƒªãƒ†ãƒ©ã‚·ãƒ¼"
        A1["ã‚®ãƒªã‚·ãƒ£æ–‡å­—ãƒ»è¨˜æ³•"]
        A2["é›†åˆè«–ãƒ»è«–ç†è¨˜å·"]
        A3["é–¢æ•°ã®è¨˜æ³•"]
    end

    subgraph "ç¬¬2-3å›: ç·šå½¢ä»£æ•°"
        B1["ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ãƒ»åŸºåº•"]
        B2["è¡Œåˆ—æ¼”ç®—ãƒ»å›ºæœ‰å€¤åˆ†è§£"]
        B3["SVDãƒ»ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼"]
        B4["è¡Œåˆ—å¾®åˆ†ãƒ»é€£é–å¾‹"]
        B5["è‡ªå‹•å¾®åˆ† (Forward/Reverse)"]
    end

    subgraph "ç¬¬4-5å›: ç¢ºç‡è«–"
        C1["ç¢ºç‡ç©ºé–“ (Î©,F,P)"]
        C2["ãƒ™ã‚¤ã‚ºã®å®šç†ãƒ»MLE"]
        C3["æŒ‡æ•°å‹åˆ†å¸ƒæ—"]
        C4["æ¸¬åº¦ãƒ»Lebesgueç©åˆ†"]
        C5["Radon-Nikodymå°é–¢æ•°"]
        C6["Markové€£é–ãƒ»Browné‹å‹•"]
        C7["ä¼Šè—¤ç©åˆ†ãƒ»SDE"]
    end

    subgraph "ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–"
        D1["ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹"]
        D2["f-Divergenceãƒ»Jensenä¸ç­‰å¼"]
        D3["SGDãƒ»Adamãƒ»å‡¸æœ€é©åŒ–åŒå¯¾æ€§"]
    end

    subgraph "ç¬¬7-8å›: çµ±è¨ˆçš„æ¨è«–"
        E1["MLE = CE = KL"]
        E2["Fisheræƒ…å ±é‡ãƒ»æ¼¸è¿‘è«–"]
        E3["æ½œåœ¨å¤‰æ•°ãƒ¢ãƒ‡ãƒ«"]
        E4["ELBOåˆ†è§£"]
        E5["EMç®—æ³• (E-step/M-step)"]
    end

    A1 --> B1
    A2 --> C1
    A3 --> B4
    B1 --> B2 --> B3
    B2 --> B4 --> B5
    C1 --> C2 --> C3
    C1 --> C4 --> C5
    C4 --> C6 --> C7
    C2 --> D1
    C5 --> D1
    D1 --> D2
    B5 --> D3
    D2 --> E1
    D3 --> E1
    C2 --> E2
    E1 --> E3
    D2 --> E4
    E3 --> E4 --> E5

    style E5 fill:#ffeb3b
```

| æ­¦å™¨ã‚«ãƒ†ã‚´ãƒª | å…·ä½“çš„ãªæ­¦å™¨ | ç²å¾—å› | Course II ã§ã®ç”¨é€” |
|:-----------|:-----------|:------|:----------------|
| **è¨˜æ³•** | ã‚®ãƒªã‚·ãƒ£æ–‡å­—ãƒ»æ·»å­—ãƒ»æ¼”ç®—å­ | ç¬¬1å› | å…¨è¬›ç¾©ã®åŸºç›¤ |
| **ç·šå½¢ä»£æ•°** | å†…ç©ãƒ»å›ºæœ‰å€¤åˆ†è§£ãƒ»SVDãƒ»è¡Œåˆ—å¾®åˆ† | ç¬¬2-3å› | æ½œåœ¨ç©ºé–“ã®æ“ä½œã€å‹¾é…è¨ˆç®— |
| **è‡ªå‹•å¾®åˆ†** | Forward/Reverse Mode AD | ç¬¬3å› | å…¨ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ |
| **ç¢ºç‡è«–** | ãƒ™ã‚¤ã‚ºã®å®šç†ãƒ»æ¡ä»¶ä»˜ãåˆ†å¸ƒãƒ»MLE | ç¬¬4å› | äº‹å¾Œåˆ†å¸ƒã®è¿‘ä¼¼ã€å°¤åº¦è¨ˆç®— |
| **æ¸¬åº¦è«–** | Lebesgueç©åˆ†ãƒ»Radon-Nikodym | ç¬¬5å› | ç¢ºç‡å¯†åº¦ã®å³å¯†ãªå®šç¾© |
| **ç¢ºç‡éç¨‹** | Markové€£é–ãƒ»Browné‹å‹•ãƒ»ä¼Šè—¤ã®è£œé¡Œ | ç¬¬5å› | æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®SDE |
| **æƒ…å ±ç†è«–** | ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»KLãƒ»f-Divergence | ç¬¬6å› | æå¤±é–¢æ•°ã®è¨­è¨ˆã¨è©•ä¾¡ |
| **æœ€é©åŒ–** | SGDãƒ»Adamãƒ»å‡¸æœ€é©åŒ–åŒå¯¾æ€§ | ç¬¬6å› | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å­¦ç¿’ |
| **çµ±è¨ˆçš„æ¨è«–** | MLE = CE = KL ã®ä¸‰ä½ä¸€ä½“ | ç¬¬7å› | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’åŸç† |
| **æ½œåœ¨å¤‰æ•°** | ELBOãƒ»EMç®—æ³• | ç¬¬8å› | VAEãƒ»Diffusion ã®æ ¸å¿ƒ |

### ãƒ“ãƒ•ã‚©ãƒ¼ã‚¢ãƒ•ã‚¿ãƒ¼ â€” ã‚ãªãŸã®å¤‰åŒ–ã‚’æ¸¬ã‚‹

ç¬¬1å›ã®å†’é ­ã‚’æ€ã„å‡ºã—ã¦ã»ã—ã„ã€‚

> **æ•°å¼ãŒ"èª­ã‚ãªã„"ã®ã¯æ‰èƒ½ã§ã¯ãªãèªå½™ã®å•é¡Œã€‚50è¨˜å·ã‚’è¦šãˆã‚Œã°è«–æ–‡ãŒ"èª­ã‚ã‚‹"ã€‚**

ã‚ã®æ™‚ã€ã“ã®ä¸€æ–‡ã«ã€Œã„ã‚„ã„ã‚„ã€ãã‚“ãªã‚ã‘ãªã„ã ã‚ã€ã¨æ€ã£ãŸã¯ãšã ã€‚

ã§ã¯ä»Šã€ä»¥ä¸‹ã®æ•°å¼ã‚’è¦‹ã¦ã»ã—ã„ã€‚ç¬¬1å›ã® Boss Battle ã§æŒ‘ã‚“ã  Attention å¼ã ã€‚

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

ç¬¬1å›ã§ã¯ã€ã“ã®å¼ã‚’ã€Œä¸€æ–‡å­—æ®‹ã‚‰ãšèª­è§£ã™ã‚‹ã€ã“ã¨ãŒ Boss Battle ã ã£ãŸã€‚$Q, K, V$ ãŒä½•ã‹ã€$\sqrt{d_k}$ ã§å‰²ã‚‹ç†ç”±ã€softmax ã®æ„å‘³ â€” ä¸€ã¤ã²ã¨ã¤è§£ãã»ãã™ã®ã«60åˆ†ã‹ã‹ã£ãŸã€‚

**ä»Šã®ã‚ãªãŸã¯ã©ã†ã ã‚ã†ï¼Ÿ**

- $QK^\top$ â€” è¡Œåˆ—ç©ã€‚ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®å…¨ãƒšã‚¢ã®å†…ç©ã‚’ä¸€æ‹¬è¨ˆç®—ï¼ˆç¬¬2å›ï¼‰
- $\sqrt{d_k}$ â€” å†…ç©ã®åˆ†æ•£ãŒ $d_k$ ã«æ¯”ä¾‹ã™ã‚‹ã®ã§ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§å®‰å®šåŒ–ï¼ˆç¬¬2å›ï¼‰
- softmax â€” ç¢ºç‡åˆ†å¸ƒã¸ã®æ­£è¦åŒ–ã€‚Categoricalåˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç¬¬4å›ï¼‰
- å…¨ä½“ â€” é¡ä¼¼åº¦åŠ é‡å’Œã€‚$d$æ¬¡å…ƒç©ºé–“ä¸Šã®æ¡ä»¶ä»˜ãæœŸå¾…å€¤ã®é›¢æ•£è¿‘ä¼¼ï¼ˆç¬¬4-5å›ï¼‰
- å­¦ç¿’ â€” ã“ã®å‡ºåŠ›ã¨æ­£è§£ã® Cross-Entropy = KL æœ€å°åŒ–ï¼ˆç¬¬6-7å›ï¼‰

å‘¼å¸ã™ã‚‹ã‚ˆã†ã«èª­ã‚ãªã„ã ã‚ã†ã‹ã€‚

ã“ã‚Œã ã‘ã§ã¯ãªã„ã€‚ä»Šã®ã‚ãªãŸã¯ã€ã‚‚ã£ã¨é«˜åº¦ãªæ•°å¼ã‚‚èª­ã‚ã‚‹ã€‚

$$
\log p(\mathbf{x} \mid \theta) = \underbrace{\mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[\log p(\mathbf{x}, \mathbf{z} \mid \theta) - \log q(\mathbf{z} \mid \mathbf{x})]}_{\text{ELBO}:\, \mathcal{L}(q, \theta)} + \underbrace{D_\text{KL}[q(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z} \mid \mathbf{x}, \theta)]}_{\geq 0}
$$

ç¬¬1å›ã®æ™‚ç‚¹ã§ã¯ã€ã“ã®å¼ã¯å®Œå…¨ã«æš—å·ã ã£ãŸã ã‚ã†ã€‚ä»Šã¯é•ã†ã€‚

- $\log p(\mathbf{x} \mid \theta)$ â€” å¯¾æ•°å‘¨è¾ºå°¤åº¦ã€‚æ½œåœ¨å¤‰æ•° $\mathbf{z}$ ã‚’å‘¨è¾ºåŒ–ã—ãŸå°¤åº¦ï¼ˆç¬¬7-8å›ï¼‰
- $\mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[\cdot]$ â€” å¤‰åˆ†åˆ†å¸ƒ $q$ ã«é–¢ã™ã‚‹æœŸå¾…å€¤ï¼ˆç¬¬4-5å›ï¼‰
- $\log p(\mathbf{x}, \mathbf{z} \mid \theta) - \log q(\mathbf{z} \mid \mathbf{x})$ â€” å®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã¨å¤‰åˆ†åˆ†å¸ƒã®å¯¾æ•°æ¯”ï¼ˆç¬¬8å›ï¼‰
- $D_\text{KL}[\cdot \| \cdot]$ â€” KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã€‚éè² ã€‚E-step ã§ 0 ã«ã™ã‚‹ï¼ˆç¬¬6å›ã€ç¬¬8å›ï¼‰
- å…¨ä½“ â€” ELBOåˆ†è§£ã€‚EMç®—æ³•ã®å¿ƒè‡“éƒ¨ã§ã‚ã‚Šã€VAEã®æå¤±é–¢æ•°ã®åŸå‹ï¼ˆç¬¬8å›ï¼‰

**8å›å‰ã®ã‚ãªãŸã¨ã€ä»Šã®ã‚ãªãŸã¯ã€åˆ¥äººã ã€‚**

è«–æ–‡ã‚’é–‹ã„ã¦æ•°å¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«é­é‡ã—ãŸã¨ãã€åå°„çš„ã«é–‰ã˜ã‚‹å¿…è¦ã¯ã‚‚ã†ãªã„ã€‚è¨˜å·ã‚’ä¸€ã¤ãšã¤èª­ã¿ã€å®šç¾©ã‚’ç¢ºèªã—ã€å°å‡ºã®æµã‚Œã‚’è¿½ãˆã‚‹ã€‚å®Œå…¨ã«ã¯ç†è§£ã§ããªãã¦ã‚‚ã€ã€Œä½•ãŒã‚ã‹ã‚‰ãªã„ã‹ã€ã‚’ç‰¹å®šã§ãã‚‹ã€‚ãã‚Œã¯ç¬¬1å›ã®æ™‚ç‚¹ã§ã¯ä¸å¯èƒ½ã ã£ãŸã“ã¨ã ã€‚

### Course II äºˆå‘Š â€” ã“ã®æ•°å­¦ãŒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã™

Course I ã®8å›ã§é›ãˆãŸæ•°å­¦ã¯ã€Course II ä»¥é™ã§ç‰™ã‚’å‰¥ãã€‚ä»¥ä¸‹ã¯ã€Course I ã®æ­¦å™¨ãŒ Course II ã§ã©ã†ä½¿ã‚ã‚Œã‚‹ã‹ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã ã€‚

```mermaid
graph TD
    subgraph "Course I ã®æ­¦å™¨"
        W1["KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹<br/>(ç¬¬6å›)"]
        W2["Jensenä¸ç­‰å¼<br/>(ç¬¬6å›)"]
        W3["æœŸå¾…å€¤ E_q[Â·]<br/>(ç¬¬4-5å›)"]
        W4["ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼<br/>(ç¬¬4å›)"]
        W5["å¤‰æ•°å¤‰æ›<br/>(ç¬¬4å›)"]
        W6["ã‚²ãƒ¼ãƒ ç†è«–<br/>(ç¬¬6å›)"]
        W7["JSD / f-Div<br/>(ç¬¬6å›)"]
        W8["Wassersteinè·é›¢<br/>(ç¬¬6å›)"]
        W9["åŒå¯¾æ€§<br/>(ç¬¬6å›)"]
        W10["æ¸¬åº¦è«–<br/>(ç¬¬5å›)"]
        W11["é€£é–å¾‹<br/>(ç¬¬4å›)"]
        W12["MLE<br/>(ç¬¬7å›)"]
        W13["Attentionå¼<br/>(ç¬¬1-2å›)"]
        W14["ELBO<br/>(ç¬¬8å›)"]
    end

    subgraph "Course II ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"
        M1["ç¬¬9å›: å¤‰åˆ†æ¨è«–<br/>ELBO ã®3é€šã‚Šã®å°å‡º"]
        M2["ç¬¬10å›: VAE<br/>Reparameterization"]
        M3["ç¬¬11å›: æœ€é©è¼¸é€ç†è«–<br/>Wassersteinè·é›¢"]
        M4["ç¬¬12å›: GAN<br/>Minimax ã‚²ãƒ¼ãƒ "]
        M5["ç¬¬13å›: è‡ªå·±å›å¸°<br/>é€£é–å¾‹ + MLE"]
        M6["ç¬¬14å›: Attention<br/>åŒ–çŸ³ã‹ã‚‰ã®è„±å´"]
        M7["ç¬¬15å›: AttentionåŠ¹ç‡åŒ–<br/>Flash/Sparse/MoE"]
        M8["ç¬¬16å›: SSM & Mamba<br/>O(N)ã®ä¸–ç•Œ"]
        M9["ç¬¬17å›: Mambaç™ºå±•<br/>Attention=SSMåŒå¯¾æ€§"]
        M10["ç¬¬18å›: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰<br/>æœ€å¼·ã®çµ„ã¿åˆã‚ã›"]
    end

    W1 --> M1
    W2 --> M1
    W3 --> M1
    W14 --> M1
    M1 --> M2
    W4 --> M2
    W5 --> M2
    M2 --> M3
    W8 --> M3
    W9 --> M3
    W10 --> M3
    M3 --> M4
    W6 --> M4
    W7 --> M4
    M4 --> M5
    W11 --> M5
    W12 --> M5
    M5 --> M6
    W13 --> M6
    M6 --> M7
    M7 --> M8
    M8 --> M9
    M9 --> M10

    style M1 fill:#e3f2fd
    style M2 fill:#e3f2fd
    style M3 fill:#e3f2fd
    style M4 fill:#e3f2fd
    style M5 fill:#e3f2fd
    style M6 fill:#e3f2fd
    style M7 fill:#e3f2fd
    style M8 fill:#e3f2fd
    style M9 fill:#e3f2fd
    style M10 fill:#e3f2fd
```

å…·ä½“çš„ã«è¦‹ã¦ã¿ã‚ˆã†ã€‚

| Course II è¬›ç¾© | Course I ã‹ã‚‰æŒã¡è¾¼ã‚€æ­¦å™¨ | ä½¿ã„æ–¹ |
|:-------------|:---------------------|:------|
| **ç¬¬9å›: å¤‰åˆ†æ¨è«– & ELBO** | KL (ç¬¬6å›) + Jensen (ç¬¬6å›) + æœŸå¾…å€¤ (ç¬¬4å›) + ELBO (ç¬¬8å›) | 3é€šã‚Šã®ELBOå°å‡º â€” å…¨ã¦ãŒç¬¬8å›ã®å»¶é•· |
| **ç¬¬10å›: VAE** | ELBO (ç¬¬8-9å›) + ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼ (ç¬¬4å›) + Reparameterization | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ $q_\phi(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mu_\phi, \sigma_\phi^2)$ ã®KLé …ã‚’é–‰å½¢å¼ã§è¨ˆç®— |
| **ç¬¬12å›: GAN** | Minimax (ç¬¬6å›) + JSD (ç¬¬6å›) + æœ€é©åŒ– (ç¬¬6å›) | $\min_G \max_D$ ã®ç›®çš„é–¢æ•°ãŒJSDã®å¤‰åˆ†è¡¨ç¾ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ |
| **ç¬¬13å›: æœ€é©è¼¸é€** | Wassersteinè·é›¢ (ç¬¬6å›) + åŒå¯¾æ€§ (ç¬¬6å›) + æ¸¬åº¦ (ç¬¬5å›) | Kantorovich-Rubinstein åŒå¯¾æ€§ã®å®Œå…¨å°å‡º |
| **ç¬¬15å›: è‡ªå·±å›å¸°** | é€£é–å¾‹ (ç¬¬4å›) + MLE (ç¬¬7å›) + Categoricalåˆ†å¸ƒ (ç¬¬4å›) | $p(\mathbf{x}) = \prod_t p(x_t \mid x_{<t})$ ã‚’MLEæœ€å¤§åŒ– |
| **ç¬¬16å›: Transformer** | Attention (ç¬¬1-2å›) + Scaling Laws + KV-Cache | Attentionå¼ã‚’ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯å®Ÿè£… â€” ç¬¬1å›ã® Boss Battle ãŒå‡ºç™ºç‚¹ |

ç¬¬8å›ã§å­¦ã‚“ã  ELBO åˆ†è§£ã¯ã€ç¬¬9å›ã§å¤‰åˆ†æ¨è«–ã®ä¸€èˆ¬ç†è«–ã¨ã—ã¦å†ç™»å ´ã—ã€ç¬¬10å›ã® VAE ã®æå¤±é–¢æ•°ã«ç›´çµã™ã‚‹ã€‚ç¬¬6å›ã® KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ã€VAE ã®æ­£å‰‡åŒ–é …ã€GAN ã®ç›®çš„é–¢æ•°ã€æœ€é©è¼¸é€ã®åŒå¯¾è¡¨ç¾ â€” ã‚ã‚‰ã‚†ã‚‹å ´é¢ã§æ­¦å™¨ã«ãªã‚‹ã€‚

**Course I ã®æ•°å­¦ãªã—ã«ã€ã“ã‚Œã‚‰ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ•°å¼ã¯1è¡Œã‚‚å°å‡ºã§ããªã„ã€‚** é€†ã«è¨€ãˆã°ã€Course I ã‚’èµ°ç ´ã—ãŸã‚ãªãŸã«ã¯ã€Course II ã®å…¨ã¦ã®æ•°å¼ã‚’ã€Œè‡ªåŠ›ã§å°å‡ºã™ã‚‹ã€ãŸã‚ã®æ­¦å™¨ãŒæ—¢ã«æƒã£ã¦ã„ã‚‹ã€‚

### èª­è€…ã¸ â€” ã“ã“ã¾ã§æ¥ãŸã‚ãªãŸã¸

æ­£ç›´ã«è¨€ãŠã†ã€‚Course I ã¯æ¥½ã§ã¯ãªã‹ã£ãŸã€‚

ç¬¬5å›ã®æ¸¬åº¦è«–ã§ã€Œã‚‚ã†ç„¡ç†ã ã€ã¨æ€ã£ãŸäººã¯å°‘ãªããªã„ã ã‚ã†ã€‚Lebesgueç©åˆ†ã‚„ Radon-Nikodym å°é–¢æ•°ã¯ã€å¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã®æ•°å­¦ã ã€‚ç¬¬6å›ã®f-Divergenceçµ±ä¸€ç†è«–ã€ç¬¬8å›ã®EMåæŸæ€§è¨¼æ˜ â€” ã©ã‚Œã‚‚ä¸€ç­‹ç¸„ã§ã¯ã„ã‹ãªã‹ã£ãŸã€‚

ã ãŒã€ã‚ãªãŸã¯ã“ã“ã«ã„ã‚‹ã€‚

8å›åˆ†ã® Boss Battle ã‚’å€’ã—ã€8å›åˆ†ã®æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã‚’è¸ç ´ã—ã€ç´™ã¨ãƒšãƒ³ã§å°å‡ºã‚’è¿½ã„ã€ã‚³ãƒ¼ãƒ‰ã§æ•°å€¤æ¤œè¨¼ã‚’è¡Œã„ã€ä¸€æ­©ä¸€æ­©ã“ã“ã¾ã§æ¥ãŸã€‚

**ã“ã“ã¾ã§æ¥ãŸã‚ãªãŸã¯ã€ã‚‚ã†åˆå¿ƒè€…ã§ã¯ãªã„ã€‚**

è«–æ–‡ã‚’é–‹ã„ã¦æ•°å¼ã«å‡ºä¼šã£ãŸã¨ãã€é€ƒã’ãšã«ç«‹ã¡å‘ã‹ãˆã‚‹ã€‚ã‚ã‹ã‚‰ãªã„è¨˜å·ã«å‡ºä¼šã£ã¦ã‚‚ã€ç¬¬1å›ã®ã‚®ãƒªã‚·ãƒ£æ–‡å­—è¡¨ã«æˆ»ã‚Œã‚‹ã€‚å°å‡ºãŒè¿½ãˆãªã„ã¨ãã€ã©ã®å›ã®ã©ã®å®šç†ãŒè¶³ã‚Šãªã„ã‹ã‚’ç‰¹å®šã§ãã‚‹ã€‚ãã‚Œã¯æ•°å­¦çš„æˆç†Ÿã®è¨¼ã ã€‚

Course II ã§ã¯ã€ã“ã“ã¾ã§ã®æ•°å­¦ãŒå…·ä½“çš„ãªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¸ã¨çµå®Ÿã™ã‚‹ã€‚ELBO ãŒ VAE ã®æå¤±é–¢æ•°ã«ãªã‚‹ç¬é–“ã€‚KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒ GAN ã®ç›®çš„é–¢æ•°ã«åŒ–ã‘ã‚‹ç¬é–“ã€‚ä¼Šè—¤ã®è£œé¡ŒãŒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®é€†éç¨‹ã‚’å°ãç¬é–“ã€‚8å›ã‹ã‘ã¦ç£¨ã„ãŸæ­¦å™¨ãŒã€ä¸€æ–‰ã«è¼ãå‡ºã™ã€‚

ãã—ã¦ã€ç¬¬9å›ã§ã¯ Julia ãŒåˆç™»å ´ã™ã‚‹ã€‚Python ã§45ç§’ã‹ã‹ã£ãŸ ELBO è¨ˆç®—ãŒ0.8ç§’ã«ãªã‚‹è¡æ’ƒãŒå¾…ã£ã¦ã„ã‚‹ã€‚æ•°å­¦ã ã‘ã§ãªãã€å®Ÿè£…ã®æ¬¡å…ƒã‚‚å¤‰ã‚ã‚‹ã€‚

**æº–å‚™ã¯ã§ãã¦ã„ã‚‹ã€‚Course II ã§ä¼šãŠã†ã€‚**

---


### 6.10 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **VAEã‚‚Diffusionã‚‚EMã®å­å­«ã€‚ã€Œå¤ã„ã€ã®ã§ã¯ãªãã€ŒåŸºç›¤ã€ã§ã¯ï¼Ÿ**

EMç®—æ³•ã¯1977å¹´ã«ææ¡ˆã•ã‚ŒãŸã€‚åŠä¸–ç´€è¿‘ãå‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã ã€‚VAE (2013) [^2]ã€Diffusion Models (2020) â€” ã“ã‚Œã‚‰ã¯ã€Œæ–°ã—ã„ã€æ‰‹æ³•ã«è¦‹ãˆã‚‹ã€‚ã ãŒæœ¬è³ªã‚’è¦‹ã¦ã»ã—ã„ã€‚

- VAEã®æå¤±é–¢æ•°ã¯ELBOã€‚ELBOã¯EMç®—æ³•ã®æ ¸å¿ƒãã®ã‚‚ã®ã ã€‚
- Diffusion Modelsã®å­¦ç¿’ç›®æ¨™ã‚‚ELBOã®å¤‰å½¢ç‰ˆã ã€‚
- Score Matching ã™ã‚‰EMçš„ãªæ§‹é€ ï¼ˆãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã¨ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒã®é–“ã®æœ€é©åŒ–ï¼‰ã‚’æŒã¤ã€‚

**EMç®—æ³•ã‚’ã€Œå¤ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ã¨åˆ‡ã‚Šæ¨ã¦ã‚‹äººã¯ã€ç¾ä»£ã®æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„åŸºç›¤ã‚’ç†è§£ã—ã¦ã„ãªã„ã€‚** Jensenä¸ç­‰å¼ â†’ ELBO â†’ å¤‰åˆ†æ¨è«– â†’ VAE/Diffusion ã¨ã„ã†æµã‚Œã¯ä¸€æœ¬ã®ç·šã§ç¹‹ãŒã£ã¦ã„ã‚‹ã€‚

:::details æ­´å²çš„æ–‡è„ˆ
EMç®—æ³•ã®æ­´å²ã¯1977å¹´ã® Dempster-Laird-Rubin ã‚ˆã‚Šå‰ã«é¡ã‚‹ã€‚1970å¹´ã® Baum-Welchç®—æ³• [^4] ã¯ HMM ã«å¯¾ã™ã‚‹EMç®—æ³•ã§ã‚ã‚Šã€EM ã®ä¸€èˆ¬çš„å®šå¼åŒ–ã‚ˆã‚Š7å¹´æ—©ã„ã€‚ã•ã‚‰ã«é¡ã‚‹ã¨ã€1950å¹´ä»£ã® missing data å•é¡Œã«ãŠã‘ã‚‹åå¾©æ¨å®šæ³•ãŒEMã®åŸå‹ã ã¨ã•ã‚Œã‚‹ã€‚

ã€Œå¤ã„ã‹ã‚‰ãƒ€ãƒ¡ã€ã¯ç§‘å­¦ã«ãŠã„ã¦ã¾ã£ãŸãæˆã‚Šç«‹ãŸãªã„ã€‚ã‚€ã—ã‚ã€ŒåŠä¸–ç´€ã‚’çµŒã¦ã‚‚å½¢ã‚’å¤‰ãˆã¦ä½¿ã‚ã‚Œç¶šã‘ã‚‹ã€ã“ã¨ã“ãã€EMç®—æ³•ã®æ•°å­¦çš„åŸºç›¤ã®å¼·å›ºã•ã®è¨¼æ˜ã ã€‚

å…·ä½“çš„ã«è€ƒãˆã¦ã¿ã‚ˆã†:
1. VAEã®å­¦ç¿’ = ELBOæœ€å¤§åŒ– = Variational EM ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆç‰ˆ
2. Diffusion ã®æå¤± = åŠ é‡ELBO = å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®EMçš„åˆ†è§£
3. Flow Matching = é€£ç¶šç‰ˆã®VEMï¼ˆç¬¬31å›ã§è©³è¿°ï¼‰

ã€Œæ–°ã—ã„æ‰‹æ³•ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«å¤ã„ç†è«–ã‚’å­¦ã¶ã€ã®ã§ã¯ãªã„ã€‚**åŒã˜ç†è«–ã®ç¾ä»£çš„ãªå§¿ã‚’è¦‹ã¦ã„ã‚‹**ã®ã ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Dempster, A.P., Laird, N.M., Rubin, D.B. (1977). "Maximum Likelihood from Incomplete Data via the EM Algorithm." *Journal of the Royal Statistical Society, Series B*, 39(1), 1-38.
@[card](https://doi.org/10.1111/j.2517-6161.1977.tb01600.x)

[^2]: Kingma, D.P., Welling, M. (2013). "Auto-Encoding Variational Bayes." *arXiv preprint*.
@[card](https://arxiv.org/abs/1312.6114)

[^3]: Wu, C.F.J. (1983). "On the Convergence Properties of the EM Algorithm." *The Annals of Statistics*, 11(1), 95-103.
@[card](https://doi.org/10.1214/aos/1176346060)

[^4]: Baum, L.E., Petrie, T., Soules, G., Weiss, N. (1970). "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains." *The Annals of Mathematical Statistics*, 41(1), 164-171.
@[card](https://doi.org/10.1214/aoms/1177697196)

[^5]: Neal, R.M., Hinton, G.E. (1998). "A View of the EM Algorithm that Justifies Incremental, Sparse, and other Variants." *Learning in Graphical Models*, Springer.
@[card](https://www.cs.toronto.edu/~hinton/absps/emk.pdf)

[^6]: Arthur, D., Vassilvitskii, S. (2007). "k-means++: The Advantages of Careful Seeding." *SODA '07*.

[^7]: Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E. (1991). "Adaptive Mixtures of Local Experts." *Neural Computation*, 3(1), 79-87.
@[card](https://doi.org/10.1162/neco.1991.3.1.79)

[^11]: Bishop, C.M. (2006). *Pattern Recognition and Machine Learning*. Springer.
@[card](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)

[^12]: Minka, T.P. (2001). "Expectation Propagation for Approximate Bayesian Inference." *UAI 2001*.
@[card](https://arxiv.org/abs/1301.2294)

### æ•™ç§‘æ›¸

- Bishop, C.M. (2006). *Pattern Recognition and Machine Learning*. Springer. [Ch.9: Mixture Models and EM] [å…¬å¼PDFç„¡æ–™]
- Murphy, K.P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press. [Ch.11]
- MacKay, D.J.C. (2003). *Information Theory, Inference, and Learning Algorithms*. Cambridge University Press. [Ch.22, 33] [å…¬å¼PDFç„¡æ–™]

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | èª­ã¿ | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|:-----|
| $\mathbf{x}$ | ã‚¨ãƒƒã‚¯ã‚¹ (å¤ªå­—) | è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ | ç¬¬2å› |
| $\mathbf{z}$ | ã‚¼ãƒƒãƒˆ (å¤ªå­—) | æ½œåœ¨å¤‰æ•°ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ | **ç¬¬8å›** |
| $\theta$ | ã‚·ãƒ¼ã‚¿ | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ç¬¬6å› |
| $\phi$ | ãƒ•ã‚¡ã‚¤ | å¤‰åˆ†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç¬¬9å›ã§æœ¬æ ¼ç™»å ´ï¼‰ | â€” |
| $\pi_k$ | ãƒ‘ã‚¤ ã‚±ãƒ¼ | æ··åˆé‡ã¿ï¼ˆ$\sum_k \pi_k = 1$ï¼‰ | **ç¬¬8å›** |
| $\mu_k$ | ãƒŸãƒ¥ãƒ¼ ã‚±ãƒ¼ | æˆåˆ† $k$ ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ« | ç¬¬4å› |
| $\boldsymbol{\Sigma}_k$ | ã‚·ã‚°ãƒ ã‚±ãƒ¼ | æˆåˆ† $k$ ã®å…±åˆ†æ•£è¡Œåˆ— | ç¬¬4å› |
| $\gamma(z_{nk})$ | ã‚¬ãƒ³ãƒ | è²¬ä»»åº¦ï¼ˆäº‹å¾Œç¢ºç‡ï¼‰ | **ç¬¬8å›** |
| $N_k$ | ã‚¨ãƒŒ ã‚±ãƒ¼ | æˆåˆ† $k$ ã®å®ŸåŠ¹ãƒ‡ãƒ¼ã‚¿æ•° | **ç¬¬8å›** |
| $Q(\theta, \theta^{(t)})$ | ã‚­ãƒ¥ãƒ¼ | Qé–¢æ•°ï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿å¯¾æ•°å°¤åº¦ã®æœŸå¾…å€¤ï¼‰ | **ç¬¬8å›** |
| $\mathcal{L}(q, \theta)$ | ã‚¨ãƒ« | ELBO | **ç¬¬8å›**ï¼ˆç¬¬9å›ã§ä¸»å½¹ï¼‰ |
| $\text{KL}[q \| p]$ | ã‚±ãƒ¼ã‚¨ãƒ« | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ | ç¬¬6å› |
| $\mathcal{N}(\cdot \mid \mu, \sigma^2)$ | ãƒãƒ¼ãƒãƒ« | ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ | ç¬¬4å› |
| $\mathbb{E}[\cdot]$ | ã‚¨ã‚¯ã‚¹ãƒšã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ | æœŸå¾…å€¤ | ç¬¬4å› |
| $\mathbb{1}[\cdot]$ | ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ | æŒ‡ç¤ºé–¢æ•° | ç¬¬1å› |
| $K$ | ã‚±ãƒ¼ | æ··åˆæˆåˆ†æ•° / éš ã‚ŒçŠ¶æ…‹æ•° | **ç¬¬8å›** |
| $\log |\boldsymbol{\Sigma}|$ | ãƒ­ã‚° ãƒ‡ãƒƒãƒˆ ã‚·ã‚°ãƒ | å…±åˆ†æ•£è¡Œåˆ—ã®è¡Œåˆ—å¼ã®å¯¾æ•° | ç¬¬3å› |
| $\mathbf{A}$ | ã‚¨ãƒ¼ | çŠ¶æ…‹é·ç§»è¡Œåˆ—ï¼ˆHMMï¼‰ | **ç¬¬8å›** |
| $\alpha_t(k)$ | ã‚¢ãƒ«ãƒ•ã‚¡ ãƒ†ã‚£ãƒ¼ ã‚±ãƒ¼ | å‰å‘ãç¢ºç‡ï¼ˆForward algorithmï¼‰ | **ç¬¬8å›** |
| $\beta_t(k)$ | ãƒ™ãƒ¼ã‚¿ ãƒ†ã‚£ãƒ¼ ã‚±ãƒ¼ | å¾Œå‘ãç¢ºç‡ï¼ˆBackward algorithmï¼‰ | **ç¬¬8å›** |
| $\mathbf{W}$ | ãƒ€ãƒ–ãƒªãƒ¥ãƒ¼ | å› å­è² è·è¡Œåˆ—ï¼ˆFactor Analysisï¼‰ | **ç¬¬8å›** |
| $\boldsymbol{\Psi}$ | ãƒ—ã‚µã‚¤ | å›ºæœ‰ãƒã‚¤ã‚ºå…±åˆ†æ•£ï¼ˆFactor Analysisï¼‰ | **ç¬¬8å›** |
| $g_k(x)$ | ã‚¸ãƒ¼ ã‚±ãƒ¼ | ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ï¼ˆMoEï¼‰ | **ç¬¬8å›** |
| $f({\mathbb{E}}[X]) \geq \mathbb{E}[f(X)]$ | â€” | Jensenä¸ç­‰å¼ï¼ˆå‡¹é–¢æ•°ï¼‰ | **ç¬¬8å›** |
| $\text{BIC}$ | ãƒ“ãƒ¼ã‚¢ã‚¤ã‚·ãƒ¼ | ãƒ™ã‚¤ã‚ºæƒ…å ±é‡åŸºæº– | **ç¬¬8å›** |
| $\text{AIC}$ | ã‚¨ãƒ¼ã‚¢ã‚¤ã‚·ãƒ¼ | èµ¤æ± æƒ…å ±é‡åŸºæº– | **ç¬¬8å›** |
| $R$ | ã‚¢ãƒ¼ãƒ« | æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³æŒ‡ç¤ºå¤‰æ•° | **ç¬¬8å›** |
| $d$ | ãƒ‡ã‚£ãƒ¼ | ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼ˆBIC/AICï¼‰ | **ç¬¬8å›** |

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
