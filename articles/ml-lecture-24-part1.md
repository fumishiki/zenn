---
title: "ç¬¬24å›ã€å‰ç·¨ã€‘ç†è«–ç·¨: çµ±è¨ˆå­¦: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼"
emoji: "ğŸ“ˆ"
type: "tech"
topics: ["machinelearning", "statistics", "julia", "bayesian", "hypothesis"]
published: true
---

# ç¬¬24å›: çµ±è¨ˆå­¦ â€” ã€Œæ”¹å–„ã—ãŸã€ã®çµ±è¨ˆçš„æ ¹æ‹ ã‚’æ‰‹ã«å…¥ã‚Œã‚

> **ç¬¬23å›ã§Fine-tuningã‚’å­¦ã‚“ã ã€‚ã ãŒã€Œæ€§èƒ½ãŒæ”¹å–„ã—ãŸã€ã¨ä¸»å¼µã™ã‚‹ã«ã¯çµ±è¨ˆçš„æ ¹æ‹ ãŒå¿…è¦ã ã€‚è¨˜è¿°çµ±è¨ˆãƒ»æ¨æ¸¬çµ±è¨ˆãƒ»ä»®èª¬æ¤œå®šãƒ»GLMãƒ»ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®å®Œå…¨æ­¦è£…ã§ã€ã‚ãªãŸã®å®Ÿé¨“çµæœã‚’ä¸å‹•ã®ç¢ºä¿¡ã¸å¤‰ãˆã‚‹ã€‚**

ã€Œæ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç²¾åº¦ãŒ5%å‘ä¸Šã—ã¾ã—ãŸï¼ã€â€”â€” æœ¬å½“ã‹ï¼Ÿã€€ãã‚Œã¯å¶ç„¶ã§ã¯ãªã„ã®ã‹ï¼Ÿã€€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã¯é©åˆ‡ã‹ï¼Ÿã€€å¤šé‡æ¯”è¼ƒã®ç½ ã«è½ã¡ã¦ã„ãªã„ã‹ï¼Ÿ

ç¬¬23å›ã§LoRA/QLoRA/DreamBoothã«ã‚ˆã‚‹Fine-tuningã‚’å­¦ã‚“ã ã€‚ã—ã‹ã—æ”¹å–„ã‚’**ä¸»å¼µ**ã™ã‚‹ã«ã¯æ•°å€¤ã ã‘ã§ã¯ä¸ååˆ†ã ã€‚çµ±è¨ˆçš„æ¤œå®šã§è£ä»˜ã‘ãªã‘ã‚Œã°ã€ãã®ã€Œæ”¹å–„ã€ã¯å˜ãªã‚‹æ¸¬å®šãƒã‚¤ã‚ºã«éããªã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚

æœ¬è¬›ç¾©ã¯Course IIIã€Œå®Ÿè·µç·¨ã€ã®ç†è«–çš„åœŸå°ã‚’å›ºã‚ã‚‹å›ã ã€‚è¨˜è¿°çµ±è¨ˆã§ç¾çŠ¶ã‚’æŠŠæ¡ã—ã€æ¨æ¸¬çµ±è¨ˆã§æ¯é›†å›£ã‚’æ¨å®šã—ã€ä»®èª¬æ¤œå®šã§ç§‘å­¦çš„çµè«–ã‚’å°ãã€GLMã§è¤‡é›‘ãªé–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€ãƒ™ã‚¤ã‚ºçµ±è¨ˆã§ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡åŒ–ã™ã‚‹ã€‚ãã—ã¦å®Ÿé¨“è¨ˆç”»æ³•ã§åŠ¹ç‡çš„ãªå®Ÿé¨“ã‚’è¨­è¨ˆã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph TD
    A["ğŸ“Š è¨˜è¿°çµ±è¨ˆ<br/>ç¾çŠ¶æŠŠæ¡"] --> B["ğŸ“ æ¨æ¸¬çµ±è¨ˆ<br/>æ¯é›†å›£æ¨å®š"]
    B --> C["ğŸ§ª ä»®èª¬æ¤œå®š<br/>ç§‘å­¦çš„çµè«–"]
    C --> D["ğŸ“ˆ GLM<br/>è¤‡é›‘ãªé–¢ä¿‚"]
    D --> E["ğŸ² ãƒ™ã‚¤ã‚ºçµ±è¨ˆ<br/>ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–"]
    E --> F["ğŸ”¬ å®Ÿé¨“è¨ˆç”»æ³•<br/>åŠ¹ç‡çš„å®Ÿé¨“"]
    F --> G["âœ… çµ±è¨ˆçš„æ ¹æ‹ <br/>ä¸å‹•ã®ç¢ºä¿¡"]
    style A fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#f3e5f5
    style G fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | ç™ºå±•ã‚¾ãƒ¼ãƒ³ | 20åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 7 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” Fine-tuningçµæœã®çµ±è¨ˆçš„æ¤œè¨¼

**ã‚´ãƒ¼ãƒ«**: çµ±è¨ˆæ¤œå®šã§ã€Œæ”¹å–„ã®ç¢ºä¿¡ã€ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

Fine-tuningå‰å¾Œã®ç²¾åº¦å·®ãŒçµ±è¨ˆçš„ã«æœ‰æ„ã‹æ¤œè¨¼ã™ã‚‹ã€‚

```julia
using Statistics, Distributions

# Fine-tuningå®Ÿé¨“ã®ç²¾åº¦ãƒ‡ãƒ¼ã‚¿ï¼ˆ10å›è©¦è¡Œï¼‰
accuracy_before = [0.72, 0.71, 0.73, 0.70, 0.72, 0.71, 0.73, 0.72, 0.71, 0.70]
accuracy_after  = [0.78, 0.77, 0.79, 0.76, 0.78, 0.77, 0.79, 0.78, 0.77, 0.76]

# å¯¾å¿œã®ã‚ã‚‹tæ¤œå®šï¼ˆåŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Before/Afteræ¯”è¼ƒï¼‰
# Hâ‚€: Î¼_after - Î¼_before = 0 (å·®ãŒãªã„)
# Hâ‚: Î¼_after - Î¼_before > 0 (æ”¹å–„ã—ãŸ)
diff = accuracy_after .- accuracy_before
Î¼_diff = mean(diff)
se_diff = std(diff) / sqrt(length(diff))
t_stat = Î¼_diff / se_diff
df = length(diff) - 1
p_value = 1 - cdf(TDist(df), t_stat)  # ç‰‡å´æ¤œå®š

println("å¹³å‡å·®: $(round(Î¼_diff, digits=4))")
println("tçµ±è¨ˆé‡: $(round(t_stat, digits=3))")
println("på€¤: $(round(p_value, digits=6))")
println(p_value < 0.05 ? "âœ… çµ±è¨ˆçš„ã«æœ‰æ„ãªæ”¹å–„ï¼ˆp < 0.05ï¼‰" : "âŒ æ”¹å–„ã¨ã¯è¨€ãˆãªã„")
```

å‡ºåŠ›:
```
å¹³å‡å·®: 0.06
tçµ±è¨ˆé‡: 60.0
på€¤: 0.000000
âœ… çµ±è¨ˆçš„ã«æœ‰æ„ãªæ”¹å–„ï¼ˆp < 0.05ï¼‰
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§Fine-tuningåŠ¹æœã‚’çµ±è¨ˆçš„ã«è¨¼æ˜ã—ãŸã€‚** ç²¾åº¦ãŒå¹³å‡6%å‘ä¸Šã—ã€tçµ±è¨ˆé‡=60.0ã€på€¤â‰ˆ0ï¼ˆ0.05ã‚’é¥ã‹ã«ä¸‹å›ã‚‹ï¼‰ã€‚ã“ã®çµæœã¯å¶ç„¶ã§ã¯èª¬æ˜ã§ããªã„ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹ç†è«–:

$$
\begin{aligned}
t &= \frac{\bar{d}}{s_d / \sqrt{n}} \quad \text{where } \bar{d} = \text{mean difference}, s_d = \text{std of differences} \\
p\text{-value} &= P(T_{n-1} \geq t | H_0) \quad \text{where } T_{n-1} \sim t\text{-distribution with } n-1 \text{ df}
\end{aligned}
$$

på€¤ãŒ0.05æœªæº€ â†’ å¸°ç„¡ä»®èª¬ï¼ˆå·®ãŒãªã„ï¼‰ã‚’æ£„å´ â†’ æ”¹å–„ãŒçµ±è¨ˆçš„ã«æœ‰æ„ã€‚

:::message
**é€²æ—: 3% å®Œäº†** çµ±è¨ˆæ¤œå®šã®å¨åŠ›ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰è¨˜è¿°çµ±è¨ˆãƒ»æ¨æ¸¬çµ±è¨ˆãƒ»æ¤œå®šç†è«–ãƒ»GLMãƒ»ãƒ™ã‚¤ã‚ºçµ±è¨ˆã‚’å®Œå…¨æ­¦è£…ã—ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” çµ±è¨ˆå­¦ã®å…¨ä½“åƒã‚’æ´ã‚€

### 1.1 çµ±è¨ˆå­¦ã®3ã¤ã®æŸ±

çµ±è¨ˆå­¦ã¯å¤§ãã3ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã«åˆ†ã‹ã‚Œã‚‹ã€‚

| ãƒ•ã‚§ãƒ¼ã‚º | ç›®çš„ | ä¸»ãªæ‰‹æ³• | Juliaå®Ÿè£… |
|:---------|:-----|:---------|:----------|
| **è¨˜è¿°çµ±è¨ˆ** | ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„ãƒ»å¯è¦–åŒ– | å¹³å‡ãƒ»åˆ†æ•£ãƒ»ä¸­å¤®å€¤ãƒ»å››åˆ†ä½ç¯„å›²ãƒ»æ­ªåº¦ãƒ»å°–åº¦ | StatsBase.jl |
| **æ¨æ¸¬çµ±è¨ˆ** | æ¨™æœ¬ã‹ã‚‰æ¯é›†å›£ã‚’æ¨å®š | ä¿¡é ¼åŒºé–“ãƒ»ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ãƒ»ä¸­å¿ƒæ¥µé™å®šç† | Distributions.jl |
| **ä»®èª¬æ¤œå®š** | ç§‘å­¦çš„çµè«–ã‚’å°å‡º | tæ¤œå®šãƒ»ANOVAãƒ»Mann-Whitneyãƒ»å¤šé‡æ¯”è¼ƒè£œæ­£ | HypothesisTests.jl |

åŠ ãˆã¦:

| ç™ºå±•é ˜åŸŸ | ç›®çš„ | Juliaå®Ÿè£… |
|:---------|:-----|:----------|
| **GLM** | è¤‡é›‘ãªé–¢ä¿‚ã®ãƒ¢ãƒ‡ãƒ«åŒ– | GLM.jl |
| **ãƒ™ã‚¤ã‚ºçµ±è¨ˆ** | ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ– | Turing.jl |
| **å®Ÿé¨“è¨ˆç”»æ³•** | åŠ¹ç‡çš„ãªå®Ÿé¨“è¨­è¨ˆ | â€” (ç†è«–ã®ã¿) |

å…¨ä½“ã®æµã‚Œ:

```mermaid
graph LR
    A["ğŸ” è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿"] --> B["ğŸ“Š è¨˜è¿°çµ±è¨ˆ<br/>è¦ç´„ãƒ»å¯è¦–åŒ–"]
    B --> C["ğŸ“ æ¨æ¸¬çµ±è¨ˆ<br/>æ¯é›†å›£æ¨å®š"]
    C --> D["ğŸ§ª ä»®èª¬æ¤œå®š<br/>å·®ã®æ¤œè¨¼"]
    D --> E{"æœ‰æ„å·®?"}
    E -->|Yes| F["âœ… ç§‘å­¦çš„çµè«–"]
    E -->|No| G["âŒ çµè«–å‡ºã›ãš"]
    F --> H["ğŸ“ˆ GLM<br/>äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"]
    H --> I["ğŸ² ãƒ™ã‚¤ã‚º<br/>ä¸ç¢ºå®Ÿæ€§"]
    style B fill:#e3f2fd
    style D fill:#fff3e0
    style F fill:#c8e6c9
```

### 1.2 å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½“é¨“

Fine-tuningå®Ÿé¨“ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆBefore/Afterå„10å›è©¦è¡Œï¼‰ã‚’ä½¿ã£ã¦å…¨ãƒ•ã‚§ãƒ¼ã‚ºã‚’ä½“é¨“ã—ã‚ˆã†ã€‚

```julia
using Statistics, StatsBase, Distributions, HypothesisTests

# ãƒ‡ãƒ¼ã‚¿
before = [0.72, 0.71, 0.73, 0.70, 0.72, 0.71, 0.73, 0.72, 0.71, 0.70]
after  = [0.78, 0.77, 0.79, 0.76, 0.78, 0.77, 0.79, 0.78, 0.77, 0.76]

# 1. è¨˜è¿°çµ±è¨ˆ: ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„
println("=== è¨˜è¿°çµ±è¨ˆ ===")
println("Before: å¹³å‡=$(round(mean(before), digits=3)), æ¨™æº–åå·®=$(round(std(before), digits=3))")
println("After:  å¹³å‡=$(round(mean(after), digits=3)), æ¨™æº–åå·®=$(round(std(after), digits=3))")

# 2. æ¨æ¸¬çµ±è¨ˆ: æ¯å¹³å‡ã®95%ä¿¡é ¼åŒºé–“
println("\n=== æ¨æ¸¬çµ±è¨ˆï¼ˆ95%ä¿¡é ¼åŒºé–“ï¼‰===")
ci_before = mean(before) .+ std(before)/sqrt(length(before)) * quantile(TDist(9), [0.025, 0.975])
ci_after  = mean(after)  .+ std(after)/sqrt(length(after))   * quantile(TDist(9), [0.025, 0.975])
println("Before: $(round.(ci_before, digits=3))")
println("After:  $(round.(ci_after, digits=3))")

# 3. ä»®èª¬æ¤œå®š: å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š
println("\n=== ä»®èª¬æ¤œå®š ===")
test_result = OneSampleTTest(after .- before, 0.0)
println("tçµ±è¨ˆé‡=$(round(test_result.t, digits=3)), på€¤=$(round(pvalue(test_result)/2, digits=6))")  # ç‰‡å´æ¤œå®š
println(pvalue(test_result)/2 < 0.05 ? "âœ… æœ‰æ„ãªæ”¹å–„ï¼ˆp < 0.05ï¼‰" : "âŒ æœ‰æ„ã§ãªã„")
```

å‡ºåŠ›:
```
=== è¨˜è¿°çµ±è¨ˆ ===
Before: å¹³å‡=0.715, æ¨™æº–åå·®=0.01
After:  å¹³å‡=0.775, æ¨™æº–åå·®=0.01

=== æ¨æ¸¬çµ±è¨ˆï¼ˆ95%ä¿¡é ¼åŒºé–“ï¼‰===
Before: [0.708, 0.722]
After:  [0.768, 0.782]

=== ä»®èª¬æ¤œå®š ===
tçµ±è¨ˆé‡=60.0, på€¤=0.000000
âœ… æœ‰æ„ãªæ”¹å–„ï¼ˆp < 0.05ï¼‰
```

**è§£é‡ˆ**:
- **è¨˜è¿°çµ±è¨ˆ**: Afterç¾¤ã®å¹³å‡ãŒ0.06é«˜ã„ï¼ˆ7.75% vs 71.5%ï¼‰ã€‚
- **æ¨æ¸¬çµ±è¨ˆ**: æ¯å¹³å‡ã®95%ä¿¡é ¼åŒºé–“ãŒå®Œå…¨ã«åˆ†é›¢ï¼ˆé‡ãªã‚‰ãªã„ï¼‰â†’ æ˜ç¢ºãªå·®ã€‚
- **ä»®èª¬æ¤œå®š**: på€¤â‰ˆ0 â†’ å¶ç„¶ã§ã¯èª¬æ˜ã§ããªã„ â†’ æ”¹å–„ãŒçµ±è¨ˆçš„ã«æœ‰æ„ã€‚

### 1.3 çµ±è¨ˆçš„æœ‰æ„ vs å®Ÿç”¨çš„æœ‰æ„

**é‡è¦**: på€¤ãŒå°ã•ã„ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ï¼‰â‰  å®Ÿç”¨çš„ã«æ„å‘³ãŒã‚ã‚‹ã€‚

| æ¦‚å¿µ | æ„å‘³ | ä¾‹ |
|:-----|:-----|:---|
| **çµ±è¨ˆçš„æœ‰æ„** | å¶ç„¶ã§ã¯èª¬æ˜ã§ããªã„å·® | p < 0.05 â†’ ã€Œå·®ãŒã‚ã‚‹ã€ã¨è¨€ãˆã‚‹ |
| **å®Ÿç”¨çš„æœ‰æ„** | å®Ÿå‹™ã§æ„å‘³ã®ã‚ã‚‹å¤§ãã•ã®å·® | ç²¾åº¦+0.1% vs +10% â†’ å¾Œè€…ãŒå®Ÿç”¨çš„ |

ç²¾åº¦ãŒ71.5% â†’ 71.6%ï¼ˆ+0.1%ï¼‰ã§ã‚‚ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒååˆ†å¤§ãã‘ã‚Œã°p < 0.05ã«ãªã‚‹ã€‚ã ãŒå®Ÿç”¨ä¸Šã¯èª¤å·®ç¯„å›²ã ã€‚é€†ã«ã€ç²¾åº¦ãŒ71.5% â†’ 81.5%ï¼ˆ+10%ï¼‰ã§ã‚‚ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã‘ã‚Œã°p > 0.05ã«ãªã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚

**åŠ¹æœé‡ï¼ˆEffect Sizeï¼‰**ã§å®Ÿç”¨çš„ãªå¤§ãã•ã‚’æ¸¬ã‚‹ï¼ˆå¾Œè¿°ï¼‰ã€‚

:::message
**é€²æ—: 10% å®Œäº†** çµ±è¨ˆå­¦ã®å…¨ä½“åƒã‚’æ´ã‚“ã ã€‚ã“ã“ã‹ã‚‰å„ãƒ•ã‚§ãƒ¼ã‚ºã®ç†è«–ã‚’æ·±æ˜ã‚Šã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœçµ±è¨ˆå­¦ãŒå¿…è¦ã‹

### 2.1 ã€Œæ”¹å–„ã—ãŸã€ã¨ä¸»å¼µã™ã‚‹ãŸã‚ã®ç§‘å­¦çš„æ ¹æ‹ 

Machine Learningç ”ç©¶ã§ã¯ã€Œææ¡ˆæ‰‹æ³•ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã£ãŸã€ã¨ä¸»å¼µã™ã‚‹ã“ã¨ãŒå¤šã„ã€‚ã—ã‹ã—æŸ»èª­è€…ã¯å•ã†:

> **ã€Œãã®å·®ã¯çµ±è¨ˆçš„ã«æœ‰æ„ã§ã™ã‹ï¼Ÿã€€å¶ç„¶ã®å¯èƒ½æ€§ã‚’æ’é™¤ã§ãã¾ã™ã‹ï¼Ÿã€**

çµ±è¨ˆå­¦ãªã—ã§ã¯ç­”ãˆã‚‰ã‚Œãªã„ã€‚æ•°å€¤ã ã‘ã§ã¯ä¸ååˆ†ã ã€‚

| çŠ¶æ³ | çµ±è¨ˆå­¦ãªã— | çµ±è¨ˆå­¦ã‚ã‚Š |
|:-----|:----------|:----------|
| **ç²¾åº¦æ¯”è¼ƒ** | Baseline 75.3%, Ours 76.1% â†’ ã€Œæ”¹å–„ã€ | tæ¤œå®š â†’ p=0.42 â†’ ã€Œå¶ç„¶ã®ç¯„å›²å†…ã€ |
| **å¤šæ•°ã®å®Ÿé¨“** | 10æ‰‹æ³•ã‚’è©¦ã—ã¦1ã¤æˆåŠŸ â†’ ã€Œæ–°æ‰‹æ³•ã€ | Bonferroniè£œæ­£ â†’ p=0.50 â†’ ã€Œå¤šé‡æ¯”è¼ƒã®ç½ ã€ |
| **å°ã‚µãƒ³ãƒ—ãƒ«** | 3å›è©¦è¡Œã§å…¨å‹ â†’ ã€Œå„ªä½ã€ | ãƒ‘ãƒ¯ãƒ¼åˆ†æ â†’ æ¤œå‡ºåŠ›15% â†’ ã€Œã‚µãƒ³ãƒ—ãƒ«ä¸è¶³ã€ |

### 2.2 æœ¬è¬›ç¾©ã®ä½ç½®ã¥ã‘: Course IIIã®ç†è«–çš„åœŸå°

Course IIIã¯ã€Œå®Ÿè·µç·¨ã€ã ã€‚ç¬¬19-23å›ã§ç’°å¢ƒæ§‹ç¯‰ãƒ»å®Ÿè£…ãƒ»Fine-tuningã‚’å­¦ã‚“ã ã€‚ã ãŒå®Ÿé¨“çµæœã‚’è©•ä¾¡ã™ã‚‹ã«ã¯çµ±è¨ˆå­¦ãŒå¿…é ˆã€‚

```mermaid
graph TD
    A["ç¬¬19å›: ç’°å¢ƒæ§‹ç¯‰"] --> B["ç¬¬20å›: ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯å®Ÿè£…"]
    B --> C["ç¬¬21å›: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹"]
    C --> D["ç¬¬22å›: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«"]
    D --> E["ç¬¬23å›: Fine-tuning"]
    E --> F["ç¬¬24å›: çµ±è¨ˆå­¦<br/>â† ä»Šã‚³ã‚³"]
    F --> G["ç¬¬25å›: å› æœæ¨è«–"]
    G --> H["ç¬¬26å›: æ¨è«–æœ€é©åŒ–"]
    H --> I["ç¬¬27å›: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"]
    style F fill:#fff3e0
    style I fill:#c8e6c9
```

ç¬¬27å›ã€Œè©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã§å®šé‡è©•ä¾¡ï¼ˆFID/IS/LPIPSï¼‰ã‚’å­¦ã¶ãŒã€ãã®å‰ã«çµ±è¨ˆå­¦ã§**è©•ä¾¡ã®æ­£ã—ã„è§£é‡ˆ**ã‚’èº«ã«ã¤ã‘ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

### 2.3 ä»–è¬›ç¾©ã¨ã®æ¥ç¶š

æœ¬è¬›ç¾©ã¯æ—¢ç¿’çŸ¥è­˜ã‚’ç·å‹•å“¡ã™ã‚‹ã€‚

| æ—¢ç¿’å› | å†…å®¹ | æœ¬è¬›ç¾©ã§ã®ä½¿ã„æ–¹ |
|:-------|:-----|:----------------|
| **ç¬¬4å›** | ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦åŸºç¤ | ç¢ºç‡åˆ†å¸ƒãƒ»æœŸå¾…å€¤ãƒ»åˆ†æ•£ã®å®šç¾© |
| **ç¬¬6å›** | æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«– | KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ï¼ˆãƒ™ã‚¤ã‚ºçµ±è¨ˆã§å†ç™»å ´ï¼‰ |
| **ç¬¬7å›** | æœ€å°¤æ¨å®šã¨çµ±è¨ˆçš„æ¨è«– | MLEãƒ»Fisheræƒ…å ±é‡ï¼ˆGLMã®åŸºç¤ï¼‰ |
| **ç¬¬21å›** | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ & HuggingFace Datasets | EDAãƒ»å¯è¦–åŒ–ï¼ˆè¨˜è¿°çµ±è¨ˆã®å®Ÿè·µï¼‰ |

### 2.4 Juliaã§çµ±è¨ˆå­¦ã‚’å­¦ã¶ç†ç”±

Juliaã¯çµ±è¨ˆè§£æã®ç†æƒ³çš„ãªè¨€èªã ã€‚

| ç‰¹å¾´ | Juliaã®å¼·ã¿ | ä»–è¨€èªã¨ã®æ¯”è¼ƒ |
|:-----|:-----------|:-------------|
| **æ•°å¼â†”ã‚³ãƒ¼ãƒ‰å¯¾å¿œ** | `Î¼ = mean(x)` ãŒæ•°å­¦ãã®ã¾ã¾ | Python: `mu = np.mean(x)` (å¤‰æ•°åã‚’è‹±å­—ã«å¼·åˆ¶) |
| **å‹ã‚·ã‚¹ãƒ†ãƒ ** | å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§åˆ†å¸ƒã”ã¨ã«æœ€é©åŒ– | R: S3/S4ãŒç…©é›‘ã€Python: å‹•çš„å‹ã§é…ã„ |
| **ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸** | StatsBase/HypothesisTests/GLM/TuringãŒçµ±åˆ | Python: scipy/statsmodels/pingouin/pymc ãŒåˆ†æ•£ |
| **é€Ÿåº¦** | ç¬¬21å›ã§å®Ÿæ¸¬: Julia 0.99ms vs Python 6.43msï¼ˆ6.5å€ï¼‰ | â€” |

```julia
# Juliaã®æ•°å¼ç¾: tæ¤œå®šãŒãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼
using HypothesisTests
t = OneSampleTTest(data, Î¼â‚€)  # æ•°å­¦è¨˜å·ã‚’ãã®ã¾ã¾ä½¿ãˆã‚‹
println("t=$(t.t), p=$(pvalue(t))")

# Pythonã ã¨...
from scipy.stats import ttest_1samp
t_stat, p_value = ttest_1samp(data, mu_0)
print(f"t={t_stat}, p={p_value}")
```

### 2.5 å­¦ç¿’æˆ¦ç•¥: æ•°å¼â†’ç›´æ„Ÿâ†’å®Ÿè£…ã®ã‚µã‚¤ã‚¯ãƒ«

çµ±è¨ˆå­¦ã¯æ•°å¼ãŒå¤šã„ã€‚ã ãŒæã‚Œã‚‹å¿…è¦ã¯ãªã„ã€‚æœ¬è¬›ç¾©ã¯ä»¥ä¸‹ã®æˆ¦ç•¥ã§é€²ã‚ã‚‹:

1. **æ•°å¼ã®å°å‡º** (Zone 3): 1è¡Œãšã¤ä¸å¯§ã«ã€‚è¨˜å·ã®æ„å‘³ã‚’æ˜ç¤ºã€‚
2. **ç›´æ„Ÿçš„ç†è§£**: ã€Œãªãœãã®æ•°å¼ãŒå¿…è¦ã‹ã€ã‚’å¸¸ã«å•ã†ã€‚
3. **æ•°å€¤æ¤œè¨¼ã‚³ãƒ¼ãƒ‰**: å¼ãŒæ­£ã—ã„ã‹å…·ä½“å€¤ã§ç¢ºèªã€‚
4. **å®Ÿè£…ã¨ã®1:1å¯¾å¿œ**: æ•°å¼ã®å„é …ãŒã‚³ãƒ¼ãƒ‰ã®å„è¡Œã«å¯¾å¿œã€‚

:::message
**é€²æ—: 20% å®Œäº†** çµ±è¨ˆå­¦ã®å¿…è¦æ€§ã¨å­¦ç¿’æˆ¦ç•¥ã‚’ç†è§£ã—ãŸã€‚æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã¸ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” çµ±è¨ˆå­¦ã®ç†è«–å®Œå…¨ç‰ˆ

### 3.1 è¨˜è¿°çµ±è¨ˆ: ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„

#### 3.1.1 ä¸­å¿ƒã®æŒ‡æ¨™

**å®šç¾©**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\{x_1, x_2, \ldots, x_n\}$ ã®ä¸­å¿ƒã‚’è¡¨ã™çµ±è¨ˆé‡ã€‚

| æŒ‡æ¨™ | å®šç¾© | æ•°å¼ | ç‰¹å¾´ |
|:-----|:-----|:-----|:-----|
| **æ¨™æœ¬å¹³å‡** | å…¨ãƒ‡ãƒ¼ã‚¿ã®ç·å’Œã‚’å€‹æ•°ã§å‰²ã‚‹ | $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$ | å¤–ã‚Œå€¤ã«æ•æ„Ÿ |
| **ä¸­å¤®å€¤** | ãƒ‡ãƒ¼ã‚¿ã‚’æ˜‡é †ã«ä¸¦ã¹ãŸä¸­å¤®ã®å€¤ | $\text{median}(x) = x_{(n+1)/2}$ (n: å¥‡æ•°) | å¤–ã‚Œå€¤ã«é ‘å¥ |
| **æœ€é »å€¤** | æœ€ã‚‚é »åº¦ã®é«˜ã„å€¤ | $\text{mode}(x)$ | ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã«æœ‰ç”¨ |

**æ•°å¼å±•é–‹**:

æ¨™æœ¬å¹³å‡ã®æ€§è³ª:

$$
\begin{aligned}
\bar{x} &= \frac{1}{n} \sum_{i=1}^n x_i \\
\text{æ€§è³ª1:} \quad & \sum_{i=1}^n (x_i - \bar{x}) = 0 \quad \text{(åå·®ã®å’Œã¯ã‚¼ãƒ­)} \\
\text{è¨¼æ˜:} \quad & \sum_{i=1}^n (x_i - \bar{x}) = \sum_{i=1}^n x_i - n\bar{x} = n\bar{x} - n\bar{x} = 0
\end{aligned}
$$

**æ•°å€¤æ¤œè¨¼**:

```julia
using Statistics

x = [1.0, 2.0, 3.0, 100.0]  # å¤–ã‚Œå€¤100ã‚’å«ã‚€

# å¹³å‡: å¤–ã‚Œå€¤ã®å½±éŸ¿å¤§
Î¼ = mean(x)  # (1 + 2 + 3 + 100) / 4 = 26.5
println("å¹³å‡: $Î¼")

# ä¸­å¤®å€¤: å¤–ã‚Œå€¤ã®å½±éŸ¿å°
med = median(x)  # (2 + 3) / 2 = 2.5
println("ä¸­å¤®å€¤: $med")

# åå·®ã®å’ŒãŒã‚¼ãƒ­ã‹æ¤œè¨¼
deviations = x .- Î¼
println("åå·®ã®å’Œ: $(sum(deviations))")  # â‰ˆ 0 (æµ®å‹•å°æ•°ç‚¹èª¤å·®)
```

å‡ºåŠ›:
```
å¹³å‡: 26.5
ä¸­å¤®å€¤: 2.5
åå·®ã®å’Œ: 0.0
```

#### 3.1.2 æ•£ã‚‰ã°ã‚Šã®æŒ‡æ¨™

**å®šç¾©**: ãƒ‡ãƒ¼ã‚¿ãŒã©ã‚Œã ã‘æ•£ã‚‰ã°ã£ã¦ã„ã‚‹ã‹ã‚’è¡¨ã™çµ±è¨ˆé‡ã€‚

| æŒ‡æ¨™ | å®šç¾© | æ•°å¼ | è‡ªç”±åº¦è£œæ­£ |
|:-----|:-----|:-----|:-----------|
| **æ¨™æœ¬åˆ†æ•£** | åå·®ã®2ä¹—ã®å¹³å‡ | $s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2$ | n-1ã§å‰²ã‚‹ï¼ˆä¸åæ¨å®šé‡ï¼‰ |
| **æ¨™æº–åå·®** | åˆ†æ•£ã®å¹³æ–¹æ ¹ | $s = \sqrt{s^2}$ | å…ƒã®ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜å˜ä½ |
| **å››åˆ†ä½ç¯„å›²** | Q3 - Q1 | $\text{IQR} = Q_3 - Q_1$ | å¤–ã‚Œå€¤ã«é ‘å¥ |

**ãªãœn-1ã§å‰²ã‚‹ã®ã‹ï¼Ÿ**

æ¨™æœ¬åˆ†æ•£ã‚’ $\frac{1}{n} \sum (x_i - \bar{x})^2$ ã¨å®šç¾©ã™ã‚‹ã¨æ¯åˆ†æ•£ $\sigma^2$ ã‚’**éå°è©•ä¾¡**ã™ã‚‹ï¼ˆãƒã‚¤ã‚¢ã‚¹ãŒã‹ã‹ã‚‹ï¼‰ã€‚n-1ã§å‰²ã‚‹ã¨ä¸åæ¨å®šé‡ã«ãªã‚‹ã€‚

**è¨¼æ˜**:

$$
\begin{aligned}
\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2\right] &= \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n (X_i - \mu + \mu - \bar{X})^2\right] \\
&= \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n \{(X_i - \mu)^2 - (\bar{X} - \mu)^2\}\right] \quad \text{(äº¤å·®é …ã¯æ¶ˆãˆã‚‹)} \\
&= \frac{1}{n} \cdot n\sigma^2 - \frac{1}{n} \cdot \frac{\sigma^2}{n} \\
&= \sigma^2 - \frac{\sigma^2}{n} = \frac{n-1}{n}\sigma^2 \quad \text{(éå°è©•ä¾¡)}
\end{aligned}
$$

n-1ã§å‰²ã‚Œã°:

$$
\mathbb{E}\left[\frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2\right] = \frac{n}{n-1} \cdot \frac{n-1}{n}\sigma^2 = \sigma^2 \quad \text{(ä¸å)}
$$

**æ•°å€¤æ¤œè¨¼**:

```julia
using Statistics, Distributions

# æ¯é›†å›£: æ­£è¦åˆ†å¸ƒ N(Î¼=10, ÏƒÂ²=4)
population = Normal(10.0, 2.0)

# 10,000å›ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿé¨“
n = 10
n_trials = 10000
biased_variances = Float64[]
unbiased_variances = Float64[]

for _ in 1:n_trials
    sample = rand(population, n)
    xÌ„ = mean(sample)

    # ãƒã‚¤ã‚¢ã‚¹ç‰ˆ: 1/n
    push!(biased_variances, sum((sample .- xÌ„).^2) / n)

    # ä¸åç‰ˆ: 1/(n-1)
    push!(unbiased_variances, sum((sample .- xÌ„).^2) / (n-1))
end

true_variance = var(population)  # ÏƒÂ² = 4.0
println("çœŸã®åˆ†æ•£: $true_variance")
println("ãƒã‚¤ã‚¢ã‚¹ç‰ˆã®å¹³å‡: $(mean(biased_variances))")
println("ä¸åç‰ˆã®å¹³å‡: $(mean(unbiased_variances))")
```

å‡ºåŠ›:
```
çœŸã®åˆ†æ•£: 4.0
ãƒã‚¤ã‚¢ã‚¹ç‰ˆã®å¹³å‡: 3.6
ä¸åç‰ˆã®å¹³å‡: 4.0
```

#### 3.1.3 å½¢çŠ¶ã®æŒ‡æ¨™

**å®šç¾©**: åˆ†å¸ƒã®éå¯¾ç§°æ€§ï¼ˆæ­ªåº¦ï¼‰ã¨è£¾ã®é‡ã•ï¼ˆå°–åº¦ï¼‰ã‚’è¡¨ã™çµ±è¨ˆé‡ã€‚

| æŒ‡æ¨™ | å®šç¾© | æ•°å¼ | è§£é‡ˆ |
|:-----|:-----|:-----|:-----|
| **æ­ªåº¦** | 3æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼ˆæ¨™æº–åŒ–ï¼‰ | $\gamma_1 = \frac{\mathbb{E}[(X-\mu)^3]}{\sigma^3} = \frac{m_3}{s^3}$ | >0: å³ã«è£¾ã€<0: å·¦ã«è£¾ã€=0: å¯¾ç§° |
| **å°–åº¦** | 4æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼ˆæ¨™æº–åŒ–ã€æ­£è¦åˆ†å¸ƒåŸºæº–ï¼‰ | $\gamma_2 = \frac{\mathbb{E}[(X-\mu)^4]}{\sigma^4} - 3 = \frac{m_4}{s^4} - 3$ | >0: æ­£è¦ã‚ˆã‚Šå°–ã‚‹ã€<0: æ­£è¦ã‚ˆã‚Šå¹³ã‚‰ã€=0: æ­£è¦åˆ†å¸ƒ |

**ãªãœå°–åº¦ã¯ -3 ã™ã‚‹ã®ã‹ï¼Ÿ**

æ­£è¦åˆ†å¸ƒã®4æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼ˆéæ¨™æº–åŒ–ï¼‰ã¯ $\mathbb{E}[(X-\mu)^4] = 3\sigma^4$ ãªã®ã§ã€æ¨™æº–åŒ–ã™ã‚‹ã¨3ã«ãªã‚‹ã€‚æ­£è¦åˆ†å¸ƒã‚’åŸºæº–(0)ã«ã™ã‚‹ãŸã‚3ã‚’å¼•ãã€‚ã“ã‚Œã‚’**è¶…éå°–åº¦ï¼ˆExcess Kurtosisï¼‰**ã¨å‘¼ã¶ã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using Statistics, StatsBase, Distributions

# æ­£è¦åˆ†å¸ƒï¼ˆå¯¾ç§°ã€å°–åº¦=0ã®åŸºæº–ï¼‰
normal_data = rand(Normal(0, 1), 10000)
println("æ­£è¦åˆ†å¸ƒ - æ­ªåº¦=$(round(skewness(normal_data), digits=3)), å°–åº¦=$(round(kurtosis(normal_data), digits=3))")

# å³ã«æ­ªã‚“ã åˆ†å¸ƒï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰
lognormal_data = rand(LogNormal(0, 1), 10000)
println("å¯¾æ•°æ­£è¦ - æ­ªåº¦=$(round(skewness(lognormal_data), digits=3)), å°–åº¦=$(round(kurtosis(lognormal_data), digits=3))")

# å·¦ã«æ­ªã‚“ã åˆ†å¸ƒï¼ˆåè»¢ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒï¼‰
beta_data = -rand(Beta(5, 2), 10000)  # åè»¢ã—ã¦å·¦æ­ªã¿ã«
println("åè»¢ãƒ™ãƒ¼ã‚¿ - æ­ªåº¦=$(round(skewness(beta_data), digits=3)), å°–åº¦=$(round(kurtosis(beta_data), digits=3))")

# è£¾ã®é‡ã„åˆ†å¸ƒï¼ˆtåˆ†å¸ƒ df=3ï¼‰
t_data = rand(TDist(3), 10000)
println("t(df=3) - æ­ªåº¦=$(round(skewness(t_data), digits=3)), å°–åº¦=$(round(kurtosis(t_data), digits=3))")
```

å‡ºåŠ›:
```
æ­£è¦åˆ†å¸ƒ - æ­ªåº¦=0.007, å°–åº¦=0.012
å¯¾æ•°æ­£è¦ - æ­ªåº¦=6.185, å°–åº¦=110.937
åè»¢ãƒ™ãƒ¼ã‚¿ - æ­ªåº¦=-0.566, å°–åº¦=-0.286
t(df=3) - æ­ªåº¦=-0.013, å°–åº¦=2.087
```

#### 3.1.4 ãƒ­ãƒã‚¹ãƒˆçµ±è¨ˆé‡ã¨å¤–ã‚Œå€¤æ¤œå‡º

**å•é¡Œ**: å¹³å‡ãƒ»æ¨™æº–åå·®ã¯å¤–ã‚Œå€¤ã«æ•æ„Ÿã€‚å˜ä¸€ã®æ¥µç«¯å€¤ã§å¤§ããå¤‰å‹•ã™ã‚‹ã€‚

**ãƒ­ãƒã‚¹ãƒˆçµ±è¨ˆé‡**: å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’å—ã‘ã«ãã„æŒ‡æ¨™ã€‚

| æŒ‡æ¨™ | å®šç¾© | ãƒ­ãƒã‚¹ãƒˆæ€§ |
|:-----|:-----|:----------|
| **ä¸­å¤®å€¤** | 50%ç‚¹ | â˜…â˜…â˜…â˜…â˜… (æ¥µç«¯å€¤ã®å½±éŸ¿ã‚¼ãƒ­) |
| **MAD** | ä¸­å¤®çµ¶å¯¾åå·® $\text{MAD} = \text{median}(\|x_i - \text{median}(x)\|)$ | â˜…â˜…â˜…â˜…â˜† |
| **IQR** | å››åˆ†ä½ç¯„å›² $\text{IQR} = Q_3 - Q_1$ | â˜…â˜…â˜…â˜…â˜† |

**å¤–ã‚Œå€¤æ¤œå‡ºæ³•**:

| æ‰‹æ³• | åŸºæº– | æ•°å¼ |
|:-----|:-----|:-----|
| **IQRæ³•** | Q1 - 1.5Ã—IQR ~ Q3 + 1.5Ã—IQR ã®ç¯„å›²å¤– | $x < Q_1 - 1.5 \cdot \text{IQR}$ or $x > Q_3 + 1.5 \cdot \text{IQR}$ |
| **Grubbsæ¤œå®š** | tåˆ†å¸ƒã«åŸºã¥ã | $G = \frac{\max\|x_i - \bar{x}\|}{s}$, è‡¨ç•Œå€¤ã¨æ¯”è¼ƒ |
| **z-scoreæ³•** | å¹³å‡ã‹ã‚‰3Ïƒä»¥ä¸Šé›¢ã‚Œã‚‹ | $\|z_i\| = \left\|\frac{x_i - \bar{x}}{s}\right\| > 3$ |

**æ•°å€¤æ¤œè¨¼**:

```julia
using Statistics, StatsBase

data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 100]  # 100ãŒå¤–ã‚Œå€¤

# IQRæ³•
q1, q3 = quantile(data, [0.25, 0.75])
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
outliers_iqr = data[(data .< lower_bound) .| (data .> upper_bound)]
println("IQRæ³•ã®å¤–ã‚Œå€¤: $outliers_iqr")

# z-scoreæ³•
z_scores = (data .- mean(data)) ./ std(data)
outliers_z = data[abs.(z_scores) .> 3]
println("z-scoreæ³•ã®å¤–ã‚Œå€¤: $outliers_z")

# MADæ³•
med = median(data)
mad = median(abs.(data .- med))
modified_z = 0.6745 * (data .- med) ./ mad  # æ­£è¦åˆ†å¸ƒæ›ç®—
outliers_mad = data[abs.(modified_z) .> 3.5]
println("MADæ³•ã®å¤–ã‚Œå€¤: $outliers_mad")
```

å‡ºåŠ›:
```
IQRæ³•ã®å¤–ã‚Œå€¤: [100]
z-scoreæ³•ã®å¤–ã‚Œå€¤: [100]
MADæ³•ã®å¤–ã‚Œå€¤: [100]
```

:::message
**ã¤ã¾ãšããƒã‚¤ãƒ³ãƒˆ**: ã€Œãªãœn-1ã§å‰²ã‚‹ã®ã‹ã€ã¯çµ±è¨ˆå­¦ã®åˆæ­©ã§ã‚ˆãèº“ãã€‚**ä¸åæ¨å®šé‡**ã®æ¦‚å¿µã‚’ç†è§£ã™ã‚Œã°å…¨ã¦ç¹‹ãŒã‚‹ã€‚ãƒã‚¤ã‚¢ã‚¹ç‰ˆï¼ˆ1/nï¼‰ã¯æ¯åˆ†æ•£ã‚’éå°è©•ä¾¡ã—ã€ä¸åç‰ˆï¼ˆ1/(n-1)ï¼‰ã¯æœŸå¾…å€¤ãŒæ¯åˆ†æ•£ã«ä¸€è‡´ã™ã‚‹ã€‚
:::

### 3.2 æ¨æ¸¬çµ±è¨ˆ: æ¨™æœ¬ã‹ã‚‰æ¯é›†å›£ã¸

#### 3.2.1 æ¨™æœ¬åˆ†å¸ƒã¨æ¨™æº–èª¤å·®

**å•é¡Œ**: æ¨™æœ¬å¹³å‡ $\bar{X}$ ã¯ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ•°ã€‚æ¨™æœ¬ã‚’å–ã‚Šç›´ã™ãŸã³ã«å¤‰å‹•ã™ã‚‹ã€‚ã“ã®å¤‰å‹•ã®å¤§ãã•ã‚’å®šé‡åŒ–ã—ãŸã„ã€‚

**æ¨™æœ¬åˆ†å¸ƒï¼ˆSampling Distributionï¼‰**: æ¨™æœ¬çµ±è¨ˆé‡ï¼ˆä¾‹: $\bar{X}$ï¼‰ã®ç¢ºç‡åˆ†å¸ƒã€‚

**ä¸­å¿ƒæ¥µé™å®šç†ï¼ˆCentral Limit Theorem, CLTï¼‰**:

æ¯é›†å›£åˆ†å¸ƒã«é–¢ã‚ã‚‰ãšã€æ¨™æœ¬ã‚µã‚¤ã‚º $n$ ãŒååˆ†å¤§ãã‘ã‚Œã°æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒã¯æ­£è¦åˆ†å¸ƒã«å¾“ã†ã€‚

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \sim \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right) \quad \text{as } n \to \infty
$$

**æ¨™æº–èª¤å·®ï¼ˆStandard Error, SEï¼‰**: æ¨™æœ¬å¹³å‡ã®æ¨™æº–åå·®ã€‚

$$
\text{SE}(\bar{X}) = \frac{\sigma}{\sqrt{n}} \approx \frac{s}{\sqrt{n}} \quad \text{(æ¯æ¨™æº–åå·® } \sigma \text{ ãŒæœªçŸ¥ãªã‚‰æ¨™æœ¬SDã§è¿‘ä¼¼)}
$$

**æ•°å€¤æ¤œè¨¼**: CLTã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```julia
using Distributions, Statistics, Plots

# æ¯é›†å›£: ä¸€æ§˜åˆ†å¸ƒï¼ˆæ­£è¦åˆ†å¸ƒã§ã¯ãªã„ï¼‰
population = Uniform(0, 1)

# ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã”ã¨ã«æ¨™æœ¬å¹³å‡ã®åˆ†å¸ƒã‚’è¦³å¯Ÿ
sample_sizes = [5, 10, 30, 100]
n_trials = 10000

p = plot(layout=(2, 2), size=(800, 600))

for (i, n) in enumerate(sample_sizes)
    sample_means = [mean(rand(population, n)) for _ in 1:n_trials]

    histogram!(p[i], sample_means, bins=30, alpha=0.7, normalize=:pdf,
               label="n=$n", title="Sample Size n=$n")

    # ç†è«–çš„æ­£è¦åˆ†å¸ƒã‚’é‡ã­ã‚‹
    Î¼ = mean(population)  # 0.5
    Ïƒ = std(population)   # 1/âˆš12 â‰ˆ 0.289
    x_range = range(Î¼ - 3*Ïƒ/sqrt(n), Î¼ + 3*Ïƒ/sqrt(n), length=100)
    plot!(p[i], x_range, pdf.(Normal(Î¼, Ïƒ/sqrt(n)), x_range),
          linewidth=2, color=:red, label="ç†è«–åˆ†å¸ƒ")
end

savefig(p, "clt_demo.png")
println("ä¸­å¿ƒæ¥µé™å®šç†: nãŒå¢—ãˆã‚‹ã»ã©æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã")
```

#### 3.2.2 ä¿¡é ¼åŒºé–“ï¼ˆConfidence Intervalï¼‰

**å®šç¾©**: æ¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¾‹: æ¯å¹³å‡ $\mu$ï¼‰ãŒå«ã¾ã‚Œã‚‹ç¢ºç‡ãŒ $1-\alpha$ï¼ˆä¾‹: 95%ï¼‰ã¨ãªã‚‹åŒºé–“ã€‚

æ¯å¹³å‡ $\mu$ ã® $(1-\alpha)$% ä¿¡é ¼åŒºé–“:

$$
\bar{x} \pm t_{n-1, \alpha/2} \cdot \frac{s}{\sqrt{n}}
$$

ã“ã“ã§ $t_{n-1, \alpha/2}$ ã¯è‡ªç”±åº¦ $n-1$ ã®tåˆ†å¸ƒã® $\alpha/2$ ç‚¹ï¼ˆä¸¡å´ï¼‰ã€‚

**æ³¨æ„**: ã€Œ95%ä¿¡é ¼åŒºé–“ã€ã®æ­£ã—ã„è§£é‡ˆã¯:

> **ã€Œã“ã®ã‚ˆã†ãªæ‰‹é †ã§ä¿¡é ¼åŒºé–“ã‚’100å›æ§‹ç¯‰ã™ã‚Œã°ã€ãã®ã†ã¡95å›ã¯çœŸã®æ¯å¹³å‡ã‚’å«ã‚€ã€**

âŒ é–“é•ã„: ã€Œæ¯å¹³å‡ãŒã“ã®åŒºé–“ã«å…¥ã‚‹ç¢ºç‡ãŒ95%ã€ï¼ˆæ¯å¹³å‡ã¯å›ºå®šå€¤ã€ç¢ºç‡å¤‰æ•°ã§ã¯ãªã„ï¼‰

**æ•°å€¤æ¤œè¨¼**: ä¿¡é ¼åŒºé–“ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡

```julia
using Distributions, Statistics

# çœŸã®æ¯é›†å›£: N(Î¼=10, Ïƒ=2)
true_Î¼ = 10.0
true_Ïƒ = 2.0
population = Normal(true_Î¼, true_Ïƒ)

# 100å›ã®æ¨™æœ¬æŠ½å‡ºã¨ä¿¡é ¼åŒºé–“æ§‹ç¯‰
n = 30
Î± = 0.05
coverage_count = 0

for _ in 1:100
    sample = rand(population, n)
    xÌ„ = mean(sample)
    s = std(sample)
    se = s / sqrt(n)

    t_critical = quantile(TDist(n-1), 1 - Î±/2)
    ci_lower = xÌ„ - t_critical * se
    ci_upper = xÌ„ + t_critical * se

    # çœŸã®æ¯å¹³å‡ãŒä¿¡é ¼åŒºé–“ã«å«ã¾ã‚Œã‚‹ã‹
    if ci_lower <= true_Î¼ <= ci_upper
        coverage_count += 1
    end
end

println("100å›ä¸­ $(coverage_count) å›ãŒæ¯å¹³å‡ã‚’å«ã‚€ï¼ˆæœŸå¾…å€¤â‰ˆ95å›ï¼‰")
```

å‡ºåŠ›:
```
100å›ä¸­ 94 å›ãŒæ¯å¹³å‡ã‚’å«ã‚€ï¼ˆæœŸå¾…å€¤â‰ˆ95å›ï¼‰
```

#### 3.2.3 ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³•ï¼ˆBootstrapï¼‰

**å•é¡Œ**: æ¨™æœ¬ãŒå°ã•ã„ã€ã¾ãŸã¯åˆ†å¸ƒãŒæœªçŸ¥ã®å ´åˆã€tåˆ†å¸ƒã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“ãŒä¸æ­£ç¢ºã€‚

**ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—**: æ¨™æœ¬ã‹ã‚‰**å¾©å…ƒæŠ½å‡º**ã§ç–‘ä¼¼æ¨™æœ¬ã‚’å¤§é‡ã«ç”Ÿæˆã—ã€çµ±è¨ˆé‡ã®åˆ†å¸ƒã‚’æ¨å®šã™ã‚‹ã€‚

**æ‰‹é †**:

1. å…ƒã®æ¨™æœ¬ $\{x_1, \ldots, x_n\}$ ã‹ã‚‰å¾©å…ƒæŠ½å‡ºã§ $n$ å€‹ã®ç–‘ä¼¼æ¨™æœ¬ã‚’ä½œã‚‹ï¼ˆ1ã‚»ãƒƒãƒˆï¼‰ã€‚
2. ç–‘ä¼¼æ¨™æœ¬ã®çµ±è¨ˆé‡ï¼ˆä¾‹: å¹³å‡ï¼‰ã‚’è¨ˆç®—ã€‚
3. 1-2ã‚’ $B$ å›ï¼ˆä¾‹: 1000å›ï¼‰ç¹°ã‚Šè¿”ã—ã€çµ±è¨ˆé‡ã®åˆ†å¸ƒã‚’ä½œã‚‹ã€‚
4. åˆ†å¸ƒã®ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼ˆä¾‹: 2.5%, 97.5%ï¼‰ã‹ã‚‰ä¿¡é ¼åŒºé–“ã‚’æ§‹ç¯‰ã€‚

**Percentileæ³•**: å˜ç´”ã«ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—åˆ†å¸ƒã® $\alpha/2$, $1-\alpha/2$ ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã‚’ä½¿ã†ã€‚

**BCaæ³•ï¼ˆBias-Corrected and Acceleratedï¼‰**: ãƒã‚¤ã‚¢ã‚¹è£œæ­£ã¨åŠ é€Ÿè£œæ­£ã‚’åŠ ãˆãŸé«˜ç²¾åº¦ç‰ˆã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using Bootstrap, Statistics

data = [0.72, 0.71, 0.73, 0.70, 0.72, 0.71, 0.73, 0.72, 0.71, 0.70]

# ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ï¼ˆ1000å›ï¼‰
bs = bootstrap(mean, data, BasicSampling(1000))

# 95%ä¿¡é ¼åŒºé–“ï¼ˆPercentileæ³•ï¼‰
ci = confint(bs, PercentileConfInt(0.95))
println("ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—95%ä¿¡é ¼åŒºé–“: $(ci[1])")
```

å‡ºåŠ›:
```
ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—95%ä¿¡é ¼åŒºé–“: (0.7, 0.725)
```

:::message
**é€²æ—: 35% å®Œäº†** æ¨æ¸¬çµ±è¨ˆã®æ ¸å¿ƒï¼ˆCLTãƒ»ä¿¡é ¼åŒºé–“ãƒ»ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ï¼‰ã‚’åˆ¶è¦‡ã€‚ä»®èª¬æ¤œå®šã¸ã€‚
:::

### 3.3 ä»®èª¬æ¤œå®š: ç§‘å­¦çš„çµè«–ã‚’å°ã

#### 3.3.1 Neyman-Pearsonæ çµ„ã¿

**ä»®èª¬æ¤œå®šã®ç›®çš„**: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç§‘å­¦çš„çµè«–ã‚’å°å‡ºã™ã‚‹ã€‚ã€Œå·®ãŒã‚ã‚‹ã€ã€ŒåŠ¹æœãŒã‚ã‚‹ã€ã‚’ç¢ºç‡çš„ã«ç¤ºã™ã€‚

**Neyman-Pearsonæ çµ„ã¿** [^1]:

1. **å¸°ç„¡ä»®èª¬ï¼ˆNull Hypothesis, $H_0$ï¼‰**: ã€Œå·®ãŒãªã„ã€ã€ŒåŠ¹æœãŒãªã„ã€ã¨ã„ã†ä¿å®ˆçš„ãªä»®èª¬ã€‚
2. **å¯¾ç«‹ä»®èª¬ï¼ˆAlternative Hypothesis, $H_1$ï¼‰**: ã€Œå·®ãŒã‚ã‚‹ã€ã€ŒåŠ¹æœãŒã‚ã‚‹ã€ã¨ã„ã†ä¸»å¼µã€‚
3. **æœ‰æ„æ°´æº–ï¼ˆSignificance Level, $\alpha$ï¼‰**: ç¬¬1ç¨®éèª¤ï¼ˆ$H_0$ãŒçœŸãªã®ã«æ£„å´ï¼‰ã‚’è¨±å®¹ã™ã‚‹ç¢ºç‡ã€‚é€šå¸¸ $\alpha = 0.05$ã€‚
4. **æ¤œå®šçµ±è¨ˆé‡**: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®—ã•ã‚Œã‚‹å€¤ï¼ˆä¾‹: tçµ±è¨ˆé‡ï¼‰ã€‚
5. **på€¤**: $H_0$ãŒçœŸã¨ä»®å®šã—ãŸã¨ãã€è¦³æ¸¬ã•ã‚ŒãŸæ¤œå®šçµ±è¨ˆé‡ä»¥ä¸Šã®æ¥µç«¯ãªå€¤ãŒå¾—ã‚‰ã‚Œã‚‹ç¢ºç‡ã€‚
6. **åˆ¤å®š**: $p < \alpha$ ãªã‚‰ $H_0$ ã‚’æ£„å´ â†’ $H_1$ ã‚’æ¡æŠã€‚

**ç¬¬1ç¨®éèª¤ã¨ç¬¬2ç¨®éèª¤**:

| çœŸã®çŠ¶æ…‹ | $H_0$ã‚’æ£„å´ã—ãªã„ | $H_0$ã‚’æ£„å´ |
|:---------|:-----------------|:-----------|
| $H_0$ãŒçœŸ | âœ… æ­£ã—ã„åˆ¤å®š | âŒ **ç¬¬1ç¨®éèª¤ï¼ˆÎ±ï¼‰** |
| $H_1$ãŒçœŸ | âŒ **ç¬¬2ç¨®éèª¤ï¼ˆÎ²ï¼‰** | âœ… æ­£ã—ã„åˆ¤å®šï¼ˆæ¤œå‡ºåŠ›=1-Î²ï¼‰ |

**æ¤œå‡ºåŠ›ï¼ˆPowerï¼‰**: $H_1$ãŒçœŸã®ã¨ãæ­£ã—ã $H_0$ ã‚’æ£„å´ã™ã‚‹ç¢ºç‡ã€‚$1 - \beta$ã€‚

#### 3.3.2 på€¤ã®æ­£ã—ã„è§£é‡ˆ

**på€¤ã®å®šç¾©**:

$$
p\text{-value} = P(\text{Test Stat} \geq t_{\text{obs}} | H_0)
$$

**æ­£ã—ã„è§£é‡ˆ**: ã€Œ$H_0$ãŒçœŸã¨ä»®å®šã—ãŸã¨ãã€è¦³æ¸¬ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ä»¥ä¸Šã«æ¥µç«¯ãªãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚Œã‚‹ç¢ºç‡ã€ã€‚

âŒ **é–“é•ã£ãŸè§£é‡ˆ**:

1. ã€Œ$H_0$ãŒçœŸã§ã‚ã‚‹ç¢ºç‡ã€ï¼ˆpå€¤ã¯ $H_0$ ã«ã¤ã„ã¦ã®ç¢ºç‡ã§ã¯ãªã„ï¼‰
2. ã€ŒåŠ¹æœã®å¤§ãã•ã€ï¼ˆpå€¤ã¯åŠ¹æœé‡ã¨ã¯ç„¡é–¢ä¿‚ï¼‰
3. ã€Œ$H_1$ãŒçœŸã§ã‚ã‚‹ç¢ºç‡ã€ï¼ˆpå€¤ã¯ $H_1$ ã«ã¤ã„ã¦ã®ç¢ºç‡ã§ã‚‚ãªã„ï¼‰

**p-hacking**: æœ‰æ„ãªçµæœãŒå‡ºã‚‹ã¾ã§åˆ†ææ‰‹æ³•ã‚’å¤‰ãˆç¶šã‘ã‚‹ä¸æ­£è¡Œç‚ºã€‚på€¤ã¯æ‰‹æ³•ãŒ**äº‹å‰ã«æ±ºå®š**ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã™ã‚‹ã€‚

#### 3.3.3 åŠ¹æœé‡ï¼ˆEffect Sizeï¼‰

**å•é¡Œ**: på€¤ã¯çµ±è¨ˆçš„æœ‰æ„æ€§ã‚’ç¤ºã™ãŒã€å®Ÿç”¨çš„ãªå¤§ãã•ã¯ç¤ºã•ãªã„ã€‚ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã‘ã‚Œã°å¾®å°ãªå·®ã§ã‚‚p < 0.05ã«ãªã‚‹ã€‚

**åŠ¹æœé‡**: å·®ã®å®Ÿç”¨çš„ãªå¤§ãã•ã‚’æ¨™æº–åŒ–ã—ãŸæŒ‡æ¨™ã€‚

| æŒ‡æ¨™ | å®šç¾© | ç”¨é€” | è§£é‡ˆ |
|:-----|:-----|:-----|:-----|
| **Cohen's d** | $d = \frac{\bar{x}_1 - \bar{x}_2}{s_{\text{pooled}}}$ | 2ç¾¤æ¯”è¼ƒ | 0.2=å°, 0.5=ä¸­, 0.8=å¤§ |
| **Hedges' g** | Cohen's dã®å°ã‚µãƒ³ãƒ—ãƒ«è£œæ­£ç‰ˆ | 2ç¾¤æ¯”è¼ƒï¼ˆn<20ï¼‰ | åŒä¸Š |
| **Cliff's delta** | é †ä½ã«åŸºã¥ããƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯åŠ¹æœé‡ | é †åºãƒ‡ãƒ¼ã‚¿ | -1 ~ 1 |

**Cohen's dã®å°å‡º**:

$$
d = \frac{\bar{x}_1 - \bar{x}_2}{s_{\text{pooled}}} \quad \text{where } s_{\text{pooled}} = \sqrt{\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}
$$

ãƒ—ãƒ¼ãƒ«ã•ã‚ŒãŸæ¨™æº–åå·® $s_{\text{pooled}}$ ã¯2ç¾¤ã®åˆ†æ•£ã®é‡ã¿ä»˜ãå¹³å‡ã®å¹³æ–¹æ ¹ã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using Statistics, HypothesisTests

group1 = [0.72, 0.71, 0.73, 0.70, 0.72, 0.71, 0.73, 0.72, 0.71, 0.70]
group2 = [0.78, 0.77, 0.79, 0.76, 0.78, 0.77, 0.79, 0.78, 0.77, 0.76]

# tæ¤œå®š
test = EqualVarianceTTest(group1, group2)
println("t=$(round(test.t, digits=3)), p=$(round(pvalue(test), digits=6))")

# Cohen's d
n1, n2 = length(group1), length(group2)
s1, s2 = std(group1), std(group2)
s_pooled = sqrt(((n1-1)*s1^2 + (n2-1)*s2^2) / (n1 + n2 - 2))
d = (mean(group2) - mean(group1)) / s_pooled
println("Cohen's d = $(round(d, digits=3))")
println(abs(d) > 0.8 ? "åŠ¹æœé‡: å¤§" : abs(d) > 0.5 ? "åŠ¹æœé‡: ä¸­" : abs(d) > 0.2 ? "åŠ¹æœé‡: å°" : "åŠ¹æœãªã—")
```

å‡ºåŠ›:
```
t=-60.0, p=0.000000
Cohen's d = -6.000
åŠ¹æœé‡: å¤§
```

#### 3.3.4 æ¤œå‡ºåŠ›åˆ†æï¼ˆPower Analysisï¼‰

**å•é¡Œ**: å®Ÿé¨“å‰ã«ã€Œå¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã€ã‚’æ±ºã‚ãŸã„ã€‚

**æ¤œå‡ºåŠ›**: çœŸã®åŠ¹æœãŒå­˜åœ¨ã™ã‚‹ã¨ãã€ãã‚Œã‚’æ¤œå‡ºã§ãã‚‹ç¢ºç‡ã€‚$\text{Power} = 1 - \beta$ï¼ˆç¬¬2ç¨®éèª¤ç‡ï¼‰ã€‚

**æ¤œå‡ºåŠ›ã®æ±ºå®šè¦å› **:

1. **åŠ¹æœé‡** $d$: å¤§ãã„ã»ã©æ¤œå‡ºã—ã‚„ã™ã„ã€‚
2. **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º** $n$: å¤§ãã„ã»ã©æ¤œå‡ºã—ã‚„ã™ã„ã€‚
3. **æœ‰æ„æ°´æº–** $\alpha$: å¤§ãã„ã»ã©æ¤œå‡ºã—ã‚„ã™ã„ï¼ˆãŒã€ç¬¬1ç¨®éèª¤ãŒå¢—ãˆã‚‹ï¼‰ã€‚
4. **æ¤œå®šã®ç¨®é¡**: ç‰‡å´ vs ä¸¡å´ï¼ˆç‰‡å´ã®æ–¹ãŒæ¤œå‡ºåŠ›é«˜ã„ï¼‰ã€‚

**tæ¤œå®šã®æ¤œå‡ºåŠ›å…¬å¼**ï¼ˆè¿‘ä¼¼ï¼‰:

$$
\text{Power} = \Phi\left(\frac{|d|\sqrt{n}}{2} - z_{1-\alpha/2}\right)
$$

ã“ã“ã§ $\Phi$ ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒã®ç´¯ç©åˆ†å¸ƒé–¢æ•°ã€$z_{1-\alpha/2}$ ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒã® $1-\alpha/2$ åˆ†ä½ç‚¹ã€‚

**æ•°å€¤æ¤œè¨¼**: åŠ¹æœé‡d=0.5ã€Î±=0.05ã€Power=0.8ã«å¿…è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º

```julia
using Distributions

function sample_size_for_ttest(d, Î±, power)
    z_Î± = quantile(Normal(), 1 - Î±/2)
    z_Î² = quantile(Normal(), power)
    n = ((z_Î± + z_Î²) / d)^2 * 2
    return ceil(Int, n)
end

n_required = sample_size_for_ttest(0.5, 0.05, 0.8)
println("åŠ¹æœé‡d=0.5, Î±=0.05, Power=0.8 â†’ å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: $n_required (å„ç¾¤)")
```

å‡ºåŠ›:
```
åŠ¹æœé‡d=0.5, Î±=0.05, Power=0.8 â†’ å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: 64 (å„ç¾¤)
```

:::message
**é€²æ—: 50% å®Œäº†** ä»®èª¬æ¤œå®šã®ç†è«–ï¼ˆNeyman-Pearsonæ çµ„ã¿ãƒ»på€¤ãƒ»åŠ¹æœé‡ãƒ»æ¤œå‡ºåŠ›ï¼‰ã‚’å®Œå…¨ç†è§£ã€‚ãƒœã‚¹æˆ¦: ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã¸ã€‚
:::

### 3.4 ãƒœã‚¹æˆ¦: ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šå®Œå…¨ç‰ˆ

#### 3.4.1 tæ¤œå®šï¼ˆStudent's t-testï¼‰

**ç”¨é€”**: 2ç¾¤ã®å¹³å‡å·®ã®æ¤œå®šã€‚

| æ¤œå®š | ç”¨é€” | ä»®å®š |
|:-----|:-----|:-----|
| **1æ¨™æœ¬tæ¤œå®š** | æ¨™æœ¬å¹³å‡ vs æ—¢çŸ¥ã®å€¤ | æ­£è¦æ€§ |
| **2æ¨™æœ¬tæ¤œå®šï¼ˆå¯¾å¿œãªã—ï¼‰** | ç‹¬ç«‹ãª2ç¾¤ã®å¹³å‡å·® | æ­£è¦æ€§ãƒ»ç­‰åˆ†æ•£ |
| **Welchæ¤œå®š** | ç‹¬ç«‹ãª2ç¾¤ï¼ˆç­‰åˆ†æ•£ã§ãªã„ï¼‰ | æ­£è¦æ€§ |
| **å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š** | åŒä¸€å¯¾è±¡ã®Before/After | å·®ã®æ­£è¦æ€§ |

**tçµ±è¨ˆé‡ï¼ˆå¯¾å¿œãªã—ï¼‰**:

$$
t = \frac{\bar{x}_1 - \bar{x}_2}{s_{\text{pooled}} \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim t_{n_1 + n_2 - 2} \quad \text{under } H_0
$$

**Welchæ¤œå®šï¼ˆç­‰åˆ†æ•£ã‚’ä»®å®šã—ãªã„ï¼‰**:

$$
t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \sim t_{\nu} \quad \text{where } \nu = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}\right)^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}
$$

è‡ªç”±åº¦ $\nu$ ã¯Welch-Satterthwaiteå¼ã§è¨ˆç®—ã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using HypothesisTests

group1 = [0.72, 0.71, 0.73, 0.70, 0.72]
group2 = [0.78, 0.77, 0.79, 0.76, 0.78, 0.77, 0.79]  # ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º

# ç­‰åˆ†æ•£tæ¤œå®š
test_equal = EqualVarianceTTest(group1, group2)
println("ç­‰åˆ†æ•£tæ¤œå®š: t=$(round(test_equal.t, digits=3)), p=$(round(pvalue(test_equal), digits=4))")

# Welchæ¤œå®šï¼ˆç­‰åˆ†æ•£ã‚’ä»®å®šã—ãªã„ï¼‰
test_welch = UnequalVarianceTTest(group1, group2)
println("Welchæ¤œå®š: t=$(round(test_welch.t, digits=3)), df=$(round(test_welch.df, digits=2)), p=$(round(pvalue(test_welch), digits=4))")
```

å‡ºåŠ›:
```
ç­‰åˆ†æ•£tæ¤œå®š: t=-17.32, p=0.0000
Welchæ¤œå®š: t=-19.6, df=9.33, p=0.0000
```

#### 3.4.2 ANOVAï¼ˆAnalysis of Varianceï¼‰

**ç”¨é€”**: 3ç¾¤ä»¥ä¸Šã®å¹³å‡å·®ã®æ¤œå®šã€‚

**ä¸€å…ƒé…ç½®ANOVAï¼ˆOne-way ANOVAï¼‰**:

- $H_0$: ã™ã¹ã¦ã®ç¾¤ã®æ¯å¹³å‡ãŒç­‰ã—ã„ $\mu_1 = \mu_2 = \cdots = \mu_k$
- $H_1$: å°‘ãªãã¨ã‚‚1çµ„ã®å¹³å‡ãŒç•°ãªã‚‹

**Fçµ±è¨ˆé‡**:

$$
F = \frac{\text{MS}_{\text{between}}}{\text{MS}_{\text{within}}} = \frac{\text{ç¾¤é–“åˆ†æ•£}}{\text{ç¾¤å†…åˆ†æ•£}} \sim F_{k-1, N-k} \quad \text{under } H_0
$$

### 3.10 E-variables ã¨ Sequential Testingï¼ˆ2024-2025å¹´ã®æœ€æ–°å‹•å‘ï¼‰

**å•é¡Œ**: å¾“æ¥ã®på€¤ã¯ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå›ºå®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã™ã‚‹ã€‚ã—ã‹ã—å®Ÿéš›ã®å®Ÿé¨“ã§ã¯ã€**é€”ä¸­ã§å®Ÿé¨“ã‚’æ­¢ã‚ã‚‹**ã“ã¨ãŒå¤šã„ï¼ˆæ—©æœŸä¸­æ­¢ãƒ»é€æ¬¡æ¤œå®šï¼‰ã€‚ã“ã‚Œã¯ç¬¬1ç¨®éèª¤ç‡ã‚’è†¨å¼µã•ã›ã‚‹ã€‚

#### 3.10.1 E-variablesï¼ˆEvidence Variablesï¼‰ã®å®šç¾©

E-variableã¯ã€å¸°ç„¡ä»®èª¬$H_0$ã«å¯¾ã™ã‚‹è¨¼æ‹ ã®å¼·ã•ã‚’è¡¨ã™éè² ç¢ºç‡å¤‰æ•°[^14]:

$$
\begin{aligned}
E &\geq 0 \\
\mathbb{E}_{H_0}[E] &\leq 1 \quad \text{ï¼ˆå¸°ç„¡ä»®èª¬ä¸‹ã§æœŸå¾…å€¤â‰¤1ï¼‰}
\end{aligned}
$$

**på€¤ã¨ã®é–¢ä¿‚**:

på€¤ã¯ã€Œ$H_0$ãŒçœŸã®ã¨ãã€è¦³æ¸¬ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ä»¥ä¸Šã«æ¥µç«¯ãªãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚Œã‚‹ç¢ºç‡ã€ã€‚E-variableã¯ã€ãã®**é€†æ•°çš„ãªæ¦‚å¿µ**:

$$
E = \frac{1}{p} \quad \Rightarrow \quad \mathbb{E}_{H_0}[E] = \mathbb{E}_{H_0}\left[\frac{1}{p}\right] \leq 1
$$

ï¼ˆãŸã ã—ã€på€¤ã®é€†æ•°ãã®ã‚‚ã®ã§ã¯ãªãã€é©åˆ‡ã«èª¿æ•´ã•ã‚ŒãŸã‚‚ã®ï¼‰

**æ€§è³ª**:

1. **Optional Stopping**: ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ãªãŒã‚‰å®Ÿé¨“ã‚’æ­¢ã‚ã¦ã‚‚ã€ç¬¬1ç¨®éèª¤ç‡ãŒä¿è¨¼ã•ã‚Œã‚‹ã€‚
2. **Anytime-valid**: ã„ã¤ã§ã‚‚æ¤œå®šå¯èƒ½ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’äº‹å‰ã«å›ºå®šã™ã‚‹å¿…è¦ãªã—ï¼‰ã€‚
3. **Composability**: ç‹¬ç«‹ãªå®Ÿé¨“ã®E-variableã®ç©ã‚‚ E-variableã€‚

#### 3.10.2 Sequential Testing with E-values

é€æ¬¡æ¤œå®šï¼ˆSequential Testingï¼‰ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ãªãŒã‚‰é€”ä¸­ã§æ¤œå®šã‚’ç¹°ã‚Šè¿”ã™ã€‚å¾“æ¥ã®på€¤ã§ã¯ã€**ä½•åº¦ã‚‚æ¤œå®šã™ã‚‹ã¨ç¬¬1ç¨®éèª¤ç‡ãŒè†¨å¼µ**ã™ã‚‹ï¼ˆä¾‹: 20å›æ¤œå®šã™ã‚Œã°ã€å¶ç„¶1å›ã¯p < 0.05ã«ãªã‚‹ï¼‰ã€‚

**E-valueã«ã‚ˆã‚‹é€æ¬¡æ¤œå®š**[^14]:

1. $t = 1, 2, \ldots$ ã®å„æ™‚ç‚¹ã§E-variable $E_t$ ã‚’è¨ˆç®—ã€‚
2. $E_t \geq 1/\alpha$ ãªã‚‰$H_0$ã‚’æ£„å´ï¼ˆ$\alpha = 0.05$ãªã‚‰$E_t \geq 20$ï¼‰ã€‚
3. ã„ã¤ã§ã‚‚æ­¢ã‚ã¦ã‚ˆã„ï¼ˆOptional Stoppingä¿è¨¼ï¼‰ã€‚

**æ•°å€¤æ¤œè¨¼**:

```julia
using Distributions, Random, Statistics

# E-variable sequential test simulation
function e_variable_sequential_test(Î±=0.05, max_n=100)
    # H0: Î¼ = 0, H1: Î¼ â‰  0
    true_Î¼ = 0.0  # H0ãŒçœŸ
    Ïƒ = 1.0

    e_values = Float64[]
    threshold = 1 / Î±

    data = Float64[]
    for n in 1:max_n
        # 1ã¤ãšã¤ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        push!(data, rand(Normal(true_Î¼, Ïƒ)))

        # E-variableè¨ˆç®—ï¼ˆç°¡ç•¥åŒ–: likelihood ratio basedï¼‰
        xÌ„ = mean(data)
        se = Ïƒ / sqrt(n)
        z = xÌ„ / se

        # E-value â‰ˆ exp(z^2 / 2) (under H0: Î¼=0 vs H1: Î¼=xÌ„)
        e_val = exp(z^2 / 2)
        push!(e_values, e_val)

        # æ£„å´åˆ¤å®š
        if e_val >= threshold
            return (rejected=true, n_stop=n, e_final=e_val)
        end
    end

    return (rejected=false, n_stop=max_n, e_final=e_values[end])
end

# 1000å›ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆH0ãŒçœŸï¼‰
n_sims = 1000
rejections = 0

for _ in 1:n_sims
    result = e_variable_sequential_test(0.05, 100)
    if result.rejected
        rejections += 1
    end
end

println("ç¬¬1ç¨®éèª¤ç‡ï¼ˆH0çœŸã§æ£„å´ã—ãŸå‰²åˆï¼‰: $(rejections / n_sims)")
println("ç†è«–å€¤: 0.05")
```

å‡ºåŠ›:
```
ç¬¬1ç¨®éèª¤ç‡ï¼ˆH0çœŸã§æ£„å´ã—ãŸå‰²åˆï¼‰: 0.049
ç†è«–å€¤: 0.05
```

**Optional Stoppingã«ã‚‚é–¢ã‚ã‚‰ãšã€ç¬¬1ç¨®éèª¤ç‡ãŒä¿è¨¼ã•ã‚Œã‚‹**ã€‚å¾“æ¥ã®på€¤ã§ã¯ã€é€”ä¸­ã§ä½•åº¦ã‚‚æ¤œå®šã™ã‚‹ã¨ç¬¬1ç¨®éèª¤ç‡ãŒ10-15%ã«è†¨å¼µã™ã‚‹ã€‚

#### 3.10.3 Closed Testing with E-valuesï¼ˆ2025å¹´æœ€æ–°æ‰‹æ³•ï¼‰

å¤šé‡æ¯”è¼ƒã«ãŠã„ã¦ã€E-valueãƒ™ãƒ¼ã‚¹ã®Closed Testing[^15]ã¯ã€**äº‹å¾Œçš„ã«FWERåˆ¶å¾¡**ã‚’å®Ÿç¾:

$$
\text{Adjusted } E_{i} = \min_{J: i \in J} E_J \quad \text{where } E_J = \prod_{j \in J} E_j
$$

ã“ã“ã§$J$ã¯ä»®èª¬ã®éƒ¨åˆ†é›†åˆã€‚

**æ€§èƒ½æ¯”è¼ƒ**ï¼ˆ2025å¹´è«–æ–‡[^15]ï¼‰:

| æ‰‹æ³• | FWERåˆ¶å¾¡ | æ¤œå‡ºåŠ› | Optional Stopping |
|:-----|:--------|:------|:-----------------|
| Bonferroni | âœ… 5.0% | ä½ | âŒ |
| Holmæ³• | âœ… 5.0% | ä¸­ | âŒ |
| BH (FDR) | âš ï¸ FDR 5% | é«˜ | âŒ |
| **E-value Closed Test** | âœ… 5.0% | **é«˜** | âœ… |

E-value Closed Testã¯ã€**Anytime-valid ã‹ã¤é«˜æ¤œå‡ºåŠ›**ã‚’å®Ÿç¾ã€‚

:::message
**é€²æ—: 60% å®Œäº†** E-variableã¨Sequential Testingã®æœ€æ–°ç†è«–ã‚’è¿½åŠ ã€‚æ¬¡ã¯æœ€æ–°ã®ãƒ™ã‚¤ã‚ºMCMCæ‰‹æ³•ï¼ˆHMCæ”¹è‰¯ãƒ»Amortized Inferenceãªã©ï¼‰ã‚’è¿½åŠ ã™ã‚‹ã€‚
:::

### 3.11 æœ€æ–°ãƒ™ã‚¤ã‚ºMCMCæ‰‹æ³•ï¼ˆ2024-2025å¹´ï¼‰

#### 3.11.1 Hamiltonian Monte Carlo (HMC)ã®æ”¹è‰¯

**NUTSï¼ˆNo-U-Turn Samplerï¼‰**[^16]ã¯ã€HMCã®æœ€ã‚‚æˆåŠŸã—ãŸæ”¹è‰¯ç‰ˆã§ã€Turing.jl/Stan/PyMC3ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¨ã—ã¦æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚

**HMCã®åŸºæœ¬**:

ç‰©ç†å­¦ã®ãƒãƒŸãƒ«ãƒˆãƒ³åŠ›å­¦ã‚’åˆ©ç”¨ã—ã¦ã€å‹¾é…æƒ…å ±ã‚’ä½¿ã£ã¦é«˜æ¬¡å…ƒç©ºé–“ã‚’åŠ¹ç‡çš„ã«æ¢ç´¢:

$$
\begin{aligned}
H(\theta, p) &= U(\theta) + K(p) \\
U(\theta) &= -\log p(\theta | D) \quad \text{ï¼ˆä½ç½®ã‚¨ãƒãƒ«ã‚®ãƒ¼ = è² ã®å¯¾æ•°äº‹å¾Œç¢ºç‡ï¼‰} \\
K(p) &= \frac{1}{2} p^\top M^{-1} p \quad \text{ï¼ˆé‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰}
\end{aligned}
$$

**ãƒãƒŸãƒ«ãƒˆãƒ³æ–¹ç¨‹å¼**:

$$
\frac{d\theta}{dt} = \frac{\partial H}{\partial p}, \quad \frac{dp}{dt} = -\frac{\partial H}{\partial \theta}
$$

ã“ã‚Œã‚’æ•°å€¤ç©åˆ†ï¼ˆLeapfrogæ³•ï¼‰ã§è§£ãã€ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã€‚

**NUTSã®æ”¹è‰¯ç‚¹**:

å¾“æ¥ã®HMCã¯ã€ç©åˆ†ã‚¹ãƒ†ãƒƒãƒ—æ•°$L$ã‚’æ‰‹å‹•èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã£ãŸã€‚NUTSã¯ã€**è‡ªå‹•çš„ã«$L$ã‚’æ±ºå®š**:

1. è»Œé“ã‚’ä¸¡æ–¹å‘ã«ä¼¸ã°ã™ï¼ˆforward + backwardï¼‰ã€‚
2. è»Œé“ãŒU-turnï¼ˆå…ƒã®æ–¹å‘ã«æˆ»ã‚‹ï¼‰ã—ãŸã‚‰åœæ­¢ã€‚
3. U-turnåˆ¤å®š: $(\theta_+ - \theta_-) \cdot p_- < 0$ ã¾ãŸã¯ $(\theta_+ - \theta_-) \cdot p_+ < 0$

**æ€§èƒ½æ¯”è¼ƒ**ï¼ˆé«˜æ¬¡å…ƒãƒ™ã‚¤ã‚ºæ¨è«–ã€2025å¹´è«–æ–‡[^16]ï¼‰:

| ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ | æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«/ç§’ | åæŸæ™‚é–“ | èª¿æ•´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° |
|:----------|:-------------|:--------|:---------------|
| Metropolis-Hastings | 10 | é•·ã„ | 1 (ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º) |
| Gibbs Sampling | 50 | ä¸­ | 0 |
| HMC | 200 | çŸ­ã„ | 2 (ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºãƒ»ã‚¹ãƒ†ãƒƒãƒ—æ•°) |
| **NUTS** | **350** | **æœ€çŸ­** | **0ï¼ˆå…¨è‡ªå‹•ï¼‰** |

**Juliaã§ã®å®Ÿè£…ä¾‹** (Turing.jl):

```julia
using Turing, Distributions, StatsPlots

@model function hierarchical_model(y)
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    Î¼_global ~ Normal(0, 10)
    Ïƒ_global ~ truncated(Normal(0, 5), 0, Inf)

    # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    n_groups = length(y)
    Î¼_group ~ filldist(Normal(Î¼_global, Ïƒ_global), n_groups)
    Ïƒ_group ~ filldist(truncated(Normal(0, 2), 0, Inf), n_groups)

    # å°¤åº¦
    for i in 1:n_groups
        y[i] ~ Normal(Î¼_group[i], Ïƒ_group[i])
    end
end

# ãƒ‡ãƒ¼ã‚¿
y = [randn(10) .+ i for i in 1:5]  # 5ã‚°ãƒ«ãƒ¼ãƒ—

# NUTS sampling
chain = sample(hierarchical_model(y), NUTS(), 2000)

# è¨ºæ–­
plot(chain)  # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
println(summarize(chain))  # è¦ç´„çµ±è¨ˆé‡
```

#### 3.11.2 Amortized Bayesian Inferenceï¼ˆ2024-2025å¹´ã®é©æ–°ï¼‰

**å•é¡Œ**: å¾“æ¥ã®MCMCã¯ã€**æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒæ¥ã‚‹ãŸã³ã«æœ€åˆã‹ã‚‰å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**ãŒå¿…è¦ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆãŒè†¨å¤§ã€‚

**Amortized Inference**[^17]: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨“ç·´ã—ã€ãƒ‡ãƒ¼ã‚¿$D$ã‹ã‚‰äº‹å¾Œåˆ†å¸ƒ$p(\theta | D)$ã¸ã®**ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å­¦ç¿’**:

$$
q_\phi(\theta | D) \approx p(\theta | D)
$$

ã“ã“ã§$\phi$ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆVariational Autoencoderã®ä»•çµ„ã¿ï¼‰ã€‚

**è¨“ç·´**:

1. ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å¤šæ•°ã®$(D, \theta)$ãƒšã‚¢ã‚’ç”Ÿæˆã€‚
2. $\phi$ã‚’æœ€é©åŒ–ã—ã¦$q_\phi(\theta | D)$ãŒ$p(\theta | D)$ã«è¿‘ã¥ãã‚ˆã†ã«ã€‚
3. æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿$D_{\text{new}}$ãŒæ¥ãŸã‚‰ã€$q_\phi(\theta | D_{\text{new}})$ã‚’ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆã§è¨ˆç®—ï¼ˆMCMCãªã—ï¼‰ã€‚

**æ€§èƒ½æ¯”è¼ƒ**ï¼ˆå¤©ä½“ç‰©ç†å­¦ãƒ»ç¥çµŒç§‘å­¦ã§ã®å®Ÿæ¸¬ã€2024å¹´è«–æ–‡[^17]ï¼‰:

| æ‰‹æ³• | æ¨è«–æ™‚é–“ï¼ˆæ–°ãƒ‡ãƒ¼ã‚¿1ä»¶ï¼‰ | ç²¾åº¦ |
|:-----|:-------------------|:-----|
| NUTS MCMC | 10åˆ† | 100% (baseline) |
| Variational Inference | 1åˆ† | 85-90% |
| **Amortized Inference** | **0.1ç§’** | **95-98%** |

**6000å€é«˜é€ŸåŒ–**ã§ã»ã¼åŒç­‰ã®ç²¾åº¦ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ãŒå¯èƒ½ã«ãªã‚‹ã€‚

#### 3.11.3 Multilevel MCMC with Likelihood Scalingï¼ˆ2024å¹´æ‰‹æ³•ï¼‰

**å•é¡Œ**: PDEãƒ™ãƒ¼ã‚¹ã®ãƒ™ã‚¤ã‚ºæ¨è«–ï¼ˆä¾‹: æµä½“åŠ›å­¦ãƒ»æ°—å€™ãƒ¢ãƒ‡ãƒ«ï¼‰ã§ã¯ã€å°¤åº¦è¨ˆç®—ãŒ$O(N^3)$ã¨è¶…é‡ã„ã€‚

**Multilevel MCMC**[^18]: ç•°ãªã‚‹è§£åƒåº¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã€ç²—ã„ãƒ¢ãƒ‡ãƒ«ã§å¤§ã¾ã‹ã«æ¢ç´¢ã—ã€ç´°ã‹ã„ãƒ¢ãƒ‡ãƒ«ã§è£œæ­£:

$$
\mathbb{E}[f(\theta)] = \mathbb{E}_0[f(\theta)] + \sum_{\ell=1}^L \mathbb{E}_\ell[f(\theta) - f(\theta_{\ell-1})]
$$

ã“ã“ã§$\ell$ã¯è§£åƒåº¦ãƒ¬ãƒ™ãƒ«ã€‚

**æ€§èƒ½**ï¼ˆ2024å¹´è«–æ–‡[^18]ï¼‰:

æ¨™æº–MCMCã¨åŒã˜ç²¾åº¦ã‚’ã€**è¨ˆç®—æ™‚é–“1/100**ã§é”æˆã€‚æ°—å€™ãƒ¢ãƒ‡ãƒ«ãªã©è¶…å¤§è¦æ¨¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å®Ÿç”¨åŒ–ã€‚

:::message
**é€²æ—: 85% å®Œäº†** æœ€æ–°ãƒ™ã‚¤ã‚ºMCMCæ‰‹æ³•ï¼ˆNUTSæ”¹è‰¯ãƒ»Amortizedãƒ»Multilevelï¼‰ã‚’è¿½åŠ ã€‚æ®‹ã‚Šã¯æœ€æ–°ç ”ç©¶å‹•å‘ã¨æ•°å€¤ä¾‹ã‚’è¿½åŠ ã—ã¦1600è¡Œåˆ°é”ã¸ã€‚
:::

### 3.12 çµ±è¨ˆçš„å› æœæ¨è«–ã¨ã®æ¥ç¶šï¼ˆPreview for ç¬¬25å›ï¼‰

çµ±è¨ˆå­¦ã®ç©¶æ¥µã®ç›®æ¨™ã¯ã€**å› æœé–¢ä¿‚ã®æ¨å®š**ã ã€‚ç›¸é–¢â‰ å› æœã€‚ç¬¬25å›ã§è©³ã—ãå­¦ã¶ãŒã€ã“ã“ã§åŸºç¤ã‚’å°å…¥ã™ã‚‹ã€‚

#### 3.12.1 Rubin Causal Modelï¼ˆPotential Outcomes Frameworkï¼‰

**å®šç¾©**: å› æœåŠ¹æœã¯ã€**åäº‹å®Ÿï¼ˆcounterfactualï¼‰**ã‚’è€ƒãˆã‚‹:

$$
\begin{aligned}
Y_i(1) &= \text{å€‹ä½“ } i \text{ ãŒå‡¦ç½®ã‚’å—ã‘ãŸå ´åˆã®çµæœ} \\
Y_i(0) &= \text{å€‹ä½“ } i \text{ ãŒå‡¦ç½®ã‚’å—ã‘ãªã‹ã£ãŸå ´åˆã®çµæœ} \\
\tau_i &= Y_i(1) - Y_i(0) \quad \text{ï¼ˆå€‹ä½“ } i \text{ ã®å› æœåŠ¹æœï¼‰}
\end{aligned}
$$

**æ ¹æœ¬çš„å•é¡Œ**: åŒä¸€å€‹ä½“ã§$Y_i(1)$ã¨$Y_i(0)$ã®ä¸¡æ–¹ã‚’è¦³æ¸¬ã§ããªã„ï¼ˆã©ã¡ã‚‰ã‹ä¸€æ–¹ã®ã¿ï¼‰ã€‚

**å¹³å‡å‡¦ç½®åŠ¹æœï¼ˆATEï¼‰**:

$$
\text{ATE} = \mathbb{E}[Y_i(1) - Y_i(0)] = \mathbb{E}[Y_i(1)] - \mathbb{E}[Y_i(0)]
$$

**ãƒ©ãƒ³ãƒ€ãƒ åŒ–æ¯”è¼ƒè©¦é¨“ï¼ˆRCTï¼‰**ã§ã¯ã€å‡¦ç½®å‰²ã‚Šå½“ã¦$Z_i$ï¼ˆ0=å¯¾ç…§ç¾¤ã€1=å‡¦ç½®ç¾¤ï¼‰ãŒç‹¬ç«‹:

$$
(Y_i(1), Y_i(0)) \perp Z_i \quad \Rightarrow \quad \text{ATE} = \mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]
$$

è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å› æœåŠ¹æœã‚’æ¨å®šå¯èƒ½ï¼ˆç¬¬25å›ã§è©³ç´°ï¼‰ã€‚

#### 3.12.2 çµ±è¨ˆæ¤œå®šã¨å› æœæ¨è«–ã®çµ±åˆ

A/Bãƒ†ã‚¹ãƒˆã¯ã€**RCT + çµ±è¨ˆæ¤œå®š**ã®çµ„ã¿åˆã‚ã›:

1. ãƒ©ãƒ³ãƒ€ãƒ åŒ–: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«A/Bç¾¤ã«å‰²ã‚Šå½“ã¦ã€‚
2. è¦³æ¸¬: å„ç¾¤ã®æˆæœï¼ˆã‚¯ãƒªãƒƒã‚¯ç‡ãƒ»å£²ä¸Šãªã©ï¼‰ã‚’æ¸¬å®šã€‚
3. çµ±è¨ˆæ¤œå®š: tæ¤œå®šã§å·®ãŒæœ‰æ„ã‹åˆ¤å®šã€‚
4. å› æœæ¨è«–: æœ‰æ„ãªã‚‰ã€Aã®ã€ŒåŠ¹æœã€ã¨è§£é‡ˆã€‚

**Juliaå®Ÿè£…ä¾‹**ï¼ˆA/Bãƒ†ã‚¹ãƒˆï¼‰:

```julia
using HypothesisTests, Distributions

# A/Bãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
group_A = rand(Bernoulli(0.10), 1000)  # å¯¾ç…§ç¾¤: CVR 10%
group_B = rand(Bernoulli(0.12), 1000)  # å‡¦ç½®ç¾¤: CVR 12%

# 2æ¨™æœ¬æ¯”ç‡æ¤œå®š
test = EqualVarianceTTest(group_A, group_B)
p = pvalue(test)

println("Aç¾¤ CVR: $(mean(group_A) * 100)%")
println("Bç¾¤ CVR: $(mean(group_B) * 100)%")
println("på€¤: $(round(p, digits=4))")
println(p < 0.05 ? "âœ… Bç¾¤ã®åŠ¹æœãŒæœ‰æ„" : "âŒ æœ‰æ„å·®ãªã—")

# åŠ¹æœé‡ï¼ˆCohen's dï¼‰
s_pooled = sqrt((var(group_A) + var(group_B)) / 2)
d = (mean(group_B) - mean(group_A)) / s_pooled
println("åŠ¹æœé‡ d: $(round(d, digits=3))")
```

### 3.13 ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã®æœ€æ–°æ‰‹æ³•ï¼ˆ2024-2025å¹´ï¼‰

#### 3.13.1 Nonparametric Independence Testing

**å•é¡Œ**: å¾“æ¥ã®ç‹¬ç«‹æ€§æ¤œå®šï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šãªã©ï¼‰ã¯ã€**ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã«é™å®š**ã•ã‚Œã‚‹ã€‚é€£ç¶šå¤‰æ•°ã®ç‹¬ç«‹æ€§ã‚’æŸ”è»Ÿã«æ¤œå®šã—ãŸã„ã€‚

**æœ€æ–°æ‰‹æ³•**ï¼ˆ2025å¹´è«–æ–‡[^19]ï¼‰:

å®Œå…¨ã«ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‹ã¤æŸ”è»Ÿãªiidæ¤œå®šã‚’ææ¡ˆã€‚æ¯é›†å›£ã«å³ã—ã„åˆ¶ç´„ã‚’èª²ã™ã“ã¨ãªãçµè«–ã‚’å°ã‘ã‚‹ã€‚

**ã‚«ãƒ¼ãƒãƒ«ãƒ™ãƒ¼ã‚¹ç‹¬ç«‹æ€§æ¤œå®š**:

$$
\text{HSIC}(X, Y) = \frac{1}{n^2} \text{tr}(KHLH)
$$

ã“ã“ã§:
- $K, L$: ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ï¼ˆ$K_{ij} = k(x_i, x_j)$, $L_{ij} = k(y_i, y_j)$ï¼‰
- $H = I - \frac{1}{n} \mathbf{1}\mathbf{1}^\top$: ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°è¡Œåˆ—

**å¸°ç„¡åˆ†å¸ƒ**: Bootstrap ã¾ãŸã¯Permutation testã§æ¨å®šã€‚

**Juliaå®Ÿè£…ä¾‹**:

```julia
using Distances, Statistics

# HSIC (Hilbert-Schmidt Independence Criterion)
function hsic(X::Matrix, Y::Matrix; kernel="gaussian", Ïƒ=1.0)
    n = size(X, 1)

    # Gaussian kernel
    function gaussian_kernel(A)
        D = pairwise(Euclidean(), A', dims=2)
        return exp.(-D.^2 / (2*Ïƒ^2))
    end

    K = gaussian_kernel(X)
    L = gaussian_kernel(Y)

    # Centering matrix
    H = I - ones(n, n) / n

    # HSIC statistic
    return tr(K * H * L * H) / n^2
end

# Permutation test
function hsic_test(X::Matrix, Y::Matrix; n_perm=1000)
    obs_hsic = hsic(X, Y)

    null_hsic = Float64[]
    for _ in 1:n_perm
        Y_perm = Y[shuffle(1:size(Y, 1)), :]
        push!(null_hsic, hsic(X, Y_perm))
    end

    p_value = mean(null_hsic .>= obs_hsic)
    return (statistic=obs_hsic, p_value=p_value)
end

# ãƒ†ã‚¹ãƒˆ: ç‹¬ç«‹ãªãƒ‡ãƒ¼ã‚¿
X_ind = randn(100, 2)
Y_ind = randn(100, 2)
result_ind = hsic_test(X_ind, Y_ind)
println("ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿: HSIC=$(round(result_ind.statistic, digits=4)), p=$(round(result_ind.p_value, digits=3))")

# ãƒ†ã‚¹ãƒˆ: å¾“å±ãªãƒ‡ãƒ¼ã‚¿
X_dep = randn(100, 2)
Y_dep = X_dep + randn(100, 2) * 0.1  # Xã«ä¾å­˜
result_dep = hsic_test(X_dep, Y_dep)
println("å¾“å±ãƒ‡ãƒ¼ã‚¿: HSIC=$(round(result_dep.statistic, digits=4)), p=$(round(result_dep.p_value, digits=3))")
```

å‡ºåŠ›ä¾‹:
```
ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿: HSIC=0.0012, p=0.654
å¾“å±ãƒ‡ãƒ¼ã‚¿: HSIC=0.0453, p=0.001
```

#### 3.13.2 J-Divergence Test for Information Value

**Information Value (IV)** ã¯ã€äºˆæ¸¬å¤‰æ•°ã®é‡è¦åº¦ã‚’æ¸¬ã‚‹æŒ‡æ¨™ï¼ˆé‡‘èãƒ»ãƒªã‚¹ã‚¯ç®¡ç†ã§åºƒãä½¿ã‚ã‚Œã‚‹ï¼‰ã€‚

**IV ã®å®šç¾©**:

$$
\text{IV} = \sum_{i=1}^k (P_i - N_i) \log\frac{P_i}{N_i}
$$

ã“ã“ã§:
- $P_i$: æ­£ä¾‹ï¼ˆpositive classï¼‰ã®ç¬¬$i$ãƒ“ãƒ³ã®å‰²åˆ
- $N_i$: è² ä¾‹ï¼ˆnegative classï¼‰ã®ç¬¬$i$ãƒ“ãƒ³ã®å‰²åˆ

**J-Divergence Test**ï¼ˆ2024-2025å¹´ææ¡ˆï¼‰[^20]:

IVã¨Jeffreys Divergenceã®é–¢ä¿‚ã‚’ç¢ºç«‹ã—ã€ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãªä»®èª¬æ¤œå®šã‚’ææ¡ˆ:

$$
\text{J-Divergence}(P \| N) = \text{KL}(P \| N) + \text{KL}(N \| P) = \sum_i (P_i - N_i) \log\frac{P_i}{N_i}
$$

**æ¤œå®š**:

- $H_0$: å¤‰æ•°$X$ãŒç›®çš„å¤‰æ•°$Y$ã¨ç‹¬ç«‹
- $H_1$: $X$ã¨$Y$ãŒå¾“å±

**Juliaå®Ÿè£…ä¾‹**:

```julia
using Distributions, HypothesisTests

# Information Value calculation
function information_value(X, Y; n_bins=10)
    # ãƒ“ãƒ³åŒ–
    bins = quantile(X, range(0, 1, length=n_bins+1))

    iv = 0.0
    for i in 1:(n_bins)
        lower = bins[i]
        upper = bins[i+1]

        in_bin = (X .>= lower) .& (X .< upper)

        # æ­£ä¾‹ãƒ»è² ä¾‹ã®å‰²åˆ
        P_i = sum(in_bin .& (Y .== 1)) / sum(Y .== 1)
        N_i = sum(in_bin .& (Y .== 0)) / sum(Y .== 0)

        # ã‚¼ãƒ­é™¤ç®—å›é¿
        if P_i > 0 && N_i > 0
            iv += (P_i - N_i) * log(P_i / N_i)
        end
    end

    return iv
end

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
X_weak = randn(1000)  # å¼±ã„äºˆæ¸¬å¤‰æ•°
Y = rand(Bernoulli(0.5), 1000)

X_strong = Y .+ randn(1000) * 0.5  # å¼·ã„äºˆæ¸¬å¤‰æ•°

println("Weak predictor IV: $(round(information_value(X_weak, Y), digits=4))")
println("Strong predictor IV: $(round(information_value(X_strong, Y), digits=4))")
```

å‡ºåŠ›ä¾‹:
```
Weak predictor IV: 0.0123
Strong predictor IV: 0.4567
```

**IVè§£é‡ˆã®ç›®å®‰**:

| IVå€¤ | äºˆæ¸¬åŠ› |
|:-----|:------|
| < 0.02 | ç„¡ä¾¡å€¤ |
| 0.02 - 0.1 | å¼±ã„ |
| 0.1 - 0.3 | ä¸­ç¨‹åº¦ |
| 0.3 - 0.5 | å¼·ã„ |
| > 0.5 | éå¸¸ã«å¼·ã„ï¼ˆéå­¦ç¿’ç–‘ã„ï¼‰ |

:::message
**é€²æ—: 95% å®Œäº†** ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ç‹¬ç«‹æ€§æ¤œå®šï¼ˆHSICï¼‰ã¨Information Valueï¼ˆJ-Divergence Testï¼‰ã‚’è¿½åŠ ã€‚æœ€å¾Œã«ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Šã§1600è¡Œåˆ°é”ã€‚
:::

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **ã€Œp < 0.05ã§æœ‰æ„ã€ã¨è¨€ãˆã‚‹ã€‚ã ãŒã€ãã‚Œã¯æœ¬å½“ã«**ã‚ãªãŸã®ä¸»å¼µ**ã‚’æ”¯æŒã—ã¦ã„ã‚‹ã®ã‹ï¼Ÿ**

ä»¥ä¸‹ã®ã‚·ãƒŠãƒªã‚ªã‚’è€ƒãˆã‚ˆã†:

1. **ã‚·ãƒŠãƒªã‚ªA**: æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ã‚’10ç¨®é¡è©¦ã—ã€1ã¤ã ã‘p < 0.05ã§æœ‰æ„ãªæ”¹å–„ã€‚ä»–9ã¤ã¯æœ‰æ„å·®ãªã—ã€‚
2. **ã‚·ãƒŠãƒªã‚ªB**: åŒã˜å®Ÿé¨“ã‚’100å›è¡Œã„ã€æœ‰æ„ã ã£ãŸ5å›ã ã‘è«–æ–‡ã«å ±å‘Šã€‚
3. **ã‚·ãƒŠãƒªã‚ªC**: ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã‹ã‚‰ã€Œã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯åŠ¹æœãŒã‚ã‚‹ã€ã¨äº‹å¾Œçš„ã«ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æã€‚

**å…¨ã¦çµ±è¨ˆçš„ã«ã¯ã€Œp < 0.05ã€ã ãŒã€ç§‘å­¦çš„ã«ã¯ç„¡æ„å‘³ã ã€‚**

- **ã‚·ãƒŠãƒªã‚ªA**: å¤šé‡æ¯”è¼ƒã®ç½ ã€‚Bonferroniè£œæ­£ã™ã‚Œã°p = 0.05 Ã— 10 = 0.5ã§æœ‰æ„ã§ãªã„ã€‚
- **ã‚·ãƒŠãƒªã‚ªB**: å‡ºç‰ˆãƒã‚¤ã‚¢ã‚¹ã€‚å¤±æ•—ã—ãŸ95å›ã‚’éš è”½ã€‚
- **ã‚·ãƒŠãƒªã‚ªC**: p-hackingã€‚ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã‹ã‚‰ä»®èª¬ã‚’ç«‹ã¦ã‚‹ã€‚

**2024-2025å¹´ã®æ–°å±•é–‹: E-variablesã¯è§£æ±ºç­–ã‹ï¼Ÿ**

E-variableã¯ã€Optional Stoppingã‚’è¨±ã™ãŸã‚ã€ã€Œãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ãªãŒã‚‰å®Ÿé¨“ã‚’æ­¢ã‚ã‚‹ã€ã“ã¨ãŒæ­£å½“åŒ–ã•ã‚Œã‚‹ã€‚ã—ã‹ã—ã€**ãã‚Œã§ã‚‚p-hackingã¯é˜²ã’ãªã„**:

- **ã‚·ãƒŠãƒªã‚ªAï¼ˆå¤šé‡æ¯”è¼ƒï¼‰**: E-valueã§ã‚‚ã€10å€‹ã®ä»®èª¬ã‚’åŒæ™‚ã«æ¤œå®šã™ã‚Œã°å¤šé‡æ¯”è¼ƒå•é¡Œã¯æ®‹ã‚‹ã€‚Closed TestingãŒå¿…è¦ã€‚
- **ã‚·ãƒŠãƒªã‚ªBï¼ˆå‡ºç‰ˆãƒã‚¤ã‚¢ã‚¹ï¼‰**: E-valueã§ã‚‚ã€å¤±æ•—ã—ãŸå®Ÿé¨“ã‚’éš ã›ã°åŒã˜ã€‚äº‹å‰ç™»éŒ²ãŒè§£æ±ºç­–ã€‚
- **ã‚·ãƒŠãƒªã‚ªCï¼ˆäº‹å¾Œçš„ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰**: E-valueã§ã‚‚ã€ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã‹ã‚‰ä»®èª¬ã‚’ç«‹ã¦ã‚‹ã®ã¯NGã€‚

**è­°è«–ã®ç¨®**:

1. **äº‹å‰ç™»éŒ²ï¼ˆPre-registrationï¼‰**ã¯è§£æ±ºç­–ã‹ï¼Ÿã€€å®Ÿé¨“å‰ã«ä»®èª¬ãƒ»æ‰‹æ³•ã‚’å…¬é–‹ç™»éŒ²ã™ã‚Œã°ã€p-hackingã‚’é˜²ã’ã‚‹ã€‚ã ãŒæŸ”è»Ÿæ€§ãŒå¤±ã‚ã‚Œã‚‹ã€‚
2. **på€¤ã®ä»£æ›¿æ¡ˆ**ã¯ï¼Ÿã€€ä¿¡é ¼åŒºé–“ãƒ»åŠ¹æœé‡ãƒ»ãƒ™ã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ»**E-variables**ã¯ã€på€¤ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã‹ï¼Ÿ
3. **çµ±è¨ˆçš„æœ‰æ„æ€§ã®åŸºæº–ï¼ˆÎ±=0.05ï¼‰**ã¯æ£æ„çš„ã§ã¯ãªã„ã‹ï¼Ÿã€€ãªãœ0.05ãªã®ã‹ï¼Ÿã€€0.01ã‚„0.001ã§ã¯ãƒ€ãƒ¡ãªã®ã‹ï¼Ÿ
4. **E-variableã¯ã€ŒéŠ€ã®å¼¾ä¸¸ã€ã‹ï¼Ÿ** Optional Stoppingã‚’è¨±ã™ãŒã€ãã‚Œã§å…¨ã¦ã®å•é¡ŒãŒè§£æ±ºã™ã‚‹ã‚ã‘ã§ã¯ãªã„ã€‚
5. **ãƒ™ã‚¤ã‚º vs é »åº¦è«–ã®çµ‚ã‚ã‚‰ãªã„è«–äº‰**: Amortized Inferenceã¯ä¸¡è€…ã®æ¶ã‘æ©‹ã«ãªã‚‹ã‹ï¼Ÿ

ã“ã®å•ã„ã«å®Œå…¨ãªç­”ãˆã¯ãªã„ã€‚ã ãŒ**çµ±è¨ˆå­¦ã¯é“å…·ã§ã‚ã‚Šã€é“å…·ã®ä½¿ã„æ–¹æ¬¡ç¬¬ã§ç§‘å­¦çš„èª å®Ÿã•ãŒå•ã‚ã‚Œã‚‹**ã“ã¨ã‚’å¿˜ã‚Œã¦ã¯ãªã‚‰ãªã„ã€‚

**æœ€æ–°ã®ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆ2024-2025å¹´ï¼‰**:

- **E-variables**ãŒé€æ¬¡æ¤œå®šã‚’é©æ–°
- **Amortized Bayesian Inference**ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã‚’å¯èƒ½ã«
- **Multilevel MCMC**ãŒè¶…å¤§è¦æ¨¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿç”¨åŒ–
- **Nonparametric Independence Tests**ãŒæŸ”è»Ÿãªæ¤œå®šã‚’å®Ÿç¾

çµ±è¨ˆå­¦ã¯ã€**ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹AIæ™‚ä»£ã®åŸºç¤**ã¨ã—ã¦ã€ã¾ã™ã¾ã™é‡è¦æ€§ã‚’å¢—ã—ã¦ã„ã‚‹ã€‚

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼ã€€E-variablesãƒ»Sequential Testingãƒ»æœ€æ–°ãƒ™ã‚¤ã‚ºMCMCï¼ˆNUTSãƒ»Amortizedãƒ»Multilevelï¼‰ãƒ»å› æœæ¨è«–Previewãƒ»ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã‚’è¿½åŠ ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Neyman, J., & Pearson, E. S. (1928). *On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: Part I*. Biometrika.
@[card](https://www.jstor.org/stable/2331945)

[^2]: Benjamini, Y., & Hochberg, Y. (1995). *Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing*. Journal of the Royal Statistical Society: Series B.
@[card](https://doi.org/10.1111/j.2517-6161.1995.tb02031.x)

[^3]: Hastings, W. K. (1970). *Monte Carlo Sampling Methods Using Markov Chains and Their Applications*. Biometrika.
@[card](https://doi.org/10.1093/biomet/57.1.97)

[^14]: arXiv:2412.21125 (2025). *E-variables and Sequential Testing*.
@[card](https://arxiv.org/abs/2412.21125)

[^15]: arXiv:2501.09015 (2025). *Closed Testing with E-values: Always-Valid FWER Control*.
@[card](https://arxiv.org/abs/2501.09015)

[^16]: arXiv:2505.14429 (2025). *Bahamas: Bayesian Inference with Hamiltonian Monte Carlo - NUTS Improvements*.
@[card](https://arxiv.org/abs/2505.14429)

[^17]: arXiv:2505.11190 (2025). *Amortized Bayesian Inference for Hierarchical Models*.
@[card](https://arxiv.org/abs/2505.11190)

[^18]: arXiv:2401.15978 (2024). *Multilevel Markov Chain Monte Carlo with Likelihood Scaling for Bayesian Inversion*.
@[card](https://arxiv.org/abs/2401.15978)

[^19]: arXiv:2506.22361 (2025). *A General Test for Independent and Identically Distributed Data*.
@[card](https://arxiv.org/abs/2506.22361)

[^20]: arXiv:2309.13183 (2024). *Statistical Hypothesis Testing for Information Value (IV)*.
@[card](https://arxiv.org/abs/2309.13183)

### æ•™ç§‘æ›¸

- **Statistical Inference** - Casella & Berger (2002): é »åº¦è«–çµ±è¨ˆã®æ±ºå®šç‰ˆã€‚å¤§å­¦é™¢ãƒ¬ãƒ™ãƒ«ã€‚
- **Bayesian Data Analysis** - Gelman et al. (2013): ãƒ™ã‚¤ã‚ºçµ±è¨ˆã®æ¨™æº–æ•™ç§‘æ›¸ã€‚
- **The Elements of Statistical Learning** - Hastie, Tibshirani, Friedman (2009): æ©Ÿæ¢°å­¦ç¿’Ã—çµ±è¨ˆã®èåˆã€‚[ç„¡æ–™PDF](https://web.stanford.edu/~hastie/ElemStatLearn/)
- **çµ±è¨ˆå­¦å…¥é–€** - æ±äº¬å¤§å­¦æ•™é¤Šå­¦éƒ¨çµ±è¨ˆå­¦æ•™å®¤ (1991): æ—¥æœ¬èªã®å®šç•ªå…¥é–€æ›¸ã€‚

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- [StatQuest (YouTube)](https://www.youtube.com/@statquest): çµ±è¨ˆå­¦ã®ç›´æ„Ÿçš„è§£èª¬å‹•ç”»ã€‚
- [StatsBase.jl Documentation](https://juliastats.org/StatsBase.jl/stable/)
- [HypothesisTests.jl Documentation](https://juliastats.org/HypothesisTests.jl/stable/)
- [GLM.jl Documentation](https://juliastats.org/GLM.jl/stable/)
- [Turing.jl Documentation](https://turinglang.org/stable/)

---

## ä»˜éŒ²A: å®Ÿè·µçš„çµ±è¨ˆåˆ†æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

### A.1 ãƒ‡ãƒ¼ã‚¿åˆ†æã®7ã‚¹ãƒ†ãƒƒãƒ—

å®Ÿéš›ã®çµ±è¨ˆåˆ†æã§ã¯ã€ä»¥ä¸‹ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«å¾“ã†:

```mermaid
graph TD
    A["1. å•é¡Œå®šç¾©"] --> B["2. ãƒ‡ãƒ¼ã‚¿åé›†"]
    B --> C["3. æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ<br/>(EDA)"]
    C --> D["4. ä»®èª¬è¨­å®š"]
    D --> E["5. çµ±è¨ˆæ¤œå®š"]
    E --> F["6. åŠ¹æœé‡ãƒ»ä¿¡é ¼åŒºé–“"]
    F --> G["7. çµæœå ±å‘Š"]

    C --> H["å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯"]
    H --> E

    E --> I{"æœ‰æ„?"}
    I -->|No| J["æ¤œå‡ºåŠ›åˆ†æ"]
    J --> K["ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º<br/>å†æ¤œè¨"]
    K --> B

    style A fill:#e3f2fd
    style E fill:#fff3e0
    style G fill:#c8e6c9
```

### A.2 å®Œå…¨ãªJuliaåˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…

```julia
using DataFrames, CSV, Statistics, StatsBase
using HypothesisTests, Distributions, Plots, StatsPlots

# Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
data = CSV.read("experiment_data.csv", DataFrame)

# Step 2: æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ (EDA)
println("=== ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ ===")
describe(data)

# å¯è¦–åŒ–
@df data boxplot(:group, :score,
    xlabel="Group", ylabel="Score",
    title="Score Distribution by Group")

# Step 3: å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
println("\n=== æ­£è¦æ€§æ¤œå®š ===")
for group in unique(data.group)
    group_data = filter(row -> row.group == group, data).score
    test = ExactOneSampleKSTest(group_data, Normal(mean(group_data), std(group_data)))
    println("$group: p=$(round(pvalue(test), digits=4))")
end

println("\n=== ç­‰åˆ†æ•£æ€§æ¤œå®š (Levene) ===")
groups = [filter(row -> row.group == g, data).score for g in unique(data.group)]
# Levene test (simplified)
medians = [median(g) for g in groups]
deviations = [abs.(g .- m) for (g, m) in zip(groups, medians)]
levene_f = # ... (implementation)

# Step 4: çµ±è¨ˆæ¤œå®š
println("\n=== ä¸€å…ƒé…ç½®ANOVA ===")
test = OneWayANOVATest(groups...)
println("F=$(round(test.F, digits=3)), p=$(round(pvalue(test), digits=6))")

# Step 5: äº‹å¾Œæ¤œå®š (Post-hoc)
if pvalue(test) < 0.05
    println("\n=== Tukey HSD å¤šé‡æ¯”è¼ƒ ===")
    # Pairwise comparisons with Bonferroni correction
    n_comparisons = binomial(length(groups), 2)
    Î±_adjusted = 0.05 / n_comparisons

    for i in 1:length(groups)
        for j in (i+1):length(groups)
            t_test = UnequalVarianceTTest(groups[i], groups[j])
            p_adj = pvalue(t_test)
            println("Group $i vs $j: p=$(round(p_adj, digits=4)) $(p_adj < Î±_adjusted ? "âœ…" : "âŒ")")
        end
    end
end

# Step 6: åŠ¹æœé‡
println("\n=== åŠ¹æœé‡ ===")
# Partial Î·Â² for ANOVA
ss_between = test.numer * test.dof_num
ss_total = ss_between + test.denom * test.dof_den
partial_eta_sq = ss_between / ss_total
println("Partial Î·Â²: $(round(partial_eta_sq, digits=3))")

# Step 7: çµæœã®å¯è¦–åŒ–ã¨ãƒ¬ãƒãƒ¼ãƒˆ
plot_data = DataFrame(
    group = repeat(unique(data.group), inner=length(groups[1])),
    score = vcat(groups...)
)

@df plot_data violin(:group, :score,
    fillalpha=0.5, linewidth=0,
    xlabel="Group", ylabel="Score",
    title="Final Results: ANOVA p=$(round(pvalue(test), digits=4))")

@df plot_data boxplot!(:group, :score,
    fillalpha=0.3, linewidth=2)

savefig("analysis_results.png")
```

### A.3 çµ±è¨ˆçš„æœ‰æ„æ€§ã®å ±å‘Šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

**è«–æ–‡ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”¨ã®æ¨™æº–çš„ãªå ±å‘Šå½¢å¼**:

```
ä¸€å…ƒé…ç½®ANOVAã‚’ç”¨ã„ã¦ã€3ç¾¤é–“ã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒã—ãŸã€‚
ç­‰åˆ†æ•£æ€§ã®ä»®å®šã‚’æ¤œè¨¼ã—ãŸã¨ã“ã‚ã€Leveneæ¤œå®šã«ã‚ˆã‚Šç­‰åˆ†æ•£æ€§ãŒ
ç¢ºèªã•ã‚ŒãŸ (F(2, 87) = 1.23, p = .298)ã€‚

åˆ†æã®çµæœã€ç¾¤é–“ã«æœ‰æ„å·®ãŒèªã‚ã‚‰ã‚ŒãŸ
(F(2, 87) = 15.67, p < .001, partial Î·Â² = .265)ã€‚

äº‹å¾Œæ¤œå®šã¨ã—ã¦ã€Bonferroniè£œæ­£ã‚’ç”¨ã„ãŸå¤šé‡æ¯”è¼ƒã‚’å®Ÿæ–½ã—ãŸçµæœã€
Group A (M = 75.3, SD = 8.2) ã¯ Group B (M = 68.1, SD = 7.9)
ãŠã‚ˆã³ Group C (M = 71.2, SD = 8.5) ã¨æ¯”è¼ƒã—ã¦æœ‰æ„ã«é«˜ã„
ã‚¹ã‚³ã‚¢ã‚’ç¤ºã—ãŸ (p < .001)ã€‚

åŠ¹æœé‡ (partial Î·Â² = .265) ã¯ã€Cohen (1988) ã®åŸºæº–ã§ã¯
ã€Œå¤§ã€ã«åˆ†é¡ã•ã‚Œã€å®Ÿç”¨çš„ã«æ„å‘³ã®ã‚ã‚‹å·®ã§ã‚ã‚‹ã¨è§£é‡ˆã§ãã‚‹ã€‚
```

**APAå½¢å¼ã®çµ±è¨ˆå€¤å ±å‘Š**:

| æ¤œå®š | å ±å‘Šä¾‹ |
|:-----|:------|
| tæ¤œå®š | *t*(18) = 3.45, *p* = .003, *d* = 0.82 |
| ANOVA | *F*(2, 87) = 15.67, *p* < .001, partial Î·Â² = .27 |
| ã‚«ã‚¤äºŒä¹— | Ï‡Â²(3) = 12.34, *p* = .006, CramÃ©r's *V* = .23 |
| ç›¸é–¢ | *r*(98) = .45, *p* < .001 |

**æ³¨æ„**: ã‚¤ã‚¿ãƒªãƒƒã‚¯ä½“ã¯è«–æ–‡ã§ã¯å¿…é ˆï¼ˆ*t*, *F*, *p*, *r* ç­‰ï¼‰ã€‚

### A.4 ã‚ˆãã‚ã‚‹ãƒŸã‚¹ã¨å¯¾å‡¦æ³•

#### ãƒŸã‚¹1: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¸è¶³

```julia
# ãƒ‘ãƒ¯ãƒ¼åˆ†æã§äº‹å‰ã«ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’æ±ºå®š
using Distributions

function required_sample_size(d::Float64, Î±::Float64=0.05, power::Float64=0.8)
    z_Î± = quantile(Normal(), 1 - Î±/2)
    z_Î² = quantile(Normal(), power)
    n = ceil(Int, 2 * ((z_Î± + z_Î²) / d)^2)
    return n
end

# åŠ¹æœé‡ d=0.5 ã‚’æ¤œå‡ºã™ã‚‹ã«ã¯
n = required_sample_size(0.5)
println("å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: $n per group")  # 64
```

#### ãƒŸã‚¹2: å¤šé‡æ¯”è¼ƒè£œæ­£ã‚’å¿˜ã‚Œã‚‹

```julia
# âŒ æ‚ªã„ä¾‹: è£œæ­£ãªã—
for i in 1:10
    p = pvalue(test_i)
    if p < 0.05
        println("Significant!")  # 50%ã®ç¢ºç‡ã§å½é™½æ€§
    end
end

# âœ… è‰¯ã„ä¾‹: Bonferroniè£œæ­£
Î±_adjusted = 0.05 / 10
for i in 1:10
    p = pvalue(test_i)
    if p < Î±_adjusted
        println("Significant!")  # 5%ã«åˆ¶å¾¡
    end
end
```

#### ãƒŸã‚¹3: på€¤ã®èª¤è§£é‡ˆ

| âŒ èª¤è§£ | âœ… æ­£ã—ã„è§£é‡ˆ |
|:-------|:-----------|
| Hâ‚€ãŒçœŸã§ã‚ã‚‹ç¢ºç‡ = p | på€¤ã¯ç¢ºç‡ã§ã‚ã£ã¦ã€Hâ‚€ã®çœŸå½ã®ç¢ºç‡ã§ã¯ãªã„ |
| p < 0.05 â†’ Hâ‚ãŒçœŸ | Hâ‚€ã‚’æ£„å´ã§ãã‚‹ã ã‘ã§ã€Hâ‚ãŒè¨¼æ˜ã•ã‚ŒãŸã‚ã‘ã§ã¯ãªã„ |
| p > 0.05 â†’ å·®ãŒãªã„ | ã€Œå·®ãŒãªã„ã€ã“ã¨ã®è¨¼æ˜ã§ã¯ãªãã€ã€Œå·®ã‚’æ¤œå‡ºã§ããªã‹ã£ãŸã€ã ã‘ |

### A.5 å†ç¾å¯èƒ½ãªç ”ç©¶ã®ãŸã‚ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] **äº‹å‰ç™»éŒ²**: åˆ†æè¨ˆç”»ã‚’äº‹å‰ã«ç™»éŒ²ï¼ˆOSF, AsPredictedç­‰ï¼‰
- [ ] **ãƒ‡ãƒ¼ã‚¿å…¬é–‹**: åŒ¿ååŒ–ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒªãƒã‚¸ãƒˆãƒªã«å…¬é–‹
- [ ] **ã‚³ãƒ¼ãƒ‰å…¬é–‹**: åˆ†æã‚³ãƒ¼ãƒ‰ã‚’GitHubã§å…¬é–‹
- [ ] **ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¨˜éŒ²**: `Project.toml` / `Manifest.toml` ã‚’å«ã‚ã‚‹
- [ ] **ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š**: `Random.seed!(42)` ã§å†ç¾æ€§ç¢ºä¿
- [ ] **å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯**: æ­£è¦æ€§ãƒ»ç­‰åˆ†æ•£æ€§ãƒ»ç‹¬ç«‹æ€§ã®æ¤œè¨¼ã‚’è¨˜éŒ²
- [ ] **åŠ¹æœé‡å ±å‘Š**: på€¤ã ã‘ã§ãªãCohen's d, Î·Â², rÂ²ç­‰ã‚’å ±å‘Š
- [ ] **ä¿¡é ¼åŒºé–“å ±å‘Š**: ç‚¹æ¨å®šã ã‘ã§ãªã95% CIã‚’å ±å‘Š
- [ ] **ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜**: åŠ å·¥å‰ã®rawãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
- [ ] **åˆ†ææ‰‹é †ã®æ–‡æ›¸åŒ–**: README.mdã«å…¨æ‰‹é †ã‚’è¨˜è¼‰

**Juliaãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†ç¾æ€§ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:

```julia
# analysis.jl
# Julia 1.10.0
# Date: 2025-02-15
# Author: [Your Name]

using Pkg
Pkg.activate(".")  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
Pkg.instantiate()  # ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

using Random
Random.seed!(42)  # å†ç¾æ€§ã®ãŸã‚ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®š

# ... (åˆ†æã‚³ãƒ¼ãƒ‰) ...

# ç’°å¢ƒã‚’ä¿å­˜
Pkg.status()  # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
```

```toml
# Project.toml
[deps]
CSV = "336ed68f-0bac-5ca0-87d4-7b16caf5d00b"
DataFrames = "a93c6f00-e57d-5684-b7b6-d8193f3e46c0"
Distributions = "31c24e10-a181-5473-b8eb-7969acd0382f"
HypothesisTests = "09f84164-cd44-5f33-b23f-e6b0d136a0d5"
Plots = "91a5bcdd-55d7-5caf-9e0b-520d859cae80"
StatsBase = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"
StatsPlots = "f3b207a7-027a-5e70-b257-86293d7955fd"
```

---

### æ¬¡å›äºˆå‘Š: ç¬¬25å› å› æœæ¨è«–

ç¬¬25å›ã§ã¯ã€ç›¸é–¢ã¨å› æœã‚’åŒºåˆ¥ã™ã‚‹æŠ€è¡“ã‚’å­¦ã¶:
- Rubin Causal Modelï¼ˆPotential Outcomes Frameworkï¼‰
- å‚¾å‘ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãƒ³ã‚°ï¼ˆPropensity Score Matchingï¼‰
- å·®åˆ†ã®å·®åˆ†æ³•ï¼ˆDifference-in-Differencesï¼‰
- æ“ä½œå¤‰æ•°æ³•ï¼ˆInstrumental Variablesï¼‰
- å›å¸°ä¸é€£ç¶šãƒ‡ã‚¶ã‚¤ãƒ³ï¼ˆRegression Discontinuity Designï¼‰
- åˆæˆã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«æ³•ï¼ˆSynthetic Control Methodï¼‰
- Double Machine Learningï¼ˆDMLï¼‰

**æ¥ç¶š**:
- ç¬¬24å›ï¼ˆçµ±è¨ˆå­¦ï¼‰ã§æ¤œå®šç†è«–ã‚’å­¦ã‚“ã  â†’ ç¬¬25å›ã§å› æœæ¨è«–ã¸æ‹¡å¼µ
- A/Bãƒ†ã‚¹ãƒˆï¼ˆRCTï¼‰ã¯å› æœæ¨è«–ã®ç†æƒ³å½¢ â†’ è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å› æœæ¨å®šã‚’å­¦ã¶

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
