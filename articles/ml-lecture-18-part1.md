---
title: "ç¬¬18å›: Attention Ã— Mamba ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å‰ç·¨ã€‘ç†è«–ç·¨"
emoji: "ğŸ”€"
type: "tech"
topics: ["machinelearning", "deeplearning", "attention", "mamba", "rust"]
published: true
slug: "ml-lecture-18-part1"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

# ç¬¬18å›: Attention Ã— Mamba ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ â€” æœ€å¼·ã¯å­˜åœ¨ã—ãªã„ã€çµ„ã¿åˆã‚ã›ã“ããŒç­”ãˆ

> **Attentionã ã‘ã§ã‚‚SSMã ã‘ã§ã‚‚è¶³ã‚Šãªã„ã€‚ç›¸è£œçš„ãªå¼·ã¿ã‚’çµ„ã¿åˆã‚ã›ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒã€2024-2025å¹´ã®LLMã‚’å†å®šç¾©ã—ã¦ã„ã‚‹ã€‚**

Attentionã¯å…¨ç³»åˆ—ã‚’è¦‹æ¸¡ã™åŠ›ã‚’æŒã¤ã€‚ã ãŒ $O(N^2)$ ã®è¨ˆç®—é‡ãŒé•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ç ´ç¶»ã™ã‚‹ã€‚SSM(State Space Model)ã¯ $O(N)$ ã§åŠ¹ç‡çš„ã«é•·è·é›¢è¨˜æ†¶ã‚’ä¿æŒã§ãã‚‹ã€‚ã ãŒAttentionã®ã‚ˆã†ãªå‹•çš„ãªé‡ã¿ä»˜ã‘ãŒè‹¦æ‰‹ã ã€‚

ã§ã¯ã€**ä¸¡æ–¹ä½¿ãˆã°ã„ã„ã®ã§ã¯ï¼Ÿ**

ã“ã®å˜ç´”ãªç™ºæƒ³ãŒã€2024å¹´ã«Jamba [^1], Zamba [^2], Griffin [^3], StripedHyenaã¨ã„ã£ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”Ÿã‚“ã ã€‚Attentionã¨SSMã‚’åŒã˜ãƒ¢ãƒ‡ãƒ«å†…ã§äº¤äº’ã«é…ç½®ã—ã€ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‹ã€‚çµæœã€ç´”ç²‹ãªTransformerã‚„Mambaã‚’è¶…ãˆã‚‹æ€§èƒ½ã¨åŠ¹ç‡ã‚’å®Ÿç¾ã—ãŸã€‚

æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ã®æœ€çµ‚å› â€” ç¬¬9å›ã‹ã‚‰å§‹ã¾ã£ãŸå¤‰åˆ†æ¨è«–ãƒ»VAEãƒ»OTãƒ»GANãƒ»è‡ªå·±å›å¸°ãƒ»Attentionãƒ»SSMã®æ—…ã®ãƒ•ã‚£ãƒŠãƒ¼ãƒ¬ã ã€‚ãã—ã¦Course IIIã€Œå®Ÿè·µç·¨ã€ã¸ã®æ©‹æ¸¡ã—ã§ã‚‚ã‚ã‚‹ã€‚

> **Note:** **ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2024-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚

```mermaid
graph LR
    A["ğŸ”· Attention<br/>å…¨ç³»åˆ—å‚ç…§<br/>O(NÂ²)"] --> C["ğŸ”€ Hybrid<br/>Layeräº¤äº’é…ç½®"]
    B["ğŸ”¶ SSM<br/>åŠ¹ç‡çš„è¨˜æ†¶<br/>O(N)"] --> C
    C --> D["ğŸ¯ ç›¸è£œçš„å¼·ã¿<br/>æ€§èƒ½ & åŠ¹ç‡"]
    D --> E["ğŸš€ Jamba/Zamba/Griffin"]
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style E fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” Attentionã¨SSMã‚’äº¤äº’ã«

**ã‚´ãƒ¼ãƒ«**: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å¨åŠ›ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

Jamba [^1] ã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’3è¡Œã§å‹•ã‹ã™ã€‚

```rust
use ndarray::{Array2, ArrayView2, Axis};

// Hybrid block: Mamba (SSM) â†’ Attention â†’ MLP
// Input: sequence x âˆˆ â„^(seq_len Ã— d_model)
fn hybrid_block(
    x: ArrayView2<f64>,
    w_ssm: ArrayView2<f64>,
    w_attn: ArrayView2<f64>,
) -> Array2<f64> {
    // SSM layer: x_ssm = x Â· W_ssm (simplified; full SSM has Î”, B, C params)
    let x_ssm = x.dot(&w_ssm);

    // Attention: softmax(QK^T / âˆšd), row-wise
    let d_k = x_ssm.ncols() as f64;
    let scores = x_ssm.dot(&x_ssm.t()) / d_k.sqrt(); // QK^T / âˆšd
    let attn = softmax_rows(&scores);

    // MLP layer: relu(attn Â· x_ssm Â· W_attn)
    relu(&attn.dot(&x_ssm).dot(&w_attn))
}

fn softmax_rows(x: &Array2<f64>) -> Array2<f64> {
    // Numerically stable: subtract row-max before exp
    let max = x.fold_axis(Axis(1), f64::NEG_INFINITY, |&a, &b| a.max(b));
    let shifted = x - &max.insert_axis(Axis(1));
    let exp = shifted.mapv(f64::exp);
    let sum = exp.sum_axis(Axis(1)).insert_axis(Axis(1));
    exp / sum
}

fn relu(x: &Array2<f64>) -> Array2<f64> {
    x.mapv(|v| v.max(0.0))
}

fn main() {
    // Test: 4 tokens, 8-dim embeddings (use rand crate for random init in practice)
    let x    = Array2::<f64>::zeros((4, 8));
    let w_ssm  = Array2::<f64>::zeros((8, 8));
    let w_attn = Array2::<f64>::zeros((8, 8));

    let out = hybrid_block(x.view(), w_ssm.view(), w_attn.view());
    println!("Input shape: {:?}, Output shape: {:?}", x.shape(), out.shape());
    println!("Hybrid block combines SSM efficiency + Attention expressivity");
}
```

å‡ºåŠ›:
```
Input shape: (4, 8), Output shape: (4, 8)
Hybrid block combines SSM efficiency + Attention expressivity
```

**3è¡Œã®ã‚³ãƒ¼ãƒ‰ã§SSMâ†’Attentionâ†’MLPã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‹•ã‹ã—ãŸã€‚** ã“ã‚ŒãŒJamba [^1] ã®åŸºæœ¬æ§‹é€ ã ã€‚å®Ÿéš›ã®Jambaã¯:

- 8å±¤ã”ã¨ã«1å±¤ã®Attention (SSM:Attention = 7:1)
- 2å±¤ã”ã¨ã«Mixture-of-Experts (MoE)
- 256K context windowã€52B total params (12B active)

ã“ã®èƒŒå¾Œã«ã‚ã‚‹ç†è«–:

$$
\begin{aligned}
\text{Pure Attention:} \quad & O(N^2) \text{ compute, } O(N^2) \text{ memory} \\
\text{Pure SSM:} \quad & O(N) \text{ compute, } O(1) \text{ memory (inference)} \\
\text{Hybrid (7 SSM + 1 Attn):} \quad & O(N) \text{ average, } \text{Attention power preserved}
\end{aligned}
$$

Attentionã®å…¨ç³»åˆ—å‚ç…§èƒ½åŠ›ã‚’ä¿ã¡ãªãŒã‚‰ã€è¨ˆç®—é‡ã‚’SSMã§å‰Šæ¸›ã™ã‚‹ã€‚ã“ã‚ŒãŒãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®å“²å­¦ã ã€‚

> **Note:** **é€²æ—: 3% å®Œäº†** ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®æ§‹é€ ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰4ã¤ã®ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£(Jamba/Zamba/Griffin/StripedHyena)ã‚’è§¦ã£ã¦ã„ãã€‚

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” 4ã¤ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚’æ¯”è¼ƒã™ã‚‹

### 1.1 ä¸»è¦ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆæ€æƒ³

2024-2025å¹´ã«ç™»å ´ã—ãŸ4ã¤ã®ä»£è¡¨çš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚’è¦‹ã¦ã„ã“ã†ã€‚

| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | çµ„ç¹” | æˆ¦ç•¥ | ç‰¹å¾´ | è«–æ–‡/ãƒªãƒªãƒ¼ã‚¹ |
|:--------------|:-----|:-----|:-----|:-------------|
| **Jamba** | AI21 Labs | SSM + Attention + MoE ã‚’ layer äº¤äº’é…ç½® | 8å±¤ã«1å±¤Attentionã€2å±¤ã”ã¨ã«MoEã€‚256K context | [arXiv:2403.19887](https://arxiv.org/abs/2403.19887) [^1] |
| **Zamba** | Zyphra | Mamba + Shared Attention | 6 Mambaå±¤ã”ã¨ã«1ã¤ã®**å…±æœ‰Attention**ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸› | [arXiv:2405.16712](https://arxiv.org/abs/2405.16712) [^2] |
| **Griffin** | Google DeepMind | Gated Linear Recurrences + Local Attention | Hawk(RNN) + Griffin(Local Attn)ã€‚RecurrentGemmaã¸ | [arXiv:2402.19427](https://arxiv.org/abs/2402.19427) [^3] |
| **StripedHyena** | Together AI | Hyena (gated conv) + Attention | éŸ³å£°ãƒ»é•·ç³»åˆ—ç‰¹åŒ–ã€‚10-50%é«˜é€Ÿ | [Together AI Blog](https://www.together.ai/blog/stripedhyena-7b) [^5] |

ãã‚Œãã‚Œã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¼ã§è¡¨ãã†ã€‚

#### 1.1.1 Jamba: Layer Alternation (äº¤äº’é…ç½®)

$$
\begin{aligned}
\mathbf{h}_1 &= \text{Mamba}(\mathbf{x}) \quad \text{(SSM layer)} \\
\mathbf{h}_2 &= \text{Mamba}(\mathbf{h}_1 + \text{MLP}(\mathbf{h}_1)) \\
&\vdots \quad \text{(7 Mamba layers)} \\
\mathbf{h}_8 &= \text{Mamba}(\mathbf{h}_7) \\
\mathbf{h}_9 &= \text{Attention}(\mathbf{h}_8) \quad \text{(1 Attention layer every 8 layers)} \\
\mathbf{h}_{10} &= \text{MoE}(\mathbf{h}_9) \quad \text{(MoE every 2 layers)}
\end{aligned}
$$

**æ¯”ç‡**: SSM:Attention = 7:1ã€‚è¨ˆç®—é‡ã®å¤§éƒ¨åˆ†ã¯SSM($O(N)$)ã€Attentionã¯8å±¤ã«1å›ã ã‘æŒ¿å…¥ã€‚


#### 1.1.2 Zamba: Shared Attention (å…±æœ‰Attention)

Zambaã®é©æ–°ã¯ã€Œ**è¤‡æ•°ã®SSMå±¤ã§1ã¤ã®Attentionå±¤ã‚’å…±æœ‰**ã€ã™ã‚‹ç‚¹ã  [^2]ã€‚

$$
\begin{aligned}
\mathbf{h}_1 &= \text{Mamba}_1(\mathbf{x}) \\
&\vdots \quad \text{(6 Mamba layers)} \\
\mathbf{h}_6 &= \text{Mamba}_6(\mathbf{h}_5) \\
\mathbf{h}_7 &= \mathbf{h}_6 + \text{Attention}_\text{shared}(\mathbf{h}_6) \quad \text{(shared, reused)}
\end{aligned}
$$

**åˆ©ç‚¹**: Attentionå±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…±æœ‰ â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å‰Šæ¸› â†’ 7Bãƒ¢ãƒ‡ãƒ«ã§é«˜æ€§èƒ½ã€‚

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | Zamba 7B | Llama-2 7B | Mamba 7B |
|:----------|:---------|:-----------|:---------|
| Parameters | 7B | 7B | 7B |
| Memory (inference) | **ä½** (shared attn) | é«˜ | ä½ |
| Long context | **å¼·** | å¼± | å¼· |
| Associative recall | **å¼·** (attnè£œå®Œ) | ä¸­ | å¼± |


#### 1.1.3 Griffin: Local Attention + Gated Linear Recurrences

Google DeepMindã®Griffin [^3] ã¯ã€Œ**Local Attention + Gated Linear Recurrences**ã€ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã€‚

$$
\begin{aligned}
\text{Hawk (RNN):} \quad & \mathbf{h}_t = \text{RG}(\mathbf{h}_{t-1}, \mathbf{x}_t) \quad \text{(Recurrent Gating)} \\
\text{Griffin (Hybrid):} \quad & \mathbf{h}_t = \text{RG}(\mathbf{h}_{t-1}, \mathbf{x}_t) + \text{LocalAttn}(\mathbf{x}_{t-w:t+w})
\end{aligned}
$$

**Local Attention**: è¿‘å‚ $\pm w$ ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å‚ç…§ â†’ $O(N \cdot w)$ ($w \ll N$)ã€‚

| ãƒ¢ãƒ‡ãƒ« | Gated Recurrence | Attention | æ€§èƒ½ (Llama-2æ¯”) |
|:-------|:----------------|:----------|:-----------------|
| Hawk | âœ… | âŒ | Mambaè¶… |
| Griffin | âœ… | âœ… (Local) | Llama-2åŒ¹æ•µï¼ˆ6å€å°‘ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã§ï¼‰ |


#### 1.1.4 StripedHyena: Hyena + Attention

Together AIã®StripedHyena [^5] ã¯ã€Œ**Hyena operator (gated convolution) + Attention**ã€ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã€‚

$$
\begin{aligned}
\text{Hyena:} \quad & \mathbf{y} = \text{Conv}_\text{gated}(\mathbf{x}) \quad \text{(long convolution with gating)} \\
\text{StripedHyena:} \quad & \mathbf{y} = \alpha \cdot \text{Hyena}(\mathbf{x}) + (1-\alpha) \cdot \text{Attention}(\mathbf{x})
\end{aligned}
$$

**ç‰¹åŒ–é ˜åŸŸ**: éŸ³å£°ãƒ»é•·ç³»åˆ—ã€‚32K-131Kç³»åˆ—ã§10-50%é«˜é€Ÿã€ãƒ¡ãƒ¢ãƒª50%å‰Šæ¸›ã€‚

| Sequence Length | FlashAttention-2 | StripedHyena | Speedup |
|:----------------|:-----------------|:-------------|:--------|
| 32K | 100% | **110%** | 1.10x |
| 64K | 100% | **120%** | 1.20x |
| 131K | 100% | **150%** | 1.50x |


### 1.2 æ€§èƒ½æ¯”è¼ƒãƒãƒˆãƒªã‚¯ã‚¹

4ã¤ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®ç‰¹æ€§ã‚’æ•´ç†ã—ã‚ˆã†ã€‚

| è»¸ | Jamba | Zamba | Griffin | StripedHyena |
|:---|:------|:------|:--------|:-------------|
| **è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³** | Layeräº¤äº’ (7 SSM : 1 Attn) | Shared Attention (6 SSM : 1 shared Attn) | Local Attention + Recurrence | Weighted Mix (Hyena + Attn) |
| **è¨ˆç®—é‡** | $O(N)$ average | $O(N)$ (shared saves params) | $O(N \cdot w)$ (local) | $O(N \log N)$ (FFT conv) |
| **ãƒ¡ãƒ¢ãƒª (inference)** | ä¸­ | **ä½** (shared attn) | ä½ | **ä½** (50%å‰Šæ¸›) |
| **Long context** | **å¼·** (256K) | å¼· (é•·ç³»åˆ—å¾—æ„) | ä¸­ (localåˆ¶ç´„) | **å¼·** (131K+) |
| **Associative recall** | å¼· (Attn 1/8) | **å¼·** (shared attn) | ä¸­ | ä¸­ |
| **è¨“ç·´åŠ¹ç‡** | MoE 16 experts | é«˜ (param sharing) | é«˜ (6xå°‘ãªã„ãƒˆãƒ¼ã‚¯ãƒ³) | **é«˜** (10-20%é«˜é€Ÿ) |
| **æ¨è«–é€Ÿåº¦** | é«˜ (SSM dominant) | é«˜ | **é«˜** (ä½latency) | **é«˜** (1.5x @ 131K) |
| **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£** | 52B total (12B active) | 7B compact | 14B max | 7B |
| **é©ç”¨é ˜åŸŸ** | æ±ç”¨LLM | æ±ç”¨LLM (deviceåˆ¶ç´„) | æ±ç”¨LLM | éŸ³å£°ãƒ»é•·ç³»åˆ—ç‰¹åŒ– |

```mermaid
graph TD
    A[Hybrid Design Space] --> B[Layer Alternation<br/>Jamba 7:1]
    A --> C[Shared Attention<br/>Zamba 6:1 shared]
    A --> D[Local Attention<br/>Griffin window-based]
    A --> E[Weighted Mix<br/>StripedHyena Î±-blend]

    B --> F[Trade-off:<br/>Compute vs Expressivity]
    C --> F
    D --> F
    E --> F

    F --> G[No Universal Winner<br/>Task-dependent]

    style A fill:#f3e5f5
    style G fill:#ffebee
```

**é‡è¦ãªæ´å¯Ÿ**: ã©ã‚ŒãŒ"æœ€å¼·"ã‹ã§ã¯ãªãã€**ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‹**ã®ãŒæœ¬è³ªã ã€‚

> **Note:** **é€²æ—: 10% å®Œäº†** 4ã¤ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆæ€æƒ³ã¨æ€§èƒ½ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ä½“æ„Ÿã—ãŸã€‚æ¬¡ã¯ãªãœãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãŒå¿…è¦ãªã®ã‹ã€ç†è«–çš„å‹•æ©Ÿã‚’æ˜ã‚Šä¸‹ã’ã‚‹ã€‚

---


> Progress: 10%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $O(N)$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãªã®ã‹ï¼Ÿ

### 2.1 å˜ç‹¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é™ç•Œ

ç¬¬14-17å›ã§å­¦ã‚“ã Attentionã¨SSMã‚’æŒ¯ã‚Šè¿”ã‚ã†ã€‚ãã‚Œãã‚Œå¼·ã¿ã¨é™ç•ŒãŒã‚ã‚‹ã€‚

#### 2.1.1 Attentionã®å¼·ã¿ã¨é™ç•Œ

**å¼·ã¿**:
- **å…¨ç³»åˆ—å‚ç…§**: ä»»æ„ã®ä½ç½®é–“ã®é–¢ä¿‚ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ– ($Q_i K_j^\top$)
- **å‹•çš„é‡ã¿ä»˜ã‘**: å…¥åŠ›ã«å¿œã˜ã¦æ³¨æ„ã®åˆ†å¸ƒãŒå¤‰ã‚ã‚‹
- **Few-shot / In-Context Learning**: å°‘æ•°ä¾‹ã‹ã‚‰æ±åŒ– (ç¬¬14å›ã§å­¦ã‚“ã Emergent Abilities)
- **æ¨è«–ã‚¿ã‚¹ã‚¯**: Chain-of-Thought reasoningã€è¤‡é›‘ãªè«–ç†

**é™ç•Œ**:
- **$O(N^2)$ è¨ˆç®—é‡**: ç³»åˆ—é•·ãŒ2å€ã«ãªã‚‹ã¨è¨ˆç®—é‡4å€
- **$O(N^2)$ ãƒ¡ãƒ¢ãƒª**: KV-Cache ãŒé•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§çˆ†ç™º
- **é•·è·é›¢ä¾å­˜ã®æ¸›è¡°**: Attentionã¯è·é›¢ã«ä¾å­˜ã—ãªã„ãŒã€å®Ÿéš›ã«ã¯softmaxã®æ€§è³ªä¸Šã€é ã„ä½ç½®ã¸ã®æ³¨æ„ã¯å¼±ããªã‚‹

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V \quad \in O(N^2 d)
$$

#### 2.1.2 SSMã®å¼·ã¿ã¨é™ç•Œ

**å¼·ã¿**:
- **$O(N)$ è¨ˆç®—é‡**: ç·šå½¢æ™‚é–“ã§å‡¦ç† (ç¬¬16å›ã®Mamba)
- **$O(1)$ ãƒ¡ãƒ¢ãƒª (inference)**: çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{h}_t \in \mathbb{R}^d$ ã®ã¿ä¿æŒ
- **é•·è·é›¢è¨˜æ†¶**: HiPPOè¡Œåˆ—ã§è¨˜æ†¶ã‚’åœ§ç¸® (ç¬¬16å›ã®S4/Mambaç†è«–)
- **é«˜é€Ÿæ¨è«–**: å†å¸°å½¢æ…‹ã§é€æ¬¡ç”Ÿæˆ

**é™ç•Œ**:
- **Associative Recallå¼±ã„**: "Key-Value" å‹ã®æ¤œç´¢ãŒè‹¦æ‰‹ (Phonebook taskã§è¨¼æ˜ [^6])
- **In-Context LearningåŠ£ã‚‹**: Few-shotã§æ€§èƒ½ä½ä¸‹
- **å›ºå®šçš„ãªè¨˜æ†¶åœ§ç¸®**: Selective SSMã§æ”¹å–„ã—ãŸãŒã€Attentionã»ã©æŸ”è»Ÿã§ã¯ãªã„

$$
\begin{aligned}
\mathbf{h}_t &= \mathbf{A} \mathbf{h}_{t-1} + \mathbf{B} \mathbf{x}_t \\
\mathbf{y}_t &= \mathbf{C} \mathbf{h}_t + \mathbf{D} \mathbf{x}_t
\end{aligned}
\quad \text{(state evolution: } O(N) \text{)}
$$

### 2.2 ç›¸è£œçš„ãªç‰¹æ€§ â†’ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®å‹•æ©Ÿ

Attentionã¨SSMã¯**ç›¸è£œçš„**ã ã€‚

| ã‚¿ã‚¹ã‚¯ç‰¹æ€§ | Attentionæœ‰åˆ© | SSMæœ‰åˆ© |
|:----------|:-------------|:--------|
| **å…¨ç³»åˆ—å‚ç…§ãŒå¿…è¦** | âœ… | âŒ |
| **å‹•çš„é‡ã¿ä»˜ã‘** | âœ… | âŒ |
| **Few-shot learning** | âœ… | âŒ |
| **Associative recall** | âœ… | âŒ |
| **é•·ç³»åˆ—å‡¦ç†** | âŒ ($O(N^2)$) | âœ… ($O(N)$) |
| **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡** | âŒ | âœ… |
| **é€æ¬¡ç”Ÿæˆé€Ÿåº¦** | âŒ (KV-Cache) | âœ… (çŠ¶æ…‹æ›´æ–°ã®ã¿) |
| **è¨“ç·´ä¸¦åˆ—åŒ–** | âœ… | âœ… (convolutionå½¢æ…‹) |

```mermaid
graph LR
    A["ğŸ“Š Task Requirements"] --> B{å…¨ç³»åˆ—å‚ç…§<br/>vs<br/>é•·ç³»åˆ—åŠ¹ç‡}
    B -->|å…¨ç³»åˆ—å‚ç…§é‡è¦–| C["ğŸ”· Attention<br/>ICL, Reasoning"]
    B -->|é•·ç³»åˆ—åŠ¹ç‡é‡è¦–| D["ğŸ”¶ SSM<br/>Memory, Speed"]
    B -->|ä¸¡æ–¹å¿…è¦| E["ğŸ”€ Hybrid<br/>Best of Both"]

    E --> F["Jamba/Zamba/Griffin<br/>ç›¸è£œçš„ã«é…ç½®"]

    style A fill:#e1f5fe
    style E fill:#f3e5f5
    style F fill:#c8e6c9
```

**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®å“²å­¦**:
- **Attentionã§è£œã†**: SSMã®å¼±ç‚¹(associative recall, ICL)ã‚’Attentionå±¤ãŒè£œå®Œ
- **SSMã§åŠ¹ç‡åŒ–**: è¨ˆç®—é‡ã®å¤§éƒ¨åˆ†ã‚’SSM($O(N)$)ã§å‡¦ç†ã—ã€Attentionã¯å¿…è¦æœ€å°é™
- **Layeré…ç½®æœ€é©åŒ–**: ã©ã®å±¤ã‚’Attention/SSMã«ã™ã‚‹ã‹ â†’ è¨­è¨ˆç©ºé–“æ¢ç´¢ (Section 3.3)

### 2.3 Course IIã®å…¨ä½“åƒ â€” 10å›ã®æ—…è·¯

ç¬¬18å›ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€(ç¬¬9-18å›) ã®æœ€çµ‚å›ã ã€‚å…¨10å›ã®æ—…è·¯ã‚’ä¿¯ç°ã—ã‚ˆã†ã€‚

```mermaid
graph TD
    L9["ç¬¬9å›<br/>å¤‰åˆ†æ¨è«– & ELBO"] --> L10["ç¬¬10å›<br/>VAE & é›¢æ•£è¡¨ç¾"]
    L10 --> L11["ç¬¬11å›<br/>æœ€é©è¼¸é€ç†è«–"]
    L11 --> L12["ç¬¬12å›<br/>GANå®Œå…¨ç‰ˆ"]
    L12 --> L13["ç¬¬13å›<br/>è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«"]
    L13 --> L14["ç¬¬14å›<br/>AttentionåŸºç¤"]
    L14 --> L15["ç¬¬15å›<br/>AttentionåŠ¹ç‡åŒ–"]
    L15 --> L16["ç¬¬16å›<br/>SSM & Mamba"]
    L16 --> L17["ç¬¬17å›<br/>Mambaç™ºå±•"]
    L17 --> L18["ç¬¬18å›<br/>Hybrid<br/>(ä»Šã“ã“)"]

    L9 -.Course Iã®æ•°å­¦.-> Math["ç·šå½¢ä»£æ•°<br/>ç¢ºç‡è«–<br/>æ¸¬åº¦è«–<br/>æœ€é©åŒ–"]

    L18 --> C3["Course III<br/>å®Ÿè·µç·¨<br/>Trainâ†’Deploy"]

    style L18 fill:#ffeb3b
    style C3 fill:#c8e6c9
```

**Course IIåˆ°é”ç‚¹**:
- **ç†è«–çš„çµ±åˆ**: ELBO/OT/Nashå‡è¡¡/Attention=SSMåŒå¯¾æ€§ â€” å…¨ã¦ãŒ"åŒã˜ã‚‚ã®"ã®ç•°ãªã‚‹è¦–ç‚¹
- **å®Ÿè£…åŠ›**: Rust/Rustã§æ•°å¼â†’ã‚³ãƒ¼ãƒ‰1:1å¯¾å¿œ
- **æœ€æ–°ç ”ç©¶**: 2024-2026ã®SOTA (R3GAN, VAR, Mamba-2, Jamba) ã‚’ç†è§£

### 2.4 æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã¨ã®æ¯”è¼ƒ

| è¦³ç‚¹ | æ¾å°¾ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚º (Course IIå®Œäº†æ™‚ç‚¹) |
|:-----|:-------|:------------------------------|
| **å¤‰åˆ†æ¨è«–** | ELBOå°å‡ºã®ã¿ | VIå®Œå…¨ç‰ˆ (CAVI/SVI/SVGD/æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯) |
| **VAE** | åŸºæœ¬VAE | VAE + Disentanglement + VQ/FSQé›¢æ•£è¡¨ç¾ |
| **GAN** | DCGAN, WGAN-GP | GANå®Œå…¨ç‰ˆ (WGAN/f-GAN/R3GAN/StyleGAN) |
| **æœ€é©è¼¸é€** | è§¦ã‚Œãªã„ | OTå®Œå…¨ç†è«– + Sinkhorn + Neural OT |
| **è‡ªå·±å›å¸°** | è§¦ã‚Œãªã„ | ARå®Œå…¨ç‰ˆ (PixelCNN/WaveNet/Decodingæˆ¦ç•¥) |
| **Attention** | Transformeræ¦‚è¦ | AttentionåŸºç¤ + åŠ¹ç‡åŒ– (Flash/Sparse/Linear/MoE) |
| **SSM** | è§¦ã‚Œãªã„ | S4â†’Mambaâ†’Mamba-2å®Œå…¨ç‰ˆ + HiPPOç†è«– |
| **Hybrid** | è§¦ã‚Œãªã„ | **æœ¬è¬›ç¾© (Jamba/Zamba/Griffin/StripedHyena)** |
| **å®Ÿè£…** | PyTorchãƒ‡ãƒ¢ | Rustè¨“ç·´ + Rustæ¨è«– (Production-ready) |
| **æœ€æ–°æ€§** | 2023å¹´ã¾ã§ | **2024-2026 SOTA** |

**å·®åˆ¥åŒ–ã®æœ¬è³ª**: æ¾å°¾ç ”ãŒã€Œæ‰‹æ³•ã®ç´¹ä»‹ã€ã«ã¨ã©ã¾ã‚‹ã®ã«å¯¾ã—ã€æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ã€Œè«–æ–‡ãŒæ›¸ã‘ã‚‹ç†è«–çš„æ·±ã• + Productionå®Ÿè£… + æœ€æ–°ç ”ç©¶ã€ã®3è»¸ã‚’è²«ãã€‚

> **âš ï¸ Warning:** **ã“ã“ãŒè¸ã‚“å¼µã‚Šã©ã“ã‚**: Course IIã®ç†è«–ã¯ã“ã“ã§å®Œçµã™ã‚‹ã€‚Zone 3ã®æ•°å¼ä¿®è¡Œã§ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¨­è¨ˆã®æ•°å­¦çš„åŸºç›¤ã‚’å®Œå…¨ç†è§£ã™ã‚‹ã€‚Course IIIã§ã¯ç†è«–ã‚’ã€Œå‹•ãã‚·ã‚¹ãƒ†ãƒ ã€ã«å¤‰ãˆã‚‹å®Ÿè·µç·¨ãŒå¾…ã£ã¦ã„ã‚‹ã€‚

### 2.5 å­¦ç¿’æˆ¦ç•¥ â€” Course IIä¿®äº† â†’ Course IIIæº–å‚™

**Course IIä¿®äº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] ELBOå°å‡ºã‚’3é€šã‚Šã®æ–¹æ³•ã§èª¬æ˜ã§ãã‚‹ (ç¬¬9å›)
- [ ] VAEã®æ½œåœ¨ç©ºé–“è£œé–“ã‚’å®Ÿè£…ã§ãã‚‹ (ç¬¬10å›)
- [ ] Wassersteinè·é›¢ã¨KL divergenceã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹ (ç¬¬11å›)
- [ ] GANè¨“ç·´ã®Nashå‡è¡¡ã‚’å›³ç¤ºã§ãã‚‹ (ç¬¬12å›)
- [ ] è‡ªå·±å›å¸°ã®é€£é–å¾‹åˆ†è§£ã‚’æ›¸ã‘ã‚‹ (ç¬¬13å›)
- [ ] Attentionã® $O(N^2)$ è¨ˆç®—é‡ã‚’å°å‡ºã§ãã‚‹ (ç¬¬14å›)
- [ ] FlashAttentionã®Tilingæˆ¦ç•¥ã‚’èª¬æ˜ã§ãã‚‹ (ç¬¬15å›)
- [ ] Mambaã®Selective SSMã‚’å®Ÿè£…ã§ãã‚‹ (ç¬¬16å›)
- [ ] Attention=SSMåŒå¯¾æ€§ (SSD) ã‚’è¨¼æ˜ã§ãã‚‹ (ç¬¬17å›)
- [ ] Jamba/Zamba/Griffinã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¯”è¼ƒã§ãã‚‹ (ç¬¬18å› â€” æœ¬è¬›ç¾©)

**Course IIIäºˆå‘Š** (ç¬¬19-24å›: å®Ÿè·µç·¨):
- ç¬¬19å›: Elixirç™»å ´ â€” åˆ†æ•£æ¨è«–ãƒ»è€éšœå®³æ€§ (ğŸ”®åˆç™»å ´)
- ç¬¬20å›: è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ (ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€/åˆ†æ•£è¨“ç·´)
- ç¬¬21å›: è©•ä¾¡æŒ‡æ¨™ & ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (FID/LPIPS/Perplexity)
- ç¬¬22å›: ãƒ‡ãƒ—ãƒ­ã‚¤æˆ¦ç•¥ (ONNX/TensorRT/é‡å­åŒ–)
- ç¬¬23å›: MLOps (Monitoring/Logging/A/Bãƒ†ã‚¹ãƒˆ)
- ç¬¬24å›: Course IIIç·ã¾ã¨ã‚ + ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³äº‹ä¾‹

**å­¦ç¿’æ™‚é–“é…åˆ†** (æœ¬è¬›ç¾©):
- Zone 0-2 (å°å…¥): 30åˆ† â†’ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®å‹•æ©Ÿç†è§£
- Zone 3 (æ•°å¼): 60åˆ† â†’ **è¸ã‚“å¼µã‚Šã©ã“ã‚** (è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ•°å­¦)
- Zone 4-5 (å®Ÿè£…): 75åˆ† â†’ Rust/Rustã§æ‰‹ã‚’å‹•ã‹ã™
- Zone 6-7 (ç™ºå±•): 30åˆ† â†’ Course IIæŒ¯ã‚Šè¿”ã‚Š + Course IIIæº–å‚™

> **Note:** **é€²æ—: 20% å®Œäº†** ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®å‹•æ©Ÿã€Course IIå…¨ä½“åƒã€å­¦ç¿’æˆ¦ç•¥ã‚’ç†è§£ã—ãŸã€‚æ¬¡ã¯Zone 3ã®æ•°å¼ä¿®è¡Œ â€” ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¨­è¨ˆã®ç†è«–çš„åŸºç›¤ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

---


> Progress: 20%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $O(N)$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¨­è¨ˆã®ç†è«–

### 3.1 ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ•°å­¦çš„å®šå¼åŒ–

#### 3.1.1 ç´”ç²‹ãªTransformer/SSMã®å®šå¼åŒ–

ã¾ãšæ¯”è¼ƒã®ãŸã‚ã€ç´”ç²‹ãªTransformerã¨SSMã‚’å®šå¼åŒ–ã—ã‚ˆã†ã€‚

**Pure Transformer Block**:

$$
\begin{aligned}
\mathbf{z} &= \text{LayerNorm}(\mathbf{x}) \\
\mathbf{a} &= \text{MultiHeadAttention}(\mathbf{z}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O \\
\text{head}_i &= \text{softmax}\left(\frac{Q_i K_i^\top}{\sqrt{d_k}}\right) V_i \\
\mathbf{x}' &= \mathbf{x} + \mathbf{a} \quad \text{(residual connection)} \\
\mathbf{x}'' &= \mathbf{x}' + \text{FFN}(\text{LayerNorm}(\mathbf{x}'))
\end{aligned}
$$

**è¨ˆç®—é‡**:
- Attention: $O(N^2 d)$ (sequence length $N$, hidden dim $d$)
- FFN: $O(N d^2)$
- Total per layer: $O(N^2 d + N d^2)$ â†’ dominated by $O(N^2 d)$ for long sequences

**Pure SSM Block** (Mamba-style):

$$
\begin{aligned}
\mathbf{z} &= \text{LayerNorm}(\mathbf{x}) \\
\Delta_t, \mathbf{B}_t, \mathbf{C}_t &= \text{Linear}_\Delta(\mathbf{z}_t), \text{Linear}_B(\mathbf{z}_t), \text{Linear}_C(\mathbf{z}_t) \quad \text{(input-dependent)} \\
\mathbf{h}_t &= \bar{\mathbf{A}} \mathbf{h}_{t-1} + \bar{\mathbf{B}}_t \mathbf{z}_t \quad \text{(discretized SSM)} \\
\mathbf{y}_t &= \mathbf{C}_t \mathbf{h}_t \\
\mathbf{x}' &= \mathbf{x} + \mathbf{y} \quad \text{(residual)} \\
\mathbf{x}'' &= \mathbf{x}' + \text{FFN}(\text{LayerNorm}(\mathbf{x}'))
\end{aligned}
$$

**è¨ˆç®—é‡**:
- SSM (with hardware-aware scan): $O(N d)$
- FFN: $O(N d^2)$
- Total per layer: $O(N d + N d^2)$ â†’ dominated by $O(N d^2)$ (FFN), not $O(N^2)$

#### 3.1.2 Hybrid Block ã®ä¸€èˆ¬çš„å®šå¼åŒ–

ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¯ã€ŒAttentionå±¤ã¨SSMå±¤ã‚’ã©ã†çµ„ã¿åˆã‚ã›ã‚‹ã‹ã€ã§å®šç¾©ã•ã‚Œã‚‹ã€‚

**General Hybrid Layer**:

$$
\mathbf{x}_{l+1} = \begin{cases}
\mathbf{x}_l + \text{Attention}(\mathbf{x}_l) + \text{FFN}(\mathbf{x}_l) & \text{if } l \in \mathcal{L}_\text{attn} \\
\mathbf{x}_l + \text{SSM}(\mathbf{x}_l) + \text{FFN}(\mathbf{x}_l) & \text{if } l \in \mathcal{L}_\text{ssm}
\end{cases}
$$

ã“ã“ã§ $\mathcal{L}_\text{attn}, \mathcal{L}_\text{ssm}$ ã¯ Attentionå±¤/SSMå±¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é›†åˆã€‚

**è¨­è¨ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- **Layeræ¯”ç‡** $r = |\mathcal{L}_\text{attn}| / (|\mathcal{L}_\text{attn}| + |\mathcal{L}_\text{ssm}|)$
- **é…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³**: äº¤äº’ / ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ / ãƒ©ãƒ³ãƒ€ãƒ 
- **Shared weights**: Attentionå±¤ã®é‡ã¿å…±æœ‰ (Zambaã‚¹ã‚¿ã‚¤ãƒ«)

#### 3.1.3 è¨ˆç®—é‡è§£æ

$L$ å±¤ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã§ã€Attentionå±¤ãŒ $L_a$ å±¤ã€SSMå±¤ãŒ $L_s$ å±¤ ($L = L_a + L_s$)ã€‚

$$
\begin{aligned}
\text{Total compute} &= L_a \cdot O(N^2 d) + L_s \cdot O(N d) + L \cdot O(N d^2) \\
&= O(L_a N^2 d + L_s N d + L N d^2)
\end{aligned}
$$

**Attentionæ¯”ç‡** $r = L_a / L$ ã®ã¨ã:

$$
\text{Compute} = O(r L N^2 d + (1-r) L N d + L N d^2)
$$

**Jamba ã®å ´åˆ** ($r = 1/8$):

$$
\text{Compute} = O\left(\frac{L}{8} N^2 d + \frac{7L}{8} N d + L N d^2\right) \approx O(L N^2 d / 8) \quad \text{(for large } N \text{)}
$$

â†’ ç´”ç²‹ãªTransformerã® $1/8$ ã® Attentionè¨ˆç®—é‡ (æ®‹ã‚Š $7/8$ ã¯SSM)ã€‚


å‡ºåŠ› (æ¦‚ç®—):


**æ´å¯Ÿ**: Jamba/Zambaã¯Transformerã® $1/4 \sim 1/5$ ã®è¨ˆç®—é‡ã§ã€Attentionã®è¡¨ç¾åŠ›ã‚’ä¿æŒã€‚

#### 3.1.4 ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è©³ç´°è§£æ

è¨ˆç®—é‡ã ã‘ã§ãªãã€**ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**ã‚‚é‡è¦ãªè¨­è¨ˆæŒ‡æ¨™ã ã€‚

**Pure Transformer ã®ãƒ¡ãƒ¢ãƒª**:

æ¨è«–æ™‚ã€KV-Cache ã‚’ä¿æŒã™ã‚‹å¿…è¦ãŒã‚ã‚‹:

$$
\begin{aligned}
\text{Memory}_\text{Transformer} &= 2 \cdot L \cdot N \cdot d \quad \text{(K, Vä¸¡æ–¹)} \\
&= O(L N d)
\end{aligned}
$$

ä¾‹: $L=24$, $N=8192$, $d=2048$ â†’ Memory = $2 \times 24 \times 8192 \times 2048 \times 4\text{ bytes} = 3.2\text{ GB}$

**Pure SSM ã®ãƒ¡ãƒ¢ãƒª**:

çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{h} \in \mathbb{R}^d$ ã®ã¿:

$$
\text{Memory}_\text{SSM} = L \cdot d = O(L d)
$$

ä¾‹: $L=24$, $d=2048$ â†’ Memory = $24 \times 2048 \times 4\text{ bytes} = 196\text{ KB}$

**Hybrid ã®ãƒ¡ãƒ¢ãƒª**:

Attentionå±¤ã®ã¿KV-Cache:

$$
\text{Memory}_\text{Hybrid} = 2 \cdot L_\text{attn} \cdot N \cdot d + L_\text{ssm} \cdot d
$$

Jamba ($L_\text{attn}=3$, $L_\text{ssm}=21$):

$$
\text{Memory}_\text{Jamba} = 2 \times 3 \times 8192 \times 2048 \times 4 + 21 \times 2048 \times 4 = 402\text{ MB}
$$

**æ¯”è¼ƒè¡¨**:

| Model | Compute (GFLOPs) | Memory (æ¨è«–) | Memoryæ¯” |
|:------|:----------------|:-------------|:---------|
| Pure Transformer | 824.6 | 3.2 GB | 1.00x |
| Jamba (1/8 attn) | 194.1 | 402 MB | 0.12x |
| Pure SSM | 108.5 | 196 KB | 0.00006x |

**æ´å¯Ÿ**: Jambaã¯ãƒ¡ãƒ¢ãƒªã‚’ **12%** ã«å‰Šæ¸›ã€‚SSMã¯æ¥µã‚ã¦çœãƒ¡ãƒ¢ãƒª (1ä¸‡åˆ†ã®1ä»¥ä¸‹)ã€‚


å‡ºåŠ›:


#### 3.1.5 ãƒãƒƒãƒå‡¦ç†æ™‚ã®ä¸¦åˆ—æ€§

Hybridè¨­è¨ˆã¯**ãƒãƒƒãƒå‡¦ç†ã®ä¸¦åˆ—æ€§**ã«ã‚‚å½±éŸ¿ã™ã‚‹ã€‚

**Attention**: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸¦åˆ—å‡¦ç†å¯èƒ½ â†’ GPU utilization é«˜

$$
\text{Attention}(\mathbf{X}) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V \quad \text{(å…¨ã¦è¡Œåˆ—æ¼”ç®—)}
$$

**SSM**: å†å¸°å½¢æ…‹ã§ã¯é€æ¬¡å‡¦ç† â†’ ä¸¦åˆ—åŒ–å›°é›£

$$
\mathbf{h}_t = \mathbf{A} \mathbf{h}_{t-1} + \mathbf{B} \mathbf{x}_t \quad \text{(} t \text{ ã«ä¾å­˜)}
$$

ãŸã ã—ã€**è¨“ç·´æ™‚**ã¯convolutionå½¢æ…‹ã§FFTä¸¦åˆ—åŒ–å¯èƒ½ (ç¬¬16å›Mambaå‚ç…§)ã€‚

**Hybrid ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:

| Phase | Pure Transformer | Pure SSM | Hybrid |
|:------|:----------------|:---------|:-------|
| **è¨“ç·´** | é«˜ä¸¦åˆ— (Attn) | é«˜ä¸¦åˆ— (Convå½¢æ…‹) | é«˜ä¸¦åˆ— |
| **æ¨è«–** | ä½ä¸¦åˆ— (KVé€æ¬¡è¿½åŠ ) | ä½ä¸¦åˆ— (å†å¸°) | ä¸­ä¸¦åˆ— |
| **ãƒãƒƒãƒæ¨è«–** | é«˜ä¸¦åˆ— | ä¸­ä¸¦åˆ— | é«˜ä¸¦åˆ— (Attnå±¤ã§ä¸¦åˆ—) |

**æœ€é©åŒ–æˆ¦ç•¥**:

1. **è¨“ç·´**: Attention/SSMä¸¡æ–¹ã¨ã‚‚ä¸¦åˆ—åŒ–å¯èƒ½ â†’ GPUæ´»ç”¨
2. **å˜ä¸€æ¨è«–**: SSMå„ªä½ (çŠ¶æ…‹æ›´æ–°ã®ã¿ã€$O(1)$)
3. **ãƒãƒƒãƒæ¨è«–**: Hybridæœ‰åˆ© (Attentionå±¤ã§ãƒãƒƒãƒä¸¦åˆ—ã€SSMå±¤ã§åŠ¹ç‡)

### 3.2 è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†é¡å­¦

#### 3.2.1 Pattern 1: Layer Alternation (å±¤äº¤äº’é…ç½®)

**å®šç¾©**: Attentionå±¤ã¨SSMå±¤ã‚’è¦å‰‡çš„ã«äº¤äº’é…ç½®ã€‚

$$
\mathcal{L}_\text{attn} = \{l \mid l \bmod k = 0\}, \quad k \in \mathbb{Z}^+
$$

ä¾‹: Jamba ($k=8$) â†’ 8å±¤ã”ã¨ã«1å±¤Attentionã€‚

**åˆ©ç‚¹**:
- ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­è¨ˆ
- å„å±¤ã®å½¹å‰²ãŒæ˜ç¢º
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å°‘ãªã„ ($k$ ã®ã¿)

**æ¬ ç‚¹**:
- å›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³ â†’ ã‚¿ã‚¹ã‚¯ã«å¿œã˜ãŸæŸ”è»Ÿæ€§ä½ã„

#### 3.2.2 Pattern 2: Shared Attention (å…±æœ‰Attention)

**å®šç¾©**: è¤‡æ•°ã®SSMå±¤ã§1ã¤ã®Attentionå±¤ã‚’å…±æœ‰ã€‚

$$
\mathbf{a}_{\text{shared}} = \text{Attention}(\mathbf{x}; \theta_{\text{shared}}) \quad \text{(same } \theta \text{ for multiple layers)}
$$

Zambaã®å ´åˆ: 6 SSMå±¤ã”ã¨ã«å…±æœ‰Attentionã€‚

**åˆ©ç‚¹**:
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å‰Šæ¸› (Attentionå±¤ã®é‡ã¿å…±æœ‰)
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Š

**æ¬ ç‚¹**:
- å±¤ã”ã¨ã®ç‰¹åŒ–ãŒé›£ã—ã„ (åŒã˜Attentionã‚’ä½¿ã„å›ã™)

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ç‡**:

$$
\text{Param reduction} = \frac{(k-1) \cdot |\theta_{\text{attn}}|}{k \cdot |\theta_{\text{ssm}}| + |\theta_{\text{attn}}|}
$$

Zamba ($k=6$): Attentionå±¤ã‚’ $1/6$ ã«å‰Šæ¸› â†’ å…¨ä½“ã§ç´„10-15%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã€‚

**è©³ç´°è¨ˆç®—**:

Attentionå±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (single-head, ç°¡ç•¥ç‰ˆ):

$$
|\theta_{\text{attn}}| = 4 \cdot d^2 \quad \text{(}W^Q, W^K, W^V, W^O\text{)}
$$

SSMå±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:

$$
|\theta_{\text{ssm}}| = 3 \cdot d^2 \quad \text{(}A, B, C\text{)}
$$

Pure Transformer (24å±¤):

$$
\text{Total params} = 24 \times 4d^2 = 96d^2
$$

Zamba (22 SSM + 2 Shared Attention):

$$
\begin{aligned}
\text{Total params} &= 22 \times 3d^2 + 2 \times 4d^2 \\
&= 66d^2 + 8d^2 = 74d^2
\end{aligned}
$$

å‰Šæ¸›ç‡:

$$
\text{Reduction} = \frac{96d^2 - 74d^2}{96d^2} = \frac{22}{96} \approx 23\%
$$


å‡ºåŠ›:


**æ´å¯Ÿ**: Shared Attentionã¯ç‹¬ç«‹Attention (Jamba) ã‚ˆã‚Š10%å‰Šæ¸›ã€‚Pure SSMãŒæœ€å°ã ãŒã€æ€§èƒ½ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã€‚

#### 3.2.3 Pattern 3: Local + Global (å±€æ‰€+å¤§åŸŸ)

**å®šç¾©**: Local Attention (è¿‘å‚ã®ã¿) + SSMã®å¤§åŸŸçš„æ–‡è„ˆã€‚

$$
\begin{aligned}
\text{Local Attention:} \quad & \text{Attend only to } [i - w, i + w] \\
\text{SSM:} \quad & \text{Capture global context via state } \mathbf{h}_t
\end{aligned}
$$

Griffin/RecurrentGemmaã®æˆ¦ç•¥ã€‚

**Local Attention ã®è¨ˆç®—é‡**:

$$
O(N \cdot w \cdot d) \quad \text{(window size } w \ll N \text{)}
$$

**åˆ©ç‚¹**:
- $w$ ã‚’å°ã•ãã™ã‚Œã° $O(N)$ ã«è¿‘ã¥ã
- Local: ç´°éƒ¨æ•æ‰ã€SSM: å¤§åŸŸçš„æ–‡è„ˆ

**æ¬ ç‚¹**:
- ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å¤–ã®ä¾å­˜é–¢ä¿‚ã‚’ç›´æ¥æ•æ‰ã§ããªã„

#### 3.2.4 Pattern 4: Weighted Combination (é‡ã¿ä»˜ãçµåˆ)

**å®šç¾©**: Attentionã¨SSMã®å‡ºåŠ›ã‚’é‡ã¿ä»˜ãå’Œã€‚

$$
\mathbf{y} = \alpha \cdot \text{SSM}(\mathbf{x}) + (1 - \alpha) \cdot \text{Attention}(\mathbf{x}), \quad \alpha \in [0, 1]
$$

StripedHyenaã®æˆ¦ç•¥ (Hyena = gated convolution)ã€‚

**åˆ©ç‚¹**:
- é€£ç¶šçš„ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•èª¿æ•´
- ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦ $\alpha$ ã‚’å­¦ç¿’å¯èƒ½

**æ¬ ç‚¹**:
- ä¸¡æ–¹ã‚’è¨ˆç®— â†’ è¨ˆç®—é‡ã¯å‰Šæ¸›ã•ã‚Œãªã„ (ä¸¦åˆ—å®Ÿè¡Œã¯å¯èƒ½)

### 3.3 è¨­è¨ˆç©ºé–“æ¢ç´¢ (Design Space Exploration)

#### 3.3.1 æ¢ç´¢ã™ã¹ããƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆç©ºé–“ã¯åºƒå¤§ã ã€‚

| ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | é¸æŠè‚¢ | Jambaã®è¨­å®š | Zambaã®è¨­å®š |
|:------------------|:------|:-----------|:-----------|
| Layeræ¯”ç‡ $r$ | $[0, 1]$ | $1/8 = 0.125$ | $1/12 \approx 0.083$ |
| é…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³ | Alternation / Block / Random | Alternation (every 8) | Block (6 SSM + 1 shared Attn) |
| Shared weights | Yes / No | No | Yes (Attn shared) |
| Local window $w$ | $[0, N]$ | N/A (global) | N/A |
| MoEçµ±åˆ | Yes / No | Yes (every 2 layers) | No |
| Headæ•° (Attn) | $[1, \infty)$ | 32 | 24 |
| State dim (SSM) | $[16, 256]$ | 16 (Mamba default) | 16 |

**æ¢ç´¢æ–¹æ³•**:
1. **Grid Search**: çµ„ã¿åˆã‚ã›ã‚’åˆ—æŒ™ (è¨ˆç®—é‡å¤§)
2. **Random Search**: ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (åŠ¹ç‡çš„)
3. **Neural Architecture Search (NAS)**: è‡ªå‹•æ¢ç´¢ (é«˜ã‚³ã‚¹ãƒˆ)
4. **Ablation Study**: 1ã¤ãšã¤å¤‰æ›´ã—ã¦åŠ¹æœæ¸¬å®š

#### 3.3.2 Jamba ã®è¨­è¨ˆæ±ºå®šã®ç†è«–çš„æ ¹æ‹ 

Jambaã®è¨­è¨ˆ [^1] ã¯ empirical study ã«åŸºã¥ã:

**å®Ÿé¨“çµæœ** (8B scale):
- Pure Mamba: æ¨™æº–LMã‚¿ã‚¹ã‚¯ã§ competitiveã€**ã ãŒ** associative recall (Phonebook task) ã§å¤§å¹…åŠ£åŒ–
- Mamba-2-Hybrid (7-8% Attention): Phonebook task è§£æ±º + Transformerè¶…ãˆ

**çµè«–**: 8å±¤ã«1å±¤Attention ($r=1/8$) ã§ååˆ† â†’ Jambaã®è¨­è¨ˆã«æ¡ç”¨ã€‚

$$
\begin{aligned}
\text{Performance} &\approx f(r) \quad \text{where } f \text{ is task-dependent} \\
\text{Jamba:} \quad & r = 1/8 \text{ balances compute vs expressivity}
\end{aligned}
$$


å‡ºåŠ› (æ¦‚ç®—):


**æ´å¯Ÿ**: $r=0.125$ (Jamba) ã§ Recallæ€§èƒ½ãŒ70%å›å¾©ã€ã‚³ã‚¹ãƒˆã¯ Pure Transformerã®20%ã€‚**Paretoæœ€é©ã«è¿‘ã„**ã€‚

### 3.4 âš”ï¸ Boss Battle: Hybrid Attention-SSM Block ã®å®Œå…¨ç†è§£

**Challenge**: Jambaã‚¹ã‚¿ã‚¤ãƒ«ã®Hybrid Blockã‚’æ•°å¼â†’ã‚³ãƒ¼ãƒ‰â†’å®Ÿè¡Œã¾ã§å®Œå…¨å†ç¾ã›ã‚ˆã€‚

#### Step 1: æ•°å¼å®šç¾©

Jamba Hybrid Block (ç°¡ç•¥ç‰ˆ):

$$
\begin{aligned}
\text{Input:} \quad & \mathbf{x} \in \mathbb{R}^{N \times d} \\
\text{SSM Layer (if } l \notin \mathcal{L}_\text{attn}\text{):} \\
\mathbf{z} &= \text{LayerNorm}(\mathbf{x}) \\
\mathbf{h}_t &= \bar{\mathbf{A}} \mathbf{h}_{t-1} + \bar{\mathbf{B}}_t \mathbf{z}_t \quad \text{(Mamba recurrence)} \\
\mathbf{y} &= \mathbf{C} \mathbf{h} \\
\mathbf{x}' &= \mathbf{x} + \mathbf{y} \\
\text{Attention Layer (if } l \in \mathcal{L}_\text{attn}\text{):} \\
\mathbf{z} &= \text{LayerNorm}(\mathbf{x}) \\
Q, K, V &= \mathbf{z} W^Q, \mathbf{z} W^K, \mathbf{z} W^V \\
\text{Attn}(\mathbf{z}) &= \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V \\
\mathbf{x}' &= \mathbf{x} + \text{Attn}(\mathbf{z}) \\
\text{FFN (always):} \\
\mathbf{x}'' &= \mathbf{x}' + \text{FFN}(\text{LayerNorm}(\mathbf{x}'))
\end{aligned}
$$

#### Step 2: Rustã‚³ãƒ¼ãƒ‰å®Ÿè£…


å‡ºåŠ›:


#### Step 3: æ¤œè¨¼

**æ¤œè¨¼é …ç›®**:
1. **Layeræ¯”ç‡**: 16å±¤ä¸­2å±¤ãŒAttention ($2/16 = 1/8$) âœ…
2. **Residual connection**: $\mathbf{x}'' = \mathbf{x}' + \text{residual}$ âœ…
3. **LayerNorm**: å„sub-layerå‰ã«é©ç”¨ âœ…
4. **è¨ˆç®—é‡**: SSMå±¤ã¯ $O(N d^2)$ã€Attentionå±¤ã¯ $O(N^2 d)$ âœ…

**è¿½åŠ æ¤œè¨¼: æ•°å€¤å®‰å®šæ€§**


å‡ºåŠ›:


**è¿½åŠ æ¤œè¨¼: å‹¾é…ãƒ•ãƒ­ãƒ¼**

LayerNormã¨Residual connectionãŒå‹¾é…æ¶ˆå¤±ã‚’é˜²ãã“ã¨ã‚’ç¢ºèªã€‚


å‡ºåŠ›:


**Boss Battleå®Œäº†** â€” Jamba-style Hybrid Blockã®å®Œå…¨å®Ÿè£…ãƒ»æ¤œè¨¼ã‚’é”æˆã—ãŸã€‚

### 3.10 æœ€æ–°ã®Hybrid SSM Architectures (2024-2025)

2024å¹´ã‹ã‚‰2025å¹´ã«ã‹ã‘ã¦ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç ”ç©¶ãŒåŠ é€Ÿã—ã¦ã„ã‚‹ã€‚Jambaä»¥é™ã€Zambaã€Griffinã€Sambaã€StripedHyenaãªã©å¤šæ§˜ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒç™»å ´ã—ã€SSMã¨Attentionã®çµ±åˆã«é–¢ã™ã‚‹ç†è«–çš„ç†è§£ãŒæ·±ã¾ã£ãŸ[@lieber2024jamba][@waleffe2024empirical]ã€‚

### 3.10.1 Zamba: Shared Attention + Hybrid SSM

Zamba [^2] ã¯Jambaã¨ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§HybridSSMã‚’å®Ÿè£…ã™ã‚‹ã€‚**Shared Attention** ã‚’æ¡ç”¨ã—ã€è¤‡æ•°ã®Mambaå±¤ã«1ã¤ã®Attentionå±¤ã‚’å…±æœ‰ã™ã‚‹ã“ã¨ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚

**Zambaã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ•°å­¦çš„å®šå¼åŒ–**:

$L$ å±¤ã®Zambaã§ã€6 Mambaå±¤ã”ã¨ã«1ã¤ã®å…±æœ‰Attentionå±¤ã‚’æŒ¿å…¥:

$$
\mathcal{L}_\text{shared\_attn} = \{6, 12, 18, \ldots\}, \quad |\mathcal{L}_\text{shared\_attn}| = L / 6
$$

å…±æœ‰Attentionå±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta_\text{attn}$ ã¯å…¨ã¦ã® $l \in \mathcal{L}_\text{shared\_attn}$ ã§åŒã˜:

$$
\text{Attn}_l(x; \theta_\text{attn}) = \text{softmax}\!\left(\frac{x W^Q_\text{shared} \cdot (x W^K_\text{shared})^\top}{\sqrt{d_k}}\right) x W^V_\text{shared} \quad \forall l \in \mathcal{L}_\text{shared\_attn}
$$

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã®è¨ˆç®—**:

Jambaæ–¹å¼ (ç‹¬ç«‹Attention) vs Zambaæ–¹å¼ (å…±æœ‰Attention) ã®æ¯”è¼ƒ:

$$
\Delta\text{Params} = \left(\frac{L}{8} - 1\right) \times |\theta_\text{attn}| \quad \text{(Jamba} \to \text{Zamba)}
$$

24å±¤ãƒ¢ãƒ‡ãƒ« ($L=24$)ã€Attentionå±¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $|\theta_\text{attn}| = 4d^2$:
- Jamba: $3$ Attentionå±¤ â†’ $12d^2$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- Zamba: $4$ Attentionä½ç½®ã ãŒå…¨ã¦å…±æœ‰ â†’ $4d^2$ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

å‰Šæ¸›: $\frac{12d^2 - 4d^2}{12d^2} = 67\%$ ã®Attentionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã€‚

**å…±æœ‰Attentionã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:

å…±æœ‰Attentionã®æ¬ ç‚¹ã¯ã€Œå±¤ã”ã¨ã®ç‰¹åŒ–ã€ãŒå¤±ã‚ã‚Œã‚‹ã“ã¨ã€‚å„å±¤ã§ç•°ãªã‚‹æ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã§ãã‚‹Jambaã¨ç•°ãªã‚Šã€Zambaã®å…¨Attentionå±¤ã¯**åŒã˜æ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³**ã‚’ä½¿ã†ã€‚ã“ã‚Œã¯:

$$
\text{Zamba Attn layer } l: \quad P_l(i,j) = \text{softmax}(q_i^l k_j^l / \sqrt{d})_{ij}
$$

ã§ã¯ Zamba ã§ã¯ $q^l = x_l W^Q_\text{shared}$ ã§å…¨å±¤å…±é€š â†’ å±¤ã”ã¨ã« $x_l$ ãŒå¤‰åŒ–ã™ã‚‹ã®ã§æ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ç•°ãªã‚‹ãŒã€**å¤‰æ›è¡Œåˆ—ãŒå…±æœ‰**ã•ã‚Œã‚‹ç‚¹ã§Jambaã‚ˆã‚ŠæŸ”è»Ÿæ€§ã¯ä½ã„ã€‚

**è¨ˆç®—é‡æ¯”è¼ƒ:**

| Model | Parameters | Active Params | FLOPs (per token) | Memory |
|-------|-----------|---------------|-------------------|--------|
| Dense Jamba | 7B | 7B | ~14 GFLOPs | 14 GB |
| Zamba (Shared Attn) | 7B | 7B | ~10 GFLOPs | 10 GB |
| GPT-3 Dense | 7B | 7B | ~28 GFLOPs | 14 GB |

Zambaã¯**Shared Attention**ã«ã‚ˆã‚Šã€Jambaã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½¿ç”¨ã‚’å®Ÿç¾ã™ã‚‹ã€‚

### 3.10.2 Griffin: Gated Linear RNN + Local Attention

Googleã®Griffinã¯**Hawk (gated linear RNN)** ã¨**local attention**ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚‹ [^3]ã€‚Mambaã®selective SSMã®ä»£ã‚ã‚Šã«ã€ã‚ˆã‚Šå˜ç´”ãªgated linear RNNã‚’ä½¿ç”¨ã™ã‚‹ã€‚

**Hawkã®å®Œå…¨å®šå¼åŒ–**:

Hawkã®çŠ¶æ…‹æ›´æ–°å‰‡ã¯æ¬¡ã®2ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰ãªã‚‹:

**Step 1 â€” Gated Linear Recurrence**:

$$
\begin{aligned}
r_t &= \sigma(W_r x_t + b_r) \quad \in (0, 1)^d \quad \text{(recurrence gate)} \\
i_t &= \sigma(W_i x_t + b_i) \quad \in (0, 1)^d \quad \text{(input gate)} \\
a_t &= \text{diag}(r_t)^{1/\Delta} \quad \in (0, 1)^d \quad \text{(decay coefficient)}
\end{aligned}
$$

ã“ã“ã§ $\Delta > 0$ ã¯æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«å®šæ•° (å®Ÿè£…ä¸Šã¯ $\Delta = 8$ ãªã©ã®å›ºå®šå€¤)ã€‚$a_t$ ã¯ $r_t$ ã‚’ $1/\Delta$ ä¹—ã™ã‚‹ã“ã¨ã§é©åˆ‡ãªæ¸›è¡°ç‡ã«èª¿æ•´ã™ã‚‹ã€‚

**Step 2 â€” State Update**:

$$
h_t = a_t \odot h_{t-1} + \sqrt{1 - a_t^2} \odot (i_t \odot x_t)
$$

ã“ã®å¼ã®ç›´æ„Ÿ: $a_t$ ãŒå¤§ãã„ â†’ éå»ã®çŠ¶æ…‹ $h_{t-1}$ ã‚’å¼·ãä¿æŒã€‚$\sqrt{1 - a_t^2}$ ã¯ $\|h_t\|_2 = \|h_{t-1}\|_2 = 1$ ã‚’ä¿ã¤æ­£è¦åŒ–é … â€” **å˜ä½ãƒãƒ«ãƒ ä¿å­˜ã®è¨­è¨ˆ**ã ã€‚

**Step 3 â€” Output Projection**:

$$
y_t = \text{LayerNorm}(h_t) \cdot W_o + b_o
$$

LayerNormã‚’æŒŸã‚€ã“ã¨ã§ã€é•·ç³»åˆ—ã§ã‚‚æ´»æ€§åŒ–ãŒç™ºæ•£ã—ãªã„æ•°å€¤å®‰å®šæ€§ã‚’ç¢ºä¿ã™ã‚‹ã€‚

**Mambaã¨Hawkã®æ¯”è¼ƒ**:

| ç‰¹æ€§ | Mamba (Selective SSM) | Hawk (Gated Linear RNN) |
|:-----|:---------------------|:------------------------|
| çŠ¶æ…‹æ›´æ–° | $h_t = \bar{A}_t h_{t-1} + \bar{B}_t u_t$ | $h_t = a_t \odot h_{t-1} + \sqrt{1-a_t^2} \odot (i_t \odot x_t)$ |
| ã‚²ãƒ¼ãƒˆæ•° | 1 ($\Delta_t$) | 2 ($r_t, i_t$) |
| çŠ¶æ…‹æ¬¡å…ƒ | $\mathbb{R}^{d \cdot N}$ ($N$: state dim) | $\mathbb{R}^d$ |
| æ­£è¦åŒ– | ãªã— (HiPPOåˆæœŸåŒ–ã§å®‰å®š) | $\sqrt{1 - a_t^2}$ ã§å˜ä½ãƒãƒ«ãƒ ä¿å­˜ |
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | $\Delta, B, C, A$ å…¨ã¦å…¥åŠ›ä¾å­˜ | $r, i$ ã®ã¿å…¥åŠ›ä¾å­˜ã€$A$ ã¯å›ºå®šå¯¾è§’ |
| ç†è«–çš„åŸºç›¤ | HiPPO + é€£ç¶šæ™‚é–“ODE | LSTM/GRUã®ç·šå½¢åŒ– |

**Griffin ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**:

Griffinã¯Hawkãƒ–ãƒ­ãƒƒã‚¯ã¨Local Attentionãƒ–ãƒ­ãƒƒã‚¯ã‚’**äº¤äº’**ã«é…ç½®ã™ã‚‹ (æ¯”ç‡ 1:1):

$$
x_{l+1} = \begin{cases}
x_l + \text{Hawk}_l(x_l) + \text{MLP}_l(x_l) & l \text{ ã¯å¥‡æ•°å±¤} \\
x_l + \text{LocalAttn}_l(x_l) + \text{MLP}_l(x_l) & l \text{ ã¯å¶æ•°å±¤}
\end{cases}
$$

Local Attentionã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º $w = 1024$ (å…¸å‹å€¤) ã‚’ä½¿ãˆã°ã€ç³»åˆ—é•· $N$ ã«å¯¾ã—ã¦ $O(Nw) = O(N)$ ã®è¨ˆç®—é‡ã§å±€æ‰€çš„ãªä¾å­˜æ€§ã‚’ç²¾å¯†ã«æ‰ãˆã‚‰ã‚Œã‚‹ã€‚HawkãŒå¤§åŸŸçš„ãªè¨˜æ†¶ã‚’ã€Local AttentionãŒå±€æ‰€çš„ãªæ–‡è„ˆã‚’æ‹…ã†**åˆ†æ¥­æ§‹é€ **ã ã€‚

**RecurrentGemmaã¨ã®é–¢ä¿‚**:

Griffinã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯Google DeepMindã®**RecurrentGemma** [^3] ã¨ã—ã¦è£½å“åŒ–ã•ã‚ŒãŸã€‚Gemmaã®èªå½™ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æµç”¨ã—ã¤ã¤ã€ã‚³ã‚¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’Griffinã«ç½®ãæ›ãˆã‚‹ã“ã¨ã§ã€åŒè¦æ¨¡ã®Transformerã«æ¯”ã¹æ¨è«–æ™‚ã®ãƒ¡ãƒ¢ãƒªã‚’å¤§å¹…å‰Šæ¸› ($O(Nd)$ â†’ $O(d)$) ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚

**Griffinã®ç‰¹å¾´**:

- **Simplicity**: Mambaã®selective SSMã‚ˆã‚Šå˜ç´”ãªå®Ÿè£…
- **Efficiency**: Local attentionã§O(NÂ²)ã‚’å›é¿
- **Trade-off**: é•·æœŸä¾å­˜æ€§ã®æ•æ‰èƒ½åŠ›ã¯Mambaã‚ˆã‚Šã‚„ã‚„åŠ£ã‚‹

### 3.10.3 Jamba vs Zamba vs Griffin: ä½“ç³»çš„æ¯”è¼ƒ

3ã¤ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®šé‡çš„ã«æ¯”è¼ƒã™ã‚‹[@waleffe2024empirical]:


å‡ºåŠ›:


**Key Findings:**

1. **Throughput**: Zamba > Griffin > Jamba (sparse activationã®åŠ¹æœ)
2. **Memory**: Zamba (8 GB) < Griffin (14 GB) < Jamba (28 GB)
3. **Quality**: Jamba â‰ˆ Zamba > Griffin (full attentionã®æœ‰ç„¡)

### 3.10.4 StripedHyena: Grouped Convolution + Attention

StripedHyenaã¯Together AIãŒé–‹ç™ºã—ãŸ**Hyena convolution**ã¨Attentionã‚’äº¤äº’ã«é…ç½®ã™ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã ã€‚Hyenaã¯é•·è·é›¢ç•³ã¿è¾¼ã¿ã‚’é™°çš„å¾®åˆ†æ–¹ç¨‹å¼ã§å®Ÿç¾ã—ã€FFTã«ã‚ˆã‚Š $O(N \log N)$ ã§è¨ˆç®—ã™ã‚‹ã€‚

**Hyenaæ¼”ç®—å­ã®æ•°å­¦çš„å®šç¾©**:

é•·ã• $N$ ã®ç³»åˆ— $u \in \mathbb{R}^N$ ã«å¯¾ã—ã¦ã€Hyenaæ¼”ç®—å­ã¯ãƒ•ã‚£ãƒ«ã‚¿ $h \in \mathbb{R}^N$ ã¨ã®ç•³ã¿è¾¼ã¿:

$$
(h * u)_t = \sum_{s=0}^{t} h_{t-s} \cdot u_s \quad \text{(å› æœç•³ã¿è¾¼ã¿)}
$$

ãŸã ã—å˜ç´”ãªç·šå½¢ç•³ã¿è¾¼ã¿ã§ã¯ãªãã€å„å±¤ã§ãƒ•ã‚£ãƒ«ã‚¿ $h$ ãŒ**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸé™°çš„å¾®åˆ†æ–¹ç¨‹å¼**ã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹:

$$
h_t = \text{MLP}_\theta(\text{PositionalEncoding}(t))
$$

ã“ã‚Œã«ã‚ˆã‚Šå›ºå®šãƒ•ã‚£ãƒ«ã‚¿ã§ã¯ãªã**å­¦ç¿’å¯èƒ½ãªé•·è·é›¢ãƒ•ã‚£ãƒ«ã‚¿**ãŒå®Ÿç¾ã™ã‚‹ã€‚

**StripedHyenaã®å±¤æ§‹æˆ**:

$l$ ç•ªç›®ã®å±¤:

$$
x_{l+1} = \begin{cases}
x_l + \text{Hyena}_l(x_l) & l \bmod 4 \neq 0 \quad \text{(75%ã®å±¤)} \\
x_l + \text{Attention}_l(x_l) & l \bmod 4 = 0 \quad \text{(25%ã®å±¤)}
\end{cases}
$$

**è¨ˆç®—é‡æ¯”è¼ƒ**:

| æ¼”ç®— | è¨ˆç®—é‡ | å‚™è€ƒ |
|:----|:------|:-----|
| Hyena convolution | $O(N \log N)$ | FFTã§é«˜é€ŸåŒ– |
| Full Attention | $O(N^2 d)$ | KV-Cacheã‚ã‚Š |
| StripedHyena (3:1 Hyena:Attn) | $O(N \log N + N^2 d / 4)$ | Attentionã‚’25%ã«å‰Šæ¸› |

**StripedHyenaã®åˆ©ç‚¹:**

- **Subquadratic**: $O(N \log N)$ complexity via FFT
- **Hardware-efficient**: Grouped convolution ã¯ GPUä¸¦åˆ—åŒ–ã«é©ã—ã¦ã„ã‚‹
- **Long-range**: Implicit convolution ã§é•·æœŸä¾å­˜æ€§ã‚’æ•æ‰
- **éŸ³å£°ç‰¹åŒ–**: éŸ³å£°ã®ã‚ˆã†ãªé•·ç³»åˆ— ($N > 10^5$) ã§ç‰¹ã«æœ‰åŠ¹ (Mambaã‚ˆã‚ŠFFTã®åˆ©ç‚¹ã‚’æ´»ã‹ã—ã‚„ã™ã„)

### 3.10.5 Hybrid SSM ã®ç†è«–çš„çµ±åˆ

æœ€è¿‘ã®ç ”ç©¶ã«ã‚ˆã‚Šã€SSMã€Linear Attentionã€Gated RNNãŒ**çµ±ä¸€çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ã§ç†è§£ã§ãã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚

**çµ±ä¸€å®šå¼åŒ–:**

ã™ã¹ã¦ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯æ¬¡ã®å½¢å¼ã§è¡¨ç¾ã§ãã‚‹:

$$
\begin{aligned}
h_t &= f_{\text{recurrent}}(h_{t-1}, x_t) \\
y_t &= g_{\text{attention}}(h_t, \{h_\tau\}_{\tau=1}^t)
\end{aligned}
$$

ã“ã“ã§:
- $f_{\text{recurrent}}$: SSM, Linear RNN, Gated RNN, Convolution ã®ã„ãšã‚Œã‹
- $g_{\text{attention}}$: Full Attention, Local Attention, None ã®ã„ãšã‚Œã‹

ã“ã®çµ±ä¸€å®šå¼åŒ–ã«ã‚ˆã‚Šã€å„ãƒ¢ãƒ‡ãƒ«ã¯ã€Œå†å¸°çš„çŠ¶æ…‹æ›´æ–°ã®é¸æŠã€ã¨ã€ŒAttentionã®å‚ç…§ç¯„å›²ã®é¸æŠã€ã¨ã„ã†2è»¸ã§åˆ†é¡ã§ãã‚‹ã€‚

**çµ±ä¸€è¡Œåˆ—è¡¨ç¾**:

ä»»æ„ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ› $\mathbf{Y} \in \mathbb{R}^{N \times d}$ ã¯æ¬¡ã®è¡Œåˆ—æ¼”ç®—ã¨ã—ã¦è¡¨ç¾ã§ãã‚‹:

$$
\mathbf{Y} = (M_\text{recurrent} + M_\text{attention}) \mathbf{X}
$$

ã“ã“ã§ $M_\text{recurrent} \in \mathbb{R}^{N \times N}$ ã¯å†å¸°çš„çŠ¶æ…‹é·ç§»ã‚’è¡¨ã™ä¸‹ä¸‰è§’Toeplitzè¡Œåˆ—ï¼ˆSSM/RNNã®å ´åˆï¼‰ã€$M_\text{attention} \in \mathbb{R}^{N \times N}$ ã¯Attentionè¡Œåˆ—ï¼ˆNoneã®å ´åˆã¯ã‚¼ãƒ­è¡Œåˆ—ï¼‰ã€‚

**Pure Mamba** ã®å ´åˆ: $M_\text{attention} = 0$ã€$M_\text{recurrent}$ ã¯Semi-Separableè¡Œåˆ—ï¼ˆç¬¬17å›ï¼‰ã€‚

**Pure Transformer** ã®å ´åˆ: $M_\text{recurrent} = 0$ã€$M_\text{attention} = \text{softmax}(QK^\top / \sqrt{d}) \cdot \mathbf{1}$ã€‚

**Hybrid** ã®å ´åˆ: ä¸¡æ–¹ãŒéã‚¼ãƒ­ â†’ **äºŒã¤ã®æƒ…å ±çµŒè·¯ã®é‡ç•³**ã€‚

ã“ã®åˆ†è§£ã®é‡è¦æ€§: $M_\text{recurrent}$ ã¯ä½ç½®ã«ä¾å­˜ã—ãŸå›ºå®šçš„ãªé‡ã¿ (ã€Œä½•ã‚¹ãƒ†ãƒƒãƒ—å‰ã‹ã€ã§æ±ºã¾ã‚‹) ã‚’ã€$M_\text{attention}$ ã¯å†…å®¹ã«ä¾å­˜ã—ãŸå‹•çš„ãªé‡ã¿ (ã€Œã©ã‚Œã ã‘é–¢é€£ã™ã‚‹ã‹ã€ã§æ±ºã¾ã‚‹) ã‚’è¡¨ã™ã€‚ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã¨ã¯ã“ã®äºŒã¤ã®æƒ…å ±çµŒè·¯ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã ã€‚

**å„ãƒ¢ãƒ‡ãƒ«ã®ä½ç½®ã¥ã‘:**

| Model | $f_{\text{recurrent}}$ | $g_{\text{attention}}$ | Mix Ratio |
|-------|------------------------|------------------------|-----------|
| Pure Mamba | Selective SSM | None | 0.0 |
| Jamba | Mamba SSM | Full Attention (sparse) | 0.125 |
| Zamba | Mamba SSM + MoE | Full Attention (sparse) | 0.125 |
| Griffin | Hawk RNN | Local Attention | 0.5 |
| StripedHyena | Grouped Conv | Full Attention | 0.25 |
| Pure Transformer | None | Full Attention | 1.0 |

### 3.10.6 ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å‹¾é…ãƒ•ãƒ­ãƒ¼è§£æ

ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŠ¹ç‡çš„ã«**è¨“ç·´**ã™ã‚‹ãŸã‚ã«ã€å‹¾é…ãƒ•ãƒ­ãƒ¼ã®æ•°å­¦çš„æ§‹é€ ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã ã€‚

**ç´”ç²‹RNNã®å‹¾é…æ¶ˆå¤±å•é¡Œ (å¾©ç¿’)**:

æå¤± $\mathcal{L}$ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $W$ ã¸ã®å‹¾é…ã¯é€£é–å¾‹ã§:

$$
\frac{\partial \mathcal{L}}{\partial W} = \sum_{t=1}^{N} \frac{\partial \mathcal{L}}{\partial h_t} \frac{\partial h_t}{\partial W}
$$

$\frac{\partial \mathcal{L}}{\partial h_t} = \frac{\partial \mathcal{L}}{\partial h_N} \prod_{k=t+1}^{N} \frac{\partial h_k}{\partial h_{k-1}}$

ã“ã®ç©é …ãŒå•é¡Œã€‚$\frac{\partial h_k}{\partial h_{k-1}} = A$ (å›ºå®šé‡ã¿) ãªã‚‰ $\|A^{N-t}\|$ ãŒæŒ‡æ•°çš„ã«0ã‹âˆã«ç™ºæ•£ã™ã‚‹ã€‚

**Hybrid ã§ã®å‹¾é…ãƒ•ãƒ­ãƒ¼**:

$L$ å±¤ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã§ã€å±¤ $l$ ãŒAttentionã‹SSMã‹ã®é¸æŠ $\tau_l \in \{\text{Attn, SSM}\}$:

$$
\frac{\partial \mathcal{L}}{\partial x_0} = \prod_{l=1}^{L} J_l, \quad J_l = \frac{\partial x_l}{\partial x_{l-1}}
$$

**Attentionå±¤ã®Jacobian** $J_l^\text{Attn}$:

$$
J_l^\text{Attn} = I + \frac{\partial \text{Attn}_l}{\partial x_{l-1}} + \frac{\partial \text{FFN}_l}{\partial x_{l-1}}
$$

Residual connectionã«ã‚ˆã‚Šå¯¾è§’æˆåˆ†ãŒ $1$ â€” ã“ã‚ŒãŒå‹¾é…æ¶ˆå¤±ã‚’é˜²ãã€‚Attentionã® $\frac{\partial \text{Attn}}{\partial x}$ ã¯å…¨ç³»åˆ—ã¸ã®ä¾å­˜ã‚’æŒã¤ãŸã‚ã€é ã„æ™‚åˆ»ã¸ã®å‹¾é…ã‚’**ç›´æ¥**ä¼æ¬ã§ãã‚‹ã€‚

**SSMå±¤ã®Jacobian** $J_l^\text{SSM}$:

$$
J_l^\text{SSM} = I + \frac{\partial \text{SSM}_l}{\partial x_{l-1}} + \frac{\partial \text{FFN}_l}{\partial x_{l-1}}
$$

$\frac{\partial \text{SSM}}{\partial x}$ ã¯Mambaã®Selectiveæœºæ§‹ã«ã‚ˆã‚Šå…¥åŠ›ä¾å­˜ã®æ¸›è¡°è¡Œåˆ— $\bar{A}_t$ ã‚’é€šã˜ã¦ä¼æ¬ã™ã‚‹ã€‚Selectiveæ©Ÿæ§‹ãŒ $\bar{A}_t \approx I$ ã‚’é¸ã¶ã“ã¨ã§ã€æƒ…å ±ä¿æŒã¨å‹¾é…ä¿å­˜ãŒåŒæ™‚ã«é”æˆã•ã‚Œã‚‹ã€‚

**Hybrid ã®åˆ©ç‚¹**:

$$
\prod_{l=1}^{L} J_l = J_L^\text{Attn} \cdot J_{L-1}^\text{SSM} \cdots J_2^\text{SSM} \cdot J_1^\text{Attn}
$$

Attentionå±¤ãŒã€Œå‹¾é…ãƒã‚¤ã‚¦ã‚§ã‚¤ã€ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã€‚è·é›¢ $k$ é›¢ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³é–“ã®å‹¾é…ã¯ã€é€”ä¸­ã«Attentionå±¤ãŒã‚ã‚Œã°ãã®å±¤ã‚’é€šã˜ã¦ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆä¼æ¬ã§ãã‚‹ã€‚$r = 1/8$ ã§8å±¤ã”ã¨ã«Attention â†’ æœ€å¤§7å±¤ã®SSMã‚’è·¨ã„ã å¾Œã«Attentionã§å‹¾é…ãŒã€Œãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã€ã•ã‚Œã‚‹ã€‚

**æ•°å€¤çš„å®‰å®šæ€§ã®æ¡ä»¶**:

ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´å®‰å®šã§ã‚ã‚‹ãŸã‚ã®ååˆ†æ¡ä»¶ [^4]:

$$
\|J_l^\text{SSM}\|_\text{spec} \leq 1 + \epsilon, \quad \|J_l^\text{Attn}\|_\text{spec} \leq 1 + \epsilon \quad \text{for all } l
$$

ã“ã‚Œã¯LayerNormã¨Residual connectionãŒæ­£ã—ãå®Ÿè£…ã•ã‚Œã¦ã„ã‚Œã°è‡ªç„¶ã«æº€ãŸã•ã‚Œã‚‹ã€‚Selective SSMã§ã¯ $\bar{A}_t$ ã®å›ºæœ‰å€¤ãŒå˜ä½å††å†… (HiPPOåˆæœŸåŒ–) ã§ã‚ã‚‹ã“ã¨ãŒé‡è¦ã€‚

**å®Ÿè£…ä¸Šã®æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯**:

**1. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–: Gradient Checkpointing**

$L$ å±¤ã®ãƒ¢ãƒ‡ãƒ«ã§naiveã«é€†ä¼æ¬ã™ã‚‹ã¨å…¨ä¸­é–“æ´»æ€§åŒ– $\{x_l\}_{l=1}^L$ ã‚’ä¿æŒã™ã‚‹ãŸã‚ $O(L \cdot N \cdot d)$ ã®ãƒ¡ãƒ¢ãƒªãŒå¿…è¦ã€‚Gradient Checkpointingã¯ $O(\sqrt{L})$ å±¤ã”ã¨ã«æ´»æ€§åŒ–ã‚’ä¿å­˜ã—ã€æ®‹ã‚Šã‚’é †ä¼æ¬ã§å†è¨ˆç®—ã™ã‚‹:

$$
\text{Memory: } O(L N d) \to O(\sqrt{L} \cdot N d), \quad \text{Compute: } O(L) \to O(L + \sqrt{L} \cdot L / \sqrt{L}) = O(L)
$$

ã¤ã¾ã‚Šè¨ˆç®—é‡ã¯å¤‰ã‚ã‚‰ãšãƒ¡ãƒ¢ãƒªã‚’ $O(\sqrt{L})$ å€å‰Šæ¸›ã§ãã‚‹ã€‚Hybrid ãƒ¢ãƒ‡ãƒ«ã§ã¯ç‰¹ã«Attentionå±¤ã® $O(N^2)$ æ´»æ€§åŒ–ã‚’Checkpointã™ã‚‹ã“ã¨ãŒåŠ¹æœçš„ã ã€‚

**2. ä¸¦åˆ—åŒ–: Pipeline Parallelism ã®æ•°å­¦**

$L$ å±¤ã‚’ $P$ ã‚¹ãƒ†ãƒ¼ã‚¸ã«åˆ†å‰² ($L/P$ å±¤ãšã¤) ã—ã€å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’åˆ¥GPUã«é…ç½®:

$$
\text{Throughput} = \frac{B \cdot N}{T_\text{forward} + T_\text{backward}} \to \frac{B \cdot N}{(T_\text{forward} + T_\text{backward}) / P + T_\text{bubble}}
$$

ãŸã ã— $T_\text{bubble} = (P-1) \cdot T_\text{microbatch}$ ã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ–ãƒ«ã€‚$P$ ãŒå¤§ãã™ãã‚‹ã¨ãƒãƒ–ãƒ«ãŒæ”¯é…çš„ã«ãªã‚‹ã€‚Hybrid ãƒ¢ãƒ‡ãƒ«ã§ã¯å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®è¨ˆç®—æ™‚é–“ã‚’å‡ç­‰ã«ã™ã‚‹ãŸã‚ã€Attentionå±¤ã®å¤šã„ã‚¹ãƒ†ãƒ¼ã‚¸ ($O(N^2)$) ã¨SSMå±¤ã®ã¿ã®ã‚¹ãƒ†ãƒ¼ã‚¸ ($O(N)$) ã§ã‚¹ãƒ†ãƒ¼ã‚¸ã‚µã‚¤ã‚ºã‚’èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

**3. æ•°å€¤å®‰å®šæ€§: Mixed Precision ã®è½ã¨ã—ç©´**

FP16/BF16ã§ã®è¨“ç·´ã§å•é¡Œã«ãªã‚‹ã®ã¯ã€SSMã®çŠ¶æ…‹ $h_t$ ãŒç´¯ç©èª¤å·®ã‚’è“„ç©ã™ã‚‹ã“ã¨ã :

$$
h_t = \bar{A}_t h_{t-1} + \bar{B}_t x_t
$$

$N$ ã‚¹ãƒ†ãƒƒãƒ—å¾Œã®èª¤å·®: $\epsilon_N \leq \|\bar{A}\|^N \epsilon_0 + \sum_{k=0}^{N-1} \|\bar{A}\|^k \delta_k$

ã“ã“ã§ $\delta_k$ ã¯FP16ã®ä¸¸ã‚èª¤å·® ($\approx 10^{-3}$)ã€‚$\|\bar{A}\| < 1$ (å®‰å®šç³») ã§ã‚‚ $N = 4096$ ãªã‚‰èª¤å·®ãŒæ•°%è“„ç©ã™ã‚‹ã€‚**è§£æ±ºç­–**: çŠ¶æ…‹ $h_t$ ã¯FP32ã§ä¿æŒã—ã€å…¥å‡ºåŠ›ã®ã¿FP16ã‚’ä½¿ã†ã€Œmixed state precisionã€ãŒæ¨™æº–çš„ãªå¯¾å‡¦æ³•ã ã€‚

### 3.10.7 Long-Context ã«ãŠã‘ã‚‹æ€§èƒ½ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç†è«–

Hybrid ãƒ¢ãƒ‡ãƒ«ã®é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ€§èƒ½ã‚’**ç†è«–çš„**ã«åˆ†æã™ã‚‹ã€‚

**Needles-in-a-Haystack (NIAH) ã‚¿ã‚¹ã‚¯ã®ç†è«–è§£æ**:

é•·ã• $N$ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä¸­ã«1ã¤ã®ã€Œé’ˆã€(é‡è¦ãªæƒ…å ±) ãŒåŸ‹ã‚è¾¼ã¾ã‚Œã€ãã‚Œã‚’å‚ç…§ã—ã¦å›ç­”ã™ã‚‹ã‚¿ã‚¹ã‚¯ã€‚

**Attention ã®å ´åˆ**: é’ˆã®ä½ç½® $p$ ã«å¯¾ã—ã¦ã€query $q$ ã¨ needle $k_p$ ã®å†…ç© $q \cdot k_p$ ãŒæœ€å¤§ã«ãªã‚Œã°æ­£è§£ã§ãã‚‹ã€‚ç³»åˆ—é•· $N$ ã«ä¾å­˜ã›ãš**åŸç†çš„ã«å®Œå…¨è§£æ±º**å¯èƒ½:

$$
\text{Attn score}(q, k_p) = \frac{q \cdot k_p}{\sqrt{d}} - \text{LSE}(q \cdot K / \sqrt{d})
$$

ãŸã ã—ã€ä½ç½® $p$ ãŒ $N$ ã«è¿‘ã„å ´åˆã¯æ³¨æ„è¡Œåˆ—ãŒå¤§ãã ($O(N^2)$ ãƒ¡ãƒ¢ãƒª)ã€å®Ÿè£…ä¸Šã¯FlashAttentionãŒå¿…é ˆã€‚

**SSM ã®å ´åˆ**: é’ˆ $k_p$ ã¯çŠ¶æ…‹ $h_p$ ã«æ›¸ãè¾¼ã¾ã‚Œã€æ™‚åˆ» $t > p$ ã«æŒ‡æ•°æ¸›è¡° $\bar{A}^{t-p}$ ã§è–„ã¾ã‚‹:

$$
h_t = \bar{A}^{t-p} h_p + \cdots, \quad \|h_t - h_p\|_2 \approx \|\bar{A}\|^{t-p} \|h_p\|
$$

ä½ç½® $p$ ãŒé ã„ã»ã©é‡ã®æƒ…å ±ã¯è–„ã¾ã‚‹ â†’ **NIAH ã§ä½ç½®ä¾å­˜ã®æ€§èƒ½åŠ£åŒ–**ãŒèµ·ãã‚‹ã€‚Mambaã®å®Ÿè¨¼å®Ÿé¨“ [^4] ã§ã‚‚ã“ã‚ŒãŒç¢ºèªã•ã‚Œã¦ã„ã‚‹ã€‚

**Hybrid ã®å„ªä½æ€§**: Attentionå±¤ãŒ8å±¤ã”ã¨ã«ã€Œè¨˜æ†¶ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã€ã™ã‚‹ã€‚é‡ãŒæœ€å¾Œã®Attentionå±¤ã®å‰ã«ã‚ã‚Œã°ã€AttentionãŒç›´æ¥å‚ç…§ã§ãã‚‹ã€‚é‡ãŒAttentionå±¤ã‚ˆã‚Šå‰ã®å ´åˆã®ã¿SSMã®è¨˜æ†¶ã«é ¼ã‚‹å¿…è¦ãŒã‚ã‚‹:

$$
\text{æœ‰åŠ¹ãªSSMã®è¨˜æ†¶é•·} \approx \frac{N}{r} \quad (r = \text{Attentionæ¯”ç‡})
$$

$r = 1/8$ ãªã‚‰ SSMã¯ $N/8$ é•·ã•ã®è¨˜æ†¶ã‚’ä¿æŒã™ã‚Œã°ååˆ† â€” ç´”ç²‹SSMãŒ $N$ å…¨ä½“ã‚’è¨˜æ†¶ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã®ã¨å¤§ããç•°ãªã‚‹ã€‚

**æœŸå¾…ã•ã‚Œã‚‹å®Ÿé¨“çµæœ:**

- **Jamba/Zamba**: é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã‚‚é«˜ç²¾åº¦ (sparse attentionã®NIAHè§£æ±ºåŠ¹æœ)
- **Griffin**: ä¸­ç¨‹åº¦ã®ç²¾åº¦ (local attentionã¯ $w$ ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å¤–ã®é‡ã‚’å‚ç…§ã§ããªã„)
- **Transformer**: çŸ­ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã¯æœ€é«˜ç²¾åº¦ã€é•·ã„ã¨è¨ˆç®—ä¸å¯èƒ½

**Boss Battleå®Œäº†** â€” Jamba-style Hybrid Blockã®å®Œå…¨å®Ÿè£…ãƒ»æ¤œè¨¼ã‚’é”æˆã—ãŸã€‚

> **Note:** **é€²æ—: 50% å®Œäº†** ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ•°å­¦çš„å®šå¼åŒ–ã€è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡ã€è¨ˆç®—é‡è§£æã€Boss Battleã‚’å®Œäº†ã—ãŸã€‚æ¬¡ã¯Zone 4ã®å®Ÿè£…ã‚¾ãƒ¼ãƒ³ â€” Rust/Rustã§å®Ÿç”¨çš„ãªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

---

## 3.11 "æœ€å¼·ã¯å­˜åœ¨ã—ãªã„" â€” No Free Lunchå®šç†ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã¸ã®é©ç”¨

### 3.11.1 No Free Lunch å®šç†ã®å³å¯†ãªé™³è¿°

**å®šç† (Wolpert & Macready, 1997)**: ä»»æ„ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  $a_1, a_2$ ã«å¯¾ã—ã¦ã€å…¨ã¦ã®æœ€é©åŒ–å•é¡Œ $f$ ã‚’ä¸€æ§˜åˆ†å¸ƒã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸã¨ãã€æœŸå¾…æ€§èƒ½ã¯ç­‰ã—ã„:

$$
\sum_f P(d_m^y \mid f, m, a_1) = \sum_f P(d_m^y \mid f, m, a_2)
$$

ã“ã“ã§ $d_m^y$ ã¯ $m$ å›ã®è©•ä¾¡å¾Œã®å‡ºåŠ›ç³»åˆ—ã€‚

ã“ã®å®šç†ã¯ã€Œã‚¿ã‚¹ã‚¯ã«ç„¡é–¢ä¿‚ãªäº‹å‰çŸ¥è­˜ãªã—ã«ã¯ã€ä¸‡èƒ½ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å­˜åœ¨ã—ãªã„ã€ã¨ã„ã†æ ¹æœ¬çš„äº‹å®Ÿã‚’è¿°ã¹ã‚‹ã€‚

### 3.11.2 ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«ã¸ã®é©ç”¨

LLM/ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ–‡è„ˆã§No Free Lunchã‚’è§£é‡ˆã™ã‚‹ã€‚

ã€Œã‚¿ã‚¹ã‚¯ã€= ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ (è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° / æ™‚ç³»åˆ—äºˆæ¸¬ / ç”»åƒç”Ÿæˆ / DNAé…åˆ—è§£æ ãªã©)

**å®šå¼åŒ–**: ã‚¿ã‚¹ã‚¯åˆ†å¸ƒ $\mathcal{T}$ ä¸Šã§ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ $\mathcal{A}$ ã®æœŸå¾…æå¤±:

$$
\mathbb{E}_{T \sim \mathcal{T}}[\mathcal{L}(\mathcal{A}, T)] = \text{const} \quad \text{(ä»»æ„ã®} \mathcal{A} \text{ã«å¯¾ã—ã¦)}
$$

ãŸã ã— $\mathcal{T}$ ãŒ**ä¸€æ§˜åˆ†å¸ƒ**ã®å ´åˆã®ã¿ã€‚ç¾å®Ÿã®ã‚¿ã‚¹ã‚¯åˆ†å¸ƒã¯ä¸€æ§˜ã§ã¯ãªã„ãŸã‚ã€ç‰¹å®šã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒå„ªä½ã«ãªã‚‹ â€” ã ãŒãã®å„ªä½æ€§ã¯**ã‚¿ã‚¹ã‚¯åˆ†å¸ƒã®é¸æŠ**ã«ä¾å­˜ã™ã‚‹ã€‚

**å®Ÿè·µçš„å«æ„**:

AttentionãŒå„ªã‚Œã‚‹ã‚¿ã‚¹ã‚¯ç¾¤ $\mathcal{T}_\text{Attn}$:
- Associative recall: "Aâ†’B, Câ†’D, ... Eâ†’?" ($O(N)$å€‹ã®KV pairã‚’å…¨å‚ç…§)
- In-context learning: æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å³åº§ã«è¨˜æ†¶
- Few-shot reasoning: ä¾‹ç¤ºã‹ã‚‰è¦å‰‡ã‚’å­¦ç¿’

SSMãŒå„ªã‚Œã‚‹ã‚¿ã‚¹ã‚¯ç¾¤ $\mathcal{T}_\text{SSM}$:
- Long-range streaming: é€æ¬¡å…¥åŠ›ã®åŠ¹ç‡çš„å‡¦ç† ($O(1)$ãƒ¡ãƒ¢ãƒª)
- Periodic pattern: æ™‚ç³»åˆ—ã®å‘¨æœŸæˆåˆ†æŠ½å‡º
- Biological sequence: DNA/ã‚¿ãƒ³ãƒ‘ã‚¯è³ªã®é•·è·é›¢ç›¸äº’ä½œç”¨

**Hybrid ã® position**: $\mathcal{T} = \mathcal{T}_\text{Attn} \cup \mathcal{T}_\text{SSM}$ ã«å¯¾ã—ã¦ã€

$$
\mathbb{E}_{T \sim \mathcal{T}}[\mathcal{L}(\text{Hybrid}, T)] < \mathbb{E}_{T \sim \mathcal{T}}[\mathcal{L}(\text{Attn only}, T)] = \mathbb{E}_{T \sim \mathcal{T}}[\mathcal{L}(\text{SSM only}, T)]
$$

ç¾å®Ÿã®LLMã‚¿ã‚¹ã‚¯åˆ†å¸ƒã¯ $\mathcal{T}_\text{Attn}$ ã¨ $\mathcal{T}_\text{SSM}$ ã®ä¸¡æ–¹ã‚’å«ã‚€ â†’ HybridãŒæœŸå¾…å€¤ã§å„ªä½ã€‚

### 3.11.3 ã‚¿ã‚¹ã‚¯é©æ€§ã®å®šé‡åŒ–

**æŒ‡æ¨™ 1: Effective Context Length**

ã‚¿ã‚¹ã‚¯ $T$ ã‚’è§£ããŸã‚ã«å¿…è¦ãªã€Œæœ‰åŠ¹ãªæ–‡è„ˆé•·ã€$k_T$:

$$
k_T = \arg\min_k \left[ \mathcal{L}(\mathcal{A}[N=k], T) \leq \mathcal{L}(\mathcal{A}[N=\infty], T) + \epsilon \right]
$$

$k_T$ ãŒå°ã•ã„ã‚¿ã‚¹ã‚¯ (å±€æ‰€çš„æ–‡è„ˆã§è§£ã‘ã‚‹) â†’ SSMæœ‰åˆ©  
$k_T$ ãŒå¤§ãã„ã‚¿ã‚¹ã‚¯ (é ã„æ–‡è„ˆãŒå¿…è¦) â†’ Attentionæœ‰åˆ©

**å®Ÿæ¸¬å€¤** (Waleffe et al. 2024 [^4]):

| Task | Effective $k_T$ | Best Arch |
|:-----|:---------------|:----------|
| Next-token prediction (LM) | $\leq 1024$ | SSM â‰ˆ Attn |
| Associative Recall (Phonebook) | $= N$ (å…¨ç³»åˆ—) | Attn >> SSM |
| Summarization | $1024 \leq k_T \leq N$ | Hybrid |
| Code generation | $\leq 2048$ | SSM â‰ˆ Attn |
| Multi-hop reasoning | $= N$ | Attn > SSM |

**æŒ‡æ¨™ 2: Content-based vs Position-based**

ã‚¿ã‚¹ã‚¯ãŒã€Œä½•ã®æƒ…å ±ã‹ã€(content) ã¨ã€Œã©ã“ã«ã‚ã‚‹ã‹ã€(position) ã®ã©ã¡ã‚‰ã«ä¾å­˜ã™ã‚‹ã‹:

$$
r_T = \frac{\text{Contentä¾å­˜åº¦}}{\text{Positionä¾å­˜åº¦}} = \frac{I(y; \text{content}(x))}{I(y; \text{position}(x))}
$$

$r_T \gg 1$: Content-dominant â†’ Attentionæœ‰åˆ© (å…¨ç³»åˆ—ã‹ã‚‰é–¢é€£ã™ã‚‹å†…å®¹ã‚’æ¤œç´¢)  
$r_T \ll 1$: Position-dominant â†’ SSMæœ‰åˆ© (ä½ç½®çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å†å¸°ã§æ‰ãˆã‚‹)  
$r_T \approx 1$: ä¸¡æ–¹å¿…è¦ â†’ Hybridæœ€é©

### 3.11.4 Paretoæœ€é©ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã¨ã—ã¦ã®Hybrid

**2ç›®çš„æœ€é©åŒ–ã®å®šå¼åŒ–**:

$$
\min_\mathcal{A} \bigl( \mathcal{L}(\mathcal{A}, \mathcal{T}),\; \text{Compute}(\mathcal{A}, N) \bigr)
$$

ã€Œæ€§èƒ½ã€ã¨ã€Œè¨ˆç®—ã‚³ã‚¹ãƒˆã€ã®2è»¸ã§Paretoæœ€é©ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’è€ƒãˆã‚‹ã€‚

ç´”ç²‹Transformer: é«˜æ€§èƒ½ãƒ»é«˜ã‚³ã‚¹ãƒˆ (frontier ã®å³ä¸Š)  
ç´”ç²‹SSM: ä½ã‚³ã‚¹ãƒˆãƒ»ä½æ€§èƒ½ (frontier ã®å·¦ä¸‹)  
**Hybrid**: Paretoæœ€é©æ›²ç·šä¸Šã®ä¸­é–“ (frontier ã®ä¸Šã«ä½ç½®)

```mermaid
graph LR
    A["Pure Transformer<br/>é«˜æ€§èƒ½ãƒ»O(NÂ²)"] -->|Pareto frontier| B["Hybrid<br/>ä¸­æ€§èƒ½ãƒ»O(N)~"]
    B -->|Pareto frontier| C["Pure SSM<br/>ä½ã‚³ã‚¹ãƒˆãƒ»O(1)ãƒ¡ãƒ¢ãƒª"]
    D["âŒ Dominated<br/>ä½æ€§èƒ½ãƒ»é«˜ã‚³ã‚¹ãƒˆ"] -.->|æ”¯é…ã•ã‚Œã‚‹| A
    style A fill:#e3f2fd
    style B fill:#c8e6c9
    style C fill:#fff3e0
    style D fill:#ffcdd2
```

**å®Ÿè¨¼**: Waleffe et al. (2024) [^4] ã¯8Bã‚¹ã‚±ãƒ¼ãƒ«ã§ã€Mamba-2-Hybrid (7-8% Attention) ãŒ:
- LMã‚¿ã‚¹ã‚¯ã§ç´”ç²‹Transformer**ã¨åŒç­‰**ã®æ€§èƒ½
- æ¨è«–é€Ÿåº¦ãŒç´”ç²‹Transformerã®**2-3å€**
- KV-Cacheãƒ¡ãƒ¢ãƒªãŒç´”ç²‹Transformerã®**12%**

Paretoæ”¹å–„ â€” åŒã˜æ€§èƒ½ã§ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã€ã¾ãŸã¯ã‚³ã‚¹ãƒˆã‚’å›ºå®šã—ã¦æ€§èƒ½å‘ä¸Šã€‚ã“ã‚ŒãŒ2024å¹´ã®LLMç•Œã§HybridãŒæ³¨ç›®ã•ã‚ŒãŸç†ç”±ã ã€‚

### 3.11.5 è¨­è¨ˆå“²å­¦: ã€Œæœ€å¼·ã€ã‚’æ¢ã•ãªã„

No Free LunchãŒç¤ºã™æœ¬è³ª: **æ±ç”¨çš„ãªæœ€å¼·ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯å­˜åœ¨ã—ãªã„**ã€‚ã‚ã‚‹ã®ã¯ã€Œã“ã®å•é¡Œè¨­å®šã«æœ€é©ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ã ã‘ã ã€‚

å®Ÿè·µçš„ãªè¨­è¨ˆæŒ‡é‡:

1. **ã‚¿ã‚¹ã‚¯åˆ†æå…ˆè¡Œ**: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’é¸ã¶å‰ã« $k_T$ ã¨ $r_T$ ã‚’æ¸¬å®šã›ã‚ˆ
2. **ã‚³ã‚¹ãƒˆåˆ¶ç´„æ˜ç¢ºåŒ–**: æ¨è«–ã‚³ã‚¹ãƒˆã€ãƒ¡ãƒ¢ãƒªã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®åˆ¶ç´„ã‚’å…ˆã«æ±ºã‚ã‚ˆ
3. **Hybridæ¯”ç‡ã‚’é€£ç¶šå¤‰æ•°ã¨ã—ã¦æ‰±ã†**: $r \in [0,1]$ ã‚’å›ºå®šå€¤ã§ã¯ãªãã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§æœ€é©åŒ–ã›ã‚ˆ
4. **ã‚¿ã‚¹ã‚¯å¤šæ§˜æ€§ã«æ¯”ä¾‹ã—ã¦Attentionã‚’å¢—ã‚„ã™**: å¤šç›®çš„ãƒ»æ±ç”¨ãƒ¢ãƒ‡ãƒ«ã»ã© $r$ ã‚’å¤§ããã™ã‚‹
5. **ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãªã‚‰ç´”ç²‹SSMã‚‚æœ‰åŠ¹**: ç”Ÿç‰©æƒ…å ±å­¦ã€ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ãªã©é€æ¬¡ãƒ»é•·ç³»åˆ—ç‰¹åŒ–ã‚¿ã‚¹ã‚¯ã§ã¯ $r=0$ ãŒæœ€é©ãªã“ã¨ã‚‚ã‚ã‚‹

Course IIã‚’é€šã˜ã¦å­¦ã‚“ã§ããŸå…¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ â€” ELBOã€OTã€GANã€è‡ªå·±å›å¸°ã€Attentionã€SSM â€” ãã‚Œãã‚ŒãŒç‰¹å®šã®å•é¡Œã«å¯¾ã—ã¦æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚Hybridè¨­è¨ˆã¯ãã®ç·åˆã ã€‚ã€Œæœ€å¼·ã€ã‚’æ±‚ã‚ã‚‹ã‚ˆã‚Šã€ã€Œä½•ãŒå¾—æ„ã§ä½•ãŒè‹¦æ‰‹ã‹ã€ã‚’ç†è§£ã—çµ„ã¿åˆã‚ã›ã‚‹çŸ¥æ€§ã“ããŒã€2020å¹´ä»£ã®æ·±å±¤å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã«æ±‚ã‚ã‚‰ã‚Œã‚‹ã€‚

> **Note:** **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
>
> 1. No Free Lunchå®šç†ãŒã€Œã‚¿ã‚¹ã‚¯åˆ†å¸ƒãŒä¸€æ§˜ã®å ´åˆã®ã¿æˆç«‹ã™ã‚‹ã€ã¨ã„ã†æ¡ä»¶ã¯ã€LLMã®å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§ã©ã†è§£é‡ˆã™ã¹ãã‹ï¼Ÿ
> 2. Hybridæ¯”ç‡ $r = 1/8$ ãŒã€ŒParetoæœ€é©ã«è¿‘ã„ã€ã¨ã„ã†ä¸»å¼µã¯ã€ã©ã®ã‚¿ã‚¹ã‚¯åˆ†å¸ƒã‚’ä»®å®šã—ã¦ã„ã‚‹ã‹ï¼Ÿã“ã®ä»®å®šãŒå´©ã‚Œã‚‹å…·ä½“çš„ãªå¿œç”¨ä¾‹ã‚’æŒ™ã’ã‚ˆã€‚

> Progress: 50%

---

## 3.12 MoE Ã— Hybrid SSM: Jambaè¨­è¨ˆã®æ•°å­¦çš„åŸç†

### 3.12.1 Mixture-of-Experts ã®å®šå¼åŒ–

Jamba [^1] ãŒãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰SSMã«Mixture-of-Experts (MoE) ã‚’çµ±åˆã—ãŸç†ç”±ã‚’æ•°å­¦ã‹ã‚‰ç†è§£ã™ã‚‹ã€‚

**MoEã®åŸºæœ¬æ§‹é€ **:

$E$ å€‹ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ $\{f_1, \ldots, f_E\}$ ã¨ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸Šä½ $K$ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã‚²ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $G$:

$$
\text{MoE}(x) = \sum_{i \in \text{Top-}K(G(x))} G_i(x) \cdot f_i(x)
$$

ã“ã“ã§ $G(x) = \text{softmax}(x W_g)$ã€$\text{Top-}K$ ã¯ä¸Šä½ $K$ å€‹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¸æŠã™ã‚‹ã€‚$K=2$, $E=8$ ãŒå…¸å‹çš„è¨­å®šã€‚

**ç–æ€§ã«ã‚ˆã‚‹è¨ˆç®—å‰Šæ¸›**:

å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒ $K/E$ ã®å‰²åˆã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ã¿è¨ˆç®—ã™ã‚‹ãŸã‚:

$$
\text{Active FLOPs} = \frac{K}{E} \times \text{Dense FLOPs}
$$

Jamba ($K=2$, $E=16$): Active FLOPs = Dense FLOPsã® $1/8$ã€‚åŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®Denseãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦æ¨è«–ãŒå¤§å¹…ã«é«˜é€Ÿã«ãªã‚‹ã€‚

### 3.12.2 Jamba ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹æˆ

Jambaã¯Lå±¤ã®ãƒ¢ãƒ‡ãƒ«ã§ã€ä»¥ä¸‹ã®3ç¨®ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’é…ç½®ã™ã‚‹:

$$
\text{Layer}_l = \begin{cases}
\text{Mamba Block} & l \notin \mathcal{L}_\text{attn},\; l \notin \mathcal{L}_\text{moe} \\
\text{Mamba Block + MoE FFN} & l \notin \mathcal{L}_\text{attn},\; l \in \mathcal{L}_\text{moe} \\
\text{Attention Block + MoE FFN} & l \in \mathcal{L}_\text{attn}
\end{cases}
$$

$\mathcal{L}_\text{attn}$: 8å±¤ã”ã¨ã«Attention ($r = 1/8$)  
$\mathcal{L}_\text{moe}$: 2å±¤ã”ã¨ã«MoE FFN ($K=2$, $E=16$)

**å…¨ä½“ã®è¨ˆç®—é‡**:

$$
\text{Compute}_\text{Jamba} = \underbrace{\frac{L}{8} \cdot O(N^2 d)}_{\text{Attention}} + \underbrace{\frac{7L}{8} \cdot O(N d)}_{\text{Mamba}} + \underbrace{\frac{L}{2} \cdot \frac{2}{16} \cdot O(N d^2)}_{\text{MoE FFN}} + \underbrace{\frac{L}{2} \cdot O(N d^2)}_{\text{Dense FFN}}
$$

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡**:

MoEã¯ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å¤šã„ãŒã€æ¨è«–æ™‚ã®FLOPsã¯å°‘ãªã„ã€ã¨ã„ã†ç‰¹æ€§ã‚’æŒã¤ã€‚Jamba 52B (total params) ã®active paramsã¯12B â€” Llama-13Bã¨åŒç¨‹åº¦ã®æ¨è«–ã‚³ã‚¹ãƒˆã§ã€ã¯ã‚‹ã‹ã«å¤§ããªãƒ¢ãƒ‡ãƒ«å®¹é‡ã‚’å®Ÿç¾ã™ã‚‹ã€‚

$$
\text{Parameter Efficiency} = \frac{\text{Total Params}}{\text{Active Params}} = \frac{52\text{B}}{12\text{B}} \approx 4.3\times
$$

åŒç­‰ã®æ¨è«–ã‚³ã‚¹ãƒˆã§4.3å€ã®ãƒ¢ãƒ‡ãƒ«å®¹é‡ â†’ çŸ¥è­˜ã®è“„ç©é‡ãŒå¢—ãˆã€perplexityãŒæ”¹å–„ã™ã‚‹ã€‚

### 3.12.3 Load Balancing: MoEã®ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°å•é¡Œ

MoEã®å®Ÿè£…ã§æœ€å¤§ã®è½ã¨ã—ç©´ãŒ**Routing Collapse** â€” å…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒåŒã˜ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«é›†ä¸­ã™ã‚‹ç¾è±¡ã€‚

$$
P(\text{collapse}) \propto \exp\!\left(\frac{\max_i G_i(x) - \bar{G}(x)}{\tau}\right)
$$

è§£æ±ºç­–: **Auxiliary Load Balancing Loss**

$$
\mathcal{L}_\text{aux} = \alpha \cdot E \cdot \sum_{i=1}^{E} f_i \cdot P_i
$$

ã“ã“ã§ $f_i$ ã¯ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ $i$ ãŒå‡¦ç†ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã®å‰²åˆã€$P_i = \frac{1}{N} \sum_t G_i(x_t)$ ã¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç¢ºç‡ã®å¹³å‡ã€‚$f_i \approx P_i \approx 1/E$ ã«ãªã‚‹ã‚ˆã†ä¿ƒã™ã€‚

$\alpha = 10^{-2}$ ãŒJambaè«–æ–‡ã®è¨­å®šã€‚ã“ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ã«ã‚ˆã‚Šå…¨ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«å‡ç­‰ã«è² è·ãŒåˆ†æ•£ã—ã€ãƒ¢ãƒ‡ãƒ«å®¹é‡ã‚’æœ€å¤§é™æ´»ç”¨ã§ãã‚‹ã€‚

---

## 3.13 Hybrid Architectures ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡

### 3.13.1 Chinchillaå‰‡ã®Hybridã¸ã®æ‹¡å¼µ

OpenAIã®Chinchillaè«–æ–‡ (Hoffmann et al. 2022) ãŒç¤ºã—ãŸã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡:

$$
\mathcal{L}(N, D) = \frac{A}{N^\alpha} + \frac{B}{D^\beta} + \mathcal{L}_\infty
$$

ã“ã“ã§ $N$: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€$D$: è¨“ç·´ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€‚$\alpha \approx 0.34$, $\beta \approx 0.28$ (Transformerã®å®Ÿæ¸¬å€¤)ã€‚

**Hybridãƒ¢ãƒ‡ãƒ«ã¸ã®Chinchillaå‰‡ã®é©ç”¨**:

Hybridãƒ¢ãƒ‡ãƒ«ã§ã¯ã€åŠ¹æœçš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒç•°ãªã‚‹è§£é‡ˆã‚’æŒã¤ã€‚Mambaå±¤ã¯Attentionå±¤ã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§ã‚‚ã€**æœ‰åŠ¹ãªè¡¨ç¾åŠ›**ãŒç•°ãªã‚‹ãŸã‚:

$$
\mathcal{L}_\text{Hybrid}(N, D, r) = \frac{A(r)}{N^{\alpha(r)}} + \frac{B}{D^\beta} + \mathcal{L}_\infty
$$

ã“ã“ã§ $r$ ã¯Attentionæ¯”ç‡ã€‚$A(r)$ ã¨ $\alpha(r)$ ã¯ $r$ ã«ä¾å­˜ã™ã‚‹ä¿‚æ•°ã€‚

**å®Ÿæ¸¬å‚¾å‘ (Waleffe et al. 2024 [^4])**:

- $r = 0$ (Pure SSM): $\alpha \approx 0.28$ (Transformerã‚ˆã‚Šå°ã•ã„ = ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒã‚„ã‚„åŠ£ã‚‹)
- $r = 0.08$ (Hybrid 1/12): $\alpha \approx 0.33$ (Transformerã«è¿‘ã„)
- $r = 1$ (Pure Transformer): $\alpha \approx 0.34$

**è§£é‡ˆ**: Hybridã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æŒ‡æ•°ã¯Attentionæ¯”ç‡ $r$ ã¨ã¨ã‚‚ã«å˜èª¿å¢—åŠ ã™ã‚‹ã€‚$r = 0.08$ ã§ã™ã§ã«Transformerã®95%ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ã‚’é”æˆ â€” ã“ã‚ŒãŒHybridãŒã€Œå°ã•ãªAttentionæ¯”ç‡ã§ååˆ†ã€ã¨ã„ã†ç†è«–çš„æ ¹æ‹ ã ã€‚

### 3.13.2 Context Length Scaling

ç³»åˆ—é•· $N$ ã«å¯¾ã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚‚é‡è¦ãªè¨­è¨ˆæŒ‡æ¨™ã ã€‚

**PPL (Perplexity) ã®ç³»åˆ—é•·ä¾å­˜æ€§**:

$$
\text{PPL}(N) = \exp\!\left(-\frac{1}{N} \sum_{t=1}^{N} \log P(x_t | x_{1:t-1})\right)
$$

é•·ã„ç³»åˆ—ã»ã© $P(x_t | x_{1:t-1})$ ã®äºˆæ¸¬ãŒé›£ã—ããªã‚‹ãŸã‚ã€$\text{PPL}(N)$ ã¯ç†æƒ³çš„ã«ã¯å˜èª¿æ¸›å°‘ (ã‚ˆã‚Šå¤šãã®æ–‡è„ˆ â†’ ã‚ˆã‚Šä½ã„perplexity)ã€‚

**Hybridã®context scaling**:

Pure Mambaã¯ $N > 10K$ ã‚ãŸã‚Šã‹ã‚‰ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åˆ©ç”¨åŠ¹ç‡ã€ãŒä½ä¸‹ã™ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ â€” é ã„è¨˜æ†¶ãŒæŒ‡æ•°æ¸›è¡°ã§è–„ã¾ã‚‹ãŸã‚ã€‚Hybrid (Jamba) ã§ã¯Attentionå±¤ãŒå®šæœŸçš„ã«ã€Œé•·è·é›¢å‚ç…§ã€ã‚’æä¾›ã™ã‚‹ãŸã‚ã€$N = 256K$ ã¾ã§ perplexityãŒæ”¹å–„ã—ç¶šã‘ã‚‹ã€‚

$$
\frac{d\,\text{PPL}}{d\,N} \bigg|_{N=N^*} = \begin{cases}
< 0 \text{ (æ”¹å–„ä¸­)} & \text{Jamba/Transformer} \\
\approx 0 \text{ (é ­æ‰“ã¡)} & \text{Pure Mamba} \quad (N^* \approx 10K)
\end{cases}
$$

**Effective Context Length**:

å„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ã€Œæœ‰åŠ¹ãªæ–‡è„ˆé•·ã€$N_\text{eff}$ (ã“ã‚Œä»¥ä¸Šå¢—ã‚„ã—ã¦ã‚‚perplexityãŒæ”¹å–„ã—ãªã„ä¸Šé™):

| Model | $N_\text{eff}$ | åˆ¶é™è¦å›  |
|:------|:-------------|:--------|
| Pure Transformer (FP16) | $N_\text{max} \approx 32K$ (OOM) | KV-Cache OOM |
| Pure Mamba | $\approx 8K\text{-}32K$ | æŒ‡æ•°æ¸›è¡°ã«ã‚ˆã‚‹è¨˜æ†¶é™ç•Œ |
| Jamba (1/8 Attn) | $> 256K$ | å®Ÿç”¨çš„ä¸Šé™ãªã— |
| Griffin (1/2 Attn) | $\approx 128K$ | Local Attentionã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ |

---

## 3.14 Course II ç†è«–çµ±åˆ â€” 10å›ã®æ—…è·¯ã®å¸°ç€ç‚¹

### 3.14.1 ELBO ã‹ã‚‰ Hybrid SSM ã¸ã®çµ±ä¸€çš„è¦–ç‚¹

Course II (ç¬¬9-18å›) ã§å­¦ã‚“ã å…¨ç†è«–ã‚’æŒ¯ã‚Šè¿”ã‚Šã€**çµ±ä¸€çš„ãªè¦–ç‚¹**ã§æ•´ç†ã™ã‚‹ã€‚

**å…±é€šãƒ†ãƒ¼ãƒ: æƒ…å ±ã®åœ§ç¸®ã¨è¡¨ç¾**

å…¨ã¦ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€Œãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’ã©ã†åœ§ç¸®ã—ã€ã©ã†è¡¨ç¾ã™ã‚‹ã‹ã€ã¨ã„ã†å•ã„ã«ç­”ãˆã¦ã„ã‚‹:

| æ‰‹æ³• | æƒ…å ±åœ§ç¸®ã®æ–¹æ³• | è¡¨ç¾ç©ºé–“ |
|:-----|:-------------|:--------|
| VI / ELBO (ç¬¬9å›) | KLæœ€å°åŒ–ã§è¿‘ä¼¼åˆ†å¸ƒã‚’å­¦ç¿’ | å¤‰åˆ†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ |
| VAE (ç¬¬10å›) | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§æ½œåœ¨å¤‰æ•°ã«åœ§ç¸® | æ½œåœ¨ç©ºé–“ $z \in \mathbb{R}^d$ |
| OT (ç¬¬11å›) | Wassersteinè·é›¢ã§åˆ†å¸ƒã‚’è¼¸é€ | æ¸¬åº¦ç©ºé–“ |
| GAN (ç¬¬12å›) | Nashå‡è¡¡ã§ç”Ÿæˆåˆ†å¸ƒã‚’å­¦ç¿’ | ç”Ÿæˆå™¨ã®å‡ºåŠ›ç©ºé–“ |
| è‡ªå·±å›å¸° (ç¬¬13å›) | é€£é–å¾‹ã§ç¢ºç‡ã‚’åˆ†è§£ | èªå½™ç©ºé–“ |
| Attention (ç¬¬14-15å›) | æ³¨æ„è¡Œåˆ—ã§æƒ…å ±ã‚’é¸æŠ | $\mathbb{R}^{N \times d}$ |
| SSM (ç¬¬16-17å›) | çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã§æ™‚ç³»åˆ—ã‚’åœ§ç¸® | çŠ¶æ…‹ç©ºé–“ $h \in \mathbb{R}^d$ |
| Hybrid (ç¬¬18å›) | ä¸¡æ–¹ã®åœ§ç¸®æˆ¦ç•¥ã‚’çµ„ã¿åˆã‚ã› | è¤‡åˆè¡¨ç¾ç©ºé–“ |

**æ•°å­¦çš„çµ±ä¸€**: å…¨æ‰‹æ³•ã¯æ¬¡ã®å¤‰åˆ†åŸç†ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã¨ã—ã¦è¦‹ã‚‰ã‚Œã‚‹:

$$
\min_{q \in \mathcal{Q}} \mathcal{F}(q, p) \quad \text{(æƒ…å ±é‡ã®æœ€å°åŒ–)}
$$

VAEã§ã¯ $\mathcal{F} = \text{ELBO}$ã€GANã§ã¯ $\mathcal{F} = \text{Jensen-Shannon divergence}$ã€SSMã§ã¯ $\mathcal{F}$ ã¯çŠ¶æ…‹ç©ºé–“ã®ã€Œè¿‘ä¼¼èª¤å·®ã€ã€Hybridã§ã¯ãã‚Œã‚‰ã®çµ„ã¿åˆã‚ã›ã€‚

### 3.14.2 ã€ŒAttention = SSM åŒå¯¾æ€§ã€ã®å«æ„

ç¬¬17å›ã®SSDå®šç† (Dao & Gu 2024) ãŒç¤ºã—ãŸæœ€é‡è¦çµè«–:

$$
\text{Causal Attentionè¡Œåˆ—} \equiv \text{Semi-Separable SSMè¡Œåˆ—}
$$

ã“ã‚ŒãŒç¤ºã™ã®ã¯**è¡¨ç¾ã®åŒå¯¾æ€§**ã ã€‚åŒã˜å…¥å‡ºåŠ›é–¢ä¿‚ã‚’ã€Attention (æ˜ç¤ºçš„ãªè¡Œåˆ—) ã¨ã—ã¦ã‚‚ã€SSM (å†å¸°çš„çŠ¶æ…‹é·ç§») ã¨ã—ã¦ã‚‚è¡¨ç¾ã§ãã‚‹ã€‚

**å®Ÿç”¨çš„å«æ„**:
1. **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é¸æŠ = å®Ÿè£…ã®é¸æŠ**: è¡¨ç¾åŠ›ã¯ç­‰ä¾¡ãªã®ã§ã€é¸æŠåŸºæº–ã¯ã€Œé€Ÿåº¦ã€ã€Œãƒ¡ãƒ¢ãƒªã€ã€Œãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–ã€
2. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®ç†è«–çš„æ­£å½“æ€§**: Attentionã¨SSMãŒã€ŒåŒã˜ã‚‚ã®ã€ãªã‚‰ã€æ··ãœã‚‹ã“ã¨ã¯ä¸€è²«ã—ãŸæ•°å­¦çš„æ“ä½œ
3. **ãƒ¢ãƒ‡ãƒ«åœ§ç¸®**: TrainedTransformerã‚’SSMã«è’¸ç•™ (ç¬¬17å›Sambaè«–æ–‡) ãŒç†è«–çš„ã«å¯èƒ½

### 3.14.3 Course IIIã¸ã®æ©‹æ¸¡ã—

Course IIIã§ç›´é¢ã™ã‚‹å®Ÿè·µçš„èª²é¡Œã¸ã®ç†è«–çš„æº–å‚™:

**è¨“ç·´å®‰å®šæ€§** (ç¬¬20å›äºˆå‘Š): LayerNorm + Residual + HiPPOåˆæœŸåŒ–ã®çµ„ã¿åˆã‚ã›ãŒä¿è¨¼ã™ã‚‹åæŸæ¡ä»¶ã¯ã€æœ¬è¬›ç¾©ã®å‹¾é…ãƒ•ãƒ­ãƒ¼è§£æ (3.10.6) ã§å­¦ã‚“ã ã€‚

**è©•ä¾¡æŒ‡æ¨™** (ç¬¬21å›äºˆå‘Š): Perplexityã®ç³»åˆ—é•·ä¾å­˜æ€§ (3.13.2) ã¯ã€é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡è¨­è¨ˆã«ç›´çµã™ã‚‹ã€‚

**ãƒ‡ãƒ—ãƒ­ã‚¤** (ç¬¬22å›äºˆå‘Š): Pure SSMã® $O(1)$ æ¨è«–ãƒ¡ãƒ¢ãƒª vs Hybrid ã® $O(L_\text{attn} \cdot N \cdot d)$ KV-Cache ã¯ã€ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–ãƒ»æœ€é©åŒ–ã®ç›®æ¨™è¨­å®šã«å½±éŸ¿ã™ã‚‹ã€‚

**No Free Lunch ã®å®Ÿè·µ** (3.11): ã€Œã©ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸ã¶ã‹ã€ã§ã¯ãªãã€Œã©ã®ã‚¿ã‚¹ã‚¯åˆ†å¸ƒã«å¯¾ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã‹ã€ã‚’å…ˆã«æ±ºã‚ã‚‹ â€” ã“ã‚ŒãŒCourse IIIã®è¨­è¨ˆæ€æƒ³ã®æ ¸å¿ƒã ã€‚

> **Note:** **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
>
> 1. Jamba ã® MoE ($E=16$, $K=2$) ãŒ Active Params ã‚’ $1/8$ ã«å‰Šæ¸›ã™ã‚‹æ•°å­¦çš„æ ¹æ‹ ã‚’ç¤ºã›ã€‚
> 2. Hybrid ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æŒ‡æ•° $\alpha(r)$ ãŒ Attention æ¯”ç‡ $r$ ã¨ã¨ã‚‚ã«å¢—åŠ ã™ã‚‹ç†ç”±ã‚’æƒ…å ±ç†è«–çš„ã«èª¬æ˜ã›ã‚ˆã€‚
> 3. SSD åŒå¯¾æ€§ãŒç¤ºã™ã€ŒAttention = SSMã€ã¨ã„ã†ç­‰ä¾¡æ€§ã¯ã€æœ‰é™ã®çŠ¶æ…‹æ¬¡å…ƒ $d < \infty$ ã®å ´åˆã§ã‚‚å³å¯†ã«æˆç«‹ã™ã‚‹ã‹ï¼Ÿ

---

## å‚è€ƒæ–‡çŒ®

[^1]: Lieber, O., et al. (2024). Jamba: A Hybrid Transformer-Mamba Language Model. *arXiv:2403.19887*.
<https://arxiv.org/abs/2403.19887>

[^2]: Glorioso, P., et al. (2024). Zamba: A Compact 7B SSM Hybrid Model. *arXiv:2405.16712*.
<https://arxiv.org/abs/2405.16712>

[^3]: De, S., et al. (2024). Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models. *arXiv:2402.19427*.
<https://arxiv.org/abs/2402.19427>

[^4]: Waleffe, R., et al. (2024). An Empirical Study of Mamba-based Language Models. *arXiv:2406.07887*.
<https://arxiv.org/abs/2406.07887>

[^5]: Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. *arXiv:2312.00752*.
<https://arxiv.org/abs/2312.00752>

[^6]: Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. *arXiv:2405.21060*.
<https://arxiv.org/abs/2405.21060>

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
