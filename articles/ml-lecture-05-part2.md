---
title: "ç¬¬5å›: æ¸¬åº¦è«–çš„ç¢ºç‡è«–ãƒ»ç¢ºç‡éç¨‹å…¥é–€: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ“"
type: "tech"
topics: ["machinelearning", "deeplearning", "measuretheory", "python"]
published: true
---

## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” æ¸¬åº¦è«–ã‚’ Python ã«ç¿»è¨³ã™ã‚‹

> **Zone 4 ç›®æ¨™**: æ¸¬åº¦è«–ã®æŠ½è±¡æ¦‚å¿µã‚’å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ã«è½ã¨ã—è¾¼ã‚€ã€‚Monte Carloç©åˆ†ã€KDEã€Markové€£é–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€Browné‹å‹•ãƒ‘ã‚¹ç”Ÿæˆã‚’å®Ÿè£…ã™ã‚‹ã€‚

### 4.1 Monte Carlo ç©åˆ† â€” Lebesgueç©åˆ†ã®è¿‘ä¼¼

ç†è«–ã§ã¯ $\int f \, d\mu$ ã¨æ›¸ããŒã€å®Ÿå‹™ã§ã¯Monte Carloæ³•ã§è¿‘ä¼¼ã™ã‚‹ã€‚

$$
\int f(x) \, p(x) \, dx \approx \frac{1}{N} \sum_{i=1}^{N} f(X_i), \quad X_i \sim p
$$

å¤§æ•°ã®æ³•å‰‡ãŒåæŸã‚’ä¿è¨¼ã™ã‚‹ã€‚

```python
import numpy as np
import time

def monte_carlo_integrate(f, sampler, n_samples: int, n_trials: int = 10):
    """Monte Carlo integration with timing.

    E[f(X)] â‰ˆ (1/N) Î£ f(X_i)
    Variance: Var[estimate] = Var[f(X)] / N
    """
    estimates = []
    for _ in range(n_trials):
        samples = sampler(n_samples)
        estimates.append(np.mean(f(samples)))
    return np.mean(estimates), np.std(estimates)

# Example 1: E[X^2] where X ~ N(0,1) â€” should be 1.0
f = lambda x: x**2
sampler = lambda n: np.random.randn(n)

print("=== Monte Carlo Integration ===\n")
print(f"Target: E[XÂ²] for X ~ N(0,1) = 1.0\n")
print(f"{'N':>10} {'Estimate':>10} {'Std':>10} {'Error':>10}")
for n in [100, 1_000, 10_000, 100_000, 1_000_000]:
    est, std = monte_carlo_integrate(f, sampler, n)
    print(f"{n:>10} {est:>10.4f} {std:>10.4f} {abs(est-1.0):>10.4f}")
```

> **è¦³å¯Ÿ**: $N$ ãŒ10å€ã«ãªã‚‹ã¨StdãŒ $\sqrt{10} \approx 3.16$ å€å°ã•ããªã‚‹ â€” Monte Carloã® $O(1/\sqrt{N})$ åæŸãƒ¬ãƒ¼ãƒˆã€‚

### 4.2 `%timeit` ãƒ‡ãƒ“ãƒ¥ãƒ¼ â€” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬

ç¬¬5å›ã‹ã‚‰ `%timeit` ã‚’ä½¿ã„å§‹ã‚ã‚‹ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆã®æ„Ÿè¦šã‚’é¤ŠãŠã†ã€‚

```python
import time

def benchmark(func, *args, n_runs=100, label=""):
    """Simple benchmark â€” %timeit equivalent for scripts."""
    times = []
    for _ in range(n_runs):
        start = time.perf_counter()
        func(*args)
        times.append(time.perf_counter() - start)
    mean_ms = np.mean(times) * 1000
    std_ms = np.std(times) * 1000
    print(f"{label:>30}: {mean_ms:.3f} Â± {std_ms:.3f} ms")
    return mean_ms

# Naive loop vs vectorized Monte Carlo
def mc_loop(n):
    """Naive loop implementation."""
    total = 0.0
    for _ in range(n):
        x = np.random.randn()
        total += x**2
    return total / n

def mc_vectorized(n):
    """Vectorized implementation."""
    x = np.random.randn(n)
    return np.mean(x**2)

N = 10_000
print("=== Benchmarking Monte Carlo ===\n")
benchmark(mc_loop, N, n_runs=50, label="Naive loop (N=10000)")
benchmark(mc_vectorized, N, n_runs=50, label="Vectorized (N=10000)")
benchmark(mc_vectorized, 100_000, n_runs=50, label="Vectorized (N=100000)")
```

> **æ•™è¨“**: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¯é€šå¸¸ **50-100å€** é«˜é€Ÿã€‚æ¸¬åº¦è«–ã®ç†è«–ã§ã¯summation orderã¯ç„¡é–¢ä¿‚ã ãŒã€å®Ÿè£…ã§ã¯**ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³**ãŒæ”¯é…çš„ã€‚

### 4.2.1 åˆ†æ•£ä½æ¸›æ³• â€” Monte Carloã‚’è³¢ãã™ã‚‹

Monte Carloã® $O(1/\sqrt{N})$ åæŸã¯å¤‰ãˆã‚‰ã‚Œãªã„ãŒã€**åˆ†æ•£ã®å®šæ•°å› å­**ã‚’æ¸›ã‚‰ã›ã‚‹ã€‚

```python
import numpy as np

def variance_reduction_comparison(n_samples=100_000):
    """Compare variance reduction techniques for E[e^X], X ~ U[0,1].

    Exact value: e - 1 â‰ˆ 1.71828
    """
    exact = np.e - 1

    # 1. Naive Monte Carlo
    x = np.random.uniform(0, 1, n_samples)
    naive = np.exp(x)

    # 2. Antithetic variates: use (X, 1-X) pairs
    x_half = np.random.uniform(0, 1, n_samples // 2)
    anti = np.concatenate([np.exp(x_half), np.exp(1 - x_half)])

    # 3. Control variate: use X as control (E[X] = 0.5 known)
    x_cv = np.random.uniform(0, 1, n_samples)
    f_cv = np.exp(x_cv)
    c_star = -np.cov(f_cv, x_cv)[0, 1] / np.var(x_cv)  # optimal c
    control = f_cv + c_star * (x_cv - 0.5)

    # 4. Stratified sampling: divide [0,1] into K strata
    K = 100
    n_per_stratum = n_samples // K
    strat_samples = []
    for k in range(K):
        u = np.random.uniform(k/K, (k+1)/K, n_per_stratum)
        strat_samples.extend(np.exp(u))
    stratified = np.array(strat_samples)

    print("=== Variance Reduction Comparison ===\n")
    print(f"Target: E[e^X] = e - 1 = {exact:.5f}\n")
    print(f"{'Method':>20} {'Mean':>10} {'Var':>12} {'Var ratio':>10}")
    naive_var = np.var(naive)
    for name, vals in [("Naive MC", naive),
                       ("Antithetic", anti),
                       ("Control Variate", control),
                       ("Stratified", stratified)]:
        v = np.var(vals)
        print(f"{name:>20} {np.mean(vals):>10.5f} {v:>12.6f} {v/naive_var:>10.3f}")

np.random.seed(42)
variance_reduction_comparison()
```


### 4.3 é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (Importance Sampling) â€” æ¸¬åº¦ã®å¤‰æ›

Radon-Nikodymå°é–¢æ•°ã®å®Ÿç”¨ç‰ˆã€‚$p$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒé›£ã—ã„å ´åˆã€åˆ¥ã®åˆ†å¸ƒ $q$ ã‚’ä½¿ã†:

$$
\mathbb{E}_p[f(X)] = \mathbb{E}_q\left[f(X) \frac{p(X)}{q(X)}\right] = \mathbb{E}_q\left[f(X) \frac{dP}{dQ}(X)\right]
$$

$\frac{p(x)}{q(x)}$ ãŒã¾ã•ã« **Radon-Nikodymå°é–¢æ•°** $\frac{dP}{dQ}(x)$ ã§ã‚ã‚‹ã€‚

```python
from scipy import stats

def importance_sampling(f, target_pdf, proposal, n_samples):
    """Importance sampling: E_p[f(X)] = E_q[f(X) * w(X)]

    w(X) = p(X) / q(X) = dP/dQ(X)  (Radon-Nikodym derivative)
    """
    samples = proposal.rvs(n_samples)
    weights = target_pdf(samples) / proposal.pdf(samples)

    # Normalize weights for stability
    weights_normalized = weights / np.sum(weights)
    estimate = np.sum(f(samples) * weights_normalized)

    # Effective sample size
    ess = 1.0 / np.sum(weights_normalized**2)

    return estimate, ess

# Target: E[X^2] where X ~ N(3, 0.5^2)
# But sample from proposal q = N(0, 2^2)
target = stats.norm(loc=3, scale=0.5)
proposal = stats.norm(loc=0, scale=2)

print("=== Importance Sampling ===\n")
print("Target: E[XÂ²] for X ~ N(3, 0.5Â²) =", 3**2 + 0.5**2, "= 9.25\n")

print(f"{'N':>8} {'Estimate':>10} {'ESS':>8} {'ESS%':>8}")
for n in [100, 1_000, 10_000, 100_000]:
    est, ess = importance_sampling(
        f=lambda x: x**2,
        target_pdf=target.pdf,
        proposal=proposal,
        n_samples=n
    )
    print(f"{n:>8} {est:>10.4f} {ess:>8.1f} {ess/n*100:>7.1f}%")
```


### 4.4 ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®š (KDE) â€” Radon-Nikodymå°é–¢æ•°ã®æ¨å®š

ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç¢ºç‡å¯†åº¦é–¢æ•°ï¼ˆ= Lebesgueæ¸¬åº¦ã«é–¢ã™ã‚‹Radon-Nikodymå°é–¢æ•°ï¼‰ã‚’æ¨å®šã™ã‚‹ã€‚

$$
\hat{f}_h(x) = \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{x - X_i}{h}\right)
$$

ãƒãƒ³ãƒ‰å¹… $h$ ã¯ã€Œæ¸¬åº¦ã®è§£åƒåº¦ã€ã‚’æ±ºã‚ã‚‹ã€‚

```python
def kde_estimate(data, x_grid, bandwidth):
    """Kernel density estimation with Gaussian kernel.

    K(u) = (1/âˆš(2Ï€)) exp(-uÂ²/2)
    fÌ‚_h(x) = (1/nh) Î£ K((x - X_i) / h)

    This estimates dP/dÎ» â€” the Radon-Nikodym derivative
    of the empirical measure w.r.t. Lebesgue measure.
    """
    n = len(data)
    # Vectorized: (n_grid, n_data)
    u = (x_grid[:, None] - data[None, :]) / bandwidth
    kernel_vals = np.exp(-0.5 * u**2) / np.sqrt(2 * np.pi)
    return np.mean(kernel_vals, axis=1) / bandwidth

# Generate mixture data
np.random.seed(42)
data = np.concatenate([
    np.random.normal(-2, 0.5, 300),
    np.random.normal(1, 0.8, 500),
    np.random.normal(4, 0.3, 200),
])
x_grid = np.linspace(-5, 7, 500)

print("=== Kernel Density Estimation ===\n")
print(f"Data: {len(data)} samples from 3-component GMM\n")
print(f"{'Bandwidth':>10} {'âˆ«fÌ‚dx':>8} {'max(fÌ‚)':>8}")
for h in [0.1, 0.3, 0.5, 1.0, 2.0]:
    density = kde_estimate(data, x_grid, h)
    integral = np.trapz(density, x_grid)
    print(f"{h:>10.1f} {integral:>8.4f} {np.max(density):>8.4f}")
```


### 4.5 Markové€£é–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ â€” å®šå¸¸åˆ†å¸ƒã¸ã®åæŸ

å®šå¸¸åˆ†å¸ƒ $\boldsymbol{\pi}$ ã¸ã®åæŸã‚’å¯è¦–åŒ–ã™ã‚‹ã€‚

```python
def simulate_markov_chain(P, initial_state, n_steps):
    """Simulate discrete Markov chain.

    P[i,j] = Pr(X_{n+1} = j | X_n = i)
    Stationary: Ï€ P = Ï€
    """
    n_states = P.shape[0]
    states = [initial_state]
    state = initial_state

    # Track empirical distribution
    counts = np.zeros(n_states)
    counts[initial_state] = 1
    empirical_history = [counts.copy() / 1]

    for step in range(1, n_steps):
        state = np.random.choice(n_states, p=P[state])
        states.append(state)
        counts[state] += 1
        empirical_history.append(counts.copy() / (step + 1))

    return np.array(states), np.array(empirical_history)

# Ehrenfest model: gas molecules between two containers
# State = number of molecules in container A (0 to N)
N_molecules = 10

def ehrenfest_transition(N):
    """Ehrenfest diffusion model.

    State i â†’ i-1 with prob i/N (molecule leaves A)
    State i â†’ i+1 with prob (N-i)/N (molecule enters A)
    """
    P = np.zeros((N+1, N+1))
    for i in range(N+1):
        if i > 0:
            P[i, i-1] = i / N
        if i < N:
            P[i, i+1] = (N - i) / N
    return P

P_ehr = ehrenfest_transition(N_molecules)

# Compute stationary distribution analytically: Binomial(N, 1/2)
from scipy.special import comb
pi_exact = np.array([comb(N_molecules, k) / 2**N_molecules
                     for k in range(N_molecules+1)])

# Simulate from extreme initial state
states, emp_hist = simulate_markov_chain(P_ehr, initial_state=0, n_steps=10_000)

print("=== Ehrenfest Diffusion Model ===\n")
print(f"N molecules = {N_molecules}")
print(f"Stationary distribution: Binomial({N_molecules}, 1/2)\n")

print(f"{'Step':>6} ", end="")
for s in range(N_molecules+1):
    print(f"{'Ï€('+str(s)+')':>7}", end="")
print()

for t in [10, 100, 1000, 5000, 10000]:
    print(f"{t:>6} ", end="")
    for s in range(N_molecules+1):
        print(f"{emp_hist[t-1, s]:>7.3f}", end="")
    print()

print(f"{'Exact':>6} ", end="")
for s in range(N_molecules+1):
    print(f"{pi_exact[s]:>7.3f}", end="")
print()

# Total variation distance
for t in [10, 100, 1000, 5000, 10000]:
    tv = 0.5 * np.sum(np.abs(emp_hist[t-1] - pi_exact))
    print(f"\nTV distance at step {t}: {tv:.4f}")
```


### 4.6 Metropolis-Hastings â€” MCMC ã®åŸºç¤

è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã‚’ä½¿ã£ã¦ã€ä»»æ„ã®ç›®æ¨™åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

$$
\alpha(x, x') = \min\left(1, \frac{\pi(x') q(x \mid x')}{\pi(x) q(x' \mid x)}\right)
$$

$\pi$ ã®æ­£è¦åŒ–å®šæ•°ã‚’çŸ¥ã‚‰ãªãã¦ã‚‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãã‚‹ â€” ã“ã‚ŒãŒãƒ™ã‚¤ã‚ºæ¨è«–ã§é‡è¦ã€‚

```python
def metropolis_hastings(log_target, proposal_std, x0, n_samples, burnin=1000):
    """Metropolis-Hastings MCMC sampler.

    Detailed balance: Ï€(x) P(xâ†’x') = Ï€(x') P(x'â†’x)
    Acceptance: Î± = min(1, Ï€(x')q(x|x') / Ï€(x)q(x'|x))
    For symmetric proposal: Î± = min(1, Ï€(x')/Ï€(x))
    """
    x = x0
    samples = []
    accepted = 0

    for i in range(n_samples + burnin):
        # Symmetric proposal: q(x'|x) = N(x, ÏƒÂ²)
        x_proposed = x + proposal_std * np.random.randn()

        # Log acceptance ratio (symmetric â†’ simplifies)
        log_alpha = log_target(x_proposed) - log_target(x)

        if np.log(np.random.rand()) < log_alpha:
            x = x_proposed
            if i >= burnin:
                accepted += 1

        if i >= burnin:
            samples.append(x)

    acceptance_rate = accepted / n_samples
    return np.array(samples), acceptance_rate

# Target: mixture of Gaussians (unnormalized)
def log_target_mixture(x):
    """Log of unnormalized mixture density."""
    return np.logaddexp(
        -0.5 * (x + 2)**2 / 0.5**2,
        -0.5 * (x - 3)**2 / 1.0**2
    )

np.random.seed(42)
print("=== Metropolis-Hastings MCMC ===\n")
print(f"{'Ïƒ_proposal':>12} {'Accept%':>10} {'Mean':>8} {'Std':>8}")
for sigma in [0.1, 0.5, 1.0, 3.0, 10.0]:
    samples, rate = metropolis_hastings(
        log_target_mixture, sigma, x0=0.0, n_samples=50_000
    )
    print(f"{sigma:>12.1f} {rate*100:>9.1f}% {np.mean(samples):>8.3f} {np.std(samples):>8.3f}")
```


### 4.7 Browné‹å‹•ãƒ‘ã‚¹ç”Ÿæˆ â€” é›¢æ•£è¿‘ä¼¼

$W(t_{k+1}) = W(t_k) + \sqrt{\Delta t} \cdot Z_k, \quad Z_k \sim \mathcal{N}(0,1)$

```python
def simulate_brownian_paths(T, n_steps, n_paths):
    """Generate Brownian motion paths.

    W(0) = 0
    W(t+Î”t) - W(t) ~ N(0, Î”t)  (independent increments)
    """
    dt = T / n_steps
    increments = np.sqrt(dt) * np.random.randn(n_paths, n_steps)
    paths = np.zeros((n_paths, n_steps + 1))
    paths[:, 1:] = np.cumsum(increments, axis=1)
    return paths, np.linspace(0, T, n_steps + 1)

def verify_brownian_properties(paths, times):
    """Verify 5 defining properties of Brownian motion."""
    dt = times[1] - times[0]
    n_paths = paths.shape[0]
    n_steps = paths.shape[1] - 1

    print("=== Brownian Motion Properties Verification ===\n")

    # Property 1: W(0) = 0
    print(f"1. W(0) = 0: max|W(0)| = {np.max(np.abs(paths[:, 0])):.6f}")

    # Property 2: Independent increments
    inc1 = paths[:, n_steps//4] - paths[:, 0]
    inc2 = paths[:, n_steps//2] - paths[:, n_steps//4]
    corr = np.corrcoef(inc1, inc2)[0, 1]
    print(f"2. Independent increments: corr = {corr:.4f} (should â‰ˆ 0)")

    # Property 3: W(t) ~ N(0, t)
    for frac, label in [(0.25, "T/4"), (0.5, "T/2"), (1.0, "T")]:
        idx = int(frac * n_steps)
        t = times[idx]
        vals = paths[:, idx]
        print(f"3. W({label}): mean={np.mean(vals):.4f} (â†’0), "
              f"var={np.var(vals):.4f} (â†’{t:.2f})")

    # Property 4: E[W(t)Â²] = t
    t_mid = times[n_steps // 2]
    emp_var = np.mean(paths[:, n_steps//2]**2)
    print(f"4. E[W(T/2)Â²] = {emp_var:.4f} (theory: {t_mid:.4f})")

    # Property 5: Quadratic variation
    increments = np.diff(paths, axis=1)
    qv = np.sum(increments**2, axis=1)  # per path
    print(f"5. Quadratic variation [W]_T: mean={np.mean(qv):.4f} "
          f"(theory: {times[-1]:.4f}), std={np.std(qv):.4f}")

np.random.seed(42)
paths, times = simulate_brownian_paths(T=1.0, n_steps=10_000, n_paths=5000)
verify_brownian_properties(paths, times)
```


### 4.8 å¹¾ä½•Browné‹å‹• (GBM) â€” ItÃ´ã®å…¬å¼ã®å®Ÿè·µ

æ ªä¾¡ãƒ¢ãƒ‡ãƒ«ã®å¤å…¸:

$$
dS = \mu S \, dt + \sigma S \, dW
$$

ItÃ´ã®å…¬å¼ã«ã‚ˆã‚Šè§£æè§£ãŒå¾—ã‚‰ã‚Œã‚‹:

$$
S(t) = S(0) \exp\left(\left(\mu - \frac{\sigma^2}{2}\right)t + \sigma W(t)\right)
$$

$-\frac{\sigma^2}{2}$ ã® **ItÃ´è£œæ­£é …** ã«æ³¨æ„ â€” ã“ã‚ŒãŒä¼Šè—¤ç©åˆ†ã®éç›´æ„Ÿçš„ãªéƒ¨åˆ†ã€‚

```python
def geometric_brownian_motion(S0, mu, sigma, T, n_steps, n_paths):
    """Simulate Geometric Brownian Motion.

    dS = Î¼S dt + ÏƒS dW
    Exact solution (ItÃ´'s formula):
    S(t) = Sâ‚€ exp((Î¼ - ÏƒÂ²/2)t + ÏƒW(t))

    The -ÏƒÂ²/2 is the ItÃ´ correction term.
    """
    dt = T / n_steps
    times = np.linspace(0, T, n_steps + 1)

    # Method 1: Exact solution using Brownian motion
    W = np.zeros((n_paths, n_steps + 1))
    W[:, 1:] = np.cumsum(np.sqrt(dt) * np.random.randn(n_paths, n_steps), axis=1)
    S_exact = S0 * np.exp((mu - 0.5 * sigma**2) * times[None, :] + sigma * W)

    # Method 2: Euler-Maruyama discretization
    S_euler = np.zeros((n_paths, n_steps + 1))
    S_euler[:, 0] = S0
    for i in range(n_steps):
        dW = W[:, i+1] - W[:, i]
        S_euler[:, i+1] = S_euler[:, i] * (1 + mu * dt + sigma * dW)

    return S_exact, S_euler, times

np.random.seed(42)
S0 = 100.0
mu = 0.1    # drift
sigma = 0.3  # volatility
T = 1.0
n_steps = 1000
n_paths = 10_000

S_exact, S_euler, times = geometric_brownian_motion(S0, mu, sigma, T, n_steps, n_paths)

print("=== Geometric Brownian Motion ===\n")
print(f"Sâ‚€ = {S0}, Î¼ = {mu}, Ïƒ = {sigma}\n")

# Theory: E[S(T)] = Sâ‚€ exp(Î¼T)
E_theory = S0 * np.exp(mu * T)
print(f"Theory E[S(T)] = Sâ‚€ exp(Î¼T) = {E_theory:.2f}")
print(f"Exact  E[S(T)] = {np.mean(S_exact[:, -1]):.2f}")
print(f"Euler  E[S(T)] = {np.mean(S_euler[:, -1]):.2f}")

# Theory: Var[S(T)] = Sâ‚€Â² exp(2Î¼T) (exp(ÏƒÂ²T) - 1)
Var_theory = S0**2 * np.exp(2*mu*T) * (np.exp(sigma**2 * T) - 1)
print(f"\nTheory Var[S(T)] = {Var_theory:.2f}")
print(f"Exact  Var[S(T)] = {np.var(S_exact[:, -1]):.2f}")
print(f"Euler  Var[S(T)] = {np.var(S_euler[:, -1]):.2f}")

# ItÃ´ correction demonstration
print(f"\nItÃ´ correction: -ÏƒÂ²/2 = {-sigma**2/2:.4f}")
print(f"Without correction: E[S(T)] would be {S0 * np.exp((mu)*T):.2f} (wrong!)")
print(f"With correction: drift = Î¼ - ÏƒÂ²/2 = {mu - sigma**2/2:.4f}")
```

### 4.9 Ornstein-Uhlenbeckéç¨‹ â€” DDPMã®é€£ç¶šæ¥µé™

Diffusion modelã®é€£ç¶šæ¥µé™ã¯Ornstein-Uhlenbeck (OU) éç¨‹:

$$
dX_t = -\theta X_t \, dt + \sigma \, dW_t
$$

å¹³å‡å›å¸°æ€§ï¼ˆmean-revertingï¼‰ã‚’æŒã¡ã€å®šå¸¸åˆ†å¸ƒã¯ $\mathcal{N}(0, \sigma^2/(2\theta))$ã€‚

```python
def ornstein_uhlenbeck(theta, sigma, x0, T, n_steps, n_paths):
    """Simulate Ornstein-Uhlenbeck process.

    dX = -Î¸X dt + Ïƒ dW
    Solution: X(t) = xâ‚€ e^{-Î¸t} + Ïƒ âˆ«â‚€áµ— e^{-Î¸(t-s)} dW(s)
    Stationary distribution: N(0, ÏƒÂ²/(2Î¸))
    """
    dt = T / n_steps
    times = np.linspace(0, T, n_steps + 1)
    X = np.zeros((n_paths, n_steps + 1))
    X[:, 0] = x0

    for i in range(n_steps):
        dW = np.sqrt(dt) * np.random.randn(n_paths)
        X[:, i+1] = X[:, i] - theta * X[:, i] * dt + sigma * dW

    return X, times

np.random.seed(42)
theta = 2.0
sigma = 1.0
stat_var = sigma**2 / (2 * theta)  # = 0.25

print("=== Ornstein-Uhlenbeck Process ===\n")
print(f"Î¸ = {theta}, Ïƒ = {sigma}")
print(f"Stationary distribution: N(0, {stat_var:.4f})\n")

# Start from x0 = 5.0 (far from stationary mean 0)
X, times = ornstein_uhlenbeck(theta, sigma, x0=5.0, T=5.0, n_steps=5000, n_paths=5000)

print(f"{'t':>6} {'E[X(t)]':>10} {'Var[X(t)]':>10} {'Theory E':>10} {'Theory V':>10}")
for t_idx in [0, 500, 1000, 2000, 5000]:
    t = times[t_idx]
    emp_mean = np.mean(X[:, t_idx])
    emp_var = np.var(X[:, t_idx])
    theory_mean = 5.0 * np.exp(-theta * t)
    theory_var = stat_var * (1 - np.exp(-2 * theta * t))
    print(f"{t:>6.2f} {emp_mean:>10.4f} {emp_var:>10.4f} "
          f"{theory_mean:>10.4f} {theory_var:>10.4f}")
```


### 4.10 Langevin Dynamics â€” Scoreé–¢æ•°ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

Score function $\nabla_x \log p(x)$ ã‚’ä½¿ã£ã¦ç›®æ¨™åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹Langevin Monte Carloæ³•:

$$
X_{k+1} = X_k + \frac{\epsilon}{2} \nabla_x \log p(X_k) + \sqrt{\epsilon} \, Z_k, \quad Z_k \sim \mathcal{N}(0, I)
$$

$\epsilon \to 0$ã€$K \to \infty$ ã§ $X_K \sim p$ ã«åæŸã™ã‚‹[^2]ã€‚

```python
import numpy as np

def langevin_mcmc(score_fn, x0, epsilon, n_steps, burnin=1000):
    """Unadjusted Langevin Algorithm (ULA).

    x_{k+1} = x_k + (Îµ/2) âˆ‡log p(x_k) + âˆšÎµ z_k
    Stationary distribution: p(x) (as Îµâ†’0)
    """
    x = np.array(x0, dtype=float)
    samples = []

    for k in range(n_steps + burnin):
        grad = score_fn(x)
        noise = np.random.randn(*x.shape)
        x = x + 0.5 * epsilon * grad + np.sqrt(epsilon) * noise

        if k >= burnin:
            samples.append(x.copy())

    return np.array(samples)

# Target: Gaussian mixture
def gmm_score(x):
    """Score of 0.3 N(-2, 0.5Â²) + 0.7 N(2, 0.8Â²)."""
    w1, mu1, s1 = 0.3, -2.0, 0.5
    w2, mu2, s2 = 0.7, 2.0, 0.8

    phi1 = np.exp(-0.5*((x-mu1)/s1)**2) / (s1*np.sqrt(2*np.pi))
    phi2 = np.exp(-0.5*((x-mu2)/s2)**2) / (s2*np.sqrt(2*np.pi))

    dphi1 = phi1 * (-(x-mu1)/s1**2)
    dphi2 = phi2 * (-(x-mu2)/s2**2)

    p = w1*phi1 + w2*phi2 + 1e-10
    return (w1*dphi1 + w2*dphi2) / p

np.random.seed(42)
print("=== Langevin MCMC ===\n")
print(f"{'Îµ':>8} {'Mean':>8} {'Std':>8} {'Mode1 frac':>12}")
for eps in [0.001, 0.01, 0.1, 0.5, 1.0]:
    samples = langevin_mcmc(gmm_score, x0=0.0, epsilon=eps, n_steps=50_000)
    mode1_frac = np.mean(samples < 0)
    print(f"{eps:>8.3f} {np.mean(samples):>8.3f} {np.std(samples):>8.3f} "
          f"{mode1_frac:>12.3f}")

print(f"\nTheory mode1 fraction: 0.3")
print(f"Theory mean: 0.3Ã—(-2) + 0.7Ã—2 = {0.3*(-2)+0.7*2:.1f}")
```


### 4.11 Euler-Maruyamaæ³• â€” SDEã®æ•°å€¤è§£æ³•

SDEã®å³å¯†è§£ãŒå¾—ã‚‰ã‚Œã‚‹ã‚±ãƒ¼ã‚¹ï¼ˆGBMã€OUéç¨‹ï¼‰ã¯å°‘æ•°æ´¾ã ã€‚ä¸€èˆ¬ã®SDEã§ã¯**æ•°å€¤è§£æ³•**ãŒå¿…è¦ã«ãªã‚‹ã€‚æœ€ã‚‚åŸºæœ¬çš„ãªæ‰‹æ³•ãŒEuler-Maruyamaæ³• â€” ODE ã®Euleræ³•ã‚’SDEã«æ‹¡å¼µã—ãŸã‚‚ã®ã€‚

#### é›¢æ•£åŒ–ã‚¹ã‚­ãƒ¼ãƒ 

SDE $dX_t = f(X_t) \, dt + g(X_t) \, dW_t$ ã‚’æ™‚é–“å¹… $\Delta t$ ã§é›¢æ•£åŒ–ã™ã‚‹:

$$
X_{n+1} = X_n + f(X_n) \Delta t + g(X_n) \sqrt{\Delta t} \, Z_n, \quad Z_n \sim \mathcal{N}(0, 1)
$$

$\sqrt{\Delta t} \, Z_n$ ãŒ Browné‹å‹•å¢—åˆ† $\Delta W_n = W_{t_{n+1}} - W_{t_n} \sim \mathcal{N}(0, \Delta t)$ ã«å¯¾å¿œã€‚

ã“ã‚Œã¯ python-hpc-report.md ã® Pattern 12 ãã®ã‚‚ã®:

```python
import numpy as np
import time

def euler_maruyama(f, g, x0, T, n_steps, n_paths=1):
    """Euler-Maruyama method for SDE: dX = f(X)dt + g(X)dW

    Discretization: X_{n+1} = X_n + f(X_n)*dt + g(X_n)*âˆšdt*Z_n
    Strong convergence: O(âˆšdt)
    Weak convergence:   O(dt)
    """
    dt = T / n_steps
    sqrt_dt = np.sqrt(dt)
    X = np.zeros((n_paths, n_steps + 1))
    X[:, 0] = x0

    for n in range(n_steps):
        Z = np.random.randn(n_paths)
        X[:, n+1] = X[:, n] + f(X[:, n]) * dt + g(X[:, n]) * sqrt_dt * Z

    return X, np.linspace(0, T, n_steps + 1)
```

#### å¼·åæŸã¨å¼±åæŸ

| åæŸã®ç¨®é¡ | å®šç¾© | Euler-Maruyama | æ„å‘³ |
|:---------|:----|:-------------|:-----|
| å¼·åæŸ | $\mathbb{E}[\|X_N - X(T)\|] \leq C \Delta t^{1/2}$ | $O(\sqrt{\Delta t})$ | ãƒ‘ã‚¹ãŒè¿‘ã„ |
| å¼±åæŸ | $\|\mathbb{E}[h(X_N)] - \mathbb{E}[h(X(T))]\| \leq C \Delta t$ | $O(\Delta t)$ | çµ±è¨ˆé‡ãŒè¿‘ã„ |

- **å¼·åæŸ**: å€‹ã€…ã®ãƒ‘ã‚¹ãŒçœŸã®è§£ã«è¿‘ã„ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»å¯è¦–åŒ–ã«é‡è¦ï¼‰
- **å¼±åæŸ**: æœŸå¾…å€¤ã‚„åˆ†å¸ƒã®æ€§è³ªãŒæ­£ã—ã„ï¼ˆçµ±è¨ˆé‡ã®æ¨å®šã«ååˆ†ï¼‰

æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã¯å¤šãã®å ´åˆã€**å¼±åæŸã§ååˆ†**ï¼ˆç”Ÿæˆç”»åƒã®åˆ†å¸ƒãŒæ­£ã—ã‘ã‚Œã°ã‚ˆã„ï¼‰ã€‚DDPM ã®é›¢æ•£ã‚¹ãƒ†ãƒƒãƒ—æ•° $T = 1000$ ã¯å¼±åæŸã®ç²¾åº¦ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã€‚

```python
import numpy as np
import time

def convergence_analysis():
    """Demonstrate strong and weak convergence of Euler-Maruyama.

    Test SDE: dX = -X dt + dW  (OU process)
    Exact solution: X(t) = xâ‚€ e^{-t} + âˆ«â‚€áµ— e^{-(t-s)} dW(s)
    E[X(T)] = xâ‚€ e^{-T},  Var[X(T)] = (1-e^{-2T})/2
    """
    T = 1.0
    x0 = 1.0
    n_paths = 50000

    # OU process parameters
    theta = 1.0
    sigma = 1.0

    # Exact statistics at T
    exact_mean = x0 * np.exp(-theta * T)
    exact_var = sigma**2 / (2 * theta) * (1 - np.exp(-2 * theta * T))

    f = lambda x: -theta * x
    g = lambda x: sigma * np.ones_like(x)

    step_sizes = [10, 50, 100, 500, 1000, 5000]

    print("=== Euler-Maruyama Convergence Analysis ===\n")
    print(f"SDE: dX = -{theta}X dt + {sigma} dW,  xâ‚€ = {x0},  T = {T}")
    print(f"Exact: E[X(T)] = {exact_mean:.6f},  Var[X(T)] = {exact_var:.6f}\n")

    print(f"{'N steps':>8} {'dt':>10} {'|E err|':>10} {'|Var err|':>10} "
          f"{'Weak O(dt)':>12} {'Time (ms)':>10}")
    print("-" * 65)

    prev_weak_err = None
    for n_steps in step_sizes:
        dt = T / n_steps
        np.random.seed(42)

        t_start = time.perf_counter()
        X, _ = euler_maruyama(f, g, x0, T, n_steps, n_paths)
        elapsed = (time.perf_counter() - t_start) * 1000

        em_mean = X[:, -1].mean()
        em_var = X[:, -1].var()
        weak_err = abs(em_mean - exact_mean)
        var_err = abs(em_var - exact_var)

        ratio = f"{prev_weak_err / weak_err:.2f}x" if prev_weak_err and weak_err > 1e-8 else "---"
        prev_weak_err = weak_err

        print(f"{n_steps:>8} {dt:>10.5f} {weak_err:>10.6f} {var_err:>10.6f} "
              f"{ratio:>12} {elapsed:>10.1f}")

    print(f"\nWeak convergence: error ~ O(dt). Doubling N should halve error.")
    print(f"Strong convergence: error ~ O(âˆšdt). Doubling N reduces by âˆš2.")

    # Numba speedup demo
    try:
        import numba

        @numba.jit(nopython=True)
        def em_numba(x0, theta, sigma, T, n_steps, n_paths):
            """Numba-accelerated Euler-Maruyama."""
            dt = T / n_steps
            sqrt_dt = np.sqrt(dt)
            X_final = np.empty(n_paths)
            for p in range(n_paths):
                x = x0
                for n in range(n_steps):
                    x = x - theta * x * dt + sigma * sqrt_dt * np.random.randn()
                X_final[p] = x
            return X_final

        # Warmup
        _ = em_numba(x0, theta, sigma, T, 100, 10)

        n_bench = 5000
        n_paths_bench = 10000

        # Pure Python (via our function)
        np.random.seed(0)
        t0 = time.perf_counter()
        X_py, _ = euler_maruyama(f, g, x0, T, n_bench, n_paths_bench)
        t_python = time.perf_counter() - t0

        # Numba
        np.random.seed(0)
        t0 = time.perf_counter()
        X_nb = em_numba(x0, theta, sigma, T, n_bench, n_paths_bench)
        t_numba = time.perf_counter() - t0

        print(f"\n=== Euler-Maruyama: NumPy vs Numba ===")
        print(f"  {n_paths_bench} paths Ã— {n_bench} steps:")
        print(f"  NumPy vectorized: {t_python*1000:.1f} ms")
        print(f"  Numba JIT:        {t_numba*1000:.1f} ms  ({t_python/t_numba:.1f}x speedup)")
        print(f"  (Sequential SDE = FOR loop â†’ Numba shines here)")
    except ImportError:
        print("\n  [Numba not installed â€” skipping benchmark]")

convergence_analysis()
```


### 4.12 åæŸå®šç†ã®æ•°å€¤æ¤œè¨¼ â€” MCT vs DCT vs Fatou

3ã¤ã®åæŸå®šç†ã‚’åŒæ™‚ã«æ¤œè¨¼ã™ã‚‹ã€‚

```python
import numpy as np

def convergence_theorems_verification():
    """Numerically verify MCT, DCT, and Fatou's lemma."""
    x = np.linspace(0.001, 10, 10_000)
    dx = x[1] - x[0]

    print("=== Convergence Theorems Verification ===\n")

    # MCT: f_n(x) = x^n * e^{-x} * 1_{[0,n]}
    # 0 â‰¤ f_1 â‰¤ f_2 â‰¤ ... â†‘ x^âˆ stuff... let's use simpler
    # f_n(x) = min(f(x), n) for f(x) = 1/âˆšx
    print("--- Monotone Convergence Theorem ---")
    print("f_n(x) = min(1/âˆšx, n), f(x) = 1/âˆšx on [0.001, 10]\n")
    f_limit = 1.0 / np.sqrt(x)
    int_limit = np.trapz(f_limit, x)

    print(f"{'n':>6} {'âˆ«f_n dx':>12} {'âˆ«f dx':>12}")
    for n in [1, 2, 5, 10, 50, 100]:
        fn = np.minimum(f_limit, n)
        int_fn = np.trapz(fn, x)
        print(f"{n:>6} {int_fn:>12.4f} {int_limit:>12.4f}")
    print(f"MCT: âˆ«f_n â†‘ âˆ«f âœ“\n")

    # DCT: f_n(x) = sin(nx)/(nx) â†’ 0 pointwise, |f_n| â‰¤ 1/|nx| ...
    # Better: f_n(x) = (1 + x/n)^{-n} â†’ e^{-x}, |f_n| â‰¤ 1
    print("--- Dominated Convergence Theorem ---")
    print("f_n(x) = (1 + x/n)^{-n} â†’ e^{-x}, |f_n| â‰¤ 1\n")

    f_target = np.exp(-x)
    int_target = np.trapz(f_target, x)

    print(f"{'n':>6} {'âˆ«f_n dx':>12} {'âˆ«e^(-x) dx':>12} {'|diff|':>10}")
    for n in [1, 2, 5, 10, 50, 100, 1000]:
        fn = (1 + x/n)**(-n)
        int_fn = np.trapz(fn, x)
        print(f"{n:>6} {int_fn:>12.6f} {int_target:>12.6f} "
              f"{abs(int_fn - int_target):>10.6f}")
    print(f"DCT: âˆ«f_n â†’ âˆ«(lim f_n) âœ“\n")

    # Fatou's lemma: lim inf âˆ«f_n â‰¥ âˆ«(lim inf f_n)
    print("--- Fatou's Lemma ---")
    print("f_n(x) = n * x * exp(-nxÂ²): âˆ«f_n = 1/2 for all n\n")

    print(f"{'n':>6} {'âˆ«f_n dx':>12} {'âˆ«(lim f_n)dx':>14}")
    for n in [1, 5, 10, 50, 100]:
        fn = n * x * np.exp(-n * x**2)
        int_fn = np.trapz(fn, x)
        print(f"{n:>6} {int_fn:>12.4f} {'0.0000':>14}")

    print(f"\nlim inf âˆ«f_n = 0.5000 â‰¥ âˆ«(lim inf f_n) = 0.0000 âœ“")
    print(f"Fatou inequality is STRICT here (not equality)")

convergence_theorems_verification()
```


### 4.13 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³é›†

| æ•°å¼ | Python | æ³¨æ„ç‚¹ |
|:--|:--|:--|
| $\int f \, d\mu$ | `np.mean(f(samples))` | Monte Carloè¿‘ä¼¼ |
| $\frac{dP}{dQ}(x)$ | `p.pdf(x) / q.pdf(x)` | Importance weight |
| $\hat{f}_h(x)$ | `kde_estimate(data, x, h)` | ãƒãƒ³ãƒ‰å¹…é¸æŠãŒé‡è¦ |
| $P^n$ | `np.linalg.matrix_power(P, n)` | å®šå¸¸åˆ†å¸ƒã¸åæŸ |
| $W(t)$ | `np.cumsum(np.sqrt(dt)*Z)` | $Z \sim \mathcal{N}(0,1)$ |
| $\sum (\Delta W)^2$ | `np.sum(np.diff(W)**2)` | $\to T$ï¼ˆäºŒæ¬¡å¤‰å‹•ï¼‰ |
| $dX = a \, dt + b \, dW$ | `X[i+1] = X[i] + a*dt + b*dW` | Euler-Maruyama |
| $e^{-\theta t}$ | `np.exp(-theta*t)` | OUéç¨‹ã®å¹³å‡å›å¸° |
| $\frac{1}{nh}\sum K(\cdot)$ | `np.mean(kernel) / h` | KDE |
| $\boldsymbol{\pi} P = \boldsymbol{\pi}$ | `eig(P.T)` ã§å›ºæœ‰å€¤1ã®å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« | å·¦å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ« |

### 4.14 LaTeX ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ â€” æ¸¬åº¦è«–è¨˜æ³•

```latex
% Measure spaces
\sigma\text{-algebra} \quad (X, \mathcal{F}, \mu) \quad \mu(A)

% Lebesgue integral
\int_A f \, d\mu \quad \int_{\mathbb{R}} f \, d\lambda

% Radon-Nikodym
\frac{dP}{dQ} \quad P \ll Q \quad (absolute continuity)

% Convergence
X_n \xrightarrow{\text{a.s.}} X \quad
X_n \xrightarrow{P} X \quad
X_n \xrightarrow{L^p} X \quad
X_n \xrightarrow{d} X

% Stochastic processes
W(t) \quad dW_t \quad [W]_t = t

% SDEs
dX_t = \mu(X_t) \, dt + \sigma(X_t) \, dW_t

% ItÃ´'s formula
df = f'(X) \, dX + \frac{1}{2} f''(X) \, (dX)^2

% Markov chains
P(X_{n+1} = j \mid X_n = i) = P_{ij}
\boldsymbol{\pi} P = \boldsymbol{\pi}

% Conditional expectation
\mathbb{E}[X \mid \mathcal{G}]
```


---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ç†è§£åº¦ãƒã‚§ãƒƒã‚¯

> **Zone 5 ç›®æ¨™**: Zone 0-4 ã®ç†è§£ã‚’è‡ªå·±è¨ºæ–­ã™ã‚‹ã€‚èª­ã¿ãƒ»æ›¸ããƒ»å®Ÿè£…ã®3è»¸ã§ç¢ºèªã€‚

### 5.1 ã‚·ãƒ³ãƒœãƒ«ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚° â€” æ¸¬åº¦è«–è¨˜å·ã‚’èª­ã‚€

ä»¥ä¸‹ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ã€æ„å‘³ã‚’èª¬æ˜ã›ã‚ˆã€‚

:::details ç­”ãˆåˆã‚ã›ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§é–‹ãï¼‰

| # | è¨˜å· | èª­ã¿ | æ„å‘³ |
|:--:|:--|:--|:--|
| 1 | $(\Omega, \mathcal{F}, P)$ | ã€Œç¢ºç‡ç©ºé–“ã‚ªãƒ¡ã‚¬ã€ã‚¨ãƒ•ã€ãƒ”ãƒ¼ã€ | æ¨™æœ¬ç©ºé–“ãƒ»Ïƒ-åŠ æ³•æ—ãƒ»ç¢ºç‡æ¸¬åº¦ã®ä¸‰ã¤çµ„ |
| 2 | $\mu(A) = \int_A d\mu$ | ã€ŒãƒŸãƒ¥ãƒ¼ã®Aã¯ã€Aä¸Šã§dãƒŸãƒ¥ãƒ¼ã‚’ç©åˆ†ã€ | é›†åˆAã®æ¸¬åº¦ |
| 3 | $P \ll Q$ | ã€ŒPã¯Qã«é–¢ã—ã¦çµ¶å¯¾é€£ç¶šã€ | $Q(A)=0 \Rightarrow P(A)=0$ |
| 4 | $\frac{dP}{dQ}$ | ã€ŒPã®Qã«é–¢ã™ã‚‹ãƒ©ãƒ‰ãƒ³ãƒ»ãƒ‹ã‚³ãƒ‡ã‚£ãƒ å°é–¢æ•°ã€ | æ¸¬åº¦ã®æ¯”ï¼ˆå¯†åº¦æ¯”ï¼‰ |
| 5 | $X_n \xrightarrow{\text{a.s.}} X$ | ã€ŒXnãŒã»ã¨ã‚“ã©ç¢ºå®Ÿã«Xã«åæŸã€ | $P(\lim X_n = X) = 1$ |
| 6 | $X_n \xrightarrow{d} X$ | ã€ŒXnãŒåˆ†å¸ƒåæŸã™ã‚‹ã€ | CDFã®å„é€£ç¶šç‚¹ã§åæŸ |
| 7 | $[W]_t = t$ | ã€ŒWã®tã¾ã§ã®äºŒæ¬¡å¤‰å‹•ã¯tã€ | Browné‹å‹•ã®ç‰¹å¾´çš„æ€§è³ª |
| 8 | $dX_t = \mu \, dt + \sigma \, dW_t$ | ã€ŒdXtã¯ãƒŸãƒ¥ãƒ¼dtãƒ—ãƒ©ã‚¹ã‚·ã‚°ãƒdWtã€ | ç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼ |
| 9 | $\mathbb{E}[X \mid \mathcal{G}]$ | ã€ŒXã®ğ’¢ã«é–¢ã™ã‚‹æ¡ä»¶ä»˜ãæœŸå¾…å€¤ã€ | Ïƒ-åŠ æ³•æ—ğ’¢å¯æ¸¬ãªæœ€è‰¯äºˆæ¸¬ |
| 10 | $\boldsymbol{\pi} P = \boldsymbol{\pi}$ | ã€Œãƒ‘ã‚¤Pã‚¤ã‚³ãƒ¼ãƒ«ãƒ‘ã‚¤ã€ | å®šå¸¸åˆ†å¸ƒã®å›ºæœ‰æ–¹ç¨‹å¼ |

:::

### 5.2 LaTeX ç­†è¨˜è©¦é¨“

ä»¥ä¸‹ã‚’æ­£ç¢ºã«LaTeXã§å†ç¾ã›ã‚ˆã€‚

| # | å•é¡Œ | æœŸå¾…å‡ºåŠ› |
|:--:|:--|:--|
| 1 | Lebesgueç©åˆ†ã®å®šç¾© | $\int_\Omega f \, d\mu = \sup\left\{\int \phi \, d\mu : \phi \leq f, \phi \text{ simple}\right\}$ |
| 2 | Radon-Nikodymå®šç† | $P(A) = \int_A \frac{dP}{dQ} \, dQ$ |
| 3 | å˜èª¿åæŸå®šç† | $0 \leq f_n \uparrow f \Rightarrow \int f_n \, d\mu \uparrow \int f \, d\mu$ |
| 4 | ItÃ´ã®å…¬å¼ï¼ˆ1æ¬¡å…ƒï¼‰ | $df = f'(X) \, dX + \frac{1}{2}f''(X) \, (dX)^2$ |
| 5 | DDPMã®é·ç§»æ ¸ | $q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\left(\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t \mathbf{I}\right)$ |

### 5.3 ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ†ã‚¹ãƒˆ

æ•°å¼ã‚’Pythonã«ç¿»è¨³ã›ã‚ˆã€‚

**å•é¡Œ 1**: Monte Carloç©åˆ† $\int_0^1 e^{-x^2} dx$ ã‚’ $N=100000$ ã‚µãƒ³ãƒ—ãƒ«ã§è¿‘ä¼¼

:::details è§£ç­”
```python
import numpy as np
np.random.seed(42)
N = 100_000
x = np.random.uniform(0, 1, N)  # U[0,1] samples
estimate = np.mean(np.exp(-x**2))
# Exact: â‰ˆ 0.7468 (error function related)
print(f"Monte Carlo estimate: {estimate:.4f}")
```
:::

**å•é¡Œ 2**: é·ç§»è¡Œåˆ— $P$ ã®å®šå¸¸åˆ†å¸ƒã‚’å›ºæœ‰å€¤åˆ†è§£ã§æ±‚ã‚ã‚‹

:::details è§£ç­”
```python
import numpy as np

P = np.array([
    [0.7, 0.2, 0.1],
    [0.3, 0.4, 0.3],
    [0.1, 0.3, 0.6]
])

# Stationary: Ï€ P = Ï€ â†” P^T Ï€^T = Ï€^T
eigenvalues, eigenvectors = np.linalg.eig(P.T)

# Find eigenvector for eigenvalue â‰ˆ 1
idx = np.argmin(np.abs(eigenvalues - 1.0))
pi = np.real(eigenvectors[:, idx])
pi = pi / pi.sum()  # normalize

print(f"Stationary distribution: {pi}")
print(f"Check Ï€ P = Ï€: {pi @ P}")
print(f"Max error: {np.max(np.abs(pi @ P - pi)):.2e}")
```
:::

**å•é¡Œ 3**: OUéç¨‹ $dX = -2X \, dt + dW$ ã®å®šå¸¸åˆ†æ•£ã‚’æ•°å€¤çš„ã«æ¨å®š

:::details è§£ç­”
```python
import numpy as np
np.random.seed(42)

theta, sigma = 2.0, 1.0
dt = 0.001
T = 50.0  # long enough for stationarity
n_steps = int(T / dt)
n_paths = 10_000

X = np.zeros(n_paths)
# Run until stationary
for _ in range(n_steps):
    dW = np.sqrt(dt) * np.random.randn(n_paths)
    X = X - theta * X * dt + sigma * dW

print(f"Theory stationary variance: ÏƒÂ²/(2Î¸) = {sigma**2/(2*theta):.4f}")
print(f"Empirical variance: {np.var(X):.4f}")
```
:::

**å•é¡Œ 4**: KDEã§ãƒãƒ³ãƒ‰å¹…ã‚’å¤‰ãˆãŸã¨ãã®$\int \hat{f}(x) dx$ã‚’ç¢ºèªï¼ˆå¸¸ã«â‰ˆ1ã«ãªã‚‹ã¹ãï¼‰

:::details è§£ç­”
```python
import numpy as np

def kde(data, x_grid, h):
    u = (x_grid[:, None] - data[None, :]) / h
    K = np.exp(-0.5 * u**2) / np.sqrt(2 * np.pi)
    return np.mean(K, axis=1) / h

np.random.seed(42)
data = np.random.randn(500)
x_grid = np.linspace(-5, 5, 1000)

for h in [0.1, 0.3, 0.5, 1.0, 2.0]:
    density = kde(data, x_grid, h)
    integral = np.trapz(density, x_grid)
    print(f"h = {h:.1f}: âˆ«fÌ‚dx = {integral:.4f}")
```
:::

**å•é¡Œ 5**: é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ $\mathbb{E}_{p}[X]$ ã‚’è¨ˆç®—ï¼ˆ$p = \mathcal{N}(5, 1)$, ææ¡ˆ $q = \mathcal{N}(0, 3)$ï¼‰

:::details è§£ç­”
```python
import numpy as np
from scipy import stats

np.random.seed(42)
p = stats.norm(5, 1)
q = stats.norm(0, 3)

N = 100_000
x = q.rvs(N)
w = p.pdf(x) / q.pdf(x)
w_normalized = w / w.sum()

estimate = np.sum(x * w_normalized)
ess = 1.0 / np.sum(w_normalized**2)

print(f"E_p[X] estimate: {estimate:.4f} (exact: 5.0)")
print(f"ESS: {ess:.0f} / {N} = {ess/N*100:.1f}%")
```
:::

### 5.4 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

#### Challenge 5.4.1: Metropolis-Hastings for 2D Gaussian

2æ¬¡å…ƒã‚¬ã‚¦ã‚¹åˆ†å¸ƒ $\mathcal{N}\left(\begin{pmatrix}1\\2\end{pmatrix}, \begin{pmatrix}1 & 0.8 \\ 0.8 & 1\end{pmatrix}\right)$ ã‹ã‚‰MHæ³•ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ç†è«–å€¤ã¨æ¯”è¼ƒã›ã‚ˆã€‚

:::details è§£ç­”
```python
import numpy as np

def mh_2d(log_target, proposal_cov, x0, n_samples, burnin=5000):
    """2D Metropolis-Hastings with multivariate Gaussian proposal."""
    x = np.array(x0, dtype=float)
    L = np.linalg.cholesky(proposal_cov)
    samples = []
    accepted = 0

    for i in range(n_samples + burnin):
        x_prop = x + L @ np.random.randn(2)
        log_alpha = log_target(x_prop) - log_target(x)

        if np.log(np.random.rand()) < log_alpha:
            x = x_prop
            if i >= burnin:
                accepted += 1
        if i >= burnin:
            samples.append(x.copy())

    return np.array(samples), accepted / n_samples

# Target: N([1, 2], [[1, 0.8], [0.8, 1]])
mu = np.array([1.0, 2.0])
Sigma = np.array([[1.0, 0.8], [0.8, 1.0]])
Sigma_inv = np.linalg.inv(Sigma)

def log_target(x):
    d = x - mu
    return -0.5 * d @ Sigma_inv @ d

np.random.seed(42)
samples, rate = mh_2d(
    log_target,
    proposal_cov=0.5 * np.eye(2),
    x0=[0.0, 0.0],
    n_samples=50_000
)

print("=== MH for 2D Gaussian ===\n")
print(f"Acceptance rate: {rate*100:.1f}%")
print(f"Mean: {np.mean(samples, axis=0)} (theory: {mu})")
print(f"Cov:\n{np.cov(samples.T).round(3)}")
print(f"Theory:\n{Sigma}")
```
:::

#### Challenge 5.4.2: Convergence Speed Comparison

$X_n = \frac{1}{n}\sum_{i=1}^{n} Z_i^2$ï¼ˆ$Z_i \sim \mathcal{N}(0,1)$ï¼‰ã«ã¤ã„ã¦ã€4ã¤ã®åæŸãƒ¢ãƒ¼ãƒ‰ã®åæŸé€Ÿåº¦ã‚’æ¯”è¼ƒã›ã‚ˆã€‚

:::details è§£ç­”
```python
import numpy as np

np.random.seed(42)
n_paths = 10_000
N_max = 10_000

# Generate Z_i ~ N(0,1) for all paths
Z = np.random.randn(n_paths, N_max)
Z_sq = Z**2

print("=== Convergence Speed Comparison ===\n")
print("X_n = (1/n) Î£ Z_iÂ² â†’ 1 (by LLN)\n")

print(f"{'n':>8} {'|E[Xn]-1|':>12} {'P(|Xn-1|>Îµ)':>14} {'E[|Xn-1|Â²]':>14}")

eps = 0.1
for n in [10, 100, 1000, 10000]:
    Xn = np.cumsum(Z_sq[:, :n], axis=1)[:, -1] / n

    # Convergence in mean: |E[Xn] - 1|
    mean_err = abs(np.mean(Xn) - 1)

    # Convergence in probability: P(|Xn - 1| > Îµ)
    prob_err = np.mean(np.abs(Xn - 1) > eps)

    # Convergence in LÂ²: E[|Xn - 1|Â²]
    l2_err = np.mean((Xn - 1)**2)

    print(f"{n:>8} {mean_err:>12.6f} {prob_err:>14.4f} {l2_err:>14.6f}")

print(f"\nTheory: Var[Xn] = Var[ZÂ²]/n = 2/n")
for n in [10, 100, 1000, 10000]:
    print(f"  n={n}: theory Var = {2/n:.6f}")
```
:::

#### Challenge 5.4.3: Mixing Time of Random Walk on Cycle

$n$-é ‚ç‚¹ã‚µã‚¤ã‚¯ãƒ«ã‚°ãƒ©ãƒ•ä¸Šã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®mixing timeã‚’æ¨å®šã›ã‚ˆã€‚ç†è«–å€¤ $t_{\text{mix}} = \Theta(n^2)$ ã¨æ¯”è¼ƒã€‚

:::details è§£ç­”
```python
import numpy as np

def random_walk_cycle_mixing(n_vertices, n_steps, n_walks):
    """Random walk on cycle graph C_n.

    Uniform stationary distribution: Ï€(i) = 1/n
    Mixing time: Î˜(nÂ²)
    """
    positions = np.zeros(n_walks, dtype=int)

    # Track TV distance over time
    tv_distances = []
    uniform = np.ones(n_vertices) / n_vertices

    for step in range(n_steps):
        moves = np.random.choice([-1, 1], size=n_walks)
        positions = (positions + moves) % n_vertices

        # Empirical distribution
        counts = np.bincount(positions, minlength=n_vertices)
        empirical = counts / n_walks
        tv = 0.5 * np.sum(np.abs(empirical - uniform))
        tv_distances.append(tv)

    return tv_distances

print("=== Mixing Time on Cycle Graph ===\n")
print(f"{'n':>6} {'t_mix(emp)':>12} {'nÂ²':>8} {'ratio':>8}")

for n in [10, 20, 50, 100]:
    tv = random_walk_cycle_mixing(n, n_steps=n**2 * 3, n_walks=100_000)

    # Find first time TV < 1/(4) (standard mixing time threshold)
    t_mix = next((t for t, d in enumerate(tv) if d < 0.25), len(tv))

    print(f"{n:>6} {t_mix:>12} {n**2:>8} {t_mix/n**2:>8.2f}")
```
:::

#### Challenge 5.4.4: SDE Solver â€” Strong vs Weak Convergence

Euler-Maruyamaæ³•ã®å¼·åæŸï¼ˆãƒ‘ã‚¹ãƒ¯ã‚¤ã‚ºï¼‰ã¨å¼±åæŸï¼ˆåˆ†å¸ƒã®æ€§è³ªï¼‰ã‚’æ¯”è¼ƒã›ã‚ˆã€‚

:::details è§£ç­”
```python
import numpy as np

def sde_strong_weak_convergence(n_paths=10_000):
    """Compare strong and weak convergence of Euler-Maruyama.

    SDE: dX = -X dt + dW, X(0) = 1
    Exact: X(t) = e^{-t} + âˆ«â‚€áµ— e^{-(t-s)} dW(s)
    E[X(T)] = e^{-T}
    Var[X(T)] = (1 - e^{-2T})/2
    """
    T = 1.0
    x0 = 1.0
    theta = 1.0
    sigma = 1.0

    # Fine grid (reference solution)
    n_fine = 2**14
    dt_fine = T / n_fine
    dW_fine = np.sqrt(dt_fine) * np.random.randn(n_paths, n_fine)

    # Exact solution via fine Euler-Maruyama
    X_ref = np.full(n_paths, x0)
    for i in range(n_fine):
        X_ref = X_ref - theta * X_ref * dt_fine + sigma * dW_fine[:, i]

    print("=== SDE Convergence Analysis ===\n")
    print(f"{'dt':>10} {'Strong err':>12} {'Weak err':>12} {'Strong O':>10} {'Weak O':>10}")

    prev_strong = None
    prev_weak = None

    for power in [6, 8, 10, 12]:
        n_steps = 2**power
        dt = T / n_steps
        ratio = n_fine // n_steps

        X = np.full(n_paths, x0)
        for i in range(n_steps):
            # Sum fine increments to match coarse step
            dW = np.sum(dW_fine[:, i*ratio:(i+1)*ratio], axis=1)
            X = X - theta * X * dt + sigma * dW

        strong_err = np.mean(np.abs(X - X_ref))
        weak_err = abs(np.mean(X) - np.mean(X_ref))

        s_order = ""
        w_order = ""
        if prev_strong is not None:
            s_order = f"{np.log2(prev_strong/strong_err):.2f}"
            w_order = f"{np.log2(prev_weak/max(weak_err, 1e-15)):.2f}"

        print(f"{dt:>10.6f} {strong_err:>12.6f} {weak_err:>12.6f} "
              f"{s_order:>10} {w_order:>10}")

        prev_strong = strong_err
        prev_weak = weak_err

    # Theory
    E_theory = x0 * np.exp(-theta * T)
    V_theory = sigma**2 / (2*theta) * (1 - np.exp(-2*theta*T))
    print(f"\nE[X(T)] theory: {E_theory:.4f}, empirical: {np.mean(X_ref):.4f}")
    print(f"Var[X(T)] theory: {V_theory:.4f}, empirical: {np.var(X_ref):.4f}")

np.random.seed(42)
sde_strong_weak_convergence()
```
:::

> **åæŸæ¬¡æ•°**: Euler-Maruyamaã¯å¼·åæŸ $O(\sqrt{\Delta t})$ã€å¼±åæŸ $O(\Delta t)$ã€‚å¼±åæŸãŒé€Ÿã„ â€” åˆ†å¸ƒã®æ€§è³ªï¼ˆå¹³å‡ãƒ»åˆ†æ•£ï¼‰ã ã‘ãŒå¿…è¦ãªã‚‰ç²—ã„åˆ»ã¿ã§ååˆ†ã€‚

#### Challenge 5.4.5: Langevin Dynamics vs MH â€” ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç²¾åº¦æ¯”è¼ƒ

åŒã˜ç›®æ¨™åˆ†å¸ƒï¼ˆ2æ¬¡å…ƒãƒãƒŠãƒŠåˆ†å¸ƒï¼‰ã«å¯¾ã—ã¦ã€Langevin Dynamicsã¨Metropolis-Hastingsã®åŠ¹ç‡ã‚’æ¯”è¼ƒã›ã‚ˆã€‚

:::details è§£ç­”
```python
import numpy as np

def banana_log_density(x, b=0.1):
    """Log density of banana-shaped distribution.

    log p(x1, x2) = -0.5 * (x1Â² + (x2 - b*x1Â²)Â²)
    """
    return -0.5 * (x[0]**2 + (x[1] - b * x[0]**2)**2)

def banana_score(x, b=0.1):
    """Score âˆ‡log p(x) of banana distribution."""
    grad = np.zeros(2)
    grad[0] = -x[0] + 2*b*x[0]*(x[1] - b*x[0]**2)
    grad[1] = -(x[1] - b*x[0]**2)
    return grad

def langevin_2d(score_fn, x0, eps, n_samples, burnin=5000):
    x = np.array(x0, dtype=float)
    samples = []
    for k in range(n_samples + burnin):
        x = x + 0.5*eps*score_fn(x) + np.sqrt(eps)*np.random.randn(2)
        if k >= burnin:
            samples.append(x.copy())
    return np.array(samples)

def mh_2d_generic(log_target, proposal_std, x0, n_samples, burnin=5000):
    x = np.array(x0, dtype=float)
    samples = []
    accepted = 0
    for k in range(n_samples + burnin):
        x_prop = x + proposal_std * np.random.randn(2)
        if np.log(np.random.rand()) < log_target(x_prop) - log_target(x):
            x = x_prop
            if k >= burnin:
                accepted += 1
        if k >= burnin:
            samples.append(x.copy())
    return np.array(samples), accepted / n_samples

np.random.seed(42)
N = 50_000

# Langevin
lang_samples = langevin_2d(banana_score, [0,0], eps=0.05, n_samples=N)

# MH
mh_samples, mh_rate = mh_2d_generic(banana_log_density, 1.0, [0,0], n_samples=N)

print("=== Banana Distribution Sampling ===\n")
print(f"{'Method':>15} {'E[x1]':>8} {'E[x2]':>8} {'Var[x1]':>8} {'Var[x2]':>8}")
print(f"{'Langevin':>15} {np.mean(lang_samples[:,0]):>8.3f} "
      f"{np.mean(lang_samples[:,1]):>8.3f} "
      f"{np.var(lang_samples[:,0]):>8.3f} "
      f"{np.var(lang_samples[:,1]):>8.3f}")
print(f"{'MH':>15} {np.mean(mh_samples[:,0]):>8.3f} "
      f"{np.mean(mh_samples[:,1]):>8.3f} "
      f"{np.var(mh_samples[:,0]):>8.3f} "
      f"{np.var(mh_samples[:,1]):>8.3f}")
print(f"\nMH acceptance rate: {mh_rate*100:.1f}%")
```
:::

#### Challenge 5.4.6: Pushforward Measure ã®æ¤œè¨¼

å¤‰æ› $Y = X^2$ ã«ã¤ã„ã¦ã€$X \sim \mathcal{N}(0,1)$ ã®ã¨ã $Y$ ãŒ $\chi^2(1)$ åˆ†å¸ƒã«å¾“ã†ã“ã¨ã‚’æ•°å€¤çš„ãƒ»è§£æçš„ã«æ¤œè¨¼ã›ã‚ˆã€‚

:::details è§£ç­”
```python
import numpy as np
from scipy import stats

np.random.seed(42)
N = 500_000

# Pushforward: Y = T(X) = XÂ², X ~ N(0,1)
X = np.random.randn(N)
Y = X**2  # Y ~ Ï‡Â²(1) by pushforward

# Compare with scipy chi-squared
chi2_theoretical = stats.chi2(df=1)

print("=== Pushforward Measure: Y = XÂ² ===\n")
print("X ~ N(0,1), T(x) = xÂ², T#P should be Ï‡Â²(1)\n")

# Moments comparison
print(f"{'Moment':>10} {'Empirical':>12} {'Theory Ï‡Â²(1)':>14}")
print(f"{'E[Y]':>10} {np.mean(Y):>12.4f} {chi2_theoretical.mean():>14.4f}")
print(f"{'Var[Y]':>10} {np.var(Y):>12.4f} {chi2_theoretical.var():>14.4f}")
print(f"{'E[YÂ²]':>10} {np.mean(Y**2):>12.4f} "
      f"{chi2_theoretical.moment(2):>14.4f}")

# KS test
ks_stat, ks_pval = stats.kstest(Y, 'chi2', args=(1,))
print(f"\nKS test: statistic={ks_stat:.4f}, p-value={ks_pval:.4f}")
print(f"p > 0.05 â†’ cannot reject Hâ‚€ (Y ~ Ï‡Â²(1)) âœ“")

# Change of variables formula verification
# p_Y(y) = p_X(âˆšy) / (2âˆšy) + p_X(-âˆšy) / (2âˆšy) for y > 0
y_grid = np.linspace(0.01, 8, 200)
pdf_theory = chi2_theoretical.pdf(y_grid)

# From change of variables
pdf_cov = (stats.norm.pdf(np.sqrt(y_grid)) + stats.norm.pdf(-np.sqrt(y_grid))) / (2 * np.sqrt(y_grid))

print(f"\nChange of variables formula max error: "
      f"{np.max(np.abs(pdf_theory - pdf_cov)):.2e}")
```
:::

#### Challenge 5.4.7: DCT (Dominated Convergence) ã®æ•°å€¤æ¤œè¨¼

$f_n(x) = n \cdot x \cdot e^{-nx^2}$ ã¯ $f_n \to 0$ (pointwise) ã ãŒ $\int f_n = 1/2$ (å®šæ•°)ã€‚ã“ã‚Œã¯DCTã®ä»®å®šãŒæº€ãŸã•ã‚Œãªã„ã‚±ãƒ¼ã‚¹ã€‚ä¸€æ–¹ã€$g_n(x) = (1+x/n)^{-n}$ ã¯ $g_n \to e^{-x}$ ã§DCTãŒæˆç«‹ã™ã‚‹ã€‚ä¸¡æ–¹ã‚’æ¤œè¨¼ã›ã‚ˆã€‚

:::details è§£ç­”
```python
import numpy as np

def dct_verification():
    """Verify Dominated Convergence Theorem numerically.

    f_n(x) = n * x * exp(-n * xÂ²)
    lim f_n(x) = 0 for all x
    âˆ« f_n dx should â†’ âˆ« 0 dx = 0

    Dominating function: g(x) = |x| * exp(-xÂ²) / (2e)
    (since max_n n*exp(-nxÂ²) = 1/(exÂ²) for xâ‰ 0, bounded)
    """
    x = np.linspace(-5, 5, 10_000)
    dx = x[1] - x[0]

    print("=== Dominated Convergence Theorem ===\n")
    print(f"f_n(x) = n Â· x Â· exp(-n xÂ²)\n")
    print(f"{'n':>6} {'âˆ«f_n dx':>12} {'max|f_n|':>12} {'âˆ«|f_n| dx':>12}")

    for n in [1, 5, 10, 50, 100, 500, 1000]:
        fn = n * x * np.exp(-n * x**2)
        integral = np.trapz(fn, x)
        max_abs = np.max(np.abs(fn))
        abs_integral = np.trapz(np.abs(fn), x)
        print(f"{n:>6} {integral:>12.6f} {max_abs:>12.4f} {abs_integral:>12.6f}")

    print(f"\nlim âˆ«f_n dx = 0 (by DCT)")
    print(f"âˆ«(lim f_n) dx = âˆ« 0 dx = 0 âœ“")

    # Verify domination
    print(f"\nDominating function verification:")
    g = 1.0 / (np.sqrt(2 * np.e) * (np.abs(x) + 0.01))  # avoid /0
    for n in [1, 10, 100]:
        fn_abs = np.abs(n * x * np.exp(-n * x**2))
        dominated = np.all(fn_abs <= g + 1e-10)
        print(f"  n={n}: |f_n| â‰¤ g everywhere? {dominated}")

dct_verification()
```
:::

### 5.5 ã‚»ãƒ«ãƒ•ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

ä»¥ä¸‹ã®è³ªå•ã«ã€Œã¯ã„ã€ã¨ç­”ãˆã‚‰ã‚Œã‚‹ã‹ç¢ºèªã—ã‚ˆã†ã€‚

- [ ] æ¸¬åº¦ç©ºé–“ $(X, \mathcal{F}, \mu)$ ã®ä¸‰ã¤çµ„ã®å„è¦ç´ ã®å½¹å‰²ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Riemannç©åˆ†ãŒå¤±æ•—ã—ã¦Lebesgueç©åˆ†ãŒæˆåŠŸã™ã‚‹ä¾‹ã‚’æŒ™ã’ã‚‰ã‚Œã‚‹
- [ ] Radon-Nikodymå°é–¢æ•°ãŒã€Œç¢ºç‡å¯†åº¦é–¢æ•°ã®å³å¯†ãªå®šç¾©ã€ã§ã‚ã‚‹ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] 4ã¤ã®åæŸãƒ¢ãƒ¼ãƒ‰ï¼ˆa.s., probability, $L^p$, distributionï¼‰ã®åŒ…å«é–¢ä¿‚ã‚’æã‘ã‚‹
- [ ] æ¡ä»¶ä»˜ãæœŸå¾…å€¤ãŒã€Œ$\sigma$-åŠ æ³•æ—$\mathcal{G}$å¯æ¸¬ãªé–¢æ•°ã€ã§ã‚ã‚‹ã“ã¨ã®æ„å‘³ãŒã‚ã‹ã‚‹
- [ ] Markové€£é–ã®ã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰å®šç†ã‚’è¿°ã¹ã€æ•°å€¤çš„ã«ç¢ºèªã§ãã‚‹
- [ ] Browné‹å‹•ã®5ã¤ã®å®šç¾©æ¡ä»¶ã‚’ã‚³ãƒ¼ãƒ‰ã§æ¤œè¨¼ã§ãã‚‹
- [ ] äºŒæ¬¡å¤‰å‹• $[W]_T = T$ ãŒä¼Šè—¤ç©åˆ†ã®ç‰¹æ®Šæ€§ã‚’ç”Ÿã‚€ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] ItÃ´ã®å…¬å¼ã® $\frac{1}{2}f''(X)(dX)^2$ é …ãŒãªãœæ¶ˆãˆãªã„ã‹ã‚ã‹ã‚‹
- [ ] DDPMã®forward processã‚’Markové€£é– + é·ç§»æ ¸ + Radon-Nikodymå°é–¢æ•°ã§è¨˜è¿°ã§ãã‚‹
- [ ] Monte Carloç©åˆ†ã®åæŸãƒ¬ãƒ¼ãƒˆ $O(1/\sqrt{N})$ ã‚’å®Ÿé¨“ã§ç¢ºèªã§ãã‚‹
- [ ] é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒã€ŒRadon-Nikodymå°é–¢æ•°ã«ã‚ˆã‚‹æ¸¬åº¦å¤‰æ›ã€ã§ã‚ã‚‹ã“ã¨ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Metropolis-Hastingsã®è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã®æ„å‘³ãŒã‚ã‹ã‚‹
- [ ] OUéç¨‹ãŒDDPMã®é€£ç¶šæ¥µé™ã§ã‚ã‚‹ã“ã¨ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Euler-Maruyamaã®å¼·åæŸã¨å¼±åæŸã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹

:::message
**é€²æ—: 85% å®Œäº†** ã‚·ãƒ³ãƒœãƒ«ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°10å•ã€LaTeX 5å•ã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³5å•ã€å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸5å•ã‚’å®Œäº†ã€‚Zone 5 ã‚¯ãƒªã‚¢ã€‚
:::

---

## ğŸš€ 6. æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Š

### 6.1 Fokker-Planckæ–¹ç¨‹å¼ã®ç›´æ„Ÿ â€” SDEã‹ã‚‰ç¢ºç‡å¯†åº¦ã®PDEã¸

SDEã¯**å€‹ã€…ã®ãƒ‘ã‚¹**ï¼ˆã‚µãƒ³ãƒ—ãƒ«è»Œé“ï¼‰ã‚’è¨˜è¿°ã™ã‚‹ã€‚ã ãŒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æœ¬è³ªçš„ãªå•ã„ã¯ã€Œç¢ºç‡å¯†åº¦ $p(x, t)$ ãŒæ™‚é–“ã¨ã¨ã‚‚ã«ã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã€ã ã€‚Fokker-Planckæ–¹ç¨‹å¼ï¼ˆKolmogorovå‰å‘ãæ–¹ç¨‹å¼ï¼‰ã¯ã€SDEã‚’ãƒ‘ã‚¹ã®é›†å›£ï¼ˆç¢ºç‡å¯†åº¦ï¼‰ã®è¨€è‘‰ã«ç¿»è¨³ã™ã‚‹ã€‚

#### SDEã‹ã‚‰Fokker-Planckã¸ã®å¯¾å¿œ

SDE:
$$
dX_t = f(X_t) \, dt + g(X_t) \, dW_t
$$

ã«å¯¾å¿œã™ã‚‹Fokker-Planckæ–¹ç¨‹å¼ (FPE):

$$
\frac{\partial p}{\partial t}(x, t) = -\frac{\partial}{\partial x}\big[f(x) \, p(x, t)\big] + \frac{1}{2}\frac{\partial^2}{\partial x^2}\big[g^2(x) \, p(x, t)\big]
$$

- ç¬¬1é …: $-\partial_x(fp)$ â€” **ãƒ‰ãƒªãƒ•ãƒˆé …**ï¼ˆç¢ºç‡ã®æµã‚Œï¼‰
- ç¬¬2é …: $\frac{1}{2}\partial_x^2(g^2 p)$ â€” **æ‹¡æ•£é …**ï¼ˆç¢ºç‡ã®åºƒãŒã‚Šï¼‰

> **ä¸€è¨€ã§è¨€ãˆã°**: SDEãŒã€Œ1ã¤ã®ç²’å­ãŒã©ã†å‹•ãã‹ã€ã‚’è¨˜è¿°ã™ã‚‹ã®ã«å¯¾ã—ã€Fokker-Planckæ–¹ç¨‹å¼ã¯ã€Œç²’å­ã®é›²ï¼ˆç¢ºç‡å¯†åº¦ï¼‰ãŒã©ã†å¤‰å½¢ã™ã‚‹ã‹ã€ã‚’è¨˜è¿°ã™ã‚‹ã€‚

#### å°å‡ºã®ç›´æ„Ÿï¼ˆå¤šæ¬¡å…ƒã¯ç¬¬30å›ï¼‰

ç¢ºç‡ã®ä¿å­˜å‰‡ï¼ˆé€£ç¶šã®æ–¹ç¨‹å¼ï¼‰ã‹ã‚‰å‡ºç™ºã™ã‚‹ã€‚$J(x, t)$ ã‚’ç¢ºç‡ãƒ•ãƒ©ãƒƒã‚¯ã‚¹ï¼ˆç¢ºç‡ã®æµã‚Œï¼‰ã¨ã™ã‚‹ã¨:

$$
\frac{\partial p}{\partial t} = -\frac{\partial J}{\partial x}
$$

ItÃ´ã®å…¬å¼ã‹ã‚‰ã€ãƒ•ãƒ©ãƒƒã‚¯ã‚¹ã¯:

$$
J(x, t) = f(x) p(x, t) - \frac{1}{2}\frac{\partial}{\partial x}\big[g^2(x) p(x, t)\big]
$$

ãƒ‰ãƒªãƒ•ãƒˆã«ã‚ˆã‚‹æµã‚Œ $fp$ ã¨ã€æ‹¡æ•£ã«ã‚ˆã‚‹åºƒãŒã‚Š $-\frac{1}{2}\partial_x(g^2 p)$ ã®å’Œã€‚ã“ã‚Œã‚’é€£ç¶šã®æ–¹ç¨‹å¼ã«ä»£å…¥ã™ã‚‹ã¨FPEãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

#### OUéç¨‹ã®å ´åˆ

$dX_t = -\theta X_t \, dt + \sigma \, dW_t$ ã®FPE:

$$
\frac{\partial p}{\partial t} = \theta \frac{\partial}{\partial x}(x \, p) + \frac{\sigma^2}{2}\frac{\partial^2 p}{\partial x^2}
$$

å®šå¸¸è§£ $\partial p / \partial t = 0$ ã‚’æ±‚ã‚ã‚‹ã¨:

$$
p_\infty(x) = \mathcal{N}\left(0, \frac{\sigma^2}{2\theta}\right)
$$

Zone 4.9 ã§æ•°å€¤çš„ã«ç¢ºèªã—ãŸOUéç¨‹ã®å®šå¸¸åˆ†å¸ƒãŒã€FPEã®å®šå¸¸è§£ã¨ã—ã¦å³å¯†ã«å°ã‹ã‚Œã‚‹ã€‚

```python
import numpy as np

def fokker_planck_demo():
    """Compare SDE histogram evolution with Fokker-Planck solution.

    OU process: dX = -Î¸X dt + Ïƒ dW
    FPE solution at time t:
      p(x,t) = N(xâ‚€ e^{-Î¸t}, ÏƒÂ²(1-e^{-2Î¸t})/(2Î¸))
    """
    theta, sigma, x0 = 2.0, 1.0, 3.0
    T = 2.0
    n_steps = 2000
    n_paths = 100000
    dt = T / n_steps

    # Simulate SDE (Euler-Maruyama)
    X = np.full(n_paths, x0)
    check_times = [0.1, 0.5, 1.0, 2.0]
    snapshots = {}

    step = 0
    for i in range(n_steps):
        t = (i + 1) * dt
        dW = np.sqrt(dt) * np.random.randn(n_paths)
        X = X - theta * X * dt + sigma * dW
        if any(abs(t - tc) < dt/2 for tc in check_times):
            snapshots[round(t, 1)] = X.copy()

    # Compare with FPE analytical solution
    print("=== Fokker-Planck vs SDE Simulation ===\n")
    print(f"OU process: dX = -{theta}X dt + {sigma} dW,  xâ‚€ = {x0}\n")
    print(f"{'t':>5} {'SDE mean':>10} {'FPE mean':>10} {'SDE var':>10} {'FPE var':>10}")
    print("-" * 50)

    for t in check_times:
        if t not in snapshots:
            continue
        sde_data = snapshots[t]
        fpe_mean = x0 * np.exp(-theta * t)
        fpe_var = sigma**2 / (2 * theta) * (1 - np.exp(-2 * theta * t))
        print(f"{t:>5.1f} {sde_data.mean():>10.4f} {fpe_mean:>10.4f} "
              f"{sde_data.var():>10.4f} {fpe_var:>10.4f}")

    stat_var = sigma**2 / (2 * theta)
    print(f"\nStationary: N(0, {stat_var:.4f})")
    print(f"SDE at T={T}: mean={snapshots[2.0].mean():.4f}, var={snapshots[2.0].var():.4f}")

    # Connection to diffusion models
    print(f"\n=== Diffusion Model Connection ===")
    print(f"Forward SDE:  dX = -Â½Î²(t)X dt + âˆšÎ²(t) dW")
    print(f"Fokker-Planck: âˆ‚p/âˆ‚t = Â½Î²(t)âˆ‚(xp)/âˆ‚x + Â½Î²(t)âˆ‚Â²p/âˆ‚xÂ²")
    print(f"â†’ Density evolves from p_data to N(0,I)")
    print(f"")
    print(f"Reverse SDE (Anderson 1982 [^9]):")
    print(f"  dX = [-Â½Î²(t)X - Î²(t)âˆ‡log p_t(X)] dt + âˆšÎ²(t) dWÌ„")
    print(f"â†’ Score âˆ‡log p_t connects FPE solution to reverse dynamics")
    print(f"â†’ ç¬¬30å›ã§ Fokker-Planck ã®å®Œå…¨å°å‡ºã¨ reverse SDE ã‚’æ‰±ã†")

fokker_planck_demo()
```

#### SDE â†” Fokker-Planck â†” Score SDE ã®ä¸‰è§’é–¢ä¿‚

```mermaid
graph TD
    SDE["SDE<br/>dX = f dt + g dW<br/>ãƒ‘ã‚¹ã®è¨˜è¿°"] -->|ItÃ´'s formula| FPE["Fokker-Planck<br/>âˆ‚p/âˆ‚t = -âˆ‚(fp) + Â½âˆ‚Â²(gÂ²p)<br/>å¯†åº¦ã®æ™‚é–“ç™ºå±•"]
    FPE -->|å®šå¸¸è§£ âˆ‚p/âˆ‚t=0| STAT["å®šå¸¸åˆ†å¸ƒ<br/>pâˆ(x)"]
    SDE -->|Anderson 1982| REV["Reverse SDE<br/>dX = [f - gÂ²âˆ‡log p]dt + g dWÌ„"]
    FPE -->|âˆ‡log p_t| SCORE["Score function<br/>âˆ‡ log p_t(x)"]
    SCORE --> REV
    REV -->|generative model| GEN["Score SDE<br/>Song+ 2020"]

    style SDE fill:#e3f2fd
    style FPE fill:#fff9c4
    style GEN fill:#c8e6c9
```

| è¦–ç‚¹ | è¨˜è¿°å¯¾è±¡ | æ•°å­¦çš„å¯¾è±¡ | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã®å½¹å‰² |
|:-----|:--------|:---------|:---------------|
| SDE | 1ã¤ã®ãƒ‘ã‚¹ | $X_t(\omega)$ | Forward/Reverse process |
| Fokker-Planck | ç¢ºç‡å¯†åº¦ã®æ™‚é–“ç™ºå±• | $p(x, t)$ | ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆ |
| Score function | å¯†åº¦ã®å‹¾é… | $\nabla \log p_t$ | NN ã§å­¦ç¿’ã™ã‚‹å¯¾è±¡ |

:::message
**ç¬¬30å›ã¸ã®äºˆå‘Š**: ã“ã“ã§ã¯1æ¬¡å…ƒãƒ»OUéç¨‹ã®å ´åˆã®Fokker-Planckã‚’å‘³è¦‹ã—ãŸã€‚ç¬¬30å›ã€ŒDiffusion Models IIã€ã§ã¯ã€å¤šæ¬¡å…ƒFPE ã®å®Œå…¨å°å‡ºã€reverse SDE ã®å³å¯†è¨¼æ˜ï¼ˆGirsanovå¤‰æ›ï¼‰ã€ãã—ã¦FPEã‹ã‚‰Score SDEã®å­¦ç¿’ç›®çš„é–¢æ•°ï¼ˆdenoising score matchingï¼‰ã‚’å°ãã€‚Fokker-Planckã¯æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ç†è«–ã®ã€Œè£ãƒœã‚¹ã€ã ã€‚
:::

### 6.2 ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ¸¬åº¦è«–çš„çµ±ä¸€

```mermaid
graph TD
    A["æ¸¬åº¦è¼¸é€<br/>T#pâ‚€ = pâ‚"] --> B["Normalizing Flows<br/>å¯é€†å¤‰æ› T"]
    A --> C["VAE<br/>æ½œåœ¨ç©ºé–“ã®æ¸¬åº¦"]
    A --> D["Diffusion<br/>SDE forward/reverse"]
    A --> E["Flow Matching<br/>ç¢ºç‡ãƒ‘ã‚¹ p_t"]

    D --> F["Score SDE<br/>âˆ‡log p_t"]
    E --> G["Rectified Flow<br/>ç›´ç·šåŒ–ãƒ‘ã‚¹"]
    E --> H["Stochastic Interpolants<br/>ä¸€èˆ¬åŒ–è£œé–“"]

    I["Radon-Nikodym<br/>dP/dQ"] -.-> D
    I -.-> F
    J["Pushforward<br/>T#Î¼"] -.-> B
    J -.-> E
    K["Markov Chain<br/>é·ç§»æ ¸"] -.-> D
```

> ã™ã¹ã¦ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€**ã‚½ãƒ¼ã‚¹æ¸¬åº¦ $p_0$ï¼ˆé€šå¸¸ã¯ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºï¼‰ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¸¬åº¦ $p_1$ï¼ˆãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒï¼‰ã«è¼¸é€ã™ã‚‹å†™åƒ**ã¨ã—ã¦çµ±ä¸€çš„ã«ç†è§£ã§ãã‚‹ã€‚æ¸¬åº¦è«–ã¯ã“ã®çµ±ä¸€çš„è¦–ç‚¹ã‚’ä¸ãˆã‚‹è¨€èªã§ã‚ã‚‹ã€‚

### 6.3 æ¨è–¦æ–‡çŒ®

| ãƒ¬ãƒ™ãƒ« | ã‚¿ã‚¤ãƒˆãƒ« | è‘—è€… | ãªãœèª­ã‚€ã¹ãã‹ |
|:--:|:--|:--|:--|
| â˜…â˜…â˜† | Probability and Measure | Billingsley | æ¸¬åº¦è«–çš„ç¢ºç‡è«–ã®å®šç•ªæ•™ç§‘æ›¸ |
| â˜…â˜…â˜† | Markov Chains and Mixing Times | Levin & Peres [^3] | Markové€£é–ã®ç†è«–ã¨å¿œç”¨ |
| â˜…â˜…â˜… | Stochastic Differential Equations | Ã˜ksendal | ItÃ´ç©åˆ†ãƒ»SDEã®æ¨™æº–æ•™ç§‘æ›¸ |
| â˜…â˜…â˜† | An Introduction to MCMC | Brooks et al. | MCMCæ³•ã®åŒ…æ‹¬çš„ãƒ¬ãƒ“ãƒ¥ãƒ¼ |
| â˜…â˜…â˜… | Score-Based Generative Modeling | Song et al. [^2] | SDEã«ã‚ˆã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€ç†è«– |
| â˜…â˜…â˜† | Flow Matching | Lipman et al. [^7] | æœ€æ–°ã®Flow Matchingç†è«– |
| â˜…â˜…â˜† | Rectified Flow | Liu et al. [^6] | ç›´ç·šåŒ–ãƒ‘ã‚¹ã«ã‚ˆã‚‹é«˜é€Ÿç”Ÿæˆ |
| â˜…â˜†â˜† | Pattern Recognition and ML | Bishop | ãƒ™ã‚¤ã‚ºæ¨è«–ã¨ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ« |

### 6.4 ç”¨èªé›† (Glossary)

| è‹±èª | æ—¥æœ¬èª | å®šç¾© |
|:--|:--|:--|
| Measure space | æ¸¬åº¦ç©ºé–“ | $(X, \mathcal{F}, \mu)$ â€” é›†åˆãƒ»Ïƒ-åŠ æ³•æ—ãƒ»æ¸¬åº¦ã®ä¸‰ã¤çµ„ |
| Ïƒ-algebra | Ïƒ-åŠ æ³•æ— | è£œé›†åˆã¨å¯ç®—åˆä½µã§é–‰ã˜ãŸé›†åˆæ— |
| Lebesgue measure | Lebesgueæ¸¬åº¦ | $\mathbb{R}^n$ ä¸Šã®ã€Œä½“ç©ã€ã®ä¸€èˆ¬åŒ– |
| Lebesgue integral | Lebesgueç©åˆ† | å€¤åŸŸå´ã‹ã‚‰åˆ†å‰²ã™ã‚‹ç©åˆ† |
| Radon-Nikodym derivative | Radon-Nikodymå°é–¢æ•° | æ¸¬åº¦ã®æ¯” $dP/dQ$ â€” PDFã®å³å¯†ãªå®šç¾© |
| Absolute continuity | çµ¶å¯¾é€£ç¶šæ€§ | $P \ll Q$: $Q(A)=0 \Rightarrow P(A)=0$ |
| Pushforward measure | æŠ¼ã—å‡ºã—æ¸¬åº¦ | $T_\# \mu(A) = \mu(T^{-1}(A))$ |
| Convergence a.s. | æ¦‚åæŸ | $P(\lim X_n = X) = 1$ |
| Convergence in probability | ç¢ºç‡åæŸ | $P(\|X_n - X\| > \epsilon) \to 0$ |
| Convergence in distribution | åˆ†å¸ƒåæŸ | CDFã®å„é€£ç¶šç‚¹ã§åæŸ |
| Conditional expectation | æ¡ä»¶ä»˜ãæœŸå¾…å€¤ | $\sigma$-åŠ æ³•æ—å¯æ¸¬ãªæœ€è‰¯è¿‘ä¼¼ |
| Markov chain | Markové€£é– | æ¬¡çŠ¶æ…‹ãŒç¾çŠ¶æ…‹ã®ã¿ã«ä¾å­˜ã™ã‚‹ç¢ºç‡éç¨‹ |
| Stationary distribution | å®šå¸¸åˆ†å¸ƒ | $\boldsymbol{\pi} P = \boldsymbol{\pi}$ ã‚’æº€ãŸã™åˆ†å¸ƒ |
| Ergodic theorem | ã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰å®šç† | æ™‚é–“å¹³å‡ = ç©ºé–“å¹³å‡ |
| Mixing time | æ··åˆæ™‚é–“ | å®šå¸¸åˆ†å¸ƒã¸ã®åæŸã«å¿…è¦ãªã‚¹ãƒ†ãƒƒãƒ—æ•° |
| Brownian motion | Browné‹å‹• | é€£ç¶šãƒ‘ã‚¹ãƒ»ç‹¬ç«‹å¢—åˆ†ãƒ»ã‚¬ã‚¦ã‚¹å¢—åˆ†ã®ç¢ºç‡éç¨‹ |
| Quadratic variation | äºŒæ¬¡å¤‰å‹• | $[W]_T = T$ â€” Browné‹å‹•ã®è’ã•ã®æ¸¬åº¦ |
| ItÃ´'s formula | ä¼Šè—¤ã®å…¬å¼ | ç¢ºç‡éç¨‹ã®é€£é–å¾‹ï¼ˆ$\frac{1}{2}f''(dX)^2$ é …ã‚’å«ã‚€ï¼‰ |
| SDE | ç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼ | $dX = \mu \, dt + \sigma \, dW$ |
| Euler-Maruyama | Euler-Maruyamaæ³• | SDEã®åŸºæœ¬çš„æ•°å€¤è§£æ³• |
| Score function | ã‚¹ã‚³ã‚¢é–¢æ•° | $\nabla_x \log p(x)$ â€” å¯¾æ•°å¯†åº¦ã®å‹¾é… |
| Importance sampling | é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | ææ¡ˆåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—é‡ã¿ã§è£œæ­£ |
| MCMC | ãƒãƒ«ã‚³ãƒ•é€£é–ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³• | Markové€£é–ã§ç›®æ¨™åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| Detailed balance | è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ | $\pi(x)P(x \to y) = \pi(y)P(y \to x)$ |
| Flow Matching | ãƒ•ãƒ­ãƒ¼ãƒãƒƒãƒãƒ³ã‚° | ç¢ºç‡ãƒ‘ã‚¹ã«æ²¿ã†é€Ÿåº¦å ´ã‚’å­¦ç¿’ |
| Rectified Flow | Rectified Flow | ãƒ‘ã‚¹ã®ç›´ç·šåŒ–ã«ã‚ˆã‚‹é«˜é€Ÿç”Ÿæˆ |

### 6.5 ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—

```mermaid
mindmap
  root((ç¬¬5å›<br/>æ¸¬åº¦è«– &<br/>ç¢ºç‡éç¨‹))
    æ¸¬åº¦ç©ºé–“
      Ïƒ-åŠ æ³•æ—
      Lebesgueæ¸¬åº¦
      ç¢ºç‡æ¸¬åº¦
    Lebesgueç©åˆ†
      å˜é–¢æ•°è¿‘ä¼¼
      MCT
      DCT
      Fatouã®è£œé¡Œ
    Radon-Nikodym
      çµ¶å¯¾é€£ç¶šæ€§
      PDF = dP/dÎ»
      å¯†åº¦æ¯” = dP/dQ
      é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    åæŸç†è«–
      æ¦‚åæŸ
      ç¢ºç‡åæŸ
      LpåæŸ
      åˆ†å¸ƒåæŸ
      CLT
    æ¡ä»¶ä»˜ãæœŸå¾…å€¤
      Ïƒ-åŠ æ³•æ—å¯æ¸¬
      æœ€è‰¯äºˆæ¸¬
      ãƒ™ã‚¤ã‚ºæ¨è«–
    Markové€£é–
      é·ç§»è¡Œåˆ—
      å®šå¸¸åˆ†å¸ƒ
      ã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰å®šç†
      æ··åˆæ™‚é–“
      MCMC
    Browné‹å‹•
      é€£ç¶šãƒ‘ã‚¹
      ç‹¬ç«‹å¢—åˆ†
      äºŒæ¬¡å¤‰å‹•
      ItÃ´ç©åˆ†
      SDE
    ç”Ÿæˆãƒ¢ãƒ‡ãƒ«
      DDPM
      Score SDE
      Flow Matching
      Rectified Flow
      æ¸¬åº¦è¼¸é€
```


---

### 6.6 ä»Šå›ã®å†’é™ºã®åç©«

| Zone | ä½•ã‚’å­¦ã‚“ã ã‹ | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ |
|:--:|:--|:--|
| 0 | ãªãœæ¸¬åº¦è«–ãŒå¿…è¦ã‹ | Cantoré›†åˆã€Riemannç©åˆ†ã®é™ç•Œã€æ··åˆåˆ†å¸ƒ |
| 1 | æ¸¬åº¦ç©ºé–“ã¨ç©åˆ† | $\sigma$-åŠ æ³•æ—ã€Lebesgueç©åˆ†ã€MCT/DCT |
| 2 | æ¸¬åº¦ã®æ¯”è¼ƒã¨å¤‰æ› | Radon-Nikodymã€pushforwardã€4ã¤ã®åæŸ |
| 3 | ç¢ºç‡éç¨‹ã¨ä¼Šè—¤è§£æ | Markové€£é–ã€Browné‹å‹•ã€ä¼Šè—¤ç©åˆ†ã€ä¼Šè—¤ã®è£œé¡Œã€SDEã€DDPM |
| 4 | å®Ÿè£… | Monte Carloã€ISã€KDEã€MHæ³•ã€GBMã€OUéç¨‹ã€Euler-Maruyama |
| 5 | è‡ªå·±è¨ºæ–­ | ã‚·ãƒ³ãƒœãƒ«èª­ã¿ã€LaTeXã€ã‚³ãƒ¼ãƒ‰ç¿»è¨³ã€ãƒãƒ£ãƒ¬ãƒ³ã‚¸ |
| 6 | å…ˆç«¯ç ”ç©¶ | Score SDEã€Flow Matchingã€Rectified Flowã€Fokker-Planck |

### 6.7 æœ€é‡è¦ãƒ†ã‚¤ã‚¯ã‚¢ã‚¦ã‚§ã‚¤

:::message alert
**3ã¤ã®æ ¸å¿ƒãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**

1. **æ¸¬åº¦è«–ã¯ã€Œç©åˆ†ã§ãã‚‹å¯¾è±¡ã€ã‚’æœ€å¤§é™ã«åºƒã’ã‚‹è¨€èª** â€” Riemannç©åˆ†ã§ã¯æ‰±ãˆãªã„é–¢æ•°ï¼ˆDirichleté–¢æ•°ã€æ··åˆåˆ†å¸ƒï¼‰ã‚’Lebesgueç©åˆ†ãŒå‡¦ç†ã™ã‚‹ã€‚ç¢ºç‡è«–ã¯ã“ã®ä¸Šã«æ§‹ç¯‰ã•ã‚Œã‚‹ã€‚

2. **Radon-Nikodymå°é–¢æ•°ã¯æ¸¬åº¦ã®ã€Œæ¯”è¼ƒã€ã‚’å¯èƒ½ã«ã™ã‚‹** â€” PDFã¯ $dP/d\lambda$ã€å°¤åº¦æ¯”ã¯ $dP/dQ$ã€importance weightã‚‚ $dP/dQ$ã€‚ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®lossã¯å¸¸ã«æ¸¬åº¦é–“ã®ã€Œè·é›¢ã€ã‚’æœ€å°åŒ–ã—ã¦ã„ã‚‹ã€‚

3. **ç¢ºç‡éç¨‹ã¯ã€Œæ™‚é–“çš„ã«ç¹‹ãŒã£ãŸæ¸¬åº¦ã®æ—ã€** â€” Markové€£é–ã¯é›¢æ•£æ™‚é–“ã€Browné‹å‹•ã¯é€£ç¶šæ™‚é–“ã€‚DDPMã¯é›¢æ•£Markové€£é–ã€Score SDEã¯é€£ç¶šSDEã€‚æ¸¬åº¦è«–ãŒä¸¡è€…ã‚’çµ±ä¸€ã™ã‚‹ã€‚
:::

### 6.8 FAQ

:::details Q1: æ¸¬åº¦è«–ã‚’å­¦ã°ãªãã¦ã‚‚æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è«–æ–‡ã¯èª­ã‚ã¾ã™ã‹ï¼Ÿ
**A**: å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã§ã¯å¯èƒ½ã€‚ã—ã‹ã—Score SDE [^2]ã€Flow Matching [^7]ã€Rectified Flow [^6] ã®ã‚ˆã†ãªç†è«–çš„ã«æ·±ã„è«–æ–‡ã¯ã€æ¸¬åº¦è«–ãªã—ã§ã¯ã€Œãªãœã“ã®å¼ãŒæ­£ã—ã„ã‹ã€ãŒç†è§£ã§ããªã„ã€‚ç‰¹ã«Radon-Nikodymå°é–¢æ•°ã¨pushforward measureã¯å¿…é ˆã®æ¦‚å¿µã€‚
:::

:::details Q2: ItÃ´ç©åˆ†ã¨Stratonovichç©åˆ†ã®é•ã„ã¯ï¼Ÿ
**A**: ItÃ´ç©åˆ†ã¯å·¦ç«¯ç‚¹ã§è©•ä¾¡ï¼ˆ$\int f(X_{t_i}) dW$ï¼‰ã€Stratonovichç©åˆ†ã¯ä¸­ç‚¹ï¼ˆ$\int f(\frac{X_{t_i}+X_{t_{i+1}}}{2}) \circ dW$ï¼‰ã€‚ItÃ´ã¯ã€Œæœªæ¥ã‚’çŸ¥ã‚‰ãªã„ã€ï¼ˆé©åˆéç¨‹ï¼‰ã¨ã„ã†è‡ªç„¶ãªæ¡ä»¶ã‚’æº€ãŸã™ãŒã€é€šå¸¸ã®é€£é–å¾‹ãŒæˆã‚Šç«‹ãŸãªã„ï¼ˆItÃ´è£œæ­£ $-\sigma^2/2$ ãŒå¿…è¦ï¼‰ã€‚Stratonovichã¯é€£é–å¾‹ãŒé€šå¸¸é€šã‚Šæˆã‚Šç«‹ã¤ãŒã€ãƒãƒ«ãƒãƒ³ã‚²ãƒ¼ãƒ«æ€§ã‚’å¤±ã†ã€‚ç‰©ç†ã§ã¯Stratonovichã€é‡‘èãƒ»ML ã§ã¯ItÃ´ãŒæ¨™æº–ã€‚
:::

:::details Q3: DDPMã®forward processã‚’ãªãœMarkové€£é–ã§å®šå¼åŒ–ã™ã‚‹ã®ã§ã™ã‹ï¼Ÿ
**A**: Markovæ€§ã«ã‚ˆã‚Šï¼š(1) åŒæ™‚åˆ†å¸ƒãŒé·ç§»æ ¸ã®ç©ã«åˆ†è§£ã§ãè¨ˆç®—ãŒ tractableã€(2) å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒã‚¤ã‚ºé‡ã‚’ç‹¬ç«‹ã«è¨­è¨ˆã§ãã‚‹ã€(3) reverse processã‚‚Markové€£é–ã«ãªã‚‹ã“ã¨ãŒBayeså‰‡ã‹ã‚‰å°ã‘ã‚‹ã€‚ã‚‚ã—Markovæ€§ãŒãªã‘ã‚Œã°ã€å…¨ã‚¹ãƒ†ãƒƒãƒ—ã®åŒæ™‚æœ€é©åŒ–ãŒå¿…è¦ã§è¨ˆç®—ãŒçˆ†ç™ºã™ã‚‹ã€‚
:::




:::details Q7: çµ¶å¯¾é€£ç¶š $P \ll Q$ ã¨ç›¸äº’çµ¶å¯¾é€£ç¶š $P \sim Q$ ã®é•ã„ã¯ï¼Ÿ
**A**: $P \ll Q$ ã¯ä¸€æ–¹å‘ â€” $Q$ ãŒã‚¼ãƒ­ã®é›†åˆã§ $P$ ã‚‚ã‚¼ãƒ­ã€‚$P \sim Q$ï¼ˆ$P \ll Q$ ã‹ã¤ $Q \ll P$ï¼‰ã¯åŒæ–¹å‘ â€” åŒã˜é›†åˆã«ã‚¼ãƒ­è³ªé‡ã‚’ç½®ãã€‚$P \sim Q$ ã®ã¨ã $dP/dQ > 0$ a.e. ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒ $p_\theta$ ã¨ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ $p_{\text{data}}$ ãŒç›¸äº’çµ¶å¯¾é€£ç¶šã§ãªã„å ´åˆï¼ˆã‚µãƒãƒ¼ãƒˆãŒç•°ãªã‚‹å ´åˆï¼‰ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ãŒ $+\infty$ ã«ãªã‚Šå­¦ç¿’ãŒä¸å®‰å®šåŒ–ã™ã‚‹ã€‚GANã®mode collapseã®ä¸€å› ã€‚
:::




### 6.9.1 ã‚ˆãã‚ã‚‹ç½  (Common Traps)

:::message alert
**Trap 1: ã€Œæ¸¬åº¦ã‚¼ãƒ­ = ç©ºé›†åˆã€ã¨æ€ã„è¾¼ã‚€**

$\mathbb{Q}$ ã¯Lebesgueæ¸¬åº¦ã‚¼ãƒ­ã ãŒã€ç¨ å¯†ï¼ˆã©ã®å®Ÿæ•°ã®è¿‘ãã«ã‚‚æœ‰ç†æ•°ãŒã‚ã‚‹ï¼‰ã€‚Cantoré›†åˆã‚‚æ¸¬åº¦ã‚¼ãƒ­ã ãŒéå¯ç®—ç„¡é™ã€‚æ¸¬åº¦ã‚¼ãƒ­ â‰  ã€Œç„¡ã„ã€ã€‚

**Trap 2: Riemannç©åˆ†ã¨Lebesgueç©åˆ†ã‚’æ··åŒã™ã‚‹**

Riemannç©åˆ†å¯èƒ½ãªé–¢æ•°ã¯Lebesgueç©åˆ†å¯èƒ½ã§å€¤ã¯ä¸€è‡´ã™ã‚‹ã€‚ã—ã‹ã—é€†ã¯æˆã‚Šç«‹ãŸãªã„ã€‚Dirichleté–¢æ•° $1_\mathbb{Q}$ ã¯Lebesgueç©åˆ† $= 0$ ã ãŒRiemannç©åˆ†ã¯å­˜åœ¨ã—ãªã„ã€‚

**Trap 3: ç¢ºç‡åæŸã¨æ¦‚åæŸã‚’æ··åŒã™ã‚‹**

ç¢ºç‡åæŸ: $P(|X_n - X| > \epsilon) \to 0$ â€” ã€Œå¤§ããªãšã‚Œã€ã®ç¢ºç‡ãŒæ¸›ã‚‹
æ¦‚åæŸ: $P(\lim X_n = X) = 1$ â€” ãƒ‘ã‚¹ãƒ¯ã‚¤ã‚ºã§åæŸ

æ¦‚åæŸ â‡’ ç¢ºç‡åæŸ ã ãŒã€é€†ã¯ä¸€èˆ¬ã«æˆã‚Šç«‹ãŸãªã„ã€‚åä¾‹: typewriter sequenceã€‚

**Trap 4: äºŒæ¬¡å¤‰å‹•ã‚’ç„¡è¦–ã—ã¦ItÃ´ã®å…¬å¼ã‚’é–“é•ãˆã‚‹**

$d(W^2) = 2W \, dW + dt$ã€‚æœ€å¾Œã® $+dt$ ã¯äºŒæ¬¡å¤‰å‹• $(dW)^2 = dt$ ã‹ã‚‰æ¥ã‚‹ã€‚é€šå¸¸ã®å¾®ç©åˆ†ã®æ„Ÿè¦šã§ $d(W^2) = 2W \, dW$ ã¨ã™ã‚‹ã¨é–“é•ã„ã€‚GBMã®ItÃ´è£œæ­£ $-\sigma^2/2$ ã‚‚åŒã˜ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã€‚

**Trap 5: ESSã‚’ç„¡è¦–ã—ã¦é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ã†**

$p$ ã¨ $q$ ã®ã‚µãƒãƒ¼ãƒˆãŒå¤§ããç•°ãªã‚‹ã¨ã€ã»ã¨ã‚“ã©ã®é‡ã¿ãŒã‚¼ãƒ­ã«è¿‘ãã€å°‘æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå·¨å¤§ãªé‡ã¿ã‚’æŒã¤ã€‚ESS < 10% ãªã‚‰çµæœã¯ä¿¡é ¼ã§ããªã„ã€‚
:::





### 6.14 æ¬¡å›äºˆå‘Š â€” ç¬¬6å›: æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–

æ¬¡ã®ç¬¬6å›ã§ã¯ **æƒ…å ±ç†è«–ã¨æœ€é©åŒ–ç†è«–** ã«é€²ã‚€ã€‚KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨SGDã§æ­¦è£…ã™ã‚‹å›ã ã€‚

:::message
**ç¬¬6å›ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ**
- Shannon Entropy: $H(X) = -\sum p(x) \log p(x)$
- KL Divergence: $D_{\text{KL}}(p \| q) = \int p \log \frac{p}{q} \, d\mu$ â€” Radon-Nikodymå°é–¢æ•°å†ã³!
- Mutual Information: $I(X;Y)$ â€” ä¾å­˜ã®æ¸¬åº¦
- f-Divergence: KLã®çµ±ä¸€çš„ä¸€èˆ¬åŒ–
- å‹¾é…é™ä¸‹æ³•: SGDãƒ»Adam â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã®æ±ºå®šç‰ˆ
- æå¤±é–¢æ•°è¨­è¨ˆ: Cross-Entropy = KLæœ€å°åŒ–ã®ç­‰ä¾¡æ€§
:::

> **ç¬¬4å›** ã®ç¢ºç‡åˆ†å¸ƒ â†’ **ç¬¬5å›** ã®æ¸¬åº¦è«–çš„åŸºç¤ â†’ **ç¬¬6å›** ã®æƒ…å ±ç†è«–ãƒ»æœ€é©åŒ–ç†è«–ã€‚3ã¤ã®è¬›ç¾©ã§ç¢ºç‡è«–ã®ã€Œä¸‰è§’å½¢ã€ãŒå®Œæˆã™ã‚‹ã€‚

---


### 6.15 ğŸ’€ ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

:::message alert
**PB Question**: Lebesgueç©åˆ†ãªãã—ã¦ç¢ºç‡å¯†åº¦ãªã—ã€‚æ¸¬åº¦ã‚’çŸ¥ã‚‰ãšã«ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’èªã‚Œã‚‹ã‹ï¼Ÿ

Riemannç©åˆ†ã®ä¸–ç•Œã§ã¯ã€$\mathbb{Q}$ ä¸Šã®ä¸€æ§˜åˆ†å¸ƒã®ã‚ˆã†ãªã€Œç—…çš„ãªã€åˆ†å¸ƒã‚’æ‰±ãˆãªã„ã€‚Lebesgueç©åˆ†ã¯ã“ã®åˆ¶é™ã‚’å–ã‚Šæ‰•ã„ã€Radon-Nikodymå°é–¢æ•°ã¨ã—ã¦ç¢ºç‡å¯†åº¦é–¢æ•°ã‚’å³å¯†ã«å®šç¾©ã™ã‚‹ã€‚

DDPMã®forward processã¯ã€ã‚¬ã‚¦ã‚¹ã®é·ç§»æ ¸ã‚’æŒã¤Markové€£é–ã§ã‚ã‚Šã€ãã®åˆ†å¸ƒã®å¤‰åŒ–ã¯ pushforward measure ã®ç³»åˆ—ã¨ã—ã¦è¨˜è¿°ã•ã‚Œã‚‹ã€‚Score SDE ã¯ã€ã“ã®é›¢æ•£éç¨‹ã‚’é€£ç¶šã®SDEã«æ‹¡å¼µã—ã€Browné‹å‹•ã®ItÃ´ç©åˆ†ã‚’ä½¿ã£ã¦å®šå¼åŒ–ã™ã‚‹ã€‚Flow Matching ã¯ã€æ¸¬åº¦è¼¸é€ã®æœ€é©åŒ–å•é¡Œã¨ã—ã¦ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’å†å®šå¼åŒ–ã™ã‚‹ã€‚

**ã™ã¹ã¦ã®é“ã¯æ¸¬åº¦è«–ã«é€šã˜ã‚‹ã€‚**

æ¸¬åº¦è«–ã‚’å­¦ã¶ã“ã¨ã¯ã€å€‹ã€…ã®æ‰‹æ³•ã®èƒŒå¾Œã«ã‚ã‚‹çµ±ä¸€çš„ãªæ§‹é€ ã‚’è¦‹ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚ãã‚Œã¯å˜ãªã‚‹æ•°å­¦çš„å³å¯†æ€§ã®ãŸã‚ã§ã¯ãªãã€**æ–°ã—ã„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’è¨­è¨ˆã™ã‚‹ãŸã‚ã®è¨€èª**ã‚’æ‰‹ã«å…¥ã‚Œã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

æ¬¡ã®ç¬¬6å›ã§ã¯ã€ã“ã®æ¸¬åº¦ã®è¨€èªã®ä¸Šã«ã€Œæƒ…å ±ã€ã®æ¦‚å¿µã‚’æ§‹ç¯‰ã™ã‚‹ã€‚KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯ $\frac{dP}{dQ}$ ã®å¯¾æ•°ã®æœŸå¾…å€¤ â€” ã¾ã•ã«Radon-Nikodymå°é–¢æ•°ãŒä¸»å½¹ã ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

[^1]: Ho, J., Jain, A., & Abbeel, P. (2020). *Denoising Diffusion Probabilistic Models*. NeurIPS 2020. arXiv:2006.11239 â€” DDPMã®åŸè«–æ–‡ã€‚ã‚¬ã‚¦ã‚¹é·ç§»æ ¸ã‚’æŒã¤Markové€£é–ã¨ã—ã¦æ‹¡æ•£éç¨‹ã‚’å®šç¾©ã€‚

[^2]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2020). *Score-Based Generative Modeling through Stochastic Differential Equations*. ICLR 2021. arXiv:2011.13456 â€” Score SDEã®åŸè«–æ–‡ã€‚DDPMã‚’é€£ç¶šSDEã«æ‹¡å¼µã—ã€reverse SDEã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

[^3]: Levin, D. A., & Peres, Y. (2017). *Markov Chains and Mixing Times* (2nd ed.). American Mathematical Society. â€” Markové€£é–ç†è«–ã®æ¨™æº–æ•™ç§‘æ›¸ã€‚ã‚¨ãƒ«ã‚´ãƒ¼ãƒ‰å®šç†ãƒ»æ··åˆæ™‚é–“ã®è©³ç´°ã€‚

[^4]: ItÃ´, K. (1944). *Stochastic Integral*. Proceedings of the Imperial Academy, 20(8), 519-524. â€” ç¢ºç‡ç©åˆ†ã®åŸè«–æ–‡ã€‚Browné‹å‹•ã«å¯¾ã™ã‚‹ç©åˆ†ã‚’å®šç¾©ã€‚

[^5]: Roberts, G. O., Gelman, A., & Gilks, W. R. (1997). *Weak convergence and optimal scaling of random walk Metropolis algorithms*. Annals of Applied Probability, 7(1), 110-120. â€” MHæ³•ã®æœ€é©å—ç†ç‡23.4%ã®ç†è«–ã€‚

[^6]: Liu, X., Gong, C., & Liu, Q. (2022). *Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow*. ICLR 2023. arXiv:2209.03003 â€” Rectified Flowã®åŸè«–æ–‡ã€‚ãƒ‘ã‚¹ã®ç›´ç·šåŒ–ã«ã‚ˆã‚‹é«˜é€Ÿç”Ÿæˆã€‚

[^7]: Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., & Le, M. (2022). *Flow Matching for Generative Modeling*. ICLR 2023. arXiv:2210.02747 â€” Flow Matchingã®åŸè«–æ–‡ã€‚æ¡ä»¶ä»˜ãé€Ÿåº¦å ´ã®å›å¸°ã§ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã€‚

[^9]: Anderson, B. D. O. (1982). *Reverse-time diffusion equation models*. Stochastic Processes and their Applications, 12(3), 313-326. â€” Reverse SDEã®ç†è«–ã€‚Score SDEã®åŸºç¤ã€‚

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | åˆå‡º |
|:--|:--|:--|
| $(\Omega, \mathcal{F}, P)$ | ç¢ºç‡ç©ºé–“ï¼ˆæ¨™æœ¬ç©ºé–“ã€Ïƒ-åŠ æ³•æ—ã€ç¢ºç‡æ¸¬åº¦ï¼‰ | Zone 1 |
| $(X, \mathcal{F}, \mu)$ | ä¸€èˆ¬ã®æ¸¬åº¦ç©ºé–“ | Zone 1 |
| $\lambda$ | Lebesgueæ¸¬åº¦ | Zone 1 |
| $\mathcal{B}(\mathbb{R})$ | Borel Ïƒ-åŠ æ³•æ— | Zone 1 |
| $\int f \, d\mu$ | Lebesgueç©åˆ† | Zone 1 |
| $f_n \uparrow f$ | $f_n$ ãŒ $f$ ã«å˜èª¿å¢—åŠ  | Zone 1 |
| $P \ll Q$ | çµ¶å¯¾é€£ç¶šæ€§ï¼ˆ$Q(A)=0 \Rightarrow P(A)=0$ï¼‰ | Zone 2 |
| $\frac{dP}{dQ}$ | Radon-Nikodymå°é–¢æ•° | Zone 2 |
| $T_\# \mu$ | Pushforward æ¸¬åº¦ | Zone 2 |
| $X_n \xrightarrow{\text{a.s.}} X$ | æ¦‚åæŸ | Zone 2 |
| $X_n \xrightarrow{P} X$ | ç¢ºç‡åæŸ | Zone 2 |
| $X_n \xrightarrow{L^p} X$ | $L^p$ åæŸ | Zone 2 |
| $X_n \xrightarrow{d} X$ | åˆ†å¸ƒåæŸ | Zone 2 |
| $\mathbb{E}[X \mid \mathcal{G}]$ | $\mathcal{G}$-å¯æ¸¬æ¡ä»¶ä»˜ãæœŸå¾…å€¤ | Zone 2 |
| $P_{ij}$ | é·ç§»ç¢ºç‡ $P(X_{n+1}=j \mid X_n=i)$ | Zone 3 |
| $\boldsymbol{\pi}$ | å®šå¸¸åˆ†å¸ƒ | Zone 3 |
| $W(t)$ / $W_t$ | Browné‹å‹•ï¼ˆWieneréç¨‹ï¼‰ | Zone 3 |
| $[W]_t$ | äºŒæ¬¡å¤‰å‹• | Zone 3 |
| $dX = \mu \, dt + \sigma \, dW$ | ç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼ | Zone 3 |
| $\beta_t$ | DDPMã®ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« | Zone 3 |
| $\bar{\alpha}_t$ | $\prod_{s=1}^t (1-\beta_s)$ï¼ˆDDPMã®ç´¯ç©æ¸›è¡°ï¼‰ | Zone 3 |
| $\nabla_x \log p(x)$ | Score function | Zone 6 |
| $v_t(x)$ | Flow Matchingã®é€Ÿåº¦å ´ | Zone 6 |
| $M_n$ | ãƒãƒ«ãƒãƒ³ã‚²ãƒ¼ãƒ« | Zone 2 |
| $\mathcal{F}_n$ | ãƒ•ã‚£ãƒ«ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ™‚åˆ»$n$ã¾ã§ã®æƒ…å ±ï¼‰ | Zone 2 |
| $\lambda(A)$ | é›†åˆ$A$ã®Lebesgueæ¸¬åº¦ | Zone 1 |
| $\text{ESS}$ | æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º | Zone 4 |
| $\alpha(x, x')$ | MHæ³•ã®å—ç†ç¢ºç‡ | Zone 4 |
| $\theta$ | OUéç¨‹ã®å¹³å‡å›å¸°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | Zone 4 |
| $t_{\text{mix}}$ | Markové€£é–ã®æ··åˆæ™‚é–“ | Zone 3 |
| $\text{TV}(P, Q)$ | å…¨å¤‰å‹•è·é›¢ $\frac{1}{2}\sum\|P-Q\|$ | Zone 3 |
| $\phi(x)$ | æ¨™æº–æ­£è¦å¯†åº¦é–¢æ•° | å…¨èˆ¬ |
| $1_A(x)$ | é›†åˆ$A$ã®æŒ‡ç¤ºé–¢æ•° | Zone 1 |
| $f_n \uparrow$ | $f_n$ ãŒå˜èª¿éæ¸›å°‘ | Zone 1 |
| $\text{a.e.}$ | almost everywhereï¼ˆã»ã¼è‡³ã‚‹ã¨ã“ã‚ï¼‰ | å…¨èˆ¬ |
| $\text{a.s.}$ | almost surelyï¼ˆã»ã¨ã‚“ã©ç¢ºå®Ÿã«ï¼‰ | å…¨èˆ¬ |
| $P \sim Q$ | $P$ ã¨ $Q$ ãŒç›¸äº’çµ¶å¯¾é€£ç¶š | Zone 2 |
| $\mathcal{B}(\mathbb{R}^d)$ | $\mathbb{R}^d$ ã®Borel Ïƒ-åŠ æ³•æ— | Zone 1 |
| $\int_0^T f_t \, dW_t$ | ä¼Šè—¤ç©åˆ†ï¼ˆå·¦ç«¯ç‚¹è©•ä¾¡ã®ç¢ºç‡ç©åˆ†ï¼‰ | Zone 3 |
| $\mathbb{E}[(\int f \, dW)^2] = \mathbb{E}[\int f^2 \, dt]$ | ä¼Šè—¤ç­‰é•·å®šç† (ItÃ´ isometry) | Zone 3 |
| $dh = h'dX + \frac{1}{2}h''g^2 dt$ | ä¼Šè—¤ã®å…¬å¼ï¼ˆç¢ºç‡å¾®ç©åˆ†ã®é€£é–å¾‹ï¼‰ | Zone 3 |
| $dW^2 = dt$ | äºŒæ¬¡å¤‰å‹•ã®ä¹—æ³•è¦å‰‡ | Zone 3 |
| $f(X_t, t)$ | SDEã®ãƒ‰ãƒªãƒ•ãƒˆä¿‚æ•° | Zone 3 |
| $g(X_t, t)$ | SDEã®æ‹¡æ•£ä¿‚æ•° | Zone 3 |
| $\partial_t p = -\partial_x(fp) + \frac{1}{2}\partial_x^2(g^2 p)$ | Fokker-Planckæ–¹ç¨‹å¼ | Zone 6 |
| $\Delta t$ | Euler-Maruyamaæ³•ã®æ™‚é–“åˆ»ã¿å¹… | Zone 4 |

---

## Appendix: Zoneé–“ã®ä¾å­˜é–¢ä¿‚

```mermaid
graph LR
    Z0["Zone 0<br/>ãªãœæ¸¬åº¦è«–ï¼Ÿ"] --> Z1["Zone 1<br/>æ¸¬åº¦ç©ºé–“"]
    Z1 --> Z2["Zone 2<br/>RNå°é–¢æ•°<br/>åæŸ"]
    Z2 --> Z3["Zone 3<br/>Markové€£é–<br/>Browné‹å‹•<br/>DDPM"]
    Z1 --> Z4["Zone 4<br/>Monte Carlo<br/>KDE"]
    Z2 --> Z4
    Z3 --> Z4
    Z4 --> Z5["Zone 5<br/>è©¦ç·´"]
    Z3 --> Z6["Zone 6<br/>Score SDE<br/>Flow Matching"]
    Z5 --> Z7["Zone 7<br/>ã¾ã¨ã‚"]
    Z6 --> Z7
```

---

:::message
**ç¬¬5å› å®Œäº†!** æ¸¬åº¦è«–ã®æŠ½è±¡çš„ãªæ¦‚å¿µã‚’ã€Cantoré›†åˆã‹ã‚‰DDPMã€Flow Matchingã¾ã§ã®å…·ä½“ä¾‹ã§ä¸€è²«ã—ã¦ç†è§£ã—ãŸã€‚æ¬¡ã®ç¬¬6å›ã§ã¯ã€ã“ã®æ¸¬åº¦ã®è¨€èªã§ã€Œæƒ…å ±ã€ã‚’å®šé‡åŒ–ã™ã‚‹ â€” KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¯Radon-Nikodymå°é–¢æ•°ã®æœŸå¾…å€¤ã ã€‚
:::

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
