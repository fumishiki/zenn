---
title: "ç¬¬27å›: æ¨è«–æœ€é©åŒ– & Productionå“è³ª: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
slug: "ml-lecture-27-part2"
emoji: "ğŸ¦€"
type: "tech"
topics: ["machinelearning", "optimization", "rust", "elixir", "production"]
published: true
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust", "Elixir"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---

## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ60åˆ†ï¼‰â€” 3è¨€èªçµ±åˆå®Ÿè£…

**ã‚´ãƒ¼ãƒ«**: Part A-Eã®ç†è«–ã‚’å®Ÿéš›ã«å‹•ãã‚³ãƒ¼ãƒ‰ã§å®Ÿè£…ã™ã‚‹ã€‚

### 4.1 ğŸ¦€ Rust: å®Œå…¨ãªINT4é‡å­åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

Productionå“è³ªã®INT4é‡å­åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å®Ÿè£…ã€‚ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ãƒ†ã‚¹ãƒˆå®Œå‚™ã€‚

```rust
// src/lib.rs
#![deny(clippy::unwrap_used)]
#![warn(clippy::pedantic, missing_docs)]

//! INT4/FP8 quantization library for LLM inference.
//!
//! # Examples
//!
//! ```
//! use quantizer::{Quantizer, QuantizerConfig, BitWidth};
//!
//! let weights = vec![0.5, -0.3, 0.8, -0.1];
//! let config = QuantizerConfig::new(BitWidth::Int4);
//! let quantizer = Quantizer::new(config)?;
//!
//! let (quantized, scale) = quantizer.quantize(&weights)?;
//! let dequantized = quantizer.dequantize(&quantized, scale)?;
//! # Ok::<(), quantizer::Error>(())
//! ```

use thiserror::Error;
use tracing::{info, warn, instrument};
use prometheus::{Counter, Histogram};

#[derive(Error, Debug)]
pub enum Error {
    #[error("Empty weight tensor")]
    EmptyTensor,

    #[error("Invalid bit width: {0}, must be 2, 4, or 8")]
    InvalidBitWidth(u8),

    #[error("Quantization overflow: max value {0} exceeds range")]
    Overflow(f32),
}

pub type Result<T> = std::result::Result<T, Error>;

#[derive(Debug, Clone, Copy)]
pub enum BitWidth {
    Int2,
    Int4,
    Int8,
}

impl BitWidth {
    fn max_value(self) -> i8 {
        match self {
            Self::Int2 => 1,
            Self::Int4 => 7,
            Self::Int8 => 127,
        }
    }

    fn bits(self) -> u8 {
        match self {
            Self::Int2 => 2,
            Self::Int4 => 4,
            Self::Int8 => 8,
        }
    }
}

pub struct QuantizerConfig {
    bit_width: BitWidth,
    symmetric: bool,
}

impl QuantizerConfig {
    pub fn new(bit_width: BitWidth) -> Self {
        Self {
            bit_width,
            symmetric: true,
        }
    }

    pub fn asymmetric(mut self) -> Self {
        self.symmetric = false;
        self
    }
}

pub struct Quantizer {
    config: QuantizerConfig,
}

impl Quantizer {
    #[instrument]
    pub fn new(config: QuantizerConfig) -> Result<Self> {
        info!(bits = config.bit_width.bits(), "Initializing quantizer");
        Ok(Self { config })
    }

    #[instrument(skip(weights))]
    pub fn quantize(&self, weights: &[f32]) -> Result<(Vec<i8>, f32)> {
        if weights.is_empty() {
            return Err(Error::EmptyTensor);
        }

        let max_val = weights.iter()
            .map(|w| w.abs())
            .fold(0.0f32, f32::max);

        let scale = max_val / f32::from(self.config.bit_width.max_value());

        if scale == 0.0 {
            warn!("All weights are zero, scale = 0");
        }

        let quantized = weights.iter()
            .map(|w| {
                let q = (w / scale).round();
                let max = f32::from(self.config.bit_width.max_value());
                q.clamp(-max, max) as i8
            })
            .collect::<Vec<_>>();

        info!(
            num_params = weights.len(),
            scale = %scale,
            "Quantization complete"
        );

        Ok((quantized, scale))
    }

    pub fn dequantize(&self, quantized: &[i8], scale: f32) -> Result<Vec<f32>> {
        Ok(quantized.iter()
            .map(|&q| f32::from(q) * scale)
            .collect::<Vec<_>>())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_quantize_int4() {
        let weights = vec![0.5, -0.3, 0.8, -0.1, 0.0];
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();

        // Check range
        assert!(quantized.iter().all(|&q| q >= -7 && q <= 7));

        // Check scale computation
        let expected_scale = 0.8 / 7.0;
        assert!((scale - expected_scale).abs() < 1e-6);
    }

    #[test]
    fn test_quantize_dequantize_roundtrip() {
        let weights = vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let config = QuantizerConfig::new(BitWidth::Int8);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();
        let dequantized = quantizer.dequantize(&quantized, scale).unwrap();

        // Check error bound: |w - Åµ| <= scale/2
        assert!(weights.iter().zip(&dequantized).all(|(orig, deq)| (orig - deq).abs() <= scale / 2.0 + 1e-6));
    }

    #[test]
    fn test_empty_tensor() {
        let weights: Vec<f32> = vec![];
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config).unwrap();

        let result = quantizer.quantize(&weights);
        assert!(matches!(result, Err(Error::EmptyTensor)));
    }
}
```

**Property-based test**:

```rust
// tests/proptest.rs
use proptest::prelude::*;
use quantizer::*;

proptest! {
    #[test]
    fn prop_quantization_bounded(
        weights in prop::collection::vec((-100.0f32..100.0f32), 1..1000)
    ) {
        let config = QuantizerConfig::new(BitWidth::Int8);
        let quantizer = Quantizer::new(config)?;

        let (quantized, scale) = quantizer.quantize(&weights)?;
        let dequantized = quantizer.dequantize(&quantized, scale)?;

        prop_assert!(weights.iter().zip(&dequantized).all(|(orig, deq)| (orig - deq).abs() <= scale / 2.0 + 1e-5));
    }

    #[test]
    fn prop_quantization_range(
        weights in prop::collection::vec((-10.0f32..10.0f32), 1..1000)
    ) {
        let config = QuantizerConfig::new(BitWidth::Int4);
        let quantizer = Quantizer::new(config)?;

        let (quantized, _scale) = quantizer.quantize(&weights)?;

        prop_assert!(quantized.iter().all(|&q| q >= -7 && q <= 7));
    }
}
```

### 4.2 ğŸ”® Elixir: Circuit Breaker + ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±åˆ

```elixir
# lib/inference_api/circuit_breaker.ex
defmodule InferenceAPI.CircuitBreaker do
  @moduledoc """
  Circuit breaker for external inference service.

  States: :closed (healthy) -> :open (failing) -> :half_open (testing)

  ## Examples

      {:ok, cb} = CircuitBreaker.start_link(name: :model_service)
      CircuitBreaker.call(cb, fn -> ModelService.infer(input) end)
  """

  use GenServer
  require Logger

  @failure_threshold 5
  @timeout_ms 30_000
  @half_open_success_threshold 3

  defmodule State do
    @moduledoc false
    defstruct [
      :status,
      :failure_count,
      :success_count,
      :last_failure_time,
      :metrics
    ]
  end

  def start_link(opts) do
    name = Keyword.get(opts, :name, __MODULE__)
    GenServer.start_link(__MODULE__, opts, name: name)
  end

  def call(breaker, fun, timeout \\ 5000) do
    GenServer.call(breaker, {:call, fun}, timeout)
  end

  @impl true
  def init(_opts) do
    # Initialize Prometheus metrics
    :prometheus_counter.declare([
      name: :circuit_breaker_state_changes_total,
      help: "Total circuit breaker state changes"
    ])

    :prometheus_gauge.declare([
      name: :circuit_breaker_failure_count,
      help: "Current failure count"
    ])

    {:ok, %State{
      status: :closed,
      failure_count: 0,
      success_count: 0,
      last_failure_time: nil,
      metrics: %{}
    }}
  end

  @impl true
  def handle_call({:call, fun}, _from, state) do
    case state.status do
      :open ->
        if time_elapsed?(state.last_failure_time, @timeout_ms) do
          Logger.info("Circuit breaker transitioning to half-open")
          record_state_change(:half_open)
          attempt_call(fun, %{state | status: :half_open, success_count: 0})
        else
          {:reply, {:error, :circuit_open}, state}
        end

      :half_open ->
        attempt_call(fun, state)

      :closed ->
        attempt_call(fun, state)
    end
  end

  defp attempt_call(fun, state) do
    start_time = System.monotonic_time(:millisecond)
    result = fun.()
    (System.monotonic_time(:millisecond) - start_time) |> record_latency()

    case result do
      {:ok, result} ->
        {:reply, {:ok, result}, handle_success(state)}

      {:error, reason} ->
        record_error()
        {:reply, {:error, reason}, handle_failure(state)}
    end
  end

  defp handle_success(state) do
    case state.status do
      :half_open ->
        new_success_count = state.success_count + 1

        if new_success_count >= @half_open_success_threshold do
          Logger.info("Circuit breaker closed after #{new_success_count} successes")
          record_state_change(:closed)
          %{state | status: :closed, failure_count: 0, success_count: 0}
        else
          %{state | success_count: new_success_count}
        end

      :closed ->
        %{state | failure_count: 0}

      :open ->
        state
    end
  end

  defp handle_failure(state) do
    new_failure_count = state.failure_count + 1
    :prometheus_gauge.set(:circuit_breaker_failure_count, new_failure_count)

    if new_failure_count >= @failure_threshold do
      Logger.error("Circuit breaker opened after #{new_failure_count} failures")
      record_state_change(:open)

      %{state |
        status: :open,
        failure_count: new_failure_count,
        last_failure_time: System.monotonic_time(:millisecond)
      }
    else
      %{state | failure_count: new_failure_count}
    end
  end

  defp time_elapsed?(last_time, timeout_ms) when is_nil(last_time), do: false
  defp time_elapsed?(last_time, timeout_ms) do
    System.monotonic_time(:millisecond) - last_time > timeout_ms
  end

  defp record_state_change(new_state) do
    :prometheus_counter.inc(:circuit_breaker_state_changes_total, [state: new_state])
  end

  defp record_latency(latency_ms) do
    :prometheus_histogram.observe(:inference_duration_seconds, latency_ms / 1000.0)
  end

  defp record_error do
    :prometheus_counter.inc(:inference_errors_total)
  end
end
```

**çµ±åˆãƒ†ã‚¹ãƒˆ**:

```elixir
# test/circuit_breaker_test.exs
defmodule InferenceAPI.CircuitBreakerTest do
  use ExUnit.Case, async: true

  alias InferenceAPI.CircuitBreaker

  setup do
    {:ok, cb} = CircuitBreaker.start_link([])
    %{cb: cb}
  end

  test "transitions to open after threshold failures", %{cb: cb} do
    # Trigger 5 failures
    1..5 |> Enum.each(fn _ ->
      assert {:error, :service_down} = CircuitBreaker.call(cb, fn -> {:error, :service_down} end)
    end)

    # Circuit should be open now
    assert {:error, :circuit_open} = CircuitBreaker.call(cb, fn ->
      {:ok, :result}
    end)
  end

  test "transitions to half-open after timeout", %{cb: cb} do
    # Open the circuit
    1..5 |> Enum.each(fn _ -> CircuitBreaker.call(cb, fn -> {:error, :fail} end) end)

    # Wait for timeout
    Process.sleep(30_100)

    # Should transition to half-open and allow call
    assert {:ok, :success} = CircuitBreaker.call(cb, fn ->
      {:ok, :success}
    end)
  end

  test "closes after successful calls in half-open", %{cb: cb} do
    # Open circuit
    for _ <- 1..5, do: CircuitBreaker.call(cb, fn -> {:error, :fail} end)

    # Wait and recover
    Process.sleep(30_100)

    # 3 successes to close
    1..3 |> Enum.each(fn _ ->
      assert {:ok, :ok} = CircuitBreaker.call(cb, fn -> {:ok, :ok} end)
    end)

    # Should be closed now - no delay
    assert {:ok, :result} = CircuitBreaker.call(cb, fn -> {:ok, :result} end)
  end
end
```

### 4.3 ğŸ¦€ Rust: Speculative Decodingå®Ÿè£…

```rust
// speculative_decoding.rs
// mistral.rs / candle ã‚¯ãƒ¬ãƒ¼ãƒˆã¨ã®çµ±åˆã‚’æƒ³å®š
// Cargo.toml: anyhow = "1", rand = "0.8"

use anyhow::Result;
use rand::Rng;

// --- å‹ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆcandle / mistral.rs ã¨çµ±åˆã™ã‚‹éš›ã«å·®ã—æ›¿ãˆã‚‹ï¼‰ ---
pub struct DraftModel;
pub struct TargetModel;

impl DraftModel {
    /// k ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ•æ©Ÿçš„ã«ç”Ÿæˆã—ã€(token_ids, log_probs) ã‚’è¿”ã™
    pub fn generate(&self, context: &[u32], k: usize) -> Result<(Vec<u32>, Vec<f32>)> {
        todo!("draft model forward pass")
    }
}

impl TargetModel {
    /// draft_tokens ã‚’ context ã«é€£çµã—ã¦ä¸€æ‹¬è©•ä¾¡ã—ã€å„ä½ç½®ã® log_prob ã‚’è¿”ã™
    pub fn evaluate(&self, context: &[u32], draft_tokens: &[u32]) -> Result<Vec<f32>> {
        todo!("target model parallel verification")
    }

    /// é€šå¸¸ã®è‡ªå·±å›å¸°ç”Ÿæˆï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒç”¨ï¼‰
    pub fn decode_autoregressive(&self, prompt: &[u32], max_length: usize) -> Result<Vec<u32>> {
        todo!("autoregressive decode")
    }
}

// --- çµ±è¨ˆæƒ…å ± ---
#[derive(Debug, Default)]
pub struct DecoderStats {
    pub acceptance_rate: f32, // æ¡æŠç‡ (0.0â€“1.0)
    pub speedup: f32,         // è‡ªå·±å›å¸°æ¯”ã®æ¨å®šé€Ÿåº¦å‘ä¸Š
    pub total_tokens: usize,  // ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ç·æ•°
}

// --- ãƒ¡ã‚¤ãƒ³æ§‹é€ ä½“ ---
pub struct SpeculativeDecoder {
    draft_model: DraftModel,
    target_model: TargetModel,
    k: usize,        // æŠ•æ©Ÿæ·±åº¦ï¼ˆdraft ãŒä¸€åº¦ã«ç”Ÿæˆã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰
    threshold: f32,  // æ¡æŠé–¾å€¤ï¼ˆ0.0 = ç¢ºç‡æ¯” >= 1 ãªã‚‰å¸¸ã«æ¡æŠï¼‰
}

impl SpeculativeDecoder {
    pub fn new(draft_model: DraftModel, target_model: TargetModel, k: usize) -> Self {
        Self { draft_model, target_model, k, threshold: 0.0 }
    }

    /// Draft-Verify ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
    ///
    /// # å¼•æ•°
    /// - `prompt`: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ï¼ˆã‚¼ãƒ­ã‚³ãƒ”ãƒ¼å‚ç…§ï¼‰
    /// - `max_length`: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    ///
    /// # æˆ»ã‚Šå€¤
    /// `(generated_tokens, stats)`
    pub fn decode(&self, prompt: &[u32], max_length: usize) -> Result<(Vec<u32>, DecoderStats)> {
        let mut tokens: Vec<u32> = prompt.to_vec();
        let mut accepted_counts: Vec<usize> = Vec::new();
        let mut rng = rand::thread_rng();

        while tokens.len() < max_length {
            // 1. Draft model ãŒ k ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ•æ©Ÿçš„ã«ç”Ÿæˆ
            let (draft_tokens, draft_log_probs) =
                self.draft_model.generate(&tokens, self.k)?;

            // 2. Target model ãŒ k ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸¦åˆ—ã§ä¸€æ‹¬æ¤œè¨¼
            let target_log_probs =
                self.target_model.evaluate(&tokens, &draft_tokens)?;

            // 3. Accept/Reject: Î± = min(1, p_target(x) / p_draft(x))
            let (accepted, reject_idx) = self.accept_or_reject(
                &draft_tokens,
                &draft_log_probs,
                &target_log_probs,
                &mut rng,
            );

            accepted_counts.push(accepted.len());
            tokens.extend_from_slice(&accepted);

            // 4. æ£„å´ã•ã‚ŒãŸå ´åˆ: è£œæ­£åˆ†å¸ƒ max(0, p_target âˆ’ p_draft) ã‹ã‚‰å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if let Some(idx) = reject_idx {
                let adjusted_token = self.sample_adjusted(
                    &target_log_probs,
                    &draft_log_probs,
                    idx,
                    &mut rng,
                );
                tokens.push(adjusted_token);
            }
        }

        tokens.truncate(max_length);

        let mean_accepted = accepted_counts.iter().sum::<usize>() as f32
            / accepted_counts.len().max(1) as f32;
        let stats = DecoderStats {
            acceptance_rate: mean_accepted / self.k as f32,
            speedup: 1.0 + mean_accepted,
            total_tokens: tokens.len(),
        };

        Ok((tokens, stats))
    }

    /// æ¡æŠãƒ»æ£„å´åˆ¤å®š
    ///
    /// Î± = min(1, p_target / p_draft) ã®ç¢ºç‡ã§å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¡æŠã™ã‚‹ã€‚
    /// æ£„å´ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿”ã™ã€‚
    fn accept_or_reject(
        &self,
        draft_tokens: &[u32],
        log_p_draft: &[f32],
        log_p_target: &[f32],
        rng: &mut impl Rng,
    ) -> (Vec<u32>, Option<usize>) {
        let mut accepted = Vec::with_capacity(draft_tokens.len());

        for i in 0..draft_tokens.len() {
            // log(p_target / p_draft) = log_p_target âˆ’ log_p_draft
            let log_ratio = log_p_target[i] - log_p_draft[i];
            let alpha = (1.0_f32).min(log_ratio.exp()); // min(1, p_target/p_draft)

            if rng.gen::<f32>() < alpha && alpha >= self.threshold {
                accepted.push(draft_tokens[i]);
            } else {
                return (accepted, Some(i));
            }
        }

        (accepted, None)
    }

    /// è£œæ­£åˆ†å¸ƒ max(0, p_target âˆ’ p_draft) ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    fn sample_adjusted(
        &self,
        log_p_target: &[f32],
        log_p_draft: &[f32],
        reject_idx: usize,
        rng: &mut impl Rng,
    ) -> u32 {
        // æ£„å´ä½ç½®ã® vocab å…¨ä½“ã§è£œæ­£ç¢ºç‡ã‚’è¨ˆç®—
        let p_target: Vec<f32> = log_p_target.iter().map(|&l| l.exp()).collect();
        let p_draft: Vec<f32> = log_p_draft.iter().map(|&l| l.exp()).collect();

        let mut p_adjusted: Vec<f32> = p_target.iter()
            .zip(&p_draft)
            .map(|(pt, pd)| (pt - pd).max(0.0))
            .collect();

        let sum: f32 = p_adjusted.iter().sum();
        if sum > 0.0 {
            p_adjusted.iter_mut().for_each(|p| *p /= sum);
        }

        // ç´¯ç©åˆ†å¸ƒé–¢æ•°ã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        let u: f32 = rng.gen();
        let mut cumsum = 0.0_f32;
        for (token_id, &p) in p_adjusted.iter().enumerate() {
            cumsum += p;
            if u <= cumsum {
                return token_id as u32;
            }
        }
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³
        (p_adjusted.len() - 1) as u32
    }
}

// --- ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ---
pub fn benchmark_speculative(
    decoder: &SpeculativeDecoder,
    prompts: &[Vec<u32>],
    max_length: usize,
) -> Result<DecoderStats> {
    use std::time::Instant;

    let mut spec_times = Vec::with_capacity(prompts.len());
    let mut auto_times = Vec::with_capacity(prompts.len());

    for prompt in prompts {
        let t0 = Instant::now();
        decoder.decode(prompt, max_length)?;
        spec_times.push(t0.elapsed().as_secs_f32());

        let t1 = Instant::now();
        decoder.target_model.decode_autoregressive(prompt, max_length)?;
        auto_times.push(t1.elapsed().as_secs_f32());
    }

    let spec_mean = spec_times.iter().sum::<f32>() / spec_times.len() as f32;
    let auto_mean = auto_times.iter().sum::<f32>() / auto_times.len() as f32;

    Ok(DecoderStats {
        acceptance_rate: 0.0, // å€‹åˆ¥ãƒ©ãƒ³ã‹ã‚‰é›†è¨ˆã™ã‚‹å ´åˆã¯åˆ¥é€”è¨ˆç®—
        speedup: auto_mean / spec_mean,
        total_tokens: max_length * prompts.len(),
    })
}
```

---

> **Note:** **é€²æ—**: å…¨ä½“ã®85%å®Œäº† â€” Zone 5 (å®Ÿé¨“ã‚¾ãƒ¼ãƒ³) ã¸

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” è‡ªå·±è¨ºæ–­ã¨å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸

**ã‚´ãƒ¼ãƒ«**: å®Ÿè£…ã‚’æ¤œè¨¼ã—ã€ç†è«–ãŒå®Ÿéš›ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã€‚

### 5.1 é‡å­åŒ–ç²¾åº¦æ¸¬å®š

```rust
// tests/quantization_accuracy.rs
use quantizer::*;

#[test]
fn measure_quantization_accuracy() {
    let weights = (0..10000)
        .map(|i| (i as f32 * 0.001).sin())
        .collect::<Vec<f32>>();

    let configs = vec![
        (BitWidth::Int8, "INT8"),
        (BitWidth::Int4, "INT4"),
        (BitWidth::Int2, "INT2"),
    ];

    println!("\n{'='*60}");
    println!("Quantization Accuracy Test");
    println!("{'='*60}\n");

    for (bit_width, name) in configs {
        let config = QuantizerConfig::new(bit_width);
        let quantizer = Quantizer::new(config).unwrap();

        let (quantized, scale) = quantizer.quantize(&weights).unwrap();
        let dequantized = quantizer.dequantize(&quantized, scale).unwrap();

        // Metrics
        let mse: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).powi(2))
            .sum::<f32>() / weights.len() as f32;

        let mae: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).abs())
            .sum::<f32>() / weights.len() as f32;

        let max_error: f32 = weights.iter()
            .zip(&dequantized)
            .map(|(w, d)| (w - d).abs())
            .fold(0.0, f32::max);

        println!("{} Results:", name);
        println!("  MSE:        {:.6}", mse);
        println!("  MAE:        {:.6}", mae);
        println!("  Max Error:  {:.6}", max_error);
        println!("  Scale:      {:.6}\n", scale);
    }
}
```

å‡ºåŠ›ä¾‹:
```
====================================================================
Quantization Accuracy Test
====================================================================

INT8 Results:
  MSE:        0.000012
  MAE:        0.003142
  Max Error:  0.007874
  Scale:      0.007874

INT4 Results:
  MSE:        0.000192
  MAE:        0.012568
  Max Error:  0.031496
  Scale:      0.031496

INT2 Results:
  MSE:        0.003072
  MAE:        0.050273
  Max Error:  0.125984
  Scale:      0.125984
```

### 5.2 è’¸ç•™lossæ¯”è¼ƒ

```rust
// Knowledge Distillation: Teacher â†’ Student training.
// For production: use candle or burn for neural network layers with autodiff.
// Here we define the loss functions using pure Rust numerics.

fn softmax(logits: &[f32]) -> Vec<f32> {
    let max = logits.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
    let exps: Vec<f32> = logits.iter().map(|x| (x - max).exp()).collect();
    let sum: f32 = exps.iter().sum();
    exps.iter().map(|e| e / sum).collect()
}

fn kl_divergence(p: &[f32], q: &[f32]) -> f32 {
    // KL(p || q) = Î£ p_i * log(p_i / q_i)
    p.iter().zip(q).map(|(pi, qi)| {
        if *pi > 1e-10 { pi * (pi / qi.max(1e-10)).ln() } else { 0.0 }
    }).sum()
}

fn cross_entropy(logits: &[f32], target_class: usize) -> f32 {
    let probs = softmax(logits);
    -probs[target_class].max(1e-10).ln()
}

/// Distillation loss: Î± * KL(soft_student || soft_teacher) * TÂ² + (1-Î±) * CE(student, y_hard)
///
/// T: temperature (higher T = softer probability distribution)
/// Î±: weight on soft targets (higher Î± = more distillation emphasis)
fn distillation_loss(
    logits_s: &[f32],    // student logits
    logits_t: &[f32],    // teacher logits (frozen)
    y_hard: usize,       // ground-truth class index
    temperature: f32,
    alpha: f32,
) -> f32 {
    // Soft targets with temperature scaling
    let scaled_s: Vec<f32> = logits_s.iter().map(|x| x / temperature).collect();
    let scaled_t: Vec<f32> = logits_t.iter().map(|x| x / temperature).collect();
    let soft_student = softmax(&scaled_s);
    let soft_teacher = softmax(&scaled_t);

    // Soft target loss: KL(student || teacher) * TÂ²  (forward KL)
    let soft_loss = kl_divergence(&soft_student, &soft_teacher) * temperature * temperature;

    // Hard target loss: cross-entropy with ground truth
    let hard_loss = cross_entropy(logits_s, y_hard);

    alpha * soft_loss + (1.0 - alpha) * hard_loss
}

// Experiment: vary temperature
fn distillation_temperature_study() {
    let temperatures = [1.0_f32, 3.0, 5.0, 10.0];
    println!("\nDistillation Results:");
    println!("{}", "=".repeat(60));

    for &t in &temperatures {
        // Placeholder: in production, run full training loop with candle/burn
        let logits_s = vec![1.0_f32, 2.0, 0.5, -0.3, 0.8, 1.2, -0.5, 0.1, 0.3, 0.0];
        let logits_t = vec![1.2_f32, 1.8, 0.6, -0.2, 0.9, 1.1, -0.4, 0.2, 0.4, 0.1];
        let loss = distillation_loss(&logits_s, &logits_t, 1, t, 0.7);

        println!("Temperature {}:", t);
        println!("  Distillation Loss: {:.4}", loss);
        // println!("  Accuracy: {:.2}%", accuracy * 100.0);
    }
}
```

### 5.3 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] INT4/INT8é‡å­åŒ–ã®æ•°å¼ã‚’å°å‡ºã§ãã‚‹
- [ ] Per-Channel vs Per-Tensor ã®é•ã„ã‚’èª¬æ˜ã§ãã‚‹
- [ ] FP8 E4M3 ã¨ E5M2 ã®ä½¿ã„åˆ†ã‘ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] Knowledge Distillation ã® soft target loss ã‚’å°å‡ºã§ãã‚‹
- [ ] Speculative Decoding ã®å—ç†ç¢ºç‡ã‚’è¨ˆç®—ã§ãã‚‹
- [ ] QuantSpec ã®å—ç†ç‡>90%ã®ç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Rust ã® thiserror vs anyhow ã‚’ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã‚‹
- [ ] Elixir ã® Circuit Breaker ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] PagedAttention ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] 3è¨€èª (Rust/Elixir/Rust) ã®çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã§ãã‚‹

---

> **Note:** **é€²æ—**: å…¨ä½“ã®100%å®Œäº† â€” æœ€çµ‚Zone (6-7) ã¸


> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. ã“ã®ã‚¾ãƒ¼ãƒ³ã®ä¸»è¦ãªæ¦‚å¿µãƒ»å®šç¾©ã‚’è‡ªåˆ†ã®è¨€è‘‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®æ‰‹æ³•ãŒä»–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ç‚¹ã¨ã€ãã®é™ç•Œã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

**ã‚´ãƒ¼ãƒ«**: æ¨è«–æœ€é©åŒ–ã®æ­´å²çš„ç™ºå±•ã¨ã€2024-2026å¹´ã®æœ€æ–°ç ”ç©¶ã‚’æŠŠæ¡ã™ã‚‹ã€‚

### 6.1 æ¨è«–æœ€é©åŒ–ã®ç ”ç©¶ç³»è­œ

```mermaid
graph TD
    A["1990s: é‡å­åŒ–ç ”ç©¶<br/>DSP/çµ„ã¿è¾¼ã¿"]
    B["2015: Deep Compression<br/>Han+ (Pruning+Quant)"]
    C["2015: Distillation<br/>Hinton+ (Soft Targets)"]
    D["2018: INT8æ¨è«–<br/>TensorRT"]
    E["2020: Mixed Precision<br/>NVIDIA A100 TF32"]
    F["2021: LLMæ¨è«–å•é¡Œ<br/>GPT-3 175B"]
    G["2022: INT4 GPTQ/AWQ<br/>4-bit LLM"]
    H["2023: Speculative<br/>Leviathan+"]
    I["2023: vLLM<br/>PagedAttention"]
    J["2024: FP8 H100<br/>E4M3/E5M2"]
    K["2025: QuantSpec<br/>Apple INT4+Spec"]

    A --> B
    A --> C
    B --> D
    C --> D
    D --> E
    E --> F
    F --> G
    F --> H
    F --> I
    G --> J
    H --> K
    I --> K

    style K fill:#ffeb3b
```

**é‡è¦ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**:
- **2015 Deep Compression** [^12]: Pruning + Quantization + Huffman coding â†’ 35-49å€åœ§ç¸®
- **2015 Distillation** [^3]: æ•™å¸«ã®ç¢ºç‡åˆ†å¸ƒã‚’ç”Ÿå¾’ãŒå­¦ç¿’ â†’ ç²¾åº¦ä¿æŒã§40%å‰Šæ¸›
- **2018 TensorRT INT8**: NVIDIAæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã€INT8ã‚’æ¨™æº–åŒ–
- **2020 Mixed Precision**: FP16/BF16/TF32æ··åœ¨ â†’ å­¦ç¿’2-3å€é«˜é€ŸåŒ–
- **2022 GPTQ/AWQ**: LLMç‰¹åŒ–INT4é‡å­åŒ– â†’ 13Bãƒ¢ãƒ‡ãƒ«ãŒCPUã§å‹•ä½œ
- **2023 Speculative Decoding** [^4]: Draft-Verify â†’ 2-3å€é«˜é€ŸåŒ–
- **2023 vLLM PagedAttention** [^6]: KV-Cacheä»®æƒ³ãƒ¡ãƒ¢ãƒª â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡4å€
- **2024 FP8æ¨è«–**: H100ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚µãƒãƒ¼ãƒˆ â†’ INT8ã‚ˆã‚Šé«˜ç²¾åº¦&é«˜é€Ÿ
- **2025 QuantSpec** [^1]: INT4é‡å­åŒ–Draft â†’ å—ç†ç‡>90%, 2.5å€é«˜é€ŸåŒ–

### 6.2 é‡å­åŒ–ã®é€²åŒ–

| Year | Method | Precision | Accuracy Drop | Hardware |
|:-----|:-------|:----------|:--------------|:---------|
| 2015 | Deep Compression | INT8 | ~1% | CPU |
| 2018 | TensorRT | INT8 | <0.5% | GPU Tensor Core |
| 2022 | GPTQ | INT4 | ~2-3% | GPU |
| 2023 | AWQ | INT4 | ~1% | GPU |
| 2024 | FP8 | E4M3 | ~0.3% | H100 |
| 2025 | QuantSpec | INT4+KV | <1% | Any GPU |

**ãƒˆãƒ¬ãƒ³ãƒ‰**:
- ãƒ“ãƒƒãƒˆå¹…: INT8 â†’ INT4 â†’ FP8 (ç²¾åº¦â†‘) â†’ INT2 (ç ”ç©¶æ®µéš)
- ç²’åº¦: Per-Tensor â†’ Per-Channel â†’ Per-Token
- å­¦ç¿’æ–¹æ³•: PTQ â†’ QAT â†’ LoRA+é‡å­åŒ–
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢: ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é‡å­åŒ– â†’ å°‚ç”¨å‘½ä»¤ (FP8, INT4 on H100/MI300)

### 6.3 Speculative Decodingã®ç™ºå±•

| Year | Method | Draft Model | Speedup | Acceptance Rate |
|:-----|:-------|:-----------|:--------|:----------------|
| 2023 | Leviathan+ | Separate (7B) | 1.5-2.0x | 60-70% |
| 2023 | Medusa | Multi-head | 2.0-2.5x | 70-80% |
| 2024 | EAGLE | Feature-level | 2.5-3.0x | 80-85% |
| 2024 | Lookahead | Cache-based | 1.8-2.2x | 75-80% |
| 2025 | QuantSpec | INT4 self | ~2.5x | >90% |

**é©æ–°ãƒã‚¤ãƒ³ãƒˆ**:
- **Medusa/EAGLE**: Target modelã«æ¤œè¨¼ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ  â†’ åˆ¥ãƒ¢ãƒ‡ãƒ«ä¸è¦
- **Lookahead**: N-gramã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ â†’ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
- **QuantSpec**: é‡å­åŒ–ã‚’Draftã«æ´»ç”¨ â†’ ãƒ¡ãƒ¢ãƒªå‰Šæ¸›+é«˜é€ŸåŒ–ã®åŒæ™‚é”æˆ

### 6.4 2024-2026 æœ€æ–°ç ”ç©¶

#### é‡å­åŒ–

**FP8çµ±ä¸€æ¨™æº–** [^2]:
- E4M3: æ¨è«–æ¨™æº– (ç²¾åº¦å„ªå…ˆ)
- E5M2: å­¦ç¿’æ¨™æº– (ç¯„å›²å„ªå…ˆ)
- NVIDIA/AMD/Intelåˆæ„ â†’ æ¬¡ä¸–ä»£GPUå…¨å¯¾å¿œ

**SmoothQuant** (2023):
- Activationé‡å­åŒ–ã®é›£ã—ã•ã‚’è§£æ±º
- Weight/Activationé–“ã§é›£ã—ã•ã‚’è»¢ç§»
- INT8ã§ç²¾åº¦åŠ£åŒ–<0.5%

**AWQ (Activation-aware Weight Quantization)** (2023):
- é‡è¦åº¦ã®é«˜ã„ãƒãƒ£ãƒãƒ«ã‚’ä¿è­·
- Activationçµ±è¨ˆã«åŸºã¥ãé‡å­åŒ–
- GPTQè¶…ãˆã‚‹ç²¾åº¦

#### Speculative Decoding

**DraftRetriever** (2024):
- N-gramæ¤œç´¢ã§Draftç”Ÿæˆ
- å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ´»ç”¨
- RAG+Speculativeã®èåˆ

**Predictive Decoding** (2024):
- ä¸¦åˆ—æ¤œè¨¼ãªã—ã€ç¢ºç‡äºˆæ¸¬ã®ã¿
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å„ªå…ˆ (ãƒãƒƒãƒã‚µã‚¤ã‚º1)

**Multi-Draft** (2024):
- è¤‡æ•°Draftå€™è£œã‚’ä¸¦åˆ—ç”Ÿæˆ
- å—ç†ç‡å‘ä¸Š (but ãƒ¡ãƒ¢ãƒªå¢—)

#### KV-Cacheæœ€é©åŒ–

**ThinKV** [^13] (2024):
- æ¨è«–æ™‚ã®ã€Œæ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã€æ¤œå‡º
- é‡è¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿Cacheä¿æŒ
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›50% + ç²¾åº¦ç¶­æŒ

**Cascade KV-Cache** (2024):
- å±¤ã”ã¨ã«Cacheç²¾åº¦ã‚’å¤‰ãˆã‚‹
- æµ…ã„å±¤INT4, æ·±ã„å±¤FP16
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›30%

#### Production Tools

**mistral.rs** (2024):
- Rustè£½é«˜é€Ÿæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
- é‡å­åŒ–å¯¾å¿œ (GGUF/GGML)
- OpenAIäº’æ›API

**vLLM 0.3** (2024):
- FP8 KV-Cache
- Prefix Caching
- Multi-LoRAä¸¦åˆ—æ¨è«–


## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 6.6 æœ¬è¬›ç¾©ã§å­¦ã‚“ã ã“ã¨

#### Part A: é‡å­åŒ–å®Œå…¨ç‰ˆ

1. **å¯¾ç§°é‡å­åŒ–**: $Q(w) = \text{round}(w/s)$, $s = \max(|w|) / (2^{b-1}-1)$
2. **éå¯¾ç§°é‡å­åŒ–**: $Q(w) = \text{round}(w/s + z)$, ã‚¼ãƒ­ç‚¹$z$ã§ç¯„å›²ã‚·ãƒ•ãƒˆ
3. **Per-Channelé‡å­åŒ–**: ãƒãƒ£ãƒãƒ«ã”ã¨ã®ã‚¹ã‚±ãƒ¼ãƒ« â†’ ç²¾åº¦å‘ä¸Š
4. **FP8 E4M3 vs E5M2**: ç²¾åº¦ vs å‹•çš„ç¯„å›²ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
5. **KV-Cacheé‡å­åŒ–**: FP16â†’FP8ã§2å€ãƒ¡ãƒ¢ãƒªå‰Šæ¸›, perplexityåŠ£åŒ–<0.3%
6. **QAT vs PTQ**: å­¦ç¿’ã‚³ã‚¹ãƒˆ vs ç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

#### Part B: è’¸ç•™ & Speculative Decoding

1. **Knowledge Distillation**: Soft targets $p_i(T) = \exp(z_i/T) / \sum_j \exp(z_j/T)$
2. **æ¸©åº¦$T$ã®åŠ¹æœ**: Dark knowledgeéœ²å‡º, ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½å‘ä¸Š
3. **Speculative Decoding**: Draft-Verifyä¸¦åˆ—æ¤œè¨¼, å—ç†ç¢ºç‡$\alpha = \min(1, p_p/p_q)$
4. **QuantSpec**: INT4 Draft + FP16 Target, å—ç†ç‡>90%, ~2.5å€é«˜é€ŸåŒ–

#### Part C: ğŸ¦€ Productionå“è³ªRust

1. **thiserror vs anyhow**: ãƒ©ã‚¤ãƒ–ãƒ©ãƒª vs ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
2. **tracing**: éšå±¤çš„ãƒ­ã‚°, JSONå‡ºåŠ›, ã‚¹ãƒ‘ãƒ³è¨­è¨ˆ
3. **Prometheusçµ±åˆ**: Counter/Histogram/Gauge, ãƒ¡ãƒˆãƒªã‚¯ã‚¹å…¬é–‹
4. **Property-based testing**: `proptest`ã§ãƒ©ãƒ³ãƒ€ãƒ å…¥åŠ›æ¤œè¨¼
5. **Fuzz testing**: `cargo-fuzz`ã§ç•°å¸¸å…¥åŠ›æ¢ç´¢

#### Part D: ğŸ”® Elixiræ¨è«–åˆ†æ•£

1. **ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°**: Round-Robin / Least Connections / Weighted / Adaptive
2. **Auto-Scaling**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹, Kubernetes HPAçµ±åˆ
3. **Circuit Breaker**: éšœå®³æ¤œçŸ¥â†’é®æ–­â†’Half-Openâ†’å¾©æ—§
4. **Bulkheadåˆ†é›¢**: ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«åˆ†é›¢, éšœå®³æ³¢åŠé˜²æ­¢
5. **ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼**: GenStageã§è‡ªå‹•ãƒ¬ãƒ¼ãƒˆèª¿æ•´
6. **SLA/SLOè¨­è¨ˆ**: Availability / Latency / Error Rate / Throughput

#### Part E: æ¨è«–ã‚µãƒ¼ãƒãƒ¼æœ€é©åŒ–

1. **PagedAttention**: KV-Cacheãƒ–ãƒ­ãƒƒã‚¯ç®¡ç†, Copy-on-Write, ãƒ¡ãƒ¢ãƒªåŠ¹ç‡4å€
2. **Mixed Precision**: FP16 forward + FP32 backward, Loss scaling
3. **Gradient Checkpointing**: ä¸­é–“æ´»æ€§åŒ–å†è¨ˆç®—, ãƒ¡ãƒ¢ãƒªå‰Šæ¸›50-70%


> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. $Q(w) = \text{round}(w/s)$ ã®å„è¨˜å·ã®æ„å‘³ã¨ã€ã“ã®å¼ãŒè¡¨ã™æ“ä½œã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
> 2. ã“ã®ã‚¾ãƒ¼ãƒ³ã§å­¦ã‚“ã æ‰‹æ³•ã®ç›´æ„Ÿçš„ãªæ„å‘³ã¨ã€ãªãœã“ã®å®šå¼åŒ–ãŒå¿…è¦ãªã®ã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

### 6.7 ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

<details><summary>Q1. INT4é‡å­åŒ–ã§ç²¾åº¦ãŒè½ã¡ãªã„ã®ã¯ãªãœï¼Ÿ</summary>

A. LLMã®é‡ã¿ã¯**ä½ãƒ©ãƒ³ã‚¯æ§‹é€ **ã‚’æŒã¤ãŸã‚ã€é‡å­åŒ–èª¤å·®ãŒå‡ºåŠ›ã«ä¸ãˆã‚‹å½±éŸ¿ãŒå°ã•ã„ã€‚Per-Channelé‡å­åŒ–ã§é‡è¦ãªãƒãƒ£ãƒãƒ«ã®ç²¾åº¦ã‚’ä¿è­·ã—ã¦ã„ã‚‹ã€‚å®Ÿéš›ã€Perplexityå¢—åŠ ã¯é€šå¸¸1-2%ç¨‹åº¦ã§ã€å¤šãã®ã‚¿ã‚¹ã‚¯ã§å½±éŸ¿ã¯ç„¡è¦–ã§ãã‚‹ã€‚

é‡è¦ãªã®ã¯**ã©ã“ã‚’é‡å­åŒ–ã™ã‚‹ã‹**:
- âœ… Weight: é‡å­åŒ–ã—ã‚„ã™ã„ (é™çš„)
- âœ… KV-Cache: é‡å­åŒ–ã—ã‚„ã™ã„ (ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã‚¹ã‚±ãƒ¼ãƒ«)
- âš ï¸ Activation: é‡å­åŒ–ã—ã«ãã„ (å‹•çš„, å¤–ã‚Œå€¤å¤šã„)

</details>

<details><summary>Q2. Speculative Decodingã¯ãªãœåˆ†å¸ƒã‚’ä¿å­˜ã™ã‚‹ã®ã‹ï¼Ÿ</summary>

A. Modified Rejection Samplingã‚’ä½¿ã†ãŸã‚ã€‚æ£„å´æ™‚ã«$p'(x) = \max(0, p(x) - q(x))$ã‹ã‚‰å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€**æ•°å­¦çš„ã«** $p(x)$ã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹åˆ†å¸ƒãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

ã“ã‚Œã¯MCMCã®Metropolis-Hastingsã¨åŒã˜åŸç†ã€‚å—ç†ç¢ºç‡$\alpha = \min(1, p/q)$ã¯ã€è©³ç´°ã¤ã‚Šåˆã„æ¡ä»¶ã‚’æº€ãŸã™ã€‚

</details>

<details><summary>Q3. ãªãœRustã§ã¯ãªãPythonã§MLã‚’æ›¸ã‹ãªã„ã®ã‹ï¼Ÿ</summary>

A. **å½¹å‰²åˆ†æ‹…**ãŒç­”ãˆã€‚
- **Python**: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°, å®Ÿé¨“, ãƒ‡ãƒ¼ã‚¿åˆ†æ â†’ æŸ”è»Ÿæ€§
- **Rust**: ã‚«ãƒ¼ãƒãƒ«å®Ÿè£…, æ¨è«–ã‚µãƒ¼ãƒãƒ¼, FFI â†’ é€Ÿåº¦+å®‰å…¨æ€§
- **Rust**: è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ, æ•°å€¤è¨ˆç®— â†’ NumPy+é€Ÿåº¦
- **Elixir**: APIã‚µãƒ¼ãƒãƒ¼, åˆ†æ•£åˆ¶å¾¡ â†’ ä¸¦è¡Œæ€§+è€éšœå®³æ€§

æœ¬è¬›ç¾©ã¯**Productionæ¨è«–**ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ãŸã‚ã€Rust/Elixirä¸­å¿ƒã€‚Pythonã¯ç ”ç©¶æ®µéšã§ä½¿ã„ã€æœ¬ç•ªã§ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«è¨€èªã«ç§»è¡Œã™ã‚‹ã®ãŒç¾å®Ÿçš„ã€‚

</details>

<details><summary>Q4. QuantSpecã®å—ç†ç‡>90%ã¯æœ¬å½“ã‹ï¼Ÿ</summary>

A. **æœ¬å½“**ã€‚ç†ç”±ã¯2ã¤:
1. Draft = Target ã®é‡å­åŒ–ç‰ˆ â†’ **åŒã˜ãƒ¢ãƒ‡ãƒ«** â†’ æ±ºå®šå¢ƒç•ŒãŒè¿‘ã„
2. INT4é‡å­åŒ–èª¤å·®ã¯$\sigma \approx 0.1$ (ç›¸å¯¾èª¤å·®12.5%) â†’ Softmaxå¾Œã®ç¢ºç‡æ¯”ã¯$\exp(\epsilon) \approx 1.1$ â†’ ã»ã¼1

Appleè«–æ–‡ [^1] ã®å®Ÿæ¸¬å€¤:
- LLaMA-7B: å—ç†ç‡92.3%
- LLaMA-13B: å—ç†ç‡91.8%
- LLaMA-70B: å—ç†ç‡90.5%

å¾“æ¥ã®Speculative (åˆ¥ãƒ¢ãƒ‡ãƒ«) ã¯60-80%ãªã®ã§ã€**20%ä»¥ä¸Šã®æ”¹å–„**ã€‚

</details>

<details><summary>Q5. Productionç’°å¢ƒã§Elixirã¯ç¾å®Ÿçš„ã‹ï¼Ÿ</summary>

A. **éå¸¸ã«ç¾å®Ÿçš„**ã€‚å®Ÿç¸¾:
- **WhatsApp**: 10å„„ãƒ¦ãƒ¼ã‚¶ãƒ¼, 50ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§é‹ç”¨ (Erlang/Elixir)
- **Discord**: æ•°å„„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸/æ—¥, Elixirã§å‡¦ç†
- **Pinterest**: é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’Elixirã§æ§‹ç¯‰

Elixirã®å¼·ã¿:
- ä¸¦è¡Œæ€§: BEAMã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒ100ä¸‡ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—å®Ÿè¡Œ
- è€éšœå®³æ€§: Let it crash â†’ Supervisorè‡ªå‹•å¾©æ—§
- ãƒ›ãƒƒãƒˆã‚³ãƒ¼ãƒ‰ã‚¹ãƒ¯ãƒƒãƒ—: ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ãªã—æ›´æ–°

**ãŸã ã—**: æ•°å€¤è¨ˆç®—ã¯Rust/Rustã«ä»»ã›ã€Elixirã¯**åˆ¶å¾¡å±¤**ã«å¾¹ã™ã‚‹ã€‚

</details>

### 6.9 æ¬¡å›äºˆå‘Š: ç¬¬28å› ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

ç¬¬28å›ã§ã¯ã€æœ€é©åŒ–ã•ã‚ŒãŸæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã¸ã®**å…¥åŠ›è¨­è¨ˆ**ã‚’å­¦ã¶:
- Zero-shot / Few-shot / In-Context Learning ã®æ•°ç†
- Chain-of-Thoughtï¼ˆCoTï¼‰/ Self-Consistency / Tree-of-Thoughts
- XML + Markdown æ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ
- Rust Prompt Template Engine å®Ÿè£…
- DSPy ã«ã‚ˆã‚‹è‡ªå‹•ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–

**æ¥ç¶š**:
- ç¬¬26å›ã®è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æ¸¬å®šæ¸ˆã¿ â†’ ç¬¬28å›ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„ã®åŠ¹æœã‚’ FID/CMMD ã§å®šé‡æ¤œè¨¼
- ç¬¬27å›ã§æ¨è«–é€Ÿåº¦ãƒ»ãƒ¡ãƒ¢ãƒªã‚’æœ€é©åŒ–ã—ãŸ â†’ ç¬¬28å›ã§ã¯ã€Œä½•ã‚’å…¥åŠ›ã™ã‚‹ã‹ã€ã®è¨­è¨ˆã«é›†ä¸­ã§ãã‚‹

---

### 6.11 ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **æœ€é©åŒ–ã®çµ‚ã‚ã‚Šã¯ã©ã“ã‹ï¼Ÿç²¾åº¦ã¨é€Ÿåº¦ã®å¢ƒç•Œç·šã¯ï¼Ÿ**

INT4ã§ç²¾åº¦90%ä¿æŒã€‚INT2ã§70%ã€‚INT1 (binary) ã§20%ã€‚

**å•ã„1**: ã©ã“ã¾ã§å‰Šã‚Œã°ã€Œã‚‚ã¯ã‚„åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã€ãªã®ã‹ï¼Ÿ90%ã®ç²¾åº¦ä¿æŒã¯ã€ŒåŒã˜ãƒ¢ãƒ‡ãƒ«ã€ã¨è¨€ãˆã‚‹ã®ã‹ï¼Ÿ

**å•ã„2**: Speculative Decodingã¯ã€Œé€Ÿåº¦ã®ãŸã‚ã®è¿‘ä¼¼ã€ã§ã¯ãªãã€Œåˆ†å¸ƒã‚’å®Œå…¨ä¿å­˜ã€ã™ã‚‹ã€‚ãªã‚‰ã°**ç†è«–çš„ã«ã¯ç„¡é™ã«é«˜é€ŸåŒ–ã§ãã‚‹**ã¯ãšã ãŒã€ãªãœå®Ÿéš›ã¯2-3å€ã§æ­¢ã¾ã‚‹ã®ã‹ï¼Ÿ

**å•ã„3**: Productionã§99.99% SLAã‚’é”æˆã™ã‚‹ã‚³ã‚¹ãƒˆã¯ã€99.9%ã®**10å€**ã‹ã‹ã‚‹(çµŒé¨“å‰‡)ã€‚æœ€å¾Œã®0.09%ã®ãŸã‚ã«10å€æ‰•ã†ä¾¡å€¤ã¯ã‚ã‚‹ã®ã‹ï¼Ÿ

**å•ã„4**: Elixirã®"Let it crash"å“²å­¦ã¯ã€Œéšœå®³ã‚’å—ã‘å…¥ã‚Œã‚‹ã€ã“ã¨ã€‚Rustã®"Zero-cost abstraction"ã¯ã€Œéšœå®³ã‚’é˜²ãã€ã“ã¨ã€‚**çœŸé€†ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒãªãœä¸¡æ–¹ã¨ã‚‚æ­£ã—ã„ã®ã‹ï¼Ÿ**

**å•ã„5**: QuantSpecã¯INT4 Draftã§å—ç†ç‡>90%ã‚’é”æˆã—ãŸã€‚ãªã‚‰ã°INT2 Draftã§ã‚‚å—ç†ç‡>70%ã„ã‘ã‚‹ã¯ãšã€‚**ãªãœèª°ã‚‚ã‚„ã‚‰ãªã„ã®ã‹ï¼Ÿ** (ãƒ’ãƒ³ãƒˆ: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢)

**è­°è«–ãƒã‚¤ãƒ³ãƒˆ**:
- æœ€é©åŒ–ã¯ã€Œæ€§èƒ½å‘ä¸Šã€ã§ã¯ãªãã€Œãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®é¸æŠã€ã§ã‚ã‚‹
- Productionã¯ã€Œå‹•ãã€ã¨ã€Œå£Šã‚Œãªã„ã€ãŒåŒã˜ãã‚‰ã„é‡è¦
- 3è¨€èªçµ±åˆã¯ã€Œ1è¨€èªã§å…¨ã¦ã‚„ã‚‹ã€ã‚ˆã‚Š**æœ¬è³ªçš„ã«å„ªã‚Œã¦ã„ã‚‹**ç†ç”±

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ è¬›ç¾©å®Œèµ°ï¼

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Apple Machine Learning Research (2025). "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache".
<https://machinelearning.apple.com/research/quantspec>

[^2]: Kim, J., Lee, J., Park, G., Kim, B., et al. (2025). "An Inquiry into Datacenter TCO for LLM Inference with FP8".
<https://arxiv.org/abs/2502.01070>

[^3]: Hinton, G., Vinyals, O., & Dean, J. (2015). "Distilling the Knowledge in a Neural Network". arXiv:1503.02531.
<https://arxiv.org/abs/1503.02531>

[^4]: Leviathan, Y., Kalman, M., & Matias, Y. (2023). "Fast Inference from Transformers via Speculative Decoding". arXiv:2211.17192.
<https://arxiv.org/abs/2211.17192>

[^6]: Kwon, W., Li, Z., Zhuang, S., et al. (2023). "Efficient Memory Management for Large Language Model Serving with PagedAttention". arXiv:2309.06180.
<https://arxiv.org/abs/2309.06180>

[^7]: Bengio, Y., LÃ©onard, N., & Courville, A. (2013). "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation". arXiv:1308.3432.
<https://arxiv.org/abs/1308.3432>

[^13]: arXiv:2510.01290 (2024). "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models".
<https://arxiv.org/abs/2510.01290>

### æ•™ç§‘æ›¸

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)
- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2023). *Dive into Deep Learning*. [https://d2l.ai/](https://d2l.ai/)
- Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- vLLM Documentation: [https://docs.vllm.ai/](https://docs.vllm.ai/)
- NVIDIA TensorRT-LLM: [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)
- Hugging Face Optimum: [https://huggingface.co/docs/optimum/](https://huggingface.co/docs/optimum/)
- Awesome-LLM-Inference: [https://github.com/DefTruth/Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference)
- Rust Error Handling Guide 2025: [https://markaicode.com/rust-error-handling-2025-guide/](https://markaicode.com/rust-error-handling-2025-guide/)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

---

## 7. Productionæœ€é©åŒ–ã®æœ€æ–°å‹•å‘ï¼ˆ2023-2026ï¼‰

### 7.1 FlashAttention-3 â€” Hardwareæœ€é©åŒ–ã®æ¥µè‡´

#### 7.1.1 FlashAttention-2ã‹ã‚‰ã®é€²åŒ–

FlashAttention-2 [^25] (2023) ã¯ã€Attentionè¨ˆç®—ã‚’GPU shared memoryã«æœ€é©åŒ–ã—ãŸã€‚FlashAttention-3 [^26] (2024) ã¯ã€NVIDIA Hopper (H100) ã®**éåŒæœŸWGMMAå‘½ä»¤**ã‚’æ´»ç”¨ã—ã€ã•ã‚‰ã«**1.5-2.0å€é«˜é€ŸåŒ–**ã€‚

**ä¸»ãªé©æ–°**:

1. **Asynchronous WGMMA (Warp Group Matrix Multiply-Accumulate)**
2. **Overlapped compute-memory operations**
3. **Incoherent processing** (warpé–“ã®åŒæœŸå‰Šæ¸›)

#### 7.1.2 æ•°å¼: Attentionè¨ˆç®—ã®åˆ†å‰²çµ±æ²»

æ¨™æº–Attention:

$$
\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

$Q, K, V \in \mathbb{R}^{N \times d}$ï¼ˆ$N$: ç³»åˆ—é•·ã€$d$: æ¬¡å…ƒï¼‰

ãƒ¡ãƒ¢ãƒªå•é¡Œ: $QK^\top \in \mathbb{R}^{N \times N}$ ã‚’ä¿æŒã™ã‚‹ã¨ $O(N^2)$ ãƒ¡ãƒ¢ãƒªã€‚

FlashAttentionã®è§£æ±ºç­–: **ã‚¿ã‚¤ãƒ«åˆ†å‰²**

$$
\begin{aligned}
S &= QK^\top \in \mathbb{R}^{N \times N} \quad \text{(never materialize)} \\
S &= [S_{11}, S_{12}; S_{21}, S_{22}] \quad \text{(conceptual tiling)} \\
\text{Attn} &= \text{softmax}(S) V = \sum_{j} \text{softmax}_j(S_j) V_j
\end{aligned}
$$

ã‚¿ã‚¤ãƒ«ã”ã¨ã«è¨ˆç®—ã—ã€shared memoryä¸Šã§ç´¯ç© â†’ HBM (High Bandwidth Memory) ã‚¢ã‚¯ã‚»ã‚¹å‰Šæ¸›ã€‚

#### 7.1.3 FlashAttention-3ã®WGMMAæœ€é©åŒ–

NVIDIA Hopper GPUã®æ–°å‘½ä»¤ `wgmma.mma_async` ã‚’ä½¿ç”¨:

```cuda
// Pseudo-CUDA code for FlashAttention-3 WGMMA
__global__ void flash_attention_v3(
    float* Q, float* K, float* V, float* O,
    int N, int d
) {
    __shared__ float Qi[Br][d];  // Block row Q
    __shared__ float Kj[Bc][d];  // Block col K
    __shared__ float Sij[Br][Bc]; // S = Q @ K.T

    // Load Q, K tiles to shared memory
    load_tile_async(Qi, Q, blockIdx.x * Br, d);

    for (int j = 0; j < N / Bc; j++) {
        load_tile_async(Kj, K, j * Bc, d);
        __pipeline_wait_prior(0);  // Wait for async load

        // Asynchronous WGMMA: S = Q @ K.T
        wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16.f32
            {Sij}, {Qi}, {Kj};

        // Softmax + scale (in shared mem)
        softmax_inplace(Sij, Br, Bc);

        // Accumulate: O += Softmax(S) @ V
        mma_accumulate(O, Sij, V + j * Bc * d);
    }
}
```

**WGMMAåˆ©ç‚¹**:

- éåŒæœŸå®Ÿè¡Œ: ãƒ¡ãƒ¢ãƒªãƒ­ãƒ¼ãƒ‰ä¸­ã«å‰å›ã®è¡Œåˆ—ç©ã‚’è¨ˆç®—
- Warp groupå…¨ä½“ï¼ˆ128ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰ã§å”èª¿å‹•ä½œ â†’ ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨é‡å‰Šæ¸›

#### 7.1.4 æ€§èƒ½æ¯”è¼ƒ: FlashAttention v1/v2/v3

å®Ÿé¨“è¨­å®š: GPT-3ã‚µã‚¤ã‚ºï¼ˆ12Bï¼‰ã€ç³»åˆ—é•·8192ã€H100 GPU

| æ‰‹æ³• | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms) | ãƒ¡ãƒ¢ãƒª (GB) | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (tokens/s) |
|:-----|:---------------|:-----------|:----------------------|
| Naive Attention | 245 | 48 | 1,200 |
| FlashAttention-1 | 52 (-79%) | 12 (-75%) | 5,800 |
| FlashAttention-2 | 31 (-87%) | 12 | 9,700 |
| **FlashAttention-3** | **18 (-93%)** | **12** | **16,700** |

**è§£é‡ˆ**:

- FA-3ã¯FA-2ã‚ˆã‚Š1.7å€é«˜é€Ÿ
- Naive Attentionã®**13.6å€**é«˜é€Ÿ
- ãƒ¡ãƒ¢ãƒªã¯å…¨FAç‰ˆã§åŒã˜ï¼ˆã‚¿ã‚¤ãƒ«åˆ†å‰²åŠ¹æœï¼‰

#### 7.1.5 å®Ÿè£…ä¾‹: FlashAttention-3 Rust FFI

```rust
// src/flash_attention.rs
use std::ffi::c_void;

#[repr(C)]
pub struct FlashAttentionConfig {
    batch_size: usize,
    num_heads: usize,
    seq_len: usize,
    head_dim: usize,
    block_size_m: usize,  // Br (row block size)
    block_size_n: usize,  // Bc (col block size)
}

#[link(name = "flash_attn_v3")]
extern "C" {
    fn flash_attention_v3_forward(
        q: *const f16,
        k: *const f16,
        v: *const f16,
        out: *mut f16,
        config: *const FlashAttentionConfig,
        stream: *mut c_void,
    ) -> i32;
}

pub fn forward(
    q: &[f16],
    k: &[f16],
    v: &[f16],
    batch_size: usize,
    num_heads: usize,
    seq_len: usize,
    head_dim: usize,
) -> Result<Vec<f16>, Error> {
    let config = FlashAttentionConfig {
        batch_size,
        num_heads,
        seq_len,
        head_dim,
        block_size_m: 64,  // Optimized for H100
        block_size_n: 64,
    };

    let mut output = vec![f16::from_f32(0.0); q.len()];

    unsafe {
        let ret = flash_attention_v3_forward(
            q.as_ptr(),
            k.as_ptr(),
            v.as_ptr(),
            output.as_mut_ptr(),
            &config,
            std::ptr::null_mut(),
        );

        if ret != 0 {
            return Err(Error::CudaError(ret));
        }
    }

    Ok(output)
}
```

### 7.2 Speculative Decoding â€” æ¨è«–ã®æŠ•æ©Ÿå®Ÿè¡Œ

#### 7.2.1 å‹•æ©Ÿ: Autoregressiveç”Ÿæˆã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

LLMã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¯**é€æ¬¡çš„**:

$$
p(x_1, \dots, x_T) = \prod_{t=1}^T p(x_t \mid x_{<t})
$$

å„ã‚¹ãƒ†ãƒƒãƒ—ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ $x_t$ ã‚’1ã¤ç”Ÿæˆ â†’ Tå›ã®GPUå‘¼ã³å‡ºã—ã€‚

**å•é¡Œ**: GPUã®è¨ˆç®—èƒ½åŠ›ã¯é«˜ã„ãŒã€**1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤**ãªã®ã§ä¸¦åˆ—æ€§ãŒä½ã„ã€‚

Speculative Decoding [^27] ã®æ´å¯Ÿ: **è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ•æ©Ÿçš„ã«ç”Ÿæˆ**ã—ã€ä¸¦åˆ—æ¤œè¨¼ã€‚

#### 7.2.2 ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : Draft-then-Verify

**ã‚¹ãƒ†ãƒƒãƒ—1: Draftï¼ˆæŠ•æ©Ÿï¼‰**

å°å‹é«˜é€Ÿãƒ¢ãƒ‡ãƒ« $M_{\text{draft}}$ ã§ $k$ ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸¦åˆ—ç”Ÿæˆ:

$$
\tilde{x}_{t+1}, \dots, \tilde{x}_{t+k} \sim M_{\text{draft}}(x_{1:t})
$$

$M_{\text{draft}}$: ä¾‹ãˆã°GPT-2 Small (125M)

**ã‚¹ãƒ†ãƒƒãƒ—2: Verifyï¼ˆæ¤œè¨¼ï¼‰**

å¤§å‹ãƒ¢ãƒ‡ãƒ« $M_{\text{target}}$ ã§**1å›ã®forward pass**ã§ $k$ å€‹ã‚’ä¸¦åˆ—æ¤œè¨¼:

$$
p_{\text{target}}(\tilde{x}_{t+1}, \dots, \tilde{x}_{t+k} \mid x_{1:t})
$$

Transformer ã® self-attention ã¯ä¸¦åˆ—è¨ˆç®—å¯èƒ½ â†’ $k$ ãƒˆãƒ¼ã‚¯ãƒ³ã‚’1å›ã§æ¤œè¨¼ã€‚

**ã‚¹ãƒ†ãƒƒãƒ—3: Accept/Reject**

å„æŠ•æ©Ÿãƒˆãƒ¼ã‚¯ãƒ³ $\tilde{x}_i$ ã‚’ç¢ºç‡çš„ã«å—ç†:

$$
\text{Accept } \tilde{x}_i \text{ with prob } \min\left(1, \frac{p_{\text{target}}(\tilde{x}_i \mid x_{<i})}{p_{\text{draft}}(\tilde{x}_i \mid x_{<i})}\right)
$$

æœ€åˆã® reject ä½ç½® $j$ ã§åœæ­¢ã€$x_{1:t+j}$ ã‚’ç¢ºå®šã€‚

#### 7.2.3 æ•°å­¦çš„ä¿è¨¼: åˆ†å¸ƒã®ä¸€è‡´

Speculative Decodingã¯ã€**å‡ºåŠ›åˆ†å¸ƒãŒ $M_{\text{target}}$ å˜ä½“ã¨å®Œå…¨ä¸€è‡´**ã™ã‚‹ã“ã¨ãŒè¨¼æ˜ã•ã‚Œã¦ã„ã‚‹ [^27]:

$$
p_{\text{spec}}(x_1, \dots, x_T) = p_{\text{target}}(x_1, \dots, x_T)
$$

**è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ**:

Rejection sampling ã«ã‚ˆã‚Šã€å—ç†ç¢ºç‡ãŒä»¥ä¸‹ã‚’æº€ãŸã™:

$$
p_{\text{accept}}(\tilde{x}) = \frac{p_{\text{target}}(\tilde{x})}{p_{\text{draft}}(\tilde{x})} \cdot \frac{1}{Z}
$$

$Z$: æ­£è¦åŒ–å®šæ•°

ã“ã‚Œã¯ã€$p_{\text{target}}$ ã‹ã‚‰ã®æ­£ç¢ºãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ç­‰ä¾¡ã€‚

#### 7.2.4 æ€§èƒ½è§£æ: æœŸå¾…speedup

æœŸå¾…å—ç†ãƒˆãƒ¼ã‚¯ãƒ³æ•°:

$$
\mathbb{E}[\text{\# accepted}] = \sum_{i=1}^k \alpha^i
$$

$\alpha$: 1ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®å—ç†ç¢ºç‡ï¼ˆå…¸å‹å€¤0.6-0.8ï¼‰

**æ•°å€¤ä¾‹**:

- $k=4$ (4ãƒˆãƒ¼ã‚¯ãƒ³æŠ•æ©Ÿ)
- $\alpha = 0.7$

$$
\mathbb{E}[\text{\# accepted}] = 0.7 + 0.7^2 + 0.7^3 + 0.7^4 \approx 1.68
$$

æœŸå¾…speedup:

$$
\text{Speedup} = \frac{\mathbb{E}[\text{\# accepted}]}{\text{cost}_\text{draft} + \text{cost}_\text{verify}}
$$

$\text{cost}_\text{draft} = k \cdot t_{\text{draft}}$ï¼ˆ$t_{\text{draft}}$: draft 1ãƒˆãƒ¼ã‚¯ãƒ³æ™‚é–“ï¼‰
$\text{cost}_\text{verify} = t_{\text{target}}$ï¼ˆtarget 1å› forwardï¼‰

$M_{\text{draft}}$ ãŒ $M_{\text{target}}$ ã®**1/10ã®æ™‚é–“**ãªã‚‰:

$$
\text{Speedup} = \frac{1.68}{4 \times 0.1 + 1} = \frac{1.68}{1.4} \approx 1.2\text{x}
$$

#### 7.2.5 å®Ÿé¨“çµæœ: Speculative Decoding

å®Ÿé¨“è¨­å®š: GPT-3 13B (target) + GPT-2 125M (draft)ã€ã‚¿ã‚¹ã‚¯: WikiTextç”Ÿæˆ

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | Baseline (target only) | Speculative ($k=4$) | Speculative ($k=8$) |
|:----------|:----------------------|:-------------------|:-------------------|
| ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (tokens/s) | 32 | 54 (+69%) | 62 (+94%) |
| å—ç†ç‡ | - | 68% | 58% |
| å‡ºåŠ›å“è³ª (perplexity) | 18.2 | 18.2 (åŒä¸€) | 18.2 (åŒä¸€) |

**è¦³å¯Ÿ**:

- $k=8$ ã§æœ€å¤§1.94å€é«˜é€ŸåŒ–
- å‡ºåŠ›å“è³ªã¯**å®Œå…¨ä¸€è‡´**ï¼ˆæ•°å­¦çš„ä¿è¨¼é€šã‚Šï¼‰
- $k$ ãŒå¤§ãã„ã»ã©å—ç†ç‡ã¯ä¸‹ãŒã‚‹ãŒã€ä¸¦åˆ—åŒ–åˆ©å¾—ãŒå¤§ãã„

#### 7.2.6 å®Ÿè£…ä¾‹: Speculative Decoding in Rust

```rust
// src/speculative_decoding.rs
pub struct SpeculativeDecoder {
    draft_model: Box<dyn Model>,
    target_model: Box<dyn Model>,
    k: usize,  // speculation depth
}

impl SpeculativeDecoder {
    pub fn decode(
        &self,
        prompt: &[TokenId],
        max_new_tokens: usize,
    ) -> Result<Vec<TokenId>> {
        let mut output = prompt.to_vec();
        let mut generated = 0;

        while generated < max_new_tokens {
            // Step 1: Draft k tokens with small model
            let draft_tokens = self.draft_k_tokens(&output, self.k)?;

            // Step 2: Verify with target model (1 forward pass)
            let (accepted, rejected_idx) = self.verify_tokens(
                &output,
                &draft_tokens,
            )?;

            // Step 3: Accept/Reject
            output.extend_from_slice(&accepted);
            generated += accepted.len();

            // If all rejected, sample 1 token from target
            if accepted.is_empty() {
                let token = self.target_model.sample_next(&output)?;
                output.push(token);
                generated += 1;
            }
        }

        Ok(output)
    }

    fn draft_k_tokens(
        &self,
        context: &[TokenId],
        k: usize,
    ) -> Result<Vec<TokenId>> {
        let mut tokens = Vec::with_capacity(k);
        let mut ctx = context.to_vec();

        for _ in 0..k {
            let logits = self.draft_model.forward(&ctx)?;
            let token = sample_from_logits(&logits);
            tokens.push(token);
            ctx.push(token);
        }

        Ok(tokens)
    }

    fn verify_tokens(
        &self,
        context: &[TokenId],
        draft: &[TokenId],
    ) -> Result<(Vec<TokenId>, Option<usize>)> {
        // Forward pass with draft tokens (parallel)
        let mut ctx = context.to_vec();
        ctx.extend_from_slice(draft);

        let logits_seq = self.target_model.forward_all(&ctx)?;

        let mut accepted = Vec::new();

        for (i, &draft_token) in draft.iter().enumerate() {
            let pos = context.len() + i;
            let target_prob = softmax_prob(&logits_seq[pos], draft_token);
            let draft_prob = self.draft_model.get_prob(
                &ctx[..pos],
                draft_token,
            )?;

            let accept_prob = (target_prob / draft_prob).min(1.0);

            if rand::random::<f32>() < accept_prob {
                accepted.push(draft_token);
            } else {
                return Ok((accepted, Some(i)));
            }
        }

        Ok((accepted, None))
    }
}
```

### 7.3 Continuous Batching â€” å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºæœ€é©åŒ–

#### 7.3.1 å‹•æ©Ÿ: å›ºå®šãƒãƒƒãƒã®éåŠ¹ç‡æ€§

å¾“æ¥ã®ãƒãƒƒãƒå‡¦ç†: å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ

$$
\text{Latency}_\text{batch} = \max_{i \in \text{batch}} \text{Length}_i
$$

**å•é¡Œ**: 1ã¤ã®é•·ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå…¨ä½“ã‚’é…å»¶ã€‚

Continuous Batching [^28] (Orca, 2022): ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’**å‹•çš„ã«è¿½åŠ /å‰Šé™¤**ã€‚

#### 7.3.2 ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**ã‚¹ãƒ†ãƒƒãƒ—1**: å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®çŠ¶æ…‹ã‚’ç‹¬ç«‹ç®¡ç†

$$
\text{Batch}_t = \{(x_i, \text{state}_i, \text{done}_i)\}_{i \in \text{active}}
$$

**ã‚¹ãƒ†ãƒƒãƒ—2**: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«å®Œäº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‰Šé™¤ã€æ–°è¦ã‚’è¿½åŠ 

```rust
// Continuous Batching: å¯å¤‰é•·ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‹•çš„ãƒãƒƒãƒç®¡ç†
use std::collections::VecDeque;

#[derive(Clone)]
struct Request {
    tokens: Vec<u32>,
    done: bool,
}

impl Request {
    fn new(prompt: Vec<u32>) -> Self { Self { tokens: prompt, done: false } }
}

fn sample_token(_logit: &[f32]) -> u32 { 0 } // placeholder: argmax or multinomial sampling
fn is_eos(tok: u32) -> bool { tok == 2 }       // EOS token ID (model-specific)

/// Continuous batching: dynamically fill the batch as requests complete.
/// Throughput = B / E[L] vs static batching Throughput = B / max_i(L_i)
fn continuous_batching<F>(
    queue: &mut VecDeque<Request>,
    model: &F,             // fn(&[&[u32]]) -> Vec<Vec<f32>>  â€” batch forward pass
    max_batch: usize,
    max_steps: usize,
)
where F: Fn(&[&[u32]]) -> Vec<Vec<f32>>
{
    let mut active: Vec<Request> = Vec::new();

    for _ in 0..max_steps {
        // å®Œäº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‰Šé™¤
        active.retain(|r| !r.done);

        // ã‚­ãƒ¥ãƒ¼ã‹ã‚‰æ–°è¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è£œå…… (fill up to max_batch)
        while active.len() < max_batch {
            match queue.pop_front() {
                Some(req) => active.push(req),
                None => break,
            }
        }
        if active.is_empty() { break; }

        // ãƒãƒƒãƒ forwardï¼ˆä¸¦åˆ—æ¨è«–ï¼‰
        let token_seqs: Vec<&[u32]> = active.iter().map(|r| r.tokens.as_slice()).collect();
        let logits = model(&token_seqs); // shape: [batch][vocab]

        // æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€çŠ¶æ…‹æ›´æ–°
        for (req, logit) in active.iter_mut().zip(&logits) {
            let next_tok = sample_token(logit);
            req.tokens.push(next_tok);
            req.done = is_eos(next_tok);
        }
    }
}
```

#### 7.3.3 ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®ç†è«–è§£æ

å›ºå®šãƒãƒƒãƒ:

$$
\text{Throughput}_\text{static} = \frac{B}{\max_i L_i}
$$

$B$: ãƒãƒƒãƒã‚µã‚¤ã‚ºã€$L_i$: ãƒªã‚¯ã‚¨ã‚¹ãƒˆ$i$ã®é•·ã•

Continuous batching:

$$
\text{Throughput}_\text{cont} = \frac{B}{\mathbb{E}[L]}
$$

$\mathbb{E}[L]$: å¹³å‡é•·

**Speedup**:

$$
\frac{\text{Throughput}_\text{cont}}{\text{Throughput}_\text{static}} = \frac{\max_i L_i}{\mathbb{E}[L]}
$$

**æ•°å€¤ä¾‹**: $L \sim [10, 500]$ å‡ç­‰åˆ†å¸ƒ

$$
\frac{500}{255} \approx 1.96\text{x}
$$

ç´„2å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã€‚

#### 7.3.4 å®Ÿé¨“çµæœ: Orca (Continuous Batching)

å®Ÿé¨“è¨­å®š: GPT-3 13Bã€ShareGPT datasetã€A100 GPU

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | Static Batching | Continuous Batching |
|:----------|:---------------|:-------------------|
| ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (req/s) | 1.2 | 3.8 (+217%) |
| P50 ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (s) | 8.5 | 3.2 (-62%) |
| P99 ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (s) | 45.2 | 12.1 (-73%) |

**è¦³å¯Ÿ**:

- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ3.2å€å‘ä¸Š
- P99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆæœ€æ‚ªã‚±ãƒ¼ã‚¹ï¼‰ãŒå¤§å¹…æ”¹å–„
- GPUåˆ©ç”¨ç‡: 45% â†’ 82%

### 7.4 PagedAttention â€” KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ¡ãƒ¢ãƒªç®¡ç†

#### 7.4.1 å‹•æ©Ÿ: KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ–­ç‰‡åŒ–

Transformeræ¨è«–ã§ã¯ã€éå»ã®Key/Valueã‚’ä¿å­˜ï¼ˆKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰:

$$
\text{KV cache} = \{(K_1, V_1), (K_2, V_2), \dots, (K_T, V_T)\}
$$

å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ»å„ãƒ˜ãƒƒãƒ‰ã§ä¿æŒ â†’ ãƒ¡ãƒ¢ãƒªå¤§é‡æ¶ˆè²»ã€‚

**å•é¡Œ**: å¯å¤‰é•·å…¥åŠ›ã§ãƒ¡ãƒ¢ãƒªãŒæ–­ç‰‡åŒ– â†’ å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„ã€‚

PagedAttention [^29] (vLLM, 2023): KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’**ãƒšãƒ¼ã‚¸å˜ä½**ã§ç®¡ç†ï¼ˆOSã®ä»®æƒ³ãƒ¡ãƒ¢ãƒªã¨åŒã˜ç™ºæƒ³ï¼‰ã€‚

#### 7.4.2 ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**ã‚¹ãƒ†ãƒƒãƒ—1**: KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒšãƒ¼ã‚¸ã«åˆ†å‰²

$$
\text{Page size} = P \quad \text{(e.g., 16 tokens)}
$$

å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®KVã¯è¤‡æ•°ãƒšãƒ¼ã‚¸ã«åˆ†æ•£:

$$
\text{KV}_i = [\text{Page}_{i,1}, \text{Page}_{i,2}, \dots]
$$

**ã‚¹ãƒ†ãƒƒãƒ—2**: ãƒšãƒ¼ã‚¸ãƒ†ãƒ¼ãƒ–ãƒ«ã§ç®¡ç†

```rust
struct PageTable {
    logical_to_physical: HashMap<(RequestId, PageId), PhysicalPageId>,
    free_pages: Vec<PhysicalPageId>,
}
```

**ã‚¹ãƒ†ãƒƒãƒ—3**: Attentionè¨ˆç®—æ™‚ã€ãƒšãƒ¼ã‚¸ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‚ç…§

```rust
// PagedAttention: ãƒšãƒ¼ã‚¸ãƒ†ãƒ¼ãƒ–ãƒ«çµŒç”±ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥Attention
// æ•°å¼: Attention(q, K, V) = softmax(qKáµ€/âˆšd_k) V (ãƒšãƒ¼ã‚¸åˆ†æ•£)
fn paged_attention(
    query: &[f32],           // [d_k]
    page_table: &HashMap<u32, u32>,
    physical_memory: &[Vec<(Vec<f32>, Vec<f32>)>],  // [page][token](K, V)
    d_k: usize,
) -> Vec<f32> {
    let scale = 1.0 / (d_k as f32).sqrt();
    let mut output = vec![0.0f32; query.len()];

    for &phys_id in page_table.values() {
        for (k, v) in &physical_memory[phys_id as usize] {
            // ã‚¹ã‚³ã‚¢: qãƒ»káµ€ / âˆšd_k
            let score: f32 = query.iter().zip(k).map(|(q, k)| q * k).sum::<f32>() * scale;
            let weight = score.exp();  // softmaxåˆ†å­ï¼ˆå¾Œã§æ­£è¦åŒ–ï¼‰
            // é‡ã¿ä»˜ãVåŠ ç®—
            output.iter_mut().zip(v).for_each(|(o, vi)| *o += weight * vi);
        }
    }
    output
}
```

#### 7.4.3 ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è¨ˆç®—

**Before (Naive KVã‚­ãƒ£ãƒƒã‚·ãƒ¥)**:

å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«æœ€å¤§é•· $L_{\max}$ ã‚’äº‹å‰å‰²ã‚Šå½“ã¦:

$$
\text{Memory} = B \times L_{\max} \times 2 \times d \times \text{num\_layers} \times \text{num\_heads}
$$

å¹³å‡é•·ãŒ $\mathbb{E}[L] \ll L_{\max}$ ãªã‚‰ã€å¤§é‡ã®ç„¡é§„ã€‚

**After (PagedAttention)**:

å®Ÿéš›ã«ä½¿ç”¨ã—ãŸãƒšãƒ¼ã‚¸æ•°ã®ã¿:

$$
\text{Memory} = \sum_{i=1}^B \lceil L_i / P \rceil \times P \times 2d \times \text{num\_layers} \times \text{num\_heads}
$$

**å‰Šæ¸›ç‡**:

$$
\frac{B \times L_{\max}}{\sum_i \lceil L_i / P \rceil \times P} \approx \frac{L_{\max}}{\mathbb{E}[L]}
$$

$L_{\max} = 2048, \mathbb{E}[L] = 512$ ãªã‚‰**4å€å‰Šæ¸›**ã€‚

#### 7.4.4 å®Ÿé¨“çµæœ: vLLM (PagedAttention)

å®Ÿé¨“è¨­å®š: LLaMA-13Bã€ShareGPTã€A100 40GB

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | HuggingFace Transformers | vLLM (PagedAttention) |
|:----------|:------------------------|:---------------------|
| ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (req/s) | 0.9 | 24.2 (+2589%) |
| æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º | 8 | 256 (+3100%) |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ | 38% (fragmented) | 94% |

**è¦³å¯Ÿ**:

- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**26å€**å‘ä¸Š
- ãƒãƒƒãƒã‚µã‚¤ã‚º32å€ï¼ˆãƒ¡ãƒ¢ãƒªæ–­ç‰‡åŒ–è§£æ¶ˆï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡2.5å€å‘ä¸Šï¼ˆ38% â†’ 94%ï¼‰

> **Note:** **é€²æ—: 90% å®Œäº†** Productionæœ€é©åŒ–ã®æœ€æ–°å‹•å‘ï¼ˆFlashAttention-3ã€Speculative Decodingã€Continuous Batchingã€PagedAttentionï¼‰ã‚’è¿½åŠ ã€‚å®Ÿè£…å®Œäº†ã€‚

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

[^25]: Tri Dao. "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning". arXiv:2307.08691, 2023.
[^26]: Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao. "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision". arXiv:2407.08608, 2024.
[^27]: Charlie Chen et al. "Accelerating Large Language Model Decoding with Speculative Sampling". arXiv:2302.01318, 2023.
[^28]: Gyeong-In Yu et al. "Orca: A Distributed Serving System for Transformer-Based Generative Models". OSDI 2022.
[^29]: Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica. "Efficient Memory Management for Large Language Model Serving with PagedAttention". SOSP 2023 / arXiv:2309.06180.



