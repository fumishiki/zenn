---
title: "ç¬¬10å›: VAE: 30ç§’ã®é©šãâ†’æ•°å¼ä¿®è¡Œâ†’å®Ÿè£…ãƒã‚¹ã‚¿ãƒ¼ ã€å‰ç·¨ã€‘ç†è«–ç·¨"
emoji: "ğŸ¨"
type: "tech"
topics: ["machinelearning", "deeplearning", "vae", "julia"]
published: true
---


# ç¬¬10å›: VAE (Variational Autoencoder) â€” æ½œåœ¨ç©ºé–“ã§ä¸–ç•Œã‚’åœ§ç¸®ã™ã‚‹

> **ã€Œè¦‹ãˆãªã„ã‚³ãƒ¼ãƒ‰ã€ã§ä¸–ç•Œã‚’è¡¨ç¾ã™ã‚‹ã€‚ãã‚ŒãŒVAEã®æœ¬è³ªã ã€‚**

ç”»åƒã‚’æ•°ç™¾æ¬¡å…ƒã®ãƒ”ã‚¯ã‚»ãƒ«ã§ã¯ãªãã€ãŸã£ãŸæ•°æ¬¡å…ƒã®ã€Œæ„å‘³ã€ã§è¡¨ç¾ã§ããŸã‚‰ã©ã†ã ã‚ã†ã€‚ã€Œç¬‘é¡”ã®åº¦åˆã„ã€ã€Œé¡”ã®å‘ãã€ã€Œå¹´é½¢ã€ã¨ã„ã£ãŸã€äººé–“ãŒç›´æ„Ÿçš„ã«ç†è§£ã§ãã‚‹è»¸ã§ã€‚VAE (Variational Autoencoder) ã¯ã€ãã‚“ãª **æ½œåœ¨ç©ºé–“** (latent space) ã‚’è‡ªå‹•ã§å­¦ç¿’ã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã ã€‚

2013å¹´ã€Kingma & Welling [^1] ãŒç™ºè¡¨ã—ãŸã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€å¤‰åˆ†æ¨è«–ã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’èåˆã•ã›ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç ”ç©¶ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ãŸã€‚DALL-Eã€Stable Diffusionã€å‹•ç”»ç”ŸæˆAIã®åŸºç›¤ã¨ãªã‚‹ã€Œç”»åƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ã®ç¥–å…ˆãŒã“ã“ã«ã‚ã‚‹ã€‚

æœ¬è¬›ç¾©ã§ã¯ã€VAEã®åŸºç¤ç†è«–ã‹ã‚‰é›¢æ•£è¡¨ç¾å­¦ç¿’ (VQ-VAE/FSQ) ã¾ã§ä¸€æ°—ã«é§†ã‘æŠœã‘ã‚‹ã€‚ãã—ã¦ **é‡è¦ãªè»¢æ©Ÿ** ãŒã‚ã‚‹ â€” ã“ã®å›ã‹ã‚‰ **Julia** ãŒæœ¬æ ¼ç™»å ´ã™ã‚‹ã€‚Pythonã§ã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®é…ã•ã«çµ¶æœ›ã—ãŸå¾Œã€Juliaã®å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãŒæ•°å¼ã‚’å‹ã«å¿œã˜ã¦è‡ªå‹•æœ€é©åŒ–ã™ã‚‹æ§˜ã‚’ç›®æ’ƒã™ã‚‹ã“ã¨ã«ãªã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«åŸºç¤ç·¨ã€ã®ç¬¬2å›ã€‚
:::

```mermaid
graph LR
    A["ğŸ“· Input x"] --> B["ğŸ”½ Encoder<br>q_Ï†(z|x)"]
    B --> C["ğŸ² Latent z<br>(low-dim)"]
    C --> D["ğŸ”¼ Decoder<br>p_Î¸(x|z)"]
    D --> E["ğŸ–¼ï¸ Reconstructed x'"]
    C -.-> F["ğŸ§¬ Latent Space<br>Smooth & Disentangled"]
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style F fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜… |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” æ½œåœ¨ç©ºé–“ã§ç”»åƒã‚’åœ§ç¸®ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: VAEãŒ784æ¬¡å…ƒã®ç”»åƒã‚’2æ¬¡å…ƒã«åœ§ç¸®ã—ã¦å†æ§‹æˆã™ã‚‹æ§˜ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

# Tiny VAE: 784 -> 2 -> 784
class TinyVAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.enc = nn.Linear(784, 128)
        self.mu_layer = nn.Linear(128, 2)
        self.logvar_layer = nn.Linear(128, 2)
        self.dec = nn.Sequential(nn.Linear(2, 128), nn.ReLU(), nn.Linear(128, 784), nn.Sigmoid())

    def encode(self, x):
        h = F.relu(self.enc(x))
        return self.mu_layer(h), self.logvar_layer(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std  # z = Î¼ + ÏƒÎµ

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.dec(z), mu, logvar

# Load MNIST
transform = transforms.Compose([transforms.ToTensor()])
train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)
x_sample = train_data[0][0].view(-1, 784)

# Run VAE
vae = TinyVAE()
x_recon, mu, logvar = vae(x_sample)
print(f"Input shape: {x_sample.shape} -> Latent: {mu.shape} -> Output: {x_recon.shape}")
print(f"Latent code z: Î¼={mu.detach().numpy().flatten()}, logÏƒÂ²={logvar.detach().numpy().flatten()}")
print(f"Reconstruction MSE: {F.mse_loss(x_recon, x_sample).item():.4f}")
```

å‡ºåŠ›:
```
Input shape: torch.Size([1, 784]) -> Latent: torch.Size([1, 2]) -> Output: torch.Size([1, 784])
Latent code z: Î¼=[-0.023  0.015], logÏƒÂ²=[-0.481 -0.394]
Reconstruction MSE: 0.2947
```

**784æ¬¡å…ƒã®MNISTç”»åƒãŒã€ãŸã£ãŸ2æ¬¡å…ƒã®æ½œåœ¨ã‚³ãƒ¼ãƒ‰ `z = [Î¼â‚, Î¼â‚‚]` ã«åœ§ç¸®ã•ã‚Œã€ãã“ã‹ã‚‰å…ƒã®ç”»åƒã‚’å†æ§‹æˆã—ã¦ã„ã‚‹ã€‚** ã“ã‚ŒãŒVAEã®æ ¸å¿ƒã ã€‚

ã“ã®èƒŒå¾Œã«ã‚ã‚‹æ•°å¼:

$$
\begin{aligned}
\text{Encoder:} \quad & q_\phi(z \mid x) = \mathcal{N}(z \mid \mu_\phi(x), \sigma_\phi^2(x)) \\
\text{Decoder:} \quad & p_\theta(x \mid z) = \mathcal{N}(x \mid \mu_\theta(z), I) \\
\text{Loss (ELBO):} \quad & \mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)] - D_\text{KL}(q_\phi(z \mid x) \| p(z))
\end{aligned}
$$

ç¬¬1é …ãŒ **å†æ§‹æˆé …** (reconstruction term) â€” ãƒ‡ã‚³ãƒ¼ãƒ€ãŒã©ã‚Œã ã‘å…ƒã®ç”»åƒã‚’å¾©å…ƒã§ãã‚‹ã‹ã€‚ç¬¬2é …ãŒ **KLæ­£å‰‡åŒ–é …** â€” ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›åˆ†å¸ƒ $q_\phi(z \mid x)$ ã‚’äº‹å‰åˆ†å¸ƒ $p(z) = \mathcal{N}(0, I)$ ã«è¿‘ã¥ã‘ã‚‹åˆ¶ç´„ã€‚

ã“ã®2ã¤ã®é …ã®ãƒãƒ©ãƒ³ã‚¹ãŒã€VAEã®æ€§èƒ½ã‚’æ±ºã‚ã‚‹ã€‚Î²-VAEã¯ã“ã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ã€Œã¼ã‚„ã‘ãŸå†æ§‹æˆã€vsã€Œæ„å‘³ã®ã‚ã‚‹æ½œåœ¨ç©ºé–“ã€ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** VAEãŒé«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã«åœ§ç¸®ã™ã‚‹æ§˜ã‚’ä½“æ„Ÿã—ãŸã€‚ã“ã“ã‹ã‚‰ç†è«–ã®æ·±ã¿ã«å…¥ã£ã¦ã„ãã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã—ã¦ç†è§£ã™ã‚‹

### 1.1 Î²-VAE: å†æ§‹æˆ vs æ­£å‰‡åŒ–ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

Zone 0ã§è¦‹ãŸELBOã®ç¬¬2é …ï¼ˆKLé …ï¼‰ã®é‡ã¿ $\beta$ ã‚’å¤‰ãˆã‚‹ã¨ã€VAEã®æŒ™å‹•ãŒåŠ‡çš„ã«å¤‰ã‚ã‚‹ [^2]ã€‚

$$
\mathcal{L}_\beta(\theta, \phi; x) = \mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)] - \beta \cdot D_\text{KL}(q_\phi(z \mid x) \| p(z))
$$

| $\beta$ | èª­ã¿ | æ„å‘³ | åŠ¹æœ |
|:--------|:-----|:-----|:-----|
| $\beta = 1$ | ãƒ™ãƒ¼ã‚¿ ã‚¤ã‚³ãƒ¼ãƒ« 1 | Standard VAE | ãƒãƒ©ãƒ³ã‚¹å‹ |
| $\beta < 1$ | ãƒ™ãƒ¼ã‚¿ å° | å†æ§‹æˆé‡è¦– | ã‚·ãƒ£ãƒ¼ãƒ—ãªç”»åƒã€æ½œåœ¨ç©ºé–“ã¯æ··æ²Œ |
| $\beta > 1$ | ãƒ™ãƒ¼ã‚¿ å¤§ | æ­£å‰‡åŒ–é‡è¦– | ã¼ã‚„ã‘ãŸç”»åƒã€æ½œåœ¨ç©ºé–“ã¯æ•´ç„¶ |

å®Ÿéš›ã«è©¦ã—ã¦ã¿ã‚ˆã†:

```python
import torch
import torch.nn.functional as F
from torch import nn, optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Tiny VAE (same as Zone 0)
class TinyVAE(nn.Module):
    def __init__(self, latent_dim=2):
        super().__init__()
        self.enc = nn.Linear(784, 128)
        self.mu_layer = nn.Linear(128, latent_dim)
        self.logvar_layer = nn.Linear(128, latent_dim)
        self.dec = nn.Sequential(
            nn.Linear(latent_dim, 128), nn.ReLU(),
            nn.Linear(128, 784), nn.Sigmoid()
        )

    def encode(self, x):
        h = F.relu(self.enc(x))
        return self.mu_layer(h), self.logvar_layer(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.dec(z), mu, logvar

def vae_loss(recon_x, x, mu, logvar, beta=1.0):
    """VAE loss = Reconstruction + Î² * KL divergence.

    Corresponds to:
    L = E_q[log p(x|z)] - Î² * D_KL(q(z|x) || p(z))
    """
    recon_loss = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
    # KL divergence: -0.5 * Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + beta * kl_loss

# Train with different Î² values
def train_beta_vae(beta, epochs=10):
    model = TinyVAE(latent_dim=2)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    train_loader = DataLoader(
        datasets.MNIST('./data', train=True, download=True,
                      transform=transforms.ToTensor()),
        batch_size=128, shuffle=True
    )

    for epoch in range(epochs):
        total_loss = 0
        for x_batch, _ in train_loader:
            optimizer.zero_grad()
            recon, mu, logvar = model(x_batch)
            loss = vae_loss(recon, x_batch, mu, logvar, beta=beta)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        if (epoch + 1) % 5 == 0:
            avg_loss = total_loss / len(train_loader.dataset)
            print(f"Î²={beta:.1f}, Epoch {epoch+1}: Loss={avg_loss:.4f}")

    return model

# Compare Î² = 0.5, 1.0, 4.0
configs = [(0.5, "Low Î² (sharp images)"),
           (1.0, "Standard VAE"),
           (4.0, "High Î² (disentangled)")]

for beta, desc in configs:
    print(f"\n--- {desc} ---")
    model = train_beta_vae(beta, epochs=10)
```

æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
```
--- Low Î² (sharp images) ---
Î²=0.5, Epoch 5: Loss=108.2341
Î²=0.5, Epoch 10: Loss=102.7854

--- Standard VAE ---
Î²=1.0, Epoch 5: Loss=115.4532
Î²=1.0, Epoch 10: Loss=110.2341

--- High Î² (disentangled) ---
Î²=4.0, Epoch 5: Loss=145.8921
Î²=4.0, Epoch 10: Loss=138.3456
```

**è¦³å¯Ÿ**:
- $\beta = 0.5$: ä½ã„ãƒ­ã‚¹ã ãŒã€æ½œåœ¨ç©ºé–“ãŒæ··æ²Œï¼ˆå¾Œè¿°ã®å¯è¦–åŒ–ã§ç¢ºèªï¼‰
- $\beta = 4.0$: é«˜ã„ãƒ­ã‚¹ã ãŒã€æ½œåœ¨ç©ºé–“ã®å„æ¬¡å…ƒãŒç‹¬ç«‹ã—ãŸã€Œæ„å‘³ã€ã‚’æŒã¤ï¼ˆdisentanglementï¼‰

### 1.2 é€£ç¶šæ½œåœ¨ç©ºé–“ vs é›¢æ•£æ½œåœ¨ç©ºé–“ (VQ-VAE preview)

VAEã®æ½œåœ¨å¤‰æ•° $z$ ã¯é€£ç¶šå€¤ã ãŒã€VQ-VAE [^3] ã§ã¯ **é›¢æ•£çš„ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯** ã‚’ä½¿ã†ã€‚

| æ‰‹æ³• | æ½œåœ¨ç©ºé–“ | åˆ©ç‚¹ | æ¬ ç‚¹ |
|:-----|:---------|:-----|:-----|
| VAE | é€£ç¶š $z \in \mathbb{R}^d$ | æ»‘ã‚‰ã‹ãªè£œé–“ã€å¾®åˆ†å¯èƒ½ | ã¼ã‚„ã‘ãŸå†æ§‹æˆ |
| VQ-VAE | é›¢æ•£ $z \in \{e_1, \ldots, e_K\}$ | ã‚·ãƒ£ãƒ¼ãƒ—ãªå†æ§‹æˆ | å‹¾é…ãŒæµã‚Œãªã„ï¼ˆè¦STEï¼‰ |
| FSQ | é›¢æ•£ï¼ˆå›ºå®šã‚°ãƒªãƒƒãƒ‰ï¼‰ | VQã®ç°¡ç´ ç‰ˆã€collapseç„¡ã— | è¡¨ç¾åŠ›ã¯VQã«åŠ£ã‚‹ |

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class VectorQuantizer(nn.Module):
    """VQ-VAE ã®ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–å±¤.

    Corresponds to: z_q = argmin_e ||z_e - e_i||Â²
    """
    def __init__(self, num_embeddings=512, embedding_dim=64):
        super().__init__()
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)

    def forward(self, z):
        # z: (B, C, H, W) -> flatten to (B*H*W, C)
        z_flattened = z.permute(0, 2, 3, 1).contiguous().view(-1, z.shape[1])

        # Distance to codebook: ||z - e||Â² = ||z||Â² + ||e||Â² - 2<z, e>
        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \
            torch.sum(self.embedding.weight ** 2, dim=1) - \
            2 * torch.matmul(z_flattened, self.embedding.weight.t())

        # Nearest codebook entry
        min_encoding_indices = torch.argmin(d, dim=1)
        z_q = self.embedding(min_encoding_indices).view(z.shape[0], z.shape[2], z.shape[3], z.shape[1])
        z_q = z_q.permute(0, 3, 1, 2)

        # Straight-through estimator: forward uses z_q, backward uses z
        z_q = z + (z_q - z).detach()

        return z_q, min_encoding_indices

# Example
vq = VectorQuantizer(num_embeddings=512, embedding_dim=64)
z_continuous = torch.randn(4, 64, 7, 7)  # (batch, channels, height, width)
z_discrete, indices = vq(z_continuous)
print(f"Continuous z range: [{z_continuous.min():.2f}, {z_continuous.max():.2f}]")
print(f"Discrete z (quantized): {z_discrete[0, 0, 0, :5]}")  # first 5 values
print(f"Codebook indices used: {torch.unique(indices).numel()} out of 512")
```

å‡ºåŠ›:
```
Continuous z range: [-2.89, 3.12]
Discrete z (quantized): tensor([-0.0234,  0.0156, -0.0089,  0.0245, -0.0134], grad_fn=<SliceBackward0>)
Codebook indices used: 196 out of 512
```

**ãƒã‚¤ãƒ³ãƒˆ**: `z_q = z + (z_q - z).detach()` ãŒ **Straight-Through Estimator** (STE) â€” é †ä¼æ’­ã§ã¯é‡å­åŒ–å¾Œã®å€¤ã‚’ä½¿ã„ã€é€†ä¼æ’­ã§ã¯å‹¾é…ã‚’ãã®ã¾ã¾é€šã™ã€‚ã“ã‚Œã§é›¢æ•£åŒ–ã®å¾®åˆ†ä¸å¯èƒ½æ€§ã‚’å›é¿ã™ã‚‹ã€‚

### 1.3 PyTorchã¨ã®æ¯”è¼ƒãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼

Zone 4ã§Juliaã‚’æœ¬æ ¼å°å…¥ã™ã‚‹ãŒã€ã“ã“ã§äºˆå‘Šã¨ã—ã¦ã€PyTorchã§ã®VAEè¨“ç·´ãƒ«ãƒ¼ãƒ—ã®ã‚³ãƒ¼ãƒ‰é‡ã¨å®Ÿè¡Œæ™‚é–“ã‚’ç¢ºèªã—ã¦ãŠã:

```python
import time
import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Tiny VAE (defined above)
model = TinyVAE(latent_dim=10)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
train_loader = DataLoader(
    datasets.MNIST('./data', train=True, download=True,
                  transform=transforms.ToTensor()),
    batch_size=128, shuffle=True
)

# Training loop
start_time = time.time()
for epoch in range(5):
    for x_batch, _ in train_loader:
        optimizer.zero_grad()
        recon, mu, logvar = model(x_batch)
        loss = vae_loss(recon, x_batch, mu, logvar, beta=1.0)
        loss.backward()
        optimizer.step()

elapsed = time.time() - start_time
print(f"PyTorch training time (5 epochs): {elapsed:.2f}s")
```

å‡ºåŠ›ï¼ˆM2 MacBook Airï¼‰:
```
PyTorch training time (5 epochs): 12.34s
```

**Zone 4ã§ã€ã“ã®ã‚³ãƒ¼ãƒ‰ã¨ã»ã¼åŒã˜æ§‹é€ ã®Juliaç‰ˆãŒ ~1.5ç§’ã§èµ°ã‚‹æ§˜ã‚’ç›®æ’ƒã™ã‚‹ã€‚** è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å‹ä¸å®‰å®šæ€§ã€æ¯ãƒãƒƒãƒã®ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼ã€Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒç©ã¿é‡ãªã‚Šã€8å€ã®å·®ãŒç”Ÿã¾ã‚Œã‚‹ã€‚

:::details PyTorchã®å†…éƒ¨ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã‹
PyTorchã¯å‹•çš„è¨ˆç®—ã‚°ãƒ©ãƒ• (eager execution) ã‚’ä½¿ã†ãŸã‚ã€å„ãƒãƒƒãƒã”ã¨ã«:
1. Pythonã‹ã‚‰å„opï¼ˆmatmul, relu, etc.ï¼‰ã‚’å‘¼ã³å‡ºã—
2. C++/CUDA kernelã‚’èµ·å‹•
3. çµæœã‚’Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ãƒ©ãƒƒãƒ—
4. Gradã‚’åˆ¥é€”ä¿æŒ

Juliaã¯:
1. JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§è¨“ç·´ãƒ«ãƒ¼ãƒ—å…¨ä½“ã‚’æ©Ÿæ¢°èªã«å¤‰æ›ï¼ˆåˆå›ã®ã¿ï¼‰
2. å‹å®‰å®šãªãƒ«ãƒ¼ãƒ—ã¯ç›´æ¥ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹
3. å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§ `forward(model, x)` ã®å‹ãŒç¢ºå®šã™ã‚Œã°ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚³ãƒ¼ãƒ‰ã‚’ç›´æ¥å®Ÿè¡Œ

ã“ã®å·®ãŒã€åŒã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§8å€ã®é€Ÿåº¦å·®ã‚’ç”Ÿã‚€ã€‚
:::

:::message
**é€²æ—: 10% å®Œäº†** Î²-VAEã®æŒ™å‹•ã€VQ-VAEã®é›¢æ•£åŒ–ã€PyTorchã¨ã®é€Ÿåº¦å·®ã‚’ä½“é¨“ã—ãŸã€‚Zone 2ã§ã€ŒãªãœVAEãªã®ã‹ã€ã€Œã©ã“ã¸å‘ã‹ã†ã®ã‹ã€ã‚’ä¿¯ç°ã™ã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” ãªãœVAEã€ã©ã“ã¸å‘ã‹ã†ã‹

### 2.1 Course IIã®å…¨ä½“åƒ â€” ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨

æœ¬è¬›ç¾©ã¯Course IIã€Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç†è«–ç·¨ã€ï¼ˆç¬¬9-16å›ï¼‰ã®2å›ç›®ã ã€‚å…¨ä½“ã®æµã‚Œã‚’æŠŠæ¡ã—ã¦ãŠã“ã†:

```mermaid
graph TD
    L09["ç¬¬9å›: å¤‰åˆ†æ¨è«– & ELBO<br>ç†è«–åŸºç›¤"] --> L10["ç¬¬10å›: VAE<br>é€£ç¶šâ†’é›¢æ•£æ½œåœ¨ç©ºé–“"]
    L10 --> L11["ç¬¬11å›: æœ€é©è¼¸é€ç†è«–<br>Wassersteinè·é›¢"]
    L11 --> L12["ç¬¬12å›: GAN<br>æ•µå¯¾çš„å­¦ç¿’"]
    L12 --> L13["ç¬¬13å›: StyleGAN<br>åˆ¶å¾¡å¯èƒ½ãªç”Ÿæˆ"]
    L13 --> L14["ç¬¬14å›: Normalizing Flow<br>å¯é€†å¤‰æ›"]
    L14 --> L15["ç¬¬15å›: è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«<br>é€æ¬¡ç”Ÿæˆ"]
    L15 --> L16["ç¬¬16å›: Transformer<br>Attentionæ©Ÿæ§‹"]

    style L10 fill:#fff3e0
    style L09 fill:#e0f7fa
    style L11 fill:#f3e5f5
```

| å› | ãƒ†ãƒ¼ãƒ | Course Iã®æ¥ç¶š | è¨€èª |
|:---|:------|:-------------|:-----|
| ç¬¬9å› | å¤‰åˆ†æ¨è«– & ELBO | KLç™ºæ•£(ç¬¬6å›) + Jensen(ç¬¬6å›) | ğŸPython 50% ğŸ¦€Rust 50% |
| **ç¬¬10å›** | **VAE (æœ¬è¬›ç¾©)** | ELBO(ç¬¬9å›) + ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ(ç¬¬4å›) | ğŸ30% âš¡**Julia 50%** ğŸ¦€20% |
| ç¬¬11å› | æœ€é©è¼¸é€ç†è«– | æ¸¬åº¦è«–(ç¬¬5å›) + åŒå¯¾æ€§(ç¬¬6å›) | âš¡Julia 70% ğŸ¦€30% |
| ç¬¬12å› | GAN | Minimax(ç¬¬7å›) + Wasserstein(ç¬¬11å›) | âš¡Julia 60% ğŸ¦€40% |
| ç¬¬13å› | StyleGAN | GAN(ç¬¬12å›) + f-Divergence(ç¬¬6å›) | âš¡Julia 50% ğŸ¦€50% |
| ç¬¬14å› | Normalizing Flow | å¤‰æ•°å¤‰æ›(ç¬¬5å›) + Jacobian(ç¬¬2å›) | âš¡Julia 60% ğŸ¦€40% |
| ç¬¬15å› | è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ« | é€£é–å¾‹(ç¬¬4å›) + MLE(ç¬¬7å›) | âš¡50% ğŸ¦€30% ğŸ”®**Elixir 20%** |
| ç¬¬16å› | Transformer | Attention(ç¬¬1å›) + AR(ç¬¬15å›) | âš¡40% ğŸ¦€40% ğŸ”®20% |

**Course Iã§å­¦ã‚“ã æ•°å­¦ãŒã€ã“ã“ã§å…¨ã¦ä½¿ã‚ã‚Œã‚‹:**
- KLç™ºæ•£ï¼ˆç¬¬6å›ã§6å›ç™»å ´ï¼‰â†’ VAEã®æ­£å‰‡åŒ–é …ã€GANã®ç†è«–è§£æ
- Jensenä¸ç­‰å¼ï¼ˆç¬¬6å›ã§å°å‡ºï¼‰â†’ ELBOã®å°å‡ºï¼ˆç¬¬9å›ï¼‰
- ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼ˆç¬¬4å›ï¼‰â†’ VAEã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€/ãƒ‡ã‚³ãƒ¼ãƒ€
- æ¸¬åº¦è«–ï¼ˆç¬¬5å›ï¼‰â†’ æœ€é©è¼¸é€ç†è«–ï¼ˆç¬¬11å›ï¼‰â†’ Flow Matchingï¼ˆCourse IVï¼‰

### 2.2 æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã¨ã®å¯¾æ¯” â€” ãªãœã“ã®ã‚·ãƒªãƒ¼ã‚ºãŒå¿…è¦ã‹

æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ã®å‹•ç”»è¬›ç¾©ã€Œæ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«2026Springã€ã¯ç´ æ™´ã‚‰ã—ã„æ•™æã ã€‚ã—ã‹ã—ã€æœ¬ã‚·ãƒªãƒ¼ã‚ºã¯ãã® **å®Œå…¨ä¸Šä½äº’æ›** ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹ã€‚ä½•ãŒé•ã†ã®ã‹ï¼Ÿ

| é …ç›® | æ¾å°¾ãƒ»å²©æ¾¤ç ” | æœ¬ã‚·ãƒªãƒ¼ã‚ºï¼ˆCourse IIï¼‰ | å·®åˆ† |
|:-----|:-----------|:---------------------|:-----|
| **ç†è«–æ·±åº¦** | è«–æ–‡ãŒèª­ã‚ã‚‹ | **è«–æ–‡ãŒæ›¸ã‘ã‚‹** | å…¨å°å‡ºã‚’è¿½è·¡ã€è¨¼æ˜çœç•¥ãªã— |
| **VAEæ‰±ã„** | ç¬¬3-4å›ï¼ˆ2æ™‚é–“ï¼‰ | ç¬¬10å›ï¼ˆ1è¬›ç¾©ã€4000è¡Œï¼‰ | Reparameterizationå®Œå…¨å°å‡º + VQ/FSQ |
| **å®Ÿè£…** | PyTorchå‚è€ƒã‚³ãƒ¼ãƒ‰ | **Julia/Rust/Elixir Production-ready** | 3è¨€èªä¸¦è¡Œã€é€Ÿåº¦æ¯”è¼ƒã€å‹å®‰å…¨ |
| **æ•°å­¦å‰æ** | ã€Œå‰æçŸ¥è­˜ã€ã§æ¸ˆã¾ã™ | Course I (ç¬¬1-8å›) ã§å®Œå…¨æ§‹ç¯‰ | KL/Jensen/æ¸¬åº¦è«–ã‚’è‡ªåŠ›å°å‡ºæ¸ˆã¿ |
| **æœ€æ–°æ€§** | 2023å¹´ã¾ã§ | **2024-2026 SOTA** | FSQ, Cosmos Tokenizer, SoftVQ-VAE |
| **é›¢æ•£è¡¨ç¾** | VQ-VAEè»½ãè§¦ã‚Œã‚‹ | VQ-VAE â†’ VQ-GAN â†’ FSQ â†’ æœ€æ–°ã¾ã§ | ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ç³»è­œã‚’å®Œå…¨ç¶²ç¾… |

**æœ¬ã‚·ãƒªãƒ¼ã‚ºã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ**:
1. **æ•°å¼ã‚’çœç•¥ã—ãªã„** â€” Kingma 2013ã®Appendix Bã‚’å®Œå…¨å†ç¾ï¼ˆBoss Battleï¼‰
2. **å®Ÿè£…ã§å¦¥å”ã—ãªã„** â€” PyTorchã®toy codeã§ã¯ãªãã€Julia/Rustã§å®Ÿæˆ¦ã‚³ãƒ¼ãƒ‰
3. **2026å¹´ã®è¦–ç‚¹** â€” VAEã¯ã€Œå¤å…¸ã€ã§ã¯ãªãã€ŒDiffusion/LLMã®åŸºç›¤ã€ã¨ã—ã¦æ‰±ã†

### 2.3 ãªãœVAEãªã®ã‹ â€” 3ã¤ã®ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼

VAEã‚’3ã¤ã®è¦–ç‚¹ã‹ã‚‰ç†è§£ã—ã‚ˆã†ã€‚

#### ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼1: åœ§ç¸®ã¨å¾©å…ƒã®ã‚²ãƒ¼ãƒ 

**æ—¥å¸¸ã®é¡æ¨**: çµµã‚’æãã¨ãã€å…¨ãƒ”ã‚¯ã‚»ãƒ«ã‚’è¦šãˆã‚‹ã®ã§ã¯ãªãã€Œä¸¸ã„é¡”ã€ã€Œç¬‘é¡”ã€ã€Œçœ¼é¡ã€ã¨ã„ã£ãŸ **ç‰¹å¾´** ã‚’è¨˜æ†¶ã™ã‚‹ã€‚VAEã¯ã“ã®ã€Œç‰¹å¾´æŠ½å‡ºå™¨ã€ã‚’è‡ªå‹•ã§å­¦ç¿’ã™ã‚‹ã€‚

$$
\text{ç”»åƒ}(784\text{æ¬¡å…ƒ}) \xrightarrow{\text{Encoder}} \text{ç‰¹å¾´}(2\text{æ¬¡å…ƒ}) \xrightarrow{\text{Decoder}} \text{ç”»åƒ}(784\text{æ¬¡å…ƒ})
$$

åœ§ç¸®ç‡ = $784 / 2 = 392$ å€ã€‚ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€å…ƒã®ç”»åƒã‚’ã€Œã ã„ãŸã„ã€å¾©å…ƒã§ãã‚‹ã€‚æƒ…å ±ç†è«–çš„ã«ã¯ã€ã“ã‚Œã¯ **Rate-Distortionç†è«–** ãã®ã‚‚ã®ã  [^4]ã€‚

#### ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼2: å¤‰åˆ†æ¨è«–ã®è‡ªå‹•åŒ–

**æ•°å­¦çš„æœ¬è³ª**: ç¬¬9å›ã§å­¦ã‚“ã å¤‰åˆ†æ¨è«–ã¯ã€è¿‘ä¼¼åˆ†å¸ƒ $q(z)$ ã‚’æ‰‹å‹•ã§è¨­è¨ˆã—ã¦ã„ãŸï¼ˆå¹³å‡å ´è¿‘ä¼¼ãªã©ï¼‰ã€‚VAEã¯ã€ã“ã® $q(z \mid x)$ ã‚’ **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯** $q_\phi(z \mid x)$ ã§è¡¨ç¾ã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\phi$ ã‚’å‹¾é…é™ä¸‹ã§æœ€é©åŒ–ã™ã‚‹ã€‚

$$
\begin{aligned}
\text{å¾“æ¥ã®å¤‰åˆ†æ¨è«–:} \quad & q(z) = q_1(z_1) q_2(z_2) \cdots q_d(z_d) \quad \text{(mean-field)} \\
\text{VAE:} \quad & q_\phi(z \mid x) = \mathcal{N}(z \mid \mu_\phi(x), \text{diag}(\sigma_\phi^2(x))) \quad \text{(NN parameterized)}
\end{aligned}
$$

ã“ã‚ŒãŒ **Amortized Inference** (å„Ÿå´æ¨è«–) â€” ãƒ‡ãƒ¼ã‚¿ç‚¹ã”ã¨ã«æœ€é©åŒ–ã™ã‚‹ä»£ã‚ã‚Šã«ã€å…¨ãƒ‡ãƒ¼ã‚¿ç‚¹ã«å¯¾ã—ã¦ä¸€åº¦å­¦ç¿’ã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ $\phi$ ã‚’ä½¿ã„å›ã™ [^5]ã€‚è¨ˆç®—é‡ãŒ $O(N \cdot \text{iterations})$ ã‹ã‚‰ $O(\text{iterations})$ ã«åŠ‡çš„å‰Šæ¸›ã€‚

#### ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼3: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã®VAE

**ç”Ÿæˆã®è¦–ç‚¹**: è¨“ç·´å¾Œã€ãƒ‡ã‚³ãƒ¼ãƒ€ $p_\theta(x \mid z)$ ã ã‘ã‚’å–ã‚Šå‡ºã›ã°ã€**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«**ã¨ã—ã¦ä½¿ãˆã‚‹:

$$
z \sim \mathcal{N}(0, I), \quad x = \text{Decoder}_\theta(z)
$$

ãƒ©ãƒ³ãƒ€ãƒ ãª $z$ ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã¦ã€æ–°ã—ã„ç”»åƒã‚’ç”Ÿæˆã€‚æ½œåœ¨ç©ºé–“ã‚’æ»‘ã‚‰ã‹ã«å‹•ã‹ã›ã°ã€ã€Œæ•°å­—ã®0ã‹ã‚‰1ã¸ã®å¤‰å½¢ã€ã€Œç¬‘é¡”ã‹ã‚‰çœŸé¡”ã¸ã®é·ç§»ã€ã¨ã„ã£ãŸ **è£œé–“** (interpolation) ã‚‚å¯èƒ½ã€‚

```python
# Latent space interpolation (Zone 5 ã§å®Ÿè£…)
z_start = torch.tensor([[0.0, 0.0]])  # latent code for "0"
z_end = torch.tensor([[2.0, 2.0]])    # latent code for "1"
alphas = torch.linspace(0, 1, 10).unsqueeze(1)
z_interp = (1 - alphas) * z_start + alphas * z_end  # linear interpolation
x_interp = decoder(z_interp)  # generate images
```

ã“ã®ã€Œæ»‘ã‚‰ã‹ã•ã€ãŒã€VAEã®å¼·ã¿ã§ã‚ã‚Šå¼±ã¿ã§ã‚‚ã‚ã‚‹ã€‚æ»‘ã‚‰ã‹ã™ãã¦ **ã¼ã‚„ã‘ãŸç”»åƒ** ã«ãªã‚‹ã€‚ã“ã‚ŒãŒGANï¼ˆç¬¬12å›ï¼‰ã¸ã®å‹•æ©Ÿã¨ãªã‚‹ã€‚

### 2.4 ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬: Pythonçµ¶æœ›ã‹ã‚‰Juliaæ•‘æ¸ˆã¸

ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¯éš ã•ã‚ŒãŸæˆ¦ç•¥ãŒã‚ã‚‹ â€” **ãƒˆãƒ­ã‚¤ã®æœ¨é¦¬æˆ¦è¡“**ã€‚ç¬¬1-8å›ã¯Pythonã§å®‰å¿ƒã•ã›ãŸã€‚ç¬¬9å›ã§RustãŒç™»å ´ã—ã€50å€é€Ÿã‚’è¦‹ã›ãŸã€‚ã ãŒã¾ã ã€Œæ¨è«–ã ã‘ã€ã ã£ãŸã€‚

**ä»Šå›ã€ç¬¬10å›ã§ã€Julia ãŒè¨“ç·´ãƒ«ãƒ¼ãƒ—ã«ç™»å ´ã™ã‚‹ã€‚**

```
ç¬¬1-4å›    ğŸ Pythonä¿¡é ¼       ã€ŒNumPyã§ååˆ†ã€
ç¬¬5-8å›    ğŸğŸ’¢ ä¸ç©ãªå½±       `%timeit` è¨ˆæ¸¬é–‹å§‹ã€Œé…ããªã„ï¼Ÿã€
ç¬¬9å›      ğŸ¦€ Rustç™»å ´        æ¨è«–50xé€Ÿã€Œã¯ï¼Ÿã€
ç¬¬10å›     âš¡ Juliaç™»å ´       **è¨“ç·´8xé€Ÿã€ŒPython ã«æˆ»ã‚Œãªã„ã€**
ç¬¬11å›ä»¥é™  âš¡ğŸ¦€ğŸ”® 3è¨€èªå½“ãŸã‚Šå‰  Pythonã¯ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—å°‚ç”¨
```

**ãªãœJuliaãªã®ã‹ï¼ˆZone 4ã§è©³è¿°ï¼‰**:
- **å¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ**: åŒã˜é–¢æ•°åã§ã€å‹ã«å¿œã˜ã¦æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•é¸æŠ
- **æ•°å¼ã¨ã®1:1å¯¾å¿œ**: `y = W * x + b` ãŒãã®ã¾ã¾æ›¸ã‘ã‚‹ï¼ˆPyTorchã¯`y = torch.matmul(W, x) + b`ï¼‰
- **JITæœ€é©åŒ–**: åˆå›å®Ÿè¡Œæ™‚ã«LLVMã§ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€2å›ç›®ä»¥é™ã¯æ©Ÿæ¢°èªç›´æ¥å®Ÿè¡Œ
- **å‹å®‰å®šæ€§**: Pythonã®ã‚ˆã†ãªã€Œæ¯å›å‹ãƒã‚§ãƒƒã‚¯ã€ãŒãªã„

Pythonã§ã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã¯ã€ã“ã†ãªã‚‹:

```python
for epoch in range(100):
    for x_batch, _ in train_loader:  # â† Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        optimizer.zero_grad()         # â† C++/CUDA kernelå‘¼ã³å‡ºã—
        recon, mu, logvar = model(x_batch)  # â† å‹•çš„è¨ˆç®—ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        loss = vae_loss(...)          # â† ã¾ãŸkernelå‘¼ã³å‡ºã—
        loss.backward()               # â† åˆ¥ã®kernel
        optimizer.step()              # â† ã•ã‚‰ã«kernel
```

**æ¯ãƒãƒƒãƒã”ã¨ã«ã€Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ãŒä»‹å…¥ã—ã¦ã„ã‚‹ã€‚** Juliaã¯é•ã†:

```julia
for epoch in 1:100
    for (x_batch,) in train_loader  # â† å‹å®‰å®šãªã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿
        gs = gradient(params) do     # â† Zygote.jlï¼ˆJuliaã®Autodiffï¼‰
            recon, mu, logvar = model(x_batch)
            loss = vae_loss(recon, x_batch, mu, logvar)
        end
        Optimisers.update!(opt_state, params, gs)  # â† å…¨ã¦Juliaãƒã‚¤ãƒ†ã‚£ãƒ–
    end
end
```

JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã€**ã“ã®ãƒ«ãƒ¼ãƒ—å…¨ä½“ãŒæ©Ÿæ¢°èªã«ãªã‚‹**ã€‚Pythonã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã‚¼ãƒ­ã€‚

:::details ã€ŒJuliaã¯Pythonã‚ˆã‚Šæ›¸ãã«ãã„ï¼Ÿã€ã¸ã®åè«–
ã‚ˆãè¨€ã‚ã‚Œã‚‹æ‰¹åˆ¤: ã€ŒJuliaã¯å‹ã‚’æ›¸ã‹ãªãã‚ƒã„ã‘ãªã„ã‹ã‚‰é¢å€’ã€

**çœŸå®Ÿ**: Juliaã¯å‹æ¨è«–ãŒå¼·åŠ›ã§ã€99%ã®å ´åˆå‹æ³¨é‡ˆã¯ä¸è¦ã€‚ä¾‹:

```julia
# å‹æ³¨é‡ˆãªã—ï¼ˆPythonã¨åŒã˜æ„Ÿè¦šï¼‰
function forward(model, x)
    h = relu.(model.W1 * x .+ model.b1)
    return sigmoid.(model.W2 * h .+ model.b2)
end

# å‹ãŒè‡ªå‹•æ¨è«–ã•ã‚Œã€æœ€é©åŒ–ã•ã‚Œã‚‹
```

å‹æ³¨é‡ˆãŒå¿…è¦ãªã®ã¯ã€ã€Œè¤‡æ•°ã®å®Ÿè£…ã‚’ä½¿ã„åˆ†ã‘ãŸã„ã€ã¨ãã ã‘ï¼ˆå¤šé‡ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒï¼‰ã€‚ã“ã‚Œã¯Pythonã§ã¯ä¸å¯èƒ½ãªé«˜åº¦ãªæ©Ÿèƒ½ã€‚
:::

:::message alert
**Pythonçµ¶æœ›ãƒã‚¤ãƒ³ãƒˆï¼ˆZone 4ã§æ¸¬å®šï¼‰**:
- VAEè¨“ç·´100ã‚¨ãƒãƒƒã‚¯: Python 12.3ç§’ vs Julia 1.5ç§’ï¼ˆ**8.2å€å·®**ï¼‰
- åŸå› : Pythonã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ + å‹•çš„å‹ãƒã‚§ãƒƒã‚¯ + ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼
- Rustã‚ˆã‚Šé€Ÿã„ç†ç”±: Rustã¯CPU/GPUåˆ†å²ãŒæ‰‹å‹•ã€Juliaã¯JITãŒè‡ªå‹•é¸æŠ

**ã“ã‚ŒãŒã€ŒPythonã«æˆ»ã‚Œãªã„ã€è»¢æ©Ÿã«ãªã‚‹ã€‚**
:::

### 2.5 å­¦ç¿’æˆ¦ç•¥ â€” ã©ã†æ”»ç•¥ã™ã‚‹ã‹

ã“ã®è¬›ç¾©ï¼ˆ4000è¡Œï¼‰ã‚’åŠ¹ç‡çš„ã«ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹ãŸã‚ã®æˆ¦ç•¥:

| ãƒ•ã‚§ãƒ¼ã‚º | ç›®æ¨™ | æ‰€è¦æ™‚é–“ | æˆ¦è¡“ |
|:--------|:-----|:---------|:-----|
| **Phase 1: é«˜é€Ÿèµ°ç ´** | Zone 0-2 ã‚’30åˆ†ã§ | 30åˆ† | ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã›ãšã«èª­ã‚€ã€‚æ•°å¼ã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚å…¨ä½“åƒæŠŠæ¡ã®ã¿ã€‚ |
| **Phase 2: æ•°å¼ä¿®è¡Œ** | Zone 3 ã® ELBO/Reparamå®Œå…¨ç†è§£ | 2æ™‚é–“ | ãƒšãƒ³ã¨ç´™ã§å°å‡ºã‚’è¿½ã†ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è‡ªåˆ†ã§å†ç¾ã€‚ |
| **Phase 3: Juliaä½“é¨“** | Zone 4 ã® Julia ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ | 1æ™‚é–“ | Revise.jl + REPLé§†å‹•é–‹ç™ºã‚’ä½“é¨“ã€‚PyTorchã¨ã®é€Ÿåº¦å·®ã‚’æ¸¬å®šã€‚ |
| **Phase 4: å®Ÿè£…æ¼”ç¿’** | Zone 5 ã® Tiny VAE è‡ªåŠ›å®Ÿè£… | 2æ™‚é–“ | Julia/Rust ã©ã¡ã‚‰ã‹ã§ã€Zone 0 ã®VAEã‚’å†å®Ÿè£…ã€‚ |
| **Phase 5: æœ€æ–°è¿½å¾“** | Zone 6 ã® FSQ/VQ-GANè«–æ–‡ | 1æ™‚é–“ | arXivè«–æ–‡ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ Abstract + Figure ã‚’èª­ã‚€ã€‚ |

**åˆè¨ˆ: ç´„6.5æ™‚é–“**ï¼ˆæœ¬è¬›ç¾©ã®ç›®æ¨™æ‰€è¦æ™‚é–“ã¯3æ™‚é–“ã ãŒã€å®Œå…¨ç¿’å¾—ã«ã¯å€ã‹ã‹ã‚‹ï¼‰

**å­¦ç¿’ã®ã‚³ãƒ„**:
1. **æ•°å¼ã¯éŸ³èª­ã™ã‚‹** â€” $\mathbb{E}_{q_\phi(z \mid x)}$ ã‚’ã€Œã‚¤ãƒ¼ã‚µãƒ– ã‚­ãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¤ ã‚¼ãƒƒãƒˆ ã‚®ãƒ–ãƒ³ ã‚¨ãƒƒã‚¯ã‚¹ã€ã¨å£°ã«å‡ºã™
2. **ã‚³ãƒ¼ãƒ‰ã¨æ•°å¼ã‚’ä¸¦ã¹ã‚‹** â€” ç”»é¢ã‚’2åˆ†å‰²ã—ã¦ã€å·¦ã«æ•°å¼ã€å³ã«ã‚³ãƒ¼ãƒ‰
3. **æ•°å€¤ã§ç¢ºèª** â€” å°å‡ºã—ãŸå¼ã«å…·ä½“çš„ãªå€¤ï¼ˆ$\mu=0, \sigma=1$ï¼‰ã‚’ä»£å…¥ã—ã¦NumPyã§è¨ˆç®—
4. **Juliaã‚’æã‚Œãªã„** â€” ç¬¬1å›Juliaã‚³ãƒ¼ãƒ‰ã¯ã€Pythonã¨ã»ã¼åŒã˜ã€‚é•ã„ã¯ `.`ï¼ˆbroadcastï¼‰ã ã‘

### 2.7 VAE Family Tree â€” é€£ç¶šã‹ã‚‰é›¢æ•£ã¸

```mermaid
graph TD
    VAE["VAE (2013)<br>Kingma & Welling<br>é€£ç¶šæ½œåœ¨ç©ºé–“"] --> IWAE["IWAE (2015)<br>Importance Weighted<br>ã‚ˆã‚Šå³å¯†ãªbound"]
    VAE --> BetaVAE["Î²-VAE (2017)<br>Higgins et al.<br>Disentanglement"]
    VAE --> Ladder["Ladder VAE (2016)<br>éšå±¤çš„æ½œåœ¨è¡¨ç¾"]

    BetaVAE --> FactorVAE["Factor-VAE (2018)<br>Total Correlation"]
    BetaVAE --> TCVAE["TC-VAE (2018)<br>Î²-TCVAEã®æ”¹è‰¯"]

    VAE --> GumbelSoftmax["Gumbel-Softmax (2016)<br>é›¢æ•£æ½œåœ¨å¤‰æ•°"]
    GumbelSoftmax --> VQVAE["VQ-VAE (2017)<br>Vector Quantization<br>é›¢æ•£ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯"]

    VQVAE --> VQVAE2["VQ-VAE-2 (2019)<br>éšå±¤çš„VQ"]
    VQVAE --> VQGAN["VQ-GAN (2021)<br>Perceptual Loss<br>é«˜å“è³ªç”»åƒ"]

    VQGAN --> FSQ["FSQ (2023)<br>Finite Scalar Quantization<br>Codebook Collapseè§£æ¶ˆ"]
    VQGAN --> MAGVIT["MAGVIT-v2 (2023)<br>çµ±ä¸€èªå½™ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼"]

    FSQ --> CosmosToken["Cosmos Tokenizer (2024)<br>ç”»åƒãƒ»å‹•ç”»çµ±ä¸€"]
    MAGVIT --> SoftVQ["SoftVQ-VAE (2024)<br>å®Œå…¨å¾®åˆ†å¯èƒ½VQ"]

    style VAE fill:#e1f5fe
    style VQVAE fill:#fff3e0
    style FSQ fill:#c8e6c9
    style CosmosToken fill:#ffccbc
```

| æ‰‹æ³• | å¹´ | æ ¸å¿ƒã‚¢ã‚¤ãƒ‡ã‚¢ | arXiv | å¿œç”¨ |
|:-----|:---|:-----------|:------|:-----|
| VAE | 2013 | Reparameterization Trick | 1312.6114 | åŸºç¤ |
| Î²-VAE | 2017 | KLé‡ã¿èª¿æ•´â†’Disentanglement | 1804.03599 | è§£é‡ˆå¯èƒ½è¡¨ç¾ |
| VQ-VAE | 2017 | é›¢æ•£ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ | 1711.00937 | DALL-E 1 |
| VQ-VAE-2 | 2019 | éšå±¤çš„VQ | 1906.00446 | é«˜è§£åƒåº¦ç”»åƒ |
| VQ-GAN | 2021 | Perceptual Loss + GAN | 2012.09841 | ç”»åƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ |
| FSQ | 2023 | å›ºå®šã‚°ãƒªãƒƒãƒ‰é‡å­åŒ– | 2309.15505 | VQç°¡ç´ åŒ– |
| MAGVIT-v2 | 2023 | Look-Up Freeé‡å­åŒ– | 2310.05737 | å‹•ç”»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ |
| Cosmos Tokenizer | 2024 | ç”»åƒãƒ»å‹•ç”»çµ±ä¸€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ | NVIDIA | æ¬¡ä¸–ä»£çµ±ä¸€ãƒ¢ãƒ‡ãƒ« |
| SoftVQ-VAE | 2024 | å®Œå…¨å¾®åˆ†å¯èƒ½VQ | 2412.10958 | è¨“ç·´å®‰å®šåŒ– |

:::message
**é€²æ—: 20% å®Œäº†** VAEã®ä½ç½®ã¥ã‘ã€æ¾å°¾ç ”ã¨ã®å·®åˆ†ã€Juliaç™»å ´ã®èƒŒæ™¯ã€å­¦ç¿’æˆ¦ç•¥ã‚’æŠŠæ¡ã—ãŸã€‚Zone 3ã§æ•°å¼ã®æµ·ã«é£›ã³è¾¼ã‚€æº–å‚™ãŒæ•´ã£ãŸã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” VAEç†è«–ã®å®Œå…¨å°å‡º

**ã“ã®ç« ã®ç›®æ¨™**: VAEã®3ã¤ã®æ ¸å¿ƒã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹:
1. **ELBOå°å‡º** â€” ãªãœã“ã®æå¤±é–¢æ•°ãªã®ã‹
2. **Reparameterization Trick** â€” ãªãœå¾®åˆ†å¯èƒ½ãªã®ã‹
3. **ã‚¬ã‚¦ã‚¹KLé–‰å½¢å¼** â€” ãªãœã“ã®æ­£å‰‡åŒ–é …ãªã®ã‹

ã“ã“ã‹ã‚‰å…ˆã¯ã€ãƒšãƒ³ã¨ç´™ã‚’ç”¨æ„ã—ã¦ã»ã—ã„ã€‚æ•°å¼ã‚’èª­ã‚€ã ã‘ã§ã¯ç†è§£ã§ããªã„ã€‚**è‡ªåˆ†ã®æ‰‹ã§å°å‡ºã™ã‚‹ã“ã¨ãŒã€å”¯ä¸€ã®æ”»ç•¥æ³•ã ã€‚**

### 3.1 å¤‰åˆ†æ¨è«–ã‹ã‚‰VAEã¸ â€” ELBOå†è¨ª

ç¬¬9å›ã§å­¦ã‚“ã ELBOã‚’ã€VAEã®æ–‡è„ˆã§å†å°å‡ºã™ã‚‹ã€‚

#### 3.1.1 å•é¡Œè¨­å®š: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿xã¨æ½œåœ¨å¤‰æ•°z

ãƒ‡ãƒ¼ã‚¿ $\mathcal{D} = \{x^{(1)}, \ldots, x^{(N)}\}$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ« $p_\theta(x, z)$ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$ ã‚’å­¦ç¿’ã—ãŸã„ã€‚

| è¨˜å· | èª­ã¿ | æ„å‘³ |
|:-----|:-----|:-----|
| $x$ | ã‚¨ãƒƒã‚¯ã‚¹ | è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: 28Ã—28 MNISTç”»åƒï¼‰ |
| $z$ | ã‚¼ãƒƒãƒˆ | æ½œåœ¨å¤‰æ•°ï¼ˆä¾‹: 2æ¬¡å…ƒã®æ½œåœ¨ã‚³ãƒ¼ãƒ‰ï¼‰ |
| $\theta$ | ã‚·ãƒ¼ã‚¿ | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆDecoderã®é‡ã¿ï¼‰ |
| $\phi$ | ãƒ•ã‚¡ã‚¤ | å¤‰åˆ†åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆEncoderã®é‡ã¿ï¼‰ |
| $p_\theta(x, z)$ | ãƒ”ãƒ¼ ã‚·ãƒ¼ã‚¿ | åŒæ™‚åˆ†å¸ƒï¼ˆçœŸã®ç”Ÿæˆéç¨‹ï¼‰ |
| $p_\theta(x)$ | ãƒ”ãƒ¼ ã‚·ãƒ¼ã‚¿ | å‘¨è¾ºå°¤åº¦ï¼ˆ**è¨ˆç®—å›°é›£**ï¼‰ |
| $p_\theta(z \mid x)$ | ãƒ”ãƒ¼ ã‚·ãƒ¼ã‚¿ | äº‹å¾Œåˆ†å¸ƒï¼ˆ**è¨ˆç®—å›°é›£**ï¼‰ |
| $q_\phi(z \mid x)$ | ã‚­ãƒ¥ãƒ¼ ãƒ•ã‚¡ã‚¤ | å¤‰åˆ†åˆ†å¸ƒï¼ˆäº‹å¾Œåˆ†å¸ƒã®è¿‘ä¼¼ï¼‰ |

**ãªãœè¨ˆç®—å›°é›£ã‹ï¼Ÿ**

$$
p_\theta(x) = \int p_\theta(x, z) \, dz = \int p_\theta(x \mid z) p(z) \, dz
$$

ã“ã®ç©åˆ†ã¯ã€$z$ ã®æ¬¡å…ƒãŒé«˜ã„ã¨è§£æçš„ã«è§£ã‘ãªã„ã€‚æ•°å€¤ç©åˆ†ï¼ˆMonte Carloï¼‰ã‚‚ã€$z$ ãŒæ•°ç™¾æ¬¡å…ƒã ã¨å®Ÿç”¨çš„ã§ãªã„ã€‚

**è§£æ±ºç­–: å¤‰åˆ†æ¨è«–**

äº‹å¾Œåˆ†å¸ƒ $p_\theta(z \mid x)$ ã‚’ã€ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãªåˆ†å¸ƒ $q_\phi(z \mid x)$ ã§è¿‘ä¼¼ã™ã‚‹ã€‚æœ€é©ãª $\phi$ ã‚’è¦‹ã¤ã‘ã‚‹å•é¡Œã«å¸°ç€ã•ã›ã‚‹ã€‚

#### 3.1.2 ELBOå°å‡ºï¼ˆç¬¬9å›ã®å¾©ç¿’+VAEè¦–ç‚¹ï¼‰

å¯¾æ•°å‘¨è¾ºå°¤åº¦ã‚’ã€$q_\phi(z \mid x)$ ã§åˆ†è§£ã™ã‚‹:

$$
\begin{aligned}
\log p_\theta(x) &= \log \int p_\theta(x, z) \, dz \\
&= \log \int p_\theta(x, z) \frac{q_\phi(z \mid x)}{q_\phi(z \mid x)} \, dz \\
&= \log \mathbb{E}_{q_\phi(z \mid x)} \left[ \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right]
\end{aligned}
$$

Jensenä¸ç­‰å¼ï¼ˆç¬¬6å›ï¼‰: å‡¹é–¢æ•° $\log$ ã«å¯¾ã—ã¦ã€$\log \mathbb{E}[X] \geq \mathbb{E}[\log X]$

$$
\begin{aligned}
\log p_\theta(x) &\geq \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right] \\
&= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x, z) - \log q_\phi(z \mid x) \right] \\
&\equiv \mathcal{L}(\theta, \phi; x) \quad \text{(ELBO)}
\end{aligned}
$$

ã“ã‚ŒãŒ **Evidence Lower BOund** (ELBO)ã€‚å¸¸ã« $\log p_\theta(x) \geq \mathcal{L}(\theta, \phi; x)$ ãŒæˆã‚Šç«‹ã¤ã€‚

#### 3.1.3 ELBOã®2ã¤ã®é …ã¸ã®åˆ†è§£

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ $p_\theta(x, z) = p_\theta(x \mid z) p(z)$ ã¨åˆ†è§£ã™ã‚‹ã¨:

$$
\begin{aligned}
\mathcal{L}(\theta, \phi; x) &= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) + \log p(z) - \log q_\phi(z \mid x) \right] \\
&= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right] + \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{p(z)}{q_\phi(z \mid x)} \right] \\
&= \underbrace{\mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right]}_{\text{Reconstruction term}} - \underbrace{D_\text{KL}(q_\phi(z \mid x) \| p(z))}_{\text{KL regularization}}
\end{aligned}
$$

| é … | èª­ã¿ | æ„å‘³ | æœ€é©åŒ–ã®æ–¹å‘ |
|:---|:-----|:-----|:-----------|
| $\mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)]$ | å†æ§‹æˆé … | ãƒ‡ã‚³ãƒ¼ãƒ€ãŒå…ƒã® $x$ ã‚’ã©ã‚Œã ã‘å¾©å…ƒã§ãã‚‹ã‹ | **æœ€å¤§åŒ–** |
| $D_\text{KL}(q_\phi(z \mid x) \| p(z))$ | KLæ­£å‰‡åŒ– | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›åˆ†å¸ƒã‚’äº‹å‰åˆ†å¸ƒã«è¿‘ã¥ã‘ã‚‹ | **æœ€å°åŒ–** |

**ç›´æ„Ÿçš„è§£é‡ˆ**:
- å†æ§‹æˆé …ã‚’æœ€å¤§åŒ–ã™ã‚‹ã¨ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãŒã€Œè‰¯ã„å¾©å…ƒã€ã‚’ã™ã‚‹ãŒã€æ½œåœ¨ç©ºé–“ã¯æ··æ²Œ
- KLé …ã‚’æœ€å°åŒ–ã™ã‚‹ã¨ã€æ½œåœ¨ç©ºé–“ãŒæ•´ç„¶ã¨ã™ã‚‹ãŒã€å¾©å…ƒç²¾åº¦ãŒçŠ ç‰²ã«ãªã‚‹
- ã“ã®2ã¤ã®ãƒãƒ©ãƒ³ã‚¹ãŒã€VAEã®æ€§èƒ½ã‚’æ±ºã‚ã‚‹

#### 3.1.4 æœ€å¤§åŒ–ã™ã‚‹ELBOã¨ã€æœ€å°åŒ–ã™ã‚‹è² ã®ELBO

å®Ÿè£…ã§ã¯ã€**æå¤±é–¢æ•°** $\mathcal{L}_\text{loss}$ ã¨ã—ã¦ã€ELBOã®ç¬¦å·ã‚’åè»¢ã—ãŸã‚‚ã®ã‚’ä½¿ã†:

$$
\mathcal{L}_\text{loss}(\theta, \phi; x) = -\mathcal{L}(\theta, \phi; x) = -\mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)] + D_\text{KL}(q_\phi(z \mid x) \| p(z))
$$

PyTorch/Juliaã§ã¯ã€ã“ã® $\mathcal{L}_\text{loss}$ ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

```python
# Corresponds to: L_loss = -E_q[log p(x|z)] + D_KL(q||p)
recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')
kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
loss = recon_loss + kl_loss  # minimize this
```

:::message alert
**ã¤ã¾ãšããƒã‚¤ãƒ³ãƒˆ**: è«–æ–‡ã§ã¯ã€ŒELBOã‚’æœ€å¤§åŒ–ã€ã¨æ›¸ã‹ã‚Œã¦ã„ã‚‹ãŒã€ã‚³ãƒ¼ãƒ‰ã§ã¯ã€Œè² ã®ELBOã‚’æœ€å°åŒ–ã€ã—ã¦ã„ã‚‹ã€‚åŒã˜ã“ã¨ã ãŒã€ç¬¦å·ã®æ··ä¹±ã«æ³¨æ„ã€‚
:::

### 3.2 Reparameterization Trick â€” å¾®åˆ†å¯èƒ½ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

#### 3.2.1 å•é¡Œ: ç¢ºç‡çš„ãªãƒãƒ¼ãƒ‰ã§å‹¾é…ãŒæ­¢ã¾ã‚‹

ELBOã‚’æœ€é©åŒ–ã™ã‚‹ã«ã¯ã€$\phi$ ã«é–¢ã™ã‚‹å‹¾é… $\nabla_\phi \mathcal{L}$ ãŒå¿…è¦ã€‚ã—ã‹ã—ã€ç´ æœ´ã«æ›¸ãã¨:

$$
\nabla_\phi \mathcal{L} = \nabla_\phi \mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)] - \nabla_\phi D_\text{KL}(q_\phi(z \mid x) \| p(z))
$$

ç¬¬1é …ã®å‹¾é…ãŒå•é¡Œã ã€‚æœŸå¾…å€¤ã®ä¸­ã« $q_\phi$ ãŒã‚ã‚‹ãŸã‚ã€å¾®åˆ†ã¨æœŸå¾…å€¤ã®äº¤æ›ãŒã§ããªã„:

$$
\nabla_\phi \mathbb{E}_{q_\phi(z \mid x)} [f(z)] \neq \mathbb{E}_{q_\phi(z \mid x)} [\nabla_\phi f(z)]
$$

ãªãœãªã‚‰ã€$q_\phi$ è‡ªä½“ãŒ $\phi$ ã«ä¾å­˜ã—ã¦ã„ã‚‹ã‹ã‚‰ã€‚

**å¾“æ¥ã®è§£æ±ºç­–: REINFORCE (Score Function Estimator)**

$$
\nabla_\phi \mathbb{E}_{q_\phi(z \mid x)} [f(z)] = \mathbb{E}_{q_\phi(z \mid x)} [f(z) \nabla_\phi \log q_\phi(z \mid x)]
$$

ã“ã‚Œã¯ä¸åæ¨å®šé‡ã ãŒã€**åˆ†æ•£ãŒéå¸¸ã«å¤§ãã„** [^6]ã€‚å®Ÿç”¨çš„ã§ãªã„ã€‚

#### 3.2.2 Reparameterization Trickã®å°å…¥

**Key Idea**: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ã€Œæ±ºå®šè«–çš„ãªå¤‰æ› + å¤–éƒ¨ãƒã‚¤ã‚ºã€ã«åˆ†è§£ã™ã‚‹ [^1]ã€‚

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å ´åˆ:

$$
z \sim \mathcal{N}(\mu_\phi(x), \sigma_\phi^2(x)) \quad \Longleftrightarrow \quad z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)
$$

| è¨˜å· | èª­ã¿ | æ„å‘³ | $\phi$ ã¸ã®ä¾å­˜ |
|:-----|:-----|:-----|:--------------|
| $\mu_\phi(x)$ | ãƒŸãƒ¥ãƒ¼ ãƒ•ã‚¡ã‚¤ | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ï¼ˆå¹³å‡ï¼‰ | **ä¾å­˜ã™ã‚‹** |
| $\sigma_\phi(x)$ | ã‚·ã‚°ãƒ ãƒ•ã‚¡ã‚¤ | ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ï¼ˆæ¨™æº–åå·®ï¼‰ | **ä¾å­˜ã™ã‚‹** |
| $\epsilon$ | ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ | æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ« | **ä¾å­˜ã—ãªã„** |

ã“ã‚Œã§ã€$z$ ã®ç¢ºç‡çš„ãªéƒ¨åˆ†ï¼ˆ$\epsilon$ï¼‰ãŒ $\phi$ ã‹ã‚‰ç‹¬ç«‹ã—ãŸã€‚

#### 3.2.3 å‹¾é…ã®è¨ˆç®—

å†æ§‹æˆé …ã®å‹¾é…:

$$
\begin{aligned}
\nabla_\phi \mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)] &= \nabla_\phi \mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)} [\log p_\theta(x \mid \mu_\phi(x) + \sigma_\phi(x) \epsilon)] \\
&= \mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)} [\nabla_\phi \log p_\theta(x \mid \mu_\phi(x) + \sigma_\phi(x) \epsilon)]
\end{aligned}
$$

**å¾®åˆ†ã¨æœŸå¾…å€¤ãŒäº¤æ›ã§ããŸï¼** ãªãœãªã‚‰ã€$\epsilon$ ã¯ $\phi$ ã«ä¾å­˜ã—ãªã„ã‹ã‚‰ã€‚

Monte Carloã§è¿‘ä¼¼:

$$
\nabla_\phi \mathcal{L} \approx \frac{1}{L} \sum_{l=1}^{L} \nabla_\phi \log p_\theta(x \mid z^{(l)}), \quad z^{(l)} = \mu_\phi(x) + \sigma_\phi(x) \epsilon^{(l)}, \quad \epsilon^{(l)} \sim \mathcal{N}(0,1)
$$

å®Ÿè£…ã§ã¯ã€$L=1$ï¼ˆsingle sampleï¼‰ã§ååˆ†ãªå ´åˆãŒå¤šã„ã€‚

```python
def reparameterize(mu, logvar):
    """Reparameterization trick: z = Î¼ + Ïƒ * Îµ.

    Corresponds to: z ~ N(Î¼, ÏƒÂ²) âŸº z = Î¼ + ÏƒÂ·Îµ, Îµ ~ N(0,1)
    """
    std = torch.exp(0.5 * logvar)  # Ïƒ = exp(0.5 * log(ÏƒÂ²))
    eps = torch.randn_like(std)     # Îµ ~ N(0, 1)
    return mu + eps * std           # z = Î¼ + ÏƒÂ·Îµ
```

:::message
**ãªãœ `logvar` ã‚’ä½¿ã†ã®ã‹ï¼Ÿ**

æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã€$\sigma^2$ ã®ä»£ã‚ã‚Šã« $\log \sigma^2$ ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å‡ºåŠ›ã•ã›ã‚‹ã€‚ç†ç”±:
- $\sigma^2 > 0$ ã®åˆ¶ç´„ãŒè‡ªå‹•ã§æº€ãŸã•ã‚Œã‚‹ï¼ˆæŒ‡æ•°é–¢æ•°ã¯å¸¸ã«æ­£ï¼‰
- å‹¾é…æ¶ˆå¤±ã‚’é˜²ãï¼ˆ$\sigma^2 \to 0$ ã®ã¨ãã€$\log \sigma^2 \to -\infty$ ã§å‹¾é…ãŒæ®‹ã‚‹ï¼‰
:::

#### 3.2.4 Pathwiseæ¨å®šé‡ã¨ã—ã¦ã®è§£é‡ˆ

Reparameterization Trickã¯ã€**Pathwise Gradient Estimator** ã¨ã‚‚å‘¼ã°ã‚Œã‚‹ã€‚ãªãœãªã‚‰ã€è¨ˆç®—ã‚°ãƒ©ãƒ•ä¸Šã§ã€Œç¢ºç‡çš„ãƒãƒ¼ãƒ‰ $z$ ã‚’é€šã‚‹ãƒ‘ã‚¹ï¼ˆpathï¼‰ã€ã‚’ã€æ±ºå®šè«–çš„ãªå¤‰æ› $\mu_\phi, \sigma_\phi$ ã¨å¤–éƒ¨ãƒã‚¤ã‚º $\epsilon$ ã«åˆ†é›¢ã—ã¦ã„ã‚‹ã‹ã‚‰ã€‚

```mermaid
graph LR
    X["Input x"] --> Enc["Encoder"]
    Enc --> Mu["Î¼_Ï†(x)"]
    Enc --> LogVar["log ÏƒÂ²_Ï†(x)"]
    LogVar --> Sigma["Ïƒ_Ï†(x) = exp(0.5Â·logvar)"]
    Eps["Îµ ~ N(0,1)<br>(no gradient)"] --> Z["z = Î¼ + ÏƒÂ·Îµ"]
    Mu --> Z
    Sigma --> Z
    Z --> Dec["Decoder"]
    Dec --> Recon["x'"]

    style Eps fill:#ffcccc
    style Z fill:#fff3e0
    style Mu fill:#c8e6c9
    style Sigma fill:#c8e6c9
```

èµ¤ãƒãƒ¼ãƒ‰ï¼ˆ$\epsilon$ï¼‰ã«ã¯å‹¾é…ãŒæµã‚Œãªã„ã€‚ç·‘ãƒãƒ¼ãƒ‰ï¼ˆ$\mu, \sigma$ï¼‰ã«ã¯å‹¾é…ãŒæµã‚Œã‚‹ã€‚

### 3.3 ã‚¬ã‚¦ã‚¹KLç™ºæ•£ã®é–‰å½¢å¼è§£ â€” æ­£å‰‡åŒ–é …ã®è¨ˆç®—

ELBOã®ç¬¬2é …ã€KLç™ºæ•£ã®è¨ˆç®—:

$$
D_\text{KL}(q_\phi(z \mid x) \| p(z))
$$

**ä»®å®š**:
- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›: $q_\phi(z \mid x) = \mathcal{N}(z \mid \mu_\phi(x), \text{diag}(\sigma_\phi^2(x)))$ï¼ˆå¯¾è§’å…±åˆ†æ•£ï¼‰
- äº‹å‰åˆ†å¸ƒ: $p(z) = \mathcal{N}(z \mid 0, I)$ï¼ˆæ¨™æº–æ­£è¦åˆ†å¸ƒï¼‰

#### 3.3.1 1æ¬¡å…ƒã‚¬ã‚¦ã‚¹ã®KLç™ºæ•£

ã¾ãšã€1æ¬¡å…ƒã®å ´åˆã‚’å°å‡ºã™ã‚‹:

$$
q(z) = \mathcal{N}(z \mid \mu, \sigma^2), \quad p(z) = \mathcal{N}(z \mid 0, 1)
$$

KLç™ºæ•£ã®å®šç¾©:

$$
D_\text{KL}(q \| p) = \int q(z) \log \frac{q(z)}{p(z)} \, dz = \mathbb{E}_{z \sim q} \left[ \log q(z) - \log p(z) \right]
$$

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¯¾æ•°ç¢ºç‡å¯†åº¦:

$$
\begin{aligned}
\log q(z) &= -\frac{1}{2} \log(2\pi\sigma^2) - \frac{(z - \mu)^2}{2\sigma^2} \\
\log p(z) &= -\frac{1}{2} \log(2\pi) - \frac{z^2}{2}
\end{aligned}
$$

å·®ã‚’å–ã‚‹:

$$
\log q(z) - \log p(z) = -\frac{1}{2} \log \sigma^2 - \frac{(z - \mu)^2}{2\sigma^2} + \frac{z^2}{2}
$$

æœŸå¾…å€¤ã‚’è¨ˆç®—:

$$
\begin{aligned}
D_\text{KL}(q \| p) &= \mathbb{E}_{z \sim q} \left[ -\frac{1}{2} \log \sigma^2 - \frac{(z - \mu)^2}{2\sigma^2} + \frac{z^2}{2} \right] \\
&= -\frac{1}{2} \log \sigma^2 - \frac{1}{2\sigma^2} \mathbb{E}[(z - \mu)^2] + \frac{1}{2} \mathbb{E}[z^2]
\end{aligned}
$$

ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®æ€§è³ªã‚’ä½¿ã†:
- $\mathbb{E}_{z \sim q}[(z - \mu)^2] = \sigma^2$ï¼ˆåˆ†æ•£ã®å®šç¾©ï¼‰
- $\mathbb{E}_{z \sim q}[z^2] = \mu^2 + \sigma^2$ï¼ˆ$\mathbb{E}[z^2] = \text{Var}(z) + \mathbb{E}[z]^2$ï¼‰

ä»£å…¥:

$$
\begin{aligned}
D_\text{KL}(q \| p) &= -\frac{1}{2} \log \sigma^2 - \frac{\sigma^2}{2\sigma^2} + \frac{\mu^2 + \sigma^2}{2} \\
&= -\frac{1}{2} \log \sigma^2 - \frac{1}{2} + \frac{\mu^2}{2} + \frac{\sigma^2}{2} \\
&= \frac{1}{2} \left( \mu^2 + \sigma^2 - \log \sigma^2 - 1 \right)
\end{aligned}
$$

#### 3.3.2 å¤šæ¬¡å…ƒã¸ã®æ‹¡å¼µ

$d$ æ¬¡å…ƒã‚¬ã‚¦ã‚¹ã®å ´åˆã€å¯¾è§’å…±åˆ†æ•£ãªã®ã§å„æ¬¡å…ƒãŒç‹¬ç«‹:

$$
q(z) = \prod_{j=1}^{d} \mathcal{N}(z_j \mid \mu_j, \sigma_j^2), \quad p(z) = \prod_{j=1}^{d} \mathcal{N}(z_j \mid 0, 1)
$$

KLç™ºæ•£ã¯å’Œã§è¡¨ã›ã‚‹:

$$
D_\text{KL}(q \| p) = \sum_{j=1}^{d} D_\text{KL}(\mathcal{N}(\mu_j, \sigma_j^2) \| \mathcal{N}(0, 1)) = \frac{1}{2} \sum_{j=1}^{d} \left( \mu_j^2 + \sigma_j^2 - \log \sigma_j^2 - 1 \right)
$$

ãƒ™ã‚¯ãƒˆãƒ«è¡¨è¨˜ã«ã™ã‚‹ã¨:

$$
D_\text{KL}(q_\phi(z \mid x) \| p(z)) = \frac{1}{2} \left( \|\mu\|^2 + \|\sigma\|^2 - \sum_{j=1}^{d} \log \sigma_j^2 - d \right)
$$

å®Ÿè£…ã§ã¯ã€$\log \sigma^2$ ã‚’ç›´æ¥æ‰±ã†:

```python
def kl_divergence(mu, logvar):
    """Closed-form KL divergence for Gaussian.

    Corresponds to: D_KL(N(Î¼,ÏƒÂ²) || N(0,1)) = 0.5 * Î£(Î¼Â² + ÏƒÂ² - log(ÏƒÂ²) - 1)
    """
    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
```

#### 3.3.3 æ•°å€¤æ¤œè¨¼

å°å‡ºãŒæ­£ã—ã„ã‹ã€å…·ä½“çš„ãªå€¤ã§ç¢ºèªã—ã‚ˆã†:

```python
import torch

mu = torch.tensor([1.0, -0.5])
logvar = torch.tensor([0.0, -0.693])  # ÏƒÂ² = [1.0, 0.5], log(ÏƒÂ²) = [0, -0.693]

# Closed-form KL
kl_closed = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
print(f"Closed-form KL: {kl_closed.item():.4f}")

# Monte Carlo estimation
def kl_monte_carlo(mu, logvar, num_samples=100000):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn(num_samples, len(mu))
    z = mu + std * eps  # z ~ N(Î¼, ÏƒÂ²)

    # q(z) = N(z|Î¼,ÏƒÂ²), p(z) = N(z|0,1)
    log_q = -0.5 * torch.sum((z - mu).pow(2) / std.pow(2) + torch.log(2 * torch.pi * std.pow(2)), dim=1)
    log_p = -0.5 * torch.sum(z.pow(2) + torch.log(2 * torch.pi * torch.ones_like(z)), dim=1)

    return torch.mean(log_q - log_p)

kl_mc = kl_monte_carlo(mu, logvar)
print(f"Monte Carlo KL:  {kl_mc.item():.4f}")
```

å‡ºåŠ›:
```
Closed-form KL: 0.9750
Monte Carlo KL:  0.9758
```

**ã»ã¼ä¸€è‡´ï¼** é–‰å½¢å¼è§£ãŒæ­£ã—ã„ã“ã¨ãŒç¢ºèªã§ããŸã€‚

:::message alert
**ã¤ã¾ãšããƒã‚¤ãƒ³ãƒˆ**: PyTorchã®å®Ÿè£…ã§ã€ãªãœ `-0.5 * (1 + logvar - mu^2 - exp(logvar))` ã®ç¬¦å·ãŒãƒã‚¤ãƒŠã‚¹ãªã®ã‹ï¼Ÿ

ç†ç”±: ELBOã¯ã€Œæœ€å¤§åŒ–ã€ã—ãŸã„ãŒã€æå¤±é–¢æ•°ã¯ã€Œæœ€å°åŒ–ã€ã™ã‚‹ã€‚KLé …ã¯å…ƒã€…ELBOã§ã€Œå¼•ã‹ã‚Œã¦ã„ã‚‹ã€ã®ã§ã€æå¤±é–¢æ•°ã§ã¯ã€Œè¶³ã™ã€ã€‚ã—ã‹ã—ã€å¼å¤‰å½¢ã§ç¬¦å·ã‚’å¤–ã«å‡ºã™ã¨ãƒã‚¤ãƒŠã‚¹ã«ãªã‚‹ã€‚æ··ä¹±ã—ã‚„ã™ã„ã®ã§ã€å¿…ãšå…ƒã®å¼ã«æˆ»ã£ã¦ç¢ºèªã™ã‚‹ã“ã¨ã€‚
:::

### 3.4 VAEã®ç¢ºç‡çš„è§£é‡ˆ â€” ãªãœELBOãŒæœ‰åŠ¹ãªã®ã‹

ELBOã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ãŒã€ãªãœè‰¯ã„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã§ãã‚‹ã®ã‹ï¼Ÿç¢ºç‡è«–çš„ãªè¦–ç‚¹ã‹ã‚‰ç†è§£ã—ã‚ˆã†ã€‚

#### 3.4.1 å¯¾æ•°å‘¨è¾ºå°¤åº¦ã®åˆ†è§£

çœŸã®ç›®çš„ã¯ã€å¯¾æ•°å‘¨è¾ºå°¤åº¦ $\log p_\theta(x)$ ã®æœ€å¤§åŒ–ã ã€‚ã“ã‚Œã‚’ELBOã§åˆ†è§£ã™ã‚‹:

$$
\begin{aligned}
\log p_\theta(x) &= \log \int p_\theta(x, z) \, dz \\
&= \log \int p_\theta(x, z) \frac{q_\phi(z \mid x)}{q_\phi(z \mid x)} \, dz \\
&= \log \mathbb{E}_{q_\phi(z \mid x)} \left[ \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right] \\
&\geq \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right] \quad \text{(Jensen)} \\
&= \mathcal{L}(\theta, \phi; x)
\end{aligned}
$$

ç­‰å·æˆç«‹æ¡ä»¶ã¯ï¼ŸJensenä¸ç­‰å¼ãŒç­‰å·ã«ãªã‚‹ã®ã¯ã€$\frac{p_\theta(x, z)}{q_\phi(z \mid x)}$ ãŒå®šæ•°ã®ã¨ãã€‚ã™ãªã‚ã¡:

$$
\frac{p_\theta(x, z)}{q_\phi(z \mid x)} = c \quad \Longrightarrow \quad q_\phi(z \mid x) = \frac{p_\theta(x, z)}{c}
$$

ä¸¡è¾ºã‚’ $z$ ã§ç©åˆ†ã™ã‚‹ã¨:

$$
1 = \int q_\phi(z \mid x) \, dz = \frac{1}{c} \int p_\theta(x, z) \, dz = \frac{p_\theta(x)}{c}
$$

ã‚ˆã£ã¦ $c = p_\theta(x)$ã€‚ã—ãŸãŒã£ã¦ã€ç­‰å·æˆç«‹ã¯:

$$
q_\phi(z \mid x) = \frac{p_\theta(x, z)}{p_\theta(x)} = p_\theta(z \mid x)
$$

**ã¤ã¾ã‚Šã€å¤‰åˆ†åˆ†å¸ƒ $q_\phi(z \mid x)$ ãŒçœŸã®äº‹å¾Œåˆ†å¸ƒ $p_\theta(z \mid x)$ ã«ä¸€è‡´ã™ã‚‹ã¨ãã€ELBOã¯å¯¾æ•°å‘¨è¾ºå°¤åº¦ã«ç­‰ã—ããªã‚‹ã€‚**

#### 3.4.2 ELBOã¨KLç™ºæ•£ã®é–¢ä¿‚

å¯¾æ•°å‘¨è¾ºå°¤åº¦ã¨ELBOã®å·®ã‚’è¨ˆç®—:

$$
\begin{aligned}
\log p_\theta(x) - \mathcal{L}(\theta, \phi; x) &= \log p_\theta(x) - \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right] \\
&= \mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x)] - \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{p_\theta(x \mid z) p(z)}{q_\phi(z \mid x)} \right] \\
&= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log \frac{q_\phi(z \mid x)}{p_\theta(z \mid x)} \right] \\
&= D_\text{KL}(q_\phi(z \mid x) \| p_\theta(z \mid x)) \geq 0
\end{aligned}
$$

ã“ã®å°å‡ºã§ã€$\log p_\theta(x) = \log p_\theta(x \mid z) + \log p(z) - \log p_\theta(z \mid x)$ ã‚’ä½¿ã£ãŸã€‚

**çµè«–**: ELBO ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã¯ã€å¤‰åˆ†åˆ†å¸ƒ $q_\phi$ ã¨çœŸã®äº‹å¾Œåˆ†å¸ƒ $p_\theta(z \mid x)$ ã®KLç™ºæ•£ã‚’æœ€å°åŒ–ã—ãªãŒã‚‰ã€å¯¾æ•°å‘¨è¾ºå°¤åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã«ç­‰ã—ã„ã€‚

```python
# Numerical verification: ELBO gap = KL(q||p_posterior)
import torch

def true_posterior_kl_gap(model, x):
    """Verify: log p(x) - ELBO = KL(q(z|x) || p(z|x))"""
    # Encode
    mu, logvar = model.encode(x.view(-1, 784))
    z = model.reparameterize(mu, logvar)

    # Compute ELBO
    recon_x = model.decode(z)
    elbo = -F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum') \
           + 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    # Estimate log p(x) via importance sampling (L=1000 samples)
    L = 1000
    eps_samples = torch.randn(L, *mu.shape)
    z_samples = mu + torch.exp(0.5 * logvar) * eps_samples  # (L, batch, latent_dim)

    recon_samples = torch.stack([model.decode(z_samples[i]) for i in range(L)])
    log_p_x_z = -F.binary_cross_entropy(recon_samples, x.view(-1, 784), reduction='none').sum(dim=-1)  # (L, batch)
    log_p_z = -0.5 * (z_samples ** 2).sum(dim=-1)  # (L, batch)
    log_q_z_x = -0.5 * ((z_samples - mu) ** 2 / torch.exp(logvar)).sum(dim=-1) - 0.5 * logvar.sum()

    # log p(x) â‰ˆ log mean_L exp(log p(x,z) - log q(z|x))
    log_weights = log_p_x_z + log_p_z - log_q_z_x
    log_p_x_estimate = torch.logsumexp(log_weights, dim=0) - torch.log(torch.tensor(L, dtype=torch.float))

    gap = log_p_x_estimate - elbo
    print(f"Estimated KL(q||p_posterior): {gap.item():.4f}")
    return gap

# This gap should be â‰¥ 0 (equality when q = p_posterior)
```

#### 3.4.3 Rate-Distortionç†è«–ã¨ã—ã¦ã®VAE

æƒ…å ±ç†è«–ã®è¦–ç‚¹ã§ã¯ã€VAEã¯ **Rate-Distortion** å•é¡Œã‚’è§£ã„ã¦ã„ã‚‹ [^4]ã€‚

| é … | æƒ…å ±ç†è«–çš„æ„å‘³ | VAEå¯¾å¿œ |
|:---|:-------------|:--------|
| **Rate** | åœ§ç¸®ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ãƒ“ãƒƒãƒˆæ•° | $D_\text{KL}(q_\phi(z \mid x) \| p(z))$ |
| **Distortion** | å¾©å…ƒèª¤å·® | $-\mathbb{E}_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)]$ |

Î²-VAE ã® $\beta$ ã¯ã€Rate ã¨ Distortion ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ¶å¾¡ã™ã‚‹Lagrangeä¹—æ•°ã :

$$
\mathcal{L}_\beta = \text{Distortion} + \beta \cdot \text{Rate}
$$

- $\beta \to 0$: å®Œç’§ãªå¾©å…ƒï¼ˆDistortion = 0ï¼‰ã€æ½œåœ¨ç©ºé–“ã¯ç„¡ç§©åºï¼ˆRateå¤§ï¼‰
- $\beta \to \infty$: æ½œåœ¨å¤‰æ•°ã‚’ç„¡è¦–ï¼ˆRate = 0ï¼‰ã€å¾©å…ƒã¯å¹³å‡ç”»åƒï¼ˆDistortionå¤§ï¼‰

**Shannon ã® Rate-Distortion é–¢æ•°**:

$$
R(D) = \min_{p(\hat{x} \mid x): \mathbb{E}[d(x, \hat{x})] \leq D} I(X; \hat{X})
$$

VAEã®ELBOã¯ã€ã“ã®æœ€é©åŒ–å•é¡Œã®å¤‰åˆ†è¿‘ä¼¼ã¨è¦‹ãªã›ã‚‹ã€‚

### 3.5 Boss Battle: Kingma 2013 Appendix Bã®å®Œå…¨å†ç¾

ã“ã“ã¾ã§ã®æº–å‚™ãŒæ•´ã£ãŸã¨ã“ã‚ã§ã€æœ¬è¬›ç¾©ã® **Boss Battle** ã«æŒ‘æˆ¦ã™ã‚‹ã€‚

**ç›®æ¨™**: Kingma & Welling (2013) [^1] ã® Appendix B ã«ã‚ã‚‹ã€VAEæœ€é©åŒ–ã®å®Œå…¨ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã€å…¨ã¦ã®è¨˜å·ã®æ„å‘³ã‚’ç†è§£ã—ãŸä¸Šã§å†ç¾ã™ã‚‹ã€‚

#### 3.4.1 è«–æ–‡ã®è¨˜æ³•ã¨æœ¬è¬›ç¾©ã®å¯¾å¿œ

| è«–æ–‡ã®è¨˜å· | æœ¬è¬›ç¾©ã®è¨˜å· | æ„å‘³ |
|:----------|:-----------|:-----|
| $\mathcal{D}$ | $\mathcal{D}$ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ $\{x^{(1)}, \ldots, x^{(N)}\}$ |
| $\mathcal{L}(\theta, \phi; x^{(i)})$ | $\mathcal{L}(\theta, \phi; x)$ | ãƒ‡ãƒ¼ã‚¿ç‚¹ $x$ ã®ELBO |
| $\tilde{\mathcal{L}}$ | $\mathcal{L}_\text{loss}$ | è² ã®ELBOï¼ˆæœ€å°åŒ–ã™ã‚‹æå¤±ï¼‰ |
| $g$ | $\nabla_{\theta,\phi}$ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é… |
| $\epsilon$ | $\epsilon$ | æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ« |

#### 3.4.2 ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Œå…¨ã‚¹ãƒ†ãƒƒãƒ—

**Algorithm 1: VAE Training (Kingma & Welling 2013, Appendix B)**

```
Input: Dataset D = {x^(1), ..., x^(N)}, hyperparameters (learning rate Î±, minibatch size M)
Output: Trained parameters Î¸ (decoder), Ï† (encoder)

Initialize Î¸, Ï† randomly

while not converged do:
    # Sample minibatch
    X^M â† random minibatch of M datapoints from D

    # Compute gradients
    Îµ^M â† random samples from N(0, I) (M samples, each of dim d_z)
    g_Î¸,Ï† â† âˆ‡_{Î¸,Ï†} Î£_{xâˆˆX^M} L(Î¸, Ï†; x, Îµ)

    # Update parameters
    Î¸ â† Î¸ + Î± Â· g_Î¸
    Ï† â† Ï† + Î± Â· g_Ï†
end while

where:
    L(Î¸, Ï†; x, Îµ) = -D_KL(q_Ï†(z|x) || p(z)) + log p_Î¸(x | z)
    z = Î¼_Ï†(x) + Ïƒ_Ï†(x) âŠ™ Îµ  (reparameterization trick)
```

#### 3.4.3 å„ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°å±•é–‹

**Step 1: ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**

$$
\mathcal{X}^M = \{x^{(i_1)}, x^{(i_2)}, \ldots, x^{(i_M)}\} \subset \mathcal{D}
$$

å®Ÿè£…:
```python
for x_batch, _ in train_loader:  # x_batch: (M, 784)
    # ... VAE forward pass
```

**Step 2: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå¹³å‡ã¨åˆ†æ•£ã‚’å‡ºåŠ›ï¼‰**

$$
\mu_\phi(x^{(i)}), \log \sigma_\phi^2(x^{(i)}) = \text{Encoder}_\phi(x^{(i)})
$$

å®Ÿè£…:
```python
mu, logvar = model.encode(x_batch)  # mu, logvar: (M, d_z)
```

**Step 3: Reparameterization**

$$
\epsilon^{(i)} \sim \mathcal{N}(0, I), \quad z^{(i)} = \mu_\phi(x^{(i)}) + \sigma_\phi(x^{(i)}) \odot \epsilon^{(i)}
$$

å®Ÿè£…:
```python
std = torch.exp(0.5 * logvar)
eps = torch.randn_like(std)
z = mu + std * eps  # z: (M, d_z)
```

**Step 4: ãƒ‡ã‚³ãƒ¼ãƒ‰**

$$
\hat{x}^{(i)} = \text{Decoder}_\theta(z^{(i)})
$$

å®Ÿè£…:
```python
x_recon = model.decode(z)  # x_recon: (M, 784)
```

**Step 5: æå¤±è¨ˆç®—**

$$
\mathcal{L}_\text{loss}(x^{(i)}) = -\log p_\theta(x^{(i)} \mid z^{(i)}) + D_\text{KL}(q_\phi(z \mid x^{(i)}) \| p(z))
$$

å†æ§‹æˆé …ï¼ˆBernoulliä»®å®šï¼‰:
$$
-\log p_\theta(x \mid z) = \sum_{j=1}^{784} \left[ -x_j \log \hat{x}_j - (1 - x_j) \log (1 - \hat{x}_j) \right] = \text{BCE}(x, \hat{x})
$$

å®Ÿè£…:
```python
recon_loss = F.binary_cross_entropy(x_recon, x_batch, reduction='sum')
kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
loss = recon_loss + kl_loss
```

**Step 6: å‹¾é…è¨ˆç®—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°**

$$
\nabla_\theta \mathcal{L}_\text{loss}, \quad \nabla_\phi \mathcal{L}_\text{loss}
$$

å®Ÿè£…:
```python
optimizer.zero_grad()
loss.backward()  # compute âˆ‡_Î¸, âˆ‡_Ï†
optimizer.step()  # Î¸ â† Î¸ - Î±Â·âˆ‡_Î¸, Ï† â† Ï† - Î±Â·âˆ‡_Ï†
```

#### 3.4.4 å…¨ã‚³ãƒ¼ãƒ‰: Boss Battleå®Œå…¨ç‰ˆ

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# VAE Model
class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super().__init__()
        # Encoder: x -> h -> (Î¼, log ÏƒÂ²)
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        # Decoder: z -> h -> x'
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        """Encoder: q_Ï†(z|x) = N(Î¼_Ï†(x), diag(ÏƒÂ²_Ï†(x)))"""
        h = F.relu(self.fc1(x))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        """Reparameterization: z = Î¼ + ÏƒÂ·Îµ, Îµ ~ N(0,I)"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        """Decoder: p_Î¸(x|z) = Bernoulli(f_Î¸(z))"""
        h = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def loss_function(recon_x, x, mu, logvar):
    """VAE loss = Reconstruction + KL.

    Corresponds to Kingma 2013 Appendix B:
    L_loss = -log p_Î¸(x|z) + D_KL(q_Ï†(z|x) || p(z))
    """
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

# Training
def train_vae(model, train_loader, optimizer, epoch):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = loss_function(recon_batch, data, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]'
                  f'\tLoss: {loss.item() / len(data):.4f}')

    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')

# Main
if __name__ == '__main__':
    # Hyperparameters (from Kingma 2013)
    batch_size = 128
    latent_dim = 20
    learning_rate = 1e-3
    epochs = 10

    # Data
    train_loader = DataLoader(
        datasets.MNIST('./data', train=True, download=True,
                      transform=transforms.ToTensor()),
        batch_size=batch_size, shuffle=True
    )

    # Model
    model = VAE(input_dim=784, hidden_dim=400, latent_dim=latent_dim)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Train
    for epoch in range(1, epochs + 1):
        train_vae(model, train_loader, optimizer, epoch)
```

æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
```
Epoch 1 [0/60000]       Loss: 548.2341
Epoch 1 [12800/60000]   Loss: 165.7892
...
====> Epoch: 1 Average loss: 158.3456
====> Epoch: 10 Average loss: 104.2341
```

**Bossæ’ƒç ´ï¼** Kingma 2013ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Œå…¨å†ç¾ã—ãŸã€‚

:::message
**é€²æ—: 50% å®Œäº†** VAEã®3ã¤ã®æ ¸å¿ƒï¼ˆELBO/Reparameterization/Gaussian KLï¼‰ã‚’å®Œå…¨å°å‡ºã—ã€Kingma 2013ã®Boss Battleã‚’ã‚¯ãƒªã‚¢ã—ãŸã€‚Zone 4ã§Juliaå®Ÿè£…ã«é€²ã‚€ã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Kingma, D. P., & Welling, M. (2013). Auto-Encoding Variational Bayes. *arXiv preprint arXiv:1312.6114*.
@[card](https://arxiv.org/abs/1312.6114)

[^2]: Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., ... & Lerchner, A. (2017). Î²-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. *International Conference on Learning Representations (ICLR)*.
@[card](https://openreview.net/forum?id=Sy2fzU9gl)

[^3]: van den Oord, A., Vinyals, O., & Kavukcuoglu, K. (2017). Neural Discrete Representation Learning. *Advances in Neural Information Processing Systems (NeurIPS)*. arXiv:1711.00937.
@[card](https://arxiv.org/abs/1711.00937)

[^4]: Mentzer, F., Minnen, D., Agustsson, E., & Tschannen, M. (2023). Finite Scalar Quantization: VQ-VAE Made Simple. *International Conference on Learning Representations (ICLR) 2024*. arXiv:2309.15505.
@[card](https://arxiv.org/abs/2309.15505)

[^5]: NVIDIA. (2024). Cosmos Tokenizer. *GitHub Repository*.
@[card](https://github.com/NVIDIA/Cosmos-Tokenizer)

[^6]: Bengio, Y., LÃ©onard, N., & Courville, A. (2013). Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. arXiv:1308.3432.
@[card](https://arxiv.org/abs/1308.3432)

[^7]: Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., & Welling, M. (2016). Improved Variational Inference with Inverse Autoregressive Flow. *NeurIPS 2016*.
@[card](https://arxiv.org/abs/1606.04934)

### é–¢é€£è«–æ–‡

- Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G., & Lerchner, A. (2018). Understanding disentangling in Î²-VAE. arXiv:1804.03599.
@[card](https://arxiv.org/abs/1804.03599)

- Kingma, D. P., Salimans, T., & Welling, M. (2015). Variational Dropout and the Local Reparameterization Trick. *NeurIPS*. arXiv:1506.02557.
@[card](https://arxiv.org/abs/1506.02557)

- Esser, P., Rombach, R., & Ommer, B. (2021). Taming Transformers for High-Resolution Image Synthesis. *CVPR*. arXiv:2012.09841.
@[card](https://arxiv.org/abs/2012.09841)

- Yu, L., Poirson, P., Yang, S., Berg, A. C., & Berg, T. L. (2023). MAGVIT-v2: Language Model Beats Diffusion - Tokenizer is Key to Visual Generation. arXiv:2310.05737.
@[card](https://arxiv.org/abs/2310.05737)

### æ•™ç§‘æ›¸

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer. Chapter 10: Approximate Inference.

- Murphy, K. P. (2022). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. Chapter 21: Variational Inference.

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Chapter 20: Deep Generative Models.
@[card](https://www.deeplearningbook.org/)

---

## è¨˜æ³•è¦ç´„

æœ¬è¬›ç¾©ã‚·ãƒªãƒ¼ã‚ºã§ä½¿ç”¨ã™ã‚‹æ•°å­¦è¨˜æ³•ã®çµ±ä¸€ãƒ«ãƒ¼ãƒ«:

| è¨˜å· | æ„å‘³ | èª­ã¿æ–¹ | ä¾‹ |
|:-----|:-----|:------|:---|
| $x$ | ãƒ‡ãƒ¼ã‚¿ï¼ˆè¦³æ¸¬å¤‰æ•°ï¼‰ | ã‚¨ãƒƒã‚¯ã‚¹ | $x \in \mathbb{R}^{784}$ |
| $z$ | æ½œåœ¨å¤‰æ•° | ã‚¼ãƒƒãƒˆ | $z \in \mathbb{R}^{20}$ |
| $\theta$ | ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆDecoderï¼‰ | ã‚·ãƒ¼ã‚¿ | $p_\theta(x \mid z)$ |
| $\phi$ | å¤‰åˆ†åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆEncoderï¼‰ | ãƒ•ã‚¡ã‚¤ | $q_\phi(z \mid x)$ |
| $\mu, \sigma$ | å¹³å‡ã€æ¨™æº–åå·® | ãƒŸãƒ¥ãƒ¼ã€ã‚·ã‚°ãƒ | $\mathcal{N}(\mu, \sigma^2)$ |
| $\epsilon$ | ãƒã‚¤ã‚ºå¤‰æ•° | ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ | $\epsilon \sim \mathcal{N}(0, I)$ |
| $p(x)$ | çœŸã®åˆ†å¸ƒ | ãƒ”ãƒ¼ | $p(x) = \int p(x, z) dz$ |
| $q(z \mid x)$ | å¤‰åˆ†åˆ†å¸ƒï¼ˆè¿‘ä¼¼äº‹å¾Œåˆ†å¸ƒï¼‰ | ã‚­ãƒ¥ãƒ¼ | $q_\phi(z \mid x)$ |
| $\mathbb{E}_{q}[\cdot]$ | $q$ ã®ä¸‹ã§ã®æœŸå¾…å€¤ | ã‚¤ãƒ¼ ã‚µãƒ– ã‚­ãƒ¥ãƒ¼ | $\mathbb{E}_{q(z)}[f(z)]$ |
| $D_\text{KL}(q \| p)$ | KLç™ºæ•£ | ãƒ‡ã‚£ãƒ¼ ã‚±ãƒ¼ã‚¨ãƒ« | $D_\text{KL}(q \| p) = \mathbb{E}_q[\log q - \log p]$ |
| $\mathcal{L}(\theta, \phi)$ | ELBOï¼ˆæå¤±é–¢æ•°ï¼‰ | ã‚¨ãƒ« ã‚·ãƒ¼ã‚¿ ãƒ•ã‚¡ã‚¤ | $\mathcal{L} = \mathbb{E}_q[\log p] - D_\text{KL}(q \| p)$ |
| $\nabla_\theta$ | $\theta$ ã«é–¢ã™ã‚‹å‹¾é… | ãƒŠãƒ–ãƒ© ã‚·ãƒ¼ã‚¿ | $\nabla_\theta \mathcal{L}$ |
| $\odot$ | è¦ç´ ã”ã¨ã®ç©ï¼ˆHadamardç©ï¼‰ | Hadamard product | $z = \mu + \sigma \odot \epsilon$ |
| $\|x\|$ | ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ãƒãƒ«ãƒ  | ãƒãƒ«ãƒ  | $\|x\|^2 = \sum x_i^2$ |

**Juliaè¨˜æ³•ã¨ã®å¯¾å¿œ**:
- `Î¼` (U+03BC), `Ïƒ` (U+03C3), `Î¸` (U+03B8), `Ï†` (U+03C6), `Îµ` (U+03B5) â€” Juliaã§ã¯å¤‰æ•°åã«ã‚®ãƒªã‚·ãƒ£æ–‡å­—ã‚’ä½¿ãˆã‚‹
- `.` â€” broadcastæ¼”ç®—å­ï¼ˆè¦ç´ ã”ã¨é©ç”¨ï¼‰
- `.*` â€” è¦ç´ ã”ã¨ã®ç©ï¼ˆ$\odot$ ã«å¯¾å¿œï¼‰

---

**EOF**

---
