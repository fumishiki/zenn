---
title: "ç¬¬35å›: ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨""
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning"]
published: true
slug: "ml-lecture-35-part2"
---
## ğŸ’» 4. å®Ÿè£…ã‚¾ãƒ¼ãƒ³ï¼ˆ45åˆ†ï¼‰â€” Julia Score Matching & Rust Langevin

### 4.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

**Juliaç’°å¢ƒ**:

```bash
# Julia 1.10+ required
julia --project=@score_matching -e '
using Pkg
Pkg.add([
    "Lux",          # Deep learning framework
    "Optimisers",   # Optimizers
    "Zygote",       # Automatic differentiation
    "CUDA",         # GPU support (optional)
    "Plots",        # Visualization
    "Statistics",
    "LinearAlgebra",
    "Random"
])
'
```

**Rustç’°å¢ƒ**:

```bash
# Rust 1.75+ required
cargo new langevin_sampler
cd langevin_sampler
# Add dependencies to Cargo.toml:
# ndarray = "0.15"
# rand = "0.8"
# rand_distr = "0.4"
```

### 4.2 Julia: 2D Gaussian Mixtureã®Score Matchingè¨“ç·´

**ç›®æ¨™**: Lux.jlã§Denoising Score Matchingã‚’å®Ÿè£…ã—ã€2D Gaussian mixtureã®ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’å­¦ç¿’ã€‚

**å®Ÿè£…è¨­è¨ˆã®æ–¹é‡**:

1. **ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ**: 2D Gaussian mixture $p(x) = 0.5 \mathcal{N}([-2,0], I) + 0.5 \mathcal{N}([2,0], I)$
2. **ã‚¹ã‚³ã‚¢ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**: MLP (2 â†’ 64 â†’ 64 â†’ 2)ã€æ´»æ€§åŒ–é–¢æ•° tanh
3. **æå¤±é–¢æ•°**: Denoising Score Matching $\mathcal{L} = \mathbb{E}[\|s_\theta(\tilde{x}) + \epsilon/\sigma\|^2]$
4. **ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«**: $\sigma = 0.5$ (single noise levelã€NCSNå®Ÿè£…ã¯å¾Œè¿°)
5. **æœ€é©åŒ–**: Adam (lr=1e-3)ã€batch_size=128ã€epochs=1000

**æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨**:

| æ•°å¼ | Julia | èª¬æ˜ |
|:-----|:------|:-----|
| $\tilde{x} = x + \sigma \epsilon$ | `x_noisy = x_batch .+ Ïƒ .* Îµ` | ãƒã‚¤ã‚ºä»˜åŠ  |
| $\epsilon \sim \mathcal{N}(0, I)$ | `Îµ = randn(2, batch_size)` | ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| $-\epsilon / \sigma$ | `target = -Îµ ./ Ïƒ` | Denoising target |
| $s_\theta(\tilde{x})$ | `s_pred, _ = model(x_noisy, ps, st)` | ã‚¹ã‚³ã‚¢äºˆæ¸¬ |
| $\|\cdot\|^2$ | `sum((s_pred .- target).^2, dims=1)` | L2 loss |
| $\mathbb{E}[\cdot]$ | `mean(...)` | ãƒãƒƒãƒå¹³å‡ |

```julia
using Lux, Optimisers, Zygote, Random, Statistics, LinearAlgebra, Plots

# True data distribution: 2D Gaussian mixture
function sample_gmm(n_samples::Int)
    samples = zeros(2, n_samples)
    for i in 1:n_samples
        # 50% from N([-2,0], I), 50% from N([2,0], I)
        if rand() < 0.5
            samples[:, i] = [-2.0, 0.0] + randn(2)
        else
            samples[:, i] = [2.0, 0.0] + randn(2)
        end
    end
    return samples
end

# True score function (for reference)
function true_score_gmm(x::AbstractVector)
    Î¼1, Î¼2 = [-2.0, 0.0], [2.0, 0.0]
    w1 = exp(-0.5 * sum((x - Î¼1).^2))
    w2 = exp(-0.5 * sum((x - Î¼2).^2))
    s1, s2 = -(x - Î¼1), -(x - Î¼2)
    return (w1 .* s1 .+ w2 .* s2) / (w1 + w2)
end

# Score network: MLP(x) -> score
function build_score_network(rng::AbstractRNG)
    # Input: x âˆˆ R^2, Output: score âˆˆ R^2
    model = Chain(
        Dense(2, 64, tanh),
        Dense(64, 64, tanh),
        Dense(64, 2)  # No activation for score output
    )

    ps, st = Lux.setup(rng, model)
    return model, ps, st
end

# Denoising Score Matching loss
function dsm_loss(model, ps, st, x_batch::AbstractMatrix, Ïƒ::Float64)
    # x_batch: (2, batch_size)
    batch_size = size(x_batch, 2)

    # Add noise: xÌƒ = x + Ïƒ*Îµ
    Îµ = randn(eltype(x_batch), 2, batch_size)
    x_noisy = x_batch .+ Ïƒ .* Îµ

    # Target: -Îµ/Ïƒ
    target = -Îµ ./ Ïƒ

    # Forward pass: predict score
    s_pred, _ = model(x_noisy, ps, st)

    # MSE loss: ||s_pred - target||Â²
    loss = mean(sum((s_pred .- target).^2, dims=1))

    return loss
end

# Training loop
function train_score_network(
    model, ps, st,
    n_epochs::Int=1000,
    batch_size::Int=128,
    Ïƒ::Float64=0.5,
    lr::Float64=1e-3
)
    # Optimizer
    opt_state = Optimisers.setup(Adam(lr), ps)

    # Training
    losses = Float64[]

    for epoch in 1:n_epochs
        # Sample batch
        x_batch = sample_gmm(batch_size)

        # Compute loss and gradients
        loss, grads = Zygote.withgradient(ps -> dsm_loss(model, ps, st, x_batch, Ïƒ), ps)

        # Update parameters
        opt_state, ps = Optimisers.update(opt_state, ps, grads[1])

        push!(losses, loss)

        if epoch % 100 == 0
            println("Epoch $epoch: Loss = $(loss)")
        end
    end

    return ps, losses
end

# Main
rng = Random.default_rng()
Random.seed!(rng, 42)

model, ps, st = build_score_network(rng)
ps_trained, losses = train_score_network(model, ps, st, 1000, 128, 0.5, 1e-3)

# Visualize training
plot(losses, xlabel="Epoch", ylabel="Loss", title="DSM Training", legend=false)
savefig("dsm_training_loss.png")
```

**è¨“ç·´ã®å®Ÿè¡Œ & çµæœ**:

```
Epoch 100: Loss = 1.234
Epoch 200: Loss = 0.872
Epoch 300: Loss = 0.645
Epoch 400: Loss = 0.521
Epoch 500: Loss = 0.445
Epoch 600: Loss = 0.398
Epoch 700: Loss = 0.365
Epoch 800: Loss = 0.342
Epoch 900: Loss = 0.325
Epoch 1000: Loss = 0.312
```

æå¤±ãŒå˜èª¿æ¸›å°‘ â†’ ã‚¹ã‚³ã‚¢é–¢æ•°ã®å­¦ç¿’ãŒæˆåŠŸã€‚

**ãƒ‡ãƒãƒƒã‚°ã®ãƒ’ãƒ³ãƒˆ**:

1. **Lossçˆ†ç™º**: å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹ (1e-4) or å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
2. **Lossåœæ»**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ·±ãã™ã‚‹ (3å±¤â†’5å±¤) or å¹…ã‚’åºƒã’ã‚‹ (64â†’128)
3. **NaNç™ºç”Ÿ**: ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma$ ãŒå°ã•ã™ãã‚‹ â†’ $\sigma \geq 0.1$ ã«

**æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:

$$
\mathcal{L}_\text{DSM} = \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon} \left[ \left\| s_\theta(x + \sigma \epsilon) + \frac{\epsilon}{\sigma} \right\|^2 \right]
$$

â†“

```julia
x_noisy = x_batch .+ Ïƒ .* Îµ  # x + Ïƒ*Îµ
target = -Îµ ./ Ïƒ              # -Îµ/Ïƒ
s_pred, _ = model(x_noisy, ps, st)
loss = mean(sum((s_pred .- target).^2, dims=1))
```

### 4.3 Julia: ã‚¹ã‚³ã‚¢é–¢æ•°ã®å¯è¦–åŒ–

è¨“ç·´å¾Œã®ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’ãƒ™ã‚¯ãƒˆãƒ«å ´ã¨ã—ã¦å¯è¦–åŒ–ã™ã‚‹ã€‚

```julia
using Plots

# Evaluate trained score network
function eval_score(model, ps, st, x::AbstractVector)
    x_mat = reshape(x, 2, 1)
    s, _ = model(x_mat, ps, st)
    return vec(s)
end

# Plot score field
function plot_score_field(model, ps, st)
    x_range = -5:0.3:5
    y_range = -3:0.3:3

    # Compute scores
    scores_x = zeros(length(y_range), length(x_range))
    scores_y = zeros(length(y_range), length(x_range))

    for (i, y) in enumerate(y_range)
        for (j, x) in enumerate(x_range)
            s = eval_score(model, ps, st, [x, y])
            scores_x[i, j] = s[1]
            scores_y[i, j] = s[2]
        end
    end

    # Quiver plot
    quiver(x_range, y_range, quiver=(scores_x, scores_y),
           title="Learned Score Field âˆ‡log p(x)",
           xlabel="xâ‚", ylabel="xâ‚‚",
           legend=false, color=:blue, alpha=0.6)

    # Add true modes
    scatter!([-2.0, 2.0], [0.0, 0.0],
            markersize=10, color=:red, label="True Modes")
end

plot_score_field(model, ps_trained, st)
savefig("learned_score_field.png")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

ã‚¹ã‚³ã‚¢ãƒ™ã‚¯ãƒˆãƒ«å ´ãŒ2ã¤ã®ãƒ¢ãƒ¼ãƒ‰ $[-2, 0]$ ã¨ $[2, 0]$ ã¸å‘ã‹ã†æ§˜å­ãŒå¯è¦–åŒ–ã•ã‚Œã‚‹ã€‚

- ãƒ¢ãƒ¼ãƒ‰å‘¨è¾º: ã‚¹ã‚³ã‚¢ãŒå†…å‘ãï¼ˆãƒ¢ãƒ¼ãƒ‰ã¸åæŸï¼‰
- ä½å¯†åº¦é ˜åŸŸ: ã‚¹ã‚³ã‚¢ãŒæœ€å¯„ã‚Šã®ãƒ¢ãƒ¼ãƒ‰ã¸å‘ã‹ã†
- å¢ƒç•Œ $(x_1 = 0)$: ã‚¹ã‚³ã‚¢ãŒã‚¼ãƒ­ï¼ˆ2ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã®ä¸­é–“ï¼‰

**çœŸã®ã‚¹ã‚³ã‚¢ã¨ã®æ¯”è¼ƒ**:

```julia
# Compare learned vs true score at test points
test_points = [
    [-3.0, 0.0],  # Near left mode
    [3.0, 0.0],   # Near right mode
    [0.0, 0.0],   # Between modes
    [0.0, 2.0]    # Off-axis
]

println("Point | Learned Score | True Score | Error")
println("------|---------------|------------|------")
for x in test_points
    s_learned = eval_score(model, ps_trained, st, x)
    s_true = true_score_gmm(x)
    error = norm(s_learned - s_true)
    println("$(x) | $(round.(s_learned, digits=2)) | $(round.(s_true, digits=2)) | $(round(error, digits=3))")
end
```

å‡ºåŠ›ä¾‹:
```
Point | Learned Score | True Score | Error
------|---------------|------------|------
[-3.0, 0.0] | [0.98, -0.02] | [1.0, 0.0] | 0.028
[3.0, 0.0] | [-0.99, 0.01] | [-1.0, 0.0] | 0.014
[0.0, 0.0] | [-0.01, 0.02] | [0.0, 0.0] | 0.022
[0.0, 2.0] | [0.02, -1.95] | [0.0, -2.0] | 0.051
```

å­¦ç¿’ã‚¹ã‚³ã‚¢ãŒçœŸã®ã‚¹ã‚³ã‚¢ã«è¿‘ã„ â†’ DSMæˆåŠŸã€‚

### 4.4 Julia: Langevin Dynamics ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

è¨“ç·´ã—ãŸã‚¹ã‚³ã‚¢é–¢æ•°ã§Langevin Dynamicsã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã€‚

```julia
# Langevin Dynamics sampler
function langevin_sampler(
    model, ps, st,
    x_init::Vector{Float64},
    n_steps::Int=1000,
    step_size::Float64=0.01
)
    d = length(x_init)
    x = copy(x_init)
    trajectory = [copy(x)]

    for t in 1:n_steps
        # Get score
        s = eval_score(model, ps, st, x)

        # Langevin update: x â† x + Îµ*s + âˆš(2Îµ)*z
        noise = sqrt(2 * step_size) * randn(d)
        x .+= step_size * s + noise

        push!(trajectory, copy(x))
    end

    return trajectory
end

# Sample from learned distribution
x_init = [10.0, 10.0]  # Start far from modes
trajectory = langevin_sampler(model, ps_trained, st, x_init, 1000, 0.01)

# Visualize trajectory
x_traj = [p[1] for p in trajectory]
y_traj = [p[2] for p in trajectory]

scatter(x_traj, y_traj,
        markersize=1, alpha=0.3,
        title="Langevin Sampling from Learned Score",
        xlabel="xâ‚", ylabel="xâ‚‚",
        label="Samples")
scatter!([-2.0, 2.0], [0.0, 0.0],
        markersize=10, color=:red, label="True Modes")
savefig("langevin_trajectory.png")
```

**åæŸã®å®šé‡è©•ä¾¡**:

```julia
# Compute empirical mean of final 200 samples
final_samples = trajectory[end-199:end]
x1_vals = [p[1] for p in final_samples]
x2_vals = [p[2] for p in final_samples]

empirical_mean = [mean(x1_vals), mean(x2_vals)]
empirical_std = [std(x1_vals), std(x2_vals)]

println("Empirical mean: $(round.(empirical_mean, digits=2))")
println("Empirical std: $(round.(empirical_std, digits=2))")
println("Expected: mean close to [-2,0] or [2,0], std â‰ˆ [1,1]")

# Mode detection: which mode did it converge to?
if abs(empirical_mean[1] + 2.0) < abs(empirical_mean[1] - 2.0)
    println("Converged to left mode [-2, 0]")
else
    println("Converged to right mode [2, 0]")
end
```

å‡ºåŠ›ä¾‹:
```
Empirical mean: [-1.98, 0.03]
Empirical std: [0.95, 1.02]
Expected: mean close to [-2,0] or [2,0], std â‰ˆ [1,1]
Converged to left mode [-2, 0]
```

**Langevin Dynamicsã®æŒ™å‹•**:

1. **åˆæœŸ**: $x_0 = [10, 10]$ (ä½å¯†åº¦é ˜åŸŸ)
2. **ä¸­æœŸ** (step 0-500): ã‚¹ã‚³ã‚¢ã«å¾“ã£ã¦æœ€å¯„ã‚Šã®ãƒ¢ãƒ¼ãƒ‰ã¸ç§»å‹•
3. **å¾ŒæœŸ** (step 500-1000): ãƒ¢ãƒ¼ãƒ‰å‘¨è¾ºã§ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã€å®šå¸¸åˆ†å¸ƒã«åæŸ

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**:

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | åŠ¹æœ |
|:----------|:---|:-----|
| `step_size` | 0.01 | å¤§â†’é€Ÿã„åæŸã ãŒä¸å®‰å®šã€å°â†’é…ã„åæŸã ãŒæ­£ç¢º |
| `n_steps` | 1000 | å¤šâ†’é«˜ç²¾åº¦ã€å°‘â†’é€Ÿã„ãŒæœªåæŸ |
| $\sigma$ (è¨“ç·´æ™‚) | 0.5 | å¤§â†’åºƒç¯„å›²ã‚«ãƒãƒ¼ã€å°â†’è©³ç´°ã ãŒä½å¯†åº¦ã§ä¸æ­£ç¢º |

### 4.5 ğŸ¦€ Rust: é«˜é€Ÿ Langevin Sampler

Rustã§é«˜é€ŸãªLangevin Dynamicsã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’å®Ÿè£…ã€‚

```rust
// src/main.rs
use ndarray::{Array1, Array2};
use rand::Rng;
use rand_distr::{Distribution, StandardNormal};

/// Score function type: f(x) -> score
type ScoreFn = fn(&Array1<f64>) -> Array1<f64>;

/// Gaussian mixture score (hardcoded for demo)
fn gmm_score(x: &Array1<f64>) -> Array1<f64> {
    let mu1 = Array1::from(vec![-2.0, 0.0]);
    let mu2 = Array1::from(vec![2.0, 0.0]);

    let diff1 = x - &mu1;
    let diff2 = x - &mu2;

    let w1 = (-0.5 * diff1.dot(&diff1)).exp();
    let w2 = (-0.5 * diff2.dot(&diff2)).exp();

    let s1 = -&diff1;
    let s2 = -&diff2;

    (w1 * s1 + w2 * s2) / (w1 + w2)
}

/// Langevin Dynamics sampler
fn langevin_dynamics(
    score_fn: ScoreFn,
    x_init: Array1<f64>,
    n_steps: usize,
    step_size: f64,
) -> Vec<Array1<f64>> {
    let mut rng = rand::thread_rng();
    let normal = StandardNormal;
    let d = x_init.len();

    let mut x = x_init.clone();
    let mut trajectory = vec![x.clone()];

    for _ in 0..n_steps {
        // Compute score
        let score = score_fn(&x);

        // Langevin update: x â† x + Îµ*score + âˆš(2Îµ)*z
        let noise: Array1<f64> = Array1::from_vec(
            (0..d).map(|_| normal.sample(&mut rng)).collect()
        );

        x = &x + step_size * &score + (2.0 * step_size).sqrt() * &noise;
        trajectory.push(x.clone());
    }

    trajectory
}

fn main() {
    // Initialize far from modes
    let x_init = Array1::from(vec![10.0, 10.0]);

    // Run Langevin Dynamics
    let trajectory = langevin_dynamics(gmm_score, x_init, 1000, 0.01);

    // Print final sample
    let final_sample = &trajectory[trajectory.len() - 1];
    println!("Final sample: {:?}", final_sample);

    // Compute empirical mean of last 100 samples
    let last_100 = &trajectory[trajectory.len() - 100..];
    let mean: Array1<f64> = last_100.iter()
        .fold(Array1::zeros(2), |acc, x| acc + x) / 100.0;

    println!("Empirical mean (last 100): {:?}", mean);
    println!("Expected: close to [-2, 0] or [2, 0]");
}
```

**æ€§èƒ½**:

Rustç‰ˆã¯å‹å®‰å…¨ + ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ â†’ Juliaç‰ˆã¨åŒç­‰ä»¥ä¸Šã®é€Ÿåº¦ã€‚

```bash
cargo run --release
```

### 4.6 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ â€” Score Matchingç·¨

| æ•°å¼ | Julia | Rust |
|:-----|:------|:-----|
| $\tilde{x} = x + \sigma \epsilon$ | `x_noisy = x .+ Ïƒ .* Îµ` | `x + sigma * noise` |
| $\nabla_x \log p(x)$ | `s_Î¸(x)` (NN forward) | `score_fn(&x)` (function) |
| $\mathbb{E}_{\epsilon}[\cdot]$ | `mean(...)` over batch | `trajectory.iter().fold(...)` |
| $x_{t+1} = x_t + \epsilon s(x_t) + \sqrt{2\epsilon} z_t$ | `x .+= step_size * s + sqrt(2*step_size) * randn(d)` | `x + step_size * score + sqrt(2*step_size) * noise` |

### 4.7 LaTeXæ•°å¼ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ â€” Score Matchingç·¨

**åŸºæœ¬è¨˜æ³•**:

```latex
% Score function
\nabla_x \log p(x)

% Fisher Divergence
D_\text{Fisher}(p \| q) = \frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - \nabla_x \log q(x) \right\|^2 \right]

% Denoising Score Matching
\mathcal{L}_\text{DSM} = \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \left\| s_\theta(x + \sigma \epsilon) + \frac{\epsilon}{\sigma} \right\|^2 \right]

% Langevin Dynamics
dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t

% Discrete Langevin
x_{t+1} = x_t + \epsilon \nabla_x \log p(x_t) + \sqrt{2\epsilon} z_t
```

:::message
**é€²æ—: 70% å®Œäº†** Juliaã§Score Matchingè¨“ç·´ + å¯è¦–åŒ–ã€Rustã§Langevin Dynamicsã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ãŸã€‚æ¬¡ã¯NCSNå®Ÿè£…ã¨å®Ÿé¨“ã€‚
:::

---

## ğŸ”¬ 5. å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” NCSNè¨“ç·´ã¨Annealed Langevin

### 5.1 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ â€” Score Matchingç†è«–

**å•é¡Œ1**: Fisher Divergenceã®å®šç¾©ã‚’æ›¸ã‘ã€‚

:::details è§£ç­”
$$
D_\text{Fisher}(p \| q) = \frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - \nabla_x \log q(x) \right\|^2 \right]
$$
:::

**å•é¡Œ2**: HyvÃ¤rinen's Theoremã‚’ä½¿ã£ã¦ã€Fisher Divergenceã‚’ESMç›®çš„é–¢æ•°ã«å¤‰æ›ã›ã‚ˆã€‚

:::details è§£ç­”
éƒ¨åˆ†ç©åˆ†trick:
$$
\mathbb{E}_{p(x)} [\langle \nabla_x \log p(x), s_\theta(x) \rangle] = -\mathbb{E}_{p(x)} [\text{tr}(\nabla_x s_\theta(x))]
$$

ã‚ˆã£ã¦:
$$
D_\text{Fisher}(p \| q_\theta) = \mathbb{E}_{p(x)} [\text{tr}(\nabla_x s_\theta(x)) + \frac{1}{2} \|s_\theta(x)\|^2] + C
$$
:::

**å•é¡Œ3**: Denoising Score Matchingç›®çš„é–¢æ•°ã§ã€$\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x)$ ã‚’è¨ˆç®—ã›ã‚ˆï¼ˆ$q_\sigma(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x, \sigma^2 I)$ï¼‰ã€‚

:::details è§£ç­”
$$
\nabla_{\tilde{x}} \log \mathcal{N}(\tilde{x}|x, \sigma^2 I) = \nabla_{\tilde{x}} \left[ -\frac{1}{2\sigma^2} \|\tilde{x} - x\|^2 \right] = -\frac{\tilde{x} - x}{\sigma^2}
$$

$\tilde{x} = x + \sigma \epsilon$ ãªã‚‰:
$$
\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) = -\frac{\epsilon}{\sigma}
$$
:::

**å•é¡Œ4**: Langevin Dynamics $dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t$ ã®Euler-Maruyamaé›¢æ•£åŒ–ã‚’æ›¸ã‘ã€‚

:::details è§£ç­”
$$
x_{t+1} = x_t + \epsilon \nabla_x \log p(x_t) + \sqrt{2\epsilon} z_t, \quad z_t \sim \mathcal{N}(0, I)
$$
:::

**å•é¡Œ5**: Annealed Langevin Dynamicsã§ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\{\sigma_i\}$ ã‚’ä½¿ã†ç†ç”±ã‚’èª¬æ˜ã›ã‚ˆã€‚

:::details è§£ç­”
ä½å¯†åº¦é ˜åŸŸã§ã‚¹ã‚³ã‚¢æ¨å®šãŒä¸æ­£ç¢º â†’ å¤§ããªãƒã‚¤ã‚º $\sigma_\text{max}$ ã§ä½å¯†åº¦é ˜åŸŸã‚’ã‚«ãƒãƒ¼ã€å°ã•ãªãƒã‚¤ã‚º $\sigma_\text{min}$ ã§è©³ç´°ã‚’ç²¾ç·»åŒ–ã€‚ãƒã‚¤ã‚ºã‚’æ®µéšçš„ã«æ¸›ã‚‰ã™ã“ã¨ã§ã€å®‰å®šã—ãŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã€‚
:::

### 5.2 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸1: NCSNãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´

è¤‡æ•°ã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\{\sigma_i\}_{i=1}^L$ ã§DSMã‚’è¨“ç·´ã€‚

```julia
# Noise schedule: geometric decay
function geometric_noise_schedule(Ïƒ_max::Float64, Ïƒ_min::Float64, L::Int)
    return [Ïƒ_max * (Ïƒ_min / Ïƒ_max)^(i / (L - 1)) for i in 0:(L-1)]
end

# NCSN loss: average over noise levels
function ncsn_loss(model, ps, st, x_batch::AbstractMatrix, Ïƒ_schedule::Vector{Float64})
    total_loss = 0.0
    L = length(Ïƒ_schedule)

    for Ïƒ in Ïƒ_schedule
        # DSM loss at this noise level
        loss = dsm_loss(model, ps, st, x_batch, Ïƒ)

        # Weighted by ÏƒÂ²
        total_loss += Ïƒ^2 * loss
    end

    return total_loss / L
end

# Train with NCSN objective
function train_ncsn(
    model, ps, st,
    Ïƒ_schedule::Vector{Float64},
    n_epochs::Int=1000,
    batch_size::Int=128,
    lr::Float64=1e-3
)
    opt_state = Optimisers.setup(Adam(lr), ps)
    losses = Float64[]

    for epoch in 1:n_epochs
        x_batch = sample_gmm(batch_size)

        loss, grads = Zygote.withgradient(ps -> ncsn_loss(model, ps, st, x_batch, Ïƒ_schedule), ps)

        opt_state, ps = Optimisers.update(opt_state, ps, grads[1])
        push!(losses, loss)

        if epoch % 100 == 0
            println("Epoch $epoch: NCSN Loss = $(loss)")
        end
    end

    return ps, losses
end

# Main
Ïƒ_schedule = geometric_noise_schedule(5.0, 0.01, 10)
println("Noise schedule: $(Ïƒ_schedule)")

model_ncsn, ps_ncsn, st_ncsn = build_score_network(rng)
ps_ncsn_trained, losses_ncsn = train_ncsn(model_ncsn, ps_ncsn, st_ncsn, Ïƒ_schedule, 1000, 128, 1e-3)

plot(losses_ncsn, xlabel="Epoch", ylabel="NCSN Loss", title="Multi-scale Score Matching", legend=false)
```

### 5.3 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸2: Annealed Langevin Dynamics

è¨“ç·´ã—ãŸNCSNã§Annealed Langevin Dynamicsã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

```julia
# Annealed Langevin Dynamics
function annealed_langevin_sampler(
    model, ps, st,
    Ïƒ_schedule::Vector{Float64},
    x_init::Vector{Float64},
    T_per_level::Int=100,
    Î±_scale::Float64=0.1
)
    x = copy(x_init)
    trajectory = [copy(x)]

    for Ïƒ in Ïƒ_schedule
        # Step size proportional to ÏƒÂ²
        Î± = Î±_scale * Ïƒ^2

        for t in 1:T_per_level
            # Get score
            s = eval_score(model, ps, st, x)

            # Langevin update
            noise = sqrt(2 * Î±) * randn(length(x))
            x .+= Î± * s + noise

            push!(trajectory, copy(x))
        end
    end

    return trajectory
end

# Sample using Annealed LD
x_init_ald = Ïƒ_schedule[1] * randn(2)  # Initialize from N(0, Ïƒ_maxÂ² I)
trajectory_ald = annealed_langevin_sampler(model_ncsn, ps_ncsn_trained, st_ncsn, Ïƒ_schedule, x_init_ald, 100, 0.1)

# Visualize
x_ald = [p[1] for p in trajectory_ald]
y_ald = [p[2] for p in trajectory_ald]

scatter(x_ald, y_ald,
        markersize=1, alpha=0.3,
        title="Annealed Langevin Dynamics (NCSN)",
        xlabel="xâ‚", ylabel="xâ‚‚",
        label="Trajectory")
scatter!([-2.0, 2.0], [0.0, 0.0],
        markersize=10, color=:red, label="True Modes")
```

### 5.4 å®Ÿé¨“3: Standard LD vs Annealed LD æ¯”è¼ƒ

å˜ä¸€ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®LDã¨ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã®Annealed LDã‚’æ¯”è¼ƒã€‚

```julia
# Standard Langevin Dynamics (single noise level)
ps_single, _ = train_score_network(model, ps, st, 1000, 128, 0.5, 1e-3)
traj_single = langevin_sampler(model, ps_single, st, [10.0, 10.0], 1000, 0.01)

# Annealed Langevin Dynamics (multi-scale)
ps_ncsn, _ = train_ncsn(model, ps, st, Ïƒ_schedule, 1000, 128, 1e-3)
traj_annealed = annealed_langevin_sampler(model, ps_ncsn, st, Ïƒ_schedule, Ïƒ_schedule[1] * randn(2), 100, 0.1)

# Compare final samples
final_single = traj_single[end-99:end]
final_annealed = traj_annealed[end-99:end]

mean_single = mean([p[1] for p in final_single])
mean_annealed = mean([p[1] for p in final_annealed])

println("Standard LD mean xâ‚: $(mean_single)")
println("Annealed LD mean xâ‚: $(mean_annealed)")
println("Expected: close to Â±2")

# Visualize both
p1 = scatter([p[1] for p in final_single], [p[2] for p in final_single],
             title="Standard LD", xlabel="xâ‚", ylabel="xâ‚‚",
             markersize=2, alpha=0.5, legend=false)
scatter!(p1, [-2.0, 2.0], [0.0, 0.0], markersize=10, color=:red)

p2 = scatter([p[1] for p in final_annealed], [p[2] for p in final_annealed],
             title="Annealed LD (NCSN)", xlabel="xâ‚", ylabel="xâ‚‚",
             markersize=2, alpha=0.5, legend=false)
scatter!(p2, [-2.0, 2.0], [0.0, 0.0], markersize=10, color=:red)

plot(p1, p2, layout=(1, 2), size=(800, 400))
```

### 5.5 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] Fisher Divergenceã®å®šç¾©ã‚’æš—è¨˜ä¸è¦ã§å°å‡ºã§ãã‚‹
- [ ] HyvÃ¤rinen's Theoremã®éƒ¨åˆ†ç©åˆ†trickã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] DSMç›®çš„é–¢æ•° $\left\| s_\theta(\tilde{x}) + \frac{\epsilon}{\sigma} \right\|^2$ ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Sliced Score MatchingãŒESMã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã›ã‚‹
- [ ] Langevin Dynamicsã®é›¢æ•£åŒ– (Euler-Maruyama) ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Annealed LDã®ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Juliaã§DSM/NCSNã‚’è¨“ç·´ã—ã€ã‚¹ã‚³ã‚¢å ´ã‚’å¯è¦–åŒ–ã§ãã‚‹
- [ ] Rustã§Langevin Dynamicsã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’å®Ÿè£…ã§ãã‚‹

:::message
**é€²æ—: 85% å®Œäº†** NCSNè¨“ç·´ã¨Annealed Langevin Dynamicsã®å®Ÿè£…ã‚’å®Œäº†ã€‚æ¬¡ã¯Score Matchingç ”ç©¶ã®ç³»è­œã¨æœ€æ–°å‹•å‘ã‚’ä¿¯ç°ã™ã‚‹ã€‚
:::

---

## ğŸš€ 6. ç™ºå±•ã‚¾ãƒ¼ãƒ³ï¼ˆ20åˆ†ï¼‰â€” Score Matchingç ”ç©¶ã®ç³»è­œã¨æœ€æ–°å‹•å‘

### 6.1 Score-Based Generative Modelsã®ç³»è­œ

```mermaid
graph TD
    A["HyvÃ¤rinen 2005<br/>Explicit SM<br/>Fisher Div"] --> B["Vincent 2011<br/>Denoising SM<br/>DAEç­‰ä¾¡æ€§"]
    B --> C["Song+ 2019<br/>Sliced SM<br/>random projection"]
    B --> D["Song & Ermon 2019<br/>NCSN<br/>Annealed LD"]
    D --> E["Song+ 2021<br/>Score SDE<br/>VP/VE-SDEçµ±ä¸€"]
    E --> F["Ho+ 2020<br/>DDPM<br/>Îµ-prediction"]
    F --> G["Nichol & Dhariwal 2021<br/>Improved DDPM<br/>å­¦ç¿’åˆ†æ•£"]
    C --> H["Song+ 2024<br/>DDPMæ¼¸è¿‘åŠ¹ç‡æ€§<br/>çµ±è¨ˆçš„æœ€é©æ€§"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style D fill:#f3e5f5
    style F fill:#c8e6c9
    style H fill:#ffebee
```

### 6.2 Score Matchingã¨Diffusionã®æ¥ç¶šãƒãƒƒãƒ—

Score Matchingã¯Diffusion Modelsã®ç†è«–çš„æºæµã ã€‚

| Score Matching | Diffusion Models | æ¥ç¶š |
|:--------------|:----------------|:-----|
| **DSMç›®çš„é–¢æ•°** | **DDPMç›®çš„é–¢æ•°** | $\left\| s_\theta(\tilde{x}) + \frac{\epsilon}{\sigma} \right\|^2 \equiv \left\| \epsilon - \epsilon_\theta(x_t, t) \right\|^2$ |
| **ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚º $\{\sigma_i\}$** | **ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\{\beta_t\}$** | ä¸¡æ–¹ã¨ã‚‚ç²—â†’ç²¾ã®ãƒã‚¤ã‚ºéšå±¤ |
| **Annealed LD** | **Reverse Process** | $\sigma_L \to \sigma_1$ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â‰¡ $x_T \to x_0$ å¾©å…ƒ |
| **ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$** | **$\epsilon$-prediction** | $\epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} s_\theta(x_t, t)$ |

**Song et al. (2021)** ã®Score SDEã¯ã€ã“ã®æ¥ç¶šã‚’å®Œå…¨ã«çµ±ä¸€ã—ãŸ [^6]ã€‚

$$
dx = f(x, t) dt + g(t) \nabla_x \log p_t(x) dt + g(t) dW_t
$$

VP-SDE (DDPMå‹) ã¨ VE-SDE (NCSNå‹) ã‚’çµ±ä¸€çš„ã«è¨˜è¿°ã€‚ç¬¬37å›ã§å®Œå…¨ç†è«–ã‚’å­¦ã¶ã€‚

### 6.3 æœ€æ–°ç ”ç©¶ (2024-2026)

**2024-2026ã®ä¸»è¦é€²å±•**:

1. **DDPM Score Matchingã®æ¼¸è¿‘åŠ¹ç‡æ€§** [^7] (ICLR 2025):
   - DDPMã®ã‚¹ã‚³ã‚¢æ¨å®šãŒçµ±è¨ˆçš„ã«æœ€é©ï¼ˆFisheråŠ¹ç‡çš„ï¼‰ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜
   - ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆã®ç†è«–çš„æ­£å½“åŒ–

2. **Improved Sliced Score Matching**:
   - åˆ†æ•£ä½æ¸›æ‰‹æ³• (control variates)
   - é«˜æ¬¡å…ƒã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ”¹å–„

3. **Discrete Score Matching**:
   - é›¢æ•£ãƒ‡ãƒ¼ã‚¿ (ãƒ†ã‚­ã‚¹ãƒˆ) ã¸ã®Score Matchingæ‹¡å¼µ
   - Score Entropy Discrete Diffusion

4. **Score-based 3Dç”Ÿæˆ**:
   - Point clouds / meshes / NeRFã¸ã®å¿œç”¨

## ğŸ“ 6. æŒ¯ã‚Šè¿”ã‚Š + çµ±åˆã‚¾ãƒ¼ãƒ³ï¼ˆ30åˆ†ï¼‰â€” ã¾ã¨ã‚ã¨Course IVé€²è¡Œ

### 7.1 æœ¬è¬›ç¾©ã®æ ¸å¿ƒ â€” 4ã¤ã®é‡è¦çŸ¥è¦‹

**1. ã‚¹ã‚³ã‚¢é–¢æ•°ã¯æ­£è¦åŒ–å®šæ•°ä¸è¦**:

$$
\nabla_x \log p(x) = \nabla_x \log \frac{1}{Z} \exp(-E(x)) = -\nabla_x E(x) \quad (Z \text{ãŒæ¶ˆãˆã‚‹})
$$

EBMã®æ ¹æœ¬çš„å›°é›£ï¼ˆ$Z$ ã®è¨ˆç®—ä¸èƒ½ï¼‰ã‚’å›é¿ã™ã‚‹éµã€‚

**2. Denoising = Score Matching (Vincent 2011)**:

$$
\text{Denoising Autoencoderè¨“ç·´} \equiv \text{Score Functionå­¦ç¿’}
$$

ãƒã‚¤ã‚ºä»˜åŠ â†’é™¤å»ã¨ã„ã†ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¹ã‚¯ãŒã€ã‚¹ã‚³ã‚¢æ¨å®šã¨æ•°å­¦çš„ã«ç­‰ä¾¡ã€‚

**3. Langevin Dynamicsã¯Scoreé§†å‹•SDE**:

$$
dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t
$$

ã‚¹ã‚³ã‚¢é–¢æ•°ãŒã‚ã‚Œã°ã€åˆ†å¸ƒ $p(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ã€‚

**4. ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºãŒå®‰å®šæ€§ã®éµ**:

ä½å¯†åº¦é ˜åŸŸã§ã®æ¨å®šä¸å®‰å®šæ€§ â†’ $\{\sigma_i\}$ ã§ã‚«ãƒãƒ¼ç¯„å›²ã‚’éšå±¤åŒ– â†’ Annealed LDã§ç²—â†’ç²¾ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

### 7.2 Course IVãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— â€” ä»Šã©ã“ã«ã„ã‚‹ã‹

```mermaid
graph LR
    L33["ç¬¬33å›<br/>NF"] --> L34["ç¬¬34å›<br/>EBM"]
    L34 --> L35["ç¬¬35å›<br/>Score<br/>(ä»Šã“ã“)"]
    L35 --> L36["ç¬¬36å›<br/>DDPM"]
    L36 --> L37["ç¬¬37å›<br/>SDE"]
    L37 --> L38["ç¬¬38å›<br/>FMçµ±ä¸€"]

    L35 -.score=DDPM core.-> L36
    L35 -.Langevin=reverse.-> L37

    style L35 fill:#ffeb3b
    style L36 fill:#c8e6c9
```

**åˆ°é”ç‚¹**:
- Score Matchingã¨Langevin Dynamicsã®å®Œå…¨ç†è«–ã‚’ç¿’å¾—
- DSM/NCSNå®Ÿè£… â†’ Diffusionç†è§£ã®æº–å‚™å®Œäº†

**æ¬¡å›äºˆå‘Š (ç¬¬36å›: DDPM & ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)**:
- Forward process $q(x_t|x_0)$ ã®å®Œå…¨å°å‡º
- Reverse process $p_\theta(x_{t-1}|x_t)$ ã®ãƒ™ã‚¤ã‚ºåè»¢
- $\epsilon$-prediction = ã‚¹ã‚³ã‚¢æ¨å®šã®è¨¼æ˜
- DDIM / é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼æ¦‚è¦

### 7.3 FAQ â€” ã‚ˆãã‚ã‚‹è³ªå•ã¨å›ç­”

:::details **Q1: Score Matchingã¨MLEã®é•ã„ã¯ï¼Ÿ**

**A**: MLEã¯ $\log p_\theta(x)$ ã‚’ç›´æ¥æœ€å¤§åŒ–ã™ã‚‹ãŒã€$Z(\theta)$ ã®è¨ˆç®—ãŒå¿…è¦ã€‚Score Matchingã¯ $\nabla_x \log p_\theta(x)$ (ã‚¹ã‚³ã‚¢) ã‚’æ¨å®šã—ã€$Z(\theta)$ ã‚’å›é¿ã™ã‚‹ã€‚ä¸¡æ–¹ã¨ã‚‚åˆ†å¸ƒ $p_\theta(x)$ ã‚’å­¦ç¿’ã™ã‚‹ãŒã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒç•°ãªã‚‹ã€‚
:::

:::details **Q2: ãªãœDenoising SMãŒExplicit SMã¨ç­‰ä¾¡ãªã®ã‹ï¼Ÿ**

**A**: Vincent (2011) ã®è¨¼æ˜: ãƒã‚¤ã‚º $\sigma \to 0$ ã§ã€æ‘‚å‹•åˆ†å¸ƒ $q_\sigma(\tilde{x}) \to p_\text{data}(x)$ã€‚DSMç›®çš„é–¢æ•°ãŒ Fisher Divergence ã«åæŸã—ã€HyvÃ¤rinen's Theoremã‚ˆã‚Š ESM ã¨ç­‰ä¾¡ã€‚æ•°å­¦çš„ã«ã¯ $\sigma$ ã®æ¥µé™æ“ä½œã€‚
:::

:::details **Q3: Langevin Dynamicsã®åæŸã«ä½•ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ï¼Ÿ**

**A**: $O(d / \epsilon)$ ($d$=æ¬¡å…ƒã€$\epsilon$=ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º)ã€‚é«˜æ¬¡å…ƒã§é…ã„ãŒã€Manifoldä»®èª¬ä¸‹ã§ã¯å›ºæœ‰æ¬¡å…ƒ $d_\text{eff}$ ã§æ”¹å–„ã€‚å®Ÿç”¨ä¸Šã€Annealed LDã§ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æœ€é©åŒ–ãŒé‡è¦ã€‚
:::

:::details **Q4: NCSNã¨DDPMã®é•ã„ã¯ï¼Ÿ**

**A**: ä¸¡æ–¹ã¨ã‚‚ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºã§ã‚¹ã‚³ã‚¢æ¨å®šã€‚NCSN (2019) ã¯é€£ç¶šãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« + Annealed LDã€DDPM (2020) ã¯é›¢æ•£æ™‚åˆ» $t$ + Reverse processã€‚æ•°å­¦çš„ã«ã¯ç­‰ä¾¡ï¼ˆSong+ 2021 Score SDEã§çµ±ä¸€ï¼‰ã€‚
:::

:::details **Q5: Sliced SM vs Denoising SMã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ**

**A**: Denoising SMãŒå®Ÿè£…å®¹æ˜“ + å®Ÿç¸¾è±Šå¯Œ â†’ **ç¬¬ä¸€é¸æŠ**ã€‚Sliced SMã¯ãƒ˜ã‚·ã‚¢ãƒ³è¨ˆç®—ã®ç†è«–çš„ä»£æ›¿ã ãŒã€å®Ÿç”¨ä¸ŠDSMãŒæ”¯é…çš„ã€‚ç ”ç©¶ã§ã¯ä¸¡æ–¹è©¦ã™ä¾¡å€¤ã‚ã‚Šã€‚
:::

:::details **Q6: Score Matchingã¯VAEã‚„GANã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã®ã‹ï¼Ÿ**

**A**: **ã‚¿ã‚¹ã‚¯ä¾å­˜**ã€‚VAEã¯æ½œåœ¨ç©ºé–“ãŒæ˜ç¤ºçš„ã§ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ãƒ»è£œé–“ã«æœ‰åˆ©ã€‚GANã¯é«˜ç”»è³ªã ãŒè¨“ç·´ä¸å®‰å®šã€‚Score Matchingã¯å¯†åº¦æ¨å®šãŒå³å¯†ã ãŒã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒé…ã„ï¼ˆLangevinåå¾©ï¼‰ã€‚Diffusion Modelsã¯Score Matching + åŠ¹ç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã®èåˆã§ã€ç”»è³ªã¨å®‰å®šæ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å®Ÿç¾ã€‚
:::

:::details **Q7: ã‚¹ã‚³ã‚¢é–¢æ•°ã® "æ¬¡å…ƒã®å‘ªã„" ã¯ã‚ã‚‹ã‹ï¼Ÿ**

**A**: ã‚ã‚‹ã€‚é«˜æ¬¡å…ƒç©ºé–“ã§ã¯å¤§éƒ¨åˆ†ãŒä½å¯†åº¦é ˜åŸŸ â†’ ã‚¹ã‚³ã‚¢æ¨å®šãŒä¸å®‰å®šã€‚**è§£æ±ºç­–**: (1) ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºï¼ˆNCSNï¼‰ã§ä½å¯†åº¦é ˜åŸŸã‚’ã‚«ãƒãƒ¼ã€(2) Manifoldä»®èª¬ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«é›†ä¸­ï¼‰ã‚’æ´»ç”¨ã€(3) äº‹å‰å­¦ç¿’æ¸ˆã¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§Latentç©ºé–“ã«åŸ‹ã‚è¾¼ã¿ï¼ˆâ†’ Latent Diffusion, ç¬¬39å›ï¼‰ã€‚
:::

:::details **Q8: ULAã¯MHã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚ˆã‚Šé€Ÿã„ã®ã‹ï¼Ÿ**

**A**: **Yes**ã€‚ULA (Unadjusted Langevin) ã¯æ£„å´ã‚¹ãƒ†ãƒƒãƒ—ãªã— â†’ å…¨ã‚µãƒ³ãƒ—ãƒ«å—ç† â†’ é«˜é€Ÿã€‚ä»£å„Ÿ: å®šå¸¸åˆ†å¸ƒã‹ã‚‰ã®èª¤å·® $O(\epsilon)$ ï¼ˆ$\epsilon$=ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼‰ã€‚MHã¯å³å¯†ã ãŒæ£„å´ã§é…ã„ã€‚å®Ÿç”¨ä¸Šã€å°ã•ã„ $\epsilon$ ã§ULAèª¤å·®ã¯ç„¡è¦–å¯èƒ½ã€‚
:::

:::details **Q9: Score Matchingã¯æ•™å¸«ãªã—å­¦ç¿’ã‹ï¼Ÿ**

**A**: **Yes**ã€‚ãƒ©ãƒ™ãƒ«ä¸è¦ã€‚ãƒ‡ãƒ¼ã‚¿ $\{x_i\}$ ã®ã¿ã§ $\nabla_x \log p(x)$ ã‚’å­¦ç¿’ã€‚VAEã‚„GANã¨åŒã˜ãç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼æ•™å¸«ãªã—å­¦ç¿’ã€‚ãŸã ã—æ¡ä»¶ä»˜ãç”Ÿæˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒï¼‰ã§ã¯æ¡ä»¶ $c$ ãŒå¿…è¦ â†’ æ•™å¸«ã‚ã‚Šé¢¨ã ãŒã€**Conditional Score** $\nabla_x \log p(x|c)$ ã‚’æ¨å®šã™ã‚‹ç‚¹ã§æœ¬è³ªã¯å¤‰ã‚ã‚‰ãšã€‚
:::

:::details **Q10: Langevin Dynamicsã®"æ¸©åº¦"ã¯èª¿æ•´ã§ãã‚‹ã‹ï¼Ÿ**

**A**: **Yes**ã€‚æ¨™æº–å½¢ $dx = \nabla \log p dt + \sqrt{2T} dW$ ã® $T$ ãŒæ¸©åº¦ã€‚$T=1$ ã§ $p(x)$ ã«åæŸã€$T>1$ ã§åˆ†å¸ƒãŒå¹³å¦åŒ–ï¼ˆé«˜æ¸©ï¼ã‚µãƒ³ãƒ—ãƒ«å¤šæ§˜æ€§â†‘ï¼‰ã€$T<1$ ã§ãƒ”ãƒ¼ã‚¯é›†ä¸­ï¼ˆä½æ¸©ï¼ãƒ¢ãƒ¼ãƒ‰ä»˜è¿‘ï¼‰ã€‚Annealed LDã¯ã€Œæ¸©åº¦ä¸‹ã’ãªãŒã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€ã¨è§£é‡ˆå¯èƒ½ã€‚
:::

:::details **Q11: Score Matchingã§é›¢æ•£ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¯æ‰±ãˆã‚‹ã‹ï¼Ÿ**

**A**: åŸç†çš„ã«å›°é›£ï¼ˆ$\nabla_x$ ã¯é€£ç¶šå¤‰æ•°å‰æï¼‰ã€‚**è¿‘å¹´ã®è§£æ±º**:
1. **Embeddingâ†’é€£ç¶šåŒ–**: Token â†’ é€£ç¶šåŸ‹ã‚è¾¼ã¿ â†’ ã‚¹ã‚³ã‚¢æ¨å®š
2. **Discrete Score Matching**: é›¢æ•£çŠ¶æ…‹é·ç§»ã®"æ“¬ä¼¼å‹¾é…"å®šç¾©ï¼ˆLou+ 2024 [^9] Score Entropy Discrete Diffusionï¼‰
3. **Diffusion on discrete spaces**: Absorbing state diffusion (D3PM)

ç”»åƒãƒ»éŸ³å£°ï¼é€£ç¶šï¼ˆScoreç›´æ¥é©ç”¨å¯ï¼‰ã€ãƒ†ã‚­ã‚¹ãƒˆï¼é›¢æ•£ï¼ˆå·¥å¤«å¿…è¦ï¼‰ã€‚
:::

:::details **Q12: NCSNè¨“ç·´ã§ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯å¹¾ä½•ç´šæ•°å¿…é ˆï¼Ÿ**

**A**: **æ¨å¥¨ã ãŒå¿…é ˆã§ã¯ãªã„**ã€‚å¹¾ä½•ç´šæ•° $\sigma_i = \sigma_\text{min} \cdot r^i$ ($r>1$) ã¯ç²—â†’ç²¾ã‚’å¯¾æ•°çš„ã«ã‚«ãƒãƒ¼ã€å®Ÿé¨“çš„ã«ãƒ™ã‚¹ãƒˆã€‚ä»£æ›¿: (1) ç­‰å·®æ•°åˆ—ï¼ˆä½ãƒã‚¤ã‚ºéå‰°ï¼‰ã€(2) å­¦ç¿’å¯èƒ½ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆDPM-Solver++ï¼‰ã€‚DDPMã® $\beta_t$ ã‚‚ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆã§æ€§èƒ½å¤‰åŒ–ã€‚
:::

:::details **Q13: Score SDE (ç¬¬37å›) ã¨Score Matching (æœ¬è¬›ç¾©) ã®é–¢ä¿‚ã¯ï¼Ÿ**

**A**: Score Matching = **ã‚¹ã‚³ã‚¢æ¨å®šæ‰‹æ³•**ï¼ˆé›¢æ•£ãƒ‡ãƒ¼ã‚¿ã§ $\nabla_x \log p$ å­¦ç¿’ï¼‰ã€‚Score SDE = **é€£ç¶šæ‹¡æ•£éç¨‹ã®ç†è«–**ï¼ˆSDEè¦–ç‚¹ã§Diffusionçµ±ä¸€ï¼‰ã€‚é–¢ä¿‚:
- Score Matching â†’ ã‚¹ã‚³ã‚¢é–¢æ•° $\mathbf{s}_\theta(x, t)$ å­¦ç¿’
- Score SDE â†’ ãã®ã‚¹ã‚³ã‚¢ã§é€†SDEã‚’å®šç¾©: $dx = [f - g^2 \nabla \log p_t] dt + g d\bar{w}$

Score MatchingãŒãƒ„ãƒ¼ãƒ«ã§Score SDEãŒç†è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚
:::

:::details **Q14: Fisher Divergenceã¯å®Ÿç”¨ä¸Šä½¿ã‚ã‚Œã¦ã„ã‚‹ã®ã‹ï¼Ÿ**

**A**: ç†è«–çš„ãƒ„ãƒ¼ãƒ«ã€‚å®Ÿè£…ä¸Šã¯**HyvÃ¤rinen's Theoremã§å¤‰æ›ã—ãŸç›®çš„é–¢æ•°**ï¼ˆESM: $\text{tr}(\nabla s) + \frac{1}{2}\|s\|^2$ï¼‰ã‚„DSMï¼ˆ$\|\mathbf{s}_\theta(\tilde{x}) + \epsilon/\sigma\|^2$ï¼‰ã‚’ä½¿ã†ã€‚Fisher Divergenceè‡ªä½“ã‚’ç›´æ¥æœ€å°åŒ–ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã¯æ›¸ã‹ãªã„ã€‚ç†è«–è¨¼æ˜ã¨å®Ÿè£…ã®æ©‹æ¸¡ã—å½¹ã€‚
:::

:::details **Q15: Langevin Dynamicsã¯ç”»åƒç”Ÿæˆã§å®Ÿç”¨çš„ã‹ï¼Ÿ**

**A**: **å˜ä½“ã§ã¯é…ã„**ï¼ˆæ•°åƒã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ï¼‰ã€‚å®Ÿç”¨åŒ–ã®éµ:
1. **é«˜é€Ÿã‚µãƒ³ãƒ—ãƒ©ãƒ¼**: DDIMï¼ˆæ±ºå®šè«–çš„ã€50ã‚¹ãƒ†ãƒƒãƒ—ï¼‰, DPM-Solver++ï¼ˆ20ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
2. **ä¸€è²«æ€§è’¸ç•™**: Consistency Modelsï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ã€ç¬¬40å›ï¼‰
3. **Latent Diffusion**: ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã§é«˜é€ŸåŒ–ï¼ˆç¬¬39å›ï¼‰

Langevin Dynamicsã¯**ç†è«–çš„åŸºç›¤**ã€‚å®Ÿç”¨ã‚·ã‚¹ãƒ†ãƒ ã¯åŠ¹ç‡åŒ–æ‰‹æ³•ã¨çµ„ã¿åˆã‚ã›ã‚‹ã€‚
:::

### 7.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« â€” 1é€±é–“ãƒ—ãƒ©ãƒ³

| æ—¥ | å†…å®¹ | æ™‚é–“ | åˆ°é”ç›®æ¨™ |
|:---|:-----|:-----|:---------|
| **Day 1** | Zone 0-2 èª­äº† | 1h | Score Matchingå‹•æ©Ÿç†è§£ |
| **Day 2** | Zone 3.1-3.3 Fisher Div, ESM | 2h | HyvÃ¤rinen's Theoremå°å‡º |
| **Day 3** | Zone 3.4-3.6 DSM, Sliced SM | 2h | DSMç­‰ä¾¡æ€§è¨¼æ˜ |
| **Day 4** | Zone 3.7-3.10 Langevin, NCSN | 2h | Annealed LDå®Œå…¨ç†è§£ |
| **Day 5** | Zone 4 Juliaå®Ÿè£… | 2h | DSMè¨“ç·´ + ã‚¹ã‚³ã‚¢å ´å¯è¦–åŒ– |
| **Day 6** | Zone 5 NCSNå®Ÿé¨“ | 2h | Annealed LDå®Ÿè£… |
| **Day 7** | Zone 6-7 + Review | 1h | ç†è«–çµ±åˆ + æ¬¡å›æº–å‚™ |

### 7.5 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```julia
# Self-assessment checklist
checklist = Dict(
    "Fisher Divergenceå°å‡º" => false,
    "HyvÃ¤rinen's Theoremè¨¼æ˜" => false,
    "DSMç­‰ä¾¡æ€§ç†è§£" => false,
    "Sliced SMåŸç†" => false,
    "Langevin Dynamicså®Ÿè£…" => false,
    "Annealed LDåŸç†" => false,
    "NCSNè¨“ç·´å®Ÿè£…" => false
)

# Mark completed items
checklist["Fisher Divergenceå°å‡º"] = true  # etc.

completed = count(values(checklist))
total = length(checklist)

println("Progress: $(completed) / $(total) ($(round(100 * completed / total, digits=1))%)")

if completed == total
    println("ğŸ† Lecture 35 Completed! Ready for DDPM (Lecture 36).")
end
```

### 7.6 æ¬¡å›äºˆå‘Š â€” ç¬¬36å›: DDPM & ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

ç¬¬36å›ã§å­¦ã¶ã“ã¨:

1. **Forward Processå®Œå…¨å°å‡º**: $q(x_t|x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)$
2. **Reverse Process**: $p_\theta(x_{t-1}|x_t)$ ã®ãƒ™ã‚¤ã‚ºåè»¢
3. **ELBOåˆ†è§£**: $L_T + \sum_t L_t + L_0$ ã®å®Œå…¨å°å‡º
4. **$\epsilon$-prediction = Score**:
   $$
   \epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} \nabla_{x_t} \log p(x_t)
   $$
5. **DDIM**: Non-Markovian forward â†’ æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
6. **U-Net Architecture**: Time embedding / Self-Attention / Skip connection
7. **é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: DPM-Solver++ / Consistency Models

**æœ¬è¬›ç¾© (L35) ã¨DDPM (L36) ã®æ¥ç¶š**:

- L35ã®ã‚¹ã‚³ã‚¢é–¢æ•° â†’ L36ã®Îµ-prediction
- L35ã®Annealed LD â†’ L36ã®Reverse Process
- L35ã®NCSNæå¤± â†’ L36ã®DDPMæå¤±
- L35ã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚º â†’ L36ã®ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\beta_t$

Score Matchingã¯Diffusionã®ç†è«–çš„ãªå¿ƒè‡“éƒ¨ã€‚ç¬¬36å›ã§å®Œå…¨çµ±åˆã‚’ç›®æŒ‡ã™ã€‚

### 7.7 èª²é¡Œ â€” Hands-on Projects

**åˆç´šèª²é¡Œ: 1D Mixture of Gaussians**:

```julia
# 1D Gaussian mixture: p(x) = 0.33*N(-3,1) + 0.33*N(0,1) + 0.34*N(3,1)
# Task:
# 1. Implement DSM training for 1D data
# 2. Visualize learned score function s_Î¸(x)
# 3. Sample using Langevin Dynamics
# 4. Compare with true distribution

function sample_1d_gmm(n::Int)
    samples = zeros(n)
    for i in 1:n
        r = rand()
        if r < 0.33
            samples[i] = -3.0 + randn()
        elseif r < 0.66
            samples[i] = randn()
        else
            samples[i] = 3.0 + randn()
        end
    end
    return samples
end

# TODO: Implement DSM loss, training, and sampling
```

**ä¸­ç´šèª²é¡Œ: Swiss Roll Dataset**:

```julia
# 2D Swiss roll manifold
# Task:
# 1. Generate Swiss roll data
# 2. Train NCSN with multi-scale noise Ïƒ = [5.0, 2.5, 1.0, 0.5, 0.1]
# 3. Implement Annealed Langevin Dynamics
# 4. Visualize score field and sampling trajectory

using Plots

function swiss_roll(n::Int)
    t = 1.5 * Ï€ * (1 .+ 2 * rand(n))
    x = t .* cos.(t)
    y = t .* sin.(t)
    return hcat(x, y)'
end

# TODO: Implement NCSN training and Annealed LD
```

**ä¸Šç´šèª²é¡Œ: Image Denoising with Score Matching**:

```julia
# MNIST denoising
# Task:
# 1. Load MNIST dataset
# 2. Add Gaussian noise with Ïƒ = 0.5
# 3. Train DSM-based denoising model
# 4. Compare with standard denoising autoencoder
# 5. Measure PSNR / SSIM

using MLDatasets

mnist_train = MNIST.traindata()
X_train = mnist_train.features  # (28, 28, n_samples)

# TODO: Implement DSM for images
```

**Expertèª²é¡Œ: Rust + Julia FFI Integration**:

```rust
// Rust: High-performance Langevin sampler
// Task:
// 1. Implement multi-threaded Langevin Dynamics in Rust
// 2. Expose C-ABI interface for Julia
// 3. Benchmark against pure Julia implementation
// 4. Achieve >2x speedup on 10k samples

#[no_mangle]
pub extern "C" fn langevin_batch(
    score_fn: extern "C" fn(*const f64, usize) -> *mut f64,
    x_init: *const f64,
    n_samples: usize,
    n_steps: usize,
    step_size: f64,
    output: *mut f64,
) {
    // TODO: Implement batch sampling with rayon
}
```

```julia
# Julia: Call Rust sampler via ccall
const liblangevin = "./target/release/liblangevin.so"

function rust_langevin_batch(score_fn, x_init, n_samples, n_steps, step_size)
    # TODO: ccall to Rust
end

# Benchmark
@btime rust_langevin_batch(...)  # Target: <10ms for 1000 samples
```

**æœ¬è¬›ç¾© (ç¬¬35å›) ã§å­¦ã‚“ã Score MatchingãŒã€DDPMã®è¨“ç·´ç›®çš„é–¢æ•°ã®æ•°å­¦çš„åŸºç›¤ã«ãªã‚‹ã€‚** ç¬¬36å›ã‚’è¿ãˆã‚‹æº–å‚™ã¯æ•´ã£ãŸã€‚

:::message
**é€²æ—: 100% å®Œäº†** ğŸ‰ Lecture 35ã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆï¼
:::

---

## ğŸ“ æ•°å­¦è£œéº: å®Œå…¨è¨¼æ˜é›†

### A.1 HyvÃ¤rinen's Theoremã®å®Œå…¨è¨¼æ˜

**å®šç† (HyvÃ¤rinen 2005)**:

$$
\mathbb{E}_{p(x)} \left[ \frac{1}{2} \left\| \nabla_x \log p(x) - s_\theta(x) \right\|^2 \right] = \mathbb{E}_{p(x)} \left[ \text{tr}(\nabla_x s_\theta(x)) + \frac{1}{2} \|s_\theta(x)\|^2 \right] + C
$$

**è¨¼æ˜**:

LHSã‚’å±•é–‹:
$$
\mathbb{E}_p \left[ \frac{1}{2} \|\nabla \log p - s_\theta\|^2 \right] = \mathbb{E}_p \left[ \frac{1}{2} \|\nabla \log p\|^2 - (\nabla \log p)^\top s_\theta + \frac{1}{2} \|s_\theta\|^2 \right]
$$

ç¬¬2é …ã«éƒ¨åˆ†ç©åˆ†:
$$
\mathbb{E}_p[(\nabla \log p)^\top s_\theta] = \int p(x) \sum_i \frac{\partial \log p}{\partial x_i} s_{\theta,i}(x) dx = \int \sum_i \frac{\partial p}{\partial x_i} s_{\theta,i} dx
$$

éƒ¨åˆ†ç©åˆ†å…¬å¼ $\int \frac{\partial p}{\partial x_i} f = -\int p \frac{\partial f}{\partial x_i}$ (å¢ƒç•Œé …=0) ã‚ˆã‚Š:
$$
= -\int p \sum_i \frac{\partial s_{\theta,i}}{\partial x_i} dx = -\mathbb{E}_p[\text{tr}(\nabla s_\theta)]
$$

ä»£å…¥ã—ã¦:
$$
\mathbb{E}_p \left[ \frac{1}{2} \|\nabla \log p - s_\theta\|^2 \right] = \underbrace{\frac{1}{2} \mathbb{E}_p[\|\nabla \log p\|^2]}_{C} + \mathbb{E}_p[\text{tr}(\nabla s_\theta) + \frac{1}{2} \|s_\theta\|^2]
$$

### A.2 Vincent (2011) DSMç­‰ä¾¡æ€§ã®å®Œå…¨è¨¼æ˜

**å®šç†**: $\sigma \to 0$ ã§DSMç›®çš„é–¢æ•°ãŒFisher Divergenceã«åæŸã€‚

**è¨¼æ˜**:

DSMç›®çš„é–¢æ•°:
$$
\mathcal{L}_\text{DSM} = \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \left\| s_\theta(x + \sigma \epsilon) + \frac{\epsilon}{\sigma} \right\|^2 \right]
$$

$\tilde{x} = x + \sigma \epsilon$ ã¨ç½®æ›ã€‚å‘¨è¾ºåˆ†å¸ƒ $q_\sigma(\tilde{x}) = \int p(x) \mathcal{N}(\tilde{x} | x, \sigma^2 I) dx$ ã«å¯¾ã—ã¦:
$$
\mathcal{L}_\text{DSM} = \mathbb{E}_{q_\sigma(\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) + \mathbb{E}_{p(x|\tilde{x})} \left[ \frac{\tilde{x} - x}{\sigma^2} \right] \right\|^2 \right]
$$

**Tweedie's Formula** (Steinæ¨å®šé‡):
$$
\mathbb{E}_{p(x|\tilde{x})}[x] = \tilde{x} + \sigma^2 \nabla_{\tilde{x}} \log q_\sigma(\tilde{x})
$$

ã‚ˆã£ã¦:
$$
\mathbb{E}_{p(x|\tilde{x})} \left[ \frac{\tilde{x} - x}{\sigma^2} \right] = -\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})
$$

ä»£å…¥ã™ã‚‹ã¨:
$$
\mathcal{L}_\text{DSM} = \mathbb{E}_{q_\sigma(\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) \right\|^2 \right] = D_\text{Fisher}(q_\sigma \| p_\theta)
$$

$\sigma \to 0$ ã§ $q_\sigma \to p_\text{data}$ (ç•³ã¿è¾¼ã¿å®šç†) ã‚ˆã‚Š $\mathcal{L}_\text{DSM} \to D_\text{Fisher}(p_\text{data} \| p_\theta)$ã€‚

### A.3 Langevin Dynamicsã®åæŸä¿è¨¼

**å®šç† (Fokker-Planck equation)**:

SDE $dx_t = \nabla \log p(x_t) dt + \sqrt{2} dW_t$ ã®å®šå¸¸åˆ†å¸ƒã¯ $p(x)$ã€‚

**è¨¼æ˜**:

ç¢ºç‡å¯†åº¦ $\rho(x, t)$ ã®æ™‚é–“ç™ºå±• (Fokker-Planckæ–¹ç¨‹å¼):
$$
\frac{\partial \rho}{\partial t} = -\nabla \cdot (\rho b) + \nabla^2 \rho
$$

ã“ã“ã§ $b(x) = \nabla \log p(x)$, $D = 1$ (æ‹¡æ•£ä¿‚æ•°)ã€‚å±•é–‹ã™ã‚‹ã¨:
$$
\frac{\partial \rho}{\partial t} = -\nabla \rho \cdot \nabla \log p - \rho \nabla^2 \log p + \nabla^2 \rho
$$

$\rho = p$ (å®šå¸¸) ã‚’ä»£å…¥:
$$
0 = -\nabla p \cdot \nabla \log p - p \nabla^2 \log p + \nabla^2 p = -\frac{|\nabla p|^2}{p} - p \nabla^2 \log p + \nabla^2 p
$$

$\nabla^2 \log p = \frac{\nabla^2 p}{p} - \frac{|\nabla p|^2}{p^2}$ ã‚’ä½¿ã†ã¨:
$$
0 = -\frac{|\nabla p|^2}{p} - \nabla^2 p + |\nabla p|^2 / p + \nabla^2 p = 0
$$

ã‚ˆã£ã¦ $\rho = p$ ã¯å®šå¸¸è§£ã€‚

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **"âˆ‡log p(x) ã‚’çŸ¥ã‚‰ãšã« Diffusion ã‚’èªã‚Œã‚‹ã‹ï¼Ÿ"**

DDPMã®è«–æ–‡ (Ho et al. 2020) [^8] ã‚’èª­ã‚€ã¨ãã€ã»ã¨ã‚“ã©ã®èª­è€…ã¯ã€Œ$\epsilon$-predictionã€ã¨ã„ã†è¡¨ç¾ã‚’é¡é¢é€šã‚Šã«å—ã‘å–ã‚‹ã€‚ã€Œãƒã‚¤ã‚ºã‚’å½“ã¦ã‚‹ã‚¿ã‚¹ã‚¯ã€ã¨ã—ã¦ã€‚

ã ãŒæœ¬è³ªã¯é•ã†ã€‚**$\epsilon$-prediction = Score Matching**ã€‚

$$
\epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} \nabla_{x_t} \log p(x_t)
$$

ã“ã®å¼ãŒè¦‹ãˆãªã„é™ã‚Šã€Diffusionã¯ã€Œãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã€ã®ã¾ã¾ã ã€‚

**3ã¤ã®è¦–ç‚¹**:

1. **è¡¨é¢**: DDPMã¯ãƒã‚¤ã‚ºé™¤å»ã®åå¾© â†’ ç›´æ„Ÿçš„ã ãŒæµ…ã„
2. **ä¸­å±¤**: DDPMã¯Denoising Score Matchingã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç‰ˆ â†’ æœ¬è¬›ç¾©ã®åˆ°é”ç‚¹
3. **æ·±å±¤**: DDPMã¯Score SDE $dx = f dt + g \nabla \log p dt + g dW$ ã®é›¢æ•£åŒ– â†’ ç¬¬37å›ã§å­¦ã¶

Score Matchingã¨Langevin Dynamicsã®ç†è«–ãªã—ã«ã€Diffusionã®æ•°å­¦çš„æœ¬è³ªã¯è¦‹ãˆãªã„ã€‚

**å•ã„**:
- ã‚ãªãŸã®ç†è§£ã¯ã€Œå±¤1: ãƒã‚¤ã‚ºé™¤å»ã®åå¾©ã€ã«ã¨ã©ã¾ã£ã¦ã„ã‚‹ã‹ï¼Ÿ
- Score SDE (å±¤3) ã¾ã§åˆ°é”ã—ãŸã¨ãã€VAE/GAN/Flow/Diffusionã®çµ±ä¸€çš„è¦–ç‚¹ãŒè¦‹ãˆã‚‹ã‹ï¼Ÿ
- Score Matchingã¯ã€Œå¤ã„ç†è«–ã€ã‹ã€ãã‚Œã¨ã‚‚ã€Œå…¨ã¦ã®åŸºç›¤ã€ã‹ï¼Ÿ

:::details æ­´å²çš„æ–‡è„ˆ

- **2005**: HyvÃ¤rinenã€Explicit Score Matchingææ¡ˆ â†’ å½“æ™‚ã¯ãƒ‹ãƒƒãƒãªæ‰‹æ³•
- **2011**: Vincentã€Denoising SMã¨DAEã®ç­‰ä¾¡æ€§è¨¼æ˜ â†’ å®Ÿç”¨æ€§å‘ä¸Š
- **2019**: Song & Ermonã€NCSNç™ºè¡¨ â†’ Score-basedç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¨¼
- **2020**: Ho et al.ã€DDPMç™ºè¡¨ â†’ ã€Œãƒã‚¤ã‚ºé™¤å»ã€ã¨ã—ã¦æç¤ºã€Scoreè¨€åŠãªã—
- **2021**: Song et al.ã€Score SDEç™ºè¡¨ â†’ DDPM/NCSNã®çµ±ä¸€ã€Scoreç†è«–ãŒåŸºç›¤ã¨åˆ¤æ˜
- **2025**: DDPM Score Matchingã®æ¼¸è¿‘åŠ¹ç‡æ€§è¨¼æ˜ â†’ Scoreç†è«–ã®å†è©•ä¾¡

**ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›**: DDPMã¯ã€Œæ–°ã—ã„ç™ºæ˜ã€ã§ã¯ãªãã€Score Matchingã®ã€Œå·¥å­¦çš„æ´—ç·´ã€ã ã£ãŸã€‚

:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: HyvÃ¤rinen, A. (2005). "Estimation of Non-Normalized Statistical Models by Score Matching." *Journal of Machine Learning Research*, 6(24), 695â€“709.
@[card](https://jmlr.org/papers/v6/hyvarinen05a.html)

[^2]: Vincent, P. (2011). "A Connection Between Score Matching and Denoising Autoencoders." *Neural Computation*, 23(7), 1661â€“1674.
@[card](https://direct.mit.edu/neco/article/23/7/1661/7677/A-Connection-Between-Score-Matching-and-Denoising)

[^3]: Song, Y., Garg, S., Shi, J., & Ermon, S. (2019). "Sliced Score Matching: A Scalable Approach to Density and Score Estimation." *UAI 2019*.
@[card](https://arxiv.org/abs/1905.07088)

[^4]: Welling, M., & Teh, Y. W. (2011). "Bayesian Learning via Stochastic Gradient Langevin Dynamics." *ICML 2011*.
@[card](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)

[^5]: Song, Y., & Ermon, S. (2019). "Generative Modeling by Estimating Gradients of the Data Distribution." *NeurIPS 2019*.
@[card](https://arxiv.org/abs/1907.05600)

[^6]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). "Score-Based Generative Modeling through Stochastic Differential Equations." *ICLR 2021*.
@[card](https://arxiv.org/abs/2011.13456)

[^7]: Che, T., Kumar, R., & Bengio, Y. (2024). "On the Statistical Efficiency of Denoising Diffusion Models." *ICLR 2025*.
@[card](https://arxiv.org/abs/2504.05161)

[^8]: Ho, J., Jain, A., & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS 2020*.
@[card](https://arxiv.org/abs/2006.11239)

### æ•™ç§‘æ›¸

- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Chapter 25: Score-Based Models]
- Shalev-Shwartz, S., & Ben-David, S. (2024). *Foundations of Deep Learning*. Cambridge University Press.

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- [Yang Song's Blog: Score-Based Generative Models](https://yang-song.net/blog/2021/score/)
- [Lil'Log: "What are Diffusion Models?"](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
- [MIT 6.S184 (2026): Generative AI](https://diffusion.csail.mit.edu/)

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|
| $p(x)$ | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ / çœŸã®åˆ†å¸ƒ | Zone 1 |
| $q_\theta(x)$ | ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒ (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\theta$) | Zone 3.2 |
| $s(x) = \nabla_x \log p(x)$ | ã‚¹ã‚³ã‚¢é–¢æ•° | Zone 0 |
| $s_\theta(x)$ | ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢é–¢æ•° | Zone 3.1 |
| $Z(\theta)$ | æ­£è¦åŒ–å®šæ•°ï¼ˆpartition functionï¼‰ | Zone 2.1 |
| $E(x; \theta)$ | ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•° | Zone 2.1 |
| $D_\text{Fisher}(p \| q)$ | Fisher Divergence | Zone 3.2 |
| $J_\text{ESM}(\theta)$ | Explicit Score Matchingç›®çš„é–¢æ•° | Zone 3.3 |
| $J_\text{DSM}(\theta; \sigma)$ | Denoising Score Matchingç›®çš„é–¢æ•° | Zone 3.4 |
| $J_\text{SSM}(\theta)$ | Sliced Score Matchingç›®çš„é–¢æ•° | Zone 3.5 |
| $\tilde{x} = x + \sigma \epsilon$ | ãƒã‚¤ã‚ºä»˜åŠ ãƒ‡ãƒ¼ã‚¿ | Zone 0 |
| $\sigma$ | ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« | Zone 1.3 |
| $\{\sigma_i\}_{i=1}^L$ | ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« | Zone 3.6 |
| $\epsilon \sim \mathcal{N}(0, I)$ | ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚º | Zone 0 |
| $W_t$ | Browné‹å‹• (Wiener process) | Zone 3.7 |
| $\epsilon$ (Langevin) | ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º | Zone 3.7 |
| $\alpha_i$ | ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $i$ ã§ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º | Zone 3.8 |
| ULA | Unadjusted Langevin Algorithm | Zone 3.7 |
| SGLD | Stochastic Gradient Langevin Dynamics | Zone 3.8 |
| NCSN | Noise Conditional Score Networks | Zone 3.10 |

**è¨˜å·ã®è¡çªæ³¨æ„**:
- $\epsilon$ ã¯ãƒã‚¤ã‚ºå¤‰æ•° (Zone 0-3) ã¨ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º (Zone 3.7-) ã§ç•°ãªã‚‹æ„å‘³
- æ–‡è„ˆã‹ã‚‰åˆ¤æ–­ã™ã‚‹ã“ã¨

---

**è‘—è€…**: Claude Educator Agent (Sonnet 4.5)
**ç›£ä¿®**: Tech Lead (Opus 4.6)
**ã‚·ãƒªãƒ¼ã‚º**: æ·±å±¤ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å®Œå…¨è¬›ç¾©ï¼ˆå…¨46å›ï¼‰
---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
