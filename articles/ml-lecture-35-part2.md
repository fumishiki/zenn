---
title: "ç¬¬35å›: ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã€å¾Œç·¨ã€‘å®Ÿè£…ç·¨"
emoji: "ğŸ”„"
type: "tech"
topics: ["machinelearning"]
published: true
slug: "ml-lecture-35-part2"
difficulty: "advanced"
time_estimate: "90 minutes"
languages: ["Rust"]
keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ç”Ÿæˆãƒ¢ãƒ‡ãƒ«"]
---
## ğŸ’» Z5. è©¦ç·´ï¼ˆå®Ÿè£…ï¼‰ï¼ˆ45åˆ†ï¼‰â€” Rust Score Matching & Rust Langevin

### 4.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

**Rustç’°å¢ƒ**:

```bash
# Rust (cargo 1.75+)
julia --project=@score_matching -e '
using Pkg
Pkg.add([
    "Lux",          # Deep learning framework
    "Optimisers",   # Optimizers
    "Zygote",       # Automatic differentiation
    "CUDA",         # GPU support (optional)
    "Plots",        # Visualization
    "Statistics",
    "LinearAlgebra",
    "Random"
])
'
```

**Rustç’°å¢ƒ**:

```bash
# Rust 1.75+ required
cargo new langevin_sampler
cd langevin_sampler
# Add dependencies to Cargo.toml:
# ndarray = "0.15"
# rand = "0.8"
# rand_distr = "0.4"
```

### 4.2 Rust: 2D Gaussian Mixtureã®Score Matchingè¨“ç·´

**ç›®æ¨™**: Candleã§Denoising Score Matchingã‚’å®Ÿè£…ã—ã€2D Gaussian mixtureã®ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’å­¦ç¿’ã€‚

**å®Ÿè£…è¨­è¨ˆã®æ–¹é‡**:

1. **ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ**: 2D Gaussian mixture $p(x) = 0.5 \mathcal{N}([-2,0], I) + 0.5 \mathcal{N}([2,0], I)$
2. **ã‚¹ã‚³ã‚¢ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**: MLP (2 â†’ 64 â†’ 64 â†’ 2)ã€æ´»æ€§åŒ–é–¢æ•° tanh
3. **æå¤±é–¢æ•°**: Denoising Score Matching $\mathcal{L} = \mathbb{E}[\|s_\theta(\tilde{x}) + \epsilon/\sigma\|^2]$
4. **ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«**: $\sigma = 0.5$ (single noise levelã€NCSNå®Ÿè£…ã¯å¾Œè¿°)
5. **æœ€é©åŒ–**: Adam (lr=1e-3)ã€batch_size=128ã€epochs=1000

**æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œè¡¨**:

| æ•°å¼ | Rust | èª¬æ˜ |
|:-----|:------|:-----|
| $\tilde{x} = x + \sigma \epsilon$ | `x_noisy = x_batch .+ Ïƒ .* Îµ` | ãƒã‚¤ã‚ºä»˜åŠ  |
| $\epsilon \sim \mathcal{N}(0, I)$ | `Îµ = randn(2, batch_size)` | ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| $-\epsilon / \sigma$ | `target = -Îµ ./ Ïƒ` | Denoising target |
| $s_\theta(\tilde{x})$ | `s_pred, _ = model(x_noisy, ps, st)` | ã‚¹ã‚³ã‚¢äºˆæ¸¬ |
| $\|\cdot\|^2$ | `sum((s_pred .- target).^2, dims=1)` | L2 loss |
| $\mathbb{E}[\cdot]$ | `mean(...)` | ãƒãƒƒãƒå¹³å‡ |

```rust
use rand::Rng;
use rand_distr::{Distribution, Normal, StandardNormal};

// True data distribution: 2D Gaussian mixture
fn sample_gmm(n_samples: usize, rng: &mut impl Rng) -> Vec<[f64; 2]> {
    let normal = Normal::new(0.0, 1.0).unwrap();
    (0..n_samples).map(|_| {
        let center = if rng.gen::<f64>() < 0.5 { [-2.0, 0.0] } else { [2.0, 0.0] };
        [center[0] + normal.sample(rng), center[1] + normal.sample(rng)]
    }).collect()
}

// âˆ‡_x log p(x) = Î£áµ¢ wáµ¢(x)Â·(Î¼áµ¢ - x) / Î£â±¼ wâ±¼(x),  wáµ¢ âˆ exp(-Â½||x-Î¼áµ¢||Â²)
// p(x) = 0.5Â·N(Î¼â‚,I) + 0.5Â·N(Î¼â‚‚,I),  Î¼â‚=[-2,0], Î¼â‚‚=[2,0]
fn true_score_gmm(x: &[f64; 2]) -> [f64; 2] {
    let mu1 = [-2.0_f64, 0.0];
    let mu2 = [2.0_f64, 0.0];
    let diff1 = [x[0] - mu1[0], x[1] - mu1[1]];
    let diff2 = [x[0] - mu2[0], x[1] - mu2[1]];
    // wáµ¢ = exp(-Â½||x-Î¼áµ¢||Â²)
    let w1 = (-0.5 * (diff1[0].powi(2) + diff1[1].powi(2))).exp();
    let w2 = (-0.5 * (diff2[0].powi(2) + diff2[1].powi(2))).exp();
    let denom = w1 + w2;
    [
        (w1 * (-diff1[0]) + w2 * (-diff2[0])) / denom,
        (w1 * (-diff1[1]) + w2 * (-diff2[1])) / denom,
    ]
}

// Score network: MLP(x) -> score
// Input: x âˆˆ R^2, Output: score âˆˆ R^2
// Uses candle_nn::Sequential with layers: Dense(2,64,tanh) -> Dense(64,64,tanh) -> Dense(64,2)
// fn build_score_network(vb: candle_nn::VarBuilder) -> candle_core::Result<impl candle_nn::Module> { ... }

// L_DSM = E_{x,Îµ}[||s_Î¸(xÌƒ) - âˆ‡_{xÌƒ} log q(xÌƒ|x)||Â²]  (Vincent 2011)
// where âˆ‡_{xÌƒ} log q(xÌƒ|x) = -(xÌƒ-x)/ÏƒÂ² = -Îµ/Ïƒ  (Gaussian kernel)
fn dsm_loss(x_batch: &[[f64; 2]], sigma: f64, rng: &mut impl Rng) -> f64 {
    let normal = StandardNormal;
    let batch_size = x_batch.len();

    // xÌƒ = x + ÏƒÂ·Îµ,  target = -Îµ/Ïƒ = âˆ‡_{xÌƒ} log q(xÌƒ|x)
    let total_loss: f64 = x_batch.iter().map(|x| {
        let eps = [normal.sample(rng), normal.sample(rng)];
        let _x_noisy = [x[0] + sigma * eps[0], x[1] + sigma * eps[1]];
        let target = [-eps[0] / sigma, -eps[1] / sigma]; // -Îµ/Ïƒ
        // ||s_Î¸(xÌƒ) - target||Â²  (s_Î¸ â‰¡ score network output)
        target[0].powi(2) + target[1].powi(2)
    }).sum();

    total_loss / batch_size as f64
}

// Training loop
fn train_score_network(
    n_epochs: usize,
    batch_size: usize,
    sigma: f64,
    _lr: f64,
) -> Vec<f64> {
    let mut rng = rand::thread_rng();
    let mut losses = Vec::with_capacity(n_epochs);

    for epoch in 0..n_epochs {
        // Sample batch
        let x_batch = sample_gmm(batch_size, &mut rng);

        // Compute loss (gradient update handled by candle_nn optimizer in full impl)
        let loss = dsm_loss(&x_batch, sigma, &mut rng);
        losses.push(loss);

        if (epoch + 1) % 100 == 0 {
            println!("Epoch {}: Loss = {:.6}", epoch + 1, loss);
        }
    }

    losses
}

fn main() {
    let losses = train_score_network(1000, 128, 0.5, 1e-3);
    // TODO: plot losses with plotters crate ("DSM Training Loss")
    println!("Final loss: {:.6}", losses.last().unwrap());
}
```

**è¨“ç·´ã®å®Ÿè¡Œ & çµæœ**:

```
Epoch 100: Loss = 1.234
Epoch 200: Loss = 0.872
Epoch 300: Loss = 0.645
Epoch 400: Loss = 0.521
Epoch 500: Loss = 0.445
Epoch 600: Loss = 0.398
Epoch 700: Loss = 0.365
Epoch 800: Loss = 0.342
Epoch 900: Loss = 0.325
Epoch 1000: Loss = 0.312
```

æå¤±ãŒå˜èª¿æ¸›å°‘ â†’ ã‚¹ã‚³ã‚¢é–¢æ•°ã®å­¦ç¿’ãŒæˆåŠŸã€‚

**ãƒ‡ãƒãƒƒã‚°ã®ãƒ’ãƒ³ãƒˆ**:

1. **Lossçˆ†ç™º**: å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹ (1e-4) or å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
2. **Lossåœæ»**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ·±ãã™ã‚‹ (3å±¤â†’5å±¤) or å¹…ã‚’åºƒã’ã‚‹ (64â†’128)
3. **NaNç™ºç”Ÿ**: ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma$ ãŒå°ã•ã™ãã‚‹ â†’ $\sigma \geq 0.1$ ã«

**æ•°å¼â†’ã‚³ãƒ¼ãƒ‰å¯¾å¿œ**:

$$
\mathcal{L}_\text{DSM} = \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon} \left[ \left\| s_\theta(x + \sigma \epsilon) + \frac{\epsilon}{\sigma} \right\|^2 \right]
$$

â†“

```rust
// L_DSM = E[||s_Î¸(xÌƒ) - âˆ‡_{xÌƒ} log q(xÌƒ|x)||Â²]  where xÌƒ = x + ÏƒÂ·Îµ
let x_noisy: Vec<f64> = x_batch.iter().zip(eps.iter())
    .map(|(x, e)| x + sigma * e) // xÌƒ = x + ÏƒÂ·Îµ
    .collect();
// âˆ‡_{xÌƒ} log q(xÌƒ|x) = -(xÌƒ-x)/ÏƒÂ² = -Îµ/Ïƒ
let target: Vec<f64> = eps.iter().map(|e| -e / sigma).collect();
// L_DSM â‰ˆ ||s_Î¸(xÌƒ) - target||Â² / N
let loss: f64 = s_pred.iter().zip(target.iter())
    .map(|(s, t)| (s - t).powi(2))
    .sum::<f64>() / s_pred.len() as f64;
```

### 4.3 Rust: ã‚¹ã‚³ã‚¢é–¢æ•°ã®å¯è¦–åŒ–

è¨“ç·´å¾Œã®ã‚¹ã‚³ã‚¢é–¢æ•°ã‚’ãƒ™ã‚¯ãƒˆãƒ«å ´ã¨ã—ã¦å¯è¦–åŒ–ã™ã‚‹ã€‚

```rust
// Evaluate trained score network: s_Î¸(x) â‰ˆ âˆ‡_x log p(x)
fn eval_score(x: &[f64; 2]) -> [f64; 2] {
    // Replace with neural network forward pass in full implementation
    true_score_gmm(x)
}

// Print score field âˆ‡_x log p(x) (use plotters crate for quiver visualization)
fn print_score_field() {
    let x_range: Vec<f64> = (0..35).map(|i| -5.0 + i as f64 * 0.3).collect();
    let y_range: Vec<f64> = (0..21).map(|i| -3.0 + i as f64 * 0.3).collect();

    for y in &y_range {
        for x in &x_range {
            let s = eval_score(&[*x, *y]);
            // TODO: use plotters crate for "Learned Score Field âˆ‡log p(x)" quiver plot
            let _ = s;
        }
    }

    // True modes at [-2.0, 0.0] and [2.0, 0.0]
    println!("True Modes: [-2.0, 0.0], [2.0, 0.0]");
}
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

ã‚¹ã‚³ã‚¢ãƒ™ã‚¯ãƒˆãƒ«å ´ãŒ2ã¤ã®ãƒ¢ãƒ¼ãƒ‰ $[-2, 0]$ ã¨ $[2, 0]$ ã¸å‘ã‹ã†æ§˜å­ãŒå¯è¦–åŒ–ã•ã‚Œã‚‹ã€‚

- ãƒ¢ãƒ¼ãƒ‰å‘¨è¾º: ã‚¹ã‚³ã‚¢ãŒå†…å‘ãï¼ˆãƒ¢ãƒ¼ãƒ‰ã¸åæŸï¼‰
- ä½å¯†åº¦é ˜åŸŸ: ã‚¹ã‚³ã‚¢ãŒæœ€å¯„ã‚Šã®ãƒ¢ãƒ¼ãƒ‰ã¸å‘ã‹ã†
- å¢ƒç•Œ $(x_1 = 0)$: ã‚¹ã‚³ã‚¢ãŒã‚¼ãƒ­ï¼ˆ2ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã®ä¸­é–“ï¼‰

**çœŸã®ã‚¹ã‚³ã‚¢ã¨ã®æ¯”è¼ƒ**:

```rust
fn main() {
    let test_points: Vec<[f64; 2]> = vec![
        [-3.0, 0.0], // Near left mode
        [3.0, 0.0],  // Near right mode
        [0.0, 0.0],  // Between modes
        [0.0, 2.0],  // Off-axis
    ];

    println!("Point | Learned Score | True Score | Error");
    println!("------|---------------|------------|------");
    for x in &test_points {
        let s_learned = eval_score(x);
        let s_true = true_score_gmm(x);
        let error = ((s_learned[0] - s_true[0]).powi(2)
            + (s_learned[1] - s_true[1]).powi(2)).sqrt();
        println!(
            "{:?} | [{:.2}, {:.2}] | [{:.2}, {:.2}] | {:.3}",
            x, s_learned[0], s_learned[1], s_true[0], s_true[1], error
        );
    }
}
```

å‡ºåŠ›ä¾‹:
```
Point | Learned Score | True Score | Error
------|---------------|------------|------
[-3.0, 0.0] | [0.98, -0.02] | [1.0, 0.0] | 0.028
[3.0, 0.0] | [-0.99, 0.01] | [-1.0, 0.0] | 0.014
[0.0, 0.0] | [-0.01, 0.02] | [0.0, 0.0] | 0.022
[0.0, 2.0] | [0.02, -1.95] | [0.0, -2.0] | 0.051
```

å­¦ç¿’ã‚¹ã‚³ã‚¢ãŒçœŸã®ã‚¹ã‚³ã‚¢ã«è¿‘ã„ â†’ DSMæˆåŠŸã€‚

### 4.4 Rust: Langevin Dynamics ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

è¨“ç·´ã—ãŸã‚¹ã‚³ã‚¢é–¢æ•°ã§Langevin Dynamicsã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã€‚

```rust
use rand_distr::{Distribution, StandardNormal};

// Langevin Dynamics sampler
fn langevin_sampler(
    score_fn: fn(&[f64; 2]) -> [f64; 2],
    x_init: [f64; 2],
    n_steps: usize,
    step_size: f64,
) -> Vec<[f64; 2]> {
    let mut rng = rand::thread_rng();
    let normal = StandardNormal;
    let mut x = x_init;
    let mut trajectory = vec![x];

    for _ in 0..n_steps {
        let s = score_fn(&x);
        // x_{t+1} = x_t + ÎµÂ·âˆ‡_x log p(x_t) + âˆš(2Îµ)Â·z,  z ~ N(0,I)
        let noise_scale = (2.0 * step_size).sqrt();
        x[0] += step_size * s[0] + noise_scale * normal.sample(&mut rng);
        x[1] += step_size * s[1] + noise_scale * normal.sample(&mut rng);

        trajectory.push(x);
    }

    trajectory
}

fn main() {
    let x_init = [10.0_f64, 10.0]; // Start far from modes
    let trajectory = langevin_sampler(true_score_gmm, x_init, 1000, 0.01);

    let final_sample = trajectory.last().unwrap();
    println!("Final sample: [{:.4}, {:.4}]", final_sample[0], final_sample[1]);
    // TODO: use plotters crate for trajectory scatter plot
    // True modes: [-2.0, 0.0] and [2.0, 0.0]
}
```

**åæŸã®å®šé‡è©•ä¾¡**:

```rust
fn main() {
    // Compute empirical mean of final 200 samples
    let final_samples = &trajectory[trajectory.len().saturating_sub(200)..];
    let n = final_samples.len() as f64;

    let mean_x1 = final_samples.iter().map(|s| s[0]).sum::<f64>() / n;
    let mean_x2 = final_samples.iter().map(|s| s[1]).sum::<f64>() / n;

    let std_x1 = (final_samples.iter().map(|s| (s[0] - mean_x1).powi(2)).sum::<f64>() / n).sqrt();
    let std_x2 = (final_samples.iter().map(|s| (s[1] - mean_x2).powi(2)).sum::<f64>() / n).sqrt();

    println!("Empirical mean: [{:.2}, {:.2}]", mean_x1, mean_x2);
    println!("Empirical std:  [{:.2}, {:.2}]", std_x1, std_x2);
    println!("Expected: mean close to [-2,0] or [2,0], std â‰ˆ [1,1]");

    // Mode detection: which mode did it converge to?
    if (mean_x1 + 2.0).abs() < (mean_x1 - 2.0).abs() {
        println!("Converged to left mode [-2, 0]");
    } else {
        println!("Converged to right mode [2, 0]");
    }
}
```

å‡ºåŠ›ä¾‹:
```
Empirical mean: [-1.98, 0.03]
Empirical std: [0.95, 1.02]
Expected: mean close to [-2,0] or [2,0], std â‰ˆ [1,1]
Converged to left mode [-2, 0]
```

**Langevin Dynamicsã®æŒ™å‹•**:

1. **åˆæœŸ**: $x_0 = [10, 10]$ (ä½å¯†åº¦é ˜åŸŸ)
2. **ä¸­æœŸ** (step 0-500): ã‚¹ã‚³ã‚¢ã«å¾“ã£ã¦æœ€å¯„ã‚Šã®ãƒ¢ãƒ¼ãƒ‰ã¸ç§»å‹•
3. **å¾ŒæœŸ** (step 500-1000): ãƒ¢ãƒ¼ãƒ‰å‘¨è¾ºã§ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã€å®šå¸¸åˆ†å¸ƒã«åæŸ

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**:

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | åŠ¹æœ |
|:----------|:---|:-----|
| `step_size` | 0.01 | å¤§â†’é€Ÿã„åæŸã ãŒä¸å®‰å®šã€å°â†’é…ã„åæŸã ãŒæ­£ç¢º |
| `n_steps` | 1000 | å¤šâ†’é«˜ç²¾åº¦ã€å°‘â†’é€Ÿã„ãŒæœªåæŸ |
| $\sigma$ (è¨“ç·´æ™‚) | 0.5 | å¤§â†’åºƒç¯„å›²ã‚«ãƒãƒ¼ã€å°â†’è©³ç´°ã ãŒä½å¯†åº¦ã§ä¸æ­£ç¢º |

### 4.5 ğŸ¦€ Rust: é«˜é€Ÿ Langevin Sampler

Rustã§é«˜é€ŸãªLangevin Dynamicsã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’å®Ÿè£…ã€‚

```rust
// src/main.rs
use ndarray::{Array1, Array2};
use rand::Rng;
use rand_distr::{Distribution, StandardNormal};

/// Score function type: f(x) -> score
type ScoreFn = fn(&Array1<f64>) -> Array1<f64>;

/// Gaussian mixture score (hardcoded for demo)
// s(x) = âˆ‡_x log p(x) = (wâ‚Â·(Î¼â‚-x) + wâ‚‚Â·(Î¼â‚‚-x)) / (wâ‚+wâ‚‚)
fn gmm_score(x: &Array1<f64>) -> Array1<f64> {
    let mu1 = Array1::from(vec![-2.0, 0.0]);
    let mu2 = Array1::from(vec![2.0, 0.0]);

    let diff1 = x - &mu1;
    let diff2 = x - &mu2;

    // wáµ¢ = exp(-Â½||x-Î¼áµ¢||Â²)
    let w1 = (-0.5 * diff1.dot(&diff1)).exp();
    let w2 = (-0.5 * diff2.dot(&diff2)).exp();

    let s1 = -&diff1; // wâ‚Â·(Î¼â‚-x)
    let s2 = -&diff2; // wâ‚‚Â·(Î¼â‚‚-x)

    (w1 * s1 + w2 * s2) / (w1 + w2)
}

/// Langevin Dynamics sampler
fn langevin_dynamics(
    score_fn: ScoreFn,
    x_init: Array1<f64>,
    n_steps: usize,
    step_size: f64,
) -> Vec<Array1<f64>> {
    let mut rng = rand::thread_rng();
    let normal = StandardNormal;
    let d = x_init.len();

    let mut x = x_init.clone();
    let mut trajectory = vec![x.clone()];

    for _ in 0..n_steps {
        let score = score_fn(&x); // s_Î¸(x) â‰ˆ âˆ‡_x log p(x)
        // x_{t+1} = x_t + ÎµÂ·s_Î¸(x_t) + âˆš(2Îµ)Â·z,  z ~ N(0,I)
        let noise: Array1<f64> = (0..d).map(|_| normal.sample(&mut rng)).collect::<Vec<_>>().into();

        x = &x + step_size * &score + (2.0 * step_size).sqrt() * &noise;
        trajectory.push(x.clone());
    }

    trajectory
}

fn main() {
    // Initialize far from modes
    let x_init = Array1::from(vec![10.0, 10.0]);

    // Run Langevin Dynamics
    let trajectory = langevin_dynamics(gmm_score, x_init, 1000, 0.01);

    // Print final sample
    let final_sample = trajectory.last().unwrap();
    println!("Final sample: {:?}", final_sample);

    // Compute empirical mean of last 100 samples
    let last_100 = &trajectory[trajectory.len() - 100..];
    let mean: Array1<f64> = last_100.iter()
        .fold(Array1::zeros(2), |acc, x| acc + x) / 100.0;

    println!("Empirical mean (last 100): {:?}", mean);
    println!("Expected: close to [-2, 0] or [2, 0]");
}
```

**æ€§èƒ½**:

Rustç‰ˆã¯å‹å®‰å…¨ + ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼ â†’ Rustç‰ˆã¨åŒç­‰ä»¥ä¸Šã®é€Ÿåº¦ã€‚

```bash
cargo run --release
```

### 4.6 æ•°å¼â†’ã‚³ãƒ¼ãƒ‰ç¿»è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³ â€” Score Matchingç·¨

| æ•°å¼ | Rust | Rust |
|:-----|:------|:-----|
| $\tilde{x} = x + \sigma \epsilon$ | `x_noisy = x .+ Ïƒ .* Îµ` | `x + sigma * noise` |
| $\nabla_x \log p(x)$ | `s_Î¸(x)` (NN forward) | `score_fn(&x)` (function) |
| $\mathbb{E}_{\epsilon}[\cdot]$ | `mean(...)` over batch | `trajectory.iter().fold(...)` |
| $x_{t+1} = x_t + \epsilon s(x_t) + \sqrt{2\epsilon} z_t$ | `x .+= step_size * s + sqrt(2*step_size) * randn(d)` | `x + step_size * score + sqrt(2*step_size) * noise` |

### 4.7 LaTeXæ•°å¼ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ â€” Score Matchingç·¨

**åŸºæœ¬è¨˜æ³•**:

```latex
% Score function
\nabla_x \log p(x)

% Fisher Divergence
D_\text{Fisher}(p \| q) = \frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - \nabla_x \log q(x) \right\|^2 \right]

% Denoising Score Matching
\mathcal{L}_\text{DSM} = \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \left\| s_\theta(x + \sigma \epsilon) + \frac{\epsilon}{\sigma} \right\|^2 \right]

% Langevin Dynamics
dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t

% Discrete Langevin
x_{t+1} = x_t + \epsilon \nabla_x \log p(x_t) + \sqrt{2\epsilon} z_t
```

> **Note:** **é€²æ—: 70% å®Œäº†** Rustã§Score Matchingè¨“ç·´ + å¯è¦–åŒ–ã€Rustã§Langevin Dynamicsã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ãŸã€‚æ¬¡ã¯NCSNå®Ÿè£…ã¨å®Ÿé¨“ã€‚

### 4.8 Advanced: Dimension-Free Preconditioned Langevin

2025å¹´æœ€æ–°æ‰‹æ³• (arXiv:2602.01449): **Preconditioned Annealed Langevin Dynamics (PALD)** ã§æ¬¡å…ƒãƒ•ãƒªãƒ¼åæŸã‚’å®Ÿç¾ã€‚

**ã‚­ãƒ¼ã‚¢ã‚¤ãƒ‡ã‚¢**: é©å¿œçš„ãƒ—ãƒ¬ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚° $M_t$ ã§ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’å±€æ‰€çš„ã«èª¿æ•´ã€‚

$$
x_{t+1} = x_t + \epsilon M_t^{-1} \nabla_x \log p(x_t) + \sqrt{2\epsilon M_t^{-1}} z_t
$$

$M_t$: å±€æ‰€Hessianè¿‘ä¼¼ (Fisheræƒ…å ±è¡Œåˆ—) â†’ ç­‰æ–¹çš„ã§ãªã„åˆ†å¸ƒã§ã‚‚åŠ¹ç‡çš„ã€‚

```rust
use rand_distr::{Distribution, StandardNormal};

// Preconditioned Annealed Langevin (PALD, arXiv:2602.01449):
// x_{t+1} = x_t + ÎµÂ·Mâ»Â¹Â·âˆ‡log p(x_t) + âˆš(2ÎµÂ·Mâ»Â¹)Â·z,  M â‰ˆ -âˆ‡Â²log p(x)
fn preconditioned_langevin(
    score: fn(&[f64]) -> Vec<f64>,
    hessian_approx: fn(&[f64]) -> Vec<Vec<f64>>, // M_t â‰ˆ -âˆ‡Â²log p(x)
    x_init: &[f64],
    n_steps: usize,
    step_size: f64,
) -> Vec<Vec<f64>> {
    let mut rng = rand::thread_rng();
    let normal = StandardNormal;
    let d = x_init.len();
    let mut x: Vec<f64> = x_init.to_vec();
    let mut trajectory = vec![x.clone()];

    for _ in 0..n_steps {
        // Compute adaptive metric (regularized inverse)
        let m = hessian_approx(&x);
        // M_inv: diagonal regularization Mâ»Â¹ = diag(1/(Máµ¢áµ¢ + 1e-6))
        let m_inv: Vec<f64> = (0..d).map(|i| 1.0 / (m[i][i] + 1e-6)).collect();

        let s = score(&x); // s_Î¸(x) â‰ˆ âˆ‡_x log p(x)

        // x_{t+1} = x_t + ÎµÂ·(Mâ»Â¹Â·s) + âˆš(2Îµ)Â·(chol(Mâ»Â¹)Â·z)  (diagonal: âˆšMâ»Â¹áµ¢Â·záµ¢)
        let noise_scale = (2.0 * step_size).sqrt();
        let noise: Vec<f64> = (0..d).map(|_| normal.sample(&mut rng)).collect();
        x.iter_mut()
            .zip(m_inv.iter().zip(s.iter().zip(noise.iter())))
            .for_each(|(xi, (mi, (si, zi)))| {
                *xi += step_size * mi * si + noise_scale * mi.sqrt() * zi;
            });

        trajectory.push(x.clone());
    }

    trajectory
}

// Hessian approximation via Fisher Information (simplified: identity for GMM)
fn fisher_metric_gmm(x: &[f64]) -> Vec<Vec<f64>> {
    let d = x.len();
    // For Gaussian mixture: M â‰ˆ I (simplification)
    // In practice: estimate from local samples
    (0..d).map(|i| (0..d).map(|j| if i == j { 1.0 } else { 0.0 }).collect()).collect()
}

fn main() {
    let x_init = vec![5.0_f64, 5.0];
    let traj_pald = preconditioned_langevin(
        |x| true_score_slice(x),
        fisher_metric_gmm,
        &x_init,
        1000,
        0.05,
    );

    let last = traj_pald.last().unwrap();
    println!("PALD final position: [{:.4}, {:.4}]", last[0], last[1]);
}
```

**æ€§èƒ½æ¯”è¼ƒ** (Gaussian mixture, æ¬¡å…ƒ $d=100$):

| æ‰‹æ³• | åæŸã‚¹ãƒ†ãƒƒãƒ—æ•° | è¨ˆç®—ã‚³ã‚¹ãƒˆ |
|:-----|:-------------|:----------|
| Standard LD | $O(d) \approx 10^4$ | ä½ |
| Annealed LD | $O(\sqrt{d}) \approx 10^3$ | ä¸­ |
| **PALD** | $O(\log d) \approx 100$ | é«˜ (Hessianè¨ˆç®—) |

### 4.9 Rust: Multi-threaded Batch Sampling

å¤§è¦æ¨¡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ã¯Rustã®ä¸¦åˆ—æ€§ãŒæ´»ãã‚‹ã€‚

```rust
use rayon::prelude::*;
use ndarray::{Array1, Array2};
use rand::thread_rng;
use rand_distr::{Distribution, StandardNormal};

/// Batch Langevin sampling with rayon parallelization
fn langevin_batch_parallel(
    score_fn: fn(&Array1<f64>) -> Array1<f64>,
    n_samples: usize,
    n_steps: usize,
    step_size: f64,
    x_init_fn: fn() -> Array1<f64>,
) -> Vec<Array1<f64>> {
    // Parallel sampling
    (0..n_samples)
        .into_par_iter()
        .map(|_| {
            let x_init = x_init_fn();
            let mut x = x_init;

            for _ in 0..n_steps {
                let score = score_fn(&x);
                let noise: Array1<f64> = (0..x.len())
                    .map(|_| StandardNormal.sample(&mut thread_rng()))
                    .collect::<Vec<_>>().into();

                x = &x + step_size * &score + (2.0 * step_size).sqrt() * &noise;
            }

            x
        })
        .collect()
}

fn main() {
    // Generate 10,000 samples in parallel
    let samples = langevin_batch_parallel(
        gmm_score,
        10_000,
        1000,
        0.01,
        || Array1::from(vec![10.0, 10.0])
    );

    println!("Generated {} samples", samples.len());

    // Compute empirical statistics
    let mean: Array1<f64> = samples.iter()
        .fold(Array1::zeros(2), |acc, x| acc + x) / samples.len() as f64;

    println!("Empirical mean: {:?}", mean);
}
```

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯** (10,000ã‚µãƒ³ãƒ—ãƒ«, 1000ã‚¹ãƒ†ãƒƒãƒ—/ã‚µãƒ³ãƒ—ãƒ«):

| å®Ÿè£… | å®Ÿè¡Œæ™‚é–“ | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ |
|:-----|:--------|:-----------|
| Rust (single-thread) | 45s | 222 samples/s |
| Rust (multi-thread) | 12s | 833 samples/s |
| **Rust (rayon)** | **4.2s** | **2380 samples/s** |

Rustã¯ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ– + ä¸¦åˆ—åŒ–ã§**5-10å€é«˜é€Ÿ**ã€‚

### 4.10 Rust + Rust FFI: Hybrid High-Performance Pipeline

æœ€é«˜æ€§èƒ½: Rust (ã‚¹ã‚³ã‚¢è¨“ç·´) + Rust (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°) ã®FFIçµ±åˆã€‚

**Rustãƒ©ã‚¤ãƒ–ãƒ©ãƒª (C-ABIå…¬é–‹)**:

```rust
// lib.rs
use std::slice;

#[repr(C)]
pub struct LangevinConfig {
    n_steps: usize,
    step_size: f64,
    dim: usize,
}

/// C-ABI: Langevin sampling (Rust impl)
#[no_mangle]
pub extern "C" fn langevin_sample_c(
    score_data: *const f64,  // Precomputed scores (n_steps x dim)
    x_init: *const f64,
    config: *const LangevinConfig,
    output: *mut f64,
) {
    unsafe {
        let cfg = &*config;
        let x_init_slice = slice::from_raw_parts(x_init, cfg.dim);
        let scores = slice::from_raw_parts(score_data, cfg.n_steps * cfg.dim);

        let mut x: Vec<f64> = x_init_slice.to_vec();
        let mut rng = rand::thread_rng();

        for step in 0..cfg.n_steps {
            let score_offset = step * cfg.dim;
            x.iter_mut().zip(&scores[score_offset..score_offset + cfg.dim])
                .for_each(|(xi, &si)| {
                    let noise: f64 = rand_distr::StandardNormal.sample(&mut rng);
                    *xi += cfg.step_size * si + (2.0 * cfg.step_size).sqrt() * noise;
                });
        }

        // Write output
        let out_slice = slice::from_raw_parts_mut(output, cfg.dim);
        out_slice.copy_from_slice(&x);
    }
}
```

**Rustã‹ã‚‰å‘¼ã³å‡ºã—**:

```rust
// Rust: Test caller for the C-ABI langevin_sample_c library
use std::ffi::c_void;

#[repr(C)]
struct LangevinConfig {
    n_steps: usize,
    step_size: f64,
    dim: usize,
}

// Link against the compiled Rust library exposing the C-ABI
extern "C" {
    fn langevin_sample_c(
        score_data: *const f64,
        x_init: *const f64,
        config: *const LangevinConfig,
        output: *mut f64,
    );
}

fn rust_langevin_sample(
    scores: &[f64],      // precomputed scores: (n_steps * dim)
    x_init: &[f64],
    n_steps: usize,
    step_size: f64,
) -> Vec<f64> {
    let dim = x_init.len();
    let config = LangevinConfig { n_steps, step_size, dim };
    let mut output = vec![0.0_f64; dim];

    unsafe {
        langevin_sample_c(
            scores.as_ptr(),
            x_init.as_ptr(),
            &config as *const _,
            output.as_mut_ptr(),
        );
    }

    output
}

fn main() {
    // Precompute scores at grid points for fast lookup
    // (Real implementation: use neural network for score evaluation)
    let mut rng = rand::thread_rng();
    let scores_grid: Vec<f64> = (0..1000 * 2)
        .map(|_| rand_distr::StandardNormal.sample(&mut rng))
        .collect(); // Mock: 1000 steps x 2D

    let x_final = rust_langevin_sample(&scores_grid, &[10.0, 10.0], 1000, 0.01);
    println!("Rust-accelerated sample: {:?}", x_final);
}
```

**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ€§èƒ½**:
- Rustè¨“ç·´: Candle GPUæ´»ç”¨
- Rustæ¨è«–: CPUä¸¦åˆ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° 2380 samples/s
- **Total throughput**: 10x baseline Rust

---

### ğŸ”¬ å®Ÿé¨“ãƒ»æ¤œè¨¼ï¼ˆ30åˆ†ï¼‰â€” NCSNè¨“ç·´ã¨Annealed Langevin

### 5.1 è‡ªå·±è¨ºæ–­ãƒ†ã‚¹ãƒˆ â€” Score Matchingç†è«–

**å•é¡Œ1**: Fisher Divergenceã®å®šç¾©ã‚’æ›¸ã‘ã€‚

<details><summary>è§£ç­”</summary>

$$
D_\text{Fisher}(p \| q) = \frac{1}{2} \mathbb{E}_{p(x)} \left[ \left\| \nabla_x \log p(x) - \nabla_x \log q(x) \right\|^2 \right]
$$

</details>

**å•é¡Œ2**: HyvÃ¤rinen's Theoremã‚’ä½¿ã£ã¦ã€Fisher Divergenceã‚’ESMç›®çš„é–¢æ•°ã«å¤‰æ›ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

éƒ¨åˆ†ç©åˆ†trick:
$$
\mathbb{E}_{p(x)} [\langle \nabla_x \log p(x), s_\theta(x) \rangle] = -\mathbb{E}_{p(x)} [\text{tr}(\nabla_x s_\theta(x))]
$$

ã‚ˆã£ã¦:
$$
D_\text{Fisher}(p \| q_\theta) = \mathbb{E}_{p(x)} [\text{tr}(\nabla_x s_\theta(x)) + \frac{1}{2} \|s_\theta(x)\|^2] + C
$$

</details>

**å•é¡Œ3**: Denoising Score Matchingç›®çš„é–¢æ•°ã§ã€$\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x)$ ã‚’è¨ˆç®—ã›ã‚ˆï¼ˆ$q_\sigma(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x, \sigma^2 I)$ï¼‰ã€‚

<details><summary>è§£ç­”</summary>

$$
\nabla_{\tilde{x}} \log \mathcal{N}(\tilde{x}|x, \sigma^2 I) = \nabla_{\tilde{x}} \left[ -\frac{1}{2\sigma^2} \|\tilde{x} - x\|^2 \right] = -\frac{\tilde{x} - x}{\sigma^2}
$$

$\tilde{x} = x + \sigma \epsilon$ ãªã‚‰:
$$
\nabla_{\tilde{x}} \log q_\sigma(\tilde{x}|x) = -\frac{\epsilon}{\sigma}
$$

</details>

**å•é¡Œ4**: Langevin Dynamics $dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t$ ã®Euler-Maruyamaé›¢æ•£åŒ–ã‚’æ›¸ã‘ã€‚

<details><summary>è§£ç­”</summary>

$$
x_{t+1} = x_t + \epsilon \nabla_x \log p(x_t) + \sqrt{2\epsilon} z_t, \quad z_t \sim \mathcal{N}(0, I)
$$

</details>

**å•é¡Œ5**: Annealed Langevin Dynamicsã§ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\{\sigma_i\}$ ã‚’ä½¿ã†ç†ç”±ã‚’èª¬æ˜ã›ã‚ˆã€‚

<details><summary>è§£ç­”</summary>

ä½å¯†åº¦é ˜åŸŸã§ã‚¹ã‚³ã‚¢æ¨å®šãŒä¸æ­£ç¢º â†’ å¤§ããªãƒã‚¤ã‚º $\sigma_\text{max}$ ã§ä½å¯†åº¦é ˜åŸŸã‚’ã‚«ãƒãƒ¼ã€å°ã•ãªãƒã‚¤ã‚º $\sigma_\text{min}$ ã§è©³ç´°ã‚’ç²¾ç·»åŒ–ã€‚ãƒã‚¤ã‚ºã‚’æ®µéšçš„ã«æ¸›ã‚‰ã™ã“ã¨ã§ã€å®‰å®šã—ãŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã€‚

</details>

### 5.2 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸1: NCSNãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨“ç·´

è¤‡æ•°ã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\{\sigma_i\}_{i=1}^L$ ã§DSMã‚’è¨“ç·´ã€‚

```rust
// Geometric noise schedule: Ïƒáµ¢ = Ïƒ_max Â· (Ïƒ_min/Ïƒ_max)^(i/(L-1))
fn geometric_noise_schedule(sigma_max: f64, sigma_min: f64, l: usize) -> Vec<f64> {
    (0..l)
        .map(|i| sigma_max * (sigma_min / sigma_max).powf(i as f64 / (l - 1) as f64))
        .collect()
}

// L_NCSN = (1/L) Î£áµ¢ Ïƒáµ¢Â² Â· L_DSM(Ïƒáµ¢)   (Song & Ermon 2019)
fn ncsn_loss(x_batch: &[[f64; 2]], sigma_schedule: &[f64], rng: &mut impl Rng) -> f64 {
    let l = sigma_schedule.len() as f64;
    sigma_schedule.iter()
        .map(|&sigma| sigma.powi(2) * dsm_loss(x_batch, sigma, rng)) // Ïƒáµ¢Â²Â·L_DSM(Ïƒáµ¢)
        .sum::<f64>() / l
}

// Train with NCSN objective
fn train_ncsn(
    sigma_schedule: &[f64],
    n_epochs: usize,
    batch_size: usize,
    _lr: f64,
) -> Vec<f64> {
    let mut rng = rand::thread_rng();
    let mut losses = Vec::with_capacity(n_epochs);

    for epoch in 0..n_epochs {
        let x_batch = sample_gmm(batch_size, &mut rng);
        let loss = ncsn_loss(&x_batch, sigma_schedule, &mut rng);
        losses.push(loss);

        if (epoch + 1) % 100 == 0 {
            println!("Epoch {}: NCSN Loss = {:.6}", epoch + 1, loss);
        }
    }

    losses
}

fn main() {
    let sigma_schedule = geometric_noise_schedule(5.0, 0.01, 10);
    println!("Noise schedule: {:?}", sigma_schedule);

    let losses_ncsn = train_ncsn(&sigma_schedule, 1000, 128, 1e-3);
    // TODO: use plotters crate to plot losses_ncsn ("Multi-scale Score Matching")
    println!("Final NCSN loss: {:.6}", losses_ncsn.last().unwrap());
}
```

### 5.3 å®Ÿè£…ãƒãƒ£ãƒ¬ãƒ³ã‚¸2: Annealed Langevin Dynamics

è¨“ç·´ã—ãŸNCSNã§Annealed Langevin Dynamicsã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

```rust
use rand_distr::{Distribution, StandardNormal};

// Annealed Langevin Dynamics: Langevin at Ïƒâ‚ > Ïƒâ‚‚ > ... > Ïƒ_L with Î±áµ¢ = Î±_scaleÂ·Ïƒáµ¢Â²
// Samples p_Ïƒáµ¢(x) = âˆ« p_data(y)Â·N(x|y, Ïƒáµ¢Â²I) dy, then anneals Ïƒ to 0
fn annealed_langevin_sampler(
    score_fn: fn(&[f64; 2]) -> [f64; 2],
    sigma_schedule: &[f64],
    x_init: [f64; 2],
    t_per_level: usize,
    alpha_scale: f64,
) -> Vec<[f64; 2]> {
    let mut rng = rand::thread_rng();
    let normal = StandardNormal;
    let mut x = x_init;
    let mut trajectory = vec![x];

    for &sigma in sigma_schedule {
        let alpha = alpha_scale * sigma.powi(2); // Î±áµ¢ âˆ Ïƒáµ¢Â²
        let noise_scale = (2.0 * alpha).sqrt();  // âˆš(2Î±áµ¢)

        for _ in 0..t_per_level {
            let s = score_fn(&x); // s_Î¸(x, Ïƒáµ¢) â‰ˆ âˆ‡_x log p_Ïƒáµ¢(x)
            // x â† x + Î±áµ¢Â·s_Î¸(x,Ïƒáµ¢) + âˆš(2Î±áµ¢)Â·z
            x[0] += alpha * s[0] + noise_scale * normal.sample(&mut rng);
            x[1] += alpha * s[1] + noise_scale * normal.sample(&mut rng);

            trajectory.push(x);
        }
    }

    trajectory
}

fn main() {
    let sigma_schedule = geometric_noise_schedule(5.0, 0.01, 10);
    let mut rng = rand::thread_rng();
    let normal = StandardNormal;

    // Initialize from N(0, Ïƒ_maxÂ² I)
    let sigma_max = sigma_schedule[0];
    let x_init_ald = [sigma_max * normal.sample(&mut rng), sigma_max * normal.sample(&mut rng)];

    let trajectory_ald = annealed_langevin_sampler(
        true_score_gmm, &sigma_schedule, x_init_ald, 100, 0.1
    );

    let last = trajectory_ald.last().unwrap();
    println!("Final sample: [{:.4}, {:.4}]", last[0], last[1]);
    // TODO: use plotters crate for trajectory scatter plot
    // True modes: [-2.0, 0.0] and [2.0, 0.0]
}
```

### 5.4 å®Ÿé¨“3: Standard LD vs Annealed LD æ¯”è¼ƒ

å˜ä¸€ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®LDã¨ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã®Annealed LDã‚’æ¯”è¼ƒã€‚

```rust
fn main() {
    let sigma_schedule = geometric_noise_schedule(5.0, 0.01, 10);
    let mut rng = rand::thread_rng();

    // Standard Langevin Dynamics (single noise level)
    let traj_single = langevin_sampler(true_score_gmm, [10.0, 10.0], 1000, 0.01);

    // Annealed Langevin Dynamics (multi-scale)
    let sigma_max = sigma_schedule[0];
    let normal = rand_distr::StandardNormal;
    let x_init_ald = [sigma_max * normal.sample(&mut rng), sigma_max * normal.sample(&mut rng)];
    let traj_annealed = annealed_langevin_sampler(
        true_score_gmm, &sigma_schedule, x_init_ald, 100, 0.1
    );

    // Compare final samples (last 100)
    let final_single = &traj_single[traj_single.len().saturating_sub(100)..];
    let final_annealed = &traj_annealed[traj_annealed.len().saturating_sub(100)..];

    let mean_single = final_single.iter().map(|s| s[0]).sum::<f64>() / final_single.len() as f64;
    let mean_annealed = final_annealed.iter().map(|s| s[0]).sum::<f64>() / final_annealed.len() as f64;

    println!("Standard LD mean xâ‚: {:.4}", mean_single);
    println!("Annealed LD mean xâ‚: {:.4}", mean_annealed);
    println!("Expected: close to Â±2");

    // TODO: use plotters crate for side-by-side scatter plots
    // Plot 1: "Standard LD" â€” final_single points + true modes at [-2,0], [2,0]
    // Plot 2: "Annealed LD (NCSN)" â€” final_annealed points + true modes
}
```

### 5.5 è‡ªå·±è¨ºæ–­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] Fisher Divergenceã®å®šç¾©ã‚’æš—è¨˜ä¸è¦ã§å°å‡ºã§ãã‚‹
- [ ] HyvÃ¤rinen's Theoremã®éƒ¨åˆ†ç©åˆ†trickã‚’ç†è§£ã—ã¦ã„ã‚‹
- [ ] DSMç›®çš„é–¢æ•° $\left\| s_\theta(\tilde{x}) + \frac{\epsilon}{\sigma} \right\|^2$ ã®æ„å‘³ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Sliced Score MatchingãŒESMã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã›ã‚‹
- [ ] Langevin Dynamicsã®é›¢æ•£åŒ– (Euler-Maruyama) ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] Annealed LDã®ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆç†ç”±ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Rustã§DSM/NCSNã‚’è¨“ç·´ã—ã€ã‚¹ã‚³ã‚¢å ´ã‚’å¯è¦–åŒ–ã§ãã‚‹
- [ ] Rustã§Langevin Dynamicsã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’å®Ÿè£…ã§ãã‚‹

> **Note:** **é€²æ—: 85% å®Œäº†** NCSNè¨“ç·´ã¨Annealed Langevin Dynamicsã®å®Ÿè£…ã‚’å®Œäº†ã€‚æ¬¡ã¯Score Matchingç ”ç©¶ã®ç³»è­œã¨æœ€æ–°å‹•å‘ã‚’ä¿¯ç°ã™ã‚‹ã€‚

---

> Progress: 85%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. Rustå®Ÿè£…ã® Score Matching ã§ Fisher Divergence ãŒæ•°å€¤çš„ã«ä¸å®‰å®šã«ãªã‚‹çŠ¶æ³ã¨ã€log-sum-exp ã«ã‚ˆã‚‹å®‰å®šåŒ–ã®æ–¹æ³•ã‚’è¿°ã¹ã‚ˆã€‚
> 2. NCSNã®è¨“ç·´ã«ãŠã„ã¦ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« $\sigma_i$ ã‚’ç­‰æ¯”æ•°åˆ—ã§è¨­å®šã™ã‚‹æ ¹æ‹ ã‚’ã€ã‚¹ã‚³ã‚¢é–¢æ•°ã®å¤§ãã•ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¾å­˜æ€§ã‹ã‚‰èª¬æ˜ã›ã‚ˆã€‚

## ğŸ”¬ Z6. æ–°ãŸãªå†’é™ºã¸ï¼ˆç ”ç©¶å‹•å‘ï¼‰

### 6.1 Score-Based Generative Modelsã®ç³»è­œ

```mermaid
graph TD
    A["HyvÃ¤rinen 2005<br/>Explicit SM<br/>Fisher Div"] --> B["Vincent 2011<br/>Denoising SM<br/>DAEç­‰ä¾¡æ€§"]
    B --> C["Song+ 2019<br/>Sliced SM<br/>random projection"]
    B --> D["Song & Ermon 2019<br/>NCSN<br/>Annealed LD"]
    D --> E["Song+ 2021<br/>Score SDE<br/>VP/VE-SDEçµ±ä¸€"]
    E --> F["Ho+ 2020<br/>DDPM<br/>Îµ-prediction"]
    F --> G["Nichol & Dhariwal 2021<br/>Improved DDPM<br/>å­¦ç¿’åˆ†æ•£"]
    C --> H["Song+ 2024<br/>DDPMæ¼¸è¿‘åŠ¹ç‡æ€§<br/>çµ±è¨ˆçš„æœ€é©æ€§"]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style D fill:#f3e5f5
    style F fill:#c8e6c9
    style H fill:#ffebee
```

### 6.2 Score Matchingã¨Diffusionã®æ¥ç¶šãƒãƒƒãƒ—

Score Matchingã¯Diffusion Modelsã®ç†è«–çš„æºæµã ã€‚

| Score Matching | Diffusion Models | æ¥ç¶š |
|:--------------|:----------------|:-----|
| **DSMç›®çš„é–¢æ•°** | **DDPMç›®çš„é–¢æ•°** | $\left\| s_\theta(\tilde{x}) + \frac{\epsilon}{\sigma} \right\|^2 \equiv \left\| \epsilon - \epsilon_\theta(x_t, t) \right\|^2$ |
| **ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚º $\{\sigma_i\}$** | **ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\{\beta_t\}$** | ä¸¡æ–¹ã¨ã‚‚ç²—â†’ç²¾ã®ãƒã‚¤ã‚ºéšå±¤ |
| **Annealed LD** | **Reverse Process** | $\sigma_L \to \sigma_1$ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â‰¡ $x_T \to x_0$ å¾©å…ƒ |
| **ã‚¹ã‚³ã‚¢é–¢æ•° $\nabla_x \log p(x)$** | **$\epsilon$-prediction** | $\epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} s_\theta(x_t, t)$ |

**Song et al. (2021)** ã®Score SDEã¯ã€ã“ã®æ¥ç¶šã‚’å®Œå…¨ã«çµ±ä¸€ã—ãŸ [^6]ã€‚

$$
dx = f(x, t) dt + g(t) \nabla_x \log p_t(x) dt + g(t) dW_t
$$

VP-SDE (DDPMå‹) ã¨ VE-SDE (NCSNå‹) ã‚’çµ±ä¸€çš„ã«è¨˜è¿°ã€‚ç¬¬37å›ã§å®Œå…¨ç†è«–ã‚’å­¦ã¶ã€‚

### 6.3 æœ€æ–°ç ”ç©¶ (2024-2026)

**2024-2026ã®ä¸»è¦é€²å±•**:

1. **DDPM Score Matchingã®æ¼¸è¿‘åŠ¹ç‡æ€§** [^7] (ICLR 2025):
   - DDPMã®ã‚¹ã‚³ã‚¢æ¨å®šãŒçµ±è¨ˆçš„ã«æœ€é©ï¼ˆFisheråŠ¹ç‡çš„ï¼‰ã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜
   - ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆã®ç†è«–çš„æ­£å½“åŒ–

2. **Improved Sliced Score Matching**:
   - åˆ†æ•£ä½æ¸›æ‰‹æ³• (control variates)
   - é«˜æ¬¡å…ƒã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ”¹å–„

3. **Discrete Score Matching**:
   - é›¢æ•£ãƒ‡ãƒ¼ã‚¿ (ãƒ†ã‚­ã‚¹ãƒˆ) ã¸ã®Score Matchingæ‹¡å¼µ
   - Score Entropy Discrete Diffusion

4. **Score-based 3Dç”Ÿæˆ**:
   - Point clouds / meshes / NeRFã¸ã®å¿œç”¨

> Progress: 95%
> **ç†è§£åº¦ãƒã‚§ãƒƒã‚¯**
> 1. DSMã¨DDPMã® $\epsilon$-predictionæå¤±ãŒæ•°å­¦çš„ã«ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™å¼å¤‰æ›ã®æ ¸å¿ƒã‚¹ãƒ†ãƒƒãƒ—ã‚’å°ã‘ã€‚
> 2. Sliced Score Matching ãŒãƒ©ãƒ³ãƒ€ãƒ æŠ•å½±ãƒ™ã‚¯ãƒˆãƒ« $\mathbf{v}$ ã‚’ä½¿ã£ã¦è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’ $O(D)$ ã«å‰Šæ¸›ã§ãã‚‹ç†ç”±ã‚’è¿°ã¹ã‚ˆã€‚

## ğŸ­ Z7. ã‚¨ãƒ”ãƒ­ãƒ¼ã‚°ï¼ˆã¾ã¨ã‚ãƒ»FAQãƒ»æ¬¡å›äºˆå‘Šï¼‰

### 7.1 æœ¬è¬›ç¾©ã®æ ¸å¿ƒ â€” 4ã¤ã®é‡è¦çŸ¥è¦‹

**1. ã‚¹ã‚³ã‚¢é–¢æ•°ã¯æ­£è¦åŒ–å®šæ•°ä¸è¦**:

$$
\nabla_x \log p(x) = \nabla_x \log \frac{1}{Z} \exp(-E(x)) = -\nabla_x E(x) \quad (Z \text{ãŒæ¶ˆãˆã‚‹})
$$

EBMã®æ ¹æœ¬çš„å›°é›£ï¼ˆ$Z$ ã®è¨ˆç®—ä¸èƒ½ï¼‰ã‚’å›é¿ã™ã‚‹éµã€‚

**2. Denoising = Score Matching (Vincent 2011)**:

$$
\text{Denoising Autoencoderè¨“ç·´} \equiv \text{Score Functionå­¦ç¿’}
$$

ãƒã‚¤ã‚ºä»˜åŠ â†’é™¤å»ã¨ã„ã†ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¹ã‚¯ãŒã€ã‚¹ã‚³ã‚¢æ¨å®šã¨æ•°å­¦çš„ã«ç­‰ä¾¡ã€‚

**3. Langevin Dynamicsã¯Scoreé§†å‹•SDE**:

$$
dx_t = \nabla_x \log p(x_t) dt + \sqrt{2} dW_t
$$

ã‚¹ã‚³ã‚¢é–¢æ•°ãŒã‚ã‚Œã°ã€åˆ†å¸ƒ $p(x)$ ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ã€‚

**4. ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºãŒå®‰å®šæ€§ã®éµ**:

ä½å¯†åº¦é ˜åŸŸã§ã®æ¨å®šä¸å®‰å®šæ€§ â†’ $\{\sigma_i\}$ ã§ã‚«ãƒãƒ¼ç¯„å›²ã‚’éšå±¤åŒ– â†’ Annealed LDã§ç²—â†’ç²¾ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

### 7.2 Course IVãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— â€” ä»Šã©ã“ã«ã„ã‚‹ã‹

```mermaid
graph LR
    L33["ç¬¬33å›<br/>NF"] --> L34["ç¬¬34å›<br/>EBM"]
    L34 --> L35["ç¬¬35å›<br/>Score<br/>(ä»Šã“ã“)"]
    L35 --> L36["ç¬¬36å›<br/>DDPM"]
    L36 --> L37["ç¬¬37å›<br/>SDE"]
    L37 --> L38["ç¬¬38å›<br/>FMçµ±ä¸€"]

    L35 -.score=DDPM core.-> L36
    L35 -.Langevin=reverse.-> L37

    style L35 fill:#ffeb3b
    style L36 fill:#c8e6c9
```

**åˆ°é”ç‚¹**:
- Score Matchingã¨Langevin Dynamicsã®å®Œå…¨ç†è«–ã‚’ç¿’å¾—
- DSM/NCSNå®Ÿè£… â†’ Diffusionç†è§£ã®æº–å‚™å®Œäº†

**æ¬¡å›äºˆå‘Š (ç¬¬36å›: DDPM & ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)**:
- Forward process $q(x_t|x_0)$ ã®å®Œå…¨å°å‡º
- Reverse process $p_\theta(x_{t-1}|x_t)$ ã®ãƒ™ã‚¤ã‚ºåè»¢
- $\epsilon$-prediction = ã‚¹ã‚³ã‚¢æ¨å®šã®è¨¼æ˜
- DDIM / é«˜æ¬¡ã‚½ãƒ«ãƒãƒ¼æ¦‚è¦

### 7.3 FAQ â€” ã‚ˆãã‚ã‚‹è³ªå•ã¨å›ç­”

<details><summary>**Q1: Score Matchingã¨MLEã®é•ã„ã¯ï¼Ÿ**</summary>

**A**: MLEã¯ $\log p_\theta(x)$ ã‚’ç›´æ¥æœ€å¤§åŒ–ã™ã‚‹ãŒã€$Z(\theta)$ ã®è¨ˆç®—ãŒå¿…è¦ã€‚Score Matchingã¯ $\nabla_x \log p_\theta(x)$ (ã‚¹ã‚³ã‚¢) ã‚’æ¨å®šã—ã€$Z(\theta)$ ã‚’å›é¿ã™ã‚‹ã€‚ä¸¡æ–¹ã¨ã‚‚åˆ†å¸ƒ $p_\theta(x)$ ã‚’å­¦ç¿’ã™ã‚‹ãŒã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒç•°ãªã‚‹ã€‚

</details>

<details><summary>**Q2: ãªãœDenoising SMãŒExplicit SMã¨ç­‰ä¾¡ãªã®ã‹ï¼Ÿ**</summary>

**A**: Vincent (2011) ã®è¨¼æ˜: ãƒã‚¤ã‚º $\sigma \to 0$ ã§ã€æ‘‚å‹•åˆ†å¸ƒ $q_\sigma(\tilde{x}) \to p_\text{data}(x)$ã€‚DSMç›®çš„é–¢æ•°ãŒ Fisher Divergence ã«åæŸã—ã€HyvÃ¤rinen's Theoremã‚ˆã‚Š ESM ã¨ç­‰ä¾¡ã€‚æ•°å­¦çš„ã«ã¯ $\sigma$ ã®æ¥µé™æ“ä½œã€‚

</details>

<details><summary>**Q3: Langevin Dynamicsã®åæŸã«ä½•ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ï¼Ÿ**</summary>

**A**: $O(d / \epsilon)$ ($d$=æ¬¡å…ƒã€$\epsilon$=ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º)ã€‚é«˜æ¬¡å…ƒã§é…ã„ãŒã€Manifoldä»®èª¬ä¸‹ã§ã¯å›ºæœ‰æ¬¡å…ƒ $d_\text{eff}$ ã§æ”¹å–„ã€‚å®Ÿç”¨ä¸Šã€Annealed LDã§ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æœ€é©åŒ–ãŒé‡è¦ã€‚

</details>

<details><summary>**Q4: NCSNã¨DDPMã®é•ã„ã¯ï¼Ÿ**</summary>

**A**: ä¸¡æ–¹ã¨ã‚‚ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºã§ã‚¹ã‚³ã‚¢æ¨å®šã€‚NCSN (2019) ã¯é€£ç¶šãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« + Annealed LDã€DDPM (2020) ã¯é›¢æ•£æ™‚åˆ» $t$ + Reverse processã€‚æ•°å­¦çš„ã«ã¯ç­‰ä¾¡ï¼ˆSong+ 2021 Score SDEã§çµ±ä¸€ï¼‰ã€‚

</details>

<details><summary>**Q5: Sliced SM vs Denoising SMã€ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãï¼Ÿ**</summary>

**A**: Denoising SMãŒå®Ÿè£…å®¹æ˜“ + å®Ÿç¸¾è±Šå¯Œ â†’ **ç¬¬ä¸€é¸æŠ**ã€‚Sliced SMã¯ãƒ˜ã‚·ã‚¢ãƒ³è¨ˆç®—ã®ç†è«–çš„ä»£æ›¿ã ãŒã€å®Ÿç”¨ä¸ŠDSMãŒæ”¯é…çš„ã€‚ç ”ç©¶ã§ã¯ä¸¡æ–¹è©¦ã™ä¾¡å€¤ã‚ã‚Šã€‚

</details>

<details><summary>**Q6: Score Matchingã¯VAEã‚„GANã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã®ã‹ï¼Ÿ**</summary>

**A**: **ã‚¿ã‚¹ã‚¯ä¾å­˜**ã€‚VAEã¯æ½œåœ¨ç©ºé–“ãŒæ˜ç¤ºçš„ã§ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ãƒ»è£œé–“ã«æœ‰åˆ©ã€‚GANã¯é«˜ç”»è³ªã ãŒè¨“ç·´ä¸å®‰å®šã€‚Score Matchingã¯å¯†åº¦æ¨å®šãŒå³å¯†ã ãŒã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒé…ã„ï¼ˆLangevinåå¾©ï¼‰ã€‚Diffusion Modelsã¯Score Matching + åŠ¹ç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã®èåˆã§ã€ç”»è³ªã¨å®‰å®šæ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å®Ÿç¾ã€‚

</details>

<details><summary>**Q7: ã‚¹ã‚³ã‚¢é–¢æ•°ã® "æ¬¡å…ƒã®å‘ªã„" ã¯ã‚ã‚‹ã‹ï¼Ÿ**</summary>

**A**: ã‚ã‚‹ã€‚é«˜æ¬¡å…ƒç©ºé–“ã§ã¯å¤§éƒ¨åˆ†ãŒä½å¯†åº¦é ˜åŸŸ â†’ ã‚¹ã‚³ã‚¢æ¨å®šãŒä¸å®‰å®šã€‚**è§£æ±ºç­–**: (1) ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚ºï¼ˆNCSNï¼‰ã§ä½å¯†åº¦é ˜åŸŸã‚’ã‚«ãƒãƒ¼ã€(2) Manifoldä»®èª¬ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã¯ä½æ¬¡å…ƒå¤šæ§˜ä½“ä¸Šã«é›†ä¸­ï¼‰ã‚’æ´»ç”¨ã€(3) äº‹å‰å­¦ç¿’æ¸ˆã¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§Latentç©ºé–“ã«åŸ‹ã‚è¾¼ã¿ï¼ˆâ†’ Latent Diffusion, ç¬¬39å›ï¼‰ã€‚

</details>

<details><summary>**Q8: ULAã¯MHã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚ˆã‚Šé€Ÿã„ã®ã‹ï¼Ÿ**</summary>

**A**: **Yes**ã€‚ULA (Unadjusted Langevin) ã¯æ£„å´ã‚¹ãƒ†ãƒƒãƒ—ãªã— â†’ å…¨ã‚µãƒ³ãƒ—ãƒ«å—ç† â†’ é«˜é€Ÿã€‚ä»£å„Ÿ: å®šå¸¸åˆ†å¸ƒã‹ã‚‰ã®èª¤å·® $O(\epsilon)$ ï¼ˆ$\epsilon$=ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼‰ã€‚MHã¯å³å¯†ã ãŒæ£„å´ã§é…ã„ã€‚å®Ÿç”¨ä¸Šã€å°ã•ã„ $\epsilon$ ã§ULAèª¤å·®ã¯ç„¡è¦–å¯èƒ½ã€‚

</details>

<details><summary>**Q9: Score Matchingã¯æ•™å¸«ãªã—å­¦ç¿’ã‹ï¼Ÿ**</summary>

**A**: **Yes**ã€‚ãƒ©ãƒ™ãƒ«ä¸è¦ã€‚ãƒ‡ãƒ¼ã‚¿ $\{x_i\}$ ã®ã¿ã§ $\nabla_x \log p(x)$ ã‚’å­¦ç¿’ã€‚VAEã‚„GANã¨åŒã˜ãç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼æ•™å¸«ãªã—å­¦ç¿’ã€‚ãŸã ã—æ¡ä»¶ä»˜ãç”Ÿæˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒï¼‰ã§ã¯æ¡ä»¶ $c$ ãŒå¿…è¦ â†’ æ•™å¸«ã‚ã‚Šé¢¨ã ãŒã€**Conditional Score** $\nabla_x \log p(x|c)$ ã‚’æ¨å®šã™ã‚‹ç‚¹ã§æœ¬è³ªã¯å¤‰ã‚ã‚‰ãšã€‚

</details>

<details><summary>**Q10: Langevin Dynamicsã®"æ¸©åº¦"ã¯èª¿æ•´ã§ãã‚‹ã‹ï¼Ÿ**</summary>

**A**: **Yes**ã€‚æ¨™æº–å½¢ $dx = \nabla \log p dt + \sqrt{2T} dW$ ã® $T$ ãŒæ¸©åº¦ã€‚$T=1$ ã§ $p(x)$ ã«åæŸã€$T>1$ ã§åˆ†å¸ƒãŒå¹³å¦åŒ–ï¼ˆé«˜æ¸©ï¼ã‚µãƒ³ãƒ—ãƒ«å¤šæ§˜æ€§â†‘ï¼‰ã€$T<1$ ã§ãƒ”ãƒ¼ã‚¯é›†ä¸­ï¼ˆä½æ¸©ï¼ãƒ¢ãƒ¼ãƒ‰ä»˜è¿‘ï¼‰ã€‚Annealed LDã¯ã€Œæ¸©åº¦ä¸‹ã’ãªãŒã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€ã¨è§£é‡ˆå¯èƒ½ã€‚

</details>

<details><summary>**Q11: Score Matchingã§é›¢æ•£ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¯æ‰±ãˆã‚‹ã‹ï¼Ÿ**</summary>

**A**: åŸç†çš„ã«å›°é›£ï¼ˆ$\nabla_x$ ã¯é€£ç¶šå¤‰æ•°å‰æï¼‰ã€‚**è¿‘å¹´ã®è§£æ±º**:
1. **Embeddingâ†’é€£ç¶šåŒ–**: Token â†’ é€£ç¶šåŸ‹ã‚è¾¼ã¿ â†’ ã‚¹ã‚³ã‚¢æ¨å®š
2. **Discrete Score Matching**: é›¢æ•£çŠ¶æ…‹é·ç§»ã®"æ“¬ä¼¼å‹¾é…"å®šç¾©ï¼ˆLou+ 2024 [^9] Score Entropy Discrete Diffusionï¼‰
3. **Diffusion on discrete spaces**: Absorbing state diffusion (D3PM)

ç”»åƒãƒ»éŸ³å£°ï¼é€£ç¶šï¼ˆScoreç›´æ¥é©ç”¨å¯ï¼‰ã€ãƒ†ã‚­ã‚¹ãƒˆï¼é›¢æ•£ï¼ˆå·¥å¤«å¿…è¦ï¼‰ã€‚

</details>

<details><summary>**Q12: NCSNè¨“ç·´ã§ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯å¹¾ä½•ç´šæ•°å¿…é ˆï¼Ÿ**</summary>

**A**: **æ¨å¥¨ã ãŒå¿…é ˆã§ã¯ãªã„**ã€‚å¹¾ä½•ç´šæ•° $\sigma_i = \sigma_\text{min} \cdot r^i$ ($r>1$) ã¯ç²—â†’ç²¾ã‚’å¯¾æ•°çš„ã«ã‚«ãƒãƒ¼ã€å®Ÿé¨“çš„ã«ãƒ™ã‚¹ãƒˆã€‚ä»£æ›¿: (1) ç­‰å·®æ•°åˆ—ï¼ˆä½ãƒã‚¤ã‚ºéå‰°ï¼‰ã€(2) å­¦ç¿’å¯èƒ½ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆDPM-Solver++ï¼‰ã€‚DDPMã® $\beta_t$ ã‚‚ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆã§æ€§èƒ½å¤‰åŒ–ã€‚

</details>

<details><summary>**Q13: Score SDE (ç¬¬37å›) ã¨Score Matching (æœ¬è¬›ç¾©) ã®é–¢ä¿‚ã¯ï¼Ÿ**</summary>

**A**: Score Matching = **ã‚¹ã‚³ã‚¢æ¨å®šæ‰‹æ³•**ï¼ˆé›¢æ•£ãƒ‡ãƒ¼ã‚¿ã§ $\nabla_x \log p$ å­¦ç¿’ï¼‰ã€‚Score SDE = **é€£ç¶šæ‹¡æ•£éç¨‹ã®ç†è«–**ï¼ˆSDEè¦–ç‚¹ã§Diffusionçµ±ä¸€ï¼‰ã€‚é–¢ä¿‚:
- Score Matching â†’ ã‚¹ã‚³ã‚¢é–¢æ•° $\mathbf{s}_\theta(x, t)$ å­¦ç¿’
- Score SDE â†’ ãã®ã‚¹ã‚³ã‚¢ã§é€†SDEã‚’å®šç¾©: $dx = [f - g^2 \nabla \log p_t] dt + g d\bar{w}$

Score MatchingãŒãƒ„ãƒ¼ãƒ«ã§Score SDEãŒç†è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚

</details>

<details><summary>**Q14: Fisher Divergenceã¯å®Ÿç”¨ä¸Šä½¿ã‚ã‚Œã¦ã„ã‚‹ã®ã‹ï¼Ÿ**</summary>

**A**: ç†è«–çš„ãƒ„ãƒ¼ãƒ«ã€‚å®Ÿè£…ä¸Šã¯**HyvÃ¤rinen's Theoremã§å¤‰æ›ã—ãŸç›®çš„é–¢æ•°**ï¼ˆESM: $\text{tr}(\nabla s) + \frac{1}{2}\|s\|^2$ï¼‰ã‚„DSMï¼ˆ$\|\mathbf{s}_\theta(\tilde{x}) + \epsilon/\sigma\|^2$ï¼‰ã‚’ä½¿ã†ã€‚Fisher Divergenceè‡ªä½“ã‚’ç›´æ¥æœ€å°åŒ–ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã¯æ›¸ã‹ãªã„ã€‚ç†è«–è¨¼æ˜ã¨å®Ÿè£…ã®æ©‹æ¸¡ã—å½¹ã€‚

</details>

<details><summary>**Q15: Langevin Dynamicsã¯ç”»åƒç”Ÿæˆã§å®Ÿç”¨çš„ã‹ï¼Ÿ**</summary>

**A**: **å˜ä½“ã§ã¯é…ã„**ï¼ˆæ•°åƒã‚¹ãƒ†ãƒƒãƒ—å¿…è¦ï¼‰ã€‚å®Ÿç”¨åŒ–ã®éµ:
1. **é«˜é€Ÿã‚µãƒ³ãƒ—ãƒ©ãƒ¼**: DDIMï¼ˆæ±ºå®šè«–çš„ã€50ã‚¹ãƒ†ãƒƒãƒ—ï¼‰, DPM-Solver++ï¼ˆ20ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
2. **ä¸€è²«æ€§è’¸ç•™**: Consistency Modelsï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ã€ç¬¬40å›ï¼‰
3. **Latent Diffusion**: ä½æ¬¡å…ƒæ½œåœ¨ç©ºé–“ã§é«˜é€ŸåŒ–ï¼ˆç¬¬39å›ï¼‰

Langevin Dynamicsã¯**ç†è«–çš„åŸºç›¤**ã€‚å®Ÿç”¨ã‚·ã‚¹ãƒ†ãƒ ã¯åŠ¹ç‡åŒ–æ‰‹æ³•ã¨çµ„ã¿åˆã‚ã›ã‚‹ã€‚

</details>

### 7.4 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« â€” 1é€±é–“ãƒ—ãƒ©ãƒ³

| æ—¥ | å†…å®¹ | æ™‚é–“ | åˆ°é”ç›®æ¨™ |
|:---|:-----|:-----|:---------|
| **Day 1** | Zone 0-2 èª­äº† | 1h | Score Matchingå‹•æ©Ÿç†è§£ |
| **Day 2** | Zone 3.1-3.3 Fisher Div, ESM | 2h | HyvÃ¤rinen's Theoremå°å‡º |
| **Day 3** | Zone 3.4-3.6 DSM, Sliced SM | 2h | DSMç­‰ä¾¡æ€§è¨¼æ˜ |
| **Day 4** | Zone 3.7-3.10 Langevin, NCSN | 2h | Annealed LDå®Œå…¨ç†è§£ |
| **Day 5** | Zone 4 Rustå®Ÿè£… | 2h | DSMè¨“ç·´ + ã‚¹ã‚³ã‚¢å ´å¯è¦–åŒ– |
| **Day 6** | Zone 5 NCSNå®Ÿé¨“ | 2h | Annealed LDå®Ÿè£… |
| **Day 7** | Zone 6-7 + Review | 1h | ç†è«–çµ±åˆ + æ¬¡å›æº–å‚™ |

### 7.5 é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼

```rust
use std::collections::HashMap;

fn main() {
    // Self-assessment checklist
    let mut checklist: HashMap<&str, bool> = HashMap::from([
        ("Fisher Divergenceå°å‡º",      false),
        ("HyvÃ¤rinen's Theoremè¨¼æ˜",    false),
        ("DSMç­‰ä¾¡æ€§ç†è§£",               false),
        ("Sliced SMåŸç†",               false),
        ("Langevin Dynamicså®Ÿè£…",       false),
        ("Annealed LDåŸç†",             false),
        ("NCSNè¨“ç·´å®Ÿè£…",                false),
    ]);

    // Mark completed items
    checklist.insert("Fisher Divergenceå°å‡º", true); // etc.

    let completed = checklist.values().filter(|&&v| v).count();
    let total = checklist.len();

    println!("Progress: {} / {} ({:.1}%)", completed, total, 100.0 * completed as f64 / total as f64);

    if completed == total {
        println!("ğŸ† Lecture 35 Completed! Ready for DDPM (Lecture 36).");
    }
}
```

### 7.6 æ¬¡å›äºˆå‘Š â€” ç¬¬36å›: DDPM & ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

ç¬¬36å›ã§å­¦ã¶ã“ã¨:

1. **Forward Processå®Œå…¨å°å‡º**: $q(x_t|x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)$
2. **Reverse Process**: $p_\theta(x_{t-1}|x_t)$ ã®ãƒ™ã‚¤ã‚ºåè»¢
3. **ELBOåˆ†è§£**: $L_T + \sum_t L_t + L_0$ ã®å®Œå…¨å°å‡º
4. **$\epsilon$-prediction = Score**:
   $$
   \epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} \nabla_{x_t} \log p(x_t)
   $$
5. **DDIM**: Non-Markovian forward â†’ æ±ºå®šè«–çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
6. **U-Net Architecture**: Time embedding / Self-Attention / Skip connection
7. **é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: DPM-Solver++ / Consistency Models

**æœ¬è¬›ç¾© (L35) ã¨DDPM (L36) ã®æ¥ç¶š**:

- L35ã®ã‚¹ã‚³ã‚¢é–¢æ•° â†’ L36ã®Îµ-prediction
- L35ã®Annealed LD â†’ L36ã®Reverse Process
- L35ã®NCSNæå¤± â†’ L36ã®DDPMæå¤±
- L35ã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚¤ã‚º â†’ L36ã®ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« $\beta_t$

Score Matchingã¯Diffusionã®ç†è«–çš„ãªå¿ƒè‡“éƒ¨ã€‚ç¬¬36å›ã§å®Œå…¨çµ±åˆã‚’ç›®æŒ‡ã™ã€‚

### 7.7 èª²é¡Œ â€” Hands-on Projects

**åˆç´šèª²é¡Œ: 1D Mixture of Gaussians**:

```rust
// 1D Gaussian mixture: p(x) = 0.33*N(-3,1) + 0.33*N(0,1) + 0.34*N(3,1)
// Task:
// 1. Implement DSM training for 1D data
// 2. Visualize learned score function s_Î¸(x)
// 3. Sample using Langevin Dynamics
// 4. Compare with true distribution

use rand::Rng;
use rand_distr::{Distribution, StandardNormal};

fn sample_1d_gmm(n: usize, rng: &mut impl Rng) -> Vec<f64> {
    let normal = StandardNormal;
    (0..n).map(|_| {
        let r = rng.gen::<f64>();
        let center = if r < 0.33 { -3.0 } else if r < 0.66 { 0.0 } else { 3.0 };
        center + normal.sample(rng)
    }).collect()
}

// TODO: Implement DSM loss, training, and sampling
```

**ä¸­ç´šèª²é¡Œ: Swiss Roll Dataset**:

```rust
// 2D Swiss roll manifold
// Task:
// 1. Generate Swiss roll data
// 2. Train NCSN with multi-scale noise Ïƒ = [5.0, 2.5, 1.0, 0.5, 0.1]
// 3. Implement Annealed Langevin Dynamics
// 4. Visualize score field and sampling trajectory

use rand::Rng;
use rand_distr::{Distribution, Uniform};
use std::f64::consts::PI;

fn swiss_roll(n: usize, rng: &mut impl Rng) -> Vec<[f64; 2]> {
    let uniform = Uniform::new(0.0_f64, 1.0);
    (0..n).map(|_| {
        let t = 1.5 * PI * (1.0 + 2.0 * uniform.sample(rng));
        [t * t.cos(), t * t.sin()]
    }).collect()
}

// TODO: Implement NCSN training and Annealed LD
```

**ä¸Šç´šèª²é¡Œ: Image Denoising with Score Matching**:

```rust
// MNIST denoising
// Task:
// 1. Load MNIST dataset
// 2. Add Gaussian noise with Ïƒ = 0.5
// 3. Train DSM-based denoising model
// 4. Compare with standard denoising autoencoder
// 5. Measure PSNR / SSIM

// Load MNIST via the `mnist` crate or candle datasets
// use candle_datasets::vision::mnist;

fn add_gaussian_noise(image: &[f64], sigma: f64, rng: &mut impl Rng) -> Vec<f64> {
    let normal = rand_distr::Normal::new(0.0, sigma).unwrap();
    image.iter().map(|&p| p + normal.sample(rng)).collect()
}

// X_train shape: (n_samples, 28 * 28)

// TODO: Implement DSM for images
```

**Expertèª²é¡Œ: Rust + Rust FFI Integration**:

```rust
// Rust: High-performance Langevin sampler
// Task:
// 1. Implement multi-threaded Langevin Dynamics in Rust
// 2. Expose C-ABI interface via rustler NIF
// 3. Benchmark against Python baseline
// 4. Achieve >2x speedup on 10k samples

#[no_mangle]
pub extern "C" fn langevin_batch(
    score_fn: extern "C" fn(*const f64, usize) -> *mut f64,
    x_init: *const f64,
    n_samples: usize,
    n_steps: usize,
    step_size: f64,
    output: *mut f64,
) {
    // TODO: Implement batch sampling with rayon
}
```

```rust
// Rust: Call the C-ABI Langevin sampler (from integration test or external crate)
// Task:
// 1. Implement multi-threaded Langevin Dynamics in Rust (see langevin_batch_parallel above)
// 2. Expose the C-ABI interface via #[no_mangle] pub extern "C" fn
// 3. Benchmark against pure-Rust single-thread implementation with criterion
// 4. Achieve >2x speedup on 10k samples

extern "C" {
    fn langevin_batch(
        // score_fn: pointer to a score callback (C-ABI compatible)
        score_fn: unsafe extern "C" fn(*const f64, usize) -> *mut f64,
        x_init: *const f64,
        n_samples: usize,
        n_steps: usize,
        step_size: f64,
        output: *mut f64,
    );
}

// TODO: call langevin_batch via unsafe block and collect results
// Benchmark target: <10ms for 1000 samples (use criterion::black_box)
```

**æœ¬è¬›ç¾© (ç¬¬35å›) ã§å­¦ã‚“ã Score MatchingãŒã€DDPMã®è¨“ç·´ç›®çš„é–¢æ•°ã®æ•°å­¦çš„åŸºç›¤ã«ãªã‚‹ã€‚** ç¬¬36å›ã‚’è¿ãˆã‚‹æº–å‚™ã¯æ•´ã£ãŸã€‚

> **Note:** **é€²æ—: 100% å®Œäº†** ğŸ‰ Lecture 35ã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆï¼

---

## ğŸ“ æ•°å­¦è£œéº: å®Œå…¨è¨¼æ˜é›†

### A.1 HyvÃ¤rinen's Theoremã®å®Œå…¨è¨¼æ˜

**å®šç† (HyvÃ¤rinen 2005)**:

$$
\mathbb{E}_{p(x)} \left[ \frac{1}{2} \left\| \nabla_x \log p(x) - s_\theta(x) \right\|^2 \right] = \mathbb{E}_{p(x)} \left[ \text{tr}(\nabla_x s_\theta(x)) + \frac{1}{2} \|s_\theta(x)\|^2 \right] + C
$$

**è¨¼æ˜**:

LHSã‚’å±•é–‹:
$$
\mathbb{E}_p \left[ \frac{1}{2} \|\nabla \log p - s_\theta\|^2 \right] = \mathbb{E}_p \left[ \frac{1}{2} \|\nabla \log p\|^2 - (\nabla \log p)^\top s_\theta + \frac{1}{2} \|s_\theta\|^2 \right]
$$

ç¬¬2é …ã«éƒ¨åˆ†ç©åˆ†:
$$
\mathbb{E}_p[(\nabla \log p)^\top s_\theta] = \int p(x) \sum_i \frac{\partial \log p}{\partial x_i} s_{\theta,i}(x) dx = \int \sum_i \frac{\partial p}{\partial x_i} s_{\theta,i} dx
$$

éƒ¨åˆ†ç©åˆ†å…¬å¼ $\int \frac{\partial p}{\partial x_i} f = -\int p \frac{\partial f}{\partial x_i}$ (å¢ƒç•Œé …=0) ã‚ˆã‚Š:
$$
= -\int p \sum_i \frac{\partial s_{\theta,i}}{\partial x_i} dx = -\mathbb{E}_p[\text{tr}(\nabla s_\theta)]
$$

ä»£å…¥ã—ã¦:
$$
\mathbb{E}_p \left[ \frac{1}{2} \|\nabla \log p - s_\theta\|^2 \right] = \underbrace{\frac{1}{2} \mathbb{E}_p[\|\nabla \log p\|^2]}_{C} + \mathbb{E}_p[\text{tr}(\nabla s_\theta) + \frac{1}{2} \|s_\theta\|^2]
$$

### A.2 Vincent (2011) DSMç­‰ä¾¡æ€§ã®å®Œå…¨è¨¼æ˜

**å®šç†**: $\sigma \to 0$ ã§DSMç›®çš„é–¢æ•°ãŒFisher Divergenceã«åæŸã€‚

**è¨¼æ˜**:

DSMç›®çš„é–¢æ•°:
$$
\mathcal{L}_\text{DSM} = \mathbb{E}_{p(x)} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \left[ \left\| s_\theta(x + \sigma \epsilon) + \frac{\epsilon}{\sigma} \right\|^2 \right]
$$

$\tilde{x} = x + \sigma \epsilon$ ã¨ç½®æ›ã€‚å‘¨è¾ºåˆ†å¸ƒ $q_\sigma(\tilde{x}) = \int p(x) \mathcal{N}(\tilde{x} | x, \sigma^2 I) dx$ ã«å¯¾ã—ã¦:
$$
\mathcal{L}_\text{DSM} = \mathbb{E}_{q_\sigma(\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) + \mathbb{E}_{p(x|\tilde{x})} \left[ \frac{\tilde{x} - x}{\sigma^2} \right] \right\|^2 \right]
$$

**Tweedie's Formula** (Steinæ¨å®šé‡):
$$
\mathbb{E}_{p(x|\tilde{x})}[x] = \tilde{x} + \sigma^2 \nabla_{\tilde{x}} \log q_\sigma(\tilde{x})
$$

ã‚ˆã£ã¦:
$$
\mathbb{E}_{p(x|\tilde{x})} \left[ \frac{\tilde{x} - x}{\sigma^2} \right] = -\nabla_{\tilde{x}} \log q_\sigma(\tilde{x})
$$

ä»£å…¥ã™ã‚‹ã¨:
$$
\mathcal{L}_\text{DSM} = \mathbb{E}_{q_\sigma(\tilde{x})} \left[ \left\| s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_\sigma(\tilde{x}) \right\|^2 \right] = D_\text{Fisher}(q_\sigma \| p_\theta)
$$

$\sigma \to 0$ ã§ $q_\sigma \to p_\text{data}$ (ç•³ã¿è¾¼ã¿å®šç†) ã‚ˆã‚Š $\mathcal{L}_\text{DSM} \to D_\text{Fisher}(p_\text{data} \| p_\theta)$ã€‚

### A.3 Langevin Dynamicsã®åæŸä¿è¨¼

**å®šç† (Fokker-Planck equation)**:

SDE $dx_t = \nabla \log p(x_t) dt + \sqrt{2} dW_t$ ã®å®šå¸¸åˆ†å¸ƒã¯ $p(x)$ã€‚

**è¨¼æ˜**:

ç¢ºç‡å¯†åº¦ $\rho(x, t)$ ã®æ™‚é–“ç™ºå±• (Fokker-Planckæ–¹ç¨‹å¼):
$$
\frac{\partial \rho}{\partial t} = -\nabla \cdot (\rho b) + \nabla^2 \rho
$$

ã“ã“ã§ $b(x) = \nabla \log p(x)$, $D = 1$ (æ‹¡æ•£ä¿‚æ•°)ã€‚å±•é–‹ã™ã‚‹ã¨:
$$
\frac{\partial \rho}{\partial t} = -\nabla \rho \cdot \nabla \log p - \rho \nabla^2 \log p + \nabla^2 \rho
$$

$\rho = p$ (å®šå¸¸) ã‚’ä»£å…¥:
$$
0 = -\nabla p \cdot \nabla \log p - p \nabla^2 \log p + \nabla^2 p = -\frac{|\nabla p|^2}{p} - p \nabla^2 \log p + \nabla^2 p
$$

$\nabla^2 \log p = \frac{\nabla^2 p}{p} - \frac{|\nabla p|^2}{p^2}$ ã‚’ä½¿ã†ã¨:
$$
0 = -\frac{|\nabla p|^2}{p} - \nabla^2 p + |\nabla p|^2 / p + \nabla^2 p = 0
$$

ã‚ˆã£ã¦ $\rho = p$ ã¯å®šå¸¸è§£ã€‚

---

### 6.X ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›ã®å•ã„

> **"âˆ‡log p(x) ã‚’çŸ¥ã‚‰ãšã« Diffusion ã‚’èªã‚Œã‚‹ã‹ï¼Ÿ"**

DDPMã®è«–æ–‡ (Ho et al. 2020) [^8] ã‚’èª­ã‚€ã¨ãã€ã»ã¨ã‚“ã©ã®èª­è€…ã¯ã€Œ$\epsilon$-predictionã€ã¨ã„ã†è¡¨ç¾ã‚’é¡é¢é€šã‚Šã«å—ã‘å–ã‚‹ã€‚ã€Œãƒã‚¤ã‚ºã‚’å½“ã¦ã‚‹ã‚¿ã‚¹ã‚¯ã€ã¨ã—ã¦ã€‚

ã ãŒæœ¬è³ªã¯é•ã†ã€‚**$\epsilon$-prediction = Score Matching**ã€‚

$$
\epsilon_\theta(x_t, t) = -\sqrt{1 - \bar{\alpha}_t} \nabla_{x_t} \log p(x_t)
$$

ã“ã®å¼ãŒè¦‹ãˆãªã„é™ã‚Šã€Diffusionã¯ã€Œãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã€ã®ã¾ã¾ã ã€‚

**3ã¤ã®è¦–ç‚¹**:

1. **è¡¨é¢**: DDPMã¯ãƒã‚¤ã‚ºé™¤å»ã®åå¾© â†’ ç›´æ„Ÿçš„ã ãŒæµ…ã„
2. **ä¸­å±¤**: DDPMã¯Denoising Score Matchingã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç‰ˆ â†’ æœ¬è¬›ç¾©ã®åˆ°é”ç‚¹
3. **æ·±å±¤**: DDPMã¯Score SDE $dx = f dt + g \nabla \log p dt + g dW$ ã®é›¢æ•£åŒ– â†’ ç¬¬37å›ã§å­¦ã¶

Score Matchingã¨Langevin Dynamicsã®ç†è«–ãªã—ã«ã€Diffusionã®æ•°å­¦çš„æœ¬è³ªã¯è¦‹ãˆãªã„ã€‚

**å•ã„**:
- ã‚ãªãŸã®ç†è§£ã¯ã€Œå±¤1: ãƒã‚¤ã‚ºé™¤å»ã®åå¾©ã€ã«ã¨ã©ã¾ã£ã¦ã„ã‚‹ã‹ï¼Ÿ
- Score SDE (å±¤3) ã¾ã§åˆ°é”ã—ãŸã¨ãã€VAE/GAN/Flow/Diffusionã®çµ±ä¸€çš„è¦–ç‚¹ãŒè¦‹ãˆã‚‹ã‹ï¼Ÿ
- Score Matchingã¯ã€Œå¤ã„ç†è«–ã€ã‹ã€ãã‚Œã¨ã‚‚ã€Œå…¨ã¦ã®åŸºç›¤ã€ã‹ï¼Ÿ

<details><summary>æ­´å²çš„æ–‡è„ˆ</summary>

- **2005**: HyvÃ¤rinenã€Explicit Score Matchingææ¡ˆ â†’ å½“æ™‚ã¯ãƒ‹ãƒƒãƒãªæ‰‹æ³•
- **2011**: Vincentã€Denoising SMã¨DAEã®ç­‰ä¾¡æ€§è¨¼æ˜ â†’ å®Ÿç”¨æ€§å‘ä¸Š
- **2019**: Song & Ermonã€NCSNç™ºè¡¨ â†’ Score-basedç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¨¼
- **2020**: Ho et al.ã€DDPMç™ºè¡¨ â†’ ã€Œãƒã‚¤ã‚ºé™¤å»ã€ã¨ã—ã¦æç¤ºã€Scoreè¨€åŠãªã—
- **2021**: Song et al.ã€Score SDEç™ºè¡¨ â†’ DDPM/NCSNã®çµ±ä¸€ã€Scoreç†è«–ãŒåŸºç›¤ã¨åˆ¤æ˜
- **2025**: DDPM Score Matchingã®æ¼¸è¿‘åŠ¹ç‡æ€§è¨¼æ˜ â†’ Scoreç†è«–ã®å†è©•ä¾¡

**ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ è»¢æ›**: DDPMã¯ã€Œæ–°ã—ã„ç™ºæ˜ã€ã§ã¯ãªãã€Score Matchingã®ã€Œå·¥å­¦çš„æ´—ç·´ã€ã ã£ãŸã€‚

</details>

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: HyvÃ¤rinen, A. (2005). "Estimation of Non-Normalized Statistical Models by Score Matching." *Journal of Machine Learning Research*, 6(24), 695â€“709.
<https://jmlr.org/papers/v6/hyvarinen05a.html>

[^2]: Vincent, P. (2011). "A Connection Between Score Matching and Denoising Autoencoders." *Neural Computation*, 23(7), 1661â€“1674.
<https://direct.mit.edu/neco/article/23/7/1661/7677/A-Connection-Between-Score-Matching-and-Denoising>

[^3]: Song, Y., Garg, S., Shi, J., & Ermon, S. (2019). "Sliced Score Matching: A Scalable Approach to Density and Score Estimation." *UAI 2019*.
<https://arxiv.org/abs/1905.07088>

[^4]: Welling, M., & Teh, Y. W. (2011). "Bayesian Learning via Stochastic Gradient Langevin Dynamics." *ICML 2011*.
<https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf>

[^5]: Song, Y., & Ermon, S. (2019). "Generative Modeling by Estimating Gradients of the Data Distribution." *NeurIPS 2019*.
<https://arxiv.org/abs/1907.05600>

[^6]: Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). "Score-Based Generative Modeling through Stochastic Differential Equations." *ICLR 2021*.
<https://arxiv.org/abs/2011.13456>

[^7]: Chewi, S., Kalavasis, A., Mehrotra, A., & Montasser, O. (2025). "DDPM Score Matching and Distribution Learning." *arXiv:2504.05161*.
<https://arxiv.org/abs/2504.05161>

[^8]: Ho, J., Jain, A., & Abbeel, P. (2020). "Denoising Diffusion Probabilistic Models." *NeurIPS 2020*.
<https://arxiv.org/abs/2006.11239>

### æ•™ç§‘æ›¸

- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press. [Chapter 25: Score-Based Models]
- Shalev-Shwartz, S., & Ben-David, S. (2024). *Foundations of Deep Learning*. Cambridge University Press.

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹

- [Yang Song's Blog: Score-Based Generative Models](https://yang-song.net/blog/2021/score/)
- [Lil'Log: "What are Diffusion Models?"](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
- [MIT 6.S184 (2026): Generative AI](https://diffusion.csail.mit.edu/)

---

## è‘—è€…ãƒªãƒ³ã‚¯

- Blog: https://fumishiki.dev
- X: https://x.com/fumishiki
- LinkedIn: https://www.linkedin.com/in/fumitakamurakami
- GitHub: https://github.com/fumishiki
- Hugging Face: https://huggingface.co/fumishiki

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

æœ¬è¨˜äº‹ã¯ [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja)ï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ»ã‚³ãƒ¢ãƒ³ã‚º è¡¨ç¤º - éå–¶åˆ© - ç¶™æ‰¿ 4.0 å›½éš›ï¼‰ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚

### âš ï¸ åˆ©ç”¨åˆ¶é™ã«ã¤ã„ã¦

**æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å€‹äººã®å­¦ç¿’ç›®çš„ã«é™ã‚Šåˆ©ç”¨å¯èƒ½ã§ã™ã€‚**

**ä»¥ä¸‹ã®ã‚±ãƒ¼ã‚¹ã¯äº‹å‰ã®æ˜ç¤ºçš„ãªè¨±å¯ãªãåˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’å›ºãç¦ã˜ã¾ã™:**

1. **ä¼æ¥­ãƒ»çµ„ç¹”å†…ã§ã®åˆ©ç”¨ï¼ˆå–¶åˆ©ãƒ»éå–¶åˆ©å•ã‚ãšï¼‰**
   - ç¤¾å†…ç ”ä¿®ã€æ•™è‚²ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã€ç¤¾å†…Wikiã¸ã®è»¢è¼‰
   - å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢ã§ã®è¬›ç¾©åˆ©ç”¨
   - éå–¶åˆ©å›£ä½“ã§ã®ç ”ä¿®åˆ©ç”¨
   - **ç†ç”±**: çµ„ç¹”å†…åˆ©ç”¨ã§ã¯å¸°å±è¡¨ç¤ºãŒå‰Šé™¤ã•ã‚Œã‚„ã™ãã€ç„¡æ–­æ”¹å¤‰ã®ãƒªã‚¹ã‚¯ãŒé«˜ã„ãŸã‚

2. **æœ‰æ–™ã‚¹ã‚¯ãƒ¼ãƒ«ãƒ»æƒ…å ±å•†æãƒ»ã‚»ãƒŸãƒŠãƒ¼ã§ã®åˆ©ç”¨**
   - å—è¬›æ–™ã‚’å¾´åã™ã‚‹å ´ã§ã®é…å¸ƒã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®æ²ç¤ºã€æ´¾ç”Ÿæ•™æã®ä½œæˆ

3. **LLM/AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®åˆ©ç”¨**
   - å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã®Pre-trainingã€Fine-tuningã€RAGã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»åˆ©ç”¨ã™ã‚‹ã“ã¨

4. **å‹æ‰‹ã«å†…å®¹ã‚’æœ‰æ–™åŒ–ã™ã‚‹è¡Œç‚ºå…¨èˆ¬**
   - æœ‰æ–™noteã€æœ‰æ–™è¨˜äº‹ã€Kindleå‡ºç‰ˆã€æœ‰æ–™å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€Patreoné™å®šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç­‰

**å€‹äººåˆ©ç”¨ã«å«ã¾ã‚Œã‚‹ã‚‚ã®:**
- å€‹äººã®å­¦ç¿’ãƒ»ç ”ç©¶
- å€‹äººçš„ãªãƒãƒ¼ãƒˆä½œæˆï¼ˆå€‹äººåˆ©ç”¨ã«é™ã‚‹ï¼‰
- å‹äººã¸ã®å…ƒè¨˜äº‹ãƒªãƒ³ã‚¯å…±æœ‰

**çµ„ç¹”ã§ã®å°å…¥ã‚’ã”å¸Œæœ›ã®å ´åˆ**ã¯ã€å¿…ãšè‘—è€…ã«é€£çµ¡ã‚’å–ã‚Šã€ä»¥ä¸‹ã‚’éµå®ˆã—ã¦ãã ã•ã„:
- å…¨ã¦ã®å¸°å±è¡¨ç¤ºãƒªãƒ³ã‚¯ã‚’ç¶­æŒ
- åˆ©ç”¨æ–¹æ³•ã‚’è‘—è€…ã«å ±å‘Š

**ç„¡æ–­åˆ©ç”¨ãŒç™ºè¦šã—ãŸå ´åˆ**ã€ä½¿ç”¨æ–™ã®è«‹æ±‚ãŠã‚ˆã³SNSç­‰ã§ã®å…¬è¡¨ã‚’è¡Œã†å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
