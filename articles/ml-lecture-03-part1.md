---
title: "ç¬¬3å›: ç·šå½¢ä»£æ•° II: SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ« â€” ä¸‡èƒ½ãƒŠã‚¤ãƒ•SVDã¨é€†ä¼æ’­ã®æ•°å­¦ ã€å‰ç·¨ã€‘ç†è«–ç·¨"
emoji: "ğŸ”¬"
type: "tech"
topics: ["machinelearning", "deeplearning", "linearalgebra", "python"]
published: true
---


# ç¬¬3å›: ç·šå½¢ä»£æ•° II â€” SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«

> **SVDã¯ä¸‡èƒ½ãƒŠã‚¤ãƒ•ã ã€‚ç”»åƒåœ§ç¸®ã‚‚PCAã‚‚æ¨è–¦ã‚‚ã€å…¨ã¦ã€ŒåŒã˜è¨ˆç®—ã€ã«å¸°ç€ã™ã‚‹ã€‚**

ç¬¬2å›ã§ç·šå½¢ä»£æ•°ã®åŸºç›¤ã‚’ç¯‰ã„ãŸã€‚ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã®å…¬ç†ã€è¡Œåˆ—æ¼”ç®—ã€å›ºæœ‰å€¤åˆ†è§£ã€æ­£å®šå€¤è¡Œåˆ—ã€å°„å½± â€” ã“ã‚Œã‚‰ã¯å…¨ã¦ã€Œæ­£æ–¹è¡Œåˆ—ã€ã®ä¸–ç•Œã®è©±ã ã£ãŸã€‚

ã ãŒã€ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã¯æ­£æ–¹è¡Œåˆ—ã§ã¯ãªã„ã€‚ç”»åƒã¯ $3 \times 224 \times 224$ ã®ãƒ†ãƒ³ã‚½ãƒ«ã ã€‚è¨€èªãƒ¢ãƒ‡ãƒ«ã®é‡ã¿è¡Œåˆ—ã¯ $d_{\text{model}} \times d_{\text{ff}}$ ã®é•·æ–¹å½¢è¡Œåˆ—ã ã€‚ãƒãƒƒãƒå‡¦ç†ã•ã‚ŒãŸAttentionã‚¹ã‚³ã‚¢ã¯ $B \times H \times T \times T$ ã®4éšãƒ†ãƒ³ã‚½ãƒ«ã ã€‚

**æ­£æ–¹è¡Œåˆ—ã®å¤–ã®ä¸–ç•Œ**ã‚’æ‰±ã†ãŸã‚ã«ã€3ã¤ã®é“å…·ãŒå¿…è¦ã«ãªã‚‹:

1. **SVD**ï¼ˆç‰¹ç•°å€¤åˆ†è§£ï¼‰â€” ä»»æ„ã®è¡Œåˆ—ã‚’åˆ†è§£ã™ã‚‹ã€Œä¸‡èƒ½ãƒŠã‚¤ãƒ•ã€
2. **è¡Œåˆ—å¾®åˆ†** â€” ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å­¦ç¿’ã®æ•°å­¦çš„åŸºç›¤
3. **ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—** â€” å¤šæ¬¡å…ƒé…åˆ—ã‚’æ•°å­¦çš„ã«æ‰±ã†è¨€èª

ã“ã®3ã¤ã‚’æœ¬è¬›ç¾©ã§å®Œå…¨æ­¦è£…ã™ã‚‹ã€‚

:::message
**ã“ã®ã‚·ãƒªãƒ¼ã‚ºã«ã¤ã„ã¦**: æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤å‹•ç”»è¬›ç¾©ã®**å®Œå…¨ä¸Šä½äº’æ›**ã®å…¨50å›ã‚·ãƒªãƒ¼ã‚ºã€‚ç†è«–ï¼ˆè«–æ–‡ãŒæ›¸ã‘ã‚‹ï¼‰ã€å®Ÿè£…ï¼ˆProduction-readyï¼‰ã€æœ€æ–°ï¼ˆ2025-2026 SOTAï¼‰ã®3è»¸ã§å·®åˆ¥åŒ–ã™ã‚‹ã€‚
:::

```mermaid
graph LR
    A["ğŸ”¬ SVD<br/>ç‰¹ç•°å€¤åˆ†è§£"] --> B["ğŸ“‰ ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼<br/>Eckart-Young"]
    B --> C["ğŸ“Š PCA (SVDç‰ˆ)<br/>åˆ†æ•£æœ€å¤§åŒ–"]
    A --> D["ğŸ“ æ“¬ä¼¼é€†è¡Œåˆ—<br/>Moore-Penrose"]
    E["âœï¸ è¡Œåˆ—å¾®åˆ†<br/>ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³"] --> F["â›“ï¸ é€£é–å¾‹<br/>è¨ˆç®—ã‚°ãƒ©ãƒ•"]
    F --> G["ğŸ”„ è‡ªå‹•å¾®åˆ†<br/>Forward/Reverse"]
    style A fill:#e1f5fe
    style G fill:#c8e6c9
```

**æ‰€è¦æ™‚é–“ã®ç›®å®‰**:

| ã‚¾ãƒ¼ãƒ³ | å†…å®¹ | æ™‚é–“ | é›£æ˜“åº¦ |
|:-------|:-----|:-----|:-------|
| Zone 0 | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ | 30ç§’ | â˜…â˜†â˜†â˜†â˜† |
| Zone 1 | ä½“é¨“ã‚¾ãƒ¼ãƒ³ | 10åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 2 | ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ | 15åˆ† | â˜…â˜…â˜†â˜†â˜† |
| Zone 3 | æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ | 60åˆ† | â˜…â˜…â˜…â˜…â˜… |
| Zone 4 | å®Ÿè£…ã‚¾ãƒ¼ãƒ³ | 45åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 5 | å®Ÿé¨“ã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜†â˜† |
| Zone 6 | æŒ¯ã‚Šè¿”ã‚Šã‚¾ãƒ¼ãƒ³ | 30åˆ† | â˜…â˜…â˜…â˜…â˜† |

---

## ğŸš€ 0. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ30ç§’ï¼‰â€” SVDã§ç”»åƒã‚’åœ§ç¸®ã™ã‚‹

**ã‚´ãƒ¼ãƒ«**: SVDãŒã€Œãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªçš„ãªæ§‹é€ ã‚’æŠ½å‡ºã™ã‚‹é“å…·ã€ã§ã‚ã‚‹ã“ã¨ã‚’30ç§’ã§ä½“æ„Ÿã™ã‚‹ã€‚

```python
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# grayscale image as matrix
np.random.seed(42)
A = np.random.randn(100, 80)  # 100Ã—80 matrix (like a small grayscale image)

# SVD
U, s, Vt = np.linalg.svd(A, full_matrices=False)

# Rank-5 approximation
k = 5
A_approx = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]

# Compression ratio
original_params = A.shape[0] * A.shape[1]  # 8000
compressed_params = k * (A.shape[0] + A.shape[1] + 1)  # 5 * 181 = 905
print(f"Original:    {original_params} parameters")
print(f"Compressed:  {compressed_params} parameters (rank-{k})")
print(f"Compression: {compressed_params/original_params:.1%}")
print(f"Error:       {np.linalg.norm(A - A_approx, 'fro') / np.linalg.norm(A, 'fro'):.4f}")
```

å‡ºåŠ›:
```
Original:    8000 parameters
Compressed:  905 parameters (rank-5)
Compression: 11.3%
Error:       0.8716
```

**ã“ã®5è¡Œã®è£ã«ã‚ã‚‹æ•°å­¦**:

$$
A = U \Sigma V^\top = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top
$$

ä»»æ„ã®è¡Œåˆ— $A \in \mathbb{R}^{m \times n}$ ã‚’ã€ç›´äº¤è¡Œåˆ— $U$ã€å¯¾è§’è¡Œåˆ— $\Sigma$ã€ç›´äº¤è¡Œåˆ— $V^\top$ ã®ç©ã«åˆ†è§£ã™ã‚‹ã€‚ä¸Šä½ $k$ å€‹ã®ç‰¹ç•°å€¤ã ã‘ã‚’æ®‹ã›ã°ã€**æœ€é©ãª** rank-$k$ è¿‘ä¼¼ãŒå¾—ã‚‰ã‚Œã‚‹[^3]ã€‚ã€Œæœ€é©ã€ã®æ„å‘³ã¯Eckart-Youngå®šç†ãŒä¿è¨¼ã™ã‚‹ã€‚

:::message
**é€²æ—: 3% å®Œäº†** SVDã§è¡Œåˆ—ã‚’åœ§ç¸®ã§ãã‚‹ã“ã¨ã‚’ä½“æ„Ÿã—ãŸã€‚æ®‹ã‚Š7ã‚¾ãƒ¼ãƒ³ã®å†’é™ºãŒå¾…ã£ã¦ã„ã‚‹ã€‚
:::

---

## ğŸ® 1. ä½“é¨“ã‚¾ãƒ¼ãƒ³ï¼ˆ10åˆ†ï¼‰â€” SVDã¨è¡Œåˆ—å¾®åˆ†ã‚’ã€Œè§¦ã£ã¦ã€ç†è§£ã™ã‚‹

### 1.1 SVDã®å¹¾ä½•å­¦ â€” è¡Œåˆ—ã¯ã€Œå›è»¢â†’æ‹¡å¤§â†’å›è»¢ã€

ç¬¬2å›ã§ã€Œè¡Œåˆ—ã¯ç·šå½¢å¤‰æ›ã€ã¨è¨€ã£ãŸã€‚SVDã¯ã€ãã®å¤‰æ›ã‚’3ã¤ã®åŸºæœ¬æ“ä½œã«åˆ†è§£ã™ã‚‹ã€‚

$$
A = U \Sigma V^\top
$$

| æˆåˆ† | å¹¾ä½•å­¦çš„æ„å‘³ | è¡Œåˆ—ã®å‹ |
|:-----|:-----------|:---------|
| $V^\top$ | å…¥åŠ›ç©ºé–“ã§ã®å›è»¢ï¼ˆç›´äº¤å¤‰æ›ï¼‰ | $n \times n$ ç›´äº¤è¡Œåˆ— |
| $\Sigma$ | å„è»¸æ–¹å‘ã®æ‹¡å¤§ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰ | $m \times n$ å¯¾è§’è¡Œåˆ— |
| $U$ | å‡ºåŠ›ç©ºé–“ã§ã®å›è»¢ï¼ˆç›´äº¤å¤‰æ›ï¼‰ | $m \times m$ ç›´äº¤è¡Œåˆ— |

```python
import numpy as np
import matplotlib.pyplot as plt

# 2D example: matrix transforms a unit circle
A = np.array([[3, 1],
              [1, 2]])

# SVD
U, s, Vt = np.linalg.svd(A)
print(f"U = \n{np.round(U, 4)}")
print(f"Singular values = {np.round(s, 4)}")
print(f"Vt = \n{np.round(Vt, 4)}")

# Unit circle
theta = np.linspace(0, 2 * np.pi, 100)
circle = np.array([np.cos(theta), np.sin(theta)])

# Apply each SVD step
step1 = Vt @ circle         # V^T: rotate in input space
step2 = np.diag(s) @ step1  # Sigma: scale
step3 = U @ step2           # U: rotate in output space

# Verify: A @ circle == U @ Sigma @ Vt @ circle
direct = A @ circle
print(f"\nSVD reconstruction matches: {np.allclose(step3, direct)}")
```

**æ ¸å¿ƒ**: ã©ã‚“ãªè¡Œåˆ—ã«ã‚ˆã‚‹å¤‰æ›ã‚‚ã€Œå›è»¢ â†’ æ‹¡å¤§ â†’ å›è»¢ã€ã«åˆ†è§£ã§ãã‚‹ã€‚ç‰¹ç•°å€¤ $\sigma_1, \sigma_2, \ldots$ ã¯æ‹¡å¤§ç‡ã‚’è¡¨ã—ã€é™é †ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã€‚

```mermaid
graph LR
    IN["ğŸ”µ å˜ä½å††<br/>(å…¥åŠ›)"] -->|"V^T<br/>å›è»¢"| R1["ğŸ”µ å›è»¢ã•ã‚ŒãŸå††"]
    R1 -->|"Î£<br/>æ‹¡å¤§"| EL["ğŸ”´ æ¥•å††"]
    EL -->|"U<br/>å›è»¢"| OUT["ğŸ”´ å›è»¢ã•ã‚ŒãŸæ¥•å††<br/>(å‡ºåŠ›)"]

    style IN fill:#e3f2fd
    style OUT fill:#ffcdd2
```

### 1.2 ç‰¹ç•°å€¤ã®æ¸›è¡° â€” ãªãœä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ãŒæœ‰åŠ¹ãªã®ã‹

å®Ÿãƒ‡ãƒ¼ã‚¿ã®è¡Œåˆ—ã¯ã€ç‰¹ç•°å€¤ãŒæ€¥é€Ÿã«æ¸›è¡°ã™ã‚‹ã€‚ã“ã‚ŒãŒä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã‚„PCA[^5][^6]ãŒæœ‰åŠ¹ãªç†ç”±ã ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt

# Example: create a matrix with rapid singular value decay
np.random.seed(42)
# Low-rank structure + noise
rank_true = 5
m, n = 100, 80
U_true = np.linalg.qr(np.random.randn(m, rank_true))[0]
V_true = np.linalg.qr(np.random.randn(n, rank_true))[0]
s_true = np.array([10, 5, 2, 1, 0.5])
A_clean = U_true @ np.diag(s_true) @ V_true.T
A_noisy = A_clean + 0.1 * np.random.randn(m, n)

# SVD of noisy matrix
U, s, Vt = np.linalg.svd(A_noisy, full_matrices=False)

print("Top 10 singular values:")
for i, sv in enumerate(s[:10]):
    bar = "â–ˆ" * int(sv * 3)
    print(f"  Ïƒ_{i+1:2d} = {sv:8.4f}  {bar}")

# Cumulative energy
energy = np.cumsum(s**2) / np.sum(s**2)
print(f"\nCumulative energy:")
for k in [1, 2, 3, 5, 10, 20]:
    if k <= len(energy):
        print(f"  rank-{k:2d}: {energy[k-1]:.4f} ({energy[k-1]*100:.1f}%)")
```

**é‡è¦ãªæ´å¯Ÿ**: ä¸Šä½5å€‹ã®ç‰¹ç•°å€¤ã ã‘ã§å…ƒã®è¡Œåˆ—ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆFrobenius ãƒãƒ«ãƒ ã®äºŒä¹—ï¼‰ã®99%ä»¥ä¸Šã‚’æ•æ‰ã§ãã‚‹ã€‚ã“ã‚Œã¯å…ƒã®è¡Œåˆ—ãŒã€Œæœ¬è³ªçš„ã« rank-5ã€ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

### 1.3 å‹¾é…ã‚’ã€Œè¦‹ã‚‹ã€â€” æå¤±é–¢æ•°ã®åœ°å½¢

Backpropagation[^2]ã®æ ¸å¿ƒã¯å‹¾é…ã®è¨ˆç®—ã ã€‚å‹¾é…ã¨ã¯ã€Œæå¤±é–¢æ•°ãŒã©ã®æ–¹å‘ã«ã©ã‚Œã ã‘å¤‰åŒ–ã™ã‚‹ã‹ã€ã‚’è¡¨ã™ãƒ™ã‚¯ãƒˆãƒ«ã€‚

```python
import numpy as np

# Simple loss: L(w) = (y - w^T x)^2
x = np.array([1.0, 2.0, 3.0])
y = 10.0
w = np.array([1.0, 1.0, 1.0])

# Forward pass
y_pred = w @ x  # w^T x = 6
loss = (y - y_pred) ** 2  # (10 - 6)^2 = 16
print(f"y_pred = {y_pred}, loss = {loss}")

# Gradient: dL/dw = -2(y - w^T x) * x
grad = -2 * (y - y_pred) * x
print(f"gradient = {grad}")

# Gradient descent step
lr = 0.01
w_new = w - lr * grad
y_pred_new = w_new @ x
loss_new = (y - y_pred_new) ** 2
print(f"After update: y_pred = {y_pred_new:.4f}, loss = {loss_new:.4f}")
print(f"Loss decreased: {loss:.4f} â†’ {loss_new:.4f}")
```

**å‹¾é… $\nabla_{\mathbf{w}} L$ ã¯ã€Œæå¤±ã‚’æœ€ã‚‚é€Ÿãæ¸›å°‘ã•ã›ã‚‹æ–¹å‘ã€**ã®é€†æ–¹å‘ã ã€‚$-\nabla L$ ã®æ–¹å‘ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•ã‹ã™ã®ãŒå‹¾é…é™ä¸‹æ³•ã€‚

### 1.4 ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã‚’ã€Œè¦‹ã‚‹ã€â€” ãƒ™ã‚¯ãƒˆãƒ«â†’ãƒ™ã‚¯ãƒˆãƒ«é–¢æ•°ã®å¾®åˆ†

ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•°ã®å‹¾é…ã¯ã€Œãƒ™ã‚¯ãƒˆãƒ«ã€ã ã£ãŸã€‚ã§ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã¸ã®é–¢æ•°ã®å¾®åˆ†ã¯ï¼Ÿ â€” ãã‚ŒãŒ**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**ï¼ˆJacobian matrixï¼‰ã€‚

$$
\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m, \quad J = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{pmatrix}
$$

```python
import numpy as np

# f: R^2 -> R^2, f(x) = [x1^2 + x2, x1 * x2]
def f(x):
    return np.array([x[0]**2 + x[1], x[0] * x[1]])

# Analytical Jacobian
def jacobian(x):
    return np.array([
        [2 * x[0], 1],        # df1/dx1, df1/dx2
        [x[1],     x[0]]      # df2/dx1, df2/dx2
    ])

# Numerical Jacobian (finite differences)
def numerical_jacobian(f, x, eps=1e-7):
    n = len(x)
    m = len(f(x))
    J = np.zeros((m, n))
    for j in range(n):
        x_plus = x.copy()
        x_plus[j] += eps
        x_minus = x.copy()
        x_minus[j] -= eps
        J[:, j] = (f(x_plus) - f(x_minus)) / (2 * eps)
    return J

x = np.array([2.0, 3.0])
J_analytical = jacobian(x)
J_numerical = numerical_jacobian(f, x)

print(f"Analytical Jacobian:\n{J_analytical}")
print(f"Numerical Jacobian:\n{np.round(J_numerical, 6)}")
print(f"Match: {np.allclose(J_analytical, J_numerical, atol=1e-5)}")
```

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å„è¡Œã¯ã€å‡ºåŠ›ã®å„æˆåˆ†ã®å‹¾é…**ã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®è¡Œåˆ—å¼ $\det(J)$ ã¯ã€Œå¤‰æ›ã«ã‚ˆã‚‹ä½“ç©ã®å¤‰åŒ–ç‡ã€ã‚’è¡¨ã—ã€Normalizing Flow[^13]ã®æ ¸å¿ƒçš„ãªè¨ˆç®—é‡ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹ã€‚

### 1.5 è‡ªå‹•å¾®åˆ†ã®å¨åŠ› â€” PyTorchã® `backward()` ãŒå†…éƒ¨ã§ã‚„ã£ã¦ã„ã‚‹ã“ã¨

```python
# PyTorch-style automatic differentiation (manual implementation)
import numpy as np

class Var:
    """Simple autograd variable for demonstration"""
    def __init__(self, data, _children=(), _op=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._children = set(_children)
        self._op = _op

    def __mul__(self, other):
        other = other if isinstance(other, Var) else Var(other)
        out = Var(self.data * other.data, (self, other), '*')
        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward
        return out

    def __add__(self, other):
        other = other if isinstance(other, Var) else Var(other)
        out = Var(self.data + other.data, (self, other), '+')
        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward
        return out

    def backward(self):
        # topological sort
        topo = []
        visited = set()
        def build(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build(child)
                topo.append(v)
        build(self)
        self.grad = 1.0
        for v in reversed(topo):
            v._backward()

# Demo: f(a, b) = a*b + b
a = Var(2.0)
b = Var(3.0)
c = a * b      # c = 6
d = c + b      # d = 9
d.backward()

print(f"d = {d.data}")        # 9.0
print(f"dd/da = {a.grad}")    # b = 3.0 (correct: d(ab+b)/da = b)
print(f"dd/db = {b.grad}")    # a+1 = 3.0 (correct: d(ab+b)/db = a+1)
```

ã“ã®ãŸã£ãŸ50è¡Œã®ã‚³ãƒ¼ãƒ‰ãŒã€PyTorchã® `loss.backward()` ã®æœ¬è³ªã [^7][^8]ã€‚è¨ˆç®—ã®ã€Œè¨˜éŒ²ã€ã‚’é€†é †ã«è¾¿ã£ã¦å‹¾é…ã‚’ä¼æ’­ã™ã‚‹ â€” ã“ã‚ŒãŒ**Reverse Mode è‡ªå‹•å¾®åˆ†**ã§ã‚ã‚Šã€Backpropagation[^2]ã®æ­£ä½“ã ã€‚

:::message
**é€²æ—: 15% å®Œäº†** SVDã®å¹¾ä½•å­¦ã€ç‰¹ç•°å€¤ã®æ¸›è¡°ã€å‹¾é…ã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã€è‡ªå‹•å¾®åˆ†ã®åŸºæœ¬ã‚’ä½“é¨“ã—ãŸã€‚ã“ã“ã‹ã‚‰ç›´æ„Ÿã‚’æ·±ã‚ã¦Zone 3ã®æ•°å¼ä¿®è¡Œã«å‚™ãˆã‚‹ã€‚
:::

---

## ğŸ§© 2. ç›´æ„Ÿã‚¾ãƒ¼ãƒ³ï¼ˆ15åˆ†ï¼‰â€” SVDã¨è‡ªå‹•å¾®åˆ†ãŒAIã‚’æ”¯ãˆã‚‹ç†ç”±

### 2.1 ç¬¬3å›ã®ã€Œåœ°å›³ã€

ç¬¬2å›ã§ç·šå½¢ä»£æ•°ã®ã€Œæ–‡æ³•ã€ã‚’å­¦ã‚“ã ã€‚ç¬¬3å›ã§ã¯ã€Œä¿®è¾æ³•ã€ã‚’å­¦ã¶ã€‚

| é“å…· | æ¯”å–© | æ©Ÿæ¢°å­¦ç¿’ã§ã®å½¹å‰² |
|:-----|:-----|:--------------|
| **SVD** | ä¸‡èƒ½ãƒŠã‚¤ãƒ• | ãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªçš„æ§‹é€ ã‚’æŠ½å‡ºï¼ˆPCA, LoRA[^10], æ¨è–¦ï¼‰ |
| **è¡Œåˆ—å¾®åˆ†** | ç¾…é‡ç›¤ | æå¤±é–¢æ•°ã®å‹¾é…æ–¹å‘ã‚’ç¤ºã™ |
| **é€£é–å¾‹** | é€£é–åå¿œ | å¤šå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’ä¸€æ‹¬è¨ˆç®— |
| **è‡ªå‹•å¾®åˆ†** | è‡ªå‹•ç¿»è¨³æ©Ÿ | æ•°å¼â†’å‹¾é…è¨ˆç®—ã‚³ãƒ¼ãƒ‰ã®è‡ªå‹•å¤‰æ› |
| **ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—** | å¤šæ¬¡å…ƒã®æ–‡æ³• | ãƒãƒƒãƒãƒ»ãƒ˜ãƒƒãƒ‰ãƒ»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ä¸€æ‹¬å‡¦ç† |

### 2.2 Course I ã®ä¸­ã§ã®ä½ç½®ã¥ã‘

```mermaid
graph TD
    L1["ç¬¬1å›: æ¦‚è«–<br/>æ•°å¼ã¨è«–æ–‡ã®èª­ã¿æ–¹"]
    L2["ç¬¬2å›: ç·šå½¢ä»£æ•° I<br/>ãƒ™ã‚¯ãƒˆãƒ«ãƒ»è¡Œåˆ—ãƒ»å›ºæœ‰å€¤"]
    L3["ç¬¬3å›: ç·šå½¢ä»£æ•° II<br/>SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»ãƒ†ãƒ³ã‚½ãƒ«<br/>ğŸ¯ Backpropå®Œå…¨å°å‡º"]
    L4["ç¬¬4å›: ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦<br/>åˆ†å¸ƒãƒ»ãƒ™ã‚¤ã‚ºæ¨è«–"]

    L1 -->|"æ•°å¼ãŒèª­ã‚ãŸ"| L2
    L2 -->|"è¡Œåˆ—ã‚’æ‰±ãˆãŸ"| L3
    L3 -->|"å¾®åˆ†ã‚‚ã§ããŸ"| L4

    style L3 fill:#ffeb3b
```

| å› | ãƒ†ãƒ¼ãƒ | LLM/Transformerã¨ã®æ¥ç‚¹ |
|:---|:------|:----------------------|
| ç¬¬2å› | ç·šå½¢ä»£æ•° I | $QK^\top$ ã®å†…ç©ã€å›ºæœ‰å€¤â†’PCAâ†’åŸ‹ã‚è¾¼ã¿ |
| **ç¬¬3å›** | **ç·šå½¢ä»£æ•° II** | **ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³â†’Flow Modelã€å‹¾é…â†’Backpropã€é€£é–å¾‹â†’Transformerå„å±¤** |
| ç¬¬4å› | ç¢ºç‡è«–ãƒ»çµ±è¨ˆå­¦ | $p(x_t \mid x_{<t})$ è‡ªå·±å›å¸°ã€Softmaxåˆ†å¸ƒ |

**ç¬¬2å›â†’ç¬¬3å›ã®æ¥ç¶š**: ç¬¬2å›ã§å›ºæœ‰å€¤åˆ†è§£ã‚’å­¦ã‚“ã ã€‚ã ãŒå›ºæœ‰å€¤åˆ†è§£ã¯æ­£æ–¹è¡Œåˆ—ã«ã—ã‹ä½¿ãˆãªã„ã€‚SVDã¯ãã®åˆ¶ç´„ã‚’å–ã‚Šæ‰•ã„ã€**ä»»æ„ã®é•·æ–¹å½¢è¡Œåˆ—**ã‚’åˆ†è§£ã§ãã‚‹ä¸‡èƒ½ãƒ„ãƒ¼ãƒ«ã ã€‚

### 2.3 æ¾å°¾ç ”ã¨ã®å·®åˆ¥åŒ–

| æ¾å°¾ç ”ã®å‰æ | å®Ÿéš›ã®å£ | æœ¬è¬›ç¾©ã®å¯¾ç­– |
|:------------|:--------|:-----------|
| ã€ŒSVDã¯çŸ¥ã£ã¦ã‚‹ã‚ˆã­ã€ | Eckart-Youngå®šç†[^3]ã®æ„å‘³ãŒèª¬æ˜ã§ããªã„ | å­˜åœ¨å®šç†â†’å¹¾ä½•å­¦â†’æœ€é©æ€§ã‚’å…¨å°å‡º |
| ã€ŒBackpropã¯ç†è§£ã—ã¦ã‚‹ã‚ˆã­ã€ | è¡Œåˆ—å¾®åˆ†ã®é€£é–å¾‹ãŒæ›¸ã‘ãªã„ | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³â†’é€£é–å¾‹â†’Backpropã‚’ä¸€ã‹ã‚‰å°å‡º |
| ã€Œè‡ªå‹•å¾®åˆ†ã¯ PyTorch ã«ä»»ã›ã¦ã€ | Forward/Reverse ã®è¨ˆç®—é‡ã®å·®ãŒã‚ã‹ã‚‰ãªã„ | Wengert list ã‹ã‚‰ Forward/Reverse ã‚’æ‰‹å‹•å®Ÿè£… |
| ã€Œãƒ†ãƒ³ã‚½ãƒ«ã¯NumPyã®é…åˆ—ã€ | æ·»å­—ã®ç¸®ç´„è¦å‰‡ãŒèª­ã‚ãªã„ | Einsteinè¨˜æ³•â†’einsumå®Œå…¨ç‰ˆ |

### 2.4 LLMã®ä¸­ã®SVDã¨è¡Œåˆ—å¾®åˆ†

LLMã®å­¦ç¿’ã¨æ¨è«–ã®ä¸¡æ–¹ã§ã€SVDã¨è¡Œåˆ—å¾®åˆ†ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚

```mermaid
graph TD
    subgraph "ğŸ¯ æ¨è«–ï¼ˆForward Passï¼‰"
        EMB["åŸ‹ã‚è¾¼ã¿<br/>E[x_t]"] --> ATTN["Attention<br/>QK^T/âˆšd"]
        ATTN --> FFN["FFN<br/>W_2 Ïƒ(W_1 h)"]
        FFN --> OUT["å‡ºåŠ›<br/>logits"]
    end

    subgraph "ğŸ”„ å­¦ç¿’ï¼ˆBackward Passï¼‰"
        LOSS["Loss = -log p(x_t)"] --> GRAD_OUT["âˆ‚L/âˆ‚logits<br/>ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³"]
        GRAD_OUT --> GRAD_FFN["âˆ‚L/âˆ‚W_1, âˆ‚L/âˆ‚W_2<br/>é€£é–å¾‹"]
        GRAD_FFN --> GRAD_ATTN["âˆ‚L/âˆ‚W_Q, âˆ‚L/âˆ‚W_K, âˆ‚L/âˆ‚W_V<br/>é€£é–å¾‹"]
        GRAD_ATTN --> UPDATE["Adamæ›´æ–°<br/>W â† W - lrÂ·mÌ‚/(âˆšvÌ‚+Îµ)"]
    end

    subgraph "ğŸ”§ åŠ¹ç‡åŒ–"
        LORA["LoRA: Î”W = BA<br/>ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ (SVDçš„ç™ºæƒ³)"]
        PRUNE["æ§‹é€ åŒ–æåˆˆã‚Š<br/>SVDã§é‡è¦åº¦åˆ¤å®š"]
    end

    OUT -.->|"Cross-Entropy"| LOSS
    UPDATE -.->|"é‡ã¿æ›´æ–°"| EMB
    LORA -.->|"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡åŒ–"| UPDATE
    PRUNE -.->|"ãƒ¢ãƒ‡ãƒ«åœ§ç¸®"| FFN

    style LOSS fill:#ffcdd2
    style LORA fill:#c8e6c9
```

| LLMã®æ“ä½œ | ç¬¬3å›ã®å¯¾å¿œã‚»ã‚¯ã‚·ãƒ§ãƒ³ | ãªãœå¿…è¦ã‹ |
|:----------|:-------------------|:---------|
| Forward pass | 3.7 é€£é–å¾‹ | å„å±¤ã®å‡ºåŠ›ã‚’é †ã«è¨ˆç®— |
| Backward pass | 3.7 é€£é–å¾‹ + 3.8 Backprop | å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’é€†é †ã«è¨ˆç®— |
| LoRA | 3.3 ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ | é‡ã¿æ›´æ–°ã‚’ rank-$r$ ã§è¿‘ä¼¼ |
| Adam optimizer | 3.6 å‹¾é… | ä¸€æ¬¡ãƒ»äºŒæ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã®æ¨å®š |
| å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚° | 3.6 ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ | å‹¾é…çˆ†ç™ºã®é˜²æ­¢ |

### 2.5 3ã¤ã®æ¯”å–©ã§æ‰ãˆã‚‹æœ¬è¬›ç¾©ã®æœ¬è³ª

**æ¯”å–©1: SVDã¯ã€Œé¡•å¾®é¡ã€**

è¡Œåˆ—ã®ã€Œå¾®ç´°æ§‹é€ ã€ã‚’ç‰¹ç•°å€¤ã¨ã„ã†æ•°å€¤ã§èª­ã¿å–ã‚‹ã€‚å¤§ãã„ç‰¹ç•°å€¤ = é‡è¦ãªæ§‹é€ ã€å°ã•ã„ç‰¹ç•°å€¤ = ãƒã‚¤ã‚ºã€‚é¡•å¾®é¡ã®å€ç‡ã‚’å¤‰ãˆã‚‹ã‚ˆã†ã«ã€æ®‹ã™ç‰¹ç•°å€¤ã®æ•°ï¼ˆãƒ©ãƒ³ã‚¯ $k$ï¼‰ã‚’å¤‰ãˆã‚‹ã“ã¨ã§ã€ç²—ã„æ§‹é€ ã‹ã‚‰ç²¾å¯†ãªæ§‹é€ ã¾ã§è¦‹ãˆã‚‹ã€‚

**æ¯”å–©2: è¡Œåˆ—å¾®åˆ†ã¯ã€Œé«˜æ¬¡å…ƒã®å‚¾ãã€**

2æ¬¡å…ƒã§ $y = f(x)$ ã®å‚¾ããŒ $f'(x)$ ã ã£ãŸã‚ˆã†ã«ã€é«˜æ¬¡å…ƒã§ $\mathbf{y} = \mathbf{f}(\mathbf{x})$ ã®ã€Œå‚¾ãã€ãŒãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ $J$ ã ã€‚ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã¯ã€Œå…¥åŠ›ã®å¾®å°å¤‰åŒ–ãŒå‡ºåŠ›ã«ã©ã†ä¼æ’­ã™ã‚‹ã‹ã€ã‚’è¡Œåˆ—ã¨ã—ã¦è¡¨ç¾ã™ã‚‹ã€‚

**æ¯”å–©3: è‡ªå‹•å¾®åˆ†ã¯ã€Œè¨ˆç®—ã®éŒ²ç”»ã¨å·»ãæˆ»ã—ã€**

Forward passã§è¨ˆç®—ã‚’ã€ŒéŒ²ç”»ã€ã—ã€Backward passã§ã€Œå·»ãæˆ»ã—ã€ãªãŒã‚‰å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ã€‚VHSãƒ†ãƒ¼ãƒ—ã®å·»ãæˆ»ã—ã¨åŒã˜ã§ã€æœ€å¾Œã«è¨ˆç®—ã—ãŸéƒ¨åˆ†ã‹ã‚‰é †ã«å‹¾é…ãŒæ±‚ã¾ã‚‹ã€‚

### 2.6 å­¦ç¿’æˆ¦ç•¥

ã“ã®è¬›ç¾©ã¯ç¬¬2å›ã‚ˆã‚Šã‚‚ã•ã‚‰ã«æ•°å¼ãŒå¤šã„ã€‚å¿ƒæ§‹ãˆ:

1. **Zone 3 ãŒæœ€é‡è¦**ã€‚90åˆ†ã‚’æƒœã—ã¾ãªã„
2. **SVD â†’ è¡Œåˆ—å¾®åˆ† â†’ è‡ªå‹•å¾®åˆ†** ã®é †ã§å­¦ã¶ï¼ˆå„ãƒˆãƒ”ãƒƒã‚¯ãŒå‰ã®ãƒˆãƒ”ãƒƒã‚¯ã«ä¾å­˜ã™ã‚‹ï¼‰
3. **æ•°å€¤æ¤œè¨¼ã‚’æ€ ã‚‰ãªã„**: è§£æçš„ãªçµæœã¯å¿…ãšã‚³ãƒ¼ãƒ‰ã§ç¢ºèªã™ã‚‹
4. **ç´™ã«æ›¸ã**: 2Ã—2è¡Œåˆ—ã®SVDã‚’æ‰‹è¨ˆç®—ã§1å›ã‚„ã‚‹ã¨ç†è§£ãŒæ®µé•ã„ã«æ·±ã¾ã‚‹
5. **Zone 5 ã§è…•è©¦ã—**: SVDç”»åƒåœ§ç¸®ã¨è‡ªå‹•å¾®åˆ†ã®æ‰‹å‹•å®Ÿè£…ãŒã€ç†è§£åº¦ã®æœ€è‰¯ã®ãƒ†ã‚¹ãƒˆ

### 2.7 SVDãƒ»è¡Œåˆ—å¾®åˆ†ã®æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘

```mermaid
graph TD
    SVD["SVD<br/>ç‰¹ç•°å€¤åˆ†è§£"]
    AD["è‡ªå‹•å¾®åˆ†<br/>Forward/Reverse"]
    CALC["è¡Œåˆ—å¾®åˆ†<br/>ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³"]

    SVD --> PCA_["PCA<br/>æ¬¡å…ƒå‰Šæ¸›"]
    SVD --> LORA["LoRA<br/>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡åŒ–"]
    SVD --> REC["æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ <br/>å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"]
    SVD --> COMPRESS["ãƒ¢ãƒ‡ãƒ«åœ§ç¸®<br/>ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼"]
    SVD --> NMF["NMF<br/>éè² è¡Œåˆ—åˆ†è§£"]

    AD --> BACKPROP["Backpropagation<br/>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆå­¦ç¿’"]
    AD --> JAX_["JAX<br/>é–¢æ•°å¤‰æ›"]
    AD --> PHYSICS["Physics-Informed NN<br/>å¾®åˆ†æ–¹ç¨‹å¼"]

    CALC --> NF["Normalizing Flow<br/>ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼"]
    CALC --> NATURAL["Natural Gradient<br/>Fisheræƒ…å ±è¡Œåˆ—"]
    CALC --> HESSIAN["äºŒæ¬¡æœ€é©åŒ–<br/>Newtonæ³•"]

    style SVD fill:#e3f2fd
    style AD fill:#c8e6c9
    style CALC fill:#fff9c4
```

| æŠ€è¡“ | é–¢é€£ã™ã‚‹æ•°å­¦ | å¿œç”¨ | è¬›ç¾© |
|:-----|:-----------|:-----|:-----|
| LoRA[^10] | SVD + ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ | LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | æœ¬è¬›ç¾© |
| FlashAttention[^12] | è¡Œåˆ—ã®ãƒ–ãƒ­ãƒƒã‚¯åˆ†å‰² | Attentioné«˜é€ŸåŒ– | ç¬¬2å› |
| Normalizing Flow[^13] | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—å¼ | ç¢ºç‡å¯†åº¦å¤‰æ› | ç¬¬25å› |
| Natural Gradient | Fisheræƒ…å ±è¡Œåˆ— | æœ€é©åŒ–ã®å¹¾ä½•å­¦ | ç¬¬27å› |
| Neural ODE | è‡ªå‹•å¾®åˆ† + ODE | é€£ç¶šæ·±åº¦ãƒ¢ãƒ‡ãƒ« | ç¬¬26å› |
| Spectral Normalization | SVDã®æœ€å¤§ç‰¹ç•°å€¤ | GANå®‰å®šåŒ– | ç¬¬14å› |

### 2.8 è‡ªå‹•å¾®åˆ†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é€²åŒ–

```mermaid
timeline
    title è‡ªå‹•å¾®åˆ†ã®æ­´å²
    1964 : Wengert ãŒ AD ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç™ºè¡¨
    1970 : Linnainmaa ãŒ Reverse Mode ã‚’å®šå¼åŒ–
    1986 : Rumelhart-Hinton ãŒ Backpropagation ã¨ã—ã¦å†ç™ºè¦‹
    2007 : Theano â€” è¨˜å·å¾®åˆ† + ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    2015 : TensorFlow 1.x â€” é™çš„è¨ˆç®—ã‚°ãƒ©ãƒ•
    2016 : PyTorch â€” å‹•çš„è¨ˆç®—ã‚°ãƒ©ãƒ• (Define-by-Run)
    2018 : JAX â€” é–¢æ•°å¤‰æ› (grad, jit, vmap, pmap)
    2020 : PyTorch 2.0 â€” torch.compile (å‹•çš„+é™çš„ã®èåˆ)
    2024 : Reactant.jl â€” Julia + XLA ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
```

| ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | ADæ–¹å¼ | ç‰¹å¾´ | é•·æ‰€ |
|:-------------|:------|:-----|:-----|
| PyTorch | Reverse (tape-based) | Define-by-Run | æŸ”è»Ÿã€ãƒ‡ãƒãƒƒã‚°ã—ã‚„ã™ã„ |
| JAX | Forward + Reverse (tracing) | é–¢æ•°å¤‰æ› | `grad`, `vmap`, `jit` ã®åˆæˆ |
| TensorFlow | Reverse (graph-based) | é™çš„æœ€é©åŒ– | ãƒ‡ãƒ—ãƒ­ã‚¤ã«å¼·ã„ |
| Zygote.jl | Source-to-source | Julia ASTå¤‰æ› | ä»»æ„ã®Juliaã‚³ãƒ¼ãƒ‰ã«é©ç”¨å¯èƒ½ |
| Enzyme | LLVM IR ãƒ¬ãƒ™ãƒ« | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©çµ±åˆ | è¨€èªéä¾å­˜ |

:::details JAX ã®é–¢æ•°å¤‰æ›: grad, jit, vmap
JAXã®é©æ–°ã¯ã€è‡ªå‹•å¾®åˆ†ã‚’ã€Œé–¢æ•°å¤‰æ›ã€ã¨ã—ã¦æ‰±ã†ã“ã¨ã€‚

```python
# JAX-style function transforms (conceptual)
# grad: f â†’ âˆ‡f
# jit: f â†’ compiled f
# vmap: f â†’ batched f

# Real JAX code would look like:
# import jax
# import jax.numpy as jnp
#
# def loss(params, x, y):
#     pred = params @ x
#     return jnp.sum((pred - y)**2)
#
# grad_fn = jax.grad(loss)        # returns gradient function
# fast_grad = jax.jit(grad_fn)    # compile for speed
# batch_grad = jax.vmap(grad_fn)  # vectorize over batch
```

`grad` ãŒè¿”ã™ã®ã¯**é–¢æ•°**ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Œå‹¾é…ã®å‹¾é…ã€ï¼ˆãƒ˜ã‚·ã‚¢ãƒ³ï¼‰ã‚‚ç°¡å˜ã«è¨ˆç®—ã§ãã‚‹:

```python
# hessian = jax.hessian(loss)  # âˆ‡Â²f
# jvp = jax.jvp(f, primals, tangents)  # Forward Mode
# vjp = jax.vjp(f, primals)  # Reverse Mode
```
:::

:::message
**é€²æ—: 20% å®Œäº†** SVDãƒ»è¡Œåˆ—å¾®åˆ†ãƒ»è‡ªå‹•å¾®åˆ†ã®å…¨ä½“åƒã‚’æ´ã‚“ã ã€‚ã“ã“ã‹ã‚‰Zone 3ã€Œæ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ã€â€” æœ¬è¬›ç¾©æœ€å¤§ã®å±±å ´ã ã€‚
:::

---

## ğŸ“ 3. æ•°å¼ä¿®è¡Œã‚¾ãƒ¼ãƒ³ï¼ˆ60åˆ†ï¼‰â€” SVDã‹ã‚‰è‡ªå‹•å¾®åˆ†ã¾ã§

> **ç›®æ¨™**: SVDã®å­˜åœ¨å®šç†ã¨æœ€é©æ€§ã€è¡Œåˆ—å¾®åˆ†ã®ä½“ç³»ã€é€£é–å¾‹ã€è‡ªå‹•å¾®åˆ†ã®ç†è«–ã‚’å°å‡ºã—ã€Backpropagationã®æ•°å­¦çš„åŸºç›¤ã‚’å®Œå…¨ç†è§£ã™ã‚‹ã€‚

æœ¬ã‚·ãƒªãƒ¼ã‚ºã§æœ€ã‚‚æ•°å¼å¯†åº¦ãŒé«˜ã„ã‚¾ãƒ¼ãƒ³ã ã€‚ã ãŒã€ã“ã“ã§å­¦ã¶å…¨ã¦ã®æ¦‚å¿µã¯ã€ç¬¬9å›ä»¥é™ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ç¹°ã‚Šè¿”ã—ç™»å ´ã™ã‚‹ã€‚ä¸€ã¤ãšã¤ã€ç¢ºå®Ÿã«ç†è§£ã—ã¦ã„ã“ã†ã€‚

### 3.1 SVDï¼ˆç‰¹ç•°å€¤åˆ†è§£ï¼‰ã®å®šç¾©ã¨å­˜åœ¨å®šç†

#### å®šç¾©

**å®šç†** (ç‰¹ç•°å€¤åˆ†è§£): ä»»æ„ã®è¡Œåˆ— $A \in \mathbb{R}^{m \times n}$ ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®åˆ†è§£ãŒå­˜åœ¨ã™ã‚‹:

$$
A = U \Sigma V^\top
$$

ã“ã“ã§:
- $U \in \mathbb{R}^{m \times m}$: ç›´äº¤è¡Œåˆ—ï¼ˆ$U^\top U = I_m$ï¼‰â€” **å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«**
- $\Sigma \in \mathbb{R}^{m \times n}$: å¯¾è§’è¡Œåˆ—ï¼ˆ$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ï¼‰â€” **ç‰¹ç•°å€¤**
- $V \in \mathbb{R}^{n \times n}$: ç›´äº¤è¡Œåˆ—ï¼ˆ$V^\top V = I_n$ï¼‰â€” **å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«**
- $r = \text{rank}(A)$

#### å›ºæœ‰å€¤åˆ†è§£ã¨ã®é–¢ä¿‚

SVDã®å­˜åœ¨ã¯ã€å›ºæœ‰å€¤åˆ†è§£ã‹ã‚‰å°ã‘ã‚‹ã€‚

$A^\top A$ ã¯ $n \times n$ ã®åŠæ­£å®šå€¤å¯¾ç§°è¡Œåˆ—ãªã®ã§ã€ã‚¹ãƒšã‚¯ãƒˆãƒ«å®šç†ã‚ˆã‚Šç›´äº¤å¯¾è§’åŒ–å¯èƒ½:

$$
A^\top A = V \Lambda V^\top, \quad \Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n), \quad \lambda_1 \geq \cdots \geq \lambda_n \geq 0
$$

ç‰¹ç•°å€¤ã‚’ $\sigma_i = \sqrt{\lambda_i}$ ã¨å®šç¾©ã™ã‚‹ã€‚$\sigma_i > 0$ ã®å€‹æ•°ãŒ $r = \text{rank}(A)$ã€‚

å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ã¯:

$$
\mathbf{u}_i = \frac{A \mathbf{v}_i}{\sigma_i} \quad (i = 1, \ldots, r)
$$

**æ¤œè¨¼**:

$$
A = U \Sigma V^\top \implies A^\top A = V \Sigma^\top U^\top U \Sigma V^\top = V \Sigma^\top \Sigma V^\top = V \Lambda V^\top \quad \checkmark
$$

åŒæ§˜ã« $AA^\top = U \Lambda' U^\top$ï¼ˆ$\Lambda'$ ã®éã‚¼ãƒ­å¯¾è§’è¦ç´ ã¯ $\Lambda$ ã¨åŒã˜ï¼‰ã€‚

```python
import numpy as np

# Verify SVD via eigendecomposition
A = np.array([[3, 2, 2],
              [2, 3, -2]])

# Method 1: np.linalg.svd
U, s, Vt = np.linalg.svd(A)
print("SVD:")
print(f"  Singular values: {np.round(s, 4)}")

# Method 2: eigendecomposition of A^T A
AtA = A.T @ A
eigenvalues, V_eig = np.linalg.eigh(AtA)
# eigh returns ascending order, reverse for descending
idx = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[idx]
V_eig = V_eig[:, idx]

print(f"\nEigenvalues of A^T A: {np.round(eigenvalues, 4)}")
print(f"Singular values (sqrt): {np.round(np.sqrt(np.maximum(eigenvalues, 0)), 4)}")
print(f"Match: {np.allclose(s, np.sqrt(np.maximum(eigenvalues, 0))[:len(s)])}")
```

#### Compact SVD ã¨ Economy SVD

Full SVDã¯è¨ˆç®—é‡ãŒç„¡é§„ã«ãªã‚‹ã“ã¨ãŒå¤šã„ã€‚å®Ÿç”¨ä¸Šã¯ä»¥ä¸‹ã‚’ä½¿ã†:

| åç§° | å®šç¾© | ã‚µã‚¤ã‚º | ç”¨é€” |
|:-----|:-----|:------|:-----|
| Full SVD | $A = U \Sigma V^\top$ | $U: m \times m, \Sigma: m \times n, V: n \times n$ | ç†è«– |
| Compact SVD | $A = U_r \Sigma_r V_r^\top$ | $U_r: m \times r, \Sigma_r: r \times r, V_r: r \times n$ | $\text{rank}(A) = r \ll \min(m,n)$ |
| Truncated SVD | $A_k = U_k \Sigma_k V_k^\top$ | $U_k: m \times k, \Sigma_k: k \times k, V_k: k \times n$ | ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ |

```python
import numpy as np

A = np.random.randn(100, 50)

# Full SVD
U_full, s_full, Vt_full = np.linalg.svd(A, full_matrices=True)
print(f"Full SVD: U={U_full.shape}, s={s_full.shape}, Vt={Vt_full.shape}")

# Economy SVD (full_matrices=False)
U_econ, s_econ, Vt_econ = np.linalg.svd(A, full_matrices=False)
print(f"Economy SVD: U={U_econ.shape}, s={s_econ.shape}, Vt={Vt_econ.shape}")

# Truncated SVD (rank-k)
k = 10
U_k = U_econ[:, :k]
s_k = s_econ[:k]
Vt_k = Vt_econ[:k, :]
A_k = U_k @ np.diag(s_k) @ Vt_k
print(f"Truncated SVD (k={k}): error = {np.linalg.norm(A - A_k, 'fro'):.4f}")
```

### 3.2 Eckart-Youngå®šç† â€” ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®æœ€é©æ€§

#### å®šç†

**å®šç†** (Eckart-Young-Mirsky[^3]): $A \in \mathbb{R}^{m \times n}$ ã® SVD ã‚’ $A = U \Sigma V^\top$ ã¨ã—ã€$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$ ã‚’ç‰¹ç•°å€¤ã¨ã™ã‚‹ã€‚ä»»æ„ã® rank-$k$ è¡Œåˆ— $B$ ã«å¯¾ã—ã¦:

$$
\min_{\text{rank}(B) \leq k} \|A - B\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}
$$

ã“ã®æœ€å°å€¤ã‚’é”æˆã™ã‚‹ $B$ ã¯:

$$
A_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top = U_k \Sigma_k V_k^\top
$$

ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã«ã¤ã„ã¦ã‚‚:

$$
\min_{\text{rank}(B) \leq k} \|A - B\|_2 = \sigma_{k+1}
$$

#### è¨¼æ˜ã®ã‚¹ã‚±ãƒƒãƒ

$B$ ã‚’ä»»æ„ã® rank-$k$ è¡Œåˆ—ã¨ã™ã‚‹ã€‚$\ker(B)$ ã®æ¬¡å…ƒã¯ $n - k$ ä»¥ä¸Šã€‚ä¸€æ–¹ã€$V_1, \ldots, V_{k+1}$ ãŒå¼µã‚‹éƒ¨åˆ†ç©ºé–“ã¯ $k+1$ æ¬¡å…ƒã€‚æ¬¡å…ƒã®å¼•æ•°ï¼ˆdimension argumentï¼‰ã‚ˆã‚Šã€$\ker(B)$ ã¨ $\text{span}\{V_1, \ldots, V_{k+1}\}$ ã¯éè‡ªæ˜ãªäº¤ã‚ã‚Šã‚’æŒã¤ã€‚

$\mathbf{w} \neq \mathbf{0}$ ã‚’ã“ã®äº¤ã‚ã‚Šã®è¦ç´ ã¨ã™ã‚‹ã¨:

$$
\|A - B\|_F^2 \geq \|(A-B)\mathbf{w}\|^2 / \|\mathbf{w}\|^2 = \|A\mathbf{w}\|^2 / \|\mathbf{w}\|^2
$$

$\mathbf{w} \in \text{span}\{V_1, \ldots, V_{k+1}\}$ ã‚ˆã‚Šã€$\mathbf{w} = \sum_{i=1}^{k+1} c_i \mathbf{v}_i$ ã¨æ›¸ã‘ã‚‹ã€‚

$$
\|A\mathbf{w}\|^2 = \sum_{i=1}^{k+1} c_i^2 \sigma_i^2 \geq \sigma_{k+1}^2 \sum_{i=1}^{k+1} c_i^2 = \sigma_{k+1}^2 \|\mathbf{w}\|^2
$$

ã—ãŸãŒã£ã¦ $\|A - B\|_2 \geq \sigma_{k+1}$ã€‚$A_k$ ãŒã“ã®ä¸‹ç•Œã‚’é”æˆã™ã‚‹ã“ã¨ã¯ç›´æ¥è¨ˆç®—ã§ç¢ºèªã§ãã‚‹ã€‚$\square$

:::message alert
ä¸Šè¨˜ã®è¨¼æ˜ã‚¹ã‚±ãƒƒãƒã¯ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ç‰ˆã§ã™ã€‚ãƒ•ãƒ­ãƒ™ãƒ‹ã‚¦ã‚¹ãƒãƒ«ãƒ ç‰ˆã®æœ€é©æ€§ã¯ $\|A - A_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2$ ã®ç›´æ¥è¨ˆç®—ã§ç¤ºã•ã‚Œã¾ã™ï¼ˆFan-Hoffmanä¸ç­‰å¼ï¼‰ã€‚
:::

```python
import numpy as np

# Verify Eckart-Young theorem
A = np.random.randn(50, 30)
U, s, Vt = np.linalg.svd(A, full_matrices=False)

for k in [1, 3, 5, 10, 20]:
    A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
    error_F = np.linalg.norm(A - A_k, 'fro')
    theoretical = np.sqrt(np.sum(s[k:]**2))
    print(f"rank-{k:2d}: ||A-A_k||_F = {error_F:.6f}, "
          f"theoretical = {theoretical:.6f}, "
          f"match = {np.isclose(error_F, theoretical)}")
```

:::message
**LoRAã¸ã®æ¥ç¶š**: LoRA[^10]ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã®é‡ã¿æ›´æ–° $\Delta W$ ã‚’ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ— $BA$ ã§è¿‘ä¼¼ã™ã‚‹ã€‚$B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times d}$ ã§ $r \ll d$ã€‚Eckart-Youngå®šç†ã¯ã€Œä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã¯æœ€é©ã€ã‚’ä¿è¨¼ã™ã‚‹ãŒã€LoRAã®å ´åˆã¯å­¦ç¿’ã§ $B, A$ ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã€SVDã¨ã¯ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚ã ãŒã€å­¦ç¿’å¾Œã® $\Delta W = BA$ ã‚’SVDã§åˆ†æã™ã‚‹ã¨ã€ç¢ºã‹ã«å°‘æ•°ã®ç‰¹ç•°å€¤ãŒæ”¯é…çš„ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã‚‹ã€‚
:::

### 3.3 ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã®å¿œç”¨ â€” ç”»åƒåœ§ç¸®ãƒ»æ¨è–¦ãƒ»LoRA

#### ç”»åƒåœ§ç¸®

```python
import numpy as np

# Create a test image-like matrix (smooth gradients + structure)
m, n = 200, 150
x = np.linspace(0, 4*np.pi, m)
y = np.linspace(0, 3*np.pi, n)
X, Y = np.meshgrid(y, x)
A = np.sin(X) * np.cos(Y) + 0.5 * np.sin(2*X + Y)  # structured image

U, s, Vt = np.linalg.svd(A, full_matrices=False)

print("Singular value decay:")
for k in [1, 5, 10, 20, 50]:
    A_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]
    rel_error = np.linalg.norm(A - A_k, 'fro') / np.linalg.norm(A, 'fro')
    storage_original = m * n
    storage_compressed = k * (m + n + 1)
    ratio = storage_compressed / storage_original
    print(f"  rank-{k:2d}: error={rel_error:.6f}, "
          f"storage={ratio:.1%} ({storage_compressed}/{storage_original})")
```

#### æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰

ãƒ¦ãƒ¼ã‚¶ãƒ¼Ã—ã‚¢ã‚¤ãƒ†ãƒ ã®è©•ä¾¡è¡Œåˆ— $R$ ã¯å¤§éƒ¨åˆ†ãŒæ¬ æï¼ˆæœªè©•ä¾¡ï¼‰ã€‚ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ $R \approx U_k \Sigma_k V_k^\top$ ã§æ¬ æå€¤ã‚’äºˆæ¸¬ã§ãã‚‹ã€‚

$$
\hat{r}_{ij} = \sum_{l=1}^{k} \sigma_l u_{il} v_{jl}
$$

```python
import numpy as np

# Toy recommendation: 5 users Ã— 4 items
R = np.array([
    [5, 3, 0, 1],
    [4, 0, 0, 1],
    [1, 1, 0, 5],
    [1, 0, 0, 4],
    [0, 1, 5, 4],
], dtype=float)

# Replace 0 (unknown) with mean for SVD
mask = R > 0
R_filled = R.copy()
R_filled[~mask] = np.mean(R[mask])

U, s, Vt = np.linalg.svd(R_filled, full_matrices=False)

# Rank-2 approximation
k = 2
R_approx = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]

print("Original (0 = unknown):")
print(R.astype(int))
print(f"\nRank-{k} approximation (predictions for unknowns):")
print(np.round(R_approx, 1))
print(f"\nPredicted ratings for unknown entries:")
for i, j in zip(*np.where(~mask)):
    print(f"  User {i+1}, Item {j+1}: {R_approx[i,j]:.1f}")
```

### 3.4 æ“¬ä¼¼é€†è¡Œåˆ—ï¼ˆMoore-Penroseï¼‰

#### å®šç¾©

$A \in \mathbb{R}^{m \times n}$ ã® **Moore-Penrose æ“¬ä¼¼é€†è¡Œåˆ—** $A^+ \in \mathbb{R}^{n \times m}$ ã¯ä»¥ä¸‹ã®4æ¡ä»¶ã‚’æº€ãŸã™å”¯ä¸€ã®è¡Œåˆ—:

1. $A A^+ A = A$
2. $A^+ A A^+ = A^+$
3. $(A A^+)^\top = A A^+$
4. $(A^+ A)^\top = A^+ A$

#### SVDã«ã‚ˆã‚‹æ§‹æˆ

$A = U \Sigma V^\top$ ãªã‚‰ã°:

$$
A^+ = V \Sigma^+ U^\top
$$

ã“ã“ã§ $\Sigma^+ = \text{diag}(1/\sigma_1, \ldots, 1/\sigma_r, 0, \ldots, 0)$ã€‚

**ç›´æ„Ÿ**: ç‰¹ç•°å€¤ã®é€†æ•°ã‚’å–ã‚‹ã€‚ãŸã ã— $\sigma_i = 0$ ã®æˆåˆ†ã¯ç„¡è¦–ã™ã‚‹ã€‚

```python
import numpy as np

# Pseudoinverse via SVD
A = np.array([[1, 2],
              [3, 4],
              [5, 6]])

# Method 1: np.linalg.pinv
A_pinv = np.linalg.pinv(A)

# Method 2: manual SVD construction
U, s, Vt = np.linalg.svd(A, full_matrices=False)
S_pinv = np.diag(1.0 / s)
A_pinv_manual = Vt.T @ S_pinv @ U.T

print(f"A (shape {A.shape}):")
print(A)
print(f"\nA+ (shape {A_pinv.shape}):")
print(np.round(A_pinv, 4))
print(f"\nManual matches: {np.allclose(A_pinv, A_pinv_manual)}")

# Verify Moore-Penrose conditions
print(f"\nMoore-Penrose conditions:")
print(f"  A A+ A = A: {np.allclose(A @ A_pinv @ A, A)}")
print(f"  A+ A A+ = A+: {np.allclose(A_pinv @ A @ A_pinv, A_pinv)}")
print(f"  (A A+)^T = A A+: {np.allclose((A @ A_pinv).T, A @ A_pinv)}")
print(f"  (A+ A)^T = A+ A: {np.allclose((A_pinv @ A).T, A_pinv @ A)}")
```

#### æœ€å°äºŒä¹—æ³•ã¨ã®é–¢ä¿‚

éå‰°æ±ºå®šç³» $A\mathbf{x} = \mathbf{b}$ï¼ˆ$m > n$, è§£ãªã—ï¼‰ã®æœ€å°äºŒä¹—è§£ã¯:

$$
\hat{\mathbf{x}} = A^+ \mathbf{b} = V \Sigma^+ U^\top \mathbf{b}
$$

ç¬¬2å›ã®æ­£è¦æ–¹ç¨‹å¼ $A^\top A \hat{\mathbf{x}} = A^\top \mathbf{b}$ ã¨åŒã˜è§£ã‚’ä¸ãˆã‚‹ãŒã€SVDç‰ˆã¯ $A^\top A$ ãŒç‰¹ç•°ãªå ´åˆã§ã‚‚æ•°å€¤çš„ã«å®‰å®šã€‚

#### Tikhonovæ­£å‰‡åŒ–ï¼ˆRidgeå›å¸°ï¼‰

æ¡ä»¶æ•°ãŒå¤§ãã„å ´åˆã€æ“¬ä¼¼é€†è¡Œåˆ—ã¯æ•°å€¤çš„ã«ä¸å®‰å®šã€‚æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $\lambda > 0$ ã‚’åŠ ãˆã‚‹:

$$
\hat{\mathbf{x}}_\lambda = (A^\top A + \lambda I)^{-1} A^\top \mathbf{b} = \sum_{i=1}^{r} \frac{\sigma_i}{\sigma_i^2 + \lambda} \mathbf{v}_i (\mathbf{u}_i^\top \mathbf{b})
$$

$\lambda$ ãŒå¤§ãã„ã»ã©ã€å°ã•ãªç‰¹ç•°å€¤ã®å½±éŸ¿ãŒæŠ‘åˆ¶ã•ã‚Œã‚‹ã€‚ã“ã‚Œã¯**Ridgeå›å¸°**ã¨ç­‰ä¾¡ã€‚

```python
import numpy as np

# Ill-conditioned system
np.random.seed(42)
A = np.random.randn(20, 10)
A[:, -1] = A[:, 0] + 1e-8 * np.random.randn(20)  # nearly collinear
b = np.random.randn(20)

print(f"Condition number: {np.linalg.cond(A):.2e}")

# Pseudoinverse (unstable)
x_pinv = np.linalg.pinv(A) @ b
print(f"||x_pinv|| = {np.linalg.norm(x_pinv):.4f}")

# Tikhonov regularization
for lam in [0.001, 0.01, 0.1, 1.0]:
    x_ridge = np.linalg.solve(A.T @ A + lam * np.eye(10), A.T @ b)
    residual = np.linalg.norm(A @ x_ridge - b)
    print(f"Î»={lam:.3f}: ||x||={np.linalg.norm(x_ridge):.4f}, "
          f"residual={residual:.4f}")
```

### 3.5 PCA ã® SVD ã«ã‚ˆã‚‹å°å‡º

ç¬¬2å›ã§ã¯å›ºæœ‰å€¤åˆ†è§£ã«ã‚ˆã‚‹PCA[^5][^6]ã‚’å°å‡ºã—ãŸã€‚ã“ã“ã§ã¯SVDã«ã‚ˆã‚‹PCAã‚’å°å‡ºã—ã€ä¸¡è€…ã®ç­‰ä¾¡æ€§ã‚’ç¤ºã™ã€‚

#### ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—ã‹ã‚‰ã®å°å‡º

ãƒ‡ãƒ¼ã‚¿è¡Œåˆ— $X \in \mathbb{R}^{n \times d}$ï¼ˆ$n$ ã‚µãƒ³ãƒ—ãƒ«ã€$d$ æ¬¡å…ƒï¼‰ã‚’ä¸­å¿ƒåŒ–ï¼ˆå„åˆ—ã®å¹³å‡ã‚’å¼•ãï¼‰ã—ãŸã‚‚ã®ã‚’ $\tilde{X}$ ã¨ã™ã‚‹ã€‚

å…±åˆ†æ•£è¡Œåˆ—:

$$
C = \frac{1}{n-1} \tilde{X}^\top \tilde{X}
$$

$\tilde{X}$ ã® SVD ã‚’ $\tilde{X} = U \Sigma V^\top$ ã¨ã™ã‚‹ã¨:

$$
C = \frac{1}{n-1} V \Sigma^\top U^\top U \Sigma V^\top = \frac{1}{n-1} V \Sigma^2 V^\top
$$

ã“ã‚Œã¯ $C$ ã®å›ºæœ‰å€¤åˆ†è§£ãã®ã‚‚ã®ã ã€‚ã¤ã¾ã‚Š:
- **PCAã®ä¸»æˆåˆ†æ–¹å‘** = $\tilde{X}$ ã®å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ« $V$ ã®åˆ—
- **PCAã®ä¸»æˆåˆ†ã®åˆ†æ•£** = $\sigma_i^2 / (n-1)$

#### åˆ†æ•£æœ€å¤§åŒ– â†” å†æ§‹æˆèª¤å·®æœ€å°åŒ–ã®ç­‰ä¾¡æ€§

**åˆ†æ•£æœ€å¤§åŒ–**: ç¬¬1ä¸»æˆåˆ†ã¯ $\mathbf{w}_1 = \arg\max_{\|\mathbf{w}\|=1} \text{Var}(\tilde{X}\mathbf{w})$

**å†æ§‹æˆèª¤å·®æœ€å°åŒ–**: rank-$k$ è¿‘ä¼¼ $\hat{X} = \tilde{X} V_k V_k^\top$ ãŒ $\|\tilde{X} - \hat{X}\|_F^2$ ã‚’æœ€å°åŒ–

ã“ã®2ã¤ã¯**ç­‰ä¾¡**:

$$
\|\tilde{X} - \hat{X}\|_F^2 = \|\tilde{X}\|_F^2 - \|\tilde{X} V_k\|_F^2 = \sum_{i=1}^{r} \sigma_i^2 - \sum_{i=1}^{k} \sigma_i^2 = \sum_{i=k+1}^{r} \sigma_i^2
$$

å†æ§‹æˆèª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹ã«ã¯ $\sum_{i=1}^{k} \sigma_i^2$ï¼ˆ= å°„å½±å¾Œã®åˆ†æ•£ã®åˆè¨ˆï¼‰ã‚’æœ€å¤§åŒ–ã™ã‚Œã°ã‚ˆã„ã€‚ã“ã‚Œã¯Eckart-Youngå®šç†[^3]ã®ç›´æ¥çš„ãªå¸°çµã€‚

```python
import numpy as np

# PCA via SVD vs eigendecomposition
np.random.seed(42)
n, d = 200, 5
X = np.random.randn(n, d) @ np.diag([5, 3, 1, 0.5, 0.1])  # structured data

# Center the data
X_centered = X - X.mean(axis=0)

# Method 1: PCA via eigendecomposition of covariance
C = X_centered.T @ X_centered / (n - 1)
eigvals, eigvecs = np.linalg.eigh(C)
idx = np.argsort(eigvals)[::-1]
eigvals = eigvals[idx]
eigvecs = eigvecs[:, idx]

# Method 2: PCA via SVD
U, s, Vt = np.linalg.svd(X_centered, full_matrices=False)
pca_variance = s**2 / (n - 1)

print("PCA via Eigendecomposition vs SVD:")
print(f"  Eigenvalues: {np.round(eigvals, 4)}")
print(f"  s^2/(n-1):   {np.round(pca_variance, 4)}")
print(f"  Match: {np.allclose(eigvals, pca_variance)}")

# Principal components
PC_eig = X_centered @ eigvecs[:, :2]  # project onto top-2
PC_svd = U[:, :2] * s[:2]             # equivalent via SVD
print(f"\nPrincipal components match: {np.allclose(np.abs(PC_eig), np.abs(PC_svd))}")

# Explained variance ratio
total_var = np.sum(pca_variance)
for k in range(1, 6):
    ratio = np.sum(pca_variance[:k]) / total_var
    print(f"  Top-{k}: {ratio:.4f} ({ratio*100:.1f}%)")
```

### 3.6 ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã¨ Einsteinè¨˜æ³•

#### ãƒ†ãƒ³ã‚½ãƒ«ã¨ã¯

ãƒ†ãƒ³ã‚½ãƒ«ã¯å¤šæ¬¡å…ƒé…åˆ—ã®æ•°å­¦çš„ãªä¸€èˆ¬åŒ–ã€‚æ©Ÿæ¢°å­¦ç¿’ã§ã¯ã€Œå¤šæ¬¡å…ƒé…åˆ—ã€ã¨åŒç¾©ã§ä½¿ã†ã“ã¨ãŒå¤šã„ã€‚

| éšæ•° | æ•°å­¦çš„åç§° | ä¾‹ | NumPy |
|:-----|:---------|:---|:------|
| 0 | ã‚¹ã‚«ãƒ©ãƒ¼ | æå¤±å€¤ $L$ | `np.float64` |
| 1 | ãƒ™ã‚¯ãƒˆãƒ« | åŸ‹ã‚è¾¼ã¿ $\mathbf{e} \in \mathbb{R}^d$ | `shape=(d,)` |
| 2 | è¡Œåˆ— | é‡ã¿ $W \in \mathbb{R}^{m \times n}$ | `shape=(m, n)` |
| 3 | 3éšãƒ†ãƒ³ã‚½ãƒ« | ãƒãƒƒãƒå…¥åŠ› $X \in \mathbb{R}^{B \times T \times d}$ | `shape=(B, T, d)` |
| 4 | 4éšãƒ†ãƒ³ã‚½ãƒ« | Multi-Head Attention $\in \mathbb{R}^{B \times H \times T \times T}$ | `shape=(B, H, T, T)` |

#### Kroneckerç©

è¡Œåˆ—å¾®åˆ†ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹éš›ã«ä¸å¯æ¬ ãªé“å…·ã¨ã—ã¦ã€Kroneckerç©ã‚’å°å…¥ã—ã¾ã™ã€‚

è¡Œåˆ— $A \in \mathbb{R}^{m \times n}$, $B \in \mathbb{R}^{p \times q}$ ã® **Kroneckerç©**:

$$
A \otimes B = \begin{pmatrix} a_{11}B & \cdots & a_{1n}B \\ \vdots & \ddots & \vdots \\ a_{m1}B & \cdots & a_{mn}B \end{pmatrix} \in \mathbb{R}^{mp \times nq}
$$

é‡è¦ãªæ€§è³ª:
- $(A \otimes B)(C \otimes D) = (AC) \otimes (BD)$
- $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$
- $\text{vec}(AXB) = (B^\top \otimes A) \text{vec}(X)$

æœ€å¾Œã®æ€§è³ªã¯è¡Œåˆ—æ–¹ç¨‹å¼ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ä¸å¯æ¬ :

```python
import numpy as np

A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Kronecker product
K = np.kron(A, B)
print(f"A âŠ— B (shape {K.shape}):")
print(K)

# vec(AXB) = (B^T âŠ— A) vec(X)
X = np.array([[1, 0], [0, 1]])
AXB = A @ X @ B
vec_AXB = AXB.flatten('F')  # column-major vectorization
kron_vec = np.kron(B.T, A) @ X.flatten('F')
print(f"\nvec(AXB) = {vec_AXB}")
print(f"(B^T âŠ— A)vec(X) = {kron_vec}")
print(f"Match: {np.allclose(vec_AXB, kron_vec)}")
```

#### Einsteinè¨˜æ³•ï¼ˆå®Œå…¨ç‰ˆï¼‰

Einsteinè¨˜æ³•ã¯ã€ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã‚’æ·»å­—ã®è¦å‰‡ã ã‘ã§è¨˜è¿°ã™ã‚‹å¼·åŠ›ãªè¨˜æ³•ã€‚NumPyã® `einsum` ã¯ã“ã®è¨˜æ³•ã‚’ç›´æ¥å®Ÿè£…ã—ã¦ã„ã‚‹ã€‚

**è¦å‰‡**: ç¹°ã‚Šè¿”ã•ã‚Œã‚‹æ·»å­—ã¯**æš—é»™ã«ç·å’Œ**ã•ã‚Œã‚‹ï¼ˆç¸®ç´„ï¼‰ã€‚

| æ¼”ç®— | æ•°å¼ | einsum | èª¬æ˜ |
|:-----|:-----|:-------|:-----|
| å†…ç© | $c = \sum_i a_i b_i$ | `'i,i->'` | ãƒ™ã‚¯ãƒˆãƒ«å†…ç© |
| å¤–ç© | $C_{ij} = a_i b_j$ | `'i,j->ij'` | ãƒ©ãƒ³ã‚¯1è¡Œåˆ— |
| è¡Œåˆ—ç© | $C_{ij} = \sum_k A_{ik} B_{kj}$ | `'ik,kj->ij'` | æ¨™æº–çš„ãªè¡Œåˆ—ç© |
| è¡Œåˆ—ã®ãƒˆãƒ¬ãƒ¼ã‚¹ | $t = \sum_i A_{ii}$ | `'ii->'` | å¯¾è§’è¦ç´ ã®å’Œ |
| è»¢ç½® | $B_{ji} = A_{ij}$ | `'ij->ji'` | è¡Œåˆ—ã®è»¢ç½® |
| å¯¾è§’æŠ½å‡º | $d_i = A_{ii}$ | `'ii->i'` | å¯¾è§’æˆåˆ† |
| ãƒãƒƒãƒè¡Œåˆ—ç© | $C_{bij} = \sum_k A_{bik} B_{bkj}$ | `'bik,bkj->bij'` | ãƒãƒƒãƒå‡¦ç† |
| Multi-Head Attention | $S_{bhij} = \sum_k Q_{bhik} K_{bhjk}$ | `'bhik,bhjk->bhij'` | $QK^\top$ per head |
| äºŒé‡ç¸®ç´„ | $s = \sum_{ij} A_{ij} B_{ij}$ | `'ij,ij->'` | Frobeniuså†…ç© |
| ãƒ†ãƒ³ã‚½ãƒ«ç¸®ç´„ | $C_{ik} = \sum_j A_{ij} B_{jk}$ | `'ij,jk->ik'` | ä¸€èˆ¬ç¸®ç´„ |

```python
import numpy as np

# einsum examples
A = np.random.randn(3, 4)
B = np.random.randn(4, 5)
v = np.random.randn(4)

# Matrix multiplication
C1 = A @ B
C2 = np.einsum('ik,kj->ij', A, B)
print(f"Matrix mul match: {np.allclose(C1, C2)}")

# Trace
t1 = np.trace(A[:3, :3])
# need square submatrix for trace
A_sq = np.random.randn(4, 4)
t1 = np.trace(A_sq)
t2 = np.einsum('ii->', A_sq)
print(f"Trace match: {np.allclose(t1, t2)}")

# Batch matrix multiplication (Attention-style)
B_size, H, T, d = 2, 4, 8, 16
Q = np.random.randn(B_size, H, T, d)
K = np.random.randn(B_size, H, T, d)

# QK^T per head
scores1 = Q @ K.transpose(0, 1, 3, 2)  # using @ and transpose
scores2 = np.einsum('bhik,bhjk->bhij', Q, K)  # using einsum
print(f"Batch attention match: {np.allclose(scores1, scores2)}")
print(f"Scores shape: {scores1.shape}")  # (2, 4, 8, 8)
```

:::details einsum ã®è¨ˆç®—ã‚°ãƒ©ãƒ•ã¨æœ€é©åŒ–
`np.einsum` ã¯æ·»å­—ã®ç¸®ç´„é †åºã‚’æœ€é©åŒ–ã§ãã‚‹ã€‚`optimize=True` ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«ã®ã‚µã‚¤ã‚ºã‚’æœ€å°åŒ–ã™ã‚‹ç¸®ç´„é †åºã‚’è‡ªå‹•çš„ã«é¸æŠã™ã‚‹ã€‚

```python
import numpy as np

# Three-tensor contraction: different orders have different costs
A = np.random.randn(100, 50)
B = np.random.randn(50, 200)
C = np.random.randn(200, 100)

# Without optimization: may choose suboptimal contraction order
result1 = np.einsum('ij,jk,kl->il', A, B, C, optimize=False)

# With optimization: chooses optimal contraction order
result2 = np.einsum('ij,jk,kl->il', A, B, C, optimize=True)
print(f"Results match: {np.allclose(result1, result2)}")

# Check optimal contraction path
path, info = np.einsum_path('ij,jk,kl->il', A, B, C, optimize='optimal')
print(f"Optimal path: {path}")
print(info)
```
:::

### 3.7 å¤šå¤‰æ•°å¾®åˆ† â€” å‹¾é…ãƒ»ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãƒ»ãƒ˜ã‚·ã‚¢ãƒ³

#### å‹¾é…ï¼ˆGradientï¼‰

ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•° $f: \mathbb{R}^n \to \mathbb{R}$ ã®**å‹¾é…**:

$$
\nabla f(\mathbf{x}) = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix} \in \mathbb{R}^n
$$

å‹¾é…ã¯ $f$ ãŒæœ€ã‚‚æ€¥ã«å¢—åŠ ã™ã‚‹æ–¹å‘ã‚’æŒ‡ã™ã€‚$-\nabla f$ ãŒæœ€æ€¥é™ä¸‹æ–¹å‘ã€‚

#### ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ï¼ˆJacobianï¼‰

ãƒ™ã‚¯ãƒˆãƒ«é–¢æ•° $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ ã®**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³**:

$$
J = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \in \mathbb{R}^{m \times n}, \quad J_{ij} = \frac{\partial f_i}{\partial x_j}
$$

ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å„è¡Œã¯ $f_i$ ã®å‹¾é… $\nabla f_i^\top$ã€‚$m = 1$ ã®ã¨ãã€ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã¯å‹¾é…ã®è»¢ç½® $\nabla f^\top$ã€‚

**å¹¾ä½•å­¦çš„æ„å‘³**: $\mathbf{x}$ ã®è¿‘å‚ã§ã€$\mathbf{f}(\mathbf{x} + \boldsymbol{\delta}) \approx \mathbf{f}(\mathbf{x}) + J \boldsymbol{\delta}$ï¼ˆç·šå½¢è¿‘ä¼¼ï¼‰ã€‚

**ä½“ç©å¤‰åŒ–**: $\det(J)$ ã¯å¤‰æ› $\mathbf{f}$ ã«ã‚ˆã‚‹å±€æ‰€çš„ãªä½“ç©ã®æ‹¡å¤§ç‡ã€‚Normalizing Flow[^13]ã§ã¯:

$$
p_Y(\mathbf{y}) = p_X(\mathbf{f}^{-1}(\mathbf{y})) \cdot |\det(J_{\mathbf{f}^{-1}}(\mathbf{y}))|
$$

#### ãƒ˜ã‚·ã‚¢ãƒ³ï¼ˆHessianï¼‰

ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•° $f: \mathbb{R}^n \to \mathbb{R}$ ã®**ãƒ˜ã‚·ã‚¢ãƒ³**:

$$
H = \nabla^2 f(\mathbf{x}) \in \mathbb{R}^{n \times n}, \quad H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
$$

ãƒ˜ã‚·ã‚¢ãƒ³ã¯å¯¾ç§°è¡Œåˆ—ï¼ˆ$\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$ã€Schwarzã®å®šç†ï¼‰ã€‚

| ãƒ˜ã‚·ã‚¢ãƒ³ã®æ€§è³ª | æ„å‘³ |
|:-------------|:-----|
| $H \succ 0$ï¼ˆæ­£å®šå€¤ï¼‰ | $\mathbf{x}$ ã¯æ¥µå°ç‚¹ |
| $H \prec 0$ï¼ˆè² å®šå€¤ï¼‰ | $\mathbf{x}$ ã¯æ¥µå¤§ç‚¹ |
| $H$ ãŒä¸å®š | $\mathbf{x}$ ã¯éç‚¹ï¼ˆsaddle pointï¼‰ |

```python
import numpy as np

# Example: f(x, y) = x^2 + 3*y^2 + 2*x*y
# Gradient: [2x + 2y, 6y + 2x]
# Hessian: [[2, 2], [2, 6]]

def f(xy):
    x, y = xy
    return x**2 + 3*y**2 + 2*x*y

def grad_f(xy):
    x, y = xy
    return np.array([2*x + 2*y, 6*y + 2*x])

H = np.array([[2, 2], [2, 6]])  # constant Hessian

# Check positive definiteness
eigvals = np.linalg.eigvalsh(H)
print(f"Hessian eigenvalues: {eigvals}")
print(f"Positive definite: {np.all(eigvals > 0)}")  # True â†’ minimum exists

# Find minimum: grad = 0 â†’ x=0, y=0
x_min = np.array([0.0, 0.0])
print(f"Minimum at: {x_min}, f = {f(x_min)}")

# Newton's method: x_new = x - H^{-1} grad(x)
x = np.array([5.0, 3.0])
for i in range(5):
    g = grad_f(x)
    x = x - np.linalg.solve(H, g)
    print(f"Step {i+1}: x = {np.round(x, 6)}, f = {f(x):.6f}")
```

### 3.8 è¡Œåˆ—å¾®åˆ†ï¼ˆMatrix Calculusï¼‰

#### åŸºæœ¬çš„ãªå¾®åˆ†å…¬å¼

ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•° $L$ ã®è¡Œåˆ— $W \in \mathbb{R}^{m \times n}$ ã«é–¢ã™ã‚‹å¾®åˆ†:

$$
\frac{\partial L}{\partial W} \in \mathbb{R}^{m \times n}, \quad \left(\frac{\partial L}{\partial W}\right)_{ij} = \frac{\partial L}{\partial W_{ij}}
$$

**Matrix Cookbook[^9] ä¸»è¦å…¬å¼15é¸**:

| # | å…¬å¼ | æ¡ä»¶ |
|:--|:-----|:-----|
| 1 | $\frac{\partial}{\partial \mathbf{x}} (\mathbf{a}^\top \mathbf{x}) = \mathbf{a}$ | |
| 2 | $\frac{\partial}{\partial \mathbf{x}} (\mathbf{x}^\top A \mathbf{x}) = (A + A^\top) \mathbf{x}$ | |
| 3 | $\frac{\partial}{\partial \mathbf{x}} (\mathbf{x}^\top A \mathbf{x}) = 2A\mathbf{x}$ | $A$ å¯¾ç§° |
| 4 | $\frac{\partial}{\partial X} \text{tr}(AX) = A^\top$ | |
| 5 | $\frac{\partial}{\partial X} \text{tr}(X^\top A) = A$ | |
| 6 | $\frac{\partial}{\partial X} \text{tr}(AXB) = A^\top B^\top$ | |
| 7 | $\frac{\partial}{\partial X} \text{tr}(X^\top AX) = (A + A^\top)X$ | |
| 8 | $\frac{\partial}{\partial X} \|X\|_F^2 = 2X$ | |
| 9 | $\frac{\partial}{\partial X} \ln \det(X) = X^{-\top}$ | $X$ æ­£å‰‡ |
| 10 | $\frac{\partial}{\partial X} \det(X) = \det(X) X^{-\top}$ | $X$ æ­£å‰‡ |
| 11 | $\frac{\partial}{\partial \mathbf{x}} \|\mathbf{x}\|^2 = 2\mathbf{x}$ | |
| 12 | $\frac{\partial}{\partial \mathbf{x}} (A\mathbf{x} - \mathbf{b})^\top (A\mathbf{x} - \mathbf{b}) = 2A^\top(A\mathbf{x} - \mathbf{b})$ | |
| 13 | $\frac{\partial}{\partial A} \text{tr}(A^{-1}B) = -(A^{-1}BA^{-1})^\top$ | $A$ æ­£å‰‡ |
| 14 | $\frac{\partial}{\partial \mathbf{x}} \sigma(\mathbf{x}) = \sigma(\mathbf{x}) \odot (1 - \sigma(\mathbf{x}))$ | $\sigma$ = sigmoid |
| 15 | $\frac{\partial}{\partial \mathbf{x}} \text{softmax}(\mathbf{x})_i = s_i(\delta_{ij} - s_j)$ | $s = \text{softmax}(\mathbf{x})$ |

```python
import numpy as np

# Verify formula 3: d/dx (x^T A x) = 2Ax for symmetric A
def verify_matrix_derivative(A, x, eps=1e-7):
    n = len(x)
    # Analytical gradient
    grad_analytical = 2 * A @ x

    # Numerical gradient
    grad_numerical = np.zeros(n)
    for i in range(n):
        x_plus = x.copy()
        x_plus[i] += eps
        x_minus = x.copy()
        x_minus[i] -= eps
        grad_numerical[i] = (x_plus @ A @ x_plus - x_minus @ A @ x_minus) / (2 * eps)

    return grad_analytical, grad_numerical

A = np.array([[2, 1], [1, 3]], dtype=float)  # symmetric
x = np.array([1.0, 2.0])

grad_a, grad_n = verify_matrix_derivative(A, x)
print(f"Analytical: {grad_a}")
print(f"Numerical:  {np.round(grad_n, 6)}")
print(f"Match: {np.allclose(grad_a, grad_n)}")

# Verify formula 9: d/dX ln det(X) = X^{-T}
X = np.array([[2.0, 0.5], [0.5, 3.0]])
grad_analytical = np.linalg.inv(X).T
print(f"\nd/dX ln det(X) = X^{{-T}}:")
print(f"  Analytical:\n{np.round(grad_analytical, 4)}")

# Numerical verification
eps = 1e-7
grad_numerical = np.zeros_like(X)
for i in range(2):
    for j in range(2):
        X_plus = X.copy()
        X_plus[i, j] += eps
        X_minus = X.copy()
        X_minus[i, j] -= eps
        grad_numerical[i, j] = (np.log(np.linalg.det(X_plus)) -
                                  np.log(np.linalg.det(X_minus))) / (2 * eps)
print(f"  Numerical:\n{np.round(grad_numerical, 4)}")
print(f"  Match: {np.allclose(grad_analytical, grad_numerical)}")
```

### 3.9 é€£é–å¾‹ â€” Backpropagationã®æ•°å­¦çš„åŸºç›¤

#### ã‚¹ã‚«ãƒ©ãƒ¼ã®é€£é–å¾‹

$y = f(g(x))$ ã®ã¨ã:

$$
\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}
$$

#### ãƒ™ã‚¯ãƒˆãƒ«ã®é€£é–å¾‹

$\mathbf{y} = \mathbf{f}(\mathbf{g}(\mathbf{x}))$ã€$\mathbf{g}: \mathbb{R}^n \to \mathbb{R}^p$ã€$\mathbf{f}: \mathbb{R}^p \to \mathbb{R}^m$ ã®ã¨ã:

$$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \frac{\partial \mathbf{f}}{\partial \mathbf{g}} \cdot \frac{\partial \mathbf{g}}{\partial \mathbf{x}} = J_{\mathbf{f}} J_{\mathbf{g}} \in \mathbb{R}^{m \times n}
$$

**ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®ç©**ã€‚ã“ã‚ŒãŒé€£é–å¾‹ã®è¡Œåˆ—ç‰ˆã€‚

#### å¤šå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¸ã®é©ç”¨

$L$ å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯:

$$
\mathbf{h}_0 = \mathbf{x}, \quad \mathbf{h}_l = f_l(W_l \mathbf{h}_{l-1} + \mathbf{b}_l), \quad L = \ell(\mathbf{h}_L, \mathbf{y})
$$

æå¤± $L$ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $W_l$ ã«é–¢ã™ã‚‹å‹¾é…:

$$
\frac{\partial L}{\partial W_l} = \frac{\partial L}{\partial \mathbf{h}_L} \cdot \frac{\partial \mathbf{h}_L}{\partial \mathbf{h}_{L-1}} \cdots \frac{\partial \mathbf{h}_{l+1}}{\partial \mathbf{h}_l} \cdot \frac{\partial \mathbf{h}_l}{\partial W_l}
$$

```mermaid
graph LR
    X["x"] --> H1["h_1 = f(W_1 x + b_1)"]
    H1 --> H2["h_2 = f(W_2 h_1 + b_2)"]
    H2 --> HL["h_L = f(W_L h_{L-1} + b_L)"]
    HL --> LOSS["L = â„“(h_L, y)"]

    LOSS -->|"âˆ‚L/âˆ‚h_L"| HL
    HL -->|"âˆ‚h_L/âˆ‚h_{L-1}"| H2
    H2 -->|"âˆ‚h_2/âˆ‚h_1"| H1
    H1 -->|"âˆ‚h_1/âˆ‚x"| X

    style LOSS fill:#ffcdd2
    style X fill:#e3f2fd
```

**Forward pass**: $\mathbf{x} \to \mathbf{h}_1 \to \cdots \to \mathbf{h}_L \to L$ï¼ˆå·¦â†’å³ï¼‰

**Backward pass**: $\frac{\partial L}{\partial \mathbf{h}_L} \to \frac{\partial L}{\partial \mathbf{h}_{L-1}} \to \cdots \to \frac{\partial L}{\partial W_l}$ï¼ˆå³â†’å·¦ï¼‰

#### Backpropagation ã®å®Œå…¨å°å‡º

1å±¤ã®ç·šå½¢å¤‰æ› + æ´»æ€§åŒ–: $\mathbf{h}_l = \sigma(\mathbf{z}_l)$, $\mathbf{z}_l = W_l \mathbf{h}_{l-1} + \mathbf{b}_l$

**èª¤å·®ä¿¡å·** $\boldsymbol{\delta}_l = \frac{\partial L}{\partial \mathbf{z}_l}$ ã‚’å®šç¾©ã™ã‚‹ã€‚

å‡ºåŠ›å±¤ ($l = L$):

$$
\boldsymbol{\delta}_L = \frac{\partial L}{\partial \mathbf{z}_L} = \frac{\partial L}{\partial \mathbf{h}_L} \odot \sigma'(\mathbf{z}_L)
$$

éš ã‚Œå±¤ ($l < L$ã€é€†ä¼æ’­ã®æœ¬ä½“ï¼‰:

$$
\boldsymbol{\delta}_l = (W_{l+1}^\top \boldsymbol{\delta}_{l+1}) \odot \sigma'(\mathbf{z}_l)
$$

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…:

$$
\frac{\partial L}{\partial W_l} = \boldsymbol{\delta}_l \mathbf{h}_{l-1}^\top, \quad \frac{\partial L}{\partial \mathbf{b}_l} = \boldsymbol{\delta}_l
$$

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

def sigmoid_deriv(x):
    s = sigmoid(x)
    return s * (1 - s)

# Simple 3-layer network: 3 -> 4 -> 2 -> 1
np.random.seed(42)
W1 = np.random.randn(4, 3) * 0.5
b1 = np.zeros(4)
W2 = np.random.randn(2, 4) * 0.5
b2 = np.zeros(2)
W3 = np.random.randn(1, 2) * 0.5
b3 = np.zeros(1)

# Input and target
x = np.array([1.0, 0.5, -0.5])
y = np.array([1.0])

# === Forward pass ===
z1 = W1 @ x + b1
h1 = sigmoid(z1)
z2 = W2 @ h1 + b2
h2 = sigmoid(z2)
z3 = W3 @ h2 + b3
h3 = sigmoid(z3)
loss = 0.5 * np.sum((h3 - y)**2)
print(f"Forward: loss = {loss:.6f}")

# === Backward pass (manual backpropagation) ===
# Output layer
dL_dh3 = h3 - y                       # dL/dh3
delta3 = dL_dh3 * sigmoid_deriv(z3)   # delta_3

# Hidden layer 2
delta2 = (W3.T @ delta3) * sigmoid_deriv(z2)

# Hidden layer 1
delta1 = (W2.T @ delta2) * sigmoid_deriv(z1)

# Parameter gradients
dL_dW3 = np.outer(delta3, h2)
dL_db3 = delta3
dL_dW2 = np.outer(delta2, h1)
dL_db2 = delta2
dL_dW1 = np.outer(delta1, x)
dL_db1 = delta1

print(f"\nGradients:")
print(f"  dL/dW3 shape: {dL_dW3.shape}, norm: {np.linalg.norm(dL_dW3):.6f}")
print(f"  dL/dW2 shape: {dL_dW2.shape}, norm: {np.linalg.norm(dL_dW2):.6f}")
print(f"  dL/dW1 shape: {dL_dW1.shape}, norm: {np.linalg.norm(dL_dW1):.6f}")

# === Numerical verification ===
def compute_loss(W1, b1, W2, b2, W3, b3, x, y):
    h1 = sigmoid(W1 @ x + b1)
    h2 = sigmoid(W2 @ h1 + b2)
    h3 = sigmoid(W3 @ h2 + b3)
    return 0.5 * np.sum((h3 - y)**2)

# Verify dL/dW1[0,0]
eps = 1e-7
W1_plus = W1.copy()
W1_plus[0, 0] += eps
W1_minus = W1.copy()
W1_minus[0, 0] -= eps
numerical = (compute_loss(W1_plus, b1, W2, b2, W3, b3, x, y) -
             compute_loss(W1_minus, b1, W2, b2, W3, b3, x, y)) / (2 * eps)
print(f"\nNumerical check dL/dW1[0,0]:")
print(f"  Analytical: {dL_dW1[0,0]:.8f}")
print(f"  Numerical:  {numerical:.8f}")
print(f"  Match: {np.isclose(dL_dW1[0,0], numerical, rtol=1e-4)}")
```

:::message
**ã“ã‚ŒãŒBackpropagation[^2]ã®å…¨ã¦ã ã€‚** ã€Œé€£é–å¾‹ã§ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã‚’é€†é †ã«æ›ã‘ã¦ã€å„å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ã€â€” ã“ã®ä¸€æ–‡ã«å…¨ã¦ãŒå‡ç¸®ã•ã‚Œã¦ã„ã‚‹ã€‚1986å¹´ã«Rumelhart, Hinton, WilliamsãŒç™ºè¡¨ã—ãŸã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒã€æ·±å±¤å­¦ç¿’ã®è¨ˆç®—çš„åŸºç›¤ã‚’ç¯‰ã„ãŸã€‚
:::

### 3.10 è‡ªå‹•å¾®åˆ†ã®ç†è«– â€” Forward Mode ã¨ Reverse Mode

è‡ªå‹•å¾®åˆ†ï¼ˆAutomatic Differentiation, ADï¼‰[^7][^8]ã¯ã€æ•°å€¤å¾®åˆ†ã§ã‚‚è¨˜å·å¾®åˆ†ã§ã‚‚ãªã„ã€ç¬¬3ã®å¾®åˆ†æ³•ã ã€‚

#### 3ã¤ã®å¾®åˆ†æ³•ã®æ¯”è¼ƒ

| æ–¹æ³• | ç²¾åº¦ | è¨ˆç®—é‡ | é•·æ‰€ | çŸ­æ‰€ |
|:-----|:-----|:------|:-----|:-----|
| æ•°å€¤å¾®åˆ† | $O(\epsilon)$ èª¤å·® | $O(n)$ å›ã®é–¢æ•°è©•ä¾¡ | å®Ÿè£…ãŒç°¡å˜ | é…ã„ã€ä¸æ­£ç¢º |
| è¨˜å·å¾®åˆ† | å³å¯† | å¼è†¨å¼µï¼ˆexpression swellï¼‰ | æ•°å­¦çš„ã«æ­£ç¢º | å¼ãŒå·¨å¤§ã« |
| è‡ªå‹•å¾®åˆ† | æ©Ÿæ¢°ç²¾åº¦ | $O(1)$ å€ï¼ˆreverse modeï¼‰ | é€Ÿã„ã€æ­£ç¢º | å®Ÿè£…ãŒè¤‡é›‘ |

#### Wengert Listï¼ˆè¨ˆç®—ãƒˆãƒ¬ãƒ¼ã‚¹ï¼‰

è‡ªå‹•å¾®åˆ†ã®æ ¸å¿ƒã¯ã€è¨ˆç®—ã‚’ãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–æ“ä½œã®åˆ—ï¼ˆWengert listï¼‰ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹ã“ã¨ã€‚

ä¾‹: $f(x_1, x_2) = x_1 x_2 + \sin(x_1)$

| Step | æ¼”ç®— | å€¤ ($x_1=2, x_2=3$) |
|:-----|:-----|:---------------------|
| $v_1 = x_1$ | å…¥åŠ› | $2$ |
| $v_2 = x_2$ | å…¥åŠ› | $3$ |
| $v_3 = v_1 \cdot v_2$ | ä¹—ç®— | $6$ |
| $v_4 = \sin(v_1)$ | sin | $0.9093$ |
| $v_5 = v_3 + v_4$ | åŠ ç®— | $6.9093$ |

#### Forward Mode AD

å…¥åŠ›ã«å¯¾ã™ã‚‹å¾®åˆ† $\dot{v}_i = \frac{\partial v_i}{\partial x_j}$ ã‚’**å‰å‘ã**ã«ä¼æ’­:

| Step | å€¤ | $\dot{v}_i = \partial v_i / \partial x_1$ |
|:-----|:---|:----------------------------------------|
| $v_1 = x_1$ | $2$ | $\dot{v}_1 = 1$ |
| $v_2 = x_2$ | $3$ | $\dot{v}_2 = 0$ |
| $v_3 = v_1 v_2$ | $6$ | $\dot{v}_3 = \dot{v}_1 v_2 + v_1 \dot{v}_2 = 3$ |
| $v_4 = \sin(v_1)$ | $0.909$ | $\dot{v}_4 = \cos(v_1) \dot{v}_1 = -0.416$ |
| $v_5 = v_3 + v_4$ | $6.909$ | $\dot{v}_5 = \dot{v}_3 + \dot{v}_4 = 2.584$ |

$\frac{\partial f}{\partial x_1} = 2.584$ã€‚æ­£ã—ã„ï¼ˆ$\frac{\partial}{\partial x_1}(x_1 x_2 + \sin x_1) = x_2 + \cos x_1 = 3 + \cos 2 = 2.584$ï¼‰ã€‚

**è¨ˆç®—é‡**: 1å›ã® Forward Mode ã§ã€1ã¤ã®å…¥åŠ›å¤‰æ•°ã«å¯¾ã™ã‚‹å¾®åˆ†ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚$n$ å€‹ã®å…¥åŠ›å¤‰æ•°ã®å‹¾é…ã‚’æ±‚ã‚ã‚‹ã«ã¯ $n$ å›ã® Forward pass ãŒå¿…è¦ã€‚

#### Reverse Mode ADï¼ˆ= Backpropagationï¼‰

å‡ºåŠ›ã«å¯¾ã™ã‚‹å¾®åˆ† $\bar{v}_i = \frac{\partial f}{\partial v_i}$ ã‚’**é€†å‘ã**ã«ä¼æ’­:

| Step (é€†é †) | $\bar{v}_i = \partial f / \partial v_i$ |
|:-----------|:---------------------------------------|
| $\bar{v}_5 = 1$ | å‡ºåŠ›ã«å¯¾ã™ã‚‹å¾®åˆ†ã¯1 |
| $\bar{v}_3 = \bar{v}_5 \cdot 1 = 1$ | $v_5 = v_3 + v_4$ ã® $v_3$ ã«å¯¾ã™ã‚‹åå¾®åˆ† |
| $\bar{v}_4 = \bar{v}_5 \cdot 1 = 1$ | $v_5 = v_3 + v_4$ ã® $v_4$ ã«å¯¾ã™ã‚‹åå¾®åˆ† |
| $\bar{v}_1 = \bar{v}_3 \cdot v_2 + \bar{v}_4 \cdot \cos(v_1) = 2.584$ | ç©ã®è¦å‰‡ + sinå¾®åˆ† |
| $\bar{v}_2 = \bar{v}_3 \cdot v_1 = 2$ | |

$\frac{\partial f}{\partial x_1} = 2.584$, $\frac{\partial f}{\partial x_2} = 2.0$ã€‚**1å›ã® Reverse pass ã§å…¨å…¥åŠ›å¤‰æ•°ã®å‹¾é…ãŒå¾—ã‚‰ã‚Œã‚‹**ã€‚

#### Forward vs Reverse: è¨ˆç®—é‡ã®æ¯”è¼ƒ

| | Forward Mode | Reverse Mode |
|:--|:------------|:------------|
| 1å›ã®passã§å¾—ã‚‰ã‚Œã‚‹ | 1ã¤ã®å…¥åŠ›ã«å¯¾ã™ã‚‹å‹¾é… | 1ã¤ã®å‡ºåŠ›ã«å¯¾ã™ã‚‹å…¨å…¥åŠ›ã®å‹¾é… |
| $n$ å…¥åŠ›, $m$ å‡ºåŠ›ã®å‹¾é… | $n$ å›ã®pass | $m$ å›ã®pass |
| æœ€é©ãªå ´åˆ | $n \ll m$ï¼ˆãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒã€Œæ¨ªé•·ã€ï¼‰ | $m \ll n$ï¼ˆãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒã€Œç¸¦é•·ã€ï¼‰ |
| æ©Ÿæ¢°å­¦ç¿’ã§ã®å…¸å‹ | â€” | $m = 1$ï¼ˆæå¤±ã¯ã‚¹ã‚«ãƒ©ãƒ¼ï¼‰â†’ **1å›ã®passã§å…¨å‹¾é…** |

**ã ã‹ã‚‰Backpropã¯Reverse Mode ADãªã®ã ã€‚** æå¤±é–¢æ•°ã¯ã‚¹ã‚«ãƒ©ãƒ¼å€¤ï¼ˆ$m = 1$ï¼‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯æ•°åå„„ï¼ˆ$n \sim 10^9$ï¼‰ã€‚Reverse modeãªã‚‰1å›ã®backward passã§å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚Forward modeã§ã¯ $10^9$ å›ã®forward passãŒå¿…è¦ã€‚

```python
import numpy as np

# Implementing Forward Mode AD with dual numbers
class Dual:
    """Dual number: a + bÎµ where Îµ^2 = 0"""
    def __init__(self, val, deriv=0.0):
        self.val = val      # primal value
        self.deriv = deriv   # tangent (derivative)

    def __add__(self, other):
        other = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val + other.val, self.deriv + other.deriv)

    def __radd__(self, other):
        return self.__add__(other)

    def __mul__(self, other):
        other = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.val * other.val,
                    self.val * other.deriv + self.deriv * other.val)

    def __rmul__(self, other):
        return self.__mul__(other)

    def __repr__(self):
        return f"Dual({self.val:.4f}, {self.deriv:.4f})"

def sin_dual(x):
    return Dual(np.sin(x.val), np.cos(x.val) * x.deriv)

# f(x1, x2) = x1*x2 + sin(x1)
def f_dual(x1, x2):
    return x1 * x2 + sin_dual(x1)

# df/dx1 at (2, 3): set x1.deriv = 1
x1 = Dual(2.0, 1.0)  # seed: dx1/dx1 = 1
x2 = Dual(3.0, 0.0)  # seed: dx2/dx1 = 0
result = f_dual(x1, x2)
print(f"f(2, 3) = {result.val:.4f}")
print(f"df/dx1  = {result.deriv:.4f}")
print(f"Expected: {3 + np.cos(2):.4f}")

# df/dx2 at (2, 3): set x2.deriv = 1
x1 = Dual(2.0, 0.0)
x2 = Dual(3.0, 1.0)
result = f_dual(x1, x2)
print(f"df/dx2  = {result.deriv:.4f}")
print(f"Expected: {2.0:.4f}")
```

### 3.11 ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã¨äºŒæ¬¡è¿‘ä¼¼

å¤šå¤‰æ•°ã®ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã¯ã€æœ€é©åŒ–ç†è«–ã®åŸºç›¤:

$$
f(\mathbf{x} + \boldsymbol{\delta}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \boldsymbol{\delta} + \frac{1}{2} \boldsymbol{\delta}^\top H(\mathbf{x}) \boldsymbol{\delta}
$$

**Newtonæ³•**: äºŒæ¬¡è¿‘ä¼¼ã‚’æœ€å°åŒ–ã™ã‚‹ $\boldsymbol{\delta}$ ã‚’æ±‚ã‚ã‚‹:

$$
\nabla f + H \boldsymbol{\delta} = 0 \implies \boldsymbol{\delta}^* = -H^{-1} \nabla f
$$

```python
import numpy as np

# Rosenbrock function (classic optimization test)
def rosenbrock(xy):
    x, y = xy
    return (1 - x)**2 + 100 * (y - x**2)**2

def rosenbrock_grad(xy):
    x, y = xy
    dx = -2*(1-x) + 100*2*(y-x**2)*(-2*x)
    dy = 100*2*(y-x**2)
    return np.array([dx, dy])

def rosenbrock_hessian(xy):
    x, y = xy
    dxx = 2 - 400*(y - x**2) + 800*x**2
    dxy = -400*x
    dyy = 200.0
    return np.array([[dxx, dxy], [dxy, dyy]])

# Newton's method
x = np.array([-1.0, 1.0])
print(f"Newton's method on Rosenbrock:")
for i in range(10):
    g = rosenbrock_grad(x)
    H = rosenbrock_hessian(x)
    delta = -np.linalg.solve(H, g)
    x = x + delta
    f_val = rosenbrock(x)
    print(f"  Step {i+1}: x={np.round(x, 6)}, f={f_val:.8f}")
    if f_val < 1e-14:
        print(f"  Converged in {i+1} steps!")
        break
```

### 3.12 Softmaxã®å¾®åˆ† â€” Attentionå­¦ç¿’ã®éµ

Softmaxã®å¾®åˆ†ã¯Transformerã®å­¦ç¿’ã§æœ€ã‚‚é »ç¹ã«ç¾ã‚Œã‚‹è¨ˆç®—ã®ä¸€ã¤ã€‚

#### Softmaxã®å®šç¾©ã¨æ€§è³ª

$$
s_i = \text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

æ€§è³ª:
- $s_i > 0$ ã‹ã¤ $\sum_i s_i = 1$ï¼ˆç¢ºç‡åˆ†å¸ƒï¼‰
- $\frac{\partial s_i}{\partial z_j} = s_i(\delta_{ij} - s_j)$

#### ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ã®å°å‡º

$i = j$ ã®ã¨ã:

$$
\frac{\partial s_i}{\partial z_i} = s_i(1 - s_i)
$$

$i \neq j$ ã®ã¨ã:

$$
\frac{\partial s_i}{\partial z_j} = -s_i s_j
$$

ã¾ã¨ã‚ã‚‹ã¨:

$$
\frac{\partial \mathbf{s}}{\partial \mathbf{z}} = \text{diag}(\mathbf{s}) - \mathbf{s}\mathbf{s}^\top
$$

```python
import numpy as np

def softmax(z):
    e = np.exp(z - np.max(z))
    return e / np.sum(e)

def softmax_jacobian(z):
    """Analytical Jacobian of softmax"""
    s = softmax(z)
    return np.diag(s) - np.outer(s, s)

# Verify with numerical differentiation
z = np.array([2.0, 1.0, 0.1])
J_analytical = softmax_jacobian(z)

eps = 1e-7
n = len(z)
J_numerical = np.zeros((n, n))
for j in range(n):
    z_plus = z.copy(); z_plus[j] += eps
    z_minus = z.copy(); z_minus[j] -= eps
    J_numerical[:, j] = (softmax(z_plus) - softmax(z_minus)) / (2 * eps)

print("Softmax Jacobian (analytical):")
print(np.round(J_analytical, 6))
print(f"\nMatch numerical: {np.allclose(J_analytical, J_numerical)}")

# Key property: each row sums to 0
print(f"Row sums: {np.round(J_analytical.sum(axis=1), 10)}")
```

#### Cross-Entropyæå¤±ã®Softmaxå¾®åˆ†

Cross-Entropyæå¤± $L = -\sum_i y_i \log s_i$ ã®Softmaxå…¥åŠ› $\mathbf{z}$ ã«é–¢ã™ã‚‹å‹¾é…:

$$
\frac{\partial L}{\partial \mathbf{z}} = \mathbf{s} - \mathbf{y}
$$

ã“ã®çµæœã¯é©šãã»ã©ã‚·ãƒ³ãƒ—ãƒ«ã€‚å°å‡º:

$$
\frac{\partial L}{\partial z_j} = -\sum_i y_i \frac{1}{s_i} \frac{\partial s_i}{\partial z_j} = -\sum_i y_i \frac{s_i(\delta_{ij} - s_j)}{s_i} = -y_j + s_j \sum_i y_i = s_j - y_j
$$

ï¼ˆ$\sum_i y_i = 1$ ã‚’ä½¿ã£ãŸï¼‰

```python
import numpy as np

z = np.array([2.0, 1.0, 0.1])
s = softmax(z)
y = np.array([1.0, 0.0, 0.0])  # one-hot target

# Analytical gradient: s - y
grad_analytical = s - y

# Numerical gradient
def cross_entropy_loss(z, y):
    s = softmax(z)
    return -np.sum(y * np.log(s + 1e-12))

eps = 1e-7
grad_numerical = np.zeros(len(z))
for j in range(len(z)):
    z_plus = z.copy(); z_plus[j] += eps
    z_minus = z.copy(); z_minus[j] -= eps
    grad_numerical[j] = (cross_entropy_loss(z_plus, y) - cross_entropy_loss(z_minus, y)) / (2 * eps)

print(f"Analytical: {np.round(grad_analytical, 6)}")
print(f"Numerical:  {np.round(grad_numerical, 6)}")
print(f"Match: {np.allclose(grad_analytical, grad_numerical)}")
```

:::message
**LLMã¸ã®æ¥ç¶š**: GPTç³»ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã§ã¯ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã§ Softmax + Cross-Entropy ã®å‹¾é… $\mathbf{s} - \mathbf{y}$ ã‚’è¨ˆç®—ã™ã‚‹ã€‚èªå½™ã‚µã‚¤ã‚ºãŒ50,000ä»¥ä¸Šã®ã¨ãã€ã“ã®è¨ˆç®—ãŒå­¦ç¿’ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ä¸€ã¤ã«ãªã‚‹ã€‚
:::

### 3.13 å¤‰åˆ†æ³•å…¥é–€ â€” å¤‰åˆ†æ¨è«–ã¸ã®äºˆå‘Š

å¤‰åˆ†æ³•ã¯ã€Œé–¢æ•°ã®é–¢æ•°ã€ï¼ˆæ±é–¢æ•°ï¼‰ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚VAEï¼ˆç¬¬15å›ï¼‰ã§ä½¿ã†å¤‰åˆ†æ¨è«–ã®æ•°å­¦çš„åŸºç›¤ã€‚

#### æ±é–¢æ•°ã¨å¤‰åˆ†

**æ±é–¢æ•°**: é–¢æ•°ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€ã‚¹ã‚«ãƒ©ãƒ¼ã‚’è¿”ã™å†™åƒã€‚

$$
F[f] = \int_a^b L(x, f(x), f'(x)) \, dx
$$

ä¾‹: æ›²ç·šã®é•·ã• $F[f] = \int_a^b \sqrt{1 + f'(x)^2} \, dx$

#### Euler-Lagrangeæ–¹ç¨‹å¼

$F[f]$ ã‚’æœ€å°åŒ–ã™ã‚‹ $f$ ã¯ä»¥ä¸‹ã‚’æº€ãŸã™:

$$
\frac{\partial L}{\partial f} - \frac{d}{dx} \frac{\partial L}{\partial f'} = 0
$$

#### å¤‰åˆ†æ¨è«–ã¨ã®æ¥ç¶šï¼ˆäºˆå‘Šï¼‰

VAEã§ã¯ã€çœŸã®äº‹å¾Œåˆ†å¸ƒ $p(\mathbf{z} \mid \mathbf{x})$ ã‚’è¿‘ä¼¼ã™ã‚‹åˆ†å¸ƒ $q(\mathbf{z} \mid \mathbf{x})$ ã‚’è¦‹ã¤ã‘ãŸã„ã€‚ã“ã‚Œã¯ã€ŒKLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã„ã†æ±é–¢æ•°ã‚’ã€åˆ†å¸ƒã®ç©ºé–“ä¸Šã§æœ€å°åŒ–ã™ã‚‹ã€å•é¡Œ:

$$
q^* = \arg\min_q \text{KL}(q(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z} \mid \mathbf{x}))
$$

ã“ã®æœ€é©åŒ–å•é¡Œã‚’è§£ãã®ãŒå¤‰åˆ†æ¨è«–ã€‚ãã®ç†è«–çš„åŸºç›¤ãŒå¤‰åˆ†æ³•ã ã€‚è©³ç´°ã¯ç¬¬15å›ï¼ˆVAEï¼‰ã§æ‰±ã†ã€‚

### 3.14 Boss Battle: Transformer 1å±¤ã®å®Œå…¨å¾®åˆ†

Transformer[^1]ã®1å±¤ã«ãŠã‘ã‚‹ Forward + Backward ã‚’è¡Œåˆ—å¾®åˆ†ã§å®Œå…¨ã«è¨˜è¿°ã™ã‚‹ã€‚

#### Forward Pass

å…¥åŠ› $H \in \mathbb{R}^{T \times d}$ï¼ˆ$T$ ãƒˆãƒ¼ã‚¯ãƒ³ã€$d$ æ¬¡å…ƒï¼‰ã«å¯¾ã—ã¦:

$$
Q = HW_Q, \quad K = HW_K, \quad V = HW_V
$$
$$
S = \frac{QK^\top}{\sqrt{d_k}}, \quad A = \text{softmax}(S), \quad O = AV
$$
$$
\text{output} = OW_O + H \quad \text{(residual connection)}
$$

#### Backward Passï¼ˆ$\frac{\partial L}{\partial W_Q}$ ã®å°å‡ºï¼‰

$L$ ã‚’ã‚¹ã‚«ãƒ©ãƒ¼æå¤±ã¨ã—ã€$\frac{\partial L}{\partial O}$ ãŒæ—¢çŸ¥ã¨ã™ã‚‹ã€‚

$$
\frac{\partial L}{\partial W_Q} = H^\top \frac{\partial L}{\partial Q}
$$

ã“ã“ã§ $\frac{\partial L}{\partial Q}$ ã¯é€£é–å¾‹ã§:

$$
\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial S} \cdot \frac{\partial S}{\partial Q}
$$

$S = QK^\top / \sqrt{d_k}$ ã‚ˆã‚Š $\frac{\partial S}{\partial Q} = K / \sqrt{d_k}$ã€ã¤ã¾ã‚Š:

$$
\frac{\partial L}{\partial Q} = \frac{1}{\sqrt{d_k}} \frac{\partial L}{\partial S} K
$$

Softmax ã®å¾®åˆ†ã¯:

$$
\frac{\partial L}{\partial S_{ij}} = \sum_k \frac{\partial L}{\partial A_{ik}} A_{ik} (\delta_{jk} - A_{ij})
$$

```python
import numpy as np

def softmax(x, axis=-1):
    e = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return e / np.sum(e, axis=axis, keepdims=True)

# Transformer single layer forward + backward
np.random.seed(42)
T, d, dk = 4, 8, 8  # 4 tokens, 8 dims
H = np.random.randn(T, d)
W_Q = np.random.randn(d, dk) * 0.1
W_K = np.random.randn(d, dk) * 0.1
W_V = np.random.randn(d, dk) * 0.1
W_O = np.random.randn(dk, d) * 0.1

# Forward
Q = H @ W_Q
K = H @ W_K
V = H @ W_V
S = Q @ K.T / np.sqrt(dk)
A = softmax(S)
O = A @ V
output = O @ W_O + H  # residual

# Backward (assume dL/doutput = random for demo)
dL_doutput = np.random.randn(T, d)

# dL/dO
dL_dO = dL_doutput @ W_O.T

# dL/dA (from O = AV)
dL_dA = dL_dO @ V.T

# dL/dS (softmax backward)
dL_dS = np.zeros_like(S)
for i in range(T):
    a = A[i, :]  # (T,)
    dL_da = dL_dA[i, :]  # (T,)
    # Jacobian of softmax: diag(a) - a a^T
    J_softmax = np.diag(a) - np.outer(a, a)
    dL_dS[i, :] = J_softmax @ dL_da

# dL/dQ, dL/dK
dL_dQ = dL_dS @ K / np.sqrt(dk)
dL_dK = dL_dS.T @ Q / np.sqrt(dk)

# dL/dW_Q, dL/dW_K, dL/dW_V
dL_dW_Q = H.T @ dL_dQ
dL_dW_K = H.T @ dL_dK
dL_dW_V = H.T @ (A.T @ dL_dO)
dL_dW_O = O.T @ dL_doutput

print("Gradient norms:")
print(f"  dL/dW_Q: {np.linalg.norm(dL_dW_Q):.6f}")
print(f"  dL/dW_K: {np.linalg.norm(dL_dW_K):.6f}")
print(f"  dL/dW_V: {np.linalg.norm(dL_dW_V):.6f}")
print(f"  dL/dW_O: {np.linalg.norm(dL_dW_O):.6f}")

# Numerical verification for dL/dW_Q[0,0]
eps = 1e-5
def forward_loss(W_Q_):
    Q_ = H @ W_Q_
    S_ = Q_ @ K.T / np.sqrt(dk)
    A_ = softmax(S_)
    O_ = A_ @ V
    out_ = O_ @ W_O + H
    return np.sum(out_ * dL_doutput)  # proxy loss

W_Q_plus = W_Q.copy(); W_Q_plus[0, 0] += eps
W_Q_minus = W_Q.copy(); W_Q_minus[0, 0] -= eps
numerical = (forward_loss(W_Q_plus) - forward_loss(W_Q_minus)) / (2 * eps)
print(f"\nNumerical check dL/dW_Q[0,0]:")
print(f"  Analytical: {dL_dW_Q[0,0]:.8f}")
print(f"  Numerical:  {numerical:.8f}")
print(f"  Match: {np.isclose(dL_dW_Q[0,0], numerical, rtol=1e-3)}")
```

:::message
**é€²æ—: 70% å®Œäº†** SVDã®ç†è«–ï¼ˆå­˜åœ¨å®šç†ãƒ»Eckart-Youngãƒ»æ“¬ä¼¼é€†è¡Œåˆ—ãƒ»PCAï¼‰ã€ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ãƒ»Einsteinè¨˜æ³•ã€è¡Œåˆ—å¾®åˆ†ã€é€£é–å¾‹ã€Backpropagationã€è‡ªå‹•å¾®åˆ†ã€Transformer 1å±¤ã®å®Œå…¨å¾®åˆ†ã‚’å°å‡ºã—ãŸã€‚
:::

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). Attention Is All You Need. *NeurIPS 2017*.
@[card](https://arxiv.org/abs/1706.03762)

[^2]: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. *Nature*, 323, 533-536.
@[card](https://doi.org/10.1038/323533a0)

[^3]: Eckart, C. & Young, G. (1936). The Approximation of One Matrix by Another of Lower Rank. *Psychometrika*, 1, 211-218.
@[card](https://doi.org/10.1007/BF02288367)

[^5]: Pearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space. *Philosophical Magazine*, 2(11), 559-572.
@[card](https://doi.org/10.1080/14786440109462720)

[^6]: Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. *Journal of Educational Psychology*, 24(6), 417-441.
@[card](https://doi.org/10.1037/h0071325)

[^7]: Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2018). Automatic Differentiation in Machine Learning: a Survey. *JMLR*, 18(153), 1-43.
@[card](https://arxiv.org/abs/1502.05767)

[^10]: Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. *ICLR 2022*.
@[card](https://arxiv.org/abs/2106.09685)

[^12]: Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & RÃ©, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. *NeurIPS 2022*.
@[card](https://arxiv.org/abs/2205.14135)

[^13]: Rezende, D. J. & Mohamed, S. (2015). Variational Inference with Normalizing Flows. *ICML 2015*.
@[card](https://arxiv.org/abs/1505.05770)

### æ•™ç§‘æ›¸

[^8]: Griewank, A. & Walther, A. (2008). *Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation* (2nd ed.). SIAM.

[^9]: Petersen, K. B. & Pedersen, M. S. (2012). *The Matrix Cookbook*. Technical Report, DTU.

---

## è¨˜æ³•è¦ç´„

| è¨˜å· | æ„å‘³ | åˆå‡º |
|:-----|:-----|:-----|
| $A, B, W$ | è¡Œåˆ—ï¼ˆå¤§æ–‡å­—ï¼‰ | 3.1 |
| $\mathbf{x}, \mathbf{v}$ | ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå¤ªå­—å°æ–‡å­—ï¼‰ | 3.7 |
| $\sigma_i$ | ç‰¹ç•°å€¤ | 3.1 |
| $\mathbf{u}_i, \mathbf{v}_i$ | å·¦/å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ« | 3.1 |
| $U, \Sigma, V$ | SVDã®æ§‹æˆè¡Œåˆ— | 3.1 |
| $A^+$ | Moore-Penroseæ“¬ä¼¼é€†è¡Œåˆ— | 3.4 |
| $A_k$ | rank-$k$ æˆªæ–­SVD | 3.2 |
| $\nabla f$ | å‹¾é… | 3.7 |
| $J$ | ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ | 3.7 |
| $H$ | ãƒ˜ã‚·ã‚¢ãƒ³ | 3.7 |
| $\boldsymbol{\delta}_l$ | ç¬¬$l$å±¤ã®èª¤å·®ä¿¡å· | 3.9 |
| $\otimes$ | Kroneckerç© | 3.6 |
| $\odot$ | Hadamardç©ï¼ˆè¦ç´ ã”ã¨ã®ç©ï¼‰ | 3.9 |
| $\text{vec}(A)$ | è¡Œåˆ—ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ– | 3.6 |
| $\kappa(A)$ | æ¡ä»¶æ•° | 4.4 |

---
